# Causal Machine Learning: A Survey and Open Problems

Jean Kaddour $^ { * , 1 }$ , Aengus Lynch $^ { * , 1 }$ , Qi Liu $2$ , Matt J. Kusner1, Ricardo Silva1 ‚àóEqual contribution. 1University College London. 2University of Oxford. {jean.kaddour.20, aengus.lynch.17}@ucl.ac.uk.

22 July 2022.

# Abstract

Causal Machine Learning (CausalML) is an umbrella term for machine learning methods that formalize the data-generation process as a structural causal model (SCM). This perspective enables us to reason about the effects of changes to this process (interventions) and what would have happened in hindsight (counterfactuals). We categorize work in CausalML into five groups according to the problems they address: (1) causal supervised learning, (2) causal generative modeling, (3) causal explanations, (4) causal fairness, and (5) causal reinforcement learning. We systematically compare the methods in each category and point out open problems. Further, we review data-modality-specific applications in computer vision, natural language processing, and graph representation learning. Finally, we provide an overview of causal benchmarks and a critical discussion of the state of this nascent field, including recommendations for future work.

1 Introduction 1   
2 Causality: A Minimal Introduction 4

2.1 Bayesian Networks . . . 4   
2.2 Causal Bayesian Networks . 7   
2.3 Structural Causal Models 11   
2.4 Causal Representation Learning . . . . 14   
2.5 Spurious Relationships due to Confounding . . . . 15   
2.6 Causal Estimand Identification 16   
2.7 Causal Influence 17

3 Causal Supervised Learning 18

3.1 Invariant Feature Learning 19   
3.2 Invariant Mechanism Learning 32   
3.3 Open Problems 35

4 Causal Generative Modeling 37

4.1 Structural Assignment Learning . . . 38   
4.2 Causal Disentanglement . 43   
4.3 Open Problems 47

5 Causal Explanations 50

5.1 Feature Attribution Explanations 51   
5.2 Contrastive Explanations 55   
5.3 Open Problems 58

6 Causal Fairness 61

6.1 Two more detailed examples 63   
6.2 Counterfactual Fairness Criteria . 64   
6.3 Interventional Fairness . 66   
6.4 Fairness under Distribution Shifts . 67

6.5 Open Problems 68

# 7 Causal Reinforcement Learning 70

7.1 Isn‚Äôt RL already ‚ÄúCausal‚Äù? 70   
7.2 Causal Bandits . 72   
7.3 Model-Based RL 75   
7.4 Multi-Task RL 78   
7.5 Off-Policy Policy Evaluation . 82   
7.6 Imitation Learning . . . 86   
7.7 Credit Assignment . 90   
7.8 Counterfactual Data Augmentation 92   
7.9 Agent Incentives . 94   
7.10 Open Problems 98

# 8 Modality-specific Applications 101

8.1 Causal Computer Vision . . 101   
8.2 Causal Natural Language Processing . . . 110   
8.3 Causal Graph Representation Learning . . . 118

# 9 Causal Benchmarks 122

9.1 Reinforcement Learning . . . 122   
9.2 Computer Vision . 124   
9.3 Natural Language Processing . 125

# 10 The Good, the Bad and the Ugly 127

10.1 The Good 127   
10.2 The Bad 128   
10.3 The Ugly . . 130

# 11 Related Work 132

11.1 Other Surveys 132   
11.2 Machine Learning for Causal Inference . . 133

# 12 Conclusion 138

# Introduction

Today, machine learning (ML) techniques excel at finding associations in independent and identically distributed (i.i.d.) data. A few fundamental principles, including empirical risk minimization, backpropagation, and inductive biases in architecture design, have led to enormous advances for solving problems in fields like computer vision, natural language processing, graph representation learning, and reinforcement learning.

However, new challenges have arisen when deploying these models to real-world settings. These challenges include: (1) large reductions in generalization performance when the data distribution shifts [1], (2) a lack of fine-grained control of samples from generative models [2], (3) biased predictions reinforcing unfair discrimination of certain sub-populations [3, 4], (4) overly abstract and problem-independent notions of interpretability [5], and (5) unstable translation of reinforcement learning methods to real-world problems [6].

Multiple works have argued that these issues are partly due to the lack of causal formalisms in modern ML systems [7, 8, 9, 10, 11]. Subsequently, there has been a surge of interest in the research community on causal machine learning (CausalML), which are machine learning methods that utilize causal knowledge about the to-bemodeled system.1 This survey covers how causality can address open ML problems.

In a nutshell, causal inference provides a language for formalizing structural knowledge about the data-generating process (DGP) via Structural Causal Models (SCMs) [12]. With SCMs, one can estimate what will happen to data after changes (called interventions) are made to its generating process. Going one step further, they also allow us to model the consequences of changes in hindsight while taking into account what happened (called counterfactuals). We introduce these concepts in more detail in Chapter 2, assuming no prior knowledge of causality.

Despite the extensive work on designing various types of CausalML algorithms, a clear categorization of its problems and methodology is lacking. We believe this is partly explained by the fact that CausalML usually involves assumptions about the data unfamiliar to large parts of ML. These assumptions are often tricky to relate across different problem setups (and untestable), making it difficult to measure progress and applicability. These issues are what motivates this survey.

# Contributions

1. We give a minimal introduction to key concepts in causality that is completely self-contained (Chapter 2). We do not assume any prior knowledge of causality. Throughout, we give examples of how these concepts can be applied to help further ground intuition.   
2. We taxonomize existing CausalML work into causal supervised learning (Chapter 3), causal generative modeling (Chapter 4), causal explanations (Chapter 5), causal fairness (Chapter 6), causal reinforcement learning (Chapter 7). For each problem class, we compare existing methods and address avenues for future work.   
3. We review data-modality-specific applications in computer vision, natural language processing, and graph representation learning (Chapter 8), and causal benchmarks (Chapter 9).   
4. We discuss the good, the bad, and the ugly (Chapter 10): what benefits CausalML provides compared to non-causal ML methods (the good), what challenges the field faces at the time of this writing (the bad), and what inevitable price one has to pay for using CausalML techniques (the ugly).

![](images/13f6b2484047aa5394f3c056146ddeb48b2bde04e8f581398aa05f2d4e16f482.jpg)  
Figure 1.1: Taxonomy of Causal Machine Learning.

# Causality: A Minimal Introduction

This section informs the reader of the concepts of causality used in CausalML research. We leave out many formal statements, e.g., proofs, and direct readers interested in such further details to [7, 12, 13, 14].

# Notation

$\mathcal { G }$ Graph

$\mathbf { d e } ( X _ { i } ) , \mathbf { d e } _ { i }$ Descendants of Xi

$\mathbf { a n } ( X _ { i } ) , \mathbf { a n } _ { i }$ Ancestors of Xi

$\mathbf { p a } ( X _ { i } ) , \mathbf { p a } _ { i }$ Causal parents of Xi

do(¬∑) do-operator

M Structural Causal Model

# 2.1 Bayesian Networks

To reason about the causal effects of some random variables on others, we need to formalize causal relations. The canonical representation of causal relations is a causal directed acyclic graph (causal DAG), also called a causal diagram. It can encode a priori assumptions about the causal structure of interest (e.g., from expert knowledge).

Before we define causal DAGs, we introduce some terminology to describe a DAG and Bayesian Networks (BNs): a probabilistic graphical model representing probabilistic relationships between random variables. From there, we motivate modeling causal relationships, as they complement BNs with the ability to reason about interventions and counterfactuals.

# 2.1.1 Graphs

A graph $\mathcal { G }$ is a collection of nodes, and edges that connect (some of) the nodes. In a directed graph, the edges are directed: pointing from one node to another. Visually, arrows indicate this direction; notation-wise, we call nodes connected by one edge adjacent.

A path in a graph is any sequence of adjacent nodes, regardless of the direction of the edges that join them. For example, $A \left. B \right. C$ is a path, but not a directed path.

![](images/dbc3438288766b0c7ecb46ac3e311ae006b1e75d1571a0ccef1f1affef03ea3a.jpg)  
Figure 2.1: DAGs of Bayesian and Causal Bayesian Networks.

The latter is a path consisting of directed edges all directed in the same direction, e.g., $A  B  C$ .

A directed cycle is a directed path that starts from a node $A$ and ends in $A$ . A directed acyclic graph (DAG) is a directed graph with no directed cycles.

In a DAG, edges point from a parent node into a child node. We denote the parents of a node $X$ with $\mathbf { p a } ( X )$ ; and $X$ an ancestor of $Y$ (denoted by $X \in \mathbf { a n } ( Y )$ ), and $Y$ a descendant of $X$ (denoted by $Y \in \mathbf { d e } ( X )$ ) if there is a directed path that starts at node $X$ and ends at node $Y$ .

We denote a random node variable as $X$ , assume that all distributions possess a mass or density function, and write $p ( x )$ to represent its distribution.

# 2.1.2 Graphs as Joint Distribution Factorizations

We are interested in modeling probability distributions over random variables in both probabilistic and causal modeling. One reason why graphs are helpful for that is that they allow one to conveniently express how a joint distribution over a set of random variables factorizes. Specifically, we will show how graphs allow one to encode (conditional) independence relationships. To explain this, we adopt an introductory example given by Pearl [12].

Imagine that you are a farmer. You want to understand better the relationship between the time of the year $T$ , duration of enabled lawn sprinkler $D$ , amount of rain $R$ , and pavement slipperiness $S$ . To be able to conduct controlled simulations, you want to model the joint distribution $p ( t , d , r , s )$ .

The chain rule of probability (or product rule) always permits us to factorize a joint distribution over $N = 4$ variables as a product of $N$ conditional distributions:

$$
p (t, d, r, s) = p (t \mid d, r, s) p (d \mid r, s) p (r \mid s) p (s). \tag {2.1}
$$

More generally, it states that

$$
p \left(x _ {1}, \dots , x _ {N}\right) = p \left(x _ {1}\right) \prod_ {i} p \left(x _ {i} \mid x _ {1}, \dots , x _ {i - 1}\right). \tag {2.2}
$$

Note how modeling this naive factorization takes an exponential number of parameters and quickly becomes intractable as $N$ increases. Neal [13] illustrates this as

follows: Suppose that each $x _ { i }$ is binary and let us denote parameters by $\pmb \theta$ . For modeling the conditional distribution $p ( x _ { n } \mid x _ { n - 1 } , . . . , x _ { 1 } )$ , we only need to model $\theta _ { n } = p ( X _ { n } = 1 \mid x _ { n - 1 } , . . . , x _ { n } )$ because

$$
p \left(X _ {n} = 0 \mid x _ {n - 1}, \dots , x _ {n}\right) = 1 - p \left(X _ {n} = 1 \mid x _ {n - 1}, \dots , x _ {n}\right). \tag {2.3}
$$

However, this implies that we need $2 ^ { n - 1 }$ parameters to model the full joint distribution. For example, if $n = 4$ , to model $p ( x _ { 4 } \mid x _ { 3 } , x _ { 2 } , x _ { 1 } )$ , we now need to store

$$
\theta_ {1} = p \left(x _ {4} \mid X _ {3} = 0, X _ {2} = 0, X _ {1} = 0\right), \tag {2.4}
$$

$$
\theta_ {2} = p \left(x _ {4} \mid X _ {3} = 0, X _ {2} = 0, X _ {1} = 1\right), \tag {2.5}
$$

$$
\vdots \tag {2.6}
$$

$$
\theta_ {8} = p \left(x _ {4} \mid X _ {3} = 1, X _ {2} = 1, X _ {1} = 1\right). \tag {2.7}
$$

Fortunately, some of your farmer colleagues have developed a DAG that describes how the variables are related, shown in Fig. 2.1a. This DAG is helpful because we can use it to describe conditional independence relationships. For example, since the time of the year is independent of all the other variables, we only need to model the marginal $p ( t )$ instead of $p ( t \mid d , r , s )$ . Overall, we can simplify the joint distribution such that, in sum, the conditionals depend only on four instead of six random variables (compared to Eq. (2.1)):

$$
p (t, d, r, s) = p (t) p (d \mid t) p (r \mid t) p (s \mid d, r). \tag {2.8}
$$

This simplification formally relies on the Markov condition.

# Def.: 2.1.1: Markov Condition [12]

Given a graph $\mathcal { G }$ of nodes $\mathbf { X }$ with joint distribution $p ( { \pmb x } )$ , the Markov Condition states that the parents $\mathbf { p a } _ { i }$ of every node $X _ { i }$ make $X _ { i }$ independent of its non-descendants $\mathbf { X } \setminus \mathbf { d e } _ { i }$ , i.e.,

$$
p (x _ {i} \mid \mathbf {p a} _ {i}) = p \left(x _ {i} \mid \mathbf {X} \setminus \mathbf {d e} _ {i}\right).
$$

This condition immediately implies the following factorization of the joint distribution

$$
p (\boldsymbol {x}) = \prod_ {i} p \left(x _ {i} \mid \mathbf {p a} _ {i}\right).
$$

This joint factorization is the product of all variables conditioned on their parents in the graph (if any). The core idea behind Bayesian Networks is to decompose a (potentially large) joint distribution $p ( { \pmb x } )$ into several small conditional ones according to the assumed DAG relations.

# ML Perspective: 2.1.1: Autoregressive Distribution Factorizations

Autoregressive models are one example of modern ML techniques exploiting joint distribution factorizations based on conditional independence assumptions. For example, in generative modeling, some methods [15, 16] represent the distribution over an image $_ { x }$ with a DAG factorization under a particular pixel by pixel order, even if no independence is imposed:

$$
p (\boldsymbol {x}) = \prod_ {i = 1} ^ {N} p \left(x _ {i} \mid \boldsymbol {x} _ {<   i}\right), \tag {2.9}
$$

where $\pmb { x } _ { < i } = [ x _ { 1 } , x _ { 2 } , . . . , x _ { i - 1 } ]$ denotes the vector of pixels with index less than $i$ . The advantage of doing so is, e.g., to make sampling easier or enable to learn conditionals in parallel.

Similarly, for sequence modeling tasks (e.g., language modeling with recurrent neural networks [17]), we often assume that the joint distribution over all tokens can be decomposed into conditionals that only depend on hidden states $^ { h }$ produced by a learnable function $f _ { \theta }$ with parameters $\pmb \theta$ , i.e.,

$$
p (\boldsymbol {x}) = \prod_ {t = 1} ^ {T} p \left(x _ {t} \mid \boldsymbol {h} _ {t - 1}\right), \quad \boldsymbol {h} _ {t} = f _ {\boldsymbol {\theta}} \left(x _ {t}, \boldsymbol {h} _ {t - 1}\right). \tag {2.10}
$$

# 2.2 Causal Bayesian Networks

# 2.2.1 Interventions

BNs give rise to utilizing structural knowledge about the distribution of interest to represent it more efficiently. However, BNs remain oblivious to interventions on the distribution‚Äôs underlying process, as we will see in this subsection.

As previously described, the random variable $D$ quantifies the duration of the sprinkler turned on. Imagine that you are interested in reasoning about what would happen if we fix it to value $d$ , given that, e.g., there might be a non-linear relationship between the sprinkler and the slipperiness that you seek to understand better.

Using the Bayesian network toolbox, perhaps the most obvious strategy to pursue is first to infer the conditional distribution

$$
p (t, r, s \mid d) = p (t) p (d \mid t) p (r \mid t) p (s \mid d, r). \tag {2.11}
$$

Then, if we are only interested in $p ( s \mid d )$ , we can marginalize the other variables out using the sum rule. Alternatively, you may train an estimator for ${ \hat { p } } ( s \mid d )$ .

However, what does $p ( s \mid d )$ mean? It is the distribution of $S$ given that we observe variable $D$ for a fixed value, e.g., $d = 1 0$ minutes. In other words, we restrict our focus to those observations in which the sprinkler was set to $d$ .

Let us think about the implications of examining this observational distribution: $p ( s \mid d )$ accounts for the probability of observing $d$ depending on the season of the year $T$ via $p ( d \mid t )$ and the amount of rain $p ( r \mid t )$ , as can be seen in Eq. (2.11).

Now, switching to the causal inference toolbox, our quantity of interest is typically an intervention, which we denote by the do-operator. Following the above example, we are interested in estimating $p \left( s \mid \operatorname { d o } \left( d \right) \right)$ . The difference in its meaning is that $p \left( s \mid \operatorname { d o } \left( d \right) \right)$ denotes the distribution of $S$ intervened upon the value of $D$ , i.e., if we set it to $d$ . This means that we are not considering a certain sub-population for which we observe $D = d$ , but we reason about what happens to the (total) population after taking the action $\mathrm { d o } \left( d \right)$ .

Returning to our example, Fig. 2.1c shows the new network representation after the action took place. Naturally, we remove the arrow $T  D$ from Fig. 2.1a, because after intervening on $D$ by setting it to a constant value1, it does not depend on any other variables anymore.

Formally, there is a small yet significant difference compared to the conditional observational distribution in Eq. (2.11):

$$
p (t, r, s \mid \operatorname {d o} (d)) = p (t) \underbrace {p (d \mid t)} _ {= 1} p (r \mid t) p (s \mid d, r) \neq p (t, r, s \mid d). \tag {2.12}
$$

We no longer care about the relationship between the year‚Äôs season and sprinklers before the action because that relationship is no longer in effect while we perform the action. Once we physically turn the sprinkler on, a new mechanism in which the season has no say determines the state of the sprinkler.

# Theorem: 2.2.1: Truncated Factorization [12]

We assume that $p$ and $\mathcal { G }$ satisfy the Markov assumption and modularity. Given a set of intervention nodes $\boldsymbol { S }$ , if $_ { \pmb { x } }$ is consistent with the intervention, then

$$
p \left(x _ {1}, \dots , x _ {n} \mid \mathrm {d o} (s)\right) = \prod_ {i \notin S} p \left(x _ {i} \mid \mathbf {p a} _ {i}\right) \tag {2.13}
$$

Otherwise, $p \left( x _ { 1 } , . . . , x _ { n } \mid \mathrm { d o } \left( s \right) \right) = 0$ .

However, to yield a valid Causal BN that enables the estimation of the above interventional distributions, we have to make stronger assumptions than for a Bayesian network, which rest on causal (and not associational) knowledge about the underlying system. We do not cover these here, as we will soon introduce another formalism of causation in Sec. 2.3.

Our takeaway is that Causal BNs differ from regular BNs, e.g. because the conditional independence assumptions in regular BNs do not necessarily imply causality.

![](images/39debe4686542c41a3da0d2662d137992b84a10ad6b98f7c329db6e3b913d72e.jpg)  
Figure 2.2: Conditioning versus intervening. Adapted from [13, 18].

However, their implied factorization will be valid for any recursive set of independencies and any ordering of the variables.

To illustrate the difference between the two, recall our initial example shown in Fig. 2.1a. Here, we conveniently made independence assumptions that seem causal already. Alternatively, we could have come up with an independence structure shown in Fig. 2.1b, where the edges are flipped

$$
p (t, d, r, s) = p (t \mid d, r) p (d, r \mid s) p (s). \tag {2.14}
$$

Based on common sense, the corresponding DAG does not seem causal. According to commonly known laws of nature, we would not expect the pavement‚Äôs slipperiness to cause the amount of rain. Yet, for the purpose of statistical inference, it is perfectly adequate to factorize the joint distribution that way.

Another, more simplistic perspective might be that causal assumptions lie at the highest abstraction level for any modeling problem, above common statistical assumptions like the model‚Äôs flexibility. In pure statistical inference (including most ML setups), we often ignore this level and do not impose strong assumptions on the causality between variables. Therefore, we do not yield the ability to compute interventions, etc.

# 2.2.1.1 Examples of Interventions

The following examples provide intuition on how conditional and interventional distributions differ.

Espresso Machine [19]: Imagine that $Y$ is the pressure in an espresso machine‚Äôs boiler, and $X$ is the reading of the built-in barometer. Given a functioning barometer, $p ( y \mid x )$ is a unimodal distribution centered around $X$ , with randomness due to measurement noise. However, if we forcefully break the barometer and set it to $0$ , it will not affect the pressure in the tank. Therefore, $p ( y ~ \vert ~ \mathrm { d o } ( x ) ) = p ( y ) \neq p ( y ~ \vert ~ x )$ .

Medical Treatment [18]: Imagine a dataset where each observation $( \boldsymbol { x } _ { i } , t _ { i } , y _ { i } ) \in \mathcal { D }$ represents a hospital patient‚Äôs medical history record ${ \bf { \sigma } } _ { { \bf { x } } _ { i } }$ , prescribed drug treatment in form of a molecular graph $\mathbf { \Delta } _ { t _ { i } }$ , and health outcome $y _ { i }$ . Fig. 2.2 illustrates how in this setup the intervention $p \left( \boldsymbol { y } \mid \operatorname { d o } \left( \boldsymbol { t } \right) \right)$ refers to the scenario in which all patients described with medical history features $_ { x }$ take treatment $\textbf { \em t }$ versus the conditional distribution $p ( y \mid t )$ restricting our focus to the subpopulation of $\mathbf { X }$ that received $\textbf { \em t }$ .

# 2.2.2 Counterfactuals

Recall our farmer example from Sec. 2.1.2 and imagine that one day, you observe that the slipperiness $S$ was very high, and even worse, a colleague of yours slipped and broke their arm. You want to determine under what circumstances $S$ would have been reduced on that day. You come up with a hypothesis that you want to test: ‚ÄúIf we had turned the sprinkler off, the slipperiness would have been low.‚Äù

Your hypothesis - an ‚Äúif ‚Äù statement in which the ‚Äúif ‚Äù portion is unrealized - is a counterfactual statement. It incorporates the factual data ( $D$ was non-zero on that day) and an intervention (setting $D$ to $0$ ), in which parts of the environment remain unchanged $( T , R )$ . Thereby, it paves the way to comparing two outcomes under the same conditions, differing only in one aspect.

To formalize a counterfactual statement, we must go beyond just the do-operator. For example, suppose that on that day, we observed slipperiness $s$ , time of year $t$ , rain $r$ , and wetness $w$ . Then, simply writing $p ( s \mid \deg ( d ) , s , t , r )$ leads to a clash between the hypothetical slipperiness and the actual slipperiness observed [20].

One way to clarify the distinction is to label the two outcomes of interests - the factual and counterfactual - with different subscripts. We denote the actual slipperiness by $S$ and the counterfactual slipperiness under the intervention $D = d$ by $S _ { d }$ , such that our estimand becomes $p ( s _ { d } \mid s _ { 0 } , t , r )$ .

This notation is convenient to express our quantity of interest but does not operationalize the estimation of it. We require Structural Causal Models (SCMs) to do the latter. We will introduce these in Sec. 2.3 and conclude the current one with two more examples of counterfactuals.

# 2.2.2.1 Examples of Counterfactuals

Intuitively, counterfactuals are hypothetical retrospective interventions given an observed outcome. They help to explain the data since we can analyze the changes resulting from manipulating each variable.

After comparing conditional and interventional distributions in Sec. 2.2.1.1, we now contrast interventional and counterfactual ones.

Medical Treatment: Let us revisit the medical treatment example in Sec. 2.2.1.1, where we described $p \left( \boldsymbol { y } \mid \boldsymbol { x } , \operatorname { d o } \left( \boldsymbol { t } \right) \right)$ as the scenario in which all patients described with medical history features $_ { x }$ take treatment $\mathbf { \Delta } _ { t }$ . Perhaps the most obvious counterfactual quantity we can think of in this scenario is $p \left( y _ { t ^ { \prime } } \mid x , t , y \right)$ .

How do $p \left( \boldsymbol { y } \mid \boldsymbol { x } , \operatorname { d o } \left( \boldsymbol { t } ^ { \prime } \right) \right)$ and $p \left( y _ { t ^ { \prime } } \mid x , t , y \right)$ semantically differ? The latter can be interpreted as imagining the former ‚Äúafter the fact‚Äù that $\scriptstyle { x , t , y }$ occurred. So instead of asking ‚Äúwhat happens if we give treatment $\textbf { \em t }$ to patient x?‚Äù, the latter is retrospective, asking ‚Äúwhat would have happened if we had given treatment $\mathbf { { \boldsymbol { t } } ^ { \prime } }$ to patient $_ { x }$ instead of $\scriptstyle { \mathbf { \mathit { t } } }$ ?‚Äù.

Table 2.1: Pearl‚Äôs Ladder of Causation [12] (also called Pearl‚Äôs Causal Hierarchy [21]). Distributions on one layer virtually underdetermine information at higher layers, e.g., counterfactuals subsume interventional and associational ones.   

<table><tr><td>Layer</td><td>Activity</td><td>Semantics</td><td>Example</td></tr><tr><td>(1) Associational p(y | x)</td><td>Seeing</td><td>How would seeing x change my belief in Y?</td><td>What does a symptom tell us about the disease?</td></tr><tr><td>(2) Interventional p(y | do(x),z)</td><td>Doing</td><td>What happens to Y if I do x?</td><td>What if I take aspirin, will my headache be cured?</td></tr><tr><td>(3) Counterfactual p(yx&#x27; | x,y)</td><td>Imagining</td><td>Was it x that caused Y?</td><td>Was it the aspirin that stopped my headache?</td></tr></table>

Image Editing: Let $\mathbf { X } \in \mathbb { R } ^ { D }$ be an image. Commonly, we presume that we can describe $\mathbf { X }$ by lower-dimensional, semantically meaningful factors of variation. For example, assume we observe an image $_ { x }$ and infer its (latent) primary object of interest $^ { o }$ and background features $^ { b }$ . We can then sample images with edited backgrounds in a controlled manner by sampling from the counterfactual distribution $\tilde { \pmb { x } } \sim p ( \pmb { x } _ { b ^ { \prime } } \mid \pmb { x } , \pmb { o } , \pmb { b } )$ .

How does $p ( \pmb { x } \mid \pmb { o } , \mathrm { d o } ( \pmb { b } ^ { \prime } ) )$ differ from $p ( \pmb { x } _ { b ^ { \prime } } \mid \pmb { x } , \pmb { o } , \pmb { b } ) ^ { * }$ ? If we sample images from the former distribution, depending on its variance, we may get a very diverse set of images with object and background features $\mathbf { \omega } _ { o , b }$ , respectively. If we sample from the latter, we expect sampled images that look identical except for their background.

# 2.2.3 Pearl‚Äôs Ladder of Causation

Table 2.1 shows Pearl‚Äôs three-layer ladder of causation [12], summarizing the differences between associational (or observational), interventional and counterfactual distributions. It is also called the Causal Hierarchy, because questions at level $i \in \{ 1 , 2 , 3 \}$ can only be answered if information from level $j \ \geq \ i$ is available. Counterfactuals subsume interventional and associational questions, so they sit at the top of the hierarchy. Models that can answer counterfactual questions can also answer questions about observations and interventions, as we will shortly see in Sec. 2.3.

# 2.3 Structural Causal Models

In Sec. 2.2, we learned how causal BNs allow us to move from associational distributions in regular BNs to interventional ones. However, we were not able to construct counterfactual distributions with causal BNs. In this section, we learn another causation formalism that permits counterfactual analysis.

This formalism is a Structural Causal Model (SCM), sometimes called Structural Equation Model or Functional Causal Model [12]. In SCMs, we express causal relationships through deterministic, functional equations. This formalism reflects Laplace‚Äôs conception of natural laws being deterministic and randomness being a purely epistemic notion [12]. Hence, we introduce stochasticity in SCMs based on the assumption that certain variables in the equations remain unobserved.

# Def.: 2.3.1: Structural Causal Model

An SCM $\mathcal { M } : = ( \mathbf { S } , p \left( \epsilon \right) )$ consists of structural assignments ${ \pmb S } = \{ f _ { i } \} _ { i = 1 } ^ { N }$

$$
x _ {i} := f _ {i} \left(\epsilon_ {i}; \mathbf {p a} _ {i}\right), \tag {2.15}
$$

where $\mathbf { p a } _ { i }$ is the set of parents of $x _ { i }$ (its direct causes), and a joint distribution $\begin{array} { r } { p ( \epsilon ) = \prod _ { i = 1 } ^ { N } p ( \epsilon _ { i } ) } \end{array}$ over mutually independent exogenous noise variables $^ { a }$ (i.e. unaccounted sources of variation).

For every SCM, we yield a DAG $\mathcal { G }$ by adding one vertex for each $X _ { i }$ and directed edges from each parent in $\mathbf { p a } _ { i }$ (the causes) to child $X _ { i }$ (the effect).

Eq. (2.15) means that, in any SCM, we have that each variable $X _ { i }$ is caused by parent variables, and unobserved exogenous ‚Äúnoise‚Äù variables $\epsilon _ { i }$ . To denote that the noise variables may play a major role causally, sometimes we use the more standard random variable notation $U _ { i }$ to represent them. However, exogenous variables are present in every SCM, and thus, we often omit them from causal graphs for brevity.

Since every SCM induces a (causal) graph, it also implies the previously introduced Markov condition in Def. 2.1.1. We call this the causal Markov condition, as the DAG contains the causal relationships among variables.

# Theorem: 2.3.1: Causal Markov Condition [12]

Every SCM $\mathcal { M }$ entails a joint density $p _ { \mathcal M } ( \pmb { x } )$ such that each variable $X _ { i }$ is independent of all its non-descendants given its parents $\mathbf { p a } _ { i }$ in $\mathcal { G }$ .

# 2.3.1 Interventions

We already learned the idea of estimating an intervention using a causal BN in Sec. 2.2.1. An SCM allows us to predict the effects of interventions, too: we substitute one or multiple of its structural assignments with the intervention‚Äôs value.

The SCM view further highlights the difference between interventions $p ( \boldsymbol { y } \mid \mathrm { d o } \left( \boldsymbol { x } ^ { \prime } \right) )$ and counterfactuals $p ( y _ { \pmb { x } ^ { \prime } } \mid \operatorname { d o } \left( \pmb { x } ^ { \prime } \right) , \pmb { x } )$ : interventions operate at the population level in the sense that, if we construct the interventional distribution through an SCM, its exogenous noise terms still consist of the prior distribution $p ( \epsilon )$ , and not from the posterior $p ( \pmb { \epsilon } \mid \pmb { x } )$ , which incorporates our knowledge of what already happened.

The solution to a counterfactual query is an individual-level answer to a ‚Äúwhat would have happened if‚Äù question where all the exogenous variable sources had been controlled, leaving information to pass through a (modified) set of structural equations. Information about the exogenous variables can be obtained from observable variables already realized.

# 2.3.2 Counterfactual Inference

To compute counterfactuals, we can manipulate an existing SCM and turn it into a counterfactual one. To do so, we estimate the exogenous noise terms $p ( \pmb { \epsilon } \mid \pmb { x } )$ given the observed datum $_ { x }$ .

# Def.: 2.3.2: Counterfactual SCM [7]

Consider an SCM $\mathcal { M } = ( \mathbf { S } , p \left( \epsilon \right) )$ over nodes $\mathbf { X }$ . Given observations $_ { x }$ , we define a counterfactual SCM by replacing the prior distribution of noise variables $p ( \epsilon )$ with the posterior $p ( \pmb { \epsilon } \mid \pmb { x } )$ :

$$
\mathcal {M} _ {\boldsymbol {x}} := (\mathbf {S}, p (\boldsymbol {\epsilon} \mid \mathbf {x})). \tag {2.16}
$$

Given a counterfactual SCM $\mathcal { M } _ { x }$ , we yield counterfactual distributions by intervening on its structural assignments S. To illustrate this, let $\widetilde { \mathbf { S } }$ denote the modified structural assignments with intervention $\mathrm { d o } ( x _ { i } = \tilde { x } _ { i } )$ . Then, we denote the modified, counterfactual SCM as $\widetilde { \mathcal { M } } : = \mathcal { M } _ { x , \mathrm { d o } ( \tilde { x } _ { i } ) }$ . Finally, $\widetilde { \mathcal { M } }$ yields the counterfactual distribution $p _ { \widetilde { \mathcal { M } } } ( \pmb { x } )$ .

We summarize this counterfactual inference procedure in the following.

# Def.: 2.3.3: Counterfactual Inference [12]

We infer counterfactual queries through a three-step procedure:

1. Abduction: Infer $p ( \pmb { \epsilon } \mid \pmb { x } )$ , the state of the world (the exogenous noise $\epsilon$ ) that is compatible with the observations $_ { x }$ .   
2. Action: Replace the equations (e.g., $\mathrm { d o } ( \tilde { x } _ { i } )$ ) corresponding to the intervention, resulting in a modified SCM $\widetilde { \mathcal { M } } = \mathcal { M } _ { \boldsymbol { x } ; \mathrm { d o } \left( \tilde { \boldsymbol { x } } _ { i } \right) } = \left( \widetilde { S } , p \left( \epsilon \mid \boldsymbol { x } \right) \right)$ .   
3. Prediction: Use the modified model to compute $p _ { \widetilde { \mathcal { M } } } ( \pmb { x } )$ .

# 2.3.3 Independent Mechanism

A nice property of SCMs is the Principle of Independent Mechanisms (sometimes also called autonomy or modularity). It is analogous to the truncated factorization property that we previously discussed in the context of Causal Bayesian Networks (Theorem 2.2.1). Its basic premise is that interventions are local, and intervening on a variable $X _ { i }$ only changes the causal mechanism for $X _ { i }$ , leaving the other mech-

anisms invariant. This principle allows us to encode many different interventional distributions in a single graph [12].

# Def.: 2.3.4: Principle of Independent Mechanisms [12, 13]

An SCM consists of autonomous modules $p ( x _ { i } \mid \mathbf { p a } _ { i } )$ ,

$$
p (\boldsymbol {x}) = p \left(x _ {1}, \dots , x _ {D}\right) = \prod_ {i = 1} ^ {D} p \left(x _ {i} \mid \mathbf {p a} _ {i}\right). \tag {2.17}
$$

This principle implies that if we intervene on a subset of nodes $S \subseteq \{ 1 , \dots , D \}$ , then for all $i$ , we have that

1. If $i \not \in S$ , then $p ( x _ { i } \mid \mathbf { p } \mathbf { a } _ { i } )$ remains unchanged.   
2. If $i \in S$ , then $p ( x _ { i } \mid \mathbf { p a } _ { i } ) = 1$ if $x _ { i }$ is the value that $X _ { i }$ was set to by the intervention; otherwise $p ( x _ { i } \mid \mathbf { p a } _ { i } ) = 0$ .

# 2.4 Causal Representation Learning

The goal of representation learning is to retrieve low-dimensional representations $\mathbf { Z }$ that summarize our high-dimensional data $\mathbf { X }$ , where $\dim ( \mathbf { Z } ) \ll \dim ( \mathbf { X } )$ . The learned representations then facilitate solving downstream tasks, as the features of interest (e.g., an object in the image) are typically not given explicitly in the granular input data (e.g., pixels). However, these representations often rely on spurious associations and yield entangled dimensions that are hard to interpret [22, 23, 24].

In contrast, Causal representation learning (CRL) assumes that an SCM over highlevel causal variables generates the data X. Representations $\mathbf { Z }$ correspond to instances of these typically latent causal variables.

With access to the SCM, we can estimate the data distribution after interventions on these variables or infer counterfactuals for specific data points. Unfortunately, learning the entire SCM is difficult without extensive supervision or domain knowledge. This task consists of three components, summarized in Def. 2.4.1.

# Def.: 2.4.1: Causal Representation Learning [11]

In causal representation learning, we aim to learn a set of causal variables $\mathbf { Z }$ that generate our data $\mathbf { X }$ , s.t. we have access to the following:

1. Causal Feature Learning: an injective mapping $g : { \mathcal { Z } }  { \mathcal { X } }$ s.t. $\mathbf { X } = g ( \mathbf { Z } )$   
2. Causal Graph Discovery: a causal graph $\mathcal { G } _ { \mathbf { Z } }$ among the causal variables $\mathbf { Z }$   
3. Causal Mechanism Learning: the generating mechanisms $p g _ { \mathbf { Z } } ( z _ { i } \mid \mathbf { p a } ( z _ { i } ) )$ for $i = 1 , . . , \dim ( \mathbf { Z } )$

where $\mathbf { p a } ( Z _ { i } ) \subset \{ Z _ { j } \} _ { j \neq i } \cup \epsilon _ { i }$ and $\epsilon _ { i }$ is the exogenous causal parent of $Z _ { i }$ .

![](images/31bc698c6c947ba640a54d88a03df737d3294586bbef2e9c1b580fd75d8e4bce.jpg)  
${ \bf ( a ) } \ p ( \boldsymbol { y } \mid \boldsymbol { x } , t )$

![](images/a99d5c75603c0d9de7400f46eb28b12bfc813667c28ab479e403409946f25463.jpg)  
$\mathbf { \phi } ( \mathbf { b } ) \ p \left( \boldsymbol { y } \mid \mathbf { \mathcal { x } } , \operatorname { d o } \left( t \right) \right)$   
Figure 2.3: Spurious Relationship between $\mathbf { T }$ and $Y$ due to $\mathbf { X }$ being an observed confounder. We adapt the Figures from [13].

# 2.5 Spurious Relationships due to Confounding

Recall the medical treatment example from Sec. 2.2.1.1, where we have a dataset where each observation $( \boldsymbol { x } _ { i } , t _ { i } , y _ { i } ) \in \mathcal { D }$ represents a hospital patient‚Äôs medical history record ${ \bf { \sigma } } _ { { \bf { x } } _ { i } }$ , prescribed drug treatment $\mathbf { \Delta } _ { t _ { i } }$ , and health outcome $y _ { i }$ . In real-world scenarios, the patient‚Äôs pre-treatment health conditions $\scriptstyle { \mathbf { x } } _ { i }$ influence both the doctor‚Äôs treatment prescription and outcome, thereby $\mathbf { X }$ confounds the effect of the treatment $\mathbf { T }$ on the outcome $Y$ (and we call $\mathbf { X }$ a confounder or confounding variable). Moreover, we say that $\mathbf { T }$ and $Y$ are confounded (by $\mathbf { X }$ ), or spuriously associated.

Fig. 2.3 visualizes how associations flow in the observational $p ( \boldsymbol { y } \mid \boldsymbol { x } , t )$ and interventional distribution $p ( \boldsymbol { y } \mid \boldsymbol { x } , \operatorname { d o } ( t ) )$ . In Fig. 2.3a, we see that $p ( \boldsymbol { y } \mid \boldsymbol { x } , t )$ entails both causal and spurious associations from $\mathbf { \Delta T }$ to Y, while $p ( \boldsymbol { y } \mid \boldsymbol { x } , \operatorname { d o } ( t ) )$ isolates the causal association from $\mathbf { T }$ to $Y$ , as shown in Fig. 2.3b. The causal effect flows along directed paths, while spurious associations flow along all unblocked paths. To determine whether a path is unblocked, certain criteria need to be checked, which we do not cover here, but one can find in [12].

Here, we want to highlight the spurious association between $\mathbf { T }$ and $Y$ : Imagine a doctor whose policy is to give expensive treatments to very ill patients with low recovery chances and cheap treatments to very healthy patients with high recovery chances. $Y$ is a scalar that denotes the post-treatment health outcome; the higher, the better. Assuming that cheap and expensive treatments are equally effective, cheap and costly treatments are nonetheless positively and negatively correlated with health outcomes, respectively. This correlation is spurious because it is due to the doctor‚Äôs policy, based on the patient‚Äôs pre-treatment health conditions $\mathbf { X }$ , and not the treatment‚Äôs actual causal effect on the outcome.

When do spurious relationships become problematic? A simplified answer is whenever the confounder is unobserved (also called hidden confounding). The reason is that without further knowledge about the data-generating process, a sophisticated ML model will likely rely on spurious associations in the training dataset, which may not occur anymore when the model is in production. This reliance is a feature, not a bug, though: it would be wasteful if the model would not utilize spurious asso-

![](images/1d7060be0c8282da9bb8faa1bba7cd465bb6cd52e546ada3f667d842a0083017.jpg)  
Figure 2.4: Spurious relationships due to hidden confounding in ImageNet [25, 26]. The hidden confounder animal environment $\mathbf { E }$ caused images of birds to include trees and boughs. The heatmaps highlight causal and spurious associations between images $\mathbf { X }$ and bird labels $Y$ .

ciations if we do not enforce it to avoid them. ML Perspective 2.5.1 illustrates how hidden confounding may harm classification models in a computer vision context. We explore remedies for this issue in Sec. 2.4.

# ML Perspective: 2.5.1: Spurious Relationships in ImageNet [25, 26]

In ML benchmark datasets, unobserved confounding may occur too. Consider the image classification scenario with image $\mathbf { X }$ , a label $Y$ , and an unobserved variable $\mathbf { E }$ that dictates the background of the classification object of interest, e.g., the environment of animals. It causes pictures of birds to include trees and boughs, as illustrated by Singla and Feizi [26] in Fig. 2.4. The authors find that classifiers trained on the ImageNet dataset rely on confounded relationships to classify objects, i.e., the same object in a different background environment is more likely to be misclassified.

# 2.6 Causal Estimand Identification

So far, we have discussed the semantic difference between observational and interventional distributions and the reasons for computing the latter. When is it feasible to estimate the latter? Identification of causal estimands refers to the process of moving from the causal estimand (e.g., $p \left( \boldsymbol { y } \mid \operatorname { d o } \left( \pmb { x } \right) \right) )$ to an equivalent statistical estimand (e.g., $p \left( y \mid \pmb { x } \right)$ ), which we can then estimate from data [13].

We call a causal estimand identifiable if it is possible to compute it from a purely statistical quantity. If it is not identifiable, then regardless of how much data we have, we will not be able to isolate the causal association of interest in our data.

In the absence of hidden confounders and as long as we know the causal graph, the causal estimand is identifiable. For example, suppose our estimand of interest is the average treatment effect $p ( y \mid \mathrm { d o } ( t ) )$ in the scenario of Fig. 2.3a, i.e., the causal effect of treatment $\textbf { \em t }$ averaged over all possible patient features $\mathbf { X }$ . Here, the backdoor criterion is satisfied, which makes $\mathbf { X }$ a valid adjustment set [12]. The mathematical procedure outputting a statistical estimand is called backdoor adjustment.

Generally speaking, given any causal DAG (potentially including unobserved confounders), graphical tests allow us to determine the identifiability of specific causal estimands [27, 28]. Besides the backdoor criterion, a frontdoor criterion exists. We do not cover these criteria here; for this survey, it is sufficient to know about their existence, and that finding them can be automated [29]. We direct readers interested in learning more about them to excellent explanations in [7, 12, 13, 14, 30].

# 2.7 Causal Influence

Besides interventional and counterfactual queries, another common quantity of interest in causal inference is one variable‚Äôs causal influence on another. For example, in Chapter 5, given an input vector $_ { x }$ and a black-box ML model $f _ { \theta } ( \cdot )$ with parameters $\pmb \theta$ , we will look at quantifying the causal influence an input feature $x _ { i }$ has on the model prediction ${ \hat { y } } = f _ { \theta } ( { \boldsymbol { x } } )$ . Another example will occur in Sec. 7.7, where we measure the social influences of agents in multi-agent systems, e.g., how the action one agent takes influences the subsequent action of another agent.

Janzing et al. [31] postulate a set of natural, intuitive requirements that a measure of causal influence should satisfy. Then, they evaluate various information-theoretic measures on whether they meet these desiderata. Finally, they conclude that the KL-divergence is a suitable measure.

# Def.: 2.7.1: Causal influence measured by KL-divergence [31]

Given an SCM $\mathcal { M }$ , the causal influence $\mathcal { M } _ { k  l }$ from node $k$ to node $l$ is

$$
\begin{array}{l} \mathcal {M} _ {k \rightarrow l} = \sum_ {\mathbf {p a} _ {l}} D _ {\mathrm {K L}} \left[ p _ {\mathcal {M}} (x _ {l} | \mathbf {p a} _ {l}) \| p _ {\widetilde {\mathcal {M}}} (x _ {l} | \mathbf {p a} _ {l} \backslash x _ {k}) \right] p _ {\mathcal {M}} (\mathbf {p a} _ {l}) \\ = D _ {\mathrm {K L}} \left[ p _ {\mathcal {M}} \left(x _ {l} \mid \mathbf {p a} _ {l}\right) \| p _ {\widetilde {\mathcal {M}}} \left(x _ {l} \mid \mathbf {p a} _ {l} \backslash x _ {k}\right) \right], \tag {2.18} \\ \end{array}
$$

where $D _ { \mathrm { K L } } ( \cdot | | \cdot )$ denotes the KL-divergence and $p _ { \widetilde { \mathcal { M } } }$ the interventional distribution after removing the edge from node $k$ fto node $l$ .

# Causal Supervised Learning

The goal of supervised learning is to learn the conditional distribution $p ( y \mid { \pmb x } )$ by training on data of the form $\mathcal { D } = \{ ( \boldsymbol { x } _ { i } , y _ { i } ) \} _ { i = 1 } ^ { N }$ , where $\mathbf { X }$ and $Y$ denote covariates and label, respectively. One of the most fundamental principles in supervised learning is to assume that our data $\mathcal { D }$ is independent and identically distributed (i.i.d.). This assumption has strong implications. On the one hand, it allows us to split a set of observations into train, validation, and test dataset, opening up an easy way to perform model training, selection, and evaluation. On the other hand, it implies that the test set and, perhaps more importantly, unseen inputs occurring when the model is in production follow the same distribution as the training set. Put differently, the i.i.d. assumption says that ‚Äúthe past is indicative of the future‚Äù.

With no surprise, the validity of this assumption has been challenged [1, 6]; it has been famously called ‚Äúthe big lie in machine learning‚Äù [32]. Whenever we deploy our models in the real world, we have little to no control over the distribution we observe; e.g., variables can change in frequency (see Fig. 3.1), and novel feature combinations can occur that are not contained in the training set. Put simply, if the i.i.d. assumption breaks down, models relying on it will perform poorly [33].

Let us consider the example in Fig. 3.1, inspired by ML Perspective 2.5.1. In the training dataset, pictures of cows typically exhibit alpine pasture backgrounds, a spurious association caused by the cow‚Äôs natural habitat. If we train a model under the i.i.d. assumption, the model will rely on this spurious association for future predictions: this is a feature, not a bug. Naturally, problems will arise when we use the model in test settings where cow pictures do not include grass backgrounds. Then, our model is at risk of misclassifying cows.

This example illustrates how the i.i.d. assumption breaks down whenever the test distribution differs from the training distribution. As an alternative to the i.i.d. assumption, we can assume that our data is sampled from interventional distributions governed by an SCM. For a given dataset generated across a set of environments $\varepsilon$ , n(xei , yei )Ni=1oe‚ààE , $\left\{ ( \pmb { x } _ { i } ^ { e } , \pmb { y } _ { i } ^ { e } ) _ { i = 1 } ^ { N } \right\} _ { e \in \mathcal { E } }$ we view each environment $e \in { \mathcal { E } }$ as being sampled from a separate interventional distribution. Regarding the example in Fig. 3.1, we view the test dataset as being generated by interventions on the latent variables for background features.

How can we estimate $p ( y \mid x )$ in a principled manner? In the following sections, we will discuss two classes of methods that aim to learn domain-robust, transferable features or mechanisms: in invariant feature learning, we learn a content representation

(A) Cow: 0.99, Pasture: 0.99, Grass: 0.99, No   
![](images/956995f746cfae38e6e5a67eb7410fc5c0c980a6e3f4911ea0b0ef339745f1bf.jpg)  
Person: 0.98, Mammal: 0.98

Figure 3.1: Image classifiers are prone to spurious relationships in the dataset [34]. Cows in ‚Äòcommon‚Äô contexts (e.g., Alpine pastures) are detected and classified correctly (A), while cows in uncommon contexts (beach, waves, and boat) are not detected (B). In such an instance, we can view (B) as a sample from a distribution with intervention on the background. We show the top five labels and confidence produced by ClarifAI.com.   
![](images/87634ee6ed741d5ab6341971b0cbdad78dc6e4c51180ef7053fb8521bdcaa3da.jpg)  
Outdoors: 0.97, Seashore: 0.97

(B) No Person: 0.99, Water: 0.98, Beach: 0.97,

$\mathbf { C }$ of the causal parents of Y, $\mathbf { p a } ( Y )$ , such that $Y \sim p ( y \mid \pmb { c } )$ across all environments. In invariant mechanism learning, we identify a set of mappings $\mathcal { F }$ that allow us to predict $Y$ from $\mathbf { X }$ across a range of interventional distributions.

# Notation

X Observed covariates   
$D$ $\dim ( \mathbf { X } )$   
$Y$ Prediction target (label)   
S Spurious (or style) variables for prediction   
$\mathbf { C }$ Content variables for prediction $( \mathbf { p a } ( Y ) )$   
 Independent exogenous causal parents   
$E$ Environment index   
$\mathcal { E }$ Set of environments   
U Unobserved confounders   
A Protected attributes

# 3.1 Invariant Feature Learning

Invariant feature learning (IFL) is the task of identifying features of our data $\mathbf { X }$ that are predictive of $Y$ across a range of environments $\varepsilon$ . From a causal perspective, the causal parents $\mathbf { p a } ( Y )$ are always predictive of $Y$ under any interventional distribution except where $Y$ itself has been intervened upon. This is because of the Principle of Independent Mechanisms [7] (Def. 2.3.4).

IFL methods often simplify the governing SCM to focus on identifying the causal parents of $Y$ . We can abstract a complex SCM into a simple SCM by collecting the causal parents of $Y$ into one variable, while the other variables are collected into

Table 3.1: Overview of Invariant Feature Learning (IFL) Methods.   

<table><tr><td>Available resources</td><td>Method</td><td>Key Idea</td><td>Ref.</td></tr><tr><td rowspan="4">Content-Invariant Transformations</td><td>Deconfound data</td><td>Perform data augmentations and train on the augmented data</td><td>Sec. 3.1.1.1</td></tr><tr><td>Deconfound intermediate representations</td><td>Deconfound representations obtained from a pre-trained model</td><td>Sec. 3.1.1.2</td></tr><tr><td>Deconfound model during training</td><td>Regularize the model to enforce invariances</td><td>Sec. 3.1.1.3</td></tr><tr><td>Deconfound post-training predictions</td><td>Deconfound predictions by identifying and subtracting the confounder bias</td><td>Sec. 3.1.1.4</td></tr><tr><td rowspan="4">Multiple Environments</td><td>Invariant risk minimization</td><td>Identify content features through constrained optimization</td><td>Sec. 3.1.2.1</td></tr><tr><td>Causal Matching</td><td>Match content features across environments</td><td>Sec. 3.1.2.2</td></tr><tr><td>SCD as latent variables</td><td>Unconstrained optimization through latent variable modeling</td><td>Sec. 3.1.2.3</td></tr><tr><td>Compositional Recognition</td><td>Leverage additional object and attribute labels in data</td><td>Sec. 3.1.2.4</td></tr></table>

another. The most general abstraction we found in the literature is the Style and Content Decomposition (SCD) in Fig. 3.2 [35, 36, 37].

<table><tr><td>Def.: 3.1.1: Style and Content Decomposition [35, 36]</td></tr><tr><td>The style and content decomposition (SCD) is a causal graph of a data generating process (DGP) for X and Y. We call S the style variables and C the content variables, where both are assumed to be latent. The content variables group all of the causal parents of Y, pa(Y), while the style variables group the rest of the variables. The generations of X and Y follow the distributions X ~ p(x | s, c), Y ~ p(y | c). (3.1)</td></tr></table>

Assuming that Def. 3.1.1 holds, we define Invariant Feature Learning as follows.

<table><tr><td colspan="2">Def.: 3.1.2: Invariant Feature Learning [32, 38]</td></tr><tr><td>Invariant Feature Learning (IFL) aims to identify the content features C that cause both X and Y, and a mapping p(y | c), such that 
C = Œ¶(X) s.t. Y ~ p(y | c)</td><td>(3.2)</td></tr></table>

![](images/feda1f6274ec8ab7bb6c6fc072b2c32c657bc4b1f3fff3f2d0b51302e5c92568.jpg)  
Figure 3.2: General set up of the Style and Content Decomposition: $\mathbf { s }$ and $\mathbf { C }$ both generate our data $\mathbf { X }$ , but only $\mathbf { C }$ generates $Y$ . The mapping $p ( y \mid c )$ is assumed to be invariant across environments generated by interventions on S. Often we observe spurious associations between $\mathbf { s }$ and $\mathbf { C }$ . This is explained by the presence of the unobserved confounder $\mathbf { U }$ .

# 3.1.1 Content-invariant transformations

This section reviews IFL methods that extract content features by applying contentinvariant transformations to the training dataset, intermediate representations, or predictions.

# 3.1.1.1 Deconfounding data

Data Augmentation (DA) is ubiquitous in modern machine learning pipelines involving high-dimensional datasets. The idea is to identify a set of transformations that can be applied to any sample in our dataset yet will not change its semantics (i.e., its content features). It requires domain knowledge from a practitioner about the types of permissible transformations that avoid diluting the content information. Its motivation is to enforce invariances in a model and, therefore, to improve generalization.

This section reviews works that motivate data augmentation through a causal perspective [39, 40, 41]. This perspective views data augmentations as constructing counterfactuals of instances, where we intervene on style features, akin to the Style and Content Decomposition (Fig. 3.2).

# Causal Perspective: 3.1.1: Data augmentation [39, 40]

Data augmentation can be viewed as a set of interventions on the style variable in the style and content decomposition of Fig. 3.2, which breaks any spurious associations between style variables and $Y$ .

The augmentations in this section are hand-crafted. Later in Chapter 4, we will review how to generate counterfactual data augmentations using causal generative models.

Ilse et al. [40] explains that DA weakens the spurious association between observed domains and task labels. To show that, they introduce the concept of interventionaugmentation equivariance, formalizing the relationship between data augmentation and interventions on features caused by the domain. If intervention-augmentation

equivariance holds, one can use data augmentation to simulate new environments $e$ using only observational data. This simulation removes the spurious association $E \left. \mathbf { U } \right. Y$ caused by a hidden confounder $\mathbf { U }$ , which allows $E$ to affect $Y$ through the back door. Based on this insight, they derive an algorithm that can select data augmentation techniques from a list of transformations, leading to better domain generalization.

Kaushik et al. [42] propose a method of data augmentation via a human-in-the-loop process in which, given some documents and their (initial) labels, humans must revise the text with edits sufficient to flip the label and thus generate a counterfactual sample. Notably, the method prohibits edits not sufficient to flip the applicable label. Hence, they generate negative and positive samples, allowing for contrastive learning as an approach to representation learning.

Teney et al. [43] develop this idea further by incorporating samples with contrasting labels but similar semantic information (i.e. similar content variables), i.e. counterfactual samples, as a means of improving the training of model $f ( \cdot )$ (see Chapter 4 for further comment). They propose a regularization term that enforces alignment between local gradients $\nabla _ { \pmb { x } } f ( \pmb { x } _ { i } )$ and a ground truth gradient. The ground truth gradient mimics the translation in the input space necessary to switch the model output between two contrasting samples and their corresponding labels.

Specifically, for two counterfactual samples $\{ ( { \pmb x } _ { i } , y _ { i } ) , ( { \pmb x } _ { j } , y _ { j } ) \}$ with $y _ { i } \neq y _ { j }$ , define $\pmb { g } _ { i } = \nabla _ { \pmb { x } } f \left( \pmb { x } _ { i } \right)$ and $\hat { \pmb { g } } _ { i } = \pmb { x } _ { j } - \pmb { x } _ { i }$ . Then their proposed regularization term enforces similarity between $\hat { \mathbf { \mathbf { g } } } _ { i }$ and ${ \bf { \nabla } } _ { { \bf { { j } } } _ { i } }$ . The method can be applied wherever a practitioner has access to counterfactual samples. The authors demonstrate improvements in generalization for vision and language tasks such as visual question answering.

Mao et al. [44] derive an optimal intervention strategy using data augmentations to mimic causal interventions and propose a loss function, GenInt, that incorporates data augmentations generated by GANs. They propose to empirically identify a suite of data augmentations that do not interfere with the object labels according to the method proposed in GANSpace [45]. They intervene directly on latent factors in the original data, such as background and angle of view, as seen in Fig. 3.3. Extensive experiments demonstrate superior OOD classification accuracy on ImageNet-C using AlexNet [46] architecture compared to rival data augmentation strategies.

# 3.1.1.2 Deconfounding intermediate representations

In this setting, we have access to samples of $\mathbf { X }$ and pre-trained representations R. Mao et al. [47] propose a method (see Algorithm 1) that improves how the pre-trained representations, learned by self-supervised learning or otherwise, are leveraged for classification models. They assume that the data is generated by the causal structure shown in Fig. 3.4, with $\mathbf { X } \ \sim \ p ( { \pmb x } \ | \ { \pmb c } , { \pmb s } )$ generated by style and content features, and $\mathbf { R }$ the representation of $\mathbf { X }$ that a pre-trained model produces. Importantly, they assume the presence of unobserved confounding $\mathbf { U } \mathbf { s } , Y$ between S

![](images/796b0129a5bb105f91d91ea4fc4508b500e237dbdee5988d2fcaaa1d40f55457.jpg)

![](images/dd091a23f3eca4860202e0a8f2fa3f7268b1d6da90a27faa20fe6c88b98334d9.jpg)

![](images/00b12874778c84a2ee53d0820d07ef2c9fa52eab90197a1d6cf13f04fccbce8d.jpg)  
Rotation ÔºàPrincipal Component 5)

![](images/8c3efc7b449cdd16a8af48e61d880fd04e10be510b81acb54e21a4607e45b518.jpg)

![](images/1767c1a229eb08455b4e64b23f61f13c4e6ac09ff58fae533eeb215c66bbc4fc.jpg)

![](images/ea8ae2d14add32383427669b03a712eceaba2bd11c7f7a73b7da48aabe8902c6.jpg)  
ScaleÔºàPrincipal Component 10Ôºâ

![](images/743221f6ed8141db9c943884b546181550e0ecefd9a890b157b4fec18bf10405.jpg)

![](images/1311304e72108f706010fc04806d805cd307fc3b691e106cc0323fff6ea2dbf2.jpg)

![](images/37fb2de8bbc58eaf5527444c1cf2ef87643a0f40475a11bb04d8e14fba75286a.jpg)  
Background (Principal Component 12)   
Figure 3.3: Mao et al. [44] use GANSpace [45] to generate a variety of data augmentations: some of the data augmentations are seen here. The data augmentations are interpreted as interventions on style features that our predictor $p ( y \mid x )$ must be invariant to.

and $Y$ , which naturally explains the cause of spurious associations between $\mathbf { X }$ and $Y$ in any training dataset.

Algorithm 1 Causal-Transportability Model Training [47]   
Input: Training set $D$ over $\{(\mathbf{X},Y)\}$ Phase 1: Compute $\hat{p} (\boldsymbol {r}\mid \boldsymbol {x})$ from representation of VAE or pre-trained model.

Phase 2: Sample $\pmb{x}_i, \pmb{r}_i, y_i \sim \mathcal{D}' := (\mathbf{X}, \mathbf{R}, Y)$

for $i = 1,\dots,K$ do Random sample $\pmb{x}_i^{\prime}$ from the same category as $\pmb{x}_i$ Train $\hat{p} (y\mid x',r)$ via minimizing $\mathcal{L}_{\mathrm{class}}$

```txt
end for 
```

Output: Model $\hat{p} (\boldsymbol {r}\mid \boldsymbol {x})$ and $\hat{p} (y\mid \boldsymbol {x},\boldsymbol {r})$

Since $\mathbf { s }$ and $Y$ are confounded by $\mathbf { U } _ { \mathbf { S } , Y }$ , and $\mathrm { ~ \bf ~ S ~ } \to \mathrm { ~ \bf ~ C ~ }$ in the assumed causal graph (Fig. 3.4), $p ( y \mid c )$ is confounded too. However, both $p \left( \boldsymbol { y } \mid \boldsymbol { c } , \operatorname { d o } \left( \boldsymbol { s } \right) \right)$ and $p \left( \boldsymbol { y } \mid \operatorname { d o } \left( \boldsymbol { c } \right) , \boldsymbol { s } \right)$ are unconfounded (i.e. $p \left( \boldsymbol { y } \mid \mathrm { d o } \left( \boldsymbol { x } \right) \right)$ is unconfounded). Thus, Mao et al. [47] suggest that $p \left( \boldsymbol { y } \mid \boldsymbol { c } , \operatorname { d o } \left( \boldsymbol { s } \right) \right)$ is an invariant predictor. To simulate this intervention for a given sample $_ { x }$ , they obtain the representation via $p ( \pmb { r } \mid \pmb { x } )$ , and a batch of corrupted images sampled randomly from the dataset. The corruption of the image is intended to destroy high level information and thus simulate interventions on $\mathbf { s }$ features.

Mao et al. [47] introduce a training algorithm (Sec. 3.1.1.2) that learns $p ( y \mid \pmb { x } , \pmb { r } )$ by minimizing a classification loss $\scriptstyle { \mathcal { L } } _ { \mathrm { c l a s s } }$ , and a separate evaluation algorithm that estimates the causal quantity $p ( \boldsymbol { y } \mid \mathrm { d o } ( \boldsymbol { x } ) ) .$ . The output of the classification algorithm reads as $\hat { y } = \mathrm { a r g m a x } _ { y } p \big ( y \mid \mathrm { d o } ( \pmb { x } ) \big )$ .

In experiments, the authors utilize pre-training methods such as SimCLR [48] and SWAV [49] to yield $\mathbf { R }$ . They then compare against the ERM loss for classification on OOD benchmarks such as ImageNet9 [50], demonstrating clear improvements. Their method performs best on ColoredMNIST [32], where it outperforms IRM (Sec. 3.1.2.1) and GenInt [44] (Sec. 3.1.1.1).

![](images/044496627a484025777103568d534af1c866ce75060629e9503ce82d0951ac4c.jpg)  
Figure 3.4: Causal Transportability [47] identifies causal parents from a set of representations: Mao et al. [47] propose an algorithm to make better use of pre-trained representations $\mathbf { R }$ by considering the shown causal graph (Algorithm 1). S and $\mathbf { C }$ are the causal parents of $\mathbf { X }$ . R is a set of representations that have been identified using a feature learner, such as a pre-training algorithm, that carry at least all the information in $\mathbf { C }$ . S and $Y$ are confounded by a hidden variable $\mathbf { U } _ { X Y }$ .

# 3.1.1.3 Deconfounding models during training

Counterfactual Invariance: Counterfactual invariance is a framework for constructing predictors whose predictions are invariant to certain perturbations on $\mathbf { X }$ [51], using practitioner specifications. To define invariance, we specify an additional variable A that captures information that should not influence predictions. However, A may causally influence the covariates X.

Consider the image dataset in Fig. 3.1 as a motivating example. We find that our classifier seems unable to identify cows when the background is changed from the mountains to a beach. In this instance, we identify background as A, since changing our background should not affect the model prediction, but it does have a causal effect on the covariates $\mathbf { X }$ .

Let $\mathbf { X } _ { a }$ denote the counterfactual $\mathbf { X }$ we would have seen had A been set to $\textbf { \em u }$ , leaving all else fixed.

# Def.: 3.1.3: Counterfactual Invariance [51]

A predictor $f ( \cdot )$ is counterfactually invariant to $\mathbf { A }$ if for any given sample $_ { x }$ , counterfactual $\pmb { x } _ { \pmb { a } } \sim p ( \pmb { x } _ { \pmb { a } } \mid \pmb { x } )$ , and $\forall a \in { \mathcal { A } }$ , we have $f ( { \pmb x } ) = f ( { \pmb x } _ { a } )$ .

This framework requires the practitioner to identify i) the causal direction $\mathbf { X }  Y$ or $\mathbf { X }  Y$ , ii) the sensitive attributes A and iii) whether the association between A and $Y$ is due to confounding or selection bias in the data collection. See Fig. 3.5 for a graphical illustration. With this information, the practitioner can appropriately choose between one of two regularizers that enforce a counterfactual invariance signature (a necessary but not sufficient condition). This approach was restricted to binary A in the implementation in Veitch et al. [51], but conceptually, one may extrapolate the idea to higher dimensions with appropriate conditioning on A.

![](images/c51dda4043f51a322c1827dd77fec2dc0ab5420132207cf5c82111bf393ac5dd.jpg)  
(A) Causal direction

![](images/36d0ad72e8ad3ac184f24186b7de4fb4051a51bdfbad201ae4554d162a9fefd7.jpg)  
(B) Anti-causal direction   
Figure 3.5: Counterfactual invariance causal graphs [51]: The counterfactual invariance framework accommodates the above causal models (Sec. 3.1.1.3). In both $\mathbf x \to Y$ and $\mathbf x \gets Y$ , $\mathbf { A }$ is not a causal parent of $Y$ and we aim to remove any spurious influence A may have on our predictor $p ( y \mid x )$ . Thus, we aim to predict $Y$ solely using $\mathbf { X } _ { 1 }$ features.

If A is a protected attribute (e.g., sex or race) that we would like to be an invariant input to the predictor, this definition is connected to counterfactual fairness, introduced in Def. 6.2.1.

Asymmetry learning: Similar to the counterfactual invariance framework, Mouli and Ribeiro [52] present asymmetry learning for learning classifiers which are counterfactually invariant to certain distribution shift settings. The practitioner specifies a set of equivalence relations in the data generating process to which a model shall be invariant. Mouli and Ribeiro [52] introduce a theoretical paradigm for out-ofdistribution generalization that models the data generating process and specifies the types of test distributions we can generalize to. Asymmetry learning takes $\mathbf { X }$ , $Y$ , the set of provided equivalence relations as input.

As a motivating example, consider the pendulum experiment included in [52]. Two global properties $\rho _ { 1 } ( . ) , \rho _ { 2 } ( . )$ are identified for any pendulum swing $_ { x }$ , one of which is the initial potential energy of the system, $\rho _ { 1 } ( . )$ . Then, they define an equivalence relation $\sim _ { 1 }$ such that for any two samples $\mathbf { \boldsymbol { x } } ^ { ( 1 ) } , \mathbf { \boldsymbol { x } } ^ { ( 2 ) }$ which have the same initial potential energy, $\rho _ { 1 } ( \pmb { x } ^ { ( 1 ) } ) = \rho _ { 1 } ( \pmb { x } ^ { ( 2 ) } )$ , we say $\pmb { x } ^ { ( 1 ) } \sim _ { 1 } \pmb { x } ^ { ( 2 ) }$ .

Each equivalence relation $\sim _ { i }$ induces a set of object transformations $\tau _ { i }$ that preserve equivalence class membership. Specifically, for any $\pmb { x } \in [ \pmb { x } ] _ { i }$ and for any $t \in \mathcal T _ { i }$ , we have $t \circ \pmb { x } \in [ \pmb { x } ] _ { i }$ . These transformations are viewed as defining how the data is generated, such that $\mathbf { X } ^ { \mathrm { t r } } : = T ^ { \mathrm { t r } } \circ \mathbf { X } ^ { \dagger }$ and $\mathbf { X } ^ { \mathrm { t e } } : = T ^ { \mathrm { t e } } \circ \mathbf { X } ^ { \dagger }$ , with $P ( T ^ { \mathrm { t r } } ) \neq P ( T ^ { \mathrm { t e } } )$ . Each $T$ is viewed as a concatenation of transformations associated with a given equivalence relation, $T = t _ { 1 } \circ \dots \circ t _ { r }$ for arbitrary $r$ . The differences between $T ^ { \mathrm { t r } }$ and $T ^ { \mathrm { t e } }$ are found in a subset of the transformations, for instance we may have $T ^ { \mathrm { t r } } = t _ { 1 } \circ t _ { \mathrm { t r } } \circ t _ { 3 }$ and $T ^ { \mathrm { t e } } = t _ { 1 } \circ t _ { \mathrm { t e } } \circ t _ { 3 }$ .

The task of OOD generalization is to make assumptions about which equivalence relation-induced transformations will remain invariant between test and training data and enforce the invariances in learning our prediction model. The training data needs sufficient variation among the global properties for the learning procedure to identify invariances.

Self-supervised learning using SCD: While the previous methods aimed to improve prediction for one task, the following methods explore how the principles of IFL can be exploited for self-supervised learning (SSL), where representations are learned to assist with a suite of downstream tasks. This strategy has paved the way to utilize a vast amount of unlabeled data in settings for which the availability of labeled data is often limited, e.g., for large-scale language modeling [53], medical image analysis [54], or molecular property prediction [55]. We can divide prevailing approaches primarily into whether they are reconstructive [56] or discriminative [48, 57, 58]. Within the discriminative regime, we aim to enforce proximity between representations of similar objects.

Contrastive learning is an approach within the discriminative regime that utilizes positive and negative sampling to enforce proximity between semantically equivalent samples and farness between semantically different samples in representation space. Many contrastive learning approaches rely on some form of data augmentation to generate positive and negative samples, as well as observing data from different environments but with similar labels (e.g., mutli-view contrastive learning [59]). K√ºgelgen et al. [41] prove that data augmentations for SSL can indeed identify the invariant content features for representation learning in both the reconstructive and discriminative setting. From a causal perspective, they interpret contrastive objectives as comparing counterfactuals under soft style interventions.

Mitrovic et al. [39] argue that contrastive pre-training approaches implicitly assume the SCD causal structure illustrated in Fig. 3.2. Therefore, it teaches an encoder to disentangle $\mathbf { C }$ and $\mathbf { s }$ causally. The authors examine the setting where we have access to data augmentations to offer positive and negative samples for the contrastive loss and propose an objective named ReLIC.

Conventional data augmentations can be interpreted as interventions on the style variable $\mathbf { s }$ (see Causal Perspective 3.1.1). Mitrovic et al. [39] frame the task of self-supervised learning as proxy task prediction, where they propose a list $\mathcal { V }$ of proxy labels $y _ { t }$ . To explicitly enforce proxy target prediction invariance under data augmentations, they formalize a criterion and propose to enforce it during training by adding a KL-divergence term to the objective. Using Principle of Independent Mechanisms (Def. 2.3.4), they conclude that under SCD, performing interventions on S does not change the conditional distribution $p \left( y _ { t } \mid c \right)$ , i.e. manipulating the value of $\mathbf { s }$ does not influence this conditional distribution. Thus, $p \left( y _ { t } \mid c \right)$ is invariant under changes in style S, i.e. for all ${ \pmb s } ^ { ( 1 ) } , { \pmb s } ^ { ( 2 ) } \in \mathcal { S }$ ,

$$
p \left(y _ {t} \mid \boldsymbol {c}, \operatorname {d o} \left(\boldsymbol {s} ^ {(1)}\right)\right) = p \left(y _ {t} \mid \boldsymbol {c}, \operatorname {d o} \left(\boldsymbol {s} ^ {(2)}\right)\right) \tag {3.3}
$$

As follow-up work, Tomasev et al. [60] propose ReLICv2, which differs from ReLIC in the selection of appropriate sets of positive and negative points and how to combine resulting views of data in the objective function.

# 3.1.1.4 Deconfounding post-training predictions

A variety of methods [61, 62, 63] propose to remove the influence of unobserved confounders $\mathbf { U }$ from predictions through counterfactual regularization, after model training has ended. This involves estimating the confounding effect of $\mathbf { U }$ on the prediction $\bar { Y }$ and then subtracting it, thus deconfounding the prediction. For a prediction over sample $_ { x }$ , $\mathbf { x } ^ { \prime }$ is generated such that it carries none of the causal information in $_ { x }$ . Then, the prediction is deconfounded using the difference

$$
\tilde {Y} _ {\text {c a u s a l}} = \tilde {Y} _ {\boldsymbol {x}} - \hat {Y} _ {\boldsymbol {x} ^ {\prime}}. \tag {3.4}
$$

![](images/f87f58359fa1c31672ce0361818ecdeb86a0fa0c85d1208ce24cd39ea0e9bcbd.jpg)  
(a) Original causal graph

![](images/661c206f070db75635caec8c83e978bb36aa85942189d459a85e6dbca5124a8d.jpg)  
(b) Intervention on X   
Figure 3.6: Counterfactual Regularization to remove unobserved confounding: These graphs explain how counterfactual regularization can eliminate the effect of unobserved confounding on model predictions, as explained in Sec. 3.1.1.4. An intervention on. $\mathbf { X }$ eliminates the influence of $\mathbf { U }$ on it, and deconfounds the causal effect $\mathbf x \to Y$ .

Chen et al. [61] apply an intervention to $\mathbf { X }$ by setting it to a zero vector (or random noise), identifying the resulting prediction of $\hat { Y }$ , and implementing Eq. (3.4). The authors propose to use this for domain generalization in trajectory prediction. Rao et al. [63] propose improving attention mechanisms for visual categorization, and Niu et al. [62] propose removing language bias in visual question answering.

# 3.1.2 Multiple Environments

It is often possible to obtain datasets collected from multiple environments $\varepsilon$ in the form n(xei , yei )Ni=1oe‚ààE . $\left\{ ( \pmb { x } _ { i } ^ { e } , \pmb { y } _ { i } ^ { e } ) _ { i = 1 } ^ { N } \right\} _ { e \in \mathcal { E } }$ For example, data may be records obtained between hospitals under varying protocols or images of houses collected in different seasons. As a more concrete example, the WILDS benchmark [64] offers a curated collection of multiple environment data from real-world scenarios.

As many machine learning algorithms operate on i.i.d. data, practitioners may shuffle the data collected from multiple environments. Arjovsky et al. [32] suggest that information about the data generating process is lost when we shuffle, and spurious associations can occur in the data after doing so. Furthermore, by shuffling the data, we lose any distinction between associations across and within environments. For example, a spurious association exists between grass features and the cow label in Fig. 3.1, yet this association does not hold across environments if one environment is alpine pastures and another is the beach.

How can we use this information effectively? From a causal perspective, we can treat each environment as being generated from a set of interventions on style variables

in the SCD (Fig. 3.2). Each environment encodes a specific set of interventions, and a greater variety of environments can reveal more style variables.

# Causal Perspective: 3.1.2: Data collected from multiple environments

For data in the form $\{ \mathbf { X } ^ { e } , Y ^ { e } \} _ { e \in \mathcal { E } }$ where $\varepsilon$ is a set of environments, it is often reasonable to view each environment as being generated by a set of interventions on the style variables in the SCD (Fig. 3.2). Of course, this perspective fails when the underlying SCM differs between environments.

The methods in this section propose identifying the predictive features across all environments and learning an invariant mapping from the invariant features to the output variable $Y$ . We interpret the invariant features as content features in the SCD or causal parents of Y.

# 3.1.2.1 Invariant Risk Minimization

Peters et al. [38] introduce Invariant Causal Prediction (ICP), an algorithm to find the causal feature set, the minimal set of features which are causal predictors of a target variable. They exploit the invariance property given by the Principle of Independent Mechanisms (Def. 2.3.4)

Invariant Risk Minimization (IRM) [32] is an extension of ICP that, instead of selecting variables, learns representations free of spurious associations. It posits the existence of a feature map $\Phi$ such that the optimal linear classifier ${ \hat { w } } : \Phi ( \mathbf { X } ) \to \mathcal { Y }$ which maps these features to the output is the same for every environment $e \in { \mathcal { E } }$ . The feature map and classifier compose to form a prediction function $f ( \pmb { x } ) = \ b { w } \circ \Phi ( \pmb { x } )$ . Arjovsky et al. [32] define the optimally-invariant classifier as one that minimizes the worst-case environment-specific empirical risk $\mathcal { R } ^ { e } : = \mathbb { E } _ { \mathbf { X } ^ { e } , Y ^ { e } } \left[ \ell \left( f \left( \mathbf { X } ^ { e } \right) , Y ^ { e } \right) \right]$ over all environments.

The authors argue that such a function will use only invariant features since noninvariant features will have varying associations with the label between different environments. We interpret this as an instance of the Style and Content Decomposition Fig. 3.2, where $\Phi ( \mathbf { X } )$ represents our content variable $\mathbf { C }$ and $\mathbf { s }$ variability is observed over a variety of environments.

# Def.: 3.1.4: Invariant Risk Minimization [32]

To learn $\Phi ( \mathbf { X } )$ and an invariant classifier function $w _ { \beta } : \Phi ( { \bf X } )  \mathcal { Y }$ parameterized by $\beta$ , the IRM objective is the following constrained optimization problem:

$$
\min  _ {\Phi , \hat {\beta}} \sum_ {e \in \mathcal {E}} \mathcal {R} ^ {e} (\Phi , \hat {\beta}) \quad \text {s . t .} \quad \hat {\beta} \in \underset {\beta} {\arg \min } \mathcal {R} ^ {e} (\Phi , \beta) \quad \forall e \in \mathcal {E}. \tag {3.5}
$$

In practice, this bilevel program is highly non-convex and difficult to solve. To find an approximate solution, the authors consider a Lagrangian form, whereby the sub-

optimality with respect to the constraint is expressed as the squared norm of the gradients of each of the inner optimization problems:

$$
\min  _ {\Phi , \hat {\beta}} \sum_ {e \in \mathcal {E}} \left[ \mathcal {R} ^ {e} (\Phi , \hat {\beta}) + \lambda \left\| \nabla_ {\hat {\beta}} \mathcal {R} ^ {e} (\Phi , \hat {\beta}) \right\| _ {2} ^ {2} \right] \tag {3.6}
$$

Assuming the inner optimization problem is convex, achieving feasibility is equivalent to the penalty term being equal to 0. Thus, Eq. (3.5) and Eq. (3.6) are equivalent if we set $\lambda = \infty$ .

Unfortunately, both Rosenfeld et al. [65] and Kamath et al. [66] show that IRM often performs no better than standard empirical risk minimization (ERM). IRM achieves its best results when the underlying causal relations are linear, and there is sufficient heterogeneity observed in the training environments, such that enough degrees of freedom are eliminated [65]. However, if either of these conditions is not met, then IRM can achieve worse results than ERM. Ahuja et al. [67] pose the IRM objective as finding the Nash equilibrium of an ensemble game among several environments.

Ahuja et al. [68] point out that while IRM-like approaches provably generalize OOD in linear regression tasks, they do not necessarily in linear classification tasks, which require much stronger restrictions in the form of support overlap assumptions on the distribution shifts. They establish that augmenting the invariance principle with information bottleneck, [69] constraints resolves some of these issues.

Similar to IRM, Krueger et al. [70] propose Risk Extrapolation (REx), a domain generalization method which also uses a weaker form of invariance than ICP. However, while IRM specifically aims for invariant prediction, REx seeks robustness to whichever forms of a distributional shift are present. The authors prove that variants of REx can recover the causal mechanisms of the targets while also providing some robustness to covariate shift.

Wang and Jordan [24] note that invariant features should be non-spurious but also efficient. To assess non-spuriousness and efficiency simultaneously, they require the representations to satisfy a probability of necessity and sufficiency (PNS) condition, which captures the following behavior.

A representation is non-spurious and efficient if the label responds to the feature both ways: non-spuriousness refers to the effectiveness of turning on the label by turning on the feature captured by the representation. Efficiency is the effectiveness of turning off the label by turning off this feature. In other words, if the feature is enabled, the label will be enabled; if it is disabled, the label will be disabled. PNS calculates the probability of this condition. The authors turn this condition into a constrained optimization objective, which they call Causal-Rep.

Jiang and Veitch [71] study anti-causal domain shifts, where $\mathbf { X }  Y$ , instead of $\mathbf { X } \to Y$ . Similar to IRM, the authors propose a training objective for extracting an invariant predictor. Recall that IRM aims to build predictors that only rely on the causal parents of $Y$ . If the observed covariates $\mathbf { X }$ may be all caused by $Y$ , the causal

parents of $Y$ are the empty set. In this case, the naive causally invariant predictor is vacuous. Jiang and Veitch [71] address these scenarios.

Wang et al. [72] introduce Invariant-feature Subspace Recovery (ISR), addressing the fact that IRM and its extensions cannot generalize to unseen environments with less than $d _ { s } + 1$ training environments, where $d _ { s }$ is the dimension of the spuriousfeature subspace. They propose two algorithms, ISR-Mean and ISR-Cov. By identifying the subspace spanned by invariant features, ISR-Mean achieves provable domain generalization with $d _ { s } + 1$ training environments. By using information of second-order moments, ISR-Cov reduces the required number of training environments to $\mathcal { O } ( 1 )$ .

# 3.1.2.2 Causal Matching

![](images/7e1cdc89f7da0a42b3fd73d611b3a4f01f5b7efd2555b3f5bb2b49e86413c01c.jpg)  
Figure 3.7: Modified Style and Content Decomposition [73]: When S and $\mathbf { C }$ are correlated, we would like to describe the nature of this association using a causal graph. Causal Matching [73] introduces the environment index variable $E$ and object variable $O$ in order to extend the causal graph (See Sec. 3.1.2.2). The algorithm aims to identify objects from content features $\mathbf { C }$ .

A progression beyond IRM is to analyze in greater detail how $\mathbf { s }$ and $\mathbf { C }$ interact across environments. While IRM assumes that $\mathbf { s }$ and $\mathbf { C }$ are spuriously associated in certain environments, Mahajan et al. [73] consider a data generating process (DGP) where $\mathbf { s }$ and $\mathbf { C }$ are confounded by an object variable $O$ and where our environment $E$ is a causal parent of $\mathbf { s }$ only. In contrast to IRM, the dependence between $\mathbf { s }$ and $\mathbf { C }$ is explicitly modelled with the causal graph Fig. 3.7.

Mahajan et al. [73]‚Äôs contribution is twofold. First, given prior knowledge of object locations in the data encoded in a matching set $\Omega$ , they propose a regularized objective that learns a representation $\Phi ( \mathbf { X } ) : \mathcal { X }  \mathcal { C }$ using $\Omega$ , and a classifier $w _ { \beta } : \Phi  \mathcal { V }$ . Second, without prior knowledge of the object locations, the authors propose a matching algorithm to identify the object locations. Then, the estimated matching set $\tilde { \Omega }$ is deployed in the classifier objective.

Specifically, they propose that for any two data points with the same label but generated in different environments $e \neq e ^ { \prime }$ , $( \pmb { x } ^ { ( e ) } , y ) , ( \pmb { x } ^ { ( e ^ { \prime } ) } , y )$ , their underlying content features $\mathbf { C }$ should be similar. This idea motivates their matching algorithm MatchDG, which alternates between i) building a matching set and ii) learning a representation $\Phi ( \mathbf { X } )$ .

# 3.1.2.3 Style and Content as Latent Variables

As opposed to learning representations for style and content, there are a group of methods that model style and content as latent variables and learn $\{ p ^ { e } ( \pmb { x } , y ) \} _ { e \in \mathcal { E } }$ across environment $e \in { \mathcal { E } }$ . Through this perspective of probabilistic style and content variables $\mathbf { C }$ and $\mathbf { s }$ in the SCD, we change the optimization objective from a constrained one to an unconstrained one.

These methods sample $\mathbf { C }$ and $\mathbf { s }$ from a learned distribution $p ^ { e } ( s , c \mid x )$ , which varies depending on the environment $e \in \mathcal { E }$ . To make a prediction, they sample from density $Y \sim p ( y \mid \pmb { c } )$ , which is invariant to the environmental changes by the Principle of Independent Mechanisms (Def. 2.3.4).

Sun et al. [74] propose LaCIM, which exploits training data from multiple environments and assumes the data is generated according to a similar structure (Fig. 3.7) to the MatchDG method seen in Sec. 3.1.2.2. However, LaCIM solves an unconstrained optimization objective. A prior $p _ { \theta } ^ { e } ( \pmb { c } , \pmb { s } )$ is learned for each environment $e$ to capture the varying dependence between $\mathbf { C }$ and $\mathbf { s }$ .

The prior model $p _ { \theta } \left( \boldsymbol { c } , \boldsymbol { s } \mid \boldsymbol { e } \right)$ , inference model $p _ { \theta } \left( \boldsymbol { c } , \boldsymbol { s } \mid \boldsymbol { x } , \boldsymbol { e } \right)$ , generative model $p _ { \theta } ( \pmb { x } \mid$ c, s) and predictive model $p _ { \theta } ( y \mid c )$ are separately learned for each environment, but crucially, the parameters of $p _ { \boldsymbol { \theta } } ( \boldsymbol { \mathbf { \em x } } \mid \boldsymbol { \mathbf { c } } , s )$ and $p _ { \theta } ( y \mid c )$ are shared across environments. As such, the causal variables $\mathbf { C }$ are identified such that $p _ { \theta } ( y \mid c )$ achieves low loss.

In contrast, Liu et al. [75] exploit training data from a single environment only, but assume C ‚ä•‚ä• S. $\mathbf { C }$ is identified by exploiting the Principle of Independent Mechanisms (Def. 2.3.4). Their approach, called the Causal Semantic Generative model (CSG), operates similarly to LaCIM.

Lu et al. [76] propose to perform the same task as LaCIM but with a three stage training procedure instead of the end-to-end approach of LaCIM and CSG. Their method is called iCaRL. In phase 1, they learn $\mathbf { Z }$ , the set of latent generative variables for $\mathbf { X }$ . In phase 2, they use the PC algorithm [77] and conditional independence testing to isolate $\mathbf { C } \subset \mathbf { Z }$ , the causal parents of $Y$ . In phase 3, a classifier learns $p ( y \mid \pmb { c } )$ . iCaRL outperforms ERM and IRM on the Colored Fashion MNIST [32] OOD classification benchmark.

# 3.1.2.4 Compositional Recognition

Compositional recognition is the problem of learning to recognize new combinations of known components. Note that this method is trained on data with both attribute A and object $O$ labels, as opposed to additional environment labels, which the previous methods in Sec. 3.1.2 exploited. Atzmon et al. [78] argue that deep discriminative models fail at compositional recognition because of two reasons: (i) distribution-shift and (ii) entanglement of representations. Hence, they propose constructing a representation based on a causal graph that models images as being caused by attributes A and objects $O$ , as illustrated in Fig. 3.8. Unlike objects or attributes by themselves, Atzmon et al. [78] argue that combinations of objects and attributes generate

the same distribution over images in train and test sets. Hence, they propose considering images of unseen combinations as generated by interventions on the attribute and object labels, casting zero-shot inference as the problem of finding which intervention caused a given image. They learn the distribution $p ( \pmb { x } \mid \pmb { a } , o )$ , which is assumed to be more stable across environments than either of $p ( \pmb { x } \mid \pmb { a } )$ or $p ( { \pmb x } \mid o )$ . Then, one can learn a classifier using these robust representations.

![](images/44ad5127bb70cdd30c4d05007db4f17bf6d15afb1fda3889118d241e8a16569e.jpg)  
cyan-cube

![](images/9c0f14c64e6b9dc7ba4d3017509fc1da8797197574202c3da3f7982a17f9b535.jpg)  
red-cube

![](images/a00b378106d0998e8877174d7aaa59602ba2d53b7c0d0d39a509c5b38290712f.jpg)  
yellow-sphere   
(a) Example dataset

![](images/dc7c8c0b2e7d06ddc5bae4161014c5e256cda556d7a10a0bf248c2788e4897d5.jpg)  
(b) Causal DAG.   
Figure 3.8: Composition of attributes leads to robust generation [78]: While objects and attributes features are unreliable features for prediction when used by themselves, combinations of such features are assumed to be reliable (Sec. 3.1.2.4). Hence, they learn $p ( \pmb { x } \mid \pmb { a } , o )$ which will be invariant across interventional distributions. Examples of the dataset required are shown in Fig. 3.8a, collected from the AO-CLEVr dataset [79].

# 3.2 Invariant Mechanism Learning

Table 3.2: Overview of IML Methods.   

<table><tr><td>Method</td><td>Key Idea</td><td>Ref.</td></tr><tr><td>Separate Networks</td><td>Networks can be combined in novel domains and account for independent interventions</td><td>Sec. 3.2.1</td></tr><tr><td>Domain Mappings</td><td>Mappings from target domain to source domain which each account for independent interventions</td><td>Sec. 3.2.2</td></tr></table>

In the previous section, we exploited the utility of invariant features to remove spurious associations from our models. Now we look at various methods with fundamentally different objectives compared to invariant feature learning.

Consider how a human may hear someone speaking quietly or loudly but can distinguish between what is said and how loudly it is. The content of the speech corresponds to a feature of the data. The speech volume corresponds to the mechanism that maps the information to the observer. This observation follows the Principle of Independent Mechanisms (Def. 2.3.4).

Invariant mechanism learning (IML) aims to identify a suite of data-generating mechanisms representing different interventional distributions. For independent latent confounders $\mathbf { U }$ of $\mathbf { X }$ and $Y$ , we view each interventional distribution as being generated by interventions on a subset of the confounders (see Fig. 3.9). Then, for

the collection of learned mechanisms $\mathcal { F }$ , we employ a subset of the mappings $\mathcal { F }$ to predict $Y$ from $\mathbf { X }$ . We emphasize that IML does not learn feature maps of the confounders U.

# Def.: 3.2.1: Invariant Mechanism Learning (IML)

The task of IML is to identify a set of mappings $\mathcal { F }$ which capture an interventional distribution generated by independent unobserved confounders $\mathbf { U } = \{ U _ { 1 } , . . , U _ { D } \}$ :

$$
\mathcal {F} = \left\{f _ {i}: \mathcal {X} \rightarrow \mathcal {Y} \mid \quad Y = f _ {i} (\mathbf {X}) \Leftrightarrow Y \sim P \left(Y \mid \mathbf {X}, \operatorname {d o} (u _ {i})\right)\right\} _ {i = 1} ^ {D} \tag {3.7}
$$

![](images/279c3df3b8ece34663a7363bc44ffc960443c5a3c5f3eb2a25af28105099f2ab.jpg)  
Figure 3.9: Graphical motivation for invariant mechanism learning: We consider $\mathbf x \to Y$ to be confounded by a set of independent confounders U, such that an intervention on any $U _ { i }$ generates an interventional distribution.

# 3.2.1 Independent Invariant Mechanisms as Separate Networks

Parascandolo et al. [80] propose a method that aims to identify a suite of competing data transformations (‚Äômechanisms‚Äô) that are trained to specialize among themselves in recovering distinct underlying structure from a sample.

Goyal et al. [81] develop on the previous work by applying independent mechanisms to sequential data, specifically video and text. They call their proposed architecture Recurrent Independent Mechanisms (RIMs). At each time step, a soft attention layer selects the top $k$ out of $N$ competing mechanisms to exploit for processing of the input, and a second attention layer allows for ‚Äôsparse communication‚Äô between the mechanisms at each time step to aid contextual understanding.

The benefits observed over other sequential architectures such as the LSTM (Hochreiter and Schmidhuber [82]) and transformers (Vaswani et al. [83]) are (i) reduced degradation of information over long-dormant phases in the input, and (ii) improved trajectory prediction for multiple objects.

Madan et al. [84] use the RIMs proposed by Goyal et al. [81] for a meta-learning algorithm that quickly adapts the parameters of the modules and slowly adapts the attention mechanism parameters. Their fast and slow learning dynamic demonstrates improvements over LSTM and RIMs in meta-learning benchmarks.

![](images/2abd5d7ccb94350ef8a7e82c66481ab0c8e63b65e91262225ebfc642ba8ba8a7.jpg)  
Figure 3.10: Invariant mappings between domains: Yue et al. [85] propose to learn a set of mappings between different environments generated by the above causal graph (Sec. 3.2.2), where $\mathbf { U }$ is confounding the causal effect $\mathbf { X }  Y$ . Hence, they propose to learn a set of mappings between source and test domain, that each account for an isolated intervention on independent parts of $\mathbf { C }$ .

# 3.2.2 Invariant Mechanisms as Mappings Between Domains

Yue et al. [85] propose to view unsupervised domain adaptation for classification as a problem of inferring a set of disentangled causal mechanisms which generate mappings from the source domain to the test domain.

They model the data generating process (DGP) using the causal graph shown in Fig. 3.10. $E$ represents an environment index and U a set of unobserved domain aware confounding variables on $\mathbf { X }$ and $Y$ . Yue et al. [85] leverage the following result from transportability theory [86]:

$$
p (y \mid \operatorname {d o} (\boldsymbol {x}), e) = \sum_ {\boldsymbol {u}} p (y \mid \boldsymbol {x}, \boldsymbol {u}) p (\boldsymbol {u} \mid e). \tag {3.8}
$$

This result shows that when we have access to the confounders $\mathbf { U }$ , we can identify the effect of an intervention $\operatorname { d o } ( \mathbf { X } )$ in a separate domain $E$ on $Y$ . Given that $\mathbf { U }$ are unobserved, the authors propose to learn a set of proxy variables $\hat { \textbf { U } }$ , and identify disentangled counterfactual mappings between domains that each correspondlearn tervention on in an unsup $U _ { i }$ which keeps all othised fashion, where $U _ { j \neq i }$ Theyand $\left\{ \left( M _ { i } , M _ { i } ^ { - 1 } \right) \right\} _ { i = 1 } ^ { k }$ $M _ { i } : X _ { \mathrm { t r } }  X _ { \mathrm { t e } }$ $M _ { i } ^ { - 1 } : X _ { \mathrm { t e } }  X _ { \mathrm { t r } }$ .

For a given input in the test domain, they propose to map the test domain input to the source domain (i.e., to what the input would have been had it been generated in the source domain) and then predict based on this counterfactual input.

Similarly, Teshima et al. [87] propose to identify an invariant mechanism that generates $\mathbf { X }$ from a set of independent components $\mathbf { Z }$ which vary across domains. The method takes in training data from multiple domains and uses nonlinear ICA to identify features $\mathbf { Z }$ and an invariant mapping $f : \mathcal { Z } \to \mathcal { X }$ across domains. The learned mapping $\hat { f }$ identifies the values of $\mathbf { Z }$ in the target domain and generates pseudo samples which mimic the target domain. A standard supervised learning algorithm can be trained on this generated data.

# 3.3 Open Problems

# 3.3.1 Lack of targeted benchmarks for invariance learning

Previous works in invariance learning often evaluate their suggested methods on novel toy experiments instead of a standardized test bed. As a result, practitioners cannot quickly determine the best approach for their problem. We summarize some existing benchmarks that may serve as inspiration for future works.

One common approach to evaluating causal supervised learning models is introducing spurious associations in the training data and evaluating performance in a test domain where the association has changed. For instance, Arjovsky et al. [32] introduce the ColoredMNIST benchmark, which adds colors to the digits and changes the strength of association between color and digit label between test and training data.

Wang and Jordan [24] varies spurious associations between face attributes of interest and irrelevant attributes of the CelebA dataset [88]. For example, the training data contains a spurious association between black hair and necklaces. Further, they propose to compare the generalization ability under changes to the specified dimension of the representations in their models.

# 3.3.2 Connections between Invariance Learning, Adversarial Robustness and Meta-learning

We note two active ML research areas akin to invariance learning: adversarial robustness and meta-learning. We put forward to investigate how causal assumptions may benefit these areas, as it is currently underexplored.

In adversarial robustness, we are interested in learning classifiers that are robust to adversarial perturbations. An adversarial perturbation is an additive random variable $\Delta$ that makes a model fail to classify $\ddot { \mathbf { X } } = \mathbf { X } + \boldsymbol { \Delta }$ , while the model correctly classifies an image $\mathbf { X }$ . Typically, $\tilde { \mathbf { X } }$ and $\mathbf { X }$ are indistinguishable to the human eye. Naturally, we can see the problem of AR from a causal perspective and interpret a robust model as one being invariant against perturbations, as we will later see in Sec. 8.2.1.2.

In meta-learning, we often seek to learn shared structure across tasks and taskspecific parameters that can be quickly adapted to unseen tasks [89, 90, 91]. Here, we can interpret the shared structure as being invariant across tasks. Future work must examine if causal assumptions can be used to learn better task-invariant representations. For example, instead of conditioning on task variables, it could be interesting to explore interventions on them.

# 3.3.3 Exploiting additional supervision signals for IFL

The previously discussed methods in invariant feature learning (Sec. 3.1) require two distinct forms of extra supervision beyond labels $Y$ : i) content-invariant transformations (Sec. 3.1.1), or ii) environment indices (Sec. 3.1.2). These additional supervisory signals serve as means to separate data among different interventions on the spurious features, which helps a model avoid any predictive dependence on them.

We suggest three directions for future research. First, we can combine these two forms of signals to exploit a combination of domain knowledge of the data generating process and data collected from multiple environments. One straightforward idea is to imagine data augmentations as themselves generating new environments and thus use existing algorithms to handle data from multiple environments, with data augmentation techniques multiplying the number of environments available.

Second, we can attempt to exploit high-dimensional environment information, as opposed to environment indices $\textit { e } \in \textit { \mathcal { E } }$ . For instance, one could complement the dataset collected for the cow images (Fig. 3.1) with an aerial photo of the pictures‚Äô location landscape, which serves as an environment variable that causes the data and label. Kaddour et al. [91] show examples of additional task descriptors, such as one image per robotics environment.

Third, we can demand labels on the data beyond just a category, such as object and attribute labels in Sec. 3.1.2.4. Identifying and exploiting other forms of labels that inform invariant feature learning is an open problem.

# Causal Generative Modeling

The goal of generative modeling is to produce samples that mimic the characteristics of our training data. The field of controllable generation refers to techniques that allow us to enforce a set of attributes that novel samples should satisfy. Non-causal controllable generation samples from a conditional distribution $p ( { \pmb x } \mid { \pmb a } )$ where A is an attribute specification [92, 93]. Such observational distributions restrict us to sample properties as seen in the training data. However, we may want to generate samples with novel combinations of attributes unobserved in our training data or edit specific samples by specifying only the attributes we want to change, with downstream causal effects automatically incorporated.

Causal Generative Modeling (CGM) offers a causal perspective on controllable generation and sample editing by estimating an interventional or counterfactual distribution, respectively. For controllable generation, given a causal representation $\mathbf { Z }$ of our data, we sample from $p ( \pmb { x } \mid \mathrm { d o } ( \pmb { a } ) )$ , where $\mathbf { A } \subset \mathbf { Z }$ is the set of attributes we wish to enforce. If we wish to edit sample $_ { x }$ according to attributes $\textbf { \em a }$ , our target estimand is $p ( \pmb { x } _ { \mathbf { A }  \pmb { a } } \mid \pmb { x } )$ .

One unique use case of CGM is scientific investigation of complex causal mechanisms: Following Pawlowski et al. [94], imagine a medical imaging setup in which we are interested in how a person‚Äôs anatomy would change if particular traits were different. By incorporating expert domain knowledge in the form of a causal graph, counterfactual distributions enable us to model the appearance of brain MRI scans under a counterfactual scenario where the person‚Äôs biological sex is different. By analyzing corresponding counterfactual samples, we gain better insight into understanding the physical manifestation of biological sex on the brain. Fig. 4.1 shows some of the counterfactual samples produced by Pawlowski et al. [94].

Another unique CGM use case is counterfactual data augmentation. Recall the example of training an image classifier in Fig. 3.1. Given training data on cows commonly found in alpine pastures, the classifier fails to generalize to unfamiliar backgrounds, where cows are observed on the beach, for example. As a result, a classifier may learn a spurious association between grass backgrounds and the cow label in such an instance. However, we can eliminate the spurious association from the dataset by augmenting the data with images of cows in varying backgrounds. For example, Sauer and Geiger [95] propose to learn a generative model that disentangles background and foreground attributes. Then, the model can generate images where cow images are intervened upon to have new backgrounds, such as a beach.

![](images/3b9071997f6b2737447bc54744504225e38431e8fde4aa685f3fc2d5408e02dd.jpg)  
Figure 4.1: Brain image counterfactual samples [94]: One way to study the effects of varying demographics on the brain structure is to generate counterfactual samples, as done here by Deep-SCM (Sec. 4.1.1). The intervention variables are a) age, s) sex, b) brain volume, and v) ventricle volume. Top row: Counterfactual samples with interventions specified for each column. Bottom row: Difference maps.

v = 19.89 mlHow can we learn such interventional and counterfactual distributions? Given that it is impossible to learn disentangled representations unsupervisedly from data without extensive domain knowledge [96, 97], we explore methods in this section that vary according to the supervision and domain knowledge they require. On the one hand, we discuss techniques demanding some domain knowledge of the underlying causal graph in structural assignment learning (Sec. 4.1). On the other hand, methods exist s=femalethat relax this requirement, which we refer to as causal disentanglement (Sec. 4.2).

# Notation

Z Generative variables

$K$ $\mathrm { d i m } ( \mathbf { Z } )$

s = femaleA Attributes / intervention variables

$I$ a = 60 yb = 1035mlIndex set of intervention variables s.t. $\mathbf { Z } _ { I } = \mathbf { A }$

$P$ v = 2 $\mathrm { d i m } ( I )$

 iginal do(s = female) do(a = 40 y) do(a = 80 y) doIndependent exogenous causal parents

G Graph adjacency matrix

# 4.1 Structural Assignment Learning

b = 1062 ml=34.87mlWe first highlight some methods that require a practitioner to specify the underlying causal graph $\mathcal { G }$ of the data generating process (DGP) over observed generative variables $\mathbf { Z }$ , but not the full SCM $\mathcal { M }$ . Instead, for $z _ { j } : = f _ { j } ( \mathbf { p a } ( z _ { j } ) , \epsilon _ { j } ) , j = 1 , . . , K$ , these methods may learn the structural assignments $\{ f _ { j } \} _ { j = 1 } ^ { K }$ from the data, where $K = | { \mathcal { G } } |$ . Note that these methods rely on the absence of any hidden confounders (Sec. 2.5.

We train these models to learn the structural assignments of the underlying SCM and identify the exogenous noise values $\{ \epsilon _ { i } \} _ { i = 1 } ^ { K }$ of a given sample. After training, we can generate counterfactuals $p ( z _ { \mathbf { A }  a } \mid z )$ for $\mathbf { A }$ the intervened variables.

Table 4.1: Overview of Structural Assignment Learning methods.   

<table><tr><td>Method</td><td>Key Idea</td><td>Ref.</td></tr><tr><td>DEEPSCM [94]</td><td>Learn each structural assignment independently</td><td>Sec. 4.1.1</td></tr><tr><td>VACA [98]</td><td>Learn structural assignments jointly with a GNN</td><td>Sec. 4.1.2</td></tr><tr><td>DCEVAE [99]</td><td>Group structural assignments in the SCM with latent variable modeling</td><td>Sec. 4.1.3</td></tr><tr><td>DEAR [100]</td><td>Learn all structural assignments in the SCM with latent variable modeling</td><td>Sec. 4.1.4</td></tr><tr><td>DIFF-SCM [101]</td><td>Learn exogenous noise with diffusion model</td><td>Sec. 4.1.5</td></tr><tr><td>CGN [95]</td><td>Learn generating mechanisms of user-specified independent factors of variation</td><td>Sec. 4.1.6</td></tr><tr><td>ADA-GVAE [102]</td><td>Given intervention sample pairs, learn independent latent variables and the generative mechanisms</td><td>Sec. 4.1.7</td></tr></table>

# 4.1.1 Independently learned structural assignments

Pawlowski et al. [94] introduce DeepSCM, a principled approach for estimating interventional and counterfactual distributions given the underlying causal DAG of the data-generating process. They propose to instantiate its SCM by learning the functional assignments $f _ { i }$ for each variable $z _ { i } : = f _ { i } ( \epsilon _ { i } ; \mathbf { p a } ( z _ { i } ) )$ from its parents $\mathbf { p a } ( z _ { i } )$ and mutually independent noise terms $\epsilon _ { i }$ , using normalizing flows [103] and variational inference [104].

To estimate a given sample‚Äôs counterfactuals, we need access to its exogenous noise values. Obtaining access to the noise values observed in a given sample is called the abduction step, as we covered in Def. 2.3.2. To perform this step, Pawlowski et al. [94] propose to learn a mapping for each observed $z _ { i }$ to its respective noise term, $\epsilon _ { i } = f _ { i } ^ { - 1 } ( z _ { i } ; \mathbf { p a } ( z _ { i } ) )$ through normalizing flows in the low dimensional setting, and through variational inference in the high dimensional setting. After learning this mapping, one can perform a counterfactual query by modifying the variables of choice in the causal graph and evaluating a prediction from the SCM with fixed noise values.

Fig. 4.1 highlights some examples of the counterfactual samples produced. We note that this method has been applied with causal graphs of order 5. Scaling up this approach to larger causal graphs is an open research problem.

# 4.1.2 Structural assignments with a GNN

Sanchez-Martin et al. [98] develop on the ideas of DeepSCM (Sec. 4.1.1) and investigate how Graph Neural Networks (GNNs) can be used to solve the same task. Unlike DeepSCM, their approach simultaneously learns all (potentially non-

linear) structural assignments during training. They refer to their approach as VACA(VAriational Causal graph Autoencoder).

They express the causal graph as an adjacency matrix $\mathbf { G }$ embedded in each layer of the GNN. Both the encoder $p _ { \pmb { \theta } } ( \pmb { w } \mid \pmb { z } , \mathbf { G } )$ and decoder $p _ { \pmb { \theta } } ( \pmb { z } \mid \pmb { w } , \mathbf { G } )$ are GNNs that take $\mathbf { G }$ as input. They consider $\mathbf { Z }$ as representing the set of endogenous causal variables and identify each latent variable $W _ { i }$ as capturing all the information about $Z _ { i }$ that cannot be explained by $\mathbf { p a } ( Z _ { i } )$ . Note that $\mathbf { W }$ does not not necessarily correspond to the exogenous variables $\epsilon$ , and $p ( \pmb { w } ) \neq p ( \pmb { \epsilon } )$ .

One limitation of this model is its expressivity w.r.t. the size of the underlying causal graph. VACA captures causal interventions if and only if the number of hidden layers in its decoder is greater than or equal to $\gamma - 1$ , with $\gamma$ being the length of the longest path between any two endogenous nodes in the true causal graph. As GNNs‚Äô performance tends to decrease sharply with depth, VACA will struggle to perform well over large causal graphs.

# 4.1.3 Group structural assignments

Kim et al. [99] develop a generative model, DCEVAE, that produces counterfactual samples by segmenting the causal DAG and learning three different segments (see Fig. 4.2). These segments must depend upon the interventions we want to perform. They propose to cluster the causal graph based on which features undergo interventions, which may carry two benefits: i) it can reduce the problem of error propagation along Markov factorizations that VACA (Sec. 4.1.2) addressed, and ii) it allows for counterfactual sampling over higher-order causal graphs than the previous methods.

This method outputs an image and takes observed concept labels $\mathbf { Z }$ , such as mustache or gender in CelebA data [88], as inputs. Given interventions on $\mathbf A \subset \mathbf Z$ , the new interventional distribution $p ( \pmb { z } ^ { I } \ | \ \mathrm { d o } ( \pmb { a } ) )$ is inferred, and $\mathbf { x } ^ { I }$ is generated from $z ^ { I }$ . The practitioner specifies which concept labels they want to intervene on, $\mathbf { A }$ , and the causal descendants of the intervened variables $\mathbf { Z } _ { d }$ . The remaining attributes are $\mathbf { Z } _ { r } = \mathbf { Z } \backslash \{ \mathbf { A } \cup \mathbf { Z } _ { d } \}$ . The exogenous variables are split too, so $\epsilon _ { d } , \epsilon _ { r }$ are the exogenous causal parents of $\mathbf { Z } _ { d } , \mathbf { Z } _ { r }$ respectively. Given a sample $_ { x }$ , the model produces a counterfactual sample from interventions on $\mathbf { A }$ by identifying the generative variables $_ z$ and the sample exogenous factors $\epsilon _ { d }$ and $\epsilon _ { r }$ , and generating counterfactual $\scriptstyle { \mathbf { { \mathit { x } } } } _ { a }$ .

The learning task is to estimate the encoder that maps the observed causal variables $_ { z }$ to the exogenous variables, $p _ { \pmb { \theta } } ( \epsilon _ { r } , \epsilon _ { d } \ \mid \ \pmb { a } , z _ { d } , z _ { r } , \pmb { x } )$ and the decoder $p _ { \pmb { \theta } } ( \pmb { z } _ { d } , \pmb { z } _ { r } , \pmb { x } , \pmb { \epsilon } _ { d } , \pmb { \epsilon } _ { r } \mid \pmb { a } )$ which generates the counterfactual sample. Due to their disentanglement of $\mathbf { Z } = \mathbf { A } \cup \mathbf { Z } _ { d } \cup \mathbf { Z } _ { d }$ and $\epsilon = \epsilon _ { d } \cup \epsilon _ { r }$ , the encoders and decoders factorize neatly.

Liu et al. [88] applied their method to the CelebA dataset and showed that it improves upon causal agnostic models in counterfactual sampling with downstream effects of interventions accounted for. However, their shown images are often blurry.

![](images/6a2c4b0509c161629270465018acfcbbc3dcb7ab30df58a8d0aabfe27d7765c4.jpg)

![](images/53841be95e4c2ee3f698c994058b5b78bb2797c639884803f07b9ad0e17b2530.jpg)  
(a) Smiling Example: A counterfactual image for (b) Causal DAG: A is one segment, its causal de-$a _ { S m i l i n g } = 0$ should be labeled as $a _ { S m i l i n g } = 1$ , and such scendants another, and the last segment for all other change may cause downstream causal effects on descen- variables. $\pmb { x }$ is generated from latent variables , $\epsilon _ { r }$ $\epsilon _ { d }$ dant attributes of $^ { a }$ , $x _ { d }$ (i.e. Mouth Slightly Open, Nar- and attributes $\pmb { A }$ . The colored arrows indicate latent row Eyes) by maintaining the other attributes intact, $x _ { r }$ . variable modeling, where we learn $p ( \epsilon _ { r } \mid z _ { r } , a , x )$ and Prior works fail to maintain the irrelevant attributes of $^ a$ . $p ( \epsilon _ { d } \mid \pmb { z } _ { d } , \pmb { z } , \pmb { x } )$ .

![](images/55a5e8c8263f49cdd6610695b729eadf218d47e0ca28690f73dad0b00a26d343.jpg)  
Figure 4.2: Disentangled Causal Effect Variational Autoencoder (DCEVAE) [99], see Sec. 4.1.3.   
Figure 4.3: DEAR [100]: the prior $p _ { \beta } ( z )$ encodes the SCM among latents $\mathbf { Z }$

# 4.1.4 GAN-based latent variable Causal

Similarly to DCEVAE, Shen et al. [100] propose a latent variable model for counterfactual sampling called DEAR (Disentangled gEnerative cAusal Representation). In contrast to DCEVAE, they propose to learn all of the structural assignments among the latent variables. They employ a GAN [105], which removes the constraint of a Gaussian prior in VAE-based methods. The practitioner must have access to the causal ordering of the variables of interest, and labels for the causal variables in each sample, as found in the CelebA dataset [88]. Given access to the causal ordering, a supergraph (i.e., a graph containing all subgraphs, akin to a superset) of the adjacency matrix $\mathbf { G }$ is initialized in the prior distribution $p _ { \beta } ( z )$ , from which they identify $\mathbf { G }$ during training.

The authors propose to learn the generative model $p _ { \pmb { \theta } } ( \pmb { x } \mid z ) p _ { \beta } ( z )$ and the encoder $q _ { \phi } ( z \mid x )$ with parameters $\theta , \beta$ and $\phi$ , and a discriminator $D _ { \phi }$ through solving a bi-level optimization objective. For given exogenous factors $\epsilon$ , $_ z$ is generated from the prior $p _ { \beta } ( z )$ by a function $F _ { \beta } ( \epsilon )$ of the form

$$
\boldsymbol {z} = F _ {\beta} (\boldsymbol {\epsilon}) := f \left(\left(\mathbf {I} - \mathbf {G} ^ {\top}\right) ^ {- 1} h (\boldsymbol {\epsilon})\right), \tag {4.1}
$$

where $\beta = \{ f , h , \mathbf { G } \}$ . They specify $f$ to be invertible, enabling a modification of Eq. (4.1) permitting simulation of interventions. Also, they enforce the encoder

![](images/70e9083c01a84c23c9565f288a0f50667b9fc8c2cef7ff8be95090047bfc3836.jpg)  
Figure 4.4: Diffusion-SCMs [101]: It let us generate counterfactuals $x _ { y }$ by intervening on $Y$ and inferring abducted noise $\textbf { \em u }$ . The diffusion process encodes for the exogenous factors $\epsilon$ , and the classifier guidance is used to simulate interventions on $Y$ . See Sec. 4.1.5.

$q ( \pmb { z } \mid \pmb { x } )$ to satisfy alignment between sample labels, which indicate causal variables of interest, and the latent variables encoded.

# 4.1.5 Diffusion-based counterfactual estimation

Diffusion models have recently emerged as a highly effective generative modeling framework for images [106], and the classifier guidance framework proposed by Dhariwal and Nichol [107] permits conditional image generation. Sanchez and Tsaftaris [101] propose Diff-SCM, a framework for counterfactual sampling that employs Denoising Diffusion Models [93, 106, 107]. They use a bi-variable causal model in their implementation, where $Y  \mathbf { X }$ as in Fig. 4.4a.

Following the score based framework of Song et al. [93] and the classifier guidance framework of Dhariwal and Nichol [107], Diff-SCM uses an anti-causal predictor as a means of classifier guidance. The anti-causal predictor steers the generation towards the counterfactual distribution, and the extent to which the guidance is balanced with the score matching objective is managed by a parameter $s$ . Sanchez and Tsaftaris [101] view the forward diffusion as the encoding for the exogenous variables $\epsilon$ (abduction step) and view classifier guidance as a means to simulate an intervention in the generation process.

In their work, they simulate interventions on the class label $Y$ , thus producing counterfactual samples which share some qualitatively high-level similarities yet differ in classification label (see Fig. 4.4b).

# 4.1.6 Disentangled mechanisms among variables of interest

Sauer and Geiger [95] propose Counterfactual Generative Networks (CGN), a generative network motivated by invariant causal mechanisms (see sec. 3.2). The authors employ three parallel BigGAN [108] mechanisms that identify and modify different factors of variation (FoV): object shape, object texture, and background. The mechanisms are trained to specialize in one FoV and are composed through a composition module C, illustrated in Fig. 4.5.

CGN relies upon the independence of the given FoVs, thus assuming a simple causal graph where each FoV causes $\mathbf { X }$ , as shown in Fig. 4.5a. CGN takes noise vector $\epsilon$ and attributes A as input and outputs either an intervention sample (where the noise values $\epsilon$ vary between the mechanisms for given attributes $\textbf { \em u }$ ) or a counterfactual sample (where the noise values and attributes are shared for each mechanism).

Figure 4.5: Counterfactual Generative Networks [95]: The factors of variation (FoVs) are object shape, object background and object texture (see Sec. 4.1.6).   
![](images/5ca27c5e7ae9ada275ddda0eb07884160616d5dcbda7819cf5f12bf8db548787.jpg)  
(a) Causal DAG with mask M, (b) ImageNet Counterfactuals. CGN learns the disentangled foreground F, and background B. FoVs and enables the generation of permutations thereof.

<table><tr><td rowspan="3">Shape
Texture
Background</td><td>red wine</td><td>cottontail rabbit</td><td>pirate ship</td><td>triumphal arch</td><td>mushroom</td></tr><tr><td>carbonara</td><td>head cabbage</td><td>banana</td><td>Indian elephant</td><td>barrel</td></tr><tr><td>baseball</td><td>valley</td><td>bittern (bird)</td><td>viaduct</td><td>grey whale</td></tr></table>

# 4.1.7 Disentangled mechanisms using intervention sample pairs

Similar to the goal of CGN, Locatello et al. [102] propose Ada-GVAE, a generative model that identifies independent representations. This method demands two data conditions: (i) the data is a collection of paired samples before and after an unspecified intervention, and (ii) we know the number of intervened latent generative variables. Specifically, we have a collection of tuples $\{ ( \pmb { x } _ { i } , \pmb { x } _ { i } ^ { I } , P ) \} _ { i = 1 } ^ { n }$ where $P$ is the number of intervened generative variables, and $\mathbf { x } ^ { I }$ denotes a sample after an intervention. In contrast to CGN, they do not require specification of the latent variables of interest, such as object shape, but instead sample pairs and latent dimension specification.

For latent generative variables $\mathbf { Z }$ , Locatello et al. [102] assume that they are independent, and admit a prior factorization $\begin{array} { r } { p ( z ) = \prod _ { i = 1 } ^ { K } p \left( z _ { i } \right) } \end{array}$ (thus a similar causal graph to that assumed in Fig. 4.5a). For $I \subset [ K ]$ the index set of intervened generative variables, $| I | = P$ , and $\mathbf { Z } _ { I } : = \mathbf { A }$ the intervened generative variables, we have

$$
p (\boldsymbol {x}, \boldsymbol {x} ^ {I}, \boldsymbol {z}, \boldsymbol {z} _ {I}, I) = p (\boldsymbol {x} \mid \boldsymbol {z}) p (\boldsymbol {x} ^ {I} \mid \boldsymbol {z} _ {\bar {I}}, \boldsymbol {z} _ {I}) p (\boldsymbol {z}) p (\boldsymbol {z} _ {I}) p (I) \tag {4.2}
$$

To identify the intervened variables $\mathbf { Z } _ { I }$ , the unintervened variables $\mathbf { Z } _ { \bar { I } }$ are chosen to be the $K - P$ variables that induce the smallest values of $D _ { \mathrm { K L } } ( p _ { \pmb \theta } ( z _ { i } \mid \pmb x ) \| p _ { \pmb \theta } ( z _ { i } \mid \pmb x ^ { I } ) )$ . After identification, the posterior distributions for unintervened variables are set to equality between the sample pairs, while the posteriors for the intervened variables are left untouched. A maximum likelihood objective is then optimized to learn the generative model.

# 4.2 Causal Disentanglement

Next, we study methods that do not require the specification of any underlying causal graph. Instead, they identify both the underlying graph and the structural

Table 4.2: Overview of Causal Disentanglement methods, differing by their data requirements.   

<table><tr><td>Method</td><td>Data Requirement</td><td>Ref.</td></tr><tr><td>CAUSALVAE [109]</td><td>Causal variable labels</td><td>Sec. 4.2.1</td></tr><tr><td>ILCM [110]</td><td>Intervention sample pairs</td><td>Sec. 4.2.2</td></tr><tr><td>CITRIS [111]</td><td>Sequential data with intervention target labels</td><td>Sec. 4.2.3</td></tr></table>

assignments between the variables, thus learning a set of causally disentangled representations [11, 14].

# Def.: 4.2.1: Causal Disentanglement [11]

We say a set of representations $\mathbf { Z }$ , s.t. $\mathbf { X } = g ( \mathbf { Z } )$ for some mapping $g$ , are causally disentangled if they permit the factorization

$$
p \left(z _ {1},.., z _ {K}\right) = \prod_ {i = 1} ^ {K} p \left(z _ {i} \mid \mathbf {p a} \left(z _ {i}\right)\right). \tag {4.3}
$$

where $\mathbf { p a } ( Z _ { i } ) \subset \{ Z _ { j } \} _ { j \neq i } \cup \epsilon _ { i }$ and $\epsilon _ { i }$ is the exogenous causal factor of $Z _ { i }$ .

These methods do not require access to the complete causal graph $\mathcal { G }$ , instead requiring practitioner knowledge about the generative variables $\mathbf { Z }$ of interest. They learn how to reproduce observational distribution $p ( { \pmb x } )$ , interventional distribution $p ( \pmb { x } \mid \mathrm { d o } ( \pmb { a } ) )$ , and counterfactual distributions $p ( \pmb { x } \mathbf { A } {  } \pmb { a } \mid \pmb { x } )$ .

# 4.2.1 Exploit causal variable labels

Yang et al. [109] propose CausalVAE, a method that learns a causal model over latent variables from data and generates counterfactual samples. The dataset must contain labels on the latent causal variables of interest for each sample, following the identifiability framework outlined in [97]. These labels represent the generative variables $\mathbf { Z }$ .

Like VACA (Sec. 4.1.2), CausalVAE expresses the SCM as an adjacency matrix $\mathbf { G }$ . However unlike VACA, CausalVAE learns $\mathbf { G }$ as well as linear structural assignments as in Eq. (4.4),

$$
\boldsymbol {z} = \mathbf {G} ^ {\top} \boldsymbol {z} + \epsilon , \tag {4.4}
$$

while VACA permits nonlinear structural assignments too.

We generate a counterfactual sample $x _ { a }$ according to attributes $\textbf { \em u }$ as follows: for a given sample $_ { x }$ , the encoder identifies the exogenous noise $\epsilon$ . The exogenous noise determines the causal latent variables $_ { z }$ via the SCM, Eq. (4.4), to obtain $z =$ $\left( \mathbf { I } - \mathbf { G } ^ { \top } \right) ^ { - 1 } \epsilon$ . Now that we have an encoding for $_ z$ , we simulate an intervention on attributes $\textbf { \em u }$ . To obtain counterfactual ${ z } _ { a }$ , $_ z$ is input into Eq. (4.4) except with

![](images/de2eaa97d8aad0437f969dd2362055c8ca5b6b42c34d6c47ff3df51f293689d8.jpg)  
Figure 4.6: Simulated interventions by ILCM [110] compared to ground truth: ILCM predicts the effects of a set of interventions $I$ on a set of causal variables $\{ c _ { i } \} _ { i \in I }$ , using the Causal3DIdent dataset [41]. Given a sample $\mathbf { X }$ , the model generates a simulation of the sample under intervention, as seen in the bottom row of the figure. See Sec. 4.2.2 for explanation.

modifications on $\mathbf { G }$ that reflect the interventions $\textbf { \em a }$ . Then counterfactual ${ z } _ { a }$ is input into the decoder to generate a counterfactual sample $x _ { a }$ .

This method was applied to the CelebA dataset [88] where they constructed an SCM over four latent variables.

# 4.2.2 Exploit intervention sample pairs

Brehmer et al. [110] improve upon Ada-GVAE by i) performing causal graph discovery and ii) removing the requirement that a practitioner has access to the number of intervened variables between sample pairs, proposing to learn a generative model over a collection of tuples $\{ ( { \pmb x } _ { i } , { \pmb x } ^ { I } ) \} _ { i = 1 } ^ { n }$ . In this setting, they propose two types of models: Explicit or Implicit Latent Causal Models. For the former, ELCM, they introduce a prior over latent variables $p ( z )$ that encodes the structure of a specified causal graph. For the latter, which they call ILCM, they propose to learn a noise encoder $p _ { \pmb { \theta } } ( \epsilon , \epsilon ^ { I } \mid \pmb { x } , \pmb { x } ^ { I } )$ that maps the data $( \mathbf { X } , \mathbf { X } ^ { I } )$ to the exogenous noise values $( \epsilon , \epsilon ^ { I } )$ in the latent causal model, before and after interventions. Note that ILCM relies upon the interventions $I$ only changing one noise value $\epsilon _ { i }$ , between $\epsilon$ and $\epsilon ^ { I }$ . A prior $p ( \epsilon , \epsilon ^ { I } , I )$ is specified, which assumes independence of exogenous causal parents, $\epsilon _ { i } \perp \perp \epsilon _ { j }$ for all $i , j \in [ K ]$ . No practitioner specifications of the underlying causal graph, the causal variables of interest, or the structural mechanisms are required for ILCM. What is required is the specification of the dimension of $\mathbf { Z }$ , $K$ , which determines the size of the causal graph.

ILCM implicitly learns the generative variables $\mathbf { Z }$ since the exogenous noise values determine the variables in an SCM. To obtain the generative variables in explicit form, they learn solution functions $s ( . , . )$ s.t. $z _ { i } = s ( \epsilon _ { i } , \epsilon _ { - i } )$ . A causal graph is learned by applying intervention-based causal discovery algorithms over the generative variables $\mathbf { Z }$ .

ILCM learns by maximizing an ELBO approximation to the maximum likelihood, and a noise decoder $p ( { \pmb x } \mid { \pmb \epsilon } )$ generates a sample from the inferred exogenous variables. Brehmer et al. [110] reproduce interventional distributions $\mathbf { X } ^ { I } \sim p ( \pmb { x } \mid \mathrm { d o } ( \pmb { a } ) )$ as demonstrated in Fig. 4.6.

# 4.2.3 Exploit sequential data with intervention labels

While ILCM exploits independence of exogenous noise values $\epsilon _ { i }$ for causal graph identification, Lippe et al. [111] propose to exploit independence of the time indexed causal variables, conditioned on the previous time step and assuming no instanta-\ neous effects. Their method, called Causal Identifiability from TempoRal Intervened \[  Sequences CITRIS, exploits data from a sequence $\{ ( \pmb { x } ^ { t } , I ^ { t } ) \} _ { t = 0 } ^ { T }$ , where each inter- vention target $I ^ { t } \in \{ 0 , 1 \} ^ { K }$ labels the causal variable intervened upon to generate sample $\mathbf { \boldsymbol { x } } ^ { t }$ . Given such data, CITRIS learns causal latent variables $\mathbf { Z }$ and a transition prior $p _ { \beta } ( z ^ { t + 1 } \mid z ^ { t } , I ^ { t + 1 } )$ , which encodes the governing SCM and is trained using an ELBO objective.

Figure 4.7: CITRIS [111]: The transition prior $p ( z ^ { t + 1 } \mid z ^ { t } , I ^ { t + 1 } )$ captures the temporal causal relations from Zt to Zt+1. $\mathbf { Z } ^ { t }$ $\mathbf { Z } ^ { t + 1 }$   
![](images/83ee9f7ba87126b0c9b8724cb7b0d3ca194a432a939a334ae7bb270b1e9cf1c6.jpg)  
(a) Causal DAG with latent (b) Architecture: the transition prior encodes the SCM between variables $Z _ { i }$ , exogenous factors $\epsilon ^ { t }$ , latents $\mathbf { Z } ^ { t }$ and $\mathbf { Z } ^ { t + 1 }$ . intervention targets $\boldsymbol { I } _ { i } ^ { t }$ and observations $\mathbf { X } ^ { t }$ .

CITRIS also accommodates multi-dimensional causal variables, i.e., it groups latent variables of the same causal factor, such that the transition prior factorizes based on these multi-dimensional variables. For example, this property helps to learn one position variable to capture 3D coordinates instead of three independent axis variables because the ground truth axes are not always well-defined. To learn multi-dimensional causal variables, the authors encourage practitioners to specify the dimension of the latent space $\mathcal { Z }$ to be larger than the number $K$ of causal variables.

CITRIS learns an assignment function $\psi : [ \dim ( \mathbf { Z } ) ] \to [ K ]$ which factorizes the latent space into causal variables and exogenous noise, such that $\mathbf { Z } = \{ Z _ { \psi _ { 1 } } , . . , Z _ { \psi _ { K } } , \epsilon \}$ . Hence, the transition prior factorizes as

$$
p _ {\beta} \left(\boldsymbol {z} ^ {t + 1} \mid \boldsymbol {z} ^ {t}, I ^ {t + 1}\right) = p _ {\beta} \left(\boldsymbol {\epsilon} ^ {t + 1} \mid \boldsymbol {z} ^ {t}, I _ {i} ^ {t + 1}\right) \cdot \prod_ {i = 1} ^ {K} p _ {\beta} \left(z _ {\psi_ {i}} ^ {t + 1} \mid \boldsymbol {z} ^ {t}, I _ {i} ^ {t + 1}\right). \tag {4.5}
$$

We obtain the parents of any latent variable $Z _ { \psi _ { i } }$ , and ultimately the causal graph, by identifying which of the latents $\{ Z _ { \psi _ { 1 } } ^ { t } , . . , Z _ { \psi _ { K } } ^ { t } \}$ influences $Z _ { \psi _ { i } } ^ { t + 1 }$ in the transition prior.

Further, CITRIS learns an invertible mapping $g _ { \pmb \theta } : \mathcal { X }  \mathcal { Z }$ between the observational data $\mathbf { X }$ and the latents $\mathbf { Z }$ . CITRIS is also capable of using representations from a pre-trained encoder $g _ { \boldsymbol { \theta } }$ , by learning a normalizing flow model which maps

the representations to disentangled representations $\mathbf { Z }$ which the transition prior can admit.

Since CITRIS assumes no instantaneous effects in the data, it fails to discover the underlying causal graph when instantaneous effects are present. For instance, [112] highlight the example where both (i) flicking a switch, and (ii) its effect on a light bulb, can occur within one time step of a sequence. In this case, both the variable light bulb and switch are not conditionally independent given the previous time step information.

Lippe et al. [112] propose to address this shortcoming with iCITRIS (instantaneous CITRIS), employing a similar framework to CITRIS, and exploiting the same type of data. In contrast to CITRIS, which accommodated soft interventions, the authors assume that the interventions are perfect, rendering an intervened variable independent of its parents. The transition prior is augmented with instantaneous causal graph structure information, and the authors propose to learn the graph and generative model parameters by solving a bi-level optimization objective.

# 4.3 Open Problems

# 4.3.1 Different levels of abstraction

Causal representation learning (CRL, Def. 2.4.1) aims to find an SCM among latent generative factors $\mathbf { Z }$ of our data $\mathbf { X }$ , where the generative factors are assumed to be causal variables. This problem is ill-posed, however, unless we specify the correct level of abstraction in the SCM. For instance, we may choose to construct a causal model between each pixel in an image ( $\dim ( \mathbf { Z } ) = \dim ( \mathbf { X } ) ,$ ) or between three types of possible objects that may be observed in an image ( $3 = \dim ( \mathbf { Z } ) \ll \dim ( \mathbf { X } )$ ). Both models may deliver equal performance (given enough data and compute), yet the latter is more efficient and interpretable for a human.

It is not obvious how practitioners should determine a reasonable level of abstraction for causal representation learning, much less do so automatically [113, 114, 115, 116]. All of the approaches in Sec. 4.2 required specifications that determined the abstraction of the SCM, for example. CausalVAE (Sec. 4.2.1) required variable labels in the data, while ILCM (Sec. 4.2.2) specified dim(Z) and required samples pairs before and after interventions. Can we identify other sensible specifications or even relax them? For instance, ILCM relaxed the specifications of Ada-GVAE (Sec. 4.1.7).

Important theoretical results determine the limits and the opportunities of how much we can automate causal representation learning. Locatello et al. [96] demonstrate that unsupervised disentangled representation learning is ill-posed without the proper model inductive biases and some extra supervisory information. Khemakhem et al. [97] prove that it is possible to identify the underlying joint distribution between observed and latent variables, as long as a practitioner specifies a factorized prior of the latent variables and provides additional supervisory signals. Gresele

et al. [117] derive identifiability results for identifying independent generative variables when considered from multiple environments, even under nonlinear mixing in the generative process. Finally, Cohen [118] identifies that disentangled representations typically permit impossible combinations of causal variables and proposes a framework for interventions, incorporating relations between causal variables that govern what is physically possible.

Thus, what kinds of supervisory signals are optimal for CRL, what trade-offs exist, and which signals are reasonable to require, are all open problems.

# 4.3.2 Scaling structural assignment learning to larger graphs

The structural assignment learning methods reviewed in Sec. 4.1 generate compelling counterfactual samples. However, they either operate on relatively low-order causal graphs or amortize the structural assignments and focus on a set of plausible interventions.

In applied sciences, we often want to model higher-order causal graphs and flexibly generate counterfactual samples from them to capture real-life dynamics, such as in cell biology [119, 120]. Yet, the approaches reviewed in Sec. 4.1 do not demonstrate results over causal graphs of order larger than 5, except for DCEVAE (Sec. 4.1.3) which amortizes the graph assignments. VACA (Sec. 4.1.2) relies on GNNs, which struggle to scale to large-scale graphs due to over-smoothing [121]. Hence, how to tackle the scalability of the underlying causal graphs in structural assignment learning remains an open problem.

# 4.3.3 Understanding counterfactual data augmentation

A growing trend has been counterfactual data augmentation (CFDA), as we saw in Sec. 3.1.1.1 and will see later in Secs. 7.8, 8.1.3.1, 8.2.3 and 8.3.2. Here, we focus on CFDA using causal generative models instead of hand-crafted transformations. While many augmentations can be hand-crafted, such as color transformations and rotations over images [122], we can also use generative models to create data augmentations. For instance, GenInt (Sec. 3.1.1.1) use a GAN [108] to create data augmentations.

However, while GenInt uses a causal agnostic generative model, it is possible to use causal generative modeling to generate CFDAs. We explored the causal perspective on data augmentation in Sec. 3.1.1, where we viewed an augmentation as an intervention on a spurious data feature. Sauer and Geiger [95] Sec. 4.1.6 compare the efficacy of training a classifier on datasets augmented with 1) the counterfactual samples and 2) non-counterfactual samples generated by a GAN and find that counterfactual samples improved the classifier‚Äôs performance more.

Exploring when and by how much CFDA using causal generative models improves generalization remains an open problem. For hand-crafted data augmentations, some understanding of their efficacy already exists. For example, Hern√°ndez-Garc√≠a and

K√∂nig [123] conclude that it is more effective than explicit regularization, such as weight decay or dropout. Chen et al. [124] show that conventional data augmentation flattens the loss surface and that it can achieve similar performance gains as flat-minima optimizer [125]. Cubuk et al. [126, 127] offer automated augmentation generators. For CFDA, works and results like those mentioned above are missing.

Further, we may use CFDA as an evaluation proxy for comparing causal generative methods by their capability of improving an external predictor‚Äôs generalization performance. For a fixed prediction model and training dataset, methods in this chapter can be compared on the prediction algorithm test loss when one trains the model using data augmentations generated by the counterfactual sampling method. Such a benchmarking method is under-explored.

# Causal Explanations

The goal of AI explainability (or explainable AI ) is to output explanations that make the decisions that a model constructs understandable to humans and provide an answer to why the output was predicted [128]. From the point of view of model interpretability [5], which refers to the degree to which an observer can understand the cause of a model prediction, an explanation falls under the category of post-hoc interpretability: the model (prediction) is analyzed after training, while intrinsic interpretability may refer to models that are restricted in their complexity and therefore sacrifice predictive performance [129]. Many explanation methods have been proposed, and we refer the reader to [130, 131, 132, 133] for excellent surveys.

In this section, we focus on two classes of explanation techniques: feature attribution and contrastive explanations. We will explain how causality enters the picture in the corresponding subsections. All discussed methods provide local explanations of individual predictions based on a single input, and are model-agnostic, i.e., they can be used across different model types. In contrast, there also exist methods providing global explanations, which describe the model‚Äôs average reliance on each feature across the whole dataset, and model-specific methods, such as saliency maps for neural networks trained on images [134, 135]. These latter two are not covered in this survey.

To motivate the usefulness of explanations, let us consider the case of someone who applied for a loan and was rejected by a financial institution‚Äôs loan distribution model. In most cases, the individual wants to understand the model‚Äôs reasoning in a bid to strengthen their next application. Fixing this scenario as a running example, we will introduce multiple techniques to produce explanations in the upcoming subsections.

# Notation

xF $\pmb { x } ^ { \mathrm { F } }$ Factual individual with undesired outcome   
xCE $\mathbf { \boldsymbol { x } } ^ { \mathrm { C E } }$ Counterfactual explanation individual with desired outcome   
xSCF $\pmb { x } ^ { \mathrm { S C F } }$ Structural counterfactual individual with desired outcome (w.r.t. causal graph)

Table 5.1: Method Overview of Causal Explanations.   

<table><tr><td>Class</td><td>Method</td><td>Key Idea</td><td>Ref.</td></tr><tr><td rowspan="4">Feature Attributions</td><td>CXPLAIN</td><td>Quantify causal influences of input features to model accuracy</td><td>Sec. 5.1.1</td></tr><tr><td>GCE</td><td>Learn latent factors which cause change in the output</td><td>Sec. 5.1.2</td></tr><tr><td>ASVs</td><td>Relaxes asymmetry axiom of Shapley values w.r.t. given causal knowledge</td><td>Sec. 5.1.3</td></tr><tr><td>CSVs</td><td>Intervenes upon features instead of conditioning on them</td><td>Sec. 5.1.4</td></tr><tr><td rowspan="2">Contrastive Explanations</td><td>Counterfactual Explanations</td><td>xCE satisfies minimal distance to xF</td><td>Sec. 5.2.1</td></tr><tr><td>Algorithmic Recourse via Minimal Interventions</td><td>xSCF satisfies minimal cost set of interventions on xF</td><td>Sec. 5.2.2</td></tr></table>

# 5.1 Feature Attribution Explanations

Attribution-based explanations assign a ranking to features, representing the marginal contribution of each feature to the output of the model. In our running example, such explanations may show that for the individual whose loan application was denied, the most important feature was their overall income, while their credit card debt did not contribute to the model‚Äôs prediction. While this explanation does not provide an instruction of what the individual should do to get the loan applied, it informs the applicant about which features of their application may have caused the rejection to which degree.

Regarding our running example, we illustrate why a loan applicant would prefer causal feature attribution explanations over associational ones, as follows. Imagine that the financial institution primarily cares about the applicant‚Äôs yearly income: the higher the income, the higher the chances of providing a loan. Other applicant features may be spuriously correlated with the income: e.g., their education level, job, industry, whether they are self-employed, etc.

A traditional, non-causal attribution method may assign uniformly high scores across all these correlated features. However, if the applicant knew that only their income matters, they likely would focus on improving it more directly: instead of even considering a career change, working directly on getting a promotion at their current job might be more effective. Hence, in this scenario, a causal attribution method would help the applicant to achieve their desired outcome more efficiently.

# 5.1.1 CXPlain

Schwab and Karlen [136] transform the task of producing feature importance estimates for an existing predictive model into one of supervised learning. They train separate supervised causal explanation (CXPlain) models to explain the predictive model. To train the explanation model, they use a causal influence (Sec. 2.7) function that quantifies both the attribution of each input feature and groups of input features to the accuracy of the prediction model.

The causal feature attribution $a _ { i } \in \mathbb { R }$ measures to what extent the $i$ th input feature causally contributed to the predictive model‚Äôs output $\hat { Y }$ , as the decrease in error, measured by a standard classification loss $\mathcal { L }$ . In other words, $a _ { i }$ denotes the causal influence of adding that feature to the set of input features. Pre-computing the feature attributionsfor $N$ training samples $\{ { \pmb x } _ { i } \} _ { i = 1 } ^ { N }$ takes $N ( D + 1 )$ evaluations of the target predictive model at training time, where $D$ is the number of input features.

They also provide confidence interval estimates of the level of uncertainty associated with each feature importance estimate $\hat { a } _ { i }$ produced by a CXPlain model. By using bootstrap ensemble methods, $M$ explanation models are trained to estimate uncertainty. Each model is trained on $N$ training samples from $\{ { \pmb x } _ { i } \} _ { i = 1 } ^ { N }$ , sampled randomly with replacement from the original training set.

# 5.1.2 Generative Causal Explanations

O‚ÄôShaughnessy et al. [137] introduce the generative causal explanations (GCE) framework for post-hoc explanations based on learned disentangled latent factors that produce a change in the classifier output distribution. For example, consider a black-box color classifier, for whose color classifications one wants to construct explanations. In this setup, one may learn a latent encoder (which they call the local explainer) that learns a low-dimensional representation $( \alpha , \beta )$ describing the color and shape of inputs. Changing $\alpha$ (color) changes the output of the classifier, which detects the color of the data sample, while changing $\beta$ (shape) does not affect the classifier output. Fig. 5.1 shows a pictorial depiction of this architecture.

To construct causal explanations, the authors propose two components: (i) a method for representing and moving within the distribution of the data and (ii) a metric that quantifies the causal influence (Sec. 2.7) of various aspects of the data on the classifier outputs. To ensure that the learned disentangled representations represent the data distribution, while encouraging a small subset of latent factors to exert a large causal impact on the classifier output, they formulate a corresponding optimization objective.

# 5.1.3 Asymmetric Shapley Values

Shapley values provide a principled, model-agnostic way to explain model predictions. They are able to capture all the interactions between features that lead to a prediction by relying on coalitional game theory: the idea is to fairly distribute the

![](images/828cbb940a52c8da7af0e663c30141a505757fcc4faa0108f701aac4b3d32579.jpg)  
Figure 5.1: Generative Causal Explanations [137]: Changing learned latent factors produces a change in the classifier output statistics. To quantify the attribution of the latent factors, O‚ÄôShaughnessy et al. [137] use causal influence measures of variables in a SCM (Sec. 2.7).

‚Äúpayout‚Äù among the features, which naturally quantifies which features contribute to a prediction [129, 138]. They are principled because they uniquely satisfy four intuitive mathematical axioms.

However, Frye et al. [139] argue that Shapley values suffer from a significant limitation: they ignore all causal structure in the data. One of the axioms is Symmetry: it places all features on equal footing in the model explanation, by requiring attributions to be equally distributed over features that are identically informative (i.e. redundant). Frye et al. [139] argue that when redundancies exist, we might instead seek a sparser explanation by relaxing this axiom.

For example, two features might be bijectively related to one another if one is known to be the deterministic causal ancestor of the other. In this case, it makes sense to attribute all the importance to the ancestor and none to the descendant, in opposition to the Symmetry axiom. Otherwise, the explanations may obfuscate known causal relationships in the data.

To relax the Symmetry axiom, the authors propose Asymmetric Shapley values (ASVs), which uniquely satisfy the other axioms and reduce to Shapley values if the distribution over the ordering in which features are fed to the model is uniform. This relaxation allows the practitioner to place a non-uniform distribution over the ordering that incorporates causal understanding into the explanation. For example, one may place nonzero weight only on permutations in which an ancestor precedes its (known) descendants. In other words, only feature permutations that are consistent with the causal structure between the features have non-zero probability. This approach favors explanations in terms of distal (i.e. root) causes, rather than explanations towards proximate (i.e. immediate) causes.

Frye et al. [139] discuss that ASVs span a continuum between maximally-data agnostic Shapley values, and causality-based explanation methods that often require the exact causal process underlying data. Hence, ASVs allow any knowledge about the data-generating process, however incomplete, to be incorporated into an explanation of its model, without the often-prohibitive requirement of full causal inference. For

example, if causal knowledge is limited, even a single known causal ancestor can be ordered first, with permutations over remaining features uniformly weighted.

# Def.: 5.1.1: Distal distribution (ASVs) [139]

Assume that there are $D$ input features, where $\Pi$ denotes the set of all permutations of them, and $\pi ( j ) < \pi ( i )$ means that feature $j$ precedes feature $i$ under ordering $\pi$ . Let $\Delta ( \Pi )$ be the set of probability measures on $\Pi$ , so that each $w \in \Delta ( \Pi )$ is a map $w : \Pi  \lfloor 0 , 1 \rfloor$ satisfying $\begin{array} { r } { \sum _ { \pi \in \Pi } w ( \pi ) = 1 } \end{array}$ .

Asymmetric Shapley values replace the uniform distribution $w \in \Delta ( \Pi )$ of Shapley values [138] with:

$$
w _ {\text {d i s t a l}} (\pi) \propto \left\{ \begin{array}{l l} 1 & \text {i f} \pi (i) <   \pi (j) \text {f o r a n y k n o w n} \\ & \text {a n c e s t o r} i \text {o f d e s c e n d a n t} j \\ 0 & \text {o t h e r w i s e} \end{array} . \right. \tag {5.1}
$$

As a consequence of using Def. 5.1.1, the ASVs of known causal ancestors indicate the effect these features have on predictions while their descendants remain unspecified. The ASVs of the descendants then represent their incremental effect upon specification.

# 5.1.4 Causal Shapley Values

Janzing et al. [140] argue that there is a misconception in previous work like Lundberg and Lee [138] on Shapley values because these methods use observational conditional distributions rather than interventional ones. While these proposals are conceptually flawed, in practice, their software implementation still works because of its approximate nature.

Heskes et al. [141] remedy this conceptual flaw by proposing causal Shapley values (CSVs) that explain the total effect of features on the prediction, taking into account their causal relationships. To incorporate causal knowledge, they replace the conventional conditioning by observation from conditional Shapley values with conditioning by intervention.

In contrast to ASVs, they point out that there is ‚Äúno need to resort to asymmetric Shapley values to incorporate causal knowledge‚Äù. Relaxing the Symmetry axiom is orthogonal to their approach. One can additionally get asymmetric causal Shapley values that implement both ideas. As for ASVs, a practitioner needs to provide only a partial causal order and a way to interpret dependencies between features that are on an equal footing.

One benefit of CSVs is that they permit a decomposition of the total effect that a feature has on a model‚Äôs prediction into direct and indirect effects. The direct effect measures the expected change in prediction when the stochastic feature $X _ { i }$ is fixed to $x _ { i }$ , without altering the other ‚Äúout-of-coalition‚Äù features. The indirect effect

measures the difference in expectation when the distribution of the other ‚Äúout-ofcoalition‚Äù features changes due to the additional intervention $\mathrm { d o } \left( X _ { i } = x _ { i } \right.$ ).

Chen et al. [142] argue that neither observational (‚Äútrue to the data‚Äù) nor interventional (‚Äútrue to the model‚Äù) conditional probabilities are preferable in general, but that the choice is application dependent. They present two real data examples from the domains of credit risk modeling and biological discovery to show how a different choice of value function performs better in each scenario, and how possible attributions are impacted by the choice of probability distribution.

Similar to CSVs, Jung et al. [143] introduce do-Shapley values, which do not have the restriction that the model is accessible, which means that it can be evaluated for arbitrary input features. Instead, their method is compatible with inaccessible models, and uses semi-Markovian causal graphs (DAGs with bidirected edges).

# 5.2 Contrastive Explanations

Sociological studies have shown that human explanations are typically contrastive: they emphasize the causal factors that explain why an event occurred instead of another event [144]. By pruning the space of all causal factors, such explanations facilitate easier communication and reduce the cognitive load for both explainer and explainee [145].

Contrastive explanations are typically counterfactual in the sense that they estimate an altered version of an observation that would have changed the model‚Äôs prediction, given our knowledge of the model‚Äôs prediction from the original datapoint. The basic implementation of this idea, which we will henceforth refer to as counterfactual explanation, is conceptually simple and does not require much causal machinery. Furthermore, we will look at algorithmic recourse (AC): instead of providing an understanding of the least distant point in feature space that results in the desired prediction, AC aims at producing the minimal cost set of actions for the individual, taking the dependencies among observed variables into account in the form of causal knowledge.

# 5.2.1 Counterfactual Explanations

Counterfactual explanations (CE) explain a prediction by computing a (usually minimal) change of an individual‚Äôs features that would cause the underlying model to classify it in a desired class [146]. By showing feature-perturbed versions of the same person who would have received the loan, counterfactual explanations can provide actionable information about what to do in the future to secure a better outcome. CE are counterfactual in that they consider the alteration of an entity in the history of the event $P$ , where $P$ is the undesired model output [147].

For example, a feature instantiation that would have changed the prediction in the above example would be ‚Äúyou would have received the loan if your income was higher by $\$ 9$ 10k‚Äù. In contrast to explanation methods that rely on approximating the

![](images/fda06bfe6cbbd440c4674080dcb6eb3ec22d69c821b04caf72163eafd14ea010.jpg)  
(a) Counterfactual Explanations [149]: $\pmb { x } ^ { \mathrm { C F } }$ is the nearest feature instantiation that lies on the other side of the decision boundary.

![](images/699921587f751be561738aa33fe890697083d8a9f6014ff39d6662810cacd0aa.jpg)  
(b) Causal Algorithmic Recourse [150]: The counterfactual instance corresponds to the minimal cost set of interventions; here, one intervention on $x _ { 4 }$ .   
Figure 5.2: Counterfactual explanations vs. Causal Algorithmic Recourse, illustrated by an individual who got their loan application denied, represented by features $\pmb { x } ^ { \mathrm { F } } = \{ x _ { 1 } ^ { \mathrm { F } } , \ldots , x _ { 5 } ^ { \mathrm { F } } \}$ .

classifier‚Äôs decision boundary [148], counterfactual explanations are always truthful w.r.t. the underlying model by using actual predictions of the algorithm.

# Def.: 5.2.1: Counterfactual Explanation [149]

A counterfactual explanation $\mathbf { x } ^ { \mathrm { C E } }$ (or nearest contrastive explanation) for an individual $\mathbf { x } ^ { \mathrm { F } }$ is given by a solution to the following optimization problem:

$$
\mathbf {x} ^ {\mathrm {C E}} \in \arg \min  _ {\mathbf {x} \in \mathcal {X}} \operatorname {d i s t} \left(\mathbf {x}, \mathbf {x} ^ {\mathrm {F}}\right) \text {s . t .} h (\boldsymbol {x}) \neq h \left(\boldsymbol {x} ^ {\mathrm {F}}\right), \boldsymbol {x} \in \mathcal {P} \tag {5.2}
$$

where $\mathrm { d i s t } ( \cdot , \cdot )$ is a similarity metric on $\mathcal { X }$ , and $_ { \mathcal { P } }$ is an optional set of plausibility constraints to reflect feasibility, or diversity of the obtained counterfactual explanations [151].

Naively explaining predictions may interfere with constructive recourse, if they violate plausibility and actionability (feasibility) constraints for suggested feature changes. For example, actions such as asking the individual to reduce their age or change their race are not feasible. Therefore, common plausibility constraints include (i) domain-consistency, (ii) density-consistency, and (iii) prototypical-consistency, while actionability constraints distuingish between (i) actionable and mutable, (ii) mutable but non-actionable, and (iii) immutable (and non-actionable) [133].

There are many flavors and variants of CE methods. Schut et al. [152] propose generating CEs in a white-box setting without an auxiliary model, by using the predictive uncertainty of the classifier. Abid et al. [153] outlines an approach for producing counterfactual explanations in terms of human-understandable concepts. Mahajan et al. [154] propose to preserve causal relationships among input features through feasibility constraints. For a more detailed review of CE methods, we refer the reader to Verma et al. [132].

# 5.2.2 Causal Algorithmic Recourse via Minimal Interventions

Previously, we looked at counterfactual explanations, which demonstrate ‚Äúhow the world would have (had) to be different for a desirable outcome to occur‚Äù [149]. However, these explanations may not always be translated into optimal recourse actions, a recommendable set of actions to help an individual to achieve a favorable outcome while respecting the costs of the actions [155]. The field of Causal Algorithmic Recourse (or consequential recommendations) deals with generating such ‚Äúwhat should be done in the future‚Äù recommendations while respecting both causal relations between features and as costs of actions [150].

From a causal perspective, actions correspond to interventions (Sec. 2.3.1). The modeling goal is to predict the effect of such interventions on one individual‚Äôs situation to ascertain whether or not the desirable outcome is achieved [147].

# Def.: 5.2.2: Algorithmic Recourse via Minimal Interventions (ARMI) [147]

Consider an SCM $\mathcal { M } = \{ \mathbb { F } , \mathbb { X } , \mathbb { U } \}$ consisting of observed variables $\mathbb { X } \in { \mathcal { X } }$ , exogenous variables $\mathbb { U } \in \mathcal { U }$ , and structural equations $\mathbb { F } : \mathcal { U }  \mathcal { X }$ . ARMI seeks the minimal cost set of interventions that results in a counterfactual instance yielding the favourable output from $h ( \cdot )$ :

$$
\boldsymbol {a} ^ {*} \in \underset {\boldsymbol {a} \in \mathcal {A}} {\arg \min } \operatorname {c o s t} \left(\boldsymbol {a}; \boldsymbol {x} ^ {\mathrm {F}}\right) \quad \text {s . t .} h \left(\boldsymbol {x} ^ {\mathrm {S C F}}\right) \neq h \left(\boldsymbol {x} ^ {\mathrm {F}}\right) \tag {5.3}
$$

$$
\boldsymbol {x} ^ {\mathrm {S C F}} = \mathbb {F} _ {\mathrm {A}} \left(\mathbb {F} ^ {- 1} \left(\boldsymbol {x} ^ {\mathrm {F}}\right)\right), \quad \boldsymbol {x} ^ {\mathrm {S C F}} \in \mathcal {P}, \quad \boldsymbol {a} \in \mathcal {A} \tag {5.4}
$$

where $\mathbfsmash { \mathbf { 0 } ^ { \ast } } \in \mathcal { A }$ directly specifies the set of feasible actions to be performed for minimally costly recourse, with $\operatorname { c o s t } ( \cdot ; \pmb { x } ^ { \operatorname { F } } ) : \mathcal { A } \times \mathcal { X }  \mathbb { R } _ { + }$ , and ${ \pmb x } ^ { * \mathrm { S C F } } =$ $\mathbb { F } _ { \mathrm { A } ^ { * } } \left( \mathbb { F } ^ { - 1 } \left( { \pmb x } ^ { \mathrm { F } } \right) \right)$ denotes the resulting structural counterfactual.

Although $\pmb { x } ^ { \mathrm { S C F } }$ is a counterfactual instance, it does not need to correspond to the nearest counterfactual explanation $\mathbf { \Delta } _ { \mathbf { \mathcal { X } } } ^ { \mathrm { C E } }$ , resulting from Eq. (5.2). Importantly, using the formulation in Eq. (5.4), Karimi et al. [147] prove formally that consequential recommendations generated by optimizing w.r.t the decision boundary may be suboptimal, as they do not benefit from the causal effect of actions towards changing the prediction.

Note that Def. 5.2.2 requires a causal model. In practice, we may learn this model from data under the assumption of no hidden confounding, but this assumption can be unrealistic for some scenarios. von K√ºgelgen et al. [156] relax this assumption and propose a partial identification approach that is compatible with unobserved confounding and arbitrary structural equations. Their approach bounds the expected counterfactual effect of recourse actions.

# 5.3 Open Problems

# 5.3.0.1 Unifying Feature Attribution and Explanations

Attribution and explanation methods can be complementary. For example, as Verma et al. [132] point out that explanations need to emphasize what should remain the same in addition to what should change in order to achieve a desired outcome.

Imagine an ML model used for loan prediction that uses ‚Äúincome‚Äù and ‚Äúyears of employment‚Äù as inputs. The ML model rejected the loan request of an individual and recommended an increase in ‚Äúincome‚Äù. Therefore, the individual changed their job and their feature ‚Äúyears of employment‚Äù was reset to zero. Despite the increase in ‚Äúincome‚Äù the model still rejected the loan request since it did not specify that the other feature should not change.

Having access to causal attribution scores may prevent such situations: a low score for the feature ‚Äôyears of employment‚Äô would inform the individual that changing its value should not cause any interference for the desired outcome. First steps towards combining both approaches can be found in [157, 158, 159, 160, 161].

Fokkema et al. [162] interpret a counterfactual explanation $\pmb { x } ^ { \mathrm { C E } }$ as attributions that describe a perturbation of ${ \pmb x } ^ { \mathrm { F } }$ . Since the differences between ${ \pmb x } ^ { \mathrm { F } }$ and ${ \pmb x } ^ { \mathrm { C E } }$ can be interpreted as the changes needed to flip the class, we can regard $\varphi _ { f } = \pmb { x } ^ { \mathrm { C E } } - \pmb { x } ^ { \mathrm { F } }$ as the attribution vector.

# 5.3.0.2 Scalability and Throughput

Computing explanations comprises solving optimization problems that are often very expensive and difficult to solve. On the attribution side, the computation time often increases exponentially with the number of features, e.g., in the case of Shapley values, making their exact solution computationally intractable when we have more than a few features. On the contrastive explanations side, incorporating plausibility and actionability constraints makes the problem NP-hard or even NP-complete when solving for [133], e.g., integer-based variables [163], neural networks, or quadratic objectives and constraints.

To make explanations deployable in large-scale systems with many users, approximate methods are highly needed. On the attribution side, one solution might be to compute contributions for only a few samples of the possible coalitions [129]. Lundberg and Lee [138] propose the approximate Kernel SHAP method, which assumes feature independence. As first steps towards that goal, Mahajan et al. [164] learn a VAE which can generate multiple counterfactuals for any given input at once.

# 5.3.0.3 Dynamics

Most explanation methods assume a static black-box model that does not change over time. However, distribution shifts occur frequently in many real-life ML do-

![](images/f99dbee53fc43eb7954f1a1345efc653c5ba07725e6b5d7a5b45996ac91906fd.jpg)  
Figure 5.3: Model Stealing through Contrastive Explanations [133]: Karimi et al. [133] illustrate the model stealing process in 2D and 3D using hypothetical non-linear decision boundaries. ‚ÄúHow many optimal contrastive explanations are needed to extract the decision regions of a classifier?‚Äù can be formulated as ‚ÄúHow many factual balls are needed to maximally pack all decision regions?‚Äù

mains: user behavior may change over time, models get updated, and a decisionmaking system‚Äôs utility function may be modified. Naturally, these issues arise for model explanations too, and there exists only limited work to address them [165, 166, 167].

# 5.3.0.4 Security and Privacy

Consider a deployed ML system that provides explanations to its users, e.g., through access to an API. In such settings, an adversary may combine predictions and explanations to extract an approximate model as well as information about the data used to train it.

For example, in the case of contrastive explanations, three possible attack strategies are model extraction [168] by (i) augmenting the surrogate‚Äôs model dataset with counterfactual instances/labels, and (ii) approximating its decision boundaries through optimal contrastive explanations, as visually illustrated in Fig. 5.3 and formalized by Karimi et al. [133], and (iii) membership inference attacks which quantify the information explanations leak about the presence of a datapoint in the training set for a model [169].

In the case of models using causal knowledge about the DGP (Secs. 5.1.3, 5.1.4 and 5.2.2), we hypothesize that it is important to guard this causal knowledge against causal-discovery-like model extraction attacks too. An attacker who is allowed to perform enough queries may use an active learning approach to actively intervene on input points and yield the causal graph of the explanation model [170].

Slack et al. [171] describes the vulnerabilities of counterfactual explanations and show that they may converge to drastically different counterfactuals under a small perturbation, indicating that they are not robust. To this end, they propose an objective to train seemingly fair models where counterfactual explanations find lower cost recourse under perturbations.

There has been very limited work on guarding explanation methods against such concerns. Shokri et al. [169] propose differentially private (DP) model explanations. They design an adaptive DP gradient descent algorithm that finds the minimal pri-

vacy budget required to produce accurate explanations. Naidu et al. [172] investigate the interpretability of DP-trained medical imaging models.

# 5.3.1 Robustness vs. Recourse Sensitivity

Two desirable properties of explanation methods are robustness and recourse sensitivity [162]. Robustness refers to changes in the inputs: if we reason that similar users should get similar options for recourse, then small changes in the input $x$ should not cause large jumps in the explanation $\varphi _ { f } ( x )$ , i.e. $\varphi _ { f }$ should be continuous. A recourse sensitive attribution method permits that the user can always achieve sufficient utility by moving in the direction of the vector $\varphi _ { f } ( x )$ .

Fokkema et al. [162] point out that attribution methods and counterfactual explanations cannot be robust and recourse sensitive at the same time. Their main conclusion is that for any way of measuring utility, there exists a model $f$ for which no attribution method $\varphi _ { f }$ can be both recourse sensitive and continuous. Future work should therefore consider workarounds to circumvent Fokkema et al. [162]‚Äôs impossibility result.

# Causal Fairness

Machine learning models increasingly assist in life-changing decisions like parole hearings, loan applications, and university admissions. Decisions in these areas may have ethical or legal implications, so a model practitioner must consider the societal impact of their work.

If the data used to train an algorithm contains demographic disparities against certain races, genders, or other groups, the algorithm will too. Instead of just maximizing measures that quantify desirable statistical properties of a predictor, algorithmic fairness aims to provide criteria that can be used to assess a model‚Äôs fairness and mitigate harmful disparities.

Causality plays a significant role in investigating a model‚Äôs fairness because it typically depends on the causal structure of the data: for some causal graphs, it can be fair to include certain input features, while for others, it is not. Unfortunately, statistical-based fairness measures are oblivious to distinguishing between different causal relationships among input variables and quickly fail to detect discrimination in the presence of statistical anomalies such as Simpson‚Äôs paradox [173].

For example, imagine that instead of considering the causal structure of the data, we would down-weight or discard sensitive attributes. Doing so may not result in a fair procedure as the sensitive attribute often correlates with other attributes. Especially in large feature spaces, sensitive attributes are often redundant given the other features. The classifier may learn a redundant encoding for the sensitive features in such settings.

Barocas et al. [174] illustrate this issue of learning classifiers using sensitive attributes without explicitly being asked to with the following example: Consider a fictitious start-up that sets out to predict your income from your genome. DNA can predict income better than random guessing because DNA encodes information about ancestry, which correlates with income in some countries, such as the United States. Hence, the learned classifier may use ancestry in an entirely implicit manner. Unfortunately, removing redundant encodings of ancestry from the genome is a difficult task that cannot be accomplished by removing a few individual genetic markers.

In contrast, by embracing the sensitive attributes and their causal relationships with the other input variables, we can intervene on them and analyze how changing their values affects the model‚Äôs predictions. A fair predictor would be invariant w.r.t. such interventions. In the DNA example, this means that ‚Äì all other things being equal ‚Äì

a fair predictor remains invariant w.r.t. ancestry by ensuring that interventions on the genetic markers do not influence the predictions.

In this chapter, we discuss two classes of causality-based fairness criteria: counterfactual (CFF) and interventional fairness (IF). CTF criteria assess the unfair influence of protected attributes on the outcome variable by analyzing counterfactuals. These criteria primarily differ based on how the counterfactuals are constructed. Further, IF approaches aim to relax some of the strong assumptions CFF require to make them more practical in real-life settings.

Table 6.1: Criterion Overview of Causal Fairness.   

<table><tr><td>Class</td><td>Criterion</td><td>Key Idea</td><td>Ref.</td></tr><tr><td rowspan="3">Ctf.</td><td>Counterfactual Fairness (CF)</td><td>A decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group</td><td>Sec. 6.2.1</td></tr><tr><td>Path-Specific CF</td><td>Refinement of CF by distinguishing between fair and unfair pathways in causal DAG</td><td>Sec. 6.2.2</td></tr><tr><td>Causal Explanation Formula</td><td>Decomposition of CF into direct, indirect, and spurious discrimination</td><td>Sec. 6.2.3</td></tr><tr><td rowspan="2">Int.</td><td>Proxy Fairness</td><td>Intervention on proxy variables of protected attributes instead of the latter directly</td><td>Sec. 6.3.1</td></tr><tr><td>Justifiable Fairness</td><td>Opens up the possibility of not requiring a full causal model: requires only separation of variables into admissible and inadmissible</td><td>Sec. 6.3.2</td></tr></table>

# Notation

A Protected (or ‚Äúsensitive‚Äù) attributes of an individual   
X Remaining attributes of an individual   
U Exogenous attributes of an individual   
W Attributes between $\mathbf { X }$ and $Y$   
Y Outcome   
YÀÜ Outcome predictor

![](images/05987b8bea5748513f58e511c6f64ecfd464f3ddc1d50efbc92f3cb4a3b54e45.jpg)  
(a)

![](images/9e027cc9a55f97ea9a075efa930c3d3e200aaca2a55692184c710058f8bb3694.jpg)  
(b)   
Figure 6.1: Two possible causal graphs considered by Counterfactual Fairness [175]: (a) shows the setup where only the latent confounder $\mathbf { U }$ causes the the outcome; in contrast, (b) considers both $\mathbf { U }$ and $\mathbf { X }$ influencing $Y$ .

# 6.1 Two more detailed examples

The aforementioned DNA example clarifies why one has to account for the causal structure of the data to ensure fair predictions. Now, we give two more detailed examples with concrete causal DAGs, as shown in Fig. 6.1. These examples are taken from Kusner et al. [175].

In both scenarios, using the seemingly fair attributes $\mathbf { X }$ to predict $Y$ is unfair.

Fig. 6.1a illustrates a setting where only latent confounder U causes $Y$ . Suppose, for example, a car insurance company wants to price insurance for car owners by predicting their accident rates $Y$ , assuming there is an unobserved variable corresponding to aggressive driving U which (a) increases the likelihood that drivers will have an accident and (b) increases the likelihood of individuals preferring red cars, captured by $\mathbf { X }$ . Furthermore, people with specific demographic characteristics may prefer driving red cars. Using the red car feature $\mathbf { X }$ to predict the accident rate $Y$ seems unfair, because these individuals are no more likely than anyone else to be aggressive or to get into accidents.

Fig. 6.1b illustrates the situation where both $\mathbf { U }$ and $\mathbf { X }$ influence the outcome $Y$ . Consider a crime prediction scenario in which a municipality wishes to estimate crime rates by neighborhood to allocate police resources. The data set contains residents‚Äô neighborhood $\mathbf { X }$ , demographics A, and binary labels $Y$ indicating a criminal arrest history. Due to historically segregated housing, the location $\mathbf { X }$ depends on A, and locations $\mathbf { X }$ do not have equally many police resources (unobserved). The number of arrests $Y$ are higher in areas with more police resources $\mathbf { X }$ . U represents the totality of socioeconomic factors and policing practices that influence where an individual may live and how likely they are to be arrested. While $Y$ depends on $\mathbf { X }$ , the predictor $\hat { Y }$ could be influenced by values of A that are not explained by U.

For another, even more exhaustive example, including calculations of statistical and causal fairness criteria using concrete data, we direct the reader to Makhlouf et al. [173].

# 6.2 Counterfactual Fairness Criteria

# 6.2.1 Kusner et al. [175]‚Äôs Counterfactual Fairness

We start with the counterfactual fairness criterion by Kusner et al. [175]. It ensures that predictors of the outcome $Y$ , which any individual with features $\{ { \pmb x } \cup { \pmb a } \}$ receives in the real world, is the same as the one they would receive in a counterfactual world in which only the protected attribute of the individual $\mathbf { A } = \pmb { a } ^ { \prime }$ has changed, everything else remaining equal. In other words, the predictor is invariant to both counterfactuals. We formalize this criterion by recalling the definition of a counterfactual in Sec. 2.3.2.

# Def.: 6.2.1: Counterfactual Fairness [175]

Given latent exogenous variables U that are not caused by any of $\mathbf { X }$ or $\mathbf { A }$ , we say that the predictor $\hat { Y }$ is counterfactually fair if

$$
p \left(\hat {y} _ {\boldsymbol {a}} (\boldsymbol {u}) \mid \boldsymbol {x}, \boldsymbol {a}\right) = p \left(\hat {y} _ {\boldsymbol {a} ^ {\prime}} (\boldsymbol {u}) \mid \boldsymbol {x}, \boldsymbol {a}\right), \quad \forall \boldsymbol {x}, \boldsymbol {a}, \boldsymbol {a} ^ {\prime}, y. \tag {6.1}
$$

We interpret the final expression of Eq. (6.1) as the probability that $\hat { Y }$ predicts $y$ for a given individual for which we observe features $_ { x }$ and protected attribute $\textbf { \em u }$ , had the protected attribute been $\mathbf { { \boldsymbol { a } } } ^ { \prime }$ instead of $\textbf { \em u }$ . An analogous interpretation of the first expression asserts that it equals $p ( \boldsymbol { y } \mid \boldsymbol { x } , \boldsymbol { a } )$ .

# 6.2.2 Path-Specific Counterfactual Fairness

The aforementioned counterfactual fairness criterion (Definition 6.2.1) considers the full effect of the sensitive attribute on the decision as problematic. However, this is not the case in certain scenarios, and we want to be more precise about their contribution to unfair predictions. Nabi and Shpitser [176] identify that discrimination can be formalized as the presence of an effect of a covariate on the outcome along specific causal pathways, but not all of them.

More concretely, Chiappa [177] substantiate this concern by using the famous Berkeley alleged gender bias case, commonly used as a textbook example of Simpson‚Äôs Paradox. In this scenario, data on college admissions showed a bias for male applications overall. However, the university rejected female applicants more often than male applicants because they applied to more competitive departments with lower admission rates. Such an effect of gender through department choice is not unfair.

Motivated by this, Chiappa [177] propose path-specific counterfactual fairness, a more fine-grained fairness criterion that deals with sensitive attributes affecting the decision along both fair and unfair pathways. It states that a decision is fair toward an individual if it coincides with the one the model would have taken in a counterfactual world in which the sensitive attribute along the unfair pathways differed.

# Def.: 6.2.2: Path-Specific Cf. Fairness [176, 177]

Define ${ \mathcal { P } } _ { { \mathcal { G } } _ { A } }$ as the set of all directed paths from $\mathbf { A }$ to $Y$ in $\mathcal { G }$ which correspond to all unfair chains of events where A causes $Y$ . Let $\mathbf { X } _ { \mathcal { P } _ { \mathcal { G } _ { A } } ^ { c } } \subseteq \mathbf { X }$ be the subset of covariates not present in any path in ${ \mathcal { P } } _ { { \mathcal { G } } _ { A } }$ . Then, Predictor $\hat { Y }$ is pathspecifically counterfactually fair w.r.t. path set ${ \mathcal { P } } _ { { \mathcal { G } } _ { A } }$ if $\forall x , a , a ^ { \prime } , y$ :

$$
p \left(\hat {y} _ {\boldsymbol {a}, \boldsymbol {x} _ {\mathcal {P} _ {\mathcal {G} _ {A}} ^ {c}}} (\boldsymbol {U}) \mid \boldsymbol {x}, \boldsymbol {a}\right) = p \left(\hat {y} _ {\boldsymbol {a} ^ {\prime}, \boldsymbol {x} _ {\mathcal {P} _ {\mathcal {G} _ {A}} ^ {c}}} (\boldsymbol {U}) \mid \boldsymbol {x}, \boldsymbol {a}\right). \tag {6.2}
$$

Similar to Chiappa [177], Wu et al. [178] define a path-specific counterfactual fairness criterion too. Their criterion enables us to reason about any population sub-group, not just individuals. Further, they address situations where identifiability may not hold by proposing a method that bounds the path-specific effects.

# 6.2.3 Causal Explanation Formula

Zhang and Bareinboim [179] propose the causal explanation formula, which allows practitioners to decompose the total observed disparity of decisions into three finegrained measures.

First, the authors assume that there is a disadvantaged group $\mathbf { \delta } _ { \mathbf { u } } ^ { \mathbf { \alpha } }$ and an advantaged one, $\pmb { a } _ { \mathrm { 0 } }$ . Further, W denotes all observed intermediate variables between A and $Y$ . Next, they define different effects; if the effect is non-zero, the predictor is unfair. They distinguish between direct and indirect spurious effects; the latter considers the back-door paths between A and $Y$ , that is, paths with an arrow into A.

# Def.: 6.2.3: Counterfactual Effects [179]

We denote all observed mediator variables between A and $Y$ as W. Then, the direct effect (DE), indirect effect (IE), and spurious effect (SE) are respectively defined as:

$$
\mathrm {D E} _ {\boldsymbol {a} _ {0}, \boldsymbol {a} _ {1}} (y \mid \boldsymbol {a}) = p \left(y _ {\boldsymbol {a} _ {1}, \mathbf {W} _ {\boldsymbol {a} _ {0}}} \mid \boldsymbol {a}\right) - p \left(y _ {\boldsymbol {a} _ {0}} \mid \boldsymbol {a}\right), \tag {6.3}
$$

$$
\operatorname {I E} _ {\boldsymbol {a} _ {0}, \boldsymbol {a} _ {1}} (y \mid \boldsymbol {a}) = p \left(y _ {\boldsymbol {a} _ {0}, \mathbf {W} _ {\boldsymbol {a} _ {1}}} \mid \boldsymbol {a}\right) - p \left(y _ {\boldsymbol {a} _ {0}} \mid \boldsymbol {a}\right), \tag {6.4}
$$

$$
\operatorname {S E} _ {\boldsymbol {a} _ {0}, \boldsymbol {a} _ {1}} (y) = p \left(y _ {\boldsymbol {a} _ {0}} \mid \boldsymbol {a} _ {1}\right) - p \left(y \mid \boldsymbol {a} _ {0}\right). \tag {6.5}
$$

Next, they aim to provide intuition on how the different effects relate to each other by decomposing the total variation (TV) into them. TV is the difference between the conditional distributions of $Y$ when passively observing A changing from $\pmb { a } _ { \mathrm { 0 } }$ to $\mathbf { \delta } _ { \mathbf { u } 1 }$ . Formally, the TV of event $\mathbf { A } = \pmb { a } _ { 1 }$ on $Y = y$ (with baseline $\mathbf { \delta } \mathbf { a } _ { 0 }$ is defined as:

$$
\mathrm {T V} _ {\boldsymbol {a} _ {0}, \boldsymbol {a} _ {1}} (y) = p (y \mid \boldsymbol {a} _ {1}) - p (y \mid \boldsymbol {a} _ {0}). \tag {6.6}
$$

# Def.: 6.2.4: Causal Explanation Formula [179]

The total variation (TV), spurious effect (SE), indirect effect (IE), and direct effect (DE) are related as

$$
\mathrm {T V} _ {\boldsymbol {a} _ {0}, \boldsymbol {a} _ {1}} (y) = \mathrm {S E} _ {\boldsymbol {a} _ {0}, \boldsymbol {a} _ {1}} (y) + \mathrm {I E} _ {\boldsymbol {a} _ {0}, \boldsymbol {a} _ {1}} (y \mid \boldsymbol {a} _ {1}) - \mathrm {D E} _ {\boldsymbol {a} _ {1}, \boldsymbol {a} _ {0}} (y \mid \boldsymbol {a} _ {1}), \tag {6.7}
$$

$$
\mathrm {T V} _ {\boldsymbol {a} _ {0}, \boldsymbol {a} _ {1}} (y) = \mathrm {D E} _ {\boldsymbol {a} _ {0}, \boldsymbol {a} _ {1}} (y \mid \boldsymbol {a} _ {0}) - \mathrm {S E} _ {\boldsymbol {a} _ {1}, \boldsymbol {a} _ {0}} (y) - \mathrm {I E} _ {\boldsymbol {a} _ {1}, \boldsymbol {a} _ {0}} (y \mid \boldsymbol {a} _ {0}). \tag {6.8}
$$

For example, the first formula shows that the total disparity experienced by the individuals who have naturally attained $\mathbf { \delta } _ { \mathbf { u } } ^ { \mathbf { \alpha } }$ equals the disparity experienced via spurious discrimination, plus the advantage is lost due to indirect discrimination minus the advantage it would have gained without direct discrimination.

# 6.3 Interventional Fairness

Counterfactual fairness involves modeling counterfactuals on an individual level, which is problematic. For example, [180, 181] point out that effects of race or gender are challenging to model even at the group level. Thus, interventions on such (often ill-defined) protected attributes are typically hard to envision. Kilbertus [182] illustrates this through the following thought experiment: imagine a pregnant woman‚Äôs job application gets rejected. Could we imagine her life as a man in that world? Was she born male or perceived as male during the hiring process? Is she a pregnant man now? The notion of counterfactual fairness renders meaningless if we compare (fictitious) individuals who are vastly different from each other.

# 6.3.1 Proxy Fairness

To address these concerns, Kilbertus et al. [183] examines population-level interventional distributions, which they call proxy discrimination. The idea is to separate the protected attribute A from its potential proxies, such as names, visual features, languages spoken at home, etc. Hence, an intervention based on proxy variables presents a more manageable problem. In practice, we are frequently limited to imperfect measurements of A, so separating the root concept from a proxy is prudent [182]. A proxy $\mathbf { P }$ is a descendant of A in the assumed causal graph.

# Def.: 6.3.1: Proxy Fairness [182]

Predictor $\hat { Y }$ exhibits no proxy discrimination based on a proxy $\mathbf { P }$ if

$$
p (y \mid \operatorname {d o} (\boldsymbol {p})) = p (y \mid \operatorname {d o} (\boldsymbol {p} ^ {\prime})), \quad \forall \boldsymbol {p}, \boldsymbol {p} ^ {\prime}. \tag {6.9}
$$

# 6.3.2 Justifiable Fairness

All previous approaches require knowledge of the causal graph. To deal with settings where a causal graph is missing but a partial knowledge of admissible variables is given, Salimi et al. [184] formalize interventional fairness as a database repair problem. They present data pre-processing algorithms for providing fairness guarantees about classifiers trained on pre-processed training data.

First, they assume that the causal graph is given and define the $K$ -fair criterion, which captures group-level fairness, similar to proxy fairness.

# Def.: 6.3.2: K-Fair [184]

For a set of attributes $\mathbf { K } \subseteq \mathbf { X }$ , we say that the predictor $\hat { Y }$ is $\mathbf { K }$ -fair w.r.t. protected attributes A if $\forall k , a , \hat { y }$ :

$$
p (\hat {y} \mid \operatorname {d o} (\boldsymbol {a}, \boldsymbol {k})) = p (y \mid \operatorname {d o} (\boldsymbol {a} ^ {\prime}, \boldsymbol {k})). \tag {6.10}
$$

A model is deemed interventionally fair if it is $\mathbf { K }$ -fair for every set $\mathbf { K }$ . This notion differs from proxy fairness in that it ensures that A does not affect $Y$ in any configuration obtained by fixing other variables to arbitrary values. As opposed to counterfactual fairness, it does not try to capture fairness at the individual level, so it employs level-2 interventions.

Next, they define the justifiable fairness criterion, which allows the user to distinguish only admissible and inadmissible variables. The former variables are a subset of the protected attributes A through which it is still permissible to influence the outcome.

# Def.: 6.3.3: Justifiable Fairness [184]

A predictor $\hat { Y }$ is justifiably fair if it is $\mathbf { K }$ -fair w.r.t. all supersets $\mathbf { K } \supseteq \mathbf { A }$

# 6.4 Fairness under Distribution Shifts

Singh et al. [185] study the problem of learning fair prediction models under covariate shift, i.e., with test set covariates distributed differently from the training set. Given the ground-truth causal graph describing the data and anticipated changes, they propose an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set.

Schrouff et al. [186] evaluates how realistic the assumptions made by Singh et al. [185] as well as other works are [187, 188]. They categorize distribution shifts into four categories: demographic shift, covariate shift, label shift, and compound shift, independently of the causal graph considered. Their study examines two real-world applications in dermatology and electronic health records and shows that clinically plausible shifts directly affect aspects of the data distribution simultaneously. There-

fore, and as empirically demonstrated, compound shifts impact the transferability of fairness properties in these applications.

# 6.5 Open Problems

# 6.5.1 Alternatives to equality

Most fairness definitions in the literature emphasize equality, ensuring that each individual or group receives the same resources, attention, or outcome. In contrast, equity [189, 190] has received little attention, which means that all individuals and groups have access to the resources they need to thrive. An exciting future direction is to operationalize this definition and examine how it enhances or contradicts existing definitions of fairness [3].

Another alternative to equality might be considering a model‚Äôs harm: Richens et al. [191] propose a definition for a harm criterion that should be able to answer the following three questions precisely: ‚ÄúQ1: Did the actions of an agent cause harm, and if so, how much? Q2: How much harm can we expect an action to cause before taking it? Q3: How can we identify actions that balance the expected harms and benefits?‚Äù They then propose a family of counterfactual objective functions that mitigate harm.

# 6.5.2 Fairness beyond predictions

In this chapter, we studied predictive fairness criteria that evolved around supervised models‚Äô predictive output. However, there is growing interest in assessing the fairness of ML techniques beyond prediction, e.g., in the realm of recourse methods (Sec. 5.2.2).

Gupta et al. [192] put forward that models should not limit the ability to achieve recourse to only those with access to expensive resources. Put another way; the model must fairly distribute the rights of recourse across (demographically defined) groups. von K√ºgelgen et al. [193] investigate the fairness of causal algorithmic recourse actions and conclude that there is still much work left. For example, the question of whether it is appropriate to perform societal interventions on all individuals in a subgroup. Huan et al. [194] deal with equality of efforts: they seek to determine whether the efforts made to achieve the same outcome level are the same or different between those in the protected group and those in the unprotected group.

# 6.5.3 Partial identification

Causal quantities, especially counterfactuals, are often unidentifiable (Sec. 2.6). That means that we can not compute them from a statistical quantity. While previous causal fairness work has partly addressed these issues [175, 178, 195], we foresee more important work to be done in sensitivity analysis through realistic simulations

to thoroughly test these methods and alter aspects of them to understand how violations of assumptions impact fairness estimation.

# 6.5.4 Manipulability of social categories

From a social science perspective, it is heavily debated whether social categories admit interventions. Kohler-Hausmann [196] criticize that counterfactuals require us to reduce race to only the signs of the category, e.g., the skin color or phenotype of a race. Hu [197] argues that causal theories about social categories such as race involve ‚Äúineliminable substantive moral and political considerations, a feature for which interventionism can not well account‚Äù. Hu and Kohler-Hausmann [198] challenge the validity of specifying a social group such as gender as a variable in a DAG while still assuming the model‚Äôs modularity assumption. More broadly, Kasirzadeh and Smart [199] review various papers at this intersection between sociology and causal modeling, concluding that social categories often do not admit counterfactual manipulation.

# 6.5.5 Trade-offs between fairness criteria

Kleinberg et al. [200], Corbett-Davies et al. [201], Chouldechova [202] show that fairness criteria can be opposed to each other, and sometimes, one cannot have multiple criteria simultaneously satisfied. Hence, in practice, the trade-off between multiple admissible criteria has to be balanced. Gultchin et al. [203] shed some first light on how to balance certain measures by using a multi-objective framework. Similarly, Nilforoshan et al. [204] highlight limitations and potential adverse consequences of causal fairness criteria. They demonstrate cases in which these criteria lead to policies that would be disfavored by every stakeholder.

# 6.5.6 Unfair models despite fair data

In debates about model fairness, a common assumption is that unfairness arises through biases in data. However, Ashurst et al. [205] point out that models can produce unfair predictions even when the training labels are fair. The authors refer to this phenomenon as introduced unfairness and investigate the conditions under which it may arise. Taking a causal perspective, they show that the notion of introduced unfairness can be applied to causal definitions of fairness too. For example, they consider path-specific introduced effects as the difference in some path-specific effects on labels and predictions.

Minimal work exists investigating when fair training labels may not yield a fair model. Ashurst et al. [205] conclude that it is difficult to rule out unfair disparities, even when the criteria of causal fairness are met. Future research may stress-test the validity of the in this section introduced fairness criteria.

# Causal Reinforcement Learning

Reinforcement Learning (RL) is a framework in which autonomous agents interact with their environments to learn optimal behaviors, improving over time through trial and error. Its central goal is learning how to map situations to actions while maximizing a numerical reward signal [206]. RL researchers typically formalize their problem setup by using Markov decision processes (MDPs), which include three ingredients: sensation (observation), action, and goal (reward).

In this section, we highlight methods benefiting RL problems by exploiting the causal paradigm (and not the other way around, see, e.g., [207]). We refer to this family of approaches as Causal Reinforcement Learning. We summarize the benefits of these approaches in Table 7.1.

# Notation

$t$ discrete time step $T$ final time step of an episode $t$ $A_{t}$ action at time $t$ $\mathbf{S}_t$ state at time $t$ $L_{t}$ regret/loss at time $t$ $R_{t}$ reward at time $t$ $\mathcal{R}$ return $\pi$ policy (decision-making rule) $\pi(a|s)$ probability of taking action $a$ in state $s$ $\boldsymbol{s}, \boldsymbol{s}^{\prime}$ true states $\boldsymbol{x}, \boldsymbol{x}^{\prime}$ observed states $v_{\pi}(s)$ value of state $s$ under policy $\pi$ (expected return) $q_{\pi}(s, a)$ value of taking action $a$ in state $s$ under policy $\pi$ $\tau$ trajectory, i.e., $\tau = \{\boldsymbol{x}_t, a_t, \boldsymbol{x}_{t+1}\}_{t=1}^T$

# 7.1 Isn‚Äôt RL already ‚ÄúCausal‚Äù?

The short answer is yes. For many years, researchers have argued that there exist links between certain flavors of RL and causal inference [7, 208, 209, 210, 211, 212, 213, 214]. For example, Bottou et al. [208] frame bandit problems and Markov decision processes (MDPs) as special cases of causal models. However, despite con-

Table 7.1: Problem Overview of Causal Reinforcement Learning.   

<table><tr><td>Problem</td><td>Output</td><td>Benefits over non-causal RL</td><td>Ref.</td></tr><tr><td>Causal Bandits</td><td>\(\hat{\pi}=\arg\min L_n(\pi)\) 
\(\pi \in \Pi\)</td><td>Optimal simple regret guarantees</td><td>Sec. 7.2</td></tr><tr><td>Model-Based RL</td><td>\(\widehat{\theta}=\arg\min\ell(\theta,(R_{t+1},S_{t+1}))\) 
\(\theta \in \Theta\)</td><td>Deconfounding</td><td>Sec. 7.3</td></tr><tr><td>Multi-Environment RL</td><td>\(\hat{\pi}=\arg\max\mathbb{E}_{c\sim p(c)}[\mathcal{R}(\pi,\mathcal{M}^c)]\) 
\(\pi \in \Pi\)</td><td>Interpretable task embeddings, systematic generalization</td><td>Sec. 7.4</td></tr><tr><td>Off-Policy Policy Evaluation</td><td>\(\hat{v}_{\pi}(s)=\mathbb{E}_{x\sim d_0}\left[\sum_{t=0}^{T-1}\gamma^tr_t\mid x_0=x\right]\)</td><td>Deconfounding</td><td>Sec. 7.5</td></tr><tr><td>Imitation Learning</td><td>\(\hat{\pi}=\arg\min\mathbb{E}_{x\sim d_{\pi^*}}[\ell(\boldsymbol{x},\pi,\pi^*(\boldsymbol{x}))]\) 
\(\pi \in \Pi\)</td><td>Deconfounding</td><td>Sec. 7.6</td></tr><tr><td>Credit Assignment</td><td>\(\mathcal{M}_{a_t\rightarrow r_{t+k}}\) or \(\mathcal{M}_{a_t\rightarrow s_{t+1}}\) or \(\mathcal{M}_{a_t^i\rightarrow a_t^j}\)</td><td>Intrinsic reward, Data-efficiency</td><td>Sec. 7.7</td></tr><tr><td>Counterfactual Data Augmentation</td><td>\(\tilde{\tau}=\{\tilde{\boldsymbol{x}}_t,\tilde{a}_t,\tilde{\boldsymbol{x}}_{t+1}\}_{t=1}^T\)</td><td>Data-efficiency</td><td>Sec. 7.8</td></tr><tr><td>Agent Incentives</td><td>Incentive criteria and measures</td><td>Avoiding unintended harmful behavior</td><td>Sec. 7.9</td></tr></table>

ceptual similarities, the two fields have mainly focused on different goals: on the one hand, the RL community has focused on building algorithms to maximize rewards; on the other hand, the focus in the causality literature has been on the identifiability and inferences of or based on given causal structure [214].

We attribute one reason for different foci among both communities to the type of applications each tackles. The vast majority of literature on modern RL evaluates methods on synthetic data simulators, able to generate large amounts of data. For instance, the popular AlphaZero algorithm assumes access to a boardgame simulation that allows the agent to play many games without a constraint on the amount of data [215]. One of its significant innovations is a tabula rasa algorithm with less handcrafted knowledge and domain-specific data augmentations. Some may argue that AlphaZero proves Sutton‚Äôs bitter lesson [216]. From a statistical point of view, it roughly states that given more compute and training data, general-purpose algorithms with low bias and high variance outperform methods with high bias and low variance.

In contrast, in the causal inference literature, we are typically given a limited-size observational dataset from an unknown policy and unknown environment and cannot interact with the environment in an online fashion. The reason behind that problem convention is that much of causal inference methodology originated from domains like medicine, econometrics, online advertisements, and the social sciences, in which conducting experiments is infeasible due to ethical or cost/time-consuming reasons. Nonetheless, causal inference is typically applied in contexts where decisions directly influence human individuals. For example, Bottou et al. [208] illustrates how causal inference can be used in the ad placement system associated with the Bing search engine.

In causal inference, instead of maximizing a reward function in expectation, a common target is to learn the heterogeneous treatment effect (HTE) (Sec. 11.2.1.1): it quantifies the expected effect of changing the observed treatment $\textbf { \em t }$ to a different treatment $\mathbf { \Delta } t ^ { \prime }$ for a certain subgroup characterized by covariates $_ { x }$ , denoted as

$$
\tau \left(\boldsymbol {t} ^ {\prime}, \boldsymbol {t}, \boldsymbol {x}\right) \triangleq \mathbb {E} \left[ Y \mid \boldsymbol {x}, \operatorname {d o} \left(\boldsymbol {t} ^ {\prime}\right) \right] - \mathbb {E} \left[ Y \mid \boldsymbol {x}, \operatorname {d o} (\boldsymbol {t}) \right]. \tag {7.1}
$$

By assumption, we observe only one treatment and outcome pair per subgroup. By using an estimator $\widehat { \tau }$ , decision-makers may reason about what treatments work well for what subgroups. Due to high risks in its applied domains, the HTE research communities prioritize strong theoretical guarantees like convergence rates as a function of the dataset size or analytical confidence intervals. This preference has led to advances like doubly-robust plug-in estimators capable of utilizing machine learning methods [217, 218].

Somewhere in between the two is the more recent subfield of offline reinforcement learning (ORL) [219]. Here, the goal is to learn optimal policies from a data set containing trajectories generated from an unobserved policy. Offline refers to the fact that the algorithms have to exploit a batch dataset from an unknown environment without access to online exploration, which matches the common problem setup in causal inference. Despite these similarities, there are also differences between the two methodologies, which we list in Table 7.2. Nonetheless, we hope to see constructive cross-pollination between these two fields.

# 7.2 Causal Bandits

A bandit problem is a sequential game between a learner and an environment [220]. The game is played over $n \in \mathbb N$ rounds, where $n$ is also called the horizon. In each round $t \in [ n ]$ , the learner first chooses an action $A _ { t }$ from a given set $\mathcal { A }$ (also called arms), and the environment then reveals a reward $R _ { t } \in \mathbb { R }$ .

When $| { \mathcal { A } } | = k \in \mathbb { N }$ , we refer to the problem as $k$ -armed bandits. When $k \geq 2$ , but $k$ itself is irrelevant, we simply call it multi-armed bandits.

Lattimore et al. [221] formalize causal bandit problems, a class of stochastic sequential decision problems in which rewards are given for repeated interventions on a fixed causal model. The motivation is to exploit the causal information for predict-

Table 7.2: Differences between Offline RL and Heterogeneous Treatment Effect (HTE) estimation. Both methodologies share the motivation of extracting information for decision-making from observational data generated by unknown policies. The shown typical desiderata are simplistic, yet, we hope they clarify differences between the two schools of thought.   

<table><tr><td>Approach</td><td>Input data</td><td>Goal</td><td>Typical Desiderata</td></tr><tr><td>Offline RL</td><td>Multi-Step Trajectories { (xt, at, rt)Tt=1}</td><td>Policy œÄ(X)</td><td>Reward Maximization in Test Environments</td></tr><tr><td>HTE</td><td>One-Step Individuals { (xi, ti, yi)Nt=1}</td><td>HTE œÑ(X, T, T&#x27;)</td><td>Analytical Convergence Rates</td></tr></table>

ing outcomes of interventions without explicitly performing them. Thereby, utilizing non-interventional (i.e. observational) data may improve the rate at which the policy learns high-reward actions. This framework generalizes classical bandits and contextual stochastic bandit problems: in the former, we have no additional observations besides a reward, and in the latter, we observe the context before an intervention is chosen. The causal bandit framework additionally allows us to use observations that occur after an intervention.

Lattimore et al. [221] describe the following scenario that motivates why using such extra observations can be helpful: Imagine a farmer who wants to maximize their crop yield. They know that crop yield is only affected by temperature, a particular soil nutrient, and moisture level, but their combination‚Äôs precise effect is unknown. Each season, the farmer has enough time and money to intervene and control at least one of these variables: deploying shade or heat lamps will set the temperature to be low or high; the nutrient can be added or removed through a choice of fertilizer, and irrigation or rain-proof covers will keep the soil wet or dry. When not intervened upon, the temperature, soil, and moisture vary naturally from season to season due to weather conditions. These variations are all observed along with the final crop yield at the end of each season and can inform the farmer to conduct an experiment to identify the single, highest-yielding intervention in a limited number of seasons.

# Def.: 7.2.1: Causal Bandit Problem [221]

Consider a causal model is given by a directed acyclic graph $\mathcal { G }$ over a set of multivariate random variables $\mathcal { X } = \{ \mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { N } \}$ and a joint distribution $p ( \mathcal X )$ that factorizes over $\mathcal { G }$ . For each state $\mathbf { X } \in { \mathcal { X } }$ , there is a reward variable $Y$ that takes on values in $\{ 0 , 1 \}$ . Further, given a set of allowed actions $\mathcal { A }$ , we denote the expected reward for the action $a = \mathrm { d o } ( { \pmb x } )$ by $\mathbf { U } _ { a } : = \mathbb { E } \left[ Y \mid \mathrm { d o } ( \pmb { x } ) \right]$ and the optimal expected reward by $\mathbf { U } ^ { * } : = \operatorname* { m a x } _ { a \in \mathcal { A } } \mathbf { U } _ { a }$ . The causal bandit game proceeds over $T$ rounds. In round $t$ , the learner intervenes by choosing $a _ { t } = \operatorname { d o } \left( \mathbf { X } _ { t } = \pmb { x } _ { t } \right) \in \mathcal { A }$ based on previous observations. In contrast to

the conventional bandit problem, before the agent takes the next action, it observes further samples for all non-intervened variables $\mathbf { X } _ { t } ^ { c }$ drawn from $p \left\{ \mathbf { X } _ { t } ^ { c } \mid d o \left( \mathbf { X } _ { t } = \pmb { x } _ { t } \right) \right\}$ , including the reward $Y _ { t } \in \{ 0 , 1 \}$ .

Typically, the objective of the learner is to minimize the simple regret $R _ { T } = { \bf U } ^ { * } -$ $\mathbb { E } \left| u _ { \hat { a } _ { T } ^ { * } } \right|$ . After $T$ observations the learner outputs an estimate of the optimal action $\hat { a } _ { T } ^ { * } \in \mathcal A$ based on its prior observations. This regret objective is sometimes referred to as a pure exploration [222] or ‚Äúbest-arm identification‚Äù problem [223] and is most appropriate when, as in drug and policy testing, the learner has a fixed experimental budget after which its policy will be fixed indefinitely.

We call this particular objective the parallel bandit problem, in which we formalize each arm as a binary variable that is an independent cause of the reward variable. Lattimore et al. [221] propose and analyze an algorithm for achieving the optimal regret in this setting. The authors propose another, more general algorithm for general causal graphs but leave lower regret bounds for future work. Lastly, they empirically verify that both algorithms improve over the optimal successive elimination algorithm [224] through using causal knowledge of the parallel bandit problem under various conditions.

Similarly, Lee and Bareinboim [225] show that whenever the underlying causal model of a decision-making process is not taken into account, the standard strategies of simultaneously intervening on multiple variables can lead to sub-optimal policies, regardless of the number of interventions performed by the agent in the environment. Therefore, the authors suggest to reduce the search space for interventions that lead to optimal rewards and are not redundant. First, they formalize a minimal intervention set (MIS) and then aim at finding possibly-optimal minimal intervention sets (POMIS).

de Kroon et al. [226], Lu et al. [227] study causal bandit problems with unknown causal graph structures. de Kroon et al. [226] formulate a causal bandit algorithm that uses outputs of causal discovery algorithms. They utilize a separating set, defined as a set $\mathbf { s }$ that renders a target variable $Y$ independent of a context variable I when conditioned upon, i.e., I ‚ä• Y | S. The context variable encodes the intervention. Then, they separately model how interventions influence the separating set Sand the expected reward given S. Their proposed estimator is unbiased and possesses lower variance than the sample mean.

Lu et al. [227] propose an algorithm for a class of causal graph types and prove that it achieves stronger worst-case regret guarantees than non-causal algorithms. Formally, they demonstrate that their goal of exploiting meaningful causal relations among variables cannot be achieved for general causal graphs, and in the worst case, there is no chance to do better than standard algorithms.

Similar to the causal bandit problem, Silva [228] study the related problem of doseresponse learning, i.e., how an outcome variable $Y$ (which can be a reward) varies under different levels of a control variable $\mathbf { X }$ (which can be an arm). The difference

to causal bandits is that their goal is not to maximize $Y$ but rather learn the relationship $f ( \pmb { x } ) \equiv \mathbb { E } \left[ Y \mid \mathrm { d o } ( \pmb { x } ) \right] , \pmb { x } \in \mathcal { X }$ , where $\mathcal { X } = \{ \mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { N } \}$ is a pre-defined set of actions. Their method couples different Gaussian process priors that combine observational and interventional data. They also consider active learning schemes to choose arms informed by the GPs‚Äô uncertainties.

Similarly motivated to both the causal bandit [221], and the dose-response learning problem, Aglietti et al. [229] generalize Bayesian optimization to scenarios where causal information is available. Bayesian optimization is an efficient heuristic to optimize objective functions whose evaluations are costly and cannot be analytically described [230], e.g., the final performance of a neural network after training with a specific set of hyper-parameters. Aglietti et al. [229] argue that exploiting the causal graph noticeably increases reasoning capabilities about optimal decisionmaking strategies, decreasing the optimization cost and avoiding suboptimal solutions. By integrating real interventional data with estimated intervened effects computed using do-calculus, they propose an algorithm to balance two trade-offs: exploration vs. exploitation and observation vs. intervention.

Bareinboim et al. [209] take a causal perspective on the conventional multi-armed bandit problem and show that the existence of unobserved confounders render observational and interventional data distinct (recall our discussion in Sec. 2.2.1.1). Further, they show that formalizing this distinction implies that previous bandit algorithms try to maximize rewards based on the observational distribution, which is not always the best strategy to pursue. Therefore, they propose an agent loss function that incorporates both observational and interventional distributions, improving over previous ones.

Saengkyongam et al. [231] develop a causal framework for characterizing the environmental shift problem in offline contextual bandit problems. While the formerly discussed work has focused on exploiting causal knowledge for improving the finite sample performance or the regret bound in a single environment, their work focuses on modeling distributional shifts and the ability to generalize to new environments.

Lu et al. [232] propose causal MDPs, extending the idea behind causal bandits to MDPs. The motivation is similar: they use prior causal knowledge about the state transition and reward functions to obtain conditional independence relations among action, reward, and state variables and use them to develop efficient algorithms.

# 7.3 Model-Based RL

Model-based reinforcement learning (MBRL) learns a model of the environment dynamics (state transition and reward function) in addition to the policy, effectively combining learning and planning methods (the latter have reversible access to the MDP dynamics) [233]. Generative dynamics models are sometimes referred to as world models [234]. The enticing benefit of MBRL is the promise of improving sample efficiency compared to model-free methods by extracting valuable information from

the observed trajectories and enabling the ability to sample simulated experiences from the model instead of the actual environment.

Confounded partial models: In prior works, dynamics models are often partial in the sense that they are neither conditioned on, nor generate the full set of observed data. For example, the popular MuZero model [235] is a partial model, because it predicts the state observation $y _ { T }$ at $T > 1$ given actions $a _ { t }$ and updated hidden states $h _ { t }$ but using no observational data beyond the initial state $s _ { 0 }$ at $t = 0$ . In other words, it generates $y _ { T }$ directly without generating intermediate observations.

Rezende et al. [236] demonstrate that the aforementioned partial models can be causally incorrect: they are confounded by the observations $y _ { < T }$ that they do not model, and can therefore lead to incorrect planning. The observations $y _ { < T }$ are confounders because they are used by the policy to produce the actions $a _ { < T }$ , while the partial model is missing the $y _ { < T }$ in its input. Therefore, the dynamics model is not robust against changes in the behavior policy.

To remedy confounding, the authors propose to use backdoor adjustment (Sec. 2.6) to make the action $a _ { t }$ conditionally independent of the agent state $\mathbf { \nabla } _ { s t }$ . They refer to models that are conditioned on the backdoor as Causal Partial Models (CPM) and models that are not as Non-Causal Partial Models (NCPM), see Fig. 7.1. They list multiple possible choices for the backdoor and discuss their trade-offs. For future work, they suggest focusing on dynamics model robustness against other types of interventions in the environment besides policy changes.

![](images/f1b27f5e82f1b58dcfe70d34c1f38247f4795ab6608b5f264db5236dd86869a0.jpg)  
(a) Standard autoregressive

![](images/517154f20367c2241cb1698a360d91e0ff1f20310834ec8b5a538786375033b4.jpg)  
(b) Non-Causal Partial Model

![](images/5891a17d73f0ae07b010e8dbee6ce89a76c0b1c75ba0a87ff09c4a77e0bd57b7.jpg)  
(c) Proposed CPM   
Figure 7.1: Causal Partial Models (CPM) motivation [236]: Comparison of causal graphs behind common dynamics models in Sec. 7.3

Causal World Models: Learned models of the environment dynamics are sometimes referred to as world models, particularly when the dynamics are observed from a sequence of high-dimensional raw pixel frames. Fundamentally, they are still used to estimate the observational conditional of the state transition function $p \left( \mathbf { s } ^ { t + 1 } \mid \mathbf { s } ^ { t } , a ^ { t } \right)$ (analogously, the reward function).

Li et al. [237] argue that such observational conditional (world) models can be biased in the real world where confounding factors exist. To make this issue apparent, they first contrast partially-observable MDPs (POMDPs) (Fig. 7.2a) used by conventional models with the causal POMDPs (Fig. 7.2b) that their causal word model (CWM) aims to learn from.

Given the causal POMDP, the authors define their quantity of interest as the interventional conditional $p \left( \mathbf { s } ^ { t + 1 } \mid \mathrm { d o } \left( \mathbf { s } ^ { t } = s _ { I } \right) , a ^ { t } \right)$ . Then, they argue that in many

![](images/3f962017740362eeb2412c93fdf7fcab0ca43561a60554b329511380b32d1a61.jpg)  
Figure 7.2: The graphical model of (a) POMDPs used by conventional world models [237], (b) causal POMDPs used by causal world models (CWMs), and (c) CWMs after intervention (dooperation).

real-world cases, the observational conditional and the interventional conditional are different because of the existence of confounding factors $\mathbf { u }$ . The observational conditional can be written as

$$
\begin{array}{l} p \left(\mathbf {s} ^ {t + 1} \mid \mathbf {s} ^ {t}, a ^ {t}\right) = \int_ {\mathbf {u}} p \left(\mathbf {s} ^ {t + 1} \mid \mathbf {u}, \mathbf {s} ^ {t}, a ^ {t}\right) p \left(\mathbf {u} \mid \mathbf {s} ^ {t}, a ^ {t}\right) d \mathbf {u} \\ = \int_ {\mathbf {u}} p \left(\mathbf {s} ^ {t + 1} \mid \mathbf {u}, \mathbf {s} ^ {t}, a ^ {t}\right) \frac {p \left(\mathbf {s} ^ {t} , a ^ {t} \mid \mathbf {u}\right)}{p \left(\mathbf {s} ^ {t} , a ^ {t}\right)} p (\mathbf {u}) d \mathbf {u}, \tag {7.2} \\ \end{array}
$$

and the interventional conditional as

$$
p \left(\mathbf {s} ^ {t + 1} \mid \operatorname {d o} \left(\mathbf {s} ^ {t}\right), a ^ {t}\right) = \int_ {\mathbf {u}} p \left(\mathbf {s} ^ {t + 1} \mid \mathbf {u}, \mathbf {s} ^ {t}, a ^ {t}\right) p (\mathbf {u}) d \mathbf {u}. \tag {7.3}
$$

The difference is highlighted in red.

Consequently, CWMs infer the interventional query ‚ÄúGiven that we have observed xt:T $\pmb { x } ^ { t : T } = \pmb { x } ^ { t : T }$ in the real world, what is the probability that $\scriptstyle x ^ { t + 1 : T }$ would have been xt+1: T 0I if $\pmb { x } _ { I } ^ { t + 1 : T ^ { \prime } }$ if $\mathbf { \boldsymbol { x } } ^ { t }$ were $\mathbf { \Delta } x _ { I } ^ { t }$ in the dream world?‚Äù Technically, they want to intervene upon the abstract state variable as shown in Fig. 7.2c, where $s _ { I } ^ { t } \in \mathcal { S }$ is the counterfactual value in the dream environment. This intervention is then rendered as an observable change applied to ${ \boldsymbol { x } } ^ { t }$ (such as, for instance, object displacement or removal) by the conditional observation distribution $\begin{array} { r } { \mathcal { U } \left( \pmb { x } ^ { t } = \pmb { x } _ { I } ^ { t } \ | \ \mathbf { d o } \left( \mathbf { s } ^ { t } = \ \pmb { s } _ { I } ^ { t } \right) \right) } \end{array}$ , where $\pmb { x } _ { I } ^ { t } \in \mathcal { O }$ represents the value of the counterfactual observation.

Task-Independent State Abstractions: Standard, non-causal MBRL dynamics models are dense in the sense that they predict the next step value of each variable based on the action and all variables in the current state. As such, they are sensitive to spurious associations.

For example, consider the example in Fig. 7.3a, where a robot faces two doors it can open and additionally observes a wall clock. Subfigure (a) shows a dense model. When door B is at angles unseen during training or the clock is at unseen times, this dense model‚Äôs prediction of door A can be inaccurate due to unnecessary dependence on the other variables. This issue has motivated state abstraction techniques [238, 239], which group many states into an abstract state by omitting some state variables, as illustrated in Subfigure (b). However, [240] argue that the generalization issues persist, as dense models are still used for the remaining variables, leaving unnecessary dependencies in the abstract states.

To this end, Wang et al. [240] introduce Causal Dynamics Learning (CDL) for Task-Independent State Abstraction, as illustrated in Subfigure (c). This approach learns

a causal model that explains which actions and state variables affect which variables from data. In the above example, its predictions about door A do not rely on door B and the clock and are therefore more robust to spurious associations than dense models.

More generally speaking, if there exist certain state variables which no other variables depend on (e.g., the clock), they can be omitted for planning. This motivates a novel form of state abstraction: Wang et al. [240] suggest partitioning the state variables into three groups: (i) those that the model can change with its actions (controllable variables, e.g., doors A and B), (ii) those that it cannot change but that still influence actions‚Äô results on those that it can (action-relevant variables, e.g., an obstacle that blocks door A‚Äôs motion), and (iii) the remainder which can be omitted entirely (action-irrelevant variables, e.g., the clock).

As shown in Fig. 7.3b, their approach represents the transition dynamics through a causal graphical model, which is then split into subgraphs corresponding to the three previously described partitions. To learn the causal dynamics model, a key challenge is to determine whether a causal edge exists between two state variables, i.e., whether $\mathbf { \Delta } \mathbf { s } _ { t } ^ { i } \  \ \mathbf { \Delta } \mathbf { s } _ { t + 1 } ^ { j }$ holds. For that, they leverage Mastakouri et al. [241]‚Äôs conditional independence test, which relies on approximating conditional mutual information.

To expose the causal relationships thoroughly, one has to collect trajectories with wide coverage of the state space. For that exploratory phase, they use a reward function that is the prediction difference between the dense predictor and the causal predictor learned so far:

$$
r _ {t} = \tanh  \left(\tau \cdot \sum_ {j = 1} ^ {d _ {\mathcal {S}}} \log \frac {\hat {p} \left(s _ {t + 1} ^ {j} \mid \boldsymbol {s} _ {t} , a _ {t}\right)}{\hat {p} \left(s _ {t + 1} ^ {j} \mid \mathbf {p a} _ {s ^ {j}}\right)}\right), \tag {7.4}
$$

where $\tau$ is a scaling factor and tanh bounds the reward. This reward encourages the exploratory agent to take transitions where the dense predictor is better than the causal predictor, which usually suggests the learned causal graph is inaccurate.

Lastly, to solve downstream tasks, CDL simultaneously learns a transition dynamics model (including reward function) and uses a planning algorithm depending on that model for action selection, as most MBRL algorithms do. In experiments, they verify that this improves generalization and sample efficiency on the learned dynamics models and the policies for downstream tasks.

# 7.4 Multi-Task RL

Multi-Task RL learning refers to settings where we expect an agent to solve multiple environments1. Further, some works assume that the environments have never been encountered during training, but there exist some invariances across all environments (e.g., the same dynamics, but observation functions with different noise sources) or

Figure 7.3: Causal Dynamics Learning (CDL) for Task-Independent State Abstraction [240].   
![](images/a12669502d6302b248eaadfd16fbc6a7a263c19eeafbca97628d6457fc27c391.jpg)  
(a) Abstraction example. Imagine an environment with two (b) State variables can be split into three doors that the robot can open and go through as well as a types. (a) Complete causal dynamics model. (b) clock on the wall. (a): Standard MBRL dynamics model pre- Split: controllable (green), action-relevant (orange), dict dynamics by unnecessarily using all variables. (b): State and action-irrelevant (blue). The dashed arrow repabstractions learn to omit the clock based on a pre-defined re- resents whether it exists does not affect $s ^ { 5 }$ to be ward but still use a dense model for the remaining variables. action-irrelevant. (c) Causal graph can be split into (c): CDL only keeps necessary dependencies (doors A and B three subgraphs, one for each type of state variable. depend on the action individually) and derives a state abstraction independent from any reward function.

there may exist an adaptation phase with limited exposure to the new task (also referred to as meta-reinforcement-learning [242]).

Causal Curiosity: Sontakke et al. [243] consider the setting in which the environment dynamics depend on hidden parameters [244] that differ across environments or over time. For example, if a body in an environment loses contact with the ground, the coefficient of friction between the body and the ground no longer affects the outcome of any action that is taken. Likewise, the outcome of an upward force applied by the agent to a body on the ground is unaffected by the friction coefficient.

However, in contrast to previous approaches that learn latent task representations, e.g., [91, 245], they aim to recover disentangled representations for the factors, i.e., independent causal mechanisms that affect the outcomes of behaviors in each environment. The motivation is that disentangled embeddings of the causal factors make the changing behaviors interpretable.

Block MDPs: Zhang et al. [246] consider the problem of learning state abstractions that generalize in block MDPs, families of environments in which the observations may change, but the latent states, dynamics, and reward function are the same. By leveraging ideas from IRM (Sec. 3.1.2.1), they propose to learn invariant state abstractions from stochastic observations across different interventions on variables in the observation space (e.g., the background color of 3D-rendered Physics simulation).

The block structure assumption states that each observation $\mathbf { X }$ can uniquely determine its generating state S. Two additional causal assumptions the authors make about the causal structure of the environment are 1) the environment state at time $t$ can only affect the values of the state at time $t + 1$ and the reward at time $t$ ,

and 2) each environment corresponds to an intervention on a single variable in the observation space. For example, in one of their experiments, they intervene on the background color of the environment and set it to a value sampled at random.

Model-Invariant State Abstractions: Tomar et al. [247] introduce model-invariant state abstractions for systematic generalization to unseen states in a single-task setting. These abstractions are built on two ideas: (1) causal sparsity in the transition dynamics over state variables and (2) causal invariance in the learned representations. (1) means that given a set of state variables, each variable only depends on a small subset of those variables in the previous timestep. (2) dictates that given a set of features, the learned representations comprise only those features that are consistently necessary for predicting the target variable of interest across different interventions. Therefore, it likely contains the true causal features and will generalize well to possible shifts in the data distribution.

Schema Networks: Kansky et al. [248] propose Schema Networks, a generative model for model-based reinforcement learning. Their approach relies on Schemas, which are local cause-effect relationships involving one or more object entities. The model‚Äôs knowledge about the class of environments is represented with schemas such that in a new environment, these cause-effect relationships are traversed to guide action selection. Thereby, experience from one scenario can be transfered to other similar scenarios that exhibit repeatable structure and sub-structure. For example, they demonstrate that Schema Networks are able to generalize to variations of the Atari Breakout [249] game with perturbed positions of objects, while model-free baselines fail to do so.

Systematic Generalization: Systematic generalization aims at learning universal (causal) relations of the environment dynamics from interactions with a few environments, such that we can deal with unseen states in a single-task setting, or approximately solve unseen other environment without further interactions in a multi-task setup.

Mutti et al. [250] define the following systematic generalization problem. First, they define a universe: a large, potentially infinite, set $\mathbb { U }$ of environments modeled as discrete MDPs without rewards,

$$
\mathbb {U} := \left\{\mathcal {M} _ {i} = \left(\left(\mathcal {S}, d _ {S}, n\right), \left(\mathcal {A}, d _ {A}, n\right), P _ {i}, \mu\right) \right\} _ {i = 1} ^ {\infty}. \tag {7.5}
$$

The agent‚Äôs goal is to acquire sufficient knowledge to approximately solve any task that can be specified over the universe $\mathbb { U }$ by drawing a finite amount of interactions.

A task is defined as any pairing of an MDP $\mathcal { M } \in \mathbb { U }$ and a reward function $r$ . Solving it refers to providing a slightly sub-optimal policy via planning, i.e., without taking additional interactions.

![](images/bb42ceaf57caa5d16e938d2388583370ef438da59744459a42cd8c2e39328b3f.jpg)  
(a) Latent state space model.

![](images/2350148610b19d5d952eae434d0bfbd3af7943912e1c8ed4cf180fc7126c42a8.jpg)  
(b) Previous transition models.

![](images/e58a185cadf86b5bada12b8b264ff18f0b65878a8623179520b0d18b1a481a8c.jpg)  
(c) VCD transition model.   
Figure 7.4: Variational Causal Dynamics (VCD) [251]. The shown DAGs illustrate one timestep and $K$ environments. The VCD model combines causal discovery with learning transition dynamics, aiming to generalize across $K$ environments by identifying sparse changes in the underlying causal graph. Each latent state variable $z _ { i }$ is represented by a separate conditional distribution $p _ { i }$ that is conditioned on a subset of the previous states and the action at each timestep. In blue, we highlight the intervened mechanism specific to environment $k$ . In black, we denote mechanisms shared across environments.

# Def.: 7.4.1: Systematic Generalization [250]

For any latent MDP $\mathcal { M } \in \mathbb { U }$ and any given reward function $r : S \times \mathcal { A }  [ 0 , 1 ]$ , the systematic generalization problem requires the agent to provide a policy $\pi$ , such that $V _ { \mathcal { M } , r } ^ { \ast } - V _ { \mathcal { M } , r } ^ { \pi } \leq \epsilon$ up to any desired sub-optimality $\epsilon > 0$ .

Since the set $\mathbb { U }$ is infinite, the authors posit the presence of common causal structure underlying the transition dynamics of the universe. This assumption makes the problem feasible and allows them to yield a provably efficient algorithm that achieves systematic generalization with polynomial sample complexity. They verify their algorithm‚Äôs effectiveness on a synthetic universe, where each environment is a person, and the MDP represents how a series of actions the person can take influences their weight and academic performance.

Variational Causal Dynamics: Lei et al. [251] learn a causally factorized latent state-space dynamics model. They sparsity regularization from causal discovery to capture sparse causal structures describing the environment dynamics. The motivation behind their model is that it can adapt to novel environments by intervening on learned latent independent mechanisms without affecting the others.

In contrast to the other approaches discussed in this section, they parameterize the belief over the causal adjacency matrix $\mathbf { G }$ as a random binary matrix. Each entry of that matrix follows a Bernoulli distribution with learned scalar variables. To jointly learn $\mathbf { G }$ as well as the latent intervention masks for each environment, they utilize variational inference and derive an ELBO objective. Fig. 7.4 summarizes their approach.

# 7.5 Off-Policy Policy Evaluation

The purpose of policy evaluation is to measure the expected return $\mathbb { E } _ { \mathfrak { p } ^ { \pi } } [ G ]$ of a target policy $\pi$ , where $G$ denote some return, e.g., the discounted return k=0 evaluation uses sample trajectories D = nhÀÜ iT oi=1,...,N c $\scriptstyle \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 }$ at time step $t$ with discount rate $\mathcal { D } = \left\{ \hat { h } _ { T } ^ { i } \right\} _ { i = 1 , \dots , N }$ $\gamma \in \ [ 0 , 1 ]$ onsisting of logged episodes and reward $R$ . This $\hat { \pmb { h } } _ { T } ^ { i } = \left( \hat { \pmb { x } } _ { 1 } ^ { i } , \hat { a } _ { 1 } ^ { i } , \dots \hat { a } _ { T - 1 } ^ { i } , \hat { \pmb { x } } _ { T } ^ { i } \right)$ generated either by the same policy $\pi$ (on-policy) or by another policy $\mathbf { U }$ (off-policy). When using the latter, we refer to the evaluation as off-policy policy evaluation (OPPE) and the policy used to generated the behavior is called the behavior policy.

In principle, OPPE is an attractive problem with many potential use cases in which online learning is not feasible, e.g., due to cost or ethical constraints on experimentation. From a technical perspective, it can guide policy learning methods to reuse off-policy experiences and find good policies more data-efficiently. However, it is generally difficult because there is a distributional mismatch between the trajectories generated by the target policy and the behavior policy. This mismatch often causes policy evaluation methods to have high variance and slow convergence [206].

Interestingly, Bannon et al. [212] emphasize that the causal inference task counterfactual inference (CFI) and OPPE are two different approaches to similar (under certain conditions identical) problems.

# Causal Perspective: 7.5.1: Off-Policy Policy Evaluation [212]

Observations, interventions and the query variable in CFI directly correspond to off-policy episodes, the target policy, and the expected return, respectively. Similarly, Parbhoo et al. [252] argue that formulating the task of OPPE from a generalized and causal perspective opens the possibility for counterfactual or retrospective off-policy evaluation at the level of individual units (e.g. patientlevel) of the population.

Observing this correspondence begs the question if the techniques developed in both fields are complementary. In the following, we discuss methods that answer in the affirmative.

Counterfactual policy evaluation: Buesing et al. [253] present Counterfactually-Guided Policy Search (CF-GPS), which uses counterfactual inference (Sec. 2.3.2) for off-policy evaluations (CF-PE) in the setting of model-based (off-policy) policy evaluation (MB-PE). In this setting, we want to evaluate the policy on synthetic data sampled from a model $\mathcal { M }$ , i.e., we can estimate the expected return $\mathbb { E } _ { p ^ { \pi } } \left[ G \right]$ by sampling the scenarios $\mathbf { U } \sim p _ { u }$ (all aspects of the environment that cannot be influenced by the agent, e.g., the initial state distribution), and then simulating a trajectory $\tau$ from the functions $f _ { i }$ and computing its return.

Assuming that we use an SCM $\mathcal { M } _ { \mathrm { S C M } }$ instead of a statistical model $\mathcal { M } _ { \mathrm { S T } }$ , Buesing et al. [253] show that CF-PE should be less biased than MB-PE. First, they

![](images/7801192ab687ca614992021f5d560875b838d8bd43b3c6f067a65ed4aaad58f0.jpg)  
Figure 7.5: Representing a POMDP as an SCM [253]: We denote initial state $\mathbf { U } _ { s _ { 1 } } = \mathbf { S } _ { 1 }$ , states $\mathbf { S } _ { t }$ and histories $\mathbf { H } _ { t }$ . The mechanism that generates the actions $A _ { t }$ is the policy $\pi$ . Scenarios U summarize immutable aspects, some of which are observed (grey), some not (white).

show that one can represent any given partially-observable Markov decision process (POMDP) under a policy $\pi$ as SCM $\mathcal { M }$ over trajectories $\tau$ (for details on POMDPs, we refer the reader to [206]).

# Causal Perspective: 7.5.2: POMDPs [253]

We can represent any given Partially-Observable MDP (POMDP) by an SCM $\mathcal { M }$ over trajectories $\tau$ in the following way. By exploiting ideas related to the reparameterization trick commonly used in variational inference (e.g., VAEs) [104], we can express all conditional distributions, e.g. the state transitions $p \left( \mathbf { S } _ { t + 1 } \mid \mathbf { S } _ { t } , A _ { t } \right)$ , as deterministic functions with independent noise variables $\mathbf { U }$ , such as $\mathbf { S } _ { t + 1 } = f _ { s t } \left( \mathbf { S } _ { t } , A _ { t } , \mathbf { U } _ { s t } \right)$ . The DAG of $\mathcal { M }$ is shown in Fig. 7.5.

Based on this representation, they then prove that counterfactual inference (Sec. 2.3.2) yields an unbiased estimator of $p ^ { \mathrm { d o } ( I ) } ( { \pmb x } )$ , the POMDP‚Äôs distribution over trajectories after intervention $I$ took place. In contrast, any bias in a statistical model $\mathcal { M } _ { \mathrm { S T } }$ propagates from $p ^ { \prime \prime }$ to the estimate $\mathbb { E } _ { p ^ { \pi } } [ G ]$ .

Algorithm 2 summarizes the CF-GPS procedure. Given data $\mathcal { D } \sim { \mathfrak { p } } ^ { u }$ and assuming no model mismatch, i.e. ${ \mathfrak { p } } ^ { u } = p ^ { u }$ , we can regard the task of off-policy evaluation of $\pi$ as a counterfactual query with data $\hat { h } _ { T } ^ { i }$ , intervention $I ( \mathbf { U } \to \pi )$ and query variable $G$ (reward). Then, the difference to MB-PE is that instead of sampling from the prior $\mathbf { \boldsymbol { u } } ^ { i } \sim p ( \mathbf { \boldsymbol { u } } )$ , we sample scenarios from the posterior $\mathbf { } u ^ { i } \sim p ^ { \mathbf { } u } \left( \pmb { u } \mid \hat { h } _ { T } ^ { i } \right)$ where we inferred scenarios $U$ in hindsight from given off-policy data $\scriptstyle { \hat { x } } _ { o }$ . Then, we evaluate the agent on these specific scenarios.

Sampling from the posteriors $\begin{array} { r } { N ^ { - 1 } \sum _ { i = 1 } ^ { N } p ^ { \pmb { u } } \left( \pmb { u } \mid \hat { h } _ { T } ^ { i } \right) } \end{array}$ has access to strictly more information than the prior $p ( \pmb { u } )$ by taking into account additional data $\hat { h } _ { T } ^ { i }$ (Sec. 2.3.2). This semi-nonparametric distribution can help to de-bias the model by effectively winnowing out parts of the domain of $\mathbf { U }$ which do not correspond to any real data.

After demonstrating that CF-PE outperforms MB-PE in partially observed gridworld settings, they conclude that one may expect CF-PE to outperform MB-PE when the transition and reward kernels $f _ { s t }$ are accurate models of the environment dynamics, but the marginal distribution over the noise sources $P _ { U }$ is difficult to model.

Algorithm 2 Counterfactual Policy Evaluation [253]   
// Counterfactual inference (CFI)  
1: procedure CFI(data $\hat{x}_o$ , SCM $\mathcal{M}$ , intervention $I$ , query $\mathbf{X}_q$ )  
2: $\hat{\boldsymbol{u}} \sim p(\boldsymbol{u}|\hat{\boldsymbol{x}}_o)$ ‚ñ∑ Sample noise variables from posterior  
3: $p(\boldsymbol{u}) \gets \delta(\boldsymbol{u} - \hat{\boldsymbol{u}})$ ‚ñ∑ Replace noise distribution in $p$ with $\hat{\boldsymbol{u}}$ 4: $f_i \gets f_i^I$ ‚ñ∑ Perform intervention $I$ 5: return $\boldsymbol{x}_q \sim p^{\mathrm{do}(I)}(\boldsymbol{x}_q|\hat{\boldsymbol{u}})$ ‚ñ∑ Simulate from the resulting model $\mathcal{M}_{\hat{x}_o}^I$ 6: end procedure  
// Counterfactual Policy Evaluation (CF-PE)  
7: procedure CF-PE(SCM $\mathcal{M}$ , policy $\pi$ , replay buffer $\mathcal{D}$ , number of samples $N$ )  
8: for $i \in \{1, \dots, N\}$ do  
9: $\hat{\boldsymbol{h}}_T^i \sim D$ ‚ñ∑ Sample from the replay buffer  
10: $g_i = \mathrm{CFI}(\hat{\boldsymbol{h}}_T^i, \mathcal{M}, I(\mathbf{U} \to \pi), G)$ ‚ñ∑ Counterfactual evaluation of return  
11: end for  
12: return $\frac{1}{N} \sum_{i=1}^{N} g_i$ ‚ñ∑ Return averaged counterfactual return  
13: end procedure

Following a similar methodology, [254] use counterfactually-generated trajectories to highlight episodes where the target and behavior policy returns differ substantially. They interpret this as a helpful procedure for off-policy ‚Äúdebugging‚Äù in high-risk settings, such as healthcare. In contrast to [253], who use counterfactuals to approximate draws from the interventional distribution, they treat the counterfactual distribution as the primary object of interest and demonstrate their method‚Äôs utility in a sepsis management simulation.

# 7.5.1 Unobserved Confounding

In OPPE, the caveat of not using the current policy for evaluation is that we almost inevitably face unobserved confounders (Sec. 2.5) that causally affect the behavior policy. Kallus and Zhou [255] illustrate the problem with the following clinical example: Suppose we want to compare the efficacy of different drugs. During normal clinical practice, we observe the outcomes (rewards) of those prescribed the drug. A drug may appear less clinically effective if those prescribed it were more unhealthy to begin with and, therefore, would have had less successful outcomes regardless. Conversely, if the drug was given only to patients who would benefit most, it could be mistakenly regarded as beneficial to everyone. While these issues can potentially be addressed by controlling for more factors that may have affected treatment decisions, they can never be entirely eliminated. Healthcare databases are often incomplete about medical histories, patient severity notes, etc., so they are especially susceptible to unobserved confounding.

To this end, Kallus and Zhou [255] study the setup of confounding-robust policy improvement (including OPPE) to account for possible unobserved confounding (UC). They develop a method for minimizing the worst-case estimated regret of a candidate policy against a baseline policy over a set of propensity weights that control the extent of UC. As a result of their theoretical analysis, they obtain generalization

![](images/dd476d3f403fdafd346d555d6b56577fe14de8383fb7e1b7e30fa9ed2b3097e3.jpg)  
(a) IVR.

![](images/145bbe3a1a9c375fd0c03e1811c446f18cdd6cb2bffe738cd8640b9514a118f2.jpg)  
(b) LSTD.

![](images/95ca77e9aaf312ffee8244a5a700cac8119daa30097ad0e5cd45b9c6010b5aa6.jpg)  
(c) Non-Linear Q-Function.   
Figure 7.6: Causal DAGs revealing the relationship between Instrumental Variable Regression (IVR), Least-Squared Temporal Difference (LSTD) and non-linear Qfunctions [265].

guarantees that ensure that their policy will be safe when put into practice. Further, it will yield the best possible uniform control on the range of all possible population regrets consistent with UC‚Äôs extent.

Infinite-Horizon: Kallus and Zhou [256] and Bennett et al. [257] consider the above setup extended to the infinite-horizon setting, as e.g., commonly considered in continuous control of physical systems [258] or quantitative trading [259]. Similarly, Namkoong et al. [260] study the case when UC occurs only at a single time step, as when an expert makes an initial decision based on unrecorded information and then follows a set of protocols based on well-recorded observations.

POMDPs: Tennenholtz et al. [261] consider OPPE in partially-observable MDP (POMDP) environments, where the unobserved variables may have confounding effects, motivating them to propose the decoupled POMDP model, which is a class of POMDPs for which observed and unobserved variables are distinctly partitioned. Bennett and Kallus [262] utilize the framework of proximal causal inference to reveal POMDP settings where identification of the target policy value is possible. Further, they construct semi-parametrically efficient estimators for these settings.

Combining offline and online data: Gasse et al. [263], Wang et al. [264] focus on using offline data to warm start online RL. Gasse et al. [263] suggests learning a latent-based transition model that explains both the interventional and observational regimes and then inferring the standard POMDP transition model. Wang et al. [264] propose confounded MDPs, which naturally captures both the offline and online setting as well as their mismatch due to confounding. They then construct deconfounding algorithms in the episodic setting with linear function approximation.

Relationship to instrumental variable regression: Many OPPE rely on estimates of the state-action value (Q-) function by minimizing the mean squared Bellman error. Chen et al. [265] reveal that this strategy leads to a regression problem with an unobserved confounder between the inputs and the output noise: Fig. 7.6 displays that the causal relationships between instrumental variable regression, the least-squared temporal difference (linear Q-function) and non-linear Q-functions are equivalent.

# 7.6 Imitation Learning

Imitation learning (IL) aims to learn control policies directly from examples of demonstrations provided by human experts [266]. The motivation behind it is to remove the need for extensive interaction with the environment during policy learning and/or designing reward functions specific to the task. Cameras and sensors of today collect and transmit vast amounts of data quickly, and processors with significant computing power are getting quicker in mapping the sensory inputs to actions. Hence, developing real-time perception and reaction models assisted through IL opens up many potential applications, such as humanoid robots, self-driving vehicles, or human-computer interaction systems.

One challenge in IL is confounding. In the following, we consider confounded settings in which it is difficult for the imitator to recover the expert‚Äôs performance. At their core, these issues stem from similar roots as the ones we have seen in the subsection on OPPE (Sec. 7.5), where one aims at learning policies based on trajectories that were not controlled by the agent itself but an external entity. However, as the goal of IL is different from OPPE, different remedies have been developed.

By and large, causal imitation learning (CIL) aims at solving confounding issues. The following works aim at deconfounding observed expert trajectories such that imitator policies achieve similar performances or define necessary and testable conditions for such.

Causal Confusion: de Haan et al. [267] study the setup of causal confusion: in which the inputs to the expert policy are fully observed; however, the mechanisms of the expert policy are latent, i.e., it is not known which of the observed input variables are actual causes of the expert‚Äôs actions and which are not (i.e., ‚Äúnuisance variables‚Äù). Here, the expert and imitator observe the same contexts, but the causal diagram is not available to the imitator.

The authors show that in this setting, the phenomenon of causal misidentification (CM) can occur: if there is a shift between inputs causing the expert‚Äôs actions and the imitator‚Äôs actions (e.g., when the latter relies on nuisance variables), then access to more data can yield worse performance. In other words, CM happens when cloned policies fail by misidentifying the true causes of expert actions, and training them on more observations can exacerbate the performance.

Consider Fig. 7.7 as an example: we aim to train a neural network to drive a car. The model input for scenario A is an image of the dashboard and windshield. The input to the model (with identical architecture) is the same image, but the dashboard is masked out. Although both cloned policies achieve low training loss, model B performs well when tested on the road, while model A does not. The dashboard has an indicator light that comes on immediately when the brake is applied, and model A wrongly learns to apply the brake only when the light is on. Even though the brake light is an effect of braking, model A could achieve low training error by misidentifying it as a cause.

![](images/f922993d61b25f81bfa2b4a927bd02ff6b3bb0388c3be865018ffbfbedaa2a0d.jpg)

![](images/9aa37f5a2b89de47258682bcf5d384b65398d10da3eace863346695bef7d86af.jpg)  
Figure 7.7: Causal misidentification: more data can yield worse imitation learning performance [267]. There exist a spurious association between braking and the car‚Äôs brake indicator. A naive behavior-cloned model ( ${ \mathcal { F } } _ { A }$ ) relying on the braking indicator will apply the brake only when the brake light is on. This model works well in the left Scenario A; however, if the braking indicator is unavailable (Scenario B, right), the imitator will fail. A better model ( $\mathcal { F } _ { B }$ ) decides whether to brake by attending to the pedestrian.

To remedy these issues, the authors suggest finding the true causal model of the expert‚Äôs actions. A two-stage pipeline can automate this: in stage 1, we jointly learn policies corresponding to various causal graphs; in stage 2, we perform targeted interventions to efficiently search the hypothesis set for the correct causal model.

Tien et al. [268] provide a systematic study of causal confusion in the context of learning reward functions from human inputs that take the form of pairwise preferences or rankings. Specifically, they demonstrate across three different robot learning tasks that causal confusion over the true reward function occurs, even with large numbers of pairwise preferences over trajectories. The authors investigate different factors affecting causal confusion: type of training data, reward model capacity, and the preference training data generation mechanism. One of their conclusions is that while human demonstrations seeking to be pedagogic lead to better sample efficiency, all the preference data collection methods were prone to causal confusion.

Self-Delusion: Ortega et al. [269] demonstrate that the common perception that popular sequence models ‚Äúdo not understand the cause and effect of their actions‚Äù is the result of conditioning on actions rather than treating them as causal interventions. The reason is that the model update differs depending on whether the collected data originated from within the model (i.e., from actions) or outside of it (i.e., from a third person whose policy we do not know), and mixing them up results in incorrect inferences.

To give intuition, the authors give a minimal example, as illustrated in Fig. 7.8. Consider a prize-or-frog problem, where there are two boxes (1&2), one containing a prize and the other a frog, respectively. The objective is to open the box containing the prize. For simplicity, let us assume that the two configurations are equiprobable. The true DGP has a joint distribution $p ( \theta , A , O )$ over the box configuration $\theta$ , the chosen box $A ( 1 { \mathrm { ~ o r ~ } } 2 )$ , and the observed content $O$ (¬±1 reward).

Suppose we learn a probabilistic model from data generated by an expert who opens the correct box when told where the prize is. Crucially, unlike the expert, we would not observe $\theta$ . The source of the delusion can be seen by comparing the two posterior beliefs $p ( \theta \mid a , o )$ and $p ( \theta \mid \operatorname { d o } ( a ) , o )$ over the task parameter, that is, where the past action is treated as a condition and an intervention, respectively (differences are

![](images/46190dba4ed1e229c7d9dad730f0f6d6ba83c77d6ec8f00c083ee05168883ba8.jpg)

![](images/0dbff20be86f86753b28fdbd18709d2a157628a9cb004c7a551ba21a7dd4c1ab.jpg)

![](images/49317a17ca34c978a0f7bf896df75fecf1cfb59d404cccd4b34fa6f6d8f78723.jpg)  
Figure 7.8: The Self-Delusion problem illustrated through a prize-or-frog game [269]: Panel (a) illustrates the self-delusion problem: conditioning on the self-generated action leads to wrong inferences about $O$ , since $A$ and $O$ are confounded by $\theta$ . Panel (b) shows that treating the self-generated action as a causal intervention circumvents the self-delusion because no information can flow backward from $A$ into $\theta$ . Panel (c) corresponds to the fully observable case. When $\theta$ is observed, conditioning or intervening on the self-generated action $A$ leads to the same prediction over $O$ .

highlighted):

$$
p (\theta \mid a, o) = \frac {p (\theta) p (a \mid \theta) p (o \mid \theta , a)}{\sum_ {\theta^ {\prime}} p \left(\theta^ {\prime}\right) p \left(a \mid \theta^ {\prime}\right) p \left(o \mid \theta^ {\prime} , a\right)} \tag {7.6}
$$

$$
p (\theta \mid \operatorname {d o} (a), o) = \frac {p (\theta) p (o \mid \theta , a)}{\sum_ {\theta^ {\prime}} p \left(\theta^ {\prime}\right) p \left(o \mid \theta^ {\prime} , a\right)}. \tag {7.7}
$$

Here, we can see that the intervention causes the dismissal of the evidence $\cdot$ produced by the self-generated action $A = a$ .

Unobserved confounding: Zhang et al. [270] tackle the setting in which a causal graph is provided, but unobserved confounders (UC) affect both actions and outcomes of the expert demonstrations. This means that the expert‚Äôs input observation may differ from the ones available to the imitator. For instance, self-driving cars rely solely on cameras or lidar, ignoring the auditory dimension. However, most human demonstrators can exploit this data, especially in dangerous situations (car horns, screeching tires) [271].

To formalize and study this problem in single-stage settings, they introduce partially observable SCMs, a particular type of SCMs that explicitly allows one to model the unobserved nature of some endogenous variables. Then, they show that the problem of imitability is orthogonal to identifiability and introduce a data-dependent, graphical criterion for determining whether imitating an expert‚Äôs performance is feasible. Based on this criterion, they provide an algorithm that checks whether there are instruments in the data, permitting it to satisfy the graphical imitability criterion. Finally, for the case of the criterion being met, they propose a procedure for estimating an imitating policy.

Sequential data: Kumor et al. [271] extend Zhang et al. [270]‚Äôs results to sequential settings, where the imitator must make multiple decisions per episode. To this end, they introduce a generalized backdoor criterion that allows one to learn imitating policies across a sequence of states and actions. They prove that their criterion is necessary for imitability.

![](images/474dba067fcbea08bcb5c27f918c4b1befdf835cfe79debf9c1384c7a87613c5.jpg)  
Figure 7.9: Problem setup of Invariant Causal Imitation Learning [272]. The observations are decomposable into environment-invariant features $\mathbf { \Delta } _ { s t }$ and environment-specific $\mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } } \mathbf { \cdot } \mathbf { \nabla } _ { \mathbf { \eta } }$ that encapsulate spurious associations with the actions. The goal is to recover $\mathbf { \Delta } _ { s t }$ such that the learned policy $\pi ( \cdot \mid s _ { t } )$ generalizes well to new environments.

Multiple Environments: Bica et al. [272] consider the imitation learning setting with multiple environments. Their goal is to learn a policy that matches the expert behavior in all possible environments from a class of environments that share a certain structure for the observations and the transition dynamics, similarly, as we have discussed in Sec. 7.4. Fig. 7.9 visualizes the causal graph of their setup.

To incentivize the learned imitation policy to stay within the distribution of the expert‚Äôs observations, the authors propose to minimize the energy of the next observation obtained by following $\pi$ given the current observation. This way, they assign a low loss to the imitation policy for staying within high-density areas of the expert‚Äôs occupancy measure and a high loss for straying from it.

Temporally Correlated Noise: Swamy et al. [273] hypothesize that in many imitation learning settings, recordings of the expert may be corrupted by an unobserved confounder that is temporally correlated, i.e., temporally correlated noise that is not independently distributed across timesteps. Table 7.3 illustrates settings where such noise can occur. The first row, for example, considers a quadcopter pilot who might have been flying under the persistent wind. An imitation learner will likely reproduce these deviations and perform poorly in test-time environments with different weather conditions. This setup is related to Sec. 7.6 but makes stronger assumptions on the unobserved confounder (additive noise).

To this end, Swamy et al. [273] leverage instrumental variable regression (IVR) to deconfound the data (Sec. 11.2.1.2). Put simply, IVR leverages an instrument, a source of random variation independent of the confounder, to deconfound inputs to a model via conditioning on the instrument.

The key idea behind their approach is to frame past states as instruments to break the spurious association between states and actions caused by the unobserved confounder. The motivation behind this is that historical transitions are unaffected by future confounding [274].

They formalize the TCN setup by an SCM and the target estimand as the interventional effect of the expert policy $\mathbb { E } \left[ \pi _ { E } ( \pmb { \mathscr { s } } ) \ | \ \pmb { \mathscr { s } } \right] = \mathbb { E } [ a \ | \ \mathrm { d o } ( \pmb { \mathscr { s } } ) ] \neq \mathbb { E } [ a \ | \ \pmb { \mathscr { s } } ]$ based on

Table 7.3: Examples of Temporally Correlated Noise in Causal Imitation Learning [273]: In these settings, spurious associations (Sec. 2.5) between states and actions can lead to inconsistent estimates of the expert‚Äôs policy.   

<table><tr><td>Setting</td><td>State</td><td>Expert Action</td><td>Observed Action</td><td>Confounder</td></tr><tr><td>Quadcopter Flying [275]</td><td>Position</td><td>Intended Heading</td><td>Actual Heading</td><td>Persistent Wind</td></tr><tr><td>Product Pricing [276]</td><td>Demand</td><td>Profit Margin</td><td>Price</td><td>Raw Materials Cost</td></tr><tr><td>ICU Treatment [277]</td><td>Symptoms</td><td>Intent to Treat</td><td>Patient Treated</td><td>Comorbidity</td></tr><tr><td>Shared Autonomy [273]</td><td>User State</td><td>Intended Action</td><td>Executed Action</td><td>Assistance</td></tr></table>

![](images/e5bbdccfff908bf79fc2fbe53182eacc000250511eb8bc190d7ba067407c42ba.jpg)  
Figure 7.10: Causal Imitation Learning under Temporally Correlated Noise (TCN) [273]: (a) Unobserved confounders (e.g. wind) induce spurious associations between states and expert actions. Non-causal imitation learning approaches like behavioral cloning can learn policies that rely on this noise, leading to poor test-time performance. (b) Causal DAG illustrating how the TCN confounder ${ \pmb u } _ { t }$ affects both the input ( $\mathbf { \nabla } _ { s \ t }$ ) and output ( $a _ { t }$ ). (c) Using instrumental variables Sec. 11.2.1.2, We can re-simulate state transitions from a past state, producing fresh samples ( $\widetilde { s _ { t } }$ ). We can then regress from these sampled states to observed expert actions to recover the expert‚Äôs policy as the noise on inputs and outputs is no longer correlated.

IVR. To estimate that estimand, they propose two approaches inspired by generative modeling and game-theoretic IVR approaches. The former utilizes access to a simulator, while the latter can be run entirely offline. Further, they derive performance bounds, which assume that the same distribution of TCN affects the learner at test time.

# 7.7 Credit Assignment

One key issue in RL is credit assignment (CA), the task of understanding the causal relationship between actions and rewards and determining to what extent an outcome is due to external, uncontrollable factors [278, 279]. That means that a correct value function allows us to disentangle the relative aspects of ‚Äúskill‚Äù and ‚Äúluck‚Äù in an agent‚Äôs performance. However, unfortunately, because of partial observability, scale, long time horizons, or increasing number of actions, each action taken by an agent may have only a vanishing effect on the outcome, making it increasingly difficult to learn proper value functions.

In the following, we look at methods that assign credits by measuring the causal influence of an action on the observed reward (Sec. 2.7).

# 7.7.1 Causal influence detection

Seitzer et al. [280] postulate that an agent can only influence its environment in certain situations. If, for instance, a robotic arm is placed in front of an object of interest on a table, the object can only be moved once contact is made between the robot and object. Some situations have an immediate causal effect, while others do not.

The authors argue that causal action influence (CAI), which quantifies the causal influence of states, can help guide the learning algorithm to seek out states with stronger (predicted) influence, even enabling it to discover helpful behavior in the absence of task-specific rewards. To derive CAI, they introduce a causal model, which allows them to quantify whether the agent is in control given its state by conditional mutual information (CMI), which they learn through neural network models. Based on their experimental results, they conclude that CAI improves sample efficiency and performance by (i) better state exploration through an exploration bonus, (ii) causal action exploration, and (iii) prioritizing experiences with causal influence during training.

# 7.7.2 Counterfactual credit assignment

Mesnard et al. [279] propose counterfactual credit assignment (CCA), a framework that uses the notion of counterfactuals to deal with the credit assignment issue. They propose to implicitly perform counterfactual policy evaluation (Sec. 7.5) by conditioning value functions on future event embeddings, which learn to extract relevant information from a trajectory. They then use the estimated counterfactual returns to form unbiased and lower variance estimates of the policy gradient by building future-conditional critics.

While conventional state-action functions estimate the return for all actions, they do so by averaging all possible futures. In contrast, CCA estimates the return for different actions while keeping many external factors constant between the return and the counterfactual estimate. This makes CCA potentially finer-grained and improves data efficiency in environments with complex credit assignment structures.

The critical ingredient for the FC-PG algorithm is to learn the hindsight statistics $\Phi$ , which embed a trajectory while not precluding any action $a$ , which was possible for $\pi$ from having potentially produced $\Phi$ . For example, $\Phi _ { t } = A _ { t }$ does not satisfy this condition, because it precludes any action $a \neq A _ { t }$ from having produced $\Phi _ { t }$ . The authors discuss several options to learn $\Phi$ and mainly focus on using a hindsight network $\varphi$ which extracts $\Phi$ directly from the observed state-action-reward triplets, i.e., $\Phi _ { t } = \varphi \left( \mathbf { X } , \boldsymbol { A } , \boldsymbol { R } \right)$ .

By establishing a connection to counterfactually-guided policy search [253], introduced in Sec. 7.5, the authors explain that the CCA estimator can be understood as sidestepping modeling the environment by discarding information from the whole

![](images/b39bb52a1994de8cb9572f17e3d0f2b2bb1f9d08a565c130bde1eb30aafdf3a3.jpg)  
Figure 7.11: Pool example motivating Counterfactual Data Augmentation (CoDA) [286]. Knowing the local causal structure allows us to mix and match factored subprocesses to form counterfactual samples based on three factual samples. We reject the first proposal because one of its factual sources (the blue ball) is not localized. Due to the swapped proposal not being locally factored in, the third proposal is rejected. Accepted proposal two may be used as additional training data for reinforcement learning agents.

trajectory $\tau$ which would require a model, and left with still useful information $\Phi _ { t }$ with which counterfactuals can be computed in a model-free way.

# 7.7.3 Multi-Agent Reinforcement Learning

Multi-agent systems consist of autonomous, distributed agents that interact with each other in a shared environment [281, 282]. Each agent strives to accomplish an assigned goal, and the interplay between agents can vary depending on the task, such that agents cooperate or act competitively to beat the competition.

Here, the credit assignment problem arises in cooperative settings when the collection of the agents‚Äô actions generates only a global, shared reward, making it difficult for each agent to deduce its contribution to the team‚Äôs success [283, 284].

Additionally to the shared rewards given to all agents, [285] proposes that agents be rewarded for influencing the actions of other agents. Agents receive this intrinsic reward in addition to their immediate reward, reflecting the effect one agent has on another. They refer to it as ‚Äúcausal influence‚Äù, similarly as Seitzer et al. [280] in the single-agent setup discussed in Sec. 7.7.1. Using counterfactual reasoning, an agent simulates possible actions and determines the effect they would have had on another agent‚Äôs behavior at every time step. Actions that significantly affect the other agent‚Äôs behavior are considered highly influential and rewarded. Generally, this added reward helps agents learn interpretable communication protocols and obtain higher collective rewards than baselines, which sometimes fail to learn completely.

# 7.8 Counterfactual Data Augmentation

The following techniques apply the idea of counterfactual data augmentation (CFDA) (Chapter 4) to RL trajectories.

Local Causal Models [286]: Consider a game of billiards, as illustrated in Fig. 7.11: each ball can be seen as its own physical process. Before the opening break, due to

![](images/d07cd1f630d8e6d8a917c8871eab031d5c9274a65c83efac154dbbc3ad1ea67b.jpg)  
ÔºàaÔºâGoalrelabeling

![](images/c3e9a4126196c5037da20c827d2ae19d6d9ee41e8085e63ccd7309c6edd7a9be.jpg)  
ÔºàbÔºâVisualaugmentation

![](images/d9dab778b0288c14ff89270b1cfd5f29252d1ee2f9d3222e096cc2843bc3f034.jpg)  
ÔºàcÔºâDyna Ôºàmodel-based)

![](images/b157c4d958c6983c1b3524e48c04fb27996a08ab22834a9c80d181e2e65d5369.jpg)  
(d) Counterfactual Data Augmentationby swapping locally independent subspaces (ours)   
Figure 7.12: Existing RL data augmentation techniques can be interpreted as specific instances of CoDA [286]: Orange nodes are relabeled, exogenous noise variables omitted for clarity. (a) Goal relabeling [287], including HER [288], augments transitions with counterfactual goals. (b) Visual feature augmentation [289, 290] changes visual features $S _ { t } ^ { V }$ (e.g., lighting, camera positions, etc.) that do not impact the physical state $S _ { t + 1 } ^ { P }$ . (c) Dyna [291], including MBPO [292], augments observed states with new actions and re-samples the next state using a learned dynamics model. (d) Given two transitions that share local causal structures, CoDA swaps connected components to form new transitions.

their initial placement, each ball has a non-zero chance of colliding with the others. Hence, to predict the expected outcome of the opening break, we require a transition dynamics model that considers all balls. However, besides the initial timestep, most interactions between balls remain sparse. In other words, only a small subset of all balls are involved at most timesteps.

Pitis et al. [286] take advantage of the fact that during the time between their interactions (locally), the subprocesses are causally independent: they propose a counterfactual data augmentation technique that is compatible with any agent architecture and does not require a forward dynamics model. By inferring whether or not local interactions occur, they swap factorized subspaces of observed trajectory pairs whenever two trajectories have the same local factorization.

The key idea behind CoDA is to leverage the principle of independent mechanisms (Def. 2.3.4). Assume that we can decompose state and action space into multiple subspaces, e.g., represented through subgraphs ${ \mathcal { G } } _ { i } , { \mathcal { G } } _ { j } \subset { \mathcal { G } }$ , where $\mathcal { G }$ is the causal DAG of the global transition dynamics. Then, the causal mechanisms represented by $\mathit { G _ { i } } , \mathit { G _ { j } }$ are independent when $\mathcal { G } _ { i }$ and $\mathcal { G } _ { j }$ are disconnected in $\mathcal { G }$ . In other words, when the global dynamics governed by $\mathcal { G }$ are divisible into two (or more) connected components, we can reason about each subgraph as an independent causal mechanism. Existing RL data augmentation techniques rely on similar global independence relationships, as illustrated in Fig. 7.12.

Personalized Policies [293]: Lu et al. [293] propose a CFDA method to tackle environment heterogeneity by personalized policies, e.g., for healthcare settings, where patients may exhibit different responses to identical treatments. Similar to CF-GPS (Sec. 7.5), they formalize the transition dynamics process as an SCM. Then, similar to methods in Sec. 7.3, they learn that SCM including its structural functions and exogenous noise variables (Sec. 2.3), they use Bidirectional Conditional GANs [294].

The authors propose two algorithms, differing in whether we assume latent environment heterogeneity in the data or not. If so, they explicitly include an environment variable in the causal system to consider variability across observations.

![](images/f0cb462888ad5e57e916315c2b5d6367cf415f9f3ac15e345c8d0eb83c38c262.jpg)  
Figure 7.13: Example of a Causal Influence Diagram (CID) in the context of a content recommendation system [301]. The goal of a content recommendation system is to choose posts that will maximize the user‚Äôs click rate. However, the system‚Äôs designers prefer the system not to manipulate the user‚Äôs opinions to obtain more clicks.

# 7.9 Agent Incentives

The development of artificial general intelligence (AGI), systems that equal or exceed human intelligence in a wide variety of cognitive tasks, raises serious safety concerns [295, 296]. The key concern underlying AGI safety research is that these systems may produce autonomous agents much more intelligent than humans, which consequently pursue goals that conflict with our own. One way to take precautions against potential catastrophes caused by AGIs is to align their incentives with ours, also referred to as AI alignment [297, 298].

Many agent incentive concepts rely on causal relationships among variables influencing the agent‚Äôs utility. For example, Miller et al. [299] postulate that classifiers designed to incentivize improvement necessarily require to perform causal inference. In other words, the authors show that any type of agent that strategically adapts performs causal modeling ‚Äúin disguise‚Äù.

In the following, we follow Everitt et al. [300] and summarize recent progress on formalizing agent incentives causally.

# 7.9.1 Causal Influence Diagrams

One formalism for studying agent incentives is a causal influence diagram (CID) [301, 302, 303], a causal DAG with decision ( $\sqcup$ ) and utility ( $\diamond$ ) variables. After defining a CID, one can analyze incentives through graphical criteria, e.g., the value of information (VoI) [304], which measures which variables a decision-maker would benefit from knowing before making a decision.

Fig. 7.13 shows an example: Imagine a content recommendation engine recommending a series of posts (decision node) to a user. For the algorithm designers to maximize the number of clicks (utility node), the content should be tailored to the user‚Äôs interest (structure node). They do not, however, want the algorithm to use provocative content to manipulate the user‚Äôs opinion (structure node). Evans and Kasirzadeh [305] refers to this problem as user tampering. Using a simulation study, the researcher demonstrates that a Q-learning algorithm learns how to exploit its opportunities to polarize users with its early recommendations to have more consistent success with later recommendations that cater to that polarization.

![](images/3b3509ce5ddaad8f911eaf4477a68f35e3198243213c17ecc7257d7837993095.jpg)  
(a) SCIM

![](images/3ca2ba96dc526467f10126e316550fd3563908dd03ef5bf84b6501ca2a51ed1b.jpg)  
(b) SCM

Figure 7.14: An example of a Structural Causal Influence Model (SCIM) in a content recommendation scenario [301]. The system aims to select posts that maximize click-through rates without manipulating user opinions. (a) Displaying political or apolitical posts $D$ will affect the user‚Äôs opinion $O$ . $D$ and $O$ influence the user‚Äôs clicks $U$ . (b) Given a policy, the SCIM becomes an SCM. (c) We can use this SCM to estimate counterfactuals. For example, the counterfactual $U _ { O _ { d } }$ represents the number of clicks if the user has the opinions they would arrive at after viewing apolitical content.   
![](images/7643e71bda61b70543d373483d74a51045bbedafeb2a0cf8bc402b0d308a400b.jpg)  
exogenous node structural node $\bigcirc$ intervened node decision node utility node

(c) SCM with counterfactual

More generally, we define a CID as follows.

# Def.: 7.9.1: Causal Influence Diagram [301, 302, 303]

A causal influence diagram (CID) is a directed acyclic graph $\mathcal { G }$ with vertices $\nu$ partitioned into $\mathcal { V } = \{ \mathcal { X } , \mathcal { D } , \mathcal { U } \}$ with structure nodes $\mathcal { X }$ , decision nodes $\mathcal { D }$ and utility nodes U. Utility nodes have no children. Edges into decisions indicate what information is available at the time of the decision, so we call them information links.

Next, to define incentive concepts, Everitt et al. [301] use a structural causal influence model (SCIM) [306], which is similar to an SCM (Def. 2.3.1), but not equivalent. There are three key differences: (i) its structural functions are only defined for non-decision endogenous variables, (ii) the vertices are partitioned as described in Def. 7.9.1, and (iii) it holds that the utility variable domains are a subset of the real numbers, i.e., $\mathcal { U } \subset \mathbb { R }$ .

Returning to the content recommendation example, Fig. 7.14 illustrates its SCIM. By specifying a policy $\pi$ , which comprises a structural function for decision nodes $\mathcal { D }$ , we turn a SCIM into an SCM, allowing us to use the standard definitions of causal interventions and counterfactuals.

Being equipped with CIDs and SCIMs, we can now analyze agent incentives and other safety-related concerns.

# 7.9.2 Incentive Concepts

We summarize five incentive concepts informally and direct readers interested in more details to Everitt et al. [301].

Materiality [307]: : Which observations are material for optimal performance? Nodes can be identified as immaterial based on the graphical structure, e.g., if they are $d$ -separated (i.e., conditionally independent) from the utility nodes.

Value of Information [304]: Generalization of materiality to unobserved nodes to assess which variables a decision-maker would benefit from knowing before making a decision.

Response Incentive [301]: An alternative generalization of generalizing a material observation from the perspective of it being one that influences optimal decisions. Under this interpretation, variables that have a response incentive are the ones that influence the optimal agent‚Äôs decisions.

Value of Control [308]: Can the agent benefit from setting a variable‚Äôs value? More specifically, can the attainable utility be increased by letting the agent decide the structural function for the variable?

Instrumental Control Incentive [301]: What is the agent interested in and able to control? Would that choice matter if the agent chooses decision $D \in \mathcal { D }$ to influence $X \in { \mathcal { X } }$ independently of how $D$ influences other aspects of the environment? Everitt et al. [301] call it an instrumental control incentive because the control of $X$ is a means to achieve utility.

# 7.9.3 User Interventions

Sometimes it is desirable to constrain or modify the policies of agents when they are deployed in practice. For example, action constraints have been used to prevent damage when training robot policies, and transformations to the policy can ensure that an agent stays within some safe region of state space [309, 310]. Whenever it is difficult to specify unsafe states formally, a human overseer might interrupt the agent [300].

Langlois and Everitt [311] address this issue by introducing modified-action MDPs (MAMDPs), where the agent‚Äôs policy outputs the agent‚Äôs decision rather than the actions. The actions are assumed to be not fully under the agent‚Äôs control but can also be influenced by the action modification. Since the agent might learn the interruption scheme from interruptions during training, the corresponding CID includes an information link from the action modification to the policy.

The MAMDP framework, including its CIDs, enables the authors to analyze the causal assumptions that prototypical RL algorithms make about the environment

![](images/c4f25f2bec2211e2a9d078516d05bec261b53409ff8046eee75450023120bddc.jpg)  
(a) Rocks and Diamonds Gridworld.

![](images/60611e0aaa141359ccf6c30a980a30098c57df54cbe7a2e576ebad246d2306e9.jpg)  
(b) Rocks and diamonds with a modifiable RF.   
Figure 7.15: Gridworld Example of Reward Tampering [312]. The agent can avoid getting the implemented reward function updated and even change it.

and to understand how they adapt to interruption via a graphical criterion for pathspecific response incentives.

They find that black-box optimization algorithms such as evolutionary strategies take all causal relationships of the environment into account and try to both obscure its policy and disable its off-switch. In contrast, with the action chosen by the agent, Q-learning and SARSA assume that the next action will be taken optimally, with no action modification. In other words, these two algorithms ignore causal effects from the action modification to the actual action taken. Another version of SARSA, based on the modified action, ignores the effect of action modification on the current action but considers the effect on subsequent actions and tries to disable the offswitch.

# 7.9.4 Reward Tampering

By default, an agent aims to maximize its observed reward, given by a (programmed) reward function. However, sometimes, it may be incentivized to tamper with this function, which we call reward tampering [312]. Everitt et al. [300] give three examples:

1. Wireheading: Rewriting the source code of its implemented reward function.   
2. Feedback tampering: Influencing users that train a learned reward model.   
3. Input-tampering: Manipulating the reward function‚Äôs inputs.

Fig. 7.15 showcases an example of an MDP with a modifiable implemented reward function. Imagine a gridworld where the agent can push rocks and diamonds by walking towards them from an adjacent cell, as illustrated in Fig. 7.15a. At time $t$ , the reward function rewards diamonds and punishes rocks for each item pushed to the goal area:

$$
R _ {t} = \# \text {d i a m o n d s i n g o a l a r e a} - \# \text {r o c k s i n g o a l a r e a}. \tag {7.8}
$$

Now, let us distinguish between intended rewards that encourage completion of the intended task and observed rewards, which are the rewards received by the agent. Generally speaking, we parameterize the returning reward function as $R _ { t } =$

$R ( S _ { t } ; \Theta ^ { \mathrm { R } } )$ , where $\Theta _ { t } ^ { \mathrm { R } }$ will denote the parameter for an implemented reward function at time $t$ , and $\Theta _ { * } ^ { \mathrm { R } }$ the parameter of an intended reward function.

To model the possibility of the agent influencing the intended function, we include a user and two reward parameters ŒòRroc $\Theta _ { \mathrm { r o c k } } ^ { \mathrm { R } }$ k and ŒòRdia $\Theta _ { \mathrm { d i a m o n d } } ^ { \mathrm { R } }$ . The new, modifiable reward function is

$$
R _ {t} = \Theta_ {\text {d i a m o n d}, t} ^ {\mathrm {R}} \cdot (\# \text {d i a m o n d s i n g o a l a r e a}) + \Theta_ {\text {r o c k}, t} ^ {\mathrm {R}} \cdot (\# \text {r o c k s i n g o a l a r e a}), \tag {7.9}
$$

where the reward parameters toggle between $^ { - 1 }$ and +1 if the agent stands on top of them. Further, the reward parameters are set to their intended value of diamondgathering ( $\Theta _ { \mathrm { r o c k } } ^ { \mathrm { R } } = - 1$ and $\Theta _ { \mathrm { d i a m o n d } } ^ { \mathrm { R } } = 1$ ) when the agent visits the user tile.

The intended agent‚Äôs task is to gather diamonds. However, in its initial implementation, $\Theta _ { 1 } ^ { \mathrm { R } }$ rewards rocks instead of diamonds, which is corrected when the agent passes the user. According to the intended reward function, the agent should not walk around the user or visit the reward parameter tile. Unfortunately, by breaking either of these conditions, the agent can observe more rewards.

Everitt et al. [312] claim that a standard RL agent may have an instrumental goal to tamper with its implemented reward function. By modeling the above problems with CIDs and analyzing their previously proposed solutions, the authors find that the latter avoid undesirable incentives by removing causal links in a way that avoids instrumental control incentives. However, there are still differences between the proposed methods, e.g., whether they use previously observed or latent variables to formulate an objective avoiding reward tampering.

# 7.9.5 Delicate States

A delicate state is as part of the environment which is hard to define a reward far (subtle) and vulnerable to deliberate manipulation towards bad outcomes (manipulable). Farquhar et al. [313] try to remove any incentive for the agent to control the delicate part of the state-space (as having a control incentive over it is dangerous).

To remove it, they declare a stable state condition, which is robust against unmotivated action and renders side-effects unlikely to be bad. They formalize these conditions through CIDs‚Äô graphical criteria for the presence of incentives to control certain parts of the state.

# 7.10 Open Problems

Lack of unified evaluation environments: For example, in causal model-based RL, all methods share the same goal of learning the environment transition dynamics (e.g., to improve sample efficiency for policy search). However, there is no overlap in their evaluation environments. Rezende et al. [236] evaluate on AvoidFuzzyBear (proposed in the paper), MiniPacman [314], and visual 3D [315] environments. Wang et al. [240] use a Chemical and manipulation environment. Li et al. [237]

conduct their experiments on the CoPhy benchmark [316] (Sec. 9.2). Similar inconsistent evaluation procedures occur in causal multi-task RL.

Unification of Formalisms: Due to the parallels between causality and reinforcement learning and the lack of maturity at the intersection of both, we fear redundancy among papers, i.e., an analogy of reinventing the wheel [317] taking place. An increasing number of formalisms in the literature may lead to decision paralysis for practitioners and, similarly as we have argued in the previous paragraph, slows down progress by increasing barriers to comparing approaches. We suggest that future work compares the applicability of different formalisms more rigorously and possibly unifies them.

For example, consider a set of MDPs with shared dynamics (e.g., the set of all physically feasible pendulums), where each single MDP is specified by additional dynamics parameters (e.g., the length and mass of a pendulum, see [91] for more concreteness). Let us assume that these parameters are unobserved.

In the literature, we notice a myriad of MDP formalisms proposed for that type of setup: MDP with Unobserved Confounders [318], Confounded MDPs [319], Causal POMDPs 1 [237], Causal POMDPs 2 [243] (they are not the same formulation), Causal MDPs [232], and discrete MDPs (a universe, Sec. 7.4) [250]. Even in the non-causal RL literature, we find multiple formalisms with similar motivations, e.g., Contextual MDPs [320], Hidden Parameter MDPs [244], and Bayes Adaptive MDPs [321]. We understand that these formalisms may not be technically identical but hypothesize that unification is possible and can facilitate progress.

Deconfounding Offline RL: Whenever we aim to learn policies from an offline dataset without access to observed policy or the environment, this dataset likely includes confounding biases. As we have seen throughout this section, a significant motivation for using causality for RL is to deconfound observed data, i.e., transform it so that confounding biases are removed. We believe that deconfounding observational data is under-explored in pure offline RL settings, and only a few works have addressed this setup [256].

Counterfactual Decision-Making: Bareinboim [213] expose that inference of counterfactuals is neglected in the RL literature, possibly due to the difficulty of learning correct SCMs. However, counterfactual reasoning in agents may bring additional benefits to agents only reasoning based on observational distributions, such as less bias during policy search (Sec. 7.5), assigning credit to single agents in a sharedreward cooperative multi-agent system [284] or considering the intended actions of humans in a human-in-the-loop system [322].

Bottou et al. [208] illustrate how counterfactual reasoning can assist large-scale realworld machine learning systems, such as the ad placement system associated with the Bing search engine. They propose a causal model for ad placement and apply it to optimizing ad auction tuning parameters. This work is very detailed and discusses multiple fundamental causal inference techniques, connections to RL, and different

design choices. We hope to see more carefully-designed counterfactual agents for real-world systems that may take inspiration from Bottou et al. [208].

# Modality-specific Applications

In the previous chapters, we familiarized ourselves with different methodologies that applied two causal primitives, interventions and counterfactuals, throughout 5 different problem domains; ranging from how to learn invariances in data up to dealing with confounding in sequential decision making settings.

In this chapter, we review methods designed for particular data modalities, namely: image (computer vision), text (natural language processing), and graph (graph representation learning) data. Some of these works are off-the-shelf applications of the core methodology introduced in the previous chapters; some are not but are too modality-specific to be presented in the earlier chapters.

We observe a few common themes across all three areas, which we use to divide the methods within each modality. For convenience, we recap and briefly summarize them here:

‚Ä¢ Causal Supervised Learning (Chapter 3): Methods disposing of spurious associations by extracting (invariant) feature maps containing only the causal parents of the predictor variable $Y$ .   
‚Ä¢ Counterfactual Data Augmentation (Chapter 4): Methods augmenting training data with counterfactual modifications to a subset of the causal factors, leaving the rest of the factors untouched.   
‚Ä¢ Counterfactual Explanations (Sec. 5.2.1): Methods explaining model predictions by computing a (minimally) altered features instantiation of an individual that would cause the underlying model to classify it in a different class.   
‚Ä¢ Causal Fairness (Chapter 6): Methods ensuring that model predictions are fair w.r.t. causal relationships involving protected attributes (e.g., gender, race, etc.).   
‚Ä¢ Miscellaneous: Everything else that does not fit into the above categories.

# 8.1 Causal Computer Vision

# 8.1.1 Causal Supervised Learning

Long-Tail Classification: Stochastic gradient descent (SGD) methods are central to neural network optimization [323]. Modern SGD variants (e.g., Adam, SGD-M, etc.) commonly used for image classification often involve some form of momentum

![](images/5d2863a8f7cb97d68bfba8529ccce7261a068ee8855eab8894fb17abd5a3d92f.jpg)

![](images/0237a3d44725629f2ff25274d93597b4bf36401775f77c568106a2dc934997d5.jpg)

![](images/39582c2fd2b16b11f4bad750f2811ad76da6933cc38ab749493fe25c02458ee0.jpg)  
Figure 8.1: SGD Momentum as Confounder [324]. (a) Causal DAG explaining the causal effect of momentum. (b) Sorted mean magnitudes of feature vectors for each class $_ i$ after training with momentum $\beta = 0 . 9$ . (c) The relative accuracy change from $b e t a = 0 . 9 8$ as a function of $\beta$ indicates that the few-shot tail is more sensitive to it.

(or acceleration), which accumulates historical gradients to speed up convergence, akin to a heavy ball rolling down the loss function landscape.

Tang et al. [324] interpret momentum M as a confounder in long-tailed classification settings. If a dataset is balanced, each class contributes equally to the momentum. Long-tailed datasets, however, will be dominated by head samples (i.e., samples belonging to the majority class). Long-tailed datasets are very common in practice, e.g., in NLP due to Zipf‚Äôs law [325] or in image segmentation, where increasing the images of tail class instances like ‚Äúremote controller‚Äù requires more head instances like ‚Äúsofa‚Äù or ‚ÄúTV‚Äù simultaneously.

Let us consider input features $\mathbf { X }$ , momentum M, classification logits $Y$ , and $\mathbf { D }$ , which denotes $\mathbf { X }$ ‚Äôs projection on the head feature direction that eventually deviates $\mathbf { x }$ . In a long-tailed dataset, few head classes possess most training samples, which have less variance than the data-poor but the class-rich tail. Therefore, the moving averaged momentum will point to a stable head direction. Specifically, the authors demonstrate that one can decompose any random feature vector $\mathbf { X }$ into ${ \bf X } = \ddot { \bf X } + { \bf D }$ , where $\mathbf { D }$ is a function of the exponential moving average features and the number of training iterations (for brevity, not shown here).

The authors show that the following causal relationships emerge: M confounds the input features $\mathbf { X }$ ( $\mathbf { M } \to \mathbf { X }$ ) and the classification logits $Y$ (via $\mathbf { M }  \mathbf { D }  Y$ ), as illustrated in Fig. 8.1(a). The backdoor path $\textbf { X }  \textbf { M }  \textbf { D }  \textbf { Y }$ causes a spurious association between $\mathbf { X }$ and $Y$ . The authors call this association a ‚Äúbad‚Äù bias. Fig. 8.1(b,c) demonstrate that momentum contributes to this bias empiricially. Further, the mediation path $\mathbf { X }  \mathbf { D }  Y$ is considered a ‚Äúgood‚Äù bias, because it respects the inter-relationships of the semantic concepts in classification.

To remedy the ‚Äúbad‚Äù momentum bias, Tang et al. [324] apply backdoor adjustment to account for the backdoor confounding path, while keeping the ‚Äúgood‚Äù mediation path. Further, for the final prediction logits, they compute the intervention $p ( y \mid x )$ to yield the direct causal effect of $\mathbf { X } \to Y$ . Their method improves over baselines in long-tailed image classification and instance segmentation benchmarks.

![](images/2e37aa8989eed7c514683365b2fc2efb5ab2029ed6d7906674ecb7e14016ddde.jpg)  
(a) relationships among different FSL paradigms

![](images/16f7046f4113d8f8643bad2f96ac3d61ca87ca8871f5aaad1fdec7623c6e5e83.jpg)  
(b) Causal DAG. Pre-trained knowledge D confounds $\mathbf { x }$ and $\mathbf { C }$ .   
Figure 8.2: Confounding (Sec. 2.5) in Few-Shot-Learning [328]: Pre-trained weights, obtained through conventional pre-training or meta-learning, can be interpreted as a confounder that may introduce transfer deficiencies (Sec. 8.1.1).

Few-Shot Learning: Few-shot learning (FSL) methods assume that deployed models in production will likely encounter novel tasks for which only a few samples with labels are available; yet, these data-poor tasks may have some structural similarity to other data-rich tasks [326, 327]. Murphy [327] give the following example: consider the task of classifying endangered bird species, which are by definition rare. However, birds bear many structural similarities across species (wings, feathers, etc.). Hence, training a model on a large dataset of non-endangered species and then transferring that knowledge to the small datasets of endangered birds may result in better performance than training on the small datasets alone.

A common strategy to deal with such settings is transfer learning. A simple transfer learning strategy is fine-tuning, which consists of two phases: First, we perform a pre-training phase, in which we train a model on a large source dataset. Second, we freeze some pre-trained parameters and continue training the rest on the few-shot target (training) dataset of interest.

Another, more advanced transfer learning strategy is meta-learning, which aims to learn a meta-model that quickly adapts to different few-shot datasets. We will not discuss how these methods work but direct interested readers to [326].

Yue et al. [328] study both FSL strategies from a causal perspective and find that pre-trained knowledge is a confounder that can limit their performance. By adjusting for this confounder, the authors develop three algorithms that achieve new state-ofthe-art results on several FSL benchmarks.

For illustration, recall the exemplary problem in Fig. 3.1, where we are interested in predicting the cow label based on cow-specific features and not the background features (e.g., the mountains). Using pre-trained knowledge $\mathbf { D }$ (e.g., a large dataset $\mathcal { D }$ or a third-party pre-trained model with parameters $\pmb { \theta }$ ), $p ( \boldsymbol { y } \mid \boldsymbol { x } )$ may fail to generalize well, because it induces spurious associations: the pre-trained weights generate features ( $\mathbf D \to \mathbf X$ ), and semantics ( $\mathbf { D }  \mathbf { C }  Y$ ) that may overly rely on mountain specifics.

The authors argue that to make FSL more robust, we need to pursue the true causality between $\mathbf { X }$ and $Y$ , i.e., the causal intervention $p ( \boldsymbol { y } \mid \mathrm { d o } \left( \pmb { x } \right) )$ . To illustrate

![](images/5a609e12e74fd33626a3a156812771de33348c5cc9a064bc3483046714bf4a8b.jpg)  
Figure 8.3: Many-shot (MSL) vs. few-shot (FSL) vs. interventional few-shot learning ‹¶(IFSL) [328]. MSL and IFSL are robust against confounding of $\mathbf { D }$ , while FSL is not.

‹ª ?? ‹´ ‹ª ?? ‹´why this is, they contrast FSL with many-shot learning (MSL), which refers to finetuning with a larger target dataset. Naturally, we would expect MSL to work better simply because we use more data; yet, the authors argue that this would not answer why MSL converges to the true causal effects as the number of samples approach infinity. Causal Perspective 8.1.1 explains this in more detail.

# Causal Perspective: 8.1.1: Many-Shot vs. Few-Shot Learning [328]

Yue et al. [328] postulate that $p ( \boldsymbol { y } \mid \mathrm { d o } \left( \boldsymbol { x } \right) ) \approx p ( \boldsymbol { y } \mid \boldsymbol { x } )$ in MSL while $p ( y \mid$ $\mathrm { d o } ( { \pmb x } ) ) \not \approx p ( { \pmb y } \mid { \pmb x } )$ in FSL. To show why this holds, let us introduce the sample ID $I$ and assume that $\pmb { x } \sim p ( \pmb { x } \mid i )$ . Naturally, we may assume that we can use $p ( y \mid i )$ to estimate $p ( y \mid x )$ , so we can incorporate $I$ into the estimation of $p ( y \mid x )$ , by writing

$$
p (y \mid \boldsymbol {x} _ {i}) := \mathbb {E} _ {\boldsymbol {x} \sim p (\boldsymbol {x} | i)} [ Y \mid \boldsymbol {x}, i ] = p (y \mid i). \tag {8.1}
$$

Now, we compare how $I$ enters the causal graphs for MSL and FSL, illustrated in Figs. 8.3a and 8.3b. For MSL, we find that $I \to \mathbf { X }$ , and not $\mathbf X \to I$ , because tracing X‚Äôs ID out of many samples is like ‚Äúfinding a needle in a haystack‚Äù. This makes $I$ an instrumental variable (Sec. 11.2.1.2), effectively meaning that $I$ and $\mathbf { D }$ are independent and $p ( y \mid \pmb { x } ) = p ( y \mid i ) \approx p \left( y \mid \mathrm { d o } \left( \pmb { x } \right) \right)$ (details in [328]‚Äôs Appendix). However, $\mathbf X \to I$ persists in FSL, because it is easier to guess the correspondance, e.g., in the 1-shot extreme case that has a trivial 1:1 mapping for $\mathbf { X }  I$ .

Next, the authors propose Interventional FSL, where the idea is to use backdoor adjustment to estimate $p ( \boldsymbol { y } \mid \operatorname { d o } ( \boldsymbol { x } ) )$ without the need for many-shot samples, as illustrated in Fig. 8.3c. This adjustment requires observing and stratifying the confounding variable, which is non-trivial when $\mathbf { D }$ is a third-party delivered pre-trained network. The authors suggest three implementations for this: (i) feature-wise adjustment, (ii) class-wise adjustment, and (iii) combined adjustment. They show that these implementations improve the baselines across all query hardnesses.

Attention Models: To make visual attention-based models less susceptible to capture spurious correlations and more robust in OOD settings, Wang et al. [319]

![](images/f45ec58894573395c2d8cf1f7282e2b77ffdec3208f00af81a01b5e4fd5b302b.jpg)  
(a) Causal Graph

![](images/e441edf2f51261018e060cf98cb73ce6d64b92df3d21771eb20e02c39b2ac0e7.jpg)  
(b) Target Intervention

![](images/2cfc13a0ce604f52f814db4a0e06fee5491a7131a213a03ed55762266eb2eb07.jpg)  
(c) Improper Intervention

![](images/ab2aea6d96aa7f7667b96b819c39b5c26980dee8a057a4721135ffc434559cc2.jpg)  
Figure 8.4: Causal DAG for Causal Attention Module [319], consisting of image $\mathbf { X }$ , label $Y$ , mediator M and unstable context confounder S. The goal is to learn the causal association $\mathbf x \to Y$ , while disposing assocations along the pathways of $\mathbf { X } \left. \mathbf { S } \right. Y$ .   
(a) When the confounders are observed, we can apply the back-door criterion to adjust for them.

![](images/17d9142bebf1cd14c114e8c55629b7d961928952c989090d34f23fb4389c0f0f.jpg)  
(b) If the confounders are unobserved but the causal association between $\mathbf { x }$ and $Y$ is mediated through $\mathbf { z }$ , we can use the front-door criterion.   
Figure 8.5: Confounder Identification-free Causal Feature Learning [329].

propose a causal attention module that annotates confounders in an unsupervised way. Fig. 8.4 shows their causal graph: $\mathbf { X } \to Y$ is the desired causal effect from image $\mathbf { X }$ to label $Y$ . Further, we assume an invariant mediator variable M that contains discriminative object parts, e.g., a bird‚Äôs wing. S is the style confounder, e.g., the non-discriminative background of an image (sky when the bird flies, ground when its wings are down). The target intervention is to disentangle the background $\mathbf { s }$ and the mediator $\mathbf { M }$ , as shown in Fig. 8.4b). However, since a perfect intervention is typically not easy to obtain when working with visual datasets, the authors propose to perform an improper intervention, illustrated in Fig. 8.4c).

To identify the content and style variables, they train two separate attention mechanisms $f _ { a } ( \cdot ) , f _ { \bar { a } } ( \cdot )$ in an adversarial fashion, respectively. Similar to training Generative Adversarial Networks [105], each iteration of the training pipeline corresponds to solving a bilevel optimization problem consisting of a minimization and maximization step. The minimization step optimizes the invariant feature extractor $f _ { a } ( { \pmb x } ) = { \pmb c }$ , while the maximization step updates the style confounder extractor $f _ { \bar { a } } ( { \pmb x } ) = { \pmb s }$ .

Unobserved Confounders: Most previous approaches adopt the backdoor criterion to mitigate the effect of confounders. However, the backdoor criterion requires the explicit identification of confounders. Since it can be challenging to identify confounders in many real-world scenarios, Li et al. [329] explore a confounder identification-free method using the front-door criterion. Fig. 8.5 illustrates the difference between using the backdoor and front-door criterion.

The front-door criterion does not require identifying the confounders if we have access to an intermediate variable $\mathbf { Z }$ , such that $\mathbf { X }  \mathbf { Z }  Y$ . The authors propose a strategy to simulate interventions on $\mathbf { Z }$ , relying on meta-gradients. They also connect this strategy to gradient-based meta-learning methods and explain why methods like MAML [330] work from a causal perspective. Overall, their method improves the cross-domain performance of vision models.

Motion Forecasting: Liu et al. [331] study motion forecasting, the task of forecasting the location of a tracked object from a video. Fig. 8.6a illustrates their causal DAG: they treat each video as a domain $e \in { \mathcal { E } }$ , and include style S, content $\mathbf { C }$ , as well as domain invariant variables $\mathbf { Z }$ , in contrast to $\mathbf { s }$ and $\mathbf { C }$ which have dependence on the domain $e \in { \mathcal { E } }$ . For example, $\mathbf { Z }$ may capture laws of physics.

Fig. 8.6b illustrates their model architecture: an invariant encoder $\phi ( \cdot )$ models domain invariant relationships, the style encoder models $\psi ( \cdot )$ models domain-specific relationships, and the style modulator $f ( \cdot )$ extracts useful domain-specific information while ignoring spurious information. A contrastive loss $\mathcal { L } _ { \mathrm { s t y l e } }$ is employed to encourage learning useful representations of domain specific information.

![](images/636f14cc3379088ea423bf3815611dfb693beae29b4dcea91cd10c2352c6dd8b.jpg)

![](images/bf917bb916e78ec4e739ed1c295e4be9e96f049ff559e5588f160e314ef213c1.jpg)  
(a) Causal Graph with domain (b) Model Architecture: The invariant encoder estimates domain invariant $\mathbf { z }$ , domain specific $\mathbf { C }$ , invariant variables $\mathbf { z }$ , while the style encoder separates between the and style S. Video index $E$ con- intra-domain predictor variables $\mathbf { C }$ and the spurious variables S. C founds S and $\mathbf { C }$ . and $\mathbf { z }$ are concatenated to make a prediction.   
Figure 8.6: Causal Motion Forecasting [331]: An approach for forecasting the location of an object over time that generalizes across environments.

Weakly-Supervised Semantic Segmentation: Zhang et al. [332] present a framework called Context Adjustment (CONTA) to deconfound pixel-level pseudo-masks in weakly-supervised semantic segmentation (WSSS). Specifically, the goal is to obtain non-spurious pixel-level pseudo-masks M of image segments $Y$ . The authors identify that three types of style features ${ \bf S } ^ { 1 }$ lead to inaccurate pseudo-masks:

1. Object ambiguity: Objects often co-occur with each other under certain contexts. For example, if most horse images contain people riding horses, there exists a spurious association between these two distinct objects. A model relying on it will learn that most horses are with people, outputting pseudo-masks that do not draw boundaries between a person and a horse.   
2. Incomplete background: Backgrounds are often composed of multiple (unlabeled) semantic objects. Hence, there is a co-occurrence of foreground, and back-

ground objects, e.g., some parts of the background ‚Äúfloor‚Äù can be misclassified as the foreground ‚Äúsofa‚Äù.

3. Incomplete foreground: Semantic parts of the foreground object may co-vary with different contexts, e.g., the window of a car may include reflections of its surroundings. Hence, a model may learn to segment less discriminative parts to represent the foreground, e.g., the ‚Äúwheel‚Äù of a ‚Äúcar‚Äù (instead of the whole car).

Therefore, they propose an SCM capturing the causal relationships between images, styles, and class labels, as illustrated in Fig. 8.7. Their deconfounding method, named context adjustment, removes the confounding bias in semantic segmentation by simulating an intervention $p ( \boldsymbol { y } \mid \operatorname { d o } \left( \pmb { x } \right) )$ using an approximation of the unobserved S.

![](images/74d862241135dd4055bc657eb8a17d5483ec37b8a059f1fcc75112ac6c7d1648.jpg)

![](images/141028ff1410f423da647f4a90091618e779923b13917a8a17a2afa95e0e36c8.jpg)  
(a) Pictorial description of each component.

![](images/32152ec72c8544f497ae337b80bbfe0ef26c79bda569d4f207126888d2f76be1.jpg)  
(b) Causal DAG with image $\mathbf { x }$ , label $Y$ , style features S, and pseudo-masks M.   
Figure 8.7: Context Adjustment (CONTA) for Weakly-Supervised Semantic Segmentation [332]. The goal is to deconfound pixel-level pseudo-masks.

Visual Grounding: Visual grounding is the task of mapping a free-form natural language query (phrase or sentence) onto its corresponding region of the image, e.g., ‚Äúthe dog next to the person who is driving a car‚Äù [333]. By investigating failure cases of existing grounding methods, Huang et al. [333] reveal spurious associations between certain subjects and their locations in common grounding datasets. For example, pictures corresponding to queries including ‚Äúsheep‚Äù are often located in the central area; ‚Äúcorner of‚Äù tend to be of smaller size; ‚Äústanding‚Äù tend to display standing people in the center because they are commonly the focus of the photographer; however, if encounter images where most people stand aside, this spurious association does not hold anymore.

Fig. 8.8 shows the causal graph Huang et al. [333] propose. The causal estimand of interest is the interventional distribution $p ( l \mid \mathrm { d o } ( r ) , \pmb { x } ) \neq p ( l \mid r , \pmb { x } )$ , where $\mathbf { X }$ denote the image, $R$ the language query, and $\mathbf { L }$ the object location. The inequality is due to the unobserved confounder $\mathbf { G }$ . To deal with this unobserved confounder, the authors exploit the previously proposed Deconfounder algorithm by Wang and Blei [334]. This algorithm allows one to learn a generative model of the substitute confounder $\hat { \mathbf { G } }$ , which Huang et al. [333] use to perform the backdoor adjustment. We note that

![](images/57a62337d26c3f0529f9ff9555d914f085d707fd38a9c7786657166323d9048b.jpg)

![](images/77c835def34b9c5999bdfbdc67ffbc1834ad93856c71ad6dca943560cca8dd86.jpg)  
Figure 8.8: Deconfounded Visual Grounding [333]: G: unobserved confounder, $\mathbf { X }$ : pixel-level image, R: language query, L: the location for the query.

this algorithm has caused some controversy, and Ogburn et al. [335], D‚ÄôAmour [336] questioned its legitimacy.

Video moment retrieval: The task of identifying the start and end of a segment in a video that corresponds to a textual query is known as video moment retrieval (VMR). Otani et al. [337] argue that VMR models often exploit spurious temporal location biases in datasets rather than learning the cross-modal matching. In other words, the temporal location of moments is a hidden confounder that spuriously correlates user queries and moment locations, making the model ignore the actual video content.

Nan et al. [338] propose to remove such spurious correlations from the video grounding model by considering interventions on the text and video input. They call this method Interventional Video Grounding (IVG), as it uses backdoor adjustment to model $p \left( \boldsymbol { y } \mid \mathrm { d o } \left( \boldsymbol { x } \right) \right)$ . They then propose a method to approximate the unobserved confounder as one cannot sample it directly from the dataset. They learn representations of the video and text inputs and enforce alignment between these representations of similar concepts with a contrastive approach.

Similarly and concurrently, Yang et al. [339] Yang et al. [339] construct an SCM consisting of four variables: $\mathbf { Q }$ (query), $\mathbf { V }$ (video moment), $Y$ (prediction), and $\mathbf { L }$ (moment location). Then, they replace the non-causal query $p ( \boldsymbol { y } \mid \boldsymbol { q } , \boldsymbol { v } )$ with $p ( \boldsymbol { y } \ | \ \mathrm { d o } ( \pmb { q } , \pmb { v } ) )$ . Thereby, the query is forced to fairly interact with all possible locations of the target based on the intervention.

# 8.1.2 Counterfactual Explanations

Goyal et al. [340] produce counterfactual explanations Sec. 5.2.1) to scrutinize the prediction of image classification models. Given a query image $\mathbf { X }$ with class label $y$ , the goal of the counterfactual visual explanation is to identify how to change the image such that an image classification model would predict a different class $y ^ { \prime }$ .

Their methods work as follows: First, one has to select a distractor image $\mathbf { X } ^ { \prime }$ that the model predicts as the class $y ^ { \prime }$ . Then, they identify spatial regions in the images $\mathbf { X }$ and $\mathbf { X } ^ { \prime }$ , replacing the spatial region in $\mathbf { X }$ with the area in $\mathbf { X } ^ { \prime }$ would lead to predicting the class $y ^ { \prime }$ . Beyond providing explanations for model prediction, the authors find that counterfactual visual explanations can help human users distinguish different classes better.

![](images/e634893aec8d702b3f5161ad149175d5eb36599b09264fa908145f3bb169b3b2.jpg)  
Figure 8.9: Counterfactual visual explanations (CVE) [340]: Consider two images of two similar looking yet different birds: $\mathbf { X } , \mathbf { X } ^ { \prime }$ is the query and distractor image with label $y , y ^ { \prime }$ , respectively. CVE identify regions in both images such that if the highlighted region in $\mathbf { X }$ looked like the highlighted region in $\mathbf { X } ^ { \prime }$ , the resulting image $\mathbf { X } ^ { * }$ would be classified more confidently as $y ^ { \prime }$ .

Hendricks et al. [341] propose to generate counterfactual explanations by inspecting which evidence an input data point misses but might contribute to a different classification decision if present in the image. The motivation is to generate post-hoc natural language explanations about what attributes might change classification decisions if present in an image, e.g., ‚ÄúThis is not a Scarlet Tanager because it does not have black wings.‚Äù

# 8.1.3 Causal Generative Modeling

# 8.1.3.1 Counterfactual Data Augmentation

Zero-Shot Learning: Yue et al. [342] suggest to generate counterfactual samples for out-of-distribution generalization of classifiers, by means of a generative model which disentangles class attributes $Y$ and sample-specific attributes $\mathbf { Z }$ of $\mathbf { X }$ . Then, they generate counterfactual samples from a counterfactual distribution where $\mathbf { Z }$ has been intervened upon while keeping $Y$ constant, $\tilde { \mathbf { \pmb { x } } } \sim p \left( \mathbf { \pmb { x } } \mid y , \mathrm { d o } \left( \tilde { \mathbf { \em z } } \right) \right)$ .

To ensure that $\scriptstyle { \dot { \mathbf { x } } }$ lies in the true distribution of the seen or unseen samples, they enforce counterfactual faithfulness by applying the contrapositive of the Consistency Rule [343]: if $_ { x }$ is dissimilar to $\tilde { \mathbf { x } }$ , the ground-truth attribute of $_ { x }$ cannot be $y$ . We can implement this criterion by training a binary classifier that distinguishes between seen and unseen data $\mathbf { X }$ .

Cross-Domain Pose Estimator: Zhang et al. [344] propose using causal representation learning to improve cross-domain 3D pose estimation tasks. Specifically, they train a counterfactual feature generator that takes domains and contents as input. They change domains to simulate interventions and steer the model to produce counterfactual features. This strategy facilitates the model to learn transferable features across domains.

![](images/a0f5becf42821151a2e0a2489de92dc23bbede7dfc8ab12c3b67e20db6cbfd9a.jpg)  
Figure 8.10: Visual Causal Discovery Network [345]: it extracts unsupervised keypoints as the state representation, infers a causal graph, and then learns a dynamics module conditioned on both.

# 8.1.3.2 Counterfactual Trajectory Generation

Our goal is to generate a counterfactual sequence $\pmb { x } _ { 1 : T } ^ { \mathrm { C F } }$ under a new initial condition $\pmb { x } _ { 0 } ^ { \mathrm { C F } }$ , given an observed initial condition ${ \bf { \mathcal { x } } } _ { 0 }$ and a sequence of $T$ frames $x _ { 1 : T }$ .

Li et al. [345] propose the Visual Causal Discovery Network (V-CDN), which is akin to tackling all three levels of causal representation learning (Def. 2.4.1). It consists of three modules: (i) a perception network employing an unsupervised keypoint detection algorithm [346] to extract useful features from a video; (ii) a structural inference module using a GNN to learn a causal graph; and (iii) a dynamics prediction model conditioned on the causal graph and the current state features to make predictions into the future. The model tackles a task that purely associational object learners fail on: modeling interactions between objects beyond collisions only.

Following up on Li et al. [345], Janny et al. [347] complement learning latent representations based on the unsupervised discovery of keypoints with additional information. Specifically, this addresses two problems: First, one must encode the shape, geometry, and relationships between multiple moving objects through the relative positions of points because each object is only discriminated through its 2D positions. Second, a 2D keypoint space may not be the optimal representation for modeling the dynamics of a physical system because of the additional imaging process that may confound the data.

The authors deploy their proposed architecture on video data from a 3D simulator of stacked blocks. An encoder learns a representation of the blocks in the form of 2D keypoints and information coefficients which encode for shape and appearance. Latent confounders are extracted from this representation, and a dynamic model forecasts a trajectory from a representation of an observation. A decoder subsequently maps the trajectory prediction into video data. The authors also introduce a novel benchmark for counterfactual trajectory prediction, which is discussed in Sec. 9.2.

# 8.2 Causal Natural Language Processing

Despite these challenges, text has been proven to be useful in causal inference applications, because text can serve as confounder [348], outcome [349], and treat-

ment [350, 351] in causal inference. Causal inference has been applied in a variety of natural language processing (NLP) tasks such as natural language explanations [352, 353], fairness [354], text classification [355], and machine translation [356]. Next we start introducing some representative work.

# 8.2.1 Causal Supervised Learning

# 8.2.1.1 Visual Question Answering

Imagine working at a social network company, and your team‚Äôs task is to build models detecting hate speech. Colleagues of yours have already developed great models for text data only; however, a remaining challenge is detecting it in memes, where the model needs to understand the combined meaning of the words and pictures.

![](images/a1bc1c899706b8778060f934fe275a131d3d97c76a8f6e2b9f105a1f60ef1c5b.jpg)

![](images/cb5cc3ef03dae719cc6072ec16d2ca2b4f4fd816e4b69304331f89d4a2f31314.jpg)

![](images/491c8ce6b1c039bdfe18bb812c473f4d1d917d24097d57a5737192b6ee4af097.jpg)

![](images/2bcd4a6f3bf299cfb950dc8d16a5d57a798a502f32772b7d755fa370aad744e8.jpg)

![](images/966bcc88121b23f87da0a7e1981e21fd378f020051e52a5988bd514e0c220745.jpg)

![](images/f08ede9cba3987ca84c5ada835a977aca0eae89ad40088e68b54edbe0d3b5671.jpg)

![](images/427702f67478ed1556767c9ae9292e55c3774f05da0015cab23adc13c547e606.jpg)  
Figure 8.11: Multimodal ‚Äúmean‚Äù memes [357]. Left: Mean memes, Middle: Benign Image Confounders, Right: Benign Text Confounders.

This problem falls into the category of Visual Question Answering (VQA), i.e., the task of answering previously unseen questions framed in natural language about a previously unseen image (e.g,. ‚ÄúDoes this image include hate speech?‚Äù). The classical approach for VQA is to learn a model from a training set made up of image $\mathbf { \boldsymbol { v } }$ , question $\mathbf { \pmb q }$ and answer $a$ triplets $\mathcal { D } = \{ \langle q _ { i } , v _ { i } , a _ { i } \rangle \} _ { i = 1 } ^ { n }$ . The model infers an embedding of the questions $e ^ { q } = f _ { q } ( q )$ , an embedding of the image $e ^ { v } = f _ { v } ( { \pmb v } )$ and a fusion function of the two $z = h ( e ^ { q } , e ^ { v } )$ into what is known as the joint space.

One challenge for such models is to deal with spurious associations between the image and text modalities. For instance, Fig. $8 . 1 1 ( L e f t )$ illustrates common mean memes; memes with text like ‚Äúlove the way you smell today‚Äù might be spuriously associated with images of unpleasant smells. These spurious associations make it harder for models to truly capture multimodal understanding, as shown by Kiela et al. [357]. The authors develop a benchmark with benign confounders, which are minimum replacement images/texts that flip the labels for a given multimodal meme from hateful to non-hateful. Benign Image and Text confounders are shown in the

Middle and Right of Fig. 8.11, respectively. They find that when evaluated on this benchmark, state-of-the-art methods perform poorly compared to humans.

In the following, we look at two causal VQA methods that propose to break spurious associations between text and image.

Counterfactual Vision and Language Learning: Abbasnejad et al. [358] suggest to train VQA models on both observational and generated counterfactual samples to improve generalization. The motivation behind this procedure is to force the model to use both input modalities instead of relying on correlations from one modality only. Typical VQA models possess feature extractors $e ^ { q } = f _ { q } ( q )$ and $e ^ { v } = f _ { v } ( { \pmb v } )$ for question and visual input $\pmb q$ and $_ { \pmb { v } }$ , respectively. In contrast, Abbasnejad et al. [358] construct an SCM in which the feature extractors depend on exogenous variables: $f _ { v }$ is replaced by $\dot { f } _ { v } \left( v , u ^ { v } \right)$ and $f _ { q }$ by $\tilde { f } _ { q } \left( \pmb q , \pmb u ^ { q } \right)$ where $\pmb { u } ^ { v }$ and $\pmb { u } ^ { q }$ are exogenous variables for image (vision module) and question (language module), respectively. During training, they intervene on either $\mathbf { \pmb { q } }$ or $\mathbf { \nabla } _ { \mathbf { v } }$ , denoted by $\tilde { \pmb q }$ and $\tilde { v }$ , respectively, and yield the corresponding embeddings ${ \tilde { e } } ^ { q }$ and $\tilde { e } ^ { v }$ . They find their approach effective on both unimodal vision and language tasks as well as multi-modal vision-and-language tasks.

Counterfactual VQA: Niu et al. [62] formulate language confounding as the direct causal effect of questions on answers. To deconfound such spurious associations, they propose to subtract the direct language effect from the total causal effect. They refer to their approach as Counterfactual VQA (CFVQA).

Fig. 8.12a illustrates how conventional and counterfactual VQA differ in their workings. Conventional VQA asks ‚ÄúWhat will answer $A$ be, if machine hears question $Q$ , sees image $V$ , and extracts the multimodal knowledge K?‚Äù. Doing so cannot disentangle the single-modal linguistic effect and the multimodal reasoning effect.

To isolate the linguistic effect, Niu et al. [62] consider the following counterfactual question: ‚ÄúWhat would have happened if the model had not performed multimodal reasoning?‚Äù. This corresponds to the counterfactual query in which the model the machine considers $Q$ , but the multimodal knowledge $K$ is intervened under the notreatment condition, i.e., as if $V$ and $Q$ had not been accessible. Since the response of $K$ to $Q$ is now blocked, the model can only rely on the single-modal effect, effectively isolating the language bias. Fig. 8.12c depicts the corresponding causal DAGs.

By inferring the counterfactual query ‚ÄúWhat would $A$ be, if the model reads $Q$ , but had not extracted $K$ or seen V ?‚Äù, CFVQA extracts the language bias. Then, to reduce it in the final query, it is subtracted from the total effect of $\mathbf { V } = v$ and $\mathbf Q = \pmb q$ on $A = a$ , also referred to as the total direct effect (TDE). This TDE estimand is different than conventional VQA‚Äôs posterior distribution $p ( a \mid \pmb { v } , \pmb { q } )$ .

# 8.2.1.2 Certified Robustness Against Natural Language Attacks

Alzantot et al. [359] expose that sentiment analysis models can be fooled by synonym substitution attacks, as illustrated by their adversarial examples in Table 8.1. This

![](images/4b2b0288e6ac3e41a7918fe709e8bda58239bfaa0a7be7e186ddb43a3117efea.jpg)  
(a) Example of spurious associations between questions and answers. The query ‚ÄúWhat color are the bananas?‚Äù is most often associated with the answer ‚Äúyellow‚Äù.

![](images/97b531d105e4fdb836a8dc27bf13795dbd89486bb984fa4fab81cf6105cfe29d.jpg)

![](images/1740b0e8285c97b2087e392592e67ce6292e8823e676ad993c881ed429f3443e.jpg)  
(b) Causal DAG for VQA. Q: question. V: image. K: multi-modal knowledge. A: answer.   
(c) Comparison between conventional VQA (left) and counterfactual VQA (right). Unshaded nodes possess the value $\mathbf { v } = \pmb { v }$ and $\mathbf { Q } = \pmb { q }$ . Shaded nodes are at the value $\mathbf { v } = { \boldsymbol { v } } ^ { * }$ and $\mathbf { Q } = { \pmb q } ^ { * }$ , where $v ^ { * } , q ^ { * }$ denote the non-treatment condition where $_ { \textbf { \em w } }$ and $\pmb q$ are not given.   
Figure 8.12: Counterfactual VQA [62]. Conventional VQA relies on spurious associations. CFVQA isolates the language effect by imaging the scenario where the model reads the question but the multi-modal knowledge (including image) is removed. By subtracting the pure language effect from the total effect, CFVQA deconfounds the answer

Table 8.1: Natural Language Adversarial Examples for the sentiment analysis task [359]. We highlight modified words in green and red for the original and adversarial texts, respectively.   

<table><tr><td>Original Text Prediction = Negative. (Confidence = 78.0%)</td></tr><tr><td>This movie had terrible acting, terrible plot, and terrible choice of actors.
(Leslie Nielsen ...come on!!!) the one part I considered slightly funny was the battling FBI/CIA agents, but because the audience was mainly kids they didn&#x27;t understand that theme.</td></tr><tr><td>Adversarial Text Prediction = Positive. (Confidence = 59.8%)</td></tr><tr><td>This movie had horrific acting, horrific plot, and horrifying choice of actors.
(Leslie Nielsen ...come on!!!) the one part I regarded slightly funny was the battling FBI/CIA agents, but because the audience was mainly youngsters they didn&#x27;t understand that theme.</td></tr></table>

![](images/99509639db049acb1039993b85446c5d12430fce20b9edab34eeab99cd1dafed.jpg)  
Figure 8.13: Causal Intervention by Semantic Smoothing (CISS) [364]: A framework towards robustness against natural language attacks by learning $p ( \boldsymbol { y } \mid \mathrm { d o } ( \boldsymbol { x } ) )$ .

has motivated a myriad of works making NLP models more robust against such attacks [360, 361, 362, 363].

Zhao et al. [364] take a causal perspective on the natural language attack problem and frame the source of adversarial vulnerability as the spurious association induced by confounders. Fig. 8.13a illustrates their Causal DAG. For instance, when considering a movie review ( $\mathbf { X }$ ) from the IMBD dataset [365], a professional reviewer likely uses jargon (N), has high standards, and therefore, is likelier to assign a lower score on average. This spurious association can be exploited, e.g., by adding more jargon words to a positive movie review.

To defend models against such attacks, Zhao et al. [364] show that a Gaussianbased randomized classifier models the interventional distribution $p ( \boldsymbol { y } \mid \mathrm { d o } ( \boldsymbol { x } ) )$ and is therefore robust against $l _ { 2 }$ -bounded attacks. However, textual input spaces are not continuous, and text substitutions do not follow Gaussian distributions. To circumvent these issues, they propose smoothening out the latent semantic space of the learned content variable $\mathbf { C }$ instead, as illustrated in Fig. 8.13b. They refer to this framework as Causal Intervention by Semantic Smoothing (CISS).

![](images/10be799c17532391d126beb39ac2f938be35ccae350c77e29cf85e5800a5acf4.jpg)  
(a) Causal mechanism

![](images/b47faee883b291ebe55dff5e4af926e44f97f2c471658f64b5bdb491d47391c1.jpg)  
(b) Total Effect

![](images/ee1d6e26638da14d8f0069b929111eba0c07452e7913ed491287a112812dfb44.jpg)  
(c) Direct Effect

![](images/b396d5059ea74c74b2e16f55486eec54973276b5e73c13c6300203ecd69415a3.jpg)  
(d) Indirect Effect   
Figure 8.14: Investigating Gender Bias using Mediation Analysis [368]. Given a prompt $\textbf { \em u }$ such as ‚ÄúThe nurse said that‚Äù, we ask a language model to generate a continuation. A biased model may assign a higher likelihood to she than to he. To understand the role of model components on this biased prediction, we perform the do-operation ${ \bf { \delta } } _ { \bf { { \delta } } } \mathbf { { \vec { x } } } =$ set-gender, which changes $\textbf { \em u }$ from nurse to man in this example. By inferring direct and indirect effects, we can analyze the causal role of specific mediators (neurons) between $_ { x }$ and $\mathbf { \Delta } _ { \mathbf { \mathcal { Y } } }$ .

# 8.2.2 Counterfactual Explanations

Causal Model Explanation Through Counterfactual Language Models: Understanding the effect of a concept in the input on a model is crucial for explanation and model dissemination. However, this usually requires generating counterfactual sequences by dropping/replacing the concept of interest, which is challenging for existing text generation models. Feder et al. [366] propose a framework named CausalLM to generate counterfactual representations instead of counterfactual sequences. To this end, Feder et al. [366] fine-tune deep contextualized embedding models with auxiliary adversarial tasks to encourage the model to ‚Äúforget‚Äù the concept of interest. The representation of the input sequence and the counterfactual representation forgetting the concept of interest are fed into a classifier to measure the effect of the concept on the classifier‚Äôs prediction.

Investigating Gender Bias Using Causal Mediation Analysis: Many text corpora contain spurious associations due to gender stereotypes propagated or amplified by NLP systems. For example, the sentence ‚Äúhe is an engineer‚Äù is more likely to appear in a corpus than ‚Äúshe is an engineer‚Äù due to the current gender disparity in engineering [367].

Vig et al. [368] propose a method to interpret which parts of a model are biased. Their method uses causal mediation analysis to locate which parts of a neural model are causally implicated, as illustrated in Fig. 8.14. This approach is arguably better than other analysis tools like probing, as probing can only measure whether the in-

formation is encoded in hidden representations rather than whether the information is used by the model.

Counterfactual Fairness in Text Classification through Robustness: Text classifiers are sensitive to specific contents of the input. For example, a toxicity model predicts that the toxicity of "Some people are gay" is $9 8 \%$ , while the toxicity of "Some people are straight" is $2 \%$ . Garg et al. [369] study counterfactual fairness in text classification by asking the counterfactual question ‚ÄúHow would the output of a classifier change if some sensitive tokens were different?‚Äù. They define a metric named counterfactual token fairness to measure the differences between the outputs before and after substituting tokens associated with identity groups. They further evaluate three methods to promote fairness: replacing all sensitive tokens with a special token, counterfactual data augmentation, and a regularization called counterfactual logit pairing.

# 8.2.3 Counterfactual Data Augmentation

Counterfactual Generator: Zeng et al. [370] propose a new data augmentation algorithm for entity recognition with counterfactual inference. Each input sequence is divided into two parts, including entity and context. An entity in the input sequence is replaced with another entity of the same entity type. The augmented example is kept if a discriminator correctly recognizes the replaced entity. Zeng et al. [370] find that this data augmentation method can improve the generalization ability in lowresource settings. Besides, training on augmented examples can partially eliminate spurious correlations between contexts and output labels.

Counterfactual Data Augmentation for Neural Machine Translation: Liu et al. [356] design a counterfactual data augmentation method for neural machine translation. The method interprets language models and phrasal alignment causally. It creates augmented parallel translation corpora by answering the question ‚ÄúGiven that $Y _ { j }$ is aligned to $X _ { i }$ , what would $Y _ { j }$ have looked like, had $X _ { i } = \hat { x } _ { i }$ and all other phrases $\mathscr { X } _ { - i } , \mathscr { y } _ { - j }$ had been held constant?‚Äù. They use a masked language model and a translation language model to replace the source and target phrases, respectively. Compared to previous work, this method takes context and alignment into account for data augmentation.

# Mitigating Gender Stereotypes in Languages with Rich Morphology:

In Sec. 8.2.2, we learned that many text corpora include gender biases. Typically, such biases exist in many different languages around the world. Most NLP research has focused on mitigating gender stereotypes in English [371, 372]. However, these approaches often create ungrammatical sentences in morphologically rich languages like Spanish. To this end, Zmigrod et al. [373] present a counterfactual data augmentation approach for mitigating gender stereotypes associated with nouns representing people in such languages.

Their unsupervised approach uses dependency trees, lemmata, part-of-speech tags, and morpho-syntactic tags from Universal Dependencies corpora [374]. It consists of four steps: (1) Analysis of the sentence (including parsing, etc.), (2) Intervention on a gendered word, (3) Inference of the new morpho-syntactic tags, and (4) Reinflection of the lemmata to their new forms. Using four different languages, they demonstrate that their approach reduces gender stereotyping by an average of 2.5 without sacrificing grammaticality.

# 8.2.4 Miscellaneous

Causal Direction of Data Collection: The supervised learning problem aims to predict a label $Y$ based on features $\mathbf { X }$ . From a causal perspective, we can further distinguish this problem into two cases: causal and anti-causal learning, determined by the data collection process being $\mathbf { X } \  \ Y$ or $Y \  \ \mathbf { X }$ , respectively. In words, if, during the data collection process, $\mathbf { X }$ is generated first, and then $Y$ is collected based on $\mathbf { X }$ (e.g., through annotation), we say that $\mathbf { X }$ causes $Y$ (causal learning). Vice versa, if $Y$ is generated first, and then $\mathbf { X }$ is collected based on $Y$ , we say that $Y$ causes $\mathbf { X }$ (anti-causal learning).

Previous works show that this simple distinction has important implications for scenarios such as covariate shift, transfer learning, semi-supervised learning [375], and adversarial examples [376].

Table 8.2: Classification of typical NLP tasks into causal, anticausal, and mixed learning tasks [377]. Causal learning means that the model takes the cause as input and predicts the effect; anticausal refers to settings where the model takes the effect as input and predicts the cause. Some tasks do not have a clear causal interpretation of the data collection process, or a mixture of both types of data is typically used.   

<table><tr><td>Category</td><td>NLP Tasks</td></tr><tr><td>Causal</td><td>Summarization, parsing, tagging, data-to-text generation, information extraction</td></tr><tr><td>Anticausal</td><td>Author attribute / review sentiment classification</td></tr><tr><td>Mixed</td><td>Machine translation, question answering, question generation, text style transfer, intent classification</td></tr></table>

Jin et al. [377] study the causal direction of the data collection process for common NLP datasets, as summarized in Table 8.2. For example, they observe that language sentence pairs are mixed up in machine translation regardless of their original source-to-target direction (e.g., whether a sentence originated in English and was translated into Spanish or vice versa). Splitting the data into subsets reveals that they exhibit different properties, such as performance differences in self-supervised or domain adaptation settings. In light of these findings, the authors make various recommendations for future research, e.g., annotating the causal direction when collecting new NLP data and incorporating it into the model.

![](images/cbed629484c18eaff02c678b733581b2e7b73a8c3272e95d2d9d0d0efe66a443.jpg)

![](images/2e40d0e63f771f7d8474b2b73f1860b9d2407c90294b6dab838d8107bff1a6c3.jpg)  
(a) Invariant Subgraph Identification: The encoder $g ( \cdot )$ extracts invariant subgraphs causing the labels by maximally preserving invariant intra-class information.   
(b) $\mathcal { G }$ -Gen. SCM

![](images/fab5f3480f75c2f7b626d98ad77e94d3111cf16bcb0297617d69e48f79104bec.jpg)  
(c) FIIF SCM

![](images/360451b9ed5383012b252dc2a4d8358760d893fa4aca47be60154738bdfcfe47.jpg)  
(d) PIIF SCM   
(e) Causal DAGs: The causal factors generating graphs G consist of environment $E$ , style $\mathbf { s }$ and content $\mathbf { C }$ .   
Figure 8.15: Graph Out-Of-Distribution (GOOD) Generalization Framework [382]: A Style- and Content Decomposition (Def. 3.1.1) perspective on graph distribution shifts.

# 8.3 Causal Graph Representation Learning

In numerous applied fields, graphs represent systems of relations and interactions. Examples include social networks [378], molecular graphs [55], protein associations [379], and programming code syntax trees [380]. Graph Neural Networks (GNNs) form an effective framework for learning representations over graph-structured data.

# 8.3.1 Causal Supervised Learning

Discovering Invariant Rationales for GNNs: Intrinsic interpretability of graph neural networks aims to discover a small subset of an input graph that contributes most to the model prediction. However, most work on intrinsic interpretability is prone to discover data biases and spurious correlation, failing to capture causal patterns. Besides, these methods suffer from performance degradation on out-of-distribution data as they are sensitive to spurious correlation. Wu et al. [381] propose a new method by discovering invariant rationale. A rationale generator is first applied to split each input graph into causal and non-causal subgraphs. Next, they encode causal and non-causal subgraphs into hidden representations. Then causal interventions are conducted on the non-causal representations to create perturbations. Finally, they utilize two classifiers for the final prediction, trained through an invariant risk loss function.

Invariance Principle Meets Out-of-Distribution Generalization on Graphs: Chen et al. [382] propose the Graph Out-Of-Distribution (GOOD) framework, akin to the Style- and Content Decomposition (Def. 3.1.1), to learn invariant graph features. To this end, a featurizer is designed to distinguish invariant subgraphs from the other parts of the graph that domain shifts can easily perturb. The authors demonstrate that this approach improves out-of-domain generalization.

Assuming the true underlying causal DAG in Fig. 8.15b, Chen et al. [382] categorize the interactions between content $\mathbf { C }$ and style $\mathbf { s }$ into three possible DAGs, depending on whether $\mathbf { C }$ is fully informative about $Y$ , i.e., $( \mathbf { S } , E ) \perp \perp Y \mid \mathbf { C } ;$ Fully Informative Invariant Features (FIIF, Fig. 8.15c), Partially Informative Invariant Features (PIIF,

Fig. 8.15d). The main difference is that $\mathbf { s }$ is directly controlled by $\mathbf { C }$ in FIIF, and indirectly controlled by $\mathbf { C }$ through $Y$ in PIIF, which can exhibit different behaviors in the observed distribution shifts.

To extract the invariant subgraph containing the latent content features $\mathbf { C }$ from the complete observed graph $\mathbf { G }$ , the authors explicitly align the two causal mechanisms during the graph generation, i.e., $C  G$ and $( \mathbf { G } _ { s } , E _ { G } , \mathbf { G } _ { c } )  G$ . Fig. 8.15a illustrates the alignment procedure: they decompose a GNN into two sub-components: i) a featurizer GNN $g : { \mathcal { G } }  { \mathcal { G } } _ { c }$ aiming to identify the desired $\mathbf { G } _ { c }$ ; and ii) a classifier GNN $f _ { c } : { \mathcal { G } } _ { c } \to \mathcal { V }$ that predicts $Y$ based on the estimated $\mathbf { G } _ { c }$ , where $\mathcal { G } _ { c }$ refers to the space of subgraphs of $\mathbf { G }$ .

Deconfounded Training for Graph Neural Networks: Sui et al. [383] aims to discriminate each input graph into critical and trivial parts. Since the trivial part is a confounder between the critical part and the label, this opens a backdoor and leads to spurious correlations. Therefore, Sui et al. [383] propose deconfounded training to mitigate the confounding effect. They use attention modules to split each graph into a critical and trivial subgraphs. Then they eliminate the confounding effect of trivial subgraphs by performing the backdoor adjustment.

# 8.3.2 Counterfactual Data Augmentation

Learning from Counterfactual Links for Link Prediction: A common GNN task is to predict edges (or links) between node pairs. For example, links can refer to citations between papers [384], relations in knowledge graphs [380], or molecule interactions [55].

Zhao et al. [385] point out that the causal relationship between the graph structure and link existence has been largely ignored. The authors give the following example of a social network. Imagine Alice and Adam live in the same neighborhood, and they are close friends. Associating neighborhood belonging with friendship could be too strong to identify the essential components of friendship, like mutual interests or family ties. Such spurious associations may also explain why they live in the same neighborhood.

Consequently, Zhao et al. [385] propose to examine the counterfactual query ‚Äúwould Alice and Adam still be close friends if they were not living in the same neighborhood?‚Äù. Generally speaking, any counterfactual link prediction queries follow the structure of ‚Äúwould the link exist or not if the graph structure was different from that observed?‚Äù. Hence, if we train link prediction models on such corresponding counterfactuals, they rely less on spurious associations, as described above.

Zhao et al. [385] generate counterfactual links to augment the training data for link prediction tasks. Their approach estimates the causal relationship between the observed graph structure (considered as the intervention) and link existence (the outcome). Fig. 8.16 summarizes their approach.

(d) Two-step procedure.   
![](images/1db386f8ed25700151276012489758fe54cb8bca2b1834df975929231b31e649.jpg)  
(a) Causal DAG: $z _ { i }$ (b) Step 1: Find counterfactual link (c) Step 2: Train a GNN-based link predictor which and $z _ { j }$ are representa- based on most similar node pair with a predicts factual and counterfactual links given the tions of nodes $v _ { i }$ and different intervention (neighborhood). node pairs and neighborhoods.

$v _ { j }$ , $T _ { i , j }$ the neighborhoods, and the outcome $A _ { i , j }$ is the link existence between $v _ { i }$ and $v _ { j }$ .

![](images/ce335697e4f2c20521ecc809561798ae8926d0d48ad4a9f16a49c1936fa2d4ad.jpg)  
Figure 8.16: Counterfactual Link Prediction Framework [385].   
(a) Causal graph

![](images/e9e6ba695deca57261fdb7102d4bb453445dfa11c1ce97b91afa3e11bfeaa755.jpg)  
(b) Original prediction

prediction   
Figure 8.17: Causal Graph Convolutional Network Inference (CGI) [387]. (a) Causal graph of CGI; (b) Original prediction; (c) causal intervention $\mathrm { d o } ( N = \emptyset )$ where the dashed arrow indicates that the effect from the predecessor is blocked.   
![](images/73bd708180d1291df65770047a648f08882ed7af14b8d022377d004fff2a612e.jpg)  
self feature   
neighbors   
convolution output

(c) Post intervention prediction

# 8.3.3 Miscellaenous

Tackling Over-Smoothing by Causal Effect Uncertainty: Real-world graphs usually exhibit locally varying structures, i.e., uneven distributions of properties such as homophily and degree. However, a commonly-faced issue faced by GNNs is oversmoothing, which means that the representations of the graph nodes of different classes become indistinguishable, often causing local structure discrepancies [386].

Feng et al. [387] aim to resolve such node-specific structure discrepancies at inference time by taking into account the uncertainty of the causal effect of the local structure on the prediction. The causal effect uncertainty indicates whether a test node should be trusted when predicting its label based on its local structure. For example, if the local structure exhibits different properties from those observed, the causal effect uncertainty will be high, suggesting that the predicted class may be incorrect.

Fig. 8.17 illustrates this approach: Using the causal graph (a), we can compare the original (b) and post intervention prediction, which estimates what the prediction would be if the target node has no neighbors (c). By computing the difference between the two, we assess the causal effect of the neighborhood on the label prediction. Depending on this causal effect and a decision threshold, we include the node or discard it from the training data.

Relating Graph Neural Networks to Structural Causal Models: Graph neural networks are a powerful tool for learning with relational data. Zeƒçeviƒá et al. [388] provide a theoretical analysis to establish several connections between graph neural networks and SCMs. They design a new neural-causal model class by formalizing

interventions for graph neural networks. The authors also provide theoretical results and proofs on the feasibility, expressivity, and identifiability of this new neural-causal model class for causal inference.

# Causal Benchmarks

This section gives an overview of benchmarks designed for CausalML tasks and includes interventional or counterfactual ground-truth data. We discuss the current limitations of benchmarks more critically in Sec. 10.3.1.

We believe it is worth mentioning that for some tasks, one can (and should) evaluate CausalML methods on ‚Äúconventional‚Äù benchmarks developed without causalityspecific design choices. For example, in Chapter 3, we discussed methods that can be used for out-of-distribution (OOD) tasks. One may evaluate these approaches on standard OOD benchmarks, such as some causal invariance learning methods (Sec. 3.1.1.2).

Similarly, conventional RL benchmarks rely on a simulation engine to generate trajectory data. Famous examples within the RL community are MuJoCo [389], a physics engine offering a suitable playground for continuous control tasks, or the Arcade Learning Environment, allowing agents to play Atari 2600 games [390]. One can use these existing benchmarks for evaluating CausalRL methods simply by interpreting (PO)MDPs as SCMs (Causal Perspective 7.5.2).

Nevertheless, not all conventional ML benchmarks are suitable for evaluating the techniques discussed in this work. For example, to probe visual reasoning models on causal comprehension (e.g., ‚ÄúWhat effect will pushing this object have?‚Äù), we require question and answer pairs that encompass causal relationships.

# 9.1 Reinforcement Learning

Several authors designed RL simulators with high-level causal variables to facilitate the symbiosis of Causality and RL. While conventional benchmarks such as Mu-JoCo can be tweaked to allow an agent to intervene on environment variables (such as masses or lengths of the physical system [91]), CausalRL benchmarks such as CausalWorld offer a well-defined API and underlying causal graph to simplify and extend environment interventions.

CausalWorld: CausalWorld [10] is a robotic manipulation simulator that provides a combinatorial family of tasks with common causal structures and underlying factors (e.g., robot and object masses, colors, and sizes). The user or the agent can intervene on a subset of causal variables that determine the environment dynamics, allowing them to control how similar tasks are.

![](images/f526e556d23a1b11b746b125b8dd18d18d7996c36673f708d3570bb5fe83948c.jpg)  
(a) CausalWorld [10]: A robotic manipulation benchmark for causal structure and transfer learning. Tasks consist of constructing 3D shapes from a given set of blacks, inspired by how children learn to build complex structures.

![](images/b86d4db357b520b0bf683509c2bc5eddf00398d4ebd30187fc406c0b48b826f7.jpg)  
(b) Visual Causal Discovery [391]: A benchmark for learning representations of high-level causal variables from raw pixel data. Environments have objects that interact according to the underlying causal graph.

![](images/fac64b793d701849068fd964119bfe769e7b389a8d91d958175977e316e5d5ca.jpg)

![](images/54efb5f6101630a992d7a10c223ff749e56af5e7b631c9038a21a260285e5acf.jpg)

![](images/b78090c63513eecddfec0de4914986653da8829d0c7d7641e0a370f54fe90d55.jpg)  
Figure 9.2: Alchemy [392]: A 3d video game implemented with the Unity game engine, revealing an accessible ground-truth task parameterization. It involves a causal structure (c) that is periodically resampled (b), offering a testbed for RL, structure learning, online inference, and hypothesis testing. It provides two observation spaces; one is symbolic and the other is image-based.

Visual Causal Disocvery: Ke et al. [391] note that RL agents often only observe low-level variables like pixels in images, and have to induce high-level causal variables (Sec. 4.3.1). To evaluate the ability of model-based RL methods to identify these causal variables and structures, they design a suite of physics and chemistry environments whose underlying causal graphs are parameterizable.

Alchemy: Wang et al. [392] propose Alchemy, a 3D video game with a latent causal structure that is re-sampled procedurally in each episode. This environment provides a task distribution whose parameterization is accessible to the researcher yet, yields challenging tasks to solve, as demonstrated through experiments in which non-trivial deep RL methods fail. Based on probing experiments and analyses, they conclude that agents have to identify relevant parts of the latent structure to solve the tasks.

CausalCity: McDuff et al. [393] develop CausalCity, a high-fidelity simulator for causal reasoning in the safety-critical context of driving. The goal is to navigate vehicles that have ‚Äúagency‚Äù, high-level configurations controlling their sequence of actions (e.g., turn left at the next intersection), deciding their low-level behaviors

![](images/c0a32624c4bbda411d319d2f71abe3896ffba1e94f8bdc944a50d148156f8b37.jpg)  
Figure 9.3: CausalCity [393]: A multi-agent driving simulator for complex temporal causal events, such as driving and vehicle navigation. It introduces agency abstractions, such that one can simulate complex scenarios by only defining high-level variables.

![](images/c45e29b2422752f6b3dfcf8f1caeea052a19e0a568634c61fe9979da44bbce1c.jpg)  
Figure 9.4: Sample video, questions, and answers from the CLEVRER dataset [394], designed to evaluate whether visual reasoning models understand the following classes of questions: (i) descriptive, (ii) explanatory, (iii) predictive, and (iv) counterfactual. All tasks, except (i), are considered causal. We include captions for the readers to understand the frames better, but they are not part of the dataset.

(e.g., their speed). The environment is designed to simulate scenarios with complex causal relationships, including different types of confounders (e.g., weather conditions)

# 9.2 Computer Vision

Clevrer: Yi et al. [394] introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset. This video dataset allows us to evaluate models on four reasoning tasks: (i) descriptive, (ii) explanatory, (iii) predictive, and (iv) counterfactual. The authors interpret (ii)-(iv) as causal tasks and show that various state-of-the-art models for visual reasoning perform poorly on these. They conclude that future methods must learn the underlying causal relations between the objects seen in the images.

CoPhy: Baradel et al. [316] develop the CoPhy benchmark, a synthetic 3D environment for causal physical reasoning. It consists of a number of physical dynamics scenarios, such as tower of blocks falling, balls bouncing against walls or objects col-

![](images/c085a2b675ea558ca10d692120a6d553505d91ae9301f1a15c93a4d0afb206d8.jpg)  
Figure 9.5: Counterfactual Physics benchmark (CoPhy) [316]. Given an observed frame $\mathbf { X } _ { 0 }$ and a sequence of future frames $\mathbf { X } _ { 1 : \tau }$ , it generates ground-truth counterfactual trajectories corresponding to if we had intervened upon $\mathbf { X } _ { 0 }$ and set it to $\mathbf { X } _ { 0 }$ (e.g., changing the initial positions of objects in the scene).

![](images/5762e2d30bd846bb8c990c5a6dcb8a1280e31437133fe6552babc926617eec22.jpg)

![](images/5b3544c9579cf384cc1b9b283fbc35162bfb3642821be3c4bc78a2086699ab6c.jpg)

![](images/1cfbd42a115143207c4576bd30cd1103ed6923b0138231620bff21f1702204e6.jpg)  
Figure 9.6: Filtered-CoPhy benchmark suite [347]. It contains three challenging scenarios involving 2D or 3D rigid body dynamics with complex interactions, including collision and resting contact. Similar to CoPhy, we can intervene on the initial conditions $\mathbf { X } _ { 0 }$ and set them to $\hat { \bf X }$ . Arrows indicate the initial motion.

liding, as illustrated in Fig. 9.5. It allows us to intervene on the initial conditions of a trajectory and simulate its ground-truth counterfactual.

Filtered-CoPhy: Janny et al. [347] propose a benchmark to test for counterfactual trajectory prediction, where given an observed initial condition $T$ frames ${ \pmb x } _ { 1 : T }$ , one aims to predict a counterfactual sequence $\pmb { x } _ { 1 : T } ^ { \mathrm { C F } }$ $\scriptstyle { \mathbf { x } } _ { 0 }$ under a new initial and a sequence of condition $\pmb { x } _ { 0 } ^ { \mathrm { C F } }$ . There are three types of video data available to test: BlocktowerCF, $B a l l s C F$ and CollisionCF, and confounders that affect the prediction are designed to be obtainable with a sufficiently powerful representation.

Causal3DIdent: K√ºgelgen et al. [41] introduce an image dataset of 3D objects with causal dependencies, that can be used to study the effectiveness of data augmentations for style and content decomposition techniques (Sec. 3.1), which aim at isolating invariant content and discarding varying style.

# 9.3 Natural Language Processing

Constructing causal benchmarks involving text is challenging as the vocabulary size of a text corpus is usually large (e.g., the vocabulary of BERT [395] has around 30K

tokens) and ground-truth data for multiple interventions is rare in natural language processing [349].

Kaushik et al. [396] provide two counterfactual NLP datasets for sentiment analysis and natural language inference, respectively. Given initial documents and labels, the authors recruited humans to revise the documents to conform to a counterfactual target label while ensuring internal consistency and avoiding gratuitous changes to facts unrelated to the target label.

Feder et al. [366] propose four NLP datasets designed for causal explanations, three of which include ground-truth counterfactual examples for a given concept. These datasets allow researchers to evaluate counterfactual representations that dropped a given concept of interest, as further explained in Sec. 8.2.2.

Frohberg and Binder [397] introduce CRASS, a counterfactual reasoning assessment benchmark for language models that is part of the BIG-bench suite [398]. For example, they showcase the query ‚ÄúA woman sees a fire. What would have happened if the woman had fed the fire?‚Äù with three possible answers (a) ‚ÄúThe fire would have become larger‚Äù, (b) ‚ÄúThe fire would have become smaller‚Äù, and (c) ‚ÄúThat is not possible‚Äù. ‚ÄúA women sees a fire‚Äù is the base premise and ‚ÄúWhat would have happened if the woman had fed the fire?‚Äù is a questionized counterfactual conditional (QCC). The possible answers, i.e., a correct consequence and a set of potential effects as distractors, define a so-called premise-counterfactual tuple (PCT). More formally, A QCC has the form ‚ÄúWhat would have happened if ${ \bf A } ^ { C F } \boldsymbol { \ ? }$ , where $\mathbf { A } ^ { \mathrm { C F } }$ is some altered version of the base premise AO, effectively generating the counterfactual.

Yang et al. [399] propose a fine-grained causal reasoning dataset, including the three tasks of causality detection, fine-grained causality extraction and Causal QA tasks. To motivate the need for finer granularity, they give the following example: ‚Äúthe spread of COVID-19 has led to the boom in online shopping, [cause], but it also has deterred [prevent] people from going shopping centres‚Äù. The authors argue that previous datasets only considered the above-annotated cause relation, but not more fine-grained causal events like enable or prevent.

Resuming this example, consider a different passage ‚ÄúCOVID-19 has accelerated change in online shopping, and given Amazon‚Äôs ... it will result in economic returns for years to come and offering more competitive prices compared to an offline business that brings pressures for the offline business recruitment.‚Äù. Previous datasets allow models to extract facts such as ‚ÄúCOVID-19 causes an increase in online shopping‚Äù, yet, they cannot detect the subsequence for Amazon to ‚Äúoffer more competitive prices‚Äù, and the negative influence on offline business recruitment. Both of these can be useful, e.g., if we ask the what-if question ‚ÄúWhat if COVID stops?‚Äù, whose correct answer in the above context should include ‚Äúthere will be more offline business recruitments.‚Äù Their experiments reveal a significant gap between model and human ceiling performance (74.1% vs. $9 0 . 5 3 \%$ accuracy), providing evidence that statistical models still struggle to solve causal reasoning problems.

# The Good, the Bad and the Ugly

In this section, we want to provide our perspective on what benefits CausalML may buy us (the good), what challenges the field is currently struggling with (the bad), and what price one has to pay for CausalML (the ugly). In other words, this section provides an informative discussion of the advantages and disadvantages of current CausalML methods.

# 10.1 The Good

We discussed many methods making explicit use of the various causal formalisms, such as SCMs (Sec. 2.3), interventions (Sec. 2.3.1) or counterfactuals (Sec. 2.3.2). Causality presents a framework (SCMs) to formally express assumptions about the data generating process that we would like to encode into our models and a mathematical tool (the do-operator) that can enforce properties thereof during the model training.

In the following, we briefly summarize some of the benefits causal formalisms can bring us across the discussed problem fields. Many of these benefits cannot be recovered with purely statistical reasoning.

With Causal Supervised Learning (Chapter 3), we can improve predictive models: Invariant feature learning approaches (Sec. 3.1) attempt to identify a set of content variables $\mathbf { C }$ that represented the causal parents of $Y$ , and learn a predictor $p ( y \mid \pmb { c } )$ that is invariant to interventions on the style variables S. In contrast, invariant mechanism learning approaches (Sec. 3.2) model the change of distribution caused by interventions on a set of independent unobserved confounders $\mathbf { U }$ , and they learn separate mechanisms to model for each. To summarize, these methods build the foundation to learn domain-robust, reusable features or mechanisms [11].

Causal Generative Modeling (Chapter 4) offers a principled framework for the task of controllable generation (i.e., generation with control over certain attributes). The structural assignment learning (Sec. 4.1) approaches allow practitioners to add domain knowledge of an underlying causal graph for the DGP. These generate counterfactual samples that consider causal dependencies between the attributes of interest and the other generative variables. The causal disentanglement (Sec. 4.2) approaches go one step further and perform causal graph discovery as well as structural assignment learning, which allows for controllable generation by exploiting weaker forms of available causal domain knowledge.

Through Causal Explanations (Chapter 5), we gain interpretability of model predictions. Without access to the underlying causal dependencies, we can use feature attribution methods (Sec. 5.1) to identify which variables under intervention are most relevant to changes in the model output. With access to the underlying causal graph, we can generate contrastive explanations (Sec. 5.2), which suggest counterfactual model outputs dependent on actionable alternative inputs.

By using Causal Fairness (Chapter 6) criteria, we obtain counterfactual, or interventional, quantities that allow us to evaluate the fairness of prediction models dependent on sensitive attributes of interest. With access to underlying causal dependencies in the DGP, we can enforce case-specific fairness criteria that deconfound potential sources of selection bias.

In Causal Reinforcement Learning (Chapter 7), we reviewed publications that use interventions for formalizing actions (Sec. 7.2), changes in the environment (Sec. 7.4), deconfounding of observed trajectory data (Secs. 7.5 and 7.6), the effects of actions on the reward (Sec. 7.7), changing structure in the state space, exposing opportunities to generate counterfactual data augmentations, allowing us to recycle already observed data.

# 10.2 The Bad

# 10.2.1 Lack of Open-Source Ecosystem

Most open-source machine learning software packages or model hubs focus on observational models. Automatic differentiation frameworks like PyTorch [400], Tensorflow [401] and JAX [402] (including Flax [403] and Haiku [404]) paired with model libraries like Transformers [405], timm [406], or PyG [407] facilitate rapid prototyping of model pipelines. Importing datasets, state-of-the-art (pre-trained) models, and launching a training loop can be done in a few lines of code. Model hubs like Tensorflow Hub, Huggingface Models or Pytorch Hub provide thousands of pre-trained models across multiple modalities ready for fine-tuning and deployment.

Unfortunately, there is a much smaller open source ecosystem for learning or performing inference with SCMs. At the time of this writing, we are unaware of software libraries providing APIs for convenient identification tests, manipulating SCMs, or importing causal benchmarks (Chapter 9) as well as pre-trained SCMs to facilitate progress on novel research ideas.

We believe that the ingredients to either (a) build novel CausalML-focused platforms or (b) incorporate CausalML techniques and models into existing ones are ready. For example, causality researchers often prove causal estimand identification formally by hand, although, e.g., Xia et al. [408] develop an algorithm that verifies identifiability automatically for differentiable SCMs. Providing convenient APIs for such algorithms would be immensely helpful.

For estimating causal estimands using machine learning techniques, there exist some libraries. For example, Scutari [409] and Kalisch et al. [410] propose R packages for causal discovery; Sharma and Kiciman [411], Chen et al. [412], Bach et al. [413] propose Python packages for treatment effect estimation (see more details on these in Sec. 11.2). James Fox et al. [414] introduce a Python library for (multi-agent) causal influence diagrams, which are commonly used for decision-making problems under uncertainty.

# 10.2.2 Lack of Comparisons to Non-Causal Methods

Several CausalML papers lack experimental comparisons to non-causal approaches that solve similar, if not identical, problems. While the methodology may differ, e.g., depending on whether causal estimands are involved, some of these methods claim to improve performance on non-causal metrics, such as accuracy in prediction problems or sample-efficiency in RL setups. This trend of not comparing against non-causal methods evaluated on the same metrics harms the measure of progress and practitioners who have to choose between a growing number of methods.

One area in which we have identified indications of this issue is invariance learning (Sec. 3.1). Some of these methods are motivated by improving a model‚Äôs generalization to out-of-distribution OOD data; however, they do not compare their method against typical domain generalization methods, e.g., as discussed in Gulrajani and Lopez-Paz [415]. For example, Mouli and Ribeiro [52] tackle OOD tasks by learning counterfactually-invariant representations with asymmetry learning (discussed in Sec. 3.1.1.3); yet, in their experiments, they do not compare against any non-causal OOD method (see e.g., Gulrajani and Lopez-Paz [415], Wang et al. [416]).

Another area is the causal model-based RL literature (Sec. 7.3). Sontakke et al. [243] (Sec. 7.4) learn disentangled latent task embeddings, arguing these are more interpretable than previous (non-causal) latent task embedding methods, which define their setup using the formalisms of Hi-Param MDPs [244] and BAMDPs [321, 417]. In the experiments, they compare their agent in terms of sample efficiency against baselines not having access to any task embeddings, excluding Hi-Param MDP and BAMPD approaches (e.g., [245, 417]). Zhang et al. [246], presented in Sec. 7.4, argue that their method makes stronger assumptions on how tasks relate to each other compared to common multi-task RL methods. However, their experiments do not compare against any multi-task RL method (e.g. [245, 330, 418]). Mutti et al. [250] aim at learning an agent that can systematically generalize across a universe, i.e., an infinite set of possible MDP tasks (Sec. 7.4). Their experiments evaluate how well their method approximates the optimal value function but do not compare against non-causal models.

# 10.3 The Ugly

# 10.3.1 Difficulties of Obtaining Ground-Truth Evaluation Data

One of the biggest open problems in CausalML is the lack of public benchmark resources to train and evaluate causal models. Cheng et al. [419] find that the reason for this lack of benchmarks is the difficulty of observing interventions in the real world because the necessary experimental conditions in the form of randomized control trials (RCTs) are often expensive, unethical, or time-consuming. In other words, collecting interventional data involves actively interacting with an environment (i.e., actions), which, outside of simulators, is much harder 1 than, e.g., crawling text from the internet and creating passively-observed datasets (i.e., perception). Evaluating estimated counterfactuals is even worse: by definition, we cannot observe them, rendering the availability of ground-truth real-world counterfactuals impossible [420].

The pessimistic view is that yielding ‚Äúenough‚Äù ground-truth data for CausalML to get deployed in real-world industrial practice is unlikely soon. Specifying how much data is ‚Äúenough‚Äù is task-dependent; however, in other fields that require active interactions with real-world environments, too (e.g., RL), progress has been much slower than in fields thriving on passively-collected data, such as NLP. For example, in robotics, some of the best-funded ML research labs shut down their robotics initiatives due to ‚Äúnot enough training data‚Äù [421], focusing more on generative image and language models trained on crawled internet data.

Moreover, a curse of the need for simulated data is its manipulability and the consequent lack of consistency across papers. Authors can easily create novel or modify existing simulations tailor-made for an empirical verification for their particular setup/method, which may not generalize to other setups. Such inconsistencies hinder progress because it is harder to compare methods. Real-world fixed datasets such as ImageNet [25] are less prone to such problems since papers altering them raise more suspicion.

The optimistic view is that the lack of benchmarks is simply due to the field being in its infancy, and both more available RCT datasets and simulators will alleviate the progress of CausalML methods. About a decade ago, similar problems existed in the field of (deep) RL, which shares a few parallels with CausalML, as debated in Chapter 7. In the meantime, RL simulators paved the way to beating worldchampions in board games [422, 423], achieving super-human performance in Atari [424] and grandmaster level in StarCraft II [425], treating sepsis in intensive care [426], or navigating stratospheric balloons [427].

# 10.3.2 Untestable Assumptions Are Inevitable

By making assumptions about the data-generating process in our SCM, we can reason about interventions and counterfactuals. However, making such assumptions can

also result in bias amplification [428] and harming external validity [429] compared to purely statistical models. Using an analogy of Ockham‚Äôs Razor [430], one may argue that more assumptions lead to wrong models more easily.

For example, Pearl [428] illustrates bias amplification in a setting of hidden confounding (Sec. 11.2.1.2). They show that while adjusting for covariates acting like instrumental variables (i.e., variables that are more strongly associated with the treatment assignment than with the outcome), one may reduce confounding bias, but at the same time, residual bias carried by unmeasured confounders, can build up at a faster rate. Put simply, by making the causal model more complex through adding more covariates that should aid backdoor adjustment, the model residual bias of the causal effect increases in harmful ways. A ‚Äúsimpler‚Äù model that excludes covariates that are predictive of the treatments can work better 2.

# Related Work

# 11.1 Other Surveys

Sch√∂lkopf [431] discusses links between machine learning and graphical causal inference while introducing causality concepts along the way. Sch√∂lkopf et al. [11] reviews fundamental concepts of causal inference and relates them to open machine learning problems with a particular focus on representation learning. They highlight two issues in current deep learning systems: robustness to distribution shifts and learning reusable and modular mechanisms.

Feder et al. [432] examines the intersection of NLP and causality and argues that causal formalisms can make NLP methods more robust and understandable. They list three problems from the NLP literature that motivate this claim: purely associational models may (i) latch onto spurious associations, failing to generalize in OOD settings (e.g., [433]); (ii) exhibit unacceptable performance differences across groups of users (e.g., [372]); (iii) be too inscrutable to incorporate into high-stakes decisions (e.g., [434]).

Cheng et al. [435] discuss how causality may address ethical challenges in socially responsible artificial intelligence. They focus on seven causal inference tools, some of which we have discussed in Chapter 2 (e.g., the do-operator or counterfactual analysis), and some of which we have left out (e.g., mediation analysis or propensity scores).

Liu et al. [436] highlight how causal reasoning can support visual representation learning. Specifically, the authors categorize existing causality-aware visual representation learning work into (i) causal visual comprehension, (ii) causal visual robustness, (iii) causal visual question answering, and (iv) causal datasets. For future work, they enumerate the following potential directions: (i) more reasonable causal relation modeling, (ii) more precise approximations of intervention distributions, (iii) more proper counterfactual synthesizing processes, and (iv) large-scale benchmarks and evaluation pipelines.

Sanchez et al. [437] explore how causal inference can be incorporated into clinical decision support systems using modern machine learning techniques. As a running example throughout their paper, they use Alzheimer‚Äôs disease (AD) to illustrate how CausalML can benefit clinical scenarios. The authors observe that important challenges in healthcare applications are processing high-dimensional and unstructured data, generalization to out-of-distribution samples, and temporal relationships. All

these challenges may be addressed through CausalML, which the authors divide into (i) causal representation learning (Sec. 2.4), (ii) causal discovery (Sec. 11.2.2), and (iii) causal reasoning (estimating interventional distributions, Sec. 2.2.1).

Vlontzos et al. [438] discuss how causality can assist in creating robust and adaptable medical image analysis algorithms. Specifically, the authors point out that many healthcare machine learning approaches fail to translate into clinical practice due to the inability to adapt and be robust to real-world conditions. One cause of such existing models‚Äô inabilities is their lack of ability to distinguish between correlations and causation, which may result in deadly mistakes. For example, [439] identified several approaches that claim to have been able to diagnose COVID-19 from chest X-rays but ultimately failed to do so as they instead relied on spurious features like hospital identifiers and the patient‚Äôs ethnicity.

# 11.2 Machine Learning for Causal Inference

This survey uses causality theory to solve common machine learning problems and provide new perspectives on such. An astute reader may wonder about research in the other direction, using machine learning to answer causal questions. We observe numerous studies using modern representation learning techniques to estimate and answer causal queries, such as causal effects. For completeness, we briefly list recent advances in two common causal inference tasks: causal effect estimation and causal discovery.

# 11.2.1 Causal Effect Estimation

Estimating causal effects from observational data is a fundamental problem in many fields that face challenges in running randomized control trials. We want to answer causal questions in many scientific or commercial setups; hence, it is fallible to argue just from observed associations. Supervised learning methods face two challenges in such settings: (i) missing interventions, i.e., the fact that we only observe one treatment for each individual means models must extrapolate to new treatments without access to ground truth, and (ii) confounding variables affecting both treatment assignment and the outcome, such that extrapolation from observation to intervention requires additional causal assumptions. The literature on treatment effect estimation deals with constructing models that address these issues.

# 11.2.1.1 Observed confounders

Generally speaking, we can identify causal effects if we observe all confounders. However, depending on the structure of the treatment effect (e.g., smoothness or sparsity [218]), different estimators behave differently. For example, Chernozhukov et al. [217] and K√ºnzel et al. [440] show that simple regression models trained on all training data points regardless of their treatment can easily lead to biased estimates due to imbalance in the treatment assignment; Shalit et al. [441] and Shi

et al. [442] make similar arguments for neural network models. Hence, the bulk of treatment effect estimation works focuses on model regularization to identify the causal associations in the data.

One line of work for causal effect estimation that utilizes modern ML techniques are meta-learners (or plug-in learners) [440, 443]: they decompose effect estimation into multiple sub-problems (so-called nuisance components), each solvable using any modern machine learning technique [18, 217, 440, 444].

We highlight a few of these techniques that utilize neural networks. For binary treatments, Curth and van der Schaar [445] implement multiple meta-learning strategies with neural networks, concluding that theoretically-optimal estimators may not perform best in finite-simple regimes. For scalar-continuous treatments, Nie et al. [446] propose using splines to preserve continuity over the treatment domain. For higherdimensional treatments, Kaddour et al. [18] present a meta-learning strategy that learns propensity features. For arbitrary treatments, Zhang et al. [447] utilize Transformer networks [83] to construct a flexible architecture.

For causal estimand identification, Malek and Chiappa [448] use the bandit framework to learn the arm that will produce the best estimator for identifying a causal estimand of interest. They assume an online setting in which the practitioner controls the data collection procedure and aims to find the estimand formula with the lowest asymptotic variance in as few samples as possible. Xia et al. [408] develop a necessary and sufficient algorithm that verifies identifiability automatically for differentiable SCMs. Their approach relies on enforcing $L _ { 1 }$ -consistency with the data distribution.

# 11.2.1.2 Unobserved confounders

There are primarily three strategies to deal with unobserved confounding (Sec. 2.5): (1) estimating bounds based on additional assumptions, also referred to as partial identification [449, 450], (2) sensitivity analysis of how strong the confounder‚Äôs effect has to be to make the true estimand substantially different from our estimate [451, 452], and (3) utilizing other observed variables, such as instrumental [453] or proxy variables [454]. Specifically for the third category, some recent work explores ML techniques, such as NNs or kernel methods, discussed below.

Proxy Variables: Proxy variables contain relevant auxiliary information on the confounder; ideally, enough to completely recover the confounder [454, 455]. For example, we may be interested in estimating the impact of flight ticket prices on sales [456]. A possible confounder is the people‚Äôs desire to fly, e.g., driven by holiday seasons that affect both the number of ticket sales and the prices customers are willing to pay. A suitable proxy variable for this desire could be the number of views of the ticket reservation page. While simply adjusting on such a proxy variable alone would produce a biased effect estimate, there are recent methods aiming at producing less biased estimates [457, 458, 459, 460, 461].

Instrumental Variable Regression: Another class of methods rely on an instrumental variable $I$ (often just called instrument) [462, 463, 464, 465, 466]. Three conditions have to hold for an instrument to be valid: (i) it is independent of the hidden confounder, $I \perp \perp C$ ; (ii) it is not independent of the treatment, $I \not \perp T$ ; and (iii) it is independent of the outcome conditional on the treatment and confounder $I \perp \perp \boldsymbol { Y } \mid \{ \boldsymbol { T } , \boldsymbol { C } \}$ .

In the above flight ticket example, we might consider shifting supply factors, such as the oil price, as a valid instrument, as it only affects sales via price, thereby identifying the customers‚Äô demand [463].

Sometimes, we observe treatment corrupted by measurement error, e.g., when we do not observe it directly. For instance, in the setting of medical treatments, suppose patients are asked to take a drug at home instead of a hospital with supervision. Some of them may not comply, and a self-reported measurement is erroneous due to the patient lying or being forgetful. Zhu et al. [467] propose a method to deal with such measurement error scenarios assuming access to an instrument.

# 11.2.2 Causal Discovery

Causal Discovery is an umbrella term for methods that try to recover the underlying causal structure of a DGP from observational and/or interventional data. Depending on the assumptions one is willing to make, the output ranges from partial node orderings to the full SCM (assuming linear structural equations).

Generally, causal discovery is difficult due to (i) structure identifiability and (ii) computational complexity. First, the identifiability challenge means that, given observational data only, the causal DAG $\mathcal { G }$ is typically not identifiable, as a set of possible graphs could have generated the data [12]. Secondly, due to the combinatorial nature of the solution space, its size grows super-exponentially with the number of variables, rendering naive methods computationally infeasible [468].

We point keen readers looking for more details on these methods to primarily three classes: (i) combinatorial methods [469, 470, 471, 472, 473, 474, 475], (ii) continuous relaxations [476, 477, 478, 479, 480, 481, 482, 483], and (iii) permutation-based methods [484, 485, 486, 487]. Further, Squires and Uhler [472], Vowels et al. [488] provide excellent surveys.

Recovering the full causal graph is especially difficult in high dimensions. Hence, one line of work focused on more specialized problems, e.g., more coarse-grained [489] ancestral structures, partial ancestral graphs [490], or the causal order among a subgraph of variables known to descend from some set of confounding covariates [491]. Another line of work aims to perform causal discovery actively to improve data efficiency and minimize the number of required interventions [492, 493, 494, 495].

# 11.2.3 Granger Causality

In many scientific fields, such as neuroscience, econometrics, and civil engineering, it is important to understand causal relationships found in time series data. In neuroscience, for example, researchers study whether activity in one brain region correlates with activity in another [496]. An econometrician may be interested in which macroeconomic indicators predict one another [497]. Civil engineers are interested in understanding differences in traffic across highways to build well-utilized transportation infrastructure [498].

Granger causality [499] is a framework for time series structure discovery that quantifies the extent to which the past of one time series aids in predicting the future evolution of another time series. It is based on the simple assumption that causes precede their effects.

# Def.: 11.2.1: Granger Causality [7, 499]

Whenever past observations of $X$ help in predicting the future of time-series $Y$ , then $X$ Granger-causes $Y$ . Formally, we write

$$
X \text {G r a n g e r - c a u s e s} Y: \Longleftrightarrow Y _ {t} \not \leftarrow X _ {\text {p a s t} (t)} \mid Y _ {\text {p a s t} (t)}. \tag {11.1}
$$

If our goal is to detect the underlying causal DAG of multiple variables observed over time, we can use Granger causality only sometimes. Specifically, we can recover the DAG if we observe all relevant variables (implying no unobserved confounding, see Sec. 2.6) and no instantaneous effects exist, i.e., when no variable causes another at the same time step. This condition and other limitations are explained in more detail by Peters et al. [7, Chapter 10].

There are several interesting extensions of Granger causality. Originally, Granger causality was defined using linear relations. Tank et al. [500] extended it to the non-linear case.

Wu et al. [501] proposes to combine the benefits of Granger causality with deep learning models. They propose relational learning with minimum predictive information (MPI) regularization. This MPI regularizer quantifies the directional predictive strength between each pair of time series.

Khanna and Tan [502] utilize Statistical Recurrent Units (SRUs) to model the network topology of Granger causal relations. Specifically, they explain that the structured sparse estimate of the internal parameters of the SRU networks, trained to predict the stochastic processes‚Äô time series measurements, relate to the Granger causal relations. Moreover, by regularizing a low-parameterized variant of SRUs, called economy-SRU, towards interpretability of its learned predictive features, this model is less likely to overfit the time series data.

L√∂we et al. [503] point out that most causal discovery methods based on Granger causality fit a new model whenever they encounter samples from a new underlying causal graph. However, time series with different underlying causal graphs may

share relevant information, e.g., when inferring synaptic brain connections between neurons based on their spiking behavior: test subjects may have varying brain connectivity but the same underlying neurochemistry. Hence, they propose Amortized Causal Discovery, an approach that separates the causal relation prediction from modeling the time series dynamics, permitting us to aggregate statistical strength across samples.

# Conclusion

We summarize some of our key findings:

1. Causal Inference (Chapter 2) encodes causal assumptions about a system of interest. Therefore, as opposed to conventional statistical or probabilistic inference, it allows us to reason about interventional and counterfactual estimands.   
2. We argue that these estimands benefit specific areas of ML research, namely:

(a) Causal Supervised Learning (Chapter 3) improves predictive generalization by learning invariant features or mechanisms, both aiming at deconfounding models‚Äô reliance w.r.t. reliance on spurious associations. Future work should investigate targeted benchmarks testing for learned invariances, connections to adversarial robustness and meta-learning, and the potential utilization of additional supervision signals.   
(b) Causal Generative Modeling (Chapter 4) supports sampling from interventional or counterfactual distributions, naturally performing principled controllable generation or sample editing tasks, respectively. All existing methods learn structural assignments; some infer the causal structure from data. However, it is underexplored what levels of abstractions should be considered for different applications, how to scale assignment learning up to larger graphs, and when counterfactually-generated data augmentations are effective (and when not).   
(c) Causal Explanations (Chapter 5) explain model predictions while accounting for the causal structure of either the model mechanics or the datagenerating process. We divide these methods into (i) feature attributions, which quantify the causal impact of input features, and (ii) contrastive explanations, representing altered instances achieving the desired outcome. So far, it is unclear how to unify both classes of methodology best, scale up explanations, make them robust against distribution shifts, secure and private against attackers, and circumvent the inevitable trade-off of their robustness against recourse sensitivity.   
(d) Causal Fairness (Chapter 6) paves the way for criteria that assess a model‚Äôs fairness as well as mitigate harmful disparities w.r.t. causal relationships of the underlying data. However, the criteria rely on either counterfactual or interventional distributions. Future work should shed light on alternatives to equality, fairness outside standard prediction settings, weaker observability

assumptions (e.g., hidden confounding), and the validity of interventionist views on social categories.

(e) Causal Reinforcement Learning (Chapter 7) describes RL methods taking the explicit causal structure of the decision-making environment into account. We categorize these methods into eight categories and observe that their claimed benefits over non-causal methods include deconfounding (resulting in better generalization), intrinsic rewards, and data efficiency. Open problems suggest that some formalisms might be unifiable, deconfounding of offline data has been largely unaddressed in offline RL sections, and agents making decisions based on counterfactuals might offer further benefits.   
(f) Modality-Applications: We review how previously introduced, and modality-specific principles provide opportunities to improve computer vision, natural language processing, and graph representation learning settings.

3. We review existing Causal Benchmarks designed for CausalML methods, including ground-truth causal interventions and counterfactuals.   
4. We discussed some of the benefits of using CausalML methodology, as well as issues around untestable assumptions, lack of benchmarks, lack of software, and lack of comparisons to non-causal methods.   
5. We touched upon two categories of previous related work; other CausalML surveys and research that utilizes machine learning to infer causal estimands.

Acknowledgements: In alphabetical order, we thank Damien Teney, David Watson, Jakob Zeitler, Limor Gultchin, Phillip Lippe, Stefano Blumberg, and Tom Everitt for helpful feedback.

[1] A. D‚ÄôAmour, K. Heller, D. Moldovan, B. Adlam, B. Alipanahi, A. Beutel, C. Chen, J. Deaton, J. Eisenstein, M. D. Hoffman et al., ‚ÄúUnderspecification presents challenges for credibility in modern machine learning,‚Äù arXiv preprint arXiv:2011.03395, 2020.   
[2] A. Jahanian*, L. Chai*, and P. Isola, ‚ÄúOn the "steerability" of generative adversarial networks,‚Äù in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id= HylsTT4FvB   
[3] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, ‚ÄúA survey on bias and fairness in machine learning,‚Äù ACM Computing Surveys (CSUR), vol. 54, no. 6, pp. 1‚Äì35, 2021.   
[4] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, ‚ÄúOn the dangers of stochastic parrots: Can language models be too big?‚Äù in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021, pp. 610‚Äì623.   
[5] Z. C. Lipton, ‚ÄúThe mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.‚Äù Queue, vol. 16, no. 3, pp. 31‚Äì57, 2018.   
[6] G. Dulac-Arnold, D. Mankowitz, and T. Hester, ‚ÄúChallenges of real-world reinforcement learning,‚Äù arXiv preprint arXiv:1904.12901, 2019.   
[7] J. Peters, D. Janzing, and B. Sch√∂lkopf, Elements of Causal Inference - Foundations and Learning Algorithms, ser. Adaptive Computation and Machine Learning Series. Cambridge, MA, USA: The MIT Press, 2017.   
[8] A. Goyal and Y. Bengio, ‚ÄúInductive biases for deep learning of higher-level cognition,‚Äù arXiv preprint arXiv:2011.15091, 2020.   
[9] J. Pearl, ‚ÄúThe seven tools of causal inference, with reflections on machine learning,‚Äù Communications of the ACM, vol. 62, no. 3, pp. 54‚Äì60, 2019.   
[10] O. Ahmed, F. Tr√§uble, A. Goyal, A. Neitz, Y. Bengio, B. Sch√∂lkopf, M. W√ºthrich, and S. Bauer, ‚ÄúCausalworld: A robotic manipulation benchmark for causal structure and transfer learning,‚Äù arXiv preprint arXiv:2010.04296, 2020.   
[11] B. Sch√∂lkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio, ‚ÄúTowards causal representation learning,‚Äù arXiv preprint arXiv:2102.11107, 2021.

[12] J. Pearl, Causality. Cambridge university press, 2009.   
[13] B. Neal, ‚ÄúIntroduction to causal inference,‚Äù 2020.   
[14] B. Sch√∂lkopf and J. von K√ºgelgen, ‚ÄúFrom statistical to causal learning,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2204.00607   
[15] A. Van Oord, N. Kalchbrenner, and K. Kavukcuoglu, ‚ÄúPixel recurrent neural networks,‚Äù in International conference on machine learning. PMLR, 2016, pp. 1747‚Äì1756.   
[16] A. van den Oord, N. Kalchbrenner, L. Espeholt, K. Kavukcuoglu, O. Vinyals, and A. Graves, ‚ÄúConditional image generation with pixelcnn decoders,‚Äù in Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, Eds., 2016, pp. 4790‚Äì4798. [Online]. Available: https://proceedings.neurips. cc/paper/2016/hash/b1301141feffabac455e1f90a7de2054-Abstract.html   
[17] Y. Yu, X. Si, C. Hu, and J. Zhang, ‚ÄúA review of recurrent neural networks: Lstm cells and network architectures,‚Äù Neural computation, vol. 31, no. 7, pp. 1235‚Äì1270, 2019.   
[18] J. Kaddour, Y. Zhu, Q. Liu, M. J. Kusner, and R. Silva, ‚ÄúCausal effect inference for structured treatments,‚Äù in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp. 24 841‚Äì24 854. [Online]. Available: https://proceedings.neurips.cc/paper/ 2021/hash/d02e9bdc27a894e882fa0c9055c99722-Abstract.html   
[19] F. Husz√°r, ‚ÄúMl beyond curve fitting: An intro to causal inference and do-calculus,‚Äù 2018. [Online]. Available: https://www.inference.vc/untitled/   
[20] M. Glymour, J. Pearl, and N. P. Jewell, Causal inference in statistics: A primer. John Wiley & Sons, 2016.   
[21] E. Bareinboim, J. D. Correa, D. Ibeling, and T. Icard, ‚ÄúOn pearl‚Äôs hierarchy and the foundations of causal inference,‚Äù in Probabilistic and Causal Inference: The Works of Judea Pearl, 2022, pp. 507‚Äì556.   
[22] V. Papyan, X. Han, and D. L. Donoho, ‚ÄúPrevalence of neural collapse during the terminal phase of deep learning training,‚Äù Proceedings of the National Academy of Sciences, vol. 117, no. 40, pp. 24 652‚Äì24 663, 2020.   
[23] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann, ‚ÄúShortcut learning in deep neural networks,‚Äù Nature Machine Intelligence, vol. 2, no. 11, pp. 665‚Äì673, 2020.

[24] Y. Wang and M. I. Jordan, ‚ÄúDesiderata for representation learning: A causal perspective,‚Äù arXiv preprint arXiv:2109.03795, 2021.   
[25] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet: A large-scale hierarchical image database,‚Äù in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248‚Äì255.   
[26] S. Singla and S. Feizi, ‚ÄúSalient imagenet: How to discover spurious features in deep learning?‚Äù in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=XVPqLyNxSyh   
[27] I. Shpitser and J. Pearl, ‚ÄúComplete identification methods for the causal hierarchy,‚Äù Journal of Machine Learning Research, vol. 9, no. 64, pp. 1941‚Äì 1979, 2008. [Online]. Available: http://jmlr.org/papers/v9/shpitser08a.html   
[28] E. Perkoviƒá, J. Textor, M. Kalisch, and M. H. Maathuis, ‚ÄúComplete graphical characterization and construction of adjustment sets in markov equivalence classes of ancestral graphs,‚Äù Journal of Machine Learning Research, vol. 18, no. 220, pp. 1‚Äì62, 2018. [Online]. Available: http: //jmlr.org/papers/v18/16-319.html   
[29] L. Gultchin, M. J. Kusner, V. Kanade, and R. Silva, ‚ÄúDifferentiable causal backdoor discovery,‚Äù in The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], ser. Proceedings of Machine Learning Research, S. Chiappa and R. Calandra, Eds., vol. 108. PMLR, 2020, pp. 3970‚Äì3979. [Online]. Available: http://proceedings.mlr.press/v108/gultchin20a.html   
[30] M. A. Hern√°n and J. M. Robins, ‚ÄúCausal inference,‚Äù 2010.   
[31] D. Janzing, D. Balduzzi, M. Grosse-Wentrup, and B. Sch√∂lkopf, ‚ÄúQuantifying causal influences,‚Äù The Annals of Statistics, vol. 41, no. 5, pp. 2324‚Äì2358, 2013.   
[32] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz, ‚ÄúInvariant risk minimization,‚Äù CoRR, vol. abs/1907.02893, 2019. [Online]. Available: http://arxiv.org/abs/1907.02893   
[33] Z. Shen, J. Liu, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui, ‚ÄúTowards out-ofdistribution generalization: A survey,‚Äù arXiv preprint arXiv:2108.13624, 2021.   
[34] S. Beery, G. Van Horn, and P. Perona, ‚ÄúRecognition in terra incognita,‚Äù in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 456‚Äì473.   
[35] J. B. Tenenbaum and W. T. Freeman, ‚ÄúSeparating style and content,‚Äù in Advances in Neural Information Processing Systems 9, NIPS, Denver, CO, USA, December 2-5, 1996, M. Mozer, M. I. Jordan, and T. Petsche, Eds. MIT Press, 1996, pp. 662‚Äì668. [Online]. Available: http://papers.nips.cc/ paper/1290-separating-style-and-content

[36] M. Gong, K. Zhang, T. Liu, D. Tao, C. Glymour, and B. Sch√∂lkopf, ‚ÄúDomain adaptation with conditional transferable components,‚Äù in Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, ser. JMLR Workshop and Conference Proceedings, M. Balcan and K. Q. Weinberger, Eds., vol. 48. JMLR.org, 2016, pp. 2839‚Äì2848. [Online]. Available: http://proceedings.mlr.press/v48/gong16.html   
[37] C. Heinze-Deml and N. Meinshausen, ‚ÄúConditional variance penalties and domain shift robustness,‚Äù Mach. Learn., vol. 110, no. 2, pp. 303‚Äì348, 2021. [Online]. Available: https://doi.org/10.1007/s10994-020-05924-1   
[38] J. Peters, P. B√ºhlmann, and N. Meinshausen, ‚ÄúCausal inference by using invariant prediction: identification and confidence intervals,‚Äù Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 78, no. 5, pp. 947‚Äì1012, 2016.   
[39] J. Mitrovic, B. McWilliams, J. C. Walker, L. H. Buesing, and C. Blundell, ‚ÄúRepresentation learning via invariant causal mechanisms,‚Äù in International Conference on Learning Representations, 2021.   
[40] M. Ilse, J. M. Tomczak, and P. Forr√©, ‚ÄúSelecting data augmentation for simulating interventions,‚Äù in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 2021, pp. 4555‚Äì4562. [Online]. Available: http://proceedings.mlr.press/v139/ilse21a.html   
[41] J. V. K√ºgelgen, Y. Sharma, L. Gresele, W. Brendel, B. Sch√∂lkopf, M. Besserve, and F. Locatello, ‚ÄúSelf-supervised learning with data augmentations provably isolates content from style,‚Äù in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available: https://openreview.net/forum?id=4pf_pOo0Dt   
[42] D. Kaushik, A. Setlur, E. H. Hovy, and Z. C. Lipton, ‚ÄúExplaining the efficacy of counterfactually augmented data,‚Äù in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [Online]. Available: https://openreview.net/forum?id=HHiiQKWsOcV   
[43] D. Teney, E. Abbasnejad, and A. van den Hengel, ‚ÄúLearning what makes a difference from counterfactual examples and gradient supervision,‚Äù in Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part X, ser. Lecture Notes in Computer Science, A. Vedaldi, H. Bischof, T. Brox, and J. Frahm, Eds., vol. 12355. Springer, 2020, pp. 580‚Äì599. [Online]. Available: https://doi.org/10.1007/978-3-030-58607-2_34

[44] C. Mao, A. Cha, A. Gupta, H. Wang, J. Yang, and C. Vondrick, ‚ÄúGenerative interventions for causal learning,‚Äù in IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. Computer Vision Foundation / IEEE, 2021, pp. 3947‚Äì3956. [Online]. Available: https://openaccess.thecvf.com/content/CVPR2021/html/Mao Generative_Interventions_for_Causal_Learning_CVPR_2021_paper.html   
[45] E. H√§rk√∂nen, A. Hertzmann, J. Lehtinen, and S. Paris, ‚ÄúGanspace: Discovering interpretable gan controls,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 9841‚Äì9850, 2020.   
[46] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classification with deep convolutional neural networks,‚Äù Advances in neural information processing systems, vol. 25, 2012.   
[47] C. Mao, K. Xia, J. Wang, H. Wang, J. Yang, E. Bareinboim, and C. Vondrick, ‚ÄúCausal transportability for visual recognition,‚Äù arXiv preprint arXiv:2204.12363, 2022.   
[48] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ‚ÄúA simple framework for contrastive learning of visual representations,‚Äù in ICML, 2020.   
[49] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, ‚ÄúUnsupervised learning of visual features by contrasting cluster assignments,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 9912‚Äì9924, 2020.   
[50] K. Xiao, L. Engstrom, A. Ilyas, and A. Madry, ‚ÄúNoise or signal: The role of image backgrounds in object recognition,‚Äù arXiv preprint arXiv:2006.09994, 2020.   
[51] V. Veitch, A. D‚ÄôAmour, S. Yadlowsky, and J. Eisenstein, ‚ÄúCounterfactual invariance to spurious correlations in text classification,‚Äù in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available: https: //openreview.net/forum?id=BdKxQp0iBi8   
[52] S. C. Mouli and B. Ribeiro, ‚ÄúAsymmetry learning for counterfactuallyinvariant classification in OOD tasks,‚Äù in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/ forum?id=avgclFZ221l   
[53] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., ‚ÄúLanguage models are few-shot learners,‚Äù in NeurIPS, 2020.   
[54] L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara, and D. Rueckert, ‚ÄúSelfsupervised learning for medical image analysis using image context restoration,‚Äù Medical image analysis, vol. 58, p. 101539, 2019.

[55] H. Wang, J. Kaddour, S. Liu, J. Tang, M. Kusner, J. Lasenby, and Q. Liu, ‚ÄúEvaluating self-supervised learning for molecular graph embeddings,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2206.08005   
[56] K. He, X. Chen, S. Xie, Y. Li, P. Doll√°r, and R. B. Girshick, ‚ÄúMasked autoencoders are scalable vision learners,‚Äù arXiv:2111.06377, 2021.   
[57] Z. Wu, Y. Xiong, S. Yu, and D. Lin, ‚ÄúUnsupervised feature learning via non-parametric instance-level discrimination,‚Äù 2018. [Online]. Available: https://arxiv.org/abs/1805.01978   
[58] J.-B. Grill, F. Strub, F. Altch√©, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar et al., ‚ÄúBootstrap your own latent: A new approach to self-supervised learning,‚Äù in NeurIPS, 2020.   
[59] Y. Tian, D. Krishnan, and P. Isola, ‚ÄúContrastive multiview coding,‚Äù in European conference on computer vision. Springer, 2020, pp. 776‚Äì794.   
[60] N. Tomasev, I. Bica, B. McWilliams, L. Buesing, R. Pascanu, C. Blundell, and J. Mitrovic, ‚ÄúPushing the limits of self-supervised resnets: Can we outperform supervised learning without labels on imagenet?‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2201.05119   
[61] G. Chen, J. Li, J. Lu, and J. Zhou, ‚ÄúHuman trajectory prediction via counterfactual analysis,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/ 2107.14202   
[62] Y. Niu, K. Tang, H. Zhang, Z. Lu, X.-S. Hua, and J.-R. Wen, ‚ÄúCounterfactual vqa: A cause-effect look at language bias,‚Äù in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021, pp. 12 700‚Äì12 710.   
[63] Y. Rao, G. Chen, J. Lu, and J. Zhou, ‚ÄúCounterfactual attention learning for fine-grained visual categorization and re-identification,‚Äù in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1025‚Äì 1034.   
[64] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao et al., ‚ÄúWilds: A benchmark of in-the-wild distribution shifts,‚Äù in International Conference on Machine Learning. PMLR, 2021, pp. 5637‚Äì5664.   
[65] E. Rosenfeld, P. K. Ravikumar, and A. Risteski, ‚ÄúThe risks of invariant risk minimization,‚Äù in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview.net/forum?id=BbNIbVPJ-42   
[66] P. Kamath, A. Tangella, D. Sutherland, and N. Srebro, ‚ÄúDoes invariant risk minimization capture invariance?‚Äù in International Conference on Artificial Intelligence and Statistics. PMLR, 2021, pp. 4069‚Äì4077.

[67] K. Ahuja, K. Shanmugam, K. Varshney, and A. Dhurandhar, ‚ÄúInvariant risk minimization games,‚Äù in International Conference on Machine Learning. PMLR, 2020, pp. 145‚Äì155.   
[68] K. Ahuja, E. Caballero, D. Zhang, J.-C. Gagnon-Audet, Y. Bengio, I. Mitliagkas, and I. Rish, ‚ÄúInvariance principle meets information bottleneck for out-of-distribution generalization,‚Äù Advances in Neural Information Processing Systems, vol. 34, 2021.   
[69] N. Tishby, F. C. Pereira, and W. Bialek, ‚ÄúThe information bottleneck method,‚Äù arXiv preprint physics/0004057, 2000.   
[70] D. Krueger, E. Caballero, J. Jacobsen, A. Zhang, J. Binas, D. Zhang, R. L. Priol, and A. C. Courville, ‚ÄúOut-of-distribution generalization via risk extrapolation (rex),‚Äù in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 2021, pp. 5815‚Äì5826. [Online]. Available: http://proceedings.mlr.press/v139/krueger21a.html   
[71] Y. Jiang and V. Veitch, ‚ÄúInvariant and transportable representations for anticausal domain shifts,‚Äù arXiv preprint arXiv:2207.01603, 2022.   
[72] H. Wang, H. Si, B. Li, and H. Zhao, ‚ÄúProvable domain generalization via invariant-feature subspace recovery,‚Äù in International Conference on Machine Learning. PMLR, 2022, pp. 23 018‚Äì23 033. [Online]. Available: https://proceedings.mlr.press/v162/wang22x.html   
[73] D. Mahajan, S. Tople, and A. Sharma, ‚ÄúDomain generalization using causal matching,‚Äù in Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 18‚Äì24 Jul 2021, pp. 7313‚Äì7324. [Online]. Available: https://proceedings.mlr.press/v139/mahajan21b.html   
[74] X. Sun, B. Wu, X. Zheng, C. Liu, W. Chen, T. Qin, and T.-Y. Liu, ‚ÄúRecovering latent causal factor for generalization to distributional shifts,‚Äù in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available: https://openreview.net/forum?id=go3GvM7aFD   
[75] C. Liu, X. Sun, J. Wang, T. Li, T. Qin, W. Chen, and T.-Y. Liu, ‚ÄúLearning causal semantic representation for out-of-distribution prediction,‚Äù 11 2020.   
[76] C. Lu, Y. Wu, J. M. Hern√°ndez-Lobato, and B. Sch√∂lkopf, ‚ÄúInvariant causal representation learning for out-of-distribution generalization,‚Äù in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=-e4EXDWXnSn   
[77] P. Spirtes, C. N. Glymour, R. Scheines, and D. Heckerman, Causation, prediction, and search. MIT press, 2000.

[78] Y. Atzmon, F. Kreuk, U. Shalit, and G. Chechik, ‚ÄúA causal view of compositional zero-shot recognition,‚Äù arXiv:2006.14610 [cs], Nov. 2020, arXiv: 2006.14610. [Online]. Available: http://arxiv.org/abs/2006.14610   
[79] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick, ‚ÄúClevr: A diagnostic dataset for compositional language and elementary visual reasoning,‚Äù in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2901‚Äì2910.   
[80] G. Parascandolo, N. Kilbertus, M. Rojas-Carulla, and B. Sch√∂lkopf, ‚ÄúLearning independent causal mechanisms,‚Äù in Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, J. Dy and A. Krause, Eds., vol. 80. PMLR, 10‚Äì15 Jul 2018, pp. 4036‚Äì4044. [Online]. Available: https: //proceedings.mlr.press/v80/parascandolo18a.html   
[81] A. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, Y. Bengio, and B. Sch√∂lkopf, ‚ÄúRecurrent independent mechanisms,‚Äù in International Conference on Learning Representations, 2021. [Online]. Available: https: //openreview.net/forum?id=mLcmdlEUxy-  
[82] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural Computation, vol. 9, no. 8, pp. 1735‚Äì1780, 1997.   
[83] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù 2017. [Online]. Available: https://arxiv.org/abs/1706.03762   
[84] K. Madan, N. R. Ke, A. Goyal, B. Sch√∂lkopf, and Y. Bengio, ‚ÄúFast and slow learning of recurrent independent mechanisms,‚Äù 2021. [Online]. Available: https://openreview.net/forum?id=Lc28QAB4ypz   
[85] Z. Yue, Q. Sun, X.-S. Hua, and H. Zhang, ‚ÄúTransporting causal mechanisms for unsupervised domain adaptation,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2107.11055   
[86] J. Pearl and E. Bareinboim, ‚ÄúExternal validity: From do-calculus to transportability across populations,‚Äù Statistical Science, vol. 29, no. 4, nov 2014.   
[87] T. Teshima, I. Sato, and M. Sugiyama, ‚ÄúFew-shot domain adaptation by causal mechanism transfer,‚Äù in International Conference on Machine Learning. PMLR, 2020, pp. 9458‚Äì9469.   
[88] Z. Liu, P. Luo, X. Wang, and X. Tang, ‚ÄúDeep learning face attributes in the wild,‚Äù in Proceedings of International Conference on Computer Vision (ICCV), December 2015.   
[89] E. Grant, C. Finn, S. Levine, T. Darrell, and T. L. Griffiths, ‚ÄúRecasting gradient-based meta-learning as hierarchical bayes,‚Äù in 6th International

Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [Online]. Available: https://openreview.net/forum?id=BJ_UL-k0b   
[90] V. Mikulik, G. Del√©tang, T. McGrath, T. Genewein, M. Martic, S. Legg, and P. Ortega, ‚ÄúMeta-trained agents implement bayes-optimal agents,‚Äù Advances in neural information processing systems, vol. 33, pp. 18 691‚Äì18 703, 2020.   
[91] J. Kaddour, S. S√¶mundsson, and M. P. Deisenroth, ‚ÄúProbabilistic active metalearning,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips. cc/paper/2020/hash/ef0d17b3bdb4ee2aa741ba28c7255c53-Abstract.html   
[92] K. Sohn, H. Lee, and X. Yan, ‚ÄúLearning structured output representation using deep conditional generative models,‚Äù in NIPS, 2015.   
[93] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, ‚ÄúScore-based generative modeling through stochastic differential equations,‚Äù arXiv preprint arXiv:2011.13456, 2020.   
[94] N. Pawlowski, D. Coelho de Castro, and B. Glocker, ‚ÄúDeep structural causal models for tractable counterfactual inference,‚Äù Advances in Neural Information Processing Systems, vol. 33, 2020.   
[95] A. Sauer and A. Geiger, ‚ÄúCounterfactual generative networks,‚Äù in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview.net/forum?id=BXewfAYMmJw   
[96] F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Sch√∂lkopf, and O. Bachem, ‚ÄúChallenging common assumptions in the unsupervised learning of disentangled representations,‚Äù in international conference on machine learning. PMLR, 2019, pp. 4114‚Äì4124.   
[97] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen, ‚ÄúVariational autoencoders and nonlinear ica: A unifying framework,‚Äù in International Conference on Artificial Intelligence and Statistics. PMLR, 2020, pp. 2207‚Äì2217.   
[98] P. Sanchez-Martin, M. Rateike, and I. Valera, ‚ÄúVaca: Design of variational graph autoencoders for interventional and counterfactual queries.‚Äù arXiv, 2021. [Online]. Available: https://arxiv.org/abs/2110.14690   
[99] H. Kim, S. Shin, J. Jang, K. Song, W. Joo, W. Kang, and I.-C. Moon, ‚ÄúCounterfactual fairness with disentangled causal effect variational autoencoder,‚Äù 2020. [Online]. Available: https://arxiv.org/abs/2011.11878   
[100] X. Shen, F. Liu, H. Dong, Q. Lian, Z. Chen, and T. Zhang, ‚ÄúDisentangled generative causal representation learning,‚Äù arXiv preprint arXiv:2010.02637, 2020.

[101] P. Sanchez and S. A. Tsaftaris, ‚ÄúDiffusion causal models for counterfactual estimation,‚Äù arXiv preprint arXiv:2202.10166, 2022.   
[102] F. Locatello, B. Poole, G. R√§tsch, B. Sch√∂lkopf, O. Bachem, and M. Tschannen, ‚ÄúWeakly-supervised disentanglement without compromises,‚Äù in International Conference on Machine Learning. PMLR, 2020, pp. 6348‚Äì6359.   
[103] I. Kobyzev, S. J. Prince, and M. A. Brubaker, ‚ÄúNormalizing flows: An introduction and review of current methods,‚Äù IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 3964‚Äì3979, nov 2021.   
[104] D. P. Kingma and M. Welling, ‚ÄúAuto-encoding variational bayes,‚Äù 2013. [Online]. Available: https://arxiv.org/abs/1312.6114   
[105] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, ‚ÄúGenerative adversarial nets,‚Äù Advances in neural information processing systems, vol. 27, 2014.   
[106] J. Ho, A. Jain, and P. Abbeel, ‚ÄúDenoising diffusion probabilistic models,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 6840‚Äì6851, 2020.   
[107] P. Dhariwal and A. Nichol, ‚ÄúDiffusion models beat gans on image synthesis,‚Äù Advances in Neural Information Processing Systems, vol. 34, 2021.   
[108] A. Brock, J. Donahue, and K. Simonyan, ‚ÄúLarge scale gan training for high fidelity natural image synthesis,‚Äù arXiv preprint arXiv:1809.11096, 2018.   
[109] M. Yang, F. Liu, Z. Chen, X. Shen, J. Hao, and J. Wang, ‚ÄúCausalvae: Disentangled representation learning via neural structural causal models,‚Äù in IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. Computer Vision Foundation / IEEE, 2021, pp. 9593‚Äì9602. [Online]. Available: https://openaccess.thecvf.com/content/CVPR2021/html/ Yang_CausalVAE_Disentangled_Representation_Learning_via_Neural Structural_Causal_Models_CVPR_2021_paper.html   
[110] J. Brehmer, P. De Haan, P. Lippe, and T. Cohen, ‚ÄúWeakly supervised causal representation learning,‚Äù arXiv preprint arXiv:2203.16437, 2022.   
[111] P. Lippe, S. Magliacane, S. L√∂we, Y. M. Asano, T. Cohen, and E. Gavves, ‚ÄúCitris: Causal identifiability from temporal intervened sequences,‚Äù arXiv preprint arXiv:2202.03169, 2022.   
[112] P. Lippe, S. Magliacane, S. L√∂we, Y. M. Asano, T. Cohen, and E. Gavves, ‚Äúicitris: Causal representation learning for instantaneous temporal effects,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2206.06169   
[113] K. Chalupka, F. Eberhardt, and P. Perona, ‚ÄúCausal feature learning: an overview,‚Äù Behaviormetrika, vol. 44, no. 1, pp. 137‚Äì164, 2017.

[114] S. Beckers and J. Y. Halpern, ‚ÄúAbstracting causal models,‚Äù in Proceedings of the aaai conference on artificial intelligence, vol. 33, no. 01, 2019, pp. 2678‚Äì 2685.   
[115] S. Beckers, F. Eberhardt, and J. Y. Halpern, ‚ÄúApproximate causal abstractions,‚Äù in Uncertainty in Artificial Intelligence. PMLR, 2020, pp. 606‚Äì615.   
[116] D. Kinney and D. Watson, ‚ÄúCausal feature learning for utility-maximizing agents,‚Äù in International conference on probabilistic graphical models. PMLR, 2020, pp. 257‚Äì268.   
[117] L. Gresele, P. K. Rubenstein, A. Mehrjou, F. Locatello, and B. Sch√∂lkopf, ‚ÄúThe incomplete rosetta stone problem: Identifiability results for multi-view nonlinear ica,‚Äù in Uncertainty in Artificial Intelligence. PMLR, 2020, pp. 217‚Äì227.   
[118] T. Cohen, ‚ÄúTowards a grounded theory of causation for embodied ai,‚Äù arXiv preprint arXiv:2206.13973, 2022.   
[119] K. Sachs, O. Perez, D. Pe‚Äôer, D. A. Lauffenburger, and G. P. Nolan, ‚ÄúCausal protein-signaling networks derived from multiparameter single-cell data,‚Äù Science, vol. 308, no. 5721, pp. 523‚Äì529, 2005.   
[120] N. Guelzim, S. Bottani, P. Bourgine, and F. K√©p√®s, ‚ÄúTopological and causal structure of the yeast transcriptional regulatory network,‚Äù Nature genetics, vol. 31, no. 1, pp. 60‚Äì63, 2002.   
[121] D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, and X. Sun, ‚ÄúMeasuring and relieving the over-smoothing problem for graph neural networks from the topological view,‚Äù 2019. [Online]. Available: https://arxiv.org/abs/1909.03211   
[122] C. Shorten and T. M. Khoshgoftaar, ‚ÄúA survey on image data augmentation for deep learning,‚Äù Journal of big data, vol. 6, no. 1, pp. 1‚Äì48, 2019.   
[123] A. Hern√°ndez-Garc√≠a and P. K√∂nig, ‚ÄúData augmentation instead of explicit regularization,‚Äù 2018. [Online]. Available: https://arxiv.org/abs/1806.03852   
[124] X. Chen, C.-J. Hsieh, and B. Gong, ‚ÄúWhen vision transformers outperform resnets without pre-training or strong data augmentations,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2106.01548   
[125] J. Kaddour, L. Liu, R. Silva, and M. J. Kusner, ‚ÄúA fair comparison of two popular flat minima optimizers: Stochastic weight averaging vs. sharpness-aware minimization,‚Äù 2022. [Online]. Available: https://arxiv.org/ abs/2202.00661   
[126] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, ‚ÄúAutoaugment: Learning augmentation policies from data,‚Äù arXiv preprint arXiv:1805.09501, 2018.

[127] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, ‚ÄúRandaugment: Practical automated data augmentation with a reduced search space,‚Äù in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, 2020, pp. 702‚Äì703.   
[128] T. Miller, ‚ÄúExplanation in artificial intelligence: Insights from the social sciences,‚Äù Artificial intelligence, vol. 267, pp. 1‚Äì38, 2019.   
[129] C. Molnar, Interpretable Machine Learning, 2nd ed., 2022. [Online]. Available: christophm.github.io/interpretable-ml-book/   
[130] N. Burkart and M. F. Huber, ‚ÄúA survey on the explainability of supervised machine learning,‚Äù Journal of Artificial Intelligence Research, vol. 70, pp. 245‚Äì 317, 2021.   
[131] F. K. Do≈°iloviƒá, M. Brƒçiƒá, and N. Hlupiƒá, ‚ÄúExplainable artificial intelligence: A survey,‚Äù in 2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO). IEEE, 2018, pp. 0210‚Äì0215.   
[132] S. Verma, J. P. Dickerson, and K. Hines, ‚ÄúCounterfactual explanations for machine learning: A review,‚Äù CoRR, vol. abs/2010.10596, 2020. [Online]. Available: https://arxiv.org/abs/2010.10596   
[133] A. Karimi, G. Barthe, B. Sch√∂lkopf, and I. Valera, ‚ÄúA survey of algorithmic recourse: definitions, formulations, solutions, and prospects,‚Äù CoRR, vol. abs/2010.04050, 2020. [Online]. Available: https://arxiv.org/abs/2010.04050   
[134] K. Simonyan, A. Vedaldi, and A. Zisserman, ‚ÄúDeep inside convolutional networks: Visualising image classification models and saliency maps,‚Äù arXiv preprint arXiv:1312.6034, 2013.   
[135] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and B. Kim, ‚ÄúSanity checks for saliency maps,‚Äù Advances in neural information processing systems, vol. 31, 2018.   
[136] P. Schwab and W. Karlen, ‚ÄúCxplain: Causal explanations for model interpretation under uncertainty,‚Äù in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d‚ÄôAlch√©-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 10 220‚Äì10 230. [Online]. Available: https://proceedings.neurips. cc/paper/2019/hash/3ab6be46e1d6b21d59a3c3a0b9d0f6ef-Abstract.html   
[137] M. R. O‚ÄôShaughnessy, G. Canal, M. Connor, C. Rozell, and M. A. Davenport, ‚ÄúGenerative causal explanations of black-box classifiers,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds.,

2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/hash/ 3a93a609b97ec0ab0ff5539eb79ef33a-Abstract.html   
[138] S. M. Lundberg and S.-I. Lee, ‚ÄúA unified approach to interpreting model predictions,‚Äù Advances in neural information processing systems, vol. 30, 2017.   
[139] C. Frye, C. Rowat, and I. Feige, ‚ÄúAsymmetric shapley values: incorporating causal knowledge into model-agnostic explainability,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/hash/ 0d770c496aa3da6d2c3f2bd19e7b9d6b-Abstract.html   
[140] D. Janzing, L. Minorics, and P. Bl√∂baum, ‚ÄúFeature relevance quantification in explainable AI: A causal problem,‚Äù in The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], ser. Proceedings of Machine Learning Research, S. Chiappa and R. Calandra, Eds., vol. 108. PMLR, 2020, pp. 2907‚Äì2916. [Online]. Available: http://proceedings.mlr.press/v108/janzing20a.html   
[141] T. Heskes, E. Sijben, I. G. Bucur, and T. Claassen, ‚ÄúCausal shapley values: Exploiting causal knowledge to explain individual predictions of complex models,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips. cc/paper/2020/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html   
[142] H. Chen, J. D. Janizek, S. Lundberg, and S.-I. Lee, ‚ÄúTrue to the model or true to the data?‚Äù 2020. [Online]. Available: https://arxiv.org/abs/2006.16234   
[143] Y. Jung, S. Kasiviswanathan, J. Tian, D. Janzing, P. Bloebaum, and E. Bareinboim, ‚ÄúOn measuring causal contributions via do-interventions,‚Äù in Proceedings of the 39th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162. PMLR, 17‚Äì23 Jul 2022, pp. 10 476‚Äì10 501. [Online]. Available: https: //proceedings.mlr.press/v162/jung22a.html   
[144] P. Lipton, ‚ÄúContrastive explanation,‚Äù Royal Institute of Philosophy Supplements, vol. 27, pp. 247‚Äì266, 1990.   
[145] A. Jacovi, S. Swayamdipta, S. Ravfogel, Y. Elazar, Y. Choi, and Y. Goldberg, ‚ÄúContrastive explanations for model interpretability,‚Äù in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds.

Association for Computational Linguistics, 2021, pp. 1597‚Äì1611. [Online]. Available: https://doi.org/10.18653/v1/2021.emnlp-main.120   
[146] S. Verma, J. Dickerson, and K. Hines, ‚ÄúCounterfactual explanations for machine learning: Challenges revisited,‚Äù arXiv preprint arXiv:2106.07756, 2021.   
[147] A. Karimi, B. Sch√∂lkopf, and I. Valera, ‚ÄúAlgorithmic recourse: from counterfactual explanations to interventions,‚Äù in FAccT ‚Äô21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, M. C. Elish, W. Isaac, and R. S. Zemel, Eds. ACM, 2021, pp. 353‚Äì362. [Online]. Available: https://doi.org/10.1145/3442188.3445899   
[148] M. T. Ribeiro, S. Singh, and C. Guestrin, ‚Äú" why should i trust you?" explaining the predictions of any classifier,‚Äù in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 1135‚Äì1144.   
[149] S. Wachter, B. D. Mittelstadt, and C. Russell, ‚ÄúCounterfactual explanations without opening the black box: Automated decisions and the GDPR,‚Äù CoRR, vol. abs/1711.00399, 2017. [Online]. Available: http://arxiv.org/abs/1711. 00399   
[150] A. Karimi, B. J. von K√ºgelgen, B. Sch√∂lkopf, and I. Valera, ‚ÄúAlgorithmic recourse under imperfect causal knowledge: a probabilistic approach,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips.cc/paper/ 2020/hash/02a3c7fb3f489288ae6942498498db20-Abstract.html   
[151] S. Joshi, O. Koyejo, W. Vijitbenjaronk, B. Kim, and J. Ghosh, ‚ÄúTowards realistic individual recourse and actionable explanations in black-box decision making systems,‚Äù CoRR, vol. abs/1907.09615, 2019. [Online]. Available: http://arxiv.org/abs/1907.09615   
[152] L. Schut, O. Key, R. McGrath, L. Costabello, B. Sacaleanu, M. Corcoran, and Y. Gal, ‚ÄúGenerating interpretable counterfactual explanations by implicit minimisation of epistemic and aleatoric uncertainties,‚Äù in The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, ser. Proceedings of Machine Learning Research, A. Banerjee and K. Fukumizu, Eds., vol. 130. PMLR, 2021, pp. 1756‚Äì1764. [Online]. Available: http://proceedings.mlr.press/v130/schut21a. html   
[153] A. Abid, M. Yuksekgonul, and J. Zou, ‚ÄúMeaningfully explaining model mistakes using conceptual counterfactuals,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2106.12723

[154] D. Mahajan, C. Tan, and A. Sharma, ‚ÄúPreserving causal constraints in counterfactual explanations for machine learning classifiers,‚Äù CoRR, vol. abs/1912.03277, 2019. [Online]. Available: http://arxiv.org/abs/1912.03277   
[155] B. Ustun, A. Spangher, and Y. Liu, ‚ÄúActionable recourse in linear classification,‚Äù in Proceedings of the Conference on Fairness, Accountability, and Transparency, ser. FAT* ‚Äô19. New York, NY, USA: Association for Computing Machinery, 2019, p. 10‚Äì19. [Online]. Available: https: //doi.org/10.1145/3287560.3287566   
[156] J. von K√ºgelgen, N. Agarwal, J. Zeitler, A. Mastouri, and B. Sch√∂lkopf, ‚ÄúAlgorithmic recourse in partially and fully confounded settings through bounding counterfactual effects,‚Äù 2021. [Online]. Available: https://arxiv.org/ abs/2106.11849   
[157] R. Kommiya Mothilal, D. Mahajan, C. Tan, and A. Sharma, ‚ÄúTowards unifying feature attribution and counterfactual explanations: Different means to the same end,‚Äù in Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021, pp. 652‚Äì663.   
[158] S. Galhotra, R. Pradhan, and B. Salimi, ‚ÄúFeature attribution and recourse via probabilistic contrastive counterfactuals,‚Äù 2021.   
[159] E. Albini, J. Long, D. Dervovic, and D. Magazzeni, ‚ÄúCounterfactual shapley additive explanations,‚Äù arXiv preprint arXiv:2110.14270, 2021.   
[160] D. S. Watson, L. Gultchin, A. Taly, and L. Floridi, ‚ÄúLocal explanations via necessity and sufficiency: unifying theory and practice,‚Äù in Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI 2021, Virtual Event, 27-30 July 2021, ser. Proceedings of Machine Learning Research, C. P. de Campos, M. H. Maathuis, and E. Quaeghebeur, Eds., vol. 161. AUAI Press, 2021, pp. 1382‚Äì1392. [Online]. Available: https://proceedings.mlr.press/v161/watson21a.html   
[161] D. Watson, ‚ÄúRational shapley values,‚Äù in 2022 ACM Conference on Fairness, Accountability, and Transparency, 2022, pp. 1083‚Äì1094.   
[162] H. Fokkema, R. de Heide, and T. van Erven, ‚ÄúAttribution-based explanations that provide recourse cannot be robust,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2205.15834   
[163] A. Artelt and B. Hammer, ‚ÄúOn the computation of counterfactual explanations‚Äìa survey,‚Äù arXiv preprint arXiv:1911.07749, 2019.   
[164] D. Mahajan, C. Tan, and A. Sharma, ‚ÄúPreserving causal constraints in counterfactual explanations for machine learning classifiers,‚Äù arXiv preprint arXiv:1912.03277, 2019.

[165] M. Pawelczyk, T. Datta, J. van-den Heuvel, G. Kasneci, and H. Lakkaraju, ‚ÄúAlgorithmic recourse in the face of noisy human responses,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2203.06768   
[166] K. Rawal, E. Kamar, and H. Lakkaraju, ‚ÄúAlgorithmic recourse in the wild: Understanding the impact of data and model shifts,‚Äù arXiv preprint arXiv:2012.11788, 2020.   
[167] A. Ghorbani, M. Kim, and J. Zou, ‚ÄúA distributional framework for data valuation,‚Äù in International Conference on Machine Learning. PMLR, 2020, pp. 3535‚Äì3544.   
[168] U. A√Øvodji, A. Bolot, and S. Gambs, ‚ÄúModel extraction from counterfactual explanations,‚Äù arXiv preprint arXiv:2009.01884, 2020.   
[169] R. Shokri, M. Strobel, and Y. Zick, ‚ÄúOn the privacy risks of model explanations,‚Äù in Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021, pp. 231‚Äì241.   
[170] Y.-B. He and Z. Geng, ‚ÄúActive learning of causal networks with intervention experiments and optimal designs,‚Äù Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2523‚Äì2547, 2008.   
[171] D. Slack, A. Hilgard, H. Lakkaraju, and S. Singh, ‚ÄúCounterfactual explanations can be manipulated,‚Äù Advances in Neural Information Processing Systems, vol. 34, 2021.   
[172] R. Naidu, A. Priyanshu, A. Kumar, S. Kotti, H. Wang, and F. Mireshghallah, ‚ÄúWhen differential privacy meets interpretability: A case study,‚Äù arXiv preprint arXiv:2106.13203, 2021.   
[173] K. Makhlouf, S. Zhioua, and C. Palamidessi, ‚ÄúSurvey on causal-based machine learning fairness notions,‚Äù arXiv preprint arXiv:2010.09553, 2020.   
[174] S. Barocas, M. Hardt, and A. Narayanan, Fairness and Machine Learning. fairmlbook.org, 2019, http://www.fairmlbook.org.   
[175] M. J. Kusner, J. R. Loftus, C. Russell, and R. Silva, ‚ÄúCounterfactual fairness,‚Äù in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 4066‚Äì4076. [Online]. Available: https://proceedings.neurips.cc/paper/2017/ hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html   
[176] R. Nabi and I. Shpitser, ‚ÄúFair inference on outcomes,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018.   
[177] S. Chiappa, ‚ÄúPath-specific counterfactual fairness,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 7801‚Äì7808.

[178] Y. Wu, L. Zhang, X. Wu, and H. Tong, ‚ÄúPc-fairness: A unified framework for measuring causality-based fairness,‚Äù Advances in Neural Information Processing Systems, vol. 32, 2019.   
[179] J. Zhang and E. Bareinboim, ‚ÄúFairness in decision-making‚Äîthe causal explanation formula,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018.   
[180] T. J. VanderWeele and W. R. Robinson, ‚ÄúOn causal interpretation of race in regressions adjusting for confounding and mediating variables,‚Äù Epidemiology (Cambridge, Mass.), vol. 25, no. 4, p. 473, 2014.   
[181] L. Hu and I. Kohler-Hausmann, ‚ÄúWhat‚Äôs sex got to do with machine learning?‚Äù in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, ser. FAT* ‚Äô20. New York, NY, USA: Association for Computing Machinery, 2020, p. 513. [Online]. Available: https: //doi.org/10.1145/3351095.3375674   
[182] N. Kilbertus, ‚ÄúBeyond traditional assumptions in fair machine learning,‚Äù Ph.D. dissertation, University of Cambridge, UK, Sep. 2020, (Cambridge-T√ºbingen-Fellowship).   
[183] N. Kilbertus, M. Rojas Carulla, G. Parascandolo, M. Hardt, D. Janzing, and B. Sch√∂lkopf, ‚ÄúAvoiding discrimination through causal reasoning,‚Äù Advances in neural information processing systems, vol. 30, 2017.   
[184] B. Salimi, L. Rodriguez, B. Howe, and D. Suciu, ‚ÄúInterventional fairness: Causal database repair for algorithmic fairness,‚Äù in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 793‚Äì810.   
[185] H. Singh, R. Singh, V. Mhasawade, and R. Chunara, ‚ÄúFairness violations and mitigation under covariate shift,‚Äù in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021, pp. 3‚Äì13.   
[186] J. Schrouff, N. Harris, O. Koyejo, I. Alabdulmohsin, E. Schnider, K. Opsahl-Ong, A. Brown, S. Roy, D. Mincu, C. Chen et al., ‚ÄúMaintaining fairness across distribution shift: do we have viable solutions for real-world applications?‚Äù arXiv preprint arXiv:2202.01034, 2022.   
[187] D. Slack, S. A. Friedler, and E. Givental, ‚ÄúFairness warnings and fair-maml: learning fairly with minimal data,‚Äù in FAT* ‚Äô20: Conference on Fairness, Accountability, and Transparency, Barcelona, Spain, January 27-30, 2020, M. Hildebrandt, C. Castillo, L. E. Celis, S. Ruggieri, L. Taylor, and G. Zanfir-Fortuna, Eds. ACM, 2020, pp. 200‚Äì209. [Online]. Available: https://doi.org/10.1145/3351095.3372839   
[188] A. Subbaswamy, R. Adams, and S. Saria, ‚ÄúEvaluating model robustness and stability to dataset shift,‚Äù in The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021,

Virtual Event, ser. Proceedings of Machine Learning Research, A. Banerjee and K. Fukumizu, Eds., vol. 130. PMLR, 2021, pp. 2611‚Äì2619. [Online]. Available: http://proceedings.mlr.press/v130/subbaswamy21a.html   
[189] M. E. Guy and S. A. McCandless, ‚ÄúSocial equity: Its legacy, its promise,‚Äù Public Administration Review, vol. 72, no. s1, pp. S5‚ÄìS13, 2012.   
[190] J. McGinnis Johnson, ‚ÄúRace and social equity: A nervous area of government,‚Äù Equality, Diversity and Inclusion: An International Journal, vol. 34, pp. 262‚Äì 264, 03 2015.   
[191] J. G. Richens, R. Beard, and D. H. Thompson, ‚ÄúFirst do no harm: counterfactual objective functions for safe & ethical ai,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2204.12993   
[192] V. Gupta, P. Nokhiz, C. D. Roy, and S. Venkatasubramanian, ‚ÄúEqualizing recourse across groups,‚Äù 2019. [Online]. Available: https://arxiv.org/abs/ 1909.03166   
[193] J. von K√ºgelgen, A.-H. Karimi, U. Bhatt, I. Valera, A. Weller, and B. Sch√∂lkopf, ‚ÄúOn the fairness of causal algorithmic recourse,‚Äù arXiv preprint arXiv:2010.06529, 2020.   
[194] W. Huan, Y. Wu, L. Zhang, and X. Wu, ‚ÄúFairness through equality of effort,‚Äù in Companion Proceedings of the Web Conference 2020, 2020, pp. 743‚Äì751.   
[195] Y. Wu, L. Zhang, and X. Wu, ‚ÄúCounterfactual fairness: Unidentification, bound and algorithm,‚Äù in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, 2019.   
[196] I. Kohler-Hausmann, ‚ÄúEddie murphy and the dangers of counterfactual causal thinking about detecting racial discrimination,‚Äù Nw. UL Rev., vol. 113, p. 1163, 2018.   
[197] L. Hu, ‚ÄúCausation in the social world,‚Äù Ph.D. dissertation, Harvard University, 2022.   
[198] L. Hu and I. Kohler-Hausmann, ‚ÄúWhat‚Äôs sex got to do with machine learning?‚Äù in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, ser. FAT* ‚Äô20. New York, NY, USA: Association for Computing Machinery, 2020, p. 513. [Online]. Available: https: //doi.org/10.1145/3351095.3375674   
[199] A. Kasirzadeh and A. Smart, ‚ÄúThe use and misuse of counterfactuals in ethical machine learning,‚Äù in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021, pp. 228‚Äì236.   
[200] J. Kleinberg, S. Mullainathan, and M. Raghavan, ‚ÄúInherent trade-offs in the fair determination of risk scores,‚Äù arXiv preprint arXiv:1609.05807, 2016.

[201] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, ‚ÄúAlgorithmic decision making and the cost of fairness,‚Äù in Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining, 2017, pp. 797‚Äì806.   
[202] A. Chouldechova, ‚ÄúFair prediction with disparate impact: A study of bias in recidivism prediction instruments,‚Äù Big data, vol. 5, no. 2, pp. 153‚Äì163, 2017.   
[203] L. Gultchin, V. Cohen-Addad, S. Giffard-Roisin, V. Kanade, and F. Mallmann-Trenn, ‚ÄúBeyond impossibility: Balancing sufficiency, separation and accuracy,‚Äù CoRR, vol. abs/2205.12327, 2022. [Online]. Available: https://doi.org/10.48550/arXiv.2205.12327   
[204] H. Nilforoshan, J. D. Gaebler, R. Shroff, and S. Goel, ‚ÄúCausal conceptions of fairness and their consequences,‚Äù in Proceedings of the 39th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162. PMLR, 17‚Äì23 Jul 2022, pp. 16 848‚Äì16 887. [Online]. Available: https://proceedings.mlr.press/v162/nilforoshan22a.html   
[205] C. Ashurst, R. Carey, S. Chiappa, and T. Everitt, ‚ÄúWhy fair labels can yield unfair predictions: Graphical conditions for introduced unfairness,‚Äù in Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022. AAAI Press, 2022, pp. 9494‚Äì9503. [Online]. Available: https: //ojs.aaai.org/index.php/AAAI/article/view/21182   
[206] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. Cambridge, MA, USA: A Bradford Book, 2018.   
[207] S. Zhu, I. Ng, and Z. Chen, ‚ÄúCausal discovery with reinforcement learning,‚Äù arXiv preprint arXiv:1906.04477, 2019.   
[208] L. Bottou, J. Peters, J. Qui√±onero-Candela, D. X. Charles, D. M. Chickering, E. Portugaly, D. Ray, P. Simard, and E. Snelson, ‚ÄúCounterfactual reasoning and learning systems: The example of computational advertising.‚Äù Journal of Machine Learning Research, vol. 14, no. 11, 2013.   
[209] E. Bareinboim, A. Forney, and J. Pearl, ‚ÄúBandits with unobserved confounders: A causal approach,‚Äù Advances in Neural Information Processing Systems, vol. 28, 2015.   
[210] S. J. Gershman, ‚ÄúReinforcement learning and causal models,‚Äù The Oxford handbook of causal reasoning, vol. 1, p. 295, 2017.   
[211] C. Szepesvari, ‚ÄúCausality from the perspective of reinforcement learning,‚Äù 2018, machine Learning for Causal Inference, Counterfactual Prediction, and

Autonomous Action (CausalML). [Online]. Available: https://sites.google. com/site/faim18wscausalml/invited-talks   
[212] J. Bannon, B. Windsor, W. Song, and T. Li, ‚ÄúCausality and batch reinforcement learning: Complementary approaches to planning in unknown domains,‚Äù arXiv preprint arXiv:2006.02579, 2020.   
[213] E. Bareinboim, ‚ÄúCausal reinforcement learning,‚Äù 2020, iCML. [Online]. Available: https://crl.causalai.net/   
[214] S. Weichwald, S. W. Mogensen, T. E. Lee, D. Baumann, O. Kroemer, I. Guyon, S. Trimpe, J. Peters, and N. Pfister, ‚ÄúLearning by doing: Controlling a dynamical system using causality, control, and reinforcement learning,‚Äù arXiv preprint arXiv:2202.06052, 2022.   
[215] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., ‚ÄúMastering chess and shogi by self-play with a general reinforcement learning algorithm,‚Äù arXiv preprint arXiv:1712.01815, 2017.   
[216] R. S. Sutton, ‚ÄúThe bitter lesson,‚Äù 2019. [Online]. Available: http: //www.incompleteideas.net/IncIdeas/BitterLesson.html   
[217] V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins, ‚ÄúDouble/debiased machine learning for treatment and structural parameters,‚Äù The Econometrics Journal, vol. 21, no. 1, pp. C1‚ÄìC68, 01 2018.   
[218] E. H. Kennedy, ‚ÄúOptimal doubly robust estimation of heterogeneous causal effects,‚Äù arXiv preprint arXiv:2004.14497, 2020.   
[219] S. Levine, A. Kumar, G. Tucker, and J. Fu, ‚ÄúOffline reinforcement learning: Tutorial, review, and perspectives on open problems,‚Äù arXiv preprint arXiv:2005.01643, 2020.   
[220] T. Lattimore and C. Szepesv√°ri, Bandit algorithms. Cambridge University Press, 2020.   
[221] F. Lattimore, T. Lattimore, and M. D. Reid, ‚ÄúCausal bandits: Learning good interventions via causal inference,‚Äù in Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, Eds., 2016, pp. 1181‚Äì1189. [Online]. Available: https://proceedings.neurips.cc/paper/2016/ hash/b4288d9c0ec0a1841b3b3728321e7088-Abstract.html   
[222] S. Bubeck, R. Munos, and G. Stoltz, ‚ÄúPure exploration in multi-armed bandits problems,‚Äù in International conference on Algorithmic learning theory. Springer, 2009, pp. 23‚Äì37.

[223] V. Gabillon, M. Ghavamzadeh, and A. Lazaric, ‚ÄúBest arm identification: A unified approach to fixed budget and fixed confidence,‚Äù Advances in Neural Information Processing Systems, vol. 25, 2012.   
[224] J.-Y. Audibert, S. Bubeck, and R. Munos, ‚ÄúBest arm identification in multiarmed bandits.‚Äù in COLT. Citeseer, 2010, pp. 41‚Äì53.   
[225] S. Lee and E. Bareinboim, ‚ÄúStructural causal bandits: Where to intervene?‚Äù in Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., vol. 31. Curran Associates, Inc., 2018. [Online]. Available: https://proceedings. neurips.cc/paper/2018/file/c0a271bc0ecb776a094786474322cb82-Paper.pdf   
[226] A. A. W. M. de Kroon, D. Belgrave, and J. M. Mooij, ‚ÄúCausal discovery for causal bandits utilizing separating sets,‚Äù 2020. [Online]. Available: https://arxiv.org/abs/2009.07916   
[227] Y. Lu, A. Meisami, and A. Tewari, ‚ÄúCausal bandits with unknown graph structure,‚Äù in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available: https://openreview.net/forum?id=9-XhLobA4z   
[228] R. Silva, ‚ÄúObservational-interventional priors for dose-response learning,‚Äù in Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, Eds., 2016, pp. 1561‚Äì1569. [Online]. Available: https://proceedings.neurips. cc/paper/2016/hash/aff1621254f7c1be92f64550478c56e6-Abstract.html   
[229] V. Aglietti, X. Lu, A. Paleyes, and J. Gonz√°lez, ‚ÄúCausal bayesian optimization,‚Äù in The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], ser. Proceedings of Machine Learning Research, S. Chiappa and R. Calandra, Eds., vol. 108. PMLR, 2020, pp. 3155‚Äì3164. [Online]. Available: http://proceedings.mlr.press/v108/aglietti20a.html   
[230] D. R. Jones, M. Schonlau, and W. J. Welch, ‚ÄúEfficient global optimization of expensive black-box functions,‚Äù Journal of Global optimization, vol. 13, no. 4, pp. 455‚Äì492, 1998.   
[231] S. Saengkyongam, N. Thams, J. Peters, and N. Pfister, ‚ÄúInvariant policy learning: A causal perspective,‚Äù CoRR, vol. abs/2106.00808, 2021. [Online]. Available: https://arxiv.org/abs/2106.00808   
[232] Y. Lu, A. Meisami, and A. Tewari, ‚ÄúEfficient reinforcement learning with prior causal knowledge,‚Äù in First Conference on Causal Learning and Reasoning, 2022. [Online]. Available: https://openreview.net/forum?id=xzJWqJadOG   
[233] T. M. Moerland, J. Broekens, and C. M. Jonker, ‚ÄúModel-based reinforcement learning: A survey,‚Äù arXiv preprint arXiv:2006.16712, 2020.

[234] D. Ha and J. Schmidhuber, ‚ÄúWorld models,‚Äù arXiv preprint arXiv:1803.10122, 2018.   
[235] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel et al., ‚ÄúMastering atari, go, chess and shogi by planning with a learned model,‚Äù Nature, vol. 588, no. 7839, pp. 604‚Äì609, 2020.   
[236] D. J. Rezende, I. Danihelka, G. Papamakarios, N. R. Ke, R. Jiang, T. Weber, K. Gregor, H. Merzic, F. Viola, J. Wang et al., ‚ÄúCausally correct partial models for reinforcement learning,‚Äù arXiv preprint arXiv:2002.02836, 2020.   
[237] M. Li, M. Yang, F. Liu, X. Chen, Z. Chen, and J. Wang, ‚ÄúCausal world models by unsupervised deconfounding of physical dynamics,‚Äù arXiv preprint arXiv:2012.14228, 2020.   
[238] D. Chapman and L. P. Kaelbling, ‚ÄúInput generalization in delayed reinforcement learning: An algorithm and performance comparisons.‚Äù in Ijcai, vol. 91. Citeseer, 1991, pp. 726‚Äì731.   
[239] A. K. McCallum, Reinforcement learning with selective perception and hidden state. University of Rochester, 1996.   
[240] Z. Wang, X. Xiao, Z. Xu, Y. Zhu, and P. Stone, ‚ÄúCausal dynamics learning for task-independent state abstraction,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2206.13452   
[241] A. A. Mastakouri, B. Sch√∂lkopf, and D. Janzing, ‚ÄúNecessary and sufficient conditions for causal feature selection in time series with latent common causes,‚Äù in International Conference on Machine Learning. PMLR, 2021, pp. 7502‚Äì 7511.   
[242] L. Weng, ‚ÄúMeta reinforcement learning,‚Äù lilianweng.github.io, 2019. [Online]. Available: https://lilianweng.github.io/posts/2019-06-23-meta-rl/   
[243] S. A. Sontakke, A. Mehrjou, L. Itti, and B. Sch√∂lkopf, ‚ÄúCausal curiosity: RL agents discovering self-supervised experiments for causal representation learning,‚Äù in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 2021, pp. 9848‚Äì9858. [Online]. Available: http: //proceedings.mlr.press/v139/sontakke21a.html   
[244] F. Doshi-Velez and G. Konidaris, ‚ÄúHidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations,‚Äù in IJCAI: proceedings of the conference, vol. 2016. NIH Public Access, 2016, p. 1432.

[245] K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen, ‚ÄúEfficient off-policy meta-reinforcement learning via probabilistic context variables,‚Äù in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 5331‚Äì5340. [Online]. Available: http: //proceedings.mlr.press/v97/rakelly19a.html   
[246] A. Zhang, C. Lyle, S. Sodhani, A. Filos, M. Kwiatkowska, J. Pineau, Y. Gal, and D. Precup, ‚ÄúInvariant causal prediction for block mdps,‚Äù in International Conference on Machine Learning. PMLR, 2020, pp. 11 214‚Äì11 224.   
[247] M. Tomar, A. Zhang, R. Calandra, M. E. Taylor, and J. Pineau, ‚ÄúModelinvariant state abstractions for model-based reinforcement learning,‚Äù 2021.   
[248] K. Kansky, T. Silver, D. A. M√©ly, M. Eldawy, M. L√°zaro-Gredilla, X. Lou, N. Dorfman, S. Sidor, S. Phoenix, and D. George, ‚ÄúSchema networks: Zeroshot transfer with a generative causal model of intuitive physics,‚Äù 2017.   
[249] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., ‚ÄúHuman-level control through deep reinforcement learning,‚Äù nature, vol. 518, no. 7540, pp. 529‚Äì533, 2015.   
[250] M. Mutti, R. De Santi, E. Rossi, J. F. Calderon, M. Bronstein, and M. Restelli, ‚ÄúProvably efficient causal model-based reinforcement learning for systematic generalization,‚Äù arXiv preprint arXiv:2202.06545, 2022.   
[251] A. Lei, B. Sch√∂lkopf, and I. Posner, ‚ÄúVariational causal dynamics: Discovering modular world models from interventions,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2206.11131   
[252] S. Parbhoo, S. Joshi, and F. Doshi-Velez, ‚ÄúGeneralizing off-policy evaluation from a causal perspective for sequential decision-making,‚Äù 2022.   
[253] L. Buesing, T. Weber, Y. Zwols, N. Heess, S. Racaniere, A. Guez, and J.-B. Lespiau, ‚ÄúWoulda, coulda, shoulda: Counterfactually-guided policy search,‚Äù in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/forum?id=BJG0voC9YQ   
[254] M. Oberst and D. A. Sontag, ‚ÄúCounterfactual off-policy evaluation with gumbel-max structural causal models,‚Äù in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 4881‚Äì 4890. [Online]. Available: http://proceedings.mlr.press/v97/oberst19a.html   
[255] N. Kallus and A. Zhou, ‚ÄúConfounding-robust policy improvement,‚Äù in Advances in Neural Information Processing Systems 31: Annual Conference

on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr√©al, Canada, S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., 2018, pp. 9289‚Äì 9299. [Online]. Available: https://proceedings.neurips.cc/paper/2018/hash/ 3a09a524440d44d7f19870070a5ad42f-Abstract.html   
[256] N. Kallus and A. Zhou, ‚ÄúConfounding-robust policy evaluation in infinite-horizon reinforcement learning,‚Äù in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 22 293‚Äì 22 304. [Online]. Available: https://proceedings.neurips.cc/paper/2020/file/ fd4f21f2556dad0ea8b7a5c04eabebda-Paper.pdf   
[257] A. Bennett, N. Kallus, L. Li, and A. Mousavi, ‚ÄúOff-policy evaluation in infinitehorizon reinforcement learning with latent confounders,‚Äù in International Conference on Artificial Intelligence and Statistics. PMLR, 2021, pp. 1999‚Äì2007.   
[258] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, ‚ÄúContinuous control with deep reinforcement learning,‚Äù arXiv preprint arXiv:1509.02971, 2015.   
[259] A. Charpentier, R. Elie, and C. Remlinger, ‚ÄúReinforcement learning in economics and finance,‚Äù Computational Economics, pp. 1‚Äì38, 2021.   
[260] H. Namkoong, R. Keramati, S. Yadlowsky, and E. Brunskill, ‚ÄúOff-policy policy evaluation for sequential decisions under unobserved confounding,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 18 819‚Äì18 831, 2020.   
[261] G. Tennenholtz, U. Shalit, and S. Mannor, ‚ÄúOff-policy evaluation in partially observable environments,‚Äù in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 2020, pp. 10 276‚Äì10 283. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/6590   
[262] A. Bennett and N. Kallus, ‚ÄúProximal reinforcement learning: Efficient offpolicy evaluation in partially observed markov decision processes,‚Äù CoRR, vol. abs/2110.15332, 2021. [Online]. Available: https://arxiv.org/abs/2110.15332   
[263] M. Gasse, D. Grasset, G. Gaudron, and P.-Y. Oudeyer, ‚ÄúCausal reinforcement learning using observational and interventional data,‚Äù arXiv preprint arXiv:2106.14421, 2021.   
[264] L. Wang, Z. Yang, and Z. Wang, ‚ÄúProvably efficient causal reinforcement learning with confounded observational data,‚Äù in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available: https://openreview.net/ forum?id=tUeeRzMXJZ

[265] Y. Chen, L. Xu, C. Gulcehre, T. L. Paine, A. Gretton, N. de Freitas, and A. Doucet, ‚ÄúOn instrumental variable regression for deep offline policy evaluation,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2105.10148   
[266] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, ‚ÄúImitation learning: A survey of learning methods,‚Äù ACM Computing Surveys (CSUR), vol. 50, no. 2, pp. 1‚Äì35, 2017.   
[267] P. de Haan, D. Jayaraman, and S. Levine, ‚ÄúCausal confusion in imitation learning,‚Äù in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d‚ÄôAlch√©-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 11 693‚Äì11 704. [Online]. Available: https://proceedings.neurips.cc/paper/ 2019/hash/947018640bf36a2bb609d3557a285329-Abstract.html   
[268] J. Tien, J. Z.-Y. He, Z. Erickson, A. D. Dragan, and D. Brown, ‚ÄúA study of causal confusion in preference-based reward learning,‚Äù arXiv preprint arXiv:2204.06601, 2022.   
[269] P. A. Ortega, M. Kunesch, G. Del√©tang, T. Genewein, J. Grau-Moya, J. Veness, J. Buchli, J. Degrave, B. Piot, J. Perolat, T. Everitt, C. Tallec, E. Parisotto, T. Erez, Y. Chen, S. Reed, M. Hutter, N. de Freitas, and S. Legg, ‚ÄúShaking the foundations: delusions in sequence models for interaction and control,‚Äù 2021.   
[270] J. Zhang, D. Kumor, and E. Bareinboim, ‚ÄúCausal imitation learning with unobserved confounders,‚Äù in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 12 263‚Äì 12 274. [Online]. Available: https://proceedings.neurips.cc/paper/2020/file/ 8fdd149fcaa7058caccc9c4ad5b0d89a-Paper.pdf   
[271] D. Kumor, J. Zhang, and E. Bareinboim, ‚ÄúSequential causal imitation learning with unobserved confounders,‚Äù in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available: https://openreview.net/forum?id=o6-k168bBD8   
[272] I. Bica, D. Jarrett, and M. van der Schaar, ‚ÄúInvariant causal imitation learning for generalizable policies,‚Äù in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available: https://openreview.net/forum?id=715E7e6j4gU   
[273] G. Swamy, S. Choudhury, J. A. Bagnell, and Z. S. Wu, ‚ÄúCausal imitation learning under temporally correlated noise,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2202.01312   
[274] A. Hefny, C. Downey, and G. J. Gordon, ‚ÄúSupervised learning for dynamical system learning,‚Äù Advances in neural information processing systems, vol. 28, 2015.

[275] A. Y. Ng, H. J. Kim, M. I. Jordan, S. Sastry, and S. Ballianda, ‚ÄúAutonomous helicopter flight via reinforcement learning.‚Äù in NIPS, vol. 16. Citeseer, 2003.   
[276] P. G. Wright, Tariff on animal and vegetable oils. Macmillan Company, New York, 1928.   
[277] T. Desautels, R. Das, J. Calvert, M. Trivedi, C. Summers, D. J. Wales, and A. Ercole, ‚ÄúPrediction of early unplanned intensive care unit readmission in a uk tertiary care hospital: a cross-sectional machine learning approach,‚Äù BMJ open, vol. 7, no. 9, p. e017199, 2017.   
[278] M. Minsky, ‚ÄúSteps toward artificial intelligence,‚Äù Proceedings of the IRE, vol. 49, no. 1, pp. 8‚Äì30, 1961.   
[279] T. Mesnard, T. Weber, F. Viola, S. Thakoor, A. Saade, A. Harutyunyan, W. Dabney, T. S. Stepleton, N. Heess, A. Guez, E. Moulines, M. Hutter, L. Buesing, and R. Munos, ‚ÄúCounterfactual credit assignment in modelfree reinforcement learning,‚Äù in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 2021, pp. 7654‚Äì7664. [Online]. Available: http://proceedings.mlr.press/v139/mesnard21a.html   
[280] M. Seitzer, B. Sch√∂lkopf, and G. Martius, ‚ÄúCausal influence detection for improving efficiency in reinforcement learning,‚Äù Advances in Neural Information Processing Systems, vol. 34, 2021.   
[281] G. Weiss, Multiagent systems: a modern approach to distributed artificial intelligence. MIT press, 1999.   
[282] S. Gronauer and K. Diepold, ‚ÄúMulti-agent deep reinforcement learning: a survey,‚Äù Artificial Intelligence Review, vol. 55, no. 2, pp. 895‚Äì943, 2022.   
[283] Y.-H. Chang, T. Ho, and L. Kaelbling, ‚ÄúAll learning is local: Multi-agent learning in global reward games,‚Äù Advances in neural information processing systems, vol. 16, 2003.   
[284] J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, ‚ÄúCounterfactual multi-agent policy gradients,‚Äù in Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, S. A. McIlraith and K. Q. Weinberger, Eds. AAAI Press, 2018, pp. 2974‚Äì2982. [Online]. Available: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17193   
[285] N. Jaques, A. Lazaridou, E. Hughes, √á. G√ºl√ßehre, P. A. Ortega, D. Strouse, J. Z. Leibo, and N. de Freitas, ‚ÄúSocial influence as intrinsic motivation for multi-agent deep reinforcement learning,‚Äù in Proceedings of the 36th

International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 3040‚Äì 3049. [Online]. Available: http://proceedings.mlr.press/v97/jaques19a.html   
[286] S. Pitis, E. Creager, and A. Garg, ‚ÄúCounterfactual data augmentation using locally factored dynamics,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/hash/ 294e09f267683c7ddc6cc5134a7e68a8-Abstract.html   
[287] L. P. Kaelbling, ‚ÄúLearning to achieve goals,‚Äù in IN PROC. OF IJCAI-93. Morgan Kaufmann, 1993, pp. 1094‚Äì1098.   
[288] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba, ‚ÄúHindsight experience replay,‚Äù in Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/2017/ file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf   
[289] M. Andrychowicz, A. Raichuk, P. Sta≈Ñczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski et al., ‚ÄúWhat matters in onpolicy reinforcement learning? a large-scale empirical study,‚Äù arXiv preprint arXiv:2006.05990, 2020.   
[290] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas, ‚ÄúReinforcement learning with augmented data,‚Äù in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 19 884‚Äì19 895. [Online]. Available: https://proceedings.neurips.cc/paper/ 2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf   
[291] R. S. Sutton, ‚ÄúDyna, an integrated architecture for learning, planning, and reacting,‚Äù ACM Sigart Bulletin, vol. 2, no. 4, pp. 160‚Äì163, 1991.   
[292] M. Janner, J. Fu, M. Zhang, and S. Levine, ‚ÄúWhen to trust your model: Model-based policy optimization,‚Äù in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d‚ÄôAlch√©-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 12 498‚Äì 12 509. [Online]. Available: https://proceedings.neurips.cc/paper/2019/hash/ 5faf461eff3099671ad63c6f3f094f7f-Abstract.html

[293] C. Lu, B. Huang, K. Wang, J. M. Hern√°ndez-Lobato, K. Zhang, and B. Sch√∂lkopf, ‚ÄúSample-efficient reinforcement learning via counterfactualbased data augmentation,‚Äù in Offline Reinforcement Learning - Workshop at the 34th Conference on Neural Information Processing Systems (NeurIPS), 2020. [Online]. Available: https://offline-rl-neurips.github.io/pdf/34.pdf   
[294] A. Jaiswal, W. AbdAlmageed, Y. Wu, and P. Natarajan, ‚ÄúBidirectional conditional generative adversarial networks,‚Äù in Asian Conference on Computer Vision. Springer, 2018, pp. 216‚Äì232.   
[295] N. Bostrom, Superintelligence: Paths, Dangers, Strategies, 1st ed. USA: Oxford University Press, Inc., 2014.   
[296] T. Everitt, G. Lea, and M. Hutter, ‚ÄúAGI safety literature review,‚Äù in Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, J. Lang, Ed. ijcai.org, 2018, pp. 5441‚Äì5449. [Online]. Available: https://doi.org/10.24963/ijcai.2018/768   
[297] E. Yudkowsky, ‚ÄúThe ai alignment problem: why it is hard, and where to start,‚Äù Symbolic Systems Distinguished Speaker, 2016.   
[298] B. Christian, The alignment problem: How can machines learn human values? Atlantic Books, 2021.   
[299] J. Miller, S. Milli, and M. Hardt, ‚ÄúStrategic classification is causal modeling in disguise,‚Äù 2019. [Online]. Available: https://arxiv.org/abs/1910.10362   
[300] T. Everitt, R. Carey, L. Hammond, J. Fox, E. Langlois, and S. Legg, ‚ÄúProgress on causal influence diagrams,‚Äù Jun 2021. [Online]. Available: https://deepmindsafetyresearch.medium.com/ progress-on-causal-influence-diagrams-a7a32180b0d1#4e50   
[301] T. Everitt, R. Carey, E. D. Langlois, P. A. Ortega, and S. Legg, ‚ÄúAgent incentives: A causal perspective,‚Äù in Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 2021, pp. 11 487‚Äì11 495. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/17368   
[302] A. Jern and C. Kemp, ‚ÄúCapturing mental state reasoning with influence diagrams,‚Äù in Proceedings of the Annual Meeting of the Cognitive Science Society, vol. 33, no. 33, 2011.   
[303] M. Kleiman-Weiner, T. Gerstenberg, S. Levine, and J. B. Tenenbaum, ‚ÄúInference of intention and permissibility in moral decision making.‚Äù in CogSci, 2015.

[304] R. A. Howard, ‚ÄúInformation value theory,‚Äù IEEE Transactions on systems science and cybernetics, vol. 2, no. 1, pp. 22‚Äì26, 1966.   
[305] C. Evans and A. Kasirzadeh, ‚ÄúUser tampering in reinforcement learning recommender systems,‚Äù CoRR, vol. abs/2109.04083, 2021. [Online]. Available: https://arxiv.org/abs/2109.04083   
[306] A. P. Dawid, ‚ÄúInfluence diagrams for causal modelling and inference,‚Äù International Statistical Review, vol. 70, no. 2, pp. 161‚Äì189, 2002.   
[307] E. Fagiuoli and M. Zaffalon, ‚ÄúA note about redundancy in influence diagrams,‚Äù International Journal of Approximate Reasoning, vol. 19, no. 3-4, pp. 351‚Äì365, 1998.   
[308] R. D. Shachter, ‚ÄúEvaluating influence diagrams,‚Äù Operations research, vol. 34, no. 6, pp. 871‚Äì882, 1986.   
[309] W. Saunders, G. Sastry, A. Stuhlm√ºller, and O. Evans, ‚ÄúTrial without error: Towards safe reinforcement learning via human intervention,‚Äù in Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, E. Andr√©, S. Koenig, M. Dastani, and G. Sukthankar, Eds. International Foundation for Autonomous Agents and Multiagent Systems Richland, SC, USA / ACM, 2018, pp. 2067‚Äì2069. [Online]. Available: http://dl.acm.org/citation.cfm?id=3238074   
[310] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, ‚ÄúSafe modelbased reinforcement learning with stability guarantees,‚Äù Advances in neural information processing systems, vol. 30, 2017.   
[311] E. D. Langlois and T. Everitt, ‚ÄúHow rl agents behave when their actions are modified,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 13, 2021, pp. 11 586‚Äì11 594.   
[312] T. Everitt, M. Hutter, R. Kumar, and V. Krakovna, ‚ÄúReward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective,‚Äù Synthese, vol. 198, no. 27, pp. 6435‚Äì6467, 2021.   
[313] S. Farquhar, R. Carey, and T. Everitt, ‚ÄúPath-specific objectives for safer agent incentives,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2204.10018   
[314] A. Guez, M. Mirza, K. Gregor, R. Kabra, S. Racani√®re, T. Weber, D. Raposo, A. Santoro, L. Orseau, T. Eccles, G. Wayne, D. Silver, and T. P. Lillicrap, ‚ÄúAn investigation of model-free planning,‚Äù in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 2464‚Äì 2473. [Online]. Available: http://proceedings.mlr.press/v97/guez19a.html

[315] K. Gregor, D. Jimenez Rezende, F. Besse, Y. Wu, H. Merzic, and A. van den Oord, ‚ÄúShaping belief states with generative environment models for rl,‚Äù Advances in Neural Information Processing Systems, vol. 32, 2019.   
[316] F. Baradel, N. Neverova, J. Mille, G. Mori, and C. Wolf, ‚ÄúCophy: Counterfactual learning of physical dynamics,‚Äù in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview. net/forum?id=SkeyppEFvS   
[317] Wikipedia contributors, ‚ÄúReinventing the wheel,‚Äù 2022, [Online; accessed 10- May-2022]. [Online]. Available: https://en.wikipedia.org/wiki/Reinventing the_wheel   
[318] J. Zhang and E. Bareinboim, ‚ÄúMarkov decision processes with unobserved confounders: A causal approach,‚Äù Technical report, Technical Report R-23, Purdue AI Lab, Tech. Rep., 2016.   
[319] T. Wang, C. Zhou, Q. Sun, and H. Zhang, ‚ÄúCausal attention for unbiased visual recognition,‚Äù in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.   
[320] A. Hallak, D. Di Castro, and S. Mannor, ‚ÄúContextual markov decision processes,‚Äù arXiv preprint arXiv:1502.02259, 2015.   
[321] A. Guez, D. Silver, and P. Dayan, ‚ÄúEfficient bayes-adaptive reinforcement learning using sample-based search,‚Äù Advances in neural information processing systems, vol. 25, 2012.   
[322] J. Zhang and E. Bareinboim, ‚ÄúCan humans be out of the loop?‚Äù in First Conference on Causal Learning and Reasoning, 2022. [Online]. Available: https://openreview.net/forum?id=P0f91v5fTK   
[323] L. Bottou, F. E. Curtis, and J. Nocedal, ‚ÄúOptimization methods for large-scale machine learning,‚Äù Siam Review, vol. 60, no. 2, pp. 223‚Äì311, 2018.   
[324] K. Tang, J. Huang, and H. Zhang, ‚ÄúLong-tailed classification by keeping the good and removing the bad momentum causal effect,‚Äù in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 1513‚Äì1524. [Online]. Available: https://proceedings.neurips.cc/paper/2020/ file/1091660f3dff84fd648efe31391c5524-Paper.pdf   
[325] W. J. Reed, ‚ÄúThe pareto, zipf and other power laws,‚Äù Economics letters, vol. 74, no. 1, pp. 15‚Äì19, 2001.   
[326] Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, ‚ÄúGeneralizing from a few examples: A survey on few-shot learning,‚Äù ACM computing surveys (csur), vol. 53, no. 3, pp. 1‚Äì34, 2020.

[327] K. P. Murphy, Probabilistic Machine Learning: An introduction. MIT Press, 2022. [Online]. Available: probml.ai   
[328] Z. Yue, H. Zhang, Q. Sun, and X. Hua, ‚ÄúInterventional few-shot learning,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips.cc/paper/ 2020/hash/1cc8a8ea51cd0adddf5dab504a285915-Abstract.html   
[329] X. Li, Z. Zhang, G. Wei, C. Lan, W. Zeng, X. Jin, and Z. Chen, ‚ÄúConfounder identification-free causal visual feature learning,‚Äù CoRR, vol. abs/2111.13420, 2021. [Online]. Available: https://arxiv.org/abs/2111.13420   
[330] C. Finn, P. Abbeel, and S. Levine, ‚ÄúModel-agnostic meta-learning for fast adaptation of deep networks,‚Äù in International conference on machine learning. PMLR, 2017, pp. 1126‚Äì1135.   
[331] Y. Liu, R. Cadei, J. Schweizer, S. Bahmani, and A. Alahi, ‚ÄúTowards robust and adaptive motion forecasting: A causal representation perspective,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2111.14820   
[332] D. Zhang, H. Zhang, J. Tang, X. Hua, and Q. Sun, ‚ÄúCausal intervention for weakly-supervised semantic segmentation,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/hash/ 07211688a0869d995947a8fb11b215d6-Abstract.html   
[333] J. Huang, Y. Qin, J. Qi, Q. Sun, and H. Zhang, ‚ÄúDeconfounded visual grounding,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2112.15324   
[334] Y. Wang and D. M. Blei, ‚ÄúThe blessings of multiple causes,‚Äù 2018. [Online]. Available: https://arxiv.org/abs/1805.06826   
[335] E. L. Ogburn, I. Shpitser, and E. J. T. Tchetgen, ‚ÄúComment on ‚Äúblessings of multiple causes‚Äù,‚Äù Journal of the American Statistical Association, vol. 114, no. 528, pp. 1611‚Äì1615, 2019.   
[336] A. D‚ÄôAmour, ‚ÄúOn multi-cause approaches to causal inference with unobserved counfounding: Two cautionary failure cases and a promising alternative,‚Äù in The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019, pp. 3478‚Äì3486.   
[337] M. Otani, Y. Nakashima, E. Rahtu, and J. Heikkil√§, ‚ÄúUncovering hidden challenges in query-based video moment retrieval,‚Äù 2020. [Online]. Available: https://arxiv.org/abs/2009.00325

[338] G. Nan, R. Qiao, Y. Xiao, J. Liu, S. Leng, H. Zhang, and W. Lu, ‚ÄúInterventional video grounding with dual contrastive learning,‚Äù in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 2765‚Äì2775.   
[339] X. Yang, F. Feng, W. Ji, M. Wang, and T.-S. Chua, ‚ÄúDeconfounded video moment retrieval with causal intervention,‚Äù in Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021, pp. 1‚Äì10.   
[340] Y. Goyal, Z. Wu, J. Ernst, D. Batra, D. Parikh, and S. Lee, ‚ÄúCounterfactual visual explanations,‚Äù in Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 09‚Äì15 Jun 2019, pp. 2376‚Äì2384. [Online]. Available: https://proceedings.mlr.press/v97/ goyal19a.html   
[341] L. A. Hendricks, R. Hu, T. Darrell, and Z. Akata, ‚ÄúGenerating counterfactual explanations with natural language,‚Äù CoRR, vol. abs/1806.09809, 2018. [Online]. Available: http://arxiv.org/abs/1806.09809   
[342] Z. Yue, T. Wang, H. Zhang, Q. Sun, and X.-S. Hua, ‚ÄúCounterfactual Zero-Shot and Open-Set Visual Recognition,‚Äù arXiv:2103.00887 [cs], Mar. 2021, arXiv: 2103.00887. [Online]. Available: http://arxiv.org/abs/2103.00887   
[343] J. Pearl, ‚ÄúBrief report: On the consistency rule in causal inference: "axiom, definition, assumption, or theorem?",‚Äù Epidemiology, vol. 21, no. 6, pp. 872‚Äì875, 2010. [Online]. Available: http://www.jstor.org/stable/20788241   
[344] X. Zhang, Y. Wong, X. Wu, J. Lu, M. Kankanhalli, X. Li, and W. Geng, ‚ÄúLearning causal representation for training cross-domain pose estimator via generative interventions,‚Äù in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11 270‚Äì11 280.   
[345] Y. Li, A. Torralba, A. Anandkumar, D. Fox, and A. Garg, ‚ÄúCausal discovery in physical systems from videos,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 9180‚Äì9192, 2020.   
[346] T. D. Kulkarni, A. Gupta, C. Ionescu, S. Borgeaud, M. Reynolds, A. Zisserman, and V. Mnih, ‚ÄúUnsupervised learning of object keypoints for perception and control,‚Äù Advances in neural information processing systems, vol. 32, 2019.   
[347] S. Janny, F. Baradel, N. Neverova, M. Nadri, G. Mori, and C. Wolf, ‚ÄúFilteredcophy: Unsupervised learning of counterfactual physics in pixel space,‚Äù arXiv preprint arXiv:2202.00368, 2022.   
[348] D. Maliniak, R. Powers, and B. F. Walter, ‚ÄúThe gender citation gap in international relations,‚Äù International Organization, vol. 67, no. 4, pp. 889‚Äì922, 2013.

[349] A. Feder, K. A. Keith, E. Manzoor, R. Pryzant, D. Sridhar, Z. Wood-Doughty, J. Eisenstein, J. Grimmer, R. Reichart, M. E. Roberts, B. M. Stewart, V. Veitch, and D. Yang, ‚ÄúCausal inference in natural language processing: Estimation, prediction, interpretation and beyond,‚Äù CoRR, vol. abs/2109.00725, 2021.   
[350] C. Fong and J. Grimmer, ‚ÄúDiscovery of treatments from text corpora,‚Äù in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2016, pp. 1600‚Äì1609.   
[351] R. Pryzant, K. Shen, D. Jurafsky, and S. Wagner, ‚ÄúDeconfounded lexicon induction for interpretable social science,‚Äù in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 1615‚Äì1625.   
[352] M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh, ‚ÄúBeyond accuracy: Behavioral testing of NLP models with checklist,‚Äù CoRR, vol. abs/2005.04118, 2020.   
[353] A. Ross, T. Wu, H. Peng, M. E. Peters, and M. Gardner, ‚ÄúTailor: Generating and perturbing text with semantic controls,‚Äù CoRR, vol. abs/2107.07150, 2021.   
[354] R. Adragna, E. Creager, D. Madras, and R. S. Zemel, ‚ÄúFairness and robustness in invariant learning: A case study in toxicity classification,‚Äù CoRR, vol. abs/2011.06485, 2020.   
[355] S. Garg, V. Perot, N. Limtiaco, A. Taly, E. H. Chi, and A. Beutel, ‚ÄúCounterfactual fairness in text classification through robustness,‚Äù CoRR, vol. abs/1809.10610, 2018.   
[356] Q. Liu, M. J. Kusner, and P. Blunsom, ‚ÄúCounterfactual data augmentation for neural machine translation,‚Äù in NAACL-HLT. Association for Computational Linguistics, 2021, pp. 187‚Äì197.   
[357] D. Kiela, H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, and D. Testuggine, ‚ÄúThe hateful memes challenge: Detecting hate speech in multimodal memes,‚Äù 2020. [Online]. Available: https://arxiv.org/abs/2005. 04790   
[358] E. Abbasnejad, D. Teney, A. Parvaneh, J. Shi, and A. van den Hengel, ‚ÄúCounterfactual Vision and Language Learning,‚Äù in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Seattle, WA, USA: IEEE, Jun. 2020, pp. 10 041‚Äì10 051. [Online]. Available: https://ieeexplore.ieee.org/document/9156448/   
[359] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-W. Chang, ‚ÄúGenerating natural language adversarial examples,‚Äù in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 2890‚Äì2896. [Online]. Available: https://aclanthology.org/D18-1316

[360] M. Iyyer, J. Wieting, K. Gimpel, and L. Zettlemoyer, ‚ÄúAdversarial example generation with syntactically controlled paraphrase networks,‚Äù 2018. [Online]. Available: https://arxiv.org/abs/1804.06059   
[361] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, ‚ÄúHotflip: White-box adversarial examples for text classification,‚Äù 2017. [Online]. Available: https://arxiv.org/abs/1712.06751   
[362] M. Ye, C. Gong, and Q. Liu, ‚ÄúSafer: A structure-free approach for certified robustness to adversarial word substitutions,‚Äù 2020. [Online]. Available: https://arxiv.org/abs/2005.14424   
[363] M. Mozes, M. Bartolo, P. Stenetorp, B. Kleinberg, and L. D. Griffin, ‚ÄúContrasting human- and machine-generated word-level adversarial examples for text classification,‚Äù in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 8258‚Äì8270. [Online]. Available: https://doi.org/10.18653/v1/2021.emnlp-main.651   
[364] H. Zhao, C. Ma, X. Dong, A. T. Luu, Z.-H. Deng, and H. Zhang, ‚ÄúCertified robustness against natural language attacks by causal intervention,‚Äù arXiv preprint arXiv:2205.12331, 2022.   
[365] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, ‚ÄúLearning word vectors for sentiment analysis,‚Äù in Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, 2011, pp. 142‚Äì150.   
[366] A. Feder, N. Oved, U. Shalit, and R. Reichart, ‚ÄúCausalm: Causal model explanation through counterfactual language models,‚Äù Comput. Linguistics, vol. 47, no. 2, pp. 333‚Äì386, 2021. [Online]. Available: https://doi.org/10.1162/coli_a_00404   
[367] M. De-Arteaga, A. Romanov, H. Wallach, J. Chayes, C. Borgs, A. Chouldechova, S. Geyik, K. Kenthapadi, and A. T. Kalai, ‚ÄúBias in bios: A case study of semantic representation bias in a high-stakes setting,‚Äù in Proceedings of the Conference on Fairness, Accountability, and Transparency, ser. FAT* ‚Äô19. New York, NY, USA: Association for Computing Machinery, 2019, p. 120‚Äì128. [Online]. Available: https://doi.org/10.1145/3287560.3287572   
[368] J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer, and S. M. Shieber, ‚ÄúInvestigating gender bias in language models using causal mediation analysis,‚Äù in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. [Online]. Available: https://proceedings.neurips. cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html

[369] S. Garg, V. Perot, N. Limtiaco, A. Taly, E. H. Chi, and A. Beutel, ‚ÄúCounterfactual fairness in text classification through robustness,‚Äù in Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019, Honolulu, HI, USA, January 27-28, 2019, V. Conitzer, G. K. Hadfield, and S. Vallor, Eds. ACM, 2019, pp. 219‚Äì226. [Online]. Available: https://doi.org/10.1145/3306618.3317950   
[370] X. Zeng, Y. Li, Y. Zhai, and Y. Zhang, ‚ÄúCounterfactual generator: A weaklysupervised method for named entity recognition,‚Äù in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Association for Computational Linguistics, 2020, pp. 7270‚Äì7280. [Online]. Available: https://doi.org/10.18653/v1/2020.emnlp-main.590   
[371] T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai, ‚ÄúMan is to computer programmer as woman is to homemaker? debiasing word embeddings,‚Äù in Advances in Neural Information Processing Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29. Curran Associates, Inc., 2016. [Online]. Available: https://proceedings.neurips.cc/ paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf   
[372] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, and K.-W. Chang, ‚ÄúMen also like shopping: Reducing gender bias amplification using corpus-level constraints,‚Äù in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Copenhagen, Denmark: Association for Computational Linguistics, Sep. 2017, pp. 2979‚Äì2989. [Online]. Available: https://aclanthology.org/D17-1323   
[373] R. Zmigrod, S. J. Mielke, H. Wallach, and R. Cotterell, ‚ÄúCounterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology,‚Äù in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 1651‚Äì1661. [Online]. Available: https://aclanthology.org/P19-1161   
[374] J. Nivre, M. Abrams, ≈Ω. Agiƒá, L. Ahrenberg, L. Antonsen, K. Aplonova, M. J. Aranzabe, G. Arutie, M. Asahara, L. Ateyah, M. Attia, A. Atutxa, L. Augustinus, E. Badmaeva, M. Ballesteros, E. Banerjee, S. Bank, V. Barbu Mititelu, V. Basmov, J. Bauer, S. Bellato, K. Bengoetxea, Y. Berzak, I. A. Bhat, R. A. Bhat, E. Biagetti, E. Bick, R. Blokland, V. Bobicev, C. B√∂rstell, C. Bosco, G. Bouma, S. Bowman, A. Boyd, A. Burchardt, M. Candito, B. Caron, G. Caron, G. Cebiroƒülu Eryiƒüit, F. M. Cecchini, G. G. A. Celano, S. ƒå√©pl√∂, S. Cetin, F. Chalub, J. Choi, Y. Cho, J. Chun, S. Cinkov√°, A. Collomb, √á. √á√∂ltekin, M. Connor, M. Courtin, E. Davidson, M.-C. de Marneffe, V. de Paiva, A. Diaz de Ilarraza, C. Dickerson, P. Dirix, K. Dobrovoljc, T. Dozat, K. Droganova, P. Dwivedi, M. Eli, A. Elkahky, B. Ephrem, T. Erjavec, A. Etienne,

R. Farkas, H. Fernandez Alcalde, J. Foster, C. Freitas, K. Gajdo≈°ov√°, D. Galbraith, M. Garcia, M. G√§rdenfors, S. Garza, K. Gerdes, F. Ginter, I. Goenaga, K. Gojenola, M. G√∂kƒ±rmak, Y. Goldberg, X. G√≥mez Guinovart, B. Gonz√°les Saavedra, M. Grioni, N. Gr¬Øuz¬Øƒ±tis, B. Guillaume, C. Guillot-Barbance, N. Habash, J. Hajiƒç, J. Hajiƒç jr., L. H√† MÀúy, N.-R. Han, K. Harris, D. Haug, B. Hladk√°, J. Hlav√°ƒçov√°, F. Hociung, P. Hohle, J. Hwang, R. Ion, E. Irimia, O. . Ishola, T. Jel√≠nek, A. Johannsen, F. J√∏rgensen, H. Ka≈üƒ±kara, S. Kahane, H. Kanayama, J. Kanerva, B. Katz, T. Kayadelen, J. Kenney, V. Kettnerov√°, J. Kirchner, K. Kopacewicz, N. Kotsyba, S. Krek, S. Kwak, V. Laippala, L. Lambertino, L. Lam, T. Lando, S. D. Larasati, A. Lavrentiev, J. Lee, P. L√™ H`√¥ng, A. Lenci, S. Lertpradit, H. Leung, C. Y. Li, J. Li, K. Li, K. Lim, N. Ljube≈°iƒá, O. Loginova, O. Lyashevskaya, T. Lynn, V. Macketanz, A. Makazhanov, M. Mandl, C. Manning, R. Manurung, C. MƒÉrƒÉnduc, D. Mareƒçek, K. Marheinecke, H. Mart√≠nez Alonso, A. Martins, J. Ma≈°ek, Y. Matsumoto, R. McDonald, G. Mendon√ßa, N. Miekka, M. Misirpashayeva, A. Missil√§, C. Mititelu, Y. Miyao, S. Montemagni, A. More, L. Moreno Romero, K. S. Mori, S. Mori, B. Mortensen, B. Moskalevskyi, K. Muischnek, Y. Murawaki, K. M√º√ºrisep, P. Nainwani, J. I. Navarro Hor√±iacek, A. Nedoluzhko, G. Ne≈°pore-B¬Øerzkalne, L. NguyÀú√™n Thi., H. NguyÀú√™n Thi. Minh, V. Nikolaev, R. Nitisaroj, H. Nurmi, S. Ojala, A. Ol√∫√≤kun, M. Omura, P. Osenova, R. √ñstling, L. √òvrelid, N. Partanen, E. Pascual, M. Passarotti, A. Patejuk, G. Paulino-Passos, S. Peng, C.-A. Perez, G. Perrier, S. Petrov, J. Piitulainen, E. Pitler, B. Plank, T. Poibeau, M. Popel, L. Pretkalnin, a, S. Pr√©vost, P. Prokopidis, A. Przepi√≥rkowski, T. Puolakainen, S. Pyysalo, A. R√§√§bis, A. Rademaker, L. Ramasamy, T. Rama, C. Ramisch, V. Ravishankar, L. Real, S. Reddy, G. Rehm, M. Rie√üler, L. Rinaldi, L. Rituma, L. Rocha, M. Romanenko, R. Rosa, D. Rovati, V. Ros,ca, O. Rudina, J. Rueter, S. Sadde, B. Sagot, S. Saleh, T. Samard≈æiƒá, S. Samson, M. Sanguinetti, B. Saul¬Øƒ±te, Y. Sawanakunanon, N. Schneider, S. Schuster, D. Seddah, W. Seeker, M. Seraji, M. Shen, A. Shimada, M. Shohibussirri, D. Sichinava, N. Silveira, M. Simi, R. Simionescu, K. Simk√≥, M. ≈†imkov√°, K. Simov, A. Smith, I. Soares-Bastos, C. Spadine, A. Stella, M. Straka, J. Strnadov√°, A. Suhr, U. Sulubacak, Z. Sz√°nt√≥, D. Taji, Y. Takahashi, T. Tanaka, I. Tellier, T. Trosterud, A. Trukhina, R. Tsarfaty, F. Tyers, S. Uematsu, Z. Ure≈°ov√°, L. Uria, H. Uszkoreit, S. Vajjala, D. van Niekerk, G. van Noord, V. Varga, E. Villemonte de la Clergerie, V. Vincze, L. Wallin, J. X. Wang, J. N. Washington, S. Williams, M. Wir√©n, T. Woldemariam, T.-s. Wong, C. Yan, M. M. Yavrumyan, Z. Yu, Z. ≈Ωabokrtsk√Ω, A. Zeldes, D. Zeman, M. Zhang, and H. Zhu, ‚ÄúUniversal dependencies 2.3,‚Äù 2018, LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (√öFAL), Faculty of Mathematics and Physics, Charles University. [Online]. Available: http://hdl.handle.net/11234/1-2895

[375] B. Sch√∂lkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. M. Mooij, ‚ÄúOn causal and anticausal learning,‚Äù in Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress, 2012. [Online]. Available: http://icml.cc/2012/papers/625.pdf   
[376] N. Kilbertus*, G. Parascandolo*, and B. Sch√∂lkopf*, ‚ÄúGeneralization in anticausal learning,‚Äù in NeurIPS 2018 Workshop on Critiquing and Correcting Trends in Machine Learning, Dec. 2018, *authors are listed in alphabetical order. [Online]. Available: https://ml-critique-correct.github.io/   
[377] Z. Jin, J. von K√ºgelgen, J. Ni, T. Vaidhya, A. Kaushal, M. Sachan, and B. Sch√∂lkopf, ‚ÄúCausal direction of data collection matters: Implications of causal and anticausal learning for nlp,‚Äù in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Nov. 2021, pp. 9499‚Äì9513, *equal contribution. [Online]. Available: https://aclanthology.org/2021.emnlp-main.748   
[378] W. Fan, Y. Ma, Q. Li, Y. He, E. Zhao, J. Tang, and D. Yin, ‚ÄúGraph neural networks for social recommendation,‚Äù in The world wide web conference, 2019, pp. 417‚Äì426.   
[379] X. Jing and J. Xu, ‚ÄúFast and effective protein model refinement using deep graph neural networks,‚Äù Nature computational science, vol. 1, no. 7, pp. 462‚Äì 469, 2021.   
[380] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, ‚ÄúOpen graph benchmark: Datasets for machine learning on graphs,‚Äù Advances in neural information processing systems, vol. 33, pp. 22 118‚Äì22 133, 2020.   
[381] Y. Wu, X. Wang, A. Zhang, X. He, and T.-S. Chua, ‚ÄúDiscovering invariant rationales for graph neural networks,‚Äù in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/ forum?id=hGXij5rfiHw   
[382] Y. Chen, Y. Zhang, H. Yang, K. Ma, B. Xie, T. Liu, B. Han, and J. Cheng, ‚ÄúInvariance principle meets out-of-distribution generalization on graphs,‚Äù arXiv preprint arXiv:2202.05441, 2022.   
[383] Y. Sui, X. Wang, J. Wu, X. He, and T.-S. Chua, ‚ÄúDeconfounded training for graph neural networks,‚Äù ArXiv, vol. abs/2112.15089, 2021.   
[384] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad, ‚ÄúCollective classification in network data,‚Äù AI magazine, vol. 29, no. 3, pp. 93‚Äì93, 2008.   
[385] T. Zhao, G. Liu, D. Wang, W. Yu, and M. Jiang, ‚ÄúLearning from counterfactual links for link prediction,‚Äù 2021. [Online]. Available: https: //arxiv.org/abs/2106.02172

[386] D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, and X. Sun, ‚ÄúMeasuring and relieving the over-smoothing problem for graph neural networks from the topological view,‚Äù in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 2020, pp. 3438‚Äì3445. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/5747   
[387] F. Feng, W. Huang, X. He, X. Xin, Q. Wang, and T.-S. Chua, ‚ÄúShould graph convolution trust neighbors? a simple causal inference method,‚Äù in Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021, pp. 1208‚Äì1218.   
[388] M. Zeƒçeviƒá, D. S. Dhami, P. Veliƒçkoviƒá, and K. Kersting, ‚ÄúRelating graph neural networks to structural causal models,‚Äù 2021.   
[389] E. Todorov, T. Erez, and Y. Tassa, ‚ÄúMujoco: A physics engine for model-based control,‚Äù in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012, pp. 5026‚Äì5033.   
[390] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, ‚ÄúThe arcade learning environment: An evaluation platform for general agents,‚Äù Journal of Artificial Intelligence Research, vol. 47, pp. 253‚Äì279, jun 2013.   
[391] N. R. Ke, A. Didolkar, S. Mittal, A. Goyal, G. Lajoie, S. Bauer, D. Rezende, Y. Bengio, M. Mozer, and C. Pal, ‚ÄúSystematic evaluation of causal discovery in visual model based reinforcement learning,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2107.00848   
[392] J. X. Wang, M. King, N. P. M. Porcel, Z. Kurth-Nelson, T. Zhu, C. Deck, P. Choy, M. Cassin, M. Reynolds, H. F. Song, G. Buttimore, D. P. Reichert, N. C. Rabinowitz, L. Matthey, D. Hassabis, A. Lerchner, and M. Botvinick, ‚ÄúAlchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents,‚Äù in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [Online]. Available: https://openreview.net/forum?id=eZu4BZxlRnX   
[393] D. McDuff, Y. Song, J. Lee, V. Vineet, S. Vemprala, N. A. Gyde, H. Salman, S. Ma, K. Sohn, and A. Kapoor, ‚ÄúCausalcity: Complex simulations with agency for causal discovery and reasoning,‚Äù in First Conference on Causal Learning and Reasoning, 2022. [Online]. Available: https://openreview.net/forum?id=YWRhER626PX   
[394] K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum, ‚ÄúClevrer: Collision events for video representation and reasoning,‚Äù in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=HkxYzANYDB

[395] J. Devlin, M. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: pre-training of deep bidirectional transformers for language understanding,‚Äù in NAACL-HLT (1). Association for Computational Linguistics, 2019, pp. 4171‚Äì4186.   
[396] D. Kaushik, E. Hovy, and Z. Lipton, ‚ÄúLearning the difference that makes a difference with counterfactually-augmented data,‚Äù in International Conference on Learning Representations, 2020. [Online]. Available: https: //openreview.net/forum?id=Sklgs0NFvr   
[397] J. Frohberg and F. Binder, ‚ÄúCrass: A novel data set and benchmark to test counterfactual reasoning of large language models,‚Äù 2021.   
[398] A. Srivastava, A. Rastogi, A. B. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. A. Rahane, A. S. Iyer, A. J. Andreassen, A. Santilli, A. Stuhlmuller, A. M. Dai, A. D. La, A. K. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubarajan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakacs, B. R. Roberts, B. S. Loe, B. Zoph, B. Bojanowski, B. Ozyurt, B. Hedayatnia, B. Neyshabur, B. Inden, B. Stein, B. Ekmekci, B. Y. Lin, B. S. Howald, C. Diao, C. Dour, C. Stinson, C. Argueta, C. F. Ram‚Äôirez, C. Singh, C. Rathkopf, C. Meng, C. Baral, C. Wu, C. Callison-Burch, C. Waites, C. Voigt, C. D. Manning, C. Potts, C. T. Ramirez, C. Rivera, C. Siro, C. Raffel, C. Ashcraft, C. Garbacea, D. Sileo, D. H. Garrette, D. Hendrycks, D. Kilman, D. Roth, D. Freeman, D. Khashabi, D. Levy, D. Gonz‚Äôalez, D. Hernandez, D. Chen, D. Ippolito, D. Gilboa, D. Dohan, D. Drakard, D. Jurgens, D. Datta, D. Ganguli, D. Emelin, D. Kleyko, D. Yuret, D. Chen, D. Tam, D. Hupkes, D. Misra, D. Buzan, D. C. Mollo, D. Yang, D.-H. Lee, E. Shutova, E. D. Cubuk, E. Segal, E. Hagerman, E. Barnes, E. P. Donoway, E. Pavlick, E. Rodol√†, E. F. Lam, E. Chu, E. Tang, E. Erdem, E. Chang, E. A. Chi, E. Dyer, E. Jerzak, E. Kim, E. E. Manyasi, E. Zheltonozhskii, F. Xia, F. Siar, F. Mart‚Äôinez-Plumed, F. Happ‚Äôe, F. Chollet, F. Rong, G. Mishra, G. I. Winata, G. de Melo, G. Kruszewski, G. Parascandolo, G. Mariani, G. Wang, G. Jaimovitch-L‚Äôopez, G. Betz, G. Gur-Ari, H. Galijasevic, H. S. Kim, H. Rashkin, H. Hajishirzi, H. Mehta, H. Bogar, H. Shevlin, H. Sch√ºtze, H. Yakura, H. Zhang, H. Wong, I. A.-S. Ng, I. Noble, J. Jumelet, J. Geissinger, J. Kernion, J. Hilton, J. Lee, J. F. Fisac, J. B. Simon, J. Koppel, J. Zheng, J. Zou, J. Koco‚Äôn, J. Thompson, J. Kaplan, J. Radom, J. Sohl-Dickstein, J. Phang, J. Wei, J. Yosinski, J. Novikova, J. Bosscher, J. Marsh, J. Kim, J. Taal, J. Engel, J. O. Alabi, J. Xu, J. Song, J. Tang, J. W. Waweru, J. Burden, J. Miller, J. U. Balis, J. Berant, J. Frohberg, J. Rozen, J. Hern√°ndez-Orallo, J. Boudeman, J. Jones, J. B. Tenenbaum, J. S. Rule, J. Chua, K. Kanclerz, K. Livescu, K. Krauth, K. Gopalakrishnan, K. Ignatyeva, K. Markert, K. D. Dhole, K. Gimpel, K. O. Omondi, K. W. Math-

ewson, K. Chiafullo, K. Shkaruta, K. Shridhar, K. McDonell, K. Richardson, L. Reynolds, L. Gao, L. Zhang, L. Dugan, L. Qin, L. Contreras-Ochando, L.-P. Morency, L. Moschella, L. Lam, L. Noble, L. Schmidt, L. He, L. O. Col‚Äôon, L. Metz, L. K. cSenel, M. Bosma, M. Sap, M. ter Hoeve, M. Andrea, M. S. Farooqi, M. Faruqui, M. Mazeika, M. Baturan, M. Marelli, M. Maru, M. Quintana, M. Tolkiehn, M. Giulianelli, M. Lewis, M. Potthast, M. Leavitt, M. Hagen, M. Schubert, M. Baitemirova, M. Arnaud, M. A. McElrath, M. A. Yee, M. Cohen, M. Gu, M. I. Ivanitskiy, M. Starritt, M. Strube, M. Swkedrowski, M. Bevilacqua, M. Yasunaga, M. Kale, M. Cain, M. Xu, M. Suzgun, M. Tiwari, M. Bansal, M. Aminnaseri, M. Geva, M. Gheini, T. MukundVarma, N. Peng, N. Chi, N. Lee, N. G.-A. Krakover, N. Cameron, N. S. Roberts, N. Doiron, N. Nangia, N. Deckers, N. Muennighoff, N. S. Keskar, N. Iyer, N. Constant, N. Fiedel, N. Wen, O. Zhang, O. Agha, O. Elbaghdadi, O. Levy, O. Evans, P. Casares, P. Doshi, P. Fung, P. P. Liang, P. Vicol, P. Alipoormolabashi, P. Liao, P. Liang, P. W. Chang, P. Eckersley, P. M. Htut, P.-B. Hwang, P. Milkowski, P. S. Patil, P. Pezeshkpour, P. Oli, Q. Mei, Q. LYU, Q. Chen, R. Banjade, R. E. Rudolph, R. Gabriel, R. Habacker, R. R. Delgado, R. Milli√®re, R. Garg, R. Barnes, R. A. Saurous, R. Arakawa, R. Raymaekers, R. Frank, R. Sikand, R. Novak, R. Sitelew, R. Lebras, R. Liu, R. Jacobs, R. Zhang, R. Salakhutdinov, R. Chi, R. Lee, R. Stovall, R. Teehan, R. Yang, S. J. Singh, S. M. Mohammad, S. Anand, S. Dillavou, S. Shleifer, S. Wiseman, S. Gruetter, S. Bowman, S. S. Schoenholz, S. Han, S. Kwatra, S. A. Rous, S. Ghazarian, S. Ghosh, S. Casey, S. Bischoff, S. Gehrmann, S. Schuster, S. Sadeghi, S. S. Hamdan, S. Zhou, S. Srivastava, S. Shi, S. Singh, S. Asaadi, S. S. Gu, S. Pachchigar, S. Toshniwal, S. Upadhyay, S. Debnath, S. Shakeri, S. Thormeyer, S. Melzi, S. Reddy, S. P. Makini, S. hwan Lee, S. B. Torene, S. Hatwar, S. Dehaene, S. Divic, S. Ermon, S. R. Biderman, S. C. Lin, S. Prasad, S. T. Piantadosi, S. M. Shieber, S. Misherghi, S. Kiritchenko, S. Mishra, T. Linzen, T. Schuster, T. Li, T. Yu, T. A. Ali, T. Hashimoto, T.-L. Wu, T. Desbordes, T. Rothschild, T. Phan, T. Wang, T. Nkinyili, T. Schick, T. N. Kornev, T. Telleen-Lawton, T. Tunduny, T. Gerstenberg, T. Chang, T. Neeraj, T. Khot, T. O. Shultz, U. Shaham, V. Misra, V. Demberg, V. Nyamai, V. Raunak, V. V. Ramasesh, V. U. Prabhu, V. Padmakumar, V. Srikumar, W. Fedus, W. Saunders, W. Zhang, W. Vossen, X. Ren, X. F. Tong, X. Wu, X. Shen, Y. Yaghoobzadeh, Y. Lakretz, Y. Song, Y. Bahri, Y. J. Choi, Y. Yang, Y. Hao, Y. Chen, Y. Belinkov, Y. Hou, Y. Hou, Y. Bai, Z. Seid, Z. Xinran, Z. Zhao, Z. F. Wang, Z. J. Wang, Z. Wang, Z. Wu, S. Singh, and U. Shaham, ‚ÄúBeyond the imitation game: Quantifying and extrapolating the capabilities of language models,‚Äù ArXiv, vol. abs/2206.04615, 2022.

[399] L. Yang, Z. Wang, Y. Wu, J. Yang, and Y. Zhang, ‚ÄúTowards fine-grained causal reasoning and qa,‚Äù arXiv preprint arXiv:2204.07408, 2022.   
[400] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, ‚ÄúAutomatic differentiation in pytorch,‚Äù 2017.

[401] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard et al., ‚Äú{TensorFlow}: a system for {Large-Scale} machine learning,‚Äù in 12th USENIX symposium on operating systems design and implementation (OSDI 16), 2016, pp. 265‚Äì283.   
[402] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang, ‚ÄúJAX: composable transformations of Python+NumPy programs,‚Äù 2018. [Online]. Available: http://github.com/google/jax   
[403] J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner, and M. van Zee, ‚ÄúFlax: A neural network library and ecosystem for JAX,‚Äù 2020. [Online]. Available: http://github.com/google/flax   
[404] T. Hennigan, T. Cai, T. Norman, and I. Babuschkin, ‚ÄúHaiku: Sonnet for JAX,‚Äù 2020. [Online]. Available: http://github.com/deepmind/dm-haiku   
[405] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, ‚ÄúTransformers: State-of-the-art natural language processing,‚Äù in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020, pp. 38‚Äì45. [Online]. Available: https://www.aclweb.org/anthology/2020.emnlp-demos.6   
[406] R. Wightman, ‚ÄúPytorch image models,‚Äù https://github.com/rwightman/ pytorch-image-models, 2019.   
[407] M. Fey and J. E. Lenssen, ‚ÄúFast graph representation learning with PyTorch Geometric,‚Äù in ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.   
[408] K. Xia, K.-Z. Lee, Y. Bengio, and E. Bareinboim, ‚ÄúThe causal-neural connection: Expressiveness, learnability, and inference,‚Äù Advances in Neural Information Processing Systems, vol. 34, 2021.   
[409] M. Scutari, ‚ÄúLearning bayesian networks with the bnlearn r package,‚Äù arXiv preprint arXiv:0908.3817, 2009.   
[410] M. Kalisch, M. M√§chler, D. Colombo, M. H. Maathuis, and P. B√ºhlmann, ‚ÄúCausal inference using graphical models with the r package pcalg,‚Äù Journal of statistical software, vol. 47, pp. 1‚Äì26, 2012.   
[411] A. Sharma and E. Kiciman, ‚ÄúDowhy: An end-to-end library for causal inference,‚Äù arXiv preprint arXiv:2011.04216, 2020.   
[412] H. Chen, T. Harinen, J.-Y. Lee, M. Yung, and Z. Zhao, ‚ÄúCausalml: Python package for causal machine learning,‚Äù arXiv preprint arXiv:2002.11631, 2020.

[413] P. Bach, V. Chernozhukov, M. S. Kurz, and M. Spindler, ‚ÄúDoubleml‚Äìan objectoriented implementation of double machine learning in r,‚Äù arXiv preprint arXiv:2103.09603, 2021.   
[414] James Fox, Tom Everitt, Ryan Carey, Eric Langlois, Alessandro Abate, and Michael Wooldridge, ‚ÄúPyCID: A Python Library for Causal Influence Diagrams,‚Äù in Proceedings of the 20th Python in Science Conference, Meghann Agarwal, Chris Calloway, Dillon Niederhut, and David Shupe, Eds., 2021, pp. 43 ‚Äì 51.   
[415] I. Gulrajani and D. Lopez-Paz, ‚ÄúIn search of lost domain generalization,‚Äù in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [Online]. Available: https://openreview.net/forum?id=lQdXeXDoWtI   
[416] J. Wang, C. Lan, C. Liu, Y. Ouyang, W. Zeng, and T. Qin, ‚ÄúGeneralizing to unseen domains: A survey on domain generalization,‚Äù arXiv preprint arXiv:2103.03097, 2021.   
[417] L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson, ‚ÄúVaribad: A very good method for bayes-adaptive deep rl via metalearning,‚Äù arXiv preprint arXiv:1910.08348, 2019.   
[418] A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, and S. Levine, ‚ÄúMetareinforcement learning of structured exploration strategies,‚Äù Advances in neural information processing systems, vol. 31, 2018.   
[419] L. Cheng, R. Guo, R. Moraffah, P. Sheth, K. S. Candan, and H. Liu, ‚ÄúEvaluation methods and measures for causal learning algorithms,‚Äù IEEE Transactions on Artificial Intelligence, 2022.   
[420] D. B. Rubin, ‚ÄúEstimating causal effects of treatments in randomized and nonrandomized studies.‚Äù Journal of educational Psychology, vol. 66, no. 5, p. 688, 1974.   
[421] K. Quach, ‚ÄúOpenai shuts down robotics team because it doesn‚Äôt have enough data yet,‚Äù 2021, [Online; accessed 30-May-2022]. [Online]. Available: https://www.theregister.com/2021/07/18/in_brief_ai/   
[422] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, ‚ÄúMastering the game of go with deep neural networks and tree search,‚Äù Nat., vol. 529, no. 7587, pp. 484‚Äì489, 2016. [Online]. Available: https://doi.org/10.1038/nature16961   
[423] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis, ‚ÄúMastering the

game of go without human knowledge,‚Äù Nat., vol. 550, no. 7676, pp. 354‚Äì359, 2017. [Online]. Available: https://doi.org/10.1038/nature24270   
[424] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller, ‚ÄúPlaying atari with deep reinforcement learning,‚Äù CoRR, vol. abs/1312.5602, 2013. [Online]. Available: http://arxiv.org/abs/ 1312.5602   
[425] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al., ‚ÄúGrandmaster level in starcraft ii using multi-agent reinforcement learning,‚Äù Nature, vol. 575, no. 7782, pp. 350‚Äì354, 2019.   
[426] M. Komorowski, L. A. Celi, O. Badawi, A. C. Gordon, and A. A. Faisal, ‚ÄúThe artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care,‚Äù Nature medicine, vol. 24, no. 11, pp. 1716‚Äì1720, 2018.   
[427] M. G. Bellemare, S. Candido, P. S. Castro, J. Gong, M. C. Machado, S. Moitra, S. S. Ponda, and Z. Wang, ‚ÄúAutonomous navigation of stratospheric balloons using reinforcement learning,‚Äù Nature, vol. 588, no. 7836, pp. 77‚Äì82, 2020.   
[428] J. Pearl, ‚ÄúInvited commentary: understanding bias amplification,‚Äù American journal of epidemiology, vol. 174, no. 11, pp. 1223‚Äì1227, 2011.   
[429] M. L Mitchell and J. M Jolley, Research design explained, 2010.   
[430] W. H. Jefferys and J. O. Berger, ‚ÄúOckham‚Äôs razor and bayesian analysis,‚Äù American scientist, vol. 80, no. 1, pp. 64‚Äì72, 1992.   
[431] B. Sch√∂lkopf, ‚ÄúCausality for machine learning,‚Äù 2019.   
[432] A. Feder, K. A. Keith, E. Manzoor, R. Pryzant, D. Sridhar, Z. Wood-Doughty, J. Eisenstein, J. Grimmer, R. Reichart, M. E. Roberts, B. M. Stewart, V. Veitch, and D. Yang, ‚ÄúCausal inference in natural language processing: Estimation, prediction, interpretation and beyond,‚Äù 2021.   
[433] T. McCoy, E. Pavlick, and T. Linzen, ‚ÄúRight for the wrong reasons: Diagnosing syntactic heuristics in natural language inference,‚Äù in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 3428‚Äì3448. [Online]. Available: https://aclanthology.org/P19-1334   
[434] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi, ‚ÄúA survey of methods for explaining black box models,‚Äù ACM Comput. Surv., vol. 51, no. 5, aug 2018. [Online]. Available: https://doi.org/10.1145/3236009   
[435] L. Cheng, A. Mosallanezhad, P. Sheth, and H. Liu, ‚ÄúCausal learning for socially responsible ai,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2104.12278

[436] Y. Liu, Y. Wei, H. Yan, G. Li, and L. Lin, ‚ÄúCausal reasoning meets visual representation learning: A prospective study,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2204.12037   
[437] P. Sanchez, J. P. Voisey, T. Xia, H. I. Watson, A. Q. ONeil, and S. A. Tsaftaris, ‚ÄúCausal machine learning for healthcare and precision medicine,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2205.11402   
[438] A. Vlontzos, D. Rueckert, and B. Kainz, ‚ÄúA review of causality for learning algorithms in medical image analysis,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2206.05498   
[439] A. J. DeGrave, J. D. Janizek, and S.-I. Lee, ‚ÄúAi for radiographic covid-19 detection selects shortcuts over signal,‚Äù Nature Machine Intelligence, vol. 3, no. 7, pp. 610‚Äì619, 2021.   
[440] S. R. K√ºnzel, J. S. Sekhon, P. J. Bickel, and B. Yu, ‚ÄúMetalearners for estimating heterogeneous treatment effects using machine learning,‚Äù Proceedings of the National Academy of Sciences, vol. 116, no. 10, pp. 4156‚Äì4165, 2019.   
[441] U. Shalit, F. D. Johansson, and D. Sontag, ‚ÄúEstimating individual treatment effect: generalization bounds and algorithms,‚Äù in International Conference on Machine Learning. PMLR, 2017, pp. 3076‚Äì3085.   
[442] C. Shi, D. Blei, and V. Veitch, ‚ÄúAdapting neural networks for the estimation of treatment effects,‚Äù in Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett, Eds., vol. 32. Curran Associates, Inc., 2019.   
[443] A. Caron, I. Manolopoulou, and G. Baio, ‚ÄúEstimating individual treatment effects using non-parametric regression models: a review,‚Äù arXiv preprint arXiv:2009.06472, 2020.   
[444] X. Nie and S. Wager, ‚ÄúQuasi-oracle estimation of heterogeneous treatment effects,‚Äù Biometrika, 09 2020.   
[445] A. Curth and M. van der Schaar, ‚ÄúNonparametric estimation of heterogeneous treatment effects: From theory to learning algorithms,‚Äù in The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, ser. Proceedings of Machine Learning Research, A. Banerjee and K. Fukumizu, Eds., vol. 130. PMLR, 2021, pp. 1810‚Äì1818.   
[446] L. Nie, M. Ye, qiang liu, and D. Nicolae, ‚Äú{VCN}et and functional targeted regularization for learning causal effects of continuous treatments,‚Äù in International Conference on Learning Representations, 2021.   
[447] Y.-F. Zhang, H. Zhang, Z. C. Lipton, L. E. Li, and E. P. Xing, ‚ÄúExploring transformer backbones for heterogeneous treatment effect estimation,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2202.01336

[448] A. Malek and S. Chiappa, ‚ÄúAsymptotically best causal effect identification with multi-armed bandits,‚Äù in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available: https://openreview.net/forum?id=1dqrBgHYC0d   
[449] C. F. Manski, ‚ÄúNonparametric bounds on treatment effects,‚Äù The American Economic Review, vol. 80, no. 2, pp. 319‚Äì323, 1990.   
[450] C. Manski, Partial identification of probability distributions. Springer, 2003, vol. 5.   
[451] K. Imai, L. Keele, and T. Yamamoto, ‚ÄúIdentification, inference and sensitivity analysis for causal mediation effects,‚Äù Statistical science, vol. 25, no. 1, pp. 51‚Äì71, 2010.   
[452] C. Cinelli, D. Kumor, B. Chen, J. Pearl, and E. Bareinboim, ‚ÄúSensitivity analysis of linear structural causal models,‚Äù in International conference on machine learning. PMLR, 2019, pp. 1252‚Äì1261.   
[453] M. Baiocchi, J. Cheng, and D. S. Small, ‚ÄúInstrumental variable methods for causal inference,‚Äù Statistics in medicine, vol. 33, no. 13, pp. 2297‚Äì2340, 2014.   
[454] W. Miao, Z. Geng, and E. J. Tchetgen Tchetgen, ‚ÄúIdentifying causal effects with proxy variables of an unmeasured confounder,‚Äù Biometrika, vol. 105, no. 4, pp. 987‚Äì993, 2018.   
[455] C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, and M. Welling, ‚ÄúCausal effect inference with deep latent-variable models,‚Äù in Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017.   
[456] L. Xu, H. Kanagawa, and A. Gretton, ‚ÄúDeep proxy causal learning and its application to confounded bandit policy evaluation,‚Äù Advances in Neural Information Processing Systems, vol. 34, 2021.   
[457] L. Gultchin, D. S. Watson, M. J. Kusner, and R. Silva, ‚ÄúOperationalizing complex causes: A pragmatic view of mediation,‚Äù in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 2021, pp. 3875‚Äì3885. [Online]. Available: http://proceedings.mlr.press/v139/gultchin21a.html   
[458] A. Mastouri, Y. Zhu, L. Gultchin, A. Korba, R. Silva, M. Kusner, A. Gretton, and K. Muandet, ‚ÄúProximal causal learning with kernels: Two-stage estimation and moment restriction,‚Äù in International Conference on Machine Learning. PMLR, 2021, pp. 7512‚Äì7523.

[459] A. Ghassami, A. Ying, I. Shpitser, and E. T. Tchetgen, ‚ÄúMinimax kernel machine learning for a class of doubly robust functionals with application to proximal causal inference,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2104.02929   
[460] A. Ying, Y. Cui, and E. J. T. Tchetgen, ‚ÄúProximal causal inference for marginal counterfactual survival curves,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2204.13144   
[461] A. Ghassami, I. Shpitser, and E. T. Tchetgen, ‚ÄúProximal causal inference with hidden mediators: Front-door and related mediation problems,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2111.02927   
[462] J. H. Stock and F. Trebbi, ‚ÄúRetrospectives: Who invented instrumental variable regression?‚Äù Journal of Economic Perspectives, vol. 17, no. 3, pp. 177‚Äì194, 2003.   
[463] R. Singh, M. Sahani, and A. Gretton, ‚ÄúKernel instrumental variable regression,‚Äù in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d‚ÄôAlch√©-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 4595‚Äì4607. [Online]. Available: https://proceedings.neurips. cc/paper/2019/hash/17b3c7061788dbe82de5abe9f6fe22b3-Abstract.html   
[464] A. Bennett, N. Kallus, and T. Schnabel, ‚ÄúDeep generalized method of moments for instrumental variable analysis,‚Äù Advances in neural information processing systems, vol. 32, 2019.   
[465] K. Muandet, A. Mehrjou, S. K. Lee, and A. Raj, ‚ÄúDual instrumental variable regression,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 2710‚Äì2721, 2020.   
[466] N. Dikkala, G. Lewis, L. Mackey, and V. Syrgkanis, ‚ÄúMinimax estimation of conditional moment models,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 12 248‚Äì12 262, 2020.   
[467] Y. Zhu, L. Gultchin, A. Gretton, M. Kusner, and R. Silva, ‚ÄúCausal inference with treatment measurement error: A nonparametric instrumental variable approach,‚Äù in The 38th Conference on Uncertainty in Artificial Intelligence, 2022. [Online]. Available: https://openreview.net/forum?id=SLcxbOUi9gq   
[468] D. M. Chickering, Learning Bayesian Networks is NP-Complete. New York, NY: Springer New York, 1996, pp. 121‚Äì130.   
[469] A. P. Singh and A. W. Moore, Finding optimal Bayesian networks by dynamic programming. Citeseer, 2005.

[470] J. Xiang and S. Kim, ‚ÄúA‚àó lasso for learning a sparse bayesian network structure for continuous variables,‚Äù in Advances in Neural Information Processing Systems, vol. 26, 2013.   
[471] J. Cussens, ‚ÄúBayesian network learning with cutting planes,‚Äù in Uncertainty in Artificial Intelligence, 2011.   
[472] C. Squires and C. Uhler, ‚ÄúCausal structure learning: a combinatorial perspective,‚Äù 2022. [Online]. Available: https://arxiv.org/abs/2206.01152   
[473] M. Scanagatta, C. P. de Campos, G. Corani, and M. Zaffalon, ‚ÄúLearning bayesian networks with thousands of variables,‚Äù in Advances in Neural Information Processing Systems, vol. 28, 2015.   
[474] B. Aragam and Q. Zhou, ‚ÄúConcave penalized estimation of sparse gaussian bayesian networks,‚Äù The Journal of Machine Learning Research, vol. 16, 2015.   
[475] J. D. Ramsey, M. Glymour, R. Sanchez-Romero, and C. Glymour, ‚ÄúA million variables and more: the fast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images,‚Äù International Journal of Data Science and Analytics, vol. 3, 2017.   
[476] X. Zheng, B. Aragam, P. Ravikumar, and E. P. Xing, ‚ÄúDags with NO TEARS: continuous optimization for structure learning,‚Äù in Advances in Neural Information Processing Systems, 2018.   
[477] Y. Yu, J. Chen, T. Gao, and M. Yu, ‚ÄúDAG-GNN: DAG structure learning with graph neural networks,‚Äù in ICML, vol. 97, 2019.   
[478] X. Zheng, C. Dan, B. Aragam, P. Ravikumar, and E. Xing, ‚ÄúLearning sparse nonparametric dags,‚Äù in International Conference on Artificial Intelligence and Statistics. PMLR, 2020, pp. 3414‚Äì3425.   
[479] N. R. Ke, O. Bilaniuk, A. Goyal, S. Bauer, H. Larochelle, B. Sch√∂lkopf, M. C. Mozer, C. Pal, and Y. Bengio, ‚ÄúLearning neural causal models from unknown interventions,‚Äù 2019. [Online]. Available: https://arxiv.org/abs/1910.01075   
[480] I. Ng, A. Ghassami, and K. Zhang, ‚ÄúOn the role of sparsity and DAG constraints for learning linear dags,‚Äù in NeurIPS, 2020.   
[481] P. Brouillard, S. Lachapelle, A. Lacoste, S. Lacoste-Julien, and A. Drouin, ‚ÄúDifferentiable causal discovery from interventional data,‚Äù in NeurIPS, 2020.   
[482] Y. He, P. Cui, Z. Shen, R. Xu, F. Liu, and Y. Jiang, ‚ÄúDARING: differentiable causal discovery with residual independence,‚Äù in KDD, 2021.   
[483] P. Lippe, T. Cohen, and E. Gavves, ‚ÄúEfficient neural causal discovery without acyclicity constraints,‚Äù in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id= eYciPrLuUhG

[484] N. Friedman and D. Koller, ‚ÄúBeing bayesian about network structure. a bayesian approach to structure discovery in bayesian networks,‚Äù Machine learning, vol. 50, 2003.   
[485] M. Gao, Y. Ding, and B. Aragam, ‚ÄúA polynomial-time algorithm for learning nonparametric causal graphs,‚Äù in Advances in Neural Information Processing Systems, 2020.   
[486] C. Cundy, A. Grover, and S. Ermon, ‚ÄúBCD nets: Scalable variational approaches for bayesian causal discovery,‚Äù in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021.   
[487] V. Zantedeschi, J. Kaddour, L. Franceschi, M. Kusner, and V. Niculae, ‚ÄúDAG learning on the permutahedron,‚Äù in ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality, 2022.   
[488] M. J. Vowels, N. C. Camgoz, and R. Bowden, ‚ÄúD‚Äôya like dags? a survey on structure learning and causal discovery,‚Äù arXiv preprint arXiv:2103.02582, 2021.   
[489] S. Magliacane, T. Claassen, and J. M. Mooij, ‚ÄúAncestral causal inference,‚Äù in Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, Eds., 2016, pp. 4466‚Äì4474. [Online]. Available: https://proceedings.neurips. cc/paper/2016/hash/f3d9de86462c28781cbe5c47ef22c3e5-Abstract.html   
[490] J. M. Mooij and T. Claassen, ‚ÄúConstraint-based causal discovery using partial ancestral graphs in the presence of cycles,‚Äù in Conference on Uncertainty in Artificial Intelligence. PMLR, 2020, pp. 1159‚Äì1168.   
[491] D. Watson and R. Silva, ‚ÄúCausal discovery under a confounder blanket,‚Äù in The 38th Conference on Uncertainty in Artificial Intelligence, 2022. [Online]. Available: https://openreview.net/forum?id=S0eeRL8jcec   
[492] F. Eberhardt, ‚ÄúCausal discovery as a game,‚Äù in Causality: Objectives and Assessment. PMLR, 2010, pp. 87‚Äì96.   
[493] A. Hyttinen, F. Eberhardt, and P. O. Hoyer, ‚ÄúExperiment selection for causal discovery,‚Äù Journal of Machine Learning Research, vol. 14, pp. 3041‚Äì3071, 2013.   
[494] J. von K√ºgelgen, P. K. Rubenstein, B. Sch√∂lkopf, and A. Weller, ‚ÄúOptimal experimental design via bayesian optimization: active causal structure learning for gaussian process networks,‚Äù arXiv preprint arXiv:1910.03962, 2019.   
[495] N. Scherrer, O. Bilaniuk, Y. Annadani, A. Goyal, P. Schwab, B. Sch√∂lkopf, M. C. Mozer, Y. Bengio, S. Bauer, and N. R. Ke, ‚ÄúLearning neural

causal models with active interventions,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2109.02429   
[496] P. A. Stokes and P. L. Purdon, ‚ÄúA study of problems encountered in granger causality analysis from a neuroscience perspective,‚Äù Proceedings of the National Academy of Sciences, vol. 114, no. 34, pp. E7063‚ÄìE7072, 2017.   
[497] H. L√ºtkepohl and M. Kr√§tzig, Applied time series econometrics. Cambridge university press, 2004.   
[498] C. Krupitzer, M. Pfannem√ºller, J. Kaddour, and C. Becker, ‚ÄúSatisfy: Towards a self-learning analyzer for time series forecasting in self-improving systems,‚Äù in 2018 IEEE 3rd International Workshops on Foundations and Applications of Self* Systems (FAS* W). IEEE, 2018, pp. 182‚Äì189.   
[499] C. W. Granger, ‚ÄúInvestigating causal relations by econometric models and cross-spectral methods,‚Äù Econometrica: journal of the Econometric Society, pp. 424‚Äì438, 1969.   
[500] A. Tank, I. Covert, N. Foti, A. Shojaie, and E. Fox, ‚ÄúNeural granger causality,‚Äù arXiv preprint arXiv:1802.05842, 2018.   
[501] T. Wu, T. Breuel, M. Skuhersky, and J. Kautz, ‚ÄúDiscovering nonlinear relations with minimum predictive information regularization,‚Äù 2020. [Online]. Available: https://arxiv.org/abs/2001.01885   
[502] S. Khanna and V. Y. F. Tan, ‚ÄúEconomy statistical recurrent units for inferring nonlinear granger causality,‚Äù in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [Online]. Available: https://openreview.net/ forum?id=SyxV9ANFDH   
[503] S. L√∂we, D. Madras, R. Zemel, and M. Welling, ‚ÄúAmortized causal discovery: Learning to infer causal graphs from time-series data,‚Äù arXiv preprint arXiv:2006.10833, 2020.

# Causal Inference:

# A Statistical Learning Approach

Stefan Wager

Stanford University

draft version, comments welcome

November 26, 2025

# Contents

# 1 Randomized Controlled Trials 3

1.1 Difference-in-means estimation . . . . 4   
1.2 Regression adjustments in randomized trials . . 8

# 2 Unconfoundedness and the Propensity Score 17

2.1 Stratified estimation 18   
2.2 Inverse-propensity weighting . . . . 22

# 3 Doubly Robust Methods 29

3.1 Double machine learning . . 32   
3.2 Efficient estimation under uncounfoundedness 39

# 4 Estimating Heterogeneous Treatment Effects 44

4.1 Semiparametric modeling . . . 46   
4.2 A loss function for treatment heterogeneity . . . . 51

# 5 Policy Learning 57

5.1 Policy evaluation 59   
5.2 Empirical-welfare maximization . 63

# 6 Adaptive Experiments 69

6.1 Low-regret data collection 70   
6.2 Inference after adaptive data collection . . . . 76

# 7 Balancing Estimators 84

7.1 Covariate-balancing propensity scores . . . 86   
7.2 Approximate balance and augmented estimators . . . . . . . . . 91

# 8 Regression Discontinuity Designs 98

8.1 Local linear regression 100   
8.2 Optimized estimation and bias-aware inference . . . . . . . . . . 103

# 9 Structural Equation Models 112

9.1 Non-parametric models and do-calculus . . . . . . . 115   
9.2 Instrumental variables regression 119

# 10 Local Average Treatment Effects 128

10.1 Non-compliance in randomized trials . . . . 129   
10.2 Economic models 132

# 11 Spillovers and Interference 141

11.1 Exposure mappings . . . . . . . 142   
11.2 Permutation tests 144

# 12 Estimating Treatment Effects under Interference 152

12.1 Finite-population methods . . . . 155   
12.2 Confidence intervals for exposure effects . . . . . 159

# 13 Event-Study Designs 167

13.1 Difference in differences . 169   
13.2 Synthetic-control methods 176

# 14 Evaluating Dynamic Policies 181

14.1 Sequential unconfoundedness . . . 183   
14.2 Doubly robust estimation 192

# 15 Markov Decision Processes 198

15.1 Doubly robust estimation 200   
15.2 Switchback experiments 209

# 16 Exercises 213

# Chapter 1 Randomized Controlled Trials

How best to understand and characterize causality is an age-old question in philosophy. As such, one might expect that any discussion of causal inference would need to be framed in terms of subtle and esoteric concepts. However, a ground-breaking line of work starting with Neyman [1923] and Rubin [1974] established that‚Äîalthough causality is in general a delicate and complicated notion‚Äîthere exists an important class of problems, randomized controlled trials, where it is possible to approach causal questions in a practical and conceptually straight-forward way via careful application of randomization, averaging, and counterfactual reasoning.1

This chapter presents a brief overview of statistical estimation and inference in randomized controlled trials (RCTs). When available, evidence drawn from RCTs is often considered gold standard statistical evidence; and thus methods for studying RCTs form the foundation of the statistical toolkit for causal inference. Furthermore, many widely used observational study designs in, e.g., econometrics or epidemiology are motivated by analogy to RCTs; and so this chapter will also serve as a stepping stone to subsequent discussions of estimation and inference in observational studies.

Average treatment effects Suppose that we have run a RCT with $n$ study participants $i = 1 , \ldots , n$ $n$ , where each unit $i$ is assigned a binary treatment $W _ { i } \in \{ 0 , 1 \}$ and we then measure an outcome $Y _ { i }$ . Our goal is to estimate the effect of the treatment on the outcome. Following the Neyman‚ÄìRubin causal model, we define the causal effect of a treatment via potential outcomes: For each treatment level $w \in \{ 0 , 1 \}$ , we define potential outcomes $Y _ { i } ( 1 )$ and $Y _ { i } ( 0 )$ corresponding to the outcome the $i$ -th subject would have experienced had they respectively received the treatment or not, such that $Y _ { i } = Y _ { i } ( W _ { i } )$ .

The individual causal effect of the treatment on the $i$ -th unit is then2

$$
\Delta_ {i} = Y _ {i} (1) - Y _ {i} (0). \tag {1.1}
$$

The fundamental problem in causal inference is that only one treatment can be assigned to a given individual, and so only one of $Y _ { i } ( 0 )$ and $Y _ { i } ( 1 )$ can ever be observed. Thus, $\Delta _ { i }$ can never be observed directly.

Although $\Delta _ { i }$ is itself unknowable, we can (perhaps remarkably) use randomized experiments to learn certain properties of the $\Delta _ { i }$ . In finite samples, without any assumptions on how study participants were generated (or equivalently, conditionally on the potential outcomes of study participants), randomization enables us to get unbiased estimates of the sample average treatment effect (SATE)

$$
\bar {\Delta} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(Y _ {i} (1) - Y _ {i} (0)\right). \tag {1.2}
$$

Furthermore, if we assume that study participants are independently drawn from a population $P$ , then randomized experiments enable unbiased and largesample consistent estimates of the (population) average treatment effect (ATE)

$$
\tau = \mathbb {E} _ {P} \left[ Y _ {i} (1) - Y _ {i} (0) \right]. \tag {1.3}
$$

This chapter will discuss properties of a number of different estimators for these two quantities.

# 1.1 Difference-in-means estimation

In a randomized controlled trial, there are many ways to estimate the average treatment effect. Perhaps the simplest and most intuitive way of doing so is via the difference-in-means estimator,

$$
\hat {\tau} _ {D M} := \frac {1}{n _ {1}} \sum_ {W _ {i} = 1} Y _ {i} - \frac {1}{n _ {0}} \sum_ {W _ {i} = 0} Y _ {i}, \quad n _ {w} = | \{i: W _ {i} = w \} |. \tag {1.4}
$$

In our setting, this difference in means estimator is unbiased essentially without assumptions, and the average treatment effect is identified directly via randomization. Suppose that the potential outcomes model given above is valid; or, as

this is often stated in the literature, that the Stable Unit Treatment Values Assumption (SUTVA) holds:

$$
Y _ {i} = Y _ {i} \left(W _ {i}\right), \quad i = 1, \dots , n. \tag {1.5}
$$

Suppose furthermore that the treatment is in fact randomized, i.e., that conditionally all the potential outcomes $\{ Y _ { i } ( 0 ) , Y _ { i } ( 1 ) \} _ { i = 1 } ^ { n }$ and the number of treated units $n _ { 1 }$ , all units are treated with the same probability:3

$$
\mathbb {P} \left[ W _ {i} = 1 \mid \left\{Y _ {i} (0), Y _ {i} (1) \right\} _ {i = 1} ^ {n}, n _ {1} \right] = \frac {n _ {1}}{n}, \quad i = 1, \dots , n. \tag {1.6}
$$

Then ${ \hat { \tau } } _ { D M }$ is finite-sample unbiased for the SATE as defined in (1.2).

Theorem 1.1. Under assumptions (1.5) and (1.6),

$$
\mathbb {E} \left[ \hat {\tau} _ {D M} \mid \{Y _ {i} (0), Y _ {i} (1) \} _ {i = 1} ^ {n}, n _ {0} > 0, n _ {1} > 0 \right] = \bar {\Delta}. \tag {1.7}
$$

Proof. Whenever $n _ { 1 } > 0$ , i.e., we have at least 1 treated unit,

$$
\begin{array}{l} \mathbb {E} \left[ \frac {1}{n _ {1}} \sum_ {W _ {i} = 1} Y _ {i} \mid \{Y _ {i} (0), Y _ {i} (1) \} _ {i = 1} ^ {n}, n _ {1} \right] \\ = \mathbb {E} \left[ \frac {1}{n _ {1}} \sum_ {i = 1} ^ {n} W _ {i} Y _ {i} \mid \{Y _ {i} (0), Y _ {i} (1) \} _ {i = 1} ^ {n}, n _ {1} \right] \\ = \mathbb {E} \left[ \frac {1}{n _ {1}} \sum_ {i = 1} ^ {n} W _ {i} Y _ {i} (1) \mid \{Y _ {i} (0), Y _ {i} (1) \} _ {i = 1} ^ {n}, n _ {1} \right] \tag {SUTVA} \\ = \frac {1}{n _ {1}} \sum_ {i = 1} ^ {n} Y _ {i} (1) \mathbb {E} \left[ W _ {i} \mid \{Y _ {i} (0), Y _ {i} (1) \} _ {i = 1} ^ {n}, n _ {1} \right] \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} Y _ {i} (1) \quad (\text {r a n d o m a s s i g n m e n t}). \\ \end{array}
$$

An analogous result holds for the average of the controls when $n _ { 0 } > 0$ .

Population Asymptotics The result in Theorem 1.1 is valuable in its generality: It provides an unbiasedness result under minimal assumptions, and in particular makes no distributional assumptions on the potential outcomes. In practical terms, this means we can apply Theorem 1.1 without making any claims about how the $n$ study participants were recruited.

A limitation of this result, however, is that it does not characterize the sampling error $\hat { \tau } _ { D M } - \overline { { \Delta } }$ , and so doesn‚Äôt directly provide a roadmap to statistical inference. In order to make progress, we here make an additional assumption that the study participants (i.e., formally, the pairs of potential outcomes $\{ Y _ { i } ( 0 ) , Y _ { i } ( 1 ) \} _ { }$ ) are independently drawn from a population $P$ . Such population-sampling assumptions then enable straight-forward distributional results and confidence intervals via standard large-sample analysis. It is also possible to obtain distributional results without making such sampling assumptions, but doing so relies on specialized statistical techniques that we will not pursue for now; we will revisit population-sampling-free methods for inference in the bibliographic notes at the end of this chapter and in Chapter 12.

Example 1. In 2008, Oregon ran a lottery to allocate additional spots in its Medicaid program to low-income adults. As reported in Finkelstein et al. [2012], $\sim 9 0 , 0 0 0$ people joined the lottery, and of them a (randomly selected) $\sim 3 5 , 0 0 0$ were allowed to apply for Medicaid. The authors consider a number of outcomes, such as healthcare use and expenditures. Finite-sample analysis following Theorem 1.1 shows that, among lottery participants, the differencein-means estimator is unbiased for the average effect of being allowed to apply for Medicaid on outcomes considered, regardless of how the set of lottery participants was created. The asymptotic tools discussed below make a further assumption that the lottery participants were independently sampled from from a relevant larger population (i.e., able-bodied, low-income, uninsured adults with interest in gaining insurance coverage).

A central limit theorem In addition to IID sampling, we will also be more specific about how treatment is randomized, and assume that we are in a Bernoulli trial with4

$$
W _ {i} \left| \left\{Y _ {i} (0), Y _ {i} (1) \right\} \stackrel {\text {i i d}} {\sim} \operatorname {B e r n o u l l i} (\pi), \quad 0 <   \pi <   1. \right. \tag {1.8}
$$

The following central limit theorem for the difference-in-means estimator can then be established via simple statistical arguments.

Theorem 1.2. Under the assumptions of Theorem 1.1, suppose furthermore that the potential outcomes are drawn as $\{ Y _ { i } ( 0 ) , Y _ { i } ( 1 ) \} \stackrel { i i d } { \sim } P$ from a distribution $P$ with bounded second moments and that we run a Bernoulli trial as in (1.8). Then,

$$
\sqrt {n} \left(\hat {\tau} _ {D M} - \tau\right) \Rightarrow \mathcal {N} (0, V _ {D M}), V _ {D M} = \frac {\operatorname {V a r} [ Y _ {i} (0) ]}{1 - \pi} + \frac {\operatorname {V a r} [ Y _ {i} (1) ]}{\pi}. \tag {1.9}
$$

Furthermore, the plug-in variance estimate

$$
\widehat {V} _ {D M} := \frac {n}{n _ {0} ^ {2}} \sum_ {W _ {i} = 0} \left(Y _ {i} - \frac {1}{n _ {0}} \sum_ {W _ {i} = 0} Y _ {i}\right) ^ {2} + \frac {n}{n _ {1} ^ {2}} \sum_ {W _ {i} = 1} \left(Y _ {i} - \frac {1}{n _ {1}} \sum_ {W _ {i} = 1} Y _ {i}\right) ^ {2} \tag {1.10}
$$

is consistent, $\widehat { V } _ { D M } \to _ { p } V _ { D M }$

Proof. Defining potential outcome residuals $\varepsilon _ { i } ( w ) = Y _ { i } ( w ) - \mathbb { E } _ { P } \left[ Y _ { i } ( w ) \right]$ for $w = 0$ , $1$ , we can express our estimation error as

$$
\begin{array}{l} \hat {\tau} _ {D M} - \tau = \frac {1}{n _ {1}} \sum_ {W _ {i} = 1} \varepsilon_ {i} (1) - \frac {1}{n _ {0}} \sum_ {W _ {i} = 1} \varepsilon_ {i} (0) \\ = \frac {n}{n _ {1}} \frac {1}{n} \sum_ {i = 1} ^ {n} W _ {i} \varepsilon_ {i} (1) - \frac {n}{n _ {0}} \frac {1}{n} \sum_ {i = 1} ^ {n} (1 - W _ {i}) \varepsilon_ {i} (0). \\ \end{array}
$$

By randomization, one can verify that $\mathfrak { T } \left[ W _ { i } \varepsilon _ { i } ( 1 ) \right] = \mathbb { P } \left[ W _ { i } \right] \mathbb { E } \left[ \varepsilon _ { i } ( 1 ) \vert W _ { i } = 1 \right] =$ $\mathbb { P } \left[ W _ { i } \right] \mathbb { E } \left[ \varepsilon _ { i } ( 1 ) \right] = 0$ and $\mathbb { E } \left[ ( 1 - W _ { i } ) \varepsilon _ { i } ( 0 ) \right] = 0$ , and finally

$$
\begin{array}{l} \operatorname {V a r} \left[ \binom {W _ {i} \varepsilon_ {i} (1)} {(1 - W _ {i}) \varepsilon_ {i} (0)} \right] = \mathbb {E} \left[ \binom {W _ {i} \varepsilon_ {i} (1)} {(1 - W _ {i}) \varepsilon_ {i} (0)}) ^ {\otimes 2} \right] \\ = \left( \begin{array}{c c} \pi \operatorname {V a r} \left[ \varepsilon_ {i} (1) \right] & 0 \\ 0 & (1 - \pi)   \operatorname {V a r} \left[ \varepsilon_ {i} (0) \right] \end{array} \right). \\ \end{array}
$$

Thus, by the standard multivariate central limit theorem

$$
\sqrt {n} \left(\frac {\frac {1}{n} \sum_ {i = 1} ^ {n} W _ {i} \varepsilon_ {i} (1)}{\frac {1}{n} \sum_ {i = 1} ^ {n} (1 - W _ {i}) \varepsilon_ {i} (0)}\right) \Rightarrow \mathcal {N} \left(0, \left( \begin{array}{c c} \pi \operatorname {V a r} [ \varepsilon_ {i} (1) ] & 0 \\ 0 & (1 - \pi) \operatorname {V a r} [ \varepsilon_ {i} (0) ] \end{array} \right)\right).
$$

The result (1.9) follows by Slutsky‚Äôs lemma because the treatment fraction of a Bernoulli trial concentrates, $n _ { 1 } / n \to _ { p } \pi$ . Meanwhile, (1.10) follows similarly via the weak law of large numbers. ‚ñ°

The above central limit theorem for ${ \hat { \tau } } _ { D M }$ immediately enables asymptotically valid Gaussian confidence intervals for $\tau$ . For any $0 < \alpha < 1$ ,

$$
\lim  _ {n \rightarrow \infty} \mathbb {P} \left[ \tau \in \left(\hat {\tau} _ {D M} \pm \Phi^ {- 1} (1 - \alpha / 2) \sqrt {\widehat {V} _ {D M} / n}\right)\right] = 1 - \alpha , \tag {1.11}
$$

where $\Phi$ denotes the standard Gaussian cumulative distribution function.

From a certain perspective, one could argue that the above is all that is needed to estimate average treatment effects in randomized trials. The difference in means estimator ${ \hat { \tau } } _ { D M }$ is consistent and allows for valid asymptotic inference; moreover, the estimator is very simple to implement, and hard to ‚Äúcheat‚Äù with (i.e., there is little room for an unscrupulous analyst to try different estimation strategies and report the one that gives the answer closest to the one they want). On the other hand, our discussion so far has not established that ${ \hat { \tau } } _ { D M }$ is an ‚Äúoptimal‚Äù way to use the data in any meaningful sense; and in fact, we‚Äôll see below that it‚Äôs often possible to design estimators with guarantees that strictly dominate those for ${ \hat { \tau } } _ { D M }$ .

# 1.2 Regression adjustments in randomized trials

When analyzing randomized controlled trials, we often have access to pretreatment covariates $X _ { i }$ observed together with the treatments $W _ { i }$ and outcomes $Y _ { i }$ . In this case, practitioners often choose to estimate treatment effects via a linear regression based approach rather than via the simple difference in means.

There are two standard ways to estimate average treatment effects via linear regression. The first is to fit a simple linear regression5

$$
Y _ {i} \sim \alpha + W _ {i} \tau + X _ {i} \cdot \beta , \tag {1.12}
$$

and then report the resulting coefficient $\hat { \tau } _ { S R E G } : = \hat { \tau }$ as an estimate of the average treatment effect. The second is to add in full treatment-covariate interactions, and to fit the interacted linear regression

$$
Y _ {i} \sim \alpha + W _ {i} \tau + X _ {i} \cdot \beta + W _ {i} X _ {i} \cdot \gamma . \tag {1.13}
$$

One can then estimate the average treatment effect via the average difference in predictions if everyone vs. no one were treated

$$
\begin{array}{l} \hat {\tau} _ {I R E G} = \frac {1}{n} \sum_ {i = 1} ^ {n} \hat {\alpha} + \hat {\tau} + X _ {i} \cdot (\hat {\beta} + \hat {\gamma}) - \frac {1}{n} \sum_ {i = 1} ^ {n} \hat {\alpha} + X _ {i} \cdot \hat {\beta}, \tag {1.14} \\ = \hat {\tau} + \bar {X} \cdot \hat {\gamma}, \quad \bar {X} := \frac {1}{n} \sum_ {i = 1} ^ {n} X _ {i}. \\ \end{array}
$$

Both the simple and interacted regression can reasonably be deployed in randomized experiments. For the rest of this chapter, we will focus on properties of the interacted regression estimator $\hat { \tau } _ { I R E G }$ because it allows for transparent analysis and is also generally regarded a best practice in the current literature on causal inference; see the bibliographic notes for further discussion.

Regression adjustments under linearity The linear regression estimator (1.13) is a statistical estimator that can be studied under a number of different models for the data. The simplest setting under which to consider the behavior of $\hat { \tau } _ { I R E G }$ (and compare it to that of ${ \hat { \tau } } _ { D M }$ ) is under an assumption that the regression model (1.13) is well specified; and this is the setting we will start with here.

Suppose for now that our samples are independently generated via a Bernoulli randomized trial (1.8) with outcomes $Y _ { i } = Y _ { i } ( W _ { i } )$ and

$$
Y _ {i} (w) = \alpha_ {(w)} + X _ {i} \cdot \beta_ {(w)} + \varepsilon_ {i} (w), \tag {1.15}
$$

$$
\mathbb {E} \left[ \varepsilon_ {i} (w) \mid X _ {i} \right] = 0, \mathrm {V a r} \left[ \varepsilon_ {i} (w) \mid X _ {i} \right] = \sigma^ {2}.
$$

Under Bernoulli randomization, one can check that the observables ( $X _ { i }$ , $Y _ { i }$ , Wi) are independently drawn from a distribution satisfying6

$$
Y _ {i} = \alpha_ {(0)} + W _ {i} \left(\alpha_ {(1)} - \alpha_ {(0)}\right) + X _ {i} \cdot \beta_ {(0)} + W _ {i} X _ {i} \cdot \left(\beta_ {(1)} - \beta_ {(0)}\right) + \varepsilon_ {i}, \tag {1.16}
$$

with $\mathbb { E } \left[ \varepsilon _ { i } \big | X _ { i } , W _ { i } \right] = 0$ and Var $\left[ \varepsilon _ { i } \mid X _ { i } , W _ { i } \right] = \sigma ^ { 2 }$ , i.e., the regression (1.13) is in fact well specified. For simplicity, we will further assume that we are in a balanced randomized trial with $\pi = 5 0 \%$ , and (without loss of generality) $\mathbb { E } \left[ X \right] = 0$ .7

As a warm-up, we first study the behavior of ${ \hat { \tau } } _ { D M }$ under this model as a baseline; we will then be able to compare it with $\hat { \tau } _ { I R E G }$ . Given our general result in Theorem 1.2 all that remains to be done is to spell out what $V _ { D M }$ is here; and, writing Var $| X | = A$ , we get (recall that we‚Äôre using $\pi = 0 . 5$ for simplicity)

$$
\begin{array}{l} V _ {D M} = \frac {\mathrm {V a r} [ Y _ {i} (0) ]}{0 . 5} + \frac {\mathrm {V a r} [ Y _ {i} (1) ]}{0 . 5} \\ = 2 \left(\operatorname {V a r} \left[ X _ {i} \beta_ {(0)} \right] + \sigma^ {2}\right) + 2 \left(\operatorname {V a r} \left[ X _ {i} \beta_ {(1)} \right] + \sigma^ {2}\right) \tag {1.17} \\ = 4 \sigma^ {2} + 2 \left\| \beta_ {(0)} \right\| _ {A} ^ {2} + 2 \left\| \beta_ {(1)} \right\| _ {A} ^ {2} \\ = 4 \sigma^ {2} + \left\| \beta_ {(0)} + \beta_ {(1)} \right\| _ {A} ^ {2} + \left\| \beta_ {(0)} - \beta_ {(1)} \right\| _ {A} ^ {2}, \\ \end{array}
$$

where we used the notation $\| v \| _ { A } ^ { 2 } = v ^ { \prime } A v$ for convenience.

Given that the linear regression model is well specified here, one should expect that $\hat { \tau } _ { I R E G }$ improves over the performance of ${ \hat { \tau } } _ { D M }$ ; the question is by how much. To study the regression estimator, it is helpful to note that the interacted regression (1.13) is algorithmically equivalent to running separate regressions for the treated and control groups and then taking differences of their predictions on the full study sample:

$$
Y _ {i} \sim \alpha_ {(0)} + X _ {i} \cdot \beta_ {(0)} \text {f o r a l l} i \text {w i t h} W _ {i} = 0,
$$

$$
Y _ {i} \sim \alpha_ {(1)} + X _ {i} \cdot \beta_ {(1)} \text {f o r a l l} i \text {w i t h} W _ {i} = 1,
$$

$$
\hat {\tau} _ {I R E G} = \hat {\alpha} _ {(1)} - \hat {\alpha} _ {(0)} + \overline {{X}} \left(\hat {\beta} _ {(1)} - \hat {\beta} _ {(0)}\right).
$$

Standard results about linear regression then imply that, under model (1.15) (recall also that, here, we assume that $\mathbb { E } \left[ X \right] = 0$ )

$$
\sqrt {n _ {w}} \left(\binom {\hat {\alpha} _ {(w)}} {\hat {\beta} _ {(w)}} - \binom {\alpha_ {(w)}} {\beta_ {(w)}}\right) \Rightarrow \mathcal {N} \left(0, \sigma^ {2} \binom {1 0} {0 A ^ {- 1}}\right), \tag {1.18}
$$

and that $\hat { \alpha } _ { ( 0 ) }$ , $\hat { \alpha } _ { ( 1 ) }$ , $\hat { \beta } _ { ( 0 ) }$ , $\hat { \beta } _ { ( 1 ) }$ and $\overline { { X } }$ are all asymptotically independent. Then,

we can write8

$$
\begin{array}{l} \hat{\tau}_{IREG} - \tau = \underbrace{\hat{\alpha}_{(1)} - \alpha_{(1)}}_{\approx \mathcal{N}(0,\sigma^{2} / n_{1})} - \underbrace{(\hat{\alpha}_{(0)} - \alpha_{(0)})}_{\approx \mathcal{N}(0,\sigma^{2} / n_{0})} + \underbrace{\overline{X}\left(\beta_{(1)} - \beta_{(0)}\right)}_{\approx \mathcal{N}\left(0,  \left\| \beta_{(1)} - \beta_{(0)}\right\|_{A}^{2} / n\right)} \\ + \underbrace {\overline {{X}} \left(\hat {\beta} _ {(1)} - \hat {\beta} _ {(0)} - \beta_ {(1)} + \beta_ {(0)}\right)} _ {\mathcal {O} _ {P} (1 / n)}, \\ \end{array}
$$

which leads us to the central limit theorem

$$
\sqrt {n} \left(\hat {\tau} _ {I R E G} - \tau\right) \Rightarrow \mathcal {N} (0, V _ {I R E G}), \quad V _ {I R E G} = 4 \sigma^ {2} + \left\| \beta_ {(0)} - \beta_ {(1)} \right\| _ {A} ^ {2}. \tag {1.19}
$$

After the dust settles we see that, under the linear model (1.15), the interacted regression estimator also satisfies a central limit theorem, and

$$
V _ {I R E G} = V _ {D M} - \left\| \beta_ {(0)} + \beta_ {(1)} \right\| _ {A} ^ {2} \leq V _ {D M}, \tag {1.20}
$$

i.e., the regression estimator usually has a better (and never has a worse) asymptotic variance than the difference-in-means estimator.

Regression adjustments without linearity We showed above that if we assume that the data is generated following a linear model then, as expected, using an estimator that leverages linearity enables more accurate estimates of the average treatment effect than one that doesn‚Äôt. A pessimist might expect that these accuracy gains come at a cost, and that linear regression estimators should face a trade-off whereby they do worse than the difference-in-means estimator when linearity doesn‚Äôt hold. Surprisingly, however, no such tradeoff exists. In randomized trials, $\hat { \tau } _ { I R E G }$ is always consistent for $\tau$ and satisfies an asymptotic non-inferiority results of the type (1.20), even when the linear regression underlying $\hat { \tau } _ { I R E G }$ may be misspecified.

We start by establishing a general central limit theorem for $\hat { \tau } _ { I R E G }$ below under an assumption that samples are independently drawn from a population, but no linearity assumption. Throughout, we will use the following notation,

$$
\mu_ {(w)} (x) = \mathbb {E} \left[ Y _ {i} (w) \mid X _ {i} = x \right], \quad \sigma_ {(w)} ^ {2} (x) = \operatorname {V a r} \left[ Y _ {i} (w) \mid X _ {i} = x \right], \tag {1.21}
$$

and assume that these quantities are well-defined and finite. The proof of the following result relies on the Huber‚ÄìWhite analysis of linear regression whereby‚Äîregardless of linearity assumptions‚Äîlinear regression consistently recovers the best linear projection coefficients

$$
\left(\alpha_ {(w)} ^ {*}, \beta_ {(w)} ^ {*}\right) = \operatorname {a r g m i n} _ {\alpha , \beta} \left\{\mathbb {E} \left[ (Y _ {i} (w) - \alpha - X _ {i} \cdot \beta) ^ {2} \right] \right\}, \tag {1.22}
$$

which characterize the best available linear-in- $X _ { i }$ predictor under mean-squared error.9 The argument below can also be extended to verify that standard non-parametric tools for statistical inference‚Äîsuch as the bootstrap or the jackknife‚Äîcan be used to build asymptotically valid normal confidence intervals for $\tau$ that are centered at $\hat { \tau } _ { I R E G }$ .

Theorem 1.3. Under the conditions of Theorem 1.2, assume furthermore that $\mathbb { E } \left[ X ^ { \prime } X \right]$ is invertible. Then,

$$
\begin{array}{l} \sqrt {n} \left(\hat {\tau} _ {I R E G} - \tau\right) \Rightarrow \mathcal {N} \left(0, V _ {I R E G}\right), \\ V _ {I R E G} = \operatorname {V a r} \left[ X _ {i} \cdot \left(\beta_ {(1)} ^ {*} - \beta_ {(0)} ^ {*}\right) \right] + \frac {1}{\pi} \mathbb {E} \left[ \left(Y _ {i} (1) - \alpha_ {(1)} ^ {*} - X _ {i} \cdot \beta_ {(1)} ^ {*}\right) ^ {2} \right] \tag {1.23} \\ + \frac {1}{1 - \pi} \mathbb {E} \left[ \left(Y _ {i} (0) - \alpha_ {(0)} ^ {*} - X _ {i} \cdot \beta_ {(0)} ^ {*}\right) ^ {2} \right]. \\ \end{array}
$$

Proof. We again assume, without loss of generality, that $\mathbb { E } \left[ X _ { i } \right] = 0$ . From the Huber‚ÄìWhite analysis of linear regression, we then obtain that10

$$
\sqrt {n _ {w}} \left(\binom {\hat {\alpha} _ {(w)}} {\hat {\beta} _ {(w)}} - \binom {\alpha_ {(w)} ^ {*}} {\beta_ {(w)} ^ {*}}\right) \Rightarrow \mathcal {N} (0, \Sigma_ {(w)}), \tag {1.24}
$$

$$
\Sigma_ {(w)} = \left( \begin{array}{c c} 1 & 0 \\ 0 & A ^ {- 1} \end{array} \right) \mathbb {E} \left[ \left(\left(Y _ {i} (w) - X _ {i} \beta_ {(w)} ^ {*} - \hat {\alpha} _ {(w)} ^ {*}\right) \left( \begin{array}{c} 1 \\ X _ {i} \end{array} \right)\right) ^ {\otimes 2} \right] \left( \begin{array}{c c} 1 & 0 \\ 0 & A ^ {- 1} \end{array} \right).
$$

We now expand out the regression estimator as given in (1.14),

$$
\hat {\tau} _ {I R E G} - \tau = \hat {\alpha} _ {(1)} - \hat {\alpha} _ {(0)} - \tau + \overline {{X}} \cdot \left(\hat {\beta} _ {(1)} - \hat {\beta} _ {(0)}\right).
$$

We start by focusing on the contribution of the first 3 summands. One can immediately verify that the average bias of the optimal linear predictions must be

0, i.e., given $\beta _ { ( w ) } ^ { * }$ , the intercept parameter must be $\alpha _ { ( w ) } ^ { \ast } = \mathbb { E } [ Y _ { i } ( w ) - X _ { i } \cdot \beta _ { ( w ) } ^ { \ast } ]$ . Thus, given $\mathbb { E } \left[ X _ { i } \right] = 0$ , we must have $\alpha _ { ( w ) } ^ { * } = \mathbb { E } \left[ Y _ { i } ( w ) \right]$ , and so

$$
\hat {\alpha} _ {(1)} - \hat {\alpha} _ {(0)} - \tau = \hat {\alpha} _ {(1)} - \alpha_ {(1)} ^ {*} - \left(\hat {\alpha} _ {(0)} - \alpha_ {(0)} ^ {*}\right).
$$

The central limit theorem (1.24) then implies that

$$
\sqrt {n} \left(\hat {\alpha} _ {(1)} - \hat {\alpha} _ {(0)} - \tau\right) \Rightarrow \mathcal {N} \left(0, \frac {M S E _ {(1)} ^ {*}}{\pi} + \frac {M S E _ {(0)} ^ {*}}{1 - \pi}\right), \text {w h e r e} \tag {1.25}
$$

$$
M S E _ {(w)} ^ {*} = \mathbb {E} \left[ \left(Y _ {i} (w) - X _ {i} \beta_ {(w)} ^ {*} - \hat {\alpha} _ {(w)} ^ {*}\right) ^ {2} \right].
$$

measures the mean-squared error of the best linear predictor. Now, moving to the last summand, we note that

$$
\overline {{X}} \cdot \left(\hat {\beta} _ {(1)} - \hat {\beta} _ {(0)}\right) = \overline {{X}} \cdot \left(\beta_ {(1)} ^ {*} - \beta_ {(0)} ^ {*}\right) + \overline {{X}} \cdot \left(\hat {\beta} _ {(1)} - \beta_ {(1)} ^ {*} - \hat {\beta} _ {(0)} + \beta_ {(0)} ^ {*}\right).
$$

Again because $\mathbb { E } \left[ X _ { i } \right] = 0$ , the average $\bar { X }$ of the covariates is near zero with asymptotically normal fluctuations of order $1 / \sqrt { n }$ , and so

$$
\sqrt {n} \bar {X} \cdot \left(\beta_ {(1)} ^ {*} - \beta_ {(0)} ^ {*}\right) \Rightarrow \mathcal {N} \left(0, \operatorname {V a r} \left[ X _ {i} \cdot \left(\beta_ {(1)} ^ {*} - \beta_ {(0)} ^ {*}\right) \right]\right). \tag {1.26}
$$

Furthermore, one can verify that the terms in (1.25) and (1.26) are asymptotically uncorrelated and thus asymptotically independent.11

Finally, because both $\overline { { X } }$ and (thanks to (1.24)) $\hat { \beta } _ { ( 0 ) } - \beta _ { ( 0 ) } ^ { \ast }$ have fluctuations on the order of $1 / \sqrt { n }$ away from 0, their product can only have fluctuations of order $1 / n$ away from 0; we write this compactly as

$$
\overline {{X}} \cdot \left(\hat {\beta} _ {(1)} - \beta_ {(1)} ^ {*} - \hat {\beta} _ {(0)} + \beta_ {(0)} ^ {*}\right) = \mathcal {O} _ {P} \left(1 / n\right).
$$

Thus, by Slutsky‚Äôs lemma, this product term can be asymptotically ignored since the leading-order terms (1.25) and (1.26) are of order $1 / \sqrt { n }$ . Putting all the pieces together recovers (1.23). ‚ñ°

With Theorem 1.3 in hand, we are ready to revisit our comparison between $\hat { \tau } _ { I R E G }$ reduces to ${ \hat { \tau } } _ { D M }$ . Does using a regression adjustment help improve precision, even without linearity assumptions? Here, we show that the answer is yes for balanced RCTs, i.e., with $\pi = 0 . 5$ , and under an assumption that the

unpredictable noise level is constant, $\sigma _ { ( 1 ) } ^ { 2 } ( x ) = \sigma _ { ( 0 ) } ^ { 2 } ( x ) = \sigma ^ { 2 }$ for all $x$ .12 Under these assumptions, and writing Var $[ X _ { i } ] = A$ as before, we can expand out the asymptotic variance from (1.23) as follows:13

$$
\begin{array}{l} V _ {I R E G} = 2 M S E _ {(0)} ^ {*} + 2 M S E _ {(1)} ^ {*} + \left\| \beta_ {(1)} ^ {*} - \beta_ {(0)} ^ {*} \right\| _ {A} ^ {2} \\ = 4 \sigma^ {2} + 2 \operatorname {V a r} \left[ \mu_ {(0)} (X) - X \beta_ {(0)} ^ {*} \right] \\ + 2 \operatorname {V a r} \left[ \mu_ {(1)} (X) - X \beta_ {(1)} ^ {*} \right] + \left\| \beta_ {(1)} ^ {*} - \beta_ {(0)} ^ {*} \right\| _ {A} ^ {2}. \\ \end{array}
$$

Next, because $X \beta _ { ( w ) } ^ { * }$ is the projection of $\mu _ { ( 0 ) } ( X )$ onto the span of $X$ , this further simplifies

$$
\begin{array}{l} \dots = 4 \sigma^ {2} + 2 \left(\operatorname {V a r} \left[ \mu_ {(0)} (X) \right] - \operatorname {V a r} \left[ X \beta_ {(0)} ^ {*} \right]\right) \\ + 2 \left(\operatorname {V a r} \left[ \mu_ {(1)} (X) \right] - \operatorname {V a r} \left[ X \beta_ {(1)} ^ {*} \right]\right) + \left\| \beta_ {(1)} ^ {*} - \beta_ {(0)} ^ {*} \right\| _ {A} ^ {2} \\ = 4 \sigma^ {2} + 2 \left(\operatorname {V a r} \left[ \mu_ {(0)} (X) \right] + \operatorname {V a r} \left[ \mu_ {(1)} (X) \right]\right) \\ + \left\| \beta_ {(1)} ^ {*} - \beta_ {(0)} ^ {*} \right\| _ {A} ^ {2} - 2 \left\| \beta_ {(0)} ^ {*} \right\| _ {A} ^ {2} - 2 \left\| \beta_ {(1)} ^ {*} \right\| _ {A} ^ {2} \\ = 4 \sigma^ {2} + 2 \left(\operatorname {V a r} \left[ \mu_ {(0)} (X) \right] + \operatorname {V a r} \left[ \mu_ {(1)} (X) \right]\right) - \left\| \beta_ {(0)} ^ {*} + \beta_ {(1)} ^ {*} \right\| _ {A} ^ {2} \\ = V _ {D M} - \left\| \beta_ {(0)} ^ {*} + \beta_ {(1)} ^ {*} \right\| _ {A} ^ {2}. \\ \end{array}
$$

In other words, whether or not the true effect function $\mu _ { w } ( x )$ is linear, interacted linear regression always either reduces or matches the asymptotic variance of the difference-in-means estimator. Moreover, the amount of variance reduction scales by the amount by which linear regression in fact chooses to fit the training data. A worst case for the regression adjustment is when $\beta _ { ( 0 ) } ^ { * } = \beta _ { ( 1 ) } ^ { * } = 0$ , i.e., when OLS asymptotically just does nothing; and in this case $\hat { \tau } _ { I R E G }$ ends up being asymptotically equivalent to ${ \hat { \tau } } _ { D M }$ .

The role of regression adjustments in RCTs The individual treatment effect $\Delta _ { i } = Y _ { i } ( 1 ) - Y _ { i } ( 0 )$ is a central object of interest in causal inference. These effects $\Delta _ { i }$ themselves are fundamentally unknowable; however, a large RCT lets us consistently recover the average treatment effect $\tau = \mathbb { E } \left[ \Delta _ { i } \right]$ . In this chapter, we presented and compared two approaches for doing so: The difference-in-means estimator and the interacted regression adjustment. Perhaps surprisingly we found that, when pre-treatment covariates are available, the regression adjustment is asymptotically at least as precise as (and usually

more precise than) the difference-in-means estimator‚Äîand this result holds whether or not the linear model underlying the regression adjustment is well specified.

A key point about our analysis of the regression adjustment is that we defined our target estimand, i.e., the average treatment effect $\tau = \mathbb { E } \left[ \Delta _ { i } \right]$ , before (and without) making any parametric (e.g., linear) modeling assumptions. The average treatment effect was defined in terms of non-parametric counterfactual reasoning. Linear regression was then used as an algorithmic tool to estimate $\tau$ , but linear modeling played no role in framing our original statistical question.

Finally, note that our regression adjustment estimator can effectively be viewed as an average difference in predictions,

$$
\hat {\tau} _ {I R E G} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\underbrace {\left(\hat {\alpha} _ {(1)} + X _ {i} \hat {\beta} _ {(1)}\right)} _ {\hat {\mu} _ {(1)} (X _ {i})} - \underbrace {\left(\hat {\alpha} _ {(0)} + X _ {i} \hat {\beta} _ {(0)}\right)} _ {\hat {\mu} _ {(0)} (X _ {i})}\right), \tag {1.27}
$$

where $\hat { \mu } _ { ( w ) } ( x )$ denotes linear regression predictions at $x$ under treatment $w$ . Could we use other methods to estimate $\hat { \mu } _ { ( w ) } ( x )$ (e.g., deep nets, forests) rather than linear regression? How would this affect asymptotic variance? Exercise 2 in Chapter 16 digs deeper on this.

# Bibliographic notes

The potential outcomes model for causal inference was first advocated by Neyman [1923] and Rubin [1974]; see Imbens and Rubin [2015] for a modern textbook treatment. One simple yet subtle aspect of the modeling framework used here is our use of SUTVA 1.5 which, through notation, rules out many plausible difficulties Imbens and Rubin [2015, Chapter 1.6]. SUTVA precludes any form of cross-unit interference (i.e., $W _ { i }$ cannot affect $Y _ { j }$ for $i \neq j$ ). Furthermore, SUTVA implicitly requires that there is only 1 ‚Äúversion‚Äù of treatment; and this assumption may become problematic if, e.g., we run a multi-site randomized trial where different sites administer treatment in a slightly different way. Thus, whenever invoked in an application, credibility of SUTVA should be carefully assessed.

One distinction that has received considerable attention in the literature is whether or not one is willing to make any stochastic assumptions on the potential outcomes. The setting without stochastic assumptions on the potential outcomes is referred to as the Neyman model for randomization inference or the finite-population model; whereas the setting with stochastic assumptions

is referred to the superpopulation or the IID-sampling model. Here, we stated Theorem 1.1 under the Neyman model, but otherwise worked under a superpopulation model. We will take a closer look at the Neyman model‚Äîand also revisit some of the results from this chapter‚Äîin the context of our discussion of causal inference under cross-unit interference in Chapter 12.

Statistical inference justified under the Neyman model is sometimes considered the highest standard of rigor in analyzing randomized trials because all inferences are justified by randomization alone: The analyst does not need to reason about how study participants were enrolled (and whether they were randomly drawn from a larger population) in order to rigorously apply results proven under this model. The cost of working under the the Neyman model establishing the sampling distribution of even fairly simple estimators requires more intricate statistical analyses; see Li and Ding [2017] for recent results in this setting. In contrast, studying randomized trials under the superpopulation model generally enables simpler analyses via application of standard statistical and econometric tools; and paves the way for more sophisticated semiparametric estimators in observational study settings. A further discussion and comparison of the SATE (1.2) and ATE (1.3) estimands is given in [Imbens, 2004].

Lin [2013] presents a thorough discussion of the role of linear regression adjustments in improving the precision of average treatment effect estimators, and why using full intereactions as in (1.13) is often considered a best practice relative to the simple regression (1.12). When the covariates $X _ { i }$ are generated via one-hot-encoding of a discrete factor (i.e., $X _ { i } \in \{ 0 , 1 \} ^ { K }$ with only one nonzero entry per unit) the interacted regression adjustment estimator is equivalent to (post-)stratification, which is also generally considered a best practice in analyzing data from randomized experiments [Miratrix, Sekhon, and Yu, 2013].

Another feature of Lin [2013] is that he works under the Neyman model for randomization inference, and shows that many of the insights from Theorem 1.3 in fact still holds in this setting. Wager et al. [2016] have a discussion of nonparametric or high-dimensional regression adjustments in randomized trials under superpopulations asymptotics that expands on the results covered here. The study of high-dimensional regression adjustmentin the Neyman model is an ongoing effort, with recent contributions from Bloniarz et al. [2016] and Lei and Ding [2021].

# Chapter 2 Unconfoundedness and the Propensity Score

Randomized controlled trials represent a powerful‚Äîyet somewhat rigid‚Äîclass of settings where we can identify and estimate causal effects. One of the overarching focuses of the literature on statistical causal inference (and also of this book) is on ways in which we relax assumptions made in RCTs while preserving our ability to rigorously estimate causal effects, thus broadening the set of problems where causal inference is possible.

In this chapter, we will consider a first, simple relaxation of the RCT assumptions. We will no longer assume that the treatment $W _ { i }$ is randomized; however, we will assume that we observe pre-treatment covariates $X _ { i }$ such that, after conditioning on $X _ { i }$ , the treatment is as good as randomized. We will then discuss a number of methods for estimating the average treatment effect that exploit this ‚Äúunconfoundedness‚Äù assumption, including ones based on estimating the propensity score (i.e., the conditional probability of receiving treatment). For simplicity, throughout this chapter (and the next ones also) we will work exclusively under the assumption that units are independently sampled from a superpopulation.

Beyond a single randomized controlled trial The simplest way to move beyond one RCT is to consider two RCTs. As a concrete example, suppose that we are interested in giving teenagers cash incentives to discourage them from smoking. A random subset of  5% of teenagers in Palo Alto, CA, and a random subset of ‚àº 20% of teenagers in Geneva, Switzerland are eligible for the study.

<table><tr><td>Palo Alto</td><td>Non-S.</td><td>Smoker</td><td>Geneva</td><td>Non-S.</td><td>Smoker</td></tr><tr><td>Treat.</td><td>152</td><td>5</td><td>Treat.</td><td>581</td><td>350</td></tr><tr><td>Control</td><td>2362</td><td>122</td><td>Control</td><td>2278</td><td>1979</td></tr></table>

Within each city, we have an RCT, and in fact readily see that the treatment helps. However, looking at aggregate data is misleading, and it looks like the treatment hurts; this is an example of what is sometimes called Simpson‚Äôs paradox:

<table><tr><td>Palo Alto + Geneva</td><td>Non-Smoker</td><td>Smoker</td></tr><tr><td>Treatment</td><td>733</td><td>401</td></tr><tr><td>Control</td><td>4640</td><td>2101</td></tr></table>

Once we aggregate the data, this is no longer an RCT because Genevans are both more likely to get treated, and more likely to smoke whether or not they get treated. In order to get a consistent estimate of the ATE, we need to estimate treatment effects in each city separately:

$$
\hat{\tau}_{\mathrm{PA}} = \frac{5}{152 + 5} -\frac{122}{2362 + 122}\approx -1.7\% ,
$$

$$
\hat{\tau}_{\mathrm{GVA}} = \frac{350}{350 + 581} -\frac{1979}{2278 + 1979}\approx -8.9\%
$$

$$
\hat{\tau} = \frac{2641}{2641 + 5188}\hat{\tau}_{\mathrm{PA}} + \frac{5188}{2641 + 5188}\hat{\tau}_{\mathrm{GVA}}\approx -6.5\%.
$$

What are the statistical properties of this estimator? How does this idea generalize to continuous $x$ ?

# 2.1 Stratified estimation

Formalizing the above discussion, suppose that we have covariates $X _ { i }$ that take values in a discrete space $X _ { i } \in { \mathcal { X } }$ , with $| \mathcal { X } | = p < \infty$ . Suppose moreover that the treatment assignment is random conditionally on $X _ { i }$ , (i.e., we have an RCT in each group defined by a level of $x$ ):

$$
\left\{Y _ {i} (0), Y _ {i} (1) \right\} \perp W _ {i} \mid X _ {i} = x, \text {f o r a l l} x \in \mathcal {X}. \tag {2.1}
$$

Define the conditional average treatment effect as

$$
\tau (x) = \mathbb {E} \left[ Y _ {i} (1) - Y _ {i} (0) \mid X _ {i} = x \right]. \tag {2.2}
$$

Then, the above suggests that ought to be able to estimate the ATE $\tau$ by aggregating estimates of the conditional average treatment effect,

$$
\hat {\tau} _ {S T R A T} = \sum_ {x \in \mathcal {X}} \frac {n _ {x}}{n} \hat {\tau} (x), \quad \hat {\tau} (x) = \frac {1}{n _ {x 1}} \sum_ {\{X _ {i} = x, W _ {i} = 1 \}} Y _ {i} - \frac {1}{n _ {x 0}} \sum_ {\{X _ {i} = x, W _ {i} = 0 \}} Y _ {i}, \quad (2. 3)
$$

where $n _ { x } = | \{ i : X _ { i } = x \} |$ and $n _ { x w } = | \{ i : X _ { i } = x , W _ { i } = w \}$ $W _ { i } = w \} |$ . Another way to look as the estimator in (2.3) is that we apply the difference-in-means estimator after stratifying the sample using the covariates $X _ { i }$ ; and for this reason we will refer to it as the stratified estimator.

The following result verifies that the stratified estimator is in fact valid under our assumptions. Remarkably, the asymptotic variance $V _ { S T R A T }$ does not depend on $| { \mathcal { X } } | = p$ , the number of groups, or equivalently the number of ‚Äúparameters‚Äù $\tau ( x )$ estimated on the road to forming (2.3). As we‚Äôll see in the next chapter, this fact plays a key role in enabling efficient non-parametric inference of average treatment effects in observational studies.

Theorem 2.1. Suppose that $\{ X _ { i } , Y _ { i } ( 0 ) , Y _ { i } ( 1 ) , W _ { i } \} \stackrel { i i d } { \sim } P$ for some distribution $P$ where $X _ { i }$ takes values in a finite cardinality set $\mathcal { X }$ and potential outcomes have bounded second moments conditionally on $X _ { i }$ . Suppose furthermore that both (2.1) and SUTVA hold, and that there is non-trivial treatment variation for each $x \in \mathcal { X }$ , i.e., writing $e ( x ) = \mathbb { P } \left[ W _ { i } = 1 \big | X _ { i } = x \right]$ , we have $0 < e ( x ) < 1$ for all $x$ . Then, using notation as in (1.21),

$$
\sqrt {n} \left(\hat {\tau} _ {S T R A T} - \tau\right) \Rightarrow \mathcal {N} \left(0, V _ {S T R A T}\right)
$$

$$
V _ {S T R A T} = \operatorname {V a r} \left[ \tau \left(X _ {i}\right) \right] + \mathbb {E} \left[ \frac {\sigma_ {(1)} ^ {2} \left(X _ {i}\right)}{e \left(X _ {i}\right)} + \frac {\sigma_ {(0)} ^ {2} \left(X _ {i}\right)}{1 - e \left(X _ {i}\right)} \right]. \tag {2.4}
$$

Proof. Write $\lambda ( x ) = \mathbb { P } \left[ X _ { i } = x \right]$ for the prevalence of each level of the covariate $x$ , and interpret $\dot { \lambda } ( x ) = n _ { x } / n$ as an estimator for it. We can then expand out the stratified estimator as

$$
\begin{array}{l} \hat {\tau} _ {S T R A T} = \sum_ {x \in \mathcal {X}} \hat {\lambda} (x) \hat {\tau} (x) = \sum_ {x \in \mathcal {X}} \lambda (x) \tau (x) + \sum_ {x \in \mathcal {X}} \left(\hat {\lambda} (x) - \lambda (x)\right) \tau (x) \\ + \sum_ {x \in \mathcal {X}} \lambda (x) (\hat {\tau} (x) - \tau (x)) + \sum_ {x \in \mathcal {X}} (\hat {\lambda} (x) - \lambda (x)) (\hat {\tau} (x) - \tau (x)). \\ \end{array}
$$

We now study each summand in the expression above. First, note that

$$
\sum_ {x \in \mathcal {X}} \lambda (x) \tau (x) = \mathbb {E} \left[ \tau (X _ {i}) \right] = \tau
$$

is our target estimand. Using simple algebraic manipulations, the second term can be re-expressed as

$$
\sum_ {x \in \mathcal {X}} \left(\hat {\lambda} (x) - \lambda (x)\right) \tau (x) = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\tau (X _ {i}) - \tau\right),
$$

and so the standard central limit theorem for IID averages implies that

$$
\sqrt {n} \left(\sum_ {x \in \mathcal {X}} \left(\hat {\lambda} (x) - \lambda (x)\right) \tau (x)\right) \Rightarrow \mathcal {N} \left(0, \operatorname {V a r} \left[ \tau \left(X _ {i}\right) \right]\right).
$$

Next, our assumptions that $\{ X _ { i } , Y _ { i } ( 0 ) , Y _ { i } ( 1 ) , W _ { i } \} \stackrel { \mathrm { i i d } } { \sim } P$ and that (2.1) hold imply that $W _ { i } \mid X _ { i } = x$ , $Y _ { i } ( 0 )$ , $Y _ { i } ( 1 ) \sim \mathrm { B e r n o u l l i } \left( e ( x ) \right)$ . Thus, by Theorem 1.2,

$$
\sqrt {n _ {x}} \left(\hat {\tau} (x) - \tau (x)\right) \mid n _ {x} \Rightarrow \mathcal {N} \left(0, \frac {\sigma_ {(1)} ^ {2} (x)}{e (x)} + \frac {\sigma_ {(0)} ^ {2} (x)}{1 - e (x)}\right),
$$

and the sampling errors in $\hat { \tau } ( x )$ are all asymptotically independent of each other. Then, because $n _ { x } / n \to _ { p } \lambda ( x )$ , Slutsky‚Äôs lemma yields

$$
\sqrt {n} \sum_ {x \in \mathcal {X}} \lambda (x) (\hat {\tau} (x) - \tau (x)) \Rightarrow \mathcal {N} \left(0, \sum_ {x \in \mathcal {X}} \lambda (x) \left(\frac {\sigma_ {(1)} ^ {2} (x)}{e (x)} + \frac {\sigma_ {(0)} ^ {2} (x)}{1 - e (x)}\right)\right).
$$

Furthermore, because our central limit theorem for $\hat { \tau } ( x )$ holds conditionally on $n _ { x }$ , the above is asymptotically independent of the second summand in our original decomposition for $\scriptstyle { \hat { \tau } } _ { S T R A T }$ , and these two terms together have the limiting distribution claimed in (2.4). Finally, the above also implies that

$$
\left(\hat {\lambda} (x) - \lambda (x)\right) (\hat {\tau} (x) - \tau (x)) = \mathcal {O} _ {P} \left(\frac {1}{n}\right) \mathrm {f o r a l l} x \in \mathcal {X},
$$

and so the fourth summand is asymptotically negligible.

Continuous $X$ and the propensity score Above, we considered a setting where $\mathcal { X }$ is discrete with a finite number levels, and treatment $W _ { i }$ is as good as random conditionally on $X _ { i } ~ = ~ x$ as in (2.1). In this case, we found that we can still accurately estimate the ATE by aggregating group-wise treatment effect estimates, and that the exact number of groups $| { \mathcal { X } } | = p$ does not affect the accuracy of inference. However, if $\mathcal { X }$ is continuous (or the cardinality of $\mathcal { X }$ is very large), this result does not apply directly‚Äîbecause we won‚Äôt be able to get enough samples for each possible value of $x \in \mathcal { X }$ to be able to define $\hat { \tau } ( x )$ as in (2.3).

In order to generalize our analysis beyond the discrete- $X$ case, we‚Äôll need to move beyond literally trying to estimate $\tau ( x )$ for each value of $x$ by simple averaging, and use a more indirect argument instead. To this end, we first need

to generalize the ‚ÄúRCT in each group‚Äù assumption. Formally, we just write the same thing,

$$
\left\{Y _ {i} (0), Y _ {i} (1) \right\} \perp W _ {i} \mid X _ {i}, \tag {2.5}
$$

although now $X _ { i }$ may be an arbitrary random variable, and interpretation of this statement may require more care. Qualitatively, one way to think about (2.5) is that we have measured enough covariates to capture any dependence between $W _ { i }$ and the potential outcomes and so, given $X _ { i }$ , $W _ { i }$ cannot ‚Äúpeek‚Äù at the $\{ Y _ { i } ( 0 ) , Y _ { i } ( 1 ) \}$ . We call this assumption unconfoundedness.

The assumption (2.5) may seem like a difficult assumption to use in practice, since it involves conditioning on a continuous random variable. However, as shown by Rosenbaum and Rubin [1983], this assumption can be made considerably more tractable by considering the propensity score

$$
e (x) = \mathbb {P} \left[ W _ {i} = 1 \mid X _ {i} = x \right]. \tag {2.6}
$$

Statistically, a key property of the propensity score is that it is a balancing score: If (2.5) holds, then in fact

$$
\left\{Y _ {i} (0), Y _ {i} (1) \right\} \perp W _ {i} | e \left(X _ {i}\right), \tag {2.7}
$$

i.e., it actually suffices to control for $e ( X )$ rather than $X$ to remove biases associated with a non-random treatment assignment. We can verify this claim as follows:

$$
\begin{array}{l} \mathbb {P} \left[ W _ {i} = w \mid \{Y _ {i} (0), Y _ {i} (1) \}, e (X _ {i}) \right] \\ = \int_ {\mathcal {X}} \mathbb {P} \left[ W _ {i} = w \mid \left\{Y _ {i} (w) \right\}, X _ {i} = x \right] \mathbb {P} \left[ X _ {i} = x \mid \left\{Y _ {i} (w) \right\}, e (X _ {i}) \right] d x \\ = \int_ {\mathcal {X}} \mathbb {P} \left[ W _ {i} = w \mid X _ {i} = x \right] \mathbb {P} \left[ X _ {i} = x \mid \left\{Y _ {i} (w) \right\}, e \left(X _ {i}\right) \right] d x \quad (\text {u n c o n f .}) \\ = \left\{ \begin{array}{l l} e (X _ {i}) & \text {i f w = 1}, \\ 1 - e (X _ {i}) & \text {e l s e}. \end{array} \right. \\ \end{array}
$$

The implication of (2.7) is that if we can partition our observations into groups with (almost) constant values of the propensity score $e ( x )$ , then we can consistently estimate the average treatment effect via variants of $\hat { \tau } _ { S T R A T }$ .

Propensity stratification One instantiation of this idea is propensity stratification, which proceeds as follows. First obtain an estimate $\hat { e } ( x )$ of the propensity score via non-parametric regression, and choose a number of strata $J$ . Then:

1. Sort the observations according to their propensity scores, such that

$$
\hat {e} \left(X _ {i _ {1}}\right) \leq \hat {e} \left(X _ {i _ {2}}\right) \leq \ldots \leq \hat {e} \left(X _ {i _ {n}}\right). (2. 8)
$$

2. Split the sample into $J$ evenly size strata using the sorted propensity score and, in each stratum $j = 1$ , ..., $J$ , compute the simple differencein-means treatment effect estimator for the stratum:

$$
\hat {\tau} _ {j} = \frac {\sum_ {j = \lfloor (j - 1) n / J \rfloor + 1} ^ {\lfloor j n / J \rfloor} W _ {i} Y _ {i}}{\sum_ {j = \lfloor (j - 1) n / J \rfloor + 1} ^ {\lfloor j n / J \rfloor} W _ {i}} - \frac {\sum_ {j = \lfloor (j - 1) n / J \rfloor + 1} ^ {\lfloor j n / J \rfloor} \left(1 - W _ {i}\right) Y _ {i}}{\sum_ {j = \lfloor (j - 1) n / J \rfloor + 1} ^ {\lfloor j n / J \rfloor} \left(1 - W _ {i}\right)}. \tag {2.9}
$$

3. Estimate the average treatment by applying the idea of (2.3) across strata:

$$
\hat {\tau} _ {P S T R A T} = \frac {1}{J} \sum_ {j = 1} ^ {J} \hat {\tau} _ {j}. \tag {2.10}
$$

The arguments described above immediately imply that, thanks to (2.7), ÀÜœÑP ST RAT is consistent for $\tau$ whenever ${ \hat { e } } ( x )$ is uniformly consistent for $e ( x )$ and the number of strata $J$ grows appropriately with $n$ ; see Exercise 4 in Chapter 16 for more details.

# 2.2 Inverse-propensity weighting

Another, algorithmically simpler way of exploiting unconfoundedness is via inverse-propensity weighting (IPW). As before, we start by estimating ${ \hat { e } } ( x )$ via non-parametric regression; however, we then use the outputs of our propensity model to build a re-weighted difference-in-means-type estimator

$$
\hat {\tau} _ {I P W} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i} Y _ {i}}{\hat {e} (X _ {i})} - \frac {(1 - W _ {i}) Y _ {i}}{1 - \hat {e} (X _ {i})}\right). \tag {2.11}
$$

The intuition behind IPW is that, if some units are very unlikely to get treated, then we should up-weight them on the rare event where they do get treated and down-weight them on the more common event where they don‚Äôt, etc., and that this re-weighting allows us to ‚Äúundo‚Äù sampling bias caused by variation in the propensity score.

The simplest way to analyze it is by comparing it to an oracle that actually knows the propensity score:

$$
\hat {\tau} _ {I P W} ^ {*} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i} Y _ {i}}{e (X _ {i})} - \frac {(1 - W _ {i}) Y _ {i}}{1 - e (X _ {i})}\right). \tag {2.12}
$$

We start by establishing asymptotic properties of the oracle IPW estimator below. Once we‚Äôve established consistency of $\hat { \tau } _ { I P W } ^ { * }$ , it follows as an (almost) immediate corollary that ${ \hat { \tau } } _ { I P W }$ is also consistent provided that $\hat { e } ( x )$ is consistent for $e ( x )$ .

Theorem 2.2. Suppose that $\{ X _ { i } , Y _ { i } ( 0 ) , Y _ { i } ( 1 ) , W _ { i } \} \stackrel { i i d } { \sim } P$ , that both (2.5) and SUTVA hold, and that all moments used in the expression for $V _ { I P W ^ { * } }$ below are finite. Then, the oracle IPW estimator is unbiased, $\mathbb { E } \left[ \hat { \tau } _ { I P W } ^ { * } \right] = \tau$ , and

$$
\sqrt {n} \left(\hat {\tau} _ {I P W} ^ {*} - \tau\right) \Rightarrow \mathcal {N} \left(0, V _ {I P W ^ {*}}\right)
$$

$$
\begin{array}{l} V _ {I P W ^ {*}} = \operatorname {V a r} [ \tau (X _ {i}) ] + \mathbb {E} \left[ \frac {\left(\mu_ {(0)} (X _ {i}) + (1 - e (X _ {i})) \tau (X _ {i})\right) ^ {2}}{e (X _ {i}) (1 - e (X _ {i}))} \right] \tag {2.13} \\ + \mathbb {E} \left[ \frac {\sigma_ {(1)} ^ {2} (X _ {i})}{e (X _ {i})} + \frac {\sigma_ {(0)} ^ {2} (X _ {i})}{1 - e (X _ {i})} \right]. \\ \end{array}
$$

Proof. We start by checking the unbiasedness statement as follows:

$$
\begin{array}{l} \mathbb {E} \left[ \hat {\tau} _ {I P W} ^ {*} \right] = \mathbb {E} \left[ \frac {W _ {i} Y _ {i}}{e (X _ {i})} - \frac {(1 - W _ {i}) Y _ {i}}{1 - e (X _ {i})} \right] \tag {I1D} \\ = \mathbb {E} \left[ \frac {W _ {i} Y _ {i} (1)}{e \left(X _ {i}\right)} - \frac {(1 - W _ {i}) Y _ {i} (0)}{1 - e \left(X _ {i}\right)} \right] \quad (\text {S U T V A}) \\ = \mathbb {E} \left[ \mathbb {E} \left[ \frac {W _ {i} Y _ {i} (1)}{e (X _ {i})} \mid X _ {i} \right] - \mathbb {E} \left[ \frac {(1 - W _ {i}) Y _ {i} (0)}{1 - e (X _ {i})} \mid X _ {i} \right] \right] \\ = \mathbb {E} \left[ \frac {\mathbb {E} \left[ W _ {i} \mid X _ {i} \right] \mathbb {E} \left[ Y _ {i} (1) \mid X _ {i} \right]}{e (X _ {i})} - \frac {\mathbb {E} \left[ 1 - W _ {i} \mid X _ {i} \right] \mathbb {E} \left[ Y _ {i} (0) \mid X _ {i} \right]}{1 - e (X _ {i})} \right] (\mathrm {u n c o n f .}) \\ = \mathbb {E} \left[ Y _ {i} (1) - Y _ {i} (0) \right] = \tau . \\ \end{array}
$$

Next, under our IID sampling assumption, (2.13) follows immediately from the central limit theorem for IID averages with

$$
V _ {I P W ^ {*}} = \mathrm {V a r} \left[ \frac {W _ {i} Y _ {i}}{e (X _ {i})} - \frac {(1 - W _ {i}) Y _ {i}}{1 - e (X _ {i})} \right],
$$

provided this variance is finite. It remains to derive the claimed alternative expression for $V _ { I P W ^ { * } }$ . To this end, building on notation from (1.21), we introduce an auxiliary function

$$
c (x) = \mu_ {(0)} (x) + (1 - e (x)) \tau (x),
$$

and write $\varepsilon _ { i } ( w ) = Y _ { i } ( w ) - \mu _ { ( w ) } ( X _ { i } )$ . Given these preliminaries, we expand out

$$
\begin{array}{l} \frac {W _ {i} Y _ {i}}{e (X _ {i})} - \frac {(1 - W _ {i}) Y _ {i}}{1 - e (X _ {i})} \\ = \frac {W _ {i} \left(\mu_ {(1)} \left(X _ {i}\right) + \varepsilon_ {i} (1)\right)}{e \left(X _ {i}\right)} - \frac {\left(1 - W _ {i}\right) \left(\mu_ {(0)} \left(X _ {i}\right) + \varepsilon_ {i} (0)\right)}{1 - e \left(X _ {i}\right)} \\ = \tau (X _ {i}) + \left(\frac {W _ {i}}{e (X _ {i})} - \frac {1 - W _ {i}}{1 - e (X _ {i})}\right) c (X _ {i}) + \frac {W _ {i} \varepsilon_ {i} (1)}{e (X _ {i})} - \frac {(1 - W _ {i}) \varepsilon_ {i} (0)}{1 - e (X _ {i})}. \\ \end{array}
$$

Furthermore, $\mathbb { E } \ \big \lfloor W _ { i } / e ( X _ { i } ) - ( 1 - W _ { i } ) / ( 1 - e ( X _ { i } ) ) \big \rfloor X _ { i } \big \rfloor \ = \ 0$ by definition of the propensity score, and $\mathbb { E } \left[ \varepsilon _ { i } ( w ) \vert X _ { i } , W _ { i } \right] = 0$ by unconfoundedness, so

$$
\begin{array}{l} \mathrm {V a r} \left[ \frac {W _ {i} Y _ {i}}{e (X _ {i})} - \frac {(1 - W _ {i}) Y _ {i}}{1 - e (X _ {i})} \right] = \mathrm {V a r} \left[ \tau (X _ {i}) \right] \\ + \mathbb {E} \left[ \left(\left(\frac {W _ {i}}{e (X _ {i})} - \frac {1 - W _ {i}}{1 - e (X _ {i})}\right) c (X _ {i})\right) ^ {2} \right] + \mathbb {E} \left[ \left(\frac {W _ {i} \varepsilon_ {i} (1)}{e (X _ {i})} - \frac {(1 - W _ {i}) \varepsilon_ {i} (0)}{1 - e (X _ {i})}\right) ^ {2} \right]. \\ \end{array}
$$

The claimed expression for $V _ { I P W ^ { * } }$ follows by simplifying the one above.

![](images/1563f0b13dad25795243378924a476a894b67640f0d64ed885882cf9cec031eb.jpg)

One noteworthy assumption made seemingly in passing above is that all moments used in (2.13) are well-defined and finite. This is, however, a highly non-trivial assumption. If the potential outcomes are uniformly bounded, then this condition is essentially equivalent to assuming that

$$
\mathbb {E} \left[ 1 / \left(e \left(X _ {i}\right) \left(1 - e \left(X _ {i}\right)\right)\right) \right] <   \infty . \tag {2.14}
$$

Meanwhile if we simply assume that the potential outcomes have finite second moments then we need to assume something stronger, e.g., there exists an $\eta > 0$ for which

$$
\eta \leq e (x) \leq 1 - \eta \text {f o r a l l} x \in \mathcal {X}. \tag {2.15}
$$

These assumptions are generally known as overlap assumptions, and codify the requirement that there must be non-trivial randomness in treatment assignment conditionally on $x$ . We refer to (2.14) as weak overlap, and (2.15) as strong overlap. Qualitatively an overlap-type assumption must in general be made for non-parametric treatment effect estimation to be possible: If treatment assignment $W _ { i }$ is perfectly predictable from $X _ { i }$ , then there is no actual randomness in treatment assignment, and so treatment effect estimation justified by treatment randomization cannot be possible.

How accurate is inverse-propensity weighting? We established above that IPW is unbiased and asymptotically normal when implemented with the true propensity scores, and consistent with estimated propensity scores. This is of course a nice result to have given the simple functional form of the IPW estimator. But do these results imply that IPW is any good?

To get a benchmark for our results about IPW, it is helpful to re-visit the setting of the beginning of this lecture where $\mathcal { X }$ is discrete, in which case we can use the result in Theorem 2.1 for $\scriptstyle { \hat { \tau } } _ { S T R A T }$ as a point of comparison. When propensity scores are known, both $\hat { \tau } _ { I P W } ^ { * }$ and $\hat { \tau } _ { S T R A T }$ are asymptotically normal, and from (2.4) and (2.13) we see that

$$
V _ {I P W ^ {*}} = V _ {S T R A T} + \mathbb {E} \left[ \frac {\left(\mu_ {(0)} (X _ {i}) + (1 - e (X _ {i})) \tau (X _ {i})\right) ^ {2}}{e (X _ {i}) (1 - e (X _ {i}))} \right]. \tag {2.16}
$$

Thus, unless $\mu _ { ( 0 ) } ( X _ { i } ) + ( 1 - e ( X _ { i } ) ) \tau ( X _ { i } )$ is zero almost surely, $\hat { \tau } _ { I P W } ^ { * }$ has a strictly worse asymptotic variance than $\scriptstyle { \hat { \tau } } _ { S T R A T }$ . Meanwhile, when propensity scores are not known, we here only proved a consistency result for ${ \hat { \tau } } _ { I P W }$ (no central limit theorem), and so we cannot even make a proper comparison. Thus, at first glance, a comparison of Theorems 2.1 and 2.2 makes the behavior of IPW seem somewhat disappointing.

However, on closer look, the picture gets more complicated: It turns out that $\scriptstyle { \hat { \tau } } _ { S T R A T }$ can actually be understood as an implementation of the IPW estimator with a specific choice of estimated propensity score ${ \hat { e } } ( x )$ . In the setting of (2.3) where $\hat { \tau } _ { S T R A T }$ is well defined, we have:

$$
\hat {\tau} _ {S T R A T} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i} Y _ {i}}{\hat {e} (X _ {i})} - \frac {(1 - W _ {i}) Y _ {i}}{1 - \hat {e} (X _ {i})}\right), \quad \hat {e} (x) = \frac {n _ {x 1}}{n _ {x}}. \qquad (2. 1 7)
$$

Thus, when $\mathcal { X }$ is discrete, it turns out that an instance of a feasible IPW estimator, namely $\hat { \tau } _ { S T R A T }$ , is actually more precise than the ‚Äúoracle‚Äù IPW estimator (see also Exercise 1 in Chapter 16).14 Understanding and resolving this seeming paradox lies will be at the heart of understanding how to design accurate estimators of the average treatment effect under unconfoundedness‚Äî including with continuous covariates.

Randomized and observational studies One nuance we glossed over is that there are two conceptually distinct ways that one could end up with potential outcomes satisfying (2.5). The first option is that the data was generated by an experiment with variable treatment propensities: Nature generated $\{ X _ { i } , Y _ { i } ( 0 ) , Y _ { i } ( 1 ) \} \sim P$ , and then an experimenter randomly assigned treatments $W _ { i } \sim \mathrm { B e r n o u l l i } ( e ( X _ { i } ) )$ for some function $e ( \cdot )$ of the covariates. Under this setting, the experimenter knows that (2.5) must hold, because they themselves generated treatment in a way that satisfies the assumption. Essentially, the experimenter is running the same Bernoulli trial as considered in (1.8), except with randomization probabilities that vary with the $X _ { i }$ . Although covariate-dependent randomization probabilities require statistical accommodation, such experiments are conceptually akin to the ones discussed in Chapter 1‚Äîand provide comparably strong, gold-standard causal evidence.

Example 2. Arceneaux, Gerber, and Green [2006] run a randomized study to measure the effectiveness of voter mobilization phone calls in getting people to vote in midterm elections. The study is run in two states, Michigan and Iowa, and randomization is stratified by both state and by competitiveness of the congressional district, with per-stratum randomization probabilities varying from 1% to 15%. This is a randomized controlled trial; however, properly accounting for variation in the randomization probabilities (e.g., via propensity stratification) is required for a valid analysis, and simply taking a global difference in means would be prone to Simpson‚Äôs paradox.

The second option is that there was no experiment: Nature generated {Xi, Yi(0), $Y _ { i } ( 1 )$ , $W _ { i } \} \sim P$ , and we simply posit that (2.5) holds. This marks a much bigger departure from the setting of Chapter 1. There is no analyst who ran an experiment; rather, we posit that data is generated as though someone had run the experiment described in the previous chapter. Such settings are referred to as natural experiments or observational study designs. Because no experiment was actually run, the assumption (2.5) can always be challenged in observational studies‚Äîand as such the resulting causal evidence is sometimes considered more tentative than evidence obtained via randomized experiments.

Example 3. LaLonde [1986] considers evaluating the benefits from a jobs training program by comparing post-intervention earnings for people enrolled in a pilot program to members of the general public who were not enrolled in the program. This is not a randomized study design, and members of the general public differ from those in the pilot program along a number of preintervention metrics. The initial assessment of LaLonde [1986] regarding the possibility of getting credible causal estimates out of such observational data

was pessimistic. However, in later work, Dehejia and Wahba [1999] showed that approaches that start by modeling the propensity score (i.e., here, the probability of joining the pilot program given pre-intervention characteristics) showed more promising behavior,15 and were often able to match experimental benchmarks.

Another major practical difference between randomized trials with covariatedependent randomization versus observational studies is that, in the former case, the treatment propensities $e ( X _ { i } )$ are usually known (because they were chosen by the experimenter), and so methods such as oracle IPW with guarantees as in Theorem 2.2 are available. In contrast, in the observational study setting, treatment propensities need to be estimated, and thus robustness of methods to errors in the propensity scores is important‚Äîparticularly in settings as below where propensity scores are hard to estimate accurately. As of now, we have not yet seen estimators that, in a setting with continuous $X _ { i }$ , can take in estimated propensity scores and output asymptotically normal average treatment effect estimates with $1 / \sqrt { n }$ -scale errors. In the next chapter, we will present an improvement to IPW that can achieve asymptotic normality even with estimated propensity scores.

Example 4. Ross et al. [2024] use electronic health record data from the Veterans‚Äô Administration to estimate the benefits of psychiatric hospitalization on suicide prevention among patients with a recent suicide attempt or suicide ideation. There is no randomization, and hospitalized versus non-hospitalized patients differ on pre-treatment characteristics. The authors argue that after controlling for rich medical history available through the electronic health records, it is plausible for unconfoundedness to hold, and proceed to use propensity score methods. However, given that the pre-treatment is high-dimensional with complex structure, it is necessary to use a machine learning approach to get reasonable propensity score estimates‚Äîand any down-stream used of these propensity scores should be robust to likely estimation errors in this step.

15One question that has received substantial attention in subsequent discussions of the work of LaLonde [1986] is how we should properly ‚Äúcontrol for‚Äù pre-intervention covariates in an observational study setting. In informal econometric practice, when an analyst says they have controlled for a set of covariates, they mean that they‚Äôve run a regression where they‚Äôve added the covariates as predictors; e.g., in our setting, they might have sought to estimate a treatment effect via the $\hat { \tau }$ coefficient from the regression $Y _ { i } \sim \alpha + W _ { i } \tau + X _ { i } \cdot \beta$ . This type of regression, however, is not justified by the unconfoundedness assumption (2.5) and, unlike IPW or other propensity-score methods, is not generally consistent for the average treatment effect under unconfoundedness. The unconfoundedness assumption (2.5) is nonparametric; and thus using it requires adjusting for $X _ { i }$ non-parametrically.

# Bibliographic notes

The central role of the propensity score in estimating causal effects was first emphasized by Rosenbaum and Rubin [1983], while associated methods for estimation such as propensity stratification are discussed in Rosenbaum and Rubin [1984]. Hirano, Imbens, and Ridder [2003] provide a detailed discussion of the asymptotics of IPW-style estimators that expands on the result given in Theorem 2.1. In particular they present conditions with continuous $X _ { i }$ under which IPW with non-parametrically estimated propensity scores can outperform oracle IPW.

Another popular way of leveraging the propensity score in practice is propensity matching, i.e., estimating treatment effects by comparing pairs of units with similar values of $\hat { e } ( X _ { i } )$ . For a some recent discussions of matching in causal inference, see Abadie and Imbens [2006, 2016], Diamond and Sekhon [2013], Zubizarreta [2012], and references therein.

# Chapter 3 Doubly Robust Methods

Inverse-propensity weighting (IPW) is a simple and transparent approach to average treatment effect estimation under unconfoundedness. However, as seen in the previous chapter, the large-sample properties of IPW are not particularly good in general, and the way estimation error in the propensity scores affects accuracy of IPW is complex. Our goal here is to move beyond the limitations of IPW and to discuss doubly robust methods, which provide a general recipe for building robust and asymptotically optimal treatment effect estimators under unconfoundedness, and enable us to rigorously and flexibly handle estimation error in the propensity score.16

Throughout this chapter, we will seek to estimate the average treatment effect $\tau = \mathbb { E } \left[ Y _ { i } ( 1 ) - Y _ { i } ( 0 ) \right]$ under the following statistical setting:

Basic setting: SUTVA, unconfoundedness and strong overlap There is a distribution $P$ that generates a stream of tuples {Xi, Yi(0), Yi(1), $W _ { i } \} \stackrel { \mathrm { i i d } } { \sim } P$ taking values in $\mathcal { X } \times \mathbb { R } \times \mathbb { R } \times \{ 0 , 1 \}$ . We get to observe $( X _ { i } , Y _ { i } , W _ { i } )$ where $Y _ { i } = Y _ { i } ( W _ { i } )$ (SUTVA). We are not necessarily in a randomized controlled trial; however, we have unconfoundedness, i.e., treatment assignment is as good as random conditionally on the features $X _ { i }$ :

$$
\left\{Y _ {i} (0), Y _ {i} (1) \right\} \perp W _ {i} \mid X _ {i}, \tag {3.1}
$$

Potential outcomes have bounded second moments, $\mathbb { E } \left[ Y _ { i } ^ { 2 } ( w ) \right] < \infty$ . Strong overlap holds, i.e., for some $\eta > 0$ ,

$$
\eta \leq e (x) \leq 1 - \eta \quad \text {f o r a l l} \quad x \in \mathcal {X}. \tag {3.2}
$$

We write $e ( x ) = \mathbb { P } \left\lfloor W _ { i } = 1 \right\rfloor X _ { i } = x \rfloor$ for the propensity score, and also use notation $\mu _ { ( w ) } ( x ) = \operatorname { \mathbb { E } } \left[ Y _ { i } ( w ) \big | X _ { i } = x \right]$ and $\sigma _ { ( w ) } ^ { 2 } ( x ) = \mathrm { V a r } \left| Y _ { i } ( w ) \right| X _ { i } = x \rfloor$ .

Two characterizations of the ATE In the previous chapter, we saw that the ATE can be characterized via IPW:

$$
\tau = \mathbb {E} \left[ \hat {\tau} _ {I P W} ^ {*} \right], \quad \hat {\tau} _ {I P W} ^ {*} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i} Y _ {i}}{e (X _ {i})} - \frac {(1 - W _ {i}) Y _ {i}}{1 - e (X _ {i})}\right). \tag {3.3}
$$

However, $\tau$ can also be characterized in terms of the conditional response surfaces $\mu _ { ( w ) } ( x )$ : Under unconfoundedness (3.1),

$$
\begin{array}{l} \tau (x) := \mathbb {E} \left[ Y _ {i} (1) - Y _ {i} (0) \mid X _ {i} = x \right] \\ = \mathbb {E} \left[ Y _ {i} (1) \mid X _ {i} = x \right] - \mathbb {E} \left[ Y _ {i} (0) \mid X _ {i} = x \right] \\ = \mathbb {E} \left[ Y _ {i} (1) \mid X _ {i} = x, W _ {i} = 1 \right] - \mathbb {E} \left[ Y _ {i} (0) \mid X _ {i} = x, W _ {i} = 0 \right] \quad (\mathrm {u n c o n f}) \\ = \mathbb {E} \left[ Y _ {i} \mid X _ {i} = x, W _ {i} = 1 \right] - \mathbb {E} \left[ Y _ {i} \mid X _ {i} = x, W _ {i} = 0 \right] \quad (\text {S U T V A}) \\ = \mu_ {(1)} (x) - \mu_ {(0)} (x), \\ \end{array}
$$

and so $\tau = \mathbb { E } \left\lfloor \mu _ { ( 1 ) } ( X _ { i } ) - \mu _ { ( 0 ) } ( X _ { i } ) \right\rfloor$ . Thus there also exists a simple and consistent (but not necessarily optimal) non-parametric regression estimator for $\tau$ : First estimate $\mu _ { ( 0 ) } ( x )$ and $\mu _ { ( 1 ) } ( x )$ non-parametrically, and then set $\begin{array} { r } { \hat { \tau } _ { R E G } = n ^ { - 1 } \sum _ { i = 1 } ^ { n } \bigl ( \hat { \mu } _ { ( 1 ) } ( X _ { i } ) - \hat { \mu } _ { ( 0 ) } ( X _ { i } ) \bigr ) } \end{array}$ .

Augmented IPW Given that the average treatment effect can be estimated in two different ways, i.e., by first non-parametrically estimating $e ( x )$ or by first estimating $\mu _ { ( 0 ) } ( x )$ and $\mu _ { ( 1 ) } ( x )$ , it is natural to ask whether it is possible to combine both strategies. This turns out to be a very good idea, and yields the augmented IPW (AIPW) estimator of Robins, Rotnitzky, and Zhao [1994]:

$$
\begin{array}{l} \hat {\tau} _ {A I P W} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\hat {\mu} _ {(1)} \left(X _ {i}\right) - \hat {\mu} _ {(0)} \left(X _ {i}\right) \right. \tag {3.4} \\ \left. + W _ {i} \frac {Y _ {i} - \hat {\mu} _ {(1)} (X _ {i})}{\hat {e} (X _ {i})} - (1 - W _ {i}) \frac {Y _ {i} - \hat {\mu} _ {(0)} (X _ {i})}{1 - \hat {e} (X _ {i})}\right). \\ \end{array}
$$

Qualitatively, AIPW can be seen as first making a best effort attempt at $\tau$ by estimating $\mu _ { ( 0 ) } ( x )$ and $\mu _ { ( 1 ) } ( x )$ ; then, it deals with any biases of the $\hat { \mu } _ { ( w ) } ( x )$ by applying IPW to the regression residuals. Statistically, it turns out that AIPW not only inherits robustness properties from both the regression and IPW estimators‚Äîit improves on both by (in a sense made rigorous below) using IPW to mitigate errors in the regression estimator and vice-versa.

Weak double robustness A first, simple-to-understand property of AIPW is the following ‚Äúweak‚Äù double robustness property:17 AIPW is consistent if

either the $\hat { \mu } _ { ( w ) } ( x )$ are consistent or $\hat { e } ( x )$ is consistent. To see this, first consider the case where $\hat { \mu } _ { ( w ) } ( x )$ is consistent, i.e., $\hat { \mu } _ { ( w ) } ( x ) \approx \mu _ { ( w ) } ( x )$ . Then,

$$
\begin{array}{l} \hat{\tau}_{AIPW} = \underbrace{\frac{1}{n}\sum_{i = 1}^{n}\left(\hat{\mu}_{(1)}(X_{i}) - \hat{\mu}_{(0)}(X_{i})\right)}_{\text{the regression estimator}} \\ + \underbrace {\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i}}{\hat {e} (X _ {i})} \left(Y _ {i} - \hat {\mu} _ {(1)} (X _ {i})\right) - \frac {1 - W _ {i}}{1 - \hat {e} (X _ {i})} \left(Y _ {i} - \hat {\mu} _ {(0)} (X _ {i})\right)\right)} _ {\approx \text {m e a n - z e r o n o i s e}}, \\ \end{array}
$$

because $\mathbb { E } \left[ Y _ { i } - \hat { \mu } _ { ( W _ { i } ) } ( X _ { i } ) \big | X _ { i } , W _ { i } \right] \approx 0$ under unconfoundedness. Thus even if we use inconsistent propensity score weights $1 / \hat { e } ( X _ { i } )$ and $1 / ( 1 - \hat { e } ( X _ { i } ) )$ , they are multiplied by roughly mean-zero error terms and so asymptotically they do not bias the estimator, and ${ \hat { \tau } } _ { A I P W }$ remains consistent.

Conversely, now suppose that ${ \hat { e } } ( x )$ is consistent, i.e., $\hat { e } ( x ) \approx e ( x )$ . Then,

$$
\begin{array}{l} \hat{\tau}_{AIPW} = \underbrace{\frac{1}{n}\sum_{i = 1}^{n}\left(\frac{W_{i}Y_{i}}{\hat{e}(X_{i})} - \frac{(1 - W_{i})Y_{i}}{1 - \hat{e}(X_{i})}\right)}_{\text{the IPW estimator}} \\ + \underbrace {\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\hat {\mu} _ {(1)} (X _ {i}) \left(1 - \frac {W _ {i}}{\hat {e} (X _ {i})}\right) - \hat {\mu} _ {(0)} (X _ {i}) \left(1 - \frac {1 - W _ {i}}{1 - \hat {e} (X _ {i})}\right)\right)} _ {\approx \text {m e a n - z e r o n o i s e}}, \\ \end{array}
$$

because $\mathbb { E } \left[ 1 - W _ { i } / \hat { e } ( X _ { i } ) \vert X _ { i } \right] \approx 0$ . Thus, even if we use inconsistent regression adjustments $\hat { \mu } _ { ( w ) } ( X _ { i } )$ , they will be multiplied by roughly mean-zero noise terms that asymptotically cancel their contribution. Thus ${ \hat { \tau } } _ { A I P W }$ inherits the consistency of ${ \hat { \tau } } _ { I P W }$ under unconfoundedness.

That being said, although the (weak) double robustness of AIPW is a nice property to have, its importance should not be overstated. Weak double robustness only guarantees consistency of ${ \hat { \tau } } _ { A I P W }$ , whereas in most treatment effect estimation applications we also care about rates of convergence and confidence intervals. Furthermore, one could also argue that, in a modern setting, one should expect practitioners to use appropriate non-parametric estimators for both $\mu _ { ( w ) } ( x )$ and $e ( x )$ that are consistent for each. In this case both $\hat { \tau } _ { R E G }$ and ${ \hat { \tau } } _ { I P W }$ would already be consistent on their own, and so the above weak double robustness statement (i.e., consistency of ${ \hat { \tau } } _ { A I P W }$ ) doesn‚Äôt add anything.

Strong double robustness There is also a much more interesting and useful class of ‚Äústrong‚Äù double robustness results for AIPW that quantify the weaker

consistency statement given above. At a high level, strong double robustness is a claim that results of the following type exist: If we use estimators $\hat { \mu } _ { ( w ) } ( x )$ and ${ \hat { e } } ( x )$ that are both consistent with root-mean squared error (RMSE) decaying faster than $n ^ { - \alpha \mu }$ and $n ^ { - \alpha _ { e } }$ respectively, and if furthermore $\alpha _ { \mu } + \alpha _ { e } \ge 1 / 2$ , then

$$
\sqrt {n} \left(\hat {\tau} _ {A I P W} - \tau\right) \Rightarrow \mathcal {N} \left(0, V _ {A I P W}\right),
$$

$$
V _ {A I P W} = \operatorname {V a r} [ \tau (X _ {i}) ] + \mathbb {E} \left[ \frac {\sigma_ {(0)} ^ {2} (X _ {i})}{1 - e (X _ {i})} \right] + \mathbb {E} \left[ \frac {\sigma_ {(1)} ^ {2} (X _ {i})}{e (X _ {i})} \right]. \tag {3.5}
$$

The reason this meta-result holds is that, in general, if the RMSE of $\hat { \mu } _ { ( w ) } ( x )$ decays faster than $n ^ { - \alpha _ { \mu } }$ and the RMSE of $\hat { e } ( x )$ decays faster than $n ^ { - \alpha _ { e } }$ , then the bias of AIPW decays faster than $n ^ { - ( \alpha _ { \mu } + \alpha _ { e } ) }$ ; and, in particular, if $\alpha _ { \mu } + \alpha _ { e } \ge 1 / 2$ then the bias is lower-order on the $1 / \sqrt { n }$ -scale. What‚Äôs remarkable about this result is that, under the same conditions, the bias of the regression estimator would in general only be bounded to order $n ^ { - \alpha _ { \mu } }$ and that of IPW to order $n ^ { - \alpha _ { e } }$ ; and so the AIPW construction succeeds in making bias substantially smaller than what either the regression or IPW estimators could achieve on their own.18

The statement given above is not a theorem‚Äîrather it‚Äôs a meta-result, and a blueprint for many types of results that hold under further technical assumptions. Below, we will discuss one specific way of constructing AIPW estimators, coined as double machine learning by Chernozhukov et al. [2018], and establish conditions under which it satisfies (3.5). Note that double machine learning is not the only way to get results of this type; and in fact results that are stronger than (3.5) can be obtained in some specialized settings. Thus, our presentation below should be seen as a first step‚Äîand not the end point‚Äîin understanding and leveraging strong double robustness of AIPW.

# 3.1 Double machine learning

Our study of strong double robustness for AIPW starts by considering the behavior of an ‚Äúoracle‚Äù AIPW estimator that is constructed in terms of true (rather than estimated) values of the conditional regression surfaces and the

propensity score:

$$
\hat {\tau} _ {A I P W} ^ {*} = \frac {1}{n} \sum_ {i = 1} ^ {n} \Gamma_ {i} \tag {3.6}
$$

$$
\Gamma_ {i} = \mu_ {(1)} (X _ {i}) - \mu_ {(0)} (X _ {i}) + W _ {i} \frac {Y _ {i} - \mu_ {(1)} (X _ {i})}{e (X _ {i})} - (1 - W _ {i}) \frac {Y _ {i} - \mu_ {(0)} (X _ {i})}{1 - e (X _ {i})}.
$$

Proposition 3.1. Under the basic setting with SUTVA, unconfoundedness and strong overlap given at the beginning of this chapter, the oracle AIPW estimator has the limit distribution given in (3.5), i.e.,

$$
\sqrt {n} \left(\hat {\tau} _ {A I P W} ^ {*} - \tau\right) \Rightarrow \mathcal {N} (0, V _ {A I P W}). \tag {3.7}
$$

Proof. The fact that the oracle AIPW estimator is unbiased follows from the discussions used to establish weak double robustness of AIPW. Furthermore, the oracle estimator is an average of IID terms, so the standard central limit theorem immediately implies that $\sqrt { n } \left( \widehat { \tau } _ { A I P W } ^ { * } - \tau \right) \Rightarrow \mathcal { N } \left( 0 , \mathrm { V a r } \left[ \Gamma _ { i } \right] \right)$ . Finally, under unconfoundedness, we can check that

$$
\begin{array}{l} \operatorname {V a r} \left[ \Gamma_ {i} \right] = \operatorname {V a r} \left[ \mu_ {(1)} \left(X _ {i}\right) - \mu_ {(0)} \left(X _ {i}\right) \right] + \mathbb {E} \left[ \left(W _ {i} \frac {Y _ {i} - \mu_ {(1)} \left(X _ {i}\right)}{e \left(X _ {i}\right)}\right) ^ {2} \right] \tag {3.8} \\ + \mathbb {E} \left[ \left((1 - W _ {i}) \frac {Y _ {i} - \mu_ {(0)} (X _ {i})}{1 - e (X _ {i})}\right) ^ {2} \right], \\ \end{array}
$$

which matches the expression for $V _ { A I P W }$ given in (3.5). Notice in particular that, by the overlap and bounded-second-moment assumptions in our basic setting, all terms in (3.8) are finite. ‚ñ°

Given this result, establishing (3.5) reduces to showing that, provided $\hat { \mu } _ { ( w ) } ( \cdot )$ and $\hat { e } ( \cdot )$ converge fast enough,

$$
\sqrt {n} \left(\hat {\tau} _ {A I P W} - \hat {\tau} _ {A I P W} ^ {*}\right)\rightarrow_ {p} 0, \tag {3.9}
$$

i.e., the feasible AIPW estimator is asymptotically equivalent to the oracle. The fact that proving results of the type (3.9) is possible under reasonable assumptions is not to be taken for granted, and is a consequence of AIPW having a strong double robustness property. Other estimators we‚Äôve discussed, such as the IPW and regression adjustment estimators, do not in general satisfy this type of oracle equivalence property.

Cross-fitting In order to establish the oracle equivalence result (3.9), it is helpful to consider the following minor algorithmic modification of AIPW using a technique called cross-fitting. At a high level, cross-fitting uses cross-fold estimation to avoid bias due to overfitting; the motivation behind doing so is closely related to the reason why we often use cross-validation when estimating the predictive accuracy of an estimator.

Cross-fitting first splits the data (at random) into two halves $\mathcal { I } _ { 1 }$ and $\mathcal { I } _ { 2 }$ , and then uses an estimator19

$$
\begin{array}{l} \hat {\tau} _ {A I P W} = \frac {| \mathcal {I} _ {1} |}{n} \hat {\tau} ^ {\mathcal {I} _ {1}} + \frac {| \mathcal {I} _ {2} |}{n} \hat {\tau} ^ {\mathcal {I} _ {2}}, \quad \hat {\tau} ^ {\mathcal {I} _ {1}} = \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \hat {\mu} _ {(0)} ^ {\mathcal {I} _ {2}} (X _ {i}) \right. \tag {3.10} \\ \left. + W _ {i} \frac {Y _ {i} - \hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i})}{\hat {e} ^ {\mathcal {I} _ {2}} (X _ {i})} - (1 - W _ {i}) \frac {Y _ {i} - \hat {\mu} _ {(0)} ^ {\mathcal {I} _ {2}} (X _ {i})}{1 - \hat {e} ^ {\mathcal {I} _ {2}} (X _ {i})}\right), \\ \end{array}
$$

where the $\hat { \mu } _ { ( w ) } ^ { \perp _ { 2 } } ( \cdot )$ and $\hat { e } ^ { \mathcal { L } _ { 2 } } ( \cdot )$ are estimates of $\mu _ { ( w ) } ( \cdot )$ and $e ( \cdot )$ obtained using only the half-sample $\mathcal { L } _ { 2 }$ , and $\hat { \tau } ^ { \mathcal { I } _ { 2 } }$ is defined analogously (with the roles of $\mathcal { L } _ { 1 }$ and $\mathcal { I } _ { 2 }$ swapped). In other words, $\hat { \tau } ^ { \mathcal { L } _ { 1 } }$ is a treatment effect estimator on $\mathcal { I } _ { 1 }$ that uses $\mathcal { I } _ { 2 }$ to estimate its non-parametric components, and vice-versa.

What cross-fitting buys us is that, e.g., if $i \in \mathcal { I } _ { 1 }$ and $W _ { i } = 0$ , then $Y _ { i } -$ $\hat { \mu } _ { ( 0 ) } ^ { \mathcal { L } _ { 2 } } ( X _ { i } )$ is an ‚Äúhonest‚Äù regression residual that cannot be artificially shrunk via overfitting. As seen below, by creating such honest residuals, cross-fitting enables us to establish results of the type (3.9) without needing to make detailed assumptions about the algorithms used to estimate $\hat { \mu } _ { ( w ) } ( x )$ and ${ \hat { e } } ( x )$ .

Theorem 3.2. Given our basic setting with SUTVA, unconfoundedness and strong overlap, suppose that we construct ${ \hat { \tau } } _ { A I P W }$ using cross-fitting with estimators satisfying, for $w \in \{ 0 , 1 \}$ and also with the roles of $\mathcal { I } _ { 1 }$ and $\mathcal { I } _ { 2 }$ swapped,

$$
n ^ {2 \alpha_ {\mu}} \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\hat {\mu} _ {(w)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \mu_ {(w)} (X _ {i})\right) ^ {2} \rightarrow_ {p} 0, \tag {3.11}
$$

$$
n ^ {2 \alpha_ {e}} \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\frac {1}{\hat {e} ^ {\mathcal {I} _ {2}} (X _ {i})} - \frac {1}{e (X _ {i})}\right) ^ {2} \rightarrow_ {p} 0,
$$

for some constants with $\alpha _ { \mu }$ , $\alpha _ { e } \geq 0$ and $\alpha _ { \mu } + \alpha _ { e } \ge 1 / 2$ . Then (3.9) and thus also (3.5) hold.

Proof. Note that, because $\hat { \tau } _ { A I P W } ^ { * }$ doesn‚Äôt rely on estimated quantities and so is unaffected by cross-fitting, we can write the oracle AIPW estimator as

$$
\hat {\tau} _ {A I P W} ^ {*} = \frac {| \mathcal {I} _ {1} |}{n} \hat {\tau} ^ {\mathcal {I} _ {1}, *} + \frac {| \mathcal {I} _ {2} |}{n} \hat {\tau} ^ {\mathcal {I} _ {2}, *}
$$

analogously to (3.10). Moreover, we can decompose $\hat { \tau } ^ { \mathcal { L } _ { 1 } }$ itself as

$$
\hat {\tau} ^ {\mathcal {I} _ {1}} = \hat {m} _ {(1)} ^ {\mathcal {I} _ {1}} - \hat {m} _ {(0)} ^ {\mathcal {I} _ {1}},
$$

$$
\hat {m} _ {(1)} ^ {\mathcal {I} _ {1}} = \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) + W _ {i} \frac {Y _ {i} - \hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i})}{\hat {e} ^ {\mathcal {I} _ {2}} (X _ {i})}\right), \tag {3.12}
$$

etc., and define ÀÜm(0) $\hat { m } _ { ( 0 ) } ^ { { \ L _ { 1 } } , * }$ I1,‚àó and ÀÜmI1,‚àó(1) $\hat { m } _ { ( 1 ) } ^ { { \ x } _ { 1 } , \ast }$ analogously. Given this setup, in order to verify (3.9), it suffices to show that

$$
\sqrt {n} \left(\hat {m} _ {(1)} ^ {\mathcal {I} _ {1}} - \hat {m} _ {(1)} ^ {\mathcal {I} _ {1}, *}\right)\rightarrow_ {p} 0. \tag {3.13}
$$

The proof can then be completed by carrying out the same argument for different folds and treatment statuses.

To this end, we decompose the error term in (3.13) as follows:

$$
\begin{array}{l} \hat {m} _ {(1)} ^ {\mathcal {I} _ {1}} - \hat {m} _ {(1)} ^ {\mathcal {I} _ {1, *}} \\ = \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) + W _ {i} \frac {Y _ {i} - \hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i})}{\hat {e} ^ {\mathcal {I} _ {2}} (X _ {i})} - \mu_ {(1)} (X _ {i}) - W _ {i} \frac {Y _ {i} - \mu_ {(1)} (X _ {i})}{e (X _ {i})}\right) \\ = \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \mu_ {(1)} (X _ {i})\right) \left(1 - \frac {W _ {i}}{e (X _ {i})}\right)\right) \\ + \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} W _ {i} \left(\left(Y _ {i} - \mu_ {(1)} (X _ {i})\right) \left(\frac {1}{\hat {e} ^ {\mathcal {I} _ {2}} (X _ {i})} - \frac {1}{e (X _ {i})}\right)\right) \\ - \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} W _ {i} \left(\left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \mu_ {(1)} (X _ {i})\right) \left(\frac {1}{\hat {e} ^ {\mathcal {I} _ {2}} (X _ {i})} - \frac {1}{e (X _ {i})}\right)\right) \\ \end{array}
$$

We can then verify that these terms are small for different reasons.

For the first term, we intricately use the fact that, thanks to our cross-fitting construction, ÀÜ¬µ(w) I2 $\hat { \mu } _ { \underline { { ( w ) } } } ^ { \perp _ { 2 } }$ can effectively be treated as deterministic when considering terms on $\mathcal { L } _ { 1 }$ . We first observe that, conditionally on $\mathcal { L } _ { 2 }$ and the observed covariate values, this term can be treated as average of independent mean-zero

terms, and

$$
\begin{array}{l} \mathbb {E} \left[ \left(\frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \mu_ {(1)} (X _ {i})\right) \left(1 - \frac {W _ {i}}{e (X _ {i})}\right)\right)\right) ^ {2} \mid \mathcal {I} _ {2}, \{X _ {i} \} \right] \\ = \operatorname {V a r} \left[ \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \mu_ {(1)} (X _ {i})\right) \left(1 - \frac {W _ {i}}{e (X _ {i})}\right)\right) \Big | \mathcal {I} _ {2}, \{X _ {i} \} \right] \\ = \frac {1}{\left| \mathcal {I} _ {1} \right| ^ {2}} \sum_ {i \in \mathcal {I} _ {1}} \mathbb {E} \left[ \left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \mu_ {(1)} (X _ {i})\right) ^ {2} \left(1 - \frac {W _ {i}}{e (X _ {i})}\right) ^ {2} \mid \mathcal {I} _ {2}, \{X _ {i} \} \right] \tag {3.14} \\ = \frac {1}{\left| \mathcal {I} _ {1} \right| ^ {2}} \sum_ {i \in \mathcal {I} _ {1}} \frac {1 - e (X _ {i})}{e (X _ {i})} \left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \mu_ {(1)} (X _ {i})\right) ^ {2} \\ \leq \frac {1 - \eta}{\eta} \frac {1}{| \mathcal {I} _ {1} | ^ {2}} \sum_ {i \in \mathcal {I} _ {1}} \left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \mu_ {(1)} (X _ {i})\right) ^ {2} = o _ {P} \left(\frac {1}{n ^ {1 + 2 \alpha_ {\mu}}}\right). \\ \end{array}
$$

The 3 equalities above are all due to cross-fitting, while the two inequalities are due to overlap (3.2) and consistency (3.11). Thus, because $\alpha _ { \mu } \geq 0$ , we can apply Chebyshev‚Äôs inequality to verify that the first summand itself is $o _ { P } ( 1 / \sqrt { n } )$ , i.e., as claimed it is negligible in probability on the $1 / \sqrt { n }$ -scale. The second summand in our decomposition above can also be bounded by a similar argument.

Finally, for the last summand, we use a Cauchy-Schwarz argument:20

$$
\begin{array}{l} \frac {1}{| \mathcal {I} _ {1} |} \sum_ {\{i: i \in \mathcal {I} _ {1}, W _ {i} = 1 \}} \left(\left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i}) - \mu_ {(1)} (X _ {i})\right) \left(\frac {1}{\hat {e} ^ {\mathcal {I} _ {2}} (X _ {i})} - \frac {1}{e (X _ {i})}\right)\right) \\ \leq \sqrt {\frac {1}{| \mathcal {I} _ {1} |} \sum_ {\{i : i \in \mathcal {I} _ {1} , W _ {i} = 1 \}} \left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} \left(X _ {i}\right) - \mu_ {(1)} \left(X _ {i}\right)\right) ^ {2}} \tag {3.15} \\ \times \sqrt {\frac {1}{| \mathcal {I} _ {1} |} \sum_ {\{i : i \in \mathcal {I} _ {1} , W _ {i} = 1 \}} \left(\frac {1}{\hat {e} ^ {\mathcal {I} _ {2}} (X _ {i})} - \frac {1}{e (X _ {i})}\right) ^ {2}} = o _ {P} \left(\frac {1}{n ^ {\alpha_ {\mu} + \alpha_ {e}}}\right), \\ \end{array}
$$

by risk decay (3.11). Thus, we find that this term is also $o _ { P } ( 1 / \sqrt { n } )$ , i.e., as claimed it is negligible in probability on the $1 / \sqrt { n }$ -scale.

Condensed notation We will be encountering cross-fit estimators frequently throughout the rest of this book. From now on, we‚Äôll use the following notation: We define the data into $K$ folds (above, $K = 2$ ), and compute estimators ¬µÀÜ(w) $\hat { \mu } _ { ( \underline { { w } } ) } ^ { ( - k ) } ( x )$ , etc., excluding the $k$ -th fold. Then, wr ing $k ( i )$ as the mapping that $k$

$$
\begin{array}{r l} \hat {\tau} _ {A I P W} = & \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\hat {\mu} _ {(1)} ^ {(- k (i))} (X _ {i}) - \hat {\mu} _ {(0)} ^ {(- k (i))} (X _ {i}) \right. \\ & \left. + W _ {i} \frac {Y _ {i} - \hat {\mu} _ {(1)} ^ {(- k (i))} (X _ {i})}{\hat {e} ^ {(- k (i))} (X _ {i})} - (1 - W _ {i}) \frac {Y _ {i} - \hat {\mu} _ {(0)} ^ {(- k (i))} (X _ {i})}{1 - \hat {e} ^ {(- k (i))} (X _ {i})}\right). \end{array} \tag {3.16}
$$

Note that the result in Theorem 3.2 applies equally well with any finite number $K$ of cross-fitting folds (and the same proof also works modulo updates to the notation).

Confidence intervals It is also important to be able to quantify uncertainty of treatment effect estimates. Thankfully, with AIPW, this turns out to be reasonably straight-forward. In the proof of Proposition 3.1, we saw that $V _ { A I P W }$ matches the variance of the summands $\Gamma _ { i }$ used to define the oracle AIPW estimator (3.6). This suggests using the following feasible variance estimate:21

$$
\widehat {V} _ {A I P W} = \frac {1}{n - 1} \sum_ {i = 1} ^ {n} \left(\widehat {\Gamma} _ {i} - \widehat {\tau} _ {A I P W}\right) ^ {2},
$$

$$
\begin{array}{l} \widehat {\Gamma} _ {i} = \hat {\mu} _ {(1)} ^ {(- k (i))} \left(X _ {i}\right) - \hat {\mu} _ {(0)} ^ {(- k (i))} \left(X _ {i}\right) \tag {3.17} \\ + W _ {i} \frac {Y _ {i} - \hat {\mu} _ {(1)} ^ {(- k (i))} (X _ {i})}{\hat {e} ^ {(- k (i))} (X _ {i})} - (1 - W _ {i}) \frac {Y _ {i} - \hat {\mu} _ {(0)} ^ {(- k (i))} (X _ {i})}{1 - \hat {e} ^ {(- k (i))} (X _ {i})}. \\ \end{array}
$$

The proof of Theorem 3.2 then implies that, under our assumptions, $\widehat { V } _ { A I P W }  _ { p }$ $V _ { A I P W }$ . We can thus produce level- $\alpha$ confidence intervals for $\tau$ as

$$
\tau \in \left(\hat {\tau} _ {A I P W} \pm \Phi^ {- 1} \left(1 - \frac {\alpha}{2}\right) \frac {1}{\sqrt {n}} \sqrt {\hat {V} _ {A I P W}}\right), \tag {3.18}
$$

where $\Phi ( \cdot )$ is the standard Gaussian CDF, and these will achieve coverage with probability $1 - \alpha$ in large samples. Similar argument can also be used to justify inference via resampling methods as in Efron [1982].

What if the propensity score is known? One special case worth considering is, what happens when the propensity score is known, and we implement the cross-fit AIPW estimator (3.16) with the true propensity scores $\hat { e } ^ { - k ( i ) } ( X _ { i } ) = e ( X _ { i } )$ . In this case Theorem 3.2 immediately implies the following.

Corollary 3.3. Under our basic setting with SUTVA, unconfoundedness and strong overlap, suppose that we know the true propensity scores and use them to construct the AIPW estimator. Suppose moreover that

$$
\frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\hat {\mu} _ {(w)} ^ {\mathcal {I} _ {2}} \left(X _ {i}\right) - \mu_ {(w)} \left(X _ {i}\right)\right) ^ {2} \rightarrow_ {p} 0, \tag {3.19}
$$

for $w \in \{ 0 , 1 \}$ and for with the roles of $\mathcal { I } _ { 1 }$ and $\mathcal { I } _ { 2 }$ swapped. Then (3.9) and (3.5) hold; and furthermore ${ \hat { \tau } } _ { A I P W }$ is exactly unbiased, $\mathbb { E } \left[ \hat { \tau } _ { A I P W } \right] = \tau$ .

Proof. The CLT statement follows from applying Theorem 3.2 with $\alpha _ { \mu } = 0$ and $\alpha _ { e } = + \infty$ . The unbiasedness claim follows by noting that, in the decomposition below (3.13), the second and third terms disappear when the true propensity scores are used, while the first term is mean-zero. ‚ñ°

This result is remarkable in that it shows that, if we use AIPW with true propensity scores, then AIPW will achieve the target asymptotic behavior (3.5) as long as we use any regression adjustment that is consistent in the extremely weak sense (3.19). In particular, no rates of convergence are required.

It is well known that there are several machine learning methods, including $k$ -nearest neighbors, that are universally consistent, i.e., they achieve error guarantees (3.19) for any IID data-generating distribution, without any assumptions on the joint distribution of $X _ { i }$ and $Y _ { i } ( w )$ other than $\mathbb { E } \left[ Y _ { i } ^ { 2 } ( w ) \right] < \infty$ [Stone, 1977]. Corollary 3.3 implies that if we run AIPW implemented with an universally consistent $\hat { \mu } _ { ( w ) } ( x )$ estimator and the true propensity scores, then it always satisfies (3.5) under our basic setting.

Corollary 3.3 also provides a practical resolution to the apparent paradox highlighted in Chapter 2, whereby IPW with oracle weights could sometimes (in specific settings) be outperformed by IPW with estimated weights. This seemed to lead to a tension where, if propensity scores were known, then we could choose to either use oracle IPW, which is always unbiased but has a larger asymptotic variance, or feasible IPW, which may be more accurate but may fail completely if we accidentally misspecify the propensity model.

The reason Corollary 3.3 helps is that, on inspection, one notices that the asymptotic variance $V _ { A I P W }$ achieved (in considerable generality) in Corollary 3.3 exactly matches the asymptotic variance $V _ { S T R A T }$ achieved by feasible IPW

(in the special case where $X _ { i }$ has discrete support). Thus, what Corollary 3.3 shows us is that, if we know the true propensity scores, then we can always (and without really any downsides, at least asymptotically) avoid the excess asymptotic variance of oracle IPW by simply using AIPW with an universally consistent regression adjustment instead.

# 3.2 Efficient estimation under uncounfoundedness

In Chapter 2 we studied average treatment effect estimation under unconfoundedness and when $X _ { i }$ is discrete. In this setting, the stratify-by- $X _ { i }$ estimator is obviously a (or perhaps the) natural thing to do; and in Theorem 2.1 we showed that it achieves an asymptotic variance $V _ { S T R A T }$ . Meanwhile, in this chapter, we studied a seemingly completely different estimator, AIPW, and showed it can also achieve an asymptotic variance $V _ { A I P W } = V _ { S T R A T }$ , but under much more general conditions (and in particular without assuming that $X _ { i }$ is discrete).

These observations suggest that the behavior

$$
\sqrt {n} \left(\hat {\tau} - \tau^ {*}\right) \Rightarrow \mathcal {N} (0, V ^ {*})
$$

$$
V ^ {*} = \operatorname {V a r} \left[ \tau (X _ {i}) \right] + \mathbb {E} \left[ \frac {\sigma_ {(0)} ^ {2} (X _ {i})}{1 - e (X _ {i})} \right] + \mathbb {E} \left[ \frac {\sigma_ {(1)} ^ {2} (X _ {i})}{e (X _ {i})} \right], \tag {3.20}
$$

may in fact be the optimal behavior we can hope to achieve for any nonparametric average treatment effect estimator $\hat { \tau }$ under unconfoundedness. Theorem 3.2 provides an upper bound, showing that this behavior can in fact be achieved by a practical estimator, ${ \hat { \tau } } _ { A I P W }$ , under considerable generality. Meanwhile, our discussion in Chapter 2 provides a heuristic lower bound; after all, how could one possibly hope to find an estimator that‚Äôs more accurate than the stratify-by- $X _ { i }$ estimator in the setting where $X _ { i }$ is discrete?

The following result establishes this conjecture, using a proof technique from Chamberlain [1992]. Following H¬¥ajek [1972], he defines optimality in terms of a local asymptotic minimax criterion: $V ^ { * }$ is called the efficient variance for estimating $\tau$ if an estimator satisfying (3.20) exists and, for any data-generating distribution $P$ , no estimator exists that is more accurate than (3.20) uniformly over a suitably expressive neighborhood of $P$ .22 Further, any estimator satisfying (3.20), potentially assuming reasonable regularity conditions, is called efficient.

Theorem 3.4. Under basic setting with SUTVA, unconfoundedness and strong overlap, $V ^ { * }$ is the efficient variance for estimating the average treatment effect.

Proof. We have already established existence of an estimator satisfying (3.20) in Theorem 3.2. For the local optimality statement, we follow the blueprint of Theorem 1 of Chamberlain [1992], and do the following: We start by considering distributions where ( $X _ { i }$ , $Y _ { i } ( 0 )$ , $Y _ { i } ( 0 )$ ) have a distribution $P$ with a jointly discrete support (i.e., both $X _ { i }$ and $Y _ { i } ( w )$ have discrete support), and verify that the asymptotic variance of the saturated maximum likelihood estimator of the ATE matches $V ^ { * }$ . We then argue that ATE estimation with a discrete $P$ is a parametric problem and so maximum likelihood estimation must be efficient; and that any continuous distribution is well approximable by a discrete distribution, so this efficiency result carries over to the continuous case. We refer to Chamberlain [1992] for technical details, and for verifying that this blueprint is in fact valid.

Consider now the case where $P$ takes on values on a discrete space $\mathcal { X } \times \mathcal { Y } \times \mathcal { Y }$ with $\mathcal { V } \subset \mathbb { R }$ . For any distribution $P$ let $\tau ( P ) = \mathbb { E } _ { P } \left[ Y _ { i } ( 1 ) - Y _ { i } ( 0 ) \right]$ and note that, under unconfoundedness and with discrete support,

$$
\tau (P) = \sum_ {x \in \mathcal {X}} P (x) \left(\sum_ {y \in \mathcal {Y}} y P _ {1} (y | x) - \sum_ {y \in \mathcal {Y}} y P _ {0} (y | x)\right) \tag {3.21}
$$

where $P ( x ) = \mathbb { E } _ { P } \left[ X _ { i } = x \right]$ and $P _ { w } ( y | x ) = \mathbb { E } _ { P } [ Y _ { i } = y \vert X _ { i } = x$ , $W _ { i } = w \rfloor$ . Now, given $n$ draws from $P$ , let $n _ { x } = | \{ i : X _ { i } = x \} |$ , $n _ { x w } = | \{ i : X _ { i } = x$ , $W _ { i } = w \} |$ and $n _ { x y w } = | \{ i : X _ { i } = x$ , $Y _ { i } = y$ , $W _ { i } = w \} |$ . The saturated maximum likelihood estimator for the data-generating distribution $P$ is given by $\widehat { P } ( x ) = n _ { x } / n$ and $\widehat { P } _ { w } ( y | x ) = n _ { x y w } / n _ { x w }$ . The maximum likelihood estimator for $\tau$ is then

$$
\hat {\tau} = \tau (\widehat {P}) = \sum_ {x \in \mathcal {X}} \widehat {P} (x) \left(\sum_ {y \in \mathcal {Y}} y \widehat {P} _ {1} (y | x) - \sum_ {y \in \mathcal {Y}} y \widehat {P} _ {0} (y | x)\right), \tag {3.22}
$$

which can be algebraically be verified to be equivalent to $\hat { \tau } _ { S T R A T }$ in this setting. Thus, the asymptotic variance of maximum likelihood here is $V _ { S T R A T }$ , which by Theorem 2.1 is equal to $V ^ { * }$ . ‚ñ°

Comparing regularity conditions One ambiguity in the definitions above is that we said that an estimator is efficient if it achieves the behavior (3.20) under ‚Äúreasonable‚Äù regularity conditions‚Äîbut what does it mean for regularity conditions to be reasonable? We have so far seen 3 results about estimators achieving the behavior (3.20): Corollary 3.3 shows this for AIPW with

known propensity scores essentially without assumptions; Theorem 3.2 shows this for AIPW with estimated propensity scores under the (moderately strong?) rate-of-convergence assumption (3.11); while Theorem 2.1 showed this for the stratify-on- $X _ { i }$ estimator under the (very strong) assumption that $X _ { i }$ is discrete.

This ambiguity is intentional, and can be helpful in describing and assessing various proposed estimators of the average treatment effect under unconfoundedness. When considering a candidate estimator, a good first question can be to ask whether it is efficient, i.e., whether it sometimes achieves the behavior (3.11). If an estimator is not efficient (e.g., like the oracle IPW estimator), then it may be worth discarding at this step. Then, among efficient estimators, a good second question to ask is how robust it is, i.e., how strong are the regularity conditions needed for efficiency. This allows to argue, e.g., that ${ \hat { \tau } } _ { A I P W }$ requires much weaker regularity conditions than $\scriptstyle { \hat { \tau } } _ { S T R A T }$ to achieve desirable asymptotic performance, and from this angle ${ \hat { \tau } } _ { A I P W }$ appears preferable.

Is efficiency a realistic goal? Until recently, the perspective taken above, i.e., that efficiency is a criterion that should guide practical choice of average treatment effect estimators, would have been considered controversial by many econometricians and statisticians. Methods that achieved efficiency were often considered fragile, complicated and/or impractical; and, in problems that called for treatment effect estimation under unconfoundedness, econometric practice largely focused on methods that require parametric assumptions and are not consistent under unconfoundedness alone (e.g., linear regression), or non-efficient but conceptually simple methods (e.g., matching).

The critique that early methods designed to achieve efficiency were hard to use in practice is on point: For example, such methods would often rely on specific smoothness assumptions, and then rely on series estimators with specific basis functions (depending on the assumed smoothness class) to form treatment effect estimators.

The double machine learning framework, however, makes widespread use of efficient treatment effect estimators much more practical. The main regularity condition (3.11) doesn‚Äôt depend on how we choose to estimate the non-parametric components, and instead only requires that they are accurate enough under squared-error loss. Machine learning methods are often tuned via cross-validation under squared error loss, and this way of tuning predictors is perfectly aligned with making the error terms in (3.11) small. Thus, perhaps surprisingly, although machine learning may at first seem like a glance seem like a technology that should be kept as far away from causal inference as possible, it turns out that‚Äîvia the double machine learning construction‚Äîmachine learning (and, more generally, automatic black-box non-parametric prediction)

is a key ingredient in making efficient treatment effect estimation practical in a wide variety of settings.

# Bibliographic notes

The literature on semiparametrically efficient treatment effect estimation via AIPW was pioneered by Robins, Rotnitzky, and Zhao [1994], and developed in a sequence of papers including Robins and Rotnitzky [1995] and Scharfstein, Rotnitzky, and Robins [1999]. The form of the AIPW estimator is also present in early work by Cassel, S¬®arndal, and Wretman [1976] in survey sampling. The effect of knowing the propensity score on the semiparametric efficiency bound for average treatment effect estimation is discussed in Hahn [1998], while the behavior of AIPW with high-dimensional regression adjustments was first considered by Farrell [2015]. These results fit into a broader literature on semiparametrics, including Bickel, Klaassen, Ritov, and Wellner [1993] and Newey [1994].

The approach taken here, with a focus on generic machine learning estimators for non-parametric components and cross-fitting, follows the double machine learning framework of Chernozhukov et al. [2018]. One major strength of this approach is in its generality and its ability to handle arbitrary machine learning estimators for $\hat { \mu } _ { ( w ) } ( x )$ and ${ \hat { e } } ( x )$ . Another, closely related framework is the targeted learning framework of van der Laan and Rubin [2006], which uses a different functional form than AIPW but can also be shown to achieve efficiency using machine learning estimators for non-parametric components [van der Laan and Rose, 2011].

There is a large number of estimators known to achieve efficiency under a variety of regularity conditions. For example, Hahn [1998] showed that non-parametric regression adjustment estimators can be efficient under strong smoothness conditions and specific regression estimators, while Hirano, Imbens, and Ridder [2003] showed this type of result for non-parametric IPW. The efficiency result given in Theorem 3.2 for AIPW is, however, much more robust‚Äîin that it allows for use of generic machine learning methods provided they satisfy the relatively mild rate conditions (3.11).

More recently, there has been considerable interest in deriving estimators that achieve efficiency under minimal conditions. In the case where the functions $\mu _ { ( w ) } ( \cdot )$ and $e ( \cdot )$ belong to H¬®older smoothness classes Robins et al. [2017] show that, writing $\alpha _ { \mu }$ and $\alpha _ { e }$ for the best constants for which rates of convergence of the type (3.11) can be achieved under the posited smoothness

assumptions, the weakest condition under which efficiency is possible is

$$
\frac {\alpha_ {\mu}}{1 - 2 \alpha_ {\mu}} + \frac {\alpha_ {e}}{1 - 2 \alpha_ {e}} \geq \frac {1}{2}, \tag {3.23}
$$

and this rate can be achieved using what Robins et al. [2017] refer to as higherorder influence function (HOIF) estimators. The improvement of the condition (3.23) over the condition $\alpha _ { \mu } + \alpha _ { e } \ge 1 / 2$ in Theorem 3.2 is considerable; for example, when both rates are equal, in Theorem 3.2 we could allow for $\alpha _ { \mu } =$ $\alpha _ { e } \geq 1 / 4$ while (3.23) allows for $\alpha _ { \mu } = \alpha _ { e } \ge 1 / 6$ .

One challenge with the HOIF estimator of Robins et al. [2017], however, is that to date it has been challenging to implement in practical applications; and so there has been work on methods that can improve over AIPW while remaining practically feasible. Hirshberg and Wager [2021] show that a variant of AIPW with a choice of propensity model specifically designed to minimize bias from errors in $\hat { \mu } _ { ( w ) } ( x )$ is efficient under conditions that, in the H¬®older case, amount to $\alpha _ { \mu } \geq 1 / 4$ (with no assumptions on $\alpha _ { e }$ ); note that this corresponds to one extreme point of the optimality surface (3.23). Meanwhile, Newey and Robins [2018] and McClean et al. [2024] show how, in some settings, the use of undersmoothed estimators and 3-way cross-fitting can achieve minimal conditions for efficiency.

# Chapter 4 Estimating Heterogeneous Treatment Effects

In many application areas, there is interest in going beyond average effects, and to understand how treatment effects vary across units. In personalized medicine, we may want to identify groups of patients who are more likely to benefit (or less likely to suffer side effects) from a drug than others; and, in online marketing, one may want to identify groups of customers more likely to respond to an offer. This chapter introduces and compares a variety of methods for estimating heterogeneous treatment effects.

The conditional average treatment effect Throughout this chapter, we will work under the same ‚Äúbasic setting‚Äù as considered in the previous chapter, i.e., with SUTVA, unconfoundedness and overlap; however, rather than focusing on the average treatment effect, we now seek to estimate, understand, and eventually act on heterogeneity in how different units respond to treatment. At first glance, one might think that estimating treatment heterogeneity should involve targeting the individual- $i$ specific individual treatment effects (ITEs) $\Delta _ { i } = Y _ { i } ( 1 ) - Y _ { i } ( 0 )$ . The ITEs, however, are generally not point-identified even under strong assumptions, and so methodologies targeting the ITEs themselves are often not practical.

A more practical way to quantify treatment heterogeneity under unconfoundedness is via the conditional average treatment effect (CATE)

$$
\tau (x) = \mathbb {E} \left[ Y _ {i} (1) - Y _ {i} (0) \mid X _ {i} = x \right]. \tag {4.1}
$$

The CATE is still an average effect; but we now consider how this average to varies when conditioning on potential effect modifiers $X _ { i }$ . Note that the definition of the CATE depends on which pre-treatment covariates are used in (4.1): If we condition on a richer set of covariates, then the CATE function

will become more expressive (and capture a higher fraction of the variance of the underlying ITEs).

There are many reasons to consider the CATE as a statistical target. It is simple to understand and work with; and, unlike the ITE, it is point-identified. There are also formal, decision theoretic reasons to pay attention to the CATE. For example, the following result (stated here without proof) shows that utilitarian targeting rules can be expressed as thresholding rules on the CATE.

Proposition 4.1. Under the basic setting with SUTVA, unconfoundedness and overlap described in Chapter 3, suppose a decision maker gets reward $Y _ { i } ( w )$ for assigning treatment arm w to unit $i$ , and needs to pay a cost $C$ every time they assign treatment (the control arm is free). Then, the decision rule that treats units whose CATE is greater than the cost $C$ , i.e., $1 \left( \left\{ \tau ( X _ { i } ) > C \right\} \right)$ ), maximizes expected rewards among all decision rules that are measurable with respect to observed pre-treatment covariates $X _ { i }$ .

Example 5. Kitagawa and Tetenov [2018] discuss optimal targeting of eligibility to training and job-search assistance under the National Job Training Partnership Act (JTPA). Here, the treatment $W _ { i }$ is program eligibility, the outcome $Y _ { i }$ is earnings within 30 months of treatment assignment, and pretreatment covariates available for targeting are Xi = {education, income}. The welfare-maximizing targeting rule then compares the CATE to the cost of treatment.23

Regularization bias Before presenting methods for CATE estimation, it is helpful to review some issues faced by a simple baseline method. Under unconfoundedness, the CATE can be written as a difference in conditional response surfaces,

$$
\tau (x) = \mu_ {(1)} (x) - \mu_ {(0)} (x), \quad \mu_ {(w)} (x) = \mathbb {E} \left[ Y _ {i} \mid X _ {i} = x, W _ {i} = w \right]. \tag {4.2}
$$

Thus, we could immediately obtain a consistent estimator for $\tau ( \cdot )$ by consistently fitting $\hat { \mu } _ { ( 0 ) } ( \cdot )$ and $\hat { \mu } _ { ( 1 ) } ( \cdot )$ via separate non-parametric regressions on the controls and treated units respectively, and then estimating the CATE as their

difference. Following the nomenclature of K¬®unzel et al. [2019], the resulting estimator is often referred to as the T-learner:

$$
\hat {\tau} _ {T} (x) = \hat {\mu} _ {(1)} (x) - \hat {\mu} _ {(0)} (x). \tag {4.3}
$$

However, while the T-learner is consistent, it may not perform well in finite samples due to a phenomenon called regularization bias: Given that we fit $\hat { \mu } _ { ( 0 ) } ( \cdot )$ and $\hat { \mu } _ { ( 1 ) } ( \cdot )$ separately, these two functions may end up being regularized in different ways from each other, creating artifacts in the learned CATE estimate ${ \hat { \tau } } _ { T } ( x )$ . This problem is particularly acute if we use methods where the amount of regularization depends on sample size, and if there are many more control than treated units (or vice-versa).24

Figure 4.1, illustrates this issue. There is no treatment effect, so $\mu _ { ( 0 ) } ( x ) =$ $\mu _ { ( 1 ) } ( x )$ and $\tau ( x ) = 0$ , but both regression surfaces oscillate with $x$ . The data is collected via a randomized trial with $\pi = 0 . 1$ , so there are many more controls than treated units. Here, there end up being enough controls for $\hat { \mu } _ { ( 0 ) } ( \cdot )$ to be well estimated and capture the underlying oscillation of the conditional response function. On the other hand, there are very few treated units, and so the best we can do with $\hat { \mu } _ { ( 1 ) } ( \cdot )$ is to heavily regularize it, resulting in an estimate that is almost constant in $x$ . Both estimates $\hat { \mu } _ { ( 0 ) } ( \cdot )$ and $\hat { \mu } _ { ( 1 ) } ( \cdot )$ are reasonable on their own; however, once we take their difference as in (4.3), we find strong apparent heterogeneity is ${ \hat { \tau } } _ { T } ( x )$ , which is concerning since in reality $\tau ( x ) = 0$ everywhere in this example.

A second concern with the T-learner, regularization-induced confounding, arises because the T-learner does not explicitly account for variation in the propensity score. If $e ( x )$ varies considerably, then our estimates of $\hat { \mu } _ { ( 0 ) } ( \cdot )$ will be driven by data in areas with more control units (i.e., with $e ( x )$ closer to $0$ ), and those of $\hat { \mu } _ { ( 1 ) } ( \cdot )$ by regions with more treated units (i.e., with $e ( x )$ closer to 1). And if there is covariate shift between the data used to learn $\hat { \mu } _ { ( 0 ) } ( \cdot )$ and $\hat { \mu } _ { ( 1 ) } ( \cdot )$ , this may create biases for their difference ${ \hat { \tau } } _ { T } ( x )$ .

# 4.1 Semiparametric modeling

As our analysis of regularization bias made clear, any good method for estimating the CATE should ‚Äúfocus‚Äù on estimating the CATE $\tau ( x )$ accurately‚Äîand, in a flexible statistical learning setting, this is not necessarily the same thing as simultaneously estimating $\mu _ { ( 0 ) } ( x )$ and $\mu _ { ( 1 ) } ( x )$ accurately. To understand what

![](images/fae377de6d803a8ed78046f5333aa0808718d6a3ebf6d67e141d697096916dc5.jpg)  
Figure 4.1: Illustration of regularization bias. Both control (blue) and treated (red) units are drawn from the same distribution. Data is generated from an RCT with $\pi = 0 . 1$ , and so there are more controls than treated units. Spline regression learns a more oscillatory model for $\mu _ { ( 0 ) } ( x )$ and a flat one for $\mu _ { ( 1 ) } ( x )$ . This results in an oscillatory CATE estimate, illustrated via shading, whereas the true CATE here is identically 0.

it takes to successfully target the CATE, it is helpful to start by considering the following semiparametric specification:

$$
\tau (x) = \psi (x) \cdot \beta , \quad \psi : \mathcal {X} \rightarrow \mathbb {R} ^ {d}, \quad \beta \in \mathbb {R} ^ {d}. \tag {4.4}
$$

For example, in the context of Example 5, if $\mathcal { X }$ contains unstructured data on income and education, one could set $\psi ( x ) = \left\{ \begin{array} { l l } \end{array} \right.$ {income in previous year, has high-school degree, has college degree}.

We refer to this specification as semiparametric because our overall specification is non-parametric (in particular, $\mu _ { ( 0 ) } ( x )$ and $e ( x )$ arbitrary), but we imposed a parametric specification on the key component of interest. Under the model (4.4), estimating the CATE reduces to estimating $\beta$ . Working under the basic setting from Chapter 3 and writing $\varepsilon _ { i } ( w ) = Y _ { i } ( w ) - \mu _ { ( w ) } ( X _ { i } )$ , the addition of the parametric constraint (4.4) lets us re-express our data-generating distribution as a partially linear model,

$$
Y _ {i} (w) = \mu_ {(0)} \left(X _ {i}\right) + w \psi \left(X _ {i}\right) \cdot \beta + \varepsilon_ {i} (w). \tag {4.5}
$$

This class of problems was studied by Robinson [1988] who showed that, for estimating $\beta$ , it is helpful to re-write (4.5) as

$$
Y _ {i} - m (X _ {i}) = \left(W _ {i} - e (X _ {i})\right) \psi (X _ {i}) \cdot \beta + \varepsilon_ {i}, \text {w h e r e} \tag {4.6}
$$

$$
m (x) = \mathbb {E} \left[ Y _ {i} \mid X _ {i} = x \right] = \mu_ {(0)} (x) + e (x) \psi (x) \cdot \beta
$$

denotes the conditional expectation of the observed $Y _ { i }$ , marginalizing over $W _ { i }$ and $\varepsilon _ { i } = \varepsilon _ { i } ( W _ { i } )$ .

The expression (4.6) shows that, if we knew $m ( x )$ and $e ( x )$ , then we could estimate $\beta$ via a simple regression algorithm: First define $Y _ { i } ^ { * } = Y _ { i } - m ( X _ { i } )$ and $\bar { Z } _ { i } ^ { * } = \psi ( X _ { i } ) ( W _ { i } - \underset { \sim } { e } ( X _ { i } ) ) _ { }$ , and then estimate $\hat { \beta } ^ { * }$ by running residual-onresidual regression $Y _ { i } ^ { * } \sim Z _ { i } ^ { * }$ . In practice, of course, $e ( x )$ may not be known and $m ( x )$ is essentially never known, and so running the above approach is not feasible.

Our discussion in Chapter 3, however, motivates trying a plug-in approach using the double machine learning framework. We first estimate the unknown components $m ( x )$ and $e ( x )$ via a machine learning method of our choice, and then plug them into (4.6) using cross-fitting:

1. Run non-parametric regressions $Y \sim X$ and $W \sim X$ using a method of our choice to get $\hat { m } ( x )$ and ${ \hat { e } } ( x )$ respectively.   
2. Use cross-fit residuals to define transformed features $\widetilde { Y } _ { i } = Y _ { i } - \hat { m } ^ { ( - k ( i ) ) } ( X _ { i } )$ and $\widetilde { Z } _ { i } = \psi ( X _ { i } ) ( W _ { i } - \hat { e } ^ { ( - k ( i ) ) } ( X _ { i } ) )$ .   
3. Estimate $\hat { \beta }$ by running a linear regression $\widetilde { Y _ { i } } \sim \widetilde { Z } _ { i }$ .

As established below, this residual-on-residual regression estimator has a similar special property as established for AIPW in Theorem 3.2: As long as the non-parametric components are reasonably accurately estimated, then $\hat { \beta }$ i s asymptotically equivalent to the oracle $\hat { \beta } ^ { * }$ , and satisfies a central limit theorem at the $1 / \sqrt { n }$ -scale.25

Theorem 4.2. Under the basic setting with SUTVA, unconfoundedness and overlap described in Chapter 3, suppose that (4.4) holds, that the regression features are bounded $\| \psi ( X _ { i } ) \| _ { \infty } \leq M$ , and that we estimate $\beta$ via a $K$ -fold

cross-fit version of residual-on-residual regression as given above. Suppose further that we use estimators for the non-parametric components such that, for all folds $k = 1 , \ldots , K$ $K$ ,

$$
n ^ {2 \alpha_ {m}} \frac {1}{\left|\left\{i : k (i) = k \right\}\right|} \sum_ {\left\{i: k (i) = k \right\}} \left(\hat {m} ^ {(- k)} \left(X _ {i}\right) - m \left(X _ {i}\right)\right) ^ {2} \rightarrow_ {p} 0, \tag {4.7}
$$

$$
n ^ {2 \alpha_ {e}} \frac {1}{| \{i : k (i) = k \} |} \sum_ {\{i: k (i) = k \}} \left(\hat {e} ^ {(- k)} (X _ {i}) - e (X _ {i})\right) ^ {2} \rightarrow_ {p} 0,
$$

for some constants satisfying $\alpha _ { m } \geq 0$ , $\alpha _ { e } \geq 1 / 4$ and $\alpha _ { m } + \alpha _ { e } \ge 1 / 2$ . Then, writing $\widetilde { Z } _ { i } ^ { * }$ and $\widetilde { Z } _ { i } ^ { * }$ are the oracle residuals as defined below (4.6),

$$
\sqrt {n} (\hat {\beta} - \beta) \Rightarrow \mathcal {N} (0, V _ {\beta}), V _ {\beta} = \operatorname {V a r} \left[ \widetilde {Z} _ {i} ^ {*} \right] ^ {- 1} \mathbb {E} \left[ \left(\varepsilon_ {i} \widetilde {Z} _ {i} ^ {*}\right) ^ {\otimes 2} \right] \operatorname {V a r} \left[ \widetilde {Z} _ {i} ^ {*} \right] ^ {- 1}, \tag {4.8}
$$

provided Var $\left[ \widetilde { Z } _ { i } ^ { * } \right]$ has full rank.

Proof. Under our basic setting and (4.4), the expression (4.6) can be viewed as a well-specified linear model with heteroskedastic errors. Thus, a standard analysis of linear regression under heteroskdasticity [White, 1980] immediately implies that the oracle residual-on-residual regression estimator $\hat { \beta } ^ { * }$ satisfies the limit result (4.8). It thus suffices to show that $\sqrt { n } ( \hat { \beta } - \hat { \beta } ^ { * } ) \to _ { p } 0$ .

We can explicitly write out the feasible and oracle residual-on-residual regression estimators as

$$
\hat {\beta} = \left(\frac {1}{n} \sum_ {i = 1} ^ {n} \widetilde {Z} _ {i} ^ {\otimes 2}\right) ^ {- 1} \frac {1}{n} \sum_ {i = 1} ^ {n} \widetilde {Z} _ {i} \widetilde {Y} _ {i}, \quad \hat {\beta} ^ {*} = \left(\frac {1}{n} \sum_ {i = 1} ^ {n} \widetilde {Z} _ {i} ^ {* \otimes 2}\right) ^ {- 1} \frac {1}{n} \sum_ {i = 1} ^ {n} \widetilde {Z} _ {i} ^ {*} \widetilde {Y} _ {i} ^ {*}. \tag {4.9}
$$

We start showing that, for each fold $k$

$$
\sqrt {n} \left(\frac {1}{n} \sum_ {\{i: k (i) = k \}} \widetilde {Z} _ {i} \widetilde {Y} _ {i} - \frac {1}{n} \sum_ {\{i: k (i) = k \}} \widetilde {Z} _ {i} ^ {*} \widetilde {Y} _ {i} ^ {*}\right)\rightarrow_ {p} 0.
$$

To do so, we spell out $\bar { Y _ { i } }$ , $\breve { Z _ { i } }$ , etc., and expand

$$
\begin{array}{l} \sum_ {\{i: k (i) = k \}} \psi (X _ {i}) \left(W _ {i} - \hat {e} ^ {(- k)} (X _ {i})\right) \left(Y _ {i} - \hat {m} ^ {(- k)} (X _ {i})\right) - \psi (X _ {i}) \left(W _ {i} - e (X _ {i})\right) \left(Y _ {i} - m (X _ {i})\right) \\ = \sum_ {\{i: k (i) = k \}} \psi \left(X _ {i}\right) \left(W _ {i} - e \left(X _ {i}\right)\right) \left(m \left(X _ {i}\right) - \hat {m} ^ {(- k)} \left(X _ {i}\right)\right) \\ + \sum_ {\{i: k (i) = k \}} \psi (X _ {i}) \left(e (X _ {i}) - \hat {e} ^ {(- k)} (X _ {i})\right) (Y _ {i} - m (X _ {i})) \\ + \sum_ {\{i: k (i) = k \}} \psi \left(X _ {i}\right) \left(e \left(X _ {i}\right) - \hat {e} ^ {(- k)} \left(X _ {i}\right)\right) \left(m \left(X _ {i}\right) - \hat {m} ^ {(- k)} \left(X _ {i}\right)\right). \\ \end{array}
$$

We then bound these terms exactly as in the proof of Theorem 3.2: For the first two terms above we rely on cross-fitting; while for the last we use Cauchy-Schwarz (relying on our assumptions that $\alpha _ { m } + \alpha _ { e } \ge 1 / 2$ and $\| \psi ( X _ { i } ) \| _ { \infty } \leq M )$ . The fact that

$$
\sqrt {n} \left(\frac {1}{n} \sum_ {\{i: k (i) = k \}} \widetilde {Z} _ {i} ^ {\otimes 2} - \frac {1}{n} \sum_ {\{i: k (i) = k \}} \widetilde {Z} _ {i} ^ {* \otimes 2}\right)\rightarrow_ {p} 0
$$

follows by the same argument, except now we need to use $2 \alpha _ { e } \geq 1 / 2$ in the Cauchy-Schwarz bound. Finally, to put everything together, we invoke Slutsky‚Äôs lemma, the fact that

$$
\frac {1}{n} \sum_ {i = 1} ^ {n} \widetilde {Z} _ {i} ^ {* \otimes 2} \rightarrow_ {p} \operatorname {V a r} \left[ \widetilde {Z} _ {i} ^ {*} \right] \succ 0,
$$

and that the matrix inverse is a continuous function in the neighborhood of full-rank matrices. ‚ñ°

The constant effect model One interesting special case of semiparametric modeling is the constant treatment effect model

$$
\mu_ {(1)} (x) - \mu_ {(0)} (x) = \tau , \tag {4.10}
$$

whereby we assert that treatment effects do not vary with covariates; this is an instance of (4.4) with $\psi ( x ) = 1$ . We can thus also apply the residual-onresidual regression approach developed above in this setting, resulting in the following:

Corollary 4.3. Under the basic setting with SUTVA, unconfoundedness and overlap from Chapter 3, suppose that the constant treatment effect model (4.10) holds, and we estimate $\tau$ via a cross-fit plug-in residual-on-residual estimator with non-parametric components satisfying (4.7). Then,

$$
\begin{array}{l} \sqrt {n} (\hat {\tau} - \tau) \Rightarrow \mathcal {N} (0, V _ {\tau}), \\ V _ {\tau} = \frac {\mathbb {E} \left[ e (X _ {i}) (1 - e (X _ {i})) \left((1 - e (X _ {i})) \sigma_ {(1)} ^ {2} (X _ {i}) + e (X _ {i}) \sigma_ {(0)} ^ {2} (X _ {i})\right) \right]}{\mathbb {E} \left[ e (X _ {i}) (1 - e (X _ {i}) \right] ^ {2}}. \tag {4.11} \\ \end{array}
$$

Note that, under the model (4.10), one could also have estimated the parameter $\tau$ via methods for the average treatment effect such as AIPW (because, when the treatment effect is constant $\tau$ , then the average treatment effect is

also $\tau$ ). However, AIPW would in this case generally be less accurate than the residual-on-residual regression estimator. In particular, in the special case where (4.10) holds and $\sigma _ { ( 0 ) } ^ { 2 } ( x ) = \sigma _ { ( 1 ) } ^ { 2 } ( x ) = \sigma ^ { 2 }$ , then26

$$
V _ {\tau} = \frac {\sigma^ {2}}{\mathbb {E} [ e (X _ {i}) (1 - e (X _ {i}) ]} \leq \sigma^ {2} \mathbb {E} \left[ \frac {1}{e (X _ {i}) (1 - e (X _ {i}))} \right] = V _ {A I P W}, \qquad (4. 1 2)
$$

where the inequality above follows from Jensen‚Äôs inequality. This observation highlights the fact that efficiency of an estimator for a specific target depends closely on assumptions made. We showed Chapter 3 that AIPW is efficient in our generic non-parametric setting; however, once we add an extra constraint like (4.10), then estimators that exploit this constraint can do better.27

# 4.2 A loss function for treatment heterogeneity

The residual-on-residual regression estimator developed above is helpful if we believe in the semiparametric specification (4.4). In order to meet our original goal of estimating the CATE in a generic setting with unconfoundedness, however, we need to generalize this estimator to a fully non-parametric setting.

As background for how to do this, it is helpful to think in terms of how this generalization was carried out in the context of simple prediction, i.e., predicting a real-valued $Y _ { i }$ from features $X _ { i }$ . The classical approach to doing so is via linear regression, but nowadays methods like decision trees, boosting and neural networks offer compelling non-parametric alternatives. Key insights in this progression include the use of flexible basis expansions to express more complicated signals; penalization to keep the complexity of the learned predictor in check despite the use of high-dimensional basis expansions; cross-validation to tune the amount of penalization; and algorithmic techniques like decision trees and neural networks to adaptively generate basis expansions suited to the task at hand. Hastie, Tibshirani, and Friedman [2009] provide an excellent book-length presentation of these concepts; Chapters 3, 5 and 7 are particularly relevant for understanding the discussion below.

Our task here is to deploy all these concepts to CATE estimation. To this end, we start by writing the residual-on-residual regression from above as a loss-minimization problem. Recall that, in the simple prediction case, the ordinary least-squares solution $\hat { \beta }$ to regressing $Y _ { i }$ on $\psi ( X _ { i } )$ using $n$ samples can

be characterized via squared-error loss minimization,

$$
\hat {\beta} = \operatorname {a r g m i n} _ {\beta} \left\{\frac {1}{n} \sum_ {i = 1} ^ {n} \ell_ {r e g} (Y _ {i}; \psi (X _ {i}) \cdot \beta) \right\}, \quad \ell_ {r e g} (y; z) = (y - z) ^ {2}. \qquad (4. 1 3)
$$

By the same argument, we can verify that our residual-on-residual regression algorithm also minimizes a certain least-squares objective, namely $^ { 2 8 }$

$$
\hat {\beta} = \operatorname {a r g m i n} _ {\beta} \left\{\frac {1}{n} \sum_ {i = 1} ^ {n} \hat {\ell} ^ {(- k (i))} \left(X _ {i}, Y _ {i}, W _ {i}; \psi \left(X _ {i}\right) \cdot \beta\right) \right\} \tag {4.14}
$$

$$
\hat {\ell} ^ {(- k)} (x, y, w; z) = \left(\left(y - \hat {m} ^ {(- k)} (x)\right) - (w - \hat {e} ^ {(- k)} (x)) z\right) ^ {2}.
$$

One critical difference between (4.13) and (4.14) is that, in our setting, the ‚Äúloss‚Äù function $\hat { \ell } ^ { ( - k ) }$ is data-dependent, and takes as input our cross-fitted predictions for $m ( \cdot )$ and $e ( \cdot )$ . The fact that our loss function is data-dependent in this way will lead to technical challenges down the road; however, it does not preclude us from proceeding with algorithm development.

We are now ready to apply the statistical learning roadmap to CATE estimation. We still start from the semiparametric specification (4.4); however, we now consider featurizations $\psi : \mathcal { X }  \mathbb { R } ^ { d _ { n } }$ that map our input covariates $X _ { i }$ into increasingly high-dimensional representations as our sample size grows. For example, $\psi$ could consist of a set of polynomial or trigonometric basis functions with increasing numbers of terms. The motivation with this approach is that, once we include enough basis functions, we will be able to accurately represent any reasonable CATE function using this basis, i.e., we have $\tau ( x ) \approx \psi ( x ) \cdot \beta$ for some $\beta \in \mathbb { R } ^ { d _ { n } }$ [Chen, 2007].

The second step in the statistical learning roadmap is to introduce penalization to control the complexity of the learned CATE function because, when $d _ { n }$ is large relative to $n$ , directly running a residual-on-residual regression with covariates $\psi ( x )$ may be unstable. One choice here is to use the lasso penalty [Tibshirani, 1996], which penalizes the sum of the absolute values of $\beta$ :

$$
\hat {\tau} (x) = \psi (x) \cdot \hat {\beta},
$$

$$
\hat {\beta} = \operatorname {a r g m i n} _ {\beta} \left\{\frac {1}{n} \sum_ {i = 1} ^ {n} \hat {\ell} ^ {(- k (i))} \left(X _ {i}, Y _ {i}, W _ {i}; \psi (X _ {i}) \cdot \beta\right) + \lambda \sum_ {j = 1} ^ {q} | \beta_ {j} | \right\}, \tag {4.15}
$$

where $\lambda \geq 0$ is a penalty parameter that controls the complexity of the learned function. A judicious choice of $\lambda$ enables us to still get a good estimate $\hat { \tau } ( x )$ ,

but protects against the risks of overfitting or numerical instability that occur when $\psi ( x )$ is high-dimensional. Using $\lambda = 0$ corresponds to just running linear regression of $Y _ { i }$ on $\psi ( X _ { i } )$ , while in the limit $\lambda \to \infty$ all coefficients $\hat { \beta }$ get pushed to 0. Another simple choice would be to use a ridge penalty, which adds a term $\lambda \sum _ { j = 1 } ^ { q } \beta _ { j } ^ { 2 }$ to the objective.

In order to make (4.15) actionable, we need a data-driven way to choose the tuning parameter $\lambda$ . The simplest way to proceed, is using a validation set, i.e., assuming that we have access to $i = 1$ , . . . , $n _ { v a l }$ independent datapoints that can be used for validation. To choose $\lambda$ , we start running (4.15) for a grid of candidate $\lambda$ values, resulting in a large number of candidate estimates ${ \hat { \tau } } _ { \lambda } ( x )$ . Then, we pick the value of $\lambda$ that minimizes the validation loss,29

$$
\hat {\lambda} = \operatorname {a r g m i n} _ {\lambda} \left\{\frac {1}{n _ {v a l}} \sum_ {\text {v a l i d a t i o n s e t}} \hat {\ell} \left(X _ {i}, Y _ {i}, W _ {i}; \hat {\tau} _ {\lambda} (X _ {i})\right) \right\}, \tag {4.16}
$$

and finally use CATE predictions $\hat { \tau } ( x ) = \hat { \tau } _ { \hat { \lambda } } ( x )$ . Another, similar way of choosing $\lambda$ that does not require access to an independent validation set is to use cross-validation; see Chapter 7 of Hastie, Tibshirani, and Friedman [2009] for details.

The last step in moving from our residual-on-residual regression estimator for semiparametric modeling to a fully flexible non-parametric CATE estimator is to use algorithmic techniques like decision trees, boosting, or neural networks to automate the choice of good basis expansions $\psi ( x )$ . Doing so, however, is beyond the scope of this book; we instead refer to Nie and Wager [2021] for a completion of this discussion. The resulting algorithmic approach is called the R-learner. The causal forest algorithm of Athey, Tibshirani, and Wager [2019] instantiates the R-learner framework using random forests [Breiman, 2001].30 Foster and Syrgkanis [2023] provide general formal results showing that, even after moving to a complex non-parametric setting, the R-learner still maintains robustness properties suggested in Theorem 4.2.

A numerical example We now test out the lasso-based R-learner based approach (4.15), and compare it with a lasso-based T-learner approach (4.3)

![](images/423ff265d3078dfb67b20d7ca5aed58b6f4629a5cc1beb4cab7d0832fdfec636.jpg)  
Figure 4.2: Test set CATE estimates generated via the lasso-based R-learner and T-learner. The true CATE function is shown as a black dashed line. The solid lines trace a smooth average of how the CATE estimates vary with $X _ { 2 } + X _ { 3 }$ .

where both $\hat { \mu } _ { ( 0 ) } ( \cdot )$ and $\hat { \mu } _ { ( 1 ) } ( \cdot )$ are fit with a lasso using predictors $\psi ( X _ { i } )$ . We independently generate $n = 4 , 0 0 0$ samples as follows:

$$
X \sim \mathcal {N} \left(0, I _ {1 0 \times 1 0}\right), W \sim \operatorname {B e r n o u l l i} (e (X)), e (X) = 1 / \left(1 + e ^ {- \left(X _ {2} + X _ {3}\right)}\right)
$$

$$
Y (w) = 2 \log \left(1 + e ^ {X _ {1} + X _ {2} + X _ {3}}\right) + w 1 \left(X _ {2} + X _ {3} \geq 0\right) + \varepsilon , \quad \varepsilon \sim \mathcal {N} (0, 1).
$$

The original covariates are 10-dimensional, but the signal is obviously nonlinear and so simple linear methods would be inappropriate here. To address this challenge, we expand our covariates into a 2555-dimensional basis expansion $\psi ( X _ { i } )$ that includes both non-linearities and interactions between the covariates.31 We then use lasso penalization with a cross-validated choice of $\lambda$ to avoid instability due to our use of a high-dimensional basis expansion.

What‚Äôs challenging about this setting is that units for which $X _ { 2 } + X _ { 3 }$ is large are simultaneously more likely to be treated, have a larger baseline effect whether or not they get treated, and have a larger treatment effect. This type

of situation may arise, e.g., in evaluating educational programs if there exists a class of, say, high-initiative people who are simultaneously more likely to seek out and benefit from the educational resources, but also would have achieved reasonably good outcomes without the resource. In settings like this, in order to avoid regularization-induced confounding, it is important to accurately correct for the correlation between propensity scores and baseline effects.

Results with both the R-learner and T-learner are shown in Figure 4.2. The $y$ -axis of the plot shows CATE estimates $\hat { \tau } ( X _ { i } )$ , while the $x$ -axis shows $X _ { i 2 } + X _ { i 3 }$ . The choice of $x$ -axis reflects that, in reality, we know that the CATE only varies with $X _ { i 2 } + X _ { i 3 }$ . The algorithm, of course, does not know this a-priori‚Äîand this is why the actual CATE estimates $\hat { \tau } ( X _ { i } )$ also depend on other aspects of the covariates (and this manifests itself as apparent noise in the estimates). Here, we see that the R-learner has somewhat noisy estimates, but gets the overall order of magnitude of the CATE right. In contrast, the Tlearner appears to suffer from severe regularization-induced confounding here, and vastly overstates the amount by which $\tau ( X _ { i } )$ grows with $X _ { i 2 } + X _ { i 3 }$ .

# Bibliographic notes

The literature on non-parametric CATE estimation has received a huge amount of attention in recent years. Some proposed methods for CATE estimation are based on specific machine learning methods, e.g., trees [Athey and Imbens, 2016], random forests [Athey, Tibshirani, and Wager, 2019] or Bayesian tree ensembles [Hahn, Murray, and Carvalho, 2020]. Others are more generic, and can be paired with multiple algorithmic approaches. We here discussed the Rlearner [Nie and Wager, 2021]; other generic approaches to CATE estimation include the X-learner [K¬®unzel et al., 2019] and the DR-learner [Kennedy, 2023], and the modified covariate learner [Tian et al., 2014].

One important topic we did not focus in this chapter is what to do after we produce a CATE estimate. After fitting a CATE estimator it is generally good practice to seek to formally validate its output and quantify the strength of heterogeneity; some proposals for how to do so are given in Chernozhukov et al. [2025] and Yadlowsky et al. [2025]. Meanwhile, if the goal of fitting a CATE model was to guide treatment choice, then Proposition 4.1 suggests that empirical thresholding rules of the form $1 \left( \{ \hat { \tau } ( x ) > C \} \right)$ are at least worth considering. Manski [2004], Stoye [2009] and Hirano and Porter [2009] study properties of such thresholding learns under the lens of statistical decision theory. Sun et al. [2021] discuss settings where the treatment cost $C _ { i }$ is random and may also vary with covariates $X _ { i }$ .

In terms of formal results, Kennedy et al. [2024] show that a variant of

the R-learner is minimax for estimating CATEs under a set of smoothness assumptions, while Foster and Syrgkanis [2023] provide guarantees for machine learning with a class of ‚Äúorthogonal‚Äù loss functions that include the $R$ -loss. Zhao, Small, and Ertefaie [2022] consider post-selection inference for the CATE in a high-dimensional linear specification using an algorithm that builds on the semiparametric estimator from Theorem 4.2.

Finally, we also note some work on treatment heterogeneity based on difference conceptual frameworks. Although the ITE is not generally pointidentified, we can still seek bounds or intervals for it. Lei and Cand`es [2021] provide one such method for doing this using conformal inference. Ding, Feller, and Miratrix [2019] study heterogeneous treatment effect estimation in a randomized trial under the strict Neyman model for randomization inference discussed in Chapter 1, and examine what can be said about treatment heterogeneity without making any sampling assumptions on the potential outcomes.

# Chapter 5 Policy Learning

So far, we‚Äôve focused on methods for estimating treatments effects. In many application areas, however, the fundamental goal of performing a causal analysis isn‚Äôt to estimate treatment effects, but rather to guide decision making: We want to understand treatment effects so that we can effectively prescribe treatment and allocate limited resources.

The problem of learning optimal treatment assignment policies is closely related to‚Äîbut subtly different from‚Äîthe problem of estimating treatment heterogeneity. On one hand, policy learning appears easier: All we care about is assigning people to treatment or to control, and we don‚Äôt care about accurately estimating treatment effects beyond that. On the other hand, when learning policies, we need to account for considerations that were not present when simply estimating treatment effects: Any policy we actually want to use must be simple enough we can actually deploy it, cannot discriminate on protected characteristics, should not rely on gameable features, etc.

Policy value For our purposes, a treatment assignment policy $\pi ( x )$ is a mapping $_ { 3 2 }$

$$
\pi : \mathcal {X} \rightarrow \{0, 1 \}, \tag {5.1}
$$

such that individuals with features $X _ { i } = x$ get treated if and only if $\pi ( x ) = 1$ . Under the potential outcome specification, the expected realized outcome when treatment is chosen according to the policy $\pi$ is

$$
V (\pi) = \mathbb {E} \left[ Y _ {i} \left(\pi \left(X _ {i}\right)\right) \right]. \tag {5.2}
$$

We refer to $V ( \pi )$ as the value of the policy $\pi$ , and assume that the decision maker wants to use data to learn a policy $\hat { \pi }$ such that $V ( \hat { \pi } )$ large. This framework relies on an implicit assumption that the outcome $Y _ { i }$ captures the relevant

benefit or reward the decision maker wants to optimize, and that the decision maker is utilitarian in the sense that their objective is to maximize the average reward across units.

Workflow Conceptually, there are three key phases in the policy learning workflow. First, we need to collect data with random or quasi-random treatment assignments $W _ { i }$ to learn a policy $\hat { \pi }$ ; throughout this chapter, we will assume that the treatment in this first stage is unconfounded and that data is drawn as in the basic setting from Chapter 3. In a second (optional) phase, we may want to evaluate the quality of the learned policy, i.e., estimate $V ( \hat { \pi } )$ . This requires a second dataset (often referred to as a test set) with random or quasi-random treatment assignment. Finally, once we‚Äôre done learning, we enter the last phase where we may choose to deploy the learned policy, i.e., we may choose to set $W _ { i } = \hat { \pi } ( X _ { i } )$ with the hope that the expected outcome $\mathbb { E } \left[ Y _ { i } \right]$ obtained via $Y _ { i } = Y _ { i } ( \hat { \pi } ( X _ { i } ) )$ will be large. In this third stage, there is no more randomness in treatment effects, so we cannot (non-parametrically) learn anything about causal effects anymore.

As noted earlier in Proposition 4.1, if we place no restrictions on $\pi$ , then the maximizer of $V ( \pi )$ is the policy that thresholds the CATE:

$$
\pi^ {*} \in \operatorname {a r g m a x} _ {\pi} \left\{V (\pi) \right\}, \quad \pi^ {*} (x) = 1 \left(\{\tau (x) > 0 \}\right). \tag {5.3}
$$

Thus, one possible approach to learning policies is to apply the plug-in principle to (5.3): One can first use methods discussed in the previous chapter to generate an estimate $\hat { \tau } ( \cdot )$ of the CATE, and then set $\hat { \pi } ( x ) ~ = ~ 1 ( \{ \hat { \tau } ( x ) ~ > ~ 0 \} )$ ). This approach may be reasonable in some applications, but may result in policies that are hard to interpret or may not respect other practical constraints that are called for in the application. The focus of this chapter will be on developing methods for learning policies that do respect such constraints; we will present such methods in Section 5.2 after first discussing some preliminaries on policy evaluation below.

Example 5 (Continued). In the previous chapter, we introduced an example from Kitagawa and Tetenov [2018] where the authors seek to target JTPA eligibility based on education and income. The optimal, unrestricted targeting rule would just threshold the CATE. For feasibility reasons, however, they are most interested in linear treatment rules of the form $^ { 3 3 }$

$$
\tau (x) = 1 \left(\left\{\text {p r i o r e a c n i n g s} \cdot \alpha_ {1} + \text {e d u c a t i o n} \cdot \alpha_ {2} > c \right\}\right).
$$

Learning welfare-maximizing rules of this type requires new methods, introduced in this chapter.

# 5.1 Policy evaluation

The key focus of this chapter is on the first ‚Äúlearning‚Äù part of the policy learning workflow, i.e., on how to use data to choose a good policy $\hat { \pi }$ . Methodologically, however, we first need to discuss the second ‚Äúevaluation‚Äù part of the workflow: If someone gives us a policy $\hat { \pi }$ , how can we estimate $V ( \hat { \pi } )$ ?

For the purpose of this section, we will assume that we have access to test set of $n$ samples with unconfounded treatment assignment as in the basic setting from Chapter 3, and that this test set is independent of the data used to learn the candidate policy $\hat { \pi }$ , i.e., the training set. We will then discuss evaluation of $\hat { \pi }$ conditionally on the training set: Here, we are not trying to estimate $\mathbb { E } \left[ V ( \hat { \pi } ) \right]$ (i.e., to integrate over randomness in $\hat { \pi }$ ), but simply to estimate $V ( \hat { \pi } )$ for the specific realization of $\hat { \pi }$ on hand. Because the test set and training sets are independent of each other, this task is equivalent to using the test set to estimate $V ( \pi )$ for an arbitrary fixed policy $\pi$ ; and for simplicity we will present the rest of this section in terms of this latter task.

Inverse-propensity weighting Consider evaluating a given deterministic policy $\pi$ under unconfoundedness. If we further know the treatment propensities $e ( x )$ , then we can obtain a simple estimate of $V ( \pi )$ via inverse-propensity weighting (IPW):

$$
\widehat {V} _ {I P W} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {1 \left(\left\{W _ {i} = \pi \left(X _ {i}\right) \right\}\right) Y _ {i}}{\mathbb {P} \left[ W _ {i} = \pi \left(X _ {i}\right) \mid X _ {i} \right]}, \tag {5.4}
$$

where $\mathbb { P } \left[ W _ { i } = \pi ( X _ { i } ) \big | X _ { i } = x \right] = e ( x )$ when $\pi ( x ) = 1$ and $1 - e ( x )$ else. Qualitatively, this approach averages outcomes across those observations for which the sampled treatment $W _ { i }$ matches the policy prescription $\pi ( X _ { i } )$ , and uses inverse-propensity weighting to account for the fact that some relevant potential outcomes remain unobserved.

When the treatment propensities are known, we can use the same argument as in Theorem 2.2 to check that, for any given policy $\pi$ , the IPW estimate

$\hat { V } _ { I P W } ( \pi )$ is unbiased for $V ( \pi )$

$$
\begin{array}{l} \mathbb {E} \left[ \widehat {V} (\pi) \right] = \mathbb {E} \left[ \frac {1 \left(\left\{W _ {i} = \pi (X _ {i}) \right\}\right) Y _ {i}}{\mathbb {P} \left[ W _ {i} = \pi (X _ {i}) \mid X _ {i} \right]} \right] \\ = \mathbb {E} \left[ \frac {1 \left(\left\{W _ {i} = \pi \left(X _ {i}\right) \right\}\right) Y _ {i} \left(\pi \left(X _ {i}\right)\right)}{\mathbb {P} \left[ W _ {i} = \pi \left(X _ {i}\right) \mid X _ {i} \right]} \right] \tag {5.5} \\ = \mathbb {E} \left[ \mathbb {E} \left[ \frac {1 \left(\left\{W _ {i} = \pi \left(X _ {i}\right) \right\}\right)}{\mathbb {P} \left[ W _ {i} = \pi \left(X _ {i}\right) \mid X _ {i} \right]} \mid X _ {i} \right] \mathbb {E} \left[ Y _ {i} \left(\pi \left(X _ {i}\right)\right) \mid X _ {i} \right] \right] \\ = \mathbb {E} \left[ Y _ {i} \left(\pi \left(X _ {i}\right)\right) \right] = V (\pi), \\ \end{array}
$$

where the second equality follows by consistency of potential outcomes and the third by unconfoundedness.

Augmented IPW In Chapter 3, we discussed how IPW-based estimators for the average treatment effect introduced in Chapter 2 are generally inefficient (at least when run with the true propensity scores) and are not robust to estimation error in $e ( x )$ ; and how the augmented IPW (AIPW) construction can be used to address both of these shortcomings. Similar considerations apply with policy evaluation. For conciseness, we do not repeat the development from Chapter 3 here, and instead simply state the AIPW estimator and its key properties.

As usual, forming the AIPW requires estimates $\hat { \mu } _ { w } ( x )$ for the conditional response functions and ${ \hat { e } } ( x )$ for the propensity score. Given such estimates, the plug-in non-parametric regression estimator for $V ( \pi )$ is obtained by averaging predictions we would get by following the policy $\pi$ , i.e.,

$$
\widehat {V} _ {R E G} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \hat {\mu} _ {\pi (X _ {i})} (X _ {i}). \tag {5.6}
$$

AIPW is obtained by using IPW to debias this estimator by extracting any remaining signal from the regression residuals,

$$
\widehat {V} _ {A I P W} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \hat {\mu} _ {\pi (X _ {i})} (X _ {i}) + \frac {1 \left(\{W _ {i} = \pi (X _ {i}) \}\right)}{\mathbb {P} \left[ W _ {i} = \pi (X _ {i}) \mid X _ {i} \right]} \left(Y _ {i} - \hat {\mu} _ {\pi (X _ {i})} (X _ {i})\right). \tag {5.7}
$$

As always with AIPW-type estimators, cross-fitting is recommended when forming the AIPW estimator. If we use cross-fitting and use estimates for

$\hat { \mu } _ { w } ( x )$ and ${ \hat { e } } ( x )$ that converge at the rates assumed in Theorem 3.2, then

$$
\begin{array}{l} \sqrt {n} \left(\widehat {V} _ {A I P W} (\pi) - V (\pi)\right) \\ \Rightarrow \mathcal {N} \left(0, \operatorname {V a r} \left[ \mu_ {\pi (X _ {i})} (X _ {i}) \right] + \mathbb {E} \left[ \frac {\sigma_ {\pi (X _ {i})} ^ {2} (X _ {i})}{\mathbb {P} \left[ W _ {i} = \pi (X _ {i}) \mid X _ {i} \right]}\right)\right), \tag {5.8} \\ \end{array}
$$

and the AIPW estimator is efficient. The proof of these results exactly mirrors the arguments used in Chapter 3.

Policy comparison It is often of interest to compare two policies $\pi _ { 1 }$ and $\pi _ { 2 }$ by estimating the difference in their values

$$
\Delta \left(\pi_ {1}, \pi_ {2}\right) = V \left(\pi_ {1}\right) - V \left(\pi_ {2}\right). \tag {5.9}
$$

For example, if $\pi _ { 0 }$ is a status-quo treatment-assignment rules, and $\hat { \pi }$ is a new proposed data-driven rule, then the difference $\Delta ( \hat { \pi } , \pi _ { 0 } )$ directly quantifies the benefit of adopting the data-driven rule relative to the status quo.

Given the above discussion, a natural way to estimate the value difference between to policies is to take the difference between their AIPW value estimates. A direct algebraic manipulation can be used to re-express the resulting estimator in condensed form as,

$$
\begin{array}{l} \widehat {\Delta} _ {A I P W} (\pi_ {1}, \pi_ {2}) = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\pi_ {1} (X _ {i}) - \pi_ {2} (X _ {i})\right) \widehat {\Gamma} _ {i}, \\ \widehat {\Gamma} _ {i} = \hat {\mu} _ {(1)} (X _ {i}) - \hat {\mu} _ {(0)} (X _ {i}) + \frac {W _ {i}}{\hat {e} (X _ {i})} \left(Y _ {i} - \hat {\mu} _ {(1)} (X _ {i})\right) \tag {5.10} \\ - \frac {1 - W _ {i}}{1 - \hat {e} (X _ {i})} \left(Y _ {i} - \hat {\mu} _ {(0)} (X _ {i})\right), \\ \end{array}
$$

and under the conditions of Theorem 3.2

$$
\begin{array}{l} \sqrt {n} \left(\widehat {\Delta} _ {A I P W} (\pi_ {1}, \pi_ {2}) - \Delta (\pi_ {1}, \pi_ {2})\right) \\ \Rightarrow \mathcal {N} \left(0, \operatorname {V a r} \left[ \left(\pi_ {1} \left(X _ {i}\right) - \pi_ {2} \left(X _ {i}\right)\right) \tau \left(X _ {i}\right) \right] \right. \tag {5.11} \\ + \mathbb {E} \left[ 1 \left(\left\{\pi_ {1} (X _ {i}) \neq \pi_ {2} (X _ {i}) \right\}\right) \left(\frac {\sigma_ {0} ^ {2} (X _ {i})}{1 - e (X _ {i})} + \frac {\sigma_ {1} ^ {2} (X _ {i})}{e (X _ {i})}\right) \right]. \\ \end{array}
$$

When $\pi _ { 1 }$ and $\pi _ { 2 }$ often agree on the action to take, then $\widehat { \Delta } _ { A I P W } ( \pi _ { 1 } , \pi _ { 2 } )$ only needs to consider outcomes in the smaller region where their recommendations differ‚Äîthus enabling a considerable improvement in precision.

One specific policy contrast that is often of interest is the comparison of a given policy $\pi$ to the never-treat policy. We use short-hand $\Delta ( \pi ) = \Delta ( \pi , 0 )$ for this quantity, and refer to it as the benefit of the policy $\pi$ . We also note that the benefit of the always-treat policy, $\Delta ( 1 )$ , corresponds exactly to the average treatment effect, and as a sanity check we can verify that in this case (5.11) is just a re-statement of the result in Theorem 3.2.

Aside: Treatment prioritization rules One type of policy that often arises in practice is treatment prioritization rules. Such policies start with a priority function $S : \mathcal { X }  \mathbb { R }$ , and then assign treatment to the top $q$ -th fraction of units as ranked by the priority $S ( X _ { i } )$ :

$$
\pi_ {S} ^ {q} = 1 \left(\left\{S \left(X _ {i}\right) \geq F _ {S} ^ {- 1} (1 - q) \right\}\right), \tag {5.12}
$$

where $F _ { S }$ is the the cumulative distribution function of the priorities $S ( X _ { i } )$ . Here, the priority function could be a CATE estimate obtained using a separate training set, a risk measure quantifying who‚Äôs most at risk of a bad outcome without treatment, or some other application-relevant notion of priority.

We can use policy evaluation to quantify the extent to which the priority function succeeds in allocating treatment to those who benefit most from it. The QINI curve estimates the benefit $\Delta ( \pi _ { S } ^ { q } )$ of treating the top $q$ -th fraction of units for different values of $q$ , and then plots $\Delta ( \pi _ { S } ^ { q } )$ on the $Y$ -axis against $q$ on the $X$ -axis. In settings where each unit has a constant cost of treatment, the QINI curve quantifies a cost-benefit exercise where we measure how the obtained benefit changes as we spend more.

Meanwhile, the TOC curve considers $q ^ { - 1 } \Delta ( \pi _ { S } ^ { q } ) - \Delta ( 1 )$ , and plots this quantity against $q$ . This curve quantifies the extent to which the top $q$ -th fraction of units as prioritized by $S ( \cdot )$ benefit more from the treatment than randomly selected units. These quantities are discussed in Yadlowsky et al. [2025]; the paper also advocates considering the area under the TOC curve with units prioritized by estimated CATE as a useful measure of overall detected treatment heterogeneity.

The value of treatment prioritization rules can again be estimated using the doubly robust approach:

$$
\widehat {\Delta} _ {A I P W} \left(\pi_ {S} ^ {q}\right) = \frac {1}{n} \sum_ {k = 1} ^ {\lfloor q n \rfloor} \widehat {\Gamma} _ {i (k)}, S \left(X _ {i (1)}\right) \geq S \left(X _ {i (2)}\right) \geq \ldots \geq S \left(X _ {i (n)}\right). (5. 1 3)
$$

One statistical challenge in studying the large-sample properties of this estimator is that it depends on the empirical $q$ -th quantile of $S ( X _ { i } )$ , which results

in an inflated asymptotic variance relative to (5.8). Yadlowsky et al. [2025] provide a central limit theorem for the value estimate in (5.13) as well as for induced area-under-the-curve metrics for QINI and TOC curve estimates; they also discuss resampling-based methods for these quantities.

# 5.2 Empirical-welfare maximization

We now return to the task of learning a policy, i.e., using experimental or quasiexperimental data to choose a good treatment assignment rule $\hat { \pi } ( \cdot )$ . Throughout, we assume that the policymaker is constrained to choose a policy $\pi$ belonging to some class $\Pi$ of acceptable policies; for example, $\Pi$ may encode restrictions on the functional form the policy is allowed to take or on which variables it is allowed to use. Simple examples of policy classes one might consider include the class of linear thresholding rules $\pi ( x ) = 1 \left( \{ a \cdot x \geq c \} \right)$ for some vector $a$ and threshold $c$ , or the class of fixed-depth decision trees.

Given this setting, the optimal policy‚Äîor policies‚Äîare those that maximize policy value among all acceptable policies:

$$
\pi^ {*} \in \operatorname {a r g m a x} \left\{V \left(\pi^ {\prime}\right): \pi^ {\prime} \in \Pi \right\}. \tag {5.14}
$$

Any non-optimal (but acceptable) policy $\pi$ falls short of this best possible policy value, and suffers regret

$$
R (\pi) = \sup  _ {\pi} \left\{V \left(\pi^ {\prime}\right): \pi^ {\prime} \in \Pi \right\} - V (\pi). \tag {5.15}
$$

Our goal is to learn a policy with guaranteed worst-case bounds on the regret $R ( { \hat { \pi } } )$ . We refer this task as a learning (rather than estimation) task because the performance of $\hat { \pi }$ is only assessed in terms of its regret. No requirements will be made on $\hat { \pi }$ converging to $\pi ^ { * }$ in terms of its functional form (and in fact no assumption is made that there is a unique optimal policy $\pi ^ { * }$ ).

If the optimal policy $\pi ^ { * }$ is a maximizer of the true value function $V ( \pi )$ over $\pi \in \Pi$ , then it is natural to attempt learn $\hat { \pi }$ by maximizing an estimated value function:

$$
\hat {\pi} = \operatorname {a r g m a x} \left\{\widehat {V} (\pi): \pi \in \Pi \right\}. \tag {5.16}
$$

This approach was coined as empirical-welfare maximization by Kitagawa and Tetenov [2018]. In the previous section we already discussed two estimators of $V ( \pi )$ using data with randomized or unconfounded treatment assignment, namely the IPW and AIPW estimators, and both can be used to learn following (5.16). We refer to the maximizer of $\widehat { V } _ { I P W } ( \pi )$ over $\pi \in \Pi$ as $\hat { \pi } _ { I P W }$ , and to the maximizer of $\widehat { V } _ { A I P W } ( \pi )$ as ${ \hat { \pi } } _ { A I P W }$ .

Regret bounds Proving that the empirical-welfare maximization approach achieves low regret is beyond the scope of this book; however, we here sketch the starting point of an argument for doing so. Let $\pi ^ { * }$ be any policy achieving the maximal policy value, and let $\hat { \pi }$ be a maximizer of the estimated value as in (5.16). Then,

$$
\begin{array}{l} R (\hat {\pi}) = V (\pi^ {*}) - V (\hat {\pi}) \\ = V \left(\pi^ {*}\right) - \widehat {V} \left(\pi^ {*}\right) + \widehat {V} \left(\pi^ {*}\right) - \widehat {V} (\widehat {\pi}) + \widehat {V} (\widehat {\pi}) - V (\widehat {\pi}). \tag {5.17} \\ \end{array}
$$

Because $\hat { \pi }$ is a maximizer of the estimated value we have $\widehat { V } \left( \pi ^ { * } \right) - \widehat { V } \left( \widehat { \pi } \right) \leq 0$ , so we can further get

$$
\begin{array}{l} R (\hat {\pi}) \leq V (\pi^ {*}) - \widehat {V} (\pi^ {*}) + \widehat {V} (\hat {\pi}) - V (\hat {\pi}) \\ \leq 2 \sup  \left\{\left| \widehat {V} (\pi) - V (\pi) \right|: \pi \in \Pi \right\}, \tag {5.18} \\ \end{array}
$$

and in particular

$$
\mathbb {E} \left[ R (\hat {\pi}) \right] \leq 2 \mathbb {E} \left[ \sup  \left\{\left| \widehat {V} (\pi) - V (\pi) \right|: \pi \in \Pi \right\} \right]. \tag {5.19}
$$

Thus, proving regret bounds for any empirical-welfare maximization approach reduces to proving uniform bounds on the error of $\widehat V ( \pi )$ that hold simultaneously for all acceptable policies $\pi \in \Pi$ .

One can use tools from empirical process theory to bound the term on the right-hand-side of (5.19); however, doing so relies on technical results beyond the scope of this presentation. To state one concrete version of a result obtained by following this path, let VC(Œ†) denote the Vapnik-Chervonenkis dimension of $1 1$ (in many practical cases, one can essentially think of VC(Œ†) as capturing the number of parameters needed to specify an element of $\Pi$ ), and assume that VC(Œ†) is finite. Then, Athey and Wager [2021] show that‚Äîunder the conditions of Theorem 3.2 along with further regularity conditions‚Äîthe policy learned by maximizing the AIPW value estimate (5.7) satisfies

$$
\lim  \sup  \sqrt {n} \mathbb {E} [ R (\hat {\pi} _ {A I P W}) ]
$$

n

$$
\leq 6 0 \sqrt {\operatorname {V C} (\Pi) \left(\operatorname {V a r} [ \tau (X _ {i}) ] + \mathbb {E} \left[ \frac {\sigma_ {0} ^ {2} (X _ {i})}{1 - e (X _ {i})} + \frac {\sigma_ {1} ^ {2} (X _ {i})}{e (X _ {i})} \right]\right)}. \tag {5.20}
$$

What‚Äôs meaningful about this bound is that it connects how the worst-case regret of empirical-welfare maximization scales with various problem primitives. Specifically, we see that the bound increases with the square root of the

dimension of the $\Pi$ (larger policy spaces are harder to learn over) and the variance of the AIPW scores (learning is harder when ATE estimation is harder), and decreases with the square root of the sample size (more data helps). The constant 60 is likely loose here, though.34

Policy learning as weighted classification The above discussion on regret shows that empirical-welfare maximization is in principle a promising approach to policy learning. However, in order to use this approach in practice, one needs to be able to carry out the optimization problem (5.16) in a computationally tractable manner. This is in general a challenging (non-convex) optimization problem; thankfully, however, it turns out that the empirical-welfare maximization problem is in many cases equivalent to a weighted classification problem, thus allowing us to leverage computational insights from that literature.

Here, we focus on maximizing the AIPW value estimate (5.7). As a first helpful step, we symmetrize the objective by defining

$$
\widehat {A} _ {A I P W} (\pi) = \widehat {V} _ {A I P W} (\pi) - \widehat {V} _ {A I P W} (1 - \pi), \tag {5.21}
$$

i.e., the estimated improvement from following $\pi$ relative to always doing the opposite of $\pi$ . Clearly, $\pi$ is a maximizer of $\widehat { V } _ { A I P W } ( \pi )$ if and only if it is a maximizer of $\widehat { A } _ { A I P W } ( \pi )$ ; thus, we can equivalently write

$$
\hat {\pi} _ {A I P W} = \operatorname {a r g m a x} \left\{\widehat {A} _ {A I P W} (\pi): \pi \in \Pi \right\}. \tag {5.22}
$$

Furthermore, following our discussion on policy comparisons, we can check that

$$
\widehat {A} _ {A I P W} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} (2 \pi (X _ {i}) - 1) \widehat {\Gamma} _ {i}, \tag {5.23}
$$

where $\hat { \Gamma } _ { i }$ is as defined in (5.10).

For the purpose of optimization, the upshot is that we can now re-write our empirical-welfare maximization problem as a weighted classification problem:

$$
\hat {\pi} _ {A I P W} = \operatorname {a r g m a x} \left\{\frac {1}{n} \sum_ {i = 1} ^ {n} \underbrace {(2 \pi (X _ {i}) - 1) \operatorname {s i g n} (\widehat {\Gamma} _ {i})} _ {\text {c l a s s i f i c a t i o n o b j e c t i v e}} \underbrace {\left| \widehat {\Gamma} _ {i} \right|} _ {\text {s a m p l e w e i g h t}}: \pi \in \Pi \right\}. \tag {5.24}
$$

Qualitatively, the intuition here, policy learning is equivalent to trying to choose a policy that matches the sign of the AIPW scores as well as possible, with weight corresponding to the magnitude of the AIPW scores. Practically, this result means that we can use any software package for weighted classification to optimize our target objective and learn ${ \hat { \pi } } _ { A I P W }$ .

The weighted classification formulation (5.24) is valuable from a computational perspective; however, one should be careful not to read into it too much. In typical signal-to-noise regimes, the signs of the AIPW scores $\widehat { \Gamma } _ { i }$ will be fairly random, and actually predicting these signs with any reliability is impossible. Even an optimal policy $\pi ^ { * }$ will make many ‚Äúerrors‚Äù according to the classification formulation; and trying to get high accuracy according to the classification metric will only result in overfitting. It is possible to have problems where empirical-welfare maximization works very well (in terms of improving value relative to a status quo), but where standard classification diagnostics applied to the formulation (5.24) would suggest poor performance.35

The role of the policy class Œ† We started with a non-parametric model (i.e., $\mu _ { ( w ) } ( x )$ and $e ( x )$ can be generic), where the welfare-maximizing unrestricted treatment assignment rule is simply $\pi _ { u n r e s t r } ^ { * } ( x ) = 1 \left( \{ \tau ( x ) > 0 \} \right.$ ). However, our goal in this chapter was not to find a way to approximate $\pi _ { u n r e s t r } ^ { * } ( \cdot )$ ; rather, given a pre-specified class of policies $1 1$ , we sought to learn a nearly regret-optimal policy from $\Pi$ . For example, $\Pi$ could consist of linear decision rules, $k$ -sparse decision rules, depth- $\ell$ decision trees, etc. Note, in particular, that we never assumed that $\pi _ { u n r e s t r } ^ { * } ( \cdot ) \in \Pi$ .

This problem setting may appear surprising at first glance. However, in many applications, it‚Äôs important to consider learning over restricted policy classes. A key reason for this is that, in policy learning problems, the features $X _ { i }$ can play multiple distinct roles. First, the $X _ { i }$ may be needed to achieve unconfoundedness

$$
\left\{Y _ {i} (0), Y _ {i} (1) \right\} \perp W _ {i} \mid X _ {i}.
$$

In general, the more pre-treatment variables we have access to, the more plau-

sible unconfoundedness becomes. In order to have a credible model of nature, it‚Äôs good to have flexible, non-parametric models for $e ( x )$ and $\mu _ { ( w ) } ( x )$ using a wide variety of features.

On the other hand, when we want to deploy a policy $\pi ( \cdot )$ , we should be much more careful about what features we use to make decisions and the form of the policy $\pi ( \cdot )$ . Depending on the application, there may be some features that are required to achieve unconfoundedness, but are problematic when used for treatment choice. This includes features that are difficult to measure in a deployed system, features that are gameable by participants in the system, or features that correspond to legally protected classes. In cases like this, these features need to be kept in the dataset to identify causal effects, but the set $\Pi$ should only contain policies $\pi$ that do not depend on them. Furthermore, many applications involve functional form constraints on $\pi ( \cdot )$ that could reasonably be deployed (e.g., if the policy needs to be communicated to employees in a non-electronic format, or audited using non-quantitative methods). Thus, when learning policies, it‚Äôs important to be able to respond to applicationdriven constraints as codified by the use of a restricted class $\Pi$ of allowable policies.

# Bibliographic notes

The driving idea behind our discussion here was that, when learning policies, the natural quantity to focus on is regret as opposed to, e.g., squared-error loss on the conditional average treatment effect function. This point is argued for in Manski [2004]. Stoye [2009] provides a discussion of exact minimax regret policy learning with discrete covariates, while Hirano and Porter [2009] consider asymptotic analysis in the limits-of-experiments framework.

The insight that policy learning under unconfoundedness can be framed as a weighted classification problem‚Äîand that we can adapt well known result results from empirical risk minimization to derive useful regret bounds‚Äîappears to have been independently discovered in statistics [Zhao et al., 2012], computer science [Swaminathan and Joachims, 2015], and economics [Kitagawa and Tetenov, 2018]. Properties of policy learning with doubly robust scoring rules are derived in Athey and Wager [2021]. The latter paper also considers policy learning in more general settings, such as with ‚Äúnudge‚Äù interventions to continuous treatments or with instruments used to identify the effects of endogenous treatments. Mbakop and Tabord-Meehan [2021] consider model selection for empirical-welfare maximization to handle policy classes with infinite VC dimension, while Zhou, Athey, and Wager [2023] consider structured treatment choice with multiple possible actions.

In this chapter, we‚Äôve discussed rates of convergence that scale as $\sqrt { \mathrm { V C } ( \Pi ) / n }$ . This is the optimal rate of convergence we can get if seek guarantees that are uniform over $\tau ( x )$ ; and the rates are sharp when the strength of the treatment effects decays with sample size at rate $1 / \sqrt { n }$ . However, if we consider asymptotics for fixed choices of $\tau ( x )$ , then super-efficiency phenomena appear and we can obtain faster than $1 / \sqrt { n }$ rates [Luedtke and Chambaz, 2020]; this phenomenon is closely related to ‚Äúlarge margin‚Äù improvements to regret bounds for classification via empirical risk minimization.

QINI curves for evaluating treatment prioritization rules were first introduced in the marketing literature to quantify the value of targeted marketing campaigns. Imai and Li [2023] provide a modern statistical treatment of QINI curves in randomized controlled trial under the Neyman model. Yadlowsky et al. [2025] provide a unified analysis of different methods for evaluating treatment prioritization rules‚Äîincluding both the QINI and TOC curves‚Äîin a general observational study setting that accommodates double machine learning. Sun et al. [2021] use QINI curves to quantify cost-benefit exercises in settings where treatment cost is also unknown and needs to be estimated, while Sverdrup et al. [2025] do so in the case of treatment prioritization rules that allow for multiple actions.

The topic of policy learning is an active area with many recent advances. For example, Bertsimas and Kallus [2020] extend the principle of learning policies by optimizing a problem-specific empirical value function to a wide variety of settings, e.g., inventory management; Luedtke and van der Laan [2016] discuss inference for the value of the optimal policy; while Kallus and Zhou [2021] consider the problem of learning policies in a way that is robust to potential failures of unconfoundedness.

# Chapter 6 Adaptive Experiments

In the previous chapter, we considered policy learning under a two-phase model. In the first ‚Äúexploration‚Äù phase, we had data from an experiment or an observational study that could be used to identify the effect of an intervention and choose a policy. Then, in the second ‚Äúexploitation‚Äù phase, we could deploy the chosen policy‚Äîand reap rewards if we chose well.

This two-phase model, also called the batch learning model in the engineering literature, is attractive for its conceptual and operational simplicity. However, in many settings where units naturally arrive in a stream and there is a cost to experimentation, using a two-phase design with pre-specified exploration and exploitation phases may seem too rigid‚Äîand instead we may want to exploit any knowledge gained during the exploration phase as soon as it‚Äôs available. For example, if at some point in the exploration phase we become confident we‚Äôve already uncovered the best policy for some subgroup of study participants, then why not just immediately use this information instead of waiting for a pre-specified end of the exploration phase? Or, in a multi-armed trial, if it becomes apparent that one of the arms is clearly inferior, why not discard it and re-focus available exploration resources on the other arms?

Example 6. Schwartz, Bradlow, and Fader [2017] describe a setting where a financial institution seeks to acquire new customers via online advertising. The advertiser needs to choose where to advertise (e.g., on which type of websites) and what type of ads to use, and is interested in using experimentation to optimize these choices. The authors show how an adaptive experimentation model enables the advertiser to seamlessly move from exploring to exploiting information about what ads work best during the same campaign, without needing to pre-commit to a rigid experimental sample size up front. One should also note that, in this setting, there‚Äôs less value in having access to standard inferential outputs from a randomized trial (e.g., in terms of confidence intervals and summary statistics), since any learnings would likely be specific to the given advertising campaign and may not generalize to other campaigns.

This chapter provides a brief introduction to the design of adaptive experiments, also known as multi-armed bandit algorithms in the engineering literature. Such experiments enable the researcher to modify their data collection scheme in response to preliminary findings, with the goal improving the quality of the collected data and/or improving the welfare of study participants. A major challenge when working with adaptive experiments is that the samples we‚Äôre using for learning are no longer independent of each other because past outcomes affect future treatment assignments; and thus methods developed for non-adaptive experiments are no longer formally justified (and in fact may fail badly).

Setting and notation As is standard when analyzing multi-armed adaptive experiments, we assume that we have access to a stream of $t = 1$ , , $T$ exper-$\cdot \cdot \cdot$ imental subjects that can each be assigned one among $k = 1$ , . . . , $K$ candidate actions. We write $W _ { t } \in \{ 1 , \ldots , K \}$ for the action taken at time $t$ and $Y _ { t }$ for the observed outcome (or reward), and will consider settings where $W _ { t }$ is a (potentially randomized) function of past data. Following the potential outcomes model, we assume that for each $t$ there are potential outcomes $\{ Y _ { t } ( k ) \} _ { k = 1 } ^ { K }$ such that $Y _ { t } = Y _ { t } ( W _ { t } )$ .

Throughout this chapter, we will also make the following. We have access to a stream of $t = 1$ , . . . , $T$ experimental subjects such that:

‚Ä¢ The potential outcomes are independent and identically distributed across time, i.e., $\{ Y _ { t } ( k ) \} _ { k = 1 } ^ { K } \overset { \mathrm { i i d } } { \sim } P$ for some distribution $P$ that does not depend on $t$ . We write $\mu _ { k } = \mathbb { E } _ { P } \left[ Y _ { t } ( k ) \right]$ for the mean reward of the $k$ -th arm.   
‚Ä¢ There are no covariates $X _ { t }$ that can be used to for targeting, and assigned actions can only depend on past actions and outcomes.

Both of these assumptions can (and often are) relaxed in the literature. There exist algorithms that can handle non-stationary and even non-stochastic potential outcomes, and also algorithms that allow use of covariates for targeting (in the engineering literature this is called the contextual bandit setting); see the bibliographic notes section for references. Here, however, we only have time to briefly scratch the surface of the literature on adaptive experiments‚Äîand will do so in the context of the restricted setting described above.

# 6.1 Low-regret data collection

There are multiple objectives one can target when designing adaptive datacollections algorithms. We will start by considering methods guided by the

simple principle of getting high cumulative rewards (and avoiding low-reward actions) for the t = 1, . . . , $T$ in-sample experimental subjects. The highest possible expected reward one can get using any data collection procedure is $T \mu ^ { * }$ , where $\mu ^ { * } = \operatorname* { m a x } \left\{ \mu _ { k } : 1 \leq k \leq K \right\}$ is the mean reward of the best arm in terms of mean reward. We will assess the quality of an adaptive data-collection procedure in terms of its regret

$$
R _ {T} = \sum_ {t = 1} ^ {T} \left(\mu^ {*} - \mu_ {W _ {t}}\right), \tag {6.1}
$$

which quantifies the shortfall in rewards relative to always playing the best regret scales linearly in arm.36 In a non-adaptive trial where $T$ , i.e., $\begin{array} { r } { { \cal { R } } _ { T } \sim T \sum _ { k = 1 } ^ { K } \left( { \mu ^ { * } } - \mu _ { k } \right) / K } \end{array}$ $W _ { t }$ is uniformly distributed on { }. A first goal of adap- $\{ 1 , \ldots , K \}$ , tive experimentation schemes is to do better, and achieve sub-linear regret. In order to do so, any algorithm will first need to explore the sampling distribution to figure out which arms $k = 1$ , . . . , $K$ are the most promising, and then exploit this knowledge to attain low regret.

The upper confidence band method One notable early solution to the explore-exploit trade-off problem in adaptive experiments in the upper confidence band (UCB) algorithm of Lai and Robbins [1985]. The algorithm proceeds as follows. First, initialize each arm using $t _ { 0 }$ draws and then,

‚Ä¢ At each time $t = K t _ { 0 } + 1$ , $K t _ { 0 } + 2$ , . . ., construct a confidence interval $\widehat { U } _ { k , t }$ for $\mu _ { k }$ based on data collected up to time $t - 1$ , and   
‚Ä¢ Pick action $W _ { t }$ corresponding to the confidence interval $\widehat { U } _ { k , t }$ with the largest upper endpoint, and observe $Y _ { t } = Y _ { t } ( W _ { t } )$ .

At a high level, the motivation behind UCB is that we always want to explore the arm with the most upside, i.e., UCB is optimistic in the face of uncertainty about arm rewards. If we have yet to learn much about a given arm, it will have a long confidence interval and UCB will optimistically sample it more. Over time, however, we‚Äôll collect enough data from the bad arms to be fairly

sure they‚Äôre suboptimal in the sense that even the upper endpoint of their confidence intervals isn‚Äôt competitive with rewards we could get from other arms‚Äîand at that point UCB will stop sampling them.

There are many different variants of UCB considered in practice that arise from different constructions for the confidence interval $\widehat { U } _ { k , t }$ used for arm selection. To get an understanding of why UCB controls regret, we here consider a simple UCB variant tailored to a Gaussian sampling model, i.e.,

$$
Y _ {t} (k) \sim \mathcal {N} (\mu_ {k}, \sigma^ {2}), \tag {6.2}
$$

where $\sigma ^ { 2 }$ is known. The Gaussianity and known $\sigma$ and $T$ assumptions help simplify the analysis; one can get rid of them at the expense of a slightly more delicate algorithm and argument.

We write the cumulative number of times the $k$ -th arm has been drawn and the current running average of rewards from it as

$$
n _ {k, t} = \sum_ {j = 1} ^ {t} 1 (\{W _ {j} = k \}), \quad \hat {\mu} _ {k, t} = \frac {1}{n _ {k , t}} \sum_ {j = 1} ^ {t} 1 (\{W _ {j} = k \}) Y _ {j}, \tag {6.3}
$$

and select actions as

$$
W _ {t} \in \operatorname {a r g m a x} \left\{\widehat {U} _ {k, t} \right\}, \quad \widehat {U} _ {k, t} = \hat {\mu} _ {k, t - 1} + 2 \sigma \sqrt {\log (T) / n _ {k , t - 1}}. \tag {6.4}
$$

This choice is induced by the UCB construction with confidence intervals for $\mu _ { k , t }$ whose width is $\sqrt { 4 \log ( T ) }$ times the standard error of the estimate. The following result shows that this algorithm in-fact achieves low regret with high probability. The variant of UCB considered here was proposed by Auer, Cesa-Bianchi, and Fischer [2002], who refer to this algorithm as the UCB1 algorithm.

Theorem 6.1. Under our sampling assumptions and with Gaussian $^ { 3 7 }$ IID potential outcomes (6.2), UCB with intervals (6.4) and $t _ { 0 } = 1$ initial draws has regret bounded as

$$
R _ {T} \leq 1 6 \sigma^ {2} \log (T) \sum_ {\{k: \mu_ {k} \neq \mu^ {*} \}} \frac {1}{\mu^ {*} - \mu_ {k}} + \sum_ {\{k: \mu_ {k} \neq \mu^ {*} \}} \left(\mu^ {*} - \mu_ {k}\right), \tag {6.5}
$$

with probability at least $1 - K / T$ .

Proof. For simplicity, we assume that there is a unique best arm with $k ^ { * }$ with $\mu _ { k ^ { * } } = \mu ^ { * }$ .38 Under our sampling model, regret $R _ { T }$ can be expressed as

$$
R _ {T} = \sum_ {k \neq k ^ {*}} n _ {k, T} \left(\mu_ {k ^ {*}} - \mu_ {k}\right). \tag {6.6}
$$

Our main task is thus to bound $n _ { k , T }$ , i.e., the number of times UCB may pull any sub-optimal arm; and it turns out that UCB is essentially an algorithm reverse-engineered to make such an argument go through.

To this end, the first thing to check is that, for each arm $k \neq k ^ { * }$ , we have

$$
\hat {\mu} _ {k, t - 1} \leq \mu_ {k} + 2 \sigma \sqrt {\log (T) / n _ {k , t - 1}} \tag {6.7}
$$

for all $t = K + 1 , \ldots$ , $T$ with probability $1 - 1 / T$ . This is true because, writing $\zeta _ { k , j }$ for the $j$ -th time arm $k$ was pulled, we have

$$
\begin{array}{l} \mathbb {P} \left[ \sup  _ {K <   t \leq T} \left\{\hat {\mu} _ {k, t - 1} - \mu_ {k} - 2 \sigma \sqrt {\log (T) / n _ {k , t - 1}} \right\} \geq 0 \right] \\ \leq \mathbb {P} \left[ \sup  _ {1 \leq j \leq n _ {k, T}} \left\{\hat {\mu} _ {k, \zeta_ {k, j}} - \mu_ {k} - 2 \sigma \sqrt {\log (T) / j} \right\} \geq 0 \right] \\ = \mathbb {P} \left[ \sup  _ {1 \leq j \leq n _ {k, T}} \left\{\frac {1}{j} \sum_ {l = 1} ^ {j} Y _ {l} ^ {\prime} (k) - \mu_ {k} - 2 \sigma \sqrt {\log (T) / j} \right\} \geq 0 \right] \\ \leq \mathbb {P} \left[ \sup  _ {1 \leq j \leq T} \left\{\frac {1}{j} \sum_ {l = 1} ^ {j} Y _ {l} ^ {\prime} (k) - \mu_ {k} - 2 \sigma \sqrt {\log (T) / j} \right\} \geq 0 \right] \\ \leq T \exp (- 2 \log (T)) = 1 / T, \\ \end{array}
$$

where the equality follows by stationarity of the data-generating process (here, $Y _ { l } ^ { \prime } ( k )$ are independent draws from $\mathcal { N } ( \mu _ { k } , \sigma ^ { 2 } ) )$ , and the last line is an application of a sub-Gaussian tail bound with a union bound. By a repeat of the same argument and another union bound we see that with probability at least $1 - K / T$ ,

$$
\mu_ {k ^ {*}} \leq \hat {\mu} _ {k ^ {*}, t - 1} + 2 \sigma \sqrt {\log (T) / n _ {k ^ {*} , t - 1}} \tag {6.8}
$$

for all $t = K + 1 , \ldots , T$ , and (6.7) holds simultaneously for all $k \neq k ^ { * }$ .

When (6.7) and (6.8) hold, we can only pull any sub-optimal arm $k \neq k ^ { * }$ under the following (necessary but not sufficient) conditions:

$$
\begin{array}{l} W _ {t} = k \Rightarrow \hat {\mu} _ {k, t - 1} + 2 \sigma \sqrt {\log (T) / n _ {k , t - 1}} \geq \hat {\mu} _ {k ^ {*}, t - 1} + 2 \sigma \sqrt {\log (T) / n _ {k ^ {*} , t - 1}} \\ \Rightarrow \hat {\mu} _ {k, t - 1} + 2 \sigma \sqrt {\log (T) / n _ {k , t - 1}} \geq \mu_ {k ^ {*}} \\ \Longrightarrow \mu_ {k} + 4 \sigma \sqrt {\log (T) / n _ {k , t - 1}} \geq \mu_ {k ^ {*}} \\ \Rightarrow n _ {k, t - 1} \leq 1 6 \sigma^ {2} \log (T) / \left(\mu_ {k ^ {*}} - \mu_ {k}\right) ^ {2}. \\ \end{array}
$$

Thus, when (6.7) and (6.8) hold, pulling the $k$ -th arm for some $k \neq k ^ { * }$ simply becomes impossible once $n _ { k , t - 1 }$ passes a certain cutoff, and so

$$
n _ {k, T} \leq 1 6 \sigma^ {2} \log (T) / \left(\mu_ {k ^ {*}} - \mu_ {k}\right) ^ {2} + 1.
$$

Plugging this into the regret expression (6.6), we obtain (6.5).

![](images/e561b3d69d61cf2a8abbbc83d3e7879b77d0892e42e1da6bcb0cac29acf35be6.jpg)

Theorem 6.1 immediately implies that UCB in fact succeeds in finding and effectively retiring sub-optimal arms reasonably fast, thus resulting in regret that only scales logarithmically in $T$ . Interestingly, the dominant term in (6.5) is due to ‚Äúgood‚Äù arms for which $\mu ^ { * } - \mu _ { k }$ is small; intuitively, the reason these arms are difficult to work with is that it takes longer to be sure that they‚Äôre sub-optimal. This implies that the cost of including some very bad arms in an adaptive experiment may be limited, since an algorithm like UCB will be able to discard them quickly.

Finally, one should note that the upper bound (6.5) appears to allow for unbounded regret due to quasi-optimal arms for which $\mu _ {  { k ^ { * } } } - \mu _ {  { k } }$ is very small. This is simply an artifact of the proof strategy that focused on the case where effects are strong. When effects may be weak, one can simply note that the worst-case regret due to any given arm $k$ is upper bounded by $T \left( \mu _ { k ^ { * } } - \mu _ { k } \right)$ ; and, combining this bound with the bound implied by (6.5), we find that the worst-case regret for any combination of arms $\mu _ { k }$ is bounded on the order of $K \sqrt { T \log ( T ) }$ .

Thompson sampling UCB is a simple approach to adaptive experimentation with strong bounds on excess regret from sampling sub-optimal arms. However, the algorithm is sensitive to a number of seemingly ad-hoc choices that are more tied to proof strategies than transparent methodological considerations, and this can lead to suboptimal performance in practice. For example, the version of the UCB algorithm given above uses relatively wide confidence intervals with a half-length of $2 \sqrt { \log ( T ) }$ standard errors; and so qualitatively,

if we understand UCB as always choosing the arm with the most upside, then this version of UCB is extremely optimistic in assessing upside. What would happen if we ran UCB with intervals with a half-length of 1.96 standard errors instead, i.e., with a more conventional amount of optimism regarding the upside from each arm? In practice, this might (and often does) work well (perhaps even better), but the proof of Theorem 6.1 would no longer go through (because the events (6.7) and (6.8) hold would no longer uniformly hold across all time with high probability).

Current empirical practice suggests that we can side-step this brittleness of UCB by using algorithms that are still driven by the general principle of optimism in the face of uncertainty, but that operationalize their optimism in terms of Bayesian rather than frequentist reasoning. Thompson sampling [Thompson, 1933] is one example of a simple and widely used algorithm that does so. To implement this algorithm, we start by picking a prior $\Pi _ { 0 }$ for the potential outcome distribution $P$ . Then, for each time $t = 1$ , . . . , $T$ , we

‚Ä¢ Compute probabilities $e _ { k , t - 1 }$ that each arm $k$ is the best arm, i.e.,

$$
e _ {k, t - 1} = \mathbb {P} _ {\Pi_ {t - 1}} [ \mu_ {k} = \mu_ {*} ], \tag {6.9}
$$

‚Ä¢ Randomly choose an action $W _ { t } \sim$ Multinomial(e¬∑,t‚àí1), and   
‚Ä¢ Observe $Y _ { t } = Y _ { t } ( W _ { t } )$ and update the posterior $\Pi _ { t }$

One can efficiently implement this algorithm via posterior sampling: First draw a joint sample $( \mu _ { 1 } ^ { \prime } , . . . , \mu _ { K } ^ { \prime } ) \sim \Pi _ { t - 1 }$ , and then set $W _ { t } = \mathrm { a r g m a x } \left. \mu _ { k } ^ { \prime } \right.$ .

Although Thompson sampling looks superficially very different from UCB, it ends up having a similar statistical intuition behind it. Just like UCB, Thompson sampling regularly explores every arm until it becomes effectively sure that the arm is not good (i.e., the posterior probability of the arm being best drops below $1 / T$ ); and intuition from, say, the Bernstein‚Äìvon Mises theorem suggests that this should happen with roughly the same amount of information as when the upper confidence band of an arm falls below the whole confidence interval of some better arm. Proving an analogue to Theorem 6.1 is however beyond the scope of this presentation, and we instead refer to Agrawal and Goyal [2017] for such a result.

From a practical perspective, Thompson sampling presents a number of advantages relative to UCB. Thompson sampling is less sensitive to implementation choices than UCB; in fact, if one is willing to initialize the algorithm by taking 1 draw from each arm, then one can run Thompson sampling with $\Pi _ { 0 }$ set to be an improper flat prior over the real line, resulting in an algorithm with

no tuning parameters.39 And, in empirical evaluations, Thompson sampling often proves itself more resilient than UCB and related algorithms [Chapelle and Li, 2011, Wu and Wager, 2022].

# 6.2 Inference after adaptive data collection

After collecting data in an adaptive trial, it may also be of interest to perform statistical inference and, e.g., give confidence intervals for the mean arm reward parameters $\mu _ { k }$ . Doing so, however, requires caution as adaptive data collection yields non-IID data and can thus void guarantees for standard approaches to inference. For example, in the case of estimating $\mu _ { k }$ , two natural estimators that immediately come to mind include the sample mean

$$
\hat {\mu} _ {k} ^ {A V G} = \hat {\mu} _ {k, T} = \frac {1}{n _ {k , T}} \sum_ {j = 1} ^ {t} 1 \left(\left\{W _ {j} = k \right\}\right) Y _ {j} \tag {6.10}
$$

and, in the case of Thompson sampling, the inverse-propensity weighted estimator

$$
\hat {\mu} _ {k} ^ {I P W} = \frac {1}{T} \sum_ {t = 1} ^ {T} \frac {1 (\{W _ {t} = k \}) Y _ {t}}{e _ {t , k}}. \tag {6.11}
$$

However, due to the adaptive data-collection scheme, neither of these estimators has an asymptotically normal limiting distribution, thus hindering their use for making confidence intervals.

The following simple example illustrates the failure of the classical central limit theorem when working with adaptively collected data:

‚Ä¢ We can sample outcome $Y _ { t } \sim \mathcal { N } \left( \mu , 1 \right)$ for a single arm with unknown mean $\mu$ .   
‚Ä¢ We first run a pilot study on $n _ { 0 }$ samples and say that the pilot study passed if the sample average of the first $n _ { 0 }$ samples is positive (and that it failed else).   
‚Ä¢ If the pilot study passed, we collect a further $1 0 n _ { 0 }$ samples, whereas if it failed we only collect $n _ { 0 }$ further samples.

![](images/4d086c1eba902f46258de500200546cf165881b713544da77f5cdb4679fa505d.jpg)  
¬µÀÜAV G  fail pilot study   
+

![](images/59bfca638b6b322f7ed5513d478ca54f86f2cb3b022a7e4a4e824d8e2385eeeb.jpg)  
$\hat { \mu } ^ { A V G }$ pass pilot study

![](images/4ca24c692ea249e3006ecd34da1c21982d2270da5b78f79b31a4bc0ecf5b9e50.jpg)  
  
unconditional distribution of $\hat { \mu } ^ { A V G }$   
Figure 6.1: Scaled distribution of $\hat { \mu } ^ { A V G }$ conditionally on passing/failing the pilot study, and unconditionally (i.e., distribution of $\sqrt { n _ { 0 } } \hat { \mu } ^ { A V G } )$ , when $\mu = 0$ .

This example is intended to capture, using a simple one-arm design, the qualitative behavior of Thompson sampling whereby the higher the current sample average of an arm the more likely we are to draw from it. Figure 6.1 displays the scaled distribution of the resulting sample average when $\mu = 0$ . We readily see that the scaled distribution of $\hat { \mu } ^ { A V G }$ is both non-Gaussian and biased downwards, and so normal confidence intervals centered at $\hat { \mu } ^ { A V G }$ would not be valid here. Nie et al. [2018] provide a general result showing that sample averages for regret-minimizing algorithms are biased downwards in considerable generality.

Meanwhile, $\hat { \mu } ^ { I P W }$ is unbiased when available (e.g., with Thompson sampling). However, as discussed in Hadad et al. [2021], it still has a non-Gaussian‚Äî and often heavy-tailed‚Äîsampling distribution. Thus, it again cannot be used for normal inference.

The topic how best to do inference with adaptively collected collected data

is still an active research topic, and a comprehensive review of the literature is beyond the scope of this presentation. However, as a pointer to available solutions, we here show how careful re-weighting of the data can avoid the non-Gaussianity issues with $\hat { \mu } ^ { A V G }$ and $\hat { \mu } ^ { I P W }$ .

Consider a sequentially randomized experiment, where the treatment probabilities $e _ { t }$ can depend on past data; Thompson sampling is an example of a sequentially randomized experiment. Then, we define the adaptively weighted estimate of $\mu _ { k }$ as

$$
\hat {\mu} _ {k} ^ {A W} = \sum_ {t = 1} ^ {T} \frac {1 (\{W _ {t} = k \}) Y _ {t}}{\sqrt {e _ {t , k}}} \Bigg / \sum_ {t = 1} ^ {T} \frac {1 (\{W _ {t} = k \})}{\sqrt {e _ {t , k}}}. (6. 1 2)
$$

The specification of this estimator may appear surprising, as units are weighted by $1 / \sqrt { e _ { t , k } }$ rather than the more familiar $1 / e _ { t , k }$ inverse-propensity weights. However, as shown below, this weighting scheme yields an asymptotic normality result. We note that the regularity condition (6.14) reduces to the familiar Lindeberg condition in the case of randomized trials with constant treatment propensities; this condition is weak provided the $e _ { t , k }$ cannot decay too fast.

Theorem 6.2. In a sequentially randomized experiment with IID potential outcomes, suppose that

$$
0 <   \sigma_ {k} ^ {2} := \operatorname {V a r} \left[ Y _ {t} (k) \right] <   \infty \tag {6.13}
$$

for all arms $k = 1$ , . . . , $K$ , that $e _ { t , k } > 0$ almost surely $^ { 4 0 }$ and that, for all $\varepsilon > 0$ ,

$$
\lim  _ {T \rightarrow \infty} \frac {1}{T} \sum_ {t = 1} ^ {T} \mathbb {E} \left[\left(Y _ {t} - \mu_ {k}\right) ^ {2} \mathbf {1} \left(\left\{\left(Y _ {t} - \mu_ {k}\right) ^ {2} \geq \varepsilon e _ {t, k} T \right\}\right) \mid \mathcal {F} _ {t - 1} \right] = 0, \tag {6.14}
$$

where $\mathcal { F } _ { t - 1 }$ denotes information collected up to time t ‚àí 1. Then,

$$
\widehat {V} _ {k} ^ {- 1 / 2} \left(\widehat {\mu} _ {k} ^ {A W} - \mu_ {k}\right) \Rightarrow \mathcal {N} (0, 1),
$$

$$
\widehat {V} _ {k} = \sum_ {t = 1} ^ {T} \left(\frac {1 \left(\left\{W _ {t} = k \right\}\right) \left(Y _ {t} - \hat {\mu} _ {k} ^ {A W}\right)}{\sqrt {\mathcal {E} _ {t , k}}}\right) ^ {2} / \left(\sum_ {t = 1} ^ {T} \frac {1 \left(\left\{W _ {t} = k \right\}\right)}{\sqrt {\mathcal {E} _ {t , k}}}\right) ^ {2}. \tag {6.15}
$$

Proof. We start by stating a technical result, the proof of which is deferred to the end of this section: Under (6.13) and (6.14),

$$
\sum_ {t = 1} ^ {T} \frac {1 (\{W _ {t} = k \})}{\sqrt {e _ {t , k}}} / \sqrt {T} \rightarrow_ {p} \infty , \tag {6.16}
$$

i.e., the denominator in (6.12) grows faster than $\sqrt { T }$ . Qualitatively, (6.16) means that our adaptive sampling scheme collects an increasing amount of data over time under the adaptive weighting scheme used in (6.12).

Now, to obtain a central limit theorem, we note that

$$
\hat {\mu} _ {k} ^ {A W} - \mu_ {k} = \sum_ {t = 1} ^ {T} \frac {1 (\{W _ {t} = k \}) (Y _ {t} - \mu_ {k})}{\sqrt {e _ {t , k}}} / \sum_ {t = 1} ^ {T} \frac {1 (\{W _ {t} = k \})}{\sqrt {e _ {t , k}}}, \tag {6.17}
$$

and start by focusing on the numerator of the above expression. Let

$$
M _ {t} = \sum_ {j = 1} ^ {t} \frac {1 \left(\left\{W _ {j} = k \right\}\right) \left(Y _ {j} - \mu_ {k}\right)}{\sqrt {e _ {j , k}}} \tag {6.18}
$$

be its partial sum. Because $W _ { t }$ is randomly chosen given information up to time $t$ , we see that $W _ { t }$ is independent of $Y _ { t } ( k )$ conditionally on information collected up to time $t - 1$ , and thus $M _ { t }$ is a martingale:

$$
\mathbb {E} \left[ M _ {t} \mid \mathcal {F} _ {t - 1} \right] = M _ {t - 1}. \tag {6.19}
$$

Furthermore, thanks to our weighting scheme, we can check that the conditional variance of each martingale step is non-random despite our use of adaptive sampling probabilities:

$$
\operatorname {V a r} \left[ M _ {t} \mid \mathcal {F} _ {t - 1} \right] = \sigma_ {k} ^ {2}. \tag {6.20}
$$

Given these two facts, the martingale central limit theorem [Helland, 1982, Theorem 2.5(a)] implies that

$$
M _ {T} / \sqrt {T \sigma_ {k} ^ {2}} \Rightarrow \mathcal {N} (0, 1) \tag {6.21}
$$

whenever

$$
\lim _ {T \to \infty} \frac {1}{T} \sum_ {t = 1} ^ {T} \mathbb {E} \left[ (M _ {t} - M _ {t - 1}) ^ {2} 1 \left(\left\{(M _ {t} - M _ {t - 1}) ^ {2} > \varepsilon T \right\}\right) \mid \mathcal {F} _ {t - 1} \right] = 0 \quad (6. 2 2)
$$

for all $\varepsilon > 0$ . In our setting

$$
\begin{array}{l} \mathbb {E} \left[ \frac {1 \left(\left\{W _ {j} = k \right\}\right) \left(Y _ {j} - \mu_ {k}\right) ^ {2}}{e _ {j , k}} 1 \left(\left\{\frac {1 \left(\left\{W _ {j} = k \right\}\right) \left(Y _ {j} - \mu_ {k}\right) ^ {2}}{e _ {j , k}} > \varepsilon T \right\}\right) \mid \mathcal {F} _ {t - 1} \right] \\ = \mathbb {E} \left[ \frac {1 (\{W _ {j} = k \}) (Y _ {j} - \mu_ {k}) ^ {2}}{e _ {j , k}} 1 \left(\left\{\frac {(Y _ {j} - \mu_ {k}) ^ {2}}{e _ {j , k}} > \varepsilon T \right\}\right) | \mathcal {F} _ {t - 1} \right] \\ = \mathbb {E} \left[ (Y _ {j} - \mu_ {k}) ^ {2} 1 \left(\left\{(Y _ {j} - \mu_ {k}) ^ {2} > \varepsilon e _ {j, k} T \right\}\right) \mid \mathcal {F} _ {t - 1} \right], \\ \end{array}
$$

meaning that (6.14) is equivalent to (6.22) and thus (6.21) holds.

We are now ready to wrap up. We first note that $\hat { \mu } _ { k } ^ { A W }$ is consistent for $\mu _ { k }$ thanks to (6.16) and (6.21). Meanwhile, under (6.14), we also have that

$$
\sum_ {t = 1} ^ {T} \left(\frac {1 \left(\left\{W _ {t} = k \right\}\right)\left(Y _ {t} - \mu_ {k}\right)}{\sqrt {e _ {t , k}}}\right) ^ {2} / \left(T \sigma_ {k} ^ {2}\right)\rightarrow_ {p} 1 \tag {6.23}
$$

by martingale concentration [Helland, 1982, Lemma 2.3]; the same holds with $\mu _ { k }$ replaced with $\hat { \mu } _ { k } ^ { A W }$ by consistency. Thus, by (6.21) and Slutsky‚Äôs lemma,

$$
M _ {T} / \sqrt {\sum_ {t = 1} ^ {T} \left(\frac {1 \left(\left\{W _ {t} = k \right\}\right) \left(Y _ {t} - \hat {\mu} _ {k} ^ {A W}\right)}{\sqrt {e _ {t , k}}}\right) ^ {2}} \Rightarrow \mathcal {N} (0, 1). \tag {6.24}
$$

Finally (6.15) follows because denominators in $\hat { \mu } _ { k } ^ { A W }$ and $\widehat { V } _ { k } ^ { 1 / 2 }$ cancel out.

The proof of Theorem 6.2 reveals why the adaptively weighted estimator $\hat { \mu } _ { k } ^ { A W }$ has a normal limiting distribution whereas the estimators $\hat { \mu } _ { k } ^ { A V G }$ or ÀÜ¬µIPk $\hat { \mu } _ { k } ^ { I P W }$ IPW may not. The weighting scheme for the adaptively weighted estimator was essentially reverse-engineered for the predictable variance condition (6.20) to go through and thus enable application of a martingale central limit theorem. The estimators $\hat { \mu } _ { k } ^ { A V G }$ 1 or $\hat { \mu } _ { k } ^ { I P W }$ do not in general have this property in adaptive experiments. Hadad et al. [2021] refer to weights that allow for application of a martingale central limit theorem as ‚Äúvariance stabilizing‚Äù, and study a family of variance stabilized estimators that include $\hat { \mu } _ { k } ^ { A W }$ as a special case.

Proof of (6.16). It now remains to establish the remaining technical claim in the proof of Theorem 6.2. Our first task is to check that

$$
E _ {T, k} / \sqrt {T} \rightarrow_ {p} \infty , \quad E _ {T, k} = \sum_ {t = 1} ^ {T} \sqrt {e _ {t , k}}. \tag {6.25}
$$

Under (6.13), we can choose an $\alpha _ { k } > 0$ be such that

$$
\mathbb {E} \left[ (Y _ {t} - \mu_ {k}) ^ {2} \mathbf {1} \left(\left\{(Y _ {t} - \mu_ {k}) ^ {2} \geq \alpha_ {k} \right\}\right) \right] \geq \frac {\sigma_ {k} ^ {2}}{2}.
$$

Then, by repeatedly applying Markov‚Äôs inequality conditionally on past data, we see that the key sum in (6.14) can be bounded from below as

$$
\begin{array}{l} \frac {1}{T} \sum_ {t = 1} ^ {T} \mathbb {E} \left[ (Y _ {t} - \mu_ {k}) ^ {2} \mathbf {1} \left(\left\{(Y _ {t} - \mu_ {k}) ^ {2} \geq \varepsilon e _ {t, k} T \right\}\right) \mid \mathcal {F} _ {t - 1} \right] \\ \geq \frac {\sigma_ {k} ^ {2}}{2} \frac {1}{T} \sum_ {t = 1} ^ {T} 1 \left(\left\{\varepsilon e _ {t, k} T \leq \alpha_ {k} \right\}\right) \geq \frac {\sigma_ {k} ^ {2}}{2} \frac {1}{T} \sum_ {t = 1} ^ {T} 1 \left(\left\{\sqrt {e _ {t , k}} \leq \sqrt {\alpha_ {k} / (\varepsilon T)} \right\}\right). \\ \end{array}
$$

By (6.14), this expression must converge to 0 in probability for every $\varepsilon > 0$ . Thus, for any $\varepsilon > 0$ , we have $\sqrt { e _ { t , k } } \ge \sqrt { \alpha _ { k } / ( \varepsilon T ) }$ for all but a vanishing fraction of units with high probability, and so (6.25) must hold.

For our next step, we form another $\mathcal { F } _ { t }$ -martingale $X _ { t }$ with differences

$$
X _ {t} - X _ {t - 1} = \sqrt {e _ {t , k}} - \frac {1 (\{W _ {t} = k \})}{\sqrt {e _ {t , k}}}.
$$

This martingale has increments bounded from above, $X _ { t } ~ - ~ X _ { t - t } ~ \le ~ 1$ , and variance increments Var $\left\lfloor X _ { t } \right\rfloor \mathcal { F } _ { t - 1 } \rfloor = 1 - e _ { t , k } \leq 1$ . Freedman [1975, Theorem 4.1] then shows that, for any $a > 0$ ,

$$
\mathbb {P} \left[ X _ {T} \geq a \right] \leq \exp \left[ - \frac {a ^ {2}}{2 (a + T)} \right]. \tag {6.26}
$$

Now, given (6.25), we know that there exists a function $r ( T )$ such that $r ( T ) $ $\infty$ and $\mathbb { P } [ E _ { T , k } / ( 2 r ( T ) \sqrt { T } ) ] \to 1$ . Plugging $a = r ( T ) \sqrt { T }$ into the above expression, we then get

$$
\lim _ {T \to \infty} \mathbb {P} \left[ \sum_ {t = 1} ^ {T} \frac {1 (\{W _ {t} = k \})}{\sqrt {e _ {t , k}}} \leq \sum_ {t = 1} ^ {T} \sqrt {e _ {t , k}} - r (T) \sqrt {T} \right] = 0,
$$

which, because $E _ { T , k } \ge 2 r ( T ) \sqrt { T }$ with high probability, implies (6.16).

Trade-offs in adaptive study design In this chapter, we have considered two high-level questions pertaining to adaptive experiments. First, we asked how to collect data such as to minimize in-sample regret; and then we asked how to build confidence intervals for mean arm rewards using adaptively collected

data. Given this background, it‚Äôs natural to ask whether it‚Äôs possible to align these two tasks‚Äîand simultaneously achieve low in-sample regret and powerful post-experiment inference.

Here, however, the answer is unfortunately an unequivocal no: Data collection schemes that aggressively optimize for in-sample regret as in (6.1) will result in fragile post-experiment inference. Bubeck, Munos, and Stoltz [2009] provide a formal trade-off in terms of the in-sample regret achieved using a data-collection scheme, and the post-experiment regret one could get by deploying the best arm from the experiment on future data. Qin and Russo [2024] derive the precise Pareto frontier for this trade-off under large-deviation asymptotics, and provide a class of algorithms that achieve this frontier. Meanwhile, Fan and Glynn [2025] show that any adaptive algorithm that achieves optimal in-sample expected regret will necessarily have a heavy-tailed regret distribution (i.e., the algorithm has a small but non-negligible probability of failing completely and incurring large regret). Finally, on a technical note, algorithms that aggressively taper propensities $e _ { t , k }$ for poorly performing arms are likely to not satisfy the Lindeberg condition (6.14), and thus may not allow for valid post-experiment inference via the proposed method.

There are thus unavoidable trade-offs in the design of adaptive experiments, and researchers should choose relevant data-collection strategies based on their goals. If the goal is to quickly roll out a policy and to immediately minimize insample regret for study participants, then algorithms like Thompson sampling provide a natural choice. If, however, a researcher also wants to use the collected data to guide future policy, then using algorithms that are less aggressive in how fast they taper the use of suboptimal arms is preferable [Bubeck et al., 2009, Fan and Glynn, 2025]. We also note a large literature on adaptive experiments that maximize our chance of identifying either the best arm [Russo, 2020] or a quasi-optimal arm [Kasy and Sautmann, 2021] after $T$ time-steps.

# Bibliographic notes

This line of work on bandit algorithms builds on early results from Lai and Robbins [1985] on the UCB algorithm. Lai and Robbins [1985] showed that a variant of UCB achieves regret scaling of the form (6.5), and that this behavior is asymptotically optimal. Finite-sample bounds of the type given in Theorem 6.1 are established in Auer, Cesa-Bianchi, and Fischer [2002], while Agrawal and Goyal [2017] provide analogous bounds for Thompson sampling. Thanks to its Bayesian specification, Thompson sampling can be generalized to a wide variety of adaptive learning problems; see Russo et al. [2018] for a recent survey. We also note that UCB and Thompson sampling are by far not the

only available algorithms for this task; for example, Russo and Van Roy [2018] propose information-directed sampling, another Bayesian heuristic which they argue presents an attractive alternative to Thompson sampling.

In Section 6.1, we considered adaptive experiments that can quickly converge on sampling the best of $K$ available actions. The econometric setting we used made 3 major assumptions that may not hold in applications: We did not consider covariates $X _ { t }$ that can be used to guide decision making; we only considered in-sample regret as an objective; and we assumed that the sampling distribution is stable over time. Each of these assumptions has been relaxed in the literature. The literature on contextual bandits allows linking potential outcomes with covariates $X _ { t }$ via either a parametric [Bastani and Bayati, 2020, Goldenshluger and Zeevi, 2013] or non-parametric [Gur, Momeni, and Wager, 2022, Hu, Kallus, and Mao, 2022a, Perchet and Rigollet, 2013] specification. The literature on best-arm selection was already discussed above [Bubeck et al., 2009, Kasy and Sautmann, 2021, Russo, 2020]. Finally, Besbes, Gur, and Zeevi [2019], Liu, Van Roy, and Xu [2023] and Qin and Russo [2025] consider different models for how the reward distribution may change over time, and propose algorithms tailored to this setting. There is also a large literature on the adversarial model where, by analogy to the Neyman model, no sampling assumptions are made on the potential outcomes and the only source of randomness is in randomized action choice; see Bubeck and Cesa-Bianchi [2012] for a review and references.

The line of work on inference with adaptively collected data via variancestabilizing weighting is pursued by a number of authors including Luedtke and van der Laan [2016], Hadad et al. [2021] and Zhang, Janson, and Murphy [2020]. One should note that this is not the only possible approach to inference in adaptive experiments. In particular, a classical alternative to inference in this setting starts from confidence-bands based on the law of the iterated logarithm and its generalizations that hold simultaneously for every value of $t$ ; see Robbins [1970] for a landmark survey and Howard et al. [2021] for recent advances. One can also build confidence intervals using diffusion approximations for adaptive experiments motivated by weak-signal asymptotics [Hirano and Porter, 2023, Kuang and Wager, 2024].

Finally, all approaches to adaptive experimentation discussed above are essentially heuristic algorithms that have good asymptotic behavior (i.e., neither UCB nor Thompson sampling can be derived directly from an optimality principle). In the Bayesian case (i.e., with an actual subjective prior for $P$ rather than just a convenience prior as used by Thompson sampling to power an algorithm with frequentist guarantees), one can solve for the optimal regretminimizing experimental design via dynamic programming [Gittins, 1979].

# Chapter 7 Balancing Estimators

The propensity score has played a central role in our presentation so far, including in understanding identification of average treatment effects under unconfoundedness, construction of efficient estimators of the average treatment effect, and the design of adaptive experiments. However, although this presentation makes it clear that the propensity score is important for causal inference, it may still remain somewhat unclear why this is true.

Here, we will re-visit the propensity score as a statistical object, and argue that a key function of the propensity score is to balance out‚Äîand thus eliminate bias captured by‚Äîobserved pre-treatment confounders. This perspective will motivate the development of new propensity score estimators with better end-to-end behavior when used for treatment effect estimation, and elucidate connections between methods for average treatment effect estimation under unconfoundedness and the broader literature on non-parametric and/or highdimensional inference. Note that this chapter will not consider any new tasks in causal inference‚Äîrather, we will focus on the problem of average treatment effect estimation under unconfoundedness and revisit the statistical principles underlying the task. As such, this chapter may be skipped on a first reading.

The role of balance Working under our familiar basic unconfoundedness setting from Chapter 3, recall the (oracle) inverse-propensity weighted (IPW) estimator of the average treatment effect (ATE):

$$
\hat {\tau} _ {I P W} ^ {*} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i} Y _ {i}}{e (X _ {i})} - \frac {(1 - W _ {i}) Y _ {i}}{1 - e (X _ {i})}\right), \quad e (x) = \mathbb {P} \left[ W _ {i} = 1 \mid X _ {i} = x \right]. \tag {7.1}
$$

In Chapter 2, we showed that the oracle IPW estimator is unbiased for the ATE, $\mathbb { E } \left[ \hat { \tau } _ { I P W } ^ { * } \right] = \tau$ where $\tau = \mathbb { E } \left[ Y _ { i } ( 1 ) - Y _ { i } ( 0 ) \right]$ . The proof given in Theorem 2.2 was an abstract application of conditional independence and the chain rule for expectations that immediately implied unbiasedness.

In an effort to get a better understanding of the statistical function of the propensity score, we start by revisiting the unbiasedness of IPW using a less elegant‚Äîbut more algorithmically explicit‚Äîargument. To this end, suppose we can write the conditional expectation functions $\mu _ { ( w ) } ( x )$ in terms of a basis expansion, i.e.,41

$$
\mu_ {(w)} (x) = \sum_ {j = 1} ^ {\infty} \beta_ {j} (w) \psi_ {j} (x) \tag {7.2}
$$

for some pre-defined set of basis function $\psi _ { j } ( \cdot )$ . Under reasonable regularity conditions (and assuming unconfoundedness), we then have

$$
\tau = m _ {(1)} - m _ {(0)}, \quad m _ {(w)} = \sum_ {j = 1} ^ {\infty} \beta_ {j} (w) \mathbb {E} [ \psi_ {j} (X _ {i}) ]. \tag {7.3}
$$

Given this setup, we can argue that IPW is unbiased as follows. Under uncondoundedness, $Y _ { i } = \mu _ { ( W _ { i } ) } ( X _ { i } ) + \varepsilon _ { i }$ with $\mathbb { E } \left[ \varepsilon _ { i } \big | X _ { i } , W _ { i } \right] = 0$ , and so (again under regularity conditions)

$$
\begin{array}{l} \mathbb {E} \left[ \frac {W _ {i} Y _ {i}}{e \left(X _ {i}\right)} \right] = \mathbb {E} \left[ \frac {W _ {i}}{e \left(X _ {i}\right)} \sum_ {j = 1} ^ {\infty} \beta_ {j} (w) \psi_ {j} \left(X _ {i}\right) \right] + \mathbb {E} \left[ \frac {W _ {i} \varepsilon_ {i}}{e \left(X _ {i}\right)} \right] \tag {7.4} \\ = \sum_ {j = 1} ^ {\infty} \beta_ {j} (w) \mathbb {E} \left[ \frac {W _ {i} \psi_ {j} (X _ {i})}{e (X _ {i})} \right] = \sum_ {j = 1} ^ {\infty} \beta_ {j} (w) \mathbb {E} \left[ \psi_ {j} (X _ {i}) \right] = m _ {(1)}, \\ \end{array}
$$

and similarly $\mathbb { E } \left[ ( 1 - W _ { i } ) Y _ { i } / ( 1 - e ( X _ { i } ) ) \right] = m _ { ( 0 ) }$ . This argument reveals that IPW works by re-weighting units so that the weighted averages of the basis functions $\psi _ { j } ( X _ { i } )$ over both treated and control samples exactly match corresponding population averages.

Population vs. sample balance Oracle IPW achieves unbiasedness by creating population balance across the treated and control groups for all $\psi _ { j } ( X _ { i } )$ :

$$
\mathbb {E} \left[ \frac {W _ {i} \psi_ {j} (X _ {i})}{e (X _ {i})} \right] = \mathbb {E} \left[ \psi_ {j} (X _ {i}) \right], \quad \mathbb {E} \left[ \frac {(1 - W _ {i}) \psi_ {j} (X _ {i})}{1 - e (X _ {i})} \right] = \mathbb {E} \left[ \psi_ {j} (X _ {i}) \right]. \quad (7. 5)
$$

In practice, we need to work with finite samples and need to estimate propensity scores. However, following (7.5), if the sample size $n$ is large enough and the

propensity score estimates $\hat { e } ( X _ { i } )$ are accurate enough, then we may hope to achieve approximate sample balance,

$$
\frac {1}{n} \sum_ {i = 1} ^ {n} \frac {W _ {i} \psi_ {j} (X _ {i})}{\hat {e} (X _ {i})} \approx \frac {1}{n} \sum_ {i = 1} ^ {n} \psi_ {j} (X _ {i}), \tag {7.6}
$$

$$
\frac {1}{n} \sum_ {i = 1} ^ {n} \frac {(1 - W _ {i}) \psi_ {j} (X _ {i})}{1 - \hat {e} (X _ {i})} \approx \frac {1}{n} \sum_ {i = 1} ^ {n} \psi_ {j} (X _ {i}),
$$

and for such sample balance in turn to imply consistency of IPW. This class of arguments can be used to show that IPW is consistent for a wide variety to consistent propensity score estimates $\hat { e } ( X _ { i } )$ .

The above argument is, however, incredibly loose. On the one hand, we claim that IPW achieves consistency by creating balance in the $\psi _ { j } ( X _ { i } )$ ; but on the other hand, the above argument lets sample balance (7.6) emerge indirectly as a consequence of consistent propensity score estimation. If we believe that good sample balance is important, shouldn‚Äôt we put more thought into how we estimate propensity scores and optimize for sample balance as in (7.6)? The answer to this question is affirmative; and the covariate-balancing propensity score methods that emerge from seeking to answer it provide a major improvement over basic IPW methods that do not consider balance.

# 7.1 Covariate-balancing propensity scores

We start by considering methods that target covariate balance under a finitedimensional parametric specification. Suppose that $X _ { i } \in \mathbb { R } ^ { p }$ , and that we know both a linear outcome model $\mu _ { ( w ) } ( x ) = x \cdot \beta ( w )$ and a logistic propensity model $e ( x ) = 1 / ( 1 + e ^ { - x \cdot \theta } )$ to be well specified. Because the outcome model is linear, achieving sample balance just involves balancing the $X _ { i }$ .

The sample balance condition (7.6) involves the ‚Äú $\approx$ ‚Äù relation that we need to disambiguate in order to proceed. Here, given that we‚Äôre in a low-dimensional setting, it‚Äôs reasonable to ask for exact balance, i.e., for (7.6) to hold with equality. If we estimate propensity scores by plugging some parameter estimate $\hat { \theta }$ into the logistic model, i.e., if we use $\hat { e } ( x ) = 1 / ( 1 + e ^ { - x \cdot \hat { \theta } } )$ , then our desired sample-balance condition (7.6) becomes:

$$
\frac {1}{n} \sum_ {i = 1} ^ {n} \left(1 + e ^ {- X _ {i} \hat {\theta}}\right) W _ {i} X _ {i} = \frac {1}{n} \sum_ {i = 1} ^ {n} X _ {i}, \tag {7.7}
$$

$$
\frac {1}{n} \sum_ {i = 1} ^ {n} \left(1 + e ^ {X _ {i} \hat {\theta}}\right) (1 - W _ {i}) X _ {i} = \frac {1}{n} \sum_ {i = 1} ^ {n} X _ {i}. \tag {7.8}
$$

Generic choices of $\hat { \theta }$ (e.g., one produced by fitting logistic regression of $W _ { i }$ o n $X _ { i }$ ) will not yield such sample balance. Covariate-balancing propensity score methods choose $\hat { \theta }$ in a way that is specifically designed to satisfy balance.

These balance equations (7.7) and (7.8) are non-linear systems of equations that may at first glance seem challenging to solve. However, it turns out that‚Äîunder non-degeneracy conditions‚Äîthe solution to (7.7) can equivalently be written as the optimum of the following convex minimization problem,

$$
\hat {\theta} = \operatorname {a r g m i n} _ {\theta} \left\{\frac {1}{n} \sum_ {i = 1} ^ {n} \ell_ {\theta} ^ {(1)} \left(X _ {i}, W _ {i}\right) \right\}, \tag {7.9}
$$

$$
\ell_ {\theta} ^ {(1)} (X _ {i}, W _ {i}) = W _ {i} e ^ {- X _ {i} \theta} + (1 - W _ {i}) X _ {i} \theta ,
$$

so it can readily be solved via numerical methods such as Newton descent. Meanwhile, the solution to (7.8) is equivalent to

$$
\hat {\theta} = \operatorname {a r g m i n} _ {\theta} \left\{\frac {1}{n} \sum_ {i = 1} ^ {n} \ell_ {\theta} ^ {(0)} \left(X _ {i}, W _ {i}\right) \right\}, \tag {7.10}
$$

$$
\ell_ {\theta} ^ {(0)} (X _ {i}, W _ {i}) = (1 - W _ {i}) e ^ {X _ {i} \theta} - W _ {i} X _ {i} \theta .
$$

Now, one subtlety here is that we may be interested in a parameter vector $\hat { \theta }$ that solves both (7.7) and (7.8) simultaneously. This, however, is not in general possible (because it would require solving $2 p$ equation using $p$ free parameters), but neither is it necessary: If the role of the propensity model is simply to create balance, then if it‚Äôs convenient there‚Äôs no strong reason not to use two different propensity models in the context of a single ATE estimator.

Putting all these pieces together to create an IPW estimator of the ATE results in a covariate-balancing propensity score (CBPS) estimator:

$$
\hat {\theta} _ {(w)} = \operatorname {a r g m i n} _ {\theta} \left\{\frac {1}{n} \sum_ {i = 1} ^ {n} \ell_ {\theta} ^ {(w)} \left(X _ {i}, W _ {i}\right) \right\}, \quad \text {f o r} \quad w = 0, 1 \tag {7.11}
$$

$$
\hat {\tau} _ {C B P S} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(1 + e ^ {- X _ {i} \hat {\theta} _ {(1)}}\right) W _ {i} Y _ {i} - \frac {1}{n} \sum_ {i = 1} ^ {n} \left(1 + e ^ {X _ {i} \hat {\theta} _ {(0)}}\right) (1 - W _ {i}) Y _ {i}.
$$

The following result shows that, unlike the oracle IPW estimator which is unbiased but with unnecessarily large variance (Theorem 2.2) or generic IPW with estimated propensity scores which is consistent but doesn‚Äôt necessarily have a good rate of convergence, the above CBPS estimator has excellent statistical properties: It is $\sqrt { n }$ -consistent with and asymptotically normal sampling distribution, and achieves the same asymptotic variance as the AIPW estimator studied in Chapter 3.

Theorem 7.1. We have samples $\{ X _ { i } , Y _ { i } ( 0 ) , Y _ { i } ( 1 ) , W _ { i } \} \stackrel { i i d } { \sim } P$ taking values in $\mathbb { R } ^ { p } \times \mathbb { R } \times \mathbb { R } \times \{ 0 , 1 \}$ such that we get to observe $( X _ { i } , Y _ { i } , W _ { i } )$ where $Y _ { i } = Y _ { i } ( W _ { i } )$ , and that unconfoundedness holds, $\{ Y _ { i } ( 0 ) , Y _ { i } ( 1 ) \} \perp W _ { i } \mid X _ { i }$ . Suppose there is a $c > 0$ for which the following exponential moments are finite,42

$$
\mathbb {E} \left[ \frac {e ^ {c \| X _ {i} \| _ {2}}}{e (X _ {i})} \right] <   \infty , \quad \mathbb {E} \left[ \frac {e ^ {c \| X _ {i} \| _ {2}}}{1 - e (X _ {i})} \right] <   \infty , \tag {7.12}
$$

and that the feature covariance matrix has full rank, $\mathbb { E } \left[ X _ { i } ^ { \otimes 2 } \right] \succ 0$ . Suppose furthermore that both the linear outcome model $\mu _ { ( w ) } ( x ) = x \cdot \beta ( w )$ and the logistic propensity model $e ( x ) = 1 / ( 1 + e ^ { - x \cdot \theta } )$ are well specified with $\left\| \theta \right\| _ { 2 } < \infty$ , and that the conditional variances $\sigma _ { w } ^ { 2 } ( x ) = \operatorname { V a r } \left[ Y _ { i } ( w ) \big | X _ { i } = x \right]$ are uniformly bounded, $\sigma _ { w } ^ { 2 } ( x ) \le M$ . Then $\hat { \tau } _ { C B P S }$ is consistent and

$$
\sqrt {n} \left(\hat {\tau} _ {C B P S} - \tau\right) \Rightarrow \mathcal {N} \left(0, \operatorname {V a r} \left[ \tau \left(X _ {i}\right) \right] + \mathbb {E} \left[ \frac {\sigma_ {0} ^ {2} \left(X _ {i}\right)}{1 - e \left(X _ {i}\right)} + \frac {\sigma_ {1} ^ {2} \left(X _ {i}\right)}{e \left(X _ {i}\right)} \right]\right). \tag {7.13}
$$

Proof. We start by examining the loss functions $\ell _ { \theta } ^ { ( 1 ) } ( x , w )$ given above, and its expectation

$$
L _ {(1)} (\theta) = \mathbb {E} \left[ \ell_ {\theta} ^ {(1)} (X _ {i}, W _ {i}) \right].
$$

The analysis of $\ell _ { \theta } ^ { ( 0 ) } ( x , w )$ and $L _ { ( 0 ) } ( \cdot )$ is essentially identical, and so we do not carry it out here. First, note that

$$
\nabla^ {2} \ell_ {\theta} ^ {(1)} (x, w) = w e ^ {- \theta \cdot x} x ^ {\otimes 2} \succeq 0,
$$

i.e., this loss functions are convex as claimed. Next, assuming that the logistic propensity model is well specified (with true parameter value $\theta$ ), we see that for any $\theta ^ { \prime }$

$$
L _ {(1)} (\theta^ {\prime}) = \mathbb {E} \left[ \frac {e ^ {- X _ {i} \theta}}{1 + e ^ {- X _ {i} \theta}} e ^ {X _ {i} (\theta - \theta^ {\prime})} + \frac {1}{1 + e ^ {X _ {i} \theta}} X _ {i} \theta^ {\prime} \right],
$$

which, because $\mathbb { E } \left\lfloor e ^ { c \| x \| _ { 2 } } \right\rfloor < \infty$ thanks to (7.12), is finite for any $\theta ^ { \prime }$ such that $\begin{array} { r } { \| \theta - \theta ^ { \prime } \| _ { 2 } \leq c } \end{array}$ . Finally, at the true parameter value $\theta$ , 4 3

$$
\nabla L _ {(1)} (\theta) = 0, \quad \nabla^ {2} L _ {(1)} (\theta) = \mathbb {E} \left[ e (X _ {i}) X _ {i} ^ {\otimes 2} \right] \succ 0,
$$

i.e., $\theta$ is in fact a minimizer of $L _ { ( 1 ) } ( \cdot )$ ; and, by convexity of ${ \cal L } _ { \theta } ^ { ( 1 ) }$ and strong convexity at $\theta$ , it is the unique minimizer $L _ { ( 1 ) } ( \cdot )$ .

Given these preliminaries, we can use standard results for convex empirical risk minimization [e.g., Van der Vaart, 1998, Theorem 5.7 and Example 19.8] to check that $\hat { \theta } _ { ( 1 ) }$ is consistent, i.e., $\hat { \theta } _ { ( 1 ) } \to _ { p } \theta$ . Thus, in particular, we see that $\hat { \theta } _ { ( 1 ) }$ must be finite with probability going to 1. It must thus (with probability going to 1) be a critical point of the loss function,

$$
\nabla \left(\frac {1}{n} \sum_ {i = 1} ^ {n} W _ {i} e ^ {- X _ {i} \hat {\theta} _ {(1)}} + (1 - W _ {i}) X _ {i} \hat {\theta} _ {(1)}\right) = 0,
$$

which in turn is equivalent to $\hat { \theta } _ { ( 1 ) }$ solving (7.7).

Applying an analogous analysis to $\hat { \theta } _ { ( 0 ) }$ and plugging these balance conditions into (7.11), we can use well-specification of the linear outcome model to verify that on the with-probability-tending-to-1 event where $\hat { \theta } _ { ( 1 ) }$ solves (7.7) and $\hat { \theta } _ { ( 0 ) }$ solves (7.8),

$$
\begin{array}{l} \hat {\tau} _ {C B P S} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(X _ {i} \left(\beta_ {(1)} - \beta_ {(0)}\right) + (2 W _ {i} - 1) \left(1 + e ^ {- (2 W _ {i} - 1) X _ {i} \hat {\theta} _ {(W _ {i})}}\right) \varepsilon_ {i}\right), \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\tau (X _ {i}) + \frac {W _ {i}}{e (X _ {i})} \varepsilon_ {i} - \frac {1 - W _ {i}}{1 - e (X _ {i})} \varepsilon_ {i}\right) \\ + \frac {1}{n} \sum_ {i = 1} ^ {n} \left(e ^ {- X _ {i} \hat {\theta} _ {(1)}} - e ^ {- X _ {i} \theta}\right) W _ {i} \varepsilon_ {i} - \frac {1}{n} \sum_ {i = 1} ^ {n} \left(e ^ {X _ {i} \hat {\theta} _ {(0)}} - e ^ {X _ {i} \theta}\right) (1 - W _ {i}) \varepsilon_ {i}, \\ \end{array}
$$

where $\varepsilon _ { i } = Y _ { i } - X _ { i } \beta _ { ( W _ { i } ) }$ . Now, the first summand above is familiar from our earlier discussions (e.g., in Chapter 2), and satisfies (7.13).

It remains to check that the last two terms are asymptotically negligible on the $1 / \sqrt { n }$ scale. To this end, note that this term is mean-zero conditionally on $\{ X _ { i } , W _ { i } \}$ (and thus also the $\widehat { \theta } _ { ( w ) }$ ), and that

$$
\begin{array}{l} n \mathbb {E} \left[ \left(\frac {1}{n} \sum_ {i = 1} ^ {n} \left(e ^ {- X _ {i} \hat {\theta} _ {(1)}} - e ^ {- X _ {i} \theta}\right) W _ {i} \varepsilon_ {i}\right) ^ {2} \mid \{X _ {i}, W _ {i} \} \right] \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(e ^ {- X _ {i} \hat {\theta} _ {(1)}} - e ^ {- X _ {i} \theta}\right) ^ {2} W _ {i} \sigma_ {1} ^ {2} (X _ {i}) \\ \leq \frac {M}{n} \sum_ {i = 1} ^ {n} \left(e ^ {- X _ {i} \hat {\theta} _ {(1)}} - e ^ {- X _ {i} \theta}\right) ^ {2} W _ {i} \\ = \frac {M}{n} \sum_ {i = 1} ^ {n} \left(e ^ {X _ {i} (\theta - \hat {\theta} _ {(1)})} - 1\right) ^ {2} e ^ {- 2 X _ {i} \theta} W _ {i}. \\ \end{array}
$$

We know that, by consistency, $\lVert \theta - \hat { \theta } _ { ( 1 ) } \rVert _ { 2 } \leq \delta / 2$ with probability tending to 1 for any $\delta > 0$ , and so, again with probability tending to 1, the above expression is bounded by

$$
\begin{array}{l} \dots \leq \frac {2 M}{n} \sum_ {i = 1} ^ {n} \left(e ^ {\delta \| X _ {i} \| _ {2}} + 1\right) e ^ {- 2 X _ {i} \theta} W _ {i} \\ = \mathcal {O} _ {P} \left(\mathbb {E} \left[ \left(e ^ {\delta \| X _ {i} \| _ {2}} + 1\right) e ^ {- 2 X _ {i} \theta} / \left(1 + e ^ {- X _ {i} \theta}\right) \right]\right) \\ = \mathcal {O} _ {P} \left(\mathbb {E} \left[ e ^ {\delta \| X _ {i} \| _ {2}} \left(1 + e ^ {- X _ {i} \theta}\right) \right]\right), \\ \end{array}
$$

where the steps above were by Markov‚Äôs inequality on the 2nd line and by direct algebraic manipulations on the 3rd line. This expression is finite for any $\delta \leq c$ by (7.12); and tends to 0 as $\delta  0$ by continuity. Thus, by consistency of $\hat { \theta } _ { ( 1 ) }$ ,

$$
n \mathbb {E} \left[ \left(\frac {1}{n} \sum_ {i = 1} ^ {n} \left(e ^ {- X _ {i} \hat {\theta} _ {(1)}} - e ^ {- X _ {i} \theta}\right) W _ {i} \varepsilon_ {i}\right) ^ {2} \mid \{X _ {i}, W _ {i} \} \right] \to_ {p} 0,
$$

and so by Chebyshev‚Äôs inequality this term is on the $1 / \sqrt { n }$ scale as we sought to show. Applying an analogous argument to the term involving $\hat { \theta } _ { ( 0 ) }$ completes the proof. ‚ñ°

Thus, if we believe in a linear-logistic specification and want to use an IPW estimator, then we should learn the propensity model by minimizing the covariate-balancing loss function rather than by the usual maximum likelihood loss used for logistic regression. Maximum likelihood is asymptotically optimal from the perspective of estimating the logistic regression parameters $\theta$ , but that‚Äôs not what matters here. When estimating the ATE via IPW, what we need from the inverse-propensity weights is for them to create balance as in (7.6); and we achieve good results with IPW when using covariate-balancing propensity scores that directly target this goal.

Exercise 8 in Chapter 16 expands on the result given above, and also establishes double-robustness properties for $\hat { \tau } _ { C B P S }$ that hold if only one of the linear or logistic models is well specified. Exercise 9 studies a covariate-balancing propensity score estimator that targets the average treatment effect on the treated.

Remark 7.1. The estimator (7.11) is not the first covariate-balancing propensity score estimator encountered in this book. In Chapter 2, we considered a setting where the feature space $\mathcal { X }$ is discrete, and found that the natural stratified estimator $\hat { \tau } _ { S T R A T }$ could be interpreted as an IPW-estimator with a smart

choice of estimated propensities that enable efficient large sample behavior; see Theorem 2.1 and (2.17). Further examination reveals that the propensity scores underlying $\hat { \tau } _ { S T R A T }$ achieve exact sample balance for indicators $1 \left( \left\{ X _ { i } = x \right\} \right.$ ) for all $x \in \mathcal { X }$ , and that $\scriptstyle { \hat { \tau } } _ { S T R A T }$ is equivalent to $\hat { \tau } _ { C B P S }$ for a saturated model. Thus, conceptually, we can think of covariate-balancing propensity score methods as the natural generalization of stratified treatment effect estimation for when $\mathcal { X }$ takes on continuous values.

# 7.2 Approximate balance and augmented estimators

We established above that, when working in a low-dimensional parametric setting, propensity score methods that target exact finite-sample balance as in (7.7) and (7.8) have a number of good statistical properties. In some settings, however, achieving exact balance may not be realistic. In some modern applications, the covariates $X _ { i } ~ \in ~ \mathbb { R } ^ { p }$ may take values in a high-dimensional space with $p \gg n$ (e.g., $X _ { i }$ may represent a patient‚Äôs genome); and in this case it‚Äôs generally not possible to find weights on $n$ samples that exactly solve $p$ covariate-balancing moment conditions. Or, as in our motivating example 7.2, we may be interested in a setting where we use an infinite sieve to approximate a non-parametric function, and in this case we have infinitely many covariate-balancing moment conditions to worry about.

Thankfully, even when exact balance is unachievable, we can still obtain good results via propensity-score methods that aim for approximate balance

$$
\sup  _ {j = 1, 2, \dots} \left| \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {W _ {i} \psi_ {j} \left(X _ {i}\right)}{\hat {e} \left(X _ {i}\right)} - \psi_ {j} \left(X _ {i}\right) \right| \leq t, \tag {7.14}
$$

$$
\sup  _ {j = 1, 2, \dots} \left| \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {\left(1 - W _ {i}\right) \psi_ {j} (X _ {i})}{1 - \hat {e} (X _ {i})} - \psi_ {j} (X _ {i}) \right| \leq t,
$$

for some small tolerance parameter $t$ . When working with approximate balance, plain IPW-type estimator as considered above may dominated by bias and no longer work well; however, using augmented IPW-type estimators can address the issue. The reason augmented estimators help with approximate balance is closely tied to the (strong) double robustness of augmented IPW discussed in Chapter 3: A reasonably accurate regression adjustment can mitigate the bias due to non-exact balance without introducing excess errors in doing so.

A comprehensive review of approximately balancing methods for highdimensional and/or non-parametric treatment effect estimation problems is beyond the scope of this presentation. Instead, we will here summarize one ap-

proach tailored to the high-dimensional setting with a sparse, linear outcome model, and present references for further reading at the end of the chapter.

Suppose that the basic unconfoundedness model from Chapter 3 holds with high-dimensional controls $X _ { i } \in \mathbb { R } ^ { p }$ , where $p$ may be much larger than $n$ . Suppose furthermore that the outcome model is sparse and linear, $\mu _ { ( w ) } ( x ) = x \cdot \beta _ { ( w ) }$ with $\| \beta _ { ( w ) } \| _ { 0 } \leq k$ for some reasonably small bound on the number of non-zero parameters $k$ , where $\lVert \boldsymbol { v } \rVert _ { 0 }$ counts the number of non-zero entries in 0. Note that we are not making any parametric assumptions on the propensity model here, and simply assume strong overlap $\eta \leq e ( X _ { i } ) \leq 1 - \eta$ .

Given this setup, Athey, Imbens, and Wager [2018b] consider learning weights $\hat { \gamma } _ { i }$ by directly minimizing an approximate balance criterion:

$$
\begin{array}{l} \hat {\gamma} ^ {(1)} = \operatorname {a r g m i n} _ {\gamma_ {i} \geq 0, t \geq 0} \frac {1}{n} \sum_ {W _ {i} = 1} \gamma_ {i} ^ {2} + \zeta n t ^ {2} \tag {7.15} \\ \text {s u b j e c t} \quad \left| \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\gamma_ {i} W _ {i} - 1\right) X _ {i} \right| \leq t \quad \text {f o r a l l} j = 1, \dots , p, \\ \end{array}
$$

as ‚Äú and $1 / \acute { e } ( X _ { i } ) = \hat { \gamma } _ { i } ^ { ( 1 ) } { } ^ { , }$ $\hat { \gamma } _ { ( 0 ) }$ is derived analogously. Conceptually, we can interpret these weights , etc., but here the weights aren‚Äôt derived from a parametric propensity model. We can then use these approximate balancing weights to derive an augmented balancing estimator modeled after the AIPW construction,

$$
\begin{array}{l} \hat {\tau} _ {A B} = \frac {1}{n} \sum_ {i = 1} ^ {n} X _ {i} \left(\hat {\beta} _ {(1)} - \hat {\beta} _ {(0)}\right) + W _ {i} \hat {\gamma} _ {i} ^ {(1)} \left(Y _ {i} - X _ {i} \hat {\beta} _ {(1)}\right) \tag {7.16} \\ - \left(1 - W _ {i}\right) \hat {\gamma} _ {i} ^ {(0)} \left(Y _ {i} - X _ {i} \hat {\beta} _ {(0)}\right), \\ \end{array}
$$

where the $\hat { \beta } _ { ( w ) }$ are estimated via some method applicable to sparse, highdimensional data such as the lasso [Tibshirani, 1996]. The key motivation behind this construction is the following lemma.

Lemma 7.2. Under unconfoundedness and SUTVA, suppose furthermore that $\mu _ { ( w ) } ( x ) = x \cdot \beta _ { ( w ) }$ , and that $\hat { \beta } _ { ( w ) }$ is an estimator of $\beta _ { ( w ) }$ with $L _ { 1 }$ -norm estimation error bounded by $C _ { ( w ) }$ for $w = 0$ , 1:

$$
\left\| \hat {\beta} _ {(w)} - \beta_ {(w)} \right\| _ {1} \leq C _ {(w)}, \quad \| v \| _ {1} = \sum_ {j = 1} ^ {p} | v _ {j} |. \tag {7.17}
$$

Then, the augmented balancing estimator (7.16) satisfies

$$
\hat {\tau} _ {A B} = \frac {1}{n} \sum_ {i = 1} ^ {n} X _ {i} (\beta_ {(1)} - \beta_ {(0)}) + W _ {i} \hat {\gamma} _ {i} ^ {(1)} \varepsilon_ {i} - (1 - W _ {i}) \hat {\gamma} _ {i} ^ {(0)} \varepsilon_ {i} + E, \tag {7.18}
$$

$$
| E | \leq C _ {(0)} \hat {t} ^ {(0)} + C _ {(1)} \hat {t} ^ {(1)},
$$

where the $\hat { t } ^ { ( w ) }$ are the bias parameters in the solution to the optimization problem (7.15) and $\varepsilon _ { i } = Y _ { i } - X _ { i } \beta _ { ( W _ { i } ) }$ .

Proof. Thanks to linearity of $\mu _ { ( w ) } ( x )$ , we immediately get that the first line of (7.18) holds with error term

$$
\begin{array}{l} E = \frac {1}{n} \sum_ {i = 1} ^ {n} X _ {i} \left(\hat {\beta} _ {(1)} - \hat {\beta} _ {(0)}\right) - X _ {i} \left(\beta_ {(1)} - \beta_ {(0)}\right) \\ + W _ {i} \hat {\gamma} _ {i} ^ {(1)} X _ {i} \left(\beta_ {(1)} - \hat {\beta} _ {(1)}\right) - (1 - W _ {i}) \hat {\gamma} _ {i} ^ {(0)} X _ {i} \left(\beta_ {(0)} - \hat {\beta} _ {(0)}\right) \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(1 - W _ {i} \hat {\gamma} _ {i} ^ {(1)}\right) X _ {i} \left(\hat {\beta} _ {(1)} - \beta_ {(1)}\right) \\ - \frac {1}{n} \sum_ {i = 1} ^ {n} \left(1 - (1 - W _ {i}) \hat {\gamma} _ {i} ^ {(0)}\right) X _ {i} \left(\hat {\beta} _ {(0)} - \beta_ {(0)}\right) \\ \end{array}
$$

An application of H¬®older‚Äôs inequality then gives

$$
\begin{array}{l} | E | \leq \left\| \frac {1}{n} \sum_ {i = 1} ^ {n} \left(1 - W _ {i} \hat {\gamma} _ {i} ^ {(1)}\right) X _ {i} \right\| _ {\infty} \left\| \hat {\beta} _ {(1)} - \beta_ {(1)} \right\| _ {1} \\ + \left\| \frac {1}{n} \sum_ {i = 1} ^ {n} \Big (1 - (1 - W _ {i}) \hat {\gamma} _ {i} ^ {(0)} \Big) X _ {i} \right\| _ {\infty} \left\| \hat {\beta} _ {(0)} - \beta_ {(0)} \right\| _ {1}, \\ \end{array}
$$

which is equivalent to the bound we seek to show.

The upshot is that, ignoring the error term $E$ , the expression for $\hat { \tau } _ { A B }$ given in (7.18) has the familiar form obtained with efficient estimators of the ATE in Chapter 3. Thus, if we can show that $E$ is negligible on the $1 / \sqrt { n }$ -scale, this result strongly suggests that we should expect good statistical behavior from $\hat { \tau } _ { A B }$ . One wrinkle that‚Äôs beyond the scope of this presentation is to provide a precise characterization of what the $\hat { \gamma } ^ { ( w ) }$ converge to; $^ { 4 4 }$ however, one simple

observation is that if we can control the average second moment of the $\hat { \gamma } ^ { ( w ) }$ (as will be done below), then (7.18) together with an error bound $| E | \ll 1 / \sqrt { n }$ implies that $\hat { \tau } _ { A B }$ is $\sqrt { n }$ -consistent and asymptotically unbiased.

It now remains to establish conditions under which $E$ is bounded. Under a widely used assumption on the covariate distribution called the ‚Äúrestricted eigenvalue condition‚Äù and under a sparsity bound $\| \beta _ { ( w ) } \| _ { 0 } \leq k$ (i.e., and assumption that the true parameter vector has at most $k$ non-zero entries), the lasso can achieve 1-norm error [e.g., Negahban et al., 2012]

$$
\left\| \hat {\beta} _ {(w)} - \beta_ {(w)} \right\| _ {1} = \mathcal {O} _ {P} \left(k \sqrt {\frac {\log (p)}{n}}\right). \tag {7.19}
$$

Meanwhile, the imbalance of approximate balancing weights can be controlled via the following result.

Lemma 7.3. Suppose that strong overlap holds, $\eta \leq e ( X _ { i } ) \leq 1 - \eta$ for some $\eta > 0$ , that the features $X _ { i }$ are bounded $| X _ { i } | \le M$ . Then, with probability at least $1 - \delta$ , the solution to the approximate balancing program (7.15) with tuning parameter $\zeta = 1 / ( 4 \log ( p ) )$ has a solution satisfying

$$
\frac {1}{n} \sum_ {W _ {i} = 1} \left(\hat {\gamma} _ {i} ^ {(1)}\right) ^ {2} = \mathcal {O} _ {P} (1), \quad \hat {t} ^ {(1)} = \mathcal {O} _ {P} \left(\sqrt {\frac {\log (p)}{n}}\right). \tag {7.20}
$$

Proof. Consider the value of the objective function in (7.15) if we were to plug-in the true propensity scores $\gamma _ { i } ^ { * } = 1 / e ( X _ { i } )$ . This choice would induce a worst-case imbalance

$$
t ^ {*} = \left\| \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i}}{e (X _ {i})} - 1\right) X _ {i} \right\| _ {\infty}.
$$

Now, for every $j = 1$ , $\cdot \cdot \cdot$ , $p$ , we have $\mathbb { E } \left[ ( W _ { i } / e ( X _ { i } ) - 1 ) X _ { i j } \right] = 0$ and, thanks to strong overlap and boundedness, we have $| ( W _ { i } / e ( X _ { i } ) - 1 ) X _ { i j } | \le M / \eta$ . Thus, we can use Hoeffding‚Äôs inequality and a union bound to verify that,

$$
\mathbb {P} \left[ | t ^ {*} | \geq \frac {M}{\eta} \sqrt {\frac {4 \log (p)}{n}} \right] \leq \frac {2}{p}.
$$

A second application of Hoeffding‚Äôs inequality to the first part of the objective and plugging in our choice for $\zeta$ then shows that,

$$
\mathbb {P} \left[ \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {W _ {i}}{e ^ {2} (X _ {i})} + n \zeta (t ^ {*}) ^ {2} \geq \mathbb {E} \left[ \frac {1}{e (X _ {i})} \right] + \frac {1}{\eta^ {2}} \sqrt {\frac {2 \log (p)}{n}} + \frac {M ^ {2}}{\eta^ {2}} \right] \leq \frac {4}{p}.
$$

Now, the true inverse-propensity scores $\gamma _ { i } ^ { * }$ are simply one feasible solution to the optimization problem (7.15), whereas $\hat { \gamma } ^ { ( 1 ) }$ was chosen such as to make the optimization objective as small as possible. Thus, by monotonicity, we must also have

$$
\mathbb {P} \left[ \frac {1}{n} \sum_ {W _ {i} = 1} \left(\hat {\gamma} _ {i} ^ {(1)}\right) ^ {2} + n \zeta (\hat {t} ^ {(1)}) ^ {2} \geq \mathbb {E} \left[ \frac {1}{e (X _ {i})} \right] + \frac {1}{\eta^ {2}} \sqrt {\frac {2 \log (p)}{n}} + \frac {M ^ {2}}{\eta^ {2}} \right] \leq \frac {4}{p}.
$$

The desired conclusion follows by noting that all terms in the objective are non-negative, and so must also be individually controlled by the given upper bound. ‚ñ°

Putting together the pieces, we can use (7.19) and (7.20) to show that, under a sparsity bound $\| \beta _ { ( w ) } \| _ { 0 } \leq k$ , the error term $E$ in Lemma 7.2 is bounded to order $| E | = \mathcal O _ { P } ( k \log ( p ) / n )$ . It is thus negligible on the $1 / \sqrt { n }$ -scale whenever the sparsity level is controlled as $k \ll \sqrt { n } / \log ( p )$ . This sparsity condition is familiar from the literature on high-dimensional inference [Javanmard and Montanari, 2014, Zhang and Zhang, 2014], and corresponds to the weakest sparsity condition under which debiased lasso methods enable valid inference without further assumptions knowledge about the distribution of the covariates $X _ { i }$ . This connection is not an accident, and the augmented balancing method presented here is in fact closely connected to debiased lasso methods for high dimensional inference; see Athey, Imbens, and Wager [2018b] for a discussion and further references.

Remark 7.2. We earlier made a claim that, when we have weights that achieve approximate (but not exact) balance, augmented estimators of the form (7.16) should be used. We are now in a position to substantiate this claim: Suppose that we are in a high-dimensional setting and use weights (7.15) to form an IPW-type estimator

$$
\hat {\tau} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(W _ {i} \hat {\gamma} _ {i} ^ {(1)} Y _ {i} - (1 - W _ {i}) \hat {\gamma} _ {i} ^ {(0)} Y _ {i}\right). \tag {7.21}
$$

We can then use Lemma 7.3 to control the bias of this estimator; however, the resulting bias bound will generally be of order $\sqrt { \log ( p ) / n }$ , and this bound dominates the error of the estimator when $p$ can grow with $n$ . Thus, our analysis only yields $\sqrt { n }$ -consistency in high dimensions when approximately balancing weights are used in an augmented estimator.

Remark 7.3. In comparing different methods discussed in this chapter, one natural question to ask is: What happens if we apply the direct balance-seeking

strategy (7.15) in a low-dimensional setting, and target exact rather than approximate balance? This results in treated weights

$$
\hat {\gamma} ^ {(1)} = \operatorname {a r g m i n} _ {\gamma_ {i} \geq 0} \left\{\frac {1}{n} \sum_ {W _ {i} = 1} \gamma_ {i} ^ {2}: \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\gamma_ {i} W _ {i} - 1\right) X _ {i} = 0 \right\}, \tag {7.22}
$$

and analogous control weights; note that this optimization problem will generally only be feasible when both the number of treated units and the number of control units is greater than $p$ . If we have exact balance, then using an augmented form as in (7.16) is no longer necessary; in fact, exact balance means that the regression adjustment term gets exactly canceled out and so the augmented estimator is numerically equal to a non-augmented one [Robins et al., 2007].45

# Bibliographic notes

The key role of covariate balance for average treatment effect estimation under unconfoundedness has long been recognized, and a standard operation procedure when working with any weighted or matching-type estimators is to use balance as a goodness of fit check [Imbens and Rubin, 2015]. For example, after fitting a propensity model by logistic regression, one could check that the induced propensity weights satisfy a sample balance condition of the type (7.6) with reasonable accuracy. If the balance condition is not satisfied, one could try fitting a different (better) propensity model.

The idea of using covariate balance as an idea to guide propensity estimation (rather than simply as a post-hoc sanity check) is more recent. Early proposals from different communities include Graham, Pinto, and Egel [2012] Hainmueller [2012] and Imai and Ratkovic [2014]; a unifying perspective on these methods via covariate-balancing loss functions is provided by Zhao [2019]. Zubizarreta [2015] proposed learning weights that achieve balance without going via an explicit application of IPW in the context of a parametric propensity model. Iacus, King, and Porro [2012] proposed coarsening a continuous covariate space into a finite number of regions, and then applying a stratified estimator over these regions to achieve balance.46 The term ‚Äúcovariate-balancing

propensity score‚Äù was coined by Imai and Ratkovic [2014], while our presentation given in Chapter 7.1 most closely builds on Graham, Pinto, and Egel [2012] and Zhao [2019].

Our presentation in Chapter 7.2 was adapted from Athey, Imbens, and Wager [2018b], who showed that approximately balancing weights and augmented estimators can be used for inference about average treatment effects with high-dimensional controls under a sparse, linear outcome model. Tan [2020] pairs an augmented construction with a lasso-penalized variant of the covariate-balancing propensity score estimator (7.10) to estimate average treatment effects in a high-dimensional linear-logistic specification. Kallus [2020] and Hirshberg and Wager [2021] consider balancing (and augmented balancing) methods in a non-parametric setting, and derive weights that approximately balance all functions in an infinite-dimensional space (e.g., all functions in a given smoothness class). In particular, Hirshberg and Wager [2021] show that if the class of balanced functions is not too large and spans the true inverse-propensity weighting functions $1 / e ( \cdot )$ and $1 / ( 1 + e ( \cdot ) )$ , then augmented approximately balancing estimators of the average treatment effect are efficient in the sense of Chapter 3.2 under weak conditions.

Finally, the principles behind balanced estimation apply more broadly than to average treatment effect estimation, and can in fact be used to estimate a wide class of econometric targets. The Riesz representer theorem gives conditions under which estimands $\theta$ that depend linearly on the sampling distribution‚Äîthis includes quantities such as average derivatives and average partial effects‚Äîcan be characterized as weighted averages $\theta = \mathbb { E } \left[ \gamma ( X _ { i } , W _ { i } ) Y _ { i } \right]$ for a weight function $\gamma ( \cdot )$ called the Riesz representer. In the case of ATE estimation under unconfoundedness and with a binary treatment, the Riesz representer is $\gamma ( x , w ) = w / e ( x ) - ( 1 - w ) / ( 1 - e ( x ) )$ , and thus IPW for ATE estimation is in fact a special case of Riesz-representer weighting. Chernozhukov et al. [2022a] use this perspective to develop doubly robust estimators for a wide class of targets by replacing the propensity-estimation step with estimation of the Riesz representer. Hirshberg and Wager [2021] show that the balancing weights construction (7.15) effectively yields a penalized empirical Riesz representer, and thus their method (and results) directly extend to the general setting of Chernozhukov et al. [2022a]. Chernozhukov, Newey, and Singh [2022b] provide a general recipe for machine-learning based estimation of Riesz representers that can be used to automate the construction of double machine learning estimators for generic linear targets.

# Chapter 8 Regression Discontinuity Designs

The cleanest and most straight-forward approach to treatment effect estimation is using approaches justified by random treatment assignment‚Äîwhere randomization can either be explicit (as in randomized controlled trials) or implicit (as in observational study analyses under an unconfoundedness assumption). All methods discussed in the book so far fall within this category.

In applied work, however, there‚Äôs also often interest in drawing causal inferences using data where it is not realistic to assume that treatment is as good as random (even after controlling for observed pre-treatment covariates), and there exist a number of widely used econometric methods for identifying and estimating causal effects in settings without random treatment assignment. This chapter‚Äîas well as the following ones‚Äîwill provide a brief introduction to such quasi-experimental approaches to causal inference. We use the term ‚Äúquasi experimental‚Äù to emphasize that these approaches are still framed using concepts from randomized experiments‚Äîsuch as potential outcomes and average treatment effects‚Äîbut require econometric innovations to compensate for the lack of random treatment assignment.

Setting and notation This chapter is about the regression discontinuity design (RDD), which is a simple and widely used quasi-experimental design. In a simple RDD, we are interested in the effect of a binary treatment $W _ { i }$ on a real-valued outcome $Y _ { i }$ , and posit potential outcomes $\{ Y _ { i } ( 0 ) , Y _ { i } ( 1 ) \}$ such that $Y _ { i } = Y _ { i } ( W _ { i } )$ . However, unlike in a randomized trial, we do not take the treatment assignment $W _ { i }$ to be random. Instead, we assume there is a running variable $Z _ { i } \in \mathbb { R }$ and a cutoff $c$ , such that $W _ { i } = 1 \left( \left\{ Z _ { i } \geq c \right\} \right)$ . This setting could arise, e.g., in education, where $Z _ { i }$ is a standardized test score and students with $Z _ { i } \geq c$ are eligible to enroll in an honors program, or in medicine, where $Z _ { i }$ is a severity score, and patients are prescribed an intervention once $Z _ { i } \geq c$ .

Qualitatively, the main idea of a regression discontinuity is that although treatment assignment $W _ { i }$ is not randomized, it‚Äôs almost as good as random

when $Z _ { i }$ is in the vicinity of the cutoff $c$ . People with $Z _ { i }$ close to $c$ ought to all be similar to each other on average, but only those with $Z _ { i } \geq c$ get treated, and so we can estimate a treatment effect by comparing people with $Z _ { i }$ right above versus right below the cutoff.

Example 7. Lee [2008] studies incumbency advantage in US House elections by examining close elections. He compares the probability that a given political party wins a House seat in an election cycle when they just barely won that seat in the previous cycle vs. when they just barely lost. Validity of this approach hinges on an understanding that results of close elections are unpredictable and subject to idiosyncratic factors (e.g., perhaps a rain storm on election day caused differential attrition in turnout that moved the two-party vote share by a small amount), and that congressional districts where one party won, say, 51% vs. 49% of the two-party vote should have roughly the same distribution of potential confounding factors. Then, once we‚Äôve established that such congressional districts are ex-ante comparable, we can obtain valid causal estimates via the regression-discontinuity approach.

Why propensity score methods can‚Äôt be used in RDDs Before discussing methods for estimation in regression discontinuity designs, it‚Äôs helpful to consider why our previously considered approaches (such as IPW) don‚Äôt apply. As emphasized in our discussion so far, the two assumptions invariably required for propensity-score methods to work are:

$$
\left\{Y _ {i} (0), Y _ {i} (1) \right\} \perp W _ {i} \mid Z _ {i}, \quad \text {u n c o n f o u n d e d n e s s , a n d} \tag {8.1}
$$

$$
0 <   \mathbb {P} \left[ W _ {i} = 1 \mid Z _ {i} \right] <   1, \quad \text {o v e r l a p .} \tag {8.2}
$$

Taken together, unconfoundedness and overlap mean that we can view our dataset as formed by pooling many small randomized trials indexed by different values of $Z _ { i }$ ; then, unconfoundedness means that treatment assignment is exogenous given $Z _ { i }$ , while overlap means that randomization in fact occurred (one can‚Äôt learn anything from a randomized trial where everyone is assigned to the same treatment arm).

In a regression discontinuity design, we have $W _ { i } = 1 ( \{ Z _ { i } \geq c \} )$ , and so unconfoundedness holds trivially (because $W _ { i }$ is a deterministic function of $Z _ { i }$ ). However, overlap clearly doesn‚Äôt hold: $\mathbb { P } \left[ W _ { i } = 1 \big | Z _ { i } = z \right]$ is always either 0 or 1. Thus, methods like IPW that involve division by $\mathbb { P }$ $\left[ W _ { i } = 1 \mid Z _ { i } \right]$ , etc., are not applicable. Instead, we‚Äôll need to compare units with $Z _ { i }$ straddling the cutoff $c$ that are similar to each other‚Äîbut do not have contiguous distributions.

# 8.1 Local linear regression

The most prevalent way to formalize the qualitative argument underlying RDD is by invoking continuity. Let $\mu _ { ( w ) } ( z ) = \mathbb { E } \left\lfloor Y _ { i } ( w ) \vert Z _ { i } \right\rfloor$ . Then, if $\mu _ { ( 0 ) } ( z )$ and $\mu _ { ( 1 ) } ( z )$ are both continuous, we can identify the conditional average treatment effect at $z = c$ , i.e., $\tau _ { c } = \mu _ { ( 1 ) } ( c ) - \mu _ { ( 0 ) } ( c )$ , via

$$
\tau_ {c} = \lim  _ {z \downarrow c} \mathbb {E} [ Y _ {i} \mid Z _ {i} = z ] - \lim  _ {z \uparrow c} \mathbb {E} [ Y _ {i} \mid Z _ {i} = z ], \tag {8.3}
$$

provided that the running variable $Z _ { i }$ has support around the cutoff $c$ . In other words, we identify $\tau _ { c }$ as the difference between the endpoints of two different regression curves; the above figure provides an illustration.

Estimation via local linear regression A simple and robust approach to estimation based on (8.3) is to use local linear regression, as illustrated in Figure 8.1. We pick a small bandwidth $h _ { n } \to 0$ and a symmetric weighting function $K ( \cdot )$ , and then fit $\mu _ { ( w ) } ( z )$ via weighted linear regression on each side of the boundary,

$$
\begin{array}{l} \hat {\tau} _ {c} = \operatorname {a r g m i n} \left\{\sum_ {i = 1} ^ {n} K \left(\frac {\left| Z _ {i} - c \right|}{h _ {n}}\right) \right. \tag {8.4} \\ \left. \times \left(Y _ {i} - a - \tau W _ {i} - \beta_ {(0)} (Z _ {i} - c) _ {-} - \beta_ {(1)} (Z _ {i} - c) _ {+}\right) ^ {2} \right\}, \\ \end{array}
$$

where the overall intercept $a$ and slope parameters $\beta _ { ( w ) }$ are nuisance parameters. Popular choices for the weighting function $K ( x )$ include the window function $K ( x ) = 1 ( \{ | x | \leq 1 \} )$ ), or the triangular kernel $K ( x ) = ( 1 - | x | ) _ { + }$ .

Consistency, asymptotics and rates of convergence It is not hard to see that, under continuity assumptions as in (8.3), the local linear regression estimator (8.4) must be consistent for reasonable choices of the bandwidth sequence $h _ { n }$ . However, in order to move beyond such a high-level statement and get any quantitative guarantees, we need to be more specific about the continuity assumptions made on $\mu _ { ( 0 ) } ( z )$ and $\mu _ { ( 1 ) } ( z )$ .

There are many ways of quantifying smoothness. One simple and widely used assumptions in practice‚Äîand the one we‚Äôll focus on here‚Äîis that the $\mu _ { ( w ) } ( z )$ are twice differentiable with a uniformly bounded second derivative47

$$
\left| \frac {d ^ {2}}{d z ^ {2}} \mu_ {(w)} (z) \right| \leq B \text {f o r a l l} z \in \mathbb {R} \text {a n d} w \in \{0, 1 \}. \tag {8.5}
$$

![](images/5a4e39e21ceaf3fff91efdb5de59e75e1673ae6f377911060d6b78901b8e37c2.jpg)  
Figure 8.1: Illustration of the local linear regression estimator in RDD. The solid blue line denotes the cutoff $c$ , and the dashed lines are at $c \pm h _ { n }$ . The local regression lines are in red, and the difference between the regression lines at $c$ yields the estimate $\hat { \tau } _ { c }$ .

One motivation for the assumption (8.5) is that it justifies local linear regression as in (8.4): If we had less smoothness (e.g., $\mu _ { ( w ) } ( z )$ is just taken to be Lipschitz) then there would be no point doing local linear regression as opposed to local averaging, whereas if we had more smoothness (e.g., bounds on the $k$ -th order derivative of $\mu _ { ( w ) } ( z )$ for $k \geq 3$ ) then we could improve rates of convergence via local regression with higher-order polynomials.

Given this assumption, we can directly bound the error rate of (8.4). The following result gives the rate of convergence of local linear regression along with a proof sketch. We refer to Imbens and Kalyanaraman [2012] for a more precise argument, along with guidance on how to choose the scale parameter $\kappa$ for the bandwidth $h _ { n }$ .

Proposition 8.1. Consider an RDD where the running variable has a continuous distribution around the cutoff, and Var $\left[ Y _ { i } | Z _ { i } = z \right] \leq \sigma ^ { 2 }$ for all $z$ . Suppose

furthermore that (8.5) holds for some $B > 0$ . Then, the local linear regression estimator (8.4) with bandwidth $h _ { n } = \kappa n ^ { - 1 / 5 }$ for some $\kappa > 0$ is consistent, and has errors scaling as

$$
\hat {\tau} _ {c} = \tau_ {c} + \mathcal {O} _ {P} \left(n ^ {- 2 / 5}\right). \tag {8.6}
$$

Proof sketch. We start by taking a Taylor expansion around $c$ , which yields

$$
\mu_ {(w)} (z) = a _ {(w)} + \beta_ {(w)} (z - c) + \rho_ {(w)} (z - c), \quad | \rho_ {(w)} (x) | \leq B x ^ {2} / 2, \tag {8.7}
$$

while noting that $\tau _ { c } = a _ { ( 1 ) } - a _ { ( 0 ) }$ . Moreover, by inspection of the problem (8.4), we see that it factors into two separate regression problems on the treated and control samples, namely

$$
\hat {a} _ {(1)}, \hat {\beta} _ {(1)} = \operatorname {a r g m i n} _ {a, \beta} \left\{\sum_ {Z _ {i} \geq c} K \left(\frac {| Z _ {i} - c |}{h _ {n}}\right) \left(Y _ {i} - a - \beta \left(Z _ {i} - c\right)\right) ^ {2} \right\}, \tag {8.8}
$$

for the treated units and an analogous problem for the controls, such that $\hat { \tau } = \hat { a } _ { ( 1 ) } - \hat { a } _ { ( 0 ) }$ .

Now, for simplicity, we focus on local linear regression with the basic window kernel $K ( x ) = 1 ( \{ | x | \leq 1 \} )$ . The linear regression problem (8.8) can then be solved in closed form, and we get

$$
\hat {a} _ {(1)} = \sum_ {c \le Z _ {i} \le c + h _ {n}} \gamma_ {i} Y _ {i}, \quad \gamma_ {i} = \frac {1}{n _ {h} ^ {+}} \frac {\widehat {\mathbb {E}} _ {(1)} \left[ (Z _ {i} - c) ^ {2} \right] - \widehat {\mathbb {E}} _ {(1)} \left[ Z _ {i} - c \right] \cdot (Z _ {i} - c)}{\widehat {\mathbb {E}} _ {(1)} \left[ (Z _ {i} - c) ^ {2} \right] - \widehat {\mathbb {E}} _ {(1)} \left[ Z _ {i} - c \right] ^ {2}}, \mathrm {w h e r e}
$$

$$
\widehat {\mathbb {E}} _ {(1)} [ Z _ {i} - c ] = \frac {1}{n _ {h} ^ {+}} \sum_ {c \leq Z _ {i} \leq c + h _ {n}} (Z _ {i} - c), \quad n _ {h} ^ {+} = | \{i: c \leq Z _ {i} \leq c + h _ {n} \} |, \tag {8.9}
$$

etc., denote sample averages over the regression window. Direct calculation reveals that $\begin{array} { r } { \sum _ { c \leq Z _ { i } \leq c + h _ { n } } \gamma _ { i } = 1 } \end{array}$ and $\begin{array} { r } { \sum _ { c \leq Z _ { i } \leq c + h _ { n } } \gamma _ { i } ( Z _ { i } - c ) = 0 } \end{array}$ , and so by (8.7)

$$
\hat {a} _ {(1)} = a _ {(1)} + \underbrace {\sum_ {c \leq Z _ {i} \leq c + h _ {n}} \gamma_ {i} \rho_ {(1)} \left(Z _ {i} - c\right)} _ {\text {c u r v a t u r e b i a s}} + \underbrace {\sum_ {c \leq Z _ {i} \leq c + h _ {n}} \gamma_ {i} \left(Y _ {i} - \mu_ {(1)} \left(Z _ {i}\right)\right)} _ {\text {s a m p l i n g n o i s e}}, \tag {8.10}
$$

and a similar expansion holds for $\hat { a } _ { ( 0 ) }$ . Thus, recalling that our estimator is $\hat { \tau } = \hat { a } _ { ( 1 ) } - \hat { a } _ { ( 0 ) }$ and out target estimand is $\tau _ { c } = a _ { ( 1 ) } - a _ { ( 0 ) }$ , we see that it suffices to bound the error terms in (8.10).

Given our bias on the curvature, we immediately see that the ‚Äúcurvature bias‚Äù term is bounded by $B h _ { n } ^ { 2 } / 2$ because $\rho _ { ( 1 ) } ( x )$ is only evaluated at points with $| x | \leq h _ { n }$ . Meanwhile, the sampling noise term is mean-zero and, provided that Var $\left\lfloor Y _ { i } \right\rfloor Z _ { i } \rfloor \leq \sigma ^ { 2 }$ , has variance bounded on the order of $\begin{array} { r } { \sigma ^ { 2 } \sum _ { c \leq Z _ { i } \leq c + h _ { n } } \gamma _ { i } ^ { 2 } } \end{array}$ .

Finally, assuming that $Z _ { i }$ has a continuous non-zero density function $f ( z )$ in a neighborhood of $z$ , one can check that

$$
\sigma^ {2} \sum_ {c \leq Z _ {i} \leq c + h _ {n}} \gamma_ {i} ^ {2} \approx \frac {4 \sigma^ {2}}{| \{i : c \leq Z _ {i} \leq c + h _ {n} \} |} \approx \frac {4 \sigma^ {2}}{f (c)} \frac {1}{n h _ {n}}. \tag {8.11}
$$

The squared bias of $\hat { \tau }$ thus scales as $h _ { n } ^ { 4 }$ , while its variance scales as $1 / ( h _ { n } n )$ . The bias-variance trade-off is minimized at $h _ { n } \sim n ^ { - 1 / 5 }$ , resulting in (8.6).

Remark 8.1. The $n ^ { - 2 / 5 }$ rate is a consequence of working with bounds on the 2nd derivative of $\mu _ { ( w ) } ( z )$ . In general, if we assume that $\mu _ { ( w ) } ( z )$ has a bounded $k$ -th order derivative, then we can achieve an $n ^ { - k / ( 2 k + 1 ) }$ rate of convergence for $\tau _ { c }$ by using local polynomial regression of order $( k - 1 )$ with a bandwidth scaling as $h _ { n } \sim n ^ { - 1 / ( 2 k + 1 ) }$ .48 Local linear regression never achieves a parametric rate of convergence, but can get close if $\mu _ { ( w ) } ( z )$ is very smooth.

Remark 8.2. While Proposition 8.1 provides bounds on the estimation error of local linear regression, it does not directly induce a method for inference about $\tau _ { c }$ . This is because, when using a bandwidth that scales at the estimation-erroroptimal rate $h _ { n } \sim n ^ { - 1 / 5 }$ , both the bias and standard error of $\hat { \tau } _ { c }$ . This means that standard tools for building confidence intervals using linear regression‚Äî which only account for variance but not bias‚Äîwill understate the size of the errors in $\hat { \tau } _ { c }$ and generally not achieve nominal coverage rates. One simple way to address this challenges is to rely on ‚Äúundersmoothing‚Äù, and pick $h _ { n } \ll n ^ { - 1 / 5 }$ so that variance dominates bias. This strategy, however, is generally not recommended, as undersmoothing results in larger-than-optimal estimation error; and furthermore it is challenging to choose an undersmoothing bandwidth in such a way as to credibly get good coverage in finite samples. A better approach is to use bias-corrections that leverage higher-order smoothness; discussing how to do so is however beyond the scope of this presentation, and we instead refer to Calonico, Cattaneo, and Titiunik [2014] for details on this approach.

# 8.2 Optimized estimation and bias-aware inference

We showed above that the conditional expectation functions have bounded curvature as in (8.5) and $Z _ { i }$ has a continuous non-zero density around $c$ (meaning that there will asymptotically be datapoints with $Z _ { i }$ arbitrarily close to $c$ ), then local linear regression can estimate $\tau _ { c }$ in an RDD with errors that decay

as $n ^ { - 2 / 5 }$ . Now, while this result is helpful conceptually and also motivates a simple estimator, some applications have features that preclude direct application of this result. First, the asymptotic argument underlying (8.3) relies on observing data $Z _ { i }$ arbitrarily close to the cutoff $c$ . In practice, however, we often have to work with discrete running variables (e.g., $Z _ { i }$ is a test score that takes integers value between 0 and 100), and in these cases the asymptotics underlying Proposition 8.1 do not apply. Moreover, in many applications, we need to work with more complicated cutoff functions (e.g., a student needs to pass 2 out of 3 tests to be eligible for a program), and it is not immediately clear how to adapt local linear regression to such settings in a way that preserves statistical power.

Linear estimators for RDD In order to address these challenges and develop estimators for a more general class of RDDs, we start with an abstract observation. In the proof of Proposition 8.1, we noted that we can write the local linear estimator as

$$
\hat {\tau} _ {c} (\gamma) = \sum_ {i = 1} ^ {n} \gamma_ {i} Y _ {i}. \tag {8.12}
$$

for some weights $\gamma _ { i }$ that only depend on the running variable $Z _ { i }$ ; the specific form of the weights induced by local linear regession with a window kernel $K ( x ) = 1 ( \{ | x | \leq 1 \} )$ is given in (8.9). We refer to estimators of this form as linear estimators because they are linear functions of the outome vector $Y$ . 4 9

Now, although the local linear regression estimator (8.4) was motivated by a regression problem, we didn‚Äôt make much use of this regression formulation in studying $\hat { \tau } _ { c }$ . Instead, for our formal discussion, we just used general properties of that hold for all linear estimators of the form (8.12).

For simplicity, consider for now a setting with homoskedatic and Gaussian errors, such that $Y _ { i } ( w ) = \mu _ { ( w ) } ( Z _ { i } ) + \varepsilon _ { i } ( w )$ with $\varepsilon _ { i } ( w ) \mid Z _ { i } \sim \mathcal { N } \left( 0 , \sigma ^ { 2 } \right)$ . Then, any linear estimator (8.12) whose weights $\gamma _ { i }$ are only functions of the $Z _ { i }$ satisfies

$$
\hat {\tau} _ {c} (\gamma) \mid \left\{Z _ {1}, \dots , Z _ {n} \right\} \sim \mathcal {N} \left(\hat {\tau} _ {c} ^ {*} (\gamma), \sigma^ {2} \| \gamma \| _ {2} ^ {2}\right),
$$

$$
\hat {\tau} _ {c} ^ {*} (\gamma) = \sum_ {i = 1} ^ {n} \gamma_ {i} \mu_ {W _ {i}} (Z _ {i}), \tag {8.13}
$$

where $W _ { i } = 1 \left( \left\{ Z _ { i } \geq c \right\} \right)$ . Thus, we immediately see that any linear estimator as in (8.12) will be an accurate estimator for $\tau _ { c }$ provided we can guarantee that $\hat { \tau } _ { c } ^ { * } \left( \gamma \right) \approx \tau _ { c }$ and $\| \gamma \| _ { 2 } ^ { 2 }$ is small.

Minimax linear estimation Motivated by this observation, it‚Äôs natural to ask: If the salient fact about local linear regression (8.4) is that we can write it as an linear estimator of the form (8.12), then is local linear regression the best estimator in this class? As we‚Äôll see below, the answer is no; however, the best estimator of the form (8.12) can readily be derived in practice via numerical convex optimization.

As noted in (8.13), the conditional variance of any linear estimator can directly be observed: It‚Äôs just $\sigma ^ { 2 } \left\| \gamma \right\| _ { 2 } ^ { 2 }$ (under our working model with homoskedastic errors). In contrast, the bias of linear estimators depends on the unknown functions $\mu _ { ( w ) } ( z )$ , and so cannot be observed:

$$
\operatorname {B i a s} \left(\hat {\tau} _ {c} (\gamma) \mid \{Z _ {1}, \dots , Z _ {n} \}\right) = \sum_ {i = 1} ^ {n} \gamma_ {i} \mu_ {W _ {i}} (Z _ {i}) - \left(\mu_ {(1)} (c) - \mu_ {(0)} (c)\right). \tag {8.14}
$$

However, although, this bias is unknown, it can still readily be bounded given smoothness assumptions on the $\mu _ { ( w ) } ( z )$ . For example, if the curvature of $\mu _ { ( w ) } ( z )$ is assumed to be bounded by $B$ for all $z$ as in (8.5), then50

$$
\left| \operatorname {B i a s} \left(\hat {\tau} _ {c} (\gamma) \mid \{Z _ {1}, \dots , Z _ {n} \}\right) \right| \leq I _ {B} (\gamma)
$$

$$
I _ {B} (\gamma) = \sup  \left\{\sum_ {i = 1} ^ {n} \gamma_ {i} \mu_ {W _ {i}} (Z _ {i}) - \left(\mu_ {(1)} (c) - \mu_ {(0)} (c)\right): \left| \mu_ {(w)} ^ {\prime \prime} (z) \right| \leq B \right\}. \tag {8.15}
$$

Now, recall that the mean-squared error of an estimator is just the sum of its variance and squared bias. Because the variance term $\sigma ^ { 2 } \left\| \gamma \right\| _ { 2 } ^ { 2 }$ doesn‚Äôt depend on the conditional response functions, we thus see that the worst-case mean squared error of any linear estimator over all problems with $| \mu _ { ( w ) } ^ { \prime \prime } ( z ) | \le B$ is just the sum of its variance and worst-case bias squared, i.e.,

$$
\operatorname {M S E} \left(\hat {\tau} _ {c} (\gamma) \mid \{Z _ {1}, \dots , Z _ {n} \}\right) \leq \sigma^ {2} \| \gamma \| _ {2} ^ {2} + I _ {B} ^ {2} (\gamma), \tag {8.16}
$$

with equality at any function that attains the worst-case bias (8.15).

It follows that, under an assumption that $| \mu _ { ( w ) } ^ { \prime \prime } ( z ) | \le B$ and conditionally on $\left\{ Z _ { 1 } , . . . , Z _ { n } \right\}$ , the minimax mean-squared error linear estimator of the form (8.12) is the one that minimizes (8.16):

$$
\hat {\tau} _ {c} (\gamma^ {B}) = \sum_ {i = 1} ^ {n} \gamma_ {i} ^ {B} Y _ {i}, \quad \gamma^ {B} = \operatorname {a r g m i n} \left\{\sigma^ {2} \| \gamma \| _ {2} ^ {2} + I _ {B} ^ {2} (\gamma) \right\}. \tag {8.17}
$$

One can check numerically that the weights implied by local linear regression do not solve this optimization problem, and so the estimator (8.17) dominates local linear regression in terms of worst-case MSE.

Deriving the minimax linear weights Of course, the estimator (8.17) is not of much use unless we can solve for the weights $\gamma _ { i } ^ { B }$ in practice. Luckily, we can do so via routine quadratic programming. To do so, it is helpful to write

$$
\mu_ {(w)} (z) = a _ {(w)} + \beta_ {(w)} (z - c) + \rho_ {(w)} (z), \tag {8.18}
$$

where $\rho _ { ( w ) } ( z )$ is a function with $\rho _ { ( w ) } ( c ) = \rho _ { ( w ) } ^ { \prime } ( c ) = 0$ and whose second derivative is bounded by B; given this representation œÑc = a(1) ‚àí a(0). $B$ $\tau _ { c } = a _ { ( 1 ) } - a _ { ( 0 ) }$

Now, the first thing to note in (8.18) is that the coefficients $a _ { ( w ) }$ and $\beta _ { ( w ) }$ are unrestricted. Thus, unless the weights $\gamma _ { i }$ account for them exactly, i.e.,

$$
\sum_ {i = 1} ^ {n} \gamma_ {i} W _ {i} = 1, \sum_ {i = 1} ^ {n} \gamma_ {i} = 0, \sum_ {i = 1} ^ {n} \gamma_ {i} (Z _ {i} - c) _ {+} = 0, \sum_ {i = 1} ^ {n} \gamma_ {i} (Z _ {i} - c) _ {-} = 0,
$$

we can choose $a _ { ( w ) }$ and $\beta _ { ( w ) }$ to make the bias of $\hat { \tau } _ { c } ( \gamma )$ arbitrarily bad (i.e., $I _ { B } ( \gamma ) = \infty$ ). Meanwhile, once we enforce these constraints, it only remains to bound the bias due to $\rho _ { ( w ) } ( z )$ , and so we can re-write (8.17) as

$$
\left\{\gamma^ {B}, t \right\} = \operatorname {a r g m i n} \quad \sigma^ {2} \left\| \gamma \right\| _ {2} ^ {2} + B ^ {2} t ^ {2}
$$

$$
\text {s u b j e c t} \sum_ {i = 1} ^ {n} \gamma_ {i} W _ {i} \rho_ {(1)} (Z _ {i}) + \sum_ {i = 1} ^ {n} \gamma_ {i} (1 - W _ {i}) \rho_ {(0)} (Z _ {i}) \leq t
$$

$$
\text {f o r a l l} \rho_ {(w)} (\cdot) \text {w i t h} \rho_ {(w)} (c) = \rho_ {(w)} ^ {\prime} (c) = 0
$$

$$
\text {a n d} \left| \rho_ {(w)} ^ {\prime \prime} (z) \right| \leq 1 \tag {8.19}
$$

$$
\sum_ {i = 1} ^ {n} \gamma_ {i} W _ {i} = 1, \quad \sum_ {i = 1} ^ {n} \gamma_ {i} = 0,
$$

$$
\sum_ {i = 1} ^ {n} \gamma_ {i} W _ {i} (Z _ {i} - c) = 0, \sum_ {i = 1} ^ {n} \gamma_ {i} (Z _ {i} - c) = 0.
$$

Given this form, the optimization should hopefully look like a tractable one. And in fact it is: The problem simplifies once we take its dual, and it can then be well approximated by a finite-dimensional quadratic program where we use a discrete approximation to the set of functions with second derivative bounded by 1; see Imbens and Wager [2019, Section II.B] for details.

Bias-aware inference The above discussion suggests that using an estimator $\begin{array} { r } { \hat { \tau } _ { c } \left( \gamma ^ { B } \right) = \sum _ { i = 1 } ^ { n } \gamma _ { i } ^ { B } Y _ { i } } \end{array}$ with weights chosen via (8.19) results in a good point estimate for for $\tau _ { c }$ if all we know is that $| \mu _ { ( w ) } ^ { \prime \prime } ( z ) | \le B$ . In particular, under this assumption and conditionally on $\left\{ Z _ { 1 } , . . . , Z _ { n } \right\}$ , it attains minimax meansquared error among all linear estimators. Because local linear regression is also

a linear estimator, we thus find that $\hat { \tau } _ { c } \left( \gamma ^ { B } \right)$ dominates local linear regression in a minimax sense.

If we want to use $\hat { \tau } _ { c } \left( \gamma ^ { B } \right)$ in practice, though, it‚Äôs important to be able to also provide confidence intervals for $\tau _ { c }$ . And, since $\hat { \tau } _ { c } \left( \gamma ^ { B } \right)$ balances out bias and variance by construction, we should not expect our estimator to be variance dominated‚Äîand any inferential procedure should account for bias.

To this end, recall (8.13), whereby conditionally on $\left\{ Z _ { 1 } , . . . , Z _ { n } \right\}$ , the errors of our estimator, err $: = \hat { \tau } _ { c } - \tau _ { c }$ , are distributed as

$$
\operatorname {e r r} \left| \left\{Z _ {1}, \dots , Z _ {n} \right\} \sim \mathcal {N} \left(\operatorname {b i a s}, \sigma^ {2} \left\| \gamma^ {B} \right\| _ {2} ^ {2}\right). \right. \tag {8.20}
$$

Furthermore, the optimization problem (8.19) yields as a by-product an upper bound for the bias in terms of the optimization variable $t$ , namely $| \mathrm { b i a s } | \le B t$ .

We can then use these facts to build confidence intervals as follows. Because the Gaussian distribution is unimodal and symmetric,

$$
\mathbb {P} \left[ | \mathrm {e r r} | \geq \zeta \right] \leq \mathbb {P} \left[ \left| B t + \sigma \left\| \gamma^ {B} \right\| _ {2} S \right| \geq \zeta \right], \quad S \sim \mathcal {N} (0, 1). \tag {8.21}
$$

Thus, we obtain level- $\alpha$ confidence intervals as follows:

$$
\mathbb {P} \left[ \tau_ {c} \in \mathcal {I} _ {\alpha} \mid \{Z _ {1}, \dots , Z _ {n} \} \right] \geq 1 - \alpha ,
$$

$$
\mathcal {I} _ {\alpha} = \left(\hat {\tau} _ {c} (\gamma^ {B}) - \zeta_ {\alpha} ^ {B}, \hat {\tau} _ {c} (\gamma^ {B}) + \zeta_ {\alpha} ^ {B}\right), \tag {8.22}
$$

$$
\zeta_ {\alpha} ^ {B} = \inf \left\{\zeta : \mathbb {P} \left[ \left| B t + \sigma \left\| \gamma^ {B} \right\| _ {2} S \right| > \zeta \right] \leq \alpha , S \sim \mathcal {N} (0, 1) \right\}.
$$

In addition to formally accounting for bias, note that these intervals hold conditionally on $Z _ { i }$ , and so hold without any distributional assumptions on the running variable. This is useful when considering regression discontinuities in non-standard settings.

Application: Discrete running variable A first example of the usefulness of having conditional-on- $Z _ { i }$ guarantees is when the running variable $Z _ { i }$ has discrete support. In this case, the regression-discontinuity parameter $\tau _ { c }$ is in general not point-identified under only the assumption $| \mu _ { ( w ) } ^ { \prime \prime } ( z ) | \le B$ because there may not be any data arbitrarily close to the boundary.51 And, without point identification, any approach to inference that relies on asymptotics with

specific rates of convergence for $\hat { \tau } _ { c }$ as discussed in the previous lecture clearly is not applicable.

In contrast, in our case, the fact that $Z _ { i }$ may have discrete support changes nothing. The confidence intervals (8.22) have coverage conditionally on $\left\{ Z _ { 1 } , . . . , Z _ { n } \right\}$ , and the empirical support $\left\{ Z _ { 1 } , . . . , Z _ { n } \right\}$ of the running variable is always discrete, so the question of whether the $Z _ { i }$ have a density in the population is irrelevant when working with (8.22). The relevance of a discrete $Z _ { i }$ only comes up asymptotically: If $Z _ { i }$ has a continuous density, then the confidence intervals (8.22) will shrink asymptotically at the optimal rate discussed in last lecture, namely $n ^ { - 2 / 5 }$ . Conversely, if the $Z _ { i }$ has discrete support, the length of the confidence intervals will not go to 0; rather, we end up in a partial identification problem. In this context, we also note that the bias-aware intervals (8.22) corresponds exactly to a type of confidence interval for partially identified parameters proposed in Imbens and Manski [2004].

Application: Multivariate running variable So far, we have focused on regression discontinuity designs where treatment is determined by a single threshold: $W _ { i } = 1 \left( \left\{ Z _ { i } \geq c \right\} \right)$ for some $Z _ { i } \in \mathbb { R }$ . However, the ideas discussed here apply in considerably more generality: One can let the running variable $Z _ { i } ~ \in ~ \mathbb { R } ^ { k }$ be multivariate, and the treatment region be generic, i.e., $W _ { i } \ =$ $1 \left( \{ Z _ { i } \in \mathcal { A } \} \right)$ for some set $\mathcal { A } \subset \mathbb { R } ^ { k }$ . For example, in an educational setting, $Z _ { i } \in \mathbb { R } ^ { 3 }$ could measure test results in 3 separate subjects, and $\mathcal { A }$ could denote the set of overall ‚Äúpassing‚Äù results given by, e.g., 2 out of 3 tests clearing a pass/fail cutoff. Or in a geographic regression discontinuity design, $Z _ { i } \in \mathbb { R } ^ { 2 }$ could denote the location of one‚Äôs household and $\mathcal { A }$ the boundary of some administrative region that deployed a specific policy.

The crux of a regression discontinuity design is that we seek to identify causal effects via sharp changes to an existing treatment assignment policy; and we can then apply the same reasoning as before to identify treatment effects along the boundary of the treatment region $\mathcal { A }$ . That being said, while the extension of regression discontinuity designs to general multivariate settings is conceptually straight-forward, the methodological extensions require some more care. In particular, it is not always clear what the best way is to generalize local linear regression to a geographic regression discontinuity design.52

The minimax linear approach, however, extends direction to a multivariate setting. When working with a multivariate running variable, one can essentially

write down (8.19) verbatim, and interpret the resulting weighted estimator similarly to before. The resulting optimization problem is harder (one needs to optimize over multivariate non-parametric functions with bounded curvature), but nothing changes conceptually.

Beyond homoskedaticity So far, we have focused on estimation and inference in the case where the noise $\varepsilon _ { i } = Y _ { i } - \mu _ { ( W _ { i } ) } ( Z _ { i } )$ was Gaussian with a known constant variance parameter $\sigma ^ { 2 }$ . In practice, of course, neither of these assumptions is likely to hold. The upshot is that the conditional Gaussianity result (8.20) no longer holds exactly; rather, we need to invoke a central limit theorem to argue that

$$
\hat {\tau} _ {c} (\gamma) \mid \{Z _ {1}, \dots , Z _ {n} \} \approx \mathcal {N} \left(\hat {\tau} _ {c} ^ {*} (\gamma), \sum_ {i = 1} ^ {n} \gamma_ {i} ^ {2} \operatorname {V a r} \left[ Y _ {i} \mid Z _ {i}, W _ {i} \right]\right). \tag {8.23}
$$

However, provided we‚Äôre willing to make assumptions under which the Gaussian approximation above is valid, we can still proceed as above to get confidence intervals. Meanwhile, we can (conservatively) estimate the conditional variance in (8.23) via

$$
\widehat {V} _ {n} = \sum_ {i = 1} ^ {n} \gamma_ {i} ^ {2} \left(Y _ {i} - \hat {\mu} _ {\left(W _ {i}\right)} \left(Z _ {i}\right)\right) ^ {2}, \tag {8.24}
$$

where, e.g., $\hat { \mu } _ { \left( W _ { i } \right) } ( Z _ { i } )$ is derived via local linear regression; note that this bound is conservative if $\hat { \mu } _ { \left( W _ { i } \right) } ( Z _ { i } )$ is misspecified, since then the misspecifiaction error will inflate the residuals.

That being said, one should emphasize that the estimator (8.17) is only minimax under homoskedastic errors with variance $\sigma ^ { 2 }$ ; if we really wanted to be minimax under heteroskedasticity then we‚Äôd need to use per-parameter variances $\sigma _ { i } ^ { 2 }$ in (8.19). Thus, one could argue that an analyst who uses the estimator (8.17) but builds confidence intervals via (8.23) and (8.24) is using an oversimplified homoskedastic model to motivate a good estimator, but then out of caution and rigor uses confidence intervals that allow for heteroskedasticity when building confidence intervals. This is generally a good idea, and in fact something that‚Äôs quite common in practice (from a certain perspective, anyone who runs OLS for point estimation but then gets confidence intervals via the bootstrap is doing the same thing); however, it‚Äôs important to be aware that one is making this choice.

Remark 8.3. Throughout this section, we assumed that the researcher knows that (8.5) holds with some specific $B$ , and proceeded accordingly. In practice, however, the researcher needs to choose $B$ , and this is a delicate task. The

data itself cannot be used to learn $B$ unless one makes further smoothness assumptions [Armstrong and Koles¬¥ar, 2018]. Armstrong and Koles¬¥ar [2020] and Imbens and Wager [2019] propose some heuristics for conservative choices of $B$ that rely on global estimation of higher-order polynomials. Eckles et al. [2025] consider a structural model for the running variable that, among other things, implies a theory-driven bound $B$ that can be used in (8.5).

# Bibliographic notes

The idea of using regression discontinuity designs for treatment effect estimation goes back to Thistlethwaite and Campbell [1960]; however, most formal work in this area is more recent. The framework of identification in regression discontinuity designs via continuity arguments and local linear regression is laid out by Hahn, Todd, and van der Klaauw [2001]. Other references on regressiondiscontinuity analysis via local linear regression include Cheng, Fan, and Marron [1997] who discuss optimal choices for the kernel weighting function, Imbens and Kalyanaraman [2012] who discuss bandwidth choice, and Calonico, Cattaneo, Farrell, and Titiunik [2019] who discuss the role of covariate adjustments. Imbens and Lemieux [2008] provide an overview of local linear regression methods in this setting, and discuss alternative specifications such as the ‚Äúfuzzy‚Äù regression discontinuities where $W _ { i }$ is random but $\mathbb { P } \left\lfloor W _ { i } = 1 \right\rfloor Z _ { i } = z \rfloor$ has a jump at the cutoff $c$ .

As noted in Remark 8.2, the construction of confidence intervals via local linear regression is challenging because, when tuned for optimal mean-squared error, the bias and sampling error of the local linear regression estimator are of the same order‚Äîand so basic delta-method or bootstrap based inference fails (because it doesn‚Äôt capture bias). Several authors have considered solutions to the problem that rely on asymptotics. Calonico, Cattaneo, and Titiunik [2014] and Calonico, Cattaneo, and Farrell [2018] proposed bias-corrections to local linear regression to obtain valid confidence intervals. Meanwhile, Armstrong and Koles¬¥ar [2020] showed that uncorrected local linear regression point estimates can also be used for valid inference provided we inflate the length of the confidence intervals by a pre-determined amount; for example, in the setting of Proposition 8.1 with an mean-square-optimal bandwidth, their proposal would involve building 95% confidence intervals for $\tau _ { c }$ as $\hat { \tau } _ { c } \pm 2 . 1 8$ standard errors (rather than the familiar $\pm 1 . 9 6$ standard errors).

The study of minimax linear estimators as considered in Chapter 8.2 goes back to Donoho [1994], who showed to following result. Suppose that we want

to estimate $\theta$ using a Gaussian random vector $Y$ ,

$$
Y = K v + \varepsilon , \quad \varepsilon \sim \mathcal {N} (0, \sigma I), \quad \theta = a \cdot v, \tag {8.25}
$$

where the matrix $K$ and vector $a$ are know, but $\boldsymbol { v }$ is unknown. Suppose moreestimator, i.e., an estimator of the form over that $\boldsymbol { v }$ is known to belong to a convex set $\begin{array} { r } { \widehat { \theta } \stackrel { } { = } \sum _ { i = 1 } ^ { n } \gamma _ { i } Y _ { i } } \end{array}$ $\nu$ . Then, there exists a linear whose risk is within a factor 1.25 of the minimax risk among all estimators (including non-linear ones), and the weights $\gamma _ { i }$ for the minimax linear estimator can be derived via convex optimization. From this perspective, the minimax RDD estimator (8.17) is a special case of the estimators studied by Donoho [1994], and in fact his results imply that this estimator is nearly minimax among all estimators (not just linear ones).

In a first application of this principle to regression discontinuity designs, Armstrong and Koles¬¥ar [2018] study minimax linear estimation over a class of function proposed by Sacks and Ylvisaker [1978] for which Taylor approximations around the cutoff $c$ are nearly sharp. Our presentation in Chapter 8.2 is adapted from Imbens and Wager [2019], who consider numerical convex optimization for flexible inference in generic regression discontinuity designs. Koles¬¥ar and Rothe [2018] advocate worst-case bias measures of the form (8.15) as a way of avoiding asymptotics and providing credible confidence intervals in regression discontinuity designs with a discrete running variable. Noack and Rothe [2024] extend methods for bias-aware inference to fuzzy regression discontinuities.

# Chapter 9 Structural Equation Models

When discussing methods for treatment effect estimation under unconfoundedness, we have effectively assumed that‚Äîpotentially after conditioning on observed covariates‚Äîthe treatment assignment is determined by as-good-asrandom factors that are irrelevant to the causal inference question at hand. In other words, we have effectively assumed treatment assignment is exogenous to the system we are studying.

In some applications, however, such exogeneity assumptions are simply not plausible. For example, when studying the effect of prices on demand, it is unrealistic to assume that potential outcomes of demand (i.e., what demand would have been at given prices) are independent of what prices actually were. Instead, it‚Äôs much more plausible to assume that prices and demand both respond to each other until a supply-demand equilibrium is reached.

This chapter‚Äîand the next one‚Äîpresent basic methods and concepts for causal inference in settings where unconfoundedness does not hold and treatment assignment is instead endogenous, i.e., treatments are assigned in a way that depends on the interplay of other variables within the system. We start by introducing non-parametric structural equation models (SEMs) as a general tool for reasoning about causal inference with endogenous treatment. In some settings, SEMs can be used to prove that unconfoundedness holds (although it may not have been obvious that it does a-priori), while in other settings SEMs can be used to motivate new methods for causal inference without unconfoundedness. Then, in Section 9.2, we consider a class of semiparametric SEMs where treatment effects are assumed to be constant, and introduce instrumental variables regression as a powerful and flexible method for causal inference in such settings. Finally, in Chapter 10, we revisit instrumental variables using a potential outcomes specification that‚Äôs more explicitly related to the causal models we‚Äôve used so far.

Structural models and causal graphs It is convenient to describe structural equation models using directed acyclic graphs (DAGs). A directed graph with nodes indexed $j = 1$ , . . . , $p$ is characterized by a set of edges $\{ E _ { i j } \}$ where $E _ { i j } = 1$ denotes the presence of an edge from node $i$ to node $j$ and $E _ { i j } = 0$ denotes lack of such an edge. Within a directed graph, a directed path is an ordered set of at least two nodes $i _ { 1 }$ , $i _ { 2 }$ , . . . , $i _ { k } \in \{ 1 , \ldots , p \}$ such that $E _ { i _ { 1 } i _ { 2 } } = E _ { i _ { 2 } i _ { 3 } } = . . . = E _ { i _ { k - 1 } i _ { k } } = 1$ ; the definition of an undirected path is analogous except it only requires that either $E _ { i _ { j } i _ { j + 1 } } = 1$ or $E _ { i _ { j + 1 } i _ { j } } = 1$ along the path. A directed graph is acyclic (i.e., a DAG) if it contains no directed cycles, i.e., directed paths with $i _ { 1 } = i _ { k }$ . Within a DAG, we say that that a node $i$ is upstream of $j$ (and that $j$ is downstream of $i$ ) if there exists a directed path starting at $i$ and ending at $j$ . We define the set of parents of node $j$ as the set of nodes $i$ with $E _ { i j } = 1$

Now, let ( $Z _ { 1 }$ , ..., $Z _ { p }$ ) denote a set of $p$ random variables relevant to a system we want to make causal queries in. Some of the variables $Z _ { j }$ may be observed by the researcher, while others may not. We say that $Z$ is generated by a structural equation model (SEM) if there exists a DAG $G$ with nodes corresponding to $Z _ { 1 }$ , . . . , $Z _ { p }$ and with edge set $\{ E _ { i j } \}$ such that

$$
Z _ {j} = f _ {j} \left(p a _ {j} (G), \varepsilon_ {j}\right), \tag {9.1}
$$

where $p a _ { j } ( G )$ stands for the parents of $Z _ { j }$ in the graph $G$ (i.e., $p a _ { j } ( G ) =$ $\{ Z _ { i } : E _ { i j } = 1 \}$ ) and the $\varepsilon _ { j } \sim F _ { j } ^ { \prime }$ are mutually independent noise terms. The key assumption here is that relationship (9.1) holds regardless of the distribution of the $\varepsilon _ { j }$ , i.e., that this model describes the structure of the data-generation process and not just its correlational structure.

Example 8. Meinshausen et al. [2016] use structural equation models to study the relationship between the expression of different genes in the yeast saccharomyces cerevisiae. The authors have access to expression levels for 6,170 genes and are interested in questions of the type: How will the expression of gene $i$ in the yeast be affected by inactivating gene $j$ ? To formalize this question, they posit that gene expressions can be modeled using a linear SEM, i.e., that there is a DAG $G$ with edges $\{ E _ { i j } \}$ such that

$$
Z _ {i} = \sum_ {\{j: E _ {j i} = 1 \}} \beta_ {i j} Z _ {j} + \varepsilon_ {i},
$$

where $Z _ { i }$ measures the expression level of the $i$ -th gene. The statistical task then reduces to estimating $\beta _ { i j }$ in this model. They estimate these quantities using the method of Peters, B¬®uhlmann, and Meinshausen [2016] which assumes cross-environment invariance of the SEM coefficients to identify causal effects.

Given a SEM (9.1), a causal query exogenously sets the values of some nodes of the graph $G$ to pre-specified values, and examines how this affects the distribution of other nodes. Given two disjoint sets of nodes $W$ , $Y \subset Z$ , the interventional distribution that arises from setting $W$ to $w$ on $Y$ is written $\mathbb { P } \left[ Y | d o ( W = w ) \right]$ , and corresponds to deleting all equations used to generate $W$ in (9.1) and plugging in $w$ for $W$ in the rest.53 In the case of a causal query on a single node $j$ , $d o ( Z _ { j } = z _ { j }$ ) the modification to the SEM (9.1) is simply

$$
Z _ {j} = f _ {j} \left(p a _ {j} (G), \varepsilon_ {j}\right), \quad Z _ {j} = z _ {j}. \tag {9.2}
$$

A first, key results about SEMs is that the observational distribution of $Z$ and any interventional distribution arising from a causal query both admit simple factorizations in terms of the graph $G$ .

Proposition 9.1. Let $Z = \{ Z _ { 1 } , . . . , Z _ { p } \}$ be a structural equation model with underlying DAG G. The observational distribution of $Z$ can be factored as

$$
\mathbb {P} [ Z ] = \prod_ {j = 1} ^ {p} \mathbb {P} \left[ Z _ {j} \mid p a _ {j} (G) \right]. \tag {9.3}
$$

Furthermore, for any set $A \subset \{ 1 , \ldots , p \}$ , the interventional distribution that arises from fixing all elements in $\mathcal { A }$ using a causal query can be factored as

$$
\mathbb {P} \left[ Z \mid \{d o \left(Z _ {k} = z _ {k}\right) \} _ {k \in \mathcal {A}} \right] = 1 \left(\left\{Z _ {k} = z _ {k}: k \in \mathcal {A} \right\}\right) \prod_ {j \notin \mathcal {A}} \mathbb {P} \left[ Z _ {j} \mid p a _ {j} (G) \right]. \tag {9.4}
$$

Proof. Because $G$ is directed and acyclic, we can create an ordering of the nodes $i ( 1 )$ , i(2), . . . , $i ( p )$ such that all edges in $G$ are aligned with this ordering (i.e., start from a lower-indexed node and end at a higher-indexed one). Without loss of generality, we can always factor the observational distribution of $Z$ as

$$
\mathbb {P} \left[ Z \right] = \prod_ {j = 1} ^ {p} \mathbb {P} \left[ Z _ {i (j)} \mid Z _ {i (1)}, \dots , Z _ {i (j - 1)} \right].
$$

Furthermore, thanks to our SEM assumption, for every $j$

$$
\begin{array}{l} \mathbb {P} \left[ Z _ {i (j)} \mid Z _ {i (1)}, \dots , Z _ {i (j - 1)} \right] = \mathbb {P} \left[ f _ {i (j)} \left(p a _ {i (j)} (G), \varepsilon_ {i (j)}\right) \mid Z _ {i (1)}, \dots , Z _ {i (j - 1)} \right] \\ = \mathbb {P} \left[ f _ {i (j)} \left(p a _ {i (j)} (G), \varepsilon_ {i (j)}\right) \mid p a _ {i (j)} (G) \right] \\ = \mathbb {P} \left[ Z _ {i (j)} \mid p a _ {i (j)} (G) \right], \\ \end{array}
$$

where for the second equality we used the fact that $\begin{array} { r l } { p a _ { i ( j ) } ( G ) } & { { } \subseteq } \end{array}$ $\left\{ Z _ { i ( 1 ) } , \ . . . , Z _ { i ( j - 1 ) } \right\}$ because our ordering is aligned with $G$ , and furthermore $\varepsilon _ { i ( j ) } \perp \perp \{ Z _ { i ( 1 ) } , ~ . ~ . ~ . , Z _ { i ( j - 1 ) } \}$ because these variables are non-descendants of $Z _ { i ( j ) }$ (again thanks to alignment of our ordering with $G$ ). The claim (9.3) then follows immediately. Meanwhile, the claim (9.4) can be established by applying the same argument to the graph modified as in (9.2); the key observation being that, after a causal query, the remaining variables can still be written as a SEM with graph $G$ . ‚ñ°

# 9.1 Non-parametric models and do-calculus

A first, natural question is: What can be said about causal inference in SEMs without making any further assumptions than those in (9.1)? This class of models is often referred to as non-parametric structural equation models (NPSEMs). Remarkably, questions of causal identification in NPSEMs can be answered using a unified framework, namely the ‚Äúdo-calculus‚Äù introduced by Pearl [1995]. The do-calculus is a set of algebraic transformations that can be used to re-express interventional distributions in terms of various conditional moments of the observational distribution, thus allowing the analyst to identify causal effects from observational data.

To understand do-calculus, we first need to formalize how graphs encode conditional independence statements in terms of $d$ -separation. Let $X$ , $Y$ and $Z$ denote disjoint sets of nodes, and write $( X \perp Y \mid Z ) _ { G }$ if $X ~ \bot ~ Y \mid Z$ for every SEM with underlying graph $G$ . Geiger, Verma, and Pearl [1990] showed that $( X \perp Y \mid Z ) _ { G }$ if and only if $X$ , $Y$ and $Z$ satisfy the following graphical condition referred to as $d$ -separation. Let $\xi$ be any undirected path from a node in $X$ to a node in $Y$ . We say that $Z$ blocks $\xi$ if there is a node $W$ on $\xi$ such that either (i) $W$ is a collider on $p$ (i.e., $W$ has two incoming edges along $\xi$ ) and neither $W$ nor any of its descendants are in $Z$ , or (ii) $W$ is not a collider and $W$ is in $Z$ . Then $Z$ $d$ -separates $X$ and $Y$ if it blocks every path between $X$ and $Y$ .

Do-calculus provides a way to simplify causal queries by referring to $d$ - separation on various sub-graphs of $G$ . To this end define $G _ { X }$ the subgraph of $G$ with all edges incoming to $X$ deleted, $G _ { \underline { { X } } }$ the subgraph of $G$ with all outgoing edges from $X$ deleted, $G _ { X { \overline { { Z } } } }$ the subgraph of $G$ with all outgoing edges from $X$ and incoming edges to $Z$ deleted, etc. Then, for any disjoint sets of edges $X$ , $Y$ , $Z$ , $W$ the following equivalence statements hold [Pearl, 1995].

1. Insertion/deletion of observations: If  Y ‚ä•‚ä• Z  W, XGW $( Y \perp Z | W , X ) _ { G _ { \overline { { { W } } } } }$ then

$$
\begin{array}{l} \mathbb {P} [ Y \mid d o (W = w), Z = z, X = x ] \\ \mathbb {P} [ Y \mid J (W = w) \dots , X = x ] \end{array} \tag {9.5}
$$

$$
= \mathbb {P} \left[ Y \mid d o (W = w), X = x \right].
$$

2. Action/observation exchange: If  Y ‚ä•‚ä• W  X, ZGWZ t $\left( Y \perp W \vert X , Z \right) _ { G _ { W } \overline { { { z } } } }$ hen

$$
\mathbb {P} \left[ Y \mid d o (W = w), X = x, d o (Z = z) \right] \tag {9.6}
$$

$$
= \mathbb {P} \left[ Y \mid W = w, X = x, d o (Z = z) \right].
$$

3. Insertion/deletion of actions: If $\left( Y \perp W \vert X , Z \right) _ { G _ { \overline { { { W ( X ) Z } } } } }$ where $W ( X )$ is the set of $W$ nodes that are not ancestors of any $X$ node in $G _ { \overline { { Z } } }$ , then

$$
\mathbb {P} \left[ Y \mid d o (W = w), X = x, d o (Z = z) \right] \tag {9.7}
$$

$$
= \mathbb {P} \left[ Y \mid X = x, d o (Z = z) \right].
$$

When applying the do-calculus, our goal is to apply these 3 rules of inference until we‚Äôve reduced a causal query to a query about observable moments of $\mathbb { P }$ , i.e., conditional expectations that do not involve the do-operator and that only depend on observed random variables. As shown in subsequent work, the docalculus is complete, i.e., if we cannot use the do-calculus to simply a causal query then it is not non-parametrically identified in terms of the structural equation model; see Pearl [2009] for a discussion and references.

Back-door identification Suppose have disjoint sets of nodes $X$ , $Y$ , $W$ , and want to query $\mathbb { P }$ $\left[ Y \mid d o ( W = w ) \right]$ . Suppose moreover that $X$ contains no nodes that are downstream for $W$ , and that $X$ $d$ -separates $W$ and $Y$ once we block all downstream edges from $W$ , i.e., that

$$
\left(Y \perp W \mid X\right) _ {G _ {W}}. \tag {9.8}
$$

Then, we can identify the effect of $W$ on $Y$ via

$$
\mathbb {P} [ Y \mid d o (W = w) ] = \sum_ {x} \mathbb {P} [ X = x ] \mathbb {P} [ Y \mid X = x, W = w ]. \tag {9.9}
$$

To verify (9.9), we can use the rules of do-calculus as follows:

$$
\begin{array}{l} \mathbb {P} \left[ Y \mid d o (W = w) \right] = \sum_ {x} \mathbb {P} \left[ X = x \mid d o (W = w) \right] \mathbb {P} \left[ Y \mid X = x, d o (W = w) \right] \\ = \sum_ {x} \mathbb {P} [ X = x ] \mathbb {P} [ Y \mid X = x, d o (W = w) ] \\ = \sum_ {x} \mathbb {P} [ X = x ] \mathbb {P} [ Y \mid X = x, W = w ], \\ \end{array}
$$

![](images/0b9af3d7034744ff1e7837c04082f6900f5091e707c4cb2588af0628a84e845d.jpg)  
Figure 9.1: In this DAG, $X$ , $Y$ and $W$ are observed but $U$ is unobserved.

where the first equality is just the chain rule, the second equality follows from rule #3 because $X$ is upstream from $W$ and so $( X \perp W ) _ { G _ { \overline { { { W } } } } }$ , and the third equality follows from rule #2 by (9.8).

The back-door criterion is closely related to unconfoundedness, and the identification strategy (9.9) exactly matches the standard regression adjustment under unconfoundedness. To understand the connection between (9.8) and unconfoundedness, consider the case where $Y$ and $W$ are both singletons and $W$ has no other downstream variables in $G$ other than $Y$ . Then, blocking downstream arrows from $W$ can be interpreted as leaving the effect of $W$ on $Y$ unspecified, and (9.8) implies

$$
F _ {Y} (w) \perp W | X, \tag {9.10}
$$

where $F _ { Y } ( w ) = f _ { Y } ( w , p a _ { Y } ^ { - } , \varepsilon _ { Y } )$ leaves all but the contribution of $w$ unspecified in (9.1) and $p a _ { Y } ^ { - }$ denotes the parents of $Y$ in $G _ { W }$ . We note that, given our SEM, (9.10) is exactly equivalent to unconfoundedness as assumed in (2.5). Thus, by Geiger, Verma, and Pearl [1990], we see that in the context of a SEM with underlying graph $G$ , unconfoundedness given a set of controls $X$ is guaranteed to hold (given restrictions implied by the SEM alone) if and only if (9.8) holds.

Verifying unconfoundedness The classical presentation based on unconfoundedness asks analysts to simply assert a conditional independence statement of the type (2.5). In many settings, this task is not hard; however, in some slightly more complicated stochastic models verifying unconfoundedness ‚Äúby inspection‚Äù may prove to be challenging. Consider, for example, the SEM given in Figure 9.1. Does unconfoundedness hold if we only condition on $X _ { 2 }$ ? What if we condition on $\{ X _ { 1 } , X _ { 2 } \}$ ?

![](images/b1df04cc76d746c640dfb3480c4f4938a9049d52787c7112e4e9d9b1ebd7500f.jpg)  
Figure 9.2: A DAG where front-door identification can by used. $W$ , $Z$ and $Y$ are observed, but $U$ is not.

One useful consequence of our finding on back-door identification is that, in the context of a non-parametric SEM, we can reason about whether unconfoundedness is plausible by using the graphical $d$ -separation rule to check whether (9.8). For example, by applying $d$ -separation to Figure 9.1, we see that (9.8) holds if we condition on $\{ X _ { 1 } , X _ { 2 } \}$ or $\{ X _ { 2 } , X _ { 3 } \}$ , but not if we only condition on $X _ { 2 }$ . Cinelli, Forney, and Pearl [2024] provide an in-depth discussion of practical considerations when using graphical criteria such as $d$ -separation to reason about which controls to use in an unconfoundedness analysis.

Front-door identification Another simple application of do-calculus arises in the graph illustrated in Figure 9.2. We still want to compute $\mathbb { P } \left[ Y | d o ( W = w ) \right]$ , but now do not observe $U$ and so cannot apply the backdoor criterion. However, if there exists a variable $Z$ which, like in the graph below, fully mediates the effect of $W$ on $Y$ without being affected by $U$ , we can use it for identification.

We proceed as follows. First, following the same line of argumentation as before, we see that

$$
\begin{array}{l} \mathbb {P} \left[ Y \mid d o (W = w) \right] = \sum_ {z} \mathbb {P} \left[ Z = z \mid d o (W = w) \right] \mathbb {P} \left[ Y \mid Z = z, d o (W = w) \right] \\ = \sum_ {z} \mathbb {P} \left[ Z = z \mid W = w \right] \mathbb {P} \left[ Y \mid Z = z, d o (W = w) \right], \\ \end{array}
$$

where the first equality is the chain rule and the second equality is from the back-door. We have to work a little harder to resolve the second term, however. Here, the main idea is to start by taking one step backwards before proceeding

![](images/a4fb382046d80306924c78fbeead9399858d395313075b7122ed7931ff2c1adb.jpg)  
Figure 9.3: A DAG representing a setting where instrumental variable methods may be used. An instrument $Z$ , a treatment $W$ , and an outcome $Y$ are all observed; but a confounder $U$ remains unobserved.

further:

$$
\begin{array}{l} \mathbb {P} \left[ Y \mid Z = z, d o (W = w) \right] = \mathbb {P} \left[ Y \mid d o (Z = z), d o (W = w) \right] \\ = \mathbb {P} \left[ Y \mid d o (Z = z) \right] \\ = \sum_ {w ^ {\prime}} \mathbb {P} \left[ W = w ^ {\prime} \right] \mathbb {P} \left[ Y \mid Z = z, W = w ^ {\prime} \right], \\ \end{array}
$$

where the first equality follows from rule #2, the second equality follows from rule #3, and the last is just the backdoor adjustment again. Plugging this in, we find that

$$
\begin{array}{l} \mathbb {P} \left[ Y \mid d o (W = w) \right] \\ = \sum_ {z} \mathbb {P} [ Z = z \mid W = w ] \sum_ {w ^ {\prime}} \mathbb {P} [ W = w ^ {\prime} ] \mathbb {P} [ Y \mid Z = z, W = w ^ {\prime} ]. \tag {9.11} \\ \end{array}
$$

This result is called the front-door formula, and it allows for identification of causal effects in the DAG given in Figure 9.2 even though nothing resembling unconfoundedness holds. Interestingly, even though it queries about a $d o ( W =$ $w$ ) intervention, it still integrates over the observed distribution of $\mathbb { P } \left[ W = w ^ { \prime } \right]$ .

# 9.2 Instrumental variables regression

One of the most widely used structural equation models in economics is represented by the DAG in Figure 9.3. We want to measure the effect of a treatment $W$ on an outcome $Y$ . There‚Äôs an unobserved confounder $U$ that rules out the use of unconfoundedness-based methods. However, we do have access to an exogenous (effectively randomized) variable $Z$ , called an instrument, that nudges the treatment $W$ without being affected by the confounder $U$ .

Example 9. Angrist, Graddy, and Imbens [2000] consider a demand estimation problem where $W _ { i }$ is the price of fish and $Y _ { i }$ is demand, and we are

concerned that the association between $W _ { i }$ and $Y _ { i }$ may be confounded by unobserved market factors. They then propose using weather conditions as an instrument $Z _ { i }$ : Stormy weather makes it harder to fish (and thus raises prices), but presumably is unrelated to the confounding market factors.

The goal of instrumental variables methods is to use the effective randomization provided by the instrument to identify the causal effect of $W$ on $Y$ . Doing so, however, will require making further assumptions than those implicit in the SEM in Figure 9.3, as the rules of do-calculus do not enable us to identify $\mathbb { P } \left[ Y | d o ( W = w ) \right]$ in this non-parametric SEM. To see this, note that if we omit the instrument $Z$ from the SEM then $\mathbb { P } \left[ Y | d o ( W = w ) \right]$ i s clearly not identified; and adding more nodes to a graph cannot help achieve identification using do-calculus (since adding nodes can only make it harder to satisfy the $d$ -separation condition).

In order to enable progress, we further make the assumption that the structural equation for $Y$ as in (9.1) is linear:

$$
Y = f _ {Y} (W, U, \varepsilon_ {Y}) = \alpha + W \tau + \varepsilon , \tag {9.12}
$$

where $\varepsilon$ is an error term that captures the contribution of both $U$ and $\varepsilon _ { Y }$ . This is a semiparametric specification, in that we impose a linear relation between $W$ and $Y$ but let the rest of the SEM (9.1) be non-parametric. Instrumental variables as illustrated in Figure 9.3 will prove to be very helpful in identifying $\tau$ in the linear model $\tau$ .54

Linear structural modeling The easiest way to understand instrumental variables regression is to work with a fully linear version of the SEM (9.1) adapted to the DAG illustrated in Figure 9.3:

$$
Y = \alpha + W \tau + \varepsilon , \quad \varepsilon \perp Z \tag {9.13}
$$

$$
W = Z \gamma + \eta .
$$

The fact that $Z$ is uncorrelated with $\varepsilon$ (or, in other words, that $Z$ is exogenous) then implies that

$$
\operatorname {C o v} [ Y, Z ] = \operatorname {C o v} [ \tau W + \varepsilon , Z ] = \tau \operatorname {C o v} [ W, Z ], \tag {9.14}
$$

and so the treatment effect parameter $\tau$ is identified as

$$
\tau = \operatorname {C o v} [ Y, Z ] / \operatorname {C o v} [ W, Z ], \tag {9.15}
$$

provided the denominator is non-zero.

The relation (9.15) also suggests a simple instrumental variables (IV) regression approach to estimating $\tau$ as a ratio of sample covariances,

$$
\hat {\tau} _ {I V} = \widehat {\mathrm {C o v}} \left[ Y _ {i}, Z _ {i} \right] / \widehat {\mathrm {C o v}} \left[ W _ {i}, Z _ {i} \right]. \tag {9.16}
$$

To interpret this estimator, note that the simple linear regressions of $Y$ and $W$ on $Z$ respectively yield fitted regression coefficients

$$
\widehat {\beta} _ {Y Z} = \widehat {\mathrm {C o v}} [ Y _ {i}, Z _ {i} ] / \widehat {\mathrm {V a r}} [ Z _ {i} ], \quad \widehat {\beta} _ {W Z} = \widehat {\mathrm {C o v}} [ W _ {i}, Z _ {i} ] / \widehat {\mathrm {V a r}} [ Z _ {i} ],
$$

and so $\hat { \tau } _ { I V } = \hat { \beta } _ { Y Z } / \hat { \beta } _ { W Z }$ can be interpreted as the ratio of the linear regression coefficients of $Y$ on $Z$ over that of $W$ on $Z$ .

Identifying assumptions The derivation of $\hat { \tau } _ { I V }$ from the model (9.13) was so simple that it‚Äôs easy to miss some important assumptions made. Before proceeding further, we here summarize three substantively meaningful assumptions backed into this identification strategy:

‚Ä¢ The instrument $Z _ { i }$ must be exogenous, which here means $\varepsilon _ { i } \perp \perp Z _ { i }$ .   
‚Ä¢ The instrument $Z _ { i }$ must be relevant, such that Cov $[ W _ { i } , Z _ { i } ] \neq 0$ .   
‚Ä¢ The instrument $Z _ { i }$ must satisfy the exclusion restriction, meaning that any effect of $Z _ { i }$ on $Y _ { i }$ must be mediated via the treatment $W _ { i }$ .

These three conditions can immediately be verified in the setting used here. However, when we seek to use instrumental variables methods to identify treatment effects in more complex settings, these conditions will prove to be helpful guiding principles to understanding when instrumental variables methods work.

Optimal instruments The full linear structural model (9.13) may be restrictive in practice: It not only specifies a linear relationship between $W$ and $Y$ , but also asks the instrument $Z$ to have a linear effect on $W$ . This may be problematic if we have potential access to multiple instruments that may all nudge our target treatment variable, or if we believe that our instrument

may act non-linearly.55 Thankfully, however, the above results on instrumental variables regression extend immediately to the following more general specification,

$$
Y = \tau W + \varepsilon , \quad \varepsilon \perp Z, \quad Y, W \in \mathbb {R}, \quad Z \in \mathcal {Z}, \tag {9.17}
$$

where $\mathcal { Z }$ may be, e.g., a high-dimensional space. By the same argument as in (9.14), we see that given any function $w : \mathcal { Z }  \mathbb { R }$ that maps $Z _ { i }$ to the real line

$$
\tau = \frac {\operatorname {C o v} [ Y , w (Z) ]}{\operatorname {C o v} [ W , w (Z) ]} \tag {9.18}
$$

provided the denominator is non-zero (i.e., provided $w ( Z )$ in fact ‚Äúnudges‚Äù the treatment), resulting in a feasible estimator

$$
\hat {\tau} _ {I V} = \frac {\widehat {\operatorname {C o v}} [ Y _ {i} , w (Z _ {i}) ]}{\widehat {\operatorname {C o v}} [ W _ {i} , w (Z _ {i}) ]} = \frac {\frac {1}{n} \sum_ {i = 1} ^ {n} \left(Y _ {i} - \bar {Y}\right) \left(w (Z _ {i}) - \bar {w (Z)}\right)}{\frac {1}{n} \sum_ {i = 1} ^ {n} \left(W _ {i} - \bar {W}\right) \left(w (Z _ {i}) - \bar {w (Z)}\right)} \tag {9.19}
$$

where $\begin{array} { r } { \overline { { Y } } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } Y _ { i } } \end{array}$ , etc. In other words, if one has access to many valid instruments, the analyst is free to compress them into any univariate instrument of their choice without worrying about linearity in the relationship between $W$ and $w ( Z )$ . The following result verifies consistency and asymptotic properties.

Theorem 9.2. Suppose $( X _ { i } , W _ { i } , Y _ { i } , Z _ { i } )$ ) are IID draws from a distribution satisfying (9.17), and let $w : \mathcal { Z }  \mathbb { R }$ be such that Cov [W, $w ( Z ) ] \ne 0$ . Then, $\hat { \tau } _ { I V }$ as given in (9.19) is consistent for $\tau$ , and

$$
\sqrt {n} \left(\hat {\tau} _ {I V} - \tau\right) \Rightarrow \mathcal {N} \left(0, V _ {w}\right), \quad V _ {w} = \frac {\operatorname {V a r} \left[ \varepsilon_ {i} \right] \operatorname {V a r} \left[ w (Z _ {i}) \right]}{\operatorname {C o v} \left[ W _ {i} , w (Z _ {i}) \right] ^ {2}}. \tag {9.20}
$$

Proof. The estimator (9.19) can be written as a $Z$ -estimator, i.e., as the solution to $\textstyle n ^ { - 1 } \sum _ { i = 1 } ^ { n } \psi _ { i } ( { \hat { \theta } } ) = 0$ with

$$
\psi_ {i} (\hat {\theta}) = \left( \begin{array}{c} (w (Z _ {i}) - \hat {\mu} _ {Z}) (Y _ {i} - \hat {\mu} _ {Y} - \hat {\tau} (W _ {i} - \hat {\mu} _ {W})) \\ Y _ {i} - \hat {\mu} _ {Y} \\ W _ {i} - \hat {\mu} _ {W} \\ w (Z _ {i}) - \hat {\mu} _ {Z} \end{array} \right), \tag {9.21}
$$

where $\hat { \theta } = ( \hat { \tau } , \hat { \mu } _ { W } , \hat { \mu } _ { W } , \hat { \mu } _ { Z } )$ contains both our target parameter and the sample means used to construct $\hat { \tau } _ { I V }$ . Standard results for $Z$ -estimation can then be used to verify that56

$$
\sqrt {n} (\hat {\theta} - \theta) \Rightarrow \mathcal {N} (0, V), \quad V = \mathbb {E} [ \nabla \psi_ {i} (\theta) ] ^ {- 1} \operatorname {V a r} [ \psi_ {i} (\theta) ] \mathbb {E} [ \nabla \psi_ {i} ^ {\prime} (\theta) ] ^ {- 1}. \tag {9.22}
$$

In our setting, we have $\mathbb { E } \left[ \nabla \psi _ { i } ( \boldsymbol { \theta } ) \right] = - \mathrm { d i a g } \left( \mathrm { C o v } \left[ \boldsymbol { w } ( \boldsymbol { Z } _ { i } ) , { W } _ { i } \right] , 1 , 1 , 1 \right)$ , and so (9.22) implies that (9.20) holds with

$$
\begin{array}{l} V _ {w} = \frac {\mathrm {V a r} \left[ (w (Z _ {i}) - \mu_ {Z}) (Y _ {i} - \mu_ {Y} - \tau (W _ {i} - \mu_ {W})) \right]}{\mathrm {C o v} [ w (Z _ {i}) , W _ {i} ] ^ {2}} \\ = \frac {\operatorname {V a r} \left[ (w (Z _ {i}) - \mathbb {E} \left[ w (Z _ {i}) \right]) \varepsilon_ {i} \right]}{\operatorname {C o v} \left[ w (Z _ {i}) , W _ {i} \right] ^ {2}} = \frac {\operatorname {V a r} \left[ w (Z _ {i}) \right] \operatorname {V a r} \left[ \varepsilon_ {i} \right]}{\operatorname {C o v} \left[ w (Z _ {i}) , W _ {i} \right] ^ {2}}, \\ \end{array}
$$

where the last step follows from independence of $Z _ { i }$ and $\varepsilon _ { i }$ .

Now, since essentially any transformation $w : \mathcal { Z }  \mathbb { R }$ yields a valid IV estimator, it‚Äôs natural to ask which such transformation maximized the precision of the resulting estimator, i.e., minimizes the variance in (9.20). It turns out that the optimal instrument has a simple form,

$$
w ^ {*} (z) = \mathbb {E} \left[ W _ {i} \mid Z _ {i} = z \right], \tag {9.23}
$$

i.e., $w ^ { * } ( Z _ { i } )$ is the best prediction of $W _ { i }$ from $Z _ { i }$ .

Theorem 9.3. In the setting of Theorem 9.2, suppose there exists a function $w ( z )$ such that Cov $[ W , w ( Z ) ] \ne 0$ . Then, the variance $V _ { w }$ in (9.20) is minimized by setting $w ( \cdot )$ to be $w ^ { \ast } ( \cdot )$ , or an affine transformation thereof. Furthermore, writing $\hat { \tau } _ { I V } ^ { * }$ for the IV estimator with an optimal instrument,

$$
\sqrt {n} \left(\hat {\tau} _ {I V} ^ {*} - \tau\right) \Rightarrow \mathcal {N} \left(0, V _ {w ^ {*}}\right), \quad V _ {w ^ {*}} = \frac {\operatorname {V a r} \left[ \varepsilon_ {i} \right]}{\operatorname {V a r} \left[ \mathbb {E} \left[ W _ {i} \mid Z _ {i} \right] \right]}. \tag {9.24}
$$

Proof. For any instrument choice $w : \mathcal { Z }  \mathbb { R }$ , we have Cov [Wi, $w ( Z _ { i } ) ] =$ Cov $\left\lfloor \mathbb { E } \left\lfloor W _ { i } \right\rfloor Z _ { i } \right\rfloor , w ( Z _ { i } ) \rfloor$ . Thus, any optimal instrument must solve

$$
w (\cdot) \in \operatorname {a r g m a x} _ {w ^ {\prime}} \left\{\operatorname {C o v} \left[ \mathbb {E} \left[ W _ {i} \mid Z _ {i} \right], w ^ {\prime} (Z _ {i}) \right] ^ {2} / \operatorname {V a r} \left[ w ^ {\prime} (Z _ {i}) \right] \right\}. \tag {9.25}
$$

By Cauchy-Schwarz, this expression is maximized whenever $w ( \cdot )$ is taken to be (potentially an affine transformation of) $\mathbb { E } \left[ W _ { i } \mid Z _ { i } \right]$ . When $w ( \cdot ) \ =$ $\alpha + \beta \mathbb { E } \left[ W _ { i } \mid Z _ { i } \right]$ , we have Cov -E -Wi  Zi , w(Zi) = Œ≤ Var $\left[ \mathbb { E } \left[ W _ { i } \mid Z _ { i } \right] \right]$ , and (9.24) then follows from (9.20). ‚ñ°

Cross-fitting and feasible estimation Given the optimal instrument is the solution to a non-parametric prediction problem, $w ^ { * } ( z ) = \mathbb { E } \left\lfloor W _ { i } \right\rfloor Z _ { i } = z \rfloor$ , one might be tempted to apply the following two-stage strategy:

1. Fit a non-parametric first stage regression, resulting in estimate $\hat { w } ( \cdot )$ of E $\left[ W _ { i } \mid Z _ { i } = z \right]$ , and then   
2. Run (9.19) with $\hat { w } ( \cdot )$ as an instrument.

This approach almost works, but may suffer from striking overfitting bias when the instrument is weak, i.e., Var $\left\lfloor \mathbb { E } \left\lfloor W _ { i } \right\rfloor Z _ { i } \right\rfloor \rfloor$ is small. The main problem is that, if $\hat { w } ( Z _ { i } )$ is fit on the training data, then we no longer have $\hat { w } ( Z _ { i } ) \perp \varepsilon _ { i }$ (because $\hat { w } ( Z _ { i } )$ depends on $W _ { i }$ , which in turn is dependent on $\varepsilon _ { i }$ ). This may seem like a subtle issue but, as pointed out by Bound, Jaeger, and Baker [1995], can in fact be a major problem in practice. They exhibit an example where the instrument $Z _ { i }$ is pure noise, yet $\hat { \tau } _ { I V }$ with instrument $\hat { w } ( Z _ { i } )$ converges to an inconsistent limit, namely the simple regression coefficient from regressing $Y _ { i }$ on $W _ { i }$ which‚Äîbecause of lack of unconfoundedness‚Äîdoes not match the target parameter $\tau$ .

Thankfully, however, we can again use cross-fitting to address this issue. We randomly split data into folds $k = 1$ , ..., $K$ and, for each $k$ , fit a regression $\hat { w } ^ { ( - k ) } ( z )$ on all but the $k$ -th fold. We then run

$$
\widehat {\tau} _ {I V} ^ {C F} = \widehat {\operatorname {C o v}} \left[ Y _ {i}, \widehat {w} ^ {(- k (i))} \left(Z _ {i}\right) \right] / \widehat {\operatorname {C o v}} \left[ W _ {i}, \widehat {w} ^ {(- k (i))} \left(Z _ {i}\right) \right], \tag {9.26}
$$

where $k ( i )$ picks out the data fold containing the $i$ -th observation. Now, by cross-fitting we directly see that $\hat { w } ^ { ( - k ( i ) ) } ( Z _ { i } ) \perp \varepsilon _ { i }$ , and so this approach recovers a valid estimate of $\tau$ . Furthermore, as shown below, if the regressions $\hat { w } ^ { ( - k ( i ) ) } ( z )$ are consistent for $\mathbb { E } \left[ W _ { i } \vert Z _ { i } = z \right]$ in mean-squared error, then the feasible estimator (9.26) is first-order equivalent to (9.19) with an optimal instrument.

Theorem 9.4. Under the conditions of Theorem 9.3, let $\hat { w } ^ { ( - k ) } ( \cdot )$ be cross-fit estimates of the optimal instrument with

$$
\frac {1}{n} \sum_ {k (i) = k} \left(\hat {w} ^ {(- k)} \left(Z _ {i}\right) - w ^ {*} \left(Z _ {i}\right)\right) ^ {2} \rightarrow_ {p} 0. \tag {9.27}
$$

Then, $\hat { \tau } _ { I V } ^ { C F }$ also satisfies the central limit theorem (9.24).

Proof. Starting from the explicit form (9.19), we can write

$$
\hat {\tau} _ {I V} ^ {C F} = \frac {\widehat {\mathrm {C o v}} [ Y _ {i} , \hat {w} ^ {(- k (i))} (Z _ {i}) ]}{\widehat {\mathrm {C o v}} [ W _ {i} , \hat {w} ^ {(- k (i))} (Z _ {i}) ]} = \frac {\frac {1}{n} \sum_ {i = 1} ^ {n} (Y _ {i} - \hat {\mu} _ {Y}) \hat {w} ^ {(- k (i))} (Z _ {i})}{\frac {1}{n} \sum_ {i = 1} ^ {n} (W _ {i} - \hat {\mu} _ {W}) \hat {w} ^ {(- k (i))} (Z _ {i})}.
$$

Furthermore, by (9.12), we can continue

$$
\begin{array}{l} \ldots = \frac {\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\left(W _ {i} - \hat {\mu} _ {W}\right) \tau + \left(\varepsilon_ {i} - \hat {\mu} _ {\varepsilon}\right)\right) \hat {w} ^ {(- k (i))} \left(Z _ {i}\right)}{\frac {1}{n} \sum_ {i = 1} ^ {n} \left(W _ {i} - \hat {\mu} _ {W}\right) \hat {w} ^ {(- k (i))} \left(Z _ {i}\right)} \\ = \tau + \frac {\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\varepsilon_ {i} - \hat {\mu} _ {\varepsilon}\right) \hat {w} ^ {(- k (i))} \left(Z _ {i}\right)}{\frac {1}{n} \sum_ {i = 1} ^ {n} \left(W _ {i} - \hat {\mu} _ {W}\right) \hat {w} ^ {(- k (i))} \left(Z _ {i}\right)}, \\ \end{array}
$$

where $\hat { \mu } _ { Y }$ , $\hat { \mu } _ { W }$ and $\hat { \mu } _ { \varepsilon }$ are sample averages of $Y _ { i }$ , $W _ { i }$ and $\varepsilon _ { i }$ respectively. The above identity holds algebraically for any estimator $\hat { w } ^ { ( - k ) } ( \cdot )$ , including the perfect estimator $\hat { w } ^ { ( - k ) } ( \cdot ) = w ^ { \ast } ( \cdot )$ , and so we only need to show that errors from an estimator $\hat { w } ^ { ( - k ) } ( \cdot )$ that is consistent estimator in the sense of (9.27) have a negligible effect on the final expression above. To this end, it suffices to verify that

$$
\begin{array}{l} \frac {1}{n} \sum_ {\substack {i = 1 \\ n}} ^ {n} \left(\varepsilon_ {i} - \hat {\mu} _ {\varepsilon}\right) \left(\hat {w} ^ {(- k (i))} \left(Z _ {i}\right) - w ^ {*} \left(Z _ {i}\right)\right) = o _ {P} \left(\frac {1}{\sqrt {n}}\right) \tag{9.28} \\ \frac {1}{n} \sum_ {i = 1} ^ {n} (W _ {i} - \hat {\mu} _ {W}) (\hat {w} ^ {(- k (i))} (Z _ {i}) - w ^ {*} (Z _ {i})) = o _ {P} \left(\frac {1}{\sqrt {n}}\right), \\ \end{array}
$$

which follows from cross-fitting and (9.27) by the same argument as used in (3.14) in the proof of Theorem 3.2. ‚ñ°

Non-parametric instrumental variables regression At the beginning of Chapter 9.2 we noted that instrumental variables methods cannot be justified via do-calculus alone, and so further structural assumptions are required. Here, we have mostly focused on methods that are valid under the linearity assumption (9.12); however, we emphasize that this is not the weakest assumption under which instrumental variable methods can be justified. One notable generalization is the non-parametric instrumental variables problem,

$$
Y _ {i} = \alpha + g \left(W _ {i}\right) + \varepsilon_ {i}, Z _ {i} \perp \varepsilon_ {i}, Y _ {i}, W _ {i} \in \mathbb {R}, Z _ {i} \in \mathcal {Z}, \tag {9.29}
$$

where $g ( \cdot )$ is some generic smooth function we want to estimate.57 The model (9.29) is still stronger than the generic SEM (9.1) because it requires the effect of $W _ { i }$ on $Y _ { i }$ to be additive; however, unlike (9.17), it now allows this additive effect to be modified by a non-linearity $g ( \cdot )$ .

Because $Z _ { i } \perp \perp \varepsilon _ { i }$ and assuming without loss of generality that $\mathbb { E } \left[ \varepsilon _ { i } \right] = 0$ , we can directly verify that

$$
\begin{array}{l} \mathbb {E} \left[ Y _ {i} \mid Z _ {i} = z \right] = \mathbb {E} \left[ \alpha + g \left(W _ {i}\right) + \varepsilon_ {i} \mid Z _ {i} = z \right] \\ = \alpha + \mathbb {E} [ g (W _ {i}) \mid Z _ {i} = z ] \tag {9.30} \\ = \alpha + \int_ {\mathbb {R}} g (w) f (w | z) d w, \\ \end{array}
$$

where $f ( w \mid z )$ denotes the conditional density of $W _ { i }$ given $Z _ { i } = z$ . This relationship suggests a two-stage scheme for learning $g ( \cdot )$ , whereby we (1) fit a non-parametric model $\hat { f } ( w \mid z )$ for the conditional density $f ( w \mid z )$ , preferably using cross-fitting, and (2) estimate $g ( w )$ via a empirical minimization over a suitably chosen function class $\mathcal { G }$ ,

$$
\hat {g} (\cdot) = \operatorname {a r g m i n} _ {g \in \mathcal {G}, \alpha} \left\{\frac {1}{n} \sum_ {i = 1} ^ {n} \left(Y _ {i} - \int_ {\mathbb {R}} g (w) \hat {f} ^ {(- k (i))} (w | Z _ {i}) d w - \alpha\right) ^ {2} \right\}. \tag {9.31}
$$

In order to solve the inverse problem (9.31) in practice, one approach is to approximate $g ( w )$ in terms of a basis expansion, $\begin{array} { r } { g _ { J } ( w ) = \sum _ { j = 1 } ^ { J } \beta _ { j } \psi _ { j } ( w ) } \end{array}$ , where the $\psi _ { j } ( \cdot )$ are a set of pre-determined basis functions and $g _ { J } ( w )$ provides an increasingly good approximation to $g ( w )$ as $J$ gets large. Then, (9.31) becomes

$$
\hat {\beta} = \operatorname {a r g m i n} _ {\alpha , \beta} \left\{\frac {1}{n} \sum_ {i = 1} ^ {n} \left(Y _ {i} - \sum_ {j = 1} ^ {J} \hat {m} _ {j} ^ {(- k (i))} \left(Z _ {i}\right) \beta_ {j} - \alpha\right) ^ {2} \right\}, \text {w h e r e} \tag {9.32}
$$

$$
\hat {m} _ {j} ^ {(- k (i))} (Z _ {i}) = \int_ {\mathbb {R}} \psi_ {j} (w) \hat {f} ^ {(- k (i))} (w \mid Z _ {i}) d w.
$$

Conditions under which this type of approach yields a consistent estimate of $g ( \cdot )$ are discussed in Newey and Powell [2003]. In general, however, one should note that solving the integral equation (9.30) is a difficult inverse problem, and so getting (9.32) to work in practice requires careful regularization‚Äîand, even so, one should expect rates of convergence to be slow.

# Bibliographic notes

The use of structural models for reasoning about observed data has a long tradition; early examples include the work of Wright [1934] on path models motivated by genetics and that of Haavelmo [1943] for reasoning about simultaneous equation models (e.g., for joint modeling of supply and demand).

Our presentation of non-parametric SEMs in Chapter 9.1, including the examples of the front- and back-door identification formulas, is adapted from Pearl [1995]. The do-calculus was proposed by Pearl [1995]; a recent overview of the literature on non-parametric SEM is given in Pearl [2009]. One should note that SEMs are not the only way of representing causal effects in complex sampling designs using DAGs; other approaches have also been developed by Robins [1986] and Spirtes, Glymour, and Scheines [1993]. In particular, the approach of Robins [1986] builds on the potential outcomes framework; see Robins and Richardson [2010] for further discussion. For a broader discussion of the role of non-parametric SEMs in econometrics see Imbens [2020], Pearl and Mackenzie [2018], and references therein.

Instrumental variable methods are widely used in modern applied econometrics; see Hansen [2022] for a recent textbook treatment. The literature on efficient estimation with instrumental variables goes back to Amemiya [1974], Chamberlain [1987], and others. Newey [1990] showed that the optimal instruments in model (9.17) can be understood as the solution to a prediction problem, thus opening the door to deriving optimal instruments via nonparametric prediction. The role of cross-fitting-type techniques in mitigating over-fitting bias with instrumental variable methods was recognized by Angrist and Krueger [1995], who refer to this technique as split-sample instrumental variable estimation. One practical consideration to be aware of is that, although the central limit theorems given in Theorem 9.2 and 9.4 technically only require the transformed instrument $w ( Z _ { i } )$ to have non-zero correlation with the treatment, confidence intervals built using a plug-in construction based on these central limit theorems may have poor finite-sample performance when the instrument is weak (i.e., when this correlation is small). More robust methods for inference for IV regression with weak instruments are discussed in Andrews, Stock, and Sun [2019].

One question we‚Äôve ignored here is the role of covariates for instrumental variables regression. Following our approach to unconfoundedness, one can extend (9.17) such that $\varepsilon _ { i } \perp \perp Z _ { i } \mid X _ { i }$ , i.e., the instrument is only exogenous after conditioning on $X _ { i }$ , and we have a heterogeneous treatment effect function identified as $\tau ( x ) = \mathrm { C o v } \left[ Y _ { i } , w ( Z _ { i } ) \vert X _ { i } = x \right] / \mathrm { C o v } \left[ W _ { i } , w ( Z _ { i } ) \vert X _ { i } = x \right]$ ; see Abadie [2003] and Aronow and Carnegie [2013] for a further discussion. Given this setting, one can then re-visit many of the questions we considered under unconfoundedness. Chernozhukov et al. [2022a] show how to build a doubly robust estimator of the average effect $\tau = \mathbb { E } \left[ \tau ( X ) \right]$ while Athey, Tibshirani, and Wager [2019] propose a random forest estimator of $\tau ( \cdot )$ ; see also Exercise 11 in Chapter 16.

# Chapter 10 Local Average Treatment Effects

Instrumental variable regression is often used to estimate the effect of an endogenous treatment. In the previous chapter we saw how, given the structural equation model depicted in Figure 9.3 and a linear specification (9.12) governing the effect of the treatment $W _ { i }$ and the outcome $Y _ { i }$ , we can use an instrument $Z _ { i }$ to identify the treatment effect parameter $\tau$ as a ratio of covariances, and consistently estimate $\tau$ via

$$
\hat {\tau} _ {I V} = \widehat {\operatorname {C o v}} \left[ Y _ {i}, Z _ {i} \right] / \widehat {\operatorname {C o v}} \left[ W _ {i}, Z _ {i} \right]. \tag {10.1}
$$

In general, however, researchers in causal inference are often skeptical of interpreting target estimands that are only defined and understood as parameters in a linear model; and so, in this chapter, we will revisit our analysis of the instrumental variable estimator $\hat { \tau } _ { I V }$ without assuming linearity‚Äîor, equivalently, under an assumption that (9.12) may be misspecified.

Without linearity, the estimator $\hat { \tau } _ { I V }$ still converges to a large-sample limit

$$
\hat {\tau} _ {I V} \rightarrow \tau_ {I V} := \operatorname {C o v} \left[ Y _ {i}, Z _ {i} \right] / \operatorname {C o v} \left[ W _ {i}, Z _ {i} \right] \tag {10.2}
$$

whenever Cov $[ W _ { i } , Z _ { i } ] \neq 0$ ; however, it is no longer immediately clear how to interpret this limit. In this chapter, we will study what this limit quantity is, and when it can be understood as a causal quantity. We will survey a number of economic models where endogenous selection into treatment may be a concern and find that‚Äîunder reasonably general conditions‚Äîthis limit is a weighted treatment effect with positive weights depending on (unobserved) attributes that control how responsive each unit is to the nudge given by the instrument. Following Imbens and Angrist [1994], when these conditions hold, we refer to this limit as the local average treatment effect (LATE), i.e., the treatment effect ‚Äúlocal‚Äù to those responsive to the instrument.

# 10.1 Non-compliance in randomized trials

The simplest setting in which we can discuss non-parametric identification using instrumental variables is when estimating the effect of a binary treatment under non-compliance. Suppose, for example, that we‚Äôve set up a randomized study to examine the effect of taking a drug to lower cholesterol. But, although we randomly assigned treatment, some people don‚Äôt obey the randomization: Some subjects given the drugs may fail to take them, while others who were assigned control may procure cholesterol lowering drugs on their own. In this case, we have58

‚Ä¢ An outcome $Y _ { i } \in \mathbb { R }$ , with the usual interpretation;   
‚Ä¢ The treatment $W _ { i } \in \{ 0 , 1 \}$ that was actually received (i.e., did the subject take the drug), which is not random because of non-compliance; and   
‚Ä¢ The assigned treatment $Z _ { i } \in \{ 0 , 1 \}$ which is random.

A popular way to analyze this type of data is using instrumental variables, where we interpret treatment assignment $Z _ { i }$ as an exogenous ‚Äúnudge‚Äù on the treatment $W _ { i }$ that was actually received.59

If one believes in the partially linear structural model (9.12) considered in the previous chapter, then one can consistently estimate $\tau$ via (10.2) provided that assigned treatment in fact nudges the received treatment, i.e., Cov $[ W _ { i } , Z _ { i } ] \neq 0$ . In practice, however, one may doubt the validity the constant treatment effect assumption (9.12), and suspect that people who comply with the treatment respond differently to the treatment than those who don‚Äôt comply. For example, there may exists a class of patients who chose to comply because they knew they‚Äôd benefit a lot from the treatment; or conversely other patients may have chosen not to comply because they knew they had a disproportionate risk of being hurt by the treatment.

Potential outcomes under non-compliance A more careful approach starts by writing down potential outcomes. First, because $W _ { i }$ is not randomized

and may respond to $Z _ { i }$ , we need to have potential outcomes for the treatment variable in terms of the instrument, i.e., there are $\{ W _ { i } ( 0 ) , W _ { i } ( 1 ) \}$ such that $W _ { i } = W _ { i } ( Z _ { i } )$ . Second, of course, we need to define potential outcomes for the outcome, which may in principle respond to both $W _ { i }$ and $Z _ { i }$ : we have $\{ Y _ { i } ( w , z ) \} _ { w , z \in \{ 0 , 1 \} }$ such that $Y _ { i } = Y _ { i } ( W _ { i } , Z _ { i } )$ .

Given this notation, we now revisit our assumptions for what makes a valid instrument:

‚Ä¢ Exclusion restriction. Treatment assignment only affects outcomes via receipt of treatment, i.e., $Y _ { i } ( w , z ) = Y _ { i } ( w )$ for all $w$ and $z$ .   
‚Ä¢ Exogeneity. The treatment assignment is randomized, meaning that {Yi(0), Yi(1), Wi(0), Wi(1)} ‚ä•‚ä• Zi.   
‚Ä¢ Relevance. The treatment assignment affects receipt of treatment, meaning that $\mathbb { E } \left[ W _ { i } ( 1 ) - W _ { i } ( 0 ) \right] \neq 0$ .

Finally, we make one last assumption about how people respond to treatment. Defining each subject‚Äôs compliance type as $C _ { i } = \{ W _ { i } ( 0 ) , W _ { i } ( 1 ) \}$ , we note that there are only 4 possible compliance types here:

<table><tr><td></td><td>Wi(1) = 0</td><td>Wi(1) = 1</td></tr><tr><td>Wi(0) = 0</td><td>never taker</td><td>compliant</td></tr><tr><td>Wi(0) = 1</td><td>defier</td><td>always taker</td></tr></table>

Our last assumption is that there are no defiers, i.e., $\mathbb { P } \left[ C _ { i } = \{ 1 , 0 \} \right] = 0$ ; this assumption is often also called monotonicity. Given these 4 assumptions, we obtain the following simple characterization of the IV estimand (10.2) as a local average treatment effect.

Theorem 10.1. Consider a sampling distribution with a binary treatment $W _ { i }$ and a binary instrument $Z _ { i }$ , and satisfying the 4 assumptions given above (exogeneity, relevance, monotonicity, and the exclusion restriction). Then,

$$
\tau_ {I V} = \mathbb {E} \left[ Y _ {i} (1) - Y _ {i} (0) \mid C _ {i} = \text {c o m p l i e r} \right]. \tag {10.3}
$$

Proof. With a binary treatment and instrument, the IV estimand (10.2) can be written as

$$
\tau_ {I V} = \frac {\mathbb {E} \left[ Y _ {i} \mid Z _ {i} = 1 \right] - \mathbb {E} \left[ Y _ {i} \mid Z _ {i} = 0 \right]}{\mathbb {E} \left[ W _ {i} \mid Z _ {i} = 1 \right] - \mathbb {E} \left[ W _ {i} \mid Z _ {i} = 0 \right]},
$$

and this ratio is well defined thanks to the relevance assumption. Furthermore,

$$
\begin{array}{l} \mathbb {E} \left[ Y _ {i} \mid Z _ {i} = 1 \right] - \mathbb {E} \left[ Y _ {i} \mid Z _ {i} = 0 \right] \\ = \mathbb {E} \left[ Y _ {i} \left(W _ {i} (1)\right) \mid Z _ {i} = 1 \right] - \mathbb {E} \left[ Y _ {i} \left(W _ {i} (0)\right) \mid Z _ {i} = 0 \right] \quad (\text {e x c l u s i o n}) \\ = \mathbb {E} \left[ Y _ {i} \left(W _ {i} (1)\right) - Y _ {i} \left(W _ {i} (0)\right) \right] \quad (\text {e x o g e n e i t y}) \\ = \mathbb {E} \left[ 1 \left(\left\{C _ {i} = \text {c o m p l i e r} \right\}\right) \left(Y _ {i} (1) - Y _ {i} (0)\right) \right], \quad (\text {m o n o t o n i c i t y}) \\ \end{array}
$$

and similarly that

$$
\mathbb {E} \left[ W _ {i} \mid Z _ {i} = 1 \right] - \mathbb {E} \left[ W _ {i} \mid Z _ {i} = 0 \right] = \mathbb {P} \left[ C _ {i} = \text {c o m p l i e r} \right].
$$

The result (10.3) follows by basic properties of conditional expectations.

![](images/5768ce59f3efecca8670cbc3e3a8fc7e56431dcc371fb168b41c5bbd1573e1d6.jpg)

Although this is a very simple result, it already gives us some encouragement that IV methods can be interpreted in a non-parametric setting: When the constant treatment effect model (9.12) doesn‚Äôt hold, the average treatment effect $\tau _ { A T E } ~ = ~ \mathbb { E } \left[ Y _ { i } ( 1 ) - Y _ { i } ( 0 ) \right]$ is clearly not identified without more data, because we don‚Äôt have any observations on treated never takers, etc. However, under reasonable assumptions, IV methods let us estimate the most meaningful quantity we can identify here, namely the average treatment effect among those who comply with the treatment as assigned by the experimenter.

Example 1 (Continued). In the example of Finkelstein et al. [2012] on the Oregon Medicaid lottery, introduced in Chapter 1, roughly 35,000 of 90,000 lottery participants were allowed to apply for Medicaid. However, of the 35,000 lottery winners, only about 30% in fact enrolled for Medicaid: Some didn‚Äôt complete the application, and some hadn‚Äôt met the requirements for joining the lottery to begin with (e.g., their income was too high). The average treatment effect measured via the difference-in-means estimator thus does not directly quantify the benefit of Medicaid enrollment here. But, because there are plausibly no defiers here, we can divide the raw difference-in-means by 0.3 to get a local average treatment effect, i.e., an estimate of the average benefit for those who would in fact enroll for Medicaid if they win the lottery.

Multiple instruments In some applications, we may have access to data from multiple randomized trials that can be used to study a treatment effect via a non-compliance analysis. Consider a marketing application where a company wants to study the effect of subscription to a loyalty program ( $W _ { i }$ ) on long-term customer revenue (Yi), and has access to multiple randomized trials whose treatments $( Z _ { i } )$ effectively nudge customers to join the loyalty program and can thus be used as instruments. For example,

one randomized trial may offer discounts for joining the loyalty program ( $Z _ { i } = 1$ ({customer received a discount})) while another may show advertisements ( $Z _ { i } = 1$ ({customer was shown an ad for the program})).

If we just focus on one of the instruments, then the methods developed above can be applied directly. One may also be tempted to somehow pool the instruments. In the previous chapter we saw that, under the linear treatment effect model, multiple instruments could be combined into a single optimal instrument, and the optimal instrument corresponds to the summary of all the instruments that best predicts the treatment (Theorem 9.3).

Without the linear treatment effect model, however, we caution that no such result is available. Different instruments may induce difference compliance patterns, and so the LATEs identified using different instruments may not be the same; and a pooled instrument produced using the construction in Theorem 9.3 may induce yet another compliance pattern. For example, in our marketing example, the ATE for customers who respond to a discount may be different from the ATE for customers who respond to an advertisement.

As such, when working without the linear treatment assumption (9.12) and if there are multiple instruments to choose from, a researcher may prefer to simply use the instrument whose LATE most closely matches a policy-relevant effect of interest. One could also run separate IV analyses using different instruments, and use discrepancies between the resulting estimates to argue for heterogeneity in treatment effects across different compliance groups.

# 10.2 Economic models

Our derivation of the local average treatment effect in randomized trials with non-compliance could be carried out by simply tabulating possible compliance types and examining how they respond to treatment assignment.60 However, instrumental variables regression is also used in many applications where endogeneity plays out through more complicated economic processes‚Äîsuch as equilibrium formation or complex choice mechanics‚Äîunder which such a simple tabulation exercise is no longer practical. We will continue our discussion

by examining how instrumental variables methods can be interpreted in the context of richer economic models when we have access to data on exogenous shocks that can be argued to nudge treatment in a quasi-random manner. Overall, we will find that instrumental variables methods still uncover local average treatment effects, but with more complex implied weights.

Supply and demand In many settings, it is of considerable interest to know the price elasticity of demand, i.e., how demand would respond to price changes. In a typical marketplace, prices are not exogenous‚Äîrather, they arise from an interplay of supply and demand‚Äîand so estimating the elasticity requires an instrument. One can formalize the structural relationship of supply and demand via potential outcomes as follows. For each marketplace $i = 1$ , ..., $n$ , there is a supply curve $S _ { i } ( p , z )$ and a demand curve $Q _ { i } ( p , z )$ , corresponding to the supply (and respectively demand) that would arise given price $p \in \mathbb R$ and some instrument $z \in \{ 0 , 1 \}$ that may affect the marketplace (the instrument could, e.g., capture the presence of supply chain events that make production harder and thus reduce supply). We take $S _ { i } ( \cdot , z )$ to be continuous and increasing and $Q _ { i } ( \cdot , z )$ to be continuous and decreasing.

Example 9 (Continued). In the setting of Angrist, Graddy, and Imbens [2000] one may argue that, on closer inspection, the DAG given in Figure 9.3 does not present a complete explanation for the interplay of supply, demand, prices and weather; and that the above market equilibrium model (with weather as the instrument) provides a better fit. The discussion below will show how we can still make sense of the basic IV estimator $\hat { \tau } _ { I V }$ while framing causal effects in terms of this equilibrium model.

Given this setting, suppose that first the instrument $Z _ { i }$ gets realized; then prices $P _ { i }$ arise by matching supply and demand, such that $P _ { i }$ is the unique solution to the market equilibrium condition $^ \mathrm { 6 1 }$ $S _ { i } ( P _ { i } , Z _ { i } ) = Q _ { i } ( P _ { i } , Z _ { i } )$ . The researcher observes the instrument $Z _ { i }$ , the market clearing price $P _ { i }$ (‚Äúthe treatment‚Äù) and the realized demand $Q _ { i } = Q _ { i } ( P _ { i } , Z _ { i } )$ (‚Äúthe outcome‚Äù). We say that $Z _ { i }$ is a valid instrument for measuring the effect of prices on demand if the following conditions hold:

‚Ä¢ Exclusion restriction. The instrument only affects demand via supply, and cannot have a direct effect on it: $Q _ { i } ( p , z ) = Q _ { i } ( p )$ for all $p$ and $z$ .

‚Ä¢ Exogeneity. The instrument is as good as random, $\{ Q _ { i } ( p ) , S _ { i } ( p , z ) \} \perp$ $Z _ { i }$ .   
‚Ä¢ Relevance. The instrument affects prices, Cov $[ P _ { i } , Z _ { i } ] \neq 0$ .   
‚Ä¢ Monotonicity. The instrument never increases supply, i.e., $S _ { i } ( P _ { i } , 1 ) \leq$ $S _ { i } ( P _ { i } , 0 )$ almost surely.

Given this setting, we seek to estimate demand elasticity via (10.2).62

Now, although this may seem like a complicated setting, it turns out that the IV estimand where we use $Z _ { i }$ as an instrument to measure the effect of $P _ { i }$ on $Q _ { i }$ is well behaved‚Äîand admits a characterization as a weighted average of the derivative of $Q _ { i } ( p )$ .

Theorem 10.2. In the above supply-demand model, suppose furthermore that $Q _ { i } ( p )$ is differentiable and write $Q _ { i } ^ { \prime } ( p )$ for its derivative. $^ { 6 3 }$ Then,

$$
\tau_ {I V} = \frac {\int \mathbb {E} \left[ Q _ {i} ^ {\prime} (p) \mid P _ {i} (0) \leq p \leq P _ {i} (1) \right] \mathbb {P} \left[ P _ {i} (0) \leq p \leq P _ {i} (1) \right] d p}{\int \mathbb {P} \left[ P _ {i} (0) \leq p \leq P _ {i} (1) \right] d p}, \tag {10.4}
$$

Proof. Because $Z _ { i }$ is binary, we can write

$$
\tau_ {I V} = \frac {\mathbb {E} \left[ Q _ {i} \mid Z _ {i} = 1 \right] - \mathbb {E} \left[ Q _ {i} \mid Z _ {i} = 0 \right]}{\mathbb {E} \left[ P _ {i} \mid Z _ {i} = 1 \right] - \mathbb {E} \left[ P _ {i} \mid Z _ {i} = 0 \right]}.
$$

Now, under the assumptions made here, i.e., that the instrument suppresses supply and that the supply and demand curves are monotone increasing and decreasing respectively, the instrument must have a monotone increasing effect on prices: $P _ { i } ( 1 ) \ge P _ { i } ( 0 )$ . Then,

$$
\begin{array}{l} \mathbb {E} \left[ Q _ {i} \mid Z _ {i} = 1 \right] - \mathbb {E} \left[ Q _ {i} \mid Z _ {i} = 0 \right] \\ = \mathbb {E} \left[ Q _ {i} \left(P _ {i} (1)\right) \mid Z _ {i} = 1 \right] - \mathbb {E} \left[ Q _ {i} \left(P _ {i} (0)\right) \mid Z _ {i} = 0 \right] \quad (\text {e x c l u s i o n}) \\ = \mathbb {E} \left[ Q _ {i} \left(P _ {i} (1)\right) - Q _ {i} \left(P _ {i} (0)\right) \right] \quad (\text {e x o g e n .}) \\ = \mathbb {E} \left[ \int_ {P _ {i} (0)} ^ {P _ {i} (1)} Q _ {i} ^ {\prime} (p) d p \right] \quad (\text {m o n o t .}) \\ = \int \mathbb {E} \left[ Q _ {i} ^ {\prime} (p) \mid P _ {i} (0) \leq p \leq P _ {i} (1) \right] \mathbb {P} \left[ P _ {i} (0) \leq p \leq P _ {i} (1) \right] d p, \quad (\text {F u b i n i}) \\ \end{array}
$$

and the denominator in (10.4) can be characterized via similar means to obtain (10.4).

The above result is not quite as interpretable as the one obtained in Theorem 10.1, where the LATE was founds to exactly match the average treatment effect for the compliers. However, as seen in the remarks below, the characterization (10.4) can still be helpful in understanding the practical behavior of IV methods in applications involving supply-demand equilibrium formation.

Remark 10.1. Under the setting of Theorem 10.2, if individual demand functions are linear in prices, $Q _ { i } ^ { \prime } ( p ) = \alpha _ { i } + \beta _ { i } p$ , then

$$
\tau_ {I V} = \mathbb {E} \left[ \beta_ {i} \left(P _ {i} (1) - P _ {i} (0)\right) \right] / \mathbb {E} \left[ P _ {i} (1) - P _ {i} (0) \right], \tag {10.5}
$$

i.e., the LATE matches the average price parameter weighted by how much the price responds to the instrument. Furthermore, if we have approximate linearity then Theorem 10.2 implies that (10.5) also still holds approximately‚Äî and can be used to quantitatively assess the effect of deviations from linearity.

Remark 10.2. Under the setting of Theorem 10.2, if individual demand functions $Q _ { i } ( p )$ are smooth and if the instrument only has a small effect on prices, i.e., $P _ { i } ( 0 )$ , $P _ { i } ( 1 ) \ \approx \ p _ { 0 }$ for some stable price $p _ { 0 }$ , then $\tau _ { I V } \approx \mathbb { E } \left[ Q _ { i } ^ { \prime } ( p _ { 0 } ) ( P _ { i } ( 1 ) - P _ { i } ( 0 ) ) \right] / \mathbb { E } \left[ P _ { i } ( 1 ) - P _ { i } ( 0 ) \right]$ .

Latent choice models Many central questions in economics can naturally be framed in terms of models where agents make choices (e.g., take a job, go to college, start a company) in a way that is determined by latent and often unobserved attributes (e.g., skills, motivation, risk tolerance), and these latent attributes also influence subsequent outcome variables of interest. For concreteness, consider a setting where someone chooses to attend college ( $W _ { i }$ ) if their (unobserved) utility $U _ { i }$ from doing so exceeds the cost of attendance, and we want to measure the effect of college attendance on lifetime income $Y _ { i }$ . In settings such as these, if we have an exogenous instrument $Z _ { i }$ that can modify the cost of taking the action (e.g., a randomly assigned tuition subsidy), then we may again seek to use this instrument to estimate get a handle on our target estimand.

The standard way to model this setting is via a threshold crossing model: We assume that each subject has a latent and endogenous variable $U _ { i }$ such that [Heckman, 1979, Roy, 1951]

$$
W _ {i} = 1 \left(\left\{U _ {i} \geq c \left(Z _ {i}\right) \right\}\right), \tag {10.6}
$$

where $c ( z )$ gives the cost of treatment as a function of the instrument $z$ , which we will here allow to be continuous valued. This boundary crossing structure yields a valid instrument under analogues to our usual assumptions:

‚Ä¢ Exclusion restriction. There are potential outcomes $\{ Y _ { i } ( 0 ) , Y _ { i } ( 1 ) \}$ such that $Y _ { i } = Y _ { i } ( W _ { i } )$   
‚Ä¢ Exogeneity. The treatment assignment is randomized, meaning that $\{ Y _ { i } ( 0 ) , Y _ { i } ( 1 ) , U _ { i } \} \perp Z _ { i }$ .   
‚Ä¢ Relevance. The threshold function $c ( Z _ { i } )$ has non-trivial variation, i.e., $\mathbb { P } \left[ U _ { i } \geq c ( Z _ { i } ) \vert Z _ { i } = z \right]$ is not constant in $z$ .   
‚Ä¢ Monotonicity.64 The threshold function $c ( z )$ is non-increasing in $z$

Finally, define the marginal treatment effect

$$
\tau (u) = \mathbb {E} \left[ Y _ {i} (1) - Y _ {i} (0) \mid U _ {i} = u \right]. \tag {10.7}
$$

Under this choice model, Heckman and Vytlacil [2005] show that IV methods recover a weighted average of the marginal treatment effect $\tau ( u )$ . Without loss of generality,65 we can transform the unobservables so that $U _ { i } \sim \mathrm { U n i f } ( [ 0 , 1 ] )$ and so the cost function can be interpreted as $c ( z ) = 1 - \mathbb { P } \left[ W _ { i } \vert Z _ { i } = z \right]$ .

Theorem 10.3. In the above threshold-crossing model under the exclusion restriction, exogeneity and relevance, and assuming that $U _ { i } \sim \mathrm { U n i f } ( [ 0 , 1 ] )$ , instrumental variables regression estimates

$$
\tau_ {I V} = \frac {\int_ {0} ^ {1} \tau (u) \mathbb {P} [ c (Z _ {i}) \leq u ] \mathbb {E} [ Z _ {i} - \mu_ {Z} | c (Z _ {i}) \leq u ] d u}{\int_ {0} ^ {1} \mathbb {P} [ c (Z _ {i}) \leq u ] \mathbb {E} [ Z _ {i} - \mu_ {Z} | c (Z _ {i}) \leq u ] d u}, \tag {10.8}
$$

where $\begin{array} { r l r } { \mu _ { Z } } & { { } = } & { \mathbb { E } \left\lfloor Z _ { i } \right\rfloor } \end{array}$ . Furthermore, under monotonicity, we have $\mathbb { E } \left[ Z _ { i } - \mu _ { Z } \big | c ( Z _ { i } ) \leq u \right] \geq 0$ for all $0 \leq u \leq 1$ , and so the above can be interpretated as a local average treatment effect.

Proof. The key task is in characterizing Cov $[ Y _ { i } , Z _ { i } ]$ . First,

$$
\begin{array}{l} \operatorname {C o v} \left[ Y _ {i}, Z _ {i} \right] = \operatorname {C o v} \left[ Y _ {i} (0) + \left(Y _ {i} (1) - Y _ {i} (0)\right) W _ {i}, Z _ {i} \right] \\ = \operatorname {C o v} \left[ \left(Y _ {i} (1) - Y _ {i} (0)\right) W _ {i}, Z _ {i} \right] \\ = \operatorname {C o v} \left[ \left(Y _ {i} (1) - Y _ {i} (0)\right) 1 \left(\left\{U _ {i} \geq c \left(Z _ {i}\right) \right\}\right), Z _ {i} \right] \\ = \operatorname {C o v} \left[ \tau (U _ {i}) 1 \left(\left\{U _ {i} \geq c \left(Z _ {i}\right) \right\}\right), Z _ {i} \right], \\ \end{array}
$$

where the first equality follows from the exclusion restriction, while the second and fourth follow from exogeneity. Recalling that $U _ { i }$ is assumed to have a uniform distribution on [0, 1], we can then continue

$$
\begin{array}{l} \dots = \mathbb {E} \left[ \tau (U _ {i}) 1 \left(\left\{U _ {i} \geq c (Z _ {i}) \right\}\right) \left(Z _ {i} - \mu_ {Z}\right) \right] \\ = \int_ {0} ^ {1} \tau (u) \mathbb {E} \left[ 1 \left(\left\{u \geq c \left(Z _ {i}\right) \right\}\right) \left(Z _ {i} - \mu_ {Z}\right) \right] d u \\ = \int_ {0} ^ {1} \tau (u) \mathbb {P} [ c (Z _ {i}) \leq u ] \mathbb {E} [ Z _ {i} - \mu_ {Z} | c (Z _ {i}) \leq u ] d u, \\ \end{array}
$$

where the second equality again relied on exogeneity. The claimed result follows by applying the same calculation to the denominator of (10.2) and noting that it is non-zero thanks to the relevance assumption. ‚ñ°

One can gain further insight by examining the expression from (10.8) in the context of specific problem structures. We start by examining the the case where the threshold function $c ( z )$ is constant with a single jump, and find that $\tau _ { I V }$ from (10.8) recovers the average treatment effect over the compliers (as in Theorem 10.1) regardless of the distribution of $Z _ { i }$ .

Corollary 10.4. Under the setting of Theorem 10.3, suppose that there are $c _ { 0 } > c _ { 1 }$ and $z _ { 1 }$ such that $c ( z ) = c _ { 0 }$ for $z < z _ { 1 }$ and $c ( z ) = c _ { 1 }$ for $z \geq z _ { 1 }$ . Then,

$$
\tau_ {I V} = \mathbb {E} [ \tau (U _ {i}) | c _ {1} \leq U _ {i} <   c _ {0} ]. \tag {10.9}
$$

Proof. If $c ( z )$ has a single jump, we can use $U _ { i }$ to split all units into never-takers with $U _ { i } < c _ { 1 }$ , compliers with $c _ { 1 } \leq U _ { i } < c _ { 0 }$ , and always takers with $U _ { i } \geq c _ { 0 }$ . Furthermore, for the never-takers $\mathbb { P } \left[ c ( Z _ { i } ) \leq U _ { i } \right] = 0$ , while for the always takers $\begin{array} { r } { \mathbb { E } \left[ Z _ { i } - \mu _ { Z } \middle | c ( Z _ { i } ) \leq U _ { i } \right] = \mathbb { E } \left[ Z _ { i } - \mu _ { Z } \right] = 0 } \end{array}$ . Thus, only compliers contribute to the expression in (10.8) and we recover the claimed expression. ‚ñ°

Next, we consider the case where $c ( z )$ has $K \geq 2$ of jumps, and find that $\tau _ { I V }$ recovers a convex combination of average treatment effects over compliance strata defined by the jumps in $c ( \cdot )$ . In the case where $c ( \cdot )$ is a bijection (i.e., the instrument $Z _ { i }$ is supported on $K + 1$ points associated with the different cost levels), the following recovers Theorem 2 of Imbens and Angrist [1994].

Corollary 10.5. Under the setting of Theorem 10.3, suppose there are $K$ jumps, i.e., $c ( z )$ takes on unique values $c _ { 0 } > c _ { 1 } > . . . > c _ { K }$ with jumps at $z _ { 1 } < . . . < z _ { K }$ . Then,

$$
\tau_ {I V} = \sum_ {k = 1} ^ {K} \mathbb {E} [ \tau (U _ {i}) \mid c _ {k} \leq U _ {i} <   c _ {k - 1} ] \gamma_ {k} / \sum_ {k = 1} ^ {K} \gamma_ {k}, \tag {10.10}
$$

$$
\gamma_ {k} = (c _ {k - 1} - c _ {k}) \mathbb {P} \left[ Z _ {i} \geq z _ {k} \right] \mathbb {E} \left[ Z _ {i} - \mu_ {z} \mid Z _ {i} \geq z _ {k} \right].
$$

Proof. The result follows from the same argument used for Corollary 10.4.

Finally, we consider the setting where $c ( z )$ is differentiable, under an added assumption that $Z _ { i }$ is normally distributed. We find that $\tau _ { I V }$ recovers a weighted average of marginal treatment effects for units with $U _ { i } = c ( Z _ { i } )$ , i.e., who are indifferent about taking up treatment. The weights are proportional to the ‚Äúlocal strength‚Äù of the instrument, i.e., $- c ^ { \prime } \left( Z _ { i } \right)$ .

Corollary 10.6. Under the setting of Theorem 10.3 suppose that $Z _ { i } \sim$ $\mathcal { N } \left( 0 , 1 \right)$ , and that $c ( z )$ is differentiable. Then, writing $\varphi ( z )$ for the standard normal density,

$$
\tau_ {I V} = \int_ {\mathbb {R}} \tau (c (z)) c ^ {\prime} (z) \varphi (z) d z / \int_ {\mathbb {R}} c ^ {\prime} (z) \varphi (z) d z. \tag {10.11}
$$

Proof. Lemma 1 of Stein [1981] shows that, when $Z _ { i }$ is normally distributed, Cov $[ H ( Z _ { i } ) , Z _ { i } ] = \mathbb { E } \left[ H ^ { \prime } ( Z _ { i } ) \right]$ for any function $H ( \cdot )$ . Applying this result to the last expression in the first display block in the proof of Theorem 10.3, we get

$$
\operatorname {C o v} \left[ Y _ {i}, Z _ {i} \right] = \mathbb {E} \left[ H ^ {\prime} (Z _ {i}) \right], H (z) = \mathbb {E} \left[ \tau (U _ {i}) 1 \left(\{U _ {i} \geq c (z) \}\right) \right] = \int_ {c (z)} ^ {1} \tau (u).
$$

Our result follows from the fundamental theorem of calculus and the chain rule for differentiation; and applying the same technique to the denominator. ‚ñ°

Estimating the marginal treatment effect Throughout this chapter, we‚Äôve taken it as a given that we‚Äôre going to target the estimand (10.2), and then have sought to interpret it in different settings. However, in the context of structural economic models, it‚Äôs often possible to target a wider variety of estimands. A first key result is that, in the latent choice model considered above, the marginal treatment effect (10.7) is identified at continuity points of $c ( z )$ via a simple ‚Äúlocal IV‚Äù construction.

Theorem 10.7. Under the setting of Theorem 10.3, suppose that $c ( z )$ is continuously differentiable at $z$ with $c ^ { \prime } ( z ) < 0$ and $U _ { i }$ has a density satisfying $f ( c ( z ) ) > 0$ . Then, the marginal treatment effec $\tau ( u )$ from (10.7) is identified as

$$
\tau (c (z)) = \frac {\frac {d}{d z} \mathbb {E} [ Y _ {i} \mid Z _ {i} = z ]}{\frac {d}{d z} \mathbb {P} [ W _ {i} = 1 \mid Z _ {i} = z ]}. \tag {10.12}
$$

Proof. Under our threshold-crossing model,

$$
\begin{array}{l} \mathbb {E} \left[ Y _ {i} \mid Z _ {i} = z \right] = \mathbb {E} \left[ Y _ {i} (0) + 1 \left(\{U _ {i} \geq c (Z _ {i}) \}\right) \left(Y _ {i} (1) - Y _ {i} (0)\right) \mid Z _ {i} = z \right] \\ = \mathbb {E} \left[ Y _ {i} (0) + 1 \left(\left\{U _ {i} \geq c (z) \right\}\right) \left(Y _ {i} (1) - Y _ {i} (0)\right) \right] \\ = \mathbb {E} \left[ Y _ {i} (0) \right] + \int_ {c (z)} ^ {1} \tau (u) f (u) d u, \\ \end{array}
$$

where the first equality is due to (10.6) and the exclusion restriction, the second is due to exogeneity, and the third is an application of Fubini‚Äôs theorem. Next, given that $c ( z )$ is continuously differentiable at $z$ , we can use the chain rule to check that

$$
\frac {d}{d z} \mathbb {E} \left[ Y _ {i} \mid Z _ {i} = z \right] = - \tau (c (z)) f (c (z)) c ^ {\prime} (z). \tag {10.13}
$$

Finally, applying the same calculation to the denominator yields (10.12).

Once we have access to the marginal treatment effect, we can use it to build estimators for weighted averages of $\mathbb { E } \left[ \gamma ( u ) \tau ( u ) \right]$ , provided the weights $\gamma ( u )$ only take positive values at points $u = c ( z )$ at which $c ( z )$ is continuous. Heckman and Vytlacil [2005] consider a variety of estimands of this type; see also Mogstad and Torgovitsky [2024].

Example 10. Carneiro, Heckman, and Vytlacil [2011] use the local IV method to estimate returns to college attendance. The authors use data from the 1979 cohort from the National Longitudinal Survey of Youth (consisting of people born between 1957 and 1964), set their outcome variable $Y _ { i }$ to be log-income in 1991, and set their treatment variable $W _ { i }$ to be ever-enrollment in college by 1991. They identify marginal treatment effects via instruments $Z _ { i }$ that shift the desirability of attending college, including the presence of a nearby college, tuition at nearby colleges, and local employment conditions at the time when people turn 17. Their main finding is that, using our notation, $\tau ( u )$ is increasing in $u$ , and that people who are more likely to attend college in the face of adverse nudges (i.e., abstractly, with a higher willingness to pay for college) in fact benefit more from college. Their results thus suggest that peoples‚Äô choices under the model (10.6) can at least directionally be rationalized via private forecasts of future income benefits from college attendance.

# Bibliographic notes

The idea of interpreting the results of instrumental variables analyses in terms of the local average treatment effect goes back to Imbens and Angrist [1994]. Our presentation of the analysis of clinical trials under non-compliance follows

Angrist, Imbens, and Rubin [1996]. We refer to Imbens [2014] and Mogstad and Torgovitsky [2024] for review and recent discussions.

Latent choice models, where people make choices if their (private) value from making that choice exceeds the cost, have a long tradition in economics. In an early example, Roy [1951] considered a model where workers pick a profession by considering their skills at different jobs and then choose the profession that enables them to maximize their wages‚Äîand used it to argue that, if worker skills are correlated across professions but productivity is more responsive to skill in some professions than in others, then we should expect higher average wages in professions with higher returns to skills. It has long been understood that such models cannot be fit via standard linear regression, and that alternative econometric strategies are needed. In one early solution to this challenge, Heckman [1979] considered a parametric latent choice model, and achieved identification by positing joint normality of latent variable $U _ { i }$ and potential outcomes.

More recently, Heckman and Vytlacil [2005] have advocated for latent choice models as a natural framework for interpreting instrumental variables analyses, and have studied methods that target a wide variety estimands beyond the LATE that may be more helpful in setting policy. The identification result (10.12) for the marginal treatment effect via the local IV construction is due to Heckman and Vytlacil [1999]. Kennedy, Lorch, and Small [2019] studies semiparametrically efficient estimation of functions of the marginal treatment effect. Our presentation of the local average treatment effect under supplydemand equilibrium is adapted from Angrist, Graddy, and Imbens [2000].

# Chapter 11 Spillovers and Interference

Throughout our discussions so far, we have relied on the SUTVA assumption whereby the treatment given to one person only affects the targeted person and not others. This assumption is reasonable in a number of setting, including when, in medicine, we want to assess the benefits of a cancer treatment or when, in marketing, we want to assess the effectiveness of a customer-retention program. In other settings, however, this assumption is obviously fraught, and cross-unit treatment spillovers are a first-order concern.

Example 11. Cai, Janvry, and Sadoulet [2015] ran a randomized experiment in rural China to understand whether take-up of government-subsidized weather insurance could be promoted via information sessions that give a detailed presentation on how the insurance product works. The authors were interested in both direct effects of the intervention on people who attend the information sessions, and in spillovers onto the friends of those who attended. Asking about spillovers reflects an underlying belief that information given to some people may affect insurance take-up by others (namely their friends).

Example 12. Blattman et al. [2021] report results on a randomized evaluation of crime-reduction measures in Bogot¬¥a, Colombia. The city identified 1,919 streets as crime hot spots, and randomized them to receive either increased police patrolling, increased municipal services, both interventions or neither; the authors were interested in measuring any effect of these measures on both violent crime or property crime. A concern in the analysis was that, instead of suppressing crime, some localized interventions may only displace it to neighboring streets; and the authors develop techniques for evaluating such spillovers.

Example 13. Ride-sharing platforms seek to connect potential riders with freelance drivers. Many existing platforms propose prices up front, i.e., they first advertise trips to riders at a given price and then seek to connect with a driver once a trip request is made. It is natural to run experiments to fine-tune

these prices for healthy market behavior, but properly accounting for spillovers is crucial in doing so. For example, if one were to randomize access to driver incentives, it is expected that drivers with access to such incentives would earn more per hour than those who don‚Äôt. However, as reported by Hall, Horton, and Knoepfle [2023], giving such incentives to everyone may not increase hourly earnings for drivers‚Äîbecause the incentives may draw more drivers to work for the platform, thus reducing utilization levels of existing drivers (i.e., existing drivers might earn more per hour while actively transporting a passenger, but have this benefit be canceled out by an increased amount of time spent idle). In other words, spillovers arise via market re-equilibriation.

Example 14. Infectious-disease vaccines provide two types of protection against disease spread: Vaccinated people may be less likely to get infected than unvaccinated people given comparable circumstances, and vaccinating a large enough fraction of the population may create a herd-immunity phenomenon that unvaccinated people also benefit from. The emergence of herd immunity is a type of spillover that is relevant to assessing public-health benefits of vaccination; Ogburn and VanderWeele [2017] discuss a modeling framework for estimating these effects.

The spillover mechanisms in all examples above are different. The end result, however, is the same: SUTVA fails, and new ideas are needed to assess the effects of an intervention. This chapter will introduce methods for modeling and testing for the presence of spillovers and, more broadly, cross-unit interference (i.e., treatment given to one person affects others); in the next chapter, we will then turn to questions of estimation and building confidence intervals. For simplicity, we will focus on randomized controlled trial (RCT) settings in this chapter and the next.

# 11.1 Exposure mappings

As in Chapter 1, we assume that we have data on $i = 1$ , . . . , $n$ people, each of whom receives a randomized binary treatment $W _ { i } \in \{ 0 , 1 \}$ and then experiences an outcome $Y _ { i } \in \mathbb { R }$ . Under interference, however, it no longer makes sense to only define two potential outcomes per unit; rather, each unit can now have up to $2 ^ { n }$ potential outcomes $\{ Y _ { i } ( \mathbf { w } ) : \mathbf { w } \in \{ 0 , 1 \} ^ { n } \}$ , corresponding to each possible treatment assignment for the whole study. The associated potential-outcome consistency assumption is66

$$
Y _ {i} = Y _ {i} \left(\mathbf {W}\right), \quad \mathbf {W} = \left(W _ {i}\right) _ {i = 1} ^ {n}. \tag {11.1}
$$

While this notation is similar to that used in Chapter 1, the problem is now substantively much harder and we have an apparent curse of dimensionality to deal with, whereby the number of potential outcomes grows exponentially faster than the study size.

Any approach to causal inference under interference needs to put some structure on the potential outcomes in order to enable accurate treatment effect estimation. Here, we will do so by assuming an exposure mapping: Each unit has an exposure function $H _ { i } : \{ 0 , 1 \} ^ { n } \to \{ \mathcal { H } _ { i }$ with the property that $Y _ { i }$ only depends on the full potential outcome vector W through $H _ { i } ( \mathbf { W } )$ .

Assumption 11.1. An exposure mapping is a set of unit-specific functions $H _ { i } : \{ 0 , 1 \} ^ { n } \to \mathcal { H } _ { i }$ . The assumption that this exposure mapping is well specified is a claim that, for all pairs $\mathbf { w }$ , $\mathbf { w } ^ { \prime } \in \{ 0 , 1 \} ^ { n }$ , we have

$$
Y _ {i} (\mathbf {w}) = Y _ {i} \left(\mathbf {w} ^ {\prime}\right) \text {w h e n e v e r} H _ {i} (\mathbf {w}) = H _ {i} \left(\mathbf {w} ^ {\prime}\right). \tag {11.2}
$$

When there is no risk of confusion, we use overloaded notation such as $Y _ { i } =$ $Y _ { i } ( H _ { i } ( \mathbf { W } ) )$ or $Y _ { i } = Y _ { i } ( H _ { i } )$ .

The simplest type of exposure mapping to work with statistically is the cluster-interference model. Under cluster interference, experimental units are divided into non-overlapping clusters, such that spillovers can be arbitrary within cluster but there are no spillovers across clusters. Formally, in the context of Assumption 11.1, cluster interference posits $H _ { i } ( \mathbf { w } ) = ( w ) _ { j \in C _ { i } }$ , where $C _ { i }$ is the set of units in the same cluster as the $i$ -th unit. The reason cluster interference is easy to work with statistically is that we can simply re-define these clusters as our experimental units of interest. Then, the fact that there is no cross-cluster interference means that SUTVA holds at the level of clusters; we can thus run a cluster-randomized experiment that we then analyze using standard techniques.

Example 15. Cr¬¥epon et al. [2013] study community-level effects of job-search assistance programs. Such job-search programs help program participants find jobs; but the authors are concerned that they may be doing so at the expense of non-participants. To measure community effects, they identify 235 independent labor markets (e.g., cities), and randomize each market to receive different saturation levels (0%, 25%, 50%, 75%, or 100%) of job-search assistance for active job seekers. The authors then run an analysis where they compare community-level outcomes across markets with different saturation levels, i.e., they analyze the data as an RCT where each labor market is a unit and the treatment given to the unit is the saturation level of job-search assistance. The exposure mapping underlying this analysis is the cluster-interference model, with job seekers clustered by labor market.

Other applications call for more complex exposure mappings. For example, in the setting of Example 11, the authors posit that a given farmers‚Äô insurance decisions may be affected by information received by their friends as well as by them directly. This suggests using the framers‚Äô social network to define an exposure mapping, e.g., via the network-interference model below (with friends acting as network neighbors).

Definition 11.1. Under the network-interference model, we assume that each unit $i = 1$ , . . . , $n$ has a set of network neighbors $\mathcal { N } _ { i } \subset \{ 1 , \ldots , n \}$ , with a convention that $i \notin { \mathcal { N } } _ { i }$ , such that the following exposure mapping holds:

$$
Y _ {i} = Y _ {i} \left(H _ {i} (\mathbf {W})\right), \quad H _ {i} (\mathbf {w}) = \left(w _ {j}\right) _ {j \in \{i \} \cup \mathcal {N} _ {i}}. \tag {11.3}
$$

In other words, the network-interference model is a generalization of the cluster-interference model that allows for non-transitivity of spillovers, and the network interference model reduces to the cluster interference model if we impose transitivity $\{ i \} \cup \mathcal { N } _ { i } = \{ j \} \cup \mathcal { N } _ { j }$ for all $j \in \mathcal N _ { i }$ . Under network interference, we can in general no longer eliminate all spillovers via clustering (because the underlying network may be fully connected); and more careful inferential techniques are thus needed. We will return to the question of estimating treatment effects under network interference in Chapter 12. Before doing so, however, we will first discuss how to test for the presence of interference below.

# 11.2 Permutation tests

In Example 11, Cai, Janvry, and Sadoulet [2015] were interested in measuring spillovers from information sharing in a social network. Suppose that for each unit $i$ we know the friends ${ \mathcal { N } } _ { i }$ who could plausibly affect their insurance choices. What might the most parsimonious model for spillovers look like? The network interference model from Definition 11.1 provides one possible answer, but is there evidence that the full generality of this model is needed?

In this setting, one could easily imagine a hierarchy of alternative exposure mappings as follows::

‚Ä¢ $H _ { 0 }$ : No causal effects. $H _ { i } ( \mathbf { w } ) = \varnothing$ , and $Y _ { i } = Y _ { i } ( \emptyset )$ regardless of treatment.   
‚Ä¢ $H _ { 1 }$ : No spillovers. $H _ { i } ( \mathbf { w } ) = w _ { i }$ , and $Y _ { i } = Y _ { i } ( W _ { i } )$ like in Chapter 1.   
‚Ä¢ $H _ { 2 }$ : Anonymous network interference. $H _ { i } ( \mathbf { w } ) ~ = ~ ( w _ { i } , z _ { i } )$ , where $z _ { i } =$ $\textstyle \sum _ { j \in { \mathcal { N } } _ { i } } w _ { i } / \left| \left\{ { \mathcal { N } } _ { i } \right\} \right|$ is the fraction of treated friends and $Y _ { i } = Y _ { i } ( W _ { i } , Z _ { i } )$ .   
‚Ä¢ $H _ { 3 }$ : Network interference. $H _ { i } ( \mathbf { w } ) = ( w _ { j } ) _ { j \in \{ i \} \cup N _ { i } }$ , and $Y _ { i } = Y _ { i } ( H _ { i } )$ .

‚Ä¢ $H _ { 4 }$ : Generic spillovers. $H _ { i } ( \mathbf { w } ) = \mathbf { w }$ , and $Y _ { i } = Y _ { i } ( \mathbf { W } )$ .

The questions about the structure of treatment effects asked in the previous paragraph can then be formalized via null-hypothesis testing. For example, one might first want to test the null ‚Äú $H _ { 0 }$ : no causal effects‚Äù and then, if that test rejects, test ‚Äú $H _ { 1 }$ : no spillovers‚Äù, etc., until one finds an exposure mapping that is not rejected given the data at hand.

Our task is to develop methods for testing each of these nulls. Here, we will do so via permutation testing. We will propose specific tests for $H _ { 0 }$ and $H _ { 1 }$ , and give a general result that can also be used to design tests more the subsequent hypotheses. The main idea of a permutation test is pick a test statistic, and then scramble the treatment assignment in a way that shouldn‚Äôt affect the test statistic under the posited null hypothesis. By construction, we should expect that‚Äîif the null holds‚Äîthen the test statistic evaluated on the original data should fit comfortably within the range on test statistics obtained after scrambling; and if the original test statistic is in fact an outlier we take this as evidence against the null.

Remark 11.1. In our discussion below, we will develop tests for individual hypotheses. It might seem that the program outlined above, i.e., where we sequentially test hypotheses until one fails to reject, would require a multiple testing correction. However, there is in fact no issue with multiple testing here because all null hypotheses are nested, and sequentially running tests on the most-to-least restrictive nulls until one of them fails to reject (and then stopping) is simultaneously be valid against all nulls thanks to the closed testing principle [Marcus, Peritz, and Gabriel, 1976].

Testing the sharp null We first consider the design of a permutation test against the no-causal-effect null $H _ { 0 }$ . This is a ‚Äúsharp‚Äù null in that it fully specifies how treatment affects outcomes (i.e., in no way whatsoever), and so it can be approached using the classical approach of Fisher [1935]: We first choose a test statistic that is likely to take on a large value when the null doesn‚Äôt hold, e.g.,6 7

$$
T (\mathbf {Y}, \mathbf {w}) = \left| \frac {\sum_ {\{i : w _ {i} = 1 \}} Y _ {i}}{| \{i : w _ {i} = 1 \} |} - \frac {\sum_ {\{i : w _ {i} = 0 \}} Y _ {i}}{| \{i : w _ {i} = 0 \} |} \right|, \tag {11.4}
$$

and then reject the null if the test statistic as computed on the realized treatment vector is unusually large relative to values it takes on alternative treatment randomizations we could have (but didn‚Äôt) get. An important fact in

enabling this approach is that, under $H _ { 0 }$ , treatment has no effect on outcomes, and so

$$
T (\mathbf {Y}, \mathbf {w}) = T (\mathbf {Y} (\mathbf {w}), \mathbf {w}) \text {f o r a l l} \mathbf {w} \in \{0, 1 \} ^ {n}, \tag {11.5}
$$

meaning that‚Äîagain under the null‚Äîwe are able impute the actual test statistic we would have computed under different treatment randomizations.

Assumption 11.2. Treatment is assigned according to a completely randomized design: There is a set of possible treatment vectors w over $\{ 0 , 1 \} ^ { n }$ such that $\mathbb { P } \left[ \mathbf { W } = \mathbf { w } \right] = 1 / \left| \boldsymbol { \mathcal { W } } \right|$ for all $\mathbf { w } \in \mathcal { W }$ , independently of potential outcomes.

Theorem 11.1. Suppose that Assumption 11.2 holds. Pick any test statistic $T ~ : ~ \mathbb { R } ^ { n } \times \{ 0 , 1 \} ^ { n } \to \mathbb { R }$ , and evaluate it on the original data to get ${ \cal T } _ { 0 } ~ = ~ { \cal T } ( { \bf Y } , { \bf W } )$ . Pick a number of permutations $B ~ \leq ~ | \mathcal { W } | ~ - ~ 1$ , draw $\mathbf { W } _ { 1 } ^ { \prime } , \ldots , \mathbf { W } _ { B } ^ { \prime }$ $\mathbf { W } _ { B } ^ { \prime }$ uniformly at random and without replacement from ${ \mathcal { W } } \backslash \mathbf { W }$ , and let $T _ { b } ^ { \prime } = T ( \mathbf { Y } , \mathbf { W } _ { b } ^ { \prime } )$ for all $b = 1$ , . . . , $B$ . Then, the permutation p-value68

$$
p = \frac {1}{1 + B} \left(1 + \sum_ {b = 1} ^ {B} 1 \left(T _ {0} \leq T _ {b} ^ {\prime}\right)\right) \tag {11.6}
$$

is valid against the null, i.e., under $H _ { 0 }$ , $\mathbb { P } \left[ p \leq \alpha \right] \leq \alpha$ for all $0 \leq \alpha \leq 1$ .

Proof. Let ${ \mathcal { W } } ^ { \prime } = \{ \mathbf { W } , \mathbf { W } _ { 1 } ^ { \prime } , \dots , \mathbf { W } _ { B } ^ { \prime } \}$ be the unordered set of considered permutations, and let $\mathcal { T } ^ { \prime } = \{ T _ { 0 } , T _ { 1 } ^ { \prime } , \ldots , T _ { B } ^ { \prime } \}$ be the set of considered test statistics. Under the null, we can impute outcomes under alternative randomizations as in (11.5), and so

$$
\mathcal {T} ^ {\prime} = \mathcal {T} ^ {\prime} (\mathcal {W} ^ {\prime}) := \left\{T \left(\mathbf {Y} (\mathbf {w}), \mathbf {w}\right): \mathbf {w} \in \mathcal {W} ^ {\prime} \right\} \tag {11.7}
$$

can be written in terms of the potential outcomes and $\mathcal { W } ^ { \prime }$ in an exchangeable way‚Äîwithout reference to the realized treatment or outcomes. Furthermore $\mathbb { P } \left[ \mathbf { W } = \mathbf { w } \middle | \mathbf { W } \in \mathcal { W } ^ { \prime } \right] = 1 / ( 1 + B )$ by Assumption 11.2, which paired with (11.7) implies that

$$
\mathbb {P} \left[ T _ {0} = t \mid \mathbf {W} \in \mathcal {W} ^ {\prime}, \{\text {p o t . o u t c o m e s} \} \right] = \frac {1}{1 + B} \text {f o r a l l} t \in \mathcal {T} ^ {\prime} (\mathcal {W} ^ {\prime}). \tag {11.8}
$$

It follows that, under Assumption 11.2 and $H _ { 0 }$ , and conditionally on $\mathcal { W }$ and the potential outcomes, the permutation statistic $p$ from (11.6) takes values uniformly at random over $\{ 1 / ( 1 + B ) , 2 / ( 1 + B ) , \ldots , 1 \}$ $\{ 1 / ( 1 + B )$ if there are no ties in $\tau ^ { \prime }$ ; and ties can only make $p$ strictly larger. Thus, the distribution of $p$ stochastically dominates $\mathrm { U n i f } ( [ 0 , 1 ] )$ , and so $p$ is in fact a valid $p$ -value. ‚ñ°

Testing for interference The next question is to design a test for $H _ { 1 }$ , i.e., to test whether SUTVA holds or instead there is evidence of spillovers. To start, we again need to choose a test statistic that will have power to measure deviations from the null‚Äîand there are many ways of doing so. Following Aronow [2012], we here consider test statistics that first choose a set of focal units ${ \mathcal { F } } \subset \{ 1 , \ldots , n \}$ , and set $T = T _ { \mathcal { F } } \left( \mathbf { Y } , \mathbf { w } \right)$ to be some pre-specified functional that only considers outcomes within the focal set. For example, in settings where we believe that spillovers will only really manifest themselves on untreated units (e.g., with informational intervention as in Example 11), one natural choice for the test statistic $T$ is to use the $z$ -coefficient in the regression obtained from regressing $Y _ { i }$ on $z _ { i }$ over the subset of observations with $i \in \mathcal { F }$ and $w _ { i } = 0$ ,

$$
T _ {\mathcal {F}} (\mathbf {Y}, \mathbf {w}) = Y _ {i} \sim z _ {i}: \{i \in \mathcal {F}, w _ {i} = 0 \}, \quad z _ {i} = \sum_ {j \in \mathcal {N} _ {i}} w _ {j} / | \{\mathcal {N} _ {i} \} |. \tag {11.9}
$$

At this point, however, we face a challenge. When testing the sharp null, (11.5) enabled us to compute counterfactual test statistics for any treatment assignment w under $H _ { 0 }$ . Now, however, treatment can affect outcomes under $H _ { 1 }$ (via the direct effect), and so we only have access to the weaker guarantee

$$
T _ {\mathcal {F}} (\mathbf {Y}, \mathbf {w}) = T _ {\mathcal {F}} (\mathbf {Y} (\mathbf {w}), \mathbf {w}) \text {i f} w _ {i} = W _ {i} \text {f o r a l l} i \in \mathcal {F}. \tag {11.10}
$$

Thus, when designing a permutation test for $H _ { 1 }$ , we can only consider those treatment assignments w which match to realized treatment W on the focal set. Doing so requires more delicate methods, which will follow from the general result given below.

Remark 11.2. With any focal unit based approach, we need the set $\mathcal { F }$ of focal units not to be either too big or too small in order for $T$ to have power. If the set of focal units $\mathcal { F }$ is too small the regression (11.9) will be noisy; whereas if the set of focal units $\mathcal { F }$ is too large the set of allowed permutations that preserve treatment assignment over $\mathcal { F }$ will be too small, thus again resulting in a loss of power. The optimal size of $\mathcal { F }$ will depend on the application.

Permutation tests for composite nulls In our setting, a composite null is any null hypothesis that allows W to have some effect on $\mathbf { Y }$ , but restricts how these effects can manifest themselves. To understand how to design permutation tests for composite nulls, it is helpful to review the ingredients that made our test for $H _ { 0 }$ work:

1. Our knowledge of the randomization design enabled us to create a set $\mathcal { W }$ of possible treatment assignments (which includes the realized one).

2. Under the null hypothesis, $T ( \mathbf { Y } ( \mathbf { w } ) , \mathbf { w } ) \ = \ T ( \mathbf { Y } ( \mathbf { W } ) , \mathbf { w } )$ for all ${ \textbf { w } } \in$ $\mathcal { W } ^ { \prime }$ , and so we can impute the counterfactual test statistics $T ( \mathbf { Y } ( \mathbf { w } ) , \mathbf { w } )$ we would have observed under alternate randomizations using only the observed outcomes $\mathbf { Y } = \mathbf { Y } ( \mathbf { W } )$ .   
3. Conditionally on knowing that we chose the set $\mathcal { W } ^ { \prime }$ in step 1, the distribution of $\mathbf { W }$ is uniformly random over $\mathcal { W } ^ { \prime }$ .

The key step here is step 2; and, under the sharp null $H _ { 0 }$ , it is easy to see that we can always impute $T ( \mathbf { Y } ( \mathbf { w } ) , \mathbf { w } )$ from $\mathbf { Y }$ for any test statistic $T$ and any treatment vector w.

In contrast, under composite nulls, we will no longer be able to impute any and all test statistics for all w because the treatment now can have some (restricted) effects on the outcomes. We will still be able to make progress by being more careful in our choice of $T$ and set $\mathcal { W } ^ { \prime }$ of considered treatments; doing so, however, leads to subtle challenges in step 3 above.

The general roadmap for designing permutation tests for a generic composite null $H$ involves first observing the realized treatment $\mathbf { W }$ , and then choosing a set of alternate treatment assignments $\mathcal { W }$ that allows us to impute test statistic $T$ under $H$ . The following result gives general guarantees for permutation tests of this type.

Theorem 11.2. Suppose that we want to test a composite null hypothesis $H$ and that Assumption 11.2 holds. After observing W, we choose a (potentially random) set of treatment vectors $\mathcal { W } ^ { \prime } \subseteq \mathcal { W }$ with $\mathbf { W } \in \mathcal { W } ^ { \prime }$ , and a (potentially random) test statistic $T$ with the property that, under $H$ , $T ( \mathbf { Y } ( \mathbf { w } ) , \mathbf { w } ) =$ $T ( \mathbf { Y } ( \mathbf { W } ) , \mathbf { w } )$ for all w ‚àà W. Let

$$
\varphi_ {\mathcal {W} ^ {\prime}, T} (\mathbf {w}) = \mathbb {P} \left[ \mathcal {W} ^ {\prime}, T \mid \mathbf {W} = \mathbf {w} \right] \tag {11.11}
$$

denote the probability of selecting the treatment set $\mathcal { W } ^ { \prime }$ and test statistic $T$ given that the realized treatment vector was w. Then, the re-weighted permutation $p$ -value

$$
p = \frac {\sum_ {\mathbf {w} \in \mathcal {W} ^ {\prime}} \varphi_ {\mathcal {W} ^ {\prime} , T} (\mathbf {w}) 1 \left(\left\{T \left(\mathbf {Y} , \mathbf {W}\right) \leq T \left(\mathbf {Y} , \mathbf {w}\right) \right\}\right)}{\sum_ {\mathbf {w} \in \mathcal {W} ^ {\prime}} \varphi_ {\mathcal {W} ^ {\prime} , T} (\mathbf {w})} \tag {11.12}
$$

is valid against the null, i.e., under $H$ , $\mathbb { P } \left[ p \leq \alpha \right] \leq \alpha$ for all $0 \leq \alpha \leq 1$

Proof. Define $\tau ^ { \prime }$ as in the proof of Theorem 11.1, and note that (11.7) still holds thanks to our careful choice of the $\mathcal { W } ^ { \prime }$ and $T$ . Now, the pair $( \mathcal { W } , T )$ is chosen only based on knowledge of $\mathbf { W }$ , and under a constraint that we must have $\mathbf { W } \in \mathcal { W } ^ { \prime }$ . Thus, under Assumption 11.2, we can use Bayes‚Äô rule to

verify that, conditionally on knowing that $\mathcal { W } ^ { \prime }$ and $T$ were selected as the set of considered randomizations and test statistic,

$$
\mathbb {P} \left[ \mathbf {W} = \mathbf {w} \mid \mathcal {W} ^ {\prime}, T \right] = \varphi_ {\mathcal {W} ^ {\prime}, T} (\mathbf {w}) / \sum_ {\mathbf {w} ^ {\prime} \in \mathcal {W} ^ {\prime}} \varphi_ {\mathcal {W} ^ {\prime}, T} \left(\mathbf {w} ^ {\prime}\right) \tag {11.13}
$$

for all $\mathbf { w } \in \mathcal { W } ^ { \prime }$ ; and this similarly induces a weighted conditional distribution for $T _ { 0 }$ over $\mathcal { T } ^ { \prime }$ as in (11.8) under the null. Let $t _ { ( 1 ) } \geq t _ { ( 2 ) } \geq . . . \geq t _ { ( | \mathcal { W } ^ { \prime } | ) }$ be order statistics of the test statistics in $\mathcal { T } ^ { \prime }$ , with associated weights œï(1), . . . , $\varphi _ { ( \left. W ^ { \prime } \right. ) }$ used in (11.12). If there are no ties in $\tau$ we can then verify that, under the null,

$$
\mathbb {P} \left[ p \leq \alpha \mid \mathcal {W} ^ {\prime}, \mathbf {Y} \right] = \max  \left\{t = \sum_ {j = 1} ^ {k} \varphi_ {(j)} / \sum_ {j = 1} ^ {| \mathcal {W} ^ {\prime} |} \varphi_ {(j)}: t \leq \alpha \right\} \leq \alpha , \tag {11.14}
$$

and again the presence of ties will again only make $p$ strictly larger.

![](images/bed0347ec8b9ce0054eb7999585886763cd5d640ef46e8d7210d6c33982b9469.jpg)

Application: Testing $H _ { 1 }$ We now return to the question of how to design a permutation test for the presence of interference using the test statistic (11.9). Using notation from Theorem 11.2, the imputability property (11.10) for focal unit based test statistics implies that we can use them together with the permutation set

$$
\mathcal {W} ^ {\prime} (\mathcal {F}) = \left\{w \in \mathcal {W}: w _ {i} = W _ {i} \text {f o r a l l} i \in \mathcal {F} \right\}. \tag {11.15}
$$

Theorem 11.2 then applies directly. The remaining challenge is that we now need to account for the weights $\varphi _ { \mathcal F } ( \mathbf { w } ) = \mathbb { P } \lfloor \mathcal { F }  \mathbf { W } = \mathbf { w } \rfloor$ , which measure dependence between our choice of focal units and the realized randomization. In principle, one could compute these quantities and apply (11.12) directly; however, in the existing literature, most proposals have sought choices of $\mathcal { F }$ obviate the need to consider weights by construction.

One way to side-step this challenge, discussed by Athey, Eckles, and Imbens [2018a], is to choose the set of focal units $\mathcal { F }$ deterministically, without looking at W. In this case, $\mathbb { P } \lfloor \mathcal { F }  \mathbf { W } = \mathbf { w } \rfloor = 1$ , and the weights vanish and can thus be ignored. Such an approach, however, may not be optimal in terms of power; e.g., if we use (11.9) as our test statistic, then there‚Äôs seemingly no value from including any treated units in $\mathcal { F }$ (since they are ignored by the test statistic).

Meanwhile, Basse, Feller, and Toulis [2019] observed that if we can guarantee that $\varphi _ { \mathcal { F } } \left( \mathbf { w } \right)$ is constant for all $\mathbf { w } \in \mathcal { W } ^ { \prime }$ , then we can ignore the weights because they cancel out in (11.12); and so we can use regular (unweighted) permutation -values even though $\mathcal { F }$ was chosen in terms of W. This happens, $p$

for example, in a completely randomized trial with $n _ { 1 }$ treated units and $n _ { 0 }$ controls, and we choose $| \mathcal F |$ units as focal units uniformly at random among all units with $W _ { i } = 0$ , so $\varphi _ { \mathcal { F } } \left( \mathbf { w } \right) = \binom { n _ { 0 } } { | \mathcal { F } | }$ is constant. Or, consider a design where all units are first divided into equally sized clusters $C _ { k }$ for $k = 1$ , . . . , $K$ , and then we randomize $n _ { 1 }$ units to treatment such that at most one person per cluster is treated, i.e., we run a completely randomized experiment over69

$$
\mathcal {W} = \left\{\mathbf {w} \in \{0, 1 \} ^ {n}: \sum_ {i} w _ {i} = n _ {1}, \sum_ {\{i \in C _ {k} \}} w _ {i} \leq 1 \text {f o r a l l} 1 \leq k \leq K \right\}. \tag {11.16}
$$

Then, if we construct $\mathcal { F }$ by selecting exactly one control unit per cluster, one can check that in fact $\varphi _ { \mathcal { F } } \left( \mathbf { w } \right)$ is constant for all $\mathbf { w } \in \mathcal { W } ^ { \prime }$ .

# Bibliographic notes

The general approach of modeling causal effects under interference using an extended set of potential outcomes goes back to early work by Halloran and Struchiner [1995], Hudgens and Halloran [2008] and Sobel [2006]. The use of exposure mappings to mitigate the curse of dimensionality was introduced by Aronow and Samii [2017] and Manski [2013].

The paradigm for causal inference used in Chapter 11.2, i.e., one focused on testing various null hypotheses that restrict how treatment can affect potential outcomes, is often called the ‚ÄúFisherian approach‚Äù in recognition of the seminal work of Fisher [1935] on permutation testing. The Fisherian approach is then contrasted with the ‚ÄúNeymanian approach‚Äù, which is focused on estimating average treatment effects (as opposed to exact restrictions on the potential outcomes)‚Äîand is also the approach we have focused on in most of this book. When the distinction needs to be made, the sharp null (e.g., $Y _ { i } ( 0 ) = Y _ { i } ( 1 )$ for all $i$ ) is often referred to as the Fisher null, while the usual (or weak) null (e.g., $\begin{array} { r } { \sum _ { i } \big ( Y _ { i } ( 1 ) - Y _ { i } ( 0 ) \big ) = 0 } \end{array}$ ) is referred to as the Neyman null; see Ding [2017] for further discussion.

Our discussion of permutation tests under interference is adapted from Athey, Eckles, and Imbens [2018a] and Basse, Feller, and Toulis [2019]. One aspect of permutation testing that we have not put much emphasis on in this chapter is the choice of test statistic: We simply used point estimates of various quantities likely to be non-zero under the alternative, e.g., the difference in

means in (11.4). Permutation tests are exact under the sharp null, regardless of our choice of test statistic. However, the choice of test statistic matters in terms of the power we get under various alternatives of interest, and here test statistics based on point estimates of treatment effects, e.g., the difference in means used in (11.4), can perform unexpectedly poorly.

To understand the power issue, consider the large-sample behavior of a permutation test in a setting with

$$
\binom {Y _ {i} (0)} {Y _ {i} (1)} \sim \mathcal {N} \left(\binom {\mu_ {0}} {\mu_ {1}}, \binom {\sigma_ {0} ^ {2}} {0}, \binom {0} {\sigma_ {1} ^ {2}}\right), \tag {11.17}
$$

and $n _ { 1 } / n = \pi \in ( 0 , 1 )$ . The difference in means test static on the original data has distribution $T _ { 0 } = \mathcal { N } \left( \mu _ { 1 } - \mu _ { 0 } , \sigma _ { T } ^ { 2 } / n \right)$ with $\sigma _ { T } ^ { 2 } = \sigma _ { 0 } ^ { 2 } / ( 1 - \pi ) + \sigma _ { 1 } ^ { 2 } / \pi$ . The usual $t$ -test would then reject the null when the ratio $\sqrt { n } T _ { 0 } / \sigma _ { T }$ is far from 0. On the other hand, because the permutation test jumbles the data, one can check that the behavior of $T _ { b } ^ { \prime }$ depends on moments of the pooled data instead, and the permutation distribution can be approximated as [Romano, 1990]

$$
\mathcal {L} \left(T _ {b} ^ {\prime}\right) \approx \mathcal {N} \left(0, \sigma_ {Y} ^ {2} / n\right), \sigma_ {Y} ^ {2} = \pi (1 - \pi) \left(\mu_ {1} - \mu_ {0}\right) ^ {2} + \frac {(1 - \pi) \sigma_ {0} ^ {2} + \pi \sigma_ {1} ^ {2}}{\pi (1 - \pi)}, \tag {11.18}
$$

thus implying that, effectively, the permutation test rejects the null when $\sqrt { n } T _ { 0 } / \sigma _ { Y }$ is far from 0. We can then directly read out several unexpected behaviors of the permutation test from this comparison. If $\sigma _ { 0 } ^ { 2 } = \sigma _ { 1 } ^ { 2 }$ and $\mu _ { 1 } \neq \mu _ { 0 }$ (i.e., the treatment shifts the mean but not that variance), then $\sigma _ { Y } ^ { 2 } > \sigma _ { T } ^ { 2 }$ and so the permutation test will be less powerful than the usual $t$ -test. On the other hand, permutation tests with a difference in means test statistic can have non-trivial power in settings where the Neymanian null of zero average effect holds, i.e., they are generally not valid (even asymptotically) against the Neymanian null. To see this, note that when if $\mu _ { 1 } = \mu _ { 0 }$ , $\pi < 0 . 5$ and $\sigma _ { 1 } ^ { 2 } > \sigma _ { 0 } ^ { 2 }$ , then $\sigma _ { Y } ^ { 2 } < \sigma _ { T } ^ { 2 }$ and so the permutation test must have more power than the usual $t$ -test (which in turn has the nominal level here).

One can solve this problem‚Äîand generally improve the large-sample behavior of permutation tests‚Äîby using studentized test statistics, e.g., a two-sample $t$ -statistic instead of (11.4), or a heteroskedasticity-robust regression $t$ -statistic instead of (11.9). Chung and Romano [2013] provide results implying that, at least in the setting of Theorem 11.1, a permutation test using a studentized test statistic pairs finite-sample validity against the sharp (Fisher) null hypothesis while matching the behavior of the usual test against the Neymanian null of a zero average treatment effect in large samples. Cohen and Fogarty [2022] discusses further results on unifying Neymanian and Fisherian approaches to testing for the presence of causal effects.

# Chapter 12 Estimating Treatment Effects under Interference

In the previous chapter, we introduced exposure mappings as a tool for modeling cross-unit interference, and permutation-based methods for testing for the presence of interference. The next natural question‚Äîand our focus in this chapter‚Äîis: Once we‚Äôve accepted that interference exists, how can we estimate relevant treatment effects that account for interference?

Exposure effects For simplicity, we will here focus on a setting where Assumption 11.1 holds with a finite-cardinality exposure mapping. Specifically, we will consider a setting where we have $i = 1 , \ldots , n$ $n$ units with outcomes $Y _ { i } ~ \in ~ \mathbb { R }$ and treatment $W _ { i } \in \{ 0 , 1 \}$ . There can be cross-unit interference; however, this interference can be captured in terms of an exposure mapping $H _ { i } : \{ 0 , 1 \} ^ { n } \to \mathcal { H }$ with a shared domain $\mathcal { H }$ with $| { \mathcal { H } } | < \infty$ . We thus have potential outcomes with a consistency condition

$$
\{Y _ {i} (h) \} _ {h \in \mathcal {H}}, \qquad Y _ {i} = Y _ {i} (H _ {i} (\mathbf {W})). \tag {12.1}
$$

Given this assumption, we can define various sample-average treatment effects by comparing mean potential outcomes across exposure levels $h$ , $h ^ { \prime } \in \mathcal { H }$ ,

$$
\bar {\tau} (h, h ^ {\prime}) = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(Y _ {i} \left(h ^ {\prime}\right) - Y _ {i} (h)\right). \tag {12.2}
$$

Our goal is to estimate these quantities and provide confidence intervals for them.

Example 16. Rogers and Feller [2018] reports results on a randomized trial to improve school attendance among students with high risk of absenteeism by sending attendance information to parents. In some settings, a family had multiple students eligible for the study, and the authors were interested in

spillovers: Did sending attendance information about one student also affect their siblings‚Äô behavior? To study this question, the authors posited an exposure mapping with 3 exposure levels: (1) student received treatment; (2) student untreated by with treated sibling; and (3) student in family with no treatment. Then, one can define a number of natural estimands of the form (12.2), such as a direct effect (1) vs. (3), and a spillover effect (2) vs. (3).

Unbiased estimation The setup considered here, i.e., with a randomized trial executed on a set of $n$ unspecified study participants, is closely related to the setting of Theorem 1.1, except that now of course SUTVA no longer holds and we instead need to rely on a more complex exposure mapping to capture interference. And it turns out that an analogue to Theorem 1.1 still holds: We can get unbiased estimates for the exposure contrasts (12.2) essentially without further assumptions.

The simplest way to construct unbiased estimators here is via inversepropensity weighting (IPW). Suppose that treatment is Bernoullirandomized,

$$
W _ {i} \sim \operatorname {B e r n o u l l i} \left(e _ {i}\right), \quad 0 <   e _ {i} <   1, \tag {12.3}
$$

independently for all $i = 1$ , . . . , $n$ , and let $e _ { i } ( h ) = \mathbb { P } \left[ H _ { i } ( \mathbf { W } ) = h \right]$ with treatment generated according to (12.3). The, the natural IPW estimator,

$$
\hat {\tau} _ {I P W} (h, h ^ {\prime}) = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {1 (\{H _ {i} (\mathbf {W}) = h ^ {\prime} \}) Y _ {i}}{e _ {i} (h ^ {\prime})} - \frac {1 (\{H _ {i} (\mathbf {W}) = h \}) Y _ {i}}{e _ {i} (h)}\right), \tag {12.4}
$$

is unbiased for $\bar { \tau } ( h , h ^ { \prime } )$ . Throughout, we use notation of the type

$$
\mathbb {E} _ {W} \left[ \hat {\tau} _ {I P W} \left(h, h ^ {\prime}\right) \right] = \mathbb {E} \left[ \hat {\tau} _ {I P W} \left(h, h ^ {\prime}\right) \mid \left\{Y _ {i} (h) \right\} _ {i = 1, \dots , n; h \in \mathcal {H}} \right], \tag {12.5}
$$

i.e., where $\mathbb { E } _ { W }$ denotes expectations over random treatment assignment while holding potential outcomes fixed.

Theorem 12.1. Under assumptions (12.1) and (12.3), suppose furthermore that $e _ { i } ( h )$ , $e _ { i } ( h ^ { \prime } ) > 0$ for all $i = 1$ , . . . , $n$ . Then

$$
\mathbb {E} _ {W} \left[ \hat {\tau} _ {I P W} \left(h, h ^ {\prime}\right) \right] = \bar {\tau} \left(h, h ^ {\prime}\right). \tag {12.6}
$$

Proof. Invoking (12.1) and randomization yields

$$
\begin{array}{l} \mathbb {E} _ {W} \left[ \hat {\tau} _ {I P W} (h, h ^ {\prime}) \right] \\ = \mathbb {E} _ {W} \left[ \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {1 \left(\left\{H _ {i} (\mathbf {W}) = h ^ {\prime} \right\}\right) Y _ {i} (h ^ {\prime})}{e _ {i} (h ^ {\prime})} - \frac {1 \left(\left\{H _ {i} (\mathbf {W}) = h \right\}\right) Y _ {i} (h)}{e _ {i} (h)}\right) \right] \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {\mathbb {E} _ {W} \left[ 1 \left(\left\{H _ {i} (\mathbf {W}) = h ^ {\prime} \right\}\right) \right] Y _ {i} (h ^ {\prime})}{e _ {i} (h ^ {\prime})} - \frac {\mathbb {E} _ {W} \left[ 1 \left(\left\{H _ {i} (\mathbf {W}) = h \right\}\right) Y _ {i} (h) \right]}{e _ {i} (h)}\right) \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(Y _ {i} \left(h ^ {\prime}\right) - Y _ {i} (h)\right). \\ \end{array}
$$

For the last equality we also used (12.3) and the fact that $e _ { i } ( h )$ , $e _ { i } ( h ^ { \prime } ) > 0$ .

![](images/a77d4695cb7ffe1c89ccd94fa6e37e4b40b8acb343c2ee87549c4dd56209fb00.jpg)

Inference and uncertainty quantification Where things get more challenging is in seeking confidence intervals. The result above was a generalization of Theorem 1.1 to settings with interference, with a proof following exactly the same blueprint. In Chapter 1, when we sought to move past unbiasedness and establish inferential results, we added an extra assumption that potential outcomes are independently sampled from a broader population (see, e.g., Theorem 1.2). However, while such an IID-sampling assumption is easy to make under SUTVA, it is much more challenging to posit general sampling assumptions for potential outcomes under interference. Units now interact with each other (e.g., they are friends in a social network), and writing down credible generative models that capture such cross-unit relationships (e.g., writing down credible generative models for friendship networks) is something that requires deep subject matter knowledge and cannot easily be done at the level of abstraction sought here.

In this chapter, we will pursue an alternate route and seek to establish inference results that only depend on random treatment assignment‚Äîand do not make any sampling assumptions on the potential outcomes. In the causal inference literature, this approach is often referred to as the finite-population approach, as it does not appeal to the existence of a superpopulation from which units were drawn. We will start, in Section 12.1, by reviewing finite-population methods under SUTVA‚Äîand revisiting our discussion from Chapter 1 without the IID sampling assumption. Then, in Section 12.2, we will extend this discussion to settings with interference.

# 12.1 Finite-population methods

Our goal here is to provide an alternative to Theorem 1.2 that enables inference in randomized-controlled trials under SUTVA without relying on superpopulation-sampling assumption. Finite-population analysis of randomized trials, including the results given here, go back to Neyman [1923]. The following result considers finite-population analysis in the case of a Bernoulli design; Neyman [1923] worked under complete randomization, i.e., where the number of treated units is fixed a-priori, but the key insights are the same. Under SUTVA, we are only interested in the treatment-control contrast, and so will use short-hand $\bar { \tau } : = \bar { \tau } ( 0 , 1 )$ for the sample-average treatment effect (SATE), $\hat { \tau } _ { I P W } : = \hat { \tau } _ { I P W } ( 0 , 1 )$ for the estimated treatment effect, and $e _ { i } = e _ { i } ( 1 )$ for the propensity score.

Theorem 12.2. Under the setting of Theorem 12.1, suppose furthermore that SUTVA holds, i.e., $H _ { i } ( \mathbf { w } ) = w _ { i }$ . Then

$$
n \operatorname {V a r} _ {W} [ \hat {\tau} _ {I P W} ] = \bar {\sigma} ^ {2} \leq \sigma^ {2},
$$

$$
\bar {\sigma} ^ {2} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {Y _ {i} ^ {2} (0)}{1 - e _ {i}} + \frac {Y _ {i} ^ {2} (1)}{e _ {i}} - \left(Y _ {i} (1) - Y _ {i} (0)\right) ^ {2}\right), \tag {12.7}
$$

$$
\sigma^ {2} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {Y _ {i} ^ {2} (0)}{1 - e _ {i}} + \frac {Y _ {i} ^ {2} (1)}{e _ {i}}\right).
$$

Furthermore, $\sigma ^ { 2 }$ admits an unbiased estimator,

$$
\mathbb {E} _ {W} \left[ \hat {\sigma} ^ {2} \right] = \sigma^ {2}, \quad \hat {\sigma} ^ {2} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {(1 - W _ {i}) Y _ {i} ^ {2}}{(1 - e _ {i}) ^ {2}} + \frac {W _ {i} Y _ {i} ^ {2}}{e _ {i} ^ {2}}\right). \qquad (1 2. 8)
$$

Proof. Thanks to Theorem 12.1, we have

$$
\begin{array}{l} n \operatorname {V a r} _ {W} \left[ \hat {\tau} _ {I P W} \right] = n \mathbb {E} _ {W} \left[ \left(\hat {\tau} _ {I P W} - \bar {\tau}\right) ^ {2} \right] \\ = n \mathbb {E} _ {W} \left[ \left(\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i}}{e _ {i}} - \frac {1 - W _ {i}}{1 - e _ {i}}\right) Y _ {i} - \frac {1}{n} \sum_ {i = 1} ^ {n} (Y _ {i} (1) - Y _ {i} (0))\right) ^ {2} \right]. \\ \end{array}
$$

By SUTVA and because the $W _ { i }$ are independent of each other, we can further

expand this expression as

$$
\begin{array}{l} n \mathbb {E} _ {W} \left[ \left(\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i}}{e _ {i}} - 1\right) Y _ {i} (1) - \left(\frac {1 - W _ {i}}{1 - e _ {i}} - 1\right) Y _ {i} (0)\right) ^ {2} \right] \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} \mathbb {E} _ {W} \left[ \left(\left(\frac {W _ {i}}{e _ {i}} - 1\right) Y _ {i} (1) - \left(\frac {1 - W _ {i}}{1 - e _ {i}} - 1\right) Y _ {i} (0)\right) ^ {2} \right] \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\mathbb {E} _ {W} \left[ \left(\frac {W _ {i} - e _ {i}}{e _ {i}}\right) ^ {2} \right] Y _ {i} ^ {2} (1) + \mathbb {E} _ {W} \left[ \left(\frac {W _ {i} - e _ {i}}{1 - e _ {i}}\right) ^ {2} \right] Y _ {i} ^ {2} (0)\right) \\ + 2 \mathbb {E} _ {W} \left[ \frac {\left(W _ {i} - e _ {i}\right) ^ {2}}{e _ {i} (1 - e _ {i})} \right] Y _ {i} (0) Y _ {i} (1) \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\left(\frac {1}{e _ {i}} - 1\right) Y _ {i} ^ {2} (1) + \left(\frac {1}{1 - e _ {i}} - 1\right) Y _ {i} ^ {2} (0) + 2 Y _ {i} (0) Y _ {i} (1)\right) \\ = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {Y _ {i} ^ {2} (1)}{e _ {i}} + \frac {Y _ {i} ^ {2} (0)}{1 - e _ {i}} - \left(Y _ {i} (1) - Y _ {i} (0)\right) ^ {2}\right), \\ \end{array}
$$

where the second equality above holds because potential outcomes are treated as fixed, and the last follows by expanding out the square $( Y _ { i } ( 1 ) - Y _ { i } ( 0 ) ) ^ { 2 }$ . This establishes (12.7). Finally, (12.8) can be proven by following the argument used in Theorem 12.1. ‚ñ°

The main observation is that, under the finite-population model, the variance $\bar { \sigma } ^ { 2 }$ depends on differences of potential outcomes, and cannot generally be estimated from data without further assumptions. However, the variance admits a simple upper bound $\sigma ^ { 2 }$ that is identified from data‚Äîand in fact this variance estimate corresponds to the usual variance estimate for ${ \hat { \tau } } _ { I P W }$ under IID sampling. Thus, exact inference for the ATE under IID sampling provides conservative inference for the SATE in the finite-population model. This fact will also show up under interference.

It remains to establish a construction for confidence intervals. Since we no longer have access to an IID stream of data, we will no longer be able to invoke a classical central-limit theorem; rather, we will need to rely on finite-sample Gaussian approximation results. In the result below, we will also consider a self-normalized version of IPW,

$$
\hat {\tau} _ {S I P W} = \frac {\sum_ {i = 1} ^ {n} W _ {i} Y _ {i} / e _ {i}}{\sum_ {i = 1} ^ {n} W _ {i} / e _ {i}} - \frac {\sum_ {i = 1} ^ {n} (1 - W _ {i}) Y _ {i} / (1 - e _ {i})}{\sum_ {i = 1} ^ {n} (1 - W _ {i}) / (1 - e _ {i})}, \tag {12.9}
$$

as this generally improves large-sample performance (see, e.g., Exercise 1).

Theorem 12.3. Suppose we have a sequence of randomized trials with growing sample size n that all satisfy the conditions of Theorem 12.2, and write $\bar { \tau } _ { n }$ for the SATE in each of these randomized trials. Suppose furthermore that there are constants $\eta$ , $M < \infty$ such that $\eta \le e _ { i } \le 1 - \eta$ and $| Y _ { i } ( 0 ) |$ , $| Y _ { i } ( 1 ) | \le M$ for all units, and that $\begin{array} { r } { \operatorname* { l i m } \operatorname* { i n f } _ { n \to \infty } \bar { \sigma } _ { n } ^ { 2 } > 0 } \end{array}$ with $\bar { \sigma } _ { n } ^ { 2 }$ as defined below. Then,

$$
\sqrt {n} \left(\frac {\hat {\tau} _ {S I P W} - \bar {\tau} _ {n}}{\bar {\sigma} _ {n}}\right) \Rightarrow \mathcal {N} (0, 1), \qquad \bar {\mu} _ {n} (w) = \frac {1}{n} \sum_ {i = 1} ^ {n} Y _ {i} (w), \qquad \qquad (1 2. 1 0)
$$

$$
\bar {\sigma} _ {n} ^ {2} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {(Y _ {i} (0) - \bar {\mu} _ {n} (0)) ^ {2}}{1 - e _ {i}} + \frac {(Y _ {i} (1) - \bar {\mu} _ {n} (1)) ^ {2}}{e _ {i}} - (Y _ {i} (1) - Y _ {i} (0)) ^ {2}\right),
$$

Furthermore, the following variance estimator

$$
\hat {\mu} _ {n} (0) = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {\left(1 - W _ {i}\right) Y _ {i}}{1 - e _ {i}}, \quad \hat {\mu} _ {n} (1) = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {W _ {i} Y _ {i}}{e _ {i}}, \tag {12.11}
$$

$$
\hat {\sigma} _ {n} ^ {2} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {(1 - W _ {i}) (Y _ {i} - \hat {\mu} _ {n} (0)) ^ {2}}{(1 - e _ {i}) ^ {2}} + \frac {W _ {i} (Y _ {i} - \hat {\mu} _ {n} (1)) ^ {2}}{e _ {i} ^ {2}}\right),
$$

is asymptotically conservative, lim $\mathrm { s u p } _ { n \to \infty } \bar { \sigma } _ { n } / \hat { \sigma } _ { n } \le _ { p } 1$ , and usual normal confidence intervals are valid

$$
\lim  _ {n \rightarrow \infty} \sup  _ {n \rightarrow \infty} \mathbb {P} \left[ | \hat {\tau} _ {S I P W} - \bar {\tau} _ {n} | \leq \hat {\sigma} _ {n} / \sqrt {n} \Phi^ {- 1} (1 - \alpha / 2) \right] \geq 1 - \alpha , \tag {12.12}
$$

for any $0 < \alpha < 1$ .

Proof. We start by establishing $\sqrt { n }$ -consistency of the $\hat { \mu } _ { n } ( w )$ as given in (12.11): For any $\varepsilon > 0$ and $w = 0$ , 1:

$$
\mathbb {P} \left[ | \hat {\mu} _ {n} (w) - \bar {\mu} _ {n} (w) | \geq \frac {\varepsilon}{\sqrt {n}} \right] \leq \frac {M ^ {2}}{\eta \varepsilon^ {2}}. \tag {12.13}
$$

Focusing on the case with $w = 1$ (the case $w = 0$ is analogous), one can follow the proofs of Theorems 12.1 and 12.2 to verify that

$$
\mathbb {E} _ {W} \left[ \hat {\mu} _ {n} (1) \right] = \bar {\mu} _ {n} (1), \quad \operatorname {V a r} _ {W} \left[ \hat {\mu} _ {n} (w) \right] = \frac {1}{n ^ {2}} \sum_ {i = 1} ^ {n} \frac {1 - e _ {i}}{e _ {i}} Y _ {i} ^ {2} (1) \leq \frac {1}{n} \frac {M ^ {2}}{\eta}; \tag {12.14}
$$

the claim (12.13) then follows for Chebyshev‚Äôs inequality. Now, moving to our

estimator itself, we can use SUTVA and self-normalization to check that

$$
\hat {\tau} _ {S I P W} - \bar {\tau} _ {n} = \Delta (1) \Big / \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {W _ {i}}{e _ {i}} - \Delta (0) \Big / \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {1 - W _ {i}}{1 - e _ {i}},
$$

$$
\Delta (0) = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {(1 - W _ {i}) (Y _ {i} (0) - \bar {\mu} _ {n} (0))}{1 - e _ {i}}, \quad \Delta (1) = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {W _ {i} (Y _ {i} (1) - \bar {\mu} _ {n} (1))}{e _ {i}}.
$$

Given overlap, we immediately see that

$$
\frac {1}{n} \sum_ {i = 1} ^ {n} \frac {1 - W _ {i}}{1 - e _ {i}} - 1, \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {W _ {i}}{e _ {i}} - 1 = \mathcal {O} _ {P} \left(\frac {1}{\sqrt {n}}\right),
$$

and this together with (12.13) implies $\Delta ( 0 )$ , $\Delta ( 1 ) = \mathcal { O } _ { P } ( 1 / \sqrt { n } )$ . Thus,

$$
\hat {\tau} _ {S I P W} - \bar {\tau} _ {n} = \Delta (1) - \Delta (0) + \mathcal {O} _ {P} \left(\frac {1}{n}\right). \tag {12.15}
$$

Next, by Theorems 12.1 and 12.2, we immediately get

$$
\mathbb {E} _ {W} [ \Delta (1) - \Delta (0) ] = 0, \quad n \operatorname {V a r} _ {W} [ \Delta (1) - \Delta (0) ] = \bar {\sigma} _ {n} ^ {2}.
$$

We can again use our overlap and boundedness assumptions to verify that all summands comprising $\Delta ( 1 ) - \Delta ( 0 )$ are bounded by $4 M / \eta$ , and so the Berry‚Äì Esseen bound implies that

$$
\sup  _ {z \in \mathbb {R}} \left| \mathbb {P} \left[ \frac {\sqrt {n} (\Delta (1) - \Delta (0))}{\bar {\sigma} _ {n}} \leq z \right] - \Phi (z) \right| \leq \frac {6 4 C M ^ {3} / \eta^ {3}}{\bar {\sigma} _ {n} ^ {3} \sqrt {n}}, \tag {12.16}
$$

where $\Phi ( \cdot )$ is the standard Gaussian cumulative distribution function and $C$ is the Berry‚ÄìEsseen constant; we also note that the right-hand side term of (12.16) goes to 0 with $n$ because we have assumed that $\begin{array} { r } { \operatorname* { l i m } \operatorname* { i n f } _ { n \to \infty } \bar { \sigma } _ { n } ^ { 2 } > 0 } \end{array}$ . The result (12.10) follows by combining (12.15) with (12.16). Finally, by standard concentration results along with (12.13),

$$
\lim _ {n \to \infty} \hat {\sigma} _ {n} ^ {2} - \sigma_ {n} ^ {2} = _ {p} 0, \quad \sigma_ {n} ^ {2} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {(Y _ {i} - \bar {\mu} _ {n} (0)) ^ {2}}{1 - e _ {i}} + \frac {(Y _ {i} - \bar {\mu} _ {n} (1)) ^ {2}}{e _ {i}}\right),
$$

and $\sigma _ { n } ^ { 2 } \geq \bar { \sigma } _ { n } ^ { 2 }$ by Theorem 12.2. The conservative inference result (12.12) then follows from (12.10) because $\begin{array} { r } { \operatorname* { l i m } \operatorname* { i n f } _ { n \to \infty } \bar { \sigma } _ { n } ^ { 2 } > 0 } \end{array}$ .

Note that in the case of uniformly randomized trials (i.e., $e _ { i } ~ = ~ \pi$ is the same for all units), the final obtained confidence interval construction (12.12) is exactly the same as (1.11) from Chapter 1.70 Earlier, we had shown (via a simple argument) that (1.11) is asymptotically exact for the ATE under IID sampling assumptions. It‚Äôs somewhat remarkable that, as found here, the same confidence interval is also asymptotically conservative for the SATE without making any sampling assumptions.

# 12.2 Confidence intervals for exposure effects

We now return to our main task of interest, i.e., inference for exposure effects as defined in (12.2). In addition to assuming a finite-cardinality exposure mapping, we will also assume network interference structure as in Definition 11.1, i.e., that each unit $i$ has a known set ${ \mathcal { N } } _ { i }$ of influencer units (or, informally friends), with $i \not \subset \mathcal { N } _ { i } \subset \{ 1 , . . . , n \}$ , such that

$$
Y _ {i} (\mathbf {w}) = Y _ {i} \left(\mathbf {w} ^ {\prime}\right) \text {w h e n e v e r} w _ {i} = w _ {i} ^ {\prime} \text {a n d} w _ {j} = w _ {j} ^ {\prime} \text {f o r a l l} j \in \mathcal {N} _ {i}. \tag {12.17}
$$

In conjunction with (12.1), the condition (12.17) can be simplified to a requirement that $H _ { i }$ only depends on $w _ { i }$ and $\mathbf { w } _ { \mathcal { N } _ { i } }$ .

The two assumptions we make on the exposure mapping, (12.1) and (12.17), play different roles: (12.1) is primarily used to justify the estimands (and we will invoke it in a SUTVA-like manner), whereas (12.17) is used to control correlations and establish convergence properties for sample averages. In particular, the network interference model induces a natural randomization dependency graph G ‚àà {0, 1}n√ón $G \in \{ 0 , 1 \} ^ { n \times n }$ on potential outcomes,

$$
G _ {i j} = 1 \left(\left\{\mathcal {N} _ {i} \cup \{i \} \right\} \cap \left\{\mathcal {N} _ {j} \cup \{j \} \right\}\right) \neq \emptyset , \tag {12.18}
$$

i.e., $G _ { i j } = 1$ if and only if there is a unit $k \in \{ 1 , \ldots , n \}$ whose treatment can affect both $Y _ { i }$ and $Y _ { j }$ under (12.17).

Under Bernoulli randomization (12.3) and the network restriction (12.17), one can immediately verify that whenever $G _ { i j } = 0$ ,

$$
H _ {i} (\mathbf {W}) \perp H _ {j} (\mathbf {W}) \quad \text {a n d s o} \quad Y _ {i} \perp_ {W} Y _ {j}, \tag {12.19}
$$

where the latter statement means that $Y _ { i }$ is independent of $Y _ { j }$ under randomness from the treatment assignment (and either conditionally on potential outcomes or treating potential outcomes as fixed).

Given these ingredients, we are now ready to generalize the results from Section 12.1 to settings with interference, and provide both an exact expression for the variance of $\hat { \tau } _ { I P W } ( h , h ^ { \prime } )$ and a conservative but estimable bound for it. Here, we start by writing down our variance estimator; our target variances will then be readily expressible in terms of moments of the variance estimator.

For any $ { \boldsymbol { h } } _ { \mathbf { \lambda } } \in  { \mathcal { H } }$ , define inverse-propensity weights as $\begin{array} { r l } { \Gamma _ { i } ( h ) } & { { } = } \end{array}$ $1 \left( \left\{ H _ { i } ( \mathbf { W } ) = h \right\} \right) / e _ { i } ( h )$ , and let $\mathbf { { \cal { F } } } ( h ) \in \mathbb { R } ^ { n }$ be the vector of these weights for all units. Given this notation and our exposure mapping,

$$
\hat {\tau} _ {I P W} (h, h ^ {\prime}) = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\Gamma_ {i} \left(h ^ {\prime}\right) Y _ {i} \left(h ^ {\prime}\right) - \Gamma_ {i} (h) Y _ {i} (h)\right), \tag {12.20}
$$

where only the weights $\Gamma _ { i }$ are taken to be random. This formulation, as well as the network independence property of the $\Gamma _ { i }$ established in (12.19), then suggests estimating the variance of the IPW estimator via the following a ‚Äúheteroskedasticity and autocorrelation consistent‚Äù (HAC) construction:71

$$
\hat {\sigma} ^ {2} (h, h ^ {\prime}) = \frac {1}{n} (\boldsymbol {\Gamma} (h ^ {\prime}) \odot \mathbf {Y} - \boldsymbol {\Gamma} (h) \odot \mathbf {Y}) ^ {\top} G (\boldsymbol {\Gamma} (h ^ {\prime}) \odot \mathbf {Y} - \boldsymbol {\Gamma} (h) \odot \mathbf {Y}), \tag {12.21}
$$

where $\odot$ denotes elementwise product.72 The following result connects the expectation of this variance estimator under random treatment assignment to the actual randomization-variance of $\hat { \tau } _ { I P W } ( h , h ^ { \prime } )$ .

Theorem 12.4. Under the setting of Theorem 12.1, suppose furthermore that (12.17) holds and that we consider a pair of exposure h, $h ^ { \prime } \in \mathcal { H }$ with $e _ { i } ( h )$ , $e _ { i } ( h ^ { \prime } ) > 0$ for all $i = 1$ , . . . , $n$ . Write $\sigma ^ { 2 } ( h , h ^ { \prime } ) : = \mathbb { E } _ { W } \left[ \hat { \sigma } ^ { 2 } ( h , h ^ { \prime } ) \right]$ for the variance estimate given in (12.21), and $\bar { \sigma } ^ { 2 } ( h , h ^ { \prime } ) : = n \mathrm { V a r } _ { W } [ \hat { \tau } _ { I P W } ( h , h ^ { \prime } ) ]$ for the scaled randomization variance of the $I P W$ estimator. Then,

$$
\sigma^ {2} (h, h ^ {\prime}) - \bar {\sigma} ^ {2} (h, h ^ {\prime}) = n ^ {- 1} \left(\mathbf {Y} \left(h ^ {\prime}\right) - \mathbf {Y} (h)\right) ^ {\top} G \left(\mathbf {Y} \left(h ^ {\prime}\right) - \mathbf {Y} (h)\right). \tag {12.22}
$$

Proof. Thanks to Theorem 12.1 and (12.1), we have

$$
\begin{array}{l} \bar {\sigma} ^ {2} (h, h ^ {\prime}) := n \operatorname {V a r} _ {W} \left[ \hat {\tau} _ {I P W} (h, h ^ {\prime}) \right] = n \mathbb {E} _ {W} \left[ \left(\hat {\tau} _ {I P W} (h, h ^ {\prime}) - \bar {\tau} (h, h ^ {\prime})\right) ^ {2} \right] \\ = n \mathbb {E} _ {W} \left[ \left(\left(\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\Gamma_ {i} \left(h ^ {\prime}\right) - \Gamma_ {i} (h)\right) Y _ {i} - \frac {1}{n} \sum_ {i = 1} ^ {n} \left(Y _ {i} \left(h ^ {\prime}\right) - Y _ {i} (h)\right)\right) ^ {2} \right] \right. \\ = n \mathbb {E} _ {W} \left[ \left(\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\Gamma_ {i} \left(h ^ {\prime}\right) - 1\right) Y _ {i} \left(h ^ {\prime}\right) - \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\Gamma_ {i} (h) - 1\right) Y _ {i} (h)\right) ^ {2} \right]. \\ \end{array}
$$

We can simplify this expression in terms of the exposure-covariance matrices

$$
U _ {i j} (h, h ^ {\prime}) = \mathbb {E} \left[ \left(\Gamma_ {i} (h) - 1\right) \left(\Gamma_ {j} \left(h ^ {\prime}\right) - 1\right) \right] = \mathbb {E} \left[ \Gamma_ {i} (h) \Gamma_ {j} \left(h ^ {\prime}\right) \right] - 1
$$

and $U ( h ) = U ( h , h )$ , etc., resulting in

$$
\begin{array}{l} \bar {\sigma} ^ {2} (h, h ^ {\prime}) = n ^ {- 1} \mathbf {Y} (h) ^ {\top} U (h) \mathbf {Y} (h) + n ^ {- 1} \mathbf {Y} (h ^ {\prime}) ^ {\top} U (h ^ {\prime}) \mathbf {Y} (h ^ {\prime}) \\ - 2 n ^ {- 1} \mathbf {Y} (h) ^ {\top} U (h, h ^ {\prime}) \mathbf {Y} (h ^ {\prime}). \\ \end{array}
$$

We next turn to studying the expectation of the proposed variance estimate $\hat { \sigma } ^ { 2 } ( h , h ^ { \prime } )$ . A direct calculation shows that

$$
\begin{array}{l} \sigma^ {2} (h, h ^ {\prime}) := \mathbb {E} _ {W} \left[ \hat {\sigma} ^ {2} (h, h ^ {\prime}) \right] = n ^ {- 1} \mathbf {Y} (h) ^ {\top} \mathbb {E} \left[ \boldsymbol {\Gamma} (h) ^ {\top} G \boldsymbol {\Gamma} (h) \right] \mathbf {Y} (h) \\ + n ^ {- 1} \mathbf {Y} (h ^ {\prime}) ^ {\top} \mathbb {E} \left[ \boldsymbol {\Gamma} (h ^ {\prime}) ^ {\top} G \boldsymbol {\Gamma} (h ^ {\prime}) \right] \mathbf {Y} (h ^ {\prime}) - 2 n ^ {- 1} \mathbf {Y} (h) ^ {\top} \mathbb {E} \left[ \boldsymbol {\Gamma} (h) ^ {\top} G \boldsymbol {\Gamma} (h ^ {\prime}) \right] \mathbf {Y} (h ^ {\prime}). \\ \end{array}
$$

Furthermore, we see from (12.19) that

$$
U _ {i j} (h) = U _ {i j} (h ^ {\prime}) = U _ {i j} (h, h ^ {\prime}) = 0 \quad \mathrm {w h e n e v e r} \quad G _ {i j} = 0,
$$

and so we can re-express $\sigma ^ { 2 } ( h , h ^ { \prime } )$ in terms the exposure-covariance matrices used above as follows.

$$
\begin{array}{l} \sigma^ {2} (h, h ^ {\prime}) = n ^ {- 1} \mathbf {Y} (h) ^ {\top} (U (h) + G) \mathbf {Y} (h) + n ^ {- 1} \mathbf {Y} (h ^ {\prime}) ^ {\top} (U (h ^ {\prime}) + G) \mathbf {Y} (h ^ {\prime}) \\ - 2 n ^ {- 1} \mathbf {Y} (h) ^ {\top} (U (h, h ^ {\prime}) + G) \mathbf {Y} (h ^ {\prime}). \\ \end{array}
$$

We can now compare our expressions for $\sigma ^ { 2 } ( h , h ^ { \prime } )$ and $\bar { \sigma } ^ { 2 } ( h , h ^ { \prime } )$ ,

$$
\begin{array}{l} \sigma^ {2} (h, h ^ {\prime}) - \bar {\sigma} ^ {2} (h, h ^ {\prime}) = n ^ {- 1} \mathbf {Y} (h) ^ {\top} G \mathbf {Y} (h) + n ^ {- 1} \mathbf {Y} (h ^ {\prime}) ^ {\top} G \mathbf {Y} (h ^ {\prime}) \\ - 2 n ^ {- 1} \mathbf {Y} (h) ^ {\top} G \mathbf {Y} (h ^ {\prime}) \\ = n ^ {- 1} \left(\mathbf {Y} \left(h ^ {\prime}\right) - \mathbf {Y} (h)\right) ^ {\top} G \left(\mathbf {Y} \left(h ^ {\prime}\right) - \mathbf {Y} (h)\right), \\ \end{array}
$$

as claimed.

Following our approach in the SUTVA case, we next consider the selfnormalized estimator,

$$
\hat {\tau} _ {S I P W} (h, h ^ {\prime}) = \frac {\sum_ {i = 1} ^ {n} \Gamma_ {i} (h ^ {\prime}) Y _ {i}}{\sum_ {i = 1} ^ {n} \Gamma_ {i} (h ^ {\prime})} - \frac {\sum_ {i = 1} ^ {n} \Gamma_ {i} (h) Y _ {i}}{\sum_ {i = 1} ^ {n} \Gamma_ {i} (h)}, \tag {12.23}
$$

and seek to establish a central limit theorem for it. As before, we work under a sequence of randomized trials with growing sample size $n$ , and write

$$
\bar {\mu} _ {n} (h) = \frac {1}{n} \sum_ {i = 1} ^ {n} Y _ {i} (h), \quad \bar {\tau} _ {n} (h, h ^ {\prime}) = \bar {\mu} _ {n} (h ^ {\prime}) - \bar {\mu} _ {n} (h). \tag {12.24}
$$

We will also use a modified variance estimator that accounts for selfnormalization:

$$
\hat {\mu} _ {n} (h) = \frac {1}{n} \sum_ {i = 1} ^ {n} \Gamma_ {i} (h) Y _ {i}, \tag {12.25}
$$

$$
\hat {\sigma} _ {n} ^ {2} (h, h ^ {\prime}) = \left(\boldsymbol {\Gamma} (h ^ {\prime}) \odot \left(\mathbf {Y} - \hat {\mu} _ {n} (h ^ {\prime})\right) - \boldsymbol {\Gamma} (h) \odot \left(\mathbf {Y} - \hat {\mu} _ {n} (h)\right)\right) ^ {\top} G _ {n}
$$

$$
\left(\boldsymbol {\Gamma} \left(h ^ {\prime}\right) \odot \left(\mathbf {Y} - \hat {\mu} _ {n} \left(h ^ {\prime}\right)\right) - \boldsymbol {\Gamma} (h) \odot \left(\mathbf {Y} - \hat {\mu} _ {n} (h)\right)\right),
$$

where ${ \bf Y } - \hat { \mu } _ { n } ( h )$ subtracts the scalar $\hat { \mu } _ { n } ( h )$ from all entries of $\mathbf { Y }$ .

Theorem 12.5. Suppose we have a sequence of randomized trials with growing sample size n that all satisfy the conditions of Theorem 12.4. Write $\deg ( G _ { n } )$ for the maximal degree of the randomization dependency graph in the $n$ -th problem, and assume that $\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty } n ^ { - 1 / 4 } \deg ( G _ { n } ) = 0 } \end{array}$ . Suppose furthermore that there are constants $0 < \eta$ , $M$ , $s _ { 0 } ^ { 2 } < \infty$ such that $e _ { i } ( h )$ , $e _ { i } ( h ^ { \prime } ) \geq \eta$ and $| Y _ { i } ( h ) |$ , $\left| Y _ { i } ( h ^ { \prime } ) \right| \leq$ M for all units throughout the sequence of problems, and that, using notation from (12.22), we have $\bar { \sigma } _ { n } ^ { 2 } ( h , h ^ { \prime } ) \geq s _ { 0 } ^ { 2 }$ for all $n$ . Then,

$$
\sqrt {n} \left(\frac {\hat {\tau} _ {S I P W} (h , h ^ {\prime}) - \bar {\tau} _ {n} (h , h ^ {\prime})}{\bar {\sigma} _ {n} (h , h ^ {\prime})}\right) \Rightarrow \mathcal {N} (0, 1)
$$

$$
\bar {\sigma} _ {n} ^ {2} (h, h ^ {\prime}) = \sigma_ {n} ^ {2} (h, h ^ {\prime}) - (\mathbf {Y} (h ^ {\prime}) - \mathbf {Y} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})) ^ {\top} G _ {n} \tag {12.26}
$$

$$
\left(\mathbf {Y} \left(h ^ {\prime}\right) - \mathbf {Y} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})\right),
$$

where $\sigma _ { n } ^ { 2 } ( h , h ^ { \prime } )$ denotes the randomization-expectation of an oracle version of $\hat { \sigma } _ { n } ^ { 2 } ( h , h ^ { \prime } )$ from (12.25) with $\hat { \mu } _ { n } ( h )$ replaced with $\bar { \mu } _ { n } ( h )$ , etc. Suppose furthermore that, for all large enough $n$ ,

$$
\left(\mathbf {Y} (h ^ {\prime}) - \mathbf {Y} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})\right) ^ {\top} G _ {n} \left(\mathbf {Y} (h ^ {\prime}) - \mathbf {Y} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})\right) \geq 0. \tag {12.27}
$$

Then our variance estimator is asymptotically conservative, lim $\mathrm { s u p } _ { n \to \infty } \bar { \sigma } _ { n } / \hat { \sigma } _ { n } \le _ { p } 1$ , and, for any $0 < \alpha < 1$ ,

$$
\begin{array}{l} \lim  _ {n \rightarrow \infty} \sup  _ {\infty} \mathbb {P} \left[ | \hat {\tau} _ {S I P W} (h, h ^ {\prime}) - \bar {\tau} _ {n} (h, h ^ {\prime}) | \right. \tag {12.28} \\ \leq \hat {\sigma} _ {n} (h, h ^ {\prime}) / \sqrt {n} \Phi^ {- 1} (1 - \alpha / 2) \Big ] \geq 1 - \alpha , \\ \end{array}
$$

i.e., the usual normal confidence intervals are valid.

Proof. We again start by noting that, thanks to self-normalization and our assumed exposure mapping,

$$
\hat {\tau} _ {S I P W} (h, h ^ {\prime}) = \bar {\tau} _ {n} (h, h ^ {\prime}) + \Delta (h ^ {\prime}) \Big / \frac {1}{n} \sum_ {i = 1} ^ {n} \Gamma_ {i} (h ^ {\prime}) - \Delta (h) \Big / \frac {1}{n} \sum_ {i = 1} ^ {n} \Gamma_ {i} (h)
$$

$$
\Delta (h) = \frac {1}{n} \sum_ {i = 1} ^ {n} \Gamma_ {i} (h) \left(Y _ {i} - \bar {\mu} _ {n} (h)\right).
$$

Theorems 12.1 and 12.4 immediately imply that, for all $n$ ,

$$
\mathbb {E} _ {W} \left[ \Delta (h ^ {\prime}) - \Delta (h) \right] = 0, \quad \mathrm {V a r} _ {W} \left[ \Delta (h ^ {\prime}) - \Delta (h) \right] = \frac {\bar {\sigma} _ {n} ^ {2} (h , h ^ {\prime})}{n}.
$$

Furthermore, Baldi and Rinott [1989, Corollary 2] provide a normal approximation result of network-correlated random variables that, in our setting implies, that

$$
\sup _ {z \in \mathbb {R}} \left| \mathbb {P} \left[ \frac {\sqrt {n} (\Delta (h ^ {\prime}) - \Delta (h))}{\bar {\sigma} _ {n} (h , h ^ {\prime})} \leq z \right] - \Phi (z) \right| \leq 3 2 (1 + \sqrt {6}) \left(\frac {4 M}{\eta s _ {0}}\right) ^ {\frac {3}{2}} \frac {\deg (G _ {n})}{n ^ {1 / 4}}.
$$

Our assumption on the degree of $G _ { n }$ makes the right-hand side go to zero, and thus

$$
\frac {\sqrt {n} (\Delta (h ^ {\prime}) - \Delta (h))}{\bar {\sigma} _ {n} (h , h ^ {\prime})} \Rightarrow \mathcal {N} (0, 1).
$$

The remainder of the proof follows the blueprint of Theorem 12.3 and so is omitted; in particular, we note that our overlap assumption immediately implies that $\begin{array} { r } { \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \Gamma _ { i } ( h ) \to _ { p } 1 } \end{array}$ . ‚ñ°

The final delicate issue to consider, however, is that‚Äîunlike under SUTVA‚Äîthe right-hand side expression in (12.26) is no longer guaranteed to be non-negative, and so we had to impose (12.27) as an additional condition to justify confidence intervals. This term will of course be non-negative if $G$ is

positive semi-definite. This is the case in some important settings, e.g., if $G$ is block-diagonal. However, $G$ is in general not positive semi-definite even in simple network models, e.g., the following $3 \times 3$ matrix has eigenvectors $v$ ,

$$
G = \left( \begin{array}{l l l} 1 & 1 & 0 \\ 1 & 1 & 1 \\ 0 & 1 & 1 \end{array} \right), \quad v _ {1} = \left( \begin{array}{l} 1 \\ \sqrt {2} \\ 1 \end{array} \right), v _ {2} = \left( \begin{array}{l} 1 \\ 0 \\ - 1 \end{array} \right), v _ {3} = \left( \begin{array}{l} - 1 \\ \sqrt {2} \\ - 1 \end{array} \right),
$$

with corresponding eigenvalues $\lambda _ { 1 } = 1 + { \sqrt { 2 } }$ , $\lambda _ { 2 } = 1$ and $\lambda _ { 3 } = 1 - \sqrt { 2 } < 0$

Thus, if one wants to argue that ${ \hat { \sigma } } ^ { 2 }$ is conservative for $\bar { \sigma } ^ { 2 }$ , one needs to make further assumptions. A first hope, of course, is that we might be working in an application where $G$ is positive semi-definite, in which case there is nothing further that needs to be done. It turns out, however, that even if $G$ is not positive semi-definite, we can still verify that if the individual exposure effects are ‚Äúuncorrelated‚Äù for non-neighboring units in the following sense,

$$
\mathbb {E} \left[ \left(Y _ {i} \left(h ^ {\prime}\right) - Y _ {i} (h) - \bar {\tau} \left(h, h ^ {\prime}\right)\right) \left(Y _ {j} \left(h ^ {\prime}\right) - Y _ {j} (h) - \bar {\tau} \left(h, h ^ {\prime}\right)\right) \mid G _ {i j} = 0 \right] = 0, \tag {12.29}
$$

then $\hat { \sigma } ^ { 2 }$ is conservative on average, with expectation taken over randomness in the potential outcomes.

Lemma 12.6. Under the conditions of Theorem 12.5, if either $G$ is positive semi-definite or if (12.29) holds, then our HAC variance estimator is conservative in the sense that (12.27) holds in expectation.

Proof. The first case is immediate. In the second case,

$$
\begin{array}{l} \mathbb {E} \left[ (\mathbf {Y} (h ^ {\prime}) - \mathbf {Y} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})) ^ {\top} G (\mathbf {Y} (h ^ {\prime}) - \mathbf {Y} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})) \right] \\ = \mathbb {E} \left[ \sum_ {i, j = 1} ^ {n} G _ {i j} \mathbb {E} \left[ \left(Y _ {i} \left(h ^ {\prime}\right) - Y _ {i} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})\right) \left(Y _ {j} \left(h ^ {\prime}\right) - Y _ {j} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})\right) \mid G _ {i j} \right] \right] \\ = \mathbb {E} \left[ \sum_ {i, j = 1} ^ {n} \mathbb {E} \left[ \left(Y _ {i} \left(h ^ {\prime}\right) - Y _ {i} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})\right) \left(Y _ {j} \left(h ^ {\prime}\right) - Y _ {j} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})\right) \mid G _ {i j} \right] \right] \\ = \mathbb {E} \left[ \left(\sum_ {i = 1} ^ {n} \left(Y _ {i} (h ^ {\prime}) - Y _ {i} (h) - \bar {\tau} _ {n} (h, h ^ {\prime})\right)\right) ^ {2} \right] \geq 0. \\ \end{array}
$$

where the second equality follows from the fact that $G _ { i j } \in \{ 0 , 1 \}$ and that, by (12.29), all terms of the form $\mathbb { E } \left[ \cdot \cdot \cdot \mid G _ { i j } = 0 \right]$ are equal to 0 and so do not affect the sum. ‚ñ°

Remark 12.1. When $G$ has block structure, the variance estimator (12.25) is conservative and equivalent to usual cluster-robust inference variance estimator that is typically motivated using IID sampling assumptions (i.e., that clusters are sampled IID); see also Abadie et al. [2023]. Thus, we have recovered a conservativeness phenomenon analogous to the one derived by Neyman [1923] under SUTVA: Standard variance estimators motivated by IID sampling (here, of clusters) is conservative for the finite-population variance that arises from treatment randomization alone in the setting where potential outcomes are considered as deterministic.

Remark 12.2. The overlap assumption $e _ { i } ( h ) \geq \eta$ used in Theorem 12.5 essentially requires ${ \mathcal { N } } _ { i }$ to be finite, even as the network grows (i.e., each unit is only influenced by the treatment given to a finite number of other units). However, even in this setting, the degree of $G$ can grow large: This can happen if there are some nodes that are very ‚Äúpopular‚Äù, in the sense that they influence many other nodes (i.e., they belong to ${ \mathcal { N } } _ { j }$ for many other units $j$ ). In this context, our assumption on $\deg ( G _ { n } )$ is essentially an upper bound on the strength of outward influence: We do not allow there to be a node whose treatment affects outcomes for more than $n ^ { 1 / 4 }$ other units.

# Bibliographic notes

The finite-population model used in this chapter‚Äîas well as the approach to inference via conservative, identifiable variance bounds‚Äîgoes back to Neyman [1923]. Here, we studied finite-population inference under Bernoulli trials; results under a number of different experimental designs is given in Li and Ding [2017]. We note that the variance bound used in Theorem 12.2 is not the only available bound; see Aronow, Green, and Lee [2014] for alternate proposals. Furthermore, the finite-population approach discussed here can also be extended to much more complex randomization designs, e.g., rerandomization as in Morgan and Rubin [2012].

Our approach to defining causal effects in terms of average outcomes under different exposure types builds on Aronow and Samii [2017]. Aronow and Samii [2017] also provided bounds on the variance of treatment effect estimators under the Neyman model; the bound we use in Theorem 12.4 is due to Leung [2022]. Building on this line of work, S¬®avje [2024] discusses interpretation of exposure-averaging estimands when the exposure mapping may be misspecified, while Leung [2022] provides inference results under an approximate network interference model, where interference effects decay (but do not vanish) as units get farther from each other in a network. Viviano [2024] consid-

ers policy learning with interference under an exposure mapping assumption. Ogburn et al. [2024] consider inference from observational data under network interference. Harshaw, S¬®avje, and Wang [2022] propose an algorithmic framework for producing IPW-like estimators for a number of causal target under wide variety models for interference.

Finally, we also note that there exist alternative ways of defining causal effects under interference that do not rely on well-specified exposure mappings. One such approach involves defining average direct and indirect effects of a treatment, which effectively measure how a unit getting treated affects the unit itself or others, while marginalizing over the treatment received by others [Halloran and Struchiner, 1995, Hu, Li, and Wager, 2022b, S¬®avje, Aronow, and Hudgens, 2021]

$$
\tau_ {A D E} = \frac {1}{n} \sum_ {\substack {i = 1 \\ n}} ^ {n} \mathbb {E} _ {W} \left[ Y _ {i} \left(w _ {i} = 1, W _ {- i}\right) - Y _ {i} \left(w _ {i} = 1, W _ {- i}\right) \right], \tag{12.30}
$$

$$
\tau_ {A I E} = \frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {j \neq i} \mathbb {E} _ {W} \left[ Y _ {j} \left(w _ {i} = 1, W _ {- i}\right) - Y _ {j} \left(w _ {i} = 1, W _ {- i}\right) \right],
$$

where $Y _ { j } \left( w _ { i } = 1 , W _ { - i } \right)$ denotes the outcome we observe for the $j$ -th unit by setting the $i$ -th treatment to 1 but letting others be as they are under the randomization distribution. Hu, Li, and Wager [2022b] interpret these estimands in the context of a number of models for interference, and connect them to notions of total treatment effects. S¬®avje, Aronow, and Hudgens [2021] provide bounds for the average direct effect under a generic interference model, while Li and Wager [2022] give exact large-sample asymptotics for the average direct and indirect effects under a random graph generative model. Munro, Kuang, and Wager [2025] consider large-sample behavior of the average direct and indirect effects in a model where interference arises via equilibrium effects where in a marketplace where prices align supply and demand; they also propose CATElike measures for treatment heterogeneity that can be used for spillover-aware targeting.

# Chapter 13 Event-Study Designs

All examples considered in this book so far involve settings where we observe a unit, they receive some treatment exposure (or not), and then reveal an outcome. In applications, however, it is common to follow units over time and to obtain multiple measurements from each unit. For example, when studying the effect of a tax policy, we will often be able to follow a country over time‚Äî and under different tax policies. Or, in medicine, we often follow a patient over time as they go through a potentially complex treatment regimen.

This chapter‚Äîas well as the following two‚Äîwill introduce methods for causal inference in settings where units are followed over time. Data collected in such settings is often referred to as panel data or longitudinal data. Incorporating full treatment dynamics‚Äîwhere treatment can toggle on and off, and we need to reason about both long- and short-term effects of actions‚Äîwill be deferred to subsequent chapters. Here, instead, we will focus on the simpler case of event studies where all units start in the control condition and then, if they ever start treatment, they never stop. Our focus on event studies will enable a gradual ramp-up in the technical tools required to work with panel data, and allow us to introduce some widely used econometric methods.

Example 17. In 1990, all but one of 477 municipalities in Argentina had water services that were either public or owned by non-profit cooperatives. By the end of the decade, 137 of these municipalities privatized their water systems, and transferred ownership to private for-profit entities. Galiani, Gertler, and Schargrodsky [2005] use this panel dataset‚Äîand exploit the fact that some municipalities are observed in the transition from public to private ownership‚Äî to study potential community health effects from privatizing water resources.

Suppose we observe a panel of $i = 1$ , . . . , $n$ units across $t = 1$ , . . . , $T$ time periods. In each $( i , t )$ pair the is in treatment condition $W _ { i t } \in \{ 0 , 1 \}$ and we observe an outcome $Y _ { i t } \in \mathbb { R }$ . Our event-study assumption requires that treatment can only ever switch off-to-on, i.e., that $W _ { i t } \le W _ { i t ^ { \prime } }$ for all $t \leq t ^ { \prime }$ .

There are two treatment patterns that fall under the event-study umbrella.

Definition 13.1. In the block-adoption design, there is a shared event time $1 \leq H < T$ such that each unit either starts treatment right after $H$ or never does. Each unit as an adoption indicator $D _ { i } ~ \in ~ \{ 0 , 1 \}$ such that $W _ { i t } = D _ { i } 1 \left( \{ t > H \} \right)$ .

Definition 13.2. In the staggered-adoption design, each unit either has its own event time $1 \leq H _ { i } \leq T - 1$ after which it starts treatment, or it never starts treatment in which case we write $H _ { i } = \infty$ . We then have $W _ { i t } = 1 ( \{ t > H _ { i } \} )$ .

As usual, we will define our causal estimands in terms of potential outcomes. As discussed in Chapter 11, defining potential outcomes for general causal inferefence problems requires considering the different possible treatment exposures a unit may face. Without any restrictions, a unit who receives a binary intervention in each of $T$ time periods could experience $2 ^ { T }$ different treatment trajectories, and one would then need to either define $2 ^ { T }$ potential outcomes for each unit or define an exposure mapping for dimensionality restriction. In event study designs, however, the off-to-on restriction on treatment assignment restricts the number of possible treatment trajectories and simplifies the definition of potential outcomes.

In the block-adoption design, a unit‚Äôs treatment trajectory is fully defined by its adoption indicator, and so we can write potential outcomes

$$
Y _ {i t} (d) \text {f o r} d = 0, 1, \tag {13.1}
$$

with a SUTVA assumption that $Y _ { i t } = Y _ { i t } ( D _ { i } )$ . In the staggered-adoption design there‚Äôs a little more flexibility as there are now $T$ possible treatment-start times; natural potential outcomes are then

$$
Y _ {i t} (h) \text {f o r} h = 1, 2, \dots , T - 1, \infty , \tag {13.2}
$$

with a SUTVA assumption $Y _ { i t } = Y _ { i t } ( H _ { i } )$ . Throughout, we will assume temporal consistency of actions, i.e., that future actions cannot affect past outcomes.

Assumption 13.1. We assume that potential outcomes do not anticipate treatment. Specifically, in the block-adoption design case, we assume that

$$
Y _ {i t} (0) = Y _ {i t} (1) \text {f o r} t = 1, \dots , H, \tag {13.3}
$$

while in the staggered-adoption design case, we assume that

$$
Y _ {i t} (h) = Y _ {i t} \left(h ^ {\prime}\right) \text {f o r} t = 1, \dots , \min  \left\{h, h ^ {\prime} \right\}. \tag {13.4}
$$

Assuming temporal consistency may seem innocuous when presented abstractly, but this is in fact an assumption that may easily fail to hold in some applications. For example, if we want to study the effect of a policy started by a country $i$ at time $H _ { i }$ , but some people were able to anticipate this policy change and adapt their behavior in advance of it, then this non-anticipation assumption would not hold. The non-anticipation assumption should thus be carefully assessed before using any of the methods presented in this chapter.

# 13.1 Difference in differences

Under the block-adoption design, one natural estimand to target is the sample average treatment effect after the event time $H$ ,

$$
\bar {\tau} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {1}{T - H} \sum_ {t = H + 1} ^ {T} Y _ {i t} (1) - Y _ {i t} (0)\right). \tag {13.5}
$$

How should we go about estimating this quantity? A first natural estimator to try is the simple difference-in-means comparison in the post-event periods,

$$
\hat {\tau} _ {D M} = \frac {\sum_ {\{i : D _ {i} = 1 \}} \sum_ {t = H + 1} ^ {T} Y _ {i t}}{| \{i : D _ {i} = 1 \} | (T - H)} - \frac {\sum_ {\{i : D _ {i} = 0 \}} \sum_ {t = H + 1} ^ {T} Y _ {i t}}{| \{i : D _ {i} = 0 \} | (T - H)}. \tag {13.6}
$$

This estimator, however, may seem wasteful in that it completely ignores available data from the pre-event periods. One popular way to leverage pre-event data available in a panel is using the difference-in-differences (DID) estimator:

$$
\begin{array}{l} \hat {\tau} _ {D I D} = \frac {1}{| \{i : D _ {i} = 1 \} |} \sum_ {\{i: D _ {i} = 1 \}} \left(\frac {1}{T - H} \sum_ {t = H + 1} ^ {T} Y _ {i t} - \frac {1}{H} \sum_ {t = 1} ^ {H} Y _ {i t}\right) \\ - \frac {1}{\left| \left\{i : D _ {i} = 0 \right\} \right|} \sum_ {\left\{i: D _ {i} = 0 \right\}} \left(\frac {1}{T - H} \sum_ {t = H + 1} ^ {T} Y _ {i t} - \frac {1}{H} \sum_ {t = 1} ^ {H} Y _ {i t}\right). \tag {13.7} \\ \end{array}
$$

In words, the DID estimator first uses pre-event data to construct a baseline outcome that is subtracted from post-event outcomes, and then compares these post-minus-pre differences across adopters and non-adopters.

As a first sanity check, both the simple difference and DID estimators can immediately be verified to be unbiased when adoption is randomized. Given random treatment, the DID estimator requires stronger assumptions for unbiasedness as it also relies on non-anticipation; however, when non-anticipation holds, DID generally has lower variance than simple differencing.

Proposition 13.1. If adoption is randomized, then $\mathbb { E } _ { W } \left[ \hat { \tau } _ { D M } \right] = \bar { \tau }$ , where EW [¬∑] averages over randomness in $W$ while holding potential outcomes fixed. Furthermore, if Assumption 13.1 holds, then $\mathbb { E } _ { W } \left[ \hat { \tau } _ { D I D } \right] = \bar { \tau }$ .

Proof. The first statement follows immediately from Theorem 1.1. The second statement follows by noting that, under Assumption 13.1, incorporating the pre-event data into the estimator has a mean-zero effect under randomized adoption. ‚ñ°

In many practical event study applications, however, treatment cannot credibly be taken to be randomized. Consider, for example, a setting where our units correspond to the $n = 5 0$ states in the United States. Some states choose to adopt a policy (e.g., to accept Federal subsidies to expand Medicaid coverage) while others don‚Äôt. We would like to use difference in differences, but treatment here is clearly not randomized, and in fact the sampling assumptions used to define the ATT in (13.5) don‚Äôt really make sense either‚Äîand so Proposition 13.1 does not apply.

Thankfully, it turns out that the difference-in-differences estimator has a double-robustness-type property whereby it can also be justified via a functional form assumption, namely parallel trends. The parallel trends assumption, made formal in Assumption 13.2, states that all non-adopter potential outcomes must evolve in parallel (but may start at different levels). When parallel trends holds, DID can be verified to be on average unbiased for the following sample-average treatment effect on the treated (SATT),

$$
\bar {\tau} _ {A T T} = \frac {\sum_ {\{i : D _ {i} = 1 \}} \sum_ {t = H + 1} ^ {T} \left(Y _ {i t} (1) - Y _ {i t} (0)\right)}{| \{i : D _ {i} = 1 \} | (T - H)}, \tag {13.8}
$$

without requiring any reference to population sampling assumptions.

Assumption 13.2. There exist $\beta _ { 2 }$ , . . . , $\beta _ { T } \in \mathbb { R }$ such that, for all units $i =$ 1, . . . , $n$ , never-treated potential outcomes satisfy

$$
\mathbb {E} \left[ Y _ {i t} (0 / \infty) - Y _ {i 1} (0 / \infty) \right] = \beta_ {t}, \quad t = 2, \dots , T. \tag {13.9}
$$

Recall that we write never-treated potential outcomes as $Y _ { i t } ( 0 )$ under block adoption and $Y _ { i t } ( \infty )$ under staggered adoption.

Theorem 13.2. In the block-adoption design suppose that some‚Äîbut not all‚Äî units are exposed to treatment (i.e., have $D _ { i } = 1$ ). Then, under Assumptions 13.1 and 13.2, $\mathbb { E } \left[ \hat { \tau } _ { D I D } - \bar { \tau } _ { A T T } \right] = 0$ .

Proof. A comparison of (13.7) and (13.8) reveals that, under Assumption 13.1,

$$
\begin{array}{l} \hat {\tau} _ {D I D} - \bar {\tau} _ {A T T} = \frac {1}{| \{i : D _ {i} = 1 \} |} \sum_ {\{i: D _ {i} = 1 \}} \left(\frac {1}{T - H} \sum_ {t = H + 1} ^ {T} Y _ {i t} (0) - \frac {1}{H} \sum_ {t = 1} ^ {H} Y _ {i t} (0)\right) \\ - \frac {1}{| \{i : D _ {i} = 0 \} |} \sum_ {\{i: D _ {i} = 0 \}} \left(\frac {1}{T - H} \sum_ {t = H + 1} ^ {T} Y _ {i t} (0) - \frac {1}{H} \sum_ {t = 1} ^ {H} Y _ {i t} (0)\right). \\ \end{array}
$$

Furthermore, under Assumption 13.2,

$$
\mathbb {E} \left[ \frac {1}{T - H} \sum_ {t = H + 1} ^ {T} Y _ {i t} (0) - \frac {1}{H} \sum_ {t = 1} ^ {H} Y _ {i t} (0) \right] = \frac {1}{T - H} \sum_ {t = H + 1} ^ {T} \beta_ {t} - \frac {1}{H} \sum_ {t = 2} ^ {H} \beta_ {t}
$$

is the same for each $i = 1$ , . . . , $n$ . The contributions of the $\beta _ { t }$ then cancel out perfectly. ‚ñ°

The parallel trend assumption is a fairly strong functional form assumption, and so guarantees obtained under this assumption are not generally comparable to guarantees for causal inference available in randomized controlled trials. They are, however, still valuable in practice, and DID type analyses have been hugely influential in applied work. For example, in one early and influential study of the empirical effects of raising the minimum wage on employment, Card and Krueger [1994] conducted a DID study comparing employment outcomes across time in New Jersey, which raised its minimum wage during the study period, to those in Pennsylvania, where the minimum wage remained fixed. This study identified treatment effects by assuming parallel trends‚Äî and still to date much of the empirical literature on minimum wage effects is justified by various parallel-trends-type assumptions.

Staggered adoption The above results were established under block adoption; in practice, however, treatment adoption follows the more general staggered-adoption design. For example, in the setting of Example 17, municipalities actually privatized water systems at different times throughout the 1990s: The privatization rate was essentially 0% in 1990, 10% in 1995, and almost 30% by 1999.

The basic DID formula (13.7) is no longer applicable under staggered adoption. However, the parallel trends assumption (Assumption 13.2) used to justify it is still a natural assumption to make; and furthermore the SATT from (13.8) generalizes to

$$
\bar {\tau} _ {A T T} = \sum_ {\{i: H _ {i} \neq \infty \}} \sum_ {t = H _ {i} + 1} ^ {T} \left(Y _ {i t} (H _ {i}) - Y _ {i t} (\infty)\right) / \sum_ {\{i: H _ {i} \neq \infty \}} (T - H _ {i}), \tag {13.10}
$$

which measures the average difference between realized potential outcomes and never-treated potential outcomes for $( i , t )$ affected by treatment. It is then natural to ask: How can we estimate $\tau _ { A T T }$ under parallel trends in a staggeredadoption design? Before presenting a valid approach, we start by discussing an alluring idea with unintuitive but notable failure modes.

Two-way fixed-effects regression One can readily verify that, under block adoption, the DID estimator $\hat { \tau } _ { D I D }$ from (13.7) is equivalent to the $\hat { \tau }$ coefficient obtained by running a two-way fixed-effects linear regression:

$$
Y _ {i t} \sim \alpha_ {i} + \beta_ {t} + W _ {i t} \tau . \tag {13.11}
$$

This connection is purely algorithmic, and does not rely on well-specification of the linear model associated with (13.11). Mechanistically, we see that the unit fixed effects $\alpha _ { i }$ absorb any additive unit-level baseline effects, and the time fixed effects $\beta _ { t }$ absorb any additive time trends.

Now what‚Äôs interesting is that, while the original DID construction (13.7) does not immediately extend to the staggered adoption setting, the two-way regression (13.11) is something that can immediately be run with under any treatment adoption design. Unfortunately, however, this simple idea does not work under the potential outcome specification considered here. Under staggered adoption, the coefficient $\hat { \tau }$ from the two-way regression is in general not consistent for $\tau _ { A T T }$ ; and, in fact, it‚Äôs possible to construct settings where $Y _ { i t } ( H _ { i } ) > Y _ { i t } ( \infty )$ for all pairs $( i , t )$ with $t > H _ { i }$ (i.e., starting treatment always strictly increases outcomes), and yet the regression coefficient $\hat { \tau }$ from the two-way model converges to a negative limit.

To understand the issue here, it is helpful to return to our discussions from Chapter 8, where we observed that the output of any linear regression estimator can always be written as a weighted average of the outcomes, $\begin{array} { r } { \hat { \tau } = \sum _ { i , t } \gamma _ { i t } Y _ { i t } } \end{array}$ , with the weights $\gamma _ { i t }$ that encode the regression model. The first two panels of Figure 13.1 plot the weights resulting from (13.11) for both a block design (in which case we already have an explicit expression for the weights thanks to (13.7)), and for a staggered adoption design. The seeming paradox from the previous paragraph arises because $\gamma _ { i t }$ can be negative for some treated $( i , t )$ pairs, and thus large positive values of $Y _ { i t } ( H _ { i } ) - Y _ { i t } ( \infty )$ for those $( i , t )$ may push $\hat { \tau }$ to be negative.

Averaged saturated regression There is, however, a simple fix to this issue: Instead of running the simple two-way regression (13.11), one can run fit a saturated two-way model where each (i, t)-cell under treatment gets its

![](images/1d4e9780593681294fa82d762f6f1d418a19c73d850e4145979c3d93bcb525bc.jpg)  
block adoption diff. in diff.

![](images/884a09e961f3b28bb13a3530554de2be4df5c41621d9b028f6e86ff821bf398e.jpg)  
staggered adoption two-way regression

![](images/b77d39092affed117797562f072887bd9d03d331a437d03874985d3d408686a6.jpg)  
staggered adoption saturated regression   
Figure 13.1: Weights implied by the difference in differences estimator under block adoption, the two-way fixed effects regression with a constant treatment effect parameter under staggered adoption, and the two-way fixed effects regression with saturated treatment effect parameter under staggered adoption. We have $n = T = 1 0$ . In the block design example $W _ { i t } = 1 ( \{ i \geq 5 , t \geq 5 \}$ ), whereas in the staggered adoption example $W _ { i t } = 1 ( \{ t \geq 1 3 - i \} )$ . In both cases, 36 out of 100 cells have active treatment.

own $\theta _ { i t }$ coefficient,

$$
Y _ {i t} \sim \alpha_ {i} + \beta_ {t} + W _ {i t} \theta_ {i t}. \tag {13.12}
$$

Then, in a second step, one estimates

$$
\hat {\tau} _ {A S R} = \sum_ {W _ {i t} = 1} \hat {\theta} _ {i t} / | \{W _ {i t} = 1 \} |. \tag {13.13}
$$

The individual $\hat { \theta } _ { i t }$ coefficients in this regression will in general not be consistent; however, their aggregate $\hat { \tau } _ { A S R }$ is able to average out these errors in a way that recovers consistency.73 The following result verifies that the $\hat { \tau } _ { A S R }$ in fact has similar properties under staggered adoption as those established for $\hat { \tau } _ { D I D }$ under block adoption.

Because $\hat { \tau } _ { A S R }$ is a linear combination of regression coefficients, it can also be expressed as a weighted average $\begin{array} { r } { \hat { \tau } _ { A S R } = \sum _ { i , t } \gamma _ { i t } Y _ { i t } } \end{array}$ ; and examining these weights can yield further insights about the behavior of the estimator. As seen in the 3rd panel of Figure 13.1, the weights $\gamma _ { i t }$ show that $\hat { \tau } _ { A S R }$ does in fact average information from throughout the panel in a stable-looking way.

Furthermore, we see that the weights for all treated time periods are equal (and positive).

Theorem 13.3. In the staggered-adoption design, suppose that some‚Äîbut not all‚Äîunits are never treated (i.e., have $H _ { i } = \infty$ ). Then, under Assumptions 13.1 and 13.2, $\mathbb { E } \left[ \hat { \tau } _ { A S R } - \bar { \tau } _ { A T T } \right] = 0$ .

Proof. Write $\begin{array} { r } { \hat { \tau } _ { A S R } = \sum _ { i , t } \gamma _ { i t } Y _ { i t } } \end{array}$ , with the weights $\gamma _ { i t }$ left implicit for now. Our proof hinges on indirectly deriving properties of the weights $\gamma _ { i t }$ by considering the behavior of $\hat { \tau } _ { A S R }$ under the the well-specified linear regression model associated with (13.12) under homoskedastic errors,

$$
Y _ {i t} = \alpha_ {i} + \beta_ {t} + W _ {i t} \theta_ {i t} + \varepsilon_ {i t}, \quad \varepsilon_ {i t} \mid W \sim \mathcal {N} (0, \sigma^ {2}). \tag {13.14}
$$

By the Gauss‚ÄìMarkov theorem, $\hat { \tau } _ { A S R }$ is the minimum-variance unbiased estimator for $\textstyle \theta = \sum _ { i , t } W _ { i t } \theta _ { i t } / \sum _ { i , t } W _ { i t }$ in this model. Now, one can check that any weighted estimator will be unbiased for $\theta$ here if and only if

$$
\sum_ {t = 1} ^ {T} \gamma_ {i t} = 0 \mathrm {f o r a l l} i = 1, \ldots , n \qquad \mathrm {(n o c o n t a m i n a t i o n f r o m} \alpha_ {i} \mathrm {)},
$$

$$
\sum_ {i = 1} ^ {n} \gamma_ {i t} = 0 \text {f o r a l l} t = 1, \dots , T \quad (\text {n o c o n t a m i n a t i o n f r o m} \beta_ {t}), \tag {13.15}
$$

$$
\gamma_ {i t} = 1 / \sum_ {i, t} W _ {i t} \mathrm {w h e n} W _ {i t} = 1 \qquad (\mathrm {c o r r e c t l y c a p t u r e s t h e t a r g e t}),
$$

and so by the Gauss‚ÄìMarkov theorem these equality constraints must be satisfied by the weights underlying $\hat { \tau } _ { A S R }$ (and these equality constraints remain valid whether or not one actually believes data was generated according to (13.14)). The assumption that some but not all units have $H _ { i } = \infty$ is necessary and sufficient for weights with these properties to exist (and thus for $\hat { \tau } _ { A S R }$ to be feasible) under staggered adoption.

We now argue that these constraints imply our desired result under actual assumptions, namely Assumptions 13.1 and 13.2. The fact that $\gamma _ { i t } ~ =$ $1 / \sum _ { i , t } W _ { i t }$ whenever $W _ { i , t } = 1$ implies that, under Assumption 13.1,

$$
\hat {\tau} _ {A S R} - \bar {\tau} _ {A T T} = \sum_ {i = 1} ^ {n} \sum_ {t = 1} ^ {T} \gamma_ {i t} Y _ {i t} (\infty).
$$

Next, because all terms in th $\begin{array} { r } { \sum _ { t = 1 } ^ { T ^ { \prime } } \gamma _ { i t } = 0 } \end{array}$ for all  withou $i$ , we can subtract unit-baseline effects from changing the final result,

$$
\hat {\tau} _ {A S R} - \bar {\tau} _ {A T T} = \sum_ {i = 1} ^ {n} \sum_ {t = 2} ^ {T} \gamma_ {i t} \left(Y _ {i t} (\infty) - Y _ {i 1} (\infty)\right).
$$

Then, by Assumption 13.2, we get that

$$
\mathbb {E} \left[ \hat {\tau} _ {A S R} - \bar {\tau} _ {A T T} \right] = \sum_ {i = 1} ^ {n} \sum_ {t = 2} ^ {T} \gamma_ {i t} \beta_ {t}.
$$

Finally, swapping the order of summation and invoking the fact that $\textstyle \sum _ { i = 1 } ^ { n } \gamma _ { i t } =$ 0 for all $t$ verifies the desired claim. ‚ñ°

Going beyond Theorem 13.3 to also prove consistency requires having the number of units $n$ grow so that the random error term in the proof above, i.e.,

$$
\sum_ {i = 1} ^ {n} \sum_ {t = 2} ^ {T} \gamma_ {i t} \left(Y _ {i t} (\infty) - Y _ {i 0} (\infty) - \beta_ {t}\right) \tag {13.16}
$$

concentrates out; see Borusyak, Jaravel, and Spiess [2024] for details. Finally, for inference‚Äîas with all DID-type methods‚Äîit is recommended to use algorithms that treat all observations from the same unit as dependent, e.g., the unit-clustered jackknife; see Bertrand, Duflo, and Mullainathan [2004] for a discussion and examples.

Remark 13.1. A by-product of the proof of Theorem 13.3 is a finding that, under spherical errors as in (13.14), $\hat { \tau } _ { A S R }$ is the best unbiased linear estimator for $\bar { \tau } _ { A T T }$ . In some applications, however, one might suspect that idiosyncratic errors are correlated across time; and it‚Äôs natural to ask whether we can build alternative estimators of $\tau _ { A T T }$ with lower variance in such settings. The answer to this question is generally yes. For example, if we believed that the nevertreated potential outcomes can be written as $Y _ { i t } ( 0 ) = \alpha _ { i } + \beta _ { t } + \varepsilon _ { i t }$ with $\mathbb { E } \left[ \varepsilon _ { i . } \right] = 0$ and Var $\left[ \varepsilon _ { i . } \right] = \Sigma$ , then the best unbiased linear estimator becomes

$$
\hat {\tau} _ {A T T} ^ {\Sigma} = \sum_ {i t} \gamma_ {i t} Y _ {i t}, \quad \gamma = \operatorname {a r g m i n} _ {\gamma} \left\{\sum_ {i = 1} ^ {n} \gamma_ {i.} ^ {\prime} \Sigma \gamma_ {i.}: \gamma \text {s a t i s f i e s (1 3 . 1 5)} \right\}. \tag {13.17}
$$

Optimality of this estimator follows by again observing that any estimator of the form $\textstyle \sum _ { i t } \gamma _ { i t } Y _ { i t }$ is unbiased for $\tau _ { A T T }$ under an additive two-way model for $Y _ { i t } ( 0 )$ if and only if the constraints (13.15) are satisfied, and the task then becomes to find the variance-minimizing estimator that satisfies these constraints. This estimator can no longer be implemented using standard linear regression software; however, the weights $\gamma _ { i t }$ in (13.17) can directly be derived via quadratic programming. For an in-depth discussion of efficient estimation under parallel trends, see Chen, Sant‚ÄôAnna, and Xie [2025].

# 13.2 Synthetic-control methods

Under the block-adoption setting, difference-in-differences provides a simple estimator of the SATT provided that non-anticipation and parallel trends hold. The parallel trends assumption, however, is a fairly strong function form assumption that can often fail to hold in applications. In this section, we will briefly discuss synthetic-control methods [Abadie, Diamond, and Hainmueller, 2010], which allow extension of difference-in-differences type methods to settings without parallel trends.

One observable implication of the parallel trends assumption paired with Assumption 13.1 is that, until the event time $H$ , both adopter (or exposed) and non-adopter (or control) units should on average evolve in parallel: Subject to a potential offset parameter $\alpha \in \mathbb { R }$ , we should have

$$
\frac {1}{| \{D _ {i} = 0 \} |} \sum_ {\{D _ {i} = 0 \}} Y _ {i t} \approx \alpha + \frac {1}{| \{D _ {i} = 1 \} |} \sum_ {\{D _ {i} = 1 \}} Y _ {i t}, \quad t = 1, \dots , H. \qquad (1 3. 1 8)
$$

Synthetic control methods are focused on settings where we observe that in fact parallel trends do not hold pre-event, yet would still like to proceed with an event-study analysis. Generally, synthetic control methods seek to mitigate bias from failures of parallel trends by carefully reweighting the control units.

Synthetic difference in differences (SDID) [Arkhangelsky et al., 2021] is a synthetic control method that makes connections to DID explicit‚Äîand so this is the variant of synthetic controls we will discuss here. The main idea of SDID is to find non-negative weights $\gamma _ { i }$ with $\textstyle \sum _ { D _ { i } = 0 } \gamma _ { i } = 1$ that restore average parallel trends in the sense of (13.18),

$$
\sum_ {\left\{D _ {i} = 0 \right\}} \gamma_ {i} Y _ {i t} \approx \alpha + \frac {1}{\left| \left\{D _ {i} = 1 \right\} \right|} \sum_ {\left\{D _ {i} = 1 \right\}} Y _ {i t}, \quad t = 1, \dots , H, \tag {13.19}
$$

and then estimate the SATT via weighted difference-in-differences

$$
\begin{array}{l} \hat {\tau} _ {S D I D} = \frac {1}{| \{i : D _ {i} = 1 \} |} \sum_ {\left\{i: D _ {i} = 1 \right\}} \left(\frac {1}{T - H} \sum_ {t = H + 1} ^ {T} Y _ {i t} - \frac {1}{H} \sum_ {t = 1} ^ {H} Y _ {i t}\right) \tag {13.20} \\ - \sum_ {\{i: D _ {i} = 0 \}} \gamma_ {i} \left(\frac {1}{T - H} \sum_ {t = H + 1} ^ {T} Y _ {i t} - \frac {1}{H} \sum_ {t = 1} ^ {H} Y _ {i t}\right). \\ \end{array}
$$

There are a number of ways one could seek weights that achieve balance as in

(13.19); one simple approach is to choose $\gamma _ { i }$ by minimizing squared-error loss:

$$
\begin{array}{l} \gamma = \operatorname {a r g m i n} _ {\gamma^ {\prime}, \alpha} \left\{\left\| \sum_ {\left\{D _ {i} = 0 \right\}} \gamma_ {i} ^ {\prime} Y _ {i (1: H)} - \frac {1}{\left| \left\{D _ {i} = 1 \right\} \right|} \sum_ {\left\{D _ {i} = 1 \right\}} ^ {n} Y _ {i (1: H)} - \alpha \right\| _ {2} ^ {2}: \right. \tag {13.21} \\ \left. \sum_ {\{D _ {i} = 0 \}} \gamma_ {i} ^ {\prime} = 1, \gamma_ {i} ^ {\prime} \geq 0 \right\}. \\ \end{array}
$$

Arkhangelsky et al. [2021] also consider re-weighting pre-event time periods for improved robustness; however, we omit this step here for simplicity, and refer to their paper for a full discussion.

To understand the motivation behind SDID note that, just like in the proof of Theorem 13.2, under non-anticipation,

$$
\begin{array}{l} \hat {\tau} _ {S D I D} - \bar {\tau} _ {A T T} \\ = \frac {1}{T - H} \sum_ {t = H + 1} ^ {T} \left(\frac {1}{\left| \left\{i : D _ {i} = 1 \right\} \right|} \sum_ {\left\{i: D _ {i} = 1 \right\}} Y _ {i t} (0) - \sum_ {\left\{i: D _ {i} = 0 \right\}} \gamma_ {i} Y _ {i t} (0)\right) \tag {13.22} \\ - \frac {1}{H} \sum_ {t = 1} ^ {H} \left(\frac {1}{| \{i : D _ {i} = 1 \} |} \sum_ {\{i: D _ {i} = 1 \}} Y _ {i t} (0) - \sum_ {\{i: D _ {i} = 0 \}} \gamma_ {i} Y _ {i t} (0)\right). \\ \end{array}
$$

Now, by the re-weighting (13.19), we know that the summands in the pre-event term of the right-hand-side expression are all roughly $\alpha$ . If similar balance also extends post-event to the unexposed potential outcomes, then summands in the first term above should also all be roughly $\alpha$ , thus making the error of $\hat { \tau } _ { S D I D }$ small. The big question, of course, is in understanding when‚Äîand under what conditions‚Äîweights obtained via (13.21) will also balance post-event unexposed potential outcomes. The technical tools for doing so are beyond the scope of this presentation. We instead refer to Abadie et al. [2010] and Arkhangelsky et al. [2021] for results of this type; $^ { 7 4 }$ see also Arkhangelsky and Hirshberg [2023] for recent advances.

Numerical example We illustrate the relationship between basic difference in differences and the synthetic control approach via a simple numerical example. We simulate data for $n = 5 0$ units and $T ' = 2 0$ time periods under block

![](images/a9d9385d2f10441c01eea2d32b4c825d67a86d7c3a22a665182f749c6dd7f135.jpg)  
difference in differences (DID)

![](images/99b2934630f9bbba334f8b8ccaad6536768e27f8e9f135ef7d4ce5932c1646a0.jpg)  
synthetic difference in differences (SDID)   
Figure 13.2: Results from applying DID and SDID on data simulated as in (13.23). In each case, treatment effects are estimated by measuring divergence of the average outcomes for the exposed units (in blue) to a comparison curve given by potentially weighted averages of unexposed outcomes (in red). The curved arrow denotes the resulting treatment effect estimate.

adoption with $H = 1 0$ . Each unit has IID latent parameters $\alpha _ { i }$ and $\beta _ { i }$ that inform trajectory evolution as follows:

$$
\alpha_ {i}, \beta_ {i} \sim \mathcal {N} (0, 1), D _ {i} \sim \mathrm {B e r n o u l l i} \left(1 / \left(1 + e ^ {1 - \beta_ {i}}\right)\right),
$$

$$
Y _ {i t} (d) = \alpha_ {i} + \frac {\beta_ {i} t}{1 0} - d \frac {(t - H) _ {+}}{1 0} + \varepsilon_ {i t}, \quad \varepsilon_ {i t} \sim \mathcal {N} \left(0, \frac {1}{1 0 ^ {2}}\right). \tag {13.23}
$$

This design satisfies non-anticipation as in Assumption 13.1. However, it does not have random treatment assignment or parallel trends as in Assumption 13.2: Units with large values of $\beta _ { i }$ both have more positive baseline trends, and are more likely to take up treatment. The DID estimator is thus not expected to be consistent here.

Figure 13.2 shows results from applying both the DID estimator (13.7) and the SDID implementation of the synthetic control approach as in (13.20) on one draw of data following (13.23). The DID estimator is confounded because exposure $D _ { i }$ is correlated with the latent factor $\beta _ { i }$ that also affects trends; and in fact we observe that the average outcomes for exposed and unexposed units do not evolve in parallel even before treatment. In contrast, SDID reweights unexposed units with the aim of restoring parallel trends. In our setting

the treatment effect is negative; and SDID correctly recovers the sign of the treatment effect here whereas DID does not.

# Bibliographic notes

Our presentation of event study designs fits within the tradition a broad literature on panel data methods in econometrics whose surface we‚Äôve only scratched here. Arellano [2003] and Wooldridge [2010] provide broad textbook overviews of the area. Arkhangelsky and Imbens [2024] provide an extensive review of recent developments in the area. The approach used here to define potential outcomes and causal estimands is adapted from Athey and Imbens [2022]. The averaged saturated regression estimator $\hat { \tau } _ { A S R }$ was proposed by Borusyak, Jaravel, and Spiess [2024]; however, they described the estimator via an imputation algorithm. The (numerically equivalent) saturated regression form of the estimator used here is due to Wooldridge [2025].

The topic of treatment heterogeneity in the context of two-way models has been the focus of a considerable amount of discussion in recent years; see de Chaisemartin and d‚ÄôHaultfoeuille [2020] for an early paper drawing attention to the phenomenon and Chiu et al. [2025] for a recent discussion and review. Here, we restricted our analysis on estimating $\bar { \tau } _ { A T T }$ as in (13.10). However, under staggered adoption, parallel trends allow for identification of a broader family of cohort-wise treatment effect estimates that may be relevant in applications [Borusyak, Jaravel, and Spiess, 2024, Callaway and Sant‚ÄôAnna, 2021, Sun and Abraham, 2021]:

$$
\bar {\tau} _ {A T T} ^ {h, t} = \sum_ {\{i: H _ {i t} = h \}} \left(Y _ {i t} (h) - Y _ {i t} (\infty)\right) / | \{i: H _ {i t} = h \} |. \tag {13.24}
$$

In particular, when there are no never-treated units as required in Theorem 13.3, then $\tau _ { A T T }$ is not identified under parallel trends, but some cohort-wise effects will still be identifiable as long as there‚Äôs some variation in the treatment start time.

The synthetic control method was introduced by Abadie and Gardeazabal [2003] and formalized by Abadie, Diamond, and Hainmueller [2010]. Extensions of synthetic controls with double-differencing structure‚Äîincluding the SDID method presented here‚Äîare discussed in Arkhangelsky et al. [2021], Ben-Michael, Feller, and Rothstein [2021] and Shen et al. [2023]. Arkhangelsky and Hirshberg [2023] study large-sample properties of synthetic control estimators when exposure is non-random and depends on unobservables.

From a formal perspective, synthetic control methods are often studied under an interactive fixed-effects model, where we posit

$$
Y _ {i t} = A _ {i.} \cdot B _ {t.} + W _ {i t} \tau + \varepsilon_ {i t}, \quad A \in \mathbb {R} ^ {n \times k}, \quad B \in \mathbb {R} ^ {T \times k}, \quad \mathbb {E} [ \varepsilon_ {i t} | W ] = 0. (1 3. 2 5)
$$

Here, unlike in the standard two-way specification (13.11), the $i$ -th unit has a $k$ -dimensional ‚Äútype‚Äù $A _ { i }$ ¬∑ that interacts with $B _ { t }$ ¬∑ in the $t$ -th time period. In the context of this model, showing that synthetic controls work involves proving that the $\gamma$ -weighting effectively eliminate bias due to imbalance in the unobserved types $A _ { i }$ ¬∑; see Arkhangelsky et al. [2021] for formal results within this paradigm.

An alternative approach to estimating $\tau$ under the interactive fixed-effects model involves fitting the full model (13.25)‚Äîincluding the unobserved baseline term $A B ^ { \prime }$ ‚Äîvia low-rank matrix estimation methods. Examples of this approach include Bai [2009], who use least-squares estimation, and Athey et al. [2021], who use nuclear-norm penalization. Agarwal et al. [2021], Lei and Ross [2023] and Xu [2017] consider a setting where a low-rank structure is assumed on the never-treated potential outcomes, but we don‚Äôt assume additive treatment effects as in (13.25). They then use matrix completion methods to estimate this low-rank structure and impute never-treated potential outcomes in the post-event periods; the SATT is finally estimated by comparing realized and imputed outcomes in these periods.

# Chapter 14 Evaluating Dynamic Policies

In the previous chapter, we considered methods for event studies where some units adopted a treatment (i.e., switched their treatment status from off-toon), and we wanted to measure the effect of making this switch. Results from event studies can be helpful in informing whether other units might also benefit from adopting the treatment. However, event-study designs‚Äîand associated methods such as difference in differences and synthetic controls‚Äîare less helpful for is in guiding dynamic decision making. Their limitations are perhaps best understood in the context of examples.

Example 18. During a financial downturn, central banks sometimes use quantitative easing to mitigate the risks of a long-term recession. During quantitative easing, the central bank seeks to increase market liquidity by purchasing government bonds and other assets. Some quantitative easing may help stimulate the economy and avoid a recession; however, too much quantitative easing‚Äîor quantitative easing that lasts for too long‚Äîmay lead to problems with excessive inflation [Boehl, Goy, and Strobel, 2024].

Example 19. Antiretroviral therapy (ART) is a crucial drug in caring for HIV-positive patients. It is understood that HIV reduces CD4 white blood cell count, and that patients are at risk of contracting AIDS-defined illnesses once CD4 count is low. The use of ART can help preserve CD4 counts and thus prevent AIDS, but it is a very intensive form of medication with a number of side effects. The topic of when to start ART has thus received considerable attention in the medical literature. Traditional guidelines for treating HIV recommend beginning ART only once CD4 count fall below a given threshold; but recent evidence is in favor of starting ART as soon as HIV is diagnosed [Group, 2015].

It is clear that a successful application of quantitative easing requires judicious consideration of when to start the intervention, how much liquidity to provide, and when to stop. However, event-study methods provide very

little guidance on questions of this type. The parallel trends assumption underlying difference-in-differences methods effectively rules out the possibility that, during a given crisis, there may be some countries that need quantitative easing (i.e., they would fall into a recession without intervention) and others that don‚Äôt (i.e., even without intervention they would be OK). Synthetic control methods could be used to study the effect of ART‚Äîor the initial effect of quantitative easing‚Äîbut do not readily give guidance as to when to start or stop the interventions.

This chapter presents a fully flexible, potential-outcome based approach to modeling causal effects over time that allows for arbitrary treatment assignment dynamics and carryover effects. Throughout, we will assume that we have data on $i = 1$ , . . . , $n$ patients, observed at times $t = 1 , \ldots , T$ $T$ . At each time point, we observe a set of (time-varying) covariates $X _ { i t }$ as well as a treatment assignment $W _ { i t } \in \{ 0 , 1 \}$ . Finally, once we reach time $T$ , we also observe an outcome $Y _ { i } \in \mathbb { R }$ . Throughout this chapter, we will take units $i$ to be sampled IID from a superpopulation.

We model causal effects using the potential outcome specification below that allows for arbitrary treatment dynamics. Note that this model implicitly encodes the fact that time- $t$ observables are only affected by actions taken up to time $t$ , and not future actions, thus generalizing the non-anticipation condition (Assumption 13.1) used in the event-study setting.

Definition 14.1. A dynamic decision process with time-horizon $T$ is characterized by outcomes time-varying covariates $X _ { i t } \in \mathcal { X } _ { t }$ and outcomes $Y _ { i } \in \mathbb { R }$ , with potential outcomes that make each observable responsive to all past treatment assignments. For each $X _ { i t }$ , we define $2 ^ { t - 1 }$ potential outcomes $X _ { i t } ( w _ { 1 : ( t - 1 ) } )$ such that $X _ { i t } = X _ { i t } ( W _ { i ( 1 : ( t - 1 ) ) } )$ , while for the final outcome we have $2 ^ { T }$ potential outcomes $Y _ { i } ( w _ { 1 : T } )$ such that $Y _ { i } = Y _ { i } ( W _ { i ( 1 : T ) } )$ .

Next, we need to define an estimand. In the dynamic setting, the number of potential treatment allocation rules grows exponentially with the horizon $T$ , and so does the number of questions we can ask. One simple estimand to consider is the expected outcome under some pre-specified treatment rule $w \in \{ 0 , 1 \} ^ { T }$ , i.e., $V ( w ) = \mathbb { E } \left[ Y _ { i } ( w ) \right]$ . Such estimands, however, are often not relevant to practice as they rule out dynamic decision making. Suppose, for example, that we‚Äôre studying cancer therapy and are asking to estimate $V ( w )$ for the treatment rule that starts chemotherapy one year after study enrollment (e.g., one year after cancer diagnosis). Then, if some patients enter remission through other means before they reach the one-year mark, evaluating $V ( w )$ would still require starting chemotherapy at this point‚Äîeven if it doesn‚Äôt make clinical sense.

In practice, it is often more relevant to evaluate treatment rules that take into account time-varying covariates. For example, we might ask about the benefit of starting chemotherapy one year after diagnosis among patients who have not yet entered remission, or we might ask about starting quantitative easing at a point when interest rates have hit 0 but economic activity is still weak. We can define a number of relevant estimands of this type via the lens of policy evaluation, in a generalization of our discussion from Chapter 5.1.

Definition 14.2. A dynamic policy is a set of mappings $\pi _ { t } : \mathcal { X } _ { t }  \{ 0 , 1 \}$ that prescribe a treatment $\pi _ { t } ( X _ { i t } )$ given the current state $X _ { i t }$ . The value of the policy $\pi$ is

$$
V (\pi) = \mathbb {E} \left[ Y _ {i} \left(\pi_ {1} \left(X _ {i 1}\right), \pi_ {2} \left(X _ {i 1}, \pi_ {1} \left(X _ {i 1}\right), X _ {i 2} \left(\pi_ {1} \left(X _ {i 1}\right)\right), \dots\right) \right], \right. \tag {14.1}
$$

i.e., it captures the expected reward from choosing treatment according to $\pi$ in a dynamic decision process.

The intricate notation in (14.1) highlights the complex causal structure inherent to dynamic decision-making problems: The treatment decision taken at time $t$ depends on $X _ { i t }$ , which in turn depends on the treatment decision taken at time $t - 1$ and thus $X _ { i ( t - 1 ) }$ , etc., until we get back to the initial state $X _ { i 1 }$ . Thankfully, these statistical objects are amenable to tractable analysis via a recursive, dynamic-programming-style approach.

# 14.1 Sequential unconfoundedness

In order to estimate the quantities defined above we need to collect data, and to make assumptions on how the treatment is assigned in the experiment in order to identify the estimands. Here, we will do so using a sequential unconfoundedness (or sequential ignorability) assumption which posits that, at every time point, treatment is as good as random given the data observed at the time:

$$
\left\{\left(\text {p o t e n t i a l} t\right) \right\} \perp W _ {i t} \mid \left\{\left(\text {h i s t o r y} t\right) \right\}. \tag {14.2}
$$

This condition is formalized below. Here, and throughout the rest of this chapter, we will use

$$
S _ {t} = \left\{X _ {1}, W _ {1}, \dots , W _ {t - 1}, X _ {t} \right\}
$$

to denote all information observed before the period- $t$ treatment $W _ { t }$ is chosen, and we will use the notational short-hand $X _ { i ( T + 1 ) } : = Y _ { i }$ (i.e., the outcome is the state variable measured after we cross the time-horizon $T$ ).

Assumption 14.1. Given a dynamic decision process, we further assume that our treatment sequence is sequentially unconfounded such that, for all $t = 1$ , . . . , $T$ , future potential outcomes are independent of current treatment conditionally on the past:75

$$
\left[ \left\{X _ {i (s + 1)} \left(W _ {i (1: (t - 1))}, w _ {t: s}\right): t \leq s \leq T, w _ {t: s} \in \left\{0, 1 \right\} ^ {s - t + 1} \right\} \perp W _ {i t} \right] \mid S _ {t}. \tag {14.3}
$$

Remark 14.1. In principle, one might also be interested in a design more directly comparable to a standard randomized controlled trial where treatment is fully randomized,

$$
\left\{\text {(a l l p o t e n t i a l o u t c o m e s)} \right\} \perp W _ {1: T}. \tag {14.4}
$$

This, however, can again lead to non-sense treatment assignments (e.g., in the case of a cancer trial, assigning a patient to chemotherapy after they have already reached remission), and so the literature on dynamic treatment rules has mostly focused on methods that work under the more flexible sequential unconfoundedness setting.

The key statistical consequence of sequential unconfoundedness is that it enables us to connect the joint distribution of $( X _ { i 1 } , . . . , X _ { i T } , X _ { i ( T + 1 ) } )$ under the observed data-collection distribution to the joint distribution of these variables under different counterfactual treatment-assignment policies $\pi$ via the $g$ -formula.76 As usual, we write $\mathbb { E } \left\lfloor \cdot \right\rfloor$ and $\mathbb { P } \left[ \cdot \right]$ for the on-policy expectations and probabilities characterizing the distribution we collect data from. For the purpose of policy evaluation, it is also convenient to also introduce off-policy measures $\mathbb { E } _ { \pi } \left[ \cdot \right]$ and $\mathbb { P } _ { \pi } \left[ \cdot \right]$ to describe distributions that would instead arise from assigning treatment according to $\pi$ as in Definition 14.2; given this notation, we can concisely write the policy value as $V ( \pi ) = \mathbb { E } _ { \pi } \left[ X _ { T + 1 } \right]$ .

We can trivially factor the the off-policy distribution $\mathbb { P } _ { \pi } \left[ \cdot \right]$ over time as

$$
\begin{array}{l} \mathbb {P} _ {\pi} \left[ X _ {1}, W _ {1}, \dots , W _ {T}, X _ {T + 1} \right] \\ = \mathbb {P} _ {\pi} \left[ X _ {1} \right] \prod_ {t = 1} ^ {T} \mathbb {P} _ {\pi} \left[ W _ {t} \mid S _ {t} \right] \mathbb {P} _ {\pi} \left[ X _ {t + 1} \mid W _ {t}, S _ {t} \right]. \tag {14.5} \\ \end{array}
$$

A key implication of sequential unconfoundedess is that in fact only the conditional treatment-assignment probabilities $\mathbb { P } _ { \pi } \left[ W _ { t } \mid S _ { t } \right]$ depend on $\pi$ ; all the

other terms in the above factorization can directly be read off from the onpolicy distribution [Robins, 1986]. The standard form of the $g$ -formula emerges by plugging (14.6) into (14.5).

Proposition 14.1. Under Assumption 14.1, terms in the factorization (14.5) that don‚Äôt integrate over $W _ { t }$ don‚Äôt depend on the policy $\pi$ , i.e.,

$$
\mathbb {P} _ {\pi} \left[ X _ {1} \right] = \mathbb {P} \left[ X _ {1} \right], \quad \mathbb {P} _ {\pi} \left[ X _ {t + 1} \mid S _ {t}, W _ {t} \right] = \mathbb {P} \left[ X _ {t + 1} \mid S _ {t}, W _ {t} \right], \tag {14.6}
$$

for all $t = 1$ , . . . , $T$ .

Proof. The state is immediate. No $X _ { 1 }$ is ob write o the first claimfor the set of all $U = \left\{ X _ { t + 1 } ( w _ { 1 : t } ) : w _ { 1 : t } \in \left\{ 0 , 1 \right\} ^ { t } \right\} _ { t = 1 } ^ { T }$ potential outcomes for a given unit across time, and note that

$$
\mathbb {P} _ {\pi} \left[ X _ {t + 1} \mid S _ {t}, W _ {t} \right] = \int \mathbb {P} _ {\pi} \left[ X _ {t + 1} \mid S _ {t}, W _ {t}, U \right] d \mathbb {P} _ {\pi} \left[ U \mid S _ {t}, W _ {t} \right],
$$

and the same expansion holds fr $\mathbb { P } \left[ X _ { t + 1 } \middle \vert S _ { t } , W _ { t } \right]$ . Clearly, for all $t = 1$ , . . . , $T$ ,

$$
\begin{array}{l} \mathbb {P} _ {\pi} \left[ X _ {t + 1} = x _ {t + 1} \mid S _ {t}, W _ {t}, U \right] = \mathbb {P} \left[ X _ {t + 1} = x _ {t + 1} \mid S _ {t}, W _ {t}, U \right] \\ = 1 \left\{u \leftrightarrow \left(x _ {t + 1}, w _ {1: t}\right) \right\}, \\ \end{array}
$$

where $1 \{ u  ( x _ { t + 1 } , w _ { 1 : t } ) \}$ denotes consistency of potential outcomes, i.e., that $u$ would in fact yield $X _ { t + 1 } = x _ { t + 1 }$ given treatment path $w _ { 1 : t }$ . This is because, once we condition on $U$ and $W _ { 1 : t }$ , there‚Äôs no randomness left in $X _ { t + 1 }$ .

Thus, to verify (14.6) it suffices to verify that, for all $t = 1$ , . . . , $T$ ,

$$
\mathbb {P} _ {\pi} [ U \mid S _ {t}, W _ {t} ] = \mathbb {P} [ U \mid S _ {t}, W _ {t} ]. \tag {14.7}
$$

We proceed by forward induction in $t$ . The base case with $t ~ = ~ 1$ follows immediately by (14.3) with $t = 1$ . Now, assume that (14.7) holds for $t - 1$ (and thus also (14.6) holds for $t - 1$ ). Then, under our data-collection policy

$$
\begin{array}{l} \mathbb {P} \left[ U = u \mid X _ {1: t} = x _ {1: t}, W _ {1: t} = w _ {1: t} \right] \\ = \mathbb {P} \left[ U = u \mid X _ {1: t} = x _ {1: t}, W _ {1: (t - 1)} = w _ {1: (t - 1)} \right] \\ = \frac {1 \left\{u \leftrightarrow \left(x _ {t} , w _ {(1 : t - 1)}\right) \right\} \mathbb {P} \left[ U = u \mid X _ {1 : (t - 1)} = x _ {1 : (t - 1)} , W _ {1 : (t - 1)} = w _ {1 : (t - 1)} \right]}{\mathbb {P} \left[ X _ {t} = x _ {t} \mid X _ {1 : (t - 1)} = x _ {1 : (t - 1)} , W _ {1 : (t - 1)} = w _ {1 : (t - 1)} \right]}, \\ \end{array}
$$

where the first equality holds by (14.3) at time $t$ , while the second equality holds by Bayes‚Äô rule. Meanwhile, under our target policy, treatment is by

Table 14.1: A synthetic two-period example reproduced from Hern¬¥an and Robins [2020, Table 20.1].   

<table><tr><td>n</td><td>Xi1</td><td>Wi1</td><td>Xi2</td><td>Wi2</td><td>Mean Y</td></tr><tr><td>2400</td><td>0</td><td>0</td><td>0</td><td>0</td><td>84</td></tr><tr><td>1600</td><td>0</td><td>0</td><td>0</td><td>1</td><td>84</td></tr><tr><td>2400</td><td>0</td><td>0</td><td>1</td><td>0</td><td>52</td></tr><tr><td>9600</td><td>0</td><td>0</td><td>1</td><td>1</td><td>52</td></tr><tr><td>4800</td><td>0</td><td>1</td><td>0</td><td>0</td><td>76</td></tr><tr><td>3200</td><td>0</td><td>1</td><td>0</td><td>1</td><td>76</td></tr><tr><td>1600</td><td>0</td><td>1</td><td>1</td><td>0</td><td>44</td></tr><tr><td>6400</td><td>0</td><td>1</td><td>1</td><td>1</td><td>44</td></tr></table>

construction independent of $U$ , and so

$$
\begin{array}{l} \mathbb {P} _ {\pi} \left[ U = u \mid X _ {1: t} = x _ {1: t}, W _ {1: t} = w _ {1: t} \right] \\ = \frac {1 \left\{u \leftrightarrow \left(x _ {t} , w _ {(1 : t - 1)}\right) \right\} \mathbb {P} _ {\pi} \left[ U = u \mid X _ {1 : (t - 1)} = x _ {1 : (t - 1)} , W _ {1 : (t - 1)} = w _ {1 : (t - 1)} \right]}{\mathbb {P} _ {\pi} \left[ X _ {t} = x _ {t} \mid X _ {1 : (t - 1)} = x _ {1 : (t - 1)} , W _ {1 : (t - 1)} = w _ {1 : (t - 1)} \right]}. \\ \end{array}
$$

Finally, we note that the two above expressions match thanks to our inductive hypothesis, and so (14.7) holds for $t$ . ‚ñ°

Treatment-confounder feedback Before introducing methods that work under sequential unconfoundedness, it is worth highlighting a subtle difficulty that arises in this setting not present in the basic (single-period) design, namely treatment-confounder feedback [Robins, 1986]. To see what may go wrong, consider the following simple example adapted from Hern¬¥an and Robins [2020], modeled after an ART trial with $T = 2$ time periods. Here, $X _ { i t } \in \{ 0 , 1 \}$ denotes CD4 count (1 is low, i.e., bad), and suppose that $X _ { i 1 } = 0$ for everyone (no one enters the trial very sick), and $W _ { i 1 }$ is randomized with probability 0.5 of receiving treatment. Then, at time period 2, we observe $X _ { i 2 }$ and assign treatment $W _ { i 2 } = 1$ with probability 0.4 if $X _ { i 2 } = 0$ and with probability 0.8 if $X _ { i 2 } = 1$ . In the end, we collect a health outcome $Y$ . This is a sequentially randomized experiment that satisfies Assumption 14.1.

We observe data as in Table 14.1, wherethe last column is the mean outcome for everyone in that row. Our goal is to estimate $\tau = \mathbb { E } \left[ Y \left( \underline { { 1 } } \right) - Y \left( \underline { { 0 } } \right) \right]$ , i.e., the difference between the always treat and never treat rules. How should we do this? As a preliminary, it‚Äôs helpful to note that the treatment obviously has no effect on the outcome $Y$ . In the first time period,

$$
\mathbb {E} \left[ Y _ {i} \mid W _ {i 1} = 0 \right] = \mathbb {E} \left[ Y _ {i} \mid W _ {i 1} = 1 \right] = 6 0,
$$

and this is obviously a causal quantity (since $W _ { i 1 }$ was randomized). Moreover, in the second time period we see by inspection that

$$
\mathbb {E} \left[ Y _ {i} \mid W _ {i 1} = w _ {1}, X _ {i 2} = x, W _ {i 2} = 0 \right] = \mathbb {E} \left[ Y _ {i} \mid W _ {i 1} = w _ {1}, X _ {i 2} = x, W _ {i 2} = 1 \right],
$$

for all values of $w _ { 1 }$ and $x$ , and again the treatment has no effect on the outcome. On the other hand, the initial treatment $W _ { i 1 }$ does have an effect on the intermediate state variable $X _ { i 2 }$ .

The challenge, however, is that if we want to actually design an estimator for $\tau$ , some simple strategies that served us well in the non-dynamic setting do not get the right answer. In particular, here are some strategies that do not get the right answer:

‚Ä¢ Ignore adaptive sampling, and use

$$
\begin{array}{l} \hat {\tau} = \widehat {\mathbb {E}} [ Y | W = \underline {{1}} ] - \widehat {\mathbb {E}} [ Y | W = \underline {{0}} ] \\ = \frac {6 4 0 0 \times 4 4 + 3 2 0 0 \times 7 6}{6 4 0 0 + 3 2 0 0} - \frac {2 4 0 0 \times 5 2 + 2 4 0 0 \times 8 4}{2 4 0 0 + 2 4 0 0} \\ = 5 4. 7 - 6 8 = - 1 3. 3. \\ \end{array}
$$

‚Ä¢ Stratify by CD4 count at time 2, to control for adaptive sampling:

$$
\hat {\tau} _ {0} = \mathbb {E} [ Y | W = \underline {{1}}, X _ {i 2} = 0 ] - \mathbb {E} [ Y | W = \underline {{0}}, X _ {i 2} = 0 ] = 7 6 - 8 4 = - 8
$$

$$
\hat {\tau} _ {1} = \mathbb {E} [ Y | W = \underline {{1}}, X _ {i 2} = 1 ] - \mathbb {E} [ Y | W = \underline {{0}}, X _ {i 2} = 1 ] = 4 4 - 5 2 = - 8
$$

$$
\hat {\tau} = \frac {(3 2 0 0 + 2 4 0 0) \hat {\tau} _ {0} + (6 4 0 0 + 2 4 0 0) \hat {\tau} _ {1}}{3 2 0 0 + 2 4 0 0 + 6 4 0 0 + 2 4 0 0} = - 8.
$$

The problem with the first strategy is obvious (we need to correct for biased sampling). But the problem with the second strategy is more subtle. We know via sequantial randomization that

$$
Y _ {i} (\dots) \perp W _ {i 2} \mid X _ {i 2},
$$

and this seems to justify stratification. But what we‚Äôd actually need for stratification is:

$$
Y _ {i} (\dots) \perp (W _ {i 1}, W _ {i 2}) \mid X _ {i 2},
$$

and this is not true by design.

To see what could go wrong, imagine that there are 3 types of people (stable, responder, acute), and tabulate their time-2 CD4 values as in Table 14.2. These types‚Äîoften called principal strata‚Äîare are unobservable but can still provide insights.77 For example:

Table 14.2: Responder types in the setting of Table 14.1.   

<table><tr><td></td><td>Wi1=0</td><td>Wi1=1</td></tr><tr><td>stable</td><td>Xi2=0</td><td>Xi2=0</td></tr><tr><td>responder</td><td>Xi2=1</td><td>Xi2=0</td></tr><tr><td>acute</td><td>Xi2=1</td><td>Xi2=1</td></tr></table>

‚Ä¢ $\mathbb { E } [ Y | W = \underline { { 1 } }$ , $X _ { i 2 } = 0 ]$  is an average over stable or responder patients, whereas E $\lfloor Y \mid W = \underline { { 0 } }$ , Xi2 = 0 is simply an average over stable patients. So the difference $\hat { \tau } _ { 0 }$ is not estimating a proper causal quantity.   
‚Ä¢ $\mathbb { E } [ Y \big | W = \underline { { 1 } }$ , $X _ { i 2 } = 1 ]$ is an average over acute patients, whereas in contrast $\mathbb { E } [ Y \vert W = \underline { { 0 } }$ , Xi2 = 1 is an average over responder or acute patients. So the difference $\hat { \tau } _ { 1 }$ is not estimating a proper causal quantity.

In other words, in sequentially randomized trials, simple stratification estimators do not successfully control for confounding.

Sequential inverse-propensity weighting Since stratification doesn‚Äôt work, we now move to study a family of approaches that do. Here, we focus on estimating the value of a policy $V ( \pi )$ as in (14.1); note that evaluating a fixed treatment sequence is a special case of this strategy. To this end, it‚Äôs helpful to define some more notation: Writing $S _ { t }$ for the information available at time $t$ as before, we define the value function $^ { 7 8 }$

$$
V _ {\pi , t} \left(S _ {t}\right) = \mathbb {E} _ {\pi} [ Y \mid S _ {t} ] \tag {14.8}
$$

that measures the expected reward we‚Äôd get if we were to start following $\pi$ given our current state as captured by $S _ { t }$ . The value function has the following key property.

Lemma 14.2. Under Assumption 14.1, the value function (14.8) satisfies

$$
\mathbb {E} \left[ V _ {\pi , t + 1} \left(S _ {t + 1}\right) \mid S _ {t}, W _ {t} = \pi_ {t} \left(S _ {t}\right) \right] = V _ {\pi , t} \left(S _ {t}\right). \tag {14.9}
$$

Proof. Under Assumption 14.1, we can use Proposition 14.1 to verify that

$$
\begin{array}{l} \mathbb {E} \left[ V _ {\pi , t + 1} (S _ {t + 1}) \mid S _ {t}, W _ {t} = \pi_ {t} (S _ {t}) \right] = \mathbb {E} _ {\pi} \left[ V _ {\pi , t + 1} (S _ {t + 1}) \mid S _ {t}, W _ {t} = \pi_ {t} (S _ {t}) \right] \\ = \mathbb {E} _ {\boldsymbol {\pi}} \left[ V _ {\boldsymbol {\pi}, t + 1} (S _ {t + 1}) \mid S _ {t} \right]. \\ \end{array}
$$

Then, by the chain rule,

$$
\begin{array}{l} \mathbb {E} _ {\pi} \left[ V _ {\pi , t + 1} \left(S _ {t + 1}\right) \mid S _ {t} \right] = \mathbb {E} _ {\pi} \left[ \mathbb {E} _ {\pi} \left[ Y \mid S _ {t + 1} \right] \mid S _ {t} \right] \tag {14.10} \\ = \mathbb {E} _ {\boldsymbol {\pi}} \left[ Y \mid S _ {t} \right] = V _ {\boldsymbol {\pi}, t} (S _ {t}), \\ \end{array}
$$

thus establishing our desired claim.

![](images/08c13f4b0b1cf90112bbb166719c3866be00d7a933c75a1f57bc699406a319f4.jpg)

The implication of (14.9) is that, given a good estimate of $V _ { \pi , t + 1 } ( S _ { t + 1 } )$ , we can obtain a good estimate of $V _ { \pi , t } ( S _ { t } )$ ; then we can recurse our way backwards to $V ( \pi )$ . The question is then how we choose to act on this insight. One simple way to do so is via an inverse-propensity weighting (IPW) construction. If we had access to $V _ { \pi , t + 1 } \big ( S _ { i ( t + 1 ) } \big )$ and many samples with $S _ { i t } = s _ { t }$ , then applying the basic IPW construction from Chapter 2 under (14.3) would suggest using

$$
\widehat {V} _ {\pi , t} (s _ {t}) = \frac {1}{| \{i : S _ {i t} = s _ {t} \} |} \sum_ {\{i: S _ {i t} = s _ {t} \}} \frac {1 (\{W _ {i t} = \pi (s _ {t}) \})}{\mathbb {P} [ W _ {i t} = \pi (s _ {t}) | S _ {i t} = s _ {t} ]} V _ {\pi , t + 1} (S _ {i (t + 1)}).
$$

A recursive application of this principle results in the IPW estimator of the policy value,

$$
\widehat {V} _ {I P W} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \gamma_ {i T} (\pi) Y _ {i}, \tag {14.11}
$$

$$
\gamma_ {i t} (\pi) = \gamma_ {i (t - 1)} (\pi) \frac {1 \left(\left\{W _ {t} = \pi_ {t} (S _ {t}) \right\}\right)}{\mathbb {P} \left[ W _ {t} = \pi_ {t} (S _ {t}) \mid S _ {t} \right]},
$$

where $\gamma _ { i 0 } ( \pi ) = 1$ . This estimator averages outcomes whose treatment trajectory exactly matches $\pi$ , while applying an IPW correction for selection effects due to measured (time-varying) confounders. We show below that the IPW estimator is unbiased if we know the inverse-propensity weights $\gamma _ { i T }$ exactly, and give an expression for its asymptotic variance.

Theorem 14.3. Consider a dynamic decision process as in Definition 14.1 with data collected under sequential unconfoundedness as in Assumption 14.1. Suppose furthermore that we seek to evaluate a policy œÄ for which strong overlap holds, i.e.,

$$
\mathbb {P} \left[ W _ {t} = \pi_ {t} \left(S _ {t}\right) \mid S _ {t} \right] \geq_ {a. s.} \eta , \tag {14.12}
$$

and that our outcomes are almost surely bounded, $\lvert Y \rvert \leq _ { a . s }$ . M for some $M <$ ‚àû. Then, the IPW estimator from (14.11) is unbiased with and asymptotically

normal sampling distribution,79

$$
\mathbb {E} \left[ \widehat {V} _ {I P W} (\pi) \right] = V (\pi), \quad \sqrt {n} \left(\widehat {V} _ {I P W} (\pi) - V (\pi)\right) \Rightarrow \mathcal {N} \left(0, \sigma_ {I P W} ^ {2}\right)
$$

$$
\sigma_ {I P W} ^ {2} = \mathbb {E} _ {\pi} \left[ Y ^ {2} / \prod_ {t = 1} ^ {T} \mathbb {P} \left[ W _ {t} = \pi_ {t} \left(S _ {t}\right) \mid S _ {t} \right] \right] - V ^ {2} (\pi). \tag {14.13}
$$

Proof. We verify unbiasedness via backwards induction, starting from $t = x$ , and argue that

$$
V _ {\pi , t} \left(S _ {t}\right) = \mathbb {E} \left[ \frac {\gamma_ {T} (\pi)}{\gamma_ {t - 1} (\pi)} Y \mid S _ {t} \right] \tag {14.14}
$$

for all $t = 0 , \ldots , T$ $T$ , where we use $S _ { 0 } = \emptyset$ and $\gamma _ { - 1 } ( \pi ) = \gamma _ { 0 } ( \pi ) = 1$ . The base case, with $t = \tau$ , corresponds exactly to the unbiasedness result in Theorem 2.2, while the final step with $t = 0$ corresponds to our desired claim. For the inductive step, suppose that (14.14) holds for $t + 1$ . Then, we can verify that

$$
\begin{array}{l} \mathbb {E} \left[ \frac {\gamma_ {T} (\pi)}{\gamma_ {t - 1} (\pi)} Y \mid S _ {t} \right] = \mathbb {E} \left[ \frac {\gamma_ {t} (\pi)}{\gamma_ {t - 1} (\pi)} \mathbb {E} \left[ \frac {\gamma_ {T} (\pi)}{\gamma_ {t} (\pi)} Y \mid S _ {t + 1} \right] \mid S _ {t} \right] \\ = \mathbb {E} \left[ \frac {1 \left(\left\{W _ {t} = \pi_ {t} \left(S _ {t}\right) \right\}\right)}{\mathbb {P} \left[ W _ {t} = \pi_ {t} \left(S _ {t}\right) \mid S _ {t} \right]} V _ {\pi , t + 1} \left(S _ {t + 1}\right) \mid S _ {t} \right] \\ = \mathbb {E} _ {\pi} \left[ V _ {\pi , t + 1} \left(S _ {t + 1}\right) \mid S _ {t} \right] \\ = V _ {\pi , t} \left(S _ {t} ^ {\prime}\right), \\ \end{array}
$$

where the first equality follows because $\gamma _ { t } ( \pi ) / \gamma _ { t - 1 } ( \pi )$ is $S _ { t }$ -measurable, the second follows by invoking the inductive hypothesis and by definition of $\gamma _ { t } ( \pi ) / \gamma _ { t - 1 } ( \pi )$ , the third follows by the $g$ -formula as given in Proposition 14.1, and the last is as in the proof of Lemma 14.2.

Given unbiasedness and IID sampling of units, the central limit theorem immediately follows with

$$
\sigma_ {I P W} ^ {2} = \mathbb {E} \left[ \gamma_ {T} ^ {2} (\pi) Y ^ {2} \right] - V ^ {2} (\pi),
$$

and it only remains to derive an explicit expression for the 2nd moment term above. Now, by repeating the same IPW argument as used above,

$$
\mathbb {E} \left[ \gamma_ {T} ^ {2} (\pi) Y ^ {2} \right] = \mathbb {E} _ {\pi} \left[ \gamma_ {T} (\pi) Y ^ {2} \right].
$$

Under the off-policy measure $\mathbb { E } _ { \pi } \left[ \cdot \right]$ , we always have $W _ { t } = \pi _ { t } ( S _ { t } )$ , and so

$$
\gamma_ {T} (\pi) = 1 \Big / \prod_ {t = 1} ^ {T} \mathbb {P} \left[ W _ {t} = \pi_ {t} (S _ {t}) \mid S _ {t} \right]
$$

almost surely, thus providing the expression claimed.

![](images/8dff4aaefc2726a0f7ce7728a1489ff40dc7d3d0b1e04be90f7aab07b60bd298.jpg)

Remark 14.2. As discussed in Chapter 12, we can often improve the asymptotic precision of IPW via self-normalization:

$$
\widehat {V} _ {S I P W} (\pi) = \sum_ {i = 1} ^ {n} \gamma_ {i T} (\pi) Y _ {i} / \sum_ {i = 1} ^ {n} \gamma_ {i T} (\pi). \tag {14.15}
$$

Under the conditions of Theorem 14.3,

$$
\sqrt {n} \left(\widehat {V} _ {S I P W} (\pi) - V (\pi)\right) \Rightarrow \mathcal {N} \left(0, \sigma_ {S I P W} ^ {2}\right)
$$

$$
\sigma_ {S I P W} ^ {2} = \mathbb {E} _ {\pi} \left[ (Y - V (\pi)) ^ {2} / \prod_ {t = 1} ^ {T} \mathbb {P} \left[ W _ {t} = \pi_ {t} \left(S _ {t}\right) \mid S _ {t} \right] \right]. \tag {14.16}
$$

This result can be established by following the same proof strategy as in, e.g., Theorem 12.3. The change in precision from self-normalization is

$$
\begin{array}{l} \sigma_ {I P W} ^ {2} - \sigma_ {S I P W} ^ {2} = \left(\mathbb {E} _ {\pi} \left[ \left(\prod_ {t = 1} ^ {T} \mathbb {P} \left[ W _ {t} = \pi_ {t} \left(S _ {t}\right) \mid S _ {t} \right]\right) ^ {- 1} \right] - 1\right) V ^ {2} (\pi) \tag {14.17} \\ + 2 \operatorname {C o v} _ {\pi} \left[ Y, \left(\prod_ {t = 1} ^ {T} \mathbb {P} \left[ W _ {t} = \pi_ {t} \left(S _ {t}\right) \mid S _ {t} \right]\right) ^ {- 1} \right] V (\pi). \\ \end{array}
$$

The first summand is always positive (and often large); however, the second summand can be negative‚Äîand could in principle be negative enough to make self-normalized IPW less precise than the basic IPW estimator.

We close our discussion by revising the example from Table 14.1, and verifying that our sequential IPW estimator resolves issues with our na¬®ƒ±ve attempted baselines. Recall that $\mathbb { P } \left[ W _ { 1 } = 1 | X _ { 1 } = 0 \right] = 0 . 5$ , $\mathbb { P } \left[ W _ { 2 } = 1 \vert X _ { 1 } = 0 \right] = 0 . 4$ , and $\mathbb { P } \left[ W _ { 2 } = 1 \big | X _ { 1 } = 1 \right] = 0 . 8$ . Thus, the IPW estimator for the values of the never- or always-treat policies are

$$
\widehat {V} _ {I P W} (\underline {{0}}) = \frac {2 , 4 0 0}{3 2 , 0 0 0} \frac {1}{0 . 5} \frac {1}{0 . 6} 8 4 + \frac {2 , 4 0 0}{3 2 , 0 0 0} \frac {1}{0 . 5} \frac {1}{0 . 2} 5 2 = 6 0,
$$

$$
\widehat {V} _ {I P W} (\underline {{1}}) = \frac {3 , 2 0 0}{3 2 , 0 0 0} \frac {1}{0 . 5} \frac {1}{0 . 4} 7 6 + \frac {6 , 4 0 0}{3 2 , 0 0 0} \frac {1}{0 . 5} \frac {1}{0 . 8} 4 4 = 6 0,
$$

where 32,000 is the total sample size. As expected, we find that there is no difference in average outcomes under the never- or always-treat policies.

# 14.2 Doubly robust estimation

Like in the single-period case discussed in Chapter 3, it is possible to improve the precision and robustness of IPW by augmenting it with a regression adjustment. Here, we show how to construct an augmented estimator for dynamic treatment rules, and verify that the resulting estimator is has a strong double robustness property: It can trade off accuracy of the regression and propensityscore models and achieve the parametric $1 / \sqrt { n }$ -rate of convergence even if input non-parametric regressions converge at slower rates.

Backwards regression adjustment Like in Chapter 3, our doubly robust construction starts by using sequential unconfoundedness to motivate an alternative, regression-based approach to estimating the value of a policy $\pi$ . Recall that, by Lemma 14.2,

$$
V _ {\pi , t} (s) = \mathbb {E} \left[ V _ {\pi , t + 1} \left(S _ {t + 1}\right) \mid S _ {t} = s, W _ {t} = \pi_ {t} (s) \right], \tag {14.18}
$$

which suggests the following backward regression approach to estimating the policy value:

‚Ä¢ First, using samples $i$ that exactly follow the target policy, i.e., with $W _ { i t } =$ $\pi ( S _ { i t } )$ for all $t = 1 , \ldots , T ^ { \prime }$ $T$ , learn $\widehat { V } _ { \pi , T } ( \cdot )$ via non-parametric regression $Y _ { i } \sim V _ { \pi , T } ( S _ { i T } )$ .   
‚Ä¢ Next, iteratively for $t = T - 1$ , T ‚àí 2, . . . , 1:

‚Äì Using samples $i$ that exactly follow the target policy up to time $t$ , i.e., with $W _ { i t ^ { \prime } } = \pi ( S _ { i t ^ { \prime } } )$ for all $t ^ { \prime } = 1 , \ldots , t$ , learn $\widehat { V } _ { \pi , t } ( \cdot )$ via nonparametric regression $\dot { V } _ { \pi , t + 1 } ( S _ { i ( t + 1 ) } ) \sim V _ { \pi , t } ( S _ { i t } )$ .

‚Ä¢ Finally, form the regression estimator for the value of $\pi$

$$
\widehat {V} _ {R E G} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \widehat {V} _ {\pi , 1} (S _ {i 1}). \tag {14.19}
$$

This backwards-regression approach can be implemented via generic machine learning. However, tailored models may also be helpful; for example, structural nested mean models [Robins, 1994] are designed to avoid spurious detection of causal effects under a null where the intervention has no effect.

A regression-augmented estimator Where there‚Äôs an IPW and a regression based estimator, there‚Äôs going to be a doubly robust estimator also. In the the last step of the backward-regression estimator (14.18), we averaged time-1 value-function estimates $\widehat { V } _ { \pi , 1 } ( X _ { 1 } )$ to obtain $\widehat { V } _ { R E G } ( \pi )$ . Now, given the backward-regression construction, it‚Äôs likely we trust the time-2 value function estimates $\widehat { V } _ { \pi , 2 }$ a little more than the time-1 estimates; and in this case we may consider using the basic augmented IPW (AIPW) construction from Chapter 3 to leverage these $\widehat { V } _ { \pi , 2 }$ estimates for improved precision:

$$
\widehat {V} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\widehat {V} _ {\pi , 1} (X _ {i 1}) + \gamma_ {i 1} (\pi) \left(\widehat {V} _ {\pi , 2} (X _ {i 1}, W _ {i 1}, X _ {i 2}) - \widehat {V} _ {\pi , 1} (X _ {i 1})\right)\right).
$$

Qualitatively, the idea here is that on the event where $W _ { i 1 }$ matches $\pi$ in the first step, we can use $\widehat { V } _ { \pi , 2 }$ to debias $\widehat { V } _ { \pi , 1 }$ ; here, the $\gamma _ { i t }$ are the inverse-propensity weights as in (14.11).

Then next natural question, of course, is why not debias $\widehat { V } _ { \pi , 2 }$ using $\widehat { V } _ { \pi , 3 }$ when $W _ { i 2 }$ also matches $\pi$ in the second step? And once we do so, why not proceed until the end of the time-horizon when we can observe the realized outcome $Y$ ? This recursive construction in fact works, and yields a natural generalization of the AIPW estimator of Robins, Rotnitzky, and Zhao [1994] discussed in Chapter 3 to the dynamic setting:

$$
\begin{array}{l} \widehat {V} _ {A I P W} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\widehat {V} _ {\pi , 1} \left(X _ {i 1}\right) \right. \tag {14.20} \\ \left. + \sum_ {t = 1} ^ {T} \hat {\gamma} _ {i t} (\pi) \left(\widehat {V} _ {\pi , t + 1} \left(S _ {i (t + 1)}\right) - \widehat {V} _ {\pi , t} \left(S _ {i t}\right)\right)\right), \\ \end{array}
$$

where we used a notational convention that $\widehat { V } _ { \pi , T + 1 } ( S _ { i ( T + 1 ) } ) = Y _ { i }$ since by time $T + 1$ the final outcome has been revealed.

Below, we analyze large-sample properties of this estimator under the double machine learning framework, and see that it preserves the strong double robustness property discussed in Chapter 3: The estimator has good properties if the product of the mean-squared errors for the $\hat { \gamma } _ { t } ( \pi )$ model and for the $\widehat { V } _ { \pi , t }$ decay fast enough. For simplicity, we assume that that the estimators for these functions are obtained using independent training data; however, as in Chapter 3, the argument generalizes immediately to $K$ -fold cross-fitting at the cost of some extra notation.

Theorem 14.4. Under the conditions of Theorem 14.3, suppose furthermore that we estimate the non-parametric components in (14.20) on independent

training data such that, for all $t = 1$ , . . . , $T$ ,8 0

$$
\mathbb {E} \left[ \left(\hat {\gamma} _ {i t} (\pi) - \gamma_ {i t} (\pi)\right) ^ {2} \right] = o _ {P} \left(n ^ {- 2 \alpha_ {\gamma}}\right),
$$

$$
\mathbb {E} \left[ \left(\widehat {V} _ {\pi , t} \left(S _ {i t}\right) - V _ {\pi , t} \left(S _ {i t}\right)\right) ^ {2} \right] = o _ {P} \left(n ^ {- 2 \alpha_ {V}}\right) \tag {14.21}
$$

for constants $\alpha _ { \gamma }$ , $\alpha _ { V } \geq 0$ with $\alpha _ { \gamma } + \alpha _ { V } \ge 1 / 2$ . Then,

$$
\begin{array}{l} \sqrt {n} \left(\widehat {V} _ {A I P W} (\pi) - V (\pi)\right) \Rightarrow \mathcal {N} \left(0, \sigma_ {A I P W} ^ {2}\right) \\ \sigma_ {A I P W} ^ {2} = \operatorname {V a r} \left[ \mathbb {E} _ {\pi} [ Y \mid X _ {1} ] \right] \tag {14.22} \\ + \sum_ {t = 1} ^ {T} \mathbb {E} _ {\pi} \left[ \mathrm {V a r} _ {\pi} \left[ \mathbb {E} _ {\pi} \left[ Y \mid S _ {t + 1} \right] \mid S _ {t} \right] \middle / \prod_ {t ^ {\prime} = 1} ^ {t} \mathbb {P} \left[ W _ {t ^ {\prime}} = \pi_ {t ^ {\prime}} (S _ {t ^ {\prime}}) \mid S _ {t ^ {\prime}} \right] \right]. \\ \end{array}
$$

Proof. As in the proof of the single time-step AIPW result in Chapter 3, we first consider properties of an oracle estimator with direct information about the functions used to form the AIPW estimator, and then show asymptotic equivalence of the feasible and oracle AIPW estimators under rate-of-convergence assumptions and with exogenous estimates of these functions. In our setting, the oracle is

$$
\begin{array}{l} \widehat {V} _ {A I P W} ^ {*} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(V _ {\pi , 1} \left(X _ {i 1}\right) \right. \tag {14.23} \\ \left. + \sum_ {t = 1} ^ {T} \gamma_ {i t} (\pi) \left(V _ {\pi , t + 1} \left(S _ {i (t + 1)}\right) - V _ {\pi , t} \left(S _ {i t}\right)\right)\right), \\ \end{array}
$$

with $\begin{array} { r l r } { V _ { \pi , t } ( S _ { i ( T + 1 ) , } ) } & { { } = } & { Y _ { i } } \end{array}$ . Recall that, by Lemma 14.2, we have $\mathbb { E } \left\lfloor V _ { \pi , t + 1 } \big ( S _ { i ( t + 1 ) } \big ) \ \middle \vert \ S _ { i t } \right.$ , $W _ { i t } = \pi ( S _ { i t } ) \big ] = V _ { \pi , t } ( S _ { i t } )$ . Thus, because $\gamma _ { i t } ( \pi )$ is a function of $S _ { i t }$ and $W _ { i t }$ , and $\gamma _ { i t } ( \pi ) \neq 0$ only when $W _ { i t } = \pi ( S _ { i t } )$ , we have

$$
\mathbb {E} \left[ \gamma_ {i t} (\pi) \left(V _ {\pi , t + 1} \left(S _ {i (t + 1)}\right) - V _ {\pi , t} \left(S _ {i t}\right)\right) \mid S _ {i t} \right] = 0, \tag {14.24}
$$

i.e., the terms $\gamma _ { i t } ( \pi ) \left( V _ { \pi , t + 1 } ( S _ { i ( t + 1 ) } ) - V _ { \pi , t } ( S _ { i t } ) \right)$ for a given unit $i$ form a martingale difference sequence. Thus, the oracle estimator is unbiased and

$$
\begin{array}{l} \mathrm {V a r} \left[ V _ {\pi , 1} (X _ {i 1}) + \sum_ {t = 1} ^ {T} \gamma_ {i t} (\pi) \left(V _ {\pi , t + 1} (S _ {i (t + 1)}) - V _ {\pi , t} (S _ {i t})\right) \right] \\ = \operatorname {V a r} \left[ V _ {\pi , 1} \left(X _ {i 1}\right) \right] + \sum_ {t = 1} ^ {T} \operatorname {V a r} \left[ \gamma_ {i t} (\pi) \left(V _ {\pi , t + 1} \left(S _ {i (t + 1)}\right) - V _ {\pi , t} \left(S _ {i t}\right)\right) \right]. \\ \end{array}
$$

One recovers the variance expression in (14.22) by moving to the off-policy measure as in the proof of Theorem 14.3 and then plugging in the definition of the value function from (14.8). Finally, given IID sampling of units $i = 1$ , . . . , $n$ our strong overlap and boundedness assumptions, the central limit theorem 14.22 follows immediately for the oracle estimator (14.23).

Now, to show asymptotic equivalence of the feasible and oracle AIPW estimators, we introduce some convenient short-hand. We write the time- $t$ value function updates as

$$
\varepsilon_ {i t} := V _ {\pi , t + 1} (S _ {i (t + 1)}) - V _ {\pi , t} (S _ {i t})
$$

for $t = 0 , \ldots , T$ $T$ , and the value function errors as

$$
\hat {\delta} _ {i t} = \widehat {V} _ {\pi , t} (S _ {i t}) - V _ {\pi , t} (S _ {i t})
$$

for $t = 1$ , . . . , $T$ . We also drop the explicit $\pi$ dependence in $\gamma _ { i t } ( \pi )$ . Given this notation, we have

$$
\widehat {V} _ {A I P W} ^ {*} (\pi) - V (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 0} ^ {T} \gamma_ {i t} \mathcal {E} _ {i t}
$$

$$
\widehat {V} _ {A I P W} (\pi) - V (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 0} ^ {T} \widehat {\gamma} _ {i t} \left(\varepsilon_ {i t} + \widehat {\delta} _ {i (t + 1)} - \widehat {\delta} _ {i t}\right),
$$

provided we use the following conventions for edge cases: $\begin{array} { r l r } { \gamma _ { i 0 } } & { { } = } & { 1 } \end{array}$ , $\widehat { V } _ { \pi , T + 1 } ( S _ { i ( T + 1 ) } ) = Y _ { i }$ , $\hat { \delta } _ { i 0 } = 0$ and $\hat { \delta } _ { i ( T + 1 ) } = 0$ . Thus,

$$
\begin{array}{l} \widehat {V} _ {A I P W} (\pi) - \widehat {V} _ {A I P W} ^ {*} (\pi) = \frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 0} ^ {T} \left(\widehat {\gamma} _ {i t} - \gamma_ {i t}\right) \varepsilon_ {i t} \\ + \frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 0} ^ {T} \gamma_ {i t} \left(\hat {\delta} _ {i (t + 1)} - \hat {\delta} _ {i t}\right) + \frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 0} ^ {T} \left(\hat {\gamma} _ {i t} - \gamma_ {i t}\right) \left(\hat {\delta} _ {i (t + 1)} - \hat {\delta} _ {i t}\right). \\ \end{array}
$$

We now bound each term separately as in the proof of Theorem 3.2. The first term is a martingale in $t$ by the same argument as used above, and so by IID sampling of units

$$
\begin{array}{l} \mathrm {V a r} \left[ \frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 0} ^ {T} (\hat {\gamma} _ {i t} - \gamma_ {i t}) \varepsilon_ {i t} \right] = \frac {1}{n} \sum_ {t = 1} ^ {T} \mathbb {E} \left[ (\hat {\gamma} _ {i t} - \gamma_ {i t}) ^ {2} \mathrm {V a r} _ {\pi} [ \varepsilon_ {i t} | S _ {i t} ] \right] \\ = \mathcal {O} \left(\frac {1}{n} \sum_ {t = 1} ^ {T} \mathbb {E} \left[ \left(\hat {\gamma} _ {i t} - \gamma_ {i t}\right) ^ {2} \right]\right), \\ \end{array}
$$

and so by (14.21) $\begin{array} { r } { \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \sum _ { t = 0 } ^ { T ^ { \prime } } \left( \widehat { \gamma } _ { i t } - \gamma _ { i t } \right) \varepsilon _ { i t } = o _ { p } \left( 1 / \sqrt { n } \right) } \end{array}$ . For the second term, we can rearrange the sum:

$$
\frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 0} ^ {T} \gamma_ {i t} \left(\hat {\delta} _ {i (t + 1)} - \hat {\delta} _ {i t}\right) = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\sum_ {t = 1} ^ {T} \left(\gamma_ {i (t - 1)} - \gamma_ {i t}\right) \hat {\delta} _ {i t} + \gamma_ {i T} \hat {\delta} _ {i (T + 1)} - \gamma_ {i 0} \hat {\delta} _ {i 0}\right),
$$

where the last two terms can be ignored because $\hat { \delta } _ { i 0 } = \hat { \delta } _ { i ( T + 1 ) } = 0$ . Given the definitions of $\gamma _ { i t }$ and $\hat { \delta } _ { i t }$ , this term can be further simplified as

$$
\ldots = \frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 1} ^ {T} \gamma_ {i (t - 1)} \left(1 - \frac {1 (\{W _ {i t} = \pi (S _ {i t}) \})}{\mathbb {P} [ W _ {i t} = \pi (S _ {i t}) | S _ {i t} ]}\right) \left(\widehat {V} _ {\pi , t} (S _ {i t}) - V _ {\pi , t} (S _ {i t})\right).
$$

The inner sum is again a martingale in $t$ , so

$$
\begin{array}{l} \mathbb {E} \left[ \left(\frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 0} ^ {T} \gamma_ {i t} \left(\hat {\delta} _ {i (t + 1)} - \hat {\delta} _ {i t}\right)\right) ^ {2} \right] \\ = \frac {1}{n} \sum_ {t = 1} ^ {T} \mathbb {E} \left[ \gamma_ {i (t - 1)} ^ {2} \frac {1 - \mathbb {P} [ W _ {i t} = \pi (S _ {i t}) | S _ {i t} ]}{\mathbb {P} [ W _ {i t} = \pi (S _ {i t}) | S _ {i t} ]} \left(\widehat {V} _ {\pi , t} (S _ {i t}) - V _ {\pi , t} (S _ {i t})\right) ^ {2} \right] \\ \leq \frac {1}{n} \sum_ {t = 1} ^ {T} \eta^ {- (2 t - 1)} \mathbb {E} \left[ \left(\widehat {V} _ {\pi , t} (S _ {i t}) - V _ {\pi , t} (S _ {i t})\right) ^ {2} \right] = o _ {p} (1 / n) \\ \end{array}
$$

by (14.21), and the term itself is again $o _ { p } ( 1 / \sqrt { n } )$ . Finally, for the 3rd term, we can swap the order of summation and apply Cauchy-Schwarz:

$$
\begin{array}{l} \frac {1}{n} \sum_ {i = 1} ^ {n} \sum_ {t = 0} ^ {T} \left(\hat {\gamma} _ {i t} - \gamma_ {i t}\right) \left(\hat {\delta} _ {i (t + 1)} - \hat {\delta} _ {i t}\right) \\ \leq \sum_ {t = 0} ^ {T} \sqrt {\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\hat {\gamma} _ {i t} - \gamma_ {i t}\right) ^ {2}} \sqrt {\frac {1}{n} \sum_ {i = 1} ^ {n} \left(\hat {\delta} _ {i (t + 1)} - \hat {\delta} _ {i t}\right) ^ {2}} \\ \leq \sum_ {t = 0} ^ {T} \sqrt {\frac {1}{n} \sum_ {i = 1} ^ {n} (\hat {\gamma} _ {i t} - \gamma_ {i t}) ^ {2}} \sqrt {\frac {2}{n} \sum_ {i = 1} ^ {n} \left(\hat {\delta} _ {i (t + 1)} ^ {2} + \hat {\delta} _ {i t} ^ {2}\right)} = o _ {P} \left(n ^ {- (\alpha_ {\gamma} + \alpha_ {V})}\right). \\ \end{array}
$$

This establishes that

$$
\widehat {V} _ {A I P W} (\pi) - \widehat {V} _ {A I P W} ^ {*} (\pi) = o _ {P} \left(1 / \sqrt {n}\right),
$$

thus concluding the proof.

![](images/489859db89e1a0129db2d11f5a35c86af3077e550318de3e0066b27cad1e58a2.jpg)

# Bibliographic notes

The approach evaluating dynamic decision rules presented here, i.e., with nested potential outcomes and with identification obtained via under sequential unconfoundedness, goes back to Robins [1986]; see Richardson and Rotnitzky [2014] for a survey of this line of work, and Hern¬¥an and Robins [2020] for a textbook treatment. One widely used algorithm from this line of work, marginal structural modeling, involves estimating the value of a parametrized policy class via inverse-propensity weighted linear regression [see Robins, 1999, for an overview]. The AIPW estimator (14.20) is discussed in Jiang and Li [2016], Thomas and Brunskill [2016] and Zhang, Tsiatis, Laber, and Davidian [2013].

Causal inference in dynamic settings is a broad topic, an exhaustive treatment of which would go beyond the scope of this book. Van der Laan and Robins [2003] and Tsiatis [2006] offer comprehensive textbook treatments, including discussions of efficiency. In particular, one consideration that‚Äôs important in many applications is the problem of censoring: Some units may leave the study before we get to observe the final outcome, and the methods discussed in this chapter need to be extended to accommodate such censoring (see Exercise 14 in Chapter 16 for one example of a result with censoring). Another interesting direction is the extension of our discussion on policy learning from Chapter 5 to the dynamic setting [Robins, 2004]. Finally, our discussion of dynamic policy evaluation is closely related to the literature on reinforcement learning; see Sutton and Barto [2018] for a textbook treatment.

# Chapter 15 Markov Decision Processes

In the previous chapter, we considered dynamic treatment rules in a general setting without modeling assumptions on how treatment effects play out over time, and introduced a set of methods for policy evaluation that only required sequential unconfoundedness for identification. The flexibility of these methods, however, comes at a cost of precision. The discussed inverse-propensity weighted method can only leverage trajectories whose assigned treatment matches the policy prescription in all $T$ time periods and involves weights whose magnitude generally scales exponentially in the time horizon $T$ ; and the augmented method faces a similar ‚Äúcurse of horizon‚Äù.

Here, we will study how judicious use of modeling assumptions can help mitigate this curse of horizon. The key insight is that, in many applications, any intervention we take is relevant for some amount of time, but its effect eventually washes out. And, if we believe that actions taken long ago are no longer relevant, then one may hope that it‚Äôs possible to meaningfully use trajectories for policy evaluation even if they deviated from the target policy at some point in the far past. The following example has this structure.

Example 20. Many ride-sharing platforms implement some kind of surge pricing mechanism, which involves temporarily raising prices in areas experiencing localized demand spikes [Castillo, Knoepfle, and Weyl, 2024]. Activating surge pricing at a given location allows the platform to rapidly shed demand at that location, and also to increase supply by encouraging idle drivers to relocate to the area with surge pricing. This helps the market rebalance itself, and avoids a situation where the platform is unable to fulfill ride requests at posted prices. In order to choose between algorithms and/or calibrate the parameters of a given algorithm, platforms often run experiments that toggle between surge algorithms in a given market.81

How should we analyze data from an experiment as described in the above example? This problem clearly involves complex treatment dynamics, and so event-study methods are not applicable. On the other hand, while surge pricing algorithms obviously have intricate short-term effects (e.g., by moving the distribution of drivers in the system), one should expect any such effects to eventually wash out (e.g., after temporarily suppressed demand has been able to re-emerge and drivers have a chance to return to their usual configuration). This suggests we should be able to develop analytic techniques that can extract meaningful insights from a long-horizon (say, multi-week) surge pricing experiment without suffering the curse-of-horizon phenomenon incurred my methods from the previous chapter.

The question, then, is how to specify a flexible and credible model that enables this type of forgetting. Here, we will do so by assuming Markovian structure. We assume that we observe a single unit over a long trajectory $t = 1$ , 2, . . . , $T$ , with a state variable $X _ { t }$ , actions $W _ { t }$ and outcomes $Y _ { t }$ . Our Markovian assumption, formalized below, is that at time $t$ , any effect of past actions on future observables is mediated by the current state $X _ { t }$ . Such Markovian structure induces forgetting‚Äîand enables consistent policy evaluation from a single trajectory‚Äîas long as the state variable $X _ { t }$ has relevant ‚Äúmixing‚Äù properites that prevent it from holding information about past treatment assignments for excessively long times.

Definition 15.1. A Markov decision process (MDP) is characterized by a series of state-transition distribution $P _ { t }$ such that, for all $t$ ,

$$
X _ {t + 1}, Y _ {t} \sim P _ {t} \left(X _ {t}, W _ {t}\right) \tag {15.1}
$$

conditionally on all information available up to time $t$ , i.e., conditionally on $X _ { 1 }$ , $W _ { 1 }$ , $Y _ { 1 }$ , $X _ { 2 }$ , . . . , $X _ { t }$ , $W _ { t }$ .

In the context of the ride-sharing example, one could define $X _ { t }$ as the current number of drivers in each neighborhood, and $W _ { t }$ as whether an experimental surge algorithm is currently active downtown. Then, our Markovian assumption would require positing that the effect of any past surge-pricing decisions is mediated by the current driver distribution. Meanwhile, a mixing assumption will essentially imply that, if we return to our default algorithm for a long enough period of time, drivers will eventually return to their usual locations and behavior patterns.

One should note that the Markovian model given above is materially more structured than the dynamic decision process model from Definition 14.1. The explicit state-evolution equation (15.1) rules out any hidden state up front, so any treatment-assignment policy $\pi$ that only depends on the current state can immediately be verified to factor according to the $g$ -formula:

$$
\begin{array}{l} \mathbb {P} _ {\pi} \left[ X _ {1}, W _ {1}, Y _ {1}, \dots , X _ {T}, W _ {T}, Y _ {T} \right] \\ = \mathbb {P} \left[ X _ {1} \right] \prod_ {t = 1} ^ {T} \mathbb {P} _ {\pi} \left[ W _ {t} \mid X _ {t} \right] P _ {t} \left(Y _ {t}, X _ {t + 1} \mid W _ {t}, X _ {t}\right). \tag {15.2} \\ \end{array}
$$

We also note that, unlike in the $g$ -formula for general dynamic decision processes as derived in Proposition 14.1, the above factorization doesn‚Äôt require a growing state-space representation at $t$ increases, i.e., it‚Äôs enough to condition on $X _ { t }$ rather than the full history observed up to time $t$ .

The main focus of this chapter will be to revisit the setting of off-policy evaluation under sequential unconfoundedness, and see how our Markovian modeling assumption can enable precision improvements relative to methods from the previous chapter. We will work under the long-horizon, $T \to \infty$ seek to estimate the long-run average value produced under a time-homogeneous target policy

$$
V (\pi) = \lim  _ {T \rightarrow \infty} \mathbb {E} _ {\pi} \left[ \frac {1}{T} \sum_ {t = 1} ^ {T} Y _ {t} \right], \quad \pi : \mathcal {X} \rightarrow \{0, 1 \}, \tag {15.3}
$$

under an assumption that this limit exists. For simplicity, we will assume throughout that we have access to collected under a sequentially unconfounded experiment with treatment generated as

$$
W _ {t} \sim e \left(X _ {t}\right), \quad e: \mathcal {X} \rightarrow (0, 1), \tag {15.4}
$$

and we will assume that $e ( x )$ is known.

# 15.1 Doubly robust estimation

The simplified form of the $g$ -formula (15.2) suggests that the MDP model should enable us to learn from long trajectories without incurring a curse of dimensionality in terms of the horizon $T$ . Our present goal is to develop an estimator for $V ( \pi )$ with this property‚Äîall while remaining robust to estimation errors in any non-parametric functions required to form the estimator. Interestingly, the form of the resulting estimator will be somewhat more complicated than in Chapter 14: In addition to conditional-response surfaces and

treatment-assignment properties, we will also require estimates of of different policies affect the stationary distribution of the state variable $X _ { t }$ . We make the following regularity assumptions throughout this section:

‚Ä¢ The MDP is time homogeneous, i.e., the state-transition distributions $P _ { t }$ from Definition 15.1 satisfy $P _ { t } = P$ for all $t$ .   
‚Ä¢ The state-variables $X _ { t }$ observed in our study, i.e., with treatment generated following (15.4), form an irreducible, aperiodic Markov chain with stationary distribution $F ^ { \prime }$ . The process is initialized from this stationary distribution, i.e., $X _ { 1 } \sim F$ .   
‚Ä¢ The $X _ { t }$ observed in our study satisfy the $\rho$ -mixing condition [see Bradley, 2005, for a survey of mixing conditions and their relationships],

$$
\sum_ {t = 1} ^ {\infty} \sup  _ {f, g \in L _ {2} (F)} | \operatorname {C o r r} \left(f \left(X _ {1}\right), g \left(X _ {t}\right)\right) | <   \infty . \tag {15.5}
$$

‚Ä¢ The state-variables $X _ { t }$ generated from the MDP under our target policy $\pi$ converge weakly to a stationary distribution $F _ { \pi }$ , and also satisfy the $\rho$ -mixing condition (15.5).   
‚Ä¢ The stationary distributions $F$ and $F _ { \pi }$ are equivalent measures (i.e., place non-zero probability mass on the same sets).

Notice that, writing ¬µœÄ(x) = EP -Yt  Xt = x, $W _ { t } = \pi ( x ) \rfloor$ , the second-tolast assumption implies that our target exists and can be expressed as $V ( \pi ) = \mathbb { E } _ { F _ { \pi } } \left[ \mu _ { \pi } ( X ) \right]$ .

Given this setup, we can write down a doubly robust estimator for $V ( \pi )$ in terms of the excess reward function

$$
Q _ {\pi} (x) = \lim  _ {T \rightarrow \infty} \mathbb {E} _ {\pi} \left[ \sum_ {t = 1} ^ {T} \left(Y _ {t} - V (\pi)\right) \mid X _ {1} = x \right], \tag {15.6}
$$

which measures the size of the expected (non-scaled) excess reward under $\pi$ from starting from a specific state $x$ rather than from a random draw from $F _ { \pi }$ , and the stationary distribution ratio

$$
\omega_ {\pi} (x) = d F _ {\pi} (x) / d F (x). \tag {15.7}
$$

Given estimates of these two quantities, and assuming that $e ( \cdot )$ is known (as it would be in a sequentially randomized experiment), we will show that the

following estimator is consistent for $V ( \pi )$ and (strongly) doubly robust in the sense discussed in Chapter 3:

$$
\widehat {V} _ {D R} (\pi) = \frac {\sum_ {t = 1} ^ {T - 1} \left(Y _ {t} + \widehat {Q} _ {\pi} \left(X _ {t + 1}\right) - \widehat {Q} _ {\pi} \left(X _ {t}\right)\right) \hat {\omega} _ {\pi} \left(X _ {t}\right) \frac {\mathbf {1} \left(\left\{W _ {t} = \pi \left(X _ {t}\right) \right\}\right)}{e _ {\pi} \left(X _ {t}\right)}}{\sum_ {t = 1} ^ {T - 1} \hat {\omega} _ {\pi} \left(X _ {t}\right) \frac {\mathbf {1} \left(\left\{W _ {t} = \pi \left(X _ {t}\right) \right\}\right)}{e _ {\pi} \left(X _ {t}\right)}}, \tag {15.8}
$$

using the notational short-hand $e _ { \pi } ( x ) = \pi ( x ) e ( x ) + ( 1 - \pi ( x ) ) ( 1 - e ( x ) )$ to denote the conditional probability of following $\pi ( \cdot )$ .

The remainder of this section will be devoted to proving this result. For simplicity, we will not rely on cross-fitting, and will instead assume that the estimates $\hat { \omega } _ { \pi } ( \cdot )$ and $\widehat { Q } _ { \pi } ( \cdot )$ have been obtained on a separate training set; however, we do note that given appropriate mixing assumptions a cross-fitting argument across long, consecutive segments of the time series $( X _ { t } , Y _ { t } , W _ { t } )$ would also be possible. Finally, as in the rest of the book, we will defer to the statistical learning literature for methods on estimating the functions $\hat { \omega } _ { \pi } ( \cdot )$ and $\widehat { Q } _ { \pi } ( \cdot )$ ; see Liao et al. [2022] and Uehara, Huang, and Jiang [2020] for recent proposals.

We start establishing two results motivating the form of the estimator (15.8). Note that these two results together already imply weak double robustness of the estimator.

Lemma 15.1. Under our stated assumptions and with Var $F _ { \pi } ^ { \prime }$ $[ \mu _ { \pi } ( X ) ] < \infty$ , the excess reward function $Q _ { \pi } ( X _ { t } )$ is absolutely integrable under $F _ { \pi }$ , almost surely finite under $X _ { t } \sim F$ , and satisfies the Bellman conditions

$$
\begin{array}{l} \mathbb {E} _ {\pi} \left[ Y _ {t} + Q _ {\pi} (X _ {t + 1}) \mid X _ {t} \right] - Q _ {\pi} (X _ {t}) = V (\pi), \\ \mathbb {E} \left[ \frac {1 \left(\left\{W _ {t} = \pi \left(X _ {t}\right) \right\}\right)}{e _ {\pi} \left(X _ {t}\right)} \left(Y _ {t} + Q _ {\pi} \left(X _ {t + 1}\right)\right) \mid X _ {t} \right] - Q _ {\pi} \left(X _ {t}\right) = V (\pi), \tag {15.9} \\ \end{array}
$$

almost surely.

Proof. Given time-homogeneity of our system, an application of the chain rule to (15.6) implies that

$$
\mathbb {E} _ {\boldsymbol \pi} \left[ Q _ {\boldsymbol \pi} (X _ {t + 1}) \mid X _ {t} = x \right] = \lim _ {T \to \infty} \mathbb {E} _ {\boldsymbol \pi} \left[ \sum_ {t = 2} ^ {T} \left(Y _ {t} - V (\boldsymbol \pi)\right) \mid X _ {1} = x \right].
$$

The first Bellman equation then follows immediately from basic algebraic manipulations‚Äîprovided we can show that $Q _ { \pi } ( X _ { t } )$ is almost surely finite under $X _ { t } \sim F$ . In order to verify this, we will show below that

$$
\sum_ {t = 1} ^ {\infty} \mathbb {E} _ {X _ {1} \sim F _ {\pi}} \left[ \left| \mathbb {E} _ {\pi} [ Y _ {t} - V (\pi) \mid X _ {1} ] \right| \right] <   \infty . \tag {15.10}
$$

It then follows from Fubini‚Äôs theorem that $Q _ { \pi } ( X _ { t } )$ is absolutely integrable under $F _ { \pi }$ , $\mathbb { E } _ { X _ { 1 } \sim F _ { \pi } } \left[ | Q _ { \pi } ( X _ { 1 } ) | \right] < \infty$ . This also implies that $Q _ { \pi } ( X _ { t } )$ is almost surely finite under $X _ { t } \sim F$ since $F$ and $F _ { \pi }$ are equivalent measures. Meanwhile, the second Bellman equation follows from the first by the standard IPW argument under sequential unconfoundedness as used in the proof of Theorem 14.3.

We now turn to verifying (15.10) under our $\rho$ -mixing assumption. Write

$$
\rho_ {\pi} ^ {t} = \sup _ {f, g \in L _ {2} (F _ {\pi})} | \mathrm {C o r r} _ {\pi} (f (X _ {1}), g (X _ {t})) |,
$$

and recall that our assumption is that these $\textstyle \sum _ { t = 1 } ^ { \infty } \rho _ { \pi } ^ { t } < \infty$ . Now, by applying Jensen‚Äôs inequality

$$
\mathbb {E} _ {\boldsymbol \pi} \left[ \left| \mathbb {E} _ {\boldsymbol \pi} \left[ Y _ {t} - V (\boldsymbol \pi) \mid X _ {1} \right] \right| \right] \leq \mathbb {E} _ {\boldsymbol \pi} \left[ \mathbb {E} _ {\boldsymbol \pi} \left[ Y _ {t} - V (\boldsymbol \pi) \mid X _ {1} \right] ^ {2} \right] ^ {\frac 12} = \mathrm {V a r} _ {\boldsymbol \pi} \left[ \mathbb {E} _ {\boldsymbol \pi} \left[ Y _ {t} \mid X _ {1} \right] \right] ^ {\frac 12},
$$

where we have left the fact that $X _ { 1 } \sim F _ { \pi }$ implicit. Furthermore,

$$
\begin{array}{l} \operatorname {V a r} _ {\pi} \left[ \mathbb {E} _ {\pi} \left[ Y _ {t} \mid X _ {1} \right] \right] = \operatorname {C o v} _ {\pi} \left[ \mu_ {\pi} (X _ {t}), \mathbb {E} _ {\pi} \left[ Y _ {t} \mid X _ {1} \right] \right] \\ = \operatorname {C o r r} _ {\pi} \left(\mu_ {\pi} \left(X _ {t}\right), \mathbb {E} _ {\pi} \left[ Y _ {t} \mid X _ {1} \right]\right) \\ \times \operatorname {V a r} _ {\pi} \left[ \mu_ {\pi} \left(X _ {t}\right) \right] ^ {1 / 2} \operatorname {V a r} _ {\pi} \left[ \mathbb {E} _ {\pi} \left[ Y _ {t} \mid X _ {1} \right] \right] ^ {1 / 2}, \\ \end{array}
$$

and so

$$
\mathrm {V a r} _ {\pi} \left[ \mathbb {E} _ {\pi} \left[ Y _ {t} \mid X _ {1} \right] \right] ^ {1 / 2} \leq \rho_ {\pi} ^ {t} \mathrm {V a r} _ {F _ {\pi}} \left[ \mu_ {\pi} (X) \right] ^ {1 / 2}.
$$

Putting everything together, we get

$$
\sum_ {t = 1} ^ {\infty} \mathbb {E} _ {X _ {1} \sim F _ {\pi}} \left[ \left| \mathbb {E} _ {\pi} \left[ Y _ {t} - V (\pi) \mid X _ {1} \right] \right| \right] \leq \mathrm {V a r} _ {F _ {\pi}} \left[ \mu_ {\pi} (X) \right] ^ {1 / 2} \sum_ {t = 1} ^ {\infty} \rho_ {\pi} ^ {t} <   \infty ,
$$

as claimed.

Lemma 15.2. Under our stated assumptions, for any time t and any measurable function $h ( X )$ ,

$$
\mathbb {E} \left[ \omega_ {\pi} \left(X _ {t}\right) h \left(X _ {t + 1}\right) \frac {1 \left(\left\{W _ {t} = \pi \left(X _ {t}\right) \right\}\right)}{e _ {\pi} \left(X _ {t}\right)} \right] = \mathbb {E} \left[ \omega_ {\pi} \left(X _ {t}\right) h \left(X _ {t}\right) \right], \tag {15.11}
$$

provided all stated expectations exist and are finite.

Proof. Starting with the right-hand side expression, we can invoke stationarity as well as a change-of-measure argument to check that

$$
\mathbb {E} \left[ \omega_ {\pi} (X _ {t}) h (X _ {t}) \right] = \mathbb {E} _ {F} \left[ \omega_ {\pi} (X) h (X) \right] = \mathbb {E} _ {F _ {\pi}} [ h (X) ].
$$

Meanwhile, for the left-hand-side, the standard IPW argument under sequential unconfoundedness implies that

$$
\mathbb {E} \left[ h (X _ {t + 1}) \frac {1 (\{W _ {t} = \pi (X _ {t}) \})}{e _ {\pi} (X _ {t})} | X _ {t} \right] = \mathbb {E} _ {\pi} [ h (X _ {t + 1}) | X _ {t} ],
$$

and so an application of the chain rule yields

$$
\begin{array}{l} \mathbb {E} \left[ \omega_ {\pi} (X _ {t}) h (X _ {t + 1}) \frac {1 (\{W _ {t} = \pi (X _ {t}) \})}{e _ {\pi} (X _ {t})} \right] \\ = \mathbb {E} \left[ \omega_ {\pi} (X _ {t}) \mathbb {E} \left[ h (X _ {t + 1}) \frac {1 (\{W _ {t} = \pi (X _ {t}) \})}{e _ {\pi} (X _ {t})} | X _ {t} \right] \right] \\ = \mathbb {E} \left[ \omega_ {\pi} \left(X _ {t}\right) \mathbb {E} _ {\pi} \left[ h \left(X _ {t + 1}\right) \mid X _ {t} \right] \right] \\ = \mathbb {E} _ {X _ {t} \sim F} \left[ \omega_ {\pi} \left(X _ {t}\right) \mathbb {E} _ {\pi} \left[ h \left(X _ {t + 1}\right) \mid X _ {t} \right] \right] \\ = \mathbb {E} _ {X _ {t} \sim F _ {\pi}} \left[ \mathbb {E} _ {\pi} \left[ h \left(X _ {t + 1}\right) \mid X _ {t} \right] \right] = \mathbb {E} _ {F _ {\pi}} \left[ h (X) \right], \\ \end{array}
$$

where the 3rd and 5th equalities leveraged stationarity.

![](images/0337abba61f156ac5c83447e5e70effef373f19b1345d75a6fcb6bd041da37a0.jpg)

Theorem 15.3. Under our stated assumptions, suppose furthermore that we estimate the nuisance components in (15.8) on independent training data such that, for all $t = 1$ , . . . , $T$ , 8 2

$$
\mathbb {E} _ {F} \left[ \left(\widehat {Q} _ {\pi} (X) - Q _ {\pi} (X)\right) ^ {2} \right] = o _ {P} \left(T ^ {- 2 \alpha_ {Q}}\right), \tag {15.12}
$$

$$
\mathbb {E} _ {F} \left[ (\hat {\omega} _ {\pi} (X) - \omega_ {\pi} (X)) ^ {2} \right] = o _ {P} \left(T ^ {- 2 \alpha_ {\omega}}\right)
$$

for constants $\alpha _ { Q }$ , $\alpha _ { \omega } \geq 0$ with $\alpha _ { \omega } + \alpha _ { Q } \ge 1 / 2$ . Then,

$$
\begin{array}{l} \sqrt {T} \left(\widehat {V} _ {D R} (\pi) - V (\pi)\right) \Rightarrow \mathcal {N} (0, \Sigma) \\ \Sigma = \mathbb {E} _ {F} \left[ \frac {\omega_ {\pi} ^ {2} (X _ {1})}{e _ {\pi} (X _ {1})} \mathbb {E} _ {\pi} \left[ \left(Y _ {1} + Q _ {\pi} (X _ {2}) - Q _ {\pi} (X _ {1}) - V (\pi)\right) ^ {2} \mid X _ {1} \right] \right], \tag {15.13} \\ \end{array}
$$

provided that $\Sigma$ is finite.

Proof. Our estimator has a self-normalized form, and so its errors can be expressed as

$$
\widehat {V} _ {D R} (\pi) - V (\pi) = \frac {\sum_ {t = 1} ^ {T - 1} \left(Y _ {t} + \widehat {Q} _ {\pi} (X _ {t + 1}) - \widehat {Q} _ {\pi} (X _ {t}) - V (\pi)\right) \widehat {\omega} _ {\pi} (X _ {t}) \frac {\mathbf {1} (\{W _ {t} = \pi (X _ {t}) \})}{e _ {\pi} (X _ {t})}}{\sum_ {t = 1} ^ {T - 1} \widehat {\omega} _ {\pi} (X _ {t}) \frac {\mathbf {1} (\{W _ {t} = \pi (X _ {t}) \})}{e _ {\pi} (X _ {t})}}.
$$

We start by considering the denominator. By stationarity,

$$
\begin{array}{l} \mathbb {E} \left[ \omega_ {\pi} (X _ {t}) \frac {\mathbf {1} (\{W _ {t} = \pi (X _ {t}) \})}{e _ {\pi} (X _ {t})} \right] = \mathbb {E} \left[ \omega_ {\pi} (X _ {t}) \mathbb {E} \left[ \frac {\mathbf {1} (\{W _ {t} = \pi (X _ {t}) \})}{e _ {\pi} (X _ {t})} \mid X _ {t} \right] \right] \\ = \mathbb {E} \left[ \omega_ {\pi} \left(X _ {t}\right) \right] = \mathbb {E} _ {F} \left[ \omega_ {\pi} (X) \right] = 1, \\ \end{array}
$$

and so we can apply the ergodic theorem [e.g., Durrett, 2019, Chapter 6.2] to verify that

$$
\frac {1}{T - 1} \sum_ {t = 1} ^ {T - 1} \omega_ {\pi} \left(X _ {t}\right) \frac {\mathbf {1} \left(\left\{W _ {t} = \pi \left(X _ {t}\right)\right\}\right)}{e _ {\pi} \left(X _ {t}\right)} \rightarrow_ {p} 1. \tag {15.14}
$$

Furthermore, we see that

$$
\begin{array}{l} \mathbb {E} \left[ \left| \frac {1}{T - 1} \sum_ {t = 1} ^ {T - 1} \left(\hat {\omega} _ {\pi} (X _ {t}) - \omega_ {\pi} (X _ {t})\right) \frac {\mathbf {1} \left(\left\{W _ {t} = \pi (X _ {t}) \right\}\right)}{e _ {\pi} (X _ {t})} \right| \right] \\ \leq \frac {1}{\eta^ {2}} \sqrt {\mathbb {E} \left[ \frac {1}{T - 1} \sum_ {t = 1} ^ {T - 1} \left(\hat {\omega} _ {\pi} (X _ {t}) - \omega_ {\pi} (X _ {t})\right) ^ {2} \right]} \\ = \frac {1}{\eta^ {2}} \sqrt {\mathbb {E} _ {F} \left[ (\hat {\omega} _ {\pi} (X) - \omega_ {\pi} (X)) ^ {2} \right]} = o _ {p} (1) \\ \end{array}
$$

by respectively invoking Cauchy-Schwarz, overlap, stationarity, and $L _ { 2 }$ - consistency of $\hat { \omega } ( \cdot )$ , thus implying that (15.14) also holds for $\omega ( \cdot )$ replaced with $\hat { \omega } ( \cdot )$ .

Meanwhile, the numerator can be decomposed as $A + B + C + D$ with

$$
A = \sum_ {t = 1} ^ {T - 1} \left(Y _ {t} + Q _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t}) - V (\pi)\right) \omega_ {\pi} (X _ {t}) \frac {\mathbf {1} \left(\left\{W _ {t} = \pi (X _ {t}) \right\}\right)}{e _ {\pi} (X _ {t})},
$$

$$
B = \sum_ {t = 1} ^ {T - 1} \left(Y _ {t} + Q _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t}) - V (\pi)\right) \left(\hat {\omega} _ {\pi} (X _ {t}) - \omega_ {\pi} (X _ {t})\right) \frac {\mathbf {1} \left(\{W _ {t} = \pi (X _ {t}) \}\right)}{e _ {\pi} (X _ {t})},
$$

$$
C = \sum_ {t = 1} ^ {T - 1} \left(\widehat {Q} _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t + 1}) - \left(\widehat {Q} _ {\pi} (X _ {t}) - Q _ {\pi} (X _ {t})\right)\right) \omega_ {\pi} (X _ {t}) \frac {\mathbf {1} \left(\{W _ {t} = \pi (X _ {t}) \}\right)}{e _ {\pi} (X _ {t})},
$$

$$
\begin{array}{l} D = \sum_ {t = 1} ^ {T - 1} \left(\widehat {Q} _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t + 1}) - \left(\widehat {Q} _ {\pi} (X _ {t}) - Q _ {\pi} (X _ {t})\right)\right) \\ \times \left(\hat {\omega} _ {\pi} (X _ {t}) - \omega_ {\pi} (X _ {t})\right) \frac {\mathbf {1} \left(\left\{W _ {t} = \pi (X _ {t}) \right\}\right)}{e _ {\pi} (X _ {t})}. \\ \end{array}
$$

We will show below that

$$
A / \sqrt {T} \Rightarrow \mathcal {N} (0, \Sigma), \quad | B |, | C |, | D | = o _ {P} (\sqrt {T}). \tag {15.15}
$$

Thus, given what was shown about the denominator above, we can establish (15.13) via Slutsky‚Äôs lemma.

Now, starting with the (dominant) term $A$ , we note that the second Bellman equation in Lemma 15.1 immediately implies that

$$
\mathbb {E} \left[ (Y _ {t} + Q _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t}) - V (\pi)) \omega_ {\pi} (X _ {t}) \frac {\mathbf {1} (\{W _ {t} = \pi (X _ {t}) \})}{e _ {\pi} (X _ {t})} \mid X _ {t} \right] = 0
$$

almost surely for all $t$ , and so the term $A$ is mean zero. Furthermore, by our assumed Markov property, the summands forming $A$ are a martingale difference sequence, because conditioning on $X _ { t }$ is equivalent to conditioning on the full past. Given this set up, we can study large-sample behavior of $A$ via the martingale central limit theorem. A key ingredient in doing so is to study the conditional variance of the individual martingale difference terms. We can again apply the ergodic theorem to verify that

$$
\frac {1}{T - 1} \sum_ {t = 1} ^ {T - 1} \mathrm {V a r} \left[ \Delta_ {t, t + 1} \mid X _ {t} \right]\rightarrow_ {p} \mathbb {E} _ {X _ {1} \sim F} \left[ \mathrm {V a r} \left[ \Delta_ {1, 2} \mid X _ {1} \right]\right],
$$

$$
\Delta_ {t, t + 1} = (Y _ {t} + Q _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t}) - V (\pi)) \omega_ {\pi} (X _ {t}) \frac {\mathbf {1} (\{W _ {t} = \pi (X _ {t}) \})}{e _ {\pi} (X _ {t})},
$$

provided the right-hand side limit is finite. Furthermore,

$$
\begin{array}{l} \mathbb {E} _ {F} \left[ \operatorname {V a r} \left[ \Delta_ {1, 2} \mid X _ {1} \right] \right] = \mathbb {E} _ {F} \left[ \mathbb {E} \left[ \Delta_ {1, 2} ^ {2} \mid X _ {1} \right] \right] \\ = \mathbb {E} _ {F} \left[ \mathbb {E} \left[ 1 \left(\left\{W _ {1} = \pi \left(X _ {1}\right) \right\}\right) \Delta_ {1, 2} ^ {2} \mid X _ {1} \right] \right] \\ = \mathbb {E} _ {F} \left[ e _ {\pi} \left(X _ {1}\right) \mathbb {E} _ {\pi} \left[ \Delta_ {1, 2} ^ {2} \mid X _ {1} \right] \right] = \Sigma , \\ \end{array}
$$

where the 2nd equality is true because $\Delta _ { 1 , 2 } ^ { 2 } = 0$ whenever $W _ { 1 } \ne \pi ( X _ { 1 } )$ , the 3rd equality is true by sequential unconfoundedness, and the 4th follows by direct algebraic manipulation. Now, we have assumed that $\Sigma < \infty$ in the theorem statement; thus the ergodic theorem in fact applies. The fact that $A / \sqrt { T } \Rightarrow \mathcal { N } ( 0 , \Sigma )$ then follows from the martingale central limit theorem [e.g., Durrett, 2019, Theorem 8.2.8].

Next, moving to the lower-order terms, Lemma 15.1 implies that

$$
\begin{array}{l} \mathbb {E} _ {0} \left[ \left(Y _ {t} + Q _ {\pi} \left(X _ {t + 1}\right) - Q _ {\pi} \left(X _ {t}\right) - V (\pi)\right) \right. \\ \times \left(\hat {\omega} _ {\pi} (X _ {t}) - \omega_ {\pi} (X _ {t})\right) \frac {\mathbf {1} \left(\{W _ {t} = \pi (X _ {t}) \}\right)}{e _ {\pi} (X _ {t})} \mid X _ {t} \biggr ] = 0, \\ \end{array}
$$

and so the term $B$ is mean-zero. Furthermore, it is again a martingale, and so its variance is equal to the sum of the expected variance of each martingale

difference term; thus, by stationarity,

$$
\begin{array}{l} \operatorname {V a r} [ B ] = (T - 1) \mathbb {E} _ {F} \left[ \operatorname {V a r} _ {0} \left[ \left(Y _ {1} + Q _ {\pi} \left(X _ {2}\right) - Q _ {\pi} \left(X _ {1}\right) - V (\pi)\right) \right. \right. \\ \left. \times \left(\hat {\omega} _ {\pi} \left(X _ {1}\right) - \omega_ {\pi} \left(X _ {1}\right)\right) \frac {\mathbf {1} \left(\left\{W _ {1} = \pi \left(X _ {1}\right) \right\}\right)}{e _ {\pi} \left(X _ {1}\right)} \mid X _ {1} \right] \Bigg ] \\ = (T - 1) \mathbb {E} _ {F} \left[ \frac {(\hat {\omega} _ {\pi} (X _ {1}) - \omega_ {\pi} (X _ {1})) ^ {2}}{e _ {\pi} (X _ {1})} \operatorname {V a r} _ {\pi} \left[ Y _ {1} + Q _ {\pi} (X _ {2}) \mid X _ {1} \right] \right] \\ = \mathcal {O} \left(\left(T - 1\right) \mathbb {E} _ {F} \left[ \left(\hat {\omega} _ {\pi} \left(X _ {1}\right) - \omega_ {\pi} \left(X _ {1}\right)\right) ^ {2} \right]\right) = o _ {p} (T), \\ \end{array}
$$

and so $B = o _ { p } ( { \sqrt { T } } )$

Meanwhile, we can verify that the term $C$ is mean zero using Lemma 15.2:

$$
\begin{array}{l} \mathbb {E} \left[ \left(\widehat {Q} _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t + 1}) - \left(\widehat {Q} _ {\pi} (X _ {t}) - Q _ {\pi} (X _ {t})\right)\right) \omega_ {\pi} (X _ {t}) \frac {\mathbf {1} \left(\left\{W _ {t} = \pi (X _ {t}) \right\}\right)}{e _ {\pi} (X _ {t})} \right] \\ = \mathbb {E} \left[ \left(\widehat {Q} _ {\pi} \left(X _ {t + 1}\right) - Q _ {\pi} \left(X _ {t + 1}\right)\right) \omega_ {\pi} \left(X _ {t}\right) \frac {\mathbf {1} \left(\left\{W _ {t} = \pi \left(X _ {t}\right) \right\}\right)}{e _ {\pi} \left(X _ {t}\right)} \right] \\ - \mathbb {E} \left[ \left(\widehat {Q} _ {\pi} (X _ {t}) - Q _ {\pi} (X _ {t})\right) \omega_ {\pi} (X _ {t}) \right] = 0. \\ \end{array}
$$

To calculate the variance of $C$ , it is helpful to split it into two parts:

$$
\begin{array}{l} C _ {1} = \sum_ {t = 1} ^ {T - 1} \left(\mathbb {E} \left[ \widehat {Q} _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t + 1}) \mid X _ {t}, W _ {t} \right] - \left(\widehat {Q} _ {\pi} (X _ {t}) - Q _ {\pi} (X _ {t})\right)\right) \\ \times \omega_ {\pi} (X _ {t}) \frac {\mathbf {1} \left(\left\{W _ {t} = \pi (X _ {t}) \right\}\right)}{e _ {\pi} (X _ {t})}, \\ \end{array}
$$

$$
\begin{array}{l} C _ {2} = \sum_ {t = 1} ^ {T - 1} \left(\widehat {Q} _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t + 1}) - \mathbb {E} \left[ \widehat {Q} _ {\pi} (X _ {t + 1}) - Q _ {\pi} (X _ {t + 1}) \mid X _ {t}, W _ {t} \right]\right) \\ \times \omega_ {\pi} (X _ {t}) \frac {\mathbf {1} \left(\{W _ {t} = \pi (X _ {t}) \}\right)}{e _ {\pi} (X _ {t})}. \\ \end{array}
$$

The latter term, $C _ { 2 }$ is a martingale and so can be can be shown to be $o _ { p } ( \sqrt { T } )$ by a similar argument as used with $B$ . The term $C _ { 1 }$ , however, is not a martingale,

and so cross-terms matter. By stationarity,

$$
\begin{array}{l} \mathrm {V a r} \left[ C _ {1} \right] = (T - 1) \mathrm {V a r} _ {F} \bigg [ \omega_ {\pi} (X _ {1}) \frac {\mathbf {1} \left(\left\{W _ {1} = \pi (X _ {1}) \right\}\right)}{e _ {\pi} (X _ {1})} \\ \left. \times \left(\mathbb {E} \left[ \widehat {Q} _ {\pi} (X _ {2}) - Q _ {\pi} (X _ {2}) \mid X _ {1}, W _ {1} \right] - \left(\widehat {Q} _ {\pi} (X _ {1}) - Q _ {\pi} (X _ {1})\right)\right) \right] \\ + (T - 2) \mathrm {C o v} _ {F} \left[ \omega_ {\pi} (X _ {1}) \frac {\mathbf {1} \left(\left\{W _ {1} = \pi (X _ {1}) \right\}\right)}{e _ {\pi} (X _ {1})} \right. \\ \times \left(\mathbb {E} \left[ \widehat {Q} _ {\pi} (X _ {2}) - Q _ {\pi} (X _ {2}) \mid X _ {1}, W _ {1} \right] - \left(\widehat {Q} _ {\pi} (X _ {1}) - Q _ {\pi} (X _ {1})\right)\right), \\ \omega_ {\pi} (X _ {2}) \frac {\mathbf {1} (\{W _ {2} = \pi (X _ {2}) \})}{e _ {\pi} (X _ {2})} \\ \times \left(\mathbb {E} \left[ \widehat {Q} _ {\pi} (X _ {3}) - Q _ {\pi} (X _ {3}) \mid X _ {2}, W _ {2} \right] - \left(\widehat {Q} _ {\pi} (X _ {2}) - Q _ {\pi} (X _ {2})\right)\right) \Biggr ] \\ + (T - 3) \dots \\ \end{array}
$$

Then, given our $\rho$ -mixing assumption, we can upper-bound this term as

$$
\begin{array}{l} \operatorname {V a r} \left[ C _ {1} \right] \leq (T - 1) \sum_ {t = 1} ^ {\infty} \rho_ {t} \operatorname {V a r} _ {F} \left[ \omega_ {\pi} (X _ {1}) \frac {\mathbf {1} \left(\left\{W _ {1} = \pi (X _ {1}) \right\}\right)}{e _ {\pi} (X _ {1})} \right. \\ \left. \times \left(\mathbb {E} \left[ \widehat {Q} _ {\pi} (X _ {2}) - Q _ {\pi} (X _ {2}) \mid X _ {1}, W _ {1} \right] - \left(\widehat {Q} _ {\pi} (X _ {1}) - Q _ {\pi} (X _ {1})\right)\right) \right], \\ \end{array}
$$

recalling that we‚Äôve assumed $\textstyle \sum _ { t = 1 } ^ { \infty } \rho _ { t } < \infty$ . Given our $L _ { \mathrm { 2 } }$ -consistency assumption on $\widehat { Q }$ and boundedness assumptions on $\omega ( X _ { t } )$ and $1 / e _ { \pi } ( X _ { t } )$ , this implies that $C _ { 1 } = o _ { p } ( \sqrt { T } )$ .

Finally, as already done in many proofs term $D$ can be bounded via Cauchy-Schwarz,

$$
\begin{array}{l} | D | \leq \frac {1}{\eta} \sqrt {\sum_ {t = 1} ^ {T - 1} \left(\widehat {Q} _ {\pi} \left(X _ {t + 1}\right) - Q _ {\pi} \left(X _ {t + 1}\right) - \left(\widehat {Q} _ {\pi} \left(X _ {t}\right) - Q _ {\pi} \left(X _ {t}\right)\right)\right) ^ {2}} \\ \times \sqrt {\sum_ {t = 1} ^ {T - 1} \left(\hat {\omega} _ {\pi} (X _ {t}) - \omega_ {\pi} (X _ {t})\right) ^ {2}} \\ = \mathcal {O} _ {P} \left(\left(T - 1\right) \mathbb {E} _ {F} \left[ \left(\widehat {Q} _ {\pi} (X) - Q _ {\pi} (X)\right) ^ {2} \right] ^ {\frac {1}{2}} \mathbb {E} _ {F} \left[ \left(\widehat {\omega} _ {\pi} (X) - \omega_ {\pi} (X)\right) ^ {2} \right] ^ {\frac {1}{2}}\right) \\ = o _ {p} (\sqrt {T}), \\ \end{array}
$$

where the second line follows by stationarity along with Markov‚Äôs inequality and the last line follows by (15.12).

# 15.2 Switchback experiments

We showed above how‚Äîat the expense of some mathematical complexity‚Äîit is possible to estimate policy values in Markov decision processes using data collected under a generic sequentially randomized design. In practice, however, it may be easier to change the data-collection procedure to more directly accommodate the problem structure, thus enabling more straight-forward analyses.

One such design is the switchback experiment. In principle, any experiment that measures treatment effects by repeatedly toggling treatment on-and-off at the system level can be referred to a switchback. In systems with temporal carryovers, however, switchbacks are typically understood to be experiments that set treatment to a given level, wait for the system to re-equilibriate, and only then toggle it again. When running switchback experiments, the goal is typically to estimate the total treatment effect,

$$
\tau_ {T O T} = V (1) - V (0) \tag {15.16}
$$

i.e., the long-run average difference between the always-treat and never-treat policies.

There are a variety of switchback designs considered in practice. The simplest (and most widely used) switchback design has a fixed treatment window of length $L$ , and toggles treatment after every $L$ time periods [Bojinov, Simchi-Levi, and Zhao, 2023]. Here, we will consider an alternative ‚Äúmemoryless‚Äù switchback design, as it allows for a particularly simple analysis in the context of the Markovian model used in this chapter. See Hu and Wager [2022] for a discussion of standard (i.e., fixed-length) switchbacks under the Markovian model, as well as results in a time-varying setting (i.e., with the $P _ { t }$ in Definition 15.1 changing over time).

Definition 15.2. A memoryless switchback with switch rate $0 < \lambda < 1$ is a design that sequentially assigns treatment $W _ { t } \in \{ 0 , 1 \}$ for $t = 1$ , 2, . . . such that $W _ { 1 } \sim$ Bernoulli(0.5) and, for $t \geq 1$ ,

$$
W _ {t + 1} \sim \operatorname {B e r n o u l l i} \left((1 - \lambda) W _ {t} + \lambda (1 - W _ {t})\right). \tag {15.17}
$$

The core fact about switchback experiments is that, if the typical amount of time between treatment switches is long enough (i.e., in the case of memoryless switchbacks, if the switch rate $\lambda$ is low enough), then the raw difference-inmeans estimator

$$
\hat {\tau} _ {S B} = \frac {1}{| W _ {t} = 1 |} \sum_ {\{t: W _ {t} = 1 \}} Y _ {t} - \frac {1}{| W _ {t} = 0 |} \sum_ {\{t: W _ {t} = 0 \}} Y _ {t} \tag {15.18}
$$

is consistent for the total effect. In practice, the behavior of this estimator can be improved by removing burn-in samples right after a switch and other algorithmic modifications [Bojinov, Simchi-Levi, and Zhao, 2023, Hu and Wager, 2022]; here, however, we will focus on the basic estimator (15.18).

To study switchback estimators, we will work in the ‚Äútabular‚Äù setting where the covariates $X _ { t } \in \mathcal { X }$ take values in a discrete space with $| { \mathcal { X } } | = k$ , meaning that we can write the full treatment-dependent state-transition matrices as $P ^ { w } \in \mathbb { R } ^ { k \times k }$ where $P _ { x x ^ { \prime } } ^ { w } = \mathbb { P } \left[ X _ { t + 1 } = x \middle | X _ { t } = x ^ { \prime } \right.$ , $W _ { t } = w ]$ . Our analysis also applies directly to non-tabular settings; however, the discrete setting considerably simplifies notation.

We will further assume geometric mixing whereby the state-transition operator is a contraction:

$$
\left\| P ^ {w} \left(\nu^ {\prime} - \nu\right) \right\| _ {1} \leq e ^ {- 1 / t _ {0}} \left\| \nu^ {\prime} - \nu \right\| _ {1} \tag {15.19}
$$

for any measures $\nu$ , $\nu ^ { \prime }$ over $\mathcal { X }$ , i.e., for vectors over [0, 1]k with $\textstyle \sum _ { x } \nu _ { x } = 1$ and likewise for $\nu ^ { \prime }$ ; this condition immediately implies existence of a unique stationary distribution and geometric convergence to the stationary distribution with a mixing time $t _ { 0 }$ .

Theorem 15.4. Consider a time-homogenous Markov decision process satisfying (15.19), and suppose furthermore that $| Y _ { t } | \le M$ almost surely. Then, writing $\tau _ { S B } ( \lambda )$ for the long-run average of $\hat { \tau } _ { S B }$ under a Markovian switchback with switcha rate $\lambda$ , we have

$$
\left| \tau_ {S B} (\lambda) - \tau_ {T O T} \right| \leq 4 M \lambda (1 + t _ {0}). \tag {15.20}
$$

Furthermore, if we run a sequence of memoryless switchbacks with horizon $T$ and switch rate $\lambda _ { T }$ , then $\hat { \tau } _ { S B }  _ { p } \tau _ { T O T }$ whenever $\lambda _ { T } \to 0$ and $T \lambda _ { T } \to \infty$ .

Proof. First, as a preliminary, we note that the mixing condition (15.19) implies that there are stationary distributions $\nu ^ { 0 }$ and $\nu ^ { 1 }$ that can be characterized as the unique solutions to $P ^ { w } \nu ^ { w } = \nu ^ { w }$ over the $k$ -dimensional simplex; and that the long-run average value of the always- and never-treat policies are $\begin{array} { r } { V ( w ) = \sum _ { x } \nu _ { x } ^ { w } \mathbb { E } \left[ Y _ { t } \vert X _ { t } = x \right. } \end{array}$ , $W _ { t } = w \rfloor$ .

Now, moving to the switchback: Our assumptions that $( X _ { t } , Y _ { t } )$ are from a Markov decision process while $W _ { t }$ is randomized in a memoryless way as given in (15.17) imply that $( X _ { t } , Y _ { t } , W _ { t } )$ together form a Markov chain. Writing $\nu ^ { w } ( \lambda )$ for the distribution of $X _ { t }$ conditionally on $W _ { t } = w$ under stationarity, the fixedpoint condition underlying the stationary joint distribution of (Xt, Wt) is

$$
\binom {\nu^ {0} (\lambda)} {\nu^ {1} (\lambda)} = \left( \begin{array}{c c} (1 - \lambda) P ^ {0} & \lambda P ^ {1} \\ \lambda P ^ {0} & (1 - \lambda) P ^ {1} \end{array} \right) \binom {\nu^ {0} (\lambda)} {\nu^ {1} (\lambda)}, \tag {15.21}
$$

and the long-run average expectation of the difference-in-means estimator is

$$
\begin{array}{l} \tau_ {S B} (\lambda) = \sum_ {x \in \mathcal {X}} \nu_ {x} ^ {1} (\lambda) \mathbb {E} \left[ Y _ {t} \mid X _ {t} = x, W _ {t} = 1 \right] \tag {15.22} \\ - \sum_ {x \in \mathcal {X}} \nu_ {x} ^ {0} (\lambda) \mathbb {E} \left[ Y _ {t} \mid X _ {t} = x, W _ {t} = 0 \right], \\ \end{array}
$$

and so by boundedness we immediately see that

$$
\left| \tau_ {S B} (\lambda) - \tau_ {T O T} \right| \leq M \left(\left\| \nu^ {0} (\lambda) - \nu^ {0} \right\| _ {1} + \left\| \nu^ {1} (\lambda) - \nu^ {1} \right\| _ {1}\right). \tag {15.23}
$$

Finally, we use our mixing assumption to bound the right-hand side above. Focusing on the case $w = 0$ , the top half of (15.21) can be re-written as

$$
\left(I - P ^ {0}\right) \nu^ {0} (\lambda) = \lambda \left(P ^ {1} \nu^ {1} (\lambda) - P ^ {0} \nu^ {0} (\lambda)\right),
$$

and because $\nu ^ { 0 }$ is a fixed point of $P ^ { 0 }$ we thus also have

$$
\left(I - P ^ {0}\right) \left(\nu^ {0} (\lambda) - \nu^ {0}\right) = \lambda \left(P ^ {1} \nu^ {1} (\lambda) - P ^ {0} \nu^ {0} (\lambda)\right).
$$

Combining this expression with (15.19), we get

$$
\begin{array}{l} \left\| \nu^ {0} (\lambda) - \nu^ {0} - \lambda \left(P ^ {1} \nu^ {1} (\lambda) - P ^ {0} \nu^ {0} (\lambda)\right) \right\| _ {1} = \left\| P ^ {0} \left(\nu^ {0} (\lambda) - \nu^ {0}\right) \right\| _ {1} \\ \leq e ^ {- 1 / t _ {0}} \left\| \nu^ {0} (\lambda) - \nu^ {0} \right\| _ {1}, \\ \end{array}
$$

and so by the triangle inequality

$$
\left(1 - e ^ {- 1 / t _ {0}}\right) \left\| \nu^ {0} (\lambda) - \nu^ {0} \right\| _ {1} \leq \lambda \left\| P ^ {1} \nu^ {1} (\lambda) - P ^ {0} \nu^ {0} (\lambda) \right\| _ {1}.
$$

The statement (15.20) follows by noting that $\left( 1 - e ^ { - 1 / t _ { 0 } } \right) ^ { - 1 } \leq 1 + t _ { 0 }$ and $\| P ^ { 1 } \nu ^ { 1 } ( \lambda ) - P ^ { 0 } \nu ^ { 0 } ( \lambda ) \| _ { 1 } \ \le \ 2$ . Finally, the consistency claim follows because $\lambda _ { T } \to 0$ implies that bias goes to 0 by the above, while the condition $\lambda _ { T } T  \infty$ implies that there are a diverging number of switches, and so $\hat { \tau } _ { S B } - \tau ( \lambda _ { T } ) \to _ { p } 0$ thanks to mixing as in (15.19). ‚ñ°

# Bibliographic notes

Markov decision processes have been an object of sustained study in the reinforcement learning literature for decades. Our discussion in this chapter fits within the area often referred to as off-policy evaluation in that literature, as we seek to use data collected under one (randomized) design to predict rewards under a different (target) policy. The off-policy setting is contrasted with the

on-policy setting, where we have access to a simulator that can be used to explore states on demand [Sutton and Barto, 2018]. Some notable off-policy algorithms developed in this literature include the temporal-difference learning algorithm which seeks to estimate the discounted value function

$$
V _ {\pi , \gamma} (x) = \mathbb {E} _ {\pi} \left[ \sum_ {t = 0} ^ {\infty} \gamma^ {t} Y _ {t} \mid X _ {0} = x \right], \quad 0 <   \gamma <   1, \tag {15.24}
$$

of a target policy by moment-matching on relevant Bellman equations [Sutton, 1988, Tsitsiklis and Van Roy, 1997],83 and the $Q$ -learning algorithm for finding the welfare-maximizing policy [Watkins and Dayan, 1992, Murphy, 2005].

The approach taken in this chapter builds on a line of work by Kallus and Uehara [2020] who emphasized the role of Markovian assumptions in mitigating the curse of dimensionality that affects the generic methods for dynamic policy evaluation discussed in the previous chapter, and Liao, Klasnja, and Murphy [2021] who showed how Markov decision processes enable identification of the long-run average value from sequentially unconfounded data. The approach to doubly robust estimation of the long-run average value presented here is adapted from Liao et al. [2022]; a similar approach to estimating discounted policy values (rather than long-run average values) is discussed in Kallus and Uehara [2022]. The setting where the density ratio $\omega _ { \pi } ( X )$ may be heavy tailed and $\Sigma$ as given in Theorem 15.3 is infinite is considered by Mehrabi and Wager [2024]; the authors show that $1 / \sqrt { T }$ -consistent estimation is no longer possible in this setting, but a properly truncated version of the doubly robust estimator from Theorem 15.3 can still achieve the minimax rate of convergence.

Switchback experiments are increasingly becoming a core part of the standard toolkit for causal inference in dynamic systems; Bojinov, Simchi-Levi, and Zhao [2023] provides a comprehensive overview of the design. The analysis presented here, i.e., with switchbacks used for policy evaluation in Markov decision processes, is adapted from Hu and Wager [2022]. One important practical distinction between the doubly robust estimators from Section 15.1 and switchback experiments is that the former require observing (and use of) the state variables $X _ { t }$ , whereas switchbacks do not. One can ask what happens to optimal inference in the setting of Section 15.1 if we no longer get to observe $X _ { t }$ and instead need to just rely on mixing (15.19) as we did for switchbacks. This setting is considered in Hu and Wager [2023], who show that $1 / \sqrt { T }$ -consistent estimation is in general not possible in this setting, and that switchback-like truncated IPW estimators achieve the minimax (slower-than- $1 / \sqrt { T }$ ) rate.

# Chapter 16 Exercises

Exercise 1. Consider a randomized controlled trial under the assumptions of Theorem 1.2. We already know that the difference-in-means estimator,

$$
\hat {\tau} _ {D M} = \frac {1}{| \{i : W _ {i} = 1 \} |} \sum_ {\{i: W _ {i} = 1 \}} Y _ {i} - \frac {1}{| \{i : W _ {i} = 0 \} |} \sum_ {\{i: W _ {i} = 0 \}} Y _ {i}, \tag {16.1}
$$

is consistent and satisfies a central limit theorem in this setting. However, following our discussion in Chapter 2, one might also consider the inversepropensity weighted estimator for $\tau$ ,

$$
\hat {\tau} _ {I P W} = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {W _ {i} Y _ {i}}{\pi} - \frac {(1 - W _ {i}) Y _ {i}}{1 - \pi}. \tag {16.2}
$$

The purpose of this question is to understand the relationship and relative benefits of these two estimators.

(a) State and prove a central limit theorem for ${ \hat { \tau } } _ { I P W }$ (you may make any regularity assumptions that are convenient for this purpose). Compare the variance of ${ \hat { \tau } } _ { I P W }$ to the asymptotic variance of ${ \hat { \tau } } _ { D M }$ given in Theorem 1.2.   
(b) What is the joint distribution of ${ \hat { \tau } } _ { D M }$ and ${ \hat { \tau } } _ { I P W }$ ? Based on your findings, would you recommend using ${ \hat { \tau } } _ { I P W }$ in a randomized study?

Exercise 2. Chapter 1 discussed the behavior of linear regression adjustments in randomized trials, and showed that such adjustments can be used to improve asymptotic precision whether or not the data follows a linear specification. The goal of this question is to extend these results to the case of generic nonparametric (or machine learning based) regression adjustments. For all parts below, you should work under the assumptions of Theorem 1.3.

(a) As shown in (1.27), the interacted regression estimator can be written as an average difference in predictions. Suppose now that we set

$$
\hat {\tau} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\hat {\mu} _ {(1)} \left(X _ {i}\right) - \hat {\mu} _ {(0)} \left(X _ {i}\right)\right), \tag {16.3}
$$

but rather than using linear regression, we get $\hat { \mu } _ { ( w ) } ( x )$ from a machine learning method that is consistent (under squared-error loss) for $\mu _ { ( w ) } ( x )$ as defined in (1.21). Are the following two statements true or false? If true, give a proof; if false, give a counterexample.

‚Ä¢ The estimator $\hat { \tau }$ is consistent.   
‚Ä¢ The estimator $\hat { \tau }$ is asymptotically normal, i.e., ${ \sqrt { n } } ( \hat { \tau } - \tau ) \Rightarrow { \mathcal { N } } ( 0 , V )$ for some finite asymptotic variance $V$ .

We now consider an improvement to the basic estimator that debiases (16.3) by considering regression residuals, and uses ‚Äúcross-fitting‚Äù to avoid overfitting. We first split the data (at random) into two halves $\mathcal { L } _ { 1 }$ and $\mathcal { I } _ { 2 }$ , and then use

$$
\begin{array}{l} \hat {\tau} _ {C F} = \frac {\hat {\tau} ^ {\mathcal {I} _ {1}} + \hat {\tau} ^ {\mathcal {I} _ {2}}}{2}, \quad \hat {\tau} ^ {\mathcal {I} _ {1}} = \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} \left(X _ {i}\right) - \hat {\mu} _ {(0)} ^ {\mathcal {I} _ {2}} \left(X _ {i}\right) \right. \tag {16.4} \\ \left. + \frac {W _ {i}}{\pi} \left(Y _ {i} - \hat {\mu} _ {(1)} ^ {\mathcal {I} _ {2}} (X _ {i})\right) - \frac {1 - W _ {i}}{1 - \pi} \left(Y _ {i} - \hat {\mu} _ {(0)} ^ {\mathcal {I} _ {2}} (X _ {i})\right)\right), \\ \end{array}
$$

where the $\hat { \mu } _ { ( w ) } ^ { \perp _ { 2 } } ( \cdot )$ are any estimates of $\mu _ { ( w ) } ( \cdot )$ obtained using only the halfsample $\mathcal { L } _ { 2 }$ , and $\hat { \tau } ^ { \mathcal { I } _ { 2 } }$ is defined analogously (with the roles of $\mathcal { L } _ { 1 }$ and $\mathcal { L } _ { 2 }$ swapped). In other words, $\hat { \tau } ^ { \mathcal { L } _ { 1 } }$ is a treatment effect estimator on $\mathcal { L } _ { 1 }$ that uses $\mathcal { L } _ { 2 }$ to estimate its regression adjustments, and vice-versa.

(b) What is the bias of the estimator (16.4), i.e., what is $\mathbb { E } \left[ \hat { \tau } _ { C F } \right] - \tau$ , where $\tau$ denotes the ATE?   
(c) Assume that our non-parametric regression adjustments $\hat { \mu } _ { ( w ) } ^ { \perp _ { 2 } } ( \cdot )$ are riskconsistent, i.e.,

$$
\lim  _ {n \rightarrow \infty} \mathbb {E} \left[ \frac {1}{| \mathcal {I} _ {1} |} \sum_ {i \in \mathcal {I} _ {1}} \left(\hat {\mu} _ {(w)} ^ {\mathcal {I} _ {2}} \left(X _ {i}\right) - \mu_ {(w)} \left(X _ {i}\right)\right) ^ {2} \right] = 0, \tag {16.5}
$$

and similarly with $\mathcal { I } _ { 1 }$ and $\mathcal { I } _ { 2 }$ swapped. Prove a central limit theorem for $\hat { \tau } _ { C F }$ , i.e., show that $\sqrt { n } ( \hat { \tau } _ { C F } - \tau ) \Rightarrow \mathcal { N } \left( 0 , V _ { C F } \right)$ for some asymptotic variance $V _ { C F }$ , and characterize $V _ { C F }$ . Compare $V _ { C F }$ to the asymptotic variance $V _ { I R E G }$ given in (1.23).

(d) Consider the setting discussed in Chapter 1 where a linear model is wellspecified,

$$
Y _ {i} (w) = X _ {i} \beta_ {(w)} + \varepsilon_ {i} (w), \quad \varepsilon_ {i} (w) \sim \mathcal {N} \left(0, \sigma^ {2}\right), \tag {16.6}
$$

and compare the asymptotic behavior of (16.4) under assumption (16.5) with the asymptotic behavior of the OLS estimator discussed in Chapter 1. Does

one estimator dominate the other? (You may assume $\pi = 0 . 5$ , etc., for convenience.)

Exercise 3. A common issue in applying the IPW estimator discussed in Chapter 2 arises when there are some units who are a-priori very unlikely to get treated, and have $e ( X _ { i } ) \approx 0$ . This situation could arise, for example, in a medical application where $W _ { i }$ denotes a candidate intervention and some patients are obviously healthy based on their $X _ { i }$ and so will never get treated. And, when $e ( X _ { i } )$ may get close to 0, the IPW estimator (which involves dividing by $e ( X _ { i } )$ ) may be unstable.

One solution to this difficulty is to change statistical targets, and to focus on the average treatment effect on the treated instead:

$$
\tau_ {A T T} = \mathbb {E} \left[ Y _ {i} (1) - Y _ {i} (0) \mid W _ {i} = 1 \right]. \tag {16.7}
$$

In many applications, focusing on the ATT can improve the precision of the available estimators‚Äîand can also be of substantive interest (since the ATT measures average the value of the treatment among people who got the treatment in the sampling distribution). Throughout this question, you may assume that the propensity scores $e ( X _ { i } )$ are known a-priori and can be used for estimation, and that $e ( X _ { i } ) \leq 1 - \eta$ for some $\eta > 0$ . You may also take $\mathbb { P } \left[ W _ { i } = 1 \right] = \pi$ to be known.

(a) Propose an IPW-style estimator for the ATT (using the true propensity scores), and prove that it is unbiased.   
(b) Derive the asymptotic variance of estimator derived in part (a), and state a central limit theorem for it.   
(c) Compare the asymptotic variance of the oracle IPW estimators for the ATE and the ATT in a setting where $e ( X _ { i } )$ may get very small, and discuss the robustness of both estimators to small propensity scores.

Exercise 4. In Chapter 2, we defined a propensity-stratified estimator $\hat { \tau } _ { P S T R A T }$ . The purpose of this question is to flesh out our study of this estimator. Under the conditions of Theorem 2.2, assume furthermore that we have overlap in the sense that $\eta \le e ( x ) \le 1 - \eta$ for all $x \in \mathcal { X }$ , that the distribution of the propensity scores $e ( X )$ admits a density $f _ { e } ( \cdot )$ that is bounded away from 0 on the interval $[ \eta , 1 - \eta ]$ , and that the outcomes are bounded $| Y _ { i } | \le M$ for some large constant $M$ .

(a) Show that if $J \ = \ n ^ { \rho }$ for some constant $0 ~ < ~ \rho ~ < ~ 1$ , then the estimator $\hat { \tau } _ { P S T R A T }$ implemented using the true propensity scores is consistent, i.e., $\hat { \tau } _ { P S T R A T }  _ { p } \tau$ where $\tau$ is the average treatment effect.

(b) Conduct a simulation study to evaluate the pros and cons of inversepropensity weighting and stratification. Generate data in $\mathtt { R }$ as follows, for $n = 1 0 0$ , 200, 400, 800, 1600, 3200 and $p = 1 0$ :

```python
X = matrix(runif(n * p, -1, 1), n, p)  
propensity = 0.1 + 0.85 * sqrt(pmax(0, 1 + X[,1] + X[,2])/3)  
W = rbinom(n, 1, propensity)  
Y = W * pmax(0, X[,1]) + exp(X[,2] + X[,3]) + rnorm(n) 
```

Fit propensities $\hat { e }$ via logistic regression, and then estimate $\tau$ via ${ \hat { \tau } } _ { I P W }$ and $\hat { \tau } _ { P S T R A T }$ using the fitted propensities.

What is the average treatment effect $\tau$ in this simulation design? What is a good choice for $J$ ? How does the performance of ${ \hat { \tau } } _ { I P W }$ compare to that of $\hat { \tau } _ { P S T R A T }$ in terms of bias? What about in terms of mean-squared error? A good analysis will rely on enough simulation replications to mitigate uncertainty due to Monte Carlo effects, and convey results via appropriate visual displays.

(c) Show that, for a properly chosen sequence $J ( n )$ , the propensity-stratified estimator (now again implemented using the true propensities) is asymptotically unbiased and Gaussian, i.e., $\sqrt { n } ( \hat { \tau } _ { P S T R A T } - \tau ) \Rightarrow \mathcal { N } ( 0 , V _ { P S T R A T } )$ . Propose a consistent variance estimator for $\dot { V } _ { P S T R A T }$ for $V _ { P S T R A T }$ , such that $\widehat { V } _ { P S T R A T } / V _ { P S T R A T } \to _ { p } 1$ . Discuss how these results can be used to build a confidence interval for $\tau$ centered at $\hat { \tau } _ { P S T R A T }$ .

(d) In Chapter 3, we showed how to ‚Äúaugment‚Äù the inverse-propensity weighted ATE estimator with a regression adjustment, and showed that the resulting AIPW estimator had improved robustness and precision properties relative to the basic IPW estimator. How would you analogously ‚Äúaugment‚Äù the propensity stratified estimator studied here? Propose an estimator, and argue for it. (Note: Your argument doesn‚Äôt need to be formal; a short qualitative argument is enough.)

Exercise 5. In Corollary 4.3, we gave asymptotic properties of the residualon-residual estimator,

$$
\hat {\tau} _ {R} = \frac {\sum_ {i = 1} ^ {n} \left(Y _ {i} - \hat {m} ^ {(- k (i))} \left(X _ {i}\right)\right) \left(W _ {i} - \hat {e} ^ {(- k (i))} \left(X _ {i}\right)\right)}{\sum_ {i = 1} ^ {n} \left(W _ {i} - \hat {e} ^ {(- k (i))} \left(X _ {i}\right)\right) ^ {2}}, \tag {16.8}
$$

for estimating the treatment parameter $\tau$ under the constant treatment effect model $Y _ { i } ( w ) = f ( X _ { i } ) + w \tau + \varepsilon _ { i }$ . The purpose of this question is to study this same residual-on-residual estimator under misspecification of the constant

treatment effect hypothesis. Assume that data is independently generated as

$$
Y _ {i} (w) = \mu_ {(w)} \left(X _ {i}\right) + \varepsilon_ {i} (w), \quad \mathbb {E} \left[ \varepsilon_ {i} (w) \mid X _ {i} = x, W _ {i} = w \right] = 0, \tag {16.9}
$$

Var $\lfloor \varepsilon _ { i } ( w ) \mid \Lambda _ { i } = x , w _ { i } = w \rfloor = \sigma ^ { - } ,$

and write $\tau ( x ) = \mu _ { ( 1 ) } ( x ) - \mu _ { ( 0 ) } ( x )$ . Our goal is to characterize asymptotic behavior of $\hat { \tau } _ { R }$ under model (16.9). Throughout this problem you may assume that $e ( x ) \in ( 0 , 1 )$ ; however, do not assume strong overlap.

(a) Let $\hat { \tau } _ { R } ^ { * }$ be the ‚Äúoracle‚Äù version of the estimator (16.8), computed using the true $m ( x )$ and $e ( x )$ . Show that $\hat { \tau } _ { R } ^ { * }$ converges in probability to a limit $\tau _ { R }$ that is a non-negative weighted average of the conditional average treatment effect $\tau ( x )$ , i.e., $\tau _ { R } = \mathbb { E } \left[ \gamma ( X _ { i } ) \tau ( X _ { i } ) \right]$ for some function with $\gamma ( \boldsymbol { x } ) \ge 0$ and $\mathbb { E } \left[ \gamma ( X _ { i } ) \right] = 1$ .   
(b) Show that this oracle estimator satisfies a central limit theorem $\sqrt { n } ( \hat { \tau } _ { R } ^ { * } -$ $\tau _ { R } ) \Rightarrow { \mathcal { N } } ( 0 , V _ { R } )$ , and provide an expression for $V _ { R }$ . How does $V _ { R }$ compare to the semiparametric efficient variance for average treatment effect estimation?   
(c) Suppose that $\hat { m } ( X _ { i } )$ and $\hat { e } ( X _ { i } )$ satisfy the rate conditions (4.7). Show that $\sqrt { n } ( \hat { \tau } _ { R } - \hat { \tau } _ { R } ^ { * } ) \to _ { p } 0$ , and so the feasible estimator (16.8) also satisfies the central limit theorem established in part (b).

Exercise 6. Consider a hypothetical company that has a phone app that they use to offer $K > 3$ different products that customers can choose to purchase. However, given the size of a phone screen, it can only show 3 (ranked) recommendations to a user at any given time. Your goal is to help the platform evaluate how different ranking strategies affect performance.

You have data on $i = 1$ , . . . , $n$ IID customers who have interacted with the platform. For each customer, the platform:

‚Ä¢ Computes scores $S _ { i 1 }$ , . . . , $S _ { i K } ~ > ~ 0$ reflecting how well each product is suited to the $i$ -th customer. (These scores are computed by some blackbox algorithm you don‚Äôt have access to, but they are recorded and are included in your dataset.)   
‚Ä¢ Randomly chooses a product $A _ { i } ^ { ( 1 ) }$ to display first, such that

$$
\mathbb {P} \left[ A _ {i} ^ {(1)} = k \right] = e ^ {S _ {i, k}} / \sum_ {\ell = 1} ^ {K} e ^ {S _ {i, \ell}} \text {f o r a l l} k = 1, \dots , K.
$$

‚Ä¢ Randomly chooses a product $A _ { i } ^ { ( 2 ) }$ to display second, such that

$$
\mathbb {P} \left[ A _ {i} ^ {(2)} = k \right] = e ^ {S _ {i, k}} \big / \sum_ {\ell \neq A _ {i} ^ {(1)}} e ^ {S _ {i, \ell}} \mathrm {f o r a l l} k \neq A _ {i} ^ {(1)}.
$$

‚Ä¢ Randomly chooses a product $A _ { i } ^ { ( 3 ) }$ to display second, such that

$$
\mathbb {P} \left[ A _ {i} ^ {(3)} = k \right] = e ^ {S _ {i, k}} \bigg / \sum_ {\ell \neq A _ {i} ^ {(1)}, A _ {i} ^ {(2)}} e ^ {S _ {i, \ell}} \mathrm {f o r a l l} k \neq A _ {i} ^ {(1)}, A _ {i} ^ {(2)}.
$$

‚Ä¢ Observes a reward $Y _ { i }$

For the purpose of the questions below, you should assume that the exact ranking $A _ { i } ^ { ( 1 ) }$ , $A _ { i } ^ { ( 2 ) }$ , $A _ { i } ^ { ( 3 ) }$ shown to the user matters. Note that the platform does not rank the other products (you may assume, e.g., that if the customer wants to select one of the other products, they need to do so by navigating to a separate static list that shows products in alphabetical order).

We will refer to (both random and deterministic) methods for ranking products as policies, and to the expected reward the platform would achieve by deploying a policy as the value $V$ of the policy. The available data

$$
\mathcal {D} _ {n} = \left\{S _ {i}, A _ {i} ^ {(1)}, A _ {i} ^ {(2)}, A _ {i} ^ {(3)}, Y _ {i} \right\} _ {i = 1} ^ {n}
$$

generated as described above, is the same for all 4 parts below. An unbiased estimator of policy value $V$ is a (measurable) function $\widehat { V }$ of the observed data $\mathcal { D } _ { n }$ for which $\mathbb { E } [ \widehat { V } ] = V$ . We assume that each unit has potential outcomes $Y _ { i } ( a _ { 1 } , a _ { 2 } , a _ { 3 } )$ such that the observed reward is

$$
Y _ {i} = Y _ {i} \left(A _ {i} ^ {(1)}, A _ {i} ^ {(2)}, A _ {i} ^ {(3)}\right),
$$

and the value of a policy $\pi$ is

$$
V (\pi) = \mathbb {E} _ {A _ {i} \sim \pi (S _ {i})} \left[ Y _ {i} (A _ {i}) \right], \quad A _ {i} = \left(A _ {i} ^ {(1)}, A _ {i} ^ {(2)}, A _ {i} ^ {(3)}\right),
$$

where $A _ { i } \ \sim \ \pi ( S _ { i } )$ means that $A _ { i }$ is generated via the (potentially random) function $\pi$ of $S _ { i }$ .

(a) Propose an estimator that, given the available data $\mathcal { D } _ { n }$ , gives an unbiased estimate of the value of the current randomized policy (i.e., the policy used in data collection).   
(b) Propose an estimator that, given the available data $\mathcal { D } _ { n }$ , gives an unbiased (i.e., sets estimate of the value of a policy that always uses a fixed ranking $A _ { i } ^ { ( 1 ) } = a _ { 1 }$ , $A _ { i } ^ { ( 2 ) } = a _ { 2 }$ , $A _ { i } ^ { ( c ) } = a _ { 3 }$ for some $1 \leq a _ { 1 } \neq a _ { 2 } \neq a _ { 3 } \leq K$ $a _ { 1 }$ , ) $a _ { 2 }$ , $a _ { 3 }$   
(c) Propose an estimator that, given the available data $\mathcal { D } _ { n }$ , gives an unbiased estimate of the value of a randomized policy that always shows some product

$a _ { 1 }$ first (i.e., deterministically sets $A _ { i } ^ { ( 1 ) } = a _ { 1 }$ for some $1 \leq a _ { 1 } \leq K$ ), but then randomly chooses A(2)i $A _ { i } ^ { ( 2 ) }$ and $A _ { i } ^ { ( 3 ) }$ using the available scores in the same way as with the data collection policy.

(d) Propose an estimator that, given the available data $\mathcal { D } _ { n }$ , gives an unbiased estimate of the value of a randomized policy that never shows some product $a _ { 0 }$ with $1 \le a _ { 0 } \le K$ , but otherwise randomly draws random products using scores as with the data collection policy (operationally, you could assume that if any of the random draws gives $A _ { i } ^ { ( \ell ) } = a _ { 0 }$ , then the platform re-draws from the same distribution until $A _ { i } ^ { ( \ell ) } \neq a _ { 0 }$ ) .

Exercise 7. Consider the following model for adaptive data-collection ( $\eta > 0$ is a tuning parameter): For $t = 1$ , . . . , $T$ time steps, we

‚Ä¢ Choose a probability $\omega _ { t } \in [ \eta , 1 ]$ , potentially using past data.   
‚Ä¢ Draw a Bernoulli random variable $Z _ { t } \sim \mathrm { B e r n } ( \omega _ { t } )$ .   
‚Ä¢ If $Z _ { t } = 1$ , we observe a draw $Y _ { t } \sim F$ ; while if $Z _ { t } = 0$ , we cannot make an observation (equivalently, we hard-code $Y _ { t } = 0$ ).

Our goal is to estimate the mean $\mu = \mathbb { E } _ { F } \left\lfloor Y \right\rfloor$ , and are considering 3 different estimators:

1. Sample average: $\begin{array} { r } { \hat { \mu } _ { 1 } = \sum _ { \{ t : Z _ { t } = 1 \} } Y _ { t } / \left| \left\{ t : Z _ { t } = 1 \right\} \right| . } \end{array}$   
2. Inverse-propensity weighting: $\begin{array} { r } { \hat { \mu } _ { 2 } = T ^ { - 1 } \sum _ { t = 1 } ^ { T } Z _ { t } Y _ { t } / \omega _ { t } } \end{array}$   
3. Self-normalized IPW: $\begin{array} { r } { \hat { \mu } _ { 3 } = \sum _ { t = 1 } ^ { T } Z _ { t } Y _ { t } / \omega _ { t } / \sum _ { t = 1 } ^ { T } Z _ { t } / \omega _ { t } . } \end{array}$

To avoid degenerate cases, you may assume that $\omega _ { 1 } = 1$ , i.e., we always collect at least 1 sample. You may also make any regularity assumption you find to be convenient (e.g., that the $Y _ { t }$ have bounded support).

(a) Which of the 3 estimators above are unbiased, i.e., satisfy $\mathbb { E } \left[ \boldsymbol { \hat { \mu } } \right] = \boldsymbol { \mu }$ ? Provide a proof or counterexample.   
(b) Now consider a large-sample limit, with $T \to \infty$ . In this setting, we say that an estimator is asymptotically unbiased if

$$
\lim _ {T \to \infty} \sqrt {T} \left(\mathbb {E} \left[ \hat {\mu} \right] - \mu\right) = 0.
$$

Which of the 3 estimators above are asymptotically unbiased? Provide a proof or counterexample.

Exercise 8. Theorem 7.1 provides the asymptotic distribution of the covariatebalancing propensity score estimator $\hat { \tau } _ { C B P S }$ under a linear-logistic specification where both

$$
\mu_ {(w)} = x \cdot \beta_ {(w)}, \quad \beta_ {(w)} \in \mathbb {R} ^ {p} \quad \text {f o r} w = 0, 1, \tag {16.10}
$$

$$
e (x) = 1 / \left(1 + e ^ {- x \cdot \theta}\right), \quad \theta \in \mathbb {R} ^ {p}, \quad \| \theta \| _ {2} <   \infty . \tag {16.11}
$$

The goal of this question is to study double robustness properties of $\hat { \tau } _ { C B P S }$ . 8 4 In answering this question, you may replace the exponential moment condition (7.12) with the stronger boundedness condition $\| X _ { i } \| _ { 2 } \leq M$ .

(a) Under the setting of Theorem 7.1, suppose that (16.10) holds but that (16.11) may not hold. Prove that $\hat { \tau } _ { C P B S } \to _ { p } \tau$ , where $\tau$ denotes the ATE. You may assume that strong overlap holds, $\eta \leq e ( X _ { i } ) \leq 1 - \eta$ , if convenient.   
(b) Under the setting of Theorem 7.1, suppose conversely that (16.11) holds but that (16.10) may not hold. Prove that $\hat { \tau } _ { C P B S } \to _ { p } \tau$ . You may assume that outcomes are bounded, $| Y _ { i } | \le M$ , if convenient.

Exercise 9. Under the conditions of Theorem 7.1 suppose that, rather than the ATE, we want to estimate the average treatment effect on the treated (ATT) as in Exercise 3, $\tau _ { A T T } = \mathbb { E } \left[ Y _ { i } ( 1 ) - Y _ { i } ( 0 ) \vert W _ { i } = 1 \right]$ . We claim that

$$
\hat {\theta} = \operatorname {a r g m i n} _ {\theta} \left\{\frac {1}{n _ {1}} \sum_ {i = 1} ^ {n} \left((1 - W _ {i}) e ^ {X _ {i} \theta} - W _ {i} X _ {i} \theta\right) \right\}, \tag {16.12}
$$

$$
\hat {\tau} _ {C B P S - A T T} = \frac {1}{n _ {1}} \sum_ {i = 1} ^ {n} \left(W _ {i} Y _ {i} - (1 - W _ {i}) e ^ {X _ {i} \hat {\theta}} Y _ {i}\right), \tag {16.13}
$$

is the natural CBPS estimator for this task, and has good statistical properties.

(a) Verify that (16.12) is a convex minimization problem.   
(b) Verify that (16.13) is in fact a CBPS estimator, i.e., that it is the IPW estimator for some specific choice $\hat { e } ( x ) = 1 / \left( 1 + e ^ { - x \hat { \theta } } \right)$ , and that $\hat { \theta }$ satisfies a relevant sample-balance condition whenever the minimization problem (16.12) has an interior solution (i.e., $\| { \hat { \theta } } \| < \infty$ ).   
(c) Prove that $\hat { \tau } _ { C B P S - A T T }$ is consistent for $\tau _ { A T T }$ , and establish a central limit theorem. For simplicity, you may assume that $\| X _ { i } \| _ { 2 } \leq M$ uniformly.

Exercise 10. Consider an IID sequence $( X _ { i } , U _ { i } , Y _ { i } , W _ { i } ) \in \mathcal { X } \times \mathcal { U } \times \mathbb { R } \times \{ 0 , 1 \}$ , where $Y _ { i } = Y _ { i } ( W _ { i } )$ for a pair of potential outcomes $\{ Y _ { i } ( 0 ) , Y _ { i } ( 1 ) \}$ . Unconfoundedness holds conditionally on $X _ { i }$ and $U _ { i }$ , i.e.,

$$
\left\{Y _ {i} (0), Y _ {i} (1) \right\} \perp W _ {i} \mid X _ {i}, U _ {i}. \tag {16.14}
$$

However, only $X _ { i }$ is observed, whereas $U _ { i }$ is an unobserved confounder. In this question, we‚Äôll study the behavior of self-normalized IPW estimators of $\mu ( 1 ) = \mathbb { E } \left[ Y _ { i } ( 1 ) \right]$ in the presence of unobserved confounding. To this end, define both the feasible and infeasible IPW estimators, the latter of which makes use of the unobserved $U _ { i }$ :

$$
\hat {\mu} _ {S I P W} (1) = \sum_ {i = 1} ^ {n} \frac {W _ {i} Y _ {i}}{e \left(X _ {i}\right)} / \sum_ {i = 1} ^ {n} \frac {W _ {i}}{e \left(X _ {i}\right)}, \tag {16.15}
$$

$$
\tilde {\mu} _ {S I P W} (1) = \sum_ {i = 1} ^ {n} \frac {W _ {i} Y _ {i}}{e (X _ {i} , U _ {i})} \Bigg / \sum_ {i = 1} ^ {n} \frac {W _ {i}}{e (X _ {i} , U _ {i})},
$$

where $e ( x ) = \mathbb { P } \left[ W _ { i } = 1 \big | X _ { i } = x \right]$ and $e ( x , u ) \ : = \ : \mathbb { P } \left\lfloor W _ { i } = 1 \right\rfloor X _ { i } = x$ , $U _ { i } = u \rfloor$ . Under the unconfoundedness condition (16.14), $\tilde { \mu } _ { S I P W } ( 1 )$ is clearly consistent for $\mu ( 1 )$ , but ${ \hat { \mu } } _ { S I P W } ( 1 )$ may not be.

In general, it‚Äôs not possible to say much about the bias of ${ \hat { \mu } } _ { S I P W } ( 1 )$ . Thus, we‚Äôll make a further assumption about how the unobserved $U _ { i }$ may affect sampling probabilities, and assume that we know a constant $\Gamma \geq 1$ such that

$$
\frac {1}{\Gamma} \leq \frac {e (X _ {i} , U _ {i})}{e (X _ {i})} \leq \Gamma \text {f o r a l l} i = 1, \dots , n, \tag {16.16}
$$

almost surely. This assumption is commonly known as the marginal sensitivity model, and can be used to assess the sensitivity of IPW to hidden confounding.

(a) Under (16.16), show that there exist weights $\Gamma _ { i } ^ { - 1 } \leq \gamma _ { i } \leq \Gamma _ { i }$ such that

$$
\tilde {\mu} _ {S I P W} (1) = \hat {\mu} _ {S I P W} (1; \gamma) := \sum_ {i = 1} ^ {n} \gamma_ {i} \frac {W _ {i} Y _ {i}}{e \left(X _ {i}\right)} / \sum_ {i = 1} ^ {n} \gamma_ {i} \frac {W _ {i}}{e \left(X _ {i}\right)}. \tag {16.17}
$$

(b) Given (16.17), we have the following upper bound for $\tilde { \mu } _ { S I P W } ( 1 )$ :

$$
\hat {\mu} _ {S I P W} ^ {+} (1) = \sup  \left\{\hat {\mu} _ {S I P W} (1; \gamma): \Gamma_ {i} ^ {- 1} \leq \gamma_ {i} \leq \Gamma_ {i} \right\}. \tag {16.18}
$$

Show that the above optimization program can be solved by linear programming, and express the problem in a way that could be plugged into standard

linear programming software, i.e., in format ‚Äúmaximize $c ^ { \prime } x$ subject to $A x \le b ^ { \prime }$ , where we optimize over the vector $x$ and take $A$ , $b$ and $c$ as given.

Hint. Consider the Charnes-Cooper transformation for linear-fractional programming.

(c) Using the construction in (16.18), propose an interval

$$
\widehat {I} _ {S I P W} (1) = \left[ \hat {\mu} _ {S I P W} ^ {-} (1), \hat {\mu} _ {S I P W} ^ {+} (1) \right] \tag {16.19}
$$

that does not use the unobserved $U _ { i }$ , but has the property that $\tilde { \mu } _ { S I P W } ( 1 ) \in \widehat { I } _ { S I P W } ( 1 )$ almost surely. Show that the interval $\widehat { I } _ { S I P W } ( 1 )$ is consistent for $\mu ( 1 )$ in the following sense: For any $\varepsilon > 0$

$$
\lim  _ {n \rightarrow \infty} \mathbb {P} [ \mu (1) \in (\hat {\mu} _ {S I P W} ^ {-} (1) - \varepsilon , \hat {\mu} _ {S I P W} ^ {+} (1) + \varepsilon) ] = 1. \tag {16.20}
$$

In doing so, you may make any regularity assumptions you find to be convenient (e.g., bounds on moments).

(d) Discuss how the intervals (16.19) could be used in practical data analysis to assess the sensitivity of IPW to the potential presence of unobserved confounders.

Exercise 11. Consider the following structural model, where $( X _ { i } , Y _ { i } , W _ { i } , Z _ { i } ) \in \mathcal { X } \times \mathbb { R } \times \{ 0 , 1 \} \times \{ 0 , 1 \}$ are taken to be IID:

$$
Y _ {i} = \alpha \left(X _ {i}\right) + W _ {i} \tau \left(X _ {i}\right) + \varepsilon_ {i}, \quad \varepsilon_ {i} \perp Z _ {i} \mid X _ {i}, \quad \mathbb {E} \left[ \varepsilon_ {i} \mid X _ {i} \right] = 0 \tag {16.21}
$$

Cov $\lfloor W _ { i } , \ Z _ { i } \mid X _ { i } = x \rfloor \geq \eta > 0 \quad { \mathrm { f o r ~ a l l ~ } } x \in { \mathcal { X } } .$

In other words, conditionally on covariates $X _ { i }$ , this is the same structural model as used in Chapter 9.2; now, however, all problem primitives may also vary with $x$ . Furthermore, we assumed that the effect of the instrument on the outcome is always positive and uniformly bounded from below.

Your goal is to develop methods to estimate the average treatment effect parameter $\tau = \mathbb { E } \left[ \tau ( X ) \right]$ . In all parts below, you may make any regularity assumptions you find to be helpful (e.g., boundedness of outcomes).

(a) Define the ‚Äúcompliance score‚Äù $\Delta ( x )$ and the associated inverse-compliance weighted estimator,

$$
\Delta (x) = \mathbb {P} \left[ W _ {i} = 1 \mid Z _ {i} = 1, X _ {i} = x \right] - \mathbb {P} \left[ W _ {i} = 1 \mid Z _ {i} = 0, X _ {i} = x \right],
$$

$$
\hat {\tau} _ {I C W} = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {1}{\Delta \left(X _ {i}\right)} \left(\frac {Z _ {i} Y _ {i}}{z \left(X _ {i}\right)} - \frac {(1 - Z _ {i}) Y _ {i}}{1 - z \left(X _ {i}\right)}\right), \tag {16.22}
$$

where $z ( x ) = \mathbb { P } \left[ Z _ { i } = 1 \big | X _ { i } = x \right]$ is an analogue to the propensity score for the instrument $Z _ { i }$ . Prove that the oracle inverse-compliance weighted estimator (i.e., using the true values of $z ( \cdot )$ and $\Delta ( \cdot )$ ) is unbiased and consistent for $\tau$ .

(b) Now suppose you obtain estimates ${ \hat { \alpha } } ( x )$ and $\hat { \tau } ( x )$ for the structural parameters in (16.21). Propose an augmented inverse-compliance weighted (AICW) estimator. Argue that your AICW estimator is (weakly) doubly robust, i.e., it is consistent if either ${ \hat { \alpha } } ( x )$ and $\hat { \tau } ( x )$ are sup-norm consistent, or $\widehat { \Delta } ( x )$ and $\hat { z } ( x )$ are sup-norm consistent (where $\widehat { \Delta } ( x )$ and $\hat { z } ( x )$ are feasible estimates of the non-parametric components in (16.22)). A high-level argument is enough here; no need to go into details.85   
(c) Show that if all the functions ${ \hat { \alpha } } ( x )$ , $\hat { \tau } ( x )$ , $\widehat { \Delta } ( x )$ and $\hat { z } ( x )$ are both sup-norm consistent and $o _ { p } ( n ^ { - 1 / 4 } )$ consistent in root-mean squared error, then AICW with cross-fitting is $\sqrt { n }$ -consistent for $\tau$ and asymptotically normal. Write down a central limit theorem, and provide an expression for the limiting variance of AICW.

Exercise 12. In Chapter 10.1, we studied instrumental variables regression with a binary treatment and binary instrument. We showed that under a ‚Äúno defiers‚Äù assumption, i.e.,

$$
\mathbb {P} \left[ W _ {i} (0) <   W _ {i} (1) \right] = 0, \tag {16.23}
$$

the instrumental variables estimator converges to the average treatment effect estimator for the compliers. Your goal in this question is to understand what happens when we relax this assumption.

Under the setting of Theorem 10.1, suppose now that we may have defiers, but there exist unobserved latent factors $U _ { i }$ for which

$$
\mathbb {P} \left[ W _ {i} = 1 \mid Z _ {i} = 1, U _ {i} = u \right] > \mathbb {P} \left[ W _ {i} = 1 \mid Z _ {i} = 0, U _ {i} = u \right], \tag {16.24}
$$

$$
\left\{Y _ {i} (0), Y _ {i} (1) \right\} \perp C _ {i} \mid U _ {i} = u, \text {f o r a l l} u,
$$

i.e., given the unobserved latent factors, we assume that the treatment effect is independent of compliance type, and that all latent types are more likely to comply than to defy. Also assume that $Z _ { i }$ is still exogenous once we include the $U _ { i }$ into the model,

$$
Z _ {i} \perp \left\{U _ {i}, Y _ {i} (0), Y _ {i} (1), W _ {i} (0), W _ {i} (1) \right\}.
$$

Write an expression for $\tau _ { I V }$ in terms of

$$
\tau (u) = \mathbb {E} \left[ Y _ {i} (1) - Y _ {i} (0) \mid U _ {i} = u \right],
$$

$$
\kappa (u) = \mathbb {P} \left[ C _ {i} = \text {c o m p l i e r} \mid U _ {i} = u \right], \text {a n d}
$$

$$
\delta (u) = \mathbb {P} \left[ C _ {i} = \text {d e f i e r} \mid U _ {i} = u \right].
$$

Show that, if $\tau ( u ) \geq 0$ for all $u$ , then $\tau _ { I V } \geq 0$

Exercise 13. Consider a set of $n$ random variables $( W _ { i } , Y _ { i } ) \in \{ 0 , 1 \} \times \mathbb { R }$ Assume that the data is generated as follows:

‚Ä¢ Each unit $i = 1$ , . . . , $n$ is characterized by (deterministic) parameters $\alpha _ { i }$ $\beta _ { i }$ , $\gamma _ { i } \in \mathbb { R }$ .   
‚Ä¢ We choose a treatment probability $\pi \in [ 0 , 1 ]$ , and independently generate $W _ { i } \sim \mathrm { B e r n o u l l i } ( \pi )$ for each $i = 1$ , . . . , $n$ .   
‚Ä¢ We observe the following, where $\varepsilon _ { i } \sim \mathcal { N } \left( 0 , \sigma ^ { 2 } \right)$ independently of everything else:

$$
Y _ {i} = \alpha_ {i} + \beta_ {i} W _ {i} + \gamma_ {i} \frac {\sum_ {j \neq i} W _ {j}}{n - 1} + \varepsilon_ {i}
$$

We use the notation $\mathbb { E } _ { \pi } \left[ Y _ { i } \right]$ for the expectation of the $i$ -th outcome under this model (with treatment probability $\pi$ ), as well as immediate generalizations of this notation. Note: Qualitatively, $\alpha _ { i }$ captures the $i$ -th unit‚Äôs baseline effect, $\beta _ { i }$ its sensitivity to its own treatment, and $\gamma _ { i }$ its sensitivity to the fraction of other units who are treated.

(a) What is the total effect, i.e., the expected difference in average outcomes when everyone is treated vs. when no one is:

$$
\tau_ {T O T} = \frac {1}{n} \sum_ {i = 1} ^ {n} \mathbb {E} _ {1} \left[ Y _ {i} \right] - \frac {1}{n} \sum_ {i = 1} ^ {n} \mathbb {E} _ {0} \left[ Y _ {i} \right].
$$

(b) Now suppose we are able to collect observations at a single $\pi \in ( 0 , 1 )$ , and seek to estimate the effect of the treatment via the na¬®ƒ±ve inverse-propensity weighted estimator that ignores spillovers,

$$
\hat {\tau} _ {I P W} = \frac {1}{n} \sum_ {i = 1} ^ {n} \left(\frac {W _ {i} Y _ {i}}{\pi} - \frac {(1 - W _ {i}) Y _ {i}}{1 - \pi}\right).
$$

What is $\mathbb { E } _ { \pi } \left[ \hat { \tau } _ { I P W } \right]$ ?

(c) In the same setting as above, what is $\operatorname { V a r } _ { \boldsymbol { \pi } } \left\lfloor \hat { \tau } _ { I P W } \right\rfloor$ ?   
(d) Is ${ \hat { \tau } } _ { I P W }$ a good estimator of $\tau _ { T O T }$ in this model? Can ${ \hat { \tau } } _ { I P W }$ be used to learn anything interesting in this model?

Exercise 14. One important question in survival analysis is to assess prognosis given a diagnosis. We have data on i = 1, . . . , $n$ people who are diagnosed with a condition at time $t = 0$ ; at this time, we also measure time-invariant convariates $X _ { i } \in { \mathcal { X } }$ . We write $Y _ { i }$ for the length of time the $i$ -th person survives post-diagnosis, and are interested in estimating $\theta = \mathbb { P } \left[ Y _ { i } > T \right]$ for some targethorizon $T$ .

The challenge, however, is that we may lose track of some patients in our study before we get to see whether they live past time $T$ . Specifically, we will assume that we follow-up with each patient at a set of pre-determined times $t = 1 , \ldots , T ^ { \prime }$ $T$ , and at each of these follow-ups we either are able track down the patient (in which case we can observe whether the patient is still alive, i.e., whether $Y _ { i } > t$ ), or we are unable to track down the patient and deem them to be censored at time $t$ (and we cease further follow-up attempts).

Formally, we assume that each unit has a (potentially non-realized) censoring time ${ C _ { i } } \in \{ 1 , 2 , . . . , T , + \infty \}$ , where $C _ { i } = + \infty$ means the unit is never censored. We then assume that, rather than getting to directly observe survival time $Y _ { i }$ , we only have access to86

$$
U _ {i} = \min  \left\{C _ {i}, Y _ {i} \right\}, \quad \Delta_ {i} = 1 \left(Y _ {i} <   C _ {i}\right), \tag {16.25}
$$

which we refer to as the observation time and the non-censoring indicator respectively. Write $H _ { i } = \operatorname* { m i n } { \{ U _ { i } , T \} }$ for the time of the last recorded visit, i.e., $H _ { i } = T$ even if the patient is still alive and uncensored at that point.

We also make the following statistical assumptions:

‚Ä¢ Censoring is ignorable, i.e.,

$$
Y _ {i} \perp C _ {i} \mid X _ {i}; \tag {16.26}
$$

‚Ä¢ Some patients are never censored, i.e., there is an $\eta > 0$ such that

$$
\mathbb {P} \left[ C _ {i} > T \mid X _ {i} = x \right] \geq \eta \text {f o r a l l} x \in \mathcal {X}. \tag {16.27}
$$

Note that these assumptions are closely related to our familiar assumptions of unconfoundedness and overlap for treatment effect estimation.

Throughout, we define the conditional survival functions

$$
S _ {Y} (t; x) = \mathbb {P} \left[ Y _ {i} > t \mid X _ {i} = x \right], \quad S _ {C} (t; x) = \mathbb {P} \left[ C _ {i} > t \mid X _ {i} = x \right], \quad (1 6. 2 8)
$$

and assume that we have access to estimates for these objects using a separate training set.87

(a) Suppose that the survival function for the censoring distribution $S _ { C } ( t ; x )$ is known. Show that, under our assumptions, the following inverse-probability of censoring (IPCW) estimator is unbiased for $\theta$ :

$$
\hat {\theta} _ {I P C W} = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {\Delta_ {i} 1 (\{U _ {i} > T \})}{S _ {C} (U _ {i} ; X _ {i})}. \tag {16.29}
$$

(b) Now, consider a setting where we have access to estimates $\hat { S } _ { Y } ( t ; x )$ and $\hat { S } _ { C } ( t ; x )$ obtained using a separate training set, and consider the following augmented IPCW (AIPCW) estimator:88

$$
\begin{array}{l} \hat {\theta} _ {A I P C W} = \frac {1}{n} \sum_ {i = 1} ^ {n} \widehat {S} _ {Y} (T; X _ {i}) \\ + \sum_ {t = 1} ^ {H _ {i} - 1} \frac {1}{\widehat {S} _ {C} (t ; X _ {i})} \left(\frac {\widehat {S} _ {Y} (T ; X _ {i})}{\widehat {S} _ {Y} (t ; X _ {i})} - \frac {\widehat {S} _ {Y} (T ; X _ {i})}{\widehat {S} _ {Y} (t - 1 ; X _ {i})}\right) \tag {16.30} \\ + \frac {\Delta_ {i}}{\widehat {S} _ {C} (H _ {i} ; X _ {i})} \left(1 (\{U _ {i} > T \}) - \frac {\widehat {S} _ {Y} (T ; X _ {i})}{\widehat {S} _ {Y} (H _ {i} - 1 ; X _ {i})}\right) \\ \end{array}
$$

Show that, under our setting, if furthermore

$$
\mathbb {E} \left[ \left(1 / \widehat {S} _ {C} (t; X _ {i}) - 1 / S _ {C} (t; X _ {i})\right) ^ {2} \right] = o _ {P} \left(n ^ {- 2 \alpha_ {C}}\right), \tag {16.31}
$$

$$
\mathbb {E} \left[ \left(1 / \widehat {S} _ {Y} (t; X _ {i}) - 1 / S _ {Y} (t; X _ {i})\right) ^ {2} \right] = o _ {P} \left(n ^ {- 2 \alpha_ {Y}}\right),
$$

for constants $\alpha _ { C }$ , $\alpha _ { Y } \geq 0$ with $\alpha _ { C } + \alpha _ { Y } \ge 1 / 2$ , then

$$
\begin{array}{l} \sqrt {n} \left(\hat {\theta} _ {A I P C W} - \theta\right) \Rightarrow \mathcal {N} \left(0, \sigma_ {A I P C W} ^ {2}\right) \\ \sigma_ {A I P C W} ^ {2} = \operatorname {V a r} \left[ S _ {Y} \left(T; X _ {i}\right) \right] \tag {16.32} \\ + \sum_ {t = 1} ^ {T} \mathbb {E} \left[ \frac {S _ {Y} ^ {2} (T ; X _ {i})}{S _ {C} (t ; X _ {i})} \frac {S _ {Y} (t - 1 ; X _ {i}) - S _ {Y} (t ; X _ {i})}{S _ {Y} (t - 1 ; X _ {i}) S _ {Y} (t ; X _ {i})} \right]. \\ \end{array}
$$

Hint: This result is a corollary of Theorem 14.4. To establish this, imagine an analogous dynamic policy evaluation problem where there is no censoring; however, all units start under the status-quo treatment, but then transition to an experimental treatment at time $C _ { i }$ if they are still alive. Argue that estimating $\theta$ in the setting of this question is equivalent to estimating $\mathbb { P } _ { \pi _ { 0 } } \left[ Y _ { i } > T \right]$ for the analogous dynamic policy evaluation setting with $\pi _ { 0 }$ corresponding to the policy that never starts the experimental treatment; and that $\hat { \theta } _ { A I P C W }$ is equivalent to the doubly robust estimator $\widehat { V } _ { A I P W } ( \pi _ { 0 } )$ derived in Chapter 14. Thus statistical properties of $\hat { \theta } _ { A I P C W }$ can be derived from Theorem 14.4.

# Bibliography

Alberto Abadie. Semiparametric instrumental variable estimation of treatment response models. Journal of Econometrics, 113(2):231‚Äì263, 2003.   
Alberto Abadie and Javier Gardeazabal. The economic costs of conflict: A case study of the Basque country. American Economic Review, 93(1):113‚Äì 132, 2003.   
Alberto Abadie and Guido W Imbens. Large sample properties of matching estimators for average treatment effects. Econometrica, 74(1):235‚Äì267, 2006.   
Alberto Abadie and Guido W Imbens. Matching on the estimated propensity score. Econometrica, 84(2):781‚Äì807, 2016.   
Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for comparative case studies: Estimating the effect of california‚Äôs tobacco control program. Journal of the American Statistical Association, 105(490):493‚Äì505, 2010.   
Alberto Abadie, Susan Athey, Guido W Imbens, and Jeffrey M Wooldridge. When should you adjust standard errors for clustering? The Quarterly Journal of Economics, 138(1):1‚Äì35, 2023.   
Anish Agarwal, Devavrat Shah, Dennis Shen, and Dogyoon Song. On robustness of principal component regression. Journal of the American Statistical Association, 116(536):1731‚Äì1745, 2021.   
Shipra Agrawal and Navin Goyal. Near-optimal regret bounds for Thompson sampling. Journal of the ACM, 64(5):1‚Äì24, 2017.   
Takeshi Amemiya. The nonlinear two-stage least-squares estimator. Journal of Econometrics, 2(2):105‚Äì110, 1974.   
Isaiah Andrews, James H Stock, and Liyang Sun. Weak instruments in instrumental variables regression: Theory and practice. Annual Review of Economics, 11(1):727‚Äì753, 2019.

Joshua D Angrist. Lifetime earnings and the Vietnam era draft lottery: Evidence from social security administrative records. American Economic Review, 80(3):313‚Äì336, 1990.   
Joshua D Angrist and Alan B Krueger. Split-sample instrumental variables estimates of the return to schooling. Journal of Business & Economic Statistics, 13(2):225‚Äì235, 1995.   
Joshua D Angrist, Guido W Imbens, and Donald B Rubin. Identification of causal effects using instrumental variables. Journal of the American Statistical Association, 91(434):444‚Äì455, 1996.   
Joshua D Angrist, Kathryn Graddy, and Guido W Imbens. The interpretation of instrumental variables estimators in simultaneous equations models with an application to the demand for fish. The Review of Economic Studies, 67 (3):499‚Äì527, 2000.   
Kevin Arceneaux, Alan S Gerber, and Donald P Green. Comparing experimental and matching methods using a large-scale voter mobilization experiment. Political Analysis, 14(1):37‚Äì62, 2006.   
Manuel Arellano. Panel Data Econometrics. Oxford university press, 2003.   
Dmitry Arkhangelsky and David Hirshberg. Large-sample properties of the synthetic control method under selection on unobservables. arXiv preprint arXiv:2311.13575, 2023.   
Dmitry Arkhangelsky and Guido Imbens. Causal models for longitudinal and panel data: A survey. The Econometrics Journal, 27(3):C1‚ÄìC61, 2024.   
Dmitry Arkhangelsky, Susan Athey, David A Hirshberg, Guido W Imbens, and Stefan Wager. Synthetic difference-in-differences. American Economic Review, 111(12):4088‚Äì4118, 2021.   
Timothy B Armstrong and Michal Koles¬¥ar. Optimal inference in a class of regression models. Econometrica, 86(2):655‚Äì683, 2018.   
Timothy B Armstrong and Michal Koles¬¥ar. Simple and honest confidence intervals in nonparametric regression. Quantitative Economics, 11(1):1‚Äì39, 2020.   
Peter M Aronow. A general method for detecting interference between units in randomized experiments. Sociological Methods & Research, 41(1):3‚Äì16, 2012.

Peter M Aronow and Allison Carnegie. Beyond LATE: Estimation of the average treatment effect with an instrumental variable. Political Analysis, 21 (4):492‚Äì506, 2013.   
Peter M Aronow and Cyrus Samii. Estimating average causal effects under general interference, with application to a social network experiment. The Annals of Applied Statistics, 11(4):1912‚Äì1947, 2017.   
Peter M Aronow, Donald P Green, and Donald KK Lee. Sharp bounds on the variance in randomized experiments. The Annals of Statistics, 42(3): 850‚Äì871, 2014.   
Susan Athey and Guido W Imbens. Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27): 7353‚Äì7360, 2016.   
Susan Athey and Guido W Imbens. Design-based analysis in difference-indifferences settings with staggered adoption. Journal of Econometrics, 226 (1):62‚Äì79, 2022.   
Susan Athey and Stefan Wager. Estimating treatment effects with causal forests: An application. Observational Studies, 5:36‚Äì51, 2019.   
Susan Athey and Stefan Wager. Policy learning with observational data. Econometrica, 89(1):133‚Äì161, 2021.   
Susan Athey, Dean Eckles, and Guido W Imbens. Exact p-values for network interference. Journal of the American Statistical Association, 113(521):230‚Äì 240, 2018a.   
Susan Athey, Guido W Imbens, and Stefan Wager. Approximate residual balancing: Debiased inference of average treatment effects in high dimensions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(4):597‚Äì623, 2018b.   
Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. The Annals of Statistics, 47(2):1148‚Äì1178, 2019.   
Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi. Matrix completion methods for causal panel data models. Journal of the American Statistical Association, 116(536):1716‚Äì1730, 2021.

Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235‚Äì256, 2002.   
Jushan Bai. Panel data models with interactive fixed effects. Econometrica, 77(4):1229‚Äì1279, 2009.   
Pierre Baldi and Yosef Rinott. On normal approximations of distributions in terms of dependency graphs. The Annals of Probability, 17(4):1646‚Äì1650, 1989.   
Heejung Bang and James M Robins. Doubly robust estimation in missing data and causal inference models. Biometrics, 61(4):962‚Äì973, 2005.   
Guillaume W Basse, Avi Feller, and Panos Toulis. Randomization tests of causal effects under interference. Biometrika, 106(2):487‚Äì494, 2019.   
Hamsa Bastani and Mohsen Bayati. Online decision making with highdimensional covariates. Operations Research, 68(1):276‚Äì294, 2020.   
Eli Ben-Michael, Avi Feller, and Jesse Rothstein. The augmented synthetic control method. Journal of the American Statistical Association, 116(536): 1789‚Äì1803, 2021.   
Marianne Bertrand, Esther Duflo, and Sendhil Mullainathan. How much should we trust differences-in-differences estimates? The Quarterly Journal of Economics, 119(1):249‚Äì275, 2004.   
Dimitris Bertsimas and Nathan Kallus. From predictive to prescriptive analytics. Management Science, 66(3):1025‚Äì1044, 2020.   
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Optimal exploration‚Äìexploitation in a multi-armed bandit problem with non-stationary rewards. Stochastic Systems, 9(4):319‚Äì337, 2019.   
Peter J Bickel, Chris AJ Klaassen, Ya‚Äôacov Ritov, and Jon A Wellner. Efficient and adaptive estimation for semiparametric models. Johns Hopkins University Press Baltimore, 1993.   
Christopher Blattman, Donald P Green, Daniel Ortega, and Santiago Tob¬¥on. Place-based interventions at scale: The direct and spillover effects of policing and city services on crime. Journal of the European Economic Association, 19(4):2022‚Äì2051, 2021.

Adam Bloniarz, Hanzhong Liu, Cun-Hui Zhang, Jasjeet S Sekhon, and Bin Yu. Lasso adjustments of treatment effect estimates in randomized experiments. Proceedings of the National Academy of Sciences, 113(27):7383‚Äì7390, 2016.   
Gregor Boehl, Gavin Goy, and Felix Strobel. A structural investigation of quantitative easing. Review of Economics and Statistics, 106(4):1028‚Äì1044, 2024.   
Iavor Bojinov, David Simchi-Levi, and Jinglong Zhao. Design and analysis of switchback experiments. Management Science, 69(7):3759‚Äì3777, 2023.   
Kirill Borusyak, Xavier Jaravel, and Jann Spiess. Revisiting event study designs: Robust and efficient estimation. Review of Economic Studies, 91(6): 3253‚Äì3285, 2024.   
John Bound, David A Jaeger, and Regina M Baker. Problems with instrumental variables estimation when the correlation between the instruments and the endogenous explanatory variable is weak. Journal of the American Statistical Association, 90(430):443‚Äì450, 1995.   
Richard C Bradley. Basic properties of strong mixing conditions: A survey and some open questions. Probability Surveys, 2:107‚Äì144, 2005.   
Leo Breiman. Random forests. Machine Learning, 45(1):5‚Äì32, 2001.   
S¬¥ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends¬Æ in Machine Learning, 5(1):1‚Äì122, 2012.   
S¬¥ebastien Bubeck, R¬¥emi Munos, and Gilles Stoltz. Pure exploration in multiarmed bandits problems. In Proceedings of the 20th International Conference Algorithmic Learning Theory, pages 23‚Äì37. Springer, 2009.   
Andreas Buja, Lawrence Brown, Richard Berk, Edward George, Emil Pitkin, Mikhail Traskin, Kai Zhang, and Linda Zhao. Models as approximations I: Consequences illustrated with linear regression. Statistical Science, 34(4): 523‚Äì544, 2019.   
Jing Cai, Alain De Janvry, and Elisabeth Sadoulet. Social networks and the decision to insure. American Economic Journal: Applied Economics, 7(2): 81‚Äì108, 2015.   
Brantly Callaway and Pedro HC Sant‚ÄôAnna. Difference-in-differences with multiple time periods. Journal of Econometrics, 225(2):200‚Äì230, 2021.

Sebastian Calonico, Matias D Cattaneo, and Rocio Titiunik. Robust nonparametric confidence intervals for regression-discontinuity designs. Econometrica, 82(6):2295‚Äì2326, 2014.   
Sebastian Calonico, Matias D Cattaneo, and Max H Farrell. On the effect of bias estimation on coverage accuracy in nonparametric inference. Journal of the American Statistical Association, 113(522):767‚Äì779, 2018.   
Sebastian Calonico, Matias D Cattaneo, Max H Farrell, and Rocio Titiunik. Regression discontinuity designs using covariates. Review of Economics and Statistics, 101(3):442‚Äì451, 2019.   
David Card and Alan B Krueger. Minimum wages and employment: A case study of the fast-food industry in New Jersey and Pennsylvania. The American Economic Review, 84(4):772‚Äì793, 1994.   
Pedro Carneiro, James J Heckman, and Edward J Vytlacil. Estimating marginal returns to education. American Economic Review, 101(6):2754‚Äì 2781, 2011.   
Claes M Cassel, Carl E S¬®arndal, and Jan H Wretman. Some results on generalized difference estimation and generalized regression estimation for finite populations. Biometrika, 63(3):615‚Äì620, 1976.   
Juan Camilo Castillo, Dan Knoepfle, and Glen Weyl. Matching and pricing in ride hailing: Wild goose chases and how to solve them. Management Science, forthcoming, 2024.   
Gary Chamberlain. Asymptotic efficiency in estimation with conditional moment restrictions. Journal of Econometrics, 34(3):305‚Äì334, 1987.   
Gary Chamberlain. Efficiency bounds for semiparametric regression. Econometrica, 60(3):567‚Äì596, 1992.   
Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. Advances in Neural Information Processing Systems, 24, 2011.   
Xiaohong Chen. Large sample sieve estimation of semi-nonparametric models. Handbook of Econometrics, 6:5549‚Äì5632, 2007.   
Xiaohong Chen, Pedro HC Sant‚ÄôAnna, and Haitian Xie. Efficient differencein-differences and event study estimators. arXiv preprint arXiv:2506.17729, 2025.

Ming-Yen Cheng, Jianqing Fan, and James S Marron. On automatic boundary corrections. The Annals of Statistics, 25(4):1691‚Äì1708, 1997.   
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):1‚Äì68, 2018.   
Victor Chernozhukov, Juan Carlos Escanciano, Hidehiko Ichimura, Whitney K Newey, and James M Robins. Locally robust semiparametric estimation. Econometrica, 90(4):1501‚Äì1535, 2022a.   
Victor Chernozhukov, Whitney K Newey, and Rahul Singh. Automatic debiased machine learning of causal and structural effects. Econometrica, 90(3): 967‚Äì1027, 2022b.   
Victor Chernozhukov, Mert Demirer, Esther Duflo, and Iv¬¥an Fern¬¥andez-Val. Fisher‚ÄìSchultz lecture: Generic machine learning inference on heterogeneous treatment effects in randomized experiments, with an application to immunization in india. Econometrica, 93(4):1121‚Äì1164, 2025.   
Albert Chiu, Xingchen Lan, Ziyi Liu, and Yiqing Xu. Causal panel analysis under parallel trends: lessons from a large reanalysis study. American Political Science Review, forthcoming, 2025.   
Eunyi Chung and Joseph P Romano. Exact and asymptotically robust permutation tests. The Annals of Statistics, 41(2):484‚Äì507, 2013.   
Carlos Cinelli, Andrew Forney, and Judea Pearl. A crash course in good and bad controls. Sociological Methods & Research, 53(3):1071‚Äì1104, 2024.   
Peter L Cohen and Colin B Fogarty. Gaussian prepivoting for finite population causal inference. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(2):295‚Äì320, 2022.   
Bruno Cr¬¥epon, Esther Duflo, Marc Gurgand, Roland Rathelot, and Philippe Zamora. Do labor market policies have displacement effects? evidence from a clustered randomized experiment. The Quarterly Journal of Economics, 128(2):531‚Äì580, 2013.   
Yifan Cui, Michael R Kosorok, Erik Sverdrup, Stefan Wager, and Ruoqing Zhu. Estimating heterogeneous treatment effects with right-censored data via causal survival forests. Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(2):179‚Äì211, 2023.

Cl¬¥ement de Chaisemartin and Xavier d‚ÄôHaultfoeuille. Two-way fixed effects estimators with heterogeneous treatment effects. American Economic Review, 110(9):2964‚Äì2996, 2020.   
Rajeev H Dehejia and Sadek Wahba. Causal effects in nonexperimental studies: Reevaluating the evaluation of training programs. Journal of the American Statistical Association, 94(448):1053‚Äì1062, 1999.   
Alexis Diamond and Jasjeet S Sekhon. Genetic matching for estimating causal effects: A general multivariate matching method for achieving balance in observational studies. Review of Economics and Statistics, 95(3):932‚Äì945, 2013.   
Peng Ding. A paradox from randomization-based causal inference. Statistical Science, 32(3):331‚Äì345, 2017.   
Peng Ding, Avi Feller, and Luke Miratrix. Decomposing treatment effect variation. Journal of the American Statistical Association, 114(525):304‚Äì317, 2019.   
David L Donoho. Statistical estimation and optimal recovery. The Annals of Statistics, 22(1):238‚Äì270, 1994.   
Rick Durrett. Probability: Theory and Examples. Cambridge University Press, Cambridge, United Kingdom, 5th edition, 2019.   
Dean Eckles, Nikolaos Ignatiadis, Stefan Wager, and Han Wu. Noise-induced randomization in regression discontinuity designs. Biometrika, 112(2): asaf003, 2025.   
Bradley Efron. The Jackknife, the Bootstrap, and other Resampling Plans. Siam, 1982.   
Bradley Efron and David Feldman. Compliance as an explanatory variable in clinical trials. Journal of the American Statistical Association, 86(413):9‚Äì17, 1991.   
Lin Fan and Peter W Glynn. The fragility of optimized bandit algorithms. Operations Research, forthcoming, 2025.   
Max H Farrell. Robust inference on average treatment effects with possibly more covariates than observations. Journal of Econometrics, 189(1):1‚Äì23, 2015.

Amy Finkelstein, Sarah Taubman, Bill Wright, Mira Bernstein, Jonathan Gruber, Joseph P Newhouse, Heidi Allen, Katherine Baicker, and the Oregon Health Study Group. The Oregon health insurance experiment: Evidence from the first year. The Quarterly Journal of Economics, 127(3):1057‚Äì1106, 2012.   
Ronald A Fisher. The Design of Experiments. Oliver and Boyd, Edinburgh, 1935.   
Dylan J Foster and Vasilis Syrgkanis. Orthogonal statistical learning. The Annals of Statistics, 51(3):879‚Äì908, 2023.   
Constantine E Frangakis and Donald B Rubin. Principal stratification in causal inference. Biometrics, 58(1):21‚Äì29, 2002.   
David A Freedman. On tail probabilities for martingales. The Annals of Probability, 3(1):100‚Äì118, 1975.   
Sebastian Galiani, Paul Gertler, and Ernesto Schargrodsky. Water for life: The impact of the privatization of water services on child mortality. Journal of Political Economy, 113(1):83‚Äì120, 2005.   
Dan Geiger, Thomas Verma, and Judea Pearl. Identifying independence in Bayesian networks. Networks, 20(5):507‚Äì534, 1990.   
Andrew Gelman and Guido W Imbens. Why high-order polynomials should not be used in regression discontinuity designs. Journal of Business & Economic Statistics, 37(3):447‚Äì456, 2019.   
Aditya Ghosh, Guido Imbens, and Stefan Wager. PLRD: Partially linear regression discontinuity inference. arXiv preprint arXiv:2503.09907, 2025.   
John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society: Series B (Methodological), 41(2):148‚Äì164, 1979.   
Alexander Goldenshluger and Assaf Zeevi. A linear response bandit problem. Stochastic Systems, 3(1):230‚Äì261, 2013.   
Bryan S Graham, Cristine Campos de Xavier Pinto, and Daniel Egel. Inverse probability tilting for moment condition models with missing data. The Review of Economic Studies, 79(3):1053‚Äì1079, 2012.   
The INSIGHT START Study Group. Initiation of antiretroviral therapy in early asymptomatic HIV infection. The New England Journal of Medicine, 373(9):795‚Äì807, 2015.

Yonatan Gur, Ahmadreza Momeni, and Stefan Wager. Smoothness-adaptive contextual bandits. Operations Research, 70(6):3198‚Äì3216, 2022.   
Trygve Haavelmo. The statistical implications of a system of simultaneous equations. Econometrica, 11(1):1‚Äì12, 1943.   
Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. Proceedings of the National Academy of Sciences, 118(15), 2021.   
Jinyong Hahn. On the role of the propensity score in efficient semiparametric estimation of average treatment effects. Econometrica, 66(2):315‚Äì331, 1998.   
Jinyong Hahn, Petra Todd, and Wilbert van der Klaauw. Identification and estimation of treatment effects with a regression-discontinuity design. Econometrica, 69(1):201‚Äì209, 2001.   
P Richard Hahn, Jared S Murray, and Carlos M Carvalho. Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects. Bayesian Analysis, 15(3):965‚Äì1056, 2020.   
Jens Hainmueller. Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political Analysis, 20(1):25‚Äì46, 2012.   
Jaroslav H¬¥ajek. Local asymptotic minimax and admissibility in estimation. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Theory of Statistics, volume 6, pages 175‚Äì195. University of California Press, 1972.   
Jonathan V Hall, John J Horton, and Daniel T Knoepfle. Ride-sharing markets re-equilibrate. Technical report, National Bureau of Economic Research, 2023.   
M Elizabeth Halloran and Claudio J Struchiner. Causal inference in infectious diseases. Epidemiology, 6(2):142‚Äì151, 1995.   
Bruce Hansen. Econometrics. Princeton University Press, 2022.   
Christopher Harshaw, Fredrik S¬®avje, and Yitan Wang. A design-based Riesz representation framework for randomized experiments. arXiv preprint arXiv:2210.08698, 2022.

Trevor Hastie, Robert Tibshirani, and Jerome H Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2 edition, 2009.   
James J Heckman. Sample selection bias as a specification error. Econometrica, 47(1):153‚Äì161, 1979.   
James J Heckman and Edward J Vytlacil. Local instrumental variables and latent variable models for identifying and bounding treatment effects. Proceedings of the National Academy of Sciences, 96(8):4730‚Äì4734, 1999.   
James J Heckman and Edward J Vytlacil. Structural equations, treatment effects, and econometric policy evaluation. Econometrica, 73(3):669‚Äì738, 2005.   
Inge S Helland. Central limit theorems for martingales with discrete or continuous time. Scandinavian Journal of Statistics, 9(2):79‚Äì94, 1982.   
Miguel A Hern¬¥an and James M Robins. Causal Inference: What If. Chapman & Hall/CRC, Boca Raton, 2020.   
Keisuke Hirano and Jack R Porter. Asymptotics for statistical treatment rules. Econometrica, 77(5):1683‚Äì1701, 2009.   
Keisuke Hirano and Jack R Porter. Asymptotic representations for sequential decisions, adaptive experiments, and batched bandits. arXiv preprint arXiv:2302.03117, 2023.   
Keisuke Hirano, Guido W Imbens, and Geert Ridder. Efficient estimation of average treatment effects using the estimated propensity score. Econometrica, 71(4):1161‚Äì1189, 2003.   
David A Hirshberg and Stefan Wager. Augmented minimax linear estimation. The Annals of Statistics, 49(6):3206‚Äì3227, 2021.   
Paul W Holland. Statistics and causal inference. Journal of the American Statistical Association, 81(396):945‚Äì960, 1986.   
Steven R Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. Timeuniform, nonparametric, nonasymptotic confidence sequences. The Annals of Statistics, 49(2):1055‚Äì1080, 2021.   
Yichun Hu, Nathan Kallus, and Xiaojie Mao. Smooth contextual bandits: Bridging the parametric and nondifferentiable regret regimes. Operations Research, 70(6):3261‚Äì3281, 2022a.

Yuchen Hu and Stefan Wager. Switchback experiments under geometric mixing. arXiv preprint arXiv:2209.00197, 2022.   
Yuchen Hu and Stefan Wager. Off-policy evaluation in partially observed Markov decision processes under sequential ignorability. The Annals of Statistics, 51(4):1561‚Äì1585, 2023.   
Yuchen Hu, Shuangning Li, and Stefan Wager. Average direct and indirect causal effects under interference. Biometrika, 109(4):1165‚Äì1172, 2022b.   
Michael G Hudgens and M Elizabeth Halloran. Toward causal inference with interference. Journal of the American Statistical Association, 103(482):832‚Äì 842, 2008.   
Stefano M Iacus, Gary King, and Giuseppe Porro. Causal inference without balance checking: Coarsened exact matching. Political Analysis, 20(1):1‚Äì24, 2012.   
Kosuke Imai and Michael Lingzhi Li. Experimental evaluation of individualized treatment rules. Journal of the American Statistical Association, 118(541): 242‚Äì256, 2023.   
Kosuke Imai and Marc Ratkovic. Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1): 243‚Äì263, 2014.   
Guido W Imbens. Nonparametric estimation of average treatment effects under exogeneity: A review. Review of Economics and Statistics, 86(1):4‚Äì29, 2004.   
Guido W Imbens. Instrumental variables: An econometrician‚Äôs perspective. Statistical Science, 29(3):323‚Äì358, 2014.   
Guido W Imbens. Potential outcome and directed acyclic graph approaches to causality: Relevance for empirical practice in economics. Journal of Economic Literature, 58(4):1129‚Äì1179, 2020.   
Guido W Imbens and Joshua D Angrist. Identification and estimation of local average treatment effects. Econometrica, 62(2):467‚Äì475, 1994.   
Guido W Imbens and Karthik Kalyanaraman. Optimal bandwidth choice for the regression discontinuity estimator. The Review of Economic Studies, 79 (3):933‚Äì959, 2012.

Guido W Imbens and Thomas Lemieux. Regression discontinuity designs: A guide to practice. Journal of Econometrics, 142(2):615‚Äì635, 2008.   
Guido W Imbens and Charles F Manski. Confidence intervals for partially identified parameters. Econometrica, 72(6):1845‚Äì1857, 2004.   
Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press, 2015.   
Guido W Imbens and Stefan Wager. Optimized regression discontinuity designs. Review of Economics and Statistics, 101(2):264‚Äì278, 2019.   
Hemant Ishwaran, Udaya B Kogalur, Eugene H Blackstone, and Michael S Lauer. Random survival forests. The Annals of Applied Statistics, pages 841‚Äì860, 2008.   
Adel Javanmard and Andrea Montanari. Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research, 15(1):2869‚Äì2909, 2014.   
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In International Conference on Machine Learning, 2016.   
Nathan Kallus. Generalized optimal matching methods for causal inference. Journal of Machine Learning Research, 21(62):1‚Äì54, 2020.   
Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evaluation in Markov decision processes. Journal of Machine Learning Research, 21(167):1‚Äì63, 2020.   
Nathan Kallus and Masatoshi Uehara. Efficiently breaking the curse of horizon in off-policy evaluation with double reinforcement learning. Operations Research, 70(6):3282‚Äì3302, 2022.   
Nathan Kallus and Angela Zhou. Minimax-optimal policy learning under unobserved confounding. Management Science, 67(5):2870‚Äì2890, 2021.   
Edward L Kaplan and Paul Meier. Nonparametric estimation from incomplete observations. Journal of the American Statistical Association, 53(282):457‚Äì 481, 1958.   
Maximilian Kasy and Anja Sautmann. Adaptive treatment assignment in experiments for policy choice. Econometrica, 89(1):113‚Äì132, 2021.

Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal effects. Electronic Journal of Statistics, 17(2):3008‚Äì3049, 2023.   
Edward H Kennedy, Scott Lorch, and Dylan S Small. Robust causal inference with continuous instruments using the local instrumental variable curve. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 81(1):121‚Äì143, 2019.   
Edward H Kennedy, Sivaraman Balakrishnan, James M Robins, and Larry Wasserman. Minimax rates for heterogeneous causal effect estimation. The Annals of Statistics, 52(2):793‚Äì816, 2024.   
Toru Kitagawa and Aleksey Tetenov. Who should be treated? empirical welfare maximization methods for treatment choice. Econometrica, 86(2):591‚Äì616, 2018.   
Denis Kojevnikov, Vadim Marmer, and Kyungchul Song. Limit theorems for network dependent random variables. Journal of Econometrics, 222(2):882‚Äì 908, 2021.   
Michal Koles¬¥ar and Christoph Rothe. Inference in regression discontinuity designs with a discrete running variable. American Economic Review, 108 (8):2277‚Äì2304, 2018.   
X. Kuang and Stefan Wager. Weak signal asymptotics for sequentially randomized experiments. Management Science, 70(10):7024‚Äì7041, 2024.   
S¬®oren R K¬®unzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the National Academy of Sciences, 116(10):4156‚Äì4165, 2019.   
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4‚Äì22, 1985.   
Robert J LaLonde. Evaluating the econometric evaluations of training programs with experimental data. American Economic Review, 76(4):604‚Äì620, 1986.   
David S Lee. Randomized experiments from non-random selection in US House elections. Journal of Econometrics, 142(2):675‚Äì697, 2008.   
Lihua Lei and Emmanuel J Cand`es. Conformal inference of counterfactuals and individual treatment effects. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(5):911‚Äì938, 2021.

Lihua Lei and Peng Ding. Regression adjustment in completely randomized experiments with a diverging number of covariates. Biometrika, 108(4):815‚Äì 828, 2021.   
Lihua Lei and Brad Ross. Estimating counterfactual matrix means with short panel data. arXiv preprint arXiv:2312.07520, 2023.   
Michael P Leung. Causal inference under approximate neighborhood interference. Econometrica, 90(1):267‚Äì293, 2022.   
Shuangning Li and Stefan Wager. Random graph asymptotics for treatment effect estimation under network interference. The Annals of Statistics, 50 (4):2334‚Äì2358, 2022.   
Xinran Li and Peng Ding. General forms of finite population central limit theorems with applications to causal inference. Journal of the American Statistical Association, 112(520):1759‚Äì1769, 2017.   
Peng Liao, Predrag Klasnja, and Susan Murphy. Off-policy estimation of longterm average outcomes with applications to mobile health. Journal of the American Statistical Association, 116(533):382‚Äì391, 2021.   
Peng Liao, Zhengling Qi, Runzhe Wan, Predrag Klasnja, and Susan A Murphy. Batch policy learning in average reward Markov decision processes. The Annals of Statistics, 50(6):3364‚Äì3387, 2022.   
Winston Lin. Agnostic notes on regression adjustments to experimental data: Reexamining Freedman‚Äôs critique. The Annals of Applied Statistics, 7(1): 295‚Äì318, 2013.   
Yueyang Liu, Benjamin Van Roy, and Kuang Xu. Nonstationary bandit learning via predictive sampling. In Proceedings of the International Conference on Artificial Intelligence and Statistics, pages 6215‚Äì6244. PMLR, 2023.   
Alex Luedtke and Antoine Chambaz. Performance guarantees for policy learning. Annales de l‚ÄôInstitut Henri Poincar¬¥e, Probabilit¬¥es et Statistiques, 56(3): 2162‚Äì2188, 2020.   
Alexander R Luedtke and Mark J van der Laan. Statistical inference for the mean outcome under a possibly non-unique optimal treatment strategy. The Annals of Statistics, 44(2):713, 2016.   
Charles F Manski. Statistical treatment rules for heterogeneous populations. Econometrica, 72(4):1221‚Äì1246, 2004.

Charles F Manski. Identification of treatment response with social interactions. The Econometrics Journal, 16(1):S1‚ÄìS23, 2013.   
Ruth Marcus, Eric Peritz, and K R Gabriel. On closed testing procedures with special reference to ordered analysis of variance. Biometrika, 63(3):655‚Äì660, 1976.   
Eric Mbakop and Max Tabord-Meehan. Model selection for treatment choice: Penalized welfare maximization. Econometrica, 89(2):825‚Äì848, 2021.   
Alec McClean, Sivaraman Balakrishnan, Edward H Kennedy, and Larry Wasserman. Double cross-fit doubly robust estimators: Beyond series regression. arXiv preprint arXiv:2403.15175, 2024.   
Mohammad Mehrabi and Stefan Wager. Off-policy evaluation in markov decision processes under weak distributional overlap. arXiv preprint arXiv:2402.08201, 2024.   
Nicolai Meinshausen, Alain Hauser, Joris M Mooij, Jonas Peters, Philip Versteeg, and Peter B¬®uhlmann. Methods for causal inference from gene perturbation experiments and validation. Proceedings of the National Academy of Sciences, 113(27):7361‚Äì7368, 2016.   
Luke W Miratrix, Jasjeet S Sekhon, and Bin Yu. Adjusting treatment effect estimates by post-stratification in randomized experiments. Journal of the Royal Statistical Society Series B: Statistical Methodology, 75(2):369‚Äì396, 2013.   
Magne Mogstad and Alexander Torgovitsky. Instrumental variables with unobserved heterogeneity in treatment effects. Handbook of Labor Economics, 5:1‚Äì114, 2024.   
Kari Lock Morgan and Donald B Rubin. Rerandomization to improve covariate balance in experiments. Annals of Statistics, 40(2):1263‚Äì1282, 2012.   
Evan Munro, X. Kuang, and Stefan Wager. Treatment effects in market equilibrium. American Economic Review, 115(10):3273‚Äì3321, 2025.   
Susan A Murphy. A generalization error for Q-learning. Journal of Machine Learning Research, 6(Jul):1073‚Äì1097, 2005.   
Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. Statistical Science, 27(4):538‚Äì557, 2012.

Whitney K Newey. Efficient instrumental variables estimation of nonlinear models. Econometrica, 58(4):809‚Äì837, 1990.   
Whitney K Newey. The asymptotic variance of semiparametric estimators. Econometrica, 62(6):1349‚Äì1382, 1994.   
Whitney K Newey and James L Powell. Instrumental variable estimation of nonparametric models. Econometrica, 71(5):1565‚Äì1578, 2003.   
Whitney K Newey and James R Robins. Cross-fitting and fast remainder rates for semiparametric estimation. arXiv preprint arXiv:1801.09138, 2018.   
Jersey Neyman. Sur les applications de la th¬¥eorie des probabilit¬¥es aux experiences agricoles: Essai des principes. Roczniki Nauk Rolniczych, 10:1‚Äì51, 1923.   
Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299‚Äì319, 2021.   
Xinkun Nie, Xiaoying Tian, Jonathan Taylor, and James Zou. Why adaptively collected data have negative bias and how to correct for it. In International Conference on Artificial Intelligence and Statistics, pages 1261‚Äì1269. PMLR, 2018.   
Claudia Noack and Christoph Rothe. Bias-aware inference in fuzzy regression discontinuity designs. Econometrica, 92(3):687‚Äì711, 2024.   
Elizabeth L Ogburn and Tyler J VanderWeele. Vaccines, contagion, and social networks. Annals of Applied Statistics, 11(2):919‚Äì948, 2017.   
Elizabeth L Ogburn, Oleg Sofrygin, Ivan Diaz, and Mark J Van der Laan. Causal inference for social network data. Journal of the American Statistical Association, 119(545):597‚Äì611, 2024.   
Judea Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669‚Äì 688, 1995.   
Judea Pearl. Causality. Cambridge University Press, 2009.   
Judea Pearl and Dana Mackenzie. The Book of Why: The New Science of Cause and Effect. Basic Books, 2018.   
Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. The Annals of Statistics, 41(2):693‚Äì721, 2013.

Jonas Peters, Peter B¬®uhlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(5):947‚Äì 1012, 2016.   
Chao Qin and Daniel Russo. Optimizing adaptive experiments: A unified approach to regret minimization and best-arm identification. arXiv preprint arXiv:2402.10592, 2024.   
Chao Qin and Daniel Russo. Adaptive experimentation in the presence of exogenous nonstationary variation. Management Science, forthcoming, 2025.   
Thomas S Richardson and Andrea Rotnitzky. Causal etiology of the research of James M. Robins. Statistical Science, 29(4):459‚Äì484, 2014.   
Herbert Robbins. Statistical methods related to the law of the iterated logarithm. The Annals of Mathematical Statistics, 41(5):1397‚Äì1409, 1970.   
Christian P. Robert and George Casella. Monte Carlo Statistical Methods. Springer Science $^ +$ Business Media, New York, 2nd edition, 2004.   
James Robins, Mariela Sued, Quanhong Lei-Gomez, and Andrea Rotnitzky. Comment: Performance of double-robust estimators when ‚Äúinverse probability‚Äù weights are highly variable. Statistical Science, 22(4):544‚Äì559, 2007.   
James M Robins. A new approach to causal inference in mortality studies with a sustained exposure period: Application to control of the healthy worker survivor effect. Mathematical Modelling, 7(9-12):1393‚Äì1512, 1986.   
James M Robins. Correcting for non-compliance in randomized trials using structural nested mean models. Communications in Statistics: Theory and Methods, 23(8):2379‚Äì2412, 1994.   
James M Robins. Association, causation, and marginal structural models. Synthese, 121(1/2):151‚Äì179, 1999.   
James M Robins. Optimal structural nested models for optimal sequential decisions. In Proceedings of the second seattle Symposium in Biostatistics, pages 189‚Äì326. Springer, 2004.   
James M Robins and Thomas S Richardson. Alternative graphical causal models and the identification of direct effects. Causality and Psychopathology: Finding the Determinants of Disorders and their Cures, pages 103‚Äì158, 2010.

James M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with missing data. Journal of the American Statistical Association, 90(429):122‚Äì129, 1995.   
James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients when some regressors are not always observed. Journal of the American Statistical Association, 89(427):846‚Äì866, 1994.   
James M Robins, Lingling Li, Rajarshi Mukherjee, Eric Tchetgen Tchetgen, and Aad van der Vaart. Minimax estimation of a functional on a structured high-dimensional model. The Annals of Statistics, 45(5):1951‚Äì1987, 2017.   
Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica, 56(4):931‚Äì954, 1988.   
Todd Rogers and Avi Feller. Reducing student absences at scale by targeting parents‚Äô misbeliefs. Nature Human Behaviour, 2(5):335‚Äì342, 2018.   
Joseph P Romano. On the behavior of randomization tests without a group invariance assumption. Journal of the American Statistical Association, 85 (411):686‚Äì692, 1990.   
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41‚Äì55, 1983.   
Paul R Rosenbaum and Donald B Rubin. Reducing bias in observational studies using subclassification on the propensity score. Journal of the American Statistical Association, 79(387):516‚Äì524, 1984.   
Eric L Ross, Robert M Bossarte, Steven K Dobscha, Sarah M Gildea, Irving Hwang, Chris J Kennedy, Howard Liu, Alex Luedtke, Brian P Marx, Matthew K Nock, et al. Estimated average treatment effect of psychiatric hospitalization in patients with suicidal behaviors: a precision treatment analysis. JAMA psychiatry, 81(2):135‚Äì143, 2024.   
Andrew D Roy. Some thoughts on the distribution of earnings. Oxford Economic Papers, 3(2):135‚Äì146, 1951.   
Daniel Rubin and Mark J van der Laan. A doubly robust censoring unbiased transformation. The International Journal of Biostatistics, 3(1), 2007.   
Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology, 66(5):688, 1974.

Daniel Russo. Simple Bayesian algorithms for best-arm identification. Operations Research, 68(6):1625‚Äì1647, 2020.   
Daniel Russo and Benjamin Van Roy. Learning to optimize via informationdirected sampling. Operations Research, 66(1):230‚Äì252, 2018.   
Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on Thompson sampling. Foundations and Trends in Machine Learning, 11(1):1‚Äì96, 2018.   
Jerome Sacks and Donald Ylvisaker. Linear estimation for approximately linear models. The Annals of Statistics, 6(5):1122‚Äì1137, 1978.   
Fredrik S¬®avje. Causal inference with misspecified exposure mappings: Separating definitions and assumptions. Biometrika, 111(1):1‚Äì15, 2024.   
Fredrik S¬®avje, Peter Aronow, and Michael Hudgens. Average treatment effects in the presence of unknown interference. The Annals of Statistics, 49(2):673, 2021.   
Daniel O Scharfstein, Andrea Rotnitzky, and James M Robins. Adjusting for nonignorable drop-out using semiparametric nonresponse models. Journal of the American Statistical Association, 94(448):1096‚Äì1120, 1999.   
Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising using multi-armed bandit experiments. Marketing Science, 36(4):500‚Äì522, 2017.   
Dennis Shen, Peng Ding, Jasjeet Sekhon, and Bin Yu. Same root different leaves: Time series and cross-sectional methods in panel data. Econometrica, 91(6):2125‚Äì2154, 2023.   
Michael E Sobel. What do randomized studies of housing mobility demonstrate? causal inference in the face of interference. Journal of the American Statistical Association, 101(476):1398‚Äì1407, 2006.   
Peter Spirtes, Clark N Glymour, and Richard Scheines. Causation, Prediction, and Search. Springer-Verlag, New York, 1993.   
Charles M Stein. Estimation of the mean of a multivariate normal distribution. The Annals of Statistics, 9(6):1135‚Äì1151, 1981.   
Charles J Stone. Consistent nonparametric regression. The Annals of Statistics, 5(4):595‚Äì620, 1977.

J¬®org Stoye. Minimax regret treatment choice with finite samples. Journal of Econometrics, 151(1):70‚Äì81, 2009.   
Hao Sun, Evan Munro, Georgy Kalashnov, Shuyang Du, and Stefan Wager. Treatment allocation under uncertain costs. arXiv preprint arXiv:2103.11066, 2021.   
Liyang Sun and Sarah Abraham. Estimating dynamic treatment effects in event studies with heterogeneous treatment effects. Journal of Econometrics, 225 (2):175‚Äì199, 2021.   
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9‚Äì44, 1988.   
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2nd edition, 2018.   
Erik Sverdrup, Han Wu, Susan Athey, and Stefan Wager. Qini curves for multiarmed treatment rules. Journal of Computational and Graphical Statistics, forthcoming, 2025.   
Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback through counterfactual risk minimization. The Journal of Machine Learning Research, 16(1):1731‚Äì1755, 2015.   
Zhiqiang Tan. Model-assisted inference for treatment effects using regularized calibrated estimation with high-dimensional data. The Annals of Statistics, 48(2):811‚Äì837, 2020.   
Donald L Thistlethwaite and Donald T Campbell. Regression-discontinuity analysis: An alternative to the ex post facto experiment. Journal of Educational Psychology, 51(6):309‚Äì317, 1960.   
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139‚Äì2148, 2016.   
William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285‚Äì294, 1933.   
Lu Tian, Ash A Alizadeh, Andrew J Gentles, and Robert Tibshirani. A simple method for estimating interactions between a treatment and a large number

of covariates. Journal of the American Statistical Association, 109(508): 1517‚Äì1532, 2014.   
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267‚Äì288, 1996.   
Anastasios A Tsiatis. Semiparametric theory and missing data. Springer, New York, 2006.   
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674‚Äì690, 1997.   
Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and qfunction learning for off-policy evaluation. In Hal Daum¬¥e III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9659‚Äì9668. PMLR, 2020.   
Mark J van der Laan and James M Robins. Unified methods for censored longitudinal data and causality. Springer, New York, 2003.   
Mark J van der Laan and Sherri Rose. Targeted learning: Causal inference for observational and experimental data. Springer Science & Business Media, 2011.   
Mark J van der Laan and Daniel Rubin. Targeted maximum likelihood learning. The International Journal of Biostatistics, 2(1), 2006.   
Aad W Van der Vaart. Asymptotic Statistics. Cambridge University Press, 1998.   
Davide Viviano. Policy targeting under network interference. Review of Economic Studies, forthcoming, 2024.   
Edward Vytlacil. Independence, monotonicity, and latent index models: An equivalence result. Econometrica, 70(1):331‚Äì341, 2002.   
Stefan Wager. On regression tables for policy learning: Comment on a paper by Jiang, Song, Li and Zeng. Statistica Sinica, 29(4):1678‚Äì1685, 2019.   
Stefan Wager, Wenfei Du, Jonathan Taylor, and Robert J Tibshirani. Highdimensional regression adjustments in randomized experiments. Proceedings of the National Academy of Sciences, 113(45):12673‚Äì12678, 2016.

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8: 279‚Äì292, 1992.   
Halbert White. A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. Econometrica, 48(4):817‚Äì838, 1980.   
Halbert White. Asymptotic Theory for Econometricians. Economic Theory, Econometrics, and Mathematical Economics. Academic Press, Orlando, Florida, 1984.   
Jeffrey M Wooldridge. Econometric Analysis of Cross Section and Panel Data. MIT press, 2010.   
Jeffrey M Wooldridge. Two-way fixed effects, the two-way Mundlak regression, and difference-in-differences estimators. Empirical Economics, forthcoming, 2025.   
Sewall Wright. The method of path coefficients. The Annals of Mathematical Statistics, 5(3):161‚Äì215, 1934.   
Han Wu and Stefan Wager. Thompson sampling with unrestricted delays. In Proceedings of the 23rd ACM Conference on Economics and Computation, pages 937‚Äì955, 2022.   
Yiqing Xu. Generalized synthetic control method: Causal inference with interactive fixed effects models. Political Analysis, 25(1):57‚Äì76, 2017.   
Steve Yadlowsky, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. Evaluating treatment prioritization rules via rank-weighted average treatment effects. Journal of the American Statistical Association, forthcoming, 2025.   
Baqun Zhang, Anastasios A Tsiatis, Eric B Laber, and Marie Davidian. Robust estimation of optimal dynamic treatment regimes for sequential treatment decisions. Biometrika, 100(3):681‚Äì694, 2013.   
Cun-Hui Zhang and Stephanie S Zhang. Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):217‚Äì242, 2014.   
Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in Neural Information Processing Systems, 33:9818‚Äì9829, 2020.

Qingyuan Zhao. Covariate balancing propensity score by tailored loss functions. The Annals of Statistics, 47(2):965‚Äì993, 2019.   
Qingyuan Zhao, Dylan S Small, and Ashkan Ertefaie. Selective inference for effect modification via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(2):382‚Äì413, 2022.   
Yingqi Zhao, Donglin Zeng, A John Rush, and Michael R Kosorok. Estimating individualized treatment rules using outcome weighted learning. Journal of the American Statistical Association, 107(499):1106‚Äì1118, 2012.   
Zhengyuan Zhou, Susan Athey, and Stefan Wager. Offline multi-action policy learning: Generalization and optimization. Operations Research, 71(1):148‚Äì 183, 2023.   
Jos¬¥e R Zubizarreta. Using mixed integer programming for matching in an observational study of kidney failure after surgery. Journal of the American Statistical Association, 107(500):1360‚Äì1371, 2012.   
Jos¬¥e R Zubizarreta. Stable weights that balance covariates for estimation with incomplete outcome data. Journal of the American Statistical Association, 110(511):910‚Äì922, 2015.

# CAUSAL INFERENCE IN STATISTICS

![](images/3740db79dad0e27f43262528121f4caed724ec2109f2f055830ccfd250d85d75.jpg)

![](images/3500294493e770af4cfa8293ef2a9bd0645037aab2f91444f8ab457ec99cebac.jpg)

![](images/65f460c095f357ce84fcb1fa22fef867d913193fb58f1bbd7973affc12143af3.jpg)

![](images/f5ef0b84febea0f84f9edb3c97b094423904a60c73e4baa371e77179a14c9537.jpg)

![](images/832bae4a8e8ef988b3627b8661d36d83ad1a463be11ba9eecb787dd6a69c864b.jpg)

![](images/09ad283233496708e9a7dd2db8197e61ff9ea01928fddaf0a431b2bbf47a4e0a.jpg)

# CAUSAL INFERENCE IN STATISTICS

A PRIMER

Judea Pearl

Computer Science and Statistics, University of California, Los Angeles, USA

Madelyn Glymour

Philosophy, Carnegie Mellon University, Pittsburgh, USA

Nicholas P. Jewell

Biostatistics and Statistics, University of California, Berkeley, USA

WILEY

This edition -rst published 2016

$¬©$ 2016 John Wiley & Sons Ltd

Registered of-ce

John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom

For details of our global editorial of-ces, for customer services and for information about how to apply for permission to reuse the copyright material in this book please see our website at www.wiley.com.

The right of the author to be identi-ed as the author of this work has been asserted in accordance with the Copyright, Designs and Patents Act 1988.

All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.

Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books.

Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and product names used in this book are trade names, service marks, trademarks or registered trademarks of their respective owners. The publisher is not associated with any product or vendor mentioned in this book.

Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing this book, they make no representations or warranties with respect to the accuracy or completeness of the contents of this book and speci-cally disclaim any implied warranties of merchantability or -tness for a particular purpose. It is sold on the understanding that the publisher is not engaged in rendering professional services and neither the publisher nor the author shall be liable for damages arising herefrom. If professional advice or other expert assistance is required, the services of a competent professional should be sought.

Library of Congress Cataloging-in-Publication Data applied for

ISBN: 9781119186847

A catalogue record for this book is available from the British Library.

Cover Image: $¬©$ gmaydos/Getty

Typeset in 10/12pt TimesLTStd by SPi Global, Chennai, India

10 9 8 7 6 5 4 3 2

To my wife, Ruth, my greatest mentor.

‚Äì Judea Pearl

To my parents, who are the causes of me.

‚Äì Madelyn Glymour

To Debra and Britta, who inspire me every day.

‚Äì Nicholas P. Jewell

![](images/586a45295e5b11455b709234470ec31e64dd2d63df73dd3fb027add39c2db5f3.jpg)

![](images/a61077d7bdbde89acc8efe4ad3b5404c475841f1cec438263e285720651cf7b7.jpg)

![](images/7165015d55776e5ecdfc204ab6f78f57d18088535ba5ee0cf46c3b69deeb47a8.jpg)

![](images/12714444e4bc72a36d1eacffc8236913c9efb1ee0f900c1f7e8338360a65f768.jpg)

![](images/dccab657880c4c2209812e01688c938e69726b763fc35ea2cb7cbad5c2d8cdee.jpg)

![](images/794be7b1039811fab1664a53c2898d5a543774b12f8b8c93c02c960a66018055.jpg)

# About the Authors

Judea Pearl is Professor of Computer Science and Statistics at the University of California, Los Angeles, where he directs the Cognitive Systems Laboratory and conducts research in arti--cial intelligence, causal inference and philosophy of science. He is a Co-Founder and Editor of the Journal of Causal Inference and the author of three landmark books in inference-related areas. His latest book, Causality: Models, Reasoning and Inference (Cambridge, 2000, 2009), has introduced many of the methods used in modern causal analysis. It won the Lakatosh Award from the London School of Economics and is cited by more than 13,000 scientific publications.

His groundbreaking book, Causality: Models, Reasoning and Inference (Cambridge, 2000, 2009), has introduced many of the methods used in modern causal analysis. It won the Lakatos Award from the London School of Economics and is cited by more than 15,000 scientific publications. His latest book, The Book of Why: The New Science of Cause and Effect (with Dana Mackenzie, Basic Books, 2018), describes the impacts of causal inference to the general public.

Madelyn Glymour is a data analyst at Carnegie Mellon University, and a science writer and editor for the Cognitive Systems Laboratory at UCLA. Her interests lie in causal discovery and in the art of making complex concepts accessible to broad audiences.

Nicholas P. Jewell is Professor of Biostatistics and Statistics at the University of California, Berkeley. He has held various academic and administrative positions at Berkeley since his arrival in 1981, most notably serving as Vice Provost from 1994 to 2000. He has also held academic appointments at the University of Edinburgh, Oxford University, the London School of Hygiene and Tropical Medicine, and at the University of Kyoto. In 2007, he was a Fellow at the Rockefeller Foundation Bellagio Study Center in Italy.

Jewell is a Fellow of the American Statistical Association, the Institute of Mathematical Statistics, and the American Association for the Advancement of Science (AAAS). He is a past winner of the Snedecor Award and the Marvin Zelen Leadership Award in Statistical Science from Harvard University. He is currently the Editor of the Journal of the American Statistical Association ‚Äì Theory & Methods, and Chair of the Statistics Section of AAAS. His research focuses on the application of statistical methods to infectious and chronic disease epidemiology, the assessment of drug safety, time-to-event analyses, and human rights.

![](images/7f2809db8ea072666977a1538c990b3034ce2fd31e212c5af56a0f23e62c37aa.jpg)

![](images/544cedab87c493eed27ada273fdf57174a1d325467142974ba1667526c90ec86.jpg)

![](images/7cf1c5e0c0124dec3a7172c5f9f3574609ac964c24aaffcf41101f6e22008ac1.jpg)

![](images/44af6a1138040cc5b24ac0cb5d461b3982f34a3284b628fa550ca307ef2d5ee4.jpg)

![](images/75f0d0e18d8b7c02e3edd98590b83cc9b99a953116587e8584677fcee38d1740.jpg)

![](images/b5325b3d7ed47229402303cdebc758e29e60de8626b065d75946ddae47c14f98.jpg)

# Preface

When attempting to make sense of data, statisticians are invariably motivated by causal questions. For example, ‚ÄúHow effective is a given treatment in preventing a disease?‚Äù; ‚ÄúCan one estimate obesity-related medical costs?‚Äù; ‚ÄúCould government actions have prevented the -nancial crisis of 2008?‚Äù; ‚ÄúCan hiring records prove an employer guilty of sex discrimination?‚Äù

The peculiar nature of these questions is that they cannot be answered, or even articulated, in the traditional language of statistics. In fact, only recently has science acquired a mathematical language we can use to express such questions, with accompanying tools to allow us to answer them from data.

The development of these tools has spawned a revolution in the way causality is treated in statistics and in many of its satellite disciplines, especially in the social and biomedical sciences. For example, in the technical program of the 2003 Joint Statistical Meeting in San Francisco, there were only 13 papers presented with the word ‚Äúcause‚Äù or ‚Äúcausal‚Äù in their titles; the number of such papers exceeded 100 by the Boston meeting in 2014. These numbers represent a transformative shift of focus in statistics research, accompanied by unprecedented excitement about the new problems and challenges that are opening themselves to statistical analysis. Harvard‚Äôs political science professor Gary King puts this revolution in historical perspective: ‚ÄúMore has been learned about causal inference in the last few decades than the sum total of everything that had been learned about it in all prior recorded history.‚Äù

Yet this excitement remains barely seen among statistics educators, and is essentially absent from statistics textbooks, especially at the introductory level. The reasons for this disparity is deeply rooted in the tradition of statistical education and in how most statisticians view the role of statistical inference.

In Ronald Fisher‚Äôs inuential manifesto, he pronounced that ‚Äúthe object of statistical methods is the reduction of data‚Äù (Fisher 1922). In keeping with that aim, the traditional task of making sense of data, often referred to generically as ‚Äúinference,‚Äù became that of -nding a parsimonious mathematical description of the joint distribution of a set of variables of interest, or of speci-c parameters of such a distribution. This general strategy for inference is extremely familiar not just to statistical researchers and data scientists, but to anyone who has taken a basic course in statistics. In fact, many excellent introductory books describe smart and effective ways to extract the maximum amount of information possible from the available data. These books take the novice reader from experimental design to parameter estimation and hypothesis testing in great detail. Yet the aim of these techniques are invariably the

description of data, not of the process responsible for the data. Most statistics books do not even have the word ‚Äúcausal‚Äù or ‚Äúcausation‚Äù in the index.

Yet the fundamental question at the core of a great deal of statistical inference is causal; do changes in one variable cause changes in another, and if so, how much change do they cause? In avoiding these questions, introductory treatments of statistical inference often fail even to discuss whether the parameters that are being estimated are the relevant quantities to assess when interest lies in cause and effects.

The best that most introductory textbooks do is this: First, state the often-quoted aphorism that ‚Äúassociation does not imply causation,‚Äù give a short explanation of confounding and how ‚Äúlurking variables‚Äù can lead to a misinterpretation of an apparent relationship between two variables of interest. Further, the boldest of those texts pose the principal question: ‚ÄúHow can a causal link between $x$ and y be established?‚Äù and answer it with the long-standing ‚Äúgold standard‚Äù approach of resorting to randomized experiment, an approach that to this day remains the cornerstone of the drug approval process in the United States and elsewhere.

However, given that most causal questions cannot be addressed through random experimentation, students and instructors are left to wonder if there is anything that can be said with any reasonable con-dence in the absence of pure randomness.

In short, by avoiding discussion of causal models and causal parameters, introductory textbooks provide readers with no basis for understanding how statistical techniques address scienti-c questions of causality.

It is the intent of this primer to -ll this gnawing gap and to assist teachers and students of elementary statistics in tackling the causal questions that surround almost any nonexperimental study in the natural and social sciences. We focus here on simple and natural methods to de-ne causal parameters that we wish to understand and to show what assumptions are necessary for us to estimate these parameters in observational studies. We also show that these assumptions can be expressed mathematically and transparently and that simple mathematical machinery is available for translating these assumptions into estimable causal quantities, such as the effects of treatments and policy interventions, to identify their testable implications.

Our goal stops there for the moment; we do not address in any detail the optimal parameter estimation procedures that use the data to produce effective statistical estimates and their associated levels of uncertainty. However, those ideas‚Äîsome of which are relatively advanced‚Äîare covered extensively in the growing literature on causal inference. We thus hope that this short text can be used in conjunction with standard introductory statistics textbooks like the ones we have described to show how statistical models and inference can easily go hand in hand with a thorough understanding of causation.

It is our strong belief that if one wants to move beyond mere description, statistical inference cannot be effectively carried out without thinking carefully about causal questions, and without leveraging the simple yet powerful tools that modern analysis has developed to answer such questions. It is also our experience that thinking causally leads to a much more exciting and satisfying approach to both the simplest and most complex statistical data analyses. This is not a new observation. Virgil said it much more succinctly than we in 29 BC:

‚ÄúFelix, qui potuit rerum cognoscere causas‚Äù (Virgil 29 BC)

(Lucky is he who has been able to understand the causes of things)

The book is organized in four chapters.

Chapter 1 provides the basic statistical, probabilistic, and graphical concepts that readers will need to understand the rest of the book. It also introduces the fundamental concepts of causality, including the causal model, and explains through examples how the model can convey information that pure data are unable to provide.

Chapter 2 explains how causal models are reected in data, through patterns of statistical dependencies. It explains how to determine whether a data set complies with a given causal model, and briey discusses how one might search for models that explain a given data set.

Chapter 3 is concerned with how to make predictions using causal models, with a particular emphasis on predicting the outcome of a policy intervention. Here we introduce techniques of reducing confounding bias using adjustment for covariates, as well as inverse probability weighing. This chapter also covers mediation analysis and contains an in-depth look at how the causal methods discussed thus far work in a linear system. Key to these methods is the fundamental distinction between regression coef-cients and structural parameters, and how students should use both to predict causal effects in linear models.

Chapter 4 introduces the concept of counterfactuals‚Äîwhat would have happened, had we chosen differently at a point in the past‚Äîand discusses how we can compute them, estimate their probabilities, and what practical questions we can answer using them. This chapter is somewhat advanced, compared to its predecessors, primarily due to the novelty of the notation and the hypothetical nature of the questions asked. However, the fact that we read and compute counterfactuals using the same scienti-c models that we used in previous chapters should make their analysis an easy journey for students and instructors. Those wishing to understand counterfactuals on a friendly mathematical level should -nd this chapter a good starting point, and a solid basis for bridging the model-based approach taken in this book with the potential outcome framework that some experimentalists are pursuing in statistics.

# Acknowledgments

This book is an outgrowth of a graduate course on causal inference that the -rst author has been teaching at UCLA in the past 20 years. It owes many of its tools and examples to former members of the Cognitive Systems Laboratory who participated in the development of this material, both as researchers and as teaching assistants. These include Alex Balke, David Chickering, David Galles, Dan Geiger, Moises Goldszmidt, Jin Kim, George Rebane, Ilya Shpitser, Jin Tian, and Thomas Verma.

We are indebted to many colleagues from whom we have learned much about causal problems, their solutions, and how to present them to general audiences. These include Clark and Maria Glymour, for providing patient ears and sound advice on matters of both causation and writing, Felix Elwert and Tyler VanderWeele for insightful comments on an earlier version of the manuscript, and the many visitors and discussants to the UCLA Causality blog who kept the discussion lively, occasionally controversial, but never boring (causality.cs.ucla.edu/blog).

Elias Bareinboim, Bryant Chen, Andrew Forney, Ang Li, Karthika Mohan, reviewed the text for accuracy and transparency. Ang and Andrew also wrote solutions to the study questions, which are available to instructors from the publisher, see <http://bayes.cs.ucla.edu/PRIMER/ CIS-Manual-PUBLIC.pdf>.

The manuscript was most diligently typed, processed, illustrated, and proofed by Kaoru Mulvihill at UCLA. Debbie Jupe and Heather Kay at Wiley deserve much credit for recognizing and convincing us that a book of this scope is badly needed in the -eld, and for encouraging us throughout the production process.

Finally, the National Science Foundation and the Of-ce of Naval Research deserve acknowledgment for faithfully and consistently sponsoring the research that led to these results, with special thanks to Behzad Kamgar-Parsi.

# List of Figures

Figure 1.1 Results of the exercise‚Äìcholesterol study, segregated by age 3   
Figure 1.2 Results of the exercise‚Äìcholesterol study, unsegregated. The data points are identical to those of Figure 1.1, except the boundaries between the various age groups are not shown 4   
Figure 1.3 Scatter plot of the results in Table 1.6, with the value of Die 1 on the $x$ -axis and the sum of the two dice rolls on the $y$ -axis 21   
Figure 1.4 Scatter plot of the results in Table 1.6, with the value of Die 1 on the $x$ -axis and the sum of the two dice rolls on the $y .$ -axis. The dotted line represents the line of best -t based on the data. The solid line represents the line of best -t we would expect in the population 21   
Figure 1.5 An undirected graph in which nodes $X$ and Y are adjacent and nodes Y and Z are adjacent but not $X$ and $Z$ 25   
Figure 1.6 A directed graph in which node $A$ is a parent of $B$ and $B$ is a parent of $C$ 25   
Figure 1.7 (a) Showing acyclic graph and (b) cyclic graph 26   
Figure 1.8 A directed graph used in Study question 1.4.1 26   
Figure 1.9 The graphical model of SCM 1.5.1, with $X$ indicating years of schooling, Y indicating years of employment, and $Z$ indicating salary 27   
Figure 1.10 Model showing an unobserved syndrome, Z, affecting both treatment (X) and outcome (Y) 31   
Figure 2.1 The graphical model of SCMs 2.2.1‚Äì2.2.3 37   
Figure 2.2 The graphical model of SCMs 2.2.5 and 2.2.6 39   
Figure 2.3 A simple collider 41   
Figure 2.4 A simple collider, Z, with one child, W, representing the scenario from Table 2.3, with $X$ representing one coin ip, Y representing the second coin ip, $Z$ representing a bell that rings if either $X$ or Y is heads, and W representing an unreliable witness who reports on whether or not the bell has rung 44   
Figure 2.5 A directed graph for demonstrating conditional independence (error terms are not shown explicitly) 45   
Figure 2.6 A directed graph in which $P$ is a descendant of a collider 45

Figure 2.7 A graphical model containing a collider with child and a fork 47   
Figure 2.8 The model from Figure 2.7 with an additional forked path between Z and Y 48   
Figure 2.9 A causal graph used in study question 2.4.1, all $U$ terms (not shown) are assumed independent 49   
Figure 3.1 A graphical model representing the relationship between temperature $( Z )$ , ice cream sales $( X )$ , and crime rates (Y) 54   
Figure 3.2 A graphical model representing an intervention on the model in Figure 3.1 that lowers ice cream sales 54   
Figure 3.3 A graphical model representing the effects of a new drug, with Z representing gender, $X$ standing for drug usage, and Y standing for recovery 55   
Figure 3.4 A modi-ed graphical model representing an intervention on the model in Figure 3.3 that sets drug usage in the population, and results in the manipulated probability $P _ { m }$ 56   
Figure 3.5 A graphical model representing the effects of a new drug, with $X$ representing drug usage, Y representing recovery, and $Z$ representing blood pressure (measured at the end of the study). Exogenous variables are not shown in the graph, implying they are mutually independent 58   
Figure 3.6 A graphical model representing the relationship between a new drug $( X )$ , recovery (Y), weight (W), and an unmeasured variable Z (socioeconomic status) 62   
Figure 3.7 A graphical model in which the backdoor criterion requires that we condition on a collider $( Z )$ in order to ascertain the effect of $X$ on $Y$ 63   
Figure 3.8 Causal graph used to illustrate the backdoor criterion in the following study questions 64   
Figure 3.9 Scatter plot with students‚Äô initial weights on the $x$ -axis and -nal weights on the $y .$ -axis. The vertical line indicates students whose initial weights are the same, and whose -nal wights are higher (on average) for plan B compared with plan A 65   
Figure 3.10 A graphical model representing the relationships between smoking $( X )$ and lung cancer (Y), with unobserved confounder $( U )$ and a mediating variable Z 66   
Figure 3.11 A graphical model representing the relationship between gender, quali-- cations, and hiring 76   
Figure 3.12 A graphical model representing the relationship between gender, quali--cations, and hiring, with socioeconomic status as a mediator between quali-cations and hiring 77   
Figure 3.13 A graphical model illustrating the relationship between path coef-cients and total effects 82   
Figure 3.14 A graphical model in which $X$ has no direct effect on Y, but a total effect that is determined by adjusting for $T$ 83   
Figure 3.15 A graphical model in which $X$ has direct effect $\alpha$ on Y 84

Figure 3.16 By removing the direct edge from $X$ to $Y$ and -nding the set of variables {W} that $d \cdot$ -separate them, we -nd the variables we need to adjust for to determine the direct effect of $X$ on Y 85   
Figure 3.17 A graphical model in which we cannot -nd the direct effect of $X$ on Y via adjustment, because the dashed double-arrow arc represents the presence of a backdoor path between $X$ and Y, consisting of unmeasured variables. In this case, $Z$ is an instrument with regard to the effect of $X$ on Y that enables the identi-cation of $\alpha$ 85   
Figure 3.18 Graph corresponding to Model 3.1 in Study question 3.8.1 86   
Figure 4.1 A model depicting the effect of Encouragement (X) on student‚Äôs score 94   
Figure 4.2 Answering a counterfactual question about a speci-c student‚Äôs score, predicated on the assumption that homework would have increased to $H = 2$ 95   
Figure 4.3 A model representing Eq. (4.7), illustrating the causal relations between college education $( X )$ , skills $( Z )$ , and salary (Y) 99   
Figure 4.4 Illustrating the graphical reading of counterfactuals. (a) The original model. (b) The modi-ed model $M _ { x }$ in which the node labeled $Y _ { x }$ represents the potential outcome Y predicated on $X = x$ 102   
Figure 4.5 (a) Showing how probabilities of necessity (PN) are bounded, as a function of the excess risk ratio (ERR) and the confounding factor (CF) (Eq. (4.31)); (b) showing how PN is identi-ed when monotonicity is assumed (Theorem 4.5.1) 118   
Figure 4.6 (a) The basic nonparametric mediation model, with no confounding. (b) A confounded mediation model in which dependence exists between $U _ { \scriptscriptstyle M }$ and $( U _ { _ { T } } , U _ { _ { Y } } )$ 121

![](images/3870fc6fae80902c22ff865ac25cfbf37ea2c869e0c9617ff9719083eee90ee3.jpg)

![](images/cc2b060c08495f8b76bdef6575aaa044f4d5dbdd5046c8299a36f28c3de4ddad.jpg)

![](images/70baa69560d53dab81ba9c65c861eaa2b0a45ee2fc81cf59b098a9a6afacf628.jpg)

![](images/50f395319b9550ba088ff7b9bf01354084b516da1e652b6acbf14c46012dc491.jpg)

![](images/dd727dc96e828e5e173f3cc7082437cf2a6e03fc38777cfabe21b25719b7bab9.jpg)

![](images/b71a6923ec9380ba388f3b5b7e03fa1f4e103824b0630445ac8639e71fd92fd9.jpg)

![](images/9d1844d90bd40f5f32d06405952e47ae5dabc2f8367abec46c0b3fe26694bddc.jpg)

![](images/002f2631448954aad985bfa24f45bc8cd16115279129b7debdad8c6aeca2c5c5.jpg)

![](images/c1d8a7d46191f72ad91a1494b59659b75e703d8df99f1ceac28cc3db94f15f63.jpg)

![](images/c7846d17d051cfc546aedf96d4d63a7f669ab484a6331777dd85a3259ab51589.jpg)

# About the Companion Website

This book is accompanied by a companion website:

www.wiley.com/go/Pearl/Causality

![](images/ea150ba54a72c0b412552a650db7e8f9529000a5fa4c0fa35134d8fe643a9869.jpg)

![](images/08e1a0de0b73c0a072b00dd4a2a9955cfb2ba34e17a8b6f3c919aa202d18bcf3.jpg)

![](images/9e671c0440608228ddafe6a1fdf3a37f0fff3b01bac16a655d20fbfb10f53489.jpg)

![](images/1a64c54e23cbd1af2fdb78a146f15baea4eaaab56243c74e191ed453098d0ed7.jpg)

![](images/77eaa835ae6285669d9b136cec6983dd6e045f0c2e7cab2fb94e424154f451a4.jpg)

![](images/2a61622f45f80c2e2e1bcc799ecc3d164fa2d99fe310d531464d4074bda3fc3e.jpg)

# Contents

About the Authors ix

Preface xi

List of Figures xv

About the Companion Website xix

1 Preliminaries: Statistical and Causal Models 1

1.1 Why Study Causation 1   
1.2 Simpson‚Äôs Paradox 1   
1.3 Probability and Statistics 7

1.3.1 Variables 7   
1.3.2 Events 8   
1.3.3 Conditional Probability 8   
1.3.4 Independence 10   
1.3.5 Probability Distributions 11   
1.3.6 The Law of Total Probability 11   
1.3.7 Using Bayes‚Äô Rule 13   
1.3.8 Expected Values 16   
1.3.9 Variance and Covariance 17   
1.3.10 Regression 20   
1.3.11 Multiple Regression 22

1.4 Graphs 24   
1.5 Structural Causal Models

1.5.1 Modeling Causal Assumptions 26   
1.5.2 Product Decomposition 29

2 Graphical Models and Their Applications 35

2.1 Connecting Models to Data 35   
2.2 Chains and Forks 35   
2.3 Colliders 40   
2.4 d-separation 45   
2.5 Model Testing and Causal Search 48

3 The Effects of Interventions 53

3.1 Interventions 53   
3.2 The Adjustment Formula 55

3.2.1 To Adjust or not to Adjust? 58   
3.2.2 Multiple Interventions and the Truncated Product Rule 60

3.3 The Backdoor Criterion 61   
3.4 The Front-Door Criterion 66   
3.5 Conditional Interventions and Covariate-Speci-c Effects 70   
3.6 Inverse Probability Weighing 72   
3.7 Mediation 75   
3.8 Causal Inference in Linear Systems 78

3.8.1 Structural versus Regression Coef-cients 80   
3.8.2 The Causal Interpretation of Structural Coef-cients 81   
3.8.3 Identifying Structural Coef-cients and Causal Effect 83   
3.8.4 Mediation in Linear Systems 87

4 Counterfactuals and Their Applications 89

4.1 Counterfactuals 89   
4.2 De-ning and Computing Counterfactuals 91

4.2.1 The Structural Interpretation of Counterfactuals 91   
4.2.2 The Fundamental Law of Counterfactuals 93   
4.2.3 From Population Data to Individual Behavior ‚Äì An Illustration 94   
4.2.4 The Three Steps in Computing Counterfactuals 96

4.3 Nondeterministic Counterfactuals 98

4.3.1 Probabilities of Counterfactuals 98   
4.3.2 The Graphical Representation of Counterfactuals 101   
4.3.3 Counterfactuals in Experimental Settings 103   
4.3.4 Counterfactuals in Linear Models 106

4.4 Practical Uses of Counterfactuals 107

4.4.1 Recruitment to a Program 107   
4.4.2 Additive Interventions 109   
4.4.3 Personal Decision Making 111   
4.4.4 Discrimination in Hiring 113   
4.4.5 Mediation and Path-disabling Interventions 114

4.5 Mathematical Tool Kits for Attribution and Mediation 116

4.5.1 A Tool Kit for Attribution and Probabilities of Causation 116   
4.5.2 A Tool Kit for Mediation 120

References 127

Index 133

# 1

# Preliminaries: Statistical and Causal Models

# 1.1 Why Study Causation

The answer to the question ‚Äúwhy study causation?‚Äù is almost as immediate as the answer to ‚Äúwhy study statistics.‚Äù We study causation because we need to make sense of data, to guide actions and policies, and to learn from our success and failures. We need to estimate the effect of smoking on lung cancer, of education on salaries, of carbon emissions on the climate. Most ambitiously, we also need to understand how and why causes in-uence their effects, which is not less valuable. For example, knowing whether malaria is transmitted by mosquitoes or ‚Äúmal-air,‚Äù as many believed in the past, tells us whether we should pack mosquito nets or breathing masks on our next trip to the swamps.

Less obvious is the answer to the question, ‚Äúwhy study causation as a separate topic, distinct from the traditional statistical curriculum?‚Äù What can the concept of ‚Äúcausation,‚Äù considered on its own, tell us about the world that tried-and-true statistical methods can‚Äôt?

Quite a lot, as it turns out. When approached rigorously, causation is not merely an aspect of statistics; it is an addition to statistics, an enrichment that allows statistics to uncover workings of the world that traditional methods alone cannot. For example, and this might come as a surprise to many, none of the problems mentioned above can be articulated in the standard language of statistics.

To understand the special role of causation in statistics, let‚Äôs examine one of the most intriguing puzzles in the statistical literature, one that illustrates vividly why the traditional language of statistics must be enriched with new ingredients in order to cope with cause‚Äìeffect relationships, such as the ones we mentioned above.

# 1.2 Simpson‚Äôs Paradox

Named after Edward Simpson (born 1922), the statistician who rst popularized it, the paradox refers to the existence of data in which a statistical association that holds for an entire population is reversed in every subpopulation. For instance, we might discover that students who

smoke get higher grades, on average, than nonsmokers get. But when we take into account the students‚Äô age, we might nd that, in every age group, smokers get lower grades than nonsmokers get. Then, if we take into account both age and income, we might discover that smokers once again get higher grades than nonsmokers of the same age and income. The reversals may continue indenitely, switching back and forth as we consider more and more attributes. In this context, we want to decide whether smoking causes grade increases and in which direction and by how much, yet it seems hopeless to obtain the answers from the data.

In the classical example used by Simpson (1951), a group of sick patients are given the option to try a new drug. Among those who took the drug, a lower percentage recovered than among those who did not. However, when we partition by gender, we see that more men taking the drug recover than do men are not taking the drug, and more women taking the drug recover than do women are not taking the drug! In other words, the drug appears to help men and women, but hurt the general population. It seems nonsensical, or even impossible‚Äîwhich is why, of course, it is considered a paradox. Some people nd it hard to believe that numbers could even be combined in such a way. To make it believable, then, consider the following example:

Example 1.2.1 We record the recovery rates of 700 patients who were given access to the drug. A total of 350 patients chose to take the drug and 350 patients did not. The results of the study are shown in Table 1.1.

The rst row shows the outcome for male patients; the second row shows the outcome for female patients; and the third row shows the outcome for all patients, regardless of gender. In male patients, drug takers had a better recovery rate than those who went without the drug $93 \%$ vs $87 \%$ ). In female patients, again, those who took the drug had a better recovery rate than nontakers $73 \%$ vs $69 \%$ ). However, in the combined population, those who did not take the drug had a better recovery rate than those who did ( $83 \%$ vs $78 \%$ ).

The data seem to say that if we know the patient‚Äôs gender‚Äîmale or female‚Äîwe can prescribe the drug, but if the gender is unknown we should not! Obviously, that conclusion is ridiculous. If the drug helps men and women, it must help anyone; our lack of knowledge of the patient‚Äôs gender cannot make the drug harmful.

Given the results of this study, then, should a doctor prescribe the drug for a woman? A man? A patient of unknown gender? Or consider a policy maker who is evaluating the drug‚Äôs overall effectiveness on the population. Should he/she use the recovery rate for the general population? Or should he/she use the recovery rates for the gendered subpopulations?

Table 1.1 Results of a study into a new drug, with gender being taken into account   

<table><tr><td></td><td>Drug</td><td>No drug</td></tr><tr><td>Men</td><td>81 out of 87 recovered (93%)</td><td>234 out of 270 recovered (87%)</td></tr><tr><td>Women</td><td>192 out of 263 recovered (73%)</td><td>55 out of 80 recovered (69%)</td></tr><tr><td>Combined data</td><td>273 out of 350 recovered (78%)</td><td>289 out of 350 recovered (83%)</td></tr></table>

The answer is nowhere to be found in simple statistics. In order to decide whether the drug will harm or help a patient, we rst have to understand the story behind the data‚Äîthe causal mechanism that led to, or generated, the results we see. For instance, suppose we knew an additional fact: Estrogen has a negative effect on recovery, so women are less likely to recover than men, regardless of the drug. In addition, as we can see from the data, women are signi- cantly more likely to take the drug than men are. So, the reason the drug appears to be harmful overall is that, if we select a drug user at random, that person is more likely to be a woman and hence less likely to recover than a random person who does not take the drug. Put differently, being a woman is a common cause of both drug taking and failure to recover. Therefore, to assess the effectiveness, we need to compare subjects of the same gender, thereby ensuring that any difference in recovery rates between those who take the drug and those who do not is not ascribable to estrogen. This means we should consult the segregated data, which shows us unequivocally that the drug is helpful. This matches our intuition, which tells us that the segregated data is ‚Äúmore specic,‚Äù hence more informative, than the unsegregated data.

With a few tweaks, we can see how the same reversal can occur in a continuous example. Consider a study that measures weekly exercise and cholesterol in various age groups. When we plot exercise on the $X$ -axis and cholesterol on the $Y$ -axis and segregate by age, as in Figure 1.1, we see that there is a general trend downward in each group; the more young people exercise, the lower their cholesterol is, and the same applies for middle-aged people and the elderly. If, however, we use the same scatter plot, but we don‚Äôt segregate by age (as in Figure 1.2), we see a general trend upward; the more a person exercises, the higher their cholesterol is. To resolve this problem, we once again turn to the story behind the data. If we know that older people, who are more likely to exercise (Figure 1.1), are also more likely to have high cholesterol regardless of exercise, then the reversal is easily explained, and easily resolved. Age is a common cause of both treatment (exercise) and outcome (cholesterol). So we should look at the age-segregated data in order to compare same-age people and thereby eliminate the possibility that the high exercisers in each group we examine are more likely to have high cholesterol due to their age, and not due to exercising.

However, and this might come as a surprise to some readers, segregated data does not always give the correct answer. Suppose we looked at the same numbers from our rst example of drug taking and recovery, instead of recording participants‚Äô gender, patients‚Äô blood pressure were

![](images/88651d28e53ad272f8d2cbd774dea72e5ce344796a21ad0e080f66b185e669f9.jpg)  
Figure 1.1 Results of the exercise‚Äìcholesterol study, segregated by age

![](images/d6a703daa40fb8028009af964816fe2f557d0464f61c0874c24a6127b3343099.jpg)  
Figure 1.2 Results of the exercise‚Äìcholesterol study, unsegregated. The data points are identical to those of Figure 1.1, except the boundaries between the various age groups are not shown

recorded at the end of the experiment. In this case, we know that the drug affects recovery by lowering the blood pressure of those who take it‚Äîbut unfortunately, it also has a toxic effect. At the end of our experiment, we receive the results shown in Table 1.2. (Table 1.2 is numerically identical to Table 1.1, with the exception of the column labels, which have been switched.)

Now, would you recommend the drug to a patient?

Once again, the answer follows from the way the data were generated. In the general population, the drug might improve recovery rates because of its effect on blood pressure. But in the subpopulations‚Äîthe group of people whose posttreatment BP is high and the group whose posttreatment BP is low‚Äîwe, of course, would not see that effect; we would only see the drug‚Äôs toxic effect.

As in the gender example, the purpose of the experiment was to gauge the overall effect of treatment on rates of recovery. But in this example, since lowering blood pressure is one of the mechanisms by which treatment affects recovery, it makes no sense to separate the results based on blood pressure. (If we had recorded the patients‚Äô blood pressure before treatment, and if it were BP that had an effect on treatment, rather than the other way around, it would be a different story.) So we consult the results for the general population, we nd that treatment increases the probability of recovery, and we decide that we should recommend treatment. Remarkably, though the numbers are the same in the gender and blood pressure examples, the correct result lies in the segregated data for the former and the aggregate data for the latter.

None of the information that allowed us to make a treatment decision‚Äînot the timing of the measurements, not the fact that treatment affects blood pressure, and not the fact that blood

Table 1.2 Results of a study into a new drug, with posttreatment blood pressure taken into account   

<table><tr><td></td><td>No drug</td><td>Drug</td></tr><tr><td>Low BP</td><td>81 out of 87 recovered (93%)</td><td>234 out of 270 recovered (87%)</td></tr><tr><td>High BP</td><td>192 out of 263 recovered (73%)</td><td>55 out of 80 recovered (69%)</td></tr><tr><td>Combined data</td><td>273 out of 350 recovered (78%)</td><td>289 out of 350 recovered (83%)</td></tr></table>

pressure affects recovery‚Äîwas found in the data. In fact, as statistics textbooks have traditionally (and correctly) warned students, correlation is not causation, so there is no statistical method that can determine the causal story from the data alone. Consequently, there is no statistical method that can aid in our decision.

Yet statisticians interpret data based on causal assumptions of this kind all the time. In fact, the very paradoxical nature of our initial, qualitative, gender example of Simpson‚Äôs problem is derived from our strongly held conviction that treatment cannot affect sex. If it could, there would be no paradox, since the causal story behind the data could then easily assume the same structure as in our blood pressure example. Trivial though the assumption ‚Äútreatment does not cause sex‚Äù may seem, there is no way to test it in the data, nor is there any way to represent it in the mathematics of standard statistics. There is, in fact, no way to represent any causal information in contingency tables (such as Tables 1.1 and 1.2), on which statistical inference is often based.

There are, however, extra-statistical methods that can be used to express and interpret causal assumptions. These methods and their implications are the focus of this book. With the help of these methods, readers will be able to mathematically describe causal scenarios of any complexity, and answer decision problems similar to those posed by Simpson‚Äôs paradox as swiftly and comfortably as they can solve for $X$ in an algebra problem. These methods will allow us to easily distinguish each of the above three examples and move toward the appropriate statistical analysis and interpretation. A calculus of causation composed of simple logical operations will clarify the intuitions we already have about the nonexistence of a drug that cures men and women but hurts the whole population and about the futility of comparing patients with equal blood pressure. This calculus will allow us to move beyond the toy problems of Simpson‚Äôs paradox into intricate problems, where intuition can no longer guide the analysis. Simple mathematical tools will be able to answer practical questions of policy evaluation as well as scientic questions of how and why events occur.

But we‚Äôre not quite ready to pull off such feats of derring-do just yet. In order to rigorously approach our understanding of the causal story behind data, we need four things:

1. A working denition of ‚Äúcausation.‚Äù   
2. A method by which to formally articulate causal assumptions‚Äîthat is, to create causal models.   
3. A method by which to link the structure of a causal model to features of data.   
4. A method by which to draw conclusions from the combination of causal assumptions embedded in a model and data.

The rst two parts of this book are devoted to providing methods for modeling causal assumptions and linking them to data sets, so that in the third part, we can use those assumptions and data to answer causal questions. But before we can go on, we must dene causation. It may seem intuitive or simple, but a commonly agreed-upon, completely encompassing definition of causation has eluded statisticians and philosophers for centuries. For our purposes, the denition of causation is simple, if a little metaphorical: A variable $X$ is a cause of a variable Y if Y in any way relies on $X$ for its value. We will expand slightly upon this denition later, but for now, think of causation as a form of listening; $X$ is a cause of $Y$ if $Y$ listens to $X$ and decides its value in response to what it hears.

Readers must also know some elementary concepts from probability, statistics, and graph theory in order to understand the aforementioned causal methods. The next two sections

will therefore provide the necessary denitions and examples. Readers with a basic understanding of probability, statistics, and graph theory may skip to Section 1.5 with no loss of understanding.

# Study questions

# Study question 1.2.1

What is wrong with the following claims?

(a) ‚ÄúData show that income and marriage have a high positive correlation. Therefore, your earnings will increase if you get married.‚Äù   
(b) ‚ÄúData show that as the number of -res increase, so does the number of -re -ghters. Therefore, to cut down on -res, you should reduce the number of -re -ghters.‚Äù   
(c) ‚ÄúData show that people who hurry tend to be late to their meetings. Don‚Äôt hurry, or you‚Äôll be late.‚Äù

# Study question 1.2.2

A baseball batter Tim has a better batting average than his teammate Frank. However, someone notices that Frank has a better batting average than Tim against both right-handed and left-handed pitchers. How can this happen? (Present your answer in a table.)

# Study question 1.2.3

Determine, for each of the following causal stories, whether you should use the aggregate or the segregated data to determine the true effect.

(a) There are two treatments used on kidney stones: Treatment A and Treatment B. Doctors are more likely to use Treatment A on large (and therefore, more severe) stones and more likely to use Treatment B on small stones. Should a patient who doesn‚Äôt know the size of his or her stone examine the general population data, or the stone size-speci-c data when determining which treatment will be more effective?   
(b) There are two doctors in a small town. Each has performed 100 surgeries in his career, which are of two types: one very dif-cult surgery and one very easy surgery. The -rst doctor performs the easy surgery much more often than the dif-cult surgery and the second doctor performs the dif-cult surgery more often than the easy surgery. You need surgery, but you do not know whether your case is easy or dif-cult. Should you consult the success rate of each doctor over all cases, or should you consult their success rates for the easy and dif-cult cases separately, to maximize the chance of a successful surgery?

# Study question 1.2.4

In an attempt to estimate the effectiveness of a new drug, a randomized experiment is conducted. In all, $50 \%$ of the patients are assigned to receive the new drug and $50 \%$ to receive a placebo. A day before the actual experiment, a nurse hands out lollipops to some patients who

show signs of depression, mostly among those who have been assigned to treatment the next day (i.e., the nurse‚Äôs round happened to take her through the treatment-bound ward). Strangely, the experimental data revealed a Simpson‚Äôs reversal: Although the drug proved bene-cial to the population as a whole, drug takers were less likely to recover than nontakers, among both lollipop receivers and lollipop nonreceivers. Assuming that lollipop sucking in itself has no effect whatsoever on recovery, answer the following questions:

(a) Is the drug bene-cial to the population as a whole or harmful?   
(b) Does your answer contradict our gender example, where sex-speci-c data was deemed more appropriate?   
(c) Draw a graph (informally) that more or less captures the story. (Look ahead to Section 1.4 if you wish.)   
(d) How would you explain the emergence of Simpson‚Äôs reversal in this story?   
(e) Would your answer change if the lollipops were handed out (by the same criterion) a day after the study?

[Hint: Use the fact that receiving a lollipop indicates a greater likelihood of being assigned to drug treatment, as well as depression, which is a symptom of risk factors that lower the likelihood of recovery.]

# 1.3 Probability and Statistics

Since statistics generally concerns itself not with absolutes but with likelihoods, the language of probability is extremely important to it. Probability is similarly important to the study of causation because most causal statements are uncertain (e.g., ‚Äúcareless driving causes accidents,‚Äù which is true, but does not mean that a careless driver is certain to get into an accident), and probability is the way we express uncertainty. In this book, we will use the language and laws of probability to express our beliefs and uncertainty about the world. To aid readers without a strong background in probability, we provide here a glossary of the most important terms and concepts they will need to know in order to understand the rest of the book.

# 1.3.1 Variables

A variable is any property or descriptor that can take multiple values. In a study that compares the health of smokers and nonsmokers, for instance, some variables might be the age of the participant, the gender of the participant, whether or not the participant has a family history of cancer, and how many years the participant has been smoking. A variable can be thought of as a question, to which the value is the answer. For instance, ‚ÄúHow old is this participant?‚Äù ‚Äú38 years old.‚Äù Here, ‚Äúage‚Äù is the variable, and ‚Äú38‚Äù is its value. The probability that variable $X$ takes value $x$ is written $P ( X = x )$ . This is often shortened, when context allows, to $P ( x )$ . We can also discuss the probability of multiple values at once; for instance, the probability that $X = x$ and $Y = y$ is written $P ( X = x , Y = y )$ , or $P ( x , y )$ . Note that $P ( X = 3 8 )$ is specically interpreted as the probability that an individual randomly selected from the population is aged 38.

A variable can be either discrete or continuous. Discrete variables (sometimes called categorical variables) can take one of a nite or countably innite set of values in any range. A variable describing the state of a standard light switch is discrete, because it has two values: ‚Äúon‚Äù

and ‚Äúoff.‚Äù Continuous variables can take any one of an innite set of values on a continuous scale (i.e., for any two values, there is some third value that lies between them). For instance, a variable describing in detail a person‚Äôs weight is continuous, because weight is measured by a real number.

# 1.3.2 Events

An event is any assignment of a value or set of values to a variable or set of variables. ${ } ^ { 6 6 } X = 1 { \mathrm { : } }$ ‚Äù is an event, as is ${ } ^ { * } X = 1$ or $X = 2$ ,‚Äù as is ${ } ^ { 6 6 } X = 1$ and $Y = 3$ ,‚Äù as is ${ } ^ { * } X = 1$ or $Y = 3$ .‚Äù ‚ÄúThe coin -ip lands on heads,‚Äù ‚Äúthe subject is older than 40,‚Äù and ‚Äúthe patient recovers‚Äù are all events. In the rst, ‚Äúoutcome of the coin -ip‚Äù is the variable, and ‚Äúheads‚Äù is the value it takes. In the second, ‚Äúage of the subject‚Äù is the variable and ‚Äúolder than $4 0 ^ { \circ }$ describes a set of values it may take. In the third, ‚Äúthe patient‚Äôs status‚Äù is the variable and ‚Äúrecovery‚Äù is the value. This denition of ‚Äúevent‚Äù runs counter to our everyday notion, which requires that some change occur. (For instance, we would not, in everyday conversation, refer to a person being a certain age as an event, but we would refer to that person turning a year older as such.) Another way of thinking of an event in probability is this: Any declarative statement (a statement that can be true or false) is an event.

# Study questions

# Study question 1.3.1

Identify the variables and events invoked in the lollipop story of Study question 1.2.4

# 1.3.3 Conditional Probability

The probability that some event A occurs, given that we know some other event $B$ has occurred, is the conditional probability of A given $B$ . The conditional probability that $X = x$ , given that $Y = y ,$ , is written $P ( X = x | Y = y )$ . As with unconditional probabilities, this is often shortened to $P ( x | y )$ . Often, the probability that we assign to the event ${ } ^ { \mathfrak { s } } X = x ^ { \mathfrak { s } }$ changes drastically, depending on the knowledge ${ } ^ { \ast } Y = y ^ { \ast }$ that we condition on. For instance, the probability that you have the -u right now is fairly low. But, that probability would become much higher if you were to take your temperature and discover that it is $1 0 2 ^ { \circ } \mathrm { F } .$

When dealing with probabilities represented by frequencies in a data set, one way to think of conditioning is -ltering a data set based on the value of one or more variables. For instance, suppose we looked at the ages of U.S. voters in the last presidential election. According to the Census Bureau, we might get the data set shown in Table 1.3.

In Table 1.3, there were 132,948,000 votes cast in total, so we would estimate that the probability that a given voter was younger than the age of 45 is

$$
P (\text {V o t e r ' s} \text {A g e} <   4 5) = \frac {2 0 , 5 3 9 , 0 0 0 + 3 0 , 7 5 6 , 0 0 0}{1 3 2 , 4 4 8 , 0 0 0} = \frac {5 1 , 2 9 5 , 0 0 0}{1 3 2 , 9 4 8 , 0 0 0} = 0. 3 8
$$

Suppose, however, we want to estimate the probability that a voter was younger than the age of 45, given that we know he was elder than the age of 29. To nd this out, we simply lter

Table 1.3 Age breakdown of voters in 2012 election (all numbers in thousands)   

<table><tr><td>Age group</td><td># of voters</td></tr><tr><td>18‚Äì29</td><td>20,539</td></tr><tr><td>30‚Äì44</td><td>30,756</td></tr><tr><td>45‚Äì64</td><td>52,013</td></tr><tr><td>65+</td><td>29,641</td></tr><tr><td></td><td>132,948</td></tr></table>

Table 1.4 Age breakdown of voters over the age of 29 in 2012 election (all numbers in thousands)   

<table><tr><td>Age group</td><td># of voters</td></tr><tr><td>30‚Äì44</td><td>30,756</td></tr><tr><td>45‚Äì64</td><td>52,013</td></tr><tr><td>65+</td><td>29,641</td></tr><tr><td></td><td>112,409</td></tr></table>

the data to form a new set (shown in Table 1.4), using only the cases where voters were older than 29.

In this new data set, there are 112,409,000 total votes, so we would estimate that

$$
P (V o t e r A g e <   4 5 | V o t e r A g e > 2 9) = \frac {3 0 , 7 5 6 , 0 0 0}{1 1 2 , 4 0 9 , 0 0 0} = 0. 2 7
$$

Conditional probabilities such as these play an important role in investigating causal questions, as we often want to compare how the probability (or, equivalently, risk) of an outcome changes under different ltering, or exposure, conditions. For example, how does the probability of developing lung cancer for smokers compare to the analogous probability for nonsmokers?

# Study questions

# Study question 1.3.2

Consider Table 1.5 showing the relationship between gender and education level in the U.S. adult population.

(a) Estimate P(High School).   
(b) Estimate P(High School OR Female).   
(c) Estimate P(High School  Female).   
(d) Estimate P(Female  High School).

Table 1.5 The proportion of males and females achieving a given education level   

<table><tr><td>Gender</td><td>Highest education achieved</td><td>Occurrence
(in hundreds of thousands)</td></tr><tr><td>Male</td><td>Never finished high school</td><td>112</td></tr><tr><td>Male</td><td>High school</td><td>231</td></tr><tr><td>Male</td><td>College</td><td>595</td></tr><tr><td>Male</td><td>Graduate school</td><td>242</td></tr><tr><td>Female</td><td>Never finished high school</td><td>136</td></tr><tr><td>Female</td><td>High school</td><td>189</td></tr><tr><td>Female</td><td>College</td><td>763</td></tr><tr><td>Female</td><td>Graduate school</td><td>172</td></tr></table>

# 1.3.4 Independence

It might happen that the probability of one event remains unaltered with the observation of another. For example, while observing your high temperature increases the probability that you have the -u, observing that your friend Joe is 38 years old does not change the probability at all. In cases such as this, we say that the two events are independent. Formally, events $A$ and $B$ are said to be independent if

$$
P (A \mid B) = P (A) \tag {1.1}
$$

that is, the knowledge that $B$ has occurred gives us no additional information about the probability of $A$ occurring. If this equality does not hold, then $A$ and $B$ are said to be dependent. Dependence and independence are symmetric relations‚Äîif $A$ is dependent on $B$ , then $B$ is dependent on $A$ , and if $A$ is independent of $B$ , then $B$ is independent of $A$ . (Formally, if $P ( A | B ) = P ( A )$ , then it must be the case that $P ( B | A ) = P ( B )$ .) This makes intuitive sense; if ‚Äúsmoke‚Äù tells us something about ‚Äúre,‚Äù then ‚Äúre‚Äù must tell us something about ‚Äúsmoke.‚Äù

Two events $A$ and $B$ are conditionally independent given a third event $C$ if

$$
P (A | B, C) = P (A | C) \tag {1.2}
$$

and $P ( B | A , C ) = P ( B | C )$ . For example, the event ‚Äúsmoke detector is on‚Äù is dependent on the event ‚Äúthere is a re nearby.‚Äù But these two events may become independent conditional on the third event ‚Äúthere is smoke nearby‚Äù; smoke detectors respond to the presence of smoke only, not to its cause. When dealing with data sets, or probability tables, $A$ and $B$ are conditionally independent given $C$ if $A$ and $B$ are independent in the new data set created by ltering on C. If $A$ and $B$ are independent in the original unltered data set, they are called marginally independent.

Variables, like events, can be dependent or independent of each other. Two variables $X$ and Y are considered independent if for every value $x$ and $y$ that $X$ and Y can take, we have

$$
P (X = x \mid Y = y) = P (X = x) \tag {1.3}
$$

(As with independence of events, independence of variables is a symmetrical relation, so it follows that Eq. (1.3) implies $P ( Y = y | X = x ) = P ( Y = y ) .$ .) If for any pair of values of $X$ and $Y$ ,

this equality does not hold, then $X$ and Y are said to be dependent. In this sense, independence of variables can be understood as a set of independencies of events. For instance, ‚Äúheight‚Äù and ‚Äúmusical talent‚Äù are independent variables; for every height $h$ and level of musical talent $m$ , the probability that a person is $h$ feet high would not change upon discovering that he/she has $m$ amount of talent.

# 1.3.5 Probability Distributions

A probability distribution for a variable $X$ is the set of probabilities assigned to each possible value of $X$ . For instance, if $X$ can take three values‚Äî1, 2, and 3‚Äîa possible probability distribution for $X$ would be $^ { * } P ( X = 1 ) = 0 . 5 , P ( X = 2 ) = 0 . 2 5 , P ( X = 3 ) = 0 . 2 5 .$ $P ( X = 3 ) = 0 . 2 5$ ‚Äù The probabilities in a probability distribution must lie between 0 and 1, and must sum to 1. An event with probability 0 is impossible; an event with probability 1 is certain.

Continuous variables also have probability distributions. The probability distribution of a continuous variable $X$ is represented by a function $f$ , called the density function. When $f$ is plotted on a coordinate plane, the probability that the value of variable $X$ lies between values $a$ and $b$ is the area under the curve between $a$ and $b$ ‚Äîor, as those who have taken calculus will know, $\textstyle \int _ { a } ^ { b } f ( x ) d x$ . The area under the entire curve‚Äîthat is, $\textstyle \int _ { - \infty } ^ { \infty } f ( x ) d x$ ‚Äîmust of course be equal to 1.

Sets of variables can also have probability distributions, called joint distributions. The joint distribution of a set of variables $V$ is the set of probabilities of each possible combination of variable values in $V$ . For instance, if $V$ is a set of two variables‚Äî $X$ and $Y .$ ‚Äîeach of which can take two values‚Äî1 and 2‚Äîthen one possible joint distribution for $V$ is ${ \sqrt { P ( X = 1 } }$ , $Y = 1 ) = 0 . 2 , P ( X = 1 , Y = 2 ) = 0 . 1 , P ( X = 2 , Y = 1 ) = 0 . 5 , P ( X = 2 , Y = 2 ) = 0 . 2 .$ .‚Äù Just as with single-variable distributions, probabilities in a joint distribution must sum to 1.

# 1.3.6 The Law of Total Probability

There are several universal probabilistic truths that are useful to know. First, for any two mutually exclusive events $A$ and $B$ (i.e., $A$ and $B$ cannot co-occur), we have

$$
P (A \text {o r} B) = P (A) + P (B) \tag {1.4}
$$

It follows that, for any two events $A$ and $B$ , we have

$$
P (A) = P (A, B) + P (A, \text {ÔºÇ n o t} B ^ {\prime \prime}) \tag {1.5}
$$

because the events ‚ÄúA and $B ^ { \ast }$ and $^ { 6 6 } A$ and ‚Äònot $B ^ { \ast \ast }$ are mutually exclusive‚Äîand because if A is true, then either ‚ÄúA and $B ^ { \prime }$ or ‚ÄúA and ‚Äònot $B ^ { \ast \ast }$ must be true. For example, ‚ÄúDana is a tall man‚Äù and ‚ÄúDana is a tall woman‚Äù are mutually exclusive, and if Dana is tall, then he or she must be either a tall man or a tall woman; therefore, P(Dana is tall) = P(‚ÄúDana is a tall man‚Äù) $^ +$ P(‚ÄúDana is a tall woman‚Äù).

More generally, for any set of events $B _ { 1 } , B _ { 2 }$ , ‚Ä¶ , $B _ { n }$ such that exactly one of the events must be true (an exhaustive, mutually exclusive set, called a partition), we have

$$
P (A) = P \left(A, B _ {1}\right) + P \left(A, B _ {2}\right) + \dots + P \left(A, B _ {n}\right) \tag {1.6}
$$

This rule, known as the law of total probability, becomes somewhat obvious as soon as we put it in real-world terms: If we pull a random card from a standard deck, the probability that the card is a Jack will be equal to the probability that it‚Äôs a Jack and a spade, plus the probability that it‚Äôs a Jack and a heart, plus the probability that it‚Äôs a Jack and a club, plus the probability that it‚Äôs a Jack and a diamond. Calculating the probability of an event A by summing up its probabilities over all $B _ { i }$ is called marginalizing over $B$ , and the resulting probability $P ( A )$ i s called the marginal probability of $A$ .

If we know the probability of $B$ and the probability of $A$ conditional on $B$ , we can deduce the probability of $A$ and $B$ by simple multiplication:

$$
P (A, B) = P (A \mid B) P (B) \tag {1.7}
$$

For instance, the probability that Joe is funny and smart is equal to the probability that a smart person is funny, multiplied by the probability that Joe is smart. The division rule

$$
P (A | B) = P (A, B) / P (B)
$$

which is formally regarded as a denition of conditional probabilities, is justied by viewing conditioning as a ltering operation, as we have done in Tables 1.3 and 1.4. When we condition on $B$ , we remove from the table all events that con-ict with $B$ . The resulting subtable, like the original, represents a probability distribution, and like all probability distributions, it must sum to one. Since the probabilities of the subtables rows in the original distribution summed to $P ( B )$ (by denition), we can determine their probabilities in the new distribution by multiplying each by $1 / P ( B )$ .

Equation (1.7) implies that the notion of independence, which until now we have used informally to mean ‚Äúgiving no additional information,‚Äù has a numerical representation in the probability distribution. In particular, for events $A$ and $B$ to be independent, we require that

$$
P (A, B) = P (A) P (B)
$$

For example, to check if the outcomes of two coins are truly independent, we should count the frequency at which both show up tails, and make sure that it equals the product of the frequencies at which each of the coins shows up tails.

Using (1.7) together with the symmetry $P ( A , B ) = P ( B , A )$ , we can immediately obtain one of the most important laws of probability, Bayes‚Äô rule:

$$
P (A \mid B) = \frac {P (B \mid A) P (A)}{P (B)} \tag {1.8}
$$

With the help of the multiplication rule in (1.7), we can express the law of total probability as a weighted sum of conditional probabilities:

$$
P (A) = P \left(A \mid B _ {1}\right) P \left(B _ {1}\right) + P \left(A \mid B _ {2}\right) P \left(B _ {2}\right) + \dots + P \left(A \mid B _ {k}\right) P \left(B _ {k}\right) \tag {1.9}
$$

This is very useful, because often we will nd ourselves in a situation where we cannot assess $P ( A )$ directly, but we can through this decomposition. It is generally easier to assess conditional probabilities such as $P ( A | B _ { k } )$ , which are tied to specic contexts, rather than $P ( A )$ , which is not attached to a context. For instance, suppose we have a stock of gadgets from two sources: $30 \%$ of them are manufactured by factory $A$ , in which one out of 5000 is defective, whereas $70 \%$

are manufactured by factory $B$ , in which one out of 10,000 is defective. To nd the probability that a randomly chosen gadget will be defective is not a trivial mental task, but when broken down according to Eq. (1.9) it becomes easy:

$$
\begin{array}{l} P (\text {d e f e c t i v e}) = P (\text {d e f e c t i v e} | A) P (A) + P (\text {d e f e c t i v e} | B) P (B) \\ = \frac {0 . 3 0}{5 , 0 0 0} + \frac {0 . 7 0}{1 0 , 0 0 0} \\ = \frac {1 . 3 0}{1 0 , 0 0 0} = 0. 0 0 0 1 3 \\ \end{array}
$$

Or, to take a somewhat harder example, suppose we roll two dice, and we want to know the probability that the second roll is higher than the rst, $P ( A ) = P ( R o l l 2 > R o l l 1 )$ ). There is no obvious way to calculate this probability all at once. But if we break it down into contexts $B _ { 1 }$ , ‚Ä¶ , $B _ { 6 }$ by conditioning on the value of the rst die, it becomes easy to solve:

$$
\begin{array}{l} P (R o l l 2 > R o l l 1) = P (R o l l 2 > R o l l 1 | R o l l 1 = 1) P (R o l l 1 = 1) \\ + P (R o l l 2 > R o l l 1 | R o l l 1 = 2) P (R o l l 1 = 2) \\ + \dots + P (R o l l 2 > R o l l 1 | R o l l 1 = 6) \times P (R o l l 1 = 6) \\ = \left(\frac {5}{6} \times \frac {1}{6}\right) + \left(\frac {4}{6} \times \frac {1}{6}\right) + \left(\frac {3}{6} \times \frac {1}{6}\right) + \left(\frac {2}{6} \times \frac {1}{6}\right) + \left(\frac {1}{6} \times \frac {1}{6}\right) + \left(\frac {0}{6} \times \frac {1}{6}\right) \\ = \frac {5}{1 2} \\ \end{array}
$$

The decomposition described in Eq. (1.9) is sometimes called ‚Äúthe law of alternatives‚Äù or ‚Äúextending the conversation‚Äù; in this book, we will refer to it as conditionalizing on $B$ .

# 1.3.7 Using Bayes‚Äô Rule

When using Bayes‚Äô rule, we sometimes loosely refer to event $A$ as the ‚Äúhypothesis‚Äù and event $B$ as the ‚Äúevidence.‚Äù This naming re-ects the reason that Bayes‚Äô theorem is so important: In many cases, we know or can easily determine $P ( B | A )$ (the probability that a piece of evidence will occur, given that our hypothesis is correct), but it‚Äôs much harder to gure out $P ( A | B )$ (the probability of the hypothesis being correct, given that we obtain a piece of evidence). Yet the latter is the question that we most often want to answer in the real world; generally, we want to update our belief in some hypothesis, $P ( A )$ , after some evidence $B$ has occurred, to $P ( A | B )$ . To precisely use Bayes‚Äô rule in this manner, we must treat each hypothesis as an event and assign to all hypotheses for a given situation a probability distribution, called a prior.

For example, suppose you are in a casino, and you hear a dealer shout ‚Äú11!‚Äù You happen to know that the only two games played at the casino that would occasion that event are craps and roulette and that there are exactly as many craps games as roulette games going on at any moment. What is the probability that the dealer is working at a game of craps, given that he shouted ‚Äú11?‚Äù

In this case, ‚Äúcraps‚Äù is our hypothesis, and ‚Äú11‚Äù is our evidence. It‚Äôs difcult to gure out this probability off-hand. But the reverse‚Äîthe probability that an 11 will result in a given round of craps‚Äîis easy to calculate; it is specied by the game. Craps is a game in which gamblers bet

the sum of a roll of two dice. So 11 will be the sum in . In roulette, there are 38 equally probable outcomes $\begin{array} { r } { \frac { 2 } { 3 6 } = \frac { 1 } { 1 8 } } \end{array}$ $P ( ^ {  } 1 1 ^ { \ ' } | \ ^ {  } c r a p s ^ { \prime \prime } ) =$ $\frac { 1 } { 1 8 }$ $\begin{array} { r } { P ( ^ {  } 1 1 ^ { \prime \prime } | \cdot ^ {  } r o u l e t t e ^ { \prime \prime } ) = \frac { 1 } { 3 8 } } \end{array}$ situation, there are two possible hypotheses; ‚Äúcraps‚Äù and ‚Äúroulette.‚Äù Since there are an equal number of craps and roulette games, $\begin{array} { r } { P ( \ddot { } ^ { \ast } c r a p s ^ { , \ast } ) = \frac { 1 } { 2 } } \end{array}$ , our prior belief before we hear the ‚Äú11‚Äù shout. Using the law of total probability,

$$
\begin{array}{l} P \left(" 1 1"\right) = P \left(" 1 1" \mid " c r a p s"\right) P \left(" c r a p s"\right) + P \left(" 1 1" \mid " r o u l e t t e" \right.) P \left(" r o u l e t t e" \right. \\ = \frac {1}{2} \times \frac {1}{1 8} + \frac {1}{2} \times \frac {1}{3 8} = \frac {7}{1 7 1} \\ \end{array}
$$

We have now fairly easily obtained all the information we need to determine P(‚Äúcraps‚Äù ‚Äú11‚Äù):

$$
P (‚Äú c r a p s ‚Äù | ‚Äú 1 1 ‚Äù) = \frac {P (‚Äú 1 1 ‚Äù | ‚Äú c r a p s ‚Äù) \times P (‚Äú c r a p s ‚Äù)}{P (‚Äú 1 1 ‚Äù)} = \frac {1 / 1 8 \times 1 / 2}{7 / 1 7 1} = 0. 6 7 9
$$

Another informative example of Bayes‚Äô rule in action is the Monty Hall problem, a classic brain teaser in statistics. In the problem, you are a contestant on a game show, hosted by Monty Hall. Monty shows you three doors‚Äî ${ \cdot } A , B$ , and $C$ ‚Äîbehind one and only one of which is a new car. (The other two doors have goats.) If you guess correctly, the car is yours; otherwise, you get a goat. You guess $A$ at random. Monty, who is forbidden from revealing where the car is, then opens Door $C$ , which, of course, has a goat behind it. He tells you that you can now switch to Door $B$ , or stick with Door A. Whichever you pick, you‚Äôll get what‚Äôs behind it.

Are you better off opening Door A, or switching to Door B?

Many people, when they rst encounter the problem, reason that, since the location of the car is independent of the door you rst choose, switching doors neither gains nor loses you anything; the probability that the car is behind Door $A$ is equal to the probability that it is behind Door $B$ .

But the correct answer, as decades of statistics students have found to their consternation, is that you are twice as likely to win the car if you switch to Door $B$ as you are if you stay with Door A. The reasoning often given for this counterintuitive solution is that, when you originally chose a door, you had a $\frac 1 3$ probability of picking the door with the car. Since Monty always opens a door with a goat, no matter whether you initially chose the car or not, you have received no new information since then. Therefore, there is still a $\frac 1 3$ probability that the door you picked hides the car, and the remaining $\frac { 2 } { 3 }$ probability must lie with the only other closed door left.

We can prove this surprising fact using Bayes‚Äô rule. Here we have three variables: $X$ , the door chosen by the player; Y, the door behind which the car is hidden; and $Z$ , the door which the host opens. X, Y, and $Z$ can all take the values $A , B$ , or $C$ . We want to prove that $P ( Y = B | X = A , Z = C ) > P ( Y = A | X = A , Z = C )$ . Our hypothesis is that the car lies behind Door A; our evidence is that Monty opened Door $C$ . We will leave the proof to the reader‚Äîsee Study question 1.3.5. To further develop your intuition, you might generalize the game to having 100 doors (which contain 1 hidden car and 99 hidden goats). The contestant still chooses one door, but now Monty opens 98 doors‚Äîall revealing goats deliberately‚Äîbefore offering the contestant the chance to switch before the nal doors are opened. Now, the choice to switch should be obvious.

Why does Monty opening Door $C$ constitute evidence about the location of the car? It didn‚Äôt, after all, provide any evidence for whether your initial choice of door was correct. And, surely,

when he was about to open a door, be it $B$ or $C$ , you knew in advance that you won‚Äôt nd a car behind it. The answer is that there was no way for Monty to open Door $A$ after you chose it‚Äîbut he could have opened Door $B$ . The fact that he didn‚Äôt makes it more likely that he opened Door $C$ because he was forced to; it provides evidence that the car lies behind Door $B$ . This is a general theme of Bayesian analysis: Any hypothesis that has withstood some test of refutation becomes more likely. Door $B$ was vulnerable to refutation (i.e., Monty could have opened it), but Door A was not. Therefore, Door $B$ becomes a more likely location, whereas Door A does not.

The reader may nd it instructive to note that the explanation above is laden with counterfactual terminology; for example, ‚ÄúHe could have opened,‚Äù ‚Äúbecause he was forced,‚Äù ‚ÄúHe was about to open.‚Äù Indeed, what makes the Monty Hall example unique among probability puzzles is its critical dependence on the process that generated the data. It shows that our beliefs should depend not merely on the facts observed but also on the process that led to those facts. In particular, the information that the car is not behind Door $C$ , in itself, is not sufcient to describe the problem; to gure out the probabilities involved, we must also know what options were available to the host before opening Door $C$ . In Chapter 4 of this book we will formulate a theory of counterfactuals that will enable us to describe such processes and alternative options, so as to form the correct beliefs about choices.

There is some controversy attached to Bayes‚Äô rule. Often, when we are trying to ascertain the probability of a hypothesis given some evidence, we have no way to calculate the prior probability of the hypothesis, $P ( A )$ , in terms of fractions or frequencies of cases. Consider: If we did not know the proportion of roulette tables to craps tables in the casino, how on Earth could we determine the prior probability P(‚Äúcraps‚Äù)? We might be tempted to postulate $\begin{array} { r } { P ( A ) = \frac { 1 } { 2 } } \end{array}$ as a way of expressing our ignorance. But what if we have a hunch that roulette tables are less common in this casino, or the tone of the voice of the caller reminds us of a craps dealer we heard yesterday? In cases such as this, in order to use Bayes‚Äô rule, we substitute, in place of $P ( A )$ , our subjective belief in the relative truth of the hypothesis compared to other possibilities. The controversy stems from the subjective nature of that belief‚Äîhow are we to know whether the assigned $P ( A )$ accurately summarizes the information we have about the hypothesis? Should we insist on distilling all of our pro and con arguments down to a single number? And even if we do, why should we update our subjective beliefs about hypotheses the same way that we update objective frequencies? Some behavioral experiments suggest that people do not update their beliefs in accordance with Bayes‚Äô rule‚Äîbut many believe that they should, and that deviations from the rule represent compromises, if not deciencies in reasoning, and lead to suboptimal decisions. Debate over the proper use of Bayes‚Äô theorem continues to this day. Despite these controversies, however, Bayes‚Äô rule is a powerful tool for statistics, and we will use it to great effect throughout this book.

# Study questions

# Study question 1.3.3

Consider the casino problem described in Section 1.3.6

(a) Compute P(‚Äúcraps‚Äù|‚Äú11‚Äù) assuming that there are twice as many roulette tables as craps games at the casino.

(b) Compute P(‚Äúroulette‚Äù ‚Äú10‚Äù) assuming that there are twice as many craps games as roulette tables at the casino.

# Study question 1.3.4

Suppose we have three cards. Card 1 has two black faces, one on each side; Card 2 has two white faces; and Card 3 has one white face and one black face. You select a card at random and place it on the table. You -nd that it is black on the face-up side. What is the probability that the face-down side of the card is also black?

(a) Use your intuition to argue that the probability that the face-down side of the card is also black is $\frac { 1 } { 2 }$ . Why might it be greater than $\frac { 1 } { 2 }$ ?   
(b) Express the probabilities and conditional probabilities that you -nd easy to estimate (for example, $P ( C _ { D } = B l a c k )$ ), in terms of the following variables:

$$
\begin{array}{l} I = \text {I d e n t i t y} (C a r d 1, C a r d 2, \text {o r} C a r d 3) \\ C _ {D} = \text {C o l o r} \\ C _ {U} = \text {C o l o r o f t h e f a c e - u p s i d e (B l a c k , W h i t e)} \\ \end{array}
$$

Find the probability that the face-down side of the selected card is black, using your estimates above.

(c) Use Bayes‚Äô theorem to -nd the correct probability of a randomly selected card‚Äôs back being black if you observe that its front is black?

# Study question 1.3.5 (Monty Hall)

Prove, using Bayes‚Äô theorem, that switching doors improves your chances of winning the car in the Monty Hall problem.

# 1.3.8 Expected Values

In statistics, one often deals with data sets and probability distributions that are too large to effectively examine each possible combination of values. Instead, we use statistical measures to represent, with some loss of information, meaningful features of the distribution. One such measure is the expected value, also called the mean, which can be used when variables take on numerical values. The expected value of a variable $X$ , denoted $E ( X )$ , is found by multiplying each possible value of the variable by the probability that the variable will take that value, then summing the products:

$$
E (X) = \sum_ {x} x P (X = x) \tag {1.10}
$$

For instance, a variablelowing probability dis $X$ represebution: $\begin{array} { r } { P ( \overset {  } { 1 } ) = \frac { 1 } { 6 } , P ( 2 ) = \frac { 1 } { 6 } , P ( 3 ) = \frac { 1 } { 6 } , P ( 4 ) = \frac { 1 } { 6 } , P ( 5 ) = \frac { 1 } { 6 } , P ( 6 ) = \frac { 1 } { 6 } } \end{array}$ The expected value of $X$ is given by:

$$
E (X) = \left(1 \times \frac {1}{6}\right) + \left(2 \times \frac {1}{6}\right) + \left(3 \times \frac {1}{6}\right) + \left(4 \times \frac {1}{6}\right) + \left(5 \times \frac {1}{6}\right) + \left(6 \times \frac {1}{6}\right) = 3. 5
$$

Similarly, the expected value of any function of $X$ ‚Äîsay, $g ( X )$ ‚Äîis obtained by summing $g ( x ) P ( X = x )$ over all values of $X$ .

$$
E [ g (X) ] = \sum_ {x} g (x) P (x) \tag {1.11}
$$

For example, if after rolling a die, I receive a cash prize equal to the square of the result, we have $g ( X ) = X ^ { 2 }$ , and the expected prize is

$$
E [ g (X) ] = \left(1 ^ {2} \times \frac {1}{6}\right) + \left(2 ^ {2} \times \frac {1}{6}\right) + \left(3 ^ {2} \times \frac {1}{6}\right) + \left(4 ^ {2} \times \frac {1}{6}\right) + \left(5 ^ {2} \times \frac {1}{6}\right) + \left(6 ^ {2} \times \frac {1}{6}\right) = 1 5. 1 7
$$

We can also calculate the expected value of $Y$ conditional on $X$ , $E ( Y | X = x )$ , by multiplying each possible value $y$ of $Y$ by $P ( Y = y | X = x )$ , and summing the products.

$$
E (Y | X = x) = \sum_ {y} y P (Y = y | X = x) \tag {1.13}
$$

$E ( X )$ is one way to make a ‚Äúbest guess‚Äù of X‚Äôs value. Specically, out of all the guesses $g$ that we can make, the choice $\cdot g = E ( X ) ^ { \prime }$ minimizes the expected square error $E ( g - X ) ^ { 2 }$ . Similarly, $E ( Y | X = x )$ represents a best guess of Y, given that we observe $X = x$ . If $g = E ( Y | X = x )$ , then g minimizes the expected square error $E [ ( g - Y ) ^ { 2 } | X = x ]$ .

For example, the expected age of a 2012 voter, as demonstrated by Table 1.3, is

$$
E (\text {V o t e r ' s A g e}) = 2 3. 5 \times 0. 1 6 + 3 7 \times 0. 2 3 + 5 4. 5 \times 0. 3 9 + 7 0 \times 0. 2 2 = 4 8. 9
$$

(For this calculation, we have assumed that every age within each category is equally likely, e.g., a voter is as likely to be 18 as 25, and as likely to be 30 as 44. We have also assumed that the oldest age of any voter is 75.) This means that if we were asked to guess the age of a randomly chosen voter, with the understanding that if we were off by $e$ years, we would lose $e ^ { 2 }$ dollars, we would lose the least money, on average, if we guessed 48.9. Similarly, if we were asked to guess the age of a random voter younger than the age of 45, our best bet would be

$$
E [ \text {V o t e r ‚Äô s A g e} \mid \text {V o t e r ‚Äô s A g e} <   4 5 ] = 2 3. 5 \times 0. 4 0 + 3 7 \times 0. 6 0 = 3 1. 6 \tag {1.14}
$$

The use of expectations as a basis for predictions or ‚Äúbest guesses‚Äù hinges to a great extent on an implicit assumption regarding the distribution of $X$ or $Y | X = x .$ , namely that such distributions are approximately symmetric. If, however, the distribution of interest is highly skewed, other methods of prediction may be better. In such cases, for example, we might use the median of the distribution of $X$ as our ‚Äúbest guess‚Äù; this estimate minimizes the expected absolute error $E ( | g - X | )$ . We will not pursue such alternative measures further here.

# 1.3.9 Variance and Covariance

The variance of a variable $X$ , denoted $V a r ( X )$ or $\sigma _ { X } ^ { 2 }$ , is a measure of roughly how ‚Äúspread out‚Äù the values of $X$ in a data set or population are from their mean. If the values of $X$ all hover close

to one value, the variance will be relatively small; if they cover a large range, the variance will be comparatively large. Mathematically, we dene the variance of a variable as the average square difference of that variable from its mean. It can be computed by rst nding its mean, $\mu$ , and then calculating

$$
\operatorname {V a r} (X) = E \left(\left(X - \mu\right) ^ {2}\right) \tag {1.15}
$$

The standard deviation $\sigma _ { X }$ of a random variable $X$ is the square root of its variance. Unlike the variance, $\sigma _ { X }$ is expressed in the same units as $X$ . For example, the variance of under-45 voters‚Äô age distribution, according to Table 1.3, can easily be calculated to be (Eq. (1.15)):

$$
\begin{array}{l} \operatorname {V a r} (X) = \left((2 3. 5 - 3 1. 5) ^ {2} \times 0. 4 1\right) + \left((3 7 - 3 1. 5) ^ {2} \times 0. 5 9\right) \\ = (6 4 \times 0. 4 1) + (3 0. 2 5 \times . 5 9) \\ = 2 6. 2 4 + 1 7. 8 5 = 4 3. 0 9 \text {y e a r s} ^ {2} \\ \end{array}
$$

while the standard deviation is

$$
\sigma_ {X} = \sqrt {(4 3 . 0 9)} = 6. 5 6 \mathrm {y e a r s}
$$

This means that, choosing a voter at random, chances are high that his/her age will fall less than 6.56 years away from the average 31.5. This kind of interpretation can be quantied. For example, for a normally distributed random variable $X$ , approximately two-thirds of the population values of $X$ fall within one standard deviation of the expectation, or mean. Further, about $9 5 \%$ fall within two standard deviations from the mean.

Of special importance is the expectation of the product $( X - E ( X ) ) ( Y - E ( Y ) )$ , which is known as the covariance of $X$ and Y,

$$
\sigma_ {X Y} \triangleq E [ (X - E (X)) (Y - E (Y)) ] \tag {1.16}
$$

It measures the degree to which $X$ and Y covary, that is, the degree to which the two variables vary together, or are ‚Äúassociated.‚Äù This measure of association actually re-ects a specic way in which $X$ and Y covary; it measures the extent to which $X$ and Y linearly covary. You can think of this as plotting $Y$ versus $X$ and considering the extent to which a straight line captures the way in which Y varies as $X$ changes.

The covariance $\sigma _ { X Y }$ is often normalized to yield the correlation coef-cient

$$
\rho_ {X Y} = \frac {\sigma_ {X Y}}{\sigma_ {X} \sigma_ {Y}} \tag {1.17}
$$

which is a dimensionless number ranging from $^ { - 1 }$ to 1, which represents the slope of the best-t line after we normalize both $X$ and $Y$ by their respective standard deviations. $\rho _ { X Y }$ is one if and only if one variable can predict the other in a linear fashion, and it is zero whenever such a linear prediction is no better than a random guess. The signicance of $\sigma _ { X Y }$ and $\rho _ { X Y }$ will be discussed in the next section. At this point, it is sufcient to note that these degrees of covariation can be readily computed from the joint distribution $P ( x , y )$ , using Eqs. (1.16) and (1.17). Moreover, both $\sigma _ { X Y }$ and $\rho _ { X Y }$ vanish when $X$ and $Y$ are independent. Note that nonlinear relationships between Y and $X$ cannot naturally be captured by a simple numerical summary; they require a full specication of the conditional probability $P ( Y = y | X = x )$ .

# Study questions

# Study question 1.3.6

(a) Prove that, in general, both $\sigma _ { X Y }$ and $\rho _ { X Y }$ vanish when X and Y are independent. [Hint: Use Eqs. (1.16) and (1.17).]   
(b) Give an example of two variables that are highly dependent and, yet, their correlation coef-cient vanishes.

# Study question 1.3.7

Two fair coins are ipped simultaneously to determine the payoffs of two players in the town‚Äôs casino. Player 1 wins a dollar if and only if at least one coin lands on head. Player 2 receives a dollar if and only if the two coins land on the same face. Let X stand for the payoff of Player 1 and Y for the payoff of Player 2.

(a) Find and describe the probability distributions

$$
P (x), P (y), P (x, y), P (y | x) \text {a n d} P (x | y)
$$

(b) Using the descriptions in (a), compute the following measures:

$$
E [ X ], E [ Y ], E [ Y | X = x ], E [ X | Y = y ]
$$

$$
V a r (X), V a r (Y), C o v (X, Y), \rho_ {X Y}
$$

(c) Given that Player 2 won a dollar, what is your best guess of Player 1‚Äôs payoff?   
(d) Given that Player 1 won a dollar, what is your best guess of Player 2‚Äôs payoff?   
(e) Are there two events, $X = x$ and $Y = y ,$ , that are mutually independent?

# Study question 1.3.8

Compute the following theoretical measures of the outcome of a single game of craps (one roll of two independent dice), where X stands for the outcome of Die 1, Z for the outcome of Die 2, and Y for their sum.

(a)

$$
E [ X ], E [ Y ], E [ Y | X = x ], E [ X | Y = y ], \text {f o r e a c h v a l u e o f} x \text {a n d} y, \text {a n d}
$$

$$
V a r (X), V a r (Y), C o v (X, Y), \rho_ {X Y}, C o v (X, Z)
$$

Table 1.6 describes the outcomes of 12 craps games.

(b) Find the sample estimates of the measures computed in (a), based on the data from Table 1.6. [Hint: Many software packages are available for doing this computation for you.]   
(c) Use the results in (a) to determine the best estimate of the sum, Y, given that we measured $X = 3$ .

Table 1.6 Results of 12 rolls of two fair dice   

<table><tr><td></td><td>X Die 1</td><td>Z Die 2</td><td>Y Sum</td></tr><tr><td>Roll 1</td><td>6</td><td>3</td><td>9</td></tr><tr><td>Roll 2</td><td>3</td><td>4</td><td>7</td></tr><tr><td>Roll 3</td><td>4</td><td>6</td><td>10</td></tr><tr><td>Roll 4</td><td>6</td><td>2</td><td>8</td></tr><tr><td>Roll 5</td><td>6</td><td>4</td><td>10</td></tr><tr><td>Roll 6</td><td>5</td><td>3</td><td>8</td></tr><tr><td>Roll 7</td><td>1</td><td>5</td><td>6</td></tr><tr><td>Roll 8</td><td>3</td><td>5</td><td>8</td></tr><tr><td>Roll 9</td><td>6</td><td>5</td><td>11</td></tr><tr><td>Roll 10</td><td>3</td><td>5</td><td>8</td></tr><tr><td>Roll 11</td><td>5</td><td>3</td><td>8</td></tr><tr><td>Roll 12</td><td>4</td><td>5</td><td>9</td></tr></table>

(d) What is the best estimate of X, given that we measured $Y = 4 ?$   
(e) What is the best estimate of X, given that we measured $Y = 4$ and $Z = 1 ?$ Explain why it is not the same as in (d).

# 1.3.10 Regression

Often, in statistics, we wish to predict the value of one variable, Y, based on the value of another variable, $X$ . For example, we may want to predict a student‚Äôs height based on his age. We noted earlier that the best prediction of $Y$ based on $X$ is given by the conditional expectation $E [ Y | X = x ]$ , at least in terms of mean-squared error. But this assumes that we know the conditional expectation, or can compute it, from the joint distribution $P ( y , x )$ . With regression, we make our prediction directly from the data. We try to nd a formula, usually a linear function, that takes observed values of $X$ as input and gives values of Y as output, such that the square error between the predicted and actual values of Y is minimized, on average.

We start with a scatter plot that takes every case in our data set and charts them on a coordinate plane, as shown in Figure 1.2. Our predictor, or input, variable goes on the $x$ -axis, and the variable whose value we are predicting goes on the y-axis.

The least squares regression line is the line for which the sum of the squared vertical distances of the points on the scatter plot from the line is minimized. That is, if there are $n$ data points $( x , y )$ on our scatter plot, and for any data point $( x _ { i } , y _ { i } )$ , the value $y _ { i } ^ { \prime }$ represents the value of the line $y = \alpha + \beta x$ at $x _ { i }$ , then the least squares regression line is the one that minimizes the value

$$
\sum_ {i} \left(y _ {i} - y _ {i} ^ {\prime}\right) ^ {2} = \sum_ {i} \left(y _ {i} - \alpha - \beta x _ {i}\right) ^ {2} \tag {1.18}
$$

To see how the slope $\beta$ relates to the probability distribution $P ( x , y )$ , suppose we play 12 successive rounds of craps, and get the results shown in Table 1.6. If we wanted to predict the sum Y of the die rolls based on the value of $X = D i e \ 1$ alone, using the data in Table 1.6, we would use the scatter plot shown in Figure 1.3. For our craps example, the least squares

![](images/43e8b486c23492f289a8cb1bd08fa2bef4da8f65dc708aebc45467d2a938c178.jpg)  
Figure 1.3 Scatter plot of the results in Table 1.6, with the value of Die 1 on the $x$ -axis and the sum of the two dice rolls on the $y$ -axis

![](images/684ea1a777b7adedc82f6302ea36c03998b57f1d0d6293a8844112443b555961.jpg)  
Figure 1.4 Scatter plot of the results in Table 1.6, with the value of Die 1 on the $x$ -axis and the sum of the two dice rolls on the y-axis. The dotted line represents the line of best t based on the data. The solid line represents the line of best t we would expect in the population

regression line is shown in Figure 1.4. Note that the regression line for the sample that we used is not necessarily the same as the regression line for the population. The population is what we get when we allow our sample size to increase to innity. The solid line in Figure 1.4 represents the theoretical least-square line, which is given by

$$
y = 3. 5 + 1. 0 x \tag {1.19}
$$

The dashed line represents the sample least-square line, which, due to sampling variations, differs from the theoretical both in slope and in intercept.

In Figure 1.4, we know the equation of the regression line for the population because we know the expected value of the sum of two dice rolls, given that the rst die lands on $x$ . The computation is simple:

$$
E [ Y | X = x ] = E [ D i e 2 + X | X = x ] = E [ D i e 2 ] + x = 3. 5 + 1. 0 x
$$

This result is not surprising, since Y (the sum of the two dice) can be written as

$$
Y = X + Z
$$

where $Z$ is the outcome of Die 2, and it stands to reason that if $X$ increases by one unit, say from $X = 3$ to $X = 4$ , then $E [ Y ]$ will, likewise, increase by one unit. The reader might be a bit surprised, however, to nd out that the reverse is not the case; the regression of $X$ on Y does not have a slope of 1.0. To see why, we write

$$
E [ X | Y = y ] = E [ Y - Z | Y = y ] = 1. 0 y - E [ Z | Y = y ] \tag {1.20}
$$

and realize that the added term, $E [ Z | Y = y ]$ , since it depends (linearly) on $y$ , makes the slope less than unity. We can in fact compute the exact value of $E [ X | Y = y ]$ by appealing to symmetry and write

$$
E [ X | Y = y ] = E [ Z | Y = y ]
$$

which gives, after substituting in Eq. (1.20),

$$
E [ X | Y = y ] = 0. 5 y
$$

The reason for this reduction is that, when we increase Y by one unit, each of $X$ and $Z$ contributes equally to this increase on average. This matches intuition; observing that the sum of the two dice is $Y = 1 0$ , our best estimate of each is $X = 5$ and $Z = 5$ .

In general, if we write the regression equation for Y on $X$ as

$$
y = a + b x \tag {1.21}
$$

the slope $b$ is denoted by $R _ { Y X }$ , and it can be written in terms of the covariate $\sigma _ { X Y }$ as follows:

$$
b = R _ {Y X} = \frac {\sigma_ {X Y}}{\sigma_ {X} ^ {2}} \tag {1.22}
$$

From this equation, we see clearly that the slope of $Y$ on $X$ may differ from the slope of $X$ on Y‚Äîthat is, in most cases, $R _ { Y X } \neq R _ { X Y }$ . $( R _ { Y X } = R _ { X Y }$ only when the variance of $X$ is equal to the variance of Y.) The slope of the regression line can be positive, negative, or zero. If it is positive, $X$ and $Y$ are said to have a positive correlation, meaning that as the value of $X$ gets higher, the value of Y gets higher; if it is negative, $X$ and $Y$ are said to have a negative correlation, meaning that as the value of $X$ gets higher, the value of $Y$ gets lower; if it is zero (a horizontal line), $X$ and Y have no linear correlation, and knowing the value of $X$ does not assist us in predicting the value of Y, at least linearly. If two variables are correlated, whether positively or negatively (or in some other way), they are dependent.

# 1.3.11 Multiple Regression

It is also possible to regress a variable on several variables, using multiple linear regression. For instance, if we wanted to predict the value of a variable Y using the values of the variables $X$ and Z, we could perform multiple linear regression of Y on $\{ X , Z \}$ , and estimate a regression relationship

$$
y = r _ {0} + r _ {1} x + r _ {2} z \tag {1.23}
$$

which represents an inclined plane through the three-dimensional coordinate system.

We can create a three-dimensional scatter plot, with values of Y on the $y$ -axis, $X$ on the $x$ -axis, and $Z$ on the $z$ -axis. Then, we can cut the scatter plot into slices along the $Z$ -axis. Each slice will constitute a two-dimensional scatter plot of the kind shown in Figure 1.4. Each of those 2-D scatter plots will have a regression line with a slope $r _ { 1 }$ . Slicing along the $X$ -axis will give the slope $r _ { 2 }$ .

The slope of $Y$ on $X$ when we hold $Z$ constant is called the partial regression coef-cient and is denoted by $R _ { Y X \cdot Z }$ . Note that it is possible for $R _ { Y X }$ to be positive, whereas $R _ { Y X \cdot Z }$ is negative as shown in Figure 1.1. This is a manifestation of Simpson‚Äôs Paradox: positive association between Y and $X$ overall, that becomes negative when we condition on the third variable $Z$ .

The computation of partial regression coefcients (e.g., $r _ { 1 }$ and $r _ { 2 }$ in (1.23)) is greatly facilitated by a theorem that is one of the most fundamental results in regression analysis. It states that if we write Y as a linear combination of variables $X _ { 1 } , X _ { 2 }$ , ‚Ä¶ , $X _ { k }$ plus a noise term $\epsilon$ ,

$$
Y = r _ {0} + r _ {1} X _ {1} + r _ {2} X _ {2} + \dots + r _ {k} X _ {k} + \epsilon \tag {1.24}
$$

then, regardless of the underlying distribution of $Y , X _ { 1 } , X _ { 2 }$ , ‚Ä¶ , $X _ { k }$ , the best least-square coef-cients are obtained when $\epsilon$ is uncorrelated with each of the regressors $X _ { 1 } , X _ { 2 } , \dots , X _ { k }$ $X _ { 1 } , X _ { 2 }$ $X _ { k }$ . That is,

$$
C o v (\epsilon , X _ {i}) = 0 \quad \text {f o r} \quad i = 1, 2, \dots , k
$$

To see how this orthogonality principle is used to our advantage, assume we wish to compute the best estimate of $X = D i e I$ given the sum

$$
Y = \text {D i e} 1 + \text {D i e} 2
$$

Writing

$$
X = \alpha + \beta Y + \epsilon \tag {1.25a}
$$

our goal is to nd $\alpha$ and $\beta$ in terms of estimable statistical measures. Assuming without loss of generality $E [ \epsilon ] = 0$ , and taking expectation on both sides of the equation, we obtain

$$
E [ X ] = \alpha + \beta E [ Y ] \tag {1.25b}
$$

Further multiplying both sides of (1.25a) by Y and taking the expectation gives

$$
E [ X Y ] = \alpha E [ Y ] + \beta E [ Y ^ {2} ] + E [ Y \epsilon ] \tag {1.26}
$$

The orthogonality principle dictates $E [ Y \epsilon ] = 0$ , and (1.25b) and (1.26) yield two equations with two unknowns, $\alpha$ and $\beta$ . Solving for $\alpha$ and $\beta$ , we obtain

$$
\alpha = E (X) - E (Y) \frac {\sigma_ {X Y}}{\sigma_ {Y} ^ {2}}
$$

$$
\beta = \frac {\sigma_ {X Y}}{\sigma_ {Y} ^ {2}}
$$

which completes the derivation. The slope $\beta$ could have been obtained from Eq. (1.22), by simply reversing $X$ and $Y$ , but the derivation above demonstrates a general method of computing slopes, in two or more dimensions.

Consider for example the problem of nding the best estimate of $Z$ given two observations, $X = x$ and $Y = y .$ . As before, we write the regression equation

$$
Z = \alpha + \beta_ {Y} Y + \beta_ {X} X + \epsilon
$$

But now, to obtain three equations for $\alpha$ , $\beta _ { Y }$ , and $\beta _ { X }$ , we also multiply both sides by $Y$ and $X$ and take expectations. Imposing the orthogonality conditions $E [ \epsilon Y ] = E [ \epsilon X ] = 0$ and solving the resulting equations gives

$$
\beta_ {Y} = R _ {Z Y \cdot X} = \frac {\sigma_ {X} ^ {2} \sigma_ {Z Y} - \sigma_ {Z X} \sigma_ {X Y}}{\sigma_ {Y} ^ {2} \sigma_ {X} ^ {2} - \sigma_ {Y X} ^ {2}} \tag {1.27}
$$

$$
\beta_ {X} = R _ {Z X \cdot Y} = \frac {\sigma_ {Y} ^ {2} \sigma_ {Z X} - \sigma_ {Z Y} \sigma_ {Y X}}{\sigma_ {Y} ^ {2} \sigma_ {X} ^ {2} - \sigma_ {Y X} ^ {2}} \tag {1.28}
$$

Equations (1.27) and (1.28) are generic; they give the linear regression coefcients $R _ { Z Y \cdot X }$ and $R _ { Z X \cdot Y }$ for any three variables in terms of their variances and covariances, and as such, they allow us to see how sensitive these slopes are to other model parameters. In practice, however, regression slopes are estimated from sampled data by efcient ‚Äúleast-square‚Äù algorithms, and rarely require memorization of mathematical equations. An exception is the task of predicting whether any of these slopes is zero, prior to obtaining any data. Such predictions are important when we contemplate choosing a set of regressors for one purpose or another, and as we shall see in Section 3.8, this task will be handled quite efciently through the use of causal graphs.

# Study question 1.3.9

(a) Prove Eq. (1.22) using the orthogonality principle. [Hint: Follow the treatment of Eq. (1.26).]   
(b) Find all partial regression coef-cients

$$
R _ {Y X \cdot Z}, R _ {X Y \cdot Z}, R _ {Y Z \cdot X}, R _ {Z Y \cdot X}, R _ {X Z \cdot Y}, a n d R _ {Z X \cdot Y}
$$

for the craps game described in Study question 1.3.8. [Hint: Apply Eq. (1.27) and use the variances and covariances computed for part (a) of Study question 1.3.8.]

# 1.4 Graphs

We learned from Simpson‚Äôs Paradox that certain decisions cannot be made on the basis of data alone, but instead depend on the story behind the data. In this section, we layout a mathematical language, graph theory, in which these stories can be conveyed. Graph theory is not generally taught in high school mathematics, but it provides a useful mathematical language that allows us to address problems of causality with simple operations similar to those used to solve arithmetic problems.

Although the word graph is used colloquially to refer to a whole range of visual aids‚Äîmore or less interchangeably with the word chart‚Äîin mathematics, a graph is a formally dened

object. A mathematical graph is a collection of vertices (or, as we will call them, nodes) and edges. The nodes in a graph are connected (or not) by the edges. Figure 1.5 illustrates a simple graph. X, Y, and $Z$ (the dots) are nodes, and $A$ and $B$ (the lines) are edges.

![](images/8a5c1048a875b58162b9ead41cae15f3dc0c0fd2a59863abd595c46963411947.jpg)  
Figure 1.5 An undirected graph in which nodes $X$ and $Y$ are adjacent and nodes Y and $Z$ are adjacent but not $X$ and $Z$

Two nodes are adjacent if there is an edge between them. In Figure 1.5, X and $Y$ are adjacent, and $Y$ and $Z$ are adjacent. A graph is said to be a complete graph if there is an edge between every pair of nodes in the graph.

A path between two nodes $X$ and $Y$ is a sequence of nodes beginning with $X$ and ending with Y, in which each node is connected to the next by an edge. For instance, in Figure 1.5, there is a path from $X$ to $Z$ , because $X$ is connected to Y, and Y is connected to $Z$ .

Edges in a graph can be directed or undirected. Both of the edges in Figure 1.5 are undirected, because they have no designated ‚Äúin‚Äù and ‚Äúout‚Äù ends. A directed edge, on the other hand, goes out of one node and into another, with the direction indicated by an arrow head. A graph in which all of the edges are directed is a directed graph. Figure 1.6 illustrates a directed graph. In Figure $1 . 6 , A$ is a directed edge from $X$ to Y and $B$ is a directed edge from Y to $Z$ .

![](images/f428567a898e2c04b3296e1029a36472e2309d719941038dc2099847baf28d9c.jpg)  
Figure 1.6 A directed graph in which node $X$ is a parent of $Y$ and $Y$ is a parent of $Z$

The node that a directed edge starts from is called the parent of the node that the edge goes into; conversely, the node that the edge goes into is the child of the node it comes from. In Figure 1.6, $X$ is the parent of Y, and Y is the parent of $Z$ ; accordingly, Y is the child of $X$ , and $Z$ is the child of Y. A path between two nodes is a directed path if it can be traced along the arrows, that is, if no node on the path has two edges on the path directed into it, or two edges directed out of it. If two nodes are connected by a directed path, then the rst node is the ancestor of every node on the path, and every node on the path is the descendant of the rst node. (Think of this as an analogy to parent nodes and child nodes: parents are the ancestors of their children, and of their children‚Äôs children, and of their children‚Äôs children‚Äôs children, etc.) For instance, in Figure $1 . 6 , X$ is the ancestor of both Y and $Z$ , and both Y and $Z$ are descendants of $X$ .

When a directed path exists from a node to itself, the path (and graph) is called cyclic. A directed graph with no cycles is acyclic. For example, in Figure 1.7(a) the graph is acyclic; however, the graph in Figure 1.7(b) is cyclic. Note that in 1.7(a) there is no directed path from any node to itself, whereas in 1.7(b) there are directed paths from $X$ back to $X$ , for example.

![](images/91ea67a105bca821db0a412c15a9e885d03b2942eec7338c5aec211f08ed00c8.jpg)  
(a)

![](images/aa7fdc17d0dffa3a0bc807c975098deb0febe080416cf05e23016944a3a1a77c.jpg)  
(b)   
Figure 1.7 (a) Showing acyclic graph and (b) cyclic graph

Study questions

# Study question 1.4.1

Consider the graph shown in Figure 1.8:

![](images/a0073bab46305c11844ee3cc8f263cd296ca341084b34bb024c87c8e092e5c90.jpg)  
Figure 1.8 A directed graph used in Study question 1.4.1

(a) Name all of the parents of Z.   
(b) Name all the ancestors of Z.   
(c) Name all the children of W.   
(d) Name all the descendants of W.   
(e) Draw all (simple) paths between X and T (i.e., no node should appear more than once).   
(f) Draw all the directed paths between X and T.

# 1.5 Structural Causal Models

# 1.5.1 Modeling Causal Assumptions

In order to deal rigorously with questions of causality, we must have a way of formally setting down our assumptions about the causal story behind a data set. To do so, we introduce the concept of the structural causal model, or SCM, which is a way of describing the relevant features of the world and how they interact with each other. Specically, a structural causal model describes how nature assigns values to variables of interest.

Formally, a structural causal model consists of two sets of variables $U$ and $V$ , and a set of functions $f$ that assigns each variable in $V$ a value based on the values of the other variables in the model. Here, as promised, we expand on our denition of causation: A variable $X$ is a direct cause of a variable Y if $X$ appears in the function that assigns Y‚Äôs value. $X$ is a cause of Y if it is a direct cause of $Y$ , or of any cause of $Y$ .

The variables in $U$ are called exogenous variables, meaning, roughly, that they are external to the model; we choose, for whatever reason, not to explain how they are caused. The variables in $V$ are endogenous. Every endogenous variable in a model is a descendant of at least one exogenous variable. Exogenous variables cannot be descendants of any other variables, and in particular, cannot be a descendant of an endogenous variable; they have no ancestors and are represented as root nodes in graphs. If we know the value of every exogenous variable, then using the functions in $f$ , we can determine with perfect certainty the value of every endogenous variable.

For example, suppose we are interested in studying the causal relationships between a treatment $X$ and lung function Y for individuals who suffer from asthma. We might assume that $Y$ also depends on, or is ‚Äúcaused by,‚Äù air pollution levels as captured by a variable Z. In this case, we would refer to $X$ and $Y$ as endogenous and $Z$ as exogenous. This is because we assume that air pollution is an external factor, that is, it cannot be caused by an individual‚Äôs selected treatment or their lung function.

Every SCM is associated with a graphical causal model, referred to informally as a ‚Äúgraphical model‚Äù or simply ‚Äúgraph.‚Äù Graphical models consist of a set of nodes representing the variables in $U$ and $V$ , and a set of edges between the nodes representing the functions in $f$ . The graphical model $G$ for an SCM M contains one node for each variable in M. If, in $M$ , the function $f _ { X }$ for a variable $X$ contains within it the variable Y (i.e., if $X$ depends on Y for its value), then, in $G$ , there will be a directed edge from Y to $X$ . We will deal primarily with SCMs for which the graphical models are directed acyclic graphs (DAGs). Because of the relationship between SCMs and graphical models, we can give a graphical denition of causation: If, in a graphical model, a variable $X$ is the child of another variable Y, then Y is a direct cause of $X$ ; if $X$ is a descendant of Y, then $Y$ is a potential cause of $X$ (there are rare intransitive cases in which Y will not be a cause of $X$ , which we will discuss in Part Two).

In this way, causal models and graphs encode causal assumptions. For instance, consider the following simple SCM:

# SCM 1.5.1 (Salary Based on Education and Experience)

$$
U = \{X, Y \}, \quad V = \{Z \}, \quad F = \{f _ {Z} \}
$$

$$
f _ {Z}: Z = 2 X + 3 Y
$$

This model represents the salary $( Z )$ that an employer pays an individual with $X$ years of schooling and Y years in the profession. $X$ and Y both appear in $f _ { Z }$ , so $X$ and $Y$ are both direct causes of $Z$ . If $X$ and Y had any ancestors, those ancestors would be potential causes of $Z$ .

The graphical model associated with SCM 1.5.1 is illustrated in Figure 1.9.

![](images/8ef9bfca0e17b6acf53034b77d39f6e0a4ad6428460f7e0c9dc2661c9b2e6bfd.jpg)  
Figure 1.9 The graphical model of SCM 1.5.1, with X indicating years of schooling, Y indicating years of employment, and Z indicating salary

Because there are edges connecting $Z$ to $X$ and Y, we can conclude just by looking at the graphical model that there is some function $f _ { Z }$ in the model that assigns $Z$ a value based on $X$ and Y, and therefore that $X$ and Y are causes of Z. However, without the fuller specication of an SCM, we can‚Äôt tell from the graph what the function is that denes Z‚Äîor, in other words, how $X$ and Y cause Z.

If graphical models contain less information than SCMs, why do we use them at all? There are several reasons. First, usually the knowledge that we have about causal relationships is not quantitative, as demanded by an SCM, but qualitative, as represented in a graphical model. We know off-hand that sex is a cause of height and that height is a cause of performance in basketball, but we would hesitate to give numerical values to these relationships. We could, instead of drawing a graph, simply create a partially specied version of the SCM:

# SCM 1.5.2 (Basketball Performance Based on Height and Sex)

$$
V = \{\text {H e i g h t}, \text {S e x}, \text {P e r f o r m a n c e} \}, \quad U = \left\{U _ {1}, U _ {2}, U _ {3} \right\}, \quad F = \left\{f 1, f 2 \right\}
$$

$$
\mathrm {S e x} = U _ {1}
$$

$$
\operatorname {H e i g h t} = f _ {1} (\operatorname {S e x}, U _ {2})
$$

$$
\text {P e r f o r m a n c e} = f _ {2} (\text {H e i g h t}, \text {S e x}, U _ {3})
$$

Here, $U = \{ U _ { 1 } , U _ { 2 } , U _ { 3 } \}$ represents unmeasured factors that we do not care to name, but that affect the variables in $V$ that we can measure. The $U$ factors are sometimes called ‚Äúerror terms‚Äù or ‚Äúomitted factors.‚Äù These represent additional unknown and/or random exogenous causes of what we observe.

But graphical models provide a more intuitive understanding of causality than do such partially specied SCMs. Consider the SCM and its associated graphical model introduced above; while the SCM and its graphical model contain the same information, that is, that $X$ causes $Z$ and $Y$ causes Z, that information is more quickly and easily ascertained by looking at the graphical model.

# Study questions

# Study question 1.5.1

Suppose we have the following SCM. Assume all exogenous variables are independent and that the expected value of each is 0.

# SCM 1.5.3

$$
\begin{array}{l} V = \{X, Y, Z \}, \quad U = \{U _ {X}, U _ {Y}, U _ {Z} \}, \quad F = \{f _ {X}, f _ {Y}, f _ {Z} \} \\ f _ {X}: X = U _ {X} \\ f _ {Y}: Y = \frac {X}{3} + U _ {Y} \\ f _ {Z}: Z = \frac {Y}{1 6} + U _ {Z} \\ \end{array}
$$

(a) Draw the graph that complies with the model.   
(b) Determine the best guess of the value (expected value) of Z, given that we observe $Y = 3$   
(c) Determine the best guess of the value of $Z ,$ , given that we observe $X = 3$ .   
(d) Determine the best guess of the value of Z, given that we observe $X = 1$ and $Y = 3$ .   
(e) Assume that all exogenous variables are normally distributed with zero means and unit variance, that is, $\sigma = 1$ .

(i) Determine the best guess of $X$ , given that we observed $Y = 2$ .   
(ii) (Advanced) Determine the best guess of Y, given that we observed $X = 1$ and $Z = 3$ . [Hint: You may wish to use the technique of multiple regression, together with the fact that, for every three normally distributed variables, say X, Y, and $Z$ , we have $E [ Y | X = x , Z = z ] = R _ { Y X \cdot Z } x + R _ { Y Z \cdot X } z . ]$

# 1.5.2 Product Decomposition

Another advantage of graphical models is that they allow us to express joint distributions very efciently. So far, we have presented joint distributions in two ways. First, we have used tables, in which we assigned a probability to every possible combination of values. This is intuitively easy to parse, but in models with many variables, it can take up a prohibitive amount of space; 10 binary variables would require a table with 1024 rows!

Second, in a fully specied SCM, we can represent the joint distributions of $n$ variables with greater efciency: We need only to specify the $n$ functions that govern the relationships between the variables, and then from the probabilities of the error terms, we can discover all the probabilities that govern the joint distribution. But we are not always in a position to fully specify a model; we may know that one variable is a cause of another but not the form of the equation relating them, or we may not know the distributions of the error terms. Even if we know these objects, writing them down may be easier said than done, especially when the variables are discrete and the functions do not have familiar algebraic expressions.

Fortunately, we can use graphical models to help overcome both of these barriers through the following rule.

# Rule of product decomposition

For any model whose graph is acyclic, the joint distribution of the variables in the model is given by the product of the conditional distributions P(child parents) over all the ‚Äúfamilies‚Äù in the graph. Formally, we write this rule as

$$
P \left(x _ {1}, x _ {2}, \dots , x _ {n}\right) = \prod_ {i} P \left(x _ {i} \mid p a _ {i}\right) \tag {1.29}
$$

where $p a _ { i }$ stands for the values of the parents of variable $X _ { i }$ , and the product $\Pi _ { i }$ runs over all $i$ , from 1 to $n$ . The relationship (1.29) follows from certain universally true independencies among the variables, which will be discussed in the next chapter in more detail.

For example, in a simple chain graph $X  Y  Z$ , we can write directly:

$$
P (X = x, Y = y, Z = z) = P (X = x) P (Y = y | X = x) P (Z = z | Y = y)
$$

This knowledge allows us to save an enormous amount of space when laying out a joint distribution. We need not create a probability table that lists a value for every possible triple $( x , y , z )$ . It will sufce to create three much smaller tables for $X$ , $( Y | X )$ , and $( Z | Y )$ , and multiply the values as necessary.

To estimate the joint distribution from a data set generated by the above model, we need not count the frequency of every triple; we can instead count the frequencies of each x, (y x), and $( z | y )$ and multiply. This saves us a great deal of processing time in large models. It also increases substantially the accuracy of frequency counting. Thus, the assumptions underlying the graph allow us to exchange a ‚Äúhigh-dimensional‚Äù estimation problem for a few ‚Äúlow-dimensional‚Äù probability distribution challenges. The graph therefore simplies an estimation problem and, simultaneously, provides more precise estimators. If we do not know the graphical structure of an SCM, estimation becomes impossible with large number of variables and small, or moderately sized, data sets‚Äîthe so-called ‚Äúcurse of dimensionality.‚Äù

Graphical models let us do all of this without always needing to know the functions relating the variables, their parameters, or the distributions of their error terms.

Here‚Äôs an evocative, if unrigorous, demonstration of the time and space saved by this strategy: Consider the chain $X \to Y \to Z \to W$ , where $X$ stands for clouds/no clouds, Y stands for rain/no rain, Z stands for wet pavement/dry pavement, and W stands for slippery pavement/unslippery pavement.

Using your own judgment, based on your experience of the world, how plausible is it that P(clouds, no-rain, dry pavement, slippery pavement $) = 0 . 2 3 ?$

This is quite a difcult question to answer straight out. But using the product rule, we can break it into pieces:

# P(clouds)P(no rain|clouds)P(dry pavement|no rain)P(slippery pavement|dry pavement)

Our general sense of the world tells us that $P ( c l o u d s )$ should be relatively high, perhaps 0.5 (lower, of course, for those of us living in the strange, weatherless city of Los Angeles). Similarly, $P ( n o r a i n | c l o u d s )$ is fairly high‚Äîsay, 0.75. And P(dry pavement no rain) would be higher still, perhaps 0.9. But the P(slippery pavement dry pavement) should be quite low, somewhere in the range of 0.05. So putting it all together, we come to a ballpark estimate of $0 . 5 \times 0 . 7 5 \times 0 . 9 \times 0 . 0 5 = 0 . 0 1 6 9$ .

We will use this product rule often in this book in cases when we need to reason with numerical probabilities, but wish to avoid writing out large probability tables.

The importance of the product decomposition rule can be particularly appreciated when we deal with estimation. In fact, much of the role of statistics focuses on effective sampling designs, and estimation strategies, that allow us to exploit an appropriate data set to estimate probabilities as precisely as we might need. Consider again the problem of estimating the probability $P ( X , Y , Z , W )$ for the chain $X \to Y \to Z \to W$ . This time, however, we attempt to estimate the probability from data, rather than our own judgment. The number of $( x , y , z , w )$ combinations that need to be assigned probabilities is $1 6 - 1 = 1 5$ . Assume that we have 45 random observations, each consisting of a vector $( x , y , z , w )$ . On the average, each $( x , y , z , w )$ cell would receive about three samples; some will receive one or two samples, and some remain empty. It is very unlikely that we would obtain a sufcient number of samples in each cell to assess the proportion in the population at large (i.e., when the sample size goes to innity).

If we use our product decomposition rule, however, the 45 samples are separated into much larger categories. In order to determine $P ( x )$ , every $( x , y , z , w )$ sample falls into one of only two cells: $( X = 1 )$ ) and $( X = 0 )$ . Clearly, the probability of leaving either of them empty is much lower, and the accuracy of estimating population frequencies is much higher. The same is true of the divisions we need to make to determine P(y x) ‚à∂ $Y = 1 , X = 1 ,$ ), $\mathit { Y } = 0 , \mathit { X } = 1 ,$ ), $( Y = 1 , X = 0 )$ ), and $( Y = 0 , X = 0 )$ ). And to determine $P ( z | y ) : ( Y = 1 , Z = 1 )$ , $( Y = 0 , Z = 1 )$ ), $( Y = 1 , Z = 0$ ), and $( Y = 0 , Z = 0 )$ . And to determine $P ( w | z ) : ( W = 1 , Z = 1 )$ , $( W = 0 , Z = 1 )$ ), $( W = 1 , Z = 0$ ), and $( W = 0 , Z = 0$ ). Each of these divisions will give us much more accurate frequencies than our original division into 15 cells. Here we explicitly see the simpler estimation problems allowed by assuming the graphical structure of an SCM and the resulting improved accuracy of our frequency estimates.

This is not the only use to which we can put the qualitative knowledge that a graph provides. As we will see in the next section, graphical models reveal much more information than is obvious at rst glance; we can learn a lot about, and infer a lot from, a data set using only the graphical model of its causal story.

# Study questions

# Study question 1.5.2

Assume that a population of patients contains a fraction r of individuals who suffer from a certain fatal syndrome Z, which simultaneously makes it uncomfortable for them to take a life-prolonging drug $X$ (Figure 1.10). Let $Z = z _ { 1 }$ and $Z = z _ { 0 }$ represent, respectively, the presence and absence of the syndrome, $Y = y _ { 1 }$ and $Y = y _ { 0 }$ represent death and survival, respectively, and $X = x _ { 1 }$ and $X = x _ { 0 }$ represent taking and not taking the drug. Assume that patients not carrying the syndrome, $Z = z _ { 0 }$ , die with probability $p _ { 2 }$ if they take the drug and with probability $p _ { 1 }$ if they don‚Äôt. Patients carrying the syndrome, $Z = z _ { 1 }$ , on the other hand, die with probability $p _ { 3 }$ if they do not take the drug and with probability $p _ { 4 }$ if they do take the drug. Further, patients having the syndrome are more likely to avoid the drug, with probabilities $q _ { 1 } = P ( x _ { 1 } | z _ { 0 } )$ and $q _ { 2 } = P ( x _ { 1 } | z _ { 1 } )$ .

(a) Based on this model, compute the joint distributions $P ( x , y , z ) , P ( x , y ) , P ( x , z )$ , and $P ( y , z )$ for all values of $x , y ,$ , and $z ,$ in terms of the parameters $( r , p _ { 1 } , p _ { 2 } , p _ { 3 } , p _ { 4 } , q _ { 1 } , q _ { 2 } )$ . [Hint: Use the product decomposition of Section 1.5.2.]   
(b) Calculate the difference $P ( y _ { 1 } | x _ { 1 } ) - P ( y _ { 1 } | x _ { o } )$ for three populations: (1) those carrying the syndrome, (2) those not carrying the syndrome, and (3) the population as a whole.

![](images/b5b050ccc89e9e71c3f5e241fb5fadfa3414b32fb851832fce69ae2048929fcb.jpg)  
Figure 1.10 Model showing an unobserved syndrome, Z, affecting both treatment $( X )$ and outcome (Y)

(c) Using your results for (b), -nd a combination of parameters that exhibits Simpson‚Äôs reversal.

# Study question 1.5.3

Consider a graph $X _ { 1 }  X _ { 2 }  X _ { 3 }  X _ { 4 }$ of binary random variables, and assume that the conditional probabilities between any two consecutive variables are given by

$$
P (X _ {i} = 1 | X _ {i - 1} = 1) = p
$$

$$
P (X _ {i} = 1 | X _ {i - 1} = 0) = q
$$

$$
P (X _ {1} = 1) = p _ {0}
$$

Compute the following probabilities

$$
P \left(X _ {1} = 1, X _ {2} = 0, X _ {3} = 1, X _ {4} = 0\right)
$$

$$
P \left(X _ {4} = 1 \mid X _ {1} = 1\right)
$$

$$
P \left(X _ {1} = 1 \mid X _ {4} = 1\right)
$$

$$
P \left(X _ {3} = 1 \mid X _ {1} = 0, X _ {4} = 1\right)
$$

# Study question 1.5.4

De-ne the structural model that corresponds to the Monty Hall problem, and use it to describe the joint distribution of all variables.

# Bibliographical Notes for Chapter 1

An extensive account of the history of Simpson‚Äôs paradox is given in Pearl (2009, pp. 174‚Äì182), including many attempts by statisticians to resolve it without invoking causation. A more recent account, geared for statistics instructors is given in (Pearl 2014b). Among the many texts that provide basic introductions to probability theory, Lindley (2014) and Pearl (1988, Chapters 1 and 2) are the closest in spirit to the Bayesian perspective used in Chapter 1. The textbooks by Selvin (2004) and Moore et al. (2014) provide excellent introductions to classical methods of statistics, including parameter estimation, hypothesis testing and regression analysis.

The Monty Hall problem, discussed in Section 1.3, appears in many introductory books on probability theory (e.g., Grinstead and Snell 1998, p. 136; Lindley 2014, p. 201) and is mathematically equivalent to the ‚ÄúThree Prisoners Dilemma‚Äù discussed in (Pearl 1988, pp. 58‚Äì62). Friendly introductions to graphical models are given in Elwert (2013), Glymour and Greenland (2008), and the more advanced texts of Pearl (1988, Chapter 3), Lauritzen (1996) and Koller and Friedman (2009). The product decomposition rule of Section 1.5.2 was used in Howard and Matheson (1981) and Kiiveri et al. (1984) and became the semantic

basis of Bayesian Networks (Pearl 1985)‚Äîdirected acyclic graphs that represent probabilistic knowledge, not necessarily causal. For inference and applications of Bayesian networks, see Darwiche (2009) and Fenton and Neil (2013), and Conrady and Jouffe (2015). The validity of the product decomposition rule for structural causal models was shown in Pearl and Verma (1991).

![](images/d5b167123d346e6524a9130fd9cb97addd72f6f2bf96bdd283c7e7907f571051.jpg)

![](images/43fd0b56466cdda06d11fe4d99eb426f43399695e69afd169c0ef1bb832a5614.jpg)

![](images/678129bb80e6bfc539a9836d532b3f6036105d6659b769fc3d53fe7f7ff05710.jpg)

![](images/ed6a5e5fc39c0a2fd352b8abf441789c82bcf39167224b60b167b7c45adc6ee9.jpg)

![](images/641f7427b2b62d10c87bc34f6025e4851cf99a20f0521d99f964f7116b60904c.jpg)

![](images/91e9e112e3496cac6bba683958669bd82adbbe8321ee7110424041ab7cf6e340.jpg)

# 2

# Graphical Models and Their Applications

# 2.1 Connecting Models to Data

In Chapter 1, we treated probabilities, graphs, and structural equations as isolated mathematical objects with little to connect them. But the three are, in fact, closely linked. In this chapter, we show that the concept of independence, which in the language of probability is de-ned by algebraic equalities, can be expressed visually using directed acyclic graphs (DAGs). Further, this graphical representation will allow us to capture the probabilistic information that is embedded in a structural equation model.

The net result is that a researcher who has scienti-c knowledge in the form of a structural equation model is able to predict patterns of independencies in the data, based solely on the structure of the model‚Äôs graph, without relying on any quantitative information carried by the equations or by the distributions of the errors. Conversely, it means that observing patterns of independencies in the data enables us to say something about whether a hypothesized model is correct. Ultimately, as we will see in Chapter 3, the structure of the graph, when combined with data, will enable us to predict quantitatively the results of interventions without actually performing them.

# 2.2 Chains and Forks

We have so far referred to causal models as representations of the ‚Äúcausal story‚Äù underlying data. Another way to think of this is that causal models represent the mechanism by which data were generated. Causal models are a sort of blueprint of the relevant part of the universe, and we can use them to simulate data from this universe. Given a truly complete causal model for, say, math test scores in high school juniors, and given a complete list of values for every exogenous variable in that model, we could theoretically generate a data point (i.e., a test score) for any individual. Of course, this would necessitate specifying all factors that may have an effect on a student‚Äôs test score, an unrealistic task. In most cases, we will not have such precise knowledge about a model. We might instead have a probability distribution characterizing the exogenous

variables, which would allow us to generate a distribution of test scores approximating that of the entire student population and relevant subgroups of students.

Suppose, however, that we do not have even a probabilistically speci-ed causal model, but only a graphical structure of the model. We know which variables are caused by which other variables, but we don‚Äôt know the strength or nature of the relationships. Even with such limited information, we can discern a great deal about the data set generated by the model. From an unspeci-ed graphical causal model‚Äîthat is, one in which we know which variables are functions of which others, but not the speci-c nature of the functions that connect them‚Äîwe can learn which variables in the data set are independent of each other and which are independent of each other conditional on other variables. These independencies will be true of every data set generated by a causal model with that graphical structure, regardless of the speci-c functions attached to the SCM.

Consider, for instance, the following three hypothetical SCMs, all of which share the same graphical model. The -rst SCM represents the causal relationships among a high school‚Äôs funding in dollars $( X )$ , its average SAT score (Y), and its college acceptance rate $( Z )$ for a given year. The second SCM represents the causal relationships among the state of a light switch $( X )$ , the state of an associated electrical circuit (Y), and the state of a light bulb $( Z )$ . The third SCM concerns the participants in a foot race. It represents causal relationships among the hours that participants work at their jobs each week $( X )$ , the hours the participants put into training each week $( Y )$ , and the completion time, in minutes, the participants achieve in the race $( Z )$ . In all three models, the exogenous variables $( U _ { X } , U _ { Y } , U _ { Z } )$ stand in for any unknown or random effects that may alter the relationship between the endogenous variables. Speci-cally, in SCMs 2.2.1 and 2.2.3, $U _ { Y }$ and $U _ { Z }$ are additive factors that account for variations among individuals. In SCM 2.2.2, $U _ { Y }$ and $U _ { Z }$ take the value 1 if there is some unobserved abnormality, and 0 if there is none.

# SCM 2.2.1 (School Funding, SAT Scores, and College Acceptance)

$$
V = \{X, Y, Z \}, U = \{U _ {X}, U _ {Y}, U _ {Z} \}, F = \{f _ {X}, f _ {Y}, f _ {Z} \}
$$

$$
f _ {X}: X = U _ {X}
$$

$$
f _ {Y}: Y = \frac {x}{3} + U _ {Y}
$$

$$
f _ {Z}: Z = \frac {y}{1 6} + U _ {Z}
$$

# SCM 2.2.2 (Switch, Circuit, and Light Bulb)

$$
V = \{X, Y, Z \}, U = \{U _ {X}, U _ {Y}, U _ {Z} \}, F = \{f _ {X}, f _ {Y}, f _ {Z} \}
$$

$$
f _ {X}: X = U _ {X}
$$

$$
f _ {Y}: Y = \left\{ \begin{array}{l} \text {C l o s e d I F (X = U p A N D U _ {Y} = 0) O R (X = D o w n A N D U _ {Y} = 1)} \\ \text {O p e n o t h e r w i s e} \end{array} \right.
$$

$$
f _ {Z}: Z = \left\{ \begin{array}{l} \text {O n I F (Y = C l o s e d A N D U _ {Z} = 0) O R (Y = O p e n A N D U _ {Z} = 1)} \\ \text {O f f o t h e r w i s e} \end{array} \right.
$$

SCM 2.2.3 (Work Hours, Training, and Race Time)

$$
\begin{array}{l} V = \{X, Y, Z \}, U = \{U _ {X}, U _ {Y}, U _ {Z} \}, F = \left\{f _ {X}, f _ {Y}, f _ {Z} \right\} \\ f _ {X}: X = U _ {X} \\ f _ {Y}: Y = 8 4 - x + U _ {Y} \\ f _ {Z}: Z = \frac {1 0 0}{y} + U _ {Z} \\ \end{array}
$$

SCMs 2.2.1‚Äì2.2.3 share the graphical model shown in Figure 2.1.

SCMs 2.2.1 and 2.2.3 deal with continuous variables; SCM 2.2.2 deals with categorical variables. The relationships between the variables in 2.2.1 are all positive (i.e., the higher the value of the parent variable, the higher the value of the child variable); the correlations between the variables in 2.2.3 are all negative (i.e., the higher the value of the parent variable, the lower the value of the child variable); the correlations between the variables in 2.2.2 are not linear at all, but logical. No two of the SCMs share any functions in common. But because they share a common graphical structure, the data sets generated by all three SCMs must share certain independencies‚Äîand we can predict those independencies simply by examining the graphical model in Figure 2.1. The independencies shared by data sets generated by these three SCMs, and the dependencies that are likely shared by all such SCMs, are these:

1. Z and Y are likely dependent

For some $z , y , P ( Z = z | Y = y ) \neq P ( Z = z )$

2. Y and $X$ are likely dependent

For some $y , x , P ( Y = y | X = x ) \neq P ( Y = y )$

3. Z and $X$ are likely dependent

For some $z , x , P ( Z = z | X = x ) \neq P ( Z = z )$

4. Z and $X$ are independent, conditional on Y

For all $x , y , z , P ( Z = z | X = x , Y = y ) = P ( Z = z | Y = y )$

To understand why these independencies and dependencies hold, let‚Äôs examine the graphical model. First, we will verify that any two variables with an edge between them are likely dependent. Remember that an arrow from one variable to another indicates that the -rst variable causes the second‚Äîthat is, the value of the -rst variable is part of the function that determines the value of the second. Therefore, the second variable depends on the -rst for its value; there

![](images/929de3871f0472fd4b11aafdce68b69dd3efedc42a1f763a376e919d414d995b.jpg)  
Figure 2.1 The graphical model of SCMs 2.2.1‚Äì2.2.3

is some case in which changing the value of the -rst variable changes the value of the second. That makes it likely that when we examine those variables in the data set, the probability that one variable takes a given value will change, given that we know the value of the other variable. So in a typical causal model, regardless of the speci-c functions, two variables connected by an edge are dependent. By this reasoning, we can see that in SCMs 2.2.1‚Äì2.2.3, $Z$ and Y are likely dependent, and Y and $X$ are likely dependent.1

From these two facts, we can conclude that $Z$ and $X$ are likely dependent. If Z depends on Y for its value, and Y depends on $X$ for its value, then $Z$ likely depends on $X$ for its value. There are pathological cases in which this is not true. Consider, for example, the following SCM, which also has the graph in Figure 2.1.

# SCM 2.2.4 (Pathological Case of Intransitive Dependence)

$$
\begin{array}{l} V = \{X, Y, Z \}, U = \left\{U _ {X}, U _ {Y}, U _ {Z} \right\}, F = \left\{f _ {X}, f _ {Y}, f _ {Z} \right\} \\ f _ {X}: X = U _ {X} \\ f _ {Y}: Y = \left\{ \begin{array}{l l} a & \text {I F X = 1 A N D U _ {Y} = 1} \\ b & \text {I F X = 2 A N D U _ {Y} = 1} \\ c & \text {I F U _ {Y} = 2} \end{array} \right. \\ f _ {Z}: Z = \left\{ \begin{array}{l l} i & \text {I F Y = c O R U _ {Z} = 1} \\ j & \text {I F U _ {Z} = 2} \end{array} \right. \\ \end{array}
$$

In this case, no matter what value $U _ { Y }$ and $U _ { Z }$ take, $X$ will have no effect on the value that Z takes; changes in $X$ account for variation in Y between $a$ and $b$ , but $Y$ doesn‚Äôt affect Z unless it takes the value $c$ . Therefore, $X$ and $Z$ vary independently in this model. We will call cases such as these intransitive cases.

However, intransitive cases form only a small number of the cases we will encounter. In most cases, the values of $X$ and $Z$ vary together just as $X$ and $Y$ do, and Y and Z. Therefore, they are likely dependent in the data set.

Now, let‚Äôs consider point 4: Z and $X$ are independent conditional on Y. Remember that when we condition on Y, we -lter the data into groups based on the value of Y. So we compare all the cases where $Y = a$ , all the cases where $Y = b$ , and so on. Let‚Äôs assume that we‚Äôre looking at the cases where $Y = a$ . We want to know whether, in these cases only, the value of $Z$ is independent of the value of $X$ . Previously, we determined that $X$ and $Z$ are likely dependent, because when the value of $X$ changes, the value of $Y$ likely changes, and when the value of $Y$ changes, the value of $Z$ is likely to change. Now, however, examining only the cases where $Y = a$ , when we select cases with different values of $X$ , the value of $U _ { Y }$ changes so as to keep $Y$ at $Y = a$ , but since Z depends only on $Y$ and $U _ { Z }$ , not on $U _ { Y }$ , the value of $Z$ remains unaltered. So selecting a different value of $X$ doesn‚Äôt change the value of Z. So, in the case where $Y = a$ , $X$ is independent of $Z$ . This is of course true no matter which speci-c value of $Y$ we condition on. So $X$ is independent of $Z$ , conditional on Y.

This con-guration of variables‚Äîthree nodes and two edges, with one edge directed into and one edge directed out of the middle variable‚Äîis called a chain. Analogous reasoning to the above tells us that in any graphical model, given any two variables $X$ and $Y$ , if the only path between $X$ and $Y$ is composed entirely of chains, then $X$ and $Y$ are independent conditional

on any intermediate variable on that path. This independence relation holds regardless of the functions that connect the variables. This gives us a rule:

Rule 1 (Conditional Independence in Chains) Two variables, X and Y, are conditionally independent given Z, if there is only one unidirectional path between X and Y and Z is any set of variables that intercepts that path.

An important note: Rule 1 only holds when we assume that the error terms $U _ { X }$ , $U _ { Y }$ , and $U _ { Z }$ are independent of each other. If, for instance, $U _ { X }$ were a cause of $U _ { Y }$ , then conditioning on Y would not necessarily make $X$ and $Z$ independent‚Äîbecause variations in $X$ could still be associated with variations in Y, through their error terms.

Now, consider the graphical model in Figure 2.2. This structure might represent, for example, the causal mechanism that connects a day‚Äôs temperature in a city in degrees Fahrenheit $( X )$ , the number of sales at a local ice cream shop on that day (Y), and the number of violent crimes in the city on that day (Z). Possible functional relationships between these variables are given in SCM 2.2.5. Or the structure might represent, as in SCM 2.2.6, the causal mechanism that connects the state (up or down) of a switch $( X )$ , the state (on or off) of one light bulb (Y), and the state (on or off) of a second light bulb (Z). The exogenous variables $U _ { X } , U _ { Y }$ , and $U _ { Z }$ represent other, possibly random, factors that inuence the operation of these devices.

SCM 2.2.5 (Temperature, Ice Cream Sales, and Crime)

$$
V = \{X, Y, Z \}, U = \{U _ {X}, U _ {Y}, U _ {Z} \}, F = \left\{f _ {X}, f _ {Y}, f _ {Z} \right\}
$$

$$
f _ {X}: X = U _ {X}
$$

$$
f _ {Y}: Y = 4 x + U _ {Y}
$$

$$
f _ {Z}: Z = \frac {x}{1 0} + U _ {Z}
$$

SCM 2.2.6 (Switch and Two Light Bulbs)

$$
V = \{X, Y, Z \}, U = \left\{U _ {X}, U _ {Y}, U _ {Z} \right\}, F = \left\{f _ {X}, f _ {Y}, f _ {Z} \right\}
$$

$$
f _ {X}: X = U _ {X}
$$

$$
f _ {Y}: Y = \left\{ \begin{array}{l} \text {O n I F (X = U p A N D U _ {Y} = 0) O R (X = D o w n A N D U _ {Y} = 1)} \\ \text {O f f o t h e r w i s e} \end{array} \right.
$$

$$
f _ {Z}: Z = \left\{ \begin{array}{l} \text {O n I F (X = U p A N D U _ {Z} = 0) O R (X = D o w n A N D U _ {Z} = 1)} \\ \text {O f f o t h e r w i s e} \end{array} \right.
$$

![](images/de4021c16dcee82bf5316d3b52c8ee90721fa61e8981c222e997cfe443a7b984.jpg)  
Figure 2.2 The graphical model of SCMs 2.2.5 and 2.2.6

If we assume that the error terms $U _ { X } , U _ { Y }$ , and $U _ { Z }$ are independent, then by examining the graphical model in Figure 2.2, we can determine that SCMs 2.2.5 and 2.2.6 share the following dependencies and independencies:

1. $X$ and Y are likely dependent.

For some $x , y , P ( X = x | Y = y ) \neq P ( X = x )$

2. X and Z are likely dependent.

For some $x , z , P ( X = x | Z = z ) \neq P ( X = x )$

3. Z and Y are likely dependent.

For some $z , y , P ( Z = z | Y = y ) \neq P ( Z = z )$

4. Y and Z are independent, conditional on $X$ .

For all $x , y , z , P ( Y = y | Z = z , X = x ) = P ( Y = y | X = x )$

Points 1 and 2 follow, once again, from the fact that $Y$ and $Z$ are both directly connected to $X$ by an arrow, so when the value of $X$ changes, the values of both $Y$ and $Z$ likely change. This tells us something further, however: If $Y$ changes when $X$ changes, and $Z$ changes when $X$ changes, then it is likely (though not certain) that Y changes together with $Z$ , and vice versa. Therefore, since a change in the value of $Y$ gives us information about an associated change in the value of Z, Y and $Z$ are likely dependent variables.

Why, then, are Y and $Z$ independent conditional on $X ?$ Well, what happens when we condition on $X ?$ We -lter the data based on the value of $X$ . So now, we‚Äôre only comparing cases where the value of $X$ is constant. Since $X$ does not change, the values of $Y$ and $Z$ do not change in accordance with it‚Äîthey change only in response to $U _ { Y }$ and $U _ { Z }$ , which we have assumed to be independent. Therefore, any additional changes in the values of Y and $Z$ must be independent of each other.

This con-guration of variables‚Äîthree nodes, with two arrows emanating from the middle variable‚Äîis called a fork. The middle variable in a fork is the common cause of the other two variables, and of any of their descendants. If two variables share a common cause, and if that common cause is part of the only path between them, then analogous reasoning to the above tells us that these dependencies and conditional independencies are true of those variables. Therefore, we come by another rule:

Rule 2 (Conditional Independence in Forks) If a variable X is a common cause of variables Y and Z, and there is only one path between Y and Z, then Y and Z are independent conditional on X.

# 2.3 Colliders

So far we have looked at two simple con-gurations of edges and nodes that can occur on a path between two variables: chains and forks. There is a third such con-guration that we speak of separately, because it carries with it unique considerations and challenges. The third con-guration contains a collider node, and it occurs when one node receives edges from two other nodes. The simplest graphical causal model containing a collider is illustrated in Figure 2.3, representing a common effect, Z, of two causes $X$ and Y.

As is the case with every graphical causal model, all SCMs that have Figure 2.3 as their graph share a set of dependencies and independencies that we can determine from the graphical

![](images/1c851901c22846ba8e88adc39fca5515e2043aff4890182c65985993fd342b9d.jpg)  
Figure 2.3 A simple collider

model alone. In the case of the model in Figure 2.3, assuming independence of $U _ { X } , U _ { Y }$ , and $U _ { Z }$ , these independencies are as follows:

1. X and Z are likely dependent.

For some $x , z , P ( X = x | Z = z ) \neq P ( X = x )$

2. Y and Z are likely dependent.

For some $y , z , P ( Y = y | Z = z ) \neq P ( Y = y )$

3. X and Y are independent.

For all $x , y , P ( X = x | Y = y ) = P ( X = x )$

4. X and Y are likely dependent conditional on Z.

For some $x , y , z , P ( X = x | Y = y , Z = z ) \neq P ( X = x | Z = z )$

The truth of the -rst two points was established in Section 2.2. Point 3 is self-evident; neither $X$ nor $Y$ is a descendant or an ancestor of the other, nor do they depend for their value on the same variable. They respond only to $U _ { X }$ and $U _ { Y }$ , which are assumed independent, so there is no causal mechanism by which variations in the value of $X$ should be associated with variations in the value of Y. This independence also reects our understanding of how causation operates in time; events that are independent in the present do not become dependent merely because they may have common effects in the future.

Why, then, does point 4 hold? Why would two independent variables suddenly become dependent when we condition on their common effect? To answer this question, we return again to the de-nition of conditioning as -ltering by the value of the conditioning variable. When we condition on $Z$ , we limit our comparisons to cases in which Z takes the same value. But remember that $Z$ depends, for its value, on $X$ and Y. So, when comparing cases where $Z$ takes some value, any change in value of $X$ must be compensated for by a change in the value of Y‚Äîotherwise, the value of Z would change as well.

The reasoning behind this attribute of colliders‚Äîthat conditioning on a collision node produces a dependence between the node‚Äôs parents‚Äîcan be dif-cult to grasp at -rst. In the most basic situation where $Z = X + Y$ , and $X$ and Y are independent variables, we have the following logic: If I tell you that $X = 3$ , you learn nothing about the potential value of Y, because the two numbers are independent. On the other hand, if I start by telling you that $Z = 1 0$ , then telling you that $X = 3$ immediately tells you that Y must be 7. Thus, $X$ and Y are dependent, given that $Z = 1 0$ .

This phenomenon can be further clari-ed through a real-life example. For instance, suppose a certain college gives scholarships to two types of students: those with unusual musical talents and those with extraordinary grade point averages. Ordinarily, musical talent and scholastic achievement are independent traits, so, in the population at large, -nding a person with musical

talent tells us nothing about that person‚Äôs grades. However, discovering that a person is on a scholarship changes things; knowing that the person lacks musical talent then tells us immediately that he is likely to have high grade point average. Thus, two variables that are marginally independent become dependent upon learning the value of a third variable (scholarship) that is a common effect of the -rst two.

Let‚Äôs examine a numerical example. Consider a simultaneous (independent) toss of two fair coins and a bell that rings whenever at least one of the coins lands on heads. Let the outcomes of the two coins be denoted $X$ and Y, respectively, and let Z stand for the state of the bell, with $Z = 1$ representing ringing, and $Z = 0$ representing silence. This mechanism can be represented as a collider as in Figure 2.3, in which the outcomes of the two coins are the parent nodes, and the state of the bell is the collision node.

If we know that Coin 1 landed on heads, it tells us nothing about the outcome of Coin 2, due to their independence. But suppose that we hear the bell ring and then we learn that Coin 1 landed on tails. We now know that Coin 2 must have landed on heads. Similarly, if we assume that we‚Äôve heard the bell ring, the probability that Coin 1 landed on heads changes if we learn that Coin 2 also landed on heads. This particular change in probability is somewhat subtler than the -rst case.

To see the latter calculation, consider the initial probabilities as shown in Table 2.1.

We see that

$$
P (X = ‚Äú H e a d s ‚Äù | Y = ‚Äú H e a d s ‚Äù) = P (X = ‚Äú T a i l s ‚Äù | Y = ‚Äú T a i l s ‚Äù) = \frac {1}{2}
$$

That is, $X$ and $Y$ are independent. Now, let‚Äôs condition on $Z = 1$ and $Z = 0$ (the bell ringing and not ringing). The resulting data subsets are shown in Table 2.2.

By calculating the probabilities in these tables, we obtain

$$
P (X = ‚Äú H e a d s ‚Äù | Z = 1) = \frac {1}{3} + \frac {1}{3} = \frac {2}{3}
$$

If we further -lter the $Z = 1$ subtable to examine only those cases where $Y = { } ^ {  } H e a d s ^ { \prime \prime }$ , we get

$$
P (X = ‚Äú H e a d s ‚Äù | Y = ‚Äú H e a d s ‚Äù, Z = 1) = \frac {1}{2}
$$

We see that, given $Z = 1$ , the probability of $X =$ ‚ÄúHeads‚Äù changes from $\frac { 2 } { 3 }$ to $\frac { 1 } { 2 }$ upon learning that Y = ‚ÄúHeads.‚Äù So, clearly, $X$ and $Y$ are dependent given $Z = 1$ . A more pronounced dependence occurs, of course, when the bell does not ring $( Z = 0$ ), because then we know that both coins must have landed on tails.

Table 2.1 Probability distribution for two ips of a fair coin, with $X$ representing ip one, Y representing ip two, and $Z$ representing a bell that rings if either ip results in heads   

<table><tr><td>X</td><td>Y</td><td>Z</td><td>P(X,Y,Z)</td></tr><tr><td>Heads</td><td>Heads</td><td>1</td><td>0.25</td></tr><tr><td>Heads</td><td>Tails</td><td>1</td><td>0.25</td></tr><tr><td>Tails</td><td>Heads</td><td>1</td><td>0.25</td></tr><tr><td>Tails</td><td>Tails</td><td>0</td><td>0.25</td></tr></table>

Table 2.2 Conditional probability distributions for the distribution in Table 2.1. (Top: Distribution conditional on $Z = 1$ . Bottom: Distribution conditional on $Z = 0$ )   

<table><tr><td>X</td><td>Y</td><td>P(X, Y|Z = 1)</td></tr><tr><td>Heads</td><td>Heads</td><td>0.333</td></tr><tr><td>Heads</td><td>Tails</td><td>0.333</td></tr><tr><td>Tails</td><td>Heads</td><td>0.333</td></tr><tr><td>Tails</td><td>Tails</td><td>0</td></tr><tr><td>X</td><td>Y</td><td>Pr(X, Y|Z = 0)</td></tr><tr><td>Heads</td><td>Heads</td><td>0</td></tr><tr><td>Heads</td><td>Tails</td><td>0</td></tr><tr><td>Tails</td><td>Heads</td><td>0</td></tr><tr><td>Tails</td><td>Tails</td><td>1</td></tr></table>

Another example of colliders in action‚Äîone that may serve to further illuminate the dif-- culty that such con-gurations can present to statisticians‚Äîis the Monty Hall Problem, which we -rst encountered in Section 1.3. At its heart, the Monty Hall Problem reects the presence of a collider. Your initial choice of door is one parent node; the door behind which the car is placed is the other parent node; and the door Monty opens to reveal a goat is the collision node, causally affected by both the other two variables. The causation here is clear: If you choose Door $A$ , and if Door A has a goat behind it, Monty is forced to open whichever of the remaining doors that has a goat behind it.

Your initial choice and the location of the car are independent; that‚Äôs why you initially have a $\frac 1 3$ chance of choosing the door with the car behind it. However, as with the two independent coins, conditional on Monty‚Äôs choice of door, your initial choice and the placement of the prizes are dependent. Though the car may only be behind Door $B$ in $\frac 1 3$ of cases, it will be behind Door $B$ in ${ \frac { 2 } { 3 } } .$ of cases in which you choose Door A and Monty opened Door $C$ .

Just as conditioning on a collider makes previously independent variables dependent, so too does conditioning on any descendant of a collider. To see why this is true, let‚Äôs return to our example of two independent coins and a bell. Suppose we do not hear the bell directly, but instead rely on a witness who is somewhat unreliable; whenever the bell does not ring, there is $50 \%$ chance that our witness will falsely report that it did. Letting W stand for the witness‚Äôs report, the causal structure is shown in Figure 2.4, and the probabilities for all combinations of X, Y, and W are shown in Table 2.3.

The reader can easily verify that, based on this table, we have

$$
P (X = ‚Äú H e a d s ‚Äù | Y = ‚Äú H e a d s ‚Äù) = P (X = ‚Äú H e a d s ‚Äù) = \frac {1}{2}
$$

and

$$
P (X = ‚Äú H e a d s ‚Äù | W = 1) = (0. 2 5 + 0. 2 5) \div (0. 2 5 + 0. 2 5 + 0. 2 5 + 0. 1 2 5) = \frac {0 . 5}{0 . 8 7 5}
$$

and

$$
P (X = ‚Äú H e a d s ‚Äù | Y = ‚Äú H e a d s ‚Äù, W = 1) = 0. 2 5 \div (0. 2 5 + 0. 2 5) = 0. 5 <   \frac {0 . 5}{0 . 8 7 5}
$$

![](images/377447c79cb693bd1f0bf8dddd4cbd0fecf23ae024895790c6c9694259623a59.jpg)  
Figure 2.4 A simple collider, Z, with one child, W, representing the scenario from Table 2.3, with $X$ representing one coin ip, Y representing the second coin ip, $Z$ representing a bell that rings if either $X$ or Y is heads, and W representing an unreliable witness who reports on whether or not the bell has rung

Table 2.3 Probability distribution for two ips of a fair coin and a bell that rings if either ip results in heads, with $X$ representing ip one, Y representing ip two, and W representing a witness who, with variable reliability, reports whether or not the bell has rung   

<table><tr><td>X</td><td>Y</td><td>W</td><td>P(X, Y, W)</td></tr><tr><td>Heads</td><td>Heads</td><td>1</td><td>0.25</td></tr><tr><td>Heads</td><td>Tails</td><td>1</td><td>0.25</td></tr><tr><td>Tails</td><td>Heads</td><td>1</td><td>0.25</td></tr><tr><td>Tails</td><td>Tails</td><td>1</td><td>0.125</td></tr><tr><td>Tails</td><td>Tails</td><td>0</td><td>0.125</td></tr></table>

Thus, $X$ and $Y$ are independent before reading the witness report, but become dependent thereafter.

These considerations lead us to a third rule, in addition to the two we established in Section 2.2.

Rule 3 (Conditional Independence in Colliders) If a variable Z is the collision node between two variables X and Y, and there is only one path between X and Y, then X and Y are unconditionally independent but are dependent conditional on Z and any descendants of Z.

Rule 3 is extremely important to the study of causality. In the coming chapters, we will see that it allows us to test whether a causal model could have generated a data set, to discover models from data, and to fully resolve Simpson‚Äôs Paradox by determining which variables to measure and how to estimate causal effects under confounding.

Remark Inquisitive students may wonder why it is that dependencies associated with conditioning on a collider are so surprising to most people‚Äîas in, for example, the Monty Hall example. The reason is that humans tend to associate dependence with causation. Accordingly, they assume (wrongly) that statistical dependence between two variables can only exist if there is a causal mechanism that generates such dependence; that is, either one of the variables causes the other or a third variable causes both. In the case of a collider, they are surprised to -nd a

![](images/5d7df839fb35a6b81f0278069b96f93043e62da429461042703eefc5d6f28f4a.jpg)  
Figure 2.5 A directed graph for demonstrating conditional independence (error terms are not shown explicitly)

![](images/99d2090aef30bba741ce42c419e7f08f521bff8a05d41d771a8e370726c76823.jpg)  
Figure 2.6 A directed graph in which $P$ is a descendant of a collider

dependence that is created in a third way, thus violating the assumption of ‚Äúno correlation without causation.‚Äù

Study questions

# Study question 2.3.1

(a) List all pairs of variables in Figure 2.5 that are independent conditional on the set $Z = \{ R , V \}$ .   
(b) For each pair of nonadjacent variables in Figure 2.5, give a set of variables that, when conditioned on, renders that pair independent.   
(c) List all pairs of variables in Figure 2.6 that are independent conditional on the set $Z = \{ R , P \}$ .   
(d) For each pair of nonadjacent variables in Figure 2.6, give a set of variables that, when conditioned on, renders that pair independent.   
(e) Suppose we generate data by the model described in Figure 2.5, and we -t them with the linear equation $Y = a + b X + c Z .$ . Which of the variables in the model may be chosen for Z so as to guarantee that the slope b would be equal to zero? [Hint: Recall, a non zero slope implies that Y and X are dependent given Z.]   
(f) Continuing question (e), but now in reference to suppose we -t th e data withFigure 2.6, the equation:

$$
Y = a + b X + c R + d S + e T + f P
$$

which of the coef-cients would be zero?

# 2.4 $\pmb { d }$ -separation

Causal models are generally not as simple as the cases we have examined so far. Speci-cally, it is rare for a graphical model to consist of a single path between variables. In most graphical models, pairs of variables will have multiple possible paths connecting them, and each path will traverse a variety of chains, forks, and colliders. The question remains whether there is a criterion or process that can be applied to a graphical causal model of any complexity in order to predict dependencies that are shared by all data sets generated by that graph.

There is, indeed, such a process: $d$ -separation, which is built upon the rules established in the previous section. $d \cdot$ -separation (the $d$ stands for ‚Äúdirectional‚Äù) allows us to determine, for any pair of nodes, whether the nodes are $d$ -connected, meaning there exists a connecting path between them, or $d$ -separated, meaning there exists no such path. When we say that a pair of nodes are $d \cdot$ -separated, we mean that the variables they represent are de-nitely independent; when we say that a pair of nodes are $d$ -connected, we mean that they are possibly, or most likely, dependent.2

Two nodes $X$ and Y are $d$ -separated if every path between them (should any exist) is blocked. If even one path between $X$ and $Y$ is unblocked, $X$ and Y are $d$ -connected. The paths between variables can be thought of as pipes, and dependence as the water that ows through them; if even one pipe is unblocked, some water can pass from one place to another, and if a single path is clear, the variables at either end will be dependent. However, a pipe need only be blocked in one place to stop the ow of water through it, and similarly, it takes only one node to block the passage of dependence in an entire path.

There are certain kinds of nodes that can block a path, depending on whether we are performing unconditional or conditional $d$ -separation. If we are not conditioning on any variable, then only colliders can block a path. The reasoning for this is fairly straightforward: as we saw in Section 2.3, unconditional dependence can‚Äôt pass through a collider. So if every path between two nodes $X$ and Y has a collider in it, then $X$ and Y cannot be unconditionally dependent; they must be marginally independent.

If, however, we are conditioning on a set of nodes $Z$ , then the following kinds of nodes can block a path:

‚Ä¢ A collider that is not conditioned on (i.e., not in $Z$ ), and that has no descendants in Z.   
‚Ä¢ A chain or fork whose middle node is in $Z$ .

The reasoning behind these points goes back to what we learned in Sections 2.2 and 2.3. A collider does not allow dependence to ow between its parents, thus blocking the path. But Rule 3 tells us that when we condition on a collider or its descendants, the parent nodes may become dependent. So a collider whose collision node is not in the conditioning set Z would block dependence from passing through a path, but one whose collision node, or its descendants, is in the conditioning set would not. Conversely, dependence can pass through noncolliders‚Äîchains and forks‚Äîbut Rules 1 and 2 tell us that when we condition on them, the variables on either end of those paths become independent (when we consider one path at a time). So any noncollision node in the conditioning set would block dependence, whereas one that is not in the conditioning set would allow dependence through.

We are now prepared to give a general de-nition of $d$ -separation:

De-nition 2.4.1 $\mathbf { \Delta } _ { \mathbf { \alpha } } \mathbf { \bar { \alpha } } _ { d }$ -separation) A path p is blocked by a set of nodes Z if and only if

![](images/e6c41c07912f87a255e4364eec4fb8c53a426093233492db54e0cfcd5bf85e9d.jpg)  
Figure 2.7 A graphical model containing a collider with child and a fork

If Z blocks every path between two nodes X and Y, then X and Y are d-separated, conditional on Z, and thus are independent conditional on Z.

Armed with the tool of $d$ -separation, we can now look at some more complex graphical models and determine which variables in them are independent and dependent, both marginally and conditional on other variables. Let‚Äôs take, for example, the graphical model in Figure 2.7. This graph might be associated with any number of causal models. The variables might be discrete, continuous, or a mixture of the two; the relationships between them might be linear, exponential, or any of an in-nite number of other relations. No matter the model, however, $d$ -separation will always provide the same set of independencies in the data the model generates.

In particular, let‚Äôs look at the relationship between $Z$ and Y. Using an empty conditioning set, they are $d \mathbf { \cdot }$ -separated, which tells us that $Z$ and $Y$ are unconditionally independent. Why? Because there is no unblocked path between them. There is only one path between $Z$ and Y, and that path is blocked by a collider $Z \to W  X .$ ).

But suppose we condition on W. $d$ -separation tells us that $Z$ and $Y$ are $d$ -connected, conditional on W. The reason is that our conditioning set is now $\{ W \}$ , and since the only path between $Z$ and Y contains a fork $( X )$ that is not in that set, and the only collider (W) on the path is in that set, that path is not blocked. (Remember that conditioning on colliders ‚Äúunblocks‚Äù them.) The same is true if we condition on $U$ , because $U$ is a descendant of a collider along the path between Z and Y.

On the other hand, if we condition on the set $\{ W , X \} , Z$ and Y remain independent. This time, the path between Z and Y is blocked by the -rst criterion, rather than the second: There is now a noncollider node $( X )$ on the path that is in the conditioning set. Though W has been unblocked by conditioning, one blocked node is suf-cient to block the entire path. Since the only path between $Z$ and $Y$ is blocked by this conditioning set, $Z$ and $Y$ are $d$ -separated conditional on $\{ W , X \}$ .

Now, consider what happens when we add another path between $Z$ and $Y$ , as in Figure 2.8. Z and $Y$ are now unconditionally dependent. Why? Because there is a path between them $( Z \left. T \right. Y )$ that contains no colliders. If we condition on $T$ , however, that path is blocked, and $Z$ and $Y$ become independent again. Conditioning on $\{ T , W \}$ , on the other hand, makes them $d \mathbf { \cdot }$ -connected again (conditioning on $T$ blocks the path $Z \left. T \right. Y$ , but conditioning on $W$ unblocks the path $Z \to W  X \to Y )$ ). And if we add $X$ to the conditioning set, making it $\{ T , W , X \} , Z$ , and $Y$ become independent yet again! In this graph, $Z$ and $Y$ are $d$ -connected (and therefore likely dependent) conditional

![](images/6ec4f68b493f47bbd681f4e1288e5bb3a1ee584ff5401e9626d9d82b3b546052.jpg)  
Figure 2.8 The model from Figure 2.7 with an additional forked path between $Z$ and $Y$

on W , U, {W , U}, {W , T }, $\{ U , T \}$ , {W , U, T }, {W , X}, $\{ U , X \}$ , and $\{ W , U , X \}$ . They are $d$ -separated (and therefore independent) conditional on T, $\{ X , T \}$ $\{ X , T \} , \{ W , X , T \} , \{ U , X , T \}$ , and $\{ W , U , X , T \}$ . Note that $T$ is in every conditioning set that $d$ -separates $Z$ and $Y$ ; that‚Äôs because $T$ is the only node in a path that unconditionally $d$ -connects $Z$ and $Y$ , so unless it is conditioned on, $Z$ and $Y$ will always be $d$ -connected.

# Study questions

# Study question 2.4.1

Figure 2.9 below represents a causal graph from which the error terms have been deleted. Assume that all those errors are mutually independent.

(a) For each pair of nonadjacent nodes in this graph, -nd a set of variables that d-separates that pair. What does this list tell us about independencies in the data?   
(b) Repeat question (a) assuming that only variables in the set $\{ Z _ { 3 } , W , X , Z _ { 1 } \}$ can be measured.   
(c) For each pair of nonadjacent nodes in the graph, determine whether they are independent conditional on all other variables.   
(d) For every variable V in the graph, -nd a minimal set of nodes that renders V independent of all other variables in the graph.   
(e) Suppose we wish to estimate the value of Y from measurements taken on all other variables in the model. Find the smallest set of variables that would yield as good an estimate of Y as when we measured all variables.   
(f) Repeat question (e) assuming that we wish to estimate the value of $Z _ { 2 }$ .   
(g) Suppose we wish to predict the value of $Z _ { 2 }$ from measurements of $Z _ { 3 }$ . Would the quality of our prediction improve if we add measurement of W? Explain.

# 2.5 Model Testing and Causal Search

The preceding sections demonstrate that causal models have testable implications in the data sets they generate. For instance, if we have a graph $G$ that we believe might have generated

![](images/d89c83893cf1dad4beade8b309be74b08d9bcbcfdc9c45bc6c0294f6a9eb1ad7.jpg)  
Figure 2.9 A causal graph used in study question 2.4.1, all $U$ terms (not shown) are assumed independent

a data set $S$ , $d$ -separation will tell us which variables in $G$ must be independent conditional on which other variables. Conditional independence is something we can test for using a data set. Suppose we list the $d$ -separation conditions in $G$ , and note that variables $A$ and $B$ must be independent conditional on $C$ . Then, suppose we estimate the probabilities based on $S$ , and discover that the data suggests that A and $B$ are not independent conditional on $C$ . We can then reject $G$ as a possible causal model for S.

We can demonstrate it on the causal model of Figure 2.9. Among the many conditional independencies advertised by the model, we -nd that W and $Z _ { 1 }$ are independent given $X$ , because X d-separates W from $Z _ { 1 }$ . Now suppose we regress W on $X$ and $Z _ { 1 }$ . Namely, we -nd the line

$$
w = r _ {\mathrm {X}} x + r _ {\mathrm {I}} z _ {\mathrm {I}}
$$

that best fits our data. If it turns out that $r _ { 1 }$ is not equal to zero, we know that W depends on $Z _ { 1 }$ given $X$ and, consequently, that the model is wrong. [Recall, conditional correlation implies conditional dependence.] Not only do we know that the model is wrong,but we also know where it is wrong; the true model must have a path between $W$ and $Z _ { 1 }$ that is not $d$ -separated by $X$ . Finally, this is a theoretical result that holds for all acyclic models with independent errors (Verma and Pearl 1990), and we also know that if every $d$ -separation condition in the model matches a conditional independence in the data, then no further test can refute the model. This means that, for any data set whatsoever, one can always -nd a set of functions $F$ for the model and an assignment of probabilities to the $U$ terms, so as to generate the data precisely.

There are other methods for testing the -tness of a model. The standard way of evaluating -tness involves a statistical hypothesis test over the entire model, that is, we evaluate how likely it is for the observed samples to have been generated by the hypothesized model, as opposed to sheer chance. However, since the model is not fully speci-ed, we need to -rst estimate its parameters before evaluating that likelihood. This can be done (approximately) when we assume a linear and Gaussian model (i.e., all functions in the model are linear and all error terms are normally distributed), because, under such assumptions, the joint distribution (also Gaussian) can be expressed succinctly in terms of the model‚Äôs parameters, and we can then evaluate the likelihood that the observed samples have been generated by the fully parameterized model (Bollen 1989).

There are, however, a number of issues with this procedure. First, if any parameter cannot be estimated, then the joint distribution cannot be estimated, and the model cannot be tested.

As we shall see in Section 3.8.3, this can occur when some of the error terms are correlated or, equivalently, when some of the variables are unobserved. Second, this procedure tests models globally. If we discover that the model is not a good -t to the data, there is no way for us to determine why that is‚Äîwhich edges should be removed or added to improve the -t. Third, when we test a model globally, the number of variables involved may be large, and if there is measurement noise and/or sampling variation associated with each variable, the test will not be reliable.

$d \cdot$ -separation presents several advantages over this global testing method. First, it is nonparametric, meaning that it doesn‚Äôt rely on the speci-c functions that connect variables; instead, it uses only the graph of the model in question. Second, it tests models locally, rather than globally. This allows us to identify speci-c areas, where our hypothesized model is awed, and to repair them, rather than starting from scratch on a whole new model. It also means that if, for whatever reason, we can‚Äôt identify the coef-cient in one area of the model, we can still get some incomplete information about the rest of the model. (As opposed to the -rst method, in which if we could not estimate one coef-cient, we could not test any part of the model.)

If we had a computer, we could test and reject many possible models in this way, eventually whittling down the set of possible models to only a few whose testable implications do not contradict the dependencies present in the data set. It is a set of models, rather than a single model, because some graphs have indistinguishable implications. A set of graphs with indistinguishable implications is called an equivalence class. Two graphs $G _ { 1 }$ and $G _ { 2 }$ are in the same equivalence class if they share a common skeleton‚Äîthat is, the same edges, regardless of the direction of those edges‚Äîand if they share common $\nu$ -structures, that is, colliders whose parents are not adjacent. Any two graphs that satisfy this criterion have identical sets of $d \mathbf { \cdot }$ -separation conditions and, therefore, identical sets of testable implications (Verma and Pearl 1990).

The importance of this result is that it allows us to search a data set for the causal models that could have generated it. Thus, not only can we start with a causal model and generate a data set‚Äîbut we can also start with a data set, and reason back to a causal model. This is enormously useful, since the object of most data-driven research is exactly to -nd a model that explains the data.

There are other methods of causal search‚Äîincluding some that rely on the kind of global model testing with which we began the section‚Äîbut a full investigation of them is beyond the scope of this book. Those interested in learning more about search should refer to (Pearl 2000; Pearl and Verma 1991; Rebane and Pearl 1987; Spirtes and Glymour 1991; Spirtes et al. 1993).

# Study questions

# Study question 2.5.1

(a) Which of the arrows in Figure 2.9 can be reversed without being detected by any statistical test? [Hint: Use the criterion for equivalence class.]   
(b) List all graphs that are observationally equivalent to the one in Figure 2.9.   
(c) List the arrows in Figure 2.9 whose directionality can be determined from nonexperimental data.

(d) Write down a regression equation for Y such that, if a certain coef-cient in that equation is nonzero, the model of Figure 2.9 is wrong.   
(e) Repeat question (d) for variable $Z _ { 3 }$ .   
(f) Repeat question (e) assuming the $X$ is not measured.   
(g) How many regression equations of the type described in (d) and (e) are needed to ensure that the model is fully tested, namely, that if it passes all these tests, it cannot be refuted by additional tests of these kind. [Hint: Ensure that you test every vanishing partial reg-ression coefficient that is implied by the product decomposition (1.29).]

# Bibliographical Notes for Chapter 2

The distinction between chains and forks in causal models was made by Simon (1953) and Reichenbach (1956) while the treatment of colliders (or common effect) can be traced back to the English economist Pigou (1911) (see Stigler 1999, pp. 36‚Äì41). In epidemiology, colliders came to be associated with ‚ÄúSelection bias‚Äù or ‚ÄúBerkson paradox‚Äù (Berkson 1946) while in arti-cial intelligence it came to be known as the ‚Äúexplaining away effect‚Äù (Kim and Pearl 1983). The rule of $d$ -separation for determining conditional independence by graphs (De-nition 2.4.1) was introduced in Pearl (1986) and formally proved in Verma and Pearl (1988) using the theory of graphoids (Pearl and Paz 1987). Gentle introductions to $d$ -separation are available in Hayduk et al. (2003), Glymour and Greenland (2008), and Pearl (2000, pp. 335‚Äì337). Algorithms and software for detecting $d$ -separation, as well as -nding minimal separating sets are described in Tian et al. (1998), Kyono (2010), and Textor et al. (2011). The advantages of local over global model testing, are discussed in Pearl (2000, pp. 144‚Äì145) and further elaborated in Chen and Pearl (2014). Recent applications of $d$ -separation include extrapolation across populations (Pearl and Bareinboim 2014), recovering from sampling selection bias (Bareinboim et al. 2014), and handling missing data (Mohan et al. 2013).

![](images/3df42c9289a3e8ae88513bc40e16c4be63cb7af031e00333ad0a3d85731ae414.jpg)

![](images/5e00b4421ed493dce9d2402150474908497bf311861ada73872259a13a5494fe.jpg)

![](images/cf6293b9084071d6545052996fe6c4bb3d0f7ba0dd6dcaa1e2b8f4670d4e9970.jpg)

![](images/1fb7f71ed1ff28c5397afad2000ddbd0b2cd350645442948036ce26ebc75af92.jpg)

![](images/9938dfe89558fc209fb1d0ac48fae6b15bfb62440874ace420142b2818d1f9c0.jpg)

![](images/1e745efd8194f8b57a9d223bd566d60b156bd1aa51466a55e7833ee4dcc20cc1.jpg)

# 3

# The Effects of Interventions

# 3.1 Interventions

The ultimate aim of many statistical studies is to predict the effects of interventions. When we collect data on factors associated with wild-res in the west, we are actually searching for something we can intervene upon in order to decrease wild-re frequency. When we perform a study on a new cancer drug, we are trying to identify how a patient‚Äôs illness responds when we intervene upon it by medicating the patient. When we research the correlation between violent television and acts of aggression in children, we are trying to determine whether intervening to reduce children‚Äôs access to violent television will reduce their aggressiveness.

As you have undoubtedly heard many times in statistics classes, ‚Äúcorrelation is not causation.‚Äù A mere association between two variables does not necessarily or even usually mean that one of those variables causes the other. (The famous example of this property is that an increase in ice cream sales is correlated with an increase in violent crime‚Äînot because ice cream causes crime, but because both ice cream sales and violent crime are more common in hot weather.) For this reason, the randomized controlled experiment is considered the golden standard of statistics. In a properly randomized controlled experiment, all factors that inuence the outcome variable are either static, or vary at random, except for one‚Äîso any change in the outcome variable must be due to that one input variable.

Unfortunately, many questions do not lend themselves to randomized controlled experiments. We cannot control the weather, so we can‚Äôt randomize the variables that affect wild-res. We could conceivably randomize the participants in a study about violent television, but it would be dif-cult to effectively control how much television each child watches, and nearly impossible to know whether we were controlling them effectively or not. Even randomized drug trials can run into problems when participants drop out, fail to take their medication, or misreport their usage.

In cases where randomized controlled experiments are not practical, researchers instead perform observational studies, in which they merely record data, rather than controlling it. The problem of such studies is that it is dif-cult to untangle the causal from the merely correlative. Our common sense tells us that intervening on ice cream sales is unlikely to have any effect on crime, but the facts are not always so clear. Consider, for instance, a recent

University of Winnipeg study that showed that heavy text messaging in teens was correlated with ‚Äúshallowness.‚Äù Media outlets jumped on this as proof that texting makes teenagers more shallow. (Or, to use the language of intervention, that intervening to make teens text less would make them less shallow.) The study, however, proved nothing of the sort. It might be the case that shallowness makes teens more drawn to texting. It might be that both shallowness and heavy texting are caused by a common factor‚Äîa gene, perhaps‚Äîand that intervening on that variable, if possible, would decrease both.

The difference between intervening on a variable and conditioning on that variable should, hopefully, be obvious. When we intervene on a variable in a model, we -x its value. We change the system, and the values of other variables often change as a result. When we condition on a variable, we change nothing; we merely narrow our focus to the subset of cases in which the variable takes the value we are interested in. What changes, then, is our perception about the world, not the world itself.

![](images/9c413f65f6cab9a205bd1c9e79d7bd4f7326f5891a691ffbaff987d58482897c.jpg)  
Figure 3.1 A graphical model representing the relationship between temperature (Z), ice cream sales $( X )$ , and crime rates (Y)

Consider, for instance, Figure 3.1 that shows a graphical model of our ice cream sales example, with $X$ as ice cream sales, Y as crime rates, and $Z$ as temperature. When we intervene to -x the value of a variable, we curtail the natural tendency of that variable to vary in response to other variables in nature. This amounts to performing a kind of surgery on the graphical model, removing all edges directed into that variable. If we were to intervene to make ice cream sales low (say, by shutting down all ice cream shops), we would have the graphical model shown in Figure 3.2. When we examine correlations in this new graph, we -nd that crime rates are, of course, totally independent of (i.e., uncorrelated with) ice cream sales since the latter is no longer associated with temperature (Z). In other words, even if we vary the level at which we hold $X$ constant, that variation will not be transmitted to variable $Y$ (crime rates). We see that intervening on a variable results in a totally different pattern of dependencies than conditioning on a variable. Moreover, the latter can be obtained

![](images/0064ccfc9c86c75cccf5676b6a4b90edf206a9faf51dad5985a688ffd4877121.jpg)  
Figure 3.2 A graphical model representing an intervention on the model in Figure 3.1 that lowers ice cream sales

directly from the data set, using the procedures described in Part One, while the former varies depending on the structure of the causal graph. It is the graph that instructs us which arrow should be removed for any given intervention.

In notation, we distinguish between cases where a variable $X$ takes a value $x$ naturally and cases where we -x $X = x$ by denoting the latter $d o ( X = x )$ . So $P ( Y = y | X = x )$ is the probability that $Y = y$ conditional on -nding $X = x$ , while $P ( Y = y | d o ( X = x ) )$ is the probability that $Y = y$ when we intervene to make $X = x$ . In the distributional terminology, $P ( Y = y | X = x )$ reects the population distribution of $Y$ among individuals whose $X$ value is $x$ . On the other hand, $P ( Y = y | d o ( X = x ) )$ represents the population distribution of $Y$ if everyone in the population had their $X$ value -xed at $x$ . We similarly write $P ( Y = y | d o ( X = x ) , Z = z )$ to denote the conditional probability of $Y = y$ , given $Z = z$ , in the distribution created by the intervention $d o ( X = x )$ .

Using $d o$ -expressions and graph surgery, we can begin to untangle the causal relationships from the correlative. In the rest of this chapter, we learn methods that can, astoundingly, tease out causal information from purely observational data, assuming of course that the graph stitutes a valid representation of re lity. It is worth noting here that we are making acon a assumption that the intervention has no ‚Äúside effects,‚Äù that is, that assigning the value tacit $x$ for the variable $X$ for an individual does not alter subsequent variables in a direct way. For example, being ‚Äúassigned‚Äù a drug might have a different effect on recovery than being forced to take the drug against one‚Äôs religious objections. When side effects are present, they need to be speci-ed explicitly in the model.

# 3.2 The Adjustment Formula

The ice cream example represents an extreme case in which the correlation between $X$ and Y was totally spurious from a causal perspective, because there was no causal path from $X$ to Y. Most real-life situations are not so clear-cut. To explore a more realistic situation, let us examine Figure 3.3, in which Y responds to both $Z$ and $X$ . Such a model could represent, for example, the -rst story we encountered for Simpson‚Äôs paradox, where $X$ stands for drug usage, Y stands for recovery, and $Z$ stands for gender. To -nd out how effective the drug is in the population, we imagine a hypothetical intervention by which we administer the drug uniformly to the entire population and compare the recovery rate to what would obtain under the complementary intervention, where we prevent everyone from using the drug. Denoting the -rst intervention by $d o ( X = 1 )$ and the second by $d o ( X = 0 )$ , our task is to estimate the difference

$$
P (Y = 1 \mid d o (X = 1)) - P (Y = 1 \mid d o (X = 0)) \tag {3.1}
$$

![](images/b2b1ba08da6a96fe864c7da3db785a3e4b349e9e52cc9fc375ce8f871c12bbaa.jpg)  
Figure 3.3 A graphical model representing the effects of a new drug, with $Z$ representing gender, $X$ standing for drug usage, and $Y$ standing for recovery

which is known as the ‚Äúcausal effect difference,‚Äù or ‚Äúaverage causal effect‚Äù (ACE). In general, however, if $X$ and Y can each take on more than one value, we would wish to predict the general causal effect $P ( Y = y | d o ( X = x ) )$ , where $x$ and $y$ are any two values that $X$ and $Y$ can take on. For example, $x$ may be the dosage of the drug and $y$ the patient‚Äôs blood pressure.

We know from -rst principles that causal effects cannot be estimated from the data set itself without a causal story. That was the lesson of Simpson‚Äôs paradox: The data itself was not suf-cient even for determining whether the effect of the drug was positive or negative. But with the aid of the graph in Figure 3.3, we can compute the magnitude of the causal effect from the data. To do so, we simulate the intervention in the form of a graph surgery (Figure 3.4) just as we did in the ice cream example. The causal effect $P ( Y = y | d o ( X = x ) )$ is equal to the conditional probability $P _ { m } ( Y = y | X = x )$ that prevails in the manipulated model of Figure 3.4. (This, of course, also resolves the question of whether the correct answer lies in the aggregated or the $Z$ -speci-c table‚Äîwhen we determine the answer through an intervention, there‚Äôs only one table to contend with.)

![](images/f31daf838917288d28236453d3b6b40dca4eda138673d85b147fcd7a82e8346b.jpg)  
Figure 3.4 A modi-ed graphical model representing an intervention on the model in Figure 3.3 that sets drug usage in the population, and results in the manipulated probability $P _ { m }$

The key to computing the causal effect lies in the observation that $P _ { m }$ , the manipulated probability, shares two essential properties with $P$ (the original probability function that prevails in the preintervention model of Figure 3.3). First, the marginal probability $P ( Z = z )$ i s invariant under the intervention, because the process determining $Z$ is not affected by removing the arrow from $Z$ to $X$ . In our example, this means that the proportions of males and females remain the same, before and after the intervention. Second, the conditional probability $P ( Y = y \vert Z = z , X = x )$ is invariant, because the process by which $Y$ responds to $X$ and Z, $Y = f ( x , z , u _ { Y } )$ , remains the same, regardless of whether $X$ changes spontaneously or by deliberate manipulation. We can therefore write two equations of invariance:

$$
P _ {m} (Y = y | Z = z, X = x) = P (Y = y | Z = z, X = x) \quad \text {a n d} \quad P _ {m} (Z = z) = P (Z = z)
$$

We can also use the fact that $Z$ and $X$ are $d$ -separated in the modi-ed model and are, therefore, independent under the intervention distribution. This tells us that $P _ { m } ( Z = z | X = x ) =$ $P _ { m } ( Z = z ) = P ( Z = z )$ , the last equality following from above. Putting these considerations together, we have

$$
\begin{array}{l} P (Y = y \mid d o (X = x)) \\ = P _ {m} (Y = y \mid X = x) \quad (\text {b y}) \tag {3.2} \\ \end{array}
$$

$$
\begin{array}{l} = \sum_ {z} P _ {m} (Y = y | X = x, Z = z) P _ {m} (Z = z | X = x) (3.3) \\ = \sum_ {z} P _ {m} (Y = y | X = x, Z = z) P _ {m} (Z = z) (3.4) \\ \end{array}
$$

Equation (3.3) is obtained using the Law of Total Probability by conditioning on and summing over all values of $Z = z$ (as in Eq. (1.9)) while Eq. (3.4) makes use of the independence of $Z$ and $X$ in the modi-ed model.

Finally, using the invariance relations, we obtain a formula for the causal effect, in terms of preintervention probabilities:

$$
P (Y = y \mid d o (X = x)) = \sum_ {z} P (Y = y \mid X = x, Z = z) P (Z = z) \tag {3.5}
$$

Equation (3.5) is called the adjustment formula, and as you can see, it computes the association between $X$ and $Y$ for each value $z$ of $Z$ , then averages over those values. This procedure is referred to as ‚Äúadjusting for $Z ^ { \ ' }$ or ‚Äúcontrolling for Z.‚Äù

This -nal expression‚Äîthe right-hand side of Eq. (3.5)‚Äîcan be estimated directly from the data, since it consists only of conditional probabilities, each of which can be computed by the -ltering procedure described in Chapter 1. Note also that no adjustment is needed in a randomized controlled experiment since, in such a setting, the data are generated by a model which already possesses the structure of Figure 3.4, hence, $P _ { m } = P$ regardless of any factors $Z$ that affect Y. Our derivation of the adjustment formula (3.5) constitutes therefore a formal proof that randomization gives us the quantity we seek to estimate, namely $P ( Y = y | d o ( X = x ) )$ . In practice, investigators use adjustments in randomized experiments as well, for the purpose of minimizing sampling variations (Cox 1958).

To demonstrate the working of the adjustment formula, let us apply it numerically to Simpson‚Äôs story, with $X = 1$ standing for the patient taking the drug, $Z = 1$ standing for the patient being male, and $Y = 1$ standing for the patient recovering. We have

$$
P (Y = 1 \mid d o (X = 1)) = P (Y = 1 \mid X = 1, Z = 1) P (Z = 1) + P (Y = 1 \mid X = 1, Z = 0) P (Z = 0)
$$

Substituting the -gures given in Table 1.1 we obtain

$$
P (Y = 1 | d o (X = 1)) = \frac {0 . 9 3 (8 7 + 2 7 0)}{7 0 0} + \frac {0 . 7 3 (2 6 3 + 8 0)}{7 0 0} = 0. 8 3 2
$$

while, similarly,

$$
P (Y = 1 \mid d o (X = 0)) = \frac {0 . 8 7 (8 7 + 2 7 0)}{7 0 0} + \frac {0 . 6 9 (2 6 3 + 8 0)}{7 0 0} = 0. 7 8 1 8
$$

Thus, comparing the effect of drug-taking $( X = 1$ ) to the effect of nontaking $( X = 0$ ), we obtain

$$
A C E = P (Y = 1 \mid d o (X = 1)) - P (Y = 1 \mid d o (X = 0)) = 0. 8 3 2 - 0. 7 8 1 8 = 0. 0 5 0 2
$$

giving a clear positive advantage to drug-taking. A more informal interpretation of ACE here is that it is simply the difference in the fraction of the population that would recover if everyone took the drug compared to when no one takes the drug.

We see that the adjustment formula instructs us to condition on gender, -nd the bene-t of the drug separately for males and females, and only then average the result using the percentage of males and females in the population. It also thus instructs us to ignore the aggregated

population data $P ( Y = 1 | X = 1 )$ ) and $P ( Y = 1 | X = 0 )$ ), from which we might (falsely) conclude that the drug has a negative effect overall.

These simple examples might give readers the impression that whenever we face the dilemma of whether to condition on a third variable Z, the adjustment formula prefers the Z-speci-c analysis over the nonspeci-c analysis. But we know this is not so, recalling the blood pressure example of Simpson‚Äôs paradox given in Table 1.2. There we argued that the more sensible method would be not to condition on blood pressure, but to examine the unconditional population table directly. How would the adjustment formula cope with situations like that?

![](images/78ddf2f9d50fec089ffe38f59f0aab469bee9d7a1b73fc3973224d63d8c6708e.jpg)  
Figure 3.5 A graphical model representing the effects of a new drug, with $X$ representing drug usage, Y representing recovery, and Z representing blood pressure (measured at the end of the study). Exogenous variables are not shown in the graph, implying that they are mutually independent

The graph in Figure 3.5 represents the causal story in the blood pressure example. It is the same as Figure 3.4, but with the arrow between $X$ and $Z$ reversed, reecting the fact that the treatment has an effect on blood pressure and not the other way around. Let us try now to evaluate the causal effect $P ( Y = 1 | d o ( X = 1 ) )$ associated with this model as we did with the gender example. First, we simulate an intervention and then examine the adjustment formula that emanates from the simulated intervention. In graphical models, an intervention is simulated by severing all arrows that enter the manipulated variable $X$ . In our case, however, the graph of Figure 3.5 shows no arrow entering $X$ , since $X$ has no parents. This means that no surgery is required; the conditions under which data were obtained were such that treatment was assigned ‚Äúas if randomized.‚Äù If there was a factor that would make subjects prefer or reject treatment, such a factor should show up in the model; the absence of such a factor gives us the license to treat $X$ as a randomized treatment.

Under such conditions, the intervention graph is equal to the original graph‚Äîno arrow need be removed‚Äîand the adjustment formula reduces to

$$
P (Y = y | d o (X = x)) = P (Y = y | X = x),
$$

which can be obtained from our adjustment formula by letting the empty set be the element adjusted for. Obviously, if we were to adjust for blood pressure, we would obtain an incorrect assessment‚Äîone corresponding to a model in which blood pressure causes people to seek treatment.

# 3.2.1 To Adjust or not to Adjust?

We are now in a position to understand what variable, or set of variables, Z can legitimately be included in the adjustment formula. The intervention procedure, which led to the adjustment formula, dictates that $Z$ should coincide with the parents of $X$ , because it is the inuence of

these parents that we neutralize when we -x $X$ by external manipulation. Denoting the parents of $X$ by $P A ( X )$ , we can therefore write a general adjustment formula and summarize it in a rule:

Rule 1 (The Causal Effect Rule) Given a graph G in which a set of variables PA are designated as the parents of $X$ , the causal effect of X on Y is given by

$$
P (Y = y \mid d o (X = x)) = \sum_ {z} P (Y = y \mid X = x, P A = z) P (P A = z) \tag {3.6}
$$

where z ranges over all the combinations of values that the variables in PA can take.

If we multiply and divide the summand in (3.6) by the probability $P ( X = x | P A = z )$ , we get a more convenient form:

$$
P (y \mid d o (x)) = \sum_ {z} \frac {P (X = x , Y = y , P A = z)}{P (X = x \mid P A = z)} \tag {3.7}
$$

which explicitly displays the role played by the parents of $X$ in predicting the results of interventions. The factor $P ( X = x | P A = z )$ is known as the ‚Äúpropensity score‚Äù and the advantages of expressing $P ( y | d o ( x ) )$ in this form will be discussed in Section 3.5.

We can appreciate now what role the causal graph plays in resolving Simpson‚Äôs paradox, and, more generally, what aspects of the graph allow us to predict causal effects from purely statistical data. We need the graph in order to determine the identity of X‚Äôs parents‚Äîthe set of factors that, under nonexperimental conditions, would be suf-cient for determining the value of $X$ , or the probability of that value.

This result alone is astounding; using graphs and their underlying assumptions, we were able to identify causal relationships in purely observational data. But, from this discussion, readers may be tempted to conclude that the role of graphs is fairly limited; once we identify the parents of $X$ , the rest of the graph can be discarded, and the causal effect can be evaluated mechanically from the adjustment formula. The next section shows that things may not be so simple. In most practical cases, the set of X‚Äôs parents will contain unobserved variables that would prevent us from calculating the conditional probabilities in the adjustment formula. Luckily, as we will see in future sections, we can adjust for other variables in the model to substitute for the unmeasured elements of $P A ( X )$ .

# Study questions

# Study questions 3.2.1

Referring to Study question 1.5.2 (Figure 1.10) and the parameters listed therein,

(a) Compute P(y do(x)) for all values of x and y, by simulating the intervention do(x) on the model.   
(b) Compute P(y|do(x)) for all values of x and y, using the adjustment formula (3.5)   
(c) Compute the ACE

$$
A C E = P \left(y _ {1} \mid d o \left(x _ {1}\right)\right) - P \left(y _ {1} \mid d o \left(x _ {0}\right)\right)
$$

and compare it to the Risk Difference

$$
R D = P (y _ {1} | x _ {1}) - P (y _ {1} | x _ {0})
$$

What is the difference between ACE and the RD? What values of the parameters would minimize the difference?

(d) Find a combination of parameters that exhibit Simpson‚Äôs reversal (as in Study question 1.5.2(c)) and show explicitly that the overall causal effect of the drug is obtained from the desegregated data.

# 3.2.2 Multiple Interventions and the Truncated Product Rule

In deriving the adjustment formula, we assumed an intervention on a single variable, X, whose parents were disconnected, so as to simulate the absence of their inuence after intervention. However, social and medical policies occasionally involve multiple interventions, such as those that dictate the value of several variables simultaneously, or those that control a variable over time. To represent multiple interventions, it is convenient to resort to the product decomposition that a graphical model imposes on joint distributions, as we have discussed in Section 1.5.2. According to the Rule of Product Decomposition, the preintervention distribution in the model of Figure 3.3 is given by the product

$$
P (x, y, z) = P (z) P (x \mid z) P (y \mid x, z) \tag {3.8}
$$

whereas the postintervention distribution, governed by the model of Figure 3.4 is given by the product

$$
P (z, y \mid d o (x)) = P _ {m} (z) P _ {m} (y \mid x, z) = P (z) P (y \mid x, z) \tag {3.9}
$$

with the factor $P ( x | z )$ purged from the product, since $X$ becomes parentless as it is -xed at $X = x$ . This coincides with the adjustment formula, because to evaluate $P ( y | d o ( x ) )$ we need to marginalize (or sum) over $z$ , which gives

$$
P (y | d o (x)) = \sum_ {z} P (z) P (y | x, z)
$$

in agreement with (3.5).

This consideration also allows us to generalize the adjustment formula to multiple interventions, that is, interventions that -x the values of a set of variables $X$ to constants. We simply write down the product decomposition of the preintervention distribution, and strike out all factors that correspond to variables in the intervention set $X$ . Formally, we write

$$
P (x _ {1}, x _ {2}, \ldots , x _ {n} | d o (x)) = \prod_ {i} P (x _ {i} | p a _ {i}) \qquad \mathrm {f o r a l l} i \mathrm {w i t h} X _ {i} \mathrm {n o t i n} X.
$$

This came to be known as the truncated product formula or $g$ -formula. To illustrate, assume that we intervene on the model of Figure 2.9 and set $X$ to $x$ and $Z _ { 3 }$ to $z _ { 3 }$ . The postintervention distribution of the other variables in the model will be

$$
P (z _ {1}, z _ {2}, w, y | d o (X = x, Z _ {3} = z _ {3})) = P (z _ {1}) P (z _ {2}) P (w | x) P (y | w, z _ {3}, z _ {2})
$$

where we have deleted the factors $P ( x | z _ { 1 } , z _ { 3 } )$ and $P ( z _ { 3 } | z _ { 1 } , z _ { 2 } )$ from the product.

It is interesting to note that combining (3.8) and (3.9), we get a simple relation between the pre- and postintervention distributions:

$$
P (z, y \mid d o (x)) = \frac {P (x , y , z)}{P (x \mid z)} \tag {3.10}
$$

It tells us that the conditional probability $P ( x | z )$ is all we need to know in order to predict the effect of an intervention $d o ( x )$ from nonexperimental data governed by the distribution $P ( x , y , z )$ .

# 3.3 The Backdoor Criterion

In the previous section, we came to the conclusion that we should adjust for a variable‚Äôs parents, when trying to determine its effect on another variable. But often, we know, or believe, that the variables have unmeasured parents that, though represented in the graph, may be inaccessible for measurement. In those cases, we need to -nd an alternative set of variables to adjust for.

This dilemma unlocks a deeper statistical question: Under what conditions does a causal story permit us to compute the causal effect of one variable on another, from data obtained by passive observations, with no interventions? Since we have decided to represent causal stories with graphs, the question becomes a graph-theoretical problem: Under what conditions is the structure of the causal graph suf-cient for computing a causal effect from a given data set?

The answer to that question is long enough‚Äîand important enough‚Äîthat we will spend the rest of the chapter addressing it. But one of the most important tools we use to determine whether we can compute a causal effect is a simple test called the backdoor criterion. Using it, we can determine, for any two variables $X$ and Y in a causal model represented bya DAG, which set of variables $Z$ in that model should be conditioned on when searching for the causal relationship between $X$ and Y.

De-nition 3.3.1 (The Backdoor Criterion) Given an ordered pair of variables $( X , Y )$ in a directed acyclic graph G, a set of variables Z satis-es the backdoor criterion relative to $( X , Y )$ if no node in Z is a descendant of X, and Z blocks every path between X and $Y$ that contains an arrow into $X$ .

If a set of variables $Z$ satis-es the backdoor criterion for $X$ and Y, then the causal effect of $X$ on Y is given by the formula

$$
P (Y = y | d o (X = x)) = \sum_ {z} P (Y = y | X = x, Z = z) P (Z = z)
$$

just as when we adjust for $P A ( X )$ . (Note that $P A ( X )$ always satis-es the backdoor criterion.)

The logic behind the backdoor criterion is fairly straightforward. In general, we would like to condition on a set of nodes $Z$ such that

1. We block all spurious paths between $X$ and Y.   
2. We leave all directed paths from $X$ to $Y$ unperturbed.   
3. We create no new spurious paths.

When trying to -nd the causal effect of $X$ on $Y$ , we want the nodes we condition on to block any ‚Äúbackdoor‚Äù path in which one end has an arrow into $X$ , because such paths may make $X$ and $Y$ dependent, but are obviously not transmitting causal inuences from $X$ , and if we do not block them, they will confound the effect that $X$ has on Y. We condition on backdoor paths so as to ful-ll our -rst requirement. However, we don‚Äôt want to condition on any nodes that are descendants of $X$ . Descendants of $X$ would be affected by an intervention on $X$ and might themselves affect Y; conditioning on them would block those pathways. Therefore, we don‚Äôt condition on descendants of $X$ so as to ful-ll our second requirement. Finally, to comply with the third requirement, we should refrain from conditioning on any collider that would unblock a new path between $X$ and Y. The requirement of excluding descendants of $X$ also protects us from conditioning on children of intermediate nodes between $X$ and Y (e.g., the collision node W in Figure 2.4.) Such conditioning would distort the passage of causal association between $X$ and $Y$ , similar to the way conditioning on their parents would.

To see what this means in practice, let‚Äôs look at a concrete example, shown in Figure 3.6.

![](images/9249d4efd065ef06e777d5c60becc05ecca504e2422cdb844343818308459627.jpg)  
Figure 3.6 A graphical model representing the relationship between a new drug $( X )$ , recovery (Y), weight (W), and an unmeasured variable $Z$ (socioeconomic status)

Here we are trying to gauge the effect of a drug $( X )$ on recovery (Y). We have also measured weight (W), which has an effect on recovery. Further, we know that socioeconomic status $( Z )$ affects both weight and the choice to receive treatment‚Äîbut the study we are consulting did not record socioeconomic status.

Instead, we search for an observed variable that -ts the backdoor criterion from $X$ to Y . A brief examination of the graph shows that W, which is not a descendant of $X$ , also blocks the backdoor path $X  Z  W  Y$ . Therefore, W meets the backdoor criterion. So long as the causal story conforms to the graph in Figure 3.6, adjusting for W will give us the causal effect of $X$ on Y. Using the adjustment formula, we -nd

$$
P (Y = y | d o (X = x)) = \sum_ {w} P (Y = y | X = x, W = w) P (W = w)
$$

This sum can be estimated from our observational data, so long as W is observed.

With the help of the backdoor criterion, you can easily and algorithmically come to a conclusion about a pressing policy concern, even in complicated graphs. Consider the model in Figure 2.8, and assume again that we wish to evaluate the effect of $X$ on Y. What variables should we condition on to obtain the correct effect? The question boils down to -nding a set of variables that satisfy the backdoor criterion, but since there are no backdoor paths from $X$ to $Y$ , the answer is trivial: The empty set satis-es the criterion, hence no adjustment is needed.

The answer is

$$
P (y \mid d o (x)) = P (y \mid x)
$$

Suppose, however, that we were to adjust for W. Would we get the correct result for the effect of $X$ on Y? Since W is a collider, conditioning on W would open the path $X  W  Z $

$T \to Y$ . This path is spurious since it lies outside the causal pathway from $X$ to Y. Opening this path will create bias and yield an erroneous answer. This means that computing the association between $X$ and Y for each value of W separately will not yield the correct effect of $X$ on $Y$ , and it might even give the wrong effect for each value of $W$ .

How then do we compute the causal effect of $X$ on $Y$ for a speci-c value $w$ of W? In Figure 2.8, W may represent, for example, the level of posttreatment pain of a patient, and we might be interested in assessing the effect of $X$ on $Y$ for only those patients who did not suffer any pain. Specifying the value of W amounts to conditioning on $W = w$ , and this, as we have realized, opens a spurious path from $X$ to Y by virtue of the fact that $W$ is a collider.

The answer is that we still have the option of blocking that path using other variables. For example, if we condition on $T$ , we would block the spurious path $X \to W  Z  T \to Y$ , even if W is part of the conditioning set. Thus to compute the $w \mathrm { . }$ -speci-c causal effect, written $P ( y | d o ( x ) , w )$ , we adjust for $T$ , and obtain

$$
P (Y = y \mid d o (X = x), W = w) = \sum_ {t} P (Y = y \mid X = x, W = w, T = t) P (T = t \mid X = x, W = w) \tag {3.11}
$$

Computing such W-speci-c causal effects is an essential step in examining effect modi-- cation or moderation, that is, the degree to which the causal effect of $X$ on $Y$ is modi-ed by different values of W. Consider, again, the model in Figure 3.6, and suppose we wish to test whether the causal effect for units at level $W = w$ is the same as for units at level $W = w ^ { \prime }$ (W may represent any pretreatment variable, such as age, sex, or ethnicity). This question calls for comparing two causal effects,

$$
P (Y = y \mid d o (X = x), W = w) \quad \text {a n d} \quad P (Y = y \mid d o (X = x), W = w ^ {\prime})
$$

In the speci-c example of Figure 3.6, the answer is simple, because $W$ satis-es the backdoor criterion. So, all we need to compare are the conditional probabilities $P ( Y = y | X = x$ , $W = w$ ) and $P ( Y = y | X = x , W = w ^ { \prime } )$ $W = w ^ { \prime }$ ; no summation is required. In the more general case, where W alone does not satisfy the backdoor criterion, yet a larger set, $T \cup W$ , does, we need to adjust for members of $T$ , which yields Eq. (3.11). We will return to this topic in Section 3.5.

From the examples seen thus far, readers may get the impression that one should refrain from adjusting for colliders. Such adjustment is sometimes unavoidable, as seen in Figure 3.7. Here, there are four backdoor paths from $X$ to $Y$ , all traversing variable Z, which is a collider on the path $X \left. E \right. Z \left. A \right. Y .$ Conditioning on $Z$ will unblock this path and will violate the backdoor criterion. To block all backdoor paths, we need to condition on one of the following sets: $\{ E , Z \}$ , $\{ A , Z \}$ , or $\{ E , Z , A \}$ . Each of these contains Z. We see, therefore, that Z, a collider, must be adjusted for in any set that yields an unbiased estimate of the effect of $X$ on $Y$ .

![](images/488c5a95be52577bdd5c6b2f5644cd7f58219cc227744b5947b3d4c71ef93239.jpg)  
Figure 3.7 A graphical model in which the backdoor criterion requires that we condition on a collider $( Z )$ in order to ascertain the effect of $X$ on $Y$

The backdoor criterion has some further possible bene-ts. Consider the fact that $P ( Y = y | d o ( X = x ) )$ is an empirical fact of nature, not a byproduct of our analysis. That means that any suitable variable or set of variables that we adjust on‚Äîwhether it be $P A ( X )$ or any other set that conforms to the backdoor criterion‚Äîmust return the same result for $P ( Y = y | d o ( X = x ) )$ . In the case we looked at in Figure 3.6, this means that

$$
\sum_ {w} P (Y = y | X = x, W = w) P (W = w) = \sum_ {z} P (Y = y | X = x, Z = z) P (Z = z)
$$

This equality is useful in two ways. First, in the cases where we have multiple observed sets of variables suitable for adjustment (e.g., in Figure 3.6, if both W and $Z$ had been observed), it provides us with a choice of which variables to adjust for. This could be useful for any number of practical reasons‚Äîperhaps one set of variables is more expensive to measure than the other, or more prone to human error, or simply has more variables and is therefore more dif-cult to calculate.

Second, the equality constitutes a testable constraint on the data when all the adjustment variables are observed, much like the rules of $d$ -separation. If we are attempting to -t a model that leads to such an equality on a data set that violates it, we can discard that model.

# Study questions

# Study question 3.3.1

Consider the graph in Figure 3.8:

![](images/20b7ec14b83e275a46097cafcc59377455eb19b846abde561d474e28bd53128a.jpg)  
Figure 3.8 Causal graph used to illustrate the backdoor criterion in the following study questions

(a) List all of the sets of variables that satisfy the backdoor criterion to determine the causal effect of X on Y.   
(b) List all of the minimal sets of variables that satisfy the backdoor criterion to determine the causal effect of X on Y (i.e., any set of variables such that, if you removed any one of the variables from the set, it would no longer meet the criterion).   
(c) List all minimal sets of variables that need be measured in order to identify the effect of D on Y. Repeat, for the effect of $\{ W , D \}$ on Y.

# Study question 3.3.2 (Lord‚Äôs paradox)

At the beginning of the year, a boarding school offers its students a choice between two meal plans for the year: Plan A and Plan B. The students‚Äô weights are recorded at the beginning and the end of the year. To determine how each plan affects students‚Äô weight gain, the school hired two statisticians who, oddly, reached different conclusions. The -rst statistician calculated the difference between each student‚Äôs weight in June $( W _ { F } ,$ ) and in September $^ { \prime } W _ { I } )$ and found that the average weight gain in each plan was zero.

The second statistician divided the students into several subgroups, one for each initial weight, WI. He -nds that for each initial weight, the -nal weight for Plan B is higher than the -nal weight for Plan A.

So, the -rst statistician concluded that there was no effect of diet on weight gain and the second concluded there was.

Figure 3.9 illustrates data sets that can cause the two statisticians to reach conicting conclusions. Statistician-1 examined the weight gain $W _ { F } - W _ { I } ,$ which, for each student, is represented by the shortest distance to the $4 5 ^ { \circ }$ line. Indeed, the average gain for each diet plan is zero; the two groups are each situated symmetrically relative to the zero-gain line, $W _ { F } = W _ { I }$ . Statistician-2, on the other hand, compared the -nal weights of plan A students to those of plan B students who entered school with the same initial weight $W _ { 0 }$ and, as the vertical line in the -gure indicates, plan B students are situated above plan A students along this vertical line. The same will be the case for any other vertical line, regardless of $W _ { 0 }$ .

(a) Draw a causal graph representing the situation.   
(b) Determine which statistician is correct.   
(c) How is this example related to Simpson‚Äôs paradox?

![](images/0235b956621ce5a0fcffe9098a19b3fec5be9366cd2878933410421269688923.jpg)  
Figure 3.9 Scatter plot with students‚Äô initial weights on the $x$ -axis and -nal weights on the $y$ -axis. The vertical line indicates students whose initial weights are the same, and whose -nal weights are higher (on average) for plan B compared with plan A

# Study questions 3.3.3

Revisit the lollipop story of Study question 1.2.4 and answer the following questions:

(a) Draw a graph that captures the story.   
(b) Determine which variables must be adjusted for by applying the backdoor criterion.

(c) Write the adjustment formula for the effect of the drug on recovery.   
(d) Repeat questions (a)‚Äì(c) assuming that the nurse gave lollipops a day after the study, still preferring patients who received treatment over those who received placebo.

# 3.4 The Front-Door Criterion

The backdoor criterion provides us with a simple method of identifying sets of covariates that should be adjusted for when we seek to estimate causal effects from nonexperimental data. It does not, however, exhaust all ways of estimating such effects. The do-operator can be applied to graphical patterns that do not satisfy the backdoor criterion to identify effects that on -rst sight seem to be beyond one‚Äôs reach. One such pattern, called front-door, is discussed in this section.

Consider the century-old debate on the relation between smoking and lung cancer. In the years preceding 1970, t he tobacco industry managed to prevent antismoking legislation by promoting the theory that the observed correlation between smoking and lung cancer could be explained by some sort of carcinogenic genotype that also induces an inborn craving for nicotine.

![](images/ebb2bea8b5c5fb42850d4347b985840dac6439e6870909322c22c731c6e61f95.jpg)  
(a)

![](images/b45f2de7244221a6b1e08b71795657e06e8a8083a17f576242a7edca840139ce.jpg)  
(b)   
Figure 3.10 A graphical model representing the relationships between smoking $( X )$ and lung cancer $( Y )$ , with unobserved confounder $( U )$ and a mediating variable $Z$

A graph depicting this example is shown in Figure 3.10(a) This graph does not satisfy the backdoor condition because the variable $U$ is unobserved and hence cannot be used to block the backdoor path from $X$ to Y. The causal effect of smoking on lung cancer is not identi-able in this model; one can never ascertain which portion of the observed correlation between $X$ and Y is spurious, attributable to their common effect, $U$ , and what portion is genuinely causative. (We note, however, that even in these circumstances, much compelling work has been done to quantify how strong the (unobserved) associates between both $U$ and $X$ , and $U$ and Y, must be in order to entirely explain the observed association between $X$ and Y.)

However, we can go much further by considering the model in Figure 3.10(b), where an additional measurement is available: the amount of tar deposits in patients  lungs. This model does not satisfy the backdoor criterion, because there is still no variable capable of blocking the spurious path $X \left. U \right. Y$ . We see, however, that the causal effect $P ( Y = y | d o ( X = x ) )$ i s nevertheless identi-able in this model, through two consecutive applications of the backdoor criterion.

How can the intermediate variable $Z$ help us to assess the effect of $X$ on Y? The answer is not at all trivial: as the following quantitative example shows, it may lead to heated debate.

Assume that a careful study was undertaken, in which the following factors were measured simultaneously on a randomly selected sample of 800,000 subjects considered to be at very high risk of cancer (because of environmental exposures such as smoking, asbestos, radon , and the like).

1. Whether the subject smoked   
2. Amount of tar in the subject‚Äôs lungs   
3. Whether lung cancer has been detected in the patient.

The data from this study are presented in Table 3.1, where, for simplicity, all three variables are assumed to be binary. All numbers are given in thousands.

Table 3.1 A hypothetical data set of randomly selected samples showing the percentage of cancer cases for smokers and nonsmokers in each tar category (numbers in thousands)   

<table><tr><td rowspan="2"></td><td colspan="2">Tar 400</td><td colspan="2">No tar 400</td><td colspan="2">All subjects 800</td></tr><tr><td>Smokers</td><td>Nonsmokers</td><td>Smokers</td><td>Nonsmokers</td><td>Smokers</td><td>Nonsmokers</td></tr><tr><td rowspan="3">No cancer</td><td>380</td><td>20</td><td>20</td><td>380</td><td>400</td><td>400</td></tr><tr><td>323</td><td>1</td><td>18</td><td>38</td><td>341</td><td>39</td></tr><tr><td>(85%)</td><td>(5%)</td><td>(90%)</td><td>(10%)</td><td>(85%)</td><td>(9.75%)</td></tr><tr><td rowspan="2">Cancer</td><td>57</td><td>19</td><td>2</td><td>342</td><td>59</td><td>361</td></tr><tr><td>(15%)</td><td>(95%)</td><td>(10%)</td><td>(90%)</td><td>(15%)</td><td>(90.25%)</td></tr></table>

Two opposing interpretations can be offered for these data. The tobacco industry argues that the table proves the bene-cial effect of smoking. They point to the fact that only $15 \%$ of the smokers have developed lung cancer, compared to $9 0 . 2 5 \%$ of the nonsmokers. Moreover, within each of two subgroups, tar and no tar, smokers show a much lower percentage of cancer than nonsmokers. (These numbers are obviously contrary to empirical observations but well illustrate our point that observations are not to be trusted.)

However, the antismoking lobbyists argue that the table tells an entirely different story‚Äîthat smoking would actually increase, not decrease, one‚Äôs risk of lung cancer. Their argument goes as follows: If you choose to smoke, then your chances of building up tar deposits are $9 5 \%$ , compared to $5 \%$ if you choose not to smoke (380/400 vs 20/400). To evaluate the effect of tar deposits, we look separately at two groups, smokers and nonsmokers, as done in Table 3.2. All numbers are given in thousands.

Table 3.2 Reorganization of the data set of Table 3.1 showing the percentage of cancer cases in each smoking-tar category (numbers in thousands)   

<table><tr><td rowspan="2"></td><td colspan="2">Smokers
400</td><td colspan="2">Nonsmokers
400</td><td colspan="2">All subjects
800</td></tr><tr><td>Tar</td><td>No tar</td><td>Tar</td><td>No tar</td><td>Tar</td><td>No tar</td></tr><tr><td rowspan="3">No cancer</td><td>380</td><td>20</td><td>20</td><td>380</td><td>400</td><td>400</td></tr><tr><td>323</td><td>18</td><td>1</td><td>38</td><td>324</td><td>56</td></tr><tr><td>(85%)</td><td>(90%)</td><td>(5%)</td><td>(10%)</td><td>(81%)</td><td>(19%)</td></tr><tr><td rowspan="2">Cancer</td><td>57</td><td>2</td><td>19</td><td>342</td><td>76</td><td>344</td></tr><tr><td>(15%)</td><td>(10%)</td><td>(95%)</td><td>(90%)</td><td>(19%)</td><td>(81%)</td></tr></table>

It appears that tar deposits have a harmful effect in both groups; in smokers it increases cancer rates from $10 \%$ to $15 \%$ , and in nonsmokers it increases cancer rates from $90 \%$ to $9 5 \%$ . Thus, regardless of whether I have a natural craving for nicotine, I should avoid the harmful effect of tar deposits, and no-smoking offers very effective means of avoiding them.

The graph of Figure 3.10(b) enables us to decide between these two groups of statisticians. First, we note that the effect of $X$ on $Z$ is identi-able, since there is no backdoor path from $X$ to Z. Thus, we can immediately write

$$
P (Z = z \mid d o (X = x)) = P (Z = z \mid X = x) \tag {3.12}
$$

Next we note that the effect of $Z$ on Y is also identi-able, since the backdoor path from $Z$ to Y, namely $Z  X  U  Y$ , can be blocked by conditioning on $X$ . Thus, we can write

$$
P (Y = y \mid d o (Z = z)) = \sum_ {x} P (Y = y \mid Z = z, X = x) P (X = x) \tag {3.13}
$$

Both (3.12) and (3.13) are obtained through the adjustment formula, the -rst by conditioning on the null set, and the second by adjusting for $X$ .

We are now going to chain together the two partial effects to obtain the overall effect of $X$ on Y. The reasoning goes as follows: If nature chooses to assign $Z$ the value $z$ , then the probability of Y would be $P ( Y = y | d o ( Z = z ) )$ . But the probability that nature would choose to do that, given that we choose to set $X$ at $x _ { \ast }$ , is $P ( Z = z | d o ( X = x ) )$ . Therefore, summing over all states $z$ of $Z$ , we have

$$
P (Y = y \mid d o (X = x)) = \sum_ {z} P (Y = y \mid d o (Z = z)) P (Z = z \mid d o (X = x)) \tag {3.14}
$$

The terms on the right-hand side of (3.14) were evaluated in (3.12) and (3.13), and we can substitute them to obtain a $d o$ -free expression for $P ( Y = y | d o ( X = x ) )$ . We also distinguish between the $x$ that appears in (3.12) and the one that appears in (3.13), the latter of which is merely an index of summation and might as well be denoted $x ^ { \prime }$ . The -nal expression we have is

$$
P (Y = y | d o (X = x)) =
$$

$$
\sum_ {z} \sum_ {x ^ {\prime}} P (Y = y \mid Z = z, X = x ^ {\prime}) P (X = x ^ {\prime}) P (Z = z \mid X = x) \tag {3.15}
$$

Equation (3.15) is known as the front-door formula.

Applying this formula to the data in Table 3.1, we see that the tobacco industry was wrong; tar deposits have a harmful effect in that they make lung cancer more likely and smoking, by increasing tar deposits, increases the chances of causing this harm.

The data in Table 3.1 are obviously unrealistic and were deliberately crafted so as to surprise readers with counterintuitive conclusions that may emerge from naive analysis of observational data. In reality, we would expect observational studies to show positive correlation between smoking and lung cancer. The estimand of (3.15) could then be used for con-rming and quantifying the harmful effect of smoking on cancer.

The preceding analysis can be generalized to structures where multiple paths lead from $X$ to Y .

De-nition 3.4.1 (Front-Door) A set of variables Z is said to satisfy the front-door criterion relative to an ordered pair of variables $( X , Y )$ i f

1. Z intercepts all directed paths from X to Y.   
2. There is no backdoor path from X to Z.   
3. All backdoor paths from Z to Y are blocked by X.

Theorem 3.4.1 (Front-Door Adjustment) If Z satis-es the front-door criterion relative to $( X , Y )$ and if $P ( x , z ) > 0$ , then the causal effect of X on Y is identi-able and is given by the formula

$$
P (y \mid d o (x)) = \sum_ {z} P (z \mid x) \sum_ {x ^ {\prime}} P (y \mid x ^ {\prime}, z) P \left(x ^ {\prime}\right) \tag {3.16}
$$

The conditions stated in De-nition 3.4.1 are overly conservative; some of the paths excluded by conditions (2) and (3) can actually be allowed provided they are blocked by some variables. There is a powerful symbolic machinery, called the do-calculus, that allows analysis of such intricate structures. In fact, the do-calculus uncovers all causal effects that can be identi-ed from a given graph. Unfortunately, it is beyond the scope of this book (see Tian and Pearl 2002, Shpitser and Pearl 2008, Pearl 2009, and Bareinboim and Pearl 2012 for details). But the combination of the adjustment formula, the backdoor criterion, and the front-door criterion covers numerous scenarios. It proves the enormous, even revelatory, power that causal graphs have in not merely representing, but actually discovering causal information.

# Study questions

# Study question 3.4.1

Assume that in Figure 3.8, only X, Y, and one additional variable can be measured. Which variable would allow the identi-cation of the effect of X on Y? What would that effect be?

# Study question 3.4.2

I went to a pharmacy to buy a certain drug, and I found that it was available in two different bottles: one priced at $\$ 1$ , the other at $10. I asked the druggist, ‚ÄúWhat‚Äôs the difference?‚Äù and he told me, ‚ÄúThe $10 bottle is fresh, whereas the $1 bottle one has been on the shelf for 3 years. But, you know, data shows that the percentage of recovery is much higher among those who bought the cheap stuff. Amazing isn‚Äôt it?‚Äù I asked if the aged drug was ever tested. He said, ‚ÄúYes, and this is even more amazing; $9 5 \%$ of the aged drug and only $5 \%$ of the fresh drug has lost the active ingredient, yet the percentage of recovery among those who got bad bottles, with none of the active ingredient, is still much higher than among those who got good bottles, with the active ingredient.‚Äù

Before ordering a cheap bottle, it occurred to me to have a good look at the data. The data were, for each previous customer, the type of bottle purchased (aged or fresh), the concentration of the active ingredient in the bottle (high or low), and whether the customer recovered from the illness. The data perfectly con-rmed the druggist‚Äôs story. However, after making some additional calculations, I decided to buy the expensive bottle after all; even without testing its

content, I could determine that a fresh bottle would offer the average patient a greater chance of recovery.

Based on two very reasonable assumptions, the data show clearly that the fresh drug is more effective. The assumptions are as follows:

(i) Customers had no information about the chemical content (high or low) of the speci-c bottle of the drug that they were buying; their choices were inuenced by price and shelf-age alone.   
(ii) The effect of the drug on any given individual depends only on its chemical content, not on its shelf age (fresh or aged).

(a) Determine the relevant variables for the problem, and describe this scenario in a causal graph.   
(b) Construct a data set compatible with the story and the decision to buy the expensive bottle.   
(c) Determine the effect of choosing the fresh versus the aged drug by using assumptions (i) and (ii), and the data given in (b).

# 3.5 Conditional Interventions and Covariate-Speci-c Effects

The interventions considered thus far have been limited to actions that merely force a variable or a group of variables $X$ to take on some speci-ed value $x .$ . In general, interventions may involve dynamic policies in which a variable $X$ is made to respond in a speci-ed way to some set $Z$ of other variables‚Äîsay, through a functional relationship $x = g ( z )$ or through a stochastic relationship, whereby $X$ is set to $x$ with probability $P ^ { * } ( x | z )$ . For example, suppose a doctor decides to administer a drug only to patients whose temperature $Z$ exceeds a certain level, $Z = z$ . In this case, the action will be conditional upon the value of $Z$ and can be written $d o ( X = g ( Z ) )$ , where $g ( Z )$ is equal to one when $Z > z$ and zero otherwise (where $X = 0$ represents no drug). Since $Z$ is a random variable, the value of $X$ chosen by the action will similarly be a random variable, tracking variations in $Z$ . The result of implementing such a policy is a probability distribution written $P ( Y = y | d o ( X = g ( Z ) ) )$ , which depends only on the function $g$ and the set $Z$ of variables that drive $X$ .

In order to estimate the effect of such a policy, let us take a closer look at another concept, the ‚Äú $z$ -speci-c effect‚Äù of $X$ , which we encountered briey in Section 3.3 (Eq. (3.11)). This effect, written $P ( Y = y | d o ( X = x ) , Z = z )$ , measures the distribution of Y in a subset of the population for which $Z$ achieves the value $z$ after the intervention. For example, we may be interested in how a treatment affects a speci-c age group, $Z = z$ , or people with a speci-c feature, $Z = z$ , which may be measured after the treatment.

The $z$ -speci-c effect can be identi-ed by a procedure similar to the backdoor adjustment. The reasoning goes as follows: When we aim to estimate $P ( Y = y | d o ( X = x ) )$ , an adjustment for a set $S$ is justi-ed if $S$ blocks all backdoor paths from $X$ to Y. Now that we wish to identify $P ( Y = y | d o ( X = x ) , Z = z )$ , we need to ensure that those paths remain blocked when we add one more variable, Z, to the conditioning set. This yields a simple criterion for the identi-cation of the $z$ -speci-c effect:

Rule 2 The z-speci-c effect $P ( Y = y | d o ( X = x ) , Z = z )$ is identi-ed whenever we can measure a set S of variables such that $S \cup Z$ satis-es the backdoor criterion. Moreover, the z-speci-c

effect is given by the following adjustment formula

$$
\begin{array}{l} P (Y = y \mid d o (X = x), Z = z) \\ = \sum_ {s} P (Y = y | X = x, S = s, Z = z) P (S = s | Z = z) \\ \end{array}
$$

This modi-ed adjustment formula is similar to Eq. (3.5) with two exceptions. First, the adjustment set is $S \cup Z$ , not just $S$ and, second, the summation goes only over S, not including Z. The ‚à™ symbol in the expression $S \cup Z$ stands for set addition (or union), which means that, if $Z$ is a subset of S, we have $S \cup Z = S$ , and $S$ alone need satisfy the backdoor criterion.

Note that the identi-ability criterion for z-speci-c effects is somewhat stricter than that for nonspeci-c effect. Adding $Z$ to the conditioning set might create dependencies that would prevent the blocking of all backdoor paths. A simple example occurs when $Z$ is a collider; conditioning on $Z$ will create a new dependency between $Z$ ‚Äôs parents and may thus violate the backdoor requirement.

We are now ready to tackle our original task of estimating conditional interventions. Suppose a policy maker contemplates an age-dependent policy whereby an amount $x$ of drug is to be administered to patients, depending on their age Z. We write it as $d o ( X = g ( Z ) )$ . To -nd out the distribution of outcome Y that results from this policy, we seek to estimate $P ( Y = y | d o ( X = g ( Z ) ) )$ .

We now show that identifying the effect of such policies is equivalent to identifying the expression for the $z$ -speci-c effect $P ( Y = y | d o ( X = x ) , Z = z )$ .

To compute $P ( Y = y | d o ( X = g ( Z ) ) )$ , we condition on $Z = z$ and write

$$
\begin{array}{l} P (Y = y \mid d o (X = g (Z))) \\ = \sum_ {z} P (Y = y | d o (X = g (Z)), Z = z) P (Z = z | d o (X = g (Z))) \\ = \sum_ {z} P (Y = y \mid d o (X = g (z)), Z = z) P (Z = z) \tag {3.17} \\ \end{array}
$$

The equality

$$
P (Z = z \mid d o (X = g (Z))) = P (Z = z)
$$

stems, of course, from the fact that $Z$ occurs before $X$ ; hence, any control exerted on $X$ can have no effect on the distribution of $Z$ . Equation (3.17) can also be written as

$$
\sum_ {z} P (Y = y | d o (X = x), Z = z) \vert_ {x = g (z)} P (Z = z)
$$

which tells us that the causal effect of a conditional policy $d o ( X = g ( Z ) )$ can be evaluated directly from the expression of $P ( Y = y | d o ( X = x ) , Z = z )$ simply by substituting $g ( z )$ for $x$ and taking the expectation over $Z$ (using the observed distribution $P ( Z = z ) ,$ ).

# Study question 3.5.1

Consider the causal model of Figure 3.8.

(a) Find an expression for the c-speci-c effect of X on Y.

(b) Identify a set of four variables that need to be measured in order to estimate the z-speci-c effect of X on Y, and -nd an expression for the size of that effect.   
(c) Using your answer to part (b), determine the expected value of Y under a Z-dependent strategy, where X is set to $O$ when Z is smaller or equal to 2 and X is set to 1 when Z is larger than 2. (Assume Z takes on integer values from 1 to 5.)

# 3.6 Inverse Probability Weighing

By now, the astute reader may have noticed a problem with our intervention procedures. The backdoor and front-door criteria tell us whether it is possible to predict the results of hypothetical interventions from data obtained in an observational study. Moreover, they tell us that we can make this prediction without simulating the intervention and without even thinking about it. All we need to do is identify a set Z of covariates satisfying one of the criteria, plug this set into the adjustment formula, and we‚Äôre done: the resulting expression is guaranteed to provide a valid prediction of how the intervention will affect the outcome.

This is lovely in theory, but in practice, adjusting for $Z$ may prove problematic. It entails looking at each value or combination of values of $Z$ separately, estimating the conditional probability of $Y$ given $X$ in that stratum and then averaging the results. As the number of strata increases, adjusting for Z will encounter both computational and estimational dif-culties. Since the set $Z$ can be comprised of dozens of variables, each spanning dozens of discrete values, the summation required by the adjustment formula may be formidable, and the number of data samples falling within each $Z = z$ cell may be too small to provide reliable estimates of the conditional probabilities involved.

All of our work in this chapter has not been for naught, however. The adjustment procedure is straightforward, and, therefore, easy to use in the explanation of intervention criteria. But there is another, more subtle procedure that overcomes the practical dif-culties of adjustment.

In this section, we discuss one way of circumventing this problem, provided only that we can obtain a reliable estimate of the function $g ( x , z ) = P ( X = x | Z = z )$ , often called the ‚Äúpropensity score,‚Äù for each $x$ and $z$ . Such an estimate can be obtained by -tting the parameters of a exible function $g ( x , z )$ to the data at hand, in much the same way that we -tted the coef-cients of a linear regression function, so as to minimize the mean square error with respect to a set of samples (Figure 1.4). The method used will depend on the nature of the random variable $X$ whether it is continuous, discrete or binary, for example.

Assuming that the function $P ( X = x | Z = z )$ is available to us, we can use it to generate arti-cial samples that act as though they were drawn from the postintervention probability $P _ { m }$ rather than $P ( x , y , z )$ . Once we obtain such -ctitious samples, we can evaluate $P ( Y = y | d o ( x ) )$ by simply counting the frequency of the event $Y = y$ , for each stratum $X = x$ in the sample. In this way, we skip the labor associated with summing over all strata $Z = z$ ; we essentially let nature do the summation for us.

The idea of estimating probabilities using -ctitious samples is not new to us; it was used all along, though implicitly, whenever we estimated conditional probabilities from -nite samples.

In Chapter 1, we characterized conditioning as a process of -ltering‚Äîthat is, ignoring all cases for which the condition $X = x$ does not hold, and normalizing the surviving cases, so that their total probabilities would add up to one. The net result of this operation is that the probability of each surviving case is boosted by a factor $1 / P ( X = x )$ . This can be seen directly

from Bayes‚Äô rule, which tells us that

$$
P (Y = y, Z = z | X = x) = \frac {P (Y = y , Z = z , X = x)}{P (X = x)}
$$

In other words, to -nd the probability of each row in the surviving table, we multiply the unconditional probability, $P ( Y = y , Z = z , X = x )$ by the constant $1 / P ( X = x )$ .

Let us now examine the population created by the $d o ( X = x )$ operation and ask how the probability of each case changes as a result of this operation. The answer is given to us by the adjustment formula, which reads

$$
P (y | d o (x)) = \sum_ {z} P (Y = y | X = x, Z = z) P (Z = z)
$$

Multiplying and dividing the expression inside the sum by the propensity score $P ( X = x | Z =$ z), we get

$$
P (y | d o (x)) = \sum_ {z} \frac {P (Y = y | X = x , Z = z) P (X = x | Z = z) P (Z = z)}{P (X = x | Z = z)}
$$

Upon realizing the numerator is none other but the pretreatment distribution of $( X , Y , Z )$ , we can write

$$
P (y | d o (x)) = \sum_ {z} \frac {P (Y = y , X = x , Z = z)}{P (X = x | Z = z)}
$$

and the answer becomes clear: each case $\mathit { \check { Y } } = { y , X } = { x , Z } = { z } )$ in the population should boost its probability by a factor equal to $1 / P ( X = x | Z = z )$ . (Hence the name ‚Äú inverse probability weighing.‚Äù)

This provides us with a simple procedure of estimating $P ( Y = y | d o ( X = x ) )$ when we have -nite samples. If we weigh each available sample by a factor $= 1 / P ( X = x | Z = z )$ , we can then treat the reweighted samples as if they were generated from $P _ { m }$ , not $P$ , and proceed to estimate $P ( Y = y | d o ( x ) )$ accordingly.

This is best demonstrated in an example.

Table 3.3 returns to our Simpson‚Äôs paradox example of the drug that seems to help men and women but to hurt the general population. We‚Äôll use the same data we used before but presented

Table 3.3 Joint probability distribution $P ( X , Y , Z )$ for the druggender-recovery story of Chapter 1 (Table 1.1)   

<table><tr><td>X</td><td>Y</td><td>Z</td><td>% of population</td></tr><tr><td>Yes</td><td>Yes</td><td>Male</td><td>0.116</td></tr><tr><td>Yes</td><td>Yes</td><td>Female</td><td>0.274</td></tr><tr><td>Yes</td><td>No</td><td>Male</td><td>0.009</td></tr><tr><td>Yes</td><td>No</td><td>Female</td><td>0.101</td></tr><tr><td>No</td><td>Yes</td><td>Male</td><td>0.334</td></tr><tr><td>No</td><td>Yes</td><td>Female</td><td>0.079</td></tr><tr><td>No</td><td>No</td><td>Male</td><td>0.051</td></tr><tr><td>No</td><td>No</td><td>Female</td><td>0.036</td></tr></table>

Table 3.4 Conditional probability distribution $P ( Y , Z | X )$ for drug users $( X = y e s )$ in the population of Table 3.3   

<table><tr><td>X</td><td>Y</td><td>Z</td><td>% of population</td></tr><tr><td>Yes</td><td>Yes</td><td>Male</td><td>0.231</td></tr><tr><td>Yes</td><td>Yes</td><td>Female</td><td>0.549</td></tr><tr><td>Yes</td><td>No</td><td>Male</td><td>0.017</td></tr><tr><td>Yes</td><td>No</td><td>Female</td><td>0.203</td></tr></table>

this time as a weighted table. In this case, $X$ represents whether or not the patient took the drug, Y represents whether the patient recovered, and $Z$ represents the patient‚Äôs gender.

If we condition on ${ } ^ { 6 6 } X = Y e s$ ,‚Äù we get the data set shown in Table 3.4, which was formed in two steps. First, all rows with $X = N o$ were excluded. Second, the weights given to the remaining rows were ‚Äúrenormalized,‚Äù that is, multiplied by a constant so as to make them sum to one. This constant, according to Bayes‚Äô rule, is $1 / P ( X = y e s )$ , and $P ( X = y e s )$ in our example, is the combined weight of the -rst four rows of Table 3.3, which amounts to

$$
P (X = y e s) = 0. 1 1 6 + 0. 2 7 4 + 0. 0 1 + 0. 1 0 1 = 0. 5 0 1
$$

The result is the weight distribution in the four rows of Table 3.4; the weight of each row has been boosted by a factor $1 / 0 . 5 0 1 = 2 . 0 0$ .

Let us now examine the population created by the $d o ( X = y e s )$ operation, representing a deliberate decision to administer the drug to the same population.

To calculate the distribution of weights in this population, we need to compute the factor $P ( X = y e s | Z = z )$ for each $z$ , which, according to Table 3.3, is given by

$$
P (X = y e s \mid Z = M a l e) = \frac {(0 . 1 1 6 + 0 . 0 1)}{(0 . 1 1 6 + 0 . 0 1 + 0 . 3 3 4 + 0 . 0 5 1)} = 0. 2 4 7
$$

$$
P (X = y e s \mid Z = F e m a l e) = \frac {(0 . 2 7 4 + 0 . 1 0 1)}{(0 . 2 7 4 + 0 . 1 0 1 + 0 . 0 7 9 + 0 . 0 3 6)} = 0. 7 6 5
$$

Multiplying the gender-matching rows by $1 / 0 . 2 4 7$ and $1 / 0 . 7 6 5$ , respectively, we obtain Table 3.5, which represents the postintervention distribution of the population of Table 3.3. The probability of recovery in this distribution can now be computed directly from the data, by summing the -rst two rows:

$$
P (Y = y e s \mid d o (X = y e s)) = 0. 4 7 6 + 0. 3 5 7 = 0. 8 3 3
$$

Table 3.5 Probability distribution for the population of Table 3.3 under the intervention $d o ( X = Y e s )$ , determined via the inverse probability method   

<table><tr><td>X</td><td>Y</td><td>Z</td><td>% of population</td></tr><tr><td>Yes</td><td>Yes</td><td>Male</td><td>0.475</td></tr><tr><td>Yes</td><td>Yes</td><td>Female</td><td>0.358</td></tr><tr><td>Yes</td><td>No</td><td>Male</td><td>0.035</td></tr><tr><td>Yes</td><td>No</td><td>Female</td><td>0.132</td></tr></table>

Three points are worth noting about this procedure. First, the redistribution of weight is no longer proportional but quite discriminatory. Row #1, for instance, boosted its weight from 0.116 to 0.476, a factor of 4.1, whereas Row #2 is boosted from 0.274 to 0.357, a factor of only 1.3. This redistribution renders $X$ independent of $Z$ , as in a randomized trial (Figure 3.4).

Second, an astute reader would notice that in this example no computational savings were realized; to estimate $P ( Y = y e s | d o ( X = y e s ) )$ we still needed to sum over all values of $Z$ , males and females. Indeed, the savings become signi-cant when the number of $Z$ values is in the thousands or millions, and the sample size is in the hundreds. In such cases, the number of $Z$ values that the inverse probability method would encounter is equal to the number of samples available, not to the number of possible $Z$ values, which is prohibitive.

Finally, an important word of caution. The method of inverse probability weighing is only valid when the set Z entering the factor $1 / P ( X = x | Z = z )$ satis-es the backdoor criterion. Lacking this assurance, the method may actually introduce more bias than the one obtained through naive conditioning, which produces Table 3.4 and the absurdities of Simpson‚Äôs paradox.

Up to this point, and in the following, we focus on unbiased estimation of causal effects. In other words, we focus on estimates that will converge to the true causal effects as the number of samples increases inde-nitely.

This is obviously important, but it is not the only issue relevant to estimation. In addition, we must also address precision. Precision refers to the variability of our causal estimates if the number of samples is -nite, and, in particular, how much our estimate would vary from experiment to experiment. Clearly, all other things being equal, we prefer estimation procedures with high precision in addition to their possessing little or no bias. Practically, high-precision estimates lead to shorter con-dence intervals that quantify our level of certainty as to how our sample estimates describe the causal effect of interest. Most of our discussion does not address the ‚Äúbest,‚Äù or most precise, way to estimate relevant causal means and effects but focuses on whether it is possible to estimate such quantities from observed data distributions, when the number of samples goes to in-nity.

For example, suppose we wish to estimate the causal effect of $X$ on $Y$ (in a causal graph as above), where $X$ and $Y$ both reect continuous variables. Suppose the effect of $Z$ is to make both high and low values of $X$ most commonly observed, with values close to the middle of the range of $X$ much less common. Then, inverse probability weighting down-weights the extreme values of $X$ on both ends of its range (since these are observed most frequently due to $Z$ ) and essentially focuses entirely on the ‚Äúmiddle‚Äù values of $X$ . If we then use a regression model to estimate the causal effect of $X$ on $Y$ (see Section 3.8, for example) using the reweighed observations to account for the role of $Z$ , the resulting estimates will be very imprecise. In such cases, we usually seek for alternative estimation strategies that are more precise. While we do not pursue these alternatives in this book, it is important to emphasize that, in addition to seeing that causal effects be identi-ed from the data, we must also devise effective strategies of using -nite data to estimate effect sizes.

# 3.7 Mediation

Often, when one variable causes another, it does so both directly and indirectly, through a set of mediating variables. For instance, in our blood pressure/treatment/recovery example of Simpson‚Äôs paradox, treatment is both a direct (negative) cause of recovery, and an indirect

(positive) cause, through the mediator of blood pressure‚Äîtreatment decreases blood pressure, which increases recovery. In many cases, it is useful to know how much of variable X‚Äôs effect on variable Y is direct and how much is mediated. In practice, however, separating these two avenues of causation has proved dif-cult.

Suppose, for example, we want to know whether and to what degree a company discriminates by gender $( X )$ in its hiring practices (Y). Such discrimination would constitute a direct effect of gender on hiring, which is illegal in many cases. However, gender also affects hiring practices in other ways; often, for instance, women are more or less likely to go into a particular -eld than men, or to have achieved advanced degrees in that -eld. So gender may also have an indirect effect on hiring through the mediating variable of quali-cations $( Z )$ .

In order to -nd the direct effect of gender on hiring, we need to somehow hold quali-cations steady, and measure the remaining relationship between gender and hiring; with quali-cations unchanging, any change in hiring would have to be due to gender alone. Traditionally, this has been done by conditioning on the mediating variable. So if P(Hired Female, Highly Quali-ed) is different from $P ( H i r e d | M a l e , H i g h l y \ Q u a l i f i e d )$ , the reasoning goes, then there is a direct effect of gender on hiring.

![](images/8e0e27da94c7984b6bfbfda68fbdf7fddfd5e7adc315e0bb565875e9b68c26ff.jpg)  
Figure 3.11 A graphical model representing the relationship between gender, quali-cations, and hiring

In the example in Figure 3.11, this is correct. But consider what happens if there are confounders of the mediating variable and the outcome variable. For instance, income: People from higher income backgrounds are more likely to have gone to college and more likely to have connections that would help them get hired.

Now, if we condition on quali-cations, we are conditioning on a collider. So if we don‚Äôt condition on quali-cations, indirect dependence can pass from gender to hiring through the path Gender Quali-cations Hiring. But if we do condition on quali-cations, indirect dependence can pass from gender to hiring through the path G ${ \it \Omega } ^ { \it { ? } n d e r }  Q u a l i f i c a t i o n s  I n c o m e $ Hiring. (To understand the problem intuitively, note that by conditioning on quali-cation, we will be comparing men and women at different levels of income, because income must change to keep quali-cation constant.) No matter how you look at it, we‚Äôre not getting the true direct effect of gender on hiring. Traditionally, therefore, statistics has had to abandon a huge class of potential mediation problems, where the concept of ‚Äúdirect effect‚Äù could not be de-ned, let alone estimated.

Luckily, we now have a conceptual way of holding the mediating variable steady without conditioning on it: We can intervene on it. If, instead of conditioning, we -x the quali-cations, the arrow between gender and quali-cations (and the one between income and quali-cations) disappears, and no spurious dependence can pass through it. (Of course, it would be impossible for us to literally change the quali-cations of applicants, but recall, this is a theoretical intervention of the kind discussed in the previous section, accomplished by choosing a proper adjustment.) So for any three variables $X , Y$ , and $Z$ , where $Z$ is a mediator between $X$ and $Y$ ,

the controlled direct effect (CDE) on Y of changing the value of $X$ from $x$ to $x ^ { \prime }$ is de-ned as

$$
C D E = P (Y = y \mid d o (X = x), d o (Z = z)) - P (Y = y \mid d o (X = x ^ {\prime}), d o (Z = z)) \tag {3.18}
$$

The obvious advantage of this de-nition over the one based on conditioning is its generality; it captures the intent of ‚Äúkeeping Z constant‚Äù even in cases where the $Z \to Y$ relationship is confounded (the same goes for the $X \to Z$ and $X  Y$ relationships). Practically, this de-nition assures us that in any case where the intervened probabilities are identi-able from the observed probabilities, we can estimate the direct effect of $X$ on Y. Note that the direct effect may differ for different values of $Z$ ; for instance, it may be that hiring practices discriminate against women in jobs with high quali-cation requirements, but they discriminate against men in jobs with low quali-cations. Therefore, to get the full picture of the direct effect, we‚Äôll have to perform the calculation for every relevant value $z$ of $Z$ . (In linear models, this will not be necessary; for more information, see Section 3.8.)

![](images/416411a6c073fd2da44837d18fad866348c061fa1d021880fe24fec01d6052be.jpg)  
Figure 3.12 A graphical model showing qualification (Z) as a mediator between gender $( X )$ and hiring $( Y )$ , and income $( I )$ as a confounder between qualification and hiring.

How do we estimate the direct effect when its expression contains two $d o$ -operators? The technique is more or less the same as the one employed in Section 3.2, where we dealt with a single $d o$ -operator by adjustment. In our example of Figure 3.12, we -rst notice that there is no backdoor path from $X$ to $Y$ in the model, hence we can replace $d o ( x )$ with simply conditioning on $x$ (this essentially amounts to adjusting for all confounders). This results in

$$
P (Y = y | X = x, d o (Z = z)) - P (Y = y | X = x ^ {\prime}, d o (Z = z))
$$

Next, we attempt to remove the $d o ( z )$ term and notice that two backdoor paths exist from Z to $Y$ , one through $X$ and one through $I$ . The -rst is blocked (since $X$ is conditioned on) and the second can be blocked if we adjust for I. This gives

$$
\sum_ {i} [ P (Y = y | X = x, Z = z, I = i) - P (Y = y | X = x ^ {\prime}, Z = z, I = i) ] P (I = i)
$$

The last formula is $d o$ -free, which means it can be estimated from nonexperimental data.

In general, the CDE of $X$ on $Y$ , mediated by $Z$ , is identi-able if the following two properties hold:

1. There exists a set $S _ { 1 }$ of variables that blocks all backdoor paths from $Z$ to $Y$   
2. There exists a set $S _ { 2 }$ of variables that blocks all backdoor paths from $X$ to $Y$ , after deleting all arrows entering $Z$ .

If these two properties hold in a model $M$ , then we can determine $P ( Y = y | d o ( X = x )$ , $d o ( Z = z ) )$ from the data set by adjusting for the appropriate variables, and estimating the conditional probabilities that ensue. Note that condition 2 is not necessary in randomized trials, because randomizing $X$ renders $X$ parentless. The same is true in cases where $X$ is judged to be exogenous (i.e., ‚Äúas if‚Äù randomized), as in the aforementioned gender discrimination example.

It is even trickier to determine the indirect effect than the direct effect, because there is simply no way to condition away the direct effect of $X$ on Y. It‚Äôs easy enough to -nd the total effect and the direct effect, so some may argue that the indirect effect should just be the difference between those two. This may be true in linear systems, but in nonlinear systems, differences don‚Äôt mean much; the change in Y might, for instance, depend on some interaction between $X$ and $Z$ ‚Äîif, as we posited above, women are discriminated against in high-quali-cation jobs and men in low-quali-cation jobs, subtracting the direct effect from the total effect would tell us very little about the effect of gender on hiring as mediated by quali-cations. Clearly, we need a de-nition of indirect effect that does not depend on the total or direct effects.

We will show in Chapter 4 that these dif-culties can be overcome through the use of counterfactuals, a more re-ned type of intervention that applies at the individual level and can be computed from structural models.

# 3.8 Causal Inference in Linear Systems

One of the advantages of the causal methods we have introduced in this book is that they work regardless of the type of equations that make up the model in question. $d$ -separation and the backdoor criterion make no assumptions about the form of the relationship between two variables‚Äîonly that the relationship exists.

However, showcasing and explaining causal methods from a nonparametric standpoint has limited our ability to present the full power of these methods as they play out in linear systems‚Äîthe arena where traditional causal analysis has primarily been conducted in the social and behavioral sciences. This is unfortunate, as many statisticians work extensively in linear systems, and nearly all statisticians are very familiar with them.

In this section, we examine in depth what causal assumptions and implications look like in systems of linear equations and how graphical methods can help us answer causal questions posed in those systems. This will serve as both a reinforcement of the methods we applied in nonparametric models and as a useful aid for those hoping to apply causal inference speci-- cally in the context of linear systems.

For instance, we might want to know the effect of birth control use on blood pressure after adjusting for confounders; the total effect of an after-school study program on test scores; the direct effect, unmediated by other variables, of the program on test scores; or the effect of enrollment in an optional work training program on future earnings, when enrollment and earnings are confounded by a common cause (e.g., motivation). Such questions, invoking continuous variables, have traditionally been formulated as linear equation models with only minor attention to the unique causal character of those equations; we make this character unambiguous.

In all models used in this section, we make the strong assumption that the relationships between variables are linear, and that all error terms have Gaussian (or ‚Äúnormal‚Äù) distributions (in some cases, we only need to assume symmetric distributions). This assumption provides an

enormous simpli-cation of the procedure needed for causal analysis. We are all familiar with the bell-shaped curve that characterizes the normal distribution of one variable. The reason it is so popular in statistics is that it occurs so frequently in nature whenever a phenomenon is a byproduct of many noisy microprocesses that add up to produce macroscopic measurements such as height, weight, income, or mortality. Our interest in the normal distribution, however, stems primarily from the way several normally distributed variables combine to shape their joint distribution. The assumption of normality gives rise to four properties that are of enormous use when working with linear systems:

1. Ef-cient representation   
2. Substitutability of expectations for probabilities   
3. Linearity of expectations   
4. Invariance of regression coef-cients.

Starting with two normal variables, $X$ and Y, we know that their joint density forms a three-dimensional cusp (like a mountain rising above the $X { - } Y$ plane) and that the planes of equal height on that cusp are ellipses like those shown in Figure 1.2. Each such ellipse is characterized by -ve parameters: $\mu _ { X } , \mu _ { Y } , \sigma _ { X } , \sigma _ { Y }$ , and $\rho _ { X Y }$ , as de-ned in Sections 1.3.8 and 1.3.9. The parameters $\mu _ { X }$ and $\mu _ { Y }$ specify the location (or the center of gravity) of the ellipse in the $X { - } Y$ plane, the standard deviations $\sigma _ { X }$ and $\sigma _ { Y }$ specify the spread of the ellipse along the $X$ and Y dimensions, respectively, and the correlation coefficient $\rho _ { X Y }$ speci-es its orientation. In three dimensions, the best way to depict the joint distribution is to imagine an oval football sus-pended in the $X { - } Y { - } Z$ space (Figure 1.2); every plane of constant $Z$ would then cut the football in a two-dimensional ellipse like the ones shown in Figure 1.1.

As we go to higher dimensions, and consider a set of $N$ normally distributed variables $X _ { 1 } , X _ { 2 } , \dots , X _ { N }$ $X _ { 1 } , X _ { 2 }$ , we need not concern ourselves with additional parameters; it is suf-cient to specify those that characterize the $N ( N - 1 ) / 2$ pairs of variables, $( X _ { i } , X _ { j } )$ . In other words, the joint density of $( X _ { 1 } , X _ { 2 } , \dots , X _ { N } )$ is fully speci-ed once we specify the bivariate density of $( X _ { i } , X _ { j } )$ , with $i$ and $j$ $( i \neq j )$ ranging from 1 to $N$ . This is an enormously useful property, as it offers an extremely parsimonious way of specifying the $N _ { ‚òâ }$ -variable joint distribution. Moreover, since the joint distribution of each pair is speci-ed by -ve parameters, we conclude that the joint distribution requires at most $5 \times N ( N - 1 ) / 2$ parameters (means, variances, and covariances), each de-ned by expectation. In fact, the total number of parameters is even smaller than this, namely $2 N + N ( N - 1 ) / 2$ ; the -rst term gives the number of mean and variance parameters, and the second the number of correlations.

This brings us to another useful feature of multivariate normal distributions: they are fully de-ned by expectations, so we need not concern ourselves with probability tables as we did when dealing with discrete variables. Conditional probabilities can be expressed as conditional expectations, and notions such as conditional independence that de-ne the structure of graphical models can be expressed in terms of equality relationships among conditional expectations. For instance, to express the conditional independence of Y and $X$ , given Z,

$$
P (Y | X, Z) = P (Y | Z)
$$

we can write

$$
E [ Y | X, Z ] = E [ Y | Z ]
$$

(where $Z$ is a set of variables).

This feature of normal systems gives us an incredibly useful ability: Substituting expectations for probabilities allows us to use regression (a predictive method) to determine causal information. The next useful feature of normal distributions is their linearity: every conditional expectation $E [ Y | X _ { 1 } , X _ { 2 }$ , ‚Ä¶ , $X _ { n } ]$ is given by a linear combination of the conditioning variables. Formally,

$$
E [ Y | X _ {1} = x _ {1}, X _ {2} = x _ {2}, \dots , X _ {n} = x _ {n} ] = r _ {0} + r _ {1} x _ {1} + r _ {2} x _ {2} + \dots + r _ {n} x _ {n}
$$

where each of the slopes $r _ { 1 } , r _ { 2 } , . . . , r _ { n }$ is a partial regression coefficient as defined in Sections 1.3.10 and 1.3.11.

The magnitudes of these slopes do not depend on the values $x _ { 1 } , x _ { 2 }$ , ‚Ä¶ , $x _ { n }$ of the conditioning variables, called regressors; they depend only on which variables are chosen as regressors. In other words, the sensitivity of Y to the measurement $X _ { i } = x _ { i }$ does not depend on the measured values of the other variables in the regression; it depends only on which variables we choose to measure. It doesn‚Äôt matter whether $X _ { i } = 1$ , $X _ { i } = 2$ , or $X _ { i } = 3 1 2 . 3$ ; as long as we regress $Y$ on $X _ { 1 } ,$ $\mathbf { \psi } _ { 1 } , X _ { 2 } , . . . , X _ { n }$ all slopes will remain the same.

This unique and useful feature of normal distributions is illustrated in Figures 1.1 and 1.2 of Chapter 1. Figure 1.1 shows that regardless of what level of age we choose, the slope of $Y$ on $X$ at that level is the same. If, however, we do not hold age constant (i.e., we do not regress on it), the slope becomes vastly different, as is shown in Figure 1.2.

The linearity assumption also permits us to fully specify the functions in the model by annotating the causal graph with a path coef-cient (or structural coef-cient) along each edge. The path coef-cient $\beta$ along the edge $X  Y$ quanti-es the contribution of $X$ in the function that de-nes Y in the model. For instance, if the function de-nes $Y = 3 X + U$ , the path coef-cient of $X  Y$ will be 3. The path coef-cients $\beta _ { 1 } , \beta _ { 2 } , \ldots , \beta _ { n }$ are fundamentally different from the regression coef-cients $r _ { 1 } , r _ { 2 } , \ldots , r _ { n }$ $r _ { 1 } , r _ { 2 }$ that we discussed in Section 1.3. The former are ‚Äústructural‚Äù or ‚Äúcausal,‚Äù whereas the latter are statistical. The difference is explained in the next section.

Many of the regression methods we discuss are far more general, applying in situations where the variables $X _ { 1 }$ , ‚Ä¶ , $X _ { k }$ follow distribution far from multivariate Normal; for example, when some of the $X _ { i }$ ‚Äôs are categorical or even binary. Such generalizations also therefore allow the conditional mean $E ( Y | X _ { 1 } = x _ { 1 }$ , ‚Ä¶ , $X _ { k } = x _ { k } ,$ ) to include nonlinear combinations of the $X _ { i }$ ‚Äôs, including such terms as $X _ { 1 } X _ { 2 }$ , for example, to allow for effect modi-cation, or interaction. Since we are conditioning on the values of the $X _ { i }$ ‚Äôs, it is usually not necessary to enforce a distributional assumption for such variables. Nevertheless, the full multivariate Normal scenario provides considerable insight into structural causal models.

# 3.8.1 Structural versus Regression Coef-cients

As we are now about to deal with linear models, and thus, as a matter of course, with regression-like equations, it is of paramount importance to de-ne the difference between regression equations and the structural equations we have used in SCMs throughout the book. A regression equation is descriptive; it makes no assumptions about causation. When we write $y = r _ { 1 } x + r _ { 2 } z + \epsilon$ , as a regression equation, we are not saying that $X$ and Z cause Y. We merely confess our need to know which values of $r _ { 1 }$ and $r _ { 2 }$ would make the equation $y = r _ { 1 } x + r _ { 2 } z$

the best linear approximation to the data, or, equivalently, the best linear approximation of $E ( y | x , z )$ .

Because of this fundamental difference between structural and regression equations, some books distinguish them by writing an arrow, instead of equality sign, in structural equations, and some distinguish the coef-cients by using a different font. We distinguish them by denoting structural coef-cients as $\alpha , \beta$ , and so on, and regression coef-cients as $r _ { 1 } , r _ { 2 }$ , and so on. In addition, we distinguish between the stochastic ‚Äúerror terms‚Äù that appear in these equations. Errors in regression equations are denoted $\epsilon _ { 1 } , \epsilon _ { 2 }$ and so on, as in Eq. (1.24), and those in, structural equations by $U _ { 1 }$ ., U2, and so on, as in SCM 1.5.2. The former denote the residual errors in observation, after fitting the equation $y = r _ { 1 } x + r _ { 2 } z$ to data, whereas the latter represent latent factors (sometimes called ‚Äúdisturbances‚Äù or ‚Äúomitted variables‚Äù) that influence Y and are not themselves affected by $X$ . The former are human-made (due to imperfect -tting); the latter are nature-made.

Though they are not causally binding themselves, regression equations are of signi-cant use in the study of causality as it pertains to linear systems. Consider: In Section 3.2, we were able to express the effects of interventions in terms of conditional probabilities, as, for example, in the adjustment formula of Eq. (3.5). In linear systems, the role of conditional probabilities will be taken over by regression coefficients, since t hese c oefficients re present th e dependencies induced by the model and, in addition, they are easily estimable using least square analyses. Similarly, whereas the testable implications of nonparametric models are expressed in the form of conditional independencies, these independencies are signified in linear models by vanishing regression coefficients, like those discussed in Section 1.3.11. Speci-cally, given the regression equation

$$
y = r _ {0} + r _ {1} x _ {1} + r _ {2} x _ {2} + \dots + r _ {n} x _ {n} + \epsilon
$$

if $r _ { i } = 0$ , then Y is independent of $X _ { i }$ conditional on all the other regression variables.

# 3.8.2 The Causal Interpretation of Structural Coef-cients

In a linear system, every path coef-cient stands for the direct effect of the independent variable, $X$ , on the dependent variable, Y. To see why this is so, we refer to the interventional de-nition of direct effect given in Section 3.7 (Eq. (3.18)), which calls for computing the change in Y as $X$ increases by one unit whereas all other parents of Y are held constant. When we apply this de-nition to any linear system, regardless of whether the disturbances are correlated or not, the result will be the path coef-cient on the arrow $X  Y$ .

Consider, for example, the model in Figure 3.13, and assume we wish to estimate the direct effect of $Z$ on Y. The structural equations in the fully speci-ed model read:

$$
\begin{array}{l} X = U _ {X} \\ Z = a X + U _ {Z} \\ W = b X + c Z + U _ {W} \\ Y = d Z + e W + U _ {Y} \\ \end{array}
$$

Writing Eq. (3.18) in expectation form, we obtain

$$
D E = E [ Y | d o (Z = z + 1), d o (W = w) ] - E [ Y | d o (Z = z), d o (W = w) ]
$$

since W is the only other parent of Y in the graph. Applying the do operators by deleting the appropriate equations from the model, the postincrease term in $D E$ becomes $d ( z + 1 ) + e w$ and the preincrease term becomes $d z + e w$ . As expected, the difference between the two is $d \mathbf { \cdot }$ ‚Äîthe path coef-cient between $Z$ and Y. Note that the license to reduce the equation in this way comes directly from the de-nition of the $d o$ -operator (Eq. (3.18)) making no assumption about correlations among the $U$ factors; the equality $D E = d$ would be valid even if the error term $U _ { Y }$ were correlated with $U _ { Z }$ , though this would have made $d$ nonidenti-able. The same goes for the other direct effects; every structural coef-cient represents a direct effect, regardless of how the error terms are distributed. Note also that variable $X$ , as well as the coef-cients $a , b$ , and $c$ , do not enter into this computation, because the ‚Äúsurgeries‚Äù required by the $d o$ operators remove them from the model.

That is all well and good for the direct effect. Suppose, however, we wish to calculate the total effect of $Z$ on $Y$ .

![](images/014b9190ba218265e5935f95e3ad2c3490e15566d50930afeb92becd2e6858e1.jpg)  
Figure 3.13 A graphical model illustrating the relationship between path coef-cients and total effects

In a linear system, the total effect of $X$ on $Y$ is simply the sum of the products of the coef-- cients of the edges on every nonbackdoor path from $X$ to $Y$ .

That‚Äôs a bit of a mouthful, so think of it as a process: To -nd the total effect of $X$ on $Y$ , -rst -nd every nonbackdoor path from $X$ to $Y$ ; then, for each path, multiply all coef-cients on the path together; then add up all the products.

The reason for this identity lies in the nature of SCMs. Consider again the graph of Figure 3.13. Since we want to -nd the total effect of $Z$ on Y, we should -rst intervene on $Z$ , removing all arrows going into $Z$ , then express $Y$ in terms of $Z$ in the remaining model. This we can do with a little algebra:

$$
\begin{array}{l} Y = d Z + e W + U _ {Y} \\ = d Z + e (b X + c Z) + U _ {Y} + e U _ {W} \\ = (d + e c) Z + e b X + U _ {Y} + e U _ {W} \\ \end{array}
$$

The -nal expression is in the form $Y = \tau Z + U$ , where $\tau = d + e c$ and $U$ contains only terms that do not depend on $Z$ in the modi-ed model. An increase of a single unit in $Z$ , therefore, will increase $Y$ by $\tau$ ‚Äîthe de-nition of the total effect. A quick examination will show that $\tau$

is the sum of the products of the coef-cients on the two nonbackdoor paths from Z to Y. This will be the case in all linear models; algebra demands it. Moreover, the sum of product rule will be valid regardless of the distributions of the $U$ variables and regardless of whether they are dependent or independent.

# 3.8.3 Identifying Structural Coef-cients and Causal Effect

Thus far, we have expressed the total and direct effects in terms of path coef-cients, assuming that the latter are either known to us a priori or estimated from interventional experiments. We now tackle a much harder problem; estimating total and direct effects from nonexperimental data. This problem is known as ‚Äúidenti-ability‚Äù and, mathematically, it amounts to expressing the path coef-cients associated with the total and direct effects in terms of the covariances $\sigma _ { X Y }$ or regression coefficients $R _ { Y X \cdot Z } ,$ where $X$ and Y are any two variables in the model, and $Z$ a s e t of variables in the model (Eqs. (1.27) and (1.28) and Section 1.3.11).

In many cases, however, it turns out that to identify direct and total effects, we do not need to identify each and every structural parameter in the model. Let us -rst demonstrate with the total effect, $\tau$ . The backdoor criterion gives us the set $Z$ of variables we need to adjust for in order to determine the causal effect of $X$ on Y. How, though, do we make use of the criterion to determine effects in a linear system? In principle, once we obtain the set, Z, we can estimate the conditional expectation of $Y$ given $X$ and $Z$ and, then, averaging over $Z$ , we can use the resultant dependence between Y and $X$ to measure the effect of $X$ on Y. We need only translate this procedure to the language of regression.

The translation is rather simple. First, we -nd a set of covariates $Z$ that satis-es the backdoor criterion from $X$ to $Y$ in the model. Then, we regress $Y$ on $X$ and Z. The coef-cient of $X$ in the resulting equation represents the true causal effect of $X$ on Y. The reasoning for this is similar to the reasoning we used to justify the backdoor criterion in the -rst place‚Äîregressing on $Z$ adds those variables into the equation, blocking all backdoor paths from $X$ and Y, thus preventing the coef-cient of $X$ from absorbing the spurious information those paths contain.

For example, consider a linear model that complies with the graph in Figure 3.14. If we want to -nd the total causal effect of $X$ on Y, we -rst determine, using the backdoor criterion, that we must adjust for $T$ . So we regress $Y$ on $X$ and $T$ , using the regression equation $y = r _ { X } X +$

![](images/23f28ac4623e116c794a4cbdc9b4a072e83ac445ad51ef124ffb707d975e1cf4.jpg)  
Figure 3.14 A graphical model in which $X$ has no direct effect on Y, but a total effect that is determined by adjusting for T

$r _ { T } T + \epsilon$ . The coef-cient $r _ { X }$ represents the total effect of $X$ on Y. Note that this identi-cation was possible without identifying any of the model parameters and without measuring variable W; the graph structure in itself gave us the license to ignore W, regress Y on $T$ and $X$ only, and identify the total effect (of $X$ on Y) with the coef-cient of $X$ in that regression.

Suppose now that instead of the total causal effect, we want to -nd $X ^ { \prime } \mathrm { s }$ direct effect on Y. In a linear system, this direct effect is the structural coef-cient $\alpha$ in the function $y = \alpha x + \beta z +$ $\cdots + U _ { Y }$ that de-nes $Y$ in the system. We know from the graph of Figure 3.14 that $\alpha = 0$ , because there is no direct arrow from $X$ to Y. So, in this particular case, the answer is trivial: the direct effect is zero. But in general, how do we -nd the magnitude of $\alpha$ from data, if the model does not determine its value?

![](images/74b0992781b2c4d2cc0928402a12c341b47ed2ca8f43b3faab181370a3558bae.jpg)  
Figure 3.15 A graphical model in which $X$ has direct effect $\alpha$ on Y

We can invoke a procedure similar to backdoor, except that now, we need to block not only backdoor paths but also indirect paths going from $X$ to Y. First, we remove the edge from $X$ to Y (if such an edge exists), and call the resulting graph $G _ { \alpha }$ . If, in $G _ { \alpha }$ , there is a set of variables $Z$ that $d$ -separates $X$ and $Y$ , then we can simply regress $Y$ on $X$ and $Z$ . The coef-cient of $X$ in the resulting equation will equal the structural coef-cient $\alpha$ .

The procedure above, which we might as well call ‚ÄúThe Regression Rule for Identi-cation‚Äù provides us with a quick way of determining whether any given parameter (say $\alpha$ ) can be identified by ordinary least square (OLS) regression and, if so, what variables should go into the regression equation. For example, in the linear model of Figure 3.15, we can -nd the direct effect of $X$ on Y by this method. First, we remove the edge between $X$ and Y and get the graph $G _ { \alpha }$ shown in Figure 3.16. It‚Äôs easy to see that in this new graph, W d-separates $X$ and Y. So we regress Y on $X$ and W, using the regression equation $Y = r _ { X } X + r _ { W } W + \epsilon .$ . The coef-cient $r _ { X }$ is the direct effect of $X$ on Y .

Summarizing our observations thus far, two interesting features emerge. First, we see that, in linear systems, regression serves as the major tool for the identi-cation and estimation of causal effects. To estimate a given effect, all we need to do is to write down a regression equation and specify (1) what variables should be included in the equation and (2) which of the coef-cients in that equation represents the effect of interest. The rest is routine least square analysis on the sampled data which, as we remarked before, is facilitated by a variety of extremely ef-cient software packages. Second, we see that, as long as the $U$ variables are independent of each

![](images/aa93e8b2a6f40017f94ebebf774e205472526ca9ced685af4ba115398d9377c5.jpg)  
Figure 3.16 By removing the direct edge from $X$ to $Y$ and -nding the set of variables $\{ W \}$ that $d$ -separate them, we -nd the variables we need to adjust for to determine the direct effect of $X$ on $Y$

other, and all variables in the graph are measured, every structural parameter can be identi-ed in this manner, namely, there is at least one identifying regression equation in which one of the coef-cients corresponds to the parameter we seek to estimate. One such equation is obviously the structural equation itself, with the parents of $Y$ serving as regressors. But there may be several other identifying equations, with possibly better features for estimation and graphical analysis can reveal them all (see Study question 3.8.1(c)). Moreover, when some variables are not measured, or when some error terms are correlated, the task of -nding an identifying regression from the structural equations themselves would normally be insurmountable; the $G _ { \alpha }$ procedure then becomes indispensable (see Study question 3.8.1(d)).

Remarkably, the regression rule procedure has eluded investigators for almost a century, possibly because it is extremely dif-cult to articulate in algebraic, nongraphical terms.

Suppose, however, there is no set of variables that $d$ -separates $X$ and Y in $G _ { \alpha }$ . For instance, in Figure 3.17, $X$ and $Y$ have an unobserved common cause represented by the dashed

![](images/8d94aab26bfe2989da9841d446af67e32517140cba637384238c71aff7a788e8.jpg)  
Figure 3.17 A graphical model in which we cannot -nd the direct effect of $X$ on $Y$ via adjustment, because the dashed double-arrow arc represents the presence of a backdoor path between $X$ and $Y$ , consisting of unmeasured variables. In this case, $Z$ is an instrument with regard to the effect of $X$ on $Y$ that enables the identi-cation of $\alpha$

double-arrowed arc. Since it hasn‚Äôt been measured, we can‚Äôt condition on it, so $X$ and $Y$ will always be dependent through it. In this particular case, we may use an instrumental variable to determine the direct effect. A variable is called an ‚Äúinstrument‚Äù if it is $d$ -separated from Y in $G _ { \alpha }$ and, it is $d$ -connected to $X$ . To see why such a variable enables us to identify structural coef-cients, we take a closer look at Figure 3.17.

In Figure 3.17, Z is an instrument with regard to the effect of $X$ on Y because it is $d$ -connected to $X$ and $d$ -separated from Y in $G _ { \alpha }$ . We regress $X$ and $Y$ on $Z$ separately, yielding the regression equations $y = r _ { 1 } z + \epsilon$ and $x = r _ { 2 } z + \epsilon$ , respectively. Since $Z$ emits no backdoors, $r _ { 2 }$ equals $\beta$ and $r _ { 1 }$ equals the total effect of $Z$ on Y, $\beta \alpha$ . Therefore, the ratio $r _ { 1 } / r _ { 2 }$ provides the desired coef-cient $\alpha$ . This example illustrates how direct effects can be identi-ed from total effects but not the other way around.

Graphical models provide us with a procedure for -nding all instrumental variables in a system, though the procedure for enumerating them is beyond the scope of this book. Those interested in learning more can (see Chen and Pearl 2014; Kyono 2010).

# Study questions

# Study question 3.8.1

# Model 3.1

$$
Y = a W _ {3} + b Z _ {3} + c W _ {2} + U \quad X = t _ {1} W _ {1} + t _ {2} Z _ {3} + U ^ {\prime}
$$

$$
W _ {3} = c _ {3} X + U _ {3} ^ {\prime} \quad W _ {1} = a _ {1} ^ {\prime} Z _ {1} + U _ {1} ^ {\prime}
$$

$$
Z _ {3} = a _ {3} Z _ {1} + b _ {3} Z _ {2} + U _ {3} \qquad \qquad Z _ {1} = U _ {1}
$$

$$
W _ {2} = c _ {2} Z _ {2} + U _ {2} ^ {\prime} \quad Z _ {2} = U _ {2}
$$

![](images/b2ffe1a836a86017094d38974b84db1060ca9f76d2ee095207344726ef54d152.jpg)  
Figure 3.18 Graph corresponding to Model 3.1 in Study question 3.8.1

Given the model depicted above, answer the following questions:

(All answers should be given in terms of regression coef-cients in speci-ed regression equations.)

(a) Identify three testable implications of this model.   
(b) Identify a testable implication assuming that only X, Y, $W _ { 3 }$ , and $Z _ { 3 }$ are observed.   
(c) For each of the parameters in the model, write a regression equation in which one of the coef-cients is equal to that parameter. Identify the parameters for which more than one such equation exists.   
(d) Suppose $X$ , Y, and $W _ { 3 }$ are the only variables observed. Which parameters can be identi-ed from the data? Can the total effect of X on Y be estimated?   
(e) If we regress $Z _ { 1 }$ on all other variables in the model, which regression coef-cient will be zero?   
(f) The model in Figure 3.18 implies that certain regression coef-cients will remain invariant when an additional variable is added as a regressor. Identify -ve such coef-cients with their added regressors.   
(g) Assume that variables $Z _ { 2 }$ and $W _ { 2 }$ cannot be measured. Find a way to estimate b using regression coef-cients. [Hint: Find a way to turn $Z _ { 1 }$ into an instrumental variable for b.]

# 3.8.4 Mediation in Linear Systems

When we can assume linear relationships between variables, mediation analysis becomes much simpler than the analysis conducted in nonlinear or nonparametric systems (Section 3.7). Estimating the direct effect of $X$ on Y, for instance, amounts to estimating the path coef-cient between the two variables, and this reduces to estimating correlation coef-cients, using the techniques introduced in Section 3.8.3. The indirect effect, similarly, is computed via the difference $I E = \tau - D E$ , where $\tau$ , the total effect, can be estimated by regression in the manner shown in Figure 3.14. In nonlinear systems, on the other hand, the direct effect is de-ned through expressions such as (3.18), or

$$
D E = E [ Y | d o (x, z) ] - E [ Y | d o (x ^ {\prime}, z) ]
$$

where $Z = z$ represents a speci-c stratum of all other parents of Y (besides $X$ ). Even when the identi-cation conditions are satis-ed, and we are able to reduce the do() operators (by adjustments) to ordinary conditional expectations, the result will still depend on the speci-c values of $x , x ^ { \prime }$ , and z. Moreover, the indirect effect cannot be given a de-nition in terms as do-expressions, since we cannot disable the capacity of Y to respond to $X$ by holding variables constant. Nor can the indirect effect be de-ned as the difference between the total and direct effects, since differences do not faithfully reect operations in nonlinear systems to $X$ .

Such an operation will be introduced in Chapter 4 (Sections 4.4.5 and 4.5.2) using the language of counterfactuals.

# Bibliographical Notes for Chapter 3

Study question 3.3.2 is a version of Lord‚Äôs paradox (Lord 1967), and is described in Glymour (2006), Hern√°ndez-D√≠az et al. (2006), Senn (2006), and Wainer (1991). A unifying treatment is given in Pearl (2016). The de-nition of the do-operator and ‚ÄúACE‚Äù in terms of a modi-ed model, has its conceptual origin with the economist Trygve Haavelmo (1943), who was the -rst

to simulate interventions by modifying equations in the model (see Pearl (2015c) for historical account). Strotz and Wold (1960) later advocated ‚Äúwiping out‚Äù the equation determining $X$ , and Spirtes et al. (1993) gave it a graphical representation in a form of a ‚Äúmanipulated graph.‚Äù The ‚Äúadjustment formula‚Äù of Eq. (3.5) as well as the ‚Äútruncated product formula‚Äù -rst appeared in Spirtes et al. (1993), though these are implicit in the $G$ -computation formula of Robins (1986), which was derived using counterfactual assumptions (see Chapter 4). The backdoor criterion of De-nition 3.3.1 and its implications for adjustments were introduced in Pearl (1993). The front-door criterion and a general calculus for identifying causal effects (named do-calculus) from observations and experimental data were introduced in Pearl (1995) and were further improved in Tian and Pearl (2002), Shpitser and Pearl (2007), and Bareinboim and Pearl (2012). Section 3.7, and the identi-cation of conditional interventions and $c$ -speci-c effects is based on (Pearl 2009, pp. 113‚Äì114). Its extension to dynamic, time-varying policies is described in Pearl and Robins (1995) and (Pearl 2009, pp. 119‚Äì126). More recently, the do-calculus was used to solve problems of external validity, data-fusion, and meta-analysis (Bareinboim and Pearl 2013, Bareinboim and Pearl 2016, and Pearl and Bareinboim 2014). The role of covariate-speci-c effects in assessing interaction, moderation or effect modi-cation is described in Morgan and Winship (2014) and Vanderweele (2015), whereas applications of Rule 2 to the detection of latent heterogeneity are described in Pearl (2015b). Additional discussions on the use of inverse probability weighting (Section 3.6) can be found in Hern√°n and Robins (2006). Our discussion of mediation (Section 3.7) and the identi-cation of CDEs are based on Pearl (2009, pp. 126‚Äì130), whereas the fallibility of ‚Äúconditioning‚Äù on a mediator to assess direct effects is demonstrated in Pearl (1998) as well as Cole and Hern√°n (2002).

The analysis of mediation has become extremely active in the past 15 years, primarily due to the advent of counterfactual logic (see Section 4.4.5); a comprehensive account of this progress is given in Vanderweele (2015). A tutorial survey of causal inference in linear systems (Section 3.8), focusing on parameter identi-cation, is provided by Chen and Pear1 (2014). Additional discussion on the confusion of regression versus structural equations can be found in Bollen and Pearl (2013).

A classic, and still the best textbook on the relationships between structural and regession coef-cients is Heise (1975) (available online: http://www.indiana.edu/~socpsy/public_-les/ CausalAnalysis.zip). Other classics are Duncan (1975), Kenny (1979), and Bollen (1989). Classical texts, however, fall short of providing graphical tools of identi-cation, such as those invoking backdoor and $G _ { \alpha }$ (see Study question 3.8.1). A recent exception is Kline (2016).

Introductions to instrumental variables can be found in Greenland (2000) and in many textbooks of econometrics (e.g., Bowden and Turkington 1984, Wooldridge 2013). Generalized instrumental variables, extending the classical de-nition of Section 3.8.3 were introduced in Brito and Pearl (2002).

The program DAGitty (which is available online: http://www.dagitty.net/dags.html), permits users to search the graph for generalized instrumental variables, and reports the resulting IV estimators (Textor et al. 2011).

# 4

# Counterfactuals and Their Applications

# 4.1 Counterfactuals

While driving home last night, I came to a fork in the road, where I had to make a choice: to take the freeway $( X = 1$ ) or go on a surface street named Sepulveda Boulevard $( X = 0$ ). I took Sepulveda, only to -nd out that the traf-c was touch and go. As I arrived home, an hour later, I said to myself: ‚ÄúGee, I should have taken the freeway.‚Äù

What does it mean to say, ‚ÄúI should have taken the freeway‚Äù? Colloquially, it means, ‚ÄúIf I had taken the freeway, I would have gotten home earlier.‚Äù Scienti-cally, it means that my mental estimate of the expected driving time on the freeway, on that same day, under the identical circumstances, and governed by the same idiosyncratic driving habits that I have, would have been lower than my actual driving time.

This kind of statement‚Äîan ‚Äúif‚Äù statement in which the ‚Äúif‚Äù portion is untrue or unrealized‚Äîis known as a counterfactual. The ‚Äúif‚Äù portion of a counterfactual is called the hypothetical condition, or more often, the antecedent. We use counterfactuals to emphasize our wish to compare two outcomes (e.g., driving times) under the exact same conditions, differing only in one aspect: the antecedent, which in our case stands for ‚Äútaking the freeway‚Äù as opposed to the surface street. The fact that we know the outcome of our actual decision is important, because my estimated driving time on the freeway after seeing the consequences of my actual decision (to take Sepulveda) may be totally different from my estimate prior to seeing the consequence. The consequence (1 hour) may provide valuable evidence for the assessment, for example, that the traf-c was particularly heavy on that day, and that it might have been due to a brush -re. My statement ‚ÄúI should have taken the freeway‚Äù conveys the judgment that whatever mechanisms impeded my speed on Sepulveda would not have affected the speed on the freeway to the same extent. My retrospective estimate is that a freeway drive would have taken less than 1 hour, and this estimate is clearly different than my prospective estimate was, when I made the decision prior to seeing the consequences‚Äîotherwise, I would have taken the freeway to begin with.

If we try to express this estimate using do-expressions, we come to an impasse. Writing

$$
E (d r i v i n g \text {t i m e} | d o (\text {f r e w a y}), d r i v i n g \text {t i m e} = 1 h o u r)
$$

leads to a clash between the driving time we wish to estimate and the actual driving time observed. Clearly, to avoid this clash, we must distinguish symbolically between the following two variables:

1. Actual driving time   
2. Hypothetical driving time under freeway conditions when actual surface driving time is known to be 1 hour.

Unfortunately, the do-operator is too crude to make this distinction. While the do-operator allows us to distinguish between two probabilities, P(driving time do(freeway)) and $P ( d r i \nu i n g t i m e | d o ( S e p u l \nu e d a ) )$ , it does not offer us the means of distinguishing between the two variables themselves, one standing for the time on Sepulveda, the other for the hypothetical time on the freeway. We need this distinction in order to let the actual driving time (on Sepulveda) inform our assessment of the hypothetical driving time.

Fortunately, making this distinction is easy; we simply use different subscripts to label the two outcomes. We denote the freeway driving time by $Y _ { X = 1 }$ (or $Y _ { 1 }$ , where context permits) and Sepulveda driving time by $Y _ { X = 0 }$ (or $Y _ { 0 . }$ ). In our case, since $Y _ { 0 }$ is the $Y$ actually observed, the quantity we wish to estimate is

$$
E \left(Y _ {X = 1} \mid X = 0, Y = Y _ {0} = 1\right) \tag {4.1}
$$

The novice student may feel somewhat uncomfortable at the sight of the last expression, which contains an eclectic mixture of three variables: one hypothetical and two observed, with the hypothetical variable $Y _ { X = 1 }$ predicated upon one event $( X = 1$ ) and conditioned upon the conicting event, $X = 0$ , which was actually observed. We have not encountered such a clash before. When we used the $d o$ -operator to predict the effect of interventions, we wrote expressions such as

$$
E [ Y | d o (X = x) ] \tag {4.2}
$$

The Y in this expression is predicated upon the event $X = x$ . With our new notation, the expression might as well have been written $E [ Y _ { X = x } ]$ . But since all variables in this expression were measured in the same world, there is no need to abandon the do-operator and invoke counterfactual notation.

We run into problems with counterfactual expressions like (4.1) because $Y _ { X = 1 } = y$ and $X = 0$ are‚Äîand must be‚Äîevents occurring under different conditions, sometimes referred to as ‚Äúdifferent worlds.‚Äù This problem does not occur in intervention expressions, because Eq. (4.1) seeks to estimate our total drive time in a world where we chose the freeway, given that the actual drive time (in the world where we chose Sepulveda) was 1 hour, whereas Eq. (4.2) seeks to estimate the expected drive time in a world where we chose the freeway, with no reference whatsoever to another world.

In Eq. (4.1), however, the clash prevents us from reducing the expression to a do-expression, which means that it cannot be estimated from interventional experiments. Indeed, a randomized controlled experiment on the two decision options will never get us the estimate we want. Such experiments can give us $E [ Y _ { 1 } ] = E [ Y | d o ( f r e e w a y ) ]$ and $E [ Y _ { 0 } ] = E [ Y | d o ( S e p u l \nu e d a ) ]$ , but the fact that we cannot take both the freeway and Sepulveda simultaneously prohibits us from estimating the quantity we wish to estimate, that is, the conditional expectation $E [ Y _ { 1 } | X = 0$ , $Y = 1 \bar { . }$ ]. One might be tempted to circumvent this dif-culty by measuring the freeway time at a later time, or of another driver, but then conditions may change with time, and the other driver may have different driving habits than I. In either case, the driving time we would be measuring under such surrogates will only be an approximation of the one we set out to estimate, $Y _ { 1 }$ , and the degree of approximation would vary with the assumptions we can make on how similar those surrogate conditions are to my own driving time had I taken the freeway. Such approximations may be appropriate for estimating the target quantity under some circumstances, but they are not appropriate for de-ning it. De-nitions should accurately capture what we wish to estimate, and for this reason, we must resort to a subscript notation, $Y _ { 1 }$ , with the understanding that $Y _ { 1 }$ is my ‚Äúwould-be‚Äù driving time, had I chosen the freeway at that very juncture of history.

Readers will be pleased to know that their discomfort with the clashing nature of Eq. (4.1) will be short-lived. Despite the hypothetical nature of the counterfactual $Y _ { 1 }$ , the structural causal models that we have studied in Part Two of the book will prove capable not only of computing probabilities of counterfactuals for any fully speci-ed model, but also of estimating those probabilities from data, when the underlying functions are not speci-ed or when some of the variables are unmeasured.

In the next section, we detail the methods for computing and estimating properties of counterfactuals. Once we have done that, we‚Äôll use those methods to solve all sorts of complex, seemingly intractable problems. We‚Äôll use counterfactuals to determine the ef-cacy of a job training program by -guring out how many enrollees would have gotten jobs had they not enrolled; to predict the effect of an additive intervention (adding $5 \mathrm { m g } / 1$ of insulin to a group of patients with varying insulin levels) from experimental studies that exercised a uniform intervention (setting the group of patients‚Äô insulin levels to the same constant value); to ascertain the likelihood that an individual cancer patient would have had a different outcome, had she chosen a different treatment; to prove, with a suf-cient probability, whether a company was discriminating when they passed over a job applicant; and to suss out, via analysis of direct and indirect effects, the ef-cacy of gender-blind hiring practices on rectifying gender disparities in the workforce.

All this and more, we can do with counterfactuals. But -rst, we have to learn how to de-ne them, how to compute them, and how to use them in practice.

# 4.2 De-ning and Computing Counterfactuals

# 4.2.1 The Structural Interpretation of Counterfactuals

We saw in the subsection on interventions that structural causal models can be used to predict the effect of actions and policies that have never been implemented before. The action of setting a variable, $X$ , to value $x$ is simulated by replacing the structural equation for $X$ with the equation $X = x$ . In this section, we show that by using the same operation in a slightly different context,

we can use SEMs to de-ne what counterfactuals stand for, how to read counterfactuals from a given model, and how probabilities of counterfactuals can be estimated when portions of the models are unknown.

We begin with a fully speci-ed model M , for which we know both the functions $\{ F \}$ and the values of all exogenous variables. In such a deterministic model, every assignment $U = u$ to the exogenous variables corresponds to a single member of, or ‚Äúunit‚Äù in a population, or to a ‚Äúsituation‚Äù in nature. The reason for this correspondence is as follows: Each assignment $U = u$ uniquely determines the values of all variables in V. Analogously, the characteristics of each individual ‚Äúunit‚Äù in a population have unique values, depending on that individual‚Äôs identity. If the population is ‚Äúpeople,‚Äù these characteristics include salary, address, education, propensity to engage in musical activity, and all other properties we associate with that individual at any given time. If the population is ‚Äúagricultural lots,‚Äù these characteristics include soil content, surrounding climate, and local wildlife, among others. There are so many of these de-ning properties that they cannot all possibly be included in the model, but taken all together, they uniquely distinguish each individual and determine the values of the variables we do include in the model. It is in this sense that every assignment $U = u$ corresponds to a single member or ‚Äúunit‚Äù in a population, or to a ‚Äúsituation‚Äù in nature.

For example, if $U = u$ stands for the de-ning characteristics of an individual named Joe, and $X$ stands for a variable named ‚Äúsalary,‚Äù then $X ( u )$ stands for Joe‚Äôs salary. If $U = u$ stands for the identity of an agricultural lot and $Y$ stands for the yield measured in a given season, then $Y ( u )$ , stands for the yield produced by lot $U = u$ in that season.

Consider now the counterfactual sentence, ‚ÄúY would be $y$ had $X$ been $x$ , in situation $U = u$ ,‚Äù denoted $Y _ { x } ( u ) = y$ , where $Y$ and $X$ are any two variables in $V$ . The key to interpreting such a sentence is to treat the phrase ‚Äúhad $X$ been $x '$ as an instruction to make a minimal modi-cation in the current model so as to establish the antecedent condition $X = x$ , which is likely to conict with the observed value of $X , X ( u )$ . Such a minimal modi-cation amounts to replacing the equation for $X$ with a constant $x _ { \ast }$ , which may be thought of as an external intervention $d o ( X = x )$ , not necessarily by a human experimenter. This replacement permits the constant $x$ to differ from the actual value of $X$ (namely, $X ( u ) ,$ without rendering the system of equations inconsistent, and in this way, it allows all variables, exogenous as well as endogenous, to serve as antecedents to other variables.

We demonstrate this de-nition on a simple causal model consisting of just three variables, $X , Y , U$ , and de-ned by two equations:

$$
X = a U \tag {4.3}
$$

$$
Y = b X + U \tag {4.4}
$$

We -rst compute the counterfactual $Y _ { x } ( u )$ , that is, what $Y$ would be had $X$ been $x$ , in situation $U = u$ . Replacing the -rst equation with $X = x$ gives the ‚Äúmodi-ed‚Äù model $M _ { x }$ :

$$
X = x
$$

$$
Y = b X + U
$$

Substituting $U = u$ and solving for $Y$ gives

$$
Y _ {x} (u) = b x + u
$$

Table 4.1 The values attained by $X ( u ) , Y ( u ) , Y _ { x } ( u )$ , and $X _ { y } ( u )$ in the linear model of Eqs. (4.3) and (4.4)   

<table><tr><td>u</td><td>X(u)</td><td>Y(u)</td><td>Y1(u)</td><td>Y2(u)</td><td>Y3(u)</td><td>X1(u)</td><td>X2(u)</td><td>X3(u)</td></tr><tr><td>1</td><td>1</td><td>2</td><td>2</td><td>3</td><td>4</td><td>1</td><td>1</td><td>1</td></tr><tr><td>2</td><td>2</td><td>4</td><td>3</td><td>4</td><td>5</td><td>2</td><td>2</td><td>2</td></tr><tr><td>3</td><td>3</td><td>6</td><td>4</td><td>5</td><td>6</td><td>3</td><td>3</td><td>3</td></tr></table>

which is expected, since the meaning of the structural equation $Y = b X + U$ is, exactly ‚Äúthe value that Nature assigns to Y must be $U$ plus $b$ times the value assigned to $X$ .‚Äù To demonstrate a less obvious result, let us examine the counterfactual $X _ { y } ( u )$ , that is, what $X$ would be had $Y$ been $y$ in situation $U = u$ . Here, we replace the second equation by the constant $Y = y$ and, solving for $X$ , we get $X _ { y } ( u ) = a u$ , which means that $X$ remains unaltered by the hypothetical condition ‚Äúhad $Y$ been y.‚Äù This should be expected, if we interpret this hypothetical condition as emanating from an external, albeit unspeci-ed, intervention. It is less expected if we do not invoke the intervention metaphor but merely treat $Y = y$ as a spontaneous, unanticipated change. The invariance of $X$ under such a counterfactual condition reects the intuition that hypothesizing future eventualities does not alter the past.

Each SCM encodes within it many such counterfactuals, corresponding to the various values that its variables can take. To illustrate additional counterfactuals generated by this model, let us assume that $U$ can take on three values, 1, 2, and 3, and let $a = b = 1$ in Eqs. (4.3) and (4.4). Table 4.1 gives the values of X(u), Y(u), Yx(u), and $X _ { y } ( u )$ for several levels of $x$ and y. For example, to compute $Y _ { 2 } ( u )$ for $u = 2$ , we simply solve a new set of equations, with $X = 2$ replacing $X = a U$ , and obtain $Y _ { 2 } ( u ) = 2 + u = 4$ . The computation is extremely simple, which goes to show that, while counterfactuals are considered hypothetical, or even mystical from a statistical view point, they emerge quite naturally from our perception of reality, as encoded in structural models. Every structural equation model assigns a de-nitive value to every conceivable counterfactual.

From this example, the reader may get the impression that counterfactuals are no different than ordinary interventions, captured by the do-operator. Note, however, that, in this example we computed not merely the probability or expected value of Y under one intervention or another, but the actual value of Y under the hypothesized new condition $X = x$ . For each situation $U = u$ , we obtained a de-nite number, $Y _ { x } ( u )$ , which stands for that hypothetical value of $Y$ in that situation. The $d o$ -operator, on the other hand, is only de-ned on probability distributions and, after deleting the factor $P ( x _ { i } | p a _ { i } )$ from the product decomposition (Eq. (1.29)), always delivers probabilistic results such as $E [ Y | d o ( x ) ]$ . From an experimentalist perspective, this difference reects a profound gap between population and individual levels of analysis; the $d o ( x )$ -operator captures the behavior of a population under intervention, whereas $Y _ { x } ( u )$ describes the behavior of a speci-c individual, $U = u$ , under such interventions. This difference has far-reaching consequences, and will enable us to de-ne probabilities of concepts such as credit, blame, and regret, which the $d o$ -operator is not able to capture.

# 4.2.2 The Fundamental Law of Counterfactuals

We are now ready to generalize the concept of counterfactuals to any structural model, M. Consider any arbitrary two variables $X$ and $Y$ , not necessarily connected by a single equation.

Let $M _ { x }$ stand for the modi-ed version of $M$ , with the equation of $X$ replaced by $X = x$ . The formal de-nition of the counterfactual $Y _ { x } ( u )$ reads

$$
Y _ {x} (u) = Y _ {M _ {x}} (u) \tag {4.5}
$$

In words: The counterfactual $Y _ { x } ( u )$ in model $M$ is de-ned as the solution for Y in the ‚Äúsurgically modi-ed‚Äù submodel $M _ { x }$ . Equation (4.5) is one of the most fundamental principles of causal inference. It allows us to take our scienti-c conception of reality, $M$ , and use it to generate answers to an enormous number of hypothetical questions of the type ‚ÄùWhat would Y be had $X$ been x?‚Äù The same de-nition is applicable when $X$ and $Y$ are sets of variables, if by $M _ { x }$ we mean a model where the equations of all members of $X$ are replaced by constants. This raises enormously the number of counterfactual sentences computable by a given model and brings up an interesting question: How can a simple model, consisting of just a few equations, assign values to so many counterfactuals? The answer is that the values that these counterfactuals receive are not totally arbitrary, but must cohere with each other to be consistent with an underlying model.

For example, if we observe $X ( u ) = 1$ and $Y ( u ) = 0$ , then $Y _ { X = 1 } ( u )$ must be zero, because setting $X$ to a value it already has, $X ( u )$ , should produce no change in the world. Hence, Y should stay at its current value of $Y ( u ) = 0$ .

In general, counterfactuals obey the following consistency rule:

$$
i f \quad X = x \quad t h e n \quad Y _ {x} = Y \tag {4.6}
$$

If $X$ is binary, then the consistency rule takes the convenient form:

$$
Y = X Y _ {1} + (1 - X) Y _ {0}
$$

which can be interpreted as follows: $Y _ { 1 }$ is equal to the observed value of Y whenever $X$ takes the value one. Symmetrically, $Y _ { 0 }$ is equal to the observed value of $Y$ whenever $X$ is zero. All these constraints are automatically satis-ed if we compute counterfactuals through Eq. (4.5).

# 4.2.3 From Population Data to Individual Behavior‚ÄîAn Illustration

To illustrate the use of counterfactuals in reasoning about the behavior of an individual unit, we refer to the model depicted in Figure 4.1, which represents an ‚Äúencouragement design‚Äù: $X$ represents the amount of time a student spends in an after-school remedial program, $H$ the amount of homework a student does, and Y a student‚Äôs score on the exam. The value of each variable is given as the number of standard deviations above the mean the student falls (i.e.,

![](images/b404ce0fd8939a79a6f0888604cbc3bc74dd060ffb418dd4aee80f9eb018cadf.jpg)  
Figure 4.1 A model depicting the effect of Encouragement $( X )$ on student‚Äôs score

the model is standardized so that all variables have mean 0 and variance 1). For example, if $Y = 1$ , then the student scored 1 standard deviation above the mean on his or her exam. This model represents a randomized pilot program, in which students are assigned to the remedial sessions by the luck of the draw.

# Model 4.1

$$
X = U _ {X}
$$

$$
H = a \cdot X + U _ {H}
$$

$$
Y = b \cdot X + c \cdot H + U _ {Y}
$$

$$
\sigma_ {U _ {i} U _ {j}} = 0 \quad \text {f o r a l l} i, j \in \{X, H, Y \}
$$

We assume that all $U$ factors are independent and that we are given the values for the coef-- cients of Model 4.1 (these can be estimated from population data):

$$
a = 0. 5, \quad b = 0. 7, \quad c = 0. 4
$$

Let us consider a student named Joe, for whom we measure $X = 0 . 5 , H = 1$ , and $Y = 1 . 5$ . Suppose we wish to answer the following query: What would Joe‚Äôs score have been had he doubled his study time?

In a linear SEM, the value of each variable is determined by the coef-cients and the $U$ variables; the latter account for all variation among individuals. As a result, we can use the evidence $X = 0 . 5 , H = 1$ , and $Y = 1 . 5$ to determine the values of the $U$ variables associated with Joe. These values are invariant to hypothetical actions (or ‚Äúmiracles‚Äù) such as those that might cause Joe to double his homework.

In this case, we are able to obtain the speci-c characteristics of Joe from the evidence:

$$
U _ {X} = 0. 5,
$$

$$
U _ {H} = 1 - 0. 5 \cdot 0. 5 = 0. 7 5, \mathrm {a n d}
$$

$$
U _ {Y} = 1. 5 - 0. 7 \cdot 0. 5 - 0. 4 \cdot 1 = 0. 7 5.
$$

Next, we simulate the action of doubling Joe‚Äôs study time by replacing the structural equation for $H$ with the constant $H = 2$ . The modi-ed model is depicted in Figure 4.2. Finally, we compute the value of $Y$ in our modi-ed model using the updated $U$ values, giving

![](images/0fb5912b4b3de36c84bc817e7349431877799c4307f2d2e2b953cbad10c6b6a2.jpg)  
Figure 4.2 Answering a counterfactual question about a speci-c student‚Äôs score, predicated on the assumption that homework would have increased to $H = 2$

$$
\begin{array}{l} Y _ {H = 2} (U _ {X} = 0. 5, U _ {H} = 0. 7 5, U _ {Y} = 0. 7 5) \\ = 0. 5 \cdot 0. 7 + 2. 0 \cdot 0. 4 + 0. 7 5 \\ = 1. 9 0 \\ \end{array}
$$

We thus conclude that Joe‚Äôs score, had he doubled his homework, would have been 1.9 instead of 1.5. This, according to our convention, would mean an increase to 1.9 standard deviations above the mean, instead of the current 1.5.

In summary, we -rst applied the evidence $X = 0 . 5 , H = 1$ , and $Y = 1 . 5$ to update the values for the $U$ variables. We then simulated an external intervention to force the condition $H = 2$ by replacing the structural equation $H = a X + U _ { H }$ with the equation $H = 2$ . Finally, we computed the value of Y given the structural equations and the updated $U$ values. (In all of the above, we, of course, assumed that the $U$ variables are unchanged by the hypothetical intervention on $H$ .)

# 4.2.4 The Three Steps in Computing Counterfactuals

The case of Joe and the after-school program illustrates the way in which the fundamental de-nition of counterfactuals can be turned into a process for obtaining the value of a given counterfactual. There is a three-step process for computing any deterministic counterfactual:

(i) Abduction: Use evidence $E = e$ to determine the value of $U$ .   
(ii) Action: Modify the model, $M$ , by removing the structural equations for the variables in $X$ and replacing them with the appropriate functions $X = x$ , to obtain the modi-ed model, $M _ { x }$ .   
(iii) Prediction: Use the modi-ed model, $M _ { x }$ , and the value of $U$ to compute the value of $Y$ the consequence of the counterfactual.

In temporal metaphors, Step (i) explains the past $( U )$ in light of the current evidence $e$ ; Step (ii) bends the course of history (minimally) to comply with the hypothetical antecedent $X = x$ ; -nally, Step (iii) predicts the future (Y) based on our new understanding of the past and our newly established condition, $X = x$ .

This process will solve any deterministic counterfactual, that is, counterfactuals pertaining to a single unit of the population in which we know the value of every relevant variable. Structural equation models are able to answer counterfactual queries of this nature because each equation represents the mechanism by which a variable obtains its values. If we know these mechanisms, we should also be able to predict what values would be obtained had some of these mechanisms been altered, given the alterations. As a result, it is natural to view counterfactuals as derived properties of structural equations. (In some frameworks, counterfactuals are taken as primitives (Holland 1986; Rubin 1974).)

But counterfactuals can also be probabilistic, pertaining to a class of units within the population; for instance, in the after-school program example, we might want to know what would have happened if all students for whom $Y < 2$ had doubled their homework time. These probabilistic counterfactuals differ from do-operator interventions because, like their deterministic counterparts, they restrict the set of individuals intervened upon, which do-expressions cannot do.

We can now advance from deterministic to probabilistic models, so we can deal with questions about probabilities and expectations of counterfactuals. For example, suppose Joe is a student participating in the study of Figure 4.1, who scored $Y = y$ in the exam. What is the probability that Joe‚Äôs score would be $Y = y ^ { \prime }$ had he had -ve more hours of encouragement

training? Or, what would his expected score be in such hypothetical world? Unlike in the example of Model 4.1, we now do not have information on all three variables, $\{ X , Y , H \}$ , and we cannot therefore determine uniquely the value $u$ that pertains to Joe. Instead, Joe may belong to a large class of units compatible with the evidence available, each having a different value of $u$ .

Nondeterminism enters causal models by assigning probabilities $P ( U = u )$ over the exogenous variables $U$ . These represent our uncertainty as to the identity of the subject under consideration or, when the subject is known, what other characteristics that subject has that might have bearing on our problem.

The exogenous probability $P ( U = u )$ induces a unique probability distribution on the endogenous variables $V , P ( \nu )$ , with the help of which we can de-ne and compute not only the probability of any single counterfactual, $Y _ { x } = y$ , but also the joint distributions of all combinations of observed and counterfactual variables. For example, we can determine $P ( Y _ { x } = y , Z _ { w } = z , X = x ^ { \prime } )$ , where $X , Y , Z$ , and $W$ are arbitrary variables in a model. Such joint probabilities refer to the proportion of individuals $u$ in the population for which all the events in the parentheses are true, namely, $Y _ { x } ( u ) = y$ and $Z _ { w } ( u ) = z$ and $X ( u ) = x ^ { \prime }$ , allowing, in particular, $w$ or $x ^ { \prime }$ to conict with $x$ .

A typical query about these probabilities asks, ‚ÄúGiven that we observe feature $E = e$ for a given individual, what would we expect the value of $Y$ for that individual to be if $X$ had been x?‚Äù This expectation is denoted ${ \cal E } [ Y _ { X = x } | { \cal E } = e ]$ , where we allow $E = e$ to conict with the antecedent $X = x$ . $E = e$ after the conditioning bar represents all information (or evidence) we might have about the individual, potentially including the values of $X$ , Y, or any other variable, as we have seen in Eq. (4.1). The subscript $X = x$ represents the antecedent speci-ed by the counterfactual sentence.

The speci-cs of how these probabilities and expectations are dealt with will be examined in the following sections, but for now, it is important to know that using them, we can generalize our three-step process to any probabilistic nonlinear system.

Given an arbitrary counterfactual of the form, $E [ Y _ { X = x } | E = e ]$ , the three-step process reads:

(i) Abduction: Update $P ( U )$ by the evidence to obtain $P ( U | E = e )$ .   
(ii) Action: Modify the model, $M$ , by removing the structural equations for the variables in $X$ and replacing them with the appropriate functions $X = x$ , to obtain the modi-ed model, $M _ { x }$ .   
(iii) Prediction: Use the modi-ed model, $M _ { x }$ , and the updated probabilities over the $U$ variables, $P ( U | E = e )$ , to compute the expectation of $Y$ , the consequence of the counterfactual.

We shall see in Section 4.4 that the above probabilistic procedure applies not only to retrospective counterfactual queries (queries of the form ‚ÄúWhat would have been the value of $Y$ had $X$ been $x ? ^ { \prime }$ ) but also to certain kinds of intervention queries. In particular, it applies when we make every individual take an action that depends on the current value of his/her $X$ . A typical example would be ‚Äúadditive intervention‚Äù: for example, adding $5 \mathrm { m g } / 1$ of insulin to every patient‚Äôs regiment, regardless of their previous dosage. Since the -nal level of insulin varies from patient to patient, this policy cannot be represented in $d o$ -notation.

For another example, suppose we wish to estimate, using Figure 4.1, the effect on test score provided by a school policy that sends students who are lazy on their homework $( H \leq H _ { 0 } )$ t o attend the after-school program for $X = 1$ . We can‚Äôt simply intervene on $X$ to set it equal to 1 in cases where $H$ is low, because in our model, $X$ is one of the causes of $H$ .

Instead, we express the expected value of this quantity in counterfactual notation as $E [ Y _ { X = 1 } | H \leq H _ { 0 } ]$ ,which can, in principle, be computed using the above three-step method. Counterfactual reasoning and the above procedure are necessary for estimating the effect of actions and policies on subsets of the population characterized by features that, in themselves, are affected by the policy (e.g., $H \leq H _ { 0 }$ ).

# 4.3 Nondeterministic Counterfactuals

# 4.3.1 Probabilities of Counterfactuals

To examine how nondeterminism is reected in the calculation of counterfactuals, let us assign probabilities to the values of $U$ in the model of Eqs. (4.3) and (4.4). Imagine that $U = \{ 1 , 2 , 3 \}$ represents three types of individuals in a population, occurring with probabilities

$$
P (U = 1) = \frac {1}{2}, P (U = 2) = \frac {1}{3}, \quad \text {a n d} \quad P (U = 3) = \frac {1}{6}
$$

All individuals within a population type have the same values of the counterfactuals, as speci--ed by the corresponding rows in Table 4.1. With these values, we can compute the probability that the counterfactuals will satisfy a speci-ed condition. For instance, we can compute the proportion of units for which Y would be 3 had $X$ been 2, or $Y _ { 2 } ( u ) = 3$ . This condition occurs only in the -rst row of the table and, since it is a property of $U = 1$ , we conclude that it will occur with probability $\frac { 1 } { 2 }$ , giving $\begin{array} { r } { P ( Y _ { 2 } = 3 ) = \frac { 1 } { 2 } } \end{array}$ . We can similarly compute the probability of any counterfactual statement, for example, $\begin{array} { r } { { P ( \tilde { Y _ { 1 } } = 4 ) = \frac { 1 } { 6 } , P ( Y _ { 1 } = 3 ) = \frac { 1 } { 3 } , P ( Y _ { 2 } > 3 ) = \frac { 1 } { 2 } } } \end{array}$ , and so on. What is remarkable, however, is that we can also compute joint probabilities of every combination of counterfactual and observable events. For example,

$$
P (Y _ {2} > 3, Y _ {1} <   4) = \frac {1}{3}
$$

$$
P (Y _ {1} <   4, Y - X > 1) = \frac {1}{3}
$$

$$
P \left(Y _ {1} <   Y _ {2}\right) = 1
$$

In the -rst of these expressions, we -nd a joint probability of two events occurring in two different worlds; the -rst $Y _ { 2 } > 3$ in an $X = 2$ world, and the second $Y _ { 1 } < 4$ , in $X = 1$ . The probability of their conjunction evaluates to $\frac 1 3$ because the two events co-occur only at $U = 2$ , which was assigned a probability of $\frac 1 3$ . Other cross-world events appear in the second and third expressions. Remarkably (and usefully), this clash between the worlds provides no barrier to calculation. In fact, cross-world probabilities are as simple to derive as intra-world ones: We simply identify the rows in which the speci-ed combination is true and sum up the probabilities assigned to those rows. This immediately gives us the capability of computing conditional probabilities among counterfactuals and de-ning notions such as dependence and conditional independence among counterfactuals, as we did in Chapter 1 when we dealt with observable variables. For instance, it is easy to verify that, among individuals for which Y is greater than 2, the probability is $\frac { 2 } { 3 }$ that Y would increase if $X$ were 3. (Because $\begin{array} { r } { P ( Y _ { 3 } > Y | Y > 2 ) = \frac { 1 } { 3 } / \frac { 1 } { 2 } = \frac { 2 } { 3 } . } \end{array}$ ) Similarly, we can verify that the difference $Y _ { x + 1 } - Y _ { x }$ is independent of $x$ , which means that the

causal effect of $X$ on Y does not vary across population types, a property shared by all linear models.

Such joint probabilities over multiple-world counterfactuals can easily be expressed using the subscript notation, as in $P ( Y _ { 1 } = y _ { 1 } , Y _ { 2 } = y _ { 2 } )$ , and can be computed from any structural model as we did in Table 4.1. They cannot however be expressed using the $d o ( x )$ notation, because the latter delivers just one probability for each intervention $X = x$ . To see the rami-- cations of this limitation, let us examine a slight modi-cation of the model in Eqs. (4.3) and (4.4), in which a third variable $Z$ acts as mediator between $X$ and Y. The new model‚Äôs equations are given by

$$
X = U _ {1} \quad Z = a X + U _ {2}, Y = b Z \tag {4.7}
$$

and its structure is depicted in Figure 4.3. To cast this model in a context, let $X = 1$ stand for having a college education, $U _ { 2 } = 1$ for having professional experience, $Z$ for the level of skill needed for a given job, and Y for salary.

Suppose our aim is to compute $E [ Y _ { X = 1 } | Z = 1 ]$ , which stands for the expected salary of individuals with skill level $Z = 1$ , had they received a college education. This quantity cannot be captured by a do-expression, because the condition $Z = 1$ and the antecedent $X = 1$ refer to two different worlds; the former represents current skills, whereas the latter represents a hypothetical education in an unrealized past. An attempt to capture this hypothetical salary using the expression $E [ Y | d o ( X = 1 ) , Z = 1 ]$ would not reveal the desired information. The do-expression stands for the expected salary of individuals who all -nished college and have since acquired skill level $Z = 1$ . The salaries of these individuals, as the graph shows, depend only on their skill, and are not affected by whether they obtained the skill through college or through work experience. Conditioning on $Z = 1$ , in this case, cuts off the effect of the intervention that we‚Äôre interested in. In contrast, some of those who currently have $Z = 1$ might not have gone to college and would have attained higher skill (and salary) had they gotten college education. Their salaries are of great interest to us, but they are not included in the do-expression. Thus, in general, the do-expression will not capture our counterfactual question:

$$
E [ Y | d o (X = 1), Z = 1 ] \neq E \left[ Y _ {X = 1} \mid Z = 1 \right] \tag {4.8}
$$

We can further con-rm this inequality by noting that, while $E [ Y | d o ( X = 1 ) , Z = 1 ]$ is equal to $E [ Y | d o ( X = 0 ) , Z = 1 ] , E [ Y _ { X = 1 } | Z = 1 ]$ is not equal to $E [ Y _ { X = 0 } | Z = 1 ]$ ; the formers treat $Z = 1$ as a postintervention condition that prevails for two different sets of units under the two antecedents, whereas the latters treat it as de-ning one set of units in the current world that would react differently under the two antecedents. The $d o ( x )$ notation cannot capture the latters because the events $X = 1$ and $Z = 1$ in the expression $E [ Y _ { X = 1 } | Z = 1 ]$ refer to two different worlds, pre- and postintervention, respectively. The expression $E [ Y | d o ( X = 1 ) , Z = 1 ]$

![](images/d925853615867148afcff2daefd54a5145627db98ff8a970a3abc20d55bf71d5.jpg)  
Figure 4.3 A model representing Eq. (4.7), illustrating the causal relations between college education (X), skills (Z), and salary (Y)

on the other hand, invokes only postintervention events, and that is why it is expressible in $d o ( x )$ notation.

A natural question to ask is whether counterfactual notation can capture the postintervention, single-world expression $E [ Y | d o ( X = 1 ) , Z = 1 ]$ . The answer is af-rmative; being more exible, counterfactuals can capture both single-world and cross-world probabilities. The translation of $E [ Y | d o ( X = 1 ) , Z = 1 ]$ into counterfactual notation is simply $E [ Y _ { X = 1 } | Z _ { X = 1 } = 1 ]$ , which explicitly designates the event $Z = 1$ as postintervention. The variable $Z _ { X = 1 }$ stands for the value that $Z$ would attain had $X$ been 1, and this is precisely what we mean when we put $Z = z$ in a $d o$ -expression by Bayes‚Äô rule:

$$
P (Y = y | d o (X = 1), Z = z) = \frac {P (Y = y , Z = z | d o (X = 1))}{P (Z = z | d o (X = 1))}
$$

This shows explicitly how the dependence of $Z$ on $X$ should be treated. In the special case where $Z$ is a preintervention variable, as age was in our discussion of conditional interventions (Section 3.5) we have $Z _ { X = 1 } = Z$ , and we need not distinguish between the two. The inequality in (4.8) then turns into an equality.

Let‚Äôs look at how this logic is reected in the numbers. Table 4.2 depicts the counterfactuals associated with the model of (4.7), with all subscripts denoting the state of X. It was constructed by the same method we used in constructing Table 4.1: replacing the equation $X = u$ with the appropriate constant (zero or one) and solving for Y and Z. Using this table, we can verify immediately that (see footnote 2)

$$
E \left[ Y _ {1} \mid Z = 1 \right] = (a + 1) b \tag {4.9}
$$

$$
E \left[ Y _ {0} \mid Z = 1 \right] = b \tag {4.10}
$$

$$
E [ Y | d o (X = 1), Z = 1 ] = b \tag {4.11}
$$

$$
E [ Y | d o (X = 0), Z = 1 ] = b \tag {4.12}
$$

These equations provide numerical con-rmation of the inequality in (4.8). They also demonstrate a peculiar property of counterfactual conditioning that we have noted before: Despite the fact that $Z$ separates $X$ from Y in the graph of Figure 4.3, we -nd that $X$ has an effect on Y for those units falling under $Z = 1$ :

$$
E [ Y _ {1} - Y _ {0} | Z = 1 ] = a b \neq 0
$$

The reason for this behavior is best explained in the context of our salary example. While the salary of those who have acquired skill level $Z = 1$ depends only on their skill, not on $X$ , the

Table 4.2 The values attained by $X ( u ) , Y ( u ) , Z ( u ) , Y _ { 0 } ( u ) , Y _ { 1 } ( u ) , Z _ { 0 } ( u )$ , and $Z _ { 1 } ( u )$ in the model of Eq. (4.7)   

<table><tr><td colspan="8">X=u1Z=aX+u2Y=bZ</td></tr><tr><td>u1</td><td>u2</td><td>X(u)</td><td>Z(u)</td><td>Y(u)</td><td>Y0(u)</td><td>Y1(u)</td><td>Z0(u)</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>ab</td><td>0</td></tr><tr><td>0</td><td>1</td><td>0</td><td>1</td><td>b</td><td>b</td><td>(a+1)b</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td><td>a</td><td>ab</td><td>0</td><td>ab</td><td>0</td></tr><tr><td>1</td><td>1</td><td>1</td><td>a+1</td><td>(a+1)b</td><td>b</td><td>(a+1)b</td><td>1</td></tr></table>

salary of those who are currently at $Z = 1$ would have been different had they had a different past. Retrospective reasoning of this sort, concerning dependence on the unrealized past, is not shown explicitly in the graph of Figure 4.3. To facilitate such reasoning, we need to devise means of representing counterfactual variables directly in the graph; we provide such representations in Section 4.3.2.

Thus far, the relative magnitudes of the probabilities of $P ( u _ { 1 } )$ and $P ( u _ { 2 } )$ have not entered into the calculations, because the condition $Z = 1$ occurs only for $u _ { 1 } = 0$ and $u _ { 2 } = 1$ (assuming that $a \neq 0$ and $a \neq 1$ ), and under these conditions, each of $Y , Y _ { 1 }$ , and $Y _ { 0 }$ has a de-nite value. These probabilities play a role, however, if we assume $a = 1$ in the model, since $Z = 1$ can now occur under two conditions: $( u _ { 1 } = 0 , u _ { 2 } = 1 )$ ) and $( u _ { 1 } = 1 , u _ { 2 } = 0 )$ ). The -rst occurs with probability $P ( u _ { 1 } = 0 ) P ( u _ { 2 } = 1 )$ and the second with probability $P ( u _ { 1 } = 1 ) P ( u _ { 2 } = 0 )$ . In such a case, we obtain

$$
E \left[ Y _ {X = 1} \mid Z = 1 \right] = b \left(1 + \frac {P \left(u _ {1} = 0\right) P \left(u _ {2} = 1\right)}{P \left(u _ {1} = 0\right) P \left(u _ {2} = 1\right) + P \left(u _ {1} = 1\right) P \left(u _ {2} = 0\right)}\right) \tag {4.13}
$$

$$
E [ Y _ {X = 0} | Z = 1 ] = b \left(\frac {P (u _ {1} = 0) P (u _ {2} = 1)}{P (u _ {1} = 0) P (u _ {2} = 1) + P (u _ {1} = 1) P (u _ {2} = 0)}\right) \tag {4.14}
$$

The fact that the -rst expression is larger than the second demonstrates again that the skill-speci-c causal effect of education on salary is nonzero, despite the fact that salaries are determined by skill only, not by education. This is to be expected, since a nonzero fraction of the workers at skill level $Z = 1$ did not receive college education, and, had they been given college education, their skill would have increased to $Z _ { 1 } = 2$ , and their salaries to $2 b$ .

# Study question 4.3.1

Consider the model in Figure 4.3 and assume that $U _ { 1 }$ and $U _ { 2 }$ are two independent Gaussian variables, each with zero mean and unit variance.

(a) Find the expected salary of workers at skill level $Z = z$ had they received $x$ years of college education. [Hint: Use Theorem 4.3.2, with $e : Z = z ,$ , and the fact that for any two Gaussian variables, say $X$ and $Z$ , we have $E [ X | Z = z ] = E [ X ] + R _ { X Z } \left( z - E [ Z ] \right)$ . Use the material in Sections 3.8.2 and 3.8.3 to express all regression coef-cients in terms of structural parameters, and show that $E [ Y _ { x } | Z = z ] = a b x + b z / ( 1 + a ^ { 2 } ) .$ ]   
(b) Based on the solution for (a), show that the skill-speci-c effect of education on salary is independent of the skill level.

# 4.3.2 The Graphical Representation of Counterfactuals

Since counterfactuals are byproducts of structural equation models, a natural question to ask is whether we can see them in the causal graphs associated with those models. The answer is af-rmative, as can be seen from the fundamental law of counterfactuals, Eq. (4.5). This law tells us that if we modify model $M$ to obtain the submodel $M _ { x }$ , then the outcome variable Y in the modi-ed model is the counterfactual $Y _ { x }$ of the original model. Since modi-cation calls for removing all arrows entering the variable $X$ , as illustrated in Figure 4.4, we conclude that the node associated with the Y variable serves as a surrogate for $Y _ { x }$ , with the understanding that the substitution is valid only under the modi-cation.

![](images/79edbdd043ea90e06ef74ac5fe42bfa85fb8d2bf9fa1222f9964d4de638c8cf4.jpg)  
(a)

![](images/8de4500646ad37fffc346595c43675c9d75f01c9add8db055a0cbce9ed90c223.jpg)  
(b)   
Figure 4.4 Illustrating the graphical reading of counterfactuals. (a) The original model. (b) The modi--ed model $M _ { x }$ in which the node labeled $Y _ { x }$ represents the potential outcome $Y$ predicated on $X = x$

This temporary visualization of counterfactuals is suf-cient to answer some fundamental questions about the statistical properties of $Y _ { x }$ and how those properties depend on other variables in the model, speci-cally when those other variables are conditioned on.

When we ask about the statistical properties of $Y _ { x } ,$ we need to examine what would cause $Y _ { x }$ to vary. According to its structural definition, $Y _ { x }$ represents the value of Y under a condition where $X$ is held constant at $X = x$ . Statistical variations of $Y _ { x }$ are therefore governed by all exogenous variables capable of influencing Y when $X$ is held constant, that is, when the arrows entering $X$ are removed, as in Figure 4.4(b). Under such conditions, the set of variables capable of transmitting variations to $Y$ are the parents of $Y$ (observed and unobserved), as well as parents of nodes on the pathways between $X$ and Y. In Figure 4.4(b), for example, these parents are $\{ Z _ { 3 } , W _ { 2 } , U _ { 3 } , U _ { Y } \}$ , where $U _ { Y }$ and $U _ { 3 }$ , the error terms of Y and $W _ { 3 }$ , are not shown in the diagram. (These variables remain the same in both models.) Any set of variables that blocks a path to these parents also blocks that path to $Y _ { x } ,$ and will result in, therefore, a conditional independence for $Y _ { x } .$ . In particular, if we have a set $Z$ of covariates that satisfies the backdoor criterion in $M$ (see Definition 3.3.1), that set also blocks all paths between $X$ and those parents, and consequently, it renders $X$ and $Y _ { x }$ independent in every stratum $Z = z$ .

These considerations are summarized formally in Theorem 4.3.1.

Theorem 4.3.1 (Counterfactual Interpretation of Backdoor) If a set $Z$ of variables satis-es the backdoor condition relative to $( X , Y ) ,$ , then, for all $x ,$ , the counterfactual $Y _ { x }$ is conditionally independent of X given Z

$$
P \left(Y _ {x} \mid X, Z\right) = P \left(Y _ {x} \mid Z\right) \tag {4.15}
$$

Theorem 4.3.1 has far-reaching consequences when it comes to estimating the probabilities of counterfactuals from observational studies. In particular, it implies that $P ( Y _ { x } = y )$ is identifiable by the adjustment formula of Eq. (3.5). To prove this, we condition on $Z$ (as in Eq. (1.9)) and write

$$
\begin{array}{l} P (Y _ {x} = y) = \sum_ {z} P (Y _ {x} = y | Z = z) P (z) \\ = \sum_ {z} P (Y _ {x} = y | Z = z, X = x) P (z) \\ = \sum_ {z} P (Y = y \mid Z = z, X = x) P (z) \tag {4.16} \\ \end{array}
$$

The second line was licensed by Theorem 4.3.1, whereas the third line was licensed by the consistency rule (4.6).

The fact that we obtained the familiar adjustment formula in Eq. (4.16) is not really surprising, because this same formula was derived in Section 3.2 (Eq. (3.4)), for $P ( Y = y | d o ( x ) )$ , and we know that $P ( Y _ { x } = y )$ is just another way of writing $P ( Y = y | d o ( x ) )$ . Interestingly, this derivation invokes only algebraic steps; it makes no reference to the model once we ensure that $Z$ satis-es the backdoor criterion. Equation (4.15), which converts this graphical reality into algebraic notation, and allows us to derive (4.16), is sometimes called ‚Äúconditional ignorability‚Äù; Theorem 4.3.1 gives this notion a scienti-c interpretation and permits us to test whether it holds in any given model.

Having a graphical representation for counterfactuals, we can resolve the dilemma we faced in Section 4.3.1 (Figure 4.3), and explain graphically why a stronger education $( X )$ would have had an effect on the salary $( Y )$ of people who are currently at skill level $Z = z$ , despite the fact that, according to the model, salary is determined by skill only. Formally, to determine if the effect of education on salary $( Y _ { x } )$ is statistically independent of the level of education, we need to locate $Y _ { x }$ in the graph and see if it is $d \mathbf { \cdot }$ -separated from $X$ given $Z$ . Referring to Figure 4.3, we see that $Y _ { x }$ can be identi-ed with $U _ { 2 }$ , the only parent of nodes on the causal path from $X$ to Y (and therefore, the only variable that produces variations in $Y _ { x }$ while $X$ is held constant). A quick inspection of Figure 4.3 tells us that $Z$ acts as a collider between $X$ and $U _ { 2 }$ , and, therefore, $X$ and $U _ { 2 }$ (and similarly $X$ and $Y _ { x } .$ ) are not $d$ -separated given $Z$ . We conclude therefore

$$
E \left[ Y _ {x} | X, Z \right] \neq E \left[ Y _ {x} | Z \right]
$$

despite the fact that

$$
E [ Y | X, Z ] = E [ Y | Z ]
$$

In Study question 4.3.1, we evaluate these counterfactual expectations explicitly, assuming a linear Gaussian model. The graphical representation established in this section permits us to determine independencies among counterfactuals by graphical means, without assuming linearity or any speci-c parametric form. This is one of the tools that modern causal analysis has introduced to statistics, and, as we have seen in the analysis of the education‚Äìskill‚Äìsalary story, it takes a task that is extremely hard to solve by unaided intuition and reduces it to simple operations on graphs. Additional methods of visualizing counterfactual dependencies, called ‚Äútwin networks,‚Äù are discussed in (Pearl 2000, pp. 213‚Äì215).

# 4.3.3 Counterfactuals in Experimental Settings

Having convinced ourselves that every counterfactual question can be answered from a fully speci-ed structural model, we next move to the experimental setting, where a model is not available, and the experimenter must answer interventional questions on the basis of a -nite sample of observed individuals. Let us refer back to the ‚Äúencouragement design‚Äù model of Figure 4.1, in which we analyzed the behavior of an individual named Joe, and assume that the experimenter observes a set of 10 individuals, with Joe being participant 1. Each individual is characterized by a distinct vector $U _ { i } = ( U _ { X } , U _ { H } , U _ { Y } )$ , as shown in the -rst three columns of Table 4.3.

Table 4.3 Potential and observed outcomes predicted by the structural model of Figure 4.1 units were selected at random, with each $U _ { i }$ uniformly distributed over [0, 1]   

<table><tr><td rowspan="2">Participant</td><td colspan="3">Participant characteristics</td><td colspan="3">Observed behavior</td><td colspan="5">Predicted potential outcomes</td></tr><tr><td>\(U_X\)</td><td>\(U_H\)</td><td>\(U_Y\)</td><td>X</td><td>Y</td><td>H</td><td>\(Y_0\)</td><td>\(Y_1\)</td><td>\(H_0\)</td><td>\(H_1\)</td><td>\(Y_{00}\cdots\)</td></tr><tr><td>1</td><td>0.5</td><td>0.75</td><td>0.75</td><td>0.5</td><td>1.50</td><td>1.0</td><td>1.05</td><td>1.95</td><td>0.75</td><td>1.25</td><td>0.75</td></tr><tr><td>2</td><td>0.3</td><td>0.1</td><td>0.4</td><td>0.3</td><td>0.71</td><td>0.25</td><td>0.44</td><td>1.34</td><td>0.1</td><td>0.6</td><td>0.4</td></tr><tr><td>3</td><td>0.5</td><td>0.9</td><td>0.2</td><td>0.5</td><td>1.01</td><td>1.15</td><td>0.56</td><td>1.46</td><td>0.9</td><td>1.4</td><td>0.2</td></tr><tr><td>4</td><td>0.6</td><td>0.5</td><td>0.3</td><td>0.6</td><td>1.04</td><td>0.8</td><td>0.50</td><td>1.40</td><td>0.5</td><td>1.0</td><td>0.3</td></tr><tr><td>5</td><td>0.5</td><td>0.8</td><td>0.9</td><td>0.5</td><td>1.67</td><td>1.05</td><td>1.22</td><td>2.12</td><td>0.8</td><td>1.3</td><td>0.9</td></tr><tr><td>6</td><td>0.7</td><td>0.9</td><td>0.3</td><td>0.7</td><td>1.29</td><td>1.25</td><td>0.66</td><td>1.56</td><td>0.9</td><td>1.4</td><td>0.3</td></tr><tr><td>7</td><td>0.2</td><td>0.3</td><td>0.8</td><td>0.2</td><td>1.10</td><td>0.4</td><td>0.92</td><td>1.82</td><td>0.3</td><td>0.8</td><td>0.8</td></tr><tr><td>8</td><td>0.4</td><td>0.6</td><td>0.2</td><td>0.4</td><td>0.80</td><td>0.8</td><td>0.44</td><td>1.34</td><td>0.6</td><td>1.1</td><td>0.2</td></tr><tr><td>9</td><td>0.6</td><td>0.4</td><td>0.3</td><td>0.6</td><td>1.00</td><td>0.7</td><td>0.46</td><td>1.36</td><td>0.4</td><td>0.9</td><td>0.3</td></tr><tr><td>10</td><td>0.3</td><td>0.8</td><td>0.3</td><td>0.3</td><td>0.89</td><td>0.95</td><td>0.62</td><td>1.52</td><td>0.8</td><td>1.3</td><td>0.3</td></tr></table>

Using this information, we can create a full data set that complies with the model. For each triplet $( U _ { X } , U _ { H } , U _ { Y } )$ , the model of Figure 4.1 enables us to complete a full row of the table, including $Y _ { 0 }$ and $Y _ { 1 }$ , which stand for the potential outcomes under treatment $( X = 1$ ) and control $( X = 0$ ) conditions, respectively. We see that the structural model in Figure 4.1 encodes in effect a synthetic population of individuals together with their predicted behavior under both observational and experimental conditions. The columns labeled $X , Y , H$ predict the results of observational studies, and those labeled $Y _ { 0 } , Y _ { 1 } , H _ { 0 } , H _ { 1 }$ predict the hypothetical outcome under two treatment regimes, $X = 0$ , and $X = 1$ . Many more, in fact in-nite, potential outcomes may be predicted; for example, $Y _ { X = 0 . 5 , Z = 2 . 0 }$ as computed for Joe from Figure 4.2, as well as all combinations of subscripted variables. From this synthetic population, one can estimate the probability of every counterfactual query on variables $X , Y , H$ , assuming, of course, that we are in possession of all entries of the table. The estimation would require us to simply count the proportion of individuals that satisfy the speci-ed q uery a s d emonstrated in Section 4.3.1.

Needless to say, the information conveyed by Table 4.3 is not available to us in either observational or experimental studies. This information was deduced from a parametric model such as the one in Figure 4.2, from which we could infer the defining characteristics $\{ U _ { X } , U _ { H } , U _ { Y } \}$ of each participant, given the observations $\{ X , H , Y \}$ . In general, in the absence of a parametric model, there is very little we learn about the potential outcomes $Y _ { 1 }$ and $Y _ { 0 }$ of individual participants, when all we have is their observed behavior $\{ X , H , Y \}$ . Theoretically, the only connection we have between the counterfactuals $\{ Y _ { 1 } , Y _ { 0 } \}$ and the observables $\{ X , H , Y \}$ is the consistency rule of Eq. (4.6), which informs us that $Y _ { 1 }$ must be equal to Y in case $X = 1$ and $Y _ { 0 }$ must be equal to $Y$ in case $X = 0$ . But aside from this tenuous connection, most of the counterfactuals associated with the individual participants will remain unobserved.

Fortunately, there is much we can learn about those counterfactuals at the population level, such as estimating their probabilities or expectation. This we have witnessed already through the adjustment formula of (4.16), where we were able to compute $E ( Y _ { 1 } - Y _ { 0 } )$ using the graph

alone, instead of a complete model. Much more can be obtained from experimental studies, where even the graph becomes dispensable.

Assume that we have no information whatsoever about the underlying model. All we have are measurements on Y taken in an experimental study in which $X$ is randomized over two levels, $X = 0$ and $X = 1$ .

Table 4.4 describes the responses of the same 10 participants (Joe being participant 1) under such experimental conditions, with participants 1, 5, 6, 8, and 10 assigned to $X = 0$ , and the rest to $X = 1$ . The -rst two columns give the true potential outcomes (taken from Table 4.3), while the last two columns describe the information available to the experimenter, where a square indicates that the response was not observed. Clearly, $Y _ { 0 }$ is observed only for participants assigned to $X = 0$ and, similarly, $Y _ { 1 }$ is observed only for those assigned to $X = 1$ . Randomization assures us that, although half of the potential outcomes are not observed, the difference between the observed means in the treatment and control groups will converge to the difference of the population averages, $E ( Y _ { 1 } - Y _ { 0 } ) = 0 . 9$ . This is because randomization distributes the black squares at random along the two rightmost columns of Table 4.4, independent of the actual values of $Y _ { 0 }$ and $Y _ { 1 }$ , so as the number of samples increases, the sample means converge to the population means.

This unique and important property of randomized experiments is not new to us, since randomization, like interventions, renders $X$ independent of any variable that may affect $Y$ (as in Figure 4.4(b)). Under such conditions, the adjustment formula (4.16) is applicable with $Z = \{ \ \}$ , yielding $E [ Y _ { x } ] = E [ Y | X = x ]$ , where $x = 1$ represents treated units and $x = 0$ untreated. Table 4.4 helps us understand what is actually computed when we take sample averages in experimental settings and how those averages are related to the underlying counterfactuals, $Y _ { 1 }$ and $Y _ { 0 }$ .

Table 4.4 Potential and observed outcomes in a randomized clinical trial with $X$ randomized over $X = 0$ and $X = 1$   

<table><tr><td rowspan="2">Participant</td><td colspan="2">Predicted potential outcomes</td><td colspan="2">Observed outcomes</td></tr><tr><td>Y0</td><td>Y1</td><td>Y0</td><td>Y1</td></tr><tr><td>1</td><td>1.05</td><td>1.95</td><td>1.05</td><td>‚ñ†</td></tr><tr><td>2</td><td>0.44</td><td>1.34</td><td>‚ñ†</td><td>1.34</td></tr><tr><td>3</td><td>0.56</td><td>1.46</td><td>‚ñ†</td><td>1.46</td></tr><tr><td>4</td><td>0.50</td><td>1.40</td><td>‚ñ†</td><td>1.40</td></tr><tr><td>5</td><td>1.22</td><td>2.12</td><td>1.22</td><td>‚ñ†</td></tr><tr><td>6</td><td>0.66</td><td>1.56</td><td>0.66</td><td>‚ñ†</td></tr><tr><td>7</td><td>0.92</td><td>1.82</td><td>‚ñ†</td><td>1.82</td></tr><tr><td>8</td><td>0.44</td><td>1.34</td><td>0.44</td><td>‚ñ†</td></tr><tr><td>9</td><td>0.46</td><td>1.36</td><td>‚ñ†</td><td>1.36</td></tr><tr><td>10</td><td>0.62</td><td>1.52</td><td>0.62</td><td>‚ñ†</td></tr><tr><td></td><td colspan="2">True average treatment effect: 0.90</td><td colspan="2">Study average treatment effect: 0.68</td></tr></table>

# 4.3.4 Counterfactuals in Linear Models

In nonparametric models, counterfactual quantities of the form $E [ Y _ { X = x } | Z = z ]$ may not be identi-able, even if we have the luxury of running experiments. In fully linear models, however, things are much easier; any counterfactual quantity is identi-able whenever the model parameters are identi-ed. This is because the parameters fully de-ne the model‚Äôs functions, and as we have seen earlier, once the functions are given, counterfactuals are computable using Eq. (4.5). Since every model parameter is identi-able from interventional studies using the interventional de-nition of direct effects, we conclude that in linear models, every counterfactual is experimentally identi-able. The question remains whether counterfactuals can be identi-ed in observational studies, when some of the model parameters are not identi-ed. It turns out that any counterfactual of the form ${ \cal E } [ Y _ { X = x } | Z = e ]$ , with $e$ an arbitrary set of evidence, is identi-ed whenever $E [ Y | d o ( X = x ) ]$ is identi-ed (Pearl 2000, p. 389). The relation between the two is summarized in Theorem 4.3.2, which provides a shortcut for computing counterfactuals.

Theorem 4.3.2 Let ùúè be the slope of the total effect of X on Y,

$$
\tau = E [ Y | d o (x + 1) ] - E [ Y | d o (x) ]
$$

then, for any evidence $Z = e$ , we have

$$
E \left[ Y _ {X = x} \mid Z = e \right] = E [ Y \mid Z = e ] + \tau (x - E [ X \mid Z = e ]) \tag {4.17}
$$

This provides an intuitive interpretation of counterfactuals in linear models: ${ \cal E } [ Y _ { X = x } | Z = e ]$ can be computed by -rst calculating the best estimate of Y conditioned on the evidence $e$ , $E [ Y | e ]$ , and then adding to it whatever change is expected in $Y$ when $X$ is shifted from its current best estimate, $E [ X | Z = e ]$ , to its hypothetical value, $x$ .

Methodologically, the importance of Theorem 4.3.2 lies in enabling researchers to answer hypothetical questions about individuals (or sets of individuals) from population data. The rami-cations of this feature in legal and social contexts will be explored in the following sections. In the situation illustrated by Figure 4.2, we computed the counterfactual $Y _ { H = 2 }$ under the evidence $e = \{ X = 0 . 5 , H = 1 , Y = 1 \}$ . We now demonstrate how Theorem 4.3.2 can be applied to this model in computing the effect of treatment on the treated

$$
E T T = E \left[ Y _ {1} - Y _ {0} \mid X = 1 \right] \tag {4.18}
$$

Substituting the evidence $e = \{ X = 1 \}$ in Eq. (4.17) we get

$$
\begin{array}{l} E T T = E \left[ Y _ {1} \mid X = 1 \right] - E \left[ Y _ {0} \mid X = 1 \right] \\ = E [ Y | X = 1 ] - E [ Y | X = 1 ] + \tau (1 - E [ X | X = 1 ]) - \tau (0 - E [ X | X = 1 ]) \\ = \tau \\ = b + a c = 0. 9 \\ \end{array}
$$

In other words, the effect of treatment on the treated is equal to the effect of treatment on the entire population. This is a general result in linear systems that can be seen directly from Eq. (4.17); $E [ Y _ { x + 1 } - Y _ { x } | e ] = \tau$ , independent on the evidence of $e$ . Things are different when a

multiplicative (i.e., nonlinear) interaction term is added to the output equation. For example, if the arrow $X  H$ were reversed in Figure 4.1, and the equation for Y read

$$
Y = b X + c H + \delta X H + U _ {Y} \tag {4.19}
$$

$\tau$ would differ from ETT. We leave it to the reader as an exercise to show that the difference $\tau - E T T$ then equals $\frac { \delta a } { 1 + a ^ { 2 } }$ (see Study question 4.3.2(c)).

# Study questions

# Study question 4.3.2

(a) Describe how the parameters $a , b , c$ in Figure 4.1 can be estimated from nonexperimental data.   
(b) In the model of Figure 4.3, -nd the effect of education on those students whose salary is $Y = 1$ . [Hint: Use Theorem 4.3.2 to compute $E [ Y _ { 1 } - Y _ { 0 } | Y = 1 ] .$ .]   
(c) Estimate $\tau$ and the $E T T = E [ Y _ { 1 } - Y _ { 0 } | X = 1 ]$ for the model described in Eq. (4.19). [Hint: Use the basic de-nition of counterfactuals, Eq. (4.5) and the equality $E [ Z | X = x ^ { \prime } ] = R _ { Z X } x ^ { \prime } .$ ]

# 4.4 Practical Uses of Counterfactuals

Now that we know how to compute counterfactuals, it will be instructive‚Äîand motivating‚Äîto see counterfactuals put to real use. In this section, we examine examples of problems that seem bafing at -rst, but that can be solved using the techniques we just laid out. Hopefully, the reader will leave this chapter with both a better understanding of how counterfactuals are used and a deeper appreciation of why we would want to use them.

# 4.4.1 Recruitment to a Program

Example 4.4.1 A government is funding a job training program aimed at getting jobless people back into the workforce. A pilot randomized experiment shows that the program is effective; a higher percentage of people were hired among those who -nished the program than among those who did not go through the program. As a result, the program is approved, and a recruitment effort is launched to encourage enrollment among the unemployed, by offering the job training program to any unemployed person who elects to enroll.

Lo and behold, enrollment is successful, and the hiring rate among the program‚Äôs graduates turns out even higher than in the randomized pilot study. The program developers are happy with the results and decide to request additional funding.

Oddly, critics claim that the program is a waste of taxpayers‚Äô money and should be terminated. Their reasoning is as follows: While the program was somewhat successful in the experimental study, where people were chosen at random, there is no proof that the program

accomplishes its mission among those who choose to enroll of their own volition. Those who self-enroll, the critics say, are more intelligent, more resourceful, and more socially connected than the eligible who did not enroll, and are more likely to have found a job regardless of the training. The critics claim that what we need to estimate is the differential bene-t of the program on those enrolled: the extent to which hiring rate has increased among the enrolled, compared to what it would have been had they not been trained.

Using our subscript notation for counterfactuals, and letting $X = 1$ represent training and $Y = 1$ represent hiring, the quantity that needs to be evaluated is the effect of training on the trained (ETT, better known as ‚Äúeffect of treatment on the treated,‚Äù Eq. (4.18)):

$$
E T T = E \left[ Y _ {1} - Y _ {0} \mid X = 1 \right] \tag {4.20}
$$

Here the difference $Y _ { 1 } - Y _ { 0 }$ represents the causal effect of training $( X )$ on hiring (Y) for a randomly chosen individual, and the condition $X = 1$ limits the choice to those actually choosing the training program on their own initiative.

As in our freeway example of Section 4.1, we are witnessing a clash between the antecedent $( X = 0$ ) of the counterfactual $Y _ { 0 }$ (hiring had training not taken place) and the event it is conditioned on, $X = 1$ . However, whereas the counterfactual analysis in the freeway example had no tangible consequences save for a personal regret statement‚Äî‚ÄúI should have taken the freeway‚Äù‚Äîhere the consequences have serious economic implications, such as terminating a training program, or possibly restructuring the recruitment strategy to attract people who would bene-t more from the program offered.

The expression for ETT does not appear to be estimable from either observational or experimental data. The reason rests, again, in the clash between the subscript of $Y _ { 0 }$ and the event $X = 1$ on which it is conditioned. Indeed, $E [ Y _ { 0 } | X = 1 ]$ stands for the expectation that a trained person $( X = 1$ ) would -nd a job had he/she not been trained. This counterfactual expectation seems to defy empirical measurement because we can never rerun history and deny training to those who received it. However, we see in the subsequent sections of this chapter that, despite this clash of worlds, the expectation $E [ Y _ { 0 } | X = 1 ]$ can be reduced to estimable expressions in many, though not all, situations. One such situation occurs when a set $Z$ of covariates satis-es the backdoor criterion with regard to the treatment and outcome variables. In such a case, ETT probabilities are given by a modi-ed adjustment formula:

$$
\begin{array}{l} P \left(Y _ {x} = y \mid X = x ^ {\prime}\right) \\ = \sum_ {z} P (Y = y | X = x, Z = z) P (Z = z | X = x ^ {\prime}) \tag {4.21} \\ \end{array}
$$

This follows directly from Theorem 4.3.1, since conditioning on $Z = z$ gives

$$
P (Y _ {x} = y | x ^ {\prime}) = \sum_ {z} P (Y _ {x} = y | z, x ^ {\prime}) P (z | x ^ {\prime})
$$

but Theorem 4.3.1 permits us to replace $x ^ { \prime }$ with $x$ , which by virtue of (4.6) permits us to remove the subscript $x$ from $Y _ { x }$ , yielding (4.21).

Comparing (4.21) to the standard adjustment formula of Eq. (3.5),

$$
P (Y = y | d o (X = x)) = \sum P (Y = y | X = x, Z = z) P (Z = z)
$$

we see that both formulas call for conditioning on $Z = z$ and averaging over $z$ , except that (4.21) calls for a different weighted average, with $P ( Z = z | X = x ^ { \prime } )$ replacing $P ( Z = z )$ .

Using Eq. (4.21), we readily get an estimable, noncounterfactual expression for ETT

$$
\begin{array}{l} E T T = E \left[ Y _ {1} - Y _ {0} \mid X = 1 \right] \\ = E [ Y _ {1} | X = 1 ] - E [ Y _ {0} | X = 1 ] \\ = E [ Y | X = 1 ] - \sum_ {z} E [ Y | X = 0, Z = z ] P (Z = z | X = 1) \\ \end{array}
$$

where the -rst term in the -nal expression is obtained using the consistency rule of Eq. (4.6). In other words, $E [ Y _ { 1 } | X = 1 ] = E [ Y | X = 1 ]$ because, conditional on $X = 1$ , the value that $Y$ would get had $X$ been 1 is simply the observed value of Y.

Another situation permitting the identi-cation of ETT occurs for binary $X$ whenever both experimental and nonexperimental data are available, in the form of $P ( Y = y | d o ( X = x ) )$ and $P ( X = x , Y = y )$ , respectively. Still another occurs when an intermediate variable is available between $X$ and Y satisfying the front-door criterion (Figure 3.10(b)). What is common to these situations is that an inspection of the causal graph can tell us whether ETT is estimable and, if so, how.

# Study questions

# Study question 4.4.1

(a) Prove that, if X is binary, the effect of treatment on the treated can be estimated from both observational and experimental data. Hint: Decompose $E [ Y _ { x } ]$ into

$$
E [ Y _ {x} ] = E [ Y _ {x} | x ^ {\prime} ] P (x ^ {\prime}) + E [ Y _ {x} | x ] P (x)
$$

(b) Apply the result of Question (a) to Simpson‚Äôs story with the nonexperimental data of Table 1.1, and estimate the effect of treatment on those who used the drug by choice. [Hint: Estimate $E [ Y _ { x } ]$ assuming that gender is the only confounder.]   
(c) Repeat Question (b) using Theorem 4.3.1 and the fact that Z in Figure 3.3 satis-es the backdoor criterion. Show that the answers to (b) and (c) coincide.

# 4.4.2 Additive Interventions

Example 4.4.2 In many experiments, the external manipulation consists of adding (or subtracting) some amount from a variable X without disabling preexisting causes of X, as required by the do(x) operator. For example, we might give 5 mg/l of insulin to a group of patients with varying levels of insulin already in their systems. Here, the preexisting causes of the manipulated variable continue to exert their inuences, and a new quantity is added, allowing for differences among units to continue. Can the effect of such interventions be predicted from observational studies, or from experimental studies in which X was set uniformly to some predetermined value x?

If we write our question using counterfactual variables, the answer becomes obvious. Suppose we were to add a quantity $q$ to a treatment variable $X$ that is currently at level $X = x ^ { \prime }$ .

The resulting outcome would be $Y _ { x ^ { \prime } + q }$ , and the average value of this outcome over all units currently at level $X = x ^ { \prime }$ would be $E [ Y _ { x } | x ^ { \prime } ]$ , with $x = x ^ { \prime } + q$ . Here, we meet again the ETT expression $E [ Y _ { x } | x ^ { \prime } ]$ , to which we can apply the results described in the previous example. In particular, we can conclude immediately that, whenever a set $Z$ in our model satis-es the backdoor criterion, the effect of an additive intervention is estimable using the ETT adjustment formula of Eq. (4.21). Substituting $x = x ^ { \prime } + q$ in (4.21) and taking expectations gives the effect of this intervention, which we call add(q):

$$
\begin{array}{l} E [ Y | a d d (q) ] - E [ Y ] \\ = \sum_ {x ^ {\prime}} E \left[ Y _ {x ^ {\prime} + q} \mid X = x ^ {\prime} \right] P (X = x ^ {\prime}) - E [ Y ] \\ = \sum_ {x ^ {\prime}} \sum_ {z} E [ Y | X = x ^ {\prime} + q, Z = z ] P (Z = z | X = x ^ {\prime}) P (X = x ^ {\prime}) - E [ Y ] \tag {4.22} \\ \end{array}
$$

In our example, Z may include variables such as age, weight, or genetic disposition; we require only that each of those variables be measured and that they satisfy the backdoor condition.

Similarly, estimability is assured for all other cases in which ETT is identi-able.

This example demonstrates the use of counterfactuals to estimate the effect of practical interventions, which cannot always be described as do-expressions, but may nevertheless be estimated under certain circumstances. A question naturally arises: Why do we need to resort to counterfactuals to predict the effect of a rather common intervention, one that could be estimated by a straightforward clinical trial at the population level? We simply split a randomly chosen group of subjects into two parts, subject half of them to an add(q) type of intervention and compare the expected value of Y in this group to that obtained in the add(0) group. What is it about additive interventions that force us to seek the advice of a convoluted oracle, in the form of counterfactuals and ETT, when the answer can be obtained by a simple randomized trial?

The answer is that we need to resort to counterfactuals only because our target quantity, $E [ Y | a d d ( q ) ]$ , could not be reduced to $d o$ -expressions, and it is through do-expressions that scientists report their experimental -ndings. This does not mean that the desired quantity $E [ Y | a d d ( q ) ]$ cannot be obtained from a specially designed experiment; it means only that save for conducting such a special experiment, the desired quantity cannot be inferred from scienti-c knowledge or from a standard experiment in which $X$ is set to $X = x$ uniformly over the population. The reason we seek to base policies on such ideal standard experiments is that they capture scienti-c knowledge. Scientists are interested in quantifying the effect of increasing insulin concentration in the blood from a given level $X = x$ to a another level $X = x + q$ , and this increase is captured by the do-expression: $E [ Y | d o ( X = x + q ) ] - E [ Y | d o ( X = x ) ]$ . We label it ‚Äúscienti-c‚Äù because it is biologically meaningful, namely its implications are invariant across populations (indeed laboratory blood tests report patients‚Äô concentration levels, $X = x .$ , which are tracked over time). In contrast, the policy question in the case of additive interventions does not have this invariance feature; it asks for the average effect of adding an increment $q$ to everyone, regardless of the current $x$ level of each individual in this particular population. It is not immediately transportable, because it is highly sensitive to the probability $P ( X = x )$ in the population under study. This creates a mismatch between what science tells us and what policy makers ask us to estimate. It is no wonder, therefore, that we need to resort to a unit-level analysis (i.e., counterfactuals) in order to translate from one language into another.

The reader may also wonder why $E [ Y | a d d ( q ) ]$ is not equal to the average causal effect

$$
\sum_ {x} [ E [ Y | d o (X = x + q) ] - E [ Y | d o (X = x) ] ] P (X = x)
$$

After all, if we know that adding $q$ to an individual at level $X = x$ would increase its expected Y by $E [ Y | d o ( X = x + q ) ] - E [ Y | d o ( X = x ) ]$ , then averaging this increase over $X$ should give us the answer to the policy question $E [ Y | a d d ( q ) ]$ . Unfortunately, this average does not capture the policy question. This average represents an experiment in which subjects are chosen at random from the population, a fraction $P ( X = x )$ are given an additional dose $q$ , and the rest are left alone. But things are different in the policy question at hand, since $P ( X = x )$ represents the proportion of subjects who entered level $X = x$ by free choice, and we cannot rule out the possibility that subjects who attain $X = x$ by free choice would react to add(q) differently from subjects who ‚Äúreceive‚Äù $X = x$ by experimental decree. For example, it is quite possible that subjects who are highly sensitive to $a d d ( q )$ would attempt to lower their $X$ level, given the choice.

We translate into counterfactual analysis and write the inequality:

$$
E [ Y | a d d (q) ] = \sum_ {x} E [ Y _ {x + q} | x ] P (X = x) \neq \sum_ {x} E [ Y _ {x + q} ] P (X = x)
$$

Equality holds only when $Y _ { x }$ is independent of $X$ , a condition that amounts to nonconfounding (see Theorem 4.3.1). Absent this condition, the estimation of $E [ Y | a d d ( q ) ]$ can be accomplished either by $q$ -speci-c intervention or through stronger assumptions that enable the translation of ETT to do-expressions, as in Eq. (4.21).

# Study question 4.4.2

Joe has never smoked before but, as a result of peer pressure and other personal factors, he decided to start smoking. He buys a pack of cigarettes, comes home, and asks himself: ‚ÄúI am about to start smoking, should I?‚Äù

(a) Formulate Joe‚Äôs question mathematically, in terms of ETT, assuming that the outcome of interest is lung cancer.   
(b) What type of data would enable Joe to estimate his chances of getting cancer given that he goes ahead with the decision to smoke, versus refraining from smoking.   
(c) Use the data in Table 3.1 to estimate the chances associated with the decision in (b).

# 4.4.3 Personal Decision Making

Example 4.4.3 Ms Jones, a cancer patient, is facing a tough decision between two possible treatments: (i) lumpectomy alone or (ii) lumpectomy plus irradiation. In consultation with her oncologist, she decides on (ii). Ten years later, Ms Jones is alive, and the tumor has not recurred. She speculates: Do I owe my life to irradiation?

Mrs Smith, on the other hand, had a lumpectomy alone, and her tumor recurred after a year. And she is regretting: I should have gone through irradiation.

Can these speculations ever be substantiated from statistical data? Moreover, what good would it do to con-rm Ms Jones‚Äôs triumph or Mrs Smith‚Äôs regret?

The overall effectiveness of irradiation can, of course, be determined by randomized experiments. Indeed, on October 17, 2002, the New England Journal of Medicine published a paper by Fisher et al. describing a 20-year follow-up of a randomized trial comparing lumpectomy alone and lumpectomy plus irradiation. The addition of irradiation to lumpectomy was shown to cause substantially fewer recurrences of breast cancer ( $14 \%$ vs $39 \%$ ).

These, however, were population results. Can we infer from them the speci-c cases of Ms Jones and Mrs Smith? And what would we gain if we do, aside from supporting Ms Jones‚Äôs satisfaction with her decision or intensifying Mrs Smith‚Äôs sense of failure?

To answer the -rst question, we must -rst cast the concerns of Ms Jones and Mrs Smith in mathematical form, using counterfactuals. If we designate remission by $Y = 1$ and the decision to undergo irradiation by $X = 1$ , then the probability that determines whether Ms Jones is justi-ed in attributing her remission to the irradiation $( X = 1$ ) is

$$
P N = P \left(Y _ {0} = 0 \mid X = 1, Y = 1\right) \tag {4.23}
$$

It reads: the probability that remission would not have occurred $Y = 0$ ) had Ms Jones not gone through irradiation, given that she did in fact go through irradiation $\boldsymbol { { X = 1 } }$ ), and remission did occur $\left. Y = 1 \right.$ ). The label PN stands for ‚Äúprobability of necessity‚Äù that measures the degree to which Ms Jones‚Äôs decision was necessary for her positive outcome.

Similarly, the probability that Ms Smith‚Äôs regret is justi-ed is given by

$$
P S = P \left(Y _ {1} = 1 \mid X = 0, Y = 0\right) \tag {4.24}
$$

It reads: the probability that remission would have occurred had Mrs Smith gone through irradiation $( Y _ { 1 } = 1 )$ ), given that she did not in fact go through irradiation $( X = 0$ ), and remission did not occur $( Y = 0 )$ ). PS stands for the ‚Äúprobability of sufficiency,‚Äù measuring the degree to which the action $X = 1$ , which was not taken.

We see that these expressions have almost the same form (save for interchanging ones with zeros) and, moreover, both are similar to Eq. (4.1), save for the fact that Y in the freeway example was a continuous variable, so its expected value was the quantity of interest.

These two probabilities (sometimes referred to as ‚Äúprobabilities of causation‚Äù) play a major role in all questions of ‚Äúattribution,‚Äù ranging from legal liability to personal decision making. They are not, in general, estimable from either observational or experimental data, but as we shall see below, they are estimable under certain conditions, when both observational and experimental data are available.

But before commencing a quantitative analysis, let us address our second question: What is gained by assessing these retrospective counterfactual parameters? One answer is that notions such as regret and success, being right or being wrong, have more than just emotional value; they play important roles in cognitive development and adaptive learning. Con-rmation of Ms Jones‚Äôs triumph reinforces her con-dence in her decision-making strategy, which may include her sources of medical information, her attitude toward risks, and her sense of priority, as well as the strategies she has been using to put all these considerations together. The same applies to regret; it drives us to identify sources of weakness in our strategies and to think of some kind of change that would improve them. It is through counterfactual reinforcement that we learn to improve our own decision-making processes and achieve higher performance. As Kathryn Schultz says in her delightful book Being Wrong, ‚ÄúHowever disorienting, dif-cult, or humbling our mistakes might be, it is ultimately wrongness, not rightness, that can teach us who we are.‚Äù

Estimating the probabilities of being right or wrong also has tangible and profound impact on critical decision making. Imagine a third lady, Ms Daily, facing the same decision as Ms Jones did, and telling herself: If my tumor is the type that would not recur under lumpectomy alone, why should I go through the hardships of irradiation? Similarly, if my tumor is the type that would recur regardless of whether I go through irradiation or not, I would rather not go through it. The only reason for me to go through this is if the tumor is the type that would remiss under treatment and recur under no treatment.

Formally, Ms Daily‚Äôs dilemma is to quantify the probability that irradiation is both necessary and suf-cient for eliminating her tumor, or

$$
P N S = P \left(Y _ {1} = 1, Y _ {0} = 0\right) \tag {4.25}
$$

where $Y _ { 1 }$ and $Y _ { 0 }$ stand for remission under treatment $( Y _ { 1 } )$ and nontreatment $( Y _ { 0 } )$ , respectively. Knowing this probability would help Ms Daily‚Äôs assessment of how likely she is to belong to the group of individuals for whom $Y _ { 1 } = 1$ and $Y _ { 0 } = 0$ .

This probability cannot, of course, be assessed from experimental studies, because we can never tell from experimental data whether an outcome would have been different had the person been assigned to a different treatment. However, casting Ms Daily‚Äôs question in mathematical form enables us to investigate algebraically what assumptions are needed for estimating PNS and from what type of data. In the next section (Section 4.5.1, Eq. (4.42)), we see that indeed, PNS can be estimated if we assume monotonicity, namely, that irradiation cannot cause the recurrence of a tumor that was about to remit. Moreover, under monotonicity, experimental data are suf-cient to conclude

$$
P N S = P (Y = 1 \mid d o (X = 1)) - P (Y = 1 \mid d o (X = 0)) \tag {4.26}
$$

For example, if we rely on the experimental data of Fisher et al. (2002), this formula permits us to conclude that Ms Daily‚Äôs PNS is

$$
P N S = 0. 8 6 - 0. 6 1 = 0. 2 5
$$

This gives her a $2 5 \%$ chance that her tumor is the type that responds to treatment‚Äîspeci-cally, that it will remit under lumpectomy plus irradiation but will recur under lumpectomy alone. Such quanti-cation of individual risks is extremely important in personal decision making, and estimates of such risks from population data can only be inferred through counterfactual analysis and appropriate assumptions.

# 4.4.4 Discrimination in Hiring

Example 4.4.4 Mary -les a law suit against the New York-based XYZ International, alleging discriminatory hiring practices. According to her, she has applied for a job with XYZ International, and she has all the credentials for the job, yet she was not hired, allegedly because she mentioned, during the course of her interview, that she is gay. Moreover, she claims, the hiring record of XYZ International shows consistent preferences for straight employees. Does she have a case? Can hiring records prove whether XYZ International was discriminating when declining her job application?

At the time of writing, U.S. law doesn‚Äôt speci-cally prohibit employment discrimination on the basis of sexual orientation, but New York law does. And New York de-nes discrimination in much the same way as federal law. U.S. courts have issued clear directives as to what constitutes employment discrimination. According to law makers, ‚ÄúThe central question in any employment-discrimination case is whether the employer would have taken the same action had the employee been of a different race (age, sex, religion, national origin, etc.) and everything else had been the same.‚Äù (In Carson vs Bethlehem Steel Corp., 70 FEP Cases 921, 7th Cir. (1996).)

The -rst thing to note in this directive is that it is not a population-based criterion, but one that appeals to the individual case of the plaintiff. The second thing to note is that it is formulated in counterfactual terminology, using idioms such as ‚Äúwould have taken,‚Äù ‚Äúhad the employee been,‚Äù and ‚Äúhad been the same.‚Äù What do they mean? Can one ever prove how an employer would have acted had Mary been straight? Certainly, this is not a variable that we can intervene upon in an experimental setting. Can data from an observational study prove an employer discriminating?

It turns out that Mary‚Äôs case, though super-cially different from Example 4.4.3, has a lot in common with the problem Ms Smith faced over her unsuccessful cancer treatment. The probability that Mary‚Äôs nonhiring is due to her sexual orientation can, similarly to Ms Smith‚Äôs cancer treatment, be expressed using the probability of suf-ciency:

$$
P S = P (Y _ {1} = 1 | X = 0, Y = 0)
$$

In this case, Y stands for Mary‚Äôs hiring, and $X$ stands for the interviewer‚Äôs perception of Mary‚Äôs sexual orientation. The expression reads: ‚Äúthe probability that Mary would have been hired had the interviewer perceived her as straight, given that the interviewer perceived her as gay, and she was not hired.‚Äù (Note that the variable in question is the interviewer‚Äôs perception of Mary‚Äôs sexual orientation, not the orientation itself, because an intervention on perception is quite simple in this case‚Äîwe need only to imagine that Mary never mentioned that she is gay; hypothesizing a change in Mary‚Äôs actual orientation, although formally acceptable, brings with it an aura of awkwardness.)

We show in 4.5.2 that, although discrimination cannot be proved in individual cases, the probability that such discrimination took place can be determined, and this probability may sometimes reach a level approaching certitude. The next example examines how the problem of discrimination‚Äîin this case on gender, not sexual orientation may appear to a policy maker, rather than a juror.

# 4.4.5 Mediation and Path-disabling Interventions

Example 4.4.5 A policy maker wishes to assess the extent to which gender disparity in hiring can be reduced by making hiring decisions gender-blind, rather than eliminating gender inequality in education or job training. The former concerns the ‚Äúdirect effect‚Äù of gender on hiring, whereas the latter concerns the ‚Äúindirect effect,‚Äù or the effect mediated via job quali-cation.

In this example, fighting employers‚Äô prejudices and launching educational reforms are two contending policy options that involve costly investments and different implementation strategies. Knowing in advance which of the two, if successful, would have a greater impact on reducing hiring disparity is essential for planning, and depends critically on mediation analysis for resolution. For example, knowing that current hiring disparities are due primarily to employers‚Äô prejudices would render educational reforms superfluous, a fact that may save substantial resources. Note, however, that the policy decisions in this example concern the enabling and disabling of processes rather than lowering or raising values of specific variables. The educational reform program calls for disabling current educational practices and replacing them with a new program in which women obtain the same educational opportunities as men. The hiring-based proposal calls for disabling the current hiring process and replacing it with one in which gender plays no role in hiring decisions.

Because we are dealing with disabling processes rather than changing levels of variables, there is no way we can express the effect of such interventions using a do-operator, as we did in the mediation analysis of Section 3.7. We can express it, however, in a counterfactual language, using the desired end result as an antecedent. For example, if we wish to assess the hiring disparity after successfully implementing gender-blind hiring procedures, we impose the condition that all female applicants be treated like males as an antecedent and proceed to estimate the hiring rate under such a counterfactual condition.

The analysis proceeds as follows: the hiring status $( Y )$ of a female applicant with quali-- cation $Q = q$ , given that the employer treats her as though she is a male is captured by the counterfactual $Y _ { X = 1 , Q = q }$ , where $X = 1$ refers to being a male. But since the value $q$ would vary among applicants, we need to average this quantity according to the distribution of female quali-cation, giving $\textstyle \sum _ { q } E [ Y _ { X = 1 , Q = q } ] P ( Q = q | X = 0 )$ . Male applicants would have a similar chance at hiring except that the average is governed by the distribution of male quali-cation, giving

$$
\sum_ {q} E [ Y _ {X = 1, Q = q} ] P (Q = q | X = 1)
$$

If we subtract the two quantities, we get

$$
\sum_ {q} E [ Y _ {X = 1, Q = q} ] [ P (Q = q | X = 0) - P (Q = q | X = 1) ]
$$

which is the indirect effect of gender on hiring, mediated by quali-cation. We call this effect the natural indirect effect (NIE), because we allow the quali-cation $Q$ to vary naturally from applicant to applicant, as opposed to the controlled direct effect in Chapter 3, where we held the mediator at a constant level for the entire population. Here we merely disable the capacity of Y to respond to $X$ but leave its response to $Q$ unaltered.

The next question to ask is whether such a counterfactual expression can be identi-ed from data. It can be shown (Pearl 2001) that, in the absence of confounding the NIE can be estimated by conditional probabilities, giving

$$
N I E = \sum_ {q} E [ Y | X = 1, Q = q ] [ P (Q = q | X = 0) - P (Q = q | X = 1) ]
$$

This expression is known as the mediation formula. It measures the extent to which the effect of $X$ on $Y$ is explained by its effect on the mediator $Q$ . Counterfactual analysis permits us to de-ne and assess NIE by ‚Äúfreezing‚Äù the direct effect of $X$ on Y, and allowing the mediator $( Q )$ of each unit to react to $X$ in a natural way, as if no freezing took place.

The mathematical tools necessary for estimating the various nuances of mediation are summarized in Section 4.5.

# 4.5 Mathematical Tool Kits for Attribution and Mediation

As we examined the practical applications of counterfactual analysis in Section 4.4, we noted several recurring patterns that shared mathematical expressions as well as methods of solution. The -rst was the effect of treatment on the treated, ETT, whose syntactic signature was the counterfactual expression $E [ Y _ { x } | X = x ^ { \prime } ]$ , with $x$ and $x ^ { \prime }$ two distinct values of $X$ . We showed that problems as varied as recruitment to a program (Section 4.4.1) and additive interventions (Example 4.4.2) rely on the estimation of this expression, and we have listed conditions under which estimation is feasible, as well as the resulting estimand (Eqs. (4.21) and (4.8)).

Another recurring pattern appeared in problems of attribution, such as personal decision problems (Example 4.4.3) and possible cases of discrimination (Example 4.4.4). Here, the pattern was the expression for the probability of necessity:

$$
P N = P (Y _ {0} = 0 | X = 1, Y = 1)
$$

The probability of necessity also pops up in problems of legal liability, where it reads: ‚ÄúThe probability that the damage would not have occurred had the action not been taken $( Y _ { 0 } = 0 )$ ), given that, in fact, the damage did occur $\left( Y = 1 \right.$ ) and the action was taken $( X = 1$ ).‚Äù Section 4.5.1 summarizes mathematical results that will enable readers to estimate (or bound) PN using a combination of observational and experimental data.

Finally, in questions of mediation (Example 4.4.5) the key counterfactual expression was

$$
E [ Y _ {x, M _ {x ^ {\prime}}} ]
$$

which reads, ‚ÄúThe expected outcome (Y) had the treatment been $X = x$ and, simultaneously, had the mediator $M$ attained the value $( M _ { x ^ { \prime } } )$ it would have attained had X been $x ^ { \prime }$ ‚Äù. Section 4.5.2 will list the conditions under which this ‚Äúnested‚Äù counterfactual expression can be estimated, as well as the resulting estimands and their interpretations.

# 4.5.1 A Tool Kit for Attribution and Probabilities of Causation

Assuming binary events, with $X = x$ and $Y = y$ representing treatment and outcome, respectively, and $X = x ^ { \prime } , \ Y = y ^ { \prime }$ their negations, our target quantity is de-ned by the English sentence:

‚ÄúFind the probability that if $X$ had been $x ^ { \prime }$ , $Y$ would be $y ^ { \prime }$ , given that, in reality, $X$ is $x$ and $Y$ is $y$ .‚Äù

Mathematically, this reads

$$
P N (x, y) = P \left(Y _ {x ^ {\prime}} = y ^ {\prime} \mid X = x, Y = y\right) \tag {4.27}
$$

This counterfactual quantity, named ‚Äúprobability of necessity‚Äù (PN), captures the legal criterion of ‚Äúbut for,‚Äù according to which judgment in favor of a plaintiff should be made if and only if it is ‚Äúmore probable than not‚Äù that the damage would not have occurred but for the defendant‚Äôs action (Robertson 1997).

Having written a formal expression for PN, Eq. (4.27), we can move on to the identi-cation phase and ask what assumptions permit us to identify PN from empirical studies, be they observational, experimental, or a combination thereof.

Mathematical analysis of this problem (described in (Pearl 2000, Chapter 9)) yields the following results:

Theorem 4.5.1 If Y is monotonic relative to $X$ , that is, $Y _ { 1 } ( u ) \ge Y _ { 0 } ( u )$ for all u, then PN is identi-able whenever the causal effect $P ( y | d o ( x ) )$ is identi-able, and

$$
P N = \frac {P (y) - P (y \mid d o \left(x ^ {\prime}\right))}{P (x , y)} \tag {4.28}
$$

or, substituting $P ( y ) = P ( y | x ) P ( x ) + P ( y | x ^ { \prime } ) ( 1 - P ( x ) )$ , we obtain

$$
P N = \frac {P (y \mid x) - P (y \mid x ^ {\prime})}{P (y \mid x)} + \frac {P (y \mid x ^ {\prime}) - P (y \mid d o \left(x ^ {\prime}\right))}{P (x , y)} \tag {4.29}
$$

The -rst term on the r.h.s. of (4.29) is called the excess risk ratio (ERR) and is often used in court cases in the absence of experimental data (Greenland 1999). It is also known as the Attributable Risk Fraction among the exposed (Jewell 2004, Chapter 4.7). The second term (the confounding factor (CF)) represents a correction needed to account for confounding bias, that is, $P ( y | d o ( x ^ { \prime } ) ) \neq P ( y | x ^ { \prime } )$ . Put in words, confounding occurs when the proportion of population for whom $Y = y$ , when $X$ is set to $x ^ { \prime }$ for everyone is not the same as the proportion of the population for whom $Y = y$ among those acquiring $X = x ^ { \prime }$ by choice. For instance, suppose there is a case brought against a car manufacturer, claiming that its car‚Äôs faulty design led to a man‚Äôs death in a car crash. The ERR tells us how much more likely people are to die in crashes when driving one of the manufacturer‚Äôs cars. If it turns out that people who buy the manufacturer‚Äôs cars are more likely to drive fast (leading to deadlier crashes) than the general population, the second term will correct for that bias.

Equation (4.29) thus provides an estimable measure of necessary causation, which can be used for monotonic $Y _ { x } ( u )$ whenever the causal effect $P ( y | d o ( x ) )$ can be estimated, be it from randomized trials or from graph-assisted observational studies (e.g., through the backdoor criterion). More signi-cantly, it has also been shown (Tian and Pearl 2000) that the expression in (4.28) provides a lower bound for PN in the general nonmonotonic case. In particular, the upper and lower bounds on PN are given by

$$
\max  \left\{0, \frac {P (y) - P \left(y \mid d o \left(x ^ {\prime}\right)\right)}{P (x , y)} \right\} \leq P N \leq \min  \left\{1, \frac {P \left(y ^ {\prime} \mid d o \left(x ^ {\prime}\right)\right) - P \left(x ^ {\prime} , y ^ {\prime}\right)}{P (x , y)} \right\} \tag {4.30}
$$

In drug-related litigation, it is not uncommon to obtain data from both experimental and observational studies. The former is usually available from the manufacturer or the agency

that approved the drug for distribution (e.g., FDA), whereas the latter is often available from surveys of the population.

A few algebraic steps allow us to express the lower bound (LB) and upper bound (UB) as

$$
L B = E R R + C F
$$

$$
U B = E R R + q + C F \tag {4.31}
$$

where ERR, CF, and $q$ are de-ned as follows:

$$
C F \triangleq \left[ P \left(y \mid x ^ {\prime}\right) - P \left(y _ {x ^ {\prime}}\right) \right] / P (x, y) \tag {4.32}
$$

$$
E R R \triangleq 1 - 1 / R R = 1 - P \left(y \mid x ^ {\prime}\right) / P (y \mid x) \tag {4.33}
$$

$$
q \triangleq P \left(y ^ {\prime} | x\right) / P (y | x) \tag {4.34}
$$

Here, CF represents the normalized degree of confounding among the unexposed $( X = x ^ { \prime } )$ ), ERR is the ‚Äúexcess risk ratio‚Äù and $q$ is the ratio of negative to positive outcomes among the exposed.

Figure 4.5(a) and (b) depicts these bounds as a function of ERR, and reveals three useful features. First, regardless of confounding, the interval $U B { - } L B$ remains constant and depends on only one observable parameter, $P ( y ^ { \prime } | x ) / P ( y | x )$ . Second, the CF may raise the lower bound to meet the criterion of ‚Äúmore probable than not,‚Äù $\begin{array} { r } { P N > \frac { 1 } { 2 } } \end{array}$ , when the ERR alone would not suf-ce. Lastly, the amount of ‚Äúrise‚Äù to both bounds is given by $C F$ , which is the only estimate needed from the experimental data; the causal effect $P ( y _ { x } ) - P ( y _ { x ^ { \prime } } )$ is not needed.

Theorem 4.5.1 further assures us that, if monotonicity can be assumed, the upper and lower bounds coincide, and the gap collapses entirely, as shown in Figure 4.5(b). This collapse does not reect $q = 0$ , but a shift from the bounds of (4.30) to the identi-ed conditions of (4.28).

If it is the case that the experimental and survey data have been drawn at random from the same population, then the experimental data can be used to estimate the counterfactuals

![](images/131649a2cbce53534a2484192b2694ecf54ca94289ee6c301c4900876858a4c3.jpg)

![](images/bb8c488a09729b97c40fd8dc089bec0ec5e80933e96d5e910b611a31fcf8e0d1.jpg)  
(b)   
Figure 4.5 (a) Showing how probabilities of necessity (PN) are bounded, as a function of the excess risk ratio (ERR) and the confounding factor (CF) (Eq. (4.31)); (b) showing how PN is identi-ed when monotonicity is assumed (Theorem 4.5.1)

of interest, for example, $P ( Y _ { x } = y )$ , for the observational as well as experimental sampled populations.

Example 4.5.1 (Attribution in Legal Setting) A lawsuit is -led against the manufacturer of drug x, charging that the drug is likely to have caused the death of Mr A, who took it to relieve back pains. The manufacturer claims that experimental data on patients with back pains show conclusively that drug x has only minor effects on death rates. However, the plaintiff argues that the experimental study is of little relevance to this case because it represents average effects on patients in the study, not on patients like Mr A who did not participate in the study. In particular, argues the plaintiff, Mr A is unique in that he used the drug of his own volition, unlike subjects in the experimental study, who took the drug to comply with experimental protocols. To support this argument, the plaintiff furnishes nonexperimental data on patients who, like Mr A, chose drug x to relieve back pains but were not part of any experiment, and who experienced lower death rates than those who didn‚Äôt take the drug. The court must now decide, based on both the experimental and nonexperimental studies, whether it is ‚Äúmore probable than not‚Äù that drug x was in fact the cause of Mr A‚Äôs death.

To illustrate the usefulness of the bounds in Eq. (4.30), consider (hypothetical) data associated with the two studies shown in Table 4.5. (In the analyses below, we ignore sampling variability.)

The experimental data provide the estimates

$$
P (y \mid d o (x)) = 1 6 / 1 0 0 0 = 0. 0 1 6 \tag {4.35}
$$

$$
P \left(y \mid d o \left(x ^ {\prime}\right)\right) = 1 4 / 1 0 0 0 = 0. 0 1 4 \tag {4.36}
$$

whereas the nonexperimental data provide the estimates

$$
P (y) = 3 0 / 2 0 0 0 = 0. 0 1 5 \tag {4.37}
$$

$$
P (x, y) = 2 / 2 0 0 0 = 0. 0 0 1 \tag {4.38}
$$

$$
P (y \mid x) = 2 / 1 0 0 0 = 0. 0 0 2 \tag {4.39}
$$

$$
P \left(y \mid x ^ {\prime}\right) = 2 8 / 1 0 0 0 = 0. 0 2 8 \tag {4.40}
$$

Table 4.5 Experimental and nonexperimental data used to illustrate the estimation of PN, the probability that drug $x$ was responsible for a person‚Äôs death (y)   

<table><tr><td></td><td colspan="2">Experimental</td><td colspan="2">Nonexperimental</td></tr><tr><td></td><td>do(x)</td><td>do(x&#x27;)</td><td>x</td><td>x&#x27;</td></tr><tr><td>Deaths (y)</td><td>16</td><td>14</td><td>2</td><td>28</td></tr><tr><td>Survivals (y&#x27;)</td><td>984</td><td>986</td><td>998</td><td>972</td></tr></table>

Assuming that drug $x$ can only cause (but never prevent) death, monotonicity holds, and Theorem 4.5.1 (Eq. (4.29)) yields

$$
\begin{array}{l} P N = \frac {P (y | x) - P (y | x ^ {\prime})}{P (y | x)} + \frac {P (y | x ^ {\prime}) - P (y | d o (x ^ {\prime}))}{P (x , y)} \\ = \frac {0 . 0 0 2 - 0 . 0 2 8}{0 . 0 0 2} + \frac {0 . 0 2 8 - 0 . 0 1 4}{0 . 0 0 1} = - 1 3 + 1 4 = 1 \tag {4.41} \\ \end{array}
$$

We see that while the observational ERR is negative (‚àí13), giving the impression that the drug is actually preventing deaths, the bias-correction term $( + 1 4 )$ recti-es this impression and sets the probability of necessity (PN) to unity. Moreover, since the lower bound of Eq. (4.30) becomes 1, we conclude that $P N = 1 . 0 0$ even without assuming monotonicity. Thus, the plaintiff was correct; barring sampling errors, the data provide us with $100 \%$ assurance that drug $x$ was in fact responsible for the death of Mr A.

To complete this tool kit for attribution, we note that the other two probabilities that came up in the discussion on personal decision-making (Example 4.4.3), PS and PNS, can be bounded by similar expressions; see (Pearl 2000, Chapter 9) and (Tian and Pearl 2000).

In particular, when $Y _ { x } ( u )$ is monotonic, we have

$$
\begin{array}{l} P N S = P (Y _ {x} = 1, Y _ {x ^ {\prime}} = 0) \\ = P \left(Y _ {x} = 1\right) - P \left(Y _ {x ^ {\prime}} = 1\right) \tag {4.42} \\ \end{array}
$$

as asserted in Example 4.4.3, Eq. (4.26).

# Study questions

# Study question 4.5.1

Consider the dilemma faced by Ms Jones, as described in Example 4.4.3. Assume that, in addition to the experimental results of Fisher et al. (2002), she also gains access to an observational study, according to which the probability of recurrent tumor in all patients (regardless of irradiation) is $30 \%$ , whereas among the recurrent cases, $70 \%$ did not choose therapy. Use the bounds provided in Eq. (4.30) to update her estimate that her decision was necessary for remission.

# 4.5.2 A Tool Kit for Mediation

The canonical model for a typical mediation problem takes the form:

$$
t = f _ {T} \left(u _ {T}\right) \quad m = f _ {M} \left(t, u _ {M}\right) \quad y = f _ {Y} \left(t, m, u _ {Y}\right) \tag {4.43}
$$

where $T$ (treatment), $M$ (mediator), and Y (outcome) are discrete or continuous random variables, $f _ { T } , f _ { M }$ , and $f _ { Y }$ are arbitrary functions, and $U _ { T } , U _ { M } , U _ { Y }$ represent, respectively, omitted factors that inuence $T , M$ , and Y. The triplet $U = ( U _ { T } , U _ { M } , U _ { Y } )$ is a random vector that accounts for all variations among individuals.

In Figure 4.6(a), the omitted factors are assumed to be arbitrarily distributed but mutually independent. In Figure 4.6(b), the dashed arcs connecting $U _ { T }$ and $U _ { M }$ (as well as $U _ { M }$ and $U _ { T }$ ) encode the understanding that the factors in question may be dependent.

![](images/f1674200f700f717ef0d0a08c9d2882bd362de5c75a213b247a3e7ff3fad5cce.jpg)

![](images/7f3cc29af96a5509151afee8a884374f05b63ad0fdaa9a5494e11b11d6d9b9d6.jpg)  
Figure 4.6 (a) The basic nonparametric mediation model, with no confounding. (b) A confounded mediation model in which dependence exists between $U _ { _ M }$ and $( U _ { _ { T } } , U _ { _ { Y } } )$

# Counterfactual de-nition of direct and indirect effects

Using the structural model of Eq. (4.43) and the counterfactual notation de-ned in Section 4.2.1, four types of effects can be de-ned for the transition from $T = 0$ to $T = 1$ . Generalizations to arbitrary reference points, say from $T = t$ to $T = t ^ { \prime }$ , are straightforward1:

(a) Total effect ‚Äì

$$
\begin{array}{l} T E = E \left[ Y _ {1} - Y _ {0} \right] \\ = E [ Y | d o (T = 1) ] - E [ Y | d o (T = 0) ] \tag {4.44} \\ \end{array}
$$

TE measures the expected increase in $Y$ as the treatment changes from $T = 0$ to $T = 1$ , while the mediator is allowed to track the change in $T$ naturally, as dictated by the function $f _ { M }$ .

(b) Controlled direct effect ‚Äì

$$
\begin{array}{l} C D E (m) = E \left[ Y _ {1, m} - Y _ {0, m} \right] \\ = E [ Y | d o (T = 1, M = m) ] - E [ Y | d o (T = 0, M = m) ] \tag {4.45} \\ \end{array}
$$

CDE measures the expected increase in Y as the treatment changes from $T = 0$ to $T = 1$ , while the mediator is set to a speci-ed level $M = m$ uniformly over the entire population.

(c) Natural direct effect ‚Äì

$$
N D E = E \left[ Y _ {1, M _ {0}} - Y _ {0, M _ {0}} \right] \tag {4.46}
$$

NDE measures the expected increase in Y as the treatment changes from $T = 0$ to $T = 1$ , while the mediator is set to whatever value it would have attained (for each individual) prior to the change, that is, under $T = 0$ .

(d) Natural indirect effect ‚Äì

$$
N I E = E \left[ Y _ {0, M _ {1}} - Y _ {0, M _ {0}} \right] \tag {4.47}
$$

NIE measures the expected increase in Y when the treatment is held constant, at $T = 0$ , and $M$ changes to whatever value it would have attained (for each individual) under $T = 1 .$ . It captures, therefore, the portion of the effect that can be explained by mediation alone, while disabling the capacity of $Y$ to respond to $X$ .

We note that, in general, the total effect can be decomposed as

$$
T E = N D E - N I E _ {r} \tag {4.48}
$$

where $N I E _ { r }$ stands for the NIE under the reverse transition, from $T = 1$ to $T = 0$ . This implies that NIE is identi-able whenever NDE and TE are identi-able. In linear systems, where reversal of transitions amounts to negating the signs of their effects, we have the standard additive formula, $T E = N D E + N I E$ .

We further note that $T E$ and $C D E ( m )$ are $d o$ -expressions and can, therefore, be estimated from experimental data or in observational studies using the backdoor or front-door adjustments. Not so for the NDE and NIE; a new set of assumptions is needed for their identi-cation.

# Conditions for identifying natural effects

The following set of conditions, marked A-1 to A-4, are suf-cient for identifying both direct and indirect natural effects.

We can identify the NDE and NIE provided that there exists a set $W$ of measured covariates such that

A-1 No member of W is a descendant of $T$ .   
A-2 W blocks all backdoor paths from $M$ to $Y$ (after removing $T \to M$ and $T \to Y$ ).   
A-3 The W-speci-c effect of $T$ on $M$ is identi-able (possibly using experiments or adjustments).   
A-4 The $W$ -speci-c joint effect of $\{ T , M \}$ on $Y$ is identi-able (possibly using experiments or adjustments).

Theorem 4.5.2 (Identi-cation of the NDE) When conditions A-1 and A-2 hold, the natural direct effect is experimentally identi-able and is given by

$$
\begin{array}{l} N D E = \sum_ {m} \sum_ {w} [ E [ Y | d o (T = 1, M = m), W = w ] - E [ Y | d o (T = 0, M = m), W = w ] ] \\ \times P (M = m \mid d o (T = 0), W = w) P (W = w) \tag {4.49} \\ \end{array}
$$

The identi-ability of the do-expressions in Eq. (4.49) is guaranteed by conditions A-3 and A-4 and can be determined using the backdoor or front-door criteria.

Corollary 4.5.1 If conditions A-1 and A-2 are satis-ed by a set W that also deconfounds the relationships in A-3 and A-4, then the do-expressions in Eq. (4.49) are reducible to conditional expectations, and the natural direct effect becomes

$$
\begin{array}{l} N D E = \sum_ {m} \sum_ {w} [ E [ Y | T = 1, M = m, W = w ] - E [ Y | T = 0, M = m, W = w ] ] \\ \times P (M = m \mid T = 0, W = w) P (W = w) \tag {4.50} \\ \end{array}
$$

In the nonconfounding case (Figure 4.6(a)), NDE reduces to

$$
N D E = \sum_ {m} [ E [ Y \mid T = 1, M = m ] - E [ Y \mid T = 0, M = m ] ] P (M = m \mid T = 0). \tag {4.51}
$$

Similarly, using (4.48) and $T E = E [ Y | T = 1 ] - E [ Y | T = 0 ]$ , NIE becomes

$$
N I E = \sum_ {m} E [ Y \mid T = 0, M = m ] [ P (M = m \mid T = 1) - P (M = m \mid T = 0) ] \tag {4.52}
$$

The last two expressions are known as the mediation formulas. We see that while NDE is a weighted average of CDE, no such interpretation can be given to NIE.

The counterfactual de-nitions of NDE and NIE (Eqs. (4.46) and (4.47)) permit us to give these effects meaningful interpretations in terms of ‚Äúresponse fractions.‚Äù The ratio $N D E / T E$ measures the fraction of the response that is transmitted directly, with $M$ ‚Äúfrozen.‚Äù NIE‚àïTE measures the fraction of the response that may be transmitted through $M$ , with Y blinded to X. Consequently, the difference $( T E - N D E ) / T E$ measures the fraction of the response that is necessarily due to $M$ .

# Numerical example: Mediation with binary variables

To anchor these mediation formulas in a concrete example, we return to the encouragementdesign example of Section 4.2.3 and assume that $T = 1$ stands for participation in an enhanced training program, $Y = 1$ for passing the exam, and $M = 1$ for a student spending more than 3 hours per week on homework. Assume further that the data described in Tables 4.6 and 4.7 were obtained in a randomized trial with no mediator-to-outcome confounding (Figure 4.6(a)). The data shows that training tends to increase both the time spent on homework and the rate of success on the exam. Moreover, training and time spent on homework together are more likely to produce success than each factor alone.

Our research question asks for the extent to which students‚Äô homework contributes to their increased success rates regardless of the training program. The policy implications of such questions lie in evaluating policy options that either curtail or enhance homework efforts, for example, by counting homework effort in the -nal grade or by providing students with

Table 4.6 The expected success (Y) for treated $T = 1$ ) and untreated $( T = 0$ ) students, as a function of their homework $( M )$   

<table><tr><td>TreatmentT</td><td>HomeworkM</td><td>Success rateE(Y|T = t,M = m)</td></tr><tr><td>1</td><td>1</td><td>0.80</td></tr><tr><td>1</td><td>0</td><td>0.40</td></tr><tr><td>0</td><td>1</td><td>0.30</td></tr><tr><td>0</td><td>0</td><td>0.20</td></tr></table>

Table 4.7 The expected homework $( M )$ done by treated $( T = 1 )$ ) and untreated $( T = 0 )$ students   

<table><tr><td>TreatmentT</td><td>HomeworkE(M|T = t)</td></tr><tr><td>0</td><td>0.40</td></tr><tr><td>1</td><td>0.75</td></tr></table>

adequate work environments at home. An extreme explanation of the data, with signi-cant impact on educational policy, might argue that the program does not contribute substantively to students‚Äô success, save for encouraging students to spend more time on homework, an encouragement that could be obtained through less expensive means. Opposing this theory, we may have teachers who argue that the program‚Äôs success is substantive, achieved mainly due to the unique features of the curriculum covered, whereas the increase in homework efforts cannot alone account for the success observed.

Substituting the data into Eqs. (4.51) and (4.52) gives

$$
N D E = (0. 4 0 - 0. 2 0) (1 - 0. 4 0) + (0. 8 0 - 0. 3 0) 0. 4 0 = 0. 3 2
$$

$$
N I E = (0. 7 5 - 0. 4 0) (0. 3 0 - 0. 2 0) = 0. 0 3 5
$$

$$
T E = 0. 8 0 \times 0. 7 5 + 0. 4 0 \times 0. 2 5 - (0. 3 0 \times 0. 4 0 + 0. 2 0 \times 0. 6 0) = 0. 4 6
$$

$$
N I E / T E = 0. 0 7, N D E / T E = 0. 6 9 6, 1 - N D E / T E = 0. 3 0 4
$$

We conclude that the program as a whole has increased the success rate by $46 \%$ and that a signi-cant portion, $3 0 . 4 \%$ , of this increase is due to the capacity of the program to stimulate improved homework effort. At the same time, only $7 \%$ of the increase can be explained by stimulated homework alone without the bene-t of the program itself.

# Study questions

# Study question 4.5.2

Consider the structural model:

$$
y = \beta_ {1} m + \beta_ {2} t + u _ {y} \tag {4.53}
$$

$$
m = \gamma_ {1} t + u _ {m} \tag {4.54}
$$

(a) Use the basic de-nition of the natural effects (Eqs. (4.46) and (4.47)) to determine TE, NDE, and NIE.   
(b) Repeat (a) assuming that uy is correlated with $u _ { m }$ .

# Study question 4.5.3

Consider the structural model:

$$
y = \beta_ {1} m + \beta_ {2} t + \beta_ {3} t m + \beta_ {4} w + u _ {y} \tag {4.55}
$$

$$
m = \gamma_ {1} t + \gamma_ {2} w + u _ {m} \tag {4.56}
$$

$$
w = \alpha t + u _ {w} \tag {4.57}
$$

with ùõΩ3tm representing an interaction term.

![](images/c364948e0a0bf72d31aab767c6805ff8925e5baabd8e0a5915d674bb4fefec68.jpg)

(a) Use the basic de-nition of the natural effects (Eqs. (4.46) and (4.47)) (treating M as the mediator), to determine the portion of the effect for which mediation is necessary (TE ‚àí NDE) and the portion for which mediation is suf-cient (NIE). Hint: Show that:

$$
N D E = \beta_ {2} + \alpha \beta_ {4} \tag {4.58}
$$

$$
N I E = \beta_ {1} \left(\gamma_ {1} + \alpha \gamma_ {2}\right) \tag {4.59}
$$

$$
T E = \beta_ {2} + \left(\gamma_ {1} + \alpha \gamma_ {2}\right) \left(\beta_ {3} + \beta_ {1}\right) + \alpha \beta_ {4} \tag {4.60}
$$

$$
T E - N D E = \left(\beta_ {1} + \beta_ {3}\right) \left(\gamma_ {1} + \alpha \gamma_ {2}\right) \tag {4.61}
$$

(b) Repeat, using W as the mediator.

# Study question 4.5.4

Apply the mediation formulas provided in this section to the discrimination case discussed in Section 4.4.4, and determine the extent to which ABC International practiced discrimination in their hiring criteria. Use the data in Tables 4.6 and 4.7, with $T = 1$ standing for male applicants, $M = 1$ standing for highly quali-ed applicants, and $Y = 1$ standing for hiring. (Find the proportion of the hiring disparity that is due to gender, and the proportion that could be explained by disparity in quali-cation alone.)

# Ending Remarks

The analysis of mediation is perhaps the best arena to illustrate the effectiveness of the counterfactual-graphical symbiosis that we have been pursuing in this book. If we examine the identifying conditions A-1 to A-4, we -nd four assertions about the model that are not too easily comprehended. To judge their plausibility in any given scenario, without the graph before us, is unquestionably a formidable, superhuman task. Yet the symbiotic analysis frees investigators from the need to understand, articulate, examine, and judge the plausibility of the assumptions needed for identi-cation. Instead, the method can con-rm or discon-rm these assumptions algorithmically from a more reliable set of assumption, those encoded in the structural model itself. Once constructed, the causal diagram allows simple path-tracing routines to replace much of the human judgment deemed necessary in mediation analysis; the judgment invoked in the construction of the diagrams is suf-cient, and that construction requires only judgment about causal relationships among realizable variables and their disturbances.

# Bibliographical Notes for Chapter 4

The de-nition of counterfactuals as derivatives of structural equations, Eq. (4.5), was introduced by Balke and Pearl (1994a,b), who applied it to the estimation of probabilities of causation in legal settings. The philosopher David Lewis de-ned counterfactuals in terms of similarity among possible worlds Lewis (1973). In statistics, the notation $Y _ { x } ( u )$ was devised by Neyman (1923), to denote the potential response of unit $u$ in a controlled randomized trial,

under treatment $X = x$ . It remained relatively unnoticed until Rubin (1974) treated $Y _ { x }$ as a random variable and connected it to observed variable via the consistency rule of Eq. (4.6), which is a theorem in both Lewis‚Äôs logic and in structural models. The relationships among these three formalisms of counterfactuals are discussed at length in Pearl (2000, Chapter 7), where they are shown to be logically equivalent; a problem solved in one framework would yield the same solution in another. Rubin‚Äôs framework, known as ‚Äúpotential outcomes,‚Äù differs from the structural account only in the language in which problems are de-ned, hence, in the mathematical tools available for their solution. In the potential outcome framework, problems are de-ned algebraically as assumptions about counterfactual independencies, also known as ‚Äúignorability assumptions.‚Äù These types of assumptions, exempli-ed in Eq. (4.15), may become too complicated to interpret or verify by unaided judgment. In the structural framework, on the other hand, problems are de-ned in the form of causal graphs, from which dependencies of counterfactuals (e.g., Eq. (4.15)) can be derived mechanically. The reason some statisticians prefer the algebraic approach is, primarily, because graphs are relatively new to statistics. Recent books in social science (e.g., Morgan and Winship 2014) and in health science (e.g., VanderWeele 2015) are taking the hybrid, graph-counterfactual approach pursued in our book.

The section on linear counterfactuals is based on Pearl (2009, pp. 389‚Äì391). Recent advances are provided in Cai and Kuroki (2006) and Chen and Pearl (2014). Our discussion of ETT (Effect of Treatment on the Treated), as well as additive interventions, is based on Shpitser and Pearl (2009), which provides a full characterization of models in which ETT is identi-able.

Legal questions of attribution, as well as probabilities of causation are discussed at length in Greenland (1999) who pioneered the counterfactual approach to such questions. Our treatment of $P N , P S$ , and PNS is based on Tian and Pearl (2000) and Pearl (2000, Chapter 9). Recent results, including the tool kit of Section 4.5.1, are given in Pearl (2015a).

Mediation analysis (Sections 4.4.5 and 4.5.2), as we remarked in Chapter 3, has a long tradition in the social sciences (Duncan 1975; Kenny 1979), but has gone through a dramatic revolution through the introduction of counterfactual analysis. A historical account of the conceptual transition from the statistical approach of Baron and Kenny (1986) to the modern, counterfactual-based approach of natural direct and indirect effects (Pearl 2001; Robins and Greenland 1992) is given in Sections 1 and 2 of Pearl (2014a). The recent text of VanderWeele (2015) enhances this development with new results and new applications. Additional advances in mediation, including sensitivity analysis, bounds, multiple mediators, and stronger identifying assumptions are discussed in Imai et al. (2010) and Muth√©n and Asparouhov (2015).

The mediation tool kit of Section 4.5.2 is based on Pearl (2014a). Shpitser (2013) has derived a general criterion for identifying indirect effects in graphs.

# References

Balke A and Pearl J 1994a Counterfactual probabilities: Computational methods, bounds, and applications. In Uncertainty in Artificial Intelligence 10 (ed. de Mantaras RL and Poole D) Morgan Kaufmann Publishers, San Mateo, CA pp. 46‚Äì54.   
Balke A and Pearl J 1994b Probabilistic evaluation of counterfactual queries. Proceedings of the Twelfth National Conference on Arti-cial Intelligence, vol. I, MIT Press, Menlo Park, CA pp. 230‚Äì237.   
Bareinboim E and Pearl J 2012 Causal inference by surrogate experiments (or, z-identifiability). Proceedings of the Twenty-eighth Conference on Uncertainty in Arti-cial Intelligence (ed. de Freitas N and Murphy K) AUAI Press, Corvallis, OR, pp. 113‚Äì120.   
Bareinboim E and Pearl J 2013 A general algorithm for deciding transportability of experimental results. Journal of Causal Inference 1 (1), 107‚Äì134.   
Bareinboim E and Pearl J 2016 Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences 113 (17), 7345‚Äì7352.   
Bareinboim E, Tian J and Pearl J 2014 Recovering from selection bias in causal and statistical inference. Proceedings of the Twenty-eighth AAAI Conference on Artificial Intelligence (ed. Brodley CE and Stone P) AAAI Press, Palo Alto, CA, pp. 2410‚Äì2416.   
Baron R and Kenny D 1986 The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. Journal of Personality and Social Psychology 51 (6), 1173‚Äì1182.   
Berkson J 1946 Limitations of the application of fourfold table analysis to hospital data. Biometrics Bulletin 2, 47‚Äì53.   
Bollen K 1989 Structural Equations with Latent Variables. John Wiley & Sons, Inc., New York.   
Bollen K and Pearl J 2013 Eight myths about causality and structural equation models. In Handbook of Causal Analysis for Social Research (ed. Morgan S) Springer-Verlag, Dordrecht, Netherlands pp. 245‚Äì274.   
Bowden R and Turkington D 1984 Instrumental Variables. Cambridge University Press, Cambridge, England.   
Brito C and Pearl J 2002 Generalized instrumental variables. Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference (ed. Darwiche A and Friedman N) Morgan Kaufmann San Francisco, CA pp. 85‚Äì93.   
Cai Z and Kuroki M 2006 Variance estimators for three ‚Äòprobabilities of causation‚Äô. Risk Analysis 25 (6), 1611‚Äì1620.

Chen B and Pearl J 2014 Graphical tools for linear structural equation modeling. Technical Report R-432, Department of Computer Science, University of California, Los Angeles, CA. Submitted, Psychometrika, http://ftp.cs.ucla.edu/pub/stat_ser/r432.pdf.   
Cole S and Hern√°n M 2002 Fallibility in estimating direct effects. International Journal of Epidemiology 31 (1), 163‚Äì165.   
Conrady S and Jouffe L 2015 Bayesian Networks and BayesiaLab: A Practical Introduction for Researchers 1st edition edn. Bayesia USA.   
Cox D 1958 The Planning of Experiments. John Wiley and Sons, New York.   
Darwiche A 2009 Modeling and Reasoning with Bayesian Networks. Cambridge University Press, New York.   
Duncan O 1975 Introduction to Structural Equation Models. Academic Press, New York.   
Elwert F 2013 Graphical causal models. In Handbook of Causal Analysis for Social Research (ed. Morgan S) Springer-Verlag, Dordrecht, Netherlands pp. 245‚Äì274.   
Fenton N and Neil M 2013 Risk Assessment and Decision Analysis with Bayesian Networks. CRC Press, Boca Raton, FL.   
Fisher R 1922 On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society of London, Series A 222, 311.   
Fisher B, Anderson S, Bryant J, Margolese RG, Deutsch M, Fisher ER, Jeong JH and Wolmark N 2002 Twenty-year follow-up of a randomized trial comparing total mastectomy, lumpectomy, and lumpectomy plus irradiation for the treatment of invasive breast cancer. New England Journal of Medicine 347 (16), 1233‚Äì1241.   
Glymour MM 2006 Using causal diagrams to understand common problems in social epidemiology. Methods in Social Epidemiology John Wiley & Sons, Inc., San Francisco, CA pp. 393‚Äì428.   
Glymour M and Greenland S 2008 Causal diagrams. In Modern Epidemiology (ed. Rothman K, Greenland S, and Lash T) 3rd edn. Lippincott Williams & Wilkins Philadelphia, PA pp. 183‚Äì209.   
Greenland S 1999 Relation of probability of causation, relative risk, and doubling dose: A methodologic error that has become a social problem. American Journal of Public Health 89 (8), 1166‚Äì1169.   
Greenland S 2000 An introduction to instrumental variables for epidemiologists. International Journal of Epidemiology 29 (4), 722‚Äì729.   
Grinstead CM and Snell JL 1998 Introduction to Probability second revised edn. American Mathematical Society, United States.   
Haavelmo T 1943 The statistical implications of a system of simultaneous equations. Econometrica 11, 1‚Äì12. Reprinted in DF Hendry and MS Morgan (Eds.), 1995 The Foundations of Econometric Analysis, Cambridge University Press pp. 477‚Äì490.   
Hayduk L, Cummings G, Stratkotter R, Nimmo M, Grygoryev K, Dosman D, Gilespie, M., Pazderka-Robinson H and Boadu K 2003 Pearl‚Äôs d-separation: One more step into causal thinking. Structural Equation Modeling 10 (2), 289‚Äì311.   
Heise D 1975 Causal Analysis. John Wiley and Sons, New York.   
Hern√°n M and Robins J 2006 Estimating causal effects from epidemiological data. Journal of Epidemiology and Community Health 60 (7), 578‚Äì586. DOI: 10.1136/jech.2004.029496.   
Hern√°ndez-D√≠az S, Schisterman E and Hern√°n M 2006 The birth weight ‚Äúparadox‚Äù uncovered? American Journal of Epidemiology 164 (11), 1115‚Äì1120.   
Holland P 1986 Statistics and causal inference. Journal of the American Statistical Association 81 (396), 945‚Äì960.   
Howard R and Matheson J 1981 Influence diagrams. In Principles and Applications of Decision Analysis (ed. Howard R and Matheson J) Strategic Decisions Group, Menlo Park, CA pp.721‚Äì762.   
Imai K, Keele L and Yamamoto T 2010 Identi-cation, i nference, a nd s ensitivity a nalysis f or causal mediation effects. Statistical Science 25 (1), 51‚Äì71.   
Jewell NP 2004 Statistics for Epidemiology. Chapman & Hall/CRC, Boca Raton, FL.   
Kenny D 1979 Correlation and Causality. John Wiley & Sons, Inc., New York.

Kiiveri H, Speed T and Carlin J 1984 Recursive causal models. Journal of Australian Math Society 36, 30‚Äì52.   
Kim J and Pearl J 1983 A computational model for combined causal and diagnostic reasoning in inference systems. Proceedings of the Eighth International Joint Conference on Artificial Intelligence (IJCAI-83), pp. 190‚Äì193, Karlsruhe, Germany.   
Kline RB 2016 Principles and Practice of Structural Equation Modeling fourth: Revised and expanded edn. Guilford Publications, Inc., New York.   
Koller K and Friedman N 2009 Probabilistic Graphical Models: Principles and Techniques. MIT Press, United States.   
Kyono T 2010 Commentator: A front-end user-interface module for graphical and structural equation modeling. Master‚Äôs thesis Department of Computer Science, University of California, Los Angeles, CA.   
Lauritzen S 1996 Graphical Models. Clarendon Press, Oxford. Reprinted 2004 with corrections.   
Lewis D 1973 Causation. Journal of Philosophy 70, 556‚Äì567.   
Lindley DV 2014 Understanding Uncertainty revised edn. John Wiley & Sons, Inc., Hoboken, NJ.   
Lord FM 1967 A paradox in the interpretation of group comparisons. Psychological Bulletin 68, 304‚Äì305.   
Mohan K, Pearl J and Tian J 2013 Graphical models for inference with missing data. In Advances in Neural Information Processing Systems 26 (ed. Burges C, Bottou L, Welling M, Ghahramani Z and Weinberger K) Neural Information Processing Systems Foundation, Inc. pp. 1277‚Äì1285.   
Moore D, McCabe G and Craig B 2014 Introduction to the Practice of Statistics. W.H. Freeman & Co., New York.   
Morgan SL and Winship C 2014 Counterfactuals and Causal Inference: Methods and Principles for Social Research, Analytical Methods for Social Research 2nd edn. Cambridge University Press, New York.   
Muth√©n B and Asparouhov T 2015 Causal effects in mediation modeling: An introduction with applications to latent variables. Structural Equation Modeling: A Multidisciplinary Journal 22 (1), 12‚Äì23.   
Neyman J 1923 On the application of probability theory to agricultural experiments. Essay on principles. Section 9. Statistical Science 5 (4), 465‚Äì480.   
Pearl J 1985 Bayesian networks: A model of self-activated memory for evidential reasoning. Proceedings, Cognitive Science Society, pp. 329‚Äì334, Irvine, CA.   
Pearl J 1986 Fusion, propagation, and structuring in belief networks. Arti-cial Intelligence 29, 241‚Äì288.   
Pearl J 1988 Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San Mateo, CA.   
Pearl J 1993 Comment: Graphical models, causality, and intervention. Statistical Science 8 (3), 266‚Äì269.   
Pearl J 1995 Causal diagrams for empirical research. Biometrika 82 (4), 669‚Äì710.   
Pearl J 1998 Graphs, causality, and structural equation models. Sociological Methods and Research 27 (2), 226‚Äì284.   
Pearl J 2000 Causality: Models, Reasoning, and Inference. Cambridge University Press, New York.   
Pearl J 2001 Direct and indirect effects. Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence Morgan Kaufmann San Francisco, CA pp. 411‚Äì420.   
Pearl J 2009 Causality: Models, Reasoning, and Inference 2nd edn. Cambridge University Press, New York.   
Pearl J 2014a Interpretation and identification of causal mediation. Psychological Methods 19, 459‚Äì481.   
Pearl J 2014b Understanding Simpson‚Äôs paradox. The American Statistician 88 (1), 8‚Äì13.   
Pearl J 2015a Causes of effects and effects of causes. Journal of Sociological Methods and Research 44, 149‚Äì164.   
Pearl J 2015b Detecting latent heterogeneity. Sociological Methods and Research DOI: 10.1177/0049124115600597, online:1‚Äì20.

Pearl J 2015c Trygve Haavelmo and the emergence of causal calculus. Econometric Theory, Special issue on Haavelmo Centennial 31 (1), 152‚Äì179.   
Pearl J 2016 Lord‚Äôs paradox revisited‚Äî(oh Lord! Kumbaya!). Journal of Causal Inference 4 (2). DOI: 10.1515/jci-2016-0021.   
Pearl J and Bareinboim E 2014 External validity: From do-calculus to transportability across populations. Statistical Science 29 ,579‚Äì595.   
Pearl J and Mackenzie D 2018 The Book of Why: The New Science of Cause and Effect. Basic Books, New York.   
Pearl J and Paz A 1987 GRAPHOIDS: A graph-based logic for reasoning about relevance relations. In Advances in Arti-cial Intelligence-II (ed. Duboulay B, Hogg D and Steels L) North-Holland Publishing Co. pp. 357‚Äì363.   
Pearl J and Robins J 1995 Probabilistic evaluation of sequential plans from causal models with hidden variables. In Uncertainty in Artificial Intelligence 11 (ed. Besnard P and Hanks S) Morgan Kaufmann, San Francisco, CA pp. 444‚Äì453.   
Pearl J and Verma T 1991 A theory of inferred causation. Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference (ed. Allena J, Fikes R and Sandewall E) Morgan Kaufmann San Mateo, CA pp. 441‚Äì452.   
Pigou A 1911 Alcoholism and Heredity. Westminster Gazette. February 2.   
Rebane G and Pearl J 1987 The recovery of causal poly-trees from statistical data. Proceedings of the Third Workshop on Uncertainty in AI, pp. 222‚Äì228, Seattle, WA.   
Reichenbach H 1956 The Direction of Time. University of California Press, Berkeley, CA.   
Robertson D 1997 The common sense of cause in fact. Texas Law Review 75 (7), 1765‚Äì1800.   
Robins J 1986 A new approach to causal inference in mortality studies with a sustained exposure period‚Äîapplications to control of the healthy workers survivor effect. Mathematical Modeling 7, 1393‚Äì1512.   
Robins J and Greenland S 1992 Identi-ability and exchangeability for direct and indirect effects. Epidemiology 3 (2), 143‚Äì155.   
Rubin D 1974 Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology 66, 688‚Äì701.   
Selvin S 2004 Biostatistics: How it Works. Pearson, New Jersey.   
Senn S 2006 Change from baseline and analysis of covariance revisited. Statistics in Medicine 25, 4334‚Äì4344.   
Shpitser I 2013 Counterfactual graphical models for longitudinal mediation analysis with unobserved confounding. Cognitive Science 37 (6), 1011‚Äì1035.   
Shpitser I and Pearl J 2007 What counterfactuals can be tested. Proceedings of the Twenty-Third Conference on Uncertainty in Arti-cial Intelligence AUAI Press Vancouver, BC, Canada pp. 352‚Äì359. Also, Journal of Machine Learning Research 9, 1941‚Äì1979, 2008.   
Shpitser I and Pearl J 2008 Complete identi-cation methods for the causal hierarchy. Journal of Machine Learning Research 9, 1941‚Äì1979.   
Shpitser I and Pearl J 2009 Effects of treatment on the treated: Identification and generalization. Proceedings of the Twenty-Fifth Conference on Uncertainty in Arti-cial Intelligence AUAI Press Montreal, Quebec pp. 514‚Äì521.   
Simon H 1953 Causal ordering and identifiability. In Studies in Econometric Method (ed. Hood WC and Koopmans T) John Wiley & Sons, Inc. New York pp. 49‚Äì74.   
Simpson E 1951 The interpretation of interaction in contingency tables. Journal of the Royal Statistical Society, Series B 13, 238‚Äì241.   
Spirtes P and Glymour C 1991 An algorithm for fast recovery of sparse causal graphs. Social Science Computer Review 9 (1), 62‚Äì72.   
Spirtes P, Glymour C and Scheines R 1993 Causation, Prediction, and Search. Springer-Verlag, New York.   
Stigler SM 1999 Statistics on the Table: The History of Statistical Concepts and Methods. Harvard University Press, Cambridge, MA, Hoboken, NJ.

Strotz R and Wold H 1960 Recursive versus nonrecursive systems: An attempt at synthesis. Econometrica 28, 417‚Äì427.   
Textor J, Hardt J and Knu√ºppel S 2011 DAGitty: A graphical tool for analyzing causal diagrams. Epidemiology 22 (5), 745.   
Tian J, Paz A and Pearl J 1998 Finding minimal d-separators. Technical Report R-254, Department of Computer Science, University of California, Los Angeles, CA. http://ftp.cs.ucla.edu/pub/stat_ser/r254 .pdf.   
Tian J and Pearl J 2000 Probabilities of causation: bounds and identi-cation. Annals of Mathematics and Arti-cial Intelligence 28, 287‚Äì313.   
Tian J and Pearl J 2002 A general identification condition for causal effects. Proceedings of the Eighteenth National Conference on Arti-cial Intelligence AAAI Press/The MIT Press Menlo Park, CA pp. 567‚Äì573.   
VanderWeele T 2015 Explanation in Causal Inference: Methods for Mediation and Interaction. Oxford University Press, New York.   
Verma T and Pearl J 1988 Causal networks: Semantics and expressiveness. Proceedings of the Fourth Workshop on Uncertainty in Artificial Intelligence, pp. 352‚Äì359, Mountain View, CA. Also in R. Shachter, T.S. Levitt, and L.N. Kanal (Eds.), Uncertainty in AI 4, Elsevier Science Publishers, 69‚Äì76, 1990.   
Verma T and Pearl J 1990 Equivalence and synthesis of causal models. Proceedings of the Sixth Conference on Uncertainty in Arti-cial Intelligence, pp. 220‚Äì227, Cambridge, MA.   
Virgil 29 BC Georgics. Verse 490, Book 2.   
Wainer H 1991 Adjusting for differential base rates: Lord‚Äôs paradox again. Psychological Bulletin 109, 147‚Äì151.   
Wooldridge J 2013 Introductory Econometrics: A Modern Approach 5th international edn. South-Western, Mason, OH.

![](images/1adb965e8bf8feddd677b5cdd24c8e93c6aabce353f6c59c35e89cb66b9eb7ba.jpg)

![](images/b48b92143278097b27efc8336266583a83b4d9a3dcaa36a964b187b0917d8439.jpg)

![](images/6df469720db3a04d4421a67d00fa7a8b8a7cae391ea4975d2c45f14d5432ba2f.jpg)

![](images/2518d2d437da635228e64ee4ca3393ad528a43d14843c0bb087ed137dd2bf7a7.jpg)

![](images/35f5ae850a19ce00851fb9063b260bea130e367314949ad6a95a7736e91d5287.jpg)

![](images/1d4ae9ba3e18995564900621abe394790c07f228d2c89031023680a37cb743ce.jpg)

# Index

actions

conditional, 70‚Äì71

and counterfactuals, 90‚Äì91, 96‚Äì98

effect of, 53‚Äì58

vs. observations, 53‚Äì55, 61, 78

sequential, 77‚Äì78

as surgeries, 54‚Äì56, 82

see also intervention

adjacency (in graphs), 25

adjustment formula, 55‚Äì60

see also confounding bias

attribution, 111‚Äì112, 116‚Äì120

attributable risk, 117

bounds on, 117‚Äì118

backdoor criterion, 61‚Äì66

adjustment using, 61‚Äì64

and ignorability, 102‚Äì103, 125‚Äì126

intuition, 61

for parameter identi-cation, 84‚Äì86

sequential, 77‚Äì78

Bayes‚Äô conditionalization, 13, 102‚Äì103

as -ltering, 8‚Äì10

Bayes‚Äô rule, 12‚Äì15

Bayesian inference, 15

Bayesian networks, 33

Berkson‚Äôs paradox, 51

blocking, 46

bounds

on counterfactual probabilities, 119

causal assumptions, 5, 26‚Äì29

testable implications of, 48‚Äì51

causal effect

adjustment formula for, 55‚Äì60

de-nition of, 55

identi-cation of, 55‚Äì60

parametric estimation for, 83‚Äì87

on the treated, 106‚Äì109

causal models, 26‚Äì29

nonparametric, 78

structural, 26‚Äì29

testing, 48‚Äì51

causal search, 48‚Äì51

causation

and counterfactuals, 89‚Äì94, 101‚Äì102

and the law, 113, 117, 119‚Äì120

and manipulation, 53‚Äì58

structural foundations, 93‚Äì94, 101‚Äì102

working de-nition, 5‚Äì6

collider, 40‚Äì48, 51, 76, 103

adjustment for, 63, 77

conditional independence, 10

de-nition, 10

graphical representation of, 38‚Äì49

conditioning

as -ltering, 8

vs. interviewing, 53‚Äì55

confounders, 76‚Äì78

confounding bias

control of, 53‚Äì60

correlation, 10

coef-cient, 18

partial, 23

test for zero partial, 50‚Äì51, 82

counterfactuals, 89‚Äì126

computation of, 93‚Äì96

de-nition, 94

and experiments, 103‚Äì105

graphical representation of, 101‚Äì103

independence of, 102‚Äì104

and legal responsibility, 113, 119‚Äì120

in linear systems, 106‚Äì107

notation, 89‚Äì90

and policy analysis, 107‚Äì111, 114‚Äì116

probability of, 98‚Äì101

as random variables, 98

structural interpretation, 91‚Äì93, 102‚Äì103

covariates

adjustment for, 55‚Äì60

treatment-dependent, 4, 66‚Äì69,

75‚Äì78

DAGs (directed acyclic graphs), 27

observational equivalence of, 50‚Äì51

decomposition

product, 29

truncated, 60‚Äì61

direct effects, 75‚Äì78

controlled, 75‚Äì78, 121

de-nition, 78, 123

examples, 76‚Äì78, 123‚Äì124

identi-cation (nonparametric), 76‚Äì78, 123‚Äì124

identi-cation (parametric), 81‚Äì86

natural, 115, 121‚Äì126

do-calculus, 69, 88

do(‚ãÖ) operator, 55

$d$ -separation, 45‚Äì48

and conditional independence, 47‚Äì48

de-nition, 46

and model testing, 48‚Äì50

and zero partial correlations, 45, 49, 51

edges, 24‚Äì25

double arrow, 85‚Äì86

effects

covariate-speci-c, 70‚Äì72

indirect, 76‚Äì77, 114‚Äì116, 121‚Äì126

natural, 121‚Äì126

see also direct effects

equivalent models

testing for, 50

error terms, 28‚Äì29

ETT (effect of treatment on the treated), 106‚Äì110

examples, 107, 109

examples

about to smoke, should I?, 111

baseball batting average, 6

casino crap game, 13‚Äì15, 19

drug, blood-pressure, recovery, 4, 58

drug, gender, recovery, 2

drug, weight, recovery, 62

education, skill, and salary, 99‚Äì101, 103

exercise and cholesterol, 3‚Äì4

freeway or Sepulveda?, 89

gender and education, 9‚Äì10

gender-blind hiring, 114

homework and exams, 94, 123‚Äì124

ice cream and crime, 54

insulin increment, 97, 109

kidney stones, 6

legal liability of drug maker, 119

lollipop and confounding, 6‚Äì8, 65

lumpectomy and irradiation, 111

Monty Hall, 14‚Äì16, 32,

recruitment to a program, 107

schooling, employment, salary, 27

sex discrimination in hiring, 113, 125

smoking, tar, and cancer, 66‚Äì68

syndrome-affected drug, 31‚Äì32

three cards, 16

two casino players, 19

two coins and a bell, 42‚Äì44

two dice rolls, 21‚Äì22

two surgents, 6

weight gain and diet, 65

excess risk ratio (ERR), 117‚Äì120

corrected for confounding, 120

exogenous

variables, 27‚Äì29

expectation, 16

conditional, 17, 20

explaining away, 51

front-door criterion, 66‚Äì71

example, 69‚Äì70

graphs, 24‚Äì26

complete, 25

cyclic, 25‚Äì26

directed, 25

as models of intervention, 53‚Äì55

observationally equivalent, 50

identi-cation, 59‚Äì66, 122‚Äì123

causal effects rule for, 59

of direct effects, 77, 84‚Äì86, 122‚Äì124

proportion explained by, 123‚Äì124

proportion due to, 123‚Äì124

ignorability, 103‚Äì104, 126

and backdoor criterion, 103‚Äì104

independence, 10

conditional, 10

indirect effects, 76‚Äì77, 87, 114‚Äì116, 121‚Äì126

instrumental variables, 86‚Äì88

interaction, 78, 124

intervention, 53‚Äì55

conditional, 70‚Äì71, 88

multiple, 60‚Äì61

notation, 55

truncated product rule for, 60‚Äì61

see also actions

inverse probability weighting, 72‚Äì75

Lord‚Äôs paradox, 65, 87

manipulated graph, 56, 88

mediation

see also indirect effects

Mediation Formula, 116, 123, 125

Neyman‚ÄìRubin model, 125‚Äì126

see also potential-outcome framework

parents

in graphs, 25‚Äì26

path coef-cients, 80, 82‚Äì83

identi-cation, 83‚Äì87

see also structural parameters

paths

backdoor, 61‚Äì66

blocked, 46

directed, 25

in graphs, 25

potential-outcome, 102, 104‚Äì105, 126

probability, 7‚Äì16

conditional, 8‚Äì9

joint, 11

marginal, 12

probabilities of causation, 111‚Äì112, 116‚Äì120, 125

bounds, 117‚Äì120

de-nition, 116

probability of necessity (PN), 112, 116‚Äì120, 126

probability of suf-ciency (PS), 112

product decomposition, 29‚Äì31

propensity score, 59, 72‚Äì73

randomized experiments, 6, 53, 57, 103‚Äì105

regression, 20‚Äì22

coef-cients, 23, 79‚Äì81

multiple, 22‚Äì24

partial, 23‚Äì24, 80

rule (for identi-cation), 84‚Äì85

risk difference, 60

root nodes, 27

sampling

variability, 21, 50, 57

SCM (structural causal models), 26‚Äì30

selection bias, 51

SEM (structural equation modeling), 26, 35, 92, 95

see also SCM

sequential backdoor, 77‚Äì78

Simpson‚Äôs paradox, 1‚Äì7, 23‚Äì24, 32, 44, 55‚Äì59, 65, 73

examples of, 6‚Äì7, 73

Simpson‚Äôs paradox (continued)

history of, 32

lessons from, 24

nonstatistical character of, 24

resolution of, 32, 55‚Äì59

single-world, 100

spurious association, 61, 63, 66

statistical vs. causal concepts, 1, 7, 24, 80‚Äì82

structural equations, 26, 80‚Äì81

interventional interpretation, 81‚Äì83

nonlinear, 107

nonparametric, 26‚Äì28

total effect, 53‚Äì55, 81‚Äì83

see also causal effect

truncated product, 60, 88

variable

instrumental, 85‚Äì88

omitted, 28, 81

random, 7

variance, 17‚Äì19

v-structure, 50