# Jonathan D. Cryer Kung-Sik Chan

# Time Series Analysis

# With Applications in R

![](images/cdad8c29342208b69119607a581c0140cae0f2a812f75b80be4d4dc2778d7540.jpg)

Second Edition

# Statistics Texts in Statistics

Series Editors:

G. Casella   
S. Fienberg   
I. Olkin

# Springer Texts in Statistics

Athreya/Lahiri: Measure Theory and Probability Theory

Bilodeau/Brenner: Theory of Multivariate Statistics

Brockwell/Davis: An Introduction to Time Series and Forecasting

Carmona: Statistical Analysis of Financial Data in S-PLUS

Chow/Teicher: Probability Theory: Independence, Interchangeability, Martingales, $3 ^ { \mathrm { r d } }$ ed.

Christensen: Advanced Linear Modeling: Multivariate, Time Series, and Spatial Data;

Nonparametric Regression and Response Surface Maximization, $2 ^ { \mathrm { n d } }$ ed.

Christensen: Log-Linear Models and Logistic Regression, $2 ^ { \mathrm { n d } }$ ed.

Christensen: Plane Answers to Complex Questions: The Theory of Linear Models, $2 ^ { \mathrm { n d } }$ ed.

Cryer/Chan: Time Series Analysis, Second Edition

Davis: Statistical Methods for the Analysis of Repeated Measurements

Dean/Voss: Design and Analysis of Experiments

Dekking/Kraaikamp/Lopuhaä/Meester: A Modern Introduction to Probability and Statistics

Durrett: Essential of Stochastic Processes

Edwards: Introduction to Graphical Modeling, $2 ^ { \mathrm { n d } }$ ed.

Everitt: An R and S-PLUS Companion to Multivariate Analysis

Gentle: Matrix Algebra: Theory, Computations, and Applications in Statistics

Ghosh/Delampady/Samanta: An Introduction to Bayesian Analysis

Gut: Probability: A Graduate Course

Heiberger/Holland: Statistical Analysis and Data Display; An Intermediate Course with Examples in S-PLUS, R, and SAS

Jobson: Applied Multivariate Data Analysis, Volume I: Regression and Experimental Design

Jobson: Applied Multivariate Data Analysis, Volume II: Categorical and Multivariate Methods

Karr: Probability

Kulkarni: Modeling, Analysis, Design, and Control of Stochastic Systems

Lange: Applied Probability

Lange: Optimization

Lehmann: Elements of Large Sample Theory

Lehmann/Romano: Testing Statistical Hypotheses, $3 ^ { \mathrm { r d } }$ ed.

Lehmann/Casella: Theory of Point Estimation, $2 ^ { \mathrm { n d } }$ ed.

Longford: Studying Human Popluations: An Advanced Course in Statistics

Marin/Robert: Bayesian Core: A Practical Approach to Computational Bayesian Statistics

Nolan/Speed: Stat Labs: Mathematical Statistics Through Applications

Pitman: Probability

Rawlings/Pantula/Dickey: Applied Regression Analysis

Robert: The Bayesian Choice: From Decision-Theoretic Foundations to Computational

Implementation, $2 ^ { \mathrm { n d } }$ ed.

Robert/Casella: Monte Carlo Statistical Methods, $2 ^ { \mathrm { n d } }$ ed.

Rose/Smith: Mathematical Statistics with Mathematica

Ruppert: Statistics and Finance: An Introduction

Sen/Srivastava: Regression Analysis: Theory, Methods, and Applications.

Shao: Mathematical Statistics, $2 ^ { \mathrm { n d } }$ ed.

Shorack: Probability for Statisticians

Shumway/Stoffer: Time Series Analysis and Its Applications, $2 ^ { \mathrm { n d } }$ ed.

Simonoff: Analyzing Categorical Data

Terrell: Mathematical Statistics: A Unified Introduction

Timm: Applied Multivariate Analysis

Toutenberg: Statistical Analysis of Designed Experiments, $2 ^ { \mathrm { n d } }$ ed.

Wasserman: All of Nonparametric Statistics

Wasserman: All of Statistics: A Concise Course in Statistical Inference

Weiss: Modeling Longitudinal Data

Whittle: Probability via Expectation, $4 ^ { \mathrm { t h } }$ ed.

# Time Series Analysis

With Applications in R Second Edition

Jonathan D. Cryer

Department of Statistics & Actuarial Science

University of Iowa

Iowa City, Iowa 52242

USA

jon-cryer@uiowa.edu

Kung-Sik Chan

Department of Statistics & Actuarial Science

University of Iowa

Iowa City, Iowa 52242

USA

kung-sik-chan@uiowa.edu

Series Editors:

George Casella

Department of Statistics

University of Florida

Gainesville, FL 32611-8545

USA

Stephen Fienberg

Department of Statistics

Carnegie Mellon University

Pittsburgh, PA 15213-3890

USA

Ingram Okin

Department of Statistics

Stanford University

Stanford, CA 94305

USA

ISBN: 978-0-387-75958-6

e-ISBN: 978-0-387-75959-3

Library of Congress Control Number: 2008923058

© 2008 Springer Science+Business Media, LLC

All rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.

The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.

Printed on acid-free paper.

9 8 7 6 5 4 3 2 (Corrected at second printing, 2008)

springer.com

To our families

# PREFACE

The theory and practice of time series analysis have developed rapidly since the appearance in 1970 of the seminal work of George E. P. Box and Gwilym M. Jenkins, Time Series Analysis: Forecasting and Control, now available in its third edition (1994) with co-author Gregory C. Reinsel. Many books on time series have appeared since then, but some of them give too little practical application, while others give too little theoretical background. This book attempts to present both application and theory at a level accessible to a wide variety of students and practitioners. Our approach is to mix application and theory throughout the book as they are naturally needed.

The book was developed for a one-semester course usually attended by students in statistics, economics, business, engineering, and quantitative social sciences. Basic applied statistics through multiple linear regression is assumed. Calculus is assumed only to the extent of minimizing sums of squares, but a calculus-based introduction to statistics is necessary for a thorough understanding of some of the theory. However, required facts concerning expectation, variance, covariance, and correlation are reviewed in appendices. Also, conditional expectation properties and minimum mean square error prediction are developed in appendices. Actual time series data drawn from various disciplines are used throughout the book to illustrate the methodology. The book contains additional topics of a more advanced nature that can be selected for inclusion in a course if the instructor so chooses.

All of the plots and numerical output displayed in the book have been produced with the R software, which is available from the R Project for Statistical Computing at www.r-project.org. Some of the numerical output has been edited for additional clarity or for simplicity. R is available as free software under the terms of the Free Software Foundation's GNU General Public License in source code form. It runs on a wide variety of UNIX platforms and similar systems, Windows, and MacOS.

R is a language and environment for statistical computing and graphics, provides a wide variety of statistical (e.g., time-series analysis, linear and nonlinear modeling, classical statistical tests) and graphical techniques, and is highly extensible. The extensive appendix An Introduction to R, provides an introduction to the R software specially designed to go with this book. One of the authors (KSC) has produced a large number of new or enhanced R functions specifically tailored to the methods described in this book. They are listed on page 468 and are available in the package named TSA on the R Project’s Website at www.r-project.org. We have also constructed R command script files for each chapter. These are available for download at www.stat.uiowa.edu/ ~kchan/TSA.htm. We also show the required R code beneath nearly every table and graphical display in the book. The datasets required for the exercises are named in each exercise by an appropriate filename; for example, larain for the Los Angeles rainfall data. However, if you are using the TSA package, the datasets are part of the package and may be accessed through the R command data(larain), for example.

All of the datasets are also available at the textbook website as ASCII files with variable names in the first row. We believe that many of the plots and calculations

described in the book could also be obtained with other software, such as SAS©, Splus©, Statgraphics©, $\operatorname { S C A } ^ { \mathbb { \odot } }$ , EViews©, RATS©, $\operatorname { O x } ^ { \circledcirc }$ , and others.

This book is a second edition of the book Time Series Analysis by Jonathan Cryer, published in 1986 by PWS-Kent Publishing (Duxbury Press). This new edition contains nearly all of the well-received original in addition to considerable new material, numerous new datasets, and new exercises. Some of the new topics that are integrated with the original include unit root tests, extended autocorrelation functions, subset ARIMA models, and bootstrapping. Completely new chapters cover the topics of time series regression models, time series models of heteroscedasticity, spectral analysis, and threshold models. Although the level of difficulty in these new chapters is somewhat higher than in the more basic material, we believe that the discussion is presented in a way that will make the material accessible and quite useful to a broad audience of users. Chapter 15, Threshold Models, is placed last since it is the only chapter that deals with nonlinear time series models. It could be covered earlier, say after Chapter 12. Also, Chapters 13 and 14 on spectral analysis could be covered after Chapter 10.

We would like to thank John Kimmel, Executive Editor, Statistics, at Springer, for his continuing interest and guidance during the long preparation of the manuscript. Professor Howell Tong of the London School of Economics, Professor Henghsiu Tsai of Academica Sinica, Taipei, Professor Noelle Samia of Northwestern University, Professor W. K. Li and Professor Kai W. Ng, both of the University of Hong Kong, and Professor Nils Christian Stenseth of the University of Oslo kindly read parts of the manuscript, and Professor Jun Yan used a preliminary version of the text for a class at the University of Iowa. Their constructive comments are greatly appreciated. We would like to thank Samuel Hao who helped with the exercise solutions and read the appendix: An Introduction to R. We would also like to thank several anonymous reviewers who read the manuscript at various stages. Their reviews led to a much improved book. Finally, one of the authors (JDC) would like to thank Dan, Marian, and Gene for providing such a great place, Casa de Artes, Club Santiago, Mexico, for working on the first draft of much of this new edition.

Iowa City, Iowa January 2008

Jonathan D. Cryer Kung-Sik Chan

# CONTENTS

# CHAPTER 1 INTRODUCTION . . . . . .

1.1 Examples of Time Series . . .   
1.2 A Model-Building Strategy . . . 8   
1.3 Time Series Plots in History . . . . 8   
1.4 An Overview of the Book . . . 9

Exercises . . 10

# CHAPTER 2 FUNDAMENTAL CONCEPTS . . . . . . . . . . 11

2.1 Time Series and Stochastic Processes . . . . . . . . 11   
2.2 Means, Variances, and Covariances . . 11   
2.3 Stationarity . . . 16   
2.4 Summary . . . 19

Exercises . . 19

Appendix A: Expectation, Variance, Covariance, and Correlation . 24

# CHAPTER 3 TRENDS . . . . 27

3.1 Deterministic Versus Stochastic Trends . . . . . . 27   
3.2 Estimation of a Constant Mean . . 28   
3.3 Regression Methods. . . 30   
3.4 Reliability and Efficiency of Regression Estimates . . . . . . . . 36   
3.5 Interpreting Regression Output . . 40   
3.6 Residual Analysis . . . 42   
3.7 Summary . . . 50

Exercises . . 50

# CHAPTER 4 MODELS FOR STATIONARY TIME SERIES. . . . . 55

4.1 General Linear Processes . . . 55   
4.2 Moving Average Processes . . 57   
4.3 Autoregressive Processes . . 66   
4.4 The Mixed Autoregressive Moving Average Model. . . . . . . . 77   
4.5 Invertibility. . . 79   
4.6 Summary . . . 80

Exercises . . 81

Appendix B: The Stationarity Region for an AR(2) Process . . . . . 84

Appendix C: The Autocorrelation Function for ARMA(p,q). . . . . . . 85

# CHAPTER 5 MODELS FOR NONSTATIONARY TIME SERIES .87

5.1 Stationarity Through Differencing . . . . . . . 88   
5.2 ARIMA Models . . . . . . 92   
5.3 Constant Terms in ARIMA Models. . . . . . . 97   
5.4 Other Transformations . . . . . . 98   
5.5 Summary . . . . . 102

Exercises. . . . 103

Appendix D: The Backshift Operator. . . . . . . 106

# CHAPTER 6 MODEL SPECIFICATION . . . . . . . . 109

6.1 Properties of the Sample Autocorrelation Function . . . . . . . 109   
6.2 The Partial and Extended Autocorrelation Functions . . . . .112   
6.3 Specification of Some Simulated Time Series . . . . . . . . . . . 117   
6.4 Nonstationarity. . . . . . 125   
6.5 Other Specification Methods . . . . . . . . 130   
6.6 Specification of Some Actual Time Series. . . . . . . 133   
6.7 Summary . . . . . 141

Exercises. . . . . 141

# CHAPTER 7 PARAMETER ESTIMATION . . . . . . . . . 149

7.1 The Method of Moments . . . . . . . 149   
7.2 Least Squares Estimation . . . . . . . 154   
7.3 Maximum Likelihood and Unconditional Least Squares . . .158   
7.4 Properties of the Estimates . . . . . 160   
7.5 Illustrations of Parameter Estimation . . . . . . . 163   
7.6 Bootstrapping ARIMA Models . . . . . 167   
7.7 Summary . . . . . .170

Exercises. . . . . 170

# CHAPTER 8 MODEL DIAGNOSTICS . . . . . . . .175

8.1 Residual Analysis . . 175   
8.2 Overfitting and Parameter Redundancy. . . . . . 185   
8.3 Summary . . . . . 188

Exercises. . . . 188

# CHAPTER 9 FORECASTING. . . . . . . 191

9.1 Minimum Mean Square Error Forecasting . . . . . . . . . . . . . 191   
9.2 Deterministic Trends . . . . . 191   
9.3 ARIMA Forecasting . 193   
9.4 Prediction Limits . . . . . 203   
9.5 Forecasting Illustrations . . . . 204   
9.6 Updating ARIMA Forecasts . . . . . 207   
9.7 Forecast Weights and Exponentially Weighted Moving Averages 207   
9.8 Forecasting Transformed Series. . . . . . 209   
9.9 Summary of Forecasting with Certain ARIMA Models . . . . 211   
9.10 Summary . . . . 213   
Exercises . . . . 213   
Appendix E: Conditional Expectation. . . . . . . 218   
Appendix F: Minimum Mean Square Error Prediction . . . . . . . . . 218   
Appendix G: The Truncated Linear Process . . . . . 221   
Appendix H: State Space Models . . . . . 222

# CHAPTER 10 SEASONAL MODELS . . . . . . 227

10.1 Seasonal ARIMA Models . . . . . 228   
10.2 Multiplicative Seasonal ARMA Models . . . . . . 230   
10.3 Nonstationary Seasonal ARIMA Models . . . . . . 233   
10.4 Model Specification, Fitting, and Checking. . . . . . . . . 234   
10.5 Forecasting Seasonal Models . . . . . 241   
10.6 Summary . . 246   
Exercises . 246

# CHAPTER 11 TIME SERIES REGRESSION MODELS . . . . . . 249

11.1 Intervention Analysis . . . 249   
11.2 Outliers . . 257   
11.3 Spurious Correlation. . . . . 260   
11.4 Prewhitening and Stochastic Regression . . . . . . . 265   
11.5 Summary . . . . . 273   
Exercises . 274

# CHAPTER 12 TIME SERIES MODELS OF HETEROSCEDASTICITY. .277

12.1 Some Common Features of Financial Time Series . . . . . . .278   
12.2 The ARCH(1) Model . . . . . . 285   
12.3 GARCH Models . . . . . . 289   
12.4 Maximum Likelihood Estimation . . 298   
12.5 Model Diagnostics . . . . . 301   
12.6 Conditions for the Nonnegativity of the Conditional Variances . 307   
12.7 Some Extensions of the GARCH Model . . . . . . . . . . . . . . . 310   
12.8 Another Example: The Daily USD/HKD Exchange Rates . .311   
12.9 Summary . . . . . 315

Exercises. . . . 316

Appendix I: Formulas for the Generalized Portmanteau Tests . . .318

# CHAPTER 13 INTRODUCTION TO SPECTRAL ANALYSIS. . . .319

13.1 Introduction . . . . . . .319   
13.2 The Periodogram . . . . . . 322   
13.3 The Spectral Representation and Spectral Distribution. . . .327   
13.4 The Spectral Density . . . . . 330   
13.5 Spectral Densities for ARMA Processes . . . . . . . . . . . . . . . 332   
13.6 Sampling Properties of the Sample Spectral Density . . . . .340   
13.7 Summary . . . . . 346

Exercises. . . . 346

Appendix J: Orthogonality of Cosine and Sine Sequences . . . . .349

# CHAPTER 14 ESTIMATING THE SPECTRUM . . . . . . . . . . . . . . 351

14.1 Smoothing the Spectral Density . . . . . . . . 351   
14.2 Bias and Variance . . . . 354   
14.3 Bandwidth . . . . . 355   
14.4 Confidence Intervals for the Spectrum . . . . . . . . 356   
14.5 Leakage and Tapering . . . . . 358   
14.6 Autoregressive Spectrum Estimation. . . . . 363   
14.7 Examples with Simulated Data . . . . . . 364   
14.8 Examples with Actual Data . . . . . 370   
14.9 Other Methods of Spectral Estimation . . . . . 376   
14.10Summary . . . . . 378   
Exercises. . . . . 378   
Appendix K: Tapering and the Dirichlet Kernel . . . . . . 381

# CHAPTER 15 THRESHOLD MODELS . . . . . . . . . . 383

15.1 Graphically Exploring Nonlinearity . . . . . . . . . . 384   
15.2 Tests for Nonlinearity . . . . . 390   
15.3 Polynomial Models Are Generally Explosive . . . . . . . . . . . 393   
15.4 First-Order Threshold Autoregressive Models . . . . . . . . . . 395   
15.5 Threshold Models. . . 399   
15.6 Testing for Threshold Nonlinearity . . . . . 400   
15.7 Estimation of a TAR Model . . . 402   
15.8 Model Diagnostics . . . 411   
15.9 Prediction . . . . . 415   
15.10 Summary . . . . . . 420

Exercises . . 420

Appendix L: The Generalized Portmanteau Test for TAR . . . . . . 421

# CHAPTER 16 APPENDIX: AN INTRODUCTION TO R. . . . . . . 423

Introduction . . 423

Chapter 1 R Commands . . . 429

Chapter 2 R Commands . . . . . 433

Chapter 3 R Commands . . . . . 433

Chapter 4 R Commands . 438

Chapter 5 R Commands 439

Chapter 6 R Commands . . 441

Chapter 7 R Commands . . 442

Chapter 8 R Commands . . . . 446

Chapter 9 R Commands . . 447

Chapter 10 R Commands . . 450

Chapter 11 R Commands . . . . . 451

Chapter 12 R Commands . 457

Chapter 13 R Commands . . 460

Chapter 14 R Commands . . 461

Chapter 15 R Commands 462

New or Enhanced Functions in the TSA Library . . . . . . 468

# DATASET INFORMATION . . . . . . . . 471

# BIBLIOGRAPHY . . . 477

# INDEX . . . . 487

# CHAPTER 1 INTRODUCTION

Data obtained from observations collected sequentially over time are extremely common. In business, we observe weekly interest rates, daily closing stock prices, monthly price indices, yearly sales figures, and so forth. In meteorology, we observe daily high and low temperatures, annual precipitation and drought indices, and hourly wind speeds. In agriculture, we record annual figures for crop and livestock production, soil erosion, and export sales. In the biological sciences, we observe the electrical activity of the heart at millisecond intervals. In ecology, we record the abundance of an animal species. The list of areas in which time series are studied is virtually endless. The purpose of time series analysis is generally twofold: to understand or model the stochastic mechanism that gives rise to an observed series and to predict or forecast the future values of a series based on the history of that series and, possibly, other related series or factors.

This chapter will introduce a variety of examples of time series from diverse areas of application. A somewhat unique feature of time series and their models is that we usually cannot assume that the observations arise independently from a common population (or from populations with different means, for example). Studying models that incorporate dependence is the key concept in time series analysis.

# 1.1 Examples of Time Series

In this section, we introduce a number of examples that will be pursued in later chapters.

# Annual Rainfall in Los Angeles

Exhibit 1.1 displays a time series plot of the annual rainfall amounts recorded in Los Angeles, California, over more than 100 years. The plot shows considerable variation in rainfall amount over the years — some years are low, some high, and many are in-between in value. The year 1883 was an exceptionally wet year for Los Angeles, while 1983 was quite dry. For analysis and modeling purposes we are interested in whether or not consecutive years are related in some way. If so, we might be able to use one year’s rainfall value to help forecast next year’s rainfall amount. One graphical way to investigate that question is to pair up consecutive rainfall values and plot the resulting scatterplot of pairs.

Exhibit 1.2 shows such a scatterplot for rainfall. For example, the point plotted near the lower right-hand corner shows that the year of extremely high rainfall, 40 inches in 1883, was followed by a middle of the road amount (about 12 inches) in 1884. The point

near the top of the display shows that the 40 inch year was preceded by a much more typical year of about 15 inches.

![](images/b64ef0244b9d7413fad9a63a4d1660eda869613e816913a281dd70ff66781fb4.jpg)  
Exhibit 1.1 Time Series Plot of Los Angeles Annual Rainfall

```txt
> library(TSA)  
> win.graph(width=4.875, height=2.5,pointsize=8)  
> data(larain); plot(larain,ylab='Inches',xlab='Year',type='o') 
```

![](images/a69d4b4cc728c2d547410838c65d65dff5f8fd0ee1ad7f87a7a54cfa1348d61c.jpg)  
Exhibit 1.2 Scatterplot of LA Rainfall versus Last Year’s LA Rainfall

```txt
> win.graph(width=3,height=3,pointsize=8)  
> plot(y=larain,x=zlag(larain),ylab='Inches', xlab='Previous Year Inches') 
```

The main impression that we obtain from this plot is that there is little if any information about this year’s rainfall amount from last year’s amount. The plot shows no “trends” and no general tendencies. There is little correlation between last year’s rainfall amount and this year’s amount. From a modeling or forecasting point of view, this is not a very interesting time series!

# An Industrial Chemical Process

As a second example, we consider a time series from an industrial chemical process. The variable measured here is a color property from consecutive batches in the process. Exhibit 1.3 shows a time series plot of these color values. Here values that are neighbors in time tend to be similar in size. It seems that neighbors are related to one another.

![](images/b23de112dbf7216d96ba3cdc63b891c67869e92bdcd6aab8341f68b5c25052fd.jpg)  
Exhibit 1.3 Time Series Plot of Color Property from a Chemical Process

```txt
> win.graph(width=4.875, height=2.5,pointsize=8)  
> data(color)  
> plot(color,ylab='Color Property',xlabel='Batch',type='o') 
```

This can be seen better by constructing the scatterplot of neighboring pairs as we did with the first example.

Exhibit 1.4 displays the scatterplot of the neighboring pairs of color values. We see a slight upward trend in this plot—low values tend to be followed in the next batch by low values, middle-sized values tend to be followed by middle-sized values, and high values tend to be followed by high values. The trend is apparent but is not terribly strong. For example, the correlation in this scatterplot is about 0.6.

Exhibit 1.4 Scatterplot of Color Value versus Previous Color Value   
![](images/c00ee0d55694178f8a671c1f50c545e26912ead77fd3ec6215f7b48e550cfaf2.jpg)  
> win.graph(width $\mathord { \left. \kern - delimiterspace \right.} 2 $ ,height=3,pointsize=8) > plot(y=color,x=zlag(color),ylab $^ { 1 = }$ 'Color Property', xlab='Previous Batch Color Property')

# Annual Abundance of Canadian Hare

Our third example concerns the annual abundance of Canadian hare. Exhibit 1.5 gives the time series plot of this abundance over about 30 years. Neighboring values here are very closely related. Large changes in abundance do not occur from one year to the next. This neighboring correlation is seen clearly in Exhibit 1.6 where we have plotted abundance versus the previous year’s abundance. As in the previous example, we see an upward trend in the plot—low values tend to be followed by low values in the next year, middle-sized values by middle-sized values, and high values by high values.

Exhibit 1.5 Abundance of Canadian Hare   
![](images/4188d79c04e227b8b121a79911c82f9ffca0602fc5bb84e9906a8dfea7bcfc10.jpg)  
> win.graph(width=4.875, height=2.5,pointsize=8)   
> data(hare); plot(hare,ylab $^ { 1 = }$ 'Abundance',xlab='Year',type='o')

![](images/89fd47c43a3f1211966cb8df5f90ee4d04da225ad0f35713d3f3de42591c20ba.jpg)  
Exhibit 1.6 Hare Abundance versus Previous Year’s Hare Abundance

```txt
> win.graph(width=3, height=3, pointsize=8) 
```

```javascript
> plot(y=hare, x=zlag(hare), ylab='Abundance', 
```

```javascript
xlab='Previous Year Abundance') 
```

# Monthly Average Temperatures in Dubuque, Iowa

The average monthly temperatures (in degrees Fahrenheit) over a number of years recorded in Dubuque, Iowa, are shown in Exhibit 1.7.

![](images/99d20daddd229952d73485f5dcb10efd5a7e192f1bb8afedbdb0fd77f9eeb4b0.jpg)  
Exhibit 1.7 Average Monthly Temperatures, Dubuque, Iowa

```txt
> win.graph(width=4.875, height=2.5, pointsize=8)  
> data(tempdb); plot(tempdb,ylab='Temperature', type='o') 
```

This time series displays a very regular pattern called seasonality. Seasonality for monthly values occurs when observations twelve months apart are related in some manner or another. All Januarys and Februarys are quite cold but they are similar in value and different from the temperatures of the warmer months of June, July, and August, for example. There is still variation among the January values and variation among the June values. Models for such series must accommodate this variation while preserving the similarities. Here the reason for the seasonality is well understood — the Northern Hemisphere’s changing inclination toward the sun.

# Monthly Oil Filter Sales

Our last example for this chapter concerns the monthly sales to dealers of a specialty oil filter for construction equipment manufactured by John Deere. When these data were first presented to one of the authors, the manager said, “There is no reason to believe that these sales are seasonal.” Seasonality would be present if January values tended to be related to other January values, February values tended to be related to other February values, and so forth. The time series plot shown in Exhibit 1.8 is not designed to display seasonality especially well. Exhibit 1.9 gives the same plot but amended to use meaningful plotting symbols. In this plot, all January values are plotted with the character J, all Februarys with F, all Marches with M, and so forth.† With these plotting symbols, it is much easier to see that sales for the winter months of January and February all tend to be high, while sales in September, October, November, and December are gener-

ally quite low. The seasonality in the data is much easier to see from this modified time series plot.

![](images/85499c6838b5b30af88408cddae6af5c9a23a03efcbfc96e1c33954383332e52.jpg)  
Exhibit 1.8 Monthly Oil Filter Sales

```txt
> data(oilfilters); plot(oilfilters,type='o',ylab='Sales') 
```

![](images/c123e1c130ac69b1d080cbb25186eb3f873fd071fcddddec4ef2aa6024436ba5.jpg)  
Exhibit 1.9 Monthly Oil Filter Sales with Special Plotting Symbols

```txt
J=January (and June and July), F=February, M=March (and May), and so forth 
```

```txt
> plot(oilfilters,type='l',ylab='Sales')
> points(y=oilfilters, x=time(oilfilters), pch=as.vector(season(oilfilters))) 
```

In general, our goal is to emphasize plotting methods that are appropriate and useful for finding patterns that will lead to suitable models for our time series data. In later chapters, we will consider several different ways to incorporate seasonality into time series models.

# 1.2 A Model-Building Strategy

Finding appropriate models for time series is a nontrivial task. We will develop a multistep model-building strategy espoused so well by Box and Jenkins (1976). There are three main steps in the process, each of which may be used several times:

1. model specification (or identification)   
2. model fitting, and   
3. model diagnostics

In model specification (or identification), the classes of time series models are selected that may be appropriate for a given observed series. In this step we look at the time plot of the series, compute many different statistics from the data, and also apply any knowledge of the subject matter in which the data arise, such as biology, business, or ecology. It should be emphasized that the model chosen at this point is tentative and subject to revision later on in the analysis.

In choosing a model, we shall attempt to adhere to the principle of parsimony; that is, the model used should require the smallest number of parameters that will adequately represent the time series. Albert Einstein is quoted in Parzen (1982, p. 68) as remarking that “everything should be made as simple as possible but not simpler.”

The model will inevitably involve one or more parameters whose values must be estimated from the observed series. Model fitting consists of finding the best possible estimates of those unknown parameters within a given model. We shall consider criteria such as least squares and maximum likelihood for estimation.

Model diagnostics is concerned with assessing the quality of the model that we have specified and estimated. How well does the model fit the data? Are the assumptions of the model reasonably well satisfied? If no inadequacies are found, the modeling may be assumed to be complete, and the model may be used, for example, to forecast future values. Otherwise, we choose another model in the light of the inadequacies found; that is, we return to the model specification step. In this way, we cycle through the three steps until, ideally, an acceptable model is found.

Because the computations required for each step in model building are intensive, we shall rely on readily available statistical software to carry out the calculations and do the plotting.

# 1.3 Time Series Plots in History

According toTufte (1983, p. 28), “The time-series plot is the most frequently used form of graphic design. With one dimension marching along to the regular rhythm of sec-

onds, minutes, hours, days, weeks, months, years, or millennia, the natural ordering of the time scale gives this design a strength and efficiency of interpretation found in no other graphic arrangement.”

Exhibit 1.10 reproduces what appears to be the oldest known example of a time series plot, dating from the tenth (or possibly eleventh) century and showing the inclinations of the planetary orbits.† Commenting on this artifact, Tufte says “It appears as a mysterious and isolated wonder in the history of data graphics, since the next extant graphic of a plotted time-series shows up some 800 years later.”

![](images/f550b70cb4793597be8e214fb4f3a74aa566c9a5cd701781948e01b8e8418cbb.jpg)  
Exhibit 1.10 A Tenth-Century Time Series Plot

# 1.4 An Overview of the Book

Chapter 2 develops the basic ideas of mean, covariance, and correlation functions and ends with the important concept of stationarity. Chapter 3 discusses trend analysis and investigates how to estimate and check common deterministic trend models, such as those for linear time trends and seasonal means.

Chapter 4 begins the development of parametric models for stationary time series, namely the so-called autoregressive moving average (ARMA) models (also known as Box-Jenkins models). These models are then generalized in Chapter 5 to encompass certain types of stochastic nonstationary cases—the ARIMA models.

Chapters 6, 7, and 8 form the heart of the model-building strategy for ARIMA modeling. Techniques are presented for tentatively specifying models (Chapter 6), efficiently estimating the model parameters using least squares and maximum likelihood (Chapter 7), and determining how well the models fit the data (Chapter 8).

Chapter 9 thoroughly develops the theory and methods of minimum mean square error forecasting for ARIMA models. Chapter 10 extends the ideas of Chapters 4

through 9 to stochastic seasonal models. The remaining chapters cover selected topics and are of a somewhat more advanced nature.

# EXERCISES

1.1 Use software to produce the time series plot shown in Exhibit 1.2, on page 2. The data are in the file named larain.†   
1.2 Produce the time series plot displayed in Exhibit 1.3, on page 3. The data file is named color.   
1.3 Simulate a completely random process of length 48 with independent, normal values. Plot the time series plot. Does it look “random”? Repeat this exercise several times with a new simulation each time.   
1.4 Simulate a completely random process of length 48 with independent, chi-square distributed values, each with 2 degrees of freedom. Display the time series plot. Does it look “random” and nonnormal? Repeat this exercise several times with a new simulation each time.   
1.5 Simulate a completely random process of length 48 with independent, $t$ -distributed values each with 5 degrees of freedom. Construct the time series plot. Does it look “random” and nonnormal? Repeat this exercise several times with a new simulation each time.   
1.6 Construct a time series plot with monthly plotting symbols for the Dubuque temperature series as in Exhibit 1.7, on page 6. The data are in the file named tempdub.

# CHAPTER 2

# FUNDAMENTAL CONCEPTS

This chapter describes the fundamental concepts in the theory of time series models. In particular, we introduce the concepts of stochastic processes, mean and covariance functions, stationary processes, and autocorrelation functions.

# 2.1 Time Series and Stochastic Processes

The sequence of random variables $\{ Y _ { t } \colon t = 0 , \pm 1 , \pm 2 , \pm 3 , \ldots \}$ is called a stochastic process and serves as a model for an observed time series. It is known that the complete probabilistic structure of such a process is determined by the set of distributions of all finite collections of the Y’s. Fortunately, we will not have to deal explicitly with these multivariate distributions. Much of the information in these joint distributions can be described in terms of means, variances, and covariances. Consequently, we concentrate our efforts on these first and second moments. (If the joint distributions of the Y’s are multivariate normal distributions, then the first and second moments completely determine all the joint distributions.)

# 2.2 Means, Variances, and Covariances

For a stochastic process $\{ Y _ { t } \colon t = 0 , \pm 1 , \pm 2 , \pm 3 , \ldots \}$ , the mean function is defined by

$$
\mu_ {t} = E \left(Y _ {t}\right) \quad \text {f o r} t = 0, \pm 1, \pm 2, \dots \tag {2.2.1}
$$

That is, $\mu _ { t }$ is just the expected value of the process at time t. In general, $\mu _ { t }$ can be different at each time point t.

The autocovariance function, $\gamma _ { t , s }$ , is defined as

$$
\gamma_ {t, s} = \operatorname {C o v} \left(Y _ {t}, Y _ {s}\right) \quad \text {f o r} t, s = 0, \pm 1, \pm 2, \dots \tag {2.2.2}
$$

where $C o \nu ( Y _ { t } , Y _ { s } ) = E [ ( Y _ { t } - \mu _ { t } ) ( Y _ { s } - \mu _ { s } ) ] = E ( Y _ { t } Y _ { s } ) - \mu _ { t } \mu _ { s } .$

The autocorrelation function, $\rho _ { t , s }$ , is given by

$$
\rho_ {t, s} = \operatorname {C o r r} \left(Y _ {t}, Y _ {s}\right) \quad \text {f o r} t, s = 0, \pm 1, \pm 2, \dots \tag {2.2.3}
$$

where

$$
C o r r \left(Y _ {t}, Y _ {s}\right) = \frac {\operatorname {C o v} \left(Y _ {t} , Y _ {s}\right)}{\sqrt {\operatorname {V a r} \left(Y _ {t}\right) \operatorname {V a r} \left(Y _ {s}\right)}} = \frac {\gamma_ {t , s}}{\sqrt {\gamma_ {t , t} \gamma_ {s , s}}} \tag {2.2.4}
$$

We review the basic properties of expectation, variance, covariance, and correlation in Appendix A on page 24.

Recall that both covariance and correlation are measures of the (linear) dependence between random variables but that the unitless correlation is somewhat easier to interpret. The following important properties follow from known results and our definitions:

$$
\left. \begin{array}{l l} \gamma_ {t, t} = \operatorname {V a r} \left(Y _ {t}\right) & \rho_ {t, t} = 1 \\ \gamma_ {t, s} = \gamma_ {s, t} & \rho_ {t, s} = \rho_ {s, t} \\ \left| \gamma_ {t, s} \right| \leq \sqrt {\gamma_ {t , t} \gamma_ {s , s}} & \left| \rho_ {t, s} \right| \leq 1 \end{array} \right\} \tag {2.2.5}
$$

Values of $\rho _ { t , s }$ near $\pm 1$ indicate strong (linear) dependence, whereas values near zero indicate weak (linear) dependence. If $\rho _ { t , s } = 0$ , we say that $Y _ { t }$ and $Y _ { s }$ are uncorrelated.

To investigate the covariance properties of various time series models, the following result will be used repeatedly: If $c _ { 1 }$ $c _ { 1 } , c _ { 2 } , . . . , c _ { m }$ $c _ { m }$ and $d _ { 1 } , d _ { 2 } , \ldots , d _ { n }$ $d _ { 1 }$ are constants and $t _ { 1 }$ , $t _ { 2 } , . . . , t _ { m }$ and $s _ { 1 }$ $\mid , s _ { 2 } , \ldots , s _ { n }$ $s _ { n }$ are time points, then

$$
C o v \left[ \sum_ {i = 1} ^ {m} c _ {i} Y _ {t _ {i}}, \sum_ {j = 1} ^ {n} d _ {j} Y _ {s _ {j}} \right] = \sum_ {i = 1} ^ {m} \sum_ {j = 1} ^ {n} c _ {i} d _ {j} C o v \left(Y _ {t _ {i}}, Y _ {s _ {j}}\right) \tag {2.2.6}
$$

The proof of Equation (2.2.6), though tedious, is a straightforward application of the linear properties of expectation. As a special case, we obtain the well-known result

$$
\operatorname {V a r} \left[ \sum_ {i = 1} ^ {n} c _ {i} Y _ {t _ {i}} \right] = \sum_ {i = 1} ^ {n} c _ {i} ^ {2} \operatorname {V a r} \left(Y _ {t _ {i}}\right) + 2 \sum_ {i = 2} ^ {n} \sum_ {j = 1} ^ {i - 1} c _ {i} c _ {j} \operatorname {C o v} \left(Y _ {t _ {i}}, Y _ {t _ {j}}\right) \tag {2.2.7}
$$

# The Random Walk

Let $e _ { 1 }$ , $e _ { 2 } , \ldots$ be a sequence of independent, identically distributed random variables each with zero mean and variance $\sigma _ { e } ^ { 2 }$ . The observed time series, $\{ Y _ { t } \colon t = 1 , 2 , \ldots \}$ , is constructed as follows:

$$
\left. \begin{array}{c} Y _ {1} = e _ {1} \\ Y _ {2} = e _ {1} + e _ {2} \\ \vdots \\ Y _ {t} = e _ {1} + e _ {2} + \dots + e _ {t} \end{array} \right\} \tag {2.2.8}
$$

Alternatively, we can write

$$
Y _ {t} = Y _ {t - 1} + e _ {t} \tag {2.2.9}
$$

with “initial condition” $Y _ { 1 } = e _ { 1 }$ . If the $e$ ’s are interpreted as the sizes of the “steps” taken (forward or backward) along a number line, then $Y _ { t }$ is the position of the “random walker” at time t. From Equation (2.2.8), we obtain the mean function

$$
\begin{array}{l} \mu_ {t} = E \left(Y _ {t}\right) = E \left(e _ {1} + e _ {2} + \dots + e _ {t}\right) = E \left(e _ {1}\right) + E \left(e _ {2}\right) + \dots + E \left(e _ {t}\right) \\ = 0 + 0 + \dots + 0 \\ \end{array}
$$

so that

$$
\mu_ {t} = 0 \quad \text {f o r a l l} t \tag {2.2.10}
$$

We also have

$$
\begin{array}{l} V a r \left(Y _ {t}\right) = V a r \left(e _ {1} + e _ {2} + \dots + e _ {t}\right) = V a r \left(e _ {1}\right) + V a r \left(e _ {2}\right) + \dots + V a r \left(e _ {t}\right) \\ = \sigma_ {e} ^ {2} + \sigma_ {e} ^ {2} + \dots + \sigma_ {e} ^ {2} \\ \end{array}
$$

so that

$$
V a r \left(Y _ {t}\right) = t \sigma_ {e} ^ {2} \tag {2.2.11}
$$

Notice that the process variance increases linearly with time.

To investigate the covariance function, suppose that $1 \leq t \leq s$ . Then we have

$$
\gamma_ {t, s} = C o v (Y _ {t}, Y _ {s}) = C o v \left(e _ {1} + e _ {2} + \dots + e _ {t}, e _ {1} + e _ {2} + \dots + e _ {t} + e _ {t + 1} + \dots + e _ {s}\right)
$$

From Equation (2.2.6), we have

$$
\gamma_ {t, s} = \sum_ {i = 1} ^ {s} \sum_ {j = 1} ^ {t} C o v (e _ {i}, e _ {j})
$$

However, these covariances are zero unless $i = j$ , in which case they equal $V a r ( e _ { i } ) = \sigma _ { e } ^ { 2 }$ There are exactly $t$ of these so that $\gamma _ { t , s } = t \sigma _ { e } ^ { 2 }$ .

Since $\gamma _ { t , s } = \gamma _ { s , t }$ , this specifies the autocovariance function for all time points $t$ and $s$ and we can write

$$
\gamma_ {t, s} = t \sigma_ {e} ^ {2} \quad \text {f o r} 1 \leq t \leq s \tag {2.2.12}
$$

The autocorrelation function for the random walk is now easily obtained as

$$
\rho_ {t, s} = \frac {\gamma_ {t , s}}{\sqrt {\gamma_ {t , t} \gamma_ {s , s}}} = \sqrt {\frac {t}{s}} \quad \text {f o r} 1 \leq t \leq s \tag {2.2.13}
$$

The following numerical values help us understand the behavior of the random walk.

$$
\rho_ {1, 2} = \sqrt {\frac {1}{2}} = 0. 7 0 7 \quad \rho_ {8, 9} = \sqrt {\frac {8}{9}} = 0. 9 4 3
$$

$$
\rho_ {2 4, 2 5} = \sqrt {\frac {2 4}{2 5}} = 0. 9 8 0 \quad \rho_ {1, 2 5} = \sqrt {\frac {1}{2 5}} = 0. 2 0 0
$$

The values of Y at neighboring time points are more and more strongly and positively correlated as time goes by. On the other hand, the values of $Y$ at distant time points are less and less correlated.

A simulated random walk is shown in Exhibit 2.1 where the $e$ ’s were selected from a standard normal distribution. Note that even though the theoretical mean function is

zero for all time points, the fact that the variance increases over time and that the correlation between process values nearby in time is nearly 1 indicate that we should expect long excursions of the process away from the mean level of zero.

The simple random walk process provides a good model (at least to a first approximation) for phenomena as diverse as the movement of common stock price, and the position of small particles suspended in a fluid—so-called Brownian motion.

Exhibit 2.1 Time Series Plot of a Random Walk   
![](images/35be928563dd4a6c8db519c957027225243dd71b26876d7624f91f1725a778dd.jpg)  
> win.graph(width=4.875, height $^ { . = 2 }$ .5,pointsize=8)   
> data(rwalk) # rwalk contains a simulated random walk   
> plot(rwalk,type='o',ylab $^ { 1 = }$ 'Random Walk')

# A Moving Average

As a second example, suppose that $\{ Y _ { t } \}$ is constructed as

$$
Y _ {t} = \frac {e _ {t} + e _ {t - 1}}{2} \tag {2.2.14}
$$

where (as always throughout this book) the $e$ ’s are assumed to be independent and identically distributed with zero mean and variance $\sigma _ { e } ^ { 2 }$ . Here

$$
\begin{array}{l} \mu_ {t} = E (Y _ {t}) = E \left\{\frac {e _ {t} + e _ {t - 1}}{2} \right\} = \frac {E (e _ {t}) + E (e _ {t - 1})}{2} \\ = 0 \\ \end{array}
$$

and

$$
\begin{array}{l} V a r (Y _ {t}) = V a r \left\{\frac {e _ {t} + e _ {t - 1}}{2} \right\} = \frac {V a r (e _ {t}) + V a r (e _ {t - 1})}{4} \\ = 0. 5 \sigma_ {e} ^ {2} \\ \end{array}
$$

Also

$$
\begin{array}{l} C o v \left(Y _ {t}, Y _ {t - 1}\right) = C o v \left\{\frac {e _ {t} + e _ {t - 1}}{2}, \frac {e _ {t - 1} + e _ {t - 2}}{2} \right\} \\ = \frac {C o v \left(e _ {t} , e _ {t - 1}\right) + C o v \left(e _ {t} , e _ {t - 2}\right) + C o v \left(e _ {t - 1} , e _ {t - 1}\right)}{4} \\ + \frac {C o v (e _ {t - 1} , e _ {t - 2})}{4} \\ = \frac {C o v \left(e _ {t - 1} , e _ {t - 1}\right)}{4} \quad \text {(a s a l l t h e o t h e r c o v a r i a n c e s a r e z e r o)} \\ = 0. 2 5 \sigma_ {e} ^ {2} \\ \end{array}
$$

or

$$
\gamma_ {t, t - 1} = 0. 2 5 \sigma_ {e} ^ {2} \quad \text {f o r a l l} t \tag {2.2.15}
$$

Furthermore,

$$
\begin{array}{l} C o v (Y _ {t}, Y _ {t - 2}) = C o v \left\{\frac {e _ {t} + e _ {t - 1}}{2}, \frac {e _ {t - 2} + e _ {t - 3}}{2} \right\} \\ = 0 \quad \text {s i n c e t h e} e ^ {\prime} \text {s a r e i n d e p e n d e n t}. \\ \end{array}
$$

Similarly, $C o \nu ( Y _ { t } , Y _ { t - k } ) = 0$ for $k > 1$ , so we may write

$$
\gamma_ {t, s} = \left\{ \begin{array}{c l} 0. 5 \sigma_ {e} ^ {2} & \text {f o r} | t - s | = 0 \\ 0. 2 5 \sigma_ {e} ^ {2} & \text {f o r} | t - s | = 1 \\ 0 & \text {f o r} | t - s | > 1 \end{array} \right.
$$

For the autocorrelation function, we have

$$
\rho_ {t, s} = \left\{ \begin{array}{c c} 1 & \text {f o r} | t - s | = 0 \\ 0. 5 & \text {f o r} | t - s | = 1 \\ 0 & \text {f o r} | t - s | > 1 \end{array} \right. \tag {2.2.16}
$$

since $0 . 2 5 \sigma _ { e } ^ { 2 } / 0 . 5 \sigma _ { e } ^ { 2 } = 0 . 5$

Notice that $\rho _ { 2 , 1 } = \rho _ { 3 , 2 } = \rho _ { 4 , 3 } = \rho _ { 9 , 8 } = 0 . 5$ . Values of Y precisely one time unit apart have exactly the same correlation no matter where they occur in time. Furthermore, $\rho _ { 3 , 1 }$ $\mathsf { \Phi } = \mathsf { \rho } _ { 9 4 , 2 } = \mathsf { \rho } _ { 9 t , t - 2 }$ and, more generally, $\rho _ { t , t - k }$ is the same for all values of t. This leads us to the important concept of stationarity.

# 2.3 Stationarity

To make statistical inferences about the structure of a stochastic process on the basis of an observed record of that process, we must usually make some simplifying (and presumably reasonable) assumptions about that structure. The most important such assumption is that of stationarity. The basic idea of stationarity is that the probability laws that govern the behavior of the process do not change over time. In a sense, the process is in statistical equilibrium. Specifically, a process $\{ Y _ { t } \}$ is said to be strictly stationary if the joint distribution of $Y _ { t _ { 1 } } , Y _ { t _ { 2 } } , . . . , Y _ { t _ { n } }$ is the same as the joint distribution of $Y _ { t _ { 1 } - k } ~ Y _ { t _ { 2 } - k } , . . . , Y _ { t _ { n } - k }$ for all choices of time points $t _ { 1 } , t _ { 2 } , . . . , t _ { n }$ and all choices of time lag k.

Thus, when $n = 1$ the (univariate) distribution of $Y _ { t }$ is the same as that of $Y _ { t - k }$ for all $t$ and $k$ ; in other words, the Y’s are (marginally) identically distributed. It then follows that $E ( Y _ { t } ) = E ( Y _ { t - k } )$ for all $t$ and $k$ so that the mean function is constant for all time. Additionally, ${ V a r } ( Y _ { t } ) = { V a r } ( Y _ { t - k } )$ for all $t$ and $k$ so that the variance is also constant over time.

Setting $n = 2$ in the stationarity definition we see that the bivariate distribution of $Y _ { t }$ and $Y _ { s }$ must be the same as that of $Y _ { t - k }$ and $Y _ { s - k }$ from which it follows that $C o \nu ( Y _ { t } , Y _ { s } )$ $= C o \nu ( Y _ { t - k } , Y _ { s - k } )$ for all $t , s$ , and $k$ . Putting $k = s$ and then $k = t$ , we obtain

$$
\begin{array}{l} \gamma_ {t, s} = \operatorname {C o v} \left(Y _ {t - s}, Y _ {0}\right) \\ = C o v \left(Y _ {0}, Y _ {s - t}\right) \\ = C o v (Y _ {0}, Y _ {| t - s |}) \\ \mathbf {\Sigma} = \gamma_ {0, | t - s |} \\ \end{array}
$$

That is, the covariance between $Y _ { t }$ and $Y _ { s }$ depends on time only through the time difference $\left| t - s \right|$ and not otherwise on the actual times $t$ and $s$ . Thus, for a stationary process, we can simplify our notation and write

$$
\gamma_ {k} = \operatorname {C o v} \left(Y _ {t}, Y _ {t - k}\right) \quad \text {a n d} \quad \rho_ {k} = \operatorname {C o r r} \left(Y _ {t}, Y _ {t - k}\right) \tag {2.3.1}
$$

Note also that

$$
\rho_ {k} = \frac {\gamma_ {k}}{\gamma_ {0}}
$$

The general properties given in Equation (2.2.5) now become

$$
\left. \begin{array}{l l} \gamma_ {0} = \operatorname {V a r} \left(Y _ {t}\right) & \rho_ {0} = 1 \\ \gamma_ {k} = \gamma_ {- k} & \rho_ {k} = \rho_ {- k} \\ \left| \gamma_ {k} \right| \leq \gamma_ {0} & \left| \rho_ {k} \right| \leq 1 \end{array} \right\} \tag {2.3.2}
$$

If a process is strictly stationary and has finite variance, then the covariance function must depend only on the time lag.

A definition that is similar to that of strict stationarity but is mathematically weaker

is the following: A stochastic process $\{ Y _ { t } \}$ is said to be weakly (or second-order) stationary if

1. The mean function is constant over time, and   
2. $\gamma _ { t , t - k } = \gamma _ { 0 , k } \qquad { \mathrm { f o r ~ a l l ~ t i m e ~ } } t { \mathrm { ~ a n d ~ l a g ~ } } k$ $\gamma _ { t , t - k } = \gamma _ { 0 , k }$

In this book the term stationary when used alone will always refer to this weaker form of stationarity. However, if the joint distributions for the process are all multivariate normal distributions, it can be shown that the two definitions coincide. For stationary processes, we usually only consider $k \geq 0$ .

# White Noise

A very important example of a stationary process is the so-called white noise process, which is defined as a sequence of independent, identically distributed random variables $\{ \boldsymbol { e } _ { t } \}$ . Its importance stems not from the fact that it is an interesting model itself but from the fact that many useful processes can be constructed from white noise. The fact that $\{ e _ { t } \}$ is strictly stationary is easy to see since

$$
P r \left(e _ {t _ {1}} \leq x _ {1}, e _ {t _ {2}} \leq x _ {2}, \dots , e _ {t _ {n}} \leq x _ {n}\right)
$$

$$
= \Pr \left(e _ {t _ {1}} \leq x _ {1}\right) \Pr \left(e _ {t _ {2}} \leq x _ {2}\right) \dots \Pr \left(e _ {t _ {n}} \leq x _ {n}\right) \quad (\text {b y})
$$

$$
= P r \left(e _ {t _ {1} - k} \leq x _ {1}\right) P r \left(e _ {t _ {2} - k} \leq x _ {2}\right) \dots P r \left(e _ {t _ {n} - k} \leq x _ {n}\right)
$$

(identical distributions)

$$
= \Pr \left(e _ {t _ {1} - k} \leq x _ {1}, e _ {t _ {2} - k} \leq x _ {2}, \dots , e _ {t _ {n} - k} \leq x _ {n}\right) \quad (\text {b y})
$$

as required. Also, ${ \mu } _ { t } = E ( e _ { t } )$ is constant and

$$
\gamma_ {k} = \left\{ \begin{array}{c c} V a r (e _ {t}) & \quad \text {f o r k = 0} \\ 0 & \quad \text {f o r k \neq 0} \end{array} \right.
$$

Alternatively, we can write

$$
\rho_ {k} = \left\{ \begin{array}{l l} 1 & \text {f o r} k = 0 \\ 0 & \text {f o r} k \neq 0 \end{array} \right. \tag {2.3.3}
$$

The term white noise arises from the fact that a frequency analysis of the model shows that, in analogy with white light, all frequencies enter equally. We usually assume that the white noise process has mean zero and denote ${ V a r } ( e _ { t } )$ by $\sigma _ { e } ^ { 2 }$ .

The moving average example, on page 14, where $Y _ { t } = ( e _ { t } + e _ { t - 1 } ) / 2$ , is another example of a stationary process constructed from white noise. In our new notation, we have for the moving average process that

$$
\rho_ {k} = \left\{ \begin{array}{l l} 1 & \quad \text {f o r k = 0} \\ 0. 5 & \quad \text {f o r | k | = 1} \\ 0 & \quad \text {f o r | k | \geq 2} \end{array} \right.
$$

# Random Cosine Wave

As a somewhat different example,† consider the process defined as follows:

$$
Y _ {t} = \cos \left[ 2 \pi \left(\frac {t}{1 2} + \Phi\right) \right] \quad \text {f o r} t = 0, \pm 1, \pm 2, \dots
$$

where $\Phi$ is selected (once) from a uniform distribution on the interval from 0 to 1. A sample from such a process will appear highly deterministic since $Y _ { t }$ will repeat itself identically every 12 time units and look like a perfect (discrete time) cosine curve. However, its maximum will not occur at $t = 0$ but will be determined by the random phase $\Phi$ . The phase $\Phi$ can be interpreted as the fraction of a complete cycle completed by time $t =$ 0. Still, the statistical properties of this process can be computed as follows:

$$
\begin{array}{l} E \left(Y _ {t}\right) = E \left\{\cos \left[ 2 \pi \left(\frac {t}{1 2} + \Phi\right) \right] \right\} \\ = \int_ {0} ^ {1} \cos \left[ 2 \pi \left(\frac {t}{1 2} + \phi\right) \right] d \phi \\ = \left. \frac {1}{2 \pi} \sin \left[ 2 \pi \left(\frac {t}{1 2} + \phi\right) \right] \right| _ {\phi = 0} ^ {1} \\ = \frac {1}{2 \pi} \left[ \sin \left(2 \pi \frac {t}{1 2} + 2 \pi\right) - \sin \left(2 \pi \frac {t}{1 2}\right) \right] \\ \end{array}
$$

But this is zero since the sines must agree. So $\mu _ { t } = 0$ for all $t$ Also

$$
\begin{array}{l} \gamma_ {t, s} = E \left\{\cos \left[ 2 \pi \left(\frac {t}{1 2} + \Phi\right) \right] \cos \left[ 2 \pi \left(\frac {s}{1 2} + \Phi\right) \right] \right\} \\ = \int_ {0} ^ {1} \cos \left[ 2 \pi \left(\frac {t}{1 2} + \phi\right) \right] \cos \left[ 2 \pi \left(\frac {s}{1 2} + \phi\right) \right] d \phi \\ = \frac {1}{2} \int_ {0} ^ {1} \left\{\cos \left[ 2 \pi \left(\frac {t - s}{1 2}\right) \right] + \cos \left[ 2 \pi \left(\frac {t + s}{1 2} + 2 \phi\right) \right] \right\} d \phi \\ = \left. \frac {1}{2} \left\{\cos \left[ 2 \pi \left(\frac {t - s}{1 2}\right) \right] + \frac {1}{4 \pi} \sin \left[ 2 \pi \left(\frac {t + s}{1 2} + 2 \phi\right) \right] \right| _ {\phi = 0} ^ {1} \right\} \\ = \frac {1}{2} \cos \left[ 2 \pi \left(\frac {| t - s |}{1 2}\right) \right] \\ \end{array}
$$

So the process is stationary with autocorrelation function

$$
\rho_ {k} = \cos \left(2 \pi \frac {k}{1 2}\right) \quad \text {f o r} k = 0, \pm 1, \pm 2, \dots \tag {2.3.4}
$$

This example suggests that it will be difficult to assess whether or not stationarity is a reasonable assumption for a given time series on the basis of the time sequence plot of the observed data.

The random walk of page 12, where $Y _ { t } = e _ { 1 } + e _ { 2 } + \cdots + e _ { t }$ , is also constructed from white noise but is not stationary. For example, the variance function, $V a r ( Y _ { t } ) =$ $t \sigma _ { e } ^ { 2 }$ , is not constant; furthermore, the covariance function $\gamma _ { t , s } = t \sigma _ { e } ^ { 2 }$ for $0 \leq t \leq s$ does not depend only on time lag. However, suppose that instead of analyzing $\{ Y _ { t } \}$ directly, we consider the differences of successive Y-values, denoted $\nabla Y _ { t }$ . Then $\nabla Y _ { t } = Y _ { t } - Y _ { t - 1 } =$ $e _ { t } ,$ so the differenced series, $\{ \nabla Y _ { t } \}$ , is stationary. This represents a simple example of a technique found to be extremely useful in many applications. Clearly, many real time series cannot be reasonably modeled by stationary processes since they are not in statistical equilibrium but are evolving over time. However, we can frequently transform nonstationary series into stationary series by simple techniques such as differencing. Such techniques will be vigorously pursued in the remaining chapters.

# 2.4 Summary

In this chapter we have introduced the basic concepts of stochastic processes that serve as models for time series. In particular, you should now be familiar with the important concepts of mean functions, autocovariance functions, and autocorrelation functions. We illustrated these concepts with the basic processes: the random walk, white noise, a simple moving average, and a random cosine wave. Finally, the fundamental concept of stationarity introduced here will be used throughout the book.

# EXERCISES

2.1 Suppose $E ( X ) = 2$ , $V a r ( X ) = 9$ , $E ( Y ) = 0$ , $V a r ( Y ) = 4$ , and $C o r r ( X , Y ) = 0 . 2 5$ . Find:

(a) $V a r ( X + Y )$ .   
(b) $C o \nu ( X , X + Y )$ .   
(c) $C o r r ( X + Y , X - Y )$ .

2.2 If $X$ and $Y$ are dependent but $V a r ( X ) = V a r ( Y )$ , find $C o \nu ( X + Y , X - Y )$ .

2.3 Let $X$ have a distribution with mean $\mu$ and variance $\sigma ^ { 2 }$ , and let $Y _ { t } = X$ for all $t$

(a) Show that $\{ Y _ { t } \}$ is strictly and weakly stationary.   
(b) Find the autocovariance function for $\{ Y _ { t } \}$   
(c) Sketch a “typical” time plot of $Y _ { t } .$

2.4 Let $\{ \boldsymbol { e } _ { t } \}$ be a zero mean white noise process. Suppose that the observed process is $Y _ { t } = e _ { t } + \theta e _ { t - 1 }$ , where θ is either 3 or 1/3.

(a) Find the autocorrelation function for $\{ Y _ { t } \}$ both when $\theta = 3$ and when $\theta = 1 / 3$   
(b) You should have discovered that the time series is stationary regardless of the value of θ and that the autocorrelation functions are the same for $\theta = 3$ and $\theta =$ 1/3. For simplicity, suppose that the process mean is known to be zero and the variance of $Y _ { t }$ is known to be 1. You observe the series $\{ Y _ { t } \}$ for $t = 1$ , 2, ... , n and suppose that you can produce good estimates of the autocorrelations $\rho _ { k }$ Do you think that you could determine which value of θ is correct (3 or 1/3) based on the estimate of $\rho _ { k } ?$ Why or why not?

2.5 Suppose $Y _ { t } = 5 + 2 t + X _ { t } ,$ where $\{ X _ { t } \}$ is a zero-mean stationary series with autocovariance function $\gamma _ { k }$ .

(a) Find the mean function for $\{ Y _ { t } \}$   
(b) Find the autocovariance function for $\{ Y _ { t } \}$ .   
(c) Is $\{ Y _ { t } \}$ stationary? Why or why not?

2.6 Let $\{ X _ { t } \}$ be a stationary time series, and define $Y _ { t } = \left\{ { X } _ { t } \atop { X _ { t } + 3 } \right.$ for t odd for t even.

(a) Show that $C o \nu ( Y _ { t } , Y _ { t - k } )$ is free of $t$ for all lags $k$   
(b) Is $\{ Y _ { t } \}$ stationary?

2.7 Suppose that $\{ Y _ { t } \}$ is stationary with autocovariance function $\gamma _ { k }$

(a) Show that $W _ { t } = \nabla Y _ { t } = Y _ { t } - Y _ { t - 1 }$ is stationary by finding the mean and autocovariance function for $\{ W _ { t } \}$ .   
(b) Show that $U _ { t } = \nabla ^ { 2 } Y _ { t } = \nabla [ Y _ { t } - Y _ { t - 1 } ] = Y _ { t } - 2 Y _ { t - 1 } + Y _ { t - 2 }$ is stationary. (You need not find the mean and autocovariance function for $\{ U _ { t } \}$ .)

2.8 Suppose that $\{ Y _ { t } \}$ is stationary with autocovariance function $\gamma _ { k }$ . Show that for any fixed positive integer $n$ and any constants $c _ { 1 }$ $c _ { 1 } , c _ { 2 } , \ldots , c _ { n }$ , the process $\{ W _ { t } \}$ defined by $W _ { t } = c _ { 1 } Y _ { t } + c _ { 2 } Y _ { t - 1 } + \dots + c _ { n } Y _ { t - n + 1 }$ is stationary. (Note that Exercise 2.7 is a special case of this result.)

2.9 Suppose $Y _ { t } = \beta _ { 0 } + \beta _ { 1 } t + X _ { t }$ , where $\{ X _ { t } \}$ is a zero-mean stationary series with autocovariance function $\gamma _ { k }$ and $\beta _ { 0 }$ and $\beta _ { 1 }$ are constants.

(a) Show that $\{ Y _ { t } \}$ is not stationary but that $W _ { t } = \nabla Y _ { t } = Y _ { t } - Y _ { t - 1 }$ is stationary.   
(b) In general, show that if $Y _ { t } = \mu _ { t } + X _ { t } ,$ where $\{ X _ { t } \}$ is a zero-mean stationary series and $\mu _ { t }$ is a polynomial in $t$ of degree $d$ , then $\nabla ^ { m } Y _ { t } = \nabla ( \nabla ^ { m - 1 } Y _ { t } )$ is stationary for $m \geq d$ and nonstationary for $0 \leq m < d$ .

2.10 Let $\{ X _ { t } \}$ be a zero-mean, unit-variance stationary process with autocorrelation function $\rho _ { k }$ . Suppose that $\mu _ { t }$ is a nonconstant function and that $\sigma _ { t }$ is a positive-valued nonconstant function. The observed series is formed as $Y _ { t } = \mu _ { t } + \sigma _ { t } X _ { t }$ .

(a) Find the mean and covariance function for the $\{ Y _ { t } \}$ process.   
(b) Show that the autocorrelation function for the $\{ Y _ { t } \}$ process depends only on the time lag. Is the $\{ Y _ { t } \}$ process stationary?   
(c) Is it possible to have a time series with a constant mean and with $C o r r ( Y _ { t } , Y _ { t - k } )$ free of $t$ but with $\{ Y _ { t } \}$ not stationary?

2.11 Suppose $C o \nu ( X _ { t } , X _ { t - k } ) = \gamma _ { k }$ is free of $t$ but that $E ( X _ { t } ) = 3 t$ .

(a) Is $\{ X _ { t } \}$ stationary?   
(b) Let $Y _ { t } = 7 - 3 t + X _ { t }$ . Is $\{ Y _ { t } \}$ stationary?

2.12 Suppose that $Y _ { t } = e _ { t } - e _ { t - 1 2 }$ . Show that $\{ Y _ { t } \}$ is stationary and that, for $k > 0$ , its autocorrelation function is nonzero only for lag $k = 1 2$ .

2.13 Let $Y _ { t } = e _ { t } - \theta ( e _ { t - 1 } ) ^ { 2 }$ . For this exercise, assume that the white noise series is normally distributed.

(a) Find the autocorrelation function for $\{ Y _ { t } \}$   
$\mathbf { ( b ) }$ Is $\{ Y _ { t } \}$ stationary?

2.14 Evaluate the mean and covariance function for each of the following processes. In each case, determine whether or not the process is stationary.

(a) $Y _ { t } = \boldsymbol { \theta } _ { 0 } + t \boldsymbol { e } _ { t }$   
(b) $\boldsymbol { W } _ { t } = \ \nabla Y _ { t } ,$ where $Y _ { t }$ is as given in part (a).   
(c) $Y _ { t } = e _ { t } e _ { t - 1 }$ . (You may assume that $\{ e _ { t } \}$ is normal white noise.)

2.15 Suppose that $X$ is a random variable with zero mean. Define a time series by $Y _ { t } = ( - 1 ) ^ { t } X$ .

(a) Find the mean function for $\{ Y _ { t } \}$   
(b) Find the covariance function for $\{ Y _ { t } \}$   
(c) Is $\{ Y _ { t } \}$ stationary?

2.16 Suppose $Y _ { t } = A + X _ { t } ,$ , where $\{ X _ { t } \}$ is stationary and $A$ is random but independent of $\{ X _ { t } \}$ . Find the mean and covariance function for $\{ Y _ { t } \}$ in terms of the mean and autocovariance function for $\{ X _ { t } \}$ and the mean and variance of $A$ ._

2.17 Let $\{ Y _ { t } \}$ be stationary with autocovariance function $\gamma _ { k }$ . Let ${ \ddot { \overline { { Y } } } } = { \frac { 1 } { n } } { \sum _ { t = 1 } ^ { n } Y _ { t } }$ Show that

$$
\begin{array}{l} V a r (\bar {Y}) = \frac {\gamma_ {0}}{n} + \frac {2}{n} \sum_ {k = 1} ^ {n - 1} \left(1 - \frac {k}{n}\right) \gamma_ {k} \\ = \frac {1}{n} \sum_ {k = - n + 1} ^ {n - 1} \left(1 - \frac {| k |}{n}\right) \gamma_ {k} \\ \end{array}
$$

2.18 Let $\{ Y _ { t } \}$ be stationary with autocovariance function $\gamma _ { k }$ . Define the sample variance as S2

(a) First sh   
(b) Use part (a) to show that   
$E ( S ^ { 2 } ) = \frac { n } { n - 1 } \gamma _ { 0 } - \frac { n } { n - 1 } V a r ( \overline { { Y } } ) = \gamma _ { 0 } - \frac { 2 } { n - 1 } { \sum _ { k = 1 } ^ { n - 1 } } \biggl ( 1 - \frac { k } { n } \biggr ) \gamma _ { k } .$

(Use the results of Exercise 2.17 for the last expression.)

(d) If $\{ Y _ { t } \}$ is a white noise process with variance $\gamma _ { 0 }$ , show that $E ( S ^ { 2 } ) = \gamma _ { 0 }$

2.19 Let $Y _ { 1 } = \boldsymbol { \theta } _ { 0 } + \boldsymbol { e } _ { 1 }$ , and then for $t > 1$ define $Y _ { t }$ recursively by $Y _ { t } = \theta _ { 0 } + Y _ { t - 1 } + e _ { t } .$ Here $\theta _ { 0 }$ is a constant. The process $\{ Y _ { t } \}$ is called a random walk with drift.

(a) Show that $Y _ { t }$ may be rewritten as $Y _ { t } = t \Theta _ { 0 } + e _ { t } + e _ { t - 1 } + \cdots + e _ { 1 }$   
(b) Find the mean function for $Y _ { t } .$   
(c) Find the autocovariance function for $Y _ { t }$

2.20 Consider the standard random walk model where $Y _ { t } = Y _ { t - 1 } + e _ { t }$ with $Y _ { 1 } = e _ { 1 }$

(a) Use the representation of $Y _ { t }$ above to show that $\mu _ { t } = \mu _ { t - 1 }$ for $t > 1$ with initial condition $\mu _ { 1 } = E ( e _ { 1 } ) = 0 .$ . Hence show that $\mu _ { t } = 0$ for all $t$ .   
(b) Similarly, show that $\mathop { V a r } ( Y _ { t } ) = \mathop { V a r } ( Y _ { t - 1 } ) + \sigma _ { e } ^ { 2 }$ for $t > 1$ with $V a r ( Y _ { 1 } ) = \sigma _ { e } ^ { 2 }$ and hence $V a r ( Y _ { t } ) = t \sigma _ { e } ^ { 2 }$ .   
(c) For $0 \leq t \leq s$ , use $Y _ { s } = Y _ { t } + e _ { t + 1 } + e _ { t + 2 } + \cdots + e _ { s }$ to show that $C o \nu ( Y _ { t } , Y _ { s } ) =$ $V a r ( Y _ { t } )$ and, hence, that $C o \nu ( Y _ { t } , Y _ { s } ) = \operatorname* { m i n } ( t , s ) \sigma _ { e } ^ { 2 }$ .

2.21 For a random walk with random starting value, let $Y _ { t } = Y _ { 0 } + e _ { t } ^ { } + e _ { t - 1 } ^ { } + \dots + e _ { 1 } ^ { }$ for $t > 0$ , where $Y _ { 0 }$ has a distribution with mean $\mu _ { 0 }$ and variance $\sigma _ { 0 } ^ { 2 }$ . Suppose further that Y0, e1, ... , $e _ { t }$ are independent.

(a) Show that $E ( Y _ { t } ) = \mu _ { 0 }$ for all $t$ .   
(b) Show that ${ V a r } ( Y _ { t } ) = t \sigma _ { e } ^ { 2 } + \sigma _ { 0 } ^ { 2 }$   
(c) Show that $C o \nu ( Y _ { t } , Y _ { s } ) \breve { = } \operatorname* { m i n } ( t , s ) \sigma _ { e } ^ { 2 } + \sigma _ { 0 } ^ { 2 }$   
(d) Show that $C o r r ( Y _ { t } , Y _ { s } ) = \sqrt { \frac { t \sigma _ { a } ^ { 2 } + \sigma _ { 0 } ^ { 2 } } { s \sigma _ { a } ^ { 2 } + \sigma _ { 0 } ^ { 2 } } }$ . for 0 ≤ ≤t s

2.22 Let $\{ \boldsymbol { e } _ { t } \}$ be a zero-mean white noise process, and let $c$ be a constant with $\left| c \right| < 1$ . Define $Y _ { t }$ recursively by $Y _ { t } = c Y _ { t - 1 } + e _ { t }$ with $Y _ { 1 } = e _ { 1 }$ .

(a) Show that $E ( Y _ { t } ) = 0$   
(b) Show that $V a r ( Y _ { t } ) = \sigma _ { e } ^ { 2 } ( 1 + c ^ { 2 } + c ^ { 4 } + \cdots + c ^ { 2 t - 2 } )$ . Is $\{ Y _ { t } \}$ stationary?   
(c) Show that

$$
\operatorname {C o r r} \left(Y _ {t}, Y _ {t - 1}\right) = c \sqrt {\frac {\operatorname {V a r} \left(Y _ {t - 1}\right)}{\operatorname {V a r} \left(Y _ {t}\right)}} \text {a n d , i n g e n e r a l},
$$

$$
\operatorname {C o r r} \left(Y _ {t}, Y _ {t - k}\right) = c ^ {k} \sqrt {\frac {\operatorname {V a r} \left(Y _ {t - k}\right)}{\operatorname {V a r} \left(Y _ {t}\right)}} \quad \text {f o r} k > 0
$$

Hint: Argue that $Y _ { t - 1 }$ is independent of $e _ { t }$ . Then use

$$
C o v \left(Y _ {t}, Y _ {t - 1}\right) = C o v \left(c Y _ {t - 1} + e _ {t}, Y _ {t - 1}\right)
$$

(d) For large $t$ , argue that

$$
V a r (Y _ {t}) \approx \frac {\sigma_ {e} ^ {2}}{1 - c ^ {2}} \qquad \text {a n d} \qquad C o r r (Y _ {t}, Y _ {t - k}) \approx c ^ {k} \qquad \text {f o r} k > 0
$$

so that $\{ Y _ { t } \}$ could be called asymptotically stationary.

(e) Suppose now that we alter the initial condition and put $Y _ { 1 } = { \frac { e _ { 1 } } { \sqrt { 1 - c ^ { 2 } } } }$ e1 Show that now $\{ Y _ { t } \}$ is stationary.

2.23 Two processes $\{ Z _ { t } \}$ and $\{ Y _ { t } \}$ are said to be independent if for any time points $t _ { 1 }$ , $t _ { 2 } , \ldots , t _ { m }$ and $s _ { 1 }$ $, s _ { 2 } , \ldots , s _ { n }$ the random variables $\{ Z _ { t _ { 1 } } , Z _ { t _ { 2 } } , . . . , Z _ { t _ { m } } \}$ are independent m of the random variables $\{ Y _ { s _ { 1 } } , Y _ { s _ { 2 } } , . . . , Y _ { s _ { n } } \}$ s sn . Show that if $\{ Z _ { t } \}$ and $\{ Y _ { t } \}$ are independent stationary processes, then $W _ { t } = Z _ { t } + Y _ { t }$ is stationary.

2.24 Let $\{ X _ { t } \}$ be a time series in which we are interested. However, because the measurement process itself is not perfect, we actually observe $Y _ { t } = X _ { t } + e _ { t }$ . We assume that $\{ X _ { t } \}$ and $\{ \boldsymbol { e } _ { t } \}$ are independent processes. We call $X _ { t }$ the signal and $e _ { t }$ the measurement noise or error process.

If $\{ X _ { t } \}$ is stationary with autocorrelation function $\rho _ { k }$ , show that $\{ Y _ { t } \}$ is also stationary with

$$
\operatorname {C o r r} \left(Y _ {t}, Y _ {t - k}\right) = \frac {\rho_ {k}}{1 + \sigma_ {e} ^ {2} / \sigma_ {X} ^ {2}} \quad \text {f o r} k \geq 1
$$

We call $\sigma _ { X } ^ { 2 } / \sigma _ { e } ^ { 2 }$ the signal-to-noise ratio, or SNR. Note that the larger the SNR, the closer the autocorrelation function of the observed process $\{ Y _ { t } \}$ is to the autocorrelation function of the desired signal $\{ X _ { t } \}$ .

2.25 Suppose $Y _ { t } = \beta _ { 0 } + \sum _ { i = 1 } ^ { k } [ A _ { i } \mathrm { c o s } ( 2 \pi f _ { i } t ) + B _ { i } \mathrm { s i n } ( 2 \pi f _ { i } t ) ]$ , where $\beta _ { 0 } , f _ { 1 } , f _ { 2 } , . . . , f _ { k }$ are constants and $1 _ { 1 } , A _ { 2 } , \ldots , A _ { k } , B _ { 1 } , B _ { 2 } , \ldots , B _ { k }$ are independent random variables with zero means and variances $V a r ( A _ { i } ) = V a r ( B _ { i } ) = \sigma _ { i } ^ { 2 }$ . Show that $\{ Y _ { t } \}$ is stationary and find its covariance function.

2.26 Define the function $\Gamma _ { t , s } = { \textstyle \frac { 1 } { 2 } } E [ ( Y _ { t } - Y _ { s } ) ^ { 2 } ]$ . In geostatistics, $\Gamma _ { t , s }$ is called the semivariogram.

(a) Show that for a stationary process .Γt s, γ0 γ t s––= $\Gamma _ { t , s } = \gamma _ { 0 } - \gamma _ { | t - s | } .$   
(b) A process is said to be intrinsically stationary if $\mathrm { T } _ { t , s }$ depends only on the time difference $\left| t - s \right|$ . Show that the random walk process is intrinsically stationary.

2.27 For a fixed, positive integer $r$ and constant $\phi$ , consider the time series defined by $Y _ { t } = e _ { t } + \phi e _ { t - 1 } + \phi ^ { 2 } e _ { t - 2 } + \cdots + \phi ^ { r } e _ { t - r }$ .

(a) Show that this process is stationary for any value of $\phi$   
(b) Find the autocorrelation function.

2.28 (Random cosine wave extended) Suppose that

$$
Y _ {t} = R \cos (2 \pi (f t + \Phi)) \qquad \mathrm {f o r} t = 0, \pm 1, \pm 2, \dots
$$

where $0 < f < \%$ is a fixed frequency and $R$ and $\Phi$ are uncorrelated random variables and with $\Phi$ uniformly distributed on the interval (0,1).

(a) Show that $E ( Y _ { t } ) = 0$ for all $t$   
(b) Show that the process is stationary with $\gamma _ { k } = { \frac { 1 } { 2 } } E ( R ^ { 2 } ) \cos ( 2 \pi f k )$

Hint: Use the calculations leading up to Equation (2.3.4), on page 19.

2.29 (Random cosine wave extended further) Suppose that

$$
Y _ {t} = \sum_ {j = 1} ^ {m} R _ {j} \cos \left[ 2 \pi \left(f _ {j} t + \Phi_ {j}\right) \right] \quad \text {f o r} t = 0, \pm 1, \pm 2, \dots
$$

where $0 < f _ { 1 } < f _ { 2 } < \dots < f _ { m } < Y _ { 2 }$ are $m$ fixed frequencies, and $R _ { 1 } , \Phi _ { 1 } , R _ { 2 } , \Phi _ { 2 } , . . . ,$ $R _ { 1 }$ $R _ { m }$ , $\Phi _ { m }$ are uncorrelated random variables with each $\Phi _ { j }$ uniformly distributed on the interval (0,1).

(a) Show that E(Yt) = 0 $E ( Y _ { t } ) = 0$ for all $t$ .   
(b) Show that the process is stationary with Hint: Do Exercise 2.28 first. $\gamma _ { k } = \frac 1 2 \sum _ { j = 1 } ^ { m } E ( R _ { j } ^ { 2 } ) \cos ( 2 \pi f _ { j } k ) \ .$

2.30 (Mathematical statistics required) Suppose that

$$
Y _ {t} = R \cos [ 2 \pi (f t + \Phi) ] \qquad \mathrm {f o r} t = 0, \pm 1, \pm 2, \dots
$$

where $R$ and $\Phi$ are independent random variables and $f$ is a fixed frequency. The phase $\Phi$ is assumed to be uniformly distributed on $( 0 , 1 )$ , and the amplitude $R$ has a Rayleigh distribution with pdf $\dot { f } ( r ) = r e ^ { - r ^ { 2 } / 2 }$ for $r > 0$ . Show that for each time point t, $Y _ { t }$ has a normal distribution. (Hint: Let $Y = R \cos { [ 2 \pi ( f t + \Phi ) ] }$ and $X = R \sin [ 2 \pi ( f t + \Phi ) ]$ . Now find the joint distribution of $X$ and Y. It can also be shown that all of the finite dimensional distributions are multivariate normal and hence the process is strictly stationary.)

# Appendix A: Expectation, Variance, Covariance, and Correlation

In this appendix, we define expectation for continuous random variables. However, all of the properties described hold for all types of random variables, discrete, continuous, or otherwise. Let $X$ have probability density function $f ( x )$ and let the pair $( X , Y )$ have joint probability density function $f ( x , y )$ .

The expected value of $X$ is defined as $E ( X ) = \int _ { - \infty } ^ { \infty } x f ( x ) d x .$ .

(If $\int _ { - \infty } ^ { \infty } { \left| x \right| } f ( x ) d x < \infty$ ; otherwise $E ( X )$ is undefined.) $E ( X )$ is also called the expectation of $X$ or the mean of $X$ and is often denoted $\mu$ or $\mu _ { X }$ .

# Properties of Expectation

If $h ( x )$ is a function such that $\int _ { - \infty } ^ { \infty } \ | h ( x ) | f ( x ) d x < \infty$ , it may be shown that

$$
E [ h (X) ] = \int_ {- \infty} ^ {\infty} h (x) f (x) d x
$$

Similarly, if $\int _ { - \infty } ^ { \infty } \int _ { - \infty } ^ { \infty } | h ( x , y ) | f ( x , y ) d x d y < \infty$ , it may be shown that

$$
E [ h (X, Y) ] = \int_ {- \infty} ^ {\infty} \int_ {- \infty} ^ {\infty} h (x, y) f (x, y) d x d y \tag {2.A.1}
$$

As a corollary to Equation (2.A.1), we easily obtain the important result

$$
E (a X + b Y + c) = a E (X) + b E (Y) + c \tag {2.A.2}
$$

We also have

$$
E (X Y) = \underline {{\int_ {\infty} ^ {\infty}}} \underline {{\int_ {\infty} ^ {\infty}}} x y f (x, y) d x d y \tag {2.A.3}
$$

The variance of a random variable $X$ is defined as

$$
\operatorname {V a r} (X) = E \left\{\left[ X - E (X) \right] ^ {2} \right\} \tag {2.A.4}
$$

(provided $E ( X ^ { 2 } )$ exists). The variance of $X$ is often denoted by $\sigma ^ { 2 }$ or $\sigma _ { X } ^ { 2 }$ .

# Properties of Variance

$$
\operatorname {V a r} (X) \geq 0 \tag {2.A.5}
$$

$$
\operatorname {V a r} (a + b X) = b ^ {2} \operatorname {V a r} (X) \tag {2.A.6}
$$

If $X$ and $Y$ are independent, then

$$
\operatorname {V a r} (X + Y) = \operatorname {V a r} (X) + \operatorname {V a r} (Y) \tag {2.A.7}
$$

In general, it may be shown that

$$
\operatorname {V a r} (X) = E \left(X ^ {2}\right) - [ E (X) ] ^ {2} \tag {2.A.8}
$$

The positive square root of the variance of $X$ is called the standard deviation of $X$ and is often denoted by $\sigma$ or $\sigma _ { X }$ . The random variable $( X - \mu _ { X } ) / \sigma _ { X }$ is called the standardized version of $X$ . The mean and standard deviation of a standardized variable are always zero and one, respectively.

The covariance of $X$ and $Y$ is defined as $C o \nu ( X , Y ) = E [ ( X - \mu _ { X } ) ( Y - \mu _ { Y } ) ] .$ .

# Properties of Covariance

$$
C o v (a + b X, c + d Y) = b d C o v (X, Y) \tag {2.A.9}
$$

$$
\operatorname {V a r} (X + Y) = \operatorname {V a r} (X) + \operatorname {V a r} (Y) + 2 \operatorname {C o v} (X, Y) \tag {2.A.10}
$$

$$
C o v (X + Y, Z) = C o v (X, Z) + C o v (Y, Z) \tag {2.A.11}
$$

$$
C o v (X, X) = \operatorname {V a r} (X) \tag {2.A.12}
$$

$$
C o v (X, Y) = C o v (Y, X) \tag {2.A.13}
$$

If $X$ and $Y$ are independent,

$$
C o v (X, Y) = 0 \tag {2.A.14}
$$

The correlation coefficient of $X$ and Y, denoted by $C o r r ( { X } , { Y } )$ or $\rho$ , is defined as

$$
\rho = \operatorname {C o r r} (X, Y) = \frac {\operatorname {C o v} (X , Y)}{\sqrt {\operatorname {V a r} (X) \operatorname {V a r} (Y)}}
$$

Alternatively, if $X ^ { * }$ is a standardized $X$ and $Y ^ { \ast }$ is a standardized Y, then $\rho = E ( X ^ { * } Y ^ { * } )$ .

# Properties of Correlation

$$
- 1 \leq \operatorname {C o r r} (X, Y) \leq 1 \tag {2.A.15}
$$

$$
C o r r (a + b X, c + d Y) = \operatorname {s i g n} (b d) C o r r (X, Y)
$$

$$
\text {w h e r e} \operatorname {s i g n} (b d) = \left\{ \begin{array}{l} 1 \text {i f} b d > 0 \\ 0 \text {i f} b d = 0 \\ - 1 \text {i f} b d <   0 \end{array} \right. \tag {2.A.16}
$$

$C o r r ( X , Y ) = \pm 1$ if and only if there are constants $a$ and $^ b$ such that $P r ( Y = a + b X ) = 1$ .

# CHAPTER 3

# TRENDS

In a general time series, the mean function is a totally arbitrary function of time. In a stationary time series, the mean function must be constant in time. Frequently we need to take the middle ground and consider mean functions that are relatively simple (but not constant) functions of time. These trends are considered in this chapter.

# 3.1 Deterministic Versus Stochastic Trends

“Trends” can be quite elusive. The same time series may be viewed quite differently by different analysts. The simulated random walk shown in Exhibit 2.1 might be considered to display a general upward trend. However, we know that the random walk process has zero mean for all time. The perceived trend is just an artifact of the strong positive correlation between the series values at nearby time points and the increasing variance in the process as time goes by. A second and third simulation of exactly the same process might well show completely different “trends.” We ask you to produce some additional simulations in the exercises. Some authors have described such trends as stochastic trends (see Box, Jenkins, and Reinsel, 1994), although there is no generally accepted definition of a stochastic trend.

The average monthly temperature series plotted in Exhibit 1.7 on page 6, shows a cyclical or seasonal trend, but here the reason for the trend is clear — the Northern Hemisphere’s changing inclination toward the sun. In this case, a possible model might be $Y _ { t } = \mu _ { t } + X _ { t } ,$ where $\mu _ { t }$ is a deterministic function that is periodic with period 12; that is $\mu _ { t }$ , should satisfy

$$
\mu_ {t} = \mu_ {t - 1 2} \qquad \text {f o r a l l} t
$$

We might assume that $X _ { t }$ , the unobserved variation around $\mu _ { t }$ , has zero mean for all $t$ so that indeed $\mu _ { t }$ is the mean function for the observed series $Y _ { t }$ . We could describe this model as having a deterministic trend as opposed to the stochastic trend considered earlier. In other situations we might hypothesize a deterministic trend that is linear in time (that is, $\mu _ { t } = \beta _ { 0 } + \beta _ { 1 } t )$ or perhaps a quadratic time trend, $\mu _ { t } = \beta _ { 0 } + \beta _ { 1 } t + \beta _ { 2 } t ^ { 2 }$ . Note that an implication of the model $Y _ { t } = \mu _ { t } + X _ { t }$ with $E ( X _ { t } ) = 0$ for all $t$ is that the deterministic trend $\mu _ { t }$ applies for all time. Thus, if $\mu _ { t } = \beta _ { 0 } + \beta _ { 1 } t$ , we are assuming that the same linear time trend applies forever. We should therefore have good reasons for assuming such a model—not just because the series looks somewhat linear over the time period observed.

In this chapter, we consider methods for modeling deterministic trends. Stochastic trends will be discussed in Chapter 5, and stochastic seasonal models will be discussed in Chapter 10. Many authors use the word trend only for a slowly changing mean function, such as a linear time trend, and use the term seasonal component for a mean function that varies cyclically. We do not find it useful to make such distinctions here.

# 3.2 Estimation of a Constant Mean

We first consider the simple situation where a constant mean function is assumed. Our model may then be written as

$$
Y _ {t} = \mu + X _ {t} \tag {3.2.1}
$$

where $\operatorname { E } ( X _ { t } ) = 0$ for all t. We wish to estimate $\mu$ with our observed time series $Y _ { 1 } , Y _ { 2 } , \ldots$ $Y _ { n }$ . The most common estimate of $\mu$ is the sample mean or average defined as

$$
\bar {Y} = \frac {1}{n} \sum_ {t = 1} ^ {n} Y _ {t} \tag {3.2.2}
$$

Under the minimal assumptions of Equation (3.2.1), we see that_ $\operatorname { E } ( { \overline { { Y } } } ) = \mu$ ; therefore $\overline { { Y } }$ is an unbiased estimate of $\mu$ . To investigate the precision of $\overline { { Y } }$ as an estimate of $\mu$ , we need to make further assumptions concerning $X _ { t }$ .

Suppose that $\{ Y _ { t } \}$ , (or, equivalently, $\{ X _ { t } \}$ of Equation (3.2.1)) is a stationary time series with autocorrelation function $\rho _ { k }$ . Then, by Exercise 2.17, we have

$$
\begin{array}{l} V a r (\bar {Y}) = \frac {\gamma_ {0}}{n} \left[ \sum_ {k = - n + 1} ^ {n - 1} \left(1 - \frac {| k |}{n}\right) \rho_ {k} \right] \\ = \frac {\gamma_ {0}}{n} \left[ 1 + 2 \sum_ {k = 1} ^ {n - 1} \left(1 - \frac {k}{n}\right) \rho_ {k} \right] \tag {3.2.3} \\ \end{array}
$$

Notice that the first factor, $\gamma _ { 0 } / n$ , is the process (population) variance divided by the sample size—a concept with which we are familiar in simpler random sampling contexts. If the series $\{ X _ { t } \}$ of Equation (3.2.1) is just white noise, then $\rho _ { k } = 0$ for $k > 0$ and $V a r ( \overline { { Y } } )$ reduces to simply $\gamma _ { 0 } / n$ .

In the (stationary) moving average model $Y _ { t } = e _ { t } - \sqrt [ 1 ] { 2 e _ { t - 1 } }$ , we find that $\rho _ { 1 } = - 0 . 4$ and ${ \rho } _ { k } = 0$ for $k > 1$ . In this case, we have

$$
\begin{array}{l} V a r (\bar {Y}) = \frac {\gamma_ {0}}{n} \Big [ 1 + 2 \Big (1 - \frac {1}{n} \Big) (- 0. 4) \Big ] \\ = \frac {\gamma_ {0}}{n} \bigg [ 1 - 0. 8 \bigg (\frac {n - 1}{n} \bigg) \bigg ] \\ \end{array}
$$

For values of $n$ usually occurring in time series $( n > 5 0$ , say), the factor $( n - 1 ) / n$ will be close to 1, so that we have

$$
V a r (\overline {{Y}}) \approx 0. 2 \frac {\gamma_ {0}}{n}
$$

We see that the negative correlation at lag 1 has improved the estimation of the mean compared with the estimation obtained in the white noise (random sample) situation. Because the series tends to oscillate back and forth across the mean, the sample mean obtained is more precise.

On the other hand, if $\rho _ { k } \geq 0$ for all $k \geq 1$ , we see from Equation (3.2.3) that $V a r ( \overline { { Y } } )$ will be larger than $\gamma _ { 0 } / n$ . Here the positive correlations make estimation of the mean more difficult than in the white noise case. In general, some correlations will be positive and some negative, and Equation (3.2.3) must be used to assess the total effect.

For many stationary processes, the autocorrelation function decays quickly enough with increasing lags that

$$
\sum_ {k = 0} ^ {\infty} \left| \rho_ {k} \right| <   \infty \tag {3.2.4}
$$

(The random cosine wave of Chapter 2 is an exception.)

Under assumption (3.2.4) and given a large sample size $n$ , the following useful approximation follows from Equation (3.2.3) (See Anderson, 1971, p. 459, for example)

$$
\operatorname {V a r} (\bar {Y}) \approx \frac {\gamma_ {0}}{n} \left[ \sum_ {k = - \infty} ^ {\infty} \rho_ {k} \right] \quad \text {f o r l a r g e} n \tag {3.2.5}
$$

Notice that to this approximation the variance is inversely proportional to the sample size $n$ .

As an example, suppose that $\rho _ { k } = \phi ^ { | k | }$ for all $k$ , where $\boldsymbol { \Phi }$ is a number strictly between $^ { - 1 }$ and $+ 1$ . Summing a geometric series yields

$$
\operatorname {V a r} (\bar {Y}) \approx \frac {(1 + \phi) ^ {\gamma_ {0}}}{(1 - \phi) n} \tag {3.2.6}
$$

For a nonstationary process (but with a constant mean), the precision of the sample mean as an estimate of $\mu$ can be strikingly different. As a useful example, suppose that in Equation (3.2.1) $\{ X _ { t } \}$ is a random walk process as described in Chapter 2. Then directly from Equation (2.2.8) we have

$$
\begin{array}{l} V a r (\bar {Y}) = \frac {1}{n ^ {2}} V a r \left[ \sum_ {i = 1} ^ {n} Y _ {i} \right] \\ = \frac {1}{n ^ {2}} V a r \left[ \sum_ {i = 1} ^ {n} \sum_ {j = 1} ^ {i} e _ {j} \right] \\ \end{array}
$$

$$
\begin{array}{l} = \frac {1}{n ^ {2}} \operatorname {V a r} \left(e _ {1} + 2 e _ {2} + 3 e _ {3} + \dots + n e _ {n}\right) \\ = \frac {\sigma_ {e} ^ {2}}{n ^ {2}} \sum_ {k = 1} ^ {n} k ^ {2} \\ \end{array}
$$

so that

$$
\operatorname {V a r} (\bar {Y}) = \sigma_ {e} ^ {2} (2 n + 1) \frac {(n + 1)}{6 n} \tag {3.2.7}
$$

Notice that in this special case the variance of our estimate of the mean actually increases as the sample size $n$ increases. Clearly this is unacceptable, and we need to consider other estimation techniques for nonstationary series.

# 3.3 Regression Methods

The classical statistical method of regression analysis may be readily used to estimate the parameters of common nonconstant mean trend models. We shall consider the most useful ones: linear, quadratic, seasonal means, and cosine trends.

# Linear and Quadratic Trends in Time

Consider the deterministic time trend expressed as

$$
\mu_ {t} = \beta_ {0} + \beta_ {1} t \tag {3.3.1}
$$

where the slope and intercept, $\beta _ { 1 }$ and $\beta _ { 0 }$ respectively, are unknown parameters. The classical least squares (or regression) method is to choose as estimates of $\beta _ { 1 }$ and $\beta _ { 0 }$ values that minimize

$$
Q (\beta_ {0}, \beta_ {1}) = \sum_ {t = 1} ^ {n} [ Y _ {t} - (\beta_ {0} + \beta_ {1} t) ] ^ {2}
$$

The solution may be obtained in several ways, for example, by computing the partial derivatives with respect to both $\beta$ ’s, setting the results equal to zero, and solving the resulting linear equations for the $\beta$ ’s. Denoting the solutions by $\hat { \beta } _ { 0 }$ and $\hat { \beta } _ { 1 }$ , we find that

$$
\begin{array}{l} \hat {\beta} _ {1} = \frac {\sum_ {t = 1} ^ {n} \left(Y _ {t} - \bar {Y}\right) (t - \bar {t})}{\sum_ {t = 1} ^ {n} (t - \bar {t}) ^ {2}} \tag {3.3.2} \\ \hat {\beta} _ {0} = \bar {Y} - \hat {\beta} _ {1} \bar {t} \\ \end{array}
$$

where $\overline { { t } } { \mathrm { ~ } } = ( n + 1 ) / 2$ is the average of 1, 2,…, n. These formulas can be simplified somewhat, and various versions of the formulas are well-known. However, we assume that

the computations will be done by statistical software and we will not pursue other expressions for $\hat { \beta } _ { 0 }$ and $\hat { \beta } _ { 1 }$ here.

# Example

Consider the random walk process that was shown in Exhibit 2.1. Suppose we (mistakenly) treat this as a linear time trend and estimate the slope and intercept by least-squares regression. Using statistical software we obtain Exhibit 3.1.

Exhibit 3.1 Least Squares Regression Estimates for Linear Time Trend   

<table><tr><td></td><td>Estimate</td><td>Std. Error</td><td>t value</td><td>Pr(&gt;|t|)</td></tr><tr><td>Intercept</td><td>-1.008</td><td>0.2972</td><td>-3.39</td><td>0.00126</td></tr><tr><td>Time</td><td>0.1341</td><td>0.00848</td><td>15.82</td><td>&lt; 0.0001</td></tr></table>

```txt
> data(rwalk)  
> model1=lm(rwalk~time(rwalk))  
> summary(model1) 
```

So here the estimated slope and intercept are $\hat { \beta } _ { 1 } = 0 . 1 3 4 1$ and $\hat { \beta } _ { 0 } = - 1 . 0 0 8$ , respectively. Exhibit 3.2 displays the random walk with the least squares regression trend line superimposed. We will interpret more of the regression output later in Section 3.5 on page 40 and see that fitting a line to these data is not appropriate.

![](images/40bdcb84637b463da8b157e5fa4be36eb800ac16d78234bb4b4ddbcbbf3d9878.jpg)  
Exhibit 3.2 Random Walk with Linear Time Trend

```python
> win.graph(width=4.875, height=2.5, pointsize=8)
> plot(rwalk,type='o', ylab='y')
> abline(model1) # add the fitted least squares line from model1 
```

# Cyclical or Seasonal Trends

Consider now modeling and estimating seasonal trends, such as for the average monthly temperature data in Exhibit 1.7. Here we assume that the observed series can be represented as

$$
Y _ {t} = \mu_ {t} + X _ {t}
$$

where $E ( X _ { t } ) = 0$ for all $t$ .

The most general assumption for $\mu _ { t }$ with monthly seasonal data is that there are 12 constants (parameters), $\beta _ { 1 }$ , $\beta _ { 2 } , \ldots$ , and $\beta _ { 1 2 }$ , giving the expected average temperature for each of the 12 months. We may write

$$
\mu_ {t} = \left\{ \begin{array}{l l} \beta_ {1} & \text {f o r} t = 1, 1 3, 2 5, \dots \\ \beta_ {2} & \text {f o r} t = 2, 1 4, 2 6, \dots \\ \vdots & \\ \beta_ {1 2} & \text {f o r} t = 1 2, 2 4, 3 6, \dots \end{array} \right. \tag {3.3.3}
$$

This is sometimes called a seasonal means model.

As an example of this model consider the average monthly temperature data shown in Exhibit 1.7 on page 6. To fit such a model, we need to set up indicator variables (sometimes called dummy variables) that indicate the month to which each of the data points pertains. The procedure for doing this will depend on the particular statistical software that you use. We also need to note that the model as stated does not contain an intercept term, and the software will need to know this also. Alternatively, we could use an intercept and leave out any one of the $\beta$ ’s in Equation (3.3.3).

Exhibit 3.3 displays the results of fitting the seasonal means model to the temperature data. Here the t-values and $P r ( > | t | )$ -values reported are of little interest since they relate to testing the null hypotheses that the $\beta$ ’s are zero—not an interesting hypothesis in this case.

Exhibit 3.3 Regression Results for the Seasonal Means Model   

<table><tr><td></td><td>Estimate</td><td>Std. Error</td><td>t-value</td><td>Pr(&gt;|t|)</td></tr><tr><td>January</td><td>16.608</td><td>0.987</td><td>16.8</td><td>&lt; 0.0001</td></tr><tr><td>February</td><td>20.650</td><td>0.987</td><td>20.9</td><td>&lt; 0.0001</td></tr><tr><td>March</td><td>32.475</td><td>0.987</td><td>32.9</td><td>&lt; 0.0001</td></tr><tr><td>April</td><td>46.525</td><td>0.987</td><td>47.1</td><td>&lt; 0.0001</td></tr><tr><td>May</td><td>58.092</td><td>0.987</td><td>58.9</td><td>&lt; 0.0001</td></tr><tr><td>June</td><td>67.500</td><td>0.987</td><td>68.4</td><td>&lt; 0.0001</td></tr><tr><td>July</td><td>71.717</td><td>0.987</td><td>72.7</td><td>&lt; 0.0001</td></tr></table>

<table><tr><td></td><td>Estimate</td><td>Std. Error</td><td>t-value</td><td>Pr(&gt;|t|)</td></tr><tr><td>August</td><td>69.333</td><td>0.987</td><td>70.2</td><td>&lt; 0.0001</td></tr><tr><td>September</td><td>61.025</td><td>0.987</td><td>61.8</td><td>&lt; 0.0001</td></tr><tr><td>October</td><td>50.975</td><td>0.987</td><td>51.6</td><td>&lt; 0.0001</td></tr><tr><td>November</td><td>36.650</td><td>0.987</td><td>37.1</td><td>&lt; 0.0001</td></tr><tr><td>December</td><td>23.642</td><td>0.987</td><td>24.0</td><td>&lt; 0.0001</td></tr><tr><td colspan="5">&gt; data(tempdb)</td></tr><tr><td colspan="5">&gt; month.=season(tempdb) # period added to improve table display</td></tr><tr><td colspan="5">&gt; model2=lm(tempdb~month.-1) # -1 removes the intercept term</td></tr><tr><td colspan="5">&gt; summary(model2)</td></tr></table>

Exhibit 3.4 shows how the results change when we fit a model with an intercept term. The software omits the January coefficient in this case. Now the February coefficient is interpreted as the difference between February and January average temperatures, the March coefficient is the difference between March and January average temperatures, and so forth. Once more, the $t$ -values and $P r ( > | t | )$ $\scriptstyle { p }$ -values) are testing hypotheses of little interest in this case. Notice that the Intercept coefficient plus the February coefficient here equals the February coefficient displayed in Exhibit 3.3.

Exhibit 3.4 Results for Seasonal Means Model with an Intercept   

<table><tr><td></td><td>Estimate</td><td>Std. Error</td><td>t-value</td><td>Pr(&gt;|t|)</td></tr><tr><td>Intercept</td><td>16.608</td><td>0.987</td><td>16.83</td><td>&lt; 0.0001</td></tr><tr><td>February</td><td>4.042</td><td>1.396</td><td>2.90</td><td>0.00443</td></tr><tr><td>March</td><td>15.867</td><td>1.396</td><td>11.37</td><td>&lt; 0.0001</td></tr><tr><td>April</td><td>29.917</td><td>1.396</td><td>21.43</td><td>&lt; 0.0001</td></tr><tr><td>May</td><td>41.483</td><td>1.396</td><td>29.72</td><td>&lt; 0.0001</td></tr><tr><td>June</td><td>50.892</td><td>1.396</td><td>36.46</td><td>&lt; 0.0001</td></tr><tr><td>July</td><td>55.108</td><td>1.396</td><td>39.48</td><td>&lt; 0.0001</td></tr><tr><td>August</td><td>52.725</td><td>1.396</td><td>37.78</td><td>&lt; 0.0001</td></tr><tr><td>September</td><td>44.417</td><td>1.396</td><td>31.82</td><td>&lt; 0.0001</td></tr><tr><td>October</td><td>34.367</td><td>1.396</td><td>24.62</td><td>&lt; 0.0001</td></tr><tr><td>November</td><td>20.042</td><td>1.396</td><td>14.36</td><td>&lt; 0.0001</td></tr><tr><td>December</td><td>7.033</td><td>1.396</td><td>5.04</td><td>&lt; 0.0001</td></tr></table>

> model3=lm(tempdub~month.) # January is dropped automatically > summary(model3)

# Cosine Trends

The seasonal means model for monthly data consists of 12 independent parameters and does not take the shape of the seasonal trend into account at all. For example, the fact that the March and April means are quite similar (and different from the June and July means) is not reflected in the model. In some cases, seasonal trends can be modeled economically with cosine curves that incorporate the smooth change expected from one time period to the next while still preserving the seasonality.

Consider the cosine curve with equation

$$
\mu_ {t} = \beta \cos (2 \pi f t + \Phi) \tag {3.3.4}
$$

We call $\beta \left( > 0 \right)$ the amplitude, $f$ the frequency, and $\Phi$ the phase of the curve. As $t$ varies, the curve oscillates between a maximum of $\beta$ and a minimum of $- \beta$ . Since the curve repeats itself exactly every 1/f time units, 1/f is called the period of the cosine wave. As noted in Chapter 2, $\Phi$ serves to set the arbitrary origin on the time axis. For monthly data with time indexed as 1, 2,…, the most important frequency is $f = 1 / 1 2$ , because such a cosine wave will repeat itself every 12 months. We say that the period is 12.

Equation (3.3.4) is inconvenient for estimation because the parameters $\beta$ and $\Phi$ do not enter the expression linearly. Fortunately, a trigonometric identity is available that reparameterizes (3.3.4) more conveniently, namely

$$
\beta \cos (2 \pi f t + \Phi) = \beta_ {1} \cos (2 \pi f t) + \beta_ {2} \sin (2 \pi f t) \tag {3.3.5}
$$

where

$$
\beta = \sqrt {\beta_ {1} ^ {2} + \beta_ {2} ^ {2}}, \quad \Phi = \operatorname {a t a n} \left(- \beta_ {2} / \beta_ {1}\right) \tag {3.3.6}
$$

and, conversely,

$$
\beta_ {1} = \beta \cos (\Phi), \quad \beta_ {2} = \beta \sin (\Phi) \tag {3.3.7}
$$

To estimate the parameters $\beta _ { 1 }$ and $\beta _ { 2 }$ with regression techniques, we simply use $\cos ( 2 \pi f t )$ and $\sin ( 2 \pi f t )$ as regressors or predictor variables.

The simplest such model for the trend would be expressed as

$$
\mu_ {t} = \beta_ {0} + \beta_ {1} \cos (2 \pi f t) + \beta_ {2} \sin (2 \pi f t) \tag {3.3.8}
$$

Here the constant term, $\beta _ { 0 }$ , can be meaningfully thought of as a cosine with frequency zero.

In any practical example, we must be careful how we measure time, as our choice of time measurement will affect the values of the frequencies of interest. For example, if we have monthly data but use 1, 2, 3,... as our time scale, then 1/12 would be the most interesting frequency, with a corresponding period of 12 months. However, if we measure time by year and fractional year, say 1980 for January, 1980.08333 for February of 1980, and so forth, then a frequency of 1 corresponds to an annual or 12 month periodicity.

Exhibit 3.5 is an example of fitting a cosine curve at the fundamental frequency to the average monthly temperature series.

Exhibit 3.5 Cosine Trend Model for Temperature Series   

<table><tr><td>Coefficient</td><td>Estimate</td><td>Std. Error</td><td>t-value</td><td>Pr(&gt;|t|)</td></tr><tr><td>Intercept</td><td>46.2660</td><td>0.3088</td><td>149.82</td><td>&lt; 0.0001</td></tr><tr><td>cos(2πt)</td><td>-26.7079</td><td>0.4367</td><td>-61.15</td><td>&lt; 0.0001</td></tr><tr><td>sin(2πt)</td><td>-2.1697</td><td>0.4367</td><td>-4.97</td><td>&lt;0.0001</td></tr></table>

> har. $=$ harmonic(tempdub,1)   
> model4=lm(tempdub~har.)   
> summary(model4)

In this output, time is measured in years, with 1964 as the starting value and a frequency of 1 per year. A graph of the time series values together with the fitted cosine curve is shown in Exhibit 3.6. The trend fits the data quite well with the exception of most of the January values, where the observations are lower than the model would predict.

![](images/46b7416ba7d81637ae932f6bd300ee0a885e5859d34220ec5ac5c8aafd6b1556.jpg)  
Exhibit 3.6 Cosine Trend for the Temperature Series

> win.graph(width=4.875, height $^ { . = 2 }$ .5,pointsize=8)   
> plot(ts(fitted(model4),freq $= \mathtt { 1 2 }$ ,start ${ \bf \Psi } = { \bf C }$ (1964,1)),

ylab $^ { \prime = }$ 'Temperature',type='l',

> ylim $\iota =$ range(c(fitted(model4),tempdub))); points(tempdub)   
> # ylim ensures that the y axis range fits the raw data and the fitted values

Additional cosine functions at other frequencies will frequently be used to model cyclical trends. For monthly series, the higher harmonic frequencies, such as 2/12 and 3/12, are especially pertinent and will sometimes improve the fit at the expense of add-

ing more parameters to the model. In fact, it may be shown that any periodic trend with period 12 may be expressed exactly by the sum of six pairs of cosine-sine functions. These ideas are discussed in detail in Fourier analysis or spectral analysis. We pursue these ideas further in Chapters 13 and 14.

# 3.4 Reliability and Efficiency of Regression Estimates

We assume that the series is represented as $Y _ { t } = \mu _ { t } + X _ { t } ,$ where $\mu _ { t }$ is a deterministic trend of the kind considered above and $\{ X _ { t } \}$ is a zero-mean stationary process with autocovariance and autocorrelation functions $\gamma _ { k }$ and $\rho _ { k }$ , respectively. Ordinary regression estimates parameters in a linear model according to the criterion of least squares regardless of whether we are fitting linear time trends, seasonal means, cosine curves, or whatever.

We first consider the easiest case—the seasonal means. As mentioned earlier, the least squares estimates of the seasonal means are just seasonal averages; thus, if we have $N$ (complete) years of monthly data, we can write the estimate for the mean for the jth season as

$$
\hat {\beta} _ {j} = \frac {1}{N} \sum_ {i = 0} ^ {N - 1} Y _ {j + 1 2 i}
$$

Since $\hat { \beta } _ { j }$ is an average like $\overline { { Y } }$ but uses only every 12th observation, Equation (3.2.3) can be easily modified to give $V a r ( \hat { \beta } _ { j } )$ . We replace $n$ by $N$ (years) and $\rho _ { k }$ by $\rho _ { 1 2 k }$ to get

$$
V a r \left(\hat {\beta} _ {j}\right) = \frac {\gamma_ {0}}{N} \left[ 1 + 2 \sum_ {k = 1} ^ {N - 1} \left(1 - \frac {k}{N}\right) \rho_ {1 2 k} \right] \quad \text {f o r} j = 1, 2, \dots , 1 2 \tag {3.4.1}
$$

We notice that if $\{ X _ { t } \}$ is white noise, then $V a r ( \hat { \beta } _ { j } )$ reduces to $\gamma _ { 0 } / N$ , as expected. Furthermore, if several $\rho _ { k }$ are nonzero but $\rho _ { 1 2 k } = 0$ , then we still have $V a r ( \hat { \beta } _ { j } ) = \gamma _ { 0 } / N .$ . In any case, only the seasonal autocorrelations, ρ12, ρ24, ρ36 ,..., enter into Equation (3.4.1). Since $N$ will rarely be very large (except perhaps for quarterly data), approximations like those shown in Equation (3.2.5) will usually not be useful.

We turn now to the cosine trends expressed as in Equation (3.3.8). For any frequency of the form $f { = } m / n$ , where m is an integer satisfying $1 \leq m < n / 2$ , explicit expressions are available for the estimates ${ \widehat { \beta } } _ { 1 }$ and ${ \widehat { \beta } } _ { 2 }$ , the amplitudes of the cosine and sine:

$$
\hat {\beta} _ {1} = \frac {2}{n} \sum_ {t = 1} ^ {n} \left[ \cos \left(\frac {2 \pi m t}{n}\right) Y _ {t} \right], \quad \hat {\beta} _ {2} = \frac {2}{n} \sum_ {t = 1} ^ {n} \left[ \sin \left(\frac {2 \pi m t}{n}\right) Y _ {t} \right] \tag {3.4.2}
$$

(These are effectively the correlations between the time series $\{ Y _ { t } \}$ and the cosine and sine waves with frequency m/n.)

Because these are linear functions of $\{ Y _ { t } \}$ , we may evaluate their variances using Equation (2.2.6). We find

$$
V a r \left(\hat {\beta} _ {1}\right) = \frac {2 \gamma_ {0}}{n} \left[ 1 + \frac {4}{n} \sum_ {s = 2} ^ {n} \sum_ {t = 1} ^ {s - 1} \cos \left(\frac {2 \pi m t}{n}\right) \cos \left(\frac {2 \pi m s}{n}\right) \rho_ {s - t} \right] \tag {3.4.3}
$$

where we have used the fact that $\sum _ { t \ : = \ : 1 } ^ { n } [ \cos ( 2 \pi m t / n ) ] ^ { 2 } \ : = \ : n / 2 .$ However, the double sum in Equation (3.4.3) does not, in general, reduce further. A similar expression holds for $V a r ( \hat { \boldsymbol { \beta } } _ { 2 } )$ if we replace the cosines by sines.

If $\{ X _ { t } \}$ is white noise, we get just $2 \gamma _ { 0 } / n$ . If ${ \rho } _ { 1 } \neq 0$ , $\rho _ { k } = 0$ for $k > 1$ , and $m / n = 1 / 1 2$ , then the variance reduces to

$$
V a r \left(\hat {\beta} _ {1}\right) = \frac {2 \gamma_ {0}}{n} \left[ 1 + \frac {4 \rho_ {1}}{n} \sum_ {t = 1} ^ {n - 1} \cos \left(\frac {\pi t}{6}\right) \cos \left(\frac {\pi t + 1}{6}\right) \right] \tag {3.4.4}
$$

To illustrate the effect of the cosine terms, we have calculated some representative values:

<table><tr><td>n</td><td>Var(β1)</td></tr><tr><td>25</td><td>(2γ0/n)(1+1.71ρ1)</td></tr><tr><td>50</td><td>(2γ0/n)(1+1.75ρ1)</td></tr><tr><td>500</td><td>(2γ0/n)(1+1.73ρ1)</td></tr><tr><td>∞</td><td>(2γ0/n)(1+2ρ1cos(π/6)) = (2γ0/n)(1+1.732ρ1)</td></tr></table>

If $\rho _ { 1 } = - 0 . 4$ , then the large sample multiplier in Equation (3.4.5) is $1 + 1 . 7 3 2 ( - 0 . 4 ) =$ 0.307 and the variance is reduced by about $70 \%$ when compared with the white noise case.

In some circumstances, seasonal means and cosine trends could be considered as competing models for a cyclical trend. If the simple cosine model is an adequate model, how much do we lose if we use the less parsimonious seasonal means model? To approach this problem, we must first consider how to compare the models. The parameters themselves are not directly comparable, but we can compare the estimates of the trend at comparable time points.

Consider the two estimates for the trend in January; that is, $\mu _ { 1 }$ . With seasonal means, this estimate is just the January average, which has variance given by Equation (3.4.1). With the cosine trend model, the corresponding estimate is

$$
\hat {\mu} _ {1} = \hat {\beta} _ {0} + \hat {\beta} _ {1} \cos \left(\frac {2 \pi}{1 2}\right) + \hat {\beta} _ {2} \sin \left(\frac {2 \pi}{1 2}\right)
$$

To compute the variance of this estimate, we need one more fact: With this model, the estimates $\ddot { \beta } _ { 0 }$ , $\ddot { \beta } _ { 1 }$ , and ${ \ddot { \beta } } _ { 2 }$ are uncorrelated.† This follows from the orthogonality relationships of the cosines and sines involved. See Bloomfield (1976) or Fuller (1996) for more details. For the cosine model, then, we have

$$
V a r \left(\hat {\mu} _ {1}\right) = V a r \left(\hat {\beta} _ {0}\right) + V a r \left(\hat {\beta} _ {1}\right) \left[ \cos \left(\frac {2 \pi}{1 2}\right) \right] ^ {2} + V a r \left(\hat {\beta} _ {2}\right) \left[ \sin \left(\frac {2 \pi}{1 2}\right) \right] ^ {2} \tag {3.4.6}
$$

For our first comparison, assume that the stochastic component is white noise. Then the variance of our estimate in the seasonal means model is just $\gamma _ { 0 } / N .$ For the cosine model, we use Equation (3.4.6), and Equation (3.4.4) and its sine equivalent, to obtain

$$
\begin{array}{l} V a r (\hat {\mu} _ {1}) = \frac {\gamma_ {0}}{n} \left\{1 + 2 \left[ \cos \left(\frac {\pi}{6}\right) \right] ^ {2} + 2 \left[ \sin \left(\frac {\pi}{6}\right) \right] ^ {2} \right\} \\ = 3 \frac {\gamma_ {0}}{n} \\ \end{array}
$$

since $( \cos \theta ) ^ { 2 } + ( \sin \theta ) ^ { 2 } = 1$ . Thus the ratio of the standard deviation in the cosine model to that in the seasonal means model is

$$
\sqrt {\frac {3 \gamma_ {0} / n}{\gamma_ {0} / N}} = \sqrt {\frac {3 N}{n}}
$$

In particular, for the monthly temperature series, we have $n = 1 4 4$ and $N = 1 2$ ; thus, the ratio is

$$
\sqrt {\frac {3 (1 2)}{1 4 4}} = 0. 5
$$

Thus, in the cosine model, we estimate the January effect with a standard deviation that is only half as large as it would be if we estimated with a seasonal means model—a substantial gain. (Of course, this assumes that the cosine trend plus white noise model is the correct model.)

Suppose now that the stochastic component is such that ${ \rho } _ { 1 } \neq 0$ but $\rho _ { k } = 0$ for $k > 1$ . With a seasonal means model, the variance of the estimated January effect will be unchanged (see Equation (3.4.1) on page 36). For the cosine trend model, if we have a reasonably large sample size, we may use Equation (3.4.5), an identical expression for $V a r ( \hat { \beta } _ { 2 } )$ , and Equation (3.2.3) on page 28 for $V a r ( \hat { \beta } _ { 0 } )$ to obtain

$$
\begin{array}{l} V a r (\hat {\mu} _ {1}) = \frac {\gamma_ {0}}{n} \Bigg \{1 + 2 \rho_ {1} + 2 \bigg [ 1 + 2 \rho_ {1} \cos \bigg (\frac {2 \pi}{1 2} \bigg) \bigg ] \Bigg \} \\ = \frac {\gamma_ {0}}{n} \left\{3 + 2 \rho_ {1} \left[ 1 + 2 \cos \left(\frac {\pi}{6}\right) \right] \right\} \\ \end{array}
$$

If $\rho _ { 1 } = - 0 . 4$ , then we have $0 . 8 1 4 \gamma _ { 0 } / n$ , and the ratio of the standard deviation in the cosine case to the standard deviation in the seasonal means case is

$$
\sqrt {\left[ \frac {(0 . 8 1 4 \gamma_ {0}) / n}{\gamma_ {0} / N} \right]} = \sqrt {\frac {0 . 8 1 4 N}{n}}
$$

If we take $n = 1 4 4$ and $N = 1 2$ , the ratio is

$$
\sqrt {\frac {0 . 8 1 4 (1 2)}{1 4 4}} = 0. 2 6
$$

a very substantial reduction indeed!

We now turn to linear time trends. For these trends, an alternative formula to Equation (3.3.2) on page 30 for ${ \widehat { \beta } } _ { 1 }$ is more convenient. It can be shown that the least squares estimate of the slope may be written

$$
\hat {\beta} _ {1} = \frac {\sum_ {t = 1} ^ {n} (t - \bar {t}) Y _ {t}}{\sum_ {t = 1} ^ {n} (t - \bar {t}) ^ {2}} \tag {3.4.7}
$$

Since the estimate is a linear combination of Y-values, some progress can be made in evaluating its variance. We have

$$
V a r \left(\hat {\beta} _ {1}\right) = \frac {1 2 \gamma_ {0}}{n \left(n ^ {2} - 1\right)} \left[ 1 + \frac {2 4}{n \left(n ^ {2} - 1\right)} \sum_ {s = 2} ^ {n} \sum_ {t = 1} ^ {s - 1} \left(t - \bar {t}\right) \left(s - \bar {t}\right) \rho_ {s - t} \right] \tag {3.4.8}
$$

where we have used $\Sigma _ { t = 1 } ^ { n } ( t - \overline { { t } } ) ^ { 2 } = n ( n ^ { 2 } - 1 ) / 1 2$ . Again the double sum does not in general reduce.

To illustrate the effect of Equation (3.4.8), consider again the case where ${ \rho } _ { 1 } \neq 0$ but ${ \rho } _ { k } = 0$ for $k > 1$ . Then, after some algebraic manipulation, again involving the sum of consecutive integers and their squares, Equation (3.4.8) can be reduced to

$$
V a r (\hat {\beta} _ {1}) = \frac {1 2 \gamma_ {0}}{n (n ^ {2} - 1)} \Big [ 1 + 2 \rho_ {1} \bigg (1 - \frac {3}{n} \bigg) \Big ]
$$

For large $n$ , we can neglect the $3 / n$ term and use

$$
\operatorname {V a r} \left(\hat {\beta} _ {1}\right) = \frac {1 2 \gamma_ {0} \left(1 + 2 \rho_ {1}\right)}{n \left(n ^ {2} - 1\right)} \tag {3.4.9}
$$

If $\rho _ { 1 } = - 0 . 4$ , then $1 + 2 \rho _ { 1 } = 0 . 2$ , and then the variance of $\hat { \boldsymbol \beta } _ { 1 }$ is only $20 \%$ of what it would be if $\{ X _ { t } \}$ were white noise. Of course, if $\rho _ { 1 } > 0$ , then the variance would be larger than for the white noise case.

We turn now to comparing the least squares estimates with the so-called best linear unbiased estimates (BLUE) or the generalized least squares (GLS) estimates. If the stochastic component $\{ X _ { t } \}$ is not white noise, estimates of the unknown parameters in the trend function may be made; they are linear functions of the data, are unbiased, and have the smallest variances among all such estimates—the so-called BLUE or GLS estimates. These estimates and their variances can be expressed fairly explicitly by using certain matrices and their inverses. (Details may be found in Draper and Smith (1981).) However, constructing these estimates requires complete knowledge of the covariance function of the stochastic component, a function that is unknown in virtually all real applications. It is possible to iteratively estimate the covariance function for $\{ X _ { t } \}$ based on a preliminary estimate of the trend. The trend is then estimated again using the estimated covariance function for $\{ X _ { t } \}$ and thus iterated to an approximate BLUE for the trend. These methods are pursued further in Chapter 11.

Fortunately, there are some results based on large sample sizes that support the use of the simpler least squares estimates for the types of trends that we have considered. In particular, we have the following result (see Fuller (1996), pp. 476–480, for more details): We assume that the trend is either a polynomial in time, a trigonometric polynomial, seasonal means, or a linear combination of these. Then, for a very general stationary stochastic component $\{ X _ { t } \}$ , the least squares estimates for the trend have the same variance as the best linear unbiased estimates for large sample sizes.

Although the simple least squares estimates may be asymptotically efficient, it does not follow that the estimated standard deviations of the coefficients as printed out by all regression routines are correct. We shall elaborate on this point in the next section. We also caution the reader that the result above is restricted to certain kinds of trends and cannot, in general, be extended to regression on arbitrary predictor variables, such as other time series. For example, Fuller (1996, pp. 518–522) shows that if $Y _ { t } = \beta Z _ { t } + X _ { t } ,$ where $\{ X _ { t } \}$ has a simple stochastic structure but $\{ Z _ { t } \}$ is also a stationary series, then the least squares estimate of $\beta$ can be very inefficient and biased even for large samples.

# 3.5 Interpreting Regression Output

We have already noted that the standard regression routines calculate least squares estimates of the unknown regression coefficients—the betas. As such, the estimates are reasonable under minimal assumptions on the stochastic component $\{ X _ { t } \}$ . However, some of the properties of the regression output depend heavily on the usual regression assumption that $\{ X _ { t } \}$ is white noise, and some depend on the further assumption that $\{ X _ { t } \}$ is approximately normally distributed. We begin with the items that depend least on the assumptions.

Consider the regression output shown in Exhibit 3.7. We shall write $\widehat { \mathbf { \mu } } _ { t }$ for the estimated trend regardless of the assumed parametric form for $\mu _ { t }$ . For example, for the linear time trend, we have $\mu _ { t } = \beta _ { 0 } + \beta _ { 1 } t$ . For each $t$ , the unobserved stochastic component

$X _ { t }$ can be estimated (predicted) by $Y _ { t } - \hat { \mu } _ { t }$ . If the $\{ X _ { t } \}$ process has constant variance, then we can estimate the standard deviation of $X _ { t } ,$ namely $\sqrt { \gamma _ { 0 } }$ , by the residual standard deviation

$$
s = \sqrt {\frac {1}{n - p} \sum_ {t = 1} ^ {n} \left(Y _ {t} - \hat {\mu} _ {t}\right) ^ {2}} \tag {3.5.1}
$$

where $p$ is the number of parameters estimated in $\mu _ { t }$ and $n - p$ is the so-called degrees of freedom for $s$ . The value of $s$ gives an absolute measure of the goodness of fit of the estimated trend—the smaller the value of $s$ , the better the fit. However, a value of $s$ of, say, 60.74 is somewhat difficult to interpret.

A unitless measure of the goodness of fit of the trend is the value of $R ^ { 2 }$ , also called the coefficient of determination or multiple $R$ -squared. One interpretation of $R ^ { 2 }$ is that it is the square of the sample correlation coefficient between the observed series and the estimated trend. It is also the fraction of the variation in the series that is explained by the estimated trend. Exhibit 3.7 is a more complete regression output when fitting the straight line to the random walk data. This extends what we saw in Exhibit 3.1 on page 31.

Exhibit 3.7 Regression Output for Linear Trend Fit of Random Walk   

<table><tr><td></td><td>Estimate</td><td>Std. Error</td><td>t-value</td><td>Pr(&gt;|t|)</td></tr><tr><td>Intercept</td><td>-1.007888</td><td>0.297245</td><td>-3.39</td><td>0.00126</td></tr><tr><td>Time</td><td>0.134087</td><td>0.008475</td><td>15.82</td><td>&lt; 0.0001</td></tr><tr><td colspan="2">Residual standard error</td><td>1.137</td><td colspan="2">with 58 degrees of freedom</td></tr><tr><td colspan="2">Multiple R-Squared</td><td colspan="3">0.812</td></tr><tr><td colspan="2">Adjusted R-squared</td><td colspan="3">0.809</td></tr><tr><td colspan="2">F-statistic</td><td>250.3</td><td colspan="2">with 1 and 58 df; p-value &lt; 0.0001</td></tr><tr><td colspan="5">&gt; model1=lm(rwalk~time(rwalk))</td></tr><tr><td colspan="5">&gt; summary(model1)</td></tr></table>

According to Exhibit 3.7, about $81 \%$ of the variation in the random walk series is explained by the linear time trend. The adjusted R-squared value is a small adjustment to ${ \bf \dot { \boldsymbol { R } } } ^ { 2 }$ that yields an approximately unbiased estimate based on the number of parameters estimated in the trend. It is useful for comparing models with different numbers of parameters. Various formulas for computing $R ^ { 2 }$ may be found in any book on regression, such as Draper and Smith (1981). The standard deviations of the coefficients labeled Std. Error on the output need to be interpreted carefully. They are appropriate only when the stochastic component is white noise—the usual regression assumption.

For example, in Exhibit 3.7 the value 1.137 is obtained from the square root of the value given by Equation (3.4.8) when ${ \rho } _ { k } = 0$ for $k > 0$ and with $\gamma _ { 0 }$ estimated by $s ^ { 2 }$ , that is, to within rounding,

$$
0. 0 0 8 4 7 5 = \sqrt {\frac {1 2 (1 . 1 3 7) ^ {2}}{6 0 (6 0 ^ {2} - 1)}}
$$

The important point is that these standard deviations assume a white noise stochastic component that will rarely be true for time series.

The $t$ -values or $t$ -ratios shown in Exhibit 3.7 are just the estimated regression coefficients, each divided by their respective standard errors. If the stochastic component is normally distributed white noise, then these ratios provide appropriate test statistics for checking the significance of the regression coefficients. In each case, the null hypothesis is that the corresponding unknown regression coefficient is zero. The significance levels and $p$ -values are determined from the $t$ -distribution with $n - p$ degrees of freedom.

# 3.6 Residual Analysis

As we have already noted, the unobserved stochastic component $\{ X _ { t } \}$ can be estimated, or predicted, by the residual

$$
\hat {X} _ {t} = Y _ {t} - \hat {\mu} _ {t} \tag {3.6.1}
$$

Predicted is really a better term. We reserve the term estimate for the guess of an unknown parameter and the term predictor for an estimate of an unobserved random variable. We call $\hat { X } _ { t }$ the residual corresponding to the tth observation. If the trend model is reasonably correct, then the residuals should behave roughly like the true stochastic component, and various assumptions about the stochastic component can be assessed by looking at the residuals. If the stochastic component is white noise, then the residuals should behave roughly like independent (normal) random variables with zero mean and standard deviation s. Since a least squares fit of any trend containing a constant term automatically produces residuals with a zero mean, we might consider standardizing the residuals as $\mathbf { \widehat { \boldsymbol { X } } } _ { t } ^ { \star } / s$ . However, most statistics software will produce standardized residuals using a more complicated standard error in the denominator that takes into account the specific regression model being fit.

With the residuals or standardized residuals in hand, the next step is to examine various residual plots. We first look at the plot of the residuals over time. If the data are possibly seasonal, we should use plotting symbols as we did in Exhibit 1.9 on page 7, so that residuals associated with the same season can be identified easily.

We will use the monthly average temperature series which we fitted with seasonal means as our first example to illustrate some of the ideas of residual analysis. Exhibit 1.7 on page 6 shows the time series plot of that series. Exhibit 3.8 shows a time series plot for the standardized residuals of the monthly temperature data fitted by seasonal means. If the stochastic component is white noise and the trend is adequately modeled, we would expect such a plot to suggest a rectangular scatter with no discernible trends whatsoever. There are no striking departures from randomness apparent in this display.

Exhibit 3.9 repeats the time series plot but now with seasonal plotting symbols. Again there are no apparent patterns relating to different months of the year.

Exhibit 3.8 Residuals versus Time for Temperature Seasonal Means   
![](images/4a2f5c6e8ed57e4d694d8d15ffb2da1c2a5873a9cb7a250f5f1e1917a9a75327.jpg)  
> plot(y=rstudent(model3),x=as.vector(time(tempdub)), xlab='Time',ylab $) =$ 'Standardized Residuals',type='o')

Exhibit 3.9 Residuals versus Time with Seasonal Plotting Symbols   
![](images/b35265367f7bbb1b4b955501fdb53ad4592ffa8ba3db7ae20790ebe9a284f565.jpg)  
> plot $\scriptstyle \cdot y =$ rstudent(model3), $\mathtt { x } = \mathtt { a } \mathtt { s }$ .vector(time(tempdub)),xlab='Time', > ylab $^ { \prime = }$ 'Standardized Residuals',type='l') > points(y=rstudent(model3), $\mathtt { x } = \mathtt { a } \mathtt { s }$ .vector(time(tempdub)), pch $\displaystyle . =$ as.vector(season(tempdub)))

Next we look at the standardized residuals versus the corresponding trend estimate, or fitted value, as in Exhibit 3.10. Once more we are looking for patterns. Are small residuals associated with small fitted trend values and large residuals with large fitted trend values? Is there less variation for residuals associated with certain sized fitted trend values or more variation with other fitted trend values? There is somewhat more variation for the March residuals and less for November, but Exhibit 3.10 certainly does not indicate any dramatic patterns that would cause us to doubt the seasonal means model.

![](images/20346ad0f705511411f72eb94f2b79e401aea3c755e0b23b7585290eb8f594fa.jpg)  
Exhibit 3.10 Standardized Residuals versus Fitted Values for the Temperature Seasonal Means Model

```txt
> plot(y=rstudent(model3), x=as.vector(fitted(model3)), xlab='Fitted Trend Values',
> ylab='Standardized Residuals', type='n')
> points(y=rstudent(model3), x=as.vector(fitted(model3)), pch=as.vector(season(temphub))) 
```

Gross nonnormality can be assessed by plotting a histogram of the residuals or standardized residuals. Exhibit 3.11 displays a frequency histogram of the standardized residuals from the seasonal means model for the temperature series. The plot is somewhat symmetric and tails off at both the high and low ends as a normal distribution does.

Exhibit 3.11 Histogram of Standardized Residuals from Seasonal Means Model   
![](images/e84f7c78216584882eb277714bae4dde01cab00e1cddfcb2a210607f93bb7c51.jpg)  
> hist(rstudent(model3),xlab='Standardized Residuals')

Normality can be checked more carefully by plotting the so-called normal scores or quantile-quantile (QQ) plot. Such a plot displays the quantiles of the data versus the theoretical quantiles of a normal distribution. With normally distributed data, the QQ plot looks approximately like a straight line. Exhibit 3.12 shows the QQ normal scores plot for the standardized residuals from the seasonal means model for the temperature series. The straight-line pattern here supports the assumption of a normally distributed stochastic component in this model.

![](images/346e48400a0567b98e3203e0b573c79447004ec581173f8624594563fa8ea8e5.jpg)  
Exhibit 3.12 Q-Q Plot: Standardized Residuals of Seasonal Means Model

> win.graph(width=2.5,height=2.5,pointsize=8)   
> qqnorm(rstudent(model3))

An excellent test of normality is known as the Shapiro-Wilk test.† It essentially calculates the correlation between the residuals and the corresponding normal quantiles. The lower this correlation, the more evidence we have against normality. Applying that test to these residuals gives a test statistic of $W = 0 . 9 9 2 9$ with a $p$ -value of 0.6954. We cannot reject the null hypothesis that the stochastic component of this model is normally distributed.

Independence in the stochastic component can be tested in several ways. The runs test examines the residuals in sequence to look for patterns—patterns that would give evidence against independence. Runs above or below their median are counted. A small number of runs would indicate that neighboring residuals are positively dependent and tend to “hang together” over time. On the other hand, too many runs would indicate that the residuals oscillate back and forth across their median. Then neighboring residuals are negatively dependent. So either too few or too many runs lead us to reject independence. Performing a runs test‡ on these residuals produces the following values: observed runs $= 6 5$ , expected runs $= 7 2 . 8 7 5$ , which leads to a $p$ -value of 0.216 and we cannot reject independence of the stochastic component in this seasonal means model.

# The Sample Autocorrelation Function

Another very important diagnostic tool for examining dependence is the sample autocorrelation function. Consider any sequence of data $Y _ { 1 }$ , $Y _ { 2 } , \dots$ , $Y _ { n }$ —whether residuals, standardized residuals, original data, or some transformation of data. Tentatively assuming stationarity, we would like to estimate the autocorrelation function $\rho _ { k }$ for a variety of lags $k = 1$ , 2,…. The obvious way to do this is to compute the sample correlation between the pairs $k$ units apart in time. That is, among $( Y _ { 1 } , Y _ { 1 + k } )$ , $( Y _ { 2 } , Y _ { 2 + k } )$ , $( Y _ { 3 } , \Upsilon _ { 3 + k } ) , \ldots$ , and $( Y _ { n - k } , Y _ { n } )$ . However, we modify this slightly, taking into account that we are assuming stationarity, which implies a common mean and variance for the series. With this in mind, we define the sample autocorrelation function, $r _ { k } ,$ at lag $k$ as

$$
r _ {k} = \frac {\sum_ {t = k + 1} ^ {n} \left(Y _ {t} - \bar {Y}\right) \left(Y _ {t - k} - \bar {Y}\right)}{\sum_ {t = 1} ^ {n} \left(Y _ {t} - \bar {Y}\right) ^ {2}} \quad \text {f o r} k = 1, 2, \dots \tag {3.6.2}
$$

Notice that we used the “grand mean,” $\overline { { Y } }$ , in all places and have also divided by the “grand sum of squares” rather than the product of the two separate standard deviations used in the ordinary correlation coefficient. We also note that the denominator is a sum of $n$ squared terms while the numerator contains only $n - k$ cross products. For a variety of reasons, this has become the standard definition for the sample autocorrelation function. A plot of $r _ { k }$ versus lag $k$ is often called a correlogram.

In our present context, we are interested in discovering possible dependence in the stochastic component; therefore the sample autocorrelation function for the standardized residuals is of interest. Exhibit 3.13 displays the sample autocorrelation for the standardized residuals from the seasonal means model of the temperature series. All values are within the horizontal dashed lines, which are placed at zero plus and minus two approximate standard errors of the sample autocorrelations, namely $\pm 2 / \sqrt { n }$ . The values of $r _ { k }$ are, of course, estimates of $\rho _ { k }$ . As such, they have their own sampling distributions, standard errors, and other properties. For now we shall use $r _ { k }$ as a descriptive tool and defer discussion of those topics until Chapters 6 and 8. According to Exhibit 3.13, for $k$ $= 1$ , 2,..., 21, none of the hypotheses $\rho _ { k } = 0$ can be rejected at the usual significance levels, and it is reasonable to infer that the stochastic component of the series is white noise.

Exhibit 3.13 Sample Autocorrelation of Residuals of Seasonal Means Model   
![](images/46b13fc61f7c96e3149e7a4920856cd0f513babf53e55fbdc871b1278f11339a.jpg)  
> win.graph(width=4.875,height=3,pointsize=8) > acf(rstudent(model3))

As a second example consider the standardized residuals from fitting a straight line to the random walk time series. Recall Exhibit 3.2 on page 31, which shows the data and fitted line. A time series plot of the standardized residuals is shown in Exhibit 3.14.

Exhibit 3.14 Residuals from Straight Line Fit of the Random Walk   
![](images/2a673587f653a9246f80945552142cd1602ff588a13a85e06bc54b696c1d56f2.jpg)  
> plot(y=rstudent(model1),x=as.vector(time(rwalk)), ylab='Standardized Residuals',xlab='Time',type='o')

In this plot, the residuals “hang together” too much for white noise—the plot is too smooth. Furthermore, there seems to be more variation in the last third of the series than in the first two-thirds. Exhibit 3.15 shows a similar effect with larger residuals associated with larger fitted values.

Exhibit 3.15 Residuals versus Fitted Values from Straight Line Fit   
![](images/1d4b800b6213aa73445b56108b8c0824f61e4e7a0c7b46638b9c775545c5d50b.jpg)  
> win.graph(width $\mathord { \left. \begin{array} { r l r } \end{array} \right. } = 4$ .875, height $^ { = 3 }$ ,pointsize ${ \mathrel { : = } } 8$ ) > plot $\scriptstyle \cdot y =$ rstudent(model1), $\mathrm { \Delta x = }$ fitted(model1), ylab $^ { \prime = }$ 'Standardized Residuals',xlab $^ { 1 = }$ 'Fitted Trend Line Values', type='p')

The sample autocorrelation function of the standardized residuals, shown in Exhibit 3.16, confirms the smoothness of the time series plot that we observed in Exhibit 3.14. The lag 1 and lag 2 autocorrelations exceed two standard errors above zero and the lag 5 and lag 6 autocorrelations more than two standard errors below zero. This is not what we expect from a white noise process.

Exhibit 3.16 Sample Autocorrelation of Residuals from Straight Line Model   
![](images/59e05b26c26bd0d8e62f398abbdde94d2cc8ea0630ee6809160bd2432f761a01.jpg)  
> acf(rstudent(model1))

Finally, we return to the annual rainfall in Los Angeles shown in Exhibit 1.1 on page 2. We found no evidence of dependence in that series, but we now look for evidence against normality. Exhibit 3.17 displays the normal quantile-quantile plot for that series. We see considerable curvature in the plot. A line passing through the first and third normal quartiles helps point out the departure from a straight line in the plot.

![](images/905d7b28f58c9488f0599d9aa5f474541e9a74cd6263a16bd3f423b89b57592b.jpg)  
Exhibit 3.17 Quantile-Quantile Plot of Los Angeles Annual Rainfall Series

> win.graph(width=2.5,height=2.5,pointsize=8)   
> qqnorm(larain); qqline(larain)

# 3.7 Summary

This chapter is concerned with describing, modeling, and estimating deterministic trends in time series. The simplest deterministic “trend” is a constant-mean function. Methods of estimating a constant mean were given but, more importantly, assessment of the accuracy of the estimates under various conditions was considered. Regression methods were then pursued to estimate trends that are linear or quadratic in time. Methods for modeling cyclical or seasonal trends came next, and the reliability and efficiency of all of these regression methods were investigated. The final section began our study of residual analysis to investigate the quality of the fitted model. This section also introduced the important sample autocorrelation function, which we will revisit throughout the remainder of the book.

# EXERCISES

3.1 Verify Equation (3.3.2) on page 30, for the least squares estimates of $\beta _ { 0 }$ and of $\beta _ { 1 }$ when the model $Y _ { t } = \beta _ { 0 } + \beta _ { 1 } t + X _ { t }$ is considered.   
3.2 Suppose $Y _ { t } = \mu + e _ { t } - e _ { t - 1 }$ . Find $\dot { V } a r ( \overline { { Y } } )$ . Note any unusual results. In particular, compare your answer to what would have been obtained if $Y _ { t } = \mu + e _ { t } .$ (Hint: You may avoid Equation (3.2.3) on page 28 by first doing some algebraic simplification on ${ et { } { ' } } { \sum } _ { t = 1 } ^ { n } ( e _ { t } - e _ { t - 1 } )$ .)

3.3 Suppose $Y _ { t } = \mu + e _ { t } + e _ { t - 1 }$ . Find $V a r ( \overline { { Y } } )$ . Compare your answer to what would have been obtained if_ $Y _ { t } = \mu + e _ { t } ,$ . Describe the effect that the autocorrelation in $\{ Y _ { t } \}$ has on $V a r ( \overline { { Y } } )$ .

3.4 The data file hours contains monthly values of the average hours worked per week in the U.S. manufacturing sector for July 1982 through June 1987.

(a) Display and interpret the time series plot for these data.   
(b) Now construct a time series plot that uses separate plotting symbols for the various months. Does your interpretation change from that in part (a)?

3.5 The data file wages contains monthly values of the average hourly wages (in dollars) for workers in the U.S. apparel and textile products industry for July 1981 through June 1987.

(a) Display and interpret the time series plot for these data.   
(b) Use least squares to fit a linear time trend to this time series. Interpret the regression output. Save the standardized residuals from the fit for further analysis.   
(c) Construct and interpret the time series plot of the standardized residuals from part (b).   
(d) Use least squares to fit a quadratic time trend to the wages time series. Interpret the regression output. Save the standardized residuals from the fit for further analysis.   
(e) Construct and interpret the time series plot of the standardized residuals from part (d).

3.6 The data file beersales contains monthly U.S. beer sales (in millions of barrels) for the period January 1975 through December 1990.

(a) Display and interpret the plot the time series plot for these data.   
(b) Now construct a time series plot that uses separate plotting symbols for the various months. Does your interpretation change from that in part (a)?   
(c) Use least squares to fit a seasonal-means trend to this time series. Interpret the regression output. Save the standardized residuals from the fit for further analysis.   
(d) Construct and interpret the time series plot of the standardized residuals from part (c). Be sure to use proper plotting symbols to check on seasonality in the standardized residuals.   
(e) Use least squares to fit a seasonal-means plus quadratic time trend to the beer sales time series. Interpret the regression output. Save the standardized residuals from the fit for further analysis.   
(f) Construct and interpret the time series plot of the standardized residuals from part (e). Again use proper plotting symbols to check for any remaining seasonality in the residuals.

3.7 The data file winnebago contains monthly unit sales of recreational vehicles from Winnebago, Inc., from November 1966 through February 1972.

(a) Display and interpret the time series plot for these data.   
(b) Use least squares to fit a line to these data. Interpret the regression output. Plot the standardized residuals from the fit as a time series. Interpret the plot.   
(c) Now take natural logarithms of the monthly sales figures and display and

interpret the time series plot of the transformed values.

(d) Use least squares to fit a line to the logged data. Display and interpret the time series plot of the standardized residuals from this fit.   
(e) Now use least squares to fit a seasonal-means plus linear time trend to the logged sales time series and save the standardized residuals for further analysis. Check the statistical significance of each of the regression coefficients in the model.   
(f) Display the time series plot of the standardized residuals obtained in part (e). Interpret the plot.

3.8 The data file retail lists total U.K. (United Kingdom) retail sales (in billions of pounds) from January 1986 through March 2007. The data are not “seasonally adjusted,” and year $2 0 0 0 = 1 0 0$ is the base year.

(a) Display and interpret the time series plot for these data. Be sure to use plotting symbols that permit you to look for seasonality.   
(b) Use least squares to fit a seasonal-means plus linear time trend to this time series. Interpret the regression output and save the standardized residuals from the fit for further analysis.   
(c) Construct and interpret the time series plot of the standardized residuals from part (b). Be sure to use proper plotting symbols to check on seasonality.

3.9 The data file prescrip gives monthly U.S. prescription costs for the months August 1986 to March 1992. These data are from the State of New Jersey’s Prescription Drug Program and are the cost per prescription claim.

(a) Display and interpret the time series plot for these data. Use plotting symbols that permit you to look for seasonality.   
(b) Calculate and plot the sequence of month-to-month percentage changes in the prescription costs. Again, use plotting symbols that permit you to look for seasonality.   
(c) Use least squares to fit a cosine trend with fundamental frequency 1/12 to the percentage change series. Interpret the regression output. Save the standardized residuals.   
(d) Plot the sequence of standardized residuals to investigate the adequacy of the cosine trend model. Interpret the plot.

3.10 (Continuation of Exercise 3.4) Consider the hours time series again.

(a) Use least squares to fit a quadratic trend to these data. Interpret the regression output and save the standardized residuals for further analysis.   
(b) Display a sequence plot of the standardized residuals and interpret. Use monthly plotting symbols so that possible seasonality may be readily identified.   
(c) Perform the Runs test of the standardized residuals and interpret the results.   
(d) Calculate and interpret the sample autocorrelations for the standardized residuals.   
(e) Investigate the normality of the standardized residuals (error terms). Consider histograms and normal probability plots. Interpret the plots.

3.11 (Continuation of Exercise 3.5) Return to the wages series.

(a) Consider the residuals from a least squares fit of a quadratic time trend.   
(b) Perform a runs test on the standardized residuals and interpret the results.   
(c) Calculate and interpret the sample autocorrelations for the standardized residuals.   
(d) Investigate the normality of the standardized residuals (error terms). Consider histograms and normal probability plots. Interpret the plots.

3.12 (Continuation of Exercise 3.6) Consider the time series in the data file beersales.

(a) Obtain the residuals from the least squares fit of the seasonal-means plus quadratic time trend model.   
(b) Perform a runs test on the standardized residuals and interpret the results.   
(c) Calculate and interpret the sample autocorrelations for the standardized residuals.   
(d) Investigate the normality of the standardized residuals (error terms). Consider histograms and normal probability plots. Interpret the plots.

3.13 (Continuation of Exercise 3.7) Return to the winnebago time series.

(a) Calculate the least squares residuals from a seasonal-means plus linear time trend model on the logarithms of the sales time series.   
(b) Perform a runs test on the standardized residuals and interpret the results.   
(c) Calculate and interpret the sample autocorrelations for the standardized residuals.   
(d) Investigate the normality of the standardized residuals (error terms). Consider histograms and normal probability plots. Interpret the plots.

3.14 (Continuation of Exercise 3.8) The data file retail contains U.K. monthly retail sales figures.

(a) Obtain the least squares residuals from a seasonal-means plus linear time trend model.   
(b) Perform a runs test on the standardized residuals and interpret the results.   
(c) Calculate and interpret the sample autocorrelations for the standardized residuals.   
(d) Investigate the normality of the standardized residuals (error terms). Consider histograms and normal probability plots. Interpret the plots.

3.15 (Continuation of Exercise 3.9) Consider again the prescrip time series.

(a) Save the standardized residuals from a least squares fit of a cosine trend with fundamental frequency 1/12 to the percentage change time series.   
(b) Perform a runs test on the standardized residuals and interpret the results.   
(c) Calculate and interpret the sample autocorrelations for the standardized residuals.   
(d) Investigate the normality of the standardized residuals (error terms). Consider histograms and normal probability plots. Interpret the plots.

3.16 Suppose that a stationary time series, $\{ Y _ { t } \}$ , has an autocorrelation function of the form $\rho _ { k } = \boldsymbol \Phi ^ { k }$ for $k > 0$ , where $\boldsymbol { \Phi }$ is a constant in the range $( - 1 , + 1 )$ .

(a) Show that .Var Y_( ) γ0---- 1 + φ------------ 2φ------ 1 φn( ) ––= ------------------- $V a r ( \overline { { { Y } } } ) = \frac { \gamma _ { 0 } } { n } \biggl [ \frac { 1 + \Phi } { 1 - \Phi } - \frac { 2 \Phi } { n } \frac { ( 1 - \Phi ^ { n } ) } { ( 1 - \Phi ) ^ { 2 } } \biggr ] .$

(Hint: Use Equation (3.2.3) on page 28, the finite geometric sum

$$
\sum_ {k = 0} ^ {n} \phi^ {k} = \frac {1 - \phi^ {n + 1}}{1 - \phi}, \text {a n d t h e r e l a t e d s u m} \quad \sum_ {k = 0} ^ {n} k \phi^ {k - 1} = \frac {d}{d \phi} \left[ \sum_ {k = 0} ^ {n} \phi^ {k} \right].)
$$

(b) If n is large, argue that .Var Y_( ) γ0n---- 1 + φ1 – φ≈ ------------ $n$ $V a r ( \overline { { Y } } ) \approx \frac { \check { \eta } _ { 0 } } { n } \biggl [ \frac { 1 + \Phi } { 1 - \Phi } \biggr ]$   
(c) Plot $( 1 + \phi ) / ( 1 - \phi )$ for $\boldsymbol { \Phi }$ over the range $^ { - 1 }$ to $+ 1$ . Interpret the plot in terms of the precision in estimating the process mean.

3.17 Verify Equation (3.2.6) on page 29. (Hint: You will need the fact that

$$
\sum_ {k = 0} ^ {\infty} \phi^ {k} = \frac {1}{1 - \phi} \text {f o r} - 1 <   \phi <   + 1.)
$$

3.18 Verify Equation (3.2.7) on page 30. (Hint: You will need the two sums

$$
\sum_ {t = 1} ^ {n} t = \frac {n (n + 1)}{2} \text {a n d} \sum_ {t = 1} ^ {n} t ^ {2} = \frac {n (n + 1) (2 n + 1)}{6}.)
$$

# CHAPTER 4

# MODELS FOR STATIONARY TIME SERIES

This chapter discusses the basic concepts of a broad class of parametric time series models— the autoregressive moving average (ARMA) models. These models have assumed great importance in modeling real-world processes.

# 4.1 General Linear Processes

We will always let $\{ Y _ { t } \}$ denote the observed time series. From here on we will also let $\{ \boldsymbol { e } _ { t } \}$ represent an unobserved white noise series, that is, a sequence of identically distributed, zero-mean, independent random variables. For much of our work, the assumption of independence could be replaced by the weaker assumption that the $\{ \boldsymbol { e } _ { t } \}$ are uncorrelated random variables, but we will not pursue that slight generality.

A general linear process, $\{ Y _ { t } \}$ , is one that can be represented as a weighted linear combination of present and past white noise terms as

$$
Y _ {t} = e _ {t} + \psi_ {1} e _ {t - 1} + \psi_ {2} e _ {t - 2} + \dots \tag {4.1.1}
$$

If the right-hand side of this expression is truly an infinite series, then certain conditions must be placed on the $\boldsymbol { \Psi }$ -weights for the right-hand side to be meaningful mathematically. For our purposes, it suffices to assume that

$$
\sum_ {i = 1} ^ {\infty} \psi_ {i} ^ {2} <   \infty \tag {4.1.2}
$$

We should also note that since $\{ \boldsymbol { e } _ { t } \}$ is unobservable, there is no loss in the generality of Equation (4.1.2) if we assume that the coefficient on $e _ { t }$ is 1; effectively, $\psi _ { 0 } = 1$ .

An important nontrivial example to which we will return often is the case where the $\boldsymbol { \Psi }$ ’s form an exponentially decaying sequence

$$
\psi_ {j} = \phi^ {j}
$$

where $\phi$ is a number strictly between $^ { - 1 }$ and $+ 1$ . Then

$$
Y _ {t} = e _ {t} + \phi e _ {t - 1} + \phi^ {2} e _ {t - 2} + \dots
$$

For this example,

$$
E (Y _ {t}) = E \left(e _ {t} + \phi e _ {t - 1} + \phi^ {2} e _ {t - 2} + \dots\right) = 0
$$

so that $\{ Y _ { t } \}$ has a constant mean of zero. Also,

$$
\begin{array}{l} V a r \left(Y _ {t}\right) = V a r \left(e _ {t} + \phi e _ {t - 1} + \phi^ {2} e _ {t - 2} + \dots\right) \\ = V a r \left(e _ {t}\right) + \phi^ {2} V a r \left(e _ {t - 1}\right) + \phi^ {4} V a r \left(e _ {t - 2}\right) + \dots \\ = \sigma_ {e} ^ {2} (1 + \phi^ {2} + \phi^ {4} + \dots) \\ = \frac {\sigma_ {e} ^ {2}}{1 - \phi^ {2}} (\text {b y s u m m i n g a g e o m e t r i c s e r i e s}) \\ \end{array}
$$

Furthermore,

$$
\begin{array}{l} C o v \left(Y _ {t}, Y _ {t - 1}\right) = C o v \left(e _ {t} + \phi e _ {t - 1} + \phi^ {2} e _ {t - 2} + \dots , e _ {t - 1} + \phi e _ {t - 2} + \phi^ {2} e _ {t - 3} + \dots\right) \\ = C o v \left(\phi e _ {t - 1}, e _ {t - 1}\right) + C o v \left(\phi^ {2} e _ {t - 2}, \phi e _ {t - 2}\right) + \dots \\ = \phi \sigma_ {e} ^ {2} + \phi^ {3} \sigma_ {e} ^ {2} + \phi^ {5} \sigma_ {e} ^ {2} + \dots \\ = \phi \sigma_ {e} ^ {2} (1 + \phi^ {2} + \phi^ {4} + \dots) \\ = \frac {\phi \sigma_ {e} ^ {2}}{1 - \phi^ {2}} (\text {a g a i n s u m m i n g a g e o m e t r i c s e r i e s}) \\ \end{array}
$$

Thus

$$
C o r r (Y _ {t}, Y _ {t - 1}) = \left[ \frac {\phi \sigma_ {e} ^ {2}}{1 - \phi^ {2}} \right] / \left[ \frac {\sigma_ {e} ^ {2}}{1 - \phi^ {2}} \right] = \phi
$$

In a similar manner, we can find $C o \nu ( Y _ { t } , Y _ { t - k } ) = \frac { \Phi ^ { k } \sigma _ { e } ^ { 2 } } { 1 - \phi ^ { 2 } }$

and thus

$$
\operatorname {C o r r} \left(Y _ {t}, Y _ {t - k}\right) = \phi^ {k} \tag {4.1.3}
$$

It is important to note that the process defined in this way is stationary—the autocovariance structure depends only on time lag and not on absolute time. For a general linear process, $Y _ { t } = e _ { t } + \Psi _ { 1 } e _ { t - 1 } + \Psi _ { 2 } e _ { t - 2 } + \cdots$ , calculations similar to those done above yield the following results:

$$
E \left(Y _ {t}\right) = 0 \quad \gamma_ {k} = C o v \left(Y _ {t}, Y _ {t - k}\right) = \sigma_ {e} ^ {2} \sum_ {i = 0} ^ {\infty} \psi_ {i} \psi_ {i + k} \quad k \geq 0 \tag {4.1.4}
$$

with $\Psi _ { 0 } = 1$ . A process with a nonzero mean $\mu$ may be obtained by adding $\mu$ to the right-hand side of Equation (4.1.1). Since the mean does not affect the covariance properties of a process, we assume a zero mean until we begin fitting models to data.

# 4.2 Moving Average Processes

In the case where only a finite number of the $\boldsymbol { \Psi }$ -weights are nonzero, we have what is called a moving average process. In this case, we change notation† somewhat and write

$$
Y _ {t} = e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} - \dots - \theta_ {q} e _ {t - q} \tag {4.2.1}
$$

We call such a series a moving average of order $\pmb q$ and abbreviate the name to $\mathrm { M A } ( q )$ . The terminology moving average arises from the fact that $Y _ { t }$ is obtained by applying the weights $1 , - \Theta _ { 1 } , - \Theta _ { 2 } , . . . , - \Theta _ { q }$ to the variables $e _ { t } , e _ { t - 1 } , e _ { t - 2 } , . . . , e _ { t - q }$ and then moving the weights and applying them to $e _ { t + 1 } , e _ { t } , e _ { t - 1 } , \ldots , e _ { t - q + 1 }$ to obtain $\dot { Y } _ { t + 1 }$ and so on. Moving average models were first considered by Slutsky (1927) and Wold (1938).

# The First-Order Moving Average Process

We consider in detail the simple but nevertheless important moving average process of order 1, that is, the MA(1) series. Rather than specialize the formulas in Equation (4.1.4), it is instructive to rederive the results. The model is $Y _ { t } = e _ { t } - \theta e _ { t - 1 }$ . Since only one θ is involved, we drop the redundant subscript 1. Clearly $E ( Y _ { t } ) = ~ 0$ and $V a r ( Y _ { t } ) = \sigma _ { e } ^ { 2 } ( 1 + \theta ^ { 2 } )$ . Now

$$
\begin{array}{l} C o v (Y _ {t}, Y _ {t - 1}) = C o v (e _ {t} - \theta e _ {t - 1}, e _ {t - 1} - \theta e _ {t - 2}) \\ = C o v (- \theta e _ {t - 1}, e _ {t - 1}) = - \theta \sigma_ {e} ^ {2} \\ \end{array}
$$

and

$$
\begin{array}{l} C o v (Y _ {t}, Y _ {t - 2}) = C o v (e _ {t} - \theta e _ {t - 1}, e _ {t - 2} - \theta e _ {t - 3}) \\ = 0 \\ \end{array}
$$

since there are no e’s with subscripts in common between $Y _ { t }$ and $Y _ { t - 2 }$ . Similarly, $C o \nu ( Y _ { t } , Y _ { t - k } ) = 0$ whenever $k \geq 2$ ; that is, the process has no correlation beyond lag 1. This fact will be important later when we need to choose suitable models for real data.

In summary, for an MA(1) model $Y _ { t } = e _ { t } - \theta e _ { t - 1 }$

$$
\left. \begin{array}{l} E \left(Y _ {t}\right) = 0 \\ \gamma_ {0} = \operatorname {V a r} \left(Y _ {t}\right) = \sigma_ {e} ^ {2} \left(1 + \theta^ {2}\right) \\ \gamma_ {1} = - \theta \sigma_ {e} ^ {2} \\ \rho_ {1} = (- \theta) / \left(1 + \theta^ {2}\right) \\ \gamma_ {k} = \rho_ {k} = 0 \quad \text {f o r} k \geq 2 \end{array} \right\} \tag {4.2.2}
$$

Some numerical values for $\rho _ { 1 }$ versus θ in Equation (4.2.2) help illustrate the possibilities. Note that the $\rho _ { 1 }$ values for negative θ can be obtained by simply negating the value given for the corresponding positive θ-value.

<table><tr><td>θ</td><td>ρ1 = -θ/(1 + θ2)</td><td>θ</td><td>ρ1 = -θ/(1 + θ2)</td></tr><tr><td>0.1</td><td>-0.099</td><td>0.6</td><td>-0.441</td></tr><tr><td>0.2</td><td>-0.192</td><td>0.7</td><td>-0.470</td></tr><tr><td>0.3</td><td>-0.275</td><td>0.8</td><td>-0.488</td></tr><tr><td>0.4</td><td>-0.345</td><td>0.9</td><td>-0.497</td></tr><tr><td>0.5</td><td>-0.400</td><td>1.0</td><td>-0.500</td></tr></table>

A calculus argument shows that the largest value that $\rho _ { 1 }$ can attain is $\rho _ { 1 } = \%$ when $\theta = - 1$ and the smallest value is $\rho _ { 1 } = - \%$ , which occurs when $\theta = + 1$ (see Exercise 4.3). Exhibit 4.1 displays a graph of the lag 1 autocorrelation values for θ ranging from $^ { - 1 }$ to $+ 1$ .

![](images/550be31962a2c8f5e1bd8939fd12a9f41c95f7ded3f6833389b1dabc63e05b9b.jpg)  
Exhibit 4.1 Lag 1 Autocorrelation of an MA(1) Process for Different θ

Exercise 4.4 asks you to show that when any nonzero value of θ is replaced by 1/θ, the same value for $\rho _ { 1 }$ is obtained. For example, $\rho _ { 1 }$ is the same for $\theta = \%$ as for $\boldsymbol { \Theta } = 1 / ( \% )$ $= 2$ . If we knew that an MA(1) process had $\rho _ { 1 } = 0 . 4$ , we still could not tell the precise value of θ. We will return to this troublesome point when we discuss invertibility in Section 4.5 on page 79.

Exhibit 4.2 shows a time plot of a simulated MA(1) series with $\theta = - 0 . 9$ and normally distributed white noise. Recall from Exhibit 4.1 that $\rho _ { 1 } = 0 . 4 9 7 2$ for this model; thus there is moderately strong positive correlation at lag 1. This correlation is evident in the plot of the series since consecutive observations tend to be closely related. If an observation is above the mean level of the series, then the next observation also tends to be above the mean. The plot is relatively smooth over time, with only occasional large fluctuations.

![](images/a778111c02492ca31e5f8a6dc75a554ffab8fd8c39c049f7bde97a91810c2335.jpg)  
Exhibit 4.2 Time Plot of an MA(1) Process with $\mathbf { \boldsymbol { \Theta } } = - \mathbf { 0 . 9 }$

```python
> win.graph(width=4.875,height=3,pointsize=8)  
> data(mal.2.s); plot(mal.2.s,ylab=expression(Y[t]),type='o') 
```

The lag 1 autocorrelation is even more apparent in Exhibit 4.3, which plots $Y _ { t }$ versus $Y _ { t - 1 }$ . Note the moderately strong upward trend in this plot.

![](images/4fb0d56a27100f2623e71cc4063dd02129ca1b80555b715059a56db1a536c742.jpg)  
Exhibit 4.3 Plot of Yt versus $\overline { { \mathsf { Y } _ { \mathsf { t } - 1 } } }$ for MA(1) Series in Exhibit 4.2

```python
> win.graph(width=3,height=3,pointsize=8)  
> plot(y=ma1.2.s,x=zlag(ma1.2.s),ylab=expression(Y[t]), xlab=expression(Y[t-1]),type='p') 
```

The plot of $Y _ { t }$ versus $Y _ { t - 2 }$ in Exhibit 4.4 gives a strong visualization of the zero autocorrelation at lag 2 for this model.

Exhibit 4.4 Plot of $\mathsf { \pmb { Y } } _ { \mathbf { t } }$ versus $\yen 2$ for MA(1) Series in Exhibit 4.2   
![](images/eb292ad5029fd299d52f145eabb2c08461a1148b64c56da14afcf57db6e900ce.jpg)  
> plot(y=ma1.2.s,x=zlag(ma1.2.s,2),ylab=expression(Y[t]), xlab=expression(Y[t-2]),type='p')

A somewhat different series is shown in Exhibit 4.5. This is a simulated MA(1) series with $\theta = + 0 . 9$ . Recall from Exhibit 4.1 that $\rho _ { 1 } = - 0 . 4 9 7$ for this model; thus there is moderately strong negative correlation at lag 1. This correlation can be seen in the plot of the series since consecutive observations tend to be on opposite sides of the zero mean. If an observation is above the mean level of the series, then the next observation tends to be below the mean. The plot is quite jagged over time—especially when compared with the plot in Exhibit 4.2.

![](images/45ccc0812fba246d6de7e1fa35cf58d93e23908e5118c567a675ac3e567007fb.jpg)  
Exhibit 4.5 Time Plot of an MA(1) Process with $\mathbf { \boldsymbol { \theta } } = + \mathbf { \boldsymbol { \mathsf { 0 } } } . \mathbf { \pmb { \ 9 } }$

```txt
> win.graph(width=4.875,height=3,pointsize=8)  
> data(mal.1.s)  
> plot(mal.1.s,ylab=expression(Y[t]),type='o' 
```

The negative lag 1 autocorrelation is even more apparent in the lag plot of Exhibit 4.6.

![](images/f4bc3925a7600522d0e06cbbc863a2f7ec334edaf8c0edc8b44d81845c543f48.jpg)  
Exhibit 4.6 Plot of Yt versus $\yen 1$ for MA(1) Series in Exhibit 4.5

```python
> win.graph(width=3, height=3, pointsize=8)  
> plot(y=ma1.1.s, x=zlag(mal.1.s), ylab=expression(Y[t]), xlab=expression(Y[t-1]), type='p') 
```

The plot of $Y _ { t }$ versus $Y _ { t - 2 }$ in Exhibit 4.7 displays the zero autocorrelation at lag 2 for this model.

![](images/b0729485c5ae9b3e317dc0786089320ee5a50fb97a8683452f7541e376326ea9.jpg)  
Exhibit 4.7 Plot of $\mathsf { \pmb { Y } } _ { \mathbf { t } }$ versus $\Upsilon _ { \pm 2 }$ for MA(1) Series in Exhibit 4.5

```txt
> plot(y=ma1.1.s, x=zlag(ma1.1.s, 2), ylab=expression(Y[t]), xlab=expression(Y[t-2]), type='p') 
```

MA(1) processes have no autocorrelation beyond lag 1, but by increasing the order of the process, we can obtain higher-order correlations.

# The Second-Order Moving Average Process

Consider the moving average process of order 2:

$$
Y _ {t} = e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2}
$$

Here

$$
\gamma_ {0} = V a r (Y _ {t}) = V a r (e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2}) = (1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2}) \sigma_ {e} ^ {2}
$$

$$
\begin{array}{l} \gamma_ {1} = C o v (Y _ {t}, Y _ {t - 1}) = C o v (e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2}, e _ {t - 1} - \theta_ {1} e _ {t - 2} - \theta_ {2} e _ {t - 3}) \\ = C o v \left(- \theta_ {1} e _ {t - 1}, e _ {t - 1}\right) + C o v \left(- \theta_ {1} e _ {t - 2}, - \theta_ {2} e _ {t - 2}\right) \\ = [ - \theta_ {1} + (- \theta_ {1}) (- \theta_ {2}) ] \sigma_ {e} ^ {2} \\ = (- \theta_ {1} + \theta_ {1} \theta_ {2}) \sigma_ {e} ^ {2} \\ \end{array}
$$

and

$$
\begin{array}{l} \gamma_ {2} = C o v \left(Y _ {t}, Y _ {t - 2}\right) = C o v \left(e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2}, e _ {t - 2} - \theta_ {1} e _ {t - 3} - \theta_ {2} e _ {t - 4}\right) \\ = C o v \left(- \theta_ {2} e _ {t - 2}, e _ {t - 2}\right) \\ = - \theta_ {2} \sigma_ {e} ^ {2} \\ \end{array}
$$

Thus, for an MA(2) process,

$$
\rho_ {1} = \frac {- \theta_ {1} + \theta_ {1} \theta_ {2}}{1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2}}
$$

$$
\rho_ {2} = \frac {- \theta_ {2}}{1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2}} \tag {4.2.3}
$$

$$
\rho_ {k} = 0 \mathrm {f o r} k = 3, 4, \dots
$$

For the specific case $Y _ { t } = e _ { t } - e _ { t - 1 } + 0 . 6 e _ { t - 2 }$ , we have

$$
\rho_ {1} = \frac {- 1 + (1) (- 0 . 6)}{1 + (1) ^ {2} + (- 0 . 6) ^ {2}} = \frac {- 1 . 6}{2 . 3 6} = - 0. 6 7 8
$$

and

$$
\rho_ {2} = \frac {0 . 6}{2 . 3 6} = 0. 2 5 4
$$

A time plot of a simulation of this MA(2) process is shown in Exhibit 4.8. The series tends to move back and forth across the mean in one time unit. This reflects the fairly strong negative autocorrelation at lag 1.

![](images/6838559777687d139b230628682e00295116da4b6d5577781c53715239e86b0d.jpg)  
Exhibit 4.8 Time Plot of an MA(2) Process with $\theta _ { 1 } = 1$ and $\overline { { { \Theta _ { 2 } } = - 0 . 6 } }$

```txt
> win.graph(width=4.875, height=3,pointsize=8)  
> data(ma2.s); plot(ma2.s,ylab=expression(Y[t]),type='o') 
```

The plot in Exhibit 4.9 reflects that negative autocorrelation quite dramatically.

![](images/048429716c16fd925cdae9b87c4f5f0b68eb9e16a910f396a145e16347d78e74.jpg)  
Exhibit 4.9 Plot of Yt versus $\overline { { \mathsf { Y } _ { \mathsf { t } - 1 } } }$ for MA(2) Series in Exhibit 4.8

```python
> win.graph(width=3,height=3,pointsize=8)  
> plot(y=ma2.s,x=zlag(ma2.s),ylab=expression(Y[t]), xlab=expression(Y[t-1]),type='p') 
```

The weak positive autocorrelation at lag 2 is displayed in Exhibit 4.10.

![](images/0ab5ba5f09c505739d9d0473b3c579807cc673d69d33c7feab7407e293a60f94.jpg)  
Exhibit 4.10 Plot of Yt versus $\overline { { \boldsymbol { \Upsilon } _ { \mathbf { t } - 2 } } }$ for MA(2) Series in Exhibit 4.8

```txt
> plot(y=ma2.s, x=zlag(ma2.s, 2), ylab=expression(Y[t]), xlab=expression(Y[t-2]), type='p') 
```

Finally, the lack of autocorrelation at lag 3 is apparent from the scatterplot in Exhibit 4.11.

Exhibit 4.11 Plot of Yt versus $\yen 3$ for MA(2) Series in Exhibit 4.8   
![](images/93a421d28ca9d65440dd2c986a43fdce6b141b5a14c516f5cd5028041f0cee6c.jpg)  
> plot(y=ma2.s,x=zlag(ma2.s,3),ylab=expression(Y[t]), xlab=expression(Y[t-3]),type='p')

# The General MA(q) Process

For the general $\mathrm { M A } ( q )$ process $Y _ { t } = e _ { t } { - } \Theta _ { 1 } e _ { t - 1 } { - } \Theta _ { 2 } e _ { t - 2 } { - } \dots { - } \Theta _ { q } e _ { t - q }$ , similar calculations show that

$$
\gamma_ {0} = \left(1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2} + \dots + \theta_ {q} ^ {2}\right) \sigma_ {e} ^ {2} \tag {4.2.4}
$$

and

$$
\rho_ {k} = \left\{ \begin{array}{l l} \frac {- \theta_ {k} + \theta_ {1} \theta_ {k + 1} + \theta_ {2} \theta_ {k + 2} + \cdots + \theta_ {q - k} \theta_ {q}}{1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2} + \cdots + \theta_ {q} ^ {2}} & \text {f o r} k = 1, 2, \dots , q \\ 0 & \text {f o r} k > q \end{array} \right. \tag {4.2.5}
$$

where the numerator of $\rho _ { q }$ is just $- 6 _ { q }$ . The autocorrelation function “cuts off” after lag $q$ ; that is, it is zero. Its shape can be almost anything for the earlier lags. Another type of process, the autoregressive process, provides models for alternative autocorrelation patterns.

# 4.3 Autoregressive Processes

Autoregressive processes are as their name suggests—regressions on themselves. Specifically, a pth-order autoregressive process $\{ Y _ { t } \}$ satisfies the equation

$$
Y _ {t} = \phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + \dots + \phi_ {p} Y _ {t - p} + e _ {t} \tag {4.3.1}
$$

The current value of the series $Y _ { t }$ is a linear combination of the $p$ most recent past values of itself plus an “innovation” term $e _ { t }$ that incorporates everything new in the series at time $t$ that is not explained by the past values. Thus, for every t, we assume that $e _ { t }$ is independent of $Y _ { t - 1 }$ , $Y _ { t - 2 }$ , Yt − 3, ... . Yule (1926) carried out the original work on autoregressive processes.†

# The First-Order Autoregressive Process

Again, it is instructive to consider the first-order model, abbreviated AR(1), in detail. Assume the series is stationary and satisfies

$$
Y _ {t} = \phi Y _ {t - 1} + e _ {t} \tag {4.3.2}
$$

where we have dropped the subscript 1 from the coefficient $\boldsymbol { \Phi }$ for simplicity. As usual, in these initial chapters, we assume that the process mean has been subtracted out so that the series mean is zero. The conditions for stationarity will be considered later.

We first take variances of both sides of Equation (4.3.2) and obtain

$$
\gamma_ {0} = \phi^ {2} \gamma_ {0} + \sigma_ {e} ^ {2}
$$

Solving for $\gamma _ { 0 }$ yields

$$
\gamma_ {0} = \frac {\sigma_ {e} ^ {2}}{1 - \phi^ {2}} \tag {4.3.3}
$$

Notice the immediate implication that $\phi ^ { 2 } < 1$ or that $\left. \Phi \right. < 1$ . Now take Equation (4.3.2), multiply both sides by $Y _ { t - k } \left( k = 1 , 2 , \ldots \right)$ , and take expected values

$$
E (Y _ {t - k} Y _ {t}) = \phi E (Y _ {t - k} Y _ {t - 1}) + E (e _ {t} Y _ {t - k})
$$

or

$$
\gamma_ {k} = \phi \gamma_ {k - 1} + E (e _ {t} Y _ {t - k})
$$

Since the series is assumed to be stationary with zero mean, and since $e _ { t }$ is independent of $Y _ { t - k }$ , we obtain

$$
E (e _ {t} Y _ {t - k}) = E (e _ {t}) E (Y _ {t - k}) = 0
$$

and so

$$
\gamma_ {k} = \phi \gamma_ {k - 1} \quad \text {f o r} k = 1, 2, 3, \dots \tag {4.3.4}
$$

Setting $k = 1$ , w e g e t $\gamma _ { 1 } = \phi \gamma _ { 0 } = \phi \sigma _ { e } ^ { 2 } / ( 1 - \phi ^ { 2 } )$ . Wi t h $k = 2$ , we obtain $\gamma _ { 2 } =$ $\phi ^ { 2 } \sigma _ { e } ^ { 2 } / ( 1 - \phi ^ { 2 } )$ . Now it is easy to see that in general

$$
\gamma_ {k} = \phi^ {k} \frac {\sigma_ {e} ^ {2}}{1 - \phi^ {2}} \tag {4.3.5}
$$

and thus

$$
\rho_ {k} = \frac {\gamma_ {k}}{\gamma_ {0}} = \phi^ {k} \quad \text {f o r} k = 1, 2, 3, \dots \tag {4.3.6}
$$

Since $\left. \phi \right. < 1$ , the magnitude of the autocorrelation function decreases exponentially as the number of lags, $k$ , increases. If $0 < \phi < 1$ , all correlations are positive; if $- 1 < \phi < 0$ , the lag 1 autocorrelation is negative $( \rho _ { 1 } = \phi$ ) and the signs of successive autocorrelations alternate from positive to negative, with their magnitudes decreasing exponentially. Portions of the graphs of several autocorrelation functions are displayed in Exhibit 4.12.

![](images/2403a0ea9a9aa2a7136c3c6983fb1cb2e956b6f90e6691f0e8222319368620b7.jpg)  
Exhibit 4.12 Autocorrelation Functions for Several AR(1) Models

![](images/0500dba7a147a6a636533e9a75399891f06a41446e72073e78e50173ea8d65a9.jpg)

![](images/084b9c8ea6605777134079c0309e6c4300e0f357bfcd4880856de9df9ed85655.jpg)

![](images/940a3ec5d1ddd306be9cf47be18d86e27a9b85edeb873dcc4ea9ade9dbf84447.jpg)

Notice that for $\phi$ near $\pm 1$ , the exponential decay is quite slow (for example, $( 0 . 9 ) ^ { 6 } =$ 0.53), but for smaller $\phi$ , the decay is quite rapid (for example, $( 0 . 4 ) ^ { 6 } = 0 . 0 0 \dot { 4 } 1 0$ ). With $\boldsymbol { \Phi }$ near $\pm 1$ , the strong correlation will extend over many lags and produce a relatively

smooth series if $\phi$ is positive and a very jagged series if $\boldsymbol { \Phi }$ is negative.

Exhibit 4.13 displays the time plot of a simulated AR(1) process with $\phi = 0 . 9$ . Notice how infrequently the series crosses its theoretical mean of zero. There is a lot of inertia in the series—it hangs together, remaining on the same side of the mean for extended periods. An observer might claim that the series has several trends. We know that in fact the theoretical mean is zero for all time points. The illusion of trends is due to the strong autocorrelation of neighboring values of the series.

Exhibit 4.13 Time Plot of an AR(1) Series with $\overline { { \phi = 0 . 9 } }$   
![](images/11f0a1656eb48ea2759402c52d02ee1c7ee6371a67c5b5a73a5e436eb738b63a.jpg)  
> win.graph(width=4.875, height=3,pointsize=8) > data(ar1.s); plot(ar1.s,ylab=expression(Y[t]),type='o')

The smoothness of the series and the strong autocorrelation at lag 1 are depicted in the lag plot shown in Exhibit 4.14.

# Exhibit 4.14 Plot of Yt versus $\overline { { \mathsf { Y } _ { \mathsf { t } - 1 } } }$ for AR(1) Series of Exhibit 4.13

![](images/64b4f67bbfb709513f115a813a4258b5894ab4b43581abb63f738c604d65dc0f.jpg)

```python
> win.graph(width=3, height=3, pointsize=8)  
> plot(y=ar1.s, x=zlag(ar1.s), ylab=expression(Y[t]), xlab=expression(Y[t-1]), type='p') 
```

This AR(1) model also has strong positive autocorrelation at lag 2, namely $\rho _ { 2 } =$ $( 0 . 9 ) ^ { 2 } = 0 . 8 1$ . Exhibit 4.15 shows this quite well.

# Exhibit 4.15 Plot of $\mathsf { \pmb { Y } } _ { \mathbf { t } }$ versus $\yen 2$ for AR(1) Series of Exhibit 4.13

![](images/13be564ee9511c52c7904aa45a3ea7c21cc5a18ab70587aca7f4e7de1497a096.jpg)

```txt
> plot(y=ar1.s, x=zlag(ar1.s, 2), ylab=expression(Y[t]), xlab=expression(Y[t-2]), type='p') 
```

Finally, at lag 3, the autocorrelation is still quite high: $\rho _ { 3 } = ( 0 . 9 ) ^ { 3 } = 0 . 7 2 9$ . Exhibit 4.16 confirms this for this particular series.

Exhibit 4.16 Plot of Yt versus $\yen 3$ for AR(1) Series of Exhibit 4.13   
![](images/36d3c782d7db7ac1fb61a7a7ad8a45bfe2a6168b44ce0cb109cf3b957c3f2146.jpg)  
> plot(y=ar1.s,x=zlag(ar1.s,3),ylab=expression(Y[t]), xlab=expression(Y[t-3]),type='p')

# The General Linear Process Version of the AR(1) Model

The recursive definition of the AR(1) process given in Equation (4.3.2) is extremely useful for interpretating the model. For other purposes, it is convenient to express the AR(1) model as a general linear process as in Equation (4.1.1). The recursive definition is valid for all t. If we use this equation with t replaced by $t - 1$ , we get $Y _ { t - 1 } =$ $\phi Y _ { t - 2 } + e _ { t - 1 }$ . Substituting this into the original expression gives

$$
\begin{array}{l} Y _ {t} = \phi (\phi Y _ {t - 2} + e _ {t - 1}) + e _ {t} \\ = e _ {t} + \phi e _ {t - 1} + \phi^ {2} Y _ {t - 2} \\ \end{array}
$$

If we repeat this substitution into the past, say $k - 1$ times, we get

$$
Y _ {t} = e _ {t} + \phi e _ {t - 1} + \phi^ {2} e _ {t - 2} + \dots + \phi^ {k - 1} e _ {t - k + 1} + \phi^ {k} Y _ {t - k} \tag {4.3.7}
$$

Assuming $\left. \Phi \right. < 1$ and letting $k$ increase without bound, it seems reasonable (this is almost a rigorous proof) that we should obtain the infinite series representation

$$
Y _ {t} = e _ {t} + \phi e _ {t - 1} + \phi^ {2} e _ {t - 2} + \phi^ {3} e _ {t - 3} + \dots \tag {4.3.8}
$$

This is in the form of the general linear process of Equation (4.1.1) with $\psi _ { j } = \phi ^ { j }$ , which we already investigated in Section 4.1 on page 55. Note that this representation reemphasizes the need for the restriction $\left. \Phi \right. < 1$ .

# Stationarity of an AR(1) Process

It can be shown that, subject to the restriction that $e _ { t }$ be independent of $Y _ { t - 1 }$ , $Y _ { t - 2 }$ , $Y _ { t - 3 } , \dots$ and that $\sigma _ { e } ^ { 2 } > 0$ , the solution of the AR(1) defining recursion $Y _ { t } = \Phi Y _ { t - 1 } + e _ { t }$ will be stationary if and only if $\left. \phi \right. < 1$ . The requirement $\left. \Phi \right. < 1$ is usually called the stationarity condition for the AR(1) process (See Box, Jenkins, and Reinsel, 1994, p. 54; Nelson, 1973, p. 39; and Wei, 2005, p. 32) even though more than stationarity is involved. See especially Exercises 4.16, 4.18, and 4.25.

At this point, we should note that the autocorrelation function for the AR(1) process has been derived in two different ways. The first method used the general linear process representation leading up to Equation (4.1.3). The second method used the defining recursion $Y _ { t } = \Phi Y _ { t - 1 } + e _ { t }$ and the development of Equations (4.3.4), (4.3.5), and (4.3.6). A third derivation is obtained by multiplying both sides of Equation (4.3.7) by $Y _ { t - k }$ , taking expected values of both sides, and using the fact that $e _ { t } , e _ { t - 1 } , e _ { t - 2 } , \ldots ,$ $e _ { t - ( k - 1 ) }$ are independent of $Y _ { t - k }$ . The second method should be especially noted since it will generalize nicely to higher-order processes.

# The Second-Order Autoregressive Process

Now consider the series satisfying

$$
Y _ {t} = \phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + e _ {t} \tag {4.3.9}
$$

where, as usual, we assume that $e _ { t }$ is independent of $Y _ { t - 1 }$ , Yt − 2, Yt − 3, ... . To discuss stationarity, we introduce the AR characteristic polynomial

$$
\phi (x) = 1 - \phi_ {1} x - \phi_ {2} x ^ {2}
$$

and the corresponding AR characteristic equation

$$
1 - \phi_ {1} x - \phi_ {2} x ^ {2} = 0
$$

We recall that a quadratic equation always has two roots (possibly complex).

# Stationarity of the AR(2) Process

It may be shown that, subject to the condition that $e _ { t }$ is independent of $Y _ { t - 1 }$ , $Y _ { t - 2 }$ , $Y _ { t - 3 } , \dots$ , a stationary solution to Equation (4.3.9) exists if and only if the roots of the AR characteristic equation exceed 1 in absolute value (modulus). We sometimes say that the roots should lie outside the unit circle in the complex plane. This statement will generalize to the pth-order case without change.†

In the second-order case, the roots of the quadratic characteristic equation are easily found to be

$$
\frac {\phi_ {1} \pm \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{- 2 \phi_ {2}} \tag {4.3.10}
$$

For stationarity, we require that these roots exceed 1 in absolute value. In Appendix B, page 84, we show that this will be true if and only if three conditions are satisfied:

$$
\phi_ {1} + \phi_ {2} <   1, \quad \phi_ {2} - \phi_ {1} <   1, \quad \text {a n d} \quad \left| \phi_ {2} \right| <   1 \tag {4.3.11}
$$

As with the AR(1) model, we call these the stationarity conditions for the AR(2) model. This stationarity region is displayed in Exhibit 4.17.

![](images/7f848ebec3fa0d89209bc76c5b71a2258837784ac2c00b14bdaf8f95997a1174.jpg)  
Exhibit 4.17 Stationarity Parameter Region for AR(2) Process

# The Autocorrelation Function for the AR(2) Process

To derive the autocorrelation function for the AR(2) case, we take the defining recursive relationship of Equation (4.3.9), multiply both sides by $Y _ { t - k }$ , and take expectations. Assuming stationarity, zero means, and that $e _ { t }$ is independent of $Y _ { t - k }$ , we get

$$
\gamma_ {k} = \phi_ {1} \gamma_ {k - 1} + \phi_ {2} \gamma_ {k - 2} \quad \text {f o r} k = 1, 2, 3, \dots \tag {4.3.12}
$$

or, dividing through by $\gamma _ { 0 }$

$$
\rho_ {k} = \phi_ {1} \rho_ {k - 1} + \phi_ {2} \rho_ {k - 2} \quad \text {f o r} k = 1, 2, 3, \dots \tag {4.3.13}
$$

Equations (4.3.12) and/or (4.3.13) are usually called the Yule-Walker equations, especially the set of two equations obtained for $k = 1$ and 2. Setting $k = 1$ and using $\rho _ { 0 } = 1$ and $\mathsf { \rho } _ { - 1 } = \mathsf { \rho } _ { 9 1 }$ , we get $\rho _ { 1 } = \phi _ { 1 } + \phi _ { 2 } \rho _ { 1 }$ and so

$$
\rho_ {1} = \frac {\phi_ {1}}{1 - \phi_ {2}} \tag {4.3.14}
$$

Using the now known values for $\rho _ { 1 }$ (and $\rho _ { 0 }$ ), Equation (4.3.13) can be used with $k = 2$ to obtain

$$
\begin{array}{l} \rho_ {2} = \phi_ {1} \rho_ {1} + \phi_ {2} \rho_ {0} \\ = \frac {\phi_ {2} \left(1 - \phi_ {2}\right) + \phi_ {1} ^ {2}}{1 - \phi_ {2}} \tag {4.3.15} \\ \end{array}
$$

Successive values of $\rho _ { k }$ may be easily calculated numerically from the recursive relationship of Equation (4.3.13).

Although Equation (4.3.13) is very efficient for calculating autocorrelation values numerically from given values of $\Phi _ { 1 }$ and $\Phi _ { 2 }$ , for other purposes it is desirable to have a more explicit formula for $\rho _ { k }$ . The form of the explicit solution depends critically on the roots of the characteristic equation $1 - \Phi _ { 1 } x - \Phi _ { 2 } x ^ { 2 } = 0$ . Denoting the reciprocals of these roots by $G _ { 1 }$ and $G _ { 2 }$ , it is shown in Appendix B, page 84, that

$$
G _ {1} = \frac {\phi_ {1} - \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{2} \quad \text {a n d} \quad G _ {2} = \frac {\phi_ {1} + \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{2}
$$

For the case $G _ { 1 } \neq G _ { 2 }$ , it can be shown that we have

$$
\rho_ {k} = \frac {\left(1 - G _ {2} ^ {2}\right) G _ {1} ^ {k + 1} - \left(1 - G _ {1} ^ {2}\right) G _ {2} ^ {k + 1}}{\left(G _ {1} - G _ {2}\right) \left(1 + G _ {1} G _ {2}\right)} \quad \text {f o r} k \geq 0 \tag {4.3.16}
$$

If the roots are complex (that is, if $\Phi _ { 1 } ^ { 2 } + 4 \Phi _ { 2 } < 0$ ), then $\rho _ { k }$ may be rewritten as

$$
\rho_ {k} = R ^ {k} \frac {\sin (\Theta k + \Phi)}{\sin (\Phi)} \quad \text {f o r} k \geq 0 \tag {4.3.17}
$$

where $R = \sqrt { - \Phi _ { 2 } }$ and $\Theta$ and $\Phi$ are defined by $\cos ( \Theta ) = \Phi _ { 1 } / ( 2 \sqrt { - \Phi _ { 2 } } )$ and  tan $( \Phi ) =$ $[ ( 1 - \Phi _ { 2 } ) / ( 1 + \Phi _ { 2 } ) ]$ .

For completeness, we note that if the roots are equal $( \Phi _ { 1 } ^ { 2 } + 4 \Phi _ { 2 } = 0$ ), then we have

$$
\rho_ {k} = \left(1 + \frac {1 + \phi_ {2}}{1 - \phi_ {2}} k\right) \left(\frac {\phi_ {1}}{2}\right) ^ {k} \quad \text {f o r} k = 0, 1, 2, \dots \tag {4.3.18}
$$

A good discussion of the derivations of these formulas can be found in Fuller (1996, Section 2.5).

The specific details of these formulas are of little importance to us. We need only note that the autocorrelation function can assume a wide variety of shapes. In all cases, the magnitude of $\rho _ { k }$ dies out exponentially fast as the lag $k$ increases. In the case of complex roots, $\rho _ { k }$ displays a damped sine wave behavior with damping factor R, $0 \leq R < 1$ , frequency $\Theta$ , and phase Φ. Illustrations of the possible shapes are given in Exhibit 4.18. (The R function ARMAacf discussed on page 450 is useful for plotting.)

![](images/2231c3a4971b477c5321fc20647f296d3efdf442871f5d96392666f2c29abee2.jpg)  
Exhibit 4.18 Autocorrelation Functions for Several AR(2) Models

![](images/2031670959cdd293337d9bf1df56af809c74ec3dae052a27052424f0caef7e6a.jpg)

![](images/9e53a467c903a2d58c4d769923da21f85ba8068838057ad16dd4d950e98648d6.jpg)

![](images/5ba7687fa38e5943363f186299b8a8ae7b1b93efcf653b152612f4bc99b8a23a.jpg)

Exhibit 4.19 displays the time plot of a simulated AR(2) series with $\phi _ { 1 } = 1 . 5$ and $\Phi _ { 2 } = - 0 . 7 5$ . The periodic behavior of $\rho _ { k }$ shown in Exhibit 4.18 is clearly reflected in the nearly periodic behavior of the series with the same period of $3 6 0 / 3 0 = 1 2$ time units. If $\Theta$ is measured in radians, $2 \pi / \Theta$ is sometimes called the quasi-period of the AR(2) process.

Exhibit 4.19 Time Plot of an AR(2) Series with $\$ 123$ and $\Uparrow _ { 2 } = - 0 . 7 5$   
![](images/b09c03b406fedd0784bab2764fd5a64814a61d7d32c209c688f35fe54f9a2cf6.jpg)  
> win.graph(width=4.875,height $^ { = 3 }$ ,pointsize=8) > data(ar2.s); plot(ar2.s,ylab $^ { 1 = }$ expression(Y[t]),type='o')

# The Variance for the AR(2) Model

The process variance $\gamma _ { 0 }$ can be expressed in terms of the model parameters $\phi _ { 1 } , \phi _ { 2 }$ , and $\sigma _ { e } ^ { 2 }$ as follows: Taking the variance of both sides of Equation (4.3.9) yields

$$
\gamma_ {0} = \left(\phi_ {1} ^ {2} + \phi_ {2} ^ {2}\right) \gamma_ {0} + 2 \phi_ {1} \phi_ {2} \gamma_ {1} + \sigma_ {e} ^ {2} \tag {4.3.19}
$$

Setting $k = 1$ in Equation (4.3.12) gives a second linear equation for $\gamma _ { 0 }$ and $\gamma _ { 1 }$ , $\gamma _ { 1 } = \Phi _ { 1 } \gamma _ { 0 } + \Phi _ { 2 } \gamma _ { 1 }$ , which can be solved simultaneously with Equation (4.3.19) to obtain

$$
\begin{array}{l} \gamma_ {0} = \frac {\left(1 - \phi_ {2}\right) \sigma_ {e} ^ {2}}{\left(1 - \phi_ {2}\right) \left(1 - \phi_ {1} ^ {2} - \phi_ {2} ^ {2}\right) - 2 \phi_ {2} \phi_ {1} ^ {2}} \tag {4.3.20} \\ = \left(\frac {1 - \phi_ {2}}{1 + \phi_ {2}}\right) \frac {\sigma_ {e} ^ {2}}{(1 - \phi_ {2}) ^ {2} - \phi_ {1} ^ {2}} \\ \end{array}
$$

# The $\boldsymbol { \Psi }$ -Coefficients for the AR(2) Model

The $\boldsymbol { \Psi }$ -coefficients in the general linear process representation for an AR(2) series are more complex than for the AR(1) case. However, we can substitute the general linear process representation using Equation (4.1.1) for $Y _ { t } ,$ for $Y _ { t - 1 }$ , and for $Y _ { t - 2 }$ into $Y _ { t } = \phi _ { 1 } Y _ { t - 1 } + \phi _ { 2 } Y _ { t - 2 } + e _ { t }$ . If we then equate coefficients of $e _ { j }$ , we get the recursive relationships

$$
\left. \begin{array}{c} \psi_ {0} = 1 \\ \psi_ {1} - \phi_ {1} \psi_ {0} = 0 \\ \psi_ {j} - \phi_ {1} \psi_ {j - 1} - \phi_ {2} \psi_ {j - 2} = 0 \quad \text {f o r} j = 2, 3, \dots \end{array} \right\} \tag {4.3.21}
$$

These may be solved recursively to obtain $\Psi _ { 0 } = 1$ , $\psi _ { 1 } = \phi _ { 1 }$ , $\psi _ { 2 } = \phi _ { 1 } ^ { 2 } + \phi _ { 2 }$ , and so on. These relationships provide excellent numerical solutions for the $\boldsymbol { \Psi }$ -coefficients for given numerical values of $\Phi _ { 1 }$ and $\Phi _ { 2 }$ .

One can also show that, for $G _ { 1 } \neq G _ { 2 }$ , an explicit solution is

$$
\psi_ {j} = \frac {G _ {1} ^ {j + 1} - G _ {2} ^ {j + 1}}{G _ {1} - G _ {2}} \tag {4.3.22}
$$

where, as before, $G _ { 1 }$ and $G _ { 2 }$ are the reciprocals of the roots of the AR characteristic equation. If the roots are complex, Equation (4.3.22) may be rewritten as

$$
\psi_ {j} = R ^ {j} \left\{\frac {\sin [ (j + 1) \Theta ]}{\sin (\Theta)} \right\} \tag {4.3.23}
$$

a damped sine wave with the same damping factor $R$ and frequency $\Theta$ as in Equation (4.3.17) for the autocorrelation function.

For completeness, we note that if the roots are equal, then

$$
\psi_ {j} = (1 + j) \phi_ {1} ^ {j} \tag {4.3.24}
$$

# The General Autoregressive Process

Consider now the $p$ th-order autoregressive model

$$
Y _ {t} = \phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + \dots + \phi_ {p} Y _ {t - p} + e _ {t} \tag {4.3.25}
$$

with AR characteristic polynomial

$$
\phi (x) = 1 - \phi_ {1} x - \phi_ {2} x ^ {2} - \dots - \phi_ {p} x ^ {p} \tag {4.3.26}
$$

and corresponding AR characteristic equation

$$
1 - \phi_ {1} x - \phi_ {2} x ^ {2} - \dots - \phi_ {p} x ^ {p} = 0 \tag {4.3.27}
$$

As noted earlier, assuming that $e _ { t }$ is independent of $Y _ { t - 1 }$ , $Y _ { t - 2 }$ , $Y _ { t - 3 } , \dots$ a stationary solution to Equation (4.3.27) exists if and only if the $p$ roots of the AR characteristic equation each exceed 1 in absolute value (modulus). Other relationships between polynomial roots and coefficients may be used to show that the following two inequalities are necessary for stationarity. That is, for the roots to be greater than 1 in modulus, it is necessary, but not sufficient, that both

$$
\left. \begin{array}{c} \phi_ {1} + \phi_ {2} + \dots + \phi_ {p} <   1 \\ \text {a n d} \quad | \phi_ {p} | <   1 \end{array} \right\} \tag {4.3.28}
$$

Assuming stationarity and zero means, we may multiply Equation (4.3.25) by $Y _ { t - k }$ take expectations, divide by $\gamma _ { 0 }$ , and obtain the important recursive relationship

$$
\rho_ {k} = \phi_ {1} \rho_ {k - 1} + \phi_ {2} \rho_ {k - 2} + \phi_ {3} \rho_ {k - 3} + \dots + \phi_ {p} \rho_ {k - p} \quad \text {f o r} k \geq 1 \tag {4.3.29}
$$

Putting $k = 1$ , 2,..., and $p$ into Equation (4.3.29) and using $\rho _ { 0 } = 1$ and $\rho _ { - k } = \rho _ { k }$ , we get the general Yule-Walker equations

$$
\left. \begin{array}{c} \rho_ {1} = \phi_ {1} + \phi_ {2} \rho_ {1} + \phi_ {3} \rho_ {2} + \dots + \phi_ {p} \rho_ {p - 1} \\ \rho_ {2} = \phi_ {1} \rho_ {1} + \phi_ {2} + \phi_ {3} \rho_ {1} + \dots + \phi_ {p} \rho_ {p - 2} \\ \vdots \\ \rho_ {p} = \phi_ {1} \rho_ {p - 1} + \phi_ {2} \rho_ {p - 2} + \phi_ {3} \rho_ {p - 3} + \dots + \phi_ {p} \end{array} \right\} \tag {4.3.30}
$$

Given numerical values for $\Phi _ { 1 }$ , φ2, ... , $\phi _ { p }$ , these linear equations can be solved to obtain numerical values for $\rho _ { 1 }$ $, \rho _ { 2 } , \ldots , \rho _ { p }$ $\rho _ { p }$ . Then Equation (4.3.29) can be used to obtain numerical values for $\rho _ { k }$ at any number of higher lags.

Noting that

$$
E (e _ {t} Y _ {t}) = E [ e _ {t} (\phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + \dots + \phi_ {p} Y _ {t - p} + e _ {t}) ] = E (e _ {t} ^ {2}) = \sigma_ {e} ^ {2}
$$

we may multiply Equation (4.3.25) by $Y _ { t } .$ , take expectations, and find

$$
\gamma_ {0} = \phi_ {1} \gamma_ {1} + \phi_ {2} \gamma_ {2} + \dots + \phi_ {p} \gamma_ {p} + \sigma_ {e} ^ {2}
$$

which, using $\rho _ { k } = \gamma _ { k } / \gamma _ { 0 }$ , can be written as

$$
\gamma_ {0} = \frac {\sigma_ {e} ^ {2}}{1 - \phi_ {1} \rho_ {1} - \phi_ {2} \rho_ {2} - \cdots - \phi_ {p} \rho_ {p}} \tag {4.3.31}
$$

and express the process variance $\gamma _ { 0 }$ in terms of the parameters $\sigma _ { e } ^ { 2 } , \phi _ { 1 } , \phi _ { 2 } , \hdots , \phi _ { p }$ , and the now known values of $\rho _ { 1 }$ $\boldsymbol { \rho } _ { 1 } , \boldsymbol { \rho } _ { 2 } , \ldots , \boldsymbol { \rho } _ { p }$ $\rho _ { p }$ . Of course, explicit solutions for $\rho _ { k }$ are essentially impossible in this generality, but we can say that $\rho _ { k }$ will be a linear combination of exponentially decaying terms (corresponding to the real roots of the characteristic equation) and damped sine wave terms (corresponding to the complex roots of the characteristic equation).

Assuming stationarity, the process can also be expressed in the general linear process form of Equation (4.1.1), but the $\boldsymbol { \Psi }$ -coefficients are complicated functions of the parameters $\Phi _ { 1 } , \Phi _ { 2 } , . . . , \Phi _ { p }$ $\Phi _ { 1 }$ . The coefficients can be found numerically; see Appendix C on page 85.

# 4.4 The Mixed Autoregressive Moving Average Model

If we assume that the series is partly autoregressive and partly moving average, we obtain a quite general time series model. In general, if

$$
\begin{array}{l} Y _ {t} = \phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + \dots + \phi_ {p} Y _ {t - p} + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} \\ - \dots - \theta_ {q} e _ {t - q} \tag {4.4.1} \\ \end{array}
$$

we say that $\{ Y _ { t } \}$ is a mixed autoregressive moving average process of orders $p$ and $q$ , respectively; we abbreviate the name to ARMA $( p , q )$ . As usual, we discuss an important special case first.†

# The ARMA(1,1) Model

The defining equation can be written

$$
Y _ {t} = \phi Y _ {t - 1} + e _ {t} - \theta e _ {t - 1} \tag {4.4.2}
$$

To derive Yule-Walker type equations, we first note that

$$
\begin{array}{l} E \left(e _ {t} Y _ {t}\right) = E \left[ e _ {t} \left(\phi Y _ {t - 1} + e _ {t} - \theta e _ {t - 1}\right) \right] \\ \mathbf {\sigma} = \sigma_ {e} ^ {2} \\ \end{array}
$$

and

$$
\begin{array}{l} E (e _ {t - 1} Y _ {t}) = E [ e _ {t - 1} (\phi Y _ {t - 1} + e _ {t} - \theta e _ {t - 1}) ] \\ = \phi \sigma_ {e} ^ {2} - \theta \sigma_ {e} ^ {2} \\ = (\phi - \theta) \sigma_ {e} ^ {2} \\ \end{array}
$$

If we multiply Equation (4.4.2) by $Y _ { t - k }$ and take expectations, we have

$$
\left. \begin{array}{l} \gamma_ {0} = \phi \gamma_ {1} + [ 1 - \theta (\phi - \theta) ] \sigma_ {e} ^ {2} \\ \gamma_ {1} = \phi \gamma_ {0} - \theta \sigma_ {e} ^ {2} \\ \gamma_ {k} = \phi \gamma_ {k - 1} \quad \text {f o r} k \geq 2 \end{array} \right\} \tag {4.4.3}
$$

Solving the first two equations yields

$$
\gamma_ {0} = \frac {\left(1 - 2 \phi \theta + \theta^ {2}\right)}{1 - \phi^ {2}} \sigma_ {e} ^ {2} \tag {4.4.4}
$$

and solving the simple recursion gives

$$
\rho_ {k} = \frac {(1 - \theta \phi) (\phi - \theta)}{1 - 2 \theta \phi + \theta^ {2}} \phi^ {k - 1} \quad \text {f o r} k \geq 1 \tag {4.4.5}
$$

Note that this autocorrelation function decays exponentially as the lag $k$ increases. The damping factor is $\phi$ , but the decay starts from initial value $\rho _ { 1 }$ , which also depends on θ. This is in contrast to the AR(1) autocorrelation, which also decays with damping factor $\boldsymbol { \Phi }$ but always from initial value $\rho _ { 0 } = 1$ . For example, if $\phi = 0 . 8$ and $\theta = 0 . 4$ , then ${ \rho } _ { 1 } = 0 . 5 2 3$ , $\rho _ { 2 } = 0 . 4 1 8$ , ${ \rho } _ { 3 } = 0 . 3 3 5$ , and so on. Several shapes for $\rho _ { k }$ are possible, depending on the sign of $\rho _ { 1 }$ and the sign of $\phi$ .

The general linear process form of the model can be obtained in the same manner that led to Equation (4.3.8). We find

$$
Y _ {t} = e _ {t} + (\phi - \theta) \sum_ {j = 1} ^ {\infty} \phi^ {j - 1} e _ {t - j}, \tag {4.4.6}
$$

that is,

$$
\psi_ {j} = (\phi - \theta) \phi^ {j - 1} \qquad \text {f o r} j \geq 1
$$

We should now mention the obvious stationarity condition $\left. \Phi \right. < 1$ , or equivalently the root of the AR characteristic equation $1 - \phi x = 0$ must exceed unity in absolute value.

For the general $\mathbf { A R M A } ( p , q )$ model, we state the following facts without proof: Subject to the condition that $e _ { t }$ is independent of $Y _ { t - 1 } , Y _ { t - 2 } , Y _ { t - 3 } , . . . ,$ a stationary solution to Equation (4.4.1) exists if and only if all the roots of the AR characteristic equation $\phi ( x ) = 0$ exceed unity in modulus.

If the stationarity conditions are satisfied, then the model can also be written as a general linear process with $\boldsymbol { \Psi }$ -coefficients determined from

$$
\left. \begin{array}{c} \psi_ {0} = 1 \\ \psi_ {1} = - \theta_ {1} + \phi_ {1} \\ \psi_ {2} = - \theta_ {2} + \phi_ {2} + \phi_ {1} \psi_ {1} \\ \vdots \\ \psi_ {j} = - \theta_ {j} + \phi_ {p} \psi_ {j - p} + \phi_ {p - 1} \psi_ {j - p + 1} + \dots + \phi_ {1} \psi_ {j - 1} \end{array} \right\} \tag {4.4.7}
$$

where we take $\psi _ { j } = 0$ for $j < 0$ and $\theta _ { j } = 0$ for $j > q$

Again assuming stationarity, the autocorrelation function can easily be shown to satisfy

$$
\rho_ {k} = \phi_ {1} \rho_ {k - 1} + \phi_ {2} \rho_ {k - 2} + \dots + \phi_ {p} \rho_ {k - p} \quad \text {f o r} k > q \tag {4.4.8}
$$

Similar equations can be developed for $k = 1$ , 2, 3, ... , $q$ that involve $\theta _ { 1 }$ $, \theta _ { 2 } , \ldots , \theta _ { q }$ . An algorithm suitable for numerical computation of the complete autocorrelation function is given in Appendix C on page 85. (This algorithm is implemented in the R function named ARMAacf.)

# 4.5 Invertibility

We have seen that for the MA(1) process we get exactly the same autocorrelation function if θ is replaced by 1/θ. In the exercises, we find a similar problem with nonuniqueness for the MA(2) model. This lack of uniqueness of MA models, given their autocorrelation functions, must be addressed before we try to infer the values of parameters from observed time series. It turns out that this nonuniqueness is related to the seemingly unrelated question stated next.

An autoregressive process can always be reexpressed as a general linear process through the $\boldsymbol { \Psi }$ -coefficients so that an AR process may also be thought of as an infinite-order moving average process. However, for some purposes, the autoregressive representations are also convenient. Can a moving average model be reexpressed as an autoregression?

To fix ideas, consider an MA(1) model:

$$
Y _ {t} = e _ {t} - \theta e _ {t - 1} \tag {4.5.1}
$$

First rewriting this as $e _ { t } = Y _ { t } + \theta e _ { t - 1 }$ and then replacing $t$ by $t - 1$ and substituting for $e _ { t - 1 }$ above, we get

$$
\begin{array}{l} e _ {t} = Y _ {t} + \theta \left(Y _ {t - 1} + \theta e _ {t - 2}\right) \\ = Y _ {t} + \theta Y _ {t - 1} + \theta^ {2} e _ {t - 2} \\ \end{array}
$$

If $\left| \boldsymbol { \Theta } \right| < 1$ , we may continue this substitution “infinitely” into the past and obtain the expression [compare with Equations (4.3.7) and (4.3.8)]

$$
e _ {t} = Y _ {t} + \theta Y _ {t - 1} + \theta^ {2} Y _ {t - 2} + \dots
$$

or

$$
Y _ {t} = \left(- \theta Y _ {t - 1} - \theta^ {2} Y _ {t - 2} - \theta^ {3} Y _ {t - 3} - \dots\right) + e _ {t} \tag {4.5.2}
$$

If $\left| \Theta \right| < 1$ , we see that the MA(1) model can be inverted into an infinite-order autoregressive model. We say that the MA(1) model is invertible if and only if $\left| \boldsymbol { \Theta } \right| < 1$ .

For a general $\mathrm { M A } ( q )$ or ARMA $( p , q )$ model, we define the MA characteristic polynomial as

$$
\theta (x) = 1 - \theta_ {1} x - \theta_ {2} x ^ {2} - \theta_ {3} x ^ {3} - \dots - \theta_ {q} x ^ {q} \tag {4.5.3}
$$

and the corresponding MA characteristic equation

$$
1 - \theta_ {1} x - \theta_ {2} x ^ {2} - \theta_ {3} x ^ {3} - \dots - \theta_ {q} x ^ {q} = 0 \tag {4.5.4}
$$

It can be shown that the MA(q) model is invertible; that is, there are coefficients $\pi _ { j }$ such that

$$
Y _ {t} = \pi_ {1} Y _ {t - 1} + \pi_ {2} Y _ {t - 2} + \pi_ {3} Y _ {t - 3} + \dots + e _ {t} \tag {4.5.5}
$$

if and only if the roots of the MA characteristic equation exceed 1 in modulus. (Compare this with stationarity of an AR model.)

It may also be shown that there is only one set of parameter values that yield an invertible MA process with a given autocorrelation function. For example, $Y _ { t } =$ $e _ { t } + 2 e _ { t - 1 }$ and $Y _ { t } = e _ { t } + { ^ { 1 } } / { _ { 2 } e _ { t - 1 } }$ both have the same autocorrelation function, but only the second one with root $^ { - 2 }$ is invertible. From here on, we will restrict our attention to the physically sensible class of invertible models.

For a general ARMA $( p , q )$ model, we require both stationarity and invertibility.

# 4.6 Summary

This chapter introduces the simple but very useful autoregressive, moving average (ARMA) time series models. The basic statistical properties of these models were derived in particular for the important special cases of moving averages of orders 1 and 2 and autoregressive processes of orders 1 and 2. Stationarity and invertibility issues have been pursued for these cases. Properties of mixed ARMA models have also been investigated. You should be well-versed in the autocorrelation properties of these models and the various representations of the models.

# EXERCISES

4.1 Use first principles to find the autocorrelation function for the stationary process defined by

$$
Y _ {t} = 5 + e _ {t} - \frac {1}{2} e _ {t - 1} + \frac {1}{4} e _ {t - 2}
$$

4.2 Sketch the autocorrelation functions for the following MA(2) models with parameters as specified:

(a) $\theta _ { 1 } = 0 . 5$ and $\theta _ { 2 } = 0 . 4$ .   
(b) $\theta _ { 1 } = 1 . 2$ and $\theta _ { 2 } = - 0 . 7$   
(c) $\theta _ { 1 } = - 1$ and $\theta _ { 2 } = - 0 . 6$ .

4.3 Verify that for an MA(1) process

$$
\max  _ {- \infty <   \theta <   \infty} \rho_ {1} = 0. 5 \qquad \text {a n d} \quad \min  _ {- \infty <   \theta <   \infty} \rho_ {1} = - 0. 5
$$

4.4 Show that when θ is replaced by 1/θ, the autocorrelation function for an MA(1) process does not change.   
4.5 Calculate and sketch the autocorrelation functions for each of the following AR(1) models. Plot for sufficient lags that the autocorrelation function has nearly died out.

(a) $\phi _ { 1 } = 0 . 6$ .   
$( { \bf b } ) \ : \phi _ { 1 } = - 0 . 6$   
(c) $\phi _ { 1 } = 0 . 9 5$ . (Do out to 20 lags.)   
(d) $\phi _ { 1 } = 0 . 3$ .

4.6 Suppose that $\{ Y _ { t } \}$ is an AR(1) process with $- 1 < \phi < + 1$

(a) Find the autocovariance function for $W _ { t } = \nabla Y _ { t } = Y _ { t } - Y _ { t - 1 }$ in terms of $\boldsymbol { \Phi }$ and $\sigma _ { e } ^ { 2 }$ .   
(b) In particular, show that $V a r ( W _ { t } ) = 2 \sigma _ { e } ^ { 2 } / ( 1 { + } \phi )$ .

4.7 Describe the important characteristics of the autocorrelation function for the following models: (a) MA(1), (b) MA(2), (c) AR(1), (d) AR(2), and (e) ARMA(1,1).   
4.8 Let $\{ Y _ { t } \}$ be an AR(2) process of the special form $Y _ { t } = \Phi _ { 2 } Y _ { t - 2 } + e _ { t }$ . Use first principles to find the range of values of $\Phi _ { 2 }$ for which the process is stationary.   
4.9 Use the recursive formula of Equation (4.3.13) to calculate and then sketch the autocorrelation functions for the following AR(2) models with parameters as specified. In each case, specify whether the roots of the characteristic equation are real or complex. If the roots are complex, find the damping factor, $R$ , and frequency, $\Theta$ , for the corresponding autocorrelation function when expressed as in Equation (4.3.17), on page 73.

(a) $\phi _ { 1 } = 0 . 6$ and $\phi _ { 2 } = 0 . 3$ .   
(b) $\phi _ { 1 } = - 0 . 4$ and $\phi _ { 2 } = 0 . 5$   
(c) $\phi _ { 1 } = 1 . 2$ and $\Phi _ { 2 } = - 0 . 7$   
(d) $\phi _ { 1 } = - 1$ and $\phi _ { 2 } = - 0 . 6$ .   
(e) $\phi _ { 1 } = 0 . 5$ and $\phi _ { 2 } = - 0 . 9$   
(f) $\phi _ { 1 } = - 0 . 5$ and $\phi _ { 2 } = - 0 . 6$ .

4.10 Sketch the autocorrelation functions for each of the following ARMA models:

(a) ARMA(1,1) with $\phi = 0 . 7$ and $\theta = 0 . 4$ .   
(b) ARMA(1,1) with $\phi = 0 . 7$ and $\theta = - 0 . 4$ .

4.11 For the ARMA(1,2) model $Y _ { t } = 0 . 8 Y _ { t - 1 } + e _ { t } + 0 . 7 e _ { t - 1 } + 0 . 6 e _ { t - 2 }$ , show that

(a) $\rho _ { k } = 0 . 8 \rho _ { k - 1 }$ for $k > 2$   
(b) $\rho _ { 2 } = 0 . 8 \rho _ { 1 } + 0 . 6 \sigma _ { e } ^ { 2 } / \gamma _ { 0 }$

4.12 Consider two MA(2) processes, one with $\theta _ { 1 } = \theta _ { 2 } = 1 / 6$ and another with $\theta _ { 1 } = - 1$ and $\theta _ { 2 } = 6$ .

(a) Show that these processes have the same autocorrelation function.   
(b) How do the roots of the corresponding characteristic polynomials compare?

4.13 Let $\{ Y _ { t } \}$ be a stationary process with $\rho _ { k } = 0$ for $k > 1$ . Show that we must have $| \rho _ { 1 } | \leq \%$ . (Hint: Consider $V a r ( Y _ { n + 1 } + Y _ { n } + \cdots + Y _ { 1 } )$ and then $V a r ( Y _ { n + 1 } - Y _ { n } +$ $Y _ { n - 1 } - \dots \pm Y _ { 1 } )$ . Use the fact that both of these must be nonnegative for all $n$ .)

4.14 Suppose that $\{ Y _ { t } \}$ is a zero mean, stationary process with $\vert \rho _ { 1 } \vert < 0 . 5$ and ${ \rho } _ { k } = 0$ for $k > 1$ . Show that $\{ Y _ { t } \}$ must be representable as an MA(1) process. That is, show that there is a white noise sequence $\{ \boldsymbol { e } _ { t } \}$ such that $Y _ { t } = e _ { t } - \theta e _ { t - 1 }$ , where $\rho _ { 1 }$ is correct and $e _ { t }$ is uncorrelated with $Y _ { t - k }$ for $k > 0$ . (Hint: Choose θ such that $| \boldsymbol { \theta } | < 1$ and ρ1 = −θ/(1 + θ2); then let et θj 0=∞= ∑ $\rho _ { 1 } = - \mathsf { { \theta } } \mathsf { { / } { ( 1 + \theta ^ { 2 } ) } }$ $\begin{array} { r } { e _ { t } = \sum _ { j = 0 } ^ { \infty } \Theta ^ { j } Y _ { t - j } . } \end{array}$ If we assume that $\{ Y _ { t } \}$ is a normal process, $e _ { t }$ will also be normal, and zero correlation is equivalent to independence.)   
4.15 Consider the AR(1) model $Y _ { t } = \Phi Y _ { t - 1 } + e _ { t }$ . Show that if $\left. \phi \right. = 1$ the process cannot be stationary. (Hint: Take variances of both sides.)   
4.16 Consider the “nonstationary” AR(1) model $Y _ { t } = 3 Y _ { t - 1 } + e _ { t } .$

(a) Show that $\begin{array} { r } { Y _ { t } = - { \sum _ { j = 1 } ^ { \infty } } ( \frac { 1 } { 3 } ) ^ { j } e _ { t + j } } \end{array}$ satisfies the AR(1) equation.   
(b) Show that the process defined in part (a) is stationary.   
(c) In what way is this solution unsatisfactory?

4.17 Consider a process that satisfies the AR(1) equation $Y _ { t } = { ^ { 1 } / { 2 } } Y _ { t - 1 } + e _ { t }$

(a) Show that $Y _ { t } = 1 0 ( ^ { 1 } / 2 ) ^ { t } + e _ { t } + ^ { 1 } / 2 e _ { t - 1 } + ( ^ { 1 / 2 } ) ^ { 2 } e _ { t - 2 } + \cdots$ is a solution of the AR(1) equation.   
(b) Is the solution given in part (a) stationary?

4.18 Consider a process that satisfies the zero-mean, “stationary” AR(1) equation $Y _ { t } =$ $\Phi Y _ { t - 1 } + e _ { t }$ with $- 1 < \Phi < + 1$ . Let $c$ be any nonzero constant, and define $W _ { t } = Y _ { t } +$ $c \phi ^ { t }$ .

(a) Show that $E ( W _ { t } ) = c \phi ^ { t }$   
(b) Show that $\{ W _ { t } \}$ satisfies the “stationary” AR(1) equation $W _ { t } = \phi W _ { t - 1 } + e _ { t } .$   
(c) Is $\{ W _ { t } \}$ stationary?

4.19 Consider an MA(6) model with $\theta _ { 1 } = 0 . 5$ , $\theta _ { 2 } = - 0 . 2 5$ , $\theta _ { 3 } = 0 . 1 2 5$ , $\theta _ { 4 } = - 0 . 0 6 2 5$ , $\theta _ { 5 } = 0 . 0 3 1 2 5$ , and $\theta _ { 6 } = - 0 . 0 1 5 6 2 5$ . Find a much simpler model that has nearly the same $\boldsymbol { \Psi }$ -weights.

4.20 Consider an MA(7) model with $\theta _ { 1 } = 1$ , $\theta _ { 2 } = - 0 . 5$ , $\theta _ { 3 } = 0 . 2 5$ , $\theta _ { 4 } = - 0 . 1 2 5$ $\theta _ { 5 } = 0 . 0 6 2 5$ , $\theta _ { 6 } = - 0 . 0 3 1 2 5$ , and $\theta _ { 7 } = 0 . 0 1 5 6 2 5$ . Find a much simpler model that has nearly the same $\boldsymbol { \Psi }$ -weights.

4.21 Consider the model $Y _ { t } = e _ { t - 1 } - e _ { t - 2 } + 0 . 5 e _ { t - 3 }$ .

(a) Find the autocovariance function for this process.   
(b) Show that this is a certain $\mathbf { A R M A } ( p , q )$ process in disguise. That is, identify values for $p$ and $q$ and for the θ’s and $\phi$ ’s such that the ARMA $( p , q )$ process has the same statistical properties as $\{ Y _ { t } \}$ .

4.22 Show that the statement “The roots of $1 - \Phi _ { 1 } x - \Phi _ { 2 } x ^ { 2 } - \cdots - \Phi _ { p } x ^ { p } = 0$ are greater than 1 in absolute value” is equivalent to the statement “The roots of ${ \overline { { x ^ { p } - \Phi _ { 1 } x ^ { p - 1 } - \Phi _ { 2 } x ^ { p - 2 } - \cdots - \Phi _ { p } } } } = 0$ are less than 1 in absolute value.” (Hint: If $G$ is a root of one equation, is $1 / G$ a root of the other?)

4.23 Suppose that $\{ Y _ { t } \}$ is an AR(1) process with ${ \rho } _ { 1 } = \Phi$ . Define the sequence $\{ b _ { t } \}$ as $b _ { t } = Y _ { t } - \Phi Y _ { t + 1 }$ .

(a) Show that $C o \nu ( b _ { t } , b _ { t - k } ) = 0$ for all $t$ and $k$ .   
(b) Show that $C o \nu ( b _ { t } , Y _ { t + k } ) = 0$ for all $t$ and $k > 0$ .

4.24 Let $\{ \boldsymbol { e } _ { t } \}$ be a zero-mean, unit-variance white noise process. Consider a process that begins at time $t = 0$ and is defined recursively as follows. Let $Y _ { 0 } = c _ { 1 } e _ { 0 }$ and $Y _ { 1 } = c _ { 2 } Y _ { 0 } + e _ { 1 }$ . Then let $Y _ { t } = \Phi _ { 1 } Y _ { t - 1 } + \Phi _ { 2 } Y _ { t - 2 } + e _ { t }$ for $t > 1$ as in an AR(2) process.

(a) Show that the process mean is zero.   
(b) For particular values of $\Phi _ { 1 }$ and $\Phi _ { 2 }$ within the stationarity region for an AR(2) model, show how to choose $c _ { 1 }$ and $c _ { 2 }$ so that both $\begin{array} { r } { V a r ( Y _ { 0 } ) = V a r ( Y _ { 1 } ) } \end{array}$ and the lag 1 autocorrelation between $Y _ { 1 }$ and $Y _ { 0 }$ match that of a stationary AR(2) process with parameters $\Phi _ { 1 }$ and $\Phi _ { 2 }$ .   
(c) Once the process $\{ Y _ { t } \}$ is generated, show how to transform it to a new process that has any desired mean and variance. (This exercise suggests a convenient method for simulating stationary AR(2) processes.)

4.25 Consider an “AR(1)” process satisfying $Y _ { t } = \Phi Y _ { t - 1 } + e _ { t } ,$ where $\boldsymbol { \Phi }$ can be any number and $\{ \boldsymbol { e } _ { t } \}$ is a white noise process such that $e _ { t }$ is independent of the past $\{ Y _ { t - 1 }$ , $Y _ { t - 2 } , \dots \}$ . Let $Y _ { 0 }$ be a random variable with mean $\mu _ { 0 }$ and variance $\sigma _ { 0 } ^ { 2 }$ .

(a) Show that for $t > 0$ we can write

$$
Y _ {t} = e _ {t} + \phi e _ {t - 1} + \phi^ {2} e _ {t - 2} + \phi^ {3} e _ {t - 3} + \dots + \phi^ {t - 1} e _ {1} + \phi^ {t} Y _ {0}.
$$

(b) Show that for $t > 0$ we have $E ( Y _ { t } ) = \boldsymbol \Phi ^ { t } \boldsymbol \mu _ { 0 }$   
(c) Show that for $t > 0$

$$
V a r (Y _ {t}) = \left\{ \begin{array}{l l} \frac {1 - \phi^ {2 t}}{1 - \phi^ {2}} \sigma_ {e} ^ {2} + \phi^ {2 t} \sigma_ {0} ^ {2} & \text {f o r} \phi \neq 1 \\ t \sigma_ {e} ^ {2} + \sigma_ {0} ^ {2} & \text {f o r} \phi = 1 \end{array} \right.
$$

(d) Suppose now that $\mu _ { 0 } = 0$ . Argue that, if $\{ Y _ { t } \}$ is stationary, we must have $\boldsymbol \Phi \ne 1$   
(e) Continuing to suppose that $\mu _ { 0 } = 0$ , show that, if $\{ Y _ { t } \}$ is stationary, then $V a r ( Y _ { t } ) = \sigma _ { e } ^ { 2 } / ( 1 - \phi ^ { 2 } )$ and so we must have $| \Phi | < 1$ .

# Appendix B: The Stationarity Region for an AR(2) Process

In the second-order case, the roots of the quadratic characteristic polynomial are easily found to be

$$
\frac {\phi_ {1} \pm \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{- 2 \phi_ {2}} \tag {4.B.1}
$$

For stationarity we require that these roots exceed 1 in absolute value. We now show that this will be true if and only if three conditions are satisfied:

$$
\phi_ {1} + \phi_ {2} <   1, \quad \phi_ {2} - \phi_ {1} <   1, \quad \text {a n d} \quad \left| \phi_ {2} \right| <   1 \tag {4.B.2}
$$

Proof: Let the reciprocals of the roots be denoted $G _ { 1 }$ and $G _ { 2 }$ . Then

$$
\begin{array}{l} G _ {1} = \frac {2 \phi_ {2}}{- \phi_ {1} - \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}} = \frac {2 \phi_ {2}}{- \phi_ {1} - \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}} \left[ \frac {- \phi_ {1} + \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{- \phi_ {1} + \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}} \right] \\ = \frac {2 \phi_ {2} (- \phi_ {1} + \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}})}{\phi_ {1} ^ {2} - (\phi_ {1} ^ {2} + 4 \phi_ {2})} = \frac {\phi_ {1} - \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{2} \\ \end{array}
$$

Similarly,

$$
G _ {2} = \frac {\phi_ {1} + \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{2}
$$

We now divide the proof into two cases corresponding to real and complex roots. The roots will be real if and only if $\phi _ { 1 } ^ { 2 } + 4 \phi _ { 2 } \geq 0$ .

I. Real Roots: $\left| G _ { i } \right| < 1$ for $i = 1$ and 2 if and only if

$$
- 1 <   \frac {\phi_ {1} - \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{2} <   \frac {\phi_ {1} + \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{2} <   1
$$

or

$$
- 2 <   \phi_ {1} - \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}} <   \phi_ {1} + \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}} <   2.
$$

Consider just the first inequality. Now $- 2 < \Phi _ { 1 } - \sqrt { \Phi _ { 1 } ^ { 2 } + 4 \Phi _ { 2 } }$ if and only if $\sqrt { \Phi _ { 1 } ^ { 2 } + 4 \Phi _ { 2 } } < \Phi _ { 1 } ^ { ' } + 2$ if and only if $\Phi _ { 1 } ^ { 2 } + 4 \Phi _ { 2 } < \Phi _ { 1 } ^ { 2 } + 4 \Phi _ { 1 } { \mathrm { \dot { + } } } 4$ if and only if $\Phi _ { 2 } < \phi _ { 1 } + 1$ , or $\phi _ { 2 } - \phi _ { 1 } < 1$ .

The inequality $\Phi _ { 1 } + \sqrt { \Phi _ { 1 } ^ { 2 } + 4 \Phi _ { 2 } } < 2$ is treated similarly and leads to $\Phi _ { 2 } + \Phi _ { 1 } < 1$

These equations together with $\Phi _ { 1 } ^ { 2 } + 4 \Phi _ { 2 } \geq 0$ define the stationarity region for the real root case shown in Exhibit 4.17.

II. Complex Roots: Now $\Phi _ { 1 } ^ { 2 } + 4 \Phi _ { 2 } < 0$ . Here $G _ { 1 }$ and $G _ { 2 }$ will be complex conjugates and $\left| G _ { 1 } \right| = \left| G _ { 2 } \right| < 1$ if and only if $| G _ { 1 } | ^ { 2 } < 1 . \mathrm { B u t } | G _ { 1 } \big | ^ { 2 } \ = \ [ \phi _ { 1 } ^ { 2 } + ( - \phi _ { 1 } ^ { 2 } - 4 \phi _ { 2 } ) ] / 4$ $\left| G _ { 1 } \right| ^ { 2 } < 1$ $\mathbf { \Phi } = \mathbf { \Phi } - \Phi _ { 2 }$ so that $\phi _ { 2 } > - \mathrm { i }$ . This together with the inequality $\dot { \Phi } _ { 1 } ^ { 2 } + 4 \Phi _ { 2 } ^ { \ } < 0$ defines the part of the stationarity region for complex roots shown in Exhibit 4.17 and establishes Equation (4.3.11). This completes the proof.

# Appendix C: The Autocorrelation Function for ARMA(p,q)

Let $\{ Y _ { t } \}$ be a stationary, invertible $\mathbf { A R M A } ( p , q )$ $( p , q )$ process. Recall that we can always write such a process in general linear process form as

$$
Y _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} e _ {t - j} \tag {4.C.1}
$$

where the $\boldsymbol { \Psi }$ -weights can be obtained recursively from Equations (4.4.7), on page 79. We then have

$$
E \left(Y _ {t + k} e _ {t}\right) = E \left(\sum_ {j = 0} ^ {\infty} \psi_ {j} e _ {t + k - j} e _ {t}\right) = \psi_ {k} \sigma_ {e} ^ {2} \text {f o r} k \geq 0 \tag {4.C.2}
$$

Thus the autocovariance must satisfy

$$
\begin{array}{l} \gamma_ {k} = E \left(Y _ {t + k} Y _ {t}\right) = E \left[ \left(\sum_ {j = 1} ^ {p} \phi_ {j} Y _ {t + k - j} - \sum_ {j = 0} ^ {q} \theta_ {j} e _ {t + k - j}\right) Y _ {t} \right] \tag {4.C.3} \\ = \sum_ {j = 1} ^ {p} \phi_ {j} \gamma_ {k - j} - \sigma_ {e} ^ {2} \sum_ {j = k} ^ {q} \theta_ {j} \psi_ {j - k} \\ \end{array}
$$

where $\theta _ { 0 } = - 1$ and the last sum is absent if $k > q$ . Setting $k = 0 , 1 , . . . , p$ and using $\gamma _ { - k } =$ $\gamma _ { k }$ leads to $p + 1$ linear equations in $\gamma _ { 0 } , \gamma _ { 1 } , . . . , \gamma _ { p }$ .

$$
\left. \begin{array}{c} \gamma_ {0} = \phi_ {1} \gamma_ {1} + \phi_ {2} \gamma_ {2} + \dots + \phi_ {p} \gamma_ {p} - \sigma_ {e} ^ {2} \left(\theta_ {0} + \theta_ {1} \psi_ {1} + \dots + \theta_ {q} \psi_ {q}\right) \\ \gamma_ {1} = \phi_ {1} \gamma_ {0} + \phi_ {2} \gamma_ {1} + \dots + \phi_ {p} \gamma_ {p - 1} - \sigma_ {e} ^ {2} \left(\theta_ {1} + \theta_ {2} \psi_ {1} + \dots + \theta_ {q} \psi_ {q - 1}\right) \\ \vdots \\ \gamma_ {p} = \phi_ {1} \gamma_ {p - 1} + \phi_ {2} \gamma_ {p - 2} + \dots + \phi_ {p} \gamma_ {0} - \sigma_ {e} ^ {2} \left(\theta_ {p} + \theta_ {p + 1} \psi_ {1} + \dots + \theta_ {q} \psi_ {q - p}\right) \end{array} \right\} \tag {4.C.4}
$$

where $\theta _ { j } = 0$ if $j > q$

For a given set of parameter values $\sigma _ { e } ^ { 2 } , \phi ^ { , }$ s, and θ’s (and hence $\boldsymbol { \Psi }$ ’s), we can solve the linear equations to obtain $\gamma _ { 0 } , \gamma _ { 1 } , . . . , \gamma _ { p }$ $\gamma _ { p }$ . The values of $\gamma _ { k }$ for $k > p$ can then be evaluated from the recursion in Equations (4.4.8), on page 79. Finally, $\rho _ { k }$ is obtained from $\rho _ { k }$ $= \gamma _ { k } / \gamma _ { 0 }$ .

# CHAPTER 5

# MODELS FOR NONSTATIONARY TIME SERIES

Any time series without a constant mean over time is nonstationary. Models of the form

$$
Y _ {t} = \mu_ {t} + X _ {t}
$$

where $\mu _ { t }$ is a nonconstant mean function and $X _ { t }$ is a zero-mean, stationary series, were considered in Chapter 3. As stated there, such models are reasonable only if there are good reasons for believing that the deterministic trend is appropriate “forever.” That is, just because a segment of the series looks like it is increasing (or decreasing) approximately linearly, do we believe that the linearity is intrinsic to the process and will persist in the future? Frequently in applications, particularly in business and economics, we cannot legitimately assume a deterministic trend. Recall the random walk displayed in Exhibit 2.1, on page 14. The time series appears to have a strong upward trend that might be linear in time. However, also recall that the random walk process has a constant, zero mean and contains no deterministic trend at all.

As an example consider the monthly price of a barrel of crude oil from January 1986 through January 2006. Exhibit 5.1 displays the time series plot. The series displays considerable variation, especially since 2001, and a stationary model does not seem to be reasonable. We will discover in Chapters 6, 7, and 8 that no deterministic trend model works well for this series but one of the nonstationary models that have been described as containing stochastic trends does seem reasonable. This chapter discusses such models. Fortunately, as we shall see, many stochastic trends can be modeled with relatively few parameters.

![](images/9f9b93f985acd7e812c5ec160fcdd2c46d71a74767cbd487c5ed1a86489252af.jpg)  
Exhibit 5.1 Monthly Price of Oil: January 1986–January 2006

> win.graph(width=4.875,height=3,pointsize=8)   
> data(oil.price)   
> plot(oil.price, ylab='Price per Barrel',type='l')

# 5.1 Stationarity Through Differencing

Consider again the AR(1) model

$$
Y _ {t} = \phi Y _ {t - 1} + e _ {t} \tag {5.1.1}
$$

We have seen that assuming $e _ { t }$ is a true “innovation” (that is, $e _ { t }$ is uncorrelated with $Y _ { t - 1 }$ , $Y _ { t - 2 } , \ldots )$ , we must have $| \Phi | < 1$ . What can we say about solutions to Equation (5.1.1) if $| \Phi | \geq 1 2$ Consider in particular the equation

$$
Y _ {t} = 3 Y _ {t - 1} + e _ {t} \tag {5.1.2}
$$

Iterating into the past as we have done before yields

$$
Y _ {t} = e _ {t} + 3 e _ {t - 1} + 3 ^ {2} e _ {t - 2} + \dots + 3 ^ {t - 1} e _ {1} + 3 ^ {t} Y _ {0} \tag {5.1.3}
$$

We see that the influence of distant past values of $Y _ { t }$ and $e _ { t }$ does not die out—indeed, the weights applied to $Y _ { 0 }$ and $e _ { 1 }$ grow exponentially large. In Exhibit 5.2, we show the values for a very short simulation of such a series. Here the white noise sequence was generated as standard normal variables and we used $Y _ { 0 } = 0$ as an initial condition.

<table><tr><td colspan="2">Exhibit 5.2</td><td colspan="7">Simulation of the Explosive “AR(1) Model”</td></tr><tr><td>t</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>et</td><td>0.63</td><td>-1.25</td><td>1.80</td><td>1.51</td><td>1.56</td><td>0.62</td><td>0.64</td><td>-0.98</td></tr><tr><td>Yt</td><td>0.63</td><td>0.64</td><td>3.72</td><td>12.67</td><td>39.57</td><td>119.33</td><td>358.63</td><td>1074.91</td></tr></table>

Exhibit 5.3 shows the time series plot of this explosive AR(1) simulation.

![](images/d7c618a8938885f7a9d46328195a28aaba585857e91891b324868438e0826a40.jpg)  
Exhibit 5.3 An Explosive “AR(1)” Series

```txt
> data(explode.s)  
> plot(explode.s, ylab=expression(Y[t]), type='o') 
```

The explosive behavior of such a model is also reflected in the model’s variance and covariance functions. These are easily found to be

$$
\operatorname {V a r} \left(Y _ {t}\right) = \frac {1}{8} \left(9 ^ {t} - 1\right) \sigma_ {e} ^ {2} \tag {5.1.4}
$$

and

$$
C o v \left(Y _ {t}, Y _ {t - k}\right) = \frac {3 ^ {k}}{8} \left(9 ^ {t - k} - 1\right) \sigma_ {e} ^ {2} \tag {5.1.5}
$$

respectively. Notice that we have

$$
C o r r \left(Y _ {t}, Y _ {t - k}\right) = 3 ^ {k} \sqrt {\frac {9 ^ {t - k} - 1}{9 ^ {t} - 1}} \approx 1 \quad \text {f o r l a r g e t a n d m o d e r a t e k}
$$

The same general exponential growth or explosive behavior will occur for any $\boldsymbol { \Phi }$ such that $| \phi | > 1$ . A more reasonable type of nonstationarity obtains when $\phi = 1$ . If $\phi = 1$ , the AR(1) model equation is

$$
Y _ {t} = Y _ {t} + e _ {t} \tag {5.1.6}
$$

This is the relationship satisfied by the random walk process of Chapter 2 (Equation (2.2.9) on page 12). Alternatively, we can rewrite this as

$$
\nabla Y _ {t} = e _ {t} \tag {5.1.7}
$$

where $\nabla Y _ { t } = Y _ { t } - Y _ { t - 1 }$ is the first difference of $Y _ { t }$ The random walk then is easily extended to a more general model whose first difference is some stationary process—not just white noise.

Several somewhat different sets of assumptions can lead to models whose first difference is a stationary process. Suppose

$$
Y _ {t} = M _ {t} + X _ {t} \tag {5.1.8}
$$

where $M _ { t }$ is a series that is changing only slowly over time. Here $M _ { t }$ could be either deterministic or stochastic. If we assume that $M _ { t }$ is approximately constant over every two consecutive time points, we might estimate (predict) $M _ { t }$ at $t$ by choosing $\beta _ { 0 }$ so that

$$
\sum_ {j = 0} ^ {1} (Y _ {t - j} - \beta_ {0, t}) ^ {2}
$$

is minimized. This clearly leads to

$$
\hat {M} _ {t} = \frac {1}{2} (Y _ {t} + Y _ {t - 1})
$$

and the “detrended” series at time $t$ is then

$$
Y _ {t} - \hat {M} _ {t} = Y _ {t} - \frac {1}{2} (Y _ {t} + Y _ {t - 1}) = \frac {1}{2} (Y _ {t} - Y _ {t - 1}) = \frac {1}{2} \nabla Y _ {t}
$$

This is a constant multiple of the first difference, $\nabla Y _ { t }$ . †

A second set of assumptions might be that $M _ { t }$ in Equation (5.1.8) is stochastic and changes slowly over time governed by a random walk model. Suppose, for example, that

$$
Y _ {t} = M _ {t} + e _ {t} \quad \text {w i t h} \quad M _ {t} = M _ {t - 1} + \varepsilon_ {t} \tag {5.1.9}
$$

where $\{ \boldsymbol { e } _ { t } \}$ and $\{ \varepsilon _ { t } \}$ are independent white noise series. Then

$$
\begin{array}{l} \nabla Y _ {t} = \nabla M _ {t} + \nabla e _ {t} \\ = \varepsilon_ {t} + e _ {t} - e _ {t - 1} \\ \end{array}
$$

which would have the autocorrelation function of an MA(1) series with

$$
\rho_ {1} = - \left\{1 / \left[ 2 + \left(\sigma_ {\varepsilon} ^ {2} / \sigma_ {e} ^ {2}\right) \right] \right\} \tag {5.1.10}
$$

In either of these situations, we are led to the study of $\nabla Y _ { t }$ as a stationary process.

Returning to the oil price time series, Exhibit 5.4 displays the time series plot of the differences of logarithms of that series.‡ The differenced series looks much more stationary when compared with the original time series shown in Exhibit 5.1, on page 88.

(We will also see later that there are outliers in this series that need to be considered to produce an adequate model.)

Exhibit 5.4 The Difference Series of the Logs of the Oil Price Time   
![](images/65a5e4d909ea91febe5d7678fd9521f3a64676ab89bcd0c6f832374128985807.jpg)  
> plot(diff(log(oil.price)),ylab='Change in Log(Price)',type='l')

We can also make assumptions that lead to stationary second-difference models. Again we assume that Equation (5.1.8) on page 90, holds, but now assume that $M _ { t }$ is linear in time over three consecutive time points. We can now estimate (predict) $M _ { t }$ at the middle time point t by choosing ${ \beta } _ { 0 , \hphantom { 0 } }$ andt $\beta _ { 1 ,  }$ to minimizet

$$
\sum_ {j = - 1} ^ {1} (Y _ {t - j} - (\beta_ {0, t} + j \beta_ {1, t})) ^ {2}
$$

The solution yields

$$
\hat {M} _ {t} = \frac {1}{3} (Y _ {t + 1} + Y _ {t} + Y _ {t - 1})
$$

and thus the detrended series is

$$
\begin{array}{l} Y _ {t} - \hat {M} _ {t} = Y _ {t} - \left(\frac {Y _ {t + 1} + Y _ {t} + Y _ {t - 1}}{3}\right) \\ = \left(- \frac {1}{3}\right) \left(Y _ {t + 1} - 2 Y _ {t} + Y _ {t - 1}\right) \\ = \left(- \frac {1}{3}\right) \nabla (\nabla Y _ {t + 1}) \\ = \left(- \frac {1}{3}\right) \nabla^ {2} \left(Y _ {t + 1}\right) \\ \end{array}
$$

a constant multiple of the centered second difference of $Y _ { t } .$ Notice that we have differenced twice, but both differences are at lag 1.

Alternatively, we might assume that

$$
Y _ {t} = M _ {t} + e _ {t}, \quad \text {w h e r e} \quad M _ {t} = M _ {t - 1} + W _ {t} \quad \text {a n d} \quad W _ {t} = W _ {t - 1} + \varepsilon_ {t} \tag {5.1.11}
$$

with $\{ \boldsymbol { e } _ { t } \}$ and $\{ \varepsilon _ { t } \}$ independent white noise time series. Here the stochastic trend $M _ { t }$ is such that its “rate of change,” $\nabla M _ { t } ,$ is changing slowly over time. Then

$$
\nabla Y _ {t} = \nabla M _ {t} + \nabla e _ {t} = W _ {t} + \nabla e _ {t}
$$

and

$$
\begin{array}{l} \nabla^ {2} Y _ {t} = \nabla W _ {t} + \nabla^ {2} e _ {t} \\ = \varepsilon_ {t} + (e _ {t} - e _ {t - 1}) - (e _ {t - 1} - e _ {t - 2}) \\ = \varepsilon_ {t} + e _ {t} - 2 e _ {t - 1} + e _ {t - 2} \\ \end{array}
$$

which has the autocorrelation function of an MA(2) process. The important point is that the second difference of the nonstationary process $\{ Y _ { t } \}$ is stationary. This leads us to the general definition of the important integrated autoregressive moving average time series models.

# 5.2 ARIMA Models

A time series $\{ Y _ { t } \}$ is said to follow an integrated autoregressive moving average model if the dth difference $W _ { t } = \nabla ^ { d } Y _ { t }$ is a stationary ARMA process. If $\{ W _ { t } \}$ follows an $\mathbf { A R M A } ( p , q )$ model, we say that $\{ Y _ { t } \}$ is an $\mathbf { A R I M A } ( p , d , q )$ process. Fortunately, for practical purposes, we can usually take $d = 1$ or at most 2.

Consider then an ARIMA $( p , 1 , q )$ process. With $W _ { t } = Y _ { t } - Y _ { t - 1 }$ , we have

$$
\begin{array}{l} W _ {t} = \phi_ {1} W _ {t - 1} + \phi_ {2} W _ {t - 2} + \dots + \phi_ {p} W _ {t - p} + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} \tag {5.2.1} \\ - \dots - \theta_ {q} e _ {t - q} \\ \end{array}
$$

or, in terms of the observed series,

$$
\begin{array}{l} Y _ {t} - Y _ {t - 1} = \phi_ {1} (Y _ {t - 1} - Y _ {t - 2}) + \phi_ {2} (Y _ {t - 2} - Y _ {t - 3}) + \dots + \phi_ {p} (Y _ {t - p} - Y _ {t - p - 1}) \\ + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} - \dots - \theta_ {q} e _ {t - q} \\ \end{array}
$$

which we may rewrite as

$$
\begin{array}{l} Y _ {t} = (1 + \phi_ {1}) Y _ {t - 1} + (\phi_ {2} - \phi_ {1}) Y _ {t - 2} + (\phi_ {3} - \phi_ {2}) Y _ {t - 3} + \dots \\ + \left(\phi_ {p} - \phi_ {p - 1}\right) Y _ {t - p} - \phi_ {p} Y _ {t - p - 1} + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} - \dots - \theta_ {q} e _ {t - q} \tag {5.2.2} \\ \end{array}
$$

We call this the difference equation form of the model. Notice that it appears to be an ARMA $( p + 1 , q )$ process. However, the characteristic polynomial satisfies

$$
\begin{array}{l} 1 - (1 + \phi_ {1}) x - (\phi_ {2} - \phi_ {1}) x ^ {2} - (\phi_ {3} - \phi_ {2}) x ^ {3} - \dots - (\phi_ {p} - \phi_ {p - 1}) x ^ {p} + \phi_ {p} x ^ {p + 1} \\ = (1 - \phi_ {1} x - \phi_ {2} x ^ {2} - \dots - \phi_ {p} x ^ {p}) (1 - x) \\ \end{array}
$$

which can be easily checked. This factorization clearly shows the root at $x = 1$ , which implies nonstationarity. The remaining roots, however, are the roots of the characteristic polynomial of the stationary process $\nabla Y _ { t } .$ .

Explicit representations of the observed series in terms of either $W _ { t }$ or the white noise series underlying $W _ { t }$ are more difficult than in the stationary case. Since nonstationary processes are not in statistical equilibrium, we cannot assume that they go infinitely into the past or that they start at $t = - \infty$ . However, we can and shall assume that they start at some time point $t = - m$ , say, where $- m$ is earlier than time $t = 1$ , at which point we first observed the series. For convenience, we take $Y _ { t } = 0$ for $t < - m$ . The difference equation $Y _ { t } - Y _ { t - 1 } = W _ { t }$ can be solved by summing both sides from $t = - m$ to $t =$ $t$ to get the representation

$$
Y _ {t} = \sum_ {j = - m} ^ {t} W _ {j} \tag {5.2.3}
$$

for the $\mathrm { A R I M A } ( p , 1 , q )$ process.

The ARIMA(p,2,q) process can be dealt with similarly by summing twice to get the representations

$$
\begin{array}{l} Y _ {t} = \sum_ {j = - m} ^ {t} \sum_ {i = - m} ^ {j} W _ {i} \\ = \sum_ {j = 0} ^ {t + m} (j + 1) W _ {t - j} \tag {5.2.4} \\ \end{array}
$$

These representations have limited use but can be used to investigate the covariance properties of ARIMA models and also to express $Y _ { t }$ in terms of the white noise series $\{ \boldsymbol { e } _ { t } \}$ . We defer the calculations until we evaluate specific cases.

If the process contains no autoregressive terms, we call it an integrated moving average and abbreviate the name to $\mathrm { I M A } ( d , q )$ . If no moving average terms are present, we denote the model as $\mathrm { A R I } ( p , d )$ . We first consider in detail the important IMA(1,1) model.

# The IMA(1,1) Model

The simple IMA(1,1) model satisfactorily represents numerous time series, especially those arising in economics and business. In difference equation form, the model is

$$
Y _ {t} = Y _ {t - 1} + e _ {t} - \theta e _ {t - 1} \tag {5.2.5}
$$

To write $Y _ { t }$ explicitly as a function of present and past noise values, we use Equation (5.2.3) and the fact that $W _ { t } = e _ { t } - \theta e _ { t - 1 }$ in this case. After a little rearrangement, we can write

$$
Y _ {t} = e _ {t} + (1 - \theta) e _ {t - 1} + (1 - \theta) e _ {t - 2} + \dots + (1 - \theta) e _ {- m} - \theta e _ {- m - 1} \tag {5.2.6}
$$

Notice that in contrast to our stationary ARMA models, the weights on the white noise terms do not die out as we go into the past. Since we are assuming that $- m < 1$ and $0 < t$ , we may usefully think of $Y _ { t }$ as mostly an equally weighted accumulation of a large number of white noise values.

From Equation (5.2.6), we can easily derive variances and correlations. We have

$$
\operatorname {V a r} \left(Y _ {t}\right) = [ 1 + \theta^ {2} + (1 - \theta) ^ {2} (t + m) ] \sigma_ {e} ^ {2} \tag {5.2.7}
$$

and

$$
\begin{array}{l} C o r r (Y _ {t}, Y _ {t - k}) = \frac {1 - \theta + \theta^ {2} + (1 - \theta) ^ {2} (t + m - k)}{[ V a r (Y _ {t}) V a r (Y _ {t - k}) ] ^ {1 / 2}} \\ \approx \sqrt {\frac {t + m - k}{t + m}} \tag {5.2.8} \\ \approx 1 \quad \text {f o r l a r g e} m \text {a n d m o d e r a t e} k \\ \end{array}
$$

We see that as $t$ increases, $V a r ( Y _ { t } )$ increases and could be quite large. Also, the correlation between $Y _ { t }$ and $Y _ { t - k }$ will be strongly positive for many lags k = 1, 2, … .

# The IMA(2,2) Model

The assumptions of Equation (5.1.11) led to an IMA(2,2) model. In difference equation form, we have

$$
\nabla^ {2} Y _ {t} = e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2}
$$

or

$$
Y _ {t} = 2 Y _ {t - 1} - Y _ {t - 2} + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} \tag {5.2.9}
$$

The representation of Equation (5.2.4) may be used to express $Y _ { t }$ in terms of $e _ { t } , e _ { t - 1 } , \ldots .$ After some tedious algebra, we find that

$$
\begin{array}{l} Y _ {t} = e _ {t} + \sum_ {j = 1} ^ {t + m} \psi_ {j} e _ {t - j} - \left[ (t + m + 1) \theta_ {1} + (t + m) \theta_ {2} \right] e _ {- m - 1} \tag {5.2.10} \\ - (t + m + 1) \theta_ {2} e _ {- m - 2} \\ \end{array}
$$

where $\Psi _ { j } = 1 + \theta _ { 2 } + ( 1 - \theta _ { 1 } - \theta _ { 2 } ) j$ for $j = 1$ , 2, 3,…, $t + m$ . Once more we see that the $\boldsymbol { \Psi }$ -weights do not die out but form a linear function of $j$ .

Again, variances and correlations for $Y _ { t }$ can be obtained from the representation given in Equation (5.2.10), but the calculations are tedious. We shall simply note that the variance of $Y _ { t }$ increases rapidly with $t$ and again $C o r r ( Y _ { t } , Y _ { t - k } )$ is nearly 1 for all moderate $k$ .

The results of a simulation of an IMA(2, 2) process are displayed in Exhibit 5.5. Notice the smooth change in the process values (and the unimportance of the zero-mean function). The increasing variance and the strong, positive neighboring correlations dominate the appearance of the time series plot.

![](images/70f35c5f1e51ee392ea1359d2fca2778c4fc0daed7a40aa0cb94ca2eb19cc588.jpg)  
Exhibit 5.5 Simulation of an IMA(2,2) Series with $\theta _ { 1 } = 1$ and $\theta _ { 2 } = - 0 . 6$

```txt
> data(ima22.s)  
> plot(ima22.s, ylab='IMA(2,2) Simulation', type='o') 
```

Exhibit 5.6 shows the time series plot of the first difference of the simulated series. This series is also nonstationary, as it is governed by an IMA(1,2) model.

![](images/6f9934e21c2c8ac0e2a999e5f7b12545cbe81ec9d8bf275cf0c79972c851b36a.jpg)  
Exhibit 5.6 First Difference of the Simulated IMA(2,2) Series

```javascript
> plot(diff(ima22.s), ylab='First Difference', type='o') 
```

Finally, the second differences of the simulated IMA(2,2) series values are plotted in Exhibit 5.7. These values arise from a stationary MA(2) model with $\theta _ { 1 } = 1$ and $\theta _ { 2 } =$ −0.6. From Equation (4.2.3) on page 63, the theoretical autocorrelations for this model are $\rho _ { 1 } = - 0 . 6 7 8$ and ${ \rho } _ { 2 } = 0 . 2 5 4$ . These correlation values seem to be reflected in the appearance of the time series plot.

Exhibit 5.7 Second Difference of the Simulated IMA(2,2) Series   
![](images/41c97bf1cf5a0e6605d7b448805a7caadfd95c5ff621cbae76ae8d56a0de1c96.jpg)  
> plot(diff(ima22.s,difference $^ { \prime = 2 }$ ),ylab $\nu =$ 'Differenced Twice',type='o')

# The ARI(1,1) Model

The ARI(1,1) process will satisfy

$$
Y _ {t} - Y _ {t - 1} = \phi \left(Y _ {t - 1} - Y _ {t - 2}\right) + e _ {t} \tag {5.2.11}
$$

or

$$
Y _ {t} = (1 + \phi) Y _ {t - 1} - \phi Y _ {t - 2} + e _ {t} \tag {5.2.12}
$$

where $| \phi | < 1$ .†

To find the $\boldsymbol { \Psi }$ -weights in this case, we shall use a technique that will generalize to arbitrary ARIMA models. It can be shown that the $\boldsymbol { \Psi }$ -weights can be obtained by equating like powers of $x$ in the identity:

$$
\begin{array}{l} (1 - \phi_ {1} x - \phi_ {2} x ^ {2} - \dots - \phi^ {p} x ^ {p}) (1 - x) ^ {d} (1 + \psi_ {1} x + \psi_ {2} x ^ {2} + \psi_ {3} x ^ {3} + \dots) \\ = \left(1 - \theta_ {1} x - \theta_ {2} x ^ {2} - \theta_ {3} x ^ {3} - \dots - \theta_ {q} x ^ {q}\right) \tag {5.2.13} \\ \end{array}
$$

In our case, this relationship reduces to

$$
(1 - \phi x) (1 - x) \left(1 + \psi_ {1} x + \psi_ {2} x ^ {2} + \psi_ {3} x ^ {3} + \dots\right) = 1
$$

or

$$
[ 1 - (1 + \phi) x + \phi x ^ {2} ] (1 + \psi_ {1} x + \psi_ {2} x ^ {2} + \psi_ {3} x ^ {3} + \dots) = 1
$$

Equating like powers of $x$ on both sides, we obtain

$$
- (1 + \phi) + \psi_ {1} = 0
$$

$$
\phi - (1 + \phi) \psi_ {1} + \psi_ {2} = 0
$$

and, in general,

$$
\psi_ {k} = (1 + \phi) \psi_ {k - 1} - \phi \psi_ {k - 2} \quad \text {f o r} k \geq 2 \tag {5.2.14}
$$

with $\Psi _ { 0 } = 1$ and $\Psi _ { 1 } = 1 + \Phi$ . This recursion with starting values allows us to compute as many $\boldsymbol { \Psi }$ -weights as necessary. It can also be shown that in this case an explicit solution to the recursion is given as

$$
\psi_ {k} = \frac {1 - \phi^ {k + 1}}{1 - \phi} \text {f o r} k \geq 1 \tag {5.2.15}
$$

(It is easy, for example, to show that this expression satisfies Equation (5.2.14).

# 5.3 Constant Terms in ARIMA Models

For an $\mathrm { A R I M A } ( p , d , q )$ model, $\nabla ^ { d } Y _ { t } = W _ { t }$ is a stationary $\mathbf { A R M A } ( p , q )$ process. Our standard assumption is that stationary models have a zero mean; that is, we are actually working with deviations from the constant mean. A nonzero constant mean, $\mu$ , in a stationary ARMA model $\{ W _ { t } \}$ can be accommodated in either of two ways. We can assume that

$$
\begin{array}{l} W _ {t} - \mu = \phi_ {1} \left(W _ {t - 1} - \mu\right) + \phi_ {2} \left(W _ {t - 2} - \mu\right) + \dots + \phi_ {p} \left(W _ {t - p} - \mu\right) \\ + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} - \dots - \theta_ {q} e _ {t - q} \\ \end{array}
$$

Alternatively, we can introduce a constant term $\theta _ { 0 }$ into the model as follows:

$$
\begin{array}{l} W _ {t} = \theta_ {0} + \phi_ {1} W _ {t - 1} + \phi_ {2} W _ {t - 2} + \dots + \phi_ {p} W _ {t - p} \\ + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} - \dots - \theta_ {q} e _ {t - q} \\ \end{array}
$$

Taking expected values on both sides of the latter expression, we find that

$$
\mu = \theta_ {0} + (\phi_ {1} + \phi_ {2} + \dots + \phi_ {p}) \mu
$$

so that

$$
\mu = \frac {\theta_ {0}}{1 - \phi_ {1} - \phi_ {2} - \cdots - \phi_ {p}} \tag {5.3.16}
$$

or, conversely, that

$$
\theta_ {0} = \mu \left(1 - \phi_ {1} - \phi_ {2} - \dots - \phi_ {p}\right) \tag {5.3.17}
$$

Since the alternative representations are equivalent, we shall use whichever parameterization is convenient.

What will be the effect of a nonzero mean for $W _ { t }$ on the undifferenced series Yt? Consider the IMA(1,1) case with a constant term. We have

$$
Y _ {t} = Y _ {t - 1} + \theta_ {0} + e _ {t} - \theta e _ {t - 1}
$$

or

$$
W _ {t} = \theta_ {0} + e _ {t} - \theta e _ {t - 1}
$$

Either by substituting into Equation (5.2.3) on page 93 or by iterating into the past, we find that

$$
Y _ {t} = e _ {t} + (1 - \theta) e _ {t - 1} + (1 - \theta) e _ {t - 2} + \dots + (1 - \theta) e _ {- m} - \theta e _ {- m - 1}
$$

(5.3.18)

$$
+ (t + m + 1) \theta_ {0}
$$

Comparing this with Equation (5.2.6), we see that we have an added linear deterministic time trend $( t + m + 1 ) \theta _ { 0 }$ with slope $\theta _ { 0 }$ .

An equivalent representation of the process would then be

$$
Y _ {t} = Y _ {t} ^ {\prime} + \beta_ {0} + \beta_ {1} t
$$

where $Y _ { t } ^ { \prime }$ is an IMA(1,1) series with $E ( \nabla Y _ { t } ^ { \prime } ) = 0$ and $E ( \nabla Y _ { t } ) = \beta _ { 1 }$

For a general $\mathbf { A R I M A } ( p , d , q )$ model where $E ( \nabla ^ { d } Y _ { t } ) \neq \dot { 0 }$ , it can be argued that $Y _ { t } =$ $Y _ { t } ^ { \prime } + \mu _ { t }$ , where $\mu _ { t }$ is a deterministic polynomial of degree $d$ and $Y _ { t } ^ { \prime }$ is ARIMA $( p , d , q )$ with $E Y _ { t } ^ { \prime } = 0$ . With $d = 2$ and $\theta _ { 0 } \neq 0$ , a quadratic trend would be implied.

# 5.4 Other Transformations

We have seen how differencing can be a useful transformation for achieving stationarity. However, the logarithm transformation is also a useful method in certain circumstances. We frequently encounter series where increased dispersion seems to be associated with higher levels of the series—the higher the level of the series, the more variation there is around that level and conversely.

Specifically, suppose that $Y _ { t } > 0$ for all $t$ and that

$$
E \left(Y _ {t}\right) = \mu_ {t} \quad \text {a n d} \quad \sqrt {\operatorname {V a r} \left(Y _ {t}\right)} = \mu_ {t} \sigma \tag {5.4.1}
$$

Then

$$
E \left[ \log \left(Y _ {t}\right) \right] \approx \log \left(\mu_ {t}\right) \quad \text {a n d} \quad V a r \left(\log \left(Y _ {t}\right)\right) \approx \sigma^ {2} \tag {5.4.2}
$$

These results follow from taking expected values and variances of both sides of the (Taylor) expansion

$$
\log \left(Y _ {t}\right) \approx \log \left(\mu_ {t}\right) + \frac {Y _ {t} - \mu_ {t}}{\mu_ {t}}
$$

In words, if the standard deviation of the series is proportional to the level of the series, then transforming to logarithms will produce a series with approximately constant variance over time. Also, if the level of the series is changing roughly exponentially, the

log-transformed series will exhibit a linear time trend. Thus, we might then want to take first differences. An alternative set of assumptions leading to differences of logged data follows.

# Percentage Changes and Logarithms

Suppose $Y _ { t }$ tends to have relatively stable percentage changes from one time period to the next. Specifically, assume that

$$
Y _ {t} = (1 + X _ {t}) Y _ {t - 1}
$$

where $1 0 0 X _ { t }$ is the percentage change (possibly negative) from $Y _ { t - 1 }$ to $Y _ { t } .$ Then

$$
\begin{array}{l} \log \left(Y _ {t}\right) - \log \left(Y _ {t - 1}\right) = \log \left(\frac {Y _ {t}}{Y _ {t - 1}}\right) \\ = \log (1 + X _ {t}) \\ \end{array}
$$

If $X _ { t }$ is restricted to, say, $\left| X _ { t } \right| < 0 . 2$ (that is, the percentage changes are at most $\pm 2 0 \%$ ), then, to a good approximation, $\log ( 1 { + } X _ { t } ) \approx X _ { t } .$ . Consequently,

$$
\nabla \left[ \log \left(Y _ {t}\right) \right] \approx X _ {t} \tag {5.4.3}
$$

will be relatively stable and perhaps well-modeled by a stationary process. Notice that we take logs first and then compute first differences—the order does matter. In financial literature, the differences of the (natural) logarithms are usually called returns.

As an example, consider the time series shown in Exhibit 5.8. This series gives the total monthly electricity generated in the United States in millions of kilowatt-hours. The higher values display considerably more variation than the lower values.

Exhibit 5.8 U.S. Electricity Generated by Month   
![](images/cb532b9f3cf824cee71bf05e6fa27e7d94eb5612b3f6c598458b06e699a25ed2.jpg)  
> data(electricity); plot(electricity)

Exhibit 5.9 displays the time series plot of the logarithms of the electricity values. Notice how the amount of variation around the upward trend is now much more uniform across high and low values of the series.

Exhibit 5.9 Time Series Plot of Logarithms of Electricity Values   
![](images/38acb5c6b9c70ebc08bfd316666aa1d9f7ded9a6710fd55551132e17a296f89a.jpg)  
> plot(log(electricity),ylab='Log(electricity)')

The differences of the logarithms of the electricity values are displayed in Exhibit 5.10. On the basis of this plot, we might well consider a stationary model as appropriate.

Exhibit 5.10 Difference of Logarithms for Electricity Time Series   
![](images/56ef909bd785ae57a4bb7a7cce1b1617f78972145997ae8618048e04041c8d9a.jpg)  
> plot(diff(log(electricity)), ylab='Difference of Log(electricity)')

# Power Transformations

A flexible family of transformations, the power transformations, was introduced by Box and Cox (1964). For a given value of the parameter $\gimel$ , the transformation is defined by

$$
g (x) = \left\{ \begin{array}{l l} \frac {x ^ {\lambda} - 1}{\lambda} & \text {f o r} \lambda \neq 0 \\ \log x & \text {f o r} \lambda = 0 \end{array} \right. \tag {5.4.4}
$$

The term $x ^ { \lambda }$ is the important part of the first expression, but subtracting 1 and dividing by $\gimel$ makes $g ( x )$ change smoothly as $\gimel$ approaches zero. In fact, a calculus argument† shows that as $\lambda  0$ , $( x ^ { \lambda } - 1 ) / \lambda { \stackrel { \cdot } { \to } } \log ( { \stackrel { \cdot } { x } } )$ . Notice that $\lambda = \%$ produces a square root transformation useful with Poisson-like data, and $\lambda = - 1$ corresponds to a reciprocal transformation.

The power transformation applies only to positive data values. If some of the values are negative or zero, a positive constant may be added to all of the values to make them all positive before doing the power transformation. The shift is often determined subjectively. For example, for nonnegative catch data in biology, the occurrence of zeros is often dealt with by adding a constant equal to the smallest positive data value to all of the data values. An alternative approach consists of using transformations applicable to any data—positive or not. A drawback of this alternative approach is that interpretations of such transformations are often less straightforward than the interpretations of the power transformations. See Yeo and Johnson (2000) and the references contained therein.

We can consider $\gimel$ as an additional parameter in the model to be estimated from the observed data. However, precise estimation of $\gimel$ is usually not warranted. Evaluation of a range of transformations based on a grid of $\gimel$ values, say $\pm 1$ , ±1/2, $\pm 1 / 3$ , $\pm 1 / 4$ , and 0, will usually suffice and may have some intuitive meaning.

Software allows us to consider a range of lambda values and calculate a log-likelihood value for each lambda value based on a normal likelihood function. A plot of these values is shown in Exhibit 5.11 for the electricity data. The $9 5 \%$ confidence interval for $\gimel$ contains the value of $\lambda = 0$ quite near its center and strongly suggests a logarithmic transformation $( \lambda = 0$ ) for these data.

Exhibit 5.11 Log-likelihood versus Lambda   
![](images/9db1c2d486971a6e6c115fbaca8e55ce9b9d593812d77bb7a20385134c4dc3ac.jpg)  
> BoxCox.ar(electricity)

# 5.5 Summary

This chapter introduced the concept of differencing to induce stationarity on certain nonstationary processes. This led to the important integrated autoregressive moving average models (ARIMA). The properties of these models were then thoroughly explored. Other transformations, namely percentage changes and logarithms, were then considered. More generally, power transformations or Box-Cox transformations were introduced as useful transformations to stationarity and often normality.

# EXERCISES

5.1 Identify the following as specific ARIMA models. That is, what are $p , d$ , and $q$ and what are the values of the parameters (the $\phi$ ’s and θ’s)?

(a) $Y _ { t } = Y _ { t - 1 } - 0 . 2 5 Y _ { t - 2 } + e _ { t } - 0 . 1 e _ { t - 1 }$   
(b) $Y _ { t } = 2 Y _ { t - 1 } - Y _ { t - 2 } + e _ { t }$ .   
(c) $Y _ { t } = 0 . 5 Y _ { t - 1 } - 0 . 5 Y _ { t - 2 } + e _ { t } - 0 . 5 e _ { t - 1 } + 0 . 2 5 e _ { t - 2 } .$

5.2 For each of the ARIMA models below, give the values for $E ( \nabla Y _ { t } )$ and $V a r ( \nabla Y _ { t } )$

(a) $Y _ { t } = 3 + Y _ { t - 1 } + e _ { t } - 0 . 7 5 e _ { t - 1 }$   
(b) $Y _ { t } = 1 0 + 1 . 2 5 Y _ { t - 1 } - 0 . 2 5 Y _ { t - 2 } + e _ { t } - 0 . 1 e _ { t - 1 } .$   
(c) $Y _ { t } = 5 + 2 Y _ { t - 1 } - 1 . 7 Y _ { t - 2 } + 0 . 7 Y _ { t - 3 } + e _ { t } - 0 . 5 e _ { t - 1 } + 0 . 2 5 e _ { t - 2 } .$

5.3 Suppose that $\{ Y _ { t } \}$ is generated according to $Y _ { t } = e _ { t } + c e _ { t - 1 } + c e _ { t - 2 } + c e _ { t - 3 } + \cdots +$ $c e _ { 0 }$ for $t > 0$ .

(a) Find the mean and covariance functions for $\{ Y _ { t } \}$ . Is $\{ Y _ { t } \}$ stationary?   
(b) Find the mean and covariance functions for $\{ \nabla Y _ { t } \}$ . Is $\{ \nabla Y _ { t } \}$ stationary?   
(c) Identify $\{ Y _ { t } \}$ as a specific ARIMA process.

5.4 Suppose that $Y _ { t } = A + B t + X _ { t } , $ where $\{ X _ { t } \}$ is a random walk. First suppose that $A$ and $B$ are constants.

(a) Is $\{ Y _ { t } \}$ stationary?   
(b) Is $\{ \nabla Y _ { t } \}$ stationary?

Now suppose that $A$ and $B$ are random variables that are independent of the random walk $\{ X _ { t } \}$ .

(c) Is $\{ Y _ { t } \}$ stationary?   
(d) Is $\{ \nabla Y _ { t } \}$ stationary?

5.5 Using the simulated white noise values in Exhibit 5.2, on page 88, verify the values shown for the explosive process $Y _ { t } .$   
5.6 Consider a stationary process $\{ Y _ { t } \}$ . Show that if $\rho _ { 1 } < \%$ , $\nabla Y _ { t }$ has a larger variance than does $Y _ { t } .$

5.7 Consider two models:

A: $Y _ { t } = 0 . 9 Y _ { t - 1 } + 0 . 0 9 Y _ { t - 2 } + e _ { t } .$   
B: $Y _ { t } = Y _ { t - 1 } + e _ { t } - 0 . 1 e _ { t - 1 }$

(a) Identify each as a specific ARIMA model. That is, what are $p , d$ , and $q$ and what are the values of the parameters, $\boldsymbol { \Phi }$ ’s and θ’s?   
(b) In what ways are the two models different?   
(c) In what ways are the two models similar? (Compare $\boldsymbol { \Psi }$ -weights and $\pi$ -weights.)

5.8 Consider a nonstationary “AR(1)” process defined as a solution to Equation (5.1.2) on page 88, with $\vert \Phi \vert > 1$ .

(a) Derive an equation similar to Equation (5.1.3) on page 88, for this more general case. Use $Y _ { 0 } = 0$ as an initial condition.   
(b) Derive an equation similar to Equation (5.1.4) on page 89, for this more general case.   
(c) Derive an equation similar to Equation (5.1.5) on page 89, for this more general case.   
(d) Is it true that for any $\vert \Phi \vert > 1$ , $C o r r ( Y _ { t } , Y _ { t - k } ) \approx 1$ for large t and moderate k?

5.9 Verify Equation (5.1.10) on page 90.

5.10 Nonstationary ARIMA series can be simulated by first simulating the corresponding stationary ARMA series and then “integrating” it (really partially summing it). Use statistical software to simulate a variety of IMA(1,1) and IMA(2, 2) series with a variety of parameter values. Note any stochastic “trends” in the simulated series.   
5.11 The data file winnebago contains monthly unit sales of recreational vehicles (RVs) from Winnebago, Inc., from November 1966 through February 1972.

(a) Display and interpret the time series plot for these data.   
(b) Now take natural logarithms of the monthly sales figures and display the time series plot of the transformed values. Describe the effect of the logarithms on the behavior of the series.   
(c) Calculate the fractional relative changes, $( Y _ { t } - Y _ { t - 1 } ) / Y _ { t - 1 }$ , and compare them with the differences of (natural) logarithms, $\nabla \log ( Y _ { t } ) = \log ( Y _ { t } ) - \log ( Y _ { t - 1 } )$ . How do they compare for smaller values and for larger values?

5.12 The data file SP contains quarterly Standard & Poor’s Composite Index stock price values from the first quarter of 1936 through the fourth quarter of 1977.

(a) Display and interpret the time series plot for these data.   
(b) Now take natural logarithms of the quarterly values and display and the time series plot of the transformed values. Describe the effect of the logarithms on the behavior of the series.   
(c) Calculate the (fractional) relative changes, $( Y _ { t } - Y _ { t - 1 } ) / Y _ { t - 1 }$ , and compare them to the differences of (natural) logarithms, $\nabla { \log } ( Y _ { t } )$ . How do they compare for smaller values and for larger values?

5.13 The data file airpass contains international airline passenger monthly totals (in thousands) flown from January 1960 through December 1971. This is a classic time series analyzed in Box and Jenkins (1976).

(a) Display and interpret the time series plot for these data.   
(b) Now take natural logarithms of the monthly values and display and the time series plot of the transformed values. Describe the effect of the logarithms on the behavior of the series.   
(c) Calculate the (fractional) relative changes, $( Y _ { t } - Y _ { t - 1 } ) / Y _ { t - 1 }$ , and compare them to the differences of (natural) logarithms, $\nabla { \log } ( Y _ { t } )$ . How do they compare for smaller values and for larger values?

5.14 Consider the annual rainfall data for Los Angeles shown in Exhibit 1.1, on page 2. The quantile-quantile normal plot of these data, shown in Exhibit 3.17, on page 50, convinced us that the data were not normal. The data are in the file larain.

(a) Use software to produce a plot similar to Exhibit 5.11, on page 102, and determine the “best” value of $\gimel$ for a power transformation of the data.   
(b) Display a quantile-quantile plot of the transformed data. Are they more normal?   
(c) Produce a time series plot of the transformed values.   
(d) Use the transformed values to display a plot of $Y _ { t }$ versus $Y _ { t - 1 }$ as in Exhibit 1.2, on page 2. Should we expect the transformation to change the dependence or lack of dependence in the series?

5.15 Quarterly earnings per share for the Johnson & Johnson Company are given in the data file named JJ. The data cover the years from 1960 through 1980.

(a) Display a time series plot of the data. Interpret the interesting features in the plot.   
(b) Use software to produce a plot similar to Exhibit 5.11, on page 102, and determine the “best” value of $\gimel$ for a power transformation of these data.   
(c) Display a time series plot of the transformed values. Does this plot suggest that a stationary model might be appropriate?   
(d) Display a time series plot of the differences of the transformed values. Does this plot suggest that a stationary model might be appropriate for the differences?

5.16 The file named gold contains the daily price of gold (in dollars per troy ounce) for the 252 trading days of year 2005.

(a) Display the time series plot of these data. Interpret the plot.   
(b) Display the time series plot of the differences of the logarithms of these data. Interpret this plot.   
(c) Calculate and display the sample ACF for the differences of the logarithms of these data and argue that the logarithms appear to follow a random walk model.   
(d) Display the differences of logs in a histogram and interpret.   
(e) Display the differences of logs in a quantile-quantile normal plot and interpret.

5.17 Use calculus to show that, for any fixed $x > 0$ , as $\lambda \to 0$ , $( x ^ { \lambda } - 1 ) / \lambda \to \log x$

# Appendix D: The Backshift Operator

Many other books and much of the time series literature use what is called the backshift operator to express and manipulate ARIMA models. The backshift operator, denoted $B$ , operates on the time index of a series and shifts time back one time unit to form a new series.† In particular,

$$
B Y _ {t} = Y _ {t - 1}
$$

The backshift operator is linear since for any constants a, b, and $c$ and series $Y _ { t }$ and $X _ { t }$ , it is easy to see that

$$
B (a Y _ {t} + b X _ {t} + c) = a B Y _ {t} + b B X _ {t} + c
$$

Consider now the MA(1) model. In terms of $B$ , we can write

$$
\begin{array}{l} Y _ {t} = e _ {t} - \theta e _ {t - 1} = e _ {t} - \theta B e _ {t} = (1 - \theta B) e _ {t} \\ = \theta (B) e _ {t} \\ \end{array}
$$

where $\theta ( B )$ is the MA characteristic polynomial “evaluated” at $B$ .

Since $B Y _ { t }$ is itself a time series, it is meaningful to consider $B B Y _ { t }$ . But clearly $B B Y _ { t }$ $= B Y _ { t - 1 } = Y _ { t - 2 }$ , and we can write

$$
B ^ {2} Y _ {t} = Y _ {t - 2}
$$

More generally, we have

$$
B ^ {m} Y _ {t} = Y _ {t - m}
$$

for any positive integer m. For a general MA(q) model, we can then write

$$
\begin{array}{l} Y _ {t} = e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} - \dots - \theta_ {q} e _ {t - q} \\ = e _ {t} - \theta_ {1} B e _ {t} - \theta_ {2} B ^ {2} e _ {t} - \dots - \theta_ {q} B ^ {q} e _ {t} \\ = (1 - \theta_ {1} B - \theta_ {2} B ^ {2} - \dots - \theta_ {q} B ^ {q}) e _ {t} \\ \end{array}
$$

or

$$
Y _ {t} = \theta (B) e _ {t}
$$

where, again, $\theta ( B )$ is the MA characteristic polynomial evaluated at $B$

For autoregressive models $\operatorname { A R } ( p )$ , we first move all of the terms involving Y to the left-hand side

$$
Y _ {t} - \phi_ {1} Y _ {t - 1} - \phi_ {2} Y _ {t - 2} - \dots - \phi_ {p} Y _ {t - p} = e _ {t}
$$

and then write

$$
Y _ {t} - \phi_ {1} B Y _ {t} - \phi_ {2} B ^ {2} Y _ {t} - \dots - \phi_ {p} B ^ {p} Y _ {t} = e _ {t}
$$

or

$$
(1 - \phi_ {1} B - \phi_ {2} B ^ {2} - \dots - \phi_ {p} B ^ {p}) Y _ {t} = e _ {t}
$$

which can be expressed as

$$
\phi (B) Y _ {t} = e _ {t}
$$

where $\Phi ( B )$ is the AR characteristic polynomial evaluated at $B$ .

Combining the two, the general ARMA $( p , q )$ model may be written compactly as

$$
\phi (B) Y _ {t} = \theta (B) e _ {t}
$$

Differencing can also be conveniently expressed in terms of $B$ . We have

$$
\begin{array}{l} \nabla Y _ {t} = Y _ {t} - Y _ {t - 1} = Y _ {t} - B Y _ {t} \\ = (1 - B) Y _ {t} \\ \end{array}
$$

with second differences given by

$$
\nabla^ {2} Y _ {t} = (1 - B) ^ {2} Y _ {t}
$$

Effectively, $\nabla = 1 - B$ and $\nabla ^ { 2 } = ( 1 - B ) ^ { 2 }$

The general ARIMA $( p , d , q )$ model is expressed concisely as

$$
\phi (B) (1 - B) ^ {d} Y _ {t} = \theta (B) e _ {t}
$$

In the literature, one must carefully distinguish from the context the use of $B$ as a backshift operator and its use as an ordinary real (or complex) variable. For example, the stationarity condition is frequently given by stating that the roots of $\Phi ( B ) = 0$ must be greater than 1 in absolute value or, equivalently, must lie outside the unit circle in the complex plane. Here $B$ is to be treated as a dummy variable in an equation rather than as the backshift operator.

# CHAPTER 6

# MODEL SPECIFICATION

We have developed a large class of parametric models for both stationary and nonstationary time series—the ARIMA models. We now begin our study and implementation of statistical inference for such models. The subjects of the next three chapters, respectively, are:

1. how to choose appropriate values for $p$ , $d$ , and $q$ for a given series;   
2. how to estimate the parameters of a specific ARIMA $_ { ( p , d , q ) }$ model;   
3. how to check on the appropriateness of the fitted model and improve it if needed.

Our overall strategy will first be to decide on reasonable—but tentative—values for p, d, and $q$ . Having done so, we shall estimate the $\phi$ ’s, θ’s, and $\sigma _ { e }$ for that model in the most efficient way. Finally, we shall look critically at the fitted model thus obtained to check its adequacy, in much the same way that we did in Section 3.6 on page 42. If the model appears inadequate in some way, we consider the nature of the inadequacy to help us select another model. We proceed to estimate that new model and check it for adequacy.

With a few iterations of this model-building strategy, we hope to arrive at the best possible model for a given series. The book by George E. P. Box and G. M. Jenkins (1976) so popularized this technique that many authors call the procedure the “Box-Jenkins method.” We begin by continuing our investigation of the properties of the sample autocorrelation function.

# 6.1 Properties of the Sample Autocorrelation Function

Recall from page 46 the definition of the sample or estimated autocorrelation function. For the observed series $Y _ { 1 }$ , Y2,…, $Y _ { n }$ , we have

$$
r _ {k} = \frac {\sum_ {t = k + 1} ^ {n} \left(Y _ {t} - \bar {Y}\right) \left(Y _ {t - k} - \bar {Y}\right)}{\sum_ {t = 1} ^ {n} \left(Y _ {t} - \bar {Y}\right) ^ {2}} \quad \text {f o r} k = 1, 2, \dots \tag {6.1.1}
$$

Our goal is to recognize, to the extent possible, patterns in $r _ { k }$ that are characteristic of the known patterns in $\rho _ { k }$ for common ARMA models. For example, we know that $\rho _ { k } = 0$ for $k > q$ in an $\mathrm { M A } ( q )$ model. However, as the $r _ { k }$ are only estimates of the $\rho _ { k }$ , we

need to investigate their sampling properties to facilitate the comparison of estimated correlations with theoretical correlations.

From the definition of $r _ { k }$ , a ratio of quadratic functions of possibly dependent variables, it should be apparent that the sampling properties of $r _ { k }$ will not be obtained easily. Even the expected value of $r _ { k }$ is difficult to determine—recall that the expected value of a ratio is not the ratio of the respective expected values. We shall be content to accept a general large-sample result and consider its implications in special cases. Bartlett (1946) carried out the original work. We shall take a more general result from Anderson (1971). A recent discussion of these results may be found in Shumway and Stoffer (2006, p. 519).

We suppose that

$$
Y _ {t} = \mu + \sum_ {j = 0} ^ {\infty} \psi_ {j} e _ {t - j}
$$

where the $e _ { t }$ are independent and identically distributed with zero means and finite, nonzero, common variances. We assume further that

$$
\sum_ {j = 0} ^ {\infty} \left| \psi_ {j} \right| <   \infty \quad \text {a n d} \quad \sum_ {j = 0} ^ {\infty} j \psi_ {j} ^ {2} <   \infty
$$

(These will be satisfied by any stationary ARMA model.)

Then, for any fixed $m$ , the joint distribution of

$$
\sqrt {n} (r _ {1} - \rho_ {1}), \sqrt {n} (r _ {2} - \rho_ {2}), \ldots , \sqrt {n} (r _ {m} - \rho_ {m})
$$

approaches, as $n \to \infty$ , a joint normal distribution with zero means, variances $c _ { j j }$ , and covariances $c _ { i j }$ ,where

$$
c _ {i j} = \sum_ {k = - \infty} ^ {\infty} \left(\rho_ {k + i} \rho_ {k + j} + \rho_ {k - i} \rho_ {k + j} - 2 \rho_ {i} \rho_ {k} \rho_ {k + j} - 2 \rho_ {j} \rho_ {k} \rho_ {k + i} + 2 \rho_ {i} \rho_ {j} \rho_ {k} ^ {2}\right) \tag {6.1.2}
$$

For large $n$ , we would say that $r _ { k }$ is approximately normally distributed with mean $\rho _ { k }$ and variance $c _ { k k } / n$ . Furthermore, $C o r r ( \boldsymbol { r } _ { k } , \boldsymbol { r } _ { j } ) \approx c _ { k j } \sqrt [ /object Object] { c _ { k k } c _ { j j } }$ . Notice that the approximate variance of $r _ { k }$ is inversely proportional to the sample size, but $C o r r ( r _ { k } , r _ { j } )$ is approximately constant for large $n$ .

Since Equation (6.1.2) is clearly difficult to interpret in its present generality, we shall consider some important special cases and simplifications. Suppose first that $\{ Y _ { t } \}$ is white noise. Then Equation (6.1.2) reduces considerably, and we obtain

$$
\operatorname {V a r} \left(r _ {k}\right) \approx \frac {1}{n} \text {a n d} \operatorname {C o r r} \left(r _ {k}, r _ {j}\right) \approx 0 \text {f o r} k \neq j \tag {6.1.3}
$$

Next suppose that $\{ Y _ { t } \}$ is generated by an AR(1) process with $\rho _ { k } = \boldsymbol \Phi ^ { k }$ for $k > 0$ . Then, after considerable algebra and summing several geometric series, Equation (6.1.2) with $i = j$ yields

$$
V a r \left(r _ {k}\right) \approx \frac {1}{n} \left[ \frac {\left(1 + \phi^ {2}\right) \left(1 - \phi^ {2 k}\right)}{1 - \phi^ {2}} - 2 k \phi^ {2 k} \right] \tag {6.1.4}
$$

In particular,

$$
\operatorname {V a r} \left(r _ {1}\right) \approx \frac {1 - \phi^ {2}}{n} \tag {6.1.5}
$$

Notice that the closer $\phi$ is to $\pm 1$ , the more precise our estimate of $\rho _ { 1 } \left( = \phi \right)$ becomes.

For large lags, the terms in Equation (6.1.4) involving $\boldsymbol { \phi } ^ { k }$ may be ignored, and we have

$$
\operatorname {V a r} \left(r _ {k}\right) \approx \frac {1}{n} \left[ \frac {1 + \phi^ {2}}{1 - \phi^ {2}} \right] \text {f o r l a r g e} k \tag {6.1.6}
$$

Notice that here, in contrast to Equation (6.1.5), values of $\phi$ close to $\pm 1$ imply large variances for $r _ { k }$ . Thus we should not expect nearly as precise estimates of $\rho _ { k } = \phi ^ { k } \approx 0$ f o r large $k$ as we do of $\rho _ { k } = \boldsymbol \Phi ^ { k }$ for small $k$ .

For the AR(1) model, Equation (6.1.2) can also be simplified (after much algebra) for general $0 < i < j$ as

$$
c _ {i j} = \frac {\left(\phi^ {j - i} - \phi^ {j + i}\right) \left(1 + \phi^ {2}\right)}{1 - \phi^ {2}} + (j - i) \phi^ {j - i} - (j + i) \phi^ {j + i} \tag {6.1.7}
$$

In particular, we find

$$
\operatorname {C o r r} \left(r _ {1}, r _ {2}\right) \approx 2 \phi \sqrt {\frac {1 - \phi^ {2}}{1 + 2 \phi^ {2} - 3 \phi^ {4}}} \tag {6.1.8}
$$

Based on Equations (6.1.4) through (6.1.8), Exhibit 6.1 gives approximate standard deviations and correlations for several lags and a few values of $\boldsymbol { \Phi }$ in AR(1) models.

<table><tr><td>Exhibit 6.1</td><td colspan="4">Large Sample Results for Selected rk from an AR(1) Model</td></tr><tr><td>φ</td><td>√Var(r1)</td><td>√Var(r2)</td><td>Corr(r1, r2)</td><td>√Var(r10)</td></tr><tr><td>±0.9</td><td>0.44/√n</td><td>0.807/√n</td><td>±0.97</td><td>2.44/√n</td></tr><tr><td>±0.7</td><td>0.71/√n</td><td>1.12/√n</td><td>±0.89</td><td>1.70/√n</td></tr><tr><td>±0.4</td><td>0.92/√n</td><td>1.11/√n</td><td>±0.66</td><td>1.18/√n</td></tr><tr><td>±0.2</td><td>0.98/√n</td><td>1.04/√n</td><td>±0.38</td><td>1.04/√n</td></tr></table>

For the MA(1) case, Equation (6.1.2) simplifies as follows:

$$
c _ {1 1} = 1 - 3 \rho_ {1} ^ {2} + 4 \rho_ {1} ^ {4} \quad \text {a n d} \quad c _ {k k} = 1 + 2 \rho_ {1} ^ {2} \text {f o r} k > 1 \tag {6.1.9}
$$

Furthermore,

$$
c _ {1 2} = 2 \rho_ {1} \left(1 - \rho_ {1} ^ {2}\right) \tag {6.1.10}
$$

Based on these expressions, Exhibit 6.2 lists large-sample standard deviations and correlations for the sample autocorrelations for several lags and several θ-values. Notice again that the sample autocorrelations can be highly correlated and that the standard deviation of $r _ { k }$ is larger for $k > 1$ than for $k = 1$ .

<table><tr><td>Exhibit 6.2</td><td colspan="3">Large-Sample Results for Selected rk from an MA(1) Model</td></tr><tr><td>θ</td><td>√Var(r1)</td><td>√Var(rk) for k &gt; 1</td><td>Corr(r1, r2)</td></tr><tr><td>±0.9</td><td>0.71/√n</td><td>1.22/√n</td><td>≠0.86</td></tr><tr><td>±0.7</td><td>0.73/√n</td><td>1.20/√n</td><td>≠0.84</td></tr><tr><td>±0.5</td><td>0.79/√n</td><td>1.15/√n</td><td>≠0.74</td></tr><tr><td>±0.4</td><td>0.89/√n</td><td>1.11/√n</td><td>≠0.53</td></tr></table>

For a general $\mathrm { M A } ( q )$ process and $i = j = k$ , Equation (6.1.2) reduces to

$$
c _ {k k} = 1 + 2 \sum_ {j = 1} ^ {q} \rho_ {j} ^ {2} \text {f o r} k > q
$$

so that

$$
\operatorname {V a r} \left(r _ {k}\right) = \frac {1}{n} \left[ 1 + 2 \sum_ {j = 1} ^ {q} \rho_ {j} ^ {2} \right] \text {f o r} k > q \tag {6.1.11}
$$

For an observed time series, we can replace ρ’s by r’s, take the square root, and obtain an estimated standard deviation of $r _ { k }$ , that is, the standard error of $r _ { k }$ for large lags. A test of the hypothesis that the series is $\mathrm { M A } ( q )$ could be carried out by comparing $r _ { k }$ to plus and minus two standard errors. We would reject the null hypothesis if and only if $r _ { k }$ lies outside these bounds. In general, we should not expect the sample autocorrelation to mimic the true autocorrelation in great detail. Thus, we should not be surprised to see ripples or “trends” in $r _ { k }$ that have no counterparts in the $\rho _ { k }$ .

# 6.2 The Partial and Extended Autocorrelation Functions

Since for $\mathrm { M A } ( q )$ models the autocorrelation function is zero for lags beyond $q$ , the sample autocorrelation is a good indicator of the order of the process. However, the autocorrelations of an $\operatorname { A R } ( p )$ model do not become zero after a certain number of lags—they die off rather than cut off. So a different function is needed to help determine the order of autoregressive models. Such a function may be defined as the correlation between $Y _ { t }$ and $Y _ { t - k }$ after removing the effect of the intervening variables $Y _ { t - 1 }$ , $Y _ { t - 2 }$ , Yt − 3,…, $Y _ { t - k + 1 }$ . This coefficient is called the partial autocorrelation at lag $k$ and will be denoted by $\phi _ { k k }$ . (The reason for the seemingly redundant double subscript on $\phi _ { k k }$ will become apparent later on in this section.)

There are several ways to make this definition precise. If $\{ Y _ { t } \}$ is a normally distributed time series, we can let

$$
\phi_ {k k} = \operatorname {C o r r} \left(Y _ {t}, Y _ {t - k} \mid Y _ {t - 1}, Y _ {t - 2}, \dots , Y _ {t - k + 1}\right) \tag {6.2.1}
$$

That is, $\phi _ { k k }$ is the correlation in the bivariate distribution of $Y _ { t }$ and $Y _ { t - k }$ conditional on $Y _ { t - 1 } , Y _ { t - 2 } , . . . , Y _ { t - k + 1 }$ .

An alternative approach, not based on normality, can be developed in the following way. Consider predicting $Y _ { t }$ based on a linear function of the intervening variables $Y _ { t - 1 }$ , $Y _ { t - 2 } , . . . , Y _ { t - k + 1 }$ , say, $\beta _ { 1 } Y _ { t - 1 } + \beta _ { 2 } Y _ { t - 2 } + \ldots + \beta _ { k - 1 } Y _ { t - k + 1 }$ , with the $\beta$ ’s chosen to minimize the mean square error of prediction. If we assume that the $\beta$ ’s have been so chosen and then think backward in time, it follows from stationarity that the best “predictor” of $Y _ { t - k }$ based on the same $Y _ { t - 1 }$ , $Y _ { t - 2 } , \dots$ , $Y _ { t - k + 1 }$ will be $\beta _ { 1 } Y _ { t - k + 1 } +$ $\beta _ { 2 } Y _ { t - k + 2 } + \ldots + \beta _ { k - 1 } Y _ { t - 1 }$ . The partial autocorrelation function at lag $k$ is then defined to be the correlation between the prediction errors; that is,

$$
\phi_ {k k} = \operatorname {C o r r} \left(Y _ {t} - \beta_ {1} Y _ {t - 1} - \beta_ {2} Y _ {t - 2} - \dots - \beta_ {k - 1} Y _ {t - 2}, \right. \tag {6.2.2}
$$

$$
Y _ {t - k} - \beta_ {1} Y _ {t - k + 1} - \beta_ {2} Y _ {t - k + 2} - \dots - \beta_ {k - 1} Y _ {t - 1})
$$

(For normally distributed series, it can be shown that the two definitions coincide.) By convention, we take $\phi _ { 1 1 } = 1$ .

As an example, consider $\phi _ { 2 2 }$ . It is shown in Appendix F on page 218 that the best linear prediction of $Y _ { t }$ based on $Y _ { t - 1 }$ alone is just $\rho _ { 1 } Y _ { t - 1 }$ . Thus, according to Equation (6.2.2), we will obtain $\phi _ { 2 2 }$ by computing

$$
C o v \left(Y _ {t} - \rho_ {1} Y _ {t - 1}, Y _ {t - 2} - \rho_ {1} Y _ {t - 1}\right) = \gamma_ {0} \left(\rho_ {2} - \rho_ {1} ^ {2} - \rho_ {1} ^ {2} + \rho_ {1} ^ {2}\right) = \gamma_ {0} \left(\rho_ {2} - \rho_ {1} ^ {2}\right)
$$

Since

$$
\begin{array}{l} V a r \left(Y _ {t} - \rho_ {1} Y _ {t - 1}\right) = V a r \left(Y _ {t - 2} - \rho_ {1} Y _ {t - 1}\right) \\ = \gamma_ {0} (1 + \rho_ {1} ^ {2} - 2 \rho_ {1} ^ {2}) \\ = \gamma_ {0} (1 - \rho_ {\mathrm {I}} ^ {2}) \\ \end{array}
$$

we have that, for any stationary process, the lag 2 partial autocorrelation can be expressed as

$$
\phi_ {2 2} = \frac {\rho_ {2} - \rho_ {1} ^ {2}}{1 - \rho_ {1} ^ {2}} \tag {6.2.3}
$$

Consider now an AR(1) model. Recall that $\rho _ { k } = \boldsymbol \Phi ^ { k }$ so that

$$
\phi_ {2 2} = \frac {\phi^ {2} - \phi^ {2}}{1 - \phi^ {2}} = 0
$$

We shall soon see that for the AR(1) case, $\Phi _ { k k } = 0$ for all $k > 1$ . Thus the partial autocorrelation is nonzero for lag 1, the order of the AR(1) process, but is zero for all lags greater than 1. We shall show this to be generally the case for $\operatorname { A R } ( p )$ models. Sometimes we say that the partial autocorrelation function for an $\operatorname { A R } ( p )$ process cuts off after the lag exceeds the order of the process.

Consider a general $\operatorname { A R } ( p )$ case. It will be shown in Chapter 9 that the best linear predictor of $Y _ { t }$ based on a linear function of the variables $Y _ { t - 1 } , Y _ { t - 2 } , . . . , Y _ { p } , . . . , Y _ { t - k + 1 }$ for $k > p$ is $\phi _ { 1 } Y _ { t - 1 } + \phi _ { 2 } Y _ { t - 2 } + \ldots + \phi _ { p } Y _ { t - p }$ . Also, the best linear predictor of $Y _ { t - k }$ is some function of $Y _ { t - 1 } , Y _ { t - 2 } , . . . , Y _ { p } , . . . , Y _ { t - k + 1 }$ , call it $h ( Y _ { t - 1 } , Y _ { t - 2 } , . . . , Y _ { p } , . . . , Y _ { t - k + 1 } )$ . So the covariance between the two prediction errors is

$$
\begin{array}{l} C o v (Y _ {t} - \phi_ {1} Y _ {t - 1} - \phi_ {2} Y _ {t - 2} - \dots - \phi_ {p} Y _ {t - p}, \\ Y _ {t - k} - h \left(Y _ {t - k + 1}, Y _ {t - k + 2}, \dots , Y _ {t - 1}\right)) \\ = C o v \left(e _ {t}, Y _ {t - k} - h \left(Y _ {t - k + 1}, Y _ {t - k + 2}, \dots , Y _ {t - 1}\right)\right) \\ = 0 \text {s i n c e} e _ {t} \text {i s i n d e p e n d e n t o f} Y _ {t - k}, Y _ {t - k + 1}, Y _ {t - k + 2}, \dots , Y _ {t - 1} \\ \end{array}
$$

Thus we have established the key fact that, for an $\operatorname { A R } ( p )$ model,

$$
\phi_ {k k} = 0 \text {f o r} k > p \tag {6.2.4}
$$

For an MA(1) model, Equation (6.2.3) quickly yields

$$
\phi_ {2 2} = \frac {- \theta^ {2}}{1 + \theta^ {2} + \theta^ {4}} \tag {6.2.5}
$$

Furthermore, for the MA(1) case, it may be shown that

$$
\phi_ {k k} = - \frac {\theta^ {k} \left(1 - \theta^ {2}\right)}{1 - \theta^ {2 (k + 1)}} \text {f o r} k \geq 1 \tag {6.2.6}
$$

Notice that the partial autocorrelation of an MA(1) model never equals zero but essentially decays to zero exponentially fast as the lag increases—rather like the autocorrelation function of the AR(1) process. More generally, it can be shown that the partial autocorrelation of an $\mathrm { M A } ( q )$ model behaves very much like the autocorrelation of an $\operatorname { A R } ( q )$ model.

A general method for finding the partial autocorrelation function for any stationary process with autocorrelation function $\rho _ { k }$ is as follows (see Anderson 1971, pp. 187–188, for example). For a given lag $k$ , it can be shown that the $\phi _ { k k }$ satisfy the Yule-Walker equations (which first appeared in Chapter 4 on page 79):

$$
\rho_ {j} = \phi_ {k 1} \rho_ {j - 1} + \phi_ {k 2} \rho_ {j - 2} + \phi_ {k 3} \rho_ {j - 3} + \dots + \phi_ {k k} \rho_ {j - k} \quad \text {f o r} j = 1, 2, \dots , k \tag {6.2.7}
$$

More explicitly, we can write these $k$ linear equations as

$$
\left. \begin{array}{c c c} \phi_ {k 1} + & \rho_ {1} \phi_ {k 2} + & \rho_ {2} \phi_ {k 3} + \dots + \rho_ {k - 1} \phi_ {k k} = \rho_ {1} \\ \rho_ {1} \phi_ {k 1} + & \phi_ {k 2} + & \rho_ {1} \phi_ {k 3} + \dots + \rho_ {k - 2} \phi_ {k k} = \rho_ {2} \\ \vdots & & \\ \rho_ {k - 1} \phi_ {k 1} + & \rho_ {k - 2} \phi_ {k 2} + & \rho_ {k - 3} \phi_ {k 3} + \dots + \\ & & \phi_ {k k} = \rho_ {k} \end{array} \right\} \tag {6.2.8}
$$

Here we are treating $\rho _ { 1 }$ , ρ2,…, $\rho _ { k }$ as given and wish to solve for $\phi _ { k 1 }$ , $\Phi _ { k 2 } , . . . , \Phi _ { k k }$ (discarding all but $\boldsymbol { \Phi } _ { k k } )$ ).

These equations yield $\phi _ { k k }$ for any stationary process. However, if the process is in fact $\operatorname { A R } ( p )$ , then since for $k = p$ Equations (6.2.8) are just the Yule-Walker equations (page 79), which the $\operatorname { A R } ( p )$ model is known to satisfy, we must have $\Phi _ { p p } = \Phi _ { p }$ . In addition, as we have already seen by an alternative derivation, $\Phi _ { k k } = 0$ for $k > \bar { p }$ . Thus the partial autocorrelation effectively displays the correct order $p$ of an autoregressive process as the highest lag $k$ before $\Phi _ { k k }$ becomes zero.

# The Sample Partial Autocorrelation Function

For an observed time series, we need to be able to estimate the partial autocorrelation function at a variety of lags. Given the relationships in Equations (6.2.8), an obvious method is to estimate the ρ’s with sample autocorrelations, the corresponding $r \mathrm { { s } }$ , and then solve the resulting linear equations for $k = 1$ , 2, 3,… to get estimates of $\phi _ { k k }$ . We call the estimated function the sample partial autocorrelation function (sample PACF) and denote it by $\hat { \Phi } _ { k k }$ .

Levinson (1947) and Durbin (1960) gave an efficient method for obtaining the solutions to Equations (6.2.8) for either theoretical or sample partial autocorrelations. They showed independently that Equations (6.2.8) can be solved recursively as follows:

$$
\phi_ {k k} = \frac {\rho_ {k} - \sum_ {j = 1} ^ {k - 1} \phi_ {k - 1 , j} \rho_ {k - j}}{1 - \sum_ {j = 1} ^ {k - 1} \phi_ {k - 1 , j} \rho_ {j}} \tag {6.2.9}
$$

where

$$
\phi_ {k, j} = \phi_ {k - 1, j} - \phi_ {k k} \phi_ {k - 1, k - j} \quad \mathrm {f o r} j = 1, 2, \dots , k - 1
$$

For example, using $\Phi _ { 1 1 } = \rho _ { 1 }$ to get started, we have

$$
\phi_ {2 2} = \frac {\rho_ {2} - \phi_ {1 1} \rho_ {1}}{1 - \phi_ {1 1} \rho_ {1}} = \frac {\rho_ {2} - \rho_ {1} ^ {2}}{1 - \rho_ {1} ^ {2}}
$$

(as before) with $\Phi _ { 2 1 } = \Phi _ { 1 1 } - \Phi _ { 2 2 } \Phi _ { 1 1 }$ , which is needed for the next step. Then

$$
\phi_ {3 3} = \frac {\rho_ {3} - \phi_ {2 1} \rho_ {2} - \phi_ {2 2} \rho_ {1}}{1 - \phi_ {2 1} \rho_ {1} - \phi_ {2 2} \rho_ {2}}
$$

We may thus calculate numerically as many values for $\phi _ { k k }$ as desired. As stated, these recursive equations give us the theoretical partial autocorrelations, but by replacing ρ’s with r’s, we obtain the estimated or sample partial autocorrelations.

To assess the possible magnitude of the sample partial autocorrelations, Quenoulle (1949) has shown that, under the hypothesis that an $\operatorname { A R } ( p )$ model is correct, the sample partial autocorrelations at lags greater than $p$ are approximately normally distributed with zero means and variances $1 / n$ . Thus, for $k > p$ , $\pm 2 / \sqrt { n }$ can be used as critical limits on $\hat { \Phi } _ { k k }$ to test the null hypothesis that an $\operatorname { A R } ( p )$ model is correct.

# Mixed Models and the Extended Autocorrelation Function

Exhibit 6.3 summarizes the behavior of the autocorrelation and partial autocorrelation functions that is useful in specifying models.

<table><tr><td colspan="4">Exhibit 6.3 General Behavior of the ACF and PACF for ARMA Models</td></tr><tr><td></td><td>AR(p)</td><td>MA(q)</td><td>ARMA(p, q), p&gt;0, and q&gt;0</td></tr><tr><td>ACF</td><td>Tails off</td><td>Cuts off after lag q</td><td>Tails off</td></tr><tr><td>PACF</td><td>Cuts off after lag p</td><td>Tails off</td><td>Tails off</td></tr></table>

# The Extended Autocorrelation Function

The sample ACF and PACF provide effective tools for identifying pure $\operatorname { A R } ( p )$ or $\mathrm { M A } ( q )$ models. However, for a mixed ARMA model, its theoretical ACF and PACF have infinitely many nonzero values, making it difficult to identify mixed models from the sample ACF and PACF. Many graphical tools have been proposed to make it easier to identify the ARMA orders, for example, the corner method (Becuin et al., 1980), the extended autocorrelation (EACF) method (Tsay and Tiao, 1984), and the smallest canonical correlation (SCAN) method (Tsay and Tiao, 1985), among others. We shall outline the EACF method, which seems to have good sampling properties for moderately large sample sizes according to a comparative simulation study done by W. S. Chan (1999).

The EACF method uses the fact that if the AR part of a mixed ARMA model is known, “filtering out” the autoregression from the observed time series results in a pure MA process that enjoys the cutoff property in its ACF. The AR coefficients may be estimated by a finite sequence of regressions. We illustrate the procedure for the case where the true model is an ARMA(1,1) model:

$$
Y _ {t} = \phi Y _ {t - 1} + e _ {t} - \theta e _ {t - 1}
$$

In this case, a simple linear regression of $Y _ { t }$ on $Y _ { t - 1 }$ results in an inconsistent estimator of $\phi$ , even with infinitely many data. Indeed, the theoretical regression coefficient equals $\mathsf { \rho } _ { 1 } = ( \Phi - \mathsf { \theta } ) ( 1 - \Phi \mathsf { \theta } ) / ( 1 - 2 \Phi \mathsf { \theta } + \mathsf { \theta } ^ { 2 } )$ , not $\boldsymbol { \Phi }$ . But the residuals from this regression do contain information about the error process $\{ \boldsymbol { e } _ { t } \}$ . A second multiple regression is performed that consists of regressing $Y _ { t }$ on $Y _ { t - 1 }$ and on the lag 1 of the residuals from the first regression. The coefficient of $Y _ { t - 1 }$ in the second regression, denoted by $\widetilde { \Phi }$ , turns out to be a consistent estimator of $\phi$ . Define $W _ { t } = Y _ { t } - \widetilde { \Phi } Y _ { t - 1 }$ , which is then approximately an MA(1) process. For an ARMA(1,2) model, a third regression that regresses $Y _ { t }$ on its lag 1, the lag 1 of the residuals from the second regression, and the lag 2 of the residuals from the first regression leads to the coefficient of $Y _ { t - 1 }$ being a consistent estimator of $\boldsymbol { \Phi }$ . Similarly, the AR coefficients of an $\mathbf { A R M A } ( p , q )$ model can be consistently estimated via a sequence of $q$ regressions.

As the AR and MA orders are unknown, an iterative procedure is required. Let

$$
W _ {t, k, j} = Y _ {t} - \widetilde {\phi} _ {1} Y _ {t - 1} - \dots - \widetilde {\phi} _ {k} Y _ {t - k} \tag {6.2.10}
$$

be the autoregressive residuals defined with the AR coefficients estimated iteratively assuming the AR order is $k$ and the MA order is $j$ . The sample autocorrelations of $W _ { t , k , j }$ are referred to as the extended sample autocorrelations. For $k = p$ and $j \geq q$ , $\{ W _ { t , k , j } \}$ is approximately an $\mathrm { M A } ( q )$ model, so that its theoretical autocorrelations of lag $q + 1$ or

higher are equal to zero. For $k > p$ , an overfitting problem occurs, and this increases the MA order for the W process by the minimum of $k - p$ and $j - q$ . Tsay and Tiao (1984) suggested summarizing the information in the sample EACF by a table with the element in the kth row and jth column equal to the symbol X if the $\log j + 1$ sample correlation of $W _ { t , k , j }$ is significantly different from 0 (that is, if its magnitude is greater than $1 . { \dot { 9 } } { \dot { 6 } } / { \sqrt { n - j - k } }$ since the sample autocorrelation is asymptotically $N ( 0 , 1 / ( n - k - j ) )$ if the W’s are approximately an MA(j) process) and 0 otherwise. In such a table, an $\mathrm { M A } ( p , q )$ process will have a theoretical pattern of a triangle of zeroes, with the upper left-hand vertex corresponding to the ARMA orders. Exhibit 6.4 displays the schematic pattern for an ARMA(1,1) model. The upper left-hand vertex of the triangle of zeros is marked with the symbol $0 ^ { * }$ and is located in the $p = 1$ row and $q = 1$ column—an indication of an ARMA(1,1) model.

Exhibit 6.4 Theoretical Extended ACF (EACF) for an ARMA(1,1) Model   

<table><tr><td>AR/MA</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td></tr><tr><td>0</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td></tr><tr><td>1</td><td>x</td><td>0*</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>4</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>5</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>6</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>7</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></table>

Of course, the sample EACF will never be this clear-cut. Displays like Exhibit 6.4 will contain $8 { \times } 1 4 = 1 1 2$ different estimated correlations, and some will be statistically significantly different from zero by chance (see Exhibit 6.17 on page 124, for an example). We will illustrate the use of the EACF in the next two sections and throughout the remainder of the book.

# 6.3 Specification of Some Simulated Time Series

To illustrate the theory of Sections 6.1 and 6.2, we shall consider the sample autocorrelation and sample partial correlation of some simulated time series.

Exhibit 6.5 displays a graph of the sample autocorrelation out to lag 20 for the simulated time series that we first saw in Exhibit 4.5 on page 61. This series, of length 120, was generated from an MA(1) model with $\theta = 0 . 9$ . From Exhibit 4.1 on page 58, the theoretical autocorrelation at lag 1 is −0.4972. The estimated or sample value shown at lag 1 on the graph is −0.474. Using Exhibit 6.2 on page 112, the approximate standard error

of this estimate is $0 . 7 1 / \sqrt { n } = 0 . 7 1 / \sqrt { 1 2 0 } = 0 . 0 6 5$ , so the estimate is well within two standard errors of the true value.

![](images/87ee6f8f0bfcf51cb1f71606f3252a91dad5ae9015eefdfb9790c8f1b95c334b.jpg)  
Exhibit 6.5 Sample Autocorrelation of an MA(1) Process with $\boldsymbol { \theta } = \mathbf { 0 . 9 }$

```julia
> data(mal.1.s)  
> win.graph(width=4.875,height=3,pointsize=8)  
> acf(mal.1.s,xaxp=c(0,20,10)) 
```

The dashed horizontal lines in Exhibit 6.5, plotted at $\pm 2 / \sqrt { n } = \pm 0 . 1 8 2 6$ , are intended to give critical values for testing whether or not the autocorrelation coefficients are significantly different from zero. These limits are based on the approximate large sample standard error that applies to a white noise process, namely $1 \nearrow \sqrt { n }$ . Notice that the sample ACF values exceed these rough critical values at lags 1, 5, and 14. Of course, the true autocorrelations at lags 5 and 14 are both zero.

Exhibit 6.6 displays the same sample ACF but with critical bounds based on plus and minus two of the more complex standard errors implied by Equation (6.1.11) on page 112. In using Equation (6.1.11), we replace ρ’s by r’s, let $q$ equal 1, 2, 3,… successively, and take the square root to obtain these standard errors.

Exhibit 6.6 Alternative Bounds for the Sample ACF for the MA(1) Process   
![](images/b1200d77de530e32c013a9ea4590d1ae0fd364caf591cd96de37a803a879c99a.jpg)  
> acf(ma1.1.s,ci.type='ma',xaxp=c(0,20,10))

Now the sample ACF value at lag 14 is insignificant and the one at lag 5 is just barely significant. The lag 1 autocorrelation is still highly significant, and the information given in these two plots taken together leads us to consider an MA(1) model for this series. Remember that the model is tentative at this point and we would certainly want to consider other “nearby” alternative models when we carry out model diagnostics.

As a second example, Exhibit 6.7 shows the sample ACF for the series shown in Exhibit 4.2 on page 59, generated by an MA(1) model with $\theta = - 0 . 9$ . The critical values based on the very approximate standard errors point to an MA(1) model for this series also.

Exhibit 6.7 Sample Autocorrelation for an MA(1) Process with $\overline { { { \bf \theta } \Theta = - { \bf 0 . 9 } } }$   
![](images/2d3135d1230be8b03d9431ae2c10bcb304459a24cd1a08652874fe17001e6d87.jpg)  
> data(ma1.2.s); acf(ma1.2.s,xaxp=c(0,20,10))

For our third example, we use the data shown in Exhibit 4.8 on page 63, which were simulated from an MA(2) model with $\theta _ { 1 } = 1$ and $\theta _ { 2 } = - 0 . 6$ . The sample ACF displays significance at lags 1, 2, 5, 6, 7, and 14 when we use the simple standard error bounds.

Exhibit 6.8 Sample ACF for an MA(2) Process with ${ \theta } _ { 1 } = 1$ and $\theta _ { 2 } = - 0 . 6$   
![](images/a8d9a71b6ded64e6f5d88310278a07fb018b89d9eccd9a1aa587b7aa5f289209.jpg)  
> data(ma2.s); acf(ma2.s,xaxp=c(0,20,10))

Exhibit 6.9 displays the sample ACF with the more sophisticated standard error bounds. Now the lag 2 ACF is no longer significant, and it appears that an MA(1) may be applicable. We will have to wait until we get further along in the model-building process to see that the MA(2) model—the correct one—is the most appropriate model for these data.

Exhibit 6.9 Alternative Bounds for the Sample ACF for the MA(2) Process   
![](images/8a7b9398ee071835ca567271afd93a04d3be8bdc7fb11482876a352a46c22428.jpg)  
> acf(ma2.s,ci.type='ma',xaxp=c(0,20,10))

How do these techniques work for autoregressive models? Exhibit 6.10 gives the sample ACF for the simulated AR(1) process we saw in Exhibit 4.13 on page 68. The positive sample ACF values at lags 1, 2, and 3 reflect the strength of the lagged relationships that we saw earlier in Exhibits 4.14, 4.15, and 4.16. However, notice that the sample ACF decreases more linearly than exponentially as theory suggests. Also contrary to theory, the sample ACF goes negative at lag 10 and remains so for many lags.

Exhibit 6.10 Sample ACF for an AR(1) Process with $\phi = 0 . 9$   
![](images/4fffc7b957a07a9c76781d916cca18f4b49d7456729b9d92b347ca74ff5976bf.jpg)  
> data(ar1.s); acf(ar1.s,xaxp=c(0,20,10))

The sample partial autocorrelation (PACF) shown in Exhibit 6.11, gives a much clearer picture about the nature of the generating model. Based on this graph, we would certainly entertain an AR(1) model for this time series.

![](images/0dc3b084ba8748b9c166ff02ab2c6f7cca4b654b23d8aff456efccfe9e1043ab.jpg)  
Exhibit 6.11 Sample Partial ACF for an AR(1) Process with $\overline { { \phi = 0 . 9 } }$

> pacf(ar1.s,xaxp=c(0,20,10))

Exhibit 6.12 displays the sample ACF for our AR(2) time series. The time series plot for this series was shown in Exhibit 4.19 on page 74. The sample ACF does look somewhat like the damped wave that Equation (4.3.17) on page 73, and Exhibit 4.18 suggest. However, the sample ACF does not damp down nearly as quickly as theory predicts.

Exhibit 6.12 Sample ACF for an AR(2) Process with $\$ 1.5$ and $\overline { { \Phi _ { 2 } = - { \bf 0 } . 7 5 } }$   
![](images/dfab27c6b5cd8f4540f9788b2a296715f95241885ebbbc240c4df063586e79b8.jpg)  
> acf(ar2.s,xaxp=c(0,20,10))

The sample PACF in Exhibit 6.13 gives a strong indication that we should consider an AR(2) model for these data. The seemingly significant sample PACF at lag 9 would need to be investigated further during model diagnostics.

![](images/0bcdf65a59c089824a0144b5675c84ab8919622c9a9d434a8dd8184f2fde0056.jpg)  
Exhibit 6.13 Sample PACF for an AR(2) Process with $\$ 1.5$ and φ2 = −0.75

```javascript
> pacf(ar2.s,xaxp=c(0,20,10)) 
```

As a final example, we simulated 100 values of a mixed ARMA(1,1) model with $\boldsymbol { \Phi }$ $= 0 . 6$ and $\theta = - 0 . 3$ . The time series plot is shown in Exhibit 6.14 and the sample ACF and PACFs are shown in Exhibit 6.15 and Exhibit 6.16, respectively. These seem to indicate that an AR(1) model should be specified.

![](images/40f15d2ff99492903193b66292b73d056e40fe6824c450fde170822d7f4f9e12.jpg)  
Exhibit 6.14 Simulated ARMA(1,1) Series with $\phi = 0 . 6$ and $\mathbf { \boldsymbol { \Theta } } = - \mathbf { 0 . 3 }$

```txt
> data(arma11.s)  
> plot(arma11.s, type='o', ylab=expression(Y[t])) 
```

![](images/09a7e72f90f6d3c3839efc698f0584cf43ae82616508b15f3755a8cd81da4593.jpg)  
Exhibit 6.15 Sample ACF for Simulated ARMA(1,1) Series

```txt
>acf(arma11.s,xaxp=c(0,20,10)) 
```

Exhibit 6.16 Sample PACF for Simulated ARMA(1,1) Series   
![](images/22aee7ecc63a4b4a81c8620fe1f53ab6e9a2c1b637fd3047f73a524f9df149f7.jpg)  
> pacf(arma11.s,xaxp=c(0,20,10))

However, the triangular region of zeros shown in the sample EACF in Exhibit 6.17 indicates quite clearly that a mixed model with $q = 1$ and with $p = 1$ or 2 would be more appropriate. We will illustrate further uses of the EACF when we specify some real series in Section 6.6.

Exhibit 6.17 Sample EACF for Simulated ARMA(1,1) Series   

<table><tr><td>AR/MA</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td></tr><tr><td>0</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>4</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>5</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>6</td><td>x</td><td>0</td><td>0</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>7</td><td>x</td><td>0</td><td>0</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td colspan="15">&gt; eacf(armal1.s)</td></tr></table>

# 6.4 Nonstationarity

As indicated in Chapter 5, many series exhibit nonstationarity that can be explained by integrated ARMA models. The nonstationarity will frequently be apparent in the time series plot of the series. A review of Exhibits 5.1, 5.5, and 5.8 is recommended here.

The sample ACF computed for nonstationary series will also usually indicate the nonstationarity. The definition of the sample autocorrelation function implicitly assumes stationarity; for example, we use lagged products of deviations from the overall mean, and the denominator assumes a constant variance over time. Thus it is not at all clear what the sample ACF is estimating for a nonstationary process. Nevertheless, for nonstationary series, the sample ACF typically fails to die out rapidly as the lags increase. This is due to the tendency for nonstationary series to drift slowly, either up or down, with apparent “trends.” The values of $r _ { k }$ need not be large even for low lags, but often they are.

Consider the oil price time series shown in Exhibit 5.1 on page 88. The sample ACF for the logarithms of these data is displayed in Exhibit 6.18. All values shown are “significantly far from zero,” and the only pattern is perhaps a linear decrease with increasing lag. The sample PACF (not shown) is also indeterminate.

![](images/fee27aa5b5fce1345240c203db8c6c3722afb26e3a48b5659ebd9fe6550bf5ac.jpg)  
Exhibit 6.18 Sample ACF for the Oil Price Time Series

```txt
> data(oil.price)  
> acf(as.vector(oil.price), xaxp=c(0,24,12)) 
```

The sample ACF computed on the first differences of the logs of the oil price series is shown in Exhibit 6.19. Now the pattern emerges much more clearly—after differencing, a moving average model of order 1 seems appropriate. The model for the original oil price series would then be a nonstationary IMA(1,1) model. (The “significant” ACF at lags 15, 16, and 20 are ignored for now.)

Exhibit 6.19 Sample ACF for the Difference of the Log Oil Price Series   
![](images/e8a9633375eefadd40fa246122cc7c99238842634307f3e2d7f61815cc11b5dc.jpg)  
> acf(diff(as.vector(log(oil.price))),xaxp=c(0,24,12))

If the first difference of a series and its sample ACF do not appear to support a stationary ARMA model, then we take another difference and again compute the sample ACF and PACF to look for characteristics of a stationary ARMA process. Usually one or at most two differences, perhaps combined with a logarithm or other transformation, will accomplish this reduction to stationarity. Additional properties of the sample ACF computed on nonstationary data are given in Wichern (1973), Roy (1977), and Hasza (1980). See also Box, Jenkins, and Reinsel (1994, p. 218).

# Overdifferencing

From Exercise 2.6 on page 20, we know that the difference of any stationary time series is also stationary. However, overdifferencing introduces unnecessary correlations into a series and will complicate the modeling process.

For example, suppose our observed series, $\{ Y _ { t } \}$ , is in fact a random walk so that one difference would lead to a very simple white noise model

$$
\nabla Y _ {t} = Y _ {t} - Y _ {t - 1} = e _ {t}
$$

However, if we difference once more (that is, overdifference) we have

$$
\nabla^ {2} Y _ {t} = e _ {t} - e _ {t - 1}
$$

which is an MA(1) model but with $\theta = 1$ . If we take two differences in this situation we unnecessarily have to estimate the unknown value of θ. Specifying an IMA(2,1) model would not be appropriate here. The random walk model, which can be thought of as IMA(1,1) with $\theta = 0$ , is the correct model.† Overdifferencing also creates a noninvert-

ible model—see Section 4.5 on page 79.† Noninvertible models also create serious problems when we attempt to estimate their parameters—see Chapter 7.

To illustrate overdifferencing, consider the random walk shown in Exhibit 2.1 on page 14. Taking one difference should lead to white noise—a very simple model. If we mistakenly take two differences (that is, overdifference) and compute the sample ACF, we obtain the graph shown in Exhibit 6.20. Based on this plot, we would likely specify at least an IMA(2,1) model for the original series and then estimate the unnecessary MA parameter. We also have a significant sample ACF value at lag 7 to think about and deal with.

![](images/10e2ba600201fd95a0295d23048a765f643c7c222a129ef650b308507c64ae87.jpg)  
Exhibit 6.20 Sample ACF of Overdifferenced Random Walk

> data(rwalk)   
> acf(diff(rwalk,difference $^ { \prime = 2 }$ ),ci.type='ma', xaxp=c(0,18,9))

In contrast, Exhibit 6.21 displays the sample ACF of the first difference of the random walk series. Viewing this graph, we would likely want to consider the correct model—the first difference looks very much like white noise.

Exhibit 6.21 Sample ACF of Correctly Differenced Random Walk   
![](images/41d5a1e48e116051cccf87c50dff5e1ad1ed798599a8495461ccccae58a70e5e.jpg)  
> acf(diff(rwalk),ci.type='ma',xaxp=c(0,18,9))

To avoid overdifferencing, we recommend looking carefully at each difference in succession and keeping the principle of parsimony always in mind—models should be simple, but not too simple.

# The Dickey-Fuller Unit-Root Test

While the approximate linear decay of the sample ACF is often taken as a symptom that the underlying time series is nonstationary and requires differencing, it is also useful to quantify the evidence of nonstationarity in the data-generating mechanism. This can be done via hypothesis testing. Consider the model

$$
Y _ {t} = \alpha Y _ {t - 1} + X _ {t} \mathrm {f o r} t = 1, 2, \dots
$$

where $\{ X _ { t } \}$ is a stationary process. The process $\{ Y _ { t } \}$ is nonstationary if the coefficient $\alpha$ $= 1$ , but it is stationary if $\vert \mathbf { \boldsymbol { a } } \vert < 1$ . Suppose that $\{ X _ { t } \}$ is an $\operatorname { A R } ( k )$ process: $X _ { t } = \Phi _ { 1 } X _ { t - 1 } +$ $\cdots + \Phi _ { k } X _ { t - k } + e _ { t } .$ . Under the null hypothesis that $\alpha = 1$ , $X _ { t } = Y _ { t } - Y _ { t - 1 }$ . Letting $a = \alpha -$ 1, we have

$$
\begin{array}{l} Y _ {t} - Y _ {t - 1} = (\alpha - 1) Y _ {t - 1} + X _ {t} \\ = a Y _ {t - 1} + \phi_ {1} X _ {t - 1} + \dots + \phi_ {k} X _ {t - k} + e _ {t} \tag {6.4.1} \\ = a Y _ {t - 1} + \phi_ {1} (Y _ {t - 1} - Y _ {t - 2}) + \dots + \phi_ {k} (Y _ {t - k} - Y _ {t - k - 1}) + e _ {t} \\ \end{array}
$$

where $a = 0$ under the hypothesis that $Y _ { t }$ is difference nonstationary. On the other hand, if $\{ Y _ { t } \}$ is stationary so that $- 1 < \alpha < 1$ , then it can be verified that $Y _ { t }$ still satisfies an equation similar to the equation above but with different coefficients; for example, $a =$ $( 1 - \Phi _ { 1 } - \dots - \Phi _ { k } ) ( 1 - \alpha ) < 0$ . Indeed, $\{ Y _ { t } \}$ is then an $\mathrm { A R } ( k + 1 )$ process whose AR characteristic equation is given by $\Phi ( x ) ( 1 - \alpha x ) = 0$ , where $\Phi ( x ) = { 1 } ^ { \prime } - \phi _ { 1 } x - \cdots - \phi _ { k } x ^ { k }$ . So, the null hypothesis corresponds to the case where the AR characteristic polynomial has a unit root and the alternative hypothesis states that it has no unit roots. Consequently, the

test for differencing amounts to testing for a unit root in the AR characteristic polynomial of $\{ Y _ { t } \}$ .

By the analysis above, the null hypothesis that $\alpha = 1$ (equivalently $a = 0$ ) can be tested by regressing the first difference of the observed time series on lag 1 of the observed series and on the past $k$ lags of the first difference of the observed series. We then test whether the coefficient $a = 0$ —the null hypothesis being that the process is difference nonstationary. That is, the process is nonstationary but becomes stationary after first differencing. The alternative hypothesis is that $a < 0$ and hence $\{ Y _ { t } \}$ is stationary. The augmented Dickey-Fuller (ADF) test statistic is the $t$ -statistic of the estimated coefficient of $a$ from the method of least squares regression. However, the ADF test statistic is not approximately $t$ -distributed under the null hypothesis; instead, it has a certain nonstandard large-sample distribution under the null hypothesis of a unit root. Fortunately, percentage points of this limit (null) distribution have been tabulated; see Fuller (1996).

In practice, even after first differencing, the process may not be a finite-order AR process, but it may be closely approximated by some AR process with the AR order increasing with the sample size. Said and Dickey (1984) (see also Chang and Park, 2002) showed that with the AR order increasing with the sample size, the ADF test has the same large-sample null distribution as the case where the first difference of the time series is a finite-order AR process. Often, the approximating AR order can be first estimated based on some information criteria (for example, AIC or BIC) before carrying out the ADF test. See Section 6.5 on page 130 for more information on the AIC and BIC criteria.

In some cases, the process may be trend nonstationary in the sense that it has a deterministic trend (for example, some linear trend) but otherwise is stationary. A unit-root test may be conducted with the aim of discerning difference stationarity from trend stationarity. This can be done by carrying out the ADF test with the detrended data. Equivalently, this can be implemented by regressing the first difference on the covariates defining the trend, the lag 1 of the original data, and the past lags of the first difference of the original data. The t-statistic based on the coefficient estimate of the lag 1 of the original data furnishes the ADF test statistic, which has another nonstandard large-sample null distribution. See Phillips and Xiao (1998) for a survey of unit root testing.

We now illustrate the ADF test with the simulated random walk shown in Exhibit 2.1 on page 14. First, we consider testing the null hypothesis of a unit root versus the alternative hypothesis that the time series is stationary with unknown mean. Hence, the regression defined by Equation (6.4.1) is augmented with an intercept to allow for the possibly nonzero mean under the alternative hypothesis. (For the alternative hypothesis that the process is a stationary process of zero mean, the ADF test statistic can be obtained by running the unaugmented regression defined by Equation (6.4.1).) To carry out the test, we must determine $k$ .† Using the AIC with the first difference of the data, we find that $k = 8$ , in which case the ADF test statistic becomes $- 0 . 6 0 1$ , with the $p$ -value

being greater than 0.1.† On the other hand, setting $k = 0$ (the true order) leads to the ADF statistic −1.738, with $p$ -value still greater than 0.1.‡ Thus, there is strong evidence supporting the unit-root hypothesis. Second, recall that the simulated random walk appears to have a linear trend. Hence, linear trend plus stationary error forms another reasonable alternative to the null hypothesis of unit root (difference nonstationarity). For this test, we include both an intercept term and the covariate time in the regression defined by Equation (6.4.1). With $k = 8$ , the ADF test statistic equals $- 2 . 2 8 9$ with $p$ -value greater than $0 . 1 ^ { \dagger \dagger }$ ; that is, we do not reject the null hypothesis of unit root. On the other hand, setting $k = 0$ , the true order that is unknown in practice, the ADF test statistic becomes $- 3 . 4 9$ with $p$ -value equal to 0.0501.‡‡ Hence, there is weak evidence that the process is linear-trend nonstationary; that is, the process equals linear time trend plus stationary error, contrary to the truth that the process is a random walk, being difference nonstationary! This example shows that with a small sample size, it may be hard to differentiate between trend nonstationarity and difference nonstationarity.

# 6.5 Other Specification Methods

A number of other approaches to model specification have been proposed since Box and Jenkins’ seminal work. One of the most studied is Akaike’s (1973) Information Criterion (AIC). This criterion says to select the model that minimizes

$$
\mathrm {A I C} = - 2 \log (\text {m a x i m u m l i k e l i h o o d}) + 2 k \tag {6.5.1}
$$

where $k = p + q + 1$ if the model contains an intercept or constant term and $k = p + q$ otherwise. Maximum likelihood estimation is discussed in Chapter 7. The addition of the term $2 ( p + q + 1 )$ or $2 ( p + q )$ serves as a “penalty function” to help ensure selection of parsimonious models and to avoid choosing models with too many parameters.

The AIC is an estimator of the average Kullback-Leibler divergence of the estimated model from the true model. Let $p ( y _ { 1 } , y _ { 2 } , . . . , y _ { n } )$ be the true pdf of $Y _ { 1 }$ , $Y _ { 2 }$ , …, $Y _ { n }$ , and $q _ { \Theta } ( y _ { 1 } , y _ { 2 } , . . . , y _ { n } )$ be the corresponding pdf under the model with parameter θ. The Kullback-Leibler divergence of $q _ { \theta }$ from $p$ is defined by the formula

$$
D (p, q _ {\theta}) = \int_ {- \infty} ^ {\infty} \int_ {- \infty} ^ {\infty} \dots \int_ {- \infty} ^ {\infty} p (y _ {1}, y _ {2}, \dots , y _ {n}) \log \left[ \frac {p (y _ {1} , y _ {2} , \dots , y _ {n})}{q _ {\theta} (y _ {1} , y _ {2} , \dots , y _ {n})} \right] d y _ {1} d y _ {2} \dots d y _ {n}
$$

The AIC estimates $E [ D ( p , q _ { \hat { \boldsymbol { \theta } } } ) ]$ , where $\widehat { \theta }$ is the maximum likelihood estimator of the vector parameter θ. However, the AIC is a biased estimator, and the bias can be appreciable for large parameter per data ratios. Hurvich and Tsai (1989) showed that the bias can be approximately eliminated by adding another nonstochastic penalty term to the AIC, resulting in the corrected AIC, denoted by $\mathrm { { A I C } _ { c } }$ and defined by the formula

$$
\mathrm {A I C} _ {\mathrm {c}} = \mathrm {A I C} + \frac {2 (k + 1) (k + 2)}{n - k - 2} \tag {6.5.2}
$$

Here $n$ is the (effective) sample size and again $k$ is the total number of parameters as above excluding the noise variance. Simulation results by Hurvich and Tsai (1989) suggest that for cases with $k / n$ greater than $10 \%$ , the $\mathrm { { A I C } _ { c } }$ outperforms many other model selection criteria, including both the AIC and BIC.

Another approach to determining the ARMA orders is to select a model that minimizes the Schwarz Bayesian Information Criterion (BIC) defined as

$$
\mathrm {B I C} = - 2 \log (\text {m a x i m u m l i k e l i h o o d}) + k \log (n) \tag {6.5.3}
$$

If the true process follows an $\mathbf { A R M A } ( p , q )$ model, then it is known that the orders specified by minimizing the BIC are consistent; that is, they approach the true orders as the sample size increases. However, if the true process is not a finite-order ARMA process, then minimizing AIC among an increasingly large class of ARMA models enjoys the appealing property that it will lead to an optimal ARMA model that is closest to the true process among the class of models under study.†

Regardless of whether we use the AIC or BIC, the methods require carrying out maximum likelihood estimation. However, maximum likelihood estimation for an ARMA model is prone to numerical problems due to multimodality of the likelihood function and the problem of overfitting when the AR and MA orders exceed the true orders. Hannan and Rissanen (1982) proposed an interesting and practical solution to this problem. Their procedure consists of first fitting a high-order AR process with the order determined by minimizing the AIC. The second step uses the residuals from the first step as proxies for the unobservable error terms. Thus, an ARMA $( k , j )$ model can be approximately estimated by regressing the time series on its own lags 1 to $k$ together with the lags 1 to $j$ of the residuals from the high order autoregression; the BIC of this autoregressive model is an estimate of the BIC obtained with maximum likelihood estimation. Hannan and Rissanen (1982) demonstrated that minimizing the approximate BIC still leads to consistent estimation of the ARMA orders.

Order determination is related to the problem of finding the subset of nonzero coefficients of an ARMA model with sufficiently high ARMA orders. A subset ARMA $( p , q )$ model is an ARMA $( p , q )$ model with a subset of its coefficients known to be zero. For example, the model

$$
Y _ {t} = 0. 8 Y _ {t - 1 2} + e _ {t} + 0. 7 e _ {t - 1 2} \tag {6.5.4}
$$

is a subset ARMA(12,12) model useful for modeling some monthly seasonal time series. For ARMA models of very high orders, such as the preceding ARMA(12,12) model, finding a subset ARMA model that adequately approximates the underlying process is more important from a practical standpoint than simply determining the ARMA orders. The method of Hannan and Rissanen (1982) for estimating the ARMA orders can be extended to solving the problem of finding an optimal subset ARMA model.

Indeed, several model selection criteria (including AIC and BIC) of the subset $\mathbf { A R M A } ( p , q )$ models $2 ^ { p + q }$ of them!) can be approximately, exhaustively, and quickly computed by the method of regression by leaps and bounds (Furnival and Wilson, 1974) applied to the subset regression of $Y _ { t }$ on its own lags and on lags of the residuals from a high-order autoregression of $\{ Y _ { t } \}$ .

It is prudent to examine a few best subset ARMA models (in terms of, for example, BIC) in order to arrive at some helpful tentative models for further study. The pattern of which lags of the observed time series and which of the error process enter into the various best subset models can be summarized succinctly in a display like that shown in Exhibit 6.22. This table is based on a simulation of the ARMA(12,12) model shown in Equation (6.5.4). Each row in the exhibit corresponds to a subset ARMA model where the cells of the variables selected for the model are shaded. The models are sorted according to their BIC, with better models (lower BIC) placed in higher rows and with darker shades. The top row tells us that the subset ARMA(14,14) model with the smallest BIC contains only lags 8 and 12 of the observed time series and lag 12 of the error process. The next best model contains lag 12 of the time series and lag 8 of the errors, while the third best model contains lags 4, 8, and 12 of the time series and lag 12 of the errors. In our simulated time series, the second best model is the true subset model. However, the BIC values for these three models are all very similar, and all three (plus the fourth best model) are worthy of further study. However, lag 12 of the time series and that of the errors are the two variables most frequently found in the various subset models summarized in the exhibit, suggesting that perhaps they are the more important variables, as we know they are!

![](images/552d79b1d2397721398efd62677832adf756cb55c3291154773cec8b1251af3a.jpg)  
Exhibit 6.22 Best Subset ARMA Selection Based on BIC

```python
> set.seed(92397)  
> test=arima.sim(model=list(ar=c(rep(0,11),.8), ma=c(rep(0,11),0.7)),n=120)  
> res=armasubsets(y/test,nar=14,nma=14,y.name='test', ar.method='ols')  
> plot(res) 
```

# 6.6 Specification of Some Actual Time Series

Consider now specification of models for some of the actual time series that we saw in earlier chapters.

# The Los Angeles Annual Rainfall Series

Annual total rainfall amounts for Los Angeles were shown in Exhibit 1.1 on page 2. In Chapter 3, we noted in Exhibit 3.17 on page 50, that rainfall amounts were not normally distributed. As is shown in Exhibit 6.23, taking logarithms improves the normality dramatically.

![](images/6b167ce103a5fa9d38cf2d62663d037e9a00446c4e4c4d31391110393b95ee09.jpg)  
Exhibit 6.23 QQ Normal Plot of the Logarithms of LA Annual Rainfall   
Theoretical Quantiles

```javascript
> data(larain); win.graph(width=2.5,height=2.5,pointsize=8) 
```

```txt
> qqnorm(log(larain)); qqline(log(larain)) 
```

Exhibit 6.24 displays the sample autocorrelations for the logarithms of the annual rainfall series.

Exhibit 6.24 Sample ACF of the Logarithms of LA Annual Rainfall   
![](images/a4002bba04464bd75bef92417d7fdc8e92d674cf59a5abfa2c1eba57a44ffed4.jpg)  
> win.graph(width=4.875,height $^ { = 3 }$ ,pointsize=8)   
> acf(log(larain),xaxp=c(0,20,10))

The log transformation has improved the normality, but there is no discernable dependence in this time series. We could model the logarithm of annual rainfall amount as independent, normal random variables with mean 2.58 and standard deviation 0.478. Both these values are in units of log(inches).

# The Chemical Process Color Property Series

The industrial chemical process color property displayed in Exhibit 1.3 on page 3, shows more promise of interesting time series modeling—especially in light of the dependence of successive batches shown in Exhibit 1.4 on page 4. The sample ACF plotted in Exhibit 6.25 might at first glance suggest an MA(1) model, as only the lag 1 autocorrelation is significantly different from zero.

Exhibit 6.25 Sample ACF for the Color Property Series   
![](images/55a590363f78e33cd21e6209a33e9922eefc56c5d6dab33ff099c1d956aa32ad.jpg)  
> data(color); acf(color,ci.type='ma')

However, the damped sine wave appearance of the plot encourages us to look further at the sample partial autocorrelation. Exhibit 6.26 displays that plot, and now we see clearly that an AR(1) model is worthy of first consideration. As always, our specified models are tentative and subject to modification during the model diagnostics stage of model building.

Exhibit 6.26 Sample Partial ACF for the Color Property Series   
![](images/0f9986b14c3c21ed91bb2e6ad05c93c1ecfc6f5823c832234eef7359f32fdb76.jpg)  
> pacf(color)

# The Annual Abundance of Canadian Hare Series

The time series of annual abundance of hare of the Hudson Bay in Canada was displayed in Exhibit 1.5 on page 5, and the year-to-year dependence was demonstrated in Exhibit 1.6. It has been suggested in the literature that a transformation might be used to produce a good model for these data. Exhibit 6.27 displays the log-likelihood as a function of the power parameter, λ. The maximum occurs at $\lambda = 0 . 4$ , but a square root transformation with $\lambda = 0 . 5$ is well within the confidence interval for $\gimel$ . We will take the square root of the abundance values for all further analyses.

Exhibit 6.27 Box-Cox Power Transformation Results for Hare Abundance   
![](images/7f781c2231b2f81ec4d023f45b0a02731f03fe16aecbacd826eb06f25ff7494e.jpg)  
> win.graph(width=3,height=3,pointsize=8)   
> data(hare); BoxCox.ar(hare)

Exhibit 6.28 shows the sample ACF for this transformed series. The fairly strong lag 1 autocorrelation dominates but, again, there is a strong indication of damped oscillatory behavior.

![](images/a72e64fdb02f87bb6220489f6d280c0283b224e599b59e25f95ae4e0106aa479.jpg)  
Exhibit 6.28 Sample ACF for Square Root of Hare Abundance

> acf(hare^.5)

The sample partial autocorrelation for the transformed series is shown in Exhibit 6.29. It gives strong evidence to support an AR(2) or possibly an AR(3) model for these data.

![](images/56ac2040843a4abf39e42962ad07cf3c9003c0455d9aa2ac45b86510fa13dfbd.jpg)  
Exhibit 6.29 Sample Partial ACF for Square Root of Hare Abundance

> pacf(hare^.5)

# The Oil Price Series

In Chapter 5, we began to look at the monthly oil price time series and argued graphically that the difference of the logarithms could be considered stationary—see Exhibit 5.1 on page 88. Software implementation of the Augmented Dickey-Fuller unit-root test applied to the logs of the original prices leads to a test statistic of −1.1119 and a $p$ -value of 0.9189. With stationarity as the alternative hypothesis, this provides strong evidence of nonstationarity and the appropriateness of taking a difference of the logs. For this test, the software chose a value of $k = 6$ in Equation (6.4.1) on page 128 based on large-sample theory.

Exhibit 6.30 shows the summary EACF table for the differences of the logarithms of the oil price data. This table suggests an ARMA model with $p = 0$ and $q = 1$ .

<table><tr><td colspan="2">Exhibit 6.30</td><td colspan="13">Extended ACF for Difference of Logarithms of Oil Price Series</td></tr><tr><td>AR / MA</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td></tr><tr><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>4</td><td>0</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>5</td><td>0</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>6</td><td>0</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>7</td><td>x</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></table>

> eacf(diff(log(oil.price)))

The results of the best subsets ARMA approach are displayed in Exhibit 6.31.

![](images/2058a50f7377d767a2b828dc7f5b23ba964b17de76e5440a6e5f6abf564c125a.jpg)  
Exhibit 6.31 Best Subset ARMA Model for Difference of Log(Oil)

```python
> res=armasubsets(y=diff(log(oil.price)), nar=7, nma=7, y.name='test', ar.method='ols')
> plot(res) 
```

Here the suggestion is that $Y _ { t } = \nabla { \log } ( \mathrm { O i l } _ { t } )$ should be modeled in terms of $Y _ { t - 1 }$ and $Y _ { t - 4 }$ and that no lags are needed in the error terms. The second best model omits the lag 4 term so that an ARIMA(1,1,0) model on the logarithms should also be investigated further.

Exhibit 6.32 suggests that we specify an MA(1) model for the difference of the log oil prices, and Exhibit 6.33 says to consider an AR(2) model (ignoring some significant spikes at lags 15, 16, and 20). We will want to look at all of these models further when we estimate parameters and perform diagnostic tests in Chapters 7 and 8. (We will see later that to obtain a suitable model for the oil price series, the outliers in the series will need to be dealt with. (Can you spot the outliers in Exhibit 5.4 on page 91?)

Exhibit 6.32 Sample ACF of Difference of Logged Oil Prices   
![](images/74aa9bb98365698f06e250b4e12d007c03e54685cec21b59a93cc3978e01019b.jpg)  
> acf(as.vector(diff(log(oil.price))),xaxp=c(0,22,11))

Exhibit 6.33 Sample PACF of Difference of Logged Oil Prices   
![](images/213d5123a593e676a5596b7f9dc589d77567632d5d33993eefe90a646dc238c8.jpg)  
> pacf(as.vector(diff(log(oil.price))),xaxp=c(0,22,11))

# 6.7 Summary

In this chapter, we considered the problem of specifying reasonable but simple models for observed times series. In particular, we investigated tools for choosing the orders $( p$ , $d$ , and $q$ ) for $\mathrm { A R I M A } ( p , d , q )$ models. Three tools, the sample autocorrelation function, the sample partial autocorrelation function, and the sample extended autocorrelation function, were introduced and studied to help with this difficult task. The Dickey-Fuller unit-root test was also introduced to help distinguish between stationary and nonstationary series. These ideas were all illustrated with both simulated and actual time series.

# EXERCISES

6.1 Verify Equation (6.1.3) on page 110 for the white noise process.   
6.2 Verify Equation (6.1.4) on page 110 for the AR(1) process.   
6.3 Verify the line in Exhibit 6.1 on page 111, for the values $\phi = \pm 0 . 9$   
6.4 Add new entries to Exhibit 6.1 on page 111, for the following values:

(a) $\Phi = \pm 0 . 9 9$   
(b) $\phi = \pm 0 . 5$   
(c) $\phi = \pm 0 . 1$

6.5 Verify Equation (6.1.9) on page 111 and Equation (6.1.10) for the MA(1) process.   
6.6 Verify the line in Exhibit 6.2 on page 112, for the values $\theta = \pm 0 . 9$   
6.7 Add new entries to Exhibit 6.2 on page 112, for the following values:

(a) $\theta = \pm 0 . 9 9$   
(b) $\theta = \pm 0 . 8$   
(c) $\theta = \pm 0 . 2$

6.8 Verify Equation (6.1.11) on page 112, for the general $\mathrm { M A } ( q )$ process.   
6.9 Use Equation (6.2.3) on page 113, to verify the value for the lag 2 partial autocorrelation function for the MA(1) process given in Equation (6.2.5) on page 114.   
6.10 Show that the general expression for the partial autocorrelation function of an MA(1) process given in Equation (6.2.6) on page 114, satisfies the Yule-Walker recursion given in Equation (6.2.7).   
6.11 Use Equation (6.2.8) on page 114, to find the (theoretical) partial autocorrelation function for an AR(2) model in terms of $\Phi _ { 1 }$ and $\Phi _ { 2 }$ and lag $k = 1$ , 2, 3, … .   
6.12 From a time series of 100 observations, we calculate $r _ { 1 } = - 0 . 4 9$ , $r _ { 2 } = 0 . 3 1$ , $r _ { 3 } = { }$ −0.21, $r _ { 4 } = 0 . 1 1$ , and $\left. r _ { k } \right. < 0 . 0 9$ for $k > 4$ . On this basis alone, what ARIMA model would we tentatively specify for the series?   
6.13 A stationary time series of length 121 produced sample partial autocorrelation of $\widehat { \Phi } _ { 1 1 } = 0 . 8$ , $\hat { \Phi } _ { 2 2 } = - 0 . 6 , \hat { \Phi } _ { 3 3 } = \bar { 0 } . 0 8$ , and $\hat { \Phi } _ { 4 4 } = 0 . 0 0$ . Based on this information alone, what model would we tentatively specify for the series?   
6.14 For a series of length 169, we find that $r _ { 1 } = 0 . 4 1$ , $r _ { 2 } = 0 . 3 2$ , $r _ { 3 } = 0 . 2 6$ , $r _ { 4 } = 0 . 2 1$ and $r _ { 5 } = 0 . 1 6$ . What ARIMA model fits this pattern of autocorrelations?

6.15 The sample ACF for a series and its first difference are given in the following table. Here $n = 1 0 0$ .

<table><tr><td>lag</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td></tr><tr><td>ACF for Yt</td><td>0.97</td><td>0.97</td><td>0.93</td><td>0.85</td><td>0.80</td><td>0.71</td></tr><tr><td>ACF for ∇Yt</td><td>-0.42</td><td>0.18</td><td>-0.02</td><td>0.07</td><td>-0.10</td><td>-0.09</td></tr></table>

Based on this information alone, which ARIMA model(s) would we consider for the series?

6.16 For a series of length 64, the sample partial autocorrelations are given as:

<table><tr><td>Lag</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>PACF</td><td>0.47</td><td>-0.34</td><td>0.20</td><td>0.02</td><td>-0.06</td></tr></table>

Which models should we consider in this case?

6.17 Consider an AR(1) series of length 100 with $\phi = 0 . 7$ .

(a) Would you be surprised if $r _ { 1 } = 0 . 6 \ ?$   
(b) Would $r _ { 1 0 } = - 0 . 1 5$ be unusual?

6.18 Suppose the $\{ X _ { t } \}$ is a stationary AR(1) process with parameter $\boldsymbol { \Phi }$ but that we can only observe $Y _ { t } = X _ { t } + N _ { t }$ where $\{ N _ { t } \}$ is the white noise measurement error independent of $\{ X _ { t } \}$ .

(a) Find the autocorrelation function for the observed process in terms of $\phi$ , $\sigma _ { X } ^ { 2 }$ and $\sigma _ { N } ^ { 2 }$ .   
(b) Which ARIMA model might we specify for $\{ Y _ { t } \}$ ?

6.19 The time plots of two series are shown below.

(a) For each of the series, describe $r _ { 1 }$ using the terms strongly positive, moderately positive, near zero, moderately negative, or strongly negative. Do you need to know the scale of measurement for the series to answer this?   
(b) Repeat part (a) for $r _ { 2 }$

![](images/68af93944c585c48aeaec634199c0b49eff5b1b80b6637152202797a228215cb.jpg)  
Series A   
Time   
Series B

![](images/26a0ee31d041ffb21cd817173206ba8d133269b7558a36ed9b565d5c44569655.jpg)  
Time

6.20 Simulate an AR(1) time series with $n = 4 8$ and with $\phi = 0 . 7$ .

(a) Calculate the theoretical autocorrelations at lag 1 and lag 5 for this model.   
(b) Calculate the sample autocorrelations at lag 1 and lag 5 and compare the values with their theoretical values. Use Equations (6.1.5) and (6.1.6) page 111, to quantify the comparisons.   
(c) Repeat part (b) with a new simulation. Describe how the precision of the estimate varies with different samples selected under identical conditions.   
(d) If software permits, repeat the simulation of the series and calculation of $r _ { 1 }$ and $r _ { 5 }$ many times and form the sampling distributions of $r _ { 1 }$ and $r _ { 5 }$ . Describe how the precision of the estimate varies with different samples selected under identical conditions. How well does the large-sample variance given in Equation (6.1.5) on page 111, approximate the variance in your sampling distribution?

6.21 Simulate an MA(1) time series with $n = 6 0$ and with $\theta = 0 . 5$ .

(a) Calculate the theoretical autocorrelation at lag 1 for this model.   
(b) Calculate the sample autocorrelation at lag 1, and compare the value with its theoretical value. Use Exhibit 6.2 on page 112, to quantify the comparisons.   
(c) Repeat part (b) with a new simulation. Describe how the precision of the estimate varies with different samples selected under identical conditions.   
(d) If software permits, repeat the simulation of the series and calculation of $r _ { 1 }$ many times and form the sampling distribution of $r _ { 1 }$ . Describe how the precision of the estimate varies with different samples selected under identical conditions. How well does the large-sample variance given in Exhibit 6.2 on page 112, approximate the variance in your sampling distribution?

6.22 Simulate an AR(1) time series with $n = 4 8$ , with

(a) $\phi = 0 . 9$ , and calculate the theoretical autocorrelations at lag 1 and lag 5;   
(b) $\phi = 0 . 6$ , and calculate the theoretical autocorrelations at lag 1 and lag 5;   
(c) $\phi = 0 . 3$ , and calculate the theoretical autocorrelations at lag 1 and lag 5.   
(d) For each of the series in parts (a), (b), and (c), calculate the sample autocorrelations at lag 1 and lag 5 and compare the values with their theoretical values. Use Equations (6.1.5) and 6.1.6, page 111, to quantify the comparisons. In general, describe how the precision of the estimate varies with the value of $\boldsymbol { \Phi }$ .

6.23 Simulate an AR(1) time series with $\phi = 0 . 6$ , with

(a) $n = 2 4$ , and estimate $\rho _ { 1 } = \phi = 0 . 6$ with $r _ { 1 }$ ;   
(b) $n = 6 0$ , and estimate $\rho _ { 1 } = \phi = 0 . 6$ with $r _ { 1 }$ ;   
(c) $n = 1 2 0$ , and estimate $\rho _ { 1 } = \phi = 0 . 6$ with $r _ { 1 }$   
(d) For each of the series in parts (a), (b), and (c), compare the estimated values with the theoretical value. Use Equation (6.1.5) on page 111, to quantify the comparisons. In general, describe how the precision of the estimate varies with the sample size.

6.24 Simulate an MA(1) time series with $\theta = 0 . 7$ , with

(a) $n = 2 4$ , and estimate $\rho _ { 1 }$ with $r _ { 1 }$ ;   
(b) $n = 6 0$ , and estimate $\rho _ { 1 }$ with $r _ { 1 }$ ;   
(c) $n = 1 2 0$ , and estimate $\rho _ { 1 }$ with $r _ { 1 }$   
(d) For each of the series in parts (a), (b), and (c), compare the estimated values of $\rho _ { 1 }$ with the theoretical value. Use Exhibit 6.2 on page 112, to quantify the comparisons. In general, describe how the precision of the estimate varies with the sample size.

6.25 Simulate an AR(1) time series of length $n = 3 6$ with $\phi = 0 . 7$ .

(a) Calculate and plot the theoretical autocorrelation function for this model. Plot sufficient lags until the correlations are negligible.   
(b) Calculate and plot the sample ACF for your simulated series. How well do the values and patterns match the theoretical ACF from part (a)?   
(c) What are the theoretical partial autocorrelations for this model?   
(d) Calculate and plot the sample ACF for your simulated series. How well do the values and patterns match the theoretical ACF from part (a)? Use the large-sample standard errors reported in Exhibit 6.1 on page 111, to quantify your answer.   
(e) Calculate and plot the sample PACF for your simulated series. How well do the values and patterns match the theoretical PACF from part (c)? Use the large-sample standard errors reported on page 115 to quantify your answer.

6.26 Simulate an MA(1) time series of length $n = 4 8$ with $\theta = 0 . 5$ .

(a) What are the theoretical autocorrelations for this model?   
(b) Calculate and plot the sample ACF for your simulated series. How well do the values and patterns match the theoretical ACF from part (a)?   
(c) Calculate and plot the theoretical partial autocorrelation function for this model. Plot sufficient lags until the correlations are negligible. (Hint: See Equation (6.2.6) on page 114.)   
(d) Calculate and plot the sample PACF for your simulated series. How well do the values and patterns match the theoretical PACF from part (c)?

6.27 Simulate an AR(2) time series of length $n = 7 2$ with $\phi _ { 1 } = 0 . 7$ and $\phi _ { 2 } = - 0 . 4$

(a) Calculate and plot the theoretical autocorrelation function for this model. Plot sufficient lags until the correlations are negligible.   
(b) Calculate and plot the sample ACF for your simulated series. How well do the values and patterns match the theoretical ACF from part (a)?   
(c) What are the theoretical partial autocorrelations for this model?   
(d) Calculate and plot the sample ACF for your simulated series. How well do the values and patterns match the theoretical ACF from part (a)?   
(e) Calculate and plot the sample PACF for your simulated series. How well do the values and patterns match the theoretical PACF from part (c)?

6.28 Simulate an MA(2) time series of length $n = 3 6$ with $\theta _ { 1 } = 0 . 7$ and $\theta _ { 2 } = - 0 . 4$ .

(a) What are the theoretical autocorrelations for this model?   
(b) Calculate and plot the sample ACF for your simulated series. How well do the values and patterns match the theoretical ACF from part (a)?   
(c) Plot the theoretical partial autocorrelation function for this model. Plot sufficient lags until the correlations are negligible. (We do not have a formula for this PACF. Instead, perform a very large sample simulation, say $n = 1 0 0 0$ , for this model and calculate and plot the sample PACF for this simulation.)   
(d) Calculate and plot the sample PACF for your simulated series of part (a). How well do the values and patterns match the “theoretical” PACF from part (c)?

6.29 Simulate a mixed ARMA(1,1) model of length $n = 6 0$ with $\phi = 0 . 4$ and $\theta = 0 . 6$ .

(a) Calculate and plot the theoretical autocorrelation function for this model. Plot sufficient lags until the correlations are negligible.   
(b) Calculate and plot the sample ACF for your simulated series. How well do the values and patterns match the theoretical ACF from part (a)?   
(c) Calculate and interpret the sample EACF for this series. Does the EACF help you specify the correct orders for the model?   
(d) Repeat parts (b) and (c) with a new simulation using the same parameter values and sample size.   
(e) Repeat parts (b) and (c) with a new simulation using the same parameter values but sample size $n = 3 6$ .   
(f) Repeat parts (b) and (c) with a new simulation using the same parameter values but sample size $n = 1 2 0$ .

6.30 Simulate a mixed ARMA(1,1) model of length $n = 1 0 0$ with $\phi = 0 . 8$ and $\theta = 0 . 4$

(a) Calculate and plot the theoretical autocorrelation function for this model. Plot sufficient lags until the correlations are negligible.   
(b) Calculate and plot the sample ACF for your simulated series. How well do the values and patterns match the theoretical ACF from part (a)?   
(c) Calculate and interpret the sample EACF for this series. Does the EACF help you specify the correct orders for the model?   
(d) Repeat parts (b) and (c) with a new simulation using the same parameter values and sample size.   
(e) Repeat parts (b) and (c) with a new simulation using the same parameter values but sample size $n = 4 8$ .   
(f) Repeat parts (b) and (c) with a new simulation using the same parameter values but sample size $n = 2 0 0$ .

6.31 Simulate a nonstationary time series with $n ~ = ~ 6 0$ according to the model ARIMA(0,1,1) with $\theta = 0 . 8$ .

(a) Perform the (augmented) Dickey-Fuller test on the series with $k = 0$ in Equation (6.4.1) on page 128. (With $k = 0$ , this is the Dickey-Fuller test and is not augmented.) Comment on the results.   
(b) Perform the augmented Dickey-Fuller test on the series with $k$ chosen by the software—that is, the “best” value for $k$ . Comment on the results.   
(c) Repeat parts (a) and (b) but use the differences of the simulated series. Comment on the results. (Here, of course, you should reject the unit root hypothesis.)

6.32 Simulate a stationary time series of length $n = 3 6$ according to an AR(1) model with $\phi = 0 . 9 5$ . This model is stationary, but just barely so. With such a series and a short history, it will be difficult if not impossible to distinguish between stationary and nonstationary with a unit root.

(a) Plot the series and calculate the sample ACF and PACF and describe what you see.   
(b) Perform the (augmented) Dickey-Fuller test on the series with $k = 0$ in Equation (6.4.1) on page 128. (With $k = 0$ this is the Dickey-Fuller test and is not augmented.) Comment on the results.   
(c) Perform the augmented Dickey-Fuller test on the series with $k$ chosen by the software—that is, the “best” value for $k$ . Comment on the results.   
(d) Repeat parts (a), (b), and (c) but with a new simulation with $n = 1 0 0$ .

6.33 The data file named deere1 contains 82 consecutive values for the amount of deviation (in 0.000025 inch units) from a specified target value that an industrial machining process at Deere & Co. produced under certain specified operating conditions.

(a) Display the time series plot of this series and comment on any unusual points.   
(b) Calculate the sample ACF for this series and comment on the results.   
(c) Now replace the unusual value by a much more typical value and recalculate the sample ACF. Comment on the change from what you saw in part (b).   
(d) Calculate the sample PACF based on the revised series that you used in part (c). What model would you specify for the revised series? (Later we will investigate other ways to handle outliers in time series modeling.)

6.34 The data file named deere2 contains 102 consecutive values for the amount of deviation (in 0.0000025 inch units) from a specified target value that another industrial machining process produced at Deere & Co.

(a) Display the time series plot of this series and comment on its appearance. Would a stationary model seem to be appropriate?   
(b) Display the sample ACF and PACF for this series and select tentative orders for an ARMA model for the series.

6.35 The data file named deere3 contains 57 consecutive measurements recorded from a complex machine tool at Deere & Co. The values given are deviations from a target value in units of ten millionths of an inch. The process employs a control mechanism that resets some of the parameters of the machine tool depending on the magnitude of deviation from target of the last item produced.

(a) Display the time series plot of this series and comment on its appearance. Would a stationary model be appropriate here?   
(b) Display the sample ACF and PACF for this series and select tentative orders for an ARMA model for the series.

6.36 The data file named robot contains a time series obtained from an industrial robot. The robot was put through a sequence of maneuvers, and the distance from a desired ending point was recorded in inches. This was repeated 324 times to form the time series.

(a) Display the time series plot of the data. Based on this information, do these data appear to come from a stationary or nonstationary process?   
(b) Calculate and plot the sample ACF and PACF for these data. Based on this additional information, do these data appear to come from a stationary or nonstationary process?   
(c) Calculate and interpret the sample EACF.   
(d) Use the best subsets ARMA approach to specify a model for these data. Compare these results with what you discovered in parts (a), (b), and (c).

6.37 Calculate and interpret the sample EACF for the logarithms of the Los Angeles rainfall series. The data are in the file named larain. Do the results confirm that the logs are white noise?

6.38 Calculate and interpret the sample EACF for the color property time series. The data are in the color file. Does the sample EACF suggest the same model that was specified by looking at the sample PACF?

6.39 The data file named days contains accounting data from the Winegard Co. of Burlington, Iowa. The data are the number of days until Winegard receives payment for 130 consecutive orders from a particular distributor of Winegard products. (The name of the distributor must remain anonymous for confidentiality reasons.)

(a) Plot the time series, and comment on the display. Are there any unusual values?   
(b) Calculate the sample ACF and PACF for this series.   
(c) Now replace each of the unusual values with a value of 35 days—much more typical values—and repeat the calculation of the sample ACF and PACF. What ARMA model would you specify for this series after removing the outliers? (Later we will investigate other ways to handle outliers in time series modeling.)

# CHAPTER 7

# PARAMETER ESTIMATION

This chapter deals with the problem of estimating the parameters of an ARIMA model based on the observed time series $Y _ { 1 }$ , $Y _ { 2 }$ ,…, $Y _ { n }$ . We assume that a model has already been specified; that is, we have specified values for p, d, and $q$ using the methods of Chapter 6. With regard to nonstationarity, since the $d$ th difference of the observed series is assumed to be a stationary $\mathbf { A R M A } ( p , q )$ process, we need only concern ourselves with the problem of estimating the parameters in such stationary models. In practice, then we treat the dth difference of the original time series as the time series from which we estimate the parameters of the complete model. For simplicity, we shall let $Y _ { 1 }$ , $Y _ { 2 }$ ,…, $Y _ { n }$ denote our observed stationary process even though it may be an appropriate difference of the original series. We first discuss the method-of-moments estimators, then the least squares estimators, and finally full maximum likelihood estimators.

# 7.1 The Method of Moments

The method of moments is frequently one of the easiest, if not the most efficient, methods for obtaining parameter estimates. The method consists of equating sample moments to corresponding theoretical moments and solving the resulting equations to obtain estimates of any unknown parameters. The simplest example of the method is to estimate a stationary process mean by a sample mean. The properties of this estimator were studied extensively in Chapter 3.

# Autoregressive Models

Consider first the AR(1) case. For this process, we have the simple relationship ${ \rho } _ { 1 } = \Phi$ . In the method of moments, $\rho _ { 1 }$ is equated to $r _ { 1 }$ , the lag 1 sample autocorrelation. Thus we can estimate $\phi$ by

$$
\hat {\phi} = r _ {1} \tag {7.1.1}
$$

Now consider the AR(2) case. The relationships between the parameters $\Phi _ { 1 }$ and $\Phi _ { 2 }$ and various moments are given by the Yule-Walker equations (4.3.13) on page 72:

$$
\rho_ {1} = \phi_ {1} + \rho_ {1} \phi_ {2} a n d \rho_ {2} = \rho_ {1} \phi_ {1} + \phi_ {2}
$$

The method of moments replaces $\rho _ { 1 }$ by $r _ { 1 }$ and $\rho _ { 2 }$ by $r _ { 2 }$ to obtain

$$
r _ {1} = \phi_ {1} + r _ {1} \phi_ {2} \mathrm {a n d} r _ {2} = r _ {1} \phi_ {1} + \phi_ {2}
$$

which are then solved to obtain

$$
\hat {\phi} _ {1} = \frac {r _ {1} \left(1 - r _ {2}\right)}{1 - r _ {1} ^ {2}} \text {a n d} \hat {\phi} _ {2} = \frac {r _ {2} - r _ {1} ^ {2}}{1 - r _ {1} ^ {2}} \tag {7.1.2}
$$

The general $\mathbf { A R } ( p )$ case proceeds similarly. Replace $\rho _ { k }$ by $r _ { k }$ throughout the Yule-Walker equations on page 79 (or page 114) to obtain

$$
\left. \begin{array}{c c c c} \phi_ {1} + & r _ {1} \phi_ {2} + & r _ {2} \phi_ {3} & + \dots + r _ {p - 1} \phi_ {p} = r _ {1} \\ r _ {1} \phi_ {1} + & \phi_ {2} + & r _ {1} \phi_ {3} & + \dots + r _ {p - 2} \phi_ {p} = r _ {2} \\ & & & \vdots \\ r _ {p - 1} \phi_ {1} + & r _ {p - 2} \phi_ {2} + & r _ {p - 3} \phi_ {3} & + \dots + \quad \phi_ {p} = r _ {p} \end{array} \right\} \tag {7.1.3}
$$

These linear equations are then solved for $\hat { \Phi } _ { 1 } , \hat { \Phi } _ { 2 } , . . . , \hat { \Phi } _ { p }$ . The Durbin-Levinson recursion of Equation (6.2.9) on page 115 provides a convenient method of solution but is subject to substantial round-off errors if the solution is close to the boundary of the stationarity region. The estimates obtained in this way are also called Yule-Walker estimates.

# Moving Average Models

Surprisingly, the method of moments is not nearly as convenient when applied to moving average models. Consider the simple MA(1) case. From Equations (4.2.2) on page 57, we know that

$$
\rho_ {1} = - \frac {\theta}{1 + \theta^ {2}}
$$

Equating $\rho _ { 1 }$ to $r _ { 1 }$ , we are led to solve a quadratic equation in θ. If $| r _ { 1 } | < 0 . 5$ , then the two real roots are given by

$$
- \frac {1}{2 r _ {1}} \pm \sqrt {\frac {1}{4 r _ {1} ^ {2}} - 1}
$$

As can be easily checked, the product of the two solutions is always equal to 1; therefore, only one of the solutions satisfies the invertibility condition $| \Theta | < 1$ .

After further algebraic manipulation, we see that the invertible solution can be written as

$$
\hat {\theta} = \frac {- 1 + \sqrt {1 - 4 r _ {1} ^ {2}}}{2 r _ {1}} \tag {7.1.4}
$$

If $r _ { 1 } = \pm 0 . 5$ , unique, real solutions exist, namely $\mp 1$ , but neither is invertible. If $| r _ { 1 } | > 0 . 5$ (which is certainly possible even though $| \rho _ { 1 } | < 0 . 5 $ ), no real solutions exist, and so the method of moments fails to yield an estimator of θ. Of course, if $| r _ { 1 } | > 0 . 5$ , the specification of an MA(1) model would be in considerable doubt.

For higher-order MA models, the method of moments quickly gets complicated. We can use Equations (4.2.5) on page 65 and replace $\rho _ { k }$ by $r _ { k }$ for $k = 1$ , 2,…, $q$ , to obtain $q$ equations in $q$ unknowns $\theta _ { 1 }$ , $\mid \Theta _ { 2 } , . . . , 6 _ { q }$ $\theta _ { q }$ . The resulting equations are highly nonlinear in the θ’s, however, and their solution would of necessity be numerical. In addition, there will be multiple solutions, of which only one is invertible. We shall not pursue this further since we shall see in Section 7.4 that, for MA models, the method of moments generally produces poor estimates.

# Mixed Models

We consider only the ARMA(1,1) case. Recall Equation (4.4.5) on page 78,

$$
\rho_ {k} = \frac {(1 - \theta \phi) (\phi - \theta)}{1 - 2 \theta \phi + \theta^ {2}} \phi^ {k - 1} \quad \text {f o r} k \geq 1
$$

Noting that $\rho _ { 2 } / \rho _ { 1 } = \phi$ , we can first estimate $\boldsymbol { \Phi }$ as

$$
\hat {\phi} = \frac {r _ {2}}{r _ {1}} \tag {7.1.5}
$$

Having done so, we can then use

$$
r _ {1} = \frac {(1 - \theta \hat {\phi}) (\hat {\phi} - \theta)}{1 - 2 \theta \hat {\phi} + \theta^ {2}} \tag {7.1.6}
$$

to solve for . Note again that a quadratic equation must be solved and only the invert-θ^ ible solution, if any, retained.

# Estimates of the Noise Variance

The final parameter to be estimated is the noise variance, $\sigma _ { e } ^ { 2 }$ . In all cases, we can first estimate the process variance, $\gamma _ { 0 } = V a r ( Y _ { t } )$ , by the sample variance

$$
s ^ {2} = \frac {1}{n - 1} \sum_ {t = 1} ^ {n} \left(Y _ {t} - \bar {Y}\right) ^ {2} \tag {7.1.7}
$$

and use known relationships from Chapter 4 among $\gamma _ { 0 }$ , $\sigma _ { e } ^ { 2 }$ , and the θ’s and $\phi$ ’s to estimate $\sigma _ { e } ^ { 2 }$ .

For the $\operatorname { A R } ( p )$ models, Equation (4.3.31) on page 77 yields

$$
\hat {\sigma} _ {e} ^ {2} = (1 - \hat {\phi} _ {1} r _ {1} - \hat {\phi} _ {2} r _ {2} - \dots - \hat {\phi} _ {p} r _ {p}) s ^ {2} \tag {7.1.8}
$$

In particular, for an AR(1) process,

$$
\hat {\sigma} _ {e} ^ {2} = (1 - r _ {1} ^ {2}) s ^ {2}
$$

since $\hat { \boldsymbol { \Phi } } = \boldsymbol { r } _ { 1 }$

For the $\mathrm { M A } ( q )$ case, we have, using Equation (4.2.4) on page 65,

$$
\hat {\theta} _ {e} ^ {2} = \frac {s ^ {2}}{1 + \hat {\theta} _ {1} ^ {2} + \hat {\theta} _ {2} ^ {2} + \cdots + \hat {\theta} _ {q} ^ {2}} \tag {7.1.9}
$$

For the ARMA(1,1) process, Equation (4.4.4) on page 78 yields

$$
\hat {\sigma} _ {e} ^ {2} = \frac {1 - \hat {\phi} ^ {2}}{1 - 2 \hat {\phi} \hat {\theta} + \hat {\theta} ^ {2}} s ^ {2} \tag {7.1.10}
$$

# Numerical Examples

The table in Exhibit 7.1 displays method-of-moments estimates for the parameters from several simulated time series. Generally speaking, the estimates for all the autoregressive models are fairly good but the estimates for the moving average models are not acceptable. It can be shown that theory confirms this observation—method-of-moments estimators are very inefficient for models containing moving average terms.

Exhibit 7.1 Method-of-Moments Parameter Estimates for Simulated Series   

<table><tr><td></td><td colspan="3">True Parameters</td><td colspan="3">Method-of-Moments Estimates</td><td></td></tr><tr><td>Model</td><td>θ</td><td>φ1</td><td>φ2</td><td>θ</td><td>φ1</td><td>φ2</td><td>n</td></tr><tr><td>MA(1)</td><td>-0.9</td><td></td><td></td><td>-0.554</td><td></td><td></td><td>120</td></tr><tr><td>MA(1)</td><td>0.9</td><td></td><td></td><td>0.719</td><td></td><td></td><td>120</td></tr><tr><td>MA(1)</td><td>-0.9</td><td></td><td></td><td>NA†</td><td></td><td></td><td>60</td></tr><tr><td>MA(1)</td><td>0.5</td><td></td><td></td><td>-0.314</td><td></td><td></td><td>60</td></tr><tr><td>AR(1)</td><td></td><td>0.9</td><td></td><td></td><td>0.831</td><td></td><td>60</td></tr><tr><td>AR(1)</td><td></td><td>0.4</td><td></td><td></td><td>0.470</td><td></td><td>60</td></tr><tr><td>AR(2)</td><td></td><td>1.5</td><td>-0.75</td><td></td><td>1.472</td><td>-0.767</td><td>120</td></tr></table>

† No method-of-moments estimate exists since $r _ { 1 } = 0 . 5 4 4$ for this simulation.

> data(ma1.2.s); data(ma1.1.s); data(ma1.3.s); data(ma1.4.s)   
> estimate.ma1.mom(ma1.2.s); estimate.ma1.mom(ma1.1.s)   
> estimate.ma1.mom(ma1.3.s); estimate.ma1.mom(ma1.4.s)   
> arima(ma1.4.s,order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (0,0,1),method $\cdot ^ { = }$ 'CSS',include.mean $\mathbf { \Psi } _ { \cdot } = \mathbf { F }$ )   
> data(ar1.s); data(ar1.2.s)   
> ar(ar1.s,order.max $^ { : = 1 }$ , $\mathbb { A } \mathbb { I } \mathbb { C } { = } \mathbb { F }$ ,method $. =$ 'yw')   
> ar(ar1.2.s,order.max $^ { : = 1 }$ ,AIC=F,method $\cdot ^ { = }$ 'yw')   
> data(ar2.s)   
> ar(ar2.s,order.max $^ { \cdot = 2 }$ , $\mathbb { A } \mathbb { I } \mathbb { C } { = } \mathbb { F }$ ,method='yw')

Consider now some actual time series. We start with the Canadian hare abundance series. Since we found in Exhibit 6.27 on page 136 that a square root transformation was appropriate here, we base all modeling on the square root of the original abundance numbers. We illustrate the estimation of an AR(2) model with the hare data, even

though we shall show later that an AR(3) model provides a better fit to the data. The first two sample autocorrelations displayed in Exhibit 6.28 on page 137 are $r _ { 1 } = 0 . 7 3 6$ and $r _ { 2 }$ $= 0 . 3 0 4$ . Using Equations (7.1.2), the method-of-moments estimates of $\Phi _ { 1 }$ and $\Phi _ { 2 }$ are

$$
\hat {\phi} _ {1} = \frac {r _ {1} \left(1 - r _ {2}\right)}{1 - r _ {1} ^ {2}} = \frac {0 . 7 3 6 (1 - 0 . 3 0 4)}{1 - (0 . 7 3 6) ^ {2}} = 1. 1 1 7 8 \tag {7.1.11}
$$

and

$$
\hat {\phi} _ {2} = \frac {r _ {2} - r _ {1} ^ {2}}{1 - r _ {1} ^ {2}} = \frac {0 . 3 0 4 - (0 . 7 3 6) ^ {2}}{1 - (0 . 7 3 6) ^ {2}} = - 0. 5 1 9 \tag {7.1.12}
$$

The sample mean and variance of this series (after taking the square root) are found to be 5.82 and 5.88, respectively. Then, using Equation (7.1.8), we estimate the noise variance as

$$
\begin{array}{l} \hat {\sigma} _ {e} ^ {2} = (1 - \hat {\phi} _ {1} r _ {1} - \hat {\phi} _ {2} r _ {2}) s ^ {2} \\ = [ 1 - (1. 1 1 7 8) (0. 7 3 6) - (- 0. 5 1 9) (0. 3 0 4) ] (5. 8 8) \tag {7.1.13} \\ = 1. 9 7 \\ \end{array}
$$

The estimated model (in original terms) is then

$$
\sqrt {Y _ {t}} - 5. 8 2 = 1. 1 1 7 8 \left(\sqrt {Y _ {t - 1}} - 5. 8 2\right) - 0. 5 1 9 \left(\sqrt {Y _ {t - 2}} - 5. 8 2\right) + e _ {t} \tag {7.1.14}
$$

or

$$
\sqrt {Y _ {t}} = 2. 3 3 5 + 1. 1 1 7 8 \sqrt {Y _ {t - 1}} - 0. 5 1 9 \sqrt {Y _ {t - 2}} + e _ {t} \tag {7.1.15}
$$

with estimated noise variance of 1.97.

Consider now the oil price series. Exhibit 6.32 on page 140 suggested that we specify an MA(1) model for the first differences of the logarithms of the series. The lag 1 sample autocorrelation in that exhibit is 0.212, so the method-of-moments estimate of θ is

$$
\hat {\theta} = \frac {- 1 + \sqrt {1 - 4 (0 . 2 1 2) ^ {2}}}{2 (0 . 2 1 2)} = - 0. 2 2 2 \tag {7.1.16}
$$

The mean of the differences of the logs is 0.004 and the variance is 0.0072. The estimated model is

$$
\nabla \log \left(Y _ {t}\right) = 0. 0 0 4 + e _ {t} + 0. 2 2 2 e _ {t - 1} \tag {7.1.17}
$$

or

$$
\log \left(Y _ {t}\right) = \log \left(Y _ {t - 1}\right) + 0. 0 0 4 + e _ {t} + 0. 2 2 2 e _ {t - 1} \tag {7.1.18}
$$

with estimated noise variance of

$$
\hat {\sigma} _ {e} ^ {2} = \frac {s ^ {2}}{1 + \hat {\theta} ^ {2}} = \frac {0 . 0 0 7 2}{1 + (- 0 . 2 2 2) ^ {2}} = 0. 0 0 6 8 6 \tag {7.1.19}
$$

Using Equation (3.2.3) on page 28 with estimated parameters yields a standard error of the sample mean of 0.0060. Thus, the observed sample mean of 0.004 is not significantly different from zero and we would remove the constant term from the model, giving a final model of

$$
\log \left(Y _ {t}\right) = \log \left(Y _ {t - 1}\right) + e _ {t} + 0. 2 2 2 e _ {t - 1} \tag {7.1.20}
$$

# 7.2 Least Squares Estimation

Because the method of moments is unsatisfactory for many models, we must consider other methods of estimation. We begin with least squares. For autoregressive models, the ideas are quite straightforward. At this point, we introduce a possibly nonzero mean, μ, into our stationary models and treat it as another parameter to be estimated by least squares.

# Autoregressive Models

Consider the first-order case where

$$
Y _ {t} - \mu = \phi \left(Y _ {t - 1} - \mu\right) + e _ {t} \tag {7.2.1}
$$

We can view this as a regression model with predictor variable $Y _ { t - 1 }$ and response variable $Y _ { t }$ . Least squares estimation then proceeds by minimizing the sum of squares of the differences

$$
(Y _ {t} - \mu) - \phi (Y _ {t - 1} - \mu)
$$

Since only $Y _ { 1 }$ $Y _ { 1 } , Y _ { 2 } , . . . , Y _ { n }$ $Y _ { n }$ are observed, we can only sum from $t = 2$ to $t = n$ . Let

$$
S _ {c} (\phi , \mu) = \sum_ {t = 2} ^ {n} \left[ \left(Y _ {t} - \mu\right) - \phi \left(Y _ {t - 1} - \mu\right) \right] ^ {2} \tag {7.2.2}
$$

This is usually called the conditional sum-of-squares function. (The reason for the term conditional will become apparent later on.) According to the principle of least squares, we estimate $\phi$ and $\mu$ by the respective values that minimize $S _ { c } ( \phi , \mu )$ given the observed values of $Y _ { 1 }$ , $Y _ { 2 } , . . . , Y _ { n }$ $Y _ { n }$ .

Consider the equation ${ \partial S _ { c } } / { \hat { \sigma } \mu } = 0$ . We have

$$
\frac {\partial S _ {c}}{\partial \mu} = \sum_ {t = 2} ^ {n} 2 [ (Y _ {t} - \mu) - \phi (Y _ {t - 1} - \mu) ] (- 1 + \phi) = 0
$$

or, simplifying and solving for $\mu$ ,

$$
\mu = \frac {1}{(n - 1) (1 - \phi)} \left[ \sum_ {t = 2} ^ {n} Y _ {t} - \phi \sum_ {t = 2} ^ {n} Y _ {t - 1} \right] \tag {7.2.3}
$$

Now, for large $n$

$$
\frac {1}{n - 1} \sum_ {t = 2} ^ {n} Y _ {t} \approx \frac {1}{n - 1} \sum_ {t = 2} ^ {n} Y _ {t - 1} \approx \overline {{Y}}
$$

Thus, regardless of the value of $\phi$ , Equation (7.2.3) reduces to

$$
\hat {\mu} \approx \frac {1}{1 - \phi} (\bar {Y} - \phi \bar {Y}) = \bar {Y} \tag {7.2.4}
$$

We sometimes say, except for end effects, $\widehat { \mu } = \overline { { Y } }$ .

Consider now the minimization of $S _ { c } ( \phi , \overline { { Y } } )$ with respect to $\phi$ . We have

$$
\frac {\partial S _ {c} (\phi , \bar {Y})}{\partial \phi} = \sum_ {t = 2} ^ {n} 2 [ (Y _ {t} - \bar {Y}) - \phi (Y _ {t - 1} - \bar {Y}) ] (Y _ {t - 1} - \bar {Y})
$$

Setting this equal to zero and solving for $\phi$ yields

$$
\hat {\phi} = \frac {\sum_ {t = 2} ^ {n} (Y _ {t} - \bar {Y}) (Y _ {t - 1} - \bar {Y})}{\sum_ {t = 2} ^ {n} (Y _ {t - 1} - \bar {Y}) ^ {2}}
$$

Except for one term missing in the denominator, namely $( Y _ { n } - { \overline { { Y } } } ) ^ { 2 }$ , this is the same as $r _ { 1 }$ . The lone missing term is negligible for stationary processes, and thus the least squares and method-of-moments estimators are nearly identical, especially for large samples.

For the general $\operatorname { A R } ( p )$ process, the methods used to obtain Equations (7.2.3) and (7.2.4) can easily be extended to yield the same result, namely

$$
\hat {\mu} = \bar {Y} \tag {7.2.5}
$$

To generalize the estimation of the $\phi$ ’s, we consider the second-order model. In accordance with Equation (7.2.5), we replace $\mu$ by $\overline { { Y } }$ in the conditional sum-of-squares function, so

$$
S _ {c} (\phi_ {1}, \phi_ {2}, \bar {Y}) = \sum_ {t = 3} ^ {n} [ (Y _ {t} - \bar {Y}) - \phi_ {1} (Y _ {t - 1} - \bar {Y}) - \phi_ {2} (Y _ {t - 2} - \bar {Y}) ] ^ {2} \qquad (7. 2. 6)
$$

Setting $\partial S _ { c } / \partial \Phi _ { 1 } = 0$ , we have

$$
- 2 \sum_ {t = 3} ^ {n} \left[ \left(Y _ {t} - \bar {Y}\right) - \phi_ {1} \left(Y _ {t - 1} - \bar {Y}\right) - \phi_ {2} \left(Y _ {t - 2} - \bar {Y}\right) \right] \left(Y _ {t - 1} - \bar {Y}\right) = 0 \tag {7.2.7}
$$

which we can rewrite as

$$
\begin{array}{l} \sum_ {t = 3} ^ {n} \left(Y _ {t} - \bar {Y}\right) \left(Y _ {t - 1} - \bar {Y}\right) = \left(\sum_ {t = 3} ^ {n} \left(Y _ {t - 1} - \bar {Y}\right) ^ {2}\right) \phi_ {1} \tag {7.2.8} \\ + \left(\sum_ {t = 3} ^ {n} \left(Y _ {t - 1} - \bar {Y}\right) \left(Y _ {t - 2} - \bar {Y}\right)\right) \phi_ {2} \\ \end{array}
$$

The sum of the lagged products $\sum _ { t = 3 } ^ { n } ( Y _ { t } - \overline { { Y } } ) ( Y _ { t - 1 } - \overline { { Y } } )$ is very nearly the numerator of $r _ { 1 }$ — we are missing one product, $( Y _ { 2 } - { \overline { { Y } } } ) ( Y _ { 1 } - { \overline { { Y } } } )$ . A similar situation exists for $\sum _ { t = 3 } ^ { n } ( Y _ { t - 1 } - \overline { { Y } } ) ( Y _ { t - 2 } - \overline { { Y } } )$ , but here we are missing $( Y _ { n } - { \overline { { Y } } } ) ( Y _ { n - 1 } - { \overline { { Y } } } )$ . If we divide both sides of Equation (7.2.8) by $\sum _ { t = 3 } ^ { n } ( Y _ { t } - { \overline { { Y } } } ) ^ { 2 }$ , then, except for end effects, which are negligible under the stationarity assumption, we obtain

$$
r _ {1} = \phi_ {1} + r _ {1} \phi_ {2} \tag {7.2.9}
$$

Approximating in a similar way with the equation $\partial S _ { c } / \partial \Phi _ { 2 } = 0$ leads to

$$
r _ {2} = r _ {1} \phi_ {1} + \phi_ {2} \tag {7.2.10}
$$

But Equations (7.2.9) and (7.2.10) are just the sample Yule-Walker equations for an AR(2) model.

Entirely analogous results follow for the general stationary $\operatorname { A R } ( p )$ case: To an excellent approximation, the conditional least squares estimates of the $\phi$ ’s are obtained by solving the sample Yule-Walker equations (7.1.3).†

# Moving Average Models

Consider now the least-squares estimation of θ in the MA(1) model:

$$
Y _ {t} = e _ {t} - \theta e _ {t - 1} \tag {7.2.11}
$$

At first glance, it is not apparent how a least squares or regression method can be applied to such models. However, recall from Equation (4.4.2) on page 77 that invertible MA(1) models can be expressed as

$$
Y _ {t} = - \theta Y _ {t - 1} - \theta^ {2} Y _ {t - 2} - \theta^ {3} Y _ {t - 3} - \dots + e _ {t}
$$

an autoregressive model but of infinite order. Thus least squares can be meaningfully carried out by choosing a value of θ that minimizes

$$
S _ {c} (\theta) = \sum (e _ {t}) ^ {2} = \sum \left[ Y _ {t} + \theta Y _ {t - 1} + \theta^ {2} Y _ {t - 2} + \theta^ {3} Y _ {t - 3} + \dots \right] ^ {2} \tag {7.2.12}
$$

where, implicitly, $e _ { t } = e _ { t } ( \theta )$ is a function of the observed series and the unknown parameter θ.

It is clear from Equation (7.2.12) that the least squares problem is nonlinear in the parameters. We will not be able to minimize $S _ { c } ( \theta )$ by taking a derivative with respect to θ, setting it to zero, and solving. Thus, even for the simple MA(1) model, we must resort to techniques of numerical optimization. Other problems exist in this case: We have not shown explicit limits on the summation in Equation (7.2.12) nor have we said how to deal with the infinite series under the summation sign.

To address these issues, consider evaluating $S _ { c } ( \boldsymbol { \theta } )$ for a single given value of θ. The only $Y \mathrm { s }$ we have available are our observed series, $Y _ { 1 }$ , Y2,…, $Y _ { n }$ . Rewrite Equation (7.2.11) as

$$
e _ {t} = Y _ {t} + \theta e _ {t - 1} \tag {7.2.13}
$$

Using this equation, $e _ { 1 }$ , e2,…, $e _ { n }$ can be calculated recursively if we have the initial value $e _ { 0 }$ . A common approximation is to set $e _ { 0 } = 0$ —its expected value. Then, conditional on $e _ { 0 } = 0$ , we can obtain

$$
\left. \begin{array}{c} e _ {1} = Y _ {1} \\ e _ {2} = Y _ {2} + \theta e _ {1} \\ e _ {3} = Y _ {3} + \theta e _ {2} \\ \vdots \\ e _ {n} = Y _ {n} + \theta e _ {n - 1} \end{array} \right\} \tag {7.2.14}
$$

and thus calculate $S _ { c } ( \boldsymbol { \Theta } ) = \sum ( e _ { t } ) ^ { 2 }$ , conditional on $e _ { 0 } = 0$ , for that single given value of θ.

For the simple case of one parameter, we could carry out a grid search over the invertible range $( - 1 , + 1 )$ for θ to find the minimum sum of squares. For more general $\mathrm { M A } ( q )$ models, a numerical optimization algorithm, such as Gauss-Newton or Nelder-Mead, will be needed.

For higher-order moving average models, the ideas are analogous and no new difficulties arise. We compute $e _ { t } = e _ { t } ( \theta _ { 1 } , \theta _ { 2 } , . . . , \theta _ { q } )$ recursively from

$$
e _ {t} = Y _ {t} + \theta_ {1} e _ {t - 1} + \theta_ {2} e _ {t - 2} + \dots + \theta_ {q} e _ {t - q} \tag {7.2.15}
$$

with $e _ { 0 } = e _ { - 1 } = \dots = e _ { - q } = 0$ . The sum of squares is minimized jointly in $\theta _ { 1 } , \theta _ { 2 } , . . . , \theta _ { q }$ using a multivariate numerical method.

# Mixed Models

Consider the ARMA(1,1) case

$$
Y _ {t} = \phi Y _ {t - 1} + e _ {t} - \theta e _ {t - 1} \tag {7.2.16}
$$

As in the pure MA case, we consider $e _ { t } = e _ { t } ( \phi , \theta )$ and wish to minimize $S _ { c } ( \Phi , \boldsymbol { \Theta } ) = \Sigma e _ { t } ^ { 2 }$ We can rewrite Equation (7.2.16) as

$$
e _ {t} = Y _ {t} - \phi Y _ {t - 1} + \theta e _ {t - 1} \tag {7.2.17}
$$

To obtain $e _ { 1 }$ , we now have an additional “startup” problem, namely $Y _ { 0 }$ . One approach is to set $Y _ { 0 } = 0$ or to $\overline { { Y } }$ if our model contains a nonzero mean. However, a better approach is to begin the recursion at $t = 2$ , thus avoiding $Y _ { 0 }$ altogether, and simply minimize

$$
S _ {c} (\phi , \theta) = \sum_ {t = 2} ^ {n} e _ {t} ^ {2}
$$

For the general ARMA $( p , q )$ model, we compute

$$
\begin{array}{l} e _ {t} = Y _ {t} - \phi_ {1} Y _ {t - 1} - \phi_ {2} Y _ {t - 2} - \dots - \phi_ {p} Y _ {t - p} \tag {7.2.18} \\ + \theta_ {1} e _ {t - 1} + \theta_ {2} e _ {t - 2} + \dots + \theta_ {q} e _ {t - q} \\ \end{array}
$$

with $e _ { p } = e _ { p - 1 } = \dots = e _ { p + 1 - q } = 0$ and then minimize $S _ { c } ( \Phi _ { 1 } , \Phi _ { 2 } , . . . , \Phi _ { p } , \Theta _ { 1 } , \Theta _ { 2 } , . . . , \Theta _ { q } )$ numerically to obtain the conditional least squares estimates of all the parameters.

For parameter sets $\theta _ { 1 }$ , $\theta _ { 2 } , . . . , \theta _ { q }$ corresponding to invertible models, the start-up values $e _ { p } , e _ { p - 1 } , . . . , e _ { p + 1 - q }$ $e _ { p }$ will have very little influence on the final estimates of the parameters for large samples.

# 7.3 Maximum Likelihood and Unconditional Least Squares

For series of moderate length and also for stochastic seasonal models to be discussed in Chapter 10, the start-up values $e _ { p } = e _ { p - 1 } = \dots = e _ { p + 1 - q } = 0$ will have a more pronounced effect on the final estimates for the parameters. Thus we are led to consider the more difficult problem of maximum likelihood estimation.

The advantage of the method of maximum likelihood is that all of the information in the data is used rather than just the first and second moments, as is the case with least squares. Another advantage is that many large-sample results are known under very general conditions. One disadvantage is that we must for the first time work specifically with the joint probability density function of the process.

# Maximum Likelihood Estimation

For any set of observations, $Y _ { 1 }$ $\mathbf { \omega } _ { 1 } ^ { \prime } , Y _ { 2 } , . . . , Y _ { n }$ $Y _ { n }$ , time series or not, the likelihood function $L$ is defined to be the joint probability density of obtaining the data actually observed. However, it is considered as a function of the unknown parameters in the model with the observed data held fixed. For ARIMA models, $L$ will be a function of the $\boldsymbol { \Phi }$ ’s, θ’s, μ, and $\sigma _ { e } ^ { 2 }$ given the observations $Y _ { 1 }$ , Y2,…, $Y _ { n }$ . The maximum likelihood estimators are then defined as those values of the parameters for which the data actually observed are most likely, that is, the values that maximize the likelihood function.

We begin by looking in detail at the AR(1) model. The most common assumption is that the white noise terms are independent, normally distributed random variables with

zero means and common standard deviation $\sigma _ { e }$ . The probability density function (pdf) of each $e _ { t }$ is then

$$
(2 \pi \sigma_ {e} ^ {2}) ^ {- 1 / 2} \exp \left(- \frac {e _ {t} ^ {2}}{2 \sigma_ {e} ^ {2}}\right) \text {f o r} - \infty <   e _ {t} <   \infty
$$

and, by independence, the joint pdf for $e _ { 2 } , e _ { 3 } , . . . , e _ { n }$ $e _ { 2 }$ is

$$
(2 \pi \sigma_ {e} ^ {2}) ^ {- (n - 1) / 2} \exp \left(- \frac {1}{2 \sigma_ {e} ^ {2}} \sum_ {t = 2} ^ {n} e _ {t} ^ {2}\right) \tag {7.3.1}
$$

Now consider

$$
\left. \begin{array}{c} Y _ {2} - \mu = \phi \left(Y _ {1} - \mu\right) + e _ {2} \\ Y _ {3} - \mu = \phi \left(Y _ {2} - \mu\right) + e _ {3} \\ \vdots \\ Y _ {n} - \mu = \phi \left(Y _ {n - 1} - \mu\right) + e _ {n} \end{array} \right\} \tag {7.3.2}
$$

If we condition on $Y _ { 1 } = y _ { 1 }$ , Equation (7.3.2) defines a linear transformation between $e _ { 2 }$ , $e _ { 3 } , . . . , e _ { n }$ and $Y _ { 2 }$ , $Y _ { 3 }$ ,…, $Y _ { n }$ (with Jacobian equal to 1). Thus the joint pdf of $Y _ { 2 }$ , $Y _ { 3 }$ ,…, $Y _ { n }$ given $Y _ { 1 } = y _ { 1 }$ can be obtained by using Equation (7.3.2) to substitute for the e’s in terms of the Y’s in Equation (7.3.1). Thus we get

$$
\begin{array}{l} f (y _ {2}, y _ {3}, \dots , y _ {n} | y _ {1}) = (2 \pi \sigma_ {e} ^ {2}) ^ {- (n - 1) / 2} \\ \times \exp \left\{- \frac {1}{2 \sigma_ {e} ^ {2}} \sum_ {t = 2} ^ {n} [ (y _ {t} - \mu) - \phi (y _ {t - 1} - \mu) ] ^ {2} \right\} \tag {7.3.3} \\ \end{array}
$$

Now consider the (marginal) distribution of $Y _ { 1 }$ . It follows from the linear process representation of the AR(1) process (Equation (4.3.8) on page 70) that $Y _ { 1 }$ will have a normal distribution with mean $\mu$ and variance $\sigma _ { e } ^ { 2 } / ( 1 - \phi ^ { 2 } )$ . Multiplying the conditional pdf in Equation (7.3.3) by the marginal pdf of $Y _ { 1 }$ gives us the joint pdf of $Y _ { 1 }$ , Y2,…, $Y _ { n }$ that we require. Interpreted as a function of the parameters $\phi , \mu$ , and $\sigma _ { e } ^ { 2 }$ , the likelihood function for an AR(1) model is given by

$$
L \left(\phi , \mu , \sigma_ {e} ^ {2}\right) = \left(2 \pi \sigma_ {e} ^ {2}\right) ^ {- n / 2} \left(1 - \phi^ {2}\right) ^ {1 / 2} \exp \left[ - \frac {1}{2 \sigma_ {e} ^ {2}} S (\phi , \mu) \right] \tag {7.3.4}
$$

where

$$
S (\phi , \mu) = \sum_ {t = 2} ^ {n} \left[ \left(Y _ {t} - \mu\right) - \phi \left(Y _ {t - 1} - \mu\right) \right] ^ {2} + (1 - \phi^ {2}) \left(Y _ {1} - \mu\right) \tag {7.3.5}
$$

The function $S ( \boldsymbol { \Phi } , \mu )$ is called the unconditional sum-of-squares function.

As a general rule, the logarithm of the likelihood function is more convenient to

work with than the likelihood itself. For the AR(1) case, the log-likelihood function, denoted $\ell ( \boldsymbol { \Phi } , \mu , \sigma _ { e } ^ { 2 } )$ , is given by

$$
\ell (\phi , \mu , \sigma_ {e} ^ {2}) = - \frac {n}{2} \log (2 \pi) - \frac {n}{2} \log (\sigma_ {e} ^ {2}) + \frac {1}{2} \log (1 - \phi^ {2}) - \frac {1}{2 \sigma_ {e} ^ {2}} S (\phi , \mu) \tag {7.3.6}
$$

For given values of $\boldsymbol { \Phi }$ and $\mu$ , $\ell ( \boldsymbol { \Phi } , \boldsymbol { \mu } , \boldsymbol { \sigma } _ { e } ^ { 2 } )$ can be maximized analytically with respect to $\sigma _ { e } ^ { 2 }$ in terms of the yet-to-be-determined estimators of $\phi$ and $\mu$ . We obtain

$$
\hat {\sigma} _ {e} ^ {2} = \frac {S (\hat {\phi} , \hat {\mu})}{n} \tag {7.3.7}
$$

As in many other similar contexts, we usually divide by $n - 2$ rather than $n$ (since we are estimating two parameters, $\phi$ and $\mu$ ) to obtain an estimator with less bias. For typical time series sample sizes, there will be very little difference.

Consider now the estimation of $\phi$ and $\mu$ . A comparison of the unconditional sum-of-squares function $S ( \Phi , \mu )$ with the earlier conditional sum-of-squares function $S _ { c } ( \phi , \mu )$ of Equation (7.2.2) on page 154, reveals one simple difference:

$$
S (\phi , \mu) = S _ {c} (\phi , \mu) + (1 - \phi^ {2}) (Y _ {1} - \mu) ^ {2} \tag {7.3.8}
$$

Since $S _ { c } ( \phi , \mu )$ involves a sum of $n - 1$ components, whereas $( 1 - \Phi ^ { 2 } ) ( Y _ { 1 } - \mu ) ^ { 2 }$ does not involve $n$ , we shall have $S ( \boldsymbol { \Phi } , \mu ) \approx S _ { c } ( \boldsymbol { \Phi } , \mu )$ . Thus the values of $\boldsymbol { \Phi }$ and $\mu$ that minimize $S ( \boldsymbol { \Phi } , \mu )$ or $S _ { c } ( \phi , \mu )$ should be very similar, at least for larger sample sizes. The effect of the rightmost term in Equation (7.3.8) will be more substantial when the minimum for $\boldsymbol { \Phi }$ occurs near the stationarity boundary of $\pm 1$ .

# Unconditional Least Squares

As a compromise between conditional least squares estimates and full maximum likelihood estimates, we might consider obtaining unconditional least squares estimates; that is, estimates minimizing $S ( \boldsymbol { \Phi } , \mu )$ . Unfortunately, the term $( 1 - \phi ^ { 2 } ) ( Y _ { 1 } - \mu ) ^ { 2 }$ causes the equations $\partial S / \partial \Phi = 0$ and $\partial S / \partial \mu = 0$ to be nonlinear in $\phi$ and $\mu$ , and reparameterization to a constant term $\theta _ { 0 } = \mu ( 1 - \phi )$ does not improve the situation substantially. Thus minimization must be carried out numerically. The resulting estimates are called unconditional least squares estimates.

The derivation of the likelihood function for more general ARMA models is considerably more involved. One derivation may be found in Appendix H: State Space Models on page 222. We refer the reader to Brockwell and Davis (1991) or Shumway and Stoffer (2006) for even more details.

# 7.4 Properties of the Estimates

The large-sample properties of the maximum likelihood and least squares (conditional or unconditional) estimators are identical and can be obtained by modifying standard maximum likelihood theory. Details can be found in Shumway and Stoffer (2006, pp. 125–129). We shall look at the results and their implications for simple ARMA models.

For large $n$ , the estimators are approximately unbiased and normally distributed. The variances and correlations are as follows:

$$
\operatorname {A R} (1): \operatorname {V a r} (\hat {\phi}) \approx \frac {1 - \phi^ {2}}{n} \tag {7.4.9}
$$

$$
\operatorname {A R} (2): \left\{ \begin{array}{l} V a r \left(\hat {\phi} _ {1}\right) \approx V a r \left(\hat {\phi} _ {2}\right) \approx \frac {1 - \phi_ {2} ^ {2}}{n} \\ C o r r \left(\hat {\phi} _ {1}, \hat {\phi} _ {2}\right) \approx - \frac {\phi_ {1}}{1 - \phi_ {2}} = - \rho_ {1} \end{array} \right. \tag {7.4.10}
$$

$$
\operatorname {M A} (1): \operatorname {V a r} (\hat {\theta}) \approx \frac {1 - \theta^ {2}}{n} \tag {7.4.11}
$$

$$
\mathrm {M A} (2): \left\{ \begin{array}{l} \operatorname {V a r} \left(\hat {\theta} _ {1}\right) \approx \operatorname {V a r} \left(\hat {\theta} _ {2}\right) \approx \frac {1 - \theta_ {2} ^ {2}}{n} \\ \operatorname {C o r r} \left(\hat {\theta} _ {1}, \hat {\theta} _ {2}\right) \approx - \frac {\theta_ {1}}{1 - \theta_ {2}} \end{array} \right. \tag {7.4.12}
$$

$$
\operatorname {A R M A} (1, 1): \left\{ \begin{array}{l} V a r \left(\hat {\phi}\right) \approx \left[ \frac {1 - \phi^ {2}}{n} \right] \left[ \frac {1 - \phi \theta}{\phi - \theta} \right] ^ {2} \\ V a r (\hat {\theta}) \approx \left[ \frac {1 - \theta^ {2}}{n} \right] \left[ \frac {1 - \phi \theta}{\phi - \theta} \right] ^ {2} \\ C o r r (\hat {\phi}, \hat {\theta}) \approx \frac {\sqrt {\left(1 - \phi^ {2}\right) \left(1 - \theta^ {2}\right)}}{1 - \phi \theta} \end{array} \right. \tag {7.4.13}
$$

Notice that, in the AR(1) case, the variance of the estimator of $\phi$ decreases as $\boldsymbol { \Phi }$ approaches $\pm 1$ . Also notice that even though an AR(1) model is a special case of an AR(2) model, the variance of $\hat { \Phi } _ { 1 }$ shown in Equations (7.4.10) shows that our estimation of $\Phi _ { 1 }$ will generally suffer if we erroneously fit an AR(2) model when, in fact, $\phi _ { 2 } = 0$ . Similar comments could be made about fitting an MA(2) model when an MA(1) would suffice or fitting an ARMA(1,1) when an AR(1) or an MA(1) is adequate.

For the ARMA(1,1) case, note the denominator of $\phi - \theta$ in the variances in Equations (7.4.13). If $\boldsymbol { \Phi }$ and θ are nearly equal, the variability in the estimators of $\boldsymbol { \Phi }$ and θ can be extremely large.

Note that in all of the two-parameter models, the estimates can be highly correlated, even for very large sample sizes.

The table shown in Exhibit 7.2 gives numerical values for the large-sample approximate standard deviations of the estimates of $\phi$ in an AR(1) model for several values of $\phi$ and several sample sizes. Since the values in the table are equal to $\sqrt { ( 1 - \phi ^ { 2 } ) / n }$ , they apply equally well to standard deviations computed according to Equations (7.4.10),

(7.4.11), and (7.4.12).

Thus, in estimating an AR(1) model with, for example, $n = 1 0 0$ and $\phi = 0 . 7$ , we can be about $9 5 \%$ confident that our estimate of $\phi$ is in error by no more than $\pm 2 ( 0 . 0 7 ) =$ $\pm 0 . 1 4$ .

Exhibit 7.2 AR(1) Model Large-Sample Standard Deviations of $\widehat { \phi }$   

<table><tr><td colspan="4">n</td></tr><tr><td>φ</td><td>50</td><td>100</td><td>200</td></tr><tr><td>0.4</td><td>0.13</td><td>0.09</td><td>0.06</td></tr><tr><td>0.7</td><td>0.10</td><td>0.07</td><td>0.05</td></tr><tr><td>0.9</td><td>0.06</td><td>0.04</td><td>0.03</td></tr></table>

For stationary autoregressive models, the method of moments yields estimators equivalent to least squares and maximum likelihood, at least for large samples. For models containing moving average terms, such is not the case. For an MA(1) model, it can be shown that the large-sample variance of the method-of-moments estimator of θ is equal to

$$
V a r (\hat {\theta}) \approx \frac {1 + \theta^ {2} + 4 \theta^ {4} + \theta^ {6} + \theta^ {8}}{n (1 - \theta^ {2}) ^ {2}} \tag {7.4.14}
$$

Comparing Equation (7.4.14) with that of Equation (7.4.11), we see that the variance for the method-of-moments estimator is always larger than the variance of the maximum likelihood estimator. The table in Exhibit 7.3 displays the ratio of the large-sample standard deviations for the two methods for several values of θ. For example, if θ is 0.5, the method-of-moments estimator has a large-sample standard deviation that is $42 \%$ larger than the standard deviation of the estimator obtained using maximum likelihood. It is clear from these ratios that the method-of-moments estimator should not be used for the MA(1) model. This same advice applies to all models that contain moving average terms.

Exhibit 7.3 Method of Moments (MM) vs. Maximum Likelihood (MLE) in MA(1) Models   

<table><tr><td>θ</td><td>SDMM/SDMLE</td></tr><tr><td>0.25</td><td>1.07</td></tr><tr><td>0.50</td><td>1.42</td></tr><tr><td>0.75</td><td>2.66</td></tr><tr><td>0.90</td><td>5.33</td></tr></table>

# 7.5 Illustrations of Parameter Estimation

Consider the simulated MA(1) series with $\theta = - 0 . 9$ . The series was displayed in Exhibit 4.2 on page 59, and we found the method-of-moments estimate of θ to be a rather poor $- 0 . 5 5 4$ ; see Exhibit 7.1 on page 152. In contrast, the maximum likelihood estimate is −0.915, the unconditional sum-of-squares estimate is −0.923, and the conditional least squares estimate is $- 0 . 8 7 9$ . For this series, the maximum likelihood estimate of −0.915 is closest to the true value used in the simulation. Using Equation (7.4.11) on page 161 and replacing θ by its estimate, we have a standard error of about

$$
\sqrt {V a \hat {r} (\hat {\theta})} \approx \sqrt {\frac {1 - \hat {\theta} ^ {2}}{n}} = \sqrt {\frac {1 - (0 . 9 1) ^ {2}}{1 2 0}} \approx 0. 0 4
$$

so none of the maximum likelihood, conditional sum-of-squares, or unconditional sum-of-squares estimates are significantly far from the true value of $- 0 . 9$ .

The second MA(1) simulation with $\theta = 0 . 9$ produced the method-of-moments estimate of 0.719 shown in Exhibit 7.1. The conditional sum-of-squares estimate is 0.958, the unconditional sum-of-squares estimate is 0.983, and the maximum likelihood estimate is 1.000. These all have a standard error of about 0.04 as above. Here the maximum likelihood estimate of $\widehat { \theta } = 1$ is a little disconcerting since it corresponds to a noninvertible model.

The third MA(1) simulation with $\theta = - 0 . 9$ produced a method-of-moments estimate of −0.719 (see Exhibit 7.1). The maximum likelihood estimate here is $- 0 . 8 9 4$ with a standard error of about

$$
\sqrt {V a r (\hat {\theta})} \approx \sqrt {\frac {1 - (0 . 8 9 4) ^ {2}}{6 0}} \approx 0. 0 6
$$

For these data, the conditional sum-of-squares estimate is −0.979 and the unconditional sum-of-squares estimate is −0.961. Of course, with a standard error of this magnitude, it is unwise to report digits in the estimates of θ beyond the tenths place.

For our simulated autoregressive models, the results are reported in Exhibits 7.4 and 7.5.

<table><tr><td>Exhibit 7.4</td><td colspan="5">Parameter Estimation for Simulated AR(1) Models</td></tr><tr><td>Parameter φ</td><td>Method-of-Moments Estimate</td><td>Conditional SS Estimate</td><td>Unconditional SS Estimate</td><td>Maximum Likelihood Estimate</td><td>n</td></tr><tr><td>0.9</td><td>0.831</td><td>0.857</td><td>0.911</td><td>0.892</td><td>60</td></tr><tr><td>0.4</td><td>0.470</td><td>0.473</td><td>0.473</td><td>0.465</td><td>60</td></tr></table>

```txt
> data(ar1.s); data(ar1.2.s)  
> ar(ar1.s, order.max = 1, AIC = F, method = 'yw')  
> ar(ar1.s, order.max = 1, AIC = F, method = 'ols')  
> ar(ar1.s, order.max = 1, AIC = F, method = 'MLE') 
```

```txt
> ar(ar1.2.s, order.max=1, AIC=F, method='yw')
> ar(ar1.2.s, order.max=1, AIC=F, method='ols')
> ar(ar1.2.s, order.max=1, AIC=F, method='mle') 
```

From Equation (7.4.9) on page 161, the standard errors for the estimates are

$$
\sqrt {V a \hat {r} (\hat {\phi})} \approx \sqrt {\frac {1 - \hat {\phi} ^ {2}}{n}} = \sqrt {\frac {1 - (0 . 8 3 1) ^ {2}}{6 0}} \approx 0. 0 7
$$

and

$$
\sqrt {V a r (\hat {\phi})} = \sqrt {\frac {1 - (0 . 4 7 0) ^ {2}}{6 0}} \approx 0. 1 1
$$

respectively. Considering the magnitude of these standard errors, all four methods estimate reasonably well for AR(1) models.

<table><tr><td>Exhibit 7.5</td><td colspan="5">Parameter Estimation for a Simulated AR(2) Model</td></tr><tr><td>Parameters</td><td>Method-of-Moments Estimates</td><td>Conditional SS Estimates</td><td>Unconditional SS Estimates</td><td>Maximum Likelihood Estimate</td><td>n</td></tr><tr><td>φ1 = 1.5</td><td>1.472</td><td>1.5137</td><td>1.5183</td><td>1.5061</td><td>120</td></tr><tr><td>φ2 = -0.75</td><td>-0.767</td><td>-0.8050</td><td>-0.8093</td><td>-0.7965</td><td>120</td></tr></table>

```javascript
> data(ar2.s)  
> ar(ar2.s, order.max=2, AIC=F, method='yw')  
> ar(ar2.s, order.max=2, AIC=F, method='ols')  
> ar(ar2.s, order.max=2, AIC=F, method='MLE') 
```

From Equation (7.4.10) on page 161, the standard errors for the estimates are

$$
\sqrt {V a \hat {r} (\hat {\phi} _ {1})} \approx \sqrt {V a \hat {r} (\hat {\phi} _ {2})} \approx \sqrt {\frac {1 - \phi_ {2} ^ {2}}{n}} = \sqrt {\frac {1 - (0 . 7 5) ^ {2}}{1 2 0}} \approx 0. 0 6
$$

Again, considering the size of the standard errors, all four methods estimate reasonably well for AR(2) models.

As a final example using simulated data, consider the ARMA(1,1) shown in Exhibit 6.14 on page 123. Here $\phi = 0 . 6$ , $\theta = - 0 . 3$ , and $n = 1 0 0$ . Estimates using the various methods are shown in Exhibit 7.6.

Exhibit 7.6 Parameter Estimation for a Simulated ARMA(1,1) Model   

<table><tr><td>Parameters</td><td>Method-of-Moments Estimates</td><td>Conditional SS Estimates</td><td>Unconditional SS Estimates</td><td>Maximum Likelihood Estimate</td><td>n</td></tr><tr><td>φ = 0.6</td><td>0.637</td><td>0.5586</td><td>0.5691</td><td>0.5647</td><td>100</td></tr><tr><td>θ = -0.3</td><td>-0.2066</td><td>-0.3669</td><td>-0.3618</td><td>-0.3557</td><td>100</td></tr></table>

> data(arma11.s)   
> arima(arma11.s, order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (1,0,1),method='CSS')   
> arima(arma11.s, order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (1,0,1),method='ML')

Now let’s look at some real time series. The industrial chemical property time series was first shown in Exhibit 1.3 on page 3. The sample PACF displayed in Exhibit 6.26 on page 135, strongly suggested an AR(1) model for this series. Exhibit 7.7 shows the various estimates of the $\phi$ parameter using four different methods of estimation.

Exhibit 7.7 Parameter Estimation for the Color Property Series   

<table><tr><td>Parameter</td><td>Method-of-Moments Estimate</td><td>Conditional SS Estimate</td><td>Unconditional SS Estimate</td><td>Maximum Likelihood Estimate</td><td>n</td></tr><tr><td>φ</td><td>0.5282</td><td>0.5549</td><td>0.5890</td><td>0.5703</td><td>35</td></tr></table>

> data(color)  
> ar(color,order.max $^ { : = 1 }$ , $\mathbb { A } \mathbb { I } \mathbb { C } { = } \mathbb { F }$ ,method $. =$ 'yw')   
> ar(color,order.max $^ { : = 1 }$ , $\mathbb { A } \mathbb { I } \mathbb { C } { = } \mathbb { F }$ ,method $. =$ 'ols')   
> ar(color,order.max $^ { : = 1 }$ , $\mathbb { A } \mathbb { I } \mathbb { C } { = } \mathbb { F }$ ,method='mle')

Here the standard error of the estimates is about

$$
\sqrt {V a r (\hat {\phi})} \approx \sqrt {\frac {1 - (0 . 5 7) ^ {2}}{3 5}} \approx 0. 1 4
$$

so all of the estimates are comparable.

As a second example, consider again the Canadian hare abundance series. As before, we base all modeling on the square root of the original abundance numbers. Based on the partial autocorrelation function shown in Exhibit 6.29 on page 137, we will estimate an AR(3) model. For this illustration, we use maximum likelihood estimation and show the results obtained from the R software in Exhibit 7.8.

Exhibit 7.8 Maximum Likelihood Estimates from R Software: Hare Series   

<table><tr><td>Coefficients:</td><td>ar1</td><td>ar2</td><td>ar3</td><td>Intercept†</td></tr><tr><td></td><td>1.0519</td><td>-0.2292</td><td>-0.3931</td><td>5.6923</td></tr><tr><td>s.e.</td><td>0.1877</td><td>0.2942</td><td>0.1915</td><td>0.3371</td></tr></table>

sigma^2 estimated as 1.066: log-likelihood = -46.54, AIC = 101.08

† The intercept here is the estimate of the process mean $\mu$ —not of $\theta _ { 0 }$ .

```txt
> data(hare)  
> arima(square(hare), order=c(3,0,0)) 
```

Here we see that $\hat { \phi } _ { 1 } = 1 . 0 5 1 9$ , $\hat { \Phi } _ { 2 } = - 0 . 2 2 9 2$ , and $\hat { \Phi } _ { 3 } = - 0 . 3 9 3 0$ . We also see that the estimated noise variance is $\hat { \sigma } _ { e } ^ { 2 } = 1 . 0 \bar { 6 } 6$ . Noting the standard errors, the estimates of the lag 1 and lag 3 autoregressive coefficients are significantly different from zero, as is the intercept term, but the lag 2 autoregressive parameter estimate is not significant.

The estimated model would be written

$$
\begin{array}{l} \sqrt {Y _ {t}} - 5. 6 9 2 3 = 1. 0 5 1 9 (\sqrt {Y _ {t - 1}} - 5. 6 9 2 3) - 0. 2 2 9 2 (\sqrt {Y _ {t - 2}} - 5. 6 9 2 3) \\ - 0. 3 9 3 0 (\sqrt {Y _ {t - 3}} - 5. 6 9 2 3) + e _ {t} \\ \end{array}
$$

or

$$
\sqrt {Y _ {t}} = 3. 2 5 + 1. 0 5 1 9 \sqrt {Y _ {t - 1}} - 0. 2 2 9 2 \sqrt {Y _ {t - 2}} - 0. 3 9 3 0 \sqrt {Y _ {t - 3}} + e _ {t}
$$

where $Y _ { t }$ is the hare abundance in year $t$ in original terms. Since the lag 2 autoregressive term is insignificant, we might drop that term (that is, set $\phi _ { 2 } = 0$ ) and obtain new estimates of $\Phi _ { 1 }$ and $\phi _ { 3 }$ with this subset model.

As a last example, we return to the oil price series. The sample ACF shown in Exhibit 6.32 on page 140, suggested an MA(1) model on the differences of the logs of the prices. Exhibit 7.9 gives the estimates of θ by the various methods and, as we have seen earlier, the method-of-moments estimate differs quite a bit from the others. The others are nearly equal given their standard errors of about 0.07.

Exhibit 7.9 Estimation for the Difference of Logs of the Oil Price Series   

<table><tr><td>Parameter</td><td>Method-of-Moments Estimate</td><td>Conditional SS Estimate</td><td>Unconditional SS Estimate</td><td>Maximum Likelihood Estimate</td><td>n</td></tr><tr><td>θ</td><td>-0.2225</td><td>-0.2731</td><td>-0.2954</td><td>-0.2956</td><td>241</td></tr></table>

```txt
> data(oil.price)  
> arima(log(oil.price), order=c(0,1,1), method='CSS')  
> arima(log(oil.price), order=c(0,1,1), method='ML') 
```

# 7.6 Bootstrapping ARIMA Models

In Section 7.4, we summarized some approximate normal distribution results for the estimator $\hat { \gamma }$ , where $\gamma$ is the vector consisting of all the ARMA parameters. These normal approximations are accurate for large samples, and statistical software generally uses those results in calculating and reporting standard errors. The standard error of some complex function of the model parameters, for example the quasi-period of the model, if it exists, is then usually obtained by the delta method. However, the general theory provides no practical guidance on how large the sample size should be for the normal approximation to be reliable. Bootstrap methods (Efron and Tibshirani, 1993; Davison and Hinkley, 2003) provide an alternative approach to assessing the uncertainty of an estimator and may be more accurate for small samples. There are several variants of the bootstrap method for dependent data—see Politis (2003). We shall confine our discussion to the parametric bootstrap that generates the bootstrap time series $\boldsymbol { Y } _ { 1 } ^ { * }$ , $Y _ { 2 } ^ { ' } ,$ … , , $\boldsymbol { Y } _ { n } ^ { * }$ by simulation from the fitted ARIMA $( p , d , q )$ model. (The bootstrap may be done by fixing the first $p + d$ initial values of $Y ^ { ^ { * } }$ to those of the observed data. For stationary models, an alternative procedure is to simulate stationary realizations from the fitted model, which can be done approximately by simulating a long time series from the fitted model and then deleting the transient initial segment of the simulated data—the so-called burn-in.) If the errors are assumed to be normally distributed, the errors may be drawn randomly and with replacement from $N ( 0 , \hat { \sigma } _ { e } ^ { 2 } )$ . For the case of an unknown error distribution, the errors can be drawn randomly and with replacement from the residuals of the fitted model. For each bootstrap series, let $\hat { \gamma } ^ { * }$ be the estimator computed based on the bootstrap time series data using the method of full maximum likelihood estimation assuming stationarity. (Other estimation methods may be used.) The bootstrap is replicated, say, $B$ times. (For example, $B = 1 0 0 0$ .) From the $B$ bootstrap parameter estimates, we can form an empirical distribution and use it to calibrate the uncertainty in $\hat { \gamma }$ . Suppose we are interested in estimating some function of $\gamma$ , say $h ( \gamma )$ —for example, the AR(1) coefficient. Using the percentile method, a $9 5 \%$ bootstrap confidence interval for $h ( \gamma )$ can be obtained as the interval from the 2.5 percentile to the 97.5 percentile of the bootstrap distribution of $h ( \hat { \gamma } ^ { * } )$ .

We illustrate the bootstrap method with the hare data. The bootstrap $9 5 \%$ confidence intervals reported in the first row of the table in Exhibit 7.10 are based on the bootstrap obtained by conditioning on the initial three observations and assuming normal errors. Those in the second row are obtained using the same method except that the errors are drawn from the residuals. The third and fourth rows report the confidence intervals based on the stationary bootstrap with a normal error distribution for the third row and the empirical residual distribution for the fourth row. The fifth row in the table shows the theoretical $9 5 \%$ confidence intervals based on the large-sample distribution results for the estimators. In particular, the bootstrap time series for the first bootstrap method is generated recursively using the equation

$$
Y _ {t} ^ {*} - \hat {\phi} _ {1} Y _ {t - 1} ^ {*} - \hat {\phi} _ {2} Y _ {t - 2} ^ {*} - \hat {\phi} _ {3} Y _ {t - 3} ^ {*} = \hat {\theta} _ {0} + e _ {t} ^ {*} \tag {7.6.1}
$$

for $t = 4$ , 5,…, 31, where the $e _ { t } ^ { * }$ are chosen independently from $N ( 0 , \hat { \sigma } _ { e } ^ { 2 } )$ , $\boldsymbol { Y } _ { 1 } ^ { * } = \boldsymbol { Y } _ { 1 }$ , $Y _ { 2 } ^ { * } = Y _ { 2 }$ , ${ \cal Y } _ { 3 } ^ { * } = { \cal Y } _ { 3 }$ ; and the parameters are set to be the estimates from the AR(3) model fitted to the (square root transformed) hare data with $\hat { \Theta } _ { 0 } = \hat { \mu } ( 1 - \hat { \phi } _ { 1 } - \hat { \phi } _ { 2 } - \hat { \phi } _ { 3 } )$ . All results are based on about 1000 bootstrap replications, but full maximum likelihood estimation fails for $6 . 3 \%$ , $6 . 3 \%$ , $3 . 8 \%$ , and $4 . 8 \%$ of 1000 cases for the four bootstrap methods I, II, III, and IV, respectively.

<table><tr><td>Exhibit 7.10</td><td colspan="5">Bootstrap and Theoretical Confidence Intervals for the AR(3) Model Fitted to the Hare Data</td></tr><tr><td>Method</td><td>ar1</td><td>ar2</td><td>ar3</td><td>intercept</td><td>noise var.</td></tr><tr><td>I</td><td>(0.593, 1.269)</td><td>(-0.655, 0.237)</td><td>(-0.666, -0.018)</td><td>(5.115, 6.394)</td><td>(0.551, 1.546)</td></tr><tr><td>II</td><td>(0.612, 1.296)</td><td>(-0.702, 0.243)</td><td>(-0.669, -0.026)</td><td>(5.004, 6.324)</td><td>(0.510, 1.510)</td></tr><tr><td>III</td><td>(0.699, 1.369)</td><td>(-0.746, 0.195)</td><td>(-0.666, -0.021)</td><td>(5.056, 6.379)</td><td>(0.499, 1.515)</td></tr><tr><td>IV</td><td>(0.674, 1.389)</td><td>(-0.769, 0.194)</td><td>(-0.665, -0.002)</td><td>(4.995, 6.312)</td><td>(0.477, 1.530)</td></tr><tr><td>Theoretical</td><td>(0.684, 1.42)</td><td>(-0.8058, 0.3474)</td><td>(-0.7684, -0.01776)</td><td>(5.032, 6.353)</td><td>(0.536, 1.597)</td></tr><tr><td colspan="6">&gt; See the Chapter 7 R scripts file for the extensive code required to generate these results.</td></tr></table>

All four methods yield similar bootstrap confidence intervals, although the conditional bootstrap approach generally yields slightly narrower confidence intervals. This is expected, as the conditional bootstrap time series bear more resemblance to each other because all are subject to identical initial conditions. The bootstrap confidence intervals are generally slightly wider than their theoretical counterparts that are derived from the large-sample results. Overall, we can draw the inference that the $\Phi _ { 2 }$ coefficient estimate is insignificant, whereas both the $\Phi _ { 1 }$ and $\phi _ { 3 }$ coefficient estimates are significant at the $5 \%$ significance level.

The bootstrap method has the advantage of allowing easy construction of confidence intervals for a model characteristic that is a nonlinear function of the model parameters. For example, the characteristic AR polynomial of the fitted AR(3) model for the hare data admits a pair of complex roots. Indeed, the roots are $0 . 8 4 \pm 0 . 6 4 7 i$ and −2.26, where $i = \sqrt { - 1 }$ . The two complex roots can be written in polar form: 1.06exp(± 0.657i). As in the discussion of the quasi-period for the AR(2) model on page 74, the quasi-period of the fitted AR(3) model can be defined as $2 \pi / 0 . 6 5 7 = 9 . 5 7$ . Thus, the fitted model suggests that the hare abundance underwent cyclical fluctuation with a period of about 9.57 years. The interesting question of constructing a $9 5 \%$ confidence interval for the quasi-period could be studied using the delta method. However, this will be quite complex, as the quasi-period is a complicated function of the parameters. But the bootstrap provides a simple solution: For each set of bootstrap parameter estimates, we can compute the quasi-period and hence obtain the bootstrap distribution of the quasi-period. Confidence intervals for the quasi-period can then be constructed using the percentile method, and the shape of the distribution can be explored via the histogram of the bootstrap quasi-period estimates. (Note that the quasi-period will be unde-

fined whenever the roots of the AR characteristic equation are all real numbers.) Among the 1000 stationary bootstrap time series obtained by simulating from the fitted model with the errors drawn randomly from the residuals with replacement, 952 series lead to successful full maximum likelihood estimation. All but one of the 952 series have well-defined quasi-periods, and the histogram of these is shown in Exhibit 7.11. The histogram shows that the sampling distribution of the quasi-period estimate is slightly skewed to the right.† The Q-Q normal plot (Exhibit 7.12) suggests that the quasi-period estimator has, furthermore, a thick-tailed distribution. Thus, the delta method and the corresponding normal distribution approximation may be inappropriate for approximating the sampling distribution of the quasi-period estimator. Finally, using the percentile method, a $9 5 \%$ confidence interval of the quasi-period is found to be (7.84,11.34).

![](images/0da0427f30a6fd9c619ab34e5f062e68f3c7900f948a245134348423ca7a7020.jpg)  
Exhibit 7.11 Histogram of Bootstrap Quasi-period Estimates

```txt
> win.graph(width=3.9,height=3.8,pointsize=8)  
> hist(period.replace,prob=T,xlab='Quasi-period',axes=F, xlim=c(5,16))  
> axis(2); axis(1,c(4,6,8,10,12,14,16),c(4,6,8,10,12,14,NA)) 
```

![](images/797e7d28f26d20b0dfa269b1275912b2bd30421ecab4d193a167f36244083743.jpg)  
Exhibit 7.12 Q-Q Normal Plot of Bootstrap Quasi-period Estimates

```javascript
> win.graph(width=2.5,height=2.5,pointsize=8)  
> qqnorm(period.replace); qqline(period.replace) 
```

# 7.7 Summary

This chapter delved into the estimation of the parameters of ARIMA models. We considered estimation criteria based on the method of moments, various types of least squares, and maximizing the likelihood function. The properties of the various estimators were given, and the estimators were illustrated both with simulated and actual time series data. Bootstrapping with ARIMA models was also discussed and illustrated.

# EXERCISES

7.1 From a series of length 100, we have computed $r _ { 1 } = 0 . 8$ , $r _ { 2 } = 0 . 5$ , $r _ { 3 } = 0 . 4$ , ${ \overline { { Y } } } = 2$ and a sample variance of 5. If we assume that an AR(2) model with a constant term is appropriate, how can we get (simple) estimates of $\Phi _ { 1 }$ , φ2, θ0, and $\sigma _ { e } ^ { 2 } ?$   
7.2 Assuming that the following data arise from a stationary process, calculate method-of-moments estimates of μ, γ0, and ρ1: 6, 5, 4, 6, 4.   
7.3 If $\{ Y _ { t } \}$ satisfies an AR(1) model with $\phi$ of about 0.7, how long of a series do we need to estimate $\boldsymbol \Phi = \mathsf { \rho } _ { \mathsf { J } }$ with $9 5 \%$ confidence that our estimation error is no more than $\pm 0 . 1 2$   
7.4 Consider an MA(1) process for which it is known that the process mean is zero. Based on a series of length $n = 3$ , we observe $Y _ { 1 } = 0$ , $Y _ { 2 } = - 1$ , and $Y _ { 3 } = { \% }$ .

(a) Show that the conditional least-squares estimate of θ is $\%$ .

(b) Find an estimate of the noise variance. (Hint: Iterative methods are not needed in this simple case.)

7.5 Given the data $Y _ { 1 } = 1 0$ , $Y _ { 2 } = 9$ , and $Y _ { 3 } = 9 . 5$ , we wish to fit an IMA(1,1) model without a constant term.

(a) Find the conditional least squares estimate of θ. (Hint: Do Exercise 7.4 first.)   
(b) Estimate $\sigma _ { e } ^ { 2 }$

7.6 Consider two different parameterizations of the AR(1) process with nonzero mean:

Model I. $Y _ { t } - \mu = \phi ( Y _ { t - 1 } - \mu ) + e _ { t } .$

Model II. $Y _ { t } = \Phi Y _ { t - 1 } + \Theta _ { 0 } + e _ { t } .$

We want to estimate $\phi$ and $\mu$ or $\boldsymbol { \Phi }$ and $\theta _ { 0 }$ using conditional least squares conditional on $Y _ { 1 }$ . Show that with Model I we are led to solve nonlinear equations to obtain the estimates, while with Model II we need only solve linear equations.

7.7 Verify Equation (7.1.4) on page 150.   
7.8 Consider an ARMA(1,1) model with $\phi = 0 . 5$ and $\textstyle \Theta = 0 . 4 5$

(a) For $n = 4 8$ , evaluate the variances and correlation of the maximum likelihood estimators of $\phi$ and θ using Equations (7.4.13) on page 161. Comment on the results.   
(b) Repeat part (a) but now with $n = 1 2 0$ . Comment on the new results.

7.9 Simulate an MA(1) series with $\theta = 0 . 8$ and $n = 4 8$ .

(a) Find the method-of-moments estimate of θ.   
(b) Find the conditional least squares estimate of θ and compare it with part (a).   
(c) Find the maximum likelihood estimate of θ and compare it with parts (a) and (b).   
(d) Repeat parts (a), (b), and (c) with a new simulated series using the same parameters and same sample size. Compare your results with your results from the first simulation.

7.10 Simulate an MA(1) series with $\theta = - 0 . 6$ and $n = 3 6$ .

(a) Find the method-of-moments estimate of θ.   
(b) Find the conditional least squares estimate of θ and compare it with part (a).   
(c) Find the maximum likelihood estimate of θ and compare it with parts (a) and (b).   
(d) Repeat parts (a), (b), and (c) with a new simulated series using the same parameters and same sample size. Compare your results with your results from the first simulation.

7.11 Simulate an MA(1) series with $\theta = - 0 . 6$ and $n = 4 8$ .

(a) Find the maximum likelihood estimate of θ.   
(b) If your software permits, repeat part (a) many times with a new simulated series using the same parameters and same sample size.   
(c) Form the sampling distribution of the maximum likelihood estimates of θ.   
(d) Are the estimates (approximately) unbiased?   
(e) Calculate the variance of your sampling distribution and compare it with the large-sample result in Equation (7.4.11) on page 161.

7.12 Repeat Exercise 7.11 using a sample size of $n = 1 2 0$ .

7.13 Simulate an AR(1) series with $\phi = 0 . 8$ and $n = 4 8$ .

(a) Find the method-of-moments estimate of $\phi$   
(b) Find the conditional least squares estimate of $\phi$ and compare it with part (a).   
(c) Find the maximum likelihood estimate of $\boldsymbol { \Phi }$ and compare it with parts (a) and (b).   
(d) Repeat parts (a), (b), and (c) with a new simulated series using the same parameters and same sample size. Compare your results with your results from the first simulation.

7.14 Simulate an AR(1) series with $\phi = - 0 . 5$ and $n = 6 0$ .

(a) Find the method-of-moments estimate of $\phi$   
(b) Find the conditional least squares estimate of $\phi$ and compare it with part (a).   
(c) Find the maximum likelihood estimate of $\boldsymbol { \Phi }$ and compare it with parts (a) and (b).   
(d) Repeat parts (a), (b), and (c) with a new simulated series using the same parameters and same sample size. Compare your results with your results from the first simulation.

7.15 Simulate an AR(1) series with $\phi = 0 . 7$ and $n = 1 0 0$ .

(a) Find the maximum likelihood estimate of $\phi$   
(b) If your software permits, repeat part (a) many times with a new simulated series using the same parameters and same sample size.   
(c) Form the sampling distribution of the maximum likelihood estimates of $\phi$   
(d) Are the estimates (approximately) unbiased?   
(e) Calculate the variance of your sampling distribution and compare it with the large-sample result in Equation (7.4.9) on page 161.

7.16 Simulate an AR(2) series with $\phi _ { 1 } = 0 . 6$ , $\phi _ { 2 } = 0 . 3$ , and $n = 6 0$ .

(a) Find the method-of-moments estimates of $\Phi _ { 1 }$ and $\Phi _ { 2 }$   
(b) Find the conditional least squares estimates of $\Phi _ { 1 }$ and $\Phi _ { 2 }$ and compare them with part (a).   
(c) Find the maximum likelihood estimates of $\Phi _ { 1 }$ and $\Phi _ { 2 }$ and compare them with parts (a) and (b).   
(d) Repeat parts (a), (b), and (c) with a new simulated series using the same parameters and same sample size. Compare these results to your results from the first simulation.

7.17 Simulate an ARMA(1,1) series with $\phi = 0 . 7$ , $\theta = 0 . 4$ , and $n = 7 2$ .

(a) Find the method-of-moments estimates of $\phi$ and θ.   
(b) Find the conditional least squares estimates of $\phi$ and θ and compare them with part (a).   
(c) Find the maximum likelihood estimates of $\phi$ and θ and compare them with parts (a) and (b).   
(d) Repeat parts (a), (b), and (c) with a new simulated series using the same parameters and same sample size. Compare your new results with your results from the first simulation.

7.18 Simulate an AR(1) series with $\phi = 0 . 6$ , $n = 3 6$ but with error terms from a $t$ -distribution with 3 degrees of freedom.

(a) Display the sample PACF of the series. Is an AR(1) model suggested?   
(b) Estimate $\phi$ from the series and comment on the results.   
(c) Repeat parts (a) and (b) with a new simulated series under the same conditions.

7.19 Simulate an MA(1) series with $\theta = - 0 . 8$ , $n = 6 0$ but with error terms from a $t$ -distribution with 4 degrees of freedom.

(a) Display the sample ACF of the series. Is an MA(1) model suggested?   
(b) Estimate θ from the series and comment on the results.   
(c) Repeat parts (a) and (b) with a new simulated series under the same conditions.

7.20 Simulate an AR(2) series with $\phi _ { 1 } = 1 . 0$ , $\phi _ { 2 } = - 0 . 6$ , $n = 4 8$ but with error terms from a t-distribution with 5 degrees of freedom.

(a) Display the sample PACF of the series. Is an AR(2) model suggested?   
(b) Estimate $\Phi _ { 1 }$ and $\Phi _ { 2 }$ from the series and comment on the results.   
(c) Repeat parts (a) and (b) with a new simulated series under the same conditions.

7.21 Simulate an ARMA(1,1) series with $\phi = 0 . 7$ , $\theta = - 0 . 6$ , $n = 4 8$ but with error terms from a t-distribution with 6 degrees of freedom.

(a) Display the sample EACF of the series. Is an ARMA(1,1) model suggested?   
(b) Estimate $\phi$ and θ from the series and comment on the results.   
(c) Repeat parts (a) and (b) with a new simulated series under the same conditions.

7.22 Simulate an AR(1) series with $\phi = 0 . 6$ , $n = 3 6$ but with error terms from a chi-square distribution with 6 degrees of freedom.

(a) Display the sample PACF of the series. Is an AR(1) model suggested?   
(b) Estimate $\phi$ from the series and comment on the results.   
(c) Repeat parts (a) and (b) with a new simulated series under the same conditions.

7.23 Simulate an MA(1) series with $\theta = - 0 . 8$ , $n = 6 0$ but with error terms from a chi-square distribution with 7 degrees of freedom.

(a) Display the sample ACF of the series. Is an MA(1) model suggested?   
(b) Estimate θ from the series and comment on the results.   
(c) Repeat parts (a) and (b) with a new simulated series under the same conditions.

7.24 Simulate an AR(2) series with $\phi _ { 1 } = 1 . 0$ , $\phi _ { 2 } = - 0 . 6$ , $n = 4 8$ but with error terms from a chi-square distribution with 8 degrees of freedom.

(a) Display the sample PACF of the series. Is an AR(2) model suggested?   
(b) Estimate $\Phi _ { 1 }$ and $\Phi _ { 2 }$ from the series and comment on the results.   
(c) Repeat parts (a) and (b) with a new simulated series under the same conditions.

7.25 Simulate an ARMA(1,1) series with $\phi = 0 . 7$ , $\theta = - 0 . 6$ , $n = 4 8$ but with error terms from a chi-square distribution with 9 degrees of freedom.

(a) Display the sample EACF of the series. Is an ARMA(1,1) model suggested?   
(b) Estimate $\phi$ and θ from the series and comment on the results.   
(c) Repeat parts (a) and (b) with a new series under the same conditions.

7.26 Consider the AR(1) model specified for the color property time series displayed in Exhibit 1.3 on page 3. The data are in the file named color.

(a) Find the method-of-moments estimate of $\phi$ .   
(b) Find the maximum likelihood estimate of $\phi$ and compare it with part (a).

7.27 Exhibit 6.31 on page 139 suggested specifying either an AR(1) or possibly an AR(4) model for the difference of the logarithms of the oil price series. The data are in the file named oil.price.

(a) Estimate both of these models using maximum likelihood and compare it with the results using the AIC criteria.   
(b) Exhibit 6.32 on page 140 suggested specifying an MA(1) model for the difference of the logs. Estimate this model by maximum likelihood and compare to your results in part (a).

7.28 The data file named deere3 contains 57 consecutive values from a complex machine tool at Deere & Co. The values given are deviations from a target value in units of ten millionths of an inch. The process employs a control mechanism that resets some of the parameters of the machine tool depending on the magnitude of deviation from target of the last item produced.

(a) Estimate the parameters of an AR(1) model for this series.   
(b) Estimate the parameters of an AR(2) model for this series and compare the results with those in part (a).

7.29 The data file named robot contains a time series obtained from an industrial robot. The robot was put through a sequence of maneuvers, and the distance from a desired ending point was recorded in inches. This was repeated 324 times to form the time series.

(a) Estimate the parameters of an AR(1) model for these data.   
(b) Estimate the parameters of an IMA(1,1) model for these data.   
(c) Compare the results from parts (a) and (b) in terms of AIC.

7.30 The data file named days contains accounting data from the Winegard Co. of Burlington, Iowa. The data are the number of days until Winegard receives payment for 130 consecutive orders from a particular distributor of Winegard products. (The name of the distributor must remain anonymous for confidentiality reasons.) The time series contains outliers that are quite obvious in the time series plot.

(a) Replace each of the unusual values with a value of 35 days, a much more typical value, and then estimate the parameters of an MA(2) model.   
(b) Now assume an MA(5) model and estimate the parameters. Compare these results with those obtained in part (a).

7.31 Simulate a time series of length $n = 4 8$ from an AR(1) model with $\Phi = 0 . 7$ . Use that series as if it were real data. Now compare the theoretical asymptotic distribution of the estimator of $\phi$ with the distribution of the bootstrap estimator of $\phi$ .

7.32 The industrial color property time series was fitted quite well by an AR(1) model. However, the series is rather short, with $n = 3 5$ . Compare the theoretical asymptotic distribution of the estimator of $\phi$ with the distribution of the bootstrap estimator of $\phi$ . The data are in the file named color.

# CHAPTER 8

# MODEL DIAGNOSTICS

We have now discussed methods for specifying models and for efficiently estimating the parameters in those models. Model diagnostics, or model criticism, is concerned with testing the goodness of fit of a model and, if the fit is poor, suggesting appropriate modifications. We shall present two complementary approaches: analysis of residuals from the fitted model and analysis of overparameterized models; that is, models that are more general than the proposed model but that contain the proposed model as a special case.

# 8.1 Residual Analysis

We already used the basic ideas of residual analysis in Section 3.6 on page 42 when we checked the adequacy of fitted deterministic trend models. With autoregressive models, residuals are defined in direct analogy to that earlier work. Consider in particular an AR(2) model with a constant term:

$$
Y _ {t} = \phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + \theta_ {0} + e _ {t} \tag {8.1.1}
$$

Having estimated $\Phi _ { 1 }$ , φ2, and $\theta _ { 0 }$ , the residuals are defined as

$$
\hat {e} _ {t} = Y _ {t} - \hat {\phi} _ {1} Y _ {t - 1} - \hat {\phi} _ {2} Y _ {t - 2} - \hat {\theta} _ {0} \tag {8.1.2}
$$

For general ARMA models containing moving average terms, we use the inverted, infinite autoregressive form of the model to define residuals. For simplicity, we assume that $\theta _ { 0 }$ is zero. From the inverted form of the model, Equation (4.5.5) on page 80, we have

$$
Y _ {t} = \pi_ {1} Y _ {t - 1} + \pi_ {2} Y _ {t - 2} + \pi_ {3} Y _ {t - 3} + \dots + e _ {t}
$$

so that the residuals are defined as

$$
\hat {e} _ {t} = Y _ {t} - \hat {\pi} _ {1} Y _ {t - 1} - \hat {\pi} _ {2} Y _ {t - 2} - \hat {\pi} _ {3} Y _ {t - 3} - \dots \tag {8.1.3}
$$

Here the $\pi$ ’s are not estimated directly but rather implicitly as functions of the $\phi$ ’s and θ’s. In fact, the residuals are not calculated using this equation but as a by-product of the estimation of the $\boldsymbol { \Phi }$ ’s and θ’s. In Chapter 9, we shall argue, that

$$
\hat {\boldsymbol {Y}} _ {t} = \hat {\boldsymbol {\pi}} _ {1} \boldsymbol {Y} _ {t - 1} + \hat {\boldsymbol {\pi}} _ {2} \boldsymbol {Y} _ {t - 2} + \hat {\boldsymbol {\pi}} _ {3} \boldsymbol {Y} _ {t - 3} + \dots
$$

is the best forecast of $Y _ { t }$ based on Yt − 1, Yt − 2, Yt − 3,… . Thus Equation (8.1.3) can be rewritten as

$$
\text {r e s i d u a l} = \text {a c t u a l} - \text {p r e d i c t e d}
$$

in direct analogy with regression models. Compare this with Section 3.6 on page 42.

If the model is correctly specified and the parameter estimates are reasonably close to the true values, then the residuals should have nearly the properties of white noise. They should behave roughly like independent, identically distributed normal variables with zero means and common standard deviations. Deviations from these properties can help us discover a more appropriate model.

# Plots of the Residuals

Our first diagnostic check is to inspect a plot of the residuals over time. If the model is adequate, we expect the plot to suggest a rectangular scatter around a zero horizontal level with no trends whatsoever.

Exhibit 8.1 shows such a plot for the standardized residuals from the AR(1) model fitted to the industrial color property series. Standardization allows us to see residuals of unusual size much more easily. The parameters were estimated using maximum likelihood. This plot supports the model, as no trends are present.

![](images/ffabcb95a038040b095464a760fe5d3f66af174ac6537c401128ea6bb78ce27f.jpg)  
Exhibit 8.1 Standardized Residuals from AR(1) Model of Color

```javascript
> win.graph(width=4.875,height=3,pointsize=8)  
> data(color)  
> m1.color=arima(color,order=c(1,0,0)); m1.color  
> plot(rstandard(ml.color),ylab='Standardized Residuals', type='o')；abline(h=0) 
```

As a second example, we consider the Canadian hare abundance series. We estimate a subset AR(3) model with $\Phi _ { 2 }$ set to zero, as suggested by the discussion following Exhibit 7.8 on page 166. The estimated model is

$$
\sqrt {Y _ {t}} = 3. 4 8 3 + 0. 9 1 9 \sqrt {Y _ {t - 1}} - 0. 5 3 1 3 \sqrt {Y _ {t - 3}} + e _ {t} \tag {8.1.4}
$$

and the time series plot of the standardized residuals from this model is shown in Exhibit 8.2. Here we see possible reduced variation in the middle of the series and increased variation near the end of the series—not exactly an ideal plot of residuals.†

![](images/cee8e6970e02f9df3b6cd42ee1af157fc94b7e2a72118fa010b5728f602b8728.jpg)  
Exhibit 8.2 Standardized Residuals from AR(3) Model for Sqrt(Hare)

> data(hare)  
> m1.hare=arima(square(hare), order=c(3,0,0)); m1.hare  
> m2.hare=arima(square(hare), order=c(3,0,0), fixed=c(NA,0,NA,NA))  
> m2.hare  
> # Note that the intercept term given in R is actually the mean in the centered form of the ARMA model; that is, if $y(t) = \text{sqrt(hare)}$ -intercept, then the model is $y(t) = 0.919 * y(t - 1) - 0.5313 * y(t - 3) + e(t)$ > # So the 'true' intercept equals 5.6889 * (1 - 0.919 + 0.5313) = 3.483  
> plot(rstandard(m2.hare), ylab='Standardized Residuals', type='o')  
> abline(h=0)

Exhibit 8.3 displays the time series plot of the standardized residuals from the IMA(1,1) model estimated for the logarithms of the oil price time series. The model was fitted using maximum likelihood estimation. There are at least two or three residuals early in the series with magnitudes larger than 3—very unusual in a standard normal distribution.‡ Ideally, we should go back to those months and try to learn what outside factors may have influenced unusually large drops or unusually large increases in the price of oil.

![](images/018f510c9b7cc9c7c588d63ef3b7fa9ecb9b1b46e46f63c6079408b2ee4adf8c.jpg)  
Exhibit 8.3 Standardized Residuals from Log Oil Price IMA(1,1) Model

```txt
> data(oil.price)  
> m1.oil=arima(log(oil.price), order=c(0,1,1))  
> plot(rstandard(m1.oil), ylab='Standardized residuals', type='l')  
> abline(h=0) 
```

# Normality of the Residuals

As we saw in Chapter 3, quantile-quantile plots are an effective tool for assessing normality. Here we apply them to residuals.

A quantile-quantile plot of the residuals from the AR(1) model estimated for the industrial color property series is shown in Exhibit 8.4. The points seem to follow the straight line fairly closely—especially the extreme values. This graph would not lead us to reject normality of the error terms in this model. In addition, the Shapiro-Wilk normality test applied to the residuals produces a test statistic of $W = 0 . 9 7 5 4$ , which corresponds to a $p$ -value of 0.6057, and we would not reject normality based on this test.

![](images/d5c909673592d9409c75d143afc2eda44a7a39c310ac09aff86647de688c3462.jpg)  
Exhibit 8.4 Quantile-Quantile Plot: Residuals from AR(1) Color Model

> win.graph(width=2.5,height=2.5,pointsize=8)   
> qqnorm(residuals(m1.color)); qqline(residuals(m1.color))

The quantile-quantile plot for the residuals from the AR(3) model for the square root of the hare abundance time series is displayed in Exhibit 8.5. Here the extreme values look suspect. However, the sample is small $( n = 3 1$ ) and, as stated earlier, the Bonferroni criteria for outliers do not indicate cause for alarm.

![](images/fdcf0cbb89e6dbec2e404d39dd5b247bf871d06455b5ea610fee199757560508.jpg)  
Exhibit 8.5 Quantile-Quantile Plot: Residuals from AR(3) for Hare

> qqnorm(residuals(m1.hare)); qqline(residuals(m1.hare))

Exhibit 8.6 gives the quantile-quantile plot for the residuals from the IMA(1,1) model that was used to model the logarithms of the oil price series. Here the outliers are quite prominent, and we will deal with them in Chapter 11.

Exhibit 8.6 Quantile-Quantile Plot: Residuals from IMA(1,1) Model for Oil   
![](images/10f0acb836babdaabf179654fdf85d64087f4619c542fbbba40bbc81b1f92cca.jpg)  
> qqnorm(residuals(m1.oil)); qqline(residuals(m1.oil))

# Autocorrelation of the Residuals

To check on the independence of the noise terms in the model, we consider the sample autocorrelation function of the residuals, denoted $\hat { r } _ { k }$ . From Equation (6.1.3) on page 110, we know that for true white noise and large $n$ , the sample autocorrelations are approximately uncorrelated and normally distributed with zero means and variance $1 / n$ . Unfortunately, even residuals from a correctly specified model with efficiently estimated parameters have somewhat different properties. This was first explored for multiple- regression models in a series of papers by Durbin and Watson (1950, 1951, 1971) and for autoregressive models in Durbin (1970). The key reference on the distribution of residual autocorrelations in ARIMA models is Box and Pierce (1970), the results of which were generalized in McLeod (1978).

Generally speaking, the residuals are approximately normally distributed with zero means; however, for small lags $k$ and $j$ , the variance of $\hat { r } _ { k }$ can be substantially less than $1 / n$ and the estimates $\hat { r } _ { k }$ and $\hat { r } j$ can be highly correlated. For larger lags, the approximate variance $1 / n$ does apply, and further $\hat { r } _ { k }$ and $\hat { r } j$ are approximately uncorrelated.

As an example of these results, consider a correctly specified and efficiently estimated AR(1) model. It can be shown that, for large $n$ ,

$$
\operatorname {V a r} \left(\hat {r} _ {1}\right) \approx \frac {\phi^ {2}}{n} \tag {8.1.5}
$$

$$
\operatorname {V a r} \left(\hat {r} _ {k}\right) \approx \frac {1 - \left(1 - \phi^ {2}\right) \phi^ {2 k - 2}}{n} \text {f o r} k > 1 \tag {8.1.6}
$$

$$
C o r r \left(\hat {r} _ {1}, \hat {r} _ {k}\right) \approx - s i g n (\phi) \frac {\left(1 - \phi^ {2}\right) \phi^ {k - 2}}{1 - \left(1 - \phi^ {2}\right) \phi^ {2 k - 2}} \text {f o r} k > 1 \tag {8.1.7}
$$

where

$$
s i g n (\phi) = \left\{ \begin{array}{l l} 1 & \text {i f} \phi > 0 \\ 0 & \text {i f} \phi = 0 \\ - 1 & \text {i f} \phi <   0 \end{array} \right.
$$

The table in Exhibit 8.7 illustrates these formulas for a variety of values of $\boldsymbol { \Phi }$ and $k$ . Notice that $V a r ( \hat { r } _ { 1 } ) \approx 1 / n$ is a reasonable approximation for $k \geq 2$ over a wide range of $\phi$ -values.

Exhibit 8.7 Approximations for Residual Autocorrelations in AR(1) Models   

<table><tr><td>φ</td><td>0.3</td><td>0.5</td><td>0.7</td><td>0.9</td><td>φ</td><td>0.3</td><td>0.5</td><td>0.7</td><td>0.9</td></tr><tr><td>k</td><td colspan="4">Standard deviation of \(\hat{r}_k\)times\(\sqrt{n}\)</td><td colspan="5">Correlation \(\hat{r}_1\)with \(\hat{r}_k\)</td></tr><tr><td>1</td><td>0.30</td><td>0.50</td><td>0.70</td><td>0.90</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td></tr><tr><td>2</td><td>0.96</td><td>0.90</td><td>0.87</td><td>0.92</td><td>-0.95</td><td>-0.83</td><td>-0.59</td><td>-0.21</td><td></td></tr><tr><td>3</td><td>1.00</td><td>0.98</td><td>0.94</td><td>0.94</td><td>-0.27</td><td>-0.38</td><td>-0.38</td><td>-0.18</td><td></td></tr><tr><td>4</td><td>1.00</td><td>0.99</td><td>0.97</td><td>0.95</td><td>-0.08</td><td>-0.19</td><td>-0.26</td><td>-0.16</td><td></td></tr><tr><td>5</td><td>1.00</td><td>1.00</td><td>0.99</td><td>0.96</td><td>-0.02</td><td>-0.09</td><td>-0.18</td><td>-0.14</td><td></td></tr><tr><td>6</td><td>1.00</td><td>1.00</td><td>0.99</td><td>0.97</td><td>-0.01</td><td>-0.05</td><td>-0.12</td><td>-0.13</td><td></td></tr><tr><td>7</td><td>1.00</td><td>1.00</td><td>1.00</td><td>0.97</td><td>-0.00</td><td>-0.02</td><td>-0.09</td><td>-0.12</td><td></td></tr><tr><td>8</td><td>1.00</td><td>1.00</td><td>1.00</td><td>0.98</td><td>-0.00</td><td>-0.01</td><td>-0.06</td><td>-0.10</td><td></td></tr><tr><td>9</td><td>1.00</td><td>1.00</td><td>1.00</td><td>0.99</td><td>-0.00</td><td>-0.00</td><td>-0.03</td><td>-0.08</td><td></td></tr></table>

If we apply these results to the AR(1) model that was estimated for the industrial color property time series with $\stackrel { \wedge } { \phi } = 0 . 5 \stackrel { \wedge } { 7 }$ and $n = 3 5$ , we obtain the results shown in Exhibit 8.8.

Exhibit 8.8 Approximate Standard Deviations of Residual ACF values   

<table><tr><td>Lag k</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>&gt;5</td></tr><tr><td>√Varhat(ˆrk)</td><td>0.096</td><td>0.149</td><td>0.163</td><td>0.167</td><td>0.168</td><td>0.169</td></tr></table>

A graph of the sample ACF of these residuals is shown in Exhibit 8.9. The dashed horizontal lines plotted are based on the large lag standard error of $\pm 2 / \sqrt { n }$ . There is no evidence of autocorrelation in the residuals of this model.

![](images/e239d41191ddc2283fae779cb6f6f0c040bdcf6ff461f90e68791f5ec128a729.jpg)  
Exhibit 8.9 Sample ACF of Residuals from AR(1) Model for Color

```txt
> win.graph(width=4.875,height=3,pointsize=8)  
> acf(residuals(m1.color)) 
```

For an AR(2) model, it can be shown that

$$
V a r \left(\hat {r} _ {1}\right) \approx \frac {\phi_ {2} ^ {2}}{n} \tag {8.1.8}
$$

and

$$
\operatorname {V a r} \left(\hat {r} _ {2}\right) \approx \frac {\phi_ {2} ^ {2} + \phi_ {1} ^ {2} (1 + \phi_ {2}) ^ {2}}{n} \tag {8.1.9}
$$

If the AR(2) parameters are not too close to the stationarity boundary shown in Exhibit 4.17 on page 72, then

$$
\operatorname {V a r} \left(\hat {r} _ {k}\right) \approx \frac {1}{n} \quad \text {f o r} k \geq 3 \tag {8.1.10}
$$

If we fit an AR(2) model† by maximum likelihood to the square root of the hare abundance series, we find that $\hat { \Phi } _ { 1 } = 1 . 3 5 1$ and $\hat { \Phi } _ { 2 } = - 0 . 7 7 6$ . Thus we have

$$
\sqrt {V a \hat {r} (\hat {r} _ {1})} \approx \frac {| - 0 . 7 7 6 |}{\sqrt {3 5}} = 0. 1 3 1
$$

$$
\sqrt {V a \hat {r} (\hat {r} _ {2})} \approx \sqrt {\frac {(- 0 . 7 7 6) ^ {2} + (1 . 3 5 1) ^ {2} (1 + (- 0 . 7 7 6)) ^ {2}}{3 5}} = 0. 1 4 1
$$

$$
\sqrt {V a \hat {r} (\hat {r} _ {k})} \approx 1 / \sqrt {3 5} = 0. 1 6 9 \text {f o r} k \geq 3
$$

Exhibit 8.10 displays the sample ACF of the residuals from the AR(2) model of the square root of the hare abundance. The lag 1 autocorrelation here equals $- 0 . 2 6 1$ , which is close to 2 standard errors below zero but not quite. The lag 4 autocorrelation equals −0.318, but its standard error is 0.169. We conclude that the graph does not show statistically significant evidence of nonzero autocorrelation in the residuals.†

Exhibit 8.10 Sample ACF of Residuals from AR(2) Model for Hare   
![](images/53f02c270e8a092175ef6df05fd15d5cd2a3f2fdefe361ca3f4c222c815267b8.jpg)  
> acf(residuals(arima(sqrt(hare),order=c(2,0,0))))

With monthly data, we would pay special attention to possible excessive autocorrelation in the residuals at lags 12, 24, and so forth. With quarterly series, lags 4, 8, and so forth would merit special attention. Chapter 10 contains examples of these ideas.

It can be shown that results analogous to those for AR models hold for MA models. In particular, replacing $\phi$ by θ in Equations (8.1.5), (8.1.6), and( 8.1.7) gives the results for the MA(1) case. Similarly, results for the MA(2) case can be stated by replacing $\Phi _ { 1 }$ and $\phi _ { 2 }$ by $\theta _ { 1 }$ and $\theta _ { 2 }$ , respectively, in Equations (8.1.8), (8.1.9), and (8.1.10). Results for general ARMA models may be found in Box and Pierce (1970) and McLeod (1978).

# The Ljung-Box Test

In addition to looking at residual correlations at individual lags, it is useful to have a test that takes into account their magnitudes as a group. For example, it may be that most of the residual autocorrelations are moderate, some even close to their critical values, but, taken together, they seem excessive. Box and Pierce (1970) proposed the statistic

$$
Q = n \left(\hat {r} _ {1} ^ {2} + \hat {r} _ {2} ^ {2} + \dots + \hat {r} _ {K} ^ {2}\right) \tag {8.1.11}
$$

to address this possibility. They showed that if the correct ARMA $( p , q )$ model is estimated, then, for large n, $Q$ has an approximate chi-square distribution with $K - p - q$

degrees of freedom. Fitting an erroneous model would tend to inflate $Q$ . Thus, a general “portmanteau” test would reject the ARMA $( p , q )$ model if the observed value of $Q$ exceeded an appropriate critical value in a chi-square distribution with $K - p - q$ degrees of freedom. (Here the maximum lag $K$ is selected somewhat arbitrarily but large enough that the $\boldsymbol { \Psi }$ -weights are negligible for $j > K .$ .)

The chi-square distribution for $Q$ is based on a limit theorem as $n  \infty$ , but Ljung and Box (1978) subsequently discovered that even for $n = 1 0 0$ , the approximation is not satisfactory. By modifying the $Q$ statistic slightly, they defined a test statistic whose null distribution is much closer to chi-square for typical sample sizes. The modified Box-Pierce, or Ljung-Box, statistic is given by

$$
Q _ {*} = n (n + 2) \left(\frac {\hat {r} _ {1} ^ {2}}{n - 1} + \frac {\hat {r} _ {2} ^ {2}}{n - 2} + \dots + \frac {\hat {r} _ {K} ^ {2}}{n - K}\right) \tag {8.1.12}
$$

Notice that since $( n + 2 ) / ( n - k ) > 1$ for every $k \geq 1$ , we have $Q * > Q$ , which partly explains why the original statistic $Q$ tended to overlook inadequate models. More details on the exact distributions of $Q \ast$ and $Q$ for finite samples can be found in Ljung and Box (1978), see also Davies, Triggs, and Newbold (1977).

Exhibit 8.11 lists the first six autocorrelations of the residuals from the AR(1) fitted model for the color property series. Here $n = 3 5$ .

Exhibit 8.11 Residual Autocorrelation Values from AR(1) Model for Color Lag k 1 2 3 4 5 6 Residual ACF −0.051 0.032 0.047 0.021 −0.017 −0.019   

<table><tr><td>&gt; acf(residuals(m1.color), plot=F) $acf</td></tr><tr><td>&gt; signif(acf(residuals(m1.color), plot=F) $acf[1:6], 2)</td></tr><tr><td>&gt; # display the first 6 acf values to 2 significant digits</td></tr></table>

The Ljung-Box test statistic with $K = 6$ is equal to

$$
\begin{array}{l} Q _ {*} = 3 5 (3 5 + 2) \left(\frac {(- 0 . 0 5 1) ^ {2}}{3 5 - 1} + \frac {(0 . 0 3 2) ^ {2}}{3 5 - 2} + \frac {(0 . 0 4 7) ^ {2}}{3 5 - 3}\right) \\ \left. + \frac {(0 . 0 2 1) ^ {2}}{3 5 - 4} + \frac {(- 0 . 0 1 7) ^ {2}}{3 5 - 5} + \frac {(- 0 . 0 1 9) ^ {2}}{3 5 - 6}\right) \approx 0. 2 8 \\ \end{array}
$$

This is referred to a chi-square distribution with $6 - 1 = 5$ degrees of freedom. This leads to a $p$ -value of 0.998, so we have no evidence to reject the null hypothesis that the error terms are uncorrelated.

Exhibit 8.12 shows three of our diagnostic tools in one display—a sequence plot of the standardized residuals, the sample ACF of the residuals, and $p$ -values for the Ljung-Box test statistic for a whole range of values of $K$ from 5 to 15. The horizontal dashed line at $5 \%$ helps judge the size of the $p$ -values. In this instance, everything looks very good. The estimated AR(1) model seems to be capturing the dependence structure of the color property time series quite well.

![](images/6e780f9196cf89a7210e91251d5d924cb24443b8dd19a4d62e825d839f734fc2.jpg)  
Exhibit 8.12 Diagnostic Display for the AR(1) Model of Color Property

![](images/3789f03b23202be44a0e2d2b11f77b8b0751fed2a54dcc53d98a4a44ba73f292.jpg)

![](images/482e8657df2dc963be6590e5a31d38b4a1550dc119cd0d3f8884618564657503.jpg)  
> win.graph(width $\mathord { \left. \begin{array} { r l r } \end{array} \right. } = 4$ .875,height=4.5)   
> tsdiag(m1.color,gof=15,omit.initial $\mathbf { \Psi } = \mathbf { F }$ )

As in Chapter 3, the runs test may also be used to assess dependence in error terms via the residuals. Applying the test to the residuals from the AR(3) model for the Canadian hare abundance series, we obtain expected runs of 16.09677 versus observed runs of 18. The corresponding $p$ -value is 0.602, so we do not have statistically significant evidence against independence of the error terms in this model.

# 8.2 Overfitting and Parameter Redundancy

Our second basic diagnostic tool is that of overfitting. After specifying and fitting what we believe to be an adequate model, we fit a slightly more general model; that is, a model “close by” that contains the original model as a special case. For example, if an AR(2) model seems appropriate, we might overfit with an AR(3) model. The original AR(2) model would be confirmed if:

1. the estimate of the additional parameter, $\phi _ { 3 }$ , is not significantly different from zero, and   
2. the estimates for the parameters in common, $\Phi _ { 1 }$ and $\Phi _ { 2 }$ , do not change significantly from their original estimates.

As an example, we have specified, fitted, and examined the residuals of an AR(1) model for the industrial color property time series. Exhibit 8.13 displays the output from the R software from fitting the AR(1) model, and Exhibit 8.14 shows the results from fitting an AR(2) model to the same series. First note that, in Exhibit 8.14, the estimate of $\Phi _ { 2 }$ is not statistically different from zero. This fact supports the choice of the AR(1) model. Secondly, we note that the two estimates of $\Phi _ { 1 }$ are quite close—especially when we take into account the magnitude of their standard errors. Finally, note that while the AR(2) model has a slightly larger log-likelihood value, the AR(1) fit has a smaller AIC value. The penalty for fitting the more complex AR(2) model is sufficient to choose the simpler AR(1) model.

Exhibit 8.13 AR(1) Model Results for the Color Property Series   

<table><tr><td>Coefficients:†</td><td>ar1</td><td>Intercept‡</td></tr><tr><td></td><td>0.5705</td><td>74.3293</td></tr><tr><td>s.e.</td><td>0.1435</td><td>1.9151</td></tr><tr><td colspan="3">gma^2 estimated as 24.83: log-likelihood = -106.07, AIC = 216.15</td></tr></table>

† m1.color # R code to obtain table   
‡ Recall that the intercept here is the estimate of the process mean $\mu$ —not $\theta _ { 0 }$ .

Exhibit 8.14 AR(2) Model Results for the Color Property Series   

<table><tr><td>Coefficients:</td><td>ar1</td><td>ar2</td><td>Intercept</td></tr><tr><td></td><td>0.5173</td><td>0.1005</td><td>74.1551</td></tr><tr><td>s.e.</td><td>0.1717</td><td>0.1815</td><td>2.1463</td></tr></table>

sigma^2 estimated as 24.6: log-likelihood = -105.92, AIC = 217.84

```txt
> arima(color,order=c(2,0,0)) 
```

A different overfit for this series would be to try an ARMA(1,1) model. Exhibit 8.15 displays the results of this fit. Notice that the standard errors of the estimated coefficients for this fit are rather larger than what we see in Exhibits 8.13 and 8.14. Regardless, the estimate of $\Phi _ { 1 }$ from this fit is not significantly different from the estimate in Exhibit 8.13. Furthermore, as before, the estimate of the new parameter, θ, is not significantly different from zero. This adds further support to the AR(1) model.

Exhibit 8.15 Overfit of an ARMA(1,1) Model for the Color Series   

<table><tr><td>Coefficients:</td><td>ar1</td><td>ma1</td><td>Intercept</td></tr><tr><td></td><td>0.6721</td><td>-0.1467</td><td>74.1730</td></tr><tr><td>s.e.</td><td>0.2147</td><td>0.2742</td><td>2.1357</td></tr><tr><td colspan="4">sigma^2 estimated as 24.63: log-likelihood = -105.94, AIC = 219.88</td></tr><tr><td colspan="4">&gt; arima(color, order=c(1,0,1))</td></tr></table>

As we have noted, any $\mathbf { A R M A } ( p , q )$ model can be considered as a special case of a more general ARMA model with the additional parameters equal to zero. However, when generalizing ARMA models, we must be aware of the problem of parameter redundancy or lack of identifiability.

To make these points clear, consider an ARMA(1,2) model:

$$
Y _ {t} = \phi Y _ {t - 1} + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} \tag {8.2.1}
$$

Now replace $t$ by $t - 1$ to obtain

$$
Y _ {t - 1} = \phi Y _ {t - 2} + e _ {t - 1} - \theta_ {1} e _ {t - 2} - \theta_ {2} e _ {t - 3} \tag {8.2.2}
$$

If we multiply both sides of Equation (8.2.2) by any constant $c$ and then subtract it from Equation (8.2.1), we obtain (after rearranging)

$$
Y _ {t} - (\phi + c) Y _ {t - 1} + \phi c Y _ {t - 2} = e _ {t} - (\theta_ {1} + c) e _ {t - 1} - (\theta_ {2} - \theta_ {1} c) e _ {t - 2} + c \theta_ {2} e _ {t - 3}
$$

This apparently defines an ARMA(2,3) process. But notice that we have the factorizations

$$
1 - (\phi + c) x + \phi c x ^ {2} = (1 - \phi x) (1 - c x)
$$

and

$$
1 - (\theta_ {1} + c) x - (\theta_ {2} - c \theta_ {1}) x ^ {2} + c \theta_ {2} x ^ {3} = (1 - \theta_ {1} x - \theta_ {2} x ^ {2}) (1 - c x)
$$

Thus the AR and MA characteristic polynomials in the ARMA(2,3) process have a common factor of $( 1 - c x )$ . Even though $Y _ { t }$ does satisfy the ARMA(2,3) model, clearly the parameters in that model are not unique—the constant $c$ is completely arbitrary. We say that we have parameter redundancy in the ARMA(2,3) model.†

The implications for fitting and overfitting models are as follows:

1. Specify the original model carefully. If a simple model seems at all promising, check it out before trying a more complicated model.   
2. When overfitting, do not increase the orders of both the AR and MA parts of the model simultaneously.

3. Extend the model in directions suggested by the analysis of the residuals. For example, if after fitting an MA(1) model, substantial correlation remains at lag 2 in the residuals, try an MA(2), not an ARMA(1,1).

As an example, consider the color property series once more. We have seen that an AR(1) model fits quite well. Suppose we try an ARMA(2,1) model. The results of this fit are shown in Exhibit 8.16. Notice that even though the estimate of $\sigma _ { e } ^ { 2 }$ and the log-likelihood and AIC values are not too far from their best values, the estimates of $\Phi _ { 1 }$ , $\Phi _ { 2 }$ , and θ are way off, and none would be considered different from zero statistically.

Exhibit 8.16 Overfitted ARMA(2,1) Model for the Color Property Series   

<table><tr><td>Coefficients:</td><td>ar1</td><td>ar2</td><td>ma1</td><td>Intercept</td></tr><tr><td></td><td>0.2189</td><td>0.2735</td><td>0.3036</td><td>74.1653</td></tr><tr><td>s.e.</td><td>2.0056</td><td>1.1376</td><td>2.0650</td><td>2.1121</td></tr></table>

sigma^2 estimated as 24.58: log-likelihood = −105.91, AIC = 219.82

```txt
> arima(color,order=c(2,0,1)) 
```

# 8.3 Summary

The ideas of residual analysis begun in Chapter 3 were considerably expanded in this chapter. We looked at various plots of the residuals, checking the error terms for constant variance, normality, and independence. The properties of the sample autocorrelation of the residuals play a significant role in these diagnostics. The Ljung-Box statistic portmanteau test was discussed as a summary of the autocorrelation in the residuals. Lastly, the ideas of overfitting and parameter redundancy were presented.

# EXERCISES

8.1 For an AR(1) model with $\phi \approx 0 . 5$ and $n = 1 0 0$ , the lag 1 sample autocorrelation of the residuals is 0.5. Should we consider this unusual? Why or why not?   
8.2 Repeat Exercise 8.1 for an MA(1) model with $\theta \approx 0 . 5$ and $n = 1 0 0$ .   
8.3 Based on a series of length $n = 2 0 0$ , we fit an AR(2) model and obtain residual autocorrelations of $\hat { r } _ { 1 } = 0 . 1 3$ , $\hat { r } _ { 2 } = 0 . 1 3$ , and $\hat { r } _ { 3 } = 0 . 1 2$ . If $\widehat { \Phi } _ { 1 } = 1 . 1$ and $\hat { \Phi } _ { 2 } = - 0 . 8$ , do these residual autocorrelations support the AR(2) specification? Individually? Jointly?

8.4 Simulate an AR(1) model with $n = 3 0$ and $\phi = 0 . 5$ .

(a) Fit the correctly specified AR(1) model and look at a time series plot of the residuals. Does the plot support the AR(1) specification?   
(b) Display a normal quantile-quantile plot of the standardized residuals. Does the plot support the AR(1) specification?   
(c) Display the sample ACF of the residuals. Does the plot support the AR(1) specification?   
(d) Calculate the Ljung-Box statistic summing to $K = 8$ . Does this statistic support the AR(1) specification?

8.5 Simulate an MA(1) model with $n = 3 6$ and $\theta = - 0 . 5$ .

(a) Fit the correctly specified MA(1) model and look at a time series plot of the residuals. Does the plot support the MA(1) specification?   
(b) Display a normal quantile-quantile plot of the standardized residuals. Does the plot support the MA(1) specification?   
(c) Display the sample ACF of the residuals. Does the plot support the MA(1) specification?   
(d) Calculate the Ljung-Box statistic summing to $K = 6$ . Does this statistic support the MA(1) specification?

8.6 Simulate an AR(2) model with $n = 4 8$ , $\phi _ { 1 } = 1 . 5$ , and $\Phi _ { 2 } = - 0 . 7 5$

(a) Fit the correctly specified AR(2) model and look at a time series plot of the residuals. Does the plot support the AR(2) specification?   
(b) Display a normal quantile-quantile plot of the standardized residuals. Does the plot support the AR(2) specification?   
(c) Display the sample ACF of the residuals. Does the plot support the AR(2) specification?   
(d) Calculate the Ljung-Box statistic summing to $K = 1 2$ . Does this statistic support the AR(2) specification?

8.7 Fit an AR(3) model by maximum likelihood to the square root of the hare abundance series (filename hare).

(a) Plot the sample ACF of the residuals. Comment on the size of the correlations.   
(b) Calculate the Ljung-Box statistic summing to $K = 9$ . Does this statistic support the AR(3) specification?   
(c) Perform a runs test on the residuals and comment on the results.   
(d) Display the quantile-quantile normal plot of the residuals. Comment on the plot.   
(e) Perform the Shapiro-Wilk test of normality on the residuals.

8.8 Consider the oil filter sales data shown in Exhibit 1.8 on page 7. The data are in the file named oilfilters.

(a) Fit an AR(1) model to this series. Is the estimate of the $\phi$ parameter significantly different from zero statistically?   
(b) Display the sample ACF of the residuals from the AR(1) fitted model. Comment on the display.

8.9 The data file named robot contains a time series obtained from an industrial robot. The robot was put through a sequence of maneuvers, and the distance from a desired ending point was recorded in inches. This was repeated 324 times to form the time series. Compare the fits of an AR(1) model and an IMA(1,1) model for these data in terms of the diagnostic tests discussed in this chapter.   
8.10 The data file named deere3 contains 57 consecutive values from a complex machine tool at Deere & Co. The values given are deviations from a target value in units of ten millionths of an inch. The process employs a control mechanism that resets some of the parameters of the machine tool depending on the magnitude of deviation from target of the last item produced. Diagnose the fit of an AR(1) model for these data in terms of the tests discussed in this chapter.   
8.11 Exhibit 6.31 on page 139, suggested specifying either an AR(1) or possibly an AR(4) model for the difference of the logarithms of the oil price series. (The filename is oil.price).

(a) Estimate both of these models using maximum likelihood and compare the results using the diagnostic tests considered in this chapter.   
(b) Exhibit 6.32 on page 140, suggested specifying an MA(1) model for the difference of the logs. Estimate this model by maximum likelihood and perform the diagnostic tests considered in this chapter.   
(c) Which of the three models AR(1), AR(4), or MA(1) would you prefer given the results of parts (a) and (b)?

# CHAPTER 9

# FORECASTING

One of the primary objectives of building a model for a time series is to be able to forecast the values for that series at future times. Of equal importance is the assessment of the precision of those forecasts. In this chapter, we shall consider the calculation of forecasts and their properties for both deterministic trend models and ARIMA models. Forecasts for models that combine deterministic trends with ARIMA stochastic components are considered also.

For the most part, we shall assume that the model is known exactly, including specific values for all the parameters. Although this is never true in practice, the use of estimated parameters for large sample sizes does not seriously affect the results.

# 9.1 Minimum Mean Square Error Forecasting

Based on the available history of the series up to time t, namely $Y _ { 1 }$ , Y2,…, $Y _ { t - 1 }$ , $Y _ { t } ,$ we would like to forecast the value of $Y _ { t + \ell }$ that will occur l time units into the future. We call time $t$ the forecast origin and l the lead time for the forecast, and denote the forecast itself as $\hat { Y } _ { t } ( \ell )$ .

As shown in Appendix F, the minimum mean square error forecast is given by

$$
\hat {Y} _ {t} (\ell) = E \left(Y _ {t + \ell} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) \tag {9.1.1}
$$

(Appendices E and F on page 218 review the properties of conditional expectation and minimum mean square error prediction.)

The computation and properties of this conditional expectation as related to forecasting will be our concern for the remainder of this chapter.

# 9.2 Deterministic Trends

Consider once more the deterministic trend model of Chapter 3,

$$
Y _ {t} = \mu_ {t} + X _ {t} \tag {9.2.1}
$$

where the stochastic component, $X _ { t }$ , has a mean of zero. For this section, we shall assume that $\{ X _ { t } \}$ is in fact white noise with variance $\gamma _ { 0 }$ . For the model in Equation (9.2.1), we have

$$
\begin{array}{l} \hat {\boldsymbol {Y}} _ {t} (\ell) = E (\mu_ {t + \ell} + X _ {t + \ell} | \boldsymbol {Y} _ {1}, \boldsymbol {Y} _ {2}, \dots , \boldsymbol {Y} _ {t}) \\ = E (\mu_ {t + \ell} | Y _ {1}, Y _ {2}, \dots , Y _ {t}) + E (X _ {t + \ell} | Y _ {1}, Y _ {2}, \dots , Y _ {t}) \\ = \mu_ {t + \ell} + E (X _ {t + \ell}) \\ \end{array}
$$

or

$$
\hat {Y} _ {t} (\ell) = \mu_ {t + \ell} \tag {9.2.2}
$$

since for $\ell \geq 1$ , $X _ { t + \ell }$ is independent of $Y _ { 1 }$ , Y2,…, $Y _ { t - 1 }$ , $Y _ { t }$ and has expected value zero. Thus, in this simple case, forecasting amounts to extrapolating the deterministic time trend into the future.

For the linear trend case, $\mu _ { t } = \beta _ { 0 } + \beta _ { 1 } t$ , the forecast is

$$
\hat {Y} _ {t} (\ell) = \beta_ {0} + \beta_ {1} (t + \ell) \tag {9.2.3}
$$

As we emphasized in Chapter 3, this model assumes that the same linear time trend persists into the future, and the forecast reflects that assumption. Note that it is the lack of statistical dependence between $Y _ { t + \ell }$ and $Y _ { 1 }$ , Y2,…, $Y _ { t - 1 }$ , $Y _ { t }$ that prevents us from improving on $\mu _ { t + \ell }$ as a forecast.

t + For seasonal models where, say, , our forecast is =μt μt 12+= Y^t( ) μ l t + +12 l= $\mu _ { t } = \mu _ { t + 1 2 }$ $\hat { Y } _ { t } ( \ell ) = \mu _ { t + 1 2 + \ell } =$ $\hat { Y } _ { t } ( \ell + 1 2 )$ . Thus the forecast will also be periodic, as desired.

The forecast error, $e _ { t } ( \ell )$ , is given by

$$
\begin{array}{l} e _ {t} (\ell) = Y _ {t + \ell} - \hat {Y} _ {t} (\ell) \\ = \mu_ {t + \ell} + X _ {t + \ell} - \mu_ {t + \ell} \\ = X _ {t + \ell} \\ \end{array}
$$

so that

$$
E (e _ {t} (\ell)) = E (X _ {t + \ell}) = 0
$$

That is, the forecasts are unbiased. Also

$$
V a r \left(e _ {t} (\ell)\right) = V a r \left(X _ {t + \ell}\right) = \gamma_ {0} \tag {9.2.4}
$$

is the forecast error variance for all lead times l.

The cosine trend model for the average monthly temperature series was estimated in Chapter 3 on page 35 as

$$
\hat {\mu} _ {t} = 4 6. 2 6 6 0 + (- 2 6. 7 0 7 9) \cos (2 \pi t) + (- 2. 1 6 9 7) \sin (2 \pi t)
$$

Here time is measured in years with a starting value of January 1964, frequency $f = 1$ per year, and the final observed value is for December 1975. To forecast the June 1976 temperature value, we use $t = 1 9 7 6 . 4 1 6 6 7$ as the time value† and obtain

$$
\begin{array}{l} \hat {\mu} _ {t} = 4 6. 2 6 6 0 + (- 2 6. 7 0 7 9) \cos (2 \pi (1 9 7 6. 4 1 6 6 7)) + (- 2. 1 6 9 7) \sin (2 \pi (1 9 7 6. 4 1 6 6 7)) \\ = 6 8. 3 ^ {\circ} \mathrm {F} \\ \end{array}
$$

Forecasts for other months are obtained similarly.

# 9.3 ARIMA Forecasting

For ARIMA models, the forecasts can be expressed in several different ways. Each expression contributes to our understanding of the overall forecasting procedure with respect to computing, updating, assessing precision, or long-term forecasting behavior.

# AR(1)

We shall first illustrate many of the ideas with the simple AR(1) process with a nonzero mean that satisfies

$$
Y _ {t} - \mu = \phi \left(Y _ {t - 1} - \mu\right) + e _ {t} \tag {9.3.1}
$$

Consider the problem of forecasting one time unit into the future. Replacing $t$ by $t + 1$ in Equation (9.3.1), we have

$$
Y _ {t + 1} - \mu = \phi \left(Y _ {t} - \mu\right) + e _ {t + 1} \tag {9.3.2}
$$

Given $Y _ { 1 } , Y _ { 2 } , . . . , Y _ { t - 1 } , Y _ { t } ,$ $Y _ { 1 }$ we take the conditional expectations of both sides of Equation (9.3.2) and obtain

$$
\hat {Y} _ {t} (1) - \mu = \phi \left[ E \left(Y _ {t} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) - \mu \right] + E \left(e _ {t + 1} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) \tag {9.3.3}
$$

Now, from the properties of conditional expectation, we have

$$
E \left(Y _ {t} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) = Y _ {t} \tag {9.3.4}
$$

Also, since $e _ { t + 1 }$ is independent of $Y _ { 1 } , Y _ { 2 } , . . . , Y _ { t - 1 } , Y _ { t } ,$ $Y _ { t } ,$ we obtain

$$
E \left(e _ {t + 1} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) = E \left(e _ {t + 1}\right) = 0 \tag {9.3.5}
$$

Thus, Equation (9.3.3) can be written as

$$
\hat {Y} _ {t} (1) = \mu + \phi \left(Y _ {t} - \mu\right) \tag {9.3.6}
$$

In words, a proportion $\phi$ of the current deviation from the process mean is added to the process mean to forecast the next process value.

Now consider a general lead time l. Replacing $t$ by $t + \ell$ in Equation (9.3.1) and taking the conditional expectations of both sides produces

$$
\hat {Y} _ {t} (\ell) = \mu + \phi [ \hat {Y} _ {t} (\ell - 1) - \mu ] \quad \text {f o r} \ell \geq 1 \tag {9.3.7}
$$

since $E ( Y _ { t + \ell - 1 } | Y _ { 1 } , Y _ { 2 } , . . . , Y _ { t } ) = \hat { Y } _ { t } ( \ell - 1 )$ and, for $\ell \geq 1$ , $e _ { t + \ell }$ is independent of $Y _ { 1 }$ $Y _ { 2 } , . . . , Y _ { t - 1 } , Y _ { t }$ .

Equation (9.3.7), which is recursive in the lead time l, shows how the forecast for any lead time l can be built up from the forecasts for shorter lead times by starting with the initial forecast $\hat { \boldsymbol Y } _ { t } ( 1 )$ computed using Equation (9.3.6). The forecast $\hat { Y } _ { t } ( 2 )$ is then obtained from $\hat { Y } _ { t } ( 2 ) \overset { \vartriangle } { = } \mu + \dot { \phi { } [ \{ Y _ { t } ( 1 ) - \mu \} ] }$ , then $\hat { Y } _ { t } ( \hat { 3 } )$ from $\hat { \boldsymbol Y } _ { t } ( 2 )$ , and so on until the desired $\hat { \boldsymbol Y } _ { t } ( t )$ is found. Equation (9.3.7) and its generalizations for other ARIMA models are most convenient for actually computing the forecasts. Equation (9.3.7) is sometimes called the difference equation form of the forecasts.

However, Equation (9.3.7) can also be solved to yield an explicit expression for the forecasts in terms of the observed history of the series. Iterating backward on l in Equation (9.3.7), we have

$$
\begin{array}{l} \hat {Y} _ {t} (\ell) = \phi [ \hat {Y} _ {t} (\ell - 1) - \mu ] + \mu \\ = \phi \left\{\phi \left[ \hat {Y} _ {t} (\ell - 2) - \mu \right] \right\} + \mu \\ \begin{array}{c} \vdots \\ \vdots \end{array} \\ = \phi^ {\ell - 1} [ \hat {Y} _ {t} (1) - \mu ] + \mu \\ \end{array}
$$

or

$$
\hat {Y} _ {t} (\ell) = \mu + \phi^ {\ell} \left(Y _ {t} - \mu\right) \tag {9.3.8}
$$

The current deviation from the mean is discounted by a factor $\phi ^ { \ell }$ , whose magnitude decreases with increasing lead time. The discounted deviation is then added to the process mean to produce the lead l forecast.

As a numerical example, consider the AR(1) model that we have fitted to the industrial color property time series. The maximum likelihood estimation results were partially shown in Exhibit 7.7 on page 165, but more complete results are shown in Exhibit 9.1.

Exhibit 9.1 Maximum Likelihood Estimation of an AR(1) Model for Color   
```txt
Coefficients: ar1 intercept† 0.5705 74.3293 s.e. 0.1435 1.9151 
```

sigma^2 estimated as 24.8: log-likelihood = −106.07, AIC = 216.15

†Remember that the intercept here is the estimate of the process mean $\mu$ —not $\theta _ { 0 }$ .

```txt
> data(color)  
> m1.color=arima(color, order=c(1,0,0))  
> m1.color 
```

For illustration purposes, we assume that the estimates $\Phi = 0 . 5 7 0 5$ and $\mu = 7 4 . 3 2 9 3$ are true values. The final forecasts may then be rounded.

The last observed value of the color property is 67, so we would forecast one time period ahead as†

$$
\begin{array}{l} \stackrel {\wedge} {Y} _ {t} (1) = 7 4. 3 2 9 3 + (0. 5 7 0 5) (6 7 - 7 4. 3 2 9 3) \\ = 7 4. 3 2 9 3 - 4. 1 8 1 3 6 6 \\ = 7 0. 1 4 7 9 3 \\ \end{array}
$$

For lead time 2, we have from Equation (9.3.7)

$$
\begin{array}{l} \hat {Y} _ {t} (2) = 7 4. 3 2 9 3 + 0. 5 7 0 5 (7 0. 1 4 7 9 3 - 7 4. 3 2 9 3) \\ = 7 4. 3 2 9 3 - 2. 3 8 5 4 7 2 \\ = 7 1. 9 4 3 8 3 \\ \end{array}
$$

Alternatively, we can use Equation (9.3.8):

$$
\begin{array}{l} \hat {Y} _ {t} (2) = 7 4. 3 2 9 3 + (0. 5 7 0 5) ^ {2} (6 7 - 7 4. 3 2 9 3) \\ = 7 1. 9 2 8 2 3 \\ \end{array}
$$

At lead 5, we have

$$
\begin{array}{l} \hat {Y} _ {t} (5) = 7 4. 3 2 9 3 + (0. 5 7 0 5) ^ {5} (6 7 - 7 4. 3 2 9 3) \\ = 7 3. 8 8 6 3 6 \\ \end{array}
$$

and by lead 10 the forecast is

$$
\hat {Y} _ {t} (1 0) = 7 4. 3 0 2 5 3
$$

which is very nearly μ $\iota ( = 7 4 . 3 2 9 3 )$ . In reporting these forecasts we would probably round to the nearest tenth.

In general, since $\left. \phi \right. < 1$ , we have simply

$$
\hat {Y} _ {t} (\ell) \approx \mu \text {f o r l a r g e} \ell \tag {9.3.9}
$$

Later we shall see that Equation (9.3.9) holds for all stationary ARMA models.

Consider now the one-step-ahead forecast error, $e _ { t } ( 1 )$ . From Equations (9.3.2) and (9.3.6), we have

$$
\begin{array}{l} e _ {t} (1) = Y _ {t + 1} - \stackrel {\wedge} {Y} _ {t} (1) \\ = [ \phi (Y _ {t} - \mu) + \mu + e _ {t + 1} ] - [ \phi (Y _ {t} - \mu) + \mu ] \\ \end{array}
$$

or

$$
e _ {t} (1) = e _ {t + 1} \tag {9.3.10}
$$

The white noise process $\{ \boldsymbol { e } _ { t } \}$ can now be reinterpreted as a sequence of one-step-ahead forecast errors. We shall see that Equation (9.3.10) persists for completely general ARIMA models. Note also that Equation (9.3.10) implies that the forecast error $e _ { t } ( 1 )$ is independent of the history of the process $Y _ { 1 }$ , $Y _ { 2 }$ , …, $Y _ { t - 1 }$ , $Y _ { t }$ up to time t. If this were not so, the dependence could be exploited to improve our forecast.

Equation (9.3.10) also implies that our one-step-ahead forecast error variance is given by

$$
\operatorname {V a r} \left(e _ {t} (1)\right) = \sigma_ {e} ^ {2} \tag {9.3.11}
$$

To investigate the properties of the forecast errors for longer leads, it is convenient to express the AR(1) model in general linear process, or MA( ), form. From Equation∞ (4.3.8) on page 70, we recall that

$$
Y _ {t} = e _ {t} + \phi e _ {t - 1} + \phi^ {2} e _ {t - 2} + \phi^ {3} e _ {t - 3} + \dots \tag {9.3.12}
$$

Then Equations (9.3.8) and (9.3.12) together yield

$$
\begin{array}{l} e _ {t} (\ell) = Y _ {t + \ell} - \mu - \phi^ {\ell} \left(Y _ {t} - \mu\right) \\ = e _ {t + \ell} + \phi e _ {t + \ell - 1} + \dots + \phi^ {\ell - 1} e _ {t + 1} + \phi^ {\ell} e _ {t} \\ + \dots - \phi^ {\ell} \left(e _ {t} + \phi e _ {t - 1} + \dots\right) \\ \end{array}
$$

so that

$$
e _ {t} (\ell) = e _ {t + \ell} + \phi e _ {t + \ell - 1} + \dots + \phi^ {\ell - 1} e _ {t + 1} \tag {9.3.13}
$$

which can also be written as

$$
e _ {t} (\ell) = e _ {t + \ell} + \psi_ {1} e _ {t + \ell - 1} + \psi_ {2} e _ {t + \ell - 2} + \dots + \psi_ {\ell - 1} e _ {t + 1} \tag {9.3.14}
$$

Equation (9.3.14) will be shown to hold for all ARIMA models (see Equation (9.3.43) on page 202).

Note that $E ( e _ { t } ( \boldsymbol { \ell } ) ) = 0$ ; thus the forecasts are unbiased. Furthermore, from Equation (9.3.14), we have

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) = \sigma_ {e} ^ {2} \left(1 + \psi_ {1} ^ {2} + \psi_ {2} ^ {2} + \dots + \psi_ {\ell - 1} ^ {2}\right) \tag {9.3.15}
$$

We see that the forecast error variance increases as the lead l increases. Contrast this with the result given in Equation (9.2.4) on page 192, for deterministic trend models.

In particular, for the AR(1) case,

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) = \sigma_ {e} ^ {2} \left[ \frac {1 - \phi^ {2 \ell}}{1 - \phi^ {2}} \right] \tag {9.3.16}
$$

which we obtain by summing a finite geometric series.

For long lead times, we have

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) \approx \frac {\sigma_ {e} ^ {2}}{1 - \phi^ {2}} \text {f o r l a r g e} \ell \tag {9.3.17}
$$

or, by Equation (4.3.3), page 66,

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) \approx \operatorname {V a r} \left(Y _ {t}\right) = \gamma_ {0} \quad \text {f o r l a r g e} \ell \tag {9.3.18}
$$

Equation (9.3.18) will be shown to be valid for all stationary ARMA processes (see Equation (9.3.39) on page 201).

# MA(1)

To illustrate how to solve the problems that arise in forecasting moving average or mixed models, consider the MA(1) case with nonzero mean:

$$
Y _ {t} = \mu + e _ {t} - \theta e _ {t - 1}
$$

Again replacing $t$ by $t + 1$ and taking conditional expectations of both sides, we have

$$
\hat {Y} _ {t} (1) = \mu - \theta E \left(e _ {t} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) \tag {9.3.19}
$$

However, for an invertible model, Equation (4.5.2) on page 80 shows that $e _ { t }$ is a function of $Y _ { 1 }$ , $Y _ { 2 }$ , …, $Y _ { t }$ and so

$$
E \left(e _ {t} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) = e _ {t} \tag {9.3.20}
$$

In fact, an approximation is involved in this equation since we are conditioning only on $Y _ { 1 }$ , $Y _ { 2 }$ , …, $Y _ { t }$ and not on the infinite history of the process. However, if, as in practice, t is large and the model is invertible, the error in the approximation will be very small. If the model is not invertible—for example, if we have overdifferenced the data—then Equation (9.3.20) is not even approximately valid; see Harvey (1981c, p.161).

Using Equations (9.3.19) and (9.3.20), we have the one-step-ahead forecast for an invertible MA(1) expressed as

$$
\hat {Y} _ {t} (1) = \mu - \theta e _ {t} \tag {9.3.21}
$$

The computation of $e _ { t }$ will be a by-product of estimating the parameters in the model.

Notice once more that the one-step-ahead forecast error is

$$
\begin{array}{l} e _ {t} (1) = Y _ {t + 1} - \hat {Y} _ {t} (1) \\ = (\mu + e _ {t + 1} - \theta e _ {t}) - (\mu - \theta e _ {t}) \\ \mathbf {\Sigma} = \mathbf {\Sigma} e _ {t + 1} \\ \end{array}
$$

as in Equation (9.3.10), and thus Equation (9.3.11) also obtains.

For longer lead times, we have

$$
\hat {\boldsymbol {Y}} _ {t} (\ell) = \mu + E (e _ {t + \ell} | Y _ {1}, Y _ {2}, \dots , Y _ {t}) - \theta E (e _ {t + \ell - 1} | Y _ {1}, Y _ {2}, \dots , Y _ {t})
$$

But, for $\ell > 1$ , both $e _ { t + \ell }$ and $e _ { t + \ell - 1 }$ are independent of $Y _ { 1 }$ $\mathbf { \omega } _ { 1 } ^ { \prime } , Y _ { 2 } , . . . , Y _ { t }$ . Consequently, these conditional expected values are the unconditional expected values, namely zero, and we have

$$
\hat {Y} _ {t} (\ell) = \mu \text {f o r} \ell > 1 \tag {9.3.22}
$$

Notice here that Equation (9.3.9) on page 195 holds exactly for the MA(1) case when $\ell >$ 1. Since for this model we trivially have $\psi _ { 1 } = - \theta$ and $\psi _ { j } = 0$ for $j > 1$ , Equations (9.3.14) and (9.3.15) also hold.

# The Random Walk with Drift

To illustrate forecasting with nonstationary ARIMA series, consider the random walk with drift defined by

$$
Y _ {t} = Y _ {t - 1} + \theta_ {0} + e _ {t} \tag {9.3.23}
$$

Here

$$
\hat {\boldsymbol {Y}} _ {t} (1) = E (\boldsymbol {Y} _ {t} | \boldsymbol {Y} _ {1}, \boldsymbol {Y} _ {2}, \dots , \boldsymbol {Y} _ {t}) + \boldsymbol {\theta} _ {0} + E (e _ {t + 1} | \boldsymbol {Y} _ {1}, \boldsymbol {Y} _ {2}, \dots , \boldsymbol {Y} _ {t})
$$

so that

$$
\hat {Y} _ {t} (1) = Y _ {t} + \theta_ {0} \tag {9.3.24}
$$

Similarly, the difference equation form for the lead l forecast is

$$
\hat {Y} _ {t} (\ell) = \hat {Y} _ {t} (\ell - 1) + \theta_ {0} \text {f o r} \ell \geq 1 \tag {9.3.25}
$$

and iterating backward on l yields the explicit expression

$$
\hat {Y} _ {t} (\ell) = Y _ {t} + \theta_ {0} \ell \text {f o r} \ell \geq 1 \tag {9.3.26}
$$

In contrast to Equation (9.3.9) on page 195, if $\theta _ { 0 } \neq 0$ , the forecast does not converge for long leads but rather follows a straight line with slope $\theta _ { 0 }$ for all l.

Note that the presence or absence of the constant term $\theta _ { 0 }$ significantly alters the nature of the forecast. For this reason, constant terms should not be included in nonstationary ARIMA models unless the evidence is clear that the mean of the differenced series is significantly different from zero. Equation (3.2.3) on page 28 for the variance of the sample mean will help assess this significance.

However, as we have seen in the AR(1) and MA(1) cases, the one-step-ahead forecast error is

$$
e _ {t} (1) = Y _ {t + 1} - \overset {\wedge} {Y} _ {t} (1) = e _ {t + 1}
$$

Also

$$
\begin{array}{l} e _ {t} (\ell) = Y _ {t + \ell} - \hat {Y} _ {t} (\ell) \\ = (Y _ {t} + \ell \theta_ {0} + e _ {t + 1} + \dots + e _ {t + \ell}) - (Y _ {t} + \ell \theta_ {0}) \\ = e _ {t + 1} + e _ {t + 2} + \dots + e _ {t + \ell} \\ \end{array}
$$

which agrees with Equation (9.3.14) on page 196 since in this model $\psi _ { j } = 1$ for all $j$ . (See Equation (5.2.6) on page 93 with $\theta = 0$ .)

So, as in Equation (9.3.15), we have

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) = \sigma_ {e} ^ {2} \sum_ {j = 0} ^ {\ell - 1} \psi_ {j} ^ {2} = \ell \sigma_ {e} ^ {2} \tag {9.3.27}
$$

In contrast to the stationary case, here $V a r ( e _ { t } ( \ell ) )$ grows without limit as the forecast lead time l increases. We shall see that this property is characteristic of the forecast error variance for all nonstationary ARIMA processes.

# ARMA(p,q)

For the general stationary ARMA $( p , q )$ model, the difference equation form for computing forecasts is given by

$$
\begin{array}{l} \hat {Y} _ {t} (\ell) = \phi_ {1} \hat {Y} _ {t} (\ell - 1) + \phi_ {2} \hat {Y} _ {t} (\ell - 2) + \dots + \phi_ {p} \hat {Y} _ {t} (\ell - p) + \theta_ {0} \\ - \theta_ {1} E \left(e _ {t + \ell - 1} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) - \theta_ {2} E \left(e _ {t + \ell - 2} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) \tag {9.3.28} \\ - \dots - \theta_ {q} E (e _ {t + \ell - q} | Y _ {1}, Y _ {2}, \dots , Y _ {t}) \\ \end{array}
$$

where

$$
E \left(e _ {t + j} \mid Y _ {1}, Y _ {2}, \dots , Y _ {t}\right) = \left\{ \begin{array}{l l} 0 & \text {f o r} j > 0 \\ e _ {t + j} & \text {f o r} j \leq 0 \end{array} \right. \tag {9.3.29}
$$

We note that $\hat { \boldsymbol Y } _ { t } ( j )$ is a true forecast for $j > 0$ , but for $j \le 0$ , $\hat { Y } _ { t } ( j ) = Y _ { t + j }$ . As in Equation (9.3.20) on page 197, Equation (9.3.29) involves some minor approximation. For an invertible model, Equation (4.5.5) on page 80 shows that, using the $\pi$ -weights, $e _ { t }$ can be expressed as a linear combination of the infinite sequence $Y _ { t } , Y _ { t - 1 }$ , Yt − 2,…. However, the $\pi$ -weights die out exponentially fast, and the approximation assumes that $\pi _ { j }$ is negligible for $j > t - q$ .

As an example, consider an ARMA(1,1) model. We have

$$
\hat {Y} _ {t} (1) = \phi Y _ {t} + \theta_ {0} - \theta e _ {t} \tag {9.3.30}
$$

with

$$
\hat {Y} _ {t} (2) = \phi \hat {Y} _ {t} (1) + \theta_ {0}
$$

and, more generally,

$$
\hat {Y} _ {t} (\ell) = \phi \hat {Y} _ {t} (\ell - 1) + \theta_ {0} \text {f o r} \ell \geq 2 \tag {9.3.31}
$$

using Equation (9.3.30) to get the recursion started.

Equations (9.3.30) and (9.3.31) can be rewritten in terms of the process mean and then solved by iteration to get the alternative explicit expression

$$
\hat {Y} _ {t} (\ell) = \mu + \phi^ {\ell} \left(Y _ {t} - \mu\right) - \phi^ {\ell - 1} e _ {t} \text {f o r} \ell \geq 1 \tag {9.3.32}
$$

As Equations (9.3.28) and (9.3.29) indicate, the noise terms $e _ { t - ( q - 1 ) } , . . . , e _ { t - \ 1 } , e _ { t }$ appear directly in the computation of the forecasts for leads $\ell = 1$ , 2,…, q. However, for $\ell > q$ , the autoregressive portion of the difference equation takes over, and we have

$$
\hat {Y} _ {t} (\ell) = \phi_ {1} \hat {Y} _ {t} (\ell - 1) + \phi_ {2} \hat {Y} _ {t} (\ell - 2) + \dots + \phi_ {p} \hat {Y} _ {t} (\ell - p) + \theta_ {0} \text {f o r} \ell > q \tag {9.3.33}
$$

Thus the general nature of the forecast for long lead times will be determined by the autoregressive parameters $\Phi _ { 1 } , \Phi _ { 2 } , . . . , \Phi _ { p }$ $\Phi _ { 1 }$ $\phi _ { p }$ (and the constant term, $\theta _ { 0 }$ , which is related to the mean of the process).

Recalling from Equation (5.3.17) on page 97 that $\theta _ { 0 } = \mu ( 1 - \phi _ { 1 } - \phi _ { 2 } - \cdots - \phi _ { p } )$ , we can rewrite Equation (9.3.33) in terms of deviations from $\mu$ as

$$
\begin{array}{l} \hat {Y} _ {t} (\ell) - \mu = \phi_ {1} [ \hat {Y} _ {t} (\ell - 1) - \mu ] + \phi_ {2} [ \hat {Y} _ {t} (\ell - 2) - \mu ] + \dots \\ + \phi_ {p} \left[ \hat {Y} _ {t} (\ell - p) - \mu \right] \text {f o r} \ell > q \tag {9.3.34} \\ \end{array}
$$

As a function of lead time l, $\hat { Y } _ { t } ( \ell ) - \mu$ follows the same Yule-Walker recursion as the autocorrelation function $\rho _ { k }$ of the process (see Equation (4.4.8), page 79). Thus, as in Section 4.3 on page 66 and Section 4.4 on page 77, the roots of the characteristic equation will determine the general behavior of $\hat { \ Y } _ { t } ( \ell ) - \mu$ for large lead times. In particular, $\hat { Y } _ { t } ( \ell ) - \mu$ can be expressed as a linear combination of exponentially decaying terms in l (corresponding to the real roots) and damped sine wave terms (corresponding to the pairs of complex roots).

Thus, for any stationary ARMA model, $\hat { Y } _ { t } ( \ell ) - \mu$ decays to zero as l increases, and the long-term forecast is simply the process mean $\mu$ as given in Equation (9.3.9) on page 195. This agrees with common sense since for stationary ARMA models the dependence dies out as the time span between observations increases, and this dependence is the only reason we can improve on the “naive” forecast of using $\mu$ alone.

To argue the validity of Equation (9.3.15) for $e _ { t } ( \ell )$ in the present generality, we need to consider a new representation for ARIMA processes. Appendix G shows that any ARIMA model can be written in truncated linear process form as

$$
Y _ {t + \ell} = C _ {t} (\ell) + I _ {t} (\ell) \text {f o r} \ell > 1 \tag {9.3.35}
$$

where, for our present purposes, we need only know that $C _ { t } ( \ell )$ is a certain function of $Y _ { t } ,$ $Y _ { t - 1 } , \dots$ and

$$
I _ {t} (\ell) = e _ {t + \ell} + \psi_ {1} e _ {t + \ell - 1} + \psi_ {2} e _ {t + \ell - 2} + \dots + \psi_ {\ell - 1} e _ {t + 1} \text {f o r} \ell \geq 1 \tag {9.3.36}
$$

Furthermore, for invertible models with $t$ reasonably large, $C _ { t } ( \ell )$ is a certain function of the finite history $Y _ { t } , Y _ { t - 1 } , . . . , Y _ { 1 }$ $Y _ { 1 }$ . Thus we have

$$
\begin{array}{l} \hat {\dot {Y}} _ {t} (\ell) = E (C _ {t} (\ell) | Y _ {1}, Y _ {2}, \dots , Y _ {t}) + E (I _ {t} (\ell) | Y _ {1}, Y _ {2}, \dots , Y _ {t}) \\ = C _ {t} (\ell) \\ \end{array}
$$

Finally,

$$
\begin{array}{l} e _ {t} (\ell) = Y _ {t + \ell} - \hat {Y} _ {t} (\ell) \\ = \left[ C _ {t} (\ell) + I _ {t} (\ell) \right] - C _ {t} (\ell) \\ = I _ {t} (\ell) \\ = e _ {t + \ell} + \psi_ {1} e _ {t + \ell - 1} + \psi_ {2} e _ {t + \ell - 2} + \dots + \psi_ {\ell - 1} e _ {t + 1} \\ \end{array}
$$

Thus, for a general invertible ARIMA process,

$$
E \left[ e _ {t} (\ell) \right] = 0 \text {f o r} \ell \geq 1 \tag {9.3.37}
$$

and

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) = \sigma_ {e} ^ {2} \sum_ {j = 0} ^ {\ell - 1} \psi_ {j} ^ {2} \text {f o r} \ell \geq 1 \tag {9.3.38}
$$

From Equations (4.1.4) and (9.3.38), we see that for long lead times in stationary ARMA models, we have

$$
V a r (e _ {t} (\ell)) \approx \sigma_ {e} ^ {2} \sum_ {j = 0} ^ {\infty} \psi_ {j} ^ {2}
$$

or

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) \approx \gamma_ {0} \text {f o r l a r g e} \ell \tag {9.3.39}
$$

# Nonstationary Models

As the random walk shows, forecasting for nonstationary ARIMA models is quite similar to forecasting for stationary ARMA models, but there are some striking differences. Recall from Equation (5.2.2) on page 92 that an ARIMA(p,1,q) model can be written as a nonstationary ARMA $( p + 1 , q )$ model, We shall write this as

$$
\begin{array}{l} Y _ {t} = \varphi_ {1} Y _ {t - 1} + \varphi_ {2} Y _ {t - 2} + \varphi_ {3} Y _ {t - 3} + \dots + \varphi_ {p} Y _ {t - p} + \varphi_ {p + 1} Y _ {t - p - 1} \tag {9.3.40} \\ + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2} - \dots - \theta_ {q} e _ {t - q} \\ \end{array}
$$

where the script coefficients $\boldsymbol { \Phi }$ are directly related to the block $\boldsymbol { \Phi }$ coefficients. In particular,

$$
\left. \begin{array}{l} \varphi_ {1} = 1 + \phi_ {1}, \varphi_ {j} = \phi_ {j} - \phi_ {j - 1} \text {f o r} j = 1, 2, \dots , p \\ \text {a n d} \\ \varphi_ {p + 1} = - \phi_ {p} \end{array} \right\} \tag {9.3.41}
$$

For a general order of differencing $d$ , we would have $p + d$ of the $\boldsymbol { \Phi }$ coefficients.

From this representation, we can immediately extend Equations (9.3.28), (9.3.29), and (9.3.30) on page 199 to cover the nonstationary cases by replacing $p$ by $p + d$ and $\boldsymbol { \Phi } _ { j }$ by $\Phi _ { j } .$ .

As an example of the necessary calculations, consider the ARIMA(1,1,1) case. Here

$$
Y _ {t} - Y _ {t - 1} = \phi (Y _ {t - 1} - Y _ {t - 2}) + \theta_ {0} + e _ {t} - \theta e _ {t - 1}
$$

so that

$$
Y _ {t} = (1 + \phi) Y _ {t - 1} - \phi Y _ {t - 2} + \theta_ {0} + e _ {t} - \theta e _ {t - 1}
$$

Thus

$$
\left. \begin{array}{l} \hat {Y} _ {t} (1) = (1 + \phi) Y _ {t} - \phi Y _ {t - 1} + \theta_ {0} - \theta e _ {t} \\ \hat {Y} _ {t} (2) = (1 + \phi) \hat {Y} _ {t} (1) - \phi Y _ {t} + \theta_ {0} \\ \text {a n d} \\ \hat {Y} _ {t} (\ell) = (1 + \phi) \hat {Y} _ {t} (\ell - 1) - \phi \hat {Y} _ {t} (\ell - 2) + \theta_ {0} \end{array} \right\} \tag {9.3.42}
$$

For the general invertible ARIMA model, the truncated linear process representation given in Equations (9.3.35) and (9.3.36) and the calculations following these equations show that we can write

$$
e _ {t} (\ell) = e _ {t + \ell} + \psi_ {1} e _ {t + \ell - 1} + \psi_ {2} e _ {t + \ell - 2} + \dots + \psi_ {\ell - 1} e _ {t + 1} \text {f o r} \ell \geq 1 \tag {9.3.43}
$$

and so

$$
E \left(e _ {t} (\ell)\right) = 0 \text {f o r} \ell \geq 1 \tag {9.3.44}
$$

and

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) = \sigma_ {e} ^ {2} \sum_ {j = 0} ^ {\ell - 1} \psi_ {j} ^ {2} \text {f o r} \ell \geq 1 \tag {9.3.45}
$$

However, for nonstationary series, the $\psi _ { j }$ -weights do not decay to zero as $j$ increases. For example, for the random walk model, $\psi _ { j } = 1$ for all $j$ ; for the IMA(1,1) model, $\psi _ { j } =$ $1 - \theta$ for $j \geq 1$ ; for the IMA(2,2) case, $\Psi _ { j } = \dot { 1 } + \Theta _ { 2 } + ( 1 - \Theta _ { 1 } - \Theta _ { 2 } ) j$ for $j \geq 1$ ; and for the ARI(1,1) model, $\Psi _ { j } = ( 1 - \phi ^ { j + 1 } ) / ( 1 - \phi )$ for $j \geq 1$ (see Chapter 5).

Thus, for any nonstationary model, Equation (9.3.45) shows that the forecast error variance will grow without bound as the lead time l increases. This fact should not be too surprising since with nonstationary series the distant future is quite uncertain.

# 9.4 Prediction Limits

As in all statistical endeavors, in addition to forecasting or predicting the unknown $Y _ { t + \ell }$ we would like to assess the precision of our predictions.

# Deterministic Trends

For the deterministic trend model with a white noise stochastic component $\{ X _ { t } \}$ , we recall that

$$
\hat {Y} _ {t} (\ell) = \mu_ {t + \ell}
$$

and

$$
V a r (e _ {t} (\ell)) = V a r (X _ {t + \ell}) = \gamma_ {0}
$$

If the stochastic component is normally distributed, then the forecast error

$$
e _ {t} (\ell) = Y _ {t + \ell} - \hat {Y} _ {t} (\ell) = X _ {t + \ell} \tag {9.4.1}
$$

is also normally distributed. Thus, for a given confidence level $1 - \alpha$ , we could use a standard normal percentile, $z _ { 1 } - \alpha / 2$ , to claim that

$$
P \left[ - z _ {1 - \alpha / 2} <   \frac {Y _ {t + \ell} - \hat {Y} _ {t} (\ell)}{\sqrt {V a r (e _ {t} (\ell))}} <   z _ {1 - \alpha / 2} \right] = 1 - \alpha
$$

or, equivalently,

$$
P [ \hat {Y} _ {t} (\ell) - z _ {1 - \alpha / 2} \sqrt {\operatorname {V a r} (e _ {t} (\ell))} <   Y _ {t + \ell} <   \hat {Y} _ {t} (\ell) + z _ {1 - \alpha / 2} \sqrt {\operatorname {V a r} (e _ {t} (\ell))} ] = 1 - \alpha
$$

Thus we may be $( 1 - \alpha ) 1 0 0 \%$ confident that the future observation $Y _ { t + \ell }$ will be contained within the prediction limits

$$
\hat {Y} _ {t} (\ell) \pm z _ {1 - \alpha / 2} \sqrt {\operatorname {V a r} \left(e _ {t} (\ell)\right)} \tag {9.4.2}
$$

As a numerical example, consider the monthly average temperature series once more. On page 192, we used the cosine model to predict the June 1976 average temperature as $6 8 . 3 ^ { \circ } \mathrm { F } .$ The estimate of $\sqrt { V a r ( e _ { t } ( \ell ) ) } = \sqrt { \gamma _ { 0 } }$ for this model is $3 . 7 ^ { \circ } \mathrm { F } .$ . Thus $9 5 \%$ prediction limits for the average June 1976 temperature are

$$
6 8. 3 \pm 1. 9 6 (3. 7) = 6 8. 3 \pm 7. 2 5 2 o r 6 1. 0 5 ^ {\circ} F t o 7 5. 5 5 ^ {\circ} F
$$

Readers who are familiar with standard regression analysis will recall that since the forecast involves estimated regression parameters, the correct forecast error variance is given by $\gamma _ { 0 } [ 1 + ( 1 / n ) + c _ { n , \ell } ]$ , where $c _ { n , \ell }$ is a certain function of the sample size $n$ and the lead time l. However, it may be shown that for the types of trends that we are considering (namely, cosines and polynomials in time) and for large sample sizes $n$ , the $1 / n$ and $c _ { n , \ell }$ are both negligible relative to 1. For example, with a cosine trend of period 12 over $N = n / 1 2$ years, we have that $c _ { n , \ell } = 2 / n$ ; thus the correct forecast error variance is

$\gamma _ { 0 } [ 1 + ( 3 / n ) ]$ rather than our approximate $\gamma _ { 0 }$ . For the linear time trend model, it can be shown that $c _ { n , \ell } = 3 ( n + 2 \ell - 1 ) ^ { 2 } \bar { / } [ n ( n ^ { 2 } - 1 ) ] \approx 3 / n$ for moderate lead l and large n. Thus, again our approximation seems justified.

# ARIMA Models

If the white noise terms $\{ \boldsymbol { e } _ { t } \}$ in a general ARIMA series each arise independently from a normal distribution, then from Equation (9.3.43) on page 202, the forecast error $e _ { t } ( \ell )$ will also have a normal distribution, and the steps leading to Equation (9.4.2) remain valid. However, in contrast to the deterministic trend model, recall that in the present case

$$
V a r (e _ {t} (\ell)) = \sigma_ {e} ^ {2} \sum_ {j = 0} ^ {\ell - 1} \psi_ {j} ^ {2}
$$

In practice, $\sigma _ { e } ^ { 2 }$ will be unknown and must be estimated from the observed time series. The necessary $\boldsymbol { \Psi }$ -weights are, of course, also unknown since they are certain functions of the unknown $\phi$ ’s and θ’s. For large sample sizes, these estimations will have little effect on the actual prediction limits given above.

As a numerical example, consider the AR(1) model that we estimated for the industrial color property series. From Exhibit 9.1 on page 194, we use $\Phi = 0 . 5 7 0 5$ , $\mu =$ 74.3293, and $\sigma _ { e } ^ { 2 } = 2 4 . 8$ . For an AR(1) model, we recall Equation (9.3.16) on page 196

$$
V a r (e _ {t} (\ell)) = \sigma_ {e} ^ {2} \bigg [ \frac {1 - \phi^ {2 \ell}}{1 - \phi^ {2}} \bigg ]
$$

For a one-step-ahead prediction, we have

$$
7 0. 1 4 7 9 3 \pm 1. 9 6 \sqrt {2 4 . 8} = 7 0. 1 4 7 9 3 \pm 9. 7 6 0 7 2 1 \text {o r} 6 0. 3 9 \text {t o} 7 9. 9 1
$$

Two steps ahead, we obtain

$$
7 1. 8 6 0 7 2 \pm 1 1. 8 8 3 4 3 o r 6 0. 7 1 t o 8 3. 1 8
$$

Notice that this prediction interval is wider than the previous interval. Forecasting ten steps ahead leads to

$$
7 4. 1 7 3 9 3 4 \pm 1 1. 8 8 4 5 1 o r 6 2. 4 2 t o 8 6. 1 9
$$

By lead 10, both the forecast and the forecast limits have settled down to their long-lead values.

# 9.5 Forecasting Illustrations

Rather than showing forecast and forecast limit calculations, it is often more instructive to display appropriate plots of the forecasts and their limits.

# Deterministic Trends

Exhibit 9.2 displays the last four years of the average monthly temperature time series together with forecasts and $9 5 \%$ forecast limits for two additional years. Since the model fits quite well with a relatively small error variance, the forecast limits are quite close to the fitted trend forecast.

![](images/f5d103a8e251e1d00c16f2fb260bd7863ba6ffa86a03768c8c14bc6b8ba47c93.jpg)  
Exhibit 9.2 Forecasts and Limits for the Temperature Cosine Trend

```txt
> data(temppub)  
> temppub1 = ts(c(temppub, rep(NA, 24)), start = start(temppub), freq = frequency(temppub))  
> har. = harmonic(temppub, 1)  
> m5.temppub = arima(temppub, order = c(0, 0, 0), xreg = har.)  
> newhar. = harmonic(ts(rep(1, 24), start = c(1976, 1), freq = 12), 1)  
> win.graph(width = 4.875, height = 2.5, pointsize = 8)  
> plot(m5.temppub, n.ahead = 24, n1 = c(1972, 1), newxreg = newhar., type = 'b', ylab = 'Temperature', xlab = 'Year') 
```

# ARIMA Models

We use the industrial color property series as our first illustration of ARIMA forecasting. Exhibit 9.3 displays this series together with forecasts out to lead time 12 with the upper and lower $9 5 \%$ prediction limits for those forecasts. In addition, a horizontal line at the estimate for the process mean is shown. Notice how the forecasts approach the mean exponentially as the lead time increases. Also note how the prediction limits increase in width.

![](images/cc18948ec95252a207065dc42851e9f4c8106af4fcaa503cf8eba8e38653541f.jpg)  
Exhibit 9.3 Forecasts and Forecast Limits for the AR(1) Model for Color

```txt
> data(color)  
> m1.color=arima(color,order=c(1,0,0))  
> plot(m1.color,n.ahead=12,type='b',xlab='Time', ylab='Color Property')  
> abline(h=coef(m1.color)[names(coef(m1.color))=='intercept']) 
```

The Canadian hare abundance series was fitted by working with the square root of the abundance numbers and then fitting an AR(3) model. Notice how the forecasts mimic the approximate cycle in the actual series even when we forecast with a lead time out to 25 years in Exhibit 9.4.

![](images/b421d25c67407872af30d1163897c27b42b56ea29640758160adae0559809697.jpg)  
Exhibit 9.4 Forecasts from an AR(3) Model for Sqrt(Hare)

```txt
> data(hare)  
> m1.hare=arima(square(hare), order=c(3,0,0))  
> plot(m1.hare, n.ahead=25,type='b', xlab='Year', ylab='Square(hare)')  
> abline(h=coef(m1.hare)[names(coef(m1.hare)) == 'intercept']) 
```

# 9.6 Updating ARIMA Forecasts

Suppose we are forecasting a monthly time series. Our last observation is, say, for February, and we forecast for March, April, and May. As time goes by, the actual value for March becomes available. With this new value in hand, we would like to update or revise (and, one hopes, improve) our forecasts for April and May. Of course, we could compute new forecasts from scratch. However, there is a simpler way.

For a general forecast origin t and lead time $\ell + 1$ , our original forecast is denoted $\hat { \boldsymbol Y } _ { t } ( \ell + 1 )$ . Once the observation at time $t + 1$ becomes available, we would like to update our forecast as $\hat { Y } _ { t + 1 } ( \ell )$ . Equations (9.3.35) and (9.3.36) on page 200 yield

$$
Y _ {t + \ell + 1} = C _ {t} (\ell + 1) + e _ {t + \ell + 1} + \psi_ {1} e _ {t + \ell} + \psi_ {2} e _ {t + \ell - 1} + \dots + \psi_ {\ell} e _ {t + 1}
$$

Since $C _ { t } ( \ell { + } 1 )$ and $e _ { t + 1 }$ are functions of $Y _ { t + 1 }$ , $Y _ { t } , \dots$ , whereas $e _ { t + \iota + 1 } , e _ { t + \ell } , . . . , e _ { t + 2 }$ are independent of $Y _ { t + 1 }$ , $Y _ { t } , \dots$ , we quickly obtain the expression

$$
\hat {Y} _ {t + 1} (\ell) = C _ {t} (\ell + 1) + \psi_ {\ell} e _ {t + 1}
$$

However, $\hat { Y } _ { t } ( \ell + 1 ) = C _ { t } ( \ell + 1 )$ , and, of course, $e _ { t + 1 } = Y _ { t + 1 } - \hat { Y } _ { t } ( 1 )$ . Thus we have the general updating equation

$$
\hat {Y} _ {t + 1} (\ell) = \hat {Y} _ {t} (\ell + 1) + \psi_ {\ell} \left[ Y _ {t + 1} - \hat {Y} _ {t} (1) \right] \tag {9.6.1}
$$

Notice that $[ Y _ { t + 1 } - \hat { Y } _ { t } ( 1 ) ]$ is the actual forecast error at time $t + 1$ once $Y _ { t + 1 }$ has been observed.

As a numerical example, consider the industrial color property time series. Following Exhibit 9.1 on page 194, we fit an AR(1) model to forecast one step ahead as $\hat { Y } _ { 3 5 } ( 1 ) = 7 0 . 0 9 6$ and twoailable as $\hat { Y } _ { 3 5 } ( 2 ) = 7 1 . 8 6 0 7 2$ . If now the next forecast for time $Y _ { t + 1 } = Y _ { 3 6 } = 6 5$ $t = 3 7$ as

$$
\hat {Y} _ {t + 1} (1) = \hat {Y} _ {3 6} (1) = 7 1. 8 6 0 7 2 + 0. 5 7 0 5 (6 5 - 7 0. 0 9 6) = 6 8. 9 5 3 4 5 2
$$

# 9.7 Forecast Weights and Exponentially Weighted Moving Averages

For ARIMA models without moving average terms, it is clear how the forecasts are explicitly determined from the observed series $Y _ { t } , Y _ { t - 1 } , . . . , Y _ { 1 }$ . However, for any model with $q > 0$ , the noise terms appear in the forecasts, and the nature of the forecasts explicitly in terms of Yt, Yt − 1,…, $Y _ { 1 }$ is hidden. To bring out this aspect of the forecasts, we return to the inverted form of any invertible ARIMA process, namely

$$
Y _ {t} = \pi_ {1} Y _ {t - 1} + \pi_ {2} Y _ {t - 2} + \pi_ {3} Y _ {t - 3} + \dots + e _ {t}
$$

(See Equation (4.5.5) on page 80.) Thus we can also write

$$
Y _ {t + 1} = \pi_ {1} Y _ {t} + \pi_ {2} Y _ {t - 1} + \pi_ {3} Y _ {t - 2} + \dots + e _ {t + 1}
$$

Taking conditional expectations of both sides, given $Y _ { t } , Y _ { t - 1 } , . . . , Y _ { 1 }$ $Y _ { 1 }$ , we obtain

$$
\hat {Y} _ {t} (1) = \pi_ {1} Y _ {t} + \pi_ {2} Y _ {t - 1} + \pi_ {3} Y _ {t - 2} + \dots \tag {9.7.1}
$$

(We are assuming the $t$ is sufficiently large and/or that the $\pi$ -weights die out sufficiently quickly so that $\pi _ { t } , \pi _ { t + 1 } , \ldots$ are all negligible.)

For any invertible ARIMA model, the $\pi$ -weights can be calculated recursively from the expressions

$$
\pi_ {j} = \left\{ \begin{array}{l} \min  (j, q) \\ \sum_ {i = 1} ^ {m i n (j, q)} \theta_ {i} \pi_ {j - i} + \varphi_ {j} \text {f o r} 1 \leq j \leq p + d \\ \sum_ {i = 1} ^ {m i n (j, q)} \theta_ {i} \pi_ {j - i} \text {f o r} j > p + d \end{array} \right. \tag {9.7.2}
$$

with initial value $\pi _ { 0 } = - 1$ . (Compare this with Equations (4.4.7) on page 79 for the $\boldsymbol { \Psi }$ -weights.)

Consider in particular the nonstationary IMA(1,1) model

$$
Y _ {t} = Y _ {t - 1} + e _ {t} - \theta e _ {t - 1}
$$

Here $p = 0$ , $d = 1$ , $q = 1$ , with $\Phi _ { 1 } = 1$ ; thus

$$
\pi_ {1} = \theta \pi_ {0} + 1 = 1 - \theta
$$

$$
\pi_ {2} = \theta \pi_ {1} = \theta (1 - \theta)
$$

and, generally,

$$
\pi_ {j} = \theta \pi_ {j - 1} \mathrm {f o r} j > 1
$$

Thus we have explicitly

$$
\pi_ {j} = (1 - \theta) \theta^ {j - 1} \text {f o r} j \geq 1 \tag {9.7.3}
$$

so that, from Equation (9.7.1), we can write

$$
\hat {Y} _ {t} (1) = (1 - \theta) Y _ {t} + (1 - \theta) \theta Y _ {t - 1} + (1 - \theta) \theta^ {2} Y _ {t - 2} + \dots \tag {9.7.4}
$$

In this case, the $\pi$ -weights decrease exponentially, and furthermore,

$$
\sum_ {j = 1} ^ {\infty} \pi_ {j} = (1 - \theta) \sum_ {j = 1} ^ {\infty} \theta^ {j - 1} = \frac {1 - \theta}{1 - \theta} = 1
$$

Thus $\hat { Y } _ { t } ( 1 )$ is called an exponentially weighted moving average (EWMA).

Simple algebra shows that we can also write

$$
\hat {Y} _ {t} (1) = (1 - \theta) Y _ {t} + \theta \hat {Y} _ {t - 1} (1) \tag {9.7.5}
$$

and

$$
\hat {Y} _ {t} (1) = \hat {Y} _ {t - 1} (1) + (1 - \theta) \left[ Y _ {t} - \hat {Y} _ {t - 1} (1) \right] \tag {9.7.6}
$$

Equations (9.7.5) and (9.7.6) show how to update forecasts from origin $t - 1$ to origin $t$ , and they express the result as a linear combination of the new observation and the old forecast or in terms of the old forecast and the last observed forecast error.

Using EWMA to forecast time series has been advocated, mostly on an ad hoc basis, for a number of years; see Brown (1962) and Montgomery and Johnson (1976).

The parameter 1 − θ is called the smoothing constant in EWMA literature, and its selection (estimation) is often quite arbitrary. From the ARIMA model-building approach, we let the data indicate whether an IMA(1,1) model is appropriate for the series under consideration. If so, we then estimate θ in an efficient manner and compute an EWMA forecast that we are confident is the minimum mean square error forecast. A comprehensive treatment of exponential smoothing methods and their relationships with ARIMA models is given in Abraham and Ledolter (1983).

# 9.8 Forecasting Transformed Series

# Differencing

Suppose we are interested in forecasting a series whose model involves a first difference to achieve stationarity. Two methods of forecasting can be considered:

1. forecasting the original nonstationary series, for example by using the difference equation form of Equation (9.3.28) on page 199, with $\boldsymbol { \Phi }$ ’s replaced by $\boldsymbol { \Phi }$ ’s throughout, or   
2. forecasting the stationary differenced series $W _ { t } = Y _ { t } - Y _ { t - 1 }$ and then “undoing” the difference by summing to obtain the forecast in original terms.

We shall show that both methods lead to the same forecasts. This follows essentially because differencing is a linear operation and because conditional expectation of a linear combination is the same linear combination of the conditional expectations.

Consider in particular the IMA(1,1) model. Basing our work on the original nonstationary series, we forecast as

$$
\hat {Y} _ {t} (1) = Y _ {t} - \theta e _ {t} \tag {9.8.1}
$$

and

$$
\hat {Y} _ {t} (\ell) = \hat {Y} _ {t} (\ell - 1) \text {f o r} \ell > 1 \tag {9.8.2}
$$

Consider now the differenced stationary MA(1) series $W _ { t } = Y _ { t } - Y _ { t - 1 }$ . We would forecast $W _ { t + \ell }$ as

$$
\hat {W} _ {t} (1) = - \theta e _ {t} \tag {9.8.3}
$$

and

$$
\hat {W} _ {t} (\ell) = 0 \text {f o r} \ell > 1 \tag {9.8.4}
$$

However, $\hat { W } _ { t } ( 1 ) = \hat { Y } _ { t } ( 1 ) - Y _ { t }$ ; thus $\hat { W } _ { t } ( 1 ) = - \Theta e _ { t }$ is equivalent to $\widehat { Y } _ { t } ( 1 ) = Y _ { t } - \theta e _ { t }$ as before. Similarly, $\hat { W } _ { t } ( \ell ) ~ = ~ \hat { Y } _ { t } ( \ell ) - \hat { Y } _ { t } ( \ell - 1 )$ , and Equation (9.8.4) becomes Equation (9.8.2), as we have claimed.

The same result would apply to any model involving differences of any order and indeed to any type of linear transformation with constant coefficients. (Certain linear transformations other than differencing may be applicable to seasonal time series. See Chapter 10.)

# Log Transformations

As we saw earlier, it is frequently appropriate to model the logarithms of the original series—a nonlinear transformation. Let $Y _ { t }$ denote the original series value and let $Z _ { t } =$ $\log ( Y _ { t } )$ . It can be shown that we always have

$$
E \left(Y _ {t + \ell} \mid Y _ {t}, Y _ {t - 1}, \dots , Y _ {1}\right) \geq \exp \left[ E \left(Z _ {t + \ell} \mid Z _ {t}, Z _ {t - 1}, \dots , Z _ {1}\right) \right] \tag {9.8.5}
$$

with equality holding only in trivial cases. Thus, the naive forecast exp $[ \hat { Z } _ { t } ( \ell ) ]$ is not the minimum mean square error forecast of $Y _ { t + \ell } .$ To evaluate the minimum mean square error forecast in original terms, we shall find the following fact useful: If $X$ has a normal distribution with mean $\mu$ and variance $\sigma ^ { 2 }$ , then

$$
E [ \exp (X) ] = \exp \left[ \mu + \frac {\sigma^ {2}}{2} \right]
$$

(This follows, for example, from the moment-generating function for $X$ .) In our application

$$
\mu = E (Z _ {t + \ell} | Z _ {t}, Z _ {t - 1}, \dots , Z _ {1})
$$

and

$$
\begin{array}{l} \sigma^ {2} = \operatorname {V a r} \left(Z _ {t + \ell} \mid Z _ {t}, Z _ {t - 1}, \dots , Z _ {1}\right) \\ = \operatorname {V a r} [ e _ {t} (\ell) + C _ {t} (\ell) | Z _ {r}, Z _ {t - 1}, \dots , Z _ {1} ] \\ = \operatorname {V a r} \left[ e _ {t} (\ell) \mid Z _ {t}, Z _ {t - 1}, \dots , Z _ {1} \right] + \operatorname {V a r} \left[ C _ {t} (\ell) \mid Z _ {t}, Z _ {t - 1}, \dots , Z _ {1} \right] \\ = \operatorname {V a r} \left[ e _ {t} (\ell) \mid Z _ {t}, Z _ {t - 1}, \dots , Z _ {1} \right] \\ = \operatorname {V a r} \left[ e _ {t} (\ell) \right] \\ \end{array}
$$

These follow from Equations (9.3.35) and (9.3.36) (applied to $Z _ { t } )$ and the fact that $C _ { t } ( l )$ is a function of $Z _ { t } , Z _ { t - 1 } , \dots$ , whereas $e _ { t } ( \ell )$ is independent of $Z _ { t } , Z _ { t - 1 } , \dots .$ Thus the minimum mean square error forecast in the original series is given by

$$
\exp \left\{\hat {Z} _ {t} (\ell) + \frac {1}{2} \operatorname {V a r} \left[ e _ {t} (\ell) \right] \right\} \tag {9.8.6}
$$

Throughout our discussion of forecasting, we have assumed that minimum mean square forecast error is the criterion of choice. For normally distributed variables, this is an

excellent criterion. However, if $Z _ { t }$ has a normal distribution, then $Y _ { t } = \exp ( Z _ { t } )$ has a lognormal distribution, for which a different criterion may be desirable. In particular, since the log-normal distribution is asymmetric and has a long right tail, a criterion based on the mean absolute error may be more appropriate. For this criterion, the optimal forecast is the median of the distribution of $Z _ { t + \ell }$ conditional on $Z _ { t } , Z _ { t - 1 } , . . . , Z _ { 1 }$ . Since the log transformation preserves medians and since, for a normal distribution, the mean and median are identical, the naive forecast exp $\lceil \hat { Z } _ { t } ( \ell ) \rceil$ is the optimal forecast for $Y _ { t + \ell }$ in the sense that it minimizes the mean absolute forecast error.

# 9.9 Summary of Forecasting with Certain ARIMA Models

Here we bring together various forecasting results for special ARIMA models.

$$
\mathbf {A R} (1): Y _ {t} = \mu + \phi \left(Y _ {t - 1} - \mu\right) + e _ {t}
$$

$$
\begin{array}{l} \hat {Y} _ {t} (\ell) = \mu + \phi [ \hat {Y} _ {t} (\ell - 1) - \mu ] \quad \text {f o r} \ell \geq 1 \\ = \mu + \phi^ {\ell} \left(Y _ {t} - \mu\right) \text {f o r} l \geq 1 \\ \end{array}
$$

${ \hat { Y } } _ { t } ( \ell ) \approx \mu { \mathrm { ~  f o r ~ l a r g e ~ } } \ell$

$$
\begin{array}{l} e _ {t} (\ell) = e _ {t + \ell} + \phi e _ {t + \ell - 1} + \dots + \phi^ {\ell - 1} e _ {t + 1} \\ V a r (e _ {t} (\ell)) = \sigma_ {e} ^ {2} \biggl [ \frac {1 - \phi^ {2 \ell}}{1 - \phi^ {2}} \biggr ] \\ V a r (e _ {t} (\ell)) \approx \frac {\sigma_ {e} ^ {2}}{1 - \phi^ {2}} = \gamma_ {0} \mathrm {f o r l a r g e} \ell \\ \psi_ {j} = \phi^ {j} \text {f o r} j > 0 \\ \end{array}
$$

$$
\begin{array}{l} \mathbf {M A} (1): Y _ {t} = \mu + e _ {t} - \theta e _ {t - 1} \\ \hat {Y} _ {t} (1) = \mu - \theta e _ {t} \\ \hat {Y} _ {t} (\ell) = \mu \text {f o r} l > 1 \\ e _ {t} (1) = e _ {t + 1} \\ e _ {t} (\ell) = e _ {t + \ell} - \theta e _ {t + \ell - 1} \text {f o r} \ell > 1 \\ V a r (e _ {t} (\ell)) = \left\{ \begin{array}{l} \sigma_ {e} ^ {2} \text {f o r} \ell = 1 \\ \sigma_ {e} ^ {2} (1 + \theta^ {2}) \text {f o r} \ell > 1 \end{array} \right. \\ \end{array}
$$

$$
\psi_ {j} = \left\{ \begin{array}{l l} - \theta & \text {f o r} j = 1 \\ 0 & \text {f o r} j > 1 \end{array} \right.
$$

IMA (1,1) with Constant Term: $Y _ { t } = Y _ { t - 1 } + \theta _ { 0 } + e _ { t } - \theta e _ { t - 1 }$

$$
\begin{array}{l} \hat {\dot {Y}} _ {t} (\ell) = \hat {\dot {Y}} _ {t} (\ell - 1) + \theta_ {0} - \theta e _ {t} \\ {\bf \nabla} = Y _ {t} + \ell \theta_ {0} - \theta e _ {t} \\ \end{array}
$$

$$
\hat {Y} _ {t} (1) = (1 - \theta) Y _ {t} + (1 - \theta) \theta Y _ {t - 1} + (1 - \theta) \theta^ {2} Y _ {t - 2} + \dots (\mathrm {t h e E W M A f o r} \theta_ {0} = 0)
$$

$$
e _ {t} (\ell) = e _ {t + \ell} + (1 - \theta) e _ {t + \ell - 1} + (1 - \theta) e _ {t + \ell - 2} + \dots + (1 - \theta) e _ {t + 1} \mathrm {f o r} \ell \geq 1
$$

$$
V a r (e _ {t} (\ell)) = \sigma_ {e} ^ {2} [ 1 + (\ell - 1) (1 - \theta) ^ {2} ]
$$

$$
\psi_ {j} = 1 - \theta \text {f o r} j > 0
$$

Note that if $\theta _ { 0 } \neq 0$ , the forecasts follow a straight line with slope $\theta _ { 0 }$ , but if $\theta _ { 0 } = 0$ , which is the usual case, then the forecast is the same for all lead times, namely

$$
\hat {Y} _ {t} (\ell) = Y _ {t} - \theta e _ {t}
$$

$$
\mathbf {I M A} (2, 2): Y _ {t} = 2 Y _ {t - 1} - Y _ {t - 2} + \theta_ {0} + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2}
$$

$$
\left. \begin{array}{l} \hat {Y} _ {t} (1) = 2 Y _ {t} - Y _ {t - 1} + \theta_ {0} - \theta_ {1} e _ {t} - \theta_ {2} e _ {t - 1} \\ \hat {Y} _ {t} (2) = 2 \hat {Y} _ {t} (1) - Y _ {t} + \theta_ {0} - \theta_ {2} e _ {t} \\ \hat {Y} _ {t} (\ell) = 2 \hat {Y} _ {t} (\ell - 1) - \hat {Y} _ {t} (\ell - 2) + \theta_ {0} \text {f o r} \ell > 2 \end{array} \right\} \tag {9.9.1}
$$

$$
\hat {Y} _ {t} (\ell) = A + B \ell + \frac {\theta_ {0}}{2} \ell^ {2} \tag {9.9.2}
$$

where

$$
A = 2 \hat {Y} _ {t} (1) - \hat {Y} _ {t} (2) + \theta_ {0} \tag {9.9.3}
$$

and

$$
B = \hat {Y} _ {t} (2) - \hat {Y} _ {t} (1) - \frac {3}{2} \theta_ {0} \tag {9.9.4}
$$

If $\theta _ { 0 } \neq 0$ , the forecasts follow a quadratic curve in l, but if $\theta _ { 0 } = 0$ , the forecasts form a straight line with slope $\hat { Y } _ { t } ( 2 ) - \hat { Y } _ { t } ( 1 )$ and will pass through the two initial forecasts $\hat { Y } _ { t } ( 1 )$ and $\hat { Y } _ { t } ( 2 )$ . It can be shown that $V a r ( e _ { t } ( \ell ) )$ is a certain cubic function of l ; see Box, Jenkins, and Reinsel (1994, p. 156). We also have

$$
\psi_ {j} = 1 + \theta_ {2} + (1 - \theta_ {1} - \theta_ {2}) j \text {f o r} j > 0 \tag {9.9.5}
$$

It can also be shown that forecasting the special case with $\theta _ { 1 } = 2 \omega$ and $\mathsf { \theta } _ { 2 } = - \mathsf { \omega } ^ { 2 }$ is equivalent to so-called double exponential smoothing with smoothing constant $1 - \infty$ ; see Abraham and Ledolter (1983).

# 9.10 Summary

Forecasting or predicting future as yet unobserved values is one of the main reasons for developing time series models. Methods discussed in this chapter are all based on minimizing the mean square forecasting error. When the model is simply deterministic trend plus zero mean white noise error, forecasting amounts to extrapolating the trend. However, if the model contains autocorrelation, the forecasts exploit the correlation to produce better forecasts than would otherwise be obtained. We showed how to do this with ARIMA models and investigated the computation and properties of the forecasts. In special cases, the computation and properties of the forecasts are especially interesting and we presented them separately. Prediction limits are especially important to assess the potential accuracy (or otherwise) of the forecasts. Finally, we addressed the problem of forecasting time series for which the models involve transformation of the original series.

# EXERCISES

9.1 For an AR(1) model with $Y _ { t } = 1 2 . 2$ , $\phi = - 0 . 5$ , and $\mu = 1 0 . 8$ ,

(a) Find $\hat { Y } _ { t } ( 1 ) _ { \dot { \Lambda } }$   
(b) Calculate $\smash { \ddot { Y } _ { t } ( 2 ) }$ in two different ways.   
(c) Calculate $\dot { Y } _ { t } ( 1 0 )$

9.2 Suppose that annual sales (in millions of dollars) of the Acme Corporation follow the AR(2) model $Y _ { t } = 5 + 1 . 1 Y _ { t - 1 } - 0 . 5 Y _ { t - 2 } + e _ { t }$ with $\sigma _ { e } ^ { 2 } = 2$ .

(a) If sales for 2005, 2006, and 2007 were $\$ 9$ million, $\$ 11$ million, and $\$ 10$ million, respectively, forecast sales for 2008 and 2009.   
(b) Show that $\psi _ { 1 } = 1 . 1$ for this model.   
(c) Calculate $9 5 \%$ prediction limits for your forecast in part (a) for 2008.   
(d) If sales in 2008 turn out to be $\$ 12$ million, update your forecast for 2009.

9.3 Using the estimated cosine trend on page 192:

(a) Forecast the average monthly temperature in Dubuque, Iowa, for April 1976.   
(b) Find a $9 5 \%$ prediction interval for that April forecast. (The estimate of $\sqrt { \gamma _ { 0 } }$ for this model is $3 . 7 1 9 ^ { \circ } \mathrm { F } .$ )   
(c) What is the forecast for April, 1977? For April 2009?

9.4 Using the estimated cosine trend on page 192:

(a) Forecast the average monthly temperature in Dubuque, Iowa, for May 1976.   
(b) Find a $9 5 \%$ prediction interval for that May 1976 forecast. (The estimate of $\sqrt { \gamma _ { 0 } }$ for this model is $3 . 7 1 9 ^ { \circ } \mathrm { F } .$ )

9.5 Using the seasonal means model without an intercept shown in Exhibit 3.3 on page 32:

(a) Forecast the average monthly temperature in Dubuque, Iowa, for April, 1976.   
(b) Find a $9 5 \%$ prediction interval for that April forecast. (The estimate of $\sqrt { \gamma _ { 0 } }$ for this model is $3 . 4 1 9 ^ { \circ } \mathrm { F } .$ )   
(c) Compare your forecast with the one obtained in Exercise 9.3.   
(d) What is the forecast for April 1977? April 2009?

9.6 Using the seasonal means model with an intercept shown in Exhibit 3.4 on page 33:

(a) Forecast the average monthly temperature in Dubuque, Iowa, for April 1976.   
(b) Find a $9 5 \%$ prediction interval for that April forecast. (The estimate of $\sqrt { \gamma _ { 0 } }$ for this model is $3 . 4 1 9 ^ { \circ } \mathrm { F } .$ )   
(c) Compare your forecast with the one obtained in Exercise 9.5.

9.7 Using the seasonal means model with an intercept shown in Exhibit 3.4 on page 33

(a) Forecast the average monthly temperature in Dubuque, Iowa, for January 1976.   
(b) Find a $9 5 \%$ prediction interval for that January forecast. (The estimate of $\sqrt { \gamma _ { 0 } }$ for this model is $3 . 4 1 9 ^ { \circ } \mathrm { F } .$ )

9.8 Consider the monthly electricity generation time series shown in Exhibit 5.8 on page 99. The data are in the file named electricity.

(a) Fit a deterministic trend model containing seasonal means together with a linear time trend to the logarithms of the electricity values.   
(b) Plot the last five years of the series together with two years of forecasts and the $9 5 \%$ forecast limits. Interpret the plot.

9.9 Simulate an AR(1) process with $\phi = 0 . 8$ and $\mu = 1 0 0$ . Simulate 48 values but set aside the last 8 values to compare forecasts to actual values.

(a) Using the first 40 values of the series, find the values for the maximum likelihood estimates of $\phi$ and $\mu$ .   
(b) Using the estimated model, forecast the next eight values of the series. Plot the series together with the eight forecasts. Place a horizontal line at the estimate of the process mean.   
(c) Compare the eight forecasts with the actual values that you set aside.   
(d) Plot the forecasts together with $9 5 \%$ forecast limits. Do the actual values fall within the forecast limits?   
(e) Repeat parts (a) through (d) with a new simulated series using the same values of the parameters and the same sample size.

9.10 Simulate an AR(2) process with $\phi _ { 1 } = 1 . 5$ , $\Phi _ { 2 } = - 0 . 7 5$ , and $\mu = 1 0 0$ . Simulate 52 values but set aside the last 12 values to compare forecasts to actual values.

(a) Using the first 40 values of the series, find the values for the maximum likelihood estimates of the $\phi$ ’s and $\mu$ .   
(b) Using the estimated model, forecast the next 12 values of the series. Plot the series together with the 12 forecasts. Place a horizontal line at the estimate of

the process mean.

(c) Compare the 12 forecasts with the actual values that you set aside.   
(d) Plot the forecasts together with $9 5 \%$ forecast limits. Do the actual values fall within the forecast limits?   
(e) Repeat parts (a) through (d) with a new simulated series using the same values of the parameters and same sample size.

9.11 Simulate an MA(1) process with $\theta = 0 . 6$ and $\mu = 1 0 0$ . Simulate 36 values but set aside the last 4 values to compare forecasts to actual values.

(a) Using the first 32 values of the series, find the values for the maximum likelihood estimates of the θ and $\mu$ .   
(b) Using the estimated model, forecast the next four values of the series. Plot the series together with the four forecasts. Place a horizontal line at the estimate of the process mean.   
(c) Compare the four forecasts with the actual values that you set aside.   
(d) Plot the forecasts together with $9 5 \%$ forecast limits. Do the actual values fall within the forecast limits?   
(e) Repeat parts (a) through (d) with a new simulated series using the same values of the parameters and same sample size.

9.12 Simulate an MA(2) process with $\theta _ { 1 } = 1$ , $\theta _ { 2 } = - 0 . 6$ , and $\mu = 1 0 0$ . Simulate 36 values but set aside the last 4 values with compare forecasts to actual values.

(a) Using the first 32 values of the series, find the values for the maximum likelihood estimates of the θ’s and $\mu$ .   
(b) Using the estimated model, forecast the next four values of the series. Plot the series together with the four forecasts. Place a horizontal line at the estimate of the process mean.   
(c) What is special about the forecasts at lead times 3 and 4?   
(d) Compare the four forecasts with the actual values that you set aside.   
(e) Plot the forecasts together with $9 5 \%$ forecast limits. Do the actual values fall within the forecast limits?   
(f) Repeat parts (a) through (e) with a new simulated series using the same values of the parameters and same sample size.

9.13 Simulate an ARMA(1,1) process with $\phi = 0 . 7$ , $\theta = - 0 . 5$ , and $\mu = 1 0 0$ . Simulate 50 values but set aside the last 10 values to compare forecasts with actual values.

(a) Using the first 40 values of the series, find the values for the maximum likelihood estimates of $\phi$ , θ, and $\mu$ .   
(b) Using the estimated model, forecast the next ten values of the series. Plot the series together with the ten forecasts. Place a horizontal line at the estimate of the process mean.   
(c) Compare the ten forecasts with the actual values that you set aside.   
(d) Plot the forecasts together with $9 5 \%$ forecast limits. Do the actual values fall within the forecast limits?   
(e) Repeat parts (a) through (d) with a new simulated series using the same values of the parameters and same sample size.

9.14 Simulate an IMA(1,1) process with $\theta = 0 . 8$ and $\theta _ { 0 } = 0$ . Simulate 35 values, but set aside the last five values to compare forecasts with actual values.

(a) Using the first 30 values of the series, find the value for the maximum likelihood estimate of θ.   
(b) Using the estimated model, forecast the next five values of the series. Plot the series together with the five forecasts. What is special about the forecasts?   
(c) Compare the five forecasts with the actual values that you set aside.   
(d) Plot the forecasts together with $9 5 \%$ forecast limits. Do the actual values fall within the forecast limits?   
(e) Repeat parts (a) through (d) with a new simulated series using the same values of the parameters and same sample size.

9.15 Simulate an IMA(1,1) process with $\theta = 0 . 8$ and $\theta _ { 0 } = 1 0$ . Simulate 35 values, but set aside the last five values to compare forecasts to actual values.

(a) Using the first 30 values of the series, find the values for the maximum likelihood estimates of θ and $\theta _ { 0 }$ .   
(b) Using the estimated model, forecast the next five values of the series. Plot the series together with the five forecasts. What is special about these forecasts?   
(c) Compare the five forecasts with the actual values that you set aside.   
(d) Plot the forecasts together with $9 5 \%$ forecast limits. Do the actual values fall within the forecast limits?   
(e) Repeat parts (a) through (d) with a new simulated series using the same values of the parameters and same sample size.

9.16 Simulate an IMA(2,2) process with $\theta _ { 1 } = 1$ , $\theta _ { 2 } = - 0 . 7 5$ , and $\theta _ { 0 } = 0$ . Simulate 45 values, but set aside the last five values to compare forecasts with actual values.

(a) Using the first 40 values of the series, find the value for the maximum likelihood estimate of $\theta _ { 1 }$ and $\theta _ { 2 }$ .   
(b) Using the estimated model, forecast the next five values of the series. Plot the series together with the five forecasts. What is special about the forecasts?   
(c) Compare the five forecasts with the actual values that you set aside.   
(d) Plot the forecasts together with $9 5 \%$ forecast limits. Do the actual values fall within the forecast limits?   
(e) Repeat parts (a) through (d) with a new simulated series using the same values of the parameters and same sample size.

9.17 Simulate an IMA(2,2) process with $\theta _ { 1 } = 1$ , $\theta _ { 2 } = - 0 . 7 5$ , and $\theta _ { 0 } = 1 0$ . Simulate 45 values, but set aside the last five values to compare forecasts with actual values.

(a) Using the first 40 values of the series, find the values for the maximum likelihood estimates of $\theta _ { 1 }$ , $\theta _ { 2 }$ , and $\theta _ { 0 }$ .   
(b) Using the estimated model, forecast the next five values of the series. Plot the series together with the five forecasts. What is special about these forecasts?   
(c) Compare the five forecasts with the actual values that you set aside.   
(d) Plot the forecasts together with $9 5 \%$ forecast limits. Do the actual values fall within the forecast limits?   
(e) Repeat parts (a) through (d) with a new simulated series using the same values of the parameters and same sample size.

9.18 Consider the model $Y _ { t } = \beta _ { 0 } + \beta _ { 1 } t + X _ { t }$ , where $X _ { t } = \Phi X _ { t - 1 } + e _ { t }$ . We assume that $\beta _ { 0 } , \beta _ { 1 }$ , and $\phi$ are known. Show that the minimum mean square error forecast l steps ahead can be written as $\hat { Y } _ { t } ( \ell ) = \beta _ { 0 } + \beta _ { 1 } ( t + \ell ) + \phi ^ { \ell } ( Y _ { t } \overset { \cdot } { - } \beta _ { 0 } - \beta _ { 1 } t )$ .

9.19 Verify Equation (9.3.16) on page 196.

9.20 Verify Equation (9.3.32) on page 200.

9.21 The data file named deere3 contains 57 consecutive values from a complex machine tool process at Deere & Co. The values given are deviations from a target value in units of ten millionths of an inch. The process employs a control mechanism that resets some of the parameters of the machine tool depending on the magnitude of deviation from target of the last item produced.

(a) Using an AR(1) model for this series, forecast the next ten values.   
(b) Plot the series, the forecasts, and $9 5 \%$ forecast limits, and interpret the results.

9.22 The data file named days contains accounting data from the Winegard Co. of Burlington, Iowa. The data are the number of days until Winegard receives payment for 130 consecutive orders from a particular distributor of Winegard products. (The name of the distributor must remain anonymous for confidentiality reasons.) The time series contains outliers that are quite obvious in the time series plot. Replace each of the unusual values at “times” 63, 106, and 129 with the much more typical value of 35 days.

(a) Use an MA(2) model to forecast the next ten values of this modified series.   
(b) Plot the series, the forecasts, and $9 5 \%$ forecast limits, and interpret the results.

9.23 The time series in the data file robot gives the final position in the $^ { \ast } x$ -direction” after an industrial robot has finished a planned set of exercises. The measurements are expressed as deviations from a target position. The robot is put through this planned set of exercises in the hope that its behavior is repeatable and thus predictable.

(a) Use an IMA(1,1) model to forecast five values ahead. Obtain $9 5 \%$ forecast limits also.   
(b) Display the forecasts, forecast limits, and actual values in a graph and interpret the results.   
(c) Now use an ARMA(1,1) model to forecast five values ahead and obtain $9 5 \%$ forecast limits. Compare these results with those obtained in part (a).

9.24 Exhibit 9.4 on page 206 displayed the forecasts and $9 5 \%$ forecast limits for the square root of the Canadian hare abundance. The data are in the file named hare. Produce a similar plot in original terms. That is, plot the original abundance values together with the squares of the forecasts and squares of the forecast limits.

9.25 Consider the seasonal means plus linear time trend model for the logarithms of the monthly electricity generation time series in Exercise 9.8. (The data are in the file named electricity.)

(a) Find the two-year forecasts and forecast limits in original terms. That is, exponentiate (antilog) the results obtained in Exercise 9.8.   
(b) Plot the last five years of the original time series together with two years of forecasts and the $9 5 \%$ forecast limits, all in original terms. Interpret the plot.

# Appendix E: Conditional Expectation

If $X$ and Y have joint pdf $f ( x , y )$ and we denote the marginal pdf of $X$ by $f ( x )$ , then the conditional pdf of $Y$ given $X = x$ is given by

$$
f (y | x) = \frac {f (x , y)}{f (x)}
$$

For a given value of $x$ , the conditional pdf has all of the usual properties of a pdf. In particular, the conditional expectation of $Y$ given $X = x$ is defined as

$$
E (Y | X = x) = \int_ {- \infty} ^ {\infty} y f (y | x) d y
$$

As an expected value or mean, the conditional expectation of $Y$ given $X = x$ has all of the usual properties. For example,

$$
E (a Y + b Z + c | X = x) = a E (Y | X = x) + b E (Z | X = x) + c \tag {9.E.1}
$$

and

$$
E [ h (Y) | X = x ] = \int_ {- \infty} ^ {\infty} y f (y | x) d x \tag {9.E.2}
$$

In addition, several new properties arise:

$$
E [ h (X) | X = x ] = h (x) \tag {9.E.3}
$$

That is, given $X = x$ , the random variable $h ( X )$ can be treated like a constant $h ( x )$ . More generally,

$$
E [ h (X, Y) | X = x ] = E (h (x, Y) | X = x) \tag {9.E.4}
$$

If we set $E ( Y | X { = } x ) = g ( x )$ , then $g ( X )$ is a random variable and we can consider $E [ g ( X ) ]$ . It can be shown that

$$
E [ g (X) ] = E (Y)
$$

which is often written as

$$
E [ E (Y | X) ] = E (Y) \tag {9.E.5}
$$

If $Y$ and $X$ are independent, then

$$
E (Y | X) = E (Y) \tag {9.E.6}
$$

# Appendix F: Minimum Mean Square Error Prediction

Suppose Y is a random variable with mean $\mu _ { Y }$ and variance $\sigma _ { Y } ^ { 2 }$ . If our object is to predict Y using only a constant $c$ , what is the best choice for $c ?$ Clearly, we must first define best. A common (and convenient) criterion is to choose $c$ to minimize the mean square error of prediction, that is, to minimize

$$
g (c) = E [ (Y - c) ^ {2} ]
$$

If we expand $g ( c )$ , we have

$$
g (c) = E (Y ^ {2}) - 2 c E (Y) + c ^ {2}
$$

Since $g ( c )$ is quadratic in $c$ and opens upward, solving $g ^ { \prime } ( c ) = 0$ will produce the required minimum. We have

$$
g ^ {\prime} (c) = - 2 E (Y) + 2 c
$$

so that the optimal $c$ is

$$
c = E (Y) = \mu \tag {9.F.1}
$$

Note also that

$$
\min  _ {- \infty <   c <   \infty} g (c) = E (Y - \mu) ^ {2} = \sigma_ {Y} ^ {2} \tag {9.F.2}
$$

Now consider the situation where a second random variable $X$ is available and we wish to use the observed value of $X$ to help predict Y. Let $\rho = C o r r \left( X , Y \right)$ . We first suppose, for simplicity, that only linear functions $a + b X$ can be used for the prediction. The mean square error is then given by

$$
g (a, b) = E (Y - a - b X) ^ {2}
$$

and expanding we gave

$$
g (a, b) = E (Y ^ {2}) + a ^ {2} + b ^ {2} E (X ^ {2}) - 2 a E (Y) + 2 a b E (X) - 2 b E (X Y)
$$

This is also quadratic in $a$ and $^ b$ and opens upward. Thus we can find the point of minimum by solving simultaneous linear equations $\partial g ( a , b ) / \partial a = 0$ and $\hat { \sigma } g ( a , b ) / \hat { \sigma } b = 0$ . We have

$$
\begin{array}{l} \partial g (a, b) / \partial a = 2 a - 2 E (Y) + 2 b E (X) = 0 \\ \partial g (a, b) / \partial b = 2 b E (X ^ {2}) + 2 a E (X) - 2 E (X Y) = 0 \\ \end{array}
$$

which we rewrite as

$$
\begin{array}{l} a + E (X) b = E (Y) \\ E (X) a + E (X ^ {2}) b = E X Y \\ \end{array}
$$

Multiplying the first equation by $E ( X )$ and subtracting yields

$$
b = \frac {E (X Y) - E (X) E (Y)}{E \left(X ^ {2}\right) - \left[ E (X) \right] ^ {2}} = \frac {\operatorname {C o v} (X , Y)}{\operatorname {V a r} (X)} = \rho \frac {\sigma_ {Y}}{\sigma_ {X}} \tag {9.F.3}
$$

Then

$$
a = E (Y) - b E (X) = \mu_ {Y} - \rho \frac {\sigma_ {Y}}{\sigma_ {X}} \mu_ {X} \tag {9.F.4}
$$

If we let $\hat { Y }$ be the minimum mean square error prediction of $Y$ based on a linear function of $X$ , then we can write

$$
\hat {Y} = \left[ \mu_ {Y} - \rho \frac {\sigma_ {Y}}{\sigma_ {X}} \mu_ {X} \right] + \left[ \rho \frac {\sigma_ {Y}}{\sigma_ {X}} \mu_ {X} \right] X \tag {9.F.5}
$$

or

$$
\left[ \frac {\hat {Y} - \mu_ {Y}}{\sigma_ {Y}} \right] = \rho \left[ \frac {X - \mu_ {X}}{\sigma_ {X}} \right] \tag {9.F.6}
$$

In terms of standardized variables $\hat { Y } ^ { * }$ and $X ^ { ^ { * } }$ , we have simply $\hat { \boldsymbol { Y } } ^ { * } = \boldsymbol { \rho } \boldsymbol { X } ^ { * }$

Also, using Equations (9.F.3) and (9.F.4), we find

$$
\min  g (a, b) = \sigma_ {Y} ^ {2} \left(1 - \rho^ {2}\right) \tag {9.F.7}
$$

which provides a proof that $- 1 \leq \rho \leq + 1$ since $g ( a , b ) \geq 0$ .

If we compare Equation (9.F.7) with Equation (9.F.2), we see that the minimum mean square error obtained when we use a linear function of $X$ to predict Y is reduced by a factor of $1 - \rho ^ { 2 }$ compared with that obtained by ignoring $X$ and simply using the constant $\mu _ { Y }$ for our prediction.

Let us now consider the more general problem of predicting Y with an arbitrary function of X. Once more our criterion will be to minimize the mean square error of prediction. We need to choose the function $h ( X )$ , say, that minimizes

$$
E [ Y - h (X) ] ^ {2} \tag {9.F.8}
$$

Using Equation (9.E.5), we can write this as

$$
E [ Y - h (X) ] ^ {2} = E \left(E \left\{\left[ Y - h (X) \right] ^ {2} \mid X \right\}\right) \tag {9.F.9}
$$

Using Equation (9.E.4), the inner expectation can be written as

$$
E \left\{\left[ Y - h (X) \right] ^ {2} \mid X = x \right\} = E \left\{\left[ Y - h (x) \right] ^ {2} \mid X = x \right\} \tag {9.F.10}
$$

For each value of x, $h ( x )$ is a constant, and we can apply the result of Equation (9.F.1) to the conditional distribution of $Y$ given $X = x$ . Thus, for each $x$ , the best choice of $h ( x )$ is

$$
h (x) = E (Y | X = x) \tag {9.F.11}
$$

Since this choice of $h ( x )$ minimizes the inner expectation in Equation (9.F.9), it must also provide the overall minimum of Equation (9.F.8). Thus

$$
h (X) = E (Y | X) \tag {9.F.12}
$$

is the best predictor of Y of all functions of $X$ .

If $X$ and $Y$ have a bivariate normal distribution, it is well-known that

$$
E (Y | X) = \mu_ {Y} + \rho \frac {\sigma_ {Y}}{\sigma_ {X}} (X - \mu_ {X})
$$

so that the solutions given in Equations (9.F.12) and (9.F.5) coincide. In this case, the linear predictor is the best of all functions.

More generally, if Y is to be predicted by a function of $X _ { 1 }$ , $X _ { 2 } , . . . , X _ { n }$ , then it can be easily argued that the minimum square error predictor is given by

$$
E (Y | X _ {1}, X _ {2}, \dots , X _ {n}) \tag {9.F.13}
$$

# Appendix G: The Truncated Linear Process

Suppose $\{ Y _ { t } \}$ satisfies the general $\mathrm { A R I M A } ( p , d , q )$ model with AR characteristic polynomial $\phi ( x )$ , MA characteristic polynomial $\theta ( x )$ , and constant term $\theta _ { 0 }$ . Then the truncated linear process representation for $\{ Y _ { t } \}$ is given by

$$
Y _ {t + l} = C _ {t} (\ell) + I _ {t} (\ell) \text {f o r} \ell \geq 1 \tag {9.G.1}
$$

where

$$
I _ {t} (\ell) = \sum_ {j = 0} ^ {\ell - 1} \psi_ {j} e _ {t + \ell - j} \text {f o r} \ell \geq 1 \tag {9.G.2}
$$

$$
C _ {t} (\ell) = \sum_ {i = 0} ^ {d} A _ {i} \ell^ {i} + \sum_ {i = 1} ^ {r} \sum_ {j = 0} ^ {p _ {i} - 1} B _ {i j} \ell^ {j} \left(G _ {i}\right) ^ {\ell} \tag {9.G.3}
$$

and $A _ { i }$ , $B _ { i j }$ , $i = 1$ , 2,…, $r , j = 1$ , 2,…, $p _ { i }$ , are constant in l and depend only on $Y _ { t }$ $Y _ { t - 1 } , \dots$ .† As always, the $\boldsymbol { \Psi }$ -weights are defined by the identity

$$
\phi (x) (1 - x) ^ {d} \left(1 + \psi_ {1} x + \psi_ {2} x ^ {2} + \dots\right) = \theta (x) \tag {9.G.4}
$$

or

$$
\varphi (x) \left(1 + \psi_ {1} x + \psi_ {2} x ^ {2} + \dots\right) = \theta (x) \tag {9.G.5}
$$

We shall show that the representation given by Equation (9.G.1) is valid by arguing that, for fixed t, $C _ { t } ( \ell )$ is essentially the complementary function of the defining difference equation, that is,

$$
C _ {t} (\ell) - \varphi_ {1} C _ {t} (\ell - 1) - \varphi_ {2} C _ {t} (\ell - 2) - \dots - \varphi_ {p + d} C _ {t} (\ell - p - d) = \theta_ {0} \text {f o r} \ell \geq 0 \tag {9.G.6}
$$

and that $I _ { t } ( \ell )$ is a particular solution (without $\boldsymbol { \theta } _ { 0 }$ ):

$$
\begin{array}{l} I _ {t} (\ell) - \varphi_ {1} I _ {t} (\ell - 1) - \varphi_ {2} I _ {t} (\ell - 2) - \dots - \varphi_ {p + d} I _ {t} (\ell - p - d) \\ = e _ {t + \ell} - \theta_ {1} e _ {t + l - 1} - \theta_ {2} e _ {t + l - 2} - \dots - \theta_ {q} e _ {t + l - q} \text {f o r} \ell > q \tag {9.G.7} \\ \end{array}
$$

Since $C _ { t } ( \ell )$ contains $p + d$ arbitrary constants (the A’s and the B’s), summing $C _ { t } ( \ell )$ and $I _ { t } ( \ell )$ yields the general solution of the ARIMA equation. Specific values for the $A ^ { : }$ ’s and $B$ ’s will be determined by initial conditions on the $\{ Y _ { t } \}$ process.

We note that $A _ { d }$ is not arbitrary. We have

$$
A _ {d} = \frac {\theta_ {0}}{(1 - \phi_ {1} - \phi_ {2} - \cdots - \phi_ {p}) d !} \tag {9.G.8}
$$

The proof that $C _ { t } ( \ell )$ as given by Equation (9.G.2) is the complementary function and satisfies Equation (9.G.6) is a standard result from the theory of difference equations

(see, for example, Goldberg, 1958). We shall show that the particular solution $I _ { t } ( \ell )$ defined by Equation (9.G.2) does satisfy Equation (9.G.7).

For convenience of notation, we let $\boldsymbol { \Phi } _ { j } = 0$ for $j > p + d$ . Consider the left-hand side of Equation (9.G.7). It can be written as:

$$
\left. \begin{array}{c} \left(\psi_ {0} e _ {t + \ell} + \psi_ {1} e _ {t + \ell - 1} + \dots + \psi_ {\ell - 1} e _ {t + 1}\right) - \varphi_ {1} \left(\psi_ {0} e _ {t + \ell - 1} + \psi_ {1} e _ {t + \ell - 2} + \dots \right. \\ \left. + \psi_ {\ell - 2} e _ {t + 1}\right) - \dots - \varphi_ {p + d} \left(\psi_ {0} e _ {t + \ell - p - d} \right. \\ \left. + \psi_ {1} e _ {t + \ell - p - d - 1} + \dots + \psi_ {\ell - p - d - 1} e _ {t + 1}\right) \end{array} \right\} \tag {9.G.9}
$$

Now grouping together common $e _ { t }$ terms and picking off their coefficients, we obtain

Coefficient of $e _ { t + \ell - 1 } : \Psi _ { 0 }$

Coefficient of $e _ { t + \ell - 2 } : \Psi _ { 1 } - \Phi _ { 1 } \Psi _ { 0 }$

Coefficient of $\begin{array} { r } { e _ { t + \ell - 3 } : \Psi _ { 2 } - \Phi _ { 1 } \Psi _ { 1 } - \Phi _ { 2 } \Psi _ { 0 } } \end{array}$ ..

Coefficient of et + 1 : ψl 1– ϕ1ψl 2–– ϕ2ψl 3–– – … ϕp d + ψl – p – 1– d– $\begin{array} { r l } { e _ { t + 1 } } & { { } : \Psi _ { \ell - 1 } - \Phi _ { 1 } \Psi _ { \ell - 2 } - \Phi _ { 2 } \Psi _ { \ell - 3 } - \dots - \Phi _ { p + d } \Psi _ { \ell - p - d - 1 } } \end{array}$

If $\ell > q$ , we can match these coefficients to the corresponding coefficients on the right-hand side of Equation (9.G.7) to obtain the relationships

$$
\left. \begin{array}{c} \psi_ {0} = 1 \\ \psi_ {1} - \varphi_ {1} \psi_ {0} = - \theta_ {1} \\ \psi_ {2} - \varphi_ {1} \psi_ {1} - \varphi_ {2} \psi_ {0} = - \theta_ {2} \\ \vdots \\ \psi_ {q} - \varphi_ {1} \psi_ {q - 1} - \varphi_ {2} \psi_ {q - 2} - \dots - \varphi_ {q} \psi_ {0} = - \theta_ {q} \\ \psi_ {\ell - 1} - \varphi_ {1} \psi_ {\ell - 2} - \varphi_ {2} \psi_ {\ell - 3} - \dots - \varphi_ {p + d} \psi_ {\ell - p - d - 1} = 0 \text {f o r} \ell > q \end{array} \right\} \tag {9.G.10}
$$

However, by comparing these relationships with Equation (9.G.5), we see that Equations (9.G.10) are precisely the equations defining the $\boldsymbol { \Psi }$ -weights and thus Equation (9.G.7) is established as required.

# Appendix H: State Space Models

Control theory engineers have developed and successfully used so-called state space models and Kalman filtering since Kalman published his seminal work in 1960. Recent references include Durbin and Koopman (2001) and Harvey et al. (2004).

Consider a general stationary and invertible $\mathbf { A R M A } ( p , q )$ $( p , q )$ process $\{ Z _ { t } \}$ . Put $m =$ $m a x ( p , q + 1 )$ and define the state of the process at time $t$ as the column vector $\mathbf { Z } ( t )$ of length m whose jth element is the forecast $\hat { Z } ( j )$ for $j = 0$ , 1, 2,…, m − 1, based on $Z _ { t }$ , $Z _ { t - 1 } , \dots .$ Note that the lead element of $\mathbf { Z } ( t )$ is just $\widehat Z ( 0 ) = Z _ { t }$ .

Recall the updating Equation (9.6.1) on page 207, which in the present context can

be written

$$
\hat {Z} _ {t + 1} (\ell) = \hat {Z} _ {t} (\ell + 1) + \psi_ {\ell} e _ {t + 1} \tag {9.H.1}
$$

We shall use this expression directly for $\ell = 0$ , 1, 2,…, m − 2. For $\ell = m - 1$ , we have

$$
\begin{array}{l} \hat {Z} _ {t + 1} (m - 1) = \hat {Z} _ {t} (m) + \psi_ {m - 1} e _ {t + 1} \\ = \phi_ {1} \hat {\Delta} _ {t} (m - 1) + \phi_ {2} \hat {\Delta} _ {t} (m - 2) + \dots + \phi_ {p} \hat {\Delta} _ {t} (m - p) + \psi_ {m - 1} e _ {t + 1} \tag {9.H.2} \\ \end{array}
$$

where the last expression comes from Equation (9.3.34) on page 200, with $\mu = 0$ .

The matrix formulation of Equations (9.H.1) and (9.H.2) relating $\mathbf { Z } ( t + 1 )$ to $\mathbf { Z } ( t )$ and $e _ { t + 1 }$ , called the equations of state (or Akaike’s Markovian representation), is given as

$$
\mathbf {Z} (t + 1) = \mathbf {F} \mathbf {Z} (t) + \mathbf {G} e _ {t + 1} \tag {9.H.3}
$$

where

$$
\boldsymbol {F} = \left[ \begin{array}{c c c c c c} 0 & 1 & 0 & 0 & \dots & 0 \\ 0 & 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 0 & 1 & \dots & 0 \\ & & & & & \vdots \\ 0 & 0 & 0 & 0 & \dots & 1 \\ \phi_ {m} & \phi_ {m - 1} & . & . & . & \phi_ {1} \end{array} \right] \tag {9.H.4}
$$

and

$$
\boldsymbol {G} = \left[ \begin{array}{c} 1 \\ \Psi_ {1} \\ \Psi_ {2} \\ \vdots \\ \Psi_ {m - 1} \end{array} \right] \tag {9.H.5}
$$

with $\boldsymbol { \Phi } _ { j } = 0$ for $j > p$ . Note that the simplicity of Equation (9.H.3) is obtained at the expense of having to deal with vector-valued processes. Because the state space formulation also usually allows for measurement error, we do not observe $Z _ { t }$ directly but only observe $Y _ { t }$ through the observational equation

$$
Y _ {t} = \mathbf {H Z} (t) + \varepsilon_ {t} \tag {9.H.6}
$$

where $\pmb { H } = [ 1 , 0 , 0 , . . . , 0 ]$ and $\{ \varepsilon _ { t } \}$ is another zero-mean white noise process independent of $\{ \boldsymbol { e } _ { t } \}$ . The special case of no measurement error is obtained by setting $ { \varepsilon } _ { t } = 0$ in Equation (9.H.6). Equivalently, this case is obtained by taking $\sigma _ { \mathrm { { \varepsilon } } } ^ { 2 } = 0$ in subsequent equations. More general state space models allow $\pmb { F }$ , $\textbf { G }$ , and $\pmb { H }$ to be more general, possibly also depending on time.

# Evaluation of the Likelihood Function and Kalman Filtering

First a definition: The covariance matrix for a vector of random variables $X$ of dimension $n { \times } 1$ is defined to be the $n \times n$ matrix whose $i j$ th element is the covariance between the ith and $j$ th components of $X$ .

If $Y = A X + B$ , then it is easily shown that the covariance matrix for $Y$ is $A V A ^ { T }$ where $V$ is the covariance matrix for $X$ and the superscript $T$ denotes matrix transpose.

Getting back to the Kalman filter, we let $\mathbf { Z } ( t + 1 | t )$ denote the $m \times 1$ vector whose $j$ th component is $E [ \hat { Z } _ { t + 1 } ( j ) | Y _ { t } , Y _ { t - 1 } , . . . , Y _ { 1 } ]$ for $j = 0$ , 1, 2,…, m − 1. Similarly, let $\mathbf { } Z ( t | t )$ be the vector whose $j$ th component is $E [ \hat { Z } _ { t } ( j ) | Y _ { t } , Y _ { t - 1 } , . . . , Y _ { 1 } ]$ for $j = 0 , 1$ , $2 , . . . , m - 1$ .

Then, since $e _ { t + 1 }$ is independent of $Z _ { t } , Z _ { t - 1 } , \dots$ , and hence also of $Y _ { t } , Y _ { t - 1 } , \ldots$ , we see from Equation (9.H.3) that

$$
\mathbf {Z} (t + 1 | t) = \mathbf {F Z} (t | t) \tag {9.H.7}
$$

Also letting $P ( t + 1 | t )$ be the covariance matrix for the “forecast error” ${ \pmb Z } ( t + 1 ) -$ $\mathbf { Z } ( t + 1 | t )$ and $P ( t | t )$ be the covariance matrix for the “forecast error” ${ \pmb Z } ( t ) - { \pmb Z } ( t | t )$ , we have from Equation (9.H.3) that

$$
\boldsymbol {P} (t + 1 | t) = \boldsymbol {F} [ \boldsymbol {P} (t | t) ] \boldsymbol {F} ^ {T} + \sigma_ {e} ^ {2} \boldsymbol {G} \boldsymbol {G} ^ {T} \tag {9.H.8}
$$

From the observational equation (Equation (9.H.6)) and then replacing $t + 1$ by $t$

$$
Y (t + 1 | t) = H Z (t + 1 | t) \tag {9.H.9}
$$

where $Y ( t + 1 | t ) = E ( Y _ { t + 1 } | Y _ { t } , Y _ { t - 1 } , . . . , Y _ { 1 } )$ .

It can now be shown that the following relationships hold (see, for example, Harvey, 1981c):

$$
\boldsymbol {Z} (t + 1 | t + 1) = \boldsymbol {Z} (t + 1 | t) + \boldsymbol {K} (t + 1) \left[ Y _ {t + 1} - Y (t + 1 | t) \right] \tag {9.H.10}
$$

where

$$
\boldsymbol {K} (t + 1) = \boldsymbol {P} (t + 1 | t) \boldsymbol {H} ^ {T} \left[ \boldsymbol {H} \boldsymbol {P} (t + 1 | t) \boldsymbol {H} ^ {T} + \sigma_ {\varepsilon} ^ {2} \right] ^ {- 1} \tag {9.H.11}
$$

and

$$
\boldsymbol {P} (t + 1 | (t + 1)) = \boldsymbol {P} (t + 1 | t) - \boldsymbol {K} (t + 1) \boldsymbol {H} \boldsymbol {P} (t + 1 | t) \tag {9.H.12}
$$

Collectively, Equations (9.H.10), (9.H.11), and (9.H.12) are referred to as the Kalman filter equations. The quantity

$$
e r r _ {t + 1} = Y _ {t + 1} - Y (t + 1 | t) \tag {9.H.13}
$$

in Equation (9.H.10) is the prediction error and is independent of (or at least uncorrelated with) the past observations Yt, Yt − 1,… . Since we are allowing for measurement error, $e r r _ { t + 1 }$ is not, in general, the same as $e _ { t + 1 }$ .

From Equations (9.H.13) and (9.H.6), we have

$$
v _ {t + 1} = \operatorname {V a r} \left(\operatorname {e r r} _ {t + 1}\right) = \boldsymbol {H} \boldsymbol {P} (t + 1 | t) \boldsymbol {H} ^ {T} + \sigma_ {\varepsilon} ^ {2} \tag {9.H.14}
$$

Now consider the likelihood function for the observed series $Y _ { 1 }$ $Y _ { 1 } , Y _ { 2 } , . . . , Y _ { n }$ $Y _ { n }$ . From the definition of the conditional probability density function, we can write

$$
f (y _ {1}, y _ {2}, \dots , y _ {n}) = f (y _ {n} | y _ {1}, y _ {2}, \dots , y _ {n - 1}) f (y _ {1}, y _ {2}, \dots , y _ {n - 1})
$$

or, by taking logs,

$$
\log f \left(y _ {1}, y _ {2}, \dots , y _ {n}\right) = \log f \left(y _ {1}, y _ {2}, \dots , y _ {n - 1}\right) + \log f \left(y _ {n} \mid y _ {1}, y _ {2}, \dots , y _ {n - 1}\right) \tag {9.H.15}
$$

Assume now that we are dealing with normal distributions, that is, that $\{ \boldsymbol { e } _ { t } \}$ a n d $\{ \varepsilon _ { t } \}$ are normal white noise processes. Then it is known that the distribution of $Y _ { n }$ conditional on $Y _ { 1 } = y _ { 1 }$ , $Y _ { 2 } = y _ { 2 } , \dots$ , $Y _ { n - 1 } = y _ { n - 1 }$ , is also normal with mean $y ( n | n - 1 )$ and variance $\nu _ { n }$ . In the remainder of this section and the next, we write $y ( n | n - 1 )$ for the observed value of $Y ( n | n - 1 )$ .The second term on the right-hand side of Equation (9.H.15) can then be written

$$
\log f (y _ {n} | y _ {1}, y _ {2}, \dots , y _ {n - 1}) = - \frac {1}{2} \mathrm {l o g} 2 \pi - \frac {1}{2} \mathrm {l o g} v _ {n} - \frac {1}{2} \frac {[ y _ {n} - y (n | n - 1) ] ^ {2}}{v _ {n}}
$$

Furthermore, the first term on the right-hand side of Equation (9.H.15) can be decomposed similarly again and again until we have

$$
\log f \left(y _ {1}, y _ {2}, \dots , y _ {n}\right) = \sum_ {t = 2} ^ {n} \log f \left(y _ {t} \mid y _ {1}, y _ {2}, \dots , y _ {t - 1}\right) + \log f \left(y _ {1}\right) \tag {9.H.16}
$$

which then becomes the prediction error decomposition of the likelihood, namely

$$
\log f \left(y _ {1}, y _ {2}, \dots , y _ {n}\right) = - \frac {n}{2} \log 2 \pi - \frac {1}{2} \sum_ {t = 1} ^ {n} v _ {t} - \frac {1}{2} \sum_ {t = 1} ^ {n} \frac {\left[ y _ {t} - y (t | t - 1) \right] ^ {2}}{v _ {t}} \tag {9.H.17}
$$

with $y ( 1 | 0 ) = 0$ and $\nu _ { 1 } = V a r ( Y _ { 1 } )$ .

The overall strategy for computing the likelihood for a given set of parameter values is to use the Kalman filter equations to generate recursively the prediction errors and their variances and then use the prediction error decomposition of the likelihood function. Only one point remains: We need initial values $\mathbf { Z } ( 0 | 0 )$ and $P ( 0 | 0 )$ to get the recursions started.

# The Initial State Covariance Matrix

The initial state vector $\mathbf { \cal { Z } } ( 0 | 0 )$ will be a vector of zeros for a zero-mean process, and $P ( 0 | 0 )$ is the covariance matrix for ${ \bf Z } ( 0 ) - { \bf Z } ( 0 | 0 ) = { \bf Z } ( 0 )$ . Now, because $\mathbf { Z } ( 0 )$ is the column vector with elements $[ Z _ { 0 } , \hat { Z } _ { 0 } ( \hat { 1 } ) , . . . , \hat { Z } _ { 0 } ( \dot { m } - 1 ) ]$ , it is necessary for us to evaluate

$$
C o v [ \hat {Z} _ {0} (i), \hat {Z} _ {0} (j) ] \qquad \mathrm {f o r} i, j = 0, 1, \dots , m - 1
$$

From the truncated linear process form, Equation (9.3.35) on page 200 with $C _ { t } ( \ell )$ $= { \hat { Z } } _ { t } ( \ell )$ , we may write, for $j > 0$

$$
Z _ {j} = \hat {Z} _ {0} (j) + \sum_ {k = - j} ^ {- 1} \psi_ {j + k} e _ {- k} \tag {9.H.18}
$$

Multiplying Equation (9.H.18) by $Z _ { 0 }$ and taking expected values yields

$$
\gamma_ {j} = E \left(Z _ {0} Z _ {j}\right) = E \left[ \hat {Z} _ {0} (0) \left(\hat {Z} _ {0} (j)\right) \right] \quad \text {f o r} j \geq 0 \tag {9.H.19}
$$

Now multiply Equation (9.H.18) by itself with $j$ replaced by $i$ and take expected values. Recalling that the $e$ ’s are independent of past Z’s and assuming $0 < i \leq j$ , we obtain

$$
\gamma_ {j - i} = C o v [ \hat {Z} _ {0} (i), \hat {Z} _ {0} (j) ] + \sigma_ {e} ^ {2} \sum_ {k = 0} ^ {i - 1} \psi_ {k} \psi_ {k + j - i} \tag {9.H.20}
$$

Combining Equations (9.H.19) and (9.H.20), we have as the required elements of $P ( 0 | 0 )$

$$
C o v [ \hat {Z} _ {0} (i), \hat {Z} _ {0} (j) ] = \left\{ \begin{array}{l l} \gamma_ {i} & 0 = i \leq j \leq m - 1 \\ \gamma_ {j - i} - \sigma_ {e} ^ {2} \sum_ {k = 0} ^ {i - 1} \psi_ {k} \psi_ {k + j - i} & 1 \leq i \leq j \leq m - 1 \end{array} \right. \tag {9.H.21}
$$

where the $\boldsymbol { \Psi }$ -weights are obtained from the recursion of Equation (4.4.7) on page 79, and $\gamma _ { k }$ , the autocovariance function for the $\{ Z _ { t } \}$ process, is obtained as in Appendix C on page 85.

The variance $\sigma _ { e } ^ { 2 }$ can be removed from the problem by dividing $\sigma _ { \mathrm { { \varepsilon } } } ^ { 2 }$ by $\sigma _ { e } ^ { 2 }$ . The prediction error variance $\nu _ { t }$ is then replaced by $\sigma _ { e } ^ { 2 } \nu _ { t }$ in the log-likelihood of Equation (9.H.17), and we set $\sigma _ { e } ^ { 2 } = 1$ in Equation (9.H.8). Dropping unneeded constants, we get the new log-likelihood

$$
\ell = \sum_ {t = 1} ^ {n} \left\{\log \left(\sigma_ {e} ^ {2} v _ {t}\right) + \frac {\left[ y _ {t} - y (t \mid t - 1) \right] ^ {2}}{v _ {t}} \right\} \tag {9.H.22}
$$

which can be minimized analytically with respect to $\sigma _ { e } ^ { 2 }$ . We obtain

$$
\sigma_ {e} ^ {2} = \sum_ {t = 1} ^ {n} \left\{\frac {\left[ y _ {t} - y (t \mid t - 1) \right] ^ {2}}{\sigma_ {e} ^ {2} v _ {t}} \right\} \tag {9.H.23}
$$

Substituting this back into Equation (9.H.22), we now find that

$$
\ell = \sum_ {t = 1} ^ {n} \log v _ {t} + n \log \sum_ {t = 1} ^ {n} \frac {\left[ y _ {t} - y (t | t - 1) \right] ^ {2}}{v _ {t}} \tag {9.H.24}
$$

which must be minimized numerically with respect to $\Phi _ { 1 }$ , φ2,…, φp, θ1, θ2,…, θq, and $\sigma _ { e } ^ { 2 }$ . Having done so, we return to Equation (9.H.23) to estimate $\sigma _ { e } ^ { 2 }$ . The function defined by Equation (9.H.24) is sometimes called the concentrated log-likelihood function.

# CHAPTER 10

# SEASONAL MODELS

In Chapter 3, we saw how seasonal deterministic trends might be modeled. However, in many areas in which time series are used, particularly business and economics, the assumption of any deterministic trend is quite suspect even though cyclical tendencies are very common in such series.

Here is an example: Levels of carbon dioxide $( \mathrm { C O } _ { 2 } )$ are monitored at several sites around the world to investigate atmospheric changes. One of the sites is at Alert, Northwest Territories, Canada, near the Arctic Circle.

![](images/6b70f5e72adda4402e8a8e710d1417b3ad8367c5a2a3f5686652a54c087f4946.jpg)

Exhibit 10.1 displays the monthly $\mathrm { C O } _ { 2 }$ levels from January 1994 through December 2004. There is a strong upward trend but also a seasonality that can be seen better in the more detailed Exhibit 10.2, where only the last few years are graphed using monthly plotting symbols.

![](images/764f868f9b1dd35a6b81d903cfdff7a6c270c415e5ce364f92b74b75e6efc323.jpg)  
Exhibit 10.1 Monthly Carbon Dioxide Levels at Alert, NWT, Canada

```txt
> data(co2)  
> win.graph(width=4.875,height=3,pointsize=8)  
> plot(co2,ylab='CO2') 
```

As we see in the displays, carbon dioxide levels are higher during the winter months and much lower in the summer. Deterministic seasonal models such as seasonal means plus linear time trend or sums of cosine curves at various frequencies plus linear time trend as we investigated in Chapter 3 could certainly be considered here. But we discover that such models do not explain the behavior of this time series. For this series and many others, it can be shown that the residuals from a seasonal means plus linear time trend model are highly autocorrelated at many lags.† In contrast, we will see that the stochastic seasonal models developed in this chapter do work well for this series.

![](images/a39efd1da788b2b6e61cd47bc136fb79340e6bf49a27a038a842d9b6741432fe.jpg)  
Exhibit 10.2 Carbon Dioxide Levels with Monthly Symbols

```txt
> plotwindow(co2,start=c(2000,1)),ylab='CO2')  
> Month=c('J','F','M','A','M','J','J','A','S','O','N','D')  
> points(window(co2,start=c(2000,1)),pch=Month) 
```

# 10.1 Seasonal ARIMA Models

We begin by studying stationary models and then consider nonstationary generalizations in Section 10.3. We let s denote the known seasonal period; for monthly series $s = 1 2$ and for quarterly series $s = 4$ .

Consider the time series generated according to

$$
Y _ {t} = e _ {t} - \Theta e _ {t - 1 2}
$$

Notice that

$$
\begin{array}{l} C o v \left(Y _ {t}, Y _ {t - 1}\right) = C o v \left(e _ {t} - \Theta e _ {t - 1 2}, e _ {t - 1} - \Theta e _ {t - 1 3}\right) \\ = 0 \\ \end{array}
$$

but that

$$
\begin{array}{l} C o v (Y _ {t}, Y _ {t - 1 2}) = C o v (e _ {t} - \Theta e _ {t - 1 2}, e _ {t - 1 2} - \Theta e _ {t - 2 4}) \\ = - \Theta \sigma_ {e} ^ {2} \\ \end{array}
$$

It is easy to see that such a series is stationary and has nonzero autocorrelations only at lag 12.

Generalizing these ideas, we define a seasonal MA(Q) model of order $\varrho$ with seasonal period s by

$$
Y _ {t} = e _ {t} - \Theta_ {1} e _ {t - s} - \Theta_ {2} e _ {t - 2 s} - \dots - \Theta_ {Q} e _ {t - Q s} \tag {10.1.1}
$$

with seasonal MA characteristic polynomial

$$
\Theta (x) = 1 - \Theta_ {1} x ^ {s} - \Theta_ {2} x ^ {2 s} - \dots - \Theta_ {Q} x ^ {Q s} \tag {10.1.2}
$$

It is evident that such a series is always stationary and that the autocorrelation function will be nonzero only at the seasonal lags of s, 2s, 3s,…, Qs. In particular,

$$
\rho_ {k s} = \frac {- \Theta_ {k} + \Theta_ {1} \Theta_ {k + 1} + \Theta_ {2} \Theta_ {k + 2} + \cdots + \Theta_ {Q - k} \Theta_ {Q}}{1 + \Theta_ {1} ^ {2} + \Theta_ {2} ^ {2} + \cdots + \Theta_ {Q} ^ {2}} \text {f o r} k = 1, 2, \dots , Q \tag {10.1.3}
$$

(Compare this with Equation (4.2.5) on page 65 for the nonseasonal MA process.) For the model to be invertible, the roots of $\Theta ( x ) = 0$ must all exceed 1 in absolute value.

It is useful to note that the seasonal $\mathbf { M A } ( Q )$ model can also be viewed as a special case of a nonseasonal MA model of order $q = Q s$ but with all θ-values zero except at the seasonal lags s, 2s, 3s,…, Qs.

Seasonal autoregressive models can also be defined. Consider

$$
Y _ {t} = \Phi Y _ {t - 1 2} + e _ {t} \tag {10.1.4}
$$

where $| \Phi | < 1$ and $e _ { t }$ is independent of $Y _ { t - 1 }$ , Yt − 2,… . It can be shown that $| \Phi | < 1$ ensures stationarity. Thus it is easy to argue that $E ( Y _ { t } ) = 0$ ; multiplying Equation (10.1.4) by $Y _ { t - k }$ , taking expectations, and dividing by $\gamma _ { 0 }$ yields

$$
\rho_ {k} = \Phi \rho_ {k - 1 2} \text {f o r} k \geq 1 \tag {10.1.5}
$$

Clearly

$$
\rho_ {1 2} = \Phi \rho_ {0} = \Phi \text {a n d} \rho_ {2 4} = \Phi \rho_ {1 2} = \Phi^ {2}
$$

More generally,

$$
\rho_ {1 2 k} = \Phi^ {k} \text {f o r} k = 1, 2, \dots \tag {10.1.6}
$$

Furthermore, setting $k = 1$ and then $k = 1 1$ in Equation (10.1.5) and using $\rho _ { k } = \rho _ { - k }$ gives us

$$
\rho_ {1} = \Phi \rho_ {1 1} \text {a n d} \rho_ {1 1} = \Phi \rho_ {1}
$$

which implies that $\rho _ { 1 } = \rho _ { 1 1 } = 0$ . Similarly, one can show that ${ \rho } _ { k } = 0$ except at the seasonal lags 12, 24, 36,… . At those lags, the autocorrelation function decays exponentially like an AR(1) model.

With this example in mind, we define a seasonal AR(P) model of order $P$ and seasonal period s by

$$
Y _ {t} = \Phi_ {1} Y _ {t - s} + \Phi_ {2} Y _ {t - 2 s} + \dots + \Phi_ {P} Y _ {t - P s} + e _ {t} \tag {10.1.7}
$$

with seasonal characteristic polynomial

$$
\Phi (x) = 1 - \Phi_ {1} x ^ {s} - \Phi_ {2} x ^ {2 s} - \dots - \Phi_ {P} x ^ {P s} \tag {10.1.8}
$$

As always, we require $e _ { t }$ to be independent of $Y _ { t - 1 }$ , Yt − 2,…, and, for stationarity, that the roots of $\Phi ( x ) = 0$ be greater than 1 in absolute value. Again, Equation (10.1.7) can be seen as a special $\operatorname { A R } ( p )$ model of order $p = P s$ with nonzero $\phi$ -coefficients only at the seasonal lags s, 2s, 3s,…, Ps.

It can be shown that the autocorrelation function is nonzero only at lags s, 2s, 3s, …, where it behaves like a combination of decaying exponentials and damped sine functions. In particular, Equations (10.1.4), (10.1.5), and (10.1.6) easily generalize to the general seasonal AR(1) model to give

$$
\rho_ {k s} = \Phi^ {k} \text {f o r} k = 1, 2, \dots \tag {10.1.9}
$$

with zero correlation at other lags.

# 10.2 Multiplicative Seasonal ARMA Models

Rarely shall we need models that incorporate autocorrelation only at the seasonal lags. By combining the ideas of seasonal and nonseasonal ARMA models, we can develop parsimonious models that contain autocorrelation for the seasonal lags but also for low lags of neighboring series values.

Consider a model whose MA characteristic polynomial is given by

$$
(1 - \theta x) (1 - \Theta x ^ {1 2})
$$

Multiplying out, we have $1 - \Theta x - \Theta x ^ { 1 2 } + \Theta \Theta x ^ { 1 3 }$ . Thus the corresponding time series satisfies

$$
Y _ {t} = e _ {t} - \Theta e _ {t - 1} - \Theta e _ {t - 1 2} + \Theta \Theta e _ {t - 1 3} \tag {10.2.1}
$$

For this model, we can check that the autocorrelation function is nonzero only at lags 1, 11, 12, and 13. We find

$$
\gamma_ {0} = (1 + \theta^ {2}) (1 + \Theta^ {2}) \sigma_ {e} ^ {2} \tag {10.2.2}
$$

$$
\rho_ {1} = - \frac {\theta}{1 + \theta^ {2}} \tag {10.2.3}
$$

$$
\rho_ {1 1} = \rho_ {1 3} = \frac {\theta \Theta}{\left(1 + \theta^ {2}\right) \left(1 + \Theta^ {2}\right)} \tag {10.2.4}
$$

and

$$
\rho_ {1 2} = - \frac {\Theta}{1 + \Theta^ {2}} \tag {10.2.5}
$$

Exhibit 10.3 displays the autocorrelation functions for the model of Equation (10.2.1) with $\theta = \pm 0 . 5$ and $\Theta = - 0 . 8$ as given by Equations (10.2.2)–(10.2.5).

# Exhibit 10.3 Autocorrelations from Equations (10.2.2)-(10.2.5)

![](images/07f087d7ed5fd3f5b66e18355dc377fa2e992a3506175c074b963548ca476910.jpg)

![](images/c8331a5a0c14e135519f9217eb42590cabf26c6ddd0c34cb988423aa5c0f7e09.jpg)

Of course, we could also introduce both short-term and seasonal autocorrelations by defining an MA model of order 12 with only $\theta _ { 1 }$ and $\theta _ { 1 2 }$ nonzero. We shall see in the next section that the “multiplicative” model arises quite naturally for nonstationary models that entail differencing.

In general, then, we define a multiplicative seasonal ARMA $( p , q ) \times ( P , Q ) _ { s }$ model with seasonal period s as a model with AR characteristic polynomial $\Phi ( x ) \Phi ( x )$ and MA characteristic polynomial $\Theta ( x ) \Theta ( x )$ , where

$$
\left. \begin{array}{l} \phi (x) = 1 - \phi_ {1} x - \phi_ {2} x ^ {2} - \dots - \phi_ {p} x ^ {p} \\ \Phi (x) = 1 - \Phi_ {1} x ^ {s} - \Phi_ {2} x ^ {2 s} - \dots - \Phi_ {P} x ^ {P s} \end{array} \right\} \tag {10.2.6}
$$

and

$$
\left. \begin{array}{l} \theta (x) = 1 - \theta_ {1} x - \theta_ {2} x ^ {2} - \dots - \theta_ {q} x ^ {q} \\ \Theta (x) = 1 - \Theta_ {1} x ^ {s} - \Theta_ {2} x ^ {2 s} - \dots - \Theta_ {Q} x ^ {Q s} \end{array} \right\} \tag {10.2.7}
$$

The model may also contain a constant term $\theta _ { 0 }$ . Note once more that we have just a special ARMA model with AR order $p + P s$ and MA order $q + Q s$ , but the coefficients are not completely general, being determined by only $p + P + q + Q$ coefficients. If $s = 1 2$ , $p + P + q + Q$ will be considerably smaller than $p + P s + q + Q s$ and will allow a much more parsimonious model.

As another example, suppose $P = q = 1$ and $p = Q = 0$ with $s = 1 2$ . The model is then

$$
Y _ {t} = \Phi Y _ {t - 1 2} + e _ {t} - \theta e _ {t - 1} \tag {10.2.8}
$$

Using our standard techniques, we find that

$$
\gamma_ {1} = \Phi \gamma_ {1 1} - \theta \sigma_ {e} ^ {2} \tag {10.2.9}
$$

and

$$
\gamma_ {k} = \Phi \gamma_ {k - 1 2} \text {f o r} k \geq 2 \tag {10.2.10}
$$

After considering the equations implied by various choices for $k$ , we arrive at

$$
\left. \begin{array}{c} \gamma_ {0} = \left[ \frac {1 + \theta^ {2}}{1 - \Phi^ {2}} \right] \sigma_ {e} ^ {2} \\ \rho_ {1 2 k} = \Phi^ {k} \text {f o r} k \geq 1 \\ \rho_ {1 2 k - 1} = \rho_ {1 2 k + 1} = \left(- \frac {\theta}{1 + \theta^ {2}} \Phi^ {k}\right) \text {f o r} k = 0, 1, 2, \dots \end{array} \right\} \tag {10.2.11}
$$

with autocorrelations for all other lags equal to zero.

Exhibit 10.4 displays the autocorrelation functions for two of these seasonal ARIMA processes with period 12: one with $\Phi = 0 . 7 5$ and $\theta = 0 . 4$ , the other with $\Phi =$ 0.75 and $\theta = - 0 . 4$ . The shape of these autocorrelations is somewhat typical of the sample autocorrelation functions for numerous seasonal time series. The even simpler autocorrelation function given by Equations (10.2.3), (10.2.4), and (10.2.5) and displayed in Exhibit 10.3 also seems to occur frequently in practice (perhaps after differencing).

![](images/0884c2afaf206a291f04fb984203c206fd9ad98d7fa28f36559a1b4d5db893eb.jpg)  
Exhibit 10.4 Autocorrelation Functions from Equation (10.2.11)

![](images/d59ca17306e718020f3a92b6d0c05c9e74a5fd30dbf0cb66857b80da75829d2d.jpg)

# 10.3 Nonstationary Seasonal ARIMA Models

An important tool in modeling nonstationary seasonal processes is the seasonal difference. The seasonal difference of period $s$ for the series $\{ Y _ { t } \}$ is denoted $\nabla _ { s } Y _ { t }$ and is defined as

$$
\nabla_ {s} Y _ {t} = Y _ {t} - Y _ {t - s} \tag {10.3.1}
$$

For example, for monthly series we consider the changes from January to January, February to February, and so forth for successive years. Note that for a series of length $n$ , the seasonal difference series will be of length $n - s$ ; that is, $s$ data values are lost due to seasonal differencing.

As an example where seasonal differencing is appropriate, consider a process generated according to

$$
Y _ {t} = S _ {t} + e _ {t} \tag {10.3.2}
$$

with

$$
S _ {t} = S _ {t - s} + \varepsilon_ {t} \tag {10.3.3}
$$

where $\{ \boldsymbol { e } _ { t } \}$ and $\{ \varepsilon _ { t } \}$ are independent white noise series. Here $\{ S _ { t } \}$ is a “seasonal random walk,” and if $\sigma _ { \varepsilon } \ll \sigma _ { e }$ , $\{ S _ { t } \}$ would model a slowly changing seasonal component.

Due to the nonstationarity of $\{ S _ { t } \}$ , clearly $\{ Y _ { t } \}$ is nonstationary. However, if we seasonally difference $\{ Y _ { t } \}$ , as given in Equation (10.3.1), we find

$$
\begin{array}{l} \nabla_ {s} Y _ {t} = S _ {t} - S _ {t - s} + e _ {t} - e _ {t - s} \\ = \varepsilon_ {t} + e _ {t} - e _ {t - s} \tag {10.3.4} \\ \end{array}
$$

An easy calculation shows that $\nabla _ { s } Y _ { t }$ is stationary and has the autocorrelation function of an $\mathbf { M A } ( 1 ) _ { s }$ model.

The model described by Equations (10.3.2) and (10.3.3) could also be generalized to account for a nonseasonal, slowly changing stochastic trend. Consider

$$
Y _ {t} = M _ {t} + S _ {t} + e _ {t} \tag {10.3.5}
$$

with

$$
S _ {t} = S _ {t - s} + \varepsilon_ {t} \tag {10.3.6}
$$

and

$$
M _ {t} = M _ {t - 1} + \xi_ {t} \tag {10.3.7}
$$

where $\{ e _ { t } \} , \ \{ \varepsilon _ { t } \}$ , and $\{ \xi _ { t } \}$ are mutually independent white noise series. Here we take both a seasonal difference and an ordinary nonseasonal difference to obtain†

$$
\begin{array}{l} \nabla \nabla_ {s} Y _ {t} = \nabla \left(M _ {t} - M _ {t - s} + \varepsilon_ {t} + e _ {t} - e _ {t - s}\right) \tag {10.3.8} \\ = (\xi_ {t} + \varepsilon_ {t} + e _ {t}) - (\varepsilon_ {t - 1} + e _ {t - 1}) - (\xi_ {t - s} + e _ {t - s}) + e _ {t - s - 1} \\ \end{array}
$$

The process defined here is stationary and has nonzero autocorrelation only at lags 1, s − 1, s, and $s + 1$ , which agrees with the autocorrelation structure of the multiplicative seasonal model ARMA(0,1)×(0,1) with seasonal period $s$ .

These examples lead to the definition of nonstationary seasonal models. A process $\{ Y _ { t } \}$ is said to be a multiplicative seasonal ARIMA model with nonseasonal (regular) orders p, d, and $q$ , seasonal orders P, D, and $Q$ , and seasonal period $s$ if the differenced series

$$
W _ {t} = \nabla^ {d} \nabla_ {s} ^ {D} Y _ {t} \tag {10.3.9}
$$

satisfies an $\mathbf { A R M A } ( p , q ) \times ( P , Q ) _ { s }$ model with seasonal period s.† We say that $\{ Y _ { t } \}$ is an $\mathrm { A R I M A } ( p , d , q ) \times ( P , D , Q ) _ { s }$ model with seasonal period s.

Clearly, such models represent a broad, flexible class from which to select an appropriate model for a particular time series. It has been found empirically that many series can be adequately fit by these models, usually with a small number of parameters, say three or four.

# 10.4 Model Specification, Fitting, and Checking

Model specification, fitting, and diagnostic checking for seasonal models follow the same general techniques developed in Chapters 6, 7, and 8. Here we shall simply highlight the application of these ideas specifically to seasonal models and pay special attention to the seasonal lags.

# Model Specification

As always, a careful inspection of the time series plot is the first step. Exhibit 10.1 on page 227 displays monthly carbon dioxide levels in northern Canada. The upward trend alone would lead us to specify a nonstationary model. Exhibit 10.5 shows the sample autocorrelation function for that series. The seasonal autocorrelation relationships are shown quite prominently in this display. Notice the strong correlation at lags 12, 24, 36, and so on. In addition, there is substantial other correlation that needs to be modeled.

Exhibit 10.5 Sample ACF of $\mathtt { c o } _ { 2 }$ Levels   
![](images/f43a8e0a6e9e2b3c4c7a41cbd2390a33e8b75ae5ee48301a174638ef5a096a48.jpg)  
> acf(as.vector(co2),lag.max=36)

Exhibit 10.6 shows the time series plot of the $\mathrm { C O } _ { 2 }$ levels after we take a first difference.

Exhibit 10.6 Time Series Plot of the First Differences of $\overline { { \pmb { \subset } \pmb { 0 } _ { 2 } } }$ Levels   
![](images/b476c111b6c1f56f20644560d3d26dff474b846862d312239c2167c4a4340725.jpg)  
> plot(diff(co2),ylab='First Difference of CO2',xlab='Time')

The general upward trend has now disappeared but the strong seasonality is still present, as evidenced by the behavior shown in Exhibit 10.7. Perhaps seasonal differencing will bring us to a series that may be modeled parsimoniously.

Exhibit 10.7 Sample ACF of First Differences of $\mathtt { c o } _ { 2 }$ Levels   
![](images/061eb651bceef5d6c21f2891b335f0cde1a8e0b657fccc3cfa6d45eb8abf1935.jpg)  
> acf(as.vector(diff(co2)),lag.max=36)

Exhibit 10.8 displays the time series plot of the $\mathrm { C O } _ { 2 }$ levels after taking both a first difference and a seasonal difference. It appears that most, if not all, of the seasonality is gone now.

Exhibit 10.8 Time Series Plot of First and Seasonal Differences of $\overline { { \mathbf { c o } _ { 2 } } }$   
![](images/1693a2994c7b8c40d3472b49bd756ecbac311e5450e78443e63d78bd52f88002.jpg)  
> plot(diff(diff(co2),lag=12),xlab='Time', ylab='First and Seasonal Difference of CO2')

Exhibit 10.9 confirms that very little autocorrelation remains in the series after these two differences have been taken. This plot also suggests that a simple model which incorporates the lag 1 and lag 12 autocorrelations might be adequate.

We will consider specifying the multiplicative, seasonal ARIMA(0,1,1)×(0,1,1)12 model

$$
\nabla_ {1 2} \nabla Y _ {t} = e _ {t} - \theta e _ {t - 1} - \Theta e _ {t - 1 2} + \theta \Theta e _ {t - 1 3} \tag {10.4.10}
$$

which incorporates many of these requirements. As usual, all models are tentative and subject to revision at the diagnostics stage of model building.

Exhibit 10.9 Sample ACF of First and Seasonal Differences of CO2   
![](images/ed8b2a7f45dfc3f467eb5a2f1ca4bfb768b5397d11fa4c500e7d9bbf6855156d.jpg)  
> acf(as.vector(diff(diff(co2),lag=12)),lag.max=36,ci.type='ma')

# Model Fitting

Having specified a tentative seasonal model for a particular time series, we proceed to estimate the parameters of that model as efficiently as possible. As we have remarked earlier, multiplicative seasonal ARIMA models are just special cases of our general ARIMA models. As such, all of our work on parameter estimation in Chapter 7 carries over to the seasonal case.

Exhibit 10.10 gives the maximum likelihood estimates and their standard errors for the ARIMA(0,1,1)×(0,1,1)12 model for $\mathrm { C O } _ { 2 }$ levels.

Exhibit 10.10 Parameter Estimates for the $\overline { { \pmb { \subset } \pmb { 0 } _ { 2 } } }$ Model   

<table><tr><td>Coefficient</td><td>θ</td><td>Θ</td></tr><tr><td>Estimate</td><td>0.5792</td><td>0.8206</td></tr><tr><td>Standard error</td><td>0.0791</td><td>0.1137</td></tr><tr><td colspan="3">\( \hat{\sigma}_{e}^{2} = 0.5446 \): log-likelihood = -139.54, AIC = 283.08</td></tr></table>

```txt
> m1.co2=arima(co2,order=c(0,1,1),seasonal=list(order=c(0,1,1), period=12))
> m1.co2 
```

The coefficient estimates are all highly significant, and we proceed to check further on this model.

# Diagnostic Checking

To check the estimated the $\mathrm { A R I M A } ( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ model, we first look at the time series plot of the residuals. Exhibit 10.11 gives this plot for standardized residuals. Other than some strange behavior in the middle of the series, this plot does not suggest any major irregularities with the model, although we may need to investigate the model further for outliers, as the standardized residual at September 1998 looks suspicious. We investigate this further in Chapter 11.

![](images/67d7c7ba05aebecc3a9dec18abc05c9d50c763587b25079c695b0e7aebdc0d4e.jpg)  
Exhibit 10.11 Residuals from the ARIMA(0,1,1)×(0,1,1)12 Model

> plotwindow(rstandard(m1.co2)，start=c(1995,2))， ylab='StandardizedResiduals',type='o')   
>abline $(\mathrm{h} = 0)$

To look further, we graph the sample ACF of the residuals in Exhibit 10.12. The only “statistically significant” correlation is at lag 22, and this correlation has a value of only $- 0 . 1 7$ , a very small correlation. Furthermore, we can think of no reasonable interpretation for dependence at lag 22. Finally, we should not be surprised that one autocorrelation out of the 36 displayed is statistically significant. This could easily happen by chance alone. Except for marginal significance at lag 22, the model seems to have captured the essence of the dependence in the series.

Exhibit 10.12 ACF of Residuals from the ARIMA(0,1,1)×(0,1,1)12 Model   
![](images/a93406fdb9ed9a9766e46db52669bd7217c5235fc27d9a83efcaa78351b5a8b4.jpg)  
> acf(as.vector(window(rstandard(m1.co2),start=c(1995,2))), lag.max=36)

The Ljung-Box test for this model gives a chi-squared value of 25.59 with 22 degrees of freedom, leading to a $p$ -value of 0.27—a further indication that the model has captured the dependence in the time series.

Next we investigate the question of normality of the error terms via the residuals. Exhibit 10.13 displays the histogram of the residuals. The shape is somewhat “bell-shaped” but certainly not ideal. Perhaps a quantile-quantile plot will tell us more.

![](images/4ee9ee11b1e0c9678a408af165ddd27faa9bf4374b190d071f8cf0cee1f6ce04.jpg)  
Exhibit 10.13 Residuals from the ARIMA(0,1,1)×(0,1,1)12 Model

```javascript
> win.graph(width=3, height=3,pointsize=8)  
> histwindow(rstandard(m1.co2),start=c(1995,2)), xlab='Standardized Residuals') 
```

Exhibit 10.14 displays the QQ-normal plot for the residuals.

# Exhibit 10.14 Residuals: ARIMA(0,1,1)×(0,1,1)12 Model

![](images/be2c5143287c02dcd710aeb93eb9fb81993f56d2aea0b8c85daa72d5a82117cc.jpg)  
Theoretical Quantiles

```txt
>win.graph(width=2.5,height=2.5,pointsize=8)  
>qqnormwindow(rstandard(m1.co2)，start=c(1995,2))）  
>qqlinewindow(rstandard(m1.co2)，start=c(1995,2)))
```

Here we again see the one outlier in the upper tail, but the Shapiro-Wilk test of normality has a test statistic of $W { = } 0 . 9 8 2$ , leading to a $p$ -value of 0.11, and normality is not rejected at any of the usual significance levels.

As one further check on the model, we consider overfitting with an ARIMA(0,1,2) $\times ( 0 , 1 , 1 ) _ { 1 2 }$ model with the results shown in Exhibit 10.15.

# Exhibit 10.15 ARIMA(0,1,2)×(0,1,1)12 Overfitted Model

Coefficient $\theta_{1}$ $\theta_{2}$ $\Theta$ Estimate 0.5714 0.0165 0.8274 Standard error 0.0897 0.0948 0.1224 $\hat{\sigma}_{e}^{2} = 0.5427:$ log-likelihood $= -139.52$ AIC $= 285.05$

```txt
> m2.co2=arima(co2,order=c(0,1,2),seasonal=list(order=c(0,1,1), period=12))
> m2.co2 
```

When we compare these results with those reported in Exhibit 10.10 on page 237, we see that the estimates of $\theta _ { 1 }$ and Θ have changed very little—especially when the size of the standard errors is taken into consideration. In addition, the estimate of the new parameter, $\theta _ { 2 }$ , is not statistically different from zero. Note also that the estimate $\widehat { \sigma } _ { e } ^ { 2 }$ and the log-likelihood have not changed much while the AIC has actually increased.

The ARIMA(0,1,1)×(0,1,1)12 model was popularized in the first edition of the seminal book of Box and Jenkins (1976) when it was found to characterize the logarithms of

a monthly airline passenger time series. This model has come to be known as the airline model. We ask you to analyze the original airline data in the exercises.

# 10.5 Forecasting Seasonal Models

Computing forecasts with seasonal ARIMA models is, as expected, most easily carried out recursively using the difference equation form for the model, as in Equations (9.3.28), (9.3.29) on page 199 and (9.3.40) on page 201. For example, consider the model ARIMA(0,1,1)×(1,0,1)12.

$$
Y _ {t} - Y _ {t - 1} = \Phi \left(Y _ {t - 1 2} - Y _ {t - 1 3}\right) + e _ {t} - \theta e _ {t - 1} - \Theta e _ {t - 1 2} + \theta \Theta e _ {t - 1 3} \tag {10.5.1}
$$

which we rewrite as

$$
Y _ {t} = Y _ {t - 1} + \Phi Y _ {t - 1 2} - \Phi Y _ {t - 1 3} + e _ {t} - \theta e _ {t - 1} - \Theta e _ {t - 1 2} + \theta \Theta e _ {t - 1 3} \tag {10.5.2}
$$

The one-step-ahead forecast from origin $t$ is then

$$
\hat {Y} _ {t} (1) = Y _ {t} + \Phi Y _ {t - 1 1} - \Phi Y _ {t - 1 2} - \theta e _ {t} - \Theta e _ {t - 1 1} + \theta \Theta e _ {t - 1 2} \tag {10.5.3}
$$

and the next one is

$$
\hat {Y} _ {t} (2) = \hat {Y} _ {t} (1) + \Phi Y _ {t - 1 0} - \Phi Y _ {t - 1 1} - \Theta e _ {t - 1 0} + \theta \Theta e _ {t - 1 1} \tag {10.5.4}
$$

and so forth. The noise terms $e _ { t - 1 3 }$ , et − 12, et − 11,…, $e _ { t }$ (as residuals) will enter into the forecasts for lead times $\ell = 1$ , 2,…, 13, but for $\ell > 1 3$ the autoregressive part of the model takes over and we have

$$
\hat {Y} _ {t} (\ell) = \hat {Y} _ {t} (\ell - 1) + \Phi \hat {Y} _ {t} (\ell - 1 2) - \Phi \hat {Y} _ {t} (\ell - 1 3) \text {f o r} \ell > 1 3 \tag {10.5.5}
$$

To understand the general nature of the forecasts, we consider several special cases.

# Seasonal AR(1)12

The seasonal $\mathrm { A R } ( 1 ) _ { 1 2 }$ model is

$$
Y _ {t} = \Phi Y _ {t - 1 2} + e _ {t} \tag {10.5.6}
$$

Clearly, we have

$$
\hat {Y} _ {t} (\ell) = \Phi \hat {Y} _ {t} (\ell - 1 2) \tag {10.5.7}
$$

However, iterating back on l, we can also write

$$
\hat {Y} _ {t} (\ell) = \Phi^ {k + 1} Y _ {t + r - 1 1} \tag {10.5.8}
$$

where $k$ and $r$ are defined by $\ell = 1 2 k + r + 1$ with $0 \leq r < 1 2$ and $k = 0$ , 1, 2,… . In other words, $k$ is the integer part of $( \ell - 1 ) / 1 2$ and $r / 1 2$ is the fractional part of $( \ell - 1 ) / 1 2$ . If our last observation is in December, then the next January value is forecast as $\Phi$ times the last observed January value, February is forecast as $\Phi$ times the last observed February

value, and so on. Two Januarys ahead is forecast as $\Phi ^ { 2 }$ times the last observed January. Looking just at January values, the forecasts into the future will decay exponentially at a rate determined by the magnitude of $\Phi .$ . All of the forecasts for each month will behave similarly but with different initial forecasts depending on the particular month under consideration.

Using Equation (9.3.38) on page 201 and the fact that the $\boldsymbol { \Psi }$ -weights are nonzero only for multiple of 12, namely,

$$
\psi_ {j} = \left\{ \begin{array}{l l} \Phi^ {j / 1 2} & \text {f o r} j = 0, 1 2, 2 4, \dots \\ 0 & \text {o t h e r w i s e} \end{array} \right. \tag {10.5.9}
$$

we have that the forecast error variance can be written as

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) = \left[ \frac {1 - \Phi^ {2 k + 2}}{1 - \Phi^ {2}} \right] \sigma_ {e} ^ {2} \tag {10.5.10}
$$

where, as before, $k$ is the integer part of $( \ell - 1 ) / 1 2$ .

# Seasonal MA(1)12

For the seasonal $\mathrm { M A } ( 1 ) _ { 1 2 }$ model, we have

$$
Y _ {t} = e _ {t} - \Theta e _ {t - 1 2} + \theta_ {0} \tag {10.5.11}
$$

In this case, we see that

$$
\left. \begin{array}{c} \hat {Y} _ {t} (1) = - \Theta e _ {t - 1 1} + \theta_ {0} \\ \hat {Y} _ {t} (2) = - \Theta e _ {t - 1 0} + \theta_ {0} \\ \vdots \\ \hat {Y} _ {t} (1 2) = - \Theta e _ {t} + \theta_ {0} \end{array} \right\} \tag {10.5.12}
$$

and

$$
\hat {Y} _ {t} (\ell) = \theta_ {0} \text {f o r} \ell > 1 2 \tag {10.5.13}
$$

Here we obtain different forecasts for the months of the first year, but from then on all forecasts are given by the process mean.

For this model, $\Psi _ { 0 } = 1$ , $\psi _ { 1 2 } = - \Theta$ , and $\psi _ { j } = 0$ otherwise. Thus, from Equation (9.3.38) on page 201,

$$
V a r \left(e _ {t} (\ell)\right) = \left\{ \begin{array}{c c} \sigma_ {e} ^ {2} & 1 \leq \ell \leq 1 2 \\ \left(1 + \Theta^ {2}\right) \sigma_ {e} ^ {2} & 1 2 <   \ell \end{array} \right. \tag {10.5.14}
$$

# ARIMA(0,0,0)×(0,1,1)12

The ARIMA $( 0 , 0 , 0 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ model is

$$
Y _ {t} - Y _ {t - 1 2} = e _ {t} - \Theta e _ {t - 1 2} \tag {10.5.15}
$$

or

$$
Y _ {t + \ell} = Y _ {t + \ell - 1 2} + e _ {t + \ell} - \Theta e _ {t + \ell - 1 2}
$$

so that

$$
\left. \begin{array}{c} \hat {Y} _ {t} (1) = Y _ {t - 1 1} - \Theta e _ {t - 1 1} \\ \hat {Y} _ {t} (2) = Y _ {t - 1 0} - \Theta e _ {t - 1 0} \\ \vdots \\ \hat {Y} _ {t} (1 2) = Y _ {t} - \Theta e _ {t} \end{array} \right\} \tag {10.5.16}
$$

and then

$$
\hat {Y} _ {t} (\ell) = \hat {Y} _ {t} (\ell - 1 2) \text {f o r} \ell > 1 2 \tag {10.5.17}
$$

It follows that all Januarys will forecast identically, all Februarys identically, and so forth.

If we invert this model, we find that

$$
Y _ {t} = (1 - \Theta) (Y _ {t - 1 2} + \Theta Y _ {t - 2 4} + \Theta^ {2} Y _ {t - 3 6} + \dots) + e _ {t}
$$

Consequently, we can write

$$
\left. \begin{array}{c} \hat {Y} _ {t} (1) = (1 - \Theta) \sum_ {j = 0} ^ {\infty} \Theta^ {j} Y _ {t - 1 1 - 1 2 j} \\ \hat {Y} _ {t} (2) = (1 - \Theta) \sum_ {j = 0} ^ {\infty} \Theta^ {j} Y _ {t - 1 0 - 1 2 j} \\ \vdots \\ \hat {Y} _ {t} (1 2) = (1 - \Theta) \sum_ {j = 0} ^ {\infty} \Theta^ {j} Y _ {t - 1 2 j} \end{array} \right\} \tag {10.5.18}
$$

From this representation, we see that the forecast for each January is an exponentially weighted moving average of all observed Januarys, and similarly for each of the other months.

In this case, we have $\psi _ { j } = 1 - \Theta$ for $j = 1 2$ , 24,…, and zero otherwise. The forecast error variance is then

$$
\operatorname {V a r} \left(e _ {t} (\ell)\right) = [ 1 + k (1 - \Theta) ^ {2} ] \sigma_ {e} ^ {2} \tag {10.5.19}
$$

where $k$ is the integer part of $( \ell - 1 ) / 1 2$ .

# ARIMA(0,1,1)×(0,1,1)12

For the ARIMA(0,1,1)×(0,1,1)12 model

$$
Y _ {t} = Y _ {t - 1} + Y _ {t - 1 2} - Y _ {t - 1 3} + e _ {t} - \Theta e _ {t - 1} - \Theta e _ {t - 1 2} + \theta \Theta e _ {t - 1 3} \tag {10.5.20}
$$

the forecasts satisfy

$$
\left. \begin{array}{c c c} \hat {Y} _ {t} (1) = Y _ {t} & + Y _ {t - 1 1} - Y _ {t - 1 2} - \theta e _ {t} - \Theta e _ {t - 1 1} + \theta \Theta e _ {t - 1 2} \\ \hat {Y} _ {t} (2) = \hat {Y} _ {t} (1) & + Y _ {t - 1 0} - Y _ {t - 1 1} & - \Theta e _ {t - 1 0} + \theta \Theta e _ {t - 1 1} \\ \vdots & & \\ \hat {Y} _ {t} (1 2) = \hat {Y} _ {t} (1 1) + Y _ {t} & - Y _ {t - 1} & - \Theta e _ {t} + \theta \Theta e _ {t - 1} \\ \hat {Y} _ {t} (1 3) = \hat {Y} _ {t} (1 2) + \hat {Y} _ {t} (1) & - Y _ {t} & + \theta \Theta e _ {t} \end{array} \right\} \tag {10.5.21}
$$

and

$$
\hat {Y} _ {t} (\ell) = \hat {Y} _ {t} (\ell - 1) + \hat {Y} _ {t} (\ell - 1 2) - \hat {Y} _ {t} (\ell - 1 3) \text {f o r} \ell > 1 3 \tag {10.5.22}
$$

To understand the general pattern of these forecasts, we can use the representation

$$
\hat {Y} _ {t} (\ell) = A _ {1} + A _ {2} \ell + \sum_ {j = 0} ^ {6} \left[ B _ {1 j} \cos \left(\frac {2 \pi j \ell}{1 2}\right) + B _ {2 j} \sin \left(\frac {2 \pi j \ell}{1 2}\right) \right] \tag {10.5.23}
$$

where the A’s and B’s are dependent on $Y _ { t } , Y _ { t - 1 } , \ldots$ , or, alternatively, determined from the initial forecasts $\hat { \boldsymbol Y } _ { t } ( 1 )$ , ,…, Y^t( ) 2 $\hat { Y } _ { t } ( 1 3 )$ . This result follows from the general theory of difference equations and involves the roots of $( 1 - x ) ( 1 - x ^ { 1 2 } ) = 0$ .

Notice that Equation (10.5.23) reveals that the forecasts are composed of a linear trend in the lead time plus a sum of periodic components. However, the coefficients $A _ { i }$ and $B _ { i j }$ are more dependent on recent data than on past data and will adapt to changes in the process as our forecast origin changes and the forecasts are updated. This is in stark contrast to forecasting with deterministic time trend plus seasonal components, where the coefficients depend rather equally on both recent and past data and remain the same for all future forecasts.

# Prediction Limits

Prediction limits are obtained precisely as in the nonseasonal case. We illustrate this with the carbon dioxide time series. Exhibit 10.16 shows the forecasts and $9 5 \%$ forecast limits for a lead time of two years for the $\mathrm { A R I M A } ( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ model that we fit. The last two years of observed data are also shown. The forecasts mimic the stochastic periodicity in the data quite well, and the forecast limits give a good feeling for the precision of the forecasts.

![](images/395d5259e4a5112ed9073589a2c4a0836a3feac983e869b483a367092ab346d8.jpg)  
Exhibit 10.16 Forecasts and Forecast Limits for the $\mathtt { c o } _ { 2 }$ Model

```javascript
> win.graph(width=4.875,height=3,pointsize=8)  
> plot(m1.co2,n1=c(2003,1),n.ahead=24,xlab='Year',type='o', ylab='CO2 Levels') 
```

Exhibit 10.17 displays the last year of observed data and forecasts out four years. At this lead time, it is easy to see that the forecast limits are getting wider, as there is more uncertainty in the forecasts.

![](images/80aa564441f9ce0eb723bf63f02b4afbae5c92d3fcf97eab121be275cd2f3acf.jpg)  
Exhibit 10.17 Long-Term Forecasts for the $\mathtt { c o } _ { 2 }$ Model

```javascript
> plot(m1.co2, n1=c(2004, 1), n.ahead=48, xlab='Year', type='b', ylab='CO2 Levels') 
```

# 10.6 Summary

Multiplicative seasonal ARIMA models provide an economical way to model time series whose seasonal tendencies are not as regular as we would have with a deterministic seasonal trend model which we covered in Chapter 3. Fortunately, these models are simply special ARIMA models so that no new theory is needed to investigate their properties. We illustrated the special nature of these models with a thorough modeling of an actual time series.

# EXERCISES

10.1 Based on quarterly data, a seasonal model of the form

$$
Y _ {t} = Y _ {t - 4} + e _ {t} - \theta_ {1} e _ {t - 1} - \theta_ {2} e _ {t - 2}
$$

has been fit to a certain time series.

(a) Find the first four $\boldsymbol { \Psi }$ -weights for this model.   
(b) Suppose that $\theta _ { 1 } = 0 . 5$ , $\theta _ { 2 } = - 0 . 2 5$ , and $\sigma _ { e } = 1$ . Find forecasts for the next four quarters if data for the last four quarters are

<table><tr><td>Quarter</td><td>I</td><td>II</td><td>III</td><td>IV</td></tr><tr><td>Series</td><td>25</td><td>20</td><td>25</td><td>40</td></tr><tr><td>Residual</td><td>2</td><td>1</td><td>2</td><td>3</td></tr></table>

(c) Find $9 5 \%$ prediction intervals for the forecasts in part (b).

10.2 An AR model has AR characteristic polynomial

$$
(1 - 1. 6 x + 0. 7 x ^ {2}) (1 - 0. 8 x ^ {1 2})
$$

(a) Is the model stationary?   
(b) Identify the model as a certain seasonal ARIMA model.

10.3 Suppose that $\{ Y _ { t } \}$ satisfies

$$
Y _ {t} = a + b t + S _ {t} + X _ {t}
$$

where $S _ { t }$ is deterministic and periodic with period $s$ and $\{ X _ { t } \}$ is a seasonal $\mathsf { A R I M A } ( p , 0 , q ) \times ( P , 1 , Q ) _ { s }$ series. What is the model for $W _ { t } = Y _ { t } - Y _ { t - s } ?$

10.4 For the seasonal model $Y _ { t } = \Phi Y _ { t - 4 } + e _ { e } - \Theta e _ { t - 1 }$ with $\left| \Phi \right| < 1$ , find $\gamma _ { 0 }$ and $\rho _ { k }$   
10.5 Identify the following as certain multiplicative seasonal ARIMA models:

(a) $Y _ { t } = 0 . 5 Y _ { t - 1 } + Y _ { t - 4 } - 0 . 5 Y _ { t - 5 } + e _ { t } - 0 . 3 e _ { t - 1 }$   
(b) $Y _ { t } = Y _ { t - 1 } + Y _ { t - 1 2 } - Y _ { t - 1 3 } + e _ { t } - 0 . 5 e _ { t - 1 } - 0 . 5 e _ { t - 1 2 } + 0 . 2 5 e _ { t - 1 3 } .$

10.6 Verify Equations (10.2.11) on page 232.

10.7 Suppose that the process $\{ Y _ { t } \}$ develops according to $Y _ { t } = Y _ { t - 4 } + e _ { t }$ with $Y _ { t } = e _ { t }$ for $t = 1 , 2 , 3$ , and 4.

(a) Find the variance function for $\{ Y _ { t } \}$   
(b) Find the autocorrelation function for $\{ Y _ { t } \}$   
(c) Identify the model for $\{ Y _ { t } \}$ as a certain seasonal ARIMA model.

10.8 Consider the Alert, Canada, monthly carbon dioxide time series shown in Exhibit 10.1 on page 227. The data are in the file named co2.

(a) Fit a deterministic seasonal means plus linear time trend model to these data. Are any of the regression coefficients “statistically significant”?   
(b) What is the multiple R-squared for this model?   
(c) Now calculate the sample autocorrelation of the residuals from this model. Interpret the results.

10.9 The monthly airline passenger time series, first investigated in Box and Jenkins (1976), is considered a classic time series. The data are in the file named airpass.

(a) Display the time series plots of both the original series and the logarithms of the series. Argue that taking logs is an appropriate transformation.   
(b) Display and interpret the time series plots of the first difference of the logged series.   
(c) Display and interpret the time series plot of the seasonal difference of the first difference of the logged series.   
(d) Calculate and interpret the sample ACF of the seasonal difference of the first difference of the logged series.   
(e) Fit the “airline model” $( \mathrm { A R I M A } ( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 } )$ ) to the logged series.   
(f) Investigate diagnostics for this model, including autocorrelation and normality of the residuals.   
(g) Produce forecasts for this series with a lead time of two years. Be sure to include forecast limits.

10.10 Exhibit 5.8 on page 99 displayed the monthly electricity generated in the United States. We argued there that taking logarithms was appropriate for modeling. Exhibit 5.10 on page 100 showed the time series plot of the first differences for this series. The filename is electricity.

(a) Calculate the sample ACF of the first difference of the logged series. Is the seasonality visible in this display?   
(b) Plot the time series of seasonal difference and first difference of the logged series. Does a stationary model seem appropriate now?   
(c) Display the sample ACF of the series after a seasonal difference and a first difference have been taken of the logged series. What model(s) might you consider for the electricity series?

10.11 The quarterly earnings per share for 1960–1980 of the U.S. company Johnson & Johnson, are saved in the file named JJ.

(a) Plot the time series and also the logarithm of the series. Argue that we should transform by logs to model this series.   
(b) The series is clearly not stationary. Take first differences and plot that series. Does stationarity now seem reasonable?   
(c) Calculate and graph the sample ACF of the first differences. Interpret the results.   
(d) Display the plot of seasonal differences and the first differences. Interpret the plot. Recall that for quarterly data, a season is of length 4.   
(e) Graph and interpret the sample ACF of seasonal differences with the first differences.   
(f) Fit the model ARIMA(0,1,1)×(0,1,1)4, and assess the significance of the estimated coefficients.   
(g) Perform all of the diagnostic tests on the residuals.   
(h) Calculate and plot forecasts for the next two years of the series. Be sure to include forecast limits.

10.12 The file named boardings contains monthly data on the number of people who boarded transit vehicles (mostly light rail trains and city buses) in the Denver, Colorado, region for August 2000 through December 2005.

(a) Produce the time series plot for these data. Be sure to use plotting symbols that will help you assess seasonality. Does a stationary model seem reasonable?   
(b) Calculate and plot the sample ACF for this series. At which lags do you have significant autocorrelation?   
(c) Fit an $\mathbf { A R M A } ( 0 , 3 ) { \times } ( 1 , 0 ) _ { 1 2 }$ model to these data. Assess the significance of the estimated coefficients.   
(d) Overfit with an ARMA $( 0 , 4 ) \times ( 1 , 0 ) _ { 1 2 }$ model. Interpret the results.

# CHAPTER 11

# TIME SERIES REGRESSION MODELS

In this chapter, we introduce several useful ideas that incorporate external information into time series modeling. We start with models that include the effects of interventions on time series’ normal behavior. We also consider models that assimilate the effects of outliers—observations, either in the observed series or in the error terms, that are highly unusual relative to normal behavior. Lastly, we develop methods to look for and deal with spurious correlation—correlation between series that is artificial and will not help model or understand the time series of interest. We will see that prewhitening of series helps us find meaningful relationships.

# 11.1 Intervention Analysis

Exhibit 11.1 shows the time plot of the logarithms of monthly airline passenger-miles in the United States from January 1996 through May 2005. The time series is highly seasonal, displaying the fact that air traffic is generally higher during the summer months and the December holidays and lower in the winter months.† Also, air traffic was increasing somewhat linearly overall until it had a sudden drop in September 2001. The sudden drop in the number of air passengers in September 2001 and several months thereafter was triggered by the terrorist acts on September 11, 2001, when four planes were hijacked, three of which were crashed into the twin towers of the World Trade Center and the Pentagon and the fourth into a rural field in Pennsylvania. The terrorist attacks of September 2001 deeply depressed air traffic around that period, but air traffic gradually regained the losses as time went on. This is an example of an intervention that results in a change in the trend of a time series.

Intervention analysis, introduced by Box and Tiao (1975), provides a framework for assessing the effect of an intervention on a time series under study. It is assumed that the intervention affects the process by changing the mean function or trend of a time series. Interventions can be natural or man-made. For example, some animal population levels crashed to a very low level in a particular year because of extreme climate in that year. The postcrash annual population level may then be expected to be different from that in the precrash period. Another example is the increase of the speed limit from 65 miles per hour to 70 miles per hour on an interstate highway. This may make driving on

the highway more dangerous. On the other hand, drivers may stay on the highway for a shorter length of time because of the faster speed, so the net effect of the increased speed limit change is unclear. The effect of the increase in speed limit may be studied by analyzing the mean function of some accident time series data; for example, the quarterly number of fatal car accidents on some segment of an interstate highway. (Note that the autocovariance function of the time series might also be changed by the intervention, but this possibility will not be pursued here.)

Exhibit 11.1 Monthly U.S. Airline Miles: January 1996 through May 2005   
![](images/8e158ccb45e85d9244ad58dce55117fa8bb2911c4e1b073bc5381c4e90459a5e.jpg)  
> win.graph(width $\mathord { \left. \begin{array} { r l r } \end{array} \right. } = 4$ .875,height=2.5,pointsize=8)   
> data(airmiles)   
> plot(log(airmiles),ylab='Log(airmiles)',xlab='Year')

We first consider the simple case of a single intervention. The general model for the time series $\{ Y _ { t } \}$ , perhaps after suitable transformation, is given by

$$
Y _ {t} = m _ {t} + N _ {t} \tag {11.1.1}
$$

where $m _ { t }$ is the change in the mean function and $N _ { t }$ is modeled as some ARIMA process, possibly seasonal. The process $\{ N _ { t } \}$ represents the underlying time series were there no intervention. It is referred to as the natural or unperturbed process, and it may be stationary or nonstationary, seasonal or nonseasonal. Suppose the time series is subject to an intervention that takes place at time T. Before T, $m _ { t }$ is assumed to be identically zero. The time series $\{ Y _ { t } , t < T \}$ is referred to as the preintervention data and can be used to specify the model for the unperturbed process $N _ { t } .$

Based on subject matter considerations, the effect of the intervention on the mean function can often be specified up to some parameters. A useful function in this specification is the step function

$$
S _ {t} ^ {(T)} = \left\{ \begin{array}{l} 1, \text {i f} t \geq T \\ 0, \text {o t h e r w i s e} \end{array} \right. \tag {11.1.2}
$$

that is 0 during the preintervention period and 1 throughout the postintervention period. The pulse function

$$
P _ {t} ^ {(T)} = S _ {t} ^ {(T)} - S _ {t - 1} ^ {(T)} \tag {11.1.3}
$$

equals 1 at t = T and 0 otherwise. That is, Pt( ) T $t = T$ $P _ { t } ^ { ( T ) }$ is the indicator or dummy variable flagging the time that the intervention takes place. If the intervention results in an immediate and permanent shift in the mean function, the shift can be modeled as

$$
m _ {t} = \omega S _ {t} ^ {(T)} \tag {11.1.4}
$$

where ω is the unknown permanent change in the mean due to the intervention. Testing whether $\omega = 0$ or not is similar to testing whether the population means are the same with data in the form of two independent random samples from the two populations. However, the major difference here is that the pre- and postintervention data cannot generally be assumed to be independent and identically distributed. The inherent serial correlation in the data makes the problem more interesting but at the same time more difficult. If there is a delay of $d$ time units before the intervention takes effect and $d$ is known, then we can specify

$$
m _ {t} = \omega S _ {t - d} ^ {(T)} \tag {11.1.5}
$$

In practice, the intervention may affect the mean function gradually, with its full force reflected only in the long run. This can be modeled by specifying ${ \underline { { m } } } _ { t }$ as an AR(1)-type model with the error term replaced by a multiple of the lag 1 of $S _ { t } ^ { ( T ) }$ :

$$
m _ {t} = \delta m _ {t - 1} + \omega S _ {t - 1} ^ {(T)} \tag {11.1.6}
$$

with the initial condition $m _ { 0 } = 0$ . After some algebra, it can be shown that

$$
m _ {t} = \left\{ \begin{array}{l} \omega \frac {1 - \delta^ {t - T}}{1 - \delta}, \text {f o r} t > T \\ 0, \text {o t h e r w i s e} \end{array} \right. \tag {11.1.7}
$$

Often δ is selected in the range $1 > \delta > 0$ . In that case, $m _ { t }$ approaches $\Theta / ( 1 - \delta )$ for large t, which is the ultimate change (gain or loss) for the mean function. Half of the ultimate change is attained when $\bar { 1 - \delta ^ { t - T } } = 0 . 5$ ; that is, when $t = T + \log ( 0 . 5 ) / \log ( \delta )$ . The duration $\log ( 0 . 5 ) / \log ( \delta )$ is called the half-life of the intervention effect, and the shorter it is, the quicker the ultimate change is felt by the system. Exhibit 11.2 displays the half-life as a function of δ, which shows that the half-life increases with δ. Indeed, the half-life becomes infinitely large when δ approaches 1.

Exhibit 11.2 Half-life based on an AR(1) Process with Step Function Input   

<table><tr><td>δ</td><td>0.2</td><td>0.4</td><td>0.6</td><td>0.8</td><td>0.9</td><td>1</td></tr><tr><td>Half-life</td><td>0.43</td><td>0.76</td><td>1.46</td><td>3.11</td><td>6.58</td><td>∞</td></tr></table>

It is interesting to note the limiting case when $\delta = 1$ . Then $m _ { t } = \omega ( T - t )$ for $t \geq T$ and 0 otherwise. The time sequence plot of $m _ { t }$ displays the shape of a ramp with slope ω. This specification implies that the intervention changes the mean function linearly in the postintervention period. This ramp effect (with a one time unit delay) is shown in Exhibit 11.3 (c).

Short-lived intervention effects may be specified using the pulse dummy variable

$$
P _ {t} ^ {(T)} = \left\{ \begin{array}{l} 1, \text {i f} t = T \\ 0, \text {o t h e r w i s e} \end{array} \right. \tag {11.1.8}
$$

For example, if the intervention impacts the mean function only at $t = T ,$ then

$$
m _ {t} = \omega P _ {t} ^ {(T)} \tag {11.1.9}
$$

Intervention effects that die out gradually may be specified via the AR(1)-type specification

$$
m _ {t} = \delta m _ {t - 1} + \omega P _ {t} ^ {(T)} \tag {11.1.10}
$$

That is, $m _ { t } = \cos ^ { T - t }$ for $t \geq T$ so that the mean changes immediately by an amount ω and subsequently the change in the mean decreases geometrically by the common factor of δ; see Exhibit 11.4 (a). Delayed changes can be incorporated by lagging the pulse function. For example, if the change in the mean takes place after a delay of one time unit and the effect dies out gradually, we can specify

$$
m _ {t} = \delta m _ {t - 1} + \omega P _ {t - 1} ^ {(T)} \tag {11.1.11}
$$

Again, we assume the initial condition $m _ { 0 } = 0$ .

It is useful to write† the preceding model in terms of the backshift operator $B$ where Bmt = mt − 1 and BPt( ) T $B m _ { t } = m _ { t - 1 }$ $B P _ { t } ^ { ( T ) } = P _ { t - 1 } ^ { ( T ) }$ . Then $( 1 - \delta B ) m _ { t } = \odot B P _ { t } ^ { ( T ) }$ . Or, we can write

$$
m _ {t} = \frac {\omega B}{1 - \delta B} P _ {t} ^ {(T)} \tag {11.1.12}
$$

Recall $( 1 - B ) S _ { t } ^ { ( T ) } = P _ { t } ^ { ( T ) }$ ,P( ) T which can be rewritten as S( ) T = $S _ { t } ^ { ( T ) } = \frac { 1 } { 1 - B } P _ { t } ^ { ( T ) }$ ( ) T

# Exhibit 11.3 Some Common Models for Step Response Interventions (All are shown with a delay of 1 time unit)

![](images/b9258d567c8c9f27ee85f2bbf436485e43aa98e58980e8c6058296c9f32a6adf.jpg)  
(a)

(b)   
![](images/28200beaad8c6b9a73d71b7de5c78276a3aa16a620e6a62087859c41b9cbd646.jpg)  
ωB -----S( ) T 1 – δB

![](images/4abf439eb56b8058270c0d5705d02899ae22eb362301c2c76a69e340b892416e.jpg)

(c)   
![](images/a66e01291f83919e8d58b0a4415cb39d9bdd115694a78fed78e6a65f4c73971a.jpg)  
ωB ------S( ) T 1 – B

![](images/8e3f240c5cd9328e78a7cb333391cac0d4b69955d858b4c71109efd6896f686b.jpg)

Several specifications can be combined to model more sophisticated intervention effects.

For example,

$$
m _ {t} = \frac {\omega_ {1} B}{1 - \delta B} P _ {t} ^ {(T)} + \frac {\omega_ {2} B}{1 - B} P _ {t} ^ {(T)} \tag {11.1.13}
$$

depicts the situation displayed in Exhibit 11.4 (b) where $\omega _ { 1 }$ and $\boldsymbol { \mathfrak { o } } _ { 2 }$ are both greater than zero, and

$$
m _ {t} = \omega_ {0} P _ {t} ^ {(T)} + \frac {\omega_ {1} B}{1 - \delta B} P _ {t} ^ {(T)} + \frac {\omega_ {2} B}{1 - B} P _ {t} ^ {(T)} \tag {11.1.14}
$$

may model situations like Exhibit 11.4 (c) with $\omega _ { 1 }$ and $\boldsymbol { \mathfrak { o } } _ { 2 }$ both negative. This last case may model the interesting situation where a special sale may cause strong rush buying, initially so much so that the sale is followed by depressed demand. More generally, we can model the change in the mean function by an ARMA-type specification

$$
m _ {t} = \frac {\omega (B)}{\delta (B)} P _ {t} ^ {(T)} \tag {11.1.15}
$$

where ${ \mathfrak { o } } ( B )$ and $\delta ( B )$ are some polynomials in $B$ . Because $( 1 - B ) S _ { t } ^ { ( T ) } = P _ { t } ^ { ( T ) }$ ( ) T , the model for $m _ { t }$ can be specified in terms of either the pulse or step dummy variable.

# Exhibit 11.4 Some Common Models for Pulse Response Interventions (All are shown with a delay of 1 time unit)

(a) ωB ---- P( ) T 1 – δB t

![](images/a20ccf0b9d8e3549161b8f4e0a9f84d9b01e1076db8dc2c8fe63c88eda239581.jpg)

(b) ω1B ω2B ( ) T $\Big [ \frac { \omega _ { 1 } B } { 1 - \delta B } + \frac { \omega _ { 2 } B } { 1 - B } \Big ] P _ { t } ^ { ( T ) }$

![](images/178a67a2646faf7fb1633cad9d5fcb901e392bb03dd5b34c337f7d7c1c643875.jpg)

(c) $\bigg [ \mathfrak { o } _ { 0 } + \frac { \mathfrak { o } _ { 1 } B } { 1 - \delta B } + \frac { \mathfrak { o } _ { 2 } B } { 1 - B } \bigg ] P _ { t } ^ { ( T ) }$

![](images/b7ed242d0ef5db42bd140c447488f4212a68bfbe853ab66289c245398010bf82.jpg)

Estimation of the parameters of an intervention model may be carried out by the method of maximum likelihood estimation. Indeed, $Y _ { t } - m _ { t }$ is a seasonal ARIMA process so that the likelihood function equals the joint pdf of $Y _ { t } - m _ { t }$ , $t = 1$ , 2,…, n, which can be computed by methods studied in Chapter 7 or else by the state space modeling methods of Appendix H on page 222.

We now revisit the monthly passenger-airmiles data. Recall that the terrorist acts in September 2001 had lingering depressing effects on air traffic. The intervention may be specified as an AR(1) process with the pulse input at September 2001. But the unexpected turn of events in September 2001 had a strong instantaneous chilling effect on air traffic. Thus, we model the intervention effect (the 9/11 effect) as

$$
m _ {t} = \omega_ {0} P _ {t} ^ {(T)} + \frac {\omega_ {1}}{1 - \omega_ {2} B} P _ {t} ^ {(T)}
$$

where $T$ denotes September 2001. In this specification, $\boldsymbol { \omega } _ { 0 } + \boldsymbol { \omega } _ { 1 }$ represents the instantaneous 9/11 effect, and, for $k \geq 1$ , $\mathfrak { o } _ { 1 } ( \mathfrak { o } _ { 2 } ) ^ { k }$ gives the 9/11 effect $k$ months afterward. It remains to specify the seasonal ARIMA structure of the underlying unperturbed process. Based on the preintervention data, an ARIMA(0,1,1)×(0,1,0)12 model was tentatively specified for the unperturbed process; see Exhibit 11.5.

Exhibit 11.5 Sample ACF for (1−B)(1−B12) Log(Air Passenger Miles) Over the Preintervention Period   
![](images/7cf5a6a893d466dd750ec0d88369d53e0008701271d04f6e125925840490b73c.jpg)  
> acf(as.vector(diff(diff(window(log(airmiles),end=c(2001,8)), 12))),lag.max=48)

Model diagnostics of the fitted model suggested that a seasonal MA(1) coefficient was needed and the existence of some additive outliers occurring in December 1996, January 1997, and December 2002. (Outliers will be discussed in more detail later; here additive outliers may be regarded as interventions of unknown nature that have a pulse response function.) Hence, the model is specified as an $\mathrm { A R I M A } ( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ plus the 9/11 intervention and three additive outliers. The fitted model is summarized in Exhibit 11.6.

Exhibit 11.6 Estimation of Intervention Model for Logarithms of Air Miles (Standard errors are shown below the estimates)   

<table><tr><td>θ</td><td>Θ</td><td>Dec96</td><td>Jan97</td><td>Dec02</td><td>ω0</td><td>ω1</td><td>ω2</td></tr><tr><td>0.383</td><td>0.650</td><td>0.099</td><td>-0.069</td><td>0.081</td><td>-0.095</td><td>-0.27</td><td>0.814</td></tr><tr><td>(0.093)</td><td>(0.119)</td><td>(0.023)</td><td>(0.022)</td><td>(0.020)</td><td>(0.046)</td><td>(0.044)</td><td>(0.098)</td></tr><tr><td colspan="8">σ2 estimated as 0.000672: log-likelihood = 219.99, AIC= -423.98</td></tr></table>

> air.m1=arimax(log(airmiles),order=c(0,1,1),
		seasonal=list(order=c(0,1,1),period=12),
		(xtransf=data.frame(I911=1*(seq(airmiles) == 69), $\mathrm{I}{911} = 1 * \left( {\operatorname{seq}\left( {\text{airmiles}}\right)  =  = {69}}\right)$ ),transfer=list(c(0,0),c(1,0)),
		(xreg=data.frame(Dec96=1*(seq(airmiles) == 12),
			Jan97=1*(seq(airmiles) == 13),Dec02=1*(seq(airmiles) == 84)),,
	(method='ML'))

Model diagnostics suggested that the fitted model above provides a good fit to the data. The open circles in the time series plot shown in Exhibit 11.7 represent the fitted values from the final estimated model. They indicate generally good agreement between the model and the data.

Exhibit 11.7 Logs of Air Passenger Miles and Fitted Values   
![](images/a28036445be2cd1290b9e172dfa93d8d56b352bce79c341f6bd477b45b06fade.jpg)  
> plot(log(airmiles),ylab='Log(airmiles)')   
> points(fitted(air.m1))

The fitted model estimates that the 9/11 intervention reduced air traffic by $3 1 \% =$ $\{ 1 - \exp ( - 0 . 0 9 4 9 - 0 . 2 7 1 5 ) \} \times 1 0 0 \%$ in September 2001, and air traffic $k$ months later was lowered by $\{ 1 - \exp ( - 0 . 2 7 1 5 \times 0 . 8 1 3 9 ^ { k } ) \} \times 1 0 0 \%$ $1 \% \times 1 0 0 \%$ . Exhibit 11.8 graphs the estimated 9/11 effects on air traffic, which indicate that air traffic regained its losses toward the end of 2003.

Exhibit 11.8 The Estimated 9/11 Effects for the Air Passenger Series   
![](images/426c52ec67667e8ac559bf17a797e793a6326359029c55bb28eb6a2b58b6ae6e.jpg)  
> Nine11p=1*(seq(airmiles)==69)   
> plot(ts(Nine11p*(-0.0949)+

```txt
filter(Nine11p, filter = .8139, method = 'recursive', side = 1) * (-0.2715), frequency = 12, start = 1996), ylab = '9/11 Effects', type = 'h'); abline(h = 0) 
```

# 11.2 Outliers

Outliers refer to atypical observations that may arise because of measurement and/or copying errors or because of abrupt, short-term changes in the underlying process. For time series, two kinds of outliers can be distinguished, namely additive outliers and innovative outliers. These two kinds of outliers are often abbreviated as AO and IO, respectively. An additive outlier occurs at time $T$ if the underlying process is perturbed additively at time $T$ so that the data equal

$$
Y _ {t} ^ {\prime} = Y _ {t} + \omega_ {A} P _ {t} ^ {(T)} \tag {11.2.1}
$$

where $\{ Y _ { t } \}$ is the unperturbed process. Henceforth in this section, $Y ^ { ' }$ denotes the observed process that may be affected by some outliers and $Y$ the unperturbed process should there be no outliers. Thus, $Y _ { T } ^ { ' } = \ : Y _ { T } + \infty _ { A }$ but $Y _ { t } ^ { ' } = Y _ { t }$ otherwise, so the time series is only affected at time $T$ if it has an additive outlier at $T .$ An additive outlier can also be treated as an intervention that has a pulse response at $T$ so that ${ m _ { t } } = { \odot _ { A } } P _ { t } ^ { ( T ) }$ .

On the other hand, an innovative outlier occurs at time $t$ if the error (also known as an innovation) at time $t$ is perturbed (that is, the errors equal $e _ { t } ^ { \prime } = e _ { t } + \omega _ { I } P _ { t } ^ { ( T ) }$ , where et $e _ { t }$ is a zero-mean white noise process). So, $e _ { T } ^ { \prime } = e _ { T } + \infty _ { I }$ but $e _ { t } ^ { \prime } = e _ { t }$ otherwise. Suppose that the unperturbed process is stationary and admits an $\mathbf { M A } ( \infty )$ representation

$$
Y _ {t} = e _ {t} + \psi_ {1} e _ {t - 1} + \psi_ {2} e _ {t - 2} + \dots
$$

Consequently, the perturbed process can be written

$$
\begin{array}{l} Y _ {t} ^ {\prime} = e _ {t} ^ {\prime} + \psi_ {1} e _ {t - 1} ^ {\prime} + \psi_ {2} e _ {t - 2} ^ {\prime} + \dots \\ = \left[ e _ {t} + \psi_ {1} e _ {t - 1} + \psi_ {2} e _ {t - 2} + \dots \right] + \psi_ {t - T} \omega_ {I} \\ \end{array}
$$

or

$$
Y _ {t} ^ {\prime} = Y _ {t} + \psi_ {t - T} \omega_ {I} \tag {11.2.2}
$$

where $\Psi _ { 0 } = 1$ and $\psi _ { j } = 0$ for negative $j$ . Thus, an innovative outlier at $T$ perturbs all observations on and after $T ,$ , although with diminishing effect, as the observation is further away from the origin of the outlier.

To detect whether an observation is an AO or IO, we use the $\operatorname { A R } ( \infty )$ representation of the unperturbed process to define the residuals:

$$
a _ {t} = Y _ {t} ^ {\prime} - \pi_ {1} Y _ {t - 1} ^ {\prime} - \pi_ {2} Y _ {t - 2} ^ {\prime} - \dots \tag {11.2.3}
$$

For simplicity, we assume the process has zero mean and that the parameters are known. In practice, the unknown parameter values are replaced by their estimates from the possibly perturbed data. Under the null hypothesis of no outliers and for large samples, this

has a negligible effect on the properties of the test procedures described below. If the series has exactly one IO at time T, then the residual $a _ { T } = \omega _ { I } + e _ { T }$ but $a _ { t } = e _ { t }$ otherwise. So $\mathfrak { O } _ { I }$ can be estimated by $\tilde { \omega } _ { I } = a _ { T }$ with variance equal to $\overline { { \sigma } } ^ { 2 }$ . Thus, a test statistic for testing for an IO at $T$ is

$$
\lambda_ {1, T} = \frac {a _ {T}}{\sigma} \tag {11.2.4}
$$

which has (approximately) a standard normal distribution under the null hypothesis that there are no outliers in the time series. When $T$ is known beforehand, the observation in question is declared an outlier if the corresponding standardized residual exceeds 1.96 in magnitude at the $5 \%$ significance level. In practice, there is often no prior knowledge about T, and the test is applied to all observations. In addition, $\sigma$ will need to be estimated. A simple conservative procedure is to use the Bonferroni rule for controlling the overall error rate of multiple tests. Let

$$
\lambda_ {1} = \max  _ {1 \leq t \leq n} \left| \lambda_ {1, t} \right| \tag {11.2.5}
$$

be attained at $t = T .$ Then the Tth observation is deemed an IO if $\lambda _ { 1 }$ exceeds the upper $0 . 0 2 5 / n \times 1 0 0$ percentile of the standard normal distribution. This procedure guarantees that there is at most a $5 \%$ probability of a false detection of an IO. Note that an outlier will inflate the maximum likelihood estimate of σ, so if there is no adjustment for outliers, the power of most tests is usually reduced. A robust estimate of the noise standard deviation may be used in lieu of the maximum likelihood estimate to increase the power of the test. For example, σ can be more robustly estimated by the mean absolute residual times $\sqrt { 2 / \pi }$ .

The detection of an AO is more complex. Suppose that the process admits an AO at $T$ and is otherwise free of outliers. Then it can be shown that

$$
a _ {t} = - \omega_ {A} \pi_ {t - T} + e _ {t} \tag {11.2.6}
$$

where $\pi _ { 0 } = - 1$ and $\pi _ { j } = 0$ for negative $j$ . Hence, $\boldsymbol { a } _ { t } = \boldsymbol { e } _ { t }$ for $t < T$ , $a _ { T } = \omega _ { A } + e _ { T } ,$ $a _ { T + 1 } = - \infty _ { A } \pi _ { 1 } + e _ { T + 1 }$ , $a _ { T + 2 } = - \infty _ { A } \pi _ { 2 } + e _ { T + 2 }$ , and so forth. A least squares estimator of ${ \mathfrak { O } } _ { A }$ is

$$
\tilde {\omega} _ {T, A} = - \rho^ {2} \sum_ {t = 1} ^ {n} \pi_ {t - T} a _ {t} \tag {11.2.7}
$$

where $\rho _ { \quad \widehat { \bf \Phi } _ { \widehat { \bf \Phi } } } ^ { 2 } = \left( 1 + \pi _ { 1 } ^ { 2 } + \pi _ { 2 } ^ { 2 } + \cdots + \pi _ { n - T } ^ { 2 } \right) ^ { - 1 }$ , with the variance of the estimate being equal to $\rho ^ { 2 } \sigma ^ { 2 }$ . We can then define

$$
\lambda_ {2, T} = \frac {\tilde {\omega} _ {T , A}}{\rho \sigma} \tag {11.2.8}
$$

as the test statistic for testing the null hypothesis that the time series has no outliers versus the alternative hypothesis of an AO at T. As before, $\rho$ and $\sigma$ will need to be estimated. The test statistic $\lambda _ { 2 , T }$ is approximately distributed as $N ( 0 , 1 )$ under the null hypothesis. Again, $T$ is often unknown, and the test is applied repeatedly to each time point. The Bonferroni rule may again be applied to control the overall error rate. Furthermore, the nature of an outlier is not known beforehand. In the case where an outlier

is detected at $T ,$ , it may be classified to be an IO if $\left| \lambda _ { 1 , T } \right| > \left| \lambda _ { 2 , T } \right|$ and an AO otherwise. See Chang et al. (1988) for another approach to classifying the nature of an outlier. When an outlier is found, it can be incorporated into the model, and the outlier-detection procedure can then be repeated with the refined model until no more outliers are found.

As a first example, we simulated a time series of length $n = 1 0 0$ from the ARIMA(1,0,1) model with $\phi = 0 . 8$ and $\theta = - 0 . 5$ . We then changed the 10th observation from $- 2 . 1 3$ to 10 (that is, ${ \mathfrak { O } } _ { A } = 1 2 . 1 3 )$ ; see Exhibit 11.9. Based on the sample ACF, PACF and EACF, an AR(1) model was tentatively identified. Based on the Bonferroni rule, the 9th, 10th, and 11th observations were found to be possible additive outliers with the corresponding robustified test statistics being −3.54, 9.55, and $- 5 . 2 0$ . The test for IO revealed that the 10th and 11th observations may be IO, with the corresponding robustified test statistics being 7.11 and −6.64. Because among the tests for AO and IO the largest magnitude occurs for the test for AO at $T = 1 0$ , the 10th observation was tentatively marked as an AO. Note that the nonrobustified test statistic for AO at $T = 1 0$ equals 7.49, which is substantially less than the more robust test value of 9.55, showing that robustifying the estimate of the noise standard deviation does increase the power of the test. After incorporating the AO in the model, no more outliers were found. However, the lag 1 residual ACF was significant, suggesting the need for an MA(1) component. Hence, an $\mathrm { A R I M A } ( 1 , 0 , 1 ) + \mathrm { A O }$ at $T = 1 0$ model was fitted to the data. This model was found to have no additional outliers and passed all model diagnostic checks.

Exhibit 11.9 Simulated ARIMA(1,0,1) Process with an Additive Outlier   
![](images/dee1563fc76c292eeb22df9a7794f83f1990d5a4da9fc3a06a443a866bc9e6c8.jpg)  
> The extensive R code for the simulation and analysis of this example may be found in the R code script file for Chapter 11.

For a real example, we return to the seasonal ARIMA(0,1,1)×(0,1,1)12 model that we fitted to the carbon dioxide time series in Chapter 10. The time series plot of the standardized residuals from this model, shown in Exhibit 10.11 on page 238, showed a suspiciously large standardized residual in September 1998. Calculation shows that there is no evidence of an additive outlier, as $\lambda _ { 2 , t }$ is not significantly large for any t. However, the robustified $\lambda _ { 1 } = \operatorname* { m a x } _ { 1 \leq t \leq n } \lvert \lambda _ { 1 , t } \rvert = 3 . 7 5 2 7$ , which is attained at $t = 5 7$ , cor-

responding to September 1998. The Bonferroni critical value with $\mathrm { \Delta } \mathrm { a } = 5 \%$ and $n = 1 3 2$ is 3.5544. So our observed $\lambda _ { 1 }$ is large enough to claim significance for an innovation outlier in September 1998. Exhibit 11.10 shows the results of fitting the ARIMA(0,1,1) $\times ( 0 , 1 , 1 ) _ { 1 2 }$ model with an IO at $t = 5 7$ to the $\mathrm { C O } _ { 2 }$ time series. These results should be compared with the earlier results shown in Exhibit 10.10 on page 237, where the outlier was not taken into account. Notice that the estimates of θ and $\Theta$ have not changed very much, the AIC is better (that is, smaller), and the IO effect is highly significant. Diagnostics based on this model turn out to be excellent, no further outliers are detected, and we have a very adequate model for this seasonal time series.

<table><tr><td colspan="4">Exhibit 11.10 ARIMA(0,1,1)×(0,1,1)12 Model with IO at t = 57 for CO2 Series</td></tr><tr><td>Coefficient</td><td>θ</td><td>Θ</td><td>IO-57</td></tr><tr><td>Estimate</td><td>0.5925</td><td>0.8274</td><td>2.6770</td></tr><tr><td>Standard Error</td><td>0.0775</td><td>0.1016</td><td>0.7246</td></tr><tr><td colspan="4">Δ2e=0.4869: log-likelihood = -133.08, AIC = 272.16</td></tr><tr><td colspan="4">&gt; m1.co2=arima(co2,order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12)); m1.co2</td></tr><tr><td colspan="4">&gt; detectAO(m1.co2); detectIO(m1.co2)</td></tr><tr><td colspan="4">&gt; m4.co2=arimax(co2,order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12),io=c(57)); m4.co2</td></tr></table>

# 11.3 Spurious Correlation

A main purpose of building a time series model is for forecasting, and the ARIMA model does this by exploiting the autocorrelation pattern in the data. Often, the time series under study may be related to, or led by, some other covariate time series. For example, Stige et al. (2006) found that pasture production in Africa is generally related to some climatic indices. In such cases, better understanding of the underlying process and/or more accurate forecasts may be achieved by incorporating relevant covariates into the time series model.

Let $Y = \{ Y _ { t } \}$ be the time series of the response variable and $X = \ \{ X _ { t } \}$ be a covariate time series that we hope will help explain or forecast Y. To explore the correlation structure between $X$ and $Y$ and their lead-led relationship, we define the cross-covariance function $\gamma _ { t , s } ( X , Y ) = C o \nu ( X _ { t } , Y _ { s } )$ for each pair of integers $t$ and s. Stationarity of a univariate time series can be easily extended to the case of multivariate time series. For example, $X$ and $Y$ are jointly (weakly) stationary if their means are constant and the covariance $\boldsymbol { \gamma } _ { t , s } ( X , Y )$ is a function of the time difference $t - s$ . For jointly stationary processes, the cross-correlation function between $X$ and $Y$ at lag $k$ can then be defined by $\mathsf { \rho } _ { \mathsf { P } _ { k } } ( X , Y ) = C o r r ( X _ { t } , Y _ { t - k } ) = C o r r ( X _ { t + k } , Y _ { t } )$ . Note that if $Y = X$ , the cross-correlation becomes the autocorrelation of $Y$ at lag $k$ . The coefficient $\rho _ { 0 } ( Y , X )$ measures the contemporaneous linear association between $X$ and Y, whereas $\rho _ { k } ( X , Y )$ measures the linear association between $X _ { t }$ and that of $Y _ { t - k }$ . Recall that the autocorrelation function is an

# 11.3 Spurious Correlation

even function, that is, $\rho _ { k } ( Y , Y ) ~ = ~ \rho _ { - k } ( Y , Y )$ . (This is because $C o r r ( Y _ { t } , Y _ { t - k } ) =$ $C o r r ( Y _ { t - k } , Y _ { t } ) = C o r r ( Y _ { t } , Y _ { t + k } )$ , by stationarity.) However, the cross-correlation function is generally not an even function since $C o r r ( X _ { t } , Y _ { t - k } )$ need not equal $C o r r ( { X } _ { t } , { Y } _ { t + k } )$ .

As an illustration, consider the regression model

$$
Y _ {t} = \beta_ {0} + \beta_ {1} X _ {t - d} + e _ {t} \tag {11.3.1}
$$

where the X’s are independent, identically distributed random variables with variance $\sigma _ { X } ^ { 2 }$ and the $e$ ’s are also white noise with variance $\sigma _ { e } ^ { 2 }$ and are independent of the X’s. It can be checked that the cross-correlation function (CCF) $\rho _ { k } ( X , Y )$ is identically zero except for lag $k = - d$ , where

$$
\rho_ {- d} (X, Y) = \frac {\beta_ {1} \sigma_ {X}}{\sqrt {\beta_ {1} ^ {2} \sigma_ {X} ^ {2} + \sigma_ {e} ^ {2}}} \tag {11.3.2}
$$

In this case, the theoretical CCF is nonzero only at lag $- d$ , reflecting the fact that $X$ is “leading” Y by $d$ units of time. The CCF can be estimated by the sample cross-correlation function (sample CCF) defined by

$$
r _ {k} (X, Y) = \frac {\sum \left(X _ {t} - \bar {X}\right) \left(Y _ {t - k} - \bar {Y}\right)}{\sqrt {\sum \left(X _ {t} - \bar {X}\right) ^ {2}} \sqrt {\sum \left(Y _ {t} - \bar {Y}\right) ^ {2}}} \tag {11.3.3}
$$

where the summations are done over all data where the summands are available. The sample CCF becomes the sample ACF when $Y = X$ . The covariate $X$ is independent of $Y$ if and only if $\beta _ { 1 } = 0$ , in which case the sample autocorrelation $r _ { k } ( X , Y )$ is approximately normally distributed with zero mean and variance $1 / n$ , where $n$ is the sample size—the number of pairs of $( X _ { t } , Y _ { t } )$ available. Sample cross-correlations that are larger than $1 . 9 6 / { \sqrt { n } }$ in magnitude are then deemed significantly different from zero.

We have simulated 100 pairs of $( X _ { t } , Y _ { t } )$ from the model of Equation (11.3.1) with $d$ $= 2$ , $\beta _ { 0 } = 0$ , and $\beta _ { 1 } = 1$ . The X’s and $e$ ’s are generated as normal random variables distributed as $N ( 0 , 1 )$ and $N ( 0 , 0 . 2 5 )$ , respectively. Theoretically, the CCF should then be zero except at lag $^ { - 2 }$ , where it equals $\rho _ { - 2 } ( \dot { X } , Y ) = 1 / \sqrt { 1 + 0 . 2 5 } = 0 . 8 9 4 4$ . Exhibit 11.11 shows the sample CCF of the simulated data, which is significant at lags $^ { - 2 }$ and 3. But the sample CCF at lag 3 is quite small and only marginally significant. Such a false alarm is not unexpected as the exhibit displays a total of 33 sample CCF values out of which we may expect $3 3 \times 0 . 0 5 = 1 . 6 5$ false alarms on average.

![](images/46ebe392422e1987e18cdf5abca2d04e43d875909599bdd2edf8e537e2264f3a.jpg)  
Exhibit 11.11 Sample Cross-Correlation from Equation (11.3.1) with $\pmb { d } = \pmb { 2 }$

```javascript
> win.graph(width=4.875,height=2.5,pointsize=8)  
> set.seed(12345); X=rnorm(105); Y=zlag(X,2)+.5*rnorm(105)  
> X=ts(X[-(1:5)],start=1,freq=1); Y=ts(Y[-(1:5)],start=1,freq=1)  
> ccf(X,Y,ylab='CCF') 
```

Even though $X _ { t - 2 }$ correlates with $Y _ { t }$ , the regression model considered above is rather restrictive, as $X$ and $Y$ are each white noise series. For stationary time series, the response variable and the covariate are each generally autocorrelated, and the error term of the regression model is also generally autocorrelated. Hence a more useful regression model is given by

$$
Y _ {t} = \beta_ {0} + \beta_ {1} X _ {t - d} + Z _ {t} \tag {11.3.4}
$$

where $Z _ { t }$ may follow some ARIMA $( p , d , q )$ model. Even if the processes $X$ and Y are independent of each other $( \beta _ { 1 } = 0$ ), the autocorrelations in Y and $X$ have the unfortunate consequence of implying that the sample CCF is no longer approximately $N ( 0 , 1 / n )$ . Under the assumption that both $X$ and Y are stationary and that they are independent of each other, it turns out that the sample variance tends to be different from $1 / n$ . Indeed, it may be shown that the variance of $\sqrt { n } r _ { k } ( X , Y )$ is approximately

$$
1 + 2 \sum_ {k = 1} ^ {\infty} \rho_ {k} (X) \rho_ {k} (Y) \tag {11.3.5}
$$

where $\rho _ { k } ( X )$ is the autocorrelation of $X$ at lag $k$ and $\rho _ { k } ( Y )$ is similarly defined for the Y-process. For refinement of this asymptotic result, see Box et al. (1994, p. 413). Suppose $X$ and $Y$ are both AR(1) processes with AR(1) coefficients $\phi _ { X }$ and $\phi _ { Y }$ respectively. Then $r _ { k } ( X , Y )$ is approximately normally distributed with zero mean, but the variance is now approximately equal to

$$
\frac {1 + \phi_ {X} \phi_ {Y}}{n \left(1 - \phi_ {X} \phi_ {Y}\right)} \tag {11.3.6}
$$

When both AR(1) coefficients are close to 1, the ratio of the sampling variance of $r _ { k } ( X , Y )$ to the nominal value of $1 / n$ approaches infinity. Thus, the unquestioned use of the $1 / n$ rule in deciding the significance of the sample CCF may lead to many more false positives than the nominal $5 \%$ error rate, even though the response and covariate time series are independent of each other. Exhibit 11.12 shows some numerical results for the case where $\Phi _ { X } = \Phi _ { Y } = \Phi$ .

Exhibit 11.12 Asymptotic Error Rates of a Nominal $5 \%$ Test of Independence for a Pair of AR(1) Processes   

<table><tr><td>φ = φX = φY</td><td>0.00</td><td>0.15</td><td>0.30</td><td>0.45</td><td>0.60</td><td>0.75</td><td>0.90</td></tr><tr><td>Error Rate</td><td>5%</td><td>6%</td><td>7%</td><td>11%</td><td>18%</td><td>30%</td><td>53%</td></tr><tr><td colspan="8">&gt; phi=seq(0, .95, .15)</td></tr><tr><td colspan="8">&gt; rejection=2* (1-pnorm(1.96*sqrt((1-phi^2)/(1+phi^2))))</td></tr><tr><td colspan="8">&gt; M significif (rbind(phi,rejection), 2)</td></tr><tr><td colspan="8">&gt; rnames(M) =c(&#x27;phi&#x27;, &#x27;Error Rate&#x27;)</td></tr><tr><td colspan="8">&gt; M</td></tr></table>

The problem of inflated variance of the sample cross-correlation coefficients becomes more acute for nonstationary data. In fact, the sample cross-correlation coefficients may no longer be approximately normally distributed even with a large sample size. Exhibit 11.13 displays the histogram of 1000 simulated lag zero cross-correlations between two independent IMA(1,1) processes each of size 500. An MA(1) coefficient of $\theta = 0 . 8$ was used for both simulated processes. Note that the distribution of $r _ { 0 } ( X , Y )$ is far from normal and widely dispersed between −1 and 1. See Phillips (1998) for a relevant theoretical discussion.

![](images/7b4687e56cde8f6bd6b6889f2fb4f5f14d5916c4e2d515f3d18c2eb115273a98.jpg)  
Exhibit 11.13 Histogram of 1000 Sample Lag Zero Cross-Correlations of Two Independent IMA(1,1) Processes Each of Size 500

```txt
> set.seed(23457)
> correlation.v=NULL; B=1000; n=500
> for (i in 1:B) {x=cumsum(arima.sim(model=list(ma=.8),n=n))
> y=cumsum(arima.sim(model=list(ma=.8),n=n))
> correlation.v=c(correlation.v, ccf(x,y,lag.max=1,
plot=F) $acf[2])}
> hist(correlation.v, prob=T, xlab=expression(r[0](X,Y))) 
```

These results provide insight into why we sometimes obtain nonsense (spurious) correlation between time series variables. The phenomenon of spurious correlation was first studied systematically by Yule (1926).

As an example, the monthly milk production and the logarithms of monthly electricity production in the United States from January 1994 to December 2005 are shown in Exhibit 11.14. Both series have an upward trend and are highly seasonal.

![](images/827a8729b81234bbf4bf25a389b44f26ea589597fb99be01e322f3ba15a6c371.jpg)  
Exhibit 11.14 Monthly Milk Production and Logarithms of Monthly Electricity Production in the U.S.

```txt
> data(milk); data(electricity)  
> milk.electricity=ts.intersect(milk, log(electricity))  
> plot(milk.electricity, yax.flip=T) 
```

Calculation shows that these series have a cross-correlation coefficient at lag zero of 0.54, which is “statistically significantly different from zero” as judged against the standard error criterion of $1 . { \dot { 9 } } 6 { \dot { / } } { \sqrt { n } } \ = \ 0 . { \dot { 1 } } 6$ . Exhibit 11.15 displays the strong crosscorrelations between these two variables at a large number of lags.

Needless to say, it is difficult to come up with a plausible reason for the relationship between monthly electricity production and monthly milk production. The nonstationarity in the milk production series and in the electricity series is more likely the cause of the spurious correlations found between the two series. The following section contains further discussion of this example.

Exhibit 11.15 Sample Cross-Correlation Between Monthly Milk Production and Logarithm of Monthly Electricity Production in the U.S.   
![](images/93eb3daa841c7268c3b2e981ff14f89a1f56bc1c82f45929a5a3c987d5ed1b96.jpg)  
> ccf(as.vector(milk.electricity[,1]), as.vector(milk.electricity[,2]),ylab='CCF')

# 11.4 Prewhitening and Stochastic Regression

In the preceding section, we found that with strongly autocorrelated data it is difficult to assess the dependence between the two processes. Thus, it is pertinent to disentangle the linear association between $X$ and Y, say, from their autocorrelation. A useful device for doing this is prewhitening. Recall that, for the case of stationary $X$ and $Y$ that are independent of each other, the variance of $r _ { k } ( X , Y )$ is approximately

$$
\frac {1}{n} \left[ 1 + 2 \sum_ {k = 1} ^ {\infty} \rho_ {k} (X) \rho_ {k} (Y) \right] \tag {11.4.1}
$$

An examination of this formula reveals that the approximate variance is $1 / n$ if either one (or both) of $X$ or Y is a white noise process. In practice, the data may be nonstationary, but they may be transformed to approximately white noise by replacing the data by the residuals from a fitted ARIMA model. For example, if $X$ follows an ARIMA(1,1,0) model with no intercept term, then

$$
\tilde {X} _ {t} = X _ {t} - X _ {t - 1} - \phi \left(X _ {t - 1} - X _ {t - 2}\right) = 1 - (1 + \phi B) + \phi B ^ {2} ] X _ {t} \tag {11.4.2}
$$

is white noise. More generally, if $X _ { t }$ follows some invertible ARIMA $. ( p , d , q )$ model, then it admits an $\mathbf { A R } ( \infty )$ representation

$$
\tilde {X} _ {t} = (1 - \pi_ {1} B - \pi_ {2} B ^ {2} - \dots) X _ {t} = \pi (B) X _ {t}
$$

where the $\tilde { X }$ ’s are white noise. The process of transforming the X’s to the $\tilde { X }$ ’s via the filter $\pi ( B ) = 1 - \pi _ { 1 } B - \pi _ { 2 } B ^ { 2 } - \cdots$ is known as whitening or prewhitening. We now can

study the CCF between $X$ and Y by prewhitening the $Y$ and $X$ using the same filter based on the $X$ process and then computing the CCF of $Y$ and $X$ ; that is, the prewhitened Y and $X$ . Since prewhitening is a linear operation, any linear relationships between the original series will be preserved after prewhitening. Note that we have abused the terminology, as $Y$ need not be white noise because the filter $\pi ( B )$ is tailor-made only to transform $X$ to a white noise process—not Y. We assume, furthermore, that $\tilde { Y }$ is stationary. This approach has two advantages: (i) the statistical significance of the sample CCF of the prewhitened data can be assessed using the cutoff $\bar { 1 } . 9 6 / \sqrt { n }$ , and (ii) the theoretical counterpart of the CCF so estimated is proportional to certain regression coefficients.

To see (ii), consider a more general regression model relating $X$ to Y and, without loss of generality, assume both processes have zero mean:

$$
Y _ {t} = \sum_ {j = - \infty} ^ {\infty} \beta_ {j} X _ {t - j} + Z _ {t} \tag {11.4.3}
$$

where $X$ is independent of $Z$ and the coefficients $\beta$ are such that the process is well-defined. In this model, the coefficients $\beta _ { k }$ could be nonzero for any integer $k$ . However, in real applications, the doubly infinite sum is often a finite sum so that the model simplifies to

$$
Y _ {t} = \sum_ {j = m _ {1}} ^ {m _ {2}} \beta_ {j} X _ {t - j} + Z _ {t}, \tag {11.4.4}
$$

which will be assumed below even though we retain the doubly infinite summation notation for ease of exposition. If the summation ranges only over a finite set of positive indices, then $X$ leads Y and the covariate $X$ serves as a useful leading indicator for future Y’s. Applying the filter $\pi ( B )$ to both sides of this model, we get

$$
\tilde {Y} _ {t} = \sum_ {k = - \infty} ^ {\infty} \beta_ {k} \tilde {X} _ {t - k} + \tilde {Z} _ {t} \tag {11.4.5}
$$

where $\tilde { Z } _ { t } = Z _ { t } - \pi _ { 1 } Z _ { t - 1 } - \pi _ { 2 } Z _ { t - 2 } - \cdots$ .The prewhitening procedure thus orthogonalizes the various lags of $X$ in the original regression model. Because $\tilde { X }$ is a white noise sequence and $\tilde { \boldsymbol { X } }$ is independent of $\tilde { Z }$ , the theoretical cross-correlation coefficient between $\tilde { X }$ and $\tilde { Y }$ at lag $k$ equals $\beta _ { - k } ( \sigma _ { \tilde { X } } / \sigma _ { \tilde { Y } } )$ . In other words, the theoretical crosscorrelation of the prewhitened processes at lag $k$ is proportional to the regression coefficient $\beta _ { - k }$ .

For a quick preliminary analysis, an approximate prewhitening can be done easily by first differencing the data (if needed) and then fitting an approximate AR model with the order determined by minimizing the AIC. For example, for the milk production and electricity consumption data, both are highly seasonal and contain trends. Consequently, they can be differenced with both regular differencing and seasonal differencing, and then the prewhitening can be carried out by filtering both differenced series by an AR model fitted to the differenced milk data. Exhibit 11.16 shows the sample CCF between the prewhitened series. None of the cross-correlations are now significant except for lag $^ { - 3 }$ , which is just marginally significant. The lone significant cross-correlation is likely a false alarm since we expect about 1.75 false alarms out of the 35 sample cross-correla-

tions examined. Thus, it seems that milk production and electricity consumption are in fact largely uncorrelated, and the strong cross-correlation pattern found between the raw data series is indeed spurious.

![](images/c22a97f3d6c3669cd74e42ae9106de8cab760bd5214bfca2e3c230d1974d3f5a.jpg)  
Exhibit 11.16 Sample CCF of Prewhitened Milk and Electricity Production

>me.dif $\equiv$ ts.intersect(diff(diff(milk,12)), diff(diff(log(electricity),12)))   
>prewhiten(as.vector(me.dif[,1]),as.vector(me.dif[,2]), ylab $=$ 'CCF')

The model defined by Equation (11.3.4) on page 262 is known variously as the transfer-function model, the distributed-lag model, or the dynamic regression model. The specification of which lags of the covariate enter into the model is often done by inspecting the sample cross-correlation function based on the prewhitened data. When the model appears to require a fair number of lags of the covariate, the regression coefficients may be parsimoniously specified via an ARMA specification similar to the case of intervention analysis; see Box et al. (1994, Chapter 11) for some details. We illustrate the method below with two examples where only one lag of the covariate appears to be needed. The specification of the stochastic noise process $Z _ { t }$ can be done by examining the residuals from an ordinary least squares (OLS) fit of $Y$ on $X$ using the techniques learned in earlier chapters.

Our first example of this section is a sales and price dataset of a certain potato chip from Bluebird Foods Ltd., New Zealand. The data consist of the log-transformed weekly unit sales of large packages of standard potato chips sold and the weekly average price over a period of 104 weeks from September 20, 1998 through September 10, 2000; see Exhibit 11.17. The logarithmic transformation is needed because the sales data are highly skewed to the right. These data are clearly nonstationary. Exhibit 11.18 shows that, after differencing and using prewhitened data, the CCF is significant only at lag 0, suggesting a strong contemporaneous negative relationship between lag 1 of price and sales. Higher prices are associated with lower sales.

![](images/4ff13751c4238c254303fbb4c480c5a4c68676308515a43012aac38018a6950d.jpg)  
Exhibit 11.17 Weekly Log(Sales) and Price for Bluebird Potato Chips

```txt
> data(bluebird)  
> plot(bluebird, yax.flip = T) 
```

![](images/e92be1093eb75ec972aea4421fa474f39b644668acc85f9069f1b1ea392c4bf0.jpg)  
Exhibit 11.18 Sample Cross Correlation Between Prewhitened Differenced Log(Sales) and Price of Bluebird Potato Chips

```javascript
>prewhiten(y=diff(bluebird)[,1],x=diff(bluebird)[,2],ylab='CCF') 
```

Exhibit 11.19 reports the estimates from the OLS regression of log(sales) on price. The residuals are, however, autocorrelated, as can be seen from their sample ACF and PACF displayed in Exhibits 11.20 and 11.21, respectively. Indeed, the sample autocorrelations of the residuals are significant for the first four lags, whereas the sample partial autocorrelations are significant at lags 1, 2, 4, and 14.

Exhibit 11.19 OLS Regression Estimates of Log(Sales) on Price   

<table><tr><td></td><td>Estimate</td><td>Std. Error</td><td>t value</td><td>Pr(&gt;)</td></tr><tr><td>Intercept</td><td>15.90</td><td>0.2170</td><td>73.22</td><td>&lt; 0.0001</td></tr><tr><td>Price</td><td>-2.489</td><td>0.1260</td><td>-19.75</td><td>&lt; 0.0001</td></tr><tr><td colspan="5">&gt; sales=bluebird[,1]; price=bluebird[,2]</td></tr><tr><td colspan="5">&gt; chip.m1=lm(sales~price,data=bluebird)</td></tr><tr><td colspan="5">&gt; summary(chip.m1)</td></tr></table>

Exhibit 11.20 Sample ACF of Residuals from OLS Regression of Log(Sales) on Price   
![](images/cad1b2fa52eeafab403fe7fdf6c15f98920f54244c20fffc03c08f85823171e6.jpg)  
> acf(residuals(chip.m1),ci.type='ma')

Exhibit 11.21 Sample PACF of Residuals from OLS Regression of Log(Sales) on Price   
![](images/a9639c1fb1b09cbc1252e7e6d012e58f05a4869e18410c47ef742c5fb7b5a96d.jpg)  
> pacf(residuals(chip.m1))

The sample EACF of the residuals, shown in Exhibit 11.22, contains a triangle of zeros with a vertex at (1,4), thereby suggesting an ARMA(1,4) model. Hence, we fit a regression model of log(sales) on price with an ARMA(1,4) error.

Exhibit 11.22 The Sample EACF of the Residuals from the OLS Regression of Log(Sales) on Price   

<table><tr><td>AR/MA</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td></tr><tr><td>0</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>x</td><td>0</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>x</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>x</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>4</td><td>0</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>5</td><td>x</td><td>x</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>6</td><td>x</td><td>x</td><td>0</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>7</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></table>

> eacf(residuals(chip.m1))

It turns out that the estimates of the AR(1) coefficient and the MA coefficients $\theta _ { 1 }$ and $\theta _ { 3 }$ are not significant, and hence a model fixing these coefficients to be zero was subsequently fitted and reported in Exhibit 11.23.

Exhibit 11.23 Maximum Likelihood Estimates of a Regression Model of Log(sales) on Price with a Subset MA(4) for the Errors   

<table><tr><td>Parameter</td><td>θ1</td><td>θ2</td><td>θ3</td><td>θ4</td><td>Intercept</td><td>Price</td></tr><tr><td>Estimate</td><td>0</td><td>-0.2884</td><td>0</td><td>-0.5416</td><td>15.86</td><td>-2.468</td></tr><tr><td>Standard Error</td><td>0</td><td>0.0794</td><td>0</td><td>0 0.1167</td><td>0.1909</td><td>0.1100</td></tr></table>

$\sigma ^ { 2 }$ estimated as 0.02623: log likelihood = 41.02, AIC = −70.05   
> chip.m2=arima(sales,order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (1,0,4),xreg $\mid =$ data.frame(price))   
> chip.m2   
$>$ chip.m3=arima(sales,order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (1,0,4),xreg $: =$ data.frame(price), fixed $\boldsymbol { \mathbf { \mathit { \Psi } } } = \boldsymbol { \mathbf { \mathit { C } } }$ (NA,0,NA,0,NA,NA,NA)); chip.m3   
> chip. $\mathfrak { m } 4 =$ arima(sales,order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (0,0,4),xreg $: =$ data.frame(price), fixed ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (0,NA,0,NA,NA,NA)); chip.m4

Note that the regression coefficient estimate on Price is similar to that from the OLS regression fit earlier, but the standard error of the estimate is about $10 \%$ lower than that from the simple OLS regression. This illustrates the general result that the simple OLS estimator is consistent but the associated standard error is generally not trustworthy.

The residuals from this fitted model by and large pass various model diagnostic tests except that the residual ACF is significant at lag 14. As a result, some Box-Ljung test statistics have $p$ -values bordering on 0.05 when 14 or more lags of the residual autocorrelations are included in the test. Even though the significant ACF at lag 14 may suggest a quarterly effect, we do not report a more complex model including lag 14 because (1) 14 weeks do not exactly make a quarter and (2) adding a seasonal MA(1) component of period 14 only results in marginal improvement in terms of model diagnostics.

For a second example, we study the impact of higher gasoline price on public transportation usage. The dataset consists of the monthly number of boardings on public transportation in the Denver, Colorado, region together with the average monthly gasoline prices in Denver from August 2000 through March 2006. Both variables are skewed to the right and hence are log-transformed. As we shall see below, the logarithmic transformation also makes the final fitted model more interpretable. The time series plots, shown in Exhibit 11.24, display the increasing trends for both variables and the seasonal fluctuation in the number of boardings. Based on the sample ACF and PACF, an ARIMA(2,1,0) model was fitted to the gasoline price data. This fitted model was then used to filter the boardings data before computing their sample CCF which is shown in Exhibit 11.25. The sample CCF is significant at lags 0 and 15, suggesting positive contemporaneous correlation between gasoline price and public transportation usage. The significant CCF at lag 15, however, is unlikely to be real, as it is hard to imagine why the number of boardings might lead the gasoline price with a lag of 15 months. In this case, the quick preliminary approach of prewhitening the series by fitting a long AR model, however, showed that none of the CCFs are significant. It turns out that even after differencing the data, the AIC selects an AR(16) model. The higher order selected coupled with the relatively short time span may substantially weaken the power to detect correlations between the two variables. Incidentally, this example warns against simply relying on the AIC to select a high-order AR model to do prewhitening, especially with relatively short time series data.

![](images/74a2651bb0369bd991c760f7508a2c4613ad0797b5f94296ebdb58a94e34f4f5.jpg)  
Exhibit 11.24 Logarithms of Monthly Public Transit Boardings and Gasoline Prices in Denver, August 2000 through March 2006

> data(boardings)   
> plot(boardings,yax.flip=T)

![](images/748b70c51d29d3da0f308f4a0adda39d7ca83e5783b25fbb63de2b8ca578ae43.jpg)  
Exhibit 11.25 Sample CCF of Prewhitened Log(Boardings) and Log(Price)

> m1=arima(boardings[,2],order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (2,1,0)) > prewhiten(x=boardings[,2],y=boardings[,1],x.model=m1)

Based on the sample ACF, PACF, and EACF of the residuals from a linear model of boardings on gasoline price, a seasonal $\mathrm { A R I M A } ( 2 , 0 , 0 ) { \times } ( 1 , 0 , 0 ) _ { 1 2 }$ model was tentatively specified for the error process in the regression model. However, the $\Phi _ { 2 }$ coefficient estimate was not significant, and hence the AR order was reduced to $p = 1$ . Using the outlier detection techniques discussed in Section 11.2, we found an additive outlier for March 2003 and an innovative outlier for March 2004. Because the test statistic for the additive outlier had a larger magnitude than that of the innovative outlier (−4.09 vs. 3.65), we incorporated the additive outlier in the model.† Diagnostics of the subsequent fitted model reveals that the residual ACF was significant at lag 3, which suggests the error process is a seasonal $\mathsf { A R I M A } ( 1 , 0 , 3 ) \times ( 1 , 0 , 0 ) _ { 1 2 } +$ outlier process. As the estimates of the coefficients $\theta _ { 1 }$ and $\theta _ { 2 }$ were found to be insignificant, they were suppressed from the final fitted model that is reported in Exhibit 11.26.

Diagnostics of the final fitted model suggest a good fit to the data. Also, no further outliers were detected. A $9 5 \%$ confidence interval for the regression coefficient on Log(Price) is (0.0249, 0.139). Note the interpretation of the fitted model: a $100 \%$ increase in the price of gasoline will lead to about an $8 . 2 \%$ increase in public transportation usage.

Exhibit 11.26 Maximum Likelihood Estimates of the Regression Model of Log(Boardings) on Log(Price) with ARMA Errors   

<table><tr><td>Parameter</td><td>φ1</td><td>θ3</td><td>Φ1</td><td>Intercept</td><td>Log(Price)</td><td>Outlier</td></tr><tr><td>Estimate</td><td>0.8782</td><td>0.3836</td><td>0.8987</td><td>12.12</td><td>0.0819</td><td>-0.0643</td></tr><tr><td>Standard Error</td><td>0.0645</td><td>0.1475</td><td>0.0395</td><td>0.1638</td><td>0.0291</td><td>0.0109</td></tr><tr><td colspan="7">σ2estimated as 0.0004094: log-likelihood = 158.02, AIC = -304.05</td></tr></table>

```txt
> log.boards = boardings[,1]
> log.price = boardings[,2]
> boardings.m1 = arima(log.boards, order=c(1,0,0),
		seasonal=list(order=c(1,0,0),period=12),
		(xreg=data.frame(log.price))
> boardings.m1
> detectAO(boardings.m1); detectIO(boardings.m1)
> boardings.m2 = arima(log.boards, order=c(1,0,3),
		seasonal=list(order=c(1,0,0),period=12),
		(xreg=data.frame(log.price,outlier=c(rep(0,31),1,rep(0,36))), 
		fixed=c(NA,0,0,rep(NA,5)))
> boardings.m2
> detectAO(boardings.m2); detectIO(boardings.m2)
> tsdiag(boardings.m2,tol=.15,gof.lag=24) 
```

It is also of interest to note that dropping the outlier term from the model results in a new regression estimate on Log(Price) of 0.0619 with a standard error of 0.0372. Thus, when the outlier is not properly modeled, the regression coefficient ceases to be significant at the $5 \%$ level. As demonstrated by this example, the presence of an outlier can adversely affect inference in time series modeling.

# 11.5 Summary

In this chapter, we used information from other events or other time series to help model the time series of main interest. We began with the so-called intervention models, which attempt to incorporate known external events that we believe have a significant effect on the time series of interest. Various simple but useful ways of modeling the effects of these interventions were discussed. Outliers are observations that deviate rather substantially from the general pattern of the data. Models were developed to detect and incorporate outliers in time series. The material in the section on spurious correlation illustrates how difficult it is to assess relationships between two time series, but methods involving prewhitening were shown to help in this regard. Several substantial examples were used to illustrate the methods and techniques discussed.

# EXERCISES

11.1 Produce a time series plot of the air passenger miles over the period January 1996 through May 2005 using seasonal plotting symbols. Display the graph full-screen and discuss the seasonality that is displayed. The data are in the file named airmiles.   
11.2 Show that the expression given for $m _ { t }$ in Equation (11.1.7) on page 251 satisfies the “AR(1)” recursion given in Equation (11.1.6) with the initial condition $m _ { 0 } = 0$ .   
11.3 Find the “half-life” for the intervention effect specified in Equation (11.1.6) on page 251 when $\delta = 0 . 7$ .   
11.4 Show that the “half-life” for the intervention effect specified in Equation (11.1.6) on page 251 increases without bound as δ increases to 1.   
11.5 Show that for the intervention effect specified by Equation (11.1.6) on page 251

$$
\lim  _ {\delta \to 1} m _ {t} = \left\{ \begin{array}{l} \omega (T - t), \text {f o r} t \geq T \\ 0, \text {o t h e r w i s e} \end{array} \right.
$$

11.6 Consider the intervention effect displayed in Exhibit 11.3, (b), page 253.

(a) Show that the jump at time $T + 1$ is of height ω as displayed.   
(b) Show that, as displayed, the intervention effect tends to $\Theta / ( 1 ~ - ~ \delta )$ as t increases without bound.

11.7 Consider the intervention effect displayed in Exhibit 11.3, (c), page 253. Show that the effect increases linearly starting at time $T + 1$ with slope ω as displayed.   
11.8 Consider the intervention effect displayed in Exhibit 11.4, (a), page 254.

(a) Show that the jump at time $T + 1$ is of height ω as displayed.   
(b) Show that, as displayed, the intervention effect tends to go back to 0 as t increases without bound.

11.9 Consider the intervention effect displayed in Exhibit 11.4, (b), page 254.

(a) Show that the jump at time $T + 1$ is of height ${ \mathfrak { o } } _ { 1 } + { \mathfrak { o } } _ { 2 }$ as displayed.   
(b) Show that, as displayed, the intervention effect tends to $\omega _ { 2 }$ as $t$ increases without bound.

11.10 Consider the intervention effect displayed in Exhibit 11.4, (c), page 254.

(a) Show that the jump at time $T$ is of height $\mathfrak { o } _ { 0 }$ as displayed.   
(a) Show that the jump at time $T + 1$ is of height ${ \mathfrak { o } } _ { 1 } + { \mathfrak { o } } _ { 2 }$ as displayed.   
(b) Show that, as displayed, the intervention effect tends to $\omega _ { 2 }$ as $t$ increases without bound.

11.11 Simulate 100 pairs of $( X _ { t } , Y _ { t } )$ from the model of Equation (11.3.1) on page 261 with $d = 3$ , $\beta _ { 0 } = 0$ , and $\beta _ { 1 } = 1$ . Use $\sigma _ { X } = 2$ and $\sigma _ { e } = 1$ . Display and interpret the sample CCF between these two series.   
11.12 Show that when the $X$ and $Y$ are independent AR(1) time series with parameters $\phi _ { X }$ and $\phi _ { Y }$ respectively, Equation (11.3.5) on page 262 reduces to give Equation (11.3.6).   
11.13 Show that for the process defined by Equation (11.4.5) on page 266, the cross-correlation between $\tilde { \boldsymbol { X } }$ and $\tilde { Y }$ at lag $k$ is given by $\beta _ { - k } ( \sigma _ { \tilde { X } } / \sigma _ { \tilde { Y } } )$ .

11.14 Simulate an AR time series with $\phi = 0 . 7$ , $\mu = 0$ , $\sigma _ { e } = 1$ , and of length $n = 4 8$ . Plot the time series, and inspect the sample ACF and PACF of the series.

(a) Now add a step function response of $\omega = 1$ unit height at time $t = 3 6$ to the simulated series. The series now has a theoretical mean of zero from $t = 1$ to 35 and a mean of 1 from $t = 3 6$ on. Plot the new time series and calculate the sample ACF and PACF for the new series. Compare these with the results for the original series.   
(b) Repeat part (a) but with an impulse response at time $t = 3 6$ of unit height, $\omega =$ 1. Plot the new time series, and calculate the sample ACF and PACF for the new series. Compare these with the results for the original series. See if you can detect the additive outlier at time $t = 3 6$ assuming that you do not know where the outlier might occur.

11.15 Consider the air passenger miles time series discussed in this chapter. The file is named airmiles. Use only the preintervention data (that is, data prior to September 2001) for this exercise.

(a) Verify that the sample ACF for the twice differenced series of the logarithms of the preintervention data is as shown in Exhibit 11.5 on page 255.   
(b) The plot created in part (a) suggests an $\mathrm { A R I M A } ( 0 , 1 , 1 ) \times ( 0 , 1 , 0 ) _ { 1 2 }$ $\left. \vert \times ( 0 , 1 , 0 ) _ { 1 2 } \right.$ . Fit this model and assess its adequacy. In particular, verify that additive outliers are detected in December 1996, January 1997, and December 2002.   
(c) Now fit an ARIMA(0,1,1)×(0,1,0)12 + three outliers model and assess its adequacy.   
(d) Finally, fit an ARIMA(0,1,1)×(0,1,1)12 + three outliers model and assess its adequacy.

11.16 Use the logarithms of the Denver region public transportation boardings and Denver gasoline price series. The data are in the file named boardings.

(a) Display the time series plot of the monthly boardings using seasonal plotting symbols. Interpret the plot.   
(b) Display the time series plot of the monthly average gasoline prices using seasonal plotting symbols. Interpret the plot.

11.17 The data file named deere1 contains 82 consecutive values for the amount of deviation (in 0.000025 inch units) from a specified target value that an industrial machining process at Deere & Co. produced under certain specified operating conditions. These data were first used in Exercise 6.33, page 146, where we observed an obvious outlier at time $t = 2 7$ .

(a) Fit an AR(2) model using the original data including the outlier.   
(b) Test the fitted AR(2) model of part (a) for both AO and IO outliers.   
(c) Now fit the AR(2) model incorporating a term in the model for the outlier.   
(d) Assess the fit of the model in part (c) using all of our diagnostic tools. In particular, compare the properties of this model with the one obtained in part (a).

11.18 The data file named days contains accounting data from the Winegard Co. of Burlington, Iowa. The data are the number of days until Winegard receives payment for 130 consecutive orders from a particular distributor of Winegard products. (The name of the distributor must remain anonymous for confidentiality reasons.) These data were first investigated in Exercise 6.39, page 147, but several outliers were observed. When the observed outliers were replaced by more typical values, an MA(2) model was suggested.

(a) Fit an MA(2) model to the original data, and test the fitted model for both AO and IO outliers.   
(b) Now fit the MA(2) model incorporating the outliers into the model.   
(c) Assess the fit of the model obtained in part (b). In particular, are any more outliers indicated?   
(d) Fit another MA(2) model incorporating any additional outliers found in part (c), and assess the fit of this model.

11.19 The data file named bluebirdlite contains weekly sales and price data for Bluebird Lite potato chips. Carry out an analysis similar to that for Bluebird Standard potato chips that was begun on page 267.

11.20 The file named units contains annual unit sales of a certain product from a widely known international company over the years 1983 through 2005. (The name of the company must remain anonymous for proprietary reasons.)

(a) Plot the time series of units and describe the general features of the plot.   
(b) Use ordinary least squares regression to fit a straight line in time to the series.   
(c) Display the sample PACF of the residuals from this model, and specify an ARIMA model for the residuals.   
(d) Now fit the model unit sales $= { \mathrm { A R } } ( 2 ) +$ $=$ time. Interpret the output. In particular, compare the estimated regression coefficient on the time variable obtained here with the one you obtained in part (b).   
(e) Perform a thorough analysis of the residuals from this last model.   
(f) Repeat parts (d) and (e) using the logarithms of unit sales as the response variable. Compare these results witjh those obtained in parts (d) and (e).

11.21 In Chapters 5–8, we investigated an IMA(1,1) model for the logarithms of monthly oil prices. Exhibit 8.3 on page 178 suggested that there may be several outliers in this series. Investigate the IMA(1,1) model for this series for outliers using the techniques developed in this chapter. Be sure to compare your results with those obtained earlier that ignored the outliers. The data are in the file named oil.

# CHAPTER 12

# TIME SERIES MODELS OF HETEROSCEDASTICITY

The models discussed so far concern the conditional mean structure of time series data. However, more recently, there has been much work on modeling the conditional variance structure of time series data—mainly motivated by the needs for financial modeling. Let $\{ Y _ { t } \}$ be a time series of interest. The conditional variance of $Y _ { t }$ given the past $Y$ values, $Y _ { t - 1 } , Y _ { t - 2 } , . . .$ , measures the uncertainty in the deviation of $Y _ { t }$ from its conditional mean $E ( Y _ { t } | Y _ { t - 1 } , Y _ { t - 2 } , . . . )$ . If $\{ Y _ { t } \}$ follows some ARIMA model, the (one-stepahead) conditional variance is always equal to the noise variance for any present and past values of the process. Indeed, the constancy of the conditional variance is true for predictions of any fixed number of steps ahead for an ARIMA process. In practice, the (one-step-ahead) conditional variance may vary with the current and past values of the process, and, as such, the conditional variance is itself a random process, often referred to as the conditional variance process. For example, daily returns of stocks are often observed to have larger conditional variance following a period of violent price movement than a relatively stable period. The development of models for the conditional variance process with which we can predict the variability of future values based on current and past data is the main concern of the present chapter. In contrast, the ARIMA models studied in earlier chapters focus on how to predict the conditional mean of future values based on current and past data.

In finance, the conditional variance of the return of a financial asset is often adopted as a measure of the risk of the asset. This is a key component in the mathematical theory of pricing a financial asset and the VaR (Value at Risk) calculations; see, for example, Tsay (2005). In an efficient market, the expected return (conditional mean) should be zero, and hence the return series should be white noise. Such series have the simplest autocorrelation structure. Thus, for ease of exposition, we shall assume in the first few sections of this chapter that the data are returns of some financial asset and are white noise; that is, serially uncorrelated data. By doing so, we can concentrate initially on studying how to model the conditional variance structure of a time series. By the end of the chapter, we discuss some simple schemes for simultaneously modeling the conditional mean and conditional variance structure by combining an ARIMA model with a model of conditional heteroscedasticity.

# 12.1 Some Common Features of Financial Time Series

As an example of financial time series, we consider the daily values of a unit of the CREF stock fund over the period from August 26, 2004 to August 15, 2006. The CREF stock fund is a fund of several thousand stocks and is not openly traded in the stock market.† Since stocks are not traded over weekends or on holidays, only on so-called trading days, the CREF data do not change over weekends and holidays. For simplicity, we will analyze the data as if they were equally spaced. Exhibit 12.1 shows the time series plot of the CREF data. It shows a generally increasing trend with a hint of higher variability with higher level of the stock value. Let $\{ p _ { t } \}$ be the time series of, say, the daily price of some financial asset. The (continuously compounded) return on the tth day is defined as

$$
r _ {t} = \log \left(p _ {t}\right) - \log \left(p _ {t - 1}\right) \tag {12.1.1}
$$

Sometimes the returns are then multiplied by 100 so that they can be interpreted as percentage changes in the price. The multiplication may also reduce numerical errors as the raw returns could be very small numbers and render large rounding errors in some calculations.

Exhibit 12.1 Daily CREF Stock Values: August 26, 2004 to August 15, 2006   
![](images/94c119a1d1a954de781a413287f3ca72fe433d88afe6069096808385a640f1c4.jpg)  
> win.graph(width=4.875,height=2.5,pointsize=8)   
> data(CREF); plot(CREF)

Exhibit 12.2 plots the CREF return series (sample size $= 5 0 0$ ). The plot shows that the returns were more volatile over some time periods and became very volatile toward the end of the study period. This observation may be more clearly seen by plotting the time sequence plot of the absolute or squared returns; see Exercise 12.1, page 316.

These results might be triggered by the instability in the Middle East due to a war in southern Lebanon from July 12 to August 14, 2006, the period that is shaded in gray in Exhibits 12.1 and 12.2. This pattern of alternating quiet and volatile periods of substantial duration is referred to as volatility clustering in the literature. Volatility in a time series refers to the phenomenon where the conditional variance of the time series varies over time. The study of the dynamical pattern in the volatility of a time series (that is, the conditional variance process of the time series) constitutes the main subject of this chapter.

![](images/cef7fd624558a1e01403a7b721482f756ccc4e74c79842b5e1bd9921aa56f604.jpg)  
Exhibit 12.2 Daily CREF Stock Returns: August 26, 2004 to August 15, 2006

> r.cref=diff(log(CREF))*100   
> plot(r.cref); abline(h=0)

The sample ACF and PACF of the daily CREF returns (multiplied by 100), shown in Exhibits 12.3 and 12.4, suggest that the returns have little serial correlation at all. The sample EACF (not shown) also suggests that a white noise model is appropriate for these data. The average CREF return equals 0.0493 with a standard error of 0.02885. Thus the mean of the return process is not statistically significantly different from zero. This is expected based on the efficient-market hypothesis alluded to in the introduction to this chapter.

Exhibit 12.3 Sample ACF of Daily CREF Returns: 8/26/04 to 8/15/06   
![](images/6cb277dc0b9ecb5216bf0a6a835fdab56bc2273196d576dc4495d2737408eff2.jpg)  
> acf(r.cref)

Exhibit 12.4 Sample PACF of Daily CREF Returns: 8/26/04 to 8/15/06   
![](images/c8ef5180ebf4114e157bcd2832f81b95eef3448cf98fc521c55ca80091cf1f7a.jpg)  
> pacf(r.cref)

However, the volatility clustering observed in the CREF return data gives us a hint that they may not be independently and identically distributed—otherwise the variance would be constant over time. This is the first occasion in our study of time series models where we need to distinguish between series values being uncorrelated and series values being independent. If series values are truly independent, then nonlinear instantaneous transformations such as taking logarithms, absolute values, or squaring preserves independence. However, the same is not true of correlation, as correlation is only a measure of linear dependence. Higher-order serial dependence structure in data can be explored by studying the autocorrelation structure of the absolute returns (of lesser sampling vari-

ability with less mathematical tractability) or that of the squared returns (of greater sampling variability but with more manageability in terms of statistical theory). If the returns are independently and identically distributed, then so are the absolute returns (as are the squared returns), and hence they will be white noise as well. Hence, if the absolute or squared returns admit some significant autocorrelations, then these autocorrelations furnish some evidence against the hypothesis that the returns are independently and identically distributed. Indeed, the sample ACF and PACF of the absolute returns and those of the squared returns in Exhibits 12.5 through 12.8 display some significant autocorrelations and hence provide some evidence that the daily CREF returns are not independently and identically distributed.

Exhibit 12.5 Sample ACF of the Absolute Daily CREF Returns   
![](images/f064506fe564f9852ed33e102a2487dc67fbdff1a4aaba7dc478634d41716533.jpg)  
> acf(abs(r.cref))

Exhibit 12.6 Sample PACF of the Absolute Daily CREF Returns   
![](images/5d6ef3d4b2afd23c6a717cd370fced8c195ad37b5aa95745f90ef73590bef65b.jpg)  
> pacf(abs(r.cref))

Exhibit 12.7 Sample ACF of the Squared Daily CREF Returns   
![](images/2c550dc76c76fecc9d2305c95fcdc3b67f6d71c4910bf368727f8153735605cd.jpg)  
> acf(r.cref^2)

Exhibit 12.8 Sample PACF of the Squared Daily CREF Returns   
![](images/fdcad2b65301ac143d3965dfba9fa93ba72b3aa70658b69a3b40797947367b14.jpg)  
> pacf(r.cref^2)

These visual tools are often supplemented by formally testing whether the squared data are autocorrelated using the Box-Ljung test. Because no model fitting is required, the degrees of freedom of the approximating chi-square distribution for the Box-Ljung statistic equals the number of correlations used in the test. Hence, if we use $m$ autocorrelations of the squared data in the test, the test statistic is approximately chi-square distributed with m degrees of freedom, if there is no ARCH. This approach can be extended to the case when the conditional mean of the process is non-zero and if an ARMA model is adequate in describing the autocorrelation structure of the data. In which case, the first m autocorrelations of the squared residuals from this model can be used to test for the presence of ARCH. The corresponding Box-Ljung statistic will have a

chi-square distribution with m degrees of freedom under the assumption of no ARCH effect, see McLeod and Li (1983) and Li(2004). Below, we shall refer to the test for ARCH effects using the Box-Ljung statistic with the squared residuals or data as the McLeod- Li test.

In practice, it is useful to apply the McLeod-Li test for ARCH using a number of lags and plot the $p$ -values of the test. Exhibit 12.9 shows that the McLeod-Li tests are all significant at the $5 \%$ significance level when more than 3 lags are included in the test. This is broadly consistent with the visual pattern in Exhibit 12.7 and formally shows strong evidence for ARCH in this data.

Exhibit 12.9 McLeod-Li Test Statistics for Daily CREF Returns   
![](images/9c5b5787f2d893b37e2b9a8fe7b741a9936f7203ea64f62aa1ac6bcda01718ab.jpg)  
> win.graph(width=4.875, height=3,pointsize=8) > McLeod.Li.test( $\scriptstyle \cdot y = x$ .cref)

The distributional shape of the CREF returns can be explored by constructing a QQ normal scores plot—see Exhibit 12.10. The QQ plot suggests that the distribution of returns may have a tail thicker than that of a normal distribution and may be somewhat skewed to the right. Indeed, the Shapiro-Wilk test statistic for testing normality equals 0.9932 with $p$ -value equal to 0.024, and hence we reject the normality hypothesis at the usual significance levels.

# Exhibit 12.10 QQ Normal Plot of Daily CREF Returns

![](images/0a2591dac7f297174061505e548de6135185d8759b813ec4e1f9ba0640a5014a.jpg)

> win.graph(width=2.5,height=2.5,pointsize=8) > qqnorm(r.cref); qqline(r.cref)

The skewness of a random variable, say Y, is defined by $E ( Y { - } \mu ) ^ { 3 } / \sigma ^ { 3 }$ , where $\mu$ and $\sigma$ are the mean and standard deviation of Y, respectively. It can be estimated by the sample skewness

$$
g _ {1} = \sum_ {i = 1} ^ {n} \left(Y _ {i} - \bar {Y}\right) ^ {3} / \left(n \hat {\sigma} ^ {3}\right) \tag {12.1.2}
$$

where ${ \widehat { \sigma } } ^ { 2 } = \Sigma ( Y _ { i } - { \overline { { Y } } } ) ^ { 2 } / n$ is the sample variance. The sample skewness of the CREF returns equals 0.116. The thickness of the tail of a distribution relative to that of a normal distribution is often measured by the (excess) kurtosis, defined as $E ( Y - \mu ) ^ { 4 } / \sigma ^ { 4 } - 3$ . For normal distributions, the kurtosis is always equal to zero. A distribution with positive kurtosis is called a heavy-tailed distribution, whereas it is called light-tailed if its kurtosis is negative. The kurtosis can be estimated by the sample kurtosis

$$
g _ {2} = \sum_ {i = 1} ^ {n} \left(Y _ {i} - \bar {Y}\right) ^ {4} / \left(n \hat {\sigma} ^ {4}\right) - 3 \tag {12.1.3}
$$

The sample kurtosis of the CREF returns equals 0.6274. An alternative definition of kurtosis modifies the formula and uses $E ( r _ { t } - \mu ) ^ { 4 } / \sigma ^ { 4 }$ ; that is, it does not subtract three from the ratio. We shall always use the former definition for kurtosis.

Another test for normality is the Jarque-Bera test, which is based on the fact that a normal distribution has zero skewness and zero kurtosis. Assuming independently and identically distributed data $Y _ { 1 } , Y _ { 2 } , . . . , Y _ { n }$ , the Jarque-Bera test statistic is defined as

$$
J B = \frac {n g _ {1} ^ {2}}{6} + \frac {n g _ {2} ^ {2}}{2 4} \tag {12.1.4}
$$

where $g _ { 1 }$ is the sample skewness and $g _ { 2 }$ is the sample kurtosis. Under the null hypothesis of normality, the Jarque-Bera test statistic is approximately distributed as $\bar { \chi } ^ { 2 }$ with two degrees of freedom. In fact, under the normality assumption, each summand defining the Jarque-Bera statistic is approximately $\chi ^ { 2 }$ with 1 degree of freedom. The Jarque-Bera test rejects the normality assumption if the test statistic is too large. For the CREF returns, $J \bar { B ^ { = } } = 5 0 0 \times 0 . 1 1 6 ^ { 2 } / \bar { 6 } + 5 0 0 \dot { \times } 0 . 6 2 7 4 ^ { 2 } / 2 4 = 1 . 1 2 + 8 . 2 0 = 9 . \bar { 3 } 2$ with a $p$ -value equal to 0.011. Recall that the upper 5 percentage point of a $\chi ^ { 2 }$ distribution with unit degree of freedom equals 3.84. Hence, the data appear not to be skewed but do have a relatively heavy tail. In particular, the normality assumption is inconsistent with the CREF return data—a conclusion that is also consistent with the finding of the Shapiro-Wilk test.

In summary, the CREF return data are found to be serially uncorrelated but admit a higher-order dependence structure, namely volatility clustering, and a heavy-tailed distribution. It is commonly observed that such characteristics are rather prevalent among financial time series data. The GARCH models introduced in the next sections attempt to provide a framework for modeling and analyzing time series that display some of these characteristics.

# 12.2 The ARCH(1) Model

Engle (1982) first proposed the autoregressive conditional heteroscedasticity (ARCH) model for modeling the changing variance of a time series. As discussed in the previous section, the return series of a financial asset, say $\{ r _ { t } \}$ , is often a serially uncorrelated sequence with zero mean, even as it exhibits volatility clustering. This suggests that the conditional variance of $r _ { t }$ given past returns is not constant. The conditional variance, also referred to as the conditional volatility, of $r _ { t }$ will be denoted by $\sigma _ { t | t - 1 } ^ { 2 }$ , with the subscript $t - 1$ signifying that the conditioning is upon returns through time $t - 1$ . When $r _ { t }$ is available, the squared return $r _ { t } ^ { 2 }$ provides an unbiased estimator of $\sigma _ { t | t - 1 } ^ { 2 }$ . A series of large squared returns may foretell a relatively volatile period. Conversely, a series of small squared returns may foretell a relatively quiet period. The ARCH model is formally a regression model with the conditional volatility as the response variable and the past lags of the squared return as the covariates. For example, the ARCH(1) model assumes that the return series $\left\{ \boldsymbol { r } _ { t } \right\}$ is generated as follows:

$$
r _ {t} = \sigma_ {t | t - 1} \varepsilon_ {t} \tag {12.2.1}
$$

$$
\sigma_ {t \mid t - 1} ^ {2} = \omega + \alpha r _ {t - 1} ^ {2} \tag {12.2.2}
$$

where $\alpha$ and ω are unknown parameters, $\left\{ \varepsilon _ { t } \right\}$ is a sequence of independently and identically distributed random variables each with zero mean and unit variance (also known as the innovations), and $\varepsilon _ { t }$ is independent of $r _ { t - j } , j = 1$ , 2,… . The innovation $\varepsilon _ { t }$ is presumed to have unit variance so that the conditional variance of $r _ { t }$ equals $\sigma _ { t \mid t - 1 } ^ { 2 }$ . This follows from

$$
\begin{array}{l} E (r _ {t} ^ {2} | r _ {t - j}, j = 1, 2, \ldots) = E (\sigma_ {t | t - 1} ^ {2} \varepsilon_ {t} ^ {2} | r _ {t - j}, j = 1, 2, \ldots) \\ = \sigma_ {t | t - 1} ^ {2} E \left(\varepsilon_ {t} ^ {2} \mid r _ {t - j}, j = 1, 2, \dots\right) \\ = \sigma_ {t | t - 1} ^ {2} E \left(\varepsilon_ {t} ^ {2}\right) \tag {12.2.3} \\ = \sigma_ {t | t - 1} ^ {2} \\ \end{array}
$$

The second equality follows because $\left. \mathtt { o } _ { t } \right| _ { t - 1 }$ is known given the past returns, the third equality holds because $\varepsilon _ { t }$ is independent of past returns, and the last equality results from the assumption that the variance of $\varepsilon _ { t }$ equals 1.

Exhibit 12.11 shows the time series plot of a simulated series of size 500 from an ARCH(1) model with $\mathrm { \odot } = 0 . 0 1$ and $\alpha = 0 . 9$ . Volatility clustering is evident in the data as larger fluctuations cluster together, although the series is able to recover from large fluctuations quickly because of the very short memory in the conditional variance process.†

![](images/5c01395481d47bf8b8680725e788b6a70404d490790942652f680175c5e0ae97.jpg)  
Exhibit 12.11 Simulated ARCH(1) Model with $\mathbf { \omega _ { \mathrm { ( 0 ) } } = 0 . 0 1 }$ and ${ \bf { \alpha } } \mathrm { ~ a } _ { 1 } = { \bf { 0 . 9 } }$

```txt
> set.seed(1235678); library(tseries)  
> garch01.sim=garch.sim(alpha=c(.01,.9), n=500)  
> plot(garch01.sim,type='l', ylab=expression(r[t]), xlab='t') 
```

While the ARCH model resembles a regression model, the fact that the conditional variance is not directly observable (and hence is called a latent variable) introduces some subtlety in the use of ARCH models in data analysis. For example, it is not obvious how to explore the regression relationship graphically. To do so, it is pertinent to replace the conditional variance by some observable in Equation (12.2.2). Let

# 12.2 The ARCH(1) Model

$$
\eta_ {t} = r _ {t} ^ {2} - \sigma_ {t | t - 1} ^ {2} \tag {12.2.4}
$$

It can be verified thatis uncorrelated with p $\{ \boldsymbol \eta _ { t } \}$ is a serially uncorrereturns. Substituting ero mean. Moreover,into Equation (12.2. $\boldsymbol { \mathsf { \Pi } } _ { \boldsymbol { \mathsf { I } } _ { t } }$ ast σt |t 1–2 $\sigma _ { t | t - 1 } ^ { 2 } = r _ { t } ^ { 2 } - \eta _ { t }$ it is obvious that

$$
r _ {t} ^ {2} = \omega + \alpha r _ {t - 1} ^ {2} + \eta_ {t} \tag {12.2.5}
$$

Thus, the squared return series satisfies an AR(1) model under the assumption of an ARCH(1) model for the return series! Based on this useful observation, an ARCH(1) model may be specified if an AR(1) specification for the squared returns is warranted by techniques learned from earlier chapters.

Besides its value in terms of data analysis, the deduced AR(1) model for the squared returns can be exploited to gain theoretical insights on the parameterization of the ARCH model. For example, because the squared returns must be nonnegative, it makes sense to always restrict the parameters ω and $\alpha$ to be nonnegative. Also, if the return series is stationary with variance $\sigma ^ { 2 }$ , then taking expectation on both sides of Equation (12.2.5) yields

$$
\sigma^ {2} = \omega + \alpha \sigma^ {2} \tag {12.2.6}
$$

That is, ${ \sigma } ^ { 2 } = { \sigma } / ( 1 - \alpha )$ and hence $0 \leq \alpha < 1$ . Indeed, it can be shown (Ling and McAleer, 2002) that the condition $0 \leq \alpha < 1$ is necessary and sufficient for the (weak) stationarity of the ARCH(1) model. At first sight, it seems that the concepts of stationarity and conditional heteroscedasticity may be incompatible. However, recall that weak stationarity of a process requires that the mean of the process be constant and the covariance of the process at any two epochs be finite and identical whenever the lags of the two epochs are the same. In particular, the variance is constant for a weakly stationary process. The condition $0 \leq \alpha < 1$ implies that there exists an initial distribution for $r _ { 0 }$ such that $r _ { t }$ defined by Equations (12.2.1) and (12.2.2) for $t \geq 1$ is weakly stationary in the sense above. It is interesting to observe that weak stationarity does not preclude the possibility of a nonconstant conditional variance process, as is the case for the ARCH(1) model! It can be checked that the ARCH(1) process is white noise. Hence, it is an example of a white noise that admits a nonconstant conditional variance process as defined by Equation (12.2.2) that varies with the lag one of the squared process.

A satisfying feature of the ARCH(1) model is that, even if the innovation $\boldsymbol { \mathsf { n } } _ { t }$ has a normal distribution, the stationary distribution of an ARCH(1) model with $1 > \mathsf { { a } } > 0$ has fat tails; that is, its kurtosis, $E ( r _ { t } ^ { 4 } ) / \sigma ^ { 4 } - 3$ , is greater than zero. (Recall that the kurtosis of a normal distribution is always equal to 0, and a distribution with positive kurtosis is said to be fat-tailed, while one with a negative kurtosis is called a light-tailed distribution.) To see the validity of this claim, consider the case where the $\left\{ \varepsilon _ { t } \right\}$ are independently and identically distributed as standard normal variables. Raising both sides of Equation (12.2.1) on page 285 to the fourth power and taking expectations gives

$$
\begin{array}{l} E (r _ {t} ^ {4}) = E [ E (\sigma_ {t | t - 1} ^ {4} \varepsilon_ {t} ^ {4} | r _ {t - j}, j = 1, 2, 3, \dots) ] \\ = E \left[ \sigma_ {t \mid t - 1} ^ {4} E \left(\varepsilon_ {t} ^ {4} \mid r _ {t - j}, j = 1, 2, 3, \dots\right) \right] \tag {12.2.7} \\ = E \left[ \sigma_ {t | t - 1} ^ {4} E \left(\varepsilon_ {t} ^ {4}\right) \right] \\ = 3 E (\sigma_ {t | t - 1} ^ {4}) \\ \end{array}
$$

The first equality follows from the iterated-expectation formula, which, in the simple case of two random variables X, Y, states that $E [ E ( X | Y ) ] = E ( X )$ . [See Equation (9.E.5) on page 218 for a review.] The second equality results from the fact that $\left. \mathtt { o } _ { t } \right| _ { t - 1 }$ is known given past returns. The third equality is a result of the independence between $\varepsilon _ { t }$ and past returns, and the final equality follows from the normality assumption. It remains to calculate $E ( \sigma _ { t | t - 1 } ^ { 4 } )$ . Now, it is unclear whether the preceding expectation exists as a finite number. For the moment, assume it does and, assuming stationarity, let it be denoted by τ. Below, we shall derive a condition for this assumption to be valid. Raising both sides of Equation (12.2.2) to the second power and taking expectation yields

$$
\tau = \omega^ {2} + 2 \omega \alpha \sigma^ {2} + \alpha^ {2} 3 \tau \tag {12.2.8}
$$

which implies

$$
\tau = \frac {\omega^ {2} + 2 \omega \alpha \sigma^ {2}}{1 - 3 \alpha^ {2}} \tag {12.2.9}
$$

This equality shows that a necessary (and, in fact, also sufficient) condition for the finiteness of $\tau$ is that $0 \leq a < 1 / \sqrt { 3 }$ , in which case the ARCH(1) process has finite fourth moment. Incidentally, this shows that a stationary ARCH(1) model need not have finite fourth moments. The existence of finite higher moments will further restrict the parameter range—a feature also shared by higher-order analogues of the ARCH model and its variants. Returning to the calculation of the kurtosis of an ARCH(1) process, it can be shown by tedious algebra that Equation (12.2.1) implies that $\tau > \sigma ^ { 4 }$ and hence $E ( r _ { t } ^ { 4 } ) > 3 \sigma ^ { 4 }$ . Thus the kurtosis of a stationary ARCH(1) process is greater than zero. This verifies our earlier statement that an ARCH(1) process has fat tails even with normal innovations. In other words, the fat tail is a result of the volatility clustering as specified by Equation (12.2.2).

A main use of the ARCH model is to predict the future conditional variances. For example, one might be interested in forecasting the $h$ -step-ahead conditional variance

$$
\sigma_ {t + h \mid t} ^ {2} = E \left(r _ {t + h} ^ {2} \mid r _ {t}, r _ {t - 1}, \dots\right) \tag {12.2.10}
$$

For $h = 1$ , the ARCH(1) model implies that

$$
\sigma_ {t + 1 \mid t} ^ {2} = \omega + \alpha r _ {t} ^ {2} = (1 - \alpha) \sigma^ {2} + \alpha r _ {t} ^ {2} \tag {12.2.11}
$$

which is a weighted average of the long-run variance and the current squared return. Similarly, using the iterated expectation formula, we have

$$
\begin{array}{l} \sigma_ {t + h | t} ^ {2} = E (r _ {t + h} ^ {2} | r _ {t}, r _ {t - 1}, \dots) \\ = E \left[ E \left(\sigma_ {t + h \mid t + h - 1} ^ {2} \varepsilon_ {t + h} ^ {2} \mid r _ {t + h - 1}, r _ {t + h - 2}, \dots\right) \mid r _ {t}, r _ {t - 1}, \dots \right] \\ = E \left[ \sigma_ {t + h \mid t + h - 1} ^ {2} E \left(\varepsilon_ {t + h} ^ {2}\right) \mid r _ {t}, r _ {t - 1}, \dots \right] \tag {12.2.12} \\ = E (\sigma_ {t + h | t + h - 1} ^ {2} | r _ {t}, r _ {t - 1}, \dots) \\ = \omega + \alpha E \left(r _ {t + h - 1} ^ {2} \mid r _ {t}, r _ {t - 1}, \dots\right) \\ = \omega + \alpha \sigma_ {t + h - 1 | t} ^ {2} \\ \end{array}
$$

where we adopt the convention that $\sigma _ { t + h | t } ^ { 2 } = r _ { t + h } ^ { 2 }$ r t h + for $h < 0$ . The formula above provides a recursive recipe for computing the $h$ -step-ahead conditional variance.

# 12.3 GARCH Models

The forecasting formulas derived in the previous section show both the strengths and weaknesses of an ARCH(1) model, as the forecasting of the future conditional variances only involves the most recent squared return. In practice, one may expect that the accuracy of forecasting may improve by including all past squared returns with lesser weight for more distant volatilities. One approach is to include further lagged squared returns in the model. The $\operatorname { A R C H } ( q )$ model, proposed by Engle (1982), generalizes Equation (12.2.2) on page 285, by specifying that

$$
\sigma_ {t \mid t - 1} ^ {2} = \omega + \alpha_ {1} r _ {t - 1} ^ {2} + \alpha_ {2} r _ {t - 2} ^ {2} + \dots + \alpha_ {q} r _ {t - q} ^ {2} \tag {12.3.1}
$$

Here, $q$ is referred to as the ARCH order. Another approach, proposed by Bollerslev (1986) and Taylor (1986), introduces $p$ lags of the conditional variance in the model, where $p$ is referred to as the GARCH order. The combined model is called the generalized autoregressive conditional heteroscedasticity, ${ \mathrm { G A R C H } } ( p , q )$ , model.

$$
\begin{array}{l} \sigma_ {t \mid t - 1} ^ {2} = \omega + \beta_ {1} \sigma_ {t - 1 \mid t - 2} ^ {2} + \dots + \beta_ {p} \sigma_ {t - p \mid t - p - 1} ^ {2} + \alpha_ {1} r _ {t - 1} ^ {2} \tag {12.3.2} \\ + \alpha_ {2} r _ {t - 2} ^ {2} + \dots + \alpha_ {q} r _ {t - q} ^ {2} \\ \end{array}
$$

In terms of the backshift $B$ notation, the model can be expressed as

$$
(1 - \beta_ {1} B - \dots - \beta_ {p} B ^ {p}) \sigma_ {t | t - 1} ^ {2} = \omega + (\alpha_ {1} B + \dots + \alpha_ {q} B ^ {q}) r _ {t} ^ {2} \tag {12.3.3}
$$

We note that in some of the literature, the notation ${ \mathrm { G A R C H } } ( p , q )$ is written as $\mathrm { G A R C H } ( q , p )$ ; that is, the orders are switched. It can be rather confusing but true that the two different sets of conventions are used in different software! A reader must find out which convention is used by the software on hand before fitting or interpreting a GARCH model.

Because conditional variances must be nonnegative, the coefficients in a GARCH model are often constrained to be nonnegative. However, the nonnegative parameter constraints are not necessary for a GARCH model to have nonnegative conditional variances with probability 1; see Nelson and Cao (1992) and Tsai and Chan (2006). Allowing the parameter values to be negative may increase the dynamical patterns that can be captured by the GARCH model. We shall return to this issue later. Henceforth, within this section, we shall assume the nonnegative constraint for the GARCH parameters.

Exhibit 12.12 shows the time series plot of a time series, of size 500, simulated from a GARCH(1,1) model with standard normal innovations and parameter values ${ \bf { \omega } } _ { \omega } = 0 . 0 2$ , $\alpha = 0 . 0 5$ , and $\beta = 0 . 9$ . Volatility clustering is evident in the plot, as large (small) fluctuations are usually succeeded by large (small) fluctuations. Moreover, the inclusion of the lag 1 of the conditional variance in the model successfully enhances the memory in the volatility.

![](images/cc1005787c4b249ca46f4df5032bbe3856d005edbd030d2a25a8f5107932e1a5.jpg)  
Exhibit 12.12 Simulated GARCH(1,1) Process

```txt
> set.seed(1234567)  
> garch11.sim=garch.sim(alpha=c(0.02, 0.05), beta=.9, n=500)  
> plot(garch11.sim,type='l', ylab=expression(r[t]), xlab='t') 
```

Except for lags 3 and 20, which are mildly significant, the sample ACF and PACF of the simulated data, shown in Exhibits 12.13 and 12.14, do not show significant correlations. Hence, the simulated process seems to be basically serially uncorrelated as it is.

Exhibit 12.13 Sample ACF of Simulated GARCH(1,1) Process   
![](images/23e15ead8cdd18978b20b78efc19674079991f412497b1abc90ad78d762d5ccf.jpg)  
> acf(garch11.sim)

Exhibit 12.14 Sample PACF of Simulated GARCH(1,1) Process   
![](images/448577107e6338b0d5bf8a454d60e12e6e543bea36a46f02457056c0ca1e0717.jpg)  
> pacf(garch11.sim)

Exhibits 12.15 through 12.18 show the sample ACF and PACF of the absolute values and the squares of the simulated data.

Exhibit 12.15 Sample ACF of the Absolute Values of the Simulated GARCH(1,1) Process   
![](images/8586d9950220d118369b20600ddb4a6ee090a4c9d550e14a7c37381901684380.jpg)  
> acf(abs(garch11.sim))

Exhibit 12.16 Sample PACF of the Absolute Values of the Simulated GARCH(1,1) Process   
![](images/7adbe98426e2e523b92941ecb35d880c42adafc9372a5536144ec49860de6899.jpg)  
> pacf(abs(garch11.sim))

These plots indicate the existence of significant autocorrelation patterns in the absolute and squared data and indicate that the simulated process is in fact serially dependent. Interestingly, the lag 1 autocorrelations are not significant in any of these last four plots.

Exhibit 12.17 Sample ACF of the Squared Values of the Simulated GARCH(1,1) Process   
![](images/882614efd123598dad7dd9b861a96d0d9f4c01a44be446a2aeaea6dc3c5fc3ab.jpg)  
> acf(garch11.sim^2)

Exhibit 12.18 Sample PACF of the Squared Values of the Simulated GARCH(1,1) Process   
![](images/f7d5442a192ea077d8ee3c0fc564b28eb665b0139ea28d1241a9c98433188c6b.jpg)  
> pacf(garch11.sim^2)

For model identification of the GARCH orders, it is again advantageous to express the monition $\boldsymbol { \eta } _ { t } = \mathbf { \check { \eta } } _ { t } ^ { 2 } - \mathbf { \sigma } _ { t \left. t - 1 \right. } ^ { 2 }$ ional variances in terms of the squared returns. Recal. Similar to the ARCH(1) model, we can show that $\{ \boldsymbol \eta _ { t } \}$ defi- is a serially uncorrelated sequence. Moreover, $\boldsymbol { \mathsf { \Pi } } _ { \boldsymbol { \mathsf { I } } _ { t } }$ is uncorrelated with past squared returns. Substituting the expression $\sigma _ { t | t - 1 } ^ { 2 } = r _ { t } ^ { 2 } - \eta _ { t }$ into Equation (12.3.2) yields

$$
\begin{array}{l} r _ {t} ^ {2} = \omega + (\beta_ {1} + \alpha_ {1}) r _ {t - 1} ^ {2} + \dots + (\beta_ {m a x (p, q)} + \alpha_ {m a x (p, q)}) r _ {t - m a x (p, q)} ^ {2} \\ + \eta_ {t} - \beta_ {1} \eta_ {t - 1} - \dots - \beta_ {p} \eta_ {t - p} \tag {12.3.4} \\ \end{array}
$$

where $\beta _ { k } = 0$ for all integers $k > p$ and ${ \bf \alpha } { \bf a } _ { k } = 0$ for $k > q$ . This shows that the ${ \mathrm { G A R C H } } ( p , q )$ model for the return series implies that the model for the squared returns is an ARMA(max(p, q),p) model. Thus, we can apply the model identification techniques for ARMA models to the squared return series to identify $p$ and $m a x ( p , q )$ . Notice that if $q$ is smaller than $p$ , it will be masked in the model identification. In such cases, we can first fit a ${ \mathrm { G A R C H } } ( p , p )$ model and then estimate $q$ by examining the significance of the resulting ARCH coefficient estimates.

As an illustration, Exhibit 12.19 shows the sample EACF of the squared values from the simulated GARCH(1,1) series.

Exhibit 12.19 Sample EACF for the Squared Simulated GARCH(1,1) Series   

<table><tr><td>AR/MA</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td></tr><tr><td>0</td><td>0</td><td>0</td><td>x</td><td>x</td><td>0</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>x</td><td>0</td><td>0</td><td>0</td><td>x</td><td>0</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>4</td><td>x</td><td>x</td><td>0</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>5</td><td>x</td><td>0</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>6</td><td>x</td><td>0</td><td>x</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>7</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td colspan="15">&gt; eacf((garch11.sim)^2)</td></tr></table>

The pattern in the EACF table is not very clear, although an ARMA(2,2) model seems to be suggested. The fuzziness of the signal in the EACF table is likely caused by the larger sampling variability when we deal with higher moments. Shin and Kang (2001) argued that, to a first-order approximation, a power transformation preserves the theoretical autocorrelation function and hence the order of a stationary ARMA process. Their result suggests that the GARCH order may also be identified by studying the absolute returns. Indeed, the sample EACF table for the absolute returns, shown in Exhibit 12.20, more convincingly suggests an ARMA(1,1) model, and therefore a GARCH(1,1) model for the original data, although there is also a hint of a GARCH(2,2) model.

Exhibit 12.20 Sample EACF for Absolute Simulated GARCH(1,1) Series   

<table><tr><td>AR/MA</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td></tr><tr><td>0</td><td>o</td><td>o</td><td>x</td><td>x</td><td>o</td><td>o</td><td>x</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td></tr><tr><td>1</td><td>x</td><td>o</td><td>o</td><td>o</td><td>x</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td></tr><tr><td>2</td><td>x</td><td>x</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td></tr><tr><td>3</td><td>x</td><td>x</td><td>o</td><td>o</td><td>o</td><td>x</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td></tr><tr><td>4</td><td>x</td><td>x</td><td>o</td><td>x</td><td>o</td><td>x</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td></tr><tr><td>5</td><td>x</td><td>o</td><td>x</td><td>x</td><td>x</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td></tr><tr><td>6</td><td>x</td><td>o</td><td>x</td><td>x</td><td>x</td><td>x</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td></tr><tr><td>7</td><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td><td>o</td><td>x</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td><td>o</td></tr></table>

> eacf(abs(garch11.sim))

For the absolute CREF daily return data, the sample EACF table is reported in Exhibit 12.21, which suggests a GARCH(1,1) model. The corresponding EACF table for the squared CREF returns (not shown) is, however, less clear and may suggest a GARCH(2,2) model.

Exhibit 12.21 Sample EACF for the Absolute Daily CREF Returns   

<table><tr><td>AR/MA</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>4</td><td>x</td><td>0</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>5</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>6</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>7</td><td>x</td><td>x</td><td>x</td><td>x</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></table>

> eacf(abs(r.cref))

Furthermore, the parameter estimates of the fitted ARMA model for the absolute data may yield initial estimates for maximum likelihood estimation of the GARCH model. For example, Exhibit 12.22 reports the estimated parameters of the fitted ARMA(1,1) model for the absolute simulated GARCH(1,1) process.

Exhibit 12.22 Parameter Estimates with ARMA(1,1) Model for the Absolute Simulated GARCH(1,1) Series   

<table><tr><td>Coefficient</td><td>ar1</td><td>ma1</td><td>Intercept</td></tr><tr><td>Estimate</td><td>0.9821</td><td>-0.9445</td><td>0.5077</td></tr><tr><td>s.e.</td><td>0.0134</td><td>0.0220</td><td>0.0499</td></tr></table>

> arima(abs(garch11.sim),order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (1,0,1))

Using Equation (12.3.4), it can be seen that $\beta$ is estimated by 0.9445, $\alpha$ is estimated by $0 . 9 8 2 1 - 0 . 9 4 4 5 = 0 . 0 3 7 6 3$ , and ω can be estimated as the variance of the original data times the estimate of $1 - { \alpha } - \beta$ , which equals 0.0073. Amazingly, these estimates turn out to be quite close to the maximum likelihood estimates reported in the next section!

We now derive the condition for a GARCH model to be weakly stationary. Assume for the moment that the return process is weakly stationary. Taking expectations on both sides of Equation (12.3.4) gives an equation for the unconditional variance $\sigma ^ { 2 }$

$$
\sigma^ {2} = \omega + \sigma^ {2} \sum_ {i = 1} ^ {\max  (p, q)} \left(\beta_ {i} + \alpha_ {i}\right) \tag {12.3.5}
$$

so that

$$
\sigma^ {2} = \frac {\omega}{1 - \sum_ {i = 1} ^ {\max  (p , q)} \left(\beta_ {i} + \alpha_ {i}\right)} \tag {12.3.6}
$$

which is finite if

$$
\sum_ {i = 1} ^ {\max  (p, q)} \left(\beta_ {i} + \alpha_ {i}\right) <   1 \tag {12.3.7}
$$

This condition can be shown to be necessary and sufficient for the weak stationarity of a $\mathrm { G A R C H } ( p , q )$ model. (Recall that we have implicitly assumed that $\mathfrak { a } _ { 1 } \geq 0 , \ldots$ , $\alpha _ { p } \geq 0$ , and $\beta _ { 1 } \geq 0 , . . . , \beta _ { q } \geq 0 .$ ${ \beta } _ { q } \geq 0$ ) Henceforth, we assume $p = q$ for ease of notation.

As in the case of an ARCH(1) model, finiteness of higher moments of the GARCH model requires further stringent conditions on the coefficients; see Ling and McAleer (2002). Also, the stationary distribution of a GARCH model is generally fat-tailed even if the innovations are normal.

In terms of forecasting the $h$ -step-ahead conditional variance $\sigma _ { t + h | t } ^ { 2 }$ , we can repeat the arguments used in the preceding section to derive the recursive formula that for $h > p$

$$
\sigma_ {t + h \mid t} ^ {2} = \omega + \sum_ {i = 1} ^ {p} \left(\alpha_ {i} + \beta_ {i}\right) \sigma_ {t + h - i \mid t} ^ {2} \tag {12.3.8}
$$

More generally, for arbitrary $h \geq 1$ , the formula is more complex, as

# 12.3 GARCH Models

$$
\sigma_ {t + h \mid t} ^ {2} = \omega + \sum_ {i = 1} ^ {p} \alpha_ {i} \sigma_ {t + h - i \mid t} ^ {2} + \sum_ {i = 1} ^ {p} \beta_ {i} \hat {\sigma} _ {t + h - i \mid t + h - i - 1} ^ {2} \tag {12.3.9}
$$

where

$$
\sigma_ {t + h \mid t} ^ {2} = r _ {t + h} ^ {2} \text {f o r} h <   0 \tag {12.3.10}
$$

and

$$
\hat {\sigma} _ {t + h - i | t + h - i - 1} ^ {2} = \left\{ \begin{array}{l l} \sigma_ {t + h - i | t} ^ {2} & \text {f o r} h - i - 1 > 0 \\ \sigma_ {t + h - i | t + h - i - 1} ^ {2} & \text {o t h e r w i s e} \end{array} \right. \tag {12.3.11}
$$

The computation of the conditional variances may be best illustrated using the GARCH(1,1) model. Suppose that there are $n$ observations $r _ { 1 } , r _ { 2 } , . . . , r _ { n }$ and

$$
\sigma_ {t \mid t - 1} ^ {2} = \omega + \alpha_ {1} r _ {t - 1} ^ {2} + \beta_ {1} \sigma_ {t - 1 \mid t - 2} ^ {2} \tag {12.3.12}
$$

To compute the conditional variances for $2 \leq t \leq n$ , we need to set the initial value $\sigma _ { 1 | 0 } ^ { 2 }$ . This may be set to the stationary unconditional variance $\sigma ^ { 2 } = \omega / ( 1 - \alpha _ { 1 } - \beta _ { 1 } )$ under the stationarity assumption or simply as $r _ { 1 } ^ { 2 }$ . Thereafter, we can compute $\sigma _ { t | t - 1 } ^ { 2 }$ by the formula defining the GARCH model. It is interesting to observe that

$$
\sigma_ {t \mid t - 1} ^ {2} = (1 - \alpha_ {1} - \beta_ {1}) \sigma^ {2} + \alpha_ {1} r _ {t - 1} ^ {2} + \beta_ {1} \sigma_ {t - 1 \mid t - 2} ^ {2} \tag {12.3.13}
$$

so that the estimate of the one-step-ahead conditional volatility is a weighted average of the long-run variance, the current squared return, and the current estimate of the conditional volatility. Further, the $\mathbf { M A } ( \infty )$ representation of the conditional variance implies that

$$
\sigma_ {t \mid t - 1} ^ {2} = \sigma^ {2} + \alpha_ {1} \left(r _ {t - 1} ^ {2} + \beta_ {1} r _ {t - 2} ^ {2} + \beta_ {1} ^ {2} r _ {t - 3} ^ {2} + \beta_ {1} ^ {3} r _ {t - 4} ^ {2} + \dots\right) \tag {12.3.14}
$$

an infinite moving average of past squared returns. The formula shows that the squared returns in the distant past receive exponentially diminishing weights. In contrast, simple moving averages of the squared returns are sometimes used to estimate the conditional variance. These, however, suffer much larger bias.

If $\mathsf { a } _ { 1 } + \mathsf { \beta } _ { 1 } = 1$ , then the GARCH(1,1) model is nonstationary and instead is called an IGARCH(1,1) model with the letter I standing for integrated. In such a case, we shall drop the subscript from the notation and let $\alpha = 1 - \beta$ . Suppose that $\Theta = 0$ . Then

$$
\sigma_ {t \mid t - 1} ^ {2} = (1 - \beta) \left(r _ {t - 1} ^ {2} + \beta r _ {t - 2} ^ {2} + \beta^ {2} r _ {t - 3} ^ {2} + \beta^ {3} r _ {t - 4} ^ {2} + \dots\right), \tag {12.3.15}
$$

an exponentially weighted average of the past squared returns. The famed Riskmetrics software in finance employs the IGARCH(1,1) model with $\beta = 0 . 9 4$ for estimating conditional variances; see Andersen et al. (2006).

# 12.4 Maximum Likelihood Estimation

The likelihood function of a GARCH model can be readily derived for the case of normal innovations. We illustrate the computation for the case of a stationary GARCH(1,1) model. Extension to the general case is straightforward. Given the parameters ω, α, and $\beta$ , the conditional variances can be computed recursively by the formula

$$
\sigma_ {t \mid t - 1} ^ {2} = \omega + \alpha r _ {t - 1} ^ {2} + \beta \sigma_ {t - 1 \mid t - 2} ^ {2} \tag {12.4.1}
$$

for $t \geq 2$ , with the initial value, $\sigma _ { 1 | 0 } ^ { 2 }$ , set under the stationarity assumption as the stationary unconditional variance $\sigma ^ { 2 } = \overset { \cdot } { \omega } / ( 1 - \alpha - \beta )$ . We use the conditional pdf

$$
f \left(r _ {t} \mid r _ {t - 1}, \dots , r _ {1}\right) = \frac {1}{\sqrt {2 \pi \sigma_ {t \mid t - 1} ^ {2}}} \exp \left[ - r _ {t} ^ {2} / \left(2 \sigma_ {t \mid t - 1} ^ {2}\right) \right] \tag {12.4.2}
$$

and the joint pdf

$$
f \left(r _ {n}, \dots , r _ {1}\right) = f \left(r _ {n - 1}, \dots , r _ {1}\right) f \left(r _ {n} \mid r _ {n - 1}, \dots , r _ {1}\right) \tag {12.4.3}
$$

Iterating this last formula and taking logs gives the following formula for the log-likelihood function:

$$
L (\omega , \alpha , \beta) = - \frac {n}{2} \log (2 \pi) - \frac {1}{2} \sum_ {i = 1} ^ {n} \left\{\log \left(\sigma_ {t - 1 \mid t - 2} ^ {2}\right) + r _ {t} ^ {2} / \sigma_ {t \mid t - 1} ^ {2} \right\} \tag {12.4.4}
$$

There is no closed-form solution for the maximum likelihood estimators of ω, α, and $\beta$ , but they can be computed by maximizing the log-likelihood function numerically. The maximum likelihood estimators can be shown to be approximately normally distributed with the true parameter values as their means. Their covariances may be collected into a matrix denoted by Λ, which can be obtained as follows. Let

$$
\theta = \left[ \begin{array}{l} \omega \\ \alpha \\ \beta \end{array} \right] \tag {12.4.5}
$$

be the vector of parameters. Write the ith component of θ as $\theta _ { i }$ so that $\theta _ { 1 } = \omega$ , $\theta _ { 2 } = \alpha$ , and $\theta _ { 3 } = \beta$ . The diagonal elements of $\Lambda$ are the approximate variances of the estimators, whereas the off-diagonal elements are their approximate covariances. So, the first diagonal element of $\Lambda$ is the approximate variance of $\hat { \omega }$ , the (1,2)th element of $\Lambda$ is the approximate covariance between $\hat { \omega }$ and $\hat { \textmd a } _ { }$ , and so forth. We now outline the computation of $\Lambda$ . Readers not interested in the mathematical details may skip the rest of this paragraph. The $3 \times 3$ matrix $\Lambda$ is approximately equal to the inverse matrix of the $3 \times 3$ matrix whose $( i , j )$ th element equals

$$
\frac {1}{2} \sum_ {t = 1} ^ {n} \frac {1}{\sigma_ {t | t - 1} ^ {4}} \frac {\partial \sigma_ {t | t - 1} ^ {2}}{\partial \theta_ {i}} \frac {\partial \sigma_ {t | t - 1} ^ {2}}{\partial \theta_ {j}} \tag {12.4.6}
$$

The partial derivatives in this expression can be obtained recursively by differentiating Equation (12.4.1). For example, differentiating both sides of Equation (12.4.1) with respect to ω yields the recursive formula

$$
\frac {\partial \sigma_ {t | t - 1} ^ {2}}{\partial \omega} = 1 + \beta \frac {\partial \sigma_ {t - 1 | t - 2} ^ {2}}{\partial \omega} \tag {12.4.7}
$$

Other partial derivatives can be computed similarly.

Recall that, in the previous section, the simulated GARCH(1,1) series was identified to be either a GARCH(1,1) model or a GARCH(2,2) model. The model fit of the GARCH(2,2) model is reported in Exhibit 12.23, where the estimate of ω is denoted by a0, that of ${ \mathfrak { a } } _ { 1 }$ by a1, that of $\beta _ { 1 }$ by b1, and so forth. Note that none of the coefficients is significant, although a2 is close to being significant. The model fit for the GARCH(1,1) model is given in Exhibit 12.24.

Exhibit 12.23 Estimates for GARCH(2,2) Model of a Simulated GARCH(1,1) Series   

<table><tr><td>Coefficient</td><td>Estimate</td><td>Std. Error</td><td>t-value</td><td>Pr(&gt;|t|)</td></tr><tr><td>a0</td><td>1.835e-02</td><td>1.515e-02</td><td>1.211</td><td>0.2257</td></tr><tr><td>a1</td><td>4.09e-15</td><td>4.723e-02</td><td>8.7e-14</td><td>1.0000</td></tr><tr><td>a2</td><td>1.136e-01</td><td>5.855e-02</td><td>1.940</td><td>0.0524</td></tr><tr><td>b1</td><td>3.369e-01</td><td>3.696e-01</td><td>0.911</td><td>0.3621</td></tr><tr><td>b2</td><td>5.100e-01</td><td>3.575e-01</td><td>1.426</td><td>0.1538</td></tr></table>

> g1=garch(garch11.sim,order=c(2,2)) > summary(g1)

Exhibit 12.24 Estimates for GARCH(1,1) Model of a Simulated GARCH(1,1) Series   

<table><tr><td>Coefficient</td><td>Estimate</td><td>Std. Error</td><td>t-value</td><td>Pr(&gt;|t|)</td></tr><tr><td>a0</td><td>0.007575</td><td>0.007590</td><td>0.998</td><td>0.3183</td></tr><tr><td>a1</td><td>0.047184</td><td>0.022308</td><td>2.115</td><td>0.0344</td></tr><tr><td>b1</td><td>0.935377</td><td>0.035839</td><td>26.100</td><td>&lt; 0.0001</td></tr></table>

> g2=garch(garch11.sim,order=c(1,1)) > summary(g2)

Now all coefficient estimates (except a0) are significant. The AIC of the fitted GARCH(2,2) model is 961.0, while that of the fitted GARCH(1,1) model is 958.0,and thus the GARCH(1,1) model provides a better fit to the data. (Here, AIC is defined as minus two times the log-likelihood of the fitted model plus twice the number of parameters. As in the case of ARIMA models, a smaller AIC is preferable.) A $9 5 \%$ confidence interval for a parameter is given (approximately) by the estimate $\pm 1 . 9 6$ times its standard error. So, an approximate $9 5 \%$ confidence interval for ω equals (−0.0073, 0.022), that of $\alpha _ { 1 }$ equals (0.00345, 0.0909), and that of $\beta _ { 1 }$ equals (0.865,1.01). These all contain their true values of 0.02, 0.05, and 0.9, respectively. Note that the standard error of b1 is 0.0358. Since the standard error is approximately proportional to $1 / { \sqrt { n } }$ , the standard error of b1 is expected to be about 0.0566 (0.0462) if the sample size $n$ is 200 (300). Indeed, fitting the GARCH(1,1) model to the first 200 simulated data, b1 was found to equal 0.0603 with standard error equal to 50.39! When the sample size was increased to 300, b1 became 0.935 with standard error equal to 0.0449. This example illustrates that fitting a GARCH model generally requires a large sample size for the theoretical sampling distribution to be valid and useful; see Shephard (1996, p. 10) for a relevant discussion.

For the CREF return data, we earlier identified either a GARCH(1,1) or GARCH(2,2) model. The AIC of the fitted GARCH(1,1) model is 969.6, whereas that of the GARCH(2,2) model is 970.3. Hence the GARCH(1,1) model provides a marginally better fit to the data. Maximum likelihood estimates of the fitted GARCH(1,1) model are reported in Exhibit 12.25.

Exhibit 12.25 Maximum Likelihood Estimates of the GARCH(1,1) Model for the CREF Stock Returns   

<table><tr><td>Parameter</td><td>Estimate†</td><td>Std. Error</td><td>t-value</td><td>Pr(&gt;|t|)</td></tr><tr><td>a0</td><td>0.01633</td><td>0.01237</td><td>1.320</td><td>0.1869</td></tr><tr><td>a1</td><td>0.04414</td><td>0.02097</td><td>2.105</td><td>0.0353</td></tr><tr><td>b1</td><td>0.91704</td><td>0.04570</td><td>20.066</td><td>&lt; 0.0001</td></tr></table>

† As remarked earlier, the analysis depends on the scale of measurement. In particular, a GARCH(1,1) model based on the raw CREF stock returns yields estimates $\mathsf { a 0 } = 0 . 0 0 0 0 0 5 1 1$ , $\mathsf { a } 1 = 0 . 0 9 4 1$ , and ${ \sf b 1 } = 0 . 7 8 9$ .   
> m1=garch( $\mathbf { x } = \mathbf { r }$ .cref,order=c(1,1)) > summary(m1)

Note that the long-term variance of the GARCH(1,1) model is estimated to be

$$
\hat {\omega} / (1 - \hat {\alpha} - \hat {\beta}) = 0. 0 1 6 3 3 / (1 - 0. 0 4 4 1 4 - 0. 9 1 7 0 4) = 0. 4 2 0 6 \tag {12.4.8}
$$

which is very close to the sample variance of 0.4161.

In practice, the innovations need not be normally distributed. In fact, many financial time series appear to have nonnormal innovations. Nonetheless, we can proceed to esti-

mate the GARCH model by pretending that the innovations are normal. The resulting likelihood function is called the Gaussian likelihood, and estimators maximizing the Gaussian likelihood are called the quasi-maximum likelihood estimators (QMLEs). It can be shown that, under some mild regularity conditions, including stationarity, the quasi-maximum likelihood estimators are approximately normal, centered at the true parameter values, and their covariance matrix equals $[ ( \kappa + 2 ) / 2 ] \Lambda$ , where κ is the (excess) kurtosis of the innovations and $\Lambda$ is the covariance matrix assuming the innovations are normally distributed—see the discussion above for the normal case. Note that the heavy-tailedness of the innovations will inflate the covariance matrix and hence result in less reliable parameter estimates. In the case where the innovations are deemed nonnormal, this result suggests a simple way to adjust the standard errors of the quasi-maximum likelihood estimates by multiplying the standard errors of the Gaussian likelihood estimates from a routine that assumes normal innovations by $\sqrt { ( \kappa + 2 ) / 2 }$ , where κ can be substituted with the sample kurtosis of the standardized residuals that are defined below. It should be noted that one disadvantage of QMLE is that the AIC is not strictly applicable.

Let the estimated conditional standard deviation be denoted by $\hat { \sigma } _ { t | t - 1 }$ . The standardized residuals are then defined as

$$
\hat {\varepsilon} _ {t} = r _ {t} / \hat {\sigma} _ {t | t - 1} \tag {12.4.9}
$$

The standardized residuals from the fitted model are proxies for the innovations and can be examined to cast light on the distributional form of the innovations. Once a (parameterized) distribution for the innovations is specified, for example a $t$ -distribution, the corresponding likelihood function can be derived and optimized to obtain maximum likelihood estimators; see Tsay (2005) for details. The price of not correctly specifying the distributional form of the innovation is a loss in efficiency of estimation, although, with large datasets, the computational convenience of the Gaussian likelihood approach may outweigh the loss of estimation efficiency.

# 12.5 Model Diagnostics

Before we accept a fitted model and interpret its findings, it is essential to check whether the model is correctly specified, that is, whether the model assumptions are supported by the data. If some key model assumptions seem to be violated, then a new model should be specified; fitted, and checked again until a model is found that provides an adequate fit to the data. Recall that the standardized residuals are defined as

$$
\hat {\varepsilon} _ {t} = r _ {t} / \hat {\sigma} _ {t | t - 1} \tag {12.5.1}
$$

which are approximately independently and identically distributed if the model is correctly specified. As in the case of model diagnostics for ARIMA models, the standardized residuals are very useful for checking the model specification. The normality assumption of the innovations can be explored by plotting the QQ normal scores plot. Deviations from a straight line pattern in the QQ plot furnish evidence against normality and may provide clues on the distributional form of the innovations. The Shapiro-Wilk

test and the Jarque-Bera test are helpful for formally testing the normality of the innovations.†

For the GARCH(1,1) model fitted to the simulated GARCH(1,1) process, the sample skewness and kurtosis of the standardized residuals equal −0.0882 and −0.104, respectively. Moreover, both the Shapiro-Wilk test and the Jarque-Bera test suggest that the standardized residuals are normal.

For the GARCH(1,1) model fitted to the CREF return data, the standardized residuals are plotted in Exhibit 12.26. There is some tendency for the residuals to be larger in magnitude towards the end of the study period, perhaps suggesting that there is some residual pattern in the volatility. The QQ plot of the standardized residuals is shown in Exhibit 12.27. The QQ plot shows a largely straight-line pattern. The skewness and the kurtosis of the standardized residuals are 0.0341 and 0.205, respectively. The $p$ -value of the Jarque-Bera test equals 0.58 and that of the Shapiro-Wilk test is 0.34. Hence, the normality assumption cannot be rejected.

Exhibit 12.26 Standardized Residuals from the Fitted GARCH(1,1) Model of Daily CREF Returns   
![](images/cbdd4c214c1624b3865b2c387c248cceaccfb4bcf7d43573cc1414625704902a.jpg)  
> plot(residuals(m1),type='h',ylab='Standardized Residuals')

![](images/071aed8508f50b940e3fa22b0140f627bb56290094f62e98ce6b5d5e6222274b.jpg)  
Exhibit 12.27 QQ Normal Scores Plot of Standardized Residuals from the Fitted GARCH(1,1) Model of Daily CREF Returns

> win.graph(width=2.5,height=2.5,pointsize=8) > qqnorm(residuals(m1)); qqline(residuals(m1))

If the GARCH model is correctly specified, then the standardized residuals $\{ \hat { \mathbf { \varepsilon } } _ { t } \}$ should be close to independently and identically distributed. The independently and identically distributed assumption of the innovations can be checked by examining their sample acf. Recall that the portmanteau statistic equals

$$
n \sum_ {k = 1} ^ {m} \hat {\rho} _ {k} ^ {2}
$$

where $\hat { \rho } _ { k }$ is the lag $k$ autocorrelation of the standardized residuals and $n$ is the sample size. (Recall that the same statistic is also known as the Box-Pierce statistic and, in a modified version, the Ljung-Box statistic.) Furthermore, it can be shown that the test statistic is approximately $\bar { x } ^ { 2 }$ distributed with m degrees of freedom under the null hypothesis that the model is correctly specified. This result relies on the fact that the sample autocorrelations of nonzero lags from an independently and identically distributed sequence are approximately independent and normally distributed with zero mean and variance $1 / n$ , and this result holds approximately also for the sample autocorrelations of the standardized residuals if the data are truly generated by a GARCH model of the same orders as those of the fitted model. However, the portmanteau test does not have strong power against uncorrelated and yet serially dependent innovations. In fact, we start out with the assumption that the return data are uncorrelated, so the preceding test is of little interest.

More useful tests may be devised by studying the autocorrelation structure of the absolute standardized residuals or the squared standardized residuals. Let the lag $k$ autocorrelation of the absolute standardized residuals be denoted by $\hat { \rho } _ { k , 1 }$ and that of the squared standardized residuals by $\hat { \rho } _ { k , 2 }$ . Unfortunately, the approximate $\chi ^ { 2 }$ distribution with m degrees of freedom for the corresponding portmanteau statistics based on $\hat { \rho } _ { k , 1 }$ $( \hat { \rho } _ { k , 2 } )$ is no longer valid, the reason being that the estimation of the unknown parame-

ters induces a nonnegligible effect on the tests. Li and Mak (1994) showed that the $\chi ^ { 2 }$ approximate distribution may be preserved by replacing the sum of squared autocorrelations by a quadratic form in the autocorrelations; see also Li (2003). For the absolute standardized residuals, the test statistic takes the form

$$
n \sum_ {i = 1} ^ {m} \sum_ {j = 1} ^ {m} q _ {i, j} \hat {\rho} _ {i, 1} \hat {\rho} _ {j, 1} \tag {12.5.2}
$$

We shall call this modified test statistic the generalized portmanteau test statistic. However, the $q$ ’s depend on $m$ , the number of lags, and they are specific to the underlying true model and so must be estimated from the data. For the squared residuals, the $q$ ’s take different values. See Appendix I on page 318 for the formulas for the $q$ ’s.

We illustrate the generalized portmanteau test with the CREF data. Exhibit 12.28, plots the sample ACF of the squared standardized residuals from the fitted GARCH(1,1) model. The (individual) critical limits in the figure are based on the $1 / n$ nominal variance under the assumption of independently and identically distributed data. As discussed above, this nominal value could be very different from the actual variance of the autocorrelations of the squared residuals even when the model is correctly specified. Nonetheless, the general impression from the figure is that the squared residuals are serially uncorrelated.

Exhibit 12.28 Sample ACF of Squared Standardized Residuals from the GARCH(1,1) Model of the Daily CREF Returns   
![](images/e3becca6ad398d7df571d69ecc68e0c451a94da7ebf4e66d59345ebf3290f295.jpg)  
> acf(residuals(m1)^2,na.action=na.omit)

Exhibit 12.29 displays the $p$ -values of the generalized portmanteau tests with the squared standardized residuals from the fitted GARCH(1,1) model of the CREF data for $m = 1$ to 20. All $p$ -values are higher than $5 \%$ , suggesting that the squared residuals are uncorrelated over time, and hence the standardized residuals may be independent.

Exhibit 12.29 Generalized Portmanteau Test p-Values for the Squared Standardized Residuals for the GARCH(1,1) Model of the Daily CREF Returns   
![](images/d85cdf260e486cf934cd2eae8b5f5365a11813867d1f69aa882a480b747cc86a.jpg)  
> gBox(m1,method='squared')

We repeated checking the model using the absolute standardized residuals—see Exhibits 12.30 and 12.31. The lag 2 autocorrelation of the absolute residuals is significant according to the nominal critical limits shown. Furthermore, the generalized portmanteau tests are significant when $m = 2$ and 3 and marginally not significant at $m = 4$ . The sample EACF table (not shown) of the absolute standardized residuals suggests an AR(2) model for the absolute residuals and hence points to the possibility that the CREF returns may be identified as a GARCH(1,2) process. However, the fitted GARCH(1,2) model to the CREF data did not improve the fit, as its AIC was 978.2—much higher than 969.6, that of the GARCH(1,1) model. Therefore, we conclude that the fitted GARCH(1,1) model provides a good fit to the CREF data.

Exhibit 12.30 Sample ACF of the Absolute Standardized Residuals from the GARCH(1,1) Model for the Daily CREF Returns   
![](images/de54a2d8388cdc7a6b145d39903fa2a9141ec899c02eef900147c47c5aa89960.jpg)  
> acf(abs(residuals(m1)),na.action=na.omit)

Exhibit 12.31 Generalized Portmanteau Test p-Values for the Absolute Standardized Residuals for the GARCH(1,1) Model of the Daily CREF Returns   
![](images/77f6f0e39a171bfacdc8b6f3bba619ee48c6f96f7b67a18e4c5582a1bf7d569d.jpg)  
> gBox(m1,method='absolute')

Given that the GARCH(1,1) model provides a good fit to the CREF data, we may use it to forecast the future conditional variances. Exhibit 12.32 shows the within-sample estimates of the conditional variances, which capture several periods of high volatility, especially the one at the end of the study period. At the final time point, the squared return equals 2.159, and the conditional variance is estimated to be 0.4411. These values combined with Equations (12.3.8) and (12.3.9) can be used to compute the forecasts of future conditional variances. For example, the one-step-ahead forecast of the conditional variance equals $0 . 0 1 6 3 3 + 0 . 0 4 4 1 4 ^ { * } 2 . 1 5 9 + 0 . 9 1 7 0 4 ^ { * } 0 . 4 4 1 1 = 0 . 5 1 6 1$ . The two-step forecast of the conditional variance equals $0 . 0 1 6 3 3 + 0 . 0 4 4 1 4 ^ { * } 0 . 5 1 6 1 +$

$0 . 9 1 7 0 4 ^ { * } 0 . 5 1 6 1 = 0 . 5 1 2 4$ , and so forth, with the longer lead forecasts eventually approaching 0.42066, the long-run variance of the model. The conditional variances may be useful for pricing financial assets through the Black-Scholes formula and calculation of the value at risk (VaR); see Tsay (2005) and Andersen et al. (2006).

It is interesting to note that the need for incorporating ARCH in the data is also supported by the McLeod-Li test applied to the residuals of the $\mathrm { A R } ( 1 ) +$ outlier model; see Exhibit (12.9), page 283.

Exhibit 12.32 Estimated Conditional Variances of the Daily CREF Returns   
![](images/ce4b60b6b29d9c402e15a6abb9c8c2e7904832c91c78e37fc82bf724fa269256.jpg)  
> plot((fitted(m1)[,1])^2,type='l',ylab='Conditional Variance', xlab='t')

# 12.6 Conditions for the Nonnegativity of the Conditional Variances

Because the conditional variance $\sigma _ { t | t - 1 } ^ { 2 }$ must be nonnegative, the GARCH parameters are often constrained to be nonnegative. However, the nonnegativity parameter constraints need not be necessary for the nonnegativity of the conditional variances. This issue was first explored by Nelson and Cao (1992) and more recently by Tsai and Chan (2006). To better understand the problem, first consider the case of an $\operatorname { A R C H } ( q )$ model. Then the conditional variance is given by the formula

$$
\sigma_ {t \mid t - 1} ^ {2} = \omega + \alpha_ {1} r _ {t - 1} ^ {2} + \alpha_ {2} r _ {t - 2} ^ {2} + \dots + \alpha_ {q} r _ {t - q} ^ {2} \tag {12.6.1}
$$

Assume that $q$ consecutive returns can take on any arbitrary set of real numbers. If one of the $\alpha$ ’s is negative, say $\mathbf { \boldsymbol { a } } _ { 1 } < 0$ , then $\sigma _ { t | t - 1 } ^ { 2 }$ will be negative if $r _ { t - 1 } ^ { 2 }$ is sufficiently large and the other r’s are sufficiently close to zero. Hence, it is clear that all $\alpha$ ’s must be nonnegative for the conditional variances to be nonnegative. Similarly, by letting the returns be close to zero, it can be seen that ω must be nonnegative—otherwise the conditional variance may become negative. Thus, it is clear that for an ARCH model, the

non-negativity of all ARCH coefficients is necessary and sufficient for the conditional variances $\sigma _ { t | t - 1 } ^ { 2 ^ { - } }$ to be always nonnegative.

The corresponding problem for a ${ \mathrm { G A R C H } } ( p , q )$ model can be studied by expressing the GARCH model as an infinite-order ARCH model. The conditional variance process $\{ \sigma _ { t | t - 1 } ^ { 2 } \}$ is an $\mathbf { A R M A } ( p , q )$ $( p , q )$ model with the squared returns playing the role of the noise process. Recall that an $\mathbf { A R M A } ( p , q )$ model can be expressed as an $\mathbf { M A } ( \infty )$ model if all the roots of the AR characteristic polynomial lie outside the unit circle. Hence, assuming that all the roots of $1 - \beta _ { 1 } x - \hat { \beta _ { 2 } } x ^ { 2 } - \cdots - \beta _ { p } x ^ { p } = 0$ have magnitude greater than 1, the conditional variances satisfy the equation

$$
\sigma_ {t \mid t - 1} ^ {2} = \omega^ {*} + \psi_ {1} r _ {t - 1} ^ {2} + \psi_ {2} r _ {t - 2} ^ {2} + \dots \tag {12.6.2}
$$

where

$$
\omega^ {*} = \omega / \left(1 - \sum_ {i = 1} ^ {p} \beta_ {i}\right) \tag {12.6.3}
$$

It can be similarly shown that the conditional variances are all nonnegative if and only if $\omega ^ { * }$ and $\psi _ { j } \geq 0$ for all integers $j \geq 1$ . The coefficients in the ARCH(∞) representation relate to the parameters of the GARCH model through the equality

$$
\frac {\alpha_ {1} B + \cdots + \alpha_ {q} B ^ {q}}{1 - \beta_ {1} B - \cdots - \beta_ {p} B ^ {p}} = \psi_ {1} B + \psi_ {2} B ^ {2} + \dots \tag {12.6.4}
$$

If $p = 1$ , then it can be easily checked that $\psi _ { k } = \beta _ { 1 } \psi _ { k - 1 }$ for $k > q$ . Thus, $\psi _ { j } \geq 0$ for all $j \geq 1$ if and only if $\beta _ { 1 } \geq 0$ and $\Psi _ { 1 } \geq 0 , . . . , \Psi _ { q } \geq 0$ . For higher GARCH order, the situation is more complex. Let $\lambda _ { j } ,$ $1 \leq j \leq p$ , be the roots of the characteristic equation

$$
1 - \beta_ {1} x - \dots - \beta_ {p} x ^ {p} = 0 \tag {12.6.5}
$$

With no loss of generality, we can and shall henceforth assume the convention that

$$
\left| \lambda_ {1} \right| \leq \left| \lambda_ {2} \right| \leq \dots \leq \left| \lambda_ {p} \right| \tag {12.6.6}
$$

Let $i = \sqrt { - 1 }$ and $\bar { \lambda }$ denote the complex conjugate of $\gimel$ , $B ( x ) = 1 - \beta _ { 1 } x - \cdots - \beta _ { p } x ^ { p } ;$ and $B ^ { ( 1 ) }$ be the first derivative of $B$ . We then have the following result.

Result 1: Consider a ${ \mathrm { G A R C H } } ( p , q )$ model where $p \geq 2$ . Assume A1, that all the roots of the equation

$$
1 - \beta_ {1} x - \beta_ {2} x ^ {2} - \dots - \beta_ {p} x ^ {p} = 0 \tag {12.6.7}
$$

have magnitude greater than 1, and A2, that none of these roots satisfy the equation

$$
\alpha_ {1} x + \dots + \alpha_ {q} x ^ {q} = 0 \tag {12.6.8}
$$

Then the following hold:

(a) $\mathbf { \omega } ^ { * } \geq 0$ if and only if $\Theta \ge 0$ .

(b) Assuming the roots $\lambda _ { 1 } , . . . , \lambda _ { p }$ are distinct, and $\left| \lambda _ { 1 } \right| < \left| \lambda _ { 2 } \right|$ , then the conditions given in Equation (12.6.9) are necessary and sufficient for $\psi _ { k } \ge 0$ for all positive integers $k$ :

$$
\left. \begin{array}{l} \lambda_ {1} \text {i s r e a l a n d} \lambda_ {1} > 1 \\ \alpha \left(\lambda_ {1}\right) > 0 \\ \psi_ {k} \geq 0 \text {f o r} k = 1, \dots , k ^ {*} \end{array} \right\} \tag {12.6.9}
$$

where $k ^ { * }$ is the smallest integer greater than or equal to

$$
\begin{array}{l} \frac {\log (r _ {1}) - \log [ (p - 1) r ^ {*} ]}{\log (\left| \lambda_ {1} \right|) - \log (\left| \lambda_ {2} \right|)}, \\ r _ {j} = - \frac {\alpha (\lambda_ {j})}{B ^ {(1)} (\lambda_ {j})}, \quad \text {f o r} 1 \leq j \leq p, \quad \text {a n d} \quad r ^ {*} = \max  _ {2 \leq j \leq p} (| r _ {j} |) \tag {12.6.10} \\ \end{array}
$$

For $p = 2$ , the $k ^ { * }$ defined in Result 1 can be shown to be $q + 1$ ; see Theorem 2 of Nelson and Cao (1992). If the $k ^ { * }$ defined in Equations (12.6.10) is a negative number, then it can be seen from the proof given in Tsai and Chan (2006) that $\psi _ { k } \geq 0$ for all positive $k$ .

Tsai and Chan (2006) have also derived some more readily verifiable conditions for the conditional variances to be always nonnegative.

Result 2: Let the assumptions of Result 1 be satisfied. Then the following hold:

(a) For a ${ \mathrm { G A R C H } } ( p , 1 )$ model, if $\lambda _ { j }$ is real and $\lambda _ { j } > 1$ , for $j = 1 , . . . , p$ , and $\alpha _ { 1 } \geq 0$ , then $\psi _ { k } \ge 0$ for all positive integers $k$ .   
(b) For a GARCH(p,1) model, if $\psi _ { k } \ge 0$ for all positive integers $k$ , then $\mathbf { \boldsymbol { a } } _ { 1 } \geq 0$ , $\sum _ { j = 1 } ^ { p } \lambda _ { j } ^ { - 1 } \geq 0 , \lambda _ { 1 }$ λj is real, and $\lambda _ { 1 } > 1$ .   
(c) For a GARCH(3,1) model, $\psi _ { k } \ge 0$ for all positive integers $k$ if and only if $\alpha _ { 1 }$ $\geq 0$ and either of the following cases hold:

Case 1. All the $\lambda _ { j }$ ’s are real numbers, $\lambda _ { 1 } > 1$ , and $\lambda _ { 1 } ^ { - 1 } + \lambda _ { 2 } ^ { - 1 } + \lambda _ { 3 } ^ { - 1 } \geq 0$

Case 2. $\lambda _ { 1 } > 1$ , and $\lambda _ { 2 } = \overline { { { \lambda } } } _ { 3 } = | \lambda _ { 2 } | e ^ { i \theta } = a + b i$ , where $a$ and $b$ are real numbers, $b > 0$ , and $0 < \Theta < \pi$ :

Case 2.1. $\theta = 2 \pi / r$ for some integer $r \geq 3$ , and $1 < \lambda _ { 1 } \leq | \lambda _ { 2 } |$ .

Case $2 . 2 . \ : \Theta \not \in \{ 2 \pi / r \mid r = 3 , 4 , \ldots \}$ , and $| \lambda _ { 2 } | / \lambda _ { 1 } \ge x _ { 0 } > 1$ , where $x _ { 0 }$ is the largest real root of $f _ { n , \theta } ( x ) = 0$ , and

$$
f _ {n, \theta} (x) = x ^ {n + 2} - x \frac {\sin [ (n + 2) \theta ]}{\sin \theta} + \frac {\sin [ (n + 1) \theta ]}{\sin \theta} \tag {12.6.11}
$$

where $n$ is the smallest positive integer such that $\sin ( ( n + 1 ) \theta ) < 0$ and $\sin ( ( n + 2 ) \theta )$ $> 0$ .

(d) For a GARCH(3,1) model, if $\lambda _ { 2 } = \overline { { { \lambda } } } _ { 3 } = | \lambda _ { 2 } | e ^ { i \theta } = a + b i$ , where $a$ and $^ b$ are real numbers, $b > 0$ , and $a \geq \lambda _ { 1 } > 1$ , then $\psi _ { k } \ge 0$ for all positive integers $k$ .   
(e) For a GARCH(4,1) model, if the $\lambda _ { j }$ ’s are real for $1 \le j \le 4$ , then a necessary and sufficient condition for $\{ \boldsymbol \psi _ { i } \} _ { i = 0 } ^ { \infty ^ { \prime } }$ to be nonnegative is that $\mathbf { a } _ { 1 } \geq 0$ , $\lambda _ { 1 } ^ { - 1 } + \lambda _ { 2 } ^ { - 1 } + \lambda _ { 3 } ^ { - 1 } + \lambda _ { 4 } ^ { - 1 } \ge 0$ , and $\lambda _ { 1 } > 1$ .

Note that $x _ { 0 }$ is the only real root of Equation (12.6.11) that is greater than or equal to 1. Also, Tsai and Chan (2006) proved that if the ARCH coefficients $( \vec { \alpha } ^ { \prime } \mathbf { s } )$ o f a ${ \mathrm { G A R C H } } ( p , q )$ model are all nonnegative, the model has nonnegative conditional variances if the nonnegativity property holds for the associated ${ \mathrm { G A R C H } } ( p , 1 )$ models with a nonnegative $\alpha _ { 1 }$ coefficient.

# 12.7 Some Extensions of the GARCH Model

The GARCH model may be generalized in several directions. First, the GARCH model assumes that the conditional mean of the time series is zero. Even for financial time series, this strong assumption need not always hold. In the more general case, the conditional mean structure may be modeled by some $\mathrm { A R M A } ( u , \nu )$ model, with the white noise term of the ARMA model modeled by some ${ \mathrm { G A R C H } } ( p , q )$ model. Specifically, let $\{ Y _ { t } \}$ be a time series given by (now we switch to using the notation $Y _ { t }$ to denote a general time series)

$$
\left. \begin{array}{c} Y _ {t} = \phi_ {1} Y _ {t - 1} + \dots + \phi_ {u} Y _ {t - u} \theta_ {0} + e _ {t} + \theta_ {1} e _ {t - 1} + \dots + \theta_ {v} e _ {t - v} \\ e _ {t} = \sigma_ {t | t - 1} \varepsilon_ {t} \\ \sigma_ {t | t - 1} ^ {2} = \omega + \alpha_ {1} e _ {t - 1} ^ {2} + \dots + \alpha_ {q} e _ {t - q} ^ {2} + \beta_ {1} \sigma_ {t - 1 | t - 2} ^ {2} + \dots + \beta_ {p} \sigma_ {t - p | t - p - 1} ^ {2} \end{array} \right\} \tag {12.7.1}
$$

and where we have used the plus convention in the MA parts of the model. The ARMA orders can be identified based on the time series $\{ Y _ { t } \}$ , whereas the GARCH orders may be identified based on the squared residuals from the fitted ARMA model. Once the orders are identified, full maximum likelihood estimation for the ARMA $^ +$ GARCH model can be carried out by maximizing the likelihood function as defined in Equation (12.4.4) on page 298 but with $r _ { t }$ there replaced by $e _ { t }$ that are recursively computed according to Equation (12.7.1). The maximum likelihood estimators of the ARMA parameters are approximately independent of their GARCH counterparts if the innovations $\varepsilon _ { t }$ have a symmetric distribution (for example, a normal or t-distribution) and their standard errors are approximately given by those in the pure ARMA case. Likewise, the GARCH parameter estimators enjoy distributional results similar to those for the pure GARCH case. However, the ARMA estimators and the GARCH estimators are correlated if the innovations have a skewed distribution. In the next section, we illustrate the ARMA + GARCH model with the daily exchange rates of the U.S. dollar to the Hong Kong dollar.

Another direction of generalization concerns nonlinearity in the volatility process. For financial data, this is motivated by a possible asymmetric market response that may,

for example, react more strongly to a negative return than a positive return of the same magnitude. The idea can be simply illustrated in the setting of an ARCH(1) model, where the asymmetry can be modeled by specifying that

$$
\sigma_ {t \mid t - 1} ^ {2} = \omega + \alpha e _ {t - 1} ^ {2} + \gamma \min  \left(e _ {t - 1}, 0\right) ^ {2} \tag {12.7.2}
$$

Such a model is known as a GJR model—a variant of which allows the threshold to be unknown and other than 0. See Tsay (2005) for other useful extensions of the GARCH models.

# 12.8 Another Example: The Daily USD/HKD Exchange Rates

As an illustration for the ARIMA $^ +$ GARCH model, we consider the daily USD/HKD (U.S. dollar to Hong Kong dollar) exchange rate from January 1, 2005 to March 7, 2006, altogether 431 days of data. The returns of the daily exchange rates are shown in Exhibit 12.33 and appear to be stationary, although volatility clustering is evident in the plot.

![](images/6c1e0ad544c80f4a32fd0eb7c2b171cd30bb15613d26c3a6c8110cf285b7651e.jpg)  
Exhibit 12.33 Daily Returns of USD/HKD Exchange Rate: 1/1/05–3/7/06

```txt
> data(USD.hkd)  
> plot(ts(USD.hkd$hkrate, freq=1), type='l', xlab='Day', ylab='Return') 
```

It is interesting to note that the need for incorporating ARCH in the data is also supported by the McLeod-Li test applied to the residuals of the $\mathrm { A R } ( 1 ) +$ outlier model; see below for further discussion of the additive outlier. Exhibit 12.34 shows that the tests are all significant when the number of lags of the autocorrelations of the squared residuals ranges from 1 to 26, displaying strong evidence of conditional heteroscedascity.

![](images/45e2c17912b8871e41e00610c89ab04caa52fb46651a021812e6e7a3f6e1e35e.jpg)  
Exhibit 12.34 McLeod-Li Test Statistics for the USD/HKD Exchange Rate

```txt
> attach(USD.hkd)  
> McLeod.Li.test(arima(hkrate, order=c(1,0,0), xreg=data.frame(outlier1))) 
```

An $\mathrm { A R } ( 1 ) + \mathrm { G A R C H } ( 3 , 1 )$ model was fitted to the (raw) return data with an additive outlier one day after July 22, 2005, the date when China revalued the yuan by $2 . 1 \%$ and adopted a floating-rate system for it. The outlier is shaded in gray in Exhibit 12.33. The intercept term in the conditional mean function was found to be insignificantly different from zero and hence is omitted from the model. Thus we take the returns to have zero mean unconditionally. The fitted model has an $\mathrm { A I C } = - 2 0 7 0 . 9$ , being smallest among various competing (weakly) stationary models—see Exhibit 12.35. Interestingly, for lower GARCH orders $( p \leq 2 )$ , the fitted models are nonstationary, but the fitted models are largely stationary when the GARCH order is higher than 2. As the data appear to be stationary, we choose the $\mathrm { A R } ( 1 ) + \mathrm { G A R C H } ( 3 , 1 )$ model as the final model.

The $\mathrm { A R + G A R C H }$ models partially reported in Exhibit 12.35 were fitted using the Proc Autoreg routine in the SAS software.† We used the default option of imposing that the Nelson-Cao inequality constraints for the GARCH conditional variance process be nonnegative. However, the inequality constraints so imposed are only necessary and sufficient for the nonnegativity of the conditional variances of a ${ \mathrm { G A R C H } } ( p , q )$ model for $p$ $\leq 2$ . For higher-order GARCH models, Proc Autoreg imposes the constraints that (1) $\psi _ { k }$ $\geq 0$ , $1 \leq k \leq \operatorname* { m a x } ( q - 1 , p ) + 1$ and (2) the nonnegativity of the in-sample conditional variances; see the SAS 9.1.3 Help and Documentation manual. Hence, higher-order GARCH models estimated by Proc Autoreg with the Nelson-Cao option need not have nonnegative conditional variances with probability one.

Exhibit 12.35 AIC Values for Various Fitted Models for the Daily Returns of the USD/HKD Exchange Rate   

<table><tr><td>AR order</td><td>GARCH order (p)</td><td>ARCH order (q)</td><td>AIC</td><td>Stationarity</td></tr><tr><td>0</td><td>3</td><td>1</td><td>-1915.3</td><td>nonstationary</td></tr><tr><td>1</td><td>1</td><td>1</td><td>-2054.3</td><td>nonstationary</td></tr><tr><td>1</td><td>1</td><td>2</td><td>-2072.5</td><td>nonstationary</td></tr><tr><td>1</td><td>1</td><td>3</td><td>-2051.0</td><td>nonstationary</td></tr><tr><td>1</td><td>2</td><td>1</td><td>-2062.2</td><td>nonstationary</td></tr><tr><td>1</td><td>2</td><td>2</td><td>-2070.5</td><td>nonstationary</td></tr><tr><td>1</td><td>2</td><td>3</td><td>-2059.2</td><td>nonstationary</td></tr><tr><td>1</td><td>3</td><td>1</td><td>-2070.9</td><td>stationary</td></tr><tr><td>1</td><td>3</td><td>2</td><td>-2064.8</td><td>stationary</td></tr><tr><td>1</td><td>3</td><td>3</td><td>-2062.8</td><td>stationary</td></tr><tr><td>1</td><td>4</td><td>1</td><td>-2061.7</td><td>nonstationary</td></tr><tr><td>1</td><td>4</td><td>2</td><td>-2054.8</td><td>stationary</td></tr><tr><td>1</td><td>4</td><td>3</td><td>-2062.4</td><td>stationary</td></tr><tr><td>2</td><td>3</td><td>1</td><td>-2066.6</td><td>stationary</td></tr></table>

For the Hong Kong exchange rate data, the fitted model from Proc Autoreg is listed in Exhibit 12.37 with the estimated conditional variances shown in Exhibit 12.36. Note that the GARCH2 $( \beta _ { 2 } )$ coefficient estimate is negative.

Exhibit 12.36 Estimated Conditional Variances of the Daily Returns of USD/HKD Exchange Rate from the Fitted AR(1) $^ +$ GARCH(3,1) Model   
![](images/f890dfa17820b3c2701e348137645d67efdd44bbc2ae0fce87ad68e8e7c245aa.jpg)  
> plot(ts(usd.hkd$v,freq=1),type='l',xlab='Day', ylab='Conditional Variance')

Since both the intercept and the ARCH coefficient are positive, we can apply part (c) of Result 2 to check whether or not the conditional variance process defined by the fitted model is always nonnegative. The characteristic equation $1 \dot { - } \beta _ { 1 } x - \beta _ { 2 } x ^ { 2 } - \beta _ { 3 } x ^ { \prime } = 0$ admits three roots equal to 1.153728 and $- 0 . 4 8 3 2 9 4 \pm 1 . 2 2 1 4 7 4 i$ . Thus $\lambda _ { 1 } = 1 . 1 5 3 7 2 8$ and $| \lambda _ { 2 } | / \lambda _ { 1 } = 1 . 1 3 8 5 7 9$ . Based on numerical computations, $n$ in Equation (12.6.11) turns out to be 2 and Equation (12.6.11) has one real root equal to 1.1385751 which is strictly less than $1 . 1 3 8 5 7 9 = | \lambda _ { 2 } | / \lambda _ { 1 }$ . Hence, we can conclude that the fitted model always results in nonnegative conditional variances.

Exhibit 12.37 Fitted AR(1) + ARCH(3,1) Model for Daily Returns of USD/HKD Exchange Rate   

<table><tr><td>Coefficient</td><td>Estimate</td><td>Std. error</td><td>t-ratio</td><td>p-value</td></tr><tr><td>AR1</td><td>0.1635</td><td>0.005892</td><td>21.29</td><td>0.0022</td></tr><tr><td>ARCH0 (ω)</td><td>2.374×10-5</td><td>6.93×10-6</td><td>3.42</td><td>0.0006</td></tr><tr><td>ARCH1 (α1)</td><td>0.2521</td><td>0.0277</td><td>9.09</td><td>&lt; 0.0001</td></tr><tr><td>GARCH1 (β1)</td><td>0.3066</td><td>0.0637</td><td>4.81</td><td>&lt; 0.0001</td></tr><tr><td>GARCH2 (β2)</td><td>-0.09400</td><td>0.0391</td><td>-2.41</td><td>0.0161</td></tr><tr><td>GARCH3 (β3)</td><td>0.5023</td><td>0.0305</td><td>16.50</td><td>&lt; 0.0001</td></tr><tr><td>Outlier</td><td>-0.1255</td><td>0.00589</td><td>-21.29</td><td>&lt; 0.0001</td></tr></table>

```txt
> SAS code: data hkex; inline 'hkrate.dat'; input hkrate;  
outlier1=0;  
day+1; if day=203 then outlier1=1;  
proc autoreg data=hkex;  
model hkrate=outlier1 /noint nlag=1 garch=(p=3,q=1)  
maxiter=200 archtest;  
/*hetero outlier /link=linear;*/  
output out=a cev=v residual=r;  
run; 
```

# 12.9 Summary

This chapter began with a brief description of some terms and issues associated with financial time series. Autoregressive conditional heteroscedasticity (ARCH) models were then introduced in an attempt to model the changing variance of a time series. The ARCH model of order 1 was thoroughly explored from identification through parameter estimation and prediction. These models were then generalized to the generalized autoregressive conditional heteroscedasticity, $\mathrm { G A R C H } ( p , q )$ , model. The GARCH models were also thoroughly explored with respect to identification, maximum likelihood estimation, prediction, and model diagnostics. Examples with both simulated and real time series data were used to illustrate the ideas.

# EXERCISES

12.1 Display the time sequence plot of the absolute returns for the CREF data. Repeat the plot with the squared returns. Comment on the volatility patterns observed in these plots. (The data are in file named CREF.)   
12.2 Plot the time sequence plot of the absolute returns for the USD/HKD exchange rate data. Repeat the plot with the squared returns. Comment on the volatility patterns observed in these plots. (The data are in the file named usd.hkd.)   
12.3 Use the definition $\eta _ { t } \dot { = } r _ { t } ^ { 2 } - \sigma _ { t | t - 1 } ^ { 2 }$ [Equation (12.2.4) on page 287] and show that $\{ \boldsymbol \eta _ { t } \}$ is a serially uncorrelated sequence. Show also that $\boldsymbol { \mathsf { \Pi } } _ { \boldsymbol { \mathsf { I } } _ { t } }$ is uncorrelated with past squared returns, that is, show that $C o r r ( \mathfrak { \eta } _ { t } , \mathfrak { r } ^ { 2 } { } _ { t - k } ) = 0$ for $k > 0$ .   
12.4 Substituting $\sigma _ { t | t _ { - } 1 } ^ { 2 } = r _ { t } ^ { 2 } - \eta _ { t }$ into Equation (12.2.2) on page 285 show the algebra that leads to Equation (12.2.5) on page 287.   
12.5 Verify Equation (12.2.8) on page 288.   
12.6 Without doing any theoretical calculations, order the kurtosis values of the following four distributions in ascending order: the t-distribution with 10 DF, the $t$ -distribution with 30 DF, the uniform distribution on [−1,1], and the normal distribution with mean 0 and variance 4. Explain your answer.   
12.7 Simulate a GARCH(1,1) process with $\alpha = 0 . 1$ and $\beta = 0 . 8$ and of length 500. Plot the time series and inspect its sample ACF, PACF, and EACF. Are the data consistent with the assumption of white noise?

(a) Square the data and identify a GARCH model for the raw data based on the sample ACF, PACF, and EACF of the squared data.   
(b) Identify a GARCH model for the raw data based on the sample ACF, PACF and EACF of the absolute data. Discuss and reconcile any discrepancy between the tentative model identified with the squared data and that with the absolute data.   
(c) Perform the McLeod-Li test on your simulated series. What do you conclude?   
(d) Repeat the exercise but now using only the first 200 simulated data. Discuss your findings.

12.8 The file cref.bond contains the daily price of the CREF bond fund from August 26, 2004 to August, 15, 2006. These data are available only on trading days, but proceed to analyze the data as if they were sampled regularly.

(a) Display the time sequence plot of the daily bond price data and comment on the main features in the data.   
(b) Compute the daily bond returns by log-transforming the data and then computing the first differences of the transformed data. Plot the daily bond returns, and comment on the result.   
(c) Perform the McLeod-Li test on the returns series. What do you conclude?   
(d) Show that the returns of the CREF bond price series appear to be independently and identically distributed and not just serially uncorrelated; that is, there is no discernible volatility clustering.

12.9 The daily returns of Google stock from August 20, 2004 to September 13, 2006 are stored in the file named google.

(a) Display the time sequence plot for the return data and show that the data are essentially uncorrelated over time.   
(b) Compute the mean of the Google daily returns. Does it appear to be significantly different from 0?   
(c) Perform the McLeod-Li test on the Google daily returns series. What do you conclude?   
(d) Identify a GARCH model for the Google daily return data. Estimate the identified model and perform model diagnostics with the fitted model.   
(e) Draw and comment on the time sequence plot of the estimated conditional variances.   
(f) Plot the QQ normal plot for the standardized residuals from the fitted model. Do the residuals appear to be normal? Discuss the effects of the normality on the model fit, for example, regarding the computation of the confidence interval.   
(g) Construct a $9 5 \%$ confidence interval for b1.   
(h) What are the stationary mean and variance according to the fitted GARCH model? Compare them with those of the data.   
(i) Based on the GARCH model, construct the $9 5 \%$ prediction intervals for $h$ -step-ahead forecast, for $h = 1$ , 2,…, 5.

12.10 In Exercise 11.21 on page 276, we investigated the existence of outliers with the logarithms of monthly oil prices within the framework of an IMA(1,1) model. Here, we explore the effects of “outliers” on the GARCH specification. The data are in the file named oil.price.

(a) Based on the sample ACF, PACF, and EACF of the absolute and squared residuals from the fitted IMA(1,1) model (without outlier adjustment), show that a GARCH(1,1) model may be appropriate for the residuals.   
(b) Fit an IMA(1,1) $^ +$ GARCH(1,1) model to the logarithms of monthly oil prices.   
(c) Draw the time sequence plot for the standardized residuals from the fitted $\mathrm { I M A } ( 1 , 1 ) + \mathrm { G A R C H } ( 1 , 1 )$ model. Are there any outliers?   
(d) For the log oil prices, fit an IMA(1,1) model with two IOs at $t = 2$ and $t = 5 6$ and an AO at $t = 8$ . Show that the residuals from the IMA plus outlier model appear to be independently and identically distributed and not just serially uncorrelated; that is, there is no discernible volatility clustering.   
(e) Between the outlier and the GARCH model, which one do you think is more appropriate for the oil price data? Explain your answer.

# Appendix I: Formulas for the Generalized Portmanteau Tests

We first present the formula for $Q = ( q _ { i , j } )$ for the case where the portmanteau test is based on the squared standardized residuals. Readers may consult Li and Mak (1994) for proofs of the formulas. Let θ denote the vector of GARCH parameters. For example, for a GARCH(1,1) model,

$$
\theta = \left[ \begin{array}{l} \omega \\ \alpha \\ \beta \end{array} \right] \tag {12.I.1}
$$

Write the ith component of θ as $\theta _ { i }$ so that $\theta _ { 1 } = \omega$ , $\theta _ { 2 } = \alpha$ , and $\theta _ { 3 } = \beta$ for the GARCH(1,1) model. In the general case, let $k = p + q + 1$ be the number of GARCH parameters. Let $J$ be an $m { \times } k$ matrix whose $( i , j )$ th element equals

$$
\frac {1}{n} \sum_ {t = i + 1} ^ {n} \frac {1}{\sigma_ {t | t - 1} ^ {2}} \frac {\partial \sigma_ {t | t - 1} ^ {2}}{\partial \theta_ {j}} \left(\varepsilon_ {t - i} ^ {2} - 1\right) \tag {12.I.2}
$$

and $\Lambda$ be the $k { \times } k$ covariance matrix of the approximate normal distribution of the maximum likelihood estimator of θ for the model assuming normal innovations; see Section 12.4. Let $Q = ( q _ { i , j } )$ be the matrix of the $q$ ’s appearing in the quadratic form of the generalized portmanteau test. It can be shown that the matrix $Q$ equals

$$
\left[ I - \frac {1}{2 (\kappa + 2)} J \Lambda J ^ {T} \right] ^ {- 1} \tag {12.I.3}
$$

where $I$ is the $m { \times } m$ identity matrix, $\boldsymbol { \kappa }$ is the (excess) kurtosis of the innovations, $J ^ { T }$ is the transpose of $J$ , and the superscript $^ { - 1 }$ denotes the matrix inverse.

Next, we present the formulas for the case where the tests are computed based on the absolute standardized residuals. In this case, the $( i , j )$ th element of the $J$ matrix equals

$$
\frac {1}{n} \sum_ {t} \frac {1}{\sigma_ {t \mid t - 1} ^ {2}} \frac {\partial \sigma_ {t \mid t - 1} ^ {2}}{\partial \theta_ {j}} \left(\left| \varepsilon_ {t - i} \right| - \tau\right) \tag {12.I.4}
$$

where $\tau = E ( \left. \boldsymbol { \varepsilon } _ { t } \right. )$ , and $Q$ equals

$$
\left[ I - \frac {- [ (\kappa + 2) \tau^ {2} ] / 8 + \tau (\nu - \tau)}{(1 - \tau^ {2}) ^ {2}} J \Lambda J ^ {T} \right] ^ {- 1} \tag {12.I.5}
$$

with $\mathbf { v } = E ( |  { \varepsilon } _ { t } ^ { 3 } | )$ .

# CHAPTER 13

# INTRODUCTION TO SPECTRAL ANALYSIS

Historically, spectral analysis began with the search for “hidden periodicities” in time series data. Chapter 3 discussed fitting cosine trends at various known frequencies to series with strong cyclical trends. In addition, the random cosine wave example in Chapter 2 on page 18, showed that it is possible for a stationary process to look very much like a deterministic cosine wave. We hinted in Chapter 3 that by using enough different frequencies with enough different amplitudes (and phases) we might be able to model nearly any stationary series.† This chapter pursues those ideas further with an introduction to spectral analysis. Previous to this chapter, we concentrated on analyzing the correlation properties of time series. Such analysis is often called time domain analysis. When we analyze frequency properties of time series, we say that we are working in the frequency domain. Frequency domain analysis or spectral analysis has been found to be especially useful in acoustics, communications engineering, geophysical science, and biomedical science, for example.

# 13.1 Introduction

Recall from Chapter 3 the cosine curve with equation‡

$$
R \cos (2 \pi f t + \Phi) \tag {13.1.1}
$$

Remember that $R \left( > 0 \right)$ is the amplitude, $f$ the frequency, and $\Phi$ the phase of the curve. Since the curve repeats itself exactly every $1 / f$ time units, 1/f is called the period of the cosine wave.

Exhibit 13.1 displays two discrete-time cosine curves with time running from 1 to 96. We would only see the discrete points, but the connecting line segments are added to help our eyes follow the pattern. The frequencies are 4/96 and 14/96, respectively. The lower-frequency curve has a phase of zero, but the higher-frequency curve is shifted by a phase of $0 . 6 \pi$ .

Exhibit 13.2 shows the graph of a linear combination of the two cosine curves with a multiplier of 2 on the low-frequency curve and a multiplier of 3 on the higher-frequency curve and a phase of $0 . 6 \pi$ ; that is,

![](images/3ff5ded25afc16b7049d438a2dad5a9ab4ec61eb5164f7e16b030f1a552c9a9c.jpg)  
Exhibit 13.1 Cosine Curves with $\pmb { n = 9 6 }$ and Two Frequencies and Phases

```matlab
> win.graph(width=4.875,height=2.5,pointsize=8)  
> t=1:96; cos1=cos(2*pi*t*4/96); cos2=cos(2*pi*(t*14/96+.3))  
> plot(t,cos1,type='o',ylab='Cosines')  
> lines(t,cos2,lty='dotted',type='o',pch=4) 
```

$$
Y _ {t} = 2 \cos \left(2 \pi t \frac {4}{9 6}\right) + 3 \cos \left[ 2 \pi \left(t \frac {1 4}{9 6} + 0. 3\right) \right] \tag {13.1.2}
$$

Now the periodicity is somewhat hidden. Spectral analysis provides tools for discovering the “hidden” periodicities quite easily. Of course, there is nothing random in this time series.

![](images/c27d514142d5c0b724cb602f3a74f2392209ac484ac6598c6370cb74ceee874c.jpg)  
Exhibit 13.2 Linear Combination of Two Cosine Curves

```javascript
> y=2\*cos1+3\*cos2; plot(t,y,type='o',ylab=expression(y[t])) 
```

As we saw earlier, Equation (13.1.1) is not convenient for estimation because the parameters $R$ and $\Phi$ do not enter the expression linearly. Instead, we use a trigonometric identity to reparameterize Equation (13.1.1) as

$$
R \cos (2 \pi f t + \Phi) = A \cos (2 \pi f t) + B \sin (2 \pi f t) \tag {13.1.3}
$$

where

$$
R = \sqrt {A ^ {2} + B ^ {2}}, \quad \Phi = \operatorname {a t a n} (- B / A) \tag {13.1.4}
$$

and, conversely,

$$
A = R \cos (\Phi), \quad B = - R \sin (\Phi) \tag {13.1.5}
$$

Then, for a fixed frequency $f ,$ , we can use $\cos ( 2 \pi f t )$ and $\sin ( 2 \pi f t )$ as predictor variables and fit the A’s and $B$ ’s from the data using ordinary least squares regression.

A general linear combination of $m$ cosine curves with arbitrary amplitudes, frequencies, and phases could be written as†

$$
Y _ {t} = A _ {0} + \sum_ {j = 1} ^ {m} \left[ A _ {j} \cos \left(2 \pi f _ {j} t\right) + B _ {j} \sin \left(2 \pi f _ {j} t\right) \right] \tag {13.1.6}
$$

Ordinary least squares regression can be used to fit the A’s and $B$ ’s, but when the frequencies of interest are of a special form, the regressions are especially easy. Suppose that $n$ is odd and write $n = 2 k + 1$ . Then the frequencies of the form 1/n, 2/n,…, k/n $( = 1 / 2 - 1 / ( 2 n ) )$ are called the Fourier frequencies. The cosine and sine predictor variables at these frequencies (and at $f = 0$ ) are known to be orthogonal,‡ and the least squares estimates are simply

$$
\hat {A} _ {0} = \bar {Y} \tag {13.1.7}
$$

$$
\hat {A} _ {j} = \frac {2}{n} \sum_ {t = 1} ^ {n} Y _ {t} \cos (2 \pi t j / n) \quad \text {a n d} \quad \hat {B} _ {j} = \frac {2}{n} \sum_ {t = 1} ^ {n} Y _ {t} \sin (2 \pi t j / n) \tag {13.1.8}
$$

If the sample size is even, say $n = 2 k$ , Equations (13.1.7) and (13.1.8) still apply for $j = 1 , 2 , . . . , k - 1$ , but

$$
\hat {A} _ {k} = \frac {1}{n} \sum_ {t = 1} ^ {n} (- 1) ^ {t} Y _ {t} \text {a n d} \hat {B} _ {k} = 0 \tag {13.1.9}
$$

Note that here $f _ { k } = k / n = \nu _ { 2 }$ .

If we were to apply these formulas to the series shown in Exhibit 13.2, we would obtain perfect results. That is, at frequency $f _ { 4 } = 4 / 9 6$ , we obtain $\hat { A } _ { 4 } = 2$ and ${ \hat { B } } _ { 4 } = 0$ , and at frequency $f _ { 1 4 } = 1 4 / 9 6$ , we obtain $\hat { A } _ { 1 4 } = - 0 . 9 2 7 0 5 1$ and $\hat { B } _ { 1 4 } = - 2 . 8 5 3 1 7$ . We would obtain estimates of zero for the regression coefficients at all other frequencies. These

results obtain because there is no randomness in this series and the cosine-sine fits are exact.

Note also that any series of any length $n$ , whether deterministic or stochastic and with or without any true periodicities, can be fit perfectly by the model in Equation (13.1.6) by choosing $m = n / 2$ if $n$ is even and $m = ( n - 1 ) / 2$ if $n$ is odd. There are then $n$ parameters to adjust (estimate) to fit the series of length $n$ .

# 13.2 The Periodogram

For odd sample sizes with $n = 2 k + 1$ , the periodogram $I$ at frequency $f = j / n$ for $j = 1$ , $2 , . . . , k$ , is defined to be

$$
I \left(\frac {j}{n}\right) = \frac {n}{2} \left(\hat {A} _ {j} ^ {2} + \hat {B} _ {j} ^ {2}\right) \tag {13.2.1}
$$

If the sample size is even and $n = 2 k$ , Equations (13.1.7) and (13.1.8) still give the $\hat { A }$ ’s and $\hat { B }$ ’s and Equation (13.2.1) gives the periodogram for $j = 1$ , 2,…, k − 1. However, at the extreme frequency $f = k / n = \%$ , Equations (13.1.9) apply and

$$
I \left(\frac {1}{2}\right) = n \left(\hat {A} _ {k}\right) ^ {2} \tag {13.2.2}
$$

Since the periodogram is proportional to the sum of squares of the regression coefficients associated with frequency $\scriptstyle { f = j / n }$ , the height of the periodogram shows the relative strength of cosine-sine pairs at various frequencies in the overall behavior of the series. Another interpretation is in terms of an analysis of variance. The periodogram $I ( j / n )$ is the sum of squares with two degrees of freedom associated with the coefficient pair $( A _ { j } , B _ { j } )$ at frequency $j / n$ , so we have

$$
\sum_ {j = 1} ^ {n} \left(Y _ {j} - \bar {Y}\right) ^ {2} = \sum_ {j = 1} ^ {k} I \binom {\dot {J}} {n} \tag {13.2.3}
$$

when $n = 2 k + 1$ is odd. A similar result holds when $n$ is even but there is a further term in the sum, $I ( \% )$ , with one degree of freedom.

For long series, the computation of a large number of regression coefficients might be intensive. Fortunately, quick, efficient numerical methods based on the fast Fourier transform (FFT) have been developed that make the computations feasible for very long time series.†

Exhibit 13.3 displays a graph of the periodogram for the time series in Exhibit 13.2. The heights show the presence and relative strengths of the two cosine-sine components quite clearly. Note also that the frequencies $4 / 9 6 \approx 0 . 0 4 1 6 7$ and $1 4 / 9 6 \approx 0 . 1 4 5 8 3$ have been marked on the frequency axis.

Exhibit 13.3 Periodogram of the Series in Exhibit 13.2   
![](images/402be04bde3367d2cc4910e742f6eb1f736e05ad1c1a909a3a824ffa710829fd.jpg)  
> periodogram(y); abline(h=0); axis(1,at=c(0.04167,.14583))

Does the periodogram work just as well when we do not know where or even if there are cosines in the series? What if the series contains additional “noise”? To illustrate, we generate a time series using randomness to select the frequencies, amplitudes, and phases and with additional additive white noise. The two frequencies are randomly chosen without replacement from among 1/96, 2/96,…, 47/96. The A’s and $B$ ’s are selected independently from normal distributions with means of zero and standard deviations of 2 for the first component and 3 for the second. Finally, a normal white noise series, $\{ W _ { t } \}$ , with zero mean and standard deviation 1, is chosen independently of the A’s and $B$ ’s and added on. The model is†

$$
Y _ {t} = A _ {1} \cos (2 \pi f _ {1} t) + B _ {1} \sin (2 \pi f _ {1} t) + A _ {2} \cos (2 \pi f _ {2} t) + B _ {2} \sin (2 \pi f _ {2} t) + W _ {t} \tag {13.2.4}
$$

and Exhibit 13.4 displays a time series of length 96 simulated from this model. Once more, the periodicities are not obvious until we view the periodogram shown in Exhibit 13.5.

![](images/f9ea64e13cffe697ae06b26173a08d7ffa50bb8b985aa726662f6315e09067ad.jpg)  
Exhibit 13.4 Time Series with “Hidden” Periodicities

```matlab
> win.graph(width=4.875,height=2.5,pointsize=8)  
> set.seed(134); t=1:96; integer=sample(48,2)  
> freq1=integer[1]/96; freq2=integer[2]/96  
> A1=rnorm(1,0,2); B1=rnorm(1,0,2)  
> A2=rnorm(1,0,3); B2=rnorm(1,0,3); w=2*pi*t  
> y=A1*cos(w*freq1)+B1*sin(w*freq1)+A2*cos(w*freq2)+B2*sin(w*freq2)+rnorm(96,0,1)  
> plot(t,y,type='o',ylab=expression(y[t])) 
```

The periodogram clearly shows that the series contains two cosine-sine pairs at frequencies of about 0.11 and 0.32 and that the higher-frequency component is much stronger. There are some other very small spikes in the periodogram, apparently caused by the additive white noise component. (When we checked the simulation in detail, we found that one frequency was chosen as $1 0 / 9 6 \approx 0 . 1 0 4 2$ and the other was selected as $3 0 / 9 6 = 0 . 3 1 2 5 .$ )

![](images/dddfde5b47fa39a5fa246357d5a70709e5efde2b2829618542cd8db8d771f9ea.jpg)  
Exhibit 13.5 Periodogram of the Time Series Shown in Exhibit 13.4

> periodogram(y);abline $(h = 0)$

Here is an example of the periodogram for a classic time series from Whittaker and Robinson (1924).† Exhibit 13.6 displays the time series plot of the brightness (magnitude) of a particular star at midnight on 600 consecutive nights.

![](images/147676d5da84188b72e777c7e9b5332a8247bc3c8b4fa7b254e3d29e41e4fc09.jpg)  
Exhibit 13.6 Variable Star Brightness on 600 Consecutive Nights

> data(star)  
> plot(star,xlab='Day',ylab='Brightness')

Exhibit 13.7 shows the periodogram for this time series. There are two very prominent peaks in the periodogram. When we inspect the actual numerical values, we find that the larger peak occurs at frequency $f = 2 1 / 6 0 0 = 0 . 0 3 5$ . This frequency corresponds to a period of $6 0 0 / 2 1 \approx 2 8 . 5 7$ , or nearly 29 days. The secondary peak occurs at $f =$ $2 5 / 6 0 0 \approx 0 . 0 4 1 6 7$ , which corresponds to a period of 24 days. The much more modest nonzero periodogram values near the major peak are likely caused by leakage.

The two sharp peaks suggest a model for this series with just two cosine-sine pairs with the appropriate frequencies or periods, namely

$$
Y _ {t} = \beta_ {0} + \beta_ {1} \cos (2 \pi f _ {1} t) + \beta_ {2} \sin (2 \pi f _ {1} t) + \beta_ {3} \cos (2 \pi f _ {2} t) + \beta_ {4} \sin (2 \pi f _ {2} t) + e _ {t} (1 3. 2. 5)
$$

where $f _ { 1 } = 1 / 2 9$ and $f _ { 2 } = 1 / 2 4$ . If we estimate this regression model as in Chapter 3, we obtain highly statistically significant regression coefficients for all five parameters and a multiple R-square value of $9 9 . 9 \%$ .

We will return to this time series in Section 14.5 on page 358, where we discuss more about leakage and tapering.

Exhibit 13.7 Periodogram of the Variable Star Brightness Time Series   
![](images/f7c431c6d5298a60657452cedc9a85598f0be646cbb84bb32aee852ab9ebcd55.jpg)  
> periodogram(star,ylab='Variable Star Periodogram');abline(h=0)

Although the Fourier frequencies are special, we extend the definition of the periodogram to all frequencies in the interval 0 to $\%$ through the Equations (13.1.8) and (13.2.1). Thus we have for $0 \leq f \leq \%$

$$
I (f) = \frac {n}{2} \left(\hat {A} _ {f} ^ {2} + \hat {B} _ {f} ^ {2}\right) \tag {13.2.6}
$$

where

$$
\hat {A} _ {f} = \frac {2}{n} \sum_ {t = 1} ^ {n} Y _ {t} \cos (2 \pi t f) \quad \text {a n d} \quad \hat {B} _ {f} = \frac {2}{n} \sum_ {t = 1} ^ {n} Y _ {t} \sin (2 \pi t f) \tag {13.2.7}
$$

When viewed in this way, the periodogram is often calculated at a grid of frequencies finer than the Fourier frequencies, and the plotted points are connected by line segments to display a somewhat smooth curve.

Why do we only consider positive frequencies? Because by the even and odd nature of cosines and sines, any cosine-sine curve with negative frequency, say $- f ,$ could just as well be expressed as a cosine-sine curve with frequency $+ f .$ . No generality is lost by using positive frequencies.†

Secondly, why do we restrict frequencies to the interval from 0 to $^ { 1 / 2 ? }$ Consider the graph shown in Exhibit 13.8. Here we have plotted two cosine curves, one with frequency $f = { \% }$ and the one shown with dashed lines at frequency $f = \%$ . If we only observe the series at the discrete-time points 0, 1, 2, 3,…, the two series are identical. With discrete-time observations, we could never distinguish between these two curves. We say that the two frequencies $^ { 1 / 4 }$ and $\%$ are aliased with one another. In general, each frequency $f$ within the interval 0 to $\nu _ { 2 }$ will be aliased with each frequency of the form

$f + k ( \% )$ for any positive integer $k$ , and it suffices to limit attention to frequencies within the interval from 0 to $\%$ .

![](images/318f2cf75c8dbbfdfb85b00c82161be84c251f23cbdb29ace3946b357a0c0d00.jpg)  
Exhibit 13.8 Illustration of Aliasing

```txt
> win.graph(width=4.875, height=2.5,pointsize=8)  
> t=seq(0,8,by=.05)  
> plot(t,cos(2*pi*t/4),axes=F,type='l',ylab=expression(Y[t]), xlab='Discrete Time t')  
> axis(1,at=c(1,2,3,4,5,6,7));axis(1);axis(2);box()  
> lines(t,cos(2*pi*t*3/4),lty='dashed',type='l');abline(h=0)  
> points(x=c(0:8),y=cos(2*pi*c(0:8)/4),pch=19) 
```

# 13.3 The Spectral Representation and Spectral Distribution

Consider a time series represented as

$$
Y _ {t} = \sum_ {j = 1} ^ {m} \left[ A _ {j} \cos \left(2 \pi f _ {j} t\right) + B _ {j} \sin \left(2 \pi f _ {j} t\right) \right] \tag {13.3.1}
$$

where the frequencies $0 < f _ { 1 } < f _ { 2 } < . . . < f _ { m } < ^ { 1 } 2$ are fixed and $A _ { j }$ and $B _ { j }$ are independent normal random variables with zero means and ${ V a r } ( A _ { j } ) = { V a r } ( B _ { j } ^ { ' } ) = \sigma _ { j } ^ { 2 ^ { ' } }$ . Then a straightforward calculation shows that $\{ Y _ { t } \}$ is stationary† with mean zero and

$$
\gamma_ {k} = \sum_ {j = 1} ^ {m} \sigma_ {j} ^ {2} \cos (2 \pi k f _ {j}) \tag {13.3.2}
$$

In particular, the process variance, $\gamma _ { 0 }$ , is a sum of the variances due to each component at the various fixed frequencies:

$$
\gamma_ {0} = \sum_ {j = 1} ^ {m} \sigma_ {j} ^ {2} \tag {13.3.3}
$$

If for $0 < f < \%$ we define two random step functions by

$$
a (f) = \sum_ {\{j \mid f _ {j} \leq f \}} A _ {j} \quad \text {a n d} \quad b (f) = \sum_ {\{j \mid f _ {j} \leq f \}} B _ {j} \tag {13.3.4}
$$

then we can write Equation (13.3.1) as

$$
Y _ {t} = \int_ {0} ^ {1 / 2} \cos (2 \pi f t) d a (f) + \int_ {0} ^ {1 / 2} \sin (2 \pi f t) d b (f) \tag {13.3.5}
$$

It turns out that any zero-mean stationary process may be represented as in Equation (13.3.5).† It shows how stationary processes may be represented as linear combinations of infinitely many cosine-sine pairs over a continuous frequency band. In general, $a ( f )$ and $b ( f )$ are zero-mean stochastic processes indexed by frequency on $0 \leq f \leq \%$ , each with uncorrelated‡ increments, and the increments of $a ( f )$ are uncorrelated with the increments of $b ( f )$ . Furthermore, we have

$$
V a r \left(\int_ {f _ {1}} ^ {f _ {2}} d a (f)\right) = V a r \left(\int_ {f _ {1}} ^ {f _ {2}} d b (f)\right) = F \left(f _ {2}\right) - F \left(f _ {1}\right), \text {s a y}. \tag {13.3.6}
$$

Equation (13.3.5) is called the spectral representation of the process. The nondecreasing function $F ( f )$ defined on $0 \leq f \leq \%$ is called the spectral distribution function of the process.

We say that the special process defined by Equation (13.3.1) has a purely discrete (or line) spectrum and, for $0 \leq f \leq \%$ ,

$$
F (f) = \sum_ {\{j \mid f _ {j} \leq f \}} \sigma_ {j} ^ {2} \tag {13.3.7}
$$

Here the heights of the jumps in the spectral distribution give the variances associated with the various periodic components, and the positions of the jumps indicate the frequencies of the periodic components.

In general, a spectral distribution function has the properties

$ \begin{array} { l } { { 1 . \quad F \mathrm { i s } \mathrm { n o n d e c r e a s i n g } } } \\ { { 2 . \quad F \mathrm { i s } \mathrm { r i g h t } \mathrm { c o n t i n u o u s } } } \\ { { 3 . \quad F ( f ) \geq 0 \mathrm { f o r } \mathrm { a l l } f } } \\ { { 4 . \quad \displaystyle \operatorname* { l i m } _ { f  1 / 2 } F ( f ) = V a r ( Y _ { t } ) = \gamma _ { 0 } } } \end{array} \}$ $F$ $F$ (13.3.8) F f( )lim Var Yt( ) γ0= =

If we consider the scaled spectral distribution function $F ( f ) / \gamma _ { 0 }$ , we have a function with the same mathematical properties as a cumulative distribution function (CDF) for a random variable on the interval 0 to $\%$ since now $F ( \% ) / \gamma _ { 0 } = 1$ .

We interpret the spectral distribution by saying that, for $0 \leq f _ { 1 } < f _ { 2 } \leq / / 2$ , the integral

$$
\int_ {f _ {1}} ^ {f _ {2}} d F (f) \tag {13.3.9}
$$

gives the portion of the (total) process variance $F ( \% ) = \gamma _ { 0 }$ that is attributable to frequencies in the range $f _ { 1 }$ to $f _ { 2 }$ .

# Sample Spectral Density

In spectral analysis, it is customary to first remove the sample mean from the series. For the remainder of this chapter, we assume that in the definition of the periodogram, $Y _ { t }$ represents deviations from its sample mean. Furthermore, for mathematical convenience, we now let various functions of frequency, such as the periodogram, be defined on the interval $( - \% , \% ]$ . In particular, we define the sample spectral density or sample spectrum as $\hat { S } ( f ) = \mathcal { V } _ { 2 } I ( f )$ for all frequencies in $( - \% , \% )$ and $\hat { S } ( \sqrt { 2 } ) = I ( \sqrt { 2 } )$ . Using straightforward but somewhat tedious algebra, we can show that the sample spectral density can also be expressed as

$$
\hat {S} (f) = \hat {\gamma} _ {0} + 2 \sum_ {k = 1} ^ {n - 1} \hat {\gamma} _ {k} \cos (2 \pi f k) \tag {13.3.10}
$$

where $\boldsymbol { \hat { \gamma } } _ { k }$ is the sample or estimated covariance function at lag $k$ $k = 0$ , 1, 2,…, n − 1) given by

$$
\hat {\gamma} _ {k} = \frac {1}{n} \sum_ {t = k + 1} ^ {n} \left(Y _ {t} - \bar {Y}\right) \left(Y _ {t - k} - \bar {Y}\right) \tag {13.3.11}
$$

In Fourier analysis terms, the sample spectral density is the (discrete-time) Fourier transform of the sample covariance function. From Fourier analysis theory, it follows that there is an inverse relationship, namely†

$$
\hat {\gamma} _ {k} = \int_ {- 1 / 2} ^ {1 / 2} \hat {S} (f) \cos (2 \pi f k) d f \tag {13.3.12}
$$

In particular, notice that the total area under the sample spectral density is the sample variance of the time series.

$$
\hat {\gamma} _ {0} = \int_ {- Y _ {2}} ^ {1 / 2} \hat {S} (f) d f = \frac {1}{n} \sum_ {t = 1} ^ {n} \left(Y _ {t} - \bar {Y}\right) ^ {2} \tag {13.3.13}
$$

Since each can be obtained from the other, the sample spectral density and the sample covariance function contain the same information about the observed time series but it is expressed in different ways. For some purposes, one is more convenient or useful, and for other purposes the other is more convenient or useful.

# 13.4 The Spectral Density

For many processes, such as all stationary ARMA processes, the covariance functions decay rapidly with increasing lag.† When that is the case, it seems reasonable to consider the expression formed by replacing sample quantities in the sample spectral density of Equation (13.3.10) with the corresponding theoretical quantities. To be precise, if the covariance function $\gamma _ { k }$ is absolutely summable, we define the theoretical (or population) spectral density for $- \% < f \leq Y _ { 2 }$ as

$$
S (f) = \gamma_ {0} + 2 \sum_ {k = 1} ^ {\infty} \gamma_ {k} \cos (2 \pi f k) \tag {13.4.1}
$$

Once more, there is an inverse relationship, given by

$$
\gamma_ {k} = \int_ {- 1 / 2} ^ {1 / 2} S (f) \cos (2 \pi f k) d f \tag {13.4.2}
$$

Mathematically, $S ( f )$ is the (discrete-time) Fourier transform of the sequence $\cdots , \check { \eta } _ { - 2 , } \check { \eta } _ { - 1 }$ , $\gamma _ { 0 } , \gamma _ { 1 } , \gamma _ { 2 } , . . . ,$ , and $\{ \gamma _ { k } \}$ is the inverse Fourier transform‡ of the spectral density $S ( f )$ defined on $- \% < f \leq Y _ { 2 }$ .

A spectral density has all of the mathematical properties of a probability density function on the interval $( - \% , \% ]$ , with the exception that the total area is $\gamma _ { 0 }$ rather than 1. Moreover, it can be shown that

where $i = \sqrt { - 1 }$ is the imaginary unit for complex numbers. This looks more like a standard discrete-time Fourier transform. In a similar way, Equation (13.4.2) may be rewritten as

$$
\gamma_ {k} = \int_ {- 1 / 2} ^ {1 / 2} S (f) e ^ {2 \pi i k f} d f.
$$

$$
F (f) = \int_ {0} ^ {f} S (x) d x \quad \text {f o r} 0 \leq f \leq 1 / 2 \tag {13.4.3}
$$

Thus, twice the area under the spectral density between frequencies $f _ { 1 }$ and $f _ { 2 }$ with $0 \leq f _ { 1 }$ $< f _ { 2 } \leq \%$ is interpreted as the portion of the variance of the process that is attributable to cosine-sine pairs in that frequency interval that compose the process.

# Time-Invariant Linear Filters

A time-invariant linear filter is defined by a sequence of absolutely summable constants …, c−1, c0, c1, c2, c3,… . If $\{ X _ { t } \}$ is a time series, we use these constants to filter $\{ X _ { t } \}$ and produce a new time series $\{ Y _ { t } \}$ using the expression

$$
Y _ {t} = \sum_ {j = - \infty} ^ {\infty} c _ {j} X _ {t - j} \tag {13.4.4}
$$

If $c _ { k } = 0$ for $k < 0$ , we say that the filter is causal. In this case, the filtering at time t involves only present and past data values and can be carried out in “real time.”

We have already seen many examples of time-invariant linear filters in previous chapters. Differencing (nonseasonal or seasonal) is an example. A combination of one seasonal difference with one nonseasonal difference is another example. Any moving average process can be considered as a linear filtering of a white noise sequence and in fact every general linear process defined by Equation (4.1.1) on page 55 is a linear filtering of white noise.

The expression on the right-hand side of Equation (13.4.4) is frequently called the (discrete-time) convolution of the two sequences $\{ c _ { t } \}$ and $\{ X _ { t } \}$ . An extremely useful property of Fourier transforms is that the somewhat complicated operation of convolution in the time domain is transformed into the very simple operation of multiplication in the frequency domain.†

In particular, let $S _ { X } ( f )$ be the spectral density for the $\{ X _ { t } \}$ process and let $S _ { Y } ( f )$ be the spectral density for the $\{ Y _ { t } \}$ process. In addition, let

$$
C \left(e ^ {- 2 \pi i f}\right) = \sum_ {j = - \infty} ^ {\infty} c _ {j} e ^ {- 2 \pi i f j} \tag {13.4.5}
$$

Then

$$
\begin{array}{l} C o v (Y _ {t}, Y _ {t - k}) = C o v \left(\sum_ {j = - \infty} ^ {\infty} c _ {j} X _ {t - j}, \sum_ {s = - \infty} ^ {\infty} c _ {s} X _ {t - k - s}\right) \\ = \sum_ {j = - \infty} ^ {\infty} \sum_ {s = - \infty} ^ {\infty} c _ {j} c _ {s} C o v (X _ {t - j}, X _ {t - k - s}) \\ \end{array}
$$

$$
\begin{array}{l} = \sum_ {j = - \infty} ^ {\infty} \sum_ {s = - \infty} ^ {\infty} c _ {j} c _ {s} \int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i (s + k - j) f} S _ {X} (f) d f \\ = \int_ {- 1 / 2} ^ {1 / 2} \left| \sum_ {s = - \infty} ^ {\infty} c _ {s} e ^ {- 2 \pi i s f} \right| ^ {2} e ^ {2 \pi i f k} S _ {X} (f) d f \\ \end{array}
$$

So

$$
C o v \left(Y _ {t}, Y _ {t - k}\right) = \int_ {- 1 / 2} ^ {1 / 2} \left| C \left(e ^ {- 2 \pi i f}\right) \right| ^ {2} S _ {X} (f) e ^ {2 \pi i f k} d f \tag {13.4.6}
$$

But

$$
C o v \left(Y _ {t}, Y _ {t - k}\right) = \int_ {- Y _ {2}} ^ {Y _ {2}} S _ {Y} (f) e ^ {2 \pi i f k} d f \tag {13.4.7}
$$

so we must have

$$
S _ {Y} (f) = \left| C \left(e ^ {- 2 \pi i f}\right) \right| ^ {2} S _ {X} (f) \tag {13.4.8}
$$

This expression is invaluable for investigating the effect of time-invariant linear filters on spectra. In particular, it helps us find the form of the spectral densities for ARMA processes. The function $\scriptstyle | C ( e ^ { - 2 \pi i f } ) | ^ { 2 }$ is often called the (power) transfer function of the filter.

# 13.5 Spectral Densities for ARMA Processes

# White Noise

From Equation (13.4.1), it is easy to see that the theoretical spectral density for a white noise process is constant for all frequencies in $- \% < f \leq / 2$ and, in particular,

$$
S (f) = \sigma_ {e} ^ {2} \tag {13.5.1}
$$

All frequencies receive equal weight in the spectral representation of white noise. This is directly analogous to the spectrum of white light in physics — all colors (that is, all frequencies) enter equally in white light. Finally, we understand the origin of the name white noise!

# MA(1) Spectral Density

An MA(1) process is a simple filtering of white noise with $c _ { 0 } = 1$ and $c _ { 1 } = - \theta$ and so

$$
\begin{array}{l} \left| C \left(e ^ {- 2 \pi i f}\right) \right| ^ {2} = (1 - \theta e ^ {2 \pi i f}) (1 - \theta e ^ {- 2 \pi i f}) \\ = 1 + \theta^ {2} - \theta \left(e ^ {2 \pi i f} + e ^ {- 2 \pi i f}\right) \tag {13.5.2} \\ = 1 + \theta^ {2} - 2 \theta \cos (2 \pi f) \\ \end{array}
$$

Thus

$$
S (f) = [ 1 + \theta^ {2} - 2 \theta \cos (2 \pi f) ] \sigma_ {e} ^ {2} \tag {13.5.3}
$$

When $\theta > 0$ , you can show that this spectral density is an increasing function of nonnegative frequency, while for $\theta < 0$ the function decreases.

Exhibit 13.9 displays the spectral density for an MA(1) process with $\theta = 0 . 9$ .† Since spectral densities are symmetric about zero frequency, we will only plot them for positive frequencies. Recall that this MA(1) process has a relatively large negative correlation at lag 1 but all other correlations are zero. This is reflected in the spectrum. We see that the density is much stronger for higher frequencies than for low frequencies. The process has a tendency to oscillate back and forth across its mean level. This rapid oscillation is high-frequency behavior. We might say that the moving average suppresses the lower-frequency components of the white noise process. Researchers sometimes refer to this type of spectrum as a blue spectrum since it emphasizes the higher frequencies (that is, those with lower period or wavelength), which correspond to blue light in the spectrum of visible light.

![](images/49fdae467cfd4617f33b3b7861a121873106e28f4582632eba7cd11aba6c4ace.jpg)  
Exhibit 13.9 Spectral Density of MA(1) Process with $\boldsymbol { \theta } = \mathbf { 0 . 9 }$

```txt
> win.graph(width=4.875,height=2.5,pointsize=8)  
> theta=.9 # Reset theta for other MA(1) plots  
> ARMAspec(model=list(ma=-theta)) 
```

Exhibit 13.10 displays the spectral density for an MA(1) process with $\theta = - 0 . 9$ . This process has positive correlation at lag 1 with all other correlations zero. Such a process will tend to change slowly from one time instance to the next. This is low-frequency behavior and is reflected in the shape of the spectrum. The density is much stronger for lower frequencies than for high frequencies. Researchers sometimes call this a red spectrum.

![](images/73c3583d5e7cb36b6ace2ed84404f43d7f039ea33aec14550ac74e21a54a6584.jpg)  
Exhibit 13.10 Spectral Density of MA(1) Process with $\mathbf { \boldsymbol { \theta } } = - \mathbf { \boldsymbol { \mathsf { 0 } } } . \mathbf { \pmb { 9 } }$

# MA(2) Spectral Density

The spectral density for an MA(2) model may be obtained similarly. The algebra is a little longer, but the final expression is

$$
S (f) = \left[ 1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2} - 2 \theta_ {1} \left(1 - \theta_ {2}\right) \cos (2 \pi f) - 2 \theta_ {2} \cos (4 \pi f) \right] \sigma_ {e} ^ {2} \tag {13.5.4}
$$

Exhibit 13.11 shows a graph of such a density when $\theta _ { 1 } = 1$ and $\theta _ { 2 } = - 0 . 6$ . The frequencies between about 0.1 and 0.18 have especially small density and there is very little density below the frequency of 0.1. Higher frequencies enter into the picture gradually, with the strongest periodic components at the highest frequencies.

![](images/c2ad4706b226277e46f70e845369f9967fb4ead67fd7efbd349eaf7184a1be08.jpg)  
Exhibit 13.11 Spectral Density of MA(2) Process with $\theta _ { 1 } = 1$ and $\overline { { \theta _ { 2 } = - 0 . 6 } }$

```txt
> theta1=1; theta2=-0.6  
> ARMAspec(model=list(ma=-c(theta1,theta2))) 
```

# AR(1) Spectral Density

To find the spectral density for AR models, we use Equation (13.4.8) “backwards.” That is, we view the white noise process as being a linear filtering of the AR process. Recalling the spectral density of the MA(1) series, this gives

$$
[ 1 + \phi^ {2} - 2 \phi \cos (2 \pi f) ] S (f) = \sigma_ {e} ^ {2} \tag {13.5.5}
$$

which we solve to obtain

$$
S (f) = \frac {\sigma_ {e} ^ {2}}{1 + \phi^ {2} - 2 \phi \cos (2 \pi f)} \tag {13.5.6}
$$

As the next two exhibits illustrate, this spectral density is a decreasing function of frequency when $\phi > 0$ , while the spectral density increases for $\phi < 0$ .

Exhibit 13.12 Spectral Density of an AR(1) Process with $\phi = 0 . 9$   
![](images/d9beb5b3d2d6082be8c47f4afb2e69b94ccf236c4aa77369a15562d3be412812.jpg)  
> phi=0.9 # Reset value of phi for other AR(1) models > ARMAspec(model=list(ar=phi))

![](images/2439ea20aff516e9c7868a39210567643ae890d534c73aff9f4392143ea75cf9.jpg)  
Exhibit 13.13 Spectral Density of an AR(1) Process with $\phi = - 0 . 6$

# AR(2) Spectral Density

For the AR(2) spectral density, we again use Equation (13.4.8) backwards together with the MA(2) result to obtain

$$
S (f) = \frac {\sigma_ {e} ^ {2}}{1 + \phi_ {1} ^ {2} + \phi_ {2} ^ {2} - 2 \phi_ {1} \left(1 - \phi_ {2}\right) \cos \left(2 \pi f\right) - 2 \phi_ {2} \cos \left(4 \pi f\right)} \tag {13.5.7}
$$

Just as with the correlation properties, the spectral density for an AR(2) model can exhibit a variety of behaviors depending on the actual values of the two $\boldsymbol { \Phi }$ parameters.

Exhibits 13.14 and 13.15 display two AR(2) spectral densities that show very different behavior of peak in one case and trough in another.

![](images/4142e3325e5f3954a4b74fcbd811839ff741348478cc0152d8454af49f929de9.jpg)  
Exhibit 13.14 Spectral Density of AR(2) Process: $\$ 123$ and $\Uparrow _ { 2 } = - 0 . 7 5$

```txt
> phi1=1.5; phi2=-.75  
> # Reset values of phi1 & phi2 for other AR(2) models  
> ARMAspec(model=list(ar=c(phi1,phi2))) 
```

Jenkins and Watts (1968, p. 229), have noted that the different spectral shapes for an AR(2) spectrum are determined by the inequality

$$
\left| \phi_ {1} \left(1 - \phi_ {2}\right) \right| <   \left| 4 \phi_ {2} \right| \tag {13.5.8}
$$

and the results are best summarized in the display in Exhibit 13.16. In this display, the dashed curve is the border between the regions of real roots and complex roots of the AR(2) characteristic equation. The solid curves are determined from the inequality given in Equation (13.5.8).

![](images/53b2066dac776bfce600652d7eaa27e6839659cd2e10f19d07390fb096ec8f85.jpg)  
Exhibit 13.15 Spectral Density of AR(2) Process with $\phi _ { 1 } = 0 . 1$ and $\phi _ { 2 } = 0 . 4$

![](images/7e63968c9020496659ee38331f3246e363e3e31800c7e9667a1939571d9df6cd.jpg)  
Exhibit 13.16 AR(2) Parameter Values for Various Spectral Density Shapes

Note that Jenkins and Watts also showed that the frequency $f _ { 0 }$ at which the peak or trough occurs will satisfy

$$
\cos \left(2 \pi f _ {0}\right) = - \frac {\phi_ {1} \left(1 - \phi_ {2}\right)}{4 \phi_ {2}} \tag {13.5.9}
$$

It is commonly thought that complex roots are associated with a peak spectrum. But notice that there is a small region of parameter values where the roots are complex but the spectrum is of either high or low frequency with no intermediate peak.

# ARMA(1,1) Spectral Density

Combining what we know for MA(1) and AR(1) models, we can easily obtain the spectral density for the ARMA(1,1) mixed model

$$
S (f) = \frac {1 + \theta^ {2} - 2 \theta \cos (2 \pi f)}{1 + \phi^ {2} - 2 \phi \cos (2 \pi f)} \sigma_ {e} ^ {2} \tag {13.5.10}
$$

Exhibit 13.17 provides an example of the spectrum for an ARMA(1,1) model with $\phi =$ 0.5 and $\theta = 0 . 8$ .

![](images/5228980469789053b42d4304699d737f4957927f92ad93b389ec65698aa7c1b4.jpg)  
Exhibit 13.17 Spectral Density of ARMA(1,1) with $\phi = 0 . 5$ and θ = 0.8

```lisp
> phi=0.5; theta=0.8  
> ARMAspec(model=list(ar=phi, ma=-theta)) 
```

# ARMA(p,q)

For the general $\mathbf { A R M A } ( p , q )$ case, the spectral density may be expressed in terms of the AR and MA characteristic polynomials as

$$
S (f) = \left| \frac {\theta \left(e ^ {- 2 \pi i f}\right)}{\phi \left(e ^ {- 2 \pi i f}\right)} \right| ^ {2} \sigma_ {e} ^ {2} \tag {13.5.11}
$$

This may be further expressed in terms of the reciprocal roots of these polynomials, but we will not pursue those expressions here. This type of spectral density is often referred to as a rational spectral density.

# Seasonal ARMA Processes

Since seasonal ARMA processes are just special ARMA processes, all of our previous work will carry over here. Multiplicative seasonal models can be thought of as applying two linear filters consecutively. We will just give two examples.

Consider the process defined by the seasonal AR model

$$
(1 - \phi B) (1 - \Phi B ^ {1 2}) Y _ {t} = e _ {t} \tag {13.5.12}
$$

Manipulating the two factors separately yields

$$
S (f) = \frac {\sigma_ {e} ^ {2}}{\left[ 1 + \phi^ {2} - 2 \phi \cos (2 \pi f) \right] \left[ 1 + \Phi^ {2} - 2 \Phi \cos (2 \pi 1 2 f) \right]} \tag {13.5.13}
$$

An example of this spectrum is shown in Exhibit 13.18, where $\phi = 0 . 5$ , $\Phi = 0 . 9$ , and $s =$ 12. The seasonality is reflected in the many spikes of decreasing magnitude at frequencies of 0, 1/12, 2/12, 3/12, 4/12, 5/12, and 6/12.

As a second example, consider a seasonal MA process

$$
Y _ {t} = (1 - \theta B) \left(1 - \Theta B ^ {1 2}\right) e _ {t} \tag {13.5.14}
$$

The corresponding spectral density is given by

$$
S (f) = [ 1 + \theta^ {2} - 2 \theta \cos (2 \pi f) ] [ 1 + \Theta^ {2} - 2 \Theta \cos (2 \pi 1 2 f) ] \sigma_ {e} ^ {2} \tag {13.5.15}
$$

Exhibit 13.19 shows this spectral density for parameter values $\theta = 0 . 4$ and $\Theta = 0 . 9$ .

![](images/ef42264eedb02984b636ffdd06994b41e580be594da53bff39ab309026358daa.jpg)  
Exhibit 13.18 Spectral Density of Seasonal AR with $\phi = 0 . 5$ , $\Phi = 0 . 9$ , s =12

```txt
> phi=.5; PHI=.9  
> ARMAspec(model=list(ar=phi, seasonal=list(sar=PHI, period=12))) 
```

![](images/500d101c8a46d7b07a83695ef80fa6e7f70723eef47af48ee493631cccabb3ef.jpg)  
Exhibit 13.19 Spectral Density of Seasonal MA with $\mathbf { \boldsymbol { \theta } = 0 . 4 }$ , $\Theta = 0 . 9$ , $\pmb { \mathscr { s } } = \pmb { 1 2 }$

> theta=.4; Theta $=$ .9   
> ARMAspec(model ${ } , { } = { }$ list(ma $. =$ -theta,seasonal=list(sma $. =$ -Theta, period=12)))

# 13.6 Sampling Properties of the Sample Spectral Density

To introduce this section, we consider a time series with known properties. Suppose that we simulate an AR(1) model with $\phi = - 0 . 6$ of length $n = 2 0 0$ . Exhibit 13.13 on page 336, shows the theoretical spectral density for such a series. The sample spectral density for our simulated series is displayed in Exhibit 13.20, with the smooth theoretical spectral density shown as a dotted line. Even with a sample of size 200, the sample spectral density is extremely variable from one frequency point to the next. This is surely not an acceptable estimate of the theoretical spectrum for this process. We must investigate the sampling properties of the sample spectral density to understand the behavior that we see here.

To investigate the sampling properties of the sample spectral density, we begin with the simplest case, where the time series $\{ Y _ { t } \}$ is zero-mean normal white noise with variance $\gamma _ { 0 }$ . Recall that

$$
\hat {A} _ {f} = \frac {2}{n} \sum_ {t = 1} ^ {n} Y _ {t} \cos (2 \pi t f) \text {a n d} \hat {B} _ {f} = \frac {2}{n} \sum_ {t = 1} ^ {n} Y _ {t} \sin (2 \pi t f) \tag {13.6.1}
$$

For now, consider only nonzero Fourier frequencies $f = j / n < \%$ . Since $\hat { A } _ { f }$ and $\hat { B } _ { f }$ are linear functions of the time series $\{ Y _ { t } \}$ , they each have a normal distribution. We can evaluate the means and variances using the orthogonality properties of the cosines and sines.† We find that $\hat { A } _ { f }$ and $\hat { B } _ { f }$ each have mean zero and variance $2 \gamma _ { 0 } / n$ . We can also use the orthogonality properties to show that $\hat { A } _ { f }$ and $\hat { B } _ { f }$ are uncorrelated and thus indepen-

dent since they are jointly bivariate normal. Similarly, it can be shown that for any two distinct Fourier frequencies $f _ { 1 }$ and $f _ { 2 }$ , $\hat { A } _ { f _ { 1 } }$ , $\hat { A } _ { f _ { 2 } }$ , $\hat { B } _ { f _ { 1 } }$ , and $\hat { B } _ { f _ { 2 } }$ are jointly independent.

# Exhibit 13.20 Sample Spectral Density for a Simulated AR(1) Process

![](images/151276868af9b51ee1b4f68e95153c01d3ebe44d65a7c1b984b429b5244fbab6.jpg)

```javascript
> win.graph(width=4.875,height=2.5,pointsize=8)
> set.seed(271435); n=200; phi=-0.6
> y=arima.sim(model=list(ar=phi),n=n)
> sp=spec(y,log='no',xlabel='Frequency',
ylab='Sample Spectral Density',sub='')
> lines(sp$freq,ARMA spec(model=list(ar=phi),freq=sp$freq,
plot=F) $spec,fty='dotted'); abline(h=0) 
```

Furthermore, we know that the square of a standard normal has a chi-square distribution with one degree of freedom and that the sum of independent chi-square variables is chi-square distributed with degrees of freedom added together. Since $S ( f ) = \gamma _ { 0 }$ , we have

$$
\frac {n}{2 \gamma_ {0}} [ (\hat {A} _ {f}) ^ {2} + (\hat {B} _ {f}) ^ {2} ] = \frac {2 \hat {S} (f)}{S (f)} \tag {13.6.2}
$$

has a chi-square distribution with two degrees of freedom.

Recall that a chi-square variable has a mean equal to its degrees of freedom and a variance equal to twice its degrees of freedom. With these facts, we quickly discover that

$$
\hat {S} \left(f _ {1}\right) \text {a n d} \hat {S} \left(f _ {2}\right) \text {a r e i n d e p e n d e n t f o r} f _ {1} \neq f _ {2} \tag {13.6.3}
$$

$$
E [ \hat {S} (f) ] = S (f) \tag {13.6.4}
$$

and

$$
\operatorname {V a r} [ \hat {S} (f) ] = S ^ {2} (f) \tag {13.6.5}
$$

Equation (13.6.4) expresses the desirable fact that the sample spectral density is an unbiased estimator of the theoretical spectral density.

Unfortunately, Equation (13.6.5) shows that the variance in no way depends on the sample size n. Even in this simple case, the sample spectral density is not a consistent estimator of the theoretical spectral density. It does not get better (that is, have smaller variance) as the sample size increases. The reason the sample spectral density is inconsistent is basically this: Even if we only consider Fourier frequencies, 1/n, 2/n,…, we are trying to estimate more and more “parameters”; that is, $S ( 1 / n )$ , $S ( 2 / n )$ ,… . As the sample size increases, there are not enough data points per parameter to produce consistent estimates.

The results expressed in Equations (13.6.3)–(13.6.5) in fact hold more generally. In the exercises, we ask you to argue that for any white noise—not necessarily normal— the mean result holds exactly and the $\hat { A } _ { f }$ and $\hat { B } _ { f }$ that make up $\hat { \boldsymbol { S } } ( f _ { 1 } )$ and $\mathsf { \widehat { S } } ( f _ { 2 } )$ are at least uncorrelated for $f _ { 1 } \neq f _ { 2 }$ .

To state more general results, suppose $\{ Y _ { t } \}$ is any linear process

$$
Y _ {t} = e _ {t} + \psi_ {1} e _ {t - 1} + \psi_ {2} e _ {t - 2} + \dots \tag {13.6.6}
$$

where the e’s are independent and identically distributed with zero mean and common variance. Suppose that the $\boldsymbol { \Psi }$ -coefficients are absolutely summable, and let $f _ { 1 } \neq f _ { 2 }$ be any frequencies in 0 to $\%$ . Then it may be shown† that as the sample size increases without limit

$$
\frac {2 \hat {S} \left(f _ {1}\right)}{S \left(f _ {1}\right)} \text {a n d} \frac {2 \hat {S} \left(f _ {2}\right)}{S \left(f _ {2}\right)} \tag {13.6.7}
$$

converge in distribution to independent chi-square random variables, each with two degrees of freedom.

To investigate the usefulness of approximations based on Equations (13.6.7), (13.6.4), and (13.6.5), we will display results from two simulations. We first simulated 1000 replications of an MA(1) time series with $\theta = 0 . 9$ , each of length $n = 4 8$ . The white noise series used to create each MA(1) series was selected independently from a $t$ -distribution with five degrees of freedom scaled to unit variance. From the 1000 series, we calculated 1000 sample spectral densities.

Exhibit 13.21 shows the average of the 1000 sample spectral densities evaluated at the 24 Fourier frequencies associated with $n = 4 8$ . The solid line is the theoretical spectral density. It appears that the sample spectral densities are unbiased to a useful approximation in this case.

![](images/a1d4d83257f9fdd071b7b193b82c497a84c71f718580fe612c9eff745c3f1d94.jpg)  
Exhibit 13.21 Average Sample Spectral Density: Simulated MA(1), $\boldsymbol { \theta } = \mathbf { 0 . 9 }$ , $\pmb { \cal n } = 4 8$

For the extensive R code to produce Exhibits 13.21 through 13.26, please see the Chapter 13 script file associated with this book.

Exhibit 13.22 plots the standard deviations of the sample spectral densities over the 1000 replications. According to Equation (13.6.5), we hope that they match the theoretical spectral density at the Fourier frequencies. Again the approximation seems to be quite acceptable.

![](images/c59ea6db866b5be1064571a1c236cdba9d351527cda81f03296740042e5df7b1.jpg)  
Exhibit 13.22 Standard Deviation of Sample Spectral Density: Simulated MA(1), $\boldsymbol { \theta } = \mathbf { 0 . 9 }$ , $\pmb { \cal n } = 4 8$

To check on the shape of the sample spectral density distribution, we constructed a QQ plot comparing the observed quantiles with those of a chi-square distribution with

two degrees of freedom. Of course, we could do those for any of the Fourier frequencies. Exhibit 13.23 shows the results at the frequency 15/48. The agreement with the chi-square distribution appears to be acceptable.

![](images/4868f2f9fa6541709098eecb80299aabb8138122fbd218dae6bbbf8667d38f0f.jpg)  
Exhibit 13.23 QQ Plot of Spectral Distribution at f = 15/48   
Chi−Square Quantiles

We repeated similar displays and calculations when the true model was an AR(2) with $\phi _ { 1 } = 1 . 5$ , $\Phi _ { 2 } = - 0 . 7 5$ , and $n = 9 6$ . Here we used normal white noise. The results are displayed in Exhibits 13.24, 13.25, and 13.26. Once more the simulation results with $n =$ 96 and 1000 replications seem to follow those suggested by limit theory quite remarkably.

![](images/6f23661b0c3f6f879268bf4c96f5c89209fb686c4526bf25309fe7a1aab1882f.jpg)  
Exhibit 13.24 Average Sample Spectral Density: Simulated AR(2), $\$ 123$ , $\Phi _ { 2 } = - 0 . 7 5$ , $\pmb { n = 9 6 }$

![](images/3294f56d6c486a8243fd8bf7b0a318e5035118100a91fdc304f846429c4c2f56.jpg)  
Exhibit 13.25 Standard Deviation of Sample Spectral Density: Simulated AR(2), $\$ 123$ , $\Phi _ { 2 } = - 0 . 7 5$ , $\pmb { n = 9 6 }$

![](images/1e428ae8f2e4c86c03824101bcf1e93d4f34cd292bc2a34c544908f9b20efc9d.jpg)  
Exhibit 13.26 QQ Plot of Spectral Distribution at $\pmb { f } = \pmb { 4 0 } / \pmb { 9 6 }$

Of course, none of these results tell us that the sample spectral density is an acceptable estimator of the underlying theoretical spectral density. The sample spectral density is quite generally approximately unbiased but also inconsistent, with way too much variability to be a useful estimator as it stands. The approximate independence at the Fourier frequencies also helps explain the extreme variability in the behavior of the sample spectral density.

# 13.7 Summary

The chapter introduces the ideas of modeling time series as linear combinations of sines and cosines—so-called spectral analysis. The periodogram was introduced as a tool for finding the contribution of the various frequencies in the spectral representation of the series. The ideas were then extended to modeling with a continuous range of frequencies. Spectral densities of the ARMA models were explored. Finally, the sampling properties of the sample spectral density were presented. Since the sample spectral density is not a consistent estimator of the theoretical spectral density, we must search further for an acceptable estimator. That is the subject of the next chapter.

# EXERCISES

13.1 Find A and $B$ so that $3 \cos ( 2 \pi f t + 0 . 4 ) = A \cos ( 2 \pi f t ) + B \sin ( 2 \pi f t )$ .   
13.2 Find $R$ and $\Phi$ so that $R \cos ( 2 \pi f t + \Phi ) = \cos ( 2 \pi f t ) + 3 \sin ( 2 \pi f t ) .$   
13.3 Consider the series displayed in Exhibit 13.2 on page 320.

(a) Verify that regressing the series on $\cos ( 2 \pi f t )$ and $\sin ( 2 \pi f t )$ for $f = 4 / 9 6$ provides perfect estimates of $A$ and $B$ .   
(b) Use Equations (13.1.5) on page 321 to obtain the relationship between $R , \Phi , A$ and $B$ for the cosine component at frequency $f = 1 4 / 9 6$ . (For this component, the amplitude is 1 and the phase is 0.6π.)   
(c) Verify that regressing the series on $\cos ( 2 \pi f t )$ and sin(2πft) for $f = 1 4 / 9 6$ provides perfect estimates of $A$ and $B$ .   
(d) Verify that regressing the series on $\cos ( 2 \pi f t )$ and $\sin ( 2 \pi f t )$ for both $f = 4 / 9 6$ and $f = 1 4 / 9 6$ together provides perfect estimates of $A _ { 4 } , B _ { 4 } , A _ { 1 4 }$ , and $B _ { 1 4 }$ .   
(e) Verify that regressing the series on $\cos ( 2 \pi f t )$ and $\sin ( 2 \pi f t )$ for $f = 3 / 9 6$ and $f =$ 13/96 together provides perfect estimates of $A _ { 3 } , B _ { 3 } , A _ { 1 3 }$ , and $B _ { 1 3 }$ .   
(f) Repeat part (d) but add a third pair of cosine-sine predictor variables at any other Fourier frequency. Verify that all of the regression coefficients are still estimated perfectly.

13.4 Generate or choose any series of length $n = 1 0$ . Show that the series may be fit exactly by a linear combination of enough cosine-sine curves at the Fourier frequencies.   
13.5 Simulate a signal $^ +$ noise time series from the model in Equation (13.2.4) on page 323. Use the same parameter values used in Exhibit 13.4 on page 324.

(a) Plot the time series and look for the periodicities. Can you see them?   
(b) Plot the periodogram for the simulated series. Are the periodicities clear now?

13.6 Show that the covariance function for the series defined by Equation (13.3.1) on page 327 is given by the expression in Equation (13.3.2).   
13.7 Display the algebra that establishes Equation (13.3.10) on page 329.

13.8 Show that if $\{ X _ { t } \}$ and $\{ Y _ { t } \}$ are independent stationary series, then the spectral density of $\{ X _ { t } + Y _ { t } \}$ is the sum of the spectral densities of $\{ X _ { t } \}$ and $\{ Y _ { t } \}$ .   
13.9 Show that when $\theta > 0$ the spectral density for an MA(1) process is an increasing function of frequency, while for $\theta < 0$ this function decreases.   
13.10 Graph the theoretical spectral density for an MA(1) process with $\theta = 0 . 6$ . Interpret the implications of the shape of the spectrum on the possible plots of the time series values.   
13.11 Graph the theoretical spectral density for an MA(1) process with $\theta = - 0 . 8$ . Interpret the implications of the shape of the spectrum on the possible plots of the time series values.   
13.12 Show that when $\phi > 0$ the spectral density for an AR(1) process is a decreasing function of frequency, while for $\phi < 0$ the spectral density increases.   
13.13 Graph the theoretical spectral density for an AR(1) time series with $\phi = 0 . 7$ . Interpret the implications of the shape of the spectrum on the possible plots of the time series values.   
13.14 Graph the theoretical spectral density for an AR(1) time series with $\Phi = - 0 . 4$ . Interpret the implications of the shape of the spectrum on the possible plots of the time series values.   
13.15 Graph the theoretical spectral density for an MA(2) time series with $\theta _ { 1 } = - 0 . 5$ and $\theta _ { 2 } = 0 . 9$ . Interpret the implications of the shape of the spectrum on the possible time series plots of the series values.   
13.16 Graph the theoretical spectral density for an MA(2) time series with $\theta _ { 1 } = 0 . 5$ and $\theta _ { 2 } = - 0 . 9$ . Interpret the implications of the shape of the spectrum on the possible time series plots of the series values.   
13.17 Graph the theoretical spectral density for an AR(2) time series with $\phi _ { 1 } = - 0 . 1$ and $\phi _ { 2 } = - 0 . 9$ . Interpret the implications of the shape of the spectrum on the possible time series plots of the series values.   
13.18 Graph the theoretical spectral density for an AR(2) process with $\phi _ { 1 } = 1 . 8$ and $\phi _ { 2 } =$ $- 0 . 9$ . Interpret the implications of the shape of the spectrum on the possible plots of the time series values.   
13.19 Graph the theoretical spectral density for an AR(2) process with $\phi _ { 1 } = - 1$ and $\phi _ { 2 } =$ $- 0 . 8$ . Interpret the implications of the shape of the spectrum on the possible plots of the time series values.   
13.20 Graph the theoretical spectral density for an AR(2) process with $\phi _ { 1 } = 0 . 5$ and $\phi _ { 2 } =$ 0.4. Interpret the implications of the shape of the spectrum on the possible plots of the time series values.   
13.21 Graph the theoretical spectral density for an AR(2) process with $\phi _ { 1 } = 0$ and $\phi _ { 2 } =$ 0.8. Interpret the implications of the shape of the spectrum on the possible plots of the time series values.   
13.22 Graph the theoretical spectral density for an AR(2) process with $\phi _ { 1 } = 0 . 8$ and $\phi _ { 2 } =$ −0.2. Interpret the implications of the shape of the spectrum on the possible plots of the time series values.   
13.23 Graph the theoretical spectral density for an ARMA(1,1) time series with $\phi = 0 . 5$ and $\theta = 0 . 8$ . Interpret the implications of the shape of the spectrum on the possible plots of the time series values.

13.24 Graph the theoretical spectral density for an ARMA(1,1) process with $\phi = 0 . 9 5$ and $\theta = 0 . 8$ . Interpret the implications of the shape of the spectrum on the possible plots of the time series values.

13.25 Let $\{ X _ { t } \}$ be a stationary time series and $\{ Y _ { t } \}$ be defined by $Y _ { t } = ( X _ { t } + X _ { t - 1 } ) / 2$

(a) Find the power transfer function for this linear filter.   
(b) Is this a causal filter?   
(c) Graph the power transfer function and describe the effect of using this filter. That is, what frequencies will be retained (emphasized) and what frequencies will be deemphasized (attenuated) by this filtering?

13.26 Let $\{ X _ { t } \}$ be a stationary time series and let $\{ Y _ { t } \}$ be defined by $Y _ { t } = X _ { t } - X _ { t - 1 }$

(a) Find the power transfer function for this linear filter.   
(b) Is this a causal filter?   
(c) Graph the power transfer function and describe the effect of using this filter. That is, what frequencies will be retained (emphasized) and what frequencies will be deemphasized (attenuated) by this filtering?

13.27 Let $\{ X _ { t } \}$ be a stationary time series and let $Y _ { t } = ( X _ { t + 1 } + X _ { t } + X _ { t - 1 } ) / 3$ define $\{ Y _ { t } \}$ .

(a) Find the power transfer function for this linear filter.   
(b) Is this a causal filter?   
(c) Graph the power transfer function and describe the effect of using this filter. That is, what frequencies will be retained (emphasized) and what frequencies will be deemphasized (attenuated) by this filtering?

13.28 Let $\{ X _ { t } \}$ be a stationary time series and let $Y _ { t } = ( X _ { t } + X _ { t - 1 } + X _ { t - 2 } ) / 3$ define $\{ Y _ { t } \}$ .

(a) Show that the power transfer function of this filter is the same as the power transfer function of the filter defined in Exercise 13.27.   
(b) Is this a causal filter?

13.29 Let $\{ X _ { t } \}$ be a stationary time series and let $Y _ { t } = X _ { t } - X _ { t - 4 }$ define $\{ Y _ { t } \}$ .

(a) Find the power transfer function for this linear filter.   
(b) Graph the power transfer function and describe the effect of using this filter. That is, what frequencies will be retained (emphasized) and what frequencies will be deemphasized (attenuated) by this filtering?

13.30 Let $\{ X _ { t } \}$ be a stationary time series and let $\{ Y _ { t } \}$ be defined by $Y _ { t } ~ =$ $( X _ { t + 1 } - 2 X _ { t } + X _ { t - 1 } ) / 3$ .

(a) Find the power transfer function for this linear filter.   
(b) Graph the power transfer function and describe the effect of using this filter. That is, what frequencies will be retained (emphasized) and what frequencies will be deemphasized (attenuated) by this filtering?

13.31 Suppose that $\{ Y _ { t } \}$ is a white noise process not necessarily normal. Use the orthogonality properties given in Appendix J to establish the following at the Fourier frequencies.

(a) The sample spectral density is an unbiased estimator of the theoretical spectral density.   
(b) The variables $\hat { A } _ { f _ { 1 } }$ and $\hat { B } _ { f _ { 2 } }$ are uncorrelated for any Fourier frequencies $f _ { 1 } , f _ { 2 }$   
(c) If the Fourier frequencies $f _ { 1 } \neq f _ { 2 }$ , the variables $\hat { A } _ { f _ { 1 } }$ and $\hat { A } _ { f _ { 2 } }$ are uncorrelated.

13.32 Carry out a simulation analysis similar to those reported in Exhibits 13.21, 13.22, 13.23, and 13.24. Use an AR(2) model with $\phi _ { 1 } = 0 . 5$ , $\Phi _ { 2 } = - 0 . 8$ , and $n = 4 8$ . Replicate the series 1000 times.

(a) Display the average sample spectral density by frequency and compare it with large sample theory.   
(b) Display the standard deviation of the sample spectral density by frequency and compare it with large sample theory.   
(c) Display the QQ plot of the appropriately scaled sample spectral density compared with large sample theory at several frequencies. Discuss your results.

13.33 Carry out a simulation analysis similar to those reported in Exhibits 13.21, 13.22, 13.23, and 13.24. Use an AR(2) model with $\phi _ { 1 } = - 1$ , $\Phi _ { 2 } = - 0 . 7 5$ , and $n = 9 6$ . Replicate the time series 1000 times.

(a) Display the average sample spectral density by frequency and compare it with the results predicted by large sample theory.   
(b) Display the standard deviation of the sample spectral density by frequency and compare it with the results predicted by large sample theory.   
(c) Display the QQ plot of the appropriately scaled sample spectral density and compare with the results predicted by large sample theory at several frequencies. Discuss your results.

13.34 Simulate a zero-mean, unit-variance, normal white noise time series of length $n =$ 1000. Display the periodogram of the series, and comment on the results.

# Appendix J: Orthogonality of Cosine and Sine Sequences

For j, $k = 0$ , 1, 2,…, n/2, we have

$$
\sum_ {t = 1} ^ {n} \cos \left(2 \pi \frac {j}{n} t\right) = 0 \quad \text {i f} j \neq 0 \tag {13.J.1}
$$

$$
\sum_ {t = 1} ^ {n} \sin \left(2 \pi \frac {j}{n} t\right) = 0 \tag {13.J.2}
$$

$$
\sum_ {t = 1} ^ {n} \cos \left(2 \pi \frac {j}{n} t\right) \sin \left(2 \pi \frac {k}{n} t\right) = 0 \tag {13.J.3}
$$

$$
\sum_ {t = 1} ^ {n} \cos \left(2 \pi \frac {j}{n} t\right) \cos \left(2 \pi \frac {k}{n} t\right) = \left\{ \begin{array}{l l} \frac {n}{2} & \text {i f} j = k (j \neq 0 \text {o r} n / 2) \\ n & \text {i f} j = k = 0 \\ 0 & \text {i f} j \neq k \end{array} \right. \tag {13.J.4}
$$

$$
\sum_ {t = 1} ^ {n} \sin \left(2 \pi \frac {j}{n} t\right) \sin \left(2 \pi \frac {k}{n} t\right) = \left\{ \begin{array}{l l} \frac {n}{2} & \text {i f} j = k (j \neq 0 \text {o r} n / 2) \\ 0 & \text {i f} j \neq k \end{array} \right. \tag {13.J.5}
$$

These are most easily proved using DeMoivre’s theorem

$$
e ^ {- 2 \pi i f} = \cos (2 \pi f) - i \sin (2 \pi f) \tag {13.J.6}
$$

or, equivalently, Euler’s formulas,

$$
\cos (2 \pi f) = \frac {e ^ {2 \pi i f} + e ^ {- 2 \pi i f}}{2} \text {a n d} \sin (2 \pi f) = \frac {e ^ {2 \pi i f} - e ^ {- 2 \pi i f}}{2 i} \tag {13.J.7}
$$

together with the result for the sum of a finite geometric series, namely

$$
\sum_ {j = 1} ^ {n} r ^ {j} = \frac {r (1 - r ^ {n})}{1 - r} \tag {13.J.8}
$$

for real or complex $r \neq 1$ .

# CHAPTER 14

# ESTIMATING THE SPECTRUM

Several alternative methods for constructing reasonable estimators of the spectral density have been proposed and investigated over the years. We will highlight just a few of them that have gained the most acceptance in light of present-day computing power. So-called nonparametric estimation of the spectral density (that is, smoothing of the sample spectral density) assumes very little about the shape of the “true” spectral density. Parametric estimation assumes that an autoregressive model—perhaps of high order—provides an adequate fit to the time series. The estimated spectral density is then based on the theoretical spectral density of the fitted AR model. Some other methods are touched on briefly.

# 14.1 Smoothing the Spectral Density

The basic idea here is that most spectral densities will change very little over small intervals of frequencies. As such, we should be able to average the values of the sample spectral density over small intervals of frequencies to gain reduced variability. In doing so, we must keep in mind that we may introduce bias into the estimates if, in fact, the theoretical spectral density does change substantially over that interval. There will always be a trade-off between reducing variability and introducing bias. We will be required to use judgment to decide how much averaging to perform in a particular case.

Let $f$ be a Fourier frequency. Consider taking a simple average of the neighboring sample spectral density values centered on frequency $f$ and extending m Fourier frequencies on either side of $f$ . We are averaging $2 m + 1$ values of the sample spectrum, and the smoothed sample spectral density is given by

$$
\bar {S} (f) = \frac {1}{2 m + 1} \sum_ {j = - m} ^ {m} \hat {S} \left(f + \frac {j}{n}\right) \tag {14.1.1}
$$

(When averaging for frequencies near the end points of 0 and $\%$ , we treat the periodogram as symmetric about 0 and $\%$ .)

More generally, we may smooth the sample spectrum with a weight function or spectral window $W _ { m } ( f )$ with the properties

$$
\left. \begin{array}{l} W _ {m} (k) \geq 0 \\ W _ {m} (k) = W _ {m} (- k) \\ \sum_ {k = - m} ^ {m} W _ {m} (k) = 1 \end{array} \right\} \tag {14.1.2}
$$

and obtain a smoothed estimator of the spectral density as

$$
\bar {S} (f) = \sum_ {k = - m} ^ {m} W _ {m} (k) \hat {S} \left(f + \frac {k}{n}\right) \tag {14.1.3}
$$

The simple averaging shown in Equation (14.1.1) corresponds to the rectangular spectral window

$$
W _ {m} (k) = \frac {1}{2 m + 1} \text {f o r} - m \leq k \leq m \tag {14.1.4}
$$

For historical reasons, this spectral window is usually called the Daniell spectral window after P. J. Daniell, who first used it in the 1940s.

As an example, consider the simulated AR(1) series whose sample spectral density was shown in Exhibit 13.20 on page 341. Exhibit 14.1 displays the smoothed sample spectrum using the Daniell window with $m = 5$ . The true spectrum is again shown as a dotted line. The smoothing did reduce some of the variability that we saw in the sample spectrum.

![](images/aa897ecce34d545fbddc40d2d8f24cf8861faca3d285e257141477ce77314acf.jpg)  
Exhibit 14.1 Smoothed Spectrum Using the Daniell Window With $m = 5$

> win.graph(width $\mathord { \left. \begin{array} { r l r } \end{array} \right. } = 4$ .875,height=2.5,pointsize=8)   
> set.seed(271435); $\scriptstyle \mathtt { n } = 2 0 0$ ; phi $. =$ -0.6   
> y=arima.sim(model=list $\mathtt { a r } \mathtt { = p h i }$ ), $\mathrm { n } = \mathrm { n }$ )   
> $\mathrm { k } =$ kernel('daniell', $\mathtt { m } = 5$ )

```txt
> sp=spec(y,kernel=k,log='no',sub='','xlab='Frequency', ylab='Smoothed Sample Spectral Density')
> lines(sp$freq,ARMAspec(model=list(ar=phi),freq=sp$freq, plot=F) $spec,fty='dotted') 
```

If we make the smoothing window wider (that is, increase m) we will reduce the variability even further. Exhibit 14.2 shows the smoothed spectrum with a choice of $m =$ 15. The danger with more and more smoothing is that we may lose important details in the spectrum and introduce bias. The amount of smoothing needed will always be a matter of judgmental trial and error, recognizing the trade-off between reducing variability at the expense of introducing bias.

![](images/cd6275c8f7ef52a9958497528c65fc33d00894d61c0902679d831dcdd458db94.jpg)  
Exhibit 14.2 Smoothed Spectrum Using the Daniell Window With m = 15

```txt
> k=kernel('daniell',m=15)
> sp=spec(y,k,log='no',sub='',xlab='Frequency',
ylab='Smoothed Sample Spectral Density')
> lines(sp$freq,ARMAspec(model=list(ar=phi),freq=sp$freq,
plot=F) $spec,fty='dotted') 
```

# Other Spectral Windows

Many other spectral windows have been suggested over the years. In particular, the abrupt change at the end points of the Daniell window could be softened by making the weights decrease at the extremes. The so-called modified Daniell spectral window simply defines the two extreme weights as half of the other weights still retaining the property that the weights sum to 1. The leftmost graph in Exhibit 14.3 shows the modified Daniell spectral window for $m = 3$ .

![](images/1aab9d96410a5287fcda14a2d61861f51c8fdc1a6338dd99bdc929b5e0d89d2e.jpg)  
Exhibit 14.3 The Modified Daniell Spectral Window and Its Convolutions   
k

![](images/9c54f2819586ebf0404228e8479af9febd095f6b572c040ae16d112a1b4ef2b6.jpg)  
k

![](images/998eb6da92557958552cdc53759a8060b429d718a294663e42bc0b1c597ddf9b.jpg)  
k

Another common way to modify spectral windows is to use them to smooth the periodogram more than once. Mathematically, this amounts to using the convolution of the spectral windows. If the modified Daniell spectral window with $m = 3$ is used twice (convolved with itself), we in fact are using the (almost) triangular-shaped window shown in the middle display of Exhibit 14.3. A third smoothing (with $m = 3$ ) is equivalent to using the spectral window shown in the rightmost panel. This spectral window appears much like a normal curve. We could also use different values of $m$ in the various components of the convolutions.

Most researchers agree that the shape of the spectral window is not nearly as important as the choice of $m$ (or the bandwidth—see below). We will use the modified Daniell spectral window—possibly with one or two convolutions—in our examples.†

# 14.2 Bias and Variance

If the theoretical spectral density does not change much over the range of frequencies that the smoothing window covers, we expect the smoothed estimator to be approximately unbiased. A calculation using this approximation, the spectral window properties in Equations (14.1.2), and a short Taylor expansion produces

$$
\begin{array}{l} E [ \bar {S} (f) ] \approx \sum_ {k = - m} ^ {m} W _ {m} (k) S \left(f + \frac {k}{n}\right) \\ \approx \sum_ {k = - m} ^ {m} W _ {m} (k) \bigg [ S (f) + \frac {k}{n} S ^ {\prime} (f) + \frac {1}{2} \Big (\frac {k}{n} \Big) ^ {2} S ^ {\prime \prime} (f) \bigg ] \\ \end{array}
$$

or

$$
E [ \bar {S} (f) ] \approx S (f) + \frac {1}{n ^ {2}} \frac {S ^ {\prime \prime} (f)}{2} \sum_ {k = - m} ^ {m} k ^ {2} W _ {m} (k) \tag {14.2.1}
$$

So an approximate value for the bias in the smoothed spectral density is given by

$$
\text {b i a s} \approx \frac {1}{n ^ {2}} \frac {S ^ {\prime \prime} (f)}{2} \sum_ {k = - m} ^ {m} k ^ {2} W _ {m} (k) \tag {14.2.2}
$$

For the Daniell rectangular spectral window, we have

$$
\frac {1}{n ^ {2}} \sum_ {k = - m} ^ {m} k ^ {2} W _ {m} (k) = \frac {2}{n ^ {2} (2 m + 1)} \left(\frac {m ^ {3}}{3} + \frac {m ^ {2}}{2} + \frac {m}{6}\right) \tag {14.2.3}
$$

and thus the bias tends to zero as $n \to \infty$ as long as $m / n  0$ .

Using the fact that the sample spectral density values at the Fourier frequencies are approximately uncorrelated and Equation (13.6.5) on page 341, we may also obtain a useful approximation for the variance of the smoothed spectral density as

$$
V a r [ \overline {{S}} (f) ] \approx \sum_ {k = - m} ^ {m} W _ {m} ^ {2} (k) V a r \Bigl [ \hat {S} \Bigl (f + \frac {k}{n} \Bigr) \Bigr ] \approx \sum_ {k = - m} ^ {m} W _ {m} ^ {2} (k) S ^ {2} (f)
$$

so that

$$
\operatorname {V a r} [ \bar {S} (f) ] \approx S ^ {2} (f) \sum_ {k = - m} ^ {m} W _ {m} ^ {2} (k) \tag {14.2.4}
$$

Note that for the Daniell or rectangular spectral window $\sum _ { \ i = - m } ^ { m } W _ { m } ^ { 2 } ( k ) = \frac { 1 } { 2 m + 1 }$ , so that as long as $m  \infty$ (as $n  \infty$ ) we have consistency.

In general, we require that as $n \to \infty$ we have $m / n  0$ to reduce bias and $m  \infty$ to reduce variance. As a practical matter, the sample size $n$ is usually fixed and we must choose m to balance bias and variance considerations.

Jenkins and Watts (1968) suggest trying three different values of m. A small value will give an idea where the large peaks in $S ( f )$ are but may show a large number of peaks, many of which are spurious. A large value of $m$ may produce a curve that is likely to be too smooth. A compromise may then be achieved with the third value of m. Chatfield (2004, p. 135) suggests using $\dot { m } = { \sqrt { n } }$ . Often trying values for $m$ of $2 { \sqrt { n } }$ , $\sqrt { n }$ , and $\sqrt [ 3 ] { 2 } \sqrt { n }$ will give you some insight into the shape of the true spectrum. Since the width of the window decreases as m decreases, this is sometimes called window closing. As Hannan (1973, p. 311) says, “Experience is the real teacher and cannot be got from a book.”

# 14.3 Bandwidth

In the approximate bias given by Equation (14.2.2), notice that the factor $S ^ { \prime \prime } ( f )$ depends on the curvature of the true spectral density and will be large in magnitude if there is a sharp peak in $S ( f )$ near $f$ but will be small when $S ( f )$ is relatively flat near $f .$ . This makes intuitive sense, as the motivation for the smoothing of the sample spectral density assumed that the true density changed very little over the range of frequencies used in the spectral window. The square root of the other factor in the approximate bias from

Equation (14.2.2) is sometimes called the bandwidth, BW, of the spectral window, namely

$$
B W = \frac {1}{n} \sqrt {\sum_ {k = - m} ^ {m} k ^ {2} W _ {m} (k)} \tag {14.3.1}
$$

As we noted in Equation (14.2.3), for the Daniell window this BW will tend to zero as n $ \infty$ as long as $m / n  0$ . From Equations (14.1.2) on page 352 a spectral window has the mathematical properties of a discrete zero-mean probability density function, so the BW defined here may be viewed as proportional to the standard deviation of the spectral window. As such, it is one way to measure the width of the spectral window. It is interpreted as a measure of width of the band of frequencies used in smoothing the sample spectral density. If the true spectrum contains two peaks that are close relative to the bandwidth of the spectral window, those peaks will be smoothed together when we cal-_ culate $\bar { \cal S } ( f )$ and they will not be seen as separate peaks. It should be noted that there are many alternative definitions of bandwidth given in the time series literature. Priestley (1981, pp. 513–528) spends considerable time discussing the advantages and disadvantages of the various definitions.

# 14.4 Confidence Intervals for the Spectrum

The approximate distributional properties of the smoothed spectral density may be easily used to obtain confidence intervals for the spectrum. The smoothed sample spectral density is a linear combination of quantities that have approximate chi-square distributions. A common approximation in such a case is to use some multiple of another chi-square distribution with degrees of freedom obtained by matching means and vari-_ ances. Assuming $\bar { S } ( f )$ to be roughly unbiased with variance given by Equation (14.2.4), matching means and variances leads to approximating the distribution of

$$
\frac {\mathrm {v} \bar {S} (f)}{S (f)} \tag {14.4.1}
$$

by a chi-square distribution with degrees of freedom given by

$$
v = \frac {2}{\sum_ {k = - m} ^ {m} W _ {m} ^ {2} (k)} \tag {14.4.2}
$$

Lettingdegrees of fr $\chi _ { \mathrm { v } , \alpha / 2 } ^ { 2 }$ be the he ineq $1 0 0 ( \alpha / 2 )$ th percentile of a chi-square distribution with ν

$$
\chi_ {\mathrm {v}, \alpha / 2} ^ {2} <   \frac {\mathrm {v} \bar {S} (f)}{S (f)} <   \chi_ {\mathrm {v}, 1 - \alpha / 2} ^ {2}
$$

can be converted into a $1 0 0 ( 1 - \alpha ) \%$ confidence statement for $S ( f )$ as

$$
\frac {\mathrm {v} \bar {S} (f)}{\chi_ {\mathrm {v} , 1 - \alpha / 2} ^ {2}} <   S (f) <   \frac {\mathrm {v} \bar {S} (f)}{\chi_ {\mathrm {v} , \alpha / 2} ^ {2}} \tag {14.4.3}
$$

In this formulation, the width of the confidence interval will vary with frequency. A_ review of Equation (14.2.4) on page 355 shows that the variance of $\bar { \cal S } ( f )$ is roughly proportional to the square of its mean. As we saw earlier in Equations (5.4.1) and (5.4.2) on page 98, this suggests that we take the logarithm of the smoothed sample spectral density to stabilize the variance and obtain confidence intervals with width independent of frequency as follows:

$$
\log [ \bar {S} (f) ] + \log \left[ \frac {\nu}{\chi_ {\mathrm {v} , 1 - \alpha / 2} ^ {2}} \right] \leq \log [ S (f) ] \leq \log [ \bar {S} (f) ] + \log \left[ \frac {\nu}{\chi_ {\mathrm {v} , \alpha / 2} ^ {2}} \right] \tag {14.4.4}
$$

For these reasons it is common practice to plot the logarithms of estimated spectra. If we redo Exhibit 14.2 on page 353 in logarithm terms, we obtain the display shown in Exhibit 14.4, where we have also drawn in the $9 5 \%$ confidence limits (dotted) and the true spectral density (dashed) from the AR(1) model. With a few exceptions, the confidence limits capture the true spectral density.

![](images/c6729188bf5d82b53c41af22f48585b09ad759123471ef73c57311ea4615db9b.jpg)  
Exhibit 14.4 Confidence Limits from the Smoothed Spectral Density

```javascript
> set.seed(271435); n=200; phi=-0.6
> y=arima.sim(model=list(ar=phi),n=n)
> k=kernel('daniell',m=15)
> sp=spec(y,kernel=k,sub=' ',xlab='Frequency',
                     ylab='Log(Smoothed Spectral Density) ', ci.plot=T,ci.col=NULL)
> lines(sp$freq,ARMAspec(model=list(ar=phi),sp$freq,plot=F) $spec,
                     lty='dashed') 
```

Exhibit 14.5 shows a less cluttered display of confidence limits. Here a $9 5 \%$ confidence interval and bandwidth guide is displayed in the upper right-hand corner—the “crosshairs.” The vertical length gives the length (width) of a confidence interval, while

the horizontal line segment indicates the central point† of the confidence interval, and its width (length) matches the bandwidth of the spectral window. If you visualize the guide repositioned with the crosshairs centered on the smoothed spectrum above any frequency, you have a visual display of a vertical confidence interval for the “true” spectral density at that frequency and a rough guide of the extent of the smoothing. In this simulated example, we also show the true spectrum as a dotted line.

![](images/3cb028f08e8d3be46dfc740bfba87e0f200abc09cbe48d00b1c36e15bf0c3c31.jpg)  
Exhibit 14.5 Logarithm of Smoothed Spectrum from Exhibit 14.2

```txt
> sp=spec(y, span=31, sub='', xlab='Frequency',
ylab='Log(Smoothed Sample Spectrum)')
> lines(sp$freq, ARMA spec(model=list(ar=phi), sp$freq,
plot=F) $spec, lty='dotted') 
```

# 14.5 Leakage and Tapering

Much of the previous discussion has assumed that the frequencies of interest are the Fourier frequencies. What happens if that is not the case? Exhibit 14.6 displays the periodogram of a series of length $n = 9 6$ with two pure cosine-sine components at frequencies $\mathnormal { f } = 0 . 0 8 8$ and $f = 1 4 / 9 6$ . The model is simply

$$
Y _ {t} = 3 \cos [ 2 \pi (0. 0 8 8) t ] + \sin \left[ 2 \pi \left(\frac {1 4}{9 6}\right) t \right] \tag {14.5.1}
$$

Note that with $n = 9 6 , f = 0 . 0 8 8$ is not a Fourier frequency. The peak with lower power at the Fourier frequency $f = 1 4 / 9 6$ is clearly indicated. However, the peak at $\mathnormal { f = } 0 . 0 8 8$ is not

there. Rather, the power at this frequency is blurred across several nearby frequencies, giving the appearance of a much wider peak.

![](images/6ea10c48c47d713c5fcf113fcf92c6dcfab4159bc81c22ac0d0cf3e004ef81ef.jpg)  
Exhibit 14.6 Periodogram of Series with Peaks at $\pmb { f = 0 . 0 8 8 }$ and f = 14/96

> win.graph(width $\mathord { \left. \begin{array} { r l r } \end{array} \right. } = 4$ .875,height $^ { . = 2 }$ .5,pointsize=8)   
> $\mathtt { t } = \mathtt { l } : 9 6$ ; f1=0.088; f2=14/96   
> y=3*cos(f1*2*pi*t)+sin(f2*2*pi*t)   
> periodogram(y); abline $\mathrm { ( h = 0 }$

An algebraic analysis† shows that we may view the periodogram as a “smoothed” spectral density formed with the Dirichlet kernel spectral window given by

$$
D (f) = \frac {1}{n} \frac {\sin (n \pi f)}{\sin (\pi f)} \tag {14.5.2}
$$

Note that for all Fourier frequencies $f = j / n , D ( f ) = 0$ $f { = } j / n$ , so this window has no effect whatsoever at those frequencies. However, the plot of $D ( f )$ given on the left-hand side of Exhibit 14.7 shows significant “side lobes” on either side of the main peak. This will cause power at non-Fourier frequencies to leak into the supposed power at the nearby Fourier frequencies, as we see in Exhibit 14.6.

Tapering is one method used to improve the issue with the side lobes. Tapering involves decreasing the data magnitudes at both ends of the series so that the values move gradually toward the data mean of zero. The basic idea is to reduce the end effects of computing a Fourier transform on a series of finite length. If we calculate the periodogram after tapering the series, the effect is to use the modified Dirichlet kernel shown on the right-hand side of Exhibit 14.7 for $n = 1 0 0$ . Now the side lobes have essentially disappeared.

![](images/cd4b63e1cee3d060299bca4909e45dfb55a4a914cba3833f49f85b147905b557.jpg)  
Exhibit 14.7 Dirichlet Kernel and Dirichlet Kernel after Tapering

![](images/3555a5c23b18656d9e738ea77cff3f5600cfc0c3069b41b6a6be0413adf1e94e.jpg)

The most common form of tapering is based on a cosine bell. We replace the original series $Y _ { t }$ by $\tilde { Y } _ { t }$ , with

$$
\tilde {Y} _ {t} = h _ {t} Y _ {t} \tag {14.5.3}
$$

where, for example, $h _ { t }$ is the cosine bell given by

$$
h _ {t} = \frac {1}{2} \left\{1 - \cos \left[ \frac {2 \pi (t - 0 . 5)}{n} \right] \right\} \tag {14.5.4}
$$

A graph of the cosine bell with $n = 1 0 0$ is given on the left-hand side of Exhibit 14.8. A much more common taper is given by a split cosine bell that applies the cosine taper only to the extremes of the time series. The split cosine bell taper is given by

$$
h _ {t} = \left\{ \begin{array}{l l} \frac {1}{2} \left\{1 - \cos \left[ \frac {\pi (t - 1 / 2)}{m} \right] \right\} & \text {f o r} 1 \leq t \leq m \\ 1 & \text {f o r} m + 1 \leq t \leq n - m \\ \frac {1}{2} \left\{1 - \cos \left[ \frac {\pi (n - t + 1 / 2)}{m} \right] \right\} & \text {f o r} n - m + 1 \leq t \leq n \end{array} \right. \tag {14.5.5}
$$

which is called a $100 p \%$ cosine bell taper with $p = 2 m / n$ . A $10 \%$ split cosine bell taper is shown on the right-hand side of Exhibit 14.8 again with $n = 1 0 0$ . Notice that there is a $10 \%$ taper on each end, resulting in a total taper of $20 \%$ . In practice, split cosine bell tapers of $10 \%$ or $20 \%$ are in common use.

![](images/ab10b0b2f57f3e64890c9818b9241eba02290811057f37ce496e1dcd8b51307f.jpg)  
Exhibit 14.8 Cosine Bell and $10 \%$ Taper Split Cosine Bell for $\begin{array} { r } { n = 1 0 0 } \end{array}$

![](images/44757e9224524087cfd34a4bf439f94f4b7872f368a091c34ab5f1aebd2567e1.jpg)

We return to the variable star brightness data first explored on page 325. Exhibit 14.9 displays four periodograms of this series, each with a different amount of tapering. Judging by the length of the $9 5 \%$ confidence intervals displayed in the respective “crosshairs”, we see that the two peaks found earlier in the raw untapered periodogram at frequencies $f _ { 1 } = 2 1 / 6 0 0$ and $f _ { 2 } { = } 2 5 / 6 0 0$ are clearly real. A more detailed analysis of the minor peaks shown best in the bottom periodogram are all in fact harmonics of the frequencies $f _ { 1 }$ and $f _ { 2 }$ . There is much more on the topic of leakage reduction and tapering in Bloomfield (2000).

![](images/7434342a50067b9216ad5481d5d62d78b6441635189f3708b4a7f5f532d17b71.jpg)  
Exhibit 14.9 Variable Star Spectra with Tapers of $0 \%$ , $10 \%$ , $20 \%$ , and $50 \%$

![](images/ed7903608c6bd05af67fb215af3cf0e7c1e11d63fba9237a330946099be21c6a.jpg)

![](images/412e4d73c66fb61356727ab9efe9a6ec1fe874ced235e71ec39a33855cfaf1f3.jpg)

![](images/2ecd2442b304ca1be97f4a43e404559f3543f57c308803133b266643176d70b2.jpg)

# 14.6 Autoregressive Spectrum Estimation

In the preceding sections on spectral density estimation, we did not make any assumptions about the parametric form of the true spectral density. However, an alternative method for estimating the spectral density would be to consider fitting an AR, MA, or ARMA model to a time series and then use the spectral density of that model with estimated parameters as our estimated spectral density. (Section 13.5, page 332, discussed the spectral densities of ARMA models.) Often AR models are used with possibly large order chosen to minimize the AIC criterion.

As an example, consider the simulated AR series with $\phi = - 0 . 6$ and $n = 2 0 0$ that we used in Exhibits 13.20, 14.1, 14.2, and 14.5. If we fit an AR model, choosing the order to minimize the AIC, and then plot the estimated spectral density for that model, we obtain the results shown in Exhibit 14.10.

![](images/876f7f1e0c05a96cde5fb643ab6a28154cc294c0aabd48d2c7a4d55539be8bb1.jpg)  
Exhibit 14.10 Autoregressive Estimation of the Spectral Density

```txt
> sp=spec(y, method='ar', sub='', xlab='Frequency',
ylab='Log(AR Spectral Density Estimate')
> lines(sp$freq, ARMAspec(model=list(ar=phi), freq=sp$freq,
plot=F) $spec, lty='dotted') 
```

Since these are simulated data, we also show the true spectral density as a dotted line. In this case, the order was chosen as $p = 1$ and the estimated spectral density follows the true density very well. We will show some examples with real time series in Section 14.8.

# 14.7 Examples with Simulated Data

A useful way to get a feel for spectral analysis is with simulated data. Here we know what the answers are and can see what the consequences are when we make choices of spectral window and bandwidth. We begin with an AR(2) model that contains a fairly strong peak in its spectrum.

# AR(2) with $\$ 123$ , $\$ 2$ : A Peak Spectrum

The spectral density for this model contained a peak at about $f = 0 . 0 8$ , as displayed in Exhibit 13.14 on page 336. We simulated a time series from this AR(2) model with normal white noise terms with unit variance and sample size $n = 1 0 0$ . Exhibit 14.11 shows three estimated spectral densities and the true density as a solid line. We used the modified Daniell spectral window with three different values for $s p a n = 2 m + 1$ of 3, 9, and 15. A span of 3 gives the least amount of smoothing and is shown as a dotted line. A span of 9 is shown as a dashed line. With span $= 1 5$ , we obtain the most smoothing, and this curve is displayed with a dot-dash pattern. The bandwidths of these three spectral windows are 0.018, 0.052, and 0.087, respectively. The confidence interval and bandwidth guide displayed apply only to the dotted curve estimate. The two others have wider bandwidths and shorter confidence intervals. The estimate based on span $= 9$ is probably the best one, but it does not represent the peak very well.

![](images/710a38693571095ed54eca42d19a6df96ebd99f535257e2aef74ab56bc83f02a.jpg)  
Exhibit 14.11 Estimated Spectral Densities

```javascript
> win.graph(width=4.875,height=2.5,pointsize=8)
> set.seed(271435); n=100; phi1=1.5; phi2=-.75
> y=arima.sim(model=list(ar=c(phil,phi2)),n=n)
> sp1=spec(y, spans=3, sub='', lty='dotted', xlab='Frequency',
ylab='Log(Estimated Spectral Density)')
> sp2=spec(y, spans=9, plot=F); sp3=spec(y, spans=15, plot=F)
> lines(sp2$freq, sp2$spec, lty='dashed')
> lines(sp3$freq, sp3$spec, lty='dotdash') 
```

> f=seq(0.001,.5,by $^ { \ast } =$ .001)   
> lines(f,ARMAspec(model=list $\mathtt { a r } \mathtt { = C }$ (phi1,phi2)),freq $\cdot = \mathtt { f }$ , plot=F)$spec,lty='solid')

We also used the parametric spectral estimation idea and let the software choose the best AR model based on the smallest AIC. The result was an estimated AR(2) model with the spectrum shown in Exhibit 14.12. This is a very good representation of the underlying spectrum, but of course the model was indeed AR(2).

![](images/a81ae191bffda29a492c2a1cd0ba53c0736b6e6d2a23dbace1289b9c3fef717f.jpg)  
Exhibit 14.12 AR Spectral Estimation: Estimated (dotted), True (solid)

> sp4=spec(y,method='ar',lty='dotted', xlab $1 = 1$ Frequency',ylab $^ { 1 = }$ 'Log(Estimated AR Spectral Density)')   
> f=seq(0.001,0.5, by 0.001)   
> lines(f,ARMAspec(model $. =$ list $\mathtt { a r } \mathtt { = C }$ (phi1,phi2)),freq=f, plot=F)$spec,lty $\underline { { \underline { { \mathbf { \Pi } } } } } =$ 'solid')   
> sp4$method # This will tell you order of the AR model selected

# AR(2) with $\phi _ { 1 } = 0 . 1$ , $\phi _ { 2 } = 0 . 4$ : A Trough Spectrum

Next we look at an AR(2) model with a trough spectrum and a larger sample size. The true spectrum is displayed in Exhibit 13.15 on page 337. We simulated this model with $n = 2 0 0$ and unit-variance normal white noise. The three smoothed spectral estimates shown are based on spans of 7, 15, and 31. As before, the confidence limits and bandwidth guide correspond to the smallest span of 7 and hence give the narrowest bandwidth and longest confidence intervals. In our opinion, the middle value of span $= 1 5$ , which is about $\sqrt { n }$ , gives a reasonable estimate of the spectrum.

Exhibit 14.13 Estimated Spectrum for AR(2) Trough Spectrum Model   
![](images/78f056224d706d617826b0aa0b9e1cf5713768a62ea23ff14f201ed6f8690386.jpg)  
> Use the R code for Exhibit 14.11 with new values for the > parameters.

Exhibit 14.14 shows the AR spectral density estimate. The minimum AIC was achieved at the true order of the underlying model, AR(2), and the estimated spectral density is quite good.

Exhibit 14.14 AR Spectral Estimation: Estimated (dotted), True (solid)   
![](images/ce68ce4e34170b80835d7addfca951fd3313fd30b73b71ca8e30a78ec65e49cc.jpg)  
> Use the R code for Exhibits 14.11 and 14.12 with new values > for the parameters.

# ARMA(1,1) with $\phi = 0 . 5$ , θ = 0.8

The true spectral density of the mixed model ARMA(1,1) with $\phi = 0 . 5$ and $\theta = 0 . 8$ was shown in Exhibit 13.17 on page 338. This model has substantial medium- and high-frequency content but very little power at low frequencies. We simulated this model with a sample size of $n = 5 0 0$ and unit-variance normal white noise. Using $\sqrt { n } \approx 2 2$ as a guide for choosing $m$ , we show three estimates with $m$ of 11, 23, and 45 in Exhibit 14.15. The confidence interval guide indicates that the many peaks produced when $m = 1 1$ are likely spurious (which, in fact, they are). With such a smooth underlying spectrum, the maximum smoothing shown with $m = 4 5$ produces a rather good estimate.

# Exhibit 14.15 Spectral Estimates for an ARMA(1,1) Process

![](images/0b20e265843d1bdfa077d23a8a9977bc60b62677eab72b678e95980913027d15.jpg)

```txt
> win.graph(width=4.875,height=2.5,pointsize=8)
> set.seed(324135); n=500; phi=.5; theta=.8
> y=arima.sim(model=list(ar=phi,ma=-theta),n=n)
> sp1=spec(y, spans=11,sub=' ',lty='dotted',
                     xlab='Frequency',ylab='Log(Estimated Spectral Density)')
> sp2=spec(y, spans=23,plot=F); sp3=spec(y, spans=45,plot=F)
> lines(sp2$freq,sp2$spec,lty='dashed')
> lines(sp3$freq,sp3$spec,lty='dotdash')
> f=seq(0.001,.5,by=.001)
> lines(f,ARMA spec(model=list(ar=phi,ma=-theta),f,
                     plot=F) $spec,lty='solid') 
```

In this case, a parametric spectral estimate based on AR models does not work well, as shown in Exhibit 14.16. The software selected an AR(3) model, but the resulting spectral density (dotted) does not reproduce the true density (solid) well at all.

![](images/7e083f12db170a7022f528ea1fd54e925477add06cfbd122d8449b8e81bc3a46.jpg)  
Exhibit 14.16 AR Spectral Estimate for an ARMA(1,1) Process

```txt
> sp4=spec(y, method='ar', lty='dotted', ylim=c(.15, 1.9), xlab='Frequency', ylab='Log(Estimated AR Spectral Density)')  
> f=seq(0.001, .5, by=.001)  
> lines(f, ARMAspec(model=list(ar=phi, ma=-theta), f, plot=F) $spec, lty='solid') 
```

# Seasonal MA with $\mathbf { \boldsymbol { \theta } } = \mathbf { 0 . 4 }$ , $\Theta = 0 . 9$ , and $\pmb { \mathscr { s } } = 1 2$

For our final example with simulated data, we choose a seasonal process. The theoretical spectral density is displayed in Exhibit 13.19 on page 340. We simulated $n = 1 4 4$ data points with unit-variance normal white noise. We may think of this as 12 years of monthly data. We used modified Daniell spectral windows with span $= 6$ , 12, and 24 based on $\sqrt { n } \approx 1 2$ .

This spectrum contains a lot of detail and is difficult to estimate with only 144 observations. The narrowest spectral window hints at the seasonality, but the two other estimates essentially smooth out the seasonality. The confidence interval widths (corresponding to $m = 6$ ) do seem to confirm the presence of real seasonal peaks.

![](images/615ff609c1c9d75eb623da8da368f8944b12276be864d63181301ba66460e2c2.jpg)  
Exhibit 14.17 Spectral Estimates for a Seasonal Process

```txt
> win.graph(width=4.875,height=2.5,pointsize=8)
> set.seed(247135); n=144; theta=.4;THETA=.9
> y=arima.sim(model=list(ma=c(-theta,rep(0,10),-THETA,theta*THETA)),n=n)
> sp1=spec(y, spans=7,sub='',lty='dotted',ylim=c(.15,9),
xlab='Frequency',ylab='Log(Estimated Spectral Density)')
> sp2=spec(y, spans=13,plot=F); sp3=spec(y, spans=25,plot=F)
> lines(sp2$freq,sp2$spec,lty='dashed')
> lines(sp3$freq,sp3$spec,lty='dotdash')
> f=seq(0.001,.5,by=.001)
> lines(f,ARMApec(model=list(ma=-theta,seasonal=list(sma=-THETA,
period=12)),freq=f,plot=F)\(spec,lty='solid') 
```

![](images/e32fbfd5e2f4b23d6c9a4c22f8c3f469f1c9447b861ff91b4f0e5d883c24ca24.jpg)  
Exhibit 14.18 AR Spectral Estimates for a Seasonal Process

```txt
> sp4=spec(y, method='ar',ylim=c(.15,15),lty='dotted', xlab='Frequency',ylab='Log(Estimated AR Spectral Density)') 
```

```r
> f=seq(0.001,.5,by=.001)
> lines(f,ARMAspec(model=list (ma=-theta,seasonal=list(sma=-THETA, period=12)),freq=f,plot=F) $spec,fty='solid') 
```

Exhibit 14.18 shows the estimated spectrum based on the best AR model. An order of 13 was chosen based on the minimum AIC, and the seasonality does show up quite well. However, the peaks are misplaced at the higher frequencies. Perhaps looking at both Exhibit 14.17 and Exhibit 14.18 we could conclude that the seasonality is real and that a narrow spectral window provides the best estimate of the underlying spectral density given the sample size available.

As a final estimate of the spectrum, we use a convolution of two modified Daniell spectral windows each with span $= 3$ , as displayed in the middle of Exhibit 14.3 on page 354. The estimated spectrum is shown in Exhibit 14.19. This is perhaps the best of the estimates that we have shown.

![](images/adc2d64f2d93f5c83621b29b2a79fed8161c0f78dd9e254f513f5a1ae349b487.jpg)  
Exhibit 14.19 Estimated Seasonal Spectrum with Convolution Window

```r
> sp5=spec(y, spans=c(3,3), sub='', lty='dotted',
xlab='Frequency', ylab='Log(Estimated Spectral Density)')
> f=seq(0.001,.5, by=.001)
> lines(f, ARMAspec(model=list(ma=-theta, seasonal=list(sma=-THETA,
period=12)), freq=f, plot=F) $spec, lty='solid') 
```

# 14.8 Examples with Actual Data

# An Industrial Robot

An industrial robot was put through a sequence of maneuvers, and the distance from a desired target end position was recorded in inches. This was repeated 324 times to form the time series shown in Exhibit 14.20.

Exhibit 14.20 Industrial Robot End Position Time Series   
![](images/efca474b0f09e749e9e9a569a31ddf531328a9dc61dbdcdaa4a47dda510975e4.jpg)  
> data(robot) > plot(robot,ylab $^ { \prime = }$ 'End Position Offset',xlab='Time')

Estimates of the spectrum are displayed in Exhibit 14.21 using the convolution of two modified Daniell spectral windows with $m = 7$ (solid) and with a $10 \%$ taper on each end of the series. A plot of this spectral window is shown in the middle of Exhibit 14.3 on page 354. The spectrum was also estimated using a fitted AR(7) model (dotted), the order of which was chosen to minimize the AIC. Given the length of the $9 5 \%$ confidence interval shown, we can conclude that the peak at around a frequency of 0.15 in both estimates is probably real, but those shown at higher frequencies may well be spurious. There is a lot of power shown at very low frequencies, and this agrees with the slowly drifting nature of the series that may be seen in the time series plot in Exhibit 14.20.

![](images/13558a465bec870e3f6fd41e13b97d7e6ccab98a88a5b47b63575ecb9f5b09ca.jpg)  
Exhibit 14.21 Estimated Spectrum for the Industrial Robot

```txt
>spec (robot, spans=c(7, 7), taper=.1, sub='', xlab='Frequency', ylab='Log (Spectrum)')
> s=spec (robot, method='ar', plot=F)
> lines(s$freq, s$spec, lty='dotted') 
```

# River Flow

Exhibit 14.22 shows monthly river flow for the Iowa River measured at Wapello, Iowa, for the period September 1958 through August 2006. The data are quite skewed toward the high values, but this was greatly improved by taking logarithms for the analysis.

Exhibit 14.22 River Flow Time Series   
![](images/72446c5f80e24bccdaa7502b279a91444f3cafdb43a88515985d75ef9aeab000.jpg)  
> data(flow); plot(flow,ylab='River Flow')

The sample size for these data is 576 with a square root of 24. The bandwidth of a modified Daniell spectral window is about 0.01. After some experimentation with several spectral window bandwidths, we decided that such a window smoothed too much and we instead used a convolution of two such windows, each with span $= 7$ . The bandwidth of this convolved window is about 0.0044. The smoothed spectral density estimate is shown as a solid curve in Exhibit 14.23 together with an estimate based on an AR(7) model (dotted) chosen to minimize the AIC. The prominent peak at frequency 1/12 represents the strong annual seasonality. There are smaller secondary peaks at about $f \approx 0 . 1 7$ and $f \approx 0 . 2 5$ that correspond to multiples of the fundamental frequency of 1/12. They are higher harmonics of the annual frequency.

![](images/721f9673cc54ceaf53becce47e99538e42d05e90d1ca7c4714538a1a3837f65c.jpg)  
Exhibit 14.23 Log(Spectrum) of Log(Flow)

```txt
>spec(log.flow),spans=c(7,7)，ylim=c(.02,13)，sub='',
ylab='Log(Spectrum)','xlab='Frequency')
> s=spec(log.flow),method='ar',plot=F)
> lines(s$freq,s$spec,lty='dotted') 
```

# Monthly Milk Production

The top portion of Exhibit 11.14 on page 264, showed U.S. monthly milk production from January 1994 through December of 2005. There is a substantial upward trend together with seasonality. We first remove the upward trend with a simple linear time trend model and consider the residuals from that regression—the seasonals. After trying several spectral bandwidths, we decided to use a convolution of two modified Daniell windows, each with span $= 3$ . We believe that otherwise there was too much smoothing. This was confirmed by estimating an AR spectrum that ended up fitting an AR of order 15 with peaks at the same frequencies. Notice that the peaks shown in Exhibit 14.24 are located at frequencies 1/12, 2/12,…, 6/12, with the peak at 1/12 showing the most power.

![](images/787add42df21acf8bce0496287301f00c0a4bc1a8aff887ed1c4121538e1dd02.jpg)  
Exhibit 14.24 Estimated Spectrum for Milk Production Seasonals

```python
> data(milk)  
> spec(milk, spans=c(3, 3), detrend=T, sub='', ylab='Estimated Log(Spectrum) ', xlab='Frequency')  
> abline(v=seq(1:6)/12, lty='dotted') 
```

For a final example in this section, consider the time series shown in Exhibit 14.25. These plots display the first 400 points of two time series of lengths 4423 and 4417, respectively. The complete series were created by recording a trombonist and a euphoniumist each sustaining a B flat (just below middle C) for about 0.4 seconds. The original recording produced data sampled at $4 4 . 1 \mathrm { M H z }$ , but this was reduced by subsampling every fourth data point for the analysis shown. Trombones and euphonia are both brass wind instruments that play in the same range, but they have different sized and shaped tubing. The euphonium has larger tubing (a larger bore) that is mostly conical in shape, while the tenor trombone is mostly cylindrical in shape and has a smaller bore. The euphonium sound is considered more mellow than the bright, brassy sound of the trombone. When one listens to these notes being played, they sound rather similar. Our ques-

tion is: Does the tubing shape and size affect the harmonics (overtones) enough that the differences may be seen in the spectra of these sounds?

# Exhibit 14.25 Trombone and Euphonium Playing Bb

![](images/b964494ef6b6976541ee5105d0959c87656c60c3bc018a0979176a9679ca5249.jpg)  
Trombone Bb

![](images/c516a5ddf37841eef6c5cd7011b035b1efea45afcc1331a38e4ce065330ff76e.jpg)  
Euphonium Bb

```matlab
> win.graph(width=4.875,height=4,pointsize=8)  
> data(tbone); data(euph); oldpar=par; par(mfrow=(c(2,1)))  
> trombone=(tbone-mean(tbone))/sd(tbone)  
> euphonium=(euph-mean(euph))/sd(euph)  
> plotwindow(trombone,end=400),main='Trombone Bb',ylab='Waveform',yaxp=c(-1,+1,2))  
> plotwindow(euphonium,end=400),main='Euphonium Bb',ylab='Waveform',yaxp=c(-1,+1,2)); par=oldpar 
```

Exhibit 14.26 displays the estimated spectra for the two waveforms. The solid curve is for the euphonium, and the dotted curve is for the trombone. We used the convolution of two modified Daniell spectral windows, each with span $= 1 1$ , on both series. Since both series are essentially the same length, the bandwidths will both be about 0.0009 and barely perceptible on the bandwidth/confidence interval crosshair shown on the graph.

The first four major peaks occur at the same frequencies, but clearly the trombone has much more spectral power at distinct higher harmonic frequencies. It is suggested

that this may account for the more brassy nature of the trombone sound as opposed to the more mellow sound of the euphonium.

![](images/af785b69b7e028c3505b1ddc954ca10be39fb8a96b5c4c9df39462b2676f667c.jpg)  
Exhibit 14.26 Spectra for Trombone (dotted) and Euphonium (solid)

```javascript
>win.graph(width=4.875,height=2.5,pointsize=8) 
```

```python
>spec(euph, spans=c(11, 11), ylab='Log Spectra', xlab='Frequency', sub='') 
```

```txt
> s=spec(tbone, spans=c(11, 11), plot=F) 
```

```txt
> lines(s$freq, $$spec, lty='dotted') 
```

# 14.9 Other Methods of Spectral Estimation

Prior to widespread use of the fast Fourier transform, computing and smoothing the sample spectrum was extremely intensive computationally —especially for long time series. Lag window estimators were used to partially mitigate the computational difficulties.

# Lag Window Estimators

Consider the sample spectrum and smoothed sample spectrum. We have

$$
\begin{array}{l} \bar {S} (f) = \sum_ {k = - m} ^ {m} W (k) \hat {S} \Big (f + \frac {k}{n} \Big) \\ = \sum_ {k = - m} ^ {m} W (k) \left[ \sum_ {j = - n + 1} ^ {n - 1} \hat {\gamma} _ {j} e ^ {- 2 \pi i \left(f + \frac {k}{n}\right) j} \right] \tag {14.9.1} \\ = \sum_ {j = - n + 1} ^ {n - 1} \hat {\gamma} _ {j} \left[ \sum_ {k = - m} ^ {m} W (k) e ^ {- 2 \pi i \frac {k}{n ^ {\prime}} j} \right] e ^ {- 2 \pi i f j} \\ \end{array}
$$

or

$$
\bar {S} (f) = \sum_ {j = - n + 1} ^ {n - 1} \hat {\gamma} _ {j} w \binom {j} {n} e ^ {- 2 \pi i f j} \tag {14.9.2}
$$

where

$$
w \left(\frac {j}{n}\right) = \sum_ {k = - m} ^ {m} W (k) e ^ {- 2 \pi i k \left(\frac {j}{n}\right)} \tag {14.9.3}
$$

Equation (14.9.2) suggests defining and investigating a class of spectral estimators defined as

$$
\widetilde {S} (f) = \sum_ {j = - n + 1} ^ {n - 1} w \binom {j} {n} \hat {\gamma} _ {j} \cos (2 \pi f j) \tag {14.9.4}
$$

where the function $w ( x )$ has the properties

$$
w (x) = w (- x)
$$

$$
w (0) = 1 \tag {14.9.5}
$$

$$
w (x) \leq 1 \quad \text {f o r} | x | \leq 1
$$

The function $w ( x )$ is called a lag window and determines how much weight is given to the sample autocovariance at each lag.

The rectangular lag window is defined by

$$
w (x) = 1 \quad \text {f o r} | x | \leq 1 \tag {14.9.6}
$$

and the corresponding lag window spectral estimator is simply the sample spectrum. This estimator clearly gives too much weight to large lags where the sample autocovariances are based on too few data points and are unreliable.

The next simplest lag window is the truncated rectangular lag window, which simply omits large lags from the computation. It is defined as

$$
w \left(\frac {j}{n}\right) = 1 \quad \text {f o r} | j | \leq m \tag {14.9.7}
$$

where the computational advantage is achieved by choosing m much smaller than n.

The triangular, or Bartlett, lag window downweights higher lags linearly and is defined as

$$
w \left(\frac {j}{n}\right) = 1 - \left| \frac {j}{m} \right| \quad \text {f o r} | j | \leq m \tag {14.9.8}
$$

Other common lag windows are associated with the names of Parzen, Tukey-Hamming, and Tukey-Hanning. We will not pursue these further here, but much more information on the lag window approach to spectral estimation may be found in the books of Bloomfield (2000), Brillinger (2001), Brockwell and Davis (1991), and Priestley (1981).

# Other Smoothing Methods

Other methods for smoothing the sample spectrum have been proposed. Kooperberg et al. (1995) proposed using splines to estimate the spectral distribution. Fan and Kreutzberger (1998) investigated local smoothing polynomials and Whittle's likelihood for spectral estimation. This approach uses automatic bandwidth selection to smooth the sample spectrum. See also Yoshihide (2006), Jiang and Hui (2004), and Fay et al. (2002).

# 14.10 Summary

Given the undesirable characteristics of the sample spectral density, we introduced the smoothed sample spectral density and showed that it could be constructed to improve the properties. The important topics of bias, variance, leakage, bandwidth, and tapering were investigated. A procedure for forming confidence intervals was discussed, and all of the ideas were illustrated with both real and simulated time series data.

# EXERCISES

14.1 Consider the variance of $\bar { \cal S } ( f )$ with the Daniell spectral window. Instead of using Equation (14.2.4) on page 355, use the fact that $2 \hat { S } ( f ) / S ( f )$ has approximately a chi-square distribution with two degrees of freedom to show that the smoothed sample spectral density has an approximate variance of $S ^ { 2 } ( f ) / ( 2 m + 1 )$ .   
14.2 Consider various convolutions of the simple Daniell rectangular spectral window.

(a) Construct a panel of three plots similar to those shown in Exhibit 14.3 on page 354 but with the Daniell spectral window and with $m = 5$ . The middle graph should be the convolution of two Daniell windows and the leftmost graph the convolution of three Daniell windows.   
(b) Evaluate the bandwidths and degrees of freedom for each of the spectral windows constructed in part (a). Use $n = 1 0 0$ .   
(c) Construct another panel of three plots similar to those shown in Exhibit 14.3 but with the modified Daniell spectral window. This time use $m = 5$ for the first graph and convolve two with $m = 5$ and $m = 7$ for the second. Convolve three windows with m’s of 5, 7, and 11 for the third graph.   
(d) Evaluate the bandwidths and degrees of freedom for each of the spectral windows constructed in part (c). Use $n = 1 0 0$ .

14.3 For the Daniell rectangular spectral window show that

(a) ${ \frac { 1 } { n ^ { 2 } } } \sum _ { k = - m } ^ { m } k ^ { 2 } W _ { m } ( k ) = { \frac { 2 } { n ^ { 2 } ( 2 m + 1 ) } } \bigg ( { \frac { m ^ { 3 } } { 3 } } + { \frac { m ^ { 2 } } { 2 } } + { \frac { m } { 6 } } \bigg )$ k 2 Wm ( ) k = m3⎛

(b) Show that if $m$ is chosen as $m = c { \sqrt { n } }$ for any constant $c$ , then the right-hand side of the expression in part (a) tends to zero as $n$ goes to infinity.   
(c) Show that if ${ \bar { m } } = c { \sqrt { n } }$ for any constant $c$ , then the approximate variance of the smoothed spectral density given by the right-hand side of Equation (14.2.4) on page 355 tends to zero as $n$ tends to infinity._

14.4 Suppose that the distribchi-square variable wit $\bar { S } ( f )$ is to be approximfreedom ν, so that $\bar { S } ( f ) \approx c \chi _ { \mathrm { 1 ) } } ^ { 2 }$ ultiple of a. Using the approximate variance of _ $\bar { S } ( f )$ given in Equation (14.2.4) on page 355 and the fact that $\bar { S } ( f )$ is approximately unbiased, equate means and variances and find the values for $c$ and ν (thus establishing Equation (14.4.2) on page 356).

14.5 Construct a time series of length $n = 4 8$ according to the expression

$$
Y _ {t} = \sin [ 2 \pi (0. 2 8) t ]
$$

Display the periodogram of the series and explain its appearance.

14.6 Estimate the spectrum of the Los Angeles annual rainfall time series. The data are in the file named larain. Because of the skewness in the series, use the logarithms of the raw rainfall values. The square root of the series length suggests a value for the span of about 11. Use the modified Daniell spectral window, and be sure to set the vertical limits of the plot so that you can see the whole confidence interval guide. Comment on the estimated spectrum.

14.7 The file named spots1 contains annual sunspot numbers for 306 years from 1700 through 2005.

(a) Display the time series plot of these data. Does stationarity seem reasonable for this series?   
(b) Estimate the spectrum using a modified Daniell spectral window convoluted with itself and a span of 3 for both. Interpret the plot.   
(c) Estimate the spectrum using an AR model with the order chosen to minimize the AIC. Interpret the plot. What order was selected?   
(d) Overlay the estimates obtained in parts (b) and (c) above onto one plot. Do they agree to a reasonable degree?

14.8 Consider the time series of average monthly temperatures in Dubuque, Iowa. The data are in the file named tempdub and cover from January 1964 to December 1975 for an n of 144.

(a) Estimate the spectrum using a variety of span values for the modified Daniell spectral window.   
(b) In your opinion, which of the estimates in part (a) best represents the spectrum of the process? Be sure to use bandwidth considerations and confidence limits to back up your argument.

14.9 An EEG (electroencephalogram) time series is given in the data file named eeg. An electroencephalogram is a noninvasive test used to detect and record the electrical activity generated in the brain. These data were measured at a sampling rate of 256 per second and came from a patient suffering a seizure. The total record length is $n = 1 3 \mathrm { , 0 0 0 }$ —or slightly less than one minute.

(a) Display the time series plot and decide if stationarity seems reasonable.   
(b) Estimate the spectrum using a modified Daniell spectral window convolved with itself and a span of 51 for both components of the convolution. Interpret the plot.   
(c) Estimate the spectrum using an AR model with the order chosen to minimize the AIC. Interpret the plot. What order was selected?   
(d) Overlay the estimates obtained in parts (b) and (c) above onto one plot. Do they agree to a reasonable degree?

14.10 The file named electricity contains monthly U. S. electricity production values from January 1994 to December 2005. A time series plot of the logarithms of these values is shown in Exhibit 11.14 on page 264. Since there is an upward trend and increasing variability at higher levels in these data, use the first difference of the logarithms for the remaining analysis.

(a) Construct a time series plot of the first difference of the logarithms of the electricity values. Does a stationary model seem warranted at this point?   
(b) Display the smoothed spectrum of the first difference of the logarithms using a modified Daniell spectral window and span values of 25, 13, and 7. Interpret the results.   
(c) Now use a spectral window that is a convolution of two modified Daniell windows each with span $= 3$ . Also use a $10 \%$ taper. Interpret the results.   
(d) Estimate the spectrum using an AR model with the order chosen to minimize the AIC. Interpret the plot. What order was selected?   
(e) Overlay the estimates obtained in parts (c) and (d) above onto one plot. Do they agree to a reasonable degree?

14.11 Consider the monthly milk production time series used in Exhibit 14.24 on page 374. The data are in the file named milk.

(a) Estimate the spectrum using a spectral window that is a convolution of two modified Daniell windows each with span $= 7$ . Compare these results with those shown in Exhibit 14.24.   
(b) Estimate the spectrum using a single modified Daniell spectral window with $s p a n = 7$ . Compare these results with those shown in Exhibit 14.24 and those in part (a).   
(c) Finally, estimate the spectrum using a single modified Daniell spectral window with span $= 1 1$ . Compare these results with those shown in Exhibit 14.24 and those in parts (a) and (b).   
(d) Among the four different estimates considered here, which do you prefer and why?

14.12 Consider the river flow series displayed in Exhibit 14.22 on page 372. An estimate of the spectrum is shown in Exhibit 14.23 on page 373. The data are in the file named flow.

(a) Here $n = 5 7 6$ and ${ \sqrt { n } } = 2 4$ . Estimate the spectrum using $s p a n = 2 5$ with the modified Daniell spectral window. Compare your results with those shown in Exhibit 14.23.   
(b) Estimate the spectrum using $s p a n = 1 3$ with the modified Daniell spectral window and compare your results to those obtained in part (a) and in Exhibit 14.23.

14.13 The time series in the file named tuba contains about 0.4 seconds of digitized sound from a tuba playing a B flat one octave and one note below middle C.

(a) Display a time series plot of the first 400 of these data and compare your results with those shown in Exhibit 14.25 on page 375, for the trombone and euphonium.   
(b) Estimate the spectrum of the tuba time series using a convolution of two modified Daniell spectral windows, each with span = 11.   
(c) Compare the estimated spectrum obtained in part (b) with those of the trombone and euphonium shown in Exhibit 14.26 on page 376. (You may want to overlay several of these spectra.) Remember that the tuba is playing one octave lower than the two other instruments.   
(d) Do the higher-frequency components of the spectrum for the tuba look more like those of the trombone or those of the euphonium? (Hint: The euphonium is sometimes called a tenor tuba!)

# Appendix K: Tapering and the Dirichlet Kernel

Suppose $Y _ { t } = \cos ( 2 \pi f _ { 0 } t + \Phi )$ for $t = 1$ , 2,…, n, where $f _ { 0 }$ is not necessarily a Fourier frequency. Since it will not affect the periodogram, we will actually suppose that

$$
Y _ {t} = e ^ {2 \pi i f _ {0} t} \tag {14.K.1}
$$

in order to simplify the mathematics. Then the discrete-time Fourier transform of this sequence is given by

$$
\frac {1}{n} \sum_ {t = 1} ^ {n} Y _ {t} e ^ {- 2 \pi i f t} = \frac {1}{n} \sum_ {t = 1} ^ {n} e ^ {2 \pi i \left(f _ {0} - f\right) t} \tag {14.K.2}
$$

By Equations (13.J.7) and (13.J.8) on page 350, for any z,

$$
\begin{array}{l} \frac {1}{n} \sum_ {t = 1} ^ {n} e ^ {2 \pi i z t} = \frac {1}{n} e ^ {2 \pi i z} \frac {\left(e ^ {2 \pi i n z} - 1\right)}{\left(e ^ {2 \pi i z} - 1\right)} \\ = \frac {1}{n} e ^ {\pi i (n + 1) z} \frac {\left(e ^ {\pi i n z} - e ^ {- \pi i n z}\right)}{\left(e ^ {\pi i z} - e ^ {- \pi i z}\right)} \\ \end{array}
$$

so that

$$
\frac {1}{n} \sum_ {t = 1} ^ {n} e ^ {2 \pi i z t} = e ^ {\pi i (n + 1) z} \left[ \frac {1}{n} \frac {\sin (\pi n z)}{\sin (\pi z)} \right] \tag {14.K.3}
$$

The function

$$
D (z) = \frac {1}{n} \frac {\sin (\pi n z)}{\sin (\pi z)} \tag {14.K.4}
$$

is the Dirichlet kernel shown on the left-hand side of Exhibit 14.7 on page 360 for $n =$ 100. These results lead to the following relationship for the periodogram of $Y _ { t }$ :

$$
I (f) \propto \left| D \left(f - f _ {0}\right) \right| ^ {2} \tag {14.K.5}
$$

Remember that for all Fourier frequencies $D ( f ) = 0$ , so that this window has no effect at those frequencies. Leakage occurs when there is substantial power at non-Fourier frequencies. Now consider tapering $Y _ { t }$ with a cosine bell. We have

$$
\begin{array}{l} \tilde {Y} _ {t} = \frac {1}{2} \left\{1 - \cos \left[ \frac {2 \pi (t - 0 . 5)}{n} \right] \right\} Y _ {t} \tag {14.K.6} \\ = \frac {1}{2} e ^ {2 \pi i f _ {0} t} - \frac {1}{4} e ^ {2 \pi i f _ {0} t + 2 \pi i (t - \frac {1}{2}) / n} - \frac {1}{4} e ^ {2 \pi i f _ {0} t - 2 \pi i (t - \frac {1}{2}) / n} \\ \end{array}
$$

and after some more algebra we obtain

$$
\begin{array}{l} \frac {1}{n} \sum_ {t = 1} ^ {n} \tilde {Y} _ {t} e ^ {- 2 \pi i f t} \tag {14.K.7} \\ = e ^ {\pi i (n + 1) (f _ {0} - f) t} \Big [ \frac {1}{4} D \Big (f - f _ {0} - \frac {1}{n} \Big) + \frac {1}{2} D (f - f _ {0}) + \frac {1}{4} D \Big (f - f _ {0} + \frac {1}{n} \Big) \Big ] \\ \end{array}
$$

The function

$$
\tilde {D} (f) = \frac {1}{4} D \left(f - f _ {0} - \frac {1}{n}\right) + \frac {1}{2} D \left(f - f _ {0}\right) + \frac {1}{4} D \left(f - f _ {0} + \frac {1}{n}\right) \tag {14.K.8}
$$

is the tapered or modified Dirichlet kernel that is plotted on the right-hand side of Exhibit 14.7 on page 360 for $n = 1 0 0$ . The periodogram of the tapered series is proportional to $| ( \tilde { D } ( f ) ) | ^ { 2 }$ , and the side lobe problem is substantially mitigated.

# CHAPTER 15

# THRESHOLD MODELS

It can be shown (Wold, 1948) that any weakly stationary process $\{ Y _ { t } \}$ admits the Wold decomposition

$$
Y _ {t} = U _ {t} + e _ {t} + \psi_ {1} e _ {t - 1} + \psi_ {2} e _ {t - 2} + \dots
$$

where $e _ { t }$ equals the deviation of $Y _ { t }$ from the best linear predictor based on all past Y values, and $\{ U _ { t } \}$ is a purely deterministic stationary process, with $e _ { t }$ being uncorrelated with $U _ { s }$ , for any $t$ and $s$ . A purely deterministic process is a process that can be predicted to arbitrary accuracy; (that is, with arbitrarily small mean squared error) by some linear predictors of finitely many past lags of the process. A simple example of a purely deterministic process is $U _ { t } \equiv \mu$ , a constant. A more subtle example is the random cosine wave model introduced on page 18. In essence, $\{ U _ { t } \}$ represents the stochastic, stationary “trend” in the data. The prediction errors $\{ \boldsymbol { e } _ { t } \}$ are a white noise sequence, and $e _ { t }$ represents the “new” component making up $Y _ { t }$ and hence is often called the innovation of the process. The Wold decomposition then states that any weakly stationary process is the sum of a (possibly infinite-order) MA process and a deterministic trend. Thus, we can compute the best linear predictor within the framework of $\mathbf { M A } ( \infty )$ processes that can further be approximated by finite-order ARMA processes. The Wold decomposition thus guarantees the versatility of the ARMA models in prediction with stationary processes.

However, except for convenience, there is no reason for restricting to linear predictors. If we allow nonlinear predictors and seek the best predictor of $Y _ { t }$ based on past values of Y that minimizes the mean squared prediction error, then the best predictor need no longer be the best linear predictor. The solution is simply the conditional mean of $Y _ { t }$ given all past Y values. The Wold decomposition makes it clear that the best one-stepahead linear predictor is the best one-step-ahead predictor if and only if $\{ \boldsymbol { e } _ { t } \}$ in the Wold decomposition satisfies the condition that the conditional mean of $e _ { t }$ given past $e$ ’s is identically equal to 0. The $\{ \boldsymbol { e } _ { t } \}$ satisfying the latter condition is called a sequence of martingale differences, so the condition will be referred to as the martingale difference condition. The martingale difference condition holds if, for example, $\{ \boldsymbol { e } _ { t } \}$ is a sequence of independent, identically distributed random variables with zero mean. But it also holds if $\{ \boldsymbol { e } _ { t } \}$ is some GARCH process. Nonetheless, when the martingale difference condition fails, nonlinear prediction will lead to a more accurate prediction. Hannan (1973) defines a linear process to be one where the best one-step-ahead linear predictor is the best one-step-ahead predictor.

The time series models discussed so far are essentially linear models in the sense that, after suitable instantaneous transformation, the one-step-ahead conditional mean is a linear function of the current and past values of the time series variable. If the errors are normally distributed, as is commonly assumed, a linear ARIMA model results in a normally distributed process. Linear time series methods have proved to be very useful in practice. However, linear, normal processes do suffer from some limitations. For example, a stationary normal process is completely characterized by its mean and autocovariance function; hence the process reversed in time has the same distribution as the original process. The latter property is known as time reversibility. Yet, many real processes appear to be time-irreversible. For example, the historical daily closing price of a stock generally rose gradually but, if it crashed, it did so precipitously, signifying a time-irreversible data mechanism. Moreover, the one-step-ahead conditional mean may be nonlinear rather than linear in the current and past values. For example, animal abundance processes may be nonlinear due to finite-resource constraints. Specifically, while moderately high abundance in one period is likely to be followed by higher abundance in the next period, extremely high abundance may lead to a population crash in the ensuing periods. Nonlinear time series models generally display rich dynamical structure. Indeed, May (1976) showed that a very simple nonlinear deterministic difference equation may admit chaotic solutions in the sense that its time series solutions are sensitive to the initial values, which may appear to be indistinguishable from a white noise sequence based on correlation analysis. Nonlinear time series analysis thus may provide more accurate predictions, which can be very substantial in certain parts of the state space, and shed novel insights on the underlying dynamics of the data. Nonlinear time series analysis was earnestly initiated around the late 1970s, prompted by the need for modeling the nonlinear dynamics shown by real data; see Tong (2007). Except for cases with well-developed theory accounting for the underlying mechanism of an observed time series, the nonlinear data mechanism is generally unknown. Thus, a fundamental problem of empirical nonlinear time series analysis concerns the choice of a general nonlinear class of models. Here, our goal is rather modest in that we introduce the threshold model, which is one of the most important classes of nonlinear time series models. For a systematic account of nonlinear time series analysis and chaos, see Tong (1990) and Chan and Tong (2001).

# 15.1 Graphically Exploring Nonlinearity

In ARIMA modeling, the innovation (error) process is often specified as independent and identically normally distributed. The normal error assumption implies that the stationary time series is also a normal process; that is, any finite set of time series observations are jointly normal. For example, the pair $( Y _ { 1 } , Y _ { 2 } )$ has a bivariate normal distribution and so does any pair of $Y \mathrm { s }$ ; the triple $( Y _ { 1 } , Y _ { 2 } , Y _ { 3 } )$ has a trivariate normal distribution and so does any triple of $Y \mathrm { s }$ , and so forth. When data are nonnormal, instantaneous transformation of the form $h ( Y _ { t } )$ , for example, $h ( Y _ { t } ) ~ = ~ \sqrt { Y } _ { t }$ , may be applied to the data in the hope that a normal ARIMA model can serve as a good approximation to the underlying data-generating mechanism. The normality assumption is mainly

adopted for convenience in statistical inference. In practice, an ARIMA model with nonnormal innovations may be entertained. Indeed, such processes have very rich and sometimes exotic dynamics; see Tong (1990). If the normal error assumption is maintained, then a nonlinear time series is generally not normally distributed. Nonlinearity may then be explored by checking whether or not a finite set of time series observations are jointly normal; for example, whether or not the two-dimensional distribution of pairs of Y’s is normal. This can be checked by plotting the scatter diagram of $Y _ { t }$ against $Y _ { t - 1 }$ or $Y _ { t - 2 }$ , and so forth. For a bivariate normal distribution, the scatter diagram should resemble an elliptical data cloud with decreasing density from its center. Departure from such a pattern (for example, existence of a large hole in the data cloud) may signify that the data are nonnormal and the underlying process may be nonlinear.

Exhibit 15.1 shows the scatter diagrams of $Y _ { t }$ versus its lag 1 to lag 6, where we simulated data from the ARIMA(2,1) model

$$
Y _ {t} = 1. 6 Y _ {t - 1} - 0. 9 4 Y _ {t - 2} + e _ {t} - 0. 6 4 e _ {t - 1} \tag {15.1.1}
$$

with the innovations being standard normal. Note that the data clouds in the scatter diagrams are roughly elliptically shaped.

To help us visualize the relationship between the response and its lags, we draw fitted nonparametric regression lines on each scatter diagram. For example, on the scatter diagram of $Y _ { t }$ against $Y _ { t - 1 }$ , a nonparametric estimate of the conditional mean function of $Y _ { t }$ given $Y _ { t - 1 }$ , also referred to as the lag 1 regression function, is superimposed. (Specifically, the lag 1 regression function equals $m _ { 1 } ( y ) = E ( Y _ { t } | Y _ { t - 1 } { = } y )$ as a function of y.) If the underlying process is linear and normal, the true lag 1 regression function must be linear and so we expect the nonparametric estimate of it to be close to a straight line. On the other hand, a curved lag 1 regression estimate may suggest that the underlying process is nonlinear. Similarly, one can explore the lag 2 regression function (that is, the conditional mean of $Y _ { t }$ given $Y _ { t - 2 } = y ,$ ) as a function of $y$ and higher-lag analogues. In the case of strong departure from linearity, the shape of these regression functions may provide some clue as to what nonlinear model may be appropriate for the data. Note that all lagged regression curves in Exhibit 15.1 are fairly straight, suggesting that the underlying process is linear, which indeed we know is the case.

![](images/473e3e64eb391232a416e78bf5594e38f5703567181ecc598a2f79315a400367.jpg)  
Exhibit 15.1 Lagged Regression Plots for a Simulated ARMA(2,1) Process. Solid lines are fitted regression curves.   
lag 1 regression plot

![](images/3eb5725083d737d2417850e9708562acb8a170c635d4a955c886fad604e41d67.jpg)  
lag 2 regression plot

![](images/61fae89e7e2714a58959c0146bfcf9516c291df2f0ec67e0533704baf441286c.jpg)  
lag 3 regression plot

![](images/fe4b0556e0d18676d0321e837fd32700bab7543a511a1f1725532ff4d7153ba9.jpg)  
lag 4 regression plot

![](images/c7370569978afa066dcfda03c0375dc164224849a68642ac7319a866502ca958.jpg)  
lag 5 regression plot

![](images/c3e3e22a24a963bb669e841c359d5ebab18a130e396ef67fd510c1fb0ae3e3ea.jpg)  
lag 6 regression plot

> win.graph(width $\mathord { \left. \begin{array} { r l r } \end{array} \right. } = 4$ .875, height=6.5,pointsize=8)   
> set.seed(2534567); par(mfrow ${ \bf \Pi } ^ { \prime } = { \bf C }$ (3,2))   
> y=arima.sim( $\mathtt { \Pi } _ { \mathtt { n } = 6 1 }$ ,model=list( $\mathtt { a r } = \mathtt { C }$ (1.6,-0.94),ma=-0.64))   
> lagplot(y)

We now illustrate the technique of a lagged regression plot with a real example. Exhibit 15.2 plots an experimental time series response as the number of individuals (Didinium natsutum, a protozoan) per ml measured every twelve hours over a period of 35 days; see Veilleux (1976) and Jost and Ellner (2000). The experiment studied the

population fluctuation of a prey-predator system; the prey is Paramecium aurelia, a unicellular ciliate protozon, whereas the predator species is Didinium natsutum. The initial part of the data appears to be nonstationary owing to transient effects. It can be seen that the increasing phase of the series is generally longer than that of the decreasing phase, suggesting that the time series is time-irreversible. Below, we shall omit the first 14 data points from the analysis; that is, only the (log-transformed) data corresponding to the solid curve in Exhibit 15.2 are used in subsequent analysis.

![](images/596dae8f969b9c92bb266aa9b144bc14a87e9fcf11c576c4cdde63a8f98173a2.jpg)  
Exhibit 15.2 Logarithmically Transformed Number of Predators. The stationary part of the time series is displayed as a solid line. Solid circles indicate data in the lower regime of a fitted threshold autoregressive model.

```txt
> data(veilleux); predator = veilleux[,1]  
> win.graph(width=4.875,height=2.5,pointsize=8)  
> plot(log(predator), lty=2,type='o', xlab='Day', ylab='Log(predator)')  
> predator EQ=window(predator,start=c(7,1))  
> lines(log(predator EQ))  
> index1=zlag(log(predator EQ), 3) <= 4.661  
> points(y=log(predator EQ)[index1], (time(predator EQ))[index1], pch=19) 
```

Exhibit 15.3 shows the lagged regression plots of the predator series. Notice that several scatter diagrams have a large hole in the center, hinting that the data need to be nonnormal. Also, the regression function estimates appear to be strongly nonlinear for lags 2 to 4, suggesting a nonlinear data mechanism; in fact, the histogram (not shown) suggests that the series is bimodal.

![](images/40fcb93cb283484871c693fec25156315d165088adda0c9dbf215e1893bf307c.jpg)  
Exhibit 15.3 Lagged Regression Plots for the Predator Series   
lag 1 regression plot

![](images/bc9f6cbf8877b94aef54efde49e06623a3be2af62cc544d5e56fb4becf06ea11.jpg)  
lag 2 regression plot

![](images/778bb20c6107b0efd27fe9794b8349e8e5dd6bce91d21113bd9179fea8bfeebd.jpg)  
lag 3 regression plot

![](images/f4286e005825908a4a98493dea0906c686a436c210ec7c6c8b9c2b0e0f6c1c6d.jpg)  
lag 4 regression plot

![](images/900a8ffd25502a2e1880436ced0d18e403f41b321969f0e0c5c42c1e0d982033.jpg)  
lag 5 regression plot

![](images/03c643f6cee0110738e3fb63ef68dd2cfced310d9d382dee7d2e1b520d410688.jpg)  
lag 6 regression plot

> win.graph(width=4.875,height=6.5,pointsize=8)   
> data(predator.eq)   
> lagplot(log(predator.eq)) # libraries mgcv and locfit required

We now elaborate on how the regression curves are estimated nonparametrically. Readers not interested in the technical details may skip to the next section. For concreteness, suppose we want to estimate the lag 1 regression function. (The extension to other lags is straightforward.) Nonparametric estimation of the lag 1 regression function gen-

erally makes use of the idea of estimating the conditional mean $m _ { 1 } ( y ) = E ( Y _ { t } | Y _ { t - 1 } = y )$ by averaging those Y’s whose lag 1 values are close to y. Clearly, the averaging may be rendered more accurate by giving more weight to those Y’s whose lag 1 value is closer to y. The weights are usually assigned systematically via some probability density function $k ( y )$ and a bandwidth parameter $h > 0$ . The data pair $( Y _ { t } , Y _ { t - 1 } )$ is assigned the weight

$$
w _ {t} = \frac {1}{h} k \left(\frac {Y _ {t - 1} - y}{h}\right) \tag {15.1.2}
$$

Hereafter we assume that $k ( \cdot )$ is the standard normal probability density function. Note that then the right-hand side of Equation (15.1.2) is the normal probability density function with mean $y$ and variance $h ^ { 2 }$ . Finally, we define the Nadaraya-Watson estimator†

$$
\hat {m} _ {1} ^ {(0)} (y) = \frac {\sum_ {t = 2} ^ {n} w _ {t} Y _ {t}}{\sum_ {t = 2} ^ {n} w _ {t}} \tag {15.1.3}
$$

(The meaning of the superscript 0 will become clear later on.) Since the normal probability density function is negligible for values that differ from the mean by more than three standard deviations, the Nadaraya-Watson estimator essentially averages the $Y _ { t }$ whose $Y _ { t - 1 }$ is within $3 h$ units from y, and the averaging is weighted with more weight to those observations whose lag 1 values are closer to y. The use of the Nadaraya-Watson estimator of the lag 1 regression function requires us to specify the bandwidth. There are several methods, including cross-validation for determining h. However, for an exploratory analysis, we can always use some default bandwidth value and vary it a bit to get some feel of the shape of the lag 1 regression function.

A more efficient nonparametric estimator may be obtained by assuming that the underlying regression function can be well-approximated locally by a linear function; see Fan and Gijbels (1996). The local linear estimator of the lag 1 regression function at y equals $\hat { m } _ { 1 } ^ { ( 1 ) } \check { ( y ) } = \check { b } _ { 0 }$ , which is obtained by minimizing the local weighted residual sum of squares:

$$
\sum_ {t = 2} ^ {n} w _ {t} \left(Y _ {t} - b _ {0} - b _ {1} Y _ {t - 1}\right) ^ {2} \tag {15.1.4}
$$

The reader may now guess that the superscript $k$ in the notation $\hat { m } _ { 1 } ^ { ( k ) } ( y )$ ^( ) k refers to the degree of the local polynomial. Often, data are unevenly spaced, in which case a single bandwidth may not work well. Instead, a variable bandwidth tied to the density of the data may be more efficient. A simple scheme is the nearest-neighbor scheme that varies the window width so that it covers a fixed fraction of data nearest to the center of the window. We set the fraction to be $70 \%$ for all our reported lagged regression plots.

It is important to remember that the local polynomial approach assumes that the true lag 1 regression function is a smooth function. If the true lag 1 regression function is discontinuous, then the local polynomial approach may yield misleading estimates. However, a sharp turn in the estimated regression function may serve as a warning that the smoothness condition may not hold for the true lag 1 regression function.

# 15.2 Tests for Nonlinearity

Several tests have been proposed for assessing the need for nonlinear modeling in time series analysis. Some of these tests, such as those studied by Keenan (1985), Tsay (1986), and Luukkonen et al. (1988), can be interpreted as Lagrange multiplier tests for specific nonlinear alternatives.

Keenan (1985) derived a test for nonlinearity analogous to Tukey’s one degree of freedom for nonadditivity test (see Tukey, 1949). Keenan’s test is motivated by approximating a nonlinear stationary time series by a second-order Volterra expansion (Wiener, 1958)

$$
Y _ {t} = \mu + \sum_ {\mu = - \infty} ^ {\infty} \theta_ {\mu} \varepsilon_ {t - \mu} + \sum_ {\nu = - \infty} ^ {\infty} \sum_ {\mu = - \infty} ^ {\infty} \theta_ {\mu \nu} \varepsilon_ {t - \mu} \varepsilon_ {t - \nu} \tag {15.2.1}
$$

where $\{ \varepsilon _ { t } , - \infty < t < \infty \}$ is a sequence of independent and identically distributed zero-mean random variables. The process $\{ Y _ { t } \}$ is linear if the double sum on the righthand side of (15.2.1) vanishes. Thus, we can test the linearity of the time series by testing whether or not the double sum vanishes. In practice, the infinite series expansion has to be truncated to a finite sum. Let $Y _ { 1 } , . . . , Y _ { n }$ denote the observations. Keenan’s test can be implemented as follows:

(i) Regress $Y _ { t }$ on $Y _ { t - 1 } , . . . , Y _ { t - m } .$ , including an intercept term, where $m$ is some prespecified positive integer; calculate the fitted values $\{ \hat { Y } _ { t } \}$ and the residuals $\{ \hat { e } _ { t } \}$ , for $t = m + 1 , . . . , n$ ; and set $\boldsymbol { R S S } = \ \Sigma \hat { e } _ { t } ^ { 2 }$ , the residual sum of squares.   
(ii) Regress $\hat { \boldsymbol Y } _ { t } ^ { 2 }$ on $Y _ { t - 1 } , . . . , Y _ { t - m }$ , including an intercept term, and calculate the residuals $\{ \hat { \boldsymbol { \xi } } _ { t } \}$ for $t = m + 1 , . . . , n$ .   
(iii) Regress $\hat { \boldsymbol e } _ { t }$ on the residuals $\hat { \xi } _ { t }$ without an intercept for $t = m + 1 , . . . , n$ , and Keenan’s test statistic, denoted by $\hat { F }$ , is obtained by multiplying $( n - 2 m - 2 ) /$ $( n - m - 1 )$ to the $F$ -statistic for testing that the last regression function is identically zero. Specifically, let

$$
\eta = \eta_ {0} \sqrt {\sum_ {t = m + 1} ^ {n} \hat {\xi} _ {t} ^ {2}} \tag {15.2.2}
$$

where $\boldsymbol { \mathsf { \Pi } } \boldsymbol { \mathsf { \Pi } } ^ { \mathsf { \Pi } }$ is the regression coefficient. Form the test statistic

$$
\hat {F} = \frac {\eta^ {2} (n - 2 m - 2)}{R S S - \eta^ {2}} \tag {15.2.3}
$$

Under the null hypothesis of linearity, the test statistic $\hat { F }$ is approximately distributed as an $F$ -distribution with degrees of freedom 1 and $n - 2 m - 2$ .

Keenan’s test can be derived heuristically as follows. Consider the following model.

$$
Y _ {t} = \theta_ {0} + \phi_ {1} Y _ {t - 1} + \dots + \phi_ {m} Y _ {t - m} + \exp \left\{\eta \left(\sum_ {j = 1} ^ {m} \phi_ {j} Y _ {t - j}\right) ^ {2} \right\} + \varepsilon_ {t} \tag {15.2.4}
$$

where $\{ \varepsilon _ { t } \}$ are independent and normally distributed with zero mean and finite variance. If $\boldsymbol \eta = 0$ , the exponential term becomes 1 and can be absorbed into the intercept term so that the preceding model becomes an $\operatorname { A R } ( m )$ model. On the other hand, for nonzero η, the preceding model is nonlinear. Using the expansion $\exp ( x ) \approx 1 + x .$ , which holds for $x$ of small magnitude, it can be seen that, for small η, $Y _ { t }$ follows approximately a quadratic AR model:

$$
Y _ {t} = \theta_ {0} + 1 + \phi_ {1} Y _ {t - 1} + \dots + \phi_ {m} Y _ {t - m} + \eta \left(\sum_ {j = 1} ^ {m} \phi_ {j} Y _ {t - j}\right) ^ {2} + \varepsilon_ {t} \tag {15.2.5}
$$

This is a restricted linear model in that the last covariate is the square of the linear term $\phi _ { 1 } Y _ { t - 1 } + \cdot \cdot \cdot + \phi _ { m } Y _ { t - m } .$ , which is replaced by the fitted values $\hat { Y } _ { t }$ under the null hypothesis. Keenan’s test is equivalent to testing $\boldsymbol \eta = 0$ in the multiple regression model (with the constant 1 being absorbed into $\theta _ { 0 }$ ):

$$
Y _ {t} = \theta_ {0} + \phi_ {1} Y _ {t - 1} + \dots + \phi_ {m} Y _ {t - m} + \eta \hat {Y} _ {t} ^ {2} + \varepsilon_ {t} \tag {15.2.6}
$$

which can be carried out in the manner described in the beginning of this section. Note that the fitted values are only available for $n \geq t \geq m + 1$ . Keenan’s test is the same as the $F$ -test for testing whether or not $\boldsymbol \eta = 0$ . A more formal approach is facilitated by the Lagrange multiplier test; see Tong (1990).

Keenan’s test is both conceptually and computationally simple and only has one degree of freedom, which makes the test very useful for small samples. However, Keenan’s test is powerful only for detecting nonlinearity in the form of the square of the approximating linear conditional mean function. Tsay (1986) extended Keenan’s approach by considering more general nonlinear alternatives. A more general alternative to nonlinearity may be formulated by replacing the term

$$
\exp \left\{\eta \left(\sum_ {j = 1} ^ {m} \phi_ {j} Y _ {t - j}\right) ^ {2} \right\} \tag {15.2.7}
$$

by

$$
\left. \begin{array}{l} \exp \left(\delta_ {1, 1} Y _ {t - 1} ^ {2} + \delta_ {1, 2} Y _ {t - 1} Y _ {t - 2} + \dots + \delta_ {1, m} Y _ {t - 1} Y _ {t - m} \right. \\ \left. + \delta_ {2, 2} Y _ {t - 2} ^ {2} + \delta_ {2, 3} Y _ {t - 2} Y _ {t - 3} + \dots + \delta_ {2, m} Y _ {t - 2} Y _ {t - m} + \dots \right. \\ \left. + \delta_ {m - 1, m - 1} Y _ {t - m + 1} ^ {2} + \delta_ {m - 1, m} Y _ {t - m + 1} Y _ {t - m} + \delta_ {m, m} Y _ {t - m} ^ {2}\right) + \varepsilon_ {t} \end{array} \right\} \tag {15.2.8}
$$

Using the approximation $\exp ( x ) \approx 1 + x$ , we see that the nonlinear model is approximately a quadratic AR model. But the coefficients of the quadratic terms are now unconstrained. Tsay’s test is equivalent to considering the following quadratic regression model:

$$
\left. \begin{array}{l} Y _ {t} = \theta_ {0} + \phi_ {1} Y _ {t - 1} + \dots + \phi_ {m} Y _ {t - m} \\ \quad + \delta_ {1, 1} Y _ {t - 1} ^ {2} + \delta_ {1, 2} Y _ {t - 1} Y _ {t - 2} + \dots + \delta_ {1, m} Y _ {t - 1} Y _ {t - m} \\ \quad + \delta_ {2, 2} Y _ {t - 2} ^ {2} + \delta_ {2, 3} Y _ {t - 2} Y _ {t - 3} + \dots + \delta_ {2, m} Y _ {t - 2} Y _ {t - m} + \dots \\ \quad + \delta_ {m - 1, m - 1} Y _ {t - m + 1} ^ {2} + \delta_ {m - 1, m} Y _ {t - m + 1} Y _ {t - m} + \delta_ {m, m} Y _ {t - m} ^ {2} + \varepsilon_ {t} \end{array} \right\} \tag {15.2.9}
$$

and testing whether or not all the $m ( m + 1 ) / 2$ coefficients $\delta _ { i , j }$ are zero. Again, this can be carried out by an $F$ -test that all $\delta _ { i , j }$ ’s are zero in the preceding equation. For a rigorous derivation of Tsay’s test as a Lagrange multiplier test, see Tong (1990).

We now illustrate these tests with two real datasets. In the first application, we use the annual American (relative) sunspot numbers collected from 1945 to 2007. The annual (relative) sunspot number is a weighted average of solar activities measured from a network of observatories. Historically, the daily sunspot number was computed as some weighted sum of the count of visible, distinct spots and that of clusters of spots on the solar surface. The sunspot number reflects the intensity of solar activity. Below, the sunspot data are square root transformed to make them more normally distributed; see Exhibit 15.4. The time series plot shows that the sunspot series tends to rise up more quickly than when it declines, suggesting that it is time-irreversible.

![](images/24be2fd6a3af65a5c029ea9866db1970d99f237f8b4b8d54d81b97b2f3d339ad.jpg)  
Exhibit 15.4 Annual American Relative Sunspot Numbers

```txt
> win.graph(width=4.875,height=2.5,pointsize=8)  
> data(spots)  
> plot(square(spots),type='o',xlabel='Year',ylab='Square Sunspot Number') 
```

To carry out the tests for nonlinearity, we have to specify m, the working autoregressive order. Under the null hypothesis that the process is linear, the order can be specified by using some information criterion, for example, the AIC. For the sunspot data, m $= 5$ based on the AIC. Both the Keenan test and the Tsay test reject linearity, with $p$ -values being 0.0002 and 0.0009, respectively.

For the second example, we consider the predator series discussed in the preceding section. The working AR order is found to be 4. Both the Keenan test and the Tsay test reject linearity, with $p$ -values being 0.00001 and 0.03, respectively, which is consistent with the inference drawn from the lagged regression plots reported earlier.

There are some other tests, such as the BDS test developed by Brock, Deckert and Seheinkman (1996), based on concepts that arise in the theory of chaos, and the neural-network test, proposed by White (1989) for testing “neglected nonlinearity.” For a recent review of tests for nonlinearity, see Tong (1990) and Granger and Teräsvirta (1993). We shall introduce one more test later.

# 15.3 Polynomial Models Are Generally Explosive

In nonlinear regression analysis, polynomial regression models of higher degrees are sometimes employed, even though they are deemed not useful for extrapolation because of their quick blowup to infinity. For this reason, polynomial regression models are of limited practical use. Based on the same reasoning, polynomial time series models may be expected to do poorly in prediction. Indeed, polynomial time series models of degree higher than 1 and with Gaussian errors are invariably explosive. To see this, consider the following simple quadratic AR(1) model.

$$
Y _ {t} = \phi Y _ {t - 1} ^ {2} + e _ {t} \tag {15.3.1}
$$

where $\{ \boldsymbol { e } _ { t } \}$ are independent and identically distributed standard normal random variables. Let $\phi > 0$ and let $c$ be a large number that is greater than $3 / \Phi$ . If $Y _ { 1 } > c$ (which may happen with positive probability due to the normality of the errors), then $Y _ { 2 } > 3 Y _ { 1 } + e _ { 2 }$ and hence $Y _ { 2 } > 2 c$ with some nonzero probability. With careful probability analysis, it can be shown that, with positive probability, the quadratic AR(1) process satisfies the inequality $Y _ { t } > 2 ^ { t } c$ for $t = 1$ , 2, 3,… and hence blows up to $+ \infty$ . Indeed, the quadratic AR(1) process, with normal errors, goes to infinity with probability 1.

As an example, Exhibit 15.5 displays a realization from a quadratic AR(1) model with $\phi = 0 . 5$ and standard normal errors that takes off to infinity at $t = 1 5$ .

Note that the quadratic AR(1) process becomes explosive only when the process takes some value of sufficiently large magnitude. If the coefficient $\phi$ is small, it may take much longer for the quadratic AR(1) process to take off to infinity. Normal errors can take arbitrarily large values, although rather rarely, but when this happens, the process becomes explosive. Thus, any noise distribution that is unbounded will guarantee the explosiveness of the quadratic AR(1) model. Chan and Tong (1994) further showed that this explosive behavior is true for any polynomial autoregressive process of degree higher than 1 and of any finite order when the noise distribution is unbounded.

![](images/c826bb5629574621201a3417103fb9df0121cdd1f16d505e940f31d24b9656e0.jpg)  
Exhibit 15.5 A Simulated Quadratic AR(1) Process with $\phi = 0 . 5$

```python
> set.seed(1234567)  
> plot(y=qar.sim(n=15, phi1=.5, sigma=1), x=1:15, type='o', ylab=expression(Y[t]), xlab='t') 
```

It is interesting to note that, for bounded errors, a polynomial autoregressive model may admit a stationary distribution that could be useful for modeling nonlinear time series data; see Chan and Tong (1994). For example, Exhibit 15.6 displays the time series solution of a deterministic logistic map, namely $Y _ { t } = 3 . 9 7 Y _ { t - 1 } ( 1 - Y _ { t - 1 } )$ , $t = 2$ , 3,… with the initial value $Y _ { 1 } = 0 . 3 7 7$ . Its corresponding sample ACF is shown in Exhibit 15.7, which, except for the mildly significant lag 4, resembles that of white noise. Note that, for a sufficiently large initial value, the solution of the logistic map will explode to infinity.

![](images/6ae3028aa9797fb42faa754d2e7e90432cd8c167db8209f137447d85efed392b.jpg)  
Exhibit 15.6 The Trajectory of the Logistic Map with Parameter 3.97 and Initial Value $Y _ { 1 } = 0 . 3 7 7$

```txt
> y=qar.sim(n=100, const=0.0, phi0=3.97, phi1=-3.97, sigma=0, init=.377) 
```

> plot(x=1:100, $\tt y = y$ ,type='l',ylab $^ { 1 = }$ expression(Y[t]),xlab='t')

![](images/af984afbd93ea8ba36ed5e04ebdf4576ee7eda1366de3e8d10fa3753b7f2c9ca.jpg)  
Exhibit 15.7 Sample ACF of the Logistic Time Series

> acf(y)

However, the bound on the noise distribution necessary for the existence of a stationary polynomial autoregressive model varies with the model parameters and the initial value, which greatly complicates the modeling task. Henceforth, we shall not pursue the use of polynomial models in time series analysis.

# 15.4 First-Order Threshold Autoregressive Models

The discussion in the preceding section provides an important insight that for a nonlinear time series model to be stationary, it must be either linear or approaching linearity in the “tail.” From this perspective, piecewise linear models, more widely known as threshold models, constitute the simplest class of nonlinear model. Indeed, the usefulness of threshold models in nonlinear time series analysis was well-documented by the seminal work of Tong (1978, 1983, 1990) and Tong and Lim (1980), resulting in an extensive literature of ongoing theoretical innovations and applications in various fields.

The specification of a threshold model requires specifying the number of linear submodels and the mechanism dictating which of them is operational. Consequently, there exist many variants of the threshold model. Here, we focus on the two-regime self-exciting threshold autoregressive (SETAR) model introduced by Tong, for which the switching between the two linear submodels depends solely on the position of the threshold variable. For the SETAR model (simply referred to as the TAR model below), the threshold variable is a certain lagged value of the process itself; hence the adjective self-exciting. (More generally, the threshold variable may be some vector covariate process or even some latent process, but this extension will not be pursued here.) To fix ideas, consider the following first-order TAR model:

$$
Y _ {t} = \left\{ \begin{array}{l l} \phi_ {1, 0} + \phi_ {1, 1} Y _ {t - 1} + \sigma_ {1} e _ {t}, & \text {i f} Y _ {t - 1} \leq r \\ \phi_ {2, 0} + \phi_ {2, 1} Y _ {t - 1} + \sigma_ {2} e _ {t}, & \text {i f} Y _ {t - 1} > r \end{array} \right. \tag {15.4.1}
$$

where the $\boldsymbol { \Phi }$ ’s are autoregressive parameters, σ’s are noise standard deviations, $r$ is the threshold parameter, and $\{ \boldsymbol { e } _ { t } \}$ is a sequence of independent and identically distributed random variables with zero mean and unit variance. Thus, if the lag 1 value of $Y _ { t }$ is not greater than the threshold, the conditional distribution of $Y _ { t }$ is the same as that of an AR(1) process with intercept $\phi _ { 1 , 0 }$ , autoregressive coefficient $\phi _ { 1 , 1 }$ , and error variance $\sigma _ { 1 } ^ { 2 }$ , in which case we may say that the first AR(1) submodel is operational. On the other hand, when the lag 1 value of $Y _ { t }$ exceeds the threshold $r$ , the second AR(1) process with parameters $( \boldsymbol { \Phi } _ { 2 , 0 } , \boldsymbol { \Phi } _ { 2 , 1 } , \sigma _ { 2 } ^ { 2 } )$ is operational. Thus, the process switches between two linear mechanisms dependent on the position of the lag 1 value of the process. When the lag 1 value does not exceed the threshold, we say that the process is in the lower (first) regime, and otherwise it is in the upper regime. Note that the error variance need not be identical for the two regimes, so that the TAR model can account for some conditional heteroscedasticity in the data.

As a concrete example, we simulate some data from the following first-order TAR model:

$$
Y _ {t} = \left\{ \begin{array}{l l} 0. 5 Y _ {t - 1} + e _ {t}, & \text {i f} Y _ {t - 1} \leq - 1 \\ - 1. 8 Y _ {t - 1} + 2 e _ {t}, & \text {i f} Y _ {t - 1} > - 1 \end{array} \right. \tag {15.4.2}
$$

Exhibit 15.8 shows the time series plot of the simulated data of size $n = 1 0 0$ . A notable feature of the plot is that the time series is somewhat cyclical, with asymmetrical cycles where the series tends to drop rather sharply but rises relatively slowly. This asymmetry means that the probabilistic structure of the process will be different if we reverse the direction of time. One way to see this is to make a transparency of the time series plot and flip the transparency over to see the time series plot with time reversed. In this case, the simulated data will rise sharply and drop slowly with time reversed. Recall that this phenomenon is known as time irreversibility. For a stationary Gaussian ARMA process, the probabilistic structure is determined by its first and second moments, which are invariant with respect to time reversal, hence the process must be time-reversible. Many real time series, for example the predator series and the relative sunspot series, appear to be time-irreversible, suggesting that the underlying process is nonlinear. Exhibit 15.9 shows the QQ normal score plot for the simulated data. It shows that the distribution of simulated data has a thicker tail than a normal distribution, despite the fact that the errors are normally distributed.

![](images/6770201912d36ad759322a530b4f595fd29c9ed496ed2722be84266d0d9db997.jpg)  
Exhibit 15.8 A Simulated First-Order TAR Process

```python
> set.seed(1234579)
> y=tar.sim(n=100, Phi1=c(0, 0.5), Phi2=c(0, -1.8), p=1, d=1, sigma1=1, thd=-1, sigma2=2) $y
> plot(y=y, x=1:100, type='o', xlab='t', ylab=expression(Y[t])) 
```

![](images/1eb1af382a8586f69505801134113d36ca90e7dede080125288f88b490d85d8f.jpg)  
Exhibit 15.9 QQ Normal Plot for the Simulated TAR Process

>win.graph(width=2.5,height=2.5,pointsize $\coloneqq$ 8)   
>qqnorm(y); qqline(y)

The autoregressive coefficient of the submodel in the upper regime equals $- 1 . 8$ , yet the simulated data appear to be stationary, which may be unexpected from a linear perspective, as an AR(1) model cannot be stationary if the autoregressive coefficient exceeds 1 in magnitude. This puzzle may be better understood by considering the case of no noise terms in either regime; that is, $\sigma _ { 1 } = \sigma _ { 2 } = 0$ . The deterministic process thus defined is referred to as the skeleton of the TAR model. We show below that, for any ini-

tial value, the skeleton is eventually a bounded process; the stability of the skeleton underlies the stationarity of the TAR model. Readers not interested in the detailed analysis verifying the ultimate boundedness of the skeleton may skip to the next paragraph. Let the initial value $y _ { 1 }$ be some large number, say 10, a value falling in the upper regime. So, the next value is $y _ { 2 } = ( - 1 . 8 ) { \times } 1 0 = - 1 8$ , which is in the lower regime. Therefore, the third value equals $y _ { 3 } = 0 . 5 { \times } ( - 1 8 ) = - 9$ . As the third value is in the lower regime, the fourth value equals $y _ { 4 } = 0 . 5 \times ( - 9 ) = - 4 . 5$ , which remains in the lower regime, so that the fifth value equals $y _ { 5 } = 0 . 5 \times ( - 4 . 5 ) = - 2 . 2 5$ . It is clear that once the data remain in the lower regime, they will be halved in the next iterate and this process continues until some future iterate crosses the threshold $^ { - 1 }$ , which occurs for $y _ { 7 } = - 0 . 5 6 2 5$ . Now the second linear submodel is operational, so that $y _ { 8 } = ( - 1 . 8 ) { \times } ( - 0 . 5 6 2 5 ) = 1 . 0 1 2 5$ and $y _ { 9 } =$ $( - 1 . 8 ) { \times } 1 . 0 1 2 5 = - 1 . 8 2 2 5$ , which is again in the lower regime. In conclusion, if some iterate is in the lower regime, the next iterate is obtained by halving the previous iterate until some future iterate exceeds $^ { - 1 }$ . On the other hand, if some iterate exceeds 1, the next iterate must be less than $^ { - 1 }$ and hence in the lower regime. By routine analysis, it can be checked that the process is eventually trapped between $^ { - 1 }$ and 1.8 and hence is a bounded process.

A bounded skeleton is stable in some sense. Chan and Tong (1985), showed that under some mild conditions, a TAR model is asymptotically stationary if its skeleton is stable. In fact, stability of the skeleton together with some regularity conditions imply the stronger property of ergodicity; namely, the process admits a stationary distribution and for any function $h ( Y _ { t } )$ having a finite stationary first moment (which holds if $h$ is a bounded function),

$$
\frac {1}{n} \sum_ {t = 1} ^ {n} h \left(Y _ {t}\right) \tag {15.4.3}
$$

converges to the stationary mean of $h ( Y _ { t } )$ , computed according to the stationary distribution. See Cline and Pu (2001) for a recent survey on the linkage between stability and ergodicity and counterexamples when this linkage may fail to hold.

The stability analysis of the skeleton can be much simplified by the fact that the ergodicity of a TAR model can be inferred from the stability of an associated skeleton defined by a difference equation obtained by modifying the equation defining the TAR model by suppressing the noise terms and the intercepts (that is, zero errors and zero intercepts) and setting the threshold to 0. For the simulated example, the associated skeleton is then defined by the following difference equation:

$$
Y _ {t} = \left\{ \begin{array}{l l} 0. 5 Y _ {t - 1}, & \text {i f} Y _ {t - 1} \leq 0 \\ - 1. 8 Y _ {t - 1}, & \text {i f} Y _ {t - 1} > 0 \end{array} \right. \tag {15.4.4}
$$

Now, the solution to the skeleton above can be readily obtained: Given a positive value for $y _ { 1 }$ , $y _ { t } = ( - 1 . 8 ) \times 0 . 5 ^ { t - 2 } \times y _ { 1 }$ , for all $t \geq 2$ . For negative $y _ { 1 }$ , $y _ { t } = 0 . 5 \AA ^ { t - 1 } \times y _ { 1 }$ . In both cases, $y _ { t } \to 0$ , as $t \to \infty$ . The origin is said to be an equilibrium point as $y _ { t } \equiv 0$ , for all $t$ , if $y _ { 1 } =$ 0. The origin is then said to be a globally exponentially stable limit point, as the skeleton approaches it exponentially fast for any nonzero initial value. It can be shown (Chan and

Tong, 1985) that the origin is a globally exponentially stable limit point for the skeleton if the parameters satisfy the constraints

$$
\phi_ {1, 1} <   1, \phi_ {2, 1} <   1, \phi_ {1, 1} \phi_ {2, 1} <   1 \tag {15.4.5}
$$

in which case the first-order TAR model is ergodic and hence stationary. Exhibit 15.10 shows the region of stationarity shaded in gray. Note that the region of stationarity is substantially larger than the region defined by the linear time series inspired constraints $| \Phi _ { 1 , 1 } | < 1$ , $| \phi _ { 2 , 1 } | < 1$ , corresponding to the region bounded by the inner square in Exhibit 15.10. For parameters lying strictly outside the region defined by the constraints (Equations (15.4.5)), the skeleton is unstable and the TAR model is nonstationary. For example, if $\phi _ { 2 , 1 } { > } 1$ , then the skeleton will escape to positive infinity for all sufficiently large initial values. On the boundary of the parametric region defined by (15.4.5), the intercept terms of the TAR model are pivotal in determining the stability of the skeleton and the stationarity of the TAR models; see Chan et al. (1985). In practice, we can check if the skeleton is stable numerically by using several different initial values. A stable skeleton gives us more confidence in assuming that the model is stationary.

![](images/79253fe9119b2de76ac043b4f90f53d91e4e5102c3ca9cfd713a9baf708711ec.jpg)  
Exhibit 15.10 Stationarity Region for the First-Order TAR Model (Shaded)

# 15.5 Threshold Models

The first-order (self-exciting) threshold autoregressive model can be readily extended to higher order and with a general integer delay:

$$
Y _ {t} = \left\{ \begin{array}{l l} \phi_ {1, 0} + \phi_ {1, 1} Y _ {t - 1} + \dots + \phi_ {1, p _ {1}} Y _ {t - p _ {1}} + \sigma_ {1} e _ {t}, & \text {i f} Y _ {t - d} \leq r \\ \phi_ {2, 0} + \phi_ {2, 1} Y _ {t - 1} + \dots + \phi_ {2, p _ {2}} Y _ {t - p _ {2}} + \sigma_ {2} e _ {t}, & \text {i f} Y _ {t - d} > r \end{array} \right. \tag {15.5.1}
$$

Note that the autoregressive orders $p _ { 1 }$ and $p _ { 2 }$ of the two submodels need not be identical, and the delay parameter $d$ may be larger than the maximum autoregressive orders. However, by including zero coefficients if necessary, we may and shall henceforth assume that $p _ { 1 } = p _ { 2 } = p$ and $1 \leq d \leq p$ , which simplifies the notation. The TAR model defined by Equation (15.5.1) is denoted as the $\mathrm { T A R } ( 2 ; p _ { 1 } , p _ { 2 } )$ model with delay d.

Again, the stability of the associated skeleton, obtained by setting the threshold to zero and suppressing the noise terms and the intercepts, implies that the TAR model is ergodic and stationary. However, the stability of the associated skeleton is now much more complex in the higher-order case so much so that the necessary and sufficient parametric conditions for the stationarity of the TAR model are still unknown. Nonetheless, there exist some simple sufficient conditions for the stationarity of a TAR model. For example, the TAR model is ergodic and hence asymptotically stationary if $| \Phi _ { 1 , 1 } |$ $+ \cdots + | \Phi _ { 1 , p } | < 1$ and $\left| \Phi _ { 2 , 1 } \right| + \cdots + \left| \Phi _ { 2 , p } \right| < 1$ ; see Chan and Tong (1985).

So far, we have considered the case of two regimes defined by the partition $- \infty < r <$ $\infty$ of the real line, so that the first (second) submodel is operational if $Y _ { t - d }$ lies in the first (second) interval. The extension to the case of $m$ regimes is straightforward and effected by partitioning the real line into $- \infty < r _ { 1 } < r _ { 2 } < \cdots < r _ { m - 1 } < \infty$ , and the position of $Y _ { t - d }$ relative to these thresholds determines which linear submodel is operational. We shall not pursue this topic further but shall restrict our discussion to the case of two regimes.

# 15.6 Testing for Threshold Nonlinearity

While Keenan’s test and Tsay’s test for nonlinearity are designed for detecting quadratic nonlinearity, they may not be sensitive to threshold nonlinearity. Here, we discuss a likelihood ratio test with the threshold model as the specific alternative. The null hypothesis is an $\operatorname { A R } ( p )$ model versus the alternative hypothesis of a two-regime TAR model of order $p$ and with constant noise variance, that is; $\sigma _ { 1 } = \sigma _ { 2 } = \sigma$ . With these assumptions, the general model can be rewritten as

$$
\begin{array}{l} Y _ {t} = \phi_ {1, 0} + \phi_ {1, 1} Y _ {t - 1} + \dots + \phi_ {1, p} Y _ {t - p} \tag {15.6.1} \\ + \left\{\phi_ {2, 0} + \phi_ {2, 1} Y _ {t - 1} + \dots + \phi_ {2, p} Y _ {t - p} \right\} I \left(Y _ {t - d} > r\right) + \sigma e _ {t} \\ \end{array}
$$

where the notation $I ( \cdot )$ is an indicator variable that equals 1 if and only if the enclosed expression is true. Moreover, in this formulation, the coefficient $\Phi _ { 2 , 0 }$ represents the change in the intercept in the upper regime relative to that of the lower regime, and similarly interpreted are $\Phi _ { 2 , 1 } , . . . , \Phi _ { 2 , p }$ . The null hypothesis states that $\begin{array} { r } { \Phi _ { 2 , 0 } = \Phi _ { 2 , 1 } = \cdots = \Phi _ { 2 , p } = } \end{array}$ 0. While the delay may be theoretically larger than the autoregressive order, this is seldom the case in practice. Hence, it is assumed that $d \leq p$ throughout this section, and

under this assumption and assuming the validity of linearity, the large-sample distribution of the test does not depend on $d$ .

In practice, the test is carried out with fixed $p$ and $d$ . The likelihood ratio test statistic can be shown to be equivalent to

$$
T _ {n} = (n - p) \log \left\{\frac {\hat {\sigma} ^ {2} \left(H _ {0}\right)}{\hat {\sigma} ^ {2} \left(H _ {1}\right)} \right\} \tag {15.6.2}
$$

where $n - p$ is the effective sample size, $\widehat { \sigma } ^ { 2 } ( H _ { 0 } )$ is the maximum likelihood estimator of the noise variance from the linear $\operatorname { A R } ( p )$ fit and $\widehat { \sigma } ^ { 2 } ( H _ { 1 } )$ from the TAR fit with the threshold searched over some finite interval. See the next section for a detailed discussion on estimating a TAR model. Under the null hypothesis that $\begin{array} { r } { \Phi _ { 2 , 0 } = \Phi _ { 2 , 1 } = \cdots = \Phi _ { 2 , p } = } \end{array}$ 0, the (nuisance) parameter $r$ is absent. Hence, the sampling distribution of the likelihood ratio test under $H _ { 0 }$ is no longer approximately $\bar { \chi } ^ { 2 }$ with $p$ degrees of freedom. Instead, it has a nonstandard sampling distribution; see Chan (1991) and Tong (1990). Chan (1991) derived an approximation method for computing the $p$ -values of the test that is highly accurate for small $p$ -values. The test depends on the interval over which the threshold parameter is searched. Typically, the interval is defined to be from the $a \times 1 0 0 { \mathrm { t h } }$ percentile to the $b \times 1 0 0 \mathrm { { t h } }$ percentile of $\{ Y _ { t } \}$ , say from the 25th percentile to the 75th percentile. The choice of $a$ and $b$ must ensure that there are adequate data falling into each of the two regimes for fitting the linear submodels.

The reader may wonder why the search of the threshold is restricted to some finite interval. Intuitively, such a restriction is desirable, as we want enough data to estimate the parameters for the two regimes under the alternative hypothesis. A deeper reason is mathematical in nature. This restriction is necessary because if the true model is linear, the threshold parameter is undefined, in which case an unrestricted search may result in the threshold estimator being close to the minimum or maximum data values, making the large-sample approximation ineffective.

We illustrate the likelihood ratio test for threshold nonlinearity using the (squareroot-transformed) relative sunspot data and the (log-transformed) predator data. Recall that both Keenan’s test and Tsay’s test suggested that these data are nonlinear. Setting $p$ $= 5$ , $a = 0 . 2 5$ , and $b = 0 . 7 5$ for the sunspot data, we tried the likelihood ratio test for threshold nonlinearity with different delays from 1 to 5, resulting in the test statistics being 46.9, 111.3, 99.1, 85.0, and 45.1, respectively.† Repeating the test with $a = 0 . 1$ and $b = 0 . 9$ yields identical results for this case. All the tests above have $p$ -values less than 0.000, suggesting that the data-generating mechanism is highly nonlinear. Notice that the test statistic attains the largest value when $d = 2$ ; hence we may tentatively estimate

the delay to be 2. But delay 3 is very competitive.

Next, consider the predator series, with $p = 4$ , $a = 0 . 2 5$ , $b = 0 . 7 5$ , and $1 \leq d \leq 4$ . The test statistics and their $p$ -values, enclosed in parentheses, are found to equal 19.3 (0.026), 28.0 (0.001), 32.0 (0.000), and 16.2 (0.073), respectively. Thus, there is some evidence that the predator series is nonlinear, with the delay likely to be 2 or 3. Note that the test is not significant for $d = 4$ at the $5 \%$ significance level.†

# 15.7 Estimation of a TAR Model

Because the stationary distribution of a TAR model does not have a closed-form solution, estimation is often carried out conditional on the $\operatorname* { m a x } ( p , d )$ initial values, where $p$ is the order of the process and $d$ the delay parameter. Moreover, the noise series is often assumed to be normally distributed, and we will make this assumption throughout this section. The normal error assumption implies that the response is conditionally normal, but see Samia, Chan and Stenseth (2007) for some recent work on the nonnormal case. If the threshold parameter $r$ and the delay parameter $d$ are known, then the data cases can be split into two parts according to whether or not $Y _ { t - d } \leq r$ . Let there be $n _ { 1 }$ data cases in the lower regime. With the data in the lower regime, we can regress $Y _ { t }$ on its lags 1 to $p$ to find the estimates of $\hat { \Phi } _ { 1 , 0 } , \hat { \Phi } _ { 1 , 1 } , . . . , \hat { \Phi } _ { 1 , p }$ and the maximum likelihood noise variance estimate $\hat { \sigma } _ { 1 } ^ { 2 }$ ; that is, the sum of squared residuals divided by $n _ { 1 }$ . The number $n _ { 1 }$ and the parameter estimates for the lower regime generally depend on $r$ and $d$ ; we sometimes write the more explicit notation, for example $n _ { 1 } ( r , d )$ , below for clarity. parameter estimates Similarly, using the data, say $\hat { \Phi } _ { 2 , 0 } , \hat { \Phi } _ { 2 , 1 } , . . . , \hat { \Phi } _ { 2 , p }$ $n _ { 2 }$ of them, falling in the upper regime, we can obtain the and $\mathsf { \bar { \alpha } } _ { 2 } ^ { 2 }$ . Clearly, $n _ { 1 } + n _ { 2 } = n - p$ , where $n$ is the sample size. Substituting these estimates into the log-likelihood function yields the so-called profile log-likelihood function of $( r , d )$ :

$$
\begin{array}{l} l (r, d) = - \frac {n - p}{2} \left\{1 + \log (2 \pi) \right\} - \frac {n _ {1} (r , d)}{2} \log \left(\left(\hat {\sigma} _ {1} (r, d)\right) ^ {2}\right) \tag {15.7.1} \\ - \frac {n _ {2} (r , d)}{2} \log ((\hat {\sigma} _ {2} (r, d)) ^ {2}) \\ \end{array}
$$

The estimates of $r$ and $d$ can be obtained by maximizing the profile likelihood function above. The optimization need only be searched with $r$ over the observed Y’s and integer $d$ between 1 and $p$ . This is because, for fixed $d$ , the function above is constant between two consecutive observations.

However, without some restrictions on the threshold parameter, the (conditional) maximum likelihood method discussed above will not work. For example, if the lower regime contains only one data case, the noise variance $\hat { \alpha } _ { 1 } ^ { 2 } = 0$ so that the conditional log-likelihood function equals $\infty$ , in which case the conditional maximum likelihood estimator is clearly inconsistent. This problem may be circumvented by restricting the

search of the threshold to be between two predetermined percentiles of Y; for example, between the tenth and ninetieth percentiles.

Another approach to handle the aforementioned difficulty is to estimate the parameters using the conditional least squares (CLS) approach. The CLS approach estimates the parameters by minimizing the predictive sum of squared errors, or equivalently conditional maximum likelihood estimation for the case of homoscedastic (constant-variance) Gaussian errors; that is, $\sigma _ { 1 } = \sigma _ { 2 } = \sigma$ so that maximizing the log-likelihood function is equivalent to minimizing the conditional residual sum of squares:

$$
\begin{array}{l} L (r, d) = \sum_ {t = p + 1} ^ {n} \left\{\left(Y _ {t} - \phi_ {1, 0} - \phi_ {1, 1} Y _ {t - 1} - \dots - \phi_ {1, p} Y _ {t - p}\right) ^ {2} I \left(Y _ {t - d} \leq r\right) \right. \tag {15.7.2} \\ + \left(Y _ {t} - \phi_ {2, 0} - \phi_ {2, 1} Y _ {t - 1} - \dots - \phi_ {2, p} Y _ {t - p}\right) ^ {2} I (Y _ {t - d} > r) \} \\ \end{array}
$$

where $I ( Y _ { t - d } \leq r )$ equals 1 if $Y _ { t - d } \leq r$ and 0 otherwise; the expression $I ( Y _ { t - d } > r )$ is similarly defined. Again, the optimization need only be done with $r$ searched over the observed $Y \mathrm { s }$ and $d$ an integer between 1 and $p$ . The conditional least squares approach has the advantage that the threshold parameter can be searched without any constraints. Under mild conditions, including stationarity and that the true conditional mean function is a discontinuous function, Chan (1993) showed that the CLS method is consistent; that is, the estimator approaches the true value with increasing sample size. As the delay is an integer, the consistency property implies that the delay estimator is eventually equal to the true value with very large sample size. Furthermore, the sampling error of the threshold estimator is of the order $1 / n$ , whereas the sampling error of the other parameters is of order $1 / { \sqrt { n } }$ . The faster convergence of the threshold parameter and the delay parameter to their true values implies that in assessing the uncertainty of the autoregressive parameter estimates, the threshold and the delay may be treated as if they were known. Consequently, the autoregressive parameter estimators from the two regimes are approximately independent of each other, and their sampling distributions are approximately the same as those from the ordinary least squares regression with data from the corresponding true regimes. These large-sample distribution results can be lifted to the case of the conditional maximum likelihood estimator provided the true parameter satisfies the regularity conditions alluded to before. Finally, we note that the preceding large-sample properties of the estimator are radically different if the true conditional mean function is continuous; see Chan and Tsay (1998).

In practice, the AR orders in the two regimes need not be identical or known. Thus, an efficient estimation procedure that also estimates the orders is essential. Recall that for linear ARMA models, the AR orders can be estimated by minimizing the AIC. For fixed $r$ and $d$ , the TAR model is essentially fitting two AR models of orders $p _ { 1 }$ and $p _ { 2 }$ , respectively, so that the AIC becomes

$$
\operatorname {A I C} \left(p _ {1}, p _ {2}, r, d\right) = - 2 l (r, d) + 2 \left(p _ {1} + p _ {2} + 2\right) \tag {15.7.3}
$$

where the number of parameters, excluding $r , d , \sigma _ { 1 }$ , and $\sigma _ { 2 }$ , equals $p _ { 1 } + p _ { 2 } + 2$ . Now, the minimum AIC (MAIC) estimation method estimates the parameters by minimizing the AIC subject to the constraint that the threshold parameter be searched over some inter-

val that guarantees any regimes have adequate data for estimation. Adding 2 to the minimum AIC so found is defined as the nominal AIC of the estimated threshold model, based on the naive idea of counting the threshold parameter as one additional parameter. Since the threshold parameter generally adds much flexibility to the model, it is likely to add more than one degree of freedom to the model. An asymptotic argument suggests that it may be equivalent to adding three degrees of freedom to the model; see Tong (1990, p. 248).

We illustrate the estimation methods with the predator series. In the estimation, the maximum order is set to be $p = 4$ and $1 \leq d \leq 4$ . This maximum order is the AR order determined by AIC, which is likely to be not smaller than the order of the true TAR model. Alternatively, the order may be determined by cross-validation, which is computer-intensive; see Cheng and Tong (1992). Using the MAIC method with the search of threshold roughly between the tenth and ninetieth percentiles, the table in Exhibit 15.11 displays the nominal AIC value of the estimated TAR model for $1 \leq d \leq 4$ . The nominal AIC is smallest when $d = 3$ , so we estimate the delay to be 3. The table in Exhibit 15.12 summarizes the corresponding model fit.

Exhibit 15.11 Nominal AIC of the TAR Models Fitted to the Log(predator) Series for ${ \pmb { 1 } } \leq { \pmb { \mathsf { d } } } \leq { \pmb { 4 } }$   

<table><tr><td>d</td><td>AIC</td><td>r</td><td>p1</td><td>p2</td></tr><tr><td>1</td><td>19.04</td><td>4.15</td><td>2</td><td>1</td></tr><tr><td>2</td><td>12.15</td><td>4.048</td><td>1</td><td>4</td></tr><tr><td>3</td><td>10.92</td><td>4.661</td><td>1</td><td>4</td></tr><tr><td>4</td><td>18.42</td><td>5.096</td><td>3</td><td>4</td></tr><tr><td colspan="5">&gt; AICM=NULL</td></tr><tr><td colspan="5">&gt; for(d in 1:4) 
{predator.tar.tar(y=log(predator(eq), p1=4, p2=4, d=d, a=.1, b=.9)</td></tr><tr><td colspan="5">&gt; AICM=rbind(AICM, 
c(d,predator.tar$AIC, signif(predator.tar$thd,4), 
predator.tar$s1, predator.tar$s2))}</td></tr><tr><td colspan="5">&gt; colnames(AICM)=c(&#x27;d&#x27;,&#x27;nominal AIC&#x27;,&#x27;r&#x27;,&#x27;p1&#x27;,&#x27;p2&#x27;)</td></tr><tr><td colspan="5">&gt; rnames(AICM)=NULL</td></tr><tr><td colspan="5">&gt; AICM</td></tr></table>

Although the maximum autoregressive order is 4, the MAIC method selects order 1 for the lower regime and order 4 for the upper regime. The submodel in each regime is estimated by ordinary least squares (OLS) using the data falling in that regime. Hence a less biased estimator of the noise variance may be estimated by the within-regime residual sum of squared errors normalized by the effective sample size which equals the number of data in that regime minus the number of autoregressive parameters (including the intercept) of the corresponding submodel. The “unbiased” noise variance $\widetilde { \sigma } _ { i } ^ { 2 }$ of the ith regime relates to its maximum likelihood counterpart by the formula

$$
\tilde {\sigma} _ {i} ^ {2} = \frac {n _ {i}}{n _ {i} - p _ {i} - 1} \hat {\sigma} _ {i} ^ {2}, \tag {15.7.4}
$$

where $p _ { i }$ is the autoregressive order of the ith submodel. Moreover, $( n _ { i } - p _ { i } - 1 ) \tilde { \sigma } _ { i } ^ { 2 } / \sigma _ { i } ^ { 2 }$ is approximately distributed as $\chi ^ { 2 }$ with $n _ { i } - p _ { i } - 1$ degrees of freedom. For each regime, the $t$ -statistics and corresponding $p$ -values reported in Exhibit 15.12 are identical with the computer output for the case of fitting an autoregressive model with the data falling in that regime. Notice that the coefficients of lags 2 and 3 in the upper regime are not significant, while that of lag 4 is mildly significant at the $5 \%$ significance level. Hence, the model for the upper regime may be approximated by a first-order autoregressive model. We shall return to this point later.

Exhibit 15.12 Fitted TAR(2;1,4) Model for the Predator Data: MAIC Method   

<table><tr><td></td><td>Estimate</td><td>Std. Error</td><td>t-statistic</td><td>p-value</td></tr><tr><td>d</td><td>3</td><td></td><td></td><td></td></tr><tr><td>r</td><td>4.661</td><td></td><td></td><td></td></tr><tr><td colspan="5">Lower Regime (n1= 30)</td></tr><tr><td>φ1,0</td><td>0.262</td><td>0.316</td><td>0.831</td><td>0.41</td></tr><tr><td>φ1,1</td><td>1.02</td><td>0.0704</td><td>14.4</td><td>0.00</td></tr><tr><td>σ21</td><td>0.0548</td><td></td><td></td><td></td></tr><tr><td colspan="5">Upper Regime (n2= 23)</td></tr><tr><td>φ2,0</td><td>4.20</td><td>1.28</td><td>3.27</td><td>0.00</td></tr><tr><td>φ2,1</td><td>0.708</td><td>0.202</td><td>3.50</td><td>0.00</td></tr><tr><td>φ2,2</td><td>-0.301</td><td>0.312</td><td>-0.965</td><td>0.35</td></tr><tr><td>φ2,3</td><td>0.279</td><td>0.406</td><td>0.686</td><td>0.50</td></tr><tr><td>φ2,4</td><td>-0.611</td><td>0.273</td><td>-2.24</td><td>0.04</td></tr><tr><td>σ22</td><td>0.0560</td><td></td><td></td><td></td></tr></table>

> predator.tar. $\beth =$ tar(y=log(predator.eq), $\mathtt { p 1 } = 4$ , $\mathtt { p } 2 = 4$ , $\mathtt { d } = 3$ ,a=.1,b=.9, print $\cdot = \mathrm { T }$ ) $>$ tar( $\cdot y { = } 1 0 9$ (predator.eq), $\mathtt { p 1 } = \mathtt { 1 }$ , $\mathtt { p } 2 = 4$ , $d = 3$ ,a=.1,b=.9,print $\cdot = \mathrm { T }$ , method $= 1$ CLS') # re-do the estimation using the CLS method > tar( $\cdot y { = } 1 0 9$ (predator.eq), $\mathtt { p 1 } = 4$ , $\mathtt { p } 2 = 4$ , $d = 3$ ,a=.1, $\mathtt { b } =$ .9,print $\cdot = \mathrm { T }$ , method $= 1$ CLS') # the CLS method does not estimate the AR orders

The threshold estimate is 4.661, roughly the 57th percentile. In general, a threshold estimate that is too close to the minimum or the maximum observation may be unreliable due to small sample size in one of the regimes, which, fortunately, is not the case

here. Exhibit 15.12 does not report the standard error of the threshold estimate because its sampling distribution is nonstandard and rather complex. Similarly, the discreteness of the delay estimator renders its standard error useless. However, a parametric bootstrap may be employed to draw inferences on the threshold and the delay parameters. An alternative is to adopt the Bayesian approach of Geweke and Terui (1993). In contrast, the fitted AR(4) model has the coefficient estimates of lags 1 to 4 equal to 0.943 (0.136), −0.171 (0.188), −0.1621 (0.186), and −0.238 (0.136), respectively, with their standard errors enclosed in parentheses; the noise variance is estimated to be 0.0852, which is substantially larger than the noise variances of the TAR(2;1,4) model. Notice that the AR(4) coefficient estimate is close to being nonsignificant, and the AR(2) and AR(3) coefficient estimates are not significant.

An interesting question concerns the interpretation of the two regimes. One way to explore the nature of the regimes is to identify which data value falls in which regime in the time series plot of the observed process. In the time series plot in Exhibit 15.2 on page 387, data falling in the lower regime (that is, those whose lag 3 values are less than 4.661) are drawn as solid circles, whereas those in the upper regime are displayed as open circles. The plot reveals that the estimated lower regime corresponds to the increasing phase of the predator cycles and the upper regime corresponds to the decreasing phase of the predator cycles. A biological interpretation is the following. When the predator number was low one and a half days earlier, the prey species would have been able to increase in the intervening period so that the predator species would begin to thrive. On the other hand, when the predator numbered more than $1 0 6 \approx \exp ( 4 . 6 6 1 )$ one and a half days earlier, the prey species crashed in the intervening period so that the predator species would begin to crash. The increasing phase (lower regime) of the predator population tends to be associated with a robust growth of the prey series that may be less affected by other environmental conditions. On the other hand, during the decreasing phase (upper regime), the predator species would be more susceptible to environmental conditions, as they were already weakened by having less food around. This may explain why the lower regime has a slightly smaller noise variance than the upper regime; hence the slight conditional heteroscedasticity. The difference of the noise variance in the two regimes is unlikely to be significant, although the conditional heteroscedasticity is more apparent in the TAR(2;1,1) model to be discussed below. In general, the regimes defined by the relative position of the lag $d$ values of the response are proxies for some underlying latent process that effects the switching between the linear submodels. With more substantive knowledge of the switching mechanism, the threshold mechanism may, however, be explicitly modeled.

While the interpretation of the regimes above is based on the time series plot, it may be confirmed by examining the fitted submodels. The fitted model of the lower regime implies that on the logarithmic scale

$$
Y _ {t} = 0. 2 6 2 + 1. 0 2 Y _ {t - 1} + 0. 2 3 4 e _ {t} \tag {15.7.5}
$$

The lag 1 coefficient is essentially equal to 1 and suggests that the predator species had a (median) growth rate of $( \exp ( 0 . 2 6 2 ) - 1 ) 1 0 0 \% \approx 3 0 \%$ every half day, although the intercept is not significant at the $5 \%$ level. This submodel is explosive because $Y _ { t } \to \infty$ as $t \to \infty$ if left unchecked.

Interpretation of the fitted model of the upper regime is less straightforward because it is an order 4 model. However, it was suggested earlier that it may be approximated by an AR(1) model. Taking up this suggestion, we reestimated the TAR model with the maximum order being 1 for both regimes.† The threshold estimate is unchanged. The lower regime gains one data case, with less of an initial data requirement, but the autoregressive coefficients are almost unchanged. The fitted model of the upper regime becomes

$$
Y _ {t} = 0. 5 1 7 + 0. 8 0 7 Y _ {t - 1} + 0. 9 8 9 e _ {t} \tag {15.7.6}
$$

which is a stationary submodel. The growth rate on the logarithmic scale equals

$$
Y _ {t} - Y _ {t - 1} = 0. 5 1 7 - 0. 1 9 3 Y _ {t - 1} + 0. 9 8 9 e _ {t} \tag {15.7.7}
$$

which has a negative median since $Y _ { t - 1 } > 4 . 6 6 1$ on the upper regime. Notice that the conditional heteroscedasticity is more apparent now than the fitted TAR(2;1,4) model. The (nominal) AIC of the TAR(2;1,1) model with $d = 3$ equals 14.78, which is, however, not directly comparable with 10.92 of the TAR(2;1,4) model because of the difference in sample size. Models with different sample sizes may be compared by their nominal AIC per observation. In this case, the normalized AIC increases from $0 . 2 0 6 = 1 0 . 9 2 / 5 3$ to $0 . 2 7 4 = 1 4 . 7 8 / 5 4$ when the order is decreased from 4 to 1, suggesting that the TAR(2;1,4) model is preferable to the TAR(2;1,1) model.

Another way to assess a nonlinear model is to examine the long-term (asymptotic) behavior of its skeleton. Recall that the skeleton of a model is obtained by suppressing the noise term from the model; that is, replacing the noise term by 0. The skeleton may diverge to infinity, or it may converge to a limit point, a limit cycle, or a strange attractor; see Chan and Tong (2001) for definitions and further discussion. The skeleton of a stationary ARMA model always converges to some limit point. On the other hand, the skeleton of a stationary nonlinear model may display the full complexity of dynamics alluded to earlier. The skeleton of the fitted TAR(2;1,4) model appears to converge to a limit cycle of period 10, as shown in Exhibit 15.13. The limit cycle is symmetric in the sense that its increase phase and decrease phase are of the same length. The apparent long-run stability of the skeleton suggests that the fitted TAR(2;1,4) model with $d = 3$ is stationary. In general, with the noise term in the model, the dynamic behavior of the model may be studied by simulating some series from the stochastic model. Exhibit 15.14 shows a typical realization from the fitted TAR(2;1,4) model.

Exhibit 15.13 Skeleton of the TAR(2;1,4) Model for the Predator Series   
![](images/ef04df4779de3933df5b6bc67bf8361ce0dcaf85b5952aad1a55c6164ac76cfe.jpg)  
> tar.skeleton(predator.tar.1)

Exhibit 15.14 Simulated TAR(2;1,4) Series   
![](images/d7e9d99f1e9936b5beafdabd6965a51d1ddbcfc82e7be0361dbdd4b20beac92b.jpg)  
> set.seed(356813)   
> plot(y=tar.sim( $\mathtt { \Gamma } _ { \mathtt { n } = 5 7 }$ ,object $=$ predator.tar.1)$y, $\mathbf { x } { = } \mathbf { 1 } : 5 7$ , ylab $^ { \prime = }$ expression(Y[t]),xlab $^ { 1 = }$ expression(t),type='o')

The limit cycle of the skeleton of the fitted TAR(2;1,1) model with $d = 3$ is asymmetric, with the increase phase of length 5 and the decrease phase of length 4; see Exhibit 15.15. A realization of the fitted TAR(2;1,1) model is shown in Exhibit 15.16.

![](images/672cbb539e14bbcd1b526e5bbf8aa79184509176e1c95137e20661871ff9ab1b.jpg)  
Exhibit 15.15 Skeleton of the First-Order TAR Model for the Predator Series

```html
> predator.tar.2=tar(log(predator EQ), p1=1, p2=1, d=3, a=.1, b=.9, print=T) 
```

```txt
> tar.skeleton(predator.tar.2) 
```

![](images/dcd86ee8981cfaae04f87d9bc9aa5e8845abb730e3117d128514a8f1891a6e27.jpg)  
Exhibit 15.16 Simulation of the Fitted TAR(2;1,1) Model

```txt
> set.seed(356813) 
```

```txt
> plot(y=tar.sim(n=57, object=predator.tar.2) $y, x=1:57, ylab=expression(Y[t]), xlab=expression(t), type='o') 
```

For the predator data, excluding the two initial transient cycles and the last incomplete cycle, the table in Exhibit 15.17 lists the length of the successive increasing and decreasing phases. Observe that the mean length of the increasing phases is 5.4 and that of the decreasing phases is 4.6.

Exhibit 15.17 Length of the Increasing and Decreasing Phases of the Predator Series   

<table><tr><td colspan="2">Phase</td></tr><tr><td>Increasing</td><td>Decreasing</td></tr><tr><td>6</td><td>4</td></tr><tr><td>7</td><td>5</td></tr><tr><td>5</td><td>4</td></tr><tr><td>4</td><td>5</td></tr><tr><td>5</td><td>5</td></tr></table>

There is some evidence of asymmetry with a longer increase phase than the decrease phase. Based on the cycle length analysis, the TAR(2;1,1) model appears to pick up the asymmetric cycle property better than the TAR(2;1,4) model, but the latter model gets the cycle length better matched to the observed average cycle length. A more rigorous comparison between the cyclical behavior of a fitted model and that of the data can be done by comparing the spectral density of the data with that of a long realization from the fitted model. Exhibit 15.18 plots the spectrum of the data using a modified Daniell window with a (3,3) span. Also plotted is the spectrum of the fitted TAR(2;1,4) model (dashed line) and that of the fitted TAR(2;1,1) model (dotted line), both of which are based on a simulated realization of size 10,000, a modified Daniell window with a (200,200) span, and $10 \%$ tapering. It can be seen that the spectrum of the TAR(2;1,4) model follows that of the predator series quite closely and is slightly better than the simplified TAR(2;1,1) model.

![](images/81ff29a413d4c2b39052e4088964cc32a34d0908219c604c2234f4b6ab423978.jpg)  
Exhibit 15.18 Spectra of Log(predator) Series, Dashed Line for TAR(2;1,1), Dotted Line for TAR(2;1,4)

```txt
> set.seed(2357125)
> yy.1.4 = tar.sim(predator.tar.1, n=10000) $y
> yy.1 = tar.sim(predator.tar.2, n=10000) $y
> spec.1.4 = spec(yy.1.4, taper=.1, span=c(200, 200), plot=F) 
```

```txt
> spec.1=spec(yy.1,taper=.1, span=c(200,200),plot=F)
> spec.predator=spec(log(predator EQ), taper=.1,
	span=c(3,3), plot=F)
> spec.predator=spec(log(predator EQ), taper=.1, span=c(3,3),
	ylim=range(c(spec.1.4$spec, spec.1$spec, spec.predator$$pec))
> lines(y=spec.1.4$$pec,x=spec.1.4$freq,lty=2)
> lines(y=spec.1$$pec,x=spec.1$freq,lty=3) 
```

We note that the conditional least squares method with the predator data yields the same threshold estimate for $d = 3$ and hence also the other parameter estimates, although this need not always be the case. Finally, a couple of clarifying remarks on the predator series analysis are in order. As the experimental prey series is also available, a bivariate time series analysis may be studied. But it is not pursued here since nonlinear time series analysis with multiple time series is not a well-charted area. Moreover, real biological data are often observational, and abundance data of the prey population are often much noisier than those of the predator population because the predator population tends to be fewer in number than the prey population. Furthermore, predators may switch from their favorite prey food to other available prey species when the former becomes scarce, rendering a more complex prey-predator system. For example, in a good year, hares may be seen hopping around in every corner in the neighborhood, whereas it is rare to spot a lynx, their predator! Thus, biological analysis often focuses on the abundance data of the predator population. Nonetheless, univariate time series analysis of the abundance of the predator species may shed valuable biological insights on the prey-predator interaction; see Stenseth et al. (1998, 1999) for some relevant discussion on a panel of Canadian lynx series. For the lynx data, a TAR(2;2,2) model with delay equal to 2 is the prototypical model, with delay 2 lending some nice biological interpretations. We note that, for the predator series, delay 2 is very competitive; see Exhibit 15.11, and hence may be preferred on biological grounds. In one exercise, we ask the reader to fit a TAR model for the predator series with delay set to 2 and interpret the findings by making use of the framework studied in Stenseth et al. (1998, 1999).

# 15.8 Model Diagnostics

In Section 15.7, we introduced some model diagnostic techniques; for example, skeleton analysis and simulation. Here, we discuss some formal statistical approaches to model diagnostics via residual analysis. The raw residuals are defined as subtracting the fitted value from the data, where the tth fitted value is the estimated conditional mean of $Y _ { t }$ given past values of $Y \mathrm { s }$ ; that is, the residuals $\widehat { \sf \varepsilon } _ { t }$ are given by

$$
\begin{array}{r l} \hat {\varepsilon} _ {t} = & Y _ {t} - \left\{\hat {\phi} _ {1, 0} + \hat {\phi} _ {1, 1} Y _ {t - 1} + \dots + \hat {\phi} _ {1, p} Y _ {t - p} \right\} I \left(Y _ {t - \hat {d}} \leq \hat {r}\right) \\ & - \left\{\hat {\phi} _ {2, 0} + \hat {\phi} _ {2, 1} Y _ {t - 1} + \dots + \hat {\phi} _ {2, p} Y _ {t - p} \right\} I \left(Y _ {t - \hat {d}} > \hat {r}\right) \end{array} \tag {15.8.1}
$$

These are the same as the raw residuals from the fitted submodels. The standardized residuals are obtained by normalizing the raw residuals by their appropriate standard deviations:

$$
\hat {\varepsilon} _ {t} = \frac {\hat {\varepsilon} _ {t}}{\hat {\sigma} _ {1} I \left(Y _ {t - \hat {d}} \leq \hat {r}\right) + \hat {\sigma} _ {2} I \left(Y _ {t - \hat {d}} > \hat {r}\right)} \tag {15.8.2}
$$

that is, raw residuals from the lower (upper) regime are normalized by the noise standard deviation estimate of the lower (upper) regime. As in the linear case, the time series plot of the standardized residuals should look random, as they should be approximately independent and identically distributed if the TAR model is the true data mechanism; that is, if the TAR model is correctly specified. As before, we look for the presence of outliers and any systematic pattern in such a plot, in which case it may provide a clue for specifying a more appropriate model. The independence assumption of the standardized errors can be checked by examining the sample ACF of the standardized residuals. Nonconstant variance may be checked by examining the sample ACF of the squared standardized residuals or that of the absolute standardized residuals.

Here, we consider the generalization of the portmanteau test based on some overall measure of the magnitude of the residual autocorrelations. The reader may want to review the discussion in Section 12.5 on page 301, where we explain that even if the model is correctly specified, the residuals are generally dependent and so are their sample autocorrelations. Unlike the case of linear ARIMA models, the dependence of the residuals necessitates the employment of a (complex) quadratic form of the residual autocorrelations:

$$
B _ {m} = n _ {\text {e f f}} \sum_ {i = 1} ^ {m} \sum_ {j = 1} ^ {m} q _ {i, j} \hat {\rho} _ {i} \hat {\rho} _ {j} \tag {15.8.3}
$$

where $n _ { \mathrm { e f f } } = n - \operatorname* { m a x } ( p _ { 1 } , p _ { 2 } , d )$ is the effective sample size, $\hat { \rho } _ { i }$ the ith-lag sample autocorrelation of the standardized residuals, and $q _ { i , j }$ some model-dependent constants given in Appendix L on page 421. If the true model is a TAR model, $\hat { \rho } _ { i }$ are likely close to zero and so is $B _ { m }$ , but $B _ { m }$ tends to be large if the model specification is incorrect. The quadratic form is designed so that $B _ { m }$ is approximately distributed as $\chi ^ { 2 }$ with m degrees of freedom. Mathematical theory predicts that the $\chi ^ { 2 }$ distribution approximation is generally more accurate with larger sample size and relatively small m as compared with the sample size.

In practice, the $p$ -value of $B _ { m }$ may be plotted against m over a range of m values to provide a more comprehensive assessment of the independence assumption on the standardized errors. The bottom figure of Exhibit 15.19 reports the portmanteau test of the TAR(2;1,1) model fitted to the predator series discussed earlier for $1 \leq m \leq 1 2$ . The top figure there is the time series plot of the standardized residuals. Except for a possible outlier, the plot shows no particular pattern. The middle figure is the ACF plot of the standardized residuals. The confidence band is based on the simple $1 . 9 6 \not \sqrt { n }$ rule and should be regarded as a rough guide on the significance of the residual ACF. It suggests that the lag 1 residual autocorrelation is significant. The more rigorous portmanteau tests are all significant for $m \le 6$ , suggesting a lack of fit for the TAR(2;1,1) model. Similar diagnostics for the TAR(2;1,4) model are shown in Exhibit 15.20. Now, the only potential problem is a possible outlier. However, the fitted model changed little upon deleting the last four data points, including the potential outlier; hence we conclude that

the fitted TAR(2;1,4) model is fairly robust. Exhibit 15.21 displays the QQ normal score plot of the standardized residuals, which is apparently straight and hence the errors appear to be normally distributed. In summary, the fitted TAR(2;1,4) model provides a good fit to the predator series.

![](images/dbeffca0064a2a218212aad2a01ac6c22f9e12d17a706828b481e73ee427d3bb.jpg)  
Exhibit 15.19 Model Diagnostics of the First-Order TAR Model: Predator Series

![](images/d31a140a61927f8ce737053c102df12aa13928bcce2760a2b5b029a248c8e23b.jpg)

![](images/58746844cee832ad16f38bd849614ca6509af01aad2e00a9b546071c37e74af7.jpg)

> win.graph(width=4.875,height=4.5)   
> tsdiag(predator.tar.2,gof.lag=20)

![](images/a8544c3f36ffd9675064aff62d7894e6fdafb8655c3dad4e3f33948d2b8d547d.jpg)  
Exhibit 15.20 Model Diagnostics for the TAR(2;1,4) Model: Predator Series

![](images/64ede6081409b3dbdd269f266ca3824da3d368de6f969a1b9a85b5553cca32fe.jpg)

![](images/3d37a955673d4780ba737efffc42ece2abb2132c7f58ef0fcfd83a50f191d7ce.jpg)  
> tsdiag(predator.tar.1,gof.lag=20)

# Exhibit 15.21 QQ Normal Plot of the Standardized Residuals

![](images/d67b9092ef9ef171b77a4a301713bc2b2e334bc4f3e33078ef63e8304022c051.jpg)

> win.graph(width=2.5,height=2.5,pointsize=8)   
> qqnorm(predator.tar.1$std.res); qqline(predator.tar.1$std.res)

# 15.9 Prediction

In this section, we consider the problem of predicting future values from a TAR process. In practice, prediction is based on an estimated TAR model. But, as in the case of ARIMA models, the uncertainty due to parameter estimation is generally small compared with the natural variation of the underlying process. So, we shall proceed below as if the fitted model were the true model. The uncertainty of a future value, say $Y _ { t + \ell }$ , is completely characterized by its conditional probability distribution given the current and past data $Y _ { t } , Y _ { t - 1 } , \ldots$ , referred to as the l-step-ahead predictive distribution below. For ARIMA models with normal errors, all predictive distributions are normal, which greatly simplifies the computation of a predictive interval, as it suffices to find the mean and variance of the predictive distribution. However, for nonlinear models, the predictive distributions are generally nonnormal and often intractable. Hence, a prediction interval may have to be computed by brute force via simulation. The simulation approach may be best explained in the context of a first-order nonlinear autoregressive model:

$$
Y _ {t + 1} = h \left(Y _ {t}, e _ {t + 1}\right) \tag {15.9.1}
$$

Given $Y _ { t } = y _ { t } , Y _ { t - 1 } = y _ { t - 1 } , \dots$ , we have $Y _ { t + 1 } = h ( y _ { t } , e _ { t + 1 } )$ so a realization of $Y _ { t + 1 }$ from the one-step-ahead predictive distribution can be obtained by drawing $e _ { t + 1 }$ from the error distribution and computing $h ( y _ { t } , e _ { t + 1 } )$ . Repeating this procedure independently $B$ times, say 1000 times, we get a random sample of $B$ values from the one-step-ahead predictive distribution. The one-step-ahead predictive mean may be estimated by the sample mean of these $B$ values. However, it is important to inspect the shape of the one-step-ahead predictive distribution in order to decide how best to summarize the predictive information. For example, if the predictive distribution is multimodal or very

skewed, the one-step-ahead predictive mean need not be an appropriate point predictor. A generally useful approach is to construct a $9 5 \%$ prediction interval for $Y _ { t + 1 }$ ; for example, the interval defined by the 2.5th percentile to the 97.5th percentile of the simulated $B$ values.

The simulation approach can be readily extended to finding the l-step-ahead predictive distribution for any integer $\ell \geq 2$ by iterating the nonlinear autoregression.

$$
\left. \begin{array}{c} Y _ {t + 1} = h \left(Y _ {t}, e _ {t + 1}\right) \\ Y _ {t + 2} = h \left(Y _ {t + 1}, e _ {t + 2}\right) \\ \vdots \\ Y _ {t + \ell} = h \left(Y _ {t + \ell - 1}, e _ {t + \ell}\right), \end{array} \right\} \tag {15.9.2}
$$

where $Y _ { t } = y _ { t }$ and $\{ e _ { t + 1 } , . . . , e _ { t + \ell } \}$ is a random sample of l values drawn from the error distribution. This procedure may be repeated $B$ times to yield a random sample from the l-step-ahead predictive distribution, with which we can compute prediction intervals of $Y _ { t + \ell }$ or any other predictive summary statistic.

Indeed, the l-tuple $( Y _ { t + 1 } , . . . , Y _ { t + \ell } )$ is a realization from the joint predictive distribution of the first l-step-ahead predictions. So, the procedure above actually yields a random sample of $B$ vectors from the joint predictive distribution of the first l-step-ahead predictions.

Henceforth in this section, we focus on the prediction problem when the true model is a TAR model. Fortunately, the simulation approach is not needed for computing the one-step-ahead predictive distribution in the case of a TAR model. To see this, consider the simple case of a first-order TAR model. In this case, $Y _ { t + 1 - d }$ is known, so that the regime for $Y _ { t + 1 }$ is known. If $Y _ { t + 1 - d } \leq r$ then $Y _ { t + 1 }$ follows the AR(1) model

$$
Y _ {t + 1} = \phi_ {1, 0} + \phi_ {1, 1} Y _ {t} + \sigma_ {1} e _ {t + 1} \tag {15.9.3}
$$

Because $Y _ { t } = y _ { t }$ is fixed, the conditional distribution of $Y _ { t + 1 }$ is normal with mean equal to $\Phi _ { 1 , 0 } + \Phi _ { 1 , 1 } y _ { t }$ and variance $\sigma _ { 1 } ^ { 2 }$ . Similarly, if $Y _ { t } > r$ , $Y _ { t + 1 }$ follows the AR(1) model of the upper regime so that, conditionally, it is normal with mean $\Phi _ { 2 , 0 } + \Phi _ { 2 , 1 } y _ { t }$ and variance $\sigma _ { 2 } ^ { 2 }$ . A similar argument shows that, for any TAR model, the one-step-ahead predictive distribution is normal. The predictive mean is, however, a piecewise linear function, and the predictive standard deviation is piecewise constant.

Similarly, it can be shown that if $\ell \leq d$ , then the $\ell \cdot$ -step-ahead predictive distribution of a TAR model is also normal. But if $\ell > d$ , the l-step-ahead predictive distribution is no longer normal. The problem can be illustrated in the simple case of a first-order TAR model with $d = 1$ and $\ell = 2$ . While $Y _ { t + 1 }$ follows a fixed linear model determined by the observed value of Yt, $Y _ { t + 2 }$ may be in the lower or upper regime, depending on the random value of $Y _ { t + 1 }$ . Suppose that $y _ { t } \leq r .$ Now, $Y _ { t + 1 }$ falls in the lower regime if $Y _ { t + 1 } =$ $\sigma _ { 1 } e _ { t + 1 } + \phi _ { 1 , 0 } + \phi _ { 1 , 1 } y _ { t } \leq r$ , which happens with probability $p _ { t } = P r ( \sigma _ { 1 } e _ { t + 1 } + \phi _ { 1 , 0 } + \phi _ { 1 , 1 } y _ { t }$ $\le r )$ and in which case

$$
\begin{array}{l} Y _ {t + 2} = \sigma_ {1} e _ {t + 2} + \phi_ {1, 0} + \phi_ {1, 1} Y _ {t + 1} \\ = \sigma_ {1} e _ {t + 2} + \phi_ {1, 1} \sigma_ {1} e _ {t + 1} + \phi_ {1, 1} \phi_ {1, 0} + \phi_ {1, 1} ^ {2} y _ {t} + \phi_ {1, 0} \tag {15.9.4} \\ \end{array}
$$

which is a normal distribution with mean equal to $\Phi _ { 1 , 1 } \Phi _ { 1 , 0 } + \Phi _ { 1 , 1 } ^ { 2 } y _ { t } + \Phi _ { 1 , 0 }$ and variance $\sigma _ { 1 } ^ { 2 } + \phi _ { 1 , 1 } ^ { 2 } \sigma _ { 1 } ^ { 2 }$ . On the other hand, with probability $1 - p _ { t } , Y _ { t + 1 }$ falls in the upper regime, in which case the conditional distribution of $Y _ { t + 2 }$ is normal but with mean $\Phi _ { 2 , 1 } ( \Phi _ { 1 , 0 } + \Phi _ { 1 , 1 } y _ { t } ) + \Phi _ { 2 , 0 }$ and variance $\mathfrak { O } _ { 2 } ^ { 2 } + \Phi _ { 2 , 1 } ^ { 2 } \mathfrak { O } _ { 1 } ^ { 2 }$ . Therefore, the conditional distribution of $Y _ { t + 2 }$ is a mixture of two normal distributions. Note that the mixture probability $p _ { t }$ depends on $y _ { t }$ . In particular, the higher-step-ahead predictive distributions are nonnormal for a TAR model if $\ell > d$ , and so we have to resort to simulation to find the predictive distributions.

As an example, we compute the prediction intervals for the logarithmically transformed predator data based on the fitted TAR(2;1,4) model with $d = 3$ ; see Exhibit 15.22, where the middle dashed line is the median of the predictive distribution and the other dashed lines are the 2.5th and 97.5th percentiles of the predictive distribution.

![](images/295a0901b7458a76be9a3c74397a123f28abe784a5813f1516c7b0b0d58a4ee9.jpg)  
Exhibit 15.22 Prediction of the Predator Series

```txt
> set.seed(2357125) 
```

```javascript
>win.graph(width=4.875,height=2.5,pointsize=8) 
```

```lua
> pred.predator = predict(predator.tar.1, n. ahead = 60, n.sim = 10000) 
```

```txt
> yy=ts(c(log(predator EQ), pred.predator$fit), frequency=2, start=start(predator EQ)) 
```

```javascript
> plot(yy,type='n',ylim=range(c(yy, pred.predator\\(pred.interval)), ylab \(=\) 'Log Predator',xlab \(\equiv\) expression(t)) 
```

```txt
> lines(log(predator EQ)) 
```

> lineswindow(yy，start $\equiv$ end(predator EQ)+c(0,1)),lty=2)

```javascript
> lines(ts(pred.predator\\(pred.interval[2,], start \)=\( end(predator eq) \(^+\) c(0,1),freq=2)，lty=2) 
```

```javascript
> lines(ts(pred.predator\\(pred.interval[1,], start \)=\( end(predator.eq) \(^+\) c(0,1),freq=2)，lty=2) 
```

The simulation size here is 10,000. In practice, a smaller size such as 1000 may be adequate. The median of the predictive distribution can serve as a point predictor. Notice that the predictive medians display the cyclical pattern of the predator data initially and then approach the long-run median with increasing number of steps ahead. Similarly, the predictive intervals approach the interval defined by the 2.5th and 97.5th percentiles of the stationary distribution of the fitted TAR model. However, a new feature is that prediction need not be less certain with increasing number of steps ahead, as the length of the prediction intervals does not increase monotonically with increasing number of steps ahead; see Exhibit 15.23. This is radically different from the case of ARIMA models, for which the prediction variance always increases with the number of prediction steps ahead.

![](images/6c10d9e9c8030e62c5dc37d0edf327d31a9f00a33b5fc6ee75f65b613c3b3422.jpg)  
Exhibit 15.23 Width of the $95 \%$ Prediction Intervals Against Lead Time

```javascript
> plot(ts.apply(pred.predator\\(pred.interval,2, function(x){x[2]-x[1]})， ylab \(=\) 'Length of Prediction Intervals', xlab \(=\) 'Number of Steps Ahead')
```

Recall that, for the TAR model, the prediction distribution is normal if and only if the number of steps ahead $\ell \leq d$ . Exhibit 15.24 shows the QQ normal score plot of the three-step-ahead predictive distribution, which is fairly straight. On the other hand, the QQ normal score plot of the six-step-ahead predictive distribution (Exhibit 15.25) is consistent with nonnormality.

# Exhibit 15.24 QQ Normal Plot of the Three-Step-Ahead Predictive Distribution

![](images/ab66af10e1686c16c580e84a85a53ba2ad90c86b0dabc90451d4f99a1fb743f8.jpg)

> win.graph(width=2.5,height=2.5,pointsize=8)   
> qqnorm(pred.predator$pred.matrix[,3])   
> qqline(pred.predator$pred.matrix[,3])

# Exhibit 15.25 QQ Normal Plot of the Six-Step-Ahead Predictive Distribution

![](images/77ba5dee1438d9a6d9b6631e46be1f7095daed81d27a2d60f78fdd749ac1f698.jpg)  
Theoretical Quantiles

> qqnorm(pred.predator$pred.matrix[,6])   
> qqline(pred.predator$pred.matrix[,6])

# 15.10 Summary

In this chapter, we have introduced an important nonlinear times serie model—the threshold model. We have shown how to test for nonlinearity and, in particular, for threshold nonlinearity. We then proceeded to consider the estimation of the unknown parameters in these models using both the minimum AIC (MAIC) criterion and the conditional least squares approach. As with all models, we learned how to criticize them through various model diagnostics, including an extended portmanteau test. Finally, we demonstrated how to form predictions from threshold models, including the calculation and display of prediction intervals. Several substantial examples were used to illustrate the methods and techniques discussed.

# EXERCISES

15.1 Fit a TAR model for the predator series with delay set to 2, and interpret the findings by making use of the framework studied in Stenseth et al. (1998, 1999). (You may first want to check whether or not their framework is approximately valid for the TAR model.) Also, compare the fitted model with the TAR(2;1,4) model with delay 3 reported in the text. (The data file is named veilleux.)   
15.2 Fit a TAR model to the square-root-transformed relative sunspot data, and examine its goodness of fit. Interpret the fitted TAR model. (The data file is named spots.)   
15.3 Predict the annual relative sunspot numbers for ten years using the fitted model obtained in Exercise 15.2. Draw the prediction intervals and the predicted medians. (The data file is named spots.)   
15.4 Examine the long-run behavior of the skeleton of the fitted model for the relative sunspot data. Is the fitted model likely to be stationary? Explain your answer.   
15.5 Simulate a series of size 1000 from the TAR model fitted to the relative sunspot data. Compute the spectrum of the simulated realization and compare it with the spectrum of the data. Does the fitted model capture the correlation structure of the data?   
15.6 Draw the lagged regression plots for the square-root-transformed hare data. Is there any evidence that the hare data are nonlinear? (The data file is named hare.)   
15.7 Carry out formal tests (Keenan’s test, Tsay’s test, and threshold likelihood ratio test) for nonlinearity for the hare data. Is the hare abundance process nonlinear? Explain your answer. (The data file is named hare.)   
15.8 Assuming that the hare data are nonlinear, fit a TAR model to the hare data and examine the goodness of fit. (The data file is named hare.)

15.9 This exercise assumes that the reader is familiar with Markov chain theory. Consider a simple TAR model that is piecewise constant:

$$
Y _ {t} = \left\{ \begin{array}{l l} \phi_ {1, 0} + \sigma_ {1} e _ {t}, & \text {i f} Y _ {t - 1} \leq r \\ \phi_ {2, 0} + \sigma_ {2} e _ {t}, & \text {i f} Y _ {t - 1} > r \end{array} \right.
$$

where $\{ \boldsymbol { e } _ { t } \}$ are independent standard normal random variables. Let $R _ { t } = 1$ if $Y _ { t } \leq r$ and 2 otherwise, which is a Markov chain.

(a) Find the transition probability matrix of $R _ { t }$ and its stationary distribution.   
(b) Derive the stationary distribution of $\{ Y _ { t } \}$   
(c) Find the lag 1 autocovariance of the TAR process.

# Appendix L: The Generalized Portmanteau Test for TAR

The basis of the portmanteau test is the result that, if the TAR model is correctly specified, $\hat { \rho } _ { 1 } , \hat { \rho } _ { 2 } , . . . , \hat { \rho } _ { m }$ $\bar { \boldsymbol \rho } _ { m }$ are approximately jointly normally distributed with zero mean and covariances $C o \nu ( \hat { \rho } _ { i } , \hat { \rho } _ { j } ) = q _ { i j }$ , where $Q$ is an $m { \times } m$ matrix whose $( i , j )$ element equals $q _ { i j }$ and whose formula is given below; See Chan (2008) for a proof of this result. It can be shown that $Q = I - U \bar { V } ^ { ^ { - 1 } U ^ { T } }$ where $I$ is an $m { \times } m$ identity matrix,

$$
U = E \left\{\left[ \begin{array}{c} e _ {t - 1} \\ e _ {t - 2} \\ \vdots \\ e _ {t - m} \end{array} \right] \begin{array}{l} [ I _ {t}, Y _ {t - 1} I _ {t}, \dots , Y _ {t - p} I _ {t}, (1 - I _ {t}), Y _ {t - 1} (1 - I _ {t}), \dots , Y _ {t - p _ {2}} (1 - I _ {t}) ] \\ \vdots \end{array} \right\}
$$

where $I _ { t } = I ( Y _ { t - d } \leq r )$ , the expectation of a matrix is taken elementwise, and

$$
V = E \left\{\left[ \begin{array}{c} I _ {t} \\ Y _ {t - 1} I _ {t} \\ \vdots \\ Y _ {t - p _ {1}} I _ {t} \\ (1 - I _ {t}) \\ Y _ {t - 1} (1 - I _ {t}) \\ \vdots \\ Y _ {t - p _ {2}} (1 - I _ {t}) \end{array} \right] \begin{array}{l} [ I _ {t}, Y _ {t - 1} I _ {t}, \dots , Y _ {t - p} I _ {t}, (1 - I _ {t}), Y _ {t - 1} (1 - I _ {t}), \dots , Y _ {t - p _ {2}} (1 - I _ {t}) ] \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdot \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdots \\ \vdash_ {\mathcal {I}} (\mathcal {I} - I) & [ I _ {t}, Y _ {t - 1} I _ {t}, \dots , Y _ {t - p} I _ {t}, (1 - I _ {t}), Y _ {t - 1} (1 - I _ {t}), \dots , Y _ {t - p _ {2}} (1 - I _ {t}) ] \\ & [ I _ {t}, Y _ {t - 1} I _ {t}, \dots , Y _ {t - p} I _ {t}, (1 - I _ {t}), Y _ {t - 1} (1 - I _ {t}), \dots , Y _ {t - p _ {2}} (1 - I _ {t}) ] & [ I _ {t}, Y _ {t - 1} I _ {t}, \dots , Y _ {t - p} I _ {t}, (1 - I _ {t}), Y _ {t - 1} (1 - I _ {t}), \dots , Y _ {t - p _ {2}} (1 - I _ {t}) ] & [ I _ {t}, Y _ {t - 1} I _ {1}, Y _ {t - p} I _ {p}, (1 - I _ {p}), Y _ {t - 1} (1 - I _ {p}), \dots , Y _ {t - p _ {2}} (1 - I _ {p}) ] & [ I _ {t}, Y _ {t - 1} I _ {1}, Y _ {t - p} I _ {p}, (1 - I _ {p}), Y _ {t - 1} (1 - I _ {p}), \dots , Y _ {t - p _ {2}} (1 - I _ {p}) ] & [ I _ {t}, Y _ {t - 1} I _ {1} + Y _ {t - p} I _ {p}, (1 - Y _ {p}), Y _ {t - 1} (1 - Y _ {p}), \dots , Y _ {t - p _ {2}} (1 - Y _ {p}) ] & [ I _ {t}, Y _ {t - 1} I, Y _ {t - p}, (1 - Y), Y, Y _ {t - p}, (1 - Y), Y, Y _ {t - p}, (1 - Y) ] & [ I _ {t}, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y ] & [ I _ {t}, Y, Y, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, X, x ] & [ I _ {t}, Y, Y, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z,
Y = E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ {- 1} E ^ -
$$

These expectations can be approximated by sample averages computed with the true errors replaced by the standardized residuals and the unknown parameters by their estimates. For example, $E \{ e _ { t - 1 } I ( Y _ { t - d } \leq r ) \}$ can be approximated by

$$
\frac {1}{n} \sum_ {t = 1} ^ {n} \hat {e} _ {t - 1} I (Y _ {t - \hat {d}} \leq \hat {r})
$$

where the initial standardized residuals $\hat { \boldsymbol e } _ { t } = 0$ for $t \leq \operatorname* { m a x } ( p _ { 1 } , p _ { 2 } , \hat { d } )$ .

# APPENDIX: AN INTRODUCTION TO R

# Introduction

All of the plots and numerical output displayed in this book were produced with the R software, which is available at no cost from the R Project for Statistical Computing. The software is available under the terms of the Free Software Foundation's GNU General Public License in source code form. It runs on a wide variety of operating systems, including Windows, Mac OS, UNIX, and similar systems, including FreeBSD and Linux. R is a language and environment for statistical computing and graphics, provides a wide variety of statistical methods (time series analysis, linear and nonlinear modeling, classical statistical tests, and so forth) and graphical techniques, and is highly extensible. In particular, one of the authors (KSC) has produced a large number of new or enhanced R functions specifically tailored to the methods described in this book. They are available for download in an R package named TSA on the R Project Website at www.r-project.org. The TSA functions are listed on page 468.

Important references for learning much more about R are also available at the R-Project Website, including An Introduction to R: Notes on R, a Programming Environment for Data Analysis and Graphics. Version 2.4.1 (2006-12-18), by W. N. Venables, D. M. Smith, and the R Development Core Team, (2006), and R: A Language and Environment for Statistical Computing Reference Index, Version 2.4.1 (2006-12-18), by The R Development Core Team (2006a).

The R software is the GNU implementation of the famed S language. It has been under active development by the R team, with contributions from many statisticians all over the world. R has become a versatile and powerful platform for doing statistical analysis. We shall confine our discussion to the Windows version of R. To obtain the software, visit the Website at www.r-project.org. Click on CRAN on the left-side of the screen under Download. Scroll down the list of CRAN Mirror sites and click on one of them nearest to you geographically. Click on the link for Windows (or Linux or MacOS X as appropriate) and click on the link named base. Finally, click on the link labeled R-2.6.1-win32.exe. (This file indicates release 2.6.1, the latest available release as of this writing. Newer versions come out frequently.) Save the file somewhere convenient, for example, on your desktop. When the download finishes, double-click the program icon and proceed with installing the software. (The discussion that follows assumes that you accept all of the defaults during installation.) At the end of this appendix, on page 468, you will find a listing and brief description of all the new or enhanced functions that are contained in the TSA package.

Before you start the R software for the first time, you should create a folder or directory, say Rwork, to hold data files that you will use with R for this project or course. This will be the working directory whenever you use R for this particular project or course. This directory is to contain the workspace, a file that contains all the objects (variables and functions) created in an R session. You should create separate

working directories for different projects or different courses.† After R is successfully installed on your computer, there will be an R shortcut icon on your desktop. If you have created your working directory, start R by clicking the R icon (shown at the right). When the software has loaded, you will have

![](images/3a120272a5a8c64235ae71c3391fdcbf8734afa22cb73bc7eeba40628739b9c0.jpg)

a console window similar to the one shown in Exhibit 1 with a bottom line that reads $>$ followed by a large rectangular cursor (probably in red). This is the R prompt. You may enter commands at this prompt, and they will be carried out when you press the Enter key. Several tasks are available through the menus.

The first task is to save your workspace in the working directory you created. To do so, select the File menu and then click on the choice Save workspace… .‡ You now may either browse to the directory Rwork that you created (which may take many steps) or type in the full path name; for ex a m p l e “ C : \ D o c u m e n t s a n d S e t t i n g s \ J o e S t u d e n t \ My Documents\Course156\Rwork”. If your working directory is on a USB flash drive designated as drive E, you might simply enter “E:Rwork”. Click OK, and from this point on in this session, R will use the folder Rwork as its working directory.

You exit R by selecting Exit on the File menu. Every time you exit R, you will receive a message as to whether or not to Save the workspace image. Click Yes to save the workspace, and it will be saved in your current working directory. The next time you want to resume work on that same project, simply navigate to that working directory and

locate the R icon there attached to the file named .RData. If you double-click this icon, R will start with this directory already selected as the working directory and you can get right to work on that project. Furthermore, you will receive the message [Previously saved workspace restored].

Exhibit 1 shows a possible screen display after you have started R, produced two different graphs, and worked with R commands in a script window using the R editor. Numerical results in R are displayed in the console window. Commands may be entered (keyed) in either the console window and executed immediately or (better) in a script window (the R editor) and then submitted to be run in R. The Menu bar and buttons will change depending on which window is currently the “focus.”

# File

Source R code...

New script

Open script...

Display fle(s)..

Load Workspace...

Save Workspace...

Load History...

Save History...

Change dir...

Print..

Save to File...

Exit

![](images/7b1b5e13783ce64f4cd70de70f423d17fc85495e894c408725d6180633ef4495.jpg)

![](images/c84f17e5737a3b32fa6cabe03b7ffc49061d4360a9d18f404f947fcdf1ef3f96.jpg)  
Exhibit 1 Windows Graphical User Interface for the R Software

# Packages

Load package...

Set CRAN mirror...

Select repositories...

Install package(s)..

Updatepackages...

Install package(s)fromlocal zip files..

# Packages

Load package...

Set CRAN mirror...

Selectrepositories...

Installpackage(s)..

Update packages...

Installpackage(s)from local zip fles..

A particularly useful feature of R is its ease of including supplementary tools in the form of libraries or packages. For example, all the datasets and the new or enhanced R functions used in this book are collected into a package called TSA that can be downloaded and installed in R. This can be done by clicking the Packages menu and then selecting Set CRAN mirror. Again select a mirror site that is closest to you geographically, and a window containing the names of all available packages will pop up.

In addition to our TSA package, you will need to install packages named leaps, locfit, MASS, mgcv, tseries, and uroot. Click the Packages menu once more, click Install package(s), and scroll through the window. Hold down the Ctrl key and click on each of these seven package names. When you have all seven selected, click OK, and they will be installed on your system by R. You only have to install them

once (but, of course, they may be updated in the future and some of them may be incorporated into the core of R and not need to be installed separately).

We will go over commands selected from the various chapters as a tutorial for R, but before delving into those, we first present an overview of R. R is an object-oriented language. The two main objects in R are data and functions. R admits many data structures. The simplest data structure is a vector that contains raw data. To create a data vector named Dat containing, say, 31, 4, 15, and 93, after the $>$ prompt in the console window, enter the following command

and then press the Enter key. The equal sign symbol signifies assigning the object on its right-hand side to the object on its left-hand side. The expression c(31,4,15,93) stands for concatenating the numbers within the parentheses to make a vector. So, the command creates an object named Dat that is a vector containing the numbers 31, 4, 15, and 93. R is case-sensitive, so the objects named Dat and DAt are different. To reveal the contents of an object, simply type the name of the object and press the Enter key. So, typing Dat in the R console window (and pressing the Enter key) will display the contents of Dat. If you subsequently enter DAt at the R prompt, it will complain by returning an error message saying that object "DAt" is not found. The name of an object is a string of characters that may contain letters, numerals, and the period sign, but the leading character is required to be a letter.† For example, Abc123.a is a valid name for an R object but $\texttt { 1 2 a }$ is not. R has some useful built-in objects, for example pi, which contains the numerical value of $\pi$ required for trigonometric operations such as computing the area of a circle.

For us, the most useful data structure is a time series. A time series is a vector with additional information on the epoch of the first datum and the number of data per a basic unit of time interval. For example, suppose we have quarterly data starting from the second quarter of 2006: 12, 31, 22, 24, 30. This time series can be created as follows:

Dat ${ \bf \Psi } = { \bf C }$ (31,4,15,93)   
> Dat2=ts(c(12,31,22,24,30), start ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (2006,2), frequency $= 4$ ) Its content can be verified by the command   
> Dat2=ts(scan('file1'), start ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (2006,2), frequency $= 4$ )   
```txt
>Dat2 Qtr1 Qtr2 Qtr3 Qtr4 2006 12 31 22 2007 24 30 
```

Larger datasets already in a data file (raw data separated by spaces, tabs, or line breaks) can be loaded into R by the command

where it is assumed that the data are contained in the file named file1 in the same directory where you start up R (or the one changed into via the change dir command). Notice that the file name, file1, is surrounded by single quotes ('). In R, all

character variables must be so enclosed. You may, however, use either single quotes or double quotes (") as long as you use them in pairs.

Datasets with several variables may be read into R by the read.table function. The data must be stored in a table form: The first row contains the variable names, and starting from the second line, the data are stored so that data from each case make up a row in the order of the variable names. The relevant command is

where file2 is the name of the file containing the data. The argument header $= \mathrm { T }$ specifies that the variable names are in the first line of the file. For example, let the contents of a file named file2 in your working directory be as follows:

Dat3=read.table('file2',header $\mathbf { \Phi } = \mathbb { T }$ )   
Y X   
1 2   
3 7   
4 8   
5 9   
>Dat3 $\equiv$ read.table('file2',header $\equiv$ T)   
>Dat3 Y X   
1 1 2   
2 3 7   
3 4 8   
4 5 9

Note that in displaying Dat3, R adds the row labels, defaulted to be from 1 to the number of data cases. The output of read.table is a data.frame, which is a data structure for a table of data. More discussion on data.frame can be found below. Presently, it suffices to remember that the variables inside a data.frame are not accessible. Think of Dat3 as a closed suitcase. It has to be opened before its variables are accessible in an R session. The command to “open” a data.frame is to attach it:

$>\mathrm{Y}$ Error:object"Y"not found   
>attach(Dat3) $\begin{array}{rl} & \mathrm{Y}\\ & [1] \end{array}$ 1345 $\mathrm{X}$ [1]2789

R can also read in data from an Excel file saved in the csv (comma-separated values) format, with the first row containing the variable names. Suppose file2.csv contains a spreadsheet containing the same information as in file2. The commands for reading in the data from file2.csv are similar to the one for a text file.

>Dat4 $\equiv$ read.csv('file2.csv',header $\equiv$ T)   
>Dat4 X   
1 1 2   
2 3 7   
3 4 8   
4 5 9

The functions scan, read.table, and read.csv have many other useful options. Use R Help to learn more about them. For example, run the command ?read.table, and a window showing detailed information for the read.table command will open. Remember that prefacing the question mark to any function name will display the function's details in a new Help window.

Functions in R are similar to functions in the programming language C. A function is invoked by typing its name followed by a list of arguments enclosed by parentheses. For example, the concatenate function has the name “c” and its purpose is to create a vector obtained by concatenating the arguments supplied to the function.

```txt
> c(12,31,22,24,30) 
```

Note that there can be no space between the left parenthesis and the function name. Even if the argument list is empty, the parentheses must be included in invoking a function. Try the command

```txt
> C 
```

R now sees the name of an object and will simply display its contents by printing the entire set of commands making up the function in the console window. R has many useful built-in functions, including abs, log, log10, exp, sin, cos, sqrt, and so forth, that are useful for manipulating data. (The function abs computes the absolute value; log does the log-transformation with base $e$ , while log10 uses base 10; exp is the exponentiation function, sin and cos are the trigonometric functions; and sqrt computes the square root.) These functions are applied to a vector or a time series element by element. For example, log(Dat2) log-transforms each element of the time series Dat2 and transfers the time series structure to the transformed data.

```txt
> Dat2=ts(c(12,31,22,24,30), start=c(2006,2), frequency=4)  
> log(Dat2) 
```

```txt
Qtr1 Qtr2 Qtr3 Qtr4   
2006 2.484907 3.433987 3.091042   
2007 3.178054 3.401197 
```

Furthermore, vectors and time series can be manipulated algebraically with the usual addition $( + )$ , subtraction (-), multiplication $( \star )$ , division $( / )$ , or power (^ or **) carried out element by element. For example, applying the transformation $y = 2 x ^ { \wedge } 3 - x + 7$ to Dat2 and saving the transformed data to a new time series named new.Dat2 can be easily carried out by the command

```txt
new.Dat2= 2\*Dat2^3-Dat2+7 
```

# Chapter 1 R Commands

# File

Source R code...

New script

Open script...

Display file(s)...

Load Workspace...

Save Workspace...

Load History...

Save History..

Change dir...

Print...

Save to File...

Exit

Now, we are ready to check out selected R commands used in Chapter 1 of the book. Script files of the commands used in each of the fifteen chapters are available for download at www.stat.uiowa.edu/~kchan/TSA.htm. The script files contain the R commands needed to carry out the analyses shown in the chapters. They also contain a limited amount of additional explanation. Download the scripts and save them in your working directory. You may then open them within R in an R editor (script) window and you will save much typing! Once they are downloaded, script files may be opened by either clicking the open file button or by using the file menu shown at the left.

# Exhibit 2 A Script Window with Chapter 1 Scripts Displayed

![](images/c5fbf24dc13aed6493de0e862c030456ba1fb505055db6820c60e6d1aa336705.jpg)

<table><tr><td>Run line or selection</td><td>Ctrl+R</td></tr><tr><td>Undo</td><td>Ctrl+Z</td></tr><tr><td>Cut</td><td>Ctrl+X</td></tr><tr><td>Copy</td><td>Ctrl+C</td></tr><tr><td>Paste</td><td>Ctrl+V</td></tr><tr><td>Delete</td><td></td></tr><tr><td>Select all</td><td>Ctrl+A</td></tr></table>

Exhibit 2 shows a portion of the script file for Chapter 1 in a script window. The first four commands have been highlighted by dragging the mouse pointer across them. They can now all be executed by either pressing Control-R (Ctrl-R) or by right-clicking the highlighted group and choosing Run from the choices displayed, as shown at the left. If the cursor is in a single command line with no highlighting, that one command may be executed similarly.

At the beginning of each session with R, you need to load the TSA library. The following command will accomplish this (but you may wish to investigate the .First function that can automate some startup tasks).

# library(TSA)

The TSA package contains all datasets and functions needed for repeating the analyses and doing the exercises.

# Exhibit 1.1 on page 2. win.graph(width $_ { - } = 4$ .875,height $^ { . = 2 }$ .5,pointsize=8)

Comments may be interspersed in the R codes to improve their readability. The # sign in a R command signifies that what follows the sign are comments, and hence ignored by R. The first R command opening with the # sign is therefore a comment. The second R command opens a window for graphics that is 4.875 inches wide and 2.5 inches tall with characters printed with point size 8. The chosen setting and similar settings produce time sequence plots that are appropriate for inclusion in the book. Other settings will be appropriate for other purposes. For example, quantile-quantile plots are best viewed with a 1:1 aspect ratio (height $=$ width). For exploratory data analysis, you will want larger graphics windows to use the full resolution of your computer screen to see more detail. The command win.graph can be safely omitted altogether. If there is currently no open graphics window, R will open a graphics window whenever a graphics command is issued. You can resize this window in the usual ways by dragging edges or corners.

# data(larain)

This loads the time series larain into the R session and makes it available for further analysis such as

plot(larain,ylab='Inches',xlab='Year',type='o')

Plot is a function. It draws the time sequence plot for larain. The argument ylab $i = 1$ Inches' specifies “Inches” as the label for the $y$ -axis. Similarly, the label for the $x$ -axis is “Year.” The argument type indicates how the data are displayed in the plot. For $\mathsf { t y p e } = \mathsf { \Omega } ^ { \prime } \circ \mathsf { \Omega } ^ { \prime }$ , the individual data points are overplotted on the curve; $\tt t y p e = " b "$ (for both) is another option that superimposes the data points on the curve, but with the curve broken around the data points. For $\mathrm { \ t y p e = " 1 " }$ , only the line segments connecting the points are shown. (Note: This character (l) is an “el,” not a one.) To show only the data points, supply the argument $\tt t y p e = " p "$ . To learn more about the plot function and the full options for the type argument, run the command

# ?plot

A Help window on the plot function will then pop up for your browsing. Try it now. What will be plotted if the option $\operatorname { t y p e } = { ^ { \prime } } \ln { ^ { \prime } }$ is used instead of $\mathsf { t y p e } = \mathsf { " o } ^ { \prime } \hat { ! }$ All graphs may be saved $( { \mathrm { F i l e } } > { \mathrm { S a v e ~ a s } } > \ldots )$ in any of several graphics formats: jpeg, pdf, etc. Saved graphs may then be imported into most word-processing programs to create high-quality reports.

# Exhibit 1.2 on page 2. win.graph(width $\hphantom { 0 } . = 3$ ,height $= 3$ ,pointsize $^ { \circ 8 }$ ) plot( $\mathrm { y } = \mathrm { 1 }$ arain,x=zlag(larain),ylab $^ { 1 = }$ 'Inches',

```javascript
xlab='Previous Year Inches') 
```

The plot function is a multipurpose function. It can do many different kinds of plots, depending on the set of arguments passed to it and their attributes. Here, it draws the scatter diagram of larain against its lag 1 values through the arguments $\mathtt { y } = \mathtt { l }$ arain (that is, larain on the $y$ -axis) and $\mathtt { x } = \mathtt { z } \mathtt { l a g }$ (larain) (that is, the lag 1 of larain is on the $x$ -axis). Note that zlag is a function in the TSA package. Run the command $\mathfrak { Q } \circ \mathrm { 1 } \circ \mathfrak { g }$ to learn what you can do with it.

```txt
Exhibit 1.3 on page 3. data(color) 
```

plot(color,ylab $=$ 'ColorProperty',xlabel $=$ 'Batch',type $=$ 'o')

Here we have supplied four arguments to the plot function to draw the time sequence plot of the time series color. The first argument is simply color, but the other supplied arguments are of the form name of the argument $=$ argument value so the first supplied argument is an unnamed argument, while the other arguments are named arguments. You may wonder how an unnamed argument is interpreted by R. To understand this, use the ?plot command to check that the argument list of the plot function is x, y, and … . You may guess that the x argument represents the $x$ -variable, and the y argument for the y-variable in a plot. The ellipsis (…) argument stands for all other allowable arguments, which must, however, be specified with the name of the argument. (Again, consult the pages of the plot function to figure out which other arguments besides x and y may be passed to plot.) Any unnamed argument is interpreted to be the value for the argument whose order matches that of the unnamed argument supplied to the function. For example, color appears as the first argument supplied to the plot function, so R interprets it as the value for the x argument. Now there is no value supplied to the y argument. In this case, plot will examine the nature of the $x$ -variable to determine what actions to be taken. Since color is a time series, plot draws a time sequence plot of color. To reinforce understanding, now try the following command in which color appears twice in the argument list, as the first and second arguments.

```txt
plot(color, color, ylab='Color Property', xlab='Batch', type='o') 
```

Guess what will be drawn by R? Now, color is interpreted as the $x$ -variable and also the $y$ -variable; hence a 45 degree line is drawn. However, the line seems to be of nonuniform thickness. (Can you see this?) Why? It is because seeing that the variables are time series, plot draws the line by connecting data points in the order they are recorded, with the order of the data points marked in the plot. This feature can be useful in some analyses but in this case this feature is distracting. A remedy is to strip the time series attribute from the $x$ -variables before plotting. (Plot takes the clue of how to do the plot from the attribute of the $x$ -variable.) To temporarily turn color into a raw data vector, use the command

```lua
as.vector(color) 
```

```txt
Now, try the command 
```

```javascript
plot(as.vector(color), color, ylab='Color Property', xlab='Batch', type='o') 
```

Exhibit 1.4 on page 4. plot(y $=$ color,x $\equiv$ zlag(color),ylab $=$ 'Color Property', xlab $=$ 'Previous Batch Color Property')

The zlag function outputs an ordinary vector; that is, zlag(color) is the lag 1 of color, but with its time series attribute stripped.

Exhibit 1.9 on page 7. plot(oilfilters,type $= \text{I}^{\prime}$ ,ylab $=$ 'Sales')

Plot is a high-level graphics function and, as such, it will replace what is currently in the graphics window or create a new graphics window if none exists. Recall that the argument $\mathrm { \ t y p e = " 1 " }$ instructs $\mathtt { p l o t }$ to just draw the line segments connecting the individual time series points.

```txt
Month=c('J', 'A', 'S', 'O', 'N', 'D', 'J', 'F', 'M', 'A', 'M', 'J') creates a vector named Month that contains 12 elements that represent the 12 months of the year beginning with July. 
```

```txt
points(oilfilters,pch=Month) 
```

Points is a low-level graphics function that draws on top of an existing graph. Since oilfilters is a time series, points plots oilfilters against time order, but the argument pch $\cdot ^ { = }$ Month instructs the points function to plot the data points using the successive values of the Month vector as plotting symbols. So, the first point plotted is plotted as a J, the second as an A, and so forth. When the values of Month are used up, they are recycled; think of Month being replicated as Month, Month, Month,…, to make up any deficiency. So, the 13th data point is plotted as a J and the 14th as an A. What letter is used for the 30th data point?

Alternatively, the exhibit can be reproduced by the following commands

plot(oilfilters,type $= 1$ ylab $=$ 'Sales') points(y $\equiv$ oilfilters,x $\equiv$ time(oilfilters), pch $\equiv$ as.vector(season(oilfilters)))

The time function outputs the epochs when the time series values were collected. The season function returns the month of the data in oilfilters; season is a smart function, as it returns the quarter of the data for quarterly data and so forth. The pch argument expects a vector as its value, but the output of the season function has been designed to be a factor object; hence the application of the as.vector function to season(oilfilters) strips its factor attribute. (See more about factor objects on page 435.)

A good way to appreciate the natural variation in a stochastic process is draw realizations from the process and plot them in a time sequence plot. For example, the independent and identically normally distributed process is often used as a data generating mechanism for completely random data; that is, data with no temporal structure. In other words, such data constitute a random sample from a normal distribution that are drawn sequentially over time. Simulating data from such a process and viewing their time sequence plots is a valuable exercise that can train our eyes to differentiate whether a time series is random or dependent over time, c.f. Exercise 1.3. The R command for simulating and storing in a variable named $y$ a random sample of size, say $n = 4 8$ , from

a standard normal distribution is $\mathtt { y } =$ rnorm(48)

The data can then be plotted using the command

plot(y, $\tt t y p e = " p "$ , ylab='IID Normal Data')

Try the $\mathsf { t y p e } = \mathsf { " o } ^ { \prime }$ option in the above command. Which plotting option do you find better to see the randomness in the data? Notice that executing the command y=rnorm(48) again will yield a different time series realization of the random process. The set.seed command discussed below addresses the issue of how to make simulations in R “reproducible.”

Data can be simulated from other distributions. For example, the command rt( $\mathtt { \Gamma } _ { \mathtt { n } = 4 8 }$ , $\mathtt { d f } = 5$ ) simulates 48 independent observations from a $t$ -distribution with 5 degrees of freedom. Similarly, rchisq( $\scriptstyle \mathtt { n } = 4 8$ , $\mathtt { d f } = 2$ ) simulates a realization of size 48 from the chi-square distribution with 2 degrees of freedom.

# Chapter 2 R Commands

We show some R code to simulate your own random walk with, say, 60 independent standard normal errors.

# Exhibit 2.1 on page 14. $\mathtt { n } = 6 0$

This assigns the value of 60 to the object named n.

set.seed(12345)

This initializes the random number generator so that the simulation is reproducible if needed.

sim.random.walk $. =$ ts(cumsum(rnorm(n)),freq $^ { \cdot = 1 }$ ,start $^ { = 1 }$ )

The expression rnorm(n) generates n independent values from the standard normal distribution. The function cumsum then computes the vector of cumulative sums of the normally distributed sample, resulting in a random walk realization. The random walk realization is then given the attribute of a time series and saved into the object named sim.random.walk.

plot(sim.random.walk,type='o',ylab $^ { 1 = }$ 'Another Random Walk') plots the simulated random walk.

# Chapter 3 R Commands

We now move to discuss some of the R commands appearing in Chapter 3.

# Exhibit 3.1 on page 31. data(rwalk)

This command loads the time series rwalk, which is a random walk realization.

model1=lm(rwalk~time(rwalk))

The function lm fits a linear model (a regression model) with its first argument being a formula. A formula is an expression including a tilde sign (~), the left-hand side of which is the response variable and the right-hand side are the covariates or explanatory variables (separated by plus signs if there are two or more covariates). By default, the intercept term is included in the model. The intercept can be removed by including the term $^ { 6 6 } - 1 ^ { , 9 }$ on the right-hand side of the tilde sign. Recall that time(rwalk) yields a time series of the time epochs at which the random walk was sampled. So the command lm(rwalk~time(rwalk)) fits a time trend regression model to the rwalk series. The model fit is saved as the object named model1.

```txt
summary(model1) 
```

The function summary prints out a summary of the fitted model passed to it. Hence the command above prints out the fitted time trend regression model for rwalk.

```txt
Exhibit 3.2 on page 31. plot(rwalk,type='o',ylab='y') abline(model1) 
```

The function abline is a low-level graphics function. If a fitted simple regression model is passed to it, it adds the fitted straight line to an existing graph. Any straight line of the form $y = \beta _ { 0 } + \beta _ { 1 } x$ can be superimposed on the graph by running the command

```txt
abline(a=beta0,b=beta1) 
```

For example, the following command adds a 45 degree line on the current graph.

abline $(a = 0,b = 1)$

Recall the lm function can fit multiple regression models, with the covariates or explanatory variables specified one by one, on the right side of the tilde sign (~) in the formula. The covariates must be separated with a plus sign $( + )$ . Suppose we want to fit a quadratic time trend model to the rwalk series. We need to create a new covariate that contains the square of the time indices. The quadratic variable may be created before invoking the lm function. Or it may be created on the fly when invoking the lm function. The latter approach is illustrated here.

```javascript
model1a=lm(rwalk~time(rwalk)+I(time(rwalk)^2)) 
```

Notice that the expression time(rwalk) $\sim _ { 2 }$ is enclosed within the I function which instructs R to create a new variable by executing the command passed into the I function. The fitted quadratic trend model can be inspected with the summary function.

```txt
> summary(model1a)  
Call:  
lm(formula = rwalk ~ time(rwalk) + I(time(rwalk)^2))  
Residuals:  
Min 1Q Median 3Q Max -2.696232 -0.768018 0.008256 0.853365 2.344685  
Coefficients: 
```

Estimate Std. Error t value $\mathsf{Pr}(|t|)$ (Intercept) -1.4272911 0.4534893 -3.147 0.00262 \*\* time(rwalk) 0.1746746 0.0343028 5.092 4.16e-06 \*\*\* I(time(rwalk)^2) -0.0006654 0.0005451 -1.221 0.22721

Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Residual standard error: 1.132 on 57 degrees of freedom Multiple R-Squared: 0.8167, Adjusted R-squared: 0.8102 F-statistic: 127 on 2 and 57 DF, p-value: < 2.2e-16

The summary function repeats the function call to the lm function. It then prints out the five-number numerical summary of the residuals, followed by a table of the parameter estimates with their standard errors, $t$ -values and $p$ -values. All significant covariates are marked with asterisks $( ^ { * } )$ ; more asterisks means higher significance, that is, smaller $p$ -value, as explained in the line labeled as Signif. codes. Finally, it outputs the residual standard error, that is, the noise standard deviation estimate, and the multiple R-squared of the fitted model. Clearly, the quadratic term is not significant so that it is not needed, as is also obvious from the time plot of the series.

The reader may wonder why the I function is needed. This is because without the I function, R interprets the term time(rwalk)+time(rwalk) $\sim _ { 2 }$ using the formula convention (run ?formula to learn more about the formula convention), which results in fitting the linear trend model! Refit the quadratic trend model but now omit the I function in the R command, and compare the model fit with those of the linear and quadratic trend models.

# Exhibit 3.3 on page 32. data(tempdub)

This loads the tempdub series. You can learn more about the dataset tempdub by running the command ?tempdub.

month. $=$ season(tempdub)

The expression season(tempdub) outputs the monthly index of tempdub as a factor, and saves it into the object month.. The first period sign (.) is part of the name (month.) and is included to make the printout from later commands more clear.

We now digress to explain what a factor is. A factor is a kind of data structure for handling qualitative (nominal) data that do not have a natural ordering like numbers do. However, for purposes of summary and graphics, the user may supply the levels argument to indicate an ordering among the factor values. For example, the following command creates a factor containing the qualitative variable sex, with the default ordering using the dictionary order.

> sex $: =$ factor(c('M','F','M','M','F')) > sex [1] M F M M F Levels: F M

We can change the ordering as follows:

> sex $: =$ factor(c('M','F','M','M','F'),levels=c('M','F')) > sex [1] M F M M F Levels: M F

Note the swap of F and M in the levels. The function table counts the frequencies of the two sexes.

```txt
> table(sex)  
sex M F  
3 2
```

The printout lists the frequencies of the values according to the order supplied in the level argument. Now, we return to the R scripts in Chapter 3.

```txt
model2=1m(tempdb~month.-1) 
```

Recall that month is a factor containing the month of the data. When a formula contains a factor covariate, the function lm replaces the factor variable by a set of indicator variables corresponding to each distinct level (value) of the factor. Here, month. has 12 distinct levels: Jan, Feb,…, and so forth. So, in place of month., lm creates 12 monthly indicator variables and replaces month. by the 12 indicator variables. Because these 12 indicator variables are linearly dependent (they add up to a vector of all ones), the intercept term has to be removed to avoid multicollinearity. The expression $^ { 6 6 } - 1 ^ { , 5 }$ in the formula takes care of this. The fitted model corresponds to fitting a mean separately for each month. If the expression $^ { 6 6 } - 1 ^ { , 9 }$ is omitted, lm deals with the multicollinearity by omitting the first indicator variable; that is, the indicator variable for January will be deleted. In such a fitted model, the intercept represents the overall January mean and the coefficients for other months are the deviations of their means from the January mean.

```txt
summary(model2) 
```

A summary of the fitted regression model is printed out with this command. Many variables derived from the fitted model can also be easily obtained. For example, the fitted values can be printed as

```lisp
fitted(model2) 
```

whereas residuals are obtained by using

```python
residuals(model2) #Exhibit 3.4 on page 33. model3=lm(tempdb~month.) #intercept is automatically included so one month (January) is dropped summary(model3) 
```

```txt
Exhibit 3.5 on page 35. 
```

har. $=$ harmonic(tempdup,1)

The first pair of harmonic functions (sine and cosine pairs) can be constructed by the harmonic function, which takes a time series as its first argument and the number of harmonic pairs as its second argument. Run ?harmonic to learn more about this function. The output of the harmonic function is a matrix that is saved into an object named har.. Again, the first period is part of the name and included to make the later printouts clearer.

```txt
model4=1m(tempdb~har.) 
```

```txt
summary(model4) 
```

We now briefly discuss the use of matrices in R. A matrix is a rectangular array of numbers. It can be created by the matrix function. Here is an example:

$>$ M=matrix(1:6,ncol=2) $>$ M  
[1,] [2]  
[2,] 4  
[3,] 5  
[3,] 6

The matrix function expects a vector as its first argument, and it uses the values in the supplied vector to fill up a matrix column by column. The column dimension of a matrix is specified by the ncol argument and the row dimension by the nrow argument. The expression 1:6 stands for the vector containing the integers from 1 to 6. So the matrix function creates a matrix consisting of two columns using the six numbers 1, 2, 3, 4, 5, and 6. Since the row dimension is missing, R assumes that the matrix has six elements and hence the missing row dimension is set to 2. The dimensions of a matrix can be extracted using the dim function.

```txt
> dim(M)  
[1] 3 2 
```

This displays the row and column dimensions of M as a vector. The function apply can process a matrix column by column, with each column operated by a supplied function. For example, the column means of M can be computed as follows:

```txt
>apply(M,2,mean) [1] 25 
```

The first argument of the apply function is the matrix on which it processes, and the second argument is MARGIN, which should be set to 1 for row processing or 2 for column processing. The third argument is FUN, which takes the user-specified function. The example above instructs R to process M column by column and apply the mean function to each column. How would you modify the preceding R command to compute the row sums of M?

```txt
Exhibit 3.6 on page 35. plot(ts(fitted(model4),freq=12,start=c(1964,1)), ylab='Temperature',type='l',ylim=range(c(fitted(model4),tempdub))) points(tempdub) 
```

The ylim option ensures that the y-axis has a range that includes both the raw data and the fitted values.

```javascript
Exhibit 3.8 on page 43. plot(y=rstudent(model3),x=as.vector(time(tempdb)), xlab='Time', ylab='Standardized Residuals', type='o') 
```

The expression rstudent(model3) returns the (externally) Studentized residuals from the fitted model. To compute the (internally) standardized residuals, use the command rstandard(model3).

```txt
Exhibit 3.11 on page 45. hist(rstudent(model3),xlab='Standardized Residuals') 
```

The function hist draws a histogram of the data passed to it as the first argument. Note that the default heading of the histogram says that the plot is a histogram of

rstudent(model3). While the default main label correctly depicts what is plotted, it is often desirable to have a less technical but more descriptive label; for example, setting the option main $. =$ 'Histogram of the Standardized Residuals'.

# Exhibit 3.12 on page 45. qqnorm(rstudent(model3))

The expression rstudent(model3) extracts the standardized residuals of model3. The qqnorm function then plots the Q-Q normal scores plot of the residuals. A reference straight line can be superimposed on the Q-Q normal score plot by running the command qqline(rstudent(model3)).

# Exhibit 3.13 on page 47. acf(rstudent(model3))

The acf function computes the sample autocorrelation function of the time series supplied to the function. The maximum number of lags is determined automatically based on the sample size. It can, however, be changed to, say, 30 by setting the option max. $\mathtt { l a g } = 3 0$ when calling the function.

The Shapiro-Wilk test and the runs test on the residuals can be carried out respectively by the following commands.

shapiro.test(rstudent(model3)) runs(rstudent(model3))

These commands compute the test statistics as well as their corresponding $p$ -values.

# Chapter 4 R Commands

# Exhibit 4.2 on page 59. data(ma1.2.s) plot(ma1.2.s,ylab $^ { 1 = }$ expression(Y[t]), $\mathsf { t y p e } = \mathsf { " o } ^ { \prime } \ \mathrm { ) }$ )

The software R can display mathematical symbols in a graph. The option ylab $^ { 1 = }$ expression(Y[t]) specifies that the $y$ label is Y with $t$ as its subscript, all in math font. Typesetting a formula does require some additional work. Read the help pages for legend (?legend) and run the command demo(mathplot) to learn more about this topic.

An MA(1) series with MA coefficient equal to $\theta _ { 1 } = - 0 . 9$ and of length $n = 1 0 0$ can be simulated by the following commands.

set.seed(12345)

This command initializes the seed of the random number generator so that a simulation can be reproduced if needed. Without this command, the random generator will initialize “randomly,” and there is no way to reproduce the simulation. The argument 12345 can be replaced by other numbers to obtain different random numbers.

y=arima.sim(model=list(ma=-c(-0.9)), $\mathtt { n = 1 0 0 }$ )

The arima.sim function simulates a time series from a given ARIMA model passed into the function as a list that contains the AR and MA parameters as vectors. The simulated model above is an MA(1) model, so there is no AR part in the model list. The soft-

ware R uses a plus convention in parameterizing the MA part, so we have to add a minus sign before the vector of MA values to agree with our parameterization. The sample size is determined by the value of the argument n. So, the command above instructs R to simulate a realization of size 100 from an MA(1) model with $\theta _ { 1 } = - 0 . 9$ .

We now digress to explain some pertinent facts about list. A list is the most flexible data structure in R. You may think of a list as a cabinet with many drawers (elements or components), each of which contains data with possibly different data structures. For example, an element of a list can be another list! The elements of a list are ordered according to the order they are entered. Also, elements can be named to facilitate their easy retrieval. A list can be created by the list function with elements supplied as its arguments. The elements may be passed into the list function in the form of name $=$ value, delimited by commas. Below is an example of a list containing three elements named a, b, and c, where a is a three-dimensional vector, b is a number, and c is a time series.

```txt
> list1=list(a=c(1,2,3),b=4,c=ts(c(5,6,7,8), start=c(2006,2),frequency=4))  
> list1  
$a  
[1] 1 2 3  
$b  
[1] 4  
$c Qtr1 Qtr2 Qtr3 Qtr4  
2006 5 6 7  
2007 8 
```

To retrieve an element of a list, run the command listname$elementname, for example

```txt
> list1$c  
Qtr1 Qtr2 Qtr3 Qtr4  
2006 5 6 7  
2007 8 
```

Data of irregular structure can be stored as a list. The output of a function is often a list. Simply entering the name of a list may result in dazzling output if the printed list is large. An alternative is to first explore the structure of a list by the function str (str stands for structure). An example follows.

```txt
> str(list1)
List of 3
$ a: num [1:3] 1 2 3
$ b: num 4
$ c: Time-Series [1:4] from 2006 to 2007: 5 6 7 8 
```

This shows that list1 has three elements and describes these elements briefly.

# Chapter 5 R Commands

Exhibit 5.4 on page 91. plot(diff(log(oil.price)),ylab='Change in Log(Price) ', type $= ^{\prime}1^{\prime}$

The function diff outputs the first difference of the supplied time series. Higher-order differences can be computed by supplying the differences argument. For example, the second difference of log(oil.price) can be computed by the command

diff(log(oil.price), differences $^ { \prime = 2 }$ )

A useful convention of R is that the name of an argument in a function can be abbreviated if it does not result in ambiguity. For example, the previous command can be shortened to

diff(log(oil.price),diff=2)

Note that the second argument of the diff function is the lag argument. By default, $\mathsf { I a g } = \mathsf { 1 }$ and the diff function computes regular differences—first or higher differences. Later, when we deal with seasonal time series data, it will sometimes be desirable to consider seasonal differences. For example, we may want to subtract this month’s number from the number of the same month one year ago; that is, the differences are computed with a lag of 12 months. This can be done by specifying $\log = 1 2$ . As an illustration, computing the seasonal differences of period 12 can be done by issuing the command diff(tempdub, $\mathtt { l a g } = 1 2$ ). What will be computed by the command diff(log(oil.price),2)? One of the authors (KSC) committed a serious error, more than once, when he tried to compute the second regular differences of some time series by running a similar command with unnamed arguments. Instead of the second regular differences, the first seasonal differences of lag 2 were actually computed by the command with unnamed arguments! Imagine his frustrations of many anxious hours, all because the data analysis from the flawed computations seriously conflicted with expectations based on theory! The moral is that passing unnamed arguments to a function is risky unless you know the positions of the relevant arguments very well. It is well to remember that unnamed arguments, if present, should appear together in the beginning part of the argument list, and there should be no unnamed argument after a named one. Indeed, mixed arguments (some named and some unnamed in a haphazard order) may result in erroneous interpretation by R. The order of the arguments in a function can be quickly checked by running the command args(function.name) or ?function.name, where function.name should be replaced by the name of the function you are checking.

# Exhibit 5.11 on page 102. library(MASS)

This loads the library MASS. Run the command library(help=MASS) to see the content of this library.

boxcox(lm(electricity~1))

The function boxcox computes the maximum likelihood estimate of the power transformation on the response variable to make a linear regression model appropriate for the data. The first argument is a fitted model by the lm function. By default, the boxcox function produces a plot of the log-likelihood function of the power parameter. The MLE of the power parameter is the value that maximizes the plotted likelihood curve. Here the model is that some power transform of electricity is given by the model of a constant mean plus normally distributed white noise. But we already know that elec-

tricity is serially correlated, so this method is not entirely correct, as the autocorrelation in the series is not accounted for.

For time series analysis, a more appropriate model is that some power transform of the time series variable follows an AR model. The function BoxCox.ar implements this approach. It has two drawbacks in that it is much more computer-intensive and that other covariates cannot be included in the model in the current version of the function. The first argument of BoxCox.ar is the name of the time series variable. The AR order may be supplied by the user through the order argument. If the AR order is missing, the function estimates the AR order by minimizing the AIC for the log-transformed data. Both boxcox and BoxCox.ar require the response variable to be positive.

BoxCox.ar(electricity)

This plots the log-likelihood function of the power parameter for the model that accounts for autocorrelation in the data.

# Chapter 6 R Commands

# Exhibit 6.9 on page 120. acf(ma2.s,ci.type $=$ 'ma',xaxp ${ \mathfrak { C } } \mathbf { C }$ (0,20,10))

The argument ci.type $=$ 'ma' instructs R to plot the sample ACF with the confidence band for the kth lag ACF computed based on the assumption of an $\mathrm { M A } ( k - 1 )$ model. See Equation (6.1.11) on page 112 for details.

# Exhibit 6.11 on page 121. pacf(ar1.s,xaxp ${ \mathfrak { C } } \mathbf { C }$ (0,20,10))

This calculates and plots the sample PACF function. Run the command ?par to learn more about the xaxp argument.

# Exhibit 6.17 on page 124. eacf(arma11.s)

This computes the sample EACF function (extended autocorrelation function) of the data arma11.s. The maximum AR and MA orders can be set via the ar.max and ma.max arguments. Their default values are seven and thirteen, respectively. For example, eacf(arma11.s,ar.max ${ \bf \chi } = 1 0$ ,ma. $\mathtt { m a x } { = } 1 0$ ) computes the EACF with maximum AR and MA orders of 10. The EACF function prints a table of symbols with X standing for a significant value and O a nonsignificant value.

library(uroot)

This loads the uroot library and the following commands illustrate the computation of the Dickey-Fuller unit-root test.

ar(diff(rwalk))

This command finds the AR order for the differenced series, which is order 8, by the minimum AIC criterion.

ADF.test(rwalk,selectlags ${ } _ { 1 } = { }$ list(mode ${ \mathfrak { z } } \mathbf { C }$ (1,2,3,4,5,6,7,8), Pmax $: = 8$ ),itsd $\boldsymbol { \mathbf { \mathit { \Psi } } } = \boldsymbol { \mathbf { \mathit { C } } }$ (1,0,0))

This computes the ADF test for the data rwalk. The selectlags argument takes a list as its value. The mode argument specifies which lags must be included, and if it is absent, then the Pmax argument sets the maximum lag and the ADF.test function determines which lags to include in the test using several methods by setting the mode to signf, aic, or bic. The option signf is the default value for mode, which estimates a subset AR model by retaining only significant lags. The argument itsd expects a vector; the first two elements are binary, indicating whether to include a constant term (if the first element is 1) or a linear time trend (if the second element is 1); and the third element zero if there are no more covariates to include in the model. See the help pages for the ADF.test function to learn more about it. Hence, the R command instructs ADF.test to carry out the test with the null hypothesis that the model has a unit root and an intercept term. The alternative is that the model is stationary, so a small $p$ -value implies stationarity!

ADF.test(rwalk,selectlags ${ } _ { 1 } = { }$ list(Pmax ${ \bf \omega } = 0$ ),itsd=c(1,0,0))

In comparison, the preceding command carries out the ADF test with the null hypothesis being that the model has a unit root, an intercept but no other lags, whereas the alternative specifies that the model is a stationary AR(1) model with an intercept. If itsd=c(0,0,0), then the alternative model is a centered stationary AR(1) model, that is, with zero mean. Such a hypothesis is not relevant unless the data are already mean-corrected.

```txt
Exhibit 6.22 on page 132.  
set.seed(92397)  
test=arima.sim(model=list(ar=c(rep(0,11),..8), ma=c(rep(0,11),0.7)),n=120) 
```

This simulates a subset ARMA model. Here rep(0,11) stands for a sequence of 11 zeros.

res=armasubsets(y $\equiv$ test,nar $= 14$ ,nma $= 14$ ,y.name $=$ 'test', ar.method $=$ 'ols')

The armasubsets function computes various subset ARMA models, with the maximum AR and MA orders specified by the nar and nma arguments, both set as 14 in the example above. The associated AR models are estimated by the default method of ols (ordinary least squares).

plot(res)

The plot function is a smart function. Seeing that res is the output from the armasubsets function, it draws a table indicating several of the best subset ARMA models.

# Chapter 7 R Commands

Below is a function that computes the method-of-moments estimator of the MA(1) coefficient of an MA(1) model. It is a simple example of an R function. Simply copy and

paste it into the R console. Press the enter key to compile the code, and the function estimate.ma1.mom will be created and then be available for use in your workspace. This function only exists in the particular workspace where it was created.

estimate.malmom $=$ function(x){r=acf(x,plot $\equiv$ F）$acf[1]；if(abs(r)<0.5)return((-1+sqrt(1-4\*r^2))/(2\*r))else return(NA)}

Readers uninterested in the specifics of R programming may skip down to the material on Exhibit 7.1. The syntax of an R function takes the form

function.name $=$ function(argument list){function body}

where function body is a set of R statements (commands). Normally, complete R commands are separated by line breaks. Alternatively, they may be separated by the semicolon symbol (;). If an R command is incomplete, R will assume that it is to be continued on the next line and so forth until R reads a complete command. So the function above has a single argument called x and contains two commands. The first one is

```txt
r=acf(x, plot=F) $acf[1]
```

which instructs R to compute the acf of x without plotting the values, extract the first element of the computed sample acf function (that is, the lag 1 autocorrelation) and then save it in an object called r. The object r is a local object; it only exists within the estimate.ma1.mom function environment. The second command is

```txt
if (abs(r) < 0.5)
return((-1 + sqrt(1 - 4*r^2)) / (2*r)) else return(NA) 
```

Note the line break after the if clause and the second half of the command. Since the if clause alone is incomplete, R assumes that it is to be continued on the next line. With the second line, R finds a complete R command and so concludes the two lines of commands together as a complete command. In other words, R sees the next command as equivalent to the following one line:

```txt
if (abs(r) < 0.5) return((-1+sqrt(1-4\*r^2))// (2\*r)) else return(NA) 
```

The function abs computes the absolute value of the argument passed to it, whereas sqrt is the function that computes the square root of its argument. Now, we are ready to interpret the second command: if the absolute value of r, the lag 1 autocorrelation of x, is less than 0.5 in magnitude, the function returns the number

$$
(- 1 + \operatorname {s q r t} (1 - 4 ^ {*} r ^ {\wedge} 2)) / (2 ^ {*} r)
$$

which is the method-of-moments estimator of the MA(1) coefficient $\theta _ { 1 }$ ; otherwise the function returns NA (see Equation (7.1.4) on page 150). The symbol NA is the code standing for a missing value in R. (NA stands for not available.) In this example, R is specifically instructed what value to return to the user. However, the default procedure is that a function returns the value created by the last command in the function body. R provides a powerful computer language for doing statistics. Please consult the documents on the R Website to learn more about R programming.

```txt
Exhibit 7.1 on page 152. data(mal.2.s) 
```

This loads a simulated MA(1) series.

estimate.ma1.mom(ma1.2.s)

This computes the MA(1) coefficient estimate by the method of moments using the user-created estime.ma1.mom function above!

data(ar1.s)

This loads a simulated AR(1) series from the TSA package.

ar(ar1.s,order.max $\ l = 1$ , $\mathbb { A } \mathbb { I } \mathbb { C } { = } \mathbb { F }$ ,method='yw')

This computes the AR coefficient estimates for the ar1.s series. The ar function estimates the AR model for the centered data (that is, mean-corrected data), so the intercept must be zero and not estimated or printed out in the output. The ar function requires the user to specify the maximum AR order through the order.max argument. The AR order may be estimated by choosing the order, between 0 and the maximum order, whose model has the smallest AIC. This option can be specified by setting the AIC argument to take the true value, that is, $\mathbb { A } \mathbb { I } \mathbb { C } { = } \mathbb { T }$ . Or we can switch off order selection by specifying $\mathbb { A } \mathbb { I } \mathbb { C } { = } \mathbb { F }$ . In the latter case, the AR order is set to the maximum AR order. The ar function can estimate the AR model using a number of methods, including solving the Yule-Walker equations, ordinary least squares, and maximum likelihood estimation (assuming normally distributed white noise error terms). These correspond to setting the option method $. = ^ { \prime } \mathrm { y w } ^ { \prime }$ , method='ols', or method='mle', respectively. In particular, the preceding R command fits an AR(1) model for the ar1.s series by solving the Yule-Walker equation.

We digress briefly to discuss the concept of a logical variable, which can take the value TRUE or FALSE. These values can be abbreviated as T and F. In binary representation, T is also represented by 1 and F by 0. R adopts the useful convention that a logical variable appearing in an arithmetic expression will be automatically converted to 1 if it is a T and 0 otherwise.

```txt
Exhibit 7.6, page 165.  
data(arma11.s)  
arima(arma11.s, order=c(1,0,1), method='CSS') 
```

The arima function estimates an $\mathbf { A R I M A } ( p , d , q )$ model for the time series passed to it as the first argument. The ARIMA order is specified by the order argument, order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (p,d,q), so the command above fits an ARMA(1,1) model to the data. Estimation can be carried out by the conditional sum-of-squares method (method= 'CSS') or maximum likelihood (method='ML'). The default estimation method is maximum likelihood, with initial values determined by the CSS method. The arima function prints out a summary of the fitted model. The fitted model may also be saved as an object that can be further manipulated, for example, for model diagnostics. By default, if $d = 0$ , a stationary ARMA model will be fitted. Also, the fitted model is in the centered form; that is, an ARMA model fitted to the series minus its sample mean. The intercept term reported in the output of the arima function is a misnomer, as it is in fact the mean! However, the mean so estimated generally differs slightly from the sample mean.

# Exhibit 7.10 on page 168.

res $, =$ arima(sqrt(hare),order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (3,0,0))

This saves the fitted AR(3) model in the object named res. The output of the arima function is a list. Run the command str(res) to find out what is saved in res. You will find that most of the things in res are not directly useful. Instead, the output of the arima function has to be processed by other functions for more informed summaries. For example, (raw) residuals from the fitted model can be computed by the residuals function via the command residuals(res). Fitted values can be obtained by running fitted(res). Other useful functions for processing a fitted ARIMA model from the arima function will be discussed below.

The empirical approach of using the bootstrap to do inference is illustrated below.

set.seed(12345)

This initializes the seed of the random number generator so that the simulation study can be repeated.

coefm.cond.norm $\lfloor =$ arima.boot(res,cond.boot ${ } _ { \cdot } = \mathbb { T }$ ,is.normal=T, $\mathtt { B = 1 0 0 0 }$ ,init $=$ sqrt(hare))

The arima.boot function carries out a bootstrap analysis based on a fitted ARIMA model. Its first argument is a fitted ARIMA model, that is, the output from the arima function. Four different bootstrap methods are available: The bootstrap series can be initialized by a supplied value (cond.boot $= \mathrm { T }$ ) or not (cond.boot ${ \bf \nabla } _ { \cdot } = \bf { F }$ ), and a nonparametric bootstrap $\left( \mathrm { i } \mathbf { s } \cdot \mathrm { n o r m a } \mathrm { 1 } = \mathrm { F } \right)$ ) or a parametric bootstrap assuming normal innovations (is.normal=T) can be used. For a conditional bootstrap, the initial values can be supplied as a vector (the arima.boot function will use the initial values from the supplied vector). The bootstrap sample size, say 1000, is specified by the $\mathtt { B = 1 0 0 0 }$ option. The function arima.boot outputs a matrix with each row being the bootstrap estimate of the ARIMA coefficients obtained by maximum likelihood estimation with the bootstrap data. So, if $\mathtt { B = 1 0 0 0 }$ and the model is an AR(3), then the output is a 1000 by 4 matrix where each row consists of the bootstrap AR(1), AR(2), and AR(3) coefficients plus the mean estimate in that order $( \hat { \Phi } _ { 1 } , \hat { \Phi } _ { 2 } , \hat { \Phi } _ { 3 } ^ { \mathrm { ^ { \scriptsize ~ \cdot } } } , \hat { \mu } )$ .

signif(apply(coefm.cond.norm,2,function(x) {quantile(x,c(.025,.975),na.rm=T) $\}$ ),3)

This is a compound R statement. It is equivalent to the two commands

temp=apply(coefm.cond.norm,2,function(x) {quantile (x,c(.025,.975),na.rm=T)}) signif(temp,3)

except that the temporary variable temp is not created in the original compound statement. Recall that the apply function is a general-purpose function for processing a matrix. Here the apply function processes the matrix coefm.cond.norm column by column, with each column supplied to the no-name user-supplied function

function(x){quantile(x,c(.025,.975),na.rm=T)}

This no-name function has one input, called x, that is processed by the quantile function. The quantile function takes a vector and computes the sample quantiles with the corresponding probability specified in the second argument. The third argu-

ment of the quantile function is specified as na.rm=T (na stands for not available and rm means remove), which means that any missing values in the input are discarded before computing the quantiles. This specification is pivotal because by default any quantile of a dataset with some missing values is defined to be a missing value (NA) in R. (Some bootstrap series may have convergence problems upon fitting an ARIMA model and hence the output of the bootstrap function may contain some missing values.) To return to the interpretation of the command on the right-hand side of temp, it instructs R to compute the 2.5th and 97.5th percentiles of each bootstrap coefficient estimate. To enable precise calculations, R maintains many significant digits in the numbers stored in an object. The printed version, however, usually requires fewer significant digits for clarity. This can be done by the signif function. The signif function outputs the object passed into it as first argument, but only to the number of significant digits specified in the second argument, which is three in the example. Altogether, the compound R command computes the $9 5 \%$ bootstrap confidence intervals for each AR coefficient.

# Chapter 8 R Commands

```txt
Exhibit 8.2 on page 177.  
data(hare)  
m1.hare=arima(square(hare), order=c(3,0,0))  
m1.hare 
```

This prints the fitted AR(3) model for the square-root-transformed hare data. The AR(2) coefficient estimate $( \hat { \Phi } _ { 2 } )$ turns out not to be significant. Note that the AR(2) coefficient is the second element in the coefficient vector, as shown in the printout of the fitted model. A constrained ARIMA model with some elements fixed at certain values can be fitted by using the fixed argument in the arima function. The fixed argument should be a vector of the same length as the coefficient vector and its elements set to NA for all of the free elements but set to zero (or another fixed value) for all of the constrained coefficients. For example, here the AR(2) coefficient is constrained to be zero $( \phi _ { 2 } = 0$ ) and hence fixed $\boldsymbol { \mathbf { \mathit { \Psi } } } . = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (NA,0,NA,NA), that is, the AR(1), AR(3), and the ‘‘intercept’’ term are free parameters, whereas the AR(2) is fixed at 0. Remember that the ‘‘intercept’’ term is last. Below is the command for fitting the constrained AR(3) model for the hare data.

m2.hare $\equiv$ arimasqrt(hare),order $= c$ 3,0,0) fixed=c(NA,0,NA,NA))   
m2.hare

Note that the intercept term is actually the mean in the centered form of the ARMA model; that is, if $y =$ sqrt(hare) − intercept, then the model is

$$
y _ {t} = 0. 9 1 9 y _ {t - 1} - 0. 5 3 1 3 y _ {t - 3} + e _ {t}
$$

so the “true” estimated intercept equals $5 . 6 8 8 9 ^ { * } ( 1 - 0 . 9 1 9 + 0 . 5 3 1 3 ) = 3 . 4 8 3$ , as stated in the text!

```txt
plot(rstandard(m2.hare), ylab='Standardized Residuals', type='b') 
```

The function rstandard computes the standardized residuals; that is, the raw residuals normalized by the estimated noise standard deviation.

abline $(h = 0)$

adds a horizontal line to the plot with zero y-intercept. Use the help in R to find out how to add a vertical line with $x$ -intercept $= 1 0$ .

```txt
Exhibit 8.12 on page 185 (prefaced by some commands in Exhibit 8.1 on page 176)  
data(color)  
m1.color=arima(color, order=c(1,0,0))  
tsdiag(m1.color, gof=15, omit.initial=F) 
```

The tsdiag function in the TSA package has been modified from that in the stats package of R. It performs model diagnostics on a fitted model. The argument gof specifies the maximum number of lags in the acf function used in the model diagnostics. Setting the argument omit.initial $\mathbf { \Omega } = \mathbf { T }$ omits the few initial residuals from the analysis. This option is especially useful for checking seasonal models where the initial residuals are close to zero by construction and including them may skew the model diagnostics. In the example, the omit.initial argument is set to be F so that the diagnostics are done with all residuals. Recall that the Ljung-Box (portmanteau) test statistic equals the weighted sum of the squared residual autocorrelations from lags 1 to K, say; see Equation (8.1.12) on page 184. Assuming that the ARIMA orders are correctly specified, the validity of the approximate chi-square distribution for the Ljung-Box test statistic requires that K be larger than the lag beyond which the original time series has negligible autocorrelation. The modified tsdiag function in the TSA package checks this requirement; consequently the Ljung-Box test is only computed for sufficiently large K. If the required K is larger than the specified maximum lag, tsdiag will return an error message. This problem can be solved by increasing the maximum lag asked for. Use ?tsdiag to learn more about the modified tsdiag function.

# Chapter 9 R Commands

# Exhibit 9.2 on page 205.

data(tempdub)

tempdub1=ts(c(tempdub,rep(NA,24)),start $=$ start(tempdub), freq $: =$ frequency(tempdub))

This appends two years of missing values to the tempdub data, as we want to forecast the temperature for two years into the future. The function start extracts the starting date of a time series. The function frequency extracts the frequency of the time series passed to it, here being 12. Hence, tempdub1 contains the Dubuque temperature series augmented by two years of missing data, with the same starting date and frequency of sampling per unit time interval.

har. $=$ harmonic(tempdub,1)

This creates the first pair of harmonic functions.

m5.tempdub $^ { 1 = }$ arima(tempdub,order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (0,0,0),xreg $: =$ har.)

This fits the harmonic regression model using the arima function. The covariates are passed to the function through the xreg argument. In the example, har. is the covariate and the arima function fits a linear regression model of the response variable on the covariate, with the errors assumed to follow an ARIMA model. Because the specified ARIMA orders $p = d = q = 0$ , the presumed error structure is white noise; that is, the arima function fits an ordinary linear regression model of tempdub on the first pair of harmonic functions. Note that the result is the same as that from the fit using the lm function, which can be verified by the following commands:

har. $=$ harmonic(tempdub,1); model4=lm(tempdub~har.) summary(model4)

The xreg argument expects the covariate input either as a matrix or a data.frame. A data.frame can be thought of as a matrix made up by binding together several covariates column by column. It can be created by the data.frame function with multiple arguments, each of which takes the form covariate.name $=$ R statement for computing the covariate. If the covariate.name is omitted, the R statement becomes the covariate name, which may be undesirable for a complex defining statement. If the R statement is a matrix, its columns are taken as covariates with the column names taken as the covariate names. Consider the example of augmenting the harmonic regression model above by a linear time trend. The augmented model can be fitted by the command

arima(tempdub,order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (0,0,0), xreg $^ { \cdot } =$ data.frame(har.,trend $. =$ time(tempdub)))

m5.tempdub

This prints the fitted model.

We now illustrate prediction with an example.

newhar. $=$ harmonic(ts(rep(1,24), start ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (1976,1),freq $= \mathtt { 1 2 }$ ),1)

This creates the harmonic functions over two years starting from January 1976. Remember that the tempdub series ends in December 1975.

plot(m5.tempdub,n.ahead $^ { } = 2 4$ , $\mathtt { n 1 } = \mathtt { C }$ (1972,1),newxreg $\mathbf { = }$ newhar., $\scriptstyle \mathbf { C O } 1 = { ' } \ x \in \mathbf { d } ^ { \prime }$ , type=’b’,ylab $^ { 1 = }$ 'Temperature',xlab $^ { 1 = }$ 'Year')

This computes and plots the forecasts based on the fitted model passed as the first argument. Here, we specify a forecast for 24 steps ahead through the argument n.ahead $^ { . = 2 4 }$ . The covariate values over the period of forecast have to be supplied by the newxreg argument. The newxreg argument should match the xreg argument in terms of the covariates except that their values are from different periods. The plot may be drawn with a starting date different from the start date of the time series data by using the n1 argument. Here, $\boldsymbol { \mathrm { n } } \boldsymbol { \mathrm { 1 } } = \boldsymbol { \mathrm { C } }$ (1972,1) specifies January 1972 as the start date for the plot. For nonseasonal data (that is, frequency $= 1$ ), n1 should be a scalar. The col and type arguments refer to the color and style of the plotted lines.

# Exhibit 9.3 on page 206.

data(color)

m1.color $: =$ arima(color,order=c(1,0,0))

```txt
plot(m1.color,n.ahead=12,col='red',type='b',xlab='Year', ylab='Temperature')  
abline(h=coef(m1.color) [names(coef(m1.color)) == 'intercept']) 
```

The final command adds the horizontal line at the estimated mean (intercept). This is a complex statement. The expression coef(m1.color) extracts the coefficient vector. The components of the coefficient vector are named. The names of a vector can be extracted by the names function, so names(coef(m1.color)) returns the vector of names of the components of the coefficient vector. The $= =$ operator compares the two vectors on its two sides element by element, resulting in a vector consisting of TRUEs and FALSEs depending on whether the elements are equal or not. (If the vectors under comparison are of unequal length, R recycles the shorter one repeatedly to match the longer one.) Hence, the command

```txt
[names(coef(ml.color)) == 'intercept'] 
```

returns a vector with the TRUE value in the position in which the “intercept” component lies and with all other elements FALSE. Finally, the intercept coefficient estimate is extracted by the “bracket” operation:

```txt
coef(ml.color) [names(coef(ml.color)) == 'intercept'] 
```

The operation within brackets subsets a vector using one of two mechanisms. Let v be a vector. A subvector of it can be formed by the command v[s], where s is a Boolean vector, (that is, consisting of TRUEs and FALSEs) that is of the same length as v. The vector v[s] is then a sub-vector of v consisting of those elements of v for which the corresponding element in s is TRUE; elements in v whose corresponding element in s is FALSE are discarded from v[s].

A second way to subset a vector is to construct s so that it contains the position of the elements to be retained and v[s] will return the desired subvector. A variation of this approach is to form a subvector by deletion. Unwanted elements are designated by giving their positions multiplied by -1. An illustration follows.

> $v = 1:5$

This creates a vector containing the first five positive integers.

```txt
> v  
[1] 1 2 3 4 5  
> names(v)  
NULL 
```

By default, the components of $\mathrm { \Delta v }$ are unnamed, so names(v) returns an empty vector denoted by the object NULL.

>names(v) $=$ c('A'，'B'，'C'，'D'，'E')

This is the method of assigning names to the components of a vector.

```txt
> V A B C D E 12345 
```

The command >names(v) $= = ^{\prime}C^{\prime}$

```txt
[1] FALSE FALSE TRUE FALSE FALSE  
finds which components of names (v) is "C."  
The command  
> v [names (v) == 'C']  
C  
3  
subsets v by Boolean extraction.  
The command  
> v [3]  
C  
3  
subsets v by supplying the positions of the retained elements.  
The command  
> v[-3]  
A B D E  
1 2 4 5 
```

subsets v by supplying the positions of the unwanted elements.

# Chapter 10 R Commands

The theoretical ACF of a stationary ARMA process can be computed by the ARMAacf function. The ar parameter vector, if present, is to be passed into the function via the ar argument. Similarly, the ma parameter vector is passed into the function via the ma argument. The maximum lag may be specified by the lag.max argument. Setting the pacf argument to TRUE computes the theoretical pacf; otherwise the function computes the theoretical acf. Consider as an example the seasonal MA model:

$$
Y _ {t} = (1 + 0. 5 B) (1 + 0. 8 B ^ {1 2}) e _ {t}
$$

Note that $( 1 + 0 . 5 B ) ( 1 + 0 . 8 B ^ { 1 2 } ) = ( 1 + 0 . 5 B + 0 . 8 B ^ { 1 2 } + 0 . 4 B ^ { 1 3 } )$ so the ma coefficients are specified by the option $\mathtt { m a } = \mathtt { C }$ (0.5,rep(0,10),0.8,0.4). Its theoretical ACF is displayed on the left side of Exhibit 10.3, which can be done by the following R commands.

```r
plot(y=ARMAacf(ma=c(0.5, rep(0, 10), 0.8, 0.4), lag.max=13)[-1], x=1:13, type='h',  
xlab='Lag k', ylab=expression(rho[k]), axes=F,ylim=c(0, 0.6))  
points(y=ARMAacf(ma=c(0.5, rep(0, 10), 0.8, 0.4), lag.max=13)[-1], x=1:13, pch=20)  
abline(h=0)  
axis(1, at=1:13, labels=c(1, NA, 3, NA, 5, NA, 7, NA, 9, NA, 11, NA, 13))  
axis(2)  
text(x=7, y=.5, labels=expression(list(theta&=-0.5, Theta&=-0.8))) 
```

As the labeling of the figure requires Greek alphabets and subscripts, the label information has to be passed via the expression function. Run the help menu

?plotmath to learn more about how to do mathematical annotations in R.

```txt
Exhibit 10.10 on page 237  
m1.co2=arima(co2, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12)) 
```

The argument seasonal supplies the information on the seasonal part of the seasonal ARIMA model. It expects a list with the seasonal order supplied in the component named order and the seasonal period entered via the period component, so the command above instructs the arima function to fit a seasonal ARIMA $( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ model to the co2 series.

```txt
m1.co2 
```

This prints a summary of the fitted seasonal ARIMA model.

# Chapter 11 R Commands

```txt
Exhibit 11.5 on page 255.  
acf(as.vector(diff(diffwindow(log(airmiles), end=c(2001,8)),12))),lag.max=48) 
```

The expression window(log(airmiles), $\mathtt { e n d } = \mathtt { C }$ (2001,8)) subsets the log(airmiles) time series by specifying a new end date of August 2001. The subtime series is first seasonally differenced with lag 12 and then regularly differenced. The doubly differenced series is then passed to the acf function for computing the sample ACF out to 48 lags.

```txt
Exhibit 11.6 on page 255.  
air.m1=arimax(log(airmiles), order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12),  
xtransf=data.frame(I911=1*(seq(airmiles) == 69), I911=1*(seq(airmiles) == 69)),  
transfer=list(c(0,0), c(1,0)),  
xreg=data.frame(Dec96=1*(seq(airmiles) == 12), Jan97=1*(seq(airmiles) == 13), Dec02=1*(seq(airmiles) == 84)), method='ML') 
```

The arimax function extends the arima function so that it can handle intervention analysis and outliers (both AO and IO) in time series. It is assumed that the intervention affects the mean function of the process, with the deviation from the unperturbed mean function modeled as the sum of the outputs of an ARMA filter of a number of covariates; the deviation is known as the transfer function. The covariates making up the transfer function are passed to the arimax function via the xtransf argument in the form of a matrix or a data.frame. For each such covariate, its contribution to the transfer function takes the form of a dynamic response given by

$$
\frac {(a _ {0} + a _ {1} B + \cdots + a _ {q} B ^ {q})}{(1 - b _ {1} B - b _ {2} B ^ {2} - \cdots - b _ {p} B ^ {p})} c o v a r i a t e _ {t}
$$

The transfer function is the sum of the dynamic responses, in the form of some ARMA filter, of all covariates in the xtransf argument. The ARMA order of the filter is

denoted by the vector $\mathtt { C } \left( \mathtt { p } , \mathtt { q } \right)$ . If $\mathtt { p } = \mathtt { q } = 0$ (that is, $\mathsf { c } \left( \mathsf { p } , \mathsf { q } \right) = \mathsf { c } \left( 0 , 0 \right)$ ), the contribution of the covariate is of the form . Ifa covariate $\mathtt { c } \left( \mathtt { p } , \mathtt { q } \right) = \mathtt { c } \left( \mathtt { 1 } , 0 \right)$ , the output becomes

$$
\frac {a _ {0}}{(1 - b _ {1} B)} c o v a r i a t e _ {t} = a _ {0} (c o v a r i a t e _ {t} + b _ {1} c o v a r i a t e _ {t - 1} + b _ {1} ^ {2} c o v a r i a t e _ {t - 2} + \dots)
$$

The ARMA orders for the dynamic components of the transfer function are supplied via the transf argument as a list containing the vectors of ARMA orders in the order of the covariates defined in the xtransf argument. Hence, the options:

xtransf $\equiv$ data.frame(I911 $= 1^{*}$ (seq(airmiles) $= = 69$ ）, I911 $= 1^{\star}$ (seq(airmiles) $= = 69$ ）), transfer $\equiv$ list(c(0,0)，c(1,0))

instruct the arimax function to create two identical covariates called I911, which is an indicator variable, say $P _ { t } ,$ that equals 1 in September 2001 and 0 otherwise, and the transfer function is the sum of two ARMA filters of the $9 / 1 1$ indicator variable of orders $\mathtt { c ( 0 , 0 ) }$ and $\mathtt { c } ( 1 , 0 )$ respectively. Hence the transfer function equals

$$
\omega_ {0} P _ {t} + \frac {\omega_ {1}}{(1 - \omega_ {2} B)} P _ {t}
$$

This is equivalent to an ARMA(1,1) filter of the form

$$
\frac {\{(\omega_ {0} + \omega_ {1}) - \omega_ {0} \omega_ {2} B \}}{(1 - \omega_ {2} B)} P _ {t}
$$

which can be specified by the following options

xtransf $\equiv$ data.frame(I911=1\* (seq(airmiles) $= = 69$ ), transfer $\equiv$ list(c(1,1))

Additive outliers (AO) in a time series can be incorporated as indicator variables passed to the xreg argument. For example, three potential AOs are included in the model by the following supplied argument:

```txt
xreg=data.frame(Dec96=1*(seq(airmiles) == 12), Jan97=1*(seq(airmiles) == 13), Dec02=1*(seq(airmiles) == 84)) 
```

Note that the first potential outlier occurs in December 1996. The corresponding indicator variable is labeled as Dec96 and is computed by the formula $\beth ^ { \star }$ (seq(airmiles) $= = \pm 2$ ), which results in a vector that equals 0 except its twelfth element, which equals 1, and the vector is of the same length as airmiles. Some specifics of this “simple” command follow. The function seq creates a vector consisting of the first $n$ positive integers, where $n$ is the length of the vector passed to the seq function. The expression seq(airmiles) $= = \pm 2$ creates a vector of the same length as airmiles, and its elements are all FALSE except that the twelfth element is TRUE. Then $\perp ^ { \star }$ (seq(airmiles) $= = \pm 2$ ) is an arithmetic expression for which R automatically converts any imbedded Boolean vector (seq(airmiles) $= = \pm 2$ ) to a binary vector. Recall that the TRUE values are converted to 1s and the FALSE values to 0s.

Multiplying by 1 does not alter the converted binary vector. Indeed, multiplication is employed to trigger the conversion from the Boolean values to binary values.

For this example, the unperturbed process is assumed to be an IMA(1,1) process, as is evident from the supplied argument order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (0,1,1). In general, a seasonal ARIMA unperturbed process is specified in the same way that it is specified for the arima function.

air.m1

This prints out the fitted intervention model, as displayed below.

>air.m1   
Call:arimax $\mathrm{x =}$ log(airmiles),order=c(0,1,1),seasonal $=$ list(order=c(0,1,1)，period=12)，xreg=data.frame(Dec96= 1\* (seq(airmiles) $= = 12$ )，Jan97 $= 1^{\star}$ (seq(airmiles) $= = 13$ )， Dec02 $= 1^{*}$ (seq(airmiles) $= = 84$ ))，method $= ^{\prime}\mathsf{ML}^{\prime}$ xtransf $\equiv$ data.frame(I911 $= 1^{*}$ (seq(airmiles) $= = 69$ )，I911 $= 1^{\star}$ （seq(airmiles) $= = 69$ ))，transfer $\equiv$ list(c(0,0)，c(1,0)))

```txt
Coefficients: mal smal Dec96 Jan97 Dec02 I911-MA0 I911.1-AR1 I911.1-MA0 -0.3825 -0.6499 0.0989 -0.0690 0.0810 -0.0949 0.8139 -0.2715 s.e. 0.0926 0.1189 0.0228 0.0218 0.0202 0.0462 0.0978 0.0439 sigma^2 estimated as 0.000672: log likelihood=219.99, aic=-423.98 
```

Note that the parameter in the transfer-function component defined by the first instance of the indicator variable I911 is labeled as I911-MA0; that is, the MA(0) coefficient. The transfer-function components defined by the second instance of the indicator variable I911 are labeled as I911.1-AR1 and I911.1-MA0. These are the AR(1) and MA(0) coefficient estimates.

We can also try the equivalent parameterization of specifying an ARMA(1,1) filter on the 9/11 indicator variable.

```javascript
>air.mla=arimax(log(airmiles),order=c(0,1,1), seasonal=list(order=c(0,1,1),period=12), xtransf=data.frame(I911=1*(seq(airmiles) == 69)), transfer=list(c(1,1)), xreg=data.frame(Dec96=1*(seq(airmiles) == 12), Jan97=1*(seq(airmiles) == 13), Dec02=1*(seq(airmiles) == 84)), method='ML') 
```

>air.mla   
Call:arimax(x=log(airmiles),order=c(0,1,1),seasonal $=$ list(order=c(0,1,1),period $\coloneqq 12$ ),xreg=data.frame(Dec96=1 \* (seq(airmiles) $= = 12$ ),Jan97 $= 1^{*}$ (seq(airmiles) $= = 13$ ),Dec02 $=$ 1\* (seq(airmiles) $= = 84$ ))，method $=$ 'ML'，xtransf $=$ data.frame(I911 $= 1^{*}$ (seq(airmiles) $= = 69$ )），transfer $=$ list(c(1,1)))

```txt
Coefficients:  
-0.3601 -0.6130 0.0949 -0.0840 0.0802 0.8094 -0.3660 0.0741  
s.e. 0.0926 0.1261 0.0222 0.0229 0.0194 0.0924 0.0233 0.0424  
sigma^2 estimated as 0.000648: log likelihood=221.76, aic=-427.52 
```

Note that the parameter estimates of this model are similar to those of the previous model but this model has a better fit, which may happen as the optimization is done numerically.

```scala
Exhibit 11.8 on page 256. Nine11p=1* (seq(airmiles) == 69) 
```

This defines the 9/11 indicator variable.

```txt
plot(ts(Nine11p*(-0.0949) + filter(Nine11p,filter=.8139, method='recursive',side=1)*(-0.2715), frequency=12,start=1996),type='h',ylab='9/11 Effects') 
```

The command

Nine11p\*(-0.0949)+filter(Nine11p,filter=.8139, method $\coloneqq$ 'recursive',side $= 1$ $\star (-0.2715)$

computes the estimated transfer function. Note that the command

filter(Nine11p,filter $=$ .8139,method $\equiv$ 'recursive',side $= 1$

computes $\left( 1 - 0 . 8 1 3 9 \star \mathrm { B } \right)$ )Nine11p. The function filter performs an MA or AR filtering on the input sequence passed to it as the first argument. Suppose the input is a vector $\pmb { x } = c ( x _ { 1 } , \mathrm { x } _ { 2 } , . . . , x _ { n } )$ . Then the output $\pmb { y } = c ( y _ { 1 } , y _ { 2 } , . . . , y _ { n } )$ defined by the MA filter

$$
y _ {t} = c _ {0} x _ {t} + c _ {1} x _ {t - 1} + \dots + c _ {q} x _ {t - q}
$$

can be computed by the command

```prolog
filter(x, filter=c(c0, c1, ..., cq), side=1). 
```

The argument side $^ { : = 1 }$ specifies that the MA operator works on current and past values when computing an output value. To compute y1, the value of $\mathtt { x 0 }$ is needed. Since the latter is not observed, the filter sets it to NA, and hence y1 is also NA. In this case, y2, y3, and so forth can be computed. For an AR filtering with the output defined recursively by the equation

$$
y _ {t} = x _ {t} + c _ {1} y _ {t - 1} + \ldots + c _ {p} y _ {t - p}
$$

the R command is

```python
filter(x, filter=c(c1, c2, ..., cp), method='recursive', side=1) 
```

Note that, unlike the case of the MA filter, the filter vector starts with c1 and there is no c0 in the equation. The argument method $. =$ 'recursive' signifies an AR type of filtering. For the AR filter, the initial values cannot be set to NA, lest all output values be NA! The default initial values are zeros although other initial values may be specified via the init argument.

abline $(\mathrm{h} = 0)$

adds a horizontal line with zero $y$ -intercept.

```txt
Exhibit 11.9 on page 259.  
set.seed(12345)  
y=arima.sim(model=list(ar=.8, ma=.5), n.start=158, n=100) 
```

This simulates an ARMA(1,1) series of sample size 100. To remove transient effects of the initial values, a burn-in of size 158 is specified. A large burn-in of the order of hundreds should generally ensure that the simulated process is approximately stationary. The number 158 is chosen for no particular good reason.

```txt
y[10] 
```

This prints out the tenth simulated value.

```txt
y[10] = 10 
```

This alters the tenth value to be 10; that is, it becomes an additive outlier, mimicking the effect of a clerical recording mistake, for example!

```txt
y=ts(y,freq=1,start=1); plot(y,type='o')  
acf(y)  
pacf(y)  
eax(y) 
```

This exploratory analysis suggests an AR(1) model.

```javascript
m1=arima(y,order=c(1,0,0)); m1; detectAO(m1) 
```

This detects the presence of any additive outliers (AO) in the fitted AR(1) model. The test requires an estimate of the standard deviation of the error (innovation) term, which by default is estimated by a robust estimation scheme, resulting in a more powerful test. The robust estimation scheme can be switched off by the argument robust ${ \bf \nabla } _ { \cdot } = \bf { F }$ , as illustrated in the command below.

```javascript
detectAO(m1, robust=F) 
```

This verifies that a nonrobust procedure is less powerful.

```txt
detectIO(m1) 
```

This detects the presence of any innovative outliers (IO) in the fitted AR(1) model. As an AO is found in the tenth case, it is incorporated as an indicator covariate in the following model.

```txt
m2=arima(y,order=c(1,0,0),xreg=data.frame(AO=seq(y) == 10))  
m2  
#Exhibit 11.10 on page 260  
data(co2)  
m1.co2=arima(co2,order=c(0,1,1),seasonal=list  
    (order=c(0,1,1),period=12))  
m1.co2  
detectAO(m1.co2)  
detectIO(m1.co2) 
```

As an IO is found in the 57th data case, it is incorporated in the model.

```txt
m4.co2=arimax(co2,order=c(0,1,1), seasonal=list(order=c(0,1,1),period=12),io=c(57)) 
```

The epochs of IOs are passed to the arimax function via the io argument, which expects a list containing the positions of the IOs either as the time index of the IO or as a vector in the form of c(year,month) that gives the year and month of the IO for seasonal data; the latter format also works similarly for seasonal data of other types. For

a single IO, it is not necessary to enclose the single vector of index in a list before passing it to the io argument.

```txt
Exhibit 11.11 on page 262.  
set.seed(12345)  
X=rnorm(105)  
Y=zlag(X,2)+.5*rnorm(105) 
```

The command zlag(X,2) computes the second lag of X.

```txt
X=ts(X[-(1:5)], start=1, freq=1) 
```

This omits the first five values of X and converts the remaining values to form a time series.

```txt
Y=ts(Y[-(1:5)],start=1,freq=1)  
ccf(X,Y,ylab='CCF') 
```

This computes the cross-correlation function of X and Y. The ylab argument is supplied in lieu of the default $y$ -label of the ccf function that is “ACF”.

```txt
Exhibit 11.14 on page 264.  
data(milk)  
data(electricity)  
milk.electricity=ts.intersect(milk, log(electricity)) 
```

The ts.intersect function merges several time series into a matrix (panel) of time series over the time frame where each series has data. The object milk.electricity is a matrix of two time series, the first column of which is the milk series and the second the log of electricity, over the time period when these two series overlap.

plot(milk.electricity,yax.flip $\equiv$ T)

The option yax.flip $\iota = \mathrm { T }$ flips the label for the $y$ -axis for the series alternately so as to make the labeling clearer.

```javascript
Exhibit 11.15 on page 265.  
ccf(milk.electricity[,1], milk.electricity[,2], main='milk & electricity', ylab='CCF') 
```

The expression milk.electricity[,1] extracts the milk series and milk.electricity[,2] the log electricity series.

The as.vector function strips the time series attribute from the time series. This is done to nullify the default way that the ccf function plots the cross-correlations. You may want to repeat the command without the as.vector function to see the default labels of the lags according to the period of the data.

```javascript
ccf((milk.electricity[,1]),(milk.electricity[,2]), main='milk & electricity',ylab='CCF') 
```

The bracket operator extracts a submatrix from a matrix, say M, in the form of M[v1,v2], where v1 indicates which rows are kept and v2 indicates which columns are retained. Consequently, the submatrix $\mathtt { M } [ \mathtt { v } 1 , \mathtt { v } 2 ]$ contains all elements of M in the intersection of the retained rows and columns. If v1 (v2) is missing, then all rows (columns) are retained. Hence, M[,1] is simply the submatrix consisting of the first column of M. However, R adopts the convention that a submatrix with a single row or column is “demoted” to a vector; that is, it loses one dimension. This convention makes

sense in most cases. However, if you do matrix algebra in R, this convention may result in strange error messages! To prevent automatic dimension reduction, use M[v1,v2,drop=F]. Instead of specifying which rows or columns are to be retained in the submatrix, you can specify which rows or columns are to be deleted by specifying the negative of their positions. Or v1 (v2) can be specified as a Boolean vector, where the positions to be retained (eliminated) are denoted by TRUE (FALSE).

```txt
Exhibit 11.16 on page 267.  
me.dif=ts.intersect(diff(diff(milk,12)), diff(diff(log(electricity),12)))  
prewhiten(as.vector(me.dif[,1]), as.vector(me.dif[,2]), ylab='CCF') 
```

The prewhiten function expects two time series input via the x and y arguments. Both series will be filtered according to an ARIMA model. The ARIMA model can be supplied via the x.model argument and should be the output of the arima function. If no ARIMA model is supplied, an AR model will be fitted to the $x$ series, with the AR order selected by minimizing the AIC. The prewhiten function computes and plots the cross-correlation function (CCF) of the residuals of the $x$ series and those of the $y$ series from the same (supplied or fitted) model.

# Chapter 12 R Commands

Below, we show how to implement the Jarque-Bera test for normality in two different ways. First, we show the direct approach.

skewness(r.cref)

This computes the skewness of the r.cref series.

kurtosis(r.cref)

This computes the kurtosis of the data.

length(r.cref)*skewness(r.cref)^2/6

The function length returns the length of the vector (time series) passed into it, so the expression above computes the first part of the Jarque-Bera statistic.

length(r.cref)*kurtosis(r.cref)^2/24

computes the second half of the Jarque-Bera statistic.

JB=length(r.cref) $\star$ (skewness(r.cref)^2/6 + kurtosis(r.cref)^2/24)

The object JB then contains the Jarque-Bera statistic and the command JB prints out the statistic. The command 1-pchisq(JB, $\mathtt { d f } = 2$ ) computes the $p$ -value of the Jarque-Bera test for normality. The function pchisq computes the cumulative probability of a chi-square distribution being less than or equal to the value in the first argument. The df argument of the pchisq function specifies the degrees of freedom for the chi-square distribution. Because the $p$ -value equals the right tail area, it equals 1 minus the cumulative probability. Besides pchisq, other functions associated with the chi-square distribution include qchisq, which computes quantiles; dchisq, which

computes the probability density; and rchisq, which simulates realizations from the chi-square distributions. Use Help in R to learn more about these functions. For other probability distributions, similar functions are available. Associated with the normal distributions are rnorm, pnorm, dnorm, and qnorm. Check out the usages of the relevant functions for the binomial (binom), Poisson, and other distributions.

# library(tseries)

This loads the tseries library, which contains a number of functions needed for the analysis reported in this chapter. Run library(help $=$ tseries) for more information about the tseries package.

jarque.bera.test(r.cref)

This carries out the Jargue-Bera test for normality with the time series r.cref.

# Exhibit 12.9 on page 283. McLeod.Li.test( $\scriptstyle \ Y = x$ .cref)

This performs the McLeod-Li test for presence of ARCH in the daily CREF returns. The first two arguments of the function are object and y, respectively. For the test with raw data, the time series is supplied to the function via the y argument. Then, the function computes the Box-Ljung statistics with the autocorrelations of the squared data to detect for conditional heteroscedascity. The test is carried out with the first m autocorrelations of the squared data, with m ranging from 1 to the maximum lag specified by the gof.lag argument. If the gof.lag argument is missing, the default is set to $n \mathrm { l o g } _ { 1 0 } ( n )$ where $n$ is the sample size.

The McLeod-Li test can also be applied to residuals from an ARMA model fitted to the data. For example, the US dollar/Hong Kong dollar exchange rate data was found to admit an $\mathrm { A R } ( 1 ) +$ $^ +$ outlier model. The need for incorporating ARCH in the model for the exchange rate data can be tested by the command

McLeod.Li.test(arima(hkrate,order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (1,0,0), xreg $^ { \cdot } =$ data.frame(outlier1)))

Note that object is the first argument so in the above command, the fitted $\mathrm { A R } ( 1 ) +$ outlier model is passed into the function. The function then computes the test statistics based on the squared residuals from the fitted $\mathrm { A R } ( 1 ) +$ outlier model. If the object argument is supplied explicitly or implicitly, the y argument is ignored by the function even if it is supplied. Remember that to apply the test to raw data, the y argument must be supplied and the object argument suppressed.

# Exhibit 12.11 on page 286. set.seed(1235678) garch01.sim $! =$ garch.sim(alpha $\boldsymbol { \mathbf { \mathit { \Psi } } } . = \mathbf { \vec { \mathbf { C } } }$ (.01,.9), $\mathtt { n } = 5 0 0$ )

The garch.sim function simulates a GARCH process, with the ARCH coefficients supplied via the alpha argument and the GARCH coefficients via the beta argument. The sample size is passed into the function via the n argument. In the example above, alpha $\mathbf { \Psi } _ { . } = \mathbf { C }$ (.01,.9) specifies that the constant term is 0.01 and the ARCH(1) coefficient equals 0.9. So, garch01.sim saves a realization from an ARCH(1) process.

# Exhibit 12.25 on page 300. m1=garch( $\mathbf { x } { = } \mathbf { r }$ .cref,order=c(1,1))

This fits a GARCH(1,1) model with the r.cref series. The garch function estimates a GARCH model by maximum likelihood. The time series is supplied into the function by the x argument and the GARCH order by the order argument. The order takes the form $\mathtt { C } \left( \mathtt { p } , \mathtt { q } \right)$ where p is the GARCH order and q the ARCH order.

summary(m1)

This summarizes the fitted GARCH(1,1) model. Ignore the Box-Ljung test results reported in the summary, as the generalized portmanteau tests should be used; see the book.

# Exhibit 12.29 on page 305. gBox(m1,method $. =$ 'squared')

The gBox function computes the generalized portmanteau test for checking whether or not there is any residual heteroscedasticity in the residuals of a fitted GARCH model. It requires supplying the fitted GARCH model from the garch function through the first argument (the model argument, the first argument of the function). By default, the tests are carried out with the squared residuals from the fitted GARCH model. To inspect absolute residuals, use the option method $. = "$ absolute'. By default, the test is carried out for the ACF for lags from 1 to, say, K, where K runs from 1 to 20. The collection of K’s can be specified by the lags argument. For example, to carry out the test for K ranging from 1 to 30, supply the option $\mathtt { l a g s } \mathtt { = 1 } : 3 0$ .

gBox(m1,lags $^ { \prime = 2 0 }$ , $\mathtt { p l o t } = \mathtt { F }$ , $\mathbf { x } = \mathbf { r }$ .cref, method $\ l = 1$ squared')$pvalue prints out the $p$ -values of the generalized portmanteau test with the squared residuals and $\mathtt { K } = 2 0$ ; that is, it tests any residual heteroscedasticity based on the first 20 lags of residual ACF of the squared residuals from the fitted GARCH model. Plotting is switched off by the $\mathtt { p l o t } = \mathtt { F }$ option. The gBox function returns a list, an element of which is named pvalue and contains the $p$ -values of the test for each K. Thus, the command prints out the $p$ -value for the test with $\mathtt { K } = 2 0$ .

# Exhibit 12.30 on page 306. acf(abs(residuals(m1)),na.action $. =$ na.omit)

As the initial residuals from a fitted GARCH model may be missing, it is essential to instruct the ACF to omit all missing values through the argument na.action ${ } = { }$ na.omit (the preferred action when encountering a missing value is to omit it). If this argument is omitted, the acf function uses all data and will return missing values if there are any missing data.

Overfitting the GARCH(1,2) model to the CREF returns can be carried out by the following command

$\mathfrak { m } 2 =$ garch( $\mathbf { x } { = } \mathbf { r }$ .cref,order $\mathbf { \bar { \Psi } } = \mathbf { C }$ (1,2)) summary(m2,diagnostics $\mathbf { \mu } = \mathbf { F }$ )

The summary is based on the summary.garch function in the tseries package. Note that the $p$ -values of the Ljung-Box test from the summary are invalid; the generalized portmanteau tests should be used instead. Hence, the diagnostics are turned off.

AIC(m2)

This computes the AIC of the fitted GARCH model m1.

# Exhibit 12.31 on page 306. gBox(m1, $\mathbf { x } = \mathbf { r }$ .cref,method $. =$ 'absolute')

This carries out the generalized portmanteau test based on the absolute residuals.

shapiro.test(na.omit(residuals(m1)))

This computes the Shapiro-Wilk test for normality with the residuals from the fitted model m1. The function na.omit strips all missing values from the residuals. Thus, the test is carried out with the nonmissing residuals. Without preprocessing the residuals by the na.omit function, the test may return a missing value if some of the residuals are missing!

# Exhibit 12.32 on page 307. plot((fitted(m1)[,1])^2,type='l', ylab $^ { 1 = }$ 'conditional variance',xlab='t')

The fitted function is a smart function that processes differently depending on the fitted model passed to it as the first argument. If the fitted model is some output from the garch function, the default output from the fitted function is a two-column matrix whose first column contains the one-step-ahead conditional standard deviations. Hence, their squares are the conditional variances. So (fitted(m1)[,1]) $\sim _ { 2 }$ computes the time series of estimated one-step-ahead conditional variances based on the model m1.

# Chapter 13 R Commands

# Exhibit 13.3 on page 323.

The periodogram of a time series can be computed and plotted by the function periodogram into which the data are passed as its first argument.

sp=periodogram(y); abline( $\mathrm { \Delta } \mathrm { h } = 0$ ); axis(1,at ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (0.04167,.14583))

The function periodogram has several useful arguments. Setting ${ \mathrm { 1 0 9 = " } } \mathrm { y e s } \ \prime$ tells R to plot on a log scale, whereas $\scriptstyle 1 \circ \mathbf { g } = ^ { \prime } \ n \circ ^ { \prime }$ (the default) says to plot on a linear scale. Other arguments for the plot function may be passed into the function to make better graphs. The function axis draws an axis with the first argument specifying the side on which the axis is drawn. The sides are labeled from 1 to 4 starting from the bottom in a clockwise direction. The vector of locations of the tick marks can be specified by the at argument. The command above instructs R to draw an (additional) axis on the bottom of the figure with tick marks placed at 0.04167 and 0.14583.

# Exhibit 13.9 on page 333. theta $. =$ .9 # Reset theta for other MA(1) plots ARMAspec(model=list(ma $. =$ -theta))

The function ARMAspec calculates and plots the theoretical spectral density function of the ARMA model supplied to the function as the first argument. Recall that R uses the plus convention in the MA specification, so the minus sign is added to theta. The format of the model is the same as that for the arima function.

# Chapter 14 R Commands

# Exhibit 14.2 on page 353.

The spec function can estimate the spectral density function by locally averaging the periodogram via some suitable kernel function. The function spec has several useful arguments. Setting $\scriptstyle 1 \circ g = ^ { \prime } Y e \mathbf { s } ^ { \prime }$ tells R to plot on a log scale whereas $\scriptstyle 1 \circ \mathbf { g } = ^ { \prime } \mathbf { n } \circ ^ { \prime }$ says to plot on a linear scale. Data may be detrended (fitting a linear time trend) by setting detrend $\mathbf { \Omega } = \mathbb { T }$ , and tapering may be enforced by setting taper to some fraction between 0 and 0.5. The default options are: taper $\cdot { = } 0$ and detrend $\mathbf { \Psi } = \mathbf { F }$ .

k=kernel('daniell', $\mathtt { m } = \mathtt { l } 5$ )

Here, the object k contains the Daniell kernel function with halfwidth 15. Use Help in R to learn more about the kernel function.

sp=spec(y,kernel $\mathbf { \tau } _ { = \mathrm { k } }$ ,log='no',sub='', xlab $^ { 1 = }$ 'Frequency',ylab $i = 1$ Smoothed Sample Spectral Density')

Specifying the kernel to be the Daniell kernel function instructs R to compute and plot the spectral density estimate, where the estimate at a certain frequency is obtained by averaging the current (raw) periodogram value, the neighboring 15 periodogram values on its left, and another 15 periodogram values on its right. More or less local averaging can be specified through the m argument in the kernel function.

lines(sp$freq,ARMAspec(model=list(ar=phi),freq=sp$freq, plot $\mathbf { \tau } = \mathbf { F }$ )$spec,lty='dotted')

This adds the theoretical spectral density function.

# Exhibits 14.11 and 14.12, page 364. # Spectral analysis of simulated series set.seed(271435) $\mathtt { n = 1 0 0 }$ phi1=1.5; phi2 $=$ -.75 # Reset parameter values to obtain Exhibits 14.13 & 14.14 y=arima.sim(model=list( $\mathtt { a r } \mathtt { = C }$ (phi1,phi2)), $\mathtt { n } = \mathtt { n }$ )

This simulates an AR(2) time series of length 100.

sp1=spec(y,spans $^ { * = 3 }$ ,sub $i = 1$ ', $\boldsymbol { \mathrm { 1 } } \boldsymbol { \mathrm { t y } } = \boldsymbol { \cdot }$ dotted', xlab $^ { 1 = }$ 'Frequency', ylab $^ { 1 = }$ 'Log(Estimated Spectral Density)')

This estimates the special density function using the modified Daniell kernel (the default kernel when the kernel argument is missing and the spans argument is supplied). The spans argument supplies the width of the kernel function; that is, it is twice the m argument in the kernel function plus 1. Here, spans $= 3$ specifies local averaging of three consecutive periodogram values. Note that local averaging may be repeated by passing a vector as the value of spans. For example, setting spans ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (3,5) performs local averaging twice. The estimated function obtained by local averaging with spans $= 3$ is then averaged again locally with spans $= 5$ . Repeated averaging with a modified Daniell (rectangular) kernel is similar to averaging using a bell-shaped kernel due to the Central Limit effect.

```txt
sp2=spec(y, spans=9, plot=F) 
```

This computes the spectrum estimate using a wider window encompassing nine periodogram values without plotting via the $\mathtt { p l o t } = \mathtt { F }$ argument. The output of the spec function is saved into an object named sp2.

```python
sp3=spec(y, spans=15, plot=F)
```

This uses an even wider window. How many periodogram values are included in each local averaging?

```csv
lines(sp2\$freq,sp2\$spec,lty='dashed') 
```

This plots the smoother spectrum estimate (spans $^ { , = 9 }$ ) as a dashed line.

```txt
lines(sp3\$freq,sp3\$spec,lty='dotdash') 
```

This plots the smoothest spectrum estimate (spans $= 1 5$ ) as a dotdash line.

```txt
f=seq(0.001,.5,by=.001) 
```

This creates an arithmetic sequence starting from 0.001 and ending at 0.5, with increments 0.001, which is then saved into the object f.

```txt
lines(f, ARMAspec (model = list(ar = c phi1, phi2)), freq = f, plot = F) $spec, lty = 'solid') 
```

This plots the theoretical spectral density function for the specified ARMA model as connected line segments on top of the estimated spectral density plot.

```txt
Exhibit 14.12 on page 365. 
```

```txt
sp4=spec(y, method='ar', lty='dotted', xlab='Frequency', ylab='Log(Estimated AR Spectral Density)') 
```

This estimates the spectral density function using the theoretical spectral density function of an AR model fitted to the data by minimizing the AIC.

```txt
f=seq(0.001,.5,by=.001) 
```

```autoit
lines(f,ARMAspec(model=list(ar=c(phil,phi2)), freq=f,plot=F) $spec,fty='solid') 
```

This plots the theoretical spectral density function.

```txt
sp4\$method 
```

This displays the order of the AR model selected.

# Chapter 15 R Commands

```txt
Exhibit 15.1 on page 386.  
set.seed(2534567)  
par(mfrow=c(3,2))  
y=arima.sim(n=61, model=list(ar=c(1.6,-0.94), ma=-0.64)) 
```

This simulates an ARMA(2,1) series of sample size 61.

```txt
lagplot(y) 
```

This plots the lagged regression plots, where the time series is plotted against its lags and a smooth curve is superimposed on each scatter diagram. The smooth curves are obtained by local linear fits to the data. By increasing the value specified in the nn argu-

ment (default $\mathtt { n n } = 0 \cdot 7$ ), the local fitting scheme uses more local data, resulting in a smoother fit that is likely to be more biased but less variable due to more smoothing. On the contrary, decreasing the value in the nn argument leads to a rougher fit that is less biased but more variable due to less smoothing. The smooth curve in the scatter diagram of the time series response versus its lag j estimates the conditional mean response given its $\log j$ as a function of the value of the lag $j$ of the response. By default, lagplot plots the lagged regression plot for lags 1 to 6. More lags can be computed via the lag.max argument. For instance, lag.max=12 computes the lagged regression plots for lags 1 through 12. Note that the lagplot function requires the installation of the locfit package of R.

```txt
Exhibit 15.2 on page 387. data(veilleux) 
```

The dataset veilleux is a matrix consisting of two time series. Its first column is the series of Didinium abundance and the second column the series of Paramecium abundance, each counted every 12 hours. The basic time unit is days, so these are series of frequency 2, as they are sampled twice per day.

```javascript
predator=veilleux[,1] 
```

This defines the predator series as the abundance series of Didinium.

```javascript
plot(log(predator),lty=2,type='b',xlab='Day', ylab='Log(predator)') 
```

This plots the entire log-transformed predator series as a dashed line.

predator EQ $\equiv$ window(predator, start=c(7,1))

This subsets the “stationary” part of the predator series that appears to begin on the seventh day of the experiment. Subsequent analyses of the predator series reported in the text were done with this log-transformed stationary subseries.

```lisp
lines(log(predator EQ)) 
```

This draws the stationary part as a solid line.

```txt
index1 = zlag(log(predator EQ), 3) <= 4.661 
```

The command zlag(log(predator.eq),3) returns the lag 3 of the (log-transformed) predator series. The expression zlag(log(predator.eq),3) $< = 4$ .661 computes a Boolean vector whose elements are TRUE if and only if their corresponding element of the lag 3 of the predator series is less than or equal to 4.661. The Boolean vector is saved in an object named index1. Other comparison operators, including $> =$ , $> , <$ , and $= =$ , can be used to compare the vectors on the two sides of the comparison operator. In the example above, the left-hand side of $< =$ is a vector, but its right-hand side is a scalar! The discrepancy is resolved by the recycling rule, that R replicates the shorter vector repeatedly to match its longer part. Note that the equality operator is denoted by the double equal sign $= =$ , as the single equal sign represents the assignment operator!

points( $\cdot y { = } 1 0 9$ (predator.eq)[index1],(time(predator.eq)) [index1],pch $_ { . = 1 9 }$ )

This draws as solid circles ( $\mathtt { \cdot p c h = 1 9 }$ ) those data points whose lag 3 of the predator abundance is less than or equal to 4.661. Run the command ?points to learn other styles for plotting data points.

# Tests for nonlinearity, page 390.

Keenan.test(sqrt(spots))

This carries out Keenan’s test for linearity. The working order of the AR process under the null hypothesis of linearity can be supplied via the order argument. For example, order $^ { \cdot } = 2$ sets the working AR order to 2. If the order argument is missing, the order is automatically determined by minimizing the AIC via the ar function. The ar function by default estimates the models by solving the Yule-Walker equations. But other estimation methods may be used by including the method argument when calling the Keenan.test function; for example, method='mle' specifies using maximum likelihood in the ar function.

Tsay.test(sqrt(spots)), page 390.

This implements Tsay’s test for linearity; see Tsay (1986). The design of the Tsay.test function and its arguments are similar to those of the Keenan.test function.

# Exhibit 15.6 on page 400.

y=qar.sim( $\mathtt { m } = \mathtt { 1 } 0 0$ ,const ${ } = 0$ .0,phi $0 = 3$ .97,

phi1=-3.97,sigma $\mathtt { . = 0 }$ ,init $=$ .377)

The function qar.sim simulates a time series realization from a first-order quadratic AR model where phi0 is the coefficient of the lag 1 and phi1 is that of the square of lag 1. The default intercept is zero, otherwise it can be set by the const argument. The innovation standard deviation is passed into the function via the sigma argument. Here, sigma $^ { . = 1 }$ sets the standard deviation to be 1. The argument $\mathtt { n } = \mathtt { 1 } 5$ sets the sample size to 15. Finally, the argument init ${ } = { }$ .377 sets the initial value to be 0.377. The default initial value is 0.

plot( $\mathbf { \chi } _ { \mathbf { x } = 1 : 1 0 0 } ^ { \prime }$ , $\tt y = y$ ,type='l',ylab $^ { 1 = }$ expression(Y[t]),xlab='t')

The output of the qar.sim function is a vector. To draw the time sequence plot, both the $x$ -variable and the $y$ -variable have to be specified.

# Exhibit 15.8 on page 411.

set.seed(1234579)

$\mathtt { y } =$ tar.sim( $\mathtt { m } = \mathtt { 1 } 0 0$ ,Phi1 $\boldsymbol { \mathbf { \ell } } _ { \cdot } = \boldsymbol { \mathbf { \ell } } _ { \cdot }$ (0,0.5),Phi2=c(0,-1.8),p=1,d=1,

sigma1 $^ { . = 1 }$ ,thd=-1,sigma $_ { 2 = 2 }$ ) $\$ 9$

The function tar.sim simulates time series realizations from a two-regime TAR model. The order of the model is specified by the p argument, so $\mathtt { p } = \mathtt { 1 }$ specifies a first-order model. The delay is passed into the function by the d argument, so $\mathtt { d } = \mathtt { 1 }$ specifies the delay to be 1. The AR coefficient vector for the lower (upper) regime, with the intercept being the first component, is supplied via the Phi1 (Phi2) argument. The $\operatorname { t h d } = - 1$ argument imposes the threshold parameter of −1. The innovation standard deviations for the lower and upper regimens are specified via the sigma1 and sigma2

arguments, respectively. The simulated TAR model in the example is conditionally heteroscedastic, as the innovation standard deviation for the upper regime is twice that for the lower regime. The sample size is set to 100 by the $\mathtt { n = 1 0 0 }$ argument.

The likelihood ratio test for threshold nonlinearity, assuming normally distributed innovations, can be carried out by the tlrt function, with which the data enter into the function as the first argument. Other required information includes the order and delay arguments. Also, the threshold parameter must be searched over a finite interval from the a times 100 percentile to the $^ b$ times 100 percentile of the data. Often, data have to be transformed before testing for nonlinearity, which can be specified by supplying the transformed data or supplying the raw data with the transform argument set to one of the available options: 'no' (means no transformation, the default), 'log', $\operatorname { \mathsf { 1 } 0 9 1 0 } ^ { \prime }$ , or 'sqrt'. For example, the following command does the likelihood ratio test of the null hypothesis that the square root transformation of relative sunspot data is an AR(5) process versus the alternative that it follows a threshold model with delay 1, order 5, and with the threshold parameter searched from the first to the third quartile of the (transformed) data.

tlrt(sqrt(spots), $\mathtt { p } = 5$ , $\scriptstyle \vec { \mathrm { d } } = 1$ ,a=0.25,b=0.75)

The tlrt function outputs a list containing the test statistic and its $p$ -value. In practice, the true delay of the threshold model is unknown, although it is likely to be between 1 and the order of the model. (The delay may be specified to some value greater than the order if this is deemed appropriate.) The command above can be replicated a number of times for each possible delay value. A more elegant way is to use a for loop as follows.

# Tests for threshold nonlinearity, page 400. pvaluem $! =$ NULL

This defines an empty object named pvaluem.

for (d in 1:5) {res=tlrt(sqrt(spots), $\mathtt { p } = 5$ , $\partial = \partial$ , $\mathtt { a } = 0$ .25,b=0.75); pvaluem= cbind(pvaluem,c(d,res$test.statistic,res$p.value))}

The statements within the curly brackets are repeated for each value the variable d takes sequentially from the vector $\beth : 5$ , which contains the first five positive integers. Thus, d is first set to 1, and the likelihood ratio test for threshold nonlinearity is carried out, with its output stored in an object named res. The command c(d,res$test.statistic,res$p.value) creates a vector containing the value 1, the likelihood ratio test statistic, and its $p$ -value. The vector so created is then augmented to the right-hand side of pvaluem to form a matrix. So, after the first loop, pvaluem is a matrix consisting of the test results for $\scriptstyle \vec { \mathrm { d } } = 1$ . Then the loop sets d to the second value, namely 2; carries out the threshold likelihood ratio test for $\scriptstyle { \vec { \mathrm { d } } } = 2$ ; augments the test results for $\scriptstyle { \vec { \mathrm { d } } } = 2$ to the right-hand side of pvaluem; and so forth until the loop exhausts all possible values for d and n and then R exits from the loop.

rownames(pvaluem) ${ } = \mathbf { C }$ ('d','test statistic','p-value')

This labels the rows of the pvaluem matrix, with the first row labeled as “d”, the second “test statistic”, and the third row $^ { \ast } p$ -value”.

round(pvaluem,3)

This prints out the matrix (table) of test results, with the numbers rounded to three decimal places. Note that the computational efficiency of the R code above can be improved by declaring pvaluem as a matrix with appropriate dimension (for example, pvaluem $! =$ matrix('NA',nrow $^ { \cdot = 3 }$ , $\mathtt { n c o l } { = } 5$ )) in which the test results are saved.

```txt
Exhibit 15.12 on page 405.  
predator.tar.1=tar(y=log(predator(eq)), p1=4, p2=4, d=3, a=.1, b=.9, print=T) 
```

This fits a threshold model with the (log-transformed) predator.eq series with maximum AR order to be 4 for both lower and upper regimes, $\bar { \mathsf { d } } = 3$ , and the threshold parameter searched from the tenth to the ninetieth percentiles. The fitted model is printed out if the print argument is set to T. By default, the function uses the MAIC (minimum AIC) method for estimation, with the AR orders estimated as well. Another method of estimation is conditional least squares, which can be specified by the method $. =$ 'CLS', as illustrated in the next command.

In the command below, we repeat the estimation but using the CLS method. Note that the CLS method does not estimate the AR orders of the two regimes. Instead, the AR orders are set as the maximum orders specified through the p1 and p2 arguments! That is why the values of p1 and p2 are set differently from the previous command and in fact set as the orders estimated from the model using the MAIC method.

```txt
tar(y=log(predator EQ), p1=1, p2=4, d=3, a=.1, b=.9, print=T, method='CLS') 
```

```txt
Exhibit 15.13 on page 408.  
tar.skeleton(predator.tar.1) 
```

This computes the skeleton of a TAR model supplied as the first argument, with a default sample size of 500 values, a burn-in of 500 values, and plots the time sequence plot of the last 50 values of the skeleton. The TAR model is usually the output of that of the object argument of the tar function. Alternatively, the model parameters can be specified in a format similar to the tar.sim function. The function also prints a summary statement on the long-run behavior of the skeleton.

```txt
#Exhibit 15.14 on page 408.   
set.seed(356813)   
plot(y \(\equiv\) tar.sim(n=57,object \(\equiv\) predator.tar.1)\\(y,x=1:57, ylab \(=\) expression(Y[t]),xlab \(\equiv\) expression(t),type \(\equiv\) 'o') 
```

This plots a simulated time series from the fitted TAR(2;1, 4) model to the predator series. The fitted model is supplied via the object argument.

```txt
Exhibit 15.20 on page 414.  
tsdiag(predator.tar.1,gof.lag=20) 
```

This carries out several model diagnostics on the fitted TAR(2;1,4) model to the predator series. The function plots a time sequence plot of the standardized residuals, the residual ACF, and the $p$ -value plots of the generalized portmanteau tests. The argument gof.lag=20 specifies that the last two plots use a maximum lag of 20.

# Exhibit 15.21 on page 415. qqnorm(predator.tar.1$std.res)

This plots the quantile-quantile normal score plot for the standardized residuals from the TAR(2;1,4) model fitted to the predator series.

qqline(predator.tar.1$std.res) adds the reference line on the Q-Q plot.

# Exhibit 15.22 on page 417. set.seed(2357125) pred.predator $: =$ predict(predator.tar.1,n.ahead $\mathtt { . = 6 0 }$ , n.sim $\mathord {  \vert } = 1 0 0 0$ )

This simulates a time series from the conditional distribution of the future values given the data and a threshold model (usually the output of the tar function, here being predator.tar.1), with a forecast horizon of a maximum sixty-step-ahead predictions. The point predictors and their $9 5 \%$ prediction limits are computed by simulation. The simulation size is specified as n.sim $\mathord {  \downarrow } 0 0 0$ . The output of the predict function is a list that contains the prediction means as a vector in the component (element) named fit and the lower and upper prediction limits as a matrix in the pred.interval component. The function predict is a smart function and recognizes that the first argument is a TAR model, on the basis of which it computes the prediction. To learn more about the predict function for TAR models, run ?predict.TAR. The extension TAR signifies the particular predict function for processing prediction based on a TAR model.

yy=ts(c(log(predator.eq),pred.predator$fit),frequency $^ { \cdot = 2 }$ , start ${ } , = { }$ start(predator.eq))

This augments the point prediction values to the data.

plot(yy,type='n', ylim $\cdot ^ { = }$ range(c(yy,pred.predator$pred.interval)), ylab='Log Prey', xlab $^ { 1 = }$ expression(t))

This sets up a plot of the data and the predicted future values without actual plotting $\operatorname { \ t y p e = 1 n } !$ ). We anticipate superimposing the prediction intervals, so the range of the y-axis is specified through the ylim argument to the vector containing the minimum and maximum of the combined vector of the observed $^ +$ predicted values (yy) and the prediction limits (pred.predator$pred.interval), computed via the range function.

lines(log(predator.eq))

This draws the data as a solid line.

lines(window(yy, start ${ } , = { }$ end(predator.eq)+c(0,1)),lty=2)

This adds the curve of the predicted values as a dashed line.

lines(ts(pred.predator$pred.interval[2,], start ${ } , = { }$ end(predator.eq) $+ \mathbf { C }$ (0,1),freq $^ { \prime = 2 }$ ),lty=2)

This adds the upper prediction limits.

lines(ts(pred.predator$pred.interval[1,], start $=$ end(predator.eq) $+ \mathbf { C }$ (0,1),freq $: = 2$ ),lty=2)

This adds the lower prediction limits.

# Exhibit 15.24 on page 419. qqnorm(pred.predator$pred.matrix[,3])

The output of the predict function is a list that contains another component, named pred.matrix, which is a matrix containing all simulated future values, with the first column consisting of the simulated one-step-ahead values, the second column those of the two-steps-ahead values, and so forth.

qqnorm(pred.predator$pred.matrix[,3])

This extracts all 1000 simulated three-steps-ahead values, which are then passed into the qqnorm function to make the Q-Q normal score plot for these data.

qqline(pred.predator$pred.matrix[,6])

This adds the reference straight line for checking the normality of the three-steps-ahead conditional distribution.

Finally, here is a listing and brief description of all the new or enhanced functions that are contained in the TSA package.

<table><tr><td colspan="2">New or Enhanced Functions in the TSA Library</td></tr><tr><td>Function</td><td>Description</td></tr><tr><td>acf</td><td>Computes and plots the sample autocorrelation function starting with lag 1.</td></tr><tr><td>arima</td><td>This command has been amended to compute the AIC according to our definition.</td></tr><tr><td>arima/boot</td><td>Bootstraps time series according to a fitted ARMA(p,d,q) model.</td></tr><tr><td>arimax</td><td>Extends the arima function, allowing the incorporation of transfer functions and innovative and additive outliers.</td></tr><tr><td>ARMAspec</td><td>Computes and plots the theoretical spectrum of an ARMA model.</td></tr><tr><td>armasubsets</td><td>Finds “best subset” ARMA models.</td></tr><tr><td>BoxCox.ar</td><td>Finds a power transformation so that the transformed time series is approximately an AR process with normal error terms.</td></tr><tr><td>detectAO</td><td>Detects additive outliers in time series.</td></tr><tr><td>detectIO</td><td>Detects innovative outliers in time series.</td></tr><tr><td>eax</td><td>Computes and displays the extended autocorrelation function of a time series.</td></tr><tr><td>garch.sim</td><td>Simulates a GARCH process.</td></tr><tr><td>gBox</td><td>Performs a goodness-of-fit test for fitted GARCH models.</td></tr><tr><td>harmonic</td><td>Creates a matrix of the first m pairs of harmonic functions for fitting a harmonic trend (cosine-sine trend, Fourier regression) model with a time series response.</td></tr><tr><td colspan="2">New or Enhanced Functions in the TSA Library (Continued)</td></tr><tr><td>Function</td><td>Description</td></tr><tr><td>Keenan.test</td><td>Carries out Keenan's test for nonlinearity against the null hypothesis that the time series follows some AR process.</td></tr><tr><td>kurtosis</td><td>Calculates the (excess) coefficient of kurtosis.</td></tr><tr><td>lagplot</td><td>Computes and plots nonparametric regression functions of a time series against its various lags.</td></tr><tr><td>periodogram</td><td>Computes the periodogram of a time series.</td></tr><tr><td>LB.test</td><td>Computes the Ljung-Box or Box-Pierce tests checking whether or not the residuals from an ARIMA model appear to be white noise.</td></tr><tr><td>McLeod.Li.test</td><td>Perform the McLeod-Li test for conditional heteroscedasticity (ARCH).</td></tr><tr><td>plot.Arima</td><td>Plots a time series and its predictions (forecasts) with 95% prediction bounds based on a fitted ARIMA model.</td></tr><tr><td>predict.TAR</td><td>Calculates predictions based on a fitted TAR model. The errors are assumed to be normally distributed and the predictive distributions are approximated by simulation.</td></tr><tr><td>prewhiten</td><td>Bivariate time series are prewhitened according to an AR model fitted to the x-component of the bivariate series. Alternatively, if an ARIMA model is provided, it is used to prewhiten both series. The CCF of the prewhitened bivariate series is then computed and plotted.</td></tr><tr><td>qar.sim</td><td>Simulates a first-order quadratic AR model with normally distributed white noise error terms.</td></tr><tr><td>rstandard.Arima</td><td>Computes internally standardized residuals from a fitted ARIMA model.</td></tr><tr><td>runs</td><td>Tests the independence of a sequence of values by checking whether there are too many or too few runs above (or below) the median.</td></tr><tr><td>season</td><td>Extracts season information from a time series and creates a vector of the season information. For example, for monthly data, the function outputs a vector containing the months of the data.</td></tr><tr><td>skewness</td><td>Calculates the skewness coefficient of a dataset.</td></tr><tr><td>spec</td><td>Allows the user to invoke either the spec .pgram function or the spec .ar function in the stats package. The seasonal attribute of the data, if it exists, is suppressed for our preferred way of presenting the output. Alters defaults to demean=T, detrend=F, taper=0, and permits plotting of confidence interval bands.</td></tr><tr><td>summary.armasub-sets</td><td>Summary method for class armasubsets, that is useful for ARMA subset selection.</td></tr><tr><td>tar</td><td>Estimates a two-regime TAR model.</td></tr><tr><td>tar.sim</td><td>Simulates a two-regime TAR model.</td></tr><tr><td>tar.skeleton</td><td>Obtains the skeleton of a TAR model by suppressing the noise term in the TAR model.</td></tr><tr><td>tlrt</td><td>Carries out the likelihood ratio test for threshold nonlinearity, with the null hypothesis being a normal AR process and the alternative hypothesis being a TAR model with homogeneous, normally distributed errors.</td></tr><tr><td>Tsay.test</td><td>Carries out Tsay's test for quadratic nonlinearity in a time series.</td></tr><tr><td>tsdiag.Arima</td><td>Modifies the tsdiag function of the stats package suppressing initial residuals and displaying Bonferroni bounds. It also checks the condition for the validity of the chi-square asymptotics for the portmanteau tests.</td></tr><tr><td>tsdiag.TAR</td><td>Displays the time series plot and the sample ACF of the standardized residuals. Also, portmanteau tests for detecting auto-correlations in the standardized residuals are computed and displayed.</td></tr><tr><td>zlag</td><td>Computes the lag of a vector, with missing elements replaced by NA.</td></tr></table>

# DATASET INFORMATION

<table><tr><td>Filename/Variable(s)</td><td>Description and Source</td><td>Page(s)</td></tr><tr><td>airmiles</td><td>Monthly U.S. airline passenger-miles: 01/1996-05/2005. Source: www.bts.gov/XML/airTraffic/src/index.xml#MonthlySystem</td><td>249</td></tr><tr><td>airpass</td><td>Monthly total international airline passengers from 01/1960-12/1971. Source: Box, G. E. P., Jenkins, G. M., and Reinsel, G. C. Time Series Analysis: Forecasting and Control, second edition, Prentice-Hall, Engelwood Cliffs, NJ, 1994.</td><td>104</td></tr><tr><td>beersales</td><td>Monthly U.S. beer sales (in millions of barrels), 01/1975-12/1990. Source: Frees, E. W., Data Analysis Using Regression Models, Prentice-Hall, Engelwood Cliffs, NJ, 1996.</td><td>51</td></tr><tr><td>bluebird: (log.sales &amp; price)</td><td>Weekly unit sales of Bluebird standard potato chips (New Zealand) and their price for 104 weeks. From the website of Dr. Andrew Balemi. Source: www.stat.auckland.ac.nz/~balemi/Assn3.xls</td><td>267</td></tr><tr><td>bluebirdlite: (log.sales &amp; price)</td><td>Weekly unit sales of Bluebird Lite potato chips (New Zealand) and their price for 104 weeks. From the website of Dr. Andrew Balemi. Source: www.stat.auckland.ac.nz/~balemi/Assn3.xls</td><td>276</td></tr><tr><td>boardings: (log.coaching &amp; log.price)</td><td>Monthly public transit boardings (mostly buses and light rail), Denver, Colorado region, 08/2000-03/2006. Source: Personal communication from Lee Cryer, Project Manager, Regional Transportation District, Denver, Colorado. Denver gasoline prices were obtained from the Energy Information Administration, U.S. Department of Energy, Washington, D.C., at www.eia.doe.gov</td><td>248, 271, 273</td></tr><tr><td>co2</td><td>Monthly carbon dioxide levels in northern Canada, 01/1994-12/2004. Source: http://cdiac.ornl.gov/ftp/trends/co2/altso.co2</td><td>234, 234</td></tr><tr><td>color</td><td>Color properties from 35 consecutive batches of an industrial chemical process. Source: Cryer, J. D. and Ryan, T. P., “The estimation of sigma for an X chart”, Journal of Quality Technology, 22, No. 3, 187-192.</td><td>3, 134, 147, 165, 176, 194</td></tr><tr><td>CREF</td><td>Daily values of one unit of the CREF (College Retirement Equity Fund) Stock fund, 08/26/04-08/15/06. Source: www.tiaa-refs.org/performance/retirement/data/index.html</td><td>278</td></tr><tr><td>cref.bond</td><td>Daily values of one unit of the CREF (College Retirement Equity Fund) Bond fund, 08/26/04-08/15/06. Source: www.tiaa-refs.org/performance/retirement/data/index.html</td><td>316</td></tr><tr><td>days</td><td>Accounts receivable data. Number of days until a distributor of Win-147, 174, tegard Company products pays their account. Source: Personal communication from Mark Selergren, Vice President, Winegard, Inc., Burlington, Iowa.</td><td>217, 276</td></tr><tr><td>Filename/ 
Variable(s)</td><td>Description and Source (Continued)</td><td>Page(s)</td></tr><tr><td>deere1</td><td>82 consecutive values for the amount of deviation (in 0.000025 inch units) from a specified target value that an industrial machining process at Deere &amp; Co. produced under certain specified operating conditions. Source: Personal communication from William F. Fulkerson, Deere &amp; Co. Technical Center, Moline, Illinois.</td><td>146, 275</td></tr><tr><td>deere2</td><td>102 consecutive values for the amount of deviation (in 0.0000025 inch units) from a specified target value that another industrial machining process produced at Deere &amp; Co. Source: Personal communication from William F. Fulkerson, Deere &amp; Co. Technical Center, Moline, Illinois.</td><td>146</td></tr><tr><td>deere3</td><td>57 consecutive values from a complex machine tool at Deere &amp; Co. The values given are deviations from a target value in units of ten millionths of an inch. The process employs a control mechanism that resets some of the parameters of the machine tool depending on the magnitude of deviation from target of the last item produced. Source: Personal communication from William F. Fulkerson, Deere &amp; Co. Technical Center, Moline, Illinois.</td><td>147, 174, 190, 217</td></tr><tr><td>eeg</td><td>An electroencephalogram (EEG) is a noninvasive test used to detect and record the electrical activity generated in the brain. These data were measured at a frequency of 256 per second and came from a patient suffering a seizure. This is a portion of a series on the website of Professor Richard Smith, University of North Carolina. His source: Professors Mike West and Andrew Krystal, Duke University. Source: http://www.stat.unc.edu/faculty/rs/s133/Data/datadoc.html</td><td>380</td></tr><tr><td>electricity</td><td>Monthly U.S. electricity generation (in millions of kilowatt hours) of all types: coal, natural gas, nuclear, petroleum, and wind, 01/1973-12/2005. Source: www.eia.doe.gov/emeu/mer/elect.html</td><td>99, 214, 247, 264, 380</td></tr><tr><td>euph</td><td>A digitized sound file of about 0.4 seconds of a Bb just below middle C played on a euphonium by one of the authors (JDC), a member of the group Tempered Brass.</td><td>374</td></tr><tr><td>flow</td><td>Flow data (in cubic feet per second) for the Iowa River measured at Wapello, Iowa, for the period 09/1958-08/2006. Source: http://waterdata.usgs.gov/ia/nwis/sw</td><td>372, 381</td></tr><tr><td>gold</td><td>Daily price of gold (in U.S. dollars per trio ounce), 01/04/2005-12/30/2005. Source: www.lbma.org.uk/2005dailygold.htm</td><td>105</td></tr><tr><td>google</td><td>Daily returns of Google stock from 08/20/04 to 09/13/06. Source:http://finance.yahoo.com/q/hp?s=GOOG</td><td>317</td></tr><tr><td>hare</td><td>Annual Canadian hare abundance, 1905–1935. Source: Stenseth, N. C., Falck, W., Bjørnstad, O. N. and Krebs, C. J. (1997) “Population regulation in snowshoe hare and Canadian lynx: Asymmetric food web configurations between hare and lynx.” Proceedings of the National Academy of Sciences, USA, 94, 5147–5152.</td><td>4, 136, 152, 176, 206</td></tr><tr><td>hours</td><td>Monthly average hours worked per week in the U.S. manufacturing sector for 07/1982–06/1987. Source: Cryer, J. D. Time Series Analysis, Duxbury Press, Boston, 1986.</td><td>51</td></tr><tr><td>JJ</td><td>Quarterly earnings per share for 1960Q1–1980Q4 of the U.S. company, Johnson &amp; Johnson, Inc. From the web site of David Stoffer. Source: www.stat.pitt.edu/stoffer/tsa2/</td><td>105, 248</td></tr><tr><td>larain</td><td>Annual rainfall totals for Los Angeles, California, 1878–1992. Source: Personal communication from Professor Donald Bentley, Pomona College, Claremont, California. For more data see www.wrh.noaa.gov/lox/climate/cvc.php</td><td>1, 49, 105, 133, 379</td></tr><tr><td>milk</td><td>Monthly U.S. milk production from 01/1994 to 12/2005. Source: National Agricultural Statistics Service: usda.mannlib .cornell.edu/MannUsda/viewDocumentInfo.do?documentID=1103</td><td>264, 374, 374</td></tr><tr><td>oil.price</td><td>Monthly spot price for crude oil, Cushing, OK (in U.S. dollars per barrel), 01/1986–01/2006. U.S. Energy Information Administration. Source: tonto.eia.doe.gov/dnav/pet/hist/rwtcM.htm</td><td>87, 125, 153, 177, 276, 317</td></tr><tr><td>oilfilters</td><td>Monthly wholesale specialty oil filter sales, Deere &amp; Co., 07/1983–06/1987. Source: Personal communication from William F. Fulkerson, Deere &amp; Co. Technical Center, Moline, Illinois.</td><td>6</td></tr><tr><td>prescrip</td><td>Monthly U.S. average prescription costs for the months 08/1986 - 03/1992. Source: Frees, E. W., Data Analysis Using Regression Models, Prentice-Hall, Engelwood Cliffs, NJ, 1996.</td><td>52</td></tr><tr><td>retail</td><td>Monthly total UK (United Kingdom) retail sales (non-food stores in billions of pounds), 01/1983–12/1987. Source: www.statistics.gov.uk/statbase/TSDdownload1.asp</td><td>52</td></tr><tr><td>robot</td><td>Final position in the “x” direction of an industrial robot put through a series of planned exercises many times. Source: Personal communication from William F. Fulkerson, Deere &amp; Co. Technical Center, Moline, Illinois.</td><td>147, 174, 190, 217, 370</td></tr><tr><td>SP</td><td>Quarterly S&amp;P Composite Index, 1936Q1–1977Q4, Source: Frees, E. W., Data Analysis Using Regression Models, Prentice-Hall, Engelwood Cliffs, NJ, 1996.</td><td>104</td></tr><tr><td>spots</td><td>Annual American (relative) sunspot numbers collected from 1945 to 2005. The annual (relative) sunspot number is a weighted average of solar activity measured from a network of observatories. Source: www.ngdc.noaa.gov/stp/SOLAR/ftpsunspotnumber.html# american</td><td>392</td></tr><tr><td>spots1</td><td>Annual international sunspot numbers, 1700–2005, NOAA National Geophysical Data Center. Source: ftp.ngdc.noaa.gov/STP/SOLAR_DATA/SUNSPOT_NUMBERS/ YEARLY.PLT</td><td>379</td></tr><tr><td>star</td><td>Brightness of a variable star at midnight on 600 successive nights. Source: www.statsci.org/data/general/star.html</td><td>325</td></tr><tr><td>tbone</td><td>A digitized sound file of about 0.4 seconds of a Bb just below middle C played on a tenor trombone by Chuck Kreeb, a member of Tem- pered Brass and a friend of one of the authors.</td><td>374</td></tr><tr><td>tempdub</td><td>Monthly average temperatures in Dubuque, Iowa, 1/1964–12/1975. Source: http://mesonet.agron.iastate.edu/climodat/index.phtml? station=ia2364&amp;report=16</td><td>6, 213, 379</td></tr><tr><td>tuba</td><td>A digitized sound file of about 0.4 seconds of a Bb an octave and one whole step below middle C played on a BBb tuba by Linda Fisher, a member of Tempered Brass and a friend of one of the authors.</td><td>381</td></tr><tr><td>units</td><td>Annual sales of certain large equipment, 1983–2005. (Proprietary sales data from a large international company.)</td><td>276</td></tr><tr><td>usd.hkd</td><td>Daily exchange rates of U.S. dollar to Hong Kong dollar, 01/2005–03/2006. A data frame with 431 observations on the following six variables. r: daily returns of USD/HKD exchange rates v: estimated conditional variances based on an AR(1)+GARCH(3,1) hkrate: daily USD/HKD exchange rates outlier1: dummy variable of day 203, corresponding to July 22, 2005 outlier2: dummy variable of day 290, another possible outlier day: calendar day Source: www.oanda.com/convert/fxhistory</td><td>310</td></tr><tr><td>veilleux: Day, Didinium, Paramecium</td><td>A bivariate time series from an experiment studying prey-predator dynamics. The first time series consists of the number of prey individuals (Didinium natsutum) per ml measured every 12 hours over a period of 35 days. The second time series consists of the corresponding number of predators (Paramecium aurelia) per ml. Source: Veilleux, B. G. (1976) “The analysis of a predatory interaction between Didinium and Paramecium.” MSc thesis, University of Alberta, Canada. See also www.journals .royalsoc.ac.uk/content/lekv0yqp2ecpabvd/archive1.pdf</td><td>386</td></tr></table>

Filename/ Description and Source (Continued) Page(s) Variable(s)   

<table><tr><td>wages</td><td>Monthly average hourly wages in the U.S. apparel industry: 07/1981–06/1987. Source: Cryer, J. D. Time Series Analysis, Duxbury Press, Boston, 1986.</td></tr></table>

<table><tr><td>winnebago</td><td>Monthly unit sales of recreational vehicles from Winnebago, Inc. 51, 104 from 11/1966 to 02/1972. Source: Roberts, H. V., Data Analysis for Managers with Minitab, second edition, The Scientific Press, Redwood City, CA, 1991.</td></tr></table>

# BIBLIOGRAPHY

Abraham, B. and Ledolter, J. (1983). Statistical Methods for Forecasting. New York: John Wiley & Sons.   
Akaike, H. (1973). “Maximum likelihood identification of Gaussian auto-regressive moving-average models.” Biometrika, 60, 255–266.   
Akaike, H. (1974). “A new look at the statistical model identification.” IEEE Transactions on Automatic Control, 19, 716–723.   
Andersen, T. G., Bollerslev, T., Christoffersen, P. F., and Diebold, F. X. (2006). “Volatility Forecasting.” To appear in Handbook of Economic Forecasting, edited by Graham Elliott, Clive W. J. Granger, and Allan Timmermann, Amsterdam: North-Holland.   
Anderson, T. W. (1971). The Statistical Analysis of Time Series. New York: John Wiley & Sons.   
Banerjee, A., Dolado, J. J., Galbraith, J. W. and Hendry, D. F. (1993). Cointegration, Error Correction, and the Econometric Analysis of Non-Stationary Data. Oxford: Oxford University Press.   
Bartlett, M. S. (1946). “On the theoretical specification of sampling properties of autocorrelated time series.” Journal of the Royal Statistical Society B, 8, 27–41.   
Becuinj, M., Gourieroucx, S., and Monfort, A. (1980). “Identification of a mixed autoregressive-moving average process: The corner method.” In Time Series, edited by O. D. Anderson, 423–436. Amsterdam: North-Holland.   
Bloomfield, P. (2000). Fourier Analysis of Time Series: An Introduction, 2nd ed. New York: John Wiley & Sons.   
Bollerslev, T. (1986). “Generalized autoregressive conditional heteroskedasticity.” Journal of Econometrics, 31, 307–327.   
Box, G. E. P. and Cox, D. R. (1964). “An analysis of transformations.” Journal of the Royal Statistical Society B, 26, 211–243.   
Box, G. E. P. and Pierce, D. A. (1970). “Distribution of residual correlations in autoregressive-integrated moving average time series models.” Journal of the American Statistical Association, 65, 1509–1526.   
Box, G. E. P. and Jenkins, G. M. (1976). Time Series Analysis, Forecasting and Control. San Francisco: Holden-Day.   
Box, G. E. P., Jenkins, G. M., and Reinsel, G. C. (1994). Time Series Analysis, Forecasting and Control, 2nd ed. New York: Prentice-Hall.

Box, G. E. P. and Tiao, G. (1975). “Intervention analysis with applications to economic and environmental problems.” Journal of the American Statistical Association, 70, 70–79.   
Brillinger, D. R. (2001). Time Series: Data Analysis and Theory. Philadelphia, SIAM.   
Brock, W. A., Deckert, W. D., and Seheinkman, J. A. (1996). “A test for independence based on the correlation dimension.” Econometric Reviews, 15, 197–235.   
Brockwell, P. J. and Davis, R. A. (1991). Time Series: Theory and Methods. 2nd ed. New York: Springer.   
Brockwell, P. J. and Davis, R. A. (2002). Introduction to Time Series and Forecasting, 2nd ed. New York: Springer.   
Brown, R. G. (1962). Smoothing, Forecasting and Prediction of Discrete Time Series. Englewood Cliffs, NJ: Prentice-Hall.   
Chan, K. S. (1991). “Percentage points of likelihood ratio tests for threshold autoregression.” Journal of the Royal Statistical Society B, 53, 3, 691–696.   
Chan, K. S. (1993). “Consistency and limiting distribution of the least squares estimator of a threshold autoregressive model.” Annals of Statistics, 21, 1, 520–533.   
Chan, K. S. (2008). “A new look at model diagnostics, with applications to time series analysis.” Unpublished manuscript.   
Chan, K. S., Mysterud, A., Oritsland, N. A., Severinsen, T., and Stenseth, N. C. (2005). “Continuous and discrete extreme climatic events affecting the dynamics of a high Arctic reindeer population.” Oecologia, 145, 556–563.   
Chan, K. S., Petruccelli, J. D., Tong, H. and Woolford, S. W. (1985). “A multiple threshold AR(1) model.” Journal of Applied Probability, 22, 267–279.   
Chan, K. S. and Tong, H. (1985). “On the use of the deterministic Lyapunov function for the ergodicity of stochastic difference equations.” Advances in Applied Probability, 17, 666–678.   
Chan, K. S. and Tong, H. (1994). “A note on noisy chaos.” Journal of the Royal Statistical Society B, 56, 2, 301–311.   
Chan, K. S. and Tong, H. (2001). Chaos: A Statistical Perspective. New York: Springer-Verlag.   
Chan, K. S. and Tsay, R. S. (1998). “Limiting properties of the least squares estimator of a continuous threshold autoregressive model.” Biometrika, 85, 413–426.   
Chan, W. S. (1999). “A comparison of some pattern identification methods for order determination of mixed ARMA models.” Statistics and Probability Letters, 42, 69–79.   
Chang, I., Tiao, G. C., and Chen, C. (1988). “Estimation of time series parameters in the presence of outliers.” Technometrics, 30, 2, 193–204.

Chang, Y. and Park, J. Y. (2002). “On the asymptotics of ADF tests for unit roots.” Econometric Reviews, 21, 431–447.   
Chatfield, C. (2004). The Analysis of Time Series, 6th ed. London: Chapman and Hall.   
Chen, Y. T. and Kuan, C. M. (2006). “A generalized Jarque-Bera test of conditional normality.” www.sinica.edu.tw/~ckuan/pdf/jb01.pdf   
Cheng, B. and Tong, H. (1992). “On consistent nonparametric order determination and chaos (Disc: pp. 451-474).” Journal of the Royal Statistical Society B, 54, 427–449.   
Cline, D. B. H. and Pu, H. H. (2001). “Stability of nonlinear time series: What does noise have to do with it?.” In Selected Proceedings of the Symposium on Inference for Stochastic Processes, IMS Lecture Notes Monograph Series, Volume 37, Edited by I. V. Basawa, C. C. Heyde, and R. L. Taylor, 151–170. Beachwood, OH: Institute of Mathematical Statistics.   
Cooley, J. W. and Tukey, J. W. (1965). “An algorithm for the machine calculation of complex Fourier series.” Mathematics of Computation, 19, 297–301.   
Cramér, H. and Leadbetter, M. R. (1967). Stationary and Related Random Processes. New York: John Wiley & Sons.   
Cryer, J. D. and Ledolter, J. (1981). “Small-sample properties of the maximum likelihood estimator in the first-order moving average model.” Biometrika, 68, 3, 691–694.   
Cryer, J. D., Nankervis, J. C., and Savin, N. E. (1989). “Mirror-Image and Invariant Distributions in Arma Models.” Econometric Theory, 5, 1, 36–52.   
Cryer, J. D., Nankervis, J. C., and Savin, N. E. (1990). “Forecast Error Symmetry in ARIMA Models.” Journal of the American Statistical Association, 85, 41, 724–728.   
Cryer, J. D. and Ryan, T. P. (1990). “The estimation of sigma for an X chart.” Journal of Quality Technology, 22, 3, 187–192.   
Davies, N., Triggs, C. M., and Newbold, P. (1977). “Significance levels of the Box-Pierce portmanteau statistic in finite samples.” Biometrika, 64, 517–522.   
Davison, A. C. and Hinkley, D. V. (2003). Bootstrap Methods and Their Application, 2nd ed. New York: Cambridge University Press.   
Diggle, P. J. (1990). Time Series: A Biostatistical Introduction. Oxford: Oxford University Press.   
Draper, N. R. and Smith, H. (1981). Applied Regression Analysis, 2nd ed. New York: John Wiley & Sons.   
Durbin, J. (1960). “The fitting of time series models.” Review of the International Institute of Statistics, 28, 233–244.

Durbin, J. (1970). “Testing for serial correlation in least-squares regression when some of the regressors are lagged independent variables.” Econometrika, 38, 410-421.   
Durbin, J. and Koopman, S. J. (2001). Time Series Analysis by State Space Methods. Oxford: Oxford University Press.   
Durbin, J. and Watson, G. S. (1950). “Testing for serial correlation in least-squares regression: I.” Biometrika, 37, 409–428.   
Durbin, J. and Watson, G. S. (1951). “Testing for serial correlation in least-squares regression: II.” Biometrika, 38, 1–19.   
Durbin, J. and Watson, G. S. (1971). “Testing for serial correlation in least-squares regression: III.” Biometrika, 58, 409–428.   
Efron, B. and Tibshirani, R. J. (1993). An Introduction to the Bootstrap. New York: Chapman and Hall.   
Engle, R. F. (1982). “Autoregressive conditional heteroscedasticity with estimates of the variance of U.K. inflation.” Econometrica, 50, 987–1007.   
Fay, G., Moulines, E., and Soulier, P. (2002). “Nonlinear functionals of the periodogram.” Journal of Time Series Analysis, 23, 5, 523–553.   
Fan, J. and Gijbels, I. (1996). Local Polynomial Modeling and Its Applications. London: Chapman and Hall.   
Fan, J., and Kreutzberger, E. (1998). “Automatic local smoothing for spectral density estimation.” Scandinavian Journal of Statistics, 25, 2, 359–369.   
Fuller, W. A. (1996). Introduction to Statistical Time Series, 2nd ed. New York: John Wiley & Sons.   
Furnival, G. M. and Wilson, Jr., R. W. (1974). “Regressions by leaps and bounds.” Technometrics, 16, 4, 499–511.   
Gardner, G., Harvey, A. C. and Phillips, G. D. A. (1980). Algorithm AS154. “An algorithm for exact maximum likelihood estimation of autoregressive-moving average models by means of Kalman filtering.” Applied Statistics, 29, 311–322.   
Gentleman, W. M. and Sande, G. (1966). “Fast Fourier transforms—for fun and profit.” Proc. American Federation of Information Processing Society, 29, 563–578.   
Geweke, J. and Terui, N. (1993). “Bayesian threshold autoregressive models for nonlinear time series.” Journal of Time Series Analysis, 14, 441–454.   
Goldberg, S. I. (1958). Introduction to Difference Equations. New York: Science Editions.   
Granger, C. W. J. and Teräsvirta, T. (1993). Modelling Nonlinear Economic Relationships. New York: Oxford University Press.   
Hannan, E. J. (1970). Multiple Time Series. New York: John Wiley & Sons.

Hannan, E. J. (1973). “The asymptotic theory of linear time-series models.” Journal of Applied Probability, 10, 130–145.   
Hannan, E. J. and Rissanen, J. (1982). “Recursive estimation of mixed autoregressive-moving average order.” Biometrika, 69, 81–94.   
Harvey, A. C. (1981a). The Econometric Analysis of Time Series. Oxford: Phillip Allen.   
Harvey, A. C. (1981b). “Finite sample prediction and overdifferencing.” Journal of Times Series Analysis, 2, 221–232.   
Harvey, A. C. (1981c). Time Series Models. New York: Halsted Press.   
Harvey, A. C. (1989). Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge: Cambridge University Press.   
Harvey, A. C. (1990). The Econometric Analysis of Time Series, 2nd ed. Boston: MIT Press.   
Harvey, A. C. (1993). Time Series Models, 2nd ed. New York: Harvester Wheatsheaf.   
Harvey, A., Koopman, S. J., and Shephard, N. (2004). State Space and Unobserved Component Models: Theory and Applications. New York: Cambridge University Press.   
Hasza, D. P. (1980). The asymptotic distribution of the sample autocorrelation for an integrated ARMA process.” Journal of the American Statistical Association, 75, 349–352.   
Hurvich, C. M. and Tsai, C. L. (1989). “Regression and time series model selection in small samples.” Biometrika, 76, 2, 297–307.   
Jenkins, G. M. and Watts, D. G. (1968). Spectral Analysis and Its Applications. San Francisco: Holden-Day.   
Jiang, J. and Hui, Y. V. (2004). “Spectral density estimation with amplitude modulation and outlier detection.” Annals of the Institute of Statistical Mathematics, 56, 4, 611.   
Jones, R. H. (1980). “Maximum likelihood fitting of ARMA models to time series with missing observations.” Technometrics, 20, 389–395.   
Jost, C. and Ellner, S. P. (2000). “Testing for predator dependence in predator-prey dynamics: A non-parametric approach.” Proceedings of the Royal Society B: Biological Sciences, 267, 1453, 1611–1620.   
Kakizawa, Y. (2006). “Bernstein polynomial estimation of a spectral density”. Journal of Time Series Analysis, 27, 2, 253–287.   
Keenan, D. (1985). “A Tukey nonlinear type test for time series nonlinearities.” Biometrika, 72, 39–44.   
Kooperberg, C., Stone, C. J., and Truong, Y. K. (1995). “Logspline estimation of a possibly mixed spectral distribution.” Journal of Time Series Analysis, 16, 359–388.

Lai, T. L. and Wei, C. Z. (1983). “Asymptotic properties of general autoregressive models and strong consistency of least squares estimates of their parameters.” Journal of Multivariate Analysis, 13, 1–13.   
Levinson, N. (1947). “The Weiner RMS error criterion in filter design and prediction.” Journal of Mathematical Physics, 25, 261–278.   
Li, W. K. (2004). Diagnostic Checks in Time Series. London: Chapman and Hall.   
Li, W. K. and Mak, T. K. (1994). “On the squared residual autocorrelations in non-linear time series with conditional heteroskedasticity.” Journal of Time Series Analysis, 15, 627–636.   
Ling, S. and McAleer, M. (2002). “Stationarity and the existence of moments of a family of GARCH processes.” Journal of Econometrics, 106, 109–117.   
Ljung, G. M. and Box, G. E. P. (1978). “On a measure of lack of fit in time series models.” Biometrika, 65, 553–564.   
Luukkonen, R., Saikkonen, P., and Teräsvirta, T. (1988). “Testing linearity against smooth transition autoregressive models.” Biometrika, 75, 491–499.   
MacLulich, D. A. (1937). Fluctuations in the number of the varying hare (Lepus americanus). Toronto: University of Toronto Press.   
May, R. M. (1976). “Simple mathematical models with very complicated dynamics.” Nature, 261, 459–467.   
McLeod, A. I. (1978). “On the distribution of residual autocorrelations in Box-Jenkins models.” Journal of the Royal Statistical Society A, 40, 296–302.   
McLeod, A. I. and W. K. Li (1983). “Diagnostic checking ARMA time series models using squared residual autocorrelations.” Journal of Time Series Analysis, 4, 269–273.   
Montgomery, D. C. and Johnson, L. A. (1976). Forecasting and Time Series Analysis. New York: McGraw-Hill.   
Nadaraya, E. A. (1964). “On estimating regression.” Theory of Probability and Its Applications, 9, 141–142.   
Nelson, C. R. (1973). Applied Time Series Analysis for Managerial Forecasting. San Francisco: Holden-Day.   
Nelson, D. B. and Cao, C. Q. (1992). “Inequality constraints in the univariate GARCH model.” Journal of Business and Economic Statistics, 10, 229–235.   
Ong, C. S., Huang, J. J., and Tzeng, G. H. (2005). “Model Identification of ARIMA family using genetic algorithms.” Applied Mathematics and Computation, 164, 885–912.   
Parzen, E. (1982). “ARARMA models for time series analysis and forecasting.” Journal of Forecasting 1, 67–82.

Percival, D. B. and Walden, A. T. (1993). Spectral Analysis for Physical Applications. Cambridge: Cambridge University Press.   
Phillips, P. C. B. (1998). “New tools for understanding spurious regressions.” Econometrica, 66, 1299–1325.   
Phillips, P. C. B. and Xiao, Z. (1998). “A primer on unit root testing.” Journal of Economic Surveys, 12, 5, 423–469.   
Politis, D. N. (2003). “The impact of bootstrap methods on time series analysis.” Statistical Science, 18, 2, 219-230.   
Priestley, M. B. (1981). Spectral Analysis and Time Series, Volumes 1 and 2. New York: Academic Press.   
Quenoulle, M. H. (1949). “Approximate tests of correlation in time series.” Journal of the Royal Statistical Society B, 11, 68–84.   
Roberts, H. V. (1991). Data Analysis for Managers with Minitab, second edition. Redwood City, CA, The Scientific Press.   
Roy, R. (1977). “On the asymptotic behaviour of the sample autocovariance function for an integrated moving average process.” Biometrika, 64, 419–421.   
Royston, P. (1982). “An extension of Shapiro and Wilk’s W test for normality to large samples.” Applied Statistics, 31, 115–124.   
Said, S. E. and Dickey, D. A. (1984). “Testing for unit roots in autoregressive-moving average models of unknown order.” Biometrika, 71, 599–607.   
Samia, N. I., Chan, K. S., and Stenseth, N. C. (2007). “A generalised threshold mixed model for analysing nonnormal nonlinear time series; with application to plague in Kazakhstan.” Biometrika, 94, 101–118.   
Schuster, A. (1897). “On lunar and solar periodicities of earthquakes.” Proceedings of the Royal Society, 61, 455–465.   
Schuster, A. (1898). “On the investigation of hidden periodicities with application to a supposed 26 day period of meteorological phenomena.” Terrestrial Magnetism, 3, 13–41.   
Shephard, N. (1996). “Statistical aspect of ARCH and stochastic volatility.” In Time Series Models: In Econometrics, Finance and Other Fields, edited by D. R. Cox, D. V. Hinkley, and O. E. Barndorff-Nielsen. London: Chapman and Hall. 1–55.   
Shibata, R. (1976). “Selection of the order of an autoregressive model by Akaike’s information criterion.” Biometrika, 63, 1, 117–126.   
Shin, K-I. and Kang, H-J. (2001). “A study on the effect of power transformation in the ARMA $( p , q )$ model.” Journal of Applied Statistics, 28, 8, 1019–1028.   
Shumway, R. H. and Stoffer, D. S. (2006). Time Series Analysis and Its Applications (with R Examples), 2nd ed. New York: Springer.

Slutsky, E. (1927). “The summation of random causes as the source of cyclic processes” (in Russian). In Problems of Economic Conditions, English translation (1937) in Econometrca, 5, 105–146.   
Stige, L. C., Stave, J., Chan, K-S, Ciannelli, L., Pettorelli, N., Glantz, M., Herren, H. R., and Stenseth, N. (2006). “The effect of climate variation on agro-pastoral production in Africa.” Proceedings of the National Academy of Science, 103, 9, 3049–3053   
Stenseth, N. C., Chan, K. S., Tavecchia, G., Coulson T., Mysterud, A., Clutton-Brock, T., and Grenfell, B. (2004). “Modelling non-additive and nonlinear signals from climatic noise in ecological time series: Soay sheep as an example.” Proceedings of the Royal Society of London Series B: Biological Sciences, 271, 1985–1993.   
Stenseth, N. C., Chan, K. S., Tong, H., Boonstra, R., Boutin, S., Krebs, C. J., Post, E., O'Donoghue, M., Yoccoz, N. G., Forchhammer, M. D., and Hurrell, J. W. (1999). “Common dynamic structure of Canada lynx populations within three climatic regions.” Science, August 13, 1071–1073.   
Stenseth, N. C., Falck, W., Bjørnstad, O. N., and Krebs, C. J. (1997). “Population regulation in snowshoe hare and Canadian lynx: Asymmetric food web configurations between hare and lynx.” Proceedings of the National Academy of Science USA, 94, 5147–5152.   
Taylor, S. J. (1986). Modeling Financial Time Series. Chichester: John Wiley & Sons.   
The R Development Core Team (2006a). R: A Language and Environment for Statistical Computing Reference Index, Version 2.4.1 (2006-12-18).   
The R Development Core Team (2006b). R Data Import/Export, Version 2.4.1 (2006-12-18).   
Tong, H. (1978). “On a threshold model.” In Pattern Recognition and Signal Processing, edited by C. H. Chen. Amsterdam: Sijthoff and Noordhoff.   
Tong, H. (1983). Threshold Models In Non-linear Time Series Analysis. New York: Springer-Verlag. 101–141.   
Tong, H. (1990). Non-linear Time Series. Oxford: Clarendon Press.   
Tong, H. (2007). “Birth of the threshold time series model.” Statistica Sinica, 17, 8–14.   
Tong, H. and Lim, K. S. (1980). “Threshold autoregression, limit cycles and cyclical data (with discussion).” Journal of the Royal Statistical Society B, 42, 245–292.   
Tsai, H. and Chan, K. S. (2006). A note on the non-negativity of continuous-time ARMA and GARCH processes. Technical Report No. 359, Department of Statistics & Actuarial Science, The University of Iowa.   
Tsay, R. S. (1984). “Regression models with time series errors.” Journal of the American Statistical Association, 79, 385, 118–24.

Tsay, R. S. (1986). “Nonlinearity tests for time series.” Biometrika, 73, 461–466.   
Tsay, R. S. (2005). Analysis of Financial Time Series, 2nd ed. New York: John Wiley & Sons.   
Tsay, R. S. and Tiao, G. (1984). “Consistent estimates of autoregressive parameters and extended sample autocorrelation function for stationary and nonstationary ARMA Models.” Journal of the American Statistical Association, 79, 385, 84–96.   
Tsay, R. and Tiao, G. (1985). “Use of canonical analysis in time series model identification.” Biometrika, 72, 299–315.   
Tufte, E. (1983). The Visual Display of Quantitative Information. Cheshire, CT.: Graphics Press.   
Tukey, J. W. (1949). “One degree of freedom for non-additivity.” Biometrics, 5, 232–242.   
Veilleux, B. G. (1976). “The analysis of a predatory interaction between Didinium and Paramecium.” MSc thesis, University of Alberta, Canada.   
Venables, W. N. and Ripley, B. D. (2002). Modern Applied Statistics with S, 4th ed. New York: Springer.   
Venables, W. N., Smith, D. M., and the R Development Core Team (2006). An Introduction to R: Notes on R: A Programming Environment for Data Analysis and Graphics. Version 2.4.1 (2006-12-18).   
Watson, G. S. (1964). “Smooth Regression Analysis.” Sankhy , 26, 359–372.a   
Wei, W. W. S. (2005). Time Series Analysis, 2nd ed. Redwood City, CA: Addison-Wesley.   
Wichern, D. W. (1973). “The behavior of the sample autocorrelation function for an integrated moving average process.” Biometrika, 60, 235–239.   
Wiener, N. (1958). Nonlinear Problems in Random Theory. Cambridge, MA: MIT Press.   
White, H. (1989). “An additional hidden unit test for neglected nonlinearities in multilayer feedforward networks.” In Proceedings of the International Joint Conference on Neural Networks, New York: IEEE Press, 451–455.   
Whittaker, E. T. and Robinson, G., (1924). The Calculus of Observations. London: Blackie and Son.   
Wold, H. O. A. (1938). A Study of the Analysis of Stationary Time Serie. (2nd ed. 1954). Uppsala: Almqvist and Wiksells.   
Wold, H. O. A. (1948). “On prediction in stationary time series.” The Annals of Mathematical Statistics, 19, 558–567.

Yeo, I-K and Johnson, R. A. (2000) “A new family of power transformations to improve normality or symmetry.” Biometrika 87, 954–959.   
Yoshihide, K. (2006). “Bernstein polynomial estimation of a spectral density.” Journal of Time Series Analysis, 27, 2, 253–287.   
Yule, G. U. (1926). “Why do we sometimes get nonsense-correlations between time-series? — A study in sampling and the nature of time-series.” Journal of the Royal Statistical Society, 89, 1, 1–63.

# INDEX

# A

additive outlier (AO) 257

augmented Dickey-Fuller test 129

airline model 241

Akaike’s

information criterion (AIC) 130

Markovian representation 223

aliased frequencies 326

amplitude 34, 319

AR characteristic polynomial 71

AR(1) 66

AR(2) 71

$\boldsymbol { \Psi }$ -coefficients 75

ARCH

model 285

order q 289

ARMA(1,1) model 77

attractor 407

autocorrelation function 11

extended 115

for ARMA(p,q) 85

partial 113

residuals 180

sample or estimated 46

sample properties 109

autocovariance function 11

sample or estimated 329

autoregressive process

general 66, 76

order one 66

order two 71

averaged sample spectral density 351

# B

backshift (lag) operator 106

bandwidth 356

bandwidth guide 357

Bartlett lag window 377

Bayesian information criterion (BIC) 131

best linear unbiased estimates (BLUE) 40

Black-Scholes formula 307

blue spectrum 333

Bonferroni rule 258

bootstrap 445

bootstraping ARIMA models 167

# C

Canadian hare 4

causal filter 331

coefficient of determination 41

color property 3

comma-separated values 427

complementary function 221

complex conjugate 308

concentrated log-likelihood function 226

conditional

expectation 218

sum-of-squares function 154

volatility 285

conditional variance process 277

confidence interval guide 357

constant terms

ARIMA models 97

contemporaneous 260

convolution

discrete time 331

correlation 26

correlogram 46

cosine

bell 360

split 360

trend 34

wave 18

covariance 25

matrix 224

CREF

bond fund 316

stock fund 278

cross-correlation function 260

sample 261

cyclical trend 32

# D

damping factor 73

Daniell spectral window 352

modified 353

De Moivre’s theorem 350

delay 251

Denver public transportation usage 271

deterministic trend 27

diagnostics 8

Dickey-Fuller unit-root test 128

difference 90

seasonal 233

second 91

Dirichlet spectral window 359

modified 382

tapering 381

discrete Fourier transform 329

discrete spectrum 328

discrete time convolution 331

distributed lag model 267

distribution

heavy-tailed 284

light-tailed 284

distribution function

spectral 328

dynamic regression model 267

# E

equations of state 223

equilibrium point 398

ergodicity 398

error variance

forecast 192

Euler’s formulas 350

euphonium 374, 381

EWMA (exponentially weighted

moving average)

smoothing constant 209

expectation

conditional 218

expected value 24

exponentially weighted moving average

(EWMA) 208

extended autocorrelation function 115

# F

filter 265, 331

first difference 90

first-order

autoregressive process, AR(1) 66

moving average process, MA(1) 57

fitting 8

forecast

ARMA(p,q) 199

error variance 192

lead time 191

MA(1) model 197

nonstationary models 201

origin 191

random walk with drift 198

unbiased 192

with differencing 209

with log transformations 210

forecast error 192

one-step-ahead 195

Fourier frequencies 38, 321

Fourier transform

discrete 329

frequency 34, 73, 319

frequency domain analysis 319

# G

GARCH

order (p, q) 289

weak stationarity condition 296

general autoregressive process

AR(p) 76

general linear process 55

generalized autoregressive conditional heteroscedasticity 289

generalized least squares (GLS) 40

GJR model 311

globally exponentially stable limit point, 398

Google stock returns 317, 472

# H

half-life 251

hare 4

heavy-tailed 284

# I

IGARCH(1,1) 297

innovation 257, 285

innovative outlier (IO) 257

integrated autoregressive moving average (ARIMA) 92

intervention analysis 249

invertibility 79

iterated-expectation formula 288

#

Jarque-Bera test 284

# K

Kalman

filter equations 224

filtering 222

Kullback-Leibler divergence 130

kurtosis 284

#

lag operator 106

lag window 377

Bartlett or triangular 377

rectangular 377

truncated 377

lead 266

lead time 191

leading indicator 266

leakage 325

least-squares estimation

autoregressive models 154

mixed models 157

moving average models 156

light-tailed 284

likelihood function

concentrated 226

line spectrum 328

linear filter 331

linear trend 30

Ljung-Box portmanteau test 183

logarithms 99

logical variable 444

log-likelihood function 160

concentrated 226

Los Angeles rainfall 1

# M

MA characteristic polynomial 80

MA(1) process 57

MA(q) process 65

martingale differences 383

maximum likelihood estimation 158

McLeod-Li test 283

mean

function 11

sample 28

mean square error of prediction 218

median 211

method of moments 149

AR models 149

mixed models 151

moving average models 150

mixed autoregressive moving average

model

ARMA(p,q) 77

model-building strategy

specification 8

modified

Daniell spectral window 353

Dirichlet kernel 382

moving average 14

order q 57, 65

multiplicative seasonal

ARIMA model 234

ARMA $. ( p , q ) \times ( P , Q ) s$ model 231

#

negative frequencies 326

noise

white 17

nonsense correlation (spurious) 264

normality of residuals 178

# O

observational equation 223

oil filter sales 6

one-step-ahead forecast error 195

orthogonal increments 328

orthogonality 349

outlier

additive (AO) 257

innovative (IO) 257

overdifferencing 126

overfitting 185

# P

parameter redundancy 187

parsimony 8

partial autocorrelation 113

sample 115

particular solution 221

percentage changes 99

period 34

periodogram 322

phase 18, 34, 73, 319

plots of residuals 176

portmanteau test 183

generalized 304

power transfer function 332

power transformations 101

prediction error decomposition 225

prediction limits

ARIMA models 204

deterministic models 203

pre-intervention data 250

prewhitening 265

profile log-likelihood function 402

pulse function 251

purely deterministic process 383

purely discrete spectrum 328

# Q

quadratic trend 30

quantile-quantile plot (QQ) 45

quasi-likelihood estimators 301

quasi-period 74

# R

random cosine wave 18

random walk 12

with drift 22

rational spectral density 339

Rayleigh distribution 24

rectangular lag window 377

rectangular spectral window 352

red spectrum 333

regression methods 30

representation 328

residual 42

autocorrelation 180

plots 176

residual analysis 42, 175

residual standard deviation 41

residuals

normality 178

returns 99, 278

Riskmetrics software 297

R-squared 41

runs test 46

# S

sample

(auto)covariance function 329

autocorrelation function 46

cross-correlation function 261

mean 28

partial autocorrelation 115

spectral density 329

smoothed 351

scaled spectral distribution

function 329

seasonal

AR characteristic polynomial 230

$\mathbf { A R } ( P )$ model 230

cosine trend 34

difference 233

MA characteristic polynomial 229

MA(Q) model 229

means 32

period 229

trend 32

seasonality 6

second-order

autoregressive process, AR(2) 71

moving average process, MA(2) 62

semivariogram 23

signal plus noise 323

significant digits 446

skeleton 397

skewness 284

smoothed sample spectral density 351

bias 355

variance 355

smoothing constant (in EWMA) 209

spectral 328

distribution function 328

distribution interpretation 329

scaled distribution function 329

spectral density 330

rational 339

sample 329

spectral window 351

bandwidth 356

Daniell 352

Dirichlet 359

modified Daniell 353

rectangular 352

spectrum

blue 333

red 333

split cosine bell taper 360

spurious correlation 264

standard deviation 25

residual 41

standardized 25

state space model 222

stationarity conditions 16

AR(1) 71

AR(2) 71, 72, 84

AR(p) 76

ARMA(p,q) 78

intrinsically 23

second-order 17

strict 16

weak 17

step function 250

stochastic

process 11

seasonal models 228

trend 27

strange attractor 407

# T

tapering 359

TAR model 399

temperatures

Dubuque, Iowa 6

Tempered Brass 472, 474

tenor tuba 381

threshold autoregressive model 399

time domain analysis 319

time-invariant linear filter 331

causal 331

trading days 278

transfer-function model 267

trend

cosine 34

cyclical 32

deterministic 27

linear 30

quadratic 30

seasonal 32

seasonal means 32

stochastic 27

triangular lag window 377

trombone 374, 381

truncated

lag window 377

linear process 200, 221

TSA Library 468

tuba 381

# U

unbiased forecast 192

unconditional

least squares 160

sum-of-squares function 159

unit-root test 128

unperturbed process 250

updating equation 207

# V

Value at Risk (VaR) 277

variance 25

volatility 285

clustering 279

# W

weakly stationary 17

weight function 351

white light 332

white noise 17, 332

whitening 265

window closing 355

Wold decomposition 383

working directory 423

workspace 423

#

$\boldsymbol { \Psi }$ -coefficients

AR(2) 75

Yule-Walker equations

AR(2) 72

AR(p) 76

Yule-Walker estimates 150

![](images/b2f70de21bf75c7a7776b7d742af406e0f7afece614efe598a388ff3cb6f48cd.jpg)

# Time Series Analysis and its Applications with R Examples

Robert H.Shumway and David S.Stoffer

Time Series Analysis and Its Applications presents a balanced and comprehensive treatment of both time and frequency domain methods with accompanying theory. Numerous examples using non-trivial data illustrate solutions to problems such as evaluating pain perception experiments using magnetic resonance imaging or monitoring a nuclear test ban treaty. The book is designed to be useful as a text for graduate level students in the physical, biological and social sciences and as a graduate level text in statistics.

2nd ed., 2006, XIII, 575 pp. Hardcover ISBN 978-0-387-29317-2

![](images/b8a7ba3b0ace82bb9c4f3e19018d53f859fc8c6d2edec554be319d2d9b9b43bf.jpg)

# Statistical Methods for Human Rights

Jana Asher, David Banks and Fritz J. Scheuren (Eds.)

Human rights issues are shaping the modern world. They define the expectations by which nations are judged and affect the policy of governments, corporations, and foundations. This book describes the statistics that underlie the social science research in human rights. It includes case studies, methodology, and research papers that discuss the fundamental measurement issues. It is intended as an introduction to applied human rights research.

2007, Approx 410 pp. Softcover ISBN 978-0-387-72836-0

![](images/a01fb41fcd09d7c0428de0ba9e6d346f0364b9fdd6127ab09abc9cb869e6719f.jpg)

# Matrix Algebra Theory, Computations, and Applications in Statistics

James E. Gentle

Matrix algebra is one of the most important areas of mathematics for data analysis and for statistical theory. The first part of this book presents the relevant aspects of the theory of matrix algebra for applications in statistics. Next begins a consideration of various types of matrices encountered in statistics and describes the special properties of those matrices. Additionally, this book covers numerical linear algebra.

2007, X,XII 528 pp. Hardcover ISBN 978-0-78702-0