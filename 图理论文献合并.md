# Introduction to Graph Neural Networks: A Starting Point for Machine Learning Engineers

James H. Tanis $^ *$ , Chris Giannella, and Adrian V. Mariano

The MITRE Corporation

∗Corresponding author (email jhtanis@mitre.org)

# Abstract

Graph neural networks are deep neural networks designed for graphs with attributes attached to nodes or edges. The number of research papers in the literature concerning these models is growing rapidly due to their impressive performance on a broad range of tasks. This survey introduces graph neural networks through the encoder-decoder framework and provides examples of decoders for a range of graph analytic tasks. It uses theory and numerous experiments on homogeneous graphs to illustrate the behavior of graph neural networks for different training sizes and degrees of graph complexity.

Keywords— Graph neural networks, graph representation learning, deep learning, encoders, graphs

# Contents

1 Introduction 2   
2 Common applications 3   
3 Introduction to encoder-decoder models 4

3.1 Encoder-decoder framework . . 5   
3.2 Shallow embedding examples 7

4 Graph Neural Networks 9

4.1 Encoder layers 10   
4.2 Decoder and loss functions . . . 13   
4.3 Learning paradigms 17

5 Experiments 18

5.1 Baseline node classification performance . . . 19   
5.2 Hyperparameters and node classification accuracy . . 21

5.2.1 Adjusting the number of hidden dimensions . . . . . . 21   
5.2.2 Adjusting the number of training epochs . . . . . 22   
5.2.3 Adjusting the number of layers and other hyperparameters . 23

5.3 Qualitative description of GNN learning . . . 29

6 Conclusion 31

A Open-source GNN libraries 32   
B Results on Individual Datasets 33

# 1 Introduction

Relationships within data are important for everyday tasks like internet search and road map navigation as well as for scientific research in fields like bioinformatics. Such relationships can be described using graphs with real vectors as attributes associated with the graph’s nodes or edges; however, traditional machine learning models operate on arrays, so they cannot directly exploit the relationships. This report surveys Graph Neural Networks (GNNs), which jointly learn from both edge and node feature information, and often produce more accurate models. These architectures have become popular due to their impressive performance on graph analysis tasks. Consequently, the number of research papers on GNNs is growing rapidly, and many surveys exist.

Some surveys discuss graph neural networks in the context of broad families such as graph networks, graph representation learning and geometric deep learning [1, 2, 3, 4, 5]. Other surveys categorize GNNs by abstracting their distinguishing properties into functional relationships [6, 7, 8, 3, 9]. Although useful for organizational purposes, generality and abstraction can be difficult to understand for those new to the field. Other surveys have a narrow focus, for example to discuss efforts to improve a specific weakness in GNN architectures [10], or to survey GNN work on a particular task, such as fake news detection or product recommendation [11, 12]. While valuable for those interested in the task, they provide little background in GNNs and therefore assume the reader already has that knowledge.

For this reason, a concrete and concise introduction to GNNs is missing. We begin by introducing GNNs as encoder-decoder architectures. To provide perspective on the ways GNNs are used, we discuss common GNN applications along with examples of task-specific decoders for turning features into predictions. We think that studying a few important examples of GNNs well will help the reader develop a feeling for the subject that would be difficult to achieve otherwise. We therefore focus on three convolutional and attentional networks, GCN, GraphSAGE, and GATv2, which are commonly used both as benchmarks and as components in other GNN architectures. We conduct numerous experiments with these GNNs at

two training sizes and on thirteen datasets of both high and low complexity. The experiments have three goals:

• Compare benchmark GNNs with other graph models.   
• Demonstrate how hyperparameter adjustments affect GNN performance.   
• Provide a qualitative picture of what happens when GNNs learn.

We hope these experiments combined with the theoretical sections will enable newcomers to use GNNs more effectively and to improve GNN performance on their problems. We also hope that experts will gain new insights from our experiments.

# 2 Common applications

Graph neural networks are suited to a variety of graph tasks.

# 1. Node classification

This task concerns categorizing nodes of a graph. There are several applications within the space of social networks, such as assigning roles or interests to individuals or predicting whether individuals are members of a group [13, 14]. Node classification tasks also include classifying documents, videos or webpages into different categories [15, 16]. There are also important applications in bioinformatics, such as classifying the biological function of proteins (nodes) and their interactions (edges) with other proteins [17].

# 2. Link prediction

Link prediction is a classification task on pairs on nodes in a graph. Most often, this is a binary classification problem, where the task is to predict whether an edge exists between two nodes, e.g. one to predict that an edge is present and zero to predict that it is absent. Link prediction also exists for graphs with multiple edge types, so edges are predicted to be one of several types [18].

Link prediction can predict the presence of a relationship (edge) between two individuals (nodes) in a social network, either presently or in the near future [19]. Recommendation systems try to recommend products to customers; this task is a link prediction problem, where one seeks edges between two different types of nodes, the product nodes and the customer nodes [20, 21]. Link prediction for entity resolution predicts links between different records in a dataset that refer to the same object [22, 23]. For example, we want to link a record describing ”John Smith” with another record for the same person written ”Smith, John”. In bioinformatics, link prediction can predict relationships between drugs and diseases [24] and the similarity between diseases [25]. Link prediction also includes finding new relationships between nodes in knowledge graphs, a task called knowledge graph completion [26, 27].

# 3. Community detection

Community detection algorithms cluster graph nodes by using some problem dependent similarity measure. They are typically not machine learning based [28, 29], but some algorithms may be trained in an unsupervised or semi-supervised manner [30, 31]. Applications include identifying social groups within a social network [32], entity resolution [33], fraud detection [34], text clustering (e.g. grouping Reddit posts into similar topics) [17] and visualization [35, 36].

# 4. Node regression and edge regression

The traffic prediction literature tries to predict traffic conditions, like traffic speed, volume, etc., in the near future from sensors on the road, which supports tasks such as travel time estimation and route recommendations [37, 38]. The road network has intersections as nodes and road segments as edges. The sensors are additional nodes on the road network, so estimating the numeric descriptors of traffic conditions at these sensors is a node regression problem. Less often, edge regression models support traffic prediction by predicting edge weights that represent traffic flow or count, [39]. Other node regression applications include predicting house prices and weather characteristics [40], and predicting the amount of internet traffic to web pages [41].

# 5. Graph classification and graph regression

Conventionally, time consuming and expensive laboratory experiments establish a molecule’s properties. Molecule property prediction is foundational for the development of new materials with industrial applications and new drugs to treat diseases, and consequently, significant resources have been devoted to developing a model that can accurately predict molecule properties quickly and cheaply. Graphs naturally represent molecules with its nodes as atoms and its edges as chemical bonds between two atoms, and GNNs, which operate directly on graphs, quickly proved to be well suited to this task [42, 43, 44, 45]. The accuracy of GNN predictions matches or exceeds that of conventional models with expert features when enough labeled data is available for training [43], but labeled data is often limited in the target domain, so prediction accuracy suffers [46]. To meet this challenge, self-supervised approaches that leverage large amounts of unlabeled data are being developed [46, 47, 48].

# 3 Introduction to encoder-decoder models

An attributed graph has a set of nodes, $\mathcal { N }$ , as well as edges that define how the nodes relate to each other. To simplify the discussion, we restrict our attention to undirected graphs, so the edges are represented by a weighted, symmetric adjacency matrix, $A = ( A _ { i j } )$ where $i , j \in \mathcal N$ . An entry $A _ { i j }$ is non-zero if an edge connects node $i$ to node $j$ and zero otherwise. Each node, $i \in \mathcal N$ has an attribute, $x _ { i } \in \mathbb { R } ^ { \ell }$ for some $\ell \in \mathbb { N }$ . Encoder-decoder models on graphs is a class of machine learning

models. Machine learning on graphs presents challenges that do not arise in conventional machine learning on vectors, because graphs are irregular data structures and do not have a natural coordinate system. In particular, standard convolutional neural networks for image arrays do not work on graphs, because the $k$ -hop neighborhoods may be different for every node. Nonetheless, a typical first step for machine learning on graphs is to obtain a low-dimensional feature vector for every node that contains all the information that is needed to complete the desired task. These feature vectors are real vectors that often contain the information needed to represent the local edge structure about each node.

A feature vector of a node is also called a node embedding or a node representation, and collectively the feature vectors can be used for tasks on nodes, tasks on edges, or tasks on the entire graph. At the graph level, applying principal component analysis (PCA) or t-distributed stochastic neighbor embedding ( $\mathrm { t }$ -SNE) to node embeddings can produce lower dimensional representations that enable visualizations to help understand the how algorithms are performing [49]. In addition, community detection algorithms use node embeddings to define the communities, either in an end-to-end fashion [50] or as part of a two-step process by applying the $k$ -means algorithm to the node embeddings [51, 52]. Node embeddings also support graph classification, where in the simplest case, a mean activation over all node embeddings of the graph determines the graph class. More sophisticated approaches are described in [53, 54]. Not surprisingly, node features are also used for node level tasks like node classification and regression, [55, 15], as well as for edge level tasks like link prediction [56], edge classification [57, 58] or edge regression [59].

Due to the importance of node embeddings, there are many techniques to obtain them for a range of goals and data conditions. Perhaps the simplest example of a node embedding is given by the rows of an adjacency matrix. The map : $\mathbf { \Phi } : \iota \to ( A _ { i j } ) _ { j \in \mathcal { N } }$ defines node embeddings in $\mathbb { R } ^ { | \mathcal { N } | }$ . However, it is difficult to use these vector representations in machine learning due to their sparsity or high dimension, which tends to lead to overfitting. The row vectors are also poor features, because they do not provide any structural information beyond each node’s 1-hop neighborhood nor do they account for any node attributes.

Instead, researchers may use rule-based descriptions of nodes, like centrality or clustering measurements, to produce low dimensional node representations that are more information dense, which may subsequently be applied to a downstream task with a traditional machine learning algorithm. The disadvantage of this approach is that hand-crafted features are not part of the algorithm’s training process, so the features are not fine-tuned to minimize the loss function. To do this, researchers use an encoder-decoder approach.

# 3.1 Encoder-decoder framework

Many machine learning models adhere to an encoder-decoder framework shown in Figure 1. The encoder is a function

$$
\operatorname {E n c}: \mathcal {N} \rightarrow \mathbb {R} ^ {\ell} \tag {1}
$$

![](images/997a6f883756ba7f3622381afb792bcf029863e2cb0e160a24b88a49bef39232.jpg)  
Figure 1

that maps nodes to node embeddings in $\mathbb { R } ^ { \ell }$ , where $\ell \ll | \mathcal { N } |$ . A good encoder creates node embeddings that contain all of the information about each node that is required to complete the task at hand.

Subsequently the decoder function

$$
\operatorname {D e c}: \mathbb {R} ^ {m} \rightarrow \mathbb {R} ^ {k}, \tag {2}
$$

converts those node embeddings into predictions, where $m \geq \ell$ and $k$ is the dimension of the model predictions. We emphasize that the decoder generally does not simply invert the encoder. It is instead a kind of interpreter that “decodes” abstract node embeddings into predictions in order to solve the given task. The decoders for common tasks like those described in Section 2 are usually simple functions with few parameters, such as an inner product followed by a softmax function. Hence, the majority of the model’s learnable parameters are usually in the encoder.

We introduce the term ground truth function

$$
\mathrm {G t}: \mathcal {G} \rightarrow \mathbb {R} ^ {k}, \tag {3}
$$

that provides reference information that is known about the graph, such as a node’s class for node classification, which the loss function compares with the $k$ - dimensional model predictions. The training and evaluation algorithms use it to assess the quality of model predictions. There does not seem to be an accepted term in the literature that accounts for all contexts that occur. Hamilton, et al. [55] consider the case of a relationship between two nodes and call it a pairwise similarity function. This occurs in link prediction, where the ground truth function may be a map : ${ \mathcal { N } } \times { \mathcal { N } } \to \{ 0 , 1 \}$ that says whether or not an edge exists between two nodes. In node classification, however, the ground truth function typically provides the node’s class. In all cases, its role in the encoder-decoder framework is the same, so we refer to it by a single name.

The loss functions and evaluation metrics link the ground truth function and the model prediction. Most algorithms learn model parameters by some form of gradient decent, where the loss functions are fairly smooth. Common examples of loss functions are cross entropy loss for classification and $L ^ { 1 }$ or $L ^ { 2 }$ loss for regression tasks. For evaluation metrics, common examples are accuracy, F1 and AUC (Area Under the receiver operator characteristic Curve) for classification and RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) for regression tasks.

# 3.2 Shallow embedding examples

We now present several representative examples of models that produce embedding lookups for nodes that were seen during the training process. These examples will illustrate the encoder-decoder framework and at the end we will note their shortcomings, which Hamilton et al. [55, 2] describes. This will lead us to more complicated encoder-decoder models called GNNs in the next section.

For each example, the input is a fixed matrix that provides a similarity statistic between any two nodes in $\mathcal { N }$ such as a weighted adjacency matrix. The output of these algorithms is a real vector (a feature vector) for each node describing the node’s neighborhood structure, and taken together, they support some downstream machine learning task.

The Laplacian eigenmaps algorithm is an early and successful nonlinear dimensionality reduction algorithm [60]. Given a user-defined parameter $t > 0$ , a weighted adjacency matrix, $W = ( W _ { i j } ) _ { i , j \in \cal N }$ , can be defined by

$$
W _ {i j} = W _ {i j} (t) = \left\{ \begin{array}{l l} \exp \left(- \frac {\| x _ {i} - x _ {j} \| ^ {2}}{t}\right) & \text {i f} A _ {i j} = 1, \\ 0 & \text {o t h e r w i s e .} \end{array} \right. \tag {4}
$$

In practice, the above weighted adjacency matrix is typically the input to the Laplacian eigenmaps algorithm, but a simple adjacency matrix or a $k$ -nearest neighbor matrix may alternatively be inputs.

The Laplacian eigenmaps algorithm can be reformulated in terms of the encoderdecoder framework [55, 2]. Define the ground truth, decoder and loss functions by

$$
\mathrm {G t}: \mathcal {N} \times \mathcal {N} \rightarrow \mathbb {R}, \quad \mathrm {G t} (i, j) = W _ {i j}, \tag {5}
$$

$$
\operatorname {D e c}: \mathbb {R} ^ {\ell} \times \mathbb {R} ^ {\ell} \rightarrow \mathbb {R} ^ {+}, \quad \operatorname {D e c} (w, z) = \| w - z \| ^ {2}, \tag {6}
$$

$$
L: \mathbb {R} \times \mathbb {R} ^ {+} \rightarrow \mathbb {R}, L (q, r) = q r. \tag {7}
$$

Then the goal is to to find the (non-constant) encoder Enc

$$
z _ {i} = \operatorname {E n c} (i) \in \mathbb {R} ^ {\ell}, \quad \text {f o r} i \in \mathcal {N} \tag {8}
$$

that minimizes the model’s loss $\mathcal { L } \in \mathbb { R } ^ { + }$ up to a scaling factor, where that loss is

$$
\begin{array}{l} \mathcal {L} = \sum L \left(\operatorname {G t} (i, j), \operatorname {D e c} \left(z _ {i}, z _ {j}\right)\right) \\ \underline {{i , j \in \mathcal {N}}} \tag {9} \\ = \sum_ {i, j \in \mathcal {N}} W _ {i j} \| z _ {i} - z _ {j} \| ^ {2}, \\ \end{array}
$$

where the minimization is subject to a constraint that prevents the solution from collapsing to a lower dimension (i.e. $Z ^ { T } D Z = I$ , where $Z = ( z _ { i } ) _ { i \in \mathcal { N } }$ ). Notice that $W _ { i , j } \geq 0$ is larger when $i$ and $j$ are adjacent. Then the above equation means that the model is punished during training for having node attributes of adjacent nodes

be far apart. (Note that the constant encoder $\mathrm { E n c } ( i ) = { \bf 1 }$ satisfies $\mathcal { L } = 0$ , but this is not useful).

Belkin et al. [60] provides an optimal solution based on generalized eigenvectors of the graph Laplacian. The graph Laplacian is the matrix $\Delta = D - W$ , where $D$ is a diagonal matrix defined by $\textstyle D _ { i i } = \sum _ { j } W _ { j i }$ [61]. The generalized eigenvectors $( f _ { k } ) _ { 1 \leq k \leq | \mathcal { N } | } \subset \mathbb { R } ^ { | \mathcal { N } | }$ are the solutions to the equation

$$
\Delta f = \lambda D f, \tag {10}
$$

where they are labeled in sorted order so that the corresponding eigenvalues satisfy

$$
0 = \lambda_ {0} \leq \lambda_ {1} \leq \dots \leq \lambda_ {| \mathcal {N} | - 1}. \tag {11}
$$

The encoder ${ \mathrm { E n c : } } \mathcal { N }  \mathbb { R } ^ { \ell }$ uses the first $\ell$ generalized eigenvectors:

$$
\operatorname {E n c} (i) = \left(f _ {1} (i), \dots , f _ {\ell} (i)\right). \tag {12}
$$

Inspired by the success of Laplacian eigenmaps, several papers define node similarity using inner products in a class of techniques called matrix factorization methods [62, 63, 64]. Matrix factorization approaches typically rely entirely on the graph’s edge structure. The three cited methods differ only in how they define the ground truth function, Gt. Hamilton et al. [55] reformulates each approach into a encoder-decoder framework:

$$
\mathrm {G t}: \mathcal {N} \times \mathcal {N} \rightarrow \mathbb {R} ^ {+}, \quad \mathrm {G t} (i, j) \in \mathbb {R} ^ {+}, \tag {13}
$$

$$
\operatorname {D e c}: \mathbb {R} ^ {\ell} \times \mathbb {R} ^ {\ell} \rightarrow \mathbb {R}, \quad \operatorname {D e c} (w, z) = w ^ {T} z, \tag {14}
$$

$$
L: \mathbb {R} ^ {+} \times \mathbb {R} \rightarrow \mathbb {R} ^ {+}, L (q, r) = \frac {1}{2} (q - r) ^ {2}. \tag {15}
$$

Given an encoder Enc with node embeddings

$$
z _ {i} = \operatorname {E n c} (i) \in \mathbb {R} ^ {\ell}, \quad \text {f o r} i \in \mathcal {N}, \tag {16}
$$

the loss $\mathcal { L } \in \mathbb { R } ^ { + }$ i s

$$
\begin{array}{l} \mathcal {L} = \sum_ {i, j \in \mathcal {N}} L \left(\operatorname {G t} (i, j), \operatorname {D e c} \left(z _ {i}, z _ {j}\right)\right) \\ = \frac {1}{2} \sum_ {i, j \in \mathcal {N}} \left(z _ {i} ^ {T} z _ {j} - \operatorname {G t} (i, j)\right) ^ {2}. \tag {17} \\ \end{array}
$$

Notice that if $Z = \left( z _ { i } \right)$ is the matrix of features in $\mathbb { R } ^ { \ell \times | \mathcal { N } | }$ , then the above loss satisfies

$$
\mathcal {L} = \frac {1}{2} \| Z ^ {T} Z - S \| ^ {2}, \tag {18}
$$

where $S$ is the matrix with entries $S _ { i j } = \operatorname { G t } ( i , j )$ . Minimizing $\mathcal { L }$ means finding a matrix $Z$ that factors the ground truth of matrix $S$ as shown in Equation (18), which is why the methods are called matrix factorization methods.

Ahmed et al. [62] define the ground truth function by $\mathrm { G t } ( i , j ) = A _ { i , j }$ , where $( A _ { i j } ) _ { i , j \in \mathcal { N } }$ are the coefficients of the adjacency matrix. Hence, their goal is to find a solution that minimizes the loss

$$
\mathcal {L} = \frac {1}{2} \sum_ {i, j \in \mathcal {N}} \left(z _ {i} ^ {T} z _ {i} - A _ {i, j}\right) ^ {2}. \tag {19}
$$

More recently in 2014, Perozzi et al. [16] introduce random walks on a graph as a tool to learn node embeddings that capture the edge structure of larger node neighborhoods in a computationally efficient manner. After a random initialization of node features, a stochastic gradient descent algorithm updates features to optimize the information necessary to estimate the probability that two nodes co-occur on the same random walk of a fixed length. Two years later Grover et al. [65] improved on Perozzi et al. by adding explore and return parameters that respectively determined the algorithm’s tendency to explore new nodes and return to the starting node. Later, [66] showed that these random walk methods are essentially matrix factorization techniques.

Matrix factorization methods have the advantage of being applicable on graphs without attributes. On attributed graphs, however, all of the examples of shallow models share several shortcomings [55]:

1. They make insubstantial use of the node attributes during training, so they do not use all available information. Moreover, these models tend to define similarity in terms of proximity, and consequently they usually produce poor results when adjacent nodes in a graph tend to be dissimilar [67].   
2. Trained models cannot be applied to unseen nodes without further training. This is impractical for dynamic graphs and for graphs that are so large that they cannot fit in memory. It also means that a model trained in a setting with a lot of labeled data is not transferrable to an unseen graph in a related domain with sparsely labeled data.   
3. The information is not efficiently stored in the model. Each trained model is the collection of node features for the graph, which means model parameters are not shared across nodes. In particular, the number of parameters grows linearly with $| \mathcal { N } |$ , which can create memory challenges for processing on large graphs.

The next section discusses more powerful encoder-decoder approaches called graph neural networks, which resolve these shortcomings.

# 4 Graph Neural Networks

Graph neural networks have several desirable properties. They jointly use node attributes and edge information for training, trained models can be applied to unseen graphs, and the number of parameters of a GNN is independent and sublinear in the number of nodes. Moreover, they apply naturally to both undirected and directed graphs.

This section focuses on GNNs that have so-called message-passing layers (described below). The vast majority of GNNs in the literature have message-passing layers.

# 4.1 Encoder layers

A typical GNN encoder can have three classes of layers: pre-processing layers, message-passing layers and post-processing layers. The pre- and post-processing layers are optional.

![](images/ea07223129fe24048fc1329862820aeb369070da6fbb4d0c483f7aeb950823f6.jpg)  
Figure 2. Three classes of layers in a GNN encoder.

In general, a single layer feedforward neural network has the form

$$
: \mathbb {R} ^ {d} \rightarrow \mathbb {R} ^ {\tilde {d}}, \quad x _ {i}: \rightarrow \sigma \left(W x _ {i} + b\right), \tag {20}
$$

where for positive integers $d$ and $\ddot { d }$ , $W : \mathbb { R } ^ { d }  \mathbb { R } ^ { \dot { d } }$ is a matrix and $b \in \mathbb { R } ^ { d }$ i s a vector, both with of trainable parameters, and $\sigma$ is an element-wise non-linear function (e.g. element-wise ReLU). Pre-processing layers are a stack of one or more of these networks

$$
\operatorname {P r e - P r o c}: \mathbb {R} ^ {m} \rightarrow \mathbb {R} ^ {\tilde {m}}, \quad \operatorname {P r e - P r o c} (x _ {i}) = \tilde {x} _ {i}, \tag {21}
$$

that maps each node attribute vector $x _ { i }$ to a node feature vector $\tilde { x } _ { i }$ in a computation that does not involve the edges of the graph.

These node features feed into the message-passing layers, which are the most important layers for the GNNs performance [68]. If $A$ is a graph with a matrix of node features $\widetilde { X } = ( \tilde { x } _ { i } ) _ { i \in \mathcal { N } }$ , then a message-passing layer is a map

$$
\text {M e s s a g e - P a s s i n g}: (\tilde {X}, A) \rightarrow (H, A) \tag {22}
$$

from the graph $A$ with node features $\widetilde { X }$ to the graph $A$ with node features $H =$ $( h _ { i } ) _ { i \in \mathcal { N } }$ , where the node feature vectors $\boldsymbol { h } _ { i } \in \mathbb { R } ^ { \tilde { \ell } }$ are obtained by aggregating information from each node’s neighborhood. Then node features from each successive message-passing layer contain information that has been aggregated over a wider set of nodes than the previous layers. At the end, an encoder of a $k$ -layer GNN that aggregates node features over a 1-hop neighborhood produces low dimensional node embeddings that summarize information in each node’s $k$ -hop neighborhood. In this way, message-passing layers resemble the highly successful convolutional neural networks for image classification.

Node features from the message-passing layers subsequently feed into the final layers of the GNN encoder called the post-processing layers. They are collectively, like the pre-processing layers, fully connected feedforward neural networks

$$
\text {P o s t - P r o c}: \mathbb {R} ^ {\widetilde {\ell}} \rightarrow \mathbb {R} ^ {\ell}, \quad \text {P o s t - P r o c} (h _ {i}) = z _ {i} \tag {23}
$$

that maps each node feature vector $h _ { i }$ that is produced by the message-passing layers to the node embedding $z _ { i }$ . Then the encoder of the GNN maps each node with its node attribute $x _ { i }$ to a node embedding $z _ { i }$

$$
\operatorname {E n c}: \mathcal {N} \rightarrow \mathbb {R} ^ {\ell}, \quad \operatorname {E n c} (i) = z _ {i}. \tag {24}
$$

Each message-passing layer of the encoder computes its output using the same process. Consider a $K$ -layer message-passing network. For each node $i$ , define $h _ { i } ^ { ( 0 ) } = \tilde { x } _ { i }$ , and for integers $0 < k < K$ , let $h _ { i } ^ { ( k ) } \in \mathbb { R } ^ { \bar { \ell } _ { k } }$ be the node feature vector that is the output of the $k _ { t h }$ message-passing layer. Starting from the output of the $k _ { t h }$ message-passing layer, for each node $_ i$ , the $k + 1$ message-passing layer computes the vector s h(k+1)i b $h _ { i } ^ { ( k + 1 ) }$ y

$$
h _ {i} ^ {(k + 1)} = \phi \left(\bigwedge \left(h _ {i} ^ {(k)}, \bigoplus_ {j \in \mathrm {N} _ {i}} \mu_ {i j}\right)\right). \tag {25}
$$

Sometimes called the update function, $\phi : \mathbb { R } ^ { \bar { \ell } _ { k } }  \mathbb { R } ^ { \bar { \ell } _ { k + 1 } }$ is a differentiable function with trainable parameters. A common choice for $\phi$ is a single layer, feedforward neural network, as in (20). The term $\oplus$ is a permutation-invariant aggregation function such as an element-wise vector-valued sum, mean or maximum, $\mathrm { N } _ { i }$ is the 1-hop neighborhood of $_ i$ (excluding $\imath$ ), and the term $\mu _ { i j } \in \mathbb { R } ^ { \bar { \ell } _ { k } }$ is a feature vector (defined below) that describes the interaction of node $i$ with node $j$ . The function $\Lambda$ is determines how each node interacts with the aggregation of its neighbors, for example by concatenation or by element-wise summation. Equation (25) is more concisely written without the function $\Lambda$ , but unlike other authors [4], we include it because our experiments show the choice of $\Lambda$ is at least as important to the behavior of the GNN as the node-to-node interaction terms, $\mu _ { i j }$ .

There are three common architecture categories for GNNs in the literature: Convolutional, Message-Passing (MP), and Attentional, and the majority of GNNs fit into one of them. The way that the features at nodes $_ i$ and $j$ interact (i.e. $\mu _ { i j }$ )

defines a GNN’s type. We denote the message-passing category by its initials, MP, to help distinguish it from message-passing layers. Our description of each GNN category follows Bronstein et al. [4]. This discussion is intended to capture key ideas rather than all subtle similarities and differences between individual models.

An architecture is in the convolutional category,

$$
\left. h _ {i} ^ {(k + 1)} = \phi \left(\bigwedge \left(h _ {i} ^ {(k)}, \bigoplus_ {j \in \mathrm {N} _ {i}} w _ {i j} \psi \left(h _ {j} ^ {(k)}\right)\right)\right), \right. \tag {26}
$$

if the value $\mu _ { i j }$ from (25) is defined by $\mu _ { i j } = w _ { i j } \psi ( h _ { j } ^ { ( k ) } )$ , where $\psi$ is a differentiable function that can have trainable parameters, such as an affine linear transformation, $\psi : \mathbb { R } ^ { \ell _ { k } }  \mathbb { R } ^ { \ell _ { k } }$ ,

$$
\psi \left(h _ {j} ^ {(k)}\right) = W h _ {j} ^ {(k)} + b, \tag {27}
$$

where $W : \mathbb { R } ^ { \ell _ { k } }  \mathbb { R } ^ { \ell _ { k } }$ is a matrix and $b \in \mathbb { R } ^ { \ell _ { k } }$ is a vector. The coefficients $w _ { i j }$ are unlearned weights, usually depending only on the local graph topology and which encode the connection strength between pairs of nodes [69, 15, 17, 68, 70]. If the graph exhibits homophily, meaning that nodes with similar features or the same class label tend to be linked [71], then in principal, the fixed weights $w _ { i j }$ make these models a good choice due to their scalability and regularization. This occurs, for example, in a social network with users connected by friendship [72]. On the downside, the rigidness of fixed weights may inhibit their ability to represent the complex relationships that arise in low homophily graphs.

An architecture in the MP category computes vectors by

$$
h _ {i} ^ {(k + 1)} = \phi \left(\bigwedge \left(h _ {i} ^ {(k)}, \bigoplus_ {j \in \mathrm {N} _ {i}} \psi \left(h _ {i} ^ {(k)}, h _ {j} ^ {(k)}\right)\right)\right), \tag {28}
$$

where $\psi : \mathbb { R } ^ { 2 \ell _ { k } }  \mathbb { R } ^ { \ell _ { k } }$ is a differentiable function with trainable parameters such as an affine linear transformation. These are the most expressive of the three flavors of GNNs, which makes them suitable for complex modeling tasks like predicting the properties of molecules or complex dynamical systems [73, 74]. However, this flexibility also makes them more challenging to train, and they require relatively large amounts of memory, which makes them difficult to run at scale compared to those in the convolutional category.

Finally, a model in the attentional category balances the expressiveness of a MP architecture with the scalability of a convolutional one. The features are computed by

$$
h _ {i} ^ {(k + 1)} = \phi \left(\bigwedge \left(h _ {i} ^ {(k)}, \bigoplus_ {j \in \mathrm {N} _ {i}} a \left(h _ {i} ^ {(k)}, h _ {j} ^ {(k)}\right) \psi \left(h _ {j} ^ {(k)}\right)\right)\right), \tag {29}
$$

where the algorithm learns the scalar-valued function $a$ and the possibly the function $\psi$ [75, 76, 77, 78, 79]. For example [78], the function $a$ may be computed

by

$$
a \left(h _ {i} ^ {(k)}, h _ {j} ^ {(k)}\right) = \frac {\exp \left(\alpha_ {i j}\right)}{\sum_ {m \in N _ {i}} \exp \left(\alpha_ {i m}\right)}, \quad \text {w h e r e} \alpha_ {i m} = \sigma \left(\mathbf {a} ^ {T} \cdot W \left(h _ {i} ^ {(k)} \mid \mid h _ {m} ^ {(k)}\right)\right). \tag {30}
$$

Here, $\sigma$ is a nonlinear function, $| |$ is concatenation, and for $d$ hidden dimensions, $\mathbf { a }$ is a $d { \times } 1$ dimensional vector and $W$ is a $d { \times } 2 \tilde { \ell } _ { k }$ dimensional matrix. Because $a$ is scalarvalued, learning interactions involves fewer parameters than for the MP networks. This makes attentional networks easier to train and run than the MP networks at the cost of being less expressive. On the other hand, attentional networks may be a little more difficult to run at scale than convolutional ones, but because attentional networks learn edge weights, they tend to outperform convolutional ones on low homophily graphs [79, 80].

While the interaction between each node and the aggregation of its neighbors (i.e. $\Lambda$ ) does not define the GNN category, it can significantly affect GNN behavior. For example, the attentional networks GAT and GATv2 get flexibility from the attention mechanism of their node-to-node interactions, $\mu _ { i j }$ , but they use vector addition for $\Lambda$ . In contrast, GraphSAGE is convolutional but uses concatenation for $\Lambda$ , so (25) becomes

$$
\sigma \left(W \left(h _ {i} ^ {(k)} \right| | \bigoplus_ {j \in \mathrm {N} _ {i}} \mu_ {i j}\right) + b \tag {31}
$$

This enables GraphSAGE to better preserve the information of each node when mixing it with that of its neighbors hurts performance. Section 5.2 presents results from numerous experiments that show GraphSAGE tends to outperform the attentional networks GAT and GATv2 on low homophily graphs, see also [17, 77, 78].

Table 1. V Versus GNN Category for Some Common GNNs   

<table><tr><td>∧</td><td>Convolutional</td><td>Attentional</td></tr><tr><td>Sum</td><td>GCN</td><td>GAT, GATv2</td></tr><tr><td>Concatenation</td><td>GraphSAGE</td><td></td></tr></table>

Lastly, we remark that a version of convolutional GNNs also exist for the spectral domain, where an aggregation function operates on the eigenvectors of the graph Laplacian [81]. Comparing with the convolutional GNNs described above, the spectral version may provide richer features, but it also is more memory intensive and does not readily extend to directed graphs nor allow predictions on unseen nodes [82, 83].

# 4.2 Decoder and loss functions

Below are minimalist examples of these components for several graph tasks from Section 2. More sophisticated examples appear in the literature. For an integer

$K > 0$ , the softmax function, softmax : $\mathbb { R } ^ { K } \to \mathbb { R } ^ { K }$ , is defined along each coordinate by

$$
\operatorname {s o f t m a x} (\mathbf {s}) _ {j} = \frac {\exp \left(s _ {j}\right)}{\sum_ {k = 0} ^ {K - 1} \exp \left(s _ {k}\right)}, \tag {32}
$$

$\mathbf { s } = ( s _ { k } ) _ { k = 0 } ^ { K - 1 } \in \mathbb { R } ^ { K }$ ${ \textstyle \sum _ { j = 0 } ^ { K - 1 } \operatorname { s o f t m a x } ( \mathbf { s } ) _ { j } = 1 }$ $\textstyle \sum _ { j = 0 } ^ { K - 1 }$

As usual, define $z _ { i } = \operatorname { E n c } ( i )$ and let $A = ( A _ { i . j } ) _ { i , j \in N }$ be the graph’s adjacency matrix.

# 1. Node classification

Let $K$ be the number of class labels, and let $\mathbf { y } _ { i } = ( y _ { i } ( c ) ) _ { c = 1 } ^ { K } \in \{ 0 , 1 \} ^ { K }$ be the ground truth vector of terms $y _ { i } ( c )$ , where $y _ { i } ( c ) = 1$ if node $i$ is in class $c$ and $y _ { i } ( c ) = 0$ otherwise. For a matrix $\Theta \in \mathbb { R } ^ { \ell \times K }$ with trainable parameters, the ground truth, decoder and loss functions are

$$
\operatorname {G t}: \mathcal {N} \rightarrow \{0, 1 \} ^ {K}, \quad \operatorname {G t} (i) = \mathbf {y} _ {i} = \left(y _ {i} (c)\right) _ {c = 1} ^ {K}, \tag {33}
$$

$$
\operatorname {D e c}: \mathbb {R} ^ {\ell} \times \mathbb {R} ^ {\ell \times K} \rightarrow (0, 1) ^ {K}, \quad \operatorname {D e c} (z _ {i}, \Theta) = \operatorname {s o f t m a x} \left(z _ {i} ^ {T} \Theta\right), \tag {34}
$$

$$
L: \{0, 1 \} ^ {K} \times [ 0, 1 ] ^ {K} \rightarrow \mathbb {R} ^ {+}, \quad L (\mathbf {y}, \hat {y}) = - \mathbf {y} ^ {T} \log (\hat {y}), \tag {35}
$$

where $\log ( \hat { y } )$ is element-wise logarithm of the prediction vector $\hat { y } \in [ 0 , 1 ] ^ { K }$ , so $L$ is the categorical cross-entropy loss function. Then the loss for the network is

$$
\begin{array}{l} \mathcal {L} = - \sum_ {i} L \left(\operatorname {G t} (i), \operatorname {D e c} (z _ {i}, \Theta)\right) \\ = - \sum_ {i} ^ {\iota} \mathbf {y} _ {i} ^ {T} \log \left(\operatorname {s o f t m a x} \left(z _ {i} ^ {T} \Theta\right)\right). \tag {36} \\ \end{array}
$$

When stochastic gradient descent is used, the sum is over a batch of nodes $B \subset { \mathcal { N } }$ . (This same comment also applies to the losses in the examples below). Also see [15, 84, 68, 77].

# 2. Link prediction

The sigmoid function, sigmoid : $\lVert { \boldsymbol { \mathbb { R } } } \to \rVert _ { \mathbb { R } }$ , is defined for $t \in \mathbb R$ by

$$
\operatorname {s i g m o i d} (t) = \frac {1}{1 + \exp (- t)}. \tag {37}
$$

Define the decoder, ground truth and loss functions by

$$
\operatorname {G t}: \mathcal {N} \times \mathcal {N} \rightarrow \{0, 1 \}, \quad \operatorname {G t} (i, j) = A _ {i j}, \tag {38}
$$

$$
\operatorname {D e c}: \mathbb {R} ^ {\ell} \times \mathbb {R} ^ {\ell} \rightarrow (0, 1), \quad \operatorname {D e c} \left(z _ {i}, z _ {j}\right) = \operatorname {s i g m o i d} \left(z _ {i} ^ {T} z _ {j}\right), \tag {39}
$$

$$
L: \{0, 1 \} \times (0, 1) \rightarrow \mathbb {R} ^ {+}, L (y, \hat {y}) = y \log (\hat {y}) + (1 - y) \log (1 - \hat {y}). \tag {40}
$$

Then the loss is

$$
\begin{array}{l} \mathcal {L} = - \sum_ {(i, j)} L (\operatorname {G t} (i), \operatorname {D e c} (z _ {i}, z _ {j})) \\ = - \sum_ {(i, j)} A _ {i j} \log \left(\operatorname {s i g m o i d} \left(z _ {i} ^ {T} z _ {j}\right)\right) + (1 - A _ {i j}) \log \left(1 - \operatorname {s i g m o i d} \left(z _ {i} ^ {T} z _ {j}\right)\right). \tag {41} \\ \end{array}
$$

See [56, 85, 86] for more sophisticated examples.

# 3. Graph classification

Graph classification can be done like the node classification example but with one additional step. After the encoder produces the node embeddings, apply a global aggregator (e.g. entry-wise addition), which combines all node embeddings produced by the encoder into a single feature vector. This feature vector represents the graph and can be converted into a prediction, as done in the node classification example.

Specifically, consider a set of graphs $\vec { \mathcal { G } }$ , and for notational convenience, for any graph $G \in { \mathcal { G } }$ , include its number of nodes, $n$ , as a subscript, $G _ { n } = G$ . Let $K$ be the number of class labels for $\vec { \mathcal { G } }$ , and let $\mathbf { y } _ { G _ { n } } = ( y _ { G _ { n } } ( c ) ) _ { c = 1 } ^ { K }$ be the ground truth vector for $G _ { n }$ , so $y _ { G _ { n } } ( c ) = 1$ if $G _ { n }$ is in class $c$ and $y _ { G _ { n } } ( c ) = 0$ otherwise. Then for a matrix $\Theta \in \mathbb { R } ^ { \ell \times K }$ with trainable parameters, the ground truth, decoder and loss functions are

$$
\operatorname {G t}: \mathcal {G} \rightarrow \{0, 1 \} ^ {K}, \quad \operatorname {G t} (G _ {n}) = \mathbf {y} _ {G _ {n}} = \left(y _ {G _ {n}} (c)\right) _ {c = 1} ^ {K}, \tag {42}
$$

$$
\operatorname {D e c}: \mathbb {R} ^ {\ell \times n} \times \mathbb {R} ^ {\ell \times K} \rightarrow (0, 1) ^ {K}, \quad \operatorname {D e c} ((z _ {i}) _ {i = 1} ^ {n}, \Theta) = \operatorname {s o f t m a x} \left(\sum_ {i = 1} ^ {n} z _ {i} ^ {T} \Theta\right), \tag {43}
$$

$$
L: \{0, 1 \} ^ {K} \times [ 0, 1 ] ^ {K} \rightarrow \mathbb {R} ^ {+}, \quad L (\mathbf {y}, \hat {y}) = - \mathbf {y} ^ {T} \log (\hat {y}). \tag {44}
$$

Then as in the node classification example, $L$ is the cross-entropy loss function. The loss for the network is

$$
\begin{array}{l} \mathcal {L} = - \sum_ {G _ {n}} L \left(\operatorname {G t} \left(G _ {n}\right), \operatorname {D e c} \left(\left(z _ {i}\right) _ {i = 1} ^ {n}, \Theta\right)\right) \\ = - \sum_ {G _ {n}} \mathbf {y} _ {G _ {n}} ^ {T} \log \left(\operatorname {s o f t m a x} \left(\sum_ {i = 1} ^ {n} z _ {i} ^ {T} \Theta\right)\right), \tag {45} \\ \end{array}
$$

where $\boldsymbol { \beta }$ is a subset of $\vec { \mathcal { G } }$ , typically selected for model training. Global mean and max aggregators are also possible. See [44, 53, 87, 46].

# 4. Community detection

Modularity is one of the most commonly used graph clustering metrics in the literature, [88, 89]. Suppose we are trying to partition the graph’s nodes into

$K > 1$ clusters. Define

$$
\delta (i, j) = \left\{ \begin{array}{l l} 1 & \text {i f i a n d j b e l o n g t o t h e s a m e c l u s t e r ,} \\ 0 & \text {o t h e r w i s e .} \end{array} \right. \tag {46}
$$

Let $E$ denote the number of edges of the graph, and let $d _ { i }$ be the degree of node $i$ . Then the modularity metric is

$$
\mathcal {Q} = \frac {1}{2 E} \sum_ {i, j \in \mathcal {N}} \left(A _ {i j} - \frac {d _ {i} d _ {j}}{2 E}\right) \delta (i, j). \tag {47}
$$

Hence, $\mathcal { Q }$ measures the divergence of the number of edges within clusters from what one would expect by random chance, that is, if the graph had the same node degrees but its edges were assigned by a uniform distribution. See [90] for the construction of random graphs with given node degrees, a method called the configuration model. Note that $\mathcal { Q }$ can be positive or negative, and a large positive value indicates an unusual number of intra-cluster edges, thereby indicating meaningful community structure.

Tsitsulin et al. [50] used the modularity metric to define the loss function for their GNN. First, they re-write $\mathcal { Q }$ in a convenient form for a gradient descent optimization. Let $\mathbf { d } = ( d _ { i } ) _ { i \in \mathcal { N } }$ be the node degree vector, and let

$$
B = A - \frac {\mathbf {d} \mathbf {d} ^ {T}}{2 E}. \tag {48}
$$

Then

$$
\mathcal {Q} = \frac {1}{2 E} T r \left(C ^ {T} B C\right), \tag {49}
$$

where $C \in \{ 0 , 1 \} ^ { | N | \times K }$ is the cluster assignment matrix (i.e. $C _ { i k } = 1$ if node $i$ belongs to cluster $k$ , and $C _ { i k } = 0$ otherwise).

Next, relax the entries of $C$ by allowing them to take values in the interval [0, 1]. This way we can apply continuous optimization methods to $\mathcal { Q }$ , which is differentiable with respect to the entries of $C$ . Specifically, let $\Theta \in \mathbb { R } ^ { \ell \times K }$ be a learnable parameter matrix, and define the decoder by:

$$
\operatorname {D e c}: \mathbb {R} ^ {\ell} \times \mathbb {R} ^ {\ell \times K} \rightarrow [ 0, 1 ] ^ {K}, \quad \operatorname {D e c} \left(z _ {i}, \Theta\right) = \operatorname {s o f t m a x} \left(z _ {i} ^ {T} \Theta\right) \tag {50}
$$

and row $i$ of $C$ is given by

$$
C = \operatorname {D e c} \left(z _ {i}, \Theta\right) \in [ 0, 1 ] ^ {K}. \tag {51}
$$

Next we will define the ground truth function. This is somewhat of a misnomer for community detection problems because these problems have no ground truth, but it will serve the same purpose: guiding training. Define

$$
\operatorname {G t}: \mathcal {N} \times \mathcal {N} \rightarrow \mathbb {R} \quad : (i, j) \rightarrow B _ {i j} = A _ {i j} - \frac {d _ {i} d _ {j}}{2 E}. \tag {52}
$$

Then the decoder outputs probability estimates that a node is in a given cluster, which determine meaningful communities when the loss

$$
\mathcal {L} = - \frac {1}{2 E} T r \left(C ^ {T} B C\right) \tag {53}
$$

has a large negative value. Notice that $\mathcal { L }$ is differentiable, so the graph neural network can be trained in an end-to-end fashion. See [50] for implementation details, which include a regularity term not included here.

We just discussed an unsupervised approach to community detection with graph neural networks, but there is also semi-supervised community detection. Here, the modeler incorporates knowledge that some nodes must be in the same class (must-link constraints) and some nodes cannot be in the same class (cannot-link constraints), [91]. Supervised community detection also exists, but this is not common and may be regarded as a type of node classification problem [92].

# 5. Node regression

Our final example illustrates that problems with a time variable can fit within the same framework. A common node regression problem is to predict numerical values of traffic speed and volume at sensors located on a road network. These models can be complex, but a relatively simple one appears in [93]. For each time $t \in \mathbb N$ , their ground truth, decoder and loss functions are

$$
\mathrm {G t} _ {t}: \mathcal {N} \rightarrow \mathbb {R}, \quad \mathrm {G t} _ {t} (i) \in \mathbb {R}, \tag {54}
$$

$$
\operatorname {D e c} _ {t}: \mathbb {R} ^ {\ell} \times \mathbb {R} ^ {\ell + 1} \rightarrow \mathbb {R}, \quad \operatorname {D e c} _ {t} (z, \Theta , b) = \sigma \left(z _ {i} ^ {T} \Theta_ {t} + b\right), \tag {55}
$$

$$
L: \mathbb {R} \times \mathbb {R} \rightarrow \mathbb {R} ^ {+}, \quad L (y, \hat {y}) = (y - \hat {y}) ^ {2}, \tag {56}
$$

where $\sigma$ is a nonlinear function, and $\Theta _ { t }$ is a vector and $b$ is a scalar, both trainable. The goal is to predict the next $T _ { \mathrm { m a x } } ~ > ~ 0$ times steps into the future, so their loss includes an average over those time steps. Specifically, their loss is the mean square error loss given by

$$
\begin{array}{l} \mathcal {L} = \frac {1}{T _ {\max } \left| \mathcal {N} _ {s} \right|} \sum_ {t = 1} ^ {T _ {\max }} \sum_ {i \in \mathcal {N} _ {s}} L \left(\operatorname {G t} _ {t} (i), \operatorname {D e c} _ {t} \left(z _ {i}, \Theta , b\right)\right) \tag {57} \\ = \frac {1}{T _ {\operatorname* {m a x}} | \mathcal {N} _ {s} |} \sum_ {t = 1} ^ {T _ {\operatorname* {m a x}}} \sum_ {i \in \mathcal {N} _ {s}} \left(\mathrm {G t} _ {t} (i) - \sigma \left(z _ {i} ^ {T} \Theta_ {t} + b\right)\right) ^ {2}, \\ \end{array}
$$

where $\mathcal { N } _ { s }$ are nodes with sensors. A loss function defined by mean absolute error is used by the top performing models [94, 95].

# 4.3 Learning paradigms

Inductive and transductive learning are the two common paradigms for reasoning with graph neural networks. In inductive learning, no test data is available during

training, whereas in transductive learning, all test data except the test labels are available during training. This means that inductive learning for node classification is the usual supervised learning. For an example with the node classification task, consider a coauthor network, where each node is an author, an edge between two nodes indicates the two authors have worked together, and node features represent key words from their papers [96]. In inductive learning, we may have a test graph that covers the years 2000-2007 and a separate training graph that covers the years 2000-2004. The goal is to predict the most active fields of study for authors in the test graph who are absent from the training graph. In transductive learning, the training graph is the same graph from 2000-2007, but the labels of the test set would be withheld during training.

# 5 Experiments

This section complements the previous theoretical sections with experimental results. The goal is to describe the behavior of GNNs under several training and dataset conditions. Our experiments focus on GCN, GATv2, and GraphSAGE because they are commonly used as benchmarks and many GNN architectures are built on top of them, for example [97, 98, 99, 56, 50, 100]. Table 1 summarizes important properties of these GNNs. Our experiments include two other graph models: Multilayer Perceptron (MLP), which only uses node features, and Deep-Walk, which only uses edges. All experiments are in the transductive setting. Two limitations are that none of our datasets are large and we only consider the node classification task.

Thirteen open-source datasets are used: seven high homophily datasets and six low homophily ones. The high homophily datasets are citation networks (Cora, PubMed, CiteSeer, DBPL [101, 102]), co-purchase networks (AmazonComputers, AmazonPhoto [96]), and a coauthor network (CoauthorCS [96]). The low homophily datasets are webpage-webpage networks (WikipediaSquirrel, WikipediaChameleon, WikipediaCrocodile, Cornell, Wisconsin [41, 103]) and a co-occurrence network (Actor [103]). All datasets are homogeneous graphs, which means they have a single node type (e.g. ”article”) and a single edge type (e.g. ”is cited by”). The Squirrel, Chameleon and Crocodile datasets are node regression datasets, so we transform them into node classification networks by partitioning the range of values into five parts, where each part defines a class. The remaining ten are natively node classification datasets.

Edge homophily and the signal-to-noise ratio (SNR) of node features are two measures of complexity in an attributed graph. Edge homophily is the fraction of edges that connects two nodes of the same class [71]. SNR is a measure of node features, where roughly speaking, it is the squared distance between the mean of each class compared to the variance within each class. Specifically, let $C$ be the node classes, and for each class $i \in C$ , let $F _ { i }$ be the node features of class $i$ . Define

the signal $S = \{ \mathrm { M e a n } ( F _ { i } ) - \mathrm { M e a n } ( F _ { j } ) \} _ { i , j \in C }$ . Then

$$
\operatorname {S N R} = \frac {1}{| C |} \frac {\left\| S \right\| ^ {2}}{\sum_ {j \in C} \operatorname {V a r} \left(F _ {j}\right)}. \tag {58}
$$

where the $\| \cdot \|$ is the $\ell ^ { 2 }$ norm, and the factor $1 / | C |$ comes from averaging: divide the numerator by $| C | ^ { 2 }$ and the denominator by $| C |$ . Table 2 records these characteristics for each dataset.

Table 2. Datasets and their homophily and SNR values   

<table><tr><td>Dataset</td><td>Num Nodes</td><td>Homo- phily</td><td>SNR</td><td>Dataset</td><td>Num Nodes</td><td>Homo- phily</td><td>SNR</td></tr><tr><td>Photo</td><td>7650</td><td>0.83</td><td>0.17</td><td>Crocodile</td><td>11631</td><td>0.25</td><td>0.047</td></tr><tr><td>DBLP</td><td>17716</td><td>0.83</td><td>0.032</td><td>Chameleon</td><td>2277</td><td>0.24</td><td>0.039</td></tr><tr><td>Cora</td><td>2708</td><td>0.81</td><td>0.069</td><td>Squirrel</td><td>5201</td><td>0.22</td><td>0.011</td></tr><tr><td>CoauthorCS</td><td>18333</td><td>0.81</td><td>0.17</td><td>Actor</td><td>7600</td><td>0.22</td><td>0.016</td></tr><tr><td>PubMed</td><td>19717</td><td>0.80</td><td>0.071</td><td>Wisconsin</td><td>251</td><td>0.20</td><td>0.31</td></tr><tr><td>Computers</td><td>13752</td><td>0.78</td><td>0.12</td><td>Cornell</td><td>183</td><td>0.13</td><td>0.22</td></tr><tr><td>CiteSeer</td><td>3327</td><td>0.74</td><td>0.04</td><td></td><td></td><td></td><td></td></tr></table>

Unlike computer vision or language models, people tend to train GNNs from random parameters instead of fine-tuning them from a pre-trained model, which is doable because they are relatively small. We take the same approach. PyTorch Geometric is the framework for all experiments. In Section 5.1, the models are run in an off-the-shelf manner without tuning any hyperparameters except the number of training epochs. In Section 5.2, the hyperparameters (e.g. the number of message-passsing layers) of GNNs are tuned to each dataset, where a GitHub repository called GraphGym manages the experiments1 [104]. All experiments are run 25 times.

# 5.1 Baseline node classification performance

The baseline GCN, GATv2 and GraphSAGE architectures are two layer messagepassing networks with 16 hidden dimensions and no pre-processing or post-processing layers, DeepWalk also has 16 hidden dimensions, MLP is a three-layer fully connected network where the first layer has 128 hidden dimensions and the second one has 64. In the literature, GAT is a benchmark more often than GATv2, but we use GATv2 because it consistently outperforms GAT in our experiments [78]. In all cases, the models are trained for at most 200 epochs with a learning rate of 0.1 and a train/val/test split. We use two training sizes, 80% or $1 \%$ , and the labels not used for training are divided evenly among the validation and test sets.

Table 3 lists CPU computation times for each model on the Cora dataset [101], which is a dataset often used for benchmarking. The graph convolutional models

have comparable processing times as MLP, while the attentional network, GATv2, is somewhat slower. The shallow embedding model, DeepWalk, is by far the slowest.

Table 3. Average CPU time in minutes: training and evaluation of Cora dataset   

<table><tr><td></td><td>GCN</td><td>GraphSAGE</td><td>GATv2</td><td>MLP</td><td>DeepWalk</td></tr><tr><td>Minutes</td><td>2.26 ± 0.26</td><td>2.23 ± 0.28</td><td>4.03 ± 0.23</td><td>2.58 ± 0.31</td><td>57.96 ± 4.29</td></tr></table>

The theory in Section 4.1 indicates that more flexible models should do better on low homophily graphs and the more rigid ones should outperform on high homophily graphs. This is illustrated in Table 6, which provides node classification accuracy scores for each model under each training condition. GATv2 and GCN use similar addition-based aggregation functions $\oplus$ , as described in Equation (25), but GATv2 is more flexible than GCN because it is attentional rather than convolutional. Accordingly, we see that GATv2 outperforms GCN on the low homophily (i.e. high edge complexity) graphs. As noted in Section 4.1, GraphSAGE is a convolutional network that is more flexible than GATv2 in the function $\Lambda$ (see Equation 31 and Table 1). This results in GraphSAGE outperforming GATv2 on low homophily graphs. In contrast, GraphSAGE’s $\Lambda$ function hurts performance on high homophily (i.e. low edge complexity) graphs, as shown by the improved performance of GATv2 and GCN in these settings. In fact, the top performing model on high homophily graphs is the most rigid one, GCN. The greatest advantage of GNNs over MLP is on high homophily datasets with little training data, which suggests that GNNs make effective use of edge information in this setting (because MLP does not use edge information). DeepWalk performs almost as well on high homophily datasets as GNNs, but because it relies entire edge information, it performs the worst on low homophily datasets.

Table 4. Average node classification accuracy of off-the-shelf GNN architectures on low and high homophily dataset collections with 1% and 80% of node labels for training.

Average Node Classification Accuracy of Default Designs   

<table><tr><td>Model Name</td><td>80% Training
High Homophily</td><td>80% Training
Low Homophily</td><td>1% Training
High Homophily</td><td>1% Training
Low Homophily</td></tr><tr><td>GCN</td><td>85.31</td><td>37.69</td><td>72.57</td><td>30.99</td></tr><tr><td>GATv2</td><td>86.95</td><td>44.89</td><td>69.41</td><td>30.93</td></tr><tr><td>GraphSAGE</td><td>83.77</td><td>56.69</td><td>65.22</td><td>34.33</td></tr><tr><td>MLP</td><td>81.67</td><td>54.97</td><td>46.52</td><td>33.89</td></tr><tr><td>DeepWalk</td><td>81.11</td><td>33.97</td><td>69.67</td><td>24.70</td></tr></table>

At first sight, it seems odd that MLP, which does not use edge information, tends to do better on high homophily graphs than low homophily ones. This is

a reflection that the SNR values of the datasets are largely correlated with their homophily values. The only exceptions are the Cornell and Wisconsin datasets, which have low homophily and high SNR. The unusually small size of these dataset hurts the accuracy of MLP despite the their high SNR values.

Table 5. Correlation of homophily and SNR, excluding the Cornell and Wisconsin datasets.   

<table><tr><td>Measure</td><td>Coefficient</td><td>p-value</td></tr><tr><td>Pearson Correlation</td><td>0.62</td><td>0.04</td></tr><tr><td>Kendall Tau</td><td>0.45</td><td>0.06</td></tr></table>

The tendency to have low SNR of node features in low homophily datasets presents an additional challenge when working with these data sets. However, the literature largely focuses on creating GNNs that effectively handle edge complexity (i.e. low homphily) while not mentioning that the node features of these datasets are often poorly separated between classes as well.

# 5.2 Hyperparameters and node classification accuracy

In this section we analyze the effect of hyperparameters on node classification accuracy, which we do one hyperparameter at a time. We analyze hyperparameters one at a time, and we study their effect separately for different training and dataset conditions. Existing studies show which hyperparameters were most influential in building the best model, but they do not show the practical effect of selecting the right hyperparameter options in terms of the evaluation metric [104, 105, 106] - it could be that modifying the hyperparameters makes very little to no difference in performance. We find that under “easy” and “hard” training and conditions (see Section 5.2.1), modifying the hyperparameters makes little difference beyond tuning the number of hidden dimensions. While automated hyperparameter tuning tools can be helpful for finding good hyperparameter configurations in a given dataset, they are not useful for this study, so we do not use them here.

# 5.2.1 Adjusting the number of hidden dimensions

Table 6 shows the average improvement in node classification accuracy over the baseline GNN designs for a range of hidden dimensions. While increasing the hidden dimensions tends to significantly improve performance for the high homophily graphs, it does not on the low homophily one. At first sight, this is surprising because in principle, larger hidden dimensions should enable modeling of more complex relationships at the possible expense of overfitting due to the additional parameters. But in practice, we see that overfitting is not a problem even for very small datasets like Wisconsin and Cornell. It is likely that getting exposure to the test features during training is helping to avoid overfitting - we expect that overfitting would be a bigger problem for inductive inference. Next, using more

hidden dimensions should help GNN performance on the more complex, low homophily datasets, but the low homophily graphs showed the least improvement. In Section 5.2.3, we see that in the 80% training size regime, there is significant improvement by tuning the other hyperparameters such as the number of layers and the skip connections. This indicates that increasing the number of hidden dimensions needs to be paired with other structural improvements to see significant improvement in node classification accuracy.

Table 6. The average improvement of node classification accuracy for the tuned designs over the default ones on high and low dataset collections, with 1% and 80% of node labels for training.   
Average Node Classification Accuracy Improvement over Default Designs   

<table><tr><td>Model Name (Difficulty)</td><td>80% Training High Homophily (Easy)</td><td>80% Training Low Homophily (Medium)</td><td>1% Training High Homophily (Medium)</td><td>1% Training Low Homophily (Hard)</td></tr><tr><td>GCN-32</td><td>+2.53</td><td>+0.34</td><td>+3.28</td><td>+0.40</td></tr><tr><td>GCN-64</td><td>+2.59</td><td>+1.47</td><td>+3.57</td><td>+0.97</td></tr><tr><td>GCN-128</td><td>+2.86</td><td>+1.65</td><td>+3.89</td><td>+0.28</td></tr><tr><td>GCN-256</td><td>+2.67</td><td>+1.20</td><td>+4.41</td><td>+0.78</td></tr><tr><td>GATv2-32</td><td>+0.13</td><td>+0.17</td><td>+2.11</td><td>-0.19</td></tr><tr><td>GATv2-64</td><td>+0.18</td><td>+0.26</td><td>+2.72</td><td>-0.44</td></tr><tr><td>GATv2-128</td><td>+0.28</td><td>+0.66</td><td>+2.73</td><td>+0.96</td></tr><tr><td>GATv2-256</td><td>+0.11</td><td>+0.33</td><td>+2.16</td><td>-0.10</td></tr><tr><td>GraphSAGE-32</td><td>+2.93</td><td>+0.66</td><td>+4.17</td><td>+0.36</td></tr><tr><td>GraphSAGE-64</td><td>+4.12</td><td>+1.22</td><td>+6.27</td><td>+1.44</td></tr><tr><td>GraphSAGE-128</td><td>+4.59</td><td>+0.58</td><td>+6.67</td><td>+1.32</td></tr><tr><td>GraphSAGE-256</td><td>+4.53</td><td>+0.62</td><td>+7.53</td><td>+0.13</td></tr></table>

We divide the dataset/training configurations into three conditions: the easy condition is where the graph is high homophily with 80% training data, the hard condition is where the graph is low homophily with only $1 \%$ training data, and the other two configurations are the medium difficulty condition. The most benefit to tuning the hidden dimensions occurs in medium difficulty conditions.

Because GNNs with 128 hidden dimensions tend to perform relatively well, we compare future hyperparameter configurations to its performance.

# 5.2.2 Adjusting the number of training epochs

We use 128 hidden dimensions for each GNN and plot their performance over the number of training epochs. Figure 3 shows that on high homophily graphs, node classification accuracy generally stabilizes and improves over 400 epochs, which is in-line with previous research [104]. In contrast, performance falls sharply for several low homophily graphs after 25 training epochs. Message-passing layers

![](images/27588c04660ccc314d7163fd7d77df208a4d7bdf1902f7f065b71913323e30bf.jpg)  
(a)

![](images/5e4cc33d5bcef3d91a48898d2f994c1d0f7d2fa4c4b955a30832cdb6c85ab279.jpg)  
(b)   
Figure 3. These figures provide the test set performance for the tuned GCN model on each dataset for medium difficulty graph complexity and training conditions. Plots for GATv2 and GraphSAGE look similar.

aggregate information over node neighborhoods, so for low homophily graphs, they tend to aggregate conflicting information, which can hurt performance. Additional layers to process node features without sharing neighborhood information could help. To this end, in Section 5.2.3, we add pre- and post-processing layers and tune other hyperparameters, and we recover a training plot for the low homophily graphs that resembles Figure 3b (see Figure 3a).

# 5.2.3 Adjusting the number of layers and other hyperparameters

GNNs have many design variables in addition to their hidden dimensions. We use the results of You et al. [104] to decide which design variables to focus on. Our fixed design variables are listed in Table 7, and we investigate how performance changes by tuning the other six, given in Table 8. You et al. [104] provide performance rankings of hyperparameter configurations but no information on their contributions to improving the evaluation metric score, so we address this question for node classification.

Table 7. Hyperparameter values shared by top performing designs in You et al. [104].   

<table><tr><td>Parameter</td><td>Value</td><td>Parameter</td><td>Value</td></tr><tr><td>Activation</td><td>PReLU</td><td>Batch Size</td><td>32</td></tr><tr><td>Batch Norm</td><td>True</td><td>Optimizer</td><td>Adam</td></tr><tr><td>Dropout</td><td>False</td><td>Epochs</td><td>400</td></tr></table>

Table 8. Hyperparameter values to tune   

<table><tr><td>Parameter</td><td>Tuning Order</td><td>Starting Value</td><td>Options</td></tr><tr><td>Message-Passing Layers</td><td>1</td><td>2</td><td>1, 2, 3, 4, 5, 6, 7, 8</td></tr><tr><td>Post-Processing Layers</td><td>2</td><td>1</td><td>1, 2, 3</td></tr><tr><td>Pre-Processing Layers</td><td>3</td><td>1</td><td>1, 2, 3</td></tr><tr><td>Layer Connectivity</td><td>4</td><td>Skip Sum</td><td>None, Skip Sum, Skip Concatenate</td></tr><tr><td>Aggregation Function</td><td>5</td><td>Mean</td><td>Add, Mean, Max</td></tr><tr><td>Learning Rate</td><td>6</td><td>0.01</td><td>0.005, 0.01, 0.0125, 0.015</td></tr></table>

We choose each option from Table 8 in a greedy fashion, by first finding the best option for the number of message-passing layers and then proceeding according to the tuning order in the table. All models are trained for 400 epochs. Every dataset is partitioned between training and test sets, and the best hyperparameter selection is the one with the highest average test set accuracy from 25 experiments. Following You et al. [104], we adjust the number of hidden layers to make the size of each design comparable, and thus enable a fair comparison of them.

Table 9. The average improvement of node classification accuracy for the tuned designs over the default design with 128 hidden dimensions (see Section 5.2.1) on high and low dataset collections, with 1% and 80% of node labels for training. The hyperparameters for each algorithm have been tuned to each dataset.

Average Node Classification Accuracy Improvement over Default with 128 Hidden Dims   

<table><tr><td>Model Name (Difficulty)</td><td>80% Training High Homophily (Easy)</td><td>80% Training Low Homophily (Medium)</td><td>1% Training High Homophily (Medium)</td><td>1% Training Low Homophily (Hard)</td></tr><tr><td>GCN</td><td>+0.57</td><td>+22.98</td><td>+0.93</td><td>-0.22</td></tr><tr><td>GATv2</td><td>+1.53</td><td>+16.39</td><td>+4.57</td><td>-0.92</td></tr><tr><td>GraphSAGE</td><td>+0.15</td><td>+6.18</td><td>+4.02</td><td>-3.89</td></tr></table>

Table 9 shows there is little or no value in tuning the structure of the GNN to the dataset in the easy or hard conditions. In fact, the off-the-shelf models outperform the hyperparameter tuned ones in the hard condition. This may be due to starting from a suboptimal design compared to the design chosen by the architecture creators, and additionally there being little benefit to tuning any individual hyperparameter selection beyond that.

The most benefit for tuning the GNN design occurs when the training and dataset conditions are of medium difficulty. Although the tuned GraphSAGE performs best on low homophily graphs and the tuned GCN performs best on high homophily ones, once tuned, all models perform comparably. Table 10 indicates that most of the gain on the low homophily graphs with plenty of training data

comes from the Cornell and Wisconsin datasets. Cornell and Wisconsin are special in that they are small datasets with low homophily and a high SNR among their node features. Having fewer nodes may have made the GNN performance more sensitive to improvements, and the datasets having a high SNR may have enabled a reasonably high node classification accuracy, with the appropriate hyperparameter configuration.

Table 10. The average improvement of node classification accuracy for tuned designs over default ones with 128 hidden dimensions (see Section 5.2.1) on the collection of Cornell and Wisconsin datasets versus the collection of other low homophily datasets, with 80% of nodes training. The hyperparameters for each algorithm have been tuned to each dataset.

<table><tr><td colspan="3">Node Classification Accuracy Improvement</td></tr><tr><td>80% Training</td><td>Cornell &amp; Wisconsin</td><td>Other Low Homophily</td></tr><tr><td>GCN</td><td>+49.32</td><td>+9.37</td></tr><tr><td>GATv2</td><td>+41.58</td><td>+3.80</td></tr><tr><td>GraphSAGE</td><td>+22.89</td><td>-2.12</td></tr></table>

The following sections analyze hyperparameter configurations from the medium difficulty conditions.

Hyperparameter selection for improved node classification accuracy

![](images/0f7227654b9f28278537e5fefb4517e25ed393ebbb3ce6f7bce3ec4669dd9fe6.jpg)  
(a)

![](images/af7092c6f208b506b4b59bc4e43b350c28e94ec138d2e82c6061c5221a60b0cb.jpg)  
(b)   
Figure 4. Following the greedy hyperparameter tuning process described in Section 5.2.3, the figures show the improved node classification accuracy after tuning the given design component of the GNN compared to the design before it was tuned.

Table 11. The mean value and $p$ -value of the number of layers and the learning rates of the hyperparameter tuned models. The hyperparemeters were tuned for each model on the low and high homophily dataset collections with 1% and 80% of node labels for training. For each hyperparameter, the $p$ -value is for the null hypothesis that the selections over the collection of datasets are drawn from a random distribution.

<table><tr><td rowspan="3">80% Training Low Homophily</td><td colspan="8">Statistics of Design Parameters of Tuned GNNs</td></tr><tr><td colspan="2">Pre Layers</td><td colspan="2">MP Layers</td><td colspan="2">Post Layers</td><td colspan="2">LR</td></tr><tr><td>mean</td><td>p-value</td><td>mean</td><td>p-value</td><td>mean</td><td>p-value</td><td>mean</td><td>p-value</td></tr><tr><td>GCN</td><td>2.17</td><td>0.23</td><td>3.67</td><td>0.21</td><td>3.0</td><td>0.0</td><td>0.024</td><td>0.0</td></tr><tr><td>GATv2</td><td>2.0</td><td>0.41</td><td>3.83</td><td>0.27</td><td>2.83</td><td>0.0</td><td>0.012</td><td>0.17</td></tr><tr><td>GraphSAGE</td><td>1.83</td><td>0.4</td><td>6.5</td><td>0.01</td><td>2.17</td><td>0.23</td><td>0.011</td><td>0.34</td></tr><tr><td>1% Training High Homophily</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GCN</td><td>1.57</td><td>0.12</td><td>6.86</td><td>0.0</td><td>1.43</td><td>0.05</td><td>0.010</td><td>0.37</td></tr><tr><td>GATv2</td><td>1.71</td><td>0.25</td><td>5.86</td><td>0.05</td><td>1.43</td><td>0.05</td><td>0.010</td><td>0.46</td></tr><tr><td>GraphSAGE</td><td>2.29</td><td>0.13</td><td>7.71</td><td>0.0</td><td>1.71</td><td>0.25</td><td>0.0075</td><td>0.02</td></tr></table>

Table 12. The most common and $\%$ occurring values of the skip connections and aggregation functions on the low and high homophily dataset collections, with 1% and 80% of nodes training. The hyperparameters for the algorithms have been tuned to each dataset. The $\%$ occurring field says how often the most common selection occurred.

<table><tr><td rowspan="3">80% Training Low Homophily</td><td colspan="4">Statistics of Design Parameters of Tuned GNNs</td></tr><tr><td colspan="2">Skip Connections</td><td colspan="2">Aggregation</td></tr><tr><td>most common</td><td>% occurring</td><td>most common</td><td>% occurring</td></tr><tr><td>GCN</td><td>skip concat</td><td>66.67</td><td>max</td><td>50.0</td></tr><tr><td>GATv2</td><td>skip concat</td><td>100.0</td><td>add</td><td>50.0</td></tr><tr><td>GraphSAGE</td><td>skip sum</td><td>66.67</td><td>add</td><td>66.67</td></tr><tr><td>1% Training High Homophily</td><td></td><td></td><td></td><td></td></tr><tr><td>GCN</td><td>skip sum</td><td>71.43</td><td>max</td><td>71.43</td></tr><tr><td>GATv2</td><td>skip sum</td><td>100.0</td><td>max</td><td>57.14</td></tr><tr><td>GraphSAGE</td><td>none</td><td>71.43</td><td>add</td><td>42.86</td></tr></table>

Figure 4 shows the improvement due to each hyperparameter relative to the design before that hyperparameter was tuned. We see that the number of messagepassing layers is the most important hyperparameter to tune on high homophily graphs, and Table 11 shows that a larger number of message-passing layers is best. (In Section 5.3, we provide a qualitative description of this when we evaluate how node features separate among classes). This preference is reinforced from there being fewer training data because deeper networks have a larger receptive field.

In contrast, Figure 4 shows that a broader collection of parameters could be profitably tuned to improve performance on low homophily graphs. On low homophily graphs, aggregating over neighborhoods produces a signal from conflicting information, so we should expect that the number of message-passing layers in the tuned networks would be lower. It is somewhat surprising that message-passing layers are helpful at all, but we see that having some message-passing layers tends to significantly outperform the node feature-only model, MLP. GNNs on these graphs tend to have higher numbers of pre- and post-processing layers, which helps to tune the features according to the node’s class, independent of their neighbors’ features.

The importance of skip connections is hard to determine from Figure 4 alone. Figure 4 shows little improvement for a given model and component, it is not clear whether this is because the starting design is the best selection or the component has little impact on performance. To answer this, Tables 11 and 12 inform the statistical likelihood of getting the results by random chance. For example, the starting designs all use skip sum connections. Figure 4 indicates that this option is not important for improving GATv2 performance in the 1% training and high homophily configuration, but Tables 11 and 12 show that skip sums are the top selection for GATv2 every time. This means that skip connections are important

![](images/ee71dbf8b2e86582852526e2218dcfd0aaef30287a8d742e18d1c6ba3974a8ca.jpg)  
(a)

![](images/6f0dbd5ad2c60e7b9ad1230b3f3b4935ab3cedd57680aea8a6fb03e58834e3c6.jpg)  
(b)   
Figure 5. These figures provide the test set performance for the tuned GCN model on each dataset for the medium difficulty conditions. Plots for the GATv2 and GraphSAGE models look the similar.

and skip sum is the best option for GATv2. On the other hand, Figure 4 and Table 12 show GraphSAGE performance tends to be best without skip connections on high homophily graphs. GraphSAGE already has a skip connection from its function $\Lambda$ being concatenation, so it apparently does not need another one.

Tuning the remaining hyperparameters appears to have relatively little effect.

# Node classification accuracy per training epoch, again

Recall that Figure 3a shows a sharp decrease in performance at around 25 epochs for several low homophily datasets. After tuning the other hyperparameters, Figure 5b shows that node classification accuracy generally stabilizes and improves for these datasets, like they do for the high homophily graphs. We hypothesize that the extra processing provided by pre- and post-processing layers without neighborhood node feature information is especially beneficial for this.

# Comparison with top-performing off-the-shelf GNNs

For context, we compare these results with a class of models called RevGNNs that are used in models toward the top of the leaderboard on node classification datasets of Open Graph Benchmark. RevGNNs provide us ground truth for “good” node classification performance for comparison. They are built on top of the GCN, GAT or GraphSAGE encoders, and are designed in a way that makes computations independent of the depth of the network [99]. This means they can be arbitrarily deep without running out of memory, although training time increases. We apply the RevGNNs in an off-the-shelf manner. All RevGNNs have 160 hidden dimensions, and 4 layers, and they were trained for 200 epochs, using an 80/10/10

train/val/test split. Tables 13 shows the hyperparameter tuned GNNs perform comparably or outperform the off-the-shelf RevGNNs.

Table 13. Node classification accuracy comparison of models on low and high homophily dataset collections, with 1% and 80% of node labels for training.   

<table><tr><td colspan="3">Node Classification Accuracy</td></tr><tr><td>Model Name</td><td>80% Training Low Homophily</td><td>1% Training High Homophily</td></tr><tr><td>GCN-128</td><td>39.34</td><td>76.46</td></tr><tr><td>\( GCNTuned \)</td><td>62.32</td><td>77.39</td></tr><tr><td>RevGCN</td><td>50.93</td><td>77.86</td></tr><tr><td>GATv2-128</td><td>45.55</td><td>72.14</td></tr><tr><td>GATv2Tuned</td><td>61.94</td><td>76.71</td></tr><tr><td>RevGATv2</td><td>53.64</td><td>77.99</td></tr><tr><td>GraphSAGE-128</td><td>57.27</td><td>71.89</td></tr><tr><td>GraphSAGE tuned</td><td>63.45</td><td>75.91</td></tr><tr><td>RevSAGE</td><td>59.61</td><td>75.87</td></tr></table>

# 5.3 Qualitative description of GNN learning

Recall from Section 4.1 that each layer of the GNN transforms the node feature vectors into new feature vectors that are inputs to the next layer. Figure 6 describes the energy of the signal and noise (defined in Equation (58)) at each epoch in the final hidden layer of each model. As the number of training epochs gets larger, the energy of the noise stays flat while that of the signal gets larger, which means that node feature vectors generally separate between classes while their within-class variance stays the same. In contrast, experiments show that MLP performance does not improve after around 100 epochs, as indicated in Figures 6c and 6d.

Di Giovanni et al. [107] mathematically analyze an energy potential as it passes through the message-passing layers, where the potential concerns the energies of the node feature signal and noise. Figures 7a and 7b experimentally illustrate signal and noise energies as node features pass through all layer types. The noise drops significantly in the pre-processing layers. In the high homophily case, the noise drops again in the message-passing layers. This is intuitive because the messagepassing layers aggregate information from their neighboring nodes, which tend to have the same class label and similar node features. This brings together similar node features, reducing the node feature noise. Figures 7c and 7d show what happens when the message-passing layer is removed. Here, there are MLP models with a comparable number of layers to what we have in tuned GNNs, but the noise of the node features is flat.

GNNs do not get the same advantage on low homophily graphs because neighboring nodes tend to be of different classes so their features have different infor-

![](images/4bf8bed1f6bd6a42c4d31c8a877c77c645f1c53b2adf7375ac26c17be7adb028.jpg)

![](images/6101bda03c8d1ef230eeeb01403c93e7c11066808f766663e265199686b9be58.jpg)  
(b)

![](images/69b8103d0ac6d394c69b45fec6314389aa07c953fde0a4af3ab8877ae4cf3afc.jpg)  
(c)

![](images/a2d879140d95255c39eed8231c3b79ba0149a297e604f2a9869d0bb346628ec6.jpg)  
(d)   
Figure 6. These figures show the energy of the signal and the noise for each model, averaged over all datasets in each medium difficulty case.

mation. This makes node features from different classes more similar. Consistent with this, the energy of the signal in GCN and GATv2 is smaller after the messagepassing step in Figure 7b. Only the energy of the signal for GraphSAGE is larger following the message-passing layers. GraphSAGE’s use of concatenation for the function $\Lambda$ explains this. Concatenation allows it to learn on each node’s feature vector directly instead of first mixing with the aggregate of its neighbors’ vectors, so the node features can more easily separate by class.

Message-passing layers in a GNN provide node features with information from the neighbors, and then post-processing layers further refine the embeddings, leading to further separation of the classes. Notice from Figures 7c and 7d that MLP models do not benefit from having more than three layers.

![](images/f0c259d023dc5307e5dbce2a3183f32966264875aa3a632bd48cdfa4d22db51b.jpg)

![](images/7254e5659fa7293ed5e36aa754b0b25a4efdd8fff327416d608df3e19c8586c9.jpg)  
(b)

![](images/4530666fcd05a2b0d1f0230d8b768bc90937861818918bd8c0467a657d014d41.jpg)  
(c)

![](images/a978f5dcc39036040e52c5e6a929d7d04d6ce14f65687f42970b5799e37f54b6.jpg)  
(d)   
Figure 7. These figures show the energy of the signal and the noise in the final hidden layer of each layer type. For some of the low homophily datasets, the tuned GraphSAGE design has no hidden post-processing layers, so the plot does not include the post-processing layers for this model.

# 6 Conclusion

A decade ago deep convolutional neural networks for image classification initiated a revolution where feature learning was integrated into the training process of a neural network, and this was subsequently extended to data structures like irregular graphs. The encoder-decoder framework neatly describes these models, and the shortcomings of simpler encoder-decoder models motivates the use of more complicated Graph Neural Networks (GNNs). Graph neural networks have attracted considerable attention due to state-of-the-art results on a range of graph analysis

tasks and datasets, but because of the great variety of graphs and graph analysis tasks, they can be difficult to use for those new to the field. As such, we hope our overview of GNNs, their construction and behavior on a variety of datasets and training conditions, has prepared the reader to solve diverse graph problems and understand the technical aspects of literature.

# A Open-source GNN libraries

To our knowledge, PyTorch Geometric and Deep Graph Library are the largest and most widely used libraries.

• PyTorch Geometric

This library is built on PyTorch and its design aims to stay close to usual Py-Torch [108]. It provides well-documented examples, and benchmark datasets and most state-of-the art GNN models are available here. Many GNNs from the literature are implemented, and it supports multi-GPU processing.

• Deep Graph Library

This library is sponsored by AWS, NSF, NVIDIA and Intel [109]. It supports multi-GPU processing and the PyTorch, TensorFlow and Apache MXNet frameworks. They have well-documented examples and example code for many state-of-the art models.

• GeometricFlux.jl

This is a Julia library for geometric deep learning [110], as described in [4]. It supports deep learning in a range of settings: Graphs and sets; grids and Euclidean spaces; groups and homogeneous spaces; geodesics and manifolds; gauges and bundles. It also offers GPU support and has integration with GNN benchmark datasets. It supports both graph network architectures, which are more general graph models than graph neural networks [1], and message-passing architectures.

• Spektral

This library is built on TensorFlow 2 and Keras [111]. It intends to feel close to the Keras API and to be flexible and easy to use. It provides code for the standard components of GNNs as well as example implementations of GNNs on specific datasets.

• Jraph

This is a library written in JAX, which is a language that enables automatic differentiation of python and numpy. It is created by DeepMind and inherits some design properties from its earlier library, Graph Nets. Like Graph Nets, it supports building graph networks and is a lightweight library with utilities for working with graphs. Unlike Graph Nets, it has a model zoo of graph neural network models.

Table 14. The node classification accuracy of default designs (see Table 4).   

<table><tr><td colspan="4">Node Classification Accuracy: 80% Training and 16 Hidden Dimensions</td></tr><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>89.48 ± 3.47</td><td>93.55 ± 0.37</td><td>77.95 ± 8.99</td></tr><tr><td>DBLP</td><td>85.16 ± 0.29</td><td>85.11 ± 0.32</td><td>84.84 ± 0.30</td></tr><tr><td>Cora</td><td>88.28 ± 0.74</td><td>85.71 ± 0.79</td><td>88.25 ± 0.66</td></tr><tr><td>CoauthorCS</td><td>93.22 ± 0.21</td><td>91.24 ± 0.29</td><td>94.26 ± 0.21</td></tr><tr><td>PubMed</td><td>87.21 ± 0.24</td><td>86.99 ± 0.26</td><td>87.89 ± 0.38</td></tr><tr><td>Computers</td><td>77.34 ± 5.09</td><td>89.88 ± 0.39</td><td>77.45 ± 5.16</td></tr><tr><td>CiteSeer</td><td>76.46 ± 0.93</td><td>76.19 ± 0.88</td><td>75.77 ± 1.03</td></tr><tr><td>Crocodile</td><td>61.66 ± 0.96</td><td>68.26 ± 0.69</td><td>73.26 ± 0.44</td></tr><tr><td>Chameleon</td><td>46.90 ± 1.17</td><td>55.34 ± 1.39</td><td>64.07 ± 1.30</td></tr><tr><td>Squirrel</td><td>29.53 ± 1.07</td><td>37.68 ± 0.73</td><td>45.24 ± 0.70</td></tr><tr><td>Cctor</td><td>27.66 ± 0.68</td><td>28.89 ± 0.65</td><td>35.08 ± 0.64</td></tr><tr><td>Wisconsin</td><td>32.00 ± 3.30</td><td>40.00 ± 2.80</td><td>66.31 ± 4.05</td></tr><tr><td>Cornell</td><td>28.42 ± 2.80</td><td>39.16 ± 3.25</td><td>56.21 ± 4.11</td></tr></table>

# • Graph Nets

This is a DeepMind library in TensorFlow and Sonnet library for building graph networks as described in [1]. It supports both CPU and GPU processing, but as of this writing, it is not actively maintained.

# • Stellar Graph

This library is built on TensorFlow 2 and uses the Keras API. It supports a variety of graph machine learning tasks including node classification, link prediction and graph classification on a homogeneous graphs, heterogeneous graphs and other graph types. As of this writing, it is not actively maintained.

# • PyTorch GNN

This Microsoft library is written in PyTorch and is primarily engineered to be fast on sparse graphs. Graph neural network models from several papers and graph analysis tasks are implemented. This library is not actively maintained as of this writing.

# B Results on Individual Datasets

All tables report results with 95% confidence intervals.

Table 15. The node classification accuracy of default designs (see Table 4).   
Node Classification Accuracy: 1% Training and 16 Hidden Dimensions   

<table><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>78.35 ± 7.05</td><td>78.71 ± 1.81</td><td>65.22 ± 5.18</td></tr><tr><td>DBLP</td><td>74.80 ± 0.69</td><td>74.63 ± 0.89</td><td>72.50 ± 0.96</td></tr><tr><td>Cora</td><td>60.55 ± 2.78</td><td>54.89 ± 2.78</td><td>55.93 ± 1.52</td></tr><tr><td>CoauthorCS</td><td>89.94 ± 0.22</td><td>77.13 ± 0.79</td><td>86.10 ± 0.36</td></tr><tr><td>PubMed</td><td>81.16 ± 0.37</td><td>79.14 ± 0.46</td><td>78.86 ± 0.33</td></tr><tr><td>Computers</td><td>74.83 ± 4.76</td><td>72.64 ± 1.63</td><td>54.20 ± 4.22</td></tr><tr><td>CiteSeer</td><td>48.38 ± 1.70</td><td>48.74 ± 2.26</td><td>43.70 ± 1.88</td></tr><tr><td>Crocodile</td><td>46.44 ± 1.58</td><td>45.38 ± 1.67</td><td>52.91 ± 1.30</td></tr><tr><td>Chameleon</td><td>30.58 ± 1.59</td><td>30.56 ± 1.35</td><td>31.47 ± 1.77</td></tr><tr><td>Squirrel</td><td>22.16 ± 0.57</td><td>22.39 ± 0.66</td><td>26.93 ± 1.08</td></tr><tr><td>Cctor</td><td>24.44 ± 0.38</td><td>23.98 ± 0.49</td><td>24.48 ± 0.62</td></tr><tr><td>Wisconsin</td><td>34.03 ± 5.74</td><td>38.60 ± 4.18</td><td>40.22 ± 4.83</td></tr><tr><td>Cornell</td><td>28.26 ± 5.58</td><td>24.65 ± 5.56</td><td>29.96 ± 5.05</td></tr></table>

Table 16. The node classification accuracy of default designs (see Table 6).   
Node Classification Accuracy: 80% Training and 32 Hidden Dimensions   

<table><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>93.45 ± 0.40</td><td>93.46 ± 0.35</td><td>91.68 ± 3.23</td></tr><tr><td>DBLP</td><td>85.85 ± 0.35</td><td>85.15 ± 0.35</td><td>84.94 ± 0.45</td></tr><tr><td>Cora</td><td>88.84 ± 0.48</td><td>86.65 ± 0.95</td><td>88.35 ± 0.63</td></tr><tr><td>CoauthorCS</td><td>93.20 ± 0.26</td><td>91.61 ± 0.38</td><td>94.36 ± 0.25</td></tr><tr><td>PubMed</td><td>87.51 ± 0.25</td><td>86.97 ± 0.31</td><td>88.16 ± 0.29</td></tr><tr><td>Computers</td><td>88.49 ± 0.56</td><td>90.15 ± 0.35</td><td>83.45 ± 4.05</td></tr><tr><td>CiteSeer</td><td>77.49 ± 0.90</td><td>75.63 ± 0.96</td><td>75.96 ± 0.90</td></tr><tr><td>Crocodile</td><td>62.01 ± 0.79</td><td>68.76 ± 0.69</td><td>72.79 ± 0.55</td></tr><tr><td>Chameleon</td><td>46.52 ± 1.19</td><td>54.53 ± 1.48</td><td>64.59 ± 1.58</td></tr><tr><td>Squirrel</td><td>29.60 ± 1.00</td><td>38.17 ± 0.89</td><td>44.64 ± 0.77</td></tr><tr><td>Cctor</td><td>28.22 ± 0.78</td><td>29.06 ± 0.72</td><td>35.23 ± 0.66</td></tr><tr><td>Wisconsin</td><td>32.62 ± 3.05</td><td>40.46 ± 2.93</td><td>67.08 ± 3.05</td></tr><tr><td>Cornell</td><td>29.26 ± 2.98</td><td>39.37 ± 4.67</td><td>59.79 ± 3.73</td></tr></table>

Table 17. The node classification accuracy of default designs (see Table 6).   
Node Classification Accuracy: 1% Training and 32 Hidden Dimensions   

<table><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>88.67 ± 0.74</td><td>81.73 ± 1.60</td><td>77.02 ± 3.34</td></tr><tr><td>DBLP</td><td>75.93 ± 0.69</td><td>75.32 ± 0.60</td><td>73.60 ± 0.75</td></tr><tr><td>Cora</td><td>63.57 ± 2.02</td><td>59.40 ± 2.53</td><td>56.16 ± 2.20</td></tr><tr><td>CoauthorCS</td><td>89.90 ± 0.25</td><td>80.23 ± 0.74</td><td>86.97 ± 0.46</td></tr><tr><td>PubMed</td><td>80.94 ± 0.43</td><td>79.10 ± 0.53</td><td>79.23 ± 0.36</td></tr><tr><td>Computers</td><td>82.05 ± 1.44</td><td>75.01 ± 1.26</td><td>64.95 ± 3.38</td></tr><tr><td>CiteSeer</td><td>49.90 ± 2.22</td><td>49.88 ± 2.31</td><td>47.79 ± 2.66</td></tr><tr><td>Crocodile</td><td>46.59 ± 1.74</td><td>47.85 ± 1.78</td><td>52.64 ± 0.90</td></tr><tr><td>Chameleon</td><td>31.42 ± 1.55</td><td>29.48 ± 1.21</td><td>31.61 ± 1.56</td></tr><tr><td>Squirrel</td><td>22.95 ± 0.40</td><td>22.46 ± 0.58</td><td>28.08 ± 0.71</td></tr><tr><td>Cctor</td><td>24.46 ± 0.50</td><td>24.03 ± 0.51</td><td>25.50 ± 0.47</td></tr><tr><td>Wisconsin</td><td>34.98 ± 5.18</td><td>36.60 ± 4.74</td><td>38.00 ± 5.26</td></tr><tr><td>Cornell</td><td>27.91 ± 5.23</td><td>24.00 ± 5.30</td><td>32.30 ± 5.78</td></tr></table>

Table 18. The node classification accuracy of default designs (see Table 6).   
Node Classification Accuracy: 80% Training and 64 Hidden Dimensions   

<table><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>93.69 ± 0.26</td><td>93.54 ± 0.36</td><td>94.24 ± 0.54</td></tr><tr><td>DBLP</td><td>85.69 ± 0.31</td><td>85.03 ± 0.26</td><td>84.72 ± 0.36</td></tr><tr><td>Cora</td><td>88.69 ± 0.70</td><td>86.10 ± 0.78</td><td>88.59 ± 0.68</td></tr><tr><td>CoauthorCS</td><td>93.48 ± 0.20</td><td>92.04 ± 0.33</td><td>94.69 ± 0.16</td></tr><tr><td>PubMed</td><td>87.64 ± 0.29</td><td>87.27 ± 0.39</td><td>88.23 ± 0.36</td></tr><tr><td>Computers</td><td>88.96 ± 0.42</td><td>90.27 ± 0.31</td><td>88.83 ± 0.75</td></tr><tr><td>CiteSeer</td><td>77.09 ± 0.82</td><td>75.68 ± 0.96</td><td>75.95 ± 0.68</td></tr><tr><td>Crocodile</td><td>63.46 ± 0.75</td><td>68.93 ± 0.50</td><td>73.05 ± 0.37</td></tr><tr><td>Chameleon</td><td>47.48 ± 1.72</td><td>56.16 ± 1.23</td><td>64.21 ± 0.97</td></tr><tr><td>Squirrel</td><td>30.20 ± 0.72</td><td>38.50 ± 0.87</td><td>44.85 ± 0.77</td></tr><tr><td>Cctor</td><td>28.59 ± 0.57</td><td>28.37 ± 0.85</td><td>34.87 ± 0.65</td></tr><tr><td>Wisconsin</td><td>36.62 ± 2.83</td><td>38.92 ± 3.82</td><td>68.62 ± 3.29</td></tr><tr><td>Cornell</td><td>28.63 ± 4.13</td><td>40.00 ± 4.25</td><td>61.89 ± 4.01</td></tr></table>

Table 19. The node classification accuracy of default designs (see Table 6).   
Node Classification Accuracy: 1% Training and 64 Hidden Dimensions   

<table><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>89.13 ± 0.69</td><td>81.97 ± 1.02</td><td>80.10 ± 1.56</td></tr><tr><td>DBLP</td><td>76.01 ± 0.50</td><td>75.08 ± 0.94</td><td>73.99 ± 0.65</td></tr><tr><td>Cora</td><td>62.96 ± 2.78</td><td>61.57 ± 2.32</td><td>58.48 ± 2.77</td></tr><tr><td>CoauthorCS</td><td>89.81 ± 0.26</td><td>80.42 ± 0.75</td><td>86.80 ± 0.55</td></tr><tr><td>PubMed</td><td>80.98 ± 0.40</td><td>79.66 ± 0.42</td><td>79.14 ± 0.38</td></tr><tr><td>Computers</td><td>83.09 ± 0.49</td><td>76.20 ± 0.81</td><td>71.25 ± 3.31</td></tr><tr><td>CiteSeer</td><td>51.01 ± 1.87</td><td>49.99 ± 2.21</td><td>50.67 ± 2.07</td></tr><tr><td>Crocodile</td><td>46.40 ± 1.59</td><td>47.51 ± 1.30</td><td>53.96 ± 0.65</td></tr><tr><td>Chameleon</td><td>31.84 ± 1.19</td><td>30.03 ± 1.80</td><td>33.39 ± 1.38</td></tr><tr><td>Squirrel</td><td>22.66 ± 0.62</td><td>22.84 ± 0.57</td><td>27.80 ± 0.82</td></tr><tr><td>Cctor</td><td>23.26 ± 0.80</td><td>23.91 ± 0.66</td><td>24.78 ± 0.54</td></tr><tr><td>Wisconsin</td><td>38.32 ± 4.71</td><td>33.56 ± 4.93</td><td>41.40 ± 5.31</td></tr><tr><td>Cornell</td><td>29.26 ± 6.03</td><td>25.09 ± 5.55</td><td>33.26 ± 6.12</td></tr></table>

Table 20. The node classification accuracy of default designs (see Table 6).   
Node Classification Accuracy: 80% Training and 128 Hidden Dimensions   

<table><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>93.87 ± 0.32</td><td>93.21 ± 0.29</td><td>95.24 ± 0.37</td></tr><tr><td>DBLP</td><td>85.69 ± 0.34</td><td>85.33 ± 0.26</td><td>84.89 ± 0.22</td></tr><tr><td>Cora</td><td>88.81 ± 0.65</td><td>85.97 ± 0.72</td><td>88.29 ± 0.70</td></tr><tr><td>CoauthorCS</td><td>93.49 ± 0.20</td><td>92.34 ± 0.29</td><td>94.73 ± 0.20</td></tr><tr><td>PubMed</td><td>88.19 ± 0.34</td><td>87.31 ± 0.29</td><td>88.65 ± 0.29</td></tr><tr><td>Computers</td><td>90.23 ± 0.31</td><td>90.27 ± 0.34</td><td>89.96 ± 0.51</td></tr><tr><td>CiteSeer</td><td>76.91 ± 0.88</td><td>76.19 ± 0.93</td><td>76.80 ± 0.94</td></tr><tr><td>Crocodile</td><td>63.13 ± 0.53</td><td>69.28 ± 0.67</td><td>73.42 ± 0.60</td></tr><tr><td>Chameleon</td><td>47.06 ± 1.88</td><td>54.46 ± 1.33</td><td>64.14 ± 1.34</td></tr><tr><td>Squirrel</td><td>29.16 ± 0.85</td><td>39.45 ± 0.88</td><td>45.02 ± 0.73</td></tr><tr><td>Cctor</td><td>28.42 ± 0.66</td><td>29.06 ± 0.48</td><td>35.15 ± 0.54</td></tr><tr><td>Wisconsin</td><td>36.92 ± 3.25</td><td>40.62 ± 3.28</td><td>68.00 ± 2.95</td></tr><tr><td>Cornell</td><td>31.37 ± 3.48</td><td>40.42 ± 4.20</td><td>57.89 ± 3.40</td></tr></table>

Table 21. The node classification accuracy of default designs (see Table 6).   
Node Classification Accuracy: 1% Training and 128 Hidden Dimensions   

<table><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>89.57 ± 0.63</td><td>81.27 ± 0.94</td><td>80.39 ± 1.61</td></tr><tr><td>DBLP</td><td>76.43 ± 0.69</td><td>75.21 ± 0.71</td><td>74.24 ± 0.54</td></tr><tr><td>Cora</td><td>64.08 ± 2.31</td><td>60.72 ± 2.97</td><td>58.55 ± 2.20</td></tr><tr><td>CoauthorCS</td><td>89.76 ± 0.24</td><td>81.40 ± 0.84</td><td>86.25 ± 0.50</td></tr><tr><td>PubMed</td><td>81.06 ± 0.33</td><td>79.19 ± 0.36</td><td>79.54 ± 0.28</td></tr><tr><td>Computers</td><td>83.53 ± 0.50</td><td>75.27 ± 1.04</td><td>72.91 ± 2.27</td></tr><tr><td>CiteSeer</td><td>50.80 ± 1.88</td><td>51.95 ± 1.86</td><td>51.30 ± 2.08</td></tr><tr><td>Crocodile</td><td>46.90 ± 1.72</td><td>47.44 ± 2.25</td><td>54.55 ± 0.85</td></tr><tr><td>Chameleon</td><td>31.24 ± 1.63</td><td>30.40 ± 1.36</td><td>34.75 ± 1.59</td></tr><tr><td>Squirrel</td><td>23.46 ± 0.51</td><td>22.36 ± 0.67</td><td>28.84 ± 0.77</td></tr><tr><td>Cctor</td><td>24.11 ± 0.64</td><td>23.71 ± 0.62</td><td>25.15 ± 0.45</td></tr><tr><td>Wisconsin</td><td>33.30 ± 5.34</td><td>36.13 ± 5.30</td><td>34.29 ± 5.57</td></tr><tr><td>Cornell</td><td>28.61 ± 5.98</td><td>31.30 ± 5.87</td><td>36.30 ± 4.84</td></tr></table>

Table 22. The node classification accuracy of default designs (see Table 6).   
Node Classification Accuracy: 80% Training and 256 Hidden Dimensions   

<table><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>93.77 ± 0.24</td><td>93.46 ± 0.37</td><td>94.94 ± 0.30</td></tr><tr><td>DBLP</td><td>85.33 ± 0.33</td><td>84.78 ± 0.38</td><td>84.78 ± 0.36</td></tr><tr><td>Cora</td><td>88.13 ± 1.00</td><td>86.69 ± 0.79</td><td>88.35 ± 0.91</td></tr><tr><td>CoauthorCS</td><td>93.37 ± 0.28</td><td>92.34 ± 0.33</td><td>94.57 ± 0.20</td></tr><tr><td>PubMed</td><td>88.31 ± 0.30</td><td>87.10 ± 0.26</td><td>88.52 ± 0.21</td></tr><tr><td>Computers</td><td>90.33 ± 0.36</td><td>90.06 ± 0.35</td><td>90.56 ± 0.32</td></tr><tr><td>CiteSeer</td><td>76.59 ± 0.66</td><td>75.02 ± 1.00</td><td>76.40 ± 0.96</td></tr><tr><td>Crocodile</td><td>64.04 ± 0.66</td><td>69.49 ± 0.54</td><td>73.29 ± 0.51</td></tr><tr><td>Chameleon</td><td>47.13 ± 1.35</td><td>56.14 ± 1.35</td><td>63.44 ± 1.21</td></tr><tr><td>Squirrel</td><td>29.07 ± 0.62</td><td>38.93 ± 0.85</td><td>45.93 ± 0.76</td></tr><tr><td>Cctor</td><td>28.13 ± 0.56</td><td>28.71 ± 0.70</td><td>35.62 ± 0.60</td></tr><tr><td>Wisconsin</td><td>34.92 ± 3.49</td><td>38.46 ± 3.25</td><td>68.77 ± 2.77</td></tr><tr><td>Cornell</td><td>30.11 ± 3.33</td><td>39.58 ± 5.06</td><td>56.84 ± 3.20</td></tr></table>

Table 23. The node classification accuracy of default designs (see Table 6).   
Node Classification Accuracy: 1% Training and 256 Hidden Dimensions   

<table><tr><td>Dataset</td><td>GCN</td><td>GATv2</td><td>GraphSAGE</td></tr><tr><td>Photo</td><td>89.08 ± 0.69</td><td>78.98 ± 1.62</td><td>81.93 ± 1.45</td></tr><tr><td>DBLP</td><td>76.65 ± 0.61</td><td>75.29 ± 0.93</td><td>74.45 ± 0.52</td></tr><tr><td>Cora</td><td>66.21 ± 2.31</td><td>62.17 ± 2.59</td><td>61.51 ± 1.83</td></tr><tr><td>CoauthorCS</td><td>89.64 ± 0.23</td><td>81.75 ± 1.28</td><td>84.78 ± 0.98</td></tr><tr><td>PubMed</td><td>81.21 ± 0.33</td><td>79.07 ± 0.44</td><td>79.39 ± 0.37</td></tr><tr><td>Computers</td><td>83.59 ± 0.56</td><td>72.81 ± 0.88</td><td>74.99 ± 1.95</td></tr><tr><td>CiteSeer</td><td>52.50 ± 2.28</td><td>50.94 ± 2.04</td><td>52.20 ± 1.44</td></tr><tr><td>Crocodile</td><td>47.64 ± 1.37</td><td>46.73 ± 1.63</td><td>54.07 ± 0.73</td></tr><tr><td>Chameleon</td><td>30.94 ± 1.69</td><td>32.71 ± 1.38</td><td>33.91 ± 1.35</td></tr><tr><td>Squirrel</td><td>23.23 ± 0.53</td><td>22.62 ± 0.54</td><td>28.52 ± 0.83</td></tr><tr><td>Cctor</td><td>24.67 ± 0.42</td><td>24.42 ± 0.61</td><td>25.73 ± 0.55</td></tr><tr><td>Wisconsin</td><td>36.22 ± 5.71</td><td>33.43 ± 5.22</td><td>40.16 ± 5.84</td></tr><tr><td>Cornell</td><td>27.91 ± 5.30</td><td>25.09 ± 6.26</td><td>24.39 ± 5.77</td></tr></table>

Table 24. The node classification accuracy of the tuned designs with 80% of nodes labeled for training (see Table 9).   
Node Classification Accuracy with 80% Training   

<table><tr><td>Dataset</td><td>GCN tuned</td><td>GraphSAGE tuned</td><td>GATv2 tuned</td></tr><tr><td>Photo</td><td>95.62 ± 0.18</td><td>95.68 ± 0.16</td><td>95.65 ± 0.18</td></tr><tr><td>DBLP</td><td>84.86 ± 0.18</td><td>84.59 ± 0.22</td><td>84.89 ± 0.17</td></tr><tr><td>Cora</td><td>88.00 ± 0.49</td><td>87.81 ± 0.47</td><td>88.26 ± 0.46</td></tr><tr><td>CoauthorCS</td><td>95.36 ± 0.12</td><td>95.17 ± 0.14</td><td>95.17 ± 0.13</td></tr><tr><td>PubMed</td><td>90.04 ± 0.22</td><td>89.89 ± 0.16</td><td>89.99 ± 0.21</td></tr><tr><td>Computers</td><td>91.95 ± 0.16</td><td>91.56 ± 0.18</td><td>91.95 ± 0.15</td></tr><tr><td>CiteSeer</td><td>75.35 ± 0.62</td><td>74.88 ± 0.49</td><td>75.41 ± 0.48</td></tr><tr><td>Crocodile</td><td>70.31 ± 0.36</td><td>70.31 ± 0.34</td><td>70.74 ± 0.34</td></tr><tr><td>Chameleon</td><td>61.20 ± 0.80</td><td>61.01 ± 0.78</td><td>60.66 ± 0.82</td></tr><tr><td>Squirrel</td><td>40.29 ± 0.62</td><td>42.23 ± 0.59</td><td>40.79 ± 0.68</td></tr><tr><td>Actor</td><td>35.19 ± 0.47</td><td>35.68 ± 0.36</td><td>35.26 ± 0.46</td></tr><tr><td>Wisconsin</td><td>87.37 ± 1.66</td><td>89.73 ± 1.80</td><td>87.45 ± 2.36</td></tr><tr><td>Cornell</td><td>79.57 ± 2.29</td><td>81.95 ± 2.00</td><td>76.76 ± 1.99</td></tr></table>

Table 25. The node classification accuracy of tuned designs with 1% of nodes labeled for training (see Table 9).   
Node Classification Accuracy with 1% Training   
Table 26. The node classification accuracy of RevGNNs with 80% of node labels for training on low homophily graphs (see Table 13).   

<table><tr><td>Dataset</td><td>GCN tuned</td><td>GraphSAGE tuned</td><td>GATv2 tuned</td></tr><tr><td>Photo</td><td>88.71 ± 0.68</td><td>85.98 ± 0.87</td><td>87.96 ± 0.78</td></tr><tr><td>DBLP</td><td>75.51 ± 0.60</td><td>73.75 ± 0.74</td><td>75.64 ± 0.60</td></tr><tr><td>Cora</td><td>66.62 ± 1.52</td><td>66.65 ± 2.15</td><td>63.53 ± 2.16</td></tr><tr><td>CoauthorCS</td><td>90.60 ± 0.43</td><td>89.59 ± 0.60</td><td>90.52 ± 0.22</td></tr><tr><td>PubMed</td><td>79.71 ± 0.40</td><td>78.06 ± 0.43</td><td>79.50 ± 0.45</td></tr><tr><td>Computers</td><td>82.54 ± 0.50</td><td>82.08 ± 0.80</td><td>83.51 ± 0.51</td></tr><tr><td>CiteSeer</td><td>58.01 ± 1.50</td><td>55.32 ± 1.94</td><td>56.32 ± 1.45</td></tr><tr><td>Crocodile</td><td>56.12 ± 0.78</td><td>57.02 ± 0.76</td><td>55.54 ± 0.60</td></tr><tr><td>Chameleon</td><td>33.46 ± 1.39</td><td>33.42 ± 1.22</td><td>33.52 ± 1.38</td></tr><tr><td>Squirrel</td><td>25.88 ± 0.59</td><td>27.70 ± 0.70</td><td>25.80 ± 0.47</td></tr><tr><td>Actor</td><td>25.65 ± 0.35</td><td>25.82 ± 0.51</td><td>25.66 ± 0.45</td></tr><tr><td>Wisconsin</td><td>27.93 ± 0.08</td><td>27.69 ± 0.42</td><td>27.69 ± 0.45</td></tr><tr><td>Cornell</td><td>17.25 ± 1.23</td><td>18.90 ± 1.52</td><td>17.52 ± 1.30</td></tr></table>

Node Classification Accuracy of RevGNNs   
Table 27. The node classification accuracy of RevGNNs with 1% of node labels for training on high homophily graphs (see Table 13).   

<table><tr><td>Dataset</td><td>RevGCN</td><td>RevSAGE</td><td>RevGATv2</td></tr><tr><td>Crocodile</td><td>69.01 ± 0.55</td><td>73.10 ± 0.53</td><td>71.36 ± 0.54</td></tr><tr><td>Chameleon</td><td>54.60 ± 1.18</td><td>64.10 ± 1.36</td><td>60.00 ± 1.49</td></tr><tr><td>Squirrel</td><td>36.25 ± 0.90</td><td>46.34 ± 1.07</td><td>42.63 ± 0.80</td></tr><tr><td>Actor</td><td>33.29 ± 0.61</td><td>36.91 ± 0.73</td><td>36.22 ± 0.67</td></tr><tr><td>Wisconsin</td><td>61.38 ± 3.50</td><td>71.08 ± 2.73</td><td>63.85 ± 3.28</td></tr><tr><td>Cornell</td><td>51.16 ± 4.06</td><td>66.11 ± 2.80</td><td>47.79 ± 2.91</td></tr></table>

Node Classification Accuracy of RevGNNs   

<table><tr><td>Dataset</td><td>RevGCN</td><td>RevSAGE</td><td>RevGATv2</td></tr><tr><td>Photo</td><td>88.89 ± 0.86</td><td>87.12 ± 0.81</td><td>90.21 ± 0.54</td></tr><tr><td>DBLP</td><td>77.73 ± 0.69</td><td>76.21 ± 0.72</td><td>77.71 ± 0.68</td></tr><tr><td>Cora</td><td>67.37 ± 1.63</td><td>62.41 ± 2.03</td><td>67.96 ± 1.94</td></tr><tr><td>CoauthorCS</td><td>91.59 ± 0.34</td><td>90.72 ± 0.29</td><td>90.86 ± 0.34</td></tr><tr><td>PubMed</td><td>81.83 ± 0.39</td><td>79.98 ± 0.36</td><td>81.02 ± 0.45</td></tr><tr><td>Computers</td><td>83.19 ± 0.59</td><td>79.80 ± 0.66</td><td>83.42 ± 0.46</td></tr><tr><td>CiteSeer</td><td>54.41 ± 1.74</td><td>54.87 ± 1.53</td><td>54.73 ± 1.39</td></tr></table>

# References

[1] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner et al., “Relational inductive biases, deep learning, and graph networks,” arXiv preprint arXiv:1806.01261, 2018.   
[2] W. L. Hamilton, “Graph representation learning,” Synthesis Lectures on Artifical Intelligence and Machine Learning, vol. 14, no. 3, pp. 1–159, 2020.   
[3] I. Chami, S. Abu-El-Haija, B. Perozzi, C. R´e, and K. Murphy, “Machine learning on graphs: A model and comprehensive taxonomy,” Journal of Machine Learning Research, vol. 23, no. 89, pp. 1–64, 2022.   
[4] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veliˇckovi´c, “Geometric deep learning: Grids, groups, graphs, geodesics, and gauges,” arXiv preprint arXiv:2104.13478, 2021.   
[5] L. Wu, P. Cui, J. Pei, L. Zhao, and X. Guo, “Graph neural networks: foundation, frontiers and applications,” in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 4840–4841.   
[6] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive survey on graph neural networks,” IEEE transactions on neural networks and learning systems, vol. 32, no. 1, pp. 4–24, 2020.   
[7] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun, “Graph neural networks: A review of methods and applications,” AI Open, vol. 1, pp. 57–81, 2020.   
[8] Z. Zhang, P. Cui, and W. Zhu, “Deep learning on graphs: A survey,” IEEE Transactions on Knowledge and Data Engineering, 2020.   
[9] Y. Zhou, H. Zheng, X. Huang, S. Hao, D. Li, and J. Zhao, “Graph neural networks: Taxonomy, advances, and trends,” ACM Transactions on Intelligent Systems and Technology (TIST), vol. 13, no. 1, pp. 1–54, 2022.   
[10] T. K. Rusch, M. M. Bronstein, and S. Mishra, “A survey on oversmoothing in graph neural networks,” arXiv preprint arXiv:2303.10993, 2023.   
[11] H. T. Phan, N. T. Nguyen, and D. Hwang, “Fake news detection: A survey of graph neural network methods,” Applied Soft Computing, vol. 139, p. 110235, 2023.   
[12] C. Gao, Y. Zheng, N. Li, Y. Li, Y. Qin, J. Piao, Y. Quan, J. Chang, D. Jin, X. He et al., “A survey of graph neural networks for recommender systems: Challenges, methods, and directions,” ACM Transactions on Recommender Systems, vol. 1, no. 1, pp. 1–51, 2023.

[13] S. Bhagat, G. Cormode, and S. Muthukrishnan, “Node classification in social networks,” in Social network data analytics. Springer, 2011, pp. 115–148.   
[14] S. Ahmad, M. Z. Asghar, F. M. Alotaibi, and I. Awan, “Detection and classification of social media-based extremist affiliations using sentiment analysis techniques,” Human-centric Computing and Information Sciences, vol. 9, no. 1, pp. 1–23, 2019.   
[15] T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in International Conference on Learning Representations, 2017.   
[16] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 2014, pp. 701–710.   
[17] W. L. Hamilton, R. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 1025–1035.   
[18] X. Jiang, Q. Wang, and B. Wang, “Adaptive convolution for multi-relational learning,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 978–987. [Online]. Available: https://aclanthology.org/N19-1103   
[19] B. Pandey, P. K. Bhanodia, A. Khamparia, and D. K. Pandey, “A comprehensive survey of edge prediction in social networks: Techniques, parameters and challenges,” Expert Systems with Applications, vol. 124, pp. 164–181, 2019.   
[20] Y. Koren, R. Bell, and C. Volinsky, “Matrix factorization techniques for recommender systems,” Computer, vol. 42, no. 8, pp. 30–37, 2009.   
[21] S. Wu, F. Sun, W. Zhang, X. Xie, and B. Cui, “Graph neural networks in recommender systems: a survey,” ACM Computing Surveys, vol. 55, no. 5, pp. 1–37, 2022.   
[22] S. Shekhar, D. Pai, and S. Ravindran, “Entity resolution in dynamic heterogeneous networks,” in Companion Proceedings of the Web Conference 2020, 2020, pp. 662–668.   
[23] B. Li, W. Wang, Y. Sun, L. Zhang, M. A. Ali, and Y. Wang, “Grapher: Token-centric entity resolution with graph convolutional neural networks,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, 2020, pp. 8172–8179.

[24] Z. Yu, F. Huang, X. Zhao, W. Xiao, and W. Zhang, “Predicting drug–disease associations through layer attention graph convolutional network,” Briefings in Bioinformatics, vol. 22, no. 4, p. bbaa243, 2021.   
[25] J. Gao, X. Zhang, L. Tian, Y. Liu, J. Wang, Z. Li, and X. Hu, “Mtgnn: Multi-task graph neural network based few-shot learning for disease similarity measurement,” Methods, 2021.   
[26] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich, “A review of relational machine learning for knowledge graphs,” Proceedings of the IEEE, vol. 104, no. 1, pp. 11–33, 2015.   
[27] S. Arora, “A survey on graph neural networks for knowledge graph completion,” arXiv preprint arXiv:2007.12374, 2020.   
[28] N. R. Smith, P. N. Zivich, L. M. Frerichs, J. Moody, and A. E. Aiello, “A guide for choosing community detection algorithms in social network studies: The question alignment approach,” American journal of preventive medicine, vol. 59, no. 4, pp. 597–605, 2020.   
[29] Z. Yang, R. Algesheimer, and C. J. Tessone, “A comparative analysis of community detection algorithms on artificial networks,” Scientific reports, vol. 6, no. 1, pp. 1–18, 2016.   
[30] S. Bandyopadhyay and V. Peter, “Unsupervised constrained community detection via self-expressive graph neural network,” in Uncertainty in Artificial Intelligence. PMLR, 2021, pp. 1078–1088.   
[31] D. Jin, Z. Liu, W. Li, D. He, and W. Zhang, “Graph convolutional networks meet markov random fields: Semi-supervised community detection in attribute networks,” in Proceedings of the AAAI conference on artificial intelligence, vol. 33, no. 01, 2019, pp. 152–159.   
[32] C. Wang, C. Hao, and X. Guan, “Hierarchical and overlapping social circle identification in ego networks based on link clustering,” Neurocomputing, vol. 381, pp. 322–335, 2020.   
[33] G. Tauer, K. Date, R. Nagi, and M. Sudit, “An incremental graphpartitioning algorithm for entity resolution,” Information Fusion, vol. 46, pp. 171–183, 2019.   
[34] S. Maddila, S. Ramasubbareddy, and K. Govinda, “Crime and fraud detection using clustering techniques,” Innovations in Computer Science and Engineering, pp. 135–143, 2020.   
[35] K. Wongsuphasawat, D. Smilkov, J. Wexler, J. Wilson, D. Mane, D. Fritz, D. Krishnan, F. B. Vi´egas, and M. Wattenberg, “Visualizing dataflow graphs of deep learning models in tensorflow,” IEEE transactions on visualization and computer graphics, vol. 24, no. 1, pp. 1–12, 2017.

[36] M. Burch, M. Hlawatsch, and D. Weiskopf, “Visualizing a sequence of a thousand graphs (or even more),” in Computer Graphics Forum, vol. 36, no. 3. Wiley Online Library, 2017, pp. 261–271.   
[37] X. Yin, G. Wu, J. Wei, Y. Shen, H. Qi, and B. Yin, “A comprehensive survey on traffic prediction,” arXiv preprint arXiv:2004.08555, 2020.   
[38] A. Derrow-Pinion, J. She, D. Wong, O. Lange, T. Hester, L. Perez, M. Nunkesser, S. Lee, X. Guo, B. Wiltshire et al., “Eta prediction with graph neural networks in google maps,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 3767–3776.   
[39] M. T. Schaub and S. Segarra, “Flow smoothing and denoising: Graph signal processing in the edge-space,” in 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP). IEEE, 2018, pp. 735–739.   
[40] K. Klemmer, N. S. Safir, and D. B. Neill, “Positional encoder graph neural networks for geographic data,” in International Conference on Artificial Intelligence and Statistics. PMLR, 2023, pp. 1379–1389.   
[41] B. Rozemberczki, C. Allen, and R. Sarkar, “Multi-scale attributed node embedding,” Journal of Complex Networks, vol. 9, no. 2, p. cnab014, 2021.   
[42] P. Reiser, M. Neubert, A. Eberhard, L. Torresi, C. Zhou, C. Shao, H. Metni, C. van Hoesel, H. Schopmans, T. Sommer et al., “Graph neural networks for materials science and chemistry,” Communications Materials, vol. 3, no. 1, p. 93, 2022.   
[43] V. Fung, J. Zhang, E. Juarez, and B. G. Sumpter, “Benchmarking graph neural networks for materials chemistry,” npj Computational Materials, vol. 7, no. 1, p. 84, 2021.   
[44] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural networks?” in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/forum?id=ryGs6iA5Km   
[45] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, “Neural message passing for quantum chemistry,” in International conference on machine learning. PMLR, 2017, pp. 1263–1272.   
[46] Y. Wang, J. Wang, Z. Cao, and A. Barati Farimani, “Molecular contrastive learning of representations via graph neural networks,” Nature Machine Intelligence, vol. 4, no. 3, pp. 279–287, 2022.   
[47] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and J. Huang, “Selfsupervised graph transformer on large-scale molecular data,” Advances in Neural Information Processing Systems, vol. 33, pp. 12 559–12 571, 2020.

[48] P. Li, J. Wang, Y. Qiao, H. Chen, Y. Yu, X. Yao, P. Gao, G. Xie, and S. Song, “An effective self-supervised framework for learning expressive molecular global representations to drug discovery,” Briefings in Bioinformatics, vol. 22, no. 6, p. bbab109, 2021.   
[49] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” Journal of machine learning research, vol. 9, no. 11, 2008.   
[50] A. Tsitsulin, J. Palowitch, B. Perozzi, and E. M¨uller, “Graph clustering with graph neural networks,” Journal of Machine Learning Research, vol. 24, no. 127, pp. 1–21, 2023.   
[51] A. Y. Ng, M. I. Jordan, and Y. Weiss, “On spectral clustering: Analysis and an algorithm,” in Advances in neural information processing systems, 2002, pp. 849–856.   
[52] P. Chunaev, “Community detection in node-attributed social networks: a survey,” Computer Science Review, vol. 37, p. 100286, 2020.   
[53] T. Pham, T. Tran, H. Dam, and S. Venkatesh, “Graph classification via deep learning with virtual nodes,” arXiv preprint arXiv:1708.04357, 2017.   
[54] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, “An end-to-end deep learning architecture for graph classification,” in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.   
[55] W. L. Hamilton, R. Ying, and J. Leskovec, “Representation learning on graphs: Methods and applications,” arXiv preprint arXiv:1709.05584, 2017.   
[56] M. Zhang and Y. Chen, “Link prediction based on graph neural networks,” Advances in Neural Information Processing Systems, vol. 31, pp. 5165–5175, 2018.   
[57] J. Kim, T. Kim, S. Kim, and C. D. Yoo, “Edge-labeling graph neural network for few-shot learning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 11–20.   
[58] T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel, “Neural relational inference for interacting systems,” in International Conference on Machine Learning. PMLR, 2018, pp. 2688–2697.   
[59] Y. Li, X. Sun, H. Zhang, Z. Li, L. Qin, C. Sun, and Z. Ji, “Cellular traffic prediction via a deep multi-reservoir regression learning network for multiaccess edge computing,” IEEE Wireless Communications, vol. 28, no. 5, pp. 13–19, 2021.   
[60] M. Belkin and P. Niyogi, “Laplacian eigenmaps for dimensionality reduction and data representation,” Neural computation, vol. 15, no. 6, pp. 1373–1396, 2003.

[61] R. Merris, “Laplacian matrices of graphs: a survey,” Linear algebra and its applications, vol. 197, pp. 143–176, 1994.   
[62] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola, “Distributed large-scale natural graph factorization,” in Proceedings of the 22nd international conference on World Wide Web, 2013, pp. 37–48.   
[63] S. Cao, W. Lu, and Q. Xu, “Grarep: Learning graph representations with global structural information,” in Proceedings of the 24th ACM international on conference on information and knowledge management, 2015, pp. 891–900.   
[64] M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 1105–1114.   
[65] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 855–864.   
[66] J. Qiu, Y. Dong, H. Ma, J. Li, K. Wang, and J. Tang, “Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec,” in Proceedings of the eleventh ACM international conference on web search and data mining, 2018, pp. 459–467.   
[67] L. F. Ribeiro, P. H. Saverese, and D. R. Figueiredo, “struc2vec: Learning node representations from structural identity,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 385–394.   
[68] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger, “Simplifying graph convolutional networks,” in International conference on machine learning. PMLR, 2019, pp. 6861–6871.   
[69] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural networks on graphs with fast localized spectral filtering,” Advances in neural information processing systems, vol. 29, pp. 3844–3852, 2016.   
[70] M. Balcilar, G. Renton, P. H´eroux, B. Gauzere, S. Adam, and P. Honeine, “Bridging the gap between spectral and spatial domains in graph neural networks,” arXiv preprint arXiv:2003.11702, 2020.   
[71] J. Zhu, Y. Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra, “Beyond homophily in graph neural networks: Current limitations and effective designs,” Advances in neural information processing systems, vol. 33, pp. 7793–7804, 2020.

[72] L. M. Aiello, A. Barrat, R. Schifanella, C. Cattuto, B. Markines, and F. Menczer, “Friendship prediction and homophily in social media,” ACM Transactions on the Web (TWEB), vol. 6, no. 2, pp. 1–33, 2012.   
[73] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, and K. kavukcuoglu, “Interaction networks for learning about objects, relations and physics,” in Proceedings of the 30th International Conference on Neural Information Processing Systems, ser. NIPS’16. Red Hook, NY, USA: Curran Associates Inc., 2016, p. 4509–4517.   
[74] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, “Neural message passing for quantum chemistry,” in Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, D. Precup and Y. W. Teh, Eds., vol. 70. PMLR, 06–11 Aug 2017, pp. 1263–1272. [Online]. Available: https://proceedings.mlr.press/v70/gilmer17a.html   
[75] F. Monti, D. Boscaini, J. Masci, E. Rodol`a, J. Svoboda, and M. M. Bronstein, “Geometric deep learning on graphs and manifolds using mixture model cnns,” in CVPR, 2017.   
[76] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D. Y. Yeung, “Gaan: Gated attention networks for learning on large and spatiotemporal graphs,” in 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018, 2018.   
[77] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and Y. Bengio, “Graph attention networks,” in International Conference on Learning Representations, 2018. [Online]. Available: https://openreview.net/forum? id=rJXMpikCZ   
[78] S. Brody, U. Alon, and E. Yahav, “How attentive are graph attention networks?” in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=F72ximsx7C1   
[79] D. Kim and A. Oh, “How to find your friendly neighborhood: Graph attention design with self-supervision,” in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview. net/forum?id=Wi5KUNlqWty   
[80] X. Zheng, Y. Liu, S. Pan, M. Zhang, D. Jin, and P. S. Yu, “Graph neural networks for graphs with heterophily: A survey,” arXiv preprint arXiv:2202.07082, 2022.   
[81] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally connected networks on graphs,” arXiv preprint arXiv:1312.6203, 2013.   
[82] S. Zhang, H. Tong, J. Xu, and R. Maciejewski, “Graph convolutional networks: a comprehensive review,” Computational Social Networks, vol. 6, no. 1, pp. 1–23, 2019.

[83] Y. Ma, J. Hao, Y. Yang, H. Li, J. Jin, and G. Chen, “Spectral-based graph convolutional network for directed graphs,” arXiv preprint arXiv:1907.08990, 2019.   
[84] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling, “Modeling relational data with graph convolutional networks,” in European semantic web conference. Springer, 2018, pp. 593–607.   
[85] P. Li, Y. Wang, H. Wang, and J. Leskovec, “Distance encoding: Design provably more powerful neural networks for graph representation learning,” Advances in Neural Information Processing Systems, vol. 33, pp. 4465–4478, 2020.   
[86] L. Cai, J. Li, J. Wang, and S. Ji, “Line graph neural networks for link prediction,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.   
[87] S. A. Tailor, F. Opolka, P. Lio, and N. D. Lane, “Do we need anisotropic graph neural networks?” in International Conference on Learning Representations, 2021.   
[88] S. Fortunato and D. Hric, “Community detection in networks: A user guide,” Physics reports, vol. 659, pp. 1–44, 2016.   
[89] M. E. Newman, “Modularity and community structure in networks,” Proceedings of the national academy of sciences, vol. 103, no. 23, pp. 8577–8582, 2006.   
[90] R. Van Der Hofstad, “Random graphs and complex networks,” Available on http://www. win. tue. nl/rhofstad/NotesRGCN. pdf, vol. 11, p. 60, 2009.   
[91] Y. Ren, K. Hu, X. Dai, L. Pan, S. C. Hoi, and Z. Xu, “Semi-supervised deep embedded clustering,” Neurocomputing, vol. 325, pp. 121–130, 2019.   
[92] Z. Chen, L. Li, and J. Bruna, “Supervised community detection with line graph neural networks,” in International conference on learning representations, 2020.   
[93] X. Wang, C. Chen, Y. Min, J. He, B. Yang, and Y. Zhang, “Efficient metropolitan traffic prediction based on graph recurrent neural network,” arXiv preprint arXiv:1811.00740, 2018.   
[94] C. Zheng, X. Fan, C. Wang, and J. Qi, “Gman: A graph multi-attention network for traffic prediction,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 01, 2020, pp. 1234–1241.   
[95] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent neural network: Data-driven traffic forecasting,” in International Conference on Learning Representations, 2018.

[96] O. Shchur, M. Mumme, A. Bojchevski, and S. G¨unnemann, “Pitfalls of graph neural network evaluation,” arXiv preprint arXiv:1811.05868, 2018.   
[97] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, “Adversarially regularized graph autoencoder for graph embedding,” in International Joint Conference on Artificial Intelligence 2018. Association for the Advancement of Artificial Intelligence (AAAI), 2018, pp. 2609–2615.   
[98] P. Veliˇckovi´c, W. Fedus, W. L. Hamilton, P. Li`o, Y. Bengio, and R. D. Hjelm, “Deep graph infomax,” in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/forum? id=rklz9iAcKQ   
[99] G. Li, M. M¨uller, B. Ghanem, and V. Koltun, “Training graph neural networks with 1000 layers,” in International conference on machine learning. PMLR, 2021, pp. 6437–6449.   
[100] Z. Xu, F. Y. Yan, R. Singh, J. T. Chiu, A. M. Rush, and M. Yu, “Teal: Learning-accelerated optimization of wan traffic engineering,” in Proceedings of the ACM SIGCOMM 2023 Conference, 2023, pp. 378–393.   
[101] Z. Yang, W. Cohen, and R. Salakhudinov, “Revisiting semi-supervised learning with graph embeddings,” in International conference on machine learning. PMLR, 2016, pp. 40–48.   
[102] A. Bojchevski and S. Gunnemann, “Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking,” in International Conference on Learning Representations, 2018. [Online]. Available: https: //openreview.net/forum?id=r1ZdKJ-0W   
[103] H. Pei, B. Wei, K. C.-C. Chang, Y. Lei, and B. Yang, “Geom-gcn: Geometric graph convolutional networks,” in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum? id=S1e2agrFvS   
[104] J. You, Z. Ying, and J. Leskovec, “Design space for graph neural networks,” Advances in Neural Information Processing Systems, vol. 33, 2020.   
[105] C. Gonzales, E. H. Lee, K. L. K. Lee, J. Tang, and S. Miret, “Hyperparameter optimization of graph neural networks for the opencatalyst dataset: a case study,” in AI for Accelerated Materials Design NeurIPS 2022 Workshop, 2022.   
[106] R. Taylor, V. Ojha, I. Martino, and G. Nicosia, “Sensitivity analysis for deep learning: ranking hyper-parameter influence,” in 2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI). IEEE, 2021, pp. 512–516.

[107] F. D. Giovanni, J. Rowbottom, B. P. Chamberlain, T. Markovich, and M. M. Bronstein, “Graph neural networks as gradient flows: understanding graph convolutions via energy,” 2023. [Online]. Available: https://openreview.net/forum?id=M3GzgrA7U4   
[108] M. Fey and J. E. Lenssen, “Fast graph representation learning with pytorch geometric,” arXiv preprint arXiv:1903.02428, 2019.   
[109] M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma, L. Yu, Y. Gai et al., “Deep graph library: A graph-centric, highly-performant package for graph neural networks,” arXiv preprint arXiv:1909.01315, 2019.   
[110] Y.-H. Tu, “Geometricflux. jl: a geometric deep learning library in julia,” Proceedings of JuliaCon, vol. 1, p. 1, 2020.   
[111] D. Grattarola and C. Alippi, “Graph neural networks in tensorflow and keras with spektral [application notes],” IEEE Computational Intelligence Magazine, vol. 16, no. 1, pp. 99–106, 2021.

Approved for Public Release; Distribution Unlimited. Public Release Case Number 24-3731. $\circledcirc$ 2024 The MITRE Corporation. ALL RIGHTS RESERVED.

# Reinhard Diestel

# Graph Theory

# Electronic Edition 2005

c Springer-Verlag Heidelberg, New York 1997, 2000, 2005

This is an electronic version of the third (2005) edition of the above Springer book, from their series Graduate Texts in Mathematics, vol. 173. The cross-references in the text and in the margins are active links: click on them to be taken to the appropriate page.

The printed edition of this book can be ordered via

http://www.math.uni-hamburg.de/home/diestel/books/graph.theory/ where also errata, reviews etc. are posted.

Substantial discounts and free copies for lecturers are available for course adoptions; see here.

Almost two decades have passed since the appearance of those graph theory texts that still set the agenda for most introductory courses taught today. The canon created by those books has helped to identify some main fields of study and research, and will doubtless continue to influence the development of the discipline for some time to come.

Yet much has happened in those 20 years, in graph theory no less than elsewhere: deep new theorems have been found, seemingly disparate methods and results have become interrelated, entire new branches have arisen. To name just a few such developments, one may think of how the new notion of list colouring has bridged the gulf between invariants such as average degree and chromatic number, how probabilistic methods and the regularity lemma have pervaded extremal graph theory and Ramsey theory, or how the entirely new field of graph minors and tree-decompositions has brought standard methods of surface topology to bear on long-standing algorithmic graph problems.

Clearly, then, the time has come for a reappraisal: what are, today, the essential areas, methods and results that should form the centre of an introductory graph theory course aiming to equip its audience for the most likely developments ahead?

I have tried in this book to offer material for such a course. In view of the increasing complexity and maturity of the subject, I have broken with the tradition of attempting to cover both theory and applications: this book offers an introduction to the theory of graphs as part of (pure) mathematics; it contains neither explicit algorithms nor ‘real world’ applications. My hope is that the potential for depth gained by this restriction in scope will serve students of computer science as much as their peers in mathematics: assuming that they prefer algorithms but will benefit from an encounter with pure mathematics of some kind, it seems an ideal opportunity to look for this close to where their heart lies!

In the selection and presentation of material, I have tried to accommodate two conflicting goals. On the one hand, I believe that an

introductory text should be lean and concentrate on the essential, so as to offer guidance to those new to the field. As a graduate text, moreover, it should get to the heart of the matter quickly: after all, the idea is to convey at least an impression of the depth and methods of the subject. On the other hand, it has been my particular concern to write with sufficient detail to make the text enjoyable and easy to read: guiding questions and ideas will be discussed explicitly, and all proofs presented will be rigorous and complete.

A typical chapter, therefore, begins with a brief discussion of what are the guiding questions in the area it covers, continues with a succinct account of its classic results (often with simplified proofs), and then presents one or two deeper theorems that bring out the full flavour of that area. The proofs of these latter results are typically preceded by (or interspersed with) an informal account of their main ideas, but are then presented formally at the same level of detail as their simpler counterparts. I soon noticed that, as a consequence, some of those proofs came out rather longer in print than seemed fair to their often beautifully simple conception. I would hope, however, that even for the professional reader the relatively detailed account of those proofs will at least help to minimize reading time. . .

If desired, this text can be used for a lecture course with little or no further preparation. The simplest way to do this would be to follow the order of presentation, chapter by chapter: apart from two clearly marked exceptions, any results used in the proof of others precede them in the text.

Alternatively, a lecturer may wish to divide the material into an easy basic course for one semester, and a more challenging follow-up course for another. To help with the preparation of courses deviating from the order of presentation, I have listed in the margin next to each proof the reference numbers of those results that are used in that proof. These references are given in round brackets: for example, a reference (4.1.2) in the margin next to the proof of Theorem 4.3.2 indicates that Lemma 4.1.2 will be used in this proof. Correspondingly, in the margin next to Lemma 4.1.2 there is a reference [ 4.3.2 ] (in square brackets) informing the reader that this lemma will be used in the proof of Theorem 4.3.2. Note that this system applies between different sections only (of the same or of different chapters): the sections themselves are written as units and best read in their order of presentation.

The mathematical prerequisites for this book, as for most graph theory texts, are minimal: a first grounding in linear algebra is assumed for Chapter 1.9 and once in Chapter 5.5, some basic topological concepts about the Euclidean plane and 3-space are used in Chapter 4, and a previous first encounter with elementary probability will help with Chapter 11. (Even here, all that is assumed formally is the knowledge of basic definitions: the few probabilistic tools used are developed in the

text.) There are two areas of graph theory which I find both fascinating and important, especially from the perspective of pure mathematics adopted here, but which are not covered in this book: these are algebraic graph theory and infinite graphs.

At the end of each chapter, there is a section with exercises and another with bibliographical and historical notes. Many of the exercises were chosen to complement the main narrative of the text: they illustrate new concepts, show how a new invariant relates to earlier ones, or indicate ways in which a result stated in the text is best possible. Particularly easy exercises are identified by the superscript $-$ , the more challenging ones carry a $^ +$ . The notes are intended to guide the reader on to further reading, in particular to any monographs or survey articles on the theme of that chapter. They also offer some historical and other remarks on the material presented in the text.

Ends of proofs are marked by the symbol $\boxed { \begin{array} { r l } \end{array} }$ . Where this symbol is found directly below a formal assertion, it means that the proof should be clear after what has been said—a claim waiting to be verified! There are also some deeper theorems which are stated, without proof, as background information: these can be identified by the absence of both proof and $\boxed { \begin{array} { r l } \end{array} }$ .

Almost every book contains errors, and this one will hardly be an exception. I shall try to post on the Web any corrections that become necessary. The relevant site may change in time, but will always be accessible via the following two addresses:

http://www.springer-ny.com/supplements/diestel/

http://www.springer.de/catalog/html-files/deutsch/math/3540609180.html

Please let me know about any errors you find.

Little in a textbook is truly original: even the style of writing and of presentation will invariably be influenced by examples. The book that no doubt influenced me most is the classic GTM graph theory text by Bollob´as: it was in the course recorded by this text that I learnt my first graph theory as a student. Anyone who knows this book well will feel its influence here, despite all differences in contents and presentation.

I should like to thank all who gave so generously of their time, knowledge and advice in connection with this book. I have benefited particularly from the help of N. Alon, G. Brightwell, R. Gillett, R. Halin, M. Hintz, A. Huck, I. Leader, T. Luczak, W. Mader, V. R¨odl, A.D. Scott, P.D. Seymour, G. Simonyi, M. Skoviera, R. Thomas, C. Thomassen andˇ P. Valtr. I am particularly grateful also to Tommy R. Jensen, who taught me much about colouring and all I know about $k$ -flows, and who invested immense amounts of diligence and energy in his proofreading of the preliminary German version of this book.

# About the second edition

Naturally, I am delighted at having to write this addendum so soon after this book came out in the summer of 1997. It is particularly gratifying to hear that people are gradually adopting it not only for their personal use but more and more also as a course text; this, after all, was my aim when I wrote it, and my excuse for agonizing more over presentation than I might otherwise have done.

There are two major changes. The last chapter on graph minors now gives a complete proof of one of the major results of the Robertson-Seymour theory, their theorem that excluding a graph as a minor bounds the tree-width if and only if that graph is planar. This short proof did not exist when I wrote the first edition, which is why I then included a short proof of the next best thing, the analogous result for path-width. That theorem has now been dropped from Chapter 12. Another addition in this chapter is that the tree-width duality theorem, Theorem 12.3.9, now comes with a (short) proof too.

The second major change is the addition of a complete set of hints for the exercises. These are largely Tommy Jensen’s work, and I am grateful for the time he donated to this project. The aim of these hints is to help those who use the book to study graph theory on their own, but not to spoil the fun. The exercises, including hints, continue to be intended for classroom use.

Apart from these two changes, there are a few additions. The most noticable of these are the formal introduction of depth-first search trees in Section 1.5 (which has led to some simplifications in later proofs) and an ingenious new proof of Menger’s theorem due to B¨ohme, G¨oring and Harant (which has not otherwise been published).

Finally, there is a host of small simplifications and clarifications of arguments that I noticed as I taught from the book, or which were pointed out to me by others. To all these I offer my special thanks.

The Web site for the book has followed me to

http://www.math.uni-hamburg.de/home/diestel/books/graph.theory/

I expect this address to be stable for some time.

Once more, my thanks go to all who contributed to this second edition by commenting on the first—and I look forward to further comments!

# About the third edition

There is no denying that this book has grown. Is it still as ‘lean and concentrating on the essential’ as I said it should be when I wrote the preface to the first edition, now almost eight years ago?

I believe that it is, perhaps now more than ever. So why the increase in volume? Part of the answer is that I have continued to pursue the original dual aim of offering two different things between one pair of covers:

a reliable first introduction to graph theory that can be used either for personal study or as a course text;   
a graduate text that offers some depth in selected areas.

For each of these aims, some material has been added. Some of this covers new topics, which can be included or skipped as desired. An example at the introductory level is the new section on packing and covering with the Erd˝os-P´osa theorem, or the inclusion of the stable marriage theorem in the matching chapter. An example at the graduate level is the Robertson-Seymour structure theorem for graphs without a given minor: a result that takes a few lines to state, but one which is increasingly relied on in the literature, so that an easily accessible reference seems desirable. Another addition, also in the chapter on graph minors, is a new proof of the ‘Kuratowski theorem for higher surfaces’—a proof which illustrates the interplay between graph minor theory and surface topology better than was previously possible. The proof is complemented by an appendix on surfaces, which supplies the required background and also sheds some more light on the proof of the graph minor theorem.

Changes that affect previously existing material are rare, except for countless local improvements intended to consolidate and polish rather than change. I am aware that, as this book is increasingly adopted as a course text, there is a certain desire for stability. Many of these local improvements are the result of generous feedback I got from colleagues using the book in this way, and I am very grateful for their help and advice.

There are also some local additions. Most of these developed from my own notes, pencilled in the margin as I prepared to teach from the book. They typically complement an important but technical proof, when I felt that its essential ideas might get overlooked in the formal write-up. For example, the proof of the Erd˝os-Stone theorem now has an informal post-mortem that looks at how exactly the regularity lemma comes to be applied in it. Unlike the formal proof, the discussion starts out from the main idea, and finally arrives at how the parameters to be declared at the start of the formal proof must be specified. Similarly, there is now a discussion pointing to some ideas in the proof of the perfect graph theorem. However, in all these cases the formal proofs have been left essentially untouched.

The only substantial change to existing material is that the old Theorem 8.1.1 (that $c r ^ { 2 } n$ edges force a $T K ^ { r }$ ) seems to have lost its nice (and long) proof. Previously, this proof had served as a welcome opportunity to explain some methods in sparse extremal graph theory. These methods have migrated to the connectivity chapter, where they now live under the roof of the new proof by Thomas and Wollan that 8kn edges make a $2 k$ -connected graph $k$ -linked. So they are still there, leaner than ever before, and just presenting themselves under a new guise. As a consequence of this change, the two earlier chapters on dense and sparse extremal graph theory could be reunited, to form a new chapter appropriately named as Extremal Graph Theory.

Finally, there is an entirely new chapter, on infinite graphs. When graph theory first emerged as a mathematical discipline, finite and infinite graphs were usually treated on a par. This has changed in recent years, which I see as a regrettable loss: infinite graphs continue to provide a natural and frequently used bridge to other fields of mathematics, and they hold some special fascination of their own. One aspect of this is that proofs often have to be more constructive and algorithmic in nature than their finite counterparts. The infinite version of Menger’s theorem in Section 8.4 is a typical example: it offers algorithmic insights into connectivity problems in networks that are invisible to the slick inductive proofs of the finite theorem given in Chapter 3.3.

Once more, my thanks go to all the readers and colleagues whose comments helped to improve the book. I am particularly grateful to Imre Leader for his judicious comments on the whole of the infinite chapter; to my graph theory seminar, in particular to Lilian Matthiesen and Philipp Spr¨ussel, for giving the chapter a test run and solving all its exercises (of which eighty survived their scrutiny); to Angelos Georgakopoulos for much proofreading elsewhere; to Melanie Win Myint for recompiling the index and extending it substantially; and to Tim Stelldinger for nursing the whale on page 366 until it was strong enough to carry its baby dinosaur.

May 2005

RD

Preface vii

1. The Basics

1.1 Graphs* 2   
1.2 The degree of a vertex* 5   
1.3 Paths and cycles* 6   
1.4 Connectivity* 10   
1.5 Trees and forests* 13   
1.6 Bipartite graphs* 17   
1.7 Contraction and minors* 18   
1.8 Euler tours* 22   
1.9 Some linear algebra 23   
1.10 Other notions of graphs 28

Exercises 30

Notes 32

2. Matching, Covering and Packing 33

2.1 Matching in bipartite graphs* 34   
2.2 Matching in general graphs(∗) 39   
2.3 Packing and covering 44   
2.4 Tree-packing and arboricity 46   
2.5 Path covers 49

Exercises 51

Notes 53

3. Connectivity 55

3.1 2-Connected graphs and subgraphs* 55   
3.2 The structure of 3-connected graphs(∗) 57   
3.3 Menger’s theorem* 62   
3.4 Mader’s theorem 67   
3.5 Linking pairs of vertices(∗) 69

Exercises 78

Notes 80

4. Planar Graphs 83

4.1 Topological prerequisites* 84   
4.2 Plane graphs* 86   
4.3 Drawings 92   
4.4 Planar graphs: Kuratowski’s theorem* 96   
4.5 Algebraic planarity criteria 101   
4.6 Plane duality 103

Exercises 106

Notes 109

5. Colouring

5.1 Colouring maps and planar graphs* 112   
5.2 Colouring vertices* 114   
5.3 Colouring edges* 119   
5.4 List colouring 121   
5.5 Perfect graphs 126

Exercises 133

Notes 136

6. Flows 139

6.1 Circulations(∗) 140   
6.2 Flows in networks* 141   
6.3 Group-valued flows 144   
6.4 $k$ -Flows for small $k$ 149   
6.5 Flow-colouring duality 152   
6.6 Tutte’s flow conjectures 156

Exercises 160

Notes 161

7. Extremal Graph Theory 163

7.1 Subgraphs* 164   
7.2 Minors(∗) . 169   
7.3 Hadwiger’s conjecture* 172   
7.4 Szemer´edi’s regularity lemma 175   
7.5 Applying the regularity lemma 183

Exercises 189

Notes 192

8. Infinite Graphs 195

8.1 Basic notions, facts and techniques* 196   
8.2 Paths, trees, and ends(∗) 204   
8.3 Homogeneous and universal graphs* 212   
8.4 Connectivity and matching 216   
8.5 The topological end space 226

Exercises 237

Notes 244

9. Ramsey Theory for Graphs 251

9.1 Ramsey’s original theorems* 252   
9.2 Ramsey numbers(∗) 255   
9.3 Induced Ramsey theorems 258   
9.4 Ramsey properties and connectivity(∗) . 268

Exercises 271

Notes 272

10. Hamilton Cycles 275

10.1 Simple sufficient conditions* 275   
10.2 Hamilton cycles and degree sequences* 278   
10.3 Hamilton cycles in the square of a graph 281

Exercises 289

Notes 290

11. Random Graphs 293

11.1 The notion of a random graph* 294   
11.2 The probabilistic method* 299   
11.3 Properties of almost all graphs* 302   
11.4 Threshold functions and second moments 306

Exercises 312

Notes 313

12. Minors, Trees and WQO 315

12.1 Well-quasi-ordering* 316   
12.2 The graph minor theorem for trees* 317   
12.3 Tree-decompositions 319   
12.4 Tree-width and forbidden minors 327   
12.5 The graph minor theorem(∗) $^ { ( * ) }$ 341

Exercises 350

Notes 354

A. Infinite sets 357   
B. Surfaces 361

Hints for all the exercises . 369

Index . 393

Symbol index . 409

This chapter gives a gentle yet concise introduction to most of the terminology used later in the book. Fortunately, much of standard graph theoretic terminology is so intuitive that it is easy to remember; the few terms better understood in their proper setting will be introduced later, when their time has come.

Section 1.1 offers a brief but self-contained summary of the most basic definitions in graph theory, those centred round the notion of a graph. Most readers will have met these definitions before, or will have them explained to them as they begin to read this book. For this reason, Section 1.1 does not dwell on these definitions more than clarity requires: its main purpose is to collect the most basic terms in one place, for easy reference later.

From Section 1.2 onwards, all new definitions will be brought to life almost immediately by a number of simple yet fundamental propositions. Often, these will relate the newly defined terms to one another: the question of how the value of one invariant influences that of another underlies much of graph theory, and it will be good to become familiar with this line of thinking early.

By $\mathbb { N }$ we denote the set of natural numbers, including zero. The set $\mathbb { Z } / n \mathbb { Z }$ of integers modulo $n$ is denoted by $\mathbb { Z } _ { n }$ ; its elements are written as ${ \bar { i } } : = i + n \mathbb { Z }$ . For a real number $x$ we denote by $\lfloor x \rfloor$ the greatest integer $\leqslant x$ , and by $\lceil x \rceil$ the least integer $\geqslant x$ . Logarithms written as ‘log’ are taken at base 2; the natural logarithm will be denoted by ‘ln’. A set $\boldsymbol { \mathcal { A } } = \{ A _ { 1 } , . . . , A _ { k } \}$ of disjoint subsets of a set $A$ is a partition of $A$ if the union $\cup A$ of all the sets $A _ { i } \in A$ is $A$ and $A _ { i } \neq \emptyset$ for every $i$ . Another partition $\{ A _ { 1 } ^ { \prime } , \ldots , A _ { \ell } ^ { \prime } \}$ of $A$ refines the partition $\mathcal { A }$ if each $A _ { i } ^ { \prime }$ is contained in some $A _ { j }$ . By $[ A ] ^ { k }$ we denote the set of all $k$ -element subsets of $A$ . Sets with $k$ elements will be called $k$ -sets; subsets with $k$ elements are $k$ -subsets.

# 1.1 Graphs

graph

vertex edge

A graph is a pair $G = ( V , E )$ of sets such that $E \subseteq [ V ] ^ { 2 }$ ; thus, the elements of $E$ are 2-element subsets of $V$ . To avoid notational ambiguities, we shall always assume tacitly that $V \cap E = \emptyset$ . The elements of $V$ are the vertices (or nodes, or points) of the graph $G$ , the elements of $E$ are its edges (or lines). The usual way to picture a graph is by drawing a dot for each vertex and joining two of these dots by a line if the corresponding two vertices form an edge. Just how these dots and lines are drawn is considered irrelevant: all that matters is the information of which pairs of vertices form an edge and which do not.

![](images/e6d2d55e2a456c3730a6994bb9e5fbf2d41351ab650b77f45a1dd45a40f81013.jpg)  
Fig. 1.1.1. The graph on $V = \{ 1 , \ldots , 7 \}$ with edge set

$$
E = \{\{1, 2 \}, \{1, 5 \}, \{2, 5 \}, \{3, 4 \}, \{5, 7 \} \}
$$

on $V ( G ) , E ( G )$

A graph with vertex set $V$ is said to be a graph on $V$ . The vertex set of a graph $G$ is referred to as $V ( G )$ , its edge set as $E ( G )$ . These conventions are independent of any actual names of these two sets: the vertex set $W$ of a graph $H = ( W , F )$ is still referred to as $V ( H )$ , not as $W ( H )$ . We shall not always distinguish strictly between a graph and its vertex or edge set. For example, we may speak of a vertex $v \in G$ (rather than $v \in V ( G )$ ), an edge $e \in G$ , and so on.

order $| G |$ , $\| G \|$

The number of vertices of a graph $G$ is its order , written as $| G |$ ; its number of edges is denoted by $\| G \|$ . Graphs are finite, infinite, countable and so on according to their order. Except in Chapter 8, our graphs will be finite unless otherwise stated.

$\varnothing$ trivial graph

For the empty graph $( \varnothing , \varnothing )$ we simply write $\varnothing$ . A graph of order 0 or 1 is called trivial . Sometimes, e.g. to start an induction, trivial graphs can be useful; at other times they form silly counterexamples and become a nuisance. To avoid cluttering the text with non-triviality conditions, we shall mostly treat the trivial graphs, and particularly the empty graph $\varnothing$ , with generous disregard.

incident ends

A vertex $v$ is incident with an edge $e$ if $v \in e$ ; then $e$ is an edge at $\boldsymbol { v }$ . The two vertices incident with an edge are its endvertices or ends, and an edge joins its ends. An edge $\{ x , y \}$ is usually written as xy (or $y x$ ). If $x \in X$ and $y \in Y$ , then $x y$ is an $X$ –Y edge. The set of all $X { - } Y$ edges in a set $E$ is denoted by $E ( X , Y )$ ; instead of $E ( \{ x \} , Y )$ and $E ( X , \{ y \} )$ we simply write $E ( x , Y )$ and $E ( X , y )$ . The set of all the edges in $E$ at a vertex $v$ is denoted by $E ( v )$ .

Two vertices $x , y$ of $G$ are adjacent, or neighbours, if $x y$ is an edge of $G$ . Two edges $e \neq f$ are adjacent if they have an end in common. If all the vertices of $G$ are pairwise adjacent, then $G$ is complete. A complete graph on $n$ vertices is a $K ^ { n }$ ; a $K ^ { 3 }$ is called a triangle.

Pairwise non-adjacent vertices or edges are called independent. More formally, a set of vertices or of edges is independent (or stable) if no two of its elements are adjacent.

Let $G = ( V , E )$ and $G ^ { \prime } = ( V ^ { \prime } , E ^ { \prime } )$ be two graphs. We call $G$ and $G ^ { \prime }$ isomorphic, and write $G \simeq G ^ { \prime }$ , if there exists a bijection $\varphi \colon V \to V ^ { \prime }$ with $x y \in E \Leftrightarrow \varphi ( x ) \varphi ( y ) \in E ^ { \prime }$ for all $x , y \in V$ . Such a map $\varphi$ is called an isomorphism; if $G = G ^ { \prime }$ , it is called an automorphism. We do not normally distinguish between isomorphic graphs. Thus, we usually write $G = G ^ { \prime }$ rather than $G \simeq G ^ { \prime }$ , speak of the complete graph on 17 vertices, and so on.

A class of graphs that is closed under isomorphism is called a graph property. For example, ‘containing a triangle’ is a graph property: if $G$ contains three pairwise adjacent vertices then so does every graph isomorphic to $G$ . A map taking graphs as arguments is called a graph invariant if it assigns equal values to isomorphic graphs. The number of vertices and the number of edges of a graph are two simple graph invariants; the greatest number of pairwise adjacent vertices is another.

![](images/6bd10d6301b55087ca5994d33d3cc2d93c2df0debbbd4ff8e82484a3f732ddd1.jpg)  
Fig. 1.1.2. Union, difference and intersection; the vertices 2,3,4 induce (or span) a triangle in $G \cup G ^ { \prime }$ but not in $G$

We set $G \cup G ^ { \prime } : = ( V \cup V ^ { \prime } , E \cup E ^ { \prime } )$ and $G \cap G ^ { \prime } : = ( V \cap V ^ { \prime } , E \cap E ^ { \prime } )$ . If $G \cap G ^ { \prime } = \emptyset$ , then $G$ and $G ^ { \prime }$ are disjoint . If $V ^ { \prime } \subseteq V$ and $E ^ { \prime } \subseteq E$ , then $G ^ { \prime }$ is a subgraph of $G$ (and $G$ a supergraph of $G ^ { \prime }$ ), written as $G ^ { \prime } \subseteq G$ . Less formally, we say that $G$ contains $G ^ { \prime }$ . If $G ^ { \prime } \subseteq G$ and $G ^ { \prime } \neq G$ , then $G ^ { \prime }$ is a proper subgraph of $G$ .

If $G ^ { \prime } \subseteq G$ and $G ^ { \prime }$ contains all the edges $x y \in E$ with $x , y \in V ^ { \prime }$ , then $G ^ { \prime }$ is an induced subgraph of $G$ ; we say that $V ^ { \prime }$ induces or spans $G ^ { \prime }$ - in $G$ ,

![](images/832d6a5a246463521233e8ecfca4908a7131c19684d2c39c65926a5e9807664d.jpg)

![](images/f8dba6089ddc2835bff7a0170fa5c186dffddf07a6a4e1457a0b93f331702a03.jpg)

![](images/87a2a18cca7eb6b088ee81fe2244873085c3a64a2b5a7670c417a405e226a629.jpg)  
Fig. 1.1.3. A graph $G$ with subgraphs $G ^ { \prime }$ and $G ^ { \prime \prime }$ : $G ^ { \prime }$ is an induced subgraph of $G$ , but $G ^ { \prime \prime }$ is not

$G [ U ]$ and write $G ^ { \prime } = : G \left[ V ^ { \prime } \right]$ . Thus if $U \subseteq V$ is any set of vertices, then $G [ U ]$ denotes the graph on $U$ whose edges are precisely the edges of $G$ with both ends in $U$ . If $H$ is a subgraph of $G$ , not necessarily induced, we spanning abbreviate $G \left[ V ( H ) \right]$ to $G \left[ H \right]$ . Finally, $G ^ { \prime } \subseteq G$ is a spanning subgraph of $G$ if $V ^ { \prime }$ spans all of $G$ , i.e. if $V ^ { \prime } = V$ .

If $U$ is any set of vertices (usually of $G$ ), we write $G - U$ for $G \left[ V \setminus U \right]$ . In other words, $G - U$ is obtained from $G$ by deleting all the vertices in $U \cap V$ and their incident edges. If $U = \{ v \}$ is a singleton, we write $G - v$ rather than $G - \{ v \}$ . Instead of $G - V ( G ^ { \prime } )$ we simply + write $G - G ^ { \prime }$ . For a subset $F$ of $[ V ] ^ { 2 }$ we write $G - F : = ( V , E \setminus F )$ and $G + F : = ( V , E \cup F )$ ; as above, $G - \{ e \}$ and $G + \{ e \}$ are abbreviated to edge-maximal $G - e$ and $G + e$ . We call $G$ edge-maximal with a given graph property if $G$ itself has the property but no graph $G + x y$ does, for non-adjacent vertices $x , y \in G$ .

minimal More generally, when we call a graph minimal or maximal with some maximal property but have not specified any particular ordering, we are referring to the subgraph relation. When we speak of minimal or maximal sets of vertices or edges, the reference is simply to set inclusion.

$G * G ^ { \prime }$ If $G$ and $G ^ { \prime }$ are disjoint, we denote by $G * G ^ { \prime }$ the graph obtained from $G \cup G ^ { \prime }$ by joining all the vertices of $G$ to all the vertices of $G ^ { \prime }$ . For comp $\overline { { G } }$ example, $K ^ { 2 } * K ^ { 3 } = K ^ { 5 }$ . The complement $\overline { { G } }$ of $G$ is the graph on $V$ with edge set $[ V ] ^ { 2 } \setminus E$ . The line graph $L ( G )$ of $G$ is the graph on $E$ in raph which $x , y \in E$ are adjacent as vertices if and only if they are adjacent $L ( G )$ as edges in $G$ .

![](images/dfac77103352a67351fe6f704bd64789dca4920948665a196a557658858833d8.jpg)

![](images/105af203a1e4e7ae5b7756ddbcae016db7aab0afd847ad0bd8fef050b48d1e8f.jpg)  
Fig. 1.1.4. A graph isomorphic to its complement

# 1.2 The degree of a vertex

Let $G = ( V , E )$ be a (non-empty) graph. The set of neighbours of a vertex $v$ in $G$ is denoted by $N _ { G } ( v )$ , or briefly by $N ( v )$ .1 More generally for $U \subseteq V$ , the neighbours in $V \setminus U$ of vertices in $U$ are called neighbours of $U$ ; their set is denoted by $N ( U )$ .

The degree (or valency) $d _ { G } ( v ) = d ( v )$ of a vertex $v$ is the number $| E ( v ) |$ of edges at $\boldsymbol { v }$ ; by our definition of a graph,2 this is equal to the number of neighbours of $\boldsymbol { v }$ . A vertex of degree 0 is isolated. The number $\delta ( G ) : = \operatorname* { m i n } \left\{ d ( v ) \ | \ v \in V \right\}$ is the minimum degree of $G$ , the number $\Delta ( G ) : = \operatorname* { m a x } \left\{ d ( v ) \ | \ v \in V \right\}$ its maximum degree. If all the vertices of $G$ have the same degree $k$ , then $G$ is $k$ -regular , or simply regular . A 3-regular graph is called cubic.

The number

$$
d (G) := \frac {1}{| V |} \sum_ {v \in V} d (v)
$$

is the average degree of $G$ . Clearly,

$$
\delta (G) \leqslant d (G) \leqslant \Delta (G).
$$

The average degree quantifies globally what is measured locally by the vertex degrees: the number of edges of $G$ per vertex. Sometimes it will be convenient to express this ratio directly, as $\varepsilon ( G ) : = | E | / | V |$ .

The quantities $d$ and $\varepsilon$ are, of course, intimately related. Indeed, if we sum up all the vertex degrees in $G$ , we count every edge exactly twice: once from each of its ends. Thus

$$
| E | = \frac {1}{2} \sum_ {v \in V} d (v) = \frac {1}{2} d (G) \cdot | V |,
$$

and therefore

$$
\varepsilon (G) = \frac {1}{2} d (G).
$$

Proposition 1.2.1. The number of vertices of odd degree in a graph is [ 10.3.3 ] always even.

Proof . A graph on $V$ has ${ \begin{array} { r } { { \frac { 1 } { 2 } } \sum _ { v \in V } d ( v ) } \end{array} }$ edges, so $\textstyle \sum d ( v )$ is an even number. 

$N ( v )$

egree $d ( v )$

isolated

$\delta ( G )$

$\Delta ( G )$

regular

cubic

$d ( G )$

average

$\varepsilon ( G )$

If a graph has large minimum degree, i.e. everywhere, locally, many edges per vertex, it also has many edges per vertex globally: $\varepsilon ( G ) =$ ${ \textstyle \frac 1 2 } d ( G ) \geqslant { \textstyle \frac 1 2 } \delta ( G )$ . Conversely, of course, its average degree may be large even when its minimum degree is small. However, the vertices of large degree cannot be scattered completely among vertices of small degree: as the next proposition shows, every graph $G$ has a subgraph whose average degree is no less than the average degree of $G$ , and whose minimum degree is more than half its average degree:

Proposition 1.2.2. Every graph $G$ with at least one edge has a subgraph $H$ with $\delta ( H ) > \varepsilon ( H ) \geqslant \varepsilon ( G )$ .

Proof . To construct $H$ from $G$ , let us try to delete vertices of small degree one by one, until only vertices of large degree remain. Up to which degree $d ( v )$ can we afford to delete a vertex $v$ , without lowering $\succeq$ ? Clearly, up to $d ( v ) = \varepsilon$ : then the number of vertices decreases by 1 and the number of edges by at most $\varepsilon$ , so the overall ratio $\varepsilon$ of edges to vertices will not decrease.

Formally, we construct a sequence $G = G _ { 0 } \supseteq G _ { 1 } \supseteq . . .$ of induced subgraphs of $G$ as follows. If $G _ { i }$ has a vertex $v _ { i }$ of degree $d ( v _ { i } ) \leqslant \varepsilon ( G _ { i } )$ , we let $G _ { i + 1 } \ : = \ G _ { i } - v _ { i }$ ; if not, we terminate our sequence and set $H : = G _ { i }$ . By the choices of $v _ { i }$ we have $\varepsilon ( G _ { i + 1 } ) \geqslant \varepsilon ( G _ { i } )$ for all $i$ , and hence $\varepsilon ( H ) \geqslant \varepsilon ( G )$ .

What else can we say about the graph $H$ ? Since $\varepsilon ( K ^ { 1 } ) = 0 < \varepsilon ( G )$ , none of the graphs in our sequence is trivial, so in particular $H \neq \emptyset$ . The fact that $H$ has no vertex suitable for deletion thus implies $\delta ( H ) > \varepsilon ( H )$ , as claimed. 

# 1.3 Paths and cycles

path

A path is a non-empty graph $P = ( V , E )$ of the form

$$
V = \left\{x _ {0}, x _ {1}, \dots , x _ {k} \right\} \quad E = \left\{x _ {0} x _ {1}, x _ {1} x _ {2}, \dots , x _ {k - 1} x _ {k} \right\},
$$

length $P ^ { k }$

where the $x _ { i }$ are all distinct. The vertices $x _ { 0 }$ and $x _ { k }$ are linked by $P$ and are called its ends; the vertices $x _ { 1 } , \ldots , x _ { k - 1 }$ are the inner vertices of $P$ . The number of edges of a path is its length, and the path of length $k$ is denoted by $P ^ { k }$ . Note that $k$ is allowed to be zero; thus, $P ^ { 0 } = K ^ { 1 }$ .

We often refer to a path by the natural sequence of its vertices,3 writing, say, $P = x _ { 0 } x _ { 1 } \ldots x _ { k }$ and calling $P$ a path from $x _ { 0 }$ to $x _ { k }$ (as well as between $x _ { 0 }$ and $x _ { k }$ ).

![](images/ff7938275aecc3458ad82a86325bcaeb8aa80ffdde343aec19eab05ee0083684.jpg)

![](images/cce9ae0ab9f311474c4f7531cd85509596eb64893b8108e1341027ce030ec180.jpg)  
Fig. 1.3.1. A path $P = P ^ { 6 }$ in $G$

For $0 \leqslant i \leqslant j \leqslant k$ we write

xP y, $\breve { P }$

$$
P x _ {i} := x _ {0} \dots x _ {i}
$$

$$
x _ {i} P := x _ {i} \dots x _ {k}
$$

$$
x _ {i} P x _ {j} := x _ {i} \dots x _ {j}
$$

and

$$
\mathring {P} := x _ {1} \dots x _ {k - 1}
$$

$$
P \mathring {\mathfrak {x}} _ {i} := x _ {0} \dots x _ {i - 1}
$$

$$
\mathring {x} _ {i} P := x _ {i + 1} \dots x _ {k}
$$

$$
\dot {x} _ {i} P \stackrel {\circ} {x} _ {j} := x _ {i + 1} \dots x _ {j - 1}
$$

for the appropriate subpaths of $P$ . We use similar intuitive notation for the concatenation of paths; for example, if the union $P x \cup x Q y \cup y R$ of three paths is again a path, we may simply denote it by $P x Q y R$ .

P xQyR

![](images/c9c525ac1a3d7be8f95995c604b022fd4d79fee48f8ae48d4821678204689fd4.jpg)

![](images/41cec3b8e7a2b7603891ffc7d77b79c40c0edc936a8f6d391c8be87dbadd5ad6.jpg)  
Fig. 1.3.2. Paths $P$ , $Q$ and $x P y Q z$

Given sets $A , B$ of vertices, we call $P = x _ { 0 } \ldots x _ { k }$ an $A$ – $B$ path if $V ( P ) \cap A = \{ x _ { 0 } \}$ and $V ( P ) \cap B = \{ x _ { k } \}$ . As before, we write $a$ – $B$ path rather than $\{ a \}$ –B path, etc. Two or more paths are independent if none of them contains an inner vertex of another. Two $a$ – $\boldsymbol { b }$ paths, for instance, are independent if and only if $a$ and $b$ are their only common vertices.

Given a graph $H$ , we call $P$ an $H$ -path if $P$ is non-trivial and meets $H$ exactly in its ends. In particular, the edge of any $H$ -path of length 1 is never an edge of $H$ .

If $P = x _ { 0 } \ldots x _ { k - 1 }$ is a path and $k \geqslant 3$ , then the graph $C : =$ $P + x _ { k - 1 } x _ { 0 }$ is called a cycle. As with paths, we often denote a cycle by its (cyclic) sequence of vertices; the above cycle $C$ might be written

A–B path

independent

$H$ -path

cycle

as $x _ { 0 } \ldots x _ { k - 1 } x _ { 0 }$ . The length of a cycle is its number of edges (or vertices); the cycle of length $k$ is called a $k$ -cycle and denoted by $C ^ { k }$ .

The minimum length of a cycle (contained) in a graph $G$ is the girth $g ( G )$ of $G$ ; the maximum length of a cycle in $G$ is its circumference. (If $G$ does not contain a cycle, we set the former to $\infty$ , the latter to zero.) An edge which joins two vertices of a cycle but is not itself an edge of the cycle is a chord of that cycle. Thus, an induced cycle in $G$ , a cycle in $G$ forming an induced subgraph, is one that has no chords (Fig. 1.3.3).

![](images/43f6a5b08be92e5c0b6b7fdb24988582f946b360135eba1a5e300a3a42c762c3.jpg)  
Fig. 1.3.3. A cycle $C ^ { 8 }$ with chord $x y$ , and induced cycles $C ^ { 6 } , C ^ { 4 }$

If a graph has large minimum degree, it contains long paths and cycles (see also Exercise 7):

Proposition 1.3.1. Every graph $G$ contains a path of length $\delta ( G )$ and a cycle of length at least $\delta ( G ) + 1$ (provided that $\delta ( G ) \geqslant 2$ ).

Proof . Let $x _ { 0 } \ldots x _ { k }$ be a longest path in $G$ . Then all the neighbours of $x _ { k }$ lie on this path (Fig. 1.3.4). Hence $k \geqslant d ( x _ { k } ) \geqslant \delta ( G )$ . If $i ~ < ~ k$ is minimal with $x _ { i } x _ { k } \in E ( G )$ , then $x _ { i } \ldots x _ { k } x _ { i }$ is a cycle of length at least $\delta ( G ) + 1$ . 

![](images/d930e00ca48b51a44b3206a59ad6ed868865fedb9c8e89267ed3acabcb6a61ff.jpg)  
Fig. 1.3.4. A longest path $x _ { 0 } \ldots x _ { k }$ , and the neighbours of $x _ { k }$

Minimum degree and girth, on the other hand, are not related (unless we fix the number of vertices): as we shall see in Chapter 11, there are graphs combining arbitrarily large minimum degree with arbitrarily large girth.

The distance $d _ { G } ( x , y )$ in $G$ of two vertices $x , y$ is the length of a shortest $x$ – $_ y$ path in $G$ ; if no such path exists, we set $d ( x , y ) : = \infty$ . The greatest distance between any two vertices in $G$ is the diameter of $G$ , denoted by diam $G$ . Diameter and girth are, of course, related:

Proposition 1.3.2. Every graph $G$ containing a cycle satisfies $g ( G ) \leqslant$ $2 \dim G + 1$ .

Proof . Let $C$ be a shortest cycle in $G$ . If $g ( G ) \geqslant 2 \dim G + 2$ , then $C$ has two vertices whose distance in $C$ is at least $\dim G + 1$ . In $G$ , these vertices have a lesser distance; any shortest path $P$ between them is therefore not a subgraph of $C$ . Thus, $P$ contains a $C$ -path $x P y$ . Together with the shorter of the two $x$ –y paths in $C$ , this path $x P y$ forms a shorter cycle than $C$ , a contradiction. 

A vertex is central in $G$ if its greatest distance from any other vertex is as small as possible. This distance is the radius of $G$ , denoted by rad $G$ . Thus, formally, $\begin{array} { r } { \operatorname { r a d } G = \operatorname* { m i n } _ { x \in V ( G ) } \operatorname* { m a x } _ { y \in V ( G ) } d _ { G } ( x , y ) } \end{array}$ . As one easily checks (exercise), we have

$$
\operatorname {r a d} G \leqslant \operatorname {d i a m} G \leqslant 2 \operatorname {r a d} G.
$$

Diameter and radius are not related to minimum, average or maximum degree if we say nothing about the order of the graph. However, graphs of large diameter and minimum degree are clearly large (much larger than forced by each of the two parameters alone; see Exercise 8), and graphs of small diameter and maximum degree must be small:

Proposition 1.3.3. A graph $G$ of radius at most $k$ and maximum degree at most $d \geqslant 3$ has fewer than $\textstyle { \frac { d } { d - 2 } } ( d - 1 ) ^ { k }$ vertices.

Proof . Let $z$ be a central vertex in $G$ , and let $D _ { i }$ denote the set of vertices of $G$ at distance $i$ from $z$ . Then $V ( G ) = \textstyle \bigcup _ { i = 0 } ^ { k } D _ { i }$ . Clearly $| D _ { 0 } | = 1$ and $| D _ { 1 } | \leqslant d$ . For $i \geqslant 1$ we have $| D _ { i + 1 } | \leqslant ( d - 1 ) | D _ { i } |$ , because every vertex in $D _ { i + 1 }$ is a neighbour of a vertex in $D _ { i }$ , and each vertex in $D _ { i }$ has at most $d - 1$ neighbours in $D _ { i + 1 }$ (since it has another neighbour in $D _ { i - 1 }$ ). Thus $| D _ { i + 1 } | \leqslant d ( d - 1 ) ^ { \iota }$ for all $i < k$ by induction, giving

$$
| G | \leqslant 1 + d \sum_ {i = 0} ^ {k - 1} (d - 1) ^ {i} = 1 + \frac {d}{d - 2} \left((d - 1) ^ {k} - 1\right) <   \frac {d}{d - 2} (d - 1) ^ {k}.
$$

Similarly, we can bound the order of $G$ from below by assuming that both its minimum degree and girth are large. For $d \in \mathbb { R }$ and $g \in \mathbb { N }$ let

$$
n _ {0} (d, g) := \left\{ \begin{array}{l l} 1 + d \sum_ {i = 0} ^ {r - 1} (d - 1) ^ {i} & \text {i f g =: 2 r + 1 i s o d d ;} \\ 2 \sum_ {i = 0} ^ {r - 1} (d - 1) ^ {i} & \text {i f g =: 2 r i s e v e n .} \end{array} \right.
$$

It is not difficult to prove that a graph of minimum degree $\delta$ and girth $g$ has at least $n _ { 0 } ( \delta , g )$ vertices (Exercise 6). Interestingly, one can obtain the same bound for its average degree:

Theorem 1.3.4. (Alon, Hoory & Linial 2002)

Let $G$ be a graph. If $d ( G ) \geqslant d \geqslant 2$ and $g ( G ) \geqslant g \in \mathbb { N }$ then $| G | \geqslant n _ { 0 } ( d , g )$ .

One aspect of Theorem 1.3.4 is that it guarantees the existence of a short cycle compared with $| G |$ . Using just the easy minimum degree version of Exercise 6, we get the following rather general bound:

[ 2.3.1 ] Corollary 1.3.5. If $\delta ( G ) \geqslant 3$ then $g ( G ) < 2 \log | G |$ .

Proof . If $g : = g ( G )$ is even then

$$
n _ {0} (3, g) = 2 \frac {2 ^ {g / 2} - 1}{2 - 1} = 2 ^ {g / 2} + (2 ^ {g / 2} - 2) > 2 ^ {g / 2},
$$

while if $g$ is odd then

$$
n _ {0} (3, g) = 1 + 3 \frac {2 ^ {(g - 1) / 2} - 1}{2 - 1} = \frac {3}{\sqrt {2}} 2 ^ {g / 2} - 2 > 2 ^ {g / 2}.
$$

As $| G | \geqslant n _ { 0 } ( 3 , g )$ , the result follows.

walk

A walk (of length $k$ ) in a graph $G$ is a non-empty alternating sequence $v _ { 0 } e _ { 0 } v _ { 1 } e _ { 1 } \ldots e _ { k - 1 } v _ { k }$ of vertices and edges in $G$ such that $e _ { i } ~ =$ $\left\{ \begin{array} { r l } \end{array} \right. \boldsymbol { v } _ { i } , \boldsymbol { v } _ { i + 1 } \left. \right\}$ for all $i < k$ . If $v _ { 0 } = v _ { k }$ , the walk is closed . If the vertices in a walk are all distinct, it defines an obvious path in $G$ . In general, every walk between two vertices contains4 a path between these vertices (proof?).

# 1.4 Connectivity

connected

A non-empty graph $G$ is called connected if any two of its vertices are linked by a path in $G$ . If $U \subseteq V ( G )$ and $G [ U ]$ is connected, we also call $U$ itself connected (in $G$ ). Instead of ‘not connected’ we usually say ‘disconnected’.

[ 1.5.2 ]

Proposition 1.4.1. The vertices of a connected graph $G$ can always be enumerated, say as $v _ { 1 } , \ldots , v _ { n }$ , so that $G _ { i } : = G \left[ v _ { 1 } , \ldots , v _ { i } \right]$ is connected for every $_ i$ .

Proof . Pick any vertex as $v _ { 1 }$ , and assume inductively that $v _ { 1 } , \ldots , v _ { i }$ have been chosen for some $i < | G |$ . Now pick a vertex $v \in G - G _ { i }$ . As $G$ is connected, it contains a path $P$ . Choose as the last vertex $v - v _ { 1 }$ $v _ { i + 1 }$ of $P$ in $G - G _ { i }$ ; then $v _ { i + 1 }$ has a neighbour in $G _ { i }$ . The connectedness of every $G _ { i }$ follows by induction on $i$ . 

Let $G = ( V , E )$ be a graph. A maximal connected subgraph of $G$ is called a component of $G$ . Note that a component, being connected, is always non-empty; the empty graph, therefore, has no components.

component

![](images/e6370256f846a3a686a6fa369b563f41b220725a93177c1f1aba535b66314329.jpg)  
Fig. 1.4.1. A graph with three components, and a minimal spanning connected subgraph in each component

If $A , B \subseteq V$ and $X \subseteq V \cup E$ are such that every $A$ – $B$ path in $G$ contains a vertex or an edge from $X$ , we say that $X$ separates the sets $A$ L and $B$ in $G$ . Note that this implies $A \cap B \subseteq X$ . More generally we say that $X$ separates $G$ if $G - X$ is disconnected, that is, if $X$ separates in $G$ some two vertices that are not in $X$ . A separating set of vertices is a separator. Separating sets of edges have no generic name, but some such sets do; see Section 1.9 for the definition of cuts and bonds. A vertex which separates two other vertices of the same component is a cutvertex , and an edge separating its ends is a bridge. Thus, the bridges in a graph are precisely those edges that do not lie on any cycle.

separate

separator

cutvertex

bridge

![](images/098ad2af2c3cae50b36b4eddedaf4c6227490e3e68043176b5efe96491167f1d.jpg)  
Fig. 1.4.2. A graph with cutvertices $v , x , y , w$ and bridge $e = x y$

The unordered pair $\{ A , B \}$ is a separation of $G$ if $A \cup B = V$ and $G$ has no edge between $A \setminus B$ and $B \setminus A$ . Clearly, the latter is equivalent to saying that $A \cap B$ separates $A$ from $B$ . If both $A \setminus B$ and $B \setminus A$ are non-empty, the separation is proper. The number $| A \cap B |$ is the order of the separation $\{ A , B \}$ .

$G$ is called $k$ -connected (for $k \in \mathbb N$ ) if $| G | > k$ and $G - X$ is connected for every set $X \subseteq V$ with $| X | < k$ . In other words, no two vertices of $G$ are separated by fewer than $k$ other vertices. Every (non-empty) graph is 0-connected, and the 1-connected graphs are precisely the non-trivial connected graphs. The greatest integer $k$ such that $G$ is $k$ -connected is the connectivity $\kappa ( G )$ of $G$ . Thus, $\kappa ( G ) ~ = ~ 0$ if and only if $G$ is disconnected or a $K ^ { 1 }$ , and $\kappa ( K ^ { n } ) = n - 1$ for all $n \geqslant 1$ .

separation

$k$ -connected

connectivity $\kappa ( G )$

-edgeconnected edgeconnectivity $\lambda ( G )$

If $| G | > 1$ and $G - F$ is connected for every set $F \subseteq E$ of fewer than $\ell$ edges, then $G$ is called -edge-connected. The greatest integer $\ell$ such that $G$ is $\ell$ -edge-connected is the edge-connectivity $\lambda ( G )$ of $G$ . In particular, we have $\lambda ( G ) = 0$ if $G$ is disconnected.

![](images/9cc62e22bf7d720d4526f2ad74fd5e3cfd2b7e6bca435592b9eaa62f666c74dd.jpg)

![](images/e6c3f5fcc514609c7970ae909d683a473010c601da38ceed87aa5f7930726c07.jpg)  
Fig. 1.4.3. The octahedron $G$ (left) with $\kappa ( G ) = \lambda ( G ) = 4$ , and a graph $H$ with $\kappa ( H ) = 2$ but $\lambda ( H ) = 4$

# Proposition 1.4.2. If $G$ is non-trivial then $\kappa ( G ) \leqslant \lambda ( G ) \leqslant \delta ( G )$ .

Proof . The second inequality follows from the fact that all the edges incident with a fixed vertex separate $G$ . To prove the first, let $F ^ { \prime }$ be any minimal subset of $E$ such that $G - F$ is disconnected. We show that $\kappa ( G ) \leqslant | F |$ .

Suppose first that $G$ has a vertex $\boldsymbol { v }$ that is not incident with an edge in $F$ . Let $C$ be the component of $G - F$ containing $v$ . Then the vertices of $C$ that are incident with an edge in $F$ separate $v$ from $G - C$ . Since no edge in $F ^ { \prime }$ has both ends in $C$ (by the minimality of $F$ ), there are at most $| F |$ such vertices, giving $\kappa ( G ) \leqslant | F |$ as desired.

Suppose now that every vertex is incident with an edge in $F ^ { \prime }$ . Let $v$ be any vertex, and let $C$ be the component of $G - F$ containing $\boldsymbol { v }$ . Then the neighbours $w$ of $v$ with $v w \not \in F$ lie in $C$ and are incident with distinct edges in $F$ , giving $d _ { G } ( v ) \leqslant | F |$ . As $N _ { G } ( v )$ separates $v$ from all the other vertices in $G$ , this yields $\kappa ( G ) \leqslant | F |$ —unless there are no other vertices, i.e. unless $\{ v \} \cup N ( v ) = V$ . But $\boldsymbol { v }$ was an arbitrary vertex. So we may assume that $G$ is complete, giving $\kappa ( G ) = \lambda ( G ) = | G | - 1$ . 

By Proposition 1.4.2, high connectivity requires a large minimum degree. Conversely, large minimum degree does not ensure high connectivity, not even high edge-connectivity (examples?). It does, however, imply the existence of a highly connected subgraph:

[ 7.2.1 ] [ 11.2.3

# Theorem 1.4.3. (Mader 1972)

Let $0 \neq k \in \mathbb { N }$ . Every graph $G$ with $d ( G ) \geqslant 4 k$ has a $( k + 1 )$ -connected subgraph $H$ such that $\varepsilon ( H ) > \varepsilon ( G ) - k$ .

Proof . Put $\gamma : = \varepsilon ( G )$ ( $\geqslant 2 k$ ), and consider the subgraphs $G ^ { \prime } \subseteq G$ such that

$$
\left| G ^ {\prime} \right| \geqslant 2 k \quad \text {a n d} \quad \left\| G ^ {\prime} \right\| > \gamma \left(\left| G ^ {\prime} \right| - k\right).
$$

Such graphs $G ^ { \prime }$ exist since $G$ is one; let $H$ be one of smallest order.

No graph $G ^ { \prime }$ as in $( * )$ can have order exactly $2 k$ , since this would imply that $\| G ^ { \prime } \| > \gamma k \geqslant 2 k ^ { 2 } > { \binom { | G ^ { \prime } | } { 2 } }$ . The minimality of $H$ therefore implies that $\delta ( H ) > \gamma$ : otherwise we could delete a vertex of degree at most $\gamma$ and obtain a graph $G ^ { \prime } \subsetneq H$ still satisfying ( ). In particular, we have $| H | \geqslant \gamma$ . Dividing the inequality of $\| H \| > \gamma \left| H \right| - \gamma k$ from ( ) by $| H |$ therefore yields $\varepsilon ( H ) > \gamma - k$ , as desired.

It remains to show that $H$ is $( k + 1 )$ -connected. If not, then $H$ has a proper separation $\{ U _ { 1 } , U _ { 2 } \}$ of order at most $k$ ; put $H [ U _ { i } ] = : H _ { i }$ . Since any vertex $v \in U _ { 1 } \setminus U _ { 2 }$ has all its $d ( v ) \geqslant \delta ( H ) > \gamma$ neighbours from $H$ in $H _ { 1 }$ , we have $| H _ { 1 } | \geqslant \gamma \geqslant 2 k$ . Similarly, $| H _ { 2 } | \geqslant 2 k$ . As by the minimality of $H$ neither $H _ { 1 }$ nor $H _ { 2 }$ satisfies ( ), we further have

$$
\left\| H _ {i} \right\| \leqslant \gamma (| H _ {i} | - k)
$$

for $i = 1 , 2$ . But then

$$
\begin{array}{l} \| H \| \leqslant \| H _ {1} \| + \| H _ {2} \| \\ \leqslant \gamma (| H _ {1} | + | H _ {2} | - 2 k) \\ \leqslant \gamma \left(| H | - k\right) \quad \mathrm {(a s} | H _ {1} \cap H _ {2} | \leqslant k), \\ \end{array}
$$

which contradicts ( $^ *$ ) for $H$ .

# 1.5 Trees and forests

An acyclic graph, one not containing any cycles, is called a forest. A connected forest is called a tree. (Thus, a forest is a graph whose components are trees.) The vertices of degree 1 in a tree are its leaves.5 Every nontrivial tree has a leaf—consider, for example, the ends of a longest path. This little fact often comes in handy, especially in induction proofs about trees: if we remove a leaf from a tree, what remains is still a tree.

(1.3.1)

$\gamma$

$H$

$H _ { 1 } , H _ { 2 }$

forest tree leaf

![](images/56c8a9ff4819b61181f7b224bd6dc397a9bffe8842ee90551ef66b2fe7a19c7c.jpg)  
Fig. 1.5.1. A tree

Theorem 1.5.1. The following assertions are equivalent for a graph $T$ :

(i) $T$ is a tree;   
(ii) Any two vertices of $T$ are linked by a unique path in $T$   
(iii) $T$ is minimally connected, i.e. $T$ is connected but $T - e$ is disconnected for every edge $e \in T$ ;   
(iv) $T$ is maximally acyclic, i.e. $T$ contains no cycle but $T + x y$ does, for any two non-adjacent vertices $x , y \in T$ . 

The proof of Theorem 1.5.1 is straightforward, and a good exercise for anyone not yet familiar with all the notions it relates. Extending our notation for paths from Section 1.3, we write $x T y$ for the unique path in a tree $T$ between two vertices $x , y$ (see (ii) above).

A frequently used application of Theorem 1.5.1 is that every connected graph contains a spanning tree: by the equivalence of (i) and (iii), any minimal connected spanning subgraph will be a tree. Figure 1.4.1 shows a spanning tree in each of the three components of the graph depicted.

Corollary 1.5.2. The vertices of a tree can always be enumerated, say as $v _ { 1 } , \ldots , v _ { n }$ , so that every $v _ { i }$ with $i \ \geqslant \ 2$ has a unique neighbour in $\left\{ \begin{array} { r l } \end{array} \right. \boldsymbol { v } _ { 1 } , \ldots , \boldsymbol { v } _ { i - 1 } \left. \right\}$ .

Proof . Use the enumeration from Proposition 1.4.1.

Corollary 1.5.3. A connected graph with $n$ vertices is a tree if and only if it has $n - 1$ edges.

Proof . Induction on $i$ shows that the subgraph spanned by the first $i$ vertices in Corollary 1.5.2 has $i - 1$ edges; for $i \ = \ n$ this proves the forward implication. Conversely, let $G$ be any connected graph with $n$ vertices and $n - 1$ edges. Let $G ^ { \prime }$ be a spanning tree in $G$ . Since $G ^ { \prime }$ has $n - 1$ edges by the first implication, it follows that $G = G ^ { \prime }$ . 

Corollary 1.5.4. If $T$ is a tree and $G$ is any graph with $\delta ( G ) \geqslant | T | - 1$ , then $T \subseteq G$ , i.e. $G$ has a subgraph isomorphic to $T$ .

Proof . Find a copy of $T$ in $G$ inductively along its vertex enumeration from Corollary 1.5.2. 

Sometimes it is convenient to consider one vertex of a tree as special; such a vertex is then called the root of this tree. A tree $T$ with a fixed root $r$ is a rooted tree. Writing $x \leqslant y$ for $x \in r T y$ then defines a partial ordering on $V ( T )$ , the tree-order associated with $T$ and $r$ . We shall think of this ordering as expressing ‘height’: if $x < y$ we say that $x$ lies below in $T$ , we call $_ y$

$$
\lceil y \rceil := \{x \mid x \leqslant y \} \quad \text {a n d} \quad \lfloor x \rfloor := \{y \mid y \geqslant x \}
$$

the down-closure of $y$ and the up-closure of $x$ , and so on. Note that the root $r$ is the least element in this partial order, the leaves of $T$ are its maximal elements, the ends of any edge of $T$ are comparable, and the down-closure of every vertex is a chain, a set of pairwise comparable elements. (Proofs?) The vertices at distance $k$ from $r$ have height $k$ and form the $k$ th level of $T$ .

A rooted tree $T$ contained in a graph $G$ is called normal in $G$ if the ends of every $T$ -path in $G$ are comparable in the tree-order of $T$ . If $T$ spans $G$ , this amounts to requiring that two vertices of $T$ must be comparable whenever they are adjacent in $G$ ; see Figure 1.5.2.

![](images/26874b450f6cefe577d209212b5dfbe3da5655d7bc9708c32d8e2b93722c8bec.jpg)  
Fig. 1.5.2. A normal spanning tree with root $r$

A normal tree $T$ in $G$ can be a powerful tool for examining the structure of $G$ , because $G$ reflects the separation properties of $T$ :

# Lemma 1.5.5. Let $T$ be a normal tree in $G$

(i) Any two vertices $x , y \in T$ are separated in $G$ by the set $\lceil x \rceil \cap \lceil y \rceil$   
(ii) If $S \subseteq V ( T ) = V ( G )$ and $S$ is down-closed, then the components of $G - S$ are spanned by the sets $\lfloor x \rfloor$ with $x$ minimal in $T - S$ .

Proof . (i) Let $P$ be any $x$ – $_ y$ path in $G$ . Since $T$ is normal, the vertices of $P$ in $T$ form a sequence $x = t _ { 1 } , \ldots , t _ { n } = y$ for which $t _ { i }$ and $t _ { i + 1 }$ are always comparable in the tree oder of $T$ . Consider a minimal such sequence of vertices in $P \cap T$ . In this sequence we cannot have $t _ { i - 1 } < t _ { i } > t _ { i + 1 }$ for any $i$ , since $t _ { i - 1 }$ and $t _ { i + 1 }$ would then be comparable and deleting $t _ { i }$ would yield a smaller such sequence. So

$$
x = t _ {1} > \ldots > t _ {k} <   \ldots <   t _ {n} = y
$$

for some $k \in \{ 1 , \ldots , n \}$ . As $t _ { k } \in [ x ] \cap [ y ] \cap V ( P )$ , the result follows.

(ii) Since $S$ is down-closed, the upper neighbours in $T$ of any vertex of $G - S$ are again in $G - S$ (and clearly in the same component), so the components $C$ of $G - S$ are up-closed. As $S$ is down-closed, minimal vertices of $C$ are also minimal in $G - S$ . By (i), this means that $C$ has only one minimal vertex $x$ and equals its up-closure $\lfloor x \rfloor$ . 

Normal spanning trees are also called depth-first search trees, because of the way they arise in computer searches on graphs (Exercise 19). This fact is often used to prove their existence. The following inductive proof, however, is simpler and illuminates nicely how normal trees capture the structure of their host graphs.

[ 6.5.3 ] [ 8.2.4 ]

Proposition 1.5.6. Every connected graph contains a normal spanning tree, with any specified vertex as its root.

Proof . Let $G$ be a connected graph and $r \in G$ any specified vertex. Let $T$ be a maximal normal tree with root $r$ in $G$ ; we show that $V ( T ) = V ( G )$ .

Suppose not, and let $C$ be a component of $G - T$ . As $T$ is normal, $N ( C )$ is a chain in $T$ . Let $x$ be its greatest element, and let $y \in C$ be adjacent to $x$ . Let $T ^ { \prime }$ be the tree obtained from $T$ by joining $y$ to $x$ ; the tree-order of $T ^ { \prime }$ then extends that of $T$ . We shall derive a contradiction by showing that $T ^ { \prime }$ is also normal in $G$ .

Let $P$ be a $T ^ { \prime }$ -path in $G$ . If the ends of $P$ both lie in $T$ , then they are comparable in the tree-order of $T$ (and hence in that of $T ^ { \prime }$ ), because then $P$ is also a $T$ -path and $T$ is normal in $G$ by assumption. If not, then $y$ is one end of $P$ , so $P$ lies in $C$ except for its other end $z$ , which lies in $N ( C )$ . Then $z \leqslant x$ , by the choice of $x$ . For our proof that $y$ and $z$ are comparable it thus suffices to show that $x < y$ , i.e. that $x \in r T ^ { \prime } y$ . This, however, is clear since $y$ is a leaf of $T ^ { \prime }$ with neighbour $x$ . 

# 1.6 Bipartite graphs

Let $r \ \geqslant \ 2$ be an integer. A graph $G = ( V , E )$ is called $r$ -partite if $V$ admits a partition into $r$ classes such that every edge has its ends in different classes: vertices in the same partition class must not be adjacent. Instead of ‘2-partite’ one usually says bipartite.

$r$ -partite

bipartite

![](images/76db5feff04df697ccec6169ce9cfeb3d78df4df8192c226757d02a27fc55e8c.jpg)

![](images/512b7522ac5da4fdf7f72a7fb241e66db8860fc7a6917d4043cf1363463d30bb.jpg)  
Fig. 1.6.1. Two 3-partite graphs

An $r$ -partite graph in which every two vertices from different partition classes are adjacent is called complete; the complete $r$ -partite graphs for all $r$ together are the complete multipartite graphs. The complete $r$ -partite graph $\overline { { K ^ { n _ { 1 } } } } * \ldots * \overline { { K ^ { n _ { r } } } }$ is denoted by $K _ { n _ { 1 } , \ldots , n _ { r } }$ ; if $n _ { 1 } = . . . = n _ { r } = : s$ , we abbreviate this to $K _ { s } ^ { r }$ . Thus, $K _ { s } ^ { \tau }$ is the complete $r$ -partite graph in which every partition class contains exactly $s$ vertices.6 (Figure 1.6.1 shows the example of the octahedron $K _ { 2 } ^ { 3 }$ ; compare its drawing with that in Figure 1.4.3.) Graphs of the form $K _ { 1 , n }$ are called stars; the vertex in the singleton partition class of this $K _ { 1 , n }$ is the star’s centre.

complete $r$ -partite

Kn1,...,nr $K _ { s } ^ { r }$

star centre

![](images/7ca1ae6e27ddac592c1d1c127f8337a9c7a4f04291560c5dbcb1d0ce66c01cbc.jpg)  
Fig. 1.6.2. Three drawings of the bipartite graph $K _ { 3 , 3 } = K _ { 3 } ^ { 2 }$

Clearly, a bipartite graph cannot contain an odd cycle, a cycle of odd length. In fact, the bipartite graphs are characterized by this property:

odd cycle

Proposition 1.6.1. A graph is bipartite if and only if it contains no odd cycle.

[ 5.3.1 ] [ 6.4.2 ]

(1.5.1) Proof . Let $G = ( V , E )$ be a graph without odd cycles; we show that $G$ is bipartite. Clearly a graph is bipartite if all its components are bipartite or trivial, so we may assume that $G$ is connected. Let $T$ be a spanning tree in $G$ , pick a root $r \in T$ , and denote the associated tree-order on $V$ by $\leqslant T$ . For each $v \in V$ , the unique path $r T v$ has odd or even length. This defines a bipartition of $V$ ; we show that $G$ is bipartite with this partition.

![](images/fdcf02bae3d22ae483f689ea18981156f3073f70c07b5f2a1805fe23f07f762b.jpg)  
Fig. 1.6.3. The cycle $C _ { e }$ in $T + e$

Let $e ~ = ~ x y$ be an edge of $G$ . If $\textit { e } \in \textit { T }$ , with $x \ < _ { T } \ y$ say, then $r T y = r T x y$ and so $x$ and $y$ lie in different partition classes. If $e \not \in T$ then $C _ { e } : = x T y + e$ is a cycle (Fig. 1.6.3), and by the case treated already the vertices along $x T y$ alternate between the two classes. Since $C _ { e }$ is even by assumption, $x$ and $y$ again lie in different classes. 

# 1.7 Contraction and minors

In Section 1.1 we saw two fundamental containment relations between graphs: the ‘subgraph’ relation, and the ‘induced subgraph’ relation. In this section we meet two more: the ‘minor’ relation, and the ‘topological minor’ relation.

G/e Let $e = x y$ be an edge of a graph $G = ( V , E )$ . By $G / e$ we denote the contraction graph obtained from $G$ by contracting the edge $e$ into a new vertex $v _ { e }$ , which becomes adjacent to all the former neighbours of $x$ and of $y$ . Formally, $G / e$ is a graph $( V ^ { \prime } , E ^ { \prime } )$ with vertex set $V ^ { \prime } : = ( V \setminus \{ x , y \} ) \cup \{ v _ { e } \}$ $v _ { e }$ (where $v _ { e }$ is the ‘new’ vertex, i.e. $v _ { e } \notin V \cup E )$ and edge set

$$
\begin{array}{l} E ^ {\prime} := \left\{v w \in E \mid \{v, w \} \cap \{x, y \} = \emptyset \right\} \\ \cup \left\{v _ {e} w \mid x w \in E \setminus \{e \} \text {o r} y w \in E \setminus \{e \} \right\}. \\ \end{array}
$$

![](images/2114c3797c2d2a2cffa1d6307fdab34a997eb969a9b82c816411aa77555611ff.jpg)  
Fig. 1.7.1. Contracting the edge $e = x y$

More generally, if $X$ is another graph and $\{ V _ { x } \ | \ x \ \in { V } ( X ) \}$ is a partition of $V$ into connected subsets such that, for any two vertices $x , y \in X$ , there is a $V _ { x } – V _ { y }$ edge in $G$ if and only if $x y \in E ( X )$ , we call $G$ an $M X$ and write7 $G = M X$ (Fig. 1.7.2). The sets $V _ { x }$ are the branch sets of this $M X$ . Intuitively, we obtain $X$ from $G$ by contracting every branch set to a single vertex and deleting any ‘parallel edges’ or ‘loops’ that may arise. In infinite graphs, branch sets are allowed to be infinite. For example, the graph shown in Figure 8.1.1 is an $M X$ with $X$ an infinite star.

![](images/77af6431c807ea386ee38ccfc0a065f9c671b76779972d94958c36c99816bcc5.jpg)  
Fig. 1.7.2. $Y \supseteq G = M X$ , so $X$ is a minor of $Y$

If $V _ { x } = U \subseteq V$ is one of the branch sets above and every other branch set consists just of a single vertex, we also write $G / U$ for the graph $X$ and ${ \boldsymbol { v } } _ { U }$ for the vertex $x \in X$ to which $U$ contracts, and think of the rest of $X$ as an induced subgraph of $G$ . The contraction of a single edge $u u ^ { \prime }$ defined earlier can then be viewed as the special case of $U = \{ u , u ^ { \prime } \}$ .

Proposition 1.7.1. $G$ is an $M X$ if and only if $X$ can be obtained from $G$ by a series of edge contractions, i.e. if and only if there are graphs $G _ { 0 } , \ldots , G _ { n }$ and edges $e _ { i } \in G _ { i }$ such that $G _ { 0 } = G , \ G _ { n } \simeq X$ , and $G _ { i + 1 } = G _ { i } / e _ { i }$ for all $i < n$ .

Proof . Induction on $| G | - | X |$ .

MX branch sets

$G / U$ vU

minor; 

subdivision $x x$ topological minor

If $G = M X$ is a subgraph of another graph $Y$ , we call $X$ a minor of $Y$ and write $X \preccurlyeq Y$ . Note that every subgraph of a graph is also its minor; in particular, every graph is its own minor. By Proposition 1.7.1, any minor of a graph can be obtained from it by first deleting some vertices and edges, and then contracting some further edges. Conversely, any graph obtained from another by repeated deletions and contractions (in any order) is its minor: this is clear for one deletion or contraction, and follows for several from the transitivity of the minor relation (Proposition 1.7.3).

If we replace the edges of $X$ with independent paths between their ends (so that none of these paths has an inner vertex on another path or in $X$ ), we call the graph $G$ obtained a subdivision of $X$ and write $G = T X$ .8 If $G = T X$ is the subgraph of another graph $Y$ , then $X$ is a topological minor of $Y$ (Fig. 1.7.3).

![](images/26e75beb261f0e6b5ee84aba8785c29788c83a391505026d27eadb8a8a619b57.jpg)  
Fig. 1.7.3. $Y \supseteq G = T X$ , so $X$ is a topological minor of $Y$

branch vertices

If $G = T X$ , we view $V ( X )$ as a subset of $V ( G )$ and call these vertices the branch vertices of $G$ ; the other vertices of $G$ are its subdividing vertices. Thus, all subdividing vertices have degree 2, while the branch vertices retain their degree from $X$ .

[ 4.4.2 ] [ 7.3.1 ] [ 12.5.3 ]

# Proposition 1.7.2.

(i) Every $_ { T X }$ is also an $M X$ (Fig. 1.7.4); thus, every topological minor of a graph is also its (ordinary) minor.   
(ii) If $\Delta ( X ) \leqslant 3$ , then every $M X$ contains a $_ { T X }$ ; thus, every minor with maximum degree at most 3 of a graph is also its topological minor. 

[ 12.4.1 ]

Proposition 1.7.3. The minor relation $\preccurlyeq$ and the topological-minor relation are partial orderings on the class of finite graphs, i.e. they are reflexive, antisymmetric and transitive. 

![](images/8008a8696a433016eaa6b241f31c2918ed7138fe76aa74ca1d0de427a1259814.jpg)  
Fig. 1.7.4. A subdivision of $K ^ { 4 }$ viewed as an $M K ^ { 4 }$

Now that we have met all the standard relations between graphs, we can also define what it means to embed one graph in another. Basically, an embedding of $G$ in $H$ is an injective map $\varphi \colon V ( G ) \to V ( H )$ that preserves the kind of structure we are interested in. Thus, $\varphi$ embeds $G$ in $H$ ‘as a subgraph’ if it preserves the adjacency of vertices, and ‘as an induced subgraph’ if it preserves both adjacency and non-adjacency. If $\varphi$ is defined on $E ( G )$ as well as on $V ( G )$ and maps the edges xy of $G$ to independent paths in $H$ between $\varphi ( x )$ and $\varphi ( y )$ , it embeds $G$ in $H$ ‘as a topological minor’. Similarly, an embedding $\varphi$ of $G$ in $H$ ‘as a minor’ would be a map from $V ( G )$ to disjoint connected vertex sets in $H$ (rather than to single vertices) such that $H$ has an edge between the sets $\varphi ( x )$ and $\varphi ( y )$ whenever $x y$ is an edge of $G$ . Further variants are possible; depending on the context, one may wish to define embeddings ‘as a spanning subgraph’, ‘as an induced minor’, and so on in the obivous way.

embedding

# 1.8 Euler tours

Any mathematician who happens to find himself in the East Prussian city of K¨onigsberg (and in the 18th century) will lose no time to follow the great Leonhard Euler’s example and inquire about a round trip through the old city that traverses each of the bridges shown in Figure 1.8.1 exactly once.

Thus inspired,9 let us call a closed walk in a graph an Euler tour if it traverses every edge of the graph exactly once. A graph is Eulerian if it admits an Euler tour.

# Theorem 1.8.1. (Euler 1736)

A connected graph is Eulerian if and only if every vertex has even degree.

Proof . The degree condition is clearly necessary: a vertex appearing $k$ times in an Euler tour (or $k + 1$ times, if it is the starting and finishing vertex and as such counted twice) must have degree $2 k$ .

Eulerian

[ 2.1.5 ]

[ 10.3.3 ]

![](images/3eda0b9f0e5d93c5f38b4f62c9434c51c769466ba27e7d689638ac7a18bf821b.jpg)  
Fig. 1.8.1. The bridges of K¨onigsberg (anno 1736)

Conversely, let $G$ be a connected graph with all degrees even, and let

$$
W = v _ {0} e _ {0} \dots e _ {\ell - 1} v _ {\ell}
$$

be a longest walk in $G$ using no edge more than once. Since $W$ cannot be extended, it already contains all the edges at ${ v } _ { \ell }$ . By assumption, the number of such edges is even. Hence ${ \boldsymbol { v } } _ { \ell } = { \boldsymbol { v } } _ { 0 }$ , so $W$ is a closed walk.

Suppose $W$ is not an Euler tour. Then $G$ has an edge $e$ outside $W$ but incident with a vertex of $W$ , say $e = u v _ { i }$ . (Here we use the connectedness of $G$ , as in the proof of Proposition 1.4.1.) Then the walk

$$
u e v _ {i} e _ {i} \dots e _ {\ell - 1} v _ {\ell} e _ {0} \dots e _ {i - 1} v _ {i}
$$

is longer than $W$ , a contradiction.

![](images/b55963f6e3893536cf249376d89cc1981ef3f2c8f870fb11d632ad9e330f26a8.jpg)  
Fig. 1.8.2. A graph formalizing the bridge problem

# 1.9 Some linear algebra

Let $G \ : = \ : ( V , E )$ be a graph with $n$ vertices and $m$ edges, say $V =$ $\{ v _ { 1 } , \ldots , v _ { n } \}$ and $E = \{ e _ { 1 } , \ldots , e _ { m } \}$ . The vertex space $\mathcal { V } ( G )$ of $G$ is the vector space over the 2-element field $\mathbb { F } _ { 2 } = \{ 0 , 1 \}$ of all functions $V  \mathbb { K } _ { 2 }$ . Every element of $\mathcal { V } ( G )$ corresponds naturally to a subset of $V$ , the set of those vertices to which it assigns a 1, and every subset of $V$ is uniquely represented in $\mathcal { V } ( G )$ by its indicator function. We may thus think of $\mathcal { V } ( G )$ as the power set of $V$ made into a vector space: the sum $U + U ^ { \prime }$ of two vertex sets $U , U ^ { \prime } \subseteq V$ is their symmetric difference (why?), and $U = - U$ for all $U \subseteq V$ . The zero in $\mathcal { V } ( G )$ , viewed in this way, is the empty (vertex) set $\emptyset$ . Since $\{ \{ v _ { 1 } \} , . . . , \{ v _ { n } \} \}$ is a basis of $\mathcal { V } ( G )$ , its standard basis, we have $\dim \mathcal { V } ( G ) = n$ .

In the same way as above, the functions $E  \mathbb { F } _ { 2 }$ form the edge space ${ \mathcal { E } } ( G )$ of $G$ : its elements are the subsets of $E$ , vector addition amounts to symmetric difference, $\varnothing \subseteq E$ is the zero, and $F = - F$ for all $F ^ { \prime } \subseteq E ^ { \prime }$ . As before, $\{ \{ e _ { 1 } \} , . . . , \{ e _ { m } \} \}$ is the standard basis of ${ \mathcal { E } } ( G )$ , and $\dim { \mathcal { E } } ( G ) = m$ .

Since the edges of a graph carry its essential structure, we shall mostly be concerned with the edge space. Given two edge sets $F , F ^ { \prime } \in$ ${ \mathcal { E } } ( G )$ and their coefficients $\lambda _ { 1 } , \ldots , \lambda _ { m }$ and $\lambda _ { 1 } ^ { \prime } , \ldots , \lambda _ { m } ^ { \prime }$ with respect to the standard basis, we write

$$
\langle F, F ^ {\prime} \rangle := \lambda_ {1} \lambda_ {1} ^ {\prime} + \dots + \lambda_ {m} \lambda_ {m} ^ {\prime} \in \mathbb {F} _ {2}.
$$

Note that $\langle F , F ^ { \prime } \rangle ~ = ~ 0$ may hold even when $F = F ^ { \prime } \neq \emptyset$ : indeed, $\langle F , F ^ { \prime } \rangle = 0$ if and only if $F$ and $F ^ { \prime }$ have an even number of edges in common. Given a subspace $\mathcal { F }$ of ${ \mathcal { E } } ( G )$ , we write

$$
\mathcal {F} ^ {\perp} := \left\{D \in \mathcal {E} (G) \mid \langle F, D \rangle = 0 \text {f o r a l l} F \in \mathcal {F} \right\}.
$$

This is again a subspace of ${ \mathcal { E } } ( G )$ (the space of all vectors solving a certain set of linear equations—which?), and we have

$$
\dim \mathcal {F} + \dim \mathcal {F} ^ {\perp} = m. \quad (\dagger)
$$

The cycle space ${ \mathcal { C } } = { \mathcal { C } } ( G )$ is the subspace of ${ \mathcal { E } } ( G )$ spanned by all the cycles in $G$ —more precisely, by their edge sets.10 The dimension of ${ \mathcal { C } } ( G )$ is sometimes called the cyclomatic number of $G$ .

Proposition 1.9.1. The induced cycles in $G$ generate its entire cycle space.

$$
G = (V, E)
$$

vertexspace $\mathcal { V } ( G )$

+

edge space $\mathcal { E } ( G )$

standard basis

space ${ \mathcal { C } } ( G )$

[ 3.2.3 ]

Proof . By definition of ${ \mathcal { C } } ( G )$ it suffices to show that the induced cycles in $G$ generate every cycle $C \subseteq G$ with a chord $e$ . This follows at once by induction on $| C |$ : the two cycles in $C + e$ that have $e$ but no other edge in common are shorter than $C$ , and their symmetric difference is precisely $C$ . 

The elements of $\boldsymbol { \mathscr { C } }$ are easily recognized by the degrees of the subgraphs they form. Moreover, to generate the cycle space from cycles we only need disjoint unions rather than arbitrary symmetric differences:

[ 4.5.1 ] Proposition 1.9.2. The following assertions are equivalent for edge sets $F ^ { \prime } \subseteq E ^ { \prime }$ :

(i) $F \in { \mathcal { C } } ( G )$ ;   
(ii) $F$ is a disjoint union of (edge sets of) cycles in $G$ ;   
(iii) All vertex degrees of the graph $( V , F )$ are even.

Proof . Since cycles have even degrees and taking symmetric differences preserves this, (i) (iii) follows by induction on the number of cycles used to generate $F$ . The implication (iii) (ii) follows by induction on $| F |$ : if $F \neq \emptyset$ then $( V , F )$ contains a cycle $C$ , whose edges we delete for the induction step. The implication (ii) (i) is immediate from the definition of ${ \mathcal { C } } ( G )$ . 

cut

If $\{ V _ { 1 } , V _ { 2 } \}$ is a partition of $V$ , the set $E ( V _ { 1 } , V _ { 2 } )$ of all the edges of $G$ crossing this partition is called a cut (or cocycle). Recall that for $V _ { 1 } = \{ v \}$ this cut is denoted by $E ( v )$ .

[ 4.6.3 ]

Proposition 1.9.3. Together with $\emptyset$ , the cuts in $G$ form a subspace ${ \mathcal { C } } ^ { * }$ of ${ \mathcal { E } } ( G )$ . This space is generated by cuts of the form $E ( v )$ .

Proof . Let $\boldsymbol { \mathcal { C } } ^ { \ast }$ denote the set of all cuts in $G$ , together with $\varnothing$ . To prove that $\boldsymbol { \mathcal { C } } ^ { * }$ is a subspace, we show that for all $D , D ^ { \prime } \in { \mathcal { C } } ^ { * }$ also $D + D ^ { \prime }$ $( = D - D ^ { \prime } )$ lies in $\boldsymbol { \mathcal { C } } ^ { * }$ . Since $D + D = \varnothing \in { \mathcal { C } } ^ { * }$ and $D + \varnothing = D \in { \mathcal { C } } ^ { * }$ , we may assume that $D$ and $D ^ { \prime }$ are distinct and non-empty. Let $\{ V _ { 1 } , V _ { 2 } \}$ and $\left\{ \ V _ { 1 } ^ { \prime } , V _ { 2 } ^ { \prime } \right\}$ be the corresponding partitions of $V$ . Then $D + D ^ { \prime }$ consists of all the edges that cross one of these partitions but not the other (Fig. 1.9.1). But these are precisely the edges between $( V _ { 1 } \cap V _ { 1 } ^ { \prime } ) \cup ( V _ { 2 } \cap V _ { 2 } ^ { \prime } )$ and $( V _ { 1 } \cap V _ { 2 } ^ { \prime } ) \cup ( V _ { 2 } \cap V _ { 1 } ^ { \prime } )$ , and by $D \neq D ^ { \prime }$ these two sets form another partition of $V$ . Hence $D + D ^ { \prime } \in { \mathcal { C } } ^ { * }$ , and $\boldsymbol { \mathcal { C } } ^ { * }$ is indeed a subspace of ${ \mathcal { E } } ( G )$ .

Our second assertion, that the cuts $E ( v )$ generate all of $\boldsymbol { \mathcal { C } } ^ { * }$ , follows from the fact that every edge $x y \in G$ lies in exactly two such cuts (in $E ( x )$ and in $E ( y )$ ); thus every partition $\left\{ \begin{array} { l l }  V _ { 1 } , V _ { 2 } \right\} \end{array}$ of $V$ satisfies $E ( V _ { 1 } , V _ { 2 } ) =$ $\textstyle \sum _ { v \in V _ { 1 } } E ( v )$ . 

![](images/a28f14957e922c5939dbefeba473259fb5672dc5e5d7e167986b59d37b2518df.jpg)  
Fig. 1.9.1. Cut edges in $D + D ^ { \prime }$

The subspace ${ \mathcal { C } } ^ { * } = : { \mathcal { C } } ^ { * } ( G )$ of ${ \mathcal { E } } ( G )$ from Proposition 1.9.3 is the cut space of $G$ . It is not difficult to find among the cuts $E ( v )$ an explicit basis for ${ \mathcal { C } } ^ { * } ( G )$ , and thus to determine its dimension (Exercise 27).

A minimal non-empty cut in $G$ is a bond. Thus, bonds are for ${ \mathcal { C } } ^ { * }$ what cycles are for $\boldsymbol { \mathscr { C } }$ : the minimal non-empty elements. Note that the ‘non-empty’ condition bites only if $G$ is disconnected. If $G$ is connected, its bonds are just its minimal cuts, and these are easy to recognize: clearly, a cut in a connected graph is minimal if and only if both sides of the corresponding vertex partition induce connected subgraphs. If $G$ is disconnected, its bonds are the minimal cuts of its components. (See also Lemma 3.1.1.)

In analogy to Proposition 1.9.2, bonds and disjoint unions suffice to generate $\boldsymbol { \mathcal { C } } ^ { * }$ :

Lemma 1.9.4. Every cut is a disjoint union of bonds. [ 4.6.2 ]

Proof . Consider first a connected graph $H = ( V , E )$ , a connected subgraph $C \subseteq H$ , and a component $D$ of $H - C$ . Then $H - D$ , too, is connected (Fig. 1.9.2), so the edges between $D$ and $H - D$ form a minimal cut. By the choice of $D$ , this cut is precisely the set $E ( C , D )$ of all $C$ – $D$ edges in $H$ .

![](images/28d20bd3171b092f3daff613cd17810d2f30ae810b07c919cfcfb9194c27253c.jpg)  
Fig. 1.9.2. $H - D$ is connected, and $E ( C , D )$ a minimal cut

To prove the lemma, let a cut in an arbitrary graph $G = ( V , E )$ be given, with partition $\{ V _ { 1 } , V _ { 2 } \}$ of $V$ say. Consider a component $C$ of $G \left[ V _ { 1 } \right]$ , and let $H$ be the component of $G$ containing $C$ . Then $E ( C , V _ { 2 } ) = E ( C , H - C )$ is the disjoint union of the edge sets $E ( C , D )$ over all the components $D$ of $H - C$ . By our earlier considerations these sets are minimal cuts in $H$ , and hence bonds in $G$ . Now the disjoint union of all these edge sets $E ( C , V _ { 2 } )$ , taken over all the components $C$ of $G \left[ V _ { 1 } \right]$ , is precisely our cut $E ( V _ { 1 } , V _ { 2 } )$ . 

Theorem 1.9.5. The cycle space $\boldsymbol { \mathscr { C } }$ and the cut space $\boldsymbol { \mathcal { C } } ^ { \ast }$ of any graph satisfy

$$
\mathcal {C} = \mathcal {C} ^ {*} ^ {\perp} \quad a n d \quad \mathcal {C} ^ {*} = \mathcal {C} ^ {\perp}.
$$

Proof . (See also Exercise 30.) Let us consider a graph $G = ( V , E )$ . Clearly, any cycle in $G$ has an even number of edges in each cut. This implies ${ \mathcal { C } } \subseteq { \mathcal { C } } ^ { * \bot }$ .

Conversely, recall from Proposition 1.9.2 that for every edge set $F \notin { \mathcal { C } }$ there exists a vertex $\boldsymbol { v }$ incident with an odd number of edges in $F$ . Then $\langle E ( v ) , F \rangle = 1$ , so $E ( v ) \in { \mathcal { C } } ^ { * }$ implies $F \notin { \mathcal { C } } ^ { * \bot }$ . This completes the proof of ${ \mathcal { C } } = { \mathcal { C } } ^ { * \bot }$ .

To prove $c ^ { * } = { \mathcal { C } } ^ { \perp }$ , it now suffices to show $\mathcal { C } ^ { * } = ( \mathcal { C } ^ { * \bot } ) ^ { \bot }$ . Here ${ \mathcal { C } } ^ { \ast } \subseteq ( { \mathcal { C } } ^ { \ast \bot } ) ^ { \bot }$ follows directly from the definition of $\perp$ . But $\boldsymbol { \mathcal { C } } ^ { \ast }$ has the same dimension as $( { \mathcal { C } } ^ { * \bot } ) ^ { \bot }$ , since $( \dag )$ implies

$$
\dim \mathcal {C} ^ {*} + \dim \mathcal {C} ^ {* \perp} = m = \dim \mathcal {C} ^ {* \perp} + \dim \left(\mathcal {C} ^ {* \perp}\right) ^ {\perp}.
$$

Hence $\mathcal { C } ^ { * } = ( \mathcal { C } ^ { * \bot } ) ^ { \bot }$ as claimed.

fundamental cycles

(1.5.1)

fundamental cuts

Consider a connected graph $G = ( V , E )$ with a spanning tree $T \subseteq G$ . Recall that for every edge $e \ \in \ E \setminus E ( T )$ there is a unique cycle $C _ { e }$ in $T + e$ (Fig. 1.6.3); these cycles $C _ { e }$ are the fundamental cycles of $G$ with respect to $T$ . On the other hand, given an edge $e \in T$ , the graph $T - e$ has exactly two components (Theorem 1.5.1 (iii)), and the set $D _ { e } \subseteq E$ of edges between these two components form a bond in $G$ (Fig.1.9.3). These bonds $D _ { e }$ are the fundamental cuts of $G$ with respect to $T$ .

It is not difficult to show directly that the fundamental cycles and cuts span the cycle and cut space of $G$ , respectively (Ex. 31–32). In the proof of the following more comprehensive theorem, this information comes for free as a consequence of Theorem 1.9.5 and the dimension formula ( ) for orthogonal subspaces.

[ 4.5.1 ]

Theorem 1.9.6. Let $G$ be a connected graph and $T \subseteq G$ a spanning tree. Then the corresponding fundamental cycles and cuts form a basis of ${ \mathcal { C } } ( G )$ and of ${ \mathcal { C } } ^ { * } ( G )$ , respectively. If $G$ has $n$ vertices and $_ { I I }$ edges, then

$$
\dim \mathcal {C} (G) = m - n + 1 \quad \text {a n d} \quad \dim \mathcal {C} ^ {*} (G) = n - 1.
$$

![](images/7d4efd9790a5d21ad46ca0abf7f462adff8add126c0a389d85db162c2efdfbe1.jpg)  
Fig. 1.9.3. The fundamental cut $D _ { e }$

Proof . Since an edge $e \in T$ lies in $D _ { e }$ but not in $D _ { e ^ { \prime } }$ for any $e ^ { \prime } \neq e$ , the cut (1.5.3) $D _ { e }$ cannot be generated by other fundamental cuts. The fundamental cuts therefore form a linearly independent subset of $\boldsymbol { \mathcal { C } } ^ { * }$ , of size $n - 1$ (Corollary 1.5.3). Similarly, an edge $e \in E \setminus E ( T )$ lies on $C _ { e }$ but not on any other fundamental cycle; so the fundamental cycles form a linearly independent subset of $\boldsymbol { \mathscr { C } }$ , of size $m - n + 1$ . Thus,

$$
\dim \mathcal {C} ^ {*} \geqslant n - 1 \quad \text {a n d} \quad \dim \mathcal {C} \geqslant m - n + 1.
$$

But

$$
\dim \mathcal {C} ^ {*} + \dim \mathcal {C} = m = (n - 1) + (m - n + 1)
$$

by Theorem 1.9.5 and (†), so the two inequalities above can hold only with equality. Hence the sets of fundamental cuts and cycles are maximal as linearly independent subsets of $\boldsymbol { \mathcal { C } } ^ { * }$ and $\boldsymbol { \mathscr { C } }$ , and hence are bases. 

The incidence matrix $B = ( b _ { i j } ) _ { n \times m }$ of a graph $G = ( V , E )$ with $V = \{ v _ { 1 } , \ldots , v _ { n } \}$ and $E = \{ e _ { 1 } , \ldots , e _ { m } \}$ is defined over $\mathbb { F } _ { 2 }$ by

$$
b _ {i j} := \left\{ \begin{array}{l l} 1 & \text {i f} v _ {i} \in e _ {j} \\ 0 & \text {o t h e r w i s e .} \end{array} \right.
$$

As usual, let $B ^ { t }$ denote the transpose of $B$ . Then $B$ and $B ^ { t }$ define linear maps $B { : } { \mathcal { E } } ( G ) \to { \mathcal { V } } ( G )$ and $B ^ { t } \colon \mathcal { V } ( G ) \to \mathcal { E } ( G )$ with respect to the standard bases.

# Proposition 1.9.7.

(i) The kernel of $B$ is ${ \mathcal { C } } ( G )$ .   
(ii) The image of $B ^ { t }$ i s ${ \mathcal { C } } ^ { * } ( G )$ .

incidence matrix

adjacency matrix

The adjacency matrix $A = ( a _ { i j } ) _ { n \times n }$ of $G$ is defined by

$$
a _ {i j} := \left\{ \begin{array}{l l} 1 & \text {i f} v _ {i} v _ {j} \in E \\ 0 & \text {o t h e r w i s e .} \end{array} \right.
$$

Our last proposition establishes a simple connection between $A$ and $B$ 2 (now viewed as real matrices). Let $D$ denote the real diagonal matrix $( d _ { i j } ) _ { n \times n }$ with $d _ { i i } = d ( v _ { i } )$ and $d _ { i j } = 0$ otherwise.

Proposition 1.9.8. $B B ^ { t } = A + D$ .

![](images/52d09b8399e7d0adab58f154cad7312a0db1fefd70d0b0889c640933e77217a1.jpg)

# 1.10 Other notions of graphs

For completeness, we now mention a few other notions of graphs which feature less frequently or not at all in this book.

hypergraph

A hypergraph is a pair $( V , E )$ of disjoint sets, where the elements of $E$ are non-empty subsets (of any cardinality) of $V$ . Thus, graphs are special hypergraphs.

directed graph

init(e) ter(e)

A directed graph (or digraph) is a pair $( V , E )$ of disjoint sets (of vertices and edges) together with two maps init: $E \to V$ and ter: $E \to V$ assigning to every edge $e$ an initial vertex $\operatorname { i n i t } ( e )$ and a terminal vertex $\mathrm { t e r } ( e )$ . The edge $e$ is said to be directed from $\operatorname { i n i t } ( e )$ to $\mathrm { t e r } ( e )$ . Note that a directed graph may have several edges between the same two vertices $x , y$ . Such edges are called multiple edges; if they have the same direction (say from $x$ to $y$ ), they are parallel . If $\operatorname { i n i t } ( e ) = \operatorname { t e r } ( e )$ , the edge $e$ is called a loop.

loop orientation oriented graph

A directed graph $D$ is an orientation of an (undirected) graph $G$ if $V ( D ) = V ( G )$ and $E ( D ) = E ( G )$ , and if $\{ \operatorname { i n i t } ( e ) , \operatorname { t e r } ( e ) \} = \{ x , y \}$ for every edge $e = x y$ . Intuitively, such an oriented graph arises from an undirected graph simply by directing every edge from one of its ends to the other. Put differently, oriented graphs are directed graphs without loops or multiple edges.

multigraph

A multigraph is a pair $( V , E )$ of disjoint sets (of vertices and edges) together with a map $E \to V \cup [ V ] ^ { 2 }$ assigning to every edge either one or two vertices, its ends. Thus, multigraphs too can have loops and multiple edges: we may think of a multigraph as a directed graph whose edge directions have been ‘forgotten’. To express that $x$ and $y$ are the ends of an edge $e$ we still write $e = x y$ , though this no longer determines $e$ uniquely.

A graph is thus essentially the same as a multigraph without loops or multiple edges. Somewhat surprisingly, proving a graph theorem more generally for multigraphs may, on occasion, simplify the proof. Moreover, there are areas in graph theory (such as plane duality; see Chapters 4.6 and 6.5) where multigraphs arise more naturally than graphs, and where

any restriction to the latter would seem artificial and be technically complicated. We shall therefore consider multigraphs in these cases, but without much technical ado: terminology introduced earlier for graphs will be used correspondingly.

A few differences, however, should be pointed out. A multigraph may have cycles of length 1 or 2: loops, and pairs of multiple edges (or double edges). A loop at a vertex makes it its own neighbour, and contributes 2 to its degree; in Figure 1.10.1, we thus have $d ( v _ { e } ) = 6$ . And the notion of edge contraction is simpler in multigraphs than in graphs. If we contract an edge $e = x y$ in a multigraph $G = ( V , E )$ to a new vertex $v _ { e }$ , there is no longer a need to delete any edges other than $e$ itself: edges parallel to $e$ become loops at $v _ { e }$ , while edges $x v$ and yv become parallel edges between $v _ { e }$ and $\boldsymbol { v }$ (Fig. 1.10.1). Thus, formally, $E ( G / e ) = E \setminus \{ e \}$ , and only the incidence map $e ^ { \prime } \mapsto \{ \operatorname { i n i t } ( e ^ { \prime } ) , \operatorname { t e r } ( e ^ { \prime } ) \}$ of $G$ has to be adjusted to the new vertex set in $G / e$ . The notion of a minor adapts to multigraphs accordingly.

![](images/c24af7a6d26f847df0fe3dba415428b1e6e1ad25f781732d319e86815ccd351b.jpg)  
Fig. 1.10.1. Contracting the edge $e$ in the multigraph corresponding to Fig. 1.8.1

If $\boldsymbol { v }$ is a vertex of degree 2 in a multigraph $G$ , then by suppressing $v$ we mean deleting $v$ and adding an edge between its two neighbours.11 (If its two incident edges are identical, i.e. form a loop at $\boldsymbol { v }$ , we add no edge and obtain just $G - v$ . If they go to the same vertex $w \ne v$ , the added edge will be a loop at $w$ . See Figure 1.10.2.) Since the degrees of all vertices other than $v$ remain unchanged when $\boldsymbol { v }$ is suppressed, suppressing several vertices of $G$ always yields a well-defined multigraph that is independent of the order in which those vertices are suppressed.

![](images/9da03d6608c98985dcd6904e46e45a6b50c6a383083dabe2479eb6a1bc702029.jpg)  
Fig. 1.10.2. Suppressing the white vertices

suppressing a vertex

Finally, it should be pointed out that authors who usually work with multigraphs tend to call them ‘graphs’; in their terminology, our graphs would be called ‘simple graphs’.

# Exercises

1. $-$ What is the number of edges in a $K ^ { n }$ ?   
2. Let $d \in \mathbb { N }$ and $V : = \{ 0 , 1 \} ^ { d }$ ; thus, $V$ is the set of all 0–1 sequences of length $d$ . The graph on $V$ in which two such sequences form an edge if and only if they differ in exactly one position is called the $d$ -dimensional cube. Determine the average degree, number of edges, diameter, girth and circumference of this graph.

(Hint for the circumference: induction on $d$ .)

3. Let $G$ be a graph containing a cycle $C$ , and assume that $G$ contains a path of length at least $k$ between two vertices of $C$ . Show that $G$ contains a cycle of length at least $\sqrt { k }$ . Is this best possible?   
4.− Is the bound in Proposition 1.3.2 best possible?   
5. Show that rad $G \leqslant \mathrm { d i a m } G \leqslant 2 \mathrm { r a d } G$ for every graph $G$   
6. Prove the weakening of Theorem 1.3.4 obtained by replacing average with minimum degree. Deduce that $| G | \geqslant n _ { 0 } ( d / 2 , g )$ for every graph $G$ as given in the theorem.   
7.+ Show that every connected graph $G$ contains a path of length at least $\operatorname* { m i n } \left\{ 2 \delta ( G ) , | G | - 1 \right\}$ .   
8.+ Find a good lower bound for the order of a connected graph in terms of its diameter and minimum degree.   
9.− Show that the components of a graph partition its vertex set. (In other words, show that every vertex belongs to exactly one component.)   
10. $-$ Show that every 2-connected graph contains a cycle.   
11. Determine $\kappa ( G )$ and $\lambda ( G )$ for $G \ = \ P ^ { m } , C ^ { n } , K ^ { n } , K _ { m , n }$ and the $d$ - dimensional cube (Exercise 2); $d , m , n \geqslant 3$ .   
12.− Is there a function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ such that, for all $k \in \mathbb N$ , every graph of minimum degree at least $f ( k )$ is $k$ -connected?   
13.+ Let $\alpha , \beta$ be two graph invariants with positive integer values. Formalize the two statements below, and show that each implies the other:

(i) $\alpha$ is bounded above by a function of $\beta$ ;   
(ii) $\beta$ can be forced up by making $\alpha$ large enough.

Show that the statement

(iii) $\beta$ is bounded below by a function of $\alpha$

is not equivalent to (i) and (ii). Which small change will make it so?

14.+ What is the deeper reason behind the fact that the proof of Theorem 1.4.3 is based on an assumption of the form $m \geqslant c _ { k } n - b _ { k }$ rather than more simply $m \geqslant c _ { k } n$ ?   
15. Prove Theorem 1.5.1.   
16. $-$ Show that every tree $T$ has at least $\Delta ( T )$ leaves.   
17. Show that a tree without a vertex of degree 2 has more leaves than other vertices. Can you find a very short proof that does not use induction?   
18. Show that the tree-order associated with a rooted tree $T$ is indeed a partial order on $V ( T )$ , and verify the claims made about this partial order in the text.   
19.+ Let $G$ be a connected graph, and let $r \in G$ be a vertex. Starting from $r$ , move along the edges of $G$ , going whenever possible to a vertex not visited so far. If there is no such vertex, go back along the edge by which the current vertex was first reached (unless the current vertex is $r$ ; then stop). Show that the edges traversed form a normal spanning tree in $G$ with root $r$ .

(This procedure has earned those trees the name of depth-first search trees.)

20. Let $\boldsymbol { \mathcal { T } }$ be a set of subtrees of a tree $T$ . Assume that the trees in $\boldsymbol { \tau }$ have pairwise non-empty intersection. Show that their overall intersection $\boldsymbol { \lceil \rceil \tau }$ is non-empty.   
21. Show that every automorphism of a tree fixes a vertex or an edge.   
22.− Are the partition classes of a regular bipartite graph always of the same size?   
23. Show that a graph is bipartite if and only if every induced cycle has even length.   
24.+ Find a function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ such that, for all $k \in \mathbb N$ , every graph of average degree at least $f ( k )$ has a bipartite subgraph of minimum degree at least $k$ .   
25. Show that the minor relation $\preccurlyeq$ defines a partial ordering on any set of (finite) graphs. Is the same true for infinite graphs?   
26. Prove or disprove that every connected graph contains a walk that traverses each of its edges exactly once in each direction.   
27. Given a graph $G$ , find among all cuts of the form $E ( v )$ a basis for the cut space of $G$ .   
28. Show that the bonds of a graph are precisely the minimal cuts of its components.   
29. Prove that the cycles and the cuts in a graph together generate its entire edge space, or find a counterexample.   
30.+ In the proof of Theorem 1.9.5, the only implication that is not proved directly (but via dimension) is ${ \mathcal { C } } ^ { \perp } \subseteq { \mathcal { C } } ^ { * }$ . Prove this implication directly.

31. Give a direct proof of the fact that the fundamental cycles of a connected graph span its cycle space.   
32. Give a direct proof of the fact that the fundamental cuts of a connected graph span its cut space.   
33. What are the dimensions of the cycle and the cut space of a graph with $k$ components?   
34. Let $A = ( a _ { i j } ) _ { n \times n }$ be the adjacency matrix of the graph $G$ . Show that the matrix $A ^ { k } = ( a _ { i j } ^ { \prime } ) _ { n \times n }$ displays, for all $i , j \leqslant n$ , the number $a _ { i j } ^ { \prime }$ of walks of length $k$ from $_ { v _ { i } }$ to $v _ { j }$ in $G$ .   
35.+ Prove Gallai’s cycle-cocycle partition theorem that the vertex set of any graph $G = ( V , E )$ can be written as a disjoint union $V = V ^ { \prime } \cup V ^ { \prime \prime }$ of possibly empty subsets such that the edge sets of both $G \left[ V ^ { \prime } \right]$ and $G \left[ V ^ { \prime \prime } \right]$ lie in the cycle space of $G$ .

# Notes

The terminology used in this book is mostly standard. Alternatives do exist, of course, and some of these are stated when a concept is first defined. There is one small point where our notation deviates slightly from standard usage. Whereas complete graphs, paths, cycles etc. of given order are mostly denoted by $K _ { n }$ , $P _ { k }$ , $C _ { \ell }$ and so on, we use superscripts instead of subscripts. This has the advantage of leaving the variables $K$ , $P$ , $C$ etc. free for ad-hoc use: we may now enumerate components as $C _ { 1 } , C _ { 2 } , \ldots$ , speak of paths $P _ { 1 } , \ldots , P _ { k }$ , and so on—without any danger of confusion.

Theorem $^ { 1 2 }$ 1.3.4 was proved by N. Alon, S. Hoory and N. Linial, The Moore bound for irregular graphs, Graphs Comb. 18 (2002), 53–57. The proof uses an ingenious argument counting random walks along the edges of the graph considered.

The main assertion of Theorem 1.4.3, that an average degree of at least $4 k$ forces a $k$ -connected subgraph, is from W. Mader, Existenz $_ n$ -fach zusammenh¨angender Teilgraphen in Graphen gen¨ugend großer Kantendichte, Abh. Math. Sem. Univ. Hamburg 37 (1972) 86–97. The stronger form stated here was obtained in 2005 by Ph. Spr¨ussel with a different proof (unpublished); our proof is due to Mader. For the history of the K¨onigsberg bridge problem, and Euler’s actual part in its solution, see N.L. Biggs, E.K. Lloyd & R.J. Wilson, Graph Theory 1736–1936, Oxford University Press 1976.

Of the large subject of algebraic methods in graph theory, Section 1.9 does not convey an adequate impression. A good introduction is N.L. Biggs, Algebraic Graph Theory (2nd edn.), Cambridge University Press 1993. A more comprehensive account is given by C.D. Godsil & G.F. Royle, Algebraic Graph Theory, Springer GTM 207, 2001. Surveys on the use of algebraic methods can also be found in the Handbook of Combinatorics (R.L. Graham, M. Gr¨otschel & L. Lov´asz, eds.), North-Holland 1995.

# Matching Covering and Packing

Suppose we are given a graph and are asked to find in it as many independent edges as possible. How should we go about this? Will we be able to pair up all its vertices in this way? If not, how can we be sure that this is indeed impossible? Somewhat surprisingly, this basic problem does not only lie at the heart of numerous applications, it also gives rise to some rather interesting graph theory.

A set $M$ of independent edges in a graph $G = ( V , E )$ is called a matching. $M$ is a matching of $U \subseteq V$ if every vertex in $U$ is incident with an edge in $M$ . The vertices in $U$ are then called matched (by $M$ ); vertices not incident with any edge of $M$ are unmatched.

A $k$ -regular spanning subgraph is called a $k$ -factor . Thus, a subgraph $H \subseteq G$ is a 1-factor of $G$ if and only if $E ( H )$ is a matching of $V$ . The problem of how to characterize the graphs that have a 1-factor, i.e. a matching of their entire vertex set, will be our main theme in the first two sections of this chapter.

A generalization of the matching problem is to find in a given graph $G$ as many disjoint subgraphs as possible that are each isomorphic to an element of a given class $\mathcal { H }$ of graphs. This is known as the packing problem. It is related to the covering problem, which asks how few vertices of $G$ suffice to meet all its subgraphs isomorphic to a graph in $\mathcal { H }$ : clearly, we need at least as many vertices for such a cover as the maximum number $k$ of $\mathcal { H }$ -graphs that we can pack disjointly into $G$ . If there is no cover by just $k$ vertices, perhaps there is always a cover by at most $f ( k )$ vertices, where $f ( k )$ may depend on $\mathcal { H }$ but not on $G$ ? In

Section 2.3 we shall prove that when $\mathcal { H }$ is the class of cycles, then there is such a function $f$ .

In Section 2.4 we consider packing and covering in terms of edges: we ask how many edge-disjoint spanning trees we can find in a given graph, and how few trees in it will cover all its edges. In Section 2.5 we prove a path cover theorem for directed graphs, which implies the well-known duality theorem of Dilworth for partial orders.

# 2.1 Matching in bipartite graphs

$$
\begin{array}{l} G = (V, E) \\ A, B \\ a, b \text {e t c .} \\ \end{array}
$$

alternating path

augmenting path

For this whole section, we let $G = ( V , E )$ be a fixed bipartite graph with bipartition $\{ A , B \}$ . Vertices denoted as $a , a ^ { \prime }$ etc. will be assumed to lie in $A$ , vertices denoted as $b$ etc. will lie in $B$ .

How can we find a matching in $G$ with as many edges as possible? Let us start by considering an arbitrary matching $M$ in $G$ . A path in $G$ which starts in $A$ at an unmatched vertex and then contains, alternately, edges from $E \setminus M$ and from $M$ , is an alternating path with respect to $M$ . An alternating path $P$ that ends in an unmatched vertex of $B$ is called an augmenting path (Fig. 2.1.1), because we can use it to turn $M$ into a larger matching: the symmetric difference of $M$ with $E ( P )$ is again a matching (consider the edges at a given vertex), and the set of matched vertices is increased by two, the ends of $P$ .

![](images/d0a2e9cdedf0067bb5392e50df35d40c0c51d782001de69547b6479df00760e4.jpg)  
Fig. 2.1.1. Augmenting the matching $M$ by the alternating path $P$

Alternating paths play an important role in the practical search for large matchings. In fact, if we start with any matching and keep applying augmenting paths until no further such improvement is possible, the matching obtained will always be an optimal one, a matching with the largest possible number of edges (Exercise 1). The algorithmic problem of finding such matchings thus reduces to that of finding augmenting paths—which is an interesting and accessible algorithmic problem.

Our first theorem characterizes the maximal cardinality of a matching in $G$ by a kind of duality condition. Let us call a set $U \subseteq V$ a (vertex) cover of $E$ if every edge of $G$ is incident with a vertex in $U$ .

# Theorem 2.1.1. (K¨onig 1931)

The maximum cardinality of a matching in $G$ is equal to the minimum cardinality of a vertex cover of its edges.

Proof . Let $M$ be a matching in $G$ of maximum cardinality. From every edge in $M$ let us choose one of its ends: its end in $B$ if some alternating path ends in that vertex, and its end in $A$ otherwise (Fig. 2.1.2). We shall prove that the set $U$ of these $| M |$ vertices covers $E$ ; since any vertex cover of $E$ must cover $M$ , there can be none with fewer than $\lvert M \rvert$ vertices, and so the theorem will follow.

![](images/ee7331658ebc77ba83829469f87a0f197caa859c1e4b757f7892f709587a2349.jpg)  
Fig. 2.1.2. The vertex cover $U$

Let $a b \in E$ be an edge; we show that either $a$ or $b$ lies in $U$ . If $a b \in M$ , this holds by definition of $U$ , so we assume that $a b \notin M$ . Since $M$ is a maximal matching, it contains an edge $a ^ { \prime } b ^ { \prime }$ with $a = a ^ { \prime }$ or $b = b ^ { \prime }$ . In fact, we may assume that $a = a ^ { \prime }$ : for if $a$ is unmatched (and $b = b ^ { \prime }$ ), then $a b$ is an alternating path, and so the end of $a ^ { \prime } b ^ { \prime } \in M$ chosen for $U$ was the vertex $\ b ^ { \prime } = \ b$ . Now if $\boldsymbol { a } ^ { \prime } = \boldsymbol { a }$ is not in $U$ , then $b ^ { \prime } \in U$ , and some alternating path $P$ ends in $b ^ { \prime }$ . But then there is also an alternating path $P ^ { \prime }$ ending in $b$ : either $P ^ { \prime } : = P b$ (if $b \in \mathcal Ḋ P Ḍ$ ) or $P ^ { \prime } : = P b ^ { \prime } a ^ { \prime } b$ . By the maximality of $M$ , however, $P ^ { \prime }$ is not an augmenting path. So $b$ must be matched, and was chosen for $U$ from the edge of $M$ containing it. 

Let us return to our main problem, the search for some necessary and sufficient conditions for the existence of a 1-factor. In our present case of a bipartite graph, we may as well ask more generally when $G$ contains a matching of $A$ ; this will define a 1-factor of $G$ if $| A | = | B |$ , a condition that has to hold anyhow if $G$ is to have a 1-factor.

A condition clearly necessary for the existence of a matching of $A$ L is that every subset of $A$ has enough neighbours in $B$ , i.e. that

$$
| N (S) | \geqslant | S | \quad \text {f o r a l l} S \subseteq A.
$$

The following marriage theorem says that this obvious necessary condition is in fact sufficient:

marriage condition

[ 2.2.3 ] Theorem 2.1.2. (Hall 1935)

$G$ contains a matching of A if and only if $\vert N ( S ) \vert \geqslant \vert S \vert$ for all $S \subseteq A$

We give three proofs, ranging from the natural and pedestrian to the slick and elegant. The theorem can also be derived easily from K¨onig’s theorem (Exercise 4).

Our first proof is algorithmic and uses alternating paths.

M First proof. Consider a matching $M$ of $G$ that leaves a vertex of $A$ L unmatched; we shall construct an augmenting path with respect to $M$ .

Let $a _ { 0 } , b _ { 1 } , a _ { 1 } , b _ { 2 } , a _ { 2 } , \dots$ be a maximal sequence of distinct vertices $a _ { i } \in A$ and $b _ { i } ~ \in ~ B$ satisfying the following conditions for all $i ~ \geqslant ~ 1$ (Fig. 2.1.3):

(i) $a _ { 0 }$ is unmatched;   
f (i) (ii) $b _ { i }$ is adjacent to some vertex $a _ { f ( i ) } \in \left\{ a _ { 0 } , \ldots , a _ { i - 1 } \right\}$ ;   
(iii) $a _ { i } b _ { i } \in M$ .

![](images/916a24f8c3b6690ac63eb96ab52fd473d49dd4da4216dd5809df5775fd197f7c.jpg)  
Fig. 2.1.3. Proving the marriage theorem by alternating paths

By the marriage condition, our sequence cannot end in a vertex of $A$ : the $i$ vertices $a _ { 0 } , \ldots , a _ { i - 1 }$ together have at least $i$ neighbours in $B$ , so we can always find a new vertex $b _ { i } \neq b _ { 1 } , \ldots , b _ { i - 1 }$ that satisfies (ii). Let $b _ { k } \in { \mathcal { B } }$ be the last vertex of the sequence. By (i)–(iii),

$$
P := b _ {k} a _ {f (k)} b _ {f (k)} a _ {f ^ {2} (k)} b _ {f ^ {2} (k)} a _ {f ^ {3} (k)} \dots a _ {f ^ {r} (k)}
$$

with $f ^ { r } ( k ) = 0$ is an alternating path.

What is it that prevents us from extending our sequence further? If $b _ { k }$ is matched, say to $a$ , we can indeed extend it by setting $a _ { k } : = a$ , unless $a = a _ { i }$ with $0 < i < k$ , in which case (iii) would imply $b _ { k } = b _ { i }$ with a contradiction. So $b _ { k }$ is unmatched, and hence $P$ is an augmenting path between $a _ { 0 }$ and $b _ { k }$ . 

Second proof. We apply induction on $| A |$ . For $| { \cal A } | = 1$ the assertion is true. Now let $| A | \geqslant 2$ , and assume that the marriage condition is sufficient for the existence of a matching of $A$ when $| A |$ is smaller.

If $| N ( S ) | \geqslant | S | + 1$ for every non-empty set $S \subsetneq A$ , we pick an edge $a b \in G$ and consider the graph ${ \cal G } ^ { \prime } : = { \cal G } - \{ a , b \}$ . Then every non-empty set $S \subseteq A \setminus \{ a \}$ satisfies

$$
\left| N _ {G ^ {\prime}} (S) \right| \geqslant \left| N _ {G} (S) \right| - 1 \geqslant | S |,
$$

so by the induction hypothesis $G ^ { \prime }$ contains a matching of $A \setminus \{ a \}$ . Together with the edge $a b$ , this yields a matching of $A$ in $G$ .

Suppose now that $A$ has a non-empty proper subset $A ^ { \prime }$ with $| B ^ { \prime } | =$ $| A ^ { \prime } |$ for $B ^ { \prime } : = N ( A ^ { \prime } )$ . By the induction hypothesis, $G ^ { \prime } : = G \left[ A ^ { \prime } \cup B ^ { \prime } \right]$ contains a matching of $A ^ { \prime }$ . But $G - G ^ { \prime }$ satisfies the marriage condition too: for any set $S \subseteq A \setminus A ^ { \prime }$ with $| N _ { G - G ^ { \prime } } ( S ) | < | S |$ we would have $| N _ { G } ( S \cup A ^ { \prime } ) | < | S \cup A ^ { \prime } |$ , contrary to our assumption. Again by induction, $G - G ^ { \prime }$ contains a matching of $A \setminus A ^ { \prime }$ . Putting the two matchings together, we obtain a matching of $A$ in $G$ . 

For our last proof, let $H$ be a spanning subgraph of $G$ that satisfies the marriage condition and is edge-minimal with this property. Note that $d _ { H } ( a ) \geqslant 1$ for every $a \in A$ , by the marriage condition with $S = \{ a \}$ .

Third proof. We show that $d _ { H } ( a ) = 1$ for every $a \in A$ . The edges of $H$ then form a matching of $A$ , since by the marriage condition no two such edges can share a vertex in $B$ .

Suppose $a$ has distinct neighbours $b _ { 1 } , b _ { 2 }$ in $H$ . By definition of $H$ , the graphs $H - a b _ { 1 }$ and $H - a b _ { 2 }$ violate the marriage condition. So for $i = 1 , 2$ there is a set $A _ { i } \subseteq A$ containing $a$ such that $\left| { A _ { i } } \right| > \left| { B _ { i } } \right|$ for $B _ { i } : = N _ { H - a b _ { i } } ( A _ { i } )$ . Since $b _ { 1 } \in B _ { 2 }$ and $b _ { 2 } \in B _ { 1 }$ , we obtain

$$
\begin{array}{l} \left| N _ {H} \left(A _ {1} \cap A _ {2} \setminus \{a \}\right) \right| \leqslant \left| B _ {1} \cap B _ {2} \right| \\ = \left| B _ {1} \right| + \left| B _ {2} \right| - \left| B _ {1} \cup B _ {2} \right. \\ = \left| B _ {1} \right| + \left| B _ {2} \right| - \left| N _ {H} \left(A _ {1} \cup A _ {2}\right) \right| \\ \leqslant \left| A _ {1} \right| - 1 + \left| A _ {2} \right| - 1 - \left| A _ {1} \cup A _ {2} \right| \\ = \left| A _ {1} \cap A _ {2} \setminus \{a \} \right| - 1. \\ \end{array}
$$

Hence $H$ violates the marriage condition, contrary to assumption.

This last proof has a pretty ‘dual’, which begins by showing that $d _ { H } ( b ) \leqslant 1$ for every $b \in B$ . See Exercise 5 and its hint for details.

Corollary 2.1.3. If $G$ is $k$ -regular with $k \geqslant 1$ , then $G$ has a 1-factor.

Proof . If $G$ is $k$ -regular, then clearly $| A | = | B |$ ; it thus suffices to show by Theorem 2.1.2 that $G$ contains a matching of $A$ . Now every set $S \subseteq A$ L is joined to $N ( S )$ by a total of $k | S |$ edges, and these are among the $k \left| N ( S ) \right|$ edges of $G$ incident with $N ( S )$ . Therefore $k \left| S \right| \leqslant k \left| N ( S ) \right|$ , so $G$ does indeed satisfy the marriage condition. 

In some real-life applications, matchings are not chosen on the basis of global criteria for the entire graph but evolve as the result of independent decisions made locally by the participating vertices. A typical situation is that vertices are not indifferent to which of their incident edges are picked to match them, but prefer some to others. Then if $M$ is a matching and $e \ = \ a b$ is an edge not in $M$ such that both $a$ and $b$ prefer $e$ to their current matching edge (if they are matched), then $a$ and $b$ may agree to change $M$ locally by including $e$ and discarding their earlier matching edges. The matching $M$ , although perhaps of maximal size, would thus be unstable.

preferences stable matching

More formally, call a family ${ \big ( } \leqslant _ { v } { \big ) } _ { v \in V }$ of linear orderings $\leqslant _ { v }$ on $E ( v )$ a set of preferences for $G$ . Then call a matching $M$ in $G$ stable if for every edge $e \in E \setminus M$ there exists an edge $f \in M$ such that $e$ and $f$ have a common vertex $v$ with $e < _ { v } \ f$ . The following result is sometimes called the stable marriage theorem:

[ 5.4.4 ]

# Theorem 2.1.4. (Gale & Shapley 1962)

For every set of preferences, $G$ has a stable matching.

Proof . Call a matching $M$ in $G$ better than a matching $M ^ { \prime } \ne M$ if $M$ makes the vertices in $B$ happier than $M ^ { \prime }$ does, that is, if every vertex $b$ in an edge $f ^ { \prime } \in M ^ { \prime }$ is incident also with some $f \in M$ such that $f ^ { \prime } \leqslant b \ f$ . Given a matching $M$ , call a vertex $a \in A$ acceptable to $b \in B$ if $e = a b \in$ $E \setminus M$ and any edge $f \in M$ at $b$ satisfies $\textit { f } < _ { b } \textit { e }$ . Call $a \in A$ happy with $M$ if $a$ is unmatched or its matching edge $f \in M$ satisfies $f > _ { a } e$ for all edges $e = a b$ such that $a$ is acceptable to $b$ .

Starting with the empty matching, let us construct a sequence of matchings that each keep all the vertices in $A$ happy. Given such a matching $M$ , consider a vertex $a \in A$ that is unmatched but acceptable to some $b \in B$ . (If no such $a$ exists, terminate the sequence.) Add to $M$ the $\leqslant a$ -maximal edge $a b$ such that $a$ is acceptable to $b$ , and discard from $M$ any other edge at $b$ .

Clearly, each matching in our sequence is better than the previous, and it is easy to check inductively that they all keep the vertices in $A$ L happy. So the sequence continues until it terminates with a matching $M$ such that every unmatched vertex in $A$ is inacceptable to all its neighbours in $B$ . As every matched vertex in $A$ is happy with $M$ , this matching is stable. 

Despite its seemingly narrow formulation, the marriage theorem counts among the most frequently applied graph theorems, both outside graph theory and within. Often, however, recasting a problem in the setting of bipartite matching requires some clever adaptation. As a simple example, we now use the marriage theorem to derive one of the earliest results of graph theory, a result whose original proof is not all that simple, and certainly not short:

# Corollary 2.1.5. (Petersen 1891)

Every regular graph of positive even degree has a 2-factor.

Proof . Let $G$ be any $2 k$ -regular graph ( $k \geqslant 1$ ), without loss of generality (1.8.1) connected. By Theorem 1.8.1, $G$ contains an Euler tour $v _ { 0 } \boldsymbol { \epsilon } _ { 0 } \ldots \boldsymbol { \epsilon } _ { \ell - 1 } v _ { \ell }$ , with ${ v _ { \ell } } = { v _ { 0 } }$ . We replace every vertex $v$ by a pair $( v ^ { - } , v ^ { + } )$ , and every edge $e _ { i } = v _ { i } v _ { i + 1 }$ by the edge $v _ { i } ^ { + } v _ { i + 1 } ^ { - }$ (Fig. 2.1.4). The resulting bipartite graph $G ^ { \prime }$ is $k$ -regular, so by Corollary 2.1.3 it has a 1-factor. Collapsing every vertex pair $( v ^ { - } , v ^ { + } )$ back into a single vertex $v$ , we turn this 1- factor of $G ^ { \prime }$ into a 2-factor of $G$ . 

![](images/82e382259a3d45add5162f0805e773cd75cb77a5e20900d3dfed6c54f3ab7371.jpg)  
Fig. 2.1.4. Splitting vertices in the proof of Corollary 2.1.5

# 2.2 Matching in general graphs

Given a graph $G$ , let us denote by $\mathit { \Delta } { } ^ { \mathit { C } } G$ the set of its components, and by $q ( G )$ the number of its odd components, those of odd order. If $G$ has a 1-factor, then clearly

$$
q (G - S) \leqslant | S | \quad \text {f o r a l l} S \subseteq V (G),
$$

$c _ { G }$

$$
q (G)
$$

Tutte’s

condition

since every odd component of $G - S$ will send a factor edge to $S$ .

Again, this obvious necessary condition for the existence of a 1-factor is also sufficient:

# Theorem 2.2.1. (Tutte 1947)

A graph $G$ has a 1-factor if and only if $q ( G - S ) \leqslant | S |$ for all $S \subseteq V ( G )$ .

![](images/a1ef2eac568e828d7e42c11b1c9c4bd47fa7b60f20c68ca468265d03cfd8b387.jpg)  
Fig. 2.2.1. Tutte’s condition $q ( G - S ) \leqslant | S |$ for $q = 3$ , and the contracted graph $G _ { S }$ from Theorem 2.2.3.

$V , E$ bad set

Proof . Let $G = ( V , E )$ be a graph without a 1-factor. Our task is to find a bad set $S \subseteq V$ , one that violates Tutte’s condition.

We may assume that $G$ is edge-maximal without a 1-factor. Indeed, if $G ^ { \prime }$ is obtained from $G$ by adding edges and $S \subseteq V$ is bad for $G ^ { \prime }$ , then $S$ is also bad for $G$ : any odd component of $G ^ { \prime } - S$ is the union of components of $G - S$ , and one of these must again be odd.

What does $G$ look like? Clearly, if $G$ contains a bad set $S$ then, by its edge-maximality and the trivial forward implication of the theorem,

$s \in S$ e components of $G - S$ are compl $G - s$ d every vertex (∗)

But also conversely, if a set $S \subseteq V$ satisfies ( ) then either $S$ or the empty set must be bad: if $S$ is not bad we can join the odd components of $G - S$ disjointly to $S$ and pair up all the remaining vertices—unless $| G |$ is odd, in which case $\emptyset$ is bad.

So it suffices to prove that $G$ has a set $S$ of vertices satisfying $( * )$ . Let $S$ be the set of vertices that are adjacent to every other vertex. If this set $S$ does not satisfy $( * )$ , then some component of $G - S$ has nonadjacent vertices $a , a ^ { \prime }$ . Let $a , b , c$ be the first three vertices on a shortest $a$ – $a ^ { \prime }$ path in this component; then $a b , b c \in E$ but $a c \notin E$ . Since $b \notin S$ , there is a vertex $d \in V$ such that $b d \notin E$ . By the maximality of $G$ , there is a matching $M _ { 1 }$ of $V$ in $G + a c$ , and a matching $M _ { 2 }$ of $V$ in $G + b d$ .

![](images/3a566b8f6ff06bef20f431434528f3be22f628ba917943e71aae93c70ca29b3f.jpg)  
Fig. 2.2.2. Deriving a contradiction if $S$ does not satisfy ( )

Let $P = d \dots v$ be a maximal path in $G$ starting at $d$ with an edge from $M _ { 1 }$ and containing alternately edges from $M _ { 1 }$ and $M _ { 2 }$ (Fig. 2.2.2). If the last edge of $P$ lies in $M _ { 1 }$ , then $v = b$ , since otherwise we could continue $P$ . Let us then set $C : = P + b d$ . If the last edge of $P$ lies in $M _ { 2 }$ , then by the maximality of $P$ the $M _ { 1 }$ -edge at $v$ must be $a c$ , so $v \in \{ a , c \}$ ; then let $C$ be the cycle dP vbd. In each case, $C$ is an even cycle with every other edge in $M _ { 2 }$ , and whose only edge not in $E$ is $_ { b d }$ . Replacing in $M _ { 2 }$ its edges on $C$ with the edges of $C - M _ { 2 }$ , we obtain a matching of $V$ contained in $E$ , a contradiction. 

# Corollary 2.2.2. (Petersen 1891)

Every bridgeless cubic graph has a 1-factor.

Proof . We show that any bridgeless cubic graph $G$ satisfies Tutte’s condition. Let $S \subseteq V ( G )$ be given, and consider an odd component $C$ of $G - S$ . Since $G$ is cubic, the degrees (in $G$ ) of the vertices in $C$ sum to an odd number, but only an even part of this sum arises from edges of $C$ . So $G$ has an odd number of $S$ – $C$ edges, and therefore has at least 3 such edges (since $G$ has no bridge). The total number of edges between $S$ and $G - S$ thus is at least $3 q ( G - S )$ . But it is also at most $3 | S |$ , because $G$ is cubic. Hence $q ( G - S ) \leqslant | S |$ , as required. 

In order to shed a little more light on the techniques used in matching theory, we now give a second proof of Tutte’s theorem. In fact, we shall prove a slightly stronger result, a result that places a structure interesting from the matching point of view on an arbitrary graph. If the graph happens to satisfy the condition of Tutte’s theorem, this structure will at once yield a 1-factor.

A graph $G = ( V , E )$ is called factor-critical if $G \neq \emptyset$ and $G - v$ has a 1-factor for every vertex $v \in G$ . Then $G$ itself has no 1-factor, because it has odd order. We call a vertex set $S \subseteq V$ matchable to $\operatorname { \mathcal { C } } G - S$ if the (bipartite1) graph $G _ { S }$ , which arises from $G$ by contracting the components $C \in { \mathcal { C } } _ { G - S }$ to single vertices and deleting all the edges inside $S$ , contains a matching of $S$ . (Formally, $G _ { S }$ is the graph with vertex set $S \cup { \mathcal { C } } _ { G - S }$ and edge set $\{ s C \mid \exists c \in C \colon s c \in E \}$ ; see Fig. 2.2.1.)

# Theorem 2.2.3. Every graph $G = ( V , E )$ contains a vertex set $S$ with the following two properties:

(i) $S$ is matchable to $\operatorname { \mathcal { C } } G - S$ ;   
(ii) Every component of $G - S$ is factor-critical.

Given any such set $S$ , the graph $G$ contains a 1-factor if and only if $| S | = | { \mathcal { C } } _ { G - S } |$ .

For any given $G$ , the assertion of Tutte’s theorem follows easily from this result. Indeed, by (i) and (ii) we have $| S | \leqslant | { \mathcal { C } } _ { G - S } | = q ( G - S )$ (since factor-critical graphs have odd order); thus Tutte’s condition of $q ( G - S ) \leqslant | S |$ implies $| S | = | { \mathcal { C } } _ { G - S } |$ , and the existence of a 1-factor follows from the last statement of Theorem 2.2.3.

(2.1.2)

Proof of Theorem 2.2.3. Note first that the last assertion of the theorem follows at once from the assertions (i) and (ii): if $G$ has a 1-factor, we have $q ( G - S ) \leqslant | S |$ and hence $| S | = | { \mathcal { C } } _ { G - S } |$ as above; conversely if $\left| S \right| = \left| \mathcal { C } _ { G - S } \right|$ , then the existence of a 1-factor follows straight from (i) and (ii).

We now prove the existence of a set $S$ satisfying (i) and (ii), by induction on $| G |$ . For $| G | = 0$ we may take $S = \emptyset$ . Now let $G$ be given with $| G | > 0$ , and assume the assertion holds for graphs with fewer vertices.

Consider the sets $T \subseteq V$ for which Tutte’s condition fails worst, i.e. for which

$$
d (T) := d _ {G} (T) := q (G - T) - | T |
$$

is maximum, and let $S$ be a largest such set $T$ . Note that $d ( S ) \geqslant d ( \emptyset ) \geqslant 0$ .

We first show that every component $C \in { \mathcal { C } } _ { G - S } = : C$ is odd. If $| C |$ is even, pick a vertex $c \in C$ , and consider $T : = S \cup \{ c \}$ . As $C - c$ has odd order it has at least one odd component, which is also a component of $G - T$ . Therefore

$$
q (G - T) \geqslant q (G - S) + 1 \quad \text {w h i l e} \quad | T | = | S | + 1,
$$

so $d ( T ) \geqslant d ( S )$ contradicting the choice of $S$ .

Next we prove the assertion (ii), that every $C \in { \mathcal { C } }$ is factor-critical. Suppose there exist ${ \cal C } \in { \cal C }$ and $c \in C$ such that $C ^ { \prime } : = C - c$ has no 1-factor. By the induction hypothesis (and the fact that, as shown earlier, for fixed $G$ our theorem implies Tutte’s theorem) there exists a set $S ^ { \prime } \subseteq V ( C ^ { \prime } )$ with

$$
q (C ^ {\prime} - S ^ {\prime}) > | S ^ {\prime} |.
$$

Since $| C |$ is odd and hence $| C ^ { \prime } |$ is even, the numbers $q ( C ^ { \prime } - S ^ { \prime } )$ and $| S ^ { \prime } |$ are either both even or both odd, so they cannot differ by exactly 1. We may therefore sharpen the above inequality to

$$
q \left(C ^ {\prime} - S ^ {\prime}\right) \geqslant \left| S ^ {\prime} \right| + 2,
$$

giving $d _ { C ^ { \prime } } ( S ^ { \prime } ) \geqslant 2$ . Then for $T : = S \cup \{ c \} \cup S ^ { \prime }$ we have

$$
d (T) \geqslant d (S) - 1 - 1 + d _ {C ^ {\prime}} \left(S ^ {\prime}\right) \geqslant d (S),
$$

where the first ‘ $- 1 ^ { \prime }$ comes from the loss of $C$ as an odd component and the second comes from including $c$ in the set $T$ . As before, this contradicts the choice of $S$ .

It remains to show that $S$ is matchable to $\operatorname { \mathcal { C } } G - S$ . If not, then by the marriage theorem there exists a set $S ^ { \prime } \subseteq S$ that sends edges to fewer than $| S ^ { \prime } |$ components in $\boldsymbol { \mathscr { C } }$ . Since the other components in $c$ are also components of $G - ( S \backslash S ^ { \prime } )$ , the set $T = S \setminus S ^ { \prime }$ satisfies $d ( T ) > d ( S )$ , contrary to the choice of $S$ . 

Let us consider once more the set $S$ from Theorem 2.2.3, together with any matching $M$ in $G$ . As before, we write $c : = c _ { G - S }$ . Let us denote by $k _ { S }$ the number of edges in $M$ with at least one end in $S$ , and by $k c$ the number of edges in $M$ with both ends in $G - S$ . Since each $C \in { \mathcal { C } }$ is odd, at least one of its vertices is not incident with an edge of the second type. Therefore every matching $M$ satisfies

$$
k _ {S} \leqslant | S | \quad \text {a n d} \quad k _ {\mathcal {C}} \leqslant \frac {1}{2} \left(| V | - | S | - | \mathcal {C} |\right). \tag {1}
$$

Moreover, $G$ contains a matching $M _ { 0 }$ with equality in both cases: first choose $| S |$ edges between $S$ and $\cup { \mathcal { C } }$ according to (i), and then use (ii) to find a suitable set of ${ \scriptstyle { \frac { 1 } { 2 } } } \left( | C | - 1 \right)$ edges in every component $C \in { \mathcal { C } }$ . This matching $M _ { 0 }$ thus has exactly

$$
\left| M _ {0} \right| = | S | + \frac {1}{2} \left(| V | - | S | - | \mathcal {C} |\right) \tag {2}
$$

edges.

Now (1) and (2) together imply that every matching $M$ of maximum cardinality satisfies both parts of (1) with equality: by $| M | \geqslant | M _ { 0 } |$ and (2), $M$ has at least $| S | + { \textstyle { \frac { 1 } { 2 } } } \big ( | V | - | S | - | { \mathcal { C } } | \big )$ edges, which implies by (1) that neither of the inequalities in (1) can be strict. But equality in (1), in turn, implies that $M$ has the structure described above: by $k _ { S } = | S |$ , every vertex $s \in S$ is the end of an edge $s t \in M$ with $t \in G - S$ , and by $\begin{array} { r } { k _ { \mathcal { C } } = \frac { 1 } { 2 } \big ( | V | - | S | - | \mathcal { C } | \big ) } \end{array}$ exactly ${ \scriptstyle { \frac { 1 } { 2 } } } ( | C | - 1 )$ edges of $M$ lie in $C$ , for every $C \in { \mathcal { C } }$ . Finally, since these latter edges miss only one vertex in each $C$ , the ends $t$ of the edges $^ { s t }$ above lie in different components $C$ for different $s$ .

The seemingly technical Theorem 2.2.3 thus hides a wealth of structural information: it contains the essence of a detailed description of all maximum-cardinality matchings in all graphs. A reference to the full statement of this structural result, known as the Gallai-Edmonds matching theorem, is given in the notes at the end of this chapter.

# 2.3 Packing and covering

Much of the charm of K¨onig’s and Hall’s theorems in Section 2.1 lies in the fact that they guarantee the existence of the desired matching as soon as some obvious obstruction does not occur. In K¨onig’s theorem, we can find $k$ independent edges in our graph unless we can cover all its edges by fewer than $k$ vertices (in which case it is obviously impossible).

More generally, if $G$ is an arbitrary graph, not necessarily bipartite, and $\mathcal { H }$ is any class of graphs, we might compare the largest number $k$ of graphs from $\mathcal { H }$ (not necessarily distinct) that we can pack disjointly into $G$ with the smallest number $s$ of vertices of $G$ that will cover all its subgraphs in $\mathcal { H }$ . If $s$ can be bounded by a function of $k$ , i.e. independently of $G$ , we say that $\mathcal { H }$ has the Erd˝os-P´osa property. (Thus, formally, $\mathcal { H }$ has this property if there exists an $\mathbb { N } \to \mathbb { R }$ function $k \mapsto f ( k )$ such that, for every $k$ and $G$ , either $G$ contains $k$ disjoint subgraphs each isomorphic to a graph in $\mathcal { H }$ , or there is a set $U \subseteq V ( G )$ of at most $f ( k )$ vertices such that $G - U$ has no subgraph in $\mathcal { H }$ .)

Our aim in this section is to prove the theorem of Erd˝os and P´osa that the class of all cycles has this property: we shall find a function $f$ (about $4 k \log k$ ) such that every graph contains either $k$ disjoint cycles or a set of at most $f ( k )$ vertices covering all its cycles.

We begin by proving a stronger assertion for cubic graphs. For $k \in \mathbb N$ , put

$$
r _ {k} := \log k + \log \log k + 4 \quad \text {a n d} \quad s _ {k} := \left\{ \begin{array}{l l} 4 k r _ {k} & \text {i f} k \geqslant 2 \\ 1 & \text {i f} k \leqslant 1. \end{array} \right.
$$

Lemma 2.3.1. Let $k \in \mathbb N$ , and let $H$ be a cubic multigraph. $[ f | H | \geqslant s _ { k }$ then $H$ contains $k$ disjoint cycles.

(1.3.5) Proof . We apply induction on $k$ . For $k \leqslant 1$ the assertion is trivial, so let $k \geqslant 2$ be given for the induction step. Let $C$ be a shortest cycle in $H$ .

$_ { m }$

$_ n$

We first show that $H - C$ contains a subdivision of a cubic multigraph $H ^ { \prime }$ with $| H ^ { \prime } | \geqslant | H | - 2 | C |$ . Let $m$ be the number of edges between $C$ and $H - C$ . Since $H$ is cubic and $d ( C ) = 2$ , we have $m \leqslant | C |$ . We now consider bipartitions $\{ V _ { 1 } , V _ { 2 } \}$ of $V ( H )$ , beginning with $V _ { 1 } : = V ( C )$ . If $H \left[ V _ { 2 } \right]$ has a vertex of degree at most 1 we move this vertex to $V _ { 1 }$ , obtaining a new partition $\left\{ \begin{array} { l l }  V _ { 1 } , V _ { 2 } \right\} \end{array}$ crossed by fewer edges. Suppose we can perform a sequence of $n$ such moves, but no more. Then the resulting partition $\left\{ \begin{array} { l l }  V _ { 1 } , V _ { 2 } \right\} \end{array}$ is crossed by at most $m - n$ edges. And $H [ V _ { 2 } ]$ has at most $m - n$ vertices of degree less than 3, because each of these is incident with a cut edge. These vertices have degree exactly 2 in $H \left[ V _ { 2 } \right]$ , since we could not move them to $V _ { 1 }$ . Let $H ^ { \prime }$ be the cubic multigraph obtained from $H \left[ V _ { 2 } \right]$ by suppressing these vertices. Then

$$
\left| H ^ {\prime} \right| \geqslant | H | - | C | - n - (m - n) \geqslant | H | - 2 | C |,
$$

as desired.

To complete the proof, it suffices to show that $| H ^ { \prime } | \geqslant s _ { k - 1 }$ . Since $| C | \leqslant 2 \log | H |$ by Corollary 1.3.5 (or by $| H | \geqslant s _ { k }$ , if $| C | = g ( H ) \leqslant 2$ ), and $\lvert H \rvert \geqslant s _ { k } \geqslant 6$ , we have

$$
\left| H ^ {\prime} \right| \geqslant | H | - 2 | C | \geqslant | H | - 4 \log | H | \geqslant s _ {k} - 4 \log s _ {k}.
$$

(In the last inequality we use that the function $x \mapsto x - 4 \log x$ increases for $x \geqslant 6$ .)

It thus remains to show that $s _ { k } - 4 \log s _ { k } \geqslant s _ { k - 1 }$ . For $k = 2$ this is clear, so we assume that $k \geqslant 3$ . Then $r _ { k } \leqslant 4 \log k$ (which is obvious for $k \geqslant 4$ , while the case of $k = 3$ has to be calculated), and hence

$$
\begin{array}{l} s _ {k} - 4 \log s _ {k} = 4 (k - 1) r _ {k} + 4 \log k + 4 \log \log k + 1 6 \\ - \left(8 + 4 \log k + 4 \log r _ {k}\right) \\ \geqslant s _ {k - 1} + 4 \log \log k + 8 - 4 \log (4 \log k) \\ = s _ {k - 1}. \\ \end{array}
$$

# Theorem 2.3.2. (Erd˝os & P´osa 1965)

There is a function $f \colon  { \mathbb { N } } \to  { \mathbb { R } }$ such that, given any $k \in \mathbb N$ , every graph contains either $k$ disjoint cycles or a set of at most $f ( k )$ vertices meeting all its cycles.

Proof . We show the result for $f ( k ) : = s _ { k } + k - 1$ . Let $k$ be given, and let $G$ be any graph. We may assume that $G$ contains a cycle, and so it has a maximal subgraph $H$ in which every vertex has degree 2 or 3. Let $U$ be its set of degree 3 vertices.

Let $\boldsymbol { \mathscr { C } }$ be the set of all cycles in $G$ that avoid $U$ and meet $H$ in exactly one vertex. Let $Z \subseteq V ( H ) \setminus U$ be the set of those vertices. For each $z \in { Z }$ pick a cycle $C _ { z } \in { \mathcal { C } }$ that meets $H$ in $z$ , and put $\mathcal { C } ^ { \prime } : = \{ C _ { z } \vert z \in Z \}$ . By the maximality of $H$ , the cycles in $\mathcal { C } ^ { \prime }$ are disjoint.

Let $\mathcal { D }$ be the set of the 2-regular components of $H$ that avoid $Z$ . Then $\mathcal { C } ^ { \prime } \cup \mathcal { D }$ is another set of disjoint cycles. If $| { \mathcal { C } } ^ { \prime } \cup { \mathcal { D } } | \geqslant k$ , we are done. Otherwise we can add to $Z$ one vertex from each cycle in $\mathcal { D }$ to obtain a set $X$ of at most $k - 1$ vertices that meets all the cycles in $c$ and all the 2-regular components of $H$ . Now consider any cycle of $G$ that avoids $X$ . By the maximality of $H$ it meets $H$ . But it is not a component of $H$ , it does not lie in $\boldsymbol { \mathscr { C } }$ , and it does not contain an $H$ - path between distinct vertices outside $U$ (by the maximality of $H$ ). So this cycle meets $U$ .

We have shown that every cycle in $G$ meets $X \cup U$ . As $| X | \leqslant k - 1$ , it thus suffices to show that $| U | < s _ { k }$ unless $H$ contains $k$ disjoint cycles. But this follows from Lemma 2.3.1 applied to the graph obtained from $H$ by suppressing its vertices of degree 2. 

We shall meet the Erd˝os-P´osa property again in Chapter 12. There, a considerable extension of Theorem 2.3.2 will appear as an unexpected and easy corollary of the theory of graph minors.

# 2.4 Tree-packing and arboricity

In this section we consider packing and covering in terms of edges rather than vertices. How many edge-disjoint spanning trees can we find in a given graph? And how few trees in it, not necessarily edge-disjoint, suffice to cover all its edges?

To motivate the tree-packing problem, assume for a moment that our graph represents a communication network, and that for every choice of two vertices we want to be able to find $k$ edge-disjoint paths between them. Menger’s theorem (3.3.6 (ii)) in the next chapter will tell us that such paths exist as soon as our graph is $k$ -edge-connected, which is clearly also necessary. This is a good theorem, but it does not tell us how to find those paths; in particular, having found them for one pair of endvertices we are not necessarily better placed to find them for another pair. If our graph has $k$ edge-disjoint spanning trees, however, there will always be $k$ canonical such paths, one in each tree. Once we have stored those trees in our computer, we shall always be able to find the $k$ paths quickly, for any given pair of endvertices.

When does a graph $G$ have $k$ edge-disjoint spanning trees? If it does, it clearly must be $k$ -edge-connected. The converse, however, is easily seen to be false (try $k = 2$ ); indeed it is not even clear at that any edge-connectivity will imply the existence of $k$ edge-disjoint spanning trees. (But see Corollary 2.4.2 below.)

Here is another necessary condition. If $G$ has $k$ edge-disjoint spanning trees, then with respect to any partition of $V ( G )$ into $r$ sets, every spanning tree of $G$ has at least $r - 1$ cross-edges, edges whose ends lie in different partition sets (why?). Thus if $G$ has $k$ edge-disjoint spanning trees, it has at least $k \left( r - 1 \right)$ cross-edges. This condition is also sufficient:

# Theorem 2.4.1. (Nash-Williams 1961; Tutte 1961)

A multigraph contains $k$ edge-disjoint spanning trees if and only if for every partition $P$ of its vertex set it has at least $k \left( | P | - 1 \right)$ cross-edges.

Before we prove Theorem 2.4.1, let us note a surprising corollary: to ensure the existence of $k$ edge-disjoint spanning trees, it suffices to raise the edge-connectivity to just $2 k$ :

Corollary 2.4.2. Every $2 k$ -edge-connected multigraph $G$ has $k$ edgedisjoint spanning trees.

Proof . Every set in a vertex partition of $G$ is joined to other partition sets by at least $2 k$ edges. Hence, for any partition into $r$ sets, $G$ has at least $\textstyle { \frac { 1 } { 2 } } \sum _ { i = 1 } ^ { r } 2 k = k r$ cross-edges. The assertion thus follows from Theorem 2.4.1. 

For the proof of Theorem 2.4.1, let a multigraph $G = ( V , E )$ and $k \in \mathbb N$ be given. Let $\mathcal { F }$ be the set of all $k$ -tuples $F = ( F _ { 1 } , \dots , F _ { k } )$ of edge-disjoint spanning forests in $G$ with the maximum total number of edges, i.e. such that $\| F \| : = { \big | } E \left[ F \right] { \big | }$ with $E \left[ F \right] : = E ( F _ { 1 } ) \cup . . . \cup E ( F _ { k } )$ is as large as possible.

If $F = ( F _ { 1 } , \ldots , F _ { k } ) \in { \mathcal { F } }$ and $e \in E \setminus E \left[ F \right]$ , then every $F _ { i } + e$ contains a cycle $( i = 1 , \ldots , k )$ ): otherwise we could replace $F _ { i }$ by $F _ { i } + e$ in $F ^ { \prime }$ and obtain a contradiction to the maximality of $\| F \|$ . Let us consider an edge $e ^ { \prime } \neq e$ of this cycle, for some fixed $_ i$ . Putting $F _ { i } ^ { \prime } : = F _ { i } + e - e ^ { \prime }$ , and $F _ { j } ^ { \prime } : = F _ { j }$ for all $j \neq i$ , we see that $F ^ { \prime } : = ( F _ { 1 } ^ { \prime } , \dots , F _ { k } ^ { \prime } )$ is again in $\mathcal { F }$ ; we say that $F ^ { \prime }$ has been obtained from $F$ by the replacement of the edge $e ^ { \prime }$ with $e$ . Note that the component of $F _ { i }$ containing $e ^ { \prime }$ keeps its vertex set when it changes into a component of $F _ { i } ^ { \prime }$ . Hence for every path $x \ldots y \subseteq F _ { i } ^ { \prime }$ there is a unique path $x F _ { i } y$ in $F _ { i }$ ; this will be used later.

⊆ iWe now consider a fixed $k$ -tuple $F ^ { 0 } = ( F _ { 1 } ^ { \cup } , \ldots , F _ { k } ^ { \cup } ) \in \mathcal { F }$ . The set of all $k$ -tuples in $\mathcal { F }$ that can be obtained from $F ^ { 0 }$ by a series of edge replacements will be denoted by ${ \mathcal { F } } ^ { 0 }$ . Finally, we let

$$
E ^ {0} := \bigcup_ {F \in \mathcal {F} ^ {0}} (E \setminus E [ F ])
$$

and $G ^ { 0 } : = ( V , E ^ { 0 } )$ .

Lemma 2.4.3. For every $e ^ { 0 } \in E \setminus E \left[ F ^ { \cup } \right]$ there exists a set $U \subseteq V$ that is connected in every $F _ { i } ^ { 0 }$ ( $i = 1 , \ldots , k )$ and contains the ends of $e ^ { 0 }$ .

Proof . As $F ^ { 0 } \in \mathcal { F } ^ { 0 }$ , we have $e ^ { 0 } \in E ^ { 0 }$ ; let $C ^ { 0 }$ be the component of $G ^ { 0 }$ containing $e ^ { 0 }$ . We shall prove the assertion for $U : = V ( C ^ { 0 } )$ .

Let $i \in \{ 1 , \ldots , k \}$ be given; we have to show that $U$ is connected in $F _ { i } ^ { 0 }$ . To this end, we first prove the following:

Let $F = ( F _ { 1 } , \ldots , F _ { k } ) \in \mathcal { F } ^ { \cup }$ , and let $( F _ { 1 } ^ { \prime } , \ldots , F _ { k } ^ { \prime } )$ have been obtained from $F$ by the replacement of an edge of $F _ { i }$ . If (1) $x , y$ are the ends of a path in $F _ { i } ^ { \prime } \cap C ^ { \mathrm { 0 } }$ , then also $x F _ { i } y \subseteq C ^ { 0 }$ .

Let $e = v w$ be the new edge in $E ( F _ { i } ^ { \prime } ) \setminus E \left[ F \right]$ ; this is the only edge of $F _ { i } ^ { \prime }$ not lying in $F _ { i }$ . We assume that $e \in x F _ { i } ^ { \prime } y$ : otherwise we would have $x F _ { i } y = x F _ { i } ^ { \prime } y$ and nothing to show. It suffices to show that $v F _ { i } w \subseteq C ^ { 0 }$ : then $( x F _ { i } ^ { \prime } y - e ) \cup v F _ { i } w$ is a connected subgraph of $F _ { i } \cap C ^ { 0 }$ that contains $x , y$ , and hence also $x F _ { i } y$ . Let $e ^ { \prime }$ be any edge of $v F _ { i } w$ . Since we could replace $e ^ { \prime }$ in $F ~ \in ~ \mathcal { F } ^ { 0 }$ by $e$ and obtain an element of $\mathcal { F } ^ { 0 }$ not containing $e ^ { \prime }$ , we have $e ^ { \prime } \in E ^ { 0 }$ . Thus $v F _ { i } w \subseteq G ^ { 0 }$ , and hence $v F _ { i } w \subseteq C ^ { 0 }$ since $v , w \in x F _ { i } ^ { \prime } y \subseteq C ^ { 0 }$ . This proves (1).

$$
\begin{array}{l} G = (V, E) \\ k, \mathcal {F} \\ E [ F ], \| F \| \\ \end{array}
$$

edge replacement

$$
x F _ {i} y
$$

$$
F ^ {0}
$$

$$
\mathcal {F} ^ {0}
$$

$$
E ^ {0}
$$

$$
G ^ {0}
$$

$$
C ^ {0}
$$

$$
U
$$

$$
i
$$

$$

$$

$$
U
$$

$$
i
$$

In order to prove that $U = V ( C ^ { 0 } )$ is connected in $F _ { i } ^ { 0 }$ we show that, for every edge $x y \in C ^ { 0 }$ , the path $x F _ { i } ^ { 0 } y$ exists and lies in $C ^ { 0 }$ . As $C ^ { 0 }$ is connected, the union of all these paths will then be a connected spanning subgraph of $F _ { i } ^ { 0 } \left[ U \right]$ .

So let $e \ = \ x y \ \in \ C ^ { \mathsf { U } }$ be given. As $e \in E ^ { 0 }$ , there exist an $s \in \mathbb { N }$ and $k$ -tuples $F ^ { r } = ( F _ { 1 } ^ { r } , \dots , F _ { k } ^ { r } )$ for $r = 1 , \dots , s$ such that each $F ^ { r }$ is obtained from $F ^ { r - 1 }$ by edge replacement and $e \in E \setminus E [ F ^ { s } ]$ . Setting $F : = F ^ { s }$ in (1), we may think of $e$ as a path of length 1 in $F _ { i } ^ { \prime } \cap C ^ { \mathrm { 0 } }$ . Successive applications of (1) to $F = F ^ { s } , \ldots , F ^ { \scriptscriptstyle 0 }$ then give $x F _ { i } ^ { 0 } y \subseteq C ^ { 0 }$ as desired. 

Proof of Theorem 2.4.1. We prove the backward implication by induction on $| G |$ . For $| G | = 2$ the assertion holds. For the induction step, we now suppose that for every partition $P$ of $V$ there are at least $k \left( | P | - 1 \right)$ cross-edges, and construct $k$ edge-disjoint spanning trees in $G$ .

Pick a $k$ -tuple $F ^ { 0 } = ( F _ { 1 } ^ { \cup } , \ldots , F _ { k } ^ { \cup } ) \in \mathcal { F }$ . If every $F _ { i } ^ { 0 }$ is a tree, we are done. If not, we have

$$
\| F ^ {0} \| = \sum_ {i = 1} ^ {k} \| F _ {i} ^ {0} \| <   k (| G | - 1)
$$

by Corollary 1.5.3. On the other hand, we have $\| G \| \geqslant k \left( | G | - 1 \right)$ by assumption: consider the partition of $V$ into single vertices. So there exists an edge $e ^ { 0 } \in E \setminus E \left[ F ^ { 0 } \right]$ . By Lemma 2.4.3, there exists a set $U \subseteq V$ that is connected in every $F _ { i } ^ { 0 }$ and contains the ends of $e _ { 0 }$ ; in particular, $| U | \geqslant 2$ . Since every partition of the contracted multigraph $G / U$ induces a partition of $G$ with the same cross-edges,2 $G / U$ has at least $k \left( | P | - 1 \right)$ cross-edges with respect to any partition $P$ . By the induction hypothesis, therefore, $G / U$ has $k$ edge-disjoint spanning trees $T _ { 1 } , \dots , T _ { k }$ . Replacing in each $T _ { i }$ the vertex ${ \boldsymbol { v } } _ { U }$ contracted from $U$ by the spanning tree $F _ { i } ^ { \mathrm { U } } \cap G \left[ U \right]$ of $G [ U ]$ , we obtain $k$ edge-disjoint spanning trees in $G$ . 

Let us say that subgraphs $G _ { 1 } , \ldots , G _ { k }$ of a graph $G$ partition $G$ if their edge sets form a partition of $E ( G )$ . Our spanning tree problem may then be recast as follows: into how many connected spanning subgraphs can we partition a given graph? The excuse for rephrasing our simple tree problem in this more complicated way is that it now has an obvious dual (cf. Theorem 1.5.1): into how few acyclic (spanning) subgraphs can we partition a given graph? Or for given $k$ : which graphs can be partitioned into at most $k$ forests?

An obvious necessary condition now is that every set $U \subseteq V ( G )$ induces at most $k \left( | U | - 1 \right)$ edges, no more than $| U | - 1$ for each forest.

Once more, this condition turns out to be sufficient too. And surprisingly, this can be shown with the help of Lemma 2.4.3, which was designed for the proof of our theorem on edge-disjoint spanning trees:

# Theorem 2.4.4. (Nash-Williams 1964)

A multigraph $G = ( V , E )$ can be partitioned into at most $k$ forests if and only if $\left\| G \left[ U \right] \right\| \leqslant k \left( \left| U \right| - 1 \right)$ for every non-empty set $U \subseteq V$ .

Proof . The forward implication was shown above. Conversely, we show that every $k$ -tuple $F = ( F _ { 1 } , \ldots , F _ { k } ) \in { \mathcal { F } }$ partitions $G$ , i.e. that $E \left[ F \right] =$ $E$ . If not, let $e \in E \setminus E \left[ F \right]$ . By Lemma 2.4.3, there exists a set $U \subseteq V$ that is connected in every $F _ { i }$ and contains the ends of $e$ . Then $G [ U ]$ contains $| U | - 1$ edges from each $F _ { i }$ , and in addition the edge $e$ . Thus $\left\| G [ U ] \right\| > k ( | U | - 1 )$ , contrary to our assumption. 

The least number of forests forming a partition of a graph $G$ is called the arboricity of $G$ . By Theorem 2.4.4, the arboricity is a measure for the maximum local density: a graph has small arboricity if and only if it is ‘nowhere dense’, i.e. if and only if it has no subgraph $H$ with $\varepsilon ( H )$ large.

We shall meet Theorem 2.4.1 again in Chapter 8.5, where we prove its infinite version. This is based not on ordinary spanning trees (for which the result is false) but on ‘topological spanning trees’: the analogous structures in a topological space formed by the graph together with its ends.

# 2.5 Path covers

Let us return once more to K¨onig’s duality theorem for bipartite graphs, Theorem 2.1.1. If we orient every edge of $G$ from $A$ to $B$ , the theorem tells us how many disjoint directed paths we need in order to cover all the vertices of $G$ : every directed path has length 0 or 1, and clearly the number of paths in such a ‘path cover’ is smallest when it contains as many paths of length 1 as possible—in other words, when it contains a maximum-cardinality matching.

In this section we put the above question more generally: how many paths in a given directed graph will suffice to cover its entire vertex set? Of course, this could be asked just as well for undirected graphs. As it turns out, however, the result we shall prove is rather more trivial in the undirected case (exercise), and the directed case will also have an interesting corollary.

A directed path is a directed graph $P \neq \emptyset$ with distinct vertices $x _ { 0 } , \ldots , x _ { k }$ and edges $e _ { 0 } , \ldots , e _ { k - 1 }$ such that $e _ { i }$ is an edge directed from $x _ { i }$ to $x _ { i + 1 }$ , for all $i < k$ . In this section, path will always mean ‘directed

$\mathrm { t e r } ( \mathcal { P } )$ path cover

path’. The vertex $x _ { k }$ above is the last vertex of the path $P$ , and when $\mathcal { P }$ is a set of paths we write $\mathrm { t e r } ( \mathcal { P } )$ for the set of their last vertices. A path cover of a directed graph $G$ is a set of disjoint paths in $G$ which together contain all the vertices of $G$ .

# Theorem 2.5.1. (Gallai & Milgram 1960)

Every directed graph $G$ has a path cover $\mathcal { P }$ and an independent set $\{ v _ { P } \mid P \in \mathcal { P } \}$ of vertices such that $v _ { P } \in P$ for every $P \in \mathcal { P }$ .

$\mathcal { P }$   
$P _ { i }$   
$v _ { i }$

$_ v$

${ \mathcal { P } } ^ { \prime } , G ^ { \prime }$

Proof . We prove by induction on $| G |$ that for every path cover $\mathcal { P } =$ $\{ P _ { 1 } , \ldots , P _ { m } \}$ of $G$ with $\mathrm { t e r } ( \mathcal { P } )$ minimal there is a set $\{ v _ { P } \mid P \in \mathcal { P } \}$ as claimed. For each $i$ , let $v _ { i }$ denote the last vertex of $P _ { i }$ .

If $\operatorname { t e r } ( { \mathcal { P } } ) = \left\{ v _ { 1 } , \dots , v _ { m } \right\}$ is independent there is nothing more to show, so we assume that $G$ has an edge from $v _ { 2 }$ to $v _ { 1 }$ . Since $P _ { 2 } v _ { 2 } v _ { 1 }$ is again a path, the minimality of $\mathrm { t e r } ( \mathcal { P } )$ implies that $v _ { 1 }$ is not the only vertex of $P _ { 1 }$ ; let $v$ be the vertex preceding $v _ { 1 }$ on $P _ { 1 }$ . Then $\mathcal { P } ^ { \prime } : =$ $\{ P _ { 1 } v , P _ { 2 } , \ldots , P _ { m } \}$ is a path cover of $G ^ { \prime } : = G - v _ { 1 }$ (Fig. 2.5.1). Clearly, any independent set of representatives for ${ \mathcal { P } } ^ { \prime }$ in $G ^ { \prime }$ will also work for $\mathcal { P }$ in $G$ , so all we have to check is that we may apply the induction hypothesis to ${ \mathcal { P } } ^ { \prime }$ . It thus remains to show that $\operatorname { t e r } ( \mathcal { P } ^ { \prime } ) = \{ v , v _ { 2 } , \dots , v _ { m } \}$ is minimal among the sets of last vertices of path covers of $G ^ { \prime }$ .

![](images/7c5504c3a9b97ddce34e6b3db02313513703db30b76859bf73dba414a8facd78.jpg)  
Fig. 2.5.1. Path covers of $G$ and $G ^ { \prime }$

Suppose then that $G ^ { \prime }$ has a path cover ${ \mathcal { P } } ^ { \prime \prime }$ - with $\mathrm { t e r } ( \mathcal { P } ^ { \prime \prime } ) \subsetneq \mathrm { t e r } ( \mathcal { P } ^ { \prime } )$ . If a path $P \in \mathcal { P } ^ { \prime \prime }$ ends in $v$ , we may replace $P$ in ${ \mathcal { P } } ^ { \prime \prime }$ by $P { v v } _ { 1 }$ to obtain a path cover of $G$ whose set of last vertices is a proper subset of $\mathrm { t e r } ( \mathcal { P } )$ , contradicting the choice of $\mathcal { P }$ . If a path $P \in \mathcal { P } ^ { \prime \prime }$ ends in $v _ { 2 }$ (but none in $\boldsymbol { v }$ ), we similarly replace $P$ in ${ \mathcal { P } } ^ { \prime \prime }$ by $P v _ { 2 } v _ { 1 }$ to obtain a contradiction to the minimality of $\mathrm { t e r } ( \mathcal { P } )$ . Hence $\mathrm { t e r } ( { \mathcal P } ^ { \prime \prime } ) \subseteq \{ v _ { 3 } , . . . , v _ { m } \}$ . But now ${ \mathcal { P } } ^ { \prime \prime }$ and the trivial path $\{ v _ { 1 } \}$ together form a path cover of $G$ that contradicts the minimality of $\mathrm { t e r } ( \mathcal { P } )$ . 

As a corollary to Theorem 2.5.1 we obtain a classical result from the theory of partial orders. Recall that a subset of a partially ordered

set $( P , \leqslant )$ is a chain in $P$ if its elements are pairwise comparable; it is an antichain if they are pairwise incomparable.

# Corollary 2.5.2. (Dilworth 1950)

In every finite partially ordered set $( P , \leqslant )$ , the minimum number of chains with union $P$ is equal to the maximum cardinality of an antichain in $P$ .

Proof . If $A$ is an antichain in $P$ of maximum cardinality, then clearly $P$ cannot be covered by fewer than $| A |$ chains. The fact that $| A |$ chains will suffice follows from Theorem 2.5.1 applied to the directed graph on $P$ with the edge set $\{ ( x , y ) \mid x < y \}$ . 

# Exercises

1. Let $M$ be a matching in a bipartite graph $G$ . Show that if $M$ is suboptimal, i.e. contains fewer edges than some other matching in $G$ , then $G$ contains an augmenting path with respect to $M$ . Does this fact generalize to matchings in non-bipartite graphs?   
2. Describe an algorithm that finds, as efficiently as possible, a matching of maximum cardinality in any bipartite graph.   
3. Show that if there exist injective functions $A  B$ and $B  A$ between two infinite sets $A$ and $B$ then there exists a bijection $A  B$ .   
4. Derive the marriage theorem from K¨onig’s theorem.   
5. Let $G$ and $H$ be defined as for the third proof of Hall’s theorem. Show that $d _ { H } ( b ) \leqslant 1$ for every $b \in B$ , and deduce the marriage theorem.   
6.+ Find an infinite counterexample to the statement of the marriage theorem.   
7. Let $k$ be an integer. Show that any two partitions of a finite set into $k$ -sets admit a common choice of representatives.   
8. Let $A$ be a finite set with subsets $A _ { 1 } , \ldots , A _ { n }$ , and let $d _ { 1 } , \ldots , d _ { n } \in \mathbb { N }$ . Show that there are disjoint subsets $D _ { k } \subseteq A _ { k }$ , with $| D _ { k } | = d _ { k }$ for all $k \leqslant n$ , if and only if

$$
\left| \bigcup_ {i \in I} A _ {i} \right| \geqslant \sum_ {i \in I} d _ {i}
$$

for all $I \subseteq \{ 1 , \ldots , n \}$ .

9.+ Prove Sperner’s lemma: in an $n$ -set $X$ there are never more than $\scriptstyle { \binom { n } { \lfloor n / 2 \rfloor } }$

(Hint. Construct $\scriptstyle { \binom { n } { \lfloor n / 2 \rfloor } }$ chains covering the power set lattice of $X$

10. $-$ Find a bipartite graph and a set of preferences such that no matching of maximal size is stable and no stable matching has maximal size.

11. $-$ Find a non-bipartite graph with a set of preferences that has no stable matching.   
12. Show that all stable matchings of a given bipartite graph cover the same vertices. (In particular, they have the same size.)   
13. Find a set $S$ for Theorem 2.2.3 when $G$ is a forest.   
14. A graph $G$ is called (vertex-) transitive if, for any two vertices $v , w \in G$ , there is an automorphism of $G$ mapping $v$ to $w$ . Using the observations following the proof of Theorem 2.2.3, show that every transitive connected graph of even order contains a 1-factor.   
15. Show that a graph $G$ contains $k$ independent edges if and only if $q ( G - S ) \leqslant | S | + | G | - 2 k$ for all sets $S \subseteq V ( G )$ .   
16. $-$ Find a cubic graph without a 1-factor.   
17. Derive the marriage theorem from Tutte’s theorem.   
18.− Disprove the analogue of K¨onig’s theorem (2.1.1) for non-bipartite graphs, but show that $\mathcal { H } = \{ K ^ { 2 } \}$ has the Erd˝os-P´osa property.   
19. For cubic graphs, Lemma 2.3.1 is considerably stronger than the Erd˝os-P´osa theorem. Extend the lemma to arbitrary multigraphs of minimum degree $\geqslant 3$ , by finding a function $g { : } \mathbb { N } \to \mathbb { N }$ such that every multigraph of minimum degree $\geqslant 3$ and order at least $g ( k )$ contains $k$ disjoint cycles, for all $k \in \mathbb N$ . Alternatively, show that no such function $g$ exists.   
20. Given a graph $G$ , let $\alpha ( G )$ denote the largest size of a set of independent vertices in $G$ . Prove that the vertices of $G$ can be covered by at most $\alpha ( G )$ disjoint subgraphs each isomorphic to a cycle or a $K ^ { 2 }$ or $K ^ { 1 }$ .   
21. Find the error in the following short ‘proof’ of Theorem 2.4.1. Call a partition non-trivial if it has at least two classes and at least one of the classes has more than one element. We show by induction on $| V | + | E |$ that $G = ( V , E )$ has $k$ edge-disjoint spanning trees if every non-trivial partition of $V$ into $r$ sets (say) has at least $k ( r - 1 )$ cross-edges. The induction starts trivially with $G = K ^ { 1 }$ if we allow $k$ copies of $K ^ { 1 }$ as a family of $k$ edge-disjoint spanning trees of $K ^ { 1 }$ . We now consider the induction step. If every non-trivial partition of $V$ into $r$ sets (say) has more than $k ( r - 1 )$ cross-edges, we delete any edge of $G$ and are done by induction. So $V$ has a non-trivial partition $\{ V _ { 1 } , \ldots , V _ { r } \}$ with exactly $k ( r - 1 )$ cross-edges. Assume that $| V _ { 1 } | \geqslant 2$ . If $G ^ { \prime } : = G [ V _ { 1 } ]$ has $k$ disjoint spanning trees, we may combine these with $k$ disjoint spanning trees that exist in $G / V _ { 1 }$ by induction. We may thus assume that $G ^ { \prime }$ has no $k$ disjoint spanning trees. Then by induction it has a non-trivial vertex partition $\{ V _ { 1 } ^ { \prime } , \ldots , V _ { s } ^ { \prime } \}$ with fewer than $k ( s - 1 )$ cross-edges. Then $\{ V _ { 1 } ^ { \prime } , \ldots , V _ { s } ^ { \prime } , V _ { 2 } , \ldots , V _ { r } \}$ is a non-trivial vertex partition of $G$ into $r + s - 1$ sets with fewer than $k ( r - 1 ) + k ( s - 1 ) = k ( ( r + s - 1 ) - 1 )$ cross-edges, a contradiction.   
22. $-$ Prove the undirected version of the theorem of Gallai & Milgram (without using the directed version).

23. Derive the marriage theorem from the theorem of Gallai & Milgram.   
24. $-$ Show that a partially ordered set of at least $r s + 1$ elements contains either a chain of size $r + 1$ or an antichain of size $s + 1$ .   
25. Prove the following dual version of Dilworth’s theorem: in every finite partially ordered set $( P , \leqslant )$ , the minimum number of antichains with union $P$ is equal to the maximum cardinality of a chain in $P$ .   
26. Derive K¨onig’s theorem from Dilworth’s theorem.   
27. Find a partially ordered set that has no infinite antichain but is not a union of finitely many chains.

# Notes

There is a very readable and comprehensive monograph about matching in finite graphs: L. Lov´asz & M.D. Plummer, Matching Theory, Annals of Discrete Math. 29, North Holland 1986. Another very comprehensive source is A. Schrijver, Combinatorial optimization, Springer 2003. All the references for the results in this chapter can be found in these two books.

As we shall see in Chapter 3, K¨onig’s Theorem of 1931 is no more than the bipartite case of a more general theorem due to Menger, of 1929. At the time, neither of these results was nearly as well known as Hall’s marriage theorem, which was proved even later, in 1935. To this day, Hall’s theorem remains one of the most applied graph-theoretic results. The first two of our proofs are folklore; the third and its dual (Exercise 5) are due to Kriesell (2005). For background and applications of the stable marriage theorem, see D. Gusfield & R.W. Irving, The Stable Marriage Problem: Structure and Algorithms, MIT Press 1989.

Our proof of Tutte’s 1-factor theorem is based on a proof by Lov´asz (1975). Our extension of Tutte’s theorem, Theorem 2.2.3 (including the informal discussion following it) is a lean version of a comprehensive structure theorem for matchings, due to Gallai (1964) and Edmonds (1965). See Lov´asz & Plummer for a detailed statement and discussion of this theorem.

Theorem 2.3.2 is due to P. Erd˝os & L P´osa, On independent circuits contained in a graph, Canad. J. Math. 17 (1965), 347–352. Our proof is essentially due to M. Simonovits, A new proof and generalization of a theorem of Erd˝os and P´osa on graphs without $k + 1$ independent circuits, Acta Sci. Hungar 18 (1967), 191–206. Calculations such as in Lemma 2.3.1 are standard for proofs where one aims to bound one numerical invariant in terms of another. This book does not emphasize this aspect of graph theory, but it is not atypical.

There is also an analogue of the Erd˝os-P´osa theorem for directed graphs (with directed cycles), which had long been conjectured but was only recently proved by B. Reed, N. Robertson, P.D. Seymour and R. Thomas, Packing directed circuits, Combinatorica 16 (1996), 535–554. Its proof is much more difficult than the undirected case; see Chapter 12.4, and in particular Corollary 12.4.10, for a glimpse of the techniques used.

Theorem 2.4.1 was proved independently by Nash-Williams and by Tutte; both papers are contained in J. London Math. Soc. 36 (1961). Theorem 2.4.4

is due to C.St.J.A. Nash-Williams, Decompositions of finite graphs into forests, J. London Math. Soc. 39 (1964), 12. Both results can be elegantly expressed and proved in the setting of matroids; see $\ S 1 8$ in B. Bollob´as, Combinatorics, Cambridge University Press 1986.

An interesting vertex analogue of Corollary 2.4.2 is to ask which connectivity forces the existence of $k$ spanning trees $T _ { 1 } , \ldots , T _ { k } ^ { \prime }$ , all rooted at a given vertex $r$ , such that for every vertex $\boldsymbol { v }$ the $k$ paths $v T _ { i } r$ are independent. For example, if $G$ is a cycle then deleting the edge left or right of $r$ produces two such spanning trees. A. Itai and A. Zehavi, Three tree-paths, J. Graph Theory 13 (1989), 175–187, conjectured that $\kappa \geqslant k$ should suffice. This conjecture has been proved for $k \leqslant 4$ ; see S. Curran, O. Lee & X. Yu, Chain decompositions and independent trees in 4-connected graphs, Proc. 14th Ann. ACM SIAM symposium on Discrete algorithms (Baltimore 2003), 186–191.

Theorem 2.5.1 is due to T. Gallai & A.N. Milgram, Verallgemeinerung eines graphentheoretischen Satzes von R´edei, Acta Sci. Math. (Szeged) 21 (1960), 181–186.

Our definition of $k$ -connectedness, given in Chapter 1.4, is somewhat unintuitive. It does not tell us much about ‘connections’ in a $k$ -connected graph: all it says is that we need at least $k$ vertices to disconnect it. The following definition—which, incidentally, implies the one above—might have been more descriptive: ‘a graph is $k$ -connected if any two of its vertices can be joined by $k$ independent paths’.

It is one of the classic results of graph theory that these two definitions are in fact equivalent, are dual aspects of the same property. We shall study this theorem of Menger (1927) in some depth in Section 3.3.

In Sections 3.1 and 3.2, we investigate the structure of the 2-connected and the 3-connected graphs. For these small values of $k$ it is still possible to give a simple general description of how these graphs can be constructed.

In Sections 3.4 and 3.5 we look at other concepts of connectedness, more recent than the standard one but no less important: the number of $H$ -paths in $G$ for a subgraph $H$ of $G$ , and the existence of disjoint paths in $G$ linking up specified pairs of vertices.

# 3.1 2-Connected graphs and subgraphs

A maximal connected subgraph without a cutvertex is called a block. Thus, every block of a graph $G$ is either a maximal 2-connected subgraph, or a bridge (with its ends), or an isolated vertex. Conversely, every such subgraph is a block. By their maximality, different blocks of $G$ overlap in at most one vertex, which is then a cutvertex of $G$ . Hence, every edge of $G$ lies in a unique block, and $G$ is the union of its blocks.

Cycles and bonds, too, are confined to a single block:

# Lemma 3.1.1.

(i) The cycles of $G$ are precisely the cycles of its blocks.   
(ii) The bonds of $G$ are precisely the minimal cuts of its blocks.

Proof . (i) Any cycle in $G$ is a connected subgraph without a cutvertex, and hence lies in some maximal such subgraph. By definition, this is a block of $G$ .   
(ii) Consider any cut in $G$ . Let be one of its edges, and $B$ the $x y$ block containing it. By the maximality of $B$ in the definition of a block, $G$ contains no $B$ -path. Hence every $x$ – $_ y$ path of $G$ lies in $B$ , so those edges of our cut that lie in $B$ separate from even in $G$ . Assertion (ii) $x$ $y$ follows easily by repeated application of this argument. 

In a sense, blocks are the 2-connected analogues of components, the maximal connected subgraphs of a graph. While the structure of $G$ is determined fully by that of its components, however, it is not captured completely by the structure of its blocks: since the blocks need not be disjoint, the way they intersect defines another structure, giving a coarse picture of $G$ as if viewed from a distance.

The following proposition describes this coarse structure of $G$ as formed by its blocks. Let $A$ denote the set of cutvertices of $G$ , and $\boldsymbol { B }$ the set of its blocks. We then have a natural bipartite graph on $A \cup B$ formed by the edges $a B$ with $a \in B$ . This block graph of $G$ is shown in Figure 3.1.1.

![](images/dd216eb922708a02481f82b378d44553dcada543b3233d648ce7de7e61592734.jpg)  
Fig. 3.1.1. A graph and its block graph

Proposition 3.1.2. The block graph of a connected graph is a tree.



Proposition 3.1.2 reduces the structure of a given graph to that of its blocks. So what can we say about the blocks themselves? The following proposition gives a simple method by which, in principle, a list of all 2-connected graphs could be compiled:

Proposition 3.1.3. A graph is 2-connected if and only if it can be constructed from a cycle by successively adding $H$ -paths to graphs $H$ already constructed (Fig. 3.1.2).

![](images/d5b82cc58fc6b7a3ed53588dd84a07529250e7af9ef057f76eabc3595a3c8a80.jpg)  
Fig. 3.1.2. The construction of 2-connected graphs

Proof . Clearly, every graph constructed as described is 2-connected. Conversely, let a 2-connected graph $G$ be given. Then $G$ contains a cycle, and hence has a maximal subgraph $H$ constructible as above. Since any edge $x y \in E ( G ) \setminus E ( H )$ with $x , y \in H$ would define an $H$ - path, $H$ is an induced subgraph of $G$ . Thus if $H \neq G$ , then by the connectedness of $G$ there is an edge $v w$ with $v \in G - H$ and $w \in H$ . As $G$ is 2-connected, $G - w$ contains a $\boldsymbol { v }$ – $H$ path $P$ . Then $w v P$ is an $H$ -path in $G$ , and $H \cup w v P$ is a constructible subgraph of $G$ larger than $H$ . This contradicts the maximality of $H$ . 

# 3.2 The structure of 3-connected graphs

In the last section we showed first how every connected graph decomposes canonically into 2-connected subgraphs (and bridges), and how these are arranged in a tree-like way to make up the whole graph. There is a similar canonical decomposition of 2-connected graphs into 3-connected pieces (and cycles), which are again organized in a tree-like way. This nontrivial structure theorem of Tutte is most naturally expressed in terms of tree-decompositions, to be introduced in Chapter 12. We therefore omit it here.1

Instead, we shall describe how every 3-connected graph can be obtained from a $K ^ { 4 }$ by a succession of elementary operations preserving 3-connectedness. We then prove a deep result of Tutte about the algebraic structure of the cycle space of 3-connected graphs; this will play an important role again in Chapter 4.5.

In Proposition 3.1.3 we saw how every 2-connected graph can be constructed inductively by a sequence of steps starting from a cycle. All

the graphs in the sequence were themselves 2-connected, so the graphs obtainable by this construction method are precisely the 2-connected graphs. Note that the cycles as starting graphs cannot be replaced by a smaller class, because they do not have proper 2-connected subgraphs.

When we try to do the same for 3-connected graphs, we soon notice that both the set of starting graphs and the construction steps required become too complicated. If we base our construction sequences on the minor relation instead of subgraphs, however, it all works smoothly again:

Lemma 3.2.1. If $G$ is 3-connected and $| G | > 4$ , then $G$ has an edge $e$ such that $G / e$ is again 3-connected.

Proof . Suppose there is no such edge $e$ . Then, for every edge $x y \in G$ , the graph $G / x y$ contains a separator $S$ of at most 2 vertices. Since $\kappa ( G ) ~ \geqslant ~ 3$ , the contracted vertex $v _ { x y }$ of $G / x y$ (see Chapter 1.7) lies in $S$ and $| S | = 2$ , i.e. $G$ has a vertex $z \not \in \{ x , y \}$ such that $\{ v _ { x y } , z \}$ separates $G / x y$ . Then any two vertices separated by $\{ v _ { x y } , z \}$ in $G / x y$ are separated in $G$ by $T : = \{ x , y , z \}$ . Since no proper subset of $T$ separates $G$ , every vertex in $T$ has a neighbour in every component $C$ of $G - T$ .

We choose the edge $x y$ , the vertex $z$ , and the component $C$ so that $| C |$ is as small as possible, and pick a neighbour $\boldsymbol { v }$ of $z$ in $C$ (Fig. 3.2.1). By assumption, $G / z v$ is again not 3-connected, so again there is a vertex $w$ such that $\{ z , v , w \}$ separates $G$ , and as before every vertex in $\{ z , v , w \}$ has a neighbour in every component of $G - \{ z , v , w \}$ .

![](images/a460830b10e032990d66c1952981096d582eb3099cca3cf9fec8e75a9c9b02ee.jpg)  
Fig. 3.2.1. Separating vertices in the proof of Lemma 3.2.1

As $x$ and $y$ are adjacent, $G - \{ z , v , w \}$ has a component $D$ such that $D \cap \{ x , y \} = \emptyset$ . Then every neighbour of $v$ in $D$ lies in $C$ (since $v \in C$ ), so $D \cap C \neq \emptyset$ and hence $D \subsetneq C$ by the choice of $D$ . This contradicts the choice of $x y$ , $z$ and $C$ . 

# Theorem 3.2.2. (Tutte 1961)

$A$ graph $G$ is 3-connected if and only if there exists a sequence $G _ { 0 } , \ldots , G _ { n }$ of graphs with the following properties:

(i) $G _ { 0 } = K ^ { 4 }$ and $G _ { n } = G$   
(ii) $G _ { i + 1 }$ has an edge xy with $d ( x ) , d ( y ) \geqslant 3$ and $G _ { i } = G _ { i + 1 } / x y$ , for every $i < n$ .

Proof . If $G$ is 3-connected, a sequence as in the theorem exists by Lemma 3.2.1. Note that all the graphs in this sequence are 3-connected.

Conversely, let $G _ { 0 } , \ldots , G _ { n }$ be a sequence of graphs as stated; we show that if $G _ { i } = G _ { i + 1 } / x y$ is 3-connected then so is $G _ { i + 1 }$ , for every $i < n$ . Suppose not, let $S$ be a separator of at most 2 vertices in $G _ { i + 1 }$ , and let $C _ { 1 } , C _ { 2 }$ be two components of $G _ { i + 1 } - S$ . As $x$ and $y$ are adjacent, we may assume that $\{ x , y \} \cap V ( C _ { 1 } ) = \emptyset$ (Fig. 3.2.2). Then $C _ { 2 }$ contains neither

xy

$S$

$C _ { 1 } , C _ { 2 }$

![](images/03ce414607e7e87c83355635e7a4bd0ee7fea2eb893a7259a1327dd9183481da.jpg)  
Fig. 3.2.2. The position of $x y \ \in \ G _ { i + 1 }$ in the proof of Theorem 3.2.2

both vertices $x , y$ nor a vertex $v \not \in \{ x , y \}$ : otherwise $v _ { x y }$ or $v$ would be separated from $C _ { 1 }$ in $G _ { i }$ by at most two vertices, a contradiction. But now $C _ { 2 }$ contains only one vertex: either $x$ or $y$ . This contradicts our assumption of $d ( x ) , d ( y ) \geqslant 3$ . 

Theorem 3.2.2 is the essential core of a result of Tutte known as his wheel theorem.2 Like Proposition 3.1.3 for 2-connected graphs, it enables us to construct all 3-connected graphs by a simple inductive process depending only on local information: starting with $K ^ { 4 }$ , we pick a vertex $v$ in a graph constructed already, split it into two adjacent vertices $v ^ { \prime } , v ^ { \prime \prime }$ , and join these to the former neighbours of $v$ as we please—provided only that $v ^ { \prime }$ and $v ^ { \prime \prime }$ each acquire at least 3 incident edges, and that every former neighbour of $v$ becomes adjacent to at least one of $v ^ { \prime } , v ^ { \prime \prime }$ .

# Theorem 3.2.3. (Tutte 1963)

The cycle space of a 3-connected graph is generated by its non-separating induced cycles.

Proof . We apply induction on the order of the graph $G$ considered. In $K ^ { 4 }$ , every cycle is a triangle or (in terms of edges) the symmetric difference of triangles. As these are induced and non-separating, the assertion holds for $| G | = 4$ .

For the induction step, let $\mathit { \Pi } _ { e } ~ = ~ \mathit { { x y } }$ be an edge of $G$ for which $G ^ { \prime } : = G / e$ is again 3-connected; cf. Lemma 3.2.1. Then every edge $e ^ { \prime } \in E ( G ^ { \prime } ) \setminus E ( G )$ is of the form $e ^ { \prime } = u v _ { e }$ , where at least one of the two edges $u x$ and uy lies in $G$ . We pick one that does (either $u x$ or $u y$ ), and identify it notationally with the edge $e ^ { \prime }$ ; thus $e ^ { \prime }$ now denotes both

[ 4.5.2 ]

(1.9.1)

$e = x y$

$G ^ { \prime }$

fundamental triangles

the edge ${ u v _ { e } }$ of $G ^ { \prime }$ and one of the two edges $u x , u y$ . In this way we may regard $E ( G ^ { \prime } )$ as a subset of $E ( G )$ , and $\mathcal { E } ( G ^ { \prime } )$ as a subspace of ${ \mathcal { E } } ( G )$ ; thus all additions of edge sets will take place unambiguously in ${ \mathcal { E } } ( G )$ .

A special role in this proof will be played by the triangles uxy in $G$ that contain the edge $e$ . We shall call these the fundamental triangles of $G$ ; they are clearly (induced and) non-separating, as otherwise $\{ u , v _ { e } \}$ would separate $G ^ { \prime }$ , contradicting its 3-connectedness.

Now consider an induced cycle $C \subseteq G$ that is not a fundamental triangle. If $e \in C$ , then $C / e$ is a cycle in $G ^ { \prime }$ . If $e \not \in C$ , then at most one of $x , y$ lies on $C$ , as otherwise $e$ would be a chord. Then the vertices of $C$ in order also form a cycle in $G ^ { \prime }$ (replace $x$ or $y$ by $v _ { e }$ as necessary); this cycle, too, will be denoted by $C / e$ . Thus, for every induced cycle $C \subseteq G$ that is not a fundamental triangle, $C / e$ denotes a unique cycle in $G ^ { \prime }$ . However, even in the case of $e \not \in C$ the edge set of $C / e$ when viewed as a subset of $E ( G )$ need not coincide with $E ( C )$ , or even form a cycle at all; an example is shown in Figure 3.2.3.

![](images/4cb1e482a21c793cc3255cf65b910659ac268fd8ee15a0e2d3234261a1f5b84b.jpg)  
Fig. 3.2.3. One of the four possibilities for $E ( C / e )$ when $e \not \in C$

basic cycles

$C$

$C ^ { \prime }$

similar

Let us refer to the non-separating induced cycles in $G$ or $G ^ { \prime }$ as basic cycles. We thus want to show that every element of ${ \mathcal { C } } ( G )$ is a sum of basic cycles in $G$ . Let $C \in { \mathcal { C } } ( G )$ be given. By Proposition 1.9.1 and our observation that fundamental triangles are themselves basic, we may assume that $C$ is an induced cycle but not a fundamental triangle. Hence, $C ^ { \prime } : = C / e$ is a cycle in $G ^ { \prime }$ .

Roughly, our plan is to generate $C ^ { \prime }$ from basic cycles in $G ^ { \prime }$ by induction, and lift the generators back to basic cycles in $G$ that generate $C$ . Now as we have seen, the edge set $C ^ { \prime }$ can differ a little from $C$ , and similarly the basic cycles of $G ^ { \prime }$ that generate $C ^ { \prime }$ may differ a little from basic cycles in $G$ . To make these differences precise, and to show that they do not really matter, let us call two sets $F , { \dot { F } } \in { \mathcal { E } } ( G )$ similar if they differ only by fundamental triangles and possibly in $e$ , i.e., if there exists a sum $D$ of fundamental triangles such that

$$
F + \tilde {F} + D \in \{\emptyset , \{e \} \}.
$$

Clearly, similarity is an equivalence relation.

Instead of generating $C$ from basic cycles, it will be enough to generate a set ${ \tilde { C } } \in { \mathcal { C } } ( G )$ similar to $C$ :

$$
\begin{array}{l} \text {I f C i s s i m i l a r t o \tilde {C} \in \mathcal {C} (G) a n d \tilde {C} i s a s u m o f b a s i c c y c l e s} \\ \text {i n G , t h e n s o i s C .} \end{array} \tag {1}
$$

For if $D$ is a sum of fundamental triangles such that $C + \bar { C } + D \in$ $\{ \emptyset , \{ e \} \}$ , then $C + \tilde { C } + D = \emptyset$ , because $C + \tilde { C } + D$ lies in ${ \mathcal { C } } ( G )$ but $\{ e \}$ does not. Hence, as $D$ is a sum of basic cycles, so is $C = \tilde { C } + D$ .

Let us begin our proof by noting that

$$
C ^ {\prime} \text {i s} C. \tag {2}
$$

Indeed, if $e \in C$ or neither $x$ nor $y$ lies on $C$ , then $C ^ { \prime }$ differs from $C$ exactly in $e$ or not at all. Otherwise, $C$ contains one of the vertices $x , y$ but not the other. Then $v _ { e } \in C ^ { \prime }$ ; let $u , w$ be the two neighbours of $v _ { e }$ on $C ^ { \prime }$ , and $e ^ { \prime } = u v _ { e }$ and $e ^ { \prime \prime } = v _ { e } w$ its incident edges (as in Fig. 3.2.3). If $e ^ { \prime } \not \in C$ , let $D _ { u }$ be the fundamental triangle uxy; otherwise put $D _ { u } : = \varnothing$ . If $e ^ { \prime \prime } \not \in C$ , let $D _ { w }$ be the fundamental triangle wxy; otherwise put $D _ { w } : = \varnothing$ . Then $C + C ^ { \prime } + D _ { u } + D _ { w } \in \{ \emptyset , \{ e \} \}$ , completing the proof of (2).

{ ∅ { }By the induction hypothesis, $C ^ { \prime }$ is a sum of basic cycles $C _ { 1 } ^ { \prime } , \ldots , C _ { k } ^ { \prime }$ in $G ^ { \prime }$ . Let us lift these back to $G$ , as follows:

$$
\begin{array}{l} \text {F o r e v e r y} i = 1, \dots , k \text {t h e r e e x i s t s a b a s i c c y c l e C _ {i}} \text {i n} G \\ \text {t h a t i s s i m i l a r t o C _ {i} ^ {\prime}}. \end{array} \tag {3}
$$

To prove (3), we shall choose the $C _ { i }$ so that $C _ { i } / e = C _ { i } ^ { \prime }$ ; these will be similar to the $C _ { i } ^ { \prime }$ as in (2). If $v _ { e } \notin C _ { i } ^ { \prime }$ then this holds with $C _ { i } : = C _ { i } ^ { \prime }$ , so we assume that $v _ { e } \in C _ { i } ^ { \prime }$ . Let $u$ and $w$ be the two neighbours of $v _ { e }$ on $C _ { i } ^ { \prime }$ , and let $P$ be the $u$ – $w$ path in $C _ { i } ^ { \prime }$ avoiding $v _ { e }$ (Fig. 3.2.4). Then $P \subseteq G$ .

![](images/40b9b097fa3ff9d59389f42e407a8c7fb156b589e50476641eaab5b525259262.jpg)  
Fig. 3.2.4. The search for a basic cycle $C _ { i }$ with $C _ { i } / e = C _ { i } ^ { \prime }$

We first assume that $\{ u x , u y , w x , w y \} \subseteq E ( G )$ , and consider (as candidates for $C _ { i }$ ) the cycles $C _ { x } : = u P w x u$ and $C _ { y } : = u P w y u$ . Both are induced cycles in $G$ (because $C _ { i } ^ { \prime }$ is induced in $G ^ { \prime }$ ), and clearly $C _ { x } / e =$ $C _ { i } ^ { \prime } = C _ { y } / e$ . Moreover, neither of these cycles separates two vertices of $G - ( V ( P ) \cup \{ x , y \} )$ in $G$ , since $C _ { i } ^ { \prime }$ does not separate such vertices in $G ^ { \prime }$ . Thus, if $C _ { x }$ (say) is a separating cycle in $G$ , then one of the

components of $G - C _ { x }$ consists just of $y$ . Likewise, if $C _ { y }$ separates $G$ then one of the arising components contains only $x$ . However, this cannot happen for both $C _ { x }$ and $C _ { y }$ at once: otherwise $N _ { G } ( \{ x , y \} ) \subseteq V ( P )$ and hence $N _ { G } ( \{ x , y \} ) = \{ u , w \}$ (since $C _ { i } ^ { \prime }$ has no chord), which contradicts $\kappa ( G ) \geqslant 3$ . Hence, at least one of $C _ { x }$ , $C _ { y }$ is a basic cycle in $G$ , and we choose this as $C _ { i }$ .

It remains to consider the case that $\{ u x , u y , w x , w y \} \not \subseteq E ( G )$ , say $u x \not \in E ( G )$ . Using the 3-connectedness of $G$ as above, we see that either uP wyu or $u P w x y u$ is a basic cycle in $G$ (which we choose as $C _ { i }$ ), according as $_ { w y }$ is an edge of $G$ or not. This completes the proof of (3).

By (3), $\ddot { C } : = C _ { 1 } + \ldots + C _ { k }$ is similar to $C ^ { \prime } = C _ { 1 } ^ { \prime } + \ldots + C _ { k } ^ { \prime }$ , which in turn is similar to $C$ by (2). By (1), this completes the proof. 

# 3.3 Menger’s theorem

The following theorem is one of the cornerstones of graph theory.

# Theorem 3.3.1. (Menger 1927)

Let $G = ( V , E )$ be a graph and $A , B \subseteq V$ . Then the minimum number of vertices separating $A$ from $B$ in $G$ is equal to the maximum number of disjoint $A$ – $B$ paths in $G$ .

We offer three proofs. Whenever $G , A , B$ are given as in the theorem, we denote by $k = k \left( G , A , B \right)$ the minimum number of vertices separating $A$ from $B$ in $G$ . Clearly, $G$ cannot contain more than $k$ disjoint $A$ – $B$ paths; our task will be to show that $k$ such paths exist.

First proof. We apply induction on $\| G \|$ . If $G$ has no edge, then $| A \cap B | = k$ and we have $k$ trivial $A$ – $B$ paths. So we assume that $G$ has an edge $e = x y$ . If $G$ has no $k$ disjoint $A$ – $B$ paths, then neither does $G / e$ ; here, we count the contracted vertex $v _ { e }$ as an element of $A$ L (resp. $B$ ) in $G / e$ if in $G$ at least one of lies in $A$ (resp. $B$ ). By the $x , y$ induction hypothesis, $G / e$ contains an $A { - } B$ separator $Y$ of fewer than $k$ vertices. Among these must be the vertex $v _ { e }$ , since otherwise $Y \subseteq V$ would be an $A$ – $B$ separator in $G$ . Then $X : = ( Y \setminus \{ v _ { e } \} ) \cup \{ x , y \}$ is an $A$ – $B$ separator in $G$ of exactly $k$ vertices.

We now consider the graph $G - e$ . Since $x , y \in X$ , every $A$ – $X$ separator in $G - e$ is also an $A$ – $B$ separator in $G$ and hence contains at least $k$ vertices. So by induction there are $k$ disjoint $A { - } X$ paths in $G - e$ , and similarly there are $k$ disjoint $X$ – $B$ paths in $G - e$ . As $X$ separates $A$ from $B$ , these two path systems do not meet outside $X$ , and can thus be combined to $k$ disjoint $A$ – $B$ paths. 

Let $\mathcal { P }$ be a set of disjoint $A$ – $B$ paths, and let $\mathcal { Q }$ be another such set. We say that $\mathcal { Q }$ exceeds $\mathcal { P }$ if the set of vertices in $A$ that lie on a path in $\mathcal { P }$ is a proper subset of the set of vertices in $A$ that lie on a path in $\mathcal { Q }$ , and likewise for $B$ . Then, in particular, $| \mathcal { Q } | \geqslant | \mathcal { P } | + 1$ .

exceeds

Second proof. We prove the following stronger statement:

If $\mathcal { P }$ is any set of fewer than $k$ disjoint $A$ – $B$ paths in $G$ , then $G$ contains a set of $| \mathcal { P } | + 1$ disjoint $A$ – $B$ paths exceeding $\mathcal { P }$ .

Keeping $G$ and $A$ fixed, we let $B$ vary and apply induction on $| \cup \mathcal { P } |$ . Let $R$ be an $A$ – $B$ path that avoids the (fewer than $k$ ) vertices of $B$ that lie on a path in $\mathcal { P }$ . If $R$ avoids all the paths in $\mathcal { P }$ , then ${ \mathcal { P } } \cup \{ R \}$ exceeds $\mathcal { P }$ , as desired. (This will happen when $\mathcal { P } = \emptyset$ , so the induction starts.) If not, let $x$ be the last vertex of $R$ that lies on some $P \in \mathcal { P }$ . Put $B ^ { \prime } : = B \cup V ( x P \cup x R )$ and $\mathcal { P } ^ { \prime } : = \left( \mathcal { P } \setminus \{ P \} \right) \cup \{ P x \}$ (Fig. 3.3.1). Then $\left| \mathcal { P } ^ { \prime } \right| = \left| \mathcal { P } \right|$ (but $| \bigcup \mathcal { P } ^ { \prime } | < | \bigcup \mathcal { P } | ,$ ) and $k ( G , A , B ^ { \prime } ) \geqslant k ( G , A , B )$ , so by the induction hypothesis there is a set $\mathcal { Q } ^ { \prime }$ of $\vert \mathcal { P } ^ { \prime } \vert + 1$ disjoint $A { - } B ^ { \prime }$ paths exceeding ${ \mathcal { P } } ^ { \prime }$ . Then $\mathcal { Q } ^ { \prime }$ contains a path $Q$ ending in $x$ , and a unique path $Q ^ { \prime }$ whose last vertex $y$ is not among the last vertices of the paths in ${ \mathcal { P } } ^ { \prime }$ . If $y \notin x P$ , we let $\mathcal { Q }$ be obtained from $\mathcal { Q } ^ { \prime }$ by adding $x P$ to $Q$ , and adding $y R$ to $Q ^ { \prime }$ i f $y \notin B$ . Otherwise $y \in \mathring { x } P$ , and we let $\mathcal { Q }$ be obtained from $\mathcal { Q } ^ { \prime }$ by adding $x R$ to $Q$ and adding $y P$ to $Q ^ { \prime }$ . In both cases $\mathcal { Q }$ exceeds $\mathcal { P }$ , as desired. 

![](images/6ab0867fd71636916a8600c7b8ff66ae939c04e900b6d72cca65d662a38bd4d3.jpg)  
Fig. 3.3.1. Paths in the second proof of Menger’s theorem

Applied to a bipartite graph, Menger’s theorem specializes to the assertion of K¨onig’s theorem (2.1.1). For our third proof, we shall adapt the alternating path proof of K¨onig’s theorem to the more general setup of Theorem 3.3.1. Let again $G , A , B$ be given, and let $\mathcal { P }$ be a set of disjoint $A$ – $B$ paths in $G$ . Let us say that an $A$ – $B$ separator $X \subseteq V$ lies on $\mathcal { P }$ if it consists of a choice of exactly one vertex from each path in $\mathcal { P }$ . If we can find such a separator $X$ , then clearly $k \leqslant | X | = | \mathcal { P } |$ , and Menger’s theorem will be proved.

Put

$$
V [ \mathcal {P} ] := \bigcup \{V (P) \mid P \in \mathcal {P} \}
$$

$$
E [ \mathcal {P} ] := \bigcup \{E (P) \mid P \in \mathcal {P} \}.
$$

W, xi, ei alternating walk

Let a walk $W = x _ { 0 } e _ { 0 } x _ { 1 } e _ { 1 } \dots e _ { n - 1 } x _ { n }$ in $G$ with $e _ { i } \neq e _ { j }$ for $i \neq j$ be said to alternate with respect to $\mathcal { P }$ (Fig. 3.3.2) if it starts in $A \setminus V \left[ \mathcal { P } \right]$ and the following three conditions hold for all $i < n$ (with $e _ { - 1 } : = e _ { 0 }$ in (iii)):

(i) if $e _ { i } = e \in E [ \mathcal { P } ]$ , then $W$ traverses the edge $e$ backwards, i.e. $x _ { i + 1 } \in P _ { x _ { i } } ^ { \cup }$ for some $P \in \mathcal { P }$ ;   
(ii) if $x _ { i } = x _ { j }$ with $i \neq j$ , then $x _ { i } \in V [ \mathcal { P } ]$   
(iii) if $x _ { i } \in V [ \mathcal { P } ]$ , then $\{ e _ { i - 1 } , e _ { i } \} \cap E \left[ \mathcal { P } \right] \neq \emptyset$ .

![](images/66f05bc108519af54db378e2c012c79e464a456531d42f4a4848483c1a606f4a.jpg)  
Fig. 3.3.2. An alternating walk from $A$ to $B$

Note that, by (ii), any vertex outside $V [ \mathcal { P } ]$ occurs at most once on $W$ . And since the edges $e _ { i }$ of $W$ are all distinct, (iii) implies that any vertex $v \in V [ \mathcal { P } ]$ occurs at most twice on $W$ . For $\boldsymbol { v } \neq \boldsymbol { x } _ { n }$ , this can happen in exactly the following two ways. If $x _ { i } = x _ { j }$ with $0 < i < j < n$ , then

$$
\begin{array}{l} e i t h e e _ {i - 1}, e _ {j} \in E [ \mathcal {P} ] a n d e _ {i}, e _ {j - 1} \notin E [ \mathcal {P} ] \\ o r e _ {i}, e _ {j - 1} \in E [ \mathcal {P} ] a n d e _ {i - 1}, e _ {j} \notin E [ \mathcal {P} ]. \\ \end{array}
$$

Unless otherwise stated, any use of the word ‘alternate’ below will refer to our fixed path system $\mathcal { P }$ .

[ 8.4.5 ] Lemma 3.3.2. If an alternating walk $W$ as above ends in $B \setminus V [ \mathcal { P } ]$ , then $G$ contains a set of disjoint $A$ – $B$ paths exceeding $\mathcal { P }$ .

Proof . We may assume that $W$ has only its first vertex in $A \setminus V [ \mathcal { P } ]$ and only its last vertex in $B \setminus V [ \mathcal { P } ]$ . Let $H$ be the graph on $V ( G )$ whose edge set is the symmetric difference of $E [ \mathcal { P } ]$ with $\{ e _ { 0 } , \ldots , e _ { n - 1 } \}$ . In $H$ , the ends of the paths in $\mathcal { P }$ and of $W$ have degree 1 (or 0, if the path or $W$ is trivial), and all other vertices have degree 0 or 2.

For each vertex $a \in ( A \cap V [ \mathcal { P } ] ) \cup \{ x _ { 0 } \}$ , therefore, the component of $H$ containing $a$ is a path, $P = v _ { 0 } \ldots v _ { k }$ say, which starts in $a$ and ends in $A$ or $B$ . Using conditions (i) and (iii), one easily shows by induction on $i = 0 , \ldots , k - 1$ that $P$ traverses each of its edges $e = v _ { i } v _ { i + 1 }$ in the forward direction with respect to $\mathcal { P }$ or $W$ . (Formally: if $e \in \ P ^ { \prime }$ - with $P ^ { \prime } \in \mathcal { P }$ , then $v _ { i } \in P ^ { \prime } \mathring { v } _ { i + 1 }$ ; if $e = e _ { j } \in W$ , then ${ \boldsymbol { v } } _ { i } = { \boldsymbol { x } } _ { j }$ and $v _ { i + 1 } = x _ { j + 1 }$ .) Hence, $P$ is an $A$ – $B$ path.

Similarly, for every $b \in \left( B \cap V \left[ \mathcal { P } \right] \right) \cup \{ x _ { n } \}$ there is an $A$ – $B$ path in $H$ that ends in $b$ . The set of $A$ – $B$ paths in $H$ therefore exceeds $\mathcal { P }$ . 

Lemma 3.3.3. If no alternating walk $W$ as above ends in $B \setminus V [ \mathcal { P } ]$ , [ 8.4.5 ] then $G$ contains an $A$ – $B$ separator on $\mathcal { P }$ .

Proof . Let

$$
A _ {1} := A \cap V [ \mathcal {P} ] \quad \text {a n d} \quad A _ {2} := A \setminus A _ {1}, \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \left. A _ {1}, A _ {2} \right\}
$$

and

$$
B _ {1} := B \cap V [ \mathcal {P} ] \quad \text {a n d} \quad B _ {2} := B \setminus B _ {1}. \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \left. B _ {1}, B _ {2} \right\}
$$

For every path $P \in \mathcal { P }$ , let $x _ { P }$ be the last vertex of $P$ that lies on some alternating walk; if no such vertex exists, let $x _ { P }$ be the first vertex of $P$ . Our aim is to show that

$$
X := \left\{x _ {P} \mid P \in \mathcal {P} \right\}
$$

meets every $A$ – $B$ path in $G$ ; then $X$ is an $A$ – $B$ separator on $\mathcal { P }$ .

Suppose there is an $A$ – $B$ path $Q$ that avoids $X$ . We know that $Q$ meets $V [ \mathcal { P } ]$ , as otherwise it would be an alternating walk ending in $B _ { 2 }$ . Now the $A { \mathrm { - } } V [ { \mathcal { P } } ]$ path in $Q$ is either an alternating walk or consists only of the first vertex of some path in $\mathcal { P }$ . Therefore $Q$ also meets the vertex set $V [ \mathcal { P } ^ { \prime } ]$ of

$$
\mathcal {P} ^ {\prime} := \left\{P x _ {P} \mid P \in \mathcal {P} \right\}.
$$

Let $y$ be the last vertex of $Q$ in $V [ \mathcal { P } ^ { \prime } ]$ , say $y \in P \in \mathcal P$ , and let $x : = x _ { P }$ . As $Q$ avoids $X$ and hence $x$ , we have $y \in P { \overset { \circ } { x } }$ . In particular, $x = x _ { P }$ is

![](images/5c552caf11a28691b3d7508622e4f38342cfc36f77f04bfea7c32511164504af.jpg)  
Fig. 3.3.3. Alternating walks in the proof of Lemma 3.3.3.

W

$x ^ { \prime } , W ^ { \prime }$

not the first vertex of $P$ , and so there is an alternating walk $W$ ending at $x$ . Then $W \cup x P y Q$ is a walk from $A _ { 2 }$ to $B$ (Fig. 3.3.3). If this walk alternates and ends in $B _ { 2 }$ , we have our desired contradiction.

How could $W \cup x P y Q$ fail to alternate? For example, $W$ might already use an edge of $x P y$ . But if $x ^ { \prime }$ is the first vertex of $W$ on $x P y$ , then $W ^ { \prime } : = W x ^ { \prime } P y$ is an alternating walk from $A _ { 2 }$ to $y$ . (By $W x ^ { \prime }$ we mean the initial segment of $W$ ending at the first occurrence of $x ^ { \prime }$ on $W$ ; from there, $W ^ { \prime }$ follows $P$ back to $y$ .) Even our new walk $W ^ { \prime } y Q$ need not yet alternate: $W ^ { \prime }$ might still meet $\mathring { y } Q$ . By definition of ${ \mathcal { P } } ^ { \prime }$ and $W$ , however, and the choice of $y$ on $Q$ , we have

$$
V \left(W ^ {\prime}\right) \cap V [ \mathcal {P} ] \subseteq V [ \mathcal {P} ^ {\prime} ] \quad \text {a n d} \quad V (\dot {y} Q) \cap V [ \mathcal {P} ^ {\prime} ] = \emptyset .
$$

Thus, $W ^ { \prime }$ and $\overset { \circ } { y } Q$ can meet only outside $\mathcal { P }$ .

If $W ^ { \prime }$ does indeed meet $\overset { \circ } { y } Q$ , we let $z$ be the first vertex of $W ^ { \prime }$ on $\mathring { y } Q$ and set $W ^ { \prime \prime } : = W ^ { \prime } z Q$ . Otherwise we set $W ^ { \prime \prime } : = W ^ { \prime } \cup y Q$ . In both cases $W ^ { \prime \prime }$ alternates with respect to ${ \mathcal { P } } ^ { \prime }$ , because $W ^ { \prime }$ does and $\overset { \circ } { y } \underset { \pmb { Q } } { Q }$ avoids $V [ \mathcal { P } ^ { \prime } ]$ . ( $W ^ { \prime \prime }$ satisfies condition (iii) at $y$ in the second case even if $y$ occurs twice on $W ^ { \prime }$ , because $W ^ { \prime \prime }$ then contains the entire walk $W ^ { \prime }$ and not just its initial segment $W ^ { \prime } y$ .) By definition of ${ \mathcal { P } } ^ { \prime }$ , therefore, $W ^ { \prime \prime }$ avoids $V [ \mathcal { P } ] \setminus V [ \mathcal { P ^ { \prime } } ]$ . Thus $W ^ { \prime \prime }$ also alternates with respect to $\mathcal { P }$ ， and ends in $B _ { 2 }$ , contrary to our assumptions. 

Third proof of Menger’s theorem. Let $\mathcal { P }$ contain as many disjoint $A$ – $B$ paths in $G$ as possible. Then by Lemma 3.3.2, no alternating walk ends in $B \setminus V [ \mathcal { P } ]$ . By Lemma 3.3.3, this implies that $G$ has an $A$ – $B$ separator $X$ on $\mathcal { P }$ , giving $k \leqslant | X | = | \mathcal { P } |$ as desired. 

fan

A set of $a$ – $B$ paths is called an $a$ – $B$ fan if any two of the paths have only $a$ in common.

[ 10.1.2 ]

Corollary 3.3.4. For $B \subseteq V$ and $a \in V \setminus B$ , the minimum number of vertices $\neq a$ separating $a$ from $B$ in $G$ is equal to the maximum number of paths forming an $a$ – $B$ fan in $G$ .

Proof . Apply Theorem 3.3.1 with $A : = N ( a )$ .

Corollary 3.3.5. Let $a$ and b be two distinct vertices of $G$

(i) $I f a b \notin E$ , then the minimum number of vertices = a, b separating $a$ from $b$ in $G$ is equal to the maximum number of independent $a$ –b paths in $G$ .   
(ii) The minimum number of edges separating $a$ from $b$ in $G$ is equal to the maximum number of edge-disjoint $a$ – $^ { b }$ paths in $G$ .

Proof . (i) Apply Theorem 3.3.1 with $A : = N ( a )$ and $B : = N ( b )$ .

(ii) Apply Theorem 3.3.1 to the line graph of $G$ , with $A : = E ( a )$ and $B : = E ( b )$ . 

# Theorem 3.3.6. (Global Version of Menger’s Theorem)

(i) A graph is $k$ -connected if and only if it contains $k$ independent paths between any two vertices.   
(ii) A graph is $k$ -edge-connected if and only if it contains $k$ edgedisjoint paths between any two vertices.

Proof . (i) If a graph $G$ contains $k$ independent paths between any two vertices, then $| G | > k$ and $G$ cannot be separated by fewer than $k$ vertices; thus, $G$ is $k$ -connected.

Conversely, suppose that $G$ is $k$ -connected (and, in particular, has more than $k$ vertices) but contains vertices $a , b$ not linked by $k$ independent paths. By Corollary 3.3.5 (i), $a$ and $b$ are adjacent; let $G ^ { \prime } : = G - a b$ . Then $G ^ { \prime }$ contains at most $k - 2$ independent $a$ – $\boldsymbol { b }$ paths. By Corollary 3.3.5 (i), we can separate $a$ and $b$ in $G ^ { \prime }$ - by a set $X$ of at most $k - 2$ vertices. As $| G | > k$ , there is at least one further vertex $v \not \in X \cup \{ a , b \}$ in $G$ . Now $X$ separates $v$ in $G ^ { \prime }$ from either $a$ or $b$ —say, from $a$ . But then $X \cup \{ b \}$ is a set of at most $k - 1$ vertices separating $v$ from $a$ in $G$ , contradicting the $k$ -connectedness of $G$ .

(ii) follows straight from Corollary 3.3.5 (ii).

# 3.4 Mader’s theorem

In analogy to Menger’s theorem we may consider the following question: given a graph $G$ with an induced subgraph $H$ , up to how many independent $H$ -paths can we find in $G$ ?

In this section, we present without proof a deep theorem of Mader, which solves the above problem in a fashion similar to Menger’s theorem. Again, the theorem says that an upper bound on the number of such paths that arises naturally from the size of certain separators is indeed attained by some suitable set of paths.

What could such an upper bound look like? Clearly, if $X \subseteq V ( G - H )$ and $F \subseteq E ( G - H )$ are such that every $H$ -path in $G$ has a vertex or an edge in $X \cup F$ , then $G$ cannot contain more than $| X \cup F |$ independent $H$ -paths. Hence, the least cardinality of such a set $X \cup F$ is a natural upper bound for the maximum number of independent $H$ -paths. (Note that every $H$ -path meets $G - H$ , because $H$ is induced in $G$ and edges of $H$ do not count as $H$ -paths.)

In contrast to Menger’s theorem, this bound can still be improved. The minimality of $| X \cup F |$ implies that no edge in $F$ has an end in $X$ : otherwise this edge would not be needed in the separator. Let $Y : =$ $V ( G - H ) \setminus X$ , and denote by $\zeta _ { F }$ the set of components of the graph $( Y , F )$ . Since every $H \cdot$ -path avoiding $X$ contains an edge from $F ^ { \prime }$ , it has

![](images/1b01d0b60e3ac3156e6b63ec7d4de688f6b214058d8c6392e4eb18bef71c543e.jpg)  
Fig. 3.4.1. An $H$ -path in $G - X$

∂C at least two vertices in $\partial C$ for some $C \in { \mathcal { C } } _ { F }$ , where $\partial C$ denotes the set of vertices in $C$ with a neighbour in $G - X - C$ (Fig. 3.4.1). The number of independent $H$ -paths in $G$ is therefore bounded above by

$$
M _ {G} (H) \qquad \qquad M _ {G} (H) := \min  \left(\left| X \right| + \sum_ {C \in \mathcal {C} _ {F}} \left\lfloor \frac {1}{2} \left| \partial C \right| \right\rfloor\right),
$$

$X$ where the minimum is taken over all $X$ and $F$ as described above: $X \subseteq$ $V ( G - H )$ and $F \subseteq E ( G - H - X )$ such that every $H$ -path in $G$ has a vertex or an edge in $X \cup F$ .

Now Mader’s theorem says that this upper bound is always attained by some set of independent $H$ -paths:

# Theorem 3.4.1. (Mader 1978)

Given a graph $G$ with an induced subgraph $H$ , there are always $M _ { G } ( H )$ independent $H$ -paths in $G$ .

$\kappa _ { G } ( H )$

$\lambda _ { G } ( H )$

In order to obtain direct analogues to the vertex and edge version of Menger’s theorem, let us consider the two special cases of the above problem where either $F$ or $X$ is required to be empty. Given an induced subgraph $H \subseteq G$ , we denote by $\kappa _ { G } ( H )$ the least cardinality of a vertex set $X \subseteq V ( G - H )$ that meets every $H$ -path in $G$ . Similarly, we let $\lambda _ { G } ( H )$ denote the least cardinality of an edge set $F \subseteq E ( G )$ that meets every $H$ -path in $G$ .

Corollary 3.4.2. Given a graph $G$ with an induced subgraph $H$ , there are at least $\scriptstyle { \frac { 1 } { 2 } } \kappa _ { G } ( H )$ independent $H$ -paths and at least $\scriptstyle { \frac { 1 } { 2 } } \lambda _ { G } ( H )$ edgedisjoint $H$ -paths in $G$ .

$k$ Proof . To prove the first assertion, let $k$ be the maximum number of independent $H$ -paths in $G$ . By Theorem 3.4.1, there are sets $X \subseteq V ( G - H )$ and $F \subseteq E ( G - H - X )$ with

$$
k = | X | + \sum_ {C \in \mathcal {C} _ {F}} \left\lfloor \frac {1}{2} | \partial C | \right\rfloor
$$

such that every $H$ -path in $G$ has a vertex in $X$ or an edge in $F$ . For every $C \in { \mathcal { C } } _ { F }$ with $\partial C \neq \emptyset$ , pick a vertex $v \in \partial C$ and let $Y _ { C } : = \partial C \setminus \{ v \}$ ; if $\partial C = \emptyset$ , let $Y _ { C } : = \varnothing$ . Then $\begin{array} { r } { \left\lfloor \frac { 1 } { 2 } \left| \partial C \right| \right\rfloor \geqslant \frac { 1 } { 2 } \left| Y _ { C } \right| } \end{array}$ for all $C \in { \mathcal { C } } _ { F }$ . Moreover, for $Y : = \bigcup _ { C \in { \mathcal { C } } _ { F } } Y _ { C }$ every $H$ 2 2 -path has a vertex in $X \cup Y$ . Hence

$$
k \geqslant | X | + \sum_ {C \in \mathcal {C} _ {F}} \frac {1}{2} \left| Y _ {C} \right| \geqslant \frac {1}{2} \left| X \cup Y \right| \geqslant \frac {1}{2} \kappa_ {G} (H)
$$

as claimed.

The second assertion follows from the first by considering the line graph of $G$ (Exercise 18). 

It may come as a surprise to see that the bounds in Corollary 3.4.2 are best possible (as general bounds): one can find examples for $G$ and $H$ where $G$ contains no more than $\scriptstyle { \frac { 1 } { 2 } } \kappa _ { G } ( H )$ independent $H$ -paths or no more than $\scriptstyle { \frac { 1 } { 2 } } \lambda _ { G } ( H )$ edge-disjoint $H$ -paths (Exercises 19 and 20).

# 3.5 Linking pairs of vertices

Let $G$ be a graph, and let $X \subseteq V ( G )$ be a set of vertices. We call $X$ linked in $G$ if whenever we pick distinct vertices $s _ { 1 } , \ldots , s _ { \ell }$ , $t _ { 1 } , \ldots , t _ { \ell }$ in $X$ we can find disjoint paths $P _ { 1 } , \ldots , P _ { \ell }$ in $G$ such that each $P _ { i }$ links $s _ { i }$ to $t _ { i }$ and has no inner vertex in $X$ . Thus, unlike in Menger’s theorem, we are not merely asking for disjoint paths between two sets of vertices: we insist that each of these paths shall link a specified pair of endvertices.

If $| G | \geqslant 2 k$ and every set of at most $2 k$ vertices is linked in $G$ , then $G$ is $k$ -linked . As is easily checked, this is equivalent to requiring that disjoint paths $P _ { i } = s _ { i } \ldots t _ { i }$ exist for every choice of exactly $2 k$ vertices $s _ { 1 } , \ldots , s _ { k }$ , $t _ { 1 } , \ldots , t _ { k }$ . In practice, the latter is easier to prove, because we need not worry about inner vertices in $X$ .

Clearly, every $k$ -linked graph is $k$ -connected. The converse, however, seems far from true: being $k$ -linked is clearly a much stronger property than $k$ -connectedness. Still, we shall prove in this section that we can force a graph to be $k$ -linked by assuming that it is $f ( k )$ -connected, for some function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ . We first give a nice and simple proof that such a function $f$ exists at all. In the remainder of the section we then prove that $f$ can even be chosen linear.

The basic idea in the simple proof is as follows. If we can prove that $G$ contains a subdivision $K$ of a large complete graph, we can use Menger’s theorem to link the vertices of $X$ disjointly to branch vertices of $K$ , and then hope to pair them up as desired through the subdivided edges of $K$ . This requires, of course, that our paths do not hit too many of the subdivided edges before reaching the branch vertices of $K$ .

To show that $K$ exists is a lemma which more properly belongs in Chapter 7, and we shall derive an improved version there from the

linearity theorem (3.5.3) proved later in this section. Instead of assuming high connectivity, it suffices that $G$ has large enough average degree:

Lemma 3.5.1. There is a function $h { : \mathbb { N } \to \mathbb { N } }$ such that every graph of average degree at least $h ( r )$ contains $K ^ { r }$ as a topological minor, for every $r \in \mathbb N$ .

Proof . For $r \leqslant 2$ , the assertion holds with $h ( r ) = 1$ ; we now assume that $r \geqslant 3$ . We show by induction on $m = r , \ldots , \left( { r \atop 2 } \right)$ that every graph $G$ with average degree $d ( G ) \geqslant 2 ^ { m }$ has a topological minor $X$ with $r$ vertices and $m$ edges; for $m = { \binom { r } { 2 } }$ this implies the assertion with $h ( r ) = 2 ^ { \binom { r } { 2 } }$ .

If $m = r$ then, by Propositions 1.2.2 and 1.3.1, $G$ contains a cycle of length at least $\varepsilon ( G ) + 1 \geqslant 2 ^ { r - 1 } + 1 \geqslant r + 1$ , and the assertion follows with $X = C ^ { r }$ .

Now let $\begin{array} { r } { r < m \leqslant \binom { r } { 2 } } \end{array}$ , and assume the assertion holds for smaller $m$ . Let $G$ with $d ( G ) \geqslant 2 ^ { m }$ be given; thus, $\varepsilon ( G ) \geqslant 2 ^ { m - 1 }$ . Since $G$ has a component $C$ with $\varepsilon ( C ) \geqslant \varepsilon ( G )$ , we may assume that $G$ is connected. Consider a maximal set $U \subseteq V ( G )$ such that $U$ is connected in $G$ and $\varepsilon ( G / U ) \geqslant 2 ^ { m - 1 }$ ; such a set $U$ exists, because $G$ itself has the form $G / U$ with $| U | = 1$ . Since $G$ is connected, we have $N ( U ) \neq \emptyset$ .

Let $H : = G \left[ N ( U ) \right]$ . If $H$ has a vertex $v$ of degree $d _ { H } ( v ) < 2 ^ { m - 1 }$ , we may add it to $U$ and obtain a contradiction to the maximality of $U$ : when we contract the edge ${ \boldsymbol { v } } { \boldsymbol { v } } _ { U }$ in $G / U$ , we lose one vertex and $d _ { H } ( v ) + 1 \leqslant$ $2 ^ { m - 1 }$ edges, so $\varepsilon$ will still be at least $2 ^ { m - 1 }$ . Therefore $d ( H ) \geqslant \delta ( H ) \geqslant$ $2 ^ { m - 1 }$ . By the induction hypothesis, $H$ contains a $T Y$ with $| Y | = r$ and $\| Y \| = m - 1$ . Let $x , y$ be two branch vertices of this $T Y$ that are non-adjacent in $Y$ . Since $x$ and $y$ lie in $N ( U )$ and $U$ is connected in $G$ , $G$ contains an $x$ – $y$ path whose inner vertices lie in $U$ . Adding this path to the $T Y$ , we obtain the desired $_ { T X }$ . 

# Theorem 3.5.2. (Jung 1970; Larman & Mani 1970)

There is a function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ such that every $f ( k )$ -connected graph is $k$ -linked, for all $k \in \mathbb N$ .

Proof . We prove the assertion for $f ( k ) = h ( 3 k ) + 2 k$ , where $h$ is a function as in Lemma 3.5.1. Let $G$ be an $f ( k )$ -connected graph. Then $d ( G ) \geqslant \delta ( G ) \geqslant \kappa ( G ) \geqslant h ( 3 k )$ ; choose $K = T K ^ { 3 k } \subseteq G$ as in Lemma 3.5.1, and let $U$ denote its set of branch vertices.

For the proof that $G$ is $k$ -linked, let distinct vertices $s _ { 1 } , \ldots , s _ { k }$ and $t _ { 1 } , \ldots , t _ { k }$ be given. By definition of $f ( k )$ , we have $\kappa ( G ) ~ \geqslant ~ 2 k$ . Hence by Menger’s theorem (3.3.1), $G$ contains disjoint paths $P _ { 1 } , \ldots , P _ { k }$ , $Q _ { 1 } , \ldots , Q _ { k }$ , such that each $P _ { i }$ starts in $s _ { i }$ , each $Q _ { i }$ starts in $t _ { i }$ , and all these paths end in $U$ but have no inner vertices in $U$ . Let the set $\mathcal { P }$ of these paths be chosen so that their total number of edges outside $E ( K )$ is as small as possible.

Let $u _ { 1 } , \ldots , u _ { k }$ be those $k$ vertices in $U$ that are not an end of a path in $\mathcal { P }$ . For each $i = 1 , \ldots , k$ , let $L _ { i }$ be the $U$ -path in $K$ (i.e., the subdivided edge of the $K ^ { 3 k }$ ) from $u _ { i }$ to the end of $P _ { i }$ in $U$ , and let $v _ { i }$ be the first vertex of $L _ { i }$ on any path $P \in \mathcal { P }$ . By definition of $\mathcal { P }$ , $P$ has no more edges outside $E ( K )$ than $P v _ { i } L _ { i } u _ { i }$ does, so $v _ { i } P = v _ { i } L _ { i }$ and hence $P = P _ { i }$ (Fig. 3.5.1). Similarly, if $M _ { i }$ denotes the $U$ -path in $K$ from $u _ { i }$ to the end of $Q _ { i }$ in $U$ , and $w _ { i }$ denotes the first vertex of $M _ { i }$ on any path in $\mathcal { P }$ , then this path is $Q _ { i }$ . Then the paths $s _ { i } P _ { i } v _ { i } L _ { i } u _ { i } M _ { i } w _ { i } Q _ { i } t _ { i }$ are disjoint for different $i$ and show that $G$ is $k$ -linked. 

![](images/ebfdc67466c538d71f50a64749396c04223355e6fff0d868978e230844e5908b.jpg)  
Fig. 3.5.1. Constructing an $s _ { i } - t _ { i }$ path via $u _ { i }$

The proof of Theorem 3.5.2 yields only an exponential upper bound for the function $f ( k )$ . As $2 \varepsilon ( G ) \geqslant \delta ( G ) \geqslant \kappa ( G )$ , the following result implies the linear bound of $f ( k ) = 1 6 k$ :

# Theorem 3.5.3. (Thomas & Wollan 2005)

Let $G$ be a graph and $k \in \mathbb N$ . If $G$ is $2 k$ -connected and $\varepsilon ( G ) \geqslant 8 k$ , then $G$ is $k$ -linked.

We begin our proof of Theorem 3.5.3 with a lemma.

Lemma 3.5.4. If $\delta ( G ) \geqslant 8 k$ and $| G | \leqslant 1 6 k$ , then $G$ has a $k$ -linked subgraph.

Proof . If $G$ itself is $k$ -linked there is nothing to show, so suppose not. Then we can find a set $X$ of $2 k$ vertices $s _ { 1 } , \ldots , s _ { k } , t _ { 1 } , \ldots , t _ { k }$ that cannot be linked in $G$ by disjoint paths $P _ { i } = s _ { i } \ldots t _ { i }$ . Let $\mathcal { P }$ be a set of as many such paths as possible, but all of length at most 7. If there are several such sets $\mathcal { P }$ , we choose one with $| \cup \mathcal { P } |$ minimum. We may assume that $\mathcal { P }$ contains no path from $s _ { 1 }$ to $t _ { 1 }$ . Let $J$ be the subgraph of $G$ induced by $X$ and all the vertices on the paths in $\mathcal { P }$ , and let $H : = G - J$ .

Note that each vertex $v \in H$ has at most three neighbours on any given $P _ { i } \in \mathcal { P }$ : if it had four, then replacing the segment $u P _ { i } w$ between its first and its last neighbour on $P _ { i }$ by the path uvw would reduce $| \cup \mathcal { P } |$ and thus contradict our choice of $\mathcal { P }$ . Moreover, $\boldsymbol { v }$ is not adjacent to both $s _ { i }$ and $t _ { i }$ whenever $s _ { i } , t _ { i } \notin \bigcup \mathcal { P }$ , by the maximality of $\mathcal { P }$ . Thus if $| { \mathcal { P } } | = : h$ , then $v$ has at most $3 h + ( 2 k - 2 h ) / 2 \leqslant 3 k$ neighbours in $J$ . As $\delta ( G ) \geqslant 8 k$

[ 7.2.1 ]

$X$ $_ { \mathcal { P } }$

$s _ { 1 }$ , t1 $J , H$

and $| G | \leqslant 1 6 k$ by assumption, while $| X | = 2 k$ , we deduce that

$$
\delta (H) \geqslant 5 k \quad \text {a n d} \quad | H | \leqslant 1 4 k. \tag {1}
$$

Our next aim is to show that $H$ is disconnected. Since each of the paths in $\mathcal { P }$ has at most eight vertices, we have $| J - \{ s _ { 1 } , t _ { 1 } \} | \leqslant 8 ( k - 1 )$ . Therefore both $s _ { 1 }$ and $t _ { 1 }$ have neighbours in $H$ . Let $S \subseteq V ( H )$ be the set of vertices at distance at most 2 (measured in $H$ ) from a neighbour of $s _ { 1 }$ in $H$ , and let $T \subseteq V ( H )$ be the corresponding set for $t _ { 1 }$ . Since $G - \cup \mathcal { P }$ contains no $s _ { 1 } - t _ { 1 }$ path of length at most 7, we have $S \cap T = \emptyset$ and there is no $S$ – $T$ edge in $H$ . To prove that $H$ is disconnected, it thus suffices to show that $V ( H ) = S \cup T$ . Pick a neighbour $s \in S$ of $s _ { 1 }$ , and a neighbour $t \in T$ of $t _ { 1 }$ . Then for any vertex $v \in H - ( S \cup T )$ the sets $N _ { H } ( s )$ , $N _ { H } ( t )$ and $N _ { H } ( v )$ are disjoint and each have size at least $5 k$ , contradicting (1).

So $H$ is disconnected; let $C$ be its smallest component. By (1),

$$
2 \delta (C) \geqslant 2 \delta (H) \geqslant 7 k + 3 k \geqslant \frac {1}{2} | H | + 3 k \geqslant | C | + 3 k. \tag {2}
$$

We complete the proof by showing that $C$ is $k$ -linked. As $\delta ( C ) \geqslant 5 k$ , we have $| C | \geqslant 2 k$ . Let $Y$ be a set of at most $2 k$ vertices in $C$ . By (2), every two vertices in $Y$ have at least $3 k$ common neighbours, at least $k$ of which lie outside $Y$ . We can therefore link any desired $\ell \leqslant k$ pairs of vertices in $Y$ inductively by paths of length 2 whose inner vertex lies outside $Y$ . 

Before we launch into the proof of Theorem 3.5.3, let us look at its main ideas. To prove that $G$ is $k$ -linked, we have to consider a given set $X$ of up to $2 k$ vertices and show that $X$ is linked in $G$ . Ideally, we would like to use Lemma 3.5.4 to find a linked subgraph $L$ somewhere in $G$ , and then use our assumption of $\kappa ( G ) \geqslant 2 k$ to obtain a set of $| X |$ disjoint $X$ – $L$ paths by Menger’s theorem (3.3.1). Then $X$ could be linked via these paths and $L$ , completing the proof.

Unfortunately, we cannot expect to find a subgraph $H$ such that $\delta ( H ) \geqslant 8 k$ and $| H | \leqslant 1 6 k$ (in which $L$ could be found by Lemma 3.5.4); cf. Corollary 11.2.3. However, it is not too difficult to find a minor $H \preccurlyeq G$ that has such a subgraph (Ex. 22, Ch. 7), even so that the vertices of $X$ come to lie in distinct branch sets of $H$ . We may then regard $X$ as a subset of $V ( H )$ , and Lemma 3.5.4 provides us with a linked subgraph $L$ of $H$ . The only problem now is that $H$ need no longer be $2 k$ -connected, that is, our assumption of $\kappa ( G ) \geqslant 2 k$ will not ensure that we can link $X$ to $L$ by $| X |$ disjoint paths in $H$ .

And here comes the clever bit of the proof: it relaxes the assumption of $\kappa \geqslant 2 k$ to a weaker assumption that does get passed on to $H$ . This weaker assumption is that $i f$ we can separate $X$ from some other

subgraph by fewer than $| X |$ vertices, then this other part must be ‘light’: roughly, its own value of $\varepsilon$ must not exceed $8 k$ . Now if we fail to link $X$ to $L$ by $| X |$ disjoint paths, then $H$ has such a separation $\{ A , B \}$ , with $X \subseteq A$ and $L \subseteq B$ and such that $| A \cap B | < | X |$ . If we choose this with $| A \cap B |$ minimum, then by Menger’s theorem we can link $A \cap B$ to $L$ in $H \left[ B \right]$ by $| A \cap B |$ disjoint paths. We may then continue our proof inside $H \left[ A \right]$ , whose value of $\varepsilon$ is still as big as before, because the $B$ -part of $H$ was ‘light’. In fact, we may even turn $A \cap B$ into a complete subgraph of $H \left[ A \right]$ , because such new edges, if used by our linking paths, can be replaced by paths through $B$ and $L$ . This helps ensure that we do not in $H \left[ A \right]$ have new separations of order less than $| X |$ that split a ‘heavy’ part away from $X$ . Hence, both our inductive assumptions—the value of $\varepsilon \geqslant 8 k$ and the fact that small separators can only split light parts away from $X$ —hold for $H \left[ A \right]$ because they did in $H$ . This will complete the inductive proof.

Given $k \in \mathbb N$ , a graph $G$ , and $A , B , X \subseteq V ( G )$ , call the ordered pair $( A , B )$ an $X$ - separation of $G$ if $\{ A , B \}$ is a proper separation of $G$ of order at most $| X |$ and $X \subseteq A$ . An $X$ - separation $( A , B )$ is small if $| A \cap B | < | X |$ , and linked if $A \cap B$ is linked in $G \left[ B \right]$ .

Call a set $U \subseteq V ( G )$ light in $G$ if $\lVert U \rVert ^ { + } \leqslant 8 k \left| U \right|$ , where $\lVert U \rVert ^ { + }$ denotes the number of edges of $G$ with at least one end in $U$ . A set of vertices is heavy if it is not light.

Proof of Theorem 3.5.3. We shall prove the following, for fixed $k \in \mathbb N$ :

Let $G = ( V , E )$ be a graph and $X \subseteq V$ a set of at most $2 k$ vertices. If $V \setminus X$ is heavy and for every small $X$ - separ- ( ) ation $( A , B )$ the set $B \setminus A$ is light, then $X$ is linked in $G$ .

To see that ( ) implies the theorem, assume that $\kappa ( G ) ~ \geqslant ~ 2 k$ and $\varepsilon ( G ) \geqslant 8 k$ , and let $X$ be a set of exactly $2 k$ vertices. Then $G$ has no small $X$ - separation. And $V \setminus X$ is heavy, since

$$
\| V \setminus X \| ^ {+} \geqslant \| G \| - \binom {2 k} {2} \geqslant 8 k | V | - \binom {2 k} {2} > 8 k | V \setminus X |.
$$

By ( ), $X$ is linked in $G$ , completing the proof that $G$ is $k$ -linked.

We prove ( ) by induction on $| G |$ , and for each value of $| G |$ by induction on $\| V \setminus X \| ^ { + }$ . If $| G | = 1$ then $X$ is linked in $G$ . For the induction step, let $G$ and $X$ be given as in (∗). We first prove the following:

We may assume that $G$ has no linked $X$ - separation. (1)

For our proof of (1), suppose that $G$ has a linked $X$ - separation $( A , B )$ $( A , B )$ . Let us choose one with $A$ minimal, and put $S : = A \cap B$ . $S$

We first consider the case that $| S | = | X |$ . If $G [ A ]$ contains $| X |$ disjoint $X$ – $S$ paths, then $X$ is linked in $G$ because $( A , B )$ is linked, completing the proof of $( * )$ . If not, then by Menger’s theorem (3.3.1) $G \left[ A \right]$ has a small $X$ - separation $( A ^ { \prime } , B ^ { \prime } )$ such that $B ^ { \prime } \supseteq S$ . If we choose this with $| A ^ { \prime } \cap B ^ { \prime } |$ minimum, we can link $A ^ { \prime } \cap B ^ { \prime }$ to $S$ in $G [ B ^ { \prime } ]$ by $| A ^ { \prime } \cap B ^ { \prime } |$ disjoint paths, again by Menger’s theorem. But then $( A ^ { \prime } , B ^ { \prime } \cup B )$ is a linked $X$ - separation of $G$ that contradicts the choice of $( A , B )$ .

So $| S | < | X |$ . Let $G ^ { \prime }$ be obtained from $G \left[ A \right]$ by adding any missing edges on $S$ , so that $G ^ { \prime } \left[ S \right]$ is a complete subgraph of $G ^ { \prime }$ . As $( A , B )$ is now a small $X$ - separation, our assumption in ( ) says that $B \setminus A$ is light in $G$ . Thus, $G ^ { \prime }$ arises from $G$ by deleting $| B \setminus A |$ vertices outside $X$ and at most $8 k | B \setminus A |$ edges, and possibly adding some edges. As $V \setminus X$ is heavy in $G$ , this implies that

$$
A \setminus X \text {i s h e a v y i n} G ^ {\prime}.
$$

$( A ^ { \prime } , B ^ { \prime } )$

In order to be able to apply the induction hypothesis to $G ^ { \prime }$ , let us show next that for every small $X$ - separation $( A ^ { \prime } , B ^ { \prime } )$ of $G ^ { \prime }$ the set $B ^ { \prime } \setminus A ^ { \prime }$ is light in $G ^ { \prime }$ . Suppose not, and choose a counterexample $( A ^ { \prime } , B ^ { \prime } )$ with $B ^ { \prime }$ minimal. As $G ^ { \prime } \left[ S \right]$ is complete, we have $S \subseteq A ^ { \prime }$ or $S \subseteq B ^ { \prime }$ .

If $S \subseteq A ^ { \prime }$ then $B ^ { \prime } \cap B \subseteq S \subseteq A ^ { \prime }$ , so $( A ^ { \prime } \cup B , B ^ { \prime } )$ is a small $X$ - separation of $G$ . Moreover,

$$
B ^ {\prime} \setminus \left(A ^ {\prime} \cup B\right) = B ^ {\prime} \setminus A ^ {\prime},
$$

and no edge of $G ^ { \prime } - E$ is incident with this set (Fig 3.5.2). Our assumption that this set is heavy in $G ^ { \prime }$ , by the choice of $( A ^ { \prime } , B ^ { \prime } )$ , therefore implies that it is heavy also in $G$ . As $( A ^ { \prime } \cup B , B ^ { \prime } )$ is a small $X$ - separation of $G$ , this contradicts our assumptions in ( ).

![](images/7df2f557a9bcb97f02df5bb597670650d932bfeac39def31a8ecab5a5ceb0b74.jpg)  
Fig. 3.5.2. If $S \subseteq A ^ { \prime }$ , then $( A ^ { \prime } \cup B , B ^ { \prime } )$ is an $X$ - separation of $G$

Hence $S \subseteq B ^ { \prime }$ . By our choice of $( A ^ { \prime } , B ^ { \prime } )$ , the graph $G ^ { \prime \prime } : = G ^ { \prime } \left[ B ^ { \prime } \right]$ satisfies the premise of ( ) for $X ^ { \prime \prime } : = A ^ { \prime } \cap B ^ { \prime }$ . By the induction hypothesis, $X ^ { \prime \prime }$ is linked in $G ^ { \prime \prime }$ . But then $X ^ { \prime \prime }$ is also linked in $G \left[ B ^ { \prime } \cup B \right]$ :

as $S$ was linked in $G \left[ B \right]$ , we simply replace any edges added on $S$ in the definition of $G ^ { \prime }$ by disjoint paths through $B$ (Fig. 3.5.3). But now $( A ^ { \prime } , B ^ { \prime } \cup B )$ is a linked $X$ - separation of $G$ that violates the minimality of $A$ in the choice of $( A , B )$ .

![](images/c1d38318382c7aecd2756a0b3cd73526d38eb1da94fc3749cba6fd01cb84e31d.jpg)  
Fig. 3.5.3. If $S \subseteq B ^ { \prime }$ , then $( A ^ { \prime } , B ^ { \prime } \cup B )$ is linked in $G$

We have thus shown that $G ^ { \prime }$ satisfies the premise of ( $^ *$ ) with respect to $X$ . Since $\{ A , B \}$ is a proper separation, $G ^ { \prime }$ has fewer vertices than $G$ . By the induction hypothesis, therefore, $X$ is linked in $G ^ { \prime }$ . Replacing edges of $G ^ { \prime } - E$ on $S$ by paths through $B$ as before, we can turn any linkage of $X$ in $G ^ { \prime }$ into one in $G$ , completing the proof of $( * )$ . This completes the proof of (1).

Our next goal is to show that, by the induction hypothesis, we may assume that $G$ has not only large average degree but even large minimum degree. For our proof that $X$ is linked in $G$ , let $s _ { 1 } , \ldots , s _ { \ell } , t _ { 1 } , \ldots , t _ { \ell }$ be the distinct vertices in $X$ which we wish to link by disjoint paths $P _ { i } = s _ { i } \ldots t _ { i }$ . Since these paths must not have any inner vertices in $X$ , we may assume that $G$ has all edges on $X$ except possibly the edges $s _ { i } t _ { i }$ : as no other edges on $X$ may be used by the paths $P _ { i }$ , we may add them without affecting either the premise or the conclusion in $( * )$ .

After this modification, we can now prove the following:

We may assume at any two ad t vertices $u , v$ which (2) $X$ $8 k - 1$

To prove (2), let $e = u v$ be such an edge, let $n$ denote the number of common neighbours of $u$ and $v$ , and let $G ^ { \prime } : = G / e$ be the graph obtained by contracting $e$ . Since $u , v$ are not both in $X$ we may view $X$ as a subset also of $V ^ { \prime } : = V ( G ^ { \prime } )$ , replacing $u$ or $v$ in $X$ with the contracted vertex $v _ { e }$ if $X \cap \{ u , v \} \neq \emptyset$ . Our aim is to show that unless $n \geqslant 8 k - 1$ as desired in (2), $G ^ { \prime }$ satisfies the premise of $( * )$ . Then $X$ will be linked in $G ^ { \prime }$ by the induction hypothesis, so the desired paths $P _ { 1 } , \ldots , P _ { \ell }$ exist in $G ^ { \prime }$ . If one of them contains $v _ { e }$ , replacing $v _ { e }$ by $u$ or $v$ or $_ { u v }$ turns it into a path in $G$ , completing the proof of ( ).

$G \left[ X \right]$

e = uv n $G ^ { \prime }$ $V ^ { \prime }$

$( A ^ { \prime } , B ^ { \prime } )$

$X ^ { \prime }$

$X ^ { \prime \prime }$

$d ^ { * }$

In order to show that $G ^ { \prime }$ satisfies the premise of $( * )$ with respect to $X$ , let us show first that $V ^ { \prime } \setminus X$ is heavy. Since $V \setminus X$ was heavy and $| V ^ { \prime } \setminus X | = | V \setminus X | - 1$ , it suffices to show that the contraction of $e$ resulted in the loss of at most $8 k$ edges incident with a vertex outside $X$ . If $u$ and $v$ are both outside $X$ , then the number of such edges lost is only $n + 1$ : one edge at every common neighbour of $u$ and $v$ , as well as $e$ . But if $u \in X$ , then $v \not \in X$ , and we lost all the $X$ – $\boldsymbol { v }$ edges $x v$ of $G$ , too: while $x v$ counted towards $\| V \setminus X \| ^ { + }$ , the edge $x v _ { e }$ lies in $G ^ { \prime } \left[ X \right]$ and does not count towards $\| V ^ { \prime } \setminus X \| ^ { + }$ . If $x \neq u$ and $x$ is not a common neighbour of $u$ and $v$ , then this is an additional loss. But $u$ is adjacent to every $x \in X \setminus \{ u \}$ except at most one (by our assumption about $G \left[ X \right]$ ), so every such $x$ except at most one is in fact a common neighbour of $u$ and $v$ . Thus in total, we lost at most $n + 2$ edges. Unless $n \geqslant 8 k - 1$ (which would prove (2) directly for $u$ and $\boldsymbol { v }$ ), this means that we lost at most $8 k$ edges, as desired for our proof that $V ^ { \prime } \setminus X$ is heavy.

It remains to show that for every small $X$ - separation $( A ^ { \prime } , B ^ { \prime } )$ of $G ^ { \prime }$ the set $B ^ { \prime } \setminus A ^ { \prime }$ is light. Let $( A ^ { \prime } , B ^ { \prime } )$ be a counterexample, chosen with $B ^ { \prime }$ minimal. Then $G ^ { \prime } \left[ B ^ { \prime } \right]$ satisfies the premise of ( ) with respect to $X ^ { \prime } : = A ^ { \prime } \cap B ^ { \prime }$ , so $X ^ { \prime }$ is linked in $G ^ { \prime } \left[ B ^ { \prime } \right]$ by induction. Let $A$ and $B$ be obtained from $A ^ { \prime }$ and $B ^ { \prime }$ by replacing $v _ { e }$ , where applicable, with both $u$ and $v$ . We may assume that $u , v \in B$ , since otherwise $( A , B )$ is a small $X$ - separation of $G$ with $B \setminus A$ heavy, contradicting our assumptions in $( * )$ . We shall prove that $X ^ { \prime \prime } : = A \cap B$ is linked in $G [ B ]$ ; then $( A , B )$ is a linked $X$ - separation of $G$ , which contradicts (1).

If $v _ { e } \notin X ^ { \prime }$ , then $u , v \in B \setminus A$ . Now $X ^ { \prime \prime }$ is linked in $G [ B ]$ because $X ^ { \prime }$ is linked in $G ^ { \prime } \left[ B ^ { \prime } \right]$ : if $v _ { e }$ occurs on one of the linking paths for $X ^ { \prime }$ , just replace it by $u$ or $v$ or $u v$ as earlier.

Now assume that $v _ { e } \in X ^ { \prime }$ . Our aim is to show that $G \left[ B \right]$ satisfies the premise of ( ) with respect to $X ^ { \prime \prime }$ , so that $X ^ { \prime \prime }$ is linked in $G \left[ B \right]$ ] by induction. We know that $B \setminus X ^ { \prime \prime } = B ^ { \prime } \setminus A ^ { \prime }$ is heavy in $G$ , since it is heavy in $G ^ { \prime }$ by the choice of $( A ^ { \prime } , B ^ { \prime } )$ . Consider a small $X ^ { \prime \prime }$ - separation $( A ^ { \prime \prime } , B ^ { \prime \prime } )$ of $G \left[ B \right]$ . Then $( A \cup A ^ { \prime \prime } , B ^ { \prime \prime } )$ is a small $X$ - separation of $G$ , so $B ^ { \prime \prime } \setminus A ^ { \prime \prime } = B ^ { \prime \prime } \setminus \left( A \cup A ^ { \prime \prime } \right)$ is light by the assumption in $( * )$ . This completes the proof that $X ^ { \prime \prime }$ is linked in $G \left[ B \right]$ , and hence the proof of (2).

Using induction by contracting an edge, we have just shown that the vertices in $V \setminus X$ may be assumed to have large degree. Using induction by deleting an edge, we now show that their degrees cannot be too large. Since ( ) holds if $V = X$ , we may assume that $V \setminus X \neq \emptyset$ ; let $d ^ { * }$ denote the smallest degree in $G$ of a vertex in $V \setminus X$ . Let us prove that

$$
8 k \leqslant d ^ {*} \leqslant 1 6 k - 1. \tag {3}
$$

The lower bound in (3) follows from (2) if we assume that $G$ has no isolated vertex outside $X$ , which we may clearly assume by induction. To prove the upper bound, let us see what happens if we delete an edge

$e$ whose ends $u , v$ are not both in $X$ . If $G - e$ satisfies the premise of $( * )$ with respect to $X$ , then $X$ is linked in $G - e$ by induction, and hence in $G$ . If not, then either $V \setminus X$ is light in $G - e$ , or $G - e$ has a small $X$ - separation $( A , B )$ such that $B \setminus A$ is heavy. If the latter happens then $e$ must be an $( A \setminus B )  – ( B \setminus A )$ edge: otherwise, $( A , B )$ would be a small $X$ - separation also of $G$ , and $B \setminus A$ would be heavy also in $G$ , in contradiction to our assumptions in $( * )$ . But if $e$ is such an edge then any common neighbours of $u$ and $v$ lie in $A \cap B$ , so there are fewer than $| X | \leqslant 2 k$ such neighbours. This contradicts (2).

So $V \setminus X$ must be light in $G - e$ . For $G$ , this yields

$$
\left\| V \setminus X \right\| ^ {+} \leqslant 8 k | V \setminus X | + 1. \tag {4}
$$

In order to show that this implies the desired upper bound for $d ^ { * }$ , let us estimate the number $f ( x )$ of edges that a vertex $x \in X$ sends to $V \setminus X$ . There must be at least one such edge, $x y$ say, as otherwise $( X , V \setminus \{ x \} )$ would be a small $X$ - separation of $G$ that contradicts our assumptions in $^ *$ ). But then, by (2), $x$ and $y$ have at least $8 k - 1$ common neighbours, at most $2 k - 1$ of which lie in $X$ . Hence $f ( x ) \geqslant 6 k$ . As

$$
2 \left\| V \setminus X \right\| ^ {+} = \sum_ {v \in V \setminus X} d _ {G} (v) + \sum_ {x \in X} f (x),
$$

an assumption of $d ^ { * } \geqslant 1 6 k$ would thus imply that

$$
2 \left(8 k | V \smallsetminus X | + 1\right) \geqslant 2 \| V \smallsetminus X \| ^ {+} \geqslant 1 6 k | V \smallsetminus X | + 6 k | X |, \tag {4}
$$

yielding the contradiction of $2 \geqslant 6 k | X |$ . This completes the proof of (3).

To complete our proof of (∗), pick a vertex $v _ { 0 } \in V \setminus X$ of degree $d ^ { * }$ , and consider the subgraph $H$ induced in $G$ by $v _ { 0 }$ and its neighbours. By (2) we have $\delta ( H ) \geqslant 8 k$ , and by (3) and the choice of $v _ { 0 }$ we have $| H | \leqslant 1 6 k$ . By Lemma 3.5.4, then, $H$ has a $k$ -linked subgraph; let $L$ be its vertex set. By definition of $k$ -linked’, we have $| L | \geqslant 2 k \geqslant | X |$ . If $G$ contains $| X |$ disjoint $X$ – $L$ paths, then $X$ is linked in $G$ , as desired. If not, then $G$ has a small $X$ - separation $( A , B )$ with $L \subseteq B$ . If we choose $( A , B )$ of minimum order, then $G [ B ]$ contains $| A \cap B |$ disjoint $( A \cap B )$ – $L$ paths by Menger’s theorem (3.3.1). But then $( A , B )$ is a linked $X$ - separation that contradicts (1). 

# Exercises

For the first three exercises let $G$ be a graph with vertices $a$ and $b$ , and let $X \subseteq V ( G ) \setminus \{ a , b \}$ be an $a$ – $^ b$ separator in $G$ .

1. $-$ Show that $X$ is minimal as an $a$ – $b$ separator if and only if every vertex in $X$ has a neighbour in the component $C _ { a }$ of $G - X$ containing $a$ , and another in the component $C _ { b }$ of $G - X$ containing $^ { b }$ .   
2. Let $X ^ { \prime } \subseteq V ( G ) \setminus \{ a , b \}$ be another $a$ – $b$ separator, and define $C _ { a } ^ { \prime }$ and $C _ { b } ^ { \prime }$ correspondingly. Show that both

and

$$
Y _ {a} := \left(X \cap C _ {a} ^ {\prime}\right) \cup \left(X \cap X ^ {\prime}\right) \cup \left(X ^ {\prime} \cap C _ {a}\right)
$$

$$
Y _ {b} := \left(X \cap C _ {b} ^ {\prime}\right) \cup \left(X \cap X ^ {\prime}\right) \cup \left(X ^ {\prime} \cap C _ {b}\right)
$$

separate $a$ from $b$ (see figure).

![](images/e57ac7f9b7bfe729d2b4a5fe16b9c795bf20975c6e3c55082f92e51312c0df83.jpg)

3. Are $Y _ { a }$ and $Y _ { b }$ minimal $a$ – $^ b$ separators if $X$ and $X ^ { \prime }$ are? Are $| Y _ { a } |$ and $| Y _ { b } |$ minimal for $a$ – $^ { \textit { b } }$ separators from $V ( G ) \setminus \{ a , b \}$ if $| X |$ and $| X ^ { \prime } |$ are?   
4. Let $X$ and $X ^ { \prime }$ be minimal separators in $G$ such that $X$ meets at least two components of $G - X ^ { \prime }$ . Show that $X ^ { \prime }$ meets all the components of $G - X$ , and that $X$ meets all the components of $G - X ^ { \prime }$ .   
5.− Prove the elementary properties of blocks mentioned at the beginning of Section 3.1.   
6. Show that the block graph of any connected graph is a tree.   
7. Show, without using Menger’s theorem, that any two vertices of a 2- connected graph lie on a common cycle.   
8. For edges $\boldsymbol { e } , \boldsymbol { e } ^ { \prime } \in G$ write $e \sim e ^ { \prime }$ if either $e = e ^ { \prime }$ o r $e$ and $e ^ { \prime }$ lie on some common cycle in $G$ . Show that $\sim$ is an equivalence relation on $E ( G )$ whose equivalence classes are the edge sets of the non-trivial blocks of $G$ .   
9. Let $G$ be a 2-connected graph but not a triangle, and let $e$ be an edge of $G$ . Show that either $G - e$ or $G / e$ is again 2-connected. Deduce a constructive characterization of the 2-connected graphs analogous to Theorem 3.2.2.   
10. Let $G$ be a 3-connected graph, and let $x y$ be an edge of $G$ . Show that $G / x y$ is 3-connected if and only if $G - \{ x , y \}$ is 2-connected.

11. $( \mathrm { i } ) ^ { - }$ Show that every cubic 3-edge-connected graph is 3-connected.   
(ii) Show that a graph is cubic and 3-connected if and only if it can be constructed from a $K ^ { 4 }$ by successive applications of the following operation: subdivide two edges by inserting a new vertex on each of them, and join the two new subdividing vertices by an edge.   
12.+ Find a finite set of 3-connected graphs from which all 3-connected graphs can be constructed iteratively by the following operation, or show that no such set exists. The operation consists of adding a new vertex to the graph $H$ constructed so far and joining it by at least three edges to some subdivision of $H$ . (In other words, every new edge is either incident with a vertex of $H$ or else with a new subdividing vertex of $H$ created for this purpose, and the new edges should obviously not all go to the same subdivided edge of $H$ including its ends.)   
13. Find the error in the following ‘simple proof’ of Menger’s theorem (3.3.1). Let $X$ be an $A$ – $B$ separator of minimum size. Denote by $G A$ the subgraph of $G$ induced by $X$ and all the components of $G - X$ that meet $A$ , and define $G _ { B }$ correspondingly. By the minimality of $X$ , there can be no $A$ – $X$ separator in $G _ { A }$ with fewer than $\vert X \vert$ vertices, so $G _ { A }$ contains $k$ disjoint $A$ – $X$ paths by induction. Similarly, $G _ { B }$ contains $k$ disjoint $X$ – $B$ paths. Together, all these paths form the desired $A$ – $B$ 2 paths in $G$ .   
14. Prove Menger’s theorem by induction on $\| G \|$ , as follows. Given an edge $e = x y$ , consider a smallest $A$ –B separator $S$ in $G - e$ . Show that the induction hypothesis implies a solution for $G$ unless $S \cup \{ x \}$ and $S \cup \{ y \}$ are smallest $A$ – $B$ separators in $G$ . Then show that if choosing neither of these separators as $X$ in the previous exercise gives a valid proof, there is only one easy case left to do.   
15. Work out the details of the proof of Corollary 3.3.5 (ii).   
16. Let $k \geqslant 2$ . Show that every $k$ -connected graph of order at least $2 k$ contains a cycle of length at least $2 k$ .   
17. Let $k \geqslant 2$ . Show that in a $k$ -connected graph any $k$ vertices lie on a common cycle.   
18. Derive the edge part of Corollary 3.4.2 from the vertex part. (Hint. Consider the $H$ -paths in the graph obtained from the disjoint union of $H$ and the line graph $L ( G )$ by adding all the edges $h e$ such that $h$ is a vertex of $H$ and $e \in E ( G ) \setminus E ( H )$ is an edge at $h$ .)   
19.− To the disjoint union of the graph $H = \overline { { K ^ { 2 m + 1 } } }$ with $k$ copies of $K ^ { 2 m + 1 }$ add edges joining $H$ bijectively to each of the $K ^ { 2 m + 1 }$ . Show that the resulting graph $G$ contains at most $k m = \textstyle { \frac { 1 } { 2 } } \kappa _ { G } ( H )$ independent $H$ - paths.   
20. Find a bipartite graph $G$ , with partition classes $A$ and $B$ say, such that for $H : = G \left[ A \right]$ there are at most $\scriptstyle { \frac { 1 } { 2 } } \lambda _ { G } ( H )$ edge-disjoint $H$ -paths in $G$ .

21. $^ +$ Derive Tutte’s 1-factor theorem (2.2.1) from Mader’s theorem.

(Hint. Extend the given graph $G$ to a graph $G ^ { \prime }$  by adding, for each vertex $v \in G$ , a new vertex $v ^ { \prime }$  and joining $v ^ { \prime }$ to $v$ . Choose $H \subseteq G ^ { \prime }$ so that the 1-factors in $G$ correspond to the large enough sets of independent $H$ -paths in $G ^ { \prime }$ .)

22. $-$ Show that $k$ -linked graphs are $( 2 k - 1 )$ -connected. Are they even $2 k$ - connected?

23. For every $k \in \mathbb N$ find an $\ell = \ell ( k )$ , as large as possible, such that not every $\ell$ -connected graph is $k$ -linked.

24. Show that if $G$ is $k$ -linked and $s _ { 1 } , \ldots , s _ { k } , t _ { 1 } , \ldots , t _ { k }$ are not necessarily distinct vertices such that $s _ { i } \neq t _ { i }$ for all $_ i$ , then $G$ contains independent paths $P _ { i } = s _ { i } \ldots t _ { i }$ for $i = 1 , \ldots , k$ .

25. Use Theorem 3.5.3 to show that the function $h$ in Lemma 3.5.1 can be chosen as $h ( r ) = c r ^ { 2 }$ , for some $c \in \mathbb { N }$ .

# Notes

Although connectivity theorems are doubtless among the most natural, and also the most applicable, results in graph theory, there is still no monograph on this subject. The most comprehensive source is perhaps A. Schrijver, Combinatorial optimization, Springer 2003, together with a number of surveys on specific topics by A. Frank, to be found on his home page. Some areas are covered in B. Bollob´as, Extremal Graph Theory, Academic Press 1978, in R. Halin, Graphentheorie, Wissenschaftliche Buchgesellschaft 1980, and in A. Frank’s chapter of the Handbook of Combinatorics (R.L. Graham, M. Gr¨otschel & L. Lov´asz, eds.), North-Holland 1995. A survey specifically of techniques and results on minimally $k$ -connected graphs (see below) is given by W. Mader, On vertices of degree $n$ in minimally $n$ -connected graphs and digraphs, in (D. Mikl´os, V.T. S´os & T. Sz˝onyi, eds.) Paul Erd˝os is 80, Vol. 2, Proc. Colloq. Math. Soc. J´anos Bolyai, Budapest 1996.

Our proof of Tutte’s Theorem 3.2.3 is due to C. Thomassen, Planarity and duality of finite and infinite graphs, J. Combin. Theory B 29 (1980), 244–271. This paper also contains Lemma 3.2.1 and its short proof from first principles. (The lemma’s assertion, of course, follows from Tutte’s wheel theorem—its significance lies in its independent proof, which has shortened the proofs of both of Tutte’s theorems considerably.)

An approach to the study of connectivity not touched upon in this chapter is the investigation of minimal $k$ -connected graphs, those that lose their $k$ -connectedness as soon as we delete an edge. Like all $k$ -connected graphs, these have minimum degree at least $k$ , and by a fundamental result of Halin (1969), their minimum degree is exactly $k$ . The existence of a vertex of small degree can be particularly useful in induction proofs about $k$ -connected graphs. Halin’s theorem was the starting point for a series of more and more sophisticated studies of minimal $k$ -connected graphs; see the books of Bollob´as and Halin cited above, and in particular Mader’s survey.

Our first proof of Menger’s theorem is extracted from Halin’s book. The second is due to T. B¨ohme, F. G¨oring and J. Harant, Menger’s theorem, J. Graph Theory 37 (2001), 35–36, the third to T. Gr¨unwald (later Gallai), Ein neuer Beweis eines Mengerschen Satzes, J. London Math. Soc. 13 (1938), 188–192. The global version of Menger’s theorem (Theorem 3.3.6) was first stated and proved by Whitney (1932).

Mader’s Theorem 3.4.1 is taken from W. Mader, Uber die Maximalzahl¨ kreuzungsfreier $H$ -Wege, Arch. Math. 31 (1978), 387–402; a short proof has been given by A. Schrijver, A short proof of Mader’s $\boldsymbol { S }$ -paths theorem, J. Combin. Theory B 82 (2001), 319–321. The theorem may be viewed as a common generalization of Menger’s theorem and Tutte’s 1-factor theorem (Exercise 21).

Theorem 3.5.3 is due to R. Thomas and P. Wollan, An improved linear bound for graph linkages, Europ. J. Combinatorics 26 (2005), 309–324. Using a more involved version of Lemma 3.5.4, they prove that $2 k$ -connected graphs even with only $\varepsilon \geqslant 5 k$ must be $k$ -linked. And for graphs of large enough girth the condition on $\varepsilon$ can be dropped altogether: as shown by W. Mader, Topological subgraphs in graphs of large girth, Combinatorica 18 (1998), 405– 412, such graphs are $k$ -linked as soon as they are $2 k$ -connected, which is best possible. (Mader assumes a lower bound on the girth that depends on $k$ , but this is not necessary; see D. K¨uhn & D. Osthus, Topological minors in graphs of large girth, J. Combin. Theory $B$ 86 (2002), 364–380.) In fact, for every $s \in \mathbb N$ there exists a $k _ { s }$ such that if $\textit { G } \nsupseteq { \kappa } _ { s , s }$ and $\kappa ( G ) \geq 2 k \geqslant k _ { s }$ then $G$ is $k$ -linked; see D. K¨uhn & D. Osthus, Complete minors in $K _ { s , s }$ -free graphs, Combinatorica 25 (2005) 49–64.

When we draw a graph on a piece of paper, we naturally try to do this as transparently as possible. One obvious way to limit the mess created by all the lines is to avoid intersections. For example, we may ask if we can draw the graph in such a way that no two edges meet in a point other than a common end.

Graphs drawn in this way are called plane graphs; abstract graphs that can be drawn in this way are called planar . In this chapter we study both plane and planar graphs—as well as the relationship between the two: the question of how an abstract graph might be drawn in fundamentally different ways. After collecting together in Section 4.1 the few basic topological facts that will enable us later to prove all results rigorously without too much technical ado, we begin in Section 4.2 by studying the structural properties of plane graphs. In Section 4.3, we investigate how two drawings of the same graph can differ. The main result of that section is that 3-connected planar graphs have essentially only one drawing, in some very strong and natural topological sense. The next two sections are devoted to the proofs of all the classical planarity criteria, conditions telling us when an abstract graph is planar. We complete the chapter with a section on plane duality, a notion with fascinating links to algebraic, colouring, and flow properties of graphs (Chapters 1.9 and 6.5).

The traditional notion of a graph drawing is that its vertices are represented by points in the Euclidean plane, its edges are represented by curves between these points, and different curves meet only in common endpoints. To avoid unnecessary topological complication, however, we shall only consider curves that are piecewise linear; it is not difficult to show that any drawing can be straightened out in this way, so the two notions come to the same thing.

# 4.1 Topological prerequisites

In this section we briefly review some basic topological definitions and facts needed later. All these facts have (by now) easy and well-known proofs; see the notes for sources. Since those proofs contain no graph theory, we do not repeat them here: indeed our aim is to collect precisely those topological facts that we need but do not want to prove. Later, all proofs will follow strictly from the definitions and facts stated here (and be guided by but not rely on geometric intuition), so the material presented now will help to keep elementary topological arguments in those proofs to a minimum.

A straight line segment in the Euclidean plane is a subset of $\mathbb { R } ^ { 2 }$ that has the form $\{ p + \lambda ( q - p ) \mid 0 \leqslant \lambda \leqslant 1 \}$ for distinct points $p , q \in \mathbb { R } ^ { 2 }$ . A polygon is a subset of $\mathbb { R } ^ { 2 }$ which is the union of finitely many straight line segments and is homeomorphic to the unit circle $S ^ { 1 }$ , the set of points in $\mathbb { R } ^ { 2 }$ at distance 1 from the origin. Here, as later, any subset of a topological space is assumed to carry the subspace topology. A polygonal arc is a subset of $\mathbb { R } ^ { 2 }$ which is the union of finitely many straight line segments and is homeomorphic to the closed unit interval $[ 0 , 1 ]$ . The images of 0 and of 1 under such a homeomorphism are the endpoints of this polygonal arc, which links them and runs between them. Instead of ‘polygonal arc’ we shall simply say arc in this chapter. If $P$ is an arc between and , we denote the point set $P \setminus \{ x , y \}$ , the interior of $P$ , $x$ $y$ by $\mathring { P }$ .

Let $O \subseteq \mathbb { R } ^ { 2 }$ be an open set. Being linked by an arc in $O$ defines an equivalence relation on $O$ . The corresponding equivalence classes are again open; they are the regions of $O$ . A closed set $X \subseteq \mathbb { R } ^ { 2 }$ is said to separate $O$ if $O \setminus X$ has more than one region. The frontier of a set $X \subseteq \mathbb { R } ^ { 2 }$ is the set $Y$ of all points $y \in \mathbb { R } ^ { 2 }$ such that every neighbourhood of meets both $X$ and $\mathbb { R } ^ { 2 } \setminus X$ . Note that if $X$ is open then its frontier $y$ lies in $\mathbb { R } ^ { 2 } \setminus X$ .

The frontier of a region $O$ of $\mathbb { R } ^ { 2 } \setminus X$ , where $X$ is a finite union of points and arcs, has two important properties. The first is accessibility: if $x \in X$ lies on the frontier of $O$ , then $x$ can be linked to some point in $O$ by a straight line segment whose interior lies wholly inside $O$ . As a consequence, any two points on the frontier of $O$ can be linked by an arc whose interior lies in $O$ (why?). The second notable property of the frontier of $O$ is that it separates $O$ from the rest of $\mathbb { R } ^ { 2 }$ . Indeed, if : $[ 0 , 1 ] \to P \subseteq \mathbb { R } ^ { 2 }$ $\varphi$ is continuous, with $\varphi ( 0 ) \in O$ and $\varphi ( 1 ) \not \in O$ , then $P$ meets the frontier of $O$ at least in the point $\varphi ( y )$ for $y : = \operatorname* { i n f } \left\{ x \mid \varphi ( x ) \neq O \right\}$ , the first point of $P$ in $\mathbb { R } ^ { 2 } \setminus O$ .

# Theorem 4.1.1. (Jordan Curve Theorem for Polygons)

For every polygon $P \subseteq \mathbb { R } ^ { 2 }$ , the set $\mathbb { R } ^ { 2 } \setminus P$ has exactly two regions. Each of these has the entire polygon $P$ as its frontier.

With the help of Theorem 4.1.1, it is not difficult to prove the following lemma.

Lemma 4.1.2. Let $P _ { 1 } , P _ { 2 } , P _ { 3 }$ be three arcs, between the same two endpoint but otherwise disjoint.

(i) $\mathbb { R } ^ { 2 } \setminus ( P _ { 1 } \cup P _ { 2 } \cup P _ { 3 } )$ ) has exactly three regions, with frontiers $P _ { 1 } \cup P _ { 2 }$ , $P _ { 2 } \cup P _ { 3 }$ and $P _ { 1 } \cup P _ { 3 }$ .   
(ii) If $P$ is an arc between a point in $\breve { P } _ { 1 }$ and a point in $\breve { P } _ { 3 }$ whose interior lies in the region of $\mathbb { R } ^ { 2 } \setminus \left( P _ { 1 } \cup P _ { 3 } \right)$ that contains ${ \dot { P } } _ { 2 }$ , then ${ \mathring { P } } \cap { \mathring { P } } _ { 2 } \not = \emptyset$ .

![](images/aa11a2556f1a3ffef252d4c107400f1b89b8b04103aa8b44181e7e7f2fa0c3f9.jpg)  
Fig. 4.1.1. The arcs in Lemma 4.1.2 (ii)

Our next lemma complements the Jordan curve theorem by saying that an arc does not separate the plane. For easier application later, we phrase this a little more generally:

Lemma 4.1.3. Let $X _ { 1 } , X _ { 2 } \ \subseteq \ \mathbb { R } ^ { 2 }$ be disjoint sets, each the union of finitely many points and arcs, and let $P$ be an arc between a point in $X _ { 1 }$ and one in $X _ { 2 }$ whose interior $\mathring { P }$ lies in a region $O$ of $\mathbb { R } ^ { 2 } \setminus \left( X _ { 1 } \cup X _ { 2 } \right)$ . Then $O \setminus \mathring { P }$ is a region of $\mathbb { R } ^ { 2 } \setminus \left( X _ { 1 } \cup P \cup X _ { 2 } \right)$ .

![](images/d450e964be51b3c2807267065b52a553cec1616f1923f5c2632b5bbaf4bb0c2a.jpg)  
Fig. 4.1.2. $P$ does not separate the region $O$ of $\mathbb { R } ^ { 2 } \setminus \left( X _ { 1 } \cup X _ { 2 } \right)$ )

It remains to introduce a few terms and facts that will be used only once, when we consider notions of equivalence for graph drawings in Chapter 4.3.

As usual, we denote by $S ^ { n }$ the $n$ -dimensional sphere, the set of points in $\mathbb { R } ^ { n + 1 }$ at distance 1 from the origin. The 2-sphere minus its ‘north pole’ $( 0 , 0 , 1 )$ is homeomorphic to the plane; let us choose a fixed such homeomorphism $\pi \colon S ^ { 2 } \setminus \{ ( 0 , 0 , 1 ) \} \to \mathbb { R } ^ { 2 }$ (for example, stereographic projection). If $P \subseteq \mathbb { R } ^ { 2 }$ is a polygon and $O$ is the bounded region of

$\mathbb { R } ^ { 2 } \setminus P$ , let us call $C : = \pi ^ { - 1 } ( P )$ a circle on $S ^ { 2 }$ , and the sets $\pi ^ { - 1 } ( O )$ and $S ^ { 2 } \setminus \pi ^ { - 1 } ( P \cup O )$ the regions of $C$ .

Our last tool is the theorem of Jordan and Schoenflies, again adapted slightly for our purposes:

[ 4.3.1 ]

Theorem 4.1.4. Let $\varphi \colon C _ { 1 } \to C _ { 2 }$ be a homeomorphism between two circles on $S ^ { 2 }$ , let $O _ { 1 }$ be a region of $C _ { 1 }$ , and let $O _ { 2 }$ be a region of $C _ { 2 }$ . Then $\varphi$ can be extended to a homeomorphism $C _ { 1 } \cup O _ { 1 } \to C _ { 2 } \cup O _ { 2 }$ .

# 4.2 Plane graphs

plane graph

A plane graph is a pair $( V , E )$ of finite sets with the following properties (the elements of $V$ are again called vertices, those of $E$ edges):

(i) $V \subseteq \mathbb { R } ^ { 2 }$   
(ii) every edge is an arc between two vertices;   
(iii) different edges have different sets of endpoints;   
(iv) the interior of an edge contains no vertex and no point of any other edge.

A plane graph $( V , E )$ defines a graph $G$ on $V$ in a natural way. As long as no confusion can arise, we shall use the name $G$ of this abstract graph also for the plane graph $( V , E )$ , or for the point set $V \cup \bigcup E$ ; similar notational conventions will be used for abstract versus plane edges, for subgraphs, and so on.1

faces

For every plane graph $G$ , the set $\mathbb { R } ^ { 2 } \setminus G$ is open; its regions are the faces of $G$ . Since $G$ is bounded—i.e., lies inside some sufficiently large disc $D$ —exactly one of its faces is unbounded: the face that contains $\mathbb { R } ^ { 2 } \setminus D$ . This face is the outer face of $G$ ; the other faces are its inner faces. We denote the set of faces of $G$ by $F ( G )$ .

$F ( G )$

The faces of plane graphs and their subgraphs are related in the obvious way:

Lemma 4.2.1. Let $G$ be a plane graph, $f \in F ( G )$ a face, and $H \subseteq G$ a subgraph.

(i) $H$ has a face $f ^ { \prime }$ containing $f$   
(ii) If the frontier of $f$ lies in $H$ , then $f ^ { \prime } = f$ .

Proof . (i) Clearly, the points in $f$ are equivalent also in $\mathbb { R } ^ { 2 } \setminus H$ ; let $f ^ { \prime }$ be the equivalence class of $\mathbb { R } ^ { 2 } \setminus H$ containing them.   
(ii) Recall from Section 4.1 that any arc between $f$ and $f ^ { \prime } \setminus f$ meets the frontier $X$ of $f$ . If $f ^ { \prime } \setminus f \ne \emptyset$ then there is such an arc inside $f ^ { \prime }$ , whose points in $X$ do not lie in $H$ . Hence $X \subsetneq H$ . 

In order to lay the foundations for the (easy but) rigorous introduction to plane graphs that this section aims to provide, let us descend once now into the realm of truly elementary topology of the plane, and prove what seems entirely obvious:2 that the frontier of a face of a plane graph $G$ is always a subgraph of $G$ —not, say, half an edge.

The following lemma states this formally, together with two similarly ‘obvious’ properties of plane graphs:

Lemma 4.2.2. Let $G$ be a plane graph and $e$ an edge of $G$ .

(i) If $X$ is the frontier of a face of $G$ , then either $e \subseteq X$ or $X \cap e = \emptyset$ .   
(ii) If $e$ lies on a cycle $C \subseteq G$ , then $e$ lies on the frontier of exactly two faces of $G$ , and these are contained in distinct faces of $C$ .   
(iii) If e lies on no cycle, then e lies on the frontier of exactly one face of $G$ .

Proof . We prove all three assertions together. Let us start by considering one point $\boldsymbol { x } _ { 0 } \in \mathring { e }$ . We show that $x _ { 0 }$ lies on the frontier of either exactly two faces or exactly one, according as $e$ lies on a cycle in $G$ or not. We then show that every other point in $\check { e }$ lies on the frontier of exactly the same faces as $x _ { 0 }$ . Then the endpoints of $e$ will also lie on the frontier of these faces—simply because every neighbourhood of an endpoint of $e$ is also the neighbourhood of an inner point of $e$ .

$G$ is the union of finitely many straight line segments; we may assume that any two of these intersect in at most one point. Around every point $x \in { \mathfrak { e } }$ we can find an open disc $D _ { x }$ , with centre $x$ , which meets only those (one or two) straight line segments that contain $x$ .

Let us pick an inner point $x _ { 0 }$ from a straight line segment $S \subseteq e$ . Then $D _ { x _ { 0 } } \cap G = D _ { x _ { 0 } } \cap S$ , so $D _ { x _ { 0 } } \setminus G$ is the union of two open half-discs. Since these half-discs do not meet $G$ , they each lie in a face of $G$ . Let us denote these faces by $f _ { 1 }$ and $f _ { 2 }$ ; they are the only faces of $G$ with $x _ { 0 }$ . on their frontier, and they may coincide (Fig. 4.2.1).

If $e$ lies on a cycle $C \subseteq G$ , then $D _ { x _ { 0 } }$ meets both faces of $C$ (Theorem 4.1.1). Since $f _ { 1 }$ and $f _ { 2 }$ are contained in faces of $C$ by Lemma 4.2.1, this implies $f _ { 1 } \neq f _ { 2 }$ . If $e$ does not lie on any cycle, then $e$ is a bridge

![](images/9e4093ae561637cd825d0d38f7b3572ced39c11cc78d347e70a4e064ca6b060d.jpg)  
Fig. 4.2.1. Faces $f _ { 1 } , f _ { 2 }$ of $G$ in the proof of Lemma 4.2.2

and thus links two disjoint point sets $X _ { 1 } , X _ { 2 }$ as in Lemma 4.1.3, with $X _ { 1 } \cup X _ { 2 } = G \setminus e$ . Clearly, $f _ { 1 } \cup \breve { e } \cup f _ { 2 }$ is the subset of a face $f$ of $G - e$ . By Lemma 4.1.3, $f \setminus \breve { e }$ is a face of $G$ . But $f \setminus \breve { e }$ contains $f _ { 1 }$ and $f _ { 2 }$ by definition of $f$ , so $f _ { 1 } = f \setminus { \overset { \circ } { e } } = f _ { 2 }$ since $f _ { 1 }$ , $f _ { 2 }$ and $f$ are all faces of $G$ .

Now consider any other point $x _ { 1 } \in { \overset { \circ } { e } }$ . Let $P$ be the arc from $x _ { 0 }$ to $x _ { 1 }$ contained in $e$ . Since $P$ is compact, finitely many of the discs $D _ { x }$ with $x \in P$ cover $P$ . Let us enumerate these discs as $\cal D _ { 0 } , \ldots , \cal D _ { n }$ in the natural order of their centres along $P$ ; adding $D _ { x _ { 0 } }$ or $D _ { x _ { 1 } }$ as necessary, we may assume that $D _ { 0 } = D _ { x _ { 0 } }$ and $D _ { n } = D _ { x _ { 1 } }$ . By induction on $n$ , one easily proves that every point $y \in D _ { n } \setminus e$ can be linked by an arc inside $( D _ { 0 } \cup . . . \cup D _ { n } ) \setminus e$ to a point $z \in D _ { 0 } \setminus e$ (Fig. 4.2.2); then $y$ and $z$ are equivalent in $\mathbb { R } ^ { 2 } \setminus G$ . Hence, every point of $D _ { n } \setminus e$ lies in $f _ { 1 }$ or in $f _ { 2 }$ , so $x _ { 1 }$ cannot lie on the frontier of any other face of $G$ . Since both half-discs of $D _ { 0 } \setminus e$ can be linked to $D _ { n } \setminus e$ in this way (swap the roles of $D _ { 0 }$ and $D _ { n }$ ), we find that $x _ { 1 }$ lies on the frontier of both $f _ { 1 }$ and $f _ { 2 }$ . 

![](images/c3723d23101f6719d13db2cdde9a27fd2824276601a5545ce98056cdc33d78ee.jpg)  
Fig. 4.2.2. An arc from $_ y$ to $D _ { 0 }$ , close to $P$

Corollary 4.2.3. The frontier of a face is always the point set of a subgraph. 

The subgraph of $G$ whose point set is the frontier of a face $f$ is said to bound $f$ and is called its boundary; we denote it by $G \left[ f \right]$ . A face is said to be incident with the vertices and edges of its boundary. By Lemma 4.2.1 (ii), every face of $G$ is also a face of its boundary; we shall use this fact frequently in the proofs to come.

[ 4.6.1 ] Proposition 4.2.4. A plane forest has exactly one face.   
(4.1.3) Proof . Use induction on the number of edges and Lemma 4.1.3.

With just one exception, different faces of a plane graph have different boundaries:

[ 4.3.1 ] Lemma 4.2.5. If a plane graph has different faces with the same boundary, then the graph is a cycle.

Proof . Let $G$ be a plane graph, and let $H \subseteq G$ be the boundary of (4.1.1) distinct faces $f _ { 1 } , f _ { 2 }$ of $G$ . Since $f _ { 1 }$ and $f _ { 2 }$ are also faces of $H$ , Proposition 4.2.4 implies that $H$ contains a cycle $C$ . By Lemma 4.2.2 (ii), $f _ { 1 }$ and $f _ { 2 }$ are contained in different faces of $C$ . Since $f _ { 1 }$ and $f _ { 2 }$ both have all of $H$ as boundary, this implies that $H = C$ : any further vertex or edge of $H$ would lie in one of the faces of $C$ and hence not on the boundary of the other. Thus, $f _ { 1 }$ and $f _ { 2 }$ are distinct faces of $C$ . As $C$ has only two faces, it follows that $f _ { 1 } \cup C \cup f _ { 2 } = \mathbb { R } ^ { 2 }$ and hence $G = C$ . 

Proposition 4.2.6. In a 2-connected plane graph, every face is bounded by a cycle.

Proof . Let $f$ be a face in a 2-connected plane graph $G$ . We show by induction on $| G |$ that $G \left[ f \right]$ is a cycle. If $G$ is itself a cycle, this holds by Theorem 4.1.1; we therefore assume that $G$ is not a cycle.

By Proposition 3.1.3, there exist a 2-connected plane graph $H \subseteq G$ and a plane $H$ -path $P$ such that $G = H \cup P$ . The interior of $P$ lies in a face $f ^ { \prime }$ of $H$ , which by the induction hypothesis is bounded by a cycle $C$ .

If $f$ is also a face of $H$ , we are home by the induction hypothesis. If not, then the frontier of $f$ meets $P \setminus H$ , so $f \subseteq f ^ { \prime }$ and $G [ f ] \subseteq C \cup P$ . By Lemma 4.2.1 (ii), then, $f$ is a face of $C \cup P$ and hence bounded by a cycle (Lemma 4.1.2 (i)). 

In a 3-connected graph, we can identify the face boundaries among the other cycles in purely combinatorial terms:

Proposition 4.2.7. The face boundaries in a 3-connected plane graph are precisely its non-separating induced cycles.

Proof . Let $G$ be a 3-connected plane graph, and let $C \subseteq G$ . If $C$ is a non-separating induced cycle, then by the Jordan curve theorem its two faces cannot both contain points of $G \setminus C$ . Therefore it bounds a face of $G$ .

Conversely, suppose that $C$ bounds a face $f$ . By Proposition 4.2.6, $C$ is a cycle. If $C$ has a chord $e = x y$ , then the components of $C - \{ x , y \}$ are linked by a $C$ -path in $G$ , because $G$ is 3-connected. This path and $e$ both run through the other face of $C$ (not $f$ ) but do not intersect, a contradiction to Lemma 4.1.2 (ii).

It remains to show that $C$ does not separate any two vertices $x , y \in$ $G - C$ . By Menger’s theorem (3.3.6), $x$ and $y$ are linked in $G$ by three independent paths. Clearly, $f$ lies inside a face of their union, and by Lemma 4.1.2 (i) this face is bounded by only two of the paths. The third therefore avoids $f$ and its boundary $C$ . 

maximal plane graph   
plane triangulation

A plane graph $G$ is called maximally plane, or just maximal , if we cannot add a new edge to form a plane graph $G ^ { \prime } \nsupseteq G$ with $V ( G ^ { \prime } ) = V ( G )$ . We call $G$ a plane triangulation if every face of $G$ (including the outer face) is bounded by a triangle.

[ 4.4.1 ] [ 5.4.2 ]

Proposition 4.2.8. A plane graph of order at least 3 is maximally plane if and only if it is a plane triangulation.

(4.1.2)

Proof . Let $G$ be a plane graph of order at least 3. It is easy to see that if every face of $G$ is bounded by a triangle, then $G$ is maximally plane. Indeed, any additional edge $e$ would have its interior inside a face of $G$ and its ends on the boundary of that face. Hence these ends are already adjacent in $G$ , so $G \cup e$ cannot satisfy condition (iii) in the definition of a plane graph.

$f$ $H$

Conversely, assume that $G$ is maximally plane and let $f \in F ( G )$ be a face; let us write $H : = G [ f ]$ . Since $G$ is maximal as a plane graph, $G \left[ H \right]$ is complete: any two vertices of $H$ that are not already adjacent in $G$ could be linked by an arc through $f$ , extending $G$ to a larger plane graph. Thus $G [ H ] = K ^ { n }$ for some $n$ —but we do not know yet which edges of $G \left[ H \right]$ lie in $H$ .

$_ n$

Let us show first that $H$ contains a cycle. If not, then $G \setminus H \neq \emptyset$ : by $G \supseteq K ^ { n }$ if $n \geqslant 3$ , or else by $| G | \geqslant 3$ . On the other hand we have $f \cup H = \mathbb { R } ^ { 2 }$ by Proposition 4.2.4 and hence $G = H$ , a contradiction.

$C , v _ { i }$

Since $H$ contains a cycle, it suffices to show that $n \leqslant 3$ : then $H = K ^ { 3 }$ as claimed. Suppose $n \geqslant 4$ , and let $C = v _ { 1 } v _ { 2 } v _ { 3 } v _ { 4 } v _ { 1 }$ be a cycle in $G \left[ H \right]$ $( = K ^ { n }$ ). By $C \subseteq G$ , our face $f$ is contained in a face $f _ { C }$ of $C$ ; let $f _ { C } ^ { \prime }$ be the other face of $C$ . Since the vertices $v _ { 1 }$ and $v _ { 3 }$ lie on the boundary of $f$ , they can be linked by an arc whose interior lies in $f _ { C }$ and avoids $G$ . Hence by Lemma 4.1.2 (ii), the plane edge $v _ { 2 } v _ { 4 }$ of $G \left[ H \right]$ runs through $f _ { C } ^ { \prime }$ rather than $f _ { C }$ (Fig. 4.2.3). Analogously, since $v _ { 2 } , v _ { 4 } \ \in \ G [ f ]$ , the edge $v _ { 1 } v _ { 3 }$ runs through $f _ { C } ^ { \prime }$ . But the edges $v _ { 1 } v _ { 3 }$ and $v _ { 2 } v _ { 4 }$ are disjoint, so this contradicts Lemma 4.1.2 (ii). 

![](images/59cc45bbfc00e0afb600267e05464c114c2ca7934b5b5921dfa2e7248e6625e3.jpg)  
Fig. 4.2.3. The edge $v 2 v 4$ of $G$ runs through the face $f _ { C } ^ { \prime }$

The following classic result of Euler (1752)—here stated in its simplest form, for the plane—marks one of the common origins of graph theory and topology. The theorem relates the number of vertices, edges and faces in a plane graph: taken with the correct signs, these numbers

always add up to 2. The general form of Euler’s theorem asserts the same for graphs suitably embedded in other surfaces, too: the sum obtained is always a fixed number depending only on the surface, not on the graph, and this number differs for distinct (orientable closed) surfaces. Hence, any two such surfaces can be distinguished by a simple arithmetic invariant of the graphs embedded in them!3

Let us then prove Euler’s theorem in its simplest form:

# Theorem 4.2.9. (Euler’s Formula)

Let $G$ be a connected plane graph with $n$ vertices, $m$ edges, and  faces. Then

$$
n - m + \ell = 2.
$$

Proof . We fix $n$ and apply induction on $m$ . For $m \leqslant n - 1 .$ , G is a tree and $m = n - 1$ (why?), so the assertion follows from Proposition 4.2.4.

Now let $m \geqslant n$ . Then $G$ has an edge $e$ that lies on a cycle; let $G ^ { \prime } : = G - e$ . By Lemma 4.2.2 (ii), $e$ lies on the boundary of exactly two faces $f _ { 1 } , f _ { 2 }$ of $G$ , and as the points in $\check { e }$ are all equivalent in $\mathbb { R } ^ { 2 } \setminus G ^ { \prime }$ , there is a face $f _ { e }$ of $G ^ { \prime }$ containing $\check { e }$ . We show that

$$
F (G) \setminus \left\{f _ {1}, f _ {2} \right\} = F \left(G ^ {\prime}\right) \setminus \left\{f _ {e} \right\}; \tag {*}
$$

then $G ^ { \prime }$ has exactly one face and one edge less than $G$ , and so the assertion follows from the induction hypothesis for $G ^ { \prime }$ .

For a proof of $( * )$ let first $f \in F ( G ) \setminus \{ f _ { 1 } , f _ { 2 } \}$ be given. By Lemma 4.2.2 (i) we have $G \left[ f \right] \subseteq G \setminus { \mathring { e } } = G ^ { \prime }$ , and hence $f \in F ( G ^ { \prime } )$ by Lemma 4.2.1 (ii). As clearly $f \neq f _ { e }$ , this establishes the forwad inclusion in ( ).

Conversely, consider any face $f ^ { \prime } \in F ( G ^ { \prime } ) \setminus \{ f _ { e } \}$ . Clearly $f ^ { \prime } \neq f _ { 1 } , f _ { 2 }$ , and $f ^ { \prime } \cap \mathring { e } \ = \ \varnothing$ . Hence every two points of $f ^ { \prime }$ lie in $\mathbb { R } ^ { 2 } \setminus G$ and are equivalent there, so $G$ has a face $f$ containing $f ^ { \prime }$ . By Lemma 4.2.1 (i), however, $f$ lies inside a face $f ^ { \prime \prime }$ of $G ^ { \prime }$ . Thus $f ^ { \prime } \subseteq f \subseteq f ^ { \prime \prime }$ and hence $f ^ { \prime } = f = f ^ { \prime \prime }$ , since both $f ^ { \prime }$ and $f ^ { \prime \prime }$ are faces of $G ^ { \prime }$ . 

Corollary 4.2.10. A plane graph with $n \geqslant 3$ vertices has at most $3 n - 6$ edges. Every plane triangulation with $n$ vertices has $3 n - 6$ edges.

Proof . By Proposition 4.2.8 it suffices to prove the second assertion. In a plane triangulation $G$ , every face boundary contains exactly three edges, and every edge lies on the boundary of exactly two faces (Lemma 4.2.2). The bipartite graph on $E ( G ) \cup F ( G )$ with edge set $\{ e f \mid e \subseteq G [ f ] \}$ thus has exactly $2 \left| E ( G ) \right| = 3 \left| F ( G ) \right|$ edges. According to this identity we may replace $\ell$ with $2 m / 3$ in Euler’s formula, and obtain $m = 3 n - 6$ . 

Euler’s formula can be useful for showing that certain graphs cannot occur as plane graphs. The graph $K ^ { 5 }$ , for example, has $1 0 > 3 \cdot 5 - 6$ edges, more than allowed by Corollary 4.2.10. Similarly, $K _ { 3 , 3 }$ cannot be a plane graph. For since $K _ { 3 , 3 }$ is 2-connected but contains no triangle, every face of a plane $K _ { 3 , 3 }$ would be bounded by a cycle of length $\geqslant 4$ (Proposition 4.2.6). As in the proof of Corollary 4.2.10 this implies $2 m \geqslant 4 \ell$ , which yields $m \leqslant 2 n - 4$ when substituted in Euler’s formula. But $K _ { 3 , 3 }$ has $9 > 2 \cdot 6 - 4$ edges.

Clearly, along with $K ^ { 5 }$ and $K _ { 3 , 3 }$ themselves, their subdivisions cannot occur as plane graphs either:

[ 4.4.5 ] [ 4.4.6 ]

Corollary 4.2.11. A plane graph contains neither $K ^ { 5 }$ nor $K _ { 3 , 3 }$ as a topological minor. 

Surprisingly, it turns out that this simple property of plane graphs identifies them among all other graphs: as Section 4.4 will show, an arbitrary graph can be drawn in the plane if and only if it has no (topological) $K ^ { 5 }$ or $K _ { 3 , 3 }$ minor.

# 4.3 Drawings

planar embedding drawing

An embedding in the plane, or planar embedding, of an (abstract) graph $G$ is an isomorphism between $G$ and a plane graph $H$ . The latter will be called a drawing of $G$ . We shall not always distinguish notationally between the vertices and edges of $G$ and of $H$ . In this section we investigate how two planar embeddings of a graph can differ.

How should we measure the likeness of two embeddings $\rho \colon G \to H$ and $\rho ^ { \prime } { : } G \to H ^ { \prime }$ of a planar graph $G$ ? An obvious way to do this is to consider the canonical isomorphism $\sigma : = \rho ^ { \prime } \circ \rho ^ { - 1 }$ between $H$ and $H ^ { \prime }$ as abstract graphs, and ask how much of their position in the plane this isomorphism respects or preserves. For example, if $\sigma$ is induced by a simple rotation of the plane, we would hardly consider $\rho$ and $\rho ^ { \prime }$ as genuinely different ways of drawing $G$ .

$\sigma$ H; $V , E , F$ $H ^ { \prime } ; V _ { \bf \epsilon } ^ { \prime } E _ { \bf \epsilon } ^ { \prime } F ^ { \prime }$

So let us begin by considering any abstract isomorphism $\sigma { : } V \to V ^ { \prime }$ between two plane graphs $H \ : = \ : ( V , E )$ and $H ^ { \prime } = ( V ^ { \prime } , E ^ { \prime } )$ , with face sets $F ( H ) = : F$ and $F ( H ^ { \prime } ) = : F ^ { \prime }$ say, and try to measure to what degree $\sigma$ respects or preserves the features of $H$ and $H ^ { \prime }$ as plane graphs. In what follows we shall propose three criteria for this in decreasing order of strictness (and increasing order of ease of handling), and then prove that for most graphs these three criteria turn out to agree. In particular, applied to the isomorphism $\sigma = \rho ^ { \prime } \circ \rho ^ { - 1 }$ considered earlier, all three criteria will say that there is essentially only one way to draw a 3-connected graph.

Our first criterion for measuring how well our abstract isomorphism $\sigma$ preserves the plane features of $H$ and $H ^ { \prime }$ is perhaps the most natural one. Intuitively, we would like to call $\sigma$ ‘topological’ if it is induced by a homeomorphism from the plane $\mathbb { R } ^ { 2 }$ to itself. To avoid having to grant the outer faces of $H$ and $H ^ { \prime }$ a special status, however, we take a detour via the homeomorphism $\pi \colon S ^ { 2 } \setminus \left\{ \left( 0 , 0 , 1 \right) \right\} \to \mathbb { R } ^ { 2 }$ chosen in Section 4.1: we call $\sigma$ a topological isomorphism between the plane graphs $H$ and $H ^ { \prime }$ if there exists a homeomorphism $\varphi \colon S ^ { 2 } \to S ^ { 2 }$ such that $\psi : = \pi \circ \varphi \circ \pi ^ { - 1 }$ induces $\sigma$ on $V \cup E$ . (More formally: we ask that $\psi$ agree with $\sigma$ on $V$ , and that it map every plane edge $x y \in H$ onto the plane edge $\sigma ( x ) \sigma ( y ) \in$ $H ^ { \prime }$ . Unless fixes the point $( 0 , 0 , 1 )$ , the map $\psi$ will be undefined at $\varphi$ $\pi ( \varphi ^ { - 1 } ( 0 , 0 , 1 ) )$ ).)

$\pi$

topological isomorphism

![](images/03ba7fae5aad0a306572a07bcdc6a1b25b17e6329adc3e4a573324d6a1ffaa9e.jpg)

![](images/e268cc1e690455661dc467e336bea8493010d90c61cdfbe2f25829826af22c80.jpg)  
Fig. 4.3.1. Two drawings of a graph that are not topologically isomorphic—why not?

It can be shown that, up to topological isomorphism, inner and outer faces are indeed no longer different: if we choose as $\varphi$ a rotation of $S ^ { 2 }$ mapping the $\pi ^ { - 1 }$ -image of a point of some inner face of $H$ to the north pole $( 0 , 0 , 1 )$ of $S ^ { 2 }$ , then $\psi$ maps the rest of this face to the outer face of $\psi ( H )$ . (To ensure that the edges of $\psi ( H )$ are again piecewise linear, however, one may have to adjust $\varphi$ a little.)

If $\sigma$ is a topological isomorphism as above, then—except possibly for a pair of missing points where $\psi$ or $\psi ^ { - 1 }$ is undefined— $\psi$ maps the faces of $H$ onto those of $H ^ { \prime }$ (proof?). In this way, $\sigma$ extends naturally to a bijection $\sigma \colon V \cup E \cup F  V ^ { \prime } \cup E ^ { \prime } \cup F ^ { \prime }$ which preserves incidence of vertices, edges and faces.

Let us single out this last property of a topological isomorphism as the second criterion for how well an abstract isomorphism between plane graphs respects their position in the plane: let us call $\sigma$ a combinatorial isomorphism of the plane graphs $H$ and $H ^ { \prime }$ if it can be extended to a bijection $\sigma { : } V \cup E \cup F {  } V ^ { \prime } \cup E ^ { \prime } \cup F ^ { \prime }$ that preserves incidence not only of vertices with edges but also of vertices and edges with faces. (Formally: we require that a vertex or edge $x \in H$ shall lie on the boundary of a face $f \in F$ if and only if $\sigma ( x )$ lies on the boundary of the face $\sigma ( f )$ .)

If $\sigma$ is a combinatorial isomorphism of the plane graphs $H$ and $H ^ { \prime }$ , it maps the face boundaries of $H$ to those of $H ^ { \prime }$ . Let us pick out this property as our third criterion, and call $\sigma$ a graph-theoretical isomorphism of

combinatorial isomorphism

graphtheoretical isomorphism

![](images/b08d45346922ef360c8afe661c8cc78ddf510d66b39574536009c335759f22f0.jpg)

![](images/a34c813d5e8b317fc7e0efff184a1aa730695ee83e2ee6652e7057ce41862753.jpg)  
Fig. 4.3.2. Two drawings of a graph that are combinatorially isomorphic but not topologically—why not?

the plane graphs $H$ and $H ^ { \prime }$ if

$$
\left\{\sigma (H [ f ]) \colon f \in F \right\} = \left\{H ^ {\prime} [ f ^ {\prime} ] \colon f ^ {\prime} \in F ^ {\prime} \right\}.
$$

Thus, we no longer keep track of which face is bounded by a given subgraph: the only information we keep is whether a subgraph bounds some face or not, and we require that $o$ map the subgraphs that do onto each other. At first glance, this third criterion may appear a little less natural than the previous two. However, it has the practical advantage of being formally weaker and hence easier to verify, and moreover, it will turn out to be equivalent to the other two in most cases.

As we have seen, every topological isomorphism between two plane graphs is also combinatorial, and every combinatorial isomorphism is also graph-theoretical. The following theorem shows that, for most graphs, the converse is true as well:

# Theorem 4.3.1.

(i) Every graph-theoretical isomorphism between two plane graphs is combinatorial. Its extension to a face bijection is unique if and only if the graph is not a cycle.   
(ii) Every combinatorial isomorphism between two 2-connected plane graphs is topological.

Proof . Let $H ~ = ~ ( V , E )$ and $H ^ { \prime } ~ = ~ ( V ^ { \prime } , E ^ { \prime } )$ be two plane graphs, put $F ( H ) = : F$ and $F ( H ^ { \prime } ) = : F ^ { \prime }$ , and let $\sigma { \colon V }  V ^ { \prime }$ be an isomorphism between the underlying abstract graphs. Extend $\sigma$ to a map $V \cup E  V ^ { \prime } \cup E ^ { \prime }$ by letting $\sigma ( x y ) : = \sigma ( x ) \sigma ( y )$ .

(i) If $H$ is a cycle, the assertion follows from the Jordan curve theorem. We now assume that $H$ is not a cycle. Let $\boldsymbol { \beta }$ and $B ^ { \prime }$ be the sets of all face boundaries in $H$ and $H ^ { \prime }$ , respectively. If $\sigma$ is a graph-theoretical isomorphism, then the map $B \mapsto \sigma ( B )$ is a bijection between $\boldsymbol { \beta }$ and $B ^ { \prime }$ . By Lemma 4.2.5, the map $f \mapsto H \left[ f \right]$ is a bijection between $F ^ { \prime }$ and $\boldsymbol { B }$ , and likewise for $F ^ { \prime }$ and $B ^ { \prime }$ . The composition of these three bijections is a bijection between $F$ and $F ^ { \prime }$ , which we choose as $\sigma { : } F \to F ^ { \prime }$ . By construction, this extension of $\sigma$ to $V \cup E \cup F$ preserves incidences (and is unique with this property), so $\sigma$ is indeed a combinatorial isomorphism.

(ii) Let us assume that $H$ is 2-connected, and that $\sigma$ is a combinatorial isomorphism. We have to construct a homeomorphism $\varphi \colon S ^ { 2 } \to S ^ { 2 }$ which, for every vertex or plane edge $x \in H$ , maps $\pi ^ { - 1 } ( x )$ to $\pi ^ { - 1 } ( \sigma ( x ) )$ . Since $\sigma$ is a combinatorial isomorphism, $\tilde { \sigma } : \pi ^ { - 1 } \circ \sigma \circ \pi$ is an incidence preserving bijection from the vertices, edges and faces4 of ${ \tilde { H } } : = \pi ^ { - 1 } ( H )$ to the vertices, edges and faces of ${ \tilde { H } } ^ { \prime } : = \pi ^ { - 1 } ( H ^ { \prime } )$ .

![](images/2d61d5621a7144398ccdd63bb7dc5bc6d6a74003756723a835237c099fa378c4.jpg)  
Fig. 4.3.3. Defining $\tilde { \sigma }$ via $\sigma$

We construct $\varphi$ in three steps. Let us first define $\varphi$ on the vertex set of $\ddot { H }$ , setting $\varphi ( x ) : = \tilde { \sigma } ( x )$ for all $x \in V ( { \tilde { H } } )$ . This is trivially a homeomorphism between $V ( H )$ and $V ( \tilde { H } ^ { \prime } )$ .

As the second step, we now extend $\varphi$ to a homeomorphism between $\tilde { H }$ and ${ \tilde { H } } ^ { \prime }$ that induces $\tilde { \sigma }$ on $V ( \tilde { H } ) \cup \underline { E } ( \tilde { H } )$ . We may do this edge by edge, as follows. Every edge $x y$ of $\tilde { H }$ is homeomorphic to the edge $\tilde { \sigma } ( x y ) = \varphi ( x ) \varphi ( y )$ of ${ \tilde { H } } ^ { \prime }$ , by a homeomorphism mapping $x$ to $\varphi ( x )$ and $y$ to $\varphi ( y )$ . Then the union of all these homeomorphisms, one for every edge of $H$ , is indeed a homeomorphism between $\tilde { H }$ and ${ \tilde { H } } ^ { \prime }$ —our desired extension of $\varphi$ to $\ddot { H }$ : all we have to check is continuity at the vertices (where the edge homeomorphisms overlap), and this follows at once from our assumption that the two graphs and their individual edges all carry the subspace topology in $\mathbb { R } ^ { 3 }$ .

In the third step we now extend our homeomorphism $\varphi \colon \tilde { H }  \tilde { H } ^ { \prime }$ to all of $S ^ { 2 }$ . This can be done analogously to the second step, face by face. By Proposition 4.2.6, all face boundaries in $\tilde { H }$ and ${ \tilde { H } } ^ { \prime }$ are cycles. Now if $f$ is a face of $\tilde { H }$ and $C$ its boundary, then $\tilde { \sigma } ( C ) : = \bigcup \{ \tilde { \sigma } ( e ) ~ | ~ e \in E ( C ) ~ \}$ bounds the face $\tilde { \sigma } ( f )$ of $\ddot { H ^ { \prime } }$ . By Theorem 4.1.4, we may therefore extend the homeomorphism $\varphi \colon C \to { \tilde { \sigma } } ( C )$ defined so far to a homeomorphism from $C \cup f$ to $\tilde { \sigma } ( C ) \cup \tilde { \sigma } ( f )$ . We finally take the union of all these homeomorphisms, one for every face $f$ of $\ddot { H }$ , as our desired homeomorphism $\varphi \colon S ^ { 2 } \to S ^ { 2 }$ ; as before, continuity is easily checked. 

Let us return now to our original goal, the definition of equivalence for planar embeddings. Let us call two planar embeddings $\rho , \rho ^ { \prime }$ of a graph

equivalent embeddings

$G$ topologically (respectively, combinatorially) equivalent if $\rho ^ { \prime } \circ \rho ^ { - 1 }$ is a topological (respectively, combinatorial) isomorphism between $\rho ( G )$ and $\rho ^ { \prime } ( G )$ . If $G$ is 2-connected, the two definitions coincide by Theorem 4.3.1, and we simply speak of equivalent embeddings. Clearly, this is indeed an equivalence relation on the set of planar embeddings of any given graph.

Note that two drawings of $G$ resulting from inequivalent embeddings may well be topologically isomorphic (exercise): for the equivalence of two embeddings we ask not only that some (topological or combinatorial) isomorphism exist between the their images, but that the canonical isomorphism $\rho ^ { \prime } \circ \rho ^ { - 1 }$ be a topological or combinatorial one.

Even in this strong sense, 3-connected graphs have only one embedding up to equivalence:

[ 12.5.4 ] Theorem 4.3.2. (Whitney 1932)

Any two planar embeddings of a 3-connected graph are equivalent.

(4.2.7) Proof . Let $G$ be a 3-connected graph with planar embeddings $\rho \colon G \to H$ and $\rho ^ { \prime } { : } G \to H ^ { \prime }$ . By Theorem 4.3.1 it suffices to show that $\rho ^ { \prime } \circ \rho ^ { - 1 }$ i s a graph-theoretical isomorphism, i.e. that $\rho ( C )$ bounds a face of $H$ if and only if $\rho ^ { \prime } ( C )$ bounds a face of $H ^ { \prime }$ , for every subgraph $C \subseteq G$ . This follows at once from Proposition 4.2.7. 

# 4.4 Planar graphs: Kuratowski’s theorem

planar

A graph is called planar if it can be embedded in the plane: if it is isomorphic to a plane graph. A planar graph is maximal, or maximally planar , if it is planar but cannot be extended to a larger planar graph by adding an edge (but no vertex).

Drawings of maximal planar graphs are clearly maximally plane. The converse, however, is not obvious: when we start to draw a planar graph, could it happen that we get stuck half-way with a proper subgraph that is already maximally plane? Our first proposition says that this can never happen, that is, a plane graph is never maximally plane just because it is badly drawn:

# Proposition 4.4.1.

(i) Every maximal plane graph is maximally planar.   
(ii) A planar graph with $n \geqslant 3$ vertices is maximally planar if and only if it has $3 n - 6$ edges.

(4.2.8)

Proof . Apply Proposition 4.2.8 and Corollary 4.2.10.

![](images/14bda09dc26accf686eaf218df1f750bee79c11317c8345b905af4464e63414a.jpg)

Which graphs are planar? As we saw in Corollary 4.2.11, no planar graph contains $K ^ { 5 }$ or $K _ { 3 , 3 }$ as a topological minor. Our aim in this section is to prove the surprising converse, a classic theorem of Kuratowski: any graph without a topological $K ^ { 5 }$ or $K _ { 3 , 3 }$ minor is planar.

Before we prove Kuratowski’s theorem, let us note that it suffices to consider ordinary minors rather than topological ones:

Lemma 4.4.2. A graph contains $K ^ { 5 }$ or $K _ { 3 , 3 }$ as a minor if and only if it contains $K ^ { 5 }$ or $K _ { 3 , 3 }$ as a topological minor.

Proof . By Proposition 1.7.2 it suffices to show that every graph $G$ (1.7.2) with a $K ^ { 5 }$ minor contains either $K ^ { 5 }$ as a topological minor or $K _ { 3 , 3 }$ as a minor. So suppose that $G \succcurlyeq K ^ { 5 }$ , and let $K \subseteq G$ be minimal such that $K = M K ^ { 5 }$ . Then every branch set of $K$ induces a tree in $K$ , and between any two branch sets $K$ has exactly one edge. If we take the tree induced by a branch set $V _ { x }$ and add to it the four edges joining it to other branch sets, we obtain another tree, $T _ { x }$ say. By the minimality of $K$ , $T _ { x }$ has exactly 4 leaves, the 4 neighbours of $V _ { x }$ in other branch sets (Fig. 4.4.1).

![](images/0970603a98ea8759477e04dbc95060ae217dcfb088485e906be1bbc148f6831b.jpg)  
Fig. 4.4.1. Every $M K ^ { \mathrm { { ^ { - } } } \mathrm { { ^ { - } } } }$ contains a $T K ^ { 5 }$ o r $M K _ { 3 , 3 }$

If each of the five trees $T _ { x }$ is a $T K _ { 1 , 4 }$ then $K$ is a $T K ^ { 5 }$ , and we are done. If one of the $T _ { x }$ is not a $T K _ { 1 , 4 }$ then it has exactly two vertices of degree 3. Contracting $V _ { x }$ onto these two vertices, and every other branch set to a single vertex, we obtain a graph on 6 vertices containing a $K _ { 3 , 3 }$ . Thus, $G \succcurlyeq K _ { 3 , 3 }$ as desired. 

We first prove Kuratowski’s theorem for 3-connected graphs. This is the heart of the proof: the general case will then follow easily.

Lemma 4.4.3. Every 3-connected graph $G$ without a $K ^ { 5 }$ or $K _ { 3 , 3 }$ minor is planar.

(3.2.1) (4.2.6)   
xy   
$\tilde { G }$   
$f , C$   
$X , Y$

$\tilde { G } ^ { \prime }$

Proof . We apply induction on $| G |$ . For $| G | = 4$ we have $G = K ^ { 4 }$ , and the assertion holds. Now let $| G | > 4$ , and assume the assertion is true for smaller graphs. By Lemma 3.2.1, $G$ has an edge $x y$ such that $G / x y$ is again 3-connected. Since the minor relation is transitive, $G / x y$ has no $K ^ { 5 }$ or $K _ { 3 , 3 }$ minor either. Thus, by the induction hypothesis, $G / x y$ has a drawing $\ddot { G }$ in the plane. Let $f$ be the face of $\dot { G } - v _ { x y }$ containing the point $v _ { x y }$ , and let $C$ be the boundary of $f$ . Let $X : = N _ { G } ( x ) \setminus \{ y \}$ and $Y : = N _ { G } ( y ) \setminus \{ x \}$ ; then $X \cup Y \subseteq V ( C )$ , because $v _ { x y } \in f$ . Clearly,

$$
\tilde {G} ^ {\prime} := \tilde {G} - \left\{v _ {x y} v \mid v \in Y \setminus X \right\}
$$

may be viewed as a drawing of $G - y$ , in which the vertex $x$ is represented by the point $v _ { x y }$ (Fig. 4.4.2). Our aim is to add $y$ to this drawing to obtain a drawing of $G$ .

![](images/c05b9e40a3c9fab3a1981641d6908ae54bc48f4d8ae8fc7251c4e8e4c6f9f54b.jpg)  
Fig. 4.4.2. $\hat { G } ^ { \prime }$ as a drawing of $G - y$ : the vertex $_ x$ is represented by the point $v _ { x y }$

x1, . . . , xk $P _ { i }$

Since $\ddot { G }$ is 3-connected, $\tilde { G } - v _ { x y }$ is 2-connected, so $C$ is a cycle (Proposition 4.2.6). Let $x _ { 1 } , \ldots , x _ { k }$ be an enumeration along this cycle of the vertices in $X$ , and let $P _ { i } = x _ { i } \dots x _ { i + 1 }$ be the $X$ -paths on $C$ between them ( $i = 1 , \ldots , k$ ; with $x _ { k + 1 } : = x _ { 1 }$ ). Let us show that $Y \subseteq V ( P _ { i } )$ for some $i$ . If not, then either $x$ and $y$ have three common neighbours on $C$ and form a $T K ^ { 5 }$ with these, or has two neighbours on $C$ that $y$ are separated in $C$ by two neighbours of $x$ , and these four vertices of $C$ form with $x$ and $y$ the branch vertices of a $T K _ { 3 , 3 }$ . In either case have a contradiction, since $G$ contains neither a $T K ^ { 5 }$ nor a $T K _ { 3 , 3 }$ .

$C _ { i }$ $f _ { i }$

Now fix $i$ so that $Y \subseteq P _ { i }$ . The set $C \setminus P _ { i }$ is contained in one of the two faces of the cycle $C _ { i } : = x x _ { i } P _ { i } x _ { i + 1 } x$ ; we denote the other face of $C _ { i }$ by $f _ { i }$ . Since $f _ { i }$ contains points of $f$ (close to $x$ ) but no points of $C$ , we have $f _ { i } \subseteq f$ . Moreover, the plane edges $x x _ { j }$ with $j \not \in \{ i , i + 1 \}$ meet $C _ { i }$ only in $x$ and end outside $f _ { i }$ in $C \setminus P _ { i }$ , so $f _ { i }$ meets none of those edges. Hence $f _ { i } \subseteq \mathbb { R } ^ { 2 } \setminus \bar { G } ^ { \prime }$ , that is, $f _ { i }$ is contained in (and hence equal to) a face of $\bar { G } ^ { \prime }$ . We may therefore extend $\bar { G } ^ { \prime }$ to a drawing of $G$ by placing $y$ and its incident edges in $f _ { i }$ . 

Compared with other proofs of Kuratowski’s theorem, the above proof has the attractive feature that it can easily be adapted to produce a drawing in which every inner face is convex (exercise); in particular, every edge can be drawn straight. Note that 3-connectedness is essential here: a 2-connected planar graph need not have a drawing with all inner faces convex (example?), although it always has a straight-line drawing (Exercise 12).

It is not difficult, in principle, to reduce the general Kuratowski theorem to the 3-connected case by manipulating and combining partial drawings assumed to exist by induction. For example, if $\kappa ( G ) = 2$ and $G = G _ { 1 } \cup G _ { 2 }$ with $V ( G _ { 1 } \cap G _ { 2 } ) = \{ x , y \}$ , and if $G$ has no $T K ^ { 5 }$ or $T K _ { 3 , 3 }$ subgraph, then neither $G _ { 1 } + x y$ nor $G _ { 2 } + x y$ has such a subgraph, and we may try to combine drawings of these graphs to one of $G + x y$ . (If $x y$ is already an edge of $G$ , the same can be done with $G _ { 1 }$ and $G _ { 2 }$ .) For $\kappa ( G ) \leqslant 1$ , things become even simpler. However, the geometric operations involved require some cumbersome shifting and scaling, even if all the plane edges occurring are assumed to be straight.

The following more combinatorial route is just as easy, and may be a welcome alternative.

Lemma 4.4.4. Let $\mathcal { X }$ be a set of 3-connected graphs. Let $G$ be a graph with $\kappa ( G ) \leqslant 2$ , and let $G _ { 1 } , G _ { 2 }$ be proper induced subgraphs of $G$ such that $G = G _ { 1 } \cup G _ { 2 }$ and $| G _ { 1 } \cap G _ { 2 } | = \kappa ( G )$ . If $G$ is edge-maximal without a topological minor in $\mathcal { X }$ , then so are $G _ { 1 }$ and $G _ { 2 }$ , and $G _ { 1 } \cap G _ { 2 } = K ^ { 2 }$ .

Proof . Note first that every vertex $v \in S : = V ( G _ { 1 } \cap G _ { 2 } )$ has a neighbour in every component of $G _ { i } - S$ , $i = 1 , 2$ : otherwise $S \setminus \{ v \}$ would separate $G$ , contradicting $| S | = \kappa ( G )$ . By the maximality of $G$ , every edge $e$ added to $G$ lies in a $T X \subseteq G + e$ with $X ~ \in ~ \mathcal { X }$ . For all the choices of $e$ considered below, the 3-connectedness of $X$ will imply that the branch vertices of this $_ { T X }$ all lie in the same $G _ { i }$ , say in $G _ { 1 }$ . (The position of $e$ will always be symmetrical with respect to $G _ { 1 }$ and $G _ { 2 }$ , so this assumption entails no loss of generality.) Then the $T X$ meets $G _ { 2 }$ at most in a path $P$ corresponding to an edge of $X$ .

If $S = \emptyset$ , we obtain an immediate contradiction by choosing $e$ with one end in $G _ { 1 }$ and the other in $G _ { 2 }$ . If $S \ : = \ : \{ v \ : \}$ is a singleton, let $e$ join a neighbour $v _ { 1 }$ of $v$ in $G _ { 1 } - S$ to a neighbour $v _ { 2 }$ of $v$ in $G _ { 2 } - S$ (Fig. 4.4.3). Then $P$ contains both $\boldsymbol { v }$ and the edge $v _ { 1 } v _ { 2 }$ ; replacing $v P v _ { 1 }$ with the edge $v v _ { 1 }$ , we obtain a $_ { T X }$ in $G _ { 1 } \subseteq G$ , a contradiction.

So $| S | = 2$ , say $S = \{ x , y \}$ . If $x y \notin G$ , we let $e : = x y$ , and in the arising $_ { T X }$ replace $e$ by an $x$ – $_ y$ path through $G _ { 2 }$ ; this gives a $_ { T X }$ in $G$ , a contradiction. Hence $x y \in G$ , and $G \left[ S \right] = K ^ { 2 }$ as claimed.

It remains to show that $G _ { 1 }$ and $G _ { 2 }$ are edge-maximal without a topological minor in $\mathcal { X }$ . So let $e ^ { \prime }$ be an additional edge for $G _ { 1 }$ , say. Replacing $x P y$ with the edge $x y$ if necessary, we obtain a $_ { T X }$ either

[ 7.3.1 ]

$S$

$X$

$P$

$x , y$

![](images/2db94e4864beadc7f11a9d239d380de4985a2b994f8dfb410d6c92f7b58b5c09.jpg)  
Fig. 4.4.3. If $G + e$ contains a $T X$ , then so does $G _ { 1 }$ or $G _ { 2 }$

in $G _ { 1 } + e ^ { \prime }$ (which shows the edge-maximality of $G _ { 1 }$ , as desired) or in $G _ { 2 }$ 2 (which contradicts $G _ { 2 } \subseteq G$ ). 

Lemma 4.4.5. If $| G | \geqslant 4$ and $G$ is edge-maximal with $T K ^ { 5 } , T K _ { 3 , 3 } \not \subseteq G$ , then $G$ is 3-connected.

(4.2.11)   
$G _ { 1 } , G _ { 2 }$   
$x , y$   
$f _ { i }$   
$z _ { i }$   
$K$

Proof . We apply induction on $| G |$ . For $| G | = 4$ , we have $G = K ^ { 4 }$ and the assertion holds. Now let $| G | > 4$ , and let $G$ be edge-maximal without a $T K ^ { 5 }$ or $T K _ { 3 , 3 }$ . Suppose $\kappa ( G ) \leqslant 2$ , and choose $G _ { 1 }$ and $G _ { 2 }$ as in Lemma 4.4.4. For $\mathcal { X } : = \{ K ^ { 5 } , K _ { 3 , 3 } \}$ , the lemma says that $G _ { 1 } \cap G _ { 2 }$ is a $K ^ { 2 }$ , with vertices $x , y$ say. By Lemmas 4.4.4, 4.4.3 and the induction hypothesis, $G _ { 1 }$ and $G _ { 2 }$ are planar. For each $i = 1 , 2$ separately, choose a drawing of $G _ { i }$ , a face $f _ { i }$ with the edge $x y$ on its boundary, and a vertex $z _ { i } ~ \neq ~ x , y$ on the boundary of $f _ { i }$ . Let $K$ be a $T K ^ { 5 }$ or $T K _ { 3 , 3 }$ in the abstract graph $G + z _ { 1 } z _ { 2 }$ (Fig. 4.4.4).

![](images/586032c09807d5aa00343f893fc16f77c4247f057c0a7a9b95e3b716668e387d.jpg)  
Fig. 4.4.4. A $T K ^ { 5 }$ o r $T K _ { 3 , 3 }$ in $G + z _ { 1 } z _ { 2 }$

If all the branch vertices of $K$ lie in the same $G _ { i }$ , then either $G _ { i } + x z _ { i }$ or $G _ { i } + y z _ { i }$ (or $G _ { i }$ itself, if $z _ { i }$ is already adjacent to $x$ or $y$ , respectively) contains a $T K ^ { 5 }$ or $T K _ { 3 , 3 }$ ; this contradicts Corollary 4.2.11, since these graphs are planar by the choice of $z _ { i }$ . Since $G + z _ { 1 } z _ { 2 }$ does not contain four independent paths between $\left( G _ { 1 } - G _ { 2 } \right)$ and $\left( G _ { 2 } - G _ { 1 } \right)$ , these subgraphs cannot both contain a branch vertex of a $T K ^ { 5 }$ , and cannot both contain two branch vertices of a $T K _ { 3 , 3 }$ . Hence $K$ is a $T K _ { 3 , 3 }$ with only one branch vertex $v$ in, say, $G _ { 2 } - G _ { 1 }$ . But then also the graph $G _ { 1 } + v + \{ v x , v y , v z _ { 1 } \}$ , which is planar by the choice of $z _ { 1 }$ , contains a $T K _ { 3 , 3 }$ . This contradicts Corollary 4.2.11. 

Theorem 4.4.6. (Kuratowski 1930; Wagner 1937) [ 4.5.1 ]

The following assertions are equivalent for graphs $G$ :

(i) $G$ is planar;   
(ii) $G$ contains neither $K ^ { 5 }$ nor $K _ { 3 , 3 }$ as a minor;   
(iii) $G$ contains neither $K ^ { 5 }$ nor $K _ { 3 , 3 }$ as a topological minor.

Proof . Combine Corollary 4.2.11 with Lemmas 4.4.2, 4.4.3 and 4.4.5. (4.2.11)

![](images/0e9883f2df928875c17a053d2fd29dd5021af3e3e6e5d507fbe5d6dcac3014cf.jpg)

Corollary 4.4.7. Every maximal planar graph with at least four vertices is 3-connected.

Proof . Apply Lemma 4.4.5 and Theorem 4.4.6.

# 4.5 Algebraic planarity criteria

One of the most conspicuous features of a plane graph $G$ are its facial cycles, the cycles that bound a face. If $G$ is 2-connected it is covered by its facial cycles, so in a sense these form a ‘large’ set. In fact, the set of facial cycles is large even in the sense that they generate the entire cycle space: every cycle in $G$ is easily seen to be the sum of the facial cycles (see below). On the other hand, the facial cycles only cover $G$ ‘thinly’, as every edge lies on at most two of them. Our first aim in this section is to show that the existence of such a large yet thinly spread family of cycles is not only a conspicuous feature of planarity but lies at its very heart: it characterizes it.

Let $G = ( V , E )$ be any graph. We call a subset $\mathcal { F }$ of its edge space ${ \mathcal { E } } ( G )$ simple if every edge of $G$ lies in at most two sets of $\mathcal { F }$ . For example, the cut space ${ \mathcal { C } } ^ { * } ( G )$ has a simple basis: according to Proposition 1.9.3 it is generated by the cuts $E ( v )$ formed by all the edges at a given vertex $\boldsymbol { v }$ , and an edge $x y \in G$ lies in $E ( v )$ only for $v = x$ and for $v = y$ .

Theorem 4.5.1. (MacLane 1937) [ 4.6.3 ]

A graph is planar if and only if its cycle space has a simple basis.

Proof . The assertion being trivial for graphs of order at most 2, we consider a graph $G$ of order at least 3. If $\kappa ( G ) \leqslant 1$ , then $G$ is the union (1.9.2) of two proper induced subgraphs $G _ { 1 } , G _ { 2 }$ with $| G _ { 1 } \cap G _ { 2 } | \leqslant 1$ . Then ${ \mathcal { C } } ( G )$ (4.1.1) is the direct sum of $\mathcal { C } ( G _ { 1 } )$ and $\mathcal { C } ( G _ { 2 } )$ , and hence has a simple basis if (4.2.2) and only if both $\mathcal { C } ( G _ { 1 } )$ and $\mathcal { C } ( G _ { 2 } )$ do (proof?). Moreover, $G$ is planar if (4.2.6)(4.4.6)

We first assume that $G$ is planar and choose a drawing. By Proposition 4.2.6, the face boundaries of $G$ are cycles, so they are elements of ${ \mathcal { C } } ( G )$ . We shall show that the face boundaries generate all the cycles in $G$ ; then ${ \mathcal { C } } ( G )$ has a simple basis by Lemma 4.2.2. Let $C \subseteq G$ be any cycle, and let $f$ be its inner face. By Lemma 4.2.2, every edge $e$ with ${ \overset { \circ } { e } } \subseteq f$ lies on exactly two face boundaries $G \left[ f ^ { \prime } \right]$ with $f ^ { \prime } \subseteq f$ , and every edge of $C$ lies on exactly one such face boundary. Hence the sum in ${ \mathcal { C } } ( G )$ of all those face boundaries is exactly $C$ .

Conversely, let $\{ C _ { 1 } , \ldots , C _ { k } \}$ be a simple basis of ${ \mathcal { C } } ( G )$ . Then, for every edge $\textit { e } \in \textit { G }$ , also $\mathcal { C } ( G - e )$ has a simple basis. Indeed, if $e$ lies in just one of the sets $C _ { i }$ , say in $C _ { 1 }$ , then $\{ C _ { 2 } , \ldots , C _ { k } \}$ is a simple basis of $\mathcal { C } ( G - e )$ ; if $e$ lies in two of the $C _ { i }$ , say in $C _ { 1 }$ and $C _ { 2 }$ , then $\{ C _ { 1 } + C _ { 2 } , C _ { 3 } , \ldots , C _ { k } \}$ is such a basis. (Note that the two bases are indeed subsets of ${ \mathcal { C } } ( G - e )$ by Proposition 1.9.2.) Thus every subgraph of $G$ has a cycle space with a simple basis. For our proof that $G$ is planar, it thus suffices to show that the cycle spaces of $K ^ { 5 }$ and $K _ { 3 , 3 }$ (and hence those of their subdivisions) do not have a simple basis: then $G$ cannot contain a $T K ^ { 5 }$ or $T K _ { 3 , 3 }$ , and so is planar by Kuratowski’s theorem.

Let us consider $K ^ { 5 }$ first. By Theorem 1.9.6, $\dim { \mathcal { C } } ( K ^ { 5 } ) = 6$ ; let $B = \{ C _ { 1 } , \ldots , C _ { 6 } \}$ be a simple basis, and put $C _ { 0 } : = C _ { 1 } + . . . + C _ { 6 }$ . As $\boldsymbol { B }$ is linearly independent, none of the sets $C _ { 0 } , \ldots , C _ { 6 }$ is empty, so each of them contains at least three edges (cf. Proposition 1.9.2). Moreover, as every edge from $C _ { 0 }$ lies in just one of $C _ { 1 } , \ldots , C _ { 6 }$ , the set $\{ C _ { 0 } , \ldots , C _ { 6 } \}$ is still simple. But this implies that $K ^ { 5 }$ should have more edges than it does, i.e. we obtain the contradiction of

$$
2 1 = 7 \cdot 3 \leqslant | C _ {0} | + \dots + | C _ {6} | \leqslant 2 \| K ^ {5} \| = 2 0.
$$

For $K _ { 3 , 3 }$ , Theorem 1.9.6 gives $\dim { \mathcal { C } } ( K _ { 3 , 3 } ) = 4$ ; let $\boldsymbol { B } = \{ \boldsymbol { C } _ { 1 } , \ldots , \boldsymbol { C } _ { 4 } \}$ be a simple basis, and put $C _ { 0 } : = C _ { 1 } + . . . + C _ { 4 }$ . As $K _ { 3 , 3 }$ has girth 4, each $C _ { i }$ contains at least four edges. We then obtain the contradiction of

$$
2 0 = 5 \cdot 4 \leqslant | C _ {0} | + \dots + | C _ {4} | \leqslant 2 \| K _ {3, 3} \| = 1 8.
$$

It is one of the hidden beauties of planarity theory that two such abstract and seemingly unintuitive results about generating sets in cycle spaces as MacLane’s theorem and Tutte’s theorem 3.2.3 conspire to produce a very tangible planarity criterion for 3-connected graphs:

# Theorem 4.5.2. (Kelmans 1978)

A 3-connected graph is planar if and only if every edge lies on at most (equivalently: exactly) two non-separating induced cycles.

Proof . The forward implication follows from Propositions 4.2.7 and 4.2.2 (and Proposition 4.2.6 for the ‘exactly two’ version); the backward implication follows from Theorems 3.2.3 and 4.5.1. 

# 4.6 Plane duality

In this section we shall use MacLane’s theorem to uncover another connection between planarity and algebraic structure: a connection between the duality of plane graphs, defined below, and the duality of the cycle and cut space hinted at in Chapters 1.9 and 2.4.

A plane multigraph is a pair $G = ( V , E )$ of finite sets (of vertices and edges, respectively) satisfying the following conditions:

(i) $V \subseteq \mathbb { R } ^ { 2 }$   
(ii) every edge is either an arc between two vertices or a polygon containing exactly one vertex (its endpoint);   
(iii) apart from its own endpoint(s), an edge contains no vertex and no point of any other edge.

We shall use terms defined for plane graphs freely for plane multigraphs. Note that, as in abstract multigraphs, both loops and double edges count as cycles.

Let us consider the plane multigraph $G$ shown in Figure 4.6.1. Let us place a new vertex inside each face of $G$ and link these new vertices up to form another plane multigraph $G ^ { * }$ , as follows: for every edge $e$ of $G$ we link the two new vertices in the faces incident with $e$ by an edge $e ^ { * }$ crossing $e$ ; if $e$ is incident with only one face, we attach a loop $e ^ { * }$ to the new vertex in that face, again crossing the edge $e$ . The plane multigraph $G ^ { * }$ formed in this way is then dual to $G$ in the following sense: if we apply the same procedure as above to $G ^ { * }$ , we obtain a plane multigraph very similar to $G$ ; in fact, $G$ itself may be reobtained from $G ^ { * }$ in this way.

![](images/694f9c632f424b5c4c5c50a7c9796e15e872f41fa7cd7c6a33ca1601170ee138.jpg)  
Fig. 4.6.1. A plane graph and its dual

To make this idea more precise, let $G = ( V , E )$ and $( V ^ { * } , E ^ { * } )$ be any two plane multigraphs, and put $F ( G ) = : F$ and $F ( ( V ^ { * } , E ^ { * } ) ) = : F ^ { * }$ . We call $( V ^ { * } , E ^ { * } )$ a plane dual of $G$ , and write $( V ^ { * } , E ^ { * } ) = : G ^ { * }$ , if there are bijections

$$
F \to V ^ {*} \qquad E \to E ^ {*} \qquad V \to F ^ {*}
$$

$$
f \mapsto v ^ {*} (f) \qquad e \mapsto e ^ {*} \qquad v \mapsto f ^ {*} (v)
$$

satisfying the following conditions:

(i) $v ^ { * } ( f ) \in f$ for all $f \in F$ ;   
(ii) $| e ^ { * } \cap G | = | \mathring { e } ^ { * } \cap \mathring { e } | = | e \cap G ^ { * } | = 1$ for all $e \in E$ , and in each of $e$ and $e ^ { * }$ this point is an inner point of a straight line segment;   
(iii) $v \in f ^ { \ast } ( v )$ for all $v \in V$ .

Every connected plane multigraph has a plane dual. Indeed, to satisfy condition (i) we start by picking from each face $f$ of $G$ a point $v ^ { * } ( f )$ as a vertex for $G ^ { * }$ . We can then link these vertices up by independent arcs as required by (ii), and using the connectedness of $G$ show that there is indeed a bijection $V \to F ^ { * }$ satisfying (iii) (Exercise 27).

If $G _ { 1 } ^ { * }$ and $G _ { 2 } ^ { * }$ are two plane duals of $G$ , then clearly $G _ { 1 } ^ { * } \simeq G _ { 2 } ^ { * }$ ; in fact, one can show that the natural bijection $v _ { 1 } ^ { * } ( f ) \mapsto v _ { 2 } ^ { * } ( f )$ is a topological isomorphism between $G _ { 1 } ^ { * }$ and $G _ { 2 } ^ { * }$ . In this sense, we may speak of the plane dual $G ^ { * }$ of $G$ .

Finally, $G$ is in turn a plane dual of $G ^ { * }$ . Indeed, this is witnessed by the inverse maps of the bijections from the definition of $G ^ { * }$ : setting $v ^ { * } ( f ^ { * } ( v ) ) : = v$ and $f ^ { * } ( v ^ { * } ( f ) ) : = f$ for $f ^ { \ast } ( v ) \in F ^ { \ast }$ and $v ^ { * } ( f ) \in V ^ { * }$ , we see that conditions (i) and (iii) for $G ^ { * }$ transform into (iii) and (i) for $G$ , while condition (ii) is symmetrical in $G$ and $G ^ { * }$ . As duals are easily seen to be connected (Exercise 26), this symmetry implies that connectedness is also a necessary condition for $G$ to have a dual.

Perhaps the most interesting aspect of plane duality is that it relates geometrically two types of edges sets—cycles and bonds—that we have previously seen to be algebraically related (Theorem 1.9.5):

[ 6.5.2 ]

(4.1.1) (4.2.4)

Proposition 4.6.1. For any connected plane multigraph $G$ , an edge set $E \subseteq E ( G )$ is the edge set of a cycle in $G$ if and only if $E ^ { * } : = \{ e ^ { * } \mid e \in E \}$ is a minimal cut in $G ^ { * }$ .

Proof . By conditions (i) and (ii) in the definition of $G ^ { * }$ , two vertices $v ^ { * } ( f _ { 1 } )$ and $v ^ { * } ( f _ { 2 } )$ of $G ^ { * }$ lie in the same component of $G ^ { * } - E ^ { * }$ if and only if $f _ { 1 }$ and $f _ { 2 }$ lie in the same region of $\mathbb { R } ^ { 2 } \setminus \bigcup E$ : every $v ^ { * } ( f _ { 1 } ) – v ^ { * } ( f _ { 2 } )$ path in $G ^ { * } - E ^ { * }$ is an arc between $f _ { 1 }$ and $f _ { 2 }$ in $\mathbb { R } ^ { 2 } \setminus \bigcup E$ , and conversely every such arc $P$ (with $P \cap V ( G ) = \emptyset$ ) defines a walk in $G ^ { * } - E ^ { * }$ between $v ^ { * } ( f _ { 1 } )$ and $v ^ { * } ( f _ { 2 } )$ .

Now if $C \subseteq G$ is a cycle and $E = E ( C )$ then, by the Jordan curve theorem and the above correspondence, $G ^ { * } - E ^ { * }$ has exactly two components, so $E ^ { * }$ is a minimal cut in $G ^ { * }$ .

Conversely, if $E \subseteq E ( G )$ is such that $E ^ { * }$ is a cut in $G ^ { * }$ , then, by Proposition 4.2.4 and the above correspondence, $E$ contains the edges of a cycle $C \subseteq G$ . If $E ^ { * }$ is minimal as a cut, then $E$ cannot contain any further edges (by the implication shown before), so $E = E ( C )$ . 

Proposition 4.6.1 suggests the following generalization of plane duality to abstract multigraphs. Call a multigraph $G ^ { * }$ an abstract dual of a multigraph $G$ if $E ( G ^ { * } ) = E ( G )$ and the bonds in $G ^ { * }$ are precisely the edge sets of cycles in $G$ . (Neither $G$ nor $G ^ { * }$ need be connected now.)

This correspondence between cycles and bonds extends to the spaces they generate:

Proposition 4.6.2. If $G ^ { * }$ is an abstract dual of $G$ , then the cut space of $G ^ { * }$ is the cycle space of $G$ , i.e.,

$$
\mathcal {C} ^ {*} (G ^ {*}) = \mathcal {C} (G).
$$

Proof . Since the cycles of $G$ are precisely the bonds of $G ^ { * }$ , the subspace (1.9.4) ${ \mathcal { C } } ( G )$ they generate in ${ \mathcal { E } } ( G ) = { \mathcal { E } } ( G ^ { * } )$ is the same as the subspace generated by the bonds in $G ^ { * }$ . By Lemma 1.9.4,5 this is the space ${ \mathcal { C } } ^ { * } ( G ^ { * } )$ .



By Theorem 1.9.5, Proposition 4.6.2 implies at once that if $G ^ { * }$ is an abstract dual of $G$ then $G$ is an abstract dual of $G ^ { * }$ . One can show that if $G$ is 3-connected, then $G ^ { * }$ is unique up to isomorphism.

Although the notion of abstract duality arose as a generalization of plane duality, it could have been otherwise. We knew already from Theorem 1.9.5 that the cycles and the bonds of a graph form natural and related sets of edges. It would not have been unthinkable to ask whether, for some graphs, the orthogonality between these collections of edge sets might give them sufficiently similar intersection patterns that a collection forming the cycles in one graph could form the bonds in another, and vice versa. In other words, for which graphs can we move their entire edge set to a new set of vertices, redefining incidences, so that precisely those sets of edges that used to form cycles now become bonds (and vice versa)? Put in this way, it seems surprising that this could ever be achieved, let alone for such a large and natural class of graphs as all planar graphs.

As the one of the highlights of classical planarity theory we now show that the planar graphs are precisely those for which this can be done. Admitting an abstract dual thus appears as a new planarity criterion. Conversely, the theorem can be read as a surprising topological characterization of the equally fundamental property of admitting an abstract dual:

# Theorem 4.6.3. (Whitney 1933)

A graph is planar if and only if it has an abstract dual.

Proof . Let $G$ be a planar graph, and consider any drawing. Every component6 $C$ of this drawing has a plane dual $C ^ { * }$ . Consider these $C ^ { * }$ as abstract multigraphs, and let $G ^ { * }$ be their disjoint union. Then the bonds of $G ^ { * }$ are precisely the minimal cuts in the $C ^ { * }$ , which by Proposition 4.6.1 correspond to the cycles in $G$ .

Conversely, suppose that $G$ has an abstract dual $G ^ { * }$ . For a proof that $G$ is planar, it suffices by Theorem 4.5.1 and Proposition 4.6.2 to show that ${ \mathcal { C } } ^ { * } ( G ^ { * } )$ has a simple basis. By Proposition 1.9.3, it does. 

The duality theory for both abstract and plane graphs can be extended to infinite graphs. As these can have infinite bonds, their duals must then have ‘infinite cycles’. Such things do indeed exist, and are quite fascinating: they arise as topological circles in a space formed by the graph and its ends; see Chapter 8.5.

# Exercises

1. Show that every graph can be embedded in $\mathbb { R } ^ { 3 }$ with all edges straight.   
2. $-$ Show directly by Lemma 4.1.2 that $K _ { 3 , 3 }$ is not planar.   
3. $-$ Find an Euler formula for disconnected graphs.   
4. Show that every connected planar graph with $n$ vertices, $m$ edges and finite girth $g$ satisfies $\begin{array} { r } { m \leqslant \frac { g } { g - 2 } ( n - 2 ) } \end{array}$ .   
5. Show that every planar graph is a union of three forests.   
6. Let $G _ { 1 } , G _ { 2 } , \ldots$ . be an infinite sequence of pairwise non-isomorphic graphs. Show that if $\operatorname* { l i m } \operatorname* { s u p } \varepsilon ( G _ { i } ) > 3$ then the graphs $G _ { i }$ have unbounded genus—that is to say, there is no (closed) surface $S$ in which all the $G _ { i }$ can be embedded.

(Hint. You may use the fact that for every surface $S$ there is a constant $\chi ( S ) \leqslant 2$ such that every graph embedded in $S$ satisfies the generalized Euler formula of $n - m + \ell \geqslant \chi ( S )$ .)

7. Find a direct proof for planar graphs of Tutte’s theorem on the cycle space of 3-connected graphs (Theorem 3.2.3).   
8.− Show that the two plane graphs in Figure 4.3.1 are not combinatorially (and hence not topologically) isomorphic.   
9. Show that the two graphs in Figure 4.3.2 are combinatorially but not topologically isomorphic.   
10.− Show that our definition of equivalence for planar embeddings does indeed define an equivalence relation.

11. Find a 2-connected planar graph whose drawings are all topologically isomorphic but whose planar embeddings are not all equivalent.   
12.+ Show that every plane graph is combinatorially isomorphic to a plane graph whose edges are all straight.

(Hint. Given a plane triangulation, construct inductively a graphtheoretically isomorphic plane graph whose edges are straight. Which additional property of the inner faces could help with the induction?)

Do not use Kuratowski’s theorem in the following two exercises.

13. Show that any minor of a planar graph is planar. Deduce that a graph is planar if and only if it is the minor of a grid. (Grids are defined in Chapter 12.3.)   
14. (i) Show that the planar graphs can in principle be characterized as in Kuratowski’s theorem, i.e., that there exists a set $\mathcal { X }$ of graphs such that a graph $G$ is planar if and only if $G$ has no topological minor in $\mathcal { X }$ .   
(ii) More generally, which graph properties can be characterized in this way?   
15.− Does every planar graph have a drawing with all inner faces convex?   
16. Modify the proof of Lemma 4.4.3 so that all inner faces become convex.   
17. Does every minimal non-planar graph $G$ (i.e., every non-planar graph $G$ whose proper subgraphs are all planar) contain an edge $e$ such that $G - e$ is maximally planar? Does the answer change if we define ‘minimal’ with respect to minors rather than subgraphs?   
18. Show that adding a new edge to a maximal planar graph of order at least 6 always produces both a $T K ^ { 5 }$ and a $T K _ { 3 , 3 }$ subgraph.   
19. Prove the general Kuratowski theorem from its 3-connected case by manipulating plane graphs, i.e. avoiding Lemma 4.4.5.

(This is not intended as an exercise in elementary topology; for the topological parts of the proof, a rough sketch will do.)

20. A graph is called outerplanar if it has a drawing in which every vertex lies on the boundary of the outer face. Show that a graph is outerplanar if and only if it contains neither $K ^ { 4 }$ nor $K _ { 2 , 3 }$ as a minor.   
21. Let $G = G _ { 1 } \cup G _ { 2 }$ , where $\left| G _ { 1 } \cap G _ { 2 } \right| \leqslant 1$ . Show that ${ \mathcal { C } } ( G )$ has a simple basis if both $\mathcal { C } ( G _ { 1 } )$ and $\mathcal { C } ( G _ { 2 } )$ have one.   
22.+ Find a cycle space basis among the face boundaries of a 2-connected plane graph.   
23. Show that a 2-connected plane graph is bipartite if and only if every face is bounded by an even cycle.

24.+ Let $C$ be a closed curve in the plane that intersects itself at most once in any given point of the plane, and where every such self-intersection is a proper crossing. Call $C$ alternating if we can turn these crossings into over- and underpasses in such a way that when we run along the curve the overpasses alternate with the underpasses.

(i) Prove that every such curve is alternating, or find a counterexample.   
(ii) Does the solution to (i) change if the curves considered are not closed?

25. $-$ What does the plane dual of a plane tree look like?   
26. $-$ Show that the plane dual of a plane multigraph is connected.   
27.+ Show that a connected plane multigraph has a plane dual.   
28. Let $G , G ^ { * }$ be dual plane multigraphs, and let $e ~ \in ~ E ( G )$ . Prove the following statements (with a suitable definition of $G / e$ ):

(i) If $e$ is not a bridge, then $G ^ { * } / e ^ { * }$ is a plane dual of $G - e$   
(ii) If $e$ is not a loop, then $G ^ { * } - e ^ { * }$ is a plane dual of $G / e$ .

29. Show that any two plane duals of a plane multigraph are combinatorially isomorphic.   
30. Let $G , G ^ { * }$ be dual plane graphs. Prove the following statements:

(i) If $G$ is 2-connected, then $G ^ { * }$ is 2-connected.   
(ii) If $G$ is 3-connected, then $G ^ { * }$ is 3-connected.   
(iii) If $G$ is 4-connected, then $G ^ { * }$ need not be 4-connected.

31. Let $G , G ^ { * }$ be dual plane graphs. Let $B _ { 1 } , \ldots , B _ { n }$ be the blocks of $G$ . Show that $B _ { 1 } ^ { * } , \ldots , B _ { n } ^ { * }$ are the blocks of $G ^ { * }$ .   
32. Show that if $G ^ { * }$ is an abstract dual of a multigraph $G$ , then $G$ is an abstract dual of $G ^ { * }$ .   
33. Show that the following statements are equivalent for connected multigraphs $G = ( V , E )$ and $G ^ { \prime } = ( V ^ { \prime } , E )$ with the same edge set:

(i) $G$ and $G ^ { \prime }$ are abstract duals of each other;

(ii) given any set $F ^ { \prime } \subseteq E$ , the multigraph $( V , F )$ is a tree if and only if $( V ^ { \prime } , E \setminus F )$ is a tree.

# Notes

There is a very thorough monograph on the embedding of graphs in surfaces, including the plane: B. Mohar & C. Thomassen, Graphs on Surfaces, Johns Hopkins University Press 2001. Proofs of the results cited in Section 4.1, as well as all references for this chapter, can be found there. A good account of the Jordan curve theorem, both polygonal and general, is given also in J. Stillwell, Classical topology and combinatorial group theory, Springer 1980.

The short proof of Corollary 4.2.10 uses a trick that deserves special mention: the so-called double counting of pairs, illustrated in the text by a bipartite graph whose edges can be counted alternatively by summing its degrees on the left or on the right. Double counting is a technique widely used in combinatorics, and there will be more examples later in the book.

The material of Section 4.3 is not normally standard for an introductory graph theory course, and the rest of the chapter can be read independently of this section. However, the results of Section 4.3 are by no means unimportant. In a way, they have fallen victim to their own success: the shift from a topological to a combinatorial setting for planarity problems which they achieve has made the topological techniques developed there dispensable for most of planarity theory.

In its original version, Kuratowski’s theorem was stated only for topological minors; the version for general minors was added by Wagner in 1937. Our proof of the 3-connected case (Lemma 4.4.3) is a weakening of a proof due to C. Thomassen, Planarity and duality of finite and infinite graphs, J. Combin. Theory B 29 (1980), 244–271, which yields a drawing in which all the inner faces are convex (Exercise 16). The existence of such ‘convex’ drawings for 3-connected planar graphs follows already from the theorem of Steinitz (1922) that these graphs are precisely the 1-skeletons of 3-dimensional convex polyhedra. Compare also W.T. Tutte, How to draw a graph, Proc. London Math. Soc. 13 (1963), 743–767.

As one readily observes, adding an edge to a maximal planar graph (of order at least 6) produces not only a topological $K ^ { 5 }$ or $K _ { 3 , 3 }$ , but both. In Chapter 7.3 we shall see that, more generally, every graph with $n$ vertices and more than $3 n - 6$ edges contains a $T K ^ { \mathrm { { \upsilon } } }$ and, with one easily described class of exceptions, also a $T K _ { 3 , 3 }$ (Ex. 26, Ch. 7).

The simple cycle space basis constructed in the proof of MacLane’s theorem, which consists of the inner face boundaries, is canonical in the following sense: for every simple basis $\boldsymbol { B }$ of the cycle space of a 2-connected planar graph there exists a drawing of that graph in which $\boldsymbol { B }$ is precisely the set of inner face boundaries. (This is proved in Mohar & Thomassen, who also mention some further planarity criteria.) Our proof of the backward direction of MacLane’s theorem is based on Kuratowski’s theorem. A more direct approach, in which a planar embedding is actually constructed from a simple basis, is adopted in K. Wagner, Graphentheorie, BI Hochschultaschenb¨ucher 1972.

Theorem 4.5.2 is widely known as ‘Tutte’s planarity criterion’, because it follows at once from Tutte’s 1963 Theorem 3.2.3 and the even earlier planarity criterion of MacLane, Theorem 4.5.1. However, Tutte appears to have been unaware of this. Theorem 4.5.2 was first noticed in the late 1970s, and proved independently of both Theorems 3.2.3 and 4.5.1, by A.K. Kelmans, The

concept of a vertex in a matroid, the non-separating cycles in a graph and a new criterion for graph planarity, in Algebraic Methods in Graph Theory, Vol. 1, Conf. Szeged 1978, Colloq. Math. Soc. J´anos Bolyai 25 (1981) 345–388. Kelmans also reproved Theorem 3.2.3 (being unaware of Tutte’s proof), and noted that it can be combined with MacLane’s criterion to a proof of Theorem 4.5.2.

The proper setting for cycle-bond duality in abstract finite graphs (and beyond) is the theory of matroids; see J.G. Oxley, Matroid Theory, Oxford University Press 1992. Duality in infinite graphs is treated in H. Bruhn & R. Diestel, Duality in infinite graphs, Combinatorics, Probability and Computing (to appear).

How many colours do we need to colour the countries of a map in such a way that adjacent countries are coloured differently? How many days have to be scheduled for committee meetings of a parliament if every committee intends to meet for one day and some members of parliament serve on several committees? How can we find a school timetable of minimum total length, based on the information of how often each teacher has to teach each class?

A vertex colouring of a graph $G = ( V , E )$ is a map $c \colon V  S$ such that $c ( v ) \neq c ( w )$ whenever $\boldsymbol { v }$ and $w$ are adjacent. The elements of the set $S$ are called the available colours. All that interests us about $S$ is its size: typically, we shall be asking for the smallest integer $k$ such that $G$ has a $k$ -colouring, a vertex colouring $c \colon V \to \{ 1 , \ldots , k \}$ . This $k$ is the (vertex-) chromatic number of $G$ ; it is denoted by $\chi ( G )$ . A graph $G$ with $\chi ( G ) = k$ is called $k$ -chromatic; if $\chi ( G ) \leqslant k$ , we call $G$ $k$ -colourable.

![](images/22e01468be1bd8089ba5ebb42569dbbacba277fcb168be37314a3e91fc31cf4f.jpg)  
Fig. 5.0.1. A vertex colouring $V  \{ 1 , \dots , 4 \}$

Note that a $k$ -colouring is nothing but a vertex partition into $k$ independent sets, now called colour classes; the non-trivial 2-colourable graphs, for example, are precisely the bipartite graphs. Historically, the colouring terminology comes from the map colouring problem stated

edge colouring chromatic index $\chi ^ { \prime } ( G )$

above, which leads to the problem of determining the maximum chromatic number of planar graphs. The committee scheduling problem, too, can be phrased as a vertex colouring problem—how?

An edge colouring of $G = ( V , E )$ is a map $c \colon E \to S$ with $c ( e ) \neq c ( f )$ for any adjacent edges $e , f$ . The smallest integer $k$ for which a $k$ -edgecolouring exists, i.e. an edge colouring $c \colon E \to \{ 1 , \ldots , k \}$ , is the edgechromatic number , or chromatic index , of $G$ ; it is denoted by $\chi ^ { \prime } ( G )$ . The third of our introductory questions can be modelled as an edge colouring problem in a bipartite multigraph (how?).

Clearly, every edge colouring of $G$ is a vertex colouring of its line graph $L ( G )$ , and vice versa; in particular, $\chi ^ { \prime } ( G ) = \chi ( L ( G ) )$ . The problem of finding good edge colourings may thus be viewed as a restriction of the more general vertex colouring problem to this special class of graphs. As we shall see, this relationship between the two types of colouring problem is reflected by a marked difference in our knowledge about their solutions: while there are only very rough estimates for $\chi$ , its sister $\chi ^ { \prime }$ always takes one of two values, either $\Delta$ or $\Delta + 1$ .

# 5.1 Colouring maps and planar graphs

If any result in graph theory has a claim to be known to the world outside, it is the following four colour theorem (which implies that every map can be coloured with at most four colours):

# Theorem 5.1.1. (Four Colour Theorem)

Every planar graph is 4-colourable.

Some remarks about the proof of the four colour theorem and its history can be found in the notes at the end of this chapter. Here, we prove the following weakening:

# Proposition 5.1.2. (Five Colour Theorem)

Every planar graph is 5-colourable.

(4.1.1) (4.2.10)

$n , m$

$_ v$ $H$ $c$

Proof . Let $G$ be a plane graph with $n \geqslant 6$ vertices and $m$ edges. We assume inductively that every plane graph with fewer than $n$ vertices can be 5-coloured. By Corollary 4.2.10,

$$
d (G) = 2 m / n \leqslant 2 (3 n - 6) / n <   6;
$$

let $v \in G$ be a vertex of degree at most 5. By the induction hypothesis, the graph $H : = G - v$ has a vertex colouring $c \colon V ( H ) \to \{ 1 , \ldots , 5 \}$ . If $c$ uses at most 4 colours for the neighbours of $v$ , we can extend it to a 5- colouring of $G$ . Let us assume, therefore, that $v$ has exactly 5 neighbours, and that these have distinct colours.

Let $D$ be an open disc around $v$ , so small that it meets only those five straight edge segments of $G$ that contain $v$ . Let us enumerate these segments according to their cyclic position in $D$ as $s _ { 1 } , \ldots , s _ { 5 }$ , and let $v v _ { i }$ be the edge containing $s _ { i }$ $i = 1 , \ldots , 5$ ; Fig. 5.1.1). Without loss of generality we may assume that $c ( v _ { i } ) = i$ for each $i$ .

D

$$
s _ {1}, \ldots , s _ {5}
$$

$$
v _ {1}, \dots , v _ {5}
$$

![](images/75802bab9f7ac390ccf55766d159135e564ebd2026f6c6219a7c63b4722dceb3.jpg)  
Fig. 5.1.1. The proof of the five colour theorem

Let us show first that every $v _ { 1 } - v _ { 3 }$ path $P \subseteq H$ separates $v _ { 2 }$ from $v _ { 4 }$ in $H$ . Clearly, this is the case if and only if the cycle $C : = v v _ { 1 } P v _ { 3 } v$ separates $v _ { 2 }$ from $v _ { 4 }$ in $G$ . We prove this by showing that $v _ { 2 }$ and $v _ { 4 }$ lie in different faces of $C$ .

Let us pick an inner point $x _ { 2 }$ of $s _ { 2 }$ in $D$ and an inner point $x _ { 4 }$ of $s _ { 4 }$ in $D$ . Then in $D \setminus ( s _ { 1 } \cup s _ { 3 } ) \subseteq \mathbb { R } ^ { 2 } \setminus C$ every point can be linked by a polygonal arc to $x _ { 2 }$ or to $x _ { 4 }$ . This implies that $x _ { 2 }$ and $x _ { 4 }$ (and hence also $v _ { 2 }$ and $v _ { 4 }$ ) lie in different faces of $C$ : otherwise $D$ would meet only one of the two faces of $C$ , which would contradict the fact that $v$ lies on the frontier of both these faces (Theorem 4.1.1).

Given $i , j \in \{ 1 , . . . , 5 \}$ , let $H _ { i , j }$ be the subgraph of $H$ induced by the vertices coloured $i$ or $j$ . We may assume that the component $C _ { 1 }$ of $H _ { 1 , 3 }$ containing $v _ { 1 }$ also contains $v _ { 3 }$ . Indeed, if we interchange the colours 1 and 3 at all the vertices of $C _ { 1 }$ , we obtain another 5-colouring of $H$ ; if $v _ { 3 } \notin C _ { 1 }$ , then $v _ { 1 }$ and $v _ { 3 }$ are both coloured 3 in this new colouring, and we may assign colour 1 to $v$ . Thus, $H _ { 1 , 3 }$ contains a $v _ { 1 } - v _ { 3 }$ path $P$ . As shown above, $P$ separates $v _ { 2 }$ from $v _ { 4 }$ in $H$ . Since $P \cap H _ { 2 , 4 } \ : = \ : \emptyset$ , this means that $v _ { 2 }$ and $v _ { 4 }$ lie in different components of $H _ { 2 , 4 }$ . In the component containing $v _ { 2 }$ , we now interchange the colours 2 and 4, thus recolouring $v _ { 2 }$ with colour 4. Now $\boldsymbol { v }$ no longer has a neighbour coloured 2, and we may give it this colour. 

As a backdrop to the two famous theorems above, let us cite another well-known result:

# Theorem 5.1.3. (Gr¨otzsch 1959)

Every planar graph not containing a triangle is 3-colourable.

$P$

$C$

Hi,j $H _ { i , j }$

# 5.2 Colouring vertices

How do we determine the chromatic number of a given graph? How can we find a vertex-colouring with as few colours as possible? How does the chromatic number relate to other graph invariants, such as average degree, connectivity or girth?

Straight from the definition of the chromatic number we may derive the following upper bound:

Proposition 5.2.1. Every graph $G$ with $m$ edges satisfies

$$
\chi (G) \leqslant \frac {1}{2} + \sqrt {2 m + \frac {1}{4}}.
$$

Proof . Let $c$ be a vertex colouring of $G$ with $k = \chi ( G )$ colours. Then $G$ has at least one edge between any two colour classes: if not, we could have used the same colour for both classes. Thus, $\begin{array} { r } { m \geqslant \frac 1 2 k ( k - 1 ) } \end{array}$ . Solving this inequality for $k$ , we obtain the assertion claimed. 

greedy algorithm

One obvious way to colour a graph $G$ with not too many colours is the following greedy algorithm: starting from a fixed vertex enumeration $v _ { 1 } , \ldots , v _ { n }$ of $G$ , we consider the vertices in turn and colour each $v _ { i }$ with the first available colour—e.g., with the smallest positive integer not already used to colour any neighbour of $v _ { i }$ among $v _ { 1 } , \ldots , v _ { i - 1 }$ . In this way, we never use more than $\Delta ( G ) + 1$ colours, even for unfavourable choices of the enumeration $v _ { 1 } , \ldots , v _ { n }$ . If $G$ is complete or an odd cycle, then this is even best possible.

In general, though, this upper bound of $\Delta + 1$ is rather generous, even for greedy colourings. Indeed, when we come to colour the vertex $v _ { i }$ in the above algorithm, we only need a supply of $d _ { G [ v _ { 1 } , \dots , v _ { i } ] } ( v _ { i } ) + 1$ rather than $d _ { G } ( v _ { i } ) + 1$ colours to proceed; recall that, at this stage, the algorithm ignores any neighbours $v _ { j }$ of $v _ { i }$ with $j > i$ . Hence in most graphs, there will be scope for an improvement of the $\Delta + 1$ bound by choosing a particularly suitable vertex ordering to start with: one that picks vertices of large degree early (when most neighbours are ignored) and vertices of small degree last. Locally, the number $d _ { G [ v _ { 1 } , \dots , v _ { i } ] } ( v _ { i } ) + 1$ of colours required will be smallest if $v _ { i }$ has minimum degree in $G [ v _ { 1 } , \ldots , v _ { i } ]$ ]. But this is easily achieved: we just choose $v _ { n }$ first, with $d ( v _ { n } ) = \delta ( G )$ , then choose as $v _ { n - 1 }$ a vertex of minimum degree in $G - v _ { n }$ , and so on.

colouring number $\operatorname { c o l } ( G )$

The least number $k$ such that $G$ has a vertex enumeration in which each vertex is preceded by fewer than $k$ of its neighbours is called the colouring number $\operatorname { c o l } ( G )$ of $G$ . The enumeration we just discussed shows that $\operatorname { c o l } ( G ) \leqslant \operatorname* { m a x } _ { H \subseteq G } \delta ( H ) + 1$ . But for $H \subseteq G$ clearly also $\operatorname { c o l } ( G ) \geq \operatorname { c o l } ( H )$ and $\operatorname { c o l } ( H ) \geqslant \delta ( H ) + 1$ , since the ‘back-degree’ of the last vertex in any enumeration of $H$ is just its ordinary degree in $H$ , which is at least $\delta ( H )$ . So we have proved the following:

Proposition 5.2.2. Every graph $G$ satisfies

$$
\chi (G) \leqslant \operatorname {c o l} (G) = \max  \left\{\delta (H) \mid H \subseteq G \right\} + 1.
$$

Corollary 5.2.3. Every graph $G$ has a subgraph of minimum degree at least $\chi ( G ) - 1$ . 

![](images/5adc3ad0ddb149901cc2dd3cfa1ba73cf68e4b295f831d4007eb6a60f5d79141.jpg)

[ 9.2.1 ] [ 7.3.9 ] [ 9.2.3 ] [ 11.2.3 ]

The colouring number of a graph is closely related to its arboricity; see the remark following Theorem 2.4.4.

As we have seen, every graph $G$ satisfies $\chi ( G ) \leqslant \Delta ( G ) + 1$ , with equality for complete graphs and odd cycles. In all other cases, this general bound can be improved a little:

# Theorem 5.2.4. (Brooks 1941)

Let $G$ be a connected graph. If $G$ is neither complete nor an odd cycle, then

$$
\chi (G) \leqslant \Delta (G).
$$

Proof . We apply induction on $| G |$ . If $\Delta ( G ) \leqslant 2$ , then $G$ is a path or a cycle, and the assertion is trivial. We therefore assume that $\Delta : =$ $\Delta ( G ) \geqslant 3$ , and that the assertion holds for graphs of smaller order. Suppose that $\chi ( G ) > \Delta$ .

Let $\boldsymbol { v } \in G$ be a vertex and $H : = G - v$ . Then $\chi ( H ) \leqslant \Delta$ : by induction, every component $H ^ { \prime }$ of $H$ satisfies $\chi ( H ^ { \prime } ) \leqslant \Delta ( H ^ { \prime } ) \leqslant \Delta$ unless $H ^ { \prime }$ is complete or an odd cycle, in which case $\chi ( H ^ { \prime } ) = \Delta ( H ^ { \prime } ) + 1 \leqslant \Delta$ as every vertex of $H ^ { \prime }$ has maximum degree in $H ^ { \prime }$ and one such vertex is also adjacent to $v$ in $G$ .

Since $H$ can be $\Delta$ -coloured but $G$ cannot, we have the following:

Every $\Delta$ -colourin $\boldsymbol { v }$ of $H$ uses all $d ( v ) = \Delta$ rs $1 , \ldots , \Delta$ on (1)

Given any $\Delta$ -colouring of $H$ , let us denote the neighbour of $v$ coloured $_ i$ by $v _ { i }$ , $i = 1 , \ldots , \Delta$ . For all $i \neq j$ , let $H _ { i , j }$ denote the subgraph of $H$ spanned by all the vertices coloured $_ i$ or $j$ .

For all ponent $i \neq j$ , of ertices . $v _ { i }$ and $v _ { j }$ lie in a common com- (2) $C _ { i , j }$ $H _ { i , j }$

Otherwise we could interchange the colours $i$ and $j$ in one of those components; then $v _ { i }$ and $v _ { j }$ would be coloured the same, contrary to (1).

$C _ { i , j } \ i s \ a I w a y s \ a \ v _ { i } – v _ { j } \ p a t h .$ (3)

Indeed, let $P$ be a $v _ { i } - v _ { j }$ path in $C _ { i , j }$ . As $d _ { H } ( v _ { i } ) \leqslant \Delta - 1$ , the neighbours of $v _ { i }$ have pairwise different colours: otherwise we could recolour $v _ { i }$ ,

contrary to (1). Hence the neighbour of $v _ { i }$ on $P$ is its only neighbour in $C _ { i , j }$ , and similarly for $v _ { j }$ . Thus if $C _ { i , j } \neq P$ , then $P$ has an inner vertex with three identically coloured neighbours in $H$ ; let $u$ be the first such vertex on $P$ (Fig. 5.2.1). Since at most $\Delta - 2$ colours are used on the neighbours of $u$ , we may recolour $u$ . But this makes $P \breve { u }$ into a component of $H _ { i , j }$ , contradicting (2).

![](images/839f2082c5e0c8025e9afba4292970b8b37aa959f5c01050ebff6b9d84ba1e94.jpg)  
Fig. 5.2.1. The proof of (3) in Brooks’s theorem

For distinct $i , j , k$ , the paths $C _ { i , j }$ and $C _ { i , k }$ meet only in vi. (4)

For if $v _ { i } \neq u \in C _ { i , j } \cap C _ { i , k }$ , then $u$ has two neighbours coloured $j$ and two coloured $k$ , so we may recolour $u$ . In the new colouring, $v _ { i }$ and $v _ { j }$ lie in different components of $H _ { i , j }$ , contrary to (2).

The proof of the theorem now follows easily. If the neighbours of $v$ are pairwise adjacent, then each has $\Delta$ neighbours in $N ( v ) \cup \{ v \}$ already, so $G = G [ N ( v ) \cup \{ v \} ] = K ^ { \Delta + 1 }$ . As $G$ is complete, there is nothing to show. We may thus assume that $v _ { 1 } v _ { 2 } \notin G$ , where $v _ { 1 } , \ldots , v _ { \Delta }$ derive their names from some fixed $\Delta$ -colouring $c$ of $H$ . Let $u \ne v _ { 2 }$ be the neighbour of $v _ { 1 }$ on the path $C _ { 1 , 2 }$ ; then $c ( u ) = 2$ . Interchanging the colours 1 and 3 in $C _ { 1 , 3 }$ , we obtain a new colouring $c ^ { \prime }$ of $H$ ; let $\boldsymbol { v } _ { i } ^ { \prime }$ , $H _ { i , j } ^ { \prime }$ , $C _ { i , j } ^ { \prime }$ etc. be defined with respect to $c ^ { \prime }$ in the obvious way. As a neighbour of $v _ { 1 } = v _ { 3 } ^ { \prime }$ , our the path vertex $u$ now lies in $\stackrel { \cup } { v } _ { 1 } C _ { 1 , 2 }$ 2,3   retained its original colouring, so $C _ { 2 , 3 } ^ { \prime }$ , since $c ^ { \prime } ( u ) = c ( u ) = 2$ . By (4) for $u \in \check { v } _ { 1 } C _ { 1 , 2 } \ \subseteq \ C _ { 1 , 2 } ^ { \prime }$ $c$ , however, . Hence $u \in C _ { 2 , 3 } ^ { \prime } \cap C _ { 1 , 2 } ^ { \prime }$ , contradicting (4) for $c ^ { \prime }$ .

As we have seen, a graph $G$ of large chromatic number must have large maximum degree: trivially at least $\chi ( G ) - 1$ , and less trivially at least $\chi ( G )$ (in most cases). What more can we say about the structure of graphs with large chromatic number?

One obvious possible cause for $\chi ( G ) \geqslant k$ is the presence of a $K ^ { k }$ subgraph. This is a local property of $G$ , compatible with arbitrary values of global invariants such as $\varepsilon$ and $\kappa$ . Hence, the assumption of $\chi ( G ) \geqslant k$ does not tell us anything about those invariants for $G$ itself. It does, however, imply the existence of a subgraph where those invariants are large: by Corollary 5.2.3, $G$ has a subgraph $H$ with $\delta ( H ) \geqslant k - 1$ , and hence by Theorem 1.4.3 a subgraph $H ^ { \prime }$ with $\kappa ( H ^ { \prime } ) \geqslant \lfloor \frac { 1 } { 4 } ( k - 1 ) \rfloor$ .

But is, conversely, the somewhat higher density of those subgraphs in any sense the ‘cause’ for $\chi$ to be large? That is to say, do arbitrary graphs with such values of $\delta$ and $\kappa$ in turn have large chromatic number, say at least $f ( k )$ for some function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ tending to infinity (however slowly)? No, not at all: the graphs $K _ { n , n }$ , for example, have a minimum degree and connectivity that exceeds any bound in terms of $k$ as $n \longrightarrow \infty$ , but are only 2-chromatic. Thus, the sort of large (constant1) average or minimum degree that a high chromatic number can force in a suitable subgraph is itself not nearly large enough to force even $\chi > 2$ .

Yet even if local edge density is not by itself responsible for $\chi$ to be large, it might still be the case that, somehow, a chromatic number of at least $k$ forces the existence of one of finitely many ‘canonical’ subgraphs of chromatic number at least, say, $f ( k )$ (with $f$ as above). However, this is radically not the case: as soon as a graph $H$ contains a cycle (which highly chromatic graphs clearly do), we cannot force an arbitrary graph $G$ to contain a copy of $H$ just by making $\chi ( G )$ large enough:

# Theorem 5.2.5. (Erd˝os 1959)

For every integer $k$ there exists a graph $G$ with girth $g ( G ) \ > \ k$ and chromatic number $\chi ( G ) > k$ .

Theorem 5.2.5 was first proved non-constructively using random graphs, and we shall give this proof in Chapter 11.2. Constructing graphs of large chromatic number and girth directly is not easy; cf. Exercise 23 for the simplest case.

The message of Erd˝os’s theorem is that, contrary to our initial guess, large chromatic number can occur as a purely global phenomenon: note that locally, around each vertex, a graph of large girth looks just like a tree, and in particular is 2-colourable there. But what exactly can cause high chromaticity as a global phenomenon remains a mystery.

Nevertheless, there exists a simple—though not always short— procedure to construct all the graphs of chromatic number at least $k$ . For each $k \in \mathbb N$ , let us define the class of $k$ -constructible graphs recursively as follows:

(i) $K ^ { k }$ i s $k$ -constructible.   
(ii) If $G$ is $k$ -constructible and $x , y \in V ( G )$ are non-adjacent, then also $( G + x y ) / x y$ is $k$ -constructible.   
(iii) If $G _ { 1 } , G _ { 2 }$ are $k$ -constructible and there are vertices $x , y _ { 1 } , y _ { 2 }$ such that $G _ { 1 } \cap G _ { 2 } = \{ x \}$ and $x y _ { 1 } \in E ( G _ { 1 } )$ and $x y _ { 2 } \in E ( G _ { 2 } )$ , then also $( G _ { 1 } \cup G _ { 2 } ) - x y _ { 1 } - x y _ { 2 } + y _ { 1 } y _ { 2 }$ is $k$ -constructible (Fig. 5.2.2).

$k$ -constructible

![](images/8d0cec572e265fdb30dd89a1a96d88d873682e1e9a6e7b4e5316cf7123ab631d.jpg)  
Fig. 5.2.2. The Haj´os construction (iii)

One easily checks inductively that all $k$ -constructible graphs—and hence their supergraphs—are at least $k$ -chromatic. Indeed, if $( G + x y ) / x y$ as in (ii) has a colouring with fewer than $k$ colours, then this defines such a colouring also for $G$ , a contradiction. Similarly, in any colouring of the graph constructed in (iii), the vertices $y _ { 1 }$ and $y _ { 2 }$ do not both have the same colour as $x$ , so this colouring induces a colouring of either $G _ { 1 }$ or $G _ { 2 }$ and hence uses at least $k$ colours.

It is remarkable, though, that the converse holds too:

# Theorem 5.2.6. (Haj´os 1961)

Let $G$ be a graph and $k \in \mathbb N$ . Then $\chi ( G ) \geqslant k$ if and only if $G$ has a $k$ -constructible subgraph.

Proof . Let $G$ be a graph with $\chi ( G ) \geqslant k$ ; we show that $G$ has a $k$ - constructible subgraph. Suppose not; then $k \geqslant 3$ . Adding some edges if necessary, let us make $G$ edge-maximal with the property that none of its subgraphs is $k$ -constructible. Now $G$ is not a complete $r$ -partite graph for any $r$ : for then $\chi ( G ) \geqslant k$ would imply $r \geqslant k$ , and $G$ would contain the $k$ -constructible graph $K ^ { k }$ .

Since $G$ is not a complete multipartite graph, non-adjacency is not an equivalence relation on $V ( G )$ . So there are vertices $y _ { 1 } , x , y _ { 2 }$ such that $y _ { 1 } x , x y _ { 2 } \notin E ( G )$ but $y _ { 1 } y _ { 2 } \ \in \ E ( G )$ . Since $G$ is edge-maximal without a $k$ -constructible subgraph, each edge $x y _ { i }$ lies in some $k$ -constructible subgraph $H _ { i }$ of $G + x y _ { i }$ $i = 1 , 2$ ).

Let $H _ { 2 } ^ { \prime }$ be an isomorphic copy of $H _ { 2 }$ that contains $x$ and $H _ { 2 } - H _ { 1 }$ but is otherwise disjoint from $G$ , together with an isomorphism $v \mapsto v ^ { \prime }$ from $H _ { 2 }$ to $H _ { 2 } ^ { \prime }$ that fixes $H _ { 2 } \cap H _ { 2 } ^ { \prime }$ pointwise. Then $H _ { 1 } \cap H _ { 2 } ^ { \prime } = \{ x \}$ , so

$$
H := \left(H _ {1} \cup H _ {2} ^ {\prime}\right) - x y _ {1} - x y _ {2} ^ {\prime} + y _ {1} y _ {2} ^ {\prime}
$$

is $k$ -constructible by (iii). One vertex at a time, let us identify in $H$ each vertex $v ^ { \prime } \in H _ { 2 } ^ { \prime } - G$ with its partner $\boldsymbol { v }$ ; since $v v ^ { \prime }$ is never an edge of $H$ , each of these identifications amounts to a construction step of type (ii). Eventually, we obtain the graph

$$
\left(H _ {1} \cup H _ {2}\right) - x y _ {1} - x y _ {2} + y _ {1} y _ {2} \subseteq G;
$$

this is the desired $k$ -constructible subgraph of $G$ .

# 5.3 Colouring edges

Clearly, every graph $G$ satisfies $\chi ^ { \prime } ( G ) \geqslant \Delta ( G )$ . For bipartite graphs, we have equality here:

# Proposition 5.3.1. (K¨onig 1916)

Every bipartite graph $G$ satisfies $\chi ^ { \prime } ( G ) = \Delta ( G )$ .

Proof . We apply induction on $\| G \|$ . For $\| G \| = 0$ the assertion holds. Now assume that $\| G \| \geqslant 1$ , and that the assertion holds for graphs with fewer edges. Let $\Delta : = \Delta ( G )$ , pick an edge $x y \in G$ , and choose a $\Delta$ - edge-colouring of $G - x y$ by the induction hypothesis. Let us refer to the edges coloured $\alpha$ as $\alpha$ -edges, etc.

In $G - x y$ , each of $x$ and $y$ is incident with at most $\Delta - 1$ edges. Hence there are $\alpha , \beta \in \{ 1 , . . . , \Delta \}$ such that $x$ is not incident with an $\alpha$ -edge and $y$ is not incident with a $\beta$ -edge. If $\alpha = \beta$ , we can colour the edge $x y$ with this colour and are done; so we may assume that $\alpha \neq \beta$ , and that $x$ is incident with a $\beta$ -edge.

Let us extend this edge to a maximal walk $W$ from $x$ whose edges are coloured $\beta$ and $\alpha$ alternately. Since no such walk contains a vertex twice (why not?), $W$ exists and is a path. Moreover, $W$ does not contain $y$ : if it did, it would end in $y$ on an $\alpha$ -edge (by the choice of $\beta$ ) and thus have even length, so $W + x y$ would be an odd cycle in $G$ (cf. Proposition 1.6.1). We now recolour all the edges on $W$ , swapping $\alpha$ with $\beta$ . By the choice of $\alpha$ and the maximality of $W$ , adjacent edges of $G - x y$ are still coloured differently. We have thus found a $\Delta$ -edge-colouring of $G - x y$ in which neither $x$ nor $y$ is incident with a $\beta$ -edge. Colouring $x y$ with $\beta$ , we extend this colouring to a $\Delta$ -edge-colouring of $G$ . 

# Theorem 5.3.2. (Vizing 1964)

Every graph $G$ satisfies

$$
\Delta (G) \leqslant \chi^ {\prime} (G) \leqslant \Delta (G) + 1.
$$

Proof . We prove the second inequality by induction on $\| G \|$ . For $\| G \| = 0$ it is trivial. For the induction step let $G = ( V , E )$ with $\Delta : = \Delta ( G ) > 0$ be given, and assume that the assertion holds for graphs with fewer edges. Instead of $( \Delta + 1 )$ -edge-colouring’ let us just say ‘colouring’. An edge coloured $\alpha$ will again be called an $\alpha$ -edge.

For every edge $\textit { e } \in \textit { G }$ there exists a colouring of $G - e$ , by the induction hypothesis. In such a colouring, the edges at a given vertex $v$ use at most $d ( v ) \leqslant \Delta$ colours, so some colour $\beta \in \{ 1 , \dots , \Delta + 1 \}$ is missing at $v$ . For any other colour $\alpha$ , there is a unique maximal walk (possibly trivial) starting at $\boldsymbol { v }$ , whose edges are coloured alternately $\alpha$ and $\beta$ . This walk is a path; we call it the $\alpha / \beta$ - path from $v$ .

Suppose that $G$ has no colouring. Then the following holds:

[ 5.4.5 ]

(1.6.1)

$\Delta , x y$

$_ \alpha$ -edge

$\alpha , \beta$

$V , E$

$\Delta$

olouring

$_ \alpha$ -edge

missing

$\alpha / \beta$ - path

Given $x y \in E$ , and any colouring of $G - x y$ in which the colour $\alpha$ is missing at $x$ and the colour $\beta$ is missing at $y$ , (1) the $\alpha / \beta$ - path from $y$ ends in $x$ .

Otherwise we could interchange the colours $\alpha$ and $\beta$ along this path and colour $x y$ with $\alpha$ , obtaining a colouring of $G$ (contradiction).

Let $x y _ { 0 } \in G$ be an edge. By induction, $G _ { 0 } : = G - x y _ { 0 }$ has a colouring $c _ { 0 }$ . Let $\alpha$ be a colour missing at $x$ in this colouring. Further, let $y _ { 0 } , y _ { 1 } , \ldots , y _ { k }$ be a maximal sequence of distinct neighbours of $x$ in $G$ , such that $c _ { 0 } ( x y _ { i } )$ is missing in $c _ { 0 }$ at $y _ { i - 1 }$ for each $i = 1 , \ldots , k$ . For each of the graphs $G _ { i } : = G - x y _ { i }$ we define a colouring $c _ { i }$ , setting

$$
c _ {i} (e) := \left\{ \begin{array}{l l} c _ {0} (x y _ {j + 1}) & \text {f o r e = x y _ {j} w i t h j \in \{0 , \ldots , i - 1 \}} \\ c _ {0} (e) & \text {o t h e r w i s e ;} \end{array} \right.
$$

note that in each of these colourings the same colours are missing at $x$ as in $c _ { 0 }$ .

Now let $\beta$ be a colour missing at $y _ { k }$ in $c _ { 0 }$ . Clearly, $\beta$ is still missing at $y _ { k }$ in $c _ { k }$ . If $\beta$ were also missing at $x$ , we could colour $x y _ { k }$ with $\beta$ and thus extend $c _ { k }$ to a colouring of $G$ . Hence, $x$ is incident with a $\beta$ -edge (in every colouring). By the maximality of $k$ , therefore, there is an $i \in \{ 1 , \ldots , k - 1 \}$ such that

$$
c _ {0} \left(x y _ {i}\right) = \beta . \tag {2}
$$

![](images/84219559c24bdb0e99f8bd16ec210be459386950a65b9821902fd522ce2c827a.jpg)  
Fig. 5.3.1. The $\alpha / \beta$ - path $P$ in $G _ { k }$

Let $P$ be the $\alpha / \beta$ - path from $y _ { k }$ in $G _ { k }$ (with respect to $c _ { k }$ ; Fig. 5.3.1). By (1), $P$ ends in $x$ , and it does so on a $\beta$ -edge, since $\alpha$ is missing at $x$ . As $\beta = c _ { 0 } ( x y _ { i } ) = c _ { k } ( x y _ { i - 1 } )$ , this is the edge $x y _ { i - 1 }$ . In $c _ { 0 }$ , however, and hence also in $c _ { i - 1 }$ , $\beta$ is missing at $y _ { i - 1 }$ (by (2) and the choice of $y _ { i }$ ); let $P ^ { \prime }$ be the $\alpha / \beta$ - path from $y _ { i - 1 }$ in $G _ { i - 1 }$ (with respect to $c _ { i - 1 }$ ). Since $P ^ { \prime }$ is uniquely determined, it starts with $y _ { i - 1 } P y _ { k }$ ; note that the edges of $P \overset { \circ } { x }$ are coloured the same in $c _ { i - 1 }$ as in $c _ { k }$ . But in $c _ { 0 }$ , and hence in $c _ { i - 1 }$ , there is no $\beta$ -edge at $y _ { k }$ (by the choice of $\beta$ ). Therefore $P ^ { \prime }$ ends in $y _ { k }$ , contradicting (1). 

Vizing’s theorem divides the finite graphs into two classes according to their chromatic index; graphs satisfying $\chi ^ { \prime } = \Delta$ are called (imaginatively) class $\mathit { 1 }$ , those with $\chi ^ { \prime } = \Delta + 1$ are class 2 .

# 5.4 List colouring

In this section, we take a look at a relatively recent generalization of the concepts of colouring studied so far. This generalization may seem a little far-fetched at first glance, but it turns out to supply a fundamental link between the classical (vertex and edge) chromatic numbers of a graph and its other invariants.

Suppose we are given a graph $G = ( V , E )$ , and for each vertex of $G$ a list of colours permitted at that particular vertex: when can we colour $G$ (in the usual sense) so that each vertex receives a colour from its list? More formally, let $( S _ { v } ) _ { v \in V }$ be a family of sets. We call a vertex colouring $c$ of $G$ with $c ( v ) \ \in \ S _ { v }$ for all $v ~ \in ~ V$ a colouring from the lists $S _ { v }$ . The graph $G$ is called $k$ -list-colourable, or $k$ -choosable, if, for every family $( S _ { v } ) _ { v \in V }$ with $| S _ { v } | = k$ for all $v$ , there is a vertex colouring of $G$ from the lists $S _ { v }$ . The least integer $k$ for which $G$ is $k$ -choosable is the list-chromatic number, or choice number $\operatorname { c h } ( G )$ of $G$ .

List-colourings of edges are defined analogously. The least integer $k$ such that $G$ has an edge colouring from any family of lists of size $k$ is the list-chromatic index $\operatorname { c h } ^ { \prime } ( G )$ of $G$ ; formally, we just set $\operatorname { c h } ^ { \prime } ( G ) : =$ $\operatorname { c h } ( L ( G ) )$ , where $L ( G )$ is the line graph of $G$ .

In principle, showing that a given graph is $k$ -choosable is more difficult than proving it to be $k$ -colourable: the latter is just the special case of the former where all lists are equal to $\{ 1 , \ldots , k \}$ . Thus,

$$
\operatorname {c h} (G) \geqslant \chi (G) \quad \text {a n d} \quad \operatorname {c h} ^ {\prime} (G) \geqslant \chi^ {\prime} (G)
$$

for all graphs $G$

In spite of these inequalities, many of the known upper bounds for the chromatic number have turned out to be valid for the choice number, too. Examples for this phenomenon include Brooks’s theorem and Proposition 5.2.2; in particular, graphs of large choice number still have subgraphs of large minimum degree. On the other hand, it is easy to construct graphs for which the two invariants are wide apart (Exercise 25). Taken together, these two facts indicate a little how far those general upper bounds on the chromatic number may be from the truth.

The following theorem shows that, in terms of its relationship to other graph invariants, the choice number differs fundamentally from the chromatic number. As mentioned before, there are 2-chromatic graphs of arbitrarily large minimum degree, e.g. the graphs $K _ { n , n }$ . The choice number, however, will be forced up by large values of invariants like $\delta$ , $\varepsilon$ or $\kappa$ :

# Theorem 5.4.1. (Alon 1993)

There exists a function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ such that, given any integer $k$ , all graphs $G$ with average degree $d ( G ) \geqslant f ( k )$ satisfy $\operatorname { c h } ( G ) \geqslant k$ .

The proof of Theorem 5.4.1 uses probabilistic methods as introduced in Chapter 11.

Although statements of the form $\operatorname { c h } ( G ) \leqslant k$ are formally stronger than the corresponding statement of $\chi ( G ) \leqslant k$ , they can be easier to prove. A pretty example is the list version of the five colour theorem: every planar graph is 5-choosable. The proof of this does not use the five colour theorem (or even Euler’s formula, on which the proof of the five colour theorem is based). We thus reobtain the five colour theorem as a corollary, with a very different proof.

# Theorem 5.4.2. (Thomassen 1994)

Every planar graph is 5-choosable.

(4.2.8) Proof . We shall prove the following assertion for all plane graphs $G$ with at least 3 vertices:

Suppose that every inner face of $G$ is bounded by a triangle and its outer face by a cycle $C = v _ { 1 } \ldots v _ { k } v _ { 1 }$ . Suppose further that $v _ { 1 }$ has already been coloured with the colour 1, and $v _ { 2 }$ has been co ured 2. Suppose finally that ( ) $C$ associated, and with every vertex of $G - C$ a list of at least 5 colours. Then the colouring of $v _ { 1 }$ and $v _ { 2 }$ can be extended to a colouring of $G$ from the given lists.

Let us check first that ( ) implies the assertion of the theorem. Let any plane graph be given, together with a list of 5 colours for each vertex. Add edges to this graph until it is a maximal plane graph $G$ . By Proposition 4.2.8, $G$ is a plane triangulation; let $v _ { 1 } v _ { 2 } v _ { 3 } v _ { 1 }$ be the boundary of its outer face. We now colour $v _ { 1 }$ and $v _ { 2 }$ (differently) from their lists, and extend this colouring by ( ) to a colouring of $G$ from the lists given.

Let us now prove ( ), by induction on $| G |$ . If $| G | = 3$ , then $G =$ $C$ and the assertion is trivial. Now let $| G | \geqslant 4$ , and assume $( * )$ for smaller graphs. If $C$ has a chord $v w$ , then $v w$ lies on two unique cycles $C _ { 1 } , C _ { 2 } \subseteq C + v w$ with $v _ { 1 } v _ { 2 } \in C _ { 1 }$ and $v _ { 1 } v _ { 2 } \notin \ C _ { 2 }$ . For $i = 1 , 2$ , let $G _ { i }$ denote the subgraph of $G$ induced by the vertices lying on $C _ { i }$ or in its inner face (Fig. 5.4.1). Applying the induction hypothesis first to $G _ { 1 }$ and then—with the colours now assigned to $v$ and $w$ —to $G _ { 2 }$ yields the desired colouring of $G$ .

![](images/cd975dc7970d21e521f9872e72b9257a4edec5bb3cbc5a79de1b9b70ca5067df.jpg)  
Fig. 5.4.1. The induction step with a chord $v w$ ; here the case of $w = v _ { 2 }$

If $C$ has no chord, let $v _ { 1 } , u _ { 1 } , \ldots , u _ { m } , v _ { k - 1 }$ be the neighbours of $v _ { k }$ in their natural cyclic order order around $v _ { k }$ ;2 by definition of $C$ , all those neighbours $u _ { i }$ lie in the inner face of $C$ (Fig. 5.4.2). As the inner faces of $C$ are bounded by triangles, $P : = v _ { 1 } u _ { 1 } \dots u _ { m } v _ { k - 1 }$ is a path in $G$ , and $C ^ { \prime } : = P \cup ( C - v _ { k } )$ a cycle.

$u _ { 1 } , \ldots , u _ { m }$

![](images/0e320f8c17461bbce15bcd36d2e16808a16bbcd970f603bda27aa8b79c6f1f1f.jpg)  
Fig. 5.4.2. The induction step without a chord

We now choose two different colours $j , \ell \neq 1$ from the list of $v _ { k }$ and delete these colours from the lists of all the vertices $u _ { i }$ . Then every list of a vertex on $C ^ { \prime }$ still has at least 3 colours, so by induction we may colour $C ^ { \prime }$ and its interior, i.e. the graph $G - v _ { k }$ . At least one of the two colours $j , \ell$ is not used for $v _ { k - 1 }$ , and we may assign that colour to $v _ { k }$ . 

As is often the case with induction proofs, the key to the proof above lies in its delicately balanced strengthening of the assertion proved. Compared with ordinary colouring, the task of finding a suitable strengthening is helped greatly by the possibility to give different vertices lists of different lengths, and thus to tailor the colouring problem more fittingly to the structure of the graph. This suggests that maybe in other unsolved

colouring problems too it might be of advantage to aim straight for their list version, i.e. to prove an assertion of the form $\operatorname { c h } ( G ) \leqslant k$ instead of the formally weaker $\chi ( G ) \leqslant k$ . Unfortunately, this approach fails for the four colour theorem: planar graphs are not in general 4-choosable.

As mentioned before, the chromatic number of a graph and its choice number may differ a lot. Surprisingly, however, no such examples are known for edge colourings. Indeed it has been conjectured that none exist:

List Colouring Conjecture. Every graph $G$ satisfies $\mathrm { c h } ^ { \prime } ( G ) = \chi ^ { \prime } ( G )$ .

$N ^ { + } ( v )$

$d ^ { + } ( v )$

kernel

We shall prove the list colouring conjecture for bipartite graphs. As a tool we shall use orientations of graphs, defined in Chapter 1.10. If $D$ is a directed graph and $v \in V ( D )$ , we denote by $N ^ { + } ( v )$ the set, and by $d ^ { + } ( v )$ the number, of vertices $w$ such that $D$ contains an edge directed from $v$ to $w$ .

To see how orientations come into play in the context of colouring, recall the greedy algorithm from Section 5.2. This colours the vertices of a graph $G$ in turn, following a previously fixed ordering $( v _ { 1 } , \ldots , v _ { n } )$ . This ordering defines an orientation of $G$ if we orient every edge $v _ { i } v _ { j }$ ‘backwards’, that is, from $v _ { i }$ to $v _ { j }$ if $i > j$ . Then to determine a colour for $v _ { i }$ the algorithm only looks at previously coloured neighbours of $v _ { i }$ , those to which $v _ { i }$ sends a directed edge. In particular, if $d ^ { + } ( v ) < k$ for all vertices $\boldsymbol { v }$ , the algorithm will use at most $k$ colours.

If we rewrite the proof of this fact (rather awkwardly) as a formal induction on $k$ , we notice that the essential property of the set $U$ of vertices coloured 1 is that every vertex in $G - U$ sends an edge to $U$ : this ensures that ${ d _ { G - U } ^ { + } ( v ) < d _ { G } ^ { + } ( v ) }$ for all $v \in G - U$ , so we can colour $G - U$ with the remaining $k - 1$ colours by the induction hypothesis.

The following lemma generalizes these observations to list colouring, and to orientations $D$ of $G$ that do not necessarily come from a vertex enumeration but may contain some directed cycles. Let us call an independent set $U \subseteq V ( D )$ a kernel of $D$ if, for every vertex $v \in D - U$ , there is an edge in $D$ directed from $v$ to a vertex in $U$ . Note that kernels of non-empty directed graphs are themselves non-empty.

Lemma 5.4.3. Let $H$ be a graph and $( S _ { v } ) _ { v \in V ( H ) }$ a family of lists. If $H$ has an orientation $D$ with $d ^ { + } ( v ) < | S _ { v } |$ for every $\boldsymbol { v }$ , and such that every induced subgraph of $D$ has a kernel, then $H$ can be coloured from the lists $S _ { v }$ .

$_ { \alpha }$

Proof . We apply induction on $| H |$ . For $| H | = 0$ we take the empty colouring. For the induction step, let $| H | > 0$ . Let $\alpha$ be a colour occurring in one of the lists $S _ { v }$ , and let $D$ be an orientation of $H$ as stated.

The vertices $\boldsymbol { v }$ with $\alpha \in S _ { v }$ span a non-empty subgraph $D ^ { \prime }$ i n $D$ ; by assumption, $D ^ { \prime }$ has a kernel $U \neq \emptyset$ .

Let us colour the vertices in $U$ with $\alpha$ , and remove $\alpha$ from the lists of all the other vertices of $D ^ { \prime }$ . Since each of those vertices sends an edge to $U$ , the modified lists $S _ { v } ^ { \prime }$ for $v \in D - U$ again satisfy the condition $d ^ { + } ( v ) < | S _ { v } ^ { \prime } |$ in $D - U$ . Since $D - U$ is an orientation of $H - U$ , we can thus colour $H - U$ from those lists by the induction hypothesis. As none of these lists contains $\alpha$ , this extends our colouring $U  \{ \alpha \}$ to the desired list colouring of $H$ . 

In our proof of the list colouring conjecture for bipartite graphs we shall apply Lemma 5.4.3 only to colourings from lists of uniform length $k$ . However, note that keeping list lengths variable is essential for the proof of the lemma itself: its simple induction could not be performed with uniform list lengths.

# Theorem 5.4.4. (Galvin 1995)

Every bipartite graph $G$ satisfies $\operatorname { c h } ^ { \prime } ( G ) = \chi ^ { \prime } ( G )$

Proof . Let $G = : ( X \cup Y , E )$ , where $\{ X , Y \}$ is a vertex bipartition of $G$ . (2.1.4) Let us say that two edges of $G$ meet in $X$ if they share an end in $X$ , and $X , Y , \ E$ correspondingly for $Y$ . Let $\chi ^ { \prime } ( G ) = : k$ , and let $c$ be a $k$ -edge-colouring $k$ of $G$ . $^ c$

Clearly, $\operatorname { c h } ^ { \prime } ( G ) \geqslant k$ ; we prove that $\operatorname { c h } ^ { \prime } ( G ) \leqslant k$ . Our plan is to use Lemma 5.4.3 to show that the line graph $H$ of $G$ is $k$ -choosable. To apply the lemma, it suffices to find an orientation $D$ of $H$ with $d ^ { + } ( e ) < k$ for every vertex $e$ of $H$ , and such that every induced subgraph of $D$ has a kernel. To define $D$ , consider adjacent $e , e ^ { \prime } \in E$ , say with $c ( e ) < c ( e ^ { \prime } )$ . If $e$ and $e ^ { \prime }$ meet in $X$ , we orient the edge $e e ^ { \prime } \in H$ from $e ^ { \prime }$ towards $e$ ; if $e$ and $e ^ { \prime }$ meet in $Y$ , we orient it from $e$ to $e ^ { \prime }$ (Fig 5.4.3).

![](images/36d5fc45fe30d679108464600b4e90f563c1dfa92063591299cee8bca15e7f84.jpg)  
Fig. 5.4.3. Orienting the line graph of $G$

Let us compute $d ^ { + } ( e )$ for given $e \in E = V ( D )$ . If $c ( e ) = i$ , say, then every $e ^ { \prime } \in N ^ { + } ( e )$ meeting $e$ in $X$ has its colour in $\{ 1 , \ldots , i - 1 \}$ , and every $e ^ { \prime } \in N ^ { + } ( e )$ meeting $e$ in $Y$ has its colour in $\{ i + 1 , \ldots , k \}$ .

$D ^ { \prime }$

As any two neighbours $e ^ { \prime }$ of $e$ meeting $e$ either both in $X$ or both in $Y$ are themselves adjacent and hence coloured differently, this implies $d ^ { + } ( e ) < k$ as desired.

It remains to show that every induced subgraph $D ^ { \prime }$ of $D$ has a kernel. This, however, is immediate by the stable marriage theorem (2.1.4) for $G$ , if we interpret the directions in $D$ as expressing preference. Indeed, given a vertex $v \in X \cup Y$ and edges $e , e ^ { \prime } \in V ( D ^ { \prime } )$ at $v$ , write $e < _ { v } e ^ { \prime }$ if the edge $e e ^ { \prime }$ of $H$ is directed from $e$ to $e ^ { \prime }$ in $D$ . Then any stable matching in the graph $( X \cup Y , V ( D ^ { \prime } ) )$ for this set of preferences is a kernel in $D ^ { \prime }$ . 

(5.3.1)

By Proposition 5.3.1, we now know the exact list-chromatic index of bipartite graphs:

Corollary 5.4.5. Every bipartite graph $G$ satisfies $\mathrm { c h } ^ { \prime } ( G ) = \Delta ( G )$ .



# 5.5 Perfect graphs

As discussed in Section 5.2, a high chromatic number may occur as a purely global phenomenon: even when a graph has large girth, and thus locally looks like a tree, its chromatic number may be arbitrarily high. Since such ‘global dependence’ is obviously difficult to deal with, one may become interested in graphs where this phenomenon does not occur, i.e. whose chromatic number is high only when there is a local reason for it.

Before we make this precise, let us note two definitions for a graph $G$ . The greatest integer $r$ such that $K ^ { r } \subseteq G$ is the clique number $\omega ( G )$ of $G$ , and the greatest integer $r$ such that ${ \overline { { K ^ { r } } } } \subseteq G$ (induced) is the independence number $\alpha ( G )$ of $G$ . Clearly, $\alpha ( G ) = \omega ( { \overline { { G } } } )$ and $\omega ( G ) = \alpha ( { \overline { { G } } } )$ .

A graph is called perfect if every induced subgraph $H \subseteq G$ has chromatic number $\chi ( H ) = \omega ( H )$ , i.e. if the trivial lower bound of $\omega ( H )$ colours always suffices to colour the vertices of $H$ . Thus, while proving an assertion of the form $\chi ( G ) > k$ may in general be difficult, even in principle, for a given graph $G$ , it can always be done for a perfect graph simply by exhibiting some $K ^ { k + 1 }$ subgraph as a ‘certificate’ for non-colourability with $k$ colours.

At first glance, the structure of the class of perfect graphs appears somewhat contrived: although it is closed under induced subgraphs (if only by explicit definition), it is not closed under taking general subgraphs or supergraphs, let alone minors (examples?). However, perfection is an important notion in graph theory: the fact that several

ω(G)

$\alpha ( G )$

perfect

fundamental classes of graphs are perfect (as if by fluke) may serve as a superficial indication of this.3

What graphs, then, are perfect? Bipartite graphs are, for instance. Less trivially, the complements of bipartite graphs are perfect, too— a fact equivalent to K¨onig’s duality theorem 2.1.1 (Exercise 36). The so-called comparability graphs are perfect, and so are the interval graphs (see the exercises); both these turn up in numerous applications.

In order to study at least one such example in some detail, we prove here that the chordal graphs are perfect: a graph is chordal (or triangulated ) if each of its cycles of length at least 4 has a chord, i.e. if it contains no induced cycles other than triangles.

To show that chordal graphs are perfect, we shall first characterize their structure. If $G$ is a graph with induced subgraphs $G _ { 1 }$ , $G _ { 2 }$ and $S$ , such that $G = G _ { 1 } \cup G _ { 2 }$ and $S = G _ { 1 } \cap G _ { 2 }$ , we say that $G$ arises from $G _ { 1 }$ and $G _ { 2 }$ by pasting these graphs together along $S$ .

Proposition 5.5.1. A graph is chordal if and only if it can be constructed recursively by pasting along complete subgraphs, starting from complete graphs.

Proof . If $G$ is obtained from two chordal graphs $G _ { 1 } , G _ { 2 }$ by pasting them together along a complete subgraph, then $G$ is clearly again chordal: any induced cycle in $G$ lies in either $G _ { 1 }$ or $G _ { 2 }$ , and is hence a triangle by assumption. Since complete graphs are chordal, this proves that all graphs constructible as stated are chordal.

Conversely, let $G$ be a chordal graph. We show by induction on $| G |$ that $G$ can be constructed as described. This is trivial if $G$ is complete. We therefore assume that $G$ is not complete, in particular that $| G | > 1$ , and that all smaller chordal graphs are constructible as stated. Let $a , b \in$ $G$ be two non-adjacent vertices, and let $X \subseteq V ( G ) \setminus \{ a , b \}$ be a minimal $a$ – $\boldsymbol { b }$ separator. Let $C$ denote the component of $G - X$ containing $a$ , and put $G _ { 1 } : = G \left[ V ( C ) \cup X \right]$ and $G _ { 2 } : = G - C$ . Then $G$ arises from $G _ { 1 }$ and $G _ { 2 }$ by pasting these graphs together along $S : = G \left[ X \right]$ .

Since $G _ { 1 }$ and $G _ { 2 }$ are both chordal (being induced subgraphs of $G$ ) and hence constructible by induction, it suffices to show that $S$ is complete. Suppose, then, that $s , t \in S$ are non-adjacent. By the minimality of $X = V ( S )$ as an $a$ – $\boldsymbol { b }$ separator, both $s$ and $t$ have a neighbour in $C$ . Hence, there is an $X$ -path from $s$ to $t$ in $G _ { 1 }$ ; we let $P _ { 1 }$ be a shortest such path. Analogously, $G _ { 2 }$ contains a shortest $X$ -path $P _ { 2 }$ from $s$ to $t$ . But then $P _ { 1 } \cup P _ { 2 }$ is a chordless cycle of length $\geqslant 4$ (Fig. 5.5.1), contradicting our assumption that $G$ is chordal. 

![](images/2cc5037aa79c2dde6c34531b7f819ba212585702a022e3d311f39e785be0ac27.jpg)  
Fig. 5.5.1. If $G _ { 1 }$ and $G _ { 2 }$ are chordal, then so is $G$

# Proposition 5.5.2. Every chordal graph is perfect.

Proof . Since complete graphs are perfect, it suffices by Proposition 5.5.1 to show that any graph $G$ obtained from perfect graphs $G _ { 1 } , G _ { 2 }$ by pasting them together along a complete subgraph $S$ is again perfect. So let $H \subseteq G$ be an induced subgraph; we show that $\chi ( H ) \leqslant \omega ( H )$ .

Let $H _ { i } : = H \cap G _ { i }$ for $i = 1 , 2$ , and let $T : = H \cap S$ . Then $T$ is again complete, and $H$ arises from $H _ { 1 }$ and $H _ { 2 }$ by pasting along $T$ . As an induced subgraph of $G _ { i }$ , each $H _ { i }$ can be coloured with $\omega ( H _ { i } )$ colours. Since $T$ is complete and hence coloured injectively, two such colourings, one of $H _ { 1 }$ and one of $H _ { 2 }$ , may be combined into a colouring of $H$ with max $\{ \omega ( H _ { 1 } ) , \omega ( H _ { 2 } ) \} \leqslant \omega ( H )$ colours—if necessary by permuting the colours in one of the $H _ { i }$ . 

By definition, every induced subgraph of a perfect graph is again perfect. The property of perfection can therefore be characterized by forbidden induced subgraphs: there exists a set $\mathcal { H }$ of imperfect graphs such that any graph is perfect if and only if it has no induced subgraph isomorphic to an element of $\mathcal { H }$ . (For example, we may choose as $\mathcal { H }$ the set of all imperfect graphs with vertices in $\mathbb { N }$ .)

Naturally, one would like to keep $\mathcal { H }$ as small as possible. It is one of the deepest results in graph theory that $\mathcal { H }$ need only contain two types of graph: the odd cycles of length $\geqslant 5$ and their complements. (Neither of these are perfect; cf. Theorem 5.5.4 below.) This fact, the famous strong perfect graph conjecture of Berge (1963), was proved only very recently:

Theorem 5.5.3. (Chudnovsky, Robertson, Seymour & Thomas 2002) A graph $G$ is perfect if and only if neither $G$ nor $\overline { G }$ contains an odd cycle of length at least 5 as an induced subgraph.

The proof of the strong perfect graph theorem is long and technical, and it would not be too illuminating to attempt to sketch it. To shed more light on the notion of perfection, we instead give two direct proofs of its most important consequence: the perfect graph theorem, formerly Berge’s weak perfect graph conjecture:

# Theorem 5.5.4. (Lov´asz 1972)

A graph is perfect if and only if its complement is perfect.

The first proof we give for Theorem 5.5.4 is Lov´asz’s original proof, which is still unsurpassed in its clarity and the amount of ‘feel’ for the problem it conveys. Our second proof, due to Gasparian (1996), is an elegant linear algebra proof of another theorem of Lov´asz’s (Theorem 5.5.6), which easily implies Theorem 5.5.4.

Let us prepare our first proof of Theorem 5.5.4 by a lemma. Let $G$ be a graph and $x \in G$ a vertex, and let $G ^ { \prime }$ be obtained from $G$ by adding a vertex $x ^ { \prime }$ and joining it to $x$ and all the neighbours of $x$ . We say that $G ^ { \prime }$ is obtained from $G$ by expanding the vertex $x$ to an edge $x x ^ { \prime }$ (Fig. 5.5.2).

![](images/1153290dbac2333f8cf78bfd80c8f95ffbfbade18e8409246c2d2ec432d1cf6f.jpg)  
Fig. 5.5.2. Expanding the vertex $x$ in the proof of Lemma 5.5.5

Lemma 5.5.5. Any graph obtained from a perfect graph by expanding a vertex is again perfect.

Proof . We use induction on the order of the perfect graph considered. Expanding the vertex of $K ^ { 1 }$ yields $K ^ { 2 }$ , which is perfect. For the induction step, let $G$ be a non-trivial perfect graph, and let $G ^ { \prime }$ be obtained from $G$ by expanding a vertex $x \in G$ to an edge $x x ^ { \prime }$ . For our proof that $G ^ { \prime }$ is perfect it suffices to show $\chi ( G ^ { \prime } ) \leqslant \omega ( G ^ { \prime } )$ : every proper induced subgraph $H$ of $G ^ { \prime }$ is either isomorphic to an induced subgraph of $G$ or obtained from a proper induced subgraph of $G$ by expanding $x$ ; in either case, $H$ is perfect by assumption and the induction hypothesis, and can hence be coloured with $\omega ( H )$ colours.

Let $\omega ( G ) = : \omega$ ; then $\omega ( G ^ { \prime } ) \in \{ \omega , \omega + 1 \}$ . If $\omega ( G ^ { \prime } ) = \omega + 1$ , then

$$
\chi (G ^ {\prime}) \leqslant \chi (G) + 1 = \omega + 1 = \omega (G ^ {\prime})
$$

and we are done. So let us assume that $\omega ( G ^ { \prime } ) = \omega$ . Then $x$ lies in no $K ^ { \omega } \subseteq G$ : together with $x ^ { \prime }$ , this would yield a $K ^ { \omega + 1 }$ in $G ^ { \prime }$ . Let us colour $G$ with $\omega$ colours. Since every $K ^ { \omega } \subseteq G$ meets the colour class $X$ of $x$ but not $x$ itself, the graph $H : = G - ( X \setminus \{ x \} )$ has clique number $\omega ( H ) < \omega$ ， (Fig. 5.5.2). Since $G$ is perfect, we may thus colour $H$ with $\omega - 1$ colours. Now $X$ is independent, so the set $( X \setminus \{ x \} ) \cup \{ x ^ { \prime } \} = V ( G ^ { \prime } - H )$ is also

independent. We can therefore extend our $( \omega - 1 )$ -colouring of $H$ to an $\omega$ -colouring of $G ^ { \prime }$ , showing that $\chi ( G ^ { \prime } ) \leqslant \omega = \omega ( G ^ { \prime } )$ as desired. 

Proof of Theorem 5.5.4. Applying induction on $| G |$ , we show that the complement $\overline { G }$ of any perfect graph $G = ( V , E )$ is again perfect. For $| G | = 1$ this is trivial, so let $| G | \geqslant 2$ for the induction step. Let $\kappa$ denote the set of all vertex sets of complete subgraphs of $G$ . Put $\alpha ( G ) = : \alpha$ , and let $\mathcal { A }$ be the set of all independent vertex sets $A$ in $G$ with $| A | = \alpha$ .

Every proper induced subgraph of $\overline { { G } }$ is the complement of a proper induced subgraph of $G$ , and is hence perfect by induction. For the perfection of $\overline { { G } }$ it thus suffices to prove $\chi ( \overline { { G } } ) \leqslant \omega ( \overline { { G } } )$ $\ = \alpha$ ). To this end, we shall find a set $K \in \mathcal { K }$ such that $K \cap A \neq \emptyset$ for all $A \in { \mathcal { A } }$ ; then

$$
\omega (\bar {G} - K) = \alpha (G - K) <   \alpha = \omega (\bar {G}),
$$

so by the induction hypothesis

$$
\chi (\overline {{G}}) \leqslant \chi (\overline {{G}} - K) + 1 = \omega (\overline {{G}} - K) + 1 \leqslant \omega (\overline {{G}})
$$

as desired.

Suppose there is no such $K$ ; thus, for every $K \in \mathcal { K }$ there exists a set $A _ { K } \in { \mathcal { A } }$ with $K \cap A _ { K } = \varnothing$ . Let us replace in $G$ every vertex $x$ by a complete graph $G _ { x }$ of order

$$
k (x) := \left| \left\{K \in \mathcal {K} \mid x \in A _ {K} \right\} \right|,
$$

joining all the vertices of $G _ { x }$ to all the vertices of $G _ { y }$ whenever $x$ and $y$ are adjacent in $G$ . The graph $G ^ { \prime }$ thus obtained has vertex set $\textstyle \bigcup _ { x \in V } V ( G _ { x } )$ , and two vertices $v \in G _ { x }$ and $w \in G _ { y }$ are adjacent in $G ^ { \prime }$ if and only if $x \ : = \ : y$ or $x y \in E$ . Moreover, $G ^ { \prime }$ can be obtained by repeated vertex expansion from the graph $G \left[ \left\{ x \in V \mid k ( x ) > 0 \right\} \right]$ ]. Being an induced subgraph of $G$ , this latter graph is perfect by assumption, so $G ^ { \prime }$ is perfect by Lemma 5.5.5. In particular,

$$
\chi \left(G ^ {\prime}\right) \leqslant \omega \left(G ^ {\prime}\right). \tag {1}
$$

In order to obtain a contradiction to (1), we now compute in turn the actual values of $\omega ( G ^ { \prime } )$ and $\chi ( G ^ { \prime } )$ . By construction of $G ^ { \prime }$ , every maximal complete subgraph of $G ^ { \prime }$ has the form $G ^ { \prime } \left[ \bigcup _ { x \in X } G _ { x } \right]$ for some $X \in \mathcal { K }$ . So there exists a set $X \in \mathcal { K }$ such that

$$
\begin{array}{l} \omega (G ^ {\prime}) = \sum_ {x \in X} k (x) \\ = \left| \left\{\left(x, K\right): x \in X, K \in \mathcal {K}, x \in A _ {K} \right\} \right| \\ = \sum_ {K \in \mathcal {K}} | X \cap A _ {K} | \\ \leqslant | \mathcal {K} | - 1; \tag {2} \\ \end{array}
$$

the last inequality follows from the fact that $| X \cap A _ { K } | \leqslant 1$ for all $K$ (since $A _ { K }$ is independent but $G \left[ X \right]$ is complete), and $| X \cap A _ { X } | = 0$ (by the choice of $A _ { X }$ ). On the other hand,

$$
\begin{array}{l} | G ^ {\prime} | = \sum_ {x \in V} k (x) \\ = \left| \left\{\left(x, K\right): x \in V, K \in \mathcal {K}, x \in A _ {K} \right\} \right| \\ = \sum_ {K \in \mathcal {K}} | A _ {K} | \\ = | \mathcal {K} | \cdot \alpha . \\ \end{array}
$$

As $\alpha ( G ^ { \prime } ) \leqslant \alpha$ by construction of $G ^ { \prime }$ , this implies

$$
\chi \left(G ^ {\prime}\right) \geqslant \frac {\left| G ^ {\prime} \right|}{\alpha \left(G ^ {\prime}\right)} \geqslant \frac {\left| G ^ {\prime} \right|}{\alpha} = | \mathcal {K} |. \tag {3}
$$

Putting (2) and (3) together we obtain

$$
\chi (G ^ {\prime}) \geqslant | \mathcal {K} | > | \mathcal {K} | - 1 \geqslant \omega (G ^ {\prime}),
$$

a contradiction to (1).

At first reading, the proof of Theorem 5.5.4 appears magical: it starts with an unmotivated lemma about expanding a vertex, shifts the problem to a strange graph $G ^ { \prime }$ obtained in this way, performs some double counting—and finished. With hindsight, however, we can understand it a little better. The proof is completely natural up to the point where we assume that for every $K \in \mathcal { K }$ there is an $A _ { K } \in { \mathcal { A } }$ such that $K \cap A _ { K } = \varnothing$ . To show that this contradicts our assumption that $G$ is perfect, we would like to show next that its subgraph $\tilde { G }$ induced by all the $A _ { K }$ has a chromatic number that is too large, larger than its clique number. And, as always when we try to bound the chromatic number from below, our only hope is to bound $| \bar { G } | / \alpha$ instead, i.e. to show that this is larger than $\omega ( \tilde { G } )$ .

But is the bound of $| \bar { G } | / \alpha$ likely to reflect the true value of $\chi ( \tilde { G } )$ ? In one special case it is: if the sets $A _ { K }$ happen to be disjoint, we have $| \bar { G } | = | \kappa | \cdot \alpha$ and $\chi ( \tilde { G } ) = | \mathcal { K } |$ , with the $A _ { K }$ as colour classes. Of course, the sets $A _ { K }$ will not in general be disjoint. But we can make them so: by replacing every vertex $x$ with $k ( x )$ vertices, where $k ( x )$ is the number of sets $A _ { K }$ it lives in! This is the idea behind $G ^ { \prime }$ . What remains is to endow $G ^ { \prime }$ with the right set of edges to make it perfect (assuming that $G$ is perfect)—which leads straight to the definition of vertex expansion and Lemma 5.5.5.

Since the following characterization of perfection is symmetrical in $G$ and $\overline { { G } }$ , it clearly implies Theorem 5.5.4. As our proof of Theorem 5.5.6 will again be from first principles, we thus obtain a second and independent proof of Theorem 5.5.4.

# Theorem 5.5.6. (Lov´asz 1972)

$A$ graph $G$ is perfect if and only if

$$
| H | \leqslant \alpha (H) \cdot \omega (H) \tag {*}
$$

for all induced subgraphs $H \subseteq G$ .

$V , v _ { i } , n$ α, ω

Proof . Let us write $V ( G ) = : V = : \{ v _ { 1 } , . . . , v _ { n } \}$ , and put $\alpha : = \alpha ( G )$ and $\omega : = \omega ( G )$ . The necessity of ( ) is immediate: if $G$ is perfect, then every induced subgraph $H$ of $G$ can be partitioned into at most $\omega ( H )$ colour classes each containing at most $\alpha ( H )$ vertices, and ( ) follows.

To prove sufficiency, we apply induction on $n = | G |$ . Assume that every induced subgraph $H$ of $G$ satisfies ( ), and suppose that $G$ is not perfect. By the induction hypothesis, every proper induced subgraph of $G$ is perfect. Hence, every non-empty independent set $U \subseteq V$ satisfies

$$
\chi (G - U) = \omega (G - U) = \omega . \tag {1}
$$

Indeed, while the first equality is immediate from the perfection of $G - U$ , the second is easy: $\leqslant { \overrightarrow { } }$ is obvious, while $\chi ( G - U ) < \omega$ would imply $\chi ( G ) \leqslant \omega$ , so $G$ would be perfect contrary to our assumption.

Let us apply (1) to a singleton $U = \{ u \}$ and consider an $\omega$ -colouring of $G - u$ . Let $K$ be the vertex set of any $K ^ { \omega }$ in $G$ . Clearly,

$$
i f u \notin K \text {t h e n} K \text {m e e t s e v e r y c o l o u r c l a s s o f} G - u; \tag {2}
$$

$$
i f u \in K \text {t h e n} K \text {m e e t s a l l b u t e x a c t l y o n e c o l o u r c l a s s o f} G - u. \tag {3}
$$

$A _ { 0 }$ $A _ { i }$ $K _ { i }$

Let $A _ { 0 } ~ = ~ \left\{ ~ u _ { 1 } , \ldots , u _ { \alpha } \right\}$ be an independent set in $G$ of size $\alpha$ . Let $A _ { 1 } , \ldots , A _ { \omega }$ be the colour classes of an $\omega$ -colouring of $G - u _ { 1 }$ , let $A _ { \omega + 1 } , \ldots , A _ { 2 \omega }$ be the colour classes of an $\omega \cdot$ -colouring of $G - u _ { 2 }$ , and so on; altogether, this gives us $\alpha \omega + 1$ independent sets $A _ { 0 } , A _ { 1 } , \ldots , A _ { \alpha \omega }$ in $G$ . For each $i = 0 , \ldots , \alpha \omega$ , there exists by (1) a $K ^ { \omega } \subseteq G - A _ { i }$ ; we denote its vertex set by $K _ { i }$ .

Note that if $K$ is the vertex set of any $K ^ { \omega }$ in $G$ , then

$$
K \cap A _ {i} = \emptyset \text {f o r e x a c t l y o n e} i \in \{0, \dots , \alpha \omega \}. \tag {4}
$$

Indeed, if $K \cap A _ { 0 } = \emptyset$ then $K \cap A _ { i } \neq \emptyset$ for all $i \neq 0$ , by definition of $A _ { i }$ and (2). Similarly if $K \cap A _ { 0 } \neq \emptyset$ , then $| K \cap A _ { 0 } | = 1$ , so $K \cap A _ { i } = \emptyset$ for

exactly one $i \neq 0$ : apply (3) to the unique vertex $u \in K \cap A _ { 0 }$ , and (2) to all the other vertices $u \in A _ { 0 }$ .

Let $J$ be the real $( \alpha \omega + 1 ) \times ( \alpha \omega + 1 )$ matrix with zero entries in the main diagonal and all other entries 1. Let $A$ be the real $( \alpha \omega + 1 ) \times n$ matrix whose rows are the incidence vectors of the subsets $A _ { i } \subseteq V$ : if $a _ { i 1 } , \ldots , a _ { i n }$ denote the entries of the $i$ th row of $A$ , then $a _ { i j } = 1$ if $v _ { j } \in A _ { i }$ , and $a _ { i j } ~ = ~ 0$ otherwise. Similarly, let $B$ denote the real $n \times ( \alpha \omega + 1 )$ matrix whose columns are the incidence vectors of the subsets $K _ { i } \subseteq V$ . Now while $\left| K _ { i } \cap A _ { i } \right| = 0$ for all $i$ by the choice of $K _ { i }$ , we have $K _ { i } \cap A _ { j } \neq \varnothing$ and hence $| K _ { i } \cap A _ { j } | = 1$ whenever $i \neq j$ , by (4). Thus,

$$
A B = J.
$$

Since $J$ is non-singular, this implies that $A$ has rank $\alpha \omega + 1$ . In particular, $n \geqslant \alpha \omega + 1$ , which contradicts ( ) for $H : = G$ . 

# Exercises

1. $-$ Show that the four colour theorem does indeed solve the map colouring problem stated in the first sentence of the chapter. Conversely, does the 4-colourability of every map imply the four colour theorem?   
2.− Show that, for the map colouring problem above, it suffices to consider maps such that no point lies on the boundary of more than three countries. How does this affect the proof of the four colour theorem?   
3. Try to turn the proof of the five colour theorem into one of the four colour theorem, as follows. Defining $v$ and $H$ as before, assume inductively that $H$ has a 4-colouring; then proceed as before. Where does the proof fail?   
4. Calculate the chromatic number of a graph in terms of the chromatic numbers of its blocks.   
5.− Show that every graph $G$ has a vertex ordering for which the greedy algorithm uses only $\chi ( G )$ colours.   
6. For every $n > 1$ , find a bipartite graph on $2 n$ vertices, ordered in such a way that the greedy algorithm uses $n$ rather than 2 colours.   
7. Consider the following approach to vertex colouring. First, find a maximal independent set of vertices and colour these with colour 1; then find a maximal independent set of vertices in the remaining graph and colour those 2, and so on. Compare this algorithm with the greedy algorithm: which is better?   
8. Show that the bound of Proposition 5.2.2 is always at least as sharp as that of Proposition 5.2.1.

9. Find a lower bound for the colouring number in terms of average degree.   
10. $-$ A $k$ -chromatic graph is called critically $k$ -chromatic, or just critical , if $\chi ( G - v ) \ < \ k$ for every $v ~ \in ~ V ( G )$ . Show that every $k$ -chromatic graph has a critical $k$ -chromatic induced subgraph, and that any such subgraph has minimum degree at least $k - 1$ .   
11. Determine the critical 3-chromatic graphs.   
12.+ Show that every critical $k$ -chromatic graph is $( k - 1 )$ - edge-connected.   
13. Given $k ~ \in ~ \mathbb { N }$ , find a constant $c _ { k } ~ > ~ 0$ such that every large enough graph $G$ with $\alpha ( G ) \leqslant k$ contains a cycle of length at least $c _ { k } \mid G \mid$ .   
14. $-$ Find a graph $G$ for which Brooks’s theorem yields a significantly weaker bound on $\chi ( G )$ than Proposition 5.2.2.   
15.+ Show that, in order to prove Brooks’s theorem for a graph $G = ( V , E )$ , we may assume that $\kappa ( G ) \geqslant 2$ and $\Delta ( G ) \geqslant 3$ . Prove the theorem under these assumptions, showing first the following two lemmas.

(i) Let $v _ { 1 } , \ldots , v _ { n }$ be an enumeration of $V$ . If every $v _ { i }$ $i < n$ ) has a neighbour $v _ { j }$ with $j > i$ , and if $v _ { 1 } v _ { n } , v _ { 2 } v _ { n } \in E$ but $v _ { 1 } v _ { 2 } \notin E$ , then the greedy algorithm uses at most $\Delta ( G )$ colours.   
(ii) If $G$ is not complete and $v _ { n }$ has maximum degree in $G$ , then $v _ { n }$ has neighbours $v _ { 1 } , v _ { 2 }$ as in (i).

16.+ Show that the following statements are equivalent for a graph $G$

(i) $\chi ( G ) \leqslant k$ ;   
(ii) $G$ has an orientation without directed paths of length $k - 1$ ;   
(iii) $G$ has an acyclic such orientation (one without directed cycles).

17. Given a graph $G$ and $k \in \mathbb N$ , let $P _ { G } ( k )$ denote the number of vertex colourings $V ( G ) \to \{ 1 , \dots , k \}$ . Show that $P _ { G }$ is a polynomial in $k$ of degree $n : = | G |$ , in which the coefficient of $k ^ { \pi }$ is 1 and the coefficient of $k ^ { n - 1 }$ is $- \| G \|$ . ( $P _ { G }$ is called the chromatic polynomial of $G$ .)

(Hint. Apply induction on $\| G \|$ .)

18.+ Determine the class of all graphs $G$ for which $P _ { G } ( k ) = k ( k - 1 ) ^ { n - 1 }$ . (As in the previous exercise, let $n : = | G |$ , and let $P _ { G }$ denote the chromatic polynomial of $G$ .)   
19. In the definition of $k$ -constructible graphs, replace the axiom (ii) by

(ii) Every supergraph of a $k$ -constructible graph is $k$ -constructible; and the axiom (iii) by   
(iii) If $G$ is a graph with vertices $x , y _ { 1 } , y _ { 2 }$ such that $y _ { 1 } y _ { 2 } \ \in \ E ( G )$ but $x y _ { 1 } , x y _ { 2 } \notin E ( G )$ , and if both $G + x y _ { 1 }$ and $G + x y _ { 2 }$ are $k$ - constructible, then $G$ is $k$ -constructible.

Show that a graph is $k$ -constructible with respect to this new definition if and only if its chromatic number is at least $k$ .

20.− An $n \times n$ - matrix with entries from $\{ 1 , \ldots , n \}$ is called a Latin square if every element of $\{ 1 , \ldots , n \}$ appears exactly once in each column and exactly once in each row. Recast the problem of constructing Latin squares as a colouring problem.   
21. Without using Proposition 5.3.1, show that $\chi ^ { \prime } ( G ) = k$ for every $k$ - regular bipartite graph $G$ .   
22. Prove Proposition 5.3.1 from the statement of the previous exercise.   
23.+ For every $k \in \mathbb N$ , construct a triangle-free $k$ -chromatic graph.   
24. $-$ Without using Theorem 5.4.2, show that every plane graph is 6-listcolourable.   
25. For every integer $k$ , find a 2-chromatic graph whose choice number is at least $k$ .   
26.− Find a general upper bound for $\operatorname { c h } ^ { \prime } ( G )$ in terms of $\chi ^ { \prime } ( G )$ .   
27. Compare the choice number of a graph with its colouring number: which is greater? Can you prove the analogue of Theorem 5.4.1 for the colouring number?   
28.+ Prove that the choice number of $K _ { 2 } ^ { \prime }$ is $r$   
29. The total chromatic number $\chi ^ { \prime \prime } ( G )$ of a graph $G = ( V , E )$ is the least number of colours needed to colour the vertices and edges of $G$ simultaneously so that any adjacent or incident elements of $V \cup E$ are coloured differently. The total colouring conjecture says that $\chi ^ { \prime \prime } ( G ) \leqslant \Delta ( G ) + 2$ . Bound the total chromatic number from above in terms of the listchromatic index, and use this bound to deduce a weakening of the total colouring conjecture from the list colouring conjecture.   
30.− Does every oriented graph have a kernel? If not, does every graph admit an orientation in which every induced subgraph has a kernel? If not, does every graph admit an orientation that has a kernel?   
31.+ Prove Richardson’s theorem: every directed graph without odd directed cycles has a kernel.   
32. Show that every bipartite planar graph is 3-list-colourable.

(Hint. Apply the previous exercise and Lemma 5.4.3.)

33.− Show that perfection is closed neither under edge deletion nor under edge contraction.   
34. $-$ Deduce Theorem 5.5.6 from the strong perfect graph theorem.   
35. Let $\mathcal { H } _ { 1 }$ and $\mathcal { H } _ { 2 }$ be two sets of imperfect graphs, each minimal with the property that a graph is perfect if and only if it has no induced subgraph in $\mathcal { H } _ { i }$ ( $i = 1 , 2$ ). Do $\mathcal { H } _ { 1 }$ and $\mathcal { H } _ { 2 }$ contain the same graphs, up to isomorphism?   
36. Use K¨onig’s Theorem 2.1.1 to show that the complement of any bipartite graph is perfect.

37. Using the results of this chapter, find a one-line proof of the following theorem of K¨onig, the dual of Theorem 2.1.1: in any bipartite graph without isolated vertices, the minimum number of edges meeting all vertices equals the maximum number of independent vertices.   
38. A graph is called a comparability graph if there exists a partial ordering of its vertex set such that two vertices are adjacent if and only if they are comparable. Show that every comparability graph is perfect.   
39. A graph $G$ is called an interval graph if there exists a set $\{ I _ { v } \mid v \in V ( G ) \}$ of real intervals such that $I _ { u } \cap I _ { v } \neq \emptyset$ if and only if $u v \in E ( G )$ .

(i) Show that every interval graph is chordal.   
(ii) Show that the complement of any interval graph is a comparability graph.

(Conversely, a chordal graph is an interval graph if its complement is a comparability graph; this is a theorem of Gilmore and Hoffman (1964).)

40. Show that $\chi ( H ) \in \{ \omega ( H ) , \omega ( H ) + 1 \}$ for every line graph $H$ .   
41. $^ +$ Characterize the graphs whose line graphs are perfect.   
42. Show that a graph $G$ is perfect if and only if every non-empty induced subgraph $H$ of $G$ contains an independent set $A \subseteq V ( H )$ such that $\omega ( H - A ) < \omega ( H )$ .   
43.+ Consider the graphs $G$ for which every induced subgraph $H$ has the property that every maximal complete subgraph of $H$ meets every maximal independent vertex set in $H$ .

(i) Show that these graphs $G$ are perfect.   
(ii) Show that these graphs $G$ are precisely the graphs not containing an induced copy of $P ^ { 3 }$ .

44.+ Show that in every perfect graph $G$ one can find a set $\mathcal { A }$ of independent vertex sets and a set $\boldsymbol { \mathcal { O } }$ of vertex sets of complete subgraphs such that $\cup { \mathcal { A } } = V ( G ) = \bigcup { \mathcal { O } }$ and every set in $\mathcal { A }$ meets every set in $\boldsymbol { \mathcal { O } }$ . (Hint. Lemma 5.5.5.)

45.+ Let $G$ be a perfect graph. As in the proof of Theorem 5.5.4, replace every vertex $_ x$ of $G$ with a perfect graph $G _ { x }$ (not necessarily complete). Show that the resulting graph $G ^ { \prime }$ is again perfect.

# Notes

The authoritative reference work on all questions of graph colouring is T.R. Jensen & B. Toft, Graph Coloring Problems, Wiley 1995. Starting with a brief survey of the most important results and areas of research in the field, this monograph gives a detailed account of over 200 open colouring problems, complete with extensive background surveys and references. Most of the remarks below are discussed comprehensively in this book, and all the references for this chapter can be found there.

The four colour problem, whether every map can be coloured with four colours so that adjacent countries are shown in different colours, was raised by a certain Francis Guthrie in 1852. He put the question to his brother Frederick, who was then a mathematics undergraduate in Cambridge. The problem was first brought to the attention of a wider public when Cayley presented it to the London Mathematical Society in 1878. A year later, Kempe published an incorrect proof, which was in 1890 modified by Heawood into a proof of the five colour theorem. In 1880, Tait announced ‘further proofs’ of the four colour conjecture, which never materialized; see the notes for Chapter 10.

The first generally accepted proof of the four colour theorem was published by Appel and Haken in 1977. The proof builds on ideas that can be traced back as far as Kempe’s paper, and were developed largely by Birkhoff and Heesch. Very roughly, the proof sets out first to show that every plane triangulation must contain at least one of 1482 certain ‘unavoidable configurations’. In a second step, a computer is used to show that each of those configurations is ‘reducible’, i.e., that any plane triangulation containing such a configuration can be 4-coloured by piecing together 4-colourings of smaller plane triangulations. Taken together, these two steps amount to an inductive proof that all plane triangulations, and hence all planar graphs, can be 4- coloured.

Appel & Haken’s proof has not been immune to criticism, not only because of their use of a computer. The authors responded with a 741 page long algorithmic version of their proof, which addresses the various criticisms and corrects a number of errors (e.g. by adding more configurations to the ‘unavoidable’ list): K. Appel & W. Haken, Every Planar Map is Four Colorable, American Mathematical Society 1989. A much shorter proof, which is based on the same ideas (and, in particular, uses a computer in the same way) but can be more readily verified both in its verbal and its computer part, has been given by N. Robertson, D. Sanders, P.D. Seymour & R. Thomas, The four-colour theorem, J. Combin. Theory B 70 (1997), 2–44.

A relatively short proof of Gr¨otzsch’s theorem was found by C. Thomassen, A short list color proof of Gr¨otzsch’s theorem, J. Combin. Theory B 88 (2003), 189–192. Although not touched upon in this chapter, colouring problems for graphs embedded in surfaces other than the plane form a substantial and interesting part of colouring theory; see B. Mohar & C. Thomassen, Graphs on Surfaces, Johns Hopkins University Press 2001.

The proof of Brooks’s theorem indicated in Exercise 15, where the greedy algorithm is applied to a carefully chosen vertex ordering, is due to Lov´asz (1973). Lov´asz (1968) was also the first to construct graphs of arbitrarily large girth and chromatic number, graphs whose existence Erd˝os had proved by probabilistic methods ten years earlier.

A. Urquhart, The graph constructions of Haj´os and Ore, J. Graph Theory 26 (1997), 211–215, showed that not only do the graphs of chromatic number at least $k$ each contain a $k$ -constructible graph (as by Haj´os’s theorem); they are in fact all themselves $k$ -constructible. Algebraic tools for showing that the chromatic number of a graph is large have been developed by Kleitman & Lov´asz (1982), and by Alon & Tarsi (1992); see Alon’s paper cited below.

List colourings were first introduced in 1976 by Vizing. Among other things, Vizing proved the list-colouring equivalent of Brooks’s theorem. Voigt

(1993) constructed a plane graph of order 238 that is not 4-choosable; thus, Thomassen’s list version of the five colour theorem is best possible. A stimulating survey on the list-chromatic number and how it relates to the more classical graph invariants (including a proof of Theorem 5.4.1) is given by N. Alon, Restricted colorings of graphs, in (K. Walker, ed.) Surveys in Combinatorics, LMS Lecture Notes 187, Cambridge University Press 1993. Both the list colouring conjecture and Galvin’s proof of the bipartite case are originally stated for multigraphs. Kahn (1994) proved that the conjecture is asymptotically correct, as follows: given any $\epsilon > 0$ , every graph $G$ with large enough maximum degree satisfies $\mathrm { c h } ^ { \prime } ( G ) \leqslant ( 1 + \epsilon ) \Delta ( G )$ .

The total colouring conjecture was proposed around 1965 by Vizing and by Behzad; see Jensen & Toft for details.

A gentle introduction to the basic facts about perfect graphs and their applications is given by M.C. Golumbic, Algorithmic Graph Theory and Perfect Graphs, Academic Press 1980. A more comprehensive treatment is given in A. Schrijver, Combinatorial optimization, Springer 2003. Surveys on various aspects of perfect graphs are included in Perfect Graphs by J. Ramirez-Alfonsin & B. Reed (eds.), Wiley 2001. Our first proof of the perfect graph theorem, Theorem 5.5.4, follows L. Lov´asz’s survey on perfect graphs in (L.W. Beineke and R.J. Wilson, eds.) Selected Topics in Graph Theory 2, Academic Press 1983. Our second proof, the proof of Theorem 5.5.6, is due to G.S. Gasparian, Minimal imperfect graphs: a simple approach, Combinatorica 16 (1996), 209–212. Theorem 5.5.3 was proved by Chudnovsky, Robertson, Seymour and Thomas, The strong perfect graph theorem, Ann. of Math. (to appear). Chudnovsky, Cornuejols, Liu, Seymour and Vuˇskovi´c, Recognizing Berge graphs, Combinatorica 25 (2005), 143–186, constructed an $O ( n ^ { 9 } )$ algorithm testing for ‘holes’ (induced odd cycles of length at least 5) and ‘antiholes’ (their induced complements), and thus by the theorem for perfection.

Let us view a graph as a network: its edges carry some kind of flow—of water, electricity, data or similar. How could we model this precisely?

For a start, we ought to know how much flow passes through each edge $e \ = \ x y$ , and in which direction. In our model, we could assign a positive integer $k$ to the pair $( x , y )$ to express that a flow of $k$ units passes through $e$ from $x$ to $y$ , or assign $- k$ to $( x , y )$ to express that $k$ units of flow pass through $e$ the other way, from $y$ to $x$ . For such an assignment $f \colon V ^ { 2 } \to \mathbb { Z }$ we would thus have $f ( x , y ) = - f ( y , x )$ whenever $x$ and $y$ are adjacent vertices of $G$ .

Typically, a network will have only a few nodes where flow enters or leaves the network; at all other nodes, the total amount of flow into that node will equal the total amount of flow out of it. For our model this means that, at most nodes $x$ , the function $f$ will satisfy Kirchhoff ’s law

$$
\sum_ {y \in N (x)} f (x, y) = 0.
$$

Kirchhoff’s law

In this chapter, we call any map $f \colon V ^ { 2 } \to \mathbb { Z }$ with the above two properties a ‘flow’ on $G$ . Sometimes, we shall replace $\mathbb { Z }$ with another group, and as a rule we consider multigraphs rather than graphs.1 As it turns out, the theory of those ‘flows’ is not only useful as a model for real flows: it blends so well with other parts of graph theory that some deep and surprising connections become visible, connections particularly with connectivity and colouring problems.

# 6.1 Circulations

In the context of flows, we have to be able to speak about the ‘directions’ G = (V, E) of an edge. Since, in a multigraph $G = ( V , E )$ , an edge $e = x y$ is not identified uniquely by the pair $( x , y )$ or $( y , x )$ , we define directed edges as triples:

$$
\vec {E} \quad \vec {E} := \left\{\left(e, x, y\right) \mid e \in E; x, y \in V; e = x y \right\}.
$$

direction $( e , x , y )$

Thus, an edge $e = x y$ with $x \neq y$ has the two directions $( e , x , y )$ and $( e , y , x )$ ; a loop $e \ : = \ : x x$ has only one direction, the triple $( e , x , x )$ . For given $\overrightarrow { e } = ( e , x , y ) \in \overrightarrow { E }$ , we set $\overleftarrow { e } : = ( e , y , x )$ , and for an arbitrary set ${ \vec { F } } \subseteq { \vec { E } }$ of edge directions we put

$$
\bar {F} \quad \bar {F} := \left\{\bar {e} \mid \bar {e} \in \bar {F} \right\}.
$$

Note that $\overrightarrow { E }$ itself is symmetrical: $\overleftarrow { E } = \overrightarrow { E }$ . For $X , Y \subseteq V$ and ${ \vec { F } } \subseteq { \vec { E } }$ define

$$
\vec {F} (X, Y) \quad \vec {F} (X, Y) := \left\{\left(e, x, y\right) \in \vec {F} \mid x \in X; y \in Y; x \neq y \right\},
$$

${ \overrightarrow { F } } ( x , Y )$ abbreviate ${ \vec { F } } ( \{ x \} , Y )$ to $\vec { F } ( x , Y )$ etc., and write

$$
\bar {F} (x) \quad \bar {F} (x) := \bar {F} (x, V) = \bar {F} (\{x \}, \overline {{\{x \}}}).
$$

$\overline { { X } }$ Here, as below, $\overline { { X } }$ denotes the complement $V \setminus X$ of a vertex set $X \subseteq V$ . Note that any loops at vertices $x \in X \cap Y$ are disregarded in the definitions of ${ \vec { F } } ( X , Y )$ and $\vec { F } ( x )$ .

0 Let $H$ be an abelian semigroup,2 written additively with zero 0. $f$ Given vertex sets $X , Y \subseteq V$ and a function $f \colon { \overrightarrow { E } }  H$ , let

$$
f (X, Y) \quad f (X, Y) := \sum_ {\vec {e} \in \vec {E} (X, Y)} f (\vec {e}).
$$

$f ( x , Y )$ Instead of $f ( \{ x \} , Y )$ we again write $f ( x , Y )$ , etc.

circulation From now on, we assume that $H$ is a group. We call $f$ a circulation on $G$ (with values in $H$ ), or an $H$ -circulation, if $f$ satisfies the following two conditions:

(F1) $f ( e , x , y ) = - f ( e , y , x )$ for all $( e , x , y ) \in { \vec { E } }$ with $x \neq y$

(F2) $f ( v , V ) = 0$ for all $v \in V$ .

If $f$ satisfies (F1), then

$$
f (X, X) = 0
$$

for all $X \subseteq V$ . If $f$ satisfies (F2), then

$$
f (X, V) = \sum_ {x \in X} f (x, V) = 0.
$$

Together, these two basic observations imply that, in a circulation, the net flow across any cut is zero:

Proposition 6.1.1. If $f$ is a circulation, then $f ( X , { \overline { { X } } } ) = 0$ for every set $X \subseteq V$ .

Proof . $f ( X , { \overline { { X } } } ) = f ( X , V ) - f ( X , X ) = 0 - 0 = 0 .$

Since bridges form cuts by themselves, Proposition 6.1.1 implies that circulations are always zero on bridges:

Corollary 6.1.2. If $f$ is a circulation and $e = x y$ is a bridge in $G$ , then $f ( e , x , y ) = 0$ . 

# 6.2 Flows in networks

In this section we give a brief introduction to the kind of network flow theory that is now a standard proof technique in areas such as matching and connectivity. By way of example, we shall prove a classic result of this theory, the so-called max-flow min-cut theorem of Ford and Fulkerson. This theorem alone implies Menger’s theorem without much difficulty (Exercise 3), which indicates some of the natural power lying in this approach.

Consider the task of modelling a network with one source $s$ and one sink $t$ , in which the amount of flow through a given link between two nodes is subject to a certain capacity of that link. Our aim is to determine the maximum net amount of flow through the network from $s$ to $t$ . Somehow, this will depend both on the structure of the network and on the various capacities of its connections—how exactly, is what we wish to find out.

Let $G = ( V , E )$ be a multigraph, $s , t \in V$ two fixed vertices, and $c \colon { \vec { E } }  \mathbb { N }$ a map; we call $c$ a capacity function on $G$ , and the tuple $N : = ( G , s , t , c )$ a network . Note that $c$ is defined independently for the two directions of an edge. A function $f \colon \overrightarrow { E }  \mathbb { R }$ is a flow in $N$ if it satisfies the following three conditions (Fig. 6.2.1):

(F1) $f ( e , x , y ) = - f ( e , y , x )$ for all $( e , x , y ) \in { \vec { E } }$ with $x \neq y$   
(F2-) $f ( v , V ) = 0$ for all $v \in V \setminus \{ s , t \}$ ;   
(F3) $f ( \overrightarrow { e } ) \leqslant c ( \overrightarrow { e } )$ for all $\vec { e } \in \vec { E }$ .

integral We call $f$ integral if all its values are integers.

![](images/132dfb7418abe4687401ae09c61b15cef159c46c59883ecffea57bd7890c61b1.jpg)  
Fig. 6.2.1. A network flow in short notation: all values refer to the direction indicated (capacities are not shown)

$f$ cut in $N$ capacity

Let $f$ be a flow in $N$ . If $S \subseteq V$ is such that $s \in S$ and $t \in { \overline { { S } } }$ , we call the pair $( S , { \overline { { S } } } )$ a cut in $N$ , and $c ( S , { \overline { { S } } } )$ the capacity of this cut.

Since $f$ now has to satisfy only (F2-) rather than (F2), we no longer have $f ( X , { \overline { { X } } } ) = 0$ for all $X \subseteq V$ (as in Proposition 6.1.1). However, the value is the same for all cuts:

Proposition 6.2.1. Every cut $( S , { \overline { { S } } } )$ in $N$ satisfies $f ( S , { \overline { { S } } } ) = f ( s , V )$ .

Proof . As in the proof of Proposition 6.1.1, we have

$$
\begin{array}{l} f (S, \overline {{S}}) = f (S, V) - f (S, S) \\ \underset {(\mathrm {F} 1)} {=} f (s, V) + \sum_ {v \in S \setminus \{s \}} f (v, V) - 0 \\ \underset {(\mathrm {F} 2 ^ {\prime})} {=} f (s, V). \\ \end{array}
$$

![](images/e004659279cbc9ffccaf0534365f6547ee20bb0633da42ba3f44136334f3ed3f.jpg)

total value $| f |$

The common value of $f ( S , { \overline { { S } } } )$ in Proposition 6.2.1 will be called the total value of $f$ and denoted by $| f |$ ;3 the flow shown in Figure 6.2.1 has total value 3.

By (F3), we have

$$
| f | = f (S, \overline {{S}}) \leqslant c (S, \overline {{S}})
$$

for every cut $( S , { \overline { { S } } } )$ in $N$ . Hence the total value of a flow in $N$ is never larger than the smallest capacity of a cut. The following max-flow mincut theorem states that this upper bound is always attained by some flow:

# Theorem 6.2.2. (Ford & Fulkerson 1956)

In every network, the maximum total value of a flow equals the minimum capacity of a cut.

Proof . Let $N = ( G , s , t , c )$ be a network, and $G = : ( V , E )$ . We shall define a sequence $f _ { 0 } , f _ { 1 } , f _ { 2 } , \ldots$ of integral flows in $N$ of strictly increasing total value, i.e. with

$$
\left| f _ {0} \right| <   \left| f _ {1} \right| <   \left| f _ {2} \right| <   \dots
$$

Clearly, the total value of an integral flow is again an integer, so in fact $| f _ { n + 1 } | \geqslant | f _ { n } | + 1$ for all $n$ . Since all these numbers are bounded above by the capacity of any cut in $N$ , our sequence will terminate with some flow $f _ { n }$ . Corresponding to this flow, we shall find a cut of capacity $c _ { n } = | f _ { n } |$ . Since no flow can have a total value greater than $c _ { n }$ , and no cut can have a capacity less than $\left| f _ { n } \right|$ , this number is simultaneously the maximum and the minimum referred to in the theorem.

For $f _ { 0 }$ , we set $f _ { 0 } ( \overrightarrow { e } ) : = 0$ for all $\vec { e } \in \vec { E }$ . Having defined an integral flow $f _ { n }$ in $N$ for some $n \in \mathbb N$ , we denote by $S _ { n }$ the set of all vertices $v$ ， $S _ { n }$ such that $G$ contains an $s$ – $v$ walk $x _ { 0 } e _ { 0 } \ldots e _ { \ell - 1 } x _ { \ell }$ with

$$
f _ {n} (\vec {e _ {i}}) <   c (\vec {e _ {i}})
$$

for all $i < \ell$ ; here, ${ \overrightarrow { e _ { i } } } : = ( e _ { i } , x _ { i } , x _ { i + 1 } )$ (and, of course, $x _ { 0 } = s$ and $x _ { \ell } = v$ ).

If $t \in S _ { n }$ , let $W = x _ { 0 } e _ { 0 } \ldots e _ { \ell - 1 } x _ { \ell }$ be the corresponding $s$ – $\mathbf { \nabla } \cdot \mathbf { \vec { \tau } }$ walk; without loss of generality we may assume that $W$ does not repeat any vertices. Let

$$
\epsilon := \min  \left\{c \left(\vec {e _ {i} ^ {\prime}}\right) - f _ {n} (\vec {e _ {i}}) \mid i <   \ell \right\}.
$$

Then $\epsilon > 0$ , and since $f _ { n }$ (like $c$ ) is integral by assumption, $\epsilon$ is an integer. Let

$$
f _ {n + 1} \colon \vec {e} \mapsto \left\{ \begin{array}{l l} f _ {n} (\vec {e}) + \epsilon & \text {f o r} \vec {e} = \vec {e _ {i}}, i = 0, \ldots , \ell - 1; \\ f _ {n} (\vec {e}) - \epsilon & \text {f o r} \vec {e} = \vec {e _ {i}}, i = 0, \ldots , \ell - 1; \\ f _ {n} (\vec {e}) & \text {f o r} e \notin W. \end{array} \right.
$$

Intuitively, $f _ { n + 1 }$ is obtained from $f _ { n }$ by sending additional flow of value $\epsilon$ along $W$ from $s$ to $t$ (Fig. 6.2.2).

![](images/e4a1490f4974f17d9c0dd629fd9db2e55a9789e2853f678490a85e7fe2959cc1.jpg)  
Fig. 6.2.2. An ‘augmenting path’ $W$ with increment $\epsilon = 2$ , for constant flow $f _ { n } = 0$ and capacities $c = 3$

Clearly, $f _ { n + 1 }$ is again an integral flow in $N$ . Let us compute its total value $| f _ { n + 1 } | = f _ { n + 1 } ( s , V )$ . Since $W$ contains the vertex $s$ only once, $\overrightarrow { e _ { 0 } }$ is the only triple $( e , x , y )$ with $x \ = \ s$ and $y \in V$ whose $f$ -value was changed. This value, and hence that of $f _ { n + 1 } ( s , V )$ was raised. Therefore $\left| f _ { n + 1 } \right| > \left| f _ { n } \right|$ as desired.

If $t \notin S _ { n }$ , then $( S _ { n } , { \overline { { S _ { n } } } } )$ is a cut in $N$ . By (F3) for $f _ { n }$ , and the definition of $S _ { n }$ , we have

$$
f _ {n} (\vec {e}) = c (\vec {e})
$$

for all ${ \overrightarrow { e } } \in { \overrightarrow { E } } ( S _ { n } , { \overline { { S _ { n } } } } )$ , so

$$
\left| f _ {n} \right| = f _ {n} \left(S _ {n}, \overline {{S _ {n}}}\right) = c \left(S _ {n}, \overline {{S _ {n}}}\right)
$$

as desired.

![](images/9d892416d0ff36b13ff5ad464adef72f7274c8a000d29892552f2212ea6e1847.jpg)

Since the flow constructed in the proof of Theorem 6.2.2 is integral, we have also proved the following:

Corollary 6.2.3. In every network (with integral capacity function) there exists an integral flow of maximum total value. 

# 6.3 Group-valued flows

Let $G = ( V , E )$ be a multigraph and $H$ an abelian group. If $f$ and are two $H$ -circulations then, clearly, $( f + g ) \colon { \overrightarrow { e } } \mapsto f ( { \overrightarrow { e } } ) + g ( { \overrightarrow { e } } )$ and $g$ $- f \colon { \overrightarrow { e } } \mapsto - f ( { \overrightarrow { e } } )$ are again $H$ -circulations. The $H$ -circulations on $G$ thus form a group in a natural way.

A function $f \colon { \overrightarrow { E } }  H$ is nowhere zero if $f ( \overrightarrow { e } ) \neq 0$ for all $\vec { e } \in \vec { E }$ . An $H$ -circulation that is nowhere zero is called an $H$ -flow.4 Note that the set of $H$ -flows on $G$ is not closed under addition: if two $H$ -flows add up to zero on some edge $\overrightarrow { e }$ , then their sum is no longer an $H$ -flow. By Corollary 6.1.2, a graph with an $H$ -flow cannot have a bridge.

For finite groups $H$ , the number of $H$ -flows on $G$ —and, in particular, their existence—surprisingly depends only on the order of $H$ , not on $H$ itself:

# Theorem 6.3.1. (Tutte 1954)

For every multigraph $G$ there exists a polynomial $P$ such that, for any finite abelian group $H$ , the number of $H$ -flows on $G$ is $P ( | H | - 1 )$ .

$f + g$

nowhere zero

$H$ -flow

Proof . Let $G = : ( V , E )$ ; we use induction on $m : = | E |$ . Let us assume (6.1.1) first that all the edges of $G$ are loops. Then, given any finite abelian group $H$ , every map $\vec { E }  H \setminus \{ 0 \}$ is an $H$ -flow on $G$ . Since $| \vec { E } | = | E |$ when all edges are loops, there are $\left( \left| H \right| - 1 \right) ^ { \prime \prime \iota }$ such maps, and $P : = x ^ { m }$ is the polynomial sought.

Now assume there is an edge $e _ { 0 } = x y \in E$ that is not a loop; let $\vec { e _ { 0 } } : = ( e _ { 0 } , x , y )$ and $E ^ { \prime } : = E \setminus \{ e _ { 0 } \}$ . We consider the multigraphs

$$
G _ {1} := G - e _ {0} \quad \text {a n d} \quad G _ {2} := G / e _ {0}.
$$

By the induction hypothesis, there are polynomials $P _ { i }$ for $i = 1 , 2$ such that, for any finite abelian group $H$ and $k : = | H | - 1$ , the number of $H$ -flows on $G _ { i }$ is $P _ { i } ( k )$ . We shall prove that the number of $H$ -flows on $G$ equals $P _ { 2 } ( k ) - P _ { 1 } ( k )$ ; then $P : = P _ { 2 } - P _ { 1 }$ is the desired polynomial.

Let $H$ be given, and denote the set of all $H$ -flows on $G$ by $F ^ { \prime }$ . We are trying to show that

$$
| F | = P _ {2} (k) - P _ {1} (k). \tag {1}
$$

The $H$ -flows on $G _ { 1 }$ are precisely the restrictions to $E ^ { \prime }$ of those $H$ -circulations on $G$ that are zero on $e _ { 0 }$ but nowhere else. Let us denote the set of these circulations on $G$ by $F _ { 1 }$ ; then

$$
P _ {1} (k) = | F _ {1} |.
$$

Our aim is to show that, likewise, the $H$ -flows on $G _ { 2 }$ correspond bijectively to those $H$ -circulations on $G$ that are nowhere zero except possibly on $e _ { 0 }$ . The set $F _ { 2 }$ of those circulations on $G$ then satisfies

$$
P _ {2} (k) = | F _ {2} |,
$$

and $F _ { 2 }$ is the disjoint union of $F _ { 1 }$ and $F ^ { \prime }$ . This will prove (1), and hence the theorem.

![](images/65ec477dddf8851156e77f28913a6387f3d35b143063ba34bbb01c59d4570ef0.jpg)  
Fig. 6.3.1. Contracting the edge $e _ { 0 }$

In $G _ { 2 }$ , let $v _ { 0 } : = v _ { e _ { 0 } }$ be the vertex contracted from $e _ { 0 }$ (Fig. 6.3.1; see Chapter 1.10). We are looking for a bijection $f \mapsto g$ between $F _ { 2 }$

$$
e _ {0} = x y
$$

$$
E ^ {\prime}
$$

$$
P _ {1}, P _ {2}
$$

$$
k
$$

$$
H
$$

$$
F
$$

$F _ { 1 }$

$F _ { 2 } ^ { \prime }$

and the set of $H$ -flows on $G _ { 2 }$ . Given $f$ , let $g$ be the restriction of $f$ to $\vec { E ^ { \prime } } \setminus \vec { E ^ { \prime } } ( y , x )$ . (As the $x$ – $_ y$ edges $\textit { e } \in \textit { E } ^ { \prime }$ become loops in $G _ { 2 }$ , they have only the one direction $( e , v _ { 0 } , v _ { 0 } )$ there; as its $g$ -value, we choose $f ( e , x , y )$ .) Then $g$ is indeed an $H$ -flow on $G _ { 2 }$ ; note that (F2) holds at $v _ { 0 }$ by Proposition 6.1.1 for $G$ , with $X : = \{ x , y \}$ .

It remains to show that the map $f \mapsto g$ is a bijection. If we are given an $H$ -flow $g$ on $G _ { 2 }$ and try to find an $f \in F _ { 2 }$ with $f \mapsto g$ , then $f ( \overrightarrow { e } )$ is already determined as $f ( \overrightarrow { e } ) = g ( \overrightarrow { e } )$ for all $\overrightarrow { e } \in \overrightarrow { E ^ { \prime } } \setminus \overrightarrow { E ^ { \prime } } ( y , x )$ ; by (F1), we further have $f ( \overrightarrow { e } ) = - f ( \overleftarrow { e } )$ for all $\overrightarrow { e } \in \overrightarrow { E ^ { \prime } } ( y , x )$ . Thus our map $f \mapsto g$ is bijective if and only if for given $g$ there is always a unique way to define the remaining values of $f ( \overrightarrow { e _ { 0 } } )$ and $f ( \overleftarrow { e _ { 0 } } )$ so that $f$ satisfies (F1) in $e _ { 0 }$ and (F2) in $x$ and $y$ .

Now $f ( \overrightarrow { e _ { 0 } } )$ is already determined by (F2) for $x$ and the known values of $f ( \overrightarrow { e } )$ for edges $e$ at $x$ , while $f ( \overleftarrow { e _ { 0 } } )$ is already determined by (F2) for $y$ r and the known values of $f ( \overrightarrow { e } )$ for edges $e$ at $y$ . Indeed, with

$$
h := \sum_ {\vec {e} \in \vec {E ^ {\prime}} (x, y)} f (\vec {e}) \quad \Big (\stackrel {} {=} \sum_ {e \in E ^ {\prime} (x, y)} g (e, v _ {0}, v _ {0}) \Big)
$$

and $V ^ { \prime } : = V \setminus \{ x , y \}$ , (F2) will hold for $f$ if and only if

$$
0 = f (x, V) = f \left(\vec {e} _ {0}\right) + h + f (x, V ^ {\prime})
$$

and

$$
0 = f (y, V) = f \left(\bar {e} _ {0}\right) - h + f (y, V ^ {\prime}),
$$

that is, if and only if we set

$$
f \left(\bar {e} _ {0}\right) := - f (x, V ^ {\prime}) - h \quad \text {a n d} \quad f \left(\bar {e} _ {0}\right) := - f (y, V ^ {\prime}) + h.
$$

Fortunately, defining $f ( \overrightarrow { e _ { 0 } } )$ and $f ( \overleftarrow { e _ { 0 } } )$ in this way also satisfies (F1) for $f$ , as

$$
f (\bar {e _ {0}}) + f (\bar {e _ {0}}) = - f (x, V ^ {\prime}) - f (y, V ^ {\prime}) = - g (v _ {0}, V ^ {\prime}) = 0
$$

by (F2) for $g$ at $v _ { 0 }$ .

flow polynomial

The polynomial $P$ of Theorem 6.3.1 is known as the flow polynomial of $G$ .

[ 6.4.5 ]

Corollary 6.3.2. If $H$ and $H ^ { \prime }$ are two finite abelian groups of equal order, then $G$ has an $H$ -flow if and only if $G$ has an $H ^ { \prime }$ -flow. 

Corollary 6.3.2 has fundamental implications for the theory of algebraic flows: it indicates that crucial difficulties in existence proofs of $H$ -flows are unlikely to be of a group-theoretic nature. On the other hand, being able to choose a convenient group can be quite helpful; we shall see a pretty example for this in Proposition 6.4.5.

Let $k \geqslant 1$ be an integer and $G = ( V , E )$ a multigraph. A $\mathbb { Z }$ -flow $f$ on $G$ such that $0 < | f ( \overrightarrow { e } ) | < k$ for all $\overrightarrow { e } \in \overrightarrow { E }$ is called a $k$ -flow. Clearly, any $k$ -flow is also an $\ell$ -flow for all $\ell > k$ . Thus, we may ask which is the least integer $k$ such that $G$ admits a $k$ -flow—assuming that such a $k$ exists. We call this least $k$ the flow number of $G$ and denote it by $\varphi ( G )$ ; if $G$ has no $k$ -flow for any $k$ , we put $\varphi ( G ) : = \infty$ .

The task of determining flow numbers quickly leads to some of the deepest open problems in graph theory. We shall consider these later in the chapter. First, however, let us see how $k$ -flows are related to the more general concept of $H$ -flows.

There is an intimate connection between $k$ -flows and $\mathbb { Z } _ { k }$ -flows. Let $\sigma _ { k }$ denote the natural homomorphism $i \mapsto { \bar { i } }$ from $\mathbb { Z }$ to $\mathbb { Z } _ { k }$ . By composition with $\sigma _ { k }$ , every $k$ -flow defines a $\mathbb { Z } _ { k }$ -flow. As the following theorem shows, the converse holds too: from every $\mathbb { Z } _ { k }$ -flow on $G$ we can construct a $k$ -flow on $G$ . In view of Corollary 6.3.2, this means that the general question about the existence of $H$ -flows for arbitrary groups $H$ reduces to the corresponding question for $k$ -flows.

# Theorem 6.3.3. (Tutte 1950)

A multigraph admits a $k$ -flow if and only if it admits a $\mathbb { Z } _ { k }$ -flow.

Proof . Let $g$ be a $\mathbb { Z } _ { k }$ -flow on a multigraph $G = ( V , E )$ ; we construct a $k$ -flow $f$ on $G$ . We may assume without loss of generality that $G$ has no loops. Let $F$ be the set of all functions $f \colon { \overrightarrow { E } }  \mathbb { Z }$ that satisfy (F1), $| f ( \overrightarrow { e } ) | < k$ for all $\vec { e } \in \vec { E }$ , and $\sigma _ { k } \circ f = g$ ; note that, like $g$ , any $f \in F$ is nowhere zero.

Let us show first that $F \neq \emptyset$ . Since we can express every value $g ( \vec { e } ) \in \mathbb { Z } _ { k }$ as $\bar { i }$ with $| i | < k$ and then put $f ( \overrightarrow { e } ) : = i$ , there is clearly a map $f \colon E \to \mathbb { Z }$ such that $| f ( \vec { e } ) | < k$ for all $\vec { e } \in E$ and $\sigma _ { k } \circ f = g$ . For each edge $e \in E$ , let us choose one of its two directions and denote this by $\overrightarrow { e }$ . We may then define $f ^ { \prime } \colon \overrightarrow { E }  \mathbb { Z }$ by setting $f ^ { \prime } ( \overrightarrow { e } ) : = f ( \overrightarrow { e } )$ and $f ^ { \prime } ( \overleftarrow { e } ) : = - f ( \overrightarrow { e } )$ for every $e \in E$ . Then $f ^ { \prime }$ is a function satisfying (F1) and with values in the desired range; it remains to show that $\sigma _ { k } \circ f ^ { \prime }$ and $g$ agree not only on the chosen directions $\overrightarrow { e }$ but also on their inverses $\scriptstyle { \overleftarrow { e } }$ . Since $\sigma _ { k }$ is a homomorphism, this is indeed so:

$$
\left(\sigma_ {k} \circ f ^ {\prime}\right) (\bar {e}) = \sigma_ {k} (- f (\bar {e})) = - \left(\sigma_ {k} \circ f\right) (\bar {e}) = - g (\bar {e}) = g (\bar {e}).
$$

Hence $f ^ { \prime } \in F$ , so $F$ is indeed non-empty.

Our aim is to find an $f \in F$ that satisfies Kirchhoff’s law (F2), and is thus a $k$ -flow. As a candidate, let us consider an $f \in F$ for which the sum

$$
K (f) := \sum_ {x \in V} | f (x, V) |
$$

of all deviations from Kirchhoff’s law is least possible. We shall prove that $K ( f ) = 0$ ; then, clearly, $f ( x , V ) = 0$ for every $x$ , as desired.

Suppose $K ( f ) \neq 0$ . Since $f$ satisfies (F1), and hence $\textstyle \sum _ { x \in V } f ( x , V ) =$ $f ( V , V ) = 0$ , there exists a vertex $x$ with

$$
f (x, V) > 0. \tag {1}
$$

Let $X \subseteq V$ be the set of all vertices $x ^ { \prime }$ for which $G$ contains a walk $x _ { 0 } e _ { 0 } \ldots e _ { \ell - 1 } x _ { \ell }$ from $x$ to $x ^ { \prime }$ such that $f ( e _ { i } , x _ { i } , x _ { i + 1 } ) > 0$ for all $i < \ell$ ; furthermore, let $X ^ { \prime } : = X \setminus \{ x \}$ .

We first show that $X ^ { \prime }$ contains a vertex $x ^ { \prime }$ with $f ( x ^ { \prime } , V ) < 0$ . By definition of $X$ , we have $f ( e , x ^ { \prime } , y ) \leqslant 0$ for all edges $e = x ^ { \prime } y$ such that $x ^ { \prime } \in X$ and $y \in { \overline { { X } } }$ . In particular, this holds for $x ^ { \prime } = x$ . Thus, (1) implies $f ( x , X ^ { \prime } ) > 0$ . Then $f ( X ^ { \prime } , x ) < 0$ by (F1), as well as $f ( X ^ { \prime } , X ^ { \prime } ) = 0$ . Therefore

$$
\sum_ {x ^ {\prime} \in X ^ {\prime}} f (x ^ {\prime}, V) = f (X ^ {\prime}, V) = f (X ^ {\prime}, \overline {{X}}) + f (X ^ {\prime}, x) + f (X ^ {\prime}, X ^ {\prime}) <   0,
$$

so some $x ^ { \prime } \in X ^ { \prime }$ must indeed satisfy

$$
f \left(x ^ {\prime}, V\right) <   0. \tag {2}
$$

As $x ^ { \prime } \in X$ , there is an $x { - } x ^ { \prime }$ walk $W = x _ { 0 } e _ { 0 } \ldots e _ { \ell - 1 } x _ { \ell }$ such that $f ( e _ { i } , x _ { i } , x _ { i + 1 } ) > 0$ for all $i < \ell$ . We now modify $f$ by sending some flow back along $W$ , letting $f ^ { \prime } \colon \overrightarrow { E }  \mathbb { Z }$ be given by

$$
f ^ {\prime} \colon \vec {e} \mapsto \left\{ \begin{array}{l l} f (\vec {e}) - k & \text {f o r} \vec {e} = (e _ {i}, x _ {i}, x _ {i + 1}), i = 0, \ldots , \ell - 1; \\ f (\vec {e}) + k & \text {f o r} \vec {e} = (e _ {i}, x _ {i + 1}, x _ {i}), i = 0, \ldots , \ell - 1; \\ f (\vec {e}) & \text {f o r} e \notin W. \end{array} \right.
$$

By definition of $W$ , we have $\left| f ^ { \prime } ( \overrightarrow { e } ) \right| < k$ for all $\vec { e } \in \vec { E }$ . Hence $f ^ { \prime }$ , like $f$ , lies in $F$ .

How does the modification of $f$ affect $K$ ? At all inner vertices $v$ ， of $W$ , as well as outside $W$ , the deviation from Kirchhoff’s law remains unchanged:

$$
f ^ {\prime} (v, V) = f (v, V) \quad \text {f o r a l l} v \in V \setminus \{x, x ^ {\prime} \}. \tag {3}
$$

For $x$ and $x ^ { \prime }$ , on the other hand, we have

$$
f ^ {\prime} (x, V) = f (x, V) - k \quad \text {a n d} \quad f ^ {\prime} \left(x ^ {\prime}, V\right) = f \left(x ^ {\prime}, V\right) + k. \tag {4}
$$

Since $g$ is a $\mathbb { Z } _ { k }$ -flow and hence

$$
\sigma_ {k} (f (x, V)) = g (x, V) = \overline {{0}} \in \mathbb {Z} _ {k}
$$

and

$$
\sigma_ {k} \left(f \left(x ^ {\prime}, V\right)\right) = g \left(x ^ {\prime}, V\right) = \overline {{0}} \in \mathbb {Z} _ {k},
$$

$f ( x , V )$ and $f ( x ^ { \prime } , V )$ are both multiples of $k$ . Thus $f ( x , V ) \geqslant k$ and $f ( x ^ { \prime } , V ) \leqslant - k$ , by (1) and (2). But then (4) implies that

$$
\left| f ^ {\prime} (x, V) \right| <   \left| f (x, V) \right| \quad \text {a n d} \quad \left| f ^ {\prime} \left(x ^ {\prime}, V\right) \right| <   \left| f \left(x ^ {\prime}, V\right) \right|.
$$

Together with (3), this gives $K ( f ^ { \prime } ) < K ( f )$ , a contradiction to the choice of $f$ .

Therefore $K ( f ) = 0$ as claimed, and $f$ is indeed a $k$ -flow.

Since the sum of two $\mathbb { Z } _ { k }$ -circulations is always another $\mathbb { Z } _ { k }$ -circulation, $\mathbb { Z } _ { k }$ -flows are often easier to construct (by summing over suitable partial flows) than $k$ -flows. In this way, Theorem 6.3.3 may be of considerable help in determining whether or not some given graph has a $k$ -flow. In the following sections we shall meet a number of examples for this.

Although Theorem 6.3.3 tells us whether a given multigraph admits a $k$ -flow (assuming we know the value of its flow-polynomial for $k - 1$ ), it does not say anything about the number of such flows. By a recent result of Kochol, this number is also a polynomial in $k$ , whose values can be bounded above and below by the corresponding values of the flow polynomial. See the notes for details.

# 6.4 $k$ -Flows for small $k$

Trivially, a graph has a 1-flow (the empty set) if and only if it has no edges. In this section we collect a few simple examples of sufficient conditions under which a graph has a 2-, 3- or 4-flow. More examples can be found in the exercises.

Proposition 6.4.1. A graph has a 2-flow if and only if all its degrees [ 6.6.1 ] are even.

Proof . By Theorem 6.3.3, a graph $G = ( V , E )$ has a 2-flow if and only if (6.3.3) it has a $\mathbb { Z } _ { 2 }$ -flow, i.e. if and only if the constant map $\vec { E }  \mathbb { Z } _ { 2 }$ with value $\overline { { 1 } }$ satisfies (F2). This is the case if and only if all degrees are even. 

even graph

For the remainder of this chapter, let us call a graph even if all its vertex degrees are even.

Proposition 6.4.2. A cubic graph has a 3-flow if and only if it is bipartite.

(1.6.1) (6.3.3)

Proof . Let $G = ( V , E )$ be a cubic graph. Let us assume first that $G$ has a 3-flow, and hence also a $\mathbb { Z } _ { 3 }$ -flow $f$ . We show that any cycle $C = x _ { 0 } \ldots x _ { \ell } x _ { 0 }$ in $G$ has even length (cf. Proposition 1.6.1). Consider two consecutive edges on $C$ , say $e _ { i - 1 } : = x _ { i - 1 } x _ { i }$ and $e _ { i } : = x _ { i } x _ { i + 1 }$ . If $f$ assigned the same value to these edges in the direction of the forward orientation of $C$ , i.e. if $f ( e _ { i - 1 } , x _ { i - 1 } , x _ { i } ) = f ( e _ { i } , x _ { i } , x _ { i + 1 } )$ , then $f$ could not satisfy (F2) at $x _ { i }$ for any non-zero value of the third edge at $x _ { i }$ . Therefore $f$ assigns the values $\overline { { 1 } }$ and $\overline { { 2 } }$ to the edges of $C$ alternately, and in particular $C$ has even length.

Conversely, let $G$ be bipartite, with vertex bipartition $\{ X , Y \}$ . Since $G$ is cubic, the map ${ \vec { E } }  \mathbb { Z } _ { 3 }$ defined by $f ( e , x , y ) ~ : = ~ 1$ and $f ( e , y , x ) : = 2$ for all edges $e ~ = ~ x y$ with $x \in X$ and $y \in Y$ is a $\mathbb { Z } _ { 3 }$ - flow on $G$ . By Theorem 6.3.3, then, $G$ has a 3-flow. 

What are the flow numbers of the complete graphs $K ^ { n }$ ? For odd $n > 1$ , we have $\varphi ( K ^ { n } ) = 2$ by Proposition 6.4.1. Moreover, $\varphi ( K ^ { 2 } ) = \infty$ , and $\varphi ( K ^ { 4 } ) = 4$ ; this is easy to see directly (and it follows from Propositions 6.4.2 and 6.4.5). Interestingly, $K ^ { 4 }$ is the only complete graph with flow number 4:

Proposition 6.4.3. For all even $n > 4 , \ \varphi ( K ^ { n } ) = 3$ .

(6.3.3)

Proof . Proposition 6.4.1 implies that $\varphi ( K ^ { n } ) \geqslant 3$ for even $n$ . We show, by induction on $n$ , that every $G = K ^ { n }$ with even $n > 4$ has a 3-flow.

For the induction start, let $n = 6$ . Then $G$ is the edge-disjoint union of three graphs $G _ { 1 }$ , $G _ { 2 }$ , $G _ { 3 }$ , with $G _ { 1 } , G _ { 2 } = K ^ { 3 }$ and $G _ { 3 } = K _ { 3 , 3 }$ . Clearly $G _ { 1 }$ and $G _ { 2 }$ each have a 2-flow, while $G _ { 3 }$ has a 3-flow by Proposition 6.4.2. The union of all these flows is a 3-flow on $G$ .

Now let $n > 6$ , and assume the assertion holds for $n - 2$ . Clearly, $G$ is the edge-disjoint union of a $K ^ { n - 2 }$ and a graph $G ^ { \prime } = ( V ^ { \prime } , E ^ { \prime } )$ with $G ^ { \prime } =$ $\overline { { K ^ { n - 2 } } } * K ^ { 2 }$ . The $K ^ { n - 2 }$ has a 3-flow by induction. By Theorem 6.3.3, it thus suffices to find a $\mathbb { Z } _ { 3 }$ -flow on $G ^ { \prime }$ . For every vertex $z$ of the ${ \overline { { K ^ { n - 2 } } } } \subseteq G ^ { \prime }$ , let $f _ { z }$ be a $\mathbb { Z } _ { 3 }$ -flow on the triangle $z x y z \subseteq G ^ { \prime }$ , where $e = x y$ is the edge of the $K ^ { 2 }$ in $G ^ { \prime }$ . Let $f \colon \vec { E ^ { \prime } }  \mathbb { Z } _ { 3 }$ be the sum of these flows. Clearly, $f$ is nowhere zero, except possibly in $( e , x , y )$ and $( e , y , x )$ . If $f ( e , x , y ) \neq 0$ , then $f$ is the desired $\mathbb { Z } _ { 3 }$ -flow on $G ^ { \prime }$ . If $f ( e , x , y ) = { \overline { { 0 } } }$ , then $f + f _ { z }$ (for any $z$ ) is a $\mathbb { Z } _ { 3 }$ -flow on $G ^ { \prime }$ . 

Proposition 6.4.4. Every 4-edge-connected graph has a 4-flow.

Proof . Let $G$ be a 4-edge-connected graph. By Corollary 2.4.2, $G$ has (2.4.2) two edge-disjoint spanning trees $T _ { i }$ , $i = 1 , 2$ . For each edge $e \notin T _ { i }$ , let $C _ { i , e }$ be the unique cycle in $T _ { i } + e$ , and let $f _ { i , e }$ be a $\mathbb { Z } _ { 4 }$ -flow of value $\bar { i }$ $f _ { 1 , e } , f _ { 2 , e }$ around Ci,e—more precisely: a Z4-circulation on G with values i and −i $C _ { i , e }$ $\mathbb { Z } _ { 4 }$ $G$ $\bar { i }$ $- \bar { i }$ on the edges of $C _ { i , e }$ and zero otherwise.

Let $\begin{array} { r } { f _ { 1 } : = \sum _ { e \not \in T _ { 1 } } f _ { 1 , e } } \end{array}$ . Since each $e \notin T _ { 1 }$ lies on only one cycle $\zeta _ { 1 , e ^ { \prime } }$ (namely, for $e = e ^ { \prime }$ 1 ), $f _ { 1 }$ takes only the values $\overline { { 1 } }$ and $- \overline { { 1 } }$ $= 3$ ) outside $T _ { 1 }$ . Let

$$
F := \left\{e \in E \left(T _ {1}\right) \mid f _ {1} (e) = \overline {{0}} \right\}
$$

and $f _ { 2 } : = \textstyle \sum _ { e \in F } f _ { 2 , e }$ . As above, $f _ { 2 } ( e ) = \overline { { { 2 } } } = - \overline { { { 2 } } }$ for all $e \in F$ . Now $f : = f _ { 1 } + f _ { 2 }$ is the sum of $\mathbb { Z } _ { 4 }$ -circulations, and hence itself a $\mathbb { Z } _ { 4 }$ -circulation. Moreover, $f$ is nowhere zero: on edges in $F$ it takes the value 2, on edges of $T _ { 1 } - F$ it agrees with $f _ { 1 }$ (and is hence non-zero by the choice of $F$ ), and on all edges outside $T _ { 1 }$ it takes one of the values $\overline { { 1 } }$ or 3. Hence, $f$ is a $\mathbb { Z } _ { 4 }$ -flow on $G$ , and the assertion follows by Theorem 6.3.3. 

The following proposition describes the graphs with a 4-flow in terms of those with a 2-flow:

# Proposition 6.4.5.

(i) A graph has a 4-flow if and only if it is the union of two even subgraphs.   
(ii) A cubic graph has a 4-flow if and only if it is 3-edge-colourable.

Proof . Let $\mathbb { Z } _ { 2 } ^ { 2 } = \mathbb { Z } _ { 2 } \times \mathbb { Z } _ { 2 }$ be the Klein four-group. (Thus, the elements of (6.3.2)(6.3.3) $\mathbb { Z } _ { 2 } ^ { 2 }$ are the pairs $( a , b )$ with $a , b \in \mathbb { Z } _ { 2 }$ , and $( a , b ) + ( a ^ { \prime } , b ^ { \prime } ) = ( a + a ^ { \prime } , b + b ^ { \prime } )$ .) By Corollary 6.3.2 and Theorem 6.3.3, a graph has a 4-flow if and only if it has a $\mathbb { Z } _ { 2 } ^ { 2 }$ -flow.

(i) now follows directly from Proposition 6.4.1.   
(ii) Let $G = ( V , E )$ be a cubic graph. We assume first that $G$ has a $\mathbb { Z } _ { 2 } ^ { 2 }$ -flow $f$ , and define an edge colouring $E \to \mathbb { Z } _ { 2 } ^ { 2 } \setminus \{ 0 \}$ . As $a = - a$ for all $a \in \mathbb { Z } _ { 2 } ^ { 2 }$ , we have $f ( \overrightarrow { e } ) = f ( \overleftarrow { e } )$ for every $\vec { e } \in E$ ; let us colour the edge $e$ with this colour $f ( \overrightarrow { e } )$ . Now if two edges with a common end $v$ had the same colour, then these two values of $f$ would sum to zero; by (F2), $f$ would then assign zero to the third edge at $\boldsymbol { v }$ . As this contradicts the definition of $f$ , our edge colouring is correct.

Conversely, since the three non-zero elements of $\mathbb { Z } _ { 2 } ^ { 2 }$ sum to zero, every 3-edge-colouring $c \colon E \to \mathbb { Z } _ { 2 } ^ { 2 } \setminus \{ 0 \}$ defines a $\mathbb { Z } _ { 2 } ^ { 2 }$ -flow on $G$ by letting $f ( { \overrightarrow { e } } ) = f ( \overleftarrow { e } ) = c ( e )$ for all $\vec { e } \in E$ . 

Corollary 6.4.6. Every cubic 3-edge-colourable graph is bridgeless.



# 6.5 Flow-colouring duality

In this section we shall see a surprising connection between flows and colouring: every $k$ -flow on a plane multigraph gives rise to a $k$ -vertexcolouring of its dual, and vice versa. In this way, the investigation of $k$ -flows appears as a natural generalization of the familiar map colouring problems in the plane.

G = (V, E) G∗

Let $G = ( V , E )$ and $G ^ { * } = ( V ^ { * } , E ^ { * } )$ be dual plane multigraphs. For simplicity, let us assume that $G$ and $G ^ { * }$ have neither bridges nor loops and are non-trivial. For edge sets $F ^ { \prime } \subseteq E ^ { \prime }$ , let us write

$F ^ { * }$

$$
F ^ {*} := \left\{e ^ {*} \in E ^ {*} \mid e \in F \right\}.
$$

Conversely, if a subset of $E ^ { * }$ is given, we shall usually write it immediately in the form $F ^ { * }$ , and thus let $F \subseteq E$ be defined implicitly via the bijection $e \mapsto e ^ { * }$ .

Suppose we are given a circulation $g$ on $G ^ { * }$ : how can we employ the duality between $G$ and $G ^ { * }$ to derive from $g$ some information about $G$ ? The most general property of all circulations is Proposition 6.1.1, which says that $g ( X , { \overline { { X } } } ) = 0$ for all $X \subseteq V ^ { * }$ . By Proposition 4.6.1, the minimal cuts $E ^ { * } ( X , { \overline { { X } } } )$ in $G ^ { * }$ correspond precisely to the cycles in $G$ . Thus if we take the composition $f$ of the maps $e \mapsto e ^ { * }$ and $g$ , and sum its values over the edges of a cycle in $G$ , then this sum should again be zero.

Of course, there is still a technical hitch: since $g$ takes its arguments not in $E ^ { * }$ but in $\vec { E ^ { * } }$ , we cannot simply define $f$ as above: we first have to refine the bijection $e \mapsto e ^ { * }$ into one from $\overrightarrow { E }$ t o $\vec { E ^ { * } }$ , i.e. assign to every $\vec { e } \in \vec { E }$ canonically one of the two directions of $e ^ { * }$ . This will be the purpose of our first lemma. After that, we shall show that $f$ does indeed sum to zero along any cycle in $G$ .

If $C = v _ { 0 } \ldots v _ { \ell - 1 } v _ { 0 }$ is a cycle with edges $e _ { i } = v _ { i } v _ { i + 1 }$ (and $\boldsymbol { v } _ { \ell } : = \boldsymbol { v } _ { 0 }$ ), we shall call

$$
\vec {C} := \left\{\left(e _ {i}, v _ {i}, v _ {i + 1}\right) \mid i <   \ell \right\}
$$

$\overrightarrow { C }$ cycle with orientation

a cycle with orientation. Note that this definition of $\vec { C }$ depends on the vertex enumeration chosen to denote $C$ : every cycle has two orientations. Conversely, of course, $C$ can be reconstructed from the set $\vec { C }$ . In practice, we shall therefore speak about $C$ freely even when, formally, only $\vec { C }$ has been defined.

Lemma 6.5.1. There exists a bijection $\mathbf { \Phi } ^ { * } \colon \overrightarrow { e } \mapsto \overrightarrow { e } ^ { * }$ from $\overrightarrow { E }$ to $\vec { E ^ { * } }$ with the following properties:

(i) The underlying edge of $\overrightarrow { e } ^ { * }$ is always $e ^ { * }$ , i.e. $\overrightarrow { e } ^ { * }$ is one of the two directions $\vec { e ^ { * } } , \bar { e ^ { * } }$ of $e ^ { * }$ ;   
(ii) If $C \subseteq G$ is a cycle, $F : = E ( C )$ , and if $X \subseteq V ^ { * }$ is such that ${ \cal F } ^ { * } = E ^ { * } ( X , \overline { { { X } } } )$ , then there exists an orientation $\vec { C }$ of $C$ with $\{ \vec { e } ^ { * } | \vec { e } \in \vec { C } \} = \vec { E ^ { * } } ( X , \overline { { X } } )$ .

The proof of Lemma 6.5.1 is not entirely trivial: it is based on the so-called orientability of the plane, and we cannot give it here. Still, the assertion of the lemma is intuitively plausible. Indeed if we define for $\mathit { \Pi } \mathit { e } \equiv \mathit { \Pi } \mathit { v } w$ and $e ^ { * } = x y$ the assignment $( e , v , w ) \mapsto ( e , v , w ) ^ { * } \ \in$ $\left\{ \begin{array} { l l } { ( e ^ { * } , x , y ) , ( e ^ { * } , y , x ) } \end{array} \right\}$ simply by turning $e$ and its ends clockwise onto $e ^ { * }$ (Fig. 6.5.1), then the resulting map $\overrightarrow { e } \mapsto \overrightarrow { e } ^ { * }$ satisfies the two assertions of the lemma.

![](images/906130ff568216012c3b24ca39998fd0d77ac056e941e76ee5d14e62fcb8feec.jpg)  
Fig. 6.5.1. Oriented cycle-cut duality

Given an abelian group $H$ , let $f \colon { \overrightarrow { E } } \to H$ and $g \colon \overrightarrow { E ^ { * } } \longrightarrow H$ be two maps $f , g$ such that

$$
f (\vec {e}) = g (\vec {e} ^ {*})
$$

for all $\vec { e } \in \vec { E }$ . For ${ \vec { F } } \subseteq { \vec { E } }$ , we set

$$
f (\vec {F}) := \sum_ {\vec {e} \in \vec {F}} f (\vec {e}). \quad f (\vec {C}) \text {e t c .}
$$

# Lemma 6.5.2.

(i) The map g satisfies (F1) if and only if $f$ does.   
(ii) The map $g$ is a circulation on $G ^ { * }$ if and only if $f$ satisfies (F1) and $f ( \overrightarrow { C } ) = 0$ for every cycle $\vec { C }$ with orientation.

Proof . (See also Exercise 17.) Assertion (i) follows from Lemma 6.5.1 (i) (4.6.1)(6.1.1) and the fact that $\overrightarrow { e } \mapsto \overrightarrow { e } ^ { * }$ is bijective.

For the forward implication of (ii), let us assume that $g$ is a circulation on $G ^ { * }$ , and consider a cycle $C \subseteq G$ with some given orientation. Let $F : = E ( C )$ . By Proposition 4.6.1, $F ^ { * }$ is a minimal cut in $G ^ { * }$ , i.e. ${ \cal F } ^ { * } = E ^ { * } ( X , \overline { { { X } } } )$ for some suitable $X \subseteq V ^ { * }$ . By definition of $f$ and $g$ , Lemma 6.5.1 (ii) and Proposition 6.1.1 give

$$
f (\vec {C}) = \sum_ {\vec {e} \in \vec {C}} f (\vec {e}) = \sum_ {\vec {d} \in E ^ {*} (X, \overline {{X}})} g (\vec {d}) = g (X, \overline {{X}}) = 0
$$

for one of the two orientations $\vec { C }$ of $C$ . Then, by $f ( { \overleftarrow { C } } ) = - f ( { \overrightarrow { C } } )$ , also

the corresponding value for our given orientation of $C$ must be zero.

For the backward implication it suffices by (i) to show that $g$ satisfies (F2), i.e. that $g ( x , V ^ { * } ) = 0$ for every $x \in V ^ { * }$ . We shall prove that $g ( x , V ( B ) ) = 0$ for every block $B$ of $G ^ { * }$ containing $x$ ; since every edge of $G ^ { * }$ at $x$ lies in exactly one such block, this will imply $g ( x , V ^ { * } ) = 0$ .

So let $x \in V ^ { * }$ be given, and let $B$ be any block of $G ^ { * }$ containing $x$ . Since $G ^ { * }$ is a non-trivial plane dual, and hence connected, we have $B - x \neq \varnothing$ . Let $F ^ { * }$ be the set of all edges of $B$ at $x$ (Fig. 6.5.2),

![](images/4709328d97f025c0ce4027d6b0ae5199b3e5ba35b3a7b02f79b2d536e21baff7.jpg)  
Fig. 6.5.2. The cut $F ^ { * }$ in $G ^ { * }$

$X$ and let $X$ be the vertex set of the component of $G ^ { * } - F ^ { * }$ containing $x$ . Then $\emptyset \neq V ( B - x ) \subseteq { \overline { { X } } }$ , by the maximality of $B$ as a cutvertex-free subgraph. Hence

$$
F ^ {*} = E ^ {*} (X, \bar {X}) \tag {1}
$$

by definition of $X$ , i.e. $F ^ { * }$ is a cut in $G ^ { * }$ . As a dual, $G ^ { * }$ is connected, so $G ^ { * } [ { \overline { { X } } } ]$ too is connected. Indeed, every vertex of $\overline { { X } }$ is linked to $x$ by a path $P \subseteq G ^ { * }$ whose last edge lies in $F ^ { * }$ . Then $P - x$ is a path in $G ^ { * } [ { \overline { { X } } } ]$ meeting $B$ . Since $x$ does not separate $B$ , this shows that $G ^ { * } [ { \overline { { X } } } ]$ is connected.

Thus, $X$ and $\overline { { X } }$ are both connected in $G ^ { * }$ , so $F ^ { * }$ is even a minimal cut in $G ^ { * }$ . Let $C \subseteq G$ be the cycle with $E ( C ) = F$ that exists by Proposition 4.6.1. By Lemma 6.5.1 (ii), $C$ has an orientation $\vec { C }$ such that $\{ \vec { e } ^ { * } | \vec { e } \in \vec { C } \} = \vec { E ^ { * } } ( X , \overline { { X } } )$ . By (1), however, $\vec { E ^ { * } } ( X , \overline { { X } } ) = \vec { E ^ { * } } ( x , V ( B ) )$ , so

$$
g (x, V (B)) = g (X, \bar {X}) = f (\vec {C}) = 0
$$

by definition of $f$ and $g$

With the help of Lemma 6.5.2, we can now prove our colouring-flow duality theorem for plane multigraphs. If $P = v _ { 0 } \ldots v _ { \ell }$ is a path with edges $e _ { i } = v _ { i } v _ { i + 1 }$ ( $i < \ell$ ), we set (depending on our vertex enumeration of $P$ )

$\overrightarrow { P }$ $\vec { P } : = \{ ( e _ { i } , v _ { i } , v _ { i + 1 } ) \mid i < \ell \}$

$v _ { 0 } \longrightarrow v _ { \ell }$ and call $\vec { P }$ a path. Again, may be given implicitly by $\vec { P }$ . $v _ { 0 } \longrightarrow v _ { \ell }$ $P$

# Theorem 6.5.3. (Tutte 1954)

For every dual pair $G , G ^ { * }$ of plane multigraphs,

$$
\chi (G) = \varphi (G ^ {*}).
$$

Proof . Let $G \ : = : \ : ( V , E )$ and $G ^ { * } = : ( V ^ { * } , E ^ { * } )$ . For $| G | \in \{ 1 , 2 \}$ the assertion is easily checked; we shall assume that $| G | \geqslant 3$ , and apply induction on the number of bridges in $G$ . If $e \in G$ is a bridge then $e ^ { * }$ is a loop, and $G ^ { * } - e ^ { * }$ is a plane dual of $G / e$ (why?). Hence, by the induction hypothesis,

(1.5.6)

$V , E$

$V ^ { * } , E ^ { * }$

$$
\chi (G) = \chi (G / e) = \varphi (G ^ {*} - e ^ {*}) = \varphi (G ^ {*});
$$

for the first and the last equality we use that, by $| G | \geqslant 3$ , $e$ is not the only edge of $G$ .

So all that remains to be checked is the induction start: let us assume that $G$ has no bridge. If $G$ has a loop, then $G ^ { * }$ has a bridge, and $\chi ( G ) = \infty = \varphi ( G ^ { * } )$ by convention. So we may also assume that $G$ has no loop. Then $\chi ( G )$ is finite; we shall prove for given $k \geqslant 2$ that $G$ is $k$ -colourable if and only if $G ^ { * }$ has a $k$ -flow. As $G$ —and hence $G ^ { * }$ — has neither loops nor bridges, we may apply Lemmas 6.5.1 and 6.5.2 to $G$ and $G ^ { * }$ . Let $\overrightarrow { e } \mapsto \overrightarrow { e } ^ { * }$ be the bijection between $\overrightarrow { E }$ and $\vec { E ^ { * } }$ from Lemma 6.5.1.

We first assume that $G ^ { * }$ has a $k$ -flow. Then $G ^ { * }$ also has a $\mathbb { Z } _ { k }$ -flow $g$ . As before, let $f \colon { \overrightarrow { E } }  \mathbb { Z } _ { k }$ be defined by $f ( \vec { e } ) : = g ( \vec { e } ^ { * } )$ . We shall use $f$ to define a vertex colouring $c \colon V \to \mathbb { Z } _ { k }$ of $G$ .

Let $T$ be a normal spanning tree of $G$ , with root $r$ , say. Put $c ( r ) : = \overline { { 0 } }$ . For every other vertex $v \in V$ let $c ( v ) : = f ( \vec { P } )$ , where $\vec { P }$ is the $r \longrightarrow v$ path in $T$ . To check that this is a proper colouring, consider an edge $e = v w \in E$ . As $T$ is normal, we may assume that $v < w$ in the tree-order of $T$ . If $e$ is an edge of $T$ then $c ( w ) - c ( v ) = f ( e , v , w )$ by definition of $c$ , so $c ( v ) \neq c ( w )$ since $g$ (and hence $f$ ) is nowhere zero. If $e \not \in T$ , let $\vec { P }$ denote the $v \longrightarrow w$ path in $T$ . Then

$$
c (w) - c (v) = f (\vec {P}) = - f (e, w, v) \neq \overline {{0}}
$$

by Lemma 6.5.2 (ii).

Conversely, we now assume that $G$ has a $k$ -colouring $c$ . Let us define $f \colon { \overrightarrow { E } } \to \mathbb { Z }$ by

$$
f (e, v, w) := c (w) - c (v),
$$

and $g \colon \vec { E ^ { * } } \to \mathbb { Z }$ by $g ( \vec { e } ^ { * } ) : = f ( \vec { e } )$ . Clearly, $f$ satisfies (F1) and takes values in $\{ \pm 1 , \ldots , \pm ( k - 1 ) \}$ , so by Lemma 6.5.2 (i) the same holds for $g$ . By definition of $f$ , we further have $f ( \overrightarrow { C } ) = 0$ for every cycle $\vec { C }$ with orientation. By Lemma 6.5.2 (ii), therefore, $g$ is a $k$ -flow. 

$k$

$_ { g }$

$f$

c

$f$

$_ { g }$

# 6.6 Tutte’s flow conjectures

How can we determine the flow number of a graph? Indeed, does every (bridgeless) graph have a flow number, a $k$ -flow for some $k$ ? Can flow numbers, like chromatic numbers, become arbitrarily large? Can we characterize the graphs admitting a $k$ -flow, for given $k$ ?

Of these four questions, we shall answer the second and third in this section: we prove that every bridgeless graph has a 6-flow. In particular, a graph has a flow number if and only if it has no bridge. The question asking for a characterization of the graphs with a $k$ -flow remains interesting for $k = 3 , 4 , 5$ . Partial answers are suggested by the following three conjectures of Tutte, who initiated algebraic flow theory.

The oldest and best known of the Tutte conjectures is his 5-flow conjecture:

# Five-Flow Conjecture. (Tutte 1954)

Every bridgeless multigraph has a 5-flow.

Which graphs have a 4-flow? By Proposition 6.4.4, the 4-edgeconnected graphs are among them. The Petersen graph (Fig. 6.6.1), on the other hand, is an example of a bridgeless graph without a 4-flow: since it is cubic but not 3-edge-colourable, it cannot have a 4-flow by Proposition 6.4.5 (ii).

![](images/e2caa108410231ffac14aec7dd88b9473726fb66d08778aeae53de42b39a4e15.jpg)  
Fig. 6.6.1. The Petersen graph

Tutte’s 4-flow conjecture states that the Petersen graph must be present in every graph without a 4-flow:

# Four-Flow Conjecture. (Tutte 1966)

Every bridgeless multigraph not containing the Petersen graph as a minor has a 4-flow.

By Proposition 1.7.2, we may replace the word ‘minor’ in the 4-flow conjecture by ‘topological minor’.

Even if true, the 4-flow conjecture will not be best possible: a $K ^ { 1 1 }$ , for example, contains the Petersen graph as a minor but has a 4-flow, even a 2-flow. The conjecture appears more natural for sparser graphs; a proof for cubic graphs was announced in 1998 by Robertson, Sanders, Seymour and Thomas.

A cubic bridgeless graph or multigraph without a 4-flow (equivalently, without a 3-edge-colouring) is called a snark. The 4-flow conjecture for cubic graphs says that every snark contains the Petersen graph as a minor; in this sense, the Petersen graph has thus been shown to be the smallest snark. Snarks form the hard core both of the four colour theorem and of the 5-flow conjecture: the four colour theorem is equivalent to the assertion that no snark is planar (exercise), and it is not difficult to reduce the 5-flow conjecture to the case of snarks.5 However, although the snarks form a very special class of graphs, none of the problems mentioned seems to become much easier by this reduction.6

# Three-Flow Conjecture. (Tutte 1972)

Every multigraph without a cut consisting of exactly one or exactly three edges has a 3-flow.

Again, the 3-flow conjecture will not be best possible: it is easy to construct graphs with three-edge cuts that have a 3-flow (exercise).

By our duality theorem (6.5.3), all three flow conjectures are true for planar graphs and thus motivated: the 3-flow conjecture translates to Gr¨otzsch’s theorem (5.1.3), the 4-flow conjecture to the four colour theorem (since the Petersen graph is not planar, it is not a minor of a planar graph), the 5-flow conjecture to the five colour theorem.

We finish this section with the main result of the chapter:

# Theorem 6.6.1. (Seymour 1981)

Every bridgeless graph has a 6-flow.

Proof . Let $G = ( V , E )$ be a bridgeless graph. Since 6-flows on the components of $G$ will add up to a 6-flow on $G$ , we may assume that $G$ is connected; as $G$ is bridgeless, it is then 2-edge-connected. Note that any two vertices in a 2-edge-connected graph lie in some common even connected subgraph—for example, in the union of two edge-disjoint paths linking these vertices by Menger’s theorem (3.3.6 (ii)). We shall use this fact repeatedly.

$H _ { 0 } , \ldots , H _ { n }$

$F _ { 1 } , \ldots , F _ { n }$

$V _ { i } , E _ { i } ^ { \prime }$

$H ^ { i }$

nstruct a sequence $H _ { 0 } , \ldots , H _ { n }$ of disjoint connected and of $G$ , together with a sequence $F _ { 1 } , \ldots , F _ { n }$ of non-empty tween them. The sets $F _ { i }$ will each contain only one or een $H _ { i }$ and $H _ { 0 } \cup . . . \cup H _ { i - 1 }$ . We write $H _ { i } = : ( V _ { i } , E _ { i } )$ ,

$$
H ^ {i} := \left(H _ {0} \cup \dots \cup H _ {i}\right) + \left(F _ {1} \cup \dots \cup F _ {i}\right)
$$

$V ^ { i } , E ^ { i }$

and $H ^ { i } = : ( V ^ { i } , E ^ { i } )$ . Note that each $H ^ { i } = ( H ^ { i - 1 } \cup H _ { i } ) + F _ { i }$ is connected (induction on $_ i$ ). Our assumption that $H _ { i }$ is even implies by Proposition 6.4.1 (or directly by Proposition 1.2.1) that $H _ { i }$ has no bridge.

As $H _ { 0 }$ we choose any $K ^ { 1 }$ in $G$ . Now assume that $H _ { 0 } , \ldots , H _ { i - 1 }$ and $F _ { 1 } , \ldots , F _ { i - 1 }$ have been defined for some $i > 0$ . If $V ^ { i - 1 } = V$ , we terminate the construction and set $i - 1 = : n$ . Otherwise, we let $X _ { i } \subseteq { \overline { { V ^ { i - 1 } } } }$ be minimal such that $X _ { i } \neq \varnothing$ and

$$
\left| E \left(X _ {i}, \overline {{V ^ {i - 1}}} \setminus X _ {i}\right) \right| \leqslant 1 \tag {1}
$$

$_ n$

$X _ { i }$

(Fig. 6.6.2); such an $X _ { i }$ exists, because $\overline { { V ^ { i - 1 } } }$ is a candidate. Since $G$ is 2-edge-connected, (1) implies that $E ( X _ { i } , V ^ { i - 1 } ) \not = \emptyset$ . By the minimality of $X _ { i }$ , the graph $G \left[ X _ { i } \right]$ is connected and bridgeless, i.e. 2-edgeconnected or a $K ^ { 1 }$ . As the elements of $F _ { i }$ we pick one or two edges from $E ( X _ { i } , V ^ { i - 1 } )$ , if possible two. As $H _ { i }$ we choose any connected even subgraph of $G \left[ X _ { i } \right]$ containing the ends in $X _ { i }$ of the edges in $F _ { i }$ .

![](images/2b8fa9f878ec601cb199a061067750b296003e1f36a906f20fe1fa55f0fce7b0.jpg)  
Fig. 6.6.2. Constructing the $H _ { i }$ and $F _ { i }$

$F _ { i } ^ { \prime }$

$H$ $E ^ { \prime }$

$f _ { n } , \ldots , f _ { 0 }$

$\vec { C _ { e } }$

$f _ { e }$

$f _ { n }$

When our construction is complete, we set $H ^ { n } \mathrel { = } : H$ and $E ^ { \prime } : =$ $E \setminus E ( H )$ . By definition of $n$ , $H$ is a spanning connected subgraph of $G$ .

We now define, by ‘reverse’ induction, a sequence $f _ { n } , \ldots , f _ { 0 }$ of $\mathbb { Z } _ { 3 }$ - circulations on $G$ . For every edge $e \in E ^ { \prime }$ , let $\vec { C _ { e } }$ be a cycle (with orientation) in $H + e$ containing $e$ , and $f _ { e }$ a positive flow around $\vec { C _ { e } }$ ; formally, we let $f _ { e }$ be a $\mathbb { Z } _ { 3 }$ -circulation on $G$ such that $f _ { e } ^ { - 1 } ( \overline { { 0 } } ) = \overrightarrow { E } \setminus ( \overrightarrow { C _ { e } } \cup \overleftarrow { C _ { e } } )$ . Let $f _ { n }$ be the sum of all these $f _ { e }$ . Since each $e ^ { \prime } \in E ^ { \prime }$ lies on just one of the cycles $C _ { e }$ (namely, on $C _ { e ^ { \prime } }$ ), we have $f _ { n } ( \overrightarrow { e } ) \ne \overline { { 0 } }$ for all $\vec { e } \in \vec { E ^ { \prime } }$ .

Assume now that $\mathbb { Z } _ { 3 }$ -circulations $f _ { n } , \ldots , f _ { i }$ on $G$ have been defined for some $i \leqslant n$ , and that

$$
f _ {i} (\vec {e}) \neq \overline {{0}} \text {f o r a l l} \vec {e} \in \vec {E ^ {\prime}} \cup \bigcup_ {j > i} \vec {F _ {j}}, \tag {2}
$$

where $\overrightarrow { F _ { j } } : = \{ \overrightarrow { e } \in \overrightarrow { E } \mid e \in F _ { j } \}$ . Our aim is to define $f _ { i - 1 }$ in such a way that (2) also holds for $i - 1$ .

We first consider the case that $| F _ { i } | = 1$ , say $F _ { i } = \{ e \}$ . We then let $f _ { i - 1 } : = f _ { i }$ , and thus have to show that $f _ { i }$ is non-zero on (the two directions of) $e$ . Our assumption of $| F _ { i } | = 1$ implies by the choice of $F _ { i }$ that $G$ contains no $X _ { i } – V ^ { i - 1 }$ edge other than $e$ . Since $G$ is 2-edgeconnected, it therefore has at least—and thus, by (1), exactly—one edge $e ^ { \prime }$ between $X _ { i }$ and ${ \overline { { V ^ { i - 1 } } } } \setminus X _ { i }$ . We show that $f _ { i }$ is non-zero on $e ^ { \prime }$ ; as $\{ e , e ^ { \prime } \}$ is a cut in $G$ , this implies by Proposition 6.1.1 that $f _ { i }$ is also non-zero on $e$ .

To show that $f _ { i }$ is non-zero on $e ^ { \prime }$ , we use (2): we show that $e ^ { \prime } \in$ $E ^ { \prime } \cup \bigcup _ { j > i } F _ { j }$ , i.e. that $e ^ { \prime }$ lies in no $H _ { k }$ and in no $F _ { j }$ with $j \leqslant i$ . Since $e ^ { \prime }$ has both ends in $\overline { { V ^ { i - 1 } } }$ , it clearly lies in no $F _ { j }$ with $j \leqslant i$ and in no $H _ { k }$ with $k < i$ . But every $H _ { k }$ with $k \geqslant i$ is a subgraph of $G [ \overline { { V ^ { i - 1 } } } ]$ . Since $e ^ { \prime }$ is a bridge of $G [ \overline { { V ^ { i - 1 } } } ]$ but $H _ { k }$ has no bridge, this means that $e ^ { \prime } \notin H _ { k }$ . Hence, $f _ { i - 1 }$ does indeed satisfy (2) for $i - 1$ in the case considered.

It remains to consider the case that $| F _ { i } | = 2$ , say $F _ { i } = \{ e _ { 1 } , e _ { 2 } \}$ . Since $H _ { i }$ and $H ^ { i - 1 }$ are both connected, we can find a cycle $C$ in $H ^ { i } =$ $( H _ { i } \cup H ^ { i - 1 } ) + F _ { i }$ that contains $e _ { 1 }$ and $e _ { 2 }$ . If $f _ { i }$ is non-zero on both these edges, we again let $f _ { i - 1 } : = f _ { i }$ . Otherwise, there are directions $\overrightarrow { e _ { 1 } }$ and $\overrightarrow { e _ { 2 } }$ of $e _ { 1 }$ and $e _ { 2 }$ such that, without loss of generality, $f _ { \underline { { i } } } ( { \vec { e _ { 1 } } } ) = \overline { { 0 } }$ and $f _ { i } ( \vec { e _ { 2 } } ) \in \{ \overline { { 0 } } , \overline { { 1 } } \}$ . Let $\vec { C }$ be the orientation of $C$ with ${ \vec { e _ { 2 } } } \in { \vec { C } }$ , and let $g$ be a flow of value $\overline { { 1 } }$ around $\vec { C }$ (formally: let $g$ be a $\mathbb { Z } _ { 3 }$ -circulation on $G$ such that $g ( \vec { e _ { 2 } } ) = \overline { { 1 } }$ and $g ^ { - 1 } ( \overline { { 0 } } ) = \vec { E } \setminus ( \vec { C } \cup \overleftarrow { C } ) )$ . We then let $f _ { i - 1 } : = f _ { i } + g$ . By choice of the directions $\overrightarrow { e _ { 1 } }$ and $\overrightarrow { e _ { 2 } }$ , $f _ { i - 1 }$ is non-zero on both edges. Since $f _ { i - 1 }$ agrees with $f _ { i }$ on all of $\textstyle { \vec { E ^ { \prime } } } \cup \bigcup _ { j > i } { \vec { F _ { j } } }$ and (2) holds for $i$ , we again have (2) also for $i - 1$ .

Eventually, $f _ { 0 }$ will be a $\mathbb { Z } _ { 3 }$ -circulation on $G$ that is nowhere zero except possibly on edges of $H _ { 0 } \cup \ldots \cup H _ { n }$ . Composing $f _ { 0 }$ with the map ${ \overline { { h } } } \mapsto { \overline { { 2 h } } }$ from $\mathbb { Z } _ { 3 }$ to $\mathbb { Z } _ { 6 }$ $h \in \{ 1 , 2 \}$ ), we obtain a $\mathbb { Z } _ { 6 }$ -circulation $f$ on $G$ with values in $\{ { \overline { { 0 } } } , { \overline { { 2 } } } , { \overline { { 4 } } } \}$ for all edges lying in some $H _ { i }$ , and with values in $\{ { \overline { { 2 } } } , { \overline { { 4 } } } \}$ for all other edges. Adding to $f$ a 2-flow on each $H _ { i }$ (formally: a $\mathbb { Z } _ { 6 }$ -circulation on $G$ with values in $\{ \overline { { 1 } } , - \overline { { 1 } } \}$ on the edges of $H _ { i }$ and $\overline { { 0 } }$ otherwise; this exists by Proposition 6.4.1), we obtain a $\mathbb { Z } _ { 6 }$ -circulation on $G$ that is nowhere zero. Hence, $G$ has a 6-flow by Theorem 6.3.3.



# Exercises

1. $-$ Prove Proposition 6.2.1 by induction on $| S |$

2. $( \mathrm { i } ) ^ { - }$ Given $n \in \mathbb N$ , find a capacity function for the network below such that the algorithm from the proof of the max-flow min-cut theorem will need more than $n$ augmenting paths $W$ if these are badly chosen.

![](images/0a00b9e43cc1b658883c36fce26af7b9b278c9d716ec32249686a3e4e06adbe8.jpg)

(ii) $^ +$ Show that, if all augmenting paths are chosen as short as possible, their number is bounded by a function of the size of the network.

3.+ Derive Menger’s Theorem 3.3.5 from the max-flow min-cut theorem. (Hint. The edge version is easy. For the vertex version, apply the edge version to a suitable auxiliary graph.)

4. $-$ Let $f$ be an $H$ -circulation on $G$ and $g \colon H \to H ^ { \prime }$ a group homomorphism. Show that $g \circ f$ is an $H ^ { \prime }$ -circulation on $G$ . Is $g \circ f$ an $H ^ { \prime }$ -flow if $f$ is an $H$ -flow?

5.− Given $k \geqslant 1$ , show that a graph has a $k$ -flow if and only if each of its blocks has a $k$ -flow.

6.− Show that $\varphi ( G / e ) \leqslant \varphi ( G )$ whenever $G$ is a multigraph and $e$ an edge of $G$ . Does this imply that, for every $k$ , the class of all multigraphs admitting a $k$ -flow is closed under taking minors?

7. $-$ Work out the flow number of $K ^ { 4 }$ directly, without using any results from the text.

8. Let $H$ be a finite abelian group, $G$ a graph, and $T$ a spanning tree of $G$ . Show that every mapping from the directions of $E ( G ) \setminus E ( T )$ to $H$ that satisfies (F1) extends uniquely to an $H$ -circulation on $G$ .

Do not use the 6-flow Theorem 6.6.1 for the following three exercises.

9. Show that $\varphi ( G ) < \infty$ for every bridgeless multigraph $G$ .

10. Assume that a graph $G$ has $m$ spanning trees such that no edge of $G$ lies in all of these trees. Show that $\varphi ( G ) \leqslant 2 ^ { m }$ .

11. Let $G$ be a bridgeless connected graph with $n$ vertices and $m$ edges. By considering a normal spanning tree of $G$ , show that $\varphi ( G ) \leqslant m - n + 2$ .

12. Show that every graph with a Hamilton cycle has a 4-flow. (A Hamilton cycle of $G$ is a cycle in $G$ that contains all the vertices of $G$ .)

13. A family of (not necessarily distinct) cycles in a graph $G$ is called a cycle double cover of $G$ if every edge of $G$ lies on exactly two of these cycles. The cycle double cover conjecture asserts that every bridgeless multigraph has a cycle double cover. Prove the conjecture for graphs with a 4-flow.

14.− Determine the flow number of $C ^ { 5 } * K ^ { 1 }$ , the wheel with 5 spokes.

15. Find bridgeless graphs $G$ and $H = G - e$ such that $2 < \varphi ( G ) < \varphi ( H )$

16. Prove Proposition 6.4.1 without using Theorem 6.3.3.

17. The proof of the backward implication of Lemma 6.5.2 (ii) is a bit pedestrian. Use Lemmas 1.9.4 and 3.1.1, Proposition 4.6.1, and Exercise 31 of Chapter 4 for a shorter higher-level proof.

18.+ Prove Heawood’s theorem that a plane triangulation is 3-colourable if and only if all its vertices have even degree.

19. Show that the 3-flow conjecture for planar multigraphs is equivalent to Gr¨otzsch’s Theorem 5.1.3.

20. (i) $-$ Show that the four colour theorem is equivalent to the non-existence of a planar snark, i.e. to the statement that every cubic bridgeless planar multigraph has a 4-flow.

(ii) Can ‘bridgeless’ in (i) be replaced by ‘3-connected’ ?

21. $^ +$ Show that a graph $G = ( V , E )$ has a $k$ -flow if and only if it admits an orientation $D$ that directs, for every $X \subseteq V$ , at least $1 / k$ of the edges in $E ( X , { \overline { { X } } } )$ from $X$ towards $\overline { { X } }$ .

22. $-$ Generalize the 6-flow Theorem 6.6.1 to multigraphs.

# Notes

Network flow theory is an application of graph theory that has had a major and lasting impact on its development over decades. As is illustrated already by the fact that Menger’s theorem can be deduced easily from the max-flow min-cut theorem (Exercise 3), the interaction between graphs and networks may go either way: while ‘pure’ results in areas such as connectivity, matching and random graphs have found applications in network flows, the intuitive power of the latter has boosted the development of proof techniques that have in turn brought about theoretic advances.

The classical reference for network flows is L.R. Ford & D.R. Fulkerson, Flows in Networks, Princeton University Press 1962. More recent and comprehensive accounts are given by R.K. Ahuja, T.L. Magnanti & J.B. Orlin, Network flows, Prentice-Hall 1993, by A. Frank in his chapter in the Handbook of Combinatorics (R.L. Graham, M. Gr¨otschel & L. Lov´asz, eds.), North-Holland 1995, and by A. Schrijver, Combinatorial optimization, Springer 2003. An introduction to graph algorithms in general is given in A. Gibbons, Algorithmic Graph Theory, Cambridge University Press 1985.

If one recasts the maximum flow problem in linear programming terms, one can derive the max-flow min-cut theorem from the linear programming duality theorem; see A. Schrijver, Theory of integer and linear programming, Wiley 1986.

The more algebraic theory of group-valued flows and $k$ -flows has been developed largely by Tutte; he gives a thorough account in his monograph

W.T. Tutte, Graph Theory, Addison-Wesley 1984. The fact that the number of $k$ -flows of a multigraph is a polynomial in $k$ , whose values can be bounded in terms of the corresponding values of the flow polynomial, was proved by M. Kochol, Polynomials associated with nowhere-zero7 flows, J. Combin. Theory B 84 (2002), 260–269.

Tutte’s flow conjectures are covered also in F. Jaeger’s survey, Nowherezero flow problems, in (L.W. Beineke & R.J. Wilson, eds.) Selected Topics in Graph Theory 3, Academic Press 1988. For the flow conjectures, see also T.R. Jensen & B. Toft, Graph Coloring Problems, Wiley 1995. Seymour’s 6- flow theorem is proved in P.D. Seymour, Nowhere-zero 6-flows, J. Combin. Theory B 30 (1981), 130–135. This paper also indicates how Tutte’s 5-flow conjecture reduces to snarks. In 1998, Robertson, Sanders, Seymour and Thomas announced a proof of the 4-flow conjecture for cubic graphs.

Finally, Tutte discovered a 2-variable polynomial associated with a graph, which generalizes both its chromatic polynomial and its flow polynomial. What little is known about this Tutte polynomial can hardly be more than the tip of the iceberg: it has far-reaching, and largely unexplored, connections to areas as diverse as knot theory and statistical physics. See D.J.A. Welsh, Complexity: knots, colourings and counting (LMS Lecture Notes 186), Cambridge University Press 1993.

# Extremal Graph Theory

In this chapter we study how global parameters of a graph, such as its edge density or chromatic number, can influence its local substructures. How many edges, for instance, do we have to give a graph on $n$ vertices to be sure that, no matter how these edges are arranged, the graph will contain a $K ^ { r }$ subgraph for some given $r$ ? Or at least a $K ^ { r }$ minor? Will some sufficiently high average degree or chromatic number ensure that one of these substructures occurs?

Questions of this type are among the most natural ones in graph theory, and there is a host of deep and interesting results. Collectively, these are known as extremal graph theory.

Extremal graph problems in this sense fall neatly into two categories, as follows. If we are looking for ways to ensure by global assumptions that a graph $G$ contains some given graph $H$ as a minor (or topological minor), it will suffice to raise $\| G \|$ above the value of some linear function of $| G |$ , i.e., to make $\varepsilon ( G )$ large enough. The precise value of $\varepsilon$ needed to force a desired minor or topological minor will be our topic in Section 7.2. Graphs whose number of edges is about1 linear in their number of vertices are called sparse, so Section 7.2 is devoted to ‘sparse extremal graph theory’.

A particularly interesting way to force an $H$ minor is to assume that $\chi ( G )$ is large. Recall that if $\chi ( G ) \geqslant k + 1$ , say, then $G$ has a subgraph $G ^ { \prime }$ with $2 \varepsilon ( G ^ { \prime } ) \geqslant \delta ( G ^ { \prime } ) \geqslant k$ (Corollary 5.2.3). The question here is whether the effect of large $\chi$ is limited to this indirect influence via $\varepsilon$ , or whether an assumption of $\chi \geqslant k + 1$ can force bigger minors than

the assumption of $2 \varepsilon \geqslant k$ can. Hadwiger’s conjecture, which we meet in Section 7.3, asserts that $\chi$ has this quality. The conjecture can be viewed as a generalization of the four colour theorem, and is regarded by many as the most challenging open problem in graph theory.

On the other hand, if we ask what global assumptions might imply the existence of some given graph $H$ as a subgraph, it will not help to raise invariants such as $\varepsilon$ or $\chi$ , let alone any of the other invariants discussed in Chapter 1. For as soon as $H$ contains a cycle, there are graphs of arbitrarily large chromatic number not containing $H$ as a subgraph (Theorem 5.2.5). In fact, unless $H$ is bipartite, any function $f$ such that $f ( n )$ edges on $n$ vertices force an $H$ subgraph must grow quadratically with $n$ : since complete bipartite graphs can have ${ \scriptstyle { \frac { 1 } { 4 } } } n ^ { 2 }$ edges, $f ( n )$ must exceed ${ \scriptstyle { \frac { 1 } { 4 } } n ^ { 2 } }$ .

Graphs with a number of edges about quadratic in their number of vertices are usually called dense; the number $\| G \| / { \binom { | G | } { 2 } }$ , the proportion of its potential edges that $G$ actually has, is the edge density of $G$ . The question of exactly which edge density is needed to force a given subgraph is the archetypal extremal graph problem, and it is our first topic in this chapter (Section 7.1). Rather than attempting to survey the wide field of ‘dense extremal graph theory’, however, we shall concentrate on its two most important results: we first prove Tur´an’s classical extremal graph theorem for $H = K ^ { r }$ —a result that has served as a model for countless similar theorems for other graphs $H$ —and then state the fundamental Erd˝os-Stone theorem, which gives precise asymptotic information for all $H$ at once.

Although the Erd˝os-Stone theorem can be proved by elementary means, we shall use the opportunity of its proof to portray a powerful modern proof technique that has transformed much of extremal graph theory in recent years: Szemer´edi regularity lemma. This lemma is presented and proved in Section 7.4. In Section 7.5, we outline a general method for applying it, and illustrate this in the proof of the Erd˝os-Stone theorem. Another application of the regularity lemma will be given in Chapter 9.2.

# 7.1 Subgraphs

Let $H$ be a graph and $n \geqslant | H |$ . How many edges will suffice to force an $H$ subgraph in any graph on $n$ vertices, no matter how these edges are arranged? Or, to rephrase the problem: which is the greatest possible number of edges that a graph on $n$ vertices can have without containing a copy of $H$ as a subgraph? What will such a graph look like? Will it be unique?

A graph $G \nsupseteq H$ on $n$ vertices with the largest possible number of edges is called extremal for $n$ and $H$ ; its number of edges is denoted by

$\operatorname { e x } ( n , H )$ . Clearly, any graph $G$ that is extremal for some $n$ and $H$ will $\operatorname { e x } ( n , H )$ also be edge-maximal with $H \nsubseteq G$ . Conversely, though, edge-maximality does not imply extremality: $G$ may well be edge-maximal with $H \subsetneq G$ while having fewer than $\operatorname { e x } ( n , H )$ edges (Fig. 7.1.1).

![](images/ce247f22e70eff5787c6a93146fd786c9f5037229ed366df9f8c4720cd9372c7.jpg)  
Fig. 7.1.1. Two graphs that are edge-maximal with $P ^ { 3 } \ \nsubseteq { G }$ ; is the right one extremal?

As a case in point, we consider our problem for $H = K ^ { r }$ (with $r > 1$ ). A moment’s thought suggests some obvious candidates for extremality here: all complete $( r - 1 )$ -partite graphs are edge-maximal without containing $K ^ { r }$ . But which among these have the greatest number of edges? Clearly those whose partition sets are as equal as possible, i.e. differ in size by at most 1: if $V _ { 1 } , V _ { 2 }$ are two partition sets with $\left| V _ { 1 } \right| - \left| V _ { 2 } \right| \geqslant 2$ , we may increase the number of edges in our complete $( r - 1 )$ -partite graph by moving a vertex from $V _ { 1 }$ to $V _ { 2 }$ .

The unique complete $( r - 1 )$ -partite graphs on $n \geqslant r - 1$ vertices whose partition sets differ in size by at most 1 are called Tur´an graphs; we denote them by $T ^ { r - 1 } ( n )$ and their number of edges by $t _ { r - 1 } ( n )$ . $T ^ { r - 1 } ( n )$ (Fig. 7.1.2). For $\textit { n } < \textit { r } - 1$ we shall formally continue to use these $t _ { r - 1 } ( n )$ definitions, with the proviso that—contrary to our usual terminology— the partition sets may now be empty; then, clearly, $T ^ { r - 1 } ( n ) = K ^ { n }$ for all $n \leqslant r - 1$ .

![](images/c741c0d33018001970f74234478572ec2473b0b835a74440e490ff6e8965e22a.jpg)  
Fig. 7.1.2. The Tur´an graph $T ^ { 3 } ( 8 )$

The following theorem tells us that $T ^ { r - 1 } ( n )$ is indeed extremal for $n$ and $K ^ { r }$ , and as such unique; in particular, $\mathrm { e x } ( n , K ^ { r } ) = t _ { r - 1 } ( n )$ .

# Theorem 7.1.1. (Tur´an 1941)

For all integers $r , n$ with $r > 1$ , every graph $G \nsup K ^ { \tau }$ with $n$ vertices and $\exp ( n , K ^ { r } )$ edges is a $T ^ { r - 1 } ( n )$ .

[ 9.2.2 ]

We give two proofs: one using induction, the other by a very short and direct local argument.

First proof. We apply induction on $n$ . For $n \leqslant r - 1$ we have $G =$ $K ^ { n } = T ^ { r - 1 } ( n )$ as claimed. For the induction step, let now $n \geqslant r$ .

$K$

Since $G$ is edge-maximal without a $K ^ { r }$ subgraph, $G$ has a subgraph $K = K ^ { r - 1 }$ . By the induction hypothesis, $G - K$ has at most $t _ { r - 1 } ( n - r + 1 )$ edges, and each vertex of $G - K$ has at most $r \mathrm { ~ - ~ } 2$ 2 neighbours in $K$ . Hence,

$$
\| G \| \leqslant t _ {r - 1} (n - r + 1) + (n - r + 1) (r - 2) + \binom {r - 1} {2} = t _ {r - 1} (n); \quad (1)
$$

the equality on the right follows by inspection of the Tur´an graph $T ^ { r - 1 } ( n )$ (Fig. 7.1.3).

![](images/26320c95810524e788f601e397729fccd17152bb473dbe983ea39b819aa8a1b5.jpg)  
Fig. 7.1.3. The equation from (1) for $r = 5$ and $n = 1 4$

Since $G$ is extremal for $K ^ { r }$ (and $T ^ { r - 1 } ( n ) \nsupseteq K ^ { r } )$ ), we have equality in (1). Thus, every vertex of $G - K$ has exactly $r - 2$ neighbours in $K$ — $x _ { 1 } , \ldots , x _ { r - 1 }$ just like the vertices $x _ { 1 } , \ldots , x _ { r - 1 }$ of $K$ itself. For $i = 1 , \ldots , r - 1$ let

$$
V _ {1}, \dots , V _ {r - 1} \quad V _ {i} := \left\{v \in V (G) \mid v x _ {i} \notin E (G) \right\}
$$

be the set of all vertices of $G$ whose $r - 2$ neighbours in $K$ are precisely the vertices other than $x _ { i }$ . Since $K ^ { r } \nsubseteq G$ , each of the sets $V _ { i }$ is independent, and they partition $V ( G )$ . Hence, $G$ is $( r - 1 )$ -partite. As $T ^ { r - 1 } ( n )$ is the unique $( r - 1 )$ -partite graph with $n$ vertices and the maximum number of edges, our claim that $G = T ^ { r - 1 } ( n )$ follows from the assumed extremality of $G$ . 

vertex duplication

In our second proof of Tur´an’s theorem we shall use an operation called vertex duplication. By duplicating a vertex $v \in G$ we mean adding to $G$ a new vertex $v ^ { \prime }$ and joining it to exactly the neighbours of $v$ (but not to $v$ itself).

Second proof. We have already seen that among the complete $k$ -partite graphs on $n$ vertices the Tur´an graphs $T ^ { k } ( n )$ have the most edges, and their degrees show that $T ^ { r - 1 } ( n )$ has more edges than any $T ^ { k } ( n )$ with $k < r - 1$ . So it suffices to show that $G$ is complete multipartite.

If not, then non-adjacency is not an equivalence relation on $V ( G )$ , and so there are vertices $y _ { 1 } , x , y _ { 2 }$ such that $y _ { 1 } x , x y _ { 2 } \notin E ( G )$ but $y _ { 1 } y _ { 2 } \in$ $E ( G )$ . If $d ( y _ { 1 } ) > d ( x )$ , then deleting $x$ and duplicating $y _ { 1 }$ yields another $K ^ { r }$ -free graph with more edges than $G$ , contradicting the choice of $G$ . So $d ( y _ { 1 } ) \leqslant d ( x )$ , and similarly $d ( y _ { 2 } ) \leqslant d ( x )$ . But then deleting both $y _ { 1 }$ and $y _ { 2 }$ and duplicating $x$ twice yields a $K ^ { r }$ -free graph with more edges than $G$ , again contradicting the choice of $G$ . 

The Tur´an graphs $T ^ { r - 1 } ( n )$ are dense: in order of magnitude, they have about $n ^ { 2 }$ edges. More exactly, for every $n$ and $r$ we have

$$
t _ {r - 1} (n) \leqslant \frac {1}{2} n ^ {2} \frac {r - 2}{r - 1},
$$

with equality whenever $r - 1$ divides $n$ (Exercise 7). It is therefore remarkable that just $\epsilon n ^ { 2 }$ more edges (for any fixed $\epsilon > 0$ and $n$ large) give us not only a $K ^ { r }$ subgraph (as does Tur´an’s theorem) but a $K _ { s } ^ { r }$ for any given integer $s$ —a graph itself teeming with $K ^ { r }$ subgraphs:

# Theorem 7.1.2. (Erd˝os & Stone 1946)

For all integers $r \geqslant 2$ and $s \geqslant 1$ , and every $\epsilon > 0$ , there exists an integer $n _ { 0 }$ such that every graph with $n \geqslant n _ { 0 }$ vertices and at least

$$
t _ {r - 1} (n) + \epsilon n ^ {2}
$$

edges contains $K _ { s } ^ { r }$ as a subgraph.

A proof of the Erd˝os-Stone theorem will be given in Section 7.5, as an illustration of how the regularity lemma may be applied. But the theorem can also be proved directly; see the notes for references.

The Erd˝os-Stone theorem is interesting not only in its own right: it also has a most interesting corollary. In fact, it was this entirely unexpected corollary that established the theorem as a kind of meta-theorem for the extremal theory of dense graphs, and thus made it famous.

Given a graph $H$ and an integer $n$ , consider the number $h _ { n } : =$ $\exp ( n , H ) / { \binom { n } { 2 } }$ : the maximum edge density that an $n$ -vertex graph can have without containing a copy of $H$ . Could it be that this critical density is essentially just a function of $H$ , that $h _ { n }$ converges as $n \longrightarrow \infty$ ? Theorem 7.1.2 implies this, and more: the limit of $h _ { n }$ is determined by a very simple function of a natural invariant of $H$ —its chromatic number!

Corollary 7.1.3. For every graph $H$ with at least one edge,

$$
\lim _ {n \to \infty} \exp (n, H) {\binom {n} {2}} ^ {- 1} = \frac {\chi (H) - 2}{\chi (H) - 1}.
$$

For the proof of Corollary 7.1.3 we need as a lemma that $t _ { r - 1 } ( n )$ never deviates much from the value it takes when $r - 1$ divides $n$ (see above), and that $t _ { r - 1 } ( n ) / { \binom { n } { 2 } }$ converges accordingly. The proof of the lemma is left as an easy exercise with hint (Exercise 8).

[ 7.1.2 ] Lemma 7.1.4.

$$
\lim  _ {n \to \infty} t _ {r - 1} (n) \binom {n} {2} ^ {- 1} = \frac {r - 2}{r - 1}.
$$



$r$

Proof of Corollary 7.1.3. Let $r : = \chi ( H )$ . Since $H$ cannot be coloured with $r - 1$ colours, we have $H \not \subseteq T ^ { r - 1 } ( n )$ for all $n \in \mathbb N$ , and hence

$$
t _ {r - 1} (n) \leqslant \exp (n, H).
$$

On the other hand, $H \subseteq K _ { s } ^ { r }$ for all sufficiently large $s$ , so

$$
\operatorname {e x} (n, H) \leqslant \operatorname {e x} (n, K _ {s} ^ {r})
$$

s

for all those $s$ . Let us fix such an $s$ . For every $\epsilon > 0$ , Theorem 7.1.2 implies that eventually (i.e. for large enough $n$ )

$$
\exp (n, K _ {s} ^ {r}) <   t _ {r - 1} (n) + \epsilon n ^ {2}.
$$

Hence for $n$ large,

$$
\begin{array}{l} t _ {r - 1} (n) / \binom {n} {2} \leqslant \exp (n, H) / \binom {n} {2} \\ \leqslant \exp (n, K _ {s} ^ {r}) / \binom {n} {2} \\ <   t _ {r - 1} (n) / \binom {n} {2} + \epsilon n ^ {2} / \binom {n} {2} \\ = t _ {r - 1} (n) / \binom {n} {2} + 2 \epsilon / \left(1 - \frac {1}{n}\right) \\ \leqslant t _ {r - 1} (n) / \binom {n} {2} + 4 \epsilon \quad (\text {a s s u m e} n \geqslant 2). \\ \end{array}
$$

Therefore, since $t _ { r - 1 } ( n ) / { \binom { n } { 2 } }$ converges to $\frac { r - 2 } { r - 1 }$ (Lemma 7.1.4), so does $\exp ( n , H ) / { \binom { n } { 2 } }$ . 

For bipartite graphs $H$ , Corollary 7.1.3 says that substantially fewer than n edges suffice to force an $H$ subgraph. It turns out that

$$
c _ {1} n ^ {2 - \frac {2}{r + 1}} \leqslant \mathrm {e x} (n, K _ {r, r}) \leqslant c _ {2} n ^ {2 - \frac {1}{r}}
$$

for suitable constants $c _ { 1 } , c _ { 2 }$ depending on $r$ ; the lower bound is obtained by random graphs,2 the upper bound is calculated in Exercise 11. If $H$ is a forest, then $H \subseteq G$ as soon as $\varepsilon ( G )$ is large enough, so $\operatorname { e x } ( n , H )$ is at most linear in $n$ (Exercise 13). Erd˝os and S´os conjectured in 1963 that $\begin{array} { r } { \exp ( n , T ) \leqslant \frac { 1 } { 2 } ( k - 1 ) n } \end{array}$ for all trees with $k \geqslant 2$ edges; as a general bound for all $n$ , this is best possible for every $T$ (Exercises 14–16).

A related but rather different question is whether large values of $\varepsilon$ or $\chi$ can force a graph $G$ to contain a given tree $T$ as an induced subgraph. Of course, we need some additional assumption for this to make sense— for example, to prevent $G$ from just being a large complete graph. The weakest sensible such assumption is that $G$ has bounded clique number, i.e., that $G \nsup K ^ { r }$ for some fixed integer $r$ . Then large average degree still does not force an induced copy of $T$ —consider complete bipartite graphs—but large chromatic number might: according to a remarkable conjecture of Gy´arf´as (1975), there exists for every $r ~ \in ~ \mathbb { N }$ and every tree $T$ an integer $k = k ( T , r )$ such that every graph $G$ with $\chi ( G ) \geqslant k$ and $\omega ( G ) < r$ contains $T$ as an induced subgraph.

# 7.2 Minors

In this section and the next, we ask how global assumptions about a graph—on its average degree, its chromatic number, or even its girth— can force it to contain a given graph as a minor or topological minor.

For example, consider the analogue of Tur´an’s theorem: how many edges on $n$ vertices force a $K ^ { r }$ minor or topological minor? We know already from Chapter 3.5 that topological $K ^ { r }$ minors can be forced in sparse graphs, i.e., that some linear number $c _ { r } n$ of edges is enough. But what can we say about $c _ { r }$ as a function of $r$ ? The upper bound $h ( r )$ on $c _ { r }$ that we found in the proof of Lemma 3.5.1 was $2 ^ { \binom { r } { 2 } }$ ; an easy lower bound is $\textstyle { \frac { 1 } { 8 } } r ^ { 2 }$ (Exercise 25).

It was only in 1996 that this lower bound was shown to be of the right order of magnitude. With the help of Theorem 3.5.3, the proof is now just a few lines:

Theorem 7.2.1. There is a constant $c \in \mathbb { R }$ such that, for every $r \in \mathbb N$ , every graph $G$ of average degree $d ( G ) \geqslant c r ^ { 2 }$ contains $K ^ { r }$ as a topological minor.

Proof . We prove the theorem with $c = 1 0$ . Let $G$ be a graph of average degree at least $1 0 r ^ { 2 }$ . By Theorem 1.4.3 with $k : = r ^ { 2 }$ , $G$ has an $r ^ { 2 }$ -connected subgraph $H$ with $\varepsilon ( H ) > \varepsilon ( G ) - r ^ { 2 } \geqslant 4 r ^ { 2 }$ . To find a $T K ^ { r }$ in $H$ , we start by picking $r$ vertices as branch vertices, and $r - 1$ neighbours of each of these as some initial subdividing vertices. These are $r ^ { 2 }$ vertices in total, so as $\delta ( H ) \geqslant \kappa ( H ) \geqslant r ^ { 2 }$ they can be chosen distinct. Now all that remains is to link up the subdividing vertices in pairs, by disjoint paths in $H$ corresponding to the edges of the $K ^ { r }$ of which we wish to find a subdivision. Such paths exist, because $H$ is ${ \scriptstyle { \frac { 1 } { 2 } } } r ^ { 2 }$ -linked by Theorem 3.5.3. 

For small $r$ , one can try to determine the exact number of edges needed to force a $T K ^ { r }$ subgraph on $n$ vertices. For $r = 4$ , this number is $2 n - 2$ ; see Corollary 7.3.2. For $r = 5$ , plane triangulations yield a lower bound of $3 n - 5$ (Corollary 4.2.10). The converse, that $3 n - 5$ edges do force a $T K ^ { 5 }$ —not just either a $T K ^ { 5 }$ or a $T K _ { 3 , 3 }$ , as they do by Corollary 4.2.10 and Kuratowski’s theorem—is already a difficult theorem (Mader 1998).

Let us now turn from topological minors to general minors. The average degree needed to force a $K ^ { r }$ minor is known almost precisely. Thomason (2001) determined, asymptotically, the smallest constant $c$ that makes the following theorem true as $\alpha + o ( 1 )$ , where $o ( 1 )$ stands for a function of $r$ tending to zero as $r \longrightarrow \infty$ and $\alpha = 0 . 5 3 1 3 1 \ldots$ . is an explicit constant.

# Theorem 7.2.2. (Kostochka 1982)

There exists a constant $c \in \mathbb { R }$ such that, for every $r \in \mathbb N$ , every graph $G$ of average degree $d ( G ) \geqslant c r \sqrt { \log r }$ contains $K ^ { r }$ as a minor. Up to the value of $c$ , this bound is best possible as a function of $r$ .

The easier implication of the theorem, the fact that in general an average degree of $c r { \sqrt { \log r } }$ is needed to force a $K ^ { r }$ minor, follows from considering random graphs, to be introduced in Chapter 11. The converse implication, that this average degree suffices, is proved by methods not dissimilar to the proof of Theorem 3.5.3.

Rather than proving Theorem 7.2.2, therefore, we devote the remainder of this section to another striking aspect of forcing minors: that we can force a $K ^ { r }$ minor in a graph simply by raising its girth (as long as we do not merely subdivide edges). At first glance, this may seem almost paradoxical. But it looks more plausible if, rather than trying to force a $K ^ { r }$ minor directly, we instead try to force a minor just of large minimum or average degree—which suffices by Theorem 7.2.2. For if the girth $g$ of a graph is large then the ball $\{ v \ | \ d ( x , v ) < \lfloor g / 2 \rfloor \}$ around a vertex $x$ induces a tree with many leaves, each of which sends all but one of its incident edges away from the tree. Contracting enough disjoint

such trees we can thus hope to obtain a minor of large average degree, which in turn will have a large complete minor.

The following lemma realizes this idea.

Lemma 7.2.3. Let $d , k \in \mathbb { N }$ with $d \geqslant 3$ , and let $G$ be a graph of minimum degree $\delta ( G ) \geqslant d$ and girth $g ( G ) \geqslant 8 k + 3$ . Then $G$ has a minor $H$ of minimum degree $\delta ( H ) \geqslant d ( d - 1 ) ^ { k }$ .

Proof . Let $X \subseteq V ( G )$ be maximal with $d ( x , y ) > 2 k$ for all $x , y \in X$ . For each $x \in X$ put $T _ { x } ^ { 0 } : = \{ x \}$ . Given $i < 2 k$ , assume that we have defined disjoint trees $T _ { x } ^ { i } \subseteq G$ (one for each $x \in X$ ) whose vertices together are precisely the vertices at distance at most $_ i$ from $X$ in $G$ . Joining each vertex at distance $i + 1$ from $X$ to a neighbour at distance $_ i$ , we obtain a similar set of disjoint trees $T _ { x } ^ { i + 1 }$ . As every vertex of $G$ has distance at most $2 k$ from $X$ (by the maximality of $X$ ), the trees $T _ { x } : = T _ { x } ^ { 2 k }$ obtained in this way partition the entire vertex set of $G$ . Let $H$ be the minor of $G$ obtained by contracting every $T _ { x }$ .

To prove that $\delta ( H ) \geqslant d ( d - 1 ) ^ { k }$ , note first that the $T _ { x }$ are induced subgraphs of $G$ , because $\mathrm { d i a m } T _ { x } \leqslant 4 k$ and $g ( G ) > 4 k + 1$ . Similarly, there is at most one edge in $G$ between any two trees $T _ { x }$ and $T _ { y }$ : two such edges, together with the paths joining their ends in $T _ { x }$ and $T _ { y }$ , would form a cycle of length at most $8 k + 2 < g ( G )$ . So all the edges leaving $T _ { x }$ are preserved in the contraction.

How many such edges are there? Note that, for every vertex $u \in$ $T _ { x } ^ { k - 1 }$ , all its $d _ { G } ( u ) \geqslant d$ neighbours $\boldsymbol { v }$ also lie in $T _ { x }$ : since $d ( v , x ) \leqslant k$ and $d ( x , y ) > 2 k$ for every other $y \in X$ , we have $d ( v , y ) > k \geqslant d ( v , x )$ , so $v$ was added to $T _ { x }$ rather than to $T _ { y }$ when those trees were defined. Therefore $T _ { x } ^ { k }$ , and hence also $T _ { x }$ , has at least $d ( d - 1 ) ^ { k - 1 }$ leaves. But every leaf of $T _ { x }$ sends at least $d - 1$ edges away from $T _ { x }$ , so $T _ { x }$ sends at least $d ( d - 1 ) ^ { k }$ edges to (distinct) other trees $T _ { y }$ . 

Lemma 7.2.3 provides Theorem 7.2.2 with the following corollary:

# Theorem 7.2.4. (Thomassen 1983)

There exists a function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ such that every graph of minimum degree at least 3 and girth at least $f ( r )$ has a $K ^ { r }$ minor, for all $r \in \mathbb N$ .

Proof . We prove the theorem with $f ( r ) : = 8 \log r + 4 \log \log r + c$ , for some constant $c \in \mathbb { R }$ . Let $k = k ( r ) \in \mathbb { N }$ be minimal with $3 \cdot 2 ^ { k } \geqslant c ^ { \prime } r { \sqrt { \log r } }$ , where $c ^ { \prime } \in \mathbb { R }$ is the constant from Theorem 7.2.2. Then for a suitable constant $c \in \mathbb { R }$ we have $8 k + 3 \leqslant 8 \log r + 4 \log \log r + c$ , and the result follows by Lemma 7.2.3 and Theorem 7.2.2. 

Large girth can also be used to force a topological $K ^ { r }$ minor . We now need some vertices of degree at least $r - 1$ to serve as branch vertices, but if we assume a minimum degree of $r - 1$ to secure these, we can even get by with a girth bound that is independent of $r$ :

[ 7.3.9 ] Theorem 7.2.5. (K¨uhn & Osthus 2002) There exists a constant $_ { g }$ such that $G \supseteq T K ^ { r }$ for every graph $G$ satisfying $\delta ( G ) \geqslant r - 1$ and $g ( G ) \geqslant g$ .

# 7.3 Hadwiger’s conjecture

As we saw in Section 7.2, an average degree of $c r { \sqrt { \log r } }$ suffices to force an arbitrary graph to have a $K ^ { r }$ minor, and an average degree of $c r ^ { 2 }$ forces it to have a topological $K ^ { \prime }$ minor. If we replace ‘average degree’ above with ‘chromatic number’ then, with almost the same constants $c$ , the two assertions remain true: this is because every graph with chromatic number $k$ has a subgraph of average degree at least $k - 1$ (Corollary 5.2.3).

Although both functions above, $c r { \sqrt { \log r } }$ and $c r ^ { 2 }$ , are best possible (up to the constant $c$ ) for the said implications with ‘average degree’, the question arises whether they are still best possible with ‘chromatic number’—or whether some slower-growing function would do in that case. What lies hidden behind this problem about growth rates is a fundamental question about the nature of the invariant $\chi$ : can this invariant have some direct structural effect on a graph in terms of forcing concrete substructures, or is its effect no greater than that of the ‘unstructural’ property of having lots of edges somewhere, which it implies trivially?

Neither for general nor for topological minors is the answer to this question known. For general minors, however, the following conjecture of Hadwiger suggests a positive answer:

# Conjecture. (Hadwiger 1943)

The following implication holds for every integer $r \ > \ 0$ and every graph $G$ :

$$
\chi (G) \geqslant r \Rightarrow G \succcurlyeq K ^ {r}.
$$

Hadwiger’s conjecture is trivial for $r \leqslant 2$ , easy for $r = 3$ and $r = 4$ (exercises), and equivalent to the four colour theorem for $r \ = \ 5$ and $r = 6$ . For $r \geqslant 7$ the conjecture is open, but it is true for line graphs (Exercise 35) and for graphs of large girth (Exercise 33; see also Corollary 7.3.9). Rephrased as $G \succcurlyeq K ^ { \chi ( G ) }$ , it is true for almost all graphs.3 In general, the conjecture for $r + 1$ implies it for $r$ (exercise).

The Hadwiger conjecture for any fixed $r$ is equivalent to the assertion that every graph without a $K ^ { r }$ minor has an $( r - 1 )$ -colouring. In this reformulation, the conjecture raises the question of what the graphs without a $K ^ { r }$ minor look like: any sufficiently detailed structural description of those graphs should enable us to decide whether or not they can be $( r - 1 )$ -coloured.

For $r = 3$ , for example, the graphs without a $K ^ { \prime }$ minor are precisely the forests (why?), and these are indeed 2-colourable. For $r = 4$ , there is also a simple structural characterization of the graphs without a $K ^ { r }$ minor:

Proposition 7.3.1. A graph with at least three vertices is edge-maximal [ 12.4.2 ] without a $K ^ { 4 }$ minor if and only if it can be constructed recursively from triangles by pasting $^ 4$ along $K ^ { 2 } s$ .

Proof . Recall first that every $M K ^ { 4 }$ contains a $T K ^ { 4 }$ , because $\Delta ( K ^ { 4 } ) = 3$ (1.7.2)(4.4.4) (Proposition 1.7.2); the graphs without a $K ^ { 4 }$ minor thus coincide with those without a topological $K ^ { 4 }$ minor. The proof that any graph constructible as described is edge-maximal without a $K ^ { 4 }$ minor is left as an easy exercise; in order to deduce Hadwiger’s conjecture for $r \ = \ 4$ , we only need the converse implication anyhow. We prove this by induction on $| G |$ .

Let $G$ be given, edge-maximal without a $K ^ { 4 }$ minor. If $| G | = 3$ then $G$ is itself a triangle, so let $| G | \geqslant 4$ for the induction step. Then $G$ is not complete; let $S \subseteq V ( G )$ be a separator of size $\kappa ( G )$ , and let $C _ { 1 } , C _ { 2 }$ be distinct components of $G - S$ . Since $S$ is a minimal separator, every vertex in $S$ has a neighbour in $C _ { 1 }$ and another in $C _ { 2 }$ . If $\vert S \vert \geqslant 3$ , this implies that $G$ contains three independent paths $P _ { 1 } , P _ { 2 } , P _ { 3 }$ between a vertex $v _ { 1 } \in C _ { 1 }$ and a vertex $v _ { 2 } \in C _ { 2 }$ . Since $\kappa ( G ) = | S | \geqslant 3$ , the graph $\boldsymbol { G } - \{ \boldsymbol { v } _ { 1 } , \boldsymbol { v } _ { 2 } \}$ is connected and contains a (shortest) path $P$ between two different $P _ { i }$ . Then $P \cup P _ { 1 } \cup P _ { 2 } \cup P _ { 3 } = T K ^ { 4 }$ , a contradiction.

Hence $\kappa ( G ) \leqslant 2$ , and the assertion follows from Lemma $4 . 4 . 4 ^ { 5 }$ and the induction hypothesis. 

One of the interesting consequences of Proposition 7.3.1 is that all the edge-maximal graphs without a $K ^ { 4 }$ minor have the same number of edges, and are thus all ‘extremal’:

Corollary 7.3.2. Every edge-maximal graph $G$ without a $K ^ { 4 }$ minor has $2 \left| G \right| - 3$ edges.

Proof . Induction on $| G |$

Corollary 7.3.3. Hadwiger’s conjecture holds for $r = 4$ .

Proof . If $G$ arises from $G _ { 1 }$ and $G _ { 2 }$ by pasting along a complete graph, then $\chi ( G ) = \operatorname* { m a x } \left\{ \chi ( G _ { 1 } ) , \chi ( G _ { 2 } ) \right\}$ (see the proof of Proposition 5.5.2). Hence, Proposition 7.3.1 implies by induction on $| G |$ that all edge-maximal (and hence all) graphs without a $K ^ { 4 }$ minor can be 3-coloured. 

It is also possible to prove Corollary 7.3.3 by a simple direct argument (Exercise 34).

By the four colour theorem, Hadwiger’s conjecture for $r = 5$ follows from the following structure theorem for the graphs without a $K ^ { 5 }$ minor, just as it follows from Proposition 7.3.1 for $r = 4$ . The proof of Theorem 7.3.4 is similar to that of Proposition 7.3.1, but considerably longer. We therefore state the theorem without proof:

# Theorem 7.3.4. (Wagner 1937)

Let $G$ be an edge-maximal graph without a $K ^ { 5 }$ minor. If $| G | \geqslant 4$ then $G$ can be constructed recursively, by pasting along triangles and $K ^ { 2 } s$ , from plane triangulations and copies of the graph $W$ (Fig. 7.3.1).

![](images/1c453dc52ded422656e237acbb93d1f797bba2184ce41e2d0eb834f957837a59.jpg)  
Fig. 7.3.1. Three representations of the Wagner graph $W$

(4.2.10) Using Corollary 4.2.10, one can easily compute which of the graphs constructed as in Theorem 7.3.4 have the most edges. It turns out that these extremal graphs without a $K ^ { 5 }$ minor have no more edges than those that are extremal with respect to $\{ M K ^ { 5 } , M K _ { 3 , 3 } \}$ , i.e. the maximal planar graphs:

Corollary 7.3.5. A graph with $_ { n }$ vertices and no $K ^ { 5 }$ minor has at most $3 n - 6$ edges. 

Since $\chi ( W ) = 3$ , Theorem 7.3.4 and the four colour theorem imply Hadwiger’s conjecture for $r = 5$ :

Corollary 7.3.6. Hadwiger’s conjecture holds for $r = 5$ .

The Hadwiger conjecture for $r = 6$ is again substantially more difficult than the case $r = 5$ , and again it relies on the four colour theorem. The proof shows (without using the four colour theorem) that any minimal-order counterexample arises from a planar graph by adding one vertex—so by the four colour theorem it is not a counterexample after all.

Theorem 7.3.7. (Robertson, Seymour & Thomas 1993) Hadwiger’s conjecture holds for $r = 6$ .

As mentioned earlier, the challenge posed by Hadwiger’s conjecture is to devise a proof technique that makes better use of the assumption of $\chi \geqslant r$ than just using its consequence of $\delta \geqslant r - 1$ in a suitable subgraph, which we know cannot force a $K ^ { r }$ minor (Theorem 7.2.2). So far, no such technique is known.

If we resign ourselves to using just $\delta \geqslant r - 1$ , we can still ask what additional assumptions might help in making this force a $K ^ { r }$ minor. Theorem 7.2.5 says that an assumption of large girth has this effect; see also Exercise 33. In fact, a much weaker assumption suffices: for any fixed $s \in \mathbb N$ and all large enough $d$ depending only on $s$ , the graphs $\textit { G } \nsupseteq K _ { s , s }$ of average degree at least $d$ can be shown to have $K ^ { r }$ minors for $r$ considerably larger than $d$ . For Hadwiger’s conjecture, this implies the following:

# Theorem 7.3.8. (K¨uhn & Osthus 2005)

For every integer s there is an integer $r _ { s }$ such that Hadwiger’s conjecture holds for all graphs $\textit { G } \nsupseteq K _ { s , s }$ and $r \geqslant r _ { s }$ .

The strengthening of Hadwiger’s conjecture that graphs of chromatic number at least $r$ contain $K ^ { r }$ as a topological minor has become known as Haj´os’s conjecture. It is false in general, but Theorem 7.2.5 implies it for graphs of large girth:

Corollary 7.3.9. There is a constant $g$ such that all graphs $G$ of girth at least $g$ satisfy the implication $\chi ( G ) \geqslant r \Rightarrow G \supseteq T K ^ { r }$ for all $r$ .

Proof . If $\chi ( G ) \geqslant r$ then, by Corollary 5.2.3, $G$ has a subgraph $H$ of (5.2.3)(7.2.5) minimum degree $\delta ( H ) \geqslant r - 1$ . As $g ( H ) \geqslant g ( G ) \geqslant g$ , Theorem 7.2.5 implies that $G \supseteq H \supseteq T K ^ { r }$ . 

# 7.4 Szemer´edi’s regularity lemma

Almost 30 years ago, in the course of the proof of a major result on the Ramsey properties of arithmetic progressions, Szemer´edi developed a graph theoretical tool whose fundamental importance has been realized more and more in recent years: his so-called regularity or uniformity lemma. Very roughly, the lemma says that all graphs can be approximated by random graphs in the following sense: every graph can be partitioned, into a bounded number of equal parts, so that most of its edges run between different parts and the edges between any two parts

$\| X , Y \|$

are distributed fairly uniformly—just as we would expect it if they had been generated at random.

In order to state the regularity lemma precisely, we need some definitions. Let $G = ( V , E )$ be a graph, and let $X , Y \subseteq V$ be disjoint. Then we denote by $\| X , Y \|$ the number of $X$ – $Y$ edges of $G$ , and call

$d ( X , Y )$

$$
d (X, Y) := \frac {\| X , Y \|}{| X |   | Y |}
$$

density

$\epsilon$ -regular pair

the density of the pair $( X , Y )$ . (This is a real number between 0 and 1.) Given some $\epsilon > 0$ , we call a pair $( A , B )$ of disjoint sets $A , B \subseteq V$ -regular if all $X \subseteq A$ and $Y \subseteq B$ with

$$
| X | \geqslant \epsilon | A | \quad \text {a n d} \quad | Y | \geqslant \epsilon | B |
$$

satisfy

$$
\left| d (X, Y) - d (A, B) \right| \leqslant \epsilon .
$$

exceptional set

The edges in an $\epsilon$ -regular pair are thus distributed fairly uniformly, the more so the smaller the $\epsilon$ we started with.

Consider a partition $\{ V _ { 0 } , V _ { 1 } , \ldots , V _ { k } \}$ of $V$ in which one set $V _ { 0 }$ has been singled out as an exceptional set. (This exceptional set $V _ { 0 }$ may be empty.6) We call such a partition an $\epsilon$ -regular partition of $G$ if it satisfies the following three conditions:

$\epsilon$ -regular partition

(i) |V0| -  |V |;   
(ii) $| V _ { 1 } | = \ldots = | V _ { k } |$   
(iii) all but at most $\epsilon k ^ { 2 }$ of the pairs $( V _ { i } , V _ { j } )$ with $1 \leqslant i < j \leqslant k$ are $\epsilon$ -regular.

The role of the exceptional set $V _ { 0 }$ is one of pure convenience: it makes it possible to require that all the other partition sets have exactly the same size. Since condition (iii) affects only the sets $V _ { 1 } , \ldots , V _ { k }$ , we may think of $V _ { 0 }$ as a kind of bin: its vertices are disregarded when the uniformity of the partition is assessed, but there are only few such vertices.

[ 9.2.2 ]

# Lemma 7.4.1. (Regularity Lemma)

For every $\epsilon > 0$ and every integer $m \geqslant 1$ there exists an integer $M$ such that every graph of order at least $m$ admits an $\epsilon$ -regular partition $\{ V _ { 0 } , V _ { 1 } , \ldots , V _ { k } \}$ with $m \leqslant k \leqslant M$ .

The regularity lemma thus says that, given any $\epsilon > 0$ , every graph has an $\epsilon$ -regular partition into a bounded number of sets. The upper bound $M$ on the number of partition sets ensures that for large graphs the partition sets are large too; note that $\epsilon$ -regularity is trivial when the partition sets are singletons, and a powerful property when they are large. The lemma also allows us to specify a lower bound $m$ for the number of partition sets. This can be used to increase the proportion of edges running between different partition sets (i.e., of edges governed by the regularity assertion) over edges inside partition sets (about which we know nothing). See Exercise 39 for more details.

Note that the regularity lemma is designed for use with dense graphs:7 for sparse graphs it becomes trivial, because all densities of pairs—and hence their differences—tend to zero (Exercise 40).

The remainder of this section is devoted to the proof of the regularity lemma. Although the proof is not difficult, a reader meeting the regularity lemma here for the first time is likely to draw more insight from seeing how the lemma is typically applied than from studying the technicalities of its proof. Any such reader is encouraged to skip to the start of Section 7.5 now and come back to the proof at his or her leisure.

We shall need the following inequality for reals $\mu _ { 1 } , \ldots , \mu _ { k } > 0$ and $e _ { 1 } , \ldots , e _ { k } \geqslant 0$ :

$$
\sum \frac {e _ {i} ^ {2}}{\mu_ {i}} \geqslant \frac {\left(\sum e _ {i}\right) ^ {2}}{\sum \mu_ {i}}. \tag {1}
$$

This follows from the Cauchy-Schwarz inequality $\sum a _ { i } ^ { 2 } \sum b _ { i } ^ { 2 } \geqslant ( \sum a _ { i } b _ { i } ) ^ { 2 }$ by taking $a _ { i } : = \sqrt { \mu _ { i } }$ and $b _ { i } : = e _ { i } / \sqrt { \mu _ { i } }$ .

Let $G = ( V , E )$ be a graph and $n : = | V |$ . For disjoint sets $A , B \subseteq V$ $G = ( V , E )$ we define $_ n$

$$
q (A, B) := \frac {| A | | B |}{n ^ {2}} d ^ {2} (A, B) = \frac {\| A , B \| ^ {2}}{| A | | B | n ^ {2}}. \quad q (A, B)
$$

For partitions $\mathcal { A }$ of $A$ and $\boldsymbol { B }$ of $B$ we set

$$
q (\mathcal {A}, \mathcal {B}) := \sum_ {A ^ {\prime} \in \mathcal {A}; B ^ {\prime} \in \mathcal {B}} q \left(A ^ {\prime}, B ^ {\prime}\right), \quad q (\mathcal {A}, \mathcal {B})
$$

and for a partition $\mathcal { P } = \{ C _ { 1 } , \ldots , C _ { k } \}$ of $V$ we let

$$
q (\mathcal {P}) := \sum_ {i <   j} q \left(C _ {i}, C _ {j}\right). \quad q (\mathcal {P})
$$

However, if $\mathcal { P } = \{ C _ { 0 } , C _ { 1 } , \ldots , C _ { k } \}$ is a partition of $V$ with exceptional set $C _ { 0 }$ , we treat $C _ { 0 }$ as a set of singletons and define

$$
q (\mathcal {P}) := q (\tilde {\mathcal {P}}),
$$

$\tilde { \mathcal P }$

where ${ \bar { \mathcal { P } } } : = \left\{ C _ { 1 } , \ldots , C _ { k } \right\} \cup \left\{ \left\{ v \right\} : v \in C _ { 0 } \right\}$

The function $q ( \mathcal { P } )$ plays a pivotal role in the proof of the regularity lemma. On the one hand, it measures the uniformity of the partition $\mathcal { P }$ : if $\mathcal { P }$ has too many irregular pairs $( A , B )$ , we may take the pairs $( X , Y )$ of subsets violating the regularity of the pairs $( A , B )$ and make those sets $X$ and $Y$ into partition sets of their own; as we shall prove, this refines $\mathcal { P }$ into a partition for which $q$ is substantially greater than for $\mathcal { P }$ . Here, ‘substantial’ means that the increase of $q ( \mathcal { P } )$ is bounded below by some constant depending only on $\epsilon$ . On the other hand,

$$
\begin{array}{l} q (\mathcal {P}) = \sum_ {i <   j} q (C _ {i}, C _ {j}) \\ = \sum_ {i <   j} \frac {\left| C _ {i} \right| \left| C _ {j} \right|}{n ^ {2}} d ^ {2} \left(C _ {i}, C _ {j}\right) \\ \leqslant \frac {1}{n ^ {2}} \sum_ {i <   j} | C _ {i} | | C _ {j} | \\ \leqslant 1. \\ \end{array}
$$

The number of times that $q ( \mathcal { P } )$ can be increased by a constant is thus also bounded by a constant—in other words, after some bounded number of refinements our partition will be $\epsilon$ -regular! To complete the proof of the regularity lemma, all we have to do then is to note how many sets that last partition can possibly have if we start with a partition into $m$ sets, and to choose this number as our desired bound $M$ .

Let us make all this precise. We begin by showing that, when we refine a partition, the value of $q$ will not decrease:

# Lemma 7.4.2.

(i) Let $C , D \subseteq V$ be disjoint. If $\boldsymbol { \mathscr { C } }$ is a partition of $C$ and $\mathcal { D }$ is a partition of $D$ , then $q ( \mathcal { C } , \mathcal { D } ) \geqslant q ( C , D )$ .   
(ii) $I f \mathcal { P } , \mathcal { P } ^ { \prime }$ are partitions of $V$ and ${ \mathcal { P } } ^ { \prime }$ refines $\mathcal { P }$ , then $q ( \mathcal { P } ^ { \prime } ) \geqslant q ( \mathcal { P } )$ .

Proof . (i) Let $\mathcal { C } = : \{ C _ { 1 } , \ldots , C _ { k } \}$ and $\mathcal { D } = : \{ D _ { 1 } , . . . , D _ { \ell } \}$ . Then

$$
\begin{array}{l} q (\mathcal {C}, \mathcal {D}) = \sum_ {i, j} q (C _ {i}, D _ {j}) \\ = \frac {1}{n ^ {2}} \sum_ {i, j} \frac {\left\| C _ {i} , D _ {j} \right\| ^ {2}}{\left| C _ {i} \right| \left| D _ {j} \right|} \\ \end{array}
$$

$$
\begin{array}{l} \geqslant \frac {1}{(1)} \frac {\left(\sum_ {i , j} \| C _ {i} , D _ {j} \|\right) ^ {2}}{\sum_ {i , j} | C _ {i} | | D _ {j} |} \\ = \frac {1}{n ^ {2}} \frac {\| C , D \| ^ {2}}{\left(\sum_ {i} | C _ {i} |\right) \left(\sum_ {j} | D _ {j} |\right)} \\ = q (C, D). \\ \end{array}
$$

(ii) Let $\mathcal { P } = : \{ C _ { 1 } , \ldots , C _ { k } \}$ , and for $i = 1 , \ldots , k$ let $\mathcal { C } _ { i }$ be the partition of $C _ { i }$ induced by ${ \mathcal { P } } ^ { \prime }$ . Then

$$
\begin{array}{l} q (\mathcal {P}) = \sum_ {i <   j} q \left(C _ {i}, C _ {j}\right) \\ \leqslant \sum_ {i <   j} q \left(\mathcal {C} _ {i}, \mathcal {C} _ {j}\right) \tag {i} \\ \leqslant q \left(\mathcal {P} ^ {\prime}\right), \\ \end{array}
$$

since $\begin{array} { r } { q ( \mathcal { P } ^ { \prime } ) = \sum _ { i } q ( \mathcal { C } _ { i } ) + \sum _ { i < j } q ( \mathcal { C } _ { i } , \mathcal { C } _ { j } ) . } \end{array}$

Next, we show that refining a partition by subpartitioning an irregular pair of partition sets increases the value of $q$ a little; since we are dealing here with a single pair only, the amount of this increase will still be less than any constant.

Lemma 7.4.3. Let $\epsilon > 0$ , and let $C , D \subseteq V$ be disjoint. $H ( C , D )$ is not $\epsilon$ - regular, then there are partitions $\mathcal { C } = \{ C _ { 1 } , C _ { 2 } \}$ of $C$ and $\mathcal { D } = \{ D _ { 1 } , D _ { 2 } \}$ of $D$ such that

$$
q (\mathcal {C}, \mathcal {D}) \geqslant q (C, D) + \epsilon^ {4} \frac {| C | | D |}{n ^ {2}}.
$$

Proof . Suppose $( C , D )$ is not $\epsilon$ -regular. Then there are sets $C _ { 1 } \subseteq C$ and $D _ { 1 } \subseteq D$ with $| C _ { 1 } | > \epsilon | C |$ and $| D _ { 1 } | > \epsilon | D |$ such that

$$
| \eta | > \epsilon \tag {2}
$$

for $\eta : = d ( C _ { 1 } , D _ { 1 } ) - d ( C , D )$ . Let $\mathcal { C } : = \{ C _ { 1 } , C _ { 2 } \}$ and $\mathcal { D } : = \{ D _ { 1 } , D _ { 2 } \}$ , where $C _ { 2 } : = C \setminus C _ { 1 }$ and $D _ { 2 } : = D \setminus D _ { 1 }$ .

Let us show that $\boldsymbol { \mathscr { C } }$ and $\mathcal { D }$ satisfy the conclusion of the lemma. We shall write $c _ { i } : = | C _ { i } | , d _ { i } : = | D _ { i } | , e _ { i j } : = | | C _ { i } , D _ { j } | | , c : = | C | , d : = | D |$ and $e : = \| C , D \|$ . As in the proof of Lemma 7.4.2,

$$
\begin{array}{l} q (\mathcal {C}, \mathcal {D}) = \frac {1}{n ^ {2}} \sum_ {i, j} \frac {e _ {i j} ^ {2}}{c _ {i} d _ {j}} \\ = \frac {1}{n ^ {2}} \left(\frac {e _ {1 1} ^ {2}}{c _ {1} d _ {1}} + \sum_ {i + j > 2} \frac {e _ {i j} ^ {2}}{c _ {i} d _ {j}}\right) \\ \end{array}
$$

$c _ { i } , d _ { i } , e _ { i j }$ c, d, e

$$
\geqslant \frac {1}{n ^ {2}} \left(\frac {e _ {1 1} ^ {2}}{c _ {1} d _ {1}} + \frac {(e - e _ {1 1}) ^ {2}}{c d - c _ {1} d _ {1}}\right).
$$

By definition of $\eta$ , we have $e _ { 1 1 } = c _ { 1 } d _ { 1 } e / c d + \eta c _ { 1 } d _ { 1 }$ , so

$$
\begin{array}{l} n ^ {2} q (\mathcal {C}, \mathcal {D}) \geqslant \frac {1}{c _ {1} d _ {1}} \left(\frac {c _ {1} d _ {1} e}{c d} + \eta c _ {1} d _ {1}\right) ^ {2} \\ + \frac {1}{c d - c _ {1} d _ {1}} \left(\frac {c d - c _ {1} d _ {1}}{c d} e - \eta c _ {1} d _ {1}\right) ^ {2} \\ = \frac {c _ {1} d _ {1} e ^ {2}}{c ^ {2} d ^ {2}} + \frac {2 e \eta c _ {1} d _ {1}}{c d} + \eta^ {2} c _ {1} d _ {1} \\ + \frac {c d - c _ {1} d _ {1}}{c ^ {2} d ^ {2}} e ^ {2} - \frac {2 e \eta c _ {1} d _ {1}}{c d} + \frac {\eta^ {2} c _ {1} ^ {2} d _ {1} ^ {2}}{c d - c _ {1} d _ {1}} \\ \geqslant \frac {e ^ {2}}{c d} + \eta^ {2} c _ {1} d _ {1} \\ \geqslant \frac {e ^ {2}}{c d} + \epsilon^ {4} c d \\ \end{array}
$$

since $c _ { 1 } \geqslant \epsilon c$ and $d _ { 1 } \geqslant \epsilon d$ by the choice of $C _ { 1 }$ and $D _ { 1 }$ .

Finally, we show that if a partition has enough irregular pairs of partition sets to fall short of the definition of an $\epsilon$ -regular partition, then subpartitioning all those pairs at once results in an increase of $q$ by a constant:

Lemma 7.4.4. Let $0 ~ < ~ \epsilon \leqslant ~ 1 / 4$ , and let $\mathcal { P } ~ = ~ \{ C _ { 0 } , C _ { 1 } , \ldots , C _ { k } \}$ be a partition of $V$ , with exceptional set $C _ { 0 }$ of size $\left| C _ { 0 } \right| \leqslant \epsilon n$ and $| C _ { 1 } | = \ldots = | C _ { k } | = : c$ . If $\mathcal { P }$ is not $\epsilon$ -regular, then there is a partition $\mathcal { P } ^ { \prime } = \{ C _ { 0 } ^ { \prime } , C _ { 1 } ^ { \prime } , \ldots , C _ { \ell } ^ { \prime } \}$ of $V$ with exceptional set $C _ { 0 } ^ { \prime }$ , where $k \leqslant \ell \leqslant k 4 ^ { k }$ , such that $| C _ { 0 } ^ { \prime } | \leqslant | C _ { 0 } | + n / 2 ^ { k }$ , all other sets $C _ { i } ^ { \prime }$ have equal size, and

$$
q (\mathcal {P} ^ {\prime}) \geqslant q (\mathcal {P}) + \epsilon^ {5} / 2.
$$

Cij $c _ { i j }$

Proof . For all $1 \leqslant i < j \leqslant k$ , let us define a partition $\mathcal { C } _ { i j }$ of $C _ { i }$ and a partition $\boldsymbol { \mathcal { C } } _ { j i }$ of $C _ { j }$ , as follows. If the pair $( C _ { i } , C _ { j } )$ is $\epsilon$ -regular, we let $\mathcal { C } _ { i j } : = \{ C _ { i } \}$ and $\mathcal { C } _ { j i } : = \{ C _ { j } \}$ . If not, then by Lemma 7.4.3 there are partitions $\mathcal { C } _ { i j }$ of $C _ { i }$ and $\boldsymbol { \mathcal { C } } _ { j i }$ of $C _ { j }$ with $| \mathcal { C } _ { i j } | = | \mathcal { C } _ { j i } | = 2$ and

$$
q \left(\mathcal {C} _ {i j}, \mathcal {C} _ {j i}\right) \geqslant q \left(C _ {i}, C _ {j}\right) + \epsilon^ {4} \frac {\left| C _ {i} \right| \left| C _ {j} \right|}{n ^ {2}} = q \left(C _ {i}, C _ {j}\right) + \frac {\epsilon^ {4} c ^ {2}}{n ^ {2}}. \tag {3}
$$

$\mathit { C _ { i } }$

For each $i = 1 , \ldots , k$ , let $c _ { i }$ be the unique minimal partition of $C _ { i }$ that refines every partition $\mathcal { C } _ { i j }$ with $j \neq i$ . (In other words, if we consider two

elements of $C _ { i }$ as equivalent whenever they lie in the same partition set of $\mathcal { C } _ { i j }$ for every $j \neq i$ , then $\mathcal { C } _ { i }$ is the set of equivalence classes.) Thus, $| { \mathcal { C } } _ { i } | \leqslant 2 ^ { k - 1 }$ . Now consider the partition

$$
\mathcal {C} := \left\{C _ {0} \right\} \cup \bigcup_ {i = 1} ^ {k} \mathcal {C} _ {i}
$$

of $V$ , with $C _ { 0 }$ as exceptional set. Then $\boldsymbol { \mathscr { C } }$ refines $\mathcal { P }$ , and

$$
k \leqslant | \mathcal {C} | \leqslant k 2 ^ {k}. \tag {4}
$$

Let $\mathcal { C } _ { 0 } : = \{ \{ v \} : v \in C _ { 0 } \}$ . Now if $\mathcal { P }$ is not $\epsilon$ -regular, then for more than $\epsilon k ^ { 2 }$ of the pairs $( C _ { i } , C _ { j } )$ with $1 \leqslant i < j \leqslant k$ the partition $\mathcal { C } _ { i j }$ is non-trivial. Hence, by our definition of $q$ for partitions with exceptional set, and Lemma 7.4.2 (i),

$$
\begin{array}{l} q (\mathcal {C}) = \sum_ {1 \leqslant i <   j} q \left(\mathcal {C} _ {i}, \mathcal {C} _ {j}\right) + \sum_ {1 \leqslant i} q \left(\mathcal {C} _ {0}, \mathcal {C} _ {i}\right) + \sum_ {0 \leqslant i} q \left(\mathcal {C} _ {i}\right) \\ \geqslant \sum_ {1 \leqslant i <   j} q \left(\mathcal {C} _ {i j}, \mathcal {C} _ {j i}\right) + \sum_ {1 \leqslant i} q \left(\mathcal {C} _ {0}, \left\{C _ {i} \right\}\right) + q \left(\mathcal {C} _ {0}\right) \\ \geqslant \sum_ {1 \leqslant i <   j} q \left(C _ {i}, C _ {j}\right) + \epsilon k ^ {2} \frac {\epsilon^ {4} c ^ {2}}{n ^ {2}} + \sum_ {1 \leqslant i} q \left(\mathcal {C} _ {0}, \left\{C _ {i} \right\}\right) + q \left(\mathcal {C} _ {0}\right) \\ = q (\mathcal {P}) + \epsilon^ {5} \left(\frac {k c}{n}\right) ^ {2} \\ \geqslant q (\mathcal {P}) + \epsilon^ {5} / 2. \\ \end{array}
$$

(For the last inequality, recall that $\left| C _ { 0 } \right| \leqslant \epsilon n \leqslant { \frac { 1 } { 4 } } n$ , so $k c \geqslant { \textstyle { \frac { 3 } { 4 } } } n$ .)

In order to turn $\boldsymbol { \mathscr { C } }$ into our desired partition ${ \mathcal { P } } ^ { \prime }$ , all that remains to do is to cut its sets up into pieces of some common size, small enough that all remaining vertices can be collected into the exceptional set without making this too large. Let $C _ { 1 } ^ { \prime } , \ldots , C _ { \ell } ^ { \prime }$ be a maximal collection of disjoint sets of size $d : = \lfloor c / 4 ^ { k } \rfloor$ such that each $C _ { i } ^ { \prime }$ is contained in some $C \in \mathcal { C } \setminus \{ C _ { 0 } \}$ , and put $C _ { 0 } ^ { \prime } : = V \setminus \bigcup C _ { i } ^ { \prime }$ . Then $\mathcal { P } ^ { \prime } = \{ C _ { 0 } ^ { \prime } , C _ { 1 } ^ { \prime } , \ldots , C _ { \ell } ^ { \prime } \}$ is indeed a partition of $V$ . Moreover, ${ \ddot { \mathcal { P } } } ^ { \prime }$ refines $\ddot { \mathcal { C } }$ , so

$$
q (\mathcal {P} ^ {\prime}) \geqslant q (\mathcal {C}) \geqslant q (\mathcal {P}) + \epsilon^ {5} / 2
$$

by Lemma 7.4.2 (ii). Since each set $C _ { i } ^ { \prime } \neq C _ { 0 } ^ { \prime }$ is also contained in one of the sets $C _ { 1 } , \ldots , C _ { k }$ , but no more than $4 ^ { k }$ sets $C _ { i } ^ { \prime }$ can lie inside the same $C _ { j }$ (by the choice of $d$ ), we also have $k \leqslant \ell \leqslant k 4 ^ { k }$ as required. Finally, the sets $C _ { 1 } ^ { \prime } , \ldots , C _ { \ell } ^ { \prime }$ use all but at most $d$ vertices from each set

$C \neq C _ { 0 }$ of $c$ . Hence,

$$
\begin{array}{l} \left| C _ {0} ^ {\prime} \right| \leqslant \left| C _ {0} \right| + d \left| \mathcal {C} \right| \\ \leqslant \left| C _ {0} \right| + \frac {c}{4 ^ {k}} k 2 ^ {k} \\ = \left| C _ {0} \right| + c k / 2 ^ {k} \\ \leqslant | C _ {0} | + n / 2 ^ {k}. \\ \end{array}
$$

![](images/c9aceacad50bda47dd66a9bf94aa606b80d5a3188ad5306756485f790c8c83d2.jpg)

The proof of the regularity lemma now follows easily by repeated application of Lemma 7.4.4:

Proof of Lemma 7.4.1. Let $\epsilon > 0$ and $m \geqslant 1$ be given; without loss of generality, $\epsilon \leqslant 1 / 4$ . Let $s : = 2 / \epsilon ^ { 5 }$ . This number $s$ is an upper bound on the number of iterations of Lemma 7.4.4 that can be applied to a partition of a graph before it becomes $\epsilon$ -regular; recall that $q ( \mathcal { P } ) \leqslant 1$ for all partitions $\mathcal { P }$ .

There is one formal requirement which a partition $\{ C _ { 0 } , C _ { 1 } , \ldots , C _ { k } \}$ with $| C _ { 1 } | = \ldots = | C _ { k } |$ has to satisfy before Lemma 7.4.4 can be (re-) applied: the size $| C _ { 0 } |$ of its exceptional set must not exceed $\epsilon n$ . With each iteration of the lemma, however, the size of the exceptional set can grow by up to $n / 2 ^ { k }$ . (More precisely, by up to $n / 2 ^ { \ell }$ , where $\ell$ is the number of other sets in the current partition; but $\ell \geqslant k$ by the lemma, so $n / 2 ^ { k }$ is certainly an upper bound for the increase.) We thus want to choose $k$ large enough that even $s$ increments of $n / 2 ^ { k }$ add up to at most $\scriptstyle { \frac { 1 } { 2 } } \epsilon n$ , and $n$ large enough that, for any initial value of $| C _ { 0 } | < k$ , we have $| C _ { 0 } | \leqslant { \frac { 1 } { 2 } } \epsilon n$ . (If we give our starting partition $k$ non-exceptional sets $C _ { 1 } , \ldots , C _ { k }$ , we should allow an initial size of up to $k$ for $C _ { 0 }$ , to be able to achieve $| C _ { 1 } | = \ldots = | C _ { k } |$ .)

So let $k \geqslant m$ be large enough that $2 ^ { k - 1 } \geqslant s / \epsilon$ . Then $s / 2 ^ { k } \leqslant \epsilon / 2$ , and hence

$$
k + \frac {s}{2 ^ {k}} n \leqslant \epsilon n \tag {5}
$$

whenever $k / n \leqslant \epsilon / 2$ , i.e. for all $n \geqslant 2 k / \epsilon$ .

Let us now choose $M$ . This should be an upper bound on the number of (non-exceptional) sets in our partition after up to $s$ iterations of Lemma 7.4.4, where in each iteration this number may grow from its current value $r$ to at most $r 4 ^ { \prime }$ . So let $f$ be the function $x \mapsto x 4 ^ { x }$ , and take ${ \cal M } : = \operatorname* { m a x } \left\{ f ^ { s } ( k ) , 2 k / \epsilon \right\}$ ; the second term in the maximum ensures that any $n \geqslant M$ is large enough to satisfy (5).

We finally have to show that every graph $G = ( V , E )$ of order at least $m$ has an $\epsilon$ -regular partition $\{ V _ { 0 } , V _ { 1 } , \ldots , V _ { k } \}$ with $m \leqslant k \leqslant M$ . So let $G$ be given, and let $n : = | G |$ . If $n \leqslant M$ , we partition $G$ into $k : = n$ singletons, choosing $V _ { 0 } : = \emptyset$ and $| V _ { 1 } | = \ldots = | V _ { k } | = 1$ . This partition of

$G$ is clearly $\epsilon$ -regular. Suppose now that $n > M$ . Let $C _ { 0 } \subseteq V$ be minimal such that $k$ divides $| V \setminus C _ { 0 } |$ , and let $\{ C _ { 1 } , \ldots , C _ { k } \}$ be any partition of $V \setminus C _ { 0 }$ into sets of equal size. Then $| C _ { 0 } | < k$ , and hence $| C _ { 0 } | \leqslant \epsilon n$ by (5). Starting with $\{ C _ { 0 } , C _ { 1 } , \ldots , C _ { k } \}$ we apply Lemma 7.4.4 again and again, until the partition of $G$ obtained is $\epsilon$ -regular; this will happen after at most $s$ iterations, since by (5) the size of the exceptional set in the partitions stays below $\epsilon n$ , so the lemma could indeed be reapplied up to the theoretical maximum of $s$ times. 

# 7.5 Applying the regularity lemma

The purpose of this section is to illustrate how the regularity lemma is typically applied in the context of (dense) extremal graph theory. Suppose we are trying to prove that a certain edge density of a graph $G$ suffices to force the occurrence of some given subgraph $H$ , and that we have an $\epsilon$ -regular partition of $G$ . For most of the pairs $( V _ { i } , V _ { j } )$ of partition sets, the edges between $V _ { i }$ and $V _ { j }$ are distributed fairly uniformly; their density, however, may depend on the pair. But since $G$ has many edges, this density cannot be zero for all the pairs: some sizeable proportion of the pairs will have positive density. Now if $G$ is large, then so are the pairs: recall that the number of partition sets is bounded, and they have equal size. But any large enough bipartite graph with equal partition sets, fixed positive edge density (however small) and a uniform distribution of edges will contain any given bipartite subgraph8—this will be made precise below. Thus if enough pairs in our partition of $G$ have positive density that $H$ can be written as the union of bipartite graphs each arising in one of those pairs, we may hope that $H \subseteq G$ as desired.

These ideas will be formalized by Lemma 7.5.2 below. We shall then use this and the regularity lemma to prove the Erd˝os-Stone theorem from Section 7.1; another application will be given later, in the proof of Theorem 9.2.2. We wind up the section with an informal review of the application of the regularity lemma that we have seen, summarizing what it can teach us for similar applications. In particular, we look at how the various parameters involved depend on each other, and in which order they have to be chosen to make the lemma work.

Let us begin by noting a simple consequence of the $\epsilon$ -regularity of a pair $( A , B )$ . For any subset $Y \subseteq B$ that is not too small, most vertices of $A$ have about the expected number of neighbours in $Y$ :

Lemma 7.5.1. Let $( A , B )$ be an $\epsilon$ -regular pair, of density $d$ say, and let $Y \subseteq B$ have size $| Y | \geqslant \epsilon | B |$ . Then all but fewer than $\epsilon \left| A \right|$ of the vertices in $A$ have (each) at least $( d - \epsilon ) | Y |$ neighbours in $Y$ .

Proof . Let $X \subseteq A$ be the set of vertices with fewer than $( d - \epsilon ) | Y |$ neighbours in $Y$ . Then $\| X , Y \| < | X | ( d - \epsilon ) | Y |$ , so

$$
d (X, Y) = \frac {\| X , Y \|}{| X | | Y |} <   d - \epsilon = d (A, B) - \epsilon .
$$

As $( A , B )$ is $\epsilon$ -regular and $| Y | \geqslant \epsilon | B |$ , this implies that $| X | < \epsilon | A |$ .

$R$   
regularity graph   
$V _ { i } ^ { s }$   
$R _ { s }$

Let $G$ be a graph with an $\epsilon$ -regular partition $\{ V _ { 0 } , V _ { 1 } , \ldots , V _ { k } \}$ , with exceptional set $V _ { 0 }$ and $| V _ { 1 } | = \ldots = | V _ { k } | = : \ell$ . Given $d \in [ 0 , 1 ]$ , let $R$ 2 be the graph on $\{ V _ { 1 } , \ldots , V _ { k } \}$ in which two vertices $V _ { i } , V _ { j }$ are adjacent if and only if they form an $\epsilon$ -regular pair in $G$ of density $\geqslant d$ . We shall call $R$ a regularity graph of $G$ with parameters $\epsilon$ , $\ell$ and $d$ . Given $s \in \mathbb N$ , let us now replace every vertex $V _ { i }$ of $R$ by a set $V _ { i } ^ { s }$ of $s$ vertices, and every edge by a complete bipartite graph between the corresponding $s$ -sets. The resulting graph will be denoted by $R _ { s }$ . (For $R = K ^ { r }$ , for example, we have $R _ { s } = K _ { s } ^ { \prime }$ . )

The following lemma says that subgraphs of $R _ { s }$ can also be found in $G$ , provided that $d > 0$ , that $\epsilon$ is small enough, and that the $V _ { i }$ are large enough. In fact, the values of $\epsilon$ and $\ell$ required depend only on ( $d$ and) the maximum degree of the subgraph:

[ 9.2.2 ]

Lemma 7.5.2. For all $d \in ( 0 , 1 ]$ and $\Delta \geqslant 1$ there exists an $\epsilon _ { 0 } > 0$ with the following property: if $G$ is any graph, $H$ is a graph with $\Delta ( H ) \leqslant \Delta$ , $s \in \mathbb { N }$ , and $R$ is any regularity graph of $G$ with parameters $\epsilon \ \leqslant \ \epsilon _ { 0 }$ , $\ell \geqslant 2 s / d ^ { \Delta }$ and $d$ , then

$$
H \subseteq R _ {s} \Rightarrow H \subseteq G.
$$

$d , \Delta , \epsilon _ { 0 }$

Proof . Given $d$ and $\Delta$ , choose $\epsilon _ { 0 } > 0$ small enough that $\epsilon _ { 0 } < d$ and

$$
\left(d - \epsilon_ {0}\right) ^ {\Delta} - \Delta \epsilon_ {0} \geqslant \frac {1}{2} d ^ {\Delta}; \tag {1}
$$

$G , H , R , R _ { s }$   
$V _ { i }$   
$\epsilon , k , \ell$   
$u _ { i } , h$   
σ   
$_ { v _ { i } }$

such a choice is possible, since $( d - \epsilon ) ^ { \Delta } - \Delta \epsilon \to d ^ { \Delta }$ as $\epsilon \to 0$ . Now let $G$ , $H$ , $s$ and $R$ be given as stated. Let $\{ V _ { 0 } , V _ { 1 } , \ldots , V _ { k } \}$ be the $\epsilon$ -regular partition of $G$ that gave rise to $R$ ; thus, ${ \epsilon } \leqslant { \epsilon } _ { 0 } , ~ V ( R ) = \{ V _ { 1 } , \ldots , V _ { k } \}$ and $| V _ { 1 } | = \ldots = | V _ { k } | = \ell \geqslant 2 s / d ^ { \Delta }$ . Let us assume that $H$ is actually a subgraph of $R _ { s }$ (not just isomorphic to one), with vertices $u _ { 1 } , \ldots , u _ { h }$ say. Each vertex $u _ { i }$ lies in one of the $s$ -sets $V _ { j } ^ { s }$ of $R _ { s }$ , which defines a map $\sigma \colon i \mapsto j$ . Our aim is to define an embedding $u _ { i } \mapsto v _ { i } \in V _ { \sigma ( i ) }$ of $H$ in $G$ as a subgraph; thus, $v _ { 1 } , \ldots , v _ { h }$ will be distinct, and $v _ { i } v _ { j }$ will be an edge of $G$ whenever $u _ { i } u _ { j }$ is an edge of $H$ .

Our plan is to choose the vertices $v _ { 1 } , \ldots , v _ { h }$ inductively. Throughout the induction, we shall have a ‘target set’ $Y _ { i } \subseteq V _ { \sigma ( i ) }$ assigned to each $u _ { i }$ ; this contains the vertices that are still candidates for the choice of $v _ { i }$ . Initially, $Y _ { i }$ is the entire set $V _ { \sigma ( i ) }$ . As the embedding proceeds, $Y _ { i }$ will get smaller and smaller (until it collapses to $\{ v _ { i } \}$ when $v _ { i }$ is chosen): whenever we choose a vertex $v _ { j }$ with $j < i$ and $u _ { j } u _ { i } \in E ( H )$ , we delete all those vertices from $Y _ { i }$ that are not adjacent to $v _ { j }$ . The set $Y _ { i }$ thus evolves as

$$
V _ {\sigma (i)} = Y _ {i} ^ {0} \supseteq \dots \supseteq Y _ {i} ^ {i} = \left\{v _ {i} \right\},
$$

where $Y _ { i } ^ { j }$ denotes the version of $Y _ { i }$ current after the definition of $v _ { j }$ and the resulting deletion of vertices from Y j−1i . $Y _ { i } ^ { j - 1 }$

In order to make this approach work, we have to ensure that the target sets $Y _ { i }$ do not get too small. When we come to embed a vertex $u _ { j }$ , we consider all the indices $i > j$ with $u _ { j } u _ { i } \in E ( H )$ ; there are at most $\Delta$ such $i$ . For each of these $i$ , we wish to select $v _ { j }$ so that

$$
Y _ {i} ^ {j} = N \left(v _ {j}\right) \cap Y _ {i} ^ {j - 1} \tag {2}
$$

is still relatively large: smaller than $Y _ { i } ^ { \jmath - 1 }$ by no more than a constant factor such as $( d - \epsilon )$ . Now this can be done by Lemma 7.5.1 (with $A = V _ { \sigma ( j ) } , \ B = V _ { \sigma ( i ) }$ and $Y = Y _ { i } ^ { j - 1 }$ ): provided that $Y _ { i } ^ { j - 1 }$ still has size at least $\boldsymbol { \epsilon } \boldsymbol { \ell }$ (which induction will ensure), all but at most $\epsilon { \ell }$ choices of $v _ { j }$ will be such that the new set $Y _ { i } ^ { j }$ as in (2) satisfies

$$
\left| Y _ {i} ^ {j} \right| \geqslant (d - \epsilon) \left| Y _ {i} ^ {j - 1} \right|. \tag {3}
$$

Excluding the bad choices for $v _ { j }$ for all the relevant values of $i$ simultaneously, we find that all but at most $\Delta \epsilon \ell$ choices of $v _ { j }$ from $V _ { \sigma ( j ) }$ , and in particular from $Y _ { j } ^ { j - 1 } \subseteq V _ { \sigma ( j ) }$ , satisfy (3) for all $i$ .

j It remains to show that the sets $Y _ { i } ^ { j - 1 }$ considered above as $Y$ for Lemma 7.5.1 never fall below the size of $\epsilon { \ell }$ , and that when we come to select $v _ { j } \in Y _ { j } ^ { j - 1 }$ we have a choice of at least $s$ suitable candidates: since before $u _ { j }$ at most $s - 1$ vertices $u$ were given an image in $V _ { \sigma ( j ) }$ , we can then choose $v _ { j }$ distinct from these.

But all this follows from our choice of $\epsilon _ { \mathrm { 0 } }$ . Indeed, the initial target sets $Y _ { i } ^ { 0 }$ have size $\ell$ , and each $Y _ { i }$ shrinks at most $\Delta$ times by a factor of $( d - \epsilon )$ when some $v _ { j }$ with $j < i$ and $u _ { j } u _ { i } \in E ( H )$ is defined. Thus,

$$
|Y_{i}^{j - 1}| - \Delta \epsilon \ell \geqslant (d - \epsilon)^{\Delta}\ell -\Delta \epsilon \ell \geqslant (d - \epsilon_{0})^{\Delta}\ell -\Delta \epsilon_{0}\ell \geqslant \frac{1}{2} d^{\Delta}\ell \geqslant s
$$

for all $j \leqslant i$ ; in particular, we have $\lvert Y _ { i } ^ { j - 1 } \rvert \geqslant \epsilon \ell$ and $| Y _ { j } ^ { j - 1 } | - \Delta \epsilon \ell \geqslant s$ as desired. 

We are now ready to prove the Erd˝os-Stone theorem.

(7.1.1) (7.1.4)   
$r , s$ $\gamma$

$\| G \|$

$$
\left\| G \right\| \geqslant t _ {r - 1} (n) + \gamma n ^ {2}
$$

edges, then $\gamma < 1$ . We want to show that $K _ { s } ^ { r } \subseteq G$ if $n$ is large enough.

Our plan is to use the regularity lemma to show that $G$ has a regularity graph $R$ dense enough to contain a $K ^ { r }$ by Tur´an’s theorem. Then $R _ { s }$ contains a $K _ { s } ^ { r }$ , so we may hope to use Lemma 7.5.2 to deduce that $K _ { s } ^ { r } \subseteq G$ .

On input $d : = \gamma$ and $\Delta : = \Delta ( K _ { s } ^ { r } )$ Lemma 7.5.2 returns an $\epsilon _ { 0 } > 0$ . To apply the regularity lemma, let $m > 1 / \gamma$ and choose $\epsilon > 0$ small enough that $\epsilon \leqslant \epsilon _ { 0 }$ ,

$$
\epsilon <   \gamma / 2 <   1, \tag {1}
$$

and

$$
\delta := 2 \gamma - \epsilon^ {2} - 4 \epsilon - d - \frac {1}{m} > 0;
$$

$\delta$   
this is possible, since $\textstyle 2 \gamma - d - { \frac { 1 } { m } } > 0$ . On input $\epsilon$ and $m$ , the regularity lemma returns an integer $M$ . Let us assume that   
$M$

$_ n$

$k$   
$\ell$

$$
n \geqslant \frac {2 M s}{d ^ {\Delta} (1 - \epsilon)}.
$$

Since this number is at least $m$ , the regularity lemma provides us with an $\epsilon$ -regular partition $\{ V _ { 0 } , V _ { 1 } , \ldots , V _ { k } \}$ of $G$ , where $m \leqslant k \leqslant M$ ; let $| V _ { 1 } | = \ldots = | V _ { k } | = : \ell$ . Then

$$
n \geqslant k \ell , \tag {2}
$$

and

$$
\ell = \frac {n - | V _ {0} |}{k} \geqslant \frac {n - \epsilon n}{M} = n \frac {1 - \epsilon}{M} \geqslant \frac {2 s}{d ^ {\Delta}}
$$

$R$

by the choice of $n$ . Let $R$ be the regularity graph of $G$ with parameters $\epsilon , \ell , d$ corresponding to the above partition. Then Lemma 7.5.2 will imply $K _ { s } ^ { r } \subseteq G$ as desired if $K ^ { r } \subseteq R$ (and hence $K _ { s } ^ { r } \subseteq R _ { s }$ ).

Our plan was to show $K ^ { r } \subseteq R$ by Tur´an’s theorem. We thus have to check that $R$ has enough edges, i.e. that enough $\epsilon$ -regular pairs $( V _ { i } , V _ { j } )$ have density at least $d$ . This should follow from our assumption that $G$ has at least $t _ { r - 1 } ( n ) + \gamma n ^ { 2 }$ edges, i.e. an edge density of about $\textstyle { \frac { r - 2 } { r - 1 } } + 2 \gamma$ : this lies substantially above the approximate density of $\textstyle { \frac { r - 2 } { r - 1 } }$ of the Tur´an graph $T ^ { r - 1 } ( k )$ , and hence substantially above any density that $G$ could derive from $t _ { r - 1 } ( k )$ dense pairs alone, even if all these had density 1.

Let us then estimate $\| R \|$ more precisely. How many edges of $G$ lie outside $\epsilon$ -regular pairs? At most $\binom { | V _ { 0 } | } { 2 }$ edges lie inside $V _ { 0 }$ , and by condition (i) in the definition of $\epsilon$ -regularity these are at most $\textstyle { \frac { 1 } { 2 } } ( \epsilon n ) ^ { 2 }$ 2 edges. At most $| V _ { 0 } | k \ell \leqslant \epsilon n k \ell$ edges join $V _ { 0 }$ to other partition sets. The at most $\epsilon k ^ { 2 }$ other pairs $( V _ { i } , V _ { j } )$ that are not $\epsilon$ -regular contain at most $\ell ^ { 2 }$ edges each, together at most $\epsilon k ^ { 2 } \ell ^ { 2 }$ . The $\epsilon$ -regular pairs of insufficient density $( < d )$ each contain no more than $d \ell ^ { 2 }$ edges, altogether at most $\scriptstyle { \frac { 1 } { 2 } } k ^ { 2 } d \ell ^ { 2 }$ edges. Finally, there are at most $\binom { \ell } { 2 }$ edges inside each of the partition sets $V _ { 1 } , \ldots , V _ { k }$ , together at most $\scriptstyle { \frac { 1 } { 2 } } \ell ^ { 2 } k$ edges. All other edges of $G$ lie in $\epsilon$ -regular pairs of density at least $d$ , and thus contribute to edges of $R$ . Since each edge of $R$ corresponds to at most $\ell ^ { 2 }$ edges of $G$ , we thus have in total

$$
\| G \| \leq \frac {1}{2} \epsilon^ {2} n ^ {2} + \epsilon n k \ell + \epsilon k ^ {2} \ell^ {2} + \frac {1}{2} k ^ {2} d \ell^ {2} + \frac {1}{2} \ell^ {2} k + \| R \| \ell^ {2}.
$$

Hence, for all sufficiently large $n$

$$
\begin{array}{l} \| R \| \geq \frac {1}{2} k ^ {2} \frac {\| G \| - \frac {1}{2} \epsilon^ {2} n ^ {2} - \epsilon n k \ell - \epsilon k ^ {2} \ell^ {2} - \frac {1}{2} d k ^ {2} \ell^ {2} - \frac {1}{2} k \ell^ {2}}{\frac {1}{2} k ^ {2} \ell^ {2}} \\ \geq \frac {1}{2} k ^ {2} \left(\frac {t _ {r - 1} (n) + \gamma n ^ {2} - \frac {1}{2} \epsilon^ {2} n ^ {2} - \epsilon n k \ell}{n ^ {2} / 2} - 2 \epsilon - d - \frac {1}{k}\right) \\ \geq \frac {1}{2} k ^ {2} \left(\frac {t _ {r - 1} (n)}{n ^ {2} / 2} + 2 \gamma - \epsilon^ {2} - 4 \epsilon - d - \frac {1}{m}\right) \\ = \frac {1}{2} k ^ {2} \left(t _ {r - 1} (n) \binom {n} {2} ^ {- 1} \left(1 - \frac {1}{n}\right) + \delta\right) \\ > \frac {1}{2} k ^ {2} \frac {r - 2}{r - 1} \\ \geqslant t _ {r - 1} (k). \\ \end{array}
$$

(The strict inequality follows from Lemma 7.1.4.) Therefore $K ^ { r } \subseteq R$ by Theorem 7.1.1, as desired. 

Having seen a typical application of the regularity lemma in full detail, let us now step back and try to separate the wheat from the chaff: what were the main ideas, how do the various parameters depend on each other, and in which order were they chosen?

The task was to show that $\gamma n ^ { 2 }$ more edges than can be accommodated on $n$ vertices without creating a $K ^ { r }$ force a $K _ { s } ^ { \tau }$ subgraph, provided that $G$ is large enough. The plan was to do this using Lemma 7.5.2, which asks for the input of two parameters: $d$ and $\Delta$ . As we wish to find a copy of $H = K _ { s } ^ { r }$ i n $G$ , it is clear that we must choose $\Delta : = \Delta ( K _ { s } ^ { r } )$ . We shall return to the question of how to choose $d$ in a moment.

Given $d$ and $\Delta$ , Lemma 7.5.2 tells us how small we must choose $\epsilon$ to make the regularity lemma provide us with a suitable partition. The regularity lemma also requires the input of a lower bound $m$ for the number of partition classes; we shall discuss this below, together with $d$ .

All that remains now is to choose $G$ large enough that the partition classes have size at least $2 s / d ^ { \Delta }$ , as required by Lemma 7.5.2. (The $s$ S here depends on the graph $H$ we wish to embed, and $s : = | H |$ would certainly be big enough. In our case, we can use the $s$ from our $H = K _ { s } ^ { r }$ .) How large is ‘large enough’ for $| G |$ follows straight from the upper bound $M$ on the number of partition classes returned by the regularity lemma: roughly, i.e. disregarding $V _ { 0 }$ , an assumption of $| G | \geqslant 2 M s / d ^ { \Delta }$ suffices.

So far, everything was entirely straightforward, and standard for any application of the regularity lemma of this kind. But now comes the interesting bit, the part specific to this proof: the observation that, if only $d$ is small enough, our $\gamma n ^ { 2 }$ ‘additional edges’ force an ‘additional dense $\epsilon$ -regular pair’ of partition sets, giving us more than $t _ { r - 1 } ( k )$ dense $\epsilon$ -regular pairs in total (where ‘dense’ means ‘of density at least $d ^ { \prime }$ ), thus forcing $R$ to contain a $K ^ { r }$ and hence $R _ { s }$ to contain a $K _ { s } ^ { r }$ .

Let us examine why this is so. Suppose we have at most $t _ { r - 1 } ( k )$ dense $\epsilon$ -regular pairs . Inside these, $G$ has at most

$$
\frac {1}{2} k ^ {2} \frac {r - 2}{r - 1} \ell^ {2} \leqslant \frac {1}{2} n ^ {2} \frac {r - 2}{r - 1}
$$

edges, even if we use those pairs to their full capacity of $\ell ^ { 2 }$ edges each (where $\ell$ is again the common size of the partition sets other than $V _ { 0 }$ , so that $k \ell$ is nearly $n$ ). Thus, we have almost exactly our $\gamma n ^ { 2 }$ additional edges left to accommodate elsewhere in the graph: either in $\epsilon$ -regular pairs of density less than $d$ , or in some exceptional way, i.e. in irregular pairs, inside a partition set, or with an end in $V _ { 0 }$ . Now the number of edges in low-density $\epsilon$ -regular pairs is less than

$$
\frac {1}{2} k ^ {2} d \ell^ {2} \leqslant \frac {1}{2} d n ^ {2},
$$

and hence less than half of our extra edges if $d \leqslant \gamma$ . The other half, the remaining ${ \scriptstyle { \frac { 1 } { 2 } } } \gamma n ^ { 2 }$ edges, are more than can be accommodated in exceptional ways, provided we choose $m$ large enough and $\epsilon$ small enough (giving an additional upper bound for $\epsilon$ ). It is now a routine matter to compute the values of $m$ and $\epsilon$ that will work.

# Exercises

1.− Show that $K _ { 1 , 3 }$ is extremal without a $P ^ { 3 }$   
2.− Given $k > 0$ , determine the extremal graphs of chromatic number at most $k$ .   
3. Determine the value of $\mathrm { e x } ( n , K _ { 1 , r } )$ for all $r , n \in \mathbb { N }$ .   
4. Is there a graph that is edge-maximal without a $K ^ { 3 }$ minor but not extremal?   
5.+ Given $k > 0$ , determine the extremal graphs without a matching of size $k$ .

(Hint. Theorem 2.2.3 and Ex. 15, Ch. 2.)

6. Without using Tur´an’s theorem, show that the maximum number of edges in a triangle-free graph of order $n > 1$ is $\lfloor n ^ { 2 } / 4 \rfloor$ .

7. Show that

$$
t _ {r - 1} (n) \leqslant \frac {1}{2} n ^ {2} \frac {r - 2}{r - 1},
$$

with equality whenever $r - 1$ divides $n$ .

8. Show that $t _ { r - 1 } ( n ) / { \binom { n } { 2 } }$ converges to $( r - 2 ) / ( r - 1 )$ as $n \longrightarrow \infty$   
(Hint. $\begin{array} { r } { t _ { r - 1 } \big ( ( r - 1 ) \lfloor \frac { n } { r - 1 } \rfloor \big ) \leqslant t _ { r - 1 } ( n ) \leqslant t _ { r - 1 } \big ( ( r - 1 ) \lceil \frac { n } { r - 1 } \rceil \big ) . } \end{array}$ .)   
9. Show that deleting at most $( m - s ) ( n - t ) / s$ edges from a $K _ { m , n }$ will never destroy all its $K _ { s , t }$ subgraphs.   
10. For $0 < s \leqslant t \leqslant n$ let $z ( n , s , t )$ denote the maximum number of edges in a bipartite graph whose partition sets both have size $_ n$ , and which does not contain a $K _ { s , t }$ . Show that $2 \exp ( n , K _ { s , t } ) \le z ( n , s , t ) \le \exp ( 2 n , K _ { s , t } )$ .   
11.+ Let $1 \leqslant r \leqslant n$ be integers. Let $G$ be a bipartite graph with bipartition $\{ A , B \}$ , where $| A | = | B | = n$ , and assume that $K _ { r , r } \ \nsubseteq { G }$ . Show that

$$
\sum_ {x \in A} \binom {d (x)} {r} \leqslant (r - 1) \binom {n} {r}.
$$

Using the previous exercise, deduce that $\exp ( n , K _ { r , r } ) \leqslant c n ^ { 2 - 1 / r }$ for some constant $c$ depending only on $r$ .

12. The upper density of an infinite graph $G$ is the infimum of all reals $\alpha$ such that the finite graphs $H \subseteq G$ with $\| H \| \binom { | H | } { 2 } ^ { - 1 } > \alpha$ have bounded order. Use the Erd˝os-Stone theorem totakes one of the countably many values $\quad 0 , 1 , { \frac { 1 } { 2 } } , { \frac { 2 } { 3 } } , { \frac { 3 } { 4 } } , \ldots .$ number always.   
13. Given a tree $T$ , find an upper bound for $\exp ( n , T )$ that is linear in $n$ and independent of the structure of $T$ , i.e. depends only on $| T |$ .   
14. Show that, as a general bound for arbitrary $n$ , the bound on $\exp ( n , T )$ claimed by the Erd˝os-S´os conjecture is best possible for every tree $T$ . Is it best possible even for every $n$ and every $T$ ?

15.− Prove the Erd˝os-S´os conjecture for the case when the tree considered is a star.   
16. Prove the Erd˝os-S´os conjecture for the case when the tree considered is a path.

(Hint. Use Exercise 7 of Chapter 1.)

17.+ For which trees $T$ is there a function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ tending to infinity, such that every graph $G$ with $\chi ( G ) < f ( d ( G ) )$ contains an induced copy of $T$ ? (In other words: can we force the chromatic number up by raising the average degree, as long as $T$ does not occur as an induced subgraph? Or, as in Gy´arf´as’s conjecture: will a large average degree force an induced copy of $T$ if the chromatic number is kept small?)   
18. Given two graph invariants $i _ { 1 }$ and $i _ { 2 }$ , write $i _ { 1 } ~ \leqslant ~ i _ { 2 }$ if we can force $i _ { 2 }$ arbitrarily high on a subgraph of $G$ by making $i _ { 1 } ( G )$ large enough. (Formally: write $i _ { 1 } \leqslant i _ { 2 }$ if there exists a function $f { : \mathbb { N } \to \mathbb { N } }$ such that, given any $k \in \mathbb N$ , every graph $G$ with $i _ { 1 } ( G ) \ \geqslant \ f ( k )$ has a subgraph $H$ with $i _ { 2 } ( H ) \geqslant k$ .) If $i _ { 1 } \leqslant i _ { 2 }$ as well as $i _ { 1 } \geqslant i _ { 2 }$ , write $i _ { 1 } \sim i _ { 2 }$ . Show that this is an equivalence relation for graph invariants, and sort the following invariants into equivalence classes ordered by $<$ <: minimum degree; average degree; connectivity; arboricity; chromatic number; colouring number; choice number; max $\{ r \mid K ^ { r } \subseteq G \}$ ; max $\{ r \mid T K ^ { r } \subseteq G \}$ ; $\operatorname* { m a x } \{ r \mid K ^ { r } \preccurlyeq G \}$ ; min max $d ^ { + } ( v )$ , where the maximum is taken over all vertices $v$ of the graph, and the minimum over all its orientations.   
19.+ Prove, from first principles, the theorem of Wagner (1964) that every graph of chromatic number at least $2 ^ { r }$ contains $K ^ { \prime }$ as a minor.

(Hint. Use induction on $r$ . For the induction step, contract a connected subgraph chosen so that the remaining graph still needs at least half as many colours as the given graph.)

20. Let $G$ be a graph of average degree at least $2 ^ { r - 2 }$ . By considering the neighbourhood of a vertex in a minimal minor $H \preccurlyeq G$ with $\varepsilon ( H ) \geqslant \varepsilon ( G )$ , prove Mader’s (1967) theorem that $G \succcurlyeq K ^ { \prime }$ .   
21. $-$ Derive Wagner’s theorem (Ex. 19) from Mader’s theorem (Ex. 20).   
22.+ Given a graph $G$ with $\varepsilon ( G ) \geqslant k \in \mathbb { N }$ , find a minor $H \preccurlyeq G$ such that both $\delta ( H ) \geqslant k$ and $\delta ( H ) \geqslant | H | / 2$ .   
23.+ Find a constant $c$ such that every graph with $_ n$ vertices and at least $n + 2 k ( \log k + \log \log k + c )$ edges contains $k$ edge-disjoint cycles (for all $k \in \mathbb N$ ). Deduce an edge-analogue of the Erd˝os-P´osa theorem (2.3.2). (Hint. Assuming $\delta \geqslant 3$ , delete the edges of a short cycle and apply induction. The calculations are similar to the proof of Lemma 2.3.1.)   
24.− Use Exercise 22 of Chapter 3 to reduce the constant $c$ in Theorem 7.2.1 from 10 to 5.   
25.+ Show that any function $h$ as in Lemma 3.5.1 satisfies the inequality $\begin{array} { r } { h ( r ) > \frac { 1 } { 8 } r ^ { 2 } } \end{array}$ for all even $r$ , and hence that Theorem 7.2.1 is best possible up to the value of the constant $c$ .

26. Characterize the graphs with $n$ vertices and more than $3 n - 6$ edges that contain no $T K _ { 3 , 3 }$ . In particular, determine $\exp ( n , T K _ { 3 , 3 } )$ .

(Hint. By a theorem of Wagner, every edge-maximal graph without a $K _ { 3 , 3 }$ minor can be constructed recursively from maximal planar graphs and copies of $K ^ { 5 }$ by pasting along $K ^ { 2 } { \mathfrak { s } }$ s.)

27. $-$ Derive the four colour theorem from Hadwiger’s conjecture for $r = 5$ .   
28. $-$ Show that Hadwiger’s conjecture for $r + 1$ implies the conjecture for $r$ .   
29. $-$ Prove the following weakening of Hadwiger’s conjecture: given any $\epsilon > 0$ , every graph of chromatic number at least $r ^ { 1 + \epsilon }$ has a $K ^ { \prime }$ minor, provided that $r$ is large enough.   
30. Show that any graph constructed as in Proposition 7.3.1 is edgemaximal without a $K ^ { 4 }$ minor.   
31. Prove the implication $\delta ( G ) \geqslant 3 \Rightarrow G \supseteq T K ^ { 4 }$ .

(Hint. Proposition 7.3.1.)

32. A multigraph is called series-parallel if it can be constructed recursively from a $K ^ { 2 }$ by the operations of subdividing and of doubling edges. Show that a 2-connected multigraph is series-parallel if and only if it has no (topological) $K ^ { 4 }$ minor.   
33. Without using Theorem 7.3.8, prove Hadwiger’s conjecture for all graphs of girth at least 11 and $r$ large enough. Without using Corollary 7.3.9, show that there is a constant $g \in \mathbb { N }$ such that all graphs of girth at least $g$ satisfy Hadwiger’s conjecture, irrespective of $r$ .   
34.+ Prove Hadwiger’s conjecture for $r = 4$ from first principles.   
35.+ Prove Hadwiger’s conjecture for line graphs.   
36. Prove Corollary 7.3.5.   
37. $-$ In the definition of an $\epsilon$ -regular pair, what is the purpose of the requirement that $| X | > \epsilon | A |$ and $| Y | > \epsilon | B |$ ?   
38. $-$ Show that any $\epsilon$ -regular pair in $G$ is also $\epsilon$ -regular in $\overline { { G } }$   
39. Consider a partition $\{ V _ { 1 } \ldots V _ { k } \}$ of a finite set $V$ . Show that the complete graph on $V$ has about $k - 1$ as many edges between different partition sets as edges inside partition sets. Explain how this leads to the choice of $m : = 1 / \gamma$ in the proof of the Erd˝os-Stone theorem.   
40. (i) Deduce the regularity lemma from the assumption that it holds, given $\epsilon > 0$ and $m \geqslant 1$ , for all graphs of order at least some $n = n ( \epsilon , m )$ .   
(ii) Prove the regularity lemma for sparse graphs, that is, for every sequence $( G _ { n } ) _ { n } \in \mathbb { N }$ of graphs $G _ { n }$ of order $n$ such that $\| G _ { n } \| / n ^ { 2 } \to 0$ as $n \longrightarrow \infty$ .

# Notes

The standard reference work for results and open problems in extremal graph theory (in a very broad sense) is still B. Bollob´as, Extremal Graph Theory, Academic Press 1978. A kind of update on the book is given by its author in his chapter of the Handbook of Combinatorics (R.L. Graham, M. Gr¨otschel & L. Lov´asz, eds.), North-Holland 1995. An instructive survey of extremal graph theory in the narrower sense of Section 7.1 is given by M. Simonovits in (L.W. Beineke & R.J. Wilson, eds.) Selected Topics in Graph Theory 2, Academic Press 1983. This paper focuses among other things on the particular role played by the Tur´an graphs. A more recent survey by the same author can be found in (R.L. Graham & J. Neˇsetˇril, eds.) The Mathematics of Paul Erd˝os, Vol. 2, Springer 1996.

Tur´an’s theorem is not merely one extremal result among others: it is the result that sparked off the entire line of research. Our first proof of Tur´an’s theorem is essentially the original one; the second is a version of a proof of Zykov due to Brandt.

Our version of the Erd˝os-Stone theorem is a slight simplification of the original. A direct proof, not using the regularity lemma, is given in L. Lov´asz, Combinatorial Problems and Exercises (2nd edn.), North-Holland 1993. Its most fundamental application, Corollary 7.1.3, was only found 20 years after the theorem, by Erd˝os and Simonovits (1966).

Of our two bounds on $\mathrm { e x } ( n , K _ { r , r } )$ the upper one is thought to give the correct order of magnitude. For vastly off-diagonal complete bipartite graphs this was verified by J. Koll´ar, L. R´onyai & T. Szab´o, Norm-graphs and bipartite Tur´an numbers, Combinatorica 16 (1996), 399–406, who proved that $\mathrm { e x } ( n , K _ { r , s } ) \geqslant c _ { r } n ^ { 2 - \frac { 1 } { r } }$ when $s > r !$ .

Details about the Erd˝os-S´os conjecture, including an approximate solution for large $k$ , can be found in the survey by Koml´os and Simonovits cited below. The case where the tree $T$ is a path (Exercise 16) was proved by Erd˝os & Gallai in 1959. It was this result, together with the easy case of stars (Exercise 15) at the other extreme, that inspired the conjecture as a possible unifying result.

Theorem 7.2.1 was first proved by B. Bollob´as & A.G. Thomason, Proof of a conjecture of Mader, Erd˝os and Hajnal on topological complete subgraphs, Europ. J. Combinatorics 19 (1998), 883–887, and independently by J. Koml´os & E. Szemer´edi, Topological cliques in graphs II, Combinatorics, Probability and Computing 5 (1996), 79–90. For large $G$ , the latter authors show that the constant $c$ in the theorem can be brought down to about $\textstyle { \frac { 1 } { 2 } }$ , which is not far from the lower bound of $\frac { 1 } { 8 }$ given in Exercise 25.

Theorem 7.2.2 was first proved in 1982 by Kostochka, and in 1984 with a better constant by Thomason. For references and more insight also in these early proofs, see A.G. Thomason, The extremal function for complete minors, J. Combin. Theory $B$ 81 (2001), 318–338, where he determines the value of $\alpha$ . Surprisingly, the average degree needed to force an incomplete minor $H$ remains at $c r { \sqrt { \log r } }$ , with $c = \alpha \sqrt { \epsilon } + o ( 1 )$ for almost all $H$ with $r$ vertices and $r ^ { 1 + \epsilon }$ edges, for every fixed $\epsilon \in \mathsf { \Gamma } ( 0 , 1 )$ ; see J.S. Myers & A.G. Thomason, The extremal function for noncomplete minors, Combinatorica (to appear).

As Theorem 7.2.2 is best possible, there is no constant $c$ such that all

graphs of average degree at least $c r$ have a $K ^ { \prime }$ minor. Strengthening this assumption to $\kappa \geqslant c r$ , however, can force a $K ^ { \prime }$ minor in all large enough graphs; this was proved by T. B¨ohme, K. Kawarabayashi, J. Maharry and B. Mohar, Linear connectivity forces large complete bipartite minors, preprint 2004.

The fact that large enough girth can force minors of arbitrarily high minimum degree, and hence large complete minors, was discovered by Thomassen in 1983. The reference can be found in W. Mader, Topological subgraphs in graphs of large girth, Combinatorica 18 (1998), 405–412, from which our Lemma 7.2.3 is extracted. Our girth assumption of $8 k + 3$ has been reduced to about $4 k$ by D. K¨uhn and D. Osthus, Minors in graphs of large girth, Random Struct. Alg. 22 (2003), 213–225, which is conjectured to be best possible.

The original reference for Theorem 7.2.5 can be found in D. K¨uhn and D. Osthus, Improved bounds for topological cliques in graphs of large girth (preprint 2005), where they re-prove their theorem with $g \ \leqslant \ 2 7$ . See also D. K¨uhn & D. Osthus, Subdivisions of $K _ { r + 2 }$ in graphs of average degree at least $r + \varepsilon$ and large but constant girth, Combinatorics, Probability and Computing 13 (2004), 361–371.

The proof of Hadwiger’s conjecture for $r \ = \ 4$ hinted at in Exercise 34 was given by Hadwiger himself, in the 1943 paper containing his conjecture. A counterexample to Haj´os’s conjecture was found as early as 1979 by Catlin. A little later, Erd˝os and Fajtlowicz proved that Haj´os’s conjecture is false for ‘almost all’ graphs (see Chapter 11). Proofs of Wagner’s Theorem 7.3.4 (with Hadwiger’s conjecture for $r \ = \ 5$ as a corollary) can be found in Bollob´as’s Extremal Graph Theory (see above) and in Halin’s Graphentheorie (2nd ed.), Wissenschaftliche Buchgesellschaft 1989. Hadwiger’s conjecture for $r = 6$ was proved by N. Robertson, P.D. Seymour and R. Thomas, Hadwiger’s conjecture for $K _ { 6 }$ -free graphs, Combinatorica 13 (1993), 279–361.

The investigation of graphs not containing a given graph as a minor, or topological minor, has a long history. It probably started with Wagner’s 1935 PhD thesis, in which he sought to ‘detopologize’ the four colour problem by classifying the graphs without a $K ^ { 5 }$ minor. His hope was to be able to show abstractly that all those graphs were 4-colourable; since the graphs without a $K ^ { 5 }$ minor include the planar graphs, this would amount to a proof of the four colour conjecture involving no topology whatsoever. The result of Wagner’s efforts, Theorem 7.3.4, falls tantalizingly short of this goal: although it succeeds in classifying the graphs without a $K ^ { 5 }$ minor in structural terms, planarity re-emerges as one of the criteria used in the classification. From this point of view, it is instructive to compare Wagner’s $K ^ { \mathrm { ^ { - b } } }$ theorem with similar classification theorems, such as his analogue for $K ^ { 4 }$ (Proposition 7.3.1), where the graphs are decomposed into parts from a finite set of irreducible graphs. See R. Diestel, Graph Decompositions, Oxford University Press 1990, for more such classification theorems.

Despite its failure to resolve the four colour problem, Wagner’s $K ^ { 5 }$ structure theorem had consequences for the development of graph theory like few others. To mention just two: it prompted Hadwiger to make his famous conjecture; and it inspired the notion of a tree-decomposition, which is fundamental to the work of Robertson and Seymour on minors (see Chapter 12). Wagner himself responded to Hadwiger’s conjecture with a proof that, in order to force a $K ^ { \prime }$ minor, it does suffice to raise the chromatic number of a graph to some

value depending only on $r$ (Exercise 19). This theorem, along with its analogue for topological minors proved independently by Dirac and by Jung, prompted the question which average degree suffices to force the desired minor.

Theorem 7.3.8 is a consequence of the more fundamental result of D. K¨uhn and D. Osthus, Complete minors in $K _ { s , s }$ -free graphs, Combinatorica 25 (2005) 49–64, that every graph without a $K _ { s , s }$ subgraph that has average degree $r \geqslant r _ { s }$ has a $K ^ { p }$ minor for $p = \lfloor r ^ { 1 + \frac { 1 } { 2 ( s - 1 ) } } / ( \log r ) ^ { 3 } \rfloor$ .

As in Gy´arf´as’s conjecture, one may ask under what additional assumptions large average degree forces an induced subdivision of a given graph $H$ . This was answered for arbitrary $H$ by D. K¨uhn and D. Osthus, Induced subdivisions in $K _ { s , s }$ -free graphs of large average degree, Combinatorica 24 (2004) 287–304, who proved that for all $r , s \in \mathbb { N }$ there exists $d \in \mathbb { N }$ such that every graph $\textit { G } \nsupseteq K _ { s , s }$ with $d ( G ) \geqslant d$ contains a $T K ^ { \prime }$ as an induced subgraph. See there also for the source of Gy´arf´as’s conjecture and related results.

The regularity lemma is proved in E. Szemer´edi, Regular partitions of graphs, Colloques Internationaux CNRS 260—Probl`emes Combinatoires et Th´eorie des Graphes, Orsay (1976), 399–401. Our rendering follows an account by Scott (personal communication). A broad survey on the regularity lemma and its applications is given by J. Koml´os & M. Simonovits in (D. Mikl´os, V.T. S´os & T. Sz˝onyi, eds.) Paul Erd˝os is 80, Vol. 2, Proc. Colloq. Math. Soc. J´anos Bolyai (1996); the concept of a regularity graph and Lemma 7.5.2 are taken from this paper. An adaptation of the regularity lemma for use with sparse graphs was developed independently by Kohayakawa and by R¨odl; see Y. Kohayakawa, Szemer´edi’s regularity lemma for sparse graphs, in (F. Cucker & M. Shub, eds.) Foundations of Computational Mathematics, Selected papers of a conference held at IMPA in Rio de Janeiro, January 1997, Springer 1997.

The study of infinite graphs is an attractive, but often neglected, part of graph theory. This chapter aims to give an introduction that starts gently, but then moves on in several directions to display both the breadth and some of the depth that this field has to offer.1 Our overall theme will be to highlight the typical kinds of phenomena that will always appear when graphs are infinite, and to show how they can lead to deep and fascinating problems.

Perhaps the most typical such phenomena occur already when the graphs are ‘only just’ infinite, when they have only countably many vertices and perhaps only finitely many edges at each vertex. This is not surprising: after all, some of the most basic structural features of graphs, such as paths, are intrinsically countable. Problems that become really interesting only for uncountable graphs tend to be interesting for reasons that have more to do with sets than with graphs, and are studied in combinatorial set theory. This, too, is a fascinating field, but not our topic in this chapter. The problems we shall consider will all be interesting for countable graphs, and set-theoretic problems will not arise.

The terminology we need is exactly the same as for finite graphs, except when we wish to describe an aspect of infinite graphs that has no finite counterpart. One important such aspect is the eventual behaviour of the infinite paths in a graph, which is captured by the notion of ends. The ends of a graph can be thought of as additional limit points at infinity to which its infinite paths converge. This convergence is described formally in terms of a natural topology placed on the graph together with its ends. In our last section we shall therefore assume familiarity with the basic concepts of point-set topology; reminders of the relevant definitions will be included as they arise.

# 8.1 Basic notions, facts and techniques

This section gives a gentle introduction to the aspects of infinity most commonly encountered in graph theory.2

After just a couple of definitions, we begin by looking at a few obvious properties of infinite sets, and how they can be employed in the context of graphs. We then illustrate how to use the three most basic common tools in infinite graph theory: Zorn’s lemma, transfinite induction, and something called ‘compactness’. We complete the section with the combinatorial definition of an end; topological aspects will be treated in Section 8.5.

A graph is locally finite if all its vertices have finite degrees. An infinite graph $( V , E )$ of the form

$$
V = \left\{x _ {0}, x _ {1}, x _ {2}, \dots \right\} \quad E = \left\{x _ {0} x _ {1}, x _ {1} x _ {2}, x _ {2} x _ {3}, \dots \right\}
$$

locally finite

rays

is called a ray, and a double ray is an infinite graph $( V , E )$ of the form

$$
V = \{\dots , x _ {- 1}, x _ {0}, x _ {1}, \dots \} \quad E = \{\dots , x _ {- 1} x _ {0}, x _ {0} x _ {1}, x _ {1} x _ {2}, \dots \};
$$

in both cases the $x _ { n }$ are assumed to be distinct. Thus, up to isomorphism, there is only one ray and one double ray, the latter being the unique infinite 2-regular connected graph. In the context of infinite graphs, finite paths rays and double rays are all called paths.

The subrays of a ray or double ray are its tails. Formally, every ray has infinitely many tails, but any two of them differ only by a finite initial segment. The union of a ray $R$ with infinitely many disjoint finite paths having precisely their first vertex on $R$ is a comb; the last vertices of those paths are the teeth of this comb, and $R$ is its spine. (If such a path is trivial, which we allow, then its unique vertex lies on $R$ and also counts as a tooth; see Figure 8.1.1.)

![](images/44dbe70bde7f6578a5edf74e516b46c4b6dcd22e2513c82a0d3ff0b30e634054.jpg)  
Fig. 8.1.1. A comb with white teeth and spine $R = x _ { 0 } x _ { 1 } \dots$ .

path tail

comb teeth, spine

Let us now look at a few very basic properties of infinite sets, and see how they appear in some typical arguments about graphs.

$$
A n \quad \text {i s i s t} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad \text {i s} \quad
$$

This trivial property is eminently useful when the infinite set in question plays the role of ‘supplies’ that keep an iterated process going. For example, let us show that if a graph $G$ is infinitely connected (that is, if $G$ is $k$ -connected for every $k \in \mathbb N$ ), then $G$ contains a subdivision of $K ^ { \aleph _ { 0 } }$ , the complete graph of order $| \mathbb { N } |$ . We embed $K ^ { \aleph _ { 0 } }$ in $G$ (as a topological minor) in one infinite sequence3 of steps, as follows. We begin by enumerating its vertices. Then at each step we embed the next vertex in $G$ , connecting it to the images of its earlier neighbours by paths in $G$ that avoid any other vertices used so far. The point here is that each new path has to avoid only finitely many previously used vertices, which is not a problem since deleting any finite set of vertices keeps $G$ infinitely connected.

If $G$ , too, is countable, can we then also find a $T K ^ { \aleph _ { 0 } }$ as a spanning subgraph of $G$ ? Although embedding $K ^ { \aleph _ { 0 } }$ in $G$ topologically as above takes infinitely many steps, it is by no means guaranteed that the $T K ^ { \aleph _ { 0 } }$ constructed uses all the vertices of $G$ . However, it is not difficult to ensure this: since we are free to choose the image of each new vertex of $K ^ { \aleph _ { 0 } }$ , we can choose this as the next unused vertex from some fixed enumeration of $V ( G )$ . In this way, every vertex of $G$ gets chosen eventually, unless it becomes part of the $T K ^ { \aleph _ { 0 } }$ before its time, as a subdividing vertex on one of the paths.

$$
\text {U n i o n s} \tag {2}
$$

This fact can be applied in two ways: to show that sets that come to us as countable unions are ‘small’, but also to rewrite a countable set deliberately as a disjoint union of infinitely many infinite subsets. For an example of the latter type of application, let us show that an infinitely edge-connected countable graph has infinitely many edge-disjoint spanning trees. (Note that the converse implication is trivial.) The trick is to construct the trees simultaneously, in one infinite sequence of steps. We first use (2) to partition $\mathbb { N }$ into infinitely many infinite subsets $N _ { i }$ ( $i \in \mathbb N$ ). Then at step $n$ we look which $N _ { i }$ contains $n$ , and add a further vertex $\boldsymbol { v }$ to the $_ i$ th tree $T _ { i }$ . As before, we choose $v$ minimal in some fixed enumeration of $V ( G )$ among the vertices not yet in $T _ { i }$ , and join $v$ to $T _ { i }$ by a path avoiding the finitely many edges used so far.

Clearly, a countable set cannot have uncountably many disjoint subsets. However,

A countable set can have uncountably many subsets whose (3)

This is a remarkable property of countable sets, and a good source of counterexamples to rash conjectures. Can you prove it without looking at Figure 8.1.4?

Another common pitfall in dealing with infinite sets is to assume that the intersection of an infinite nested sequence $A _ { 0 } \supseteq A _ { 1 } \supseteq \hdots$ of uncountable sets must still be uncountable. It need not be; in fact it may be empty. (Example?)

There are a few basic proof techniques that are specific to infinite combinatorics. The two most common of these are the use of Zorn’s lemma and transfinite induction. Rather than describing these formally,4 we illustrate their use by a simple example.

Proposition 8.1.1. Every connected graph contains a spanning tree.

First proof (by Zorn’s lemma).

Given a connected graph $G$ , consider the set of all trees $T \subseteq G$ , ordered by the subgraph relation. Since $G$ is connected, any maximal such tree contains every vertex of $G$ , i.e. is a spanning tree of $G$ .

To prove that a maximal tree exists, we have to show that for any chain $\boldsymbol { \mathscr { C } }$ of such trees there is an upper bound: a tree $T ^ { * } \subseteq G$ containing every tree in $\boldsymbol { \mathscr { C } }$ as a subgraph. We claim that $T ^ { * } : = \cup \mathcal { C }$ is such a tree.

To show that $T ^ { * }$ is connected, let $u , v \in T ^ { * }$ be two vertices. Then in $\boldsymbol { \mathscr { C } }$ there is a tree $T _ { u }$ containing $u$ and a tree $T _ { v }$ containing $v$ . One of these is a subgraph of the other, say $T _ { u } \subseteq T _ { v }$ . Then $T _ { v }$ contains a path from $u$ to $v$ , and this path is also contained in $T ^ { * }$ .

To show that $T ^ { * }$ is acyclic, suppose it contains a cycle $C$ . Each of the edges of $C$ lies in some tree in $\boldsymbol { \mathscr { C } }$ . These trees form a finite subchain of $c$ , which has a maximal element $T$ . Then $C \subseteq T$ , a contradiction. 

Transfinite induction and recursion are very similar to finite inductive proofs and constructions, respectively. Basically, one proceeds step by step, and may at each step assume as known what was shown or constructed before. The only difference is that one may ‘start again’ after performing any infinite number of steps. This is formalized by the use of ordinals rather than natural numbers for counting the steps; see the appendix.

Just as with finite graphs, it is usually more intuitive to construct a desired object (such as a spanning tree) step by step, rather than starting with some unknown ‘maximal’ object and then proving that it has the desired properties. More importantly, a step-by-step construction is

almost always the best way to find the desired object: only later, when one understands the construction well, can one devise an inductive ordering (one whose chains have upper bounds) in which the desired objects appear as the maximal elements. Thus, although Zorn’s lemma may at times provide an elegant way to wrap up a constructive proof, it cannot in general replace a good understanding of transfinite induction—just as a preference for elegant direct definitions of finite objects cannot, for a thorough understanding, replace the more pedestrian algorithmic approach.

Our second proof of Proposition 8.1.1 illustrates both the constructive and the proof aspect of transfinite induction in a typical manner: we first define a subgraph $T ^ { * } \subseteq G$ recursively, hoping that it turns out to be a spanning tree, and then prove inductively that it is.

# Second proof (by transfinite induction).

Let $G$ be a connected graph. We define non-empty subgraphs $T _ { \alpha } \subseteq G$ recursively, as follows. Let $T _ { 0 }$ consist of a single vertex. Now consider an ordinal $\alpha > 0$ . If $\alpha$ is a limit, we put $\textstyle T _ { \alpha } : = \bigcup _ { \beta < \alpha } T _ { \beta }$ . If $\alpha$ is a successor, of $\beta$ say, we check whether $G - T _ { \beta } = \emptyset$ . If so, we terminate the recursion and put $T _ { \beta } \ = : T$ . If not, then $G - T _ { \beta }$ has a vertex $v _ { \alpha }$ that sends an edge $e _ { \alpha }$ to a vertex in $T _ { \beta }$ . Let $T _ { \alpha }$ be obtained from $T _ { \beta }$ by adding $v _ { \alpha }$ and $e _ { \alpha }$ . This recursion terminates, since if $v _ { \beta + 1 }$ (where $\beta + 1$ denotes the successor of $\beta$ ) gets defined for all $\beta < \gamma$ then $\beta \mapsto v _ { \beta + 1 }$ is an injective map showing that $| \gamma | \leqslant | G |$ , which cannot hold for all ordinals $\gamma$ .

We now prove by induction on $\alpha$ that every graph $T _ { \alpha }$ we defined is a tree. Since $T$ is one of the $T _ { \alpha }$ and is, by definition, a spanning subgraph of $G$ , this will complete the proof. Let $\alpha$ be given, and assume that every $T _ { \beta }$ with $\beta < \alpha$ is a tree. If $\alpha$ is a successor, of $\beta$ say, then $T _ { \alpha }$ is clearly connected and acyclic, because $T _ { \beta }$ is.

Suppose now that $\alpha$ is a limit. To show that $T _ { \alpha }$ is connected, let $u , v$ be any two of its vertices. Since $\textstyle T _ { \alpha } = \bigcup _ { \beta < \alpha } T _ { \beta }$ , there exist $\beta ( u ) , \beta ( v ) < \alpha$ such that $u \in T _ { \beta ( u ) }$ and $v \in T _ { \beta ( v ) }$ , say with $\beta ( u ) \leqslant \beta ( v )$ . Then $T _ { \beta ( v ) }$ contains a $u$ – $v$ path, which is also contained in $T _ { \alpha }$ . Now suppose that $T _ { \alpha }$ contains a cycle $C$ . For each of its vertices $v$ there is an ordinal $\beta ( v ) < \alpha$ with $v \in T _ { \beta ( v ) }$ ; let $\beta$ be the largest among these. Then $C \subseteq T _ { \beta }$ , contradicting our assumption that $T _ { \beta }$ is a tree. 

Why did these proofs work so smoothly? The reason is that the forbidden or required substructures, cycles and connecting paths, were finite and therefore could not arise or vanish unexpectedly at limit steps. This has helped to keep our two model proofs simple, but it is not typical. If we want to construct a rayless graph, for example, the edges of different rayless graphs $G _ { \beta }$ might combine to form a ray in $\textstyle G _ { \alpha } = \bigcup _ { \beta < \alpha } G _ { \beta }$ when $\alpha$ is a limit. And indeed, here lies the challenge in most transfinite

compactness proofs

constructions: to make the right choices at successor steps to ensure that the structure will also be as desired at limits.

Our third basic proof technique, somewhat mysteriously referred to as compactness (see below for why), offers a formalized way to make the right choices in certain standard cases. These are cases where, unlike in the above examples, a wrong choice may necessarily lead to a dead end after another finite number of steps, even though nothing unexpected happens at limits.

For example, let $G$ be a graph whose finite subgraphs are all $k$ - colourable. It is natural then to try to construct a $k$ -colouring of $G$ as a limit of $k$ -colourings of its finite subgraphs. Now each finite subgraph will have several $k$ -colourings; will it matter which we choose? Clearly, it will. When $G ^ { \prime } \subseteq G ^ { \prime \prime }$ are two finite subgraphs and $u , v$ are vertices of $G ^ { \prime }$ that receive the same colour in every $k$ -colouring of $G ^ { \prime \prime }$ (and hence also in any $k$ -colouring of $G$ ), we must not give them different colours in the colouring we choose for $G ^ { \prime }$ , even if such a colouring exists. However if we do manage, somehow, to colour the finite subgraphs of $G$ compatibly, we shall automatically have a colouring of all of $G$ .

For countable graphs, compactness proofs are formalized by the following lemma:

# Lemma 8.1.2. (K¨onig’s Infinity Lemma)

Let $V _ { 0 } , V _ { 1 } , \ldots$ be an infinite sequence of disjoint non-empty finite sets, and let $G$ be a graph on their union. Assume that every vertex $v$ in a set $V _ { n }$ with $n \geqslant 1$ has a neighbour $f ( v )$ in $V _ { n - 1 }$ . Then $G$ contains a ray $v _ { 0 } v _ { 1 } \ldots .$ with $v _ { n } \in V _ { n }$ for all $n$ .

![](images/9109161b9f62fab865aef268795f67ed97865b1208a7faf74ebf69e7f8d015c3.jpg)  
Fig. 8.1.2. K¨onig’s infinity lemma

Proof . Let $\mathcal { P }$ be the set of all finite paths of the form $v f ( v ) f ( f ( v ) ) \ldots$ ending in $V _ { 0 }$ . Since $V _ { 0 }$ is finite but $\mathcal { P }$ is infinite, infinitely many of the paths in $\mathcal { P }$ end at the same vertex $v _ { 0 } ~ \in ~ V _ { 0 }$ . Of these paths, infinitely many also agree on their penultimate vertex $v _ { 1 } \in V _ { 1 }$ , because $V _ { 1 }$ is finite. Of those paths, infinitely many agree even on their vertex $v _ { 2 }$ in $V _ { 2 }$ —and so on. Although the set of paths considered decreases from step to step, it is still infinite after any finite number of steps, so $v _ { n }$ gets defined for every $n \in \mathbb N$ . By definition, each vertex $v _ { n }$ is adjacent to $v _ { n - 1 }$ on one of those paths, so $\boldsymbol { v } _ { 0 } \boldsymbol { v } _ { 1 } \ldots$ is indeed a ray. 

The following ‘compactness theorem’, the first of its kind in graph theory, answers our question about colourings:

# Theorem 8.1.3. (de Bruijn & Erd˝os, 1951)

Let $G = ( V , E )$ be a graph and $k \in \mathbb N$ . If every finite subgraph of $G$ has chromatic number at most $k$ , then so does $G$ .

# First proof (for $G$ countable, by the infinity lemma).

Let $v _ { 0 } , v _ { 1 } , \ldots$ be an enumeration of $V$ and put $G _ { n } : = G \left[ v _ { 0 } , \ldots , v _ { n } \right]$ . Write $V _ { n }$ for the set of all $k$ -colourings of $G _ { n }$ with colours in $\{ 1 , \ldots , k \}$ . Define a graph on $\textstyle \bigcup _ { n \in \mathbb { N } } V _ { n }$ by inserting all edges $c c ^ { \prime }$ such that $c \in V _ { n }$ and $c ^ { \prime } \in V _ { n - 1 }$ is the restriction of $c$ to $\left\{ \begin{array} { l } { v _ { 0 } , \dots , v _ { n - 1 } } \end{array} \right\}$ . Let $c _ { 0 } c _ { 1 } \ldots$ be a ray in this graph with $c _ { n } \in V _ { n }$ for all $n$ . Then $c : = \textstyle \bigcup _ { n \in \mathbb { N } } c _ { n }$ is a colouring of $G$ with colours in $\{ 1 , \ldots , k \}$ . 

Our second proof of Theorem 8.1.3 appeals directly to compactness as defined in topology. Recall that a topological space is compact if its closed sets have the ‘finite intersection property’, which means that the overall intersection $\cap A$ of a set $\mathcal { A }$ of closed sets is non-empty whenever every finite subset of $\boldsymbol { A }$ has a non-empty intersection. By Tychonoff’s theorem of general topology, any product of compact spaces is compact in the usual product topology.

Second proof (for $G$ arbitrary, by Tychonoff’s theorem).

Consider the product space

$$
X := \prod_ {V} \{1, \dots , k \} = \{1, \dots , k \} ^ {V}
$$

of $| V |$ copies of the finite set $\{ 1 , \ldots , k \}$ endowed with the discrete topology. By Tychonoff’s theorem, this is a compact space. Its basic open sets have the form

$$
O _ {h} := \left\{f \in X: f | _ {U} = h \right\},
$$

where $h$ is some map from a finite set $U \subseteq V$ to $\{ 1 , \ldots , k \}$

For every finite set $U \subseteq V$ , let $A _ { U }$ be the set of all $f \in X$ whose restriction to $U$ is a $k$ -colouring of $G [ U ]$ . These sets $A _ { U }$ are closed (as well as open—why?), and for any finite set $\boldsymbol { u }$ of finite subsets of $V$ we have $\cap _ { U \in \mathcal { U } } A _ { U } \neq \emptyset$ , because $G [ \cup U ]$ has a $k$ -colouring. By the finite intersection property of the sets $A _ { U }$ , their overall intersection is nonempty, and every element of this intersection is a $k$ -colouring of $G$ . 

Although our two compactness proofs look formally different, it is instructive to compare them in detail, checking how the requirements in one are reflected in the other (cf. Exercise 10).

As the reader may expect, the standard use for compactness proofs is to transfer theorems from finite to infinite graphs, or conversely. This is not always quite as straightforward as above; often, the statement has to be modified a little to make it susceptible to a compactness argument.

As an example—see Exercises 12–17 for more—let us prove the locally finite version of the following famous conjecture. Call a bipartition of the vertex set of a graph unfriendly if every vertex has at least as many neighbours in the other class as in its own. Clearly, every finite graph has an unfriendly partition: just take any partition that maximizes the number of edges between the partition classes. At the other extreme, it can be shown by set-theoretic methods that uncountable graphs need not have such partitions. Thus, intriguingly, it is the countable case that has remained unsolved:

Unfriendly Partition Conjecture. Every countable graph admits an unfriendly partition of its vertex set.

Proof for locally finite graphs. Let $G = ( V , E )$ be an infinite but locally finite graph, and enumerate its vertices as $v _ { 0 } , v _ { 1 } , \ldots$ . For every $n \in \mathbb N$ , let $\nu _ { n }$ be the set of partitions of $V _ { n } : = \{ v _ { 0 } , \ldots , v _ { n } \}$ into two sets $U _ { n }$ and $W _ { n }$ such that every vertex $v \in V _ { n }$ with $N _ { G } ( v ) \subseteq V _ { n }$ has at least as many neighbours in the other class as in its own. Since the conjecture holds for finite graphs, the sets $\nu _ { n }$ are non-empty. For all $n \geqslant 1$ , every $( U _ { n } , W _ { n } ) \ \in \ \mathcal { V } _ { n }$ induces a partition $( U _ { n - 1 } , W _ { n - 1 } )$ of $V _ { n - 1 }$ , which lies in $\nu _ { n - 1 }$ . By the infinity lemma, there is an infinite sequence of partitions $( U _ { n } , W _ { n } ) \in \mathcal { V } _ { n }$ , one for every $n \in \mathbb N$ , such that each is induced by the next. Then $\textstyle ( \bigcup _ { n \in \mathbb { N } } U _ { n } , \bigcup _ { n \in \mathbb { N } } W _ { n } )$ is an unfriendly partition of $G$ . 

The trick that made this proof possible was to require, for the partitions of $V _ { n }$ , correct positions only of vertices that send no edge out of $V _ { n }$ : this weakening is necessary to ensure that partitions from $\nu _ { n }$ induce partitions in $\nu _ { n - 1 }$ ; but since, by local finiteness, every vertex has this property eventually (for large enough $n$ ), the weaker assumption suffices to ensure that the limit partition is unfriendly.

Let us complete this section with an introduction to the one important concept of infinite graph theory that has no finite counterpart, the notion of an end. An $e n d ^ { 5 }$ of a graph $G$ is an equivalence class of rays in $G$ , where two rays are considered equivalent if, for every finite set $S \subseteq V ( G )$ , both have a tail in the same component of $G - S$ . This is indeed an equivalence relation: note that, since $S$ is finite, there is exactly one such component for each ray. If two rays are equivalent— and only then—they can be linked by infinitely many disjoint paths: just

choose these inductively, taking as $S$ the union of the vertex sets of the first finitely many paths to find the next. The set of ends of $G$ is denoted by $\Omega ( G )$ , and we write $G = ( V , E , \Omega )$ to express that $G$ has vertex, edge and end sets $V , E , \Omega$ .

For example, let us determine the ends of the 2-way infinite ladder shown in Figure 8.1.3. Every ray in this graph contains vertices arbitrarily far to the left or vertices arbitrarily far to the right, but not both. These two types of rays are clearly equivalence classes, so the ladder has exactly two ends. (In Figure 8.1.3 these are shown as two isolated dots—one on the left, the other on the right.)

![](images/54c3dd0de7a8023884416bc84feb0a74feb92a34bb5ee32cb4025647a98c10da.jpg)  
Fig. 8.1.3. The 2-way ladder has two ends

The ends of a tree are particularly simple: two rays in a tree are equivalent if and only if they share a tail, and for every fixed vertex $\boldsymbol { v }$ each end contains exactly one ray starting at $\boldsymbol { v }$ . Even a locally finite tree can have uncountably many ends. The prototype example (see Exercise 21) is the binary tree $T _ { 2 }$ , the rooted tree in which every vertex has exactly two upper neighbours. Often, the vertex set of $T _ { 2 }$ is taken to be the set of finite 0–1 sequences (with the empty sequence as the root), as indicated in Figure 8.1.4. The ends of $T _ { 2 }$ then correspond bijectively to its rays starting at $\varnothing$ , and hence to the infinite 0–1 sequences.

![](images/95b9ec77af375a2e3bf2b1196533c81773c2c9136df1d0c5a8f303a85a63c945.jpg)  
Fig. 8.1.4. The binary tree $T _ { 2 }$ has continuum many ends, one for every infinite 0–1 sequence

These examples suggest that the ends of a graph can be thought of as ‘points at infinity’ to which its rays converge. We shall formalize this

$\Omega ( G )$

binary tree $T _ { 2 }$

end degrees

in Section 8.5, where we define a natural topology on a graph and its ends in which rays will indeed converge to their respective ends.

The maximum number of disjoint rays in an end is the (combinatorial) vertex-degree of that end, the maximum number of edge-disjoint rays in it is its (combinatorial) edge-degree. These maxima are indeed attained: if an end contains a set of $k$ (edge-) disjoint rays for every integer $k$ , it also contains an infinite set of (edge-) disjoint rays (Exercise 33). Thus, every end has a vertex-degree and an edge-degree in $\mathbb { N } \cup \{ \infty \}$ .

# 8.2 Paths, trees, and ends

There are two fundamentally different aspects to the infinity of an infinite connected graph: one of ‘length’, expressed in the presence of rays, and one of ‘width’, expressed locally by infinite degrees. The infinity lemma tells us that at least one of these must occur:

Proposition 8.2.1. Every infinite connected graph has a vertex of infinite degree or contains a ray.

(8.1.2)

Proof . Let $G$ be an infinite connected graph with all degrees finite. Let $v _ { 0 }$ be a vertex, and for every $ { n _ { \mathrm { ~ \scriptsize ~ \in ~ \mathbb ~ N ~ } } }$ let $V _ { n }$ be the set of vertices at distance $n$ from $v _ { 0 }$ . Induction on $n$ shows that the sets $V _ { n }$ are finite, and hence that $V _ { n + 1 } \neq \emptyset$ (because $G$ is infinite and connected). Furthermore, the neighbour of a vertex $v \in V _ { n + 1 }$ on any shortest $\boldsymbol { v }$ – $v _ { 0 }$ path lies in $V _ { n }$ . By Lemma 8.1.2, $G$ contains a ray. 

Often it is useful to have more detailed information on how this ray or vertex of infinite degree lies in $G$ . The following lemma enables us to find it ‘close to’ any given infinite set of vertices.

[ 8.5.5 ]

Lemma 8.2.2. (Star-Comb Lemma)

Let $U$ be an infinite set of vertices in a connected graph $G$ . Then $G$ contains either a comb with all teeth in $U$ or a subdivision of an infinite star with all leaves in $U$ .

Proof . As $G$ is connected, it contains a path between two vertices in $U$ . This path is a tree $T \subseteq G$ every edge of which lies on a path in $T$ between two vertices in $U$ . By Zorn’s lemma there is a maximal such tree $T ^ { * }$ . Since $U$ is infinite and $G$ is connected, $T ^ { * }$ is infinite. If $T ^ { * }$ has a vertex of infinite degree, it contains the desired subdivided star.

Suppose now that $T ^ { * }$ is locally finite. Then $T ^ { * }$ contains a ray $R$ (Proposition 8.2.1). Let us construct a sequence $P _ { 1 } , P _ { 2 } , \ldots$ of disjoint $R$ – $U$ paths in $T ^ { * }$ . Having chosen $P _ { i }$ for every $i < n$ for some $n$ , pick $v \in R$ so that $v R$ meets none of those paths $P _ { i }$ . The first edge of $v R$ lies on a path $P$ in $T ^ { * }$ between two vertices in $U$ ; let us think of $P$ as

traversing this edge in the same direction as $R$ . Let $w$ be the last vertex of $v P$ on $v R$ . Then $P _ { n } : = w P$ contains an $R$ – $U$ path, and $P _ { n } \cap P _ { i } = \emptyset$ 5 for all $i < n$ because $P _ { i } \cup R w \cup P _ { n }$ contains no cycle. 

We shall often apply Lemma 8.2.2 in locally finite graphs, in which case it always yields a comb.

Recall that a rooted tree $T \subseteq G$ is normal in $G$ if the endvertices of every $T$ -path in $G$ are comparable in the tree-order of $T$ . If $T$ is a spanning tree, the only $T$ - paths are edges of $G$ that are not edges of $T$ .

Normal spanning trees are perhaps the single most important structural tool in infinite graph theory. As in finite graphs, they exhibit the separation properties of the graph they span.6 Moreover, their normal rays, those that start at the root, reflect its end structure:

normal ray

Lemma 8.2.3. If $T$ is a normal spanning tree of $G$ , then every end of [ 8.5.7 ] $G$ contains exactly one normal ray of $T$ .

Proof . Let $\omega \in \Omega ( G )$ be given. Apply the star-comb lemma in $T$ with (1.5.5) $U$ the vertex set of a ray $R \in \omega$ . If the lemma gives a subdivided star with leaves in $U$ and centre $z$ , say, then the finite down-closure $\lceil z \rceil$ of $z$ in $T$ separates infinitely many vertices $u \ > \ z$ of $U$ pairwise in $G$ (Lemma 1.5.5). This contradicts our choice of $U$ .

So $T$ contains a comb with teeth on $R$ . Let $R ^ { \prime } \subseteq T$ be its spine. Since every ray in $T$ has an increasing tail (Exercise 4), we may assume that $R ^ { \prime }$ is a normal ray. Since $R ^ { \prime }$ is equivalent to $R$ , it lies in $\omega$ .

Conversely, distinct normal rays of $T$ are separated in $G$ by the (finite) down-closure of their greatest common vertex (Lemma 1.5.5), so they cannot belong to the same end of $G$ . 

Not all connected graphs have a normal spanning tree; complete uncountable graphs, for example, have none. (Why not?) The quest to characterize the graphs that have a normal spanning tree is not entirely over, and it has held some surprises.7 One of the most useful sufficient conditions is that the graph contains no $T K ^ { \aleph _ { 0 } }$ ; see Theorem 12.4.13. For our purposes, the following result suffices:

Theorem 8.2.4. (Jung 1967) [ 8.5.9 ]

Every countable connected graph has a normal spanning tree.

Proof . The proof follows that of Proposition 1.5.6; we only sketch the (1.5.6) differences. Starting with a single vertex, we construct an infinite se-

quence $T _ { 0 } \subseteq T _ { 1 } \subseteq . . .$ of finite normal trees in $G$ , all with the same root, whose union $T$ will be a normal spanning tree.

To ensure that $T$ spans $G$ , we fix an enumeration $v _ { 0 } , v _ { 1 } , \ldots$ of $V ( G )$ and see to it that $T _ { n }$ contains $v _ { n }$ . It is clear that $T$ will be a tree (since any cycle in $T$ would lie in some $T _ { n }$ , and every two vertices of $T$ lie in a common $T _ { n }$ and can be linked there), and clearly the tree order of $T$ induces that of the $T _ { n }$ . Finally, $T$ will be normal, because the endvertices of any edge of $G$ that is not an edge of $T$ lie in some $T _ { n }$ : since that $T _ { n }$ is normal, they must be comparable there, and hence in $T$ .

It remains to specify how to construct $T _ { n + 1 }$ from $T _ { n }$ . If $v _ { n + 1 } \in T _ { n }$ , put $T _ { n + 1 } : = T _ { n }$ . If not, let $C$ be the component of $G - T _ { n }$ containing $v _ { n + 1 }$ . Let $x$ be the greatest element of the chain $N ( C )$ in $T _ { n }$ , and let $T _ { n + 1 }$ be the union of $T _ { n }$ and an $x - v _ { n + 1 }$ path $P$ with ${ \check { P } } \subseteq C$ . Then the neighbourhood in $T _ { n + 1 }$ of any new component $C ^ { \prime } \subseteq C$ of $G - T _ { n + 1 }$ is a chain in $T _ { n + 1 }$ , so $T _ { n + 1 }$ is again normal. 

One of the most basic problems in an infinite setting that has no finite equivalent is whether or not ‘arbitrarily many’, in some context, implies ‘infinitely many’. Suppose we can find $k$ disjoint rays in some given graph $G$ , for every $k \in \mathbb N$ ; does $G$ also contain an infinite set of disjoint rays?

The answer to the corresponding question for finite paths (of any fixed length) is clearly ‘yes’, since a finite path $P$ can never get in the way of more than $| P |$ disjoint other paths. A badly chosen ray, however, can meet infinitely many other rays, preventing them from being selected for the same disjoint set. Rather than collecting our disjoint rays greedily, we therefore have to construct them carefully and all simultaneously.

The proof of the following theorem is a nice example of a construction in an infinite sequence of steps, where the final object emerges only at the limit step. Each of the steps in the sequence will involve a nontrivial application of Menger’s theorem (3.3.1).

# Theorem 8.2.5. (Halin 1965)

(i) If an infinite graph $G$ contains $k$ disjoint rays for every $k \in \mathbb N$ then $G$ contains infinitely many disjoint rays.   
(ii) If an infinite graph $G$ contains $k$ edge-disjoint rays for every $k \in \mathbb N$ then $G$ contains infinitely many edge-disjoint rays.

(3.3.1) Proof . (i) We construct our infinite system of disjoint rays inductively in $\omega$ steps. After step $n$ , we shall have found $n$ disjoint rays $R _ { 1 } ^ { n } , \ldots , R _ { n } ^ { n }$ and chosen initial segments $R _ { i } ^ { n } x _ { i } ^ { n }$ of these rays. In step $n + 1$ we choose the rays $R _ { 1 } ^ { n + 1 } , \ldots , R _ { n + 1 } ^ { n + 1 }$ i i so as to extend these initial segments, i.e. so that $R _ { i } ^ { n } x _ { i } ^ { n }$ is a proper initial segment of $R _ { i } ^ { n + 1 } x _ { i } ^ { n + 1 }$ , for $i = 1 , \ldots , n$ . Then, clearly, the graphs $\begin{array} { r } { R _ { i } ^ { * } : = \bigcup _ { n \in \mathbb { N } } R _ { i } ^ { n } x _ { i } ^ { n } } \end{array}$ will form an infinite family $( R _ { i } ^ { * } ) _ { i \in \mathbb { N } }$ of disjoint rays in $G$ .

For $n = 0$ the empty set of rays is as required. So let us assume that $R _ { 1 } ^ { n } , \ldots , R _ { n } ^ { n }$ have been chosen, and describe step $n + 1$ . For simplicity, let us abbreviate $R _ { i } ^ { n } = : R _ { i }$ and $x _ { i } ^ { n } = : x _ { i }$ . Let $\mathcal { R }$ be any set of $| R _ { 1 } x _ { 1 } \cup . . . \cup R _ { n } x _ { n } | + n ^ { 2 } + 1$ disjoint rays (which exists by assumption), and immediately delete those rays from $\mathcal { R }$ that meet any of the paths $R _ { 1 } x _ { 1 } , \ldots , R _ { n } x _ { n }$ ; then $\mathcal { R }$ still contains at least $n ^ { 2 } + 1$ rays.

We begin by repeating the following step as often as possible. If there exists an $i \in \{ 1 , \ldots , n \}$ such that $R _ { i } ^ { n + 1 }$ has not yet been defined i i rays from and $\bar { x } _ { i } R _ { i }$ meets at most $\mathcal { R }$ , put $R _ { i } ^ { n + 1 } : = R _ { i }$ $n$ of the rays currently in , and choose as $x _ { i } ^ { n + 1 }$ Rthe successor of $\mathcal { R }$ , we delete those $x _ { i }$ on $R _ { i }$ . Having performed this step as often as possible, we let $I$ denote the set of those $i \in \{ 1 , \ldots , n \}$ for which $R _ { i } ^ { n + 1 }$ is still undefined, and put $| I | = : m$ . Then $\mathcal { R }$ still contains at least $n ^ { 2 } + 1 - ( n - m ) n \geqslant m ^ { 2 } + 1$ rays. Every $R _ { i }$ with $i \in I$ meets more than $n \geqslant m$ of the rays in $\mathcal { R }$ ; let $z _ { i }$ be its first vertex on the $m$ th ray it meets. Then $\textstyle Z : = \bigcup _ { i \in I } x _ { i } R _ { i } z _ { i }$ meets at most $m ^ { 2 }$ of the rays in $\mathcal { R }$ ; we delete all the other rays from $\mathcal { R }$ , choosing one of them as $R _ { n + 1 } ^ { n + 1 }$ R(with $x _ { n + 1 } ^ { n + 1 }$ arbitrary).

On each remaining ray $R \in \mathcal { R }$ we now pick a vertex $y = y ( R )$ after its last vertex in $Z$ , and put $Y : = \{ y ( R ) \mid R \in { \mathcal { R } } \}$ . Let $H$ be the union of $Z$ and all the paths $R y$ ( $R \in \mathcal { R }$ ). Then $X : = \{ x _ { i } \mid i \in I \}$ cannot be separated from $Y$ in $H$ by fewer than $m$ vertices, because these would miss both one of the $m$ rays $R _ { i }$ with $i \in I$ and one of the $m$ rays in $\mathcal { R }$ that meet $x _ { i } R _ { i } z _ { i }$ for this $i$ . So by Menger’s theorem (3.3.1) there are $m$ disjoint $X { - } Y$ paths $P _ { i } = x _ { i } \ldots y _ { i }$ ( $i \in I$ ) in $H$ . For each $i \in I$ let $R _ { i } ^ { \prime }$ denote the ray from $\mathcal { R }$ that contains $y _ { i }$ , choose as $R _ { i } ^ { n + 1 }$ the ray $R _ { i } x _ { i } P _ { i } y _ { i } R _ { i } ^ { \prime }$ , and put $x _ { i } ^ { n + 1 } : = y _ { i }$ .

(ii) is analogous.

Does Theorem 8.2.5 generalize to other graphs than rays? Let us call a graph $H$ ubiquitous with respect to a relation $\leqslant$ between graphs (such as the subgraph relation $\subseteq$ , or the minor relation $\preccurlyeq$ ) if $n H \leqslant G$ for all $n \in \mathbb N$ implies $\aleph _ { 0 } H \leq G$ , where $n H$ denotes the disjoint union of $n$ copies of $H$ . Ubiquity appears to be closely related to questions of wellquasi-ordering as discussed in Chapter 12. Non-ubiquitous graphs exist for all the standard graph orderings; see Exercise 36 for an example of a locally finite graph that is not ubiquitous under the subgraph relation.

# Ubiquity conjecture. (Andreae 2002)

Every locally finite connected graph is ubiquitous with respect to the minor relation.

Just as in Theorem 8.2.5 one can show that an end contains infinitely many disjoint rays as soon as the number of disjoint rays in it is not finitely bounded, and similarly for edge-disjoint rays (Exercise 33).

thick/thin

Hence, the maxima in our earlier definitions of the vertex- and edgedegrees of an end exist as claimed. Ends of infinite vertex-degree are called thick; ends of finite vertex-degree are thin.

grid

The $\mathbb { N } \times \mathbb { N }$ grid, for example, the graph on $\mathbb { N } ^ { 2 }$ in which two vertices $( n , m )$ and $( n ^ { \prime } , m ^ { \prime } )$ are adjacent if and only if $| n - n ^ { \prime } | + | m - m ^ { \prime } | = 1$ , has only one end, which is thick. In fact, the $\mathbb { N } \times \mathbb { N }$ grid is a kind of prototype for thick ends: every graph with a thick end contains it as a minor. This is another classical result of Halin, which we prove in the remainder of this section.

For technical reasons, we shall prove Halin’s theorem for hexagonal rather than square grids. These may seem a little unwieldy at first, but have the advantage that they can be found as topological rather than ordinary minors (Proposition 1.7.2), which makes them much easier to handle. We shall define the hexagonal grid $H ^ { \infty }$ so that it is a subgraph of the $\mathbb { N } \times \mathbb { N }$ grid, and it will be easy to see that, conversely, the $\mathbb { N } \times \mathbb { N }$ grid is a minor of $H ^ { \infty }$ (cf. Ex. 47, Ch. 12.)

$H ^ { \infty }$

To define our standard copy of the hexagonal quarter grid $H ^ { \infty }$ , we delete from the $\mathbb { N } \times \mathbb { N }$ grid $H$ the vertex $( 0 , 0 )$ , the vertices $( n , m )$ with $n > m$ , and all edges $( n , m ) ( n + 1 , m )$ such that $n$ and $m$ have equal parity (Fig. 8.2.1). Thus, $H ^ { \infty }$ consists of the vertical rays

$U _ { n }$

$$
U _ {0} := H \left[ \left\{\left(0, m\right) \mid 1 \leqslant m \right\} \right]
$$

$$
U _ {n} := H \left[ \left\{\left(n, m\right) \mid n \leqslant m \right\} \right] \quad (n \geqslant 1)
$$

and between these a set of horizontal edges,

$$
E := \left\{\left(n, m\right) (n + 1, m) \mid n \not \equiv m (\mathrm {m o d} 2) \right\}.
$$

e1, e2, . . .

To enumerate these edges, as $e _ { 1 } , e _ { 2 } , \ldots$ say, we order them colexicographically: the edge $( n , m ) ( n + 1 , m )$ precedes the edge $( n ^ { \prime } , m ^ { \prime } ) ( n ^ { \prime } + 1 , m ^ { \prime } )$ if $m < m ^ { \prime }$ , or if $m = m ^ { \prime }$ and $n < n ^ { \prime }$ (Fig. 8.2.1).

# Theorem 8.2.6. (Halin 1965)

Whenever a graph contains a thick end, it has a $T H ^ { \infty }$ subgraph whose rays belong to that end.

(8.1.2)

-

$\omega$

Proof . Given two infinite sets $\mathcal { P } , \mathcal { P } ^ { \prime }$ of finite or infinite paths, let us write $\mathcal { P } \geqslant \mathcal { P } ^ { \prime }$ if ${ \mathcal { P } } ^ { \prime }$ consists of final segments of paths in $\mathcal { P }$ . (Thus, if $\mathcal { P }$ is a set of rays, then so is ${ \mathcal { P } } ^ { \prime }$ .)

Let $G$ be any graph with a thick end $\omega$ . Our task is to find disjoint rays in $\omega$ that can serve as ‘vertical’ (subdivided) rays $U _ { n }$ for our desired grid, and to link these up by suitable disjoint ‘horizontal’ paths. We begin by constructing a sequence $Q _ { 0 } , Q _ { 1 } , \ldots$ of rays (of which we shall later choose some tails $Q _ { n } ^ { \prime }$ as ‘vertical rays’), together with path systems $\mathcal { P } ( Q _ { i } )$ between the $Q _ { i }$ and suitable $Q _ { p ( i ) }$ with $p ( i ) < i$ (from which we

![](images/267ba044740c137b40f4a5387d8689f923d6717084515a0206f976716ada0214.jpg)  
Fig. 8.2.1. The hexagonal quarter grid $H ^ { \infty }$ .

shall later choose the ‘horizontal paths’). We shall aim to find the $Q _ { n }$ in ‘supply sets’ $\mathcal { R } _ { 0 } \geqslant \mathcal { R } _ { 1 } \geqslant . . .$ of unused rays.

We start with any infinite set $\mathcal { R } _ { 0 }$ of disjoint rays in $\omega$ ; this exists by our assumption that $\omega$ is a thick end. At step $n \in \mathbb N$ of the construction, we shall choose the following:

(1) a ray $Q _ { n } \in \omega$ disjoint from $Q _ { 0 } \cup . . . \cup Q _ { n - 1 }$ ;   
(2) if $n \geqslant 1$ , an integer $p ( n ) < n$ ;   
(3) for every $i$ with $1 \leqslant i \leqslant n$ , an infinite set $\textstyle { \mathcal { P } } _ { n } ( Q _ { i } )$ of disjoint $Q _ { i }$ –Q paths, such that

(i) $\bigcup { \mathcal { P } } _ { n } ( Q _ { i } ) \cap \bigcup { \mathcal { P } } _ { n } ( Q _ { j } ) = \emptyset$ for distinct $i , j \leqslant n$ , and   
(ii) $\bigcup { \mathcal { P } } _ { n } ( Q _ { i } ) \cap Q _ { j } = \emptyset$ for distinct $i , j \leqslant n$ with $j \neq p ( i )$

(4) an infinite set $\mathcal { R } _ { n + 1 } \leqslant \mathcal { R } _ { n }$ of disjoint rays that are disjoint from $Q _ { 0 } \cup \ldots \cup Q _ { n }$ and from $\cup \mathcal { P } _ { n } ( Q _ { i } )$ whenever $1 \leqslant i \leqslant n$ .

Thus, while the rays $Q _ { i }$ and the predecessor map $i \mapsto p ( i )$ remain unchanged once defined for some $i$ , the path system $\mathcal { P } _ { n } ( Q _ { i } )$ between $Q _ { i }$ and $Q _ { p ( i ) }$ changes as $n$ increases. More precisely, we shall have

(5) ${ \mathcal { P } } _ { n } ( Q _ { i } ) \subseteq { \mathcal { P } } _ { n - 1 } ( Q _ { i } )$ whenever $1 \leqslant i < n$

Informally, we think of $\textstyle { \mathcal { P } } _ { n } ( Q _ { i } )$ as our best candidate at time $n$ for a system of horizontal paths linking $Q _ { i }$ to $Q _ { p ( i ) }$ . But, as new rays $Q _ { m }$ with $m > n$ get selected, we may have to change our mind about $\mathcal { P } _ { n } ( Q _ { i } )$ and, again and again, prune it to a smaller system $\mathcal { P } _ { m } ( Q _ { i } )$ . This may leave us with an empty system at the end of of the construction. Thus, when we later come to construct our grid, we shall have to choose its horizontal paths between $Q _ { i }$ and $Q _ { p ( i ) }$ from these provisional sets $\mathcal { P } _ { n } ( Q _ { i } )$ , not from their (possibly empty) intersection over all $n$ .

$R _ { n } ^ { 0 }$

Let $n \in \mathbb N$ be given. If $n = 0$ , choose any ray from $\mathcal { R } _ { 0 }$ as $Q _ { 0 }$ , and put $\mathcal { R } _ { 1 } : = \mathcal { R } _ { 0 } \backslash \{ Q _ { 0 } \}$ . Then conditions (1)–(5) hold for $n = 0$ .

Suppose now that $n \geqslant 1$ , and consider a ray $R _ { n } ^ { 0 } \in \mathcal { R } _ { n }$ . By (4), $R _ { n } ^ { 0 }$ i s disjoint from

$$
H := Q _ {0} \cup \dots \cup Q _ {n - 1} \cup \bigcup_ {i = 1} ^ {n - 1} \mathcal {P} _ {n - 1} (Q _ {i}).
$$

By the choice of $\mathcal { R } _ { 0 }$ and (4), we know that $R _ { n } ^ { 0 } \in \omega$ . As also $Q _ { 0 } \in \omega$ , there exists an infinite set $\mathcal { P }$ of disjoint $R _ { n } ^ { 0 } – H$ paths. If possible, we choose $\mathcal { P }$ so that $\bigcup { \mathcal { P } } \cap \bigcup { \mathcal { P } } _ { n - 1 } ( Q _ { i } ) = \emptyset$ for all $i \leqslant n - 1$ . We may then further choose $\mathcal { P }$ so that $\bigcup { \mathcal { P } } \cap Q _ { i } \neq \emptyset$ for only one $i$ , since by (1) the $Q _ { i }$ are disjoint for different $i$ . We define $p ( n )$ as this $_ i$ , and put ${ \mathcal { P } } _ { n } ( Q _ { j } ) : = { \mathcal { P } } _ { n - 1 } ( Q _ { j } )$ for all $j \leqslant n - 1$ .

If $\mathcal { P }$ cannot be chosen in this way, we may choose it so that all its vertices in $H$ lie in $\cup \mathcal { P } _ { n - 1 } ( Q _ { i } )$ for the same $i$ , since by (3) the graphs $\cup \mathcal { P } _ { n - 1 } ( Q _ { i } )$ are disjoint for different $i$ . We can then find infinite disjoint subsets $\textstyle { \mathcal { P } } _ { n } ( Q _ { i } )$ of $\mathcal { P } _ { n - 1 } ( Q _ { i } )$ and ${ \mathcal { P } } ^ { \prime }$ of $\mathcal { P }$ . We continue infinitely many of the paths in ${ \mathcal { P } } ^ { \prime }$ along paths from $\mathcal { P } _ { n - 1 } ( Q _ { i } ) \backslash \mathcal { P } _ { n } ( Q _ { i } )$ to $Q _ { i }$ or to $Q _ { p ( i ) }$ , to obtain an infinite set ${ \mathcal { P } } ^ { \prime \prime }$ of disjoint $R _ { n } ^ { 0 } { - } Q _ { i }$ or $R _ { n } ^ { 0 } - Q _ { p ( i ) }$ paths, and define $p ( n )$ as $i$ or as $p ( i )$ accordingly. The paths in ${ \mathcal { P } } ^ { \prime \prime }$ then avoid $\cup \mathcal { P } _ { n } ( Q _ { j } )$ for all $j \leqslant n - 1$ (with ${ \mathcal { P } } _ { n } ( Q _ { j } ) : = { \mathcal { P } } _ { n - 1 } ( Q _ { j } )$ for $j \neq i$ ) and $Q _ { j }$ for all $j \neq p ( n )$ . We rename ${ \mathcal { P } } ^ { \prime \prime }$ as $\mathcal { P }$ , to simplify notation.

In either case, we have now defined $\mathcal { P } _ { n } ( Q _ { i } )$ for all $i < n$ so as to satisfy (5) for $n$ , chosen $p ( n )$ as in (2), and found an infinite set $\mathcal { P }$ of disjoint $R _ { n } ^ { 0 } { - } Q _ { p ( n ) }$ paths that avoid all other $Q _ { j }$ and all the sets $\textstyle { \mathcal { P } } _ { n } ( Q _ { i } )$ . All that can prevent us from choosing $R _ { n } ^ { 0 }$ as $Q _ { n }$ and $\mathcal { P }$ as ${ \mathcal { P } } _ { n } ( Q _ { n } )$ and $\mathcal { R } _ { n + 1 } \leqslant \mathcal { R } _ { n } \setminus \left\{ \mathcal { R } _ { n } ^ { \cup } \right\}$ is condition (4): if $\mathcal { P }$ meets all but finitely many rays in $\mathcal { R } _ { n }$ infinitely, we cannot find an infinite set $\mathcal { R } _ { n + 1 } \leqslant \mathcal { R } _ { n }$ of rays avoiding $\mathcal { P }$ .

However, we may now assume the following:

Whenever $R \in \mathcal { R } _ { n }$ and $\mathcal { P } ^ { \prime } \leqslant \mathcal { P }$ is an infinite set of $R$ – $Q _ { p ( n ) }$ paths, there is a ray $R ^ { \prime } \ne R$ in $\mathcal { R } _ { n }$ that meets ${ \mathcal { P } } ^ { \prime }$ infinitely. $( * )$

For if ( ) failed, we could choose $R$ as $Q _ { n }$ and ${ \mathcal { P } } ^ { \prime }$ as ${ \mathcal { P } } _ { n } ( Q _ { n } )$ , and select from every ray $R ^ { \prime } \ne R$ in $\mathcal { R } _ { n }$ a tail avoiding ${ \mathcal { P } } ^ { \prime }$ to form $\mathcal { R } _ { n + 1 }$ . This would satisfy conditions (1)–(5) for $n$ .

Consider the paths in $\mathcal { P }$ as linearly ordered by the natural order of their starting vertices on $R _ { n } ^ { 0 }$ . This induces an ordering on every $\mathcal { P } ^ { \prime } \leqslant \mathcal { P }$ . If ${ \mathcal { P } } ^ { \prime }$ is a set of $R$ – $Q _ { p ( n ) }$ paths for some ray $R$ , we shall call this ordering of ${ \mathcal { P } } ^ { \prime }$ compatible with $R$ if the ordering it induces on the first vertices of its paths coincides with the natural ordering of those vertices on $R$ .

Using assumption $( * )$ , let us choose two sequences $R _ { n } ^ { 0 } , R _ { n } ^ { 1 } , \ldots$ . and $\mathcal { P } ^ { \mathrm { 0 } } \geqslant \mathcal { P } ^ { \mathrm { 1 } } \geqslant . . .$ such that every $R _ { n } ^ { k }$ is a tail of a ray in $\mathcal { R } _ { n }$ and each $\mathcal { P } ^ { k }$ n is an infinite set of Rkn–Qp(n) paths whose ordering is compatible

$\mathcal { P } _ { n } ( Q _ { i } )$ for $i < n$

$p ( n )$ , $_ { \mathcal { P } }$

with $R _ { n } ^ { k }$ . The first path of $\mathcal { P } ^ { k }$ in this ordering will be denoted by $P _ { k }$ , its nstarting vertex on $R _ { n } ^ { k }$ by $v _ { k }$ P, and the path in $\mathcal { P } ^ { k - 1 }$ containing $P _ { k }$ k by $P _ { k } ^ { - }$ (Fig. 8.2.2). Clearly, $\mathcal { P } _ { 0 } : = \mathcal { P }$ is as required for $k = 0$ ; put $P _ { 0 } ^ { - } : = P _ { 0 }$ . For $k \geqslant 1$ P P, we may use (∗) with $R \supseteq R _ { n } ^ { k - 1 }$ and $\mathcal { P } ^ { \prime } = \mathcal { P } ^ { k - 1 }$ 0to find in $\mathcal { R } _ { n }$ a ray $R ^ { \prime } \nsupseteq R _ { n } ^ { k - 1 }$ that meets $\mathcal { P } ^ { k - 1 }$ ⊇ n P P infinitely but has a tail $R _ { n } ^ { k }$ R  avoiding the finite subgraph $P _ { 0 } ^ { - } \cup \ldots \cup P _ { k - 1 } ^ { - }$ . Let $P _ { k } ^ { - }$ be a path in $\mathcal { P } ^ { k - 1 }$ that meets $R _ { n } ^ { k }$ and let $\boldsymbol { v }$ be its ‘highest’ vertex on $R _ { n } ^ { k }$ , that is, the last vertex of $R _ { n } ^ { k }$ i n $V ( P _ { k } ^ { - } )$ . Replacing $R _ { n } ^ { k }$ with its tail $v R _ { n } ^ { k }$ , we can arrange that $P _ { k } ^ { - }$ k has only the vertex $\boldsymbol { v }$ on $R _ { n } ^ { k }$ . Then $P _ { k } : = v P _ { k } ^ { - }$ is an $R _ { n } ^ { k } { - } Q _ { p ( n ) }$ path starting at $\mathit { v } _ { k } ~ = ~ \mathit { v }$ . We may now select an infinite set $\mathcal { P } ^ { k } \leqslant \mathcal { P } ^ { k - 1 }$ of $R _ { n } ^ { k } { - } Q _ { p ( n ) }$ paths compatible with $R _ { n } ^ { k }$ and containing $P _ { k }$ is its first path.

![](images/c21f80b23c86fb63c7e057c1dc0726c8c49f987c3cc18aaa33d4f046189cb123.jpg)  
Fig. 8.2.2. Constructing $Q _ { n }$ from condition (∗)

Since $P _ { k } ^ { - }$ contains $v _ { k } \in R _ { n } ^ { k }$ but $R _ { n } ^ { k } \cap P _ { k - 1 } = \emptyset$ , we have $P _ { k } ^ { - } \neq P _ { k - 1 }$ , so the $P _ { k }$ are all disjoint. For each $k$ , let $v _ { k + 1 } ^ { - }$ denote the starting vertex of $P _ { k + 1 } ^ { - }$ on $R _ { n } ^ { k }$ , and put $R _ { n + 1 } ^ { k } : = \mathring { v } _ { k + 1 } ^ { - } R _ { n } ^ { k }$ k+1 . Then let

v−k Rkn+

$$
Q _ {n} := v _ {0} R _ {n} ^ {0} v _ {1} ^ {-} P _ {1} ^ {-} v _ {1} R _ {n} ^ {1} v _ {2} ^ {-} P _ {2} ^ {-} v _ {2} R _ {n} ^ {2} \dots
$$

$$
\mathcal {P} _ {n} (Q _ {n}) := \left\{P _ {0}, P _ {1}, P _ {2}, \dots \right\}
$$

$$
\mathcal {R} _ {n + 1} := \left\{R _ {n + 1} ^ {k} \mid k \in \mathbb {N} \right\}.
$$

Let us check that these definitions satisfy (1)–(5) for $n$ . We have already verified (2) and (5). For the disjointness requirements in (1) and (3), recall that $Q _ { n }$ and ${ \mathcal { P } } _ { n } ( Q _ { n } )$ consist of segments of paths in $\mathcal { R } _ { n }$ and $\mathcal { P }$ ; these are disjoint from $Q _ { i }$ and $\mathcal { P } _ { n } ( Q _ { i } )$ for all $i < n$ by definition of $\mathcal { P }$ and (4) for $n - 1$ (together with (5) for $n$ ). For the disjointness P −requirement in (4) note that $R _ { n + 1 } ^ { k }$ does not meet $Q _ { n }$ or ${ \mathcal { P } } _ { n } ( Q _ { n } )$ inside

any path $P _ { j } ^ { - }$ with $j > k + 1$ , since these $P _ { j } ^ { - }$ are proper final segments of $R _ { n } ^ { k } { - } Q _ { p ( n ) }$ paths in $\mathcal { P } ^ { k }$ . Since $R _ { n + 1 } ^ { k }$ j does not, by definition, meet $Q _ { n }$ or ${ \mathcal { P } } _ { n } ( Q _ { n } )$ inside any path $P _ { j } ^ { - }$ with $j \leqslant k + 1$ , condition (4) holds for $n$ .

It remains to use our rays $Q _ { n }$ , path systems $\textstyle { \mathcal { P } } _ { n } ( Q _ { i } )$ , and supply sets $\mathcal { R } _ { n }$ of rays to construct the desired grid. By the infinity lemma (8.1.2), there is a sequence $n _ { 0 } < n _ { 1 } < n _ { 2 } < . . .$ such that either $p ( n _ { i } ) = n _ { i - 1 }$ for every $i \geqslant 1$ or $p ( n _ { i } ) = n _ { 0 }$ for every $i \geqslant 1$ . We treat these two cases in turn.

In the first case, let us assume for notational simplicity that $n _ { i } = i$ for all $_ i$ , i.e. discard any $Q _ { n }$ with $n \not \in \{ n _ { 0 } , n _ { 1 } , . . . \}$ . Then for every $i \geqslant 1$ and every $n \geqslant i$ we have an infinite set $\textstyle { \mathcal { P } } _ { n } ( Q _ { i } )$ of disjoint $Q _ { i }$ – $Q _ { i - 1 }$ paths. Our aim is to choose tails $Q _ { n } ^ { \prime }$ of our rays $Q _ { n }$ that will correspond to the vertical rays $U _ { n } \subseteq H ^ { \infty }$ , and paths $S _ { 1 } , S _ { 2 } , \ldots$ between the $Q _ { n } ^ { \prime }$ that will correspond to the horizontal edges $e _ { 1 } , e _ { 2 } , \ldots$ of $H ^ { \infty }$ . We shall find the paths $S _ { 1 } , S _ { 2 } , \ldots$ inductively, choosing the $Q _ { n } ^ { \prime }$ as needed as we go along (but also in the order of increasing $n$ , starting with $Q _ { 0 } ^ { \prime } : = Q _ { 0 }$ ). At every step of the construction, we shall have selected only finitely many $S _ { k }$ and only finitely many $Q _ { n } ^ { \prime }$ .

Let $k$ and $n$ be minimal such that $S _ { k }$ and $Q _ { n } ^ { \prime }$ are still undefined. We describe how to choose $S _ { k }$ , and $Q _ { n } ^ { \prime }$ if the definition of $S _ { k }$ requires it. Let $i$ be such that $e _ { k }$ joins $U _ { i - 1 }$ to $U _ { i }$ in $H ^ { \infty }$ . If $i = n$ , let $Q _ { n } ^ { \prime }$ be a tail of $Q _ { n }$ that avoids the finitely many paths $S _ { 1 } , \ldots , S _ { k - 1 }$ ; otherwise, $Q _ { i } ^ { \prime }$ has already been defined, and so has $Q _ { i - 1 } ^ { \prime }$ . Now choose $S _ { k } \in \mathcal P _ { n } ( Q _ { i } )$ ‘high enough’ between $Q _ { i - 1 } ^ { \prime }$ and $Q _ { i } ^ { \prime }$ to mirror the position of $e _ { k }$ in $H ^ { \infty }$ , and to avoid $S _ { 1 } \cup \ldots \cup S _ { k - 1 }$ . By (3)(ii), $S _ { k }$ will also avoid every other $Q _ { j } ^ { \prime }$ already defined. Since every $Q _ { n } ^ { \prime }$ is chosen so as to avoid all previously defined $S _ { k }$ , and every $S _ { k }$ avoids all previously defined $Q _ { j } ^ { \prime }$ (except $Q _ { i - 1 } ^ { \prime }$ and $Q _ { i } ^ { \prime }$ ), the $Q _ { n } ^ { \prime }$ and $S _ { k }$ are pairwise disjoint for all $n , k \in \mathbb { N }$ , except for the required incidences. Our construction thus yields the desired subdivision of $H ^ { \infty }$ .

It remains to treat the case that $p ( n _ { i } ) = n _ { 0 }$ for all $i \geqslant 1$ . Let us rename $Q _ { n _ { 0 } }$ as $Q$ , and $n _ { i }$ as $i - 1$ for $i \geqslant 1$ . Then our sets $\textstyle { \mathcal { P } } _ { n } ( Q _ { i } )$ consist of disjoint $Q _ { i }$ – $Q$ paths. We choose rays $Q _ { n } ^ { \prime } \subseteq Q _ { n }$ and paths $S _ { k }$ inductively as before, except that $S _ { k }$ now consists of three parts: an initial segment from $\mathcal { P } _ { n } ( Q _ { i - 1 } )$ , followed by a middle segment from $Q$ , and a final segment from $\textstyle { \mathcal { P } } _ { n } ( Q _ { i } )$ . Such $S _ { k }$ can again be found, since at every stage of the construction only a finite part of $Q$ has been used. 

# 8.3 Homogeneous and universal graphs

Unlike finite graphs, infinite graphs offer the possibility to represent an entire graph property $\mathcal { P }$ by just one specimen, a single graph that contains all the graphs in $\mathcal { P }$ up to some fixed cardinality. Such graphs are called ‘universal’ for this property.

More precisely, if $\leqslant$ is a graph relation (such as the minor, topological minor, subgraph, or induced subgraph relation up to isomorphism), we call a countable graph $G ^ { * }$ universal in $\mathcal { P }$ (for $\leqslant$ ) if $G ^ { \ast } \in \mathcal { P }$ and $G \leqslant G ^ { * }$ for every countable graph $G \in { \mathcal { P } }$ .

Is there a graph that is universal in the class of all countable graphs? Suppose a graph $R$ has the following property:

Whenever $U$ and $W$ are disjoint finite sets of vertices in $R$ , there exists a vertex $v \in R - U - W$ that is adjacent in $R$ (∗) to all the vertices in $U$ but to none in $W$ .

Then $R$ is universal even for the strongest of all graph relations, the induced subgraph relation. Indeed, in order to embed a given countable graph $G$ in $R$ we just map its vertices $v _ { 1 } , v _ { 2 } , \ldots$ to $R$ inductively, making sure that $v _ { n }$ gets mapped to a vertex $v \in R$ adjacent to the images of all the neighbours of $v _ { n }$ in $G [ v _ { 1 } , \ldots , v _ { n } ]$ but not adjacent to the image of any non-neighbour of $v _ { n }$ in $G [ v _ { 1 } , \ldots , v _ { n } ]$ . Clearly, this map is an isomorphism between $G$ and the subgraph of $R$ induced by its image.

# Theorem 8.3.1. (Erd˝os and R´enyi 1963)

There exists a unique countable graph $R$ with property $( * )$ .

Proof . To prove existence, we construct a graph $R$ with property $( * )$ inductively. Let $R _ { 0 } : = K ^ { 1 }$ . For all $n \in \mathbb N$ , let $R _ { n + 1 }$ be obtained from $R _ { n }$ by adding for every set $U \subseteq V ( R _ { n } )$ a new vertex $\boldsymbol { v }$ joined to all the vertices in $U$ but to none outside $U$ . (In particular, the new vertices form an independent set in $R _ { n + 1 }$ .) Clearly $R : = \textstyle \bigcup _ { n \in \mathbb { N } } R _ { n }$ has property (∗).

To prove uniqueness, let $R = ( V , E )$ and $R ^ { \prime } = ( V ^ { \prime } , E ^ { \prime } )$ be two graphs with property ( ), each given with a fixed vertex enumeration. We construct a bijection $\varphi \colon V \to V ^ { \prime }$ in an infinite sequence of steps, defining $\varphi ( v )$ for one new vertex $v \in V$ at each step.

At every odd step we look at the first vertex $v$ in the enumeration of $V$ for which $\varphi ( v )$ has not yet been defined. Let $U$ be the set of those of its neighbours $u$ in $R$ for which $\varphi ( u )$ has already been defined. This is a finite set. Using ( ) for $R ^ { \prime }$ , find a vertex $\boldsymbol { v } ^ { \prime } \in V ^ { \prime }$ that is adjacent in $R ^ { \prime }$ to all the vertices in $\varphi ( U )$ but to no other vertex in the image of $\varphi$ (which, so far, is still a finite set). Put $\varphi ( v ) : = v ^ { \prime } $ .

At even steps in the definition process we do the same thing with the roles of $R$ and $R ^ { \prime }$ interchanged: we look at the first vertex $v ^ { \prime }$ in the enumeration of $V ^ { \prime }$ that does not yet lie in the image of $\varphi$ , and set $\varphi ( v ) = v ^ { \prime }$ for a vertex $v$ that matches the adjacencies and non-adjacencies of $v ^ { \prime }$ among the vertices for which $\varphi$ (resp. $\varphi ^ { - 1 }$ ) has already been defined.

By our minimum choices of $v$ and $v ^ { \prime }$ , the bijection gets defined on all of $V$ and all of $V ^ { \prime }$ , and it is clearly an isomorphism. 

universal

[ 11.3.5 ]

R

Rado graph

‘the’ random graph

The graph $R$ in Theorem 8.3.1 is usually called the Rado graph, named after Richard Rado who gave one of its earliest explicit definitions. The method of constructing a bijection in alternating steps, as in the uniqueness part of the proof, is known as the back-and-forth technique.

The Rado graph $R$ is unique in another rather fascinating respect. We shall hear more about this in Chapter 11.3, but in a nutshell it is the following. If we generate a countably infinite random graph by admitting its pairs of vertices as edges independently with some fixed positive probability $p \in ( 0 , 1 )$ , then with probability 1 the resulting graph has property ( $^ *$ ), and is hence isomorphic to $R$ ! In the context of infinite graphs, the Rado graph is therefore also called the (countably infinite) random graph.

As one would expect of a random graph, the Rado graph shows a high degree of uniformity. One aspect of this is its resilience against small changes: the deletion of finitely many vertices or edges, and similar local changes, leave it ‘unchanged’ and result in just another copy of $R$ (Exercise 41).

The following rather extreme aspect of uniformity, however, is still surprising: no matter how we partition the vertex set of $R$ into two parts, at least one of the parts will induce another isomorphic copy of $R$ . Trivial examples aside, the Rado graph is the only countable graph with this property, and hence unique in yet another respect:

Proposition 8.3.2. The Rado graph is the only countable graph $G$ other than $K ^ { \aleph _ { 0 } }$ and $\overline { { K ^ { \aleph _ { 0 } } } }$ such that, no matter how $V ( G )$ is partitioned into two parts, one of the parts induces an isomorphic copy of $G$ .

Proof . We first show that the Rado graph $R$ has the partition property. Let $\{ V _ { 1 } , V _ { 2 } \}$ be a partition of $V ( R )$ . If ( ) fails in both $R \left[ V _ { 1 } \right]$ and $R [ V _ { 2 } ]$ , say for sets $U _ { 1 } , W _ { 1 }$ and $U _ { 2 } , W _ { 2 }$ , respectively, then ( ) fails for $U = U _ { 1 } \cup U _ { 2 }$ and $W = W _ { 1 } \cup W _ { 2 }$ in $R$ , a contradiction.

To show uniqueness, let $G = ( V , E )$ be a countable graph with the partition property. Let $V _ { 1 }$ be its set of isolated vertices, and $V _ { 2 }$ the rest. If $V _ { 1 } \neq \emptyset$ then $G \not \simeq G [ V _ { 2 } ]$ , since $G$ has isolated vertices but $G \left[ V _ { 2 } \right]$ does not. Hence $G = G \left[ V _ { 1 } \right] \simeq K ^ { \aleph _ { 0 } }$ . Similarly, if $G$ has a vertex adjacent to all other vertices, then $G = K ^ { \aleph _ { 0 } }$ .

Assume now that $G$ has no isolated vertex and no vertex joined to all other vertices. If $G$ is not the Rado graph then there are sets $U , W$ for which $( * )$ fails in $G$ ; choose these with $| U \cup W |$ minimum. Assume first that $U \neq \emptyset$ , and pick $u \in U$ . Let $V _ { 1 }$ consist of $u$ and all vertices outside $U \cup W$ that are not adjacent to $u$ , and let $V _ { 2 }$ contain the remaining vertices. As $u$ is isolated in $G \left[ V _ { 1 } \right]$ , we have $G \not \simeq G [ V _ { 1 } ]$ and hence $G \simeq G [ V _ { 2 } ]$ . By the minimality of $| U \cup W |$ , there is a vertex $v \in G \lfloor V _ { 2 } \rfloor - U - W$ that is adjacent to every vertex in $U \backslash \{ u \}$ and to none in $W$ . But $v$ is also adjacent to $u$ , because it lies in $V _ { 2 }$ . So $U$ , $W$ and $v$ satisfy $( * )$ for $G$ , contrary to assumption.

Finally, assume that $U = \emptyset$ . Then $W \neq \emptyset$ . Pick $w \in W$ , and consider the partition $\left\{ V _ { 1 } , V _ { 2 } \right\}$ of $V$ where $V _ { 1 }$ consists of $w$ and all its neighbours outside $W$ . As before, $G \not \simeq G [ V _ { 1 } ]$ and hence $G \simeq G [ V _ { 2 } ]$ . Therefore $U$ and $W \setminus \{ w \}$ satisfy $( * )$ in $G \left[ V _ { 2 } \right]$ , with $v \in V _ { 2 } \setminus W$ say, and then $U , W , v$ satisfy ( ) in $G$ . 

Another indication of the high degree of uniformity in the structure of the Rado graph is its large automorphism group. For example, $R$ is easily seen to be vertex-transitive: given any two vertices $x$ and $y$ , there is an automorphism of $R$ mapping $x$ to $y$ .

In fact, much more is true: using the back-and-forth technique, one can easily show that the Rado graph is homogeneous: every isomorphism between two finite induced subgraphs can be extended to an automorphism of the entire graph (Exercise 42).

Which other countable graphs are homogeneous? The complete graph $K ^ { \aleph _ { 0 } }$ and its complement are again obvious examples. Moreover, for every integer $r \geqslant 3$ there is a homogeneous $K ^ { r }$ -free graph $R ^ { r }$ , constructed as follows. Let $R _ { 0 } ^ { r } : = K ^ { 1 }$ , and let $R _ { n + 1 } ^ { r }$ be obtained from $R _ { n } ^ { r }$ by joining, for every subgraph $H \not \simeq K ^ { r - 1 }$ of $R _ { n } ^ { r }$ , a new vertex $v _ { H }$ to every vertex in $H$ . Then let $\textstyle R ^ { r } : = \bigcup _ { n \in \mathbb { N } } R _ { n } ^ { r }$ . Clearly, as the new vertices $v _ { H }$ of $R _ { n + 1 } ^ { r }$ n are independent, there is no $K ^ { r }$ i n $R _ { n + 1 } ^ { r }$ if there was none in $R _ { n } ^ { r }$ , so $R ^ { r } \nsubseteq K ^ { r }$ by induction on $n$ . Just like the Rado graph, $R ^ { r }$ is clearly universal among the $K ^ { \prime }$ -free countable graphs, and it is clearly homogeneous.

By the following deep theorem of Lachlan and Woodrow, the countable homogeneous graphs we have seen so far are essentially all:

# Theorem 8.3.3. (Lachlan & Woodrow 1980)

Every countably infinite homogeneous graph is one of the following:

a disjoint union of complete graphs of the same order, or the complement of such a graph;   
• the graph $R ^ { r }$ or its complement, for some $r \geqslant 3$   
the Rado graph $R$ .

To conclude this section, let us return to our original problem: for which graph properties is there a graph that is universal with this property? Most investigations into this problem have addressed it from a more general model-theoretic point of view, and have therefore been based on the strongest of all graph relations, the induced subgraph relation. Unfortunately, most of these results are negative; see the notes.

From a graph-theoretic point of view, it seems more promising to look instead for universal graphs for the weaker subgraph relation, or even the topological minor or minor relation. For example, while there is no universal planar graph for subgraphs or induced subgraphs, there is one for minors:

# Theorem 8.3.4. (Diestel & K¨uhn 1999)

There exists a universal planar graph for the minor relation.

So far, this theorem is the only one of its kind. But it should be possible to find more. For instance: for which graphs $X$ is there a minoruniversal graph in the class For $\operatorname { \partial } _ { \prec } ( X ) = \{ G \mid X \not \prec G \} ^ { \cdot }$ ?

# 8.4 Connectivity and matching

In this section we look at infinite versions of Menger’s theorem and of the matching theorems from Chapter 2. This area of infinite graph theory is one of its best developed fields, with several deep results. One of these, however, stands out among the rest: a version of Menger’s theorem that had been conjectured by Erd˝os and was proved only recently by Aharoni and Berger. The techniques developed for its proof inspired, over the years, much of the theory in this area.

We shall prove this theorem for countable graphs, which will take up most of this section. Although the countable case is much easier, it is still quite hard and will give a good impression of the general proof. We then wind up with an overview of infinite matching theorems and a conjecture conceived in the same spirit.

Recall that Menger’s theorem, in its simplest form, says that if $A$ L and $B$ are sets of vertices in a finite graph $G$ , not necessarily disjoint, and if $k = k ( G , A , B )$ is the minimum number of vertices separating $A$ L from $B$ in $G$ , then $G$ contains $k$ disjoint $A$ – $B$ paths. (Clearly, it cannot contain more.) The same holds, and is easily deduced from the finite case, when $G$ is infinite but $k$ is still finite:

Proposition 8.4.1. Let $G$ be any graph, $k \in \mathbb N$ , and let $A , B$ be two sets of vertices in $G$ that can be separated by $k$ but no fewer than $k$ vertices. Then $G$ contains $k$ disjoint $A$ – $B$ paths.

(3.3.1) Proof . By assumption, every set of disjoint $A$ – $B$ paths has cardinality at most $k$ . Choose one, $\mathcal { P }$ say, of maximum cardinality. Suppose $| \mathcal { P } | < k$ . Then no set $X$ consisting of one vertex from each path in $\mathcal { P }$ separates $A$ 1 from $B$ . For each $X$ , let $P _ { X }$ be an $A$ – $B$ path avoiding $X$ . Let $H$ be the union of $\cup \mathcal { P }$ with all these paths $P _ { X }$ . This is a finite graph in which no set of $| \mathcal { P } |$ vertices separates $A$ from $B$ . So $H \subseteq G$ contains more than $| \mathcal { P } |$ paths from $A$ to $B$ by Menger’s theorem (3.3.1), which contradicts the choice of $\mathcal { P }$ . 

When $k$ is infinite, however, the result suddenly becomes trivial. Indeed, let $\mathcal { P }$ be any maximal set of disjoint $A$ – $B$ paths in $G$ . Then the union of all these paths separates $A$ from $B$ , so $\mathcal { P }$ must be infinite. But

then the cardinality of this union is no bigger than $| \mathcal { P } |$ . Thus, $\mathcal { P }$ contains $| \mathcal { P } | = | \bigcup \mathcal { P } | \geqslant k$ disjoint $A$ – $B$ paths, as desired.

Of course, this is no more than a trick played on us by infinite cardinal arithmetic: although, numerically, the $A$ – $B$ separator consisting of all the inner vertices of paths in $\mathcal { P }$ is no bigger than $| \mathcal { P } |$ , it uses far more vertices to separate $A$ from $B$ than should be necessary. Or put another way: when our path systems and separators are infinite, their cardinalities alone are no longer a sufficiently fine tool to distinguish carefully chosen ‘small’ separators from unnecessarily large and wasteful ones.

To overcome this problem, Erd˝os suggested an alternative form of Menger’s theorem, which for finite graphs is clearly equivalent to the standard version. Recall that an $A$ – $B$ separator $X$ is said to lie on a set $\mathcal { P }$ of disjoint $A$ – $B$ paths if $X$ consists of a choice of exactly one vertex from each path in $\mathcal { P }$ . The following so-called Erd˝os-Menger conjecture, now a theorem, influenced much of the development of infinite connectivity and matching theory:

# Theorem 8.4.2. (Aharoni & Berger 2005)

Let $G$ be any graph, and let $A , B \subseteq V ( G )$ . Then $G$ contains a set $\mathcal { P }$ of disjoint $A$ – $B$ paths and an $A$ – $B$ separator on $\mathcal { P }$ .

The next few pages give a proof of Theorem 8.4.2 for countable $G$ .

Of the three proofs we gave for the finite case of Menger’s theorem, only the last has any chance of being adaptable to the infinite case: the others were by induction on $| \mathcal { P } |$ or on $| G | + \| G \|$ , and both these parameters may now be infinite. The third proof, however, looks more promising: recall that, by Lemmas 3.3.2 and 3.3.3, it provided us with a tool to either find a separator on a given system of $A$ – $B$ paths, or to construct another system of $A$ – $B$ paths that covers more vertices in $A$ and in $B$ .

Lemmas 3.3.2 and 3.3.3 (whose proofs work for infinite graphs too) will indeed form a cornerstone of our proof for Theorem 8.4.2. However, it will not do just to apply these lemmas infinitely often. Indeed, although any finite number of applications of Lemma 3.3.2 leaves us with another system of disjoint $A$ – $B$ paths, an infinite number of iterations may leave nothing at all: each edge may be toggled on and off infinitely often by successive alternating paths, so that no ‘limit system’ of $A$ – $B$ 2 paths will be defined. We shall therefore take another tack: starting at $A$ , we grow simultaneously as many disjoint paths towards $B$ as possible.

To make this precise, we need some terminology. Given a set $X \subseteq$ $V ( G )$ , let us write $G _ { X  B }$ for the subgraph of $G$ induced by $X$ and all the components of $G - X$ that meet $B$ .

Let ${ \mathcal { W } } = ( W _ { a } \mid a \in A )$ be a family of disjoint paths such that every $W _ { a }$ starts in $a$ . We call $\mathcal { W }$ an $A  B$ wave in $G$ if the set $Z$ of final vertices of paths in $\mathcal { W }$ separates $A$ from $B$ in $G$ . (Note that $\mathcal { W }$ may contain infinite paths, which have no final vertex.) Sometimes, we shall

wish to consider $A  B$ waves in subgraphs of $G$ that contain $A$ but not all of $B$ . For this reason we do not formally require that $B \subseteq V ( G )$ .

![](images/c290b9ae8e5d0fb004e9144e8fea65b591ac816a30d803e0a853cb75067f1c81.jpg)  
Fig. 8.4.1. A small $A  B$ wave $\mathcal { W }$ with boundary $X$

boundary $( \mathcal { W } , X )$

large/small proper

$u + \nu$

-

limit wave

maximal wave

When $\mathcal { W }$ is a wave, then the set $X \subseteq Z$ of those vertices in $Z$ that either lie in $B$ or have a neighbour in $G _ { Z  B } - Z$ is a minimal A–B separator in $G$ ; note that $z \in \ Z$ lies in $X$ if and only if it can be linked to $B$ by a path that has no vertex other than $z$ on $\mathcal { W }$ . We call $X$ the boundary of $\mathcal { W }$ , and often use $( \mathcal W , X )$ as shorthand for the wave $\mathcal { W }$ together with its boundary $X$ . If all the paths in $\mathcal { W }$ are finite and $X = Z$ , we call the wave $\mathcal { W }$ large; otherwise it is small . We shall call $\mathcal { W }$ proper if at least one of the paths in $\mathcal { W }$ is non-trivial, or if all its paths are trivial but its boundary is a proper subset of $A$ . Every small wave, for example, is proper. Note that while some $A  B$ wave always exists, e.g. the family $( \{ a \} | a \in A )$ ) of singleton paths, $G$ need not have a proper $A  B$ wave. (For example, if $A$ consists of two vertices of $G = K ^ { 1 0 }$ and $B$ of three other vertices, there is no proper $A  B$ wave.)

If $( \mathcal { U } , X )$ is an $A  B$ wave in $G$ and $( \nu , Y )$ is an $X  B$ wave in $G _ { X  B }$ , then the family $\mathscr { W } = \mathscr { U } + \mathscr { V }$ obtained from $\boldsymbol { \mathcal { U } }$ by appending the paths of $\nu$ (to those paths of $\boldsymbol { \mathcal { U } }$ that end in $X$ ) is clearly an $A  B$ wave in $G$ , with boundary $Y$ . Note that $\mathcal { W }$ is large if and only if both $\nu$ and $\boldsymbol { u }$ are large. $\mathcal { W }$ is greater than $\boldsymbol { u }$ in the following sense.

Given two path systems $\mathcal { U } = ( U _ { a } \vert a \in A )$ and ${ \mathcal { W } } = ( W _ { a } \mid a \in A )$ , write $u \leqslant \mathcal { W }$ if $U _ { a } \subseteq W _ { a }$ for every $a \in A$ . Given a chain $( \mathcal { W } ^ { i } , X ^ { i } ) _ { i \in I }$ of waves in this ordering, with $\mathcal { W } ^ { i } = ( W _ { a } ^ { i } \ | \ a \in A )$ ) say, let $\mathcal { W } ^ { \ast } = \left( W _ { a } ^ { \ast } \right)$ $a \in A$ ) be defined by $\textstyle W _ { a } ^ { * } : = \bigcup _ { i \in I } W _ { a } ^ { \iota }$ . Then $\mathcal { W } ^ { \ast }$ is an $A  B$ wave: any $A$ – $B$ path is finite but meets every $X ^ { i }$ , so at least one of its vertices lies in $X ^ { i }$ for arbitrarily large $( \mathcal { W } ^ { i } , X ^ { i } )$ and hence is the final vertex of a path in $\mathcal { W } ^ { \ast }$ . Clearly $\mathcal { W } ^ { i } \leqslant \mathcal { W } ^ { * }$ for all $i \in I$ ; we call $\mathcal { W } ^ { \ast }$ the limit of the waves $\mathcal { W } ^ { i }$ .

As every chain of $A  B$ waves is bounded above by its limit wave, Zorn’s lemma implies that $G$ has a maximal $A  B$ wave $\mathcal { W }$ ; let $X$ be its boundary. This wave $( \mathcal W , X )$ forms the first step in our proof for Theorem 8.4.2: if we can now find disjoint paths in $G _ { X  B }$ linking all

the vertices of $X$ to $B$ , then $X$ will be an $A$ – $B$ separator on these paths preceded by the paths of $\mathcal { W }$ that end in $X$ .

By the maximality of $\mathcal { W }$ , there is no proper $X  B$ wave in $G _ { X  B }$ . For our proof it will thus suffice to prove the following (renaming $X$ as $A$ ):

Lemma 8.4.3. If $G$ has no proper $A  B$ wave, then $G$ contains a set of disjoint $A$ – $B$ paths linking all of $A$ to $B$ .

Our approach to the proof of Lemma 8.4.3 is to enumerate the vertices in $A = : \{ a _ { 1 } , a _ { 2 } , \dots \}$ , and to find the required $A$ – $B$ paths $P _ { n } =$ $a _ { n } \ldots b _ { n }$ in turn for $n = 1 , 2 , \ldots$ . Since our premise in Lemma 8.4.3 is that $G$ has no proper $A  B$ wave, we would like to choose $P _ { 1 }$ so that $G - P _ { 1 }$ has no proper $( A \setminus \{ a _ { 1 } \} ) \to B$ wave: this would restore the same premise to $G - P _ { 1 }$ , and we could proceed to find $P _ { 2 }$ in $G - P _ { 1 }$ in the same way.

We shall not be able to choose $P _ { 1 }$ just like this, but we shall be able to do something almost as good. We shall construct $P _ { 1 }$ so that deleting it (as well as a few more vertices outside $A$ ) leaves a graph that has a large maximal $( A \setminus \{ a _ { 1 } \} ) \to B$ wave $( \mathcal { W } , A ^ { \prime } )$ . We then earmark the paths $W _ { n } = a _ { n } \ldots a _ { n } ^ { \prime }$ ( $n \geqslant 2$ ) of this wave as initial segments for the paths $P _ { n }$ . By the maximality of $\mathcal { W }$ , there is no proper $A ^ { \prime }  B$ wave in $G _ { A ^ { \prime }  B }$ . In other wordn find there an have r path iginal p. Then $G _ { A ^ { \prime }  B }$ $A ^ { \prime } { - } B$ $P _ { 2 } ^ { \prime } = a _ { 2 } ^ { \prime } \ldots b _ { 2 }$ $P _ { 2 } : = a _ { 2 } W _ { 2 } a _ { 2 } ^ { \prime } P _ { 2 } ^ { \prime }$ is our second path for Lemma 8.4.3, and we continue inductively inside $G _ { A ^ { \prime }  B }$ .

Given a set $\hat { A }$ of vertices in $G$ , let us call a vertex $a \not \in { \hat { A } }$ linkable for $( G , { \hat { A } } , B )$ if $G - { \hat { A } }$ contains an $a$ – $B$ path $P$ and a set $X \supseteq V ( P )$ of vertices such that $G - X$ has a large maximal $\hat { A }  B$ wave. (The first such $a$ we shall be considering will be $a _ { 1 }$ , and $\hat { A }$ will be the set $\{  a _ { 2 } , a _ { 3 } , \ldots \}$ .)

Lemma 8.4.4. Let $a ^ { * } \in A$ and ${ \hat { A } } : = A \setminus \left\{ a ^ { * } \right\}$ , and assume that $G$ has no proper $A  B$ wave. Then $a ^ { * }$ is linkable for $( G , { \hat { A } } , B )$ .

Proof of Lemma 8.4.3 (assuming Lemma 8.4.4). Let $G$ be as in Lemma 8.4.3, i.e. assume that $G$ has no proper $A  B$ wave. We construct subgraphs $G _ { 1 } , G _ { 2 } , \ldots$ of $G$ satisfying the following statement (Fig. 8.4.2):

$G _ { n }$ contains a set $A ^ { n } = \{ a _ { n } ^ { n } , a _ { n + 1 } ^ { n } , a _ { n + 2 } ^ { n } , . . . \}$ of distinct vertices such that $G _ { n }$ has no proper $A ^ { n } \to B$ wave. In $G$ there are disjoint paths $P _ { i }$ ( $\textit { i } < \textit { n }$ ) and $W _ { i } ^ { n }$ $\mathit { \Delta } _ { i } \geqslant n$ ) ( ) starting at $a _ { i }$ . The $P _ { i }$ are disjoint from $G _ { n }$ and end in $B$ . The $W _ { i } ^ { n }$ end in $a _ { i } ^ { \scriptscriptstyle n }$ and are otherwise disjoint from $G _ { n }$ .

Clearly, the paths $P _ { 1 } , P _ { 2 } , \ldots$ will satisfy Lemma 8.4.3.

$$
\begin{array}{c} a _ {1}, a _ {2}, \ldots \\ P _ {n} \end{array}
$$

linkable

![](images/d80e9462c1df883e1344670aa1f1f2296f11e107effed83c7983ee30b54e690e.jpg)  
Fig. 8.4.2. $G _ { n }$ has no proper $A ^ { \pi } \longrightarrow B$ wave

Let $G _ { 1 } \ : = \ G$ , and put $a _ { i } ^ { 1 } : = a _ { i }$ and $W _ { i } ^ { 1 } : = \{ a _ { i } \}$ for all $i ~ \geqslant ~ 1$ . Since by assumption $G$ has no proper $A  B$ wave, these definitions satisfy $( * )$ for $n = 1$ . Suppose now that ( ) has been satisfied for $n$ . Put ${ \ddot { A } } _ { . } ^ { n } : = A ^ { n } \setminus \left\{ a _ { n } ^ { n } \right\}$ . By Lemma 8.4.4 applied to $G _ { n }$ , we can find in $G _ { n } - { \hat { A } } ^ { n }$ an $a _ { n } ^ { n } – B$ path $P$ and a set $X _ { n } \supseteq V ( P )$ such that $G _ { n } - X _ { n }$ has a large maximal $\hat { A } ^ { n } \to B$ wave $( \mathcal { W } , A ^ { n + 1 } )$ . Let $P _ { n }$ be the path $W _ { n } ^ { n } \cup P$ . For $i \geqslant n + 1$ , let $\boldsymbol { W } _ { i } ^ { n + 1 }$ be ${ W } _ { i } ^ { n }$ followed by the path of $\mathcal { W }$ n starting at $a _ { i } ^ { n }$ i, and call its last vertex $a _ { i } ^ { n + 1 }$ . By the maximality of $\mathcal { W }$ Wthere is no proper $A ^ { n + 1 }  B$ wave in $G _ { n + 1 } : = ( G _ { n } - X _ { n } ) _ { A ^ { n + 1 }  B }$ , so (∗) is satisfied for $n + 1$ . 

To complete our proof of Theorem 8.4.2, it remains to prove Lemma 8.4.4. For this, we need another lemma:

Lemma 8.4.5. Let $x$ be a vertex in $G - A$ . If $G$ has no proper $A  B$ wave but $G - x$ does, then every $A  B$ wave in $G - x$ is large.

Proof . Suppose $G - x$ has a small $A  B$ wave $( \mathcal W , X )$ . Put $B ^ { \prime } : =$ $X \cup \{ x \}$ , and let $\mathcal { P }$ denote the set of $A$ – $X$ paths in $\mathcal { W }$ (Fig. 8.4.3). If $G$ contains an A– $B ^ { \prime }$ separator $S$ on $\mathcal { P }$ , then replacing in $\mathcal { W }$ every $P \in \mathcal { P }$

![](images/ab9178294d0d4b4fb367027d4895db16f41262a96abeaf1f14a6f16ad783cf45.jpg)  
Fig. 8.4.3. A hypothetical small $A  B$ wave in $G - x$

with its initial segment ending in $S$ we obtain a small (and hence proper) $A  B$ wave in $G$ , which by assumption does not exist. By Lemmas 3.3.3 and 3.3.2, therefore, $G$ contains a set ${ \mathcal { P } } ^ { \prime }$ of disjoint $A { - } B ^ { \prime }$ paths exceeding $\mathcal { P }$ . The set of last vertices of these paths contains $X$ properly, and hence must be all of $B ^ { \prime } = X \cup \{ x \}$ . But $B ^ { \prime }$ separates $A$ from $B$ in $G$ , so we can turn $\mathcal { P } ^ { \prime }$ into an $A  B$ wave in $G$ by adding as singleton paths any vertices of $A$ it does not cover. As $x$ lies on ${ \mathcal { P } } ^ { \prime }$ but not in $A$ , this is a proper wave, which by assumption does not exist. 

Proof of Lemma 8.4.4. We inductively construct trees $T _ { 0 } \subseteq T _ { 1 } \subseteq . . .$ in $G - ( \hat { A } \cup B )$ and path systems $\mathcal { W } _ { 0 } \leqslant \mathcal { W } _ { 1 } \leqslant . . .$ in $G$ so that each $\mathcal { W } _ { n }$ is a large maximal $\hat { A } \to B$ wave in $G - T _ { n }$ .

Let $\mathcal { W } _ { 0 } : = ( \{ a \} | a \in \hat { A } )$ . Clearly, $\mathcal { W } _ { 0 }$ is an $\hat { A } \to B$ wave in $G - a ^ { * }$ , and it is large and maximal: if not, then $G - a ^ { * }$ has a proper $\hat { A }  B$ wave, and adding the trivial path $\{ a ^ { * } \}$ to this wave turns it into a proper $A  B$ wave (which by assumption does not exist). If $a ^ { \ast } \in B$ , the existence of $\mathcal { W } _ { 0 }$ makes $a ^ { * }$ linkable for $( G , { \hat { A } } , B )$ . So we assume that $a ^ { * } \notin B$ . Now $T _ { 0 } : = \{ a ^ { * } \}$ and $\mathcal { W } _ { 0 }$ are as desired.

Suppose now that $T _ { n }$ and $\mathcal { W } _ { n }$ have been defined, and let $A _ { n }$ denote the set of last vertices of the paths in $\mathcal { W } _ { n }$ . Since $\mathcal { W } _ { n }$ is large, $A _ { n }$ is its boundary, and since $\mathcal { W } _ { n }$ is maximal, $G _ { n } : = ( G - T _ { n } ) _ { A _ { n }  B }$ has no proper $A _ { n } \to B$ wave (Fig. 8.4.4).

![](images/01b28bd16197729d6546d74dee24aa8f00a884bb65d579ba396cd133b767782e.jpg)  
Fig. 8.4.4. As $\mathcal { W } _ { n }$ is maximal, $G _ { n }$ has no proper $A _ { n } \to B$ wave

Note that $A _ { n }$ does not separate $A$ from $B$ in $G$ : if it did, then $\textstyle \mathcal { W } _ { n } \cup \{ a ^ { * } \}$ would be a small $A  B$ wave in $G$ , which does not exist. Hence, $G - A _ { n }$ contains an $A$ – $B$ path $P$ , which meets $T _ { n }$ because $( { \mathcal { W } } _ { n } , A _ { n } )$ is a wave in $G - T _ { n }$ . Let $P _ { n }$ be such a path $P$ , chosen so that its vertex $p _ { n }$ following its last vertex $t _ { n }$ in $T _ { n }$ is chosen minimal in some fixed enumeration of $V ( G )$ . Note that $p _ { n } P _ { n } \subseteq G _ { n } - A _ { n }$ , by definition of $G _ { n }$ .

Now $P _ { n } ^ { \prime } = a ^ { * } T _ { n } t _ { n } P _ { n }$ is an $a ^ { * } { - } B$ path in $G - { \hat { A } } - A ^ { n }$ . If $G _ { n } - p _ { n } P _ { n }$ has no proper $A _ { n } \to B$ wave, then $\mathcal { W } _ { n }$ is large and maximal not only in

$G - T _ { n }$ but also in $G - T _ { n } - p _ { n } P _ { n }$ , and $a ^ { * }$ is linkable for $( G , { \hat { A } } , B )$ with $a ^ { * } – B$ path $P _ { n } ^ { \prime }$ and $X = V ( T _ { n } \cup p _ { n } P _ { n } )$ . We may therefore assume that $G _ { n } - p _ { n } P _ { n }$ has a proper $A _ { n } \to B$ wave.

Let $x _ { n }$ be the first vertex on $p _ { n } P _ { n }$ such that $G _ { n } - p _ { n } P _ { n } x _ { n }$ has a proper $A _ { n } \to B$ wave. Then $G _ { n } ^ { \prime } : = G _ { n } - p _ { n } P _ { n } \mathring { x } _ { n }$ has no proper $A _ { n } \to B$ wave but $G _ { n } ^ { \prime } \mathrm { ~ - ~ } x _ { n }$ does, so by Lemma 8.4.5 every $A _ { n } \to B$ wave in $\begin{array} { r } { G _ { n } ^ { \prime } - x _ { n } = G _ { n } - p _ { n } P _ { n } x _ { n } } \end{array}$ is large. Let $\mathcal { W }$ be a maximal such wave, put $\mathcal { W } _ { n + 1 } : = \mathcal { W } _ { n } + \mathcal { W }$ , and let $T _ { n + 1 } : = T _ { n } \cup t _ { n } P _ { n } x _ { n }$ . Then $\mathcal { W } _ { n + 1 }$ is a large maximal $\hat { A } \to B$ wave in $G - T _ { n + 1 }$ . If $x _ { n } \in B$ , then $T _ { n + 1 }$ contains a path linking $a ^ { * }$ to $B$ , which satisfies the lemma with $\mathcal { W } _ { n + 1 }$ and $X = V ( T _ { n + 1 } )$ . We may therefore assume that $x _ { n } \notin B$ , giving $T _ { n + 1 } \subseteq G - ( \hat { A } \cup B )$ as required.

Put $\textstyle T ^ { * } : = \bigcup _ { n \in \mathbb { N } } T _ { n }$ . Then the $\mathcal { W } _ { n }$ are $\hat { A } \to B$ waves in $G - T ^ { * }$ ; let $( \mathcal { W } ^ { * } , A ^ { * } )$ be their limit. Our aim is to show that $A ^ { * }$ separates $A$ from $B$ not only in $G - T ^ { * }$ but even in $G$ : then $( \mathcal { W } ^ { * } \cup \{ a ^ { * } \} , A ^ { * } )$ is a small $A  B$ wave in $G$ , a contradiction.

Suppose there exists an $A$ – $B$ path $Q$ in $G - A ^ { * }$ . Let $t$ be its last vertex in $T ^ { * }$ . Since $T ^ { * }$ does not meet $B$ , there is a vertex $p$ following $t$ on $Q$ . Since $T ^ { * }$ contains every $p _ { n }$ but not $p$ , the path $P = a ^ { * } T ^ { * } t Q$ was never chosen as $P _ { n }$ . Now let $n$ be large enough that $t \in T _ { n }$ , and that $p$ precedes $p _ { n }$ in our fixed enumeration of $V ( G )$ . The fact that $P$ was not chosen as $P _ { n }$ then means that its portion $p Q$ outside $T _ { n }$ meets $A _ { n }$ , say in a vertex $q$ . Now $q \notin A ^ { * }$ by the choice of $Q$ . Let $W$ be the path in $\mathcal { W } _ { n }$ that joins $\hat { A }$ to $q$ ; this path too avoids $A ^ { * }$ . But then $W q Q$ contains an $\hat { A }$ – $B$ path in $G - T ^ { * }$ avoiding $A ^ { * }$ , which contradicts the definition of $A ^ { * }$ . 

The proof of Theorem 8.4.2 for countable $G$ is now complete.

Turning now to matching, let us begin with a simple problem that is intrinsically infinite. Given two sets $A , B$ and injective functions $A  B$ and $B  A$ , is there necessarily also a bijection between $A$ and $B$ ? Indeed there is—this is the famous Cantor-Bernstein theorem from elementary set theory. Recast in terms of matchings, the proof becomes very simple:

Proposition 8.4.6. Let $G$ be a bipartite graph, with bipartition $\{ A , B \}$ say. If $G$ contains a matching of $A$ and a matching of $B$ , then $G$ has a 1-factor.

Proof . Let $H$ be the multigraph on $V ( G )$ whose edge set is the disjoint union of the two matchings. (Thus, any edge that lies in both matchings becomes a double edge in $H$ .) Every vertex in $H$ has degree 1 or 2. In fact, it is easy to check that every component of $H$ is an even cycle or an infinite path. Picking every other edge from each component, we obtain a 1-factor of $G$ . 

The corresponding path problem in non-bipartite graphs, with sets of disjoint $A$ – $B$ paths instead of matchings, is less trivial. Let us say that a set $\mathcal { P }$ of paths in $G$ covers a set $U$ of vertices if every vertex in $U$ is an endvertex of a path in $\mathcal { P }$ .

# Theorem 8.4.7. (Pym 1969)

Let $G$ be a graph, and let $A , B \subseteq V ( G )$ . Suppose that $G$ contains two sets of disjoint $A$ – $B$ paths, one covering $A$ and one covering $B$ . Then $G$ contains a set of disjoint $A$ – $B$ paths covering $A \cup B$ .

Some hints for a proof of Theorem 8.4.7 are included with Exercise 52.

Next, let us see how the standard matching theorems for finite graphs—K¨onig, Hall, Tutte, Gallai-Edmonds—extend to infinite graphs. For locally finite graphs, they all have straightforward extensions by compactness; see Exercises 14–16. But there are also very satisfactory extensions to graphs of arbitrary cardinality. Their proofs form a coherent body of theory and are much deeper, so we shall only be able to state those results and point out how some of them are related. But, as with Menger’s theorem, the statements themselves are interesting too: finding the ‘right’ restatement of a given finite result to make a substantial infinite theorem is by no means easy, and most of them were found only as the theory itself developed over the years.

Let us start with bipartite graphs. The following Erd˝os-Menger-type extension of K¨onig’s theorem (2.1.1) is now a corollary of Theorem 8.4.2:

# Theorem 8.4.8. (Aharoni 1984)

Every bipartite graph has a matching, $M$ say, and a vertex cover of its edge set that consists of exactly one vertex from every edge in $M$ .

What about an infinite version of the marriage theorem (2.1.2)? The finite theorem says that a matching exists as soon as every subset $S$ of the first partition class has enough neighbours in the second. But how do we measure ‘enough’ in an infinite graph? Just as in Menger’s theorem, comparing cardinalities is not enough (Exercise 15).

However, there is a neat way of rephrasing the marriage condition for a finite graph without appealing to cardinalities. Call a subset $X$ of one partition class matchable to a subset $Y$ of the other if the subgraph spanned by $X$ and $Y$ contains a matching of $X$ . Now if $S$ is minimal with $\vert S \vert > \vert N ( S ) \vert$ , then $S$ is ‘larger’ than $N ( S )$ in the sense that $S$ is not matchable to $N ( S )$ but $N ( S )$ is matchable to $S$ —by the marriage theorem! (Indeed, by the minimality of $S$ and the marriage theorem, any $S ^ { \prime } \subseteq S$ with $\left| S ^ { \prime } \right| = \left| S \right| - 1$ can be matched to $N ( S )$ . As $| S ^ { \prime } | = | S | - 1 \geqslant$ $| N ( S ) |$ , this matching covers $N ( S )$ .) Thus, if there is any obstruction $S$ to a perfect matching of the type $\vert S \vert > \vert N ( S ) \vert$ , there is also one where

covers

matchable

$S$ is larger than $N ( S )$ in this other sense: that $S$ is not matchable to $N ( S )$ but $N ( S )$ is matchable to $S$ .

Rewriting the marriage condition in this way does indeed yield an infinite version of Hall’s theorem, which follows from Theorem 8.4.8 just as the marriage theorem follows from K¨onig’s theorem:

Corollary 8.4.9. A bipartite graph with bipartition $\{ A , B \}$ contains a matching of $A$ unless there is a set $S \subseteq A$ such that $S$ is not matchable to $N ( S )$ but $N ( S )$ is matchable to $S$ .

Proof . Consider a matching $M$ and a cover $U$ as in Theorem 8.4.8. Then $U \cap B \supseteq N ( A \setminus U )$ is matchable to $A \setminus U$ , by the edges of $M$ . And if $A \setminus U$ is matchable to $N ( A \setminus U )$ , then adding this matching to the edges of $M$ incident with $A \cap U$ yields a matching of $A$ . 

Applied to a finite graph, Corollary 8.4.9 implies the marriage theorem: if $N ( S )$ is matchable to $S$ but not conversely, then clearly $\vert S \vert > \vert N ( S ) \vert$ .

Let us now turn to non-bipartite graphs. If a finite graph has a 1- factor, then the set of vertices covered by any partial matching—one that leaves some vertices unmatched—can be increased by an augmenting path, an alternating path whose first and last vertex are unmatched (Ex. 1, Ch. 2). In an infinite graph we no longer insist that augmenting paths be finite, as long as they have a first vertex. Then, starting at any unmatched vertex with an edge of the 1-factor that we are assuming to exist, we can likewise find a unique maximal alternating path that will either be a ray or end at another unmatched vertex. Switching edges along this path we can then improve our current matching to increase the set of matched vertices, just as in a finite graph.

The existence of an inaugmentable partial matching, therefore, is an obvious obstruction to the existence of a 1-factor. The following theorem asserts that this obstruction is the only one:

# Theorem 8.4.10. (Steffens 1977)

A countable graph has a 1-factor if and only if for every partial matching there exists an augmenting path.

Unlike its finite counterpart, Theorem 8.4.10 is far from trivial: augmenting a given matching ‘blindly’ need not lead to a well-defined matching at limit steps, since a given edge may get toggled on and off infinitely often (in which case its status will be undefined at the limit—example?). We therefore cannot simply find the desired 1-factor inductively.

In fact, Theorem 8.4.10 does not extend to uncountable graphs (Exercise 55). However, from the obstruction of inaugmentable partial matchings one can derive a Tutte-type condition that does extend.

Given a set $S$ of vertices in a graph $G$ , let us write $\mathcal { C } _ { G - S } ^ { \prime }$ for the set of factor-critical components of $G - S$ , and $G _ { S } ^ { \prime }$ for the bipartite graph with vertex set $S \cup \mathcal { C } _ { G - S } ^ { \prime }$ and edge set $\{ s C \mid \exists c \in C \colon s c \in E ( G ) \}$ .

# Theorem 8.4.11. (Aharoni 1988)

$A$ graph $G$ has a 1-factor if and only if, for every set $S \subseteq V ( G )$ , the set $\mathcal { C } _ { G - S } ^ { \prime }$ is matchable to $S$ in $G _ { S } ^ { \prime }$ .

Applied to a finite graph, Theorem 8.4.11 implies Tutte’s 1-factor theorem (2.2.1): if $\mathcal { C } _ { G - S } ^ { \prime }$ is not matchable to $S$ in $G _ { S } ^ { \prime }$ , then by the marriage theorem there is a subset $S ^ { \prime }$ of $S$ that sends edges to more than $| S ^ { \prime } |$ components in $\mathcal { C } _ { G - S } ^ { \prime }$ that are also components of $G - S ^ { \prime }$ , and these components are odd because they are factor-critical.

Theorems 8.4.8 and 8.4.11 also imply an infinite version of the Gallai-Edmonds theorem (2.2.3):

Corollary 8.4.12. Every graph $G = ( V , E )$ has a set $S$ of vertices that is matchable to $\mathcal { C } _ { G - S } ^ { \prime }$ in $G _ { S } ^ { \prime }$ and such that every component of $G - S$ not in $\mathcal { C } _ { G - S } ^ { \prime }$ has a 1-factor. Given any such set $S$ , the graph $G$ has a 1-factor if and only if $\mathcal { C } _ { G - S } ^ { \prime }$ is matchable to $S$ in $G _ { S } ^ { \prime }$ .

Proof . Given a pair $( S , M )$ where $S \subseteq V$ and $M$ is a matching of $S$ ： in $G _ { S } ^ { \prime }$ , and given another such pair $( S ^ { \prime } , M ^ { \prime } )$ , write $( S , M ) \leqslant ( S ^ { \prime } , M ^ { \prime } )$ if

$$
S \subseteq S ^ {\prime} \subseteq V \setminus \bigcup \{V (C) \mid C \in \mathcal {C} _ {G - S} ^ {\prime} \}
$$

and $M \subseteq M ^ { \prime }$ . Since $\mathcal { C } _ { G - S } ^ { \prime } \subseteq \mathcal { C } _ { G - S ^ { \prime } } ^ { \prime }$ for any such $S$ and $S ^ { \prime }$ , Zorn’s lemma implies that there is a maximal such pair $( S , M )$ .

For the first statement, we have to show that every component $C$ of $G - S$ that is not in $\mathcal { C } _ { G - S } ^ { \prime }$ has a 1-factor. If it does not, then by Theorem 8.4.11 there is a set $T \subseteq V ( C )$ such that $\mathcal { C } _ { C - T } ^ { \prime }$ is not matchable to $T$ in $C _ { T } ^ { \prime }$ C. By Corollary 8.4.9, this means that $\mathcal { C } _ { C - T } ^ { \prime }$ has a subset $\boldsymbol { \mathscr { C } }$ that is not matchable in $C _ { T } ^ { \prime }$ to the set $T ^ { \prime } \subseteq T$ of its neighbours, while $T ^ { \prime }$ is matchable to $\boldsymbol { \mathscr { C } }$ ; let $M ^ { \prime }$ be such a matching. Then $( S , M ) <$ $( S \cup T ^ { \prime } , M \cup M ^ { \prime } )$ , contradicting the maximality of $( S , M )$ .

Of the second statement, only the backward implication is nontrivial. Our assumptions now are that $\mathcal { C } _ { G - S } ^ { \prime }$ is matchable to $S$ in $G _ { S } ^ { \prime }$ and vice versa (by the choice of $S$ ), so Proposition 8.4.6 yields that $G _ { S } ^ { \prime }$ has a 1-factor. This defines a matching of $S$ in $G$ that picks one vertex $x _ { C }$ from every component $C \in \mathcal { C } _ { G - S } ^ { \prime }$ and leaves the other components of $G - S$ untouched. Adding to this matching a 1-factor of $C - x _ { C }$ for every $C \in \mathcal { C } _ { G - S } ^ { \prime }$ and a 1-factor of every other component of $G - S$ , we obtain the desired 1-factor of $G$ . 

$S , M$

Infinite matching theory may seem rather mature and complete as it stands, but there are still fascinating unsolved problems in the Erd˝os-Menger spirit concerning related discrete structures, such as posets or hypergraphs. We conclude with one about graphs.

Call an infinite graph $G$ perfect if every induced subgraph $H \subseteq G$ has a complete subgraph $K$ of order $\chi ( H )$ , and strongly perfect if $K$ can always be chosen so that it meets every colour class of some $\chi ( H )$ - colouring of $H$ . (Exercise 58 gives an example of a perfect graph that is not strongly perfect.) Call $G$ weakly perfect if the chromatic number of every induced subgraph $H \subseteq G$ is at most the supremum of the orders of its complete subgraphs.

# Conjecture. (Aharoni & Korman 1993)

Every weakly perfect graph without infinite independent sets of vertices is strongly perfect.

# 8.5 The topological end space

In this last section we shall develop a deeper understanding of the global structure of infinite graphs, especially locally finite ones, that can be attained only by studying their ends. This structure is intrinsically topological, but no more than the most basic concepts of point-set topology will be needed.

Our starting point will be to make precise the intuitive idea that the ends of a graph are the ‘points at infinity’ to which its rays converge. To do so, we shall define a topological space $| G |$ associated with a graph $G = ( V , E , \Omega )$ and its ends.8 By considering topological versions of paths, cycles and spanning trees in this space, we shall then be able to extend to infinite graphs some parts of finite graph theory that would not otherwise have infinite counterparts (see the notes for more examples). Thus, the ends of an infinite graph turn out to be more than a curious new phenomenon: they form an integral part of the picture, without which it cannot be properly understood.

To build the space $| G |$ formally, we start with the set $V \cup \Omega$ . For every edge $e = u v$ we add a set $\mathring { e } = ( u , v )$ of continuum many points, making these sets $\check { e }$ disjoint from each other and from $V \cup \Omega$ . We then choose for each $e$ some fixed bijection between $\check { e }$ and the real interval $( 0 , 1 )$ , and extend this bijection to one between $[ u , v ] : = \{ u \} \cup \mathring { e } \cup \{ v \}$ and $[ 0 , 1 ]$ . This bijection defines a metric on $[ u , v ]$ ; we call $[ u , v ]$ a topological edge with inner points $x \in { \mathfrak { e } }$ . Given any $F \subseteq E$ we write ${ \check { F } } : = \bigcup \{ { \check { e } } \mid e \in F \}$ .

When we speak of a ‘graph’ $H \subseteq G$ , we shall often also mean its corresponding point set $V ( H ) \cup { \check { E } } ( H )$ .

Having thus defined the point set of $| G |$ , let us choose a basis of open sets to define its topology. For every edge $u v$ , declare as open all subsets of $( u , v )$ that correspond, by our fixed bijection between $( u , v )$ and $( 0 , 1 )$ , to an open set in $( 0 , 1 )$ . For every vertex $u$ and $\epsilon > 0$ , declare as open the ‘open star around $u$ of radius $\epsilon { \ ' }$ , that is, the set of all points on edges $[ u , v ]$ at distance less than $\epsilon$ from $u$ , measured individually for each edge in its metric inherited from $[ 0 , 1 ]$ . Finally, for every end $\omega$ and every finite set $S \subseteq V$ , there is a unique component $C ( S , \omega )$ of $G - S$ that contains a ray from $\omega$ . Let $\Omega ( S , \omega ) : = \{ \omega ^ { \prime } \in \Omega \ | \ C ( S , \omega ^ { \prime } ) = C ( S , \omega ) \}$ . For every $\epsilon > 0$ , write $\check { E } _ { \epsilon } ( S , \omega )$ for the set of all inner points of $S -$ $C ( S , \omega )$ edges at distance less than $\epsilon$ from their endpoint in $C ( S , \omega )$ . Then declare as open all sets of the form

$$
\hat {C} _ {\epsilon} (S, \omega) := C (S, \omega) \cup \Omega (S, \omega) \cup \dot {\tilde {E}} _ {\epsilon} (S, \omega). \quad \hat {C} _ {\epsilon} (S, \omega)
$$

This completes the definition of $| G |$ , whose open sets are the unions of the sets we explicitly chose as open above.

Any subsets of $| G |$ we consider, including those that are ‘graphs’ in their own right (i.e., unions of vertices and topological edges of $G$ ), will carry the subspace topology in $| G |$ . Such sets may contain ends of $G$ , quite independently of whether they contain any rays from such ends: they are just subsets of the point set $| G |$ .9 A standard subspace of $| G |$ is one that contains every edge (including its endvertices) of which it contains an inner point.

The closure of a set $X \subseteq | G |$ will be denoted by $\overline { { X } }$ . For example, ${ \overline { { V } } } = V \cup \Omega$ (because every neighbourhood of an end contains a vertex), and the closure of a ray is obtained by adding its end. More generally, if $X \subseteq V$ is the set of teeth of a comb then $\overline { { X } }$ contains the end of its spine, while conversely for every end $\omega \in { \overline { { X } } }$ and every ray $R \in \omega$ there is a comb with spine $R$ and teeth in $X$ (Exercise 59). In particular, the closure of the subgraph $C ( S , \omega )$ considered above is the set $C ( S , \omega ) \cup \Omega ( S , \omega )$ .

By definition, $| G |$ is always Hausdorff. When $G$ is connected and locally finite, then $| G |$ is also compact:10

Proposition 8.5.1. If $G$ is connected and locally finite, then $| G |$ is a compact Hausdorff space.

Proof . Let $\boldsymbol { \mathcal { O } }$ be an open cover of $| G |$ ; we show that $\boldsymbol { \mathcal { O } }$ has a finite (8.1.2) subcover. Pick a vertex $v _ { 0 } \in G$ , write $D _ { n }$ for the (finite) set of vertices at distance $n$ from $v _ { 0 }$ , and put $S _ { n } : = D _ { 0 } \cup . . . \cup D _ { n - 1 }$ . For every $v \in D _ { n }$ , let $C ( v )$ denote the component of $G - S _ { n }$ containing $v$ , and let $\hat { C } ( v )$ be

its closure together with all inner points of $C ( v )$ – $S _ { n }$ edges. Then $G \left[ S _ { n } \right]$ and these $\hat { C } ( v )$ together partition $| G |$ .

We wish to prove that, for some $n$ , each of the sets $\hat { C } ( v )$ with $v \in D _ { n }$ is contained in some $O ( v ) \in \mathcal { O }$ . For then we can take a finite subcover of $\boldsymbol { \mathcal { O } }$ for $G \left[ S _ { n } \right]$ (which is compact, being a finite union of edges and vertices), and add to it these finitely many sets $O ( v )$ to obtain the desired finite subcover for $| G |$ .

Suppose there is no such $n$ . Then for each $n$ the set $V _ { n }$ of vertices $v \in D _ { n }$ such that no set from $\boldsymbol { \mathcal { O } }$ contains $\hat { C } ( \boldsymbol { v } )$ is non-empty. Moreover, for every neighbour $u \in D _ { n - 1 }$ of $v \in V _ { n }$ we have $C ( v ) \subseteq C ( u )$ because $S _ { n - 1 } \subseteq S _ { n }$ , and hence $u \in V _ { n - 1 }$ ; let $f ( v )$ be such a vertex $u$ . By the infinity lemma (8.1.2) there is a ray $R = v _ { 0 } v _ { 1 } \ldots$ with $v _ { n } \in V _ { n }$ for all $n$ . Let $\omega$ be its end, and let $O \in { \mathcal { O } }$ contain $\omega$ . Since $O$ is open, it contains a basic open neighbourhood of $\omega$ : there exist a finite set $S \subseteq V$ and $\epsilon > 0$ 】 such that $\hat { C } _ { \epsilon } ( S , \omega ) \subseteq O$ . Now choose $n$ large enough that $S _ { n }$ contains $S$ and all its neighbours. Then $\hat { C } ( v _ { n } )$ lies inside a component of $G - S$ . As $C ( v _ { n } )$ contains $v _ { n } R \in \omega$ , this component must be $C ( S , \omega )$ . Thus

$$
\hat {C} (v _ {n}) \subseteq C (S, \omega) \subseteq O \in \mathcal {O},
$$

contradicting the fact that $v _ { n } \in V _ { n }$ .

If $G$ has a vertex of infinite degree then $| G |$ cannot be compact. (Why not?) But $\Omega ( G )$ can be compact; see Exercise 61 for when it is.

What else can we say about the space $| G |$ in general? For example, is it metrizable? Using a normal spanning tree $T$ of $G$ , it is indeed not difficult to define a metric on $| G |$ that induces its topology. But not every connected graph has a normal spanning tree, and it is not easy to determine which graphs do. Surprisingly, though, it is possible conversely to deduce the existence of a normal spanning tree just from the assumption that the subspace $V \cup \Omega$ of $| G |$ is metric. Thus whenever $| G |$ is metrizable, a natural metric can be made visible in this simple structural way:

Theorem 8.5.2. For a connected graph $G$ , the space $| G |$ is metrizable if and only if $G$ has a normal spanning tree.

The proof of Theorem 8.5.2 is indicated in Exercises 30 and 63.

Our next aim is to review, or newly define, some topological notions of paths and connectedness, of cycles, and of spanning trees. By substituting these topological notions with respect to $| G |$ for the corresponding graph-theoretical notions with respect to $G$ , one can extend to locally finite graphs a number of theorems about paths, cycles and spanning trees in finite graphs that would not otherwise extend. We shall do this, as a case in point, for the tree-packing theorem of Nash-Williams and

Tutte (Theorem 2.4.1); references for more such results are given in the notes.

Let $X$ be an arbitrary Hausdorff space. (Later, this will be a subspace of $| G |$ .) $X$ is (topologically) connected if it is not a union of two disjoint non-empty open subsets. If we think of two points of $X$ as equivalent if $X$ has a connected subspace containing both, we have an equivalence relation whose classes are the (connected) components of $X$ . These are the maximal connected subspaces of $X$ . Components are always closed, but if $X$ has infinitely many components they need not be open.

We shall need the following lemma; see the notes for a reference.

Lemma 8.5.3. If $X$ is compact and $A _ { 1 } , A _ { 2 }$ are distinct components of $X$ , then $X$ is a union of disjoint open sets $X _ { 1 } , X _ { 2 }$ such that $A _ { 1 } \subseteq X _ { 1 }$ and $A _ { 2 } \subseteq X _ { 2 }$ .

An arc in $X$ is a homeomorphic image in $X$ of the real unit interval $[ 0 , 1 ]$ ; it links the images of 0 and 1, which are its endpoints. Being linked by an arc is also an equivalence relation on $X$ (since every $x - y$ arc $A$ has a first point $p$ on any $y$ – $_ z$ arc $B$ , because $B$ is closed, so $A p B$ is an $x - z$ arc); the equivalence classes are the arc-components of $X$ . If there is only one arc-component, then $X$ is arc-connected. Since $[ 0 , 1 ]$ is connected, arc-connectedness implies connectedness. The converse implication is false in general, even for spaces $X \subseteq | G |$ with $G$ locally finite. But it holds in an important special case:

Lemma 8.5.4. If $G$ is a locally finite graph, then every closed connected subspace of $| G |$ is arc-connected.

The proof of Lemma 8.5.4 is not easy; see the notes for a reference.

Every finite path in $G$ defines an arc in $| G |$ in an obvious way. Similarly, every ray is an arc linking its starting vertex to its end, and a double ray in $G$ forms an arc in $| G |$ together with the two ends of its tails, if these ends are distinct. Consider an end $\omega$ in a standard subspace $X$ of $| G |$ , and $k \in \mathbb { N } \cup \{ \infty \}$ . If $k$ is the maximum number of arcs in $X$ that have $\omega$ as their common endpoint and are otherwise disjoint, then $k$ is the (topological) vertex-degree of $\omega$ in $X$ . The (topological) edge-degree of $\omega$ in $X$ is defined analogously, using edge-disjoint arcs. In analogy to Theorem 8.2.5 one can show that these maxima are always attained, so every end of $G$ that lies in $X$ has a topological vertex- and edge-degree there. For $X = | G |$ and $G$ locally finite, the (topological) end degrees in $X$ coincide with the combinatorial end degrees defined earlier.

Unlike finite paths, arcs can jump across a vertex partition without containing an edge from the corresponding cut, provided the cut is infinite:

Lemma 8.5.5. Let $G$ be connected and locally finite, $\{ X , Y \}$ a partition of $V ( G )$ , and $F : = E ( X , Y )$ .

(i) $F$ is finite if and only if ${ \overline { { X } } } \cap { \overline { { Y } } } = \emptyset$ .   
(ii) If $F$ is finite, there is no arc in $| G | \setminus { \dot { F } }$ with one endpoint in $X$ and the other in $Y$ .   
(iii) If $F$ is infinite and $X$ and $Y$ are both connected in $G$ , there is such an arc.

(8.2.2) Proof . (i) Suppose first that $F ^ { \prime }$ is infinite. Since $G$ is locally finite, the set $X ^ { \prime }$ of endvertices of $F ^ { \prime }$ in $X$ is also infinite. By the star-comb lemma (8.2.2), there is a comb in $G$ with teeth in $X ^ { \prime }$ ; let $\omega$ be the end of its spine. Then every basic open neighbourhood $\hat { C } _ { \epsilon } ( S , \omega )$ of $\omega$ meets $X ^ { \prime }$ infinitely and hence also meets $Y$ , giving $\omega \in { \overline { { X } } } \cap { \overline { { Y } } }$ .

Suppose now that $F$ is finite. Let $S$ be the set of vertices incident with edges in $F ^ { \prime }$ . Then $S$ is finite and separates $X$ from $Y$ . Since every basic open set of the form $\hat { C } _ { \epsilon } ( S , \omega )$ misses $X$ or $Y$ , no end $\omega$ lies in the closure of both.

(ii) Clearly $| G | \setminus { \overset { \circ } { F } } = { \overline { { G \left[ X \right] } } } \cup { \overline { { G \left[ Y \right] } } }$ , and by (i) this union is disjoint. Hence no connected subset of $| G | \setminus { \check { F } }$ can meet both $X$ and $Y$ , but arcs are continuous images of $[ 0 , 1 ]$ and hence connected.   
(iii) By (i), there is an end $\omega \in \overline { { X } } \cap \overline { { Y } }$ . Apply the star-comb lemma in $G \left[ X \right]$ to any sequence of vertices in $X$ converging to $\omega$ ; this yields a comb whose spine $R$ lies in $\omega$ . Similarly, there is a comb in $G \left[ Y \right]$ whose spine $R ^ { \prime }$ lies in $\omega$ . Now $R \cup \{ \omega \} \cup R ^ { \prime }$ is the desired arc. 

circle

A circle in a topological space is a homeomorphic image of the unit circle $S ^ { 1 } \subseteq \mathbb { R } ^ { 2 }$ . For example, if $G$ is the 2-way infinite ladder shown in Figure 8.1.3, and we delete all its rungs (the vertical edges), what remains is a disjoint union $D$ of two double rays; the closure of $D$ in $| G |$ , obtained by adding the two ends of $G$ , is a circle. Similarly, the double ray ‘around the outside’ of the 1-way ladder forms a circle together with the unique end of that ladder.

A more adventurous example of a circle is shown in Figure 8.5.1. Let $G$ be the graph obtained from the binary tree $T _ { 2 }$ by joining for every finite 0–1 sequence $\ell$ the vertices $\ell 0 1$ and $\ell { 1 0 }$ by a new edge $e _ { \ell }$ . Together with all the ends of $G$ , the double rays $D _ { \ell } \ \ni \ e _ { \ell }$ shown in the figure form an arc $A$ in $| G |$ , whose union with the bottom double ray $D$ is a circle in $| G |$ (Exercise 69). Note that no two of the double rays in $A$ are consecutive: between any two there lies a third. This is why end degrees in subspaces are defined in terms of arcs rather than rays, so that the ends in a circle can always have degree 2 in it. And indeed they do (Exercise 70):

![](images/6ac71d7f117112283c08c51ca3bdbd7978336e190937d8b414e2418d9d50fd0a.jpg)  
Fig. 8.5.1. A circle containing uncountably many ends

Lemma 8.5.6. Let $G$ be locally finite. A closed standard subspace $C$ of $| G |$ is a circle in $| G |$ if and only if $C$ is connected, every vertex in $C$ is incident with exactly two edges in $C$ , and every end in $C$ has vertex-degree 2 (equivalently: edge-degree 2) in $C$ .

It is not hard to show that every circle $C$ in a space $| G |$ is a standard subspace; the set $D$ of edges it contains will be called its circuit. Then $C$ is the closure of the point set $\cup D$ , as every neighbourhood in $C$ of a vertex or end meets an edge, which must then be contained in $C$ and hence lie in $D$ . In particular, there are no circles consisting only of ends, and every circle is uniquely determined by its circuit.

A topological spanning tree of $G$ is an arc-connected standard subspace of $| G |$ that contains every vertex and every end but contains no circle. Clearly, such a subspace $X$ must be closed. With respect to the addition or deletion of edges, it is both minimally arc-connected and maximally ‘acirclic’. As with ordinary trees, one can show that every two points of $X$ are joined by a unique arc in $X$ . Thus, adding a new edge $e$ to $X$ creates a unique circle in $X \cup e$ ; its edges form the fundamental circuit $C _ { e }$ of $e$ with respect to $X$ . Similarly, for every edge $e \subseteq X$ the space $X \setminus { \overset { \circ } { e } }$ has exactly two arc-components; the set of edges between these is the fundamental cut $D _ { e }$ . If $G$ is locally finite, then its fundamental cuts are finite (Exercise 74).

One might expect that the closure $\overline { T }$ of an ordinary spanning tree $T$ of $G$ is always a topological spanning tree of $| G |$ . However, this can fail in two ways: if $T$ has a vertex of infinite degree then $\overline { T }$ may fail to be arc-connected (although it will be topologically connected, because $T$ is); if $T$ is locally finite, then $\overline { T }$ will be arc-connected but may contain a circle (Figure 8.5.2). On the other hand, a subgraph whose closure is a topological spanning tree may well be disconnected: the vertical rays

![](images/39d2511dd48bbd77ad78c4ff41d42c1b929e2c49f31d65378f556708da1c5dcb.jpg)  
Fig. 8.5.2. $\overline { { T } } _ { 1 }$ is a topological spanning tree, but $\overline { { T } } _ { 2 }$ contains three circles

in the $\mathbb { N } \times \mathbb { N }$ grid, for example, form a topological spanning tree of the grid together with its unique end.

In general, there seems to be no canonical way to construct topological spanning trees, and it is unknown whether every connected graph has one. Countable connected graphs, however, do have topological spanning trees, by Theorem 8.2.4:

Lemma 8.5.7. The closure of any normal spanning tree is a topological spanning tree.

(1.5.5)(8.2.3) Pro . t $T$ be a normal spann g ee of $G$ . mma 8.2.3, every $G$ $R$ $T$

$\omega$ to the root of $T$ , so $\overline { T }$ is arc-connected.

It remains to check that $\overline { T }$ contains no circle. Suppose it does, and let $A$ be the $u$ – $v$ arc obtained from that circle by deleting the inner points of an edge $e = u v$ it contains. Clearly, $e \in T$ . Assume that $u < v$ in the tree-order of $T$ , let $T _ { u }$ and $T _ { v }$ denote the components of $T - e$ containing $u$ and $v$ , and notice that $V ( T _ { v } )$ is the up-closure $\lfloor v \rfloor$ of $\boldsymbol { v }$ in $T$ .

Now let $S : = \lceil u \rceil$ . By Lemma 1.5.5 (ii), $\lfloor v \rfloor$ is the vertex set of a component $C$ of $G - S$ . Thus, $V ( C ) = V ( T _ { v } )$ and $V ( G - C ) = V ( T _ { u } )$ , so the set $E ( C , S )$ of edges between these sets contains no edge of $A$ . But $\overline { { C } }$ and $\overline { { G - C } }$ partition $| G | \setminus { \check { E } } ( C , S )$ into two open sets.11 As $A \subseteq | G | \setminus \check { E } ( C , S )$ , this contradicts the fact that $A$ is topologically connected. 

We now extend the notion of the cycle space to locally finite infinite graphs $G$ , based on their (possibly infinite) circuits.

Call a family $( D _ { i } ) _ { i \in I }$ of subsets of $E ( G )$ thin if no edge lies in $D _ { i }$ for infinitely many $i$ . Let the sum $\textstyle \sum _ { i \in I } D _ { i }$ of this family be the set of all edges that lie in $D _ { i }$ for an odd number of indices $i$ . Now define the (topological) cycle space ${ \mathcal { C } } ( G )$ of $G$ as the subspace of its edge space ${ \mathcal { E } } ( G )$ consisting of all sums of (thin families of) circuits. (Note that ${ \mathcal { C } } ( G )$ is closed under addition: just combine the two thin families into one.) Clearly, this definition of ${ \mathcal { C } } ( G )$ agrees with that from Chapter 1.9 when $G$ is finite.

We say that a given set $\mathcal { Z }$ of circuits generates ${ \mathcal { C } } ( G )$ if every element of ${ \mathcal { C } } ( G )$ is a sum of elements of $\mathcal { L }$ . For example, the cycle space of the ladder in Figure 8.1.3 can be generated by all its squares (the 4-element circuits), or by the infinite circuit consisting of all horizontal edges and all squares but one. Similarly, the ‘wild’ circuit of Figure 8.5.1 is the sum of all the finite face boundaries in that graph.

The following two theorems summarize how the properties of the cycle spaces of finite graphs, familiar from Chapter 1, extend to locally finite graphs with topological cycle spaces.

# Theorem 8.5.8. (Diestel & K¨uhn 2004)

Let $G = ( V , E , \Omega )$ be a locally finite connected graph.

(i) ${ \mathcal { C } } ( G )$ contains precisely those subsets of $E$ that meet every finite cut in an even number of edges.   
(ii) Every element of ${ \mathcal { C } } ( G )$ is a disjoint sum of circuits.   
(iii) The fundamental circuits of any topological spanning tree of $G$ generate ${ \mathcal { C } } ( G )$ .

While the proofs of parts (i) and (iii) of Theorem 8.5.8 are straightforward, part (ii) is not that easy. This is because it is no longer straightforward to isolate a single circuit from a given element of ${ \mathcal { C } } ( G )$ . For example, we know that the ‘wild’ circuit of the graph in Figure 8.5.1 must lie in its cycle space, since it is clearly the sum of the finite circuits bounding a face. But in order to construct a ‘decomposition’ of this element of ${ \mathcal { C } } ( G )$ into ‘disjoint circuits’, the proof of (ii) has to, somehow, construct this circuit without appealing to the special structure of the graph. Our proof below circumvents these difficulties by appealing to our unproved Lemma 8.5.4 that closed connected subsets of $| G |$ are arcconnected, and to the unproved topological Lemma 8.5.3.

Proof of Theorem 8.5.8. (i) Let $D \in { \mathcal { C } } ( G )$ be given, and consider a finite cut $F ^ { \prime }$ . By definition, $D$ is a sum of a thin family of circuits. Only finitely many of these can meet $F$ , so it suffices to show that every circuit meets $F ^ { \prime }$ evenly.

To prove this, consider a circle $C$ in $| G |$ . As $F ^ { \prime }$ is a finite cut, any arc in $| G |$ that links the two sides of the corresponding vertex partition contains an edge from $F$ , by Lemma 8.5.5 (ii). Hence every arc on $C$ between two consecutive edges from $F ^ { \prime }$ links these at their endvertices on the same side of $F$ , which implies that $C$ contains an even number of edges from $F$ .

Conversely, let $D$ be any set of edges that meets every finite cut evenly. Let $T$ be a normal spanning tree of $G$ (Theorem 8.2.4). We claim that

$$
D = \sum_ {e \in D \setminus E (T)} C _ {e}, \tag {*}
$$

where $C _ { e }$ denotes the fundamental circuit of $e$ with respect to $T$ . To prove this, consider the edges $f$ of $G$ separately. If $f \not \in T$ , then clearly $f \in D$ if and only if $f$ lies in the sum in $( * )$ , since $C _ { f }$ is the unique fundamental circuit containing $f$ . Suppose now that $f \in T$ . Then $f$ lies in precisely those $C _ { e }$ for which $e$ lies in the fundamental cut $D _ { f }$ of $f$ . Thus all we need to show is that $D _ { f }$ is finite: then $D \cap D _ { f }$ is even by assumption, so $f \in D$ if and only if an odd number of other edges $e \in D _ { f }$ lie in $D$ , which is the case if and only if $f$ lies in the sum in $( * )$ . (In particular, the sum is one over a thin family, and hence well-defined.)

To show that $D _ { f }$ is finite, assume that $f = x y$ with $x \ < \ y$ in the tree-order of $T$ . Then the up-closure $\lfloor y \rfloor$ of $y$ in $T$ is one of the two components of $T - f$ , and by Lemma 1.5.5 it spans a component of $G - \lceil x \rceil$ . Hence every edge in $D _ { f }$ has one endvertex in $\lfloor y \rfloor$ and the other in $\lceil x \rceil$ . As $\lceil x \rceil$ is finite and $G$ is locally finite, this means that there are only finitely many such edges.

(ii) Let $D \in { \mathcal { C } } ( G )$ be given. Consider a maximal set of disjoint circuits contained in $D$ , and let $Z$ be their union. Clearly $Z \in { \mathcal { C } } ( G )$ , and hence $Z ^ { \prime } : = D - Z \in { \mathcal { C } } ( G )$ . We wish to show that $Z ^ { \prime } = \emptyset$ .

Suppose not. Let $e = u v$ be an edge in $Z ^ { \prime }$ and put

$$
X := \left(V \cup \Omega \cup \bigcup Z ^ {\prime}\right) \setminus \mathring {e}.
$$

Clearly, $X$ is a closed in $| G |$ , and hence is a compact subspace (Proposition 8.5.1). Let us show that $u$ and $\boldsymbol { v }$ lie in different components of $X$ . If they lie in the same component, $A$ say, then $A$ is closed in $X$ (being a component) and hence in $| G |$ , so $A$ is arc-connected by Lemma 8.5.4. But any $u$ – $v$ arc in $A$ forms a circle with $e$ that contradicts the maximality of $Z$ . Thus, $u$ and $v$ lie in different components of $X$ .

By Lemma 8.5.3, $X$ is a union of disjoint open subsets $X _ { u } \ni u$ and $X _ { v } ~ \ni ~ v$ . Put $V _ { u } : = X _ { u } \cap V$ and $V _ { v } : = X _ { v } \cap V$ . As $X _ { u }$ and $X _ { v }$ are complements in $X$ , they are closed (as well as open) in $X$ and hence closed in $| G |$ , so $\overline { { V _ { u } } } \subseteq X _ { u }$ and ${ \overline { { V _ { v } } } } \subseteq X _ { v }$ . In particular, $\overline { { V _ { u } } } \cap \overline { { V _ { v } } } \ : = \ : \emptyset$ , so by Lemma 8.5.5 (i) the cut $F : = E ( V _ { u } , V _ { v } )$ of $G$ is finite. Moreover, $F \cap Z ^ { \prime } = \left\{ e \right\}$ , since every other edge of $Z ^ { \prime }$ lies in $X$ , and hence in $X _ { u }$ or in $X _ { v }$ . As $Z ^ { \prime } \in { \mathcal { C } } ( G )$ , this contradicts (i).

(iii) In our proof of (i) we already proved the most important case of (iii), where the topological spanning tree in question is the closure of a normal spanning tree. The proof for arbitrary topological spanning trees is the same, except for the proof that all their fundamental cuts are finite (Exercise 74). 

Corollary 8.5.9. ${ \mathcal { C } } ( G )$ is generated by finite circuits, and is closed under infinite (thin) sums.

Proof . By Theorem 8.2.4, $G$ has a normal spanning tree, $T$ say. By (8.2.4) Lemma 8.5.7, its closure $\overline { T }$ in $| G |$ is a topological spanning tree. The fundamental circuits of $\overline { T }$ coincide with those of $T$ , and are therefore finite. By Theorem 8.5.8 (iii), they generate ${ \mathcal { C } } ( G )$ .

Let $\textstyle \sum _ { i \in I } D _ { i }$ be a sum of elements of ${ \mathcal { C } } ( G )$ . By Theorem 8.5.8 (ii), each $D _ { i }$ is a disjoint union of circuits. Together, these form a thin family, whose sum equals $\textstyle \sum _ { i \in I } D _ { i }$ and lies in ${ \mathcal { C } } ( G )$ . 

To complete this section, we apply our new notions to extend the tree-packing theorem of Nash-Williams and Tutte (2.4.1) to locally finite graphs. Note that all our definitions extend naturally to multigraphs.

Theorem 8.5.10. The following statements are equivalent for all $k \in \mathbb N$ and locally finite multigraphs $G$ :

(i) $G$ has $k$ edge-disjoint topological spanning trees.   
(ii) For every finite partition of $V ( G )$ , into $\ell$ sets say, $G$ has at least $k \left( \ell - 1 \right)$ cross-edges.

We begin our proof of Theorem 8.5.10 with a compactness extension of the finite theorem, which will give us a slightly weaker statement at the limit. Following Tutte, let us call a spanning submultigraph $H$ of $G$ semiconnected in $G$ if every finite cut of $G$ contains an edge of $H$ .

Lemma 8.5.11. If for every finite partition of $V ( G )$ , into $\ell$ sets say, $G$ has at least $k \left( \ell - 1 \right)$ cross-edges, then $G$ has $k$ edge-disjoint semiconnected spanning subgraphs.

Proof . Pick an enumeration $v _ { 0 } , v _ { 1 } , . . .$ of $V ( G )$ . For every $n \in \mathbb N$ let $G _ { n }$ be the finite multigraph obtained from $G$ by contracting every component of $G - \{ v _ { 0 } , \ldots , v _ { n } \}$ to a vertex, deleting any loops but no parallel edges that arise in the contraction. Then $G [ v _ { 0 } , \ldots , v _ { n } ]$ is an induced submultigraph of $G _ { n }$ . Let $\nu _ { n }$ denote the set of all $k$ -tuples $( H _ { n } ^ { 1 } , \ldots , H _ { n } ^ { k } )$ 5 of edge-disjoint connected spanning subgraphs of $G _ { n }$ .

Since every partition $P$ of $V ( G _ { n } )$ induces a partition of $V ( G )$ , since $G$ has enough cross-edges for that partition, and since all these crossedges are also cross-edges of $P$ , Theorem 2.4.1 implies that $\nu _ { n } ~ \neq ~ \emptyset$ . Since every $( H _ { n } ^ { 1 } , \ldots , H _ { n } ^ { k } ) \in \mathcal { V } _ { n }$ induces an element $( H _ { n - 1 } ^ { 1 } , \ldots , H _ { n - 1 } ^ { k } )$ of $\nu _ { n - 1 }$ n n V, the infinity lemma (8.1.2), yields a sequence $( H _ { n } ^ { 1 } , \ldots , H _ { n } ^ { k } ) _ { n \in \mathbb { N } }$ of $k$ -tuples, one from each $\nu _ { n }$ , with a limit $( H ^ { 1 } , \ldots , H ^ { k } )$ n defined by the nested unions

$$
H ^ {i} := \bigcup_ {n \in \mathbb {N}} H _ {n} ^ {i} \left[ v _ {0}, \ldots , v _ {n} \right].
$$

These $H ^ { i }$ are edge-disjoint for distinct $_ i$ (because the $H _ { n } ^ { i }$ are), but they need not be connected. To show that they are semiconnected in $G$ ,

consider a finite cut $F$ of $G$ . Choose $n$ large enough that all the endvertices of edges in $F ^ { \prime }$ are among $v _ { 0 } , \ldots , v _ { n }$ . Then $F$ is also a cut of $G _ { n }$ . Now consider the $k$ -tuple $( H _ { n } ^ { 1 } , \ldots , H _ { n } ^ { k } )$ which the infinity lemma picked from $\nu _ { n }$ . Each of these $H _ { n } ^ { i }$ is a connected spanning subgraph of $G _ { n }$ , so it contains an edge from $F$ . But $H _ { n } ^ { i }$ agrees with $H ^ { i }$ on $\{ v _ { 0 } , \ldots , v _ { n } \}$ , so $H ^ { i }$ too contains this edge from $F$ . 

At first glance, the notion of semiconnectedness appears to be somewhat ad-hoc: it summarizes what happens to be left of the connectedness of the graphs $H _ { n } ^ { i }$ at their limit $H ^ { i }$ —and this, no doubt, is why Tutte introduced it. In our context, however, it acquires an unexpected natural meaning:

Lemma 8.5.12. A spanning subgraph $H \subseteq G$ is semiconnected in $G$ if and only if its closure $\overline { H }$ in $| G |$ is topologically connected.

Proof . If $\overline { H }$ is disconnected, it is contained in the union of two closed subsets $O _ { 1 } , O _ { 2 }$ of $| G |$ that both meet $\overline { H }$ and satisfy $O _ { 1 } \cap O _ { 2 } \cap { \overline { { H } } } = \emptyset$ . Since $\overline { H }$ is a standard subspace containing $V ( G )$ , the sets $O _ { i }$ partition $V ( G )$ into two non-empty sets $X _ { 1 } , X _ { 2 }$ . Then

$$
\overline {{X}} _ {1} \cap \overline {{X}} _ {2} \subseteq O _ {1} \cap O _ {2} \cap \Omega (G) \subseteq O _ {1} \cap O _ {2} \cap \overline {{H}} = \emptyset .
$$

By Lemma 8.5.5 (i), this implies that $G$ has only finitely many $X _ { 1 } { - } X _ { 2 }$ edges. As edges are connected, none of them can lie in $H$ . Hence, $H$ is not semiconnected.

The converse implication is straightforward (and not needed in our proof of Theorem 8.5.10): a finite cut of $G$ containing no edge of $H$ defines a partition of $\overline { H }$ into non-empty open subsets, showing that $\overline { H }$ is disconnected. 

Lemma 8.5.13. Every closed, connected, standard subspace $X$ of $| G |$ that contains $V ( G )$ also contains a topological spanning tree of $G$ .

Proof . By Lemma 8.5.4, $X$ is arc-connected. Since $X$ contains all vertices, $G$ cannot be disconnected, so its local finiteness implies that it is countable. Let $e _ { 0 } , e _ { 1 } , \ldots$ be an enumeration of the edges in $X$ .

We now delete these edges one by one, keeping $X$ arc-connected. Starting with $X _ { 0 } : = X$ , we define $X _ { n + 1 } : = X _ { n } \setminus \check { e } _ { n }$ if this keeps $X _ { n + 1 }$ arcconnected; if not, we put $X _ { n + 1 } : = X _ { n }$ . Finally, we put $T : = \lceil \rceil _ { n \in \mathbb { N } } X _ { n }$ .

Clearly, $T$ is closed, contains every vertex and every end of $G$ , but contains no circle: any circle in $T$ would contain an edge, which should have got deleted. To show that $T$ is arc-connected, it suffices by Lemmas 8.5.4 and 8.5.12 to check that every finite cut of $G$ contains an edge from $T$ . By Lemma 8.5.5 (ii), the edges in such a cut could not all be deleted, so one of them lies in $T$ . 

Proof of Theorem 8.5.10. The implication (ii) (i) follows from our three lemmas. For (i) (ii), let $G$ have edge-disjoint topological spanning trees $T _ { 1 } , \dots , T _ { k }$ , and consider a partition $P$ of $V ( G )$ into $\ell$ sets. If there are infinitely many cross-edges, there is nothing to show; so we assume there are only finitely many. For each $i \in \{ 1 , \ldots , k \}$ , let $T _ { i } ^ { \prime }$ be the multigraph of order $\ell$ which the edges of $T _ { i }$ induce on $P$ .

To establish that $G$ has at least $k ( \ell - 1 )$ cross-edges, we show that the graphs $T _ { i } ^ { \prime }$ are connected. If not, then some $T _ { i } ^ { \prime }$ has a vertex partition crossed by no edge of $T _ { i }$ . This partition induces a cut of $G$ that contains no edge of $T _ { i }$ . By our assumption that $G$ has only finitely many crossedges, this cut is finite. By Lemma 8.5.5 (ii), this contradicts the arcconnectedness of $T _ { i }$ . 

# Exercises

1.− Show that a connected graph is countable if all its vertices have countable degrees.   
2.− Given countably many sequences $\sigma ^ { i } = s _ { 1 } ^ { i } , s _ { 2 } ^ { i } , . . .$ . $i \in \mathbb N$ ) of natural numbers, find one sequence $\sigma = s _ { 1 } , s _ { 2 } , . . . .$ that beats every $\sigma ^ { i }$ eventually, i.e. such that for every $_ i$ there exists an $n ( i )$ such that $s _ { n } > s _ { n } ^ { i }$ for all $n \geqslant n ( i )$ .   
3. Can a countable set have uncountably many subsets whose intersections have finitely bounded size?   
4. $-$ Let $T$ be an infinite rooted tree. Show that every ray in $T$ has an increasing tail, that is, a tail whose sequence of vertices increases in the tree-order associated with $T$ and its root.   
5. $-$ Let $G$ be an infinite graph and $A , B \subseteq V ( G )$ . Show that if no finite set of vertices separates $A$ from $B$ in $G$ , then $G$ contains an infinite set of disjoint $A$ – $B$ paths.   
6. $-$ In Proposition 8.1.1, the existence of a spanning tree was proved using Zorn’s lemma ‘from below’, to find a maximal acyclic subgraph. For finite graphs, one can also use induction ‘from above’, to find a minimal spanning connected subgraph. What happens if we apply Zorn’s lemma ‘from above’ to find such a subgraph?   
7.− Show that for every $k \in \mathbb N$ there exists an infinitely connected graph of girth at least $k$ .   
8. Construct, for any given $k \in \mathbb N$ , a planar $k$ -connected graph. Can you construct one whose girth is also at least $k$ ? Can you construct an infinitely connected planar graph?

9.− Theorem 8.1.3 implies that there exists an $\ N \longrightarrow \ N$ function $f _ { \chi }$ such that, for every $k \in \mathbb N$ , every infinite graph of chromatic number at least $f _ { x } ( k )$ has a finite subgraph of chromatic number at least $k$ . (Namely, let $f _ { \chi }$ be the identity on $\mathbb { N }$ .) Are there similar functions $f _ { \delta }$ and $f _ { \kappa }$ for the minimum degree and connectivity?   
10. Prove Theorem 8.1.3 for countable graphs using the fact that, in this case, the topological space $X$ defined in the second proof of the theorem is sequentially compact. (Thus, every infinite sequence of points in $X$ has a convergent subsequence: there is an $x \in X$ such that every neighbourhood of $X$ contains a tail of the subsequence.)   
11.+ Show that, given $k \in \mathbb N$ and an edge $e$ in a graph $G$ , there are only finitely many bonds in $G$ that consist of exactly $k$ edges and contain $e$ .   
12. $-$ Extend Theorem 2.4.4 to infinite graphs.   
13. Rephrase Gallai’s cycle-cocycle partition theorem (Ex. 35, Ch. 1) in terms of degrees, and extend the equivalent version to locally finite graphs.   
14. Prove Theorem 8.4.8 for locally finite graphs. Does your proof extend to arbitrary countable graphs?   
15. Extend the marriage theorem to locally finite graphs, but show that it fails for countable graphs with infinite degrees.   
16.+ Show that a locally finite graph $G$ has a 1-factor if and only if, for every finite set $S \subseteq V ( G )$ , the graph $G - S$ has at most $| S |$ odd (finite) components. Find a counterexample that is not locally finite.   
17. $^ +$ Extend Kuratowski’s theorem to countable graphs.   
18. $-$ A vertex $v \in G$ is said to dominate an end $\omega$ of $G$ if any of the following three assertions holds; show that they are equivalent.

(i) For some ray $R \in \omega$ there is an infinite $v$ – $R$ fan in $G$   
(ii) For every ray $R \in \omega$ there is an infinite $v$ – $R$ fan in $G$ .   
(iii) No finite subset of $V ( G - v )$ separates $\boldsymbol { v }$ from a ray in $\omega$ .

19. Show that a graph $G$ contains a $T K ^ { \aleph _ { 0 } }$ if and only if some end of $G$ is dominated by infinitely many vertices.   
20. Construct a countable graph with uncountably many thick ends.   
21. Show that a countable tree has uncountably many ends if and only if it contains a subdivision of the binary tree $T _ { 2 }$ .   
22. A graph $G \ : = \ : ( V , E )$ is called bounded if for every vertex labelling $\ell \colon V  \mathbb { N }$ there exists a function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ that exceeds the labelling along any ray in $G$ eventually. (Formally: for every ray $v _ { 1 } v _ { 2 } \ldots$ in $G$ there exists an $n _ { 0 }$ such that $f ( n ) > \ell ( v _ { n } )$ for every $n > n _ { 0 }$ .) Prove the following assertions:

(i) The ray is bounded.   
(ii) Every locally finite connected graph is bounded.

(iii)+ A countable tree is bounded if and only if it contains no subdivision of the $\aleph _ { 0 }$ -regular tree $T _ { \aleph _ { 0 } }$ .

23.+ Let $T$ be a tree with root $r$ , and let $\leqslant$ denote the tree-order on $V ( T )$ associated with $T$ and $r$ . Show that $T$ contains no subdivision of the $\aleph _ { 1 }$ -regular tree $T _ { \aleph _ { 1 } }$ if and only if $T$ has an ordinal labelling $t \mapsto o ( t )$ such that $o ( t ) \geqslant o ( t ^ { \prime } )$ whenever $t \ : < \ : t ^ { \prime }$ but no more than countably many vertices of $T$ have the same label.   
24. Show that a locally finite connected vertex-transitive graph has exactly 0, 1, 2 or infinitely many ends.   
25.+ Show that the automorphisms of a graph $G = ( V , E )$ act naturally on its ends, i.e., that every automorphism $\sigma { : } V \to V$ can be extended to a map $\sigma { \colon } \Omega ( G ) \to \Omega ( G )$ such that $\sigma ( R ) \in \sigma ( \omega )$ whenever $R$ is a ray in an end $\omega$ . Prove that, if $G$ is connected, every automorphism $\sigma$ of $G$ fixes a finite set of vertices or an end. If $\sigma$ fixes no finite set of vertices, can it fix more than one end? More than two?   
26.− Show that a locally finite spanning tree of a graph $G$ contains a ray from every end of $G$ .   
27. A ray in a graph follows another ray if the two have infinitely many vertices in common. Show that if $T$ is a normal spanning tree of $G$ then every ray of $G$ follows a unique normal ray of $T$ .   
28. Show that the following assertions are equivalent for connected countable graphs $G$ .

(i) $G$ has a locally finite spanning tree.   
(ii) $G$ has a locally finite normal spanning tree.   
(iii) Every normal spanning tree of $G$ is locally finite.   
(iv) For no finite separator $X \subseteq V ( G )$ does $G - X$ have infinitely many components.

29. Use the previous exercise to show that every (countable) planar 3- connected graph has a locally finite spanning tree.   
30. Let $G$ be a connected graph. Call a set $U \subseteq V ( G )$ dispersed if every ray in $G$ can be separated from $U$ by a finite set of vertices. (In the topology of Section 8.5, these are precisely the closed subsets of $V ( G )$ .)

(i) Prove Jung’s theorem that $G$ has a normal spanning tree if and only if $V ( G )$ is a countable union of dispersed sets.   
(ii) Deduce that if $G$ has a normal spanning tree then so does every connected minor of $G$ .

31. $-$ Use Exercise 21 to prove that a countable graph with uncountably many ends has continuum many ends.   
32.+ Show that the vertices of any infinite connected locally finite graph can be enumerated in such a way that every vertex is adjacent to some later vertex.

33. (i) Prove that if a given end of a graph contains $k$ disjoint rays for every $k \in \mathbb N$ then it contains infinitely many disjoint rays.   
(ii) Prove that if a given end of a graph contains $k$ edge-disjoint rays for every $k \in \mathbb N$ then it contains infinitely many edge-disjoint rays.   
34.+ Prove that if a graph contains $k$ disjoint double rays for every $k \in \mathbb N$ then it contains infinitely many disjoint double rays.   
35. Show that, in the ubiquity conjecture, the host graphs $G$ considered can be assumed to be locally finite too.   
36. Show that the modified comb below is not ubiquitous with respect to the subgraph relation. Does it become ubiquitous if we delete its 3-star on the left?

![](images/cc5f0f836a1622905dac00e6387b342ac049df5c7200c447a00a35d9f385f810.jpg)

37. Show that if a graph contains infinitely many distinct cycles then it contains infinitely many edge-disjoint cycles.   
38. Imitate the proof of Theorem 8.2.6 to find a function $f { : \mathbb { N } } \to \mathbb { N }$ such that whenever an end $\omega$ of a graph $G$ contains $f ( k )$ disjoint rays there is a $k \times \mathbb N$ grid in $G$ whose rays all belong to $\omega$ .   
39. Show that there is no universal locally finite connected graph for the subgraph relation.   
40. Construct a universal locally finite connected graph for the minor relation. Is there one for the topological minor relation?   
41. $-$ Show that each of the following operations performed on the Rado graph $R$ leaves a graph isomorphic to $R$ :

(i) taking the complement, i.e. changing all edges into non-edges and vice versa;   
(ii) deleting finitely many vertices;   
(iii) changing finitely many edges into non-edges or vice versa;   
(iv) changing all the edges between a finite vertex set $X \subseteq V ( R )$ and its complement $V ( R ) \setminus X$ into non-edges, and vice versa.

42.− Prove that the Rado graph is homogeneous.

43. Show that a homogeneous countable graph is determined uniquely, up to isomorphism, by the class of (the isomorphism types of) its finite subgraphs.   
44. Recall that subgraphs $H _ { 1 } , H _ { 2 } , \ldots$ of a graph $G$ are said to partition $G$ if their edge sets form a partition of $E ( G )$ . Show that the Rado graph can be partitioned into any given countable set of countable locally finite graphs, as long as each of them contains at least one edge.

45.− A linear order is called dense if between any two elements there lies a third.

(i) Find, or construct, a countable dense linear order that has neither a maximal nor a minimal element.   
(ii) Show that this order is unique, i.e. that every two such orders are order-isomorphic. (Definition?)   
(iii) Show that this ordering is universal among the countable linear orders. Is it homogeneous? (Supply appropriate definitions.)

46. Given a bijection $f$ between N and $[ \mathbb { N } ] ^ { < \omega }$ , let $G _ { f }$ be the graph on N in which $u , v \in \mathbb { N }$ are adjacent if $u \in f ( v )$ or vice versa. Prove that all such graphs $G _ { f }$ are isomorphic.   
47. (for set theorists) Show that, given any countable model of set theory, the graph whose vertices are the sets and in which two sets are adjacent if and only if one contains the other as an element, is the Rado graph.   
48. Let $G$ be a locally finite graph. Let us say that a finite set $S$ of vertices separates two ends $\omega$ and $\omega ^ { \prime }$ if $C ( S , \omega ) \neq C ( S , \omega ^ { \prime } )$ . Use Proposition 8.4.1 to show that if $\omega$ can be separated from $\omega ^ { \prime }$ by $k \in \mathbb N$ but no fewer vertices, then $G$ contains $k$ disjoint double rays each with one tail in $\omega$ and one in $\omega ^ { \prime }$ . Is the same true for all graphs that are not locally finite?   
49.+ Prove the following more structural version of Exercise 33 (i). Let $\omega$ be an end of a countable graph $G$ . Show that either $G$ contains a $T K ^ { \aleph _ { 0 } }$ with all its rays in $\omega$ , or there are disjoint finite sets $S _ { 0 } , S _ { 1 } , S _ { 2 } , . . .$ such that $| S _ { 1 } | \leqslant | S _ { 2 } | \leqslant \ldots .$ and, with $C _ { i } : = C ( S _ { 0 } \cup S _ { i } , \omega )$ , we have for all $i < j$ that $C _ { i } \supseteq C _ { j }$ and $G _ { i } : = G \left[ S _ { i } \cup C _ { i } \right]$ contains $| S _ { i } |$ disjoint $S _ { i } { - } S _ { i + 1 }$ paths.   
50. Construct an example of a small limit of large waves.   
51. $^ +$ Prove Theorem 8.4.2 for trees.   
52.+ Prove Pym’s theorem (8.4.7).   
53. (i) $-$ Prove the naive extension of Dilworth’s theorem to arbitrary infinite posets $P$ : if $P$ has no antichain of order $k \in \mathbb N$ , then $P$ can be partitioned into fewer than $k$ chains. (A proof for countable $P$ will do.)   
(ii) $-$ Find a poset that has no infinite antichain and no partition into finitely many chains.   
(iii) For posets without infinite chains, deduce from Theorem 8.4.8 the following Erd˝os-Menger-type extension of Dilworth’s theorem: every such poset has a partition $c$ into chains such that some antichain meets all the chains in $c$ .   
54. Let $G$ be a countable graph in which for every partial matching there is an augmenting path. Let $M$ be any matching. Is there a sequence, possibly transfinite, of augmenting paths (each for the then current matching) that turns $M$ into a 1-factor?   
55. Find an uncountable graph in which every partial matching admits an augmenting path but which has no 1-factor.

56. Construct a locally finite factor-critical graph (or prove that none exists).   
57. $-$ Let $G$ be a countable graph whose finite subgraphs are all perfect. Show that $G$ is weakly perfect but not necessarily perfect.   
58.+ Let $G$ be the incomparability graph of the binary tree. (Thus, $V ( G ) =$ $V ( T _ { 2 } )$ , and two vertices are adjacent if and only if they are incomparable in the tree-order of $T _ { 2 }$ .) Show that $G$ is perfect but not strongly perfect.   
59. Let $G$ be a graph, $X \subseteq V ( G )$ , and $R \in \omega \in \Omega ( G )$ . Show that $G$ contains a comb with spine $R$ and teeth in $X$ if and only if $\omega \in { \overline { { X } } }$ .   
60. Give an independent proof of Proposition 8.5.1 using sequential compactness and the infinity lemma.   
61.+ Let $G$ be a connected countable graph that is not locally finite. Show that $| G |$ is not compact, but that $\Omega ( G )$ is compact if and only if for every finite set $S \subseteq V ( G )$ only finitely many components of $G - S$ contain a ray.   
62. Given graphs $H \subseteq G$ , let $\eta \colon \Omega ( H ) \to \Omega ( G )$ assign to every end of $H$ the unique end of $G$ containing it as a subset (of rays). For the following questions, assume that $H$ is connected and $V ( H ) = V ( G )$ .

(i) Show that $\eta$ need not be injective. Must it be surjective?   
(ii) Investigate how $\eta$ relates the subspace $\Omega ( H )$ of $| H |$ to its image in $| G |$ . Is $\eta$ always continuous? Is it open? Do the answers to these questions change if $\eta$ is known to be injective?   
(iii) A spanning tree is called end-faithful if $\eta$ is bijective, and topologically end-faithful if $\eta$ is a homeomorphism. Show that every connected countable graph has a topologically end-faithful spanning tree.

63.+ Let $G$ be a connected graph. Assuming that $G$ has a normal spanning tree, define a metric on $| G |$ that induces its usual topology. Conversely, use Jung’s theorem of Exercise 30 to show that if $V \cup \Omega \ \subseteq \ | G |$ is metrizable then $G$ has a normal spanning tree.   
64.+ (for topologists) In a locally compact, connected, and locally connected Hausdorff space $X$ , consider sequences $U _ { 1 } \supseteq U _ { 2 } \supseteq \dots$ . of open, nonempty, connected subsets with compact frontiers such that $\bigcap _ { i \in \mathbb { N } } { \overline { { U _ { i } } } } = \emptyset$ . Call such a sequence equivalent to another such sequence if every set of one sequence contains some set of the other, and vice versa. Note that this is indeed an equivalence relation, and call its classes the Freudenthal ends of $X$ . Now add these to the space $X$ , and define a natural topology on the extended space $\hat { X }$ that makes it homeomorphic to $\lvert X \rvert$ if $X$ is a graph, by a homeomorphism that is the identity on $X$ .   
65. Let $F ^ { \prime }$ be a set of edges in a locally finite graph $G$ , and let $A : = \overline { { \bigcup F } }$ be its closure in $| G |$ . Show that $F ^ { \prime }$ is a circuit if and only if, for every two edges $e , e ^ { \prime } \in F$ , the set $A \setminus { \check { e } }$ is connected but $A \setminus \left( \mathring { e } \cup \mathring { e } ^ { \prime } \right)$ is disconnected in $| G |$ .

66. Does every infinite locally finite 2-connected graph contain an infinite circuit? Does it contain an infinite bond?   
67. Show that the union of all the edges contained in an arc or circle $C$ in $| G |$ is dense in $C$ .   
68. Let $T$ be a spanning tree of a graph $G$ . Note that $\overline { T }$ is a connected subset of $| G |$ . Without using Lemma 8.5.4, show that if $T$ is locally finite then $\overline { T }$ is arc-connected. Find an example where $\overline { T }$ is not arcconnected.   
69. Prove that the circle shown in Figure 8.5.1 is really a circle, by exhibiting a homeomorphism with $S ^ { 1 }$ .   
70. Deduce Lemma 8.5.6 from Lemma 8.5.4.   
71. Let $G$ be a connected locally finite graph. Show that the following assertions are equivalent for a spanning subgraph $T$ of $G$ :

(i) $\overline { T }$ is a topological spanning tree of $| G |$ ;   
(ii) $T$ is edge-maximal such that $\overline { T }$ contains no circle;   
(iii) $T$ is edge-minimal with $\overline { T }$ arc-connected.

72. $-$ Observe that a topological spanning tree need not be homeomorphic to a tree. Is it homeomorphic to the space $| T |$ for a suitable tree $T$ ?   
73. Show that connected graphs with only one end have topological spanning trees.   
74.+ Let $G$ be a locally finite graph and $X$ a standard subspace of $| G |$ . Prove that arc-components $A$ of $X$ are closed in $X$ . Deduce that the fundamental cuts of any topological spanning tree of $G$ are finite.   
75. To show that Theorem 3.2.3 does not generalize to infinite graphs with the ‘finite’ cycle space as defined in Chapter 1.9, construct a 3- connected locally finite planar graph with a separating cycle that is not a finite sum of non-separating induced cycles. Can you find an example where even infinite sums of finite non-separating induced cycles do not generate all separating cycles?   
76.− As a converse to Theorem 8.5.8 (iii), show that the fundamental circuits of an ordinary spanning tree $T$ of a locally finite graph $G$ do not generate ${ \mathcal { C } } ( G )$ unless $\overline { T }$ is a topological spanning tree.   
77. Prove that the edge set of a countable graph $G$ can be partitioned into finite circuits if $G$ has no odd cut. Where does your argument break down if $G$ is uncountable?   
78. Explain why Theorem 8.5.8 (ii) is needed in the proof of Corollary 8.5.9: can’t we just combine the constituent sums of circuits for the $D _ { i }$ (from our assumption that $D _ { i } \in { \mathcal { C } } ( G )$ ) into one big family? If not, can you still prove the same statement without appealing to Theorem 8.5.8 (ii)?

79.+ Call a continuous (but not necessarily injective) map $\sigma \colon S ^ { 1 } \to | G |$ a topological Euler tour of $G$ if every inner point of an edge of $G$ is the image of exactly one point of $S ^ { 1 }$ . (Thus, every edge is traversed exactly once, and in a ‘straight’ manner.) Use Theorem 8.5.8 (ii) to show that $G$ admits a topological Euler tour if and only if $G$ is connected and $E ( G ) \in { \mathcal { C } } ( G )$ .

80.+ An open Euler tour in an infinite graph $G$ is a 2-way infinite walk $\cdots e _ { - 1 } v _ { 0 } e _ { 0 } \ldots$ that contains every edge of $G$ exactly once. Show that $G$ contains an open Euler tour if and only if $G$ is countable, $G$ is connected, every vertex has even or infinite degree, and any finite cut $F = E ( V _ { 1 } , V _ { 2 } )$ with both $V _ { 1 }$ and $V _ { 2 }$ infinite is odd.

# Notes

There is no comprehensive monograph on infinite graph theory, but over time several surveys have been published. A relatively wide-ranging collection of survey articles can be found in R. Diestel (ed.), Directions in Infinite Graph Theory and Combinatorics, North-Holland 1992. (This has been reprinted as Volume 95 of the journal Discrete Mathematics.) Some of the articles there address purely graph-theoretic aspects of infinite graphs, while others point to connections with other fields in mathematics such as differential geometry, topological groups, or logic.

A survey of infinite graph theory as a whole was given by C. Thomassen, Infinite graphs, in (L.W. Beineke & R.J. Wilson, eds.) Selected Topics in Graph Theory 2, Academic Press 1983. This also treats a number of aspects of infinite graph theory not considered in our chapter here, including problems of Erd˝os concerning infinite chromatic number, infinite Ramsey theory (also known as partition calculus), and reconstruction. The first two of these topics receive much attention also in A. Hajnal’s chapter of the Handbook of Combinatorics (R.L. Graham, M. Gr¨otschel & L. Lov´asz, eds.), North-Holland 1995, which has a strong set-theoretical flavour. (See the end of these notes for more references in this direction.) A specific survey on reconstruction by Nash-Williams can be found in the Directions volume cited above. A relatively recent collection of various unsolved problems is offered in R. Halin, Miscellaneous problems on infinite graphs, J. Graph Theory 35 (2000), 128–151.

A good general reference for infinite graphs (as well as finite) is R. Halin, Graphentheorie (2nd ed.), Wissenschaftliche Buchgesellschaft 1989. A more specific monograph on the theory of simplicial decompositions (see Chapter 12) is R. Diestel, Graph Decompositions, Oxford University Press 1990. Chapter 12.4 closes with a few theorems about forbidden minors in infinite graphs.

Infinite graph theory has a number of interesting individual results which, as yet, stand essentially by themselves. One such is a theorem of A. Huck, F. Niedermeyer and S. Shelah, Large $\kappa$ -preserving sets in infinite graphs, $J$ . Graph Theory 18 (1994), 413–426, which says that every infinitely connected graph $G$ has a set $S$ of $| G |$ vertices such that $\kappa ( G - S ^ { \prime } ) = \kappa ( G )$ for every $S ^ { \prime } \subseteq S$ . Another is Halin’s bounded graph conjecture, which characterizes the bounded graphs by four forbidden substructures. (See Exercise 22 (iii) for

the definition of ‘bounded’ and the tree case of the conjecture.) A proof can be found in R. Diestel & I.B. Leader, A proof of the bounded graph conjecture, Invent. math. 108 (1992), 131–162.

K¨onig’s infinity lemma, or K¨onig’s lemma for short, is as old as the firstever book on graph theory, which includes it: D. K¨onig, Theorie der endlichen und unendlichen Graphen, Akademische Verlagsgesellschaft, Leipzig 1936. In addition to this and Tychonoff’s theorem, compactness proofs can also come in the following two guises (see Hajnal’s Handbook chapter): as applications of Rado’s selection lemma, or of G¨odel’s compactness theorem from first-order logic. Both are logically equivalent to Tychonoff’s theorem; the choice of which to use is more a matter of familiarity with one terminology or the other than of any material importance.

Theorem 8.1.3 is due to N. G. de Bruijn and P. Erd˝os, A colour problem for infinite graphs and a problem in the theory of relations, Indag. Math. 13 (1951), 371–373. Unlike for the chromatic number, a bound on the colouring number of all finite subgraphs does not extend to the whole graph by compactness. P. Erd˝os & A. Hajnal, On the chromatic number of graphs and set systems, Acta Math. Acad. Sci. Hung. 17 (1966), 61–99, proved that if every finite subgraph of $G$ has colouring number at most $k$ then $G$ has colouring number at most $2 k - 2$ , and showed that this is best possible.

The unfriendly partition conjecture is one of the best-known open problems in infinite graph theory, but there are few results. E.C. Milner and S. Shelah, Graphs with no unfriendly partitions, in (A. Baker, B. Bollob´as & A. Hajnal, eds.), A tribute to Paul Erd˝os, Cambridge University Press 1990, construct an uncountable counterexample, but show that every graph has an unfriendly partition into three classes. (The original conjecture, which they attribute to R. Cowan and W. Emerson (unpublished), appears to have asserted for every graph the existence of a vertex partition into any given finite number of classes such that every vertex has at least as many neighbours in other classes as in its own.) Some positive results for bipartitions were obtained by R. Aharoni, E.C. Milner and K. Prikry, Unfriendly partitions of graphs, J. Combin. Theory B 50 (1990), 1–10.

Theorem 8.2.4 is a special case of the result stated in Exercise 30 (i), which is due to H.A. Jung, Wurzelb¨aume und unendliche Wege in Graphen, Math. Nachr. 41 (1969), 1–22. The graphs that admit a normal spanning tree can be characterized by forbidden minors: as shown in R. Diestel & I. Leader, Normal spanning trees, Aronszajn trees and excluded minors, J. London Math. Soc. 63 (2001), 16–32, there are two types of graphs that are easily seen not to have normal spanning trees, and one of these must occur as a minor in every graph without a normal spanning tree. Note that such a characterization is possible only because the class of graphs admitting a normal spanning tree is closed under taking connected minors—a consequence of Jung’s theorem (see Exercise 30 (ii)) for which, oddly, no direct proof is known. One corollary of the characterization is that a connected graph has a normal spanning tree if and only if all its minors have countable colouring number.

Theorems 8.2.5 and 8.2.6 are from R. Halin, Uber die Maximalzahl frem-¨ der unendlicher Wege, Math. Nachr. 30 (1965), 63–85. Our proof of Theorem 8.2.5 is due to Andreae (unpublished); our proof of Theorem 8.2.6 is new. Halin’s paper also includes a structure theorem for graphs that do not contain

infinitely many disjoint rays. Except for a finite set of vertices, such a graph can be written as an infinite chain of rayless subgraphs each overlapping the previous in exactly $m$ vertices, where $m$ is the maximum number of disjoint rays (which exists by Theorem 8.2.5). These overlap sets are disjoint, and there are $m$ disjoint rays containing exactly one vertex from each of them.

A good reference on ubiquity, including the ubiquity conjecture, is Th. Andreae, On disjoint configurations in infinite graphs, J. Graph Theory 39 (2002), 222–229.

Universal graphs have been studied mostly with respect to the induced subgraph relation, with numerous but mostly negative results. See G. Cherlin, S. Shelah & N. Shi, Universal graphs with forbidden subgraphs and algebraic closure, Adv. Appl. Math. 22 (1999), 454–491, for an overview and a modeltheoretic framework for the proof techniques typically applied.

The Rado graph is probably the best-studied single graph in the graph theory literature (with the Petersen graph a close runner-up). The most comprehensive source for anything related to it (and far beyond) is R. Fra¨ıss´e, Theory of Relations (2nd edn.), Elsevier 2000. More accessible introductions are given by N. Sauer in his appendix to Fra¨ıss´e’s book, and by P.J. Cameron, The random graph, in (R.L. Graham & J. Neˇsetˇril, eds.): The Mathematics of Paul Erd˝os, Springer 1997, and its references.

Theorem 8.3.1 is due to P. Erd˝os and A. R´enyi, Asymmetric graphs, Acta Math. Acad. Sci. Hung. 14 (1963), 295–315. The existence part of their proof is probabilistic and will be given in Theorem 11.3.5. Rado’s explicit definition of the graph $R$ was given in R. Rado, Universal graphs and universal functions, Acta Arithm. 9 (1964), 393–407. However, its universality and that of $R ^ { \prime }$ are already included in more general results of B. J´onsson, Universal relational systems, Math. Scand. 4 (1956), 193–208.

Theorem 8.3.3 is due to A.H. Lachlan and R.E. Woodrow, Countable ultrahomogeneous undirected graphs, Trans. Amer. Math. Soc. 262 (1980), 51– 94. The classification of the countable homogeneous directed graphs is much more difficult still. It was achieved by G. Cherlin, The classification of countable homogeneous directed graphs and countable homogeneous $n$ -tournaments, Mem. Am. Math. Soc. 621 (1998), which also includes a shorter proof of Theorem 8.3.3.

Proposition 8.3.2, too, has a less trivial directed analogue: the countable directed graphs that are isomorphic to at least one of the two sides induced by any bipartition of their vertex set are precisely the edgeless graph, the random tournament, the transitive tournaments of order type $\omega ^ { \alpha }$ , and two specific orientations of the Rado graph (R. Diestel, I. Leader, A. Scott & S. Thomass´e, Partitions and orientations of the Rado graph, Trans. Amer. Math. Soc. (to appear).

Theorem 8.3.4 is proved in R. Diestel & D. K¨uhn, A universal planar graph under the minor relation, J. Graph Theory 32 (1999), 191–206. It is not known whether or not there is a universal planar graph for the topological minor relation. However it can be shown that there is no minor-universal graph for embeddability in any closed surface other than the sphere; see the above paper.

When Erd˝os conjectured his extension of Menger’s theorem is not known; C.St.J.A. Nash-Williams, Infinite graphs – a survey, J. Combin. Theory B 3

(1967), 286–301, cites the proceedings of a 1963 conference as its source. Its proof as Theorem 8.4.2 by Aharoni and Berger, Menger’s theorem for infinite graphs (preprint 2005), came as the culmination of a long effort over many years, for the most part also due to Aharoni. Our proof of its countable case is adapted from R. Aharoni, Menger’s theorem for countable graphs, J. Combin. Theory $B$ 43 (1987), 303–313.

Theorem 8.4.2 can be extended to ends, as follows. Given two sets $A , B \subseteq$ $V ( G ) \cup \Omega ( G )$ , let us say that $G$ satisfies the Erd˝os-Menger conjecture for $A$ and $B$ if $G$ contains a set $\mathcal { P }$ of paths (finite or infinite) whose closures in the space $| G |$ defined in Section 8.5 are disjoint arcs each linking a point of $A$ to a point of $B$ , and there is a set $X$ consisting of one vertex or end from each path in $\mathcal { P }$ such that every path in $G$ whose closure links a point of $A$ to one of $B$ has a vertex or end in $X$ . (Note that if $A , B \subseteq V ( G )$ then this statement coincides with Theorem 8.4.2.) Then every graph $G$ satisfies the Erd˝os-Menger conjecture for all sets $A , B \subseteq V ( G ) \cup \Omega ( G )$ satisfying $A \cap { \overline { { B } } } = \emptyset = { \overline { { A } } } \cap B$ , and there are counterexamples when this condition is violated. See H. Bruhn, R. Diestel & M. Stein, Menger’s theorem for infinite graphs with ends, J. Graph Theory (to appear).

There is also a purely topological version of the Erd˝os-Menger conjecture that asks for any set of disjoint $A$ – $B$ arcs in $| G |$ together with a selection $X$ of points, one from each of these arcs, that meets every $A$ – $B$ arc in $| G |$ . An example of K¨uhn shows that this version of the Erd˝os-Menger conjecture can fail if ${ \overline { { A } } } \cap { \overline { { B } } } \neq \emptyset$ . However if we assume that ${ \overline { { A } } } \cap { \overline { { B } } } = \emptyset$ , then the separator $X$ provided by the theorem stated at the end of the last paragraph can be shown to meet every $A$ – $B$ arc in $| G |$ , not only those that are paths or closures of rays or double rays. Thus, the theorem cited above implies the purely topological version of the Erd˝os-Menger conjecture too.

Theorem 8.4.7 is due to J.S. Pym, A proof of the linkage theorem, J. Math. Anal. Appl. 27 (1969), 636–638. The short proof outlined in Exercise 52 can be found in R. Diestel & C. Thomassen, A Cantor-Bernstein theorem for paths in graphs, Amer. Math. Monthly (to appear).

The matching theorems of Chapter 2—K¨onig’s duality theorem, Hall’s marriage theorem, Tutte’s 1-factor theorem, and the Gallai-Edmonds matching theorem—extend essentially unchanged to locally finite graphs by compactness; see e.g. Exercises 14–16. For non-locally-finite graphs, matching theory is considerably deeper. A good survey and open problems can be found in R. Aharoni, Infinite matching theory, in the Directions volume cited earlier. A thorough account is given in M. Holz, K.P. Podewski & K. Steffens, Injective choice functions, Lecture Notes in Mathematics 1238 Springer-Verlag 1987.

Most of the results and techniques for infinite matching were developed first for countable graphs, by Podewski and Steffens in the 1970s. In the 1980s, Aharoni extended them to arbitrary graphs, where things are more difficult still and additional methods are required. Theorem 8.4.8 is due to R. Aharoni, K¨onig’s duality theorem for infinite bipartite graphs, J. London Math. Soc. 29 (1984), 1–12. The proof builds on R. Aharoni, C.St.J.A. Nash-Willaims & S. Shelah, A general criterion for the existence of transversals, Proc. London Math. Soc. 47 (1983), 43–68, and is described in detail in the book of Holz, Podewski and Steffens. Theorem 8.4.10 can be derived from the material in K. Steffens, Matchings in countable graphs, Can. J. Math. 29 (1977), 165–168.

Theorem 8.4.11 is due to R. Aharoni, Matchings in infinite graphs, J. Combin. Theory B 44 (1988), 87–125; a shorter proof was given by Niedermeyer and Podewski, Matchable infinite graphs, J. Combin. Theory B 62 (1994), 213–227. The theorem was extended to $f$ -factors by F. Niedermeyer, $f$ -optimal factors of infinite graphs, also in the Directions volume cited earlier.

The topology on $G$ introduced in Section 8.5 coincides, when $G$ is locally finite, with the usual topology of a 1-dimensional CW-complex. Then $| G |$ can be interpreted as the compactification of $G$ suggested by H. Freudenthal, Uber die Enden topologischer R¨aume und Gruppen,¨ Math. Zeit. 33 (1931), 692–713; see Exercise 64. For graphs that are not locally finite, the graphtheoretical notion of an end is more general than the topological one; see R. Diestel & D. K¨uhn, Graph-theoretical versus topological ends of graphs, J. Combin. Theory B 87 (2003), 197–206. Topological aspects of the subspaces $\Omega$ and $V \cup \Omega$ were studied extensively by Polat; see e.g. N. Polat, Ends and multi-endings I & II, J. Combin. Theory B 67 (1996), 56–110.

The usual notion of an $_ x$ –y path in a topological space $X$ is that of a continuous (but not necessarily injective) map from $[ 0 , 1 ]$ to $X$ that maps 0 to $x$ and 1 to $_ y$ . One can show that the image of an $x$ – $_ y$ path in a Hausdorff space always contains an $_ x$ – $_ y$ arc— in particular, arc-connectedness is the same as the more common topological notion of path-connectedness—so it is largely a matter of convenience which of the two notions to consider. In the context of graphs it seems best to consider arcs: not only because topological paths could be confused with graph-theoretical paths, but also because the latter are ‘injective’ by definition, and are hence best generalized by arcs. A locally finite graph $G$ for which $| G |$ has a connected subset that is not arc-connected has been constructed by A. Georgakopoulos, Connected but not path-connected subspaces of infinite graphs, preprint 2005. A proof that closed connected subsets of $| G |$ are arc-connected (Lemma 8.5.4) is given in R. Diestel & D. K¨uhn, Topological paths, cycles and spanning trees in infinite graphs, Europ. J. Combinatorics 25 (2004), 835–862.

The (combinatorial) vertex-degree of an end is traditionally known as its multiplicity. The term ‘degree’, as well as its topological counterpart based on arcs, was introduced by H. Bruhn and M. Stein, On end degrees and infinite circuits in locally finite graphs (preprint 2004). Their paper includes proofs that the maxima in the definitions of topological end degrees are attained, that the topological degrees of the ends of $G$ taken in the entire space $| G |$ coincide with their combinatorial degrees, and of Lemma 8.5.6. Their main result is that the entire edge set of a locally finite graph lies in its cycle space if and only if every vertex and every end has even degree, with an appropriate division of the ends of infinite degree into ‘even’ and ‘odd’. They conjecture that, like Proposition 1.9.2, this equivalence should extend to arbitrary sets $F \subseteq E ( G )$ , with topological edge-degrees of ends.

An interesting new aspect of end degrees is that they could make it possible to study extremal-type problems for infinite graphs that would otherwise make sense only for finite graphs. For example, while finite graphs of large enough minimum degree contain any desired topological minor or minor (see Chapter 7), an infinite graph of large minimum degree can be a tree. The ends of a tree, however, have degree 1. An assumption that the degrees of both vertices and ends of an infinite graph are large can still not force a non-planar

minor (because such graphs can be planar), but it might force arbitrarily highly connected subgraphs. Another approach to ‘extremal’ infinite graph theory, which seeks to force infinite substructures by assuming a lower bound for $\| G [ v _ { 1 } , \ldots v _ { n } ] \|$ when $V ( G ) = \{ v _ { 1 } , v _ { 2 } , \dots \}$ , is taken by J. Czipszer, P. Erd˝os and A. Hajnal, Some extremal problems on infinite graphs, Publ. Math. Inst. Hung. Acad. Sci., Ser. A 7 (1962), 441–457.

For graphs $G$ that are not locally finite, it can be natural to consider a coarser topology on $| G |$ , obtained by taking as basic open sets $\hat { C } _ { \epsilon } ( S , \omega )$ only those with $\epsilon = 1$ . Under this topology, $| G |$ is no longer Hausdorff, because every vertex dominating an end $\omega$ will lie in the closure of every $\hat { C } ( S , \omega )$ . But $| G |$ can now be compact, and it can have a natural quotient space—in which ends are identified with vertices dominating them and rays converge to vertices—that is both Hausdorff and compact. For details see R. Diestel, On end spaces and spanning trees (preprint 2004), where also Theorem 8.5.2 is proved. A proof of Lemma 8.5.3 can be found in $\ S 4 7$ of K. Kuratowski, Topology $I I$ , Academic Press 1968.

Unlike the cycle space, the cut space ${ \mathcal { C } } ^ { * } ( G )$ of an infinite graph $G$ can be defined as for finite graphs. It then contains infinite as well as finite cuts (which makes it a suitable partner of the cycle space, e.g. for plane duality), but this does not affect the proofs of its basic properties: it is still generated by the cuts of the form $E ( v )$ (Proposition 1.9.3); it consists of precisely those sets of edges that meet every finite circuit in an even number of edges (Ex. 30, Ch. 1); and every cut is a disjoint union of bonds (Proposition 1.9.4).

Our topological notion of the cycle space ${ \mathcal { C } } ( G )$ may appear natural in an infinite setting, but historically it is very young. It was developed in order to extend the classical applications of the cycle space of finite graphs, such as in planarity and duality, to locally finite graphs. As in the case of the tree-packing theorem (Theorem 8.5.10), those extensions fail when only finite circuits and sums are permitted, but they do hold for topological cycle spaces. Examples include Tutte’s theorem (3.2.3) that the non-separating induced cycles generate the whole cycle space; MacLane’s (4.5.1), Kelmans’s (4.5.2) and Whitney’s (4.6.3) characterizations of planarity; and Gallai’s cycle-cocycle partition theorem (Ex. 35, Ch. 1). An expository account of examples and ideas that led to the topological definition of ${ \mathcal { C } } ( G )$ is given in R. Diestel, The cycle space of an infinite graph, Combinatorics, Probability and Computing 14 (2005), 59–79. These show that ${ \mathcal { C } } ( G )$ is not unnecessarily complicated, in that no smaller collection of circuits suffices to generalize even the most basic facts about the cycle space of a finite graph. It also gives a survey of applications of ${ \mathcal { C } } ( G )$ and of open problems, as well as references for all the results of Section 8.5 other than Theorem 8.5.10 (which is new). For graphs that are not locally finite, the problem of how best to define their cycle space is still far from solved.

Theorem 8.5.8 is from R. Diestel & D. K¨uhn, On infinite cycles I–II, Combinatorica 24 (2004), 69–116. Our proof of part (ii) via Lemma 8.5.4 was inspired by A. Vella, A fundamentally topological perspective on graph theory, PhD thesis, Waterloo 2004. Its corollary that locally finite graphs without odd cuts have edge-partitions into finite circuits easily extends to arbitrary countable graphs (Exercise 77), and is true even for uncountable graphs. This is a difficult theorem of C.St.J.A. Nash-Williams, Decomposition of graphs into

closed and endless chains, Proc. London Math. Soc. 10 (1960), 221–238.

Lacking the concept of an infinite circuit as we defined it here, Nash-Williams also sought to generalize the above and other theorems about finite cycles by replacing ‘cycle’ with ‘2-regular connected graph’ (which may be finite or infinite). The resulting statements are not always as smooth as the finite theorems they generalize, but some substantial work has been done in this direction. C.St.J.A. Nash-Williams, Decompositions of graphs into twoway infinite paths, Can. J. Math. 15 (1963), 479–485, characterizes the graphs admitting edge-decompositions into double rays. F. Laviolette, Decompositions of infinite graphs I–II, J. Combin. Theory B 94 (2005), 259–333, characterizes the graphs admitting edge-decompositions into cycles and double rays. Results on the existence of spanning rays or double rays are referenced in the notes for Chapter 10.

Topological spanning trees were introduced by R. Diestel and D. K¨uhn, Topological paths, cycles and spanning trees in infinite graphs, Europ. J. Combinatorics 25 (2004), 835–862. They are essential for the infinite tree packing theorem: if we replace them by ordinary spanning trees, Theorem 8.5.10 becomes false. This was shown by J.G. Oxley, On a packing problem for infinite graphs and independence spaces, J. Combin. Theory B 26 (1979), 123–130, disproving Nash-Williams’s conjecture that the finite theorem should extend verbatim. What Tutte thought about an infinite version of the tree packing theorem is not recorded: in his original paper he treats the infinite case by defining ‘semiconnected’ subgraphs and proving Lemma 8.5.11, and leaves things at that.

The companion to the finite tree-packing theorem, Nash-Williams’s Theorem 2.4.4 that the edges of a graph can be covered by $k$ forests if no set of $\ell$ vertices spans more than $k ( \ell - 1 )$ edges, extends easily by compactness (Exercise 12). However, in the infinite case it seems natural to ask for more: that the forests also have ‘acirclic’ closures. Suprisingly, perhaps, the assumption that no set of $\ell$ vertices spans more than $k ( \ell - 1 )$ edges does not imply that the edges of $G$ (locally finite) can be covered by $k$ such topological forests. However, if we assume in addition that every end of $G$ has degree less than $2 k$ , then such a cover was shown to exist by M. Stein, Arboricity and tree-packing in locally finite graphs, preprint 2004.

Finally, when sets get bigger than countable, combinatorial set theory offers some interesting ways other than cardinality to distinguish ‘small’ from ‘large’ sets. Among these are the use of clubs and stationary sets, of ultrafilters, and of measure and category. See P. Erd˝os, A. Hajnal, A. M´at´e & R. Rado, Combinatorial Set Theory: partition relations for cardinals, North-Holland 1984; W.W. Comfort & S. Negropontis, The Theory of Ultrafilters, Springer 1974; J.C. Oxtoby, Measure and Category: a survey of the analogies between topological and measure spaces (2nd ed.), Springer 1980.

# Ramsey Theory for Graphs

In this chapter we set out from a type of problem which, on the face of it, appears to be similar to the theme of the last chapter: what kind of substructures are necessarily present in every large enough graph?

The regularity lemma of Chapter 7.4 provides one possible answer to this question: every (large) graph $G$ contains large random-like subgraphs. If we are looking for a concrete interesting subgraph $H$ , on the other hand, our problem becomes more like Hadwiger’s conjecture: we cannot expect an arbitrary graph $G$ to contain a copy of $H$ , but if it does not then this might have some interesting structural implications for $G$ .

The kind of structural implication that will be typical for this chapter is simply that of containing some other (induced) subgraph. For example: given an integer $r$ , does every large enough graph contain either a $K ^ { r }$ or an induced $\overline { { K ^ { r } } }$ ? Does every large enough connected graph contain either a $K ^ { r }$ or else a large induced path or star?

Despite its superficial similarity to extremal problems, the above type of question leads to a kind of mathematics with a distinctive flavour of its own. Indeed, the theorems and proofs in this chapter have more in common with similar results in algebra or geometry, say, than with most other areas of graph theory. The study of their underlying methods, therefore, is generally regarded as a combinatorial subject in its own right: the discipline of Ramsey theory.

In line with the subject of this book, we shall focus on results that are naturally expressed in terms of graphs. Even from the viewpoint of general Ramsey theory, however, this is not as much of a limitation as it might seem: graphs are a natural setting for Ramsey problems, and the material in this chapter brings out a sufficient variety of ideas and methods to convey some of the fascination of the theory as a whole.

# 9.1 Ramsey’s original theorems

In its simplest version, Ramsey’s theorem says that, given an integer $r \geqslant 0$ , every large enough graph $G$ contains either $K ^ { r }$ or $\overline { { K ^ { r } } }$ as an induced subgraph. At first glance, this may seem surprising: after all, we need about $( r - 2 ) / ( r - 1 )$ of all possible edges to force a $K ^ { r }$ subgraph in $G$ (Corollary 7.1.3), but neither $G$ nor $\overline { { G } }$ can be expected to have more than half of all possible edges. However, as the Tur´an graphs illustrate well, squeezing many edges into $G$ without creating a $K ^ { r }$ imposes additional structure on $G$ , which may help us find an induced $\overline { { K ^ { r } } }$ .

So how could we go about proving Ramsey’s theorem? Let us try to build a $K ^ { \prime }$ or $\overline { { K ^ { r } } }$ in $G$ inductively, starting with an arbitrary vertex $v _ { 1 } \in V _ { 1 } : = V ( G )$ . If $| G |$ is large, there will be a large set $V _ { 2 } \subseteq V _ { 1 } \setminus \{ v _ { 1 } \}$ of vertices that are either all adjacent to $v _ { 1 }$ or all non-adjacent to $v _ { 1 }$ . Accordingly, we may think of $v _ { 1 }$ as the first vertex of a $K ^ { r }$ or $\overline { { K ^ { r } } }$ whose other vertices all lie in $V _ { 2 }$ . Let us then choose another vertex $v _ { 2 } ~ \in ~ V _ { 2 }$ for our $K ^ { r }$ or $\overline { { K ^ { r } } }$ . Since $V _ { 2 }$ is large, it will have a subset $V _ { 3 }$ , still fairly large, of vertices that are all ‘of the same type’ with respect to $v _ { 2 }$ as well: either all adjacent or all non-adjacent to it. We then continue our search for vertices inside $V _ { 3 }$ , and so on (Fig. 9.1.1).

![](images/52050a0cce1a7db7e015a31725a394d2404f2b3a7a102e39ee91c6742778d322.jpg)  
Fig. 9.1.1. Choosing the sequence $v _ { 1 } , v _ { 2 } , \ldots$

How long can we go on in this way? This depends on the size of our initial set $V _ { 1 }$ : each set $V _ { i }$ has at least half the size of its predecessor $V _ { i - 1 }$ , so we shall be able to complete $s$ construction steps if $G$ has order about $2 ^ { s }$ . As the following proof shows, the choice of $s = 2 r - 3$ vertices $v _ { i }$ suffices to find among them the vertices of a $K ^ { r }$ or $\overline { { K ^ { r } } }$ .

# [ 9.2.2 ] Theorem 9.1.1. (Ramsey 1930)

For every $r \in \mathbb N$ there exists an $n \in \mathbb N$ such that every graph of order at least $n$ contains either $K ^ { r }$ or $\overline { { K ^ { r } } }$ as an induced subgraph.

Proof . The assertion is trivial for $r \leqslant 1$ ; we assume that $r \geqslant 2$ . Let $n : = 2 ^ { 2 r - 3 }$ , and let $G$ be a graph of order at least $n$ . We shall define a sequence $V _ { 1 } , \ldots , V _ { 2 r - 2 }$ of sets and choose vertices $v _ { i } ~ \in ~ V _ { i }$ with the following properties:

(i) $| V _ { i } | = 2 ^ { 2 r - 2 - i } ( i = 1 , \ldots , 2 r - 2 ) ;$

(ii) $V _ { i } \subseteq V _ { i - 1 } \setminus \{ v _ { i - 1 } \} ( i = 2 , \ldots , 2 r - 2 ) ;$   
(iii) $v _ { i - 1 }$ is adjacent either to all vertices in $V _ { i }$ or to no vertex in $V _ { i }$ $( i = 2 , \ldots , 2 r - 2 )$ .

Let $V _ { 1 } \subseteq V ( G )$ be any set of $2 ^ { 2 r - 3 }$ vertices, and pick $v _ { 1 } \in V _ { 1 }$ arbitrarily. Then (i) holds for $i = 1$ , while (ii) and (iii) hold trivially. Suppose now that $V _ { i - 1 }$ and $v _ { i - 1 } \in V _ { i - 1 }$ have been chosen so as to satisfy (i)–(iii) for $i - 1$ , where $1 < i \leqslant 2 r - 2$ . Since

$$
\left| V _ {i - 1} \setminus \left\{v _ {i - 1} \right\} \right| = 2 ^ {2 r - 1 - i} - 1
$$

is odd, $V _ { i - 1 }$ has a subset $V _ { i }$ satisfying (i)–(iii); we pick $v _ { i } \in V _ { i }$ arbitrarily.

Among the $2 r - 3$ vertices $v _ { 1 } , \ldots , v _ { 2 r - 3 }$ , there are $r - 1$ vertices that show the same behaviour when viewed as $v _ { i - 1 }$ in (iii), being adjacent either to all the vertices in $V _ { i }$ or to none. Accordingly, these $r - 1$ vertices and induce either a $K ^ { r }$ or a $\overline { { K ^ { r } } }$ i n $G$ , because $v _ { i } , \ldots , v _ { 2 r - 2 } \in V _ { i }$ $v _ { 2 r - 2 }$ for all $i$ . 

The least integer $n$ associated with $r$ as in Theorem 9.1.1 is the Ramsey number $R ( r )$ of $r$ ; our proof shows that $R ( r ) \leqslant 2 ^ { 2 r - 3 }$ . In Chapter 11 we shall use a simple probabilistic argument to show that $R ( r )$ is bounded below by $2 ^ { r / 2 }$ (Theorem 11.1.3).

It is customary in Ramsey theory to think of partitions as colourings: a colouring of (the elements of) a set $X$ with c colours, or $c$ -colouring for short, is simply a partition of $X$ into $c$ classes (indexed by the ‘colours’). In particular, these colourings need not satisfy any non-adjacency requirements as in Chapter 5. Given a $c$ -colouring of $[ X ] ^ { k }$ , the set of all $k$ -subsets of $X$ , we call a set $Y \subseteq X$ monochromatic if all the elements of $[ Y ] ^ { k }$ have the same colour,1 i.e. belong to the same of the $c$ partition classes of $[ X ] ^ { k }$ . Similarly, if $G = ( V , E )$ is a graph and all the edges of $H \subseteq G$ have the same colour in some colouring of $E$ , we call $H$ a monochromatic subgraph of $G$ , speak of a red (green, etc.) $H$ in $G$ , and so on.

In the above terminology, Ramsey’s theorem can be expressed as follows: for every $r$ there exists an $n$ such that, given any $n$ -set $X$ , every 2-colouring of $[ X ] ^ { 2 }$ yields a monochromatic $r$ -set $Y \subseteq X$ . Interestingly, this assertion remains true for $c$ -colourings of $[ X ] ^ { k }$ with arbitrary $c$ and $k$ —with almost exactly the same proof!

We first prove the infinite version, which is easier, and then deduce the finite version.

Theorem 9.1.2. Let $k , c$ be positive integers, and $X$ an infinite set. If $[ X ] ^ { k }$ is coloured with c colours, then $X$ has an infinite monochromatic subset.

Proof . We prove the theorem by induction on $k$ , with $c$ fixed. For $k = 1$ the assertion holds, so let $k > 1$ and assume the assertion for smaller values of $k$ .

Let $[ X ] ^ { k }$ be coloured with $c$ colours. We shall construct an infinite sequence $X _ { 0 } , X _ { 1 } , \ldots$ of infinite subsets of $X$ and choose elements $x _ { i } \in X _ { i }$ with the following properties (for all $i$ ):

(i) $X _ { i + 1 } \subseteq X _ { i } \setminus \{ x _ { i } \} ;$   
(ii) all $k$ -sets $\{ x _ { i } \} \cup Z$ with $Z ~ \in ~ [ X _ { i + 1 } ] ^ { k - 1 }$ have the same colour, which we associate with $x _ { i }$ .

We start with $X _ { 0 } : = X$ and pick $x _ { 0 } \in X _ { 0 }$ arbitrarily. By assumption, $X _ { 0 }$ is infinite. Having chosen an infinite set $X _ { i }$ and $x _ { i } \in X _ { i }$ for some $i$ , we $c$ -colour $[ X _ { i } \setminus \{ x _ { i } \} ] ^ { k - 1 }$ by giving each set $Z$ the colour of $\{ x _ { i } \} \cup Z$ from our $c$ -colouring of $[ X ] ^ { k }$ . By the induction hypothesis, $X _ { i } \setminus \{ x _ { i } \}$ has an infinite monochromatic subset, which we choose as $X _ { i + 1 }$ . Clearly, this choice satisfies (i) and (ii). Finally, we pick $x _ { i + 1 } \in X _ { i + 1 }$ arbitrarily.

Since $c$ is finite, one of the $c$ colours is associated with infinitely many $x _ { i }$ . These $x _ { i }$ form an infinite monochromatic subset of $X$ . 

If desired, the finite version of Theorem 9.1.2 could be proved just like the infinite version above. However to ensure that the relevant sets are large enough at all stages of the induction, we have to keep track of their sizes, which involves a good deal of boring calculation. As long as we are not interested in bounds, the more elegant route is to deduce the finite version from the infinite ‘by compactness’, that is, using K¨onig’s infinity lemma (8.1.2).

[ 9.3.3 ]

(8.1.2)

$k , c , r$

bad colouring

Theorem 9.1.3. For all $k , c , r \geqslant 1$ there exists an $n \geqslant k$ such that every $n$ -set $X$ has a monochromatic $r$ -subset with respect to any $c$ -colouring of $[ X ] ^ { k }$ .

Proof . As is customary in set theory, we denote by $ { n } \in \mathbb { N }$ (also) the set $\{ 0 , \ldots , n - 1 \}$ . Suppose the assertion fails for some $k , c , r$ . Then for every $n \geqslant k$ there exist an $n$ -set, without loss of generality the set $n$ , and a $c$ -colouring $[ n ] ^ { k } \to c$ such that $n$ contains no monochromatic $r$ -set. Let us call such colourings $b a d$ ; we are thus assuming that for every $n \geqslant k$ there exists a bad colouring of $[ n ] ^ { k }$ . Our aim is to combine these into a bad colouring of $[ \mathbb { N } ] ^ { k }$ , which will contradict Theorem 9.1.2.

For every $n \geqslant k$ let $V _ { n } \neq \emptyset$ be the set of bad colourings of $[ n ] ^ { k }$ . For $n > k$ , the restriction $f ( g )$ of any $g \in V _ { n }$ to $[ n - 1 ] ^ { k }$ is still bad, and hence lies in $V _ { n - 1 }$ . By the infinity lemma (8.1.2), there is an infinite sequence $g _ { k } , g _ { k + 1 } , \ldots$ of bad colourings $g _ { n } \in V _ { n }$ such that $f ( g _ { n } ) = g _ { n - 1 }$ for all $n > k$ . For every $m \geqslant k$ , all colourings $g _ { n }$ with $n \geqslant m$ agree on $[ m ] ^ { k }$ , so for each $Y \in [ \mathbb { N } ] ^ { k }$ the value of $g _ { n } ( Y )$ coincides for all $n \_$ max $Y$ . Let us define $g ( Y )$ as this common value $g _ { n } ( Y )$ . Then $g$ is a bad colouring

of $[ \mathbb { N } ] ^ { k }$ : every $r$ -set $S \subseteq \mathbb { N }$ is contained in some sufficiently large $n$ , so $S$ cannot be monochromatic since $g$ coincides on $[ n ] ^ { k }$ with the bad colouring $g _ { n }$ . 

The least integer $n$ associated with $k , c , r$ as in Theorem 9.1.3 is the Ramsey number for these parameters; we denote it by $R ( k , c , r )$ .

# 9.2 Ramsey numbers

Ramsey’s theorem may be rephrased as follows: if $H \ = \ K ^ { \prime }$ and $G$ is a graph with sufficiently many vertices, then either $G$ itself or its complement $\overline { G }$ contains a copy of $H$ as a subgraph. Clearly, the same is true for any graph $H$ , simply because $H \subseteq K ^ { h }$ for $h : = | H |$ .

However, if we ask for the least $n$ such that every graph $G$ of order $n$ has the above property—this is the Ramsey number $R ( H )$ of $H$ —then the above question makes sense: if $H$ has only few edges, it should embed more easily in $G$ or $\overline { { G } }$ , and we would expect $R ( H )$ to be smaller than the Ramsey number $R ( h ) = R ( K ^ { h } )$ .

A little more generally, let $R ( H _ { 1 } , H _ { 2 } )$ denote the least $n \in \mathbb N$ such that $H _ { 1 } \subseteq G$ or $H _ { 2 } \subseteq \overline { { G } }$ for every graph $G$ of order $n$ . For most graphs $H _ { 1 } , H _ { 2 }$ , only very rough estimates are known for $R ( H _ { 1 } , H _ { 2 } )$ . Interestingly, lower bounds given by random graphs (as in Theorem 11.1.3) are often sharper than even the best bounds provided by explicit constructions.

The following proposition describes one of the few cases where exact Ramsey numbers are known for a relatively large class of graphs:

Proposition 9.2.1. Let $s , t$ be positive integers, and let $T$ be a tree of order $t$ . Then $R ( T , K ^ { s } ) = ( s - 1 ) ( t - 1 ) + 1$ .

Proof . The disjoint union of $s - 1$ graphs $K ^ { t - 1 }$ contains no copy of $T$ , while the complement of this graph, the complete $( s - 1 )$ -partite graph $K _ { t - 1 } ^ { s - 1 }$ , does not contain $K ^ { s }$ . This proves $R ( T , K ^ { s } ) \geqslant ( s - 1 ) ( t - 1 ) + 1$ .

Conversely, let $G$ be any graph of order $n = ( s - 1 ) ( t - 1 ) + 1$ whose complement contains no $K ^ { s }$ . Then $s > 1$ , and in any vertex colouring of $G$ (in the sense of Chapter 5) at most $s - 1$ vertices can have the same colour. Hence, $\chi ( G ) \geqslant \lceil n / ( s - 1 ) \rceil = t$ . By Corollary 5.2.3, $G$ has a subgraph $H$ with $\delta ( H ) \geqslant t - 1$ , which by Corollary 1.5.4 contains a copy of $T$ . 

As the main result of this section, we shall now prove one of those rare general theorems providing a relatively good upper bound for the Ramsey numbers of a large class of graphs, a class defined in terms of a standard graph invariant. The theorem deals with the Ramsey numbers of sparse graphs: it says that the Ramsey number of graphs $H$

Ramsey number $R ( k , c , r )$

Ramsey number $R ( H )$

$R ( H _ { 1 } , H _ { 2 } )$

(5.2.3) (1.5.4)

with bounded maximum degree grows only linearly in $| H |$ —an enormous improvement on the exponential bound from the proof of Theorem 9.1.1.

Theorem 9.2.2. (Chv´atal, R¨odl, Szemer´edi & Trotter 1983)

For every positive integer $\Delta$ there is a constant c such that

$$
R (H) \leqslant c | H |
$$

for all graphs $H$ with $\Delta ( H ) \leqslant \Delta$ .

Proof . The basic idea of the proof is as follows. We wish to show that $H \subseteq G$ or $H \subseteq { \overline { { G } } }$ if $| G |$ is large enough (though not too large). Consider an $\epsilon$ -regular partition of $G$ , as provided by the regularity lemma. If enough of the $\epsilon$ -regular pairs in this partition have high density, we may hope to find a copy of $H$ in $G$ . If most pairs have low density, we try to find $H$ in $\overline { { G } }$ . Let $R$ , $R ^ { \prime }$ and $R ^ { \prime \prime }$ be the regularity graphs of $G$ whose edges correspond to the pairs of density $\geqslant 0$ ; $\geqslant 1 / 2$ ; $< 1 / 2$ respectively.2 Then $R$ is the edge-disjoint union of $R ^ { \prime }$ and $R ^ { \prime \prime }$ .

Now to obtain $H \subseteq G$ or $H \subseteq { \overline { { G } } }$ , it suffices by Lemma 7.5.2 to ensure that $H$ is contained in a suitable ‘inflated regularity graph’ $R _ { s } ^ { \prime }$ or $R _ { s } ^ { \prime \prime }$ . Since $\chi ( H ) \leqslant \Delta ( H ) + 1 \leqslant \Delta + 1$ , this will be the case if $s \geqslant \alpha ( H )$ and we can find a $K ^ { \Delta + 1 }$ in $R ^ { \prime }$ or in $R ^ { \prime \prime }$ . But that is easy to ensure: we just need that $K ^ { r } \subseteq R$ , where $r$ is the Ramsey number of $\Delta + 1$ , which will follow from Tur´an’s theorem because $R$ is dense.

For the formal proof let now $\Delta \geqslant 1$ be given. On input $d : = 1 / 2$ 2 and $\Delta$ , Lemma 7.5.2 returns an $\epsilon _ { 0 }$ . Let $m : = R ( \Delta + 1 )$ be the Ramsey number of $\Delta + 1$ . Let $\epsilon \leqslant \epsilon _ { 0 }$ be positive but small enough that for $k = m$ (and hence for all $k \geqslant m$ )

$$
2 \epsilon <   \frac {1}{m - 1} - \frac {1}{k}; \tag {1}
$$

then in particular $\epsilon < 1$ . Finally, let $M$ be the integer returned by the regularity lemma (7.4.1) on input $\epsilon$ and $m$ .

All the quantities defined so far depend only on $\Delta$ . We shall prove the theorem with

$$
c := \frac {2 ^ {\Delta + 1} M}{1 - \epsilon}.
$$

Let $H$ with $\Delta ( H ) \leqslant \Delta$ be given, and let $s : = | H |$ . Let $G$ be an arbitrary graph of order $n \geqslant c | H |$ ; we show that $H \subseteq G$ or $H \subseteq G$ .

By Lemma 7.4.1, $G$ has an $\epsilon$ -regular partition $\{ V _ { 0 } , V _ { 1 } , \ldots , V _ { k } \}$ with exceptional set $V _ { 0 }$ and $| V _ { 1 } | = \ldots = | V _ { k } | = : \ell$ , where $m \leqslant k \leqslant M$ . Then

$$
\ell = \frac {n - | V _ {0} |}{k} \geqslant n \frac {1 - \epsilon}{M} \geqslant c s \frac {1 - \epsilon}{M} \geqslant 2 ^ {\Delta + 1} s = 2 s / d ^ {\Delta}. \tag {2}
$$

Let $R$ be the regularity graph with parameters $\epsilon , \ell , 0$ corresponding to this partition. By definition, $R$ has $k$ vertices and

$$
\begin{array}{l} \| R \| \geqslant \binom {k} {2} - \epsilon k ^ {2} \\ = \frac {1}{2} k ^ {2} \left(1 - \frac {1}{k} - 2 \epsilon\right) \\ \geqslant \frac {1}{2} k ^ {2} \left(1 - \frac {1}{k} - \frac {1}{m - 1} + \frac {1}{k}\right) \\ = \frac {1}{2} k ^ {2} \frac {m - 2}{m - 1} \\ \geqslant t _ {m - 1} (k) \\ \end{array}
$$

edges. By Theorem 7.1.1, therefore, $R$ has a subgraph $K = K ^ { m }$ .

We now colour the edges of $R$ with two colours: red if the edge corresponds to a pair $( V _ { i } , V _ { j } )$ of density at least $1 / 2$ , and green otherwise. Let $R ^ { \prime }$ be the spanning subgraph of $R$ formed by the red edges, and $R ^ { \prime \prime }$ the spanning subgraph of $R$ formed by the green edges and those whose corresponding pair has density exactly $1 / 2$ . Then $R ^ { \prime }$ is a regularity graph of $G$ with parameters $\epsilon$ , $\ell$ and $1 / 2$ . And $R ^ { \prime \prime }$ is a regularity graph of $\overline { { G } }$ , with the same parameters: as one easily checks, every pair $( V _ { i } , V _ { j } )$ that is $\epsilon$ -regular for $G$ is also $\epsilon$ -regular for $\overline { G }$ .

By definition of $m$ , our graph $K$ contains a red or a green $K ^ { r }$ , for $r : = \chi ( H ) \leqslant \Delta + 1$ . Correspondingly, $H \subseteq R _ { s } ^ { \prime }$ or $H \subseteq R _ { s } ^ { \prime \prime }$ . Since $\epsilon \leqslant \epsilon _ { 0 }$ and $\ell \geqslant 2 s / d ^ { \Delta }$ by (2), both $R ^ { \prime }$ and $R ^ { \prime \prime }$ satisfy the requirements of Lemma 7.5.2, so $H \subseteq G$ or $H \subseteq { \overline { { G } } }$ as desired. 

So far in this section, we have been asking what is the least order of a graph $G$ such that every 2-colouring of its edges yields a monochromatic copy of some given graph $H$ . Rather than focusing on the order of $G$ , we might alternatively try to minimize $G$ itself, with respect to the subgraph relation. Given a graph $H$ , let us call a graph $G$ Ramsey-minimal for $H$ if $G$ is minimal with the property that every 2-colouring of its edges yields a monochromatic copy of $H$ .

What do such Ramsey-minimal graphs look like? Are they unique? The following result, which we include for its pretty proof, answers the second question for some $H$ :

Proposition 9.2.3. If $T$ is a tree but not a star, then infinitely many graphs are Ramsey-minimal for $T$ .

Proof . Let $| T | = : r$ . We show that for every $n \in \mathbb N$ there is a graph of order at least $n$ that is Ramsey-minimal for $T$ .

By Theorem 5.2.5, there exists a graph $G$ with chromatic number $\chi ( G ) > r ^ { 2 }$ and girth $g ( G ) > n$ . If we colour the edges of $G$ red and green, then the red and the green subgraph cannot both have an $r$ - (vertex-)colouring in the sense of Chapter 5: otherwise we could colour the vertices of $G$ with the pairs of colours from those colourings and obtain a contradiction to $\chi ( G ) > r ^ { 2 }$ . So let $G ^ { \prime } \subseteq G$ be monochromatic with $\chi ( G ^ { \prime } ) > r$ . By Corollary 5.2.3, $G ^ { \prime }$ has a subgraph of minimum degree at least $r$ , which contains a copy of $T$ by Corollary 1.5.4.

Let $G ^ { * } \subseteq G$ be Ramsey-minimal for $T$ . Clearly, $G ^ { * }$ is not a forest: the edges of any forest can be 2-coloured (partitioned) so that no monochromatic subforest contains a path of length 3, let alone a copy of $T$ . (Here we use that $T$ is not a star, and hence contains a $P ^ { 3 }$ .) So $G ^ { * }$ contains a cycle, which has length $g ( G ) > n$ since $G ^ { \ast } \subseteq G$ . In particular, $| G ^ { * } | > n$ as desired. 

# 9.3 Induced Ramsey theorems

Ramsey’s theorem can be rephrased as follows. For every graph $H = K ^ { r }$ there exists a graph $G$ such that every 2-colouring of the edges of $G$ yields a monochromatic $H \subseteq G$ ; as it turns out, this is witnessed by any large enough complete graph as $G$ . Let us now change the problem slightly and ask for a graph $G$ in which every 2-edge-colouring yields a monochromatic induced $H \subseteq G$ , where $H$ is now an arbitrary given graph.

This slight modification changes the character of the problem dramatically. What is needed now is no longer a simple proof that $G$ is ‘big enough’ (as for Theorem 9.1.1), but a careful construction: the construction of a graph that, however we bipartition its edges, contains an induced copy of $H$ with all edges in one partition class. We shall call such a graph a Ramsey graph for $H$ .

The fact that such a Ramsey graph exists for every choice of $H$ is one of the fundamental results of graph Ramsey theory. It was proved around 1973, independently by Deuber, by Erd˝os, Hajnal & P´osa, and by R¨odl.

Theorem 9.3.1. Every graph has a Ramsey graph. In other words, for every graph $H$ there exists a graph $G$ that, for every partition $\{ E _ { 1 } , E _ { 2 } \}$ of $E ( G )$ , has an induced subgraph $H$ with $E ( H ) \subseteq E _ { 1 }$ or $E ( H ) \subseteq E _ { 2 }$ .

We give two proofs. Each of these is highly individual, yet each offers a glimpse of true Ramsey theory: the graphs involved are used as hardly more than bricks in the construction, but the edifice is impressive.

First proof. In our construction of the desired Ramsey graph we shall repeatedly replace vertices of a graph $G = ( V , E )$ already constructed by copies of another graph $H$ . For a vertex set $U \subseteq V$ let $G \left[ U \to H \right]$ denote the graph obtained from $G$ by replacing the vertices $u \in U$ with copies $H ( u )$ of $H$ and joining each $H ( u )$ completely to all $H ( u ^ { \prime } )$ with $u u ^ { \prime } \in E$ and to all vertices $v \in V \setminus U$ with $u v \in E$ (Fig. 9.3.1). Formally,

$$
G [ U \to H ]
$$

$H ( u )$

![](images/4f6014b47a8e4ac16a20de0f0d0961ac3da69b352878a091720df0f0ce5fe7e5.jpg)  
Fig. 9.3.1. A graph $G \left[ U \to H \right]$ with $H = K ^ { 3 }$ ：

$G \left[ U \to H \right]$ is the graph on

$$
(U \times V (H)) \cup ((V \smallsetminus U) \times \{\emptyset \})
$$

in which two vertices $( v , w )$ and $( v ^ { \prime } , w ^ { \prime } )$ are adjacent if and only if either $v v ^ { \prime } \in E$ , or else $v = v ^ { \prime } \in U$ and $w w ^ { \prime } \in E ( H )$ .3

We prove the following formal strengthening of Theorem 9.3.1:

For any two graphs $H _ { 1 } , H _ { 2 }$ there exists a graph $G \ =$

$G ( H _ { 1 } , H _ { 2 } )$ such that every edge colouring of $G$ with the colours 1 and 2 yields either an induced $H _ { 1 } \subseteq G$ with all (∗)

its edges coloured 1 or an induced $H _ { 2 } \subseteq G$ with all its edges coloured 2.

$G ( H _ { 1 } , H _ { 2 } )$

This formal strengthening makes it possible to apply induction on $| H _ { 1 } | + | H _ { 2 } |$ , as follows.

If either $H _ { 1 }$ or $H _ { 2 }$ has no edges (in particular, if $\begin{array} { r } { | H _ { 1 } | + | H _ { 2 } | \leqslant 1 } \end{array}$ ), then (∗) holds with $G = { \overline { { K ^ { n } } } }$ for large enough $n$ . For the induction step, we now assume that both $H _ { 1 }$ and $H _ { 2 }$ have at least one edge, and that $( * )$ holds for all pairs $( H _ { 1 } ^ { \prime } , H _ { 2 } ^ { \prime } )$ with smaller $| H _ { 1 } ^ { \prime } | + | H _ { 2 } ^ { \prime } |$ .

$x _ { i }$ $H _ { i } ^ { \prime } , H _ { i } ^ { \prime \prime }$

For each $i = 1 , 2$ , pick a vertex $x _ { i } \ \in \ H _ { i }$ that is incident with an edge. Let $H _ { i } ^ { \prime } : = H _ { i } - x _ { i }$ , and let $H _ { i } ^ { \prime \prime }$ be the subgraph of $H _ { i } ^ { \prime }$ induced by the neighbours of $x _ { i }$ .

We shall construct a sequence $G ^ { 0 } , \ldots , G ^ { n }$ of disjoint graphs; $G ^ { n }$ will be the desired Ramsey graph $G ( H _ { 1 } , H _ { 2 } )$ . Along with the graphs $G _ { i }$ , we shall define subsets $V ^ { i } \subseteq V ( G ^ { i } )$ and a map

$$
f \colon V ^ {1} \cup \dots \cup V ^ {n} \to V ^ {0} \cup \dots \cup V ^ {n - 1}
$$

such that

$$
f (V ^ {i}) = V ^ {i - 1} \tag {1}
$$

$f ^ { i }$

origin

for all $i \geqslant 1$ . Writing $f ^ { i } : = f \circ . . . \circ f$ for the $i$ -fold composition of $f$ , and $f ^ { 0 }$ for the identity map on $V ^ { 0 } = V ( G ^ { 0 } )$ , we thus have $f ^ { i } ( v ) \in V ^ { 0 }$ for all $v \in V ^ { i }$ . We call $f ^ { i } ( v )$ the origin of $v$ .

The subgraphs $G ^ { i } \left[ V ^ { i } \right]$ will reflect the structure of $G ^ { 0 }$ as follows:

Vertices in and only if $V ^ { i }$ with different origins areir origins are adjacent in $G ^ { 0 }$ djacent in . $G ^ { i }$ i f (2)

Assertion (2) will not be used formally in the proof below. However, it can help us to visualize the graphs $G ^ { i }$ : every $G ^ { \ i }$ (more precisely, every $G ^ { i } \left[ V ^ { i } \right]$ —there will also be some vertices $x \in G ^ { i } - V ^ { i } )$ ) is essentially an inflated copy of $G ^ { 0 }$ in which every vertex $w \in G ^ { 0 }$ has been replaced by the set of all vertices in $V ^ { i }$ with origin $w$ , and the map $f$ links vertices with the same origin across the various $G ^ { i }$ .

By the induction hypothesis, there are Ramsey graphs

$G _ { 1 } , G _ { 2 }$

$$
G _ {1} := G \left(H _ {1}, H _ {2} ^ {\prime}\right) \quad \text {a n d} \quad G _ {2} := G \left(H _ {1} ^ {\prime}, H _ {2}\right).
$$

$G ^ { 0 } , V ^ { 0 }$

$W _ { i } ^ { \prime }$

n

$W _ { i } ^ { \prime \prime }$

$U ^ { i - 1 }$

$G _ { 2 } ( u )$

$\tilde { G } ^ { i - 1 }$

Let $G ^ { 0 }$ be a copy of $G _ { 1 }$ , and set $V ^ { 0 } : = V ( G ^ { 0 } )$ . Let $W _ { 0 } ^ { \prime } , \ldots , W _ { n - 1 } ^ { \prime }$ be the subsets of $V ^ { 0 }$ spanning an $H _ { 2 } ^ { \prime }$ in $G ^ { 0 }$ . Thus, $n$ is defined as the number of induced copies of $H _ { 2 } ^ { \prime }$ in $G ^ { 0 }$ , and we shall construct a graph $G ^ { \ i }$ for every set $W _ { i - 1 } ^ { \prime }$ , $i = 1 , \ldots , n$ . For $i = 0 , \ldots , n - 1$ , let $W _ { i } ^ { \prime \prime }$ be the image of $V ( H _ { 2 } ^ { \prime \prime } )$ under some isomorphism $H _ { 2 } ^ { \prime } \longrightarrow G ^ { 0 } \left[ W _ { i } ^ { \prime } \right]$ .

Assume now that $G ^ { 0 } , \ldots , G ^ { i - 1 }$ and $V ^ { 0 } , \ldots , V ^ { i - 1 }$ have been defined for some $i \geqslant 1$ , and that $f$ has been defined on $V ^ { 1 } \cup . . . \cup V ^ { i - 1 }$ and satisfies (1) for all $j \leqslant i$ . We construct $G ^ { i }$ from $G ^ { i - 1 }$ in two steps. For the first step, consider the set $U ^ { i - 1 }$ of all the vertices $v \in V ^ { i - 1 }$ whose origin $f ^ { i - 1 } ( v )$ lies in $W _ { i - 1 } ^ { \prime \prime }$ . (For $i = 1$ , this gives $U ^ { 0 } = W _ { 0 } ^ { \prime \prime }$ .) Expand $G ^ { i - 1 }$ to a new graph $\tilde { G } ^ { i - 1 }$ (disjoint from $G ^ { i - 1 }$ ) by replacing every vertex $u \in U ^ { i - 1 }$ with a copy $G _ { 2 } ( u )$ of $G _ { 2 }$ , i.e. let

$$
\tilde {G} ^ {i - 1} := G ^ {i - 1} \left[ U ^ {i - 1} \rightarrow G _ {2} \right]
$$

![](images/758b7f8e27bd2e41bcb89e6a95dc05c3c0a6cbcd16cb4c22954a193cc49190be.jpg)  
Fig. 9.3.2. The construction of $G ^ { 1 }$

(see Figures 9.3.2 and 9.3.3). Set $f ( u ^ { \prime } ) : = u$ for all $u \in U ^ { i - 1 }$ and $u ^ { \prime } \in G _ { 2 } ( u )$ , and $f ( v ^ { \prime } ) : = v$ for all $\boldsymbol { v ^ { \prime } } = ( v , \emptyset )$ with $v \in V ^ { i - 1 } \setminus U ^ { i - 1 }$ . (Recall that $( v , \emptyset )$ is simply the unexpanded copy of a vertex $v \in G ^ { i - 1 }$ in $\hat { G } ^ { i - 1 }$ .) Let $V ^ { i }$ be the set of those vertices $v ^ { \prime }$ or $u ^ { \prime }$ of $\hat { G } ^ { i - 1 }$ for which $f$ has thus been defined, i.e. the vertices that either correspond directly to a vertex $v$ in $V ^ { i - 1 }$ or else belong to an expansion $G _ { 2 } ( u )$ of such a vertex $u$ . Then (1) holds for $i$ . Also, if we assume (2) inductively for $i - 1$ , then (2) holds again for $_ i$ (in $\tilde { G } ^ { i - 1 }$ ). The graph $\bar { G } ^ { i - 1 }$ is already the essential part of $G ^ { i }$ : the part that looks like an inflated copy of $G ^ { 0 }$ .

In the second step we now extend $\tilde { G } ^ { i - 1 }$ to the desired graph $G ^ { i }$ by adding some further vertices $x \notin V ^ { i }$ . Let $\mathcal { F }$ denote the set of all families $F ^ { \prime }$ of the form

$$
F = \left(H _ {1} ^ {\prime} (u) \mid u \in U ^ {i - 1}\right),
$$

where each $H _ { 1 } ^ { \prime } ( u )$ is an induced subgraph of $G _ { 2 } ( u )$ isomorphic to $H _ { 1 } ^ { \prime }$ . (Less formally: $\mathcal { F }$ is the collection of ways to select simultaneously from each $G _ { 2 } ( u )$ exactly one induced copy of $H _ { 1 } ^ { \prime }$ .) For each $F \in \mathcal { F }$ , add a vertex $x ( F )$ to $\hat { G } ^ { i - 1 }$ and join it, for every $u \in U ^ { i - 1 }$ , to all the vertices in the image $H _ { 1 } ^ { \prime \prime } ( u ) \subseteq H _ { 1 } ^ { \prime } ( u )$ of $H _ { 1 } ^ { \prime \prime }$ under some isomorphism from $H _ { 1 } ^ { \prime }$ to the $H _ { 1 } ^ { \prime } ( u ) \subseteq G _ { 2 } ( u )$ selected by $F ^ { \prime }$ (Fig. 9.3.2). Denote the resulting graph by $G ^ { i }$ . This completes the inductive definition of the graphs $G ^ { 0 } , \ldots , G ^ { n }$ .

Let us now show that $G : = G ^ { n }$ satisfies ( ). To this end, we prove the following assertion $( * * )$ about $G ^ { i }$ for $i = 0 , \ldots , n$ :

For every edge colouring with the colours 1 and 2, $G ^ { i }$ contains either an induced $H _ { 1 }$ coloured 1, or an induced $H _ { 2 }$ coloured 2, or an induced subgraph $H$ coloured 2 such that $( * * )$ $V ( H ) \subseteq V ^ { i }$ and the restriction of $f ^ { i }$ to $V ( H )$ is an isomorphism between $H$ and $G ^ { 0 } \left[ W _ { k } ^ { \prime } \right]$ for some $k \in \{ i , \dotsc , n - 1 \}$ .

Note that the third of the above cases cannot arise for $i = n$ , so $( * * )$ for $n$ is equivalent to ( ) with $G : = G ^ { n }$ .

For $i ~ = ~ 0$ , $( * * )$ follows from the choice of $G ^ { 0 }$ as a copy of $G _ { 1 } =$ $G ( H _ { 1 } , H _ { 2 } ^ { \prime } )$ and the definition of the sets $W _ { k } ^ { \prime }$ . Now let $1 \leqslant i \leqslant n$ , and assume $( * * )$ for smaller values of $i$ .

Let an edge colouring of $G ^ { i }$ be given. For each $u \in U ^ { i - 1 }$ there is a copy of $G _ { 2 }$ in $G ^ { i }$ :

$$
G ^ {i} \supseteq G _ {2} (u) \simeq G (H _ {1} ^ {\prime}, H _ {2}).
$$

If $G _ { 2 } ( u )$ contains an induced $H _ { 2 }$ coloured 2 for some $u \in U ^ { i - 1 }$ , we are done. If not, then every $G _ { 2 } ( u )$ has an induced subgraph $H _ { 1 } ^ { \prime } ( u ) \simeq H _ { 1 } ^ { \prime }$ coloured 1. Let $F$ be the family of these graphs $H _ { 1 } ^ { \prime } ( u )$ , one for each $u \in U ^ { i - 1 }$ , and let $x : = x ( F )$ . If, for some $u \in U ^ { i - 1 }$ , all the $x { - } H _ { 1 } ^ { \prime \prime } ( u )$ edges in $G ^ { i }$ are also coloured 1, we have an induced copy of $H _ { 1 }$ in $G ^ { \ i }$ and are again done. We may therefore assume that each $H _ { 1 } ^ { \prime \prime } ( u )$ has a vertex $y _ { u }$ for which the edge $x y _ { u }$ is coloured 2. The restriction $y _ { u } \mapsto u$ of $f$ to

$_ x$

$_ { y u }$

$\hat { U } ^ { i - 1 }$

$$
\hat {U} ^ {i - 1} := \left\{y _ {u} \mid u \in U ^ {i - 1} \right\} \subseteq V ^ {i}
$$

extends by $( v , \emptyset ) \mapsto v$ to an isomorphism from

$\hat { G } ^ { i - 1 }$

$$
\hat {G} ^ {i - 1} := G ^ {i} \left[ \hat {U} ^ {i - 1} \cup \left\{\left(v, \emptyset\right) \mid v \in V (G ^ {i - 1}) \setminus U ^ {i - 1} \right\} \right]
$$

to $G ^ { i - 1 }$ , and so our edge colouring of $G ^ { i }$ induces an edge colouring of $G ^ { i - 1 }$ . If this colouring yields an induced $H _ { 1 } \subseteq G ^ { i - 1 }$ coloured 1 or an induced $H _ { 2 } \subseteq G ^ { i - 1 }$ coloured 2, we have these also in ${ \hat { G } } ^ { i - 1 } \subseteq G ^ { i }$ and are again home.

$H ^ { \prime }$

${ \hat { H } } ^ { \prime }$

By $( * * )$ for $i - 1$ we may therefore assume that $G ^ { i - 1 }$ has an induced subgraph $H ^ { \prime }$ coloured 2, with $V ( H ^ { \prime } ) \subseteq V ^ { i - 1 }$ , and such that the restriction of $f ^ { i - 1 }$ to $V ( H ^ { \prime } )$ is an isomorphism from $H ^ { \prime }$ t o $G ^ { 0 } [ W _ { k } ^ { \prime } ] \simeq H _ { 2 } ^ { \prime }$ for some $k \in \{ i - 1 , \ldots , n - 1 \}$ . Let ${ \hat { H } } ^ { \prime }$ be the corresponding induced subgraph of ${ \hat { G } } ^ { i - 1 } \subseteq G ^ { i }$ (also coloured 2); then $V ( { \hat { H } } ^ { \prime } ) \subseteq V ^ { i }$ ,

$$
f ^ {i} (V (\hat {H} ^ {\prime})) = f ^ {i - 1} (V (H ^ {\prime})) = W _ {k} ^ {\prime},
$$

and $f ^ { i } { \colon } \hat { H } ^ { \prime } { \longrightarrow } G ^ { 0 } \left[ W _ { k } ^ { \prime } \right]$ is an isomorphism.

If $k \geqslant i$ , this completes the proof of $( * * )$ with $H : = \hat { H } ^ { \prime }$ ; we therefore assume that $k \ < \ i$ , and hence $k = i - 1$ (Fig. 9.3.3). By definition of $U ^ { i - 1 }$ and $\hat { G } ^ { i - 1 }$ , the inverse image of $W _ { i - 1 } ^ { \prime \prime }$ under the isomorphism $f ^ { i } { : \hat { H } ^ { \prime } } \longrightarrow G ^ { 0 } \ [ \ W _ { i { - 1 } } ^ { \prime } ]$ is a subset of ${ \hat { U } } ^ { i - 1 }$ . Since $x$ is joined to precisely those vertices of $\dot { H } ^ { \prime }$ that lie in $\hat { U } ^ { i - 1 }$ , and all these edges $x y _ { u }$ have colour 2, the graph ${ \hat { H } } ^ { \prime }$ and $x$ together induce in $G ^ { i }$ a copy of $H _ { 2 }$ coloured 2, and the proof of $( * * )$ is complete. 

![](images/866e01a34374de0a24d984cd6449d9988d51b684f82be3eb29de398e9d885a38.jpg)  
Fig. 9.3.3. A monochromatic copy of $H _ { 2 }$ in $G ^ { i }$

Let us return once more to the reformulation of Ramsey’s theorem considered at the beginning of this section: for every graph $H$ there exists a graph $G$ such that every 2-colouring of the edges of $G$ yields a monochromatic $H \subseteq G$ . The graph $G$ for which this follows at once from Ramsey’s theorem is a sufficiently large complete graph. If we ask, however, that $G$ shall not contain any complete subgraphs larger than those in $H$ , i.e. that $\omega ( G ) = \omega ( H )$ , the problem again becomes difficult—even if we do not require $H$ to be induced in $G$ .

Our second proof of Theorem 9.3.1 solves both problems at once: given $H$ , we shall construct a Ramsey graph for $H$ with the same clique number as $H$ .

For this proof, i.e. for the remainder of this section, let us view bipartite graphs $P$ as triples $( V _ { 1 } , V _ { 2 } , E )$ , where $V _ { 1 }$ and $V _ { 2 }$ are the two vertex classes and $E \subseteq V _ { 1 } \times V _ { 2 }$ is the set of edges. The reason for this more explicit notation is that we want embeddings between bipartite graphs to respect their bipartitions: given another bipartite graph $P ^ { \prime } =$ $( V _ { 1 } ^ { \prime } , V _ { 2 } ^ { \prime } , E ^ { \prime } )$ , an injective map $\varphi \colon V _ { 1 } \cup V _ { 2 } \to V _ { 1 } ^ { \prime } \cup V _ { 2 } ^ { \prime }$ will be called an embedding of $P$ in $P ^ { \prime }$ if $\varphi ( V _ { i } ) \subseteq V _ { i } ^ { \prime }$ for $i = 1 , 2$ and $\varphi ( v _ { 1 } ) \varphi ( v _ { 2 } )$ is an edge of $P ^ { \prime }$ if and only if $v _ { 1 } v _ { 2 }$ is an edge of $P$ . (Note that such embeddings

bipartite

embedding $P \longrightarrow P ^ { \prime }$

are ‘induced’.) Instead of $\varphi \colon V _ { 1 } \cup V _ { 2 } \to V _ { 1 } ^ { \prime } \cup V _ { 2 } ^ { \prime }$ we may simply write $\varphi \colon P \to P ^ { \prime }$ .

We need two lemmas.

$E$

Lemma 9.3.2. Every bipartite graph can be embedded in a bipartite graph of the form $( X , [ X ] ^ { k } , E )$ with $E = \{ x Y \mid x \in Y \}$ .

Proof . Let $P$ be any bipartite graph, with vertex classes $\{ a _ { 1 } , \ldots , a _ { n } \}$ and $\{ b _ { 1 } , \ldots , b _ { m } \}$ , say. Let $X$ be a set with $2 n + m$ elements, say

$$
X = \left\{x _ {1}, \dots , x _ {n}, y _ {1}, \dots , y _ {n}, z _ {1}, \dots , z _ {m} \right\};
$$

we shall define an embedding $\varphi \colon P \to ( X , [ X ] ^ { n + 1 } , E )$ .

Let us start by setting $\varphi ( a _ { i } ) : = x _ { i }$ for all $i = 1 , \ldots , n$ . Which $( n + 1 )$ -sets $Y \subseteq X$ are suitable candidates for the choice of $\varphi ( b _ { i } )$ for a given vertex $b _ { i }$ ? Clearly those adjacent exactly to the images of the neighbours of $b _ { i }$ , i.e. those satisfying

$$
Y \cap \left\{x _ {1}, \dots , x _ {n} \right\} = \varphi \left(N _ {P} \left(b _ {i}\right)\right). \tag {1}
$$

Since $d ( b _ { i } ) \leqslant n$ , the requirement of (1) leaves at least one of the $n + 1$ elements of $Y$ unspecified. In addition to $\varphi ( N _ { P } ( b _ { i } ) )$ , we may therefore include in each $Y = \varphi ( b _ { i } )$ the vertex $z _ { i }$ as an ‘index’; this ensures that $\varphi ( b _ { i } ) \neq \varphi ( b _ { j } )$ for $i \neq j$ , even when $b _ { i }$ and $b _ { j }$ have the same neighbours in $P$ . To specify the sets $Y = \varphi ( b _ { i } )$ completely, we finally fill them up with ‘dummy’ elements $y _ { j }$ until $| Y | = n + 1$ . 

Our second lemma already covers the bipartite case of the theorem: it says that every bipartite graph has a Ramsey graph—even a bipartite one.

Lemma 9.3.3. For every bipartite graph $P$ there exists a bipartite graph $P ^ { \prime }$ such that for every 2-colouring of the edges of $P ^ { \prime }$ there is an embedding $\varphi : P \to P ^ { \prime }$ for which all the edges of $\varphi ( P )$ have the same colour.

(9.1.3)

$P , X , k , E$

$P ^ { \prime } , X ^ { \prime } , k ^ { \prime }$

Proof . We may assume by Lemma 9.3.2 that $P$ has the form $( X , [ X ] ^ { k } , E )$ with $E = \{ x Y \mid x \in Y \}$ . We show the assertion for the graph $P ^ { \prime } : = { }$ $( X ^ { \prime } , [ X ^ { \prime } ] ^ { k ^ { \prime } } , E ^ { \prime } )$ , where $k ^ { \prime } : = 2 k - 1 , \ X ^ { \prime }$ is any set of cardinality

$$
| X ^ {\prime} | = R \left(k ^ {\prime}, 2 \binom {k ^ {\prime}} {k}, k | X | + k - 1\right),
$$

(this is the Ramsey number defined after Theorem 9.1.3), and

$E ^ { \prime }$

$$
E ^ {\prime} := \left\{x ^ {\prime} Y ^ {\prime} \mid x ^ {\prime} \in Y ^ {\prime} \right\}.
$$

Let us then colour the edges of $P ^ { \prime }$ with two colours $\alpha$ and $\beta$ . Of the $| Y ^ { \prime } | = 2 k - 1$ edges incident with a vertex $Y ^ { \prime } \in [ X ^ { \prime } ] ^ { k ^ { \prime } }$ , at least $k$ must have the same colour. For each $Y ^ { \prime }$ we may therefore choose a fixed $k$ -set $Z ^ { \prime } \subseteq Y ^ { \prime }$ such that all the edges $x ^ { \prime } Y ^ { \prime }$ with $x ^ { \prime } \in Z ^ { \prime }$ have the same colour; we shall call this colour associated with $Y ^ { \prime }$ .

The sets $Z ^ { \prime }$ can lie within their supersets $Y ^ { \prime }$ in $\textstyle { \binom { k ^ { \prime } } { k } }$ ways, as follows. Let $X ^ { \prime }$ be linearly ordered. Then for every $Y ^ { \prime } \in [ X ^ { \prime } ] ^ { k ^ { \prime } }$ there is a unique order-preserving bijection $\sigma _ { Y ^ { \prime } } { : } Y ^ { \prime }  \{ 1 , \ldots , k ^ { \prime } \}$ , which maps $Z ^ { \prime }$ to one of $\binom { k ^ { \prime } } { k }$ possible images.

kWe now colour $[ X ^ { \prime } ] ^ { k ^ { \prime } }$ with the $2 { \binom { k ^ { \prime } } { k } }$ elements of the set

$$
[ \{1, \dots , k ^ {\prime} \} ] ^ {k} \times \{\alpha , \beta \}
$$

as colours, giving each $Y ^ { \prime } ~ \in ~ [ X ^ { \prime } ] ^ { k ^ { \prime } }$ as its colour the pair $\left( \sigma _ { Y ^ { \prime } } ( Z ^ { \prime } ) , \gamma \right)$ , where $\gamma$ is the colour $\alpha$ or $\beta$ associated with $Y ^ { \prime }$ . Since $| X ^ { \prime } |$ was chosen as the Ramsey number with parameters $k ^ { \prime }$ , $2 { \binom { k ^ { \prime } } { k } }$ and $k \left| X \right| + k - 1$ , we know that $X ^ { \prime }$ has a monochromatic subset $W$ of cardinality $k \left| X \right| + k - 1$ . All $Z ^ { \prime }$ with $Y ^ { \prime } \subseteq W$ thus lie within their $Y ^ { \prime }$ in the same way, i.e. there exists an $S \in [ \{ 1 , \ldots , k ^ { \prime } \} ] ^ { k }$ such that $\sigma _ { Y ^ { \prime } } ( Z ^ { \prime } ) = S$ for all $Y ^ { \prime } \in [ W ] ^ { k ^ { \prime } }$ , and all $Y ^ { \prime } \in [ W ] ^ { k ^ { \prime } }$ are associated with the same colour, say with $\alpha$ .

We now construct the desired embedding of $P$ in $P ^ { \prime }$ . We first $\varphi$ define $\varphi$ on $X = : \{ x _ { 1 } , \ldots , x _ { n } \}$ , choosing images $\varphi ( x _ { i } ) = : w _ { i } \in W$ so that $w _ { i } < w _ { j }$ in our ordering of $X ^ { \prime }$ whenever $i < j$ . Moreover, we choose the $w _ { i }$ so that exactly $k - 1$ elements of $W$ are smaller than $w _ { 1 }$ , exactly $k - 1$ lie between $w _ { i }$ and $w _ { i + 1 }$ for $i = 1 , \ldots , n - 1$ , and exactly $k - 1$ are bigger than $w _ { n }$ . Since $| W | = k n + k - 1$ , this can indeed be done (Fig. 9.3.4).

We now define $\varphi$ on $[ X ] ^ { k }$ . Given $Y ~ \in ~ [ X ] ^ { k }$ , we wish to choose $\varphi ( Y ) = : Y ^ { \prime } \in [ X ^ { \prime } ] ^ { k ^ { \prime } }$ so that the neighbours of $Y ^ { \prime }$ among the vertices in $\varphi ( X )$ are precisely the images of the neighbours of $Y$ in $P$ , i.e. the $k$ vertices $\varphi ( x )$ with $x \in Y$ , and so that all these edges at $Y ^ { \prime }$ are coloured $\alpha$ . To find such a set $Y ^ { \prime }$ , we first fix its subset $Z ^ { \prime }$ as $\{ \varphi ( x ) \ \mid \ x \in Y \}$ (these are $k$ vertices of type $w _ { i }$ ) and then extend $Z ^ { \prime }$ by $k ^ { \prime } - k$ further vertices $u \in W \setminus \varphi ( X )$ to a set $Y ^ { \prime } \in [ W ] ^ { k ^ { \prime } }$ , in such a way that $Z ^ { \prime }$ lies correctly within $Y ^ { \prime }$ , i.e. so that $\sigma _ { Y ^ { \prime } } ( Z ^ { \prime } ) = S$ . This can be done, because $k - 1 = k ^ { \prime } - k$ other vertices of $W$ lie between any two $w _ { i }$ . Then

$$
Y ^ {\prime} \cap \varphi (X) = Z ^ {\prime} = \left\{\varphi (x) \mid x \in Y \right\},
$$

so $Y ^ { \prime }$ has the correct neighbours in $\varphi ( X )$ , and all the edges between $Y ^ { \prime }$ and these neighbours are coloured $\alpha$ (because those neighbours lie in $Z ^ { \prime }$ and $Y ^ { \prime }$ is associated with $\alpha$ ). Finally, $\varphi$ is injective on $[ X ] ^ { k }$ : the images $Y ^ { \prime }$ of different vertices $Y$ are distinct, because their intersections with $\varphi ( X )$ differ. Hence, our map $\varphi$ is indeed an embedding of $P$ in $P ^ { \prime }$ . 

![](images/d85daffa3e0f2613949bf810d3df8f2f7bea78a1d6b9e44ef00b9139cdcc1534.jpg)  
Fig. 9.3.4. The graph of Lemma 9.3.3

Second proof of Theorem 9.3.1. Let $H$ be given as in the theorem, and let $n : = R ( r )$ be the Ramsey number of $r : = | H |$ . Then, for every 2-colouring of its edges, the graph $K = K ^ { n }$ contains a monochromatic copy of $H$ —although not necessarily induced.

We start by constructing a graph $G ^ { 0 }$ , as follows. Imagine the vertices of $K$ to be arranged in a column, and replace every vertex by a row of $\binom { n } { r }$ vertices. Then each of the $\binom { n } { r }$ columns arising can be associated with one of the $\binom { n } { r }$ ways of embedding $V ( H )$ in $V ( K )$ ; let us furnish this column with the edges of such a copy of $H$ . The graph $G ^ { 0 }$ thus arising consists of $\binom { n } { r }$ disjoint copies of $H$ and $\left( n - r \right) \left( { n \atop r } \right)$ isolated vertices (Fig. 9.3.5).

In order to define $G ^ { 0 }$ formally, we assume that $V ( K ) = \{ 1 , \dots , n \}$ and choose copies $H _ { 1 } , \ldots , H _ { ( ^ { n } ) }$ of $H$ in $K$ with pairwise distinct vertex sets. (Thus, on each $r$ r-set in $V ( K )$ we have one fixed copy $H _ { j }$ of $H$ .) We then define

$$
V (G ^ {0}) := \left\{(i, j) \mid i = 1, \ldots , n; j = 1, \ldots , \binom {n} {r} \right\}
$$

and

![](images/b3c14e1102ee6c54a02645f9781815d0a5070b84f9920157c37f9b46f5f9d2c5.jpg)  
Fig. 9.3.5. The graph $G ^ { 0 }$

$$
E (G ^ {0}) := \bigcup_ {j = 1} ^ {\binom {n} {r}} \left\{(i, j) \left(i ^ {\prime}, j\right) \mid i i ^ {\prime} \in E \left(H _ {j}\right) \right\}.
$$

The idea of the proof now is as follows. Our aim is to reduce the general case of the theorem to the bipartite case dealt with in Lemma 9.3.3. Applying the lemma iteratively to all the pairs of rows of $G ^ { 0 }$ , we construct a very large graph $G$ such that for every edge colouring of $G$ there is an induced copy of $G ^ { 0 }$ in $G$ that is monochromatic on all the bipartite subgraphs induced by its pairs of rows, i.e. in which edges between the same two rows always have the same colour. The projection of this $G ^ { 0 } \subseteq G$ to $\{ 1 , \ldots , n \}$ (by contracting its rows) then defines an edge colouring of $K$ . (If the contraction does not yield all the edges of $K$ , colour the missing edges arbitrarily.) By the choice of $| K |$ , some $K ^ { r } \subseteq K$ will be monochromatic. The $H _ { j }$ inside this $K ^ { r }$ then occurs with the same colouring in the $j$ th column of our $G ^ { 0 }$ , where it is an induced subgraph of $G ^ { 0 }$ , and hence of $G$ .

Formally, we shall define a sequence $G ^ { 0 } , \ldots , G ^ { m }$ of $n$ -partite graphs $G ^ { k }$ , with $n$ -partition $\{ V _ { 1 } ^ { k } , \ldots , V _ { n } ^ { k } \}$ say, and then let $G : = G ^ { m }$ . The graph $G ^ { 0 }$ has been defined above; let $V _ { 1 } ^ { 0 } , \ldots , V _ { n } ^ { 0 }$ be its rows:

$$
V _ {i} ^ {0} := \left\{(i, j) \mid j = 1, \dots , \binom {n} {r} \right\}.
$$

Now let $e _ { 1 } , \ldots , e _ { m }$ be an enumeration of the edges of $K$ . For $k \ =$ $0 , \ldots , m - 1$ , construct $G ^ { k + 1 }$ from $G ^ { k }$ as follows. If $e _ { k + 1 } = i _ { 1 } i _ { 2 }$ , say, let $P = ( V _ { i _ { 1 } } ^ { k } , V _ { i _ { 2 } } ^ { k } , E )$ be the bipartite subgraph of $G ^ { k }$ induced by its $i _ { 1 } \mathrm { t h }$ $i _ { 1 }$ and $i _ { 2 }$ th row. By Lemma 9.3.3, $P$ has a bipartite Ramsey graph $P ^ { \prime } = ( W _ { 1 } , W _ { 2 } , E ^ { \prime } )$ . We wish to define $G ^ { k + 1 } \supseteq P ^ { \prime }$ in such a way that every (monochromatic) embedding $P \longrightarrow P ^ { \prime }$ can be extended to an embedding $G ^ { k } \to G ^ { k + 1 }$ respecting their $n$ -partitions. Let $\{ \varphi _ { 1 } , . . . , \varphi _ { q } \}$ be the set of all embeddings of $P$ in $P ^ { \prime }$ , and let

$$
V \left(G ^ {k + 1}\right) := V _ {1} ^ {k + 1} \cup \dots \cup V _ {n} ^ {k + 1},
$$

where

$$
V _ {i} ^ {k + 1} := \left\{ \begin{array}{l l} W _ {1} & \text {f o r} i = i _ {1} \\ W _ {2} & \text {f o r} i = i _ {2} \\ \bigcup_ {p = 1} ^ {q} (V _ {i} ^ {k} \times \{p \}) & \text {f o r} i \not \in \{i _ {1}, i _ {2} \}. \end{array} \right.
$$

now define the edge set of (Thus for $i \neq i _ { 1 } , i _ { 2 }$ , we take as $G ^ { k + 1 }$ i iso that the obvious extensions of $V _ { i } ^ { k + 1 }$ just $q$ disjoint copies of $V _ { i } ^ { k }$ .) We $\varphi _ { p }$ to all of $V ( G ^ { k } )$ become embeddings of $G ^ { k }$ i n $G ^ { k + 1 }$ : for $p = 1 , \ldots , q$ , let $\psi _ { p } \colon V ( G ^ { k } ) \to V ( G ^ { k + 1 } )$ be defined by

$$
\psi_ {p} (v) := \left\{ \begin{array}{l l} \varphi_ {p} (v) & \text {f o r} v \in P \\ (v, p) & \text {f o r} v \notin P \end{array} \right.
$$

and let

$$
E (G ^ {k + 1}) := \bigcup_ {p = 1} ^ {q} \left\{\psi_ {p} (v) \psi_ {p} \left(v ^ {\prime}\right) \mid v v ^ {\prime} \in E (G ^ {k}) \right\}.
$$

Now for every 2-colouring of its edges, $G ^ { k + 1 }$ contains an induced copy $\psi _ { p } ( G ^ { k } )$ of $G ^ { k }$ whose edges in $P$ , i.e. those between its $i _ { 1 }$ th and $i _ { 2 }$ th row, have the same colour: just choose $p$ so that $\varphi _ { p } ( P )$ is the monochromatic induced copy of $P$ in $P ^ { \prime }$ that exists by Lemma 9.3.3.

We claim that $G : = G ^ { m }$ satisfies the assertion of the theorem. So let a 2-colouring of the edges of $G$ be given. By the construction of $G ^ { m }$ from $G ^ { m - 1 }$ , we can find in $G ^ { m }$ an induced copy of $G ^ { m - 1 }$ such that for $e _ { m } = i i ^ { \prime }$ all edges between the $i$ th and the $i ^ { \prime }$ th row have the same colour. In the same way, we find inside this copy of $G ^ { m - 1 }$ an induced copy of $G ^ { m - 2 }$ whose edges between the $i$ th and the $i ^ { \prime }$ th row have the same colour also for $i i ^ { \prime } = e _ { m - 1 }$ . Continuing in this way, we finally arrive at an induced copy of $G ^ { 0 }$ in $G$ such that, for each pair $( i , i ^ { \prime } )$ , all the edges between $V _ { i } ^ { \mathrm { 0 } }$ and $V _ { i ^ { \prime } } ^ { \mathrm { 0 } }$ have the same colour. As shown earlier, this $G ^ { 0 }$ contains a monochromatic induced copy $H _ { j }$ of $H$ . 

# 9.4 Ramsey properties and connectivity

According to Ramsey’s theorem, every large enough graph $G$ has a very dense or a very sparse induced subgraph of given order, a $K ^ { r }$ or $\overline { { K ^ { r } } }$ . If we assume that $G$ is connected, we can say a little more:

Proposition 9.4.1. For every $r \in \mathbb N$ there is an $n \in \mathbb N$ such that every connected graph of order at least $n$ contains $K ^ { r }$ , $K _ { 1 , r }$ or $P ^ { r }$ as an induced subgraph.

Proof . Let $d + 1$ be the Ramsey number of $r$ , let $n : = \textstyle { \frac { d } { d - 2 } } ( d - 1 ) ^ { r }$ , (1.3.3) and let $G$ be a graph of order at least $n$ . If $G$ has a vertex $v$ of degree at least $d + 1$ then, by Theorem 9.1.1 and the choice of $d$ , either $N ( v )$ induces a $K ^ { r }$ in $G$ or $\{ v \} \cup N ( v )$ induces a $K _ { 1 , r }$ . On the other hand, if $\Delta ( G ) \leqslant d$ , then by Proposition 1.3.3 $G$ has radius $> r$ , and hence contains two vertices at a distance $\geqslant r$ . Any shortest path in $G$ between these two vertices contains a $P ^ { r }$ . 

In principle, we could now look for a similar set of ‘unavoidable’ $k$ -connected subgraphs for any given connectivity $k$ . To keep thse ‘unavoidable sets’ small, it helps to relax the containment relation from ‘induced subgraph’ for $k = 1$ (as above) to ‘topological minor’ for $k = 2$ , and on to ‘minor’ for $k = 3$ and $k = 4$ . For larger $k$ , no similar results are known.

Proposition 9.4.2. For every $r \in \mathbb N$ there is an $n \in \mathbb N$ such that every 2-connected graph of order at least $_ { n }$ contains $C ^ { r }$ or $K _ { 2 , r }$ as a topological minor.

Proof . Let $d$ be the $n$ associated with $r$ in Proposition 9.4.1, and let $G$ be a 2-connected graph with at least $\begin{array} { r } { \frac { d } { d - 2 } ( d - 1 ) ^ { r } } \end{array}$ vertices. By Proposition 1.3.3, either $G$ has a vertex of degree $> d$ or $\mathrm { d i a m } G \geqslant \mathrm { r a d } G > r$ .

In the latter case let $a , b \in G$ be two vertices at distance $> r$ . By Menger’s theorem (3.3.6), $G$ contains two independent $a$ – $^ b$ paths. These form a cycle of length $> r$ .

Assume now that $G$ has a vertex $\boldsymbol { v }$ of degree $> d$ . Since $G$ is 2- connected, $G - v$ is connected and thus has a spanning tree; let $T$ be a minimal tree in $G - v$ that contains all the neighbours of $v$ . Then every leaf of $T$ is a neighbour of $\boldsymbol { v }$ . By the choice of $d$ , either $T$ has a vertex of degree $\geqslant r$ or $T$ contains a path of length $\geqslant r$ , without loss of generality linking two leaves. Together with $v$ , such a path forms a cycle of length $\geqslant r$ . A vertex $u$ of degree $\geqslant r$ in $T$ can be joined to $\boldsymbol { v }$ by $r$ independent paths through $T$ , to form a $T K _ { 2 , r }$ . 

# Theorem 9.4.3. (Oporowski, Oxley & Thomas 1993)

For every $r \in \mathbb N$ there is an $n \in \mathbb N$ such that every 3-connected graph of order at least $n$ contains a wheel of order $r$ or a $K _ { 3 , r }$ as a minor.

Let us call a graph of the form $C ^ { n } * \overline { { K ^ { 2 } } }$ $n \geqslant 4$ ) a double wheel, the 1-skeleton of a triangulation of the cylinder as in Fig. 9.4.1 a crown, and the 1-skeleton of a triangulation of the M¨obius strip a M¨obius crown.

![](images/031903ddd48da27d32f2da7906dd02c6cfe1dc3320783da836bc380f35a6b3ce.jpg)

![](images/8019a6d2525881d95932b62bf889da3690b1c41c9c18ed3dc74ae28886113d55.jpg)  
Fig. 9.4.1. A crown and a M¨obius crown

# Theorem 9.4.4. (Oporowski, Oxley & Thomas 1993)

For every $r \in \mathbb N$ there is an $n \in \mathbb N$ such that every 4-connected graph with at least $n$ vertices has a minor of order $\geqslant r$ that is a double wheel, a crown, a M¨obius crown, or a $K _ { 4 , s }$ .

At first glance, the ‘unavoidable’ substructures presented in the four theorems above may seem to be chosen somewhat arbitrarily. In fact, the contrary is true: these sets are smallest possible, and as such unique.

To make this precise, let us consider graph properties $\mathcal { P }$ each containing arbitrarily large graphs. Given an order relation $\leqslant$ between graphs (such as the subgraph relation $\subseteq$ , or the minor relation $\preccurlyeq$ ), we write $\mathcal { P } \leqslant \mathcal { P } ^ { \prime }$ if for every $G \in { \mathcal { P } }$ there is a $G ^ { \prime } \in \mathcal { P } ^ { \prime }$ such that $G \leqslant G ^ { \prime }$ . If $\mathcal { P } \leqslant \mathcal { P } ^ { \prime }$ as well as $\mathcal { P } \geqslant \mathcal { P } ^ { \prime }$ , we call $\mathcal { P }$ and ${ \mathcal { P } } ^ { \prime }$ equivalent and write $\mathcal { P } \sim \mathcal { P } ^ { \prime }$ . For example, if $\leqslant$ is the subgraph relation, $\mathcal { P }$ is the class of all paths, ${ \mathcal { P } } ^ { \prime }$ is the class of paths of even length, and $\boldsymbol { S }$ is the class of all subdivisions of stars, then $\mathcal { P } \sim \mathcal { P } ^ { \prime } \leqslant \mathcal { S } \notin \mathcal { P }$ .

If $\boldsymbol { \mathscr { C } }$ is a collection of such properties, we call a finite subset $\{ \mathcal { P } _ { 1 } , \ldots , \mathcal { P } _ { k } \}$ of $\boldsymbol { \mathscr { C } }$ a Kuratowski set for $c$ (with respect to $\leqslant$ ) if the $\mathcal { P } _ { i }$ are incomparable (i.e., $\mathcal { P } _ { i } \not \leqslant \mathcal { P } _ { j }$ whenever $i \neq j$ ) and for every ${ \mathcal { P } } \in { \mathcal { C } }$ 2 there is an $i$ such that $\mathcal { P } _ { i } \leqslant \mathcal { P }$ . We call this Kuratowski set unique if every Kuratowski set for $\boldsymbol { \mathscr { C } }$ can be written as $\{ \mathcal { Q } _ { 1 } , \ldots , \mathcal { Q } _ { k } \}$ with ${ \mathcal { Q } } _ { i } \sim { \mathcal { P } } _ { i }$ for all $i$ .

The essence of our last four theorems can now be stated more comprehensively as follows (cf. Exercise 18).

# Theorem 9.4.5.

(i) The stars and the paths form the unique (2-element) Kuratowski set for the properties of connected graphs, with respect to the subgraph relation.   
(ii) The cycles and the graphs $K _ { 2 , r }$ ( $r ~ \in ~ \mathbb { N }$ ) form the unique (2- element) Kuratowski set for the properties of 2-connected graphs, with respect to the topological minor relation.   
(iii) The wheels and the graphs $K _ { 3 , r }$ $r ~ \in ~ \mathbb { N }$ ) form the unique (2- element) Kuratowski set for the properties of 3-connected graphs, with respect to the minor relation.   
(iv) The double wheels, the crowns, the M¨obius crowns, and the graphs $K _ { 4 , r }$ ( $r \in \mathbb N$ ) form the unique (4-element) Kuratowski set for the properties of 4-connected graphs, with respect to the minor relation. 

# Exercises

1. $-$ Determine the Ramsey number $R ( 3 )$ .

2.− Deduce the case $k = 2$ (but $c$ arbitrary) of Theorem 9.1.3 directly from Theorem 9.1.1.

3. Can you improve the exponential upper bound on the Ramsey number $R ( n )$ for perfect graphs?

4.+ Construct a graph on $\mathbb { R }$ that has neither a complete nor an edgeless induced subgraph on $| \mathbb { R } | = 2 ^ { \aleph _ { 0 } }$ vertices. (So Ramsey’s theorem does not extend to uncountable sets.)

5.+ Prove the edge version of the Erd˝os-P´osa theorem (2.3.2): there exists a function $g \colon  { \mathbb { N } } \to  { \mathbb { R } }$ such that, given $k \in \mathbb N$ , every graph contains either $k$ edge-disjoint cycles or a set of at most $g ( k )$ edges meeting all its cycles. (Hint. Consider in each component a normal spanning tree $T$ . If $T$ has many chords $x y$ , use any regular pattern of how the paths $x T y$ intersect to find many edge-disjoint cycles.)

6.+ Use Ramsey’s theorem to show that for any $k , \ell \in  { \mathbb { N } }$ there is an $n \in \mathbb N$ such that every sequence of $n$ distinct integers contains an increasing subsequence of length $k + 1$ or a decreasing subsequence of length $\ell + 1$ Find an example showing that $n > k \ell$ . Then prove the theorem of Erd˝os and Szekeres that $n = k \ell + 1$ will do.

7. Sketch a proof of the following theorem of Erd˝os and Szekeres: for every $k \in \mathbb N$ there is an $\textit { n } \in  { \mathbb { N } }$ such that among any $n$ points in the plane, no three of them collinear, there are $k$ points spanning a convex $k$ -gon, i.e. such that none of them lies in the convex hull of the others.

8. Prove the following result of Schur: for every $k \in \mathbb N$ there is an $n \in \mathbb N$ such that, for every partition of $\{ 1 , \ldots , n \}$ into $k$ sets, at least one of the subsets contains numbers $x , y , z$ such that $x + y = z$ .

9. Let $( X , \leqslant )$ be a totally ordered set, and let $G = ( V , E )$ be the graph on $V : = [ X ] ^ { 2 }$ with $E : = \{ ( x , y ) ( x ^ { \prime } , y ^ { \prime } ) \mid x < y = x ^ { \prime } < y ^ { \prime } \}$ .

(i) Show that $G$ contains no triangle.   
(ii) Show that $\chi ( G )$ will get arbitrarily large if $| X |$ is chosen large enough.

10. A family of sets is called a $\Delta$ -system if every two of the sets have the same intersection. Show that every infinite family of sets of the same finite cardinality contains an infinite $\Delta$ -system.   
11. Prove that for every $r \in \mathbb N$ and every tree $T$ there exists a $k \in \mathbb N$ such that every graph $G$ with $\chi ( G ) \geqslant k$ and $\omega ( G ) < r$ contains a subdivision of $T$ in which no two branch vertices are adjacent in $G$ (unless they are adjacent in $T$ ).   
12. Let $m , n \in \mathbb { N }$ , and assume that $m - 1$ divides $n - 1$ . Show that every tree $T$ of order $m$ satisfies $R ( T , K _ { 1 , n } ) = m + n - 1$ .

13. Prove that $2 ^ { c } < R ( 2 , c , 3 ) \leqslant 3 c !$ ! for every $c \in \mathbb { N }$ . (Hint. Induction on $c$ .)   
14. $-$ Derive the statement ( ) in the first proof of Theorem 9.3.1 from the theorem itself, i.e. show that $( * )$ is only formally stronger than the theorem.   
15. Show that, given any two graphs $H _ { 1 }$ and $H _ { 2 }$ , there exists a graph $G = G ( H _ { 1 } , H _ { 2 } )$ such that, for every vertex-colouring of $G$ with colours 1 and 2, there is either an induced copy of $H _ { 1 }$ coloured 1 or an induced copy of $H _ { 2 }$ coloured 2 in $G$ .   
16. Show that the Ramsey graph $G$ for $H$ constructed in the second proof of Theorem 9.3.1 does indeed satisfy $\omega ( G ) = \omega ( H )$ .   
17.− The $K ^ { \prime }$ from Ramsey’s theorem, last sighted in Proposition 9.4.1, conspicuously fails to make an appearance from Proposition 9.4.2 onwards. Can it be excused?   
18. Deduce Theorem 9.4.5 from the other four results in Section 9.4, and vice versa.

# Notes

Due to increased interaction with research on random and pseudo-random4 structures (the latter being provided, for example, by the regularity lemma), the Ramsey theory of graphs has recently seen a period of major activity and advance. Theorem 9.2.2 is an early example of this development.

For the more classical approach, the introductory text by R.L. Graham, B.L. Rothschild & J.H. Spencer, Ramsey Theory (2nd edn.), Wiley 1990, makes stimulating reading. This book includes a chapter on graph Ramsey theory, but is not confined to it. Surveys of finite and infinite Ramsey theory are given by J. Neˇsetˇril and A. Hajnal in their chapters in the Handbook of Combinatorics (R.L. Graham, M. Gr¨otschel & L. Lov´asz, eds.), North-Holland 1995. The Ramsey theory of infinite sets forms a substantial part of combinatorial set theory, and is treated in depth in P. Erd˝os, A. Hajnal, A. M´at´e & R. Rado, Combinatorial Set Theory, North-Holland 1984. An attractive collection of highlights from various branches of Ramsey theory, including applications in algebra, geometry and point-set topology, is offered in B. Bollob´as, Graph Theory, Springer GTM 63, 1979.

Theorem 9.2.2 is due to V. Chv´atal, V. R¨odl, E. Szemer´edi & W.T. Trotter, The Ramsey number of a graph with bounded maximum degree, J. Combin. Theory B 34 (1983), 239–243. Our proof follows the sketch in J. Koml´os & M. Simonovits, Szemer´edi’s Regularity Lemma and its applications in graph theory, in (D. Mikl´os, V.T. S´os & T. Sz˝onyi, eds.) Paul Erd˝os is 80, Vol. 2, Proc. Colloq. Math. Soc. J´anos Bolyai (1996). The theorem marks a breakthrough towards a conjecture of Burr and Erd˝os (1975), which asserts that the

Ramsey numbers of graphs with bounded average degree in every subgraph are linear: for every $d \in \mathbb { N }$ , the conjecture says, there exists a constant $c$ such that $R ( H ) \leqslant c | H |$ for all graphs $H$ with $d ( H ^ { \prime } ) \leqslant d$ for all $H ^ { \prime } \subseteq H$ . This conjecture has been verified approximately by A. Kostochka and B. Sudakov, On Ramsey numbers of sparse graphs, Combinatorics, Probability and Computing 12 (2003), 627–641, who proved that $R ( H ) \leqslant | H | ^ { 1 + o ( 1 ) }$ .

Our first proof of Theorem 9.3.1 is based on W. Deuber, A generalization of Ramsey’s theorem, in (A. Hajnal, R. Rado & V.T. S´os, eds.) Infinite and finite sets, North-Holland 1975. The same volume contains the alternative proof of this theorem by Erd˝os, Hajnal and P´osa. R¨odl proved the same result in his MSc thesis at the Charles University, Prague, in 1973. Our second proof of Theorem 9.3.1, which preserves the clique number of $H$ for $G$ , is due to J. Neˇsetˇril & V. R¨odl, A short proof of the existence of restricted Ramsey graphs by means of a partite construction, Combinatorica 1 (1981), 199–202.

The two theorems in Section 9.4 are due to B. Oporowski, J. Oxley & R. Thomas, Typical subgraphs of 3- and 4-connected graphs, J. Combin. Theory B 57 (1993), 239–257.

In Chapter 1.8 we briefly discussed the problem of when a graph contains an Euler tour, a closed walk traversing every edge exactly once. The simple Theorem 1.8.1 solved that problem quite satisfactorily. Let us now ask the analogous question for vertices: when does a graph $G$ contain a closed walk that contains every vertex of $G$ exactly once? If $| G | \geqslant 3$ , then any such walk is a cycle: a Hamilton cycle of $G$ . If $G$ has a Hamilton cycle, it is called hamiltonian. Similarly, a path in $G$ containing every vertex of $G$ is a Hamilton path.

To determine whether or not a given graph has a Hamilton cycle is much harder than deciding whether it is Eulerian, and no good characterization is known1 of the graphs that do. We shall begin this chapter by presenting the standard sufficient conditions for the existence of a Hamilton cycle (Sections 10.1 and 10.2). The rest of the chapter is then devoted to the beautiful theorem of Fleischner that the ‘square’ of every 2-connected graph has a Hamilton cycle. This is one of the main results in the field of Hamilton cycles. The simple proof we present (due to R´ıha)ˇ is still a little longer than other proofs in this book, but not difficult.

# 10.1 Simple sufficient conditions

What kind of condition might be sufficient for the existence of a Hamilton cycle in a graph $G$ ? Purely global assumptions, like high edge density, will not be enough: we cannot do without the local property that every vertex has at least two neighbours. But neither is any large (but constant) minimum degree sufficient: it is easy to find graphs without a Hamilton cycle whose minimum degree exceeds any given constant bound.

The following classic result derives its significance from this background:

# Theorem 10.1.1. (Dirac 1952)

Every graph with $n \geqslant 3$ vertices and minimum degree at least $n / 2$ has a Hamilton cycle.

Proof . Let $G = ( V , E )$ be a graph with $| G | = n \geqslant 3$ and $\delta ( G ) \geqslant n / 2$ . Then $G$ is connected: otherwise, the degree of any vertex in the smallest component $C$ of $G$ would be less than $| C | \leqslant n / 2$ .

Let $P = x _ { 0 } \ldots x _ { k }$ be a longest path in $G$ . By the maximality of $P$ , all the neighbours of $x _ { 0 }$ and all the neighbours of $x _ { k }$ lie on $P$ . Hence at least $n / 2$ of the vertices $x _ { 0 } , \ldots , x _ { k - 1 }$ are adjacent to $x _ { k }$ , and at least $n / 2$ of these same $k < n$ vertices $x _ { i }$ are such that $x _ { 0 } x _ { i + 1 } \in E$ . By the pigeon hole principle, there is a vertex $x _ { i }$ that has both properties, so we have $x _ { 0 } x _ { i + 1 } \in E$ and $x _ { i } x _ { k } \in E$ for some $i < k$ (Fig. 10.1.1).

![](images/00d4ae9a2c42d47269b934cbcc50ad164f67887be1db6a6cd47d52d98d16d021.jpg)  
Fig. 10.1.1. Finding a Hamilton cycle in the proof Theorem 10.1.1

We claim that the cycle $C : = x _ { 0 } x _ { i + 1 } P x _ { k } x _ { i } P x _ { 0 }$ is a Hamilton cycle of $G$ . Indeed, since $G$ is connected, $C$ would otherwise have a neighbour in $G - C$ , which could be combined with a spanning path of $C$ into a path longer than $P$ . 

Theorem 10.1.1 is best possible in that we cannot replace the bound of $n / 2$ with $\lfloor n / 2 \rfloor$ : if $n$ is odd and $G$ is the union of two copies of $K ^ { | n / 2 | }$ 1 meeting in one vertex, then $\delta ( G ) = \lfloor n / 2 \rfloor$ but $\kappa ( G ) = 1$ , so $G$ cannot have a Hamilton cycle. In other words, the high level of the bound of $\delta \geqslant n / 2$ is needed to ensure, if nothing else, that $G$ is 2-connected: a condition just as trivially necessary for hamiltonicity as a minimum degree of at least 2. It would seem, therefore, that prescribing some high (constant) value for $\kappa$ rather than for $\delta$ stands a better chance of implying hamiltonicity. However, this is not so: although every large enough $k$ -connected graph contains a cycle of length at least $2 k$ (Ex. 16, Ch. 3), the graphs $K _ { k , n }$ show that this is already best possible.

Slightly more generally, a graph $G$ with a separating set $S$ of $k$ vertices such that $G - S$ has more than $k$ components is clearly not hamiltonian. Could it be true that all non-hamiltonian graphs have such a separating set, one that leaves many components compared with its size? We shall address this question in a moment.

For now, just note that such graphs as above also have relatively large independent sets: pick one vertex from each component of $G - S$ to obtain one of order at least $k + 1$ . Might we be able to force a Hamilton cycle by forbidding large independent sets?

By itself, the assumption of $\alpha ( G ) \leqslant k$ already guarantees a cycle of length at least $| G | / k$ (Ex. 13, Ch. 5). And combined with the assumption of $k$ -connectedness, it does indeed imply hamiltonicity:

Proposition 10.1.2. Every graph $G$ with $| G | \geqslant 3$ and $\alpha ( G ) \leqslant \kappa ( G )$ has a Hamilton cycle.

Proof . Put $\kappa ( G ) = : k$ , and let $C$ be a longest cycle in $G$ . Enumerate the (3.3.4) vertices of $C$ cyclically, say as $V ( C ) = \{ v _ { i } \mid i \in \mathbb { Z } _ { n } \}$ with $v _ { i } v _ { i + 1 } \in E ( C )$ $k$ for all $i \in \mathbb { Z } _ { n }$ . If $C$ is not a Hamilton cycle, pick a vertex $v \in G - C$ and a $\boldsymbol { v }$ – $C$ fan ${ \mathcal { F } } = \{ P _ { i } \ | \ i \in I \}$ in $G$ , where $I \subseteq \mathbb { Z } _ { n }$ and each $P _ { i }$ ends in $v _ { i }$ . Let $\mathcal { F }$ be chosen with maximum cardinality; then $v v _ { j } \notin E ( G )$ for any $j \notin I$ , and

$$
| \mathcal {F} | \geqslant \min  \left\{k, | C | \right\} \tag {1}
$$

by Menger’s theorem (3.3.4).

![](images/f157d7600b82658a30857f2aa80cd414877ef254ff7a74a93fd10490d761d21e.jpg)

![](images/55bbfc611e5c84eccd3620c4dcd7f60bd7fd8779de9ec11bb4b1677689eac6d2.jpg)  
Fig. 10.1.2. Two cycles longer than $C$

For every $i \in I$ , we have $i + 1 \notin I$ : otherwise, $( C \cup P _ { i } \cup P _ { i + 1 } ) - v _ { i } v _ { i + 1 }$ would be a cycle longer than $C$ (Fig. 10.1.2, left). Thus $| \mathcal { F } | < | C |$ , and hence $| I | = | \mathcal { F } | \geqslant k$ by (1). Furthermore, $v _ { i + 1 } v _ { j + 1 } \not \in E ( G )$ for all $i , j \in I$ , as otherwise $( C \cup P _ { i } \cup P _ { j } ) + v _ { i + 1 } v _ { j + 1 } - v _ { i } v _ { i + 1 } - v _ { j } v _ { j + 1 }$ would be a cycle longer than $C$ (Fig. 10.1.2, right). Hence $\{ v _ { i + 1 } \mid i \in I \} \cup \{ v \}$ is a set of $k + 1$ or more independent vertices in $G$ , contradicting $\alpha ( G ) \leqslant k$ . 

Let us return to the question whether an assumption that no small separator leaves many components can guarantee a Hamilton cycle. A graph $G$ is called $t$ -tough, where $t > 0$ is any real number, if for every separator $S$ the graph $G - S$ has at most $| S | / t$ components. Clearly, hamiltonian graphs must be 1-tough—so what about the converse?

Unfortunately, it is not difficult to find even small graphs that are 1-tough but have no Hamilton cycle (Exercise 5), so toughness does not provide a characterization of hamiltonian graphs in the spirit of Menger’s theorem or Tutte’s 1-factor theorem. However, a famous conjecture asserts that $t$ -toughness for some $t$ will force hamiltonicity:

# Toughness Conjecture. (Chv´atal 1973)

There exists an integer $t$ such that every $t$ -tough graph has a Hamilton cycle.

The toughness conjecture was long expected to hold even with $t = 2$ . This has recently been disproved, but the general conjecture remains open. See the exercises for how the conjecture ties in with the results given in the remainder of this chapter.

It may come as a surprise to learn that hamiltonicity is also related to the four colour problem. As we noted in Chapter 6.6, the four colour theorem is equivalent to the non-existence of a planar snark, i.e. to the assertion that every bridgeless planar cubic graph has a 4-flow. It is easily checked that ‘bridgeless’ can be replaced with ‘3-connected’ in this assertion, and that every hamiltonian graph has a 4-flow (Ex. 12, Ch. 6). For a proof of the four colour theorem, therefore, it would suffice to show that every 3-connected planar cubic graph has a Hamilton cycle!

Unfortunately, this is not the case: the first counterexample was found by Tutte in 1946. Ten years later, Tutte proved the following deep theorem as a best possible weakening:

# Theorem 10.1.3. (Tutte 1956)

Every 4-connected planar graph has a Hamilton cycle.

Although, at first glance, it appears that the study of Hamilton cycles is a part of graph theory that cannot possibly extend to infinite graphs, there is a fascinating conjecture that does just that. Recall that a circle in an infinite graph $G$ is a homeomorphic copy of the unit circle $S ^ { 1 }$ in the topological space $| G |$ formed by $G$ and its ends (see Chapter 8.5). A Hamilton circle of $G$ is a circle that contains every vertex of $G$ .

# Conjecture. (Bruhn 2003)

Every locally finite 4-connected planar graph has a Hamilton circle.

# 10.2 Hamilton cycles and degree sequences

Historically, Dirac’s theorem formed the point of departure for the discovery of a series of weaker and weaker degree conditions, all sufficient for hamiltonicity. The development culminated in a single theorem that encompasses all the earlier results: the theorem we shall prove in this section.

If $G$ is a graph with $n$ vertices and degrees $d _ { 1 } \leqslant . . . \leqslant d _ { n }$ , then the $n$ -tuple $( d _ { 1 } , \ldots , d _ { n } )$ is called the degree sequence of $G$ . Note that this

sequence is unique, even though $G$ has several vertex enumerations giving rise to its degree sequence. Let us call an arbitrary integer sequence $( a _ { 1 } , \ldots , a _ { n } )$ hamiltonian if every graph with $n$ vertices and a degree sequence pointwise greater than $( a _ { 1 } , \ldots , a _ { n } )$ is hamiltonian. (A sequence $( d _ { 1 } , \ldots , d _ { n } )$ is pointwise greater than $( a _ { 1 } , \ldots , a _ { n } )$ if $d _ { i } \geqslant a _ { i }$ for all $i$ .)

The following theorem characterizes all hamiltonian sequences:

hamiltonian sequence pointwise greater

# Theorem 10.2.1. (Chv´atal 1972)

An integer sequence $( a _ { 1 } , \ldots , a _ { n } )$ such that $0 \leqslant a _ { 1 } \leqslant \ldots \leqslant a _ { n } < n$ and $n \geqslant 3$ is hamiltonian if and only if the following holds for every $i < n / 2$ :

$$
a _ {i} \leqslant i \Rightarrow a _ {n - i} \geqslant n - i.
$$

Proof . Let $( a _ { 1 } , \ldots , a _ { n } )$ be an arbitrary integer sequence such that $( a _ { 1 } , \ldots , a _ { n } )$ $0 \leqslant a _ { 1 } \leqslant \ldots \leqslant a _ { n } < n$ and $n \geqslant 3$ . We first assume that this sequence satisfies the condition of the theorem and prove that it is hamiltonian.

Suppose not. Then there exists a graph whose degree sequence $( d _ { 1 } , \ldots , d _ { n } )$ satisfies

$$
d _ {i} \geqslant a _ {i} \quad \text {f o r a l l} i \tag {1}
$$

but which has no Hamilton cycle. Let $G = ( V , E )$ be such a graph, $G = ( V , E )$ chosen with the maximum number of edges.

By (1), our assumptions for $( a _ { 1 } , \ldots , a _ { n } )$ transfer to the degree sequence $( d _ { 1 } , \ldots , d _ { n } )$ of $G$ ; thus,

$$
d _ {i} \leqslant i \Rightarrow d _ {n - i} \geqslant n - i \quad \text {f o r a l l} i <   n / 2. \tag {2}
$$

Let $x , y$ be distinct and non-adjacent vertices in $G$ , with $d ( x ) \leqslant d ( y )$ and $d ( x ) + d ( y )$ as large as possible. One easily checks that the degree sequence of $G + x y$ is pointwise greater than $( d _ { 1 } , \ldots , d _ { n } )$ , and hence than $( a _ { 1 } , \ldots , a _ { n } )$ . Hence, by the maximality of $G$ , the new edge $x y$ lies on a Hamilton cycle $H$ of $G + x y$ . Then $H - x y$ is a Hamilton path $x _ { 1 } , \ldots , x _ { n }$ in $G$ , with $x _ { 1 } = x$ and $x _ { n } = y$ say.

As in the proof of Dirac’s theorem, we now consider the index sets

$$
I := \left\{i \mid x x _ {i + 1} \in E \right\} \quad \text {a n d} \quad J := \left\{j \mid x _ {j} y \in E \right\}.
$$

Then $I \cup J \subseteq \{ 1 , \dotsc , n - 1 \}$ , and $I \cap J = \emptyset$ because $G$ has no Hamilton cycle. Hence

$$
d (x) + d (y) = | I | + | J | <   n, \tag {3}
$$

so $h : = d ( x ) < n / 2$ by the choice of $x$ .

Since $x _ { i } y ~ \notin ~ E$ for all $i \in I$ , all these $x _ { i }$ were candidates for the choice of $x$ (together with $y$ ). Our choice of $\{ x , y \}$ with $d ( x ) + d ( y )$ maximum thus implies that $d ( x _ { i } ) \leqslant d ( x )$ for all $i \in I$ . Hence $G$ has at

least $| I | = h$ vertices of degree at most $h$ , so $d _ { h } \leqslant h$ . By (2), this implies that $d _ { n - h } \geqslant n - h$ , i.e. the $h + 1$ vertices with the degrees $d _ { n - h } , \ldots , d _ { n }$ all have degree at least $n - h$ . Since $d ( x ) = h$ , one of these vertices, $z$ say, is not adjacent to $x$ . Since

$$
d (x) + d (z) \geqslant h + (n - h) = n,
$$

this contradicts the choice of $x$ and $y$ by (3).

Let us now show that, conversely, for every sequence $( a _ { 1 } , \ldots , a _ { n } )$ as in the theorem, but with

$$
a _ {h} \leqslant h \quad \mathrm {a n d} \quad a _ {n - h} \leqslant n - h - 1
$$

for some $h < n / 2$ , there exists a graph that has a pointwise greater degree sequence than $( a _ { 1 } , \ldots , a _ { n } )$ but no Hamilton cycle. As the sequence

$$
(\underbrace {h , \ldots , h} _ {h \mathrm {t i m e s}}, \underbrace {n - h - 1 , \ldots , n - h - 1} _ {n - 2 h \mathrm {t i m e s}}, \underbrace {n - 1 , \ldots , n - 1} _ {h \mathrm {t i m e s}})
$$

is pointwise greater than $( a _ { 1 } , \ldots , a _ { n } )$ , it suffices to find a graph with this degree sequence that has no Hamilton cycle.

![](images/f2b7541286ba77807f27ae01d07612da668ab53e5c6d68ac4169361da374930a.jpg)  
Fig. 10.2.1. Any cycle containing $v _ { 1 } , \ldots , v _ { h }$ misses $v _ { h + 1 }$

Figure 10.2.1 shows such a graph, with vertices $v _ { 1 } , \ldots , v _ { n }$ and the edge set

$$
\left\{v _ {i} v _ {j} \mid i, j > h \right\} \cup \left\{v _ {i} v _ {j} \mid i \leqslant h; j > n - h \right\};
$$

it is the union of a $K ^ { n - h }$ on the vertices $v _ { h + 1 } , \ldots , v _ { n }$ and a $K _ { h , h }$ with partition sets $\{ v _ { 1 } , \ldots , v _ { h } \}$ and $\{ v _ { n - h + 1 } , \ldots , v _ { n } \}$ . 

By applying Theorem 10.2.1 to $G * K ^ { 1 }$ , one can easily prove the following adaptation of the theorem to Hamilton paths. Let an integer sequence be called path-hamiltonian if every graph with a pointwise greater degree sequence has a Hamilton path.

Corollary 10.2.2. An integer sequence $( a _ { 1 } , \ldots , a _ { n } )$ such that $n \geqslant 2$ and $0 \leqslant a _ { 1 } \leqslant \ldots \leqslant a _ { n } < n$ is path-hamiltonian if and only if every $i \leqslant n / 2$ is such that $a _ { i } < i \Rightarrow a _ { n + 1 - i } \geqslant n - i$ . 

# 10.3 Hamilton cycles in the square of a graph

Given a graph $G$ and a positive integer $d$ , we denote by $G ^ { d }$ the graph on $V ( G )$ in which two vertices are adjacent if and only if they have distance at most $d$ in $G$ . Clearly, $G = G ^ { 1 } \subseteq G ^ { 2 } \subseteq . . .$ Our goal in this section is to prove the following fundamental result:

# Theorem 10.3.1. (Fleischner 1974)

If $G$ is a 2-connected graph, then $G ^ { 2 }$ has a Hamilton cycle.

We begin with three simple lemmas. Let us say that an edge $e \in G ^ { 2 }$ bridges a vertex $v \in G$ if its ends are neighbours of $v$ in $G$ .

$G ^ { d }$

bridges

Lemma 10.3.2. Let $P = v _ { 0 } \ldots v _ { k }$ be a path $k \geqslant 1$ ), and let $G$ be the graph obtained from $P$ by adding two vertices , together with the $u , w$ edges $u v _ { 1 }$ and $w v _ { k }$ (Fig. 10.3.1).

(i) $P ^ { 2 }$ contains a path $Q$ from $v _ { 0 }$ to $v _ { 1 }$ with $V ( Q ) = V ( P )$ and $v _ { k - 1 } v _ { k } \ \in \ E ( Q )$ , such that each of the vertices $v _ { 1 } , \ldots , v _ { k - 1 }$ is bridged by an edge of $Q$ .   
(ii) $G ^ { 2 }$ contains disjoint paths $Q$ from $v _ { 0 }$ to $v _ { k }$ and $Q ^ { \prime }$ from u to $w$ , such that $V ( Q ) \cup V ( Q ^ { \prime } ) = V ( G )$ and each of the vertices $v _ { 1 } , \ldots , v _ { k }$ is bridged by an edge of $Q$ or $Q ^ { \prime }$ .

![](images/718fad47504d1333d7f52927928493fd01703140d1462fa4acf0f4c03604c62f.jpg)  
Fig. 10.3.1. The graph $G$ in Lemma 10.3.2

Proof . (i) If $k$ is even, let $Q : = v _ { 0 } v _ { 2 } \ldots v _ { k - 2 } v _ { k } v _ { k - 1 } v _ { k - 3 } \ldots v _ { 3 } v _ { 1 }$ . If $k$ i s odd, let $\begin{array} { r } { Q : = v _ { 0 } v _ { 2 } \ldots v _ { k - 1 } v _ { k } v _ { k - 2 } \ldots v _ { 3 } v _ { 1 } } \end{array}$ .

(ii) If $k$ is even, let $Q : = v _ { 0 } v _ { 2 } \ldots v _ { k - 2 } v _ { k }$ ; if $k$ is odd, let $Q : =$ $v _ { 0 } v _ { 1 } v _ { 3 } \ldots v _ { k - 2 } v _ { k }$ . In both cases, let $Q ^ { \prime }$ be the $u$ – $w$ path on the remaining vertices of $G ^ { 2 }$ . 

Lemma 10.3.3. Let $G = ( V , E )$ be a cubic multigraph with a Hamilton cycle $C$ . Let $e \in E ( C )$ and $f \in E \setminus E ( C )$ be edges with a common end $v$ ， (Fig. 10.3.2). Then there exists a closed walk in $G$ that traverses $e$ once, every other edge of $C$ once or twice, and every edge in $E \setminus E ( C )$ once. This walk can be chosen to contain the triple $( e , v , f )$ , that is, it traverses e in the direction of $\boldsymbol { v }$ and then leaves $v$ by the edge $f$ .

![](images/1c8a81e3dd37b2d5d0c752ea91cb2b1da41d60afc8428dc47c39a1bc4c66e787.jpg)

![](images/d914d067cfa77a5896c10e3855175af00f57e5ffb1eb3bc43b64966d9f3bc74d.jpg)  
Fig. 10.3.2. The multigraphs $G$ and $G ^ { \prime }$ in Lemma 10.3.3

Proof . By Proposition 1.2.1, $C$ has even length. Replace every other edge of $C$ by a double edge, in such a way that $e$ does not get replaced. In the arising 4-regular multigraph $G ^ { \prime }$ , split $v$ into two vertices $v ^ { \prime } , v ^ { \prime \prime }$ , making $v ^ { \prime }$ incident with $e$ and $f$ , and $v ^ { \prime \prime }$ incident with the other two edges at $v$ (Fig. 10.3.2). By Theorem 1.8.1 this multigraph has an Euler tour, which induces the desired walk in $G$ . 

Lemma 10.3.4. For every 2-connected graph $G$ and $x \in V ( G )$ , there is a cycle $C \subseteq G$ that contains $x$ as well as a vertex $y \neq x$ with $N _ { G } ( y ) \subseteq V ( C )$ .

Proof . If $G$ has a Hamilton cycle, there is nothing more to show. If not, let $C ^ { \prime } \subseteq G$ be any cycle containing $x$ ; such a cycle exists, since $G$ is 2-connected. Let $D$ be a component of $G - C ^ { \prime }$ . Assume that $C ^ { \prime }$ and $D$ are chosen so that $| D |$ is minimal. Since $G$ is 2-connected, $D$ has at least two neighbours on $C ^ { \prime }$ . Then $C ^ { \prime }$ contains a path $P$ between two such neighbours $u$ and $v$ , whose interior $\mathring { P }$ does not contain $x$ and has no neighbour in $D$ (Fig. 10.3.3). Replacing $P$ in $C ^ { \prime }$ by a $u$ – $v$ path

![](images/a5a67c8de7ccac2d692209e406a4a13e20f7731ea68531b811cbcd76081d86a1.jpg)  
Fig. 10.3.3. The proof of Lemma 10.3.4

through $D$ , we obtain a cycle $C$ that contains $x$ and a vertex $y \in D$ . If $y$ had a neighbour $z$ in $G - C$ , then $z$ would lie in a component $D ^ { \prime } \subsetneq D$ of $G - C$ , contradicting the choice of $C ^ { \prime }$ and $D$ . Hence all the neighbours of $y$ lie on $C$ , and $C$ satisfies the assertion of the lemma. 

Proof of Theorem 10.3.1. We show by induction on $| G |$ that, given any vertex $x ^ { \ast } \in G$ , there is a Hamilton cycle $H$ in $G ^ { 2 }$ with the following property:

$$
\text {B o t h e d g e s o f H a t} x ^ {*} \text {l i e i n G}. \tag {*}
$$

For $| G | = 3$ we have $G \ = \ K ^ { 3 }$ , and the assertion is trivial. So let $| G | \geqslant 4$ , assume the assertion for graphs of smaller order, and let $x ^ { * } \in V ( G )$ be given. By Lemma 10.3.4, there is a cycle $C \subseteq G$ that contains both $x ^ { * }$ and a vertex $y ^ { * } \neq x ^ { * }$ whose neighbours in $G$ all lie on $C$ .

If $C$ is a Hamilton cycle of $G$ , there is nothing to show; so assume that $G - C \neq \emptyset$ . Consider a component $D$ of $G - C$ . Let $\ddot { D }$ denote the graph $G / ( G - D )$ obtained from $G$ by contracting $G - D$ into a new vertex $\ddot { x }$ . If $| D | = 1$ , set ${ \mathcal { P } } ( D ) : = \{ D \}$ . If $| D | > { \underline { { 1 } } }$ , then $\tilde { D }$ is again 2-connected. Hence, by the induction hypothesis, $\bar { D } ^ { 2 }$ has a Hamilton cycle $\bar { C }$ whose edges at $\ddot { x }$ both lie in $\tilde { D }$ . Note that the path $\ddot { C } - \tilde { x }$ may have some edges that do not lie in $G ^ { 2 }$ : edges joining two neighbours of $\ddot { x }$ that have no common neighbour in $G$ (and are themselves non-adjacent in $G$ ). Let $\tilde { E }$ denote the set of these edges, and let $\mathcal { P } ( D )$ denote the set of components of $( \tilde { C } - \tilde { x } ) - \tilde { E }$ ; this is a set of paths in $G ^ { 2 }$ whose ends are adjacent to $\ddot { x }$ in $\ddot { D }$ (Fig. 10.3.4).

![](images/86d1046266da401fd1d5188afefff03a8139a9902f83557bd6af08caafce42c9.jpg)  
Fig. 10.3.4. $\mathcal { P } ( D )$ consists of three paths, one of which is trivial

Let $\mathcal { P }$ denote the union of the sets ${ \mathcal { P } } ( D )$ over all components $D$ of $G - C$ . Clearly, $\mathcal { P }$ has the following properties:

The elements of $\mathcal { P }$ are pairwise disjoint paths in $G ^ { 2 }$ avoiding pat $C$ $\begin{array} { r } { V ( G ) = V ( C ) \cup \bigcup _ { P \in { \mathcal P } } V ( P ) } \end{array}$ . Every end ; we choose $y$ of ach a (1) $P \in \mathcal { P }$ $C$ $G$ neighbour and call it the foot of $P$ at $y$ .

If $P \in \mathcal { P }$ is trivial, then $P$ has exactly one foot. If $P$ is non-trivial, then $P$ has a foot at each of its ends. These two feet need not be distinct, however; so any non-trivial $P$ has either one or two feet.

We shall now modify $\mathcal { P }$ a little, preserving the properties summarized under (1); no properties of $\mathcal { P }$ other than those will be used later in the proof. If a vertex of $C$ is a foot of two distinct paths $P , P ^ { \prime } \in \mathcal { P }$ , say at $y \in P$ and at $y ^ { \prime } \in P ^ { \prime }$ , then $y y ^ { \prime }$ is an edge and $P y y ^ { \prime } P ^ { \prime }$ is a path in $G ^ { 2 }$ ; we replace $P$ and $P ^ { \prime }$ in $\mathcal { P }$ by this path. We repeat this modification of $\mathcal { P }$ until the following holds:

$$
\text {N o v e r t e x o f C i s a f o o t o f t w o d i s t i n c t p a t h s i n} \mathcal {P}. \tag {2}
$$

$\mathcal { P } _ { 1 } , \mathcal { P } _ { 2 }$ $X _ { 1 } , X _ { 2 }$

For $i = 1 , 2$ let $\mathcal { P } _ { i } \subseteq \mathcal { P }$ denote the set of all paths in $\mathcal { P }$ with exactly $i$ feet, and let $X _ { i } \subseteq V ( C )$ denote the set of all feet of paths in $\mathcal { P } _ { i }$ . Then $X _ { 1 } \cap X _ { 2 } = \emptyset$ by (2), and $y ^ { * } \notin X _ { 1 } \cup X _ { 2 }$ .

Let us also simplify $G$ a little; again, these changes will affect neither the paths in $\mathcal { P }$ nor the validity of (1) and (2). First, we shall assume from now on that all elements of $\mathcal { P }$ are paths in $G$ itself, not just in $G ^ { 2 }$ . This assumption may give us some additional edges for $G ^ { 2 }$ , but we shall not use these in our construction of the desired Hamilton cycle $H$ . (Indeed, $H$ will contain all the paths from $\mathcal { P }$ whole, as subpaths.) Thus if $H$ lies in $G ^ { 2 }$ and satisfies $( * )$ for the modified version of $G$ , it will do so also for the original. For every $P \in \mathcal { P }$ , we further delete all $P$ – $C$ edges in $G$ except those between the ends of $P$ and its corresponding feet. Finally, we delete all chords of $C$ in $G$ . We are thus assuming without loss of generality:

The only edges of $G$ between $C$ and a path $P \in \mathcal { P }$ are the twfeet. ( etween the ends of , these two edges c $P$ and its correspondingncide.) The only edges (3) $T | P | = 1$ of $G$ with both ends on $C$ are the edges of $C$ itself.

Our goal is to construct the desired Hamilton cycle $H$ of $G ^ { 2 }$ from the paths in $\mathcal { P }$ and suitable paths in $C ^ { 2 }$ . As a first approximation, we shall construct a closed walk $W$ in the graph

$\tilde { G }$

$$
\tilde {G} := G - \bigcup \mathcal {P} _ {1},
$$

a walk that will already satisfy a ( )-type condition and traverse every path in $\mathcal { P } _ { 2 }$ exactly once. Later, we shall modify $W$ so that it passes through every vertex of $C$ exactly once and, finally, so as to include the paths from $\mathcal { P } _ { 1 }$ . For the construction of $W$ we assume that $\mathcal { P } _ { 2 } \neq \emptyset$ ; the case of $\mathcal { P } _ { 2 } = \varnothing$ is much simpler and will be treated later.

We start by choosing a fixed cyclic orientation of $C$ , a bijection $i \mapsto v _ { i }$ from $\mathbb { Z } _ { | C | }$ to $V ( C )$ with $v _ { i } v _ { i + 1 } \in E ( C )$ for all $i \in \mathbb { Z } _ { | C | }$ . Let us

think of this orientation as clockwise; then every vertex $v _ { i } \in C$ has a right neighbour $v _ { i } ^ { + } : = v _ { i + 1 }$ and a left neighbour $v _ { i } ^ { - } : = v _ { i - 1 }$ . Accordingly, the edge $v ^ { - } v$ lies to the left of $v$ , the edge $v v ^ { + }$ lies on its right, and so on.

A non-trivial path $P = v _ { i } v _ { i + 1 } . . . v _ { j - 1 } v _ { j }$ in $C$ such that $V ( P ) \cap X _ { 2 } =$ $\{ v _ { i } , v _ { j } \}$ will be called an interval , with left end $v _ { i }$ and right end $v _ { j }$ . Thus, $C$ is the union of $| X _ { 2 } | = 2 | \mathcal { P } _ { 2 } |$ intervals. As usual, we write $P = :$ = $[ v _ { i } , v _ { j } ]$ and set $( v _ { i } , v _ { j } ) : = \bar { P }$ as well as $[ v _ { i } , v _ { j } ) : = P \mathring { v } _ { j }$ and $\left( v _ { i } , v _ { j } \right] : = \stackrel { \circ } { v } _ { i } P$ . For intervals $[ u , v ]$ and $[ v , w ]$ with a common end $v$ we say that $[ u , v ]$ lies to the left of $[ v , w ]$ , and $[ v , w ]$ lies to the right of $[ u , v ]$ . We denote the unique interval $[ v , w ]$ with $\boldsymbol { x } ^ { * } \in \left( \boldsymbol { v } , \boldsymbol { w } \right]$ as $I ^ { * }$ , the path in $\mathcal { P } _ { 2 }$ with foot $w$ as $P ^ { * }$ , and the path $I ^ { * } w P ^ { * }$ as $Q ^ { * }$ .

For the construction of $W$ , we may think of $\ddot { G }$ as a multigraph $M$ on $X _ { 2 }$ whose edges are the intervals on $C$ and the paths in $\mathcal { P } _ { 2 }$ (with their feet as ends). By (2), $M$ is cubic, so we may apply Lemma 10.3.3 with $e : = I ^ { * }$ and $f : = P ^ { * }$ . The lemma provides us with a closed walk $W$ in $\ddot { G }$ which traverses $I ^ { * }$ once, every other interval of $C$ once or twice, and every path in $\mathcal { P } _ { 2 }$ once. Moreover, $W$ contains $Q ^ { * }$ as a subpath. The two edges at $x ^ { * }$ of this path lie in $G$ ; in this sense, $W$ already satisfies ( ).

Let us now modify $W$ so that $W$ passes through every vertex of $C$ exactly once. Simultaneously, we shall prepare for the later inclusion of the paths from $\mathcal { P } _ { 1 }$ by defining a map $v \mapsto e ( v )$ that is injective on $X _ { 1 }$ and assigns to every $v \in X _ { 1 }$ an edge $e ( v )$ of the modified $W$ with the following property:

The edge latter cas $e ( v )$ bridand $v$ ncident with it. In the. $( * * )$ $e ( v ) \in C$ $e ( v ) \neq v x ^ { * }$

For simplicity, we shall define the map $v \mapsto e ( v )$ on all of $V ( C ) \setminus X _ { 2 }$ , a set that includes $X _ { 1 }$ by (2). To ensure injectivity on $X _ { 1 }$ , we only have to make sure that no edge $v w \in C$ is chosen both as $e ( v )$ and as $e ( w )$ . Indeed, since $| X _ { 1 } | \geqslant 2$ if injectivity is a problem, and $\mathcal { P } _ { 2 } \neq \emptyset$ by assumption, we have $\left| C - y ^ { * } \right| \geqslant \left| X _ { 1 } \right| + 2 \left| \mathcal { P } _ { 2 } \right| \geqslant 4$ and hence $| C | \geqslant 5$ ; thus, no edge of $G ^ { 2 }$ can bridge more than one vertex of $C$ , or bridge a vertex of $C$ and lie on $C$ at the same time.

For our intended adjustments of $W$ at the vertices of $C$ , we consider the intervals of $C$ one at a time. By definition of $W$ , every interval is of one of the following three types:

Type 1 : $W$ traverses $I$ once;

Type 2 : $W$ traverses $I$ twice, in one direction and back immediately afterwards (formally: $W$ contains a triple $( e , x , e )$ with $x \in X _ { 2 }$ and $e \in E ( I )$ );

Type 3 : $W$ traverses $I$ twice, on separate occasions (i.e., there is no triple as above).

$v ^ { + }$ , right

$v ^ { - }$ , left

interval

$[ v , w ]$ etc.

$I ^ { * } , P ^ { * }$

$Q ^ { * }$

$W$

$e ( v )$

dead end

By definition of $W$ , the interval $I ^ { * }$ is of type 1. The vertex $x$ in the definition of a type 2 interval will be called the dead end of that interval. Finally, since $Q ^ { * }$ is a subpath of $W$ and $W$ traverses both $I ^ { * }$ and $P ^ { * }$ only once, we have:

The interval to the right of $I ^ { * }$ is of type 2 and has its dead (4)

$$
\begin{array}{l} I, x _ {1}, x _ {2} \\ y _ {1}, y _ {2} \\ I ^ {-} \\ \end{array}
$$

Consider a fixed interval $I = \left[ x _ { 1 } , x _ { 2 } \right]$ . Let $y _ { 1 }$ be the neighbour of , and the neighbour of on a path in $\mathcal { P } _ { 2 }$ . Let $I ^ { - }$ denote the $x _ { 1 }$ $y _ { 2 }$ $x _ { 2 }$ interval to the left of $I$ .

Suppose first that $I$ is of type 1. We then leave $W$ unchanged on $I$ . If $I \ne I ^ { * }$ we choose as $e ( v )$ , for each $v \in \mathring { I }$ , the edge to the left of $\boldsymbol { v }$ . As $I ^ { - } \ne I ^ { * }$ by (4), and hence $x _ { 1 } \neq x ^ { * }$ , these choices of $e ( v )$ satisfy $( * * )$ . If $I = I ^ { * }$ , we define $e ( v )$ as the edge left of $v$ if $v \in \left( x _ { 1 } , x ^ { * } \right] \cap { \dot { I } }$ , and as the edge right of $\boldsymbol { v }$ if $v \in ( x ^ { * } , x _ { 2 } )$ . These choices of $e ( v )$ are again compatible with $( * * )$ .

Suppose now that $I$ is of type 2. Assume first that $x _ { 2 }$ is the dead end of $I$ . Then $W$ contains the walk $y _ { 1 } x _ { 1 } I x _ { 2 } I x _ { 1 } I ^ { - }$ (possibly in reverse order). We now apply Lemma 10.3.2 (i) with $P : = y _ { 1 } x _ { 1 } I { \ddot { x } } _ { 2 }$ , and replace in $W$ the subwalk $y _ { 1 } x _ { 1 } I x _ { 2 } I x _ { 1 }$ by the $y _ { 1 } - x _ { 1 }$ path $Q \subseteq G ^ { 2 }$ of the lemma (Fig. 10.3.5). Then $V ( \check { Q } ) = V ( P ) \setminus \{ y _ { 1 } , x _ { 1 } \} = V ( \check { I } )$ . The vertices

![](images/64c89dc7080b1f9e92eae120a7e2c93814c695ee42da36ec937742d9417af1ba.jpg)  
Fig. 10.3.5. How to modify $W$ on an interval of type 2

$v \in ( x _ { 1 } , x _ { 2 } ^ { - } )$ are each bridged by an edge of $Q$ , which we choose as $e ( v )$ . As $e ( x _ { 2 } ^ { - } )$ we choose the edge to the left of $x _ { 2 } ^ { - }$ (unless $x _ { 2 } ^ { - } = x _ { 1 }$ ). This edge, too, lies on $Q$ , by the lemma. Moreover, by (4) it is not incident with $x ^ { * }$ (since $x _ { 2 }$ is the dead end of $I$ , by assumption) and hence satisfies $( * * )$ . The case that $x _ { 1 }$ is the dead end of $I$ can be treated in the same way: using Lemma 10.3.2 (i), we replace in $W$ the subwalk $y _ { 2 } x _ { 2 } I x _ { 1 } I x _ { 2 }$ by a $y _ { 2 } - x _ { 2 }$ path $Q \subseteq G ^ { 2 }$ with $V ( \check { Q } ) = V ( \check { I } )$ , choose as $e ( v )$ for $v \in ( x _ { 1 } ^ { + } , x _ { 2 } )$ an edge of $Q$ bridging $\boldsymbol { v }$ , and define $e ( x _ { 1 } ^ { + } )$ as the edge to the right of $x _ { 1 } ^ { + }$ (unless $x _ { 1 } ^ { + } = x _ { 2 }$ ).

Suppose finally that $I$ is of type 3. Since $W$ traverses the edge $y _ { 1 } x _ { 1 }$ only once and the interval $I ^ { - }$ no more than twice, $W$ contains $y _ { 1 } x _ { 1 } I$ and $I ^ { - } \cup I$ as subpaths, and $I ^ { - }$ is of type 1. By (4), however, $I ^ { - } \ne I ^ { * }$ . Hence, when $e ( v )$ was defined for the vertices $v \in \breve { I } ^ { - }$ , the rightmost edge $x _ { 1 } ^ { - } x _ { 1 }$ of $I ^ { - }$ was not chosen as $e ( v )$ for any $v$ , so we may now replace this edge. Since $W$ traverses $I ^ { + }$ no more than twice, it must traverse the edge $x _ { 2 } y _ { 2 }$ immediately after one of its two subpaths $y _ { 1 } x _ { 1 } I$ and $x _ { 1 } ^ { - } x _ { 1 } I$ .

Take the starting vertex of this subpath ( $y _ { 1 }$ or $x _ { 1 } ^ { - }$ ) as the vertex $u$ in Lemma 10.3.2 (ii), and the other vertex in $\{ y _ { 1 } , x _ { 1 } ^ { - } \}$ as $v _ { 0 }$ ; moreover, set $v _ { k } : = \boldsymbol { \mathscr { x } } _ { 2 }$ and $w : = y _ { 2 }$ . Then the lemma enables us to replace these two subpaths of $W$ between $\{ y _ { 1 } , x _ { 1 } ^ { - } \}$ and $\left\{ { { x } _ { 2 } } , { y } _ { 2 } \right\}$ by disjoint paths in $G ^ { 2 }$ (Fig. 10.3.6), and furthermore assigns to every vertex $v \in \mathring { I }$ an edge $e ( v )$ of one of those paths, bridging $v$ .

![](images/14486e39eaf51862dd37de88642381bdfeaa01090bab2ebc36a6860bf0c50511.jpg)  
Fig. 10.3.6. A type 3 modification for the case $u = y _ { 1 }$ and $k$ odd

Following the above modifications, $W$ is now a closed walk in $\bar { G } ^ { 2 }$ . Let us check that, moreover, $W$ contains every vertex of $\ddot { G }$ exactly once. For vertices of the paths in $\mathcal { P } _ { 2 }$ this is clear, because $W$ still traverses every such path once and avoids it otherwise. For the vertices of $C - X _ { 2 }$ , it follows from the above modifications by Lemma 10.3.2. So how about the vertices in $X _ { 2 }$ ?

Let $x \in X _ { 2 }$ be given, and let $y$ be its neighbour on a path in $\mathcal { P } _ { 2 }$ . Let $I _ { 1 }$ denote the interval $I$ that satisfied $y x I \subseteq W$ before the modification of $W$ , and let $I _ { 2 }$ denote the other interval ending in $x$ . If $I _ { 1 }$ is of type 1, then $I _ { 2 }$ is of type 2 with dead end $x$ . In this case, $x$ was retained in $W$ when $W$ was modified on $I _ { 1 }$ but skipped when $W$ was modified on $I _ { 2 }$ , and is thus contained exactly once in $W$ now. If $I _ { 1 }$ is of type 2, then $x$ is not its dead end, and $I _ { 2 }$ is of type 1. The subwalk of $W$ that started with $y x$ and then went along $I _ { 1 }$ and back, was replaced with a $y$ – $_ { x }$ path. This path is now followed on $W$ by the unchanged interval $I _ { 2 }$ , so in this case too the vertex $x$ is now contained in $W$ exactly once. Finally, if $I _ { 1 }$ is of type 3, then $x$ was contained in one of the replacement paths $Q , Q ^ { \prime }$ from Lemma 10.3.2 (ii); as these paths were disjoint by the assertion of the lemma, $x$ is once more left on $W$ exactly once.

We have thus shown that $W$ , after the modifications, is a closed walk in $\hat { G } ^ { 2 }$ containing every vertex of $\ddot { G }$ exactly once, so $W$ defines a Hamilton cycle $\tilde { H }$ of $\hat { G } ^ { 2 }$ . Since $W$ still contains the path $Q ^ { * }$ , $\ddot { H }$ satisfies $( * )$ .

Up until now, we have assumed that $\mathcal { P } _ { 2 }$ is non-empty. If $\mathcal { P } _ { 2 } = \varnothing$ , let us set ${ \ddot { H } } : = { \ddot { G } } = C$ ; then, again, $\ddot { H }$ satisfies ( ). It remains to turn $\tilde { H }$ into a Hamilton cycle $H$ of $G ^ { 2 }$ by incorporating the paths from $\mathcal { P } _ { 1 }$ . In order to be able to treat the case of $\mathcal { P } _ { 2 } = \emptyset$ along with the case of $\mathcal { P } _ { 2 } \neq \emptyset$ , we define a map $v \mapsto e ( v )$ also when $\mathcal { P } _ { 2 } = \emptyset$ , as follows: for every $v \in C - y ^ { * }$ , set

$$
e (v) := \left\{ \begin{array}{l l} v v ^ {+} & \text {i f} v \in [ x ^ {*}, y ^ {*}) \\ v v ^ {-} & \text {i f} v \in (y ^ {*}, x ^ {*}). \end{array} \right.
$$

(Here, $[ \boldsymbol { x } ^ { * } , \boldsymbol { y } ^ { * } )$ and $( y ^ { * } , x ^ { * } )$ denote the obvious paths in $C$ defined analogously to intervals.) As before, this map $v \mapsto e ( v )$ is injective, satisfies $( * * )$ , and is defined on a superset of $X _ { 1 }$ ; recall that $y ^ { * }$ cannot lie in $X _ { 1 }$ by definition.

Let $P \in \mathcal { P } _ { 1 }$ be a path to be incorporated into $\ddot { H }$ , say with foot $v \in X _ { 1 }$ and ends $y _ { 1 } , y _ { 2 }$ . (If $| P | = 1$ , then $y _ { 1 } = y _ { 2 }$ .) Our aim is to replace the edge $e : = e ( v )$ in $\tilde { H }$ by $P$ ; we thus have to show that the ends of $P$ are joined to those of $e$ by suitable edges of $G ^ { 2 }$ .

By (2) and (3), $\boldsymbol { v }$ has only two neighbours in $\ddot { G }$ , its neighbours on $C$ . If is incident with , i.e. if with $i \in \{ 1 , 2 \}$ , we $x _ { 1 } , x _ { 2 }$ $\boldsymbol { v }$ $e$ $e = v x _ { i }$ replace $e$ by the path $v y _ { 1 } P y _ { 2 } x _ { i } \subseteq G ^ { 2 }$ (Fig. 10.3.7). If $v$ is not incident

![](images/424b4c273a9d14647b57b76c5136aa927ed94143b6ccdf189b249f05d99da037.jpg)  
Fig. 10.3.7. Replacing the edge $e$ in $\tilde { H }$

with $e$ then $e$ bridges $v$ , by $( * * )$ . Then $e ~ = ~ x _ { 1 } x _ { 2 }$ , and we replace $e$ by the path $x _ { 1 } y _ { 1 } P y _ { 2 } x _ { 2 } \subseteq G ^ { 2 }$ (Fig. 10.3.8). Since $v \mapsto e ( v )$ is injective on $X _ { 1 }$ , assertion (2) implies that all these modifications of $\tilde { H }$ (one for every $P \in \mathcal { P } _ { 1 }$ ) can be performed independently, and hence produce a Hamilton cycle $H$ of $G ^ { 2 }$ .

![](images/fe24e84429686922c42bccff696df1ba0e7943f8770572918494d92c925f29f5.jpg)  
Fig. 10.3.8. Replacing the edge $e$ in $\tilde { H }$

Let us finally check that $H$ satisfies ( ), i.e. that both edges of $H$ at $x ^ { * }$ lie in $G$ . Since ( ) holds for $\tilde { H }$ , it suffices to show that any edge $e = x ^ { \ast } z$ of $\tilde { H }$ that is not in $H$ (and hence has the form $e = e ( \boldsymbol { v } )$ for some $v \in X _ { 1 }$ ) was replaced by an $x ^ { * } { - } z$ path whose first edge lies in $G$ .

Where can the vertex $v$ lie? Let us show that $\boldsymbol { v }$ must be incident with $e$ . If not then $\mathcal { P } _ { 2 } \neq \emptyset$ , and $e$ bridges $v$ . Now $\mathcal { P } _ { 2 } \neq \emptyset$ and $v \in X _ { 1 }$ together imply that $\left| C - y ^ { * } \right| \geqslant \left| X _ { 1 } \right| + 2 \left| \mathcal { P } _ { 2 } \right| \geqslant 3$ , so $| C | \geqslant 4$ . As $e \in G$ (by ( ) for $\tilde { H }$ ), the fact that $e$ bridges $v$ thus contradicts (3).

So $\boldsymbol { v }$ is indeed incident with $e$ . Hence $v \in \{ x ^ { * } , z \}$ by definition of $e$ , while $e \neq v x ^ { * }$ by $( * * )$ . Thus $v = x ^ { * }$ , and $e$ was replaced by a path of the form $x ^ { \ast } y _ { 1 } P y _ { 2 } z$ . Since $x ^ { * } y _ { 1 }$ is an edge of $G$ , this replacement again preserves ( ). Therefore $H$ does indeed satisfy $( * )$ , and our induction is complete. 

Just like Tutte’s theorem (10.1.3), Fleischner’s theorem might extend to infinite graphs with circles:

Conjecture. The square of every 2-connected locally finite graph contains a Hamilton circle.

We close the chapter with a far-reaching conjecture generalizing Dirac’s theorem:

# Conjecture. (Seymour 1974)

Let $G$ be a graph of order $n \geqslant 3$ , and let $k$ be a positive integer. If $G$ has minimum degree

$$
\delta (G) \geqslant \frac {k}{k + 1} n,
$$

then $G$ has a Hamilton cycle $H$ such that $H ^ { k } \subseteq G$ .

For $k = 1$ , this is precisely Dirac’s theorem. The conjecture was proved for large enough $n$ (depending on $k$ ) by Koml´os, S´ark¨ozy and Szemer´edi (1998).

# Exercises

1. An oriented complete graph is called a tournament. Show that every tournament contains a (directed) Hamilton path.

2. Show that every uniquely 3-edge-colourable cubic graph is hamiltonian. (‘Unique’ means that all 3-edge-colourings induce the same edge partition.)

3. Given an even positive integer $k$ , construct for every $n \geqslant k$ a $k$ -regular graph of order $2 n + 1$ .

4.− Prove or disprove the following strengthening of Proposition 10.1.2: ‘Every $k$ -connected graph $G$ with $| G | \geqslant 3$ and $\chi ( G ) \geqslant | G | / k$ has a Hamilton cycle.’

5. $( \mathrm { i } ) ^ { - }$ Show that hamiltonian graphs are 1-tough.

(ii) Find a graph that is 1-tough but not hamiltonian.

6. Prove the toughness conjecture for planar graphs. Does it hold with $t = 2$ , or even with some $t < 2$ ?

7. $-$ Find a hamiltonian graph whose degree sequence is not hamiltonian.

8. $-$ Let $G$ be a graph with fewer than $_ i$ vertices of degree at most $_ i$ i, for every $i < | G | / 2$ . Use Chv´atal’s theorem to show that $G$ is hamiltonian. (Thus in particular, Chv´atal’s theorem implies Dirac’s theorem.)

9. Prove that the square $G ^ { 2 }$ of a $k$ -connected graph $G$ is $k$ -tough. Use this to deduce Fleischner’s theorem for graphs satisfying the toughness conjecture with $t = 2$ .   
10. Show that Exercise $5 ( \mathrm { i } )$ has the following weak converse: for every non-hamiltonian graph $G$ there exists a graph $G ^ { \prime }$ that has a pointwise greater degree-sequence than $G$ but is not 1-tough.   
11. Find a connected graph $G$ whose square $G ^ { 2 }$ has no Hamilton cycle.   
12.+ Show by induction on $| G |$ that the third power $G ^ { 3 }$ of a connected graph $G$ contains a Hamilton path between any two vertices. Deduce that $G ^ { 3 }$ s is hamiltonian.   
13. $^ +$ Let $G$ be a graph in which every vertex has odd degree. Show that every edge of $G$ lies on an even number of Hamilton cycles.

(Hint. Let $x y ~ \in ~ E ( G )$ be given. The Hamilton cycles through xy correspond to the Hamilton paths in $G - x y$ from $_ x$ to $_ y$ . Consider the set $\mathcal { H }$ of all Hamilton paths in $G - x y$ starting at $_ x$ , and show that an even number of these end in $_ y$ . To show this, define a graph on $\mathcal { H }$ so that the desired assertion follows from Proposition 1.2.1.)

# Notes

The problem of finding a Hamilton cycle in a graph has the same kind of origin as its Euler tour counterpart and the four colour problem: all three problems come from mathematical puzzles older than graph theory itself. What began as a game invented by W.R. Hamilton in 1857—in which ‘Hamilton cycles’ had to be found on the graph of the dodecahedron—reemerged over a hundred years later as a combinatorial optimization problem of prime importance: the travelling salesman problem. Here, a salesman has to visit a number of customers, and his problem is to arrange these in a suitable circular route. (For reasons not included in the mathematical brief, the route has to be such that after visiting a customer the salesman does not pass through that town again.) Much of the motivation for considering Hamilton cycles comes from variations of this algorithmic problem.

The lack of a good characterization of hamiltonicity also has to do with an algorithmic problem: deciding whether or not a given graph is hamiltonian is NP-hard (indeed, this was one of the early prototypes of an NP-complete decision problem), while the existence of a good characterization would place it in NP co-NP, which is widely believed to equal P. Thus, unless $\mathrm { P } = \mathrm { N P }$ , no good characterization of hamiltonicity exists. See the introduction to Chapter 12.5, or the end of the notes for Chapter 12, for more.

The ‘proof’ of the four colour theorem indicated at the end of Section 10.1, which is based on the (false) premise that every 3-connected cubic planar graph is hamiltonian, is usually attributed to the Scottish mathematician P.G. Tait. Following Kempe’s flawed proof of 1879 (see the notes for Chapter 5), it seems that Tait believed to be in possession of at least one ‘new proof of Kempe’s theorem’. However, when he addressed the Edinburgh Mathematical Society on

this subject in 1883, he seems to have been aware that he could not—really— prove the above statement about Hamilton cycles. His account in P.G. Tait, Listing’s topologie, Phil. Mag. 17 (1884), 30–46, makes some entertaining reading.

A shorter proof of Tutte’s theorem that 4-connected planar graphs are hamiltonian has been given by C. Thomassen, A theorem on paths in planar graphs, J. Graph Theory 7 (1983), 169–176. Tutte’s counterexample to Tait’s assumption that even 3-connectedness suffices (at least for cubic graphs) is shown in Bollob´as, and in J.A. Bondy & U.S.R. Murty, Graph Theory with Applications, Macmillan 1976 (where Tait’s attempted proof is discussed in some detail).

Bruhn’s conjecture generalizing Tutte’s theorem to infinite graphs was first stated in R. Diestel, The cycle space of an infinite graph, Combinatorics, Probability and Computing 14 (2005), 59–79. As the notion of a Hamilton circle is relatively recent, earlier generalizations of Hamilton cycle theorems asked for spanning double rays. Now a ray can pass through a finite separator only finitely often, so a necessary condition for the existence of a spanning ray or double ray is that the graph has at most one or two ends, respectively. Confirming a long-standing conjecture of Nash-Williams, X. Yu, Infinite paths in planar graphs I–III (preprints 2004) announced that a 4-connected planar graph with at most two ends contains a spanning double ray. N. Dean, R. Thomas and X Yu, Spanning paths in infinite planar graphs, J. Graph Theory 23 (1996), 163–174, proved Nash-Williams’s conjecture that a one-ended 4-connected planar graph has a spanning ray.

Proposition 10.1.2 is due to Chv´atal and Erd˝os (1972). The toughness invariant and conjecture were proposed by V. Chv´atal, Tough graphs and hamiltonian circuits, Discrete Math. 5 (1973), 215–228. If true with $t = 2$ , the conjecture would have implied Fleischner’s thereom; see Exercise 9. However, it was disproved for $t = 2$ by D. Bauer, H.J. Broersma & H.J. Veldman, Not every 2-tough graph is hamiltonian, Discrete Appl. Math. 99 (2000), 317– 321. Theorem 10.2.1 is due to V. Chv´atal, On Hamilton’s ideals, J. Combin. Theory B 12 (1972), 163–168.

Our proof of Fleischner’s theorem is based on S. R´ıha, A new proof of theˇ theorem by Fleischner, J. Combin. Theory B 52 (1991), 117–123. C. Thomassen, Hamiltonian paths in squares of infinite locally finite blocks, Ann. Discrete Math. 3 (1978), 269–277, proved that the square of every 2-connected oneended locally finite graph contains a spanning ray.

Seymour’s conjecture is from P.D. Seymour, Problem 3, in (T.P. McDonough and V.C. Mavron, eds.) Combinatorics, Cambridge University Press 1974. Its proof for large $_ n$ is due to J. Koml´os, G.N. S´ark¨ozy & E. Szemer´edi, Proof of the Seymour conjecture for large graphs, Ann. Comb. 2 (1998), 43–60.

At various points in this book, we already encountered the following fundamental theorem of Erd˝os: for every integer $k$ there is a graph $G$ with $g ( G ) > k$ and $\chi ( G ) > k$ . In plain English: there exist graphs combining arbitrarily large girth with arbitrarily high chromatic number.

How could one prove such a theorem? The standard approach would be to construct a graph with those two properties, possibly in steps by induction on $k$ . However, this is anything but straightforward: the global nature of the second property forced by the first, namely, that the graph should have high chromatic number ‘overall’ but be acyclic (and hence 2-colourable) locally, flies in the face of any attempt to build it up, constructively, from smaller pieces that have the same or similar properties.

In his pioneering paper of 1959, Erd˝os took a radically different approach: for each $n$ he defined a probability space on the set of graphs with $n$ vertices, and showed that, for some carefully chosen probability measures, the probability that an $n$ -vertex graph has both of the above properties is positive for all large enough $n$ .

This approach, now called the probabilistic method, has since unfolded into a sophisticated and versatile proof technique, in graph theory as much as in other branches of discrete mathematics. The theory of random graphs is now a subject in its own right. The aim of this chapter is to offer an elementary but rigorous introduction to random graphs: no more than is necessary to understand its basic concepts, ideas and techniques, but enough to give an inkling of the power and elegance hidden behind the calculations.

Erd˝os’s theorem asserts the existence of a graph with certain properties: it is a perfectly ordinary assertion showing no trace of the randomness employed in its proof. There are also results in random graphs that are generically random even in their statement: these are theorems about almost all graphs, a notion we shall meet in Section 11.3. In the

last section, we give a detailed proof of a theorem of Erd˝os and R´enyi that illustrates a proof technique frequently used in random graphs, the so-called second moment method.

# 11.1 The notion of a random graph

Let $V$ be a fixed set of $n$ elements, say $V = \{ 0 , \ldots , n - 1 \}$ . Our aim is to turn the set $\vec { \mathcal { G } }$ of all graphs on $V$ into a probability space, and then to consider the kind of questions typically asked about random objects: What is the probability that a graph $G \in { \mathcal { G } }$ has this or that property? What is the expected value of a given invariant on $G$ , say its expected girth or chromatic number?

Intuitively, we should be able to generate $G$ randomly as follows. For each $e \in [ V ] ^ { 2 }$ we decide by some random experiment whether or not $e$ shall be an edge of $G$ ; these experiments are performed independently, and for each the probability of success—i.e. of accepting $e$ as an edge for $G$ —is equal to some fixed1 number $p \in \mathbf { \Theta } \left[ 0 , 1 \right]$ . Then if $G _ { 0 }$ is some fixed graph on $V$ , with $m$ edges say, the elementary event $\{ G _ { 0 } \}$ has a probability of $p ^ { m } q ^ { ( n ) - m }$ (where $q : = 1 - p$ ): with this probability, our randomly generated graph $G$ is this particular graph $G _ { 0 }$ . (The probability that $G$ is isomorphic to $G _ { 0 }$ will usually be greater.) But if the probabilities of all the elementary events are thus determined, then so is the entire probability measure of our desired space $\mathcal { G }$ . Hence all that remains to be checked is that such a probability measure on $\vec { \mathcal { G } }$ , one for which all individual edges occur independently with probability $p$ , does indeed exist.2

In order to construct such a measure on $\vec { \mathcal { G } }$ formally, we start by defining for every potential edge $e \in [ V ] ^ { 2 }$ its own little probability space $\Omega _ { e } ~ : = ~ \{ 0 _ { e } , 1 _ { e } \}$ , choosing $P _ { e } ( \{ 1 _ { e } \} ) : = p$ and $P _ { e } ( \{ 0 _ { e } \} ) : = q$ as the probabilities of its two elementary events. As our desired probability space $\mathcal { G } = \mathcal { G } ( n , p )$ we then take the product space

$$
\Omega := \prod_ {e \in [ V ] ^ {2}} \Omega_ {e}.
$$

Thus, formally, an element of $\Omega$ is a map $\omega$ assigning to every $e \in [ V ] ^ { 2 }$ 2 either $0 _ { e }$ or $1 _ { e }$ , and the probability measure $P$ on $\Omega$ is the product measure of all the measures $P _ { e }$ . In practice, of course, we identify $\omega$ ， with the graph $G$ on $V$ whose edge set is

$$
E (G) = \left\{e \mid \omega (e) = 1 _ {e} \right\},
$$

and call $G$ a random graph on $V$ with edge probability $p$

Following standard probabilistic terminology, we may now call any set of graphs on $V$ an event in $\mathcal { G } ( n , p )$ . In particular, for every $e \in [ V ] ^ { 2 }$ the set

$$
A _ {e} := \left\{\omega \mid \omega (e) = 1 _ {e} \right\}
$$

of all graphs $G$ on $V$ with $e \in E ( G )$ is an event: the event that $e$ is an edge of $G$ . For these events, we can now prove formally what had been our guiding intuition all along:

Proposition 11.1.1. The events $A _ { e }$ are independent and occur with probability $p$ .

Proof . By definition,

$$
A _ {e} = \left\{1 _ {e} \right\} \times \prod_ {e ^ {\prime} \neq e} \Omega_ {e ^ {\prime}}.
$$

Since $P$ is the product measure of all the measures $P _ { e }$ , this implies

$$
P (A _ {e}) = p \cdot \prod_ {e ^ {\prime} \neq e} 1 = p.
$$

Similarly, if $\{ e _ { 1 } , \ldots , e _ { k } \}$ is any subset of $[ V ] ^ { 2 }$ , then

$$
\begin{array}{l} P \left(A _ {e _ {1}} \cap \dots \cap A _ {e _ {k}}\right) = P \left(\left\{1 _ {e _ {1}} \right\} \times \dots \times \left\{1 _ {e _ {k}} \right\} \times \prod \Omega_ {e}\right) \\ = p ^ {k} \quad e \notin \{e _ {1}, \dots , e _ {k} \} \\ = P \left(A _ {e _ {1}}\right) \dots P \left(A _ {e _ {k}}\right). \\ \end{array}
$$

![](images/99d65f921de72edc58e1d52a5a2e1dea11b252fa73b9bdbce9cea1f2a4d28268.jpg)

As noted before, $P$ is determined uniquely by the value of $p$ and our assumption that the events $A _ { e }$ are independent. In order to calculate probabilities in $\mathcal { G } ( n , p )$ , it therefore generally suffices to work with these two assumptions: our concrete model for $\mathcal { G } ( n , p )$ has served its purpose and will not be needed again.

As a simple example of such a calculation, consider the event that $G$ contains some fixed graph $H$ on a subset of $V$ as a subgraph; let $| H | = : k$ and $\| H \| = : \ell$ . The probability of this event $H \subseteq G$ is the product of the probabilities $A _ { e }$ over all the edges $e \in H$ , so $P \left[ H \subseteq G \right] = p ^ { \ell }$ . In

contrast, the probability that $H$ is an induced subgraph of $G$ is $p ^ { \ell } q ^ { \binom { k } { 2 } - \ell }$ : now the edges missing from $H$ are required to be missing from $G$ too, and they do so independently with probability $q$ .

The probability $P _ { H }$ that $G$ has an induced subgraph isomorphic to $H$ is usually more difficult to compute: since the possible instances of $H$ on subsets of $V$ overlap, the events that they occur in $G$ are not independent. However, the sum (over all $k$ -sets $U \subseteq V$ ) of the probabilities $P \left[ H \simeq G \left[ U \right] \right]$ is always an upper bound for $P _ { H }$ , since $P _ { H }$ is the measure of the union of all those events. For example, if $H = K ^ { k }$ , we have the following trivial upper bound on the probability that $G$ contains an induced copy of $H$ :

[ 11.2.1 ] [ 11.3.4 ]

Lemma 11.1.2. For all integers $n , k$ with $n \geqslant k \geqslant 2$ , the probability that $G \in { \mathcal { G } } ( n , p )$ has a set of $k$ independent vertices is at most

$$
P \left[ \alpha (G) \geqslant k \right] \leqslant \binom {n} {k} q ^ {\binom {k} {2}}.
$$

Proof . The probability that a fixed $k$ -set $U \subseteq V$ is independent in $G$ is $q ^ { \binom { k } { 2 } }$ ⊆. The assertion thus follows from the fact that there are only $\textstyle { \binom { n } { k } }$ such sets $U$ . 

Analogously, the probability that $G \in { \mathcal { G } } ( n , p )$ contains a $K ^ { k }$ is at most

$$
P \left[ \omega (G) \geqslant k \right] \leqslant \binom {n} {k} p ^ {\binom {k} {2}}.
$$

Now if $k$ is fixed, and $n$ is small enough that these bounds for the probabilities $P \left[ \alpha ( G ) \beta \ k \right]$ and $P \left[ \omega ( G ) \geq k \right]$ sum to less than 1, then $\mathcal { G }$ contains graphs that have neither property: graphs which contain neither a $K ^ { k }$ nor a $\overline { { K ^ { k } } }$ induced. But then any such $n$ is a lower bound for the Ramsey number of $k$ !

As the following theorem shows, this lower bound is quite close to the upper bound of $2 ^ { 2 k - 3 }$ implied by the proof of Theorem 9.1.1:

# Theorem 11.1.3. (Erd˝os 1947)

For every integer $k \geqslant 3$ , the Ramsey number of $k$ satisfies

$$
R (k) > 2 ^ {k / 2}.
$$

Proof . For $k = 3$ we trivially have $R ( 3 ) \geqslant 3 > 2 ^ { 3 / 2 }$ , so $\operatorname* { l e t } k \geqslant 4$ . We show that, for all $n \leqslant 2 ^ { k / 2 }$ and $G \in \mathcal { G } ( n , \textstyle \frac { 1 } { 2 } )$ , the probabilities $P \left[ \alpha ( G ) \beta \right]$ and $P \left[ \omega ( G ) \geqslant k \right]$ are both less than $\begin{array} { l } { { \frac { 1 } { 2 } } } \end{array}$ .

Since $\textstyle p = q = { \frac { 1 } { 2 } }$ , Lemma 11.1.2 and the analogous assertion for $\omega ( G )$ 2imply the following for all $n \leqslant 2 ^ { k / 2 }$ (use that $k ! > 2 ^ { k }$ for $k \geqslant 4$ ):

$$
\begin{array}{l} P \left[ \alpha (G) \geqslant k \right], P \left[ \omega (G) \geqslant k \right] \leqslant \binom {n} {k} \left(\frac {1}{2}\right) ^ {\binom {k} {2}} \\ <   \left(n ^ {k} / 2 ^ {k}\right) 2 ^ {- \frac {1}{2} k (k - 1)} \\ \leqslant \left(2 ^ {k ^ {2} / 2} / 2 ^ {k}\right) 2 ^ {- \frac {1}{2} k (k - 1)} \\ = 2 ^ {- k / 2} \\ <   \frac {1}{2}. \\ \end{array}
$$

![](images/0c31e00635f989357f4cde84a929cbad96a323904b8618024475e6c7b57f43e2.jpg)

In the context of random graphs, each of the familiar graph invariants (like average degree, connectivity, girth, chromatic number, and so on) may be interpreted as a non-negative random variable on $\mathcal { G } ( n , p )$ , a function

$$
X \colon \mathcal {G} (n, p) \to [ 0, \infty).
$$

The mean or expected value of $X$ is the number

$$
E (X) := \sum_ {G \in \mathcal {G} (n, p)} P (\{G \}) \cdot X (G).
$$

Note that the operator $E$ , the expectation, is linear: we have $E ( X + Y ) =$ $E ( X ) + E ( Y )$ and $E ( \lambda X ) = \lambda E ( X )$ for any two random variables $X , Y$ on $\mathcal { G } ( n , p )$ and $\lambda \in \mathbb R$ .

Computing the mean of a random variable $X$ can be a simple and effective way to establish the existence of a graph $G$ such that $X ( G ) < a$ for some fixed $a > 0$ and, moreover, $G$ has some desired property $\mathcal { P }$ . Indeed, if the expected value of $X$ is small, then $X ( G )$ cannot be large for more than a few graphs in $\mathcal { G } ( n , p )$ , because $X ( G ) \geqslant 0$ for all $G \in { \mathcal { G } } ( n , p )$ . Hence $X$ must be small for many graphs in $\mathcal { G } ( n , p )$ , and it is reasonable to expect that among these we may find one with the desired property $\mathcal { P }$ .

This simple idea lies at the heart of countless non-constructive existence proofs using random graphs, including the proof of Erd˝os’s theorem presented in the next section. Quantified, it takes the form of the following lemma, whose proof follows at once from the definition of the expectation and the additivity of $P$ :

# Lemma 11.1.4. (Markov’s Inequality)

Let $X \geqslant 0$ be a random variable on $\mathcal { G } ( n , p )$ and $a > 0$ . Then

random variable

mean

expectation

E(X)

[ 11.2.2 ] [ 11.4.1 ] [ 11.4.3 ]

$$
P [ X \geqslant a ] \leqslant E (X) / a.
$$

Proof .

$$
E (X) = \sum_ {G \in \mathcal {G} (n, p)} P (\{G \}) \cdot X (G)
$$

$$
\begin{array}{l} \geqslant \sum_{\substack{G\in \mathcal{G}(n,p)\\ X(G)\geqslant a}}P(\{G\})\cdot X(G) \\ \geqslant \sum_{\substack{G\in \mathcal{G}(n,p)\\ X(G)\geqslant a}}P(\{G\})\cdot a \\ = P [ X \geqslant a ] \cdot a. \\ \end{array}
$$

![](images/8c4720e6813fc9960963d4eb358d384b5672e60cf4a6cec463e80e8c666b4fdf.jpg)

Since our probability spaces are finite, the expectation can often be computed by a simple application of double counting, a standard combinatorial technique we met before in the proofs of Corollary 4.2.10 and Theorem 5.5.4. For example, if $X$ is a random variable on $\mathcal { G } ( n , p )$ that counts the number of subgraphs of $G$ in some fixed set $\mathcal { H }$ of graphs on $V$ , then $E ( X )$ , by definition, counts the number of pairs $( G , H )$ such that $H \in \mathcal { H }$ and $H \subseteq G$ , each weighted with the probability of $\{ G \}$ . Algorithmically, we compute $E ( X )$ by going through the graphs $G \in$ $\mathcal { G } ( n , p )$ in an ‘outer loop’ and performing, for each $G$ , an ‘inner loop’ that runs through the graphs $H \in \mathcal { H }$ and counts $\cdot P ( \{ G \} ) $ whenever $H \subseteq G$ . Alternatively, we may count the same set of weighted pairs with $H$ in the outer and $G$ in the inner loop: this amounts to adding up, over all $H \in \mathcal { H }$ , the probabilities $P \left[ H \subseteq G \right]$ .

To illustrate this once in detail, let us compute the expected number of cycles of some given length $k \geqslant 3$ in a random graph $G \in { \mathcal { G } } ( n , p )$ . So let $X \colon \mathcal { G } ( n , p ) \to \mathbb { N }$ be the random variable that assigns to every random graph $G$ its number of $k$ -cycles, the number of subgraphs isomorphic to $C ^ { k }$ . Let us write

$X$

$( n ) _ { k }$

$$
(n) _ {k} := n (n - 1) (n - 2) \dots (n - k + 1)
$$

for the number of sequences of $k$ distinct elements of a given $n$ -set.

[ 11.2.2 ] [ 11.4.3 ]

Lemma 11.1.5. The expected number of $k$ -cycles in $G \in { \mathcal { G } } ( n , p )$ is

$$
E (X) = \frac {(n) _ {k}}{2 k} p ^ {k}.
$$

Proof . For every $k$ -cycle $C$ with vertices in $V = \{ 0 , \ldots , n - 1 \}$ , the vertex set of the graphs in $\mathcal { G } ( n , p )$ , let $X _ { C } \colon \mathcal { G } ( n , p ) \to \{ 0 , 1 \}$ denote the indicator random variable of $C$ :

$$
X _ {C} \colon G \mapsto \left\{ \begin{array}{l l} 1 & \text {i f} C \subseteq G; \\ 0 & \text {o t h e r w i s e .} \end{array} \right.
$$

Since $X _ { C }$ takes only 1 as a positive value, its expectation $E ( X _ { C } )$ equals the measure $P \left[ X _ { C } = 1 \right]$ of the set of all graphs in $\mathcal { G } ( n , p )$ that contain $C$ . But this is just the probability that $C \subseteq G$ :

$$
E \left(X _ {C}\right) = P [ C \subseteq G ] = p ^ {k}. \tag {1}
$$

How many such cycles $C = v _ { 0 } \ldots v _ { k - 1 } v _ { 0 }$ are there? There are $( n ) _ { k }$ sequences $v _ { 0 } \ldots v _ { k - 1 }$ of distinct vertices in $V$ , and each cycle is identified by $2 k$ of those sequences—so there are exactly $( n ) _ { k } / 2 k$ such cycles.

Our random variable $X$ assigns to every graph $G$ its number of $k$ - cycles. Clearly, this is the sum of all the values $X _ { C } ( G )$ , where $C$ varies over the $( n ) _ { k } / 2 k$ cycles of length $k$ with vertices in $V$ :

$$
X = \sum_ {C} X _ {C}.
$$

Since the expectation is linear, (1) thus implies

$$
E (X) = E \left(\sum_ {C} X _ {C}\right) = \sum_ {C} E \left(X _ {C}\right) = \frac {(n) _ {k}}{2 k} p ^ {k}
$$

as claimed.

![](images/686b41394abac5b783f1c07eeea0a39730e3c834eda58f43cf97a93971d0a3cc.jpg)

# 11.2 The probabilistic method

Very roughly, the probabilistic method in discrete mathematics has developed from the following idea. In order to prove the existence of an object with some desired property, one defines a probability space on some larger—and certainly non-empty—class of objects, and then shows that an element of this space has the desired property with positive probability. The ‘objects’ inhabiting this probability space may be of any kind: partitions or orderings of the vertices of some fixed graph arise as naturally as mappings, embeddings and, of course, graphs themselves. In this section, we illustrate the probabilistic method by giving a detailed account of one of its earliest results: of Erd˝os’s classic theorem on large girth and chromatic number (Theorem 5.2.5).

Erd˝os’s theorem says that, given any positive integer $k$ , there is a graph $G$ with girth $g ( G ) > k$ and chromatic number $\chi ( G ) > k$ . Let us call cycles of length at most k short, and sets of $| G | / k$ or more vertices big. For a proof of Erd˝os’s theorem, it suffices to find a graph $G$ without short cycles and without big independent sets of vertices: then the colour classes in any vertex colouring of $G$ are small (not big), so we need more than $k$ colours to colour $G$ .

How can we find such a graph $G$ ? If we choose $p$ small enough, then a random graph in $\mathcal { G } ( n , p )$ is unlikely to contain any (short) cycles. If we choose $p$ large enough, then $G$ is unlikely to have big independent vertex sets. So the question is: do these two ranges of $p$ overlap, that is, can we choose $p$ so that, for some $n$ , it is both small enough to give $P \left[ g \leqslant k \right] < \frac { 1 } { 2 }$ and large enough for $\begin{array} { r } { P \left[ \alpha \geqslant n / k \right] < \frac { 1 } { 2 } \cdot } \end{array}$ ? If so, then

$\mathcal { G } ( n , p )$ will contain at least one graph without either short cycles or big independent sets.

Unfortunately, such a choice of $p$ is impossible: the two ranges of $p$ do not overlap! As we shall see in Section 11.4, we must keep $p$ below $n ^ { - 1 }$ to make the occurrence of short cycles in $G$ unlikely—but for any such $p$ there will most likely be no cycles in $G$ at all (Exercise 18), so $G$ will be bipartite and hence have at least $n / 2$ independent vertices.

But all is not lost. In order to make big independent sets unlikely, we shall fix $p$ above $n ^ { - 1 }$ , at $n ^ { \epsilon - 1 }$ for some $\epsilon > 0$ . Fortunately, though, if $\epsilon$ is small enough then this will produce only few short cycles in $G$ , even compared with $n$ (rather than, more typically, with $n ^ { k }$ ). If we then delete a vertex in each of those cycles, the graph $H$ obtained will have no short cycles, and its independence number $\alpha ( H )$ will be at most that of $G$ . Since $H$ is not much smaller than $G$ , its chromatic number will thus still be large, so we have found a graph with both large girth and large chromatic number.

To prepare for the formal proof of Erd˝os’s theorem, we first show that an edge probability of $p = n ^ { \epsilon - 1 }$ is indeed always large enough to ensure that $G \in { \mathcal { G } } ( n , p )$ ‘almost surely’ has no big independent set of vertices. More precisely, we prove the following slightly stronger assertion:

Lemma 11.2.1. Let $k > 0$ be an integer, and let $p = p ( n )$ be a function of $_ { n }$ such that $p \geqslant ( 6 k \ln n ) n ^ { - 1 }$ for $n$ large. Then

$$
\lim _ {n \to \infty} P [ \alpha \geqslant \frac {1}{2} n / k ] = 0.
$$

(11.1.2) Proof . For all integers $n , r$ with $n \geqslant r \geqslant 2$ , and all $G \in { \mathcal { G } } ( n , p )$ , Lemma 11.1.2 implies

$$
\begin{array}{l} P [ \alpha \geqslant r ] \leqslant \left( \begin{array}{c} n \\ r \end{array} \right) q ^ {\binom {r} {2}} \\ \leqslant n ^ {r} q ^ {\binom {r} {2}} \\ = \left(n q ^ {(r - 1) / 2}\right) ^ {r} \\ \leqslant \left(n e ^ {- p (r - 1) / 2}\right) ^ {r}; \\ \end{array}
$$

here, the last inequality follows from the fact that $1 - p \leqslant e ^ { - p }$ for all $p$ . (Compare the functions $x \mapsto e ^ { x }$ and $x \mapsto x + 1$ for $x \ = \ - p$ .) Now if $p \geqslant ( 6 k \ln n ) n ^ { - 1 }$ and $r \geqslant { \frac { 1 } { 2 } } n / k$ , then the term under the exponent satisfies

$$
\begin{array}{l} n e ^ {- p (r - 1) / 2} = n e ^ {- p r / 2 + p / 2} \\ \leqslant n e ^ {- (3 / 2) \ln n + p / 2} \\ \leqslant n n ^ {- 3 / 2} e ^ {1 / 2} \\ = \sqrt {e} / \sqrt {n} \underset {n \to \infty} {\longrightarrow} 0. \\ \end{array}
$$

Since $p \geqslant ( 6 k \ln n ) n ^ { - 1 }$ for $n$ large, we thus obtain for $r : = \lceil \frac { 1 } { 2 } n / k \rceil$

$$
\lim  _ {n \to \infty} P [ \alpha \geqslant \frac {1}{2} n / k ] = \lim  _ {n \to \infty} P [ \alpha \geqslant r ] = 0,
$$

as claimed.

We are now ready to prove Theorem 5.2.5, which we restate:

# Theorem 11.2.2. (Erd˝os 1959)

For every integer $k$ there exists a graph $H$ with girth $g ( H ) \ > \ k$ and chromatic number $\chi ( H ) > k$ .

Proof . Assume that $k \geqslant 3$ , fix $\epsilon$ with $0 < \epsilon < 1 / k$ , and let $p : = n ^ { \epsilon - 1 }$ . Let $X ( G )$ denote the number of short cycles in a random graph $G \in { \mathcal { G } } ( n , p )$ , i.e. its number of cycles of length at most $k$ .

By Lemma 11.1.5, we have

$$
E (X) = \sum_ {i = 3} ^ {k} \frac {(n) _ {i}}{2 i} p ^ {i} \leqslant \frac {1}{2} \sum_ {i = 3} ^ {k} n ^ {i} p ^ {i} \leqslant \frac {1}{2} (k - 2) n ^ {k} p ^ {k};
$$

note that $( n p ) ^ { i } \leqslant ( n p ) ^ { k }$ , because $n p = n ^ { \epsilon } \geqslant 1$ . By Lemma 11.1.4,

$$
\begin{array}{l} P [ X \geqslant n / 2 ] \leqslant E (X) / (n / 2) \\ \leqslant (k - 2) n ^ {k - 1} p ^ {k} \\ = (k - 2) n ^ {k - 1} n ^ {(\epsilon - 1) k} \\ = (k - 2) n ^ {k \epsilon - 1}. \\ \end{array}
$$

As $k \epsilon - 1 < 0$ by our choice of $\epsilon$ , this implies that

$$
\lim  _ {n \to \infty} P \left[ X \geqslant n / 2 \right] = 0.
$$

Let $n$ be large enough that $\begin{array} { r } { P \left[ X \geqslant n / 2 \right] < \frac { 1 } { 2 } } \end{array}$ and $\begin{array} { r } { P [ \alpha \geqslant \frac { 1 } { 2 } n / k ] < \frac { 1 } { 2 } } \end{array}$ ; the latter is possible by our choice of $p$ and Lemma 11.2.1. Then there is a graph $G \in \mathcal { G } ( n , p )$ with fewer than $n / 2$ short cycles and $\alpha ( G ) <$ ${ \scriptstyle { \frac { 1 } { 2 } } n / k }$ . From each of those cycles delete a vertex, and let $H$ be the graph obtained. Then $| H | \geqslant n / 2$ and $H$ has no short cycles, so $g ( H ) > k$ . By definition of $G$ ,

$$
\chi (H) \geqslant \frac {| H |}{\alpha (H)} \geqslant \frac {n / 2}{\alpha (G)} > k.
$$

Corollary 11.2.3. There are graphs with arbitrarily large girth and arbitrarily large values of the invariants $\kappa$ , $\varepsilon$ and $\delta$ .

Proof . Apply Corollary 5.2.3 and Theorem 1.4.3.

(1.4.3) (5.2.3)

# 11.3 Properties of almost all graphs

almost all etc.

Recall that a graph property is a class of graphs that is closed under isomorphism, one that contains with every graph $G$ also the graphs isomorphic to $G$ . If $p = p ( n )$ is a fixed function (possibly constant), and $\mathcal { P }$ is a graph property, we may ask how the probability $P \left[ G \in { \mathcal { P } } \right]$ behaves for $G \in { \mathcal { G } } ( n , p )$ as $n \longrightarrow \infty$ . If this probability tends to 1, we say that $G \in { \mathcal { P } }$ for almost all (or almost every) $G \in { \mathcal { G } } ( n , p )$ , or that $G \in { \mathcal { P } }$ almost surely; if it tends to 0, we say that almost no $G \in \mathcal { G } ( n , p )$ has the property $\mathcal { P }$ . (For example, in Lemma 11.2.1 we proved that, for a certain $p$ , almost no $G \in { \mathcal { G } } ( n , p )$ has a set of more than ${ \scriptstyle { \frac { 1 } { 2 } } n / k }$ independent vertices.)

To illustrate the new concept let us show that, for constant $p$ , every fixed abstract3 graph $H$ is an induced subgraph of almost all graphs:

Proposition 11.3.1. For every constant $p \in ( 0 , 1 )$ and every graph $H$ , almost every $G \in { \mathcal { G } } ( n , p )$ contains an induced copy of $H$ .

Proof . Let $H$ be given, and $k : = | H |$ . If $n \geqslant k$ and $U \subseteq \{ 0 , \ldots , n - 1 \}$ is a fixed set of $k$ vertices of $G$ , then $G [ U ]$ is isomorphic to $H$ with a certain probability $r > 0$ . This probability $r$ depends on $p$ , but not on $n$ (why not?). Now $G$ contains a collection of $\lfloor n / k \rfloor$ disjoint such sets $U$ . The probability that none of the corresponding graphs $G [ U ]$ is isomorphic to $H$ is $( 1 - r ) ^ { \lfloor n / k \rfloor }$ , since these events are independent by the disjointness of the edges sets $[ U ] ^ { 2 }$ . Thus

$$
P \left[ H \nsubseteq G \text {i n d u c e d} \right] \leqslant (1 - r) ^ {\lfloor n / k \rfloor} \underset {n \to \infty} {\longrightarrow} 0,
$$

which implies the assertion.

$\mathcal { P } _ { i , j }$

The following lemma is a simple device enabling us to deduce that quite a number of natural graph properties (including that of Proposition 11.3.1) are shared by almost all graphs. Given $i , j ~ \in \ \mathbb { N }$ , let $\mathcal { P } _ { i , j }$ denote the property that the graph considered contains, for any disjoint vertex sets $U , W$ with $| U | \leqslant i$ and $| W | \leqslant j$ , a vertex $v \not \in U \cup W$ that is adjacent to all the vertices in $U$ but to none in $W$ .

Lemma 11.3.2. For every constant $p \in ( 0 , 1 )$ and $i , j \in  { \mathbb { N } }$ , almost every graph $G \in { \mathcal { G } } ( n , p )$ has the property $\mathcal { P } _ { i , j }$ .

Proof . For fixed $U , W$ and $v \in G - ( U \cup W )$ , the probability that $v$ is adjacent to all the vertices in $U$ but to none in $W$ , is

$$
p ^ {| U |} q ^ {| W |} \geqslant p ^ {i} q ^ {j}.
$$

Hence, the probability that no suitable $v$ exists for these $U$ and $W$ , is

$$
\left(1 - p ^ {| U |} q ^ {| W |}\right) ^ {n - | U | - | W |} \leqslant \left(1 - p ^ {i} q ^ {j}\right) ^ {n - i - j}
$$

(for $\textit { n } \geqslant \textit { i } + \textit { j }$ ), since the corresponding events are independent for different $v$ . As there are no more than $n ^ { i + j }$ pairs of such sets $U , W$ in $V ( G )$ (encode sets $U$ of fewer than $i$ points as non-injective maps $\{ 0 , \dots , i - 1 \} \to \{ 0 , \dots , n - 1 \}$ , etc.), the probability that some such pair has no suitable $v$ is at most

$$
n ^ {i + j} (1 - p ^ {i} q ^ {j}) ^ {n - i - j},
$$

which tends to zero as $n \longrightarrow \infty$ since $1 - p ^ { i } q ^ { j } < 1$ .

Corollary 11.3.3. For every constant $p \in ( 0 , 1 )$ and $k \in \mathbb N$ , almost every graph in $\mathcal { G } ( n , p )$ is $k$ -connected.

Proof . By Lemma 11.3.2, it is enough to show that every graph in $\mathcal { P } _ { 2 , k - 1 }$ is $k$ -connected. But this is easy: any graph in $\mathcal { P } _ { 2 , k - 1 }$ has order at least $k + 2$ , and if $W$ is a set of fewer than $k$ vertices, then by definition of $\mathcal { P } _ { 2 , k - 1 }$ any other two vertices $x , y$ have a common neighbour $v \not \in W$ ; in particular, $W$ does not separate $x$ from $y$ . 

In the proof of Corollary 11.3.3, we showed substantially more than was asked for: rather than finding, for any two vertices $x , y \notin W$ , some $x$ – $y$ path avoiding $W$ , we showed that $x$ and $y$ have a common neighbour outside $W$ ; thus, all the paths needed to establish the desired connectivity could in fact be chosen of length 2. What seemed like a clever trick in this particular proof is in fact indicative of a more fundamental phenomenon for constant edge probabilities: by an easy result in logic, any statement about graphs expressed by quantifying over vertices only (rather than over sets or sequences of vertices) $^ 4$ is either almost surely true or almost surely false. All such statements, or their negations, are in fact immediate consequences of an assertion that the graph has property $\mathcal { P } _ { i , j }$ , for some suitable $i , j$ .

As a last example of an ‘almost all’ result we now show that almost every graph has a surprisingly high chromatic number:

Proposition 11.3.4. For every constant $p \in \mathsf { \Gamma } ( 0 , 1 )$ and every $\epsilon > 0$ , almost every graph $G \in { \mathcal { G } } ( n , p )$ has chromatic number

$$
\chi (G) > \frac {\log (1 / q)}{2 + \epsilon} \cdot \frac {n}{\log n}.
$$

(11.1.2) Proof . For any fixed $n \geqslant k \geqslant 2$ , Lemma 11.1.2 implies

$$
\begin{array}{l} P \left[ \alpha \geqslant k \right] \leqslant \binom {n} {k} q ^ {\binom {k} {2}} \\ \leqslant n ^ {k} q ^ {\binom {k} {2}} \\ = q ^ {k \frac {\log n}{\log q} + \frac {1}{2} k (k - 1)} \\ = q ^ {\frac {k}{2} \left(- \frac {2 \log n}{\log (1 / q)} + k - 1\right)}. \\ \end{array}
$$

For

$$
k := (2 + \epsilon) \frac {\log n}{\log (1 / q)}
$$

the exponent of this expression tends to infinity with $n$ , so the expression itself tends to zero. Hence, almost every $G \in { \mathcal { G } } ( n , p )$ is such that in any vertex colouring of $G$ no $k$ vertices can have the same colour, so every colouring uses more than

$$
{\frac {n}{k}} = {\frac {\log (1 / q)}{2 + \epsilon}} \cdot {\frac {n}{\log n}}
$$

colours.

By a result of Bollob´as (1988), Proposition 11.3.4 is sharp in the following sense: if we replace $\epsilon$ by $- \epsilon$ , then the lower bound given for $\chi$ turns into an upper bound.

Most of the results of this section have the interesting common feature that the values of $p$ played no role whatsoever: if almost every graph in $\mathcal { G } ( n , \textstyle \frac { 1 } { 2 } )$ had the property considered, then the same was true for almost every graph in $\mathcal { G } ( n , 1 / 1 0 0 0 )$ . How could this happen?

Such insensitivity of our random model to changes of $p$ was certainly not intended: after all, among all the graphs with a certain property $\mathcal { P }$ it is often those having $\mathcal { P }$ ‘only just’ that are the most interesting—for those graphs are most likely to have different properties too, properties to which $\mathcal { P }$ might thus be set in relation. (The proof of Erd˝os’s theorem is a good example.) For most properties, however—and this explains the above phenomenon—the critical order of magnitude of $p$ around which the property will ‘just’ occur or not occur lies far below any constant value of $p$ : it is typically a function of $n$ tending to zero as $n \longrightarrow \infty$ .

Let us then see what happens if $p$ is allowed to vary with $n$ . Almost immediately, a fascinating picture unfolds. For edge probabilities $p$ whose order of magnitude lies below $n ^ { - 2 }$ , a random graph $G \in { \mathcal { G } } ( n , p )$ almost surely has no edges at all. As $p$ grows, $G$ acquires more and more structure: from about $p = { \sqrt { n } } n ^ { - 2 }$ onwards, it almost surely has a component with more than two vertices, these components grow into trees, and around $p = n ^ { - 1 }$ the first cycles are born. Soon, some of these will have several crossing chords, making the graph non-planar. At the same time, one component outgrows the others, until it devours them around $p = ( \log n ) n ^ { - 1 }$ , making the graph connected. Hardly later, at $p = ( 1 + \epsilon ) ( \log n ) n ^ { - 1 }$ , our graph almost surely has a Hamilton cycle!

It has become customary to compare this development of random graphs as $p$ grows to the evolution of an organism: for each $p = p ( n )$ , one thinks of the properties shared by almost all graphs in $\mathcal { G } ( n , p )$ as properties of ‘the’ typical random graph $G \in \mathcal { G } ( n , p )$ , and studies how $G$ changes its features with the growth rate of $p$ . As with other species, the evolution of random graphs happens in relatively sudden jumps: the critical edge probabilities mentioned above are thresholds below which almost no graph and above which almost every graph has the property considered. More precisely, we call a real function $t = t ( n )$ with $t ( n ) \neq 0$ for all $n$ a threshold function for a graph property $\mathcal { P }$ if the following holds for all $p = p ( n )$ , and $G \in { \mathcal { G } } ( n , p )$ :

$$
\lim  _ {n \to \infty} P [ G \in \mathcal {P} ] = \left\{ \begin{array}{l l} 0 & \text {i f} p / t \to 0 \text {a s} n \to \infty \\ 1 & \text {i f} p / t \to \infty \text {a s} n \to \infty . \end{array} \right.
$$

If $\mathcal { P }$ has a threshold function $t$ , then clearly any positive multiple $c t$ of $t$ is also a threshold function for $\mathcal { P }$ ; thus, threshold functions in the above sense are only ever unique up to a multiplicative constant.5

Which graph properties have threshold functions? Natural candidates for such properties are increasing ones, properties closed under the addition of edges. (Graph properties of the form $\{ G \mid G \supseteq H \}$ , with $H$ fixed, are common increasing properties; connectedness is another.) And indeed, Bollob´as & Thomason (1987) have shown that all increasing properties, trivial exceptions aside, have threshold functions.

In the next section we shall study a general method to compute threshold functions.

We finish this section with a little gem, the one and only theorem about infinite random graphs. Let $\mathcal { G } ( \aleph _ { 0 } , p )$ be defined exactly like $\mathcal { G } ( n , p )$ for $n = \aleph _ { 0 }$ , as the (product) space of random graphs on $\mathbb { N }$ whose edges are chosen independently with probability $p$ .

As we saw in Lemma 11.3.2, the properties $\mathcal { P } _ { i , j }$ hold almost surely for finite random graphs with constant edge probability. It will therefore hardly come as a surprise that an infinite random graph almost surely (which now has the usual meaning of ‘with probability 1’) has all these properties at once. However, in Chapter 8.3 we saw that, up to isomorphism, there is exactly one countable graph, the Rado graph $R$ , that has property $\mathcal { P } _ { i , j }$ for all $i , j \in \mathbb { N }$ simultaneously; this joint property was denoted as ( $^ *$ ) there. Combining these facts, we get the following rather bizarre result:

# Theorem 11.3.5. (Erd˝os and R´enyi 1963)

With probability 1, a random graph $G \in \mathcal { G } ( \aleph _ { 0 } , p )$ with $0 \textless p \textless 1$ i s isomorphic to the Rado graph $R$ .

(8.3.1) Proof . Given fixed disjoint finite sets $U , W \subseteq \mathbb { N }$ , the probability that a vertex $v \not \in U \cup W$ is not joined to $U \cup W$ as expressed in property ( ) of Chapter 8.3 (i.e., is not joined to all of $U$ or is joined to some vertex in $W$ ) is some number $r \ < \ 1$ depending only on $U$ and $W$ . The probability that none of $k$ given vertices $\boldsymbol { v }$ is joined to $U \cup W$ as in $( * )$ is $r ^ { k }$ , which tends to 0 as $k \to \infty$ . Hence the probability that all the (infinitely many) vertices outside $U \cup W$ fail to witness ( ) for these sets $U$ and $W$ is 0.

Now there are only countably many choices for $U$ and $W$ as above. Since the union of countably many sets of measure 0 again has measure 0, the probability that ( ) fails for any sets $U$ and $W$ is still 0. Therefore $G$ satisfies ( ) with probability 1. By Theorem 8.3.1 this means that, almost surely, $G \simeq R$ . 

How can we make sense of the paradox that the result of infinitely many independent choices can be so predictable? The answer, of course, lies in the fact that the uniqueness of $R$ holds only up to isomorphism. Now, constructing an automorphism for an infinite graph with property ( ) is a much easier task than finding one for a finite random graph, so in this sense the uniqueness is no longer that surprising. Viewed in this way, Theorem 11.3.5 expresses not a lack of variety in infinite random graphs but rather the abundance of symmetry that glosses over this variety when the graphs $G \in \mathcal { G } ( \aleph _ { 0 } , p )$ are viewed only up to isomorphism.

# 11.4 Threshold functions and second moments

Consider a graph property of the form

$$
\mathcal {P} = \left\{G \mid X (G) \geqslant 1 \right\},
$$

$X \geqslant 0$ where $X \geqslant 0$ is a random variable on $\mathcal { G } ( n , p )$ . Many properties can be expressed naturally in this way; if $X$ denotes the number of spanning

How could we prove that $\mathcal { P }$ has a threshold function $t$ ? Any such proof will consist of two parts: a proof that almost no $G \in \mathcal { G } ( n , p )$ has $\mathcal { P }$ when $p$ is small compared with $t$ , and one showing that almost every $G$ has $\mathcal { P }$ when $p$ is large.

Since $X \geqslant 0$ , we may use Markov’s inequality for the first part of the proof and find an upper bound for $E ( X )$ instead of $P \left[ X \geqslant 1 \right]$ : if $E ( X )$ is much smaller than 1 then $X ( G )$ can be at least 1 only for few $G \in { \mathcal { G } } ( n , p )$ , and for almost no $G$ if $E ( X ) \to 0$ as $n \longrightarrow \infty$ . Besides, the expectation is much easier to calculate than probabilities: without worrying about such things as independence or incompatibility of events, we may compute the expectation of a sum of random variables—for example, of indicator random variables—simply by adding up their individual expected values.

For the second part of the proof, things are more complicated. In order to show that $P \left[ X \geqslant 1 \right]$ is large, it is not enough to bound $E ( X )$ from below: since $X$ is not bounded above, $E ( X )$ may be large simply because $X$ is very large on just a few graphs $G$ —so $X$ may still be zero for most $G \in \mathcal { G } ( n , p )$ .6 In order to prove that $P [ X \geqslant 1 ]  1$ , we thus have to show that this cannot happen, i.e. that $X$ does not deviate a lot from its mean too often.

The following elementary tool from probability theory achieves just that. As is customary, we write

$$
\mu := E (X)
$$

and define $\sigma \geqslant 0$ by setting

$$
\sigma^ {2} := E \left(\left(X - \mu\right) ^ {2}\right).
$$

This quantity $\sigma ^ { 2 }$ is called the variance or second moment of $X$ ; by definition, it is a (quadratic) measure of how much $X$ deviates from its mean. Since $E$ is linear, the defining term for $\sigma ^ { 2 }$ expands to

$$
\sigma^ {2} = E (X ^ {2} - 2 \mu X + \mu^ {2}) = E (X ^ {2}) - \mu^ {2}.
$$

Note that $\mu$ and $\sigma ^ { 2 }$ always refer to a random variable on some fixed probability space. In our setting, where we consider the spaces $\mathcal { G } ( n , p )$ , both quantities are functions of $n$ .

The following lemma says exactly what we need: that $X$ cannot deviate a lot from its mean too often.

$\mu$

$\sigma ^ { 2 }$

Lemma 11.4.1. (Chebyshev’s Inequality)

For all real $\lambda > 0$ ,

$$
P \left[ | X - \mu | \geqslant \lambda \right] \leqslant \sigma^ {2} / \lambda^ {2}.
$$

(11.1.4) Proof . By Lemma 11.1.4 and definition of $\sigma ^ { 2 }$ ,

$$
P \big [ | X - \mu | \geqslant \lambda \big ] = P \big [ (X - \mu) ^ {2} \geqslant \lambda^ {2} \big ] \leqslant \sigma^ {2} / \lambda^ {2}.
$$

![](images/25212df7dd3f24e1ba496b4950c9e930106ccb7501b9e63d95fb0de866e18b2d.jpg)

For a proof that $X ( G ) \geqslant 1$ for almost all $G \in { \mathcal { G } } ( n , p )$ , Chebyshev’s inequality can be used as follows:

Lemma 11.4.2. If $\mu > 0$ for $n$ large, and $\sigma ^ { 2 } / \mu ^ { 2 } \to 0$ as $n \longrightarrow \infty$ , then $X ( G ) > 0$ for almost all $G \in { \mathcal { G } } ( n , p )$ .

Proof . Any graph $G$ with $X ( G ) = 0$ satisfies $| X ( G ) - \mu | = \mu$ . Hence Lemma 11.4.1 implies with $\lambda : = \mu$ that

$$
P \left[ X = 0 \right] \leqslant P \left[ | X - \mu | \geqslant \mu \right] \leqslant \sigma^ {2} / \mu^ {2} \underset {n \rightarrow \infty} {\longrightarrow} 0.
$$

Since $X \geqslant 0$ , this means that $X > 0$ almost surely, i.e. that $X ( G ) > 0$ for almost all $G \in { \mathcal { G } } ( n , p )$ . 

$\mathcal { P } _ { H }$ balanced

As the main result of this section, we now prove a theorem that will at once give us threshold functions for a number of natural properties. Given a graph $H$ , we denote by $\mathcal { P } _ { H }$ the graph property of containing a copy of $H$ as a subgraph. We shall call $H$ balanced if $\varepsilon ( H ^ { \prime } ) \leqslant \varepsilon ( H )$ for all subgraphs $H ^ { \prime }$ of $H$ .

Theorem 11.4.3. (Erd˝os & R´enyi 1960)

$k , \ell$ If $H$ is a balanced graph with $k$ vertices and $\ell \geqslant 1$ edges, then $t ( n ) : =$ t $n ^ { - k / \ell }$ is a threshold function for $\mathcal { P } _ { H }$ .

(11.1.4) Proof . Let $X ( G )$ denote the number of subgraphs of $G$ isomorphic to $H$ . (11.1.5) Given $n \in \mathbb { N }$ , let $\mathcal { H }$ denote the set of all graphs isomorphic to $H$ whose $X$ vertices lie in $\{ 0 , \ldots , n - 1 \}$ , the vertex set of the graphs $G \in { \mathcal { G } } ( n , p )$ :

$\mathcal { H }$ $\mathcal { H } : = \left\{ H ^ { \prime } \mid H ^ { \prime } \simeq H , { \cal V } ( H ^ { \prime } ) \subseteq \left\{ 0 , \ldots , n - 1 \right\} \right\} .$

Given $H ^ { \prime } \in \mathcal { H }$ and $G \in \mathcal { G } ( n , p )$ , we shall write $H ^ { \prime } \subseteq G$ to express that $H ^ { \prime }$ itself—not just an isomorphic copy of $H ^ { \prime }$ —is a subgraph of $G$ .

By $h$ we denote the number of isomorphic copies of $H$ on a fixed $k$ -set; clearly, $h \leqslant k ! .$ . As there are $\textstyle { \binom { n } { k } }$ possible vertex sets for the graphs in $\mathcal { H }$ , we thus have

$$
\left| \mathcal {H} \right| = \binom {n} {k} h \leqslant \binom {n} {k} k! \leqslant n ^ {k}. \tag {1}
$$

Given $p = p ( n )$ , we set $\gamma : = p / t$ ; then

$$
p = \gamma n ^ {- k / \ell}. \tag {2}
$$

We have to show that almost no $G \in \mathcal { G } ( n , p )$ lies in $\mathcal { P } _ { H }$ if $\gamma \longrightarrow 0$ as $n \longrightarrow \infty$ and that almost all $G \in { \mathcal { G } } ( n , p )$ lie in $\mathcal { P } _ { H }$ if $\gamma \longrightarrow \infty$ as $n \longrightarrow \infty$ .

For the first part of the proof, we find an upper bound for $E ( X )$ , the expected number of subgraphs of $G$ isomorphic to $H$ . As in the proof of Lemma 11.1.5, double counting gives

$$
E (X) = \sum_ {H ^ {\prime} \in \mathcal {H}} P \left[ H ^ {\prime} \subseteq G \right]. \tag {3}
$$

For every fixed $H ^ { \prime } \in \mathcal { H }$ , we have

$$
P [ H ^ {\prime} \subseteq G ] = p ^ {\ell}, \tag {4}
$$

because $\| H \| = \ell$ . Hence,

$$
E (X) \underset {(3, 4)} {=} | \mathcal {H} | p ^ {\ell} \underset {(1, 2)} {\leqslant} n ^ {k} (\gamma n ^ {- k / \ell}) ^ {\ell} = \gamma^ {\ell}. \tag {5}
$$

Thus if $\gamma \longrightarrow 0$ as $n \longrightarrow \infty$ , then

$$
P \left[ G \in \mathcal {P} _ {H} \right] = P \left[ X \geqslant 1 \right] \leqslant E (X) \leqslant \gamma^ {\ell} \underset {n \rightarrow \infty} {\longrightarrow} 0
$$

by Markov’s inequality (11.1.4), so almost no $G \in { \mathcal { G } } ( n , p )$ lies in $\mathcal { P } _ { H }$

We now come to the second part of the proof: we show that almost all $G \in { \mathcal { G } } ( n , p )$ lie in $\mathcal { P } _ { H }$ if $\gamma \longrightarrow \infty$ as $n \longrightarrow \infty$ . Note first that, for $n \geqslant k$ ,

$$
\begin{array}{l} \binom {n} {k} n ^ {- k} = \frac {1}{k !} \left(\frac {n}{n} \dots \frac {n - k + 1}{n}\right) \\ \geqslant \frac {1}{k !} \left(\frac {n - k + 1}{n}\right) ^ {k} \\ \geqslant \frac {1}{k !} \left(1 - \frac {k - 1}{k}\right) ^ {k}; \tag {6} \\ \end{array}
$$

thus, $n ^ { k }$ exceeds $\binom { n } { k }$ by no more than a factor independent of $n$

Our goal is to apply Lemma 11.4.2, and hence to bound $\sigma ^ { 2 } / \mu ^ { 2 } =$ $\left( E ( X ^ { 2 } ) - \mu ^ { 2 } \right) / \mu ^ { 2 }$ from above. As in (3) we have

$$
E \left(X ^ {2}\right) = \sum_ {\left(H ^ {\prime}, H ^ {\prime \prime}\right) \in \mathcal {H} ^ {2}} P \left[ H ^ {\prime} \cup H ^ {\prime \prime} \subseteq G \right]. \tag {7}
$$

Let us then calculate these probabilities $P \left[ H ^ { \prime } \cup H ^ { \prime \prime } \subseteq G \right.$ ]. Given $H ^ { \prime } , H ^ { \prime \prime } \in \mathcal { H }$ , we have

$$
P \left[ H ^ {\prime} \cup H ^ {\prime \prime} \subseteq G \right] = p ^ {2 \ell - \| H ^ {\prime} \cap H ^ {\prime \prime} \|}.
$$

i

Since $H$ is balanced, $\varepsilon ( H ^ { \prime } \cap H ^ { \prime \prime } ) \leqslant \varepsilon ( H ) = \ell / k$ . With $| H ^ { \prime } \cap H ^ { \prime \prime } | = : i$ this yields $\| H ^ { \prime } \cap H ^ { \prime \prime } \| \leqslant i \ell / k$ , so by $0 \leqslant p \leqslant 1$ ,

$$
P \left[ H ^ {\prime} \cup H ^ {\prime \prime} \subseteq G \right] \leqslant p ^ {2 \ell - i \ell / k}. \tag {8}
$$

We have now estimated the individual summands in (7); what does this imply for the sum as a whole? Since (8) depends on the parameter $i = | H ^ { \prime } \cap H ^ { \prime \prime } |$ , we partition the range $\mathcal { H } ^ { 2 }$ of the sum in (7) into the subsets

$\mathcal { H } _ { i } ^ { 2 }$

$$
\mathcal {H} _ {i} ^ {2} := \left\{\left(H ^ {\prime}, H ^ {\prime \prime}\right) \in \mathcal {H} ^ {2}: | H ^ {\prime} \cap H ^ {\prime \prime} | = i \right\}, \qquad i = 0, \ldots , k,
$$

and calculate for each $\mathcal { H } _ { i } ^ { 2 }$ the corresponding sum

$A _ { i }$

$$
A _ {i} := \sum_ {i} P \left[ H ^ {\prime} \cup H ^ {\prime \prime} \subseteq G \right]
$$

$\textstyle \sum _ { i }$

by itself. (Here, as below, we use $\textstyle \sum _ { i }$ to denote sums over all pairs $( H ^ { \prime } , H ^ { \prime \prime } ) \in \mathcal { H } _ { i } ^ { 2 }$ . )

If $i = 0$ then $H ^ { \prime }$ and $H ^ { \prime \prime }$ are disjoint, so the events $H ^ { \prime } \subseteq G$ and $H ^ { \prime \prime } \subseteq G$ are independent. Hence,

$$
\begin{array}{l} A _ {0} = \sum_ {0} P \left[ H ^ {\prime} \cup H ^ {\prime \prime} \subseteq G \right] \\ = \sum_ {0} P [ H ^ {\prime} \subseteq G ] \cdot P [ H ^ {\prime \prime} \subseteq G ] \\ \leqslant \sum_ {\left(H ^ {\prime}, H ^ {\prime \prime}\right) \in \mathcal {H} ^ {2}} P \left[ H ^ {\prime} \subseteq G \right] \cdot P \left[ H ^ {\prime \prime} \subseteq G \right] \\ = \Big (\sum_ {H ^ {\prime} \in \mathcal {H}} P [ H ^ {\prime} \subseteq G ] \Big) \cdot \Big (\sum_ {H ^ {\prime \prime} \in \mathcal {H}} P [ H ^ {\prime \prime} \subseteq G ] \Big) \\ \stackrel {=} {(3)} \mu^ {2}. \tag {9} \\ \end{array}
$$



for Let us now estimate $\textstyle \sum _ { H ^ { \prime \prime } \in { \mathcal { H } } }$ , we note that $A _ { i }$ $\textstyle \sum _ { i }$ for T $i \geqslant 1$ . Writing e written as $\Sigma ^ { \prime }$ $\sum ^ { \prime } \sum _ { | H ^ { \prime } \cap H ^ { \prime \prime } | = i } ^ { \prime \prime }$ for $\sum _ { H ^ { \prime } \in \mathcal { H } }$ and . For $\sum ^ { \prime \prime }$ fixed $H ^ { \prime }$ (corresponding to the first sum $\sum ^ { \prime }$ ), the second sum ranges over

$$
\left( \begin{array}{c} k \\ i \end{array} \right) \left( \begin{array}{c} n - k \\ k - i \end{array} \right) h
$$

summands: the number of graphs $H ^ { \prime \prime } \in \mathcal { H }$ with $| H ^ { \prime \prime } \cap H ^ { \prime } | = i$ . Hence, for all $i \geqslant 1$ and suitable constants $c _ { 1 } , c _ { 2 }$ independent of $n$ ,

$$
\begin{array}{l} A _ {i} = \sum_ {i} P \left[ H ^ {\prime} \cup H ^ {\prime \prime} \subseteq G \right] \\ \leqslant \sum^ {\prime} \binom {k} {i} \binom {n - k} {k - i} h p ^ {2 \ell} p ^ {- i \ell / k} (8) \\ \equiv \mathbf {\Phi} _ {(2)} | \mathcal {H} | \binom {k} {i} \binom {n - k} {k - i} h p ^ {2 \ell} \left(\gamma n ^ {- k / \ell}\right) ^ {- i \ell / k} \\ \leqslant | \mathcal {H} | p ^ {\ell} c _ {1} n ^ {k - i} h p ^ {\ell} \gamma^ {- i \ell / k} n ^ {i} \\ \underset {(5)} {=} \mu c _ {1} n ^ {k} h p ^ {\ell} \gamma^ {- i \ell / k} \\ \leqslant \mu c _ {2} \binom {n} {k} h p ^ {\ell} \gamma^ {- i \ell / k} (6) \\ \underset {(1, 5)} {=} \mu^ {2} c _ {2} \gamma^ {- i \ell / k} \\ \leqslant \mu^ {2} c _ {2} \gamma^ {- \ell / k} \\ \end{array}
$$

if $\gamma \geqslant 1$ . By definition of the $A _ { i }$ , this implies with $c _ { 3 } : = k c _ { 2 }$ that

$$
E (X ^ {2}) / \mu^ {2} \underset {(7)} {=} \left(A _ {0} / \mu^ {2} + \sum_ {i = 1} ^ {k} A _ {i} / \mu^ {2}\right) \underset {(9)} {\leqslant} 1 + c _ {3} \gamma^ {- \ell / k}
$$

and hence

$$
\frac{\sigma^{2}}{\mu^{2}} = \frac{E(X^{2}) - \mu^{2}}{\mu^{2}}\leqslant c_{3}\gamma^{-\ell /k} \underset {\gamma \to \infty}{\longrightarrow}0.
$$

By Lemma 11.4.2, therefore, $X ~ > ~ 0$ almost surely, i.e. almost all $G \in$ $\mathcal { G } ( n , p )$ have a subgraph isomorphic to $H$ and hence lie in $\mathcal { P } _ { H }$ . 

Theorem 11.4.3 allows us to read off threshold functions for a number of natural graph properties.

Corollary 11.4.4. If $k \geqslant 3$ , then $t ( n ) = n ^ { - 1 }$ is a threshold function for the property of containing a $k$ -cycle. 

Interestingly, the threshold function in Corollary 11.4.4 is independent of the cycle length $k$ considered: in the evolution of random graphs, cycles of all (constant) lengths appear at about the same time!

There is a similar phenomenon for trees. Here, the threshold function does depend on the order of the tree considered, but not on its shape:

Corollary 11.4.5. If $T$ is a tree of order $k \geqslant 2$ , then $t ( n ) = n ^ { - k / ( k - 1 ) }$ is a threshold function for the property of containing a copy of $T$ .

We finally have the following result for complete subgraphs:

Corollary 11.4.6. If $k \geqslant 2$ , then $t ( n ) = n ^ { - 2 / ( k - 1 ) }$ is a threshold function for the property of containing a $K ^ { k }$ .

Proof . $K ^ { k }$ is balanced, because $\begin{array} { r } { \varepsilon ( K ^ { i } ) = \frac { 1 } { 2 } ( i - 1 ) < \frac { 1 } { 2 } ( k - 1 ) = \varepsilon ( K ^ { k } ) } \end{array}$ for $i < k$ . With $\ell : = \| K ^ { k } \| = \frac { 1 } { 2 } k ( k - 1 )$ , we obtain $n ^ { - k / \bar { \ell } } = n ^ { - 2 / ( k - 1 ) }$ . 

It is not difficult to adapt the proof of Theorem 11.4.3 to the case that $H$ is unbalanced. The threshold then becomes $t ( n ) = n ^ { - 1 / \varepsilon ^ { \prime } ( H ) }$ , where $\varepsilon ^ { \prime } ( H ) : = \operatorname* { m a x } \left\{ \varepsilon ( F ) \mid F \subseteq H \right\}$ ; see Exercise 21.

# Exercises

1. $-$ What is the probability that a random graph in $\mathcal { G } ( n , p )$ has exactly $m$ edges, for $\begin{array} { r } { 0 \leqslant m \leqslant \binom { n } { 2 } } \end{array}$ fixed?

2. What is the expected number of edges in $G \in \mathcal { G } ( n , p )$ ?

3. What is the expected number of $K ^ { \prime }$ -subgraphs in $G \in \mathcal { G } ( n , p )$ ?

4. Characterize the graphs that occur as a subgraph in every graph of sufficiently large average degree.

5. In the usual terminology of measure spaces (and in particular, of probability spaces), the phrase ‘almost all’ is used to refer to a set of points whose complement has measure zero. Rather than considering a limit of probabilities in $\mathcal { G } ( n , p )$ as $n \longrightarrow \infty$ , would it not be more natural to define a probability space on the set of all finite graphs (one copy of each) and to investigate properties of ‘almost all’ graphs in this space, in the sense above?

6. Show that if almost all $G \in \mathcal { G } ( n , p )$ have a graph property $\mathcal { P } _ { 1 }$ and almost all $G \in \mathcal { G } ( n , p )$ have a graph property $\mathcal { P } _ { 2 }$ , then almost all $G \in \mathcal { G } ( n , p )$ have both properties, i.e. have the property $\mathcal { P } _ { 1 } \cap \mathcal { P } _ { 2 }$ .

7.− Show that, for constant $p \in \mathsf { ( 0 , 1 ) }$ , almost every graph in $\mathcal { G } ( n , p )$ has diameter 2.

8. Show that, for constant $p \in \mathsf { \Gamma } ( 0 , 1 )$ , almost no graph in $\mathcal { G } ( n , p )$ has a separating complete subgraph.

9. Derive Proposition 11.3.1 from Lemma 11.3.2.

10. Let $\epsilon > 0$ and $p = p ( n ) > 0$ , and let $r \geqslant ( 1 + \epsilon ) ( 2 \ln n ) / p$ be an integervalued function of $n$ . Show that almost no graph in $\mathcal { G } ( n , p )$ contains $r$ independent vertices.

11. Show that for every graph $H$ there exists a function $p = p ( n )$ such that $\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty } p ( n ) = 0 } \end{array}$ but almost every $G \in \mathcal { G } ( n , p )$ contains an induced copy of $H$ .

12.+ (i) Show that, for every $0 < \epsilon \leqslant 1$ and $p = ( 1 - \epsilon ) ( \ln n ) n ^ { - 1 }$ , almost every $G \in { \mathcal { G } } ( n , p )$ has an isolated vertex.

(ii) Find a probability $p = p ( n )$ such that almost every $G \in \mathcal { G } ( n , p )$ is disconnected but the expected number of spanning trees of $G$ tends to infinity as $n \longrightarrow \infty$ .

(Hint for (ii): A theorem of Cayley states that $K ^ { \pi }$ has exactly $n ^ { n - 2 }$ spanning trees.)

13. $^ +$ Given $r ~ \in ~ \mathbb { N }$ , find a $c > 0$ such that, for $p \ = \ c n ^ { - 1 }$ , almost every $G \in { \mathcal { G } } ( n , p )$ has a $K ^ { \prime }$ minor. Can $c$ be chosen independently of $r$ ?

14. Find an increasing graph property without a threshold function, and a property that is not increasing but has a threshold function.

15.− Let $H$ be a graph of order $k$ , and let $h$ denote the number of graphs isomorphic to $H$ on some fixed set of $k$ elements. Show that $h \leqslant k !$ . For which graphs $H$ does equality hold?

16. $-$ For every $k \geqslant 1$ , find a threshold function for $\{ G \mid \Delta ( G ) \geqslant k \}$ .

17. $-$ Given $d \in \mathbb { N }$ , is there a threshold function for the property of containing a $d$ -dimensional cube (see Ex. 2, Ch. 1)? If so, which; if not, why not?

18. Show that $t ( n ) = n ^ { - 1 }$ is also a threshold function for the property of containing any cycle.

19. Does the property of containing any tree of order $k$ (for $k \geqslant 2$ fixed) have a threshold function? If so, which?

20.+ Given a graph $H$ , let $\mathcal { P }$ be the property of containing an induced copy of $H$ . If $H$ is complete then, by Corollary 11.4.6, $\mathcal { P }$ has a threshold function. Show that $\mathcal { P }$ has no threshold function if $H$ is not complete.

21.+ Prove the following version of Theorem 11.4.3 for unbalanced subgraphs. Let $H$ be any graph with at least one edge, and put $\varepsilon ^ { \prime } ( H ) : =$ max $\{ \varepsilon ( F ) \mid \emptyset \neq F \subseteq H \}$ . Then the threshold function for $\mathcal { P } _ { H }$ is t(n) = n−1/ε-(H). $t ( n ) = n ^ { - 1 / \varepsilon ^ { \prime } ( H ) }$

# Notes

There are a number of monographs and texts on the subject of random graphs. The first comprehensive monograph was B. Bollob´as, Random Graphs, Academic Press 1985. Another advanced but very readable monograph is S. Janson, T. Luczak & A. Ruci´nski, Random Graphs, Wiley 2000; this concentrates on areas developed since Random Graphs was published. E.M. Palmer, Graphical Evolution, Wiley 1985, covers material similar to parts of Random Graphs but is written in a more elementary way. Compact introductions going

beyond what is covered in this chapter are given by B. Bollob´as, Graph Theory, Springer GTM 63, 1979, and by M. Karo´nski, Handbook of Combinatorics (R.L. Graham, M. Gr¨otschel & L. Lov´asz, eds.), North-Holland 1995.

A stimulating advanced introduction to the use of random techniques in discrete mathematics more generally is given by N. Alon & J.H. Spencer, The Probabilistic Method, Wiley 1992. One of the attractions of this book lies in the way it shows probabilistic methods to be relevant in proofs of entirely deterministic theorems, where nobody would suspect it. Other examples for this phenomenon are Alon’s proof of Theorem 5.4.1, or the proof of Theorem 1.3.4; see the notes for Chapters 5 and 1, respectively.

The probabilistic method had its first origins in the 1940s, one of its earliest results being Erd˝os’s probabilistic lower bound for Ramsey numbers (Theorem 11.1.3). Lemma 11.3.2 about the properties $\mathcal { P } _ { i , j }$ is taken from Bollob´as’s Springer text cited above. A very readable rendering of the proof that, for constant $p$ , every first order sentence about graphs is either almost surely true or almost surely false, is given by P. Winkler, Random structures and zero-one laws, in (N.W. Sauer et al., eds.) Finite and Infinite Combinatorics in Sets and Logic (NATO ASI Series C 411), Kluwer 1993.

Theorem 11.3.5 is due to P. Erd˝os and A. R´enyi, Asymmetric graphs, Acta Math. Acad. Sci. Hungar. 14 (1963), 295–315. For further references about the infinite random graph $R$ see the notes in Chapter 8.

The seminal paper on graph evolution is P. Erd˝os & A. R´enyi, On the evolution of random graphs, Publ. Math. Inst. Hungar. Acad. Sci. 5 (1960), 17–61. This paper also includes Theorem 11.4.3 and its proof. The generalization of this theorem to unbalanced subgraphs was first proved by Bollob´as in 1981, using advanced methods; a simple adaptation of the original Erd˝os-Renyi proof was found by Ruci´nski & Vince (1986), and is presented in Karo´nski’s Handbook chapter.

There is another way of defining a random graph $G$ , which is just as natural and common as the model we considered. Rather than choosing the edges of $G$ independently, we choose the entire graph $G$ uniformly at random from among all the graphs on $\{ 0 , \ldots , n - 1 \}$ that have exactly $M = M ( n )$ edges: then each of these graphs occurs with the same probability of $\textstyle { \binom { N } { M } }$ , where $N : = { \binom { n } { 2 } }$ . Just as we studied the likely properties of the graphs in $\mathcal { G } ( n , p )$ for different functions $p = p ( n )$ , we may investigate how the properties of $G$ in the other model depend on the function $M ( n )$ . If $M$ is close to $p N$ , the expected number of edges of a graph in $\mathcal { G } ( n , p )$ , then the two models behave very similarly. It is then largely a matter of convenience which of them to consider; see Bollob´as for details.

In order to study threshold phenomena in more detail, one often considers the following random graph process: starting with a $\overline { { K ^ { n } } }$ as stage zero, one chooses additional edges one by one (uniformly at random) until the graph is complete. This is a simple example of a Markov chain, whose $M$ th stage corresponds to the ‘uniform’ random graph model described above. A survey about threshold phenomena in this setting is given by T. Luczak, The phase transition in a random graph, in (D. Mikl´os, V.T. S´os & T. Sz˝onyi, eds.) Paul Erd˝os is 80, Vol. 2, Proc. Colloq. Math. Soc. J´anos Bolyai (1996).

Our goal in this last chapter is a single theorem, one which dwarfs any other result in graph theory and may doubtless be counted among the deepest theorems that mathematics has to offer: in every infinite set of graphs there are two such that one is a minor of the other. This graph minor theorem (or minor theorem for short), inconspicuous though it may look at first glance, has made a fundamental impact both outside graph theory and within. Its proof, due to Neil Robertson and Paul Seymour, takes well over 500 pages.

So we have to be modest: of the actual proof of the minor theorem, this chapter will convey only a very rough impression. However, as with most truly fundamental results, the proof has sparked off the development of methods of quite independent interest and potential. This is true particularly for the use of tree-decompositions, a technique we shall meet in Section 12.3. Section 12.1 gives an introduction to well-quasi-ordering, a concept central to the minor theorem. In Section 12.2 we apply this concept to prove the minor theorem for trees. In Section 12.4 we look at forbidden-minor theorems: results in the spirit of Kuratowski’s theorem (4.4.6) or Wagner’s theorem (7.3.4), which describe the structure of the graphs not containing some specified graph or graphs as a minor. We prove one such theorem in full (excluding a given planar graph) and state another (excluding $K ^ { n }$ ); both are central results and tools in the theory of graph minors. In Section 12.5 we give a direct proof of the ‘generalized Kuratowski’ theorem that embeddability in any fixed surface can be characterized by forbidding finitely many minors. We conclude with an overview of the proof and implications of the graph minor theorem itself.

# 12.1 Well-quasi-ordering

well-quasiordering

good pair

good/bad sequence

A reflexive and transitive relation is called a quasi-ordering. A quasiordering $\leqslant$ on $X$ is a well-quasi-ordering, and the elements of $X$ are well-quasi-ordered by $\leqslant$ , if for every infinite sequence $x _ { 0 } , x _ { 1 } , \dotsc$ in $X$ there are indices $i < j$ such that $x _ { i } \leqslant x _ { j }$ . Then $( x _ { i } , x _ { j } )$ is a good pair of this sequence. A sequence containing a good pair is a good sequence; thus, a quasi-ordering on $X$ is a well-quasi-ordering if and only if every infinite sequence in $X$ is good. An infinite sequence is bad if it is not good.

Proposition 12.1.1. A quasi-ordering $\leqslant$ on $X$ is a well-quasi-ordering if and only if $X$ contains neither an infinite antichain nor an infinite strictly decreasing sequence $x _ { 0 } > x _ { 1 } > . . .$ .

(9.1.2)

Proof . The forward implication is trivial. Conversely, let $x _ { 0 } , x _ { 1 } , \dotsc$ be any infinite sequence in $X$ . Let $K$ be the complete graph on $\mathbb { N } =$ $\{ 0 , 1 , \ldots \}$ . Colour the edges $_ { i j }$ $( i < j )$ ) of $K$ with three colours: green if $x _ { i } \leqslant x _ { j }$ , red if $x _ { i } ~ > ~ x _ { j }$ , and amber if $x _ { i } , x _ { j }$ are incomparable. By Ramsey’s theorem (9.1.2), $K$ has an infinite induced subgraph whose edges all have the same colour. If there is neither an infinite antichain nor an infinite strictly decreasing sequence in $X$ , then this colour must be green, i.e. $x _ { 0 } , x _ { 1 } , \dotsc$ has an infinite subsequence in which every pair is good. In particular, the sequence $x _ { 0 } , x _ { 1 } , \dotsc$ is good. 

In the proof of Proposition 12.1.1, we showed more than was needed: rather than finding a single good pair in $x _ { 0 } , x _ { 1 } , \dotsc$ , we found an infinite increasing subsequence. We have thus shown the following:

Corollary 12.1.2. If $X$ is well-quasi-ordered, then every infinite sequence in $X$ has an infinite increasing subsequence. 

The following lemma, and the idea of its proof, are fundamental to the theory of well-quasi-ordering. Let $\leqslant$ be a quasi-ordering on a set $X$ . For finite subsets $A , B \subseteq X$ , write $A \leqslant B$ if there is an injective mapping $f \colon A \to B$ such that $a \leqslant f ( a )$ for all $a \in A$ . This naturally extends $\leqslant$ to a quasi-ordering on $[ X ] ^ { < \omega }$ , the set of all finite subsets of $X$ .

-

[X]<ω

[ 12.2.1 ] [ 12.5.1 ]

Lemma 12.1.3. If $X$ is well-quasi-ordered by $\leqslant$ , then so is $[ X ] ^ { < \omega }$ .

Proof . Suppose that $\leqslant$ is a well-quasi-ordering on $X$ but not on $[ X ] ^ { < \omega }$ . We start by constructing a bad sequence $( A _ { n } ) _ { n \in \mathbb { N } }$ in $[ X ] ^ { < \omega }$ , as follows. Given $\textit { n } \in  { \mathbb { N } }$ , assume inductively that $A _ { i }$ has been defined for every $\textit { i } < \textit { n }$ , and that there exists a bad sequence in $[ X ] ^ { < \omega }$ starting with $A _ { 0 } , \ldots , A _ { n - 1 }$ . (This is clearly true for $n = 0$ : by assumption, $[ X ] ^ { < \omega }$ contains a bad sequence, and this has the empty sequence as an initial

segment.) Choose $A _ { n } \in [ X ] ^ { < \omega }$ so that some bad sequence in $[ X ] ^ { < \omega }$ starts with $A _ { 0 } , \ldots , A _ { n }$ and $\left| A _ { n } \right|$ is as small as possible.

Clearly, $( A _ { n } ) _ { n \in \mathbb { N } }$ is a bad sequence in $[ X ] ^ { < \omega }$ ; in particular, $A _ { n } \neq \emptyset$ for all $n$ . For each $n$ pick an element $a _ { n } \in A _ { n }$ and set $B _ { n } : = A _ { n } \setminus \left\{ \left. a _ { n } \right. \right\}$ .

By Corollary 12.1.2, the sequence $( a _ { n } ) _ { n \in \mathbb { N } }$ has an infinite increasing subsequence $( a _ { n _ { i } } ) _ { i \in \mathbb { N } }$ . By the minimal choice of $A _ { n _ { 0 } }$ , the sequence

$$
A _ {0}, \dots , A _ {n _ {0} - 1}, B _ {n _ {0}}, B _ {n _ {1}}, B _ {n _ {2}}, \dots
$$

is good; consider a good pair. Since $( A _ { n } ) _ { n \in \mathbb { N } }$ is bad, this pair cannot have the form $( A _ { i } , A _ { j } )$ or $( A _ { i } , B _ { j } )$ , as $B _ { j } \ \leqslant \ A _ { j }$ . So it has the form $( B _ { i } , B _ { j } )$ . Extending the injection $B _ { i } \to B _ { j }$ by $a _ { i } \mapsto a _ { j }$ , we deduce again that $( A _ { i } , A _ { j } )$ is good, a contradiction. 

# 12.2 The graph minor theorem for trees

The minor theorem can be expressed by saying that the finite graphs are well-quasi-ordered by the minor relation $\preccurlyeq$ . Indeed, by Proposition 12.1.1 and the obvious fact that no strictly descending sequence of minors can be infinite, being well-quasi-ordered is equivalent to the non-existence of an infinite antichain, the formulation used earlier.

In this section, we prove a strong version of the graph minor theorem for trees:

# Theorem 12.2.1. (Kruskal 1960)

The finite trees are well-quasi-ordered by the topological minor relation.

We shall base the proof of Theorem 12.2.1 on the following notion of an embedding between rooted trees, which strengthens the usual embedding as a topological minor. Consider two trees $T$ and $T ^ { \prime }$ , with roots $r$ and $r ^ { \prime }$ say. Let us write $T \leqslant T ^ { \prime }$ if there exists an isomorphism $\varphi$ , from some subdivision of $T$ to a subtree $T ^ { \prime \prime }$ of $T ^ { \prime }$ , that preserves the tree-order on $V ( T )$ associated with $T$ and $r$ . (Thus if $x < y$ in $T$ then $\varphi ( x ) < \varphi ( y )$ in $T ^ { \prime }$ ; see Fig. 12.2.1.) As one easily checks, this is a quasi-ordering on the class of all rooted trees.

# Proof of Theorem 12.2.1. We show that the rooted trees are wellquasi-ordered by the relation $\leqslant$ defined above; this clearly implies the theorem.

Suppose not. To derive a contradiction, we proceed as in the proof of Lemma 12.1.3. Given $n \in \mathbb N$ , assume inductively that we have chosen a sequence $T _ { 0 } , \ldots , T _ { n - 1 }$ of rooted trees such that some bad sequence of rooted trees starts with this sequence. Choose as $T _ { n }$ a minimum-order

[ 12.5.1 ]

$T \leqslant T ^ { \prime }$

(12.1.3)

$T _ { n }$

![](images/ac894171c47de1233dfe1255df450db8ec215dde795cad8e76644f96eacf5e6a.jpg)  
Fig. 12.2.1. An embedding of $T$ in $T ^ { \prime }$ showing that $T \leqslant T ^ { \prime }$

rooted tree such that some bad sequence starts with $T _ { 0 } , \ldots , T _ { n }$ . For each $n \in \mathbb N$ , denote the root of $T _ { n }$ by $r _ { n }$ .

Clearly, $( T _ { n } ) _ { n \in \mathbb { N } }$ is a bad sequence. For each $n$ , let $A _ { n }$ denote the set of components of $T _ { n } - r _ { n }$ , made into rooted trees by choosing the neighbours of $r _ { n }$ as their roots. Note that the tree-order of these trees is that induced by $T _ { n }$ . Let us prove that the set $A : = \textstyle \bigcup _ { n \in \mathbb { N } } A _ { n }$ of all these trees is well-quasi-ordered.

Let $( T ^ { k } ) _ { k \in \mathbb { N } }$ be any sequence of trees in $A$ . For every $k \in \mathbb N$ choose an $n = n ( k )$ such that $T ^ { k } \in A _ { n }$ . Pick a $k$ with smallest $n ( k )$ . Then

$$
T _ {0}, \dots , T _ {n (k) - 1}, T ^ {k}, T ^ {k + 1}, \dots
$$

is a good sequence, by the minimal choice of $T _ { n ( k ) }$ and $T ^ { k } \subsetneq T _ { n ( k ) }$ . Let $( T , T ^ { \prime } )$ be a good pair of this sequence. Since $( T _ { n } ) _ { n \in \mathbb { N } }$ is bad, $T$ cannot be among the first $n ( k )$ members $T _ { 0 } , \ldots , T _ { n ( k ) - 1 }$ of our sequence: then $T ^ { \prime }$ would be some $T ^ { i }$ with $i \geqslant k$ , i.e.

$$
T \leqslant T ^ {\prime} = T ^ {i} \leqslant T _ {n (i)};
$$

since $n ( k ) \leqslant n ( i )$ by the choice of $k$ , this would make $( T , T _ { n ( i ) } )$ a good pair in the bad sequence $( T _ { n } ) _ { n \in \mathbb { N } }$ . Hence $( T , T ^ { \prime } )$ is a good pair also in $( T ^ { k } ) _ { k \in \mathbb { N } }$ , completing the proof that $A$ is well-quasi-ordered.

By Lemma 12.1.3,1 the sequence $( A _ { n } ) _ { n \in \mathbb { N } }$ in $[ A ] ^ { < \omega }$ has a good pair $( A _ { i } , A _ { j } )$ ; let $f \colon A _ { i } \to A _ { j }$ be injective with $T \leqslant f ( T )$ for all $T \in A _ { i }$ . Now extend the union of the embeddings $T \to f ( T )$ to a map $\varphi$ from $V ( T _ { i } )$ to $V ( T _ { j } )$ by letting $\varphi ( r _ { i } ) : = r _ { j }$ . This map $\varphi$ preserves the tree-order of $T _ { i }$ , and it defines an embedding to show that $T _ { i } \leqslant T _ { j }$ , since the edges $r _ { i } r \in T _ { i }$ map naturally to the paths $r _ { j } T _ { j } \varphi ( r )$ . Hence $( T _ { i } , T _ { j } )$ is a good pair in our original bad sequence of rooted trees, a contradiction. 

# 12.3 Tree-decompositions

Trees are graphs with some very distinctive and fundamental properties; consider Theorem 1.5.1 and Corollary 1.5.2, or the more sophisticated example of Kruskal’s theorem. It is therefore legitimate to ask to what degree those properties can be transferred to more general graphs, graphs that are not themselves trees but tree-like in some sense.2 In this section, we study a concept of tree-likeness that permits generalizations of all the tree properties referred to above (including Kruskal’s theorem), and which plays a crucial role in the proof of the graph minor theorem.

Let $G$ be a graph, $T$ a tree, and let $\mathcal { V } = ( V _ { t } ) _ { t \in T }$ be a family of vertex sets $V _ { t } \subseteq V ( G )$ indexed by the vertices $t$ of $T$ . The pair $( T , \nu )$ is called a tree-decomposition of $G$ if it satisfies the following three conditions:

(T1) $\textstyle V ( G ) = \bigcup _ { t \in T } V _ { t } ;$ ;   
(T2) for every edge $e \in G$ there exists a $t \in T$ such that both ends of $e$ lie in $V _ { t }$ ;   
(T3) $V _ { t _ { 1 } } \cap V _ { t _ { 3 } } \subseteq V _ { t _ { 2 } }$ whenever $t _ { 1 } , t _ { 2 } , t _ { 3 } \in T$ satisfy $t _ { 2 } \in t _ { 1 } T t _ { 3 }$ .

Conditions (T1) and (T2) together say that $G$ is the union of the subgraphs $G \left[ V _ { t } \right]$ ; we call these subgraphs and the sets $V _ { t }$ themselves the parts of $( T , \nu )$ and say that $( T , \nu )$ is a tree-decomposition of $G$ into these parts. Condition (T3) implies that the parts of $( T , \nu )$ are organized roughly like a tree (Fig. 12.3.1).

![](images/5d964c49c9dbc2f0837885f80e0404cd99debd06b765945d142b9b8120c76529.jpg)  
Fig. 12.3.1. Edges and parts ruled out by (T2) and (T3)

Before we discuss the role that tree-decompositions play in the proof of the minor theorem, let us note some of their basic properties. Consider a fixed tree-decomposition $( T , \nu )$ of $G$ , with $\mathcal { V } = ( V _ { t } ) _ { t \in T }$ as above.

Perhaps the most important feature of a tree-decomposition is that it transfers the separation properties of its tree to the graph decomposed:

Lemma 12.3.1. Let $t _ { 1 } t _ { 2 }$ be any edge of $T$ and let $T _ { 1 } , T _ { 2 }$ be the components of $T - t _ { 1 } t _ { 2 }$ , with $t _ { 1 } \in T _ { 1 }$ and $t _ { 2 } \in T _ { 2 }$ . Then $V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ separates $\textstyle U _ { 1 } : = \bigcup _ { t \in T _ { 1 } } V _ { t }$ from $\textstyle U _ { 2 } : = \bigcup _ { t \in T _ { 2 } } V _ { t }$ in $G$ (Fig. 12.3.2).

![](images/4f4b2ea0a6a7dbfd0132771ec66fd372eb319a4c93d2bc6cfb91a32128ecb592.jpg)  
Fig. 12.3.2. $V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ separates $U _ { 1 }$ from $U _ { 2 }$ in $G$

Proof . Both $t _ { 1 }$ and $t _ { 2 }$ lie on every $t$ – $t ^ { \prime }$ path in $T$ with $t \in T _ { 1 }$ and $t ^ { \prime } \in T _ { 2 }$ . Therefore $U _ { 1 } \cap U _ { 2 } \subseteq V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ by (T3), so all we have to show is that $G$ has no edge $u _ { 1 } u _ { 2 }$ with $u _ { 1 } \in U _ { 1 } \setminus U _ { 2 }$ and $u _ { 2 } \in U _ { 2 } \setminus U _ { 1 }$ . If $u _ { 1 } u _ { 2 }$ is such an edge, then by (T2) there is a $t \in T$ with $u _ { 1 } , u _ { 2 } \in V _ { t }$ . By the choice of $u _ { 1 }$ and $u _ { 2 }$ we have neither $t \in T _ { 2 }$ nor $t \in T _ { 1 }$ , a contradiction. 

Note that tree-decompositions are passed on to subgraphs:

[ 12.4.2 ] Lemma 12.3.2. For every $H \subseteq G$ , the pair $( T , ( V _ { t } \cap V ( H ) ) _ { t \in T } )$ is a tree-decomposition of $H$ . 

Similarly for contractions:

Lemma 12.3.3. Suppose that $G$ is an $M H$ with branch sets $U _ { h }$ , $h \in V ( H )$ . Let $f \colon V ( G ) \to V ( H )$ be the map assigning to each vertex of $G$ the index of the branch set containing it. For all $t \in T$ let $W _ { t } : = \{ f ( v ) \mid v \in V _ { t } \}$ , and put $\mathcal { W } : = ( W _ { t } ) _ { t \in T }$ . Then $( T , \mathcal { W } )$ is a treedecomposition of $H$ .

Proof . The assertions (T1) and (T2) for $( T , \mathcal { W } )$ follow immediately from the corresponding assertions for $( T , \nu )$ . Now let $t _ { 1 } , t _ { 2 } , t _ { 3 } \in T$ be as in (T3), and consider a vertex $h \in W _ { t _ { 1 } } \cap W _ { t _ { 3 } }$ of $H$ ; we show that $h \in W _ { t _ { 2 } }$ . By definition of $W _ { t _ { 1 } }$ and $W _ { t _ { 3 } }$ , there are vertices $v _ { 1 } \in V _ { t _ { 1 } } \cap U _ { h }$ and $v _ { 3 } \in V _ { t _ { 3 } } \cap U _ { h }$ . Since $U _ { h }$ is connected in $G$ and $V _ { t _ { 2 } }$ separates $v _ { 1 }$ from $v _ { 3 }$ in $G$ by Lemma 12.3.1, $V _ { t _ { 2 } }$ has a vertex in $U _ { h }$ . By definition of $W _ { t _ { 2 } }$ , this implies $h \in W _ { t _ { 2 } }$ . 

Here is another useful consequence of Lemma 12.3.1:

Lemma 12.3.4. Given a set $W \subseteq V ( G )$ , there is either a $t \in T$ such that $W \subseteq V _ { t }$ , or there are vertices $w _ { 1 } , w _ { 2 } \in W$ and an edge $t _ { 1 } t _ { 2 } \in T$ such that $w _ { 1 } , w _ { 2 }$ lie outside the set $V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ and are separated by it in $G$ .

Proof . Let us orient the edges of $T$ as follows. For each edge $t _ { 1 } t _ { 2 } \in T$ , define $U _ { 1 } , U _ { 2 }$ as in Lemma 12.3.1; then $V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ separates $U _ { 1 }$ from $U _ { 2 }$ . If $V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ does not separate any two vertices of $W$ that lie outside it, we can find an $i \in \{ 1 , 2 \}$ such that $W \subseteq U _ { i }$ , and orient $t _ { 1 } t _ { 2 }$ towards $t _ { i }$ .

Let $t$ be the last vertex of a maximal directed path in $T$ ; we claim that $W \subseteq V _ { t }$ . Given $w \in W$ , let $t ^ { \prime } \in T$ be such that $w \in V _ { t ^ { \prime } }$ . If $t ^ { \prime } \neq t$ , then the edge $e$ at $t$ that separates $t ^ { \prime }$ from $t$ in $T$ is directed towards $t$ , so $w$ also lies in $V _ { t ^ { \prime \prime } }$ for some $t ^ { \prime \prime }$ in the component of $T - e$ containing $t$ . Therefore $w \in V _ { t }$ by (T3). 

The following special case of Lemma 12.3.4 is used particularly often:

Lemma 12.3.5. Any complete subgraph of $G$ is contained in some part [ 12.4.2 ] of $( T , \nu )$ . 

As indicated by Figure 12.3.1, the parts of $( T , \nu )$ reflect the structure of the tree $T$ , so in this sense the graph $G$ decomposed resembles a tree. However, this is valuable only inasmuch as the structure of $G$ within each part is negligible: the smaller the parts, the closer the resemblance.

This observation motivates the following definition. The width of $( T , \nu )$ is the number

$$
\max  \left\{\left| V _ {t} \right| - 1: t \in T \right\},
$$

and the tree-width $\operatorname { t w } ( G )$ of $G$ is the least width of any tree-decomposition of $G$ . As one easily checks,3 trees themselves have tree-width 1.

By Lemmas 12.3.2 and 12.3.3, the tree-width of a graph will never be increased by deletion or contraction:

Proposition 12.3.6. If $H \preccurlyeq G$ then $\operatorname { t w } ( H ) \leqslant \operatorname { t w } ( G ) .$ .  [ 12.4.2 ] [12.4.2]

Graphs of bounded tree-width are sufficiently similar to trees that it becomes possible to adapt the proof of Kruskal’s theorem to the class of these graphs; very roughly, one has to iterate the ‘minimal bad sequence’ argument from the proof of Lemma $1 2 . 1 . 3 \ \mathrm { t w } ( G )$ times. This takes us a step further towards a proof of the graph minor theorem:

Theorem 12.3.7. (Robertson & Seymour 1990) [12.5.3] [ 12.5.3 ]

For every integer $k > 0$ , the graphs of tree-width $< k$ are well-quasiordered by the minor relation.

width

tree-width $\operatorname { t w } ( G )$

[ 12.5.1 ]

In order to make use of Theorem 12.3.7 for a proof of the general minor theorem, we should be able to say something about the graphs it does not cover, i.e. to deduce some information about a graph from the assumption that its tree-width is large. Our next theorem achieves just that: it identifies a canonical obstruction to small tree-width, a structural phenomenon that occurs in a graph if and only if its tree-width is large.

Let us say that two subsets of $V ( G )$ touch if they have a vertex in common or $G$ contains an edge between them. A set of mutually touching connected vertex sets in $G$ is a bramble. Extending our terminology of Chapter 2, we say that a subset of $V ( G )$ covers (or is a cover of) a bramble $\boldsymbol { B }$ if it meets every element of $\boldsymbol { B }$ . The least number of vertices covering a bramble is the order of that bramble.

The following simple observation will be useful:

Lemma 12.3.8. Any set of vertices separating two covers of a bramble also covers that bramble.

Proof . Since each set in the bramble is connected and meets both of the covers, it also meets any set separating these covers. 

A typical example of a bramble is the set of crosses in a grid. The $k \times k$ grid is the graph on $\{ 1 , \ldots , k \} ^ { 2 }$ with the edge set

$$
\left\{\left(i, j\right) \left(i ^ {\prime}, j ^ {\prime}\right): \left| i - i ^ {\prime} \right| + \left| j - j ^ {\prime} \right| = 1 \right\}.
$$

The crosses of this grid are the $k ^ { 2 }$ sets

$$
C _ {i j} := \left\{\left(i, \ell\right) \mid \ell = 1, \dots , k \right\} \cup \left\{\left(\ell , j\right) \mid \ell = 1, \dots , k \right\}.
$$

Thus, the cross $C _ { i j }$ is the union of the grid’s $i$ th column and its $j$ th row. Clearly, the crosses of the $k \times k$ grid form a bramble of order $k$ : they are covered by any row or column, while any set of fewer than $k$ vertices misses both a row and a column, and hence a cross.

The following result is sometimes called the tree-width duality theorem:

# Theorem 12.3.9. (Seymour & Thomas 1993)

Let $k \geqslant 0$ be an integer. A graph has tree-width $\geqslant k$ if and only if it contains a bramble of order $> k$ .

(3.3.1) Proof . For the backward implication, let $\boldsymbol { B }$ be any bramble in a graph $G$ . We show that every tree-decomposition $( T , ( V _ { t } ) _ { t \in T } )$ of $G$ has a part that covers $\boldsymbol { B }$ .

As in the proof of Lemma 12.3.4 we start by orienting the edges $t _ { 1 } t _ { 2 }$ of $T$ . If $X : = V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ covers $\boldsymbol { \beta }$ , we are done. If not, then for each $B \in B$

disjoint from $X$ there is an $i \in \{ 1 , 2 \}$ such that $B \subseteq U _ { i } \setminus X$ (defined as in Lemma 12.3.1); recall that $B$ is connected. This $i$ is the same for all such $B$ , because they touch. We now orient the edge $t _ { 1 } t _ { 2 }$ towards $t _ { i }$ .

If every edge of $T$ is oriented in this way and $t$ is the last vertex of a maximal directed path in $T$ , then $V _ { t }$ covers $\boldsymbol { B }$ —just as in the proof of Lemma 12.3.4.

To prove the forward direction, we now assume that $G$ contains no bramble of order $> k$ . We show that for every bramble $\boldsymbol { \beta }$ in $G$ there is a $\boldsymbol { \beta }$ -admissible tree-decomposition of $G$ , one in which any part of order $> k$ fails to cover $\boldsymbol { B }$ . For $B = \emptyset$ this implies that $\operatorname { t w } ( G ) < k$ , because every set covers the empty bramble.

Let $\boldsymbol { B }$ be given, and assume inductively that for every bramble $B ^ { \prime }$ containing more sets than $\boldsymbol { B }$ there is a $B ^ { \prime }$ -admissible tree-decomposition of $G$ . (The induction starts, since no bramble in $G$ has more than $2 ^ { | G | }$ sets.) Let $X \subseteq V ( G )$ be a cover of $\boldsymbol { B }$ with as few vertices as possible; then $\ell : = | X | \leqslant k$ is the order of $\boldsymbol { B }$ . Our aim is to show the following:

For every component tree-decomposition o $C$ $G [ X \cup V ( C ) ]$ $G - X$ re exis with $X$ a $\boldsymbol { \beta }$ -admissible a part. (∗)

Then these tree-decompositions can be combined to a $\boldsymbol { \beta }$ -admissible treedecomposition of $G$ by identifying their nodes corresponding to $X$ . (If $X = V ( G )$ , then the tree-decomposition with $X$ as its only part is $\boldsymbol { \beta }$ - admissible.)

So let $C$ be a fixed component of $G - X$ , write $H : = G \left[ X \cup V ( C ) \right]$ , and put $B ^ { \prime } : = B \cup \{ C \}$ . If $B ^ { \prime }$ is not a bramble then $C$ fails to touch some element of $\boldsymbol { B }$ , and hence $Y : = V ( C ) \cup N ( C )$ does not cover $\boldsymbol { B }$ . Then the tree-decomposition of $H$ consisting of the two parts $X$ and $Y$ satisfies $( * )$ .

So we may assume that $B ^ { \prime }$ is a bramble. Since $X$ covers $\boldsymbol { B }$ , we have $C \notin B$ and hence $\left| B ^ { \prime } \right| > \left| B \right|$ . Our induction hypothesis therefore ensures that $G$ has a $B ^ { \prime }$ -admissible tree-decomposition $( T , ( V _ { t } ) _ { t \in T } )$ . If this decomposition is also $\boldsymbol { \beta }$ -admissible, there is nothing more to show. If not, then one of its parts of order $> k$ , $V _ { s }$ say, covers $\boldsymbol { B }$ . Since no set of fewer than $\ell$ vertices covers $\boldsymbol { B }$ , Lemma 12.3.8 implies with Menger’s theorem (3.3.1) that $V _ { s }$ and $X$ are linked by $\ell$ disjoint paths $P _ { 1 } , \ldots , P _ { \ell }$ . As $V _ { s }$ fails to cover $B ^ { \prime }$ and hence lies in $G - C$ , the paths $P _ { i }$ meet $H$ only in their ends $x _ { i } \in X$ .

For each $i = 1 , \ldots , \ell$ pick a $t _ { i } \in T$ with $x _ { i } \in V _ { t _ { i } }$ , and let

$$
W _ {t} := \left(V _ {t} \cap V (H)\right) \cup \left\{x _ {i} \mid t \in s T t _ {i} \right\}
$$

for all $t \in T$ (Fig. 12.3.3). Then $( T , ( W _ { t } ) _ { t \in T } )$ is the tree-decomposition which $( T , ( V _ { t } ) _ { t \in T } )$ induces on $H$ (cf. Lemma 12.3.2), except that a few $x _ { i }$ have been added to some of the parts. Despite these additions, we

![](images/f8b89a7055fa0910f60b7d3a4059909d4144a7b3ebd5f741f07bcac98e798333.jpg)  
Fig. 12.3.3. $W _ { t }$ contains $x _ { 2 }$ and $x _ { 3 }$ but not $x _ { 1 }$ ; $W _ { t ^ { \prime } }$ contains no $x _ { i }$

still have $| W _ { t } | \leqslant | V _ { t } |$ for all $t$ : for each $x _ { i } \in W _ { t } \setminus V _ { t }$ we have $t \in s T t _ { i }$ , so $V _ { t }$ contains some other vertex of $P _ { i }$ (Lemma 12.3.1); that vertex does not lie in $W _ { t }$ , because $P _ { i }$ meets $H$ only in $x _ { i }$ . Moreover, $( T , ( W _ { t } ) _ { t \in T } )$ clearly satisfies (T3), because each $x _ { i }$ is added to every part along some path in $T$ containing $t _ { i }$ , so it is again a tree-decomposition.

As $W _ { s } = X$ , all that is left to show for ( ) is that this decomposition is $\boldsymbol { \beta }$ -admissible. Consider any $W _ { t }$ of order $> k$ . Then $W _ { t }$ meets $C$ , because $| X | = \ell \leqslant k$ . Since $( T , ( V _ { t } ) _ { t \in T } )$ is $B ^ { \prime }$ -admissible and $| V _ { t } | \geq$ $| W _ { t } | > k$ , we know that $V _ { t }$ fails to meet some $B \in B$ ; let us show that $W _ { t }$ does not meet this $B$ either. If it does, it must do so in some $x _ { i } \in W _ { t } \setminus V _ { t }$ . Then $B$ is a connected set meeting both $V _ { s }$ and $V _ { t _ { i } }$ but not $V _ { t }$ . As $t \in s T t _ { i }$ by definition of $W _ { t }$ , this contradicts Lemma 12.3.1. 

Often, Theorem 12.3.9 is stated in terms of the bramble number of a graph, the largest order of any bramble in it. The theorem then says that the tree-width of a graph is exactly one less than its bramble number.

How useful even the easy backward direction of Theorem 12.3.9 can be is exemplified once more by our example of the crosses bramble in the $k \times k$ grid: this bramble has order $k$ , so by the theorem the $k \times k$ grid has tree-width at least $k - 1$ . (Try to show this without the theorem!)

In fact, the $k \times k$ grid has tree-width $k$ (Exercise 21). But more important than its precise value is the fact that the tree-width of grids tends to infinity with their size. For as we shall see, large grid minors pose another canonical obstruction to small tree-width: not only do large grids (and hence all graphs containing large grids as minors; cf. Proposition 12.3.6) have large tree-width, but conversely every graph of large tree-width has a large grid minor (Theorem 12.4.4).

Yet another canonical obstruction to small tree-width is described in Exercise 35.

Let us call our tree-decomposition $( T , \nu )$ of $G$ linked , or lean,4 if it linked/lean satisfies the following condition:

(T4) Given $t _ { 1 } , t _ { 2 } \in T$ and vertex sets $Z _ { 1 } \subseteq V _ { t _ { 1 } }$ and $Z _ { 2 } \subseteq V _ { t _ { 2 } }$ such that $| Z _ { 1 } | = | Z _ { 2 } | = : k$ , either $G$ contains $k$ disjoint $Z _ { 1 } - Z _ { 2 }$ paths or there exists an edge $t t ^ { \prime } \in t _ { 1 } T t _ { 2 }$ with $V _ { t } \cap V _ { t ^ { \prime } } < k$ .

The ‘branches’ in a lean tree-decomposition are thus stripped of any bulk not necessary to maintain their connecting qualities: if a branch is thick (i.e. the separators $V _ { t } \cap V _ { t ^ { \prime } }$ along a path in $T$ are large), then $G$ is highly connected along this branch. For $t _ { 1 } = t _ { 2 }$ , (T4) says that the parts themselves are no larger than their ‘external connectivity’ in $G$ requires; cf. Lemma 12.4.5 and Exercise 35.

In our quest for tree-decompositions into ‘small’ parts, we now have two criteria to choose between: the global ‘worst case’ criterion of width, which ensures that $T$ is nontrivial (unless $G$ is complete) but says nothing about the tree-likeness of $G$ among parts other than the largest, and the more subtle local criterion of leanness, which ensures tree-likeness everywhere along $T$ but might be difficult to achieve except with trivial or near-trivial $T$ . Surprisingly, though, it is always possible to find a tree-decomposition that is optimal with respect to both criteria at once:

# Theorem 12.3.10. (Thomas 1990)

Every graph $G$ has a lean tree-decomposition of width $\operatorname { t w } ( G )$

There is now a short proof of Theorem 12.3.10; see the notes. The fact that this theorem gives us a useful property of minimum-width tree-decompositions ‘for free’ has made it a valuable tool wherever treedecompositions are applied.

The tree-decomposition $( T , \nu )$ of $G$ is called simplicial if all the separators $V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ induce complete subgraphs in $G$ . This assumption can enable us to lift assertions about the parts of the decomposition to $G$ itself. For example, if all the parts in a simplicial tree-decomposition of $G$ are $k$ -colourable, then so is $G$ (proof?). The same applies to the property of not containing a $K ^ { r }$ minor for some fixed $r$ . Algorithmically, it is easy to obtain a simplicial tree-decomposition of a given graph into irreducible parts. Indeed, all we have to do is split the graph recursively along complete separators; if these are always chosen minimal, then the set of parts obtained will even be unique (Exercise 27).

Conversely, if $G$ can be constructed recursively from a set $\mathcal { H }$ of graphs by pasting along complete subgraphs, then $G$ has a simplicial tree-decomposition into elements of $\mathcal { H }$ . For example, by Wagner’s Theorem 7.3.4, any graph without a $K ^ { 5 }$ minor has a supergraph with a simplicial tree-decomposition into plane triangulations and copies of the

Wagner graph $W$ , and similarly for graphs without $K ^ { 4 }$ minors (see Proposition 12.4.2).

Tree-decompositions may thus lead to intuitive structural characterizations of graph properties. A particularly simple example is the following characterization of chordal graphs:

[ 12.4.2 ] Proposition 12.3.11. $G$ is chordal if and only if $G$ has a tree-decomposition into complete parts.

(5.5.1) Proof . We apply induction on $| G |$ . We first assume that $G$ has a treedecomposition $( T , \nu )$ such that $G \left[ V _ { t } \right]$ is complete for every $t \in T$ ; let us choose $( T , \nu )$ with $| T |$ minimal. If $| T | \leqslant 1$ , then $G$ is complete and hence chordal. So let $t _ { 1 } t _ { 2 } \in T$ be an edge, and for $i = 1 , 2$ define $T _ { i }$ and $G _ { i } : = G [ U _ { i } ]$ as in Lemma 12.3.1. Then $G = G _ { 1 } \cup G _ { 2 }$ by (T1) and (T2), and $V ( G _ { 1 } \cap G _ { 2 } ) = V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ by the lemma; thus, $G _ { 1 } \cap G _ { 2 }$ is complete. Since $( T _ { i } , ( V _ { t } ) _ { t \in T _ { i } } )$ is a tree-decomposition of $G _ { i }$ into complete parts, both $G _ { i }$ are chordal by the induction hypothesis. (By the choice of $( T , \nu )$ , neither $G _ { i }$ is a subgraph of $G \left[ V _ { t _ { 1 } } \cap V _ { t _ { 2 } } \right] = G _ { 1 } \cap G _ { 2 }$ , so both $G _ { i }$ are indeed smaller than $G$ .) Since $G _ { 1 } \cap G _ { 2 }$ is complete, any induced cycle in $G$ lies in $G _ { 1 }$ or in $G _ { 2 }$ and hence has a chord, so $G$ too is chordal.

Conversely, assume that $G$ is chordal. If $G$ is complete, there is nothing to show. If not then, by Proposition 5.5.1, $G$ is the union of smaller chordal graphs $G _ { 1 } , G _ { 2 }$ with $G _ { 1 } \cap G _ { 2 }$ complete. By the induction hypothesis, $G _ { 1 }$ and $G _ { 2 }$ have tree-decompositions $( T _ { 1 } , \ V _ { 1 } )$ and $( T _ { 2 } , \nu _ { 2 } )$ into complete parts. By Lemma 12.3.5, $G _ { 1 } \cap G _ { 2 }$ lies inside one of those parts in each case, say with indices $t _ { 1 } \in T _ { 1 }$ and $t _ { 2 } \in T _ { 2 }$ . As one easily checks, $( ( T _ { 1 } \cup T _ { 2 } ) + t _ { 1 } t _ { 2 } , \mathcal { V } _ { 1 } \cup \mathcal { V } _ { 2 } )$ is a tree-decomposition of $G$ into complete parts. 

Corollary 12.3.12. $\mathrm { t w } ( G ) = \operatorname* { m i n } { \big \{ } \omega ( H ) - 1 ~ | ~ G \subseteq H ; ~ H \operatorname { c h o r d a l } { \big \} } .$

Proof . By Lemma 12.3.5 and Proposition 12.3.11, each of the graphs $H$ considered for the minimum has a tree-decomposition of width $\omega ( H ) - 1$ . Every such tree-decomposition induced one of $G$ by Lemma 12.3.2, so $\operatorname { t w } ( G ) \leqslant \omega ( H ) - 1$ for every $H$ .

Conversely, let us construct an $H$ as above with $\omega ( H ) - 1 \leqslant \operatorname { t w } ( G )$ . Let $( T , \nu )$ be a tree-decomposition of $G$ of width $\operatorname { t w } ( G )$ . For every $t \in T$ let $K _ { t }$ denote the complete graph on $V _ { t }$ , and put $H : = \textstyle \bigcup _ { t \in T } K _ { t }$ . Clearly, $( T , \nu )$ is also a tree-decomposition of $H$ . By Proposition 12.3.11, $H$ is chordal, and by Lemma 12.3.5, $\omega ( H ) - 1$ is at most the width of $( T , \nu )$ , i.e. at most $\operatorname { t w } ( G )$ . 

# 12.4 Tree-width and forbidden minors

If $\mathcal { H }$ is any set or class of graphs, then the class

$$
\operatorname {F o r b} _ {\preccurlyeq} (\mathcal {H}) := \left\{G \mid G \neq H \text {f o r a l l} H \in \mathcal {H} \right\}
$$

Forb-(H)

of all graphs without a minor in $\mathcal { H }$ is a graph property, i.e. is closed under isomorphism.5 When it is written as above, we say that this property is expressed by specifying the graphs $H \in \mathcal { H }$ as forbidden (or excluded) minors.

By Proposition 1.7.3, Forb( ) is closed under taking minors, or minor-closed: if $G ^ { \prime } \preccurlyeq G \in \operatorname { F o r b } _ { \prec } ( \mathcal { H } )$ then $G ^ { \prime } \in \mathrm { F o r b } _ { \prec } ( \mathcal { H } )$ . Every minorclosed property can in turn be expressed by forbidden minors:

forbidden minors

(1.7.3)

Proposition 12.4.1. A graph property $\mathcal { P }$ can be expressed by forbidden minors if and only if it is closed under taking minors.

Proof . For the ‘if’ part, note that $\mathcal { P } ~ = ~ \mathrm { F o r b } _ { \prec } ( \overline { { \mathcal { P } } } )$ , where $\overline { { \mathcal { P } } }$ is the complement of $\mathcal { P }$ . 

$\overline { { \mathcal { P } } }$

In Section 12.5, we shall return to the general question of how a given minor-closed property is best represented by forbidden minors. In this section, we are interested in one particular example of such a property: bounded tree-width.

Consider the property of having tree-width less than some given integer $k$ . By Propositions 12.3.6 and 12.4.1, this property can be expressed by forbidden minors. Choosing their set $\mathcal { H }$ as small as possible, we find that $\mathcal { H } = \{ K ^ { 3 } \}$ for $k = 2$ : the graphs of tree-width $< 2$ are precisely the forests. For $k = 3$ , we have $\mathcal { H } = \{ K ^ { 4 } \}$ :

Proposition 12.4.2. A graph has tree-width $< 3$ if and only if it has no $K ^ { 4 }$ minor.

Proof . By Lemma 12.3.5, we have $\mathrm { t w } ( K ^ { 4 } ) \geqslant 3$ . By Proposition 12.3.6, therefore, a graph of tree-width $< 3$ cannot contain $K ^ { 4 }$ as a minor.

Conversely, let $G$ be a graph without a $K ^ { 4 }$ minor; we assume that $| G | \geqslant 3$ . Add edges to $G$ until the graph $G ^ { \prime }$ obtained is edge-maximal without a $K ^ { 4 }$ minor. By Proposition 7.3.1, $G ^ { \prime }$ can be constructed recursively from triangles by pasting along $K ^ { 2 } \mathrm { s }$ . By induction on the number of recursion steps and Lemma 12.3.5, every graph constructible in this way has a tree-decomposition into triangles (as in the proof of Proposition 12.3.11). Such a tree-decomposition of $G ^ { \prime }$ has width 2, and by Lemma 12.3.2 it is also a tree-decomposition of $G$ . 

(12.3.11)

(4.4.6)

As $k$ grows, the list of forbidden minors characterizing the graphs of tree-width $< k$ seems to grow fast. They are known explicitly only up to $k = 4$ ; see the notes.

A question converse to the above is to ask for which $H$ (other than $K ^ { 3 }$ and $K ^ { 4 }$ ) the tree-width of the graphs in Forb (H) is bounded. Interestingly, it is not difficult to show that any such $H$ must be planar. Indeed, as all grids and their minors are planar (why?), every class Forb $\preccurlyeq ( H )$ with non-planar $H$ contains all grids; yet as we saw after Theorem 12.3.9, the grids have unbounded tree-width.

The following deep and surprising theorem says that, conversely, the tree-width of the graphs in Forb $\preccurlyeq ( H )$ is bounded for every planar $H$ :

# Theorem 12.4.3. (Robertson & Seymour 1986)

Given a graph $H$ , the graphs without an $H$ minor have bounded treewidth if and only if $H$ is planar.

The rest of this section is devoted to the proof of Theorem 12.4.3 and an application.

To prove Theorem 12.4.3 we have to show that forbidding any planar graph $H$ as a minor bounds the tree-width of a graph. In fact, we only have to show this for the special cases when $H$ is a grid, because every planar graph is a minor of some grid. (To see this, take a drawing of the graph, fatten its vertices and edges, and superimpose a sufficiently fine plane grid.) It thus suffices to show the following:

[ 12.5.1 ] [ 12.5.3 ]

# Theorem 12.4.4. (Robertson & Seymour 1986)

For every integer $r$ there is an integer $k$ such that every graph of treewidth at least $k$ has an $r \times r$ grid minor.

Our proof of Theorem 12.4.4 proceeds as follows. Let $r$ be given, and let $G$ be any graph of large enough tree-width (depending on $r$ ). We first show that $G$ contains a large family $\mathcal { A } = \{ A _ { 1 } , \ldots , A _ { m } \}$ of disjoint connected vertex sets such that each pair $A _ { i } , A _ { j } \in { \mathcal { A } }$ can be linked in $G$ by a family $\mathcal { P } _ { i j }$ of many disjoint $A _ { i } { - } A _ { j }$ paths avoiding all the other sets in $\mathcal { A }$ . We then consider all the pairs $\left( \mathcal { P } _ { i j } , \mathcal { P } _ { i ^ { \prime } j ^ { \prime } } \right)$ of these path families. If we can find a pair among these such that many of the paths in $\mathcal { P } _ { i j }$ meet many of the paths in $\mathcal { P } _ { i ^ { \prime } j ^ { \prime } }$ , we shall think of the paths in $\mathcal { P } _ { i j }$ as horizontal and the paths in $\mathcal { P } _ { i ^ { \prime } j ^ { \prime } }$ as vertical and extract a subdivision of an $r \times r$ grid from their union. (This will be the difficult part of the proof, because these paths will in general meet in a less orderly way than they do in a grid.) If not, then for every pair $( \mathcal { P } _ { i j } , \mathcal { P } _ { i ^ { \prime } j ^ { \prime } } )$ many of the paths in $\mathcal { P } _ { i j }$ avoid many of the paths in $\mathcal { P } _ { i ^ { \prime } j ^ { \prime } }$ . We can then select one path $P _ { i j } \in \mathcal { P } _ { i j }$ from each family so that these selected paths are pairwise disjoint. Contracting each of the connected sets $A \in { \mathcal { A } }$ will then give us a $K ^ { m }$ minor in $G$ , which contains the desired $r \times r$ grid if $m \geqslant r ^ { 2 }$ .

To implement these ideas formally, we need a few definitions. Let us call a set $X \subseteq V ( G )$ externally $k$ -connected in $G$ if $| X | \geq k$ and for all disjoint subsets $Y , Z \subseteq X$ with $| Y | = | Z | \leq k$ there are $| Y |$ disjoint $Y { - } Z$ paths in $G$ that have no inner vertex or edge in $G \left[ X \right]$ . Note that the vertex set of a $k$ -connected subgraph of $G$ need not be externally $k$ -connected in $G$ . On the other hand, any horizontal path in the $r \times r$ grid is externally $k$ -connected in that grid for every $k \leqslant r$ . (How?)

One of the first things we shall prove below is that any graph of large enough tree-width—not just grids—contains a large externally $k$ - connected set of vertices (Lemma 12.4.5). Conversely, it is easy to show that large externally $k$ -connected sets (with $k$ large) can exist only in graphs of large tree-width (Exercise 35). So, like large grid minors, these sets form a canonical obstruction to small tree-width: they can be found in a graph if and only if its tree-width is large.

An ordered pair $( A , B )$ of subgraphs of $G$ will be called a premesh in $G$ if $G = A \cup B$ and $A$ contains a tree $T$ such that

(i) $T$ has maximum degree $\leq 3$ ;   
(ii) every vertex of $A \cap B$ lies in $T$ and has degree $\leq 2$ in $T$ ;   
(iii) $T$ has a leaf in $A \cap B$ , or $| T | = 1$ and $T \subseteq A \cap B$ .

The order of such a premesh is the number $| A \cap B |$ , and if $V ( A \cap B )$ is externally $k$ -connected in $B$ then this premesh is a $k$ -mesh in $G$ .

Lemma 12.4.5. Let $G$ be a graph and let $h \geq k \geq 1$ be integers. If $G$ contains no $k$ -mesh of order $h$ then $G$ has tree-width $< h + k - 1$ .

Proof . We may assume that $G$ is connected. Let $U \subseteq V ( G )$ be maximal such that $G [ U ]$ has a tree-decomposition $\mathcal { D }$ of width $< h + k - 1$ with the additional property that, for every component $C$ of $G - U$ , the neighbours of $C$ in $U$ lie in one part of $\mathcal { D }$ and $( G - C , { \bar { C } } )$ is a premesh of order $\leq h$ , where ${ \ddot { C } } : = G \left[ V ( C ) \cup N ( C ) \right]$ . Clearly, $U \neq \emptyset$ .

We claim that $U = V ( G )$ . Suppose not. Let $C$ be a component of $G - U$ , put $X : = N ( C )$ , and let $T$ be a tree associated with the premesh $( G - C , { \tilde { C } } )$ .

By assumption, $| X | \leq h$ ; let us show that equality holds here. If not, let $u \in X$ be a leaf of $T$ (respectively $\{ u \} : = V ( T ) )$ ) as in (iii), and let $v \in C$ be a neighbour of $u$ . Put $U ^ { \prime } : = U \cup \{ v \}$ and $X ^ { \prime } : = X \cup \{ v \}$ , let $T ^ { \prime }$ be the tree obtained from $T$ by joining $\boldsymbol { v }$ to $u$ , and let $\mathcal { D } ^ { \prime }$ be the tree-decomposition of $G [ U ^ { \prime } ]$ obtained from $\mathcal { D }$ by adding $X ^ { \prime }$ as a new part (joined to a part of $\mathcal { D }$ containing $X$ , which exists by our choice of $U$ ; see Fig. 12.4.1). Clearly $\mathcal { D } ^ { \prime }$ still has width $< h + k - 1$ . Consider a component $C ^ { \prime }$ of $G - U ^ { \prime }$ . If $C ^ { \prime } \cap C = \emptyset$ then $C ^ { \prime }$ is also a component of $G - U$ , so $N ( C ^ { \prime } )$ lies inside a part of $\mathcal { D }$ (and hence of $\mathcal { D } ^ { \prime }$ ), and $( G - C ^ { \prime } , \bar { C } ^ { \prime } )$ is a premesh of order $\leq h$ by assumption. If $C ^ { \prime } \cap C \neq \emptyset$ , then $C ^ { \prime } \subseteq C$ and $N ( C ^ { \prime } ) \subseteq X ^ { \prime }$ . Moreover, $v \in N ( C ^ { \prime } )$ : otherwise $N ( C ^ { \prime } ) \subseteq X$ would

separate $C ^ { \prime }$ from $v$ , contradicting the fact that $C ^ { \prime }$ and $\boldsymbol { v }$ lie in the same component $C$ of $G - X$ . Since $v$ is a leaf of $T ^ { \prime }$ , it is straightforward to check that $( G - C ^ { \prime } , \bar { C } ^ { \prime } )$ is again a premesh of order $\leq h$ , contrary to the maximality of $U$ .

![](images/9e08f5694a69a1b959f02b8b88ccbaf7c07f6147d4162f17743e0e013736fb00.jpg)  
Fig. 12.4.1. Extending $U$ and $\mathcal { D }$ when $| X | < h$

Thus $| X | = h$ , so by assumption our premesh $( G - C , { \tilde { C } } )$ cannot be a $k$ -mesh; let $Y , Z \subseteq X$ be sets to witness this. Let $\mathcal { P }$ be a set of as many disjoint $Y { - } Z$ paths in $H : = G \left[ V ( C ) \cup Y \cup Z \right] - E ( G \left[ Y \cup Z \right] )$ as possible. Since all these paths are ‘external’ to $X$ in $\tilde { C }$ , we have $k ^ { \prime } : = | \mathcal { P } | < | Y | = | Z | \leqslant k$ by the choice of $Y$ and $Z$ . By Menger’s theorem (3.3.1), $Y$ and $Z$ are separated in $H$ by a set $S$ of $k ^ { \prime }$ vertices. Clearly, $S$ has exactly one vertex on each path in $\mathcal { P }$ ; we denote the path containing the vertex $s \in S$ by $P _ { s }$ (Fig. 12.4.2).

![](images/3e9b4bc81ff729c9370cfd7f93d44c1f2f425a4685ed5ce9c9d9ddf8fe7f9ecc.jpg)  
Fig. 12.4.2. $S$ separates $Y$ from $Z$ in $H$

Let $X ^ { \prime } : = \ X \cup S$ and $U ^ { \prime } : = U \cup S$ , and let $\mathcal { D } ^ { \prime }$ be the treedecomposition of $G [ U ^ { \prime } ]$ obtained from $\mathcal { D }$ by adding $X ^ { \prime }$ as a new part. Clearly, $| X ^ { \prime } | \leq | X | + | S | \leq h + k - 1$ . We show that $U ^ { \prime }$ contradicts the maximality of $U$ .

Since $Y \cup Z \subseteq N ( C )$ and $| S | < | Y | = | Z |$ we have $S \cap C \neq \emptyset$ , so $U ^ { \prime }$ is larger than $U$ . Let $C ^ { \prime }$ be a component of $G - U ^ { \prime }$ . If $C ^ { \prime } \cap C = \emptyset$ ,

we argue as earlier. So $C ^ { \prime } \subseteq C$ and $N ( C ^ { \prime } ) \subseteq X ^ { \prime }$ . As before, $C ^ { \prime }$ has at least one neighbour $v$ in $S \cap C$ , since $X$ cannot separate $C ^ { \prime } \subseteq C$ from $S \cap C$ . By definition of $S$ , $C ^ { \prime }$ cannot have neighbours in both $Y \setminus S$ and $Z \backslash S$ ; we assume it has none in $Y \setminus S$ . Let $T ^ { \prime }$ be the union of $T$ and all the $Y$ – $S$ subpaths of paths $P _ { s }$ with $s \in N ( C ^ { \prime } ) \cap C$ ; since these subpaths start in $Y \setminus S$ and have no inner vertices in $X ^ { \prime }$ , they cannot meet $C ^ { \prime }$ . Therefore $( G - C ^ { \prime } , \bar { C } ^ { \prime } )$ is a premesh with tree $T ^ { \prime }$ and leaf $\boldsymbol { v }$ ; the degree conditions on $T ^ { \prime }$ are easily checked. Its order is $| N ( C ^ { \prime } ) | \leq$ $| X | - | Y | + | S | = h - | Y | + k ^ { \prime } < h$ , a contradiction to the maximality of $U$ . 

Lemma 12.4.6. Let $k \geq 2$ be an integer. Let $T$ be a tree of maximum degree $\leqslant 3$ and $X \subseteq V ( T )$ . Then $T$ has a set $F ^ { \prime }$ of edges such that every component of $T - F$ has between $k$ and $2 k - 1$ vertices in $X$ , except that one such component may have fewer vertices in $X$ .

Proof . We apply induction on $| X |$ . If $| X | \le 2 k - 1$ we put $F = \emptyset$ . So assume that $| X | \geq 2 k$ . Let $e$ be an edge of $T$ such that some component $T ^ { \prime }$ of $T - e$ has at least $k$ vertices in $X$ and $| T ^ { \prime } |$ is as small as possible. As $\Delta ( T ) \ \leq \ 3$ , the end of $e$ in $T ^ { \prime }$ has degree at most two in $T ^ { \prime }$ , so the minimality of $T ^ { \prime }$ implies that $| X \cap V ( T ^ { \prime } ) | \leq 2 k - 1$ . Applying the induction hypothesis to $T - T ^ { \prime }$ we complete the proof. 

Lemma 12.4.7. Let $G$ be a bipartite graph with bipartition $\{ A , B \}$ , $| A | = a$ , $| B | = b$ , and let $c \leq a$ and $d \leq b$ be positive integers. Assume that $G$ has at most $( a - c ) ( b - d ) / d$ edges. Then there exist $C \subseteq A$ and $\boldsymbol { D } \subseteq B$ such that $| C | = c$ and $| D | = d$ and $C \cup D$ is independent in $G$ .

Proof . As $| | G | | \leq ( a - c ) ( b - d ) / d$ , fewer than $b - d$ vertices in $B$ have more than $( a - c ) / d$ neighbours in $A$ . Choose $\boldsymbol { D } \subseteq B$ so that $| D | = d$ and each vertex in $D$ has at most $( a - c ) / d$ neighbours in $A$ . Then $D$ sends a total of at most $a - c$ edges to $A$ , so $A$ has a subset $C$ of $c$ vertices without a neighbour in $D$ . 

Given a tree $T$ , call an $r$ -tuple $( x _ { 1 } , \ldots , x _ { r } )$ of distinct vertices of $T$ good if, for every $j = 1 , \ldots , r - 1$ , the $x _ { j } - x _ { j + 1 }$ path in $T$ contains none of the other vertices in this $r$ -tuple.

Lemma 12.4.8. Every tree $T$ of order at least $r ( r - 1 )$ contains a good $r$ -tuple of vertices.

Proof . Pick a vertex $x \in T$ . Then $T$ is the union of its subpaths $x T y$ , where $y$ ranges over its leaves. Hence unless one of these paths has at least $r$ vertices, $T$ has at least $| T | / ( r - 1 ) \geqslant r$ leaves. Since any path of $r$ vertices and any set of $r$ leaves gives rise to a good $r$ -tuple in $T$ , this proves the assertion. 

Our next lemma shows how to obtain a grid from two large systems of paths that intersect in a particularly orderly way.

Lemma 12.4.9. Let $d , r \geq 2$ be integers such that $d \ge r ^ { 2 r + 2 }$ . Let $G$ be a graph containing a set $\mathcal { H }$ of $r ^ { 2 } - 1$ disjoint paths and a set $\mathcal { V } = \{ V _ { 1 } , \ldots , V _ { d } \}$ of $d$ disjoint paths. Assume that every path in $\nu$ meets every path in $\mathcal { H }$ , and that each path $H \in \mathcal { H }$ consists of $d$ consecutive (vertex-disjoint) segments such that $V _ { i }$ meets $H$ only in its ith segment, for every $i = 1 , \ldots , d$ (Fig. 12.4.3). Then $G$ has an $r \times r$ grid minor.

![](images/03e3e653c7cc94b2c130cf8b798f4d8c6f50bba13896ec2852e7e3c5a214bda7.jpg)  
Fig. 12.4.3. Paths intersecting as in Lemma 12.4.9

$T _ { i }$

$H ^ { 1 } , \ldots , H ^ { r }$

$I , \ i _ { k }$

$\mathcal { H } ^ { \prime }$

Proof . For each $i = 1 , \ldots , d$ , consider the graph with vertex set $\mathcal { H }$ in which two paths are adjacent whenever $V _ { i }$ contains a subpath between them that meets no other path in $\mathcal { H }$ . Since $V _ { i }$ meets every path in $\mathcal { H }$ , this is a connected graph; let $T _ { i }$ be a spanning tree in it. Since $| \mathcal { H } | \geq r ( r - 1 )$ , Lemma 12.4.8 implies that each of these $d \geq r ^ { 2 } ( r ^ { 2 } ) ^ { r }$ trees $T _ { i }$ has a good $r$ -tuple of vertices. Since there are no more than $( r ^ { 2 } ) ^ { r }$ distinct $r$ -tuples on $\mathcal { H }$ , some $r ^ { 2 }$ of the trees $T _ { i }$ have a common good $r$ -tuple $( H ^ { 1 } , \ldots , H ^ { r } )$ . Let $I = \{ i _ { 1 } , . . . , i _ { r ^ { 2 } } \}$ be the index set of these trees (with $\textit { i j } < \textit { i } _ { k }$ for $j < k$ ) and put $\mathcal { H } ^ { \prime } : = \{ H ^ { 1 } , \ldots , H ^ { r } \}$ .

Here is an informal description of how we construct our $r \times r$ grid. Its ‘horizontal’ paths will be the paths $H ^ { 1 } , \ldots , H ^ { r }$ . Its ‘vertical’ paths will be pieced together edge by edge, as follows. The $r - 1$ edges of the first vertical path will come from the first $r - 1$ trees $T _ { i }$ , trees with their index $i$ among the first $r$ elements of $I$ . More precisely, its ‘edge’ between $H ^ { j }$ and $H ^ { j + 1 }$ will be the sequence of subpaths of $V _ { i _ { j } }$ (together with some connecting horizontal bits taken from paths in $\mathcal { H } \backslash \mathcal { H ^ { \prime } }$ ) induced by the edges of an $H ^ { j } { - } H ^ { j + 1 }$ path in $T _ { i _ { j } }$ that has no inner vertices in $\mathcal { H } ^ { \prime }$ ; see Fig. 12.4.4. (This is why we need $( H ^ { 1 } , \ldots , H ^ { r } )$ to be a good $r$ -tuple in every tree $T _ { i }$ .) Similarly, the $j$ th edge of the second vertical path will come from an $H ^ { j } { - } H ^ { j + 1 }$ path in $T _ { i _ { r + j } }$ , and so on. (Although we need only $r - 1$ edges for each vertical path, we reserve $r$ rather than just $r - 1$ of the paths $V _ { i }$ for each vertical path to make the indexing more lucid.

![](images/1536e8cdad5e9c982b0d0fb1a8ddd0dca38e1375689f3c319b6d19dcba79b48d.jpg)

![](images/50633f09c7b4656c9902c5dd24d36a023f7685c91075d0f6f9042298385f2e1b.jpg)  
The $H ^ { j } { - } H ^ { j + 1 }$ path $P$ in $\mathit { T } _ { i _ { j } }$

![](images/92236e887c9d2fb799bcc2cfd9018ce707cef6dca519c74ddbab025121747688.jpg)

![](images/d7b3e86266025e25c47fd0c0a3a7ea65e499a0ed758663a0876755af27a296f7.jpg)  
$P ^ { \prime }$ viewed as a (subdivided) $H ^ { j } { - } H ^ { j + 1 }$ edge   
Fig. 12.4.4. An $H ^ { j } { - } H ^ { j + 1 }$ path in $\mathit { T } _ { i _ { j } }$ inducing segments of $V _ { i _ { j } }$ for the $j$ th edge of the grid’s first vertical path

(3.3.1)

The paths $V _ { i _ { r } } , V _ { i _ { 2 r } } , \ldots$ are left unused.) To merge these individual edges into $r$ vertical paths, we then contract in each horizontal path the initial segment that meets the first $r$ paths $V _ { i }$ with $i \in I$ , then contract the segment that meets the following $r$ paths $V _ { i }$ with $i \in I$ , and so on.

Formally, we proceed as follows. Consider all $j , k \in \{ 1 , \dots , r \}$ . (We shall think of the index $j$ as counting the horizontal paths, and of the index $k$ as counting the vertical paths of the grid to be constructed.) Let $H _ { k } ^ { j }$ be the minimal subpath of $H ^ { j }$ that contains the $_ i$ th segment of $H ^ { j }$ for all $i$ with $i _ { ( k - 1 ) r } < i \leq i _ { k r }$ (put $i _ { 0 } : = 0$ ). Let $\hat { H } ^ { j }$ be obtained from $H ^ { j }$ by first deleting any vertices following its $i _ { r ^ { 2 } } \mathrm { t h }$ segment and then contracting every subpath $H _ { k } ^ { j }$ to one vertex $v _ { k } ^ { j }$ . Thus, $\hat { H } ^ { j } = v _ { 1 } ^ { j } \ldots v _ { r } ^ { j }$ .

Given $j \in \{ 1 , \dots , r - 1 \}$ and $k \in \{ 1 , \ldots , r \}$ , we have to define a path $V _ { k } ^ { j }$ { − } { }that will form the subdivided ‘vertical edge’ $v _ { k } ^ { j } v _ { k } ^ { j + 1 }$ . This path will consist of segments of the path $V _ { i }$ together with some otherwise unused segments of paths from $\mathcal { H } \backslash \mathcal { H ^ { \prime } }$ , for $i : = i _ { ( k - 1 ) r + j }$ ; recall that, precisely in vertices that were contr by definition of $\hat { H } ^ { j }$ and $\hat { H } ^ { j + 1 }$ , this $V _ { i }$ does indeed meet ed into $v _ { k } ^ { j }$ and $v _ { k } ^ { j + 1 }$ $H ^ { j }$ , respectively. and $H ^ { j + 1 }$ To define $V _ { k } ^ { j }$ , consider an $H ^ { j } { - } H ^ { j + 1 }$ path $P = H _ { 1 } \ldots H _ { t }$ in $T _ { i }$ that has no inner vertices in $\mathcal { H } ^ { \prime }$ . (Thus, $H _ { 1 } \ = \ H ^ { j }$ and $H _ { t } ~ = ~ H ^ { j + 1 }$ .) Every edge $H _ { s } H _ { s + 1 }$ of $P$ corresponds to an $H _ { s } - H _ { s + 1 }$ subpath of $V _ { i }$ that has no inner vertex on any path in $\mathcal { H }$ . Together with (parts of) the $i$ th segments of $H _ { 2 } , \ldots , H _ { t - 1 }$ , these subpaths of $V _ { i }$ form an $H ^ { j } { - } H ^ { j + 1 }$ path $P ^ { \prime }$ in $G$ that has no inner vertices on any of the paths $H ^ { 1 } , \ldots , H ^ { r }$ and meets no path from $\mathcal { H }$ outside its $i$ th segment. Replacing the ends of $P ^ { \prime }$ on $H ^ { j }$ and $H ^ { j + 1 }$ H with $v _ { k } ^ { j }$ and $v _ { k } ^ { j + 1 }$ , respectively, we obtain our desired path $V _ { k } ^ { \mathcal { I } }$ forming the $j$ th (subdivided) edge of the $k$ th ‘vertical’ path of our grid. Since the paths $P ^ { \prime }$ are disjoint for different $i$ and different pairs $( j , k )$ give rise to different $_ i$ , the paths $V _ { k } ^ { \mathcal { I } }$ are disjoint except for possible common ends $v _ { k } ^ { j }$ . Moreover, they have no inner vertices on any of the paths $H ^ { 1 } , \ldots , H ^ { r }$ , because none of these $H ^ { j }$ is an inner vertex of any of the paths $P \subseteq T _ { i }$ used in the construction of $V _ { k } ^ { j }$ . 

Proof of Theorem 12.4.4. We are now ready to prove the following quantitative version of our theorem (which clearly implies it):

Let $r , m > 0$ be integers, and let $G$ be a graph of tree-width at least $r ^ { 4 m ^ { 2 } ( r + 2 ) }$ . Then $G$ contains either the $r \times r$ grid or $K ^ { m }$ as a minor.

$c , k$

Since $K ^ { r ^ { 2 } }$ contains the $r \times r$ grid as a subgraph we may assume that $2 \leq m \leq r ^ { 2 }$ . Put $c : = r ^ { 4 ( r + 2 ) }$ , and let $k : = c ^ { \hat { 2 } ( { m } ) }$ . Then $c \geqslant 2 ^ { 1 6 }$ and hence $2 m + 3 \leq c ^ { m }$ , so $G$ has tree-width at least

$$
c ^ {m ^ {2}} = c ^ {m} k \geqslant (2 m + 3) k \geqslant (m + 1) (2 k - 1) + k - 1,
$$

enough for Lemma 12.4.5 to ensure that $G$ contains a $k$ -mesh $( A , B )$ of order $( m + 1 ) ( 2 k - 1 )$ . Let $T \subseteq A$ be a tree associated with the premesh $( A , B )$ ; then $X : = V ( A \cap B ) \subseteq V ( T )$ . By Lemma 12.4.6, $T$ has $| X | / ( 2 k - 1 ) - 1 = m$ disjoint subtrees each containing at least $k$ vertices of $X$ ; let $A _ { 1 } , \ldots , A _ { m }$ be the vertex sets of these trees. By definition of a $k$ -mesh, $B$ contains for all $1 \leq i < j \leq m$ a set $\mathcal { P } _ { i j }$ of $k$ disjoint $A _ { i } { - } A _ { j }$ paths that have no inner vertices in $A$ . These sets $\mathcal { P } _ { i j }$ will shrink a little and be otherwise modified later in the proof, but they will always consist of ‘many’ disjoint $A _ { i } { - } A _ { j }$ paths.

One option in our proof will be to find single paths $P _ { i j } \in \mathcal { P } _ { i j }$ that are disjoint for different pairs $_ { i j }$ and thus link up the sets $A _ { i }$ to form a $K ^ { m }$ minor of $G$ . If this fails, we shall instead exhibit two specific sets $\mathcal { P } _ { i j }$ and $\mathcal { P } _ { p q }$ such that many paths of $\mathcal { P } _ { i j }$ meet many paths of $\mathcal { P } _ { p q }$ , forming an $r \times r$ grid between them by Lemma 12.4.9.

Let us impose a linear ordering on the index pairs $_ { i j }$ by fixing an arbitrary bijection $\sigma : \{ i j | 1 \leq i < j \leq m \} \to \{ 0 , 1 , . . . , { \binom { m } { 2 } } - 1 \}$ . For $\ell = 0 , 1 , \ldots$ in turn, we shall consider the pair $p q$ with $\sigma ( p q ) = \ell$ and choose an $A _ { p } - A _ { q }$ path $P _ { p q }$ that is disjoint from all previously selected such paths, i.e. from the paths $P _ { s t }$ with $\sigma ( s t ) < \ell$ . At the same time, we shall replace all the ‘later’ sets $\mathcal { P } _ { i j }$ —or what has become of them—by smaller sets containing only paths that are disjoint from $P _ { p q }$ . Thus for each pair $_ { i j }$ , we shall define a sequence $\mathcal { P } _ { i j } = \mathcal { P } _ { i j } ^ { \cup } , \mathcal { P } _ { i j } ^ { \bot } , \ldots .$ of smaller and smaller sets of paths, which eventually collapses to $\mathcal { \bar { P } } _ { i j } ^ { \ell } = \{ P _ { i j } \}$ when $\ell$ has risen to $\ell = \sigma ( i j )$ .

More formally, let $\ell ^ { * } \leq { \binom { m } { 2 } }$ be the greatest integer such that, for all $0 \leq \ell < \ell ^ { * }$ and all $1 \leqslant i < j \leqslant m$ , there exist sets $\mathcal { P } _ { i j } ^ { \ell }$ satisfying the following five conditions:

(i) $\mathcal { P } _ { i j } ^ { \ell }$ is a non-empty set of disjoint $A _ { i } { - } A _ { j }$ paths in $B$ that meet $A$ only in their endpoints.

Whenever a set $\mathcal { P } _ { i j } ^ { \ell }$ is defined, we shall write $H _ { i j } ^ { \ell } : = \cup \mathcal { P } _ { i j } ^ { \ell }$ for the union of its paths.

(ii) If $\sigma ( i j ) < \ell$ then $\mathcal { P } _ { i j } ^ { \ell }$ has exactly one element $P _ { i j }$ , and $P _ { i j }$ does not meet any path belonging to a set $\mathcal { P } _ { s t } ^ { \ell }$ with $i j \neq s t$ .   
(iii) If $\sigma ( i j ) = \ell$ , then $| \mathcal { P } _ { i j } ^ { \ell } | = k / c ^ { 2 \ell }$ .   
(iv) If $\sigma ( i j ) > \ell$ , then $| \mathcal { P } _ { i j } ^ { \ell } | = k / c ^ { 2 \ell + 1 }$ .   
(v) If $\ell = \sigma ( p q ) < \sigma ( i j )$ , then for every $e \in E ( H _ { i j } ^ { \ell } ) \setminus E ( H _ { p q } ^ { \ell } ) _ { , }$ there are no $k / c ^ { 2 \ell + 1 }$ disjoint $A _ { i } { - } A _ { j }$ paths in the graph $( H _ { p q } ^ { \ell } \cup \bar { H } _ { i j } ^ { \ell } ) - e$ .

Note that, by (iv), the paths considered in (v) do exist in $H _ { i j } ^ { \ell }$ . The purpose of (v) is to force those paths to reuse edges from $H _ { p q } ^ { \ell ^ { - } }$ whenever possible, using new edges $e \notin H _ { p q } ^ { \ell }$ only if necessary. Note further that since $\sigma ( i j ) < { \binom { m } { 2 } }$ by definition of $\sigma$ , conditions (iii) and (iv) give $| \mathcal { P } _ { i j } ^ { \ell } | \geq c ^ { 2 }$ whenever $\sigma ( i j ) \geq \ell$ .

$( A , B )$

$T$

$X$

$A _ { 1 } , \ldots , A _ { m }$

$\mathcal { P } _ { i j }$

$^ o$

$\ell ^ { * }$

$H _ { i j } ^ { \ell }$

$P _ { i j }$

Clearly if $\ell ^ { * } = { \binom { m } { 2 } }$ then by (i) and (ii) we have a (subdivided) $K ^ { m }$ 2minor with branch sets $A _ { 1 } , \ldots , A _ { m }$ in $G$ . Suppose then that $\ell ^ { * } < { \binom { m } { 2 } }$ . Let us show that $\ell ^ { * } > 0$ . Let $p q : = \sigma ^ { - 1 } ( 0 )$ and put $\mathcal { P } _ { p q } ^ { 0 } : = \mathcal { P } _ { p q }$ . To define $\mathcal { P } _ { i j } ^ { 0 }$ for $\sigma ( i j ) > 0$ put $H _ { i j } : = \cup \mathcal { P } _ { i j }$ , let $F \subseteq E ( H _ { i j } ) \setminus E ( H _ { p q } ^ { 0 } )$ Pij be maximal such that $( H _ { p q } ^ { \cup } \cup H _ { i j } ) - F$ P still contains $k / c$ \ disjoint $A _ { i } { - } A _ { j }$ paths, and let $\mathcal { P } _ { i j } ^ { \mathrm { 0 } }$ be such a set of paths. Since the vertices from $A _ { p } \cup A _ { q }$ Pijhave degree 1 in $H _ { p q } ^ { \mathrm { 0 } } \cup H _ { i j }$ unless they also lie in $A _ { i } \cup A _ { j }$ ∪ , these paths have no inner vertices in $A$ . Our choices of $\mathcal { P } _ { i j } ^ { 0 }$ therefore satisfy (i)–(v) for $\ell = 0$ .

Having shown that $\ell ^ { * } > 0$ , let us now consider $\ell : = \ell ^ { \ast } - 1$ . Thus, conditions (i)–(v) are satisfied for $\ell$ but cannot be satisfied for $\ell + 1$ . Let $p q : = \sigma ^ { - 1 } ( \ell )$ . If $\mathcal { P } _ { p q } ^ { \ell }$ contains a path $P$ that avoids a set $\mathcal { Q } _ { i j }$ of some $| \mathcal { P } _ { i j } ^ { \ell } | / c$ of the paths in $\mathcal { P } _ { i j } ^ { \ell }$ for all $_ { i j }$ with $\sigma ( i j ) > \ell$ , then we can define $\mathcal { P } _ { i j } ^ { \ell + 1 }$ ii and put for all $_ { i j }$ ij as before (with a contradiction). Indeed, let . For write $s t : =$ , $\sigma ^ { - 1 } ( \ell + 1 )$ $\mathcal { P } _ { s t } ^ { \ell + 1 } : = \mathcal { Q } _ { s t }$ $\sigma ( i j ) > \ell + 1$ $H _ { i j } : = \cup \mathcal { Q } _ { i j }$ let $F \subseteq E ( H _ { i j } ) \setminus E ( H _ { s t } ^ { \ell + 1 } )$ Q   be maximal such that $( H _ { s t } ^ { \ell + 1 } \cup H _ { i j } ) - F$ Q still |of paths. Setting ⊆ ij  contains at least $| \mathcal { P } _ { i j } ^ { \ell } | / c ^ { 2 }$ $\mathcal { P } _ { p q } ^ { \ell + 1 } : = \{ P \} _ { \rho }$ disjoint $A _ { i } – A _ { j }$ $\mathcal { P } _ { i j } ^ { \ell + 1 } : = \mathcal { P } _ { i j } ^ { \ell } = \{ P _ { i j } ^ { \ell ^ { \prime } } \}$ spaths, and let $\mathcal { P } _ { i j } ^ { \ell + 1 }$ for ij  −   be such a set $\sigma ( i j ) < \ell$ then gives us a family of sets $\mathcal { P } _ { i j } ^ { \ell + 1 }$ that contradicts the maximality of $\ell ^ { * }$ .

Thus for every path $P \in \dot { \mathcal { P } } _ { p q } ^ { \ell }$ there exists a pair $_ { i j }$ with $\sigma ( i j ) > \ell$ such that $P$ avoids fewer than $| \mathcal { P } _ { i j } ^ { \ell } | / c$ of the paths in $\mathcal { P } _ { i j } ^ { \ell }$ . For some $\lceil \rceil \mathcal { P } _ { p q } ^ { \ell } \rceil / ( \binom { m } { 2 } \rceil$ of these $P$ that pair $_ { i j }$ will be the same; let $_ { \mathcal { P } }$ denote the set of those $P$ , and keep $_ { i j }$ fixed from now on. Note that $\vert \mathcal { P } \vert \ge \vert \mathcal { P } _ { p q } ^ { \ell } \vert / ( { m \atop 2 } ) =$ $c | \mathcal { P } _ { i j } ^ { \ell } | / \binom { m } { 2 }$ by (iii) and (iv).

ij | 2Let us use Lemma 12.4.7 to find sets $\mathcal { V } \subseteq \mathcal { P } \subseteq \mathcal { P } _ { p q } ^ { \ell }$ and $\mathcal { H } \subseteq \mathcal { P } _ { i j } ^ { \ell }$ such that

$$
\begin{array}{l} | \mathcal {V} | \geqslant \frac {1}{2} | \mathcal {P} | \quad \left( \begin{array}{c c} \geqslant & \frac {c}{m ^ {2}} | \mathcal {P} _ {i j} ^ {\ell} | \end{array} \right) \\ | \mathcal {H} | = r ^ {2} \\ \end{array}
$$

and every path in $\nu$ meets every path in $\mathcal { H }$ . We have to check that the bipartite graph with vertex sets $\mathcal { P }$ and $\mathcal { P } _ { i j } ^ { \ell }$ in which $P \in \mathcal { P }$ is adjacent to $Q \in \mathcal { P } _ { i j } ^ { \ell }$ whenever $P \cap Q = \emptyset$ does not have too many edges. Since every $P \in \mathcal { P }$ has fewer than $| \mathcal { P } _ { i j } ^ { \ell } | / c$ neighbours (by definition of $\mathcal { P }$ ), this graph indeed has at most

$$
\begin{array}{l} | \mathcal {P} | | \mathcal {P} _ {i j} ^ {\ell} | / c \leqslant | \mathcal {P} | | \mathcal {P} _ {i j} ^ {\ell} | / 6 r ^ {2} \\ \leqslant \lfloor | \mathcal {P} | / 2 \rfloor | \mathcal {P} _ {i j} ^ {\ell} | / 2 r ^ {2} \\ \leqslant \left\lfloor | \mathcal {P} | / 2 \right\rfloor \left(\left| \mathcal {P} _ {i j} ^ {\ell} \right| / r ^ {2} - 1\right) \\ = \left(| \mathcal {P} | - \lceil | \mathcal {P} | / 2 \rceil\right) \left(| \mathcal {P} _ {i j} ^ {\ell} | - r ^ {2}\right) / r ^ {2} \\ \end{array}
$$

$\nu , H$

edges, as required. Hence, $\nu$ and $\mathcal { H }$ exist as claimed.

Although all the (‘vertical’) paths in $\nu$ meet all the (‘horizontal’) paths in $\mathcal { H }$ , these paths do not necessarily intersect in such an orderly

way as required for Lemma 12.4.9. In order to divide the paths from $\mathcal { H }$ into segments, and to select paths from $\nu$ meeting them only in the appropriate segments, we shall first pick a path ${ \cal Q } \in { \mathcal H }$ to serve as a yardstick: we shall divide $Q$ into segments each meeting lots of paths from $\nu$ , select a ‘non-crossing’ subset $V _ { 1 } , \ldots , V _ { d }$ of these vertical paths, one from each segment (which is the most delicate task; we shall need condition (v) from the definition of the sets $\mathcal { P } _ { i j } ^ { \ell }$ here), and finally divide the other horizontal paths into the ‘induced’ segments, accommodating one $V _ { n }$ each.

So let us pick a path $Q \in \mathcal { H }$ , and put

$$
d := \lfloor \sqrt {c} / m \rfloor = \lfloor r ^ {2 r + 4} / m \rfloor \geq r ^ {2 r + 2}.
$$

Note that $| \mathcal { V } | \geqslant ( c / m ^ { 2 } ) | \mathcal { P } _ { i j } ^ { \ell } | \geqslant d ^ { 2 } | \mathcal { P } _ { i j } ^ { \ell } |$ .

For $n = 1 , 2 , \ldots , d - 1$ let $e _ { n }$ be the first edge of $Q$ (on its way from $A _ { i }$ $n d | \mathcal { P } _ { i j } ^ { \ell } |$ to $A _ { j }$ different path ) such that the initial component rom $\nu$ , and such that $Q _ { n }$ of $e _ { n }$ −  is not $\boldsymbol { Q } - \boldsymbol { e _ { n } }$ meets at least n edge of $H _ { p q } ^ { \ell }$ $Q$ $\nu$ in $Q$ by an edge not in $H _ { p q } ^ { \ell }$ , each of these $Q _ { n }$ meets exactly $n d | \mathcal { P } _ { i j } ^ { \ell } |$ paths from $\nu$ . Put $Q _ { 0 } : = \emptyset$ and $Q _ { d } : = Q$ . Since $| \mathcal { V } | \geq d ^ { 2 } | \mathcal { P } _ { i j } ^ { \ell } |$ , we have thus divided $Q$ into $d$ consecutive disjoint segments $Q _ { n } ^ { \prime } : = Q _ { n } - Q _ { n - 1 }$ $n = 1 , \ldots , d$ ) each meeting at least $d | \mathcal { P } _ { i j } ^ { \ell } |$ paths from $\nu$ .

For each $n = 1 , \ldots , d - 1$ , Menger’s theorem (3.3.1) and conditions (iv) and (v) imply that $H _ { p q } ^ { \ell } \cup H _ { i j } ^ { \ell }$ has a set $S _ { n }$ of $| \mathcal { P } _ { i j } ^ { \ell } | - 1$ vertices such that $( H _ { p q } ^ { \ell } \cup H _ { i j } ^ { \ell } ) - e _ { n } - S _ { n }$ contains no path from $A _ { i }$ to $A _ { j }$ . Let $S$ denote the union of all these sets $S _ { n }$ . Then $\vert S \vert < d \vert \mathcal { P } _ { i j } ^ { \ell } \vert$ , so each $Q _ { n } ^ { \prime }$ meets at least one path $V _ { n } \in \mathcal V$ that avoids $S$ (Fig. 12.4.5).

$Q$

$^ d$

en $e _ { n }$

$Q _ { n }$

$Q _ { 1 } ^ { \prime } , \ldots , Q _ { d } ^ { \prime }$

$S _ { n }$

S

$V _ { n }$

![](images/48bb2b3120e771e4d1a7a2c7ed692382c1052b92b35499d9f033d96b0ea307f1.jpg)  
Fig. 12.4.5. $V _ { n }$ meets every horizontal path but avoids $S$

Clearly, each $S _ { n }$ consists of a choice of exactly one vertex $x$ from every path $P \in \mathcal { P } _ { i j } ^ { \ell } \setminus \{ Q \}$ . Denote the initial component of $P - x$ by $P _ { n }$ ,

$P _ { 1 } ^ { \prime } , \ldots , P _ { d } ^ { \prime }$

put $P _ { 0 } : = \emptyset$ and $P _ { d } : = P$ , and let $P _ { n } ^ { \prime } : = P _ { n } - P _ { n - 1 }$ for $n = 1 , \ldots , d$ . The separation properties of the sets $S _ { n }$ now imply that $V _ { n } \cap P \subseteq P _ { n } ^ { \prime }$ for $n = 1 , \ldots , d$ (and hence in particular that $P _ { n } ^ { \prime } \neq \emptyset$ , i.e. that $P _ { n - 1 } \subset P _ { n }$ ). Indeed $V _ { n }$ cannot meet $P _ { n - 1 }$ , because $P _ { n - 1 } \cup V _ { n } \cup ( Q - Q _ { n - 1 } )$ would then contain an $A _ { i } – A _ { j }$ path in $( H _ { p q } ^ { \ell } \cup H _ { i j } ^ { \ell } ) - e _ { n - 1 } - S _ { n - 1 }$ , and likewise (consider $S _ { n }$ ) $V _ { n }$ cannot meet $P - P _ { n }$ . Thus for all $n = 1 , \ldots , d$ , the path $V _ { n }$ meets every path $P \in { \mathcal { H } } \backslash \{ Q \}$ precisely in its $n$ th segment $P _ { n } ^ { \prime }$ . Applying Lemma 12.4.9 to the path systems $\mathcal { H } \backslash \{ Q \}$ and $\{ V _ { 1 } , \ldots , V _ { d } \}$ now yields the desired grid minor. 

Theorem 12.4.3 has an interesting application. Recall that a class $\mathcal { H }$ of graphs has the Erd˝os-P´osa property if the number of vertices in a graph needed to cover all its subgraphs in $\mathcal { H }$ is bounded by a function of its maximum number of disjoint subgraphs in $\mathcal { H }$ . Now let $H$ be a fixed connected graph, and consider the class $\mathcal { H } = M H$ of graphs that contract to a copy of $H$ . (Thus, $G$ has a subgraph in $\mathcal { H }$ if and only if $H \preccurlyeq G$ .)

Corollary 12.4.10. If $H$ is planar, then $\mathcal { H } = M H$ has the Erd˝os-P´osa property.

Proof . We have to find a function $f \colon  { \mathbb { N } } \to  { \mathbb { N } }$ such that, given $k \in \mathbb N$ and a graph $G$ , either $G$ has $k$ disjoint subgraphs in $M H$ or there is a set $U$ of at most $f ( k )$ vertices in $G$ such that $G - U$ has no subgraph in $M H$ , i.e. $H \not \prec G - U$ .

By Theorem 12.4.3, there exists for every $k \geqslant 1$ an integer $w _ { k }$ such that every graph of tree-width at least $w _ { k }$ contains the disjoint union of $k$ copies of $H$ (which is again planar) as a minor. Define

$$
f (k) := 2 f (k - 1) + w _ {k}
$$

inductively, starting with $f ( 0 ) = f ( 1 ) = 0$ .

To verify that $f$ does what it should, we apply induction on $k$ . For $k \leqslant 1$ there is nothing to show. Now let $k$ and $G$ be given for the induction step. If $\operatorname { t w } ( G ) \ \geqslant \ w _ { k }$ , we are home by definition of $w _ { k }$ . So assume that $\operatorname { t w } ( G ) < w _ { k }$ , and let $( T , ( V _ { t } ) _ { t \in T } )$ be a tree-decomposition of $G$ of width $< ~ w _ { k }$ . Let us direct the edges $t _ { 1 } t _ { 2 }$ of the tree $T$ as follows. Let $T _ { 1 } , T _ { 2 }$ be the components of $T - t _ { 1 } t _ { 2 }$ containing $t _ { 1 }$ and $t _ { 2 }$ , respectively, and put

$$
G _ {1} := G \left[ \bigcup_ {t \in T _ {1}} \left(V _ {t} \setminus V _ {t _ {2}}\right) \right] \quad \text {a n d} \quad G _ {2} := G \left[ \bigcup_ {t \in T _ {2}} \left(V _ {t} \setminus V _ {t _ {1}}\right) \right].
$$

We direct the edge $t _ { 1 } t _ { 2 }$ towards $G _ { i }$ if $H \preccurlyeq G _ { i }$ , thereby giving $t _ { 1 } t _ { 2 }$ either one or both or neither direction.

If every edge of $T$ receives at most one direction, we follow these to a node $t \in T$ such that no edge at $t$ in $T$ is directed away from $t$ . As $H$ is connected, this implies by Lemma 12.3.1 that $V _ { t }$ meets every $M H$ in $G$ . This completes the proof with $U = V _ { t }$ , since $| V _ { t } | \leqslant w _ { k } \leqslant f ( k )$ by the choice of our tree-decomposition.

Suppose now that $T$ has an edge $t _ { 1 } t _ { 2 }$ that received both directions. For each $i = 1 , 2$ let us ask if we can cover all the $M H$ subgraphs of $G _ { i }$ by at most $f ( k - 1 )$ vertices. If we can, for both $i$ , then by Lemma 12.3.1 the two covers combine with $V _ { t _ { 1 } } \cap V _ { t _ { 2 } }$ to the desired cover $U$ for $G$ . Suppose now that $G _ { 1 }$ has no such cover. Then, by the induction hypothesis, $G _ { 1 }$ has $k - 1$ disjoint $M H$ subgraphs. Since $t _ { 1 } t _ { 2 }$ was also directed towards $t _ { 2 }$ , there is another such subgraph in $G _ { 2 }$ . This gives the desired total of $k$ disjoint $M H$ subgraphs in $G$ . 

Note that Corollary 12.4.10 contains the Erd˝os-P´osa theorem 2.3.2 as the special case of $H = K ^ { 3 }$ . It is best possible in that if $H$ is nonplanar, then $M H$ does not have the Erd˝os-P´osa property (Exercise 39).

We conclude this section with statements of the structure theorems for the graphs not containing a given complete graph as a minor. These are far more difficult to prove than any of the results we have seen so far, and they are not even that easy to state. But it’s worth an effort: the statement of the excluded- $K ^ { n }$ theorem is interesting, it is central to the proof of the graph minor theorem, and it can be applied elsewhere.

The torsos of a tree-decomposition $( T , ( V _ { t } ) _ { t \in T } )$ of a graph $G$ are the graphs $H _ { t }$ ( $t \in T$ ) obtained from $G \left[ V _ { t } \right]$ by adding all the edges $x y$ such that $x , y \in V _ { t } \cap V _ { t ^ { \prime } }$ for some neighbour $t ^ { \prime }$ of $t$ in $T$ . (Thus, if a tree-decomposition happens to be simplicial, its torsos are just its parts.)

A linear decomposition of $G$ is a family $( V _ { i } ) _ { i \in I }$ of vertex sets indexed by some linear order $I$ such that $\textstyle \bigcup _ { i \in I } V _ { i } = V ( G )$ , every edge of $G$ has both its ends in some $V _ { i }$ , and $V _ { i } \cap V _ { k } \subseteq V _ { j }$ whenever $i < j < k$ . When $G$ is finite, this is just a tree-decomposition whose decomposition tree is a path, and usually called a path-decomposition. If each $V _ { i }$ contains at most $k$ vertices and $k$ is minimal with this property, then $( V _ { i } ) _ { i \in I }$ has width $k - 1$ .

Let $S ^ { \prime }$ be a subspace of a surface6 $S$ obtained by removing the interiors of finitely many disjoint closed discs, with boundary circles $C _ { 1 } , \ldots , C _ { k }$ say. This space is determined up to homeomorphism by $S$ and the number $k$ , and we denote it by $S - k$ . Each $C _ { i }$ is the image of a continuous map $f _ { i }$ : $[ 0 , 1 ] \to S ^ { \prime }$ that is injective except for $f _ { i } ( 0 ) = f _ { i } ( 1 )$ . We call $C _ { 1 } , \ldots , C _ { k }$ the cuffs of $S ^ { \prime }$ and the points $f _ { 1 } ( 0 ) , \dots , f _ { k } ( 0 )$ their roots. The other points of each $C _ { i }$ are linearly ordered by $f _ { i }$ as images of $( 0 , 1 )$ ; when we use cuffs as index sets for linear decompositions below, we shall be referring to these linear orders.

$k$ -near embedding

Let $H$ be a graph, $S$ a surface, and $k \in \mathbb N$ . We say that $H$ is $k$ -nearly embeddable in $S$ if $H$ has a set $X$ of at most $k$ vertices such that $H - X$ can be written as $H _ { 0 } \cup H _ { 1 } \cup . . . \cup H _ { k }$ so that

(N1) there exists an embedding $\sigma \colon H _ { 0 } \hookrightarrow S - k$ that maps only vertices to cuffs and no vertex to the root of a cuff;   
(N2) the graphs $H _ { 1 } , \ldots , H _ { k }$ are pairwise disjoint (and may be empty), and $H _ { 0 } \cap H _ { i } = \sigma ^ { - 1 } ( C _ { i } )$ for each $i$ ;   
(N3) every $H _ { i }$ with $i \geqslant 1$ has a linear decomposition $\left( V _ { z } ^ { i } \right) _ { z \in C _ { i } \cap \sigma ( H _ { 0 } ) }$ of width at most $k$ such that $z \in V _ { z } ^ { i }$ for all $z$ .

Here, then, is the structure theorem for the graphs without a $K ^ { n }$ minor:

# Theorem 12.4.11. (Robertson & Seymour 2003)

For every $\textit { n } \in  { \mathbb { N } }$ there exists a $k ~ \in ~ \mathbb { N }$ such that every graph $G$ not containing $K ^ { n }$ as a minor has a tree-decomposition whose torsos are $k$ -nearly embeddable in a surface in which $K ^ { n }$ is not embeddable.

Note that there are only finitely many surfaces in which $K ^ { n }$ is not embeddable. The set of those surfaces in the statement of Theorem 12.4.11 could therefore be replaced by just two surfaces: the orientable and the non-orientable surface of maximum genus in this set. Note also that the separators $V _ { t } \cap V _ { t ^ { \prime } }$ in the tree-decomposition of $G$ (for edges $t t ^ { \prime }$ of the decomposition tree) have bounded size, e.g. at most $2 k + n$ , because they induce complete subgraphs in the torsos and these are $k$ - nearly embeddable in one of those two surfaces.

We remark that Theorem 12.4.11 has only a qualitative converse: graphs that admit a decomposition as described can clearly have a $K ^ { n }$ minor, but there exists an integer $r$ depending only on $n$ such that none of them has a $K ^ { r }$ minor.

Theorem 12.4.11, as stated above, is true also for infinite graphs (Diestel & Thomas 1999). There are also structure theorems for excluding infinite minors, and we state two of these.

First, the structure theorem for excluding $K ^ { \aleph _ { 0 } }$ . Call a graph $H$ nearly planar if $H$ has a finite set $X$ of vertices such that $H - X$ can be written as $H _ { 0 } \cup H _ { 1 }$ so that (N1–2) hold with $S \ = \ S ^ { 2 }$ (the sphere) and $k = 1$ , while (N3) holds with $k = | X |$ . (In other words, deleting a bounded number of vertices makes $H$ planar except for a subgraph of bounded linear width sewn on to the unique cuff of $S ^ { 2 } - 1$ .) A treedecomposition $( T , ( V _ { t } ) _ { t \in T } )$ of a graph $G$ has finite adhesion if for every edge $t t ^ { \prime } \in T$ the set $V _ { t } \cap V _ { t ^ { \prime } }$ is finite and for every infinite path $t _ { 1 } t _ { 2 } \ldots$ - in $T$ the value of $\operatorname* { l i m } \operatorname* { i n f } _ { i \to \infty } | V _ { t _ { i } } \cap V _ { t _ { i + 1 } } |$ is finite.

Unlike its counterpart for $K ^ { n }$ , the excluded- $K ^ { \aleph _ { 0 } }$ structure theorem has a direct converse. It thus characterizes the graphs without a $K ^ { \aleph _ { 0 } }$ minor, as follows:

Theorem 12.4.12. (Diestel, Robertson, Seymour & Thomas 1995–99) $A$ graph $G$ has no $K ^ { \aleph _ { 0 } }$ minor if and only if $G$ has a tree-decomposition of finite adhesion whose torsos are nearly planar.

Finally, a structure theorem for excluding $K ^ { \aleph _ { 0 } }$ as a topological minor. Let us say that $G$ has finite tree-width if $G$ admits a tree-decomposition $( T , ( V _ { t } ) _ { t \in T } )$ into finite parts such that for every infinite path $t _ { 1 } t _ { 2 } \ldots$ - in $T$ the set $\textstyle \bigcup _ { j \geqslant 1 } \lceil \big \rceil _ { i \geqslant j } V _ { t _ { i } }$ is finite.

Theorem 12.4.13. (Diestel, Robertson, Seymour & Thomas 1992–94) The following assertions are equivalent for connected graphs $G$ :

(i) $G$ does not contain $K ^ { \aleph _ { 0 } }$ as a topological minor;   
(ii) $G$ has finite tree-width;   
(iii) $G$ has a normal spanning tree $T$ such that for every ray $R$ in $T$ there are only finitely many vertices $\boldsymbol { v }$ that can be linked to $R$ by infinitely many paths meeting pairwise only in $v$ .

# 12.5 The graph minor theorem

Graph properties that are closed under taking minors occur frequently in graph theory. Among the most natural examples are the properties of being embeddable in some fixed surface, such as planarity.

By Kuratowski’s theorem, planarity can be expressed by forbidding the minors $K ^ { 5 }$ and $K _ { 3 , 3 }$ . This is a good characterization of planarity in the following sense. Suppose we wish to persuade someone that a certain graph is planar: this is easy (at least intuitively) if we can produce a drawing of the graph. But how do we persuade someone that a graph is non-planar? By Kuratowski’s theorem, there is also an easy way to do that: we just have to exhibit an $M K ^ { 5 }$ or $M K _ { 3 , 3 }$ in our graph, as an easily checked ‘certificate’ for non-planarity. Our simple Proposition 12.4.2 is another example of a good characterization: if a graph has tree width $< 3$ , we can prove this by exhibiting a suitable tree-decomposition; if not, we can produce an $M K ^ { 4 }$ as evidence.

Theorems that characterize a property $\mathcal { P }$ by a set of forbidden minors are doubtless among the most attractive results in graph theory. As we saw in Proposition 12.4.1, such a characterization exists whenever $\mathcal { P }$ is minor-closed: then $\mathcal { P } = \mathrm { F o r b } _ { \prec } ( \overline { { \mathcal { P } } } )$ , where $\overline { { \mathcal { P } } }$ is the complement of $\mathcal { P }$ . However, one naturally seeks to make the set of forbidden minors as small as possible. And there is indeed a unique smallest such set: the set

$$
\mathcal {K} _ {\mathcal {P}} := \left\{H \mid H \text {i s \preccurlyeq - m i n i m a l i n} \overline {{\mathcal {P}}} \right\}
$$

satisfies $\mathcal { P } = \mathrm { F o r b } _ { \prec } ( \mathcal { K } _ { \mathcal { P } } )$ and is contained in every other set $\mathcal { H }$ such that $\mathcal { P } = \mathrm { F o r b } _ { \prec } ( \mathcal { H } )$ . We call $\scriptstyle { \mathcal { K } } _ { \mathcal { P } }$ the Kuratowski set for $\mathcal { P }$ .

Clearly, the elements of $\scriptstyle { \mathcal { K } } _ { \mathcal { P } }$ are incomparable under the minor relation $\preccurlyeq$ . Now the graph minor theorem of Robertson & Seymour says that any set of $\preccurlyeq$ -incomparable graphs must be finite:

graph minor theorem

Theorem 12.5.1. (Robertson & Seymour 1986–2004) The finite graphs are well-quasi-ordered by the minor relation .

We shall give a sketch of the proof of the graph minor theorem at the end of this section.

Corollary 12.5.2. The Kuratowski set for any minor-closed graph property is finite. 

As a special case of Corollary 12.5.2 we have, at least in principle, a Kuratowski-type theorem for every surface $S$ : the property ${ \mathcal { P } } ( S )$ of embeddability in $S$ is characterized by the finite set ${ \dot { \kappa } } _ { { \mathcal { P } } ( S ) }$ of forbidden minors.

Corollary 12.5.3. For every surface $S$ there exists a finite set of graphs $H _ { 1 } , \ldots , H _ { n }$ such that a graph is embeddable in $S$ if and only if it contains none of $H _ { 1 } , \ldots , H _ { n }$ as a minor. 

The proof of Corollary 12.5.3 does not need the full strength of the minor theorem. We shall give a direct proof, which runs as follows. The main step is to prove that the graphs in $\chi _ { \mathcal { P } ( S ) }$ do not contain arbitrarily large grids as minors (Lemma 12.5.4). Then their tree-width is bounded (Theorem 12.4.4), so $\chi _ { \mathcal { P } ( S ) }$ is well-quasi-ordered (Theorem 12.3.7) and therefore finite.

The proof of Lemma 12.5.4 gives a good impression of the interplay between graph minors and surface topology, which—by way of Theorem 12.4.11, which we could not prove here—is also one of the key ingredients of the proof of the graph minor theorem. Appendix B summarizes the necessary background on surfaces, including a lemma. For convenience (cf. Proposition 1.7.2 (ii)), we shall work with hexagonal rather than square grids.

Denote by $H ^ { r }$ the plane hexagonal grid whose dual has radius $r$ (Figure 12.5.1). The face corresponding to the central vertex of its dual is its central face. (Generally, when we speak of the faces of $H ^ { r }$ , we mean its hexagonal faces, not its outer face.) A subgrid $H ^ { k }$ of $H ^ { r }$ is canonical if their central faces coincide. We write $S _ { k }$ for the perimeter cycle of the canonical subgrid $H ^ { k }$ in $H ^ { r }$ ; for example, $S _ { 1 }$ is the hexagon bounding the central face of $H ^ { r }$ . The ring $R _ { k }$ is the subgraph of $H ^ { r }$ formed by $S _ { k }$ and $S _ { k + 1 }$ and the edges between them.

$H ^ { \prime }$ faces canonical $S _ { 1 } , \ldots , S _ { r }$ ring $R _ { k }$

![](images/c80793abddb6182938f2d993c36e8cafbc7e6f409cc0daab625061333b61eb7a.jpg)  
Fig. 12.5.1. The hexagonal grid $H ^ { 6 }$ with central face $f$ and rings $R _ { 2 }$ and $R _ { 5 }$

Lemma 12.5.4. For every surface $S$ there exists an integer $r$ such that no graph that is minimal with the property of not being embeddable in $S$ contains $H ^ { r }$ as a topological minor.

Proof . Let $G$ be a graph that cannot be embedded in $S$ and is minimal with this property. Our proof will run roughly as follows. Since $G$ is minimally not embeddable in $S$ , we can embed it in an only slightly larger surface $S ^ { \prime }$ . If $G$ contains a very large $H ^ { r }$ grid, then by Lemma B.6 some large $H ^ { m }$ subgrid will be flat in $S ^ { \prime }$ , that is, the union of its faces in $S ^ { \prime }$ will be a disc $D ^ { \prime }$ . We then pick an edge $e$ from the middle of this $H ^ { m }$ grid and embed $G - e$ in $S$ . Again by Lemma B.6, one of the rings of our $H ^ { m }$ will be flat in $S$ . In this ring we can embed the (planar) subgraph of $G$ which our first embedding had placed in $D ^ { \prime }$ ; note that this subgraph contains the edge $e$ . The rest of $G$ can then be embedded in $S$ outside this ring much as before, yielding an embedding of all of $G$ in $S$ (a contradiction).

More formally, let $\varepsilon : = \varepsilon ( S )$ denote the Euler genus of $S$ . Let $T$ be large enough that $H ^ { r }$ contains $\varepsilon + 3$ disjoint copies of $H ^ { m + 1 }$ , where $m : = 3 \varepsilon + 4$ . We show that $G$ has no $T H ^ { r }$ subgraph.

Let $\boldsymbol { e } ^ { \prime } \ : = \ : \boldsymbol { u } ^ { \prime } \boldsymbol { v } ^ { \prime }$ be any edge of $G$ , and choose an embedding $\sigma ^ { \prime }$ of $G - e ^ { \prime }$ in $S$ . Choose a face with $u ^ { \prime }$ on its boundary, and another with $v ^ { \prime }$ on its boundary. Cut a disc out of each face and add a handle between the two holes, to obtain a surface $S ^ { \prime }$ of Euler genus $\varepsilon + 2$ (Lemma B.3). Embedding $e ^ { \prime }$ along this handle, extend $\sigma ^ { \prime }$ to an embedding of $G$ in $S ^ { \prime }$ .

Suppose $G$ has a subgraph $H \ : = \ : T H ^ { \prime }$ . Let $f \colon H ^ { r } \to H$ map the vertices of $H ^ { r }$ to the corresponding branch vertices of $H$ , and its edges to the corresponding paths in $H$ between those vertices. Let us show that $H ^ { r }$ has a subgrid $H ^ { m }$ (not necessarily canonical) whose hexagonal face boundaries correspond (by $\sigma ^ { \prime } \circ f$ ) to circles in $S ^ { \prime }$ that bound disjoint open discs there.

By the choice of $r$ , we can find $\varepsilon + 3$ disjoint copies of $H ^ { m + 1 }$ in $H ^ { r }$ . The canonical subgrids $H ^ { m }$ of these $H ^ { m + 1 }$ are not only disjoint, but

(App. B)

$r , m$

$\varepsilon$

$H$

$f$

sufficiently spaced out in $H ^ { r }$ that their deletion leaves a tree $T \subseteq H ^ { r }$ that sends an edge to each of them (Figure 12.5.2). If each of these $H ^ { m }$ has a face whose boundary maps to a circle in $S ^ { \prime }$ not bounding a disc there, and $\boldsymbol { \mathscr { C } }$ denotes the set of those $\varepsilon + 3$ circles, then $S ^ { \prime } \setminus \bigcup { \mathcal { C } }$ has a component $D _ { 0 }$ whose closure meets every circle in $\boldsymbol { \mathscr { C } }$ : the component containing $( \sigma ^ { \prime } \circ f ) ( T )$ . As $\varepsilon ( S ^ { \prime } ) = \varepsilon + 2$ , this contradicts Lemma B.6.

Hence for one of our copies of $H ^ { m }$ in $H ^ { r }$ , every hexagon of $H ^ { m }$ bounds an open disc in $S ^ { \prime }$ . If these discs are not disjoint, then one of them, $D$ say, meets the boundary of another such disc. But since the frontier $C$ of $D$ separates $D$ in $S ^ { \prime }$ from the rest of $S ^ { \prime }$ , and $\sigma ^ { \prime } ( H ) \setminus C$ is connected, this means that the closure of $D$ contains the entire graph $\sigma ^ { \prime } ( H )$ . Contracting $( \sigma ^ { \prime } \circ f ) ( S _ { r } )$ in $\sigma ^ { \prime } ( H )$ now yields a 3-connected graph embedded in a disc. By Theorem 4.3.2, its faces correspond to those of $H ^ { r } / S _ { r }$ in the plane, i.e. are disjoint discs. Thus, $H ^ { m }$ exists as claimed.

From now on, we shall work with this fixed $H ^ { m }$ and will no longer consider its supergraph $H ^ { r }$ . We write $H ^ { \prime } : = f ( H ^ { m } )$ for the corresponding $T H ^ { m }$ in $G$ and $C _ { i } : = f ( S _ { i } )$ for its concentric cycles, the images of the cycles $S _ { i }$ of this $H ^ { m }$ $i = 1 , \ldots , m$ ).

![](images/14af95e5ece6b5514084a3650a790997937f06a3d10b0961eea056072d3c769c.jpg)  
Fig. 12.5.2. Disjoint copies of $H ^ { m }$ ( $m = 3$ ) linked up by a tree in the rest of $H ^ { \prime }$

e $\sigma { : } G - e \hookrightarrow S$ Pick an edge $e ~ { = } ~ u v$ of $C _ { 1 }$ , and choose an embedding $\sigma$ of $G - e$ in $S$ . As before, Lemma B.6 implies that one of the $\varepsilon + 1$ disjoint rings $k$ $R _ { 3 i + 2 }$ in $H ^ { m }$ $( i = 0 , \ldots , \varepsilon$ ), $R _ { k }$ say, has the property that its hexagons correspond (by $\sigma \circ f$ ) to circles in $S$ that bound disjoint open discs there R (Figure 12.5.3). Let $R \supseteq ( \sigma \circ f ) ( R _ { k } )$ be the closure in $S$ of the union of those discs, which is a cylinder in $S$ . One of its two boundary circles is $C$ the image under $\sigma$ of the cycle $C : = C _ { k + 1 }$ in $H ^ { \prime }$ to which $f$ maps the perimeter cycle $S _ { k + 1 }$ of our special ring $R _ { k }$ in $H ^ { m }$ .   
$H ^ { \prime \prime }$ Let $H ^ { \prime \prime } : = f ( H ^ { k + 1 } ) \subseteq G$ , where $H ^ { k + 1 }$ is canonical in $H ^ { m }$ . Recall that $\sigma ^ { \prime } \circ f$ maps the hexagons of $H ^ { k + 1 }$ to circles in $S ^ { \prime }$ bounding disjoint open discs there. The closure in $S ^ { \prime }$ of the union of these discs is a disc

![](images/9740d0bc2d1ce6cd1b79947d017eb084ad8e5eab8b8143e39cf89ee69bb73af6.jpg)  
Fig. 12.5.3. A tree linking up hexagons selected from the rings $R _ { 2 } , R _ { 5 } , R _ { 8 } \ldots$ .

$D ^ { \prime }$ in $S ^ { \prime }$ , bounded by $\sigma ^ { \prime } ( C )$ . Deleting a small open disc inside $D ^ { \prime }$ that does not meet $\sigma ^ { \prime } ( G )$ , we obtain a cylinder $R ^ { \prime } \subseteq S ^ { \prime }$ that contains $\sigma ^ { \prime } ( H ^ { \prime \prime } )$ .

We shall now combine the embeddings $\sigma { : } G - e \hookrightarrow S$ and $\sigma ^ { \prime } { : } G \hookrightarrow S ^ { \prime }$ to an embedding $\sigma ^ { \prime \prime } { \colon } G \ \hookrightarrow \ S$ , which will contradict the choice of $G$ . Let $\varphi : \sigma ^ { \prime } ( C ) \to \sigma ( C )$ be a homeomorphism between the images of $C$ in $S ^ { \prime }$ and in $S$ that commutes with these embeddings, i.e., is such that $\sigma | _ { C } = ( \varphi \circ \sigma ^ { \prime } ) | _ { C }$ . Then extend this to a homeomorphism $\varphi \colon R ^ { \prime } \to R$ . The idea now is to define $\sigma ^ { \prime \prime }$ as $\varphi \circ \sigma ^ { \prime }$ on the part of $G$ which $\sigma ^ { \prime }$ maps to $D ^ { \prime }$ (which includes the edge $e$ on which $\sigma$ is undefined), and as $\sigma$ on the rest of $G$ (Fig. 12.5.4).

![](images/f7489709f04e3ab242dd2e4e58c3ffab74af2946e358d3b35d20aedafd748d76.jpg)  
Fig. 12.5.4. Combining $\sigma ^ { \prime } { : } G \hookrightarrow S ^ { \prime }$ and $\sigma { : } G - e \hookrightarrow S$ to $\sigma ^ { \prime \prime } { : } G \hookrightarrow \mathrm { S }$

To make these two partial maps compatible, we start by defining $\sigma ^ { \prime \prime }$ on $C$ as $\sigma | _ { C } = ( \varphi \circ \sigma ^ { \prime } ) | _ { C }$ . Next, we define $\sigma ^ { \prime \prime }$ separately on the components of $G - C$ . Since $\sigma ^ { \prime } ( C )$ bounds the disc $D ^ { \prime }$ in $S ^ { \prime }$ , we know that $\sigma ^ { \prime }$ maps each component $J$ of $G - C$ either entirely to $D ^ { \prime }$ or entirely to $S ^ { \prime } \setminus D ^ { \prime }$ . On all the components $J$ such that $\sigma ^ { \prime } ( J ) \subseteq D ^ { \prime }$ , and on all the edges they send to $G$ , we define $\sigma ^ { \prime \prime }$ as $\varphi \circ \sigma ^ { \prime }$ . Thus, $\sigma ^ { \prime \prime }$ embeds these components in $R$ . Since $e \in f ( H ^ { k } ) = H ^ { \prime \prime } - C$ , this includes the

component of $G - C$ that contains $e$

It remains to define $\sigma ^ { \prime \prime }$ on the components of $G - C$ which $\sigma ^ { \prime }$ maps to $S ^ { \prime } \setminus D ^ { \prime }$ . As $\sigma ^ { \prime } ( C _ { k } ) \subseteq D ^ { \prime }$ , these do not meet $C _ { k }$ . Since $\sigma ( C \cup C _ { k } )$ is the frontier of $R$ in $S$ , this means that $\sigma ( J ) \subseteq S \setminus R$ or $\sigma ( J ) \subseteq R$ for every such component $J$ .

For the component $J _ { 0 }$ of $G - C$ that contains $C _ { k + 2 }$ we cannot have $\sigma ( J _ { 0 } ) \subseteq R$ : as $S _ { k + 2 } \cap R _ { k } = \emptyset$ , this would mean that $\sigma ( C _ { k + 2 } )$ lies in a disc $D \subseteq R$ corresponding to a face of $R _ { k }$ , which is impossible since $S _ { k + 2 }$ sends edges to vertices of $S _ { k + 1 }$ outside the boundary of that face. We thus have $\sigma ( J _ { 0 } ) \subseteq S \setminus R$ , and define $\sigma ^ { \prime \prime }$ as $\sigma$ on $J _ { 0 }$ and on all the $J _ { 0 }$ – $C$ edges of $G$ .

Next, consider any remaining component $J$ of $G - C$ that sends no edge to $C$ . If $\sigma ( J ) \subseteq S \setminus R$ , we define $\sigma ^ { \prime \prime }$ on $J$ as $\sigma$ . If $\sigma ( J ) \subseteq R$ , then $J$ is planar. Since $J$ sends no edge to $C$ , we can have $\sigma ^ { \prime \prime }$ map $J$ to any open disc in $R$ that has not so far been used by $\sigma ^ { \prime \prime }$ .

It remains to define $\sigma ^ { \prime \prime }$ on the components $J \ne J _ { 0 }$ of $G - C$ which $\sigma ^ { \prime }$ maps to $S ^ { \prime } \backslash D ^ { \prime }$ and for which $G$ contains a $J$ – $C$ edge. Let $\mathcal { I }$ be the set of all those components $J$ . We shall group them by the way they attach to $C$ , and define $\sigma ^ { \prime \prime }$ for these groups in turn.

Since $m \geqslant k + 2$ , the disc $D ^ { \prime }$ lies inside a larger disc in $S ^ { \prime }$ , which is the union of $D ^ { \prime }$ and closed discs $D ^ { \prime \prime }$ bounded by the images under $\sigma ^ { \prime } \circ f$ of the hexagons in $R _ { k + 1 }$ . By definition of $\mathcal { I }$ , the embedding $\sigma ^ { \prime }$ maps every $\textit { J } \in \textit { J }$ to such a disc $D ^ { \prime \prime }$ (Fig. 12.5.5). On the path $P$ in $C$ such that $\sigma ^ { \prime } ( P ) = \sigma ^ { \prime } ( C ) \cap D ^ { \prime \prime }$ (which is the image under $f$ of one or two consecutive edges on $S _ { k + 1 }$ ), let $v _ { 1 } , \ldots , v _ { n }$ be the vertices with a neighbour in $J _ { 0 }$ , in their natural order along $P$ , and write $P _ { i }$ for the segment of $P$ from $v _ { i }$ to $v _ { i + 1 }$ . For any $v _ { i }$ with $1 < i < n$ , pick a $v _ { i } - J _ { 0 }$ edge and extend it through $J _ { 0 }$ to a path $Q$ from $v _ { i }$ to $C _ { k + 2 }$ (which exists by definition of $J _ { 0 }$ ); let $w$ be its first vertex that $\sigma ^ { \prime }$ maps to the boundary circle of $D ^ { \prime \prime }$ . By Lemma 4.1.2 applied to $\sigma ^ { \prime } ( v _ { i } Q w )$ and the two arcs joining $\sigma ^ { \prime } ( v _ { i } )$ to $\sigma ^ { \prime } ( w )$ along the boundary circle of $D ^ { \prime \prime }$ , there is no arc through $D ^ { \prime \prime }$ that links $\sigma ^ { \prime } ( P _ { i - 1 } )$ to $\sigma ^ { \prime } ( P _ { i } )$ but avoids $\sigma ^ { \prime } ( v _ { i } Q w )$ . Hence, every $J \in \mathcal { I }$ with $\sigma ^ { \prime } ( J ) \subseteq D ^ { \prime \prime }$ has all its neighbours on $C$ in the same $P _ { i }$ , and $\sigma ^ { \prime }$ maps $J$ to the face $f _ { i }$ of the plane graph $\sigma ^ { \prime } ( G [ J _ { 0 } \cup C ] ) \cap D ^ { \prime \prime }$ whose boundary contains $P _ { i }$ . We shall define $\sigma ^ { \prime \prime }$ jointly on all those $J \in \mathcal { I }$ which $\sigma ^ { \prime }$ maps to this $f _ { i }$ , for $i = 1 , \ldots , n - 1$ in turn.

To do so, we choose an open disc $D _ { i }$ in $S \setminus R$ that has a boundary circle containing $\sigma ( P _ { i } )$ and avoids the image of $\sigma ^ { \prime \prime }$ as defined until now. Such $D _ { i }$ exists in a strip neighbourhood of $\sigma ( C )$ in $S$ , because components $J ^ { \prime } \in \mathcal { T }$ attaching to a segment $P _ { j } \neq P _ { i }$ of $C$ send no edge to ${ \dot { P } } _ { i }$ . Choose a homeomorphism $\varphi _ { i }$ from the boundary circle of $f _ { i }$ to that of $D _ { i }$ so that $\sigma | _ { P _ { i } } = ( \varphi _ { i } \circ \sigma ^ { \prime } ) | _ { P _ { i } }$ , and extend this to a homeomorphism $\varphi _ { i }$ from the closure of $f _ { i }$ in $S ^ { \prime }$ to the closure of $D _ { i }$ in $S$ . For every $J \in \mathcal { I }$ with $\sigma ^ { \prime } ( J ) \subseteq f _ { i }$ , and for all $J$ – $C$ edges of $G$ , define $\sigma ^ { \prime \prime }$ as $\varphi _ { i } \circ \sigma ^ { \prime }$ . 

![](images/039bafe6b6934585df00c5c384d68be836ce129883f24e61ac3af52237d52f9b.jpg)  
Fig. 12.5.5. Define $\sigma ^ { \prime \prime }$ jointly for the components $J , J ^ { \prime } \in \mathcal { I }$ that attach to the same $P _ { i } \subseteq C$

(1.7.2) Proof of Corollary 12.5.3. By their minimality, the graphs in (12.3.7) ${ \dot { \kappa } } _ { { \mathcal { P } } ( S ) }$ are incomparable under the minor-relation. If their tree-width is (12.4.4) bounded, then $\chi _ { \mathcal { P } ( S ) }$ is well-quasi-ordered by the minor relation (Theorem 12.3.7), and hence must be finite. So assume their tree-width is unbounded, and let $r$ be as in Lemma 12.5.4. By Theorem 12.4.4, some $H \in { \mathcal { K } } _ { { \mathcal { P } } ( S ) }$ has a grid minor large enough to contain $H ^ { \prime }$ . By Proposition 1.7.2, $H ^ { r }$ is a topological minor of $H$ , contrary to the choice of $r$ . 

We finally come to the proof of the graph minor theorem itself. The complete proof would still fill a book or two, but we are well equipped now to get a good understanding of its main ideas and overall structure. For background on surfaces, we once more refer to Appendix B.

(12.1.3) Proof of the graph minor theorem (sketch). We have to show that (12.2.1)(12.3.7) every infinite sequence (12.4.4)

$$
G _ {0}, G _ {1}, G _ {2}, \ldots
$$

of finite graphs contains a good pair: two graphs $G _ { i } \preccurlyeq G _ { j }$ with $i < j$ . We may assume that $G _ { 0 } \not \prec G _ { i }$ for all $i \geqslant 1$ , since $G _ { 0 }$ forms a good pair with any graph $G _ { i }$ of which it is a minor. Thus all the graphs $G _ { 1 } , G _ { 2 } , \ldots$ lie in Forb $\preccurlyeq ( G _ { 0 } )$ , and we may use the structure common to these graphs in our search for a good pair.

We have already seen how this works when $G _ { 0 }$ is planar: then the graphs in Forb $\preccurlyeq ( G _ { 0 } )$ have bounded tree-width (Theorem 12.4.3) and are therefore well-quasi-ordered by Theorem 12.3.7. In general, we need only consider the cases of $G _ { 0 } = K ^ { n }$ : since $G _ { 0 } \preccurlyeq K ^ { n }$ for $n : = | G _ { 0 } |$ , we may assume that $K ^ { n } \preccurlyeq G _ { i }$ for all $i \geqslant 1$ .

The proof now follows the same lines as above: again the graphs in $\mathrm { F o r b } _ { \preccurlyeq } ( K ^ { n } )$ can be characterized by their tree-decompositions, and again their tree structure helps, as in Kruskal’s theorem, with the proof that they are well-quasi-ordered. The parts in these tree-decompositions are no longer restricted in terms of order now, but they are constrained in more subtle structural terms. Roughly speaking, for every $n$ there exists a finite set $\boldsymbol { S }$ of surfaces such that every graph without a $K ^ { n }$ minor has a tree-decomposition into parts each ‘nearly’ embeddable in one of the surfaces $S \in S$ ; see Theorem 12.4.11. By a generalization of Theorem 12.3.7—and hence of Kruskal’s theorem—it now suffices, essentially, to prove that the set of all the parts in these tree-decompositions is well-quasi-ordered: then the graphs decomposing into these parts are well-quasi-ordered, too. Since $\boldsymbol { S }$ is finite, every infinite sequence of such parts has an infinite subsequence whose members are all (nearly) embeddable in the same surface $S \in S$ . Thus all we have to show is that, given any surface $S$ , all the graphs embeddable in $S$ are well-quasiordered by the minor relation.

This is shown by induction on the Euler genus of $S$ , using the same approach as before: if $H _ { 0 } , H _ { 1 } , H _ { 2 } , . . .$ is an infinite sequence of graphs embeddable in $S$ , we may assume that none of the graphs $H _ { 1 } , H _ { 2 } , \ldots$ contains $H _ { 0 }$ as a minor. If $S = S ^ { 2 }$ we are back in the case that $H _ { 0 }$ is planar, so the induction starts. For the induction step we now assume that $S \ne S ^ { 2 }$ . Again, the exclusion of $H _ { 0 }$ as a minor constrains the structure of the graphs $H _ { 1 } , H _ { 2 } , \ldots$ , this time topologically: each $H _ { i }$ with $i \geqslant 1$ has an embedding in $S$ which meets some circle $C _ { i } \subseteq S$ that does not bound a disc in $S$ in no more than a bounded number of vertices (and no edges), say in $X _ { i } \subseteq V ( H _ { i } )$ . (The bound on $\left| X _ { i } \right|$ depends on $H _ { 0 }$ , but not on $H _ { i }$ .) Cutting along $C _ { i }$ and capping the hole(s), we obtain one or two new surfaces of smaller Euler genus. If the cut produces only one new surface $S _ { i }$ , then our embedding of $H _ { i } - X _ { i }$ still counts as a near-embedding of $H _ { i }$ in $S _ { i }$ (since $X _ { i }$ is small). If this happens for infinitely many $_ i$ , then infinitely many of the surfaces $S _ { i }$ are also the same, and the induction hypothesis gives us a good pair among the corresponding graphs $H _ { i }$ . On the other hand, if we get two surfaces $S _ { i } ^ { \prime }$ and $S _ { i } ^ { \prime \prime }$ for infinitely many $i$ (without loss of generality the same two surfaces), then $H _ { i }$ decomposes accordingly into subgraphs $H _ { i } ^ { \prime }$ and $H _ { i } ^ { \prime \prime }$ embedded in these surfaces, with $V ( H _ { i } ^ { \prime } \cap H _ { i } ^ { \prime \prime } ) = X _ { i }$ . The set of all these subgraphs taken together is again well-quasi-ordered by the induction hypothesis, and hence so are the pairs $( H _ { i } ^ { \prime } , H _ { i } ^ { \prime \prime } )$ by Lemma 12.1.3. Using a sharpening of the lemma that takes into account not only the graphs $H _ { i } ^ { \prime }$ and $H _ { i } ^ { \prime \prime }$ themselves but also how $X _ { i }$ lies inside them, we finally obtain indices $i , j$ not only with $H _ { i } ^ { \prime } \preccurlyeq H _ { j } ^ { \prime }$ and $H _ { i } ^ { \prime \prime } \preccurlyeq H _ { j } ^ { \prime \prime }$ , but also such that these minor embeddings extend to the desired minor embedding of $H _ { i }$ in $H _ { j }$ —completing the proof of the graph minor theorem.

The graph minor theorem does not extend to graphs of arbitrary cardinality, but it might extend to countable graphs. Whether or not it does appears to be a difficult problem. It may be related to the following intriguing conjecture, which easily implies the graph minor theorem for finite graphs (Exercise 44). Call a graph $H$ a proper minor of $G$ if $G$ contains an $M H$ with at least one non-trivial branch set.

# Self-minor conjecture. (Seymour 1980s)

Every countably infinite graph is a proper minor of itself.

In addition to its impact on ‘pure’ graph theory, the graph minor theorem has had far-reaching algorithmic consequences. Using their structure theorem for the graphs in Forb $\preccurlyeq ( K ^ { n } )$ , Theorem 12.4.11, Robertson and Seymour have shown that testing for any fixed minor is ‘fast’: for every graph $H$ there is a polynomial-time algorithm7 that decides whether or not the input graph contains $H$ as a minor. By the minor theorem, then, every minor-closed graph property $\mathcal { P }$ can be decided in polynomial (even cubic) time: if $K _ { \mathcal { P } } = \{ H _ { 1 } , \ldots , H _ { k } \}$ is the corresponding set of forbidden minors, then testing a graph $G$ for membership in $\mathcal { P }$ reduces to testing the $k$ assertions $H _ { i } \prec G$ .

The following example gives an indication of how deeply this algorithmic corollary affects the complexity theory of graph algorithms. Let us call a graph knotless if it can be embedded in $\mathbb { R } ^ { 3 }$ so that none of its cycles forms a non-trivial knot. Before the graph minor theorem, it was an open problem whether knotlessness is decidable, that is, whether any algorithm exists (no matter how slow) that decides for any given graph whether or not that graph is knotless. To this day, no such algorithm is known. The property of knotlessness, however, is easily ‘seen’ to be closed under taking minors: contracting an edge of a graph embedded in 3-space will not create a knot where none had been before. Hence, by the minor theorem, there exists an algorithm that decides knotlessness— even in polynomial (cubic) time!

However spectacular such unexpected solutions to long-standing problems may be, viewing the graph minor theorem merely in terms of its corollaries will not do it justice. At least as important are the techniques developed for its proof, the various ways in which minors are handled or constructed. Most of these have not even been touched upon here, yet they seem set to influence the development of graph theory for many years to come.

# Exercises

1. $-$ Let $\leqslant$ be a quasi-ordering on a set $X$ . Call two elements $x , y \in X$ equivalent if both $x \ \leqslant \ y$ and $y \ \leqslant \ x$ . Show that this is indeed an equivalence relation on $X$ , and that $\leqslant$ induces a partial ordering on the set of equivalence classes.

2. Let $( A , \leqslant )$ be a quasi-ordering. For subsets $X \subseteq A$ write

$$
\operatorname {F o r b} _ {\leqslant} (X) := \left\{a \in A \mid a \geqslant x \text {f o r a l l} x \in X \right\}.
$$

Show that $\leqslant$ is a well-quasi-ordering on $A$ if and only if every subset $B \subseteq A$ that is closed under $\geqslant$ (i.e. such that $x \leqslant y \in B \Rightarrow x \in B$ ) can be written as $B = \operatorname { F o r b } _ { \leqslant } ( X )$ with finite $X$ .

3. Prove Proposition 12.1.1 and Corollary 12.1.2 directly, without using Ramsey’s theorem.   
4. Given a quasi-ordering $( X , \leqslant )$ and subsets $A , B \subseteq X$ , write $A \leqslant ^ { \prime } B$ if there exists an order preserving injection $f \colon A \to B$ with $a \leqslant f ( a )$ for all $a \in A$ . Does Lemma 12.1.3 still hold if the quasi-ordering considered for $[ X ] ^ { < \omega }$ is $\leqslant ^ { \prime }$ ?   
5.− Show that the relation $\leqslant$ between rooted trees defined in the text is indeed a quasi-ordering.   
6. Show that the finite trees are not well-quasi-ordered by the subgraph relation.   
7. The last step of the proof of Kruskal’s theorem considers a ‘topological’ embedding of $T _ { m }$ in $T _ { n }$ that maps the root of $T _ { m }$ to the root of $T _ { n }$ . Suppose we assume inductively that the trees of $A _ { m }$ are embedded in the trees of $A _ { n }$ in the same way, with roots mapped to roots. We thus seem to obtain a proof that the finite rooted trees are well-quasi-ordered by the subgraph relation, even with roots mapped to roots. Where is the error?   
8. Are the connected finite graphs well-quasi-ordered by contraction alone (i.e. by taking minors without deleting edges or vertices)?   
9.+ Relax the minor relation by not insisting that branch sets be connected. Show that the finite graphs are well-quasi-ordered by this relation.   
10.+ Show that the finite graphs are not well-quasi-ordered by the topological minor relation.   
11. $^ +$ Given $k \in \mathbb N$ , is the class $\{ G \ | \ G \ \nsupseteq { P ^ { k } } \}$ well-quasi-ordered by the subgraph relation?   
12.− Let $G$ be a graph, $T$ a tree, and $\mathcal { V } = ( V _ { t } ) _ { t \in T }$ a family of subsets of $V ( G )$ . Show that $( T , \nu )$ is a tree-decomposition of $G$ if and only if

(i) for every $v \in V ( G )$ the set $T _ { v } : = \{ t \mid v \in V _ { t } \}$ induces a subtree of $T$ ;

(ii) $T _ { u } \cap T _ { v } \neq \emptyset$ for every edge uv of $G$ .

13.− Show that every graph $G$ has a tree-decomposition of width $\operatorname { t w } ( G )$ in which no part contains another.   
14. Show that a graph has tree-width at most 1 if and only if it is a forest.   
15. Let $G$ be a graph, $T$ a set, and $( V _ { t } ) _ { t \in T }$ a family of subsets of $V ( G )$ satisfying (T1) and (T2) from the definition of a tree-decomposition. Show that there exists a tree on $T$ that makes (T3) true if and only if there exists an enumeration $t _ { 1 } , \ldots , t _ { n }$ of $T$ such that for every $k = 2 , \ldots , n$ there is a $j < k$ satisfying $\begin{array} { r } { V _ { t _ { k } } \cap \bigcup _ { i < k } V _ { t _ { i } } \subseteq V _ { t _ { j } } } \end{array}$ .

(The new condition tends to be more convenient to check than (T3). It can help, for example, with the construction of a tree-decomposition into a given set of parts.)

16. Prove the following converse of Lemma 12.3.1: if $( T , \nu )$ satisfies condition (T1) and the statement of the lemma, then $( T , \nu )$ is a treedecomposition of $G$ .   
17. Can the tree-width of a subdivision of a graph $G$ be smaller than $\operatorname { t w } ( G )$ ? Can it be larger?   
18.+ Show that if a graph has circumference $k \neq 0$ , then its tree-width is at most $k - 1$ .   
19. Call two separations $\{ U _ { 1 } , U _ { 2 } \}$ and $\{ W _ { 1 } , W _ { 2 } \}$ of $G$ compatible if we can choose $i , j \in \{ 1 , 2 \}$ so that $U _ { i } \subseteq W _ { j }$ and $U _ { 3 - i } \supseteq W _ { 3 - j }$ .

(i) Show that the separations $S _ { e } : = \{ U _ { 1 } , U _ { 2 } \}$ in Lemma 12.3.1 are compatible for different choices of the edge $e = t _ { 1 } t _ { 2 } \in T ^ { \prime }$ .   
(ii) $^ +$ Conversely, show that given a set $\boldsymbol { S }$ of compatible separations of $G$ there is a tree-decomposition $( \nu , T )$ of $G$ such that $s =$ $\{ S _ { e } \mid e \in E ( T ) \}$ .

20.+ Show that every 2-connected graph has a tree-decomposition $( T , ( V _ { t } ) _ { t \in T } )$ such that $| V _ { t } \cap V _ { t ^ { \prime } } | = 2$ for every edge $t t ^ { \prime } \in T$ and all torsos are either 3-connected or a cycle. Conversely, show that every graph with such a tree-decomposition is 2-connected.

(Hint. Try a tree-decomposition defined, as in Exercise 19 (ii), by the set of all 2-separations (separations $\{ U _ { 1 } , U _ { 2 } \}$ such that $| U _ { 1 } \cap U _ { 2 } | = 2$ ) that are compatible with all other 2-separations.)

21. Apply Theorem 12.3.9 to show that the $k \times k$ grid has tree-width at least $k$ , and find a tree-decomposition of width exactly $k$ .   
22. Let $_ { \beta }$ be a maximum-order bramble in a graph $G$ . Show that every minimum-width tree-decomposition of $G$ has a unique part covering $\boldsymbol { B }$ .   
23. $^ +$ In the second half of the proof of Theorem 12.3.9, let $H ^ { \prime }$ be the union of $H$ and the paths $P _ { 1 } , \ldots , P _ { \ell }$ , let $H ^ { \prime \prime }$ be the graph obtained from $H ^ { \prime }$ by contracting each $P _ { i }$ , and let $( T , \left( W _ { t } ^ { \prime \prime } \right) _ { t \in T } )$ be the tree-decomposition induced on $H ^ { \prime \prime }$ (as in Lemma 12.3.3) by the decomposition that $( T , ( V _ { t } ) _ { t \in T } )$ induces on $H ^ { \prime }$ . Is this, after the obvious identification of $H ^ { \prime \prime }$ with $H$ , the same decomposition as the one used in the proof, i.e. is $W _ { t } ^ { \prime \prime } = W _ { t }$ for all $t \in \mathbf { \Gamma } _ { 2 } ^ { \prime }$ ?

24. $-$ Show that any graph with a simplicial tree-decomposition into $k$ - colourable parts is itself $k$ -colourable.   
25. Let $\mathcal { H }$ be a set of graphs, and let $G$ be constructed recursively from elements of $\mathcal { H }$ by pasting along complete subgraphs. Show that $G$ has a simplicial tree-decomposition into elements of $\mathcal { H }$ .   
26. Use the previous exercise to show that $G$ has no $K ^ { 5 }$ minor if and only if $G$ has a tree-decomposition in which every torso is either planar or a copy of the Wagner graph $W$ (Figure 7.3.1).   
27.+ Call a graph irreducible if it is not separated by any complete subgraph. Every finite graph $G$ can be decomposed into irreducible induced subgraphs, as follows. If $G$ has a separating complete subgraph $S$ , then decompose $G$ into proper induced subgraphs $G ^ { \prime }$ and $G ^ { \prime \prime }$ with $G = G ^ { \prime } \cup G ^ { \prime \prime }$ and $G ^ { \prime } \cap G ^ { \prime \prime } = S$ . Then decompose $G ^ { \prime }$ and $G ^ { \prime \prime }$ in the same way, and so on, until all the graphs obtained are irreducible. By Exercise 25, $G$ has a simplicial tree-decomposition into these irreducible subgraphs. Show that they are uniquely determined if the complete separators were all chosen minimal.   
28. If $\mathcal { F }$ is a family of sets, then the graph $G$ on $\mathcal { F }$ with $X Y \in E ( G ) \Leftrightarrow$ $X \cap Y \neq \emptyset$ is called the intersection graph of $\mathcal { F }$ . Show that a graph is chordal if and only if it is isomorphic to the intersection graph of a family of (vertex sets of) subtrees of a tree.   
29. Show that a graph has a path-decomposition into complete graphs if and only if it is isomorphic to an interval graph. (Interval graphs are defined in Ex. 39, Ch. 5.)   
30. (continued)

The path-width $\operatorname { p w } ( G )$ of a graph $G$ is the least width of a path-decomposition of $G$ . Prove the following analogue of Corollary 12.3.12 for path-width: every graph $G$ satisfies $\operatorname { p w } ( G ) = \operatorname* { m i n } \omega ( H ) - 1$ , where the minimum is taken over all interval graphs $H$ containing $G$ .

31. $^ +$ Do trees have unbounded path-width?   
32. $-$ Let $\mathcal { P }$ be a minor-closed graph property. Show that strengthening the notion of a minor (for example, to that of topological minor) increases the set of forbidden minors required to characterize $\mathcal { P }$ .   
33. Deduce from the minor theorem that every minor-closed property can be expressed by forbidding finitely many topological minors. Is the same true for every property that is closed under taking topological minors?   
34. $-$ Show that every horizontal path in the $k \times k$ grid is externally $k$ - connected in that grid.   
35.+ Show that the tree-width of a graph is large if and only if it contains a large externally $k$ -connected set of vertices, with $k$ large. For example, show that graphs of tree-width $< k$ contain no externally $( k + 1 )$ - connected set of $_ { 3 k }$ vertices, and that graphs containing no externally $( k + 1 )$ -connected set of $3 k$ vertices have tree-width $< 4 k$ .

36.+ (continued)

Find an $\mathbb { N } \to \mathbb { N } ^ { 2 }$ function $k \mapsto ( h , \ell )$ such that every graph with an externally -connected set of $h$ vertices contains a bramble of order at least $k$ . Deduce the weakening of Theorem 12.3.9 that, given $k$ , every graph of large enough tree-width contains a bramble of order at least $k$ .

A tangle of order $k \in \mathbb N$ in a graph $G = ( V , E )$ is a set $\boldsymbol { \tau }$ of ordered pairs $( A , B )$ of subsets of $V$ satisfying the following conditions.

( 1) For every $( A , B ) \in \mathcal { T }$ , the 2-set $\{ A , B \}$ is a separation in $G$ or order $< k$   
$( \tau 2 )$ For every separation $\{ A , B \}$ of order $< k$ in $G$ , at least one of $( A , B )$ , $( B , A )$ is an element of $\boldsymbol { \mathcal { T } }$ .   
$( \mathcal T 3 )$ If $( A _ { 1 } , B _ { 1 } ) , ( A _ { 2 } , B _ { 2 } ) , ( A _ { 3 } , B _ { 3 } ) \in \mathcal { T }$ then $A _ { 1 } \cup A _ { 2 } \cup A _ { 3 } \neq V$ .   
( 4) No $( A , B ) \in \mathcal { T }$ is such that $A = V$ .   
37. Deduce from Exercise 35 that every graph of tree-width at least $4 k$ has a tangle of order $k$ .   
38. Extend Corollary 12.4.10 as follows. Let $H$ be a connected planar graph, let $\mathcal { X }$ be any set of connected graphs including $H$ , and let $\mathcal { H } : =$ $\{ M X \mid X \in { \mathcal { X } } \}$ . Show that $\mathcal { H }$ has the Erd˝os-P´osa property, witnessed by the same function $f$ as defined in the proof of Corollary 12.4.10. Explain how it is possible that $f$ depends on $H$ but not on any of the other graphs in $\mathcal { X }$ .   
39.+ Show that, for every non-planar graph $H$ , the class $M H$ fails to have the Erd˝os-P´osa property.

(Hint. Embed $H$ in a surface $S$ , and consider only graphs embedded in $S$ .)

40.+ Extend Corollary 12.4.10 to disconnected graphs $H$ , or find a counterexample.   
41.+ Show that the four ingredients to the structure of the graphs in Forb $\preccurlyeq ( K ^ { n } )$ as described in Theorem 12.4.11—tree-decomposition, an apex set $X$ , genus, and vortices $H _ { 1 } , \ldots , H _ { k }$ —are all needed to capture all the graphs in $\mathrm { F o r b } _ { \prec } ( K ^ { n } )$ . More precisely, find examples of graphs in $\mathrm { F o r b } _ { \prec } ( K ^ { n } )$ showing that Theorem 12.4.11 becomes false if we require in addition that the tree-decomposition has only one part, or that $X$ is always empty, or that $S$ is always the sphere, or that $H _ { 1 } , \ldots , H _ { k }$ are always empty. No exact proofs are required.   
42. Without using the minor theorem, show that the chromatic number of the graphs in any $\preccurlyeq$ -antichain is bounded.   
43. Let $S _ { g }$ denote the surface obtained from the sphere by adding $_ { g }$ handles. Find a lower bound for $| K _ { \mathcal { P } ( S ) } |$ in terms of $g$ .

(Hint. The smallest $_ { g }$ such that a given graph can be embedded in $S _ { g }$ r is its orientable genus. Use the theorem that the orientable genus of a graph is equal to the sum of the genera of its blocks.)

44. Deduce the graph minor theorem from the self-minor conjecture.

45. Prove Theorem 12.4.13, assuming that $G$ has a normal spanning tree.   
46. Let $G$ be a locally finite graph obtained from the $\mathbb { Z } \times \mathbb { Z }$ grid $H$ by adding an infinite set of edges $x y$ with $d _ { H } ( x , y )$ unbounded. Show that $G \succcurlyeq K ^ { \aleph _ { 0 } }$ . Can you do the same if the distances $d _ { H } ( x , y )$ are bounded (but at least 3)?   
47. Is the infinite $\mathbb { Z } \times \mathbb { Z }$ grid a minor of the $\mathbb { Z } \times \mathbb { N }$ grid? Is the latter a minor of the $\mathbb { N } \times \mathbb { N }$ grid?   
48.+ Extend Proposition 12.3.11 to infinite graphs not containing an infinite complete subgraph.   
49. Using the previous exercise, prove that if every finite subgraph of $G$ has tree-width less than $k \in \mathbb N$ then so does $G$ :

(i) for countable $G$ , using the infinity lemma;   
(ii) $^ +$ for arbitrary $G$ , using Zorn’s lemma.

50. Show that no assumption of large finite connectivity can ensure that a countable graph has a $K ^ { \prime }$ minor when $r \geqslant 5$ . However, using the previous exercise show that sufficiently large finite connectivity forces any given planar minor.

# Notes

Kruskal’s theorem on the well-quasi-ordering of finite trees was first published in J.A. Kruskal, Well-quasi ordering, the tree theorem, and V´aszonyi’s conjecture, Trans. Amer. Math. Soc. 95 (1960), 210–225. Our proof is due to Nash-Williams, who introduced the versatile proof technique of choosing a ‘minimal bad sequence’. This technique was also used in our proof of Higman’s Lemma 12.1.3.

Nash-Williams generalized Kruskal’s theorem to infinite graphs. This extension is much more difficult than the finite case. Its proof introduces as a tool the notion of better-quasi-ordering, a concept that has profoundly influenced well-quasi-ordering theory. The graph minor theorem is false for uncountable graphs; this was shown by R. Thomas, A counterexample to ‘Wagner’s conjecture’ for infinite graphs, Math. Proc. Camb. Phil. Soc. 103 (1988), 55– 57. Whether or not the countable graphs are well-quasi-ordered as minors, and whether the finite (or the countable) graphs are better-quasi-ordered as minors, are related questions that remain wide open. Both are related also to the self-minor conjecture. This, too, was originally intended to include graphs of arbitrary cardinality, but was disproved for uncountable graphs by B. Oporowski, A counterexample to Seymour’s self-minor conjecture, J. Graph Theory 14 (1990), 521–524.

The notions of tree-decomposition and tree-width were first introduced (under different names) by R. Halin, $S$ -functions for graphs, J. Geometry 8 (1976), 171–186. Among other things, Halin showed that grids can have arbitrarily large tree-width. Robertson & Seymour reintroduced the two concepts, apparently unaware of Halin’s paper, with direct reference to K. Wagner,

Uber eine Eigenschaft der ebenen Komplexe,¨ Math. Ann. 114 (1937), 570– 590. (This is the seminal paper that introduced simplicial tree-decompositions to prove Theorem 7.3.4; cf. Exercise 26.) Simplicial tree-decompositions are treated in depth in R. Diestel, Graph Decompositions, Oxford University Press 1990.

Robertson & Seymour usually refer to the graph minor theorem as Wagner’s conjecture. Wagner did indeed discuss this problem in the 1960s with his then students, Halin and Mader, and it is not unthinkable that one of them conjectured a positive solution. Wagner himself always insisted that he did not—even after the graph minor theorem had been proved.

Robertson & Seymour’s proof of the graph minor theorem is given in the numbers IV–VII, IX–XII and XIV–XX of their series of over 20 papers under the common title of Graph Minors, most of which appeared in the Journal of Combinatorial Theory, Series $B$ , between 1983 and 2004. Of their theorems cited in this chapter, Theorem 12.3.7 is from Graph Minors IV, Theorems 12.4.3 and 12.4.4 are from Graph Minors V, and Theorem 12.4.11 is from Graph Minors XVI. Our short proof of Theorems 12.4.3 and 12.4.4 is from R. Diestel, K.Yu. Gorbunov, T.R. Jensen & C. Thomassen, Highly connected sets and the excluded grid theorem, J. Combin. Theory B 75 (1999), 61–73.

Theorem 12.3.9 is due to P.D. Seymour & R. Thomas, Graph searching and a min-max theorem for tree-width, J. Combin. Theory B 58 (1993), 22–33. Our proof is a simplification of the original proof. B.A. Reed gives an instructive introductory survey on tree-width and graph minors, including some algorithmic aspects, in (R.A. Bailey, ed) Surveys in Combinatorics 1997 , Cambridge University Press 1997, 87–162. Reed also introduced the term ‘bramble’; in Seymour & Thomas’s paper, brambles are called ‘screens’.

The obstructions to small tree-width actually used in the proof of the graph minor theorem are not brambles of large order but tangles; see Exercise 37. Tangles are more powerful than brambles and well worth studying. See Graph Minors X or Reed’s survey for an introduction to tangles and their relation to brambles and tree-decompositions.

Theorem 12.3.10 is due to R. Thomas, A Menger-like property of treewidth; the finite case, J. Combin. Theory B 48 (1990), 67–76. For a short proof see P. Bellenbaum & R. Diestel, Two short proofs concerning tree-decompositions, Combinatorics, Probability and Computing 11 (2002), 541–547.

The Kuratowski set for the graphs of tree-width $< ~ 4$ have been determined by S. Arnborg, D.G. Corneil and A. Proskurowski, Forbidden minors characterization of partial 3-trees, Discrete Math. 80 (1990), 1–19. They are: $K ^ { 5 }$ , the octahedron $K _ { 2 , 2 , 2 }$ , the 5-prism $C ^ { 5 } \times K ^ { 2 }$ , and the Wagner graph $W$ .

As a forerunner to Theorem 12.4.3, Robertson & Seymour proved its following analogue for path-width (Graph Minors I): excluding a graph $H$ a s a minor bounds the path-width of a graph if and only if $H$ is a forest. A short proof of this result, with optimal bounds, can be found in the first edition of this book, or in R. Diestel, Graph Minors I: a short proof of the path width theorem, Combinatorics, Probability and Computing 4 (1995), 27–30.

The Kuratowski set ${ \mathrm { \ d ~ } } \chi _ { \mathcal { P } ( S ) }$ for a given surface $S$ has been determined explicitly for only one surface other than the sphere, the projective plane. It consists of 35 forbidden minors; see D. Archdeacon, A Kuratowski theorem for the projective plane, J. Graph Theory 5 (1981), 243–246. It is not difficult to

show that $| K _ { \mathcal { P } ( S ) } |$ grows rapidly with the genus of $S$ (Exercise 43). An upper bound is given in P.D. Seymour, A bound on the excluded minors for a surface, J. Combin. Theory $B$ (to appear).   
A survey of finite forbidden minor theorems is given in Chapter 6.1 of R. Diestel, Graph Decompositions, Oxford University Press 1990. More recent developments are surveyed in R. Thomas, Recent excluded minor theorems, in (J.D. Lamb & D.A. Preece, eds) Surveys in Combinatorics 1999 , Cambridge University Press 1999, 201–222. A survey of infinite forbidden minor theorems was given by N. Robertson, P.D. Seymour & R. Thomas, Excluding infinite minors, Discrete Math. 95 (1991), 303–319.   
The existence of normal spanning trees for graphs with no topological $K ^ { \aleph _ { 0 } }$ minor was proved by R. Halin, Simplicial decompositions of infinite graphs, in: (B. Bollob´as, ed.) Advances in Graph Theory, Annals of Discrete Mathematics 3, North-Holland 1978. Its strengthening, part (iii) of Theorem 12.4.13, was observed in R. Diestel, The depth-first search tree structure of $T K _ { \aleph _ { 0 } }$ -free graphs, J. Combin. Theory B 61 (1994), 260–262. Part (iii) easily implies part (ii), which had been proved independently by N. Robertson, P.D. Seymour & R. Thomas, Excluding infinite clique subdivisions, Trans. Amer. Math. Soc. 332 (1992), 211–223. Theorem 12.4.12 was proved in R. Diestel & R. Thomas, Excluding a countable clique, J. Combin. Theory B 76 (1999), 41–67; the proof builds on the main result of N. Robertson, P.D. Seymour & R. Thomas, Excluding infinite clique minors, Mem. Amer. Math. Soc. 118 (1995).   
Our proof of the ‘generalized Kuratowski theorem’, Corollary 12.5.3, was inspired by J. Geelen, B. Richter & G. Salazar, Embedding grids in surfaces, Europ. J. Combinatorics 25 (2004), 785–792. An alternative proof, which bypasses Theorem 12.3.7 by proving directly that the graphs in ${ \mathrm { \ d ~ } } \ K _ { \mathcal { P } ( S ) }$ have bounded order, is given by B. Mohar & C. Thomassen, Graphs on Surfaces, Johns Hopkins University Press 2001. Mohar (see there) also developed a set of algorithms, one for each surface, that decide embeddability in that surface in linear time. As a corollary, he obtains an independent and constructive proof of Corollary 12.5.3.   
For every graph $X$ , Graph Minors XIII gives an explicit algorithm that decides in cubic time for every input graph $G$ whether $X \preccurlyeq G$ . The constants in the cubic polynomials bounding the running time of these algorithms depend on $X$ but are constructively bounded from above. For an overview of the algorithmic implications of the Graph Minors series, see Johnson’s NPcompleteness column in J. Algorithms 8 (1987), 285–303.   
The concept of a ‘good characterization’ of a graph property was first suggested by J. Edmonds, Minimum partition of a matroid into independent subsets, J. Research of the National Bureau of Standards ( $B$ ) 69 (1965) 67–72. In the language of complexity theory, a characterization is good if it specifies two assertions about a graph such that, given any graph $G$ , the first assertion holds for $G$ if and only if the second fails, and such that each assertion, if true for $G$ , provides a certificate for its truth. Thus every good characterization has the corollary that the decision problem corresponding to the property it characterizes lies in NP co-NP.

This appendix gives a minimum-fuss summary of the set-theoretic notions and facts, such as Zorn’s lemma and transfinite induction, that are used in Chapter 8.

Let $A , B$ be sets. If there exists a bijective map between $A$ and $B$ , we write $| A | = | B |$ and say that $A$ and $B$ have the same cardinality. This is clearly an equivalence relation between sets, and we may think of the cardinality $| A |$ of $A$ as the equivalence class containing $A$ . We write $| A | \leqslant | B |$ if there exists an injective map $A  B$ . This is clearly welldefined, and it is a partial ordering: if there are injective maps $A  B$ and $B  A$ , there is also a bijection $A  B$ .1 For every set there exists another that is bigger; for example, $| A | < | B |$ when $B$ is the power set of $A$ , the set of all its subsets.

The natural numbers are defined inductively as $n : = \{ 0 , \ldots , n - 1 \}$ , starting with $0 : = \emptyset$ . The usual expression of $| A | = n$ can then be read more formally as an abbreviation for $| A | = | n |$ .

A set $A$ is finite if there is a natural number $n$ such that $| A | = n$ ; otherwise it is infinite. $A$ is countable if $| A | \leqslant | \mathbb { N } |$ , and countably infinite if $| A | = | \mathbb { N } |$ . A bijection $\ N  A$ is an enumeration of $A$ . If $A$ is infinite then $\left| \mathbb { N } \right| \leqslant \left| A \right|$ . Thus, $| \mathbb { N } |$ is the smallest infinite cardinality; it is denoted by $\aleph _ { 0 }$ . There is also a smallest uncountable cardinality, denoted by $\aleph _ { 1 }$ . If $| A | = \left| \mathbb { R } \right|$ then $A$ is uncountable, and we say that $A$ has continuum many elements. For example, there are continuum many infinite 0–1 sequences. (Whether $\left| \mathbb { R } \right|$ is equal to $\aleph _ { 1 }$ or greater depends on the axioms of set theory assumed; in our context, this question does not arise.) We remark that if $A$ is infinite and its elements are countable sets, then the union of all these sets is no bigger than $A$ itself: $| \cup A | = | A |$ .

cardinality

N

successor

wellordering

limit

An element $x$ of a partially ordered set $X$ is minimal in $X$ if there is no $y \in X$ with $y < x$ , and maximal if there is no $z \in X$ with $x < z$ . A partially ordered set may have one or many elements that are maximal or minimal, or none at all. An upper bound in $X$ of a subset $Y \subseteq X$ is any $x \in X$ such that $y \leqslant x$ for all $y \in Y$ .

A chain is a partially ordered set in which every two elements are comparable. If $( C , \leqslant )$ is a chain, and if $x , y \in C$ satisfy $x \ < \ y$ but no element $z$ of $C$ is such that $x < z < y$ , then $x$ is called the predecessor of $y$ in $C$ , and $y$ the successor of $x$ . A set of the form $\{ x \in C \mid x < z \}$ , for a given $z \in C$ , is a proper initial segment of $C$ .

A partially ordered set $( X , \leqslant )$ is well-founded if every non-empty subset of $X$ has a minimal element, and a well-founded chain is said to be well-ordered. For example, $\mathbb { N }$ , $\mathbb { Z }$ and $\mathbb { R }$ are all chains (with their usual orderings), but only $\mathbb { N }$ is well-ordered. Note that every element $x$ of a well-ordered set $X$ has a successor (unless $x$ is maximal in $X$ ): the unique minimal element of $\left\{ y \in X \mid x < y \right\} \subset X$ . However, an element of a well-ordered set need not have a predecessor, even if it is not minimal. An element that has no predecessor is called a limit; for example, the number 1 is a limit in the well-ordered set

$$
A = \left\{1 - \frac {1}{n + 1} \mid n \in \mathbb {N} \right\} \cup \left\{2 - \frac {1}{n + 1} \mid n \in \mathbb {N} \right\}
$$

of rationals.

One of the many statements equivalent to the axiom of choice (which we assume throughout) is that for every set $X$ there exists a relation by which $X$ is well-ordered:

# Well-ordering theorem. Every set can be well-ordered.

Two well-ordered sets are said to have the same order type if there is a bijection between them which preserves their orders. Thus N and the set of even natural numbers have the same order type, but this differs from the order type of the set $A$ defined above. Having the same order type is clearly an equivalence relation, which justifies the term if we think of those order types themselves as equivalence classes.

When one considers properties shared by all well-ordered sets of the same order type, it is convenient to represent each order type by a specially chosen set of that type, its ordinal . The ordinal representing the order type of $\mathbb { N }$ , for instance, is by custom denoted as $\omega$ ; our example above thus says that the set of even natural numbers has (the) order type (of) $\omega$ . Finite chains of the same cardinality always have the same order type; we choose $n$ as the ordinal representing the chains of order $n$ .

If an ordinal $\beta$ has the same order type as a proper initial segment of another ordinal $\alpha$ , we write $\beta < \alpha$ . For example, we have $0 \leqslant n < \omega$ for every natural number $n$ . It can be shown that $<$ < defines an ordering, even

a well-ordering, on every set of ordinals. On $\mathbb { N }$ , this ordering coincides with the usual one, so our notation is unambiguous.

Since a set $S$ of ordinals is itself well-ordered, it has an order type— just like any other well-ordered set. If the ordinal $\alpha$ is a strict upper bound for $S$ , then the order type of $S$ is at most $\alpha$ ; it is equal to $\alpha$ if $S$ consists of all the ordinals up to (but excluding) $\alpha$ . In fact, just like the natural numbers, infinite ordinals are usually defined in such a way that $\alpha$ and $\{ \beta \mid \beta < \alpha \}$ are actually identical; then our ordering $<$ < for ordinals coincides with the relation $\in$ .

This makes it natural to write a well-ordered set $S$ , of order type $\alpha$ say, as a family $S = \{ s _ { \beta } \mid \beta < \alpha \}$ with $s _ { \gamma } < s _ { \beta }$ for all $\gamma < \beta < \alpha$ . This is common practice when one proves statements about the elements of $S$ by transfinite induction, which works as follows.

Suppose we want to show that every $s \in S$ satisfies some proposition $P$ ; let us write $P ( s )$ to express that it does. Just as in ordinary induction we prove, for every $\beta < \alpha$ , that if $P$ holds for every with $s _ { \gamma }$ $\gamma < \beta$ then $P$ also holds for $s _ { \beta }$ . In practice, we usually have to distinguish the two cases of $\beta$ being a limit ordinal or a successor. Checking $P ( s _ { 0 } )$ from first principles, as in ordinary induction, is part of the first case, because 0 counts as a limit and the premise of $P _ { \gamma }$ for all $\gamma < 0$ is void. The conclusion then is that $P ( s _ { \beta } )$ for every $\beta < \alpha$ , that is, every $s \in S$ satisfies $P$ .

This is certainly simple—but is it correct? Well, any proper justification of transfinite induction requires a formal treatment of set theory, but so does ordinary induction. Informally, what we have shown is that the set

$$
\{\beta <   \alpha \mid P (s _ {\beta}) \text {f a i l s} \}
$$

has no least element. Since it is well-ordered, it must therefore be empty, so $P ( s _ { \beta } )$ holds for all $\beta < \alpha$ .

Similarly, we may define things inductively. Such a recursive definition specifies for each ordinal $\alpha$ some object $x _ { \alpha }$ , in a way that may refer to the objects $x _ { \beta }$ with $\beta < \alpha$ (which we think of as ‘having been defined earlier’). Our definition of the natural numbers at the start of this appendix is a simple example. In practice, the definition of $x _ { \alpha }$ often makes sense only for ordinals $\alpha$ less than some fixed ordinal $\alpha ^ { * }$ , although the smallest such $\alpha ^ { * }$ may not be known in advance. For example, if the $x _ { \alpha }$ are to be distinct vertices picked recursively from a graph $G$ according to some given rules, it is clear that we shall not be able to find such $x _ { \alpha }$ for all $\alpha < \alpha ^ { * }$ when $| \alpha ^ { * } | > | G |$ , because $\alpha \mapsto x _ { \alpha }$ would be an injective map from $\alpha ^ { * }$ to $V ( G )$ showing that $| \alpha ^ { * } | \leqslant | G |$ . Since there exist ordinals larger than $| G |$ , such as any ordinal equivalent to a well-ordering of the power set of $V ( G )$ , this means that our recursion cannot go on indefinitely, i.e. we shall not be able to define $x _ { \alpha }$ for all ordinals $\alpha$ . We may not know which is the smallest ordinal $\alpha$ at which the recursion gets

stuck, i.e. for which $x _ { \alpha }$ cannot be found in compliance with our rules. But this does not matter: we simply define $\alpha ^ { * }$ as the first ordinal $\alpha$ for which $x _ { \alpha }$ cannot be found, content ourselves with having defined $x _ { \alpha }$ for all $\alpha < \alpha ^ { * }$ , and say that our recursion terminates at step $\alpha ^ { * }$ . (In fact, we usually want a recursive definition to terminate. In our example, we might wish to consider the set of all vertices $x \in G$ that got picked by our definition, and this will be the set $\{ x _ { \alpha } \mid \alpha < \alpha ^ { * } \}$ .)

Finally, our recursive definition for $x _ { \alpha }$ may involve choices. In our example, $x _ { \alpha }$ might be required to be a neighbour of some $x _ { \beta }$ with $\beta < \alpha$ , but there may be several such $x _ { \beta }$ , each with several neighbours that have not yet been picked. This does not cause our recursion to get stuck at step $\alpha$ : we just pick one eligible vertex as $x _ { \alpha }$ , and proceed. In other words, we accept $\{ x _ { \alpha } \mid \alpha < \alpha ^ { * } \}$ as a properly defined set even though we may not ‘know’ its elements $x _ { \alpha }$ constructively.

Finally, here is a formal statement of Zorn’s lemma:

Zorn’s Lemma. Let $( X , \leqslant )$ be a partially ordered set such that every chain in $X$ has an upper bound in $X$ . Then $X$ contains at least one maximal element.

Note that, in applications of Zorn’s lemma, the relation $\leqslant$ need not correspond to an intuitive notion of ‘smaller than’. Applied to sets or to graphs, for example, it can stand for $\supseteq$ ’ just as much as for ‘ $\subseteq$ ’. Then the ‘upper bound’ of a chain $\boldsymbol { \mathscr { C } }$ is typically its overall intersection $\cap { \mathcal { C } }$ .

This appendix offers a summary of background information about surfaces, as needed for an understanding of their role in the proof of the graph minor theorem or the proof of the ‘general Kuratowski theorem’ for arbitrary surfaces given in Chapter 12.5. In order to be read at a rigorous level it requires familiarity with some basic definitions of general topology (such as of the product and the identification topology), but no more.

A surface, for the purpose of this book, is a compact connected1 Hausdorff topological space $S$ in which every point has a neighbourhood homeomorphic to the Euclidean plane $\mathbb { R } ^ { 2 }$ . An arc, a circle, and a disc in $S$ are subsets that are homeomorphic in the subspace topology to the real interval $[ 0 , 1 ]$ , to the unit circle $S ^ { 1 } = \left\{ x \in \mathbb { R } ^ { 2 } : \left\| x \right\| = 1 \right\}$ , and to the unit disc $\{ x \in \mathbb { R } ^ { 2 } : \| x \| \leqslant 1 \}$ or $\{ x \in \mathbb { R } ^ { 2 } : \| x \| < 1 \}$ , respectively.

The components of a subset $X$ of $S$ are the equivalence classes of points in $X$ where two points are equivalent if they can be joined by an arc in $X$ . The surface $S$ itself, being connected, has only one component.

The frontier of $X$ is the set of all points $y$ in $S$ such that every neighbourhood of $y$ meets both $X$ and $S \setminus X$ . The frontier $F$ of $X$ separates $S \setminus X$ from $X$ : since $X \cup F$ is closed, every arc from $S \setminus X$ to $X$ has a first point in $X \cup F$ , which must lie in $F$ . A component of the frontier of $X$ that is a circle in $S$ is a boundary circle of $X$ . A boundary circle of a disc in $S$ is said to bound that disc.

There is a fundamental theorem about surfaces, their classification. This says that, up to homeomorphism, every surface can be obtained from the sphere $S ^ { 2 } = \{ x \in \mathbb { R } ^ { 3 } : \| x \| = 1 \}$ by ‘adding finitely many handles or finitely many crosscaps’, and that surfaces obtained by adding different numbers of handles or crosscaps are distinct. We shall not need

the classification theorem, but to form a picture2 let us see what the handle above operations mean. To add a handle to a surface $S$ , we remove two open discs whose closures in $S$ are disjoint, and identify3 their boundary circles with the circles $S ^ { 1 } \times \{ 0 \}$ and $S ^ { 1 } \times \{ 1 \}$ of a copy of $S ^ { 1 } \times [ 0 , 1 ]$ crosscap disjoint from $S$ . To add a crosscap, we remove one open disc, and then identify opposite points on its boundary circle in pairs.

In order to see that these operations do indeed give new surfaces, we have to check that every identification point ends up with a neighbourhood homeomorphic to $\mathbb { R } ^ { 2 }$ . To do this rigorously, let us first look at circles more generally.

cylinder A cylinder is the product space $S ^ { 1 } \times [ 0 , 1 ]$ , or any space homeomorphic to it. Its middle circle is the circle $S ^ { 1 } \times \{ { \textstyle { \frac { 1 } { 2 } } } \}$ . A M¨obius strip M¨obiusstrip is any space homeomorphic to the product space $[ 0 , 1 ] \times [ 0 , 1 ]$ after identification of $( 1 , y )$ with $( 0 , 1 - y )$ for all $y \in \ [ 0 , 1 ]$ . Its middle circle is the set $\{ ( x , \frac { 1 } { 2 } ) \mid 0 < x < 1 \} \cup \{ p \}$ , where $p$ is the point resulting from the identification of $( 1 , { \frac { 1 } { 2 } } )$ with $( 0 , { \frac { 1 } { 2 } } )$ . It can be shown4 that every circle strip neigh- $C$ in a surface $S$ is the middle circle of a suitable cylinder or M¨obius strip $N$ in $S$ , which can be chosen small enough to avoid any given compact subset of $S \setminus C$ . If this strip neighbourhood is a cylinder, then $N \backslash C$ has two-sided two components and we call $C$ two-sided ; if it is a M¨obius strip, then one-sided $N \backslash C$ has only one component and we call $C$ one-sided.

Using small neighbourhoods inside a strip neighbourhood of the (two-sided) boundary circle of the disc or discs we removed from $S$ in order to attach a crosscap or handle, one can show easily that both operations do produce new surfaces.

Since $S$ is connected, $S \setminus C$ cannot have more components than separating $N \backslash C$ . If $S \backslash C$ has two components, we call $C$ a separating circle in $S$ ; if it has only one, then $C$ is non-separating. While one-sided circles are obviously non-separating, two-sided circles can be either separating or non-separating. For example, the middle circle of a cylinder added to $S$ as a ‘handle’ is a two-sided non-separating circle in the new surface obtained. When $S ^ { \prime }$ is obtained from $S$ by adding a crosscap in place of a disc $D$ , then every arc in $S$ that runs half-way round the boundary circle of $D$ becomes a one-sided circle in $S ^ { \prime }$ .

The classification theorem thus has the following corollary:

Lemma B.1. Every surface other than the sphere contains a nonseparating circle.

We shall see below that, in a sense, our two examples of non-separating circles are all there are: cutting a surface along any non-separating circle (and patching up the holes) will always produce a surface with fewer handles or crosscaps.

An embedding $G \hookrightarrow S$ of a graph $G$ in $S$ is a map $\sigma$ that maps the vertices of $G$ to distinct points in $S$ and its edges $x y$ to $\sigma ( x ) – \sigma ( y )$ arcs in $S$ , so that no inner point of such an arc is the image of a vertex or lies on another arc. We then write $\sigma ( G )$ for the union of all those points and arcs in $S$ . A face of $G$ in $S$ is a component of $S \setminus \sigma ( G )$ , and the subgraph of $G$ that $\sigma$ maps to the frontier of this face is its boundary. Note that while faces in the sphere are always discs (if $G$ is connected), in general they need not be.

One can prove that in every surface one can embed a suitable graph so that every face becomes a disc. The following general version of Euler’s theorem 4.2.9 therefore applies to all surfaces:

Theorem B.2. For every surface $S$ there exists an integer $\chi ( S )$ such that whenever a graph $G$ with $n$ vertices and m edges is embedded in $S$ ： so that there are $\ell$ faces and every face is a disc, we have

$$
n - m + \ell = \chi (S).
$$

This invariant of $S$ is its Euler characteristic. For computational $\chi$ simplicity we usually work instead with the derived invariant

$$
\varepsilon (S) := 2 - \chi (S), \quad \varepsilon (S)
$$

the Euler genus of $S$ , because $\chi$ is negative for most surfaces but $\varepsilon$ takes its values in $\mathbb { N }$ (see below).

Perhaps the most striking feature of Euler’s theorem is that it works with almost any graph embedded in $S$ . This makes it easy to see how the Euler genus is affected by the addition of a handle or crosscap.

Indeed, let $D$ and $D ^ { \prime }$ be two open discs in $S$ that we wish to remove in order to attach a handle there. Let $G$ be any graph embedded in $S$ so that every face is a disc. If necessary, shift $G$ on $S$ so that $D$ and $D ^ { \prime }$ each lie inside a face, $f$ and $f ^ { \prime }$ , say. Add cycles $C$ and $C ^ { \prime }$ on the boundary circles of $D$ and $D ^ { \prime }$ , and join them by an edge to the old boundaries of $f$ and $f ^ { \prime }$ , respectively. Then every face of the resulting graph is again a disc, and $D$ and $D ^ { \prime }$ are among these. Now remove $D$ and $D ^ { \prime }$ , and add a handle with an additional $C$ – $C ^ { \prime }$ edge running along it. This operation makes the new handle into one new face, which is a disc. It thus reduces the total number of faces by 1 (since we lost $D$ and $D ^ { \prime }$ but gained the new face on the handle) and increases the number of edges by 1, but leaves the number of vertices unchanged. As a result, $\varepsilon$ grows by 2.

Similarly, replacing a disc $D$ bounded by a cycle $C \subseteq G$ with a crosscap decreases the number of faces by 1 (since we lose $D$ ), but leaves $n - m$ unchanged if we arrange the cycle $C$ in such a way that vertices get identified with vertices when we identify opposite points.

We have thus shown the following:

# Lemma B.3.

(i) Adding a handle to a surface raises its Euler genus by 2.   
(ii) Adding a crosscap to a surface raises its Euler genus by 1.

Since the sphere has Euler genus 0 (Theorem 4.2.9), the classification theorem and Lemma B.3 tell us that $\varepsilon$ has all its values in N. We may thus try to prove theorems about surfaces by induction on $\varepsilon$ . For the induction step, we could simply undo the addition of a handle or crosscap described earlier, cutting along the new non-separating circle it produced (which runs around the new handle or ‘half-way’ around the crosscap) and restoring the old surface by putting back the disc or discs we removed. A problem with this is that we do not normally know where on our surface this circle lies, say with respect to a given graph embedded in it.

However, the genus-reducing cut-and-paste operation can be carried out with any non-separating circle: we do not have to use one that we know came from a new handle or crosscap. This is an example of a more general technique known as surgery, and works as follows.

Let $C$ be a non-separating circle in a surface $S \ne S ^ { 2 }$ . To cut $S$ ： along $C$ , we form a new space $S ^ { \prime }$ from $S$ by replacing every point $x \in C$ with two points $x ^ { \prime } , x ^ { \prime \prime }$ and defining the topology on the modified set as follows.5 Let $N$ be any strip neighbourhood of $C$ in $S$ , and put $X ^ { \prime } : =$ $\{ x ^ { \prime } \mid x \in C \}$ and $X ^ { \prime \prime } : = \{ x ^ { \prime \prime } | x \in C \}$ . If $N$ is a cylinder, then $N \backslash C$ has two components $N ^ { \prime }$ and $N ^ { \prime \prime }$ , and we choose the neighbourhoods of the new points $x ^ { \prime }$ and $x ^ { \prime \prime }$ in $S ^ { \prime }$ so that $X ^ { \prime }$ and $X ^ { \prime \prime }$ become boundary circles of $N ^ { \prime }$ and $N ^ { \prime \prime }$ in $S ^ { \prime }$ , respectively, and $N ^ { \prime } \cup X ^ { \prime }$ and $N ^ { \prime \prime } \cup X ^ { \prime \prime }$ become disjoint cylinders in $S ^ { \prime }$ . If $N$ is a M¨obius strip, we choose these neighbourhoods so that $X ^ { \prime }$ and $X ^ { \prime \prime }$ each form an arc in $S ^ { \prime }$ and $X ^ { \prime } \cup X ^ { \prime \prime }$ is a boundary circle of $N \backslash C$ in $S ^ { \prime }$ , with $( N \backslash C ) \cup X ^ { \prime } \cup X ^ { \prime \prime }$ forming one cylinder in $S ^ { \prime }$ . Finally, we turn $S ^ { \prime }$ into a surface by capping its holes: for each of the (two or one) boundary circles $X ^ { \prime }$ and $X ^ { \prime \prime }$ or $X ^ { \prime } \cup X ^ { \prime \prime }$ of $S \backslash C$ in $S ^ { \prime }$ we take a disc disjoint from $S ^ { \prime }$ and identify its boundary circle with $X ^ { \prime }$ , $X ^ { \prime \prime }$ or $X ^ { \prime } \cup X ^ { \prime \prime }$ , respectively, so that the space obtained is again a surface.

Computing how these operations affect the Euler genus of $S$ is again easy, assuming we can embed a graph in $S$ so that every face is a disc and $C$ is the image of a cycle. (This can always be done, but it is not easy to prove.6) Indeed, by doubling $C$ we left $n - m$ unchanged, because a cycle has the same number of vertices as edges. So all we changed was $\ell$ , which increased by 2 in the first case and by 1 in the second.

Lemma B.4. Let $C$ be any non-separating circle in a surface $S$ , and let $S ^ { \prime }$ be obtained from $S$ by cutting along $C$ and capping the hole or holes.

(i) If $C$ is one-sided in $S$ , then $\varepsilon ( S ^ { \prime } ) = \varepsilon ( S ) - 1$ .   
(ii) If $C$ is two-sided in $S$ , then $\varepsilon ( S ^ { \prime } ) = \varepsilon ( S ) - 2$ .

Lemma B.4 gives us a large supply of circles to cut along in an induction on the Euler genus. Still, it is sometimes more convenient to cut along a separating circle, and many of these can be used too:

Lemma B.5. Let $C$ be a separating circle in a surface $S$ , and let $S ^ { \prime }$ and $S ^ { \prime \prime }$ be the two surfaces obtained from $S$ by cutting along $C$ and capping the holes. Then

$$
\varepsilon (S) = \varepsilon (S ^ {\prime}) + \varepsilon (S ^ {\prime \prime}).
$$

In particular, if $C$ does not bound a disc in $S$ , both $S ^ { \prime }$ and $S ^ { \prime \prime }$ have smaller Euler genus than $S$ .

Proof . As before, embed a graph $G$ in $S$ so that every face is a disc and $C$ is the image of a cycle in $G$ , and let $G ^ { \prime } \hookrightarrow S ^ { \prime }$ and $G ^ { \prime \prime } \hookrightarrow S ^ { \prime \prime }$ be the two graphs obtained in the surgery. Thus, $G ^ { \prime }$ and $G ^ { \prime \prime }$ both contain a copy of the cycle on $C$ , which we assume to have $k$ vertices and edges. Then, with the obvious notation, we have

$$
\begin{array}{l} \varepsilon \left(S ^ {\prime}\right) + \varepsilon \left(S ^ {\prime \prime}\right) = \left(2 - n ^ {\prime} + m ^ {\prime} - \ell^ {\prime}\right) + \left(2 - n ^ {\prime \prime} + m ^ {\prime \prime} - \ell^ {\prime \prime}\right) \\ = 4 - (n + k) + (m + k) - (\ell + 2) \\ = 2 - n + m - \ell \\ = \varepsilon (S). \\ \end{array}
$$

Now if $S ^ { \prime }$ (say) is a sphere, then $S ^ { \prime } \cap S$ was a disc in $S$ bounded by $C$ . Hence, if $C$ does not bound a disc in $S$ then $\varepsilon ( S ^ { \prime } )$ and $\varepsilon ( S ^ { \prime \prime } )$ are both non-zero, giving the second statement of the lemma. 

We now apply these techniques to prove a lemma for our direct proof in Chapter 12 of the ‘Kuratowski theorem for arbitrary surfaces’, Corollary 12.5.3.

[ 12.5.4 ]

Lemma B.6. Let $S$ be a surface, and let $\boldsymbol { \mathscr { C } }$ be a finite set of disjoint circles in $S$ . Assume that none of these circles bounds a disc in $S$ , and that $S \setminus \bigcup { \mathcal { C } }$ has a component $D _ { 0 }$ whose closure in $S$ meets every circle in $\boldsymbol { \mathscr { C } }$ . Then $\varepsilon ( S ) \geqslant | { \mathcal { C } } |$ .

Proof . We begin with the observation that the closure of $D _ { 0 }$ not only meets but even contains every circle ${ \textit { C } } \in { \textit { C } }$ . This is because $C$ has a strip neighbourhood $N$ disjoint from all the other circles in $\boldsymbol { \mathscr { C } }$ (since their union is compact), and each of the (one or two) components of $N \setminus C$ has all of $C$ in its closure. Since $D _ { 0 }$ meets, and hence contains, at least one component of $N \setminus C$ , its closure contains $C$ .

$\zeta _ { 1 }$ , $\mathcal { C } _ { 2 } ^ { 1 }$ , $\mathcal { C } _ { 2 } ^ { 2 }$

Let us partition $c$ as $\mathcal { C } = \mathcal { C } _ { 1 } \cup \mathcal { C } _ { 2 } ^ { 1 } \cup \mathcal { C } _ { 2 } ^ { 2 }$ , where the circles in $\zeta _ { 1 }$ are one-sided, those in $\mathcal { C } _ { 2 } ^ { 1 }$ are two-sided but non-separating, and those in $\mathcal { C } _ { 2 } ^ { 2 }$ are separating. We shall, in turn, cut along all the circles in $\zeta _ { 1 }$ , some $| \mathcal { C } _ { 2 } ^ { 2 } |$ circles not in $c$ , and at least half the circles in $\mathcal { C } _ { 2 } ^ { \bot }$ . This will give us a sequence $S _ { 0 } , \ldots , S _ { n }$ of surfaces, where $S _ { 0 } ~ = ~ S$ , and $S _ { i + 1 }$ is obtained from $S _ { i }$ by cutting along a circle $C _ { i }$ and capping the hole(s). Our task will be to ensure that $C _ { i }$ is non-separating in $S _ { i }$ for every $i = 0 , \ldots , n - 1$ . Then Lemma B.4 will imply that $\varepsilon ( S _ { i + 1 } ) \leqslant \varepsilon ( S _ { i } ) - 1$ for all $i$ and $\varepsilon ( S _ { i + 1 } ) \leqslant \varepsilon ( S _ { i } ) - 2$ whenever $C _ { i } \in \mathcal { C } _ { 2 } ^ { 1 }$ , giving

$$
\varepsilon (S) \geqslant \varepsilon (S _ {n}) + | \mathcal {C} _ {1} | + | \mathcal {C} _ {2} ^ {2} | + 2 | \mathcal {C} _ {2} ^ {1} | / 2 \geqslant | \mathcal {C} |
$$

as desired.

![](images/ae43ad676078f14dfd501a47f1f931c318af69bd0dde1b9cbf73ca32bad221f1.jpg)  
Fig. B.1. Cutting the 1-sided circle $C _ { 1 }$ and the 2-sided circles $C _ { 2 } , C _ { 3 }$ and $C _ { 5 } , C _ { 7 } , C _ { 8 }$ and $C _ { 9 } ^ { \prime }$ does not separate $S$ r

Cutting along the circles in $\zeta _ { 1 }$ (and capping the holes) is straightforward: since these circles are one-sided, they are always non-separating.

Next, we consider the circles in $\mathcal { C } _ { 2 } ^ { 2 }$ , such as $C _ { 9 }$ in Figure B.1. For every $C \in \mathcal { C } _ { 2 } ^ { 2 }$ , denote by $D ( C )$ the component of $S \setminus C$ that does not contain $D _ { 0 }$ . Since every circle in $\boldsymbol { \mathscr { C } }$ lies in the closure of $D _ { 0 }$ but no

point of $D ( C )$ does, these $D ( C )$ are also components of $S \setminus \bigcup { \mathcal { C } }$ . In particular, they are disjoint for different $C$ . Thus, each $D ( C )$ will also be a component of $S _ { i } \setminus C$ , where $S _ { i }$ is the current surface after any surgery performed on the circles in $\zeta _ { 1 }$ and inside $D ( C ^ { \prime } )$ for some $C ^ { \prime } \neq C$ . Given a fixed circle $C \in \mathcal { C } _ { 2 } ^ { 2 }$ , let $S ^ { \prime }$ be the surface obtained from $D ( C )$ by capping its hole. Since $C$ does not bound a disc in $S$ , we know that $S ^ { \prime }$ is not a sphere and hence contains a non-separating circle $C ^ { \prime }$ (Lemma B.1). We choose $C ^ { \prime }$ so that it avoids the cap we added to form $S ^ { \prime }$ , i.e. so that $C ^ { \prime } \subseteq S \setminus C$ . Then $C ^ { \prime }$ is also non-separating in the current surface $S _ { i }$ (since every point of $S _ { i } \setminus C ^ { \prime }$ can be joined by an arc in $S _ { i } \setminus C ^ { \prime }$ t o $C$ , which is connected), and we may select $C ^ { \prime }$ as a circle $C _ { i }$ to cut along.

It remains to select at least half of the circles in $\mathcal { C } _ { 2 } ^ { 1 }$ as circles $C _ { i }$ to cut along. We begin by selecting all those whose entire strip neighbourhoods (i.e., both their ‘sides’) lie in $D _ { 0 }$ . (In Figure B.1, these are the circles $C _ { 2 }$ and $C _ { 3 }$ .) These circles $C$ are non-separating also in the surface $S _ { i }$ current before they are cut, because $D _ { 0 }$ will lie inside a component of $S _ { i } \setminus C$ . Every other $C \in \mathcal { C } _ { 2 } ^ { 1 }$ lies in the closure also of a component $D ( C ) \ne D _ { 0 }$ . of $S \setminus \bigcup { \mathcal { C } }$ . (In Figure B.1, these are the circles $C _ { 4 } , \ldots , C _ { 8 }$ .) For every component $D$ of $S \setminus \bigcup { \mathcal { C } }$ we select all but one of the circles $C \ \in \ C _ { 2 } ^ { 1 }$ with $D ( C ) = D$ as a cutting circle $C _ { i }$ . Clearly, each of these $C _ { i }$ will be non-separating also in its current surface $S _ { i }$ , and their total number at least $| \mathcal { C } _ { 2 } ^ { 1 } | / 2$ . 

# Hints for all the Exercises

Caveat. These hints are intended to set on the right track anyone who has already spent some time over an exercise but somehow failed to make much progress. They are not designed to be particularly intelligible without such an initial attempt, and they will rarely spoil the fun by giving away the key idea. They may, however, narrow one’s mind by focusing on what is just one of several possible ways to think about a problem. . .

# Hints for Chapter 1

1. $-$ How many edges are there at each vertex?   
2. Average degree and edges: consider the vertex degrees. Diameter: how do you determine the distance between two vertices from the corresponding 0–1 sequences? Girth: does the graph have a cycle of length 3? Circumference: partition the $d$ -dimensional cube into cubes of lower dimension and use induction.   
3. Consider how the path intersects $C$ . Where can you see cycles, and can they all be short?   
4. $-$ Can you find graphs for which Proposition 1.3.2 holds with equality?   
5. Estimate the distances within $G$ as seen from a central vertex.   
6. Count vertices as in the proof of Proposition 1.3.3. For the even case, start with two adjacent vertices.   
7. $^ +$ Consider a longest path $P$ in $G$ . Where do its endvertices have their neighbours? Can $G [ P ]$ contain a cycle on $V ( P )$ ?   
8.+ Pick two vertices $x , y$ of maximum distance, and show that many of the distance classes $D _ { i }$ from $_ x$ have to be large.   
9.− Assume the contrary, and derive a contradiction.

10.− Find two vertices that are linked by two independent paths.   
11. For each type of graph, the solution requires separate proofs of (coinciding) upper and a lower bounds. For the cube, use induction on $_ n$ .   
12.− Try to find counterexamples for $k = 1$ .   
13.+ Rephrase (i) and (ii) as statements about the existence of two $\ N \longrightarrow \ N$ functions. To show the equivalence, express each of these functions in terms of the other. Show that (iii) may hold even if (i) and (ii) do not, and strengthen (iii) to remedy this.   
14.+ Try to imitate the proof assuming $\varepsilon ( G ) \geqslant c _ { k }$ instead of condition (ii). Why does this fail, and why does condition (ii) remedy the problem?   
15. Show ${ \mathrm { ( i ) } } \ \Rightarrow \ { \mathrm { ( i i ) } } \ \Rightarrow \ { \mathrm { ( i i i ) } } \ \Rightarrow \ { \mathrm { ( i v ) } } \ \Rightarrow \ { \mathrm { ( i ) } }$ from the definitions of the relevant concepts.   
16.− How can we turn distinct neighbours into distinct leaves?   
17. Average degree.   
18. Theorem 1.5.1.   
19.+ Induction.   
20. The easiest solution is to apply induction on $| T |$ . What kind of vertex of $T$ will be best to delete in the induction step?   
21. Induction on $| T |$ is a possibility, but not the only one.   
22. $-$ Count the edges.   
23. Show that if a graph contains any odd cycle at all it also contains an induced one.   
24.+ Given a graph $G$ , how would you split its vertex set into two parts $A$ and $B$ so that the bipartite graph $H$ defined by the $A$ – $B$ edges of $G$ has minimum degree as large as possible? To find $f$ , apply this method to a suitable subgraph $G$ of a given graph $G ^ { \prime }$ , and determine how large $d ( G ^ { \prime } )$ must be to ensure that $\delta ( H ) \geqslant k$ .   
25. Try to carry the proof for finite graphs over to the infinite case. Where does it fail?   
26. Try to imitate the proof of Theorem 1.8.1.   
27. Why do all the cuts $E ( v )$ generate the cut space? Will they still do so if we omit one of them? Or even two?   
28. Be clear about what exactly the word ‘minimal’ refers to in its various contexts.   
29. Start with the case that the graph considered is a cycle.   
30.+ Consider a set $F ^ { \prime } \subseteq E$ that meets every cycle in an even number of edges. Contract all edges not in $F ^ { \prime }$ . What can you say about the structure of the arising multigraph?   
31. Given a cycle $C$ to be generated, for which edges $e$ should $C _ { e }$ be among the generators of $C$ ?   
32. Given a cut $D$ to be generated, for which edges $e$ should $D _ { e }$ be among the generators of $D$ ?   
33. Apply Theorem 1.9.6.

34. Induction on $k$

35.+ Apply induction on $| G |$ . Delete a vertex $v$ of odd degree, and apply the induction hypothesis to a suitable modification of $G - v$ .

# Hints for Chapter 2

1. Recall how an augmenting path turns a given matching into a larger one. Can you reverse this process to obtain an augmenting path from the two matchings?

2. Augmenting paths.

3. Turn the functions into a graph, and consider its components.

4. If there is no matching of $A$ , then by K¨onig’s theorem few vertices cover all the edges. How can this assumption help you to find a large subset of $A$ with few neighbours?

5. Show that the marriage condition fails in $H$ for $A _ { 1 } \cup A _ { 2 }$ . The proof is almost a mirror image of the third proof, with unions and intersections interchanged.

6.+ If you have $S \subsetneq S ^ { \prime } \subseteq A$ with $\vert { S } \vert \ : = \ : \vert { N } ( { S } ) \vert$ in the finite case, the marriage condition ensures that $N ( S ) \subsetneq N ( S ^ { \prime } )$ : increasing $S$ makes more neighbours available. Use the fact that this fails when $S$ is infinite.

7. Apply the marriage theorem.

8. Construct a bipartite graph in which $A$ is one side, and the other side consists of a suitable number of copies of the sets $A _ { i }$ . Define the edge set of the graph so that the desired result can be derived from the marriage theorem.

9.+ Construct chains in the power set lattice of $X$ as follows. For each $k < n / 2$ , use the marriage theorem to find a 1–1 map $\varphi$ from the set $A$ of $k$ -subsets to the set $B$ of $( k + 1 )$ -subsets of $X$ such that $Y \subseteq \varphi ( Y )$ for all $Y \in A$ .

10. $-$ Try $C ^ { 6 }$

11. $-$ Change occurs most likely if unhappy vertices can bring it about without having to ask the happy ones. (If philosophy does not help, try $K ^ { 3 }$ .)

12. Alternating paths.

13. Decide where the leaves should go: in factor-critical components or in $S$ ?

14. By transitivity, every vertex lies in a set $S$ as in Theorem 2.2.3.

15. For the ‘if’ direction apply Tutte’s 1-factor theorem to the graph $G * K ^ { | G | - 2 k }$ , or use the remarks on maximum-cardinality matchings following Theorem 2.2.3.

16. $-$ Corollary 2.2.2.

17. Let $G$ be a bipartite graph that satisfies the marriage condition, with bipartition $\{ A , B \}$ say. Reduce the problem to the case of $| A | = | B |$ . To verify the premise of Tutte’s theorem for a given set $S \subseteq V ( G )$ , bound $| S |$ from below in terms of the number of components of $G - S$ that contain more vertices from $A$ than from $B$ and vice versa.

18.− For the first task, consider a typical non-bipartite graph. For the second, start with any maximal set of independent edges.   
19. Where in the proof of Lemma 2.3.1 do we use that $\Delta ( G ) \leqslant 3 ?$   
20. Find a subgraph $H$ isomorphic to a cycle or $K ^ { 2 }$ or $K ^ { 1 }$ that contains a vertex not adjacent to any vertex in $G - H$ . Then apply induction on $\alpha$ .   
21. If you cannot spot the error just by reading the proof very carefully (which you should be able to do, really—but this case it is tricky), it is a good idea to test the assertion for extreme cases or small graphs. When you have found a counterexample, go through the proof with this graph in mind and see where exactly it fails.   
22. $-$ Consider any smallest path cover.   
23. Direct all the edges from $A$ to $B$ .   
24. $-$ Dilworth.   
25. Start with the set of minimal elements in $P$ .   
26. Think of the elements of $A$ as being smaller than their neighbours in $B$ .   
27. Construct a poset from arbitrarily large finite antichains.

# Hints for Chapter 3

1. $-$ Recall the definitions of ‘separate’ and ‘component’.   
2. Describe in words what the picture suggests.   
3. Use Exercise 1 to answer the first question. The second requires an elementary calculation, which the figure may already suggest.   
4. Only the first part needs arguing; the second then follows by symmetry. Suppose a component of $G - X$ is not met by $X ^ { \prime }$ . Where does $X ^ { \prime }$ lie in this picture? Remember Exercise 1.   
5.− How can a block fail to be a maximal 2-connected subgraph? And what else follows then?   
6. Deduce the connectedness of the block graph from that of the graph itself, and its acyclicity from the maximality of each block.   
7. Prove the statement inductively using Proposition 3.1.3. Alternatively, choose a cycle through one of the two vertices and with minimum distance from the other vertex. Show that this distance cannot be positive.   
8. Belonging to the same block is an equivalence relation on the edge set; see Exercise 5.   
9. Induction along Proposition 3.1.3.   
10. Assuming that $G / x y$ is not 3-connected, distinguish the cases when $v _ { x y }$ lies inside or outside a separator of at most 2 vertices.   
11. $( \mathrm { i } ) ^ { - }$ Consider the edges incident with a smaller separator.

(ii) Induction shows that all the graphs obtained by the construction are cubic and 3-connected. For the converse, consider a maximal subgraph $T H \subseteq G$ such that $H$ is constructible as stated; then show that $H = G$ .

12. $^ +$ If such a finite set exists, then every other 3-connected graph can be

made into a smaller 3-connected graph by deleting one vertex and suppressing any arising vertices of degree 2. (Why?) For which graphs is this possible?

13. Check the induction.

14. How big is $S$ ? To recognize the easy remaining case, it helps to have solved the previous exercise first.

15. Choose the disjoint $A$ – $B$ paths in $L ( G )$ minimal.

16. Consider a longest cycle $C$ . How are the other vertices joined to $C$ ?

17. Consider a cycle through as many of the $k$ given vertices as possible. If one them is missed, can you re-route the cycle through it?

18. Consider the graph of the hint. Show that any subset of its vertices that meets all $H$ -paths (but not $H$ ) corresponds to a similar subset of $E ( G ) \setminus E ( H )$ . What does a pair of independent $H$ -paths in the auxiliary graph correspond to in $G$ ?

19.− How many paths can a single $K ^ { 2 m + 1 }$ accomodate?

20. Choose suitable degrees for the vertices in $B$ .

21. $^ +$ Let $H$ be the (edgeless) graph on the new vertices. Consider the sets $X$ and $F ^ { \prime }$ that Mader’s theorem provides if $G ^ { \prime }$ does not contain $| G | / 2$ independent $H$ -paths. If $G$ has no 1-factor, use these to find a suitable set that can play the role of $S$ in Tutte’s theorem.

22.− If two vertices $s , t$ are separated by fewer than $2 k - 1$ vertices, extend $\{ s \}$ and $\{ t \}$ to $k$ -sets $S$ and $T$ showing that $G$ is not $k$ -linked.

23. To construct a highly connected graph that is not $k$ -linked, start by writing down the vertices $s _ { 1 } , \ldots , s _ { k } , t _ { 1 } , \ldots , t _ { k }$ . By specifying suitable non-edges, make the paths in the required linkage need more vertices in total than there are vertices left in the graph. To make the graph highly connected, add all edges other than the specified non-edges.

24. Use induction on $2 k - | S \cup T |$ , where $S : = \{ s _ { 1 } , . . . , s _ { k } \}$ and $T : =$ $\left\{ \begin{array} { l } { t _ { 1 } , \dots , t _ { k } } \end{array} \right\}$ . For the induction step recall that $\delta ( G ) \ \geqslant \ 2 k - 1$ , by Exercise 22.

25. To construct the $T K ^ { \prime }$ , start by picking the branch vertices and their neighbours.

# Hints for Chapter 4

1. Embed the vertices inductively. Where should you not put the new vertex?   
2.− Figure 1.6.2.   
3.− Make the given graph connected.   
4. This is a generalization of Corollary 4.2.10.   
5. Theorem 2.4.4.   
6. Imitate the proof of Corollary 4.2.10.   
7. Proposition 4.2.7.

8.− Express the difference between the two drawings as a formal statement about vertices, faces, and the incidences between them.   
9. Combinatorially: use the definition. Topologically: express the relative position of the short (respectively, the long) branches of $G ^ { \prime }$ formally as a property of $G ^ { \prime }$ which any topological isomorphism would preserve but $G$ lacks.   
10. $-$ Reflexivity, symmetry, transitivity.   
11. Look for a graph whose drawings all look the same, but which admits an automorphism that does not extend to a homeomorphism of the plane. Interpret this automorphism as $\sigma _ { 2 } \circ \sigma _ { 1 } ^ { - 1 }$ .   
12.+ Star-shape: every inner face contains a point that sees the entire face boundary.   
13. Work with plane rather than planar graphs.   
14. (i) The set $\mathcal { X }$ may be infinite.   
(ii) If $Y$ is a $T X$ , then every $T Y$ is also a $T X$   
15. $-$ By the next exercise, any counterexample can be disconnected by at most two vertices.   
16. Incorporate the extra condition into the induction hypothesis of the proof. It may help to disallow polygons with 180 degree angles.   
17. Number of edges.   
18. Use that maximal planar graphs are 3-connected, and that the neighbours of each vertex induce a cycle.   
19. If $G = G _ { 1 } \cup G _ { 2 }$ with $G _ { 1 } \cap G _ { 2 } = \overline { { K ^ { 2 } } }$ , we have a problem. This will go away if we embed a little more than necessary.   
20. Use a suitable modification of the given graph $G$ to simulate outerplanarity.   
21. Use the fact that ${ \mathcal { C } } ( G )$ is the direct sum of $\mathcal { C } ( G _ { 1 } )$ and $\mathcal { C } ( G _ { 2 } )$   
22.+ Euler.   
23. The face boundaries generate ${ \mathcal { C } } ( G )$   
24.+ Solve the previous exercise first.   
25.− How many vertices does it have?   
26.− Join two given vertices of the dual by a straight line, and use this to find a path between them in the dual graph.   
27.+ Define the required bijections $F \to V ^ { * }$ , $E \to E ^ { * }$ , $V \to F ^ { * }$ successively in this order, while at the same time constructing $G ^ { * }$ .   
28. Solve the previous exercise first.   
29. Use the bijections that come with the two duals to define the desired isomorphism and to prove that it is combinatorial.   
30. Apply Menger’s theorem and Proposition 4.6.1. For (iii), consider a 4-connected graph with six vertices.   
31. Apply induction on $_ n$ , starting with part (i) of the previous exercise.   
32. Theorem 1.9.5.   
33. This can be proved directly, i.e. without planarity.

# Hints for Chapter 5

1.− Duality.   
2.− Whenever more than three countries have some point in common, apply a small local change to the map where this happens.   
3. Where does the five colour proof use the fact that $v$ has no more neighbours than there are colours?   
4. How can the colourings of different blocks interfere with each other?   
5.− Use a colouring of $G$ to derive a suitable ordering.   
6. Consider how the removal of certain edges may lead the greedy algorithm to use more colours.   
7. Describe more precisely how to implement this alternative algorithm. Then, where is the difference to the traditional greedy algorithm?   
8. Compare the number of edges in a subgraph $H$ as in Proposition 5.2.2 with the number $m$ of edges in $G$ .   
9. Go via minimum degrees.   
10.− Remove vertices successively until the graph becomes critically $k$ - chromatic. What can you say about the degree of any vertex that remains?   
11. Proposition 1.6.1.   
12.+ Modify colourings of the two sides of a hypothetical cut of fewer than $k - 1$ edges so that they combine to a $( k - 1 )$ -colouring of the entire graph (with a contradiction).   
13. Proposition 1.3.1.   
14. $-$ For which graphs with large maximum degree does Proposition 5.2.2 give a particularly small upper bound?   
15.+ (i) How will $v _ { 1 }$ and $v _ { 2 }$ be coloured, and how $v _ { n }$ ?   
(ii) Consider the subgraph induced by the neighbours of $v _ { n }$   
16. $^ +$ For the implication (ii) (i), consider a maximal spanning directed subgraph $D$ of the given orientation of $G$ that contains no directed cycle. Use the fact that all directed paths in $D$ are short to $k$ -colour its underlying undirected graph, and show that this colouring is even a $k$ -colouring of $G$ .   
17. In the induction step, compare the values of $P _ { G } ( k )$ , $P _ { G - e } ( k )$ and $P _ { G / e } ( k )$   
18.+ Multiplicities of zeros.   
19. Imitate the proof of Theorem 5.2.6.   
20. $-$ $K _ { n , n }$   
21. How are edge colourings related to matchings?   
22. Construct a bipartite $\Delta ( G )$ -regular graph that contains $G$ as subgraph. It may be necessary to add some vertices.   
23.+ Induction on $k$ . In the induction step $k \longrightarrow k + 1$ , consider using several copies of the graph you found for $k$ .   
24. $-$ Vertex degrees.

25. $K _ { n , n }$ . To choose $_ n$ so that $K _ { n , n }$ is not even $k$ -choosable, consider lists of $k$ -subsets of a $k ^ { 2 }$ -set.   
26.− Vizing’s theorem.   
27. All you need are the definitions, Proposition 5.2.2, and a standard argument from Chapter 1.2.   
28.+ Try induction on $r$ . In the induction step, you would like to to delete one pair of vertices and only one colour from the other vertices’ lists. What can you say about the lists if this is impossible? This information alone will enable you to find a colouring directly, without even looking at the graph again.   
29. Show that $\chi ^ { \prime \prime } ( G ) ~ \leqslant ~ \mathrm { c h } ^ { \prime } ( G ) + 2$ , and use this to deduce $\chi ^ { \prime \prime } ( G ) \ \leqslant$ $\Delta ( G ) + 3$ from the list colouring conjecture.   
30. $-$ For the first question, try to construct an oriented graph without a kernel edge by edge. For the second and third question, recall the motivational remarks in the text concerning the notion of a kernel.   
31. $^ +$ Call a set $S$ of vertices in a directed graph $D$ a core if $D$ contains a directed $v$ – $S$ path for every vertex $v \in D - S$ . If, in addition, $D$ contains no directed path between any two vertices of $S$ , call $S$ a strong core. Show first that every core contains a strong core. Next, define inductively a partition of $V ( D )$ into ‘levels’ $L _ { 0 } , \ldots , L _ { n }$ such that, for even $_ i$ , $L _ { i }$ is a suitable strong core in $D _ { i } : = D - ( L _ { 0 } \cup . . . \cup L _ { i - 1 } )$ , while for odd $_ i$ , $L _ { i }$ consists of the vertices of $D _ { i }$ that send an edge to $\boldsymbol { L } _ { i - 1 }$ . Show that, if $D$ has no directed odd cycle, the even levels together form a kernel of $D$ .   
32. Construct the orientation needed for Lemma 5.4.3 in steps: if, in the current orientation, there are still vertices $\boldsymbol { v }$ with $d ^ { + } ( v ) \geqslant 3$ , reverse the directions of an edge at $v$ and take care of the knock-on effect of this change. If you need to bound the average degree of a bipartite planar graph, remember Euler’s formula.   
33.− Start with a non-perfect graph.   
34.− Do odd cycles or their complements satisfy (∗)?   
35. Apply the property of $\mathcal { H } _ { 1 }$ to the graphs in $\mathcal { H } _ { 2 }$ , and vice versa.   
36. K¨onig’s theorem asserts the existence of a set of vertices meeting every edge. Rephrase perfection as asserting the existence of a set of vertices meeting all colour classes.   
37. Look at the complement.   
38. Define the colour classes of a given induced subgraph $H \subseteq G$ inductively, starting with the class of all minimal elements.   
39. (i) Can the vertices on an induced cycle contain each other as intervals? (ii) Use the natural ordering of the reals.   
40. Compare $\omega ( H )$ with $\Delta ( G )$ (where $H = L ( G )$ ).   
41. $^ +$ Which graphs are such that their line graphs contain no induced cycles of odd length $\geqslant 5$ ? To prove that the edges of such a graph $G$ can be coloured with $\omega ( L ( G ) )$ colours, imitate the proof of Vizing’s theorem.

42. Use $A$ as a colour class.   
43.+ (i) Induction.

(ii) Assume that $G$ contains no induced $P ^ { 3 }$ . Suppose some $H$ has a maximal complete subgraph $K$ and a maximal set $A$ of independent vertices disjoint from $K$ . For each vertex $v \in K$ , consider the set of neighbours of $v$ in $A$ . How do these sets intersect? Is there a smallest one?

44. $^ +$ Start with a candidate for the set $\boldsymbol { \mathcal { O } }$ , i.e. a set of maximal complete subgraphs covering the vertex set of $G$ . If all the elements of $\boldsymbol { \mathcal { O } }$ happen to have order $\omega ( G )$ , how does the existence of $\mathcal { A }$ follow from the perfection of $G$ ? If not, can you expand $G$ (maintaining perfection) so that they do and adapt the $\mathcal { A }$ for the expanded graph to $G$ ?   
45.+ Reduce the general case to the case when all but one of the $G _ { x }$ are trivial; then imitate the proof of Lemma 5.5.5.

# Hints for Chapter 6

1. $-$ Move the vertices, one by one, from $\overline { S }$ to $S$ . How does the value of $f ( S , { \overline { { S } } } )$ change each time?   
2. (i) Trick the algorithm into repeatedly using the middle edge in alternating directions.   
(ii) At any given time during the algorithm, consider for each vertex $\boldsymbol { v }$ the shortest $_ { s - v }$ walk that qualifies as an initial segment of an augmenting path. Show for each $v$ that the length of this $s$ –v walk never decreases during the algorithm. Now consider an edge which is used twice for an augmenting path, in the same direction. Show that the second of these paths must have been longer than the first. Now derive the desired bound.   
3.+ For the edge version, define the capacity function so that a flow of maximum value gives rise to sufficiently many edge-disjoint paths. For the vertex version, split every vertex $x$ into two adjacent vertices $x ^ { - } , x ^ { + }$ . Define the edges of the new graph and their capacities in such a way that positive flow through an edge $x ^ { - } x ^ { + }$ corresponds to the use of $_ x$ by a path in $G$ .   
4. $-$ $H$ -flows are nowhere zero, by definition.   
5.− Use the definition and Proposition 6.1.1.   
6. $-$ Do subgraphs also count as minors?   
7. $-$ Try $k = 2 , 3 , \ldots$ in turn. In searching for a $k$ -flow, tentatively fix the flow value through an edge and investigate which consequences this has for the adjacent edges.   
8. To establish uniqueness, consider cuts of a special type.   
9. Express $G$ as the union of cycles.   
10. Combine $\mathbb { Z } _ { 2 }$ -flows on suitable subgraphs to a flow on $G$ .   
11. Begin by sending a small amount of flow through every edge outside $T$ .

12. View $G$ as the union of suitably chosen cycles.   
13. Corollary 6.3.2 and Proposition 6.4.1.   
14. $-$ Duality.   
15. Take as $H$ your favourite graph of large flow number. Can you decrease its flow number by adding edges?   
16. Euler.   
17. Instead of proving (F2) for $g$ , show more generally that $g ( X , { \overline { { X } } } ) = 0$ for every cut ${ \vec { E ^ { * } } } ( X , { \overline { { X } } } )$ of $G ^ { * }$ .   
18.+ Theorem 6.5.3.   
19. Theorem 6.5.3.   
20. (i) Theorem 6.5.3.

(ii) Yes it can. Show, by considering a smallest counterexample, that if every 3-connected cubic planar multigraph is 3-edge-colourable (and hence has a 4-flow), then so is every bridgeless cubic planar multigraph.

21.+ For the ‘only if’ implication apply Proposition 6.1.1. Conversely, consider a circulation $f$ on $G$ , with values in $\{ 0 , \pm 1 , \ldots , \pm ( k - 1 ) \}$ , that respects the given orientation (i.e. is positive or zero on the edge directions assigned by $D$ ) and is zero on as few edges as possible. Then show that $f$ is nowhere zero, as follows. If $f$ is zero on $e = s t \in E$ and $D$ directs $e$ from $t$ to $s$ , define a network $N = ( G , s , t , c )$ such that any flow in $N$ of positive total value contradicts the choice of $f$ , but any cut in $N$ of zero capacity contradicts the property assumed for $D$ .

22. $-$ Convert the given multigraph into a graph with the same flow properties.

# Hints for Chapter 7

1. $-$ Straightforward from the definition.   
2.− When constructing the graphs, start by fixing the colour classes.   
3. It is not difficult to determine an upper bound for $\mathrm { e x } ( n , K _ { 1 , r } )$ . What remains to be proved is that this bound can be achieved for all $r$ and $_ n$ .   
4. Proposition 1.7.2 (ii).   
5.+ What is the maximum number of edges in a graph of the structure given by Theorem 2.2.3 if it has no matching of size $k$ ? What is the optimal distribution of vertices between $S$ and the components of $G - S$ ? Is there always a graph whose number of edges attains the corresponding upper bound?   
6. Consider a vertex $x \in G$ of maximum degree, and count the edges in $G - x$ .   
7. Choose $k$ and $_ i$ so that $n = ( r - 1 ) k + i$ with $0 \leqslant i < r - 1$ . Treat the case of $i = 0$ first, and then show for the general case that $t _ { r - 1 } ( n ) =$ $\begin{array} { r } { \frac { 1 } { 2 } \frac { r - 2 } { r - 1 } ( n ^ { 2 } - i ^ { 2 } ) + \binom { i } { 2 } } \end{array}$ .

8. The bounds given in the hint are the sizes of two particularly simple Tur´an graphs—which ones?   
9. Choose among the $m$ vertices a set of $s$ vertices that are still incident with as many edges as possible.   
10. For the first inequality, double the vertex set of an extremal graph for $K _ { s , t }$ to obtain a bipartite graph with twice as many edges but still not containing a $K _ { s , t }$ .   
11. $^ +$ For the displayed inequality, count the pairs $( x , Y )$ such that $x \in A$ and $Y \subseteq B$ , with $| Y | = r$ and $_ x$ adjacent to all of $Y$ . For the bound on $\mathrm { e x } ( n , K _ { r , r } )$ , use the estimate $( s / t ) ^ { t } \leq { \binom { s } { t } } \leq s ^ { t }$ and the fact that the function $z \mapsto z ^ { \tau }$ is convex.   
12. Assume that the upper density is larger than $\textstyle 1 - { \frac { 1 } { r - 1 } }$ . What does this mean precisely, and what does the Erd˝os-Stone theorem then imply?   
13. Proposition 1.2.2 and Corollary 1.5.4.   
14. Complete graphs.   
15. $-$ A vertex of high degree is nearly a star.   
16. Do more than $\textstyle { \frac { 1 } { 2 } } ( k - 1 ) n$ edges force a subgraph of suitable minimum degree?   
17.+ Consider your favourite graphs with high average degree and low chromatic number. Which trees do they contain induced? Is there some reason to expect that exactly these trees may always be found induced in graphs of large average degree and small chromatic number?   
18. All the implications sought are either very easy to prove or follow from material stated in the text (not necessarily in this chapter).   
19.+ Contract a set of the form $\{ v \mid d ( v _ { 0 } , v ) \leqslant i \}$ .   
20. Induction on $r$ .   
21. $-$ Does a large chromatic number force up the average degree? If in doubt, consult Chapter 5.   
22.+ Let $G ^ { \prime } \prec G$ be a minimal minor with $\varepsilon ( G ^ { \prime } ) \geqslant k$ . Show that, for every vertex $v \in G ^ { \prime }$ , the subgraph $H$ of $G ^ { \prime }$ induced by the neighbours of $v$ has minimum degree at least $k$ . Can you choose $v$ so that $| H | \leqslant 2 k$ ?   
23.+ First show that we need only consider graphs $G$ of minimum degree at least 3. Then Corollary 1.3.5 gives us a cycle $C$ of length at most about $2 \log n$ . Assuming without loss of generality that $G$ has exactly $n + \left\lceil 2 k \left( \log k + \log \log k + c \right) \right\rceil \geqslant 3 n / 2$ edges, bound $\| C \|$ from above in terms of $k$ , and show that, for a suitable choice of $c$ , deleting only this many edges makes the induction step work.   
24.− Imitate the proof of Theorem 7.2.1, replacing $r ^ { 2 }$ by $\binom { r } { 2 }$   
25.+ How can we best make a $T K ^ { 2 r }$ fit into a $K _ { s , s }$ when we want to keep $s$ small?   
26. Which of the graphs constructed as in the hint have the largest average degree?   
27. $-$ What does planarity have to do with minors?   
28.− Consider a suitable supergraph.

29.− Apply a theorem from this chapter.

30. Induction on the number of construction steps.

31. Induction on $| G |$ .

32. Note the previous exercise.

33. Start with a suitable subgraph of large minimum degree. Which result or technique from Section 7.2 can be used to boost its minimum degree further to make suitable input for Theorem 7.2.2?

34.+ Show by induction on $| G |$ that any 3-colouring of an induced cycle in $G \not \approx K ^ { 4 }$ extends to all of $G$ .

35.+ Reduce the statement to critical $k$ -chromatic graphs and apply Vizing’s theorem.

36. Which of the graphs constructed as in Theorem 7.3.4 have the largest average degree?

37. $-$ Why would it be impractical to include, say, 1-element sets $X , Y$ in the comparison?

38. $-$ Apply the definition of an $\scriptstyle \epsilon $ -regular pair.

39. For the meaning of the word ‘about’, assume that $| V |$ is large compared with $k$ . For the second task, do not refer to the details of the proof of Theorem 7.1.2, but to the informal explanations follows it.

40. For (i) just make $M$ large enough. For (ii) use the analogue of (i) for the graphs considered, putting $k : = m$ when the graph is large.

# Hints for Chapter 8

1. $-$ Count the vertices, ‘moving out’ from a fixed vertex.   
2. $-$ Make $\sigma$ beat $\sigma ^ { i }$ from $s _ { i }$ onwards.   
3. Let $\mathcal { A }$ be a set of subsets of a countable set $A$ such that $| A ^ { \prime } \cap A ^ { \prime \prime } | \leqslant k$ for all distinct $A ^ { \prime } , A ^ { \prime \prime } \in { \mathcal { A } }$ and some fixed $k \in \mathbb N$ . Consider a fixed $k$ -set $S$ . How many sets in $\mathcal { A }$ can contain $S$ ?   
4. $-$ Consider a ray $v _ { 0 } v _ { 1 } \ldots .$ Can it be decreasing, ie such that $v _ { 0 } > v _ { 1 } >$ . . . ? If not, can it go down again once it has gone up, ie, can it contain vertices $x _ { i - 1 } < x _ { i } > x _ { i + 1 }$ ?   
5. $-$ Construct the paths inductively. Alternatively, use Zorn’s lemma to find a maximal set of disjoint $A$ – $B$ paths. Can it be finite?   
6.− If you cannot make this approach work, describe how it fails.   
7. $-$ Construct such a graph inductively. Can you do it in one infinite sequence of steps?   
8. Construct the graph inductively, starting from a vertex or a cycle. To ensure that the final graph has high connectivity, join each new vertex by many edges to the infinite set of vertices yet to be defined.   
9.− Use the previous exercise.

10. Starting from the definition of the topology on $X$ , describe what it means for a sequence of points in $X$ to converge. What must a sequence look like whose convergent subsequences all determine proper colourings of $G$ ? Can you deduce from the assumptions that such a sequence exists? It may help to look at the infinity lemma for ideas.   
11. $^ +$ Apply induction on $k$ .   
12. $-$ This is a standard compactness proof: use the infinity lemma for countable graphs, and Tychonov’s theorem for arbitrary graphs.   
13. Apply the infinity lemma. Find a statement about a vertex partition of $G _ { n } = G [ v _ { 1 } , \ldots , v _ { n } ]$ that implies the corresponding statement for the induced partition of $G _ { n - 1 }$ , and whose truth for the partitions of the $G _ { n }$ induced by a given partition of $G$ implies that this partition of $G$ is as desired.   
14. Apply the infinity lemma to a suitably weakened statement about finite subgraphs.   
15. For the positive result use the infinity lemma, considering the finite subgraphs spanned by a given finite subset of $A$ and all its neighbours in $B$ . For the counterexample, note that if $S \subsetneq S ^ { \prime } \subseteq A$ with $| S | = | N ( S ) |$ in the finite case, the marriage condition ensures that $N ( S ) \subsetneq N ( S ^ { \prime } )$ : increasing $S$ makes more neighbours available. Use the fact that this can fail when $S$ is infinite.   
16.+ Note that, in order to apply the infinity lemma, it is enough to find in every finite induced subgraph $G _ { n }$ of $G$ a set of independent edges covering those vertices that have no neighbour in $G - G _ { n }$ . To find such a set of edges, apply the finite 1-factor theorem to the graph $H _ { n }$ obtained from $G _ { n }$ by adding a large complete graph $K$ joined completely to all those vertices of $G _ { n }$ that have a neighbour in $G - G _ { n }$ . If you get stuck, change the parity of $| K |$ .   
17. $^ +$ Use the material from Chapter 4.3 to make drawings susceptible to an application of the infinity lemma. To construct the final drawing from a ray in the infinity lemma graph, make sure that the partial drawings constructed inductively are really definite drawings in the plane, not merely abstract equivalence types of drawings.   
18.− Adapt the hint for Exercise 5 to prove the appropriate fan version of Menger’s theorem.   
19. Construct the $T K ^ { \aleph _ { 0 } }$ inductively.   
20. Start with the binary tree $T _ { 2 }$ , and make its ends thick while keeping the graph countable.   
21. You can prove the forward implication either ‘from above’ by recursively pruning away parts of the tree that are certain not to lie in a subdivided $T _ { 2 }$ , or ‘from below’ by constructing a subdivided $T _ { 2 }$ inductively inside the given tree.   
22. For (i), note that a ray has countably many subrays. For the forward implication in (iii), prune the given tree recursively by chopping off locally finite subtrees and bounding these; then combine all the bounding

functions obtained into one. It will help in the proof if you make this final function increasing.

23.+ Does $\boldsymbol { T } _ { \aleph _ { 1 } }$ have such a labelling? If $T \nsupseteq T _ { \aleph _ { 1 } }$ , construct a labelling of $T$ inductively. Supposing a labelling exists: where in $T$ will the vertices labelled zero lie? Where the vertices labelled 1?   
24. Suppose a locally finite connected graph $G$ has three distinct ends. Let $S$ be a finite set of vertices separating these pairwise. Take an automorphism that maps $S$ ‘far away’ into a component of $G - S$ . Can you show that the image of $S$ separates this component in such a way that $G$ must have more than three ends?   
25.+ Pick a vertex $\boldsymbol { v }$ . Is its orbit $U = \{ v , \sigma ( v ) , \sigma ( \sigma ( v ) ) , . . . \}$ finite or infinite? To determine the position of $U$ within $G$ , let $P$ be a path from $\boldsymbol { v }$ to $\sigma ( v )$ and consider the infinite union $P \cup \sigma ( P ) \cup \sigma ( \sigma ( P ) ) \cup \ldots .$ Does this, somehow, define an end? And what about the sequence $v , \sigma ^ { - 1 } ( v ) , \sigma ^ { - 2 } ( v ) , . . . ?$   
26.− Lemma 8.2.2.   
27. Lemma 8.2.3.   
28. Prove the implication (i) (iv) first.   
29. Show that deleting a finite set of vertices never leaves infinitely many components.   
30. To construct the normal spanning tree in (i), imitate the proof for countable $G$ . Well-order each of the dispersed sets, concatenate these well-orderings into one well-ordering of $V ( G )$ , and construct the tree recursively.   
31. $-$ Normal spanning trees.   
32.+ For simplicity, replace the graph with a spanning tree in it, $T$ say. Which vertices have to appear earlier in the enumeration than others?   
33. Imitate the proof of Theorem 8.2.5, choosing all the rays used from the given end. Do the rays constructed also belong to that end? If not, how can this be achieved?   
34.+ Imitate the proof of Theorem 8.2.5. Work with rays rather than double rays whenever possible.   
35. The task is to find in any graph $G$ that contains arbitrarily many disjoint $M H$ a locally finite subgraph with the same property. In a first step, find a countable such subgraph $G ^ { \prime }$ , and enumerate its vertices. Then use the enumeration to find a locally finite such subgraph $G ^ { \prime \prime } \subseteq G ^ { \prime }$ by ensuring that each vertex of $G ^ { \prime }$ is used by only finitely many $M H$ .   
36. To construct a graph that contains arbitrarily but not infinitely many copies of the modified comb $T$ , start with infinitely many disjoint copies of $T$ . Group these into disjoint sets $S _ { 1 } , S _ { 2 } , \ldots$ so that $S _ { n }$ is a disjoint union of $n$ copies of $T$ . Then identify vertices from different sets $S _ { n }$ , so as to spoil infinite ‘diagonal’ sets of disjoint copies of $T$ .   
37. Fundamental cycles.

38. Unlike in the proof of Theorem 8.2.6, you can use suitable tails of all the rays in the (large but finite) set $\mathcal { R } _ { 0 }$ as rays $Q _ { n }$ . The part of the proof that start with assumption $^ *$ ) can thus be replaced by a much simpler algorithm that finds $Q _ { n }$ and an infinite set of disjoint Qn– $Q _ { p ( n ) }$ paths. To determine how many rays are needed, start with a suitable finite analogue to the infinity lemma: any large enough rooted tree either has a vertex with at least $k$ successors or contains a path of length $k$ .   
39. Suppose there is a universal graph $G$ . Construct a locally finite connected graph $H$ whose vertex degrees ‘grow too fast’ for any embedding of $H$ in $G$ .   
40. Modify $K ^ { \aleph _ { 0 } }$ or the Rado graph. Or try a direct construction.   
41. $-$ Property (∗).   
42.− Back-and-forth.   
43. Back-and-forth.   
44. Find the partition inductively, deleting the edge set of one graph at a time and showing that what remains is still isomorphic to $R$ . How can you ensure that, once all the required edge sets have been deleted, there is no edge left?   
45.− This is a theorem of Cantor. To prove it, use density like property (∗).   
46. $R$   
47. For the vertex $\boldsymbol { v }$ in property ( ), try putting $v : = U$ first. How can this fail? And how can you amend it if it fails? You may wish to use the Axiom of Foundation, by which there is no sequence $x _ { 1 } \in \ldots \in x _ { n }$ of sets with $n \geqslant 2$ and $x _ { 1 } = x _ { n }$ .   
48. Look at Exercise 49 and its hint. For locally finite $G$ the sets $S _ { i } ^ { \prime }$ are very easy to find, and no normal spanning tree is needed.   
49.+ Use a normal spanning tree to find provisional sets $S _ { 1 } ^ { \prime } , S _ { 2 } ^ { \prime } , \ldots$ of arbitrary finite cardinality that have the separation properties required of the $S _ { i }$ . Then use these to find the $S _ { i }$ .   
50. Pick $a \in A$ , and construct a sequence of waves $\mathcal { W } _ { 1 } , \mathcal { W } _ { 2 } , \dotsc$ that each contain the trivial path $\{ a \}$ . Define the edges at $a$ so that $a$ is in the boundary of every $\mathcal { W } _ { n }$ , but not in the boundary of the limit wave.   
51. $^ +$ The general problem reduces to Lemma 8.4.3, just as in the countable case. Prove the lemma for forests.   
52.+ Starting with $\mathcal { P }$ , recursively define path systems $\mathcal { P } _ { \alpha }$ that link $A$ to more and more of $B$ . In the recursion step, pick an uncovered vertex $b \in \ B$ and follow the path $Q \in \mathcal { Q }$ containing it back until it hits $\mathcal { P } _ { \alpha }$ , say in $P = a \ldots b ^ { \prime }$ . You could then re-route $P$ to follow $Q$ to $b$ from there, but this would leave $b ^ { \prime }$ uncovered. Still, could it be that these changes produce an increase of the covered part of $B$ at limit steps? To prove that it does, can you define an ‘index’ parameter that grows (or decreases) with every step but cannot do so indefinitely?

Alternatively, prove and apply a suitable infinite version of the stable marriage theorem (2.1.4).

53. (i) is just compactness. A neat 1-line proof uses Theorem 8.1.3. For (ii), construct a poset from arbitrarily large finite antichains. For (iii), define a bipartite graph as follows. For every point $x \in \mathcal Ḋ P Ḍ$ take two vertices $x ^ { \prime }$ and $x ^ { \prime \prime }$ . Then add all edges $x ^ { \prime } y ^ { \prime \prime }$ such that $x \ : < y$ . Now consider a matching $M$ and a vertex cover $U$ in this graph as provided by Theorem 8.4.8. How does $M$ define a partition of $P$ into chains? For how many points $x$ of such a chain can $x ^ { \prime }$ or $x ^ { \prime \prime }$ lie in $U$ ?   
54. Do the assumptions imply that there exists a 1-factor? If so, can you use it?   
55. To ensure that every partial matching can be augmented, give your graph lots of edges. How can you nevertheless prevent a 1-factor?   
56. Try to prove, e.g. by compactness, that an infinite factor-critical graph must have a 1-factor. If your proof fails, does it lead you to a construction?   
57. $-$ Consider first the case that the complete subgraphs of $H$ have finitely bounded order. You may use a result from Section 8.1.   
58.+ For the perfection of $G$ in (ii), show that every subset of $T _ { 2 }$ with arbitrarily large finite antichains also has an infinite antichain.   
59. For the backwards implication, note that no finite set of vertices separates $R$ from $X$ . Use this to construct the $R$ – $X$ paths inductively, or apply a trivial version of Menger’s theorem.   
60. A sequentially compact space (one where every infinite sequence of points has a convergent subsequence) is compact if it has a countable basis. If the infinity lemma does not seem to help, look at Lemma 8.2.2.   
61.+ For the compactness proof, use a normal spanning tree and imitate the proof of Proposition 8.5.1.   
62. Your answer may depend on whether $H$ is known to be locally finite. Remember that a continuous bijection from a compact space to Hausdorff space is a homeomorphism. For (iii), you may use a theorem from the text.   
63.+ For the first task, scale the lengths of the edges of the tree down to ensure that the total length of a ray starting at the root becomes finite. Then adjust the lengths of the other edges of $G$ , and extend the metric obtained to the ends of $G$ . For the second, notice that for any metric inducing the given topology on $V \cup \Omega$ the sets $V _ { n }$ of vertices at distance at least $1 / n$ from every end are closed, and show that these sets cover $V$ as $n$ ranges over the positive integers.   
64.+ To define the topology on $\hat { X }$ , imitate the definition of the usual onepoint compactification.   
65. You may use that deleting an open interval from the unit circle leaves a connected rest, but that deleting two disjoint open intervals does not. Remember that closed connected subsets of $| G |$ are path-connected.   
66. Construct two rays that belong to the same end and start at the same vertex but are otherwise disjoint. This can be done by considering a normal ray and using the fact that none of its vertices is a cutvertex.

67. Recall that, in $S ^ { 1 }$ , every point has a neighbourhood basis consisting of arcs in $\mathbb { R } ^ { 2 }$ . Can you show that every arc in $C$ that links two ends must meet an edge? If not, can you show that it meets a vertex? If not, remember the proof of Lemma 8.5.5.   
68. Exercise 26.   
69. Enumerate the double rays $D$ and $D _ { \ell }$ in one infinite sequence, and inductively define partial homeomorphisms between these $D _ { \ell }$ and suitable segments of $S ^ { 1 }$ . When this is done, extend the partial homeomorphism on the union of all the double rays to the ends of $G$ so as to make the final map continuous.   
70. The main assertion to be proved is that every subspace $C$ satisfying the conditions is a circle. Let $A \subseteq C$ be an arc linking two vertices $x _ { 0 }$ and $y _ { 0 }$ . If $\boldsymbol { v }$ is any vertex in $C \setminus A$ , the arc-connectedness of $C$ yields a $v$ – $A$ arc in $C$ , which has a first point on $A$ . By the degree condition assumed, this must be $x _ { 0 }$ or $y _ { 0 }$ . Starting from an enumeration $v _ { 0 } , v _ { 1 } , \ldots$ of the vertices in $C$ , construct a 2-way infinite sequence $\ldots . . x _ { - 2 } , x _ { - 1 } , x _ { 0 } , y _ { 0 } , y _ { 1 } , y _ { 2 } \ldots$ of vertices such that $C$ contains arcs $A _ { i }$ linking $x _ { - i - 1 }$ to $x _ { - i }$ and $B _ { i }$ linking $y _ { i }$ to $y _ { i + 1 }$ for all $i \in \mathbb N$ , so that the union $U$ of $A$ and all these arcs is a homeomorphic copy of $( 0 , 1 )$ in $C$ . Use the connectedness of its ‘tails’ to show that these converge to unique ends in $C$ . Deduce from the degree assumptions that these two ends coincide, and that $\overline { { U } } = C$ is a circle.   
71. Use Lemma 8.5.4. You may also use that every circle contains an edge.   
72. $-$ Show that if a topological spanning tree is homeomorphic to a space $| T |$ with $T$ a tree, but does not itself have this form, it contains an end which this homeomorphism maps to a point in $T$ (i.e., not to an end). Can you find a topological spanning tree for which this is impossible?   
73. Start with a maximal set of disjoint rays.   
74.+ Given a point $\omega \in { \overline { { A } } } \setminus A$ , pick a sequence $v _ { 1 } , v _ { 2 } , \ldots$ . of vertices in $A$ that converges to $\omega$ , and arcs $A _ { n } \subseteq A$ from $v _ { n }$ to $v _ { n + 1 }$ . Then use the infinity lemma to concatenate suitable portions of the $A _ { n }$ to form a continuous function $\alpha$ : $[ 0 , 1 ] \to | G |$ that maps $[ 0 , 1 )$ to $A$ and $^ { 1 }$ to $\omega$ . You may use the fact that the image of such a function $\alpha$ contains an arc from $\alpha ( 0 ) \in A$ to $\alpha ( 1 ) = \omega$ .   
75. Recall that non-separating induced cycles of a plane graph are face boundaries.   
76.− How can $\overline { T }$ fail to be a topological spanning tree?   
77. Find the circuits greedily, making sure all edges are captured.   
78. Check thinness. For an alternative proof, use Theorem 8.5.8 (i) instead of (ii).   
79. $^ +$ For the ‘only if’ part, use a theorem from the text. The task in the ‘if’ part is to combine the edge-disjoint circles from Theorem 8.5.8 (ii) into a single continuous image of $S ^ { 1 }$ . Start with one of those circles, and incorporate the others step by step. Check that the ‘limit map’ $\sigma \colon S ^ { 1 } \to | G |$ is continuous (and defined) on all of $S ^ { 1 }$ .

80.+ The conditions are easily seen to be necessary. To prove sufficiency, construct an Euler tour inductively, incorporating at once any finite components arising in the remaining graph. To ensure that all edges get included, enumerate them, and always target the next edge for inclusion. There are two cases. If $G$ has an odd cut, cover those edges first, join up their endvertices in pairs as far as possible, and proceed separately in the two infinite components of the rest. If $G$ has no odd cut, cover its edges inductively by a sequence of finite closed walks, so that each of these meets the next in a vertex. Then find an Euler tour in the union of these cycles.

# Hints for Chapter 9

1. $-$ Can you colour the edges of $K ^ { 5 }$ red and green without creating a red or a green triangle? Can you do the same for a $K ^ { 6 }$ ?   
2.− Induction on $c$ . In the induction step, unite two of the colour classes.   
3. If the chromatic number of a graph is small, does this imply the existence of a large induced $\overline { { K ^ { r } } }$ ? If so, how large?   
4.+ Choose a well-ordering of $\mathbb { R }$ , and compare it with the natural ordering. Use the fact that countable unions of countable sets are countable.   
5.+ Suppose there are many chords $x y$ , with $\textit { x } < _ { T } \textit { y }$ say, whose paths $x T y$ meet pairwise in at least one edge. Find either a large set of such vertices $_ x$ whose partner vertices $_ y$ coincide, or a vertex in $T$ with many incomparable vertices $_ y$ above it, or a long ascending path in $T$ whose maximal vertices $t _ { y }$ on $x T y$ are distinct for many $_ y$ . Then find a long sequence $x _ { 1 } \leqslant . . . \leqslant x _ { n }$ of vertices $x$ corresponding to these $_ y$ , and show that the union of the paths $x _ { i } T y _ { i }$ together with the chords $x _ { i } y _ { i }$ contains many edge-disjoint cycles.   
6.+ The first and second question are easy. To prove the theorem of Erd˝os and Szekeres, use induction on $k$ for fixed $\ell$ , and consider in the induction step the last elements of increasing subsequences of length $k$ . Alternatively, apply Dilworth’s Theorem.   
7. Use the fact that $n \geqslant 4$ points span a convex polygon if and only if every four of them do.   
8. Translate the given $k$ -partition of $\{ 1 , 2 , \ldots , n \}$ into a $k$ -colouring of the edges of $K ^ { \pi }$ .   
9. (i) is easy. For (ii) use the existence of $R ( 2 , k , 3 )$ .   
10. Begin by finding infinitely many sets whose pairwise intersections all have the same size.   
11. The exercise offers more information than you need. Consult Chapter 7.2 to see what is relevant.   
12. Imitate the proof of Proposition 9.2.1.   
13. The lower bound is easy. Given a colouring for the upper bound, consider a vertex and the neighbours joined to it by suitably coloured edges.

14.− Given $H _ { 1 }$ and $H _ { 2 }$ , construct a graph $H$ for which the $G$ of Theorem 9.3.1 satisfies ( ).   
15. $G \left[ U \to H \right]$ .   
16. Show inductively for $k = 0 , \ldots , m$ that $\omega ( G ^ { k } ) = \omega ( H )$ .   
17.− How exactly does Proposition 9.4.1 fail if we delete $K ^ { \prime }$ from the statement?   
18. As an example, prove that Theorem 9.4.5 (ii) is equivalent to Proposition 9.4.2. The other three equivalences are very similar.

# Hints for Chapter 10

1. Induction.   
2. Consider the union of two colour classes.   
3. Induction on $k$ with $n$ fixed; for the induction step consider $\overline { { G } }$   
4. $-$ What do $k$ -connected graphs look like that satisfy $\chi ( G ) \geqslant | G | / k$ but not $\alpha ( G ) \leqslant k$ ?   
5. Note that subdividing the edges at a vertex of odd degree is a useful trick to produce non-hamiltonian graphs. To find an example for (ii), apply this trick to a small but highly connected graph.   
6. How high can the connectivity of a planar graph be?   
7. $-$ Recall the definition of a hamiltonian sequence.   
8. $-$ On which kind of vertices does the Chv´atal condition come to bear? To check the validity of the condition for $G$ , first find such a vertex.   
9. Consider a $k$ -separator in $G ^ { 2 }$ . Where do its vertices send their $G$ -edges?   
10. Theorem 10.2.1.   
11. How does an arbitrary connected graph differ from the kind of graph whose square contains a Hamilton cycle by Fleischner’s theorem? How could this difference obstruct the existence of a Hamilton cycle?   
12.+ In the induction step consider a minimal cut.   
13.+ How can a Hamilton path $P \in \mathcal { H }$ be modified into another? In how many ways? What has this got to do with the degree in $G$ of the last vertex of $P$ ?

# Hints for Chapter 11

1. $-$ Consider a fixed choice of $m$ edges on $\{ 0 , 1 , \ldots , n \}$ . What is the probability that $G \in { \mathcal { G } } ( n , p )$ has precisely this edge set?   
2. Consider the appropriate indicator random variables, as in the proof of Lemma 11.1.5.   
3. Consider the appropriate indicator random variables.   
4. Erd˝os.

5. What would be the measure of the set $\{ G \}$ for a fixed $G$ ?

6. Consider the complementary properties.

7. $-$ $\mathcal { P } _ { 2 , 1 }$

8. Apply Lemma 11.3.2.

9. Induction on $| H |$ with the aid of Exercise 6.

10. Imitate the proof of Lemma 11.2.1.

11. Imitate the proof of Proposition 11.3.1. To bound the probabilities involved, use the inequality $1 - x \leqslant e ^ { - x }$ as in the proof of Lemma 11.2.1.

12.+ (i) Calculate the expected number of isolated vertices, and apply Lemma 11.4.2 as in the proof of Theorem 11.4.3.

(ii) Linearity.

13.+ Chapter 7.2, the proof of Erd˝os’s theorem, and a bit of Chebyshev.

14. For the first problem modify an increasing property slightly, so that it ceases to be increasing but keeps its threshold function. For the second, look for an increasing property whose probability does not really depend on $p$ .

15. $-$ Permutations of $V ( H )$

16. $-$ This is a result from the text in disguise.

17. $-$ Balance.

18. For $p / t \to 0$ apply Lemmas 11.1.4 and 11.1.5. For $p / t \to \infty$ apply Corollary 11.4.4.

19. There are only finitely many trees of order $k$

20.+ Show first that no such threshold function $t = t ( n )$ can tend to zero as $n \longrightarrow \infty$ . Then use Exercise 11.

21.+ Examine the various steps in the proof of Theorem 11.4.3, identify the two points where it now fails, and repair them. While the first part requires a slightly different tack as a consequence, the second adapts more mechanically.

# Hints for Chapter 12

1. $-$ Antisymmetry.

2. For the backward implication, assume first that $A$ has an infinite antichain; this case is easier. The proof for other case is not quite as obvious but similar; note that $A = \mathbb { Z }$ is not a counterexample.

3. To prove Proposition 12.1.1, consider an infinite sequence in which every strictly decreasing subsequence is finite. How does the last element of a maximal decreasing subsequence compare with the elements that come after it? For Corollary 12.1.2, start by proving that at least one element forms a good pair with infinitely many later elements.

4. An obvious approach is to try to imitate the proof of Lemma 12.1.3 for $\leqslant ^ { \prime }$ ; if it fails, what is the reason? Alternatively, you might try to modify the injective map produced by Lemma 12.1.3 into an orderpreserving one, without losing the property of $a \leqslant f ( a )$ for all $a$ .   
5.− This is an exercise in precision: ‘easy to see’ is not a proof. . .   
6. The trees in any bad sequence must get arbitrarily large. We are thus looking for trees $T , T ^ { \prime }$ such that $| T | < | T ^ { \prime } |$ but $T \not \leqslant T ^ { \prime }$ . Consider some simple examples, and iterate one to a bad sequence.   
7. Does the original proof ever map the root of a tree to an ordinary vertex of another tree?   
8. Can you extend a given graph $G$ to another graph from which $G$ can be obtained by deletion but not by contraction? Can you iterate this to build an infinite antichain?   
9.+ Can the graphs $G$ in a bad sequence have arbitrarily many independent edges? If not, they have bounded-size subsets of vertices that cover all their edges. (Why?) Consider a subsequence where these vertex sets all induce the same graph, and find a good subsequence therein.   
10. $^ +$ When we try to embed a graph $T G$ in another graph $H$ , the branch vertices of the $T G$ can be mapped only to vertices of at least the same degree. Extend a suitable graph $G$ to a similar graph $H$ that does not contain $G$ as a topological minor because these vertices are inconveniently positioned. Then iterate this example to obtain an infinite antichain.   
11. $^ +$ It is. One possible proof uses normal spanning trees with labels, and imitates the proof of Kruskal’s theorem.   
12.− The point about the ‘subtrees’ is that they are connected. Recall our convention that connected graphs are non-empty.   
13.− Start with any tree-decomposition of least width and modify it in steps.   
14. Why are there no cycles of tree-width 1?   
15. For the forward implication, apply Corollary 1.5.2. For the converse, use induction on $n$ .   
16. To prove (T2), consider the edge $e$ of Figure 12.3.1. Checking (T3) is easy.   
17. For the first question, recall Proposition 12.3.6. For the second, try to modify a tree-decomposition of $G$ into one of the $^ { \prime } G \mathbf { \Lambda }$ without increasing its width.   
18.+ Use a normal spanning tree $T$ as the decomposition tree, and let $t _ { 1 } , \ldots , t _ { n }$ be an enumeration of $V ( T )$ such that $t _ { 1 }$ is the root and all the sets $\{ t _ { 1 } , \ldots , t _ { i } \}$ are connected in $T$ . Define the parts $V _ { t }$ inductively for $t = t _ { 1 } , \ldots , t _ { n }$ so as to satisfy the condition in Exercise 15.   
19. For (i), translate the compatibility condition to a similar condition on the components of $T - e$ for the two choices of $e$ . For (ii), either find an ingenious way to define the $V _ { t }$ directly, or apply induction on $| S |$ and delete from $s$ a separation $\{ A , B \}$ with $A$ minimal. In the treedecomposition corresponding to ${ \mathcal { S } } \setminus \{ \{ A , B \} \}$ , find the part to which

the new part should be joined by orienting the tree edges as in the proof of Lemma 12.3.4.

20.+ For the first statement, let $H = H _ { t }$ be a torso that is not 3-connected. Show that there exists a cycle $C = v _ { 1 } \ldots v _ { k } v _ { 1 }$ with $V ( C ) \subseteq V ( H )$ (but not necessarily $C \subseteq H$ ) such that, for all $u , v , x , y \in V ( C )$ , the vertices $_ u$ and $\boldsymbol { v }$ separate $x$ from $_ y$ in $C$ if and only if they do so in $H$ . Choose $C$ maximal with respect to subdivision, and show that $H = C$ . For the second statement, build the graph up inductively from the torsos of its tree-decomposition, chosen in an order that keeps the partial decomposition tree connected.   
21. Modify the proof given in the text that the $k \times k$ grid has tree-width at least $k - 1$ .   
22. Existence was shown in Theorem 12.3.9; the task is to show uniqueness.   
23. $^ +$ Work out an explicit description of the sets $W _ { t } ^ { \prime }$ similar to the definition of the $W _ { t }$ , and compare the two.   
24. $-$ Induction.   
25. Induction.   
26. Use a result from Chapter 7.3. And don’t despair at a subgraph of $W$ !   
27.+ Show that the parts are precisely the maximal irreducible induced subgraphs of $G$ .   
28. Exercise 12.   
29. For the forward implication, interpret the subpaths of the decomposition path as intervals. Which subpath corresponds naturally to a given vertex of $G$ ?   
30. Follow the proof of Corollary 12.3.12.   
31. $^ +$ They do. To prove it, show first that every connected graph $G$ contains a path whose deletion decreases the path-width of $G$ . Then apply induction on a suitable set of trees, deleting a suitable path in the induction step.   
32. $-$ Compare $\scriptstyle { \mathcal { K P } }$ with its analogue for the stronger notion.   
33. To answer the first part, construct for each forbidden minor $X$ a finite set of graphs whose exclusion as topological minors is equivalent to forbidding $X$ as a minor. For the second part you may use Exercise 10.   
34. $-$ Find the required paths one by one.   
35.+ One direction is just a weakening of Lemma 12.4.5. For the other, imitate the proof of Lemma 12.3.4.   
36.+ Let $X$ be an externally $\ell$ -connected set of $h$ vertices in a graph $G$ , where $h$ and $\ell$ are large. Consider a small separator $S$ in $G$ : clearly, most of $X$ will lie in the same component of $G - S$ . Try to make these ‘large’ components, perhaps together with their separators $S$ , into the desired connected vertex sets.   
37. A tangle of order $k$ is a way of ‘directing’ the separations of order $< k$ . Direct them towards the set that Exercise 35 provides as a ‘certificate’ for large tree-width.

38. How much harder does it get to cover all the $M X$ in $G$ when $\mathcal { X }$ and the graphs $X \in \mathcal { X }$ get larger? How does the problem change if we replace $\mathcal { X }$ by the set of its minor-minimal elements?   
39.+ Let $S$ be a surface in which $H$ can be embedded. You may use the fact that the number of copies of $H$ that can be disjointly embedded in $S$ is bounded by some number $n \in \mathbb N$ . To show that $f$ cannot be defined for $k > n$ , consider a candidate $\ell \in \mathbb { N }$ for $f ( k )$ and extend a fixed drawing of $H$ on $S$ to a graph $H ^ { \prime }$ on $S$ that, after deleting any $\ell$ vertices, still has an $H$ minor.   
40. $^ +$ Find a counterexample.   
41. $^ +$ For an example showing that non-trivial tree-decompositions are necessary, use Exercise 31 and the fact that no surface can accommodate unboundedly many disjoint copies of $K ^ { 5 }$ . For the remaining examples, work with modifications of large grids or grid-like graphs on other surfaces than the sphere.   
42. Consult Chapter 7.2 for substructures to be found in graphs of large chromatic number.   
43. $K ^ { 5 }$   
44. Derive the minor theorem first for connected graphs.   
45. Use the separation properties of normal spanning trees proved in Chapter 1.5. If desired, you may use any exercise from Chapter 8.   
46. Choose suitable rays in $H$ as branch sets and new edges to join them.   
47. For the first question, consider in the $\mathbb { Z } \times \mathbb { Z }$ grid concentric cycles and paths between them, and use the fact that the $\mathbb { Z } \times \mathbb { N }$ grid is planar.   
48.+ The proof of the forward implication differs from the finite case in that we now have to construct the decomposition tree together with the parts. Try to do this inductively, starting with a maximal complete subgraph $H$ as the first part. To extend the decomposition into a component $C$ of $G - H$ , consider a vertex in $C$ with as many neighbours in $H$ as possible, and show that these include all the neighbours of $C$ in $H$ .   
49. For (i), assume that every finite subgraph of $G$ has a chordal supergraph of clique number at most $k$ , and show that so does $G$ . For (ii), add edges to make $G$ edge-maximal with the property that every finite subgraph has tree-width at most $k$ . Show that this supergraph of $G$ must be chordal.   
50. Planarity. You may use any exercise in Chapter 8.

Page numbers in italics refer to definitions; in the case of author names, they refer to theorems due to that author. The alphabetical order ignores letters that stand as variables; for example, $k$ -chromatic’ is listed under the letter c.

above, 15

abstract

dual, 105–106, 108

graph, 3, 83, 86, 92, 302

acyclic, 13–14, 48, 134

adhesion, 340, 341

adjacency matrix, 28, 32

adjacent, 3

Aharoni, R., 217, 223, 225, 226, 245, 247, 248

Ahuja, R.K., 161

algebraic

colouring theory, 137

flow theory, 144–159, 161

graph theory, 23–28, 32

planarity criteria, 101–102

algorithmic graph theory, 161, 349, 355–356

almost, 302, 312–313

Alon, N., 10, 32, 122, 137–138, 314

alternating

path, 34, 224

walk, 64

Andreae, Th., 207, 245, 246

antichain, 51, 53, 241, 316, 388, 389

antihole, 138

apex vertices, 340, 353

Appel, K., 137

arboricity, 46–49, 115, 190, 235, 250

arc, 84, 229, 243, 247, 248, 361, 385

-component, 229, 243

-connected, 229, 243, 248

Archdeacon, D., 355

Arnborg, S., 355

articulation point, see cutvertex

at, 2

augmenting path

for matching, 34, 51, 224, 241, 371

for network flow, 143, 160

automorphism, 3, 31, 215, 239, 374

average degree, 5

of bipartite planar graph, 376

bounded, 273

and choice number, 122

and chromatic number, 117, 122, 169, 172, 190

and connectivity, 12

forcing minors, 163, 170–171, 191, 192–194

forcing topological minors, 70, 169– 170

and girth, 8, 9–10, 301

and list colouring, 122

and minimum degree, 5–6

and number of edges, 5

and Ramsey numbers, 273

and regularity lemma, 176, 191

back-and-forth technique, 213–214, 383

bad sequence, 316, 354

balanced, 308

Bauer, 291

Behzad, M., 138

Bellenbaum, P., 355

below, 15

Berge, C., 128

Berger, E., 217, 247

between, 6, 84

Biggs, N.L., 32

binary tree, 203, 238

bipartite graphs, 17–18, 31, 107, 111, 127

edge colouring of, 119, 135, 136

flow number of cubic, 150

forced as subgraph, 169, 183

list-chromatic index of, 125–126, 138

matching in, 34–39, 222–224

in Ramsey theory, 263–264, 272

Birkhoff, G.D., 137

block, 55, 108, 372

graph, 56, 78, 372

B¨ohme, T., 81, 193

Bollob´as, B., 54, 80, 192, 193, 245, 272, 291, 304, 305, 313, 314, 356

bond, 25, 31, 56, 104–106, 110, 238

-cycle duality, 104–106, 152–154

space, see cut space

Bondy, J.A., 291

boundary circle, 3

of a face, 88–90, 107, 363

of a wave, 218

bounded subset of $\mathbb { R } ^ { 2 }$ , 86, 361

bounded graph conjecture, 238, 239, 244–245

bramble, 322–324, 351, 353, 355

number, 324

order of, 322

branch

set, 19

in tree-decomposition, 325

vertex, 20

Brandt, S., 192

bridge, 11, 41, 141, 151, 156–157

to bridge, 281

Broersma, 291

Brooks, R.L., 115, 134

theorem, 115, 137

list colouring version, 137

Bruhn, H., 110, 247, 248, 278, 291

Burr, S.A., 272

Cameron, P.J., 246

capacity, 142

function, 141

cardinality, 357

Catlin, P.A., 193

Cayley, A., 137, 313

central

face in grid, 342

vertex, 9, 342, 369

centre, 17

certificate, 126, 341, 356, 390

chain, 15, 51, 53, 241, 358, 360

Chebyshev inequality, 308, 388

Cherlin, G., 246

choice number, 121

and average degree, 122

of bipartite planar graphs, 135

of planar graphs, 122

$k$ -choosable, 121

chord, 8

chordal, 127–128, 136, 326, 352, 391

supergraph, 391

$k$ -chromatic, 111, 134

chromatic index, 112, 119

of bipartite graphs, 119

vs. list-chromatic index, 121, 124

and maximum degree, 119–121

chromatic number, 111, 134, 155, 201, 244, 353

and $K ^ { \prime }$ -subgraphs, 116–117, 126, 226

of almost all graphs, 304

and average degree, 117, 122, 169, 172, 190

vs. choice number, 121

and colouring number, 115

and connectivity, 116–117

constructions, 117–118, 134, 137

in extremal graph theory, 168

and flow number, 155

forcing minors, 172–175, 190, 191, 193–194

forcing short cycles, 117, 301

forcing subgraphs, 116–117, 238, 271

forcing a triangle, 135, 271

and girth, 117, 137, 175, 301

as a global phenomenon, 117, 126

and maximum degree, 115

and minimum degree, 115, 116

and number of edges, 114

chromatic polynomial, 134, 162

Chudnovsky, M., 128, 138

Chv´atal, V., 256, 278, 279, 291

circle

boundary circle, 361

in graph with ends, 106, 230, 231, 361

one/two-sided, 362

in surface, 348, 361, 362, 365

unit circle $S ^ { 1 }$ , 361

circuit, 23, 231, 242

circulation, 140–141, 153, 162

circumference, 8, 351

and connectivity, 79, 276

and minimum degree, 8

class 1 vs. class 2, 121

classification of surfaces, 361 –362

clique number, 126–133, 263, 326

of a random graph, 296

threshold function, 312

closed

under addition, 144, 232

under infinite sums, 235

under isomorphism, 3, 302, 327

wrt. minors, 135, 160, 245, 327, 341, 342, 349, 352

wrt. subgraphs, 126, 135

wrt. supergraphs, 126, 305

up or down, in tree-order, 15

walk, 10, 22

closure (of a set), 227

cocycle, see cut

$k$ -colourable, 111, 121, 201, 325

colour class, 111

colour-critical, see critically $k$ -chromatic

colouring, 111–138, 173, 201

algorithms, 114, 133

and flows, 152–155

number, 114, 134, 135, 245

plane graphs, 112–113, 152–155

in Ramsey theory, 253

total, 135, 138

3-colour theorem, see three colour thm.

4-colour theorem, see four colour thm.

5-colour theorem, see five colour thm.

comb, 196, 242

modified, 240

star-comb lemma, 204

combinatorial

isomorphism, 93, 94, 107, 108

set theory, 250, 272

Comfort, W.W., 250

compactness, 201, 227, 229, 242

proof technique, 200, 235–237, 238, 245

comparability graph, 127, 136

compatible separations, 351

complement

of a bipartite graph, 127, 135

of a graph, 4

and perfection, 129, 376

of a property, 327, 341

complete

bipartite graph, 17

graph, 3, 150

infinite graph, 197, 341

matching, see 1-factor

minor, 97, 101, 169–175, 190, 191, 193-194，340-341，347-348

multipartite graph, 17, 167

part of path-decomposition, 352

part of tree-decomposition, 326

$r$ -partite graph, 17

separator, 325, 352

subgraph, 117, 126–127, 163–167, 296, 312, 321

topological minor, 67–70, 81, 97, 101, 109, 169–170, 172, 175, 190, 194

complexity theory, 127, 341, 356

component, 11, 229, 361

connected, 10

arc-connected, 229, 243, 248

2-connected graphs, 55–57, 78, 89, 94, 270, 281

3-connected graphs, 57–62, 78, 89, 96, 97, 102, 269, 270

4-connected graphs, 108, 270, 278

$k$ -connected, 11, 12, 67, 79 externally, 329, 352

infinitely connected, 197, 237, 244

minimally connected, 14

minimally $k$ -connected, 80

semiconnected, 235–236

topologically, 229

and vertex enumeration, 10, 14

connectedness, 10, 14

connectivity, 11, 10–13, 55–81

and average degree, 12

and chromatic number, 116–117

and circumference, 79

and edge-connectivity, 12

external, 325, 329, 352, 353, 390

and girth, 237, 301

and Hamilton cycles, 277–278

in infinite graphs, 216–226

forcing minors, 354

and linkability, 70–71, 80, 81

and minimum degree, 12, 249

and plane duality, 108

and plane representation, 96

and Ramsey properties, 268–270

of a random graph, 303

via spanning trees, 46, 54

$k$ -constructible, 117–118, 134, 137

contains, 3

continuum many, 357

contraction, 18–21

and 3-connectedness, 58–59

and minors, 18–21

in multigraphs, 28–30, 160

and tree-width, 320, 321

convex

drawing, 99, 107, 109, 386

polygon, 271

core, 376

Corneil, D.G., 355

Cornuejols, G., 138

countable

graph, 2

set, 357

countably infinite, 357

cover

by antichains, 53

of a bramble, 322

by chains, 51

by edges, 136

by paths, 49–51, 223

by trees, 49, 106, 250

by vertices, 33, 34–35, 44–46, 322, 338

critical, 134

critically $k$ -chromatic, 134, 375, 380

crosscap, 362, 364

cross-edges, 24, 46, 235

crosses in grid, 322

crown, 269–270

$d$ -dimensional, 30, 313

of a graph, $G ^ { 3 }$ , 290

cubic graph, 5

connectivity of, 79

1-factor in, 41, 52

flow number of, 150, 151, 157, 161, 162

multigraph, 44, 52, 157, 282

cuff, 339

Curran, S., 54

cut, 24

capacity of, 142, 143

-cycle duality, 104–106, 152–154

-edge, see bridge

even/odd, 233, 243, 244, 249

flow across, 141

fundamental, 26, 32, 231, 243

minimal, 25, 31, 56, 104

in network, 142

space, 25–28, 31, 32, 101, 105, 249

cutvertex, 11, 55–56

cycle, 7 –8

-bond duality, 104–106, 152–154

directed, 134, 135

disjoint cycles, 44–45

double cover conjecture, 157, 160

edge-disjoint cycles, 190, 240, 271

expected number, 298

facial, 101

fundamental, 26, 32, 382

Hamilton, 160, 275–291

infinite, 278, 289

induced, 8, 23, 59, 89, 102, 127, 128, 243, 376, 380, 385

infinite, 106, 230–231, 249, 278

length, 8

long, 8, 30, 79, 134

in multigraphs, 29

non-separating, 59, 89, 102, 243, 385

odd, 17, 115, 128, 370, 376

with orientation, 152–154

short, 10, 117, 171–172, 299–301

space, 23–28, 31, 32, 59–62, 101–102, 105, 107, 109, 232–235, 243, 244, 248, 249, 374

topological, 232–235, 248, 249

threshold function, 311, 313

cyclomatic number, 23

cylinder, 362

Czipszer, J., 249

Dean, N., 291

de Bruijn, N.G., 201, 245

degeneracy, see colouring number

of an end, 204, 229, 231, 248

at a loop, 29

sequence, 278

deletion, 4

$\Delta$ -system, 271

graphs, 164, 167

linear order, 241

density

edge density, 164

of pair of vertex sets, 176

upper density, 189

depth-first search tree, 16, 31

Deuber, W., 258, 273

diameter, 8 –9, 312

and girth, 8

and radius, 9

Diestel, R., 110, 193, 216, 228, 233, 235, 244–250, 291, 340, 341, 355, 356

difference of graphs, 4, 86

digon, see double edge

digraph, see directed graph

Dilworth, R.P., 51, 53, 241, 372, 386

Dirac, G.A., 194, 276

directed

cycle, 134, 135

edge, 28

graph, 28, 49–50, 124, 135, 246, 376

path, 49, 134, 375, 376

direction, 140

disc, 361

disconnected, 10

disjoint graphs, 3

dispersed, 239

distance, 8

dominated, 238, 249

double

counting, 91, 109, 130–131, 298, 309

edge, 29, 103

ray, 196, 240, 250, 291

wheel, 269–270

down (-closure), 15

drawing, 2, 83, 92–96, 381

convex, 99, 109

straight-line, 99, 107

dual

abstract, 105–106, 108

and connectivity, 108

plane, 103–105, 108

duality

cycles and bonds, 26–28, 104–106, 152

flows and colourings, 152–155, 378

for infinite graphs, 106, 109, 110

of plane multigraphs, 103–106

tree-decompositions and brambles, 322

duplicating a vertex, 129, 166

edge, 2

crossing a partition, 24

directed, 28

double, 29

of a multigraph, 28

plane, 86

space, 23

topological, 226

$X { - } Y$ edge, 2

edge-chromatic number, see chromatic index

edge colouring, 112, 119–121, 253, 259

and flow number, 151

and matchings, 135

-edge-connected, 12

edge-connectivity, 12, 46, 67, 79, 134, 150, 197

edge contraction, 18

and 3-connectedness, 58

vs. minors, 19

in multigraph, 29

edge cover, 136

edge density, 5, 6, 164

and average degree, 5

forcing minors, 170

forcing path linkages, 71–77

forcing subgraphs, 164–169

forcing topological minors, 70, 169

and regularity lemma, 176, 191

edge-disjoint spanning trees, 46–49, 52, 197

edge-maximal, 4

vs. extremal, 165, 173

without $M K ^ { 5 }$ , 174

without ${ } T K _ { 3 , 3 }$ , 191

without $T K ^ { 4 }$ , 173

without $T K ^ { 5 }$ , $T K _ { 3 , 3 }$ , 100

edge space, 23, 31, 101, 232

Edmonds, J., 53, 225, 356

embedding

of bipartite graphs, 263–265

of graphs, 21

$k$ -near embedding, 340

in the plane, 92, 95–110

in $S ^ { 2 }$ , 85–86, 93

self-embedding, 349

in surface, 91, 109, 341–349, 353, 356, 363

empty graph, 2, 11

end

degree, 204, 229, 231, 248

in subspaces, 229, 231, 248–249

of edge, 2, 28

-faithful spanning tree, 242

of graph, 49, 106, 195, 202–203, 204–212, 226–244, 248–249

of path, 6

space, 226–237, 242

thick/thin, 208–212, 238

of topological space, 242

endpoints of arc, 84, 229

endvertex, 2, 28

terminal vertex, 28

enumeration, 357

equivalence

in definition of an end, 202, 242

of graph invariants, 190

of graph properties, 270

of planar embeddings, 92–96, 106, 107

of points in topological space, 84, 361

in quasi-order, 350

Erd˝os, P., 45, 53, 117, 137, 167, 169,

185, 192, 193, 194, 201, 213, 216,

217, 244, 245, 246–247, 249, 250,

258, 271, 272, 273, 277, 291, 293–

294, 296, 299–301, 306, 308, 314,

387

Erd˝os-Menger conjecture, 217, 247

Erd˝os-P´osa property, 44, 52, 338–339, 353

Erd˝os-P´osa theorem, 45, 53

edge version, 190, 271

generalization, 338–339

Erd˝os-S´os conjecture, 169, 189–190, 193

Erd˝os-Stone theorem, 164, 167–168, 186–187, 193

Euler, L., 22, 32, 91

characteristic, 363

formula, 91–92, 106, 363, 376

genus, 343, 363–366

tour, 22, 244, 378, 385

Eulerian graph, 22

infinite, 233, 244, 248, 249–250

even

degree, 22, 39

graph, 150, 151, 161, 248

event, 295

evolution of random graphs, 305, 313, 314

exceptional set, 176

excluded minors, see forbidden minors

existence proof, probabilistic, 137, 293, 297, 299–301

expanding a vertex, 129

expectation, 297–298, 307

exterior face, see outer face

external connectivity, 329, 352, 353

extremal

bipartite graph, 189

vs. edge-maximal, 164–165, 173

graph theory, 163–194, 248–249

graph, 164–166

without $M K ^ { 5 }$ , 174

without $^ { T K _ { 3 , 3 } }$ , 191

without $T K ^ { 4 }$ , 173

face, 86, 363

central face, 342

of hexagonal grid, 342

facial cycle, 101

factor, 33

1-factor, 33–43, 52, 216–226, 238, 241

1-factor theorem, 39, 41, 52, 53, 80, 81, 225, 247

2-factor, 39

k-factor, 33

factor-critical, 41, 225, 242, 371, 384

Fajtlowicz, S., 193

fan, 66, 238

-version of Menger’s theorem, 66, 238

finite

adhesion, 340, 341

graph, 2

set, 357

tree-width, 341

finite intersection property, 201

first order sentence, 303, 314

first point on frontier, 84

five colour theorem, 112, 137, 157

list version, 122, 138

five-flow conjecture, 156, 157, 162

Fleischner, H., 281, 289, 291, 387

flow, 139–162, 141–142

2-flow, 149

3-flow, 150, 157, 161

4-flow, 150–151, 156–157, 160, 161, 162

6-flow theorem, 157–159, 161, 162

$k$ -flow, 147–151, 156–159, 160, 161, 162

$H$ -flow, 144–149, 160

-colouring duality, 152–155, 378

conjectures, 156–157, 161, 162

group-valued, 144–149, 160, 161–162

integral, 142, 144

network flow, 141–144, 160, 161, 378

number, 147–151, 156, 160, 161

in plane graphs, 152–155

polynomial, 146, 149, 162

total value of, 142

forbidden minors

and chromatic number, 172–175

expressed by, 327, 340–349

in infinite graphs, 216, 244, 245, 340– 341

minimal set of, 341, 352, 355

planar, 328

and tree-width, 327–341

forcibly hamiltonian, see hamiltonian sequence

forcing

MKr, 169–175, 192–194, 340, 353

$M K ^ { \aleph _ { 0 } }$ , 341, 354

$T K ^ { 5 }$ , 174, 193

T Kr, 70, 169–170, 172, 175, 193–194

edge-disjoint spanning trees, 46

Hamilton cycles, 276–278, 281, 289

high connectivity, 12

induced trees, 169

large chromatic number, 117–118

linkability, 70–72, 81

long cycles, 8, 30, 79, 134, 275–291

long paths, 8, 30

minor with large minimum degree, 171, 193

short cycles, 10, 171–172, 175, 301

subgraph, 15, 163–169, 175–194

tree, 15, 169

triangle, 135, 271

Ford, L.R. Jr., 143, 161

forest, 13, 173, 327

minor, 355

partitions, 48–49, 53, 250

plane, 88, 106

topological, 250

tree-width of, 327, 351

four colour problem, 137, 193

four colour theorem, 112, 157, 161, 172, 174, 191, 278, 290

history, 137

four-flow conjecture, 156–157

Fra¨ıss´e, R., 246

Frank, A., 80, 161

Freudenthal, H., 248

compactification, 227, 248

ends, 242

Frobenius, F.G, 53

from . . . to, 6

frontier, 84, 361

Fulkerson, D.R., 122, 143, 161

fundamental

circuit, 231, 233, 243

cocycle, 26, 32

cut, 26, 32, 231, 243

cycle, 26, 32

Gale, D., 38

Gallai, T., 32, 43, 50, 52, 53, 54, 81, 192, 238, 249

Gallai-Edmonds matching theorem, 41– 43, 53, 225, 247

Galvin, F., 125, 138

Gasparian, G.S., 129, 138

Geelen, J., 356

generated, 233

genus

and colouring, 137

Euler genus, 343, 363–366

of a graph, 106, 353

orientable, 353

of a surface, 348

geometric dual, see plane dual

Georgakopoulos, A., 248

Gibbons, A., 161

Gilmore, P.C., 136

girth, 8

and average degree, 9–10, 301

and chromatic number, 117, 137, 299–301

and connectivity, 81, 237, 301

and diameter, 8

and minimum degree, 8, 10, 30, 171, 301

and minors, 170–172, 191, 193

and planarity, 106, 237

and topological minors, 172, 175

Godsil, C., 32

Golumbic, M.C., 138

good

characterization, 341, 356

pair, 316, 347

sequence, 316

Gorbunov, K.Yu., 355

G¨oring, F., 81

Graham, R.L., 272

graph, 2 –4, 28, 30

homogeneous, 215, 240, 246

invariant, 3, 30, 190, 297

minor theorem, 315, 341–348, 342, 349, 354, 355

for trees, 317 –318

partition, 48

plane, 86–92, 103–106, 112–113, 122– 124, 152–155

process, 314

property, 3, 212, 270, 302, 312, 327, 342, 356

simple, 30

universal, 212–216, 213, 240, 246

graphic sequence, see degree sequence

graph-theoretical isomorphism, 93–94

greedy algorithm, 114, 124, 133

grid, 107, 208, 322

canonical subgrid, 342

hexagonal grid, 208, 209, 342–346

minor, 240, 324, 328–338, 354

theorem, 328

tree-width of, 324, 351, 354

Gr¨otzsch, H., 113, 137, 157, 161

group-valued flow, 144–149, 160, 161– 162

Gr¨unwald, T., see Gallai

Gusfield, D., 53

Guthrie, F., 137

Gy´arf´as, A., 169, 190, 194

Hadwiger, H., 172, 193

conjecture, 172–175, 191, 193

Hajnal, A., 244, 245, 249, 250, 258, 272, 273

Haj´os, G., 118, 137, 175

conjecture, 175, 193

construction, 117–118

Haken, W., 137

Halin, R., 80, 206, 208, 244, 245–246, 354–355, 356

Hall, P., 36, 51, 53, 224

Hamilton, W.R., 290

Hamilton circle, 278, 289, 291

Hamilton cycle, 275–291

in $G ^ { 2 }$ , 281–289

in $G ^ { 3 }$ , 290

in almost all graphs, 305

and degree sequence, 278–281, 289

and the four colour theorem, 278

and 4-flows, 160, 278

in infinite graph, see Hamilton circle

and minimum degree, 276

in planar graphs, 278

power of, 289

sufficient conditions, 275–281

Hamilton path, 275, 280–281, 289, 290

hamiltonian

graph, 275

sequence, 279

handle, 362, 364

Harant, J., 81

head, see terminal vertex

Heawood, P.J., 137, 161

Heesch, H., 137

height, 15

hexagonal grid, 208, 209, 342–346

Higman, D.G., 316, 354

Hoffman, A.J., 136

hole, 138

Holz, M., 247

homogeneous graphs, 215, 240, 246

Hoory, S., 10, 32

Huck, A., 244

hypergraph, 28

incidence, 2

encoding of planar embedding, see combinatorial isomorphism

map, 29

matrix, 27

incident, 2, 88

incomparability graph, 242

increasing property, 305, 313

independence number, 126–133

and connectivity, 276–277

and covers, 50, 52

and Hamilton cycles, 276–277

and long cycles, 134

and perfection, 132

of random graph, 296, 312

independent

edges, 3, 33–43, 52

events, 295

paths, 7, 66–67, 677–69, 370

vertices, 3, 50, 124, 296

indicator random variable, 298, 387

induced subgraph, 3 –4, 68, 126, 128, 132, 376

of almost all graphs, 302, 313

cycle, 8, 23, 31, 59, 89, 102, 127, 128, 249, 376, 380, 385

of all imperfect graphs, 129, 135

of all large connected graphs, 268

in Ramsey theory, 252, 258–268, 271

in random graph, 296, 313

tree, 169, 190

induction

transfinite, 198–199, 359

Zorn’s Lemma, 198, 237, 360

inductive ordering, 199

infinite

graphs, 2, 19, 31, 51, 110, 189, 195– 250, 253, 278, 289, 291, 305–306, 340–341, 349, 354, 356

sequence of steps, 197, 206

set, 357

basic properties, 197–198

infinitely connected, 197, 237, 244

infinity lemma, 200, 245, 383

initial

segment, 358

vertex, 28

inner

face, 86

point, 226

vertex, 6

integral

flow, 142, 144

function, 142

interior

of an arc, 84

of a path, $\mathring { P }$ , 6–7

internally disjoint, see independent

intersection, 3

graph, 352

interval graph, 127, 136, 352

into, 319

invariant, 3

irreducible graph, 352

Irving, R.W., 53

isolated vertex, 5, 313

isomorphic, 3

isomorphism, 3

of plane graphs, 92–96

isthmus, see bridge

Itai, A., 54

Jaeger, F., 162

Janson, S., 313

Jensen, T.R., 136, 162, 355

Johnson, D., 356

join, 2

J´onsson, B., 246

Jordan, C., 84, 86

Jordan Curve Theorem, 84, 109

Jung, H.A., 70, 194, 205, 239, 245

Kahn, J., 138

Karo´nski, M., 314

Kawarabayashi, K., 193

Kelmans, A.K., 102, 109–110

Kempe, A.B., 137, 290

kernel

of directed graph, 124, 135

of incidence matrix, 27

Kirchhoff’s law, 139, 140

Klein four-group, 151

Kleitman, D.J., 137

knotless graph, 349

knot theory, 162

Kochol, M., 149, 162

Kohayakawa, Y., 194

Koll´ar, J., 192

Koml´os, J., 192, 194, 272, 289, 291

K¨onig, D., 35, 53, 119, 200, 245

duality theorem, 35, 49, 51, 52, 63, 127, 136, 223

infinity lemma, 200, 245

K¨onigsberg bridges, 21

Korman, V., 226

Kostochka, A.V., 170, 192, 273

Kriesell, 53

Kruskal, J.A., 317, 354, 389

K¨uhn, D., 81, 172, 175, 193, 194, 216, 233, 246–250

Kuratowski, C., 96–101, 109, 238, 249, 356

-theorem for higher surfaces, 342

-type characterization, 107, 270, 341– 342, 355–356

Kuratowski set

of graphs, 341–342, 355

of graph properties, 270

Lachlan, A.H., 215, 246

large wave, 218

Larman, D.G., 70

Latin square, 135

Laviolette, F., 250

Leader, I.B., 245, 246

leaf, 13, 15, 31, 204

lean tree-decomposition, 325

Lee, O., 54

of a cycle, 8

of a path, 6, 8

of a walk, 10

level, 15

limit, 199–200, 358

wave, 218

line (edge), 2

graph, 4, 112, 136, 191

segment, 84

linear

algebra, 23–28, 59–61, 101–102, 132

decomposition, 339–340

programming, 161

Linial, N., 10, 32

linkable, 219

linked

by an arc, 84

by a path, 6

$k$ -linked, 69–77, 80, 81, 170

vs. $k$ -connected, 69–71, 80, 81

tree-decomposition, 325

vertices, 6, 84

-chromatic index, 121, 124–126, 135, 138

-chromatic number, see choice number

colouring, 121–126, 137–138

bipartite graphs, 124–126, 135

Brooks’s theorem, 137

conjecture, 124, 135, 138

$k$ -list-colourable, see $k$ -choosable

Liu, X., 138

Lloyd, E.K., 32

locally finite, 196, 248, 249

logarithms, 1

loop, 28

Lov´asz, L., 53, 129, 132, 137, 138, 192

Luczak, T., 313, 314

MacLane, S., 101, 109–110

Mader, W., 12, 32, 67–69, 80, 81, 170, 190, 192, 193, 355

Magnanti, T.L., 161

Maharry, J., 193

Mani, P., 70

map colouring, 111–113, 133, 136, 152

Markov chain, 314

Markov’s inequality, 297, 301, 307, 309

marriage theorem, 35 –36, 39, 51, 53, 223–224, 238, 371

stable, 38, 53, 126, 383

matchable, 41, 223

matching, 33–54

in bipartite graphs, 34–39, 127

and edge colouring, 135

in general graphs, 39–43

in infinite graphs, 222–226, 241–242, 247–248

partial, 224, 241

stable, 38, 51, 52, 126

of vertex set, 33

M´at´e, A., 250, 272

matroid theory, 54, 110, 356

max-flow min-cut theorem, 141, 143, 160, 161

maximal, 4

acyclic graph, 14

element, 358, 360

planar graph, 96, 101, 107, 109, 174, 191, 374

plane graph, 90, 96

wave, 218

maximum degree, 5

bounded, 184, 256

and chromatic number, 115

and chromatic index, 119–121

and list-chromatic index, 126, 138

and radius, 9

and Ramsey numbers, 256–257

and total chromatic number, 135

Menger, K., 53, 62–67, 79, 81, 160,206, 216–226, 241, 246–247

theorem of, 62–67, 79, 81, 160, 206– 207, 216, 217, 238, 246–247

$k$ -mesh, 329

metrizable, 228, 242

Milgram, A.N., 50, 52, 53, 54

Milner, E.C., 245

minimal, 4

connected graph, 14

$k$ -connected graph, 80

cut, 25, 31, 56, 104, 152

element, 358

non-planar graph, 107

separator, 78

set of forbidden minors, 341, 353, 355–356

minimum degree, 5

and average degree, 5

and choice number, 121–122

and chromatic number, 115, 116–117

and circumference, 8

and connectivity, 12, 80, 249

and edge-connectivity, 12

forcing Hamilton cycle, 276, 289

forcing long cycles, 8

forcing long paths, 8, 30

forcing short cycles, 10, 171–172, 175, 301

forcing trees, 15

and girth, 8, 9, 10, 170–172, 193, 301

and linkability, 71

minor, 18–21, 20, 169–172

$K _ { 3 , 3 }$ , 109, 191

$K ^ { 4 }$ , 173, 327

$K ^ { 5 }$ , 174, 193, 352

$K ^ { 5 }$ and $K _ { 3 , 3 }$ , 96–101

$K ^ { 6 }$ , 175

$K ^ { r }$ , 170, 171, 172, 190, 191, 193–194, 313, 340, 353, 354

$K ^ { \aleph _ { 0 } }$ , 341, 354

of all large 3- or 4-connected graphs, 269–270

-closed graph property, 327, 341–349, 352

excluded, see forbidden

forbidden, 172–175, 216, 244, 327– 349, 352, 354–356

forced, 171, 172, 169–175

incomplete, 192

infinite, 197, 207–208, 216, 240, 244, 245, 246, 248–249, 354, 356

of multigraph, 29

Petersen graph, 156

and planarity, 96–101, 107

proper, 349

relation, 20, 31, 207, 216, 240, 246, 270, 321, 342

theorem, 315, 341–349, 342, 354–355 proof, 342–348

for trees, 317–318

vs. topological minor, 20–21, 97

and WQO, 315–356

(see also topological minor)

M¨obius

crown, 269–270

ladder, 174

strip, 362

Mohar, B., 109, 137, 193, 356

moment

first, see Markov’s inequality second, 306–312

monochromatic (in Ramsey theory)

induced subgraph, 257–268

(vertex) set, 253 –255

subgraph, 253, 255–257

Moore bound, 10, 32

multigraph, 28–30

cubic, 44, 52, 157, 282

list chromatic index of, 138

plane, 103

multiple edge, 28

multiplicity, 248

Murty, U.S.R., 291

Myers, J.S., 192

Nash-Williams, C.St.J.A., 46, 49, 53, 235, 244, 246, 247, 249–250, 291, 354

$k$ -near embedding, 340

nearly planar, 340, 341

Negropontis, S., 250

neighbour of a set of vertices, 5 of a vertex, 3

Neˇsetˇril, J., 272, 273

network, 141–144 theory, 161

Niedermeyer, F., 244, 248

node (vertex), 2

normal tree, 15–16, 31, 155, 160, 271, 389 in infinite graphs, 205, 228, 232, 239, 242, 245, 341, 356

ray, 205, 239, 384

nowhere

dense, 49

zero, 144, 162

null, see empty

obstruction

to small tree-width, 322–324, 328– 329, 354, 355

octahedron, 12, 17, 355

odd

component, 39, 238

cycle, 17, 115, 128, 135, 138, 370, 376

degree, 5, 290, 387

on, 2

one-factor theorem, 39, 53, 81, 225

open Euler tour, 244

Oporowski, B., 269, 270, 273, 354

order

of a bramble, 322

of a graph, 2

of a mesh or premesh, 329

partial, 15, 20, 31, 50–51, 53, 136, 350, 357, 358, 360

quasi-, 316

of a separation, 11

tree-, 15, 31

type, 358

well-quasi-, 315–317, 342, 350, 354

ordinal, 358–359

orientable surface, 353

plane as, 153

orientation, 28, 124, 134, 161, 190, 376

cycle with, 152–153

oriented graph, 28, 289

Orlin, J.B., 161

Osthus, D., 81, 172, 175, 193, 194

outer face, 86, 93–94, 107

outerplanar, 107

Oxley, J.G., 93, 110, 250, 269, 270, 273

Oxtoby, J.C., 250

packing, 33, 44–49, 52, 235, 250

Palmer, E.M., 313

parallel edges, 29

parity, 5, 39, 42, 290

part of tree-decomposition, 319

partially ordered set, 50–51, 53, 241, 358, 360

$r$ -partite, 17

partition, 1, 48, 253

pasting, 127, 173, 174, 191, 325, 352

path, 6 –10, 196

$^ a$ –b-path, 7, 66

A–B-path, 7, 62–67, 79, 216–223, 237

$H$ -path, 7, 57, 67–69, 79, 80, 81

alternating, 34 –35, 37, 63

between given pairs of vertices, 69–77

-connected, 248, 384

cover, 49–51, 50, 223, 372

-decomposition, 339, 352

directed, 49

disjoint paths, 50, 62–67, 69–77, 217– 222

edge-disjoint, 46, 66–67, 68–69

-hamiltonian sequence, 280–281

independent paths, 7, 66–67, 67–69, 79, 80, 370

induced, 270

length, 6

linkage, 69–77, 81, 373

-width, 352, 355

perfect, 126–133, 135–136, 137–138, 226

graph conjectures, 128

graph theorems, 128, 129, 135, 138

matching, see 1-factor

strongly, 226, 242

weakly, 226, 242

Petersen, J., 39, 41

Petersen graph, 156–157

piecewise linear, 83

planar, 96–110, 112–113, 122, 216, 328, 338, 341

embedding, 92, 96–110

nearly planar, 340, 341

planarity criteria

Kelmans, 102

Kuratowski, 101

MacLane, 101

Tutte, 109

Whitney, 105

plane

dual, 103

duality, 103–106, 108, 152–155

graph, 86–92

multigraph, 103–106, 108, 152–155

triangulation, 90, 91, 161, 325

Plummer, M.D., 53

Podewski, K.P., 247, 248

point (vertex), 2

pointwise greater, 279

Polat, N., 248

polygon, 84

polygonal arc, 84, 85

P´osa, L., 45, 53, 258, 273

power

of a graph, 281

set, 357

predecessor, 358

preferences, 38, 51, 126

premesh, 329

Prikry, K., 245

probabilistic method, 293, 299–302, 314

projective plane, 355

proper

minor, 349

separation, 11

subgraph, 3

wave, 218

property, 3, 270, 302

of almost all graphs, 302–306, 311– 312

increasing, 305

minor-closed, 327, 352

Proskurowski, A., 355

pseudo-random graph, 272

Pym, J.S., 223, 247

quasi-ordering, 315–317, 342, 350, 354

radius, 9

and diameter, 9, 30

and maximum degree, 9

Rado, R., 245, 246, 250, 272

graph, 214–215, 240, 241, 246, 306

Rado’s selection lemma, 245

Ramsey, F.P., 252–255

Ramsey

graph, 258

-minimal, 257–258

numbers, 253, 255, 271, 272–273, 296, 314

Ramsey theory, 251–273

and connectivity, 268–270

induced, 258–268

infinite, 253–254, 271, 272

random graph, 170, 175, 255, 293–314, 295

evolution, 305, 311, 314

infinite, 305–306

process, 314

uniform model, 314

random variable, 297

indicator r.v., 298, 387

ray, 196, 200, 204, 206, 239, 240, 242, 341

double, 196, 240, 250, 291

normal, 205, 239, 384

spanning, 291

recursive definition, 359–360

reducible configuration, 137

Reed, B.A., 53, 355

refining a partition, 1, 178–182

region, 84–86

on $S ^ { 2 }$ , 86

regular, 5, 37, 39, 135, 289

$\epsilon$ -regular

pair, 176, 191

partition, 176

regularity

graph, 184

inflated, $R _ { s }$ , 256

lemma, 164, 175–188, 176, 191, 193– 194, 272

R´enyi, A., 213, 246, 306, 308, 314

Richardson, M., 135

Richter, B., 356

rigid-circuit, see chordal

R´ıha, S., 291 ˇ

ring, 342–343

Robertson, N., 53, 128, 137, 138, 162, 175, 193, 321, 328, 340, 341, 342, 354–355, 356

R¨odl, V., 194, 256, 258, 272–273

R´onyai, L., 192

root, 15

rooted tree, 15, 317, 350

Rothschild, B.L., 272

Royle, G.F., 32

Ruci´nski, A., 313, 314

Salazar, G., 356

Sanders, D.P., 137

S´ark¨ozy, G.N., 289, 291

saturated, see edge-maximal

Sauer, N., 246

Schelp, R.H., 210

Schoenflies, A.M., 86

Schrijver, A., 53, 80, 81, 138, 161

Schur, I., 271

Scott, A.D., 194, 246

second moment, 306–312, 307

self-minor conjecture, 349, 353, 354

semiconnected, 235–236

separate

a graph, 11, 62, 66, 67

the plane, 84

separating circle, 362, 365

separation, 11

compatible, 351

order of, 11

and tree-decompositions, 320, 351, 353

separator, 11

sequential colouring, see greedy algorithm

series-parallel, 191

set

$k$ -set, 1

countable, 357

countably infinite, 357

finite, 357

infinite, 357

power set, 357

system, see hypergraph

well-founded, 358

Seymour, P.D., 53, 128, 137, 138, 157, 162, 175, 193, 289, 291, 321, 322, 328, 340, 341, 342, 349, 354, 355, 356

Shapley, L.S., 38

Shelah, S., 244, 245, 246, 247

Shi, N., 246

shift-graph, 271

Simonovits, M., 53, 192, 194, 272

simple

basis, 101, 109

graph, 30

simplicial tree-decomposition, 244, 325, 352, 355

six-flow theorem, 157, 162

small wave, 218

snark, 157

planar, 157, 161, 278

S´os, V., 169, 189, 190, 192

spanned subgraph, 4

spanning

ray, 291

subgraph, 4

trees, 14, 16

edge-disjoint, 46–49

end-faithful, 242

normal, 15–16, 31, 205, 228, 232, 239, 242, 245, 341, 356

number of, 313

topological, 49, 231–237, 242, 243, 250, 385

sparse graphs, 163, 169–172, 191, 194, 255–256, 273

Spencer, J.H., 272, 314

Sperner’s lemma, 51

sphere $S ^ { 2 }$ , 86, 93–95, 361

spine, 196

Spr¨ussel, Ph., 32

square

of a graph, 281–289, 290, 291

Latin, 135

stability number, see independence number

stable

marriage, 38, 53, 126, 383

matching, 38, 51, 52, 126

set, 3

standard

basis, 23

subspace, 227, 231, 236, 243

star, 17, 190, 258, 270

centre of, 17

induced, 268

infinite, 204

-shape, 374

star-comb lemma, 204, 205

Steffens, K., 224, 247

Stein, M., 247, 248, 250

Steinitz, E., 109

stereographic projection, 85

Stillwell, J., 109

Stone, A.H., 167, 183

straight line segment, 84

strip neighbourhood, 88, 362

strong core, 376

strongly perfect, 226, 242

subcontraction, see minor

subdividing vertex, 20

subdivision, 20

subgraph, 3

of all large $k$ -connected graphs, 268– 270

forced by edge density, 164–169, 175– 188, 189, 190, 191

of high connectivity, 12

induced, 3

of large minimum degree, 6, 115, 134

spanning, 4

successor, 358

Sudakov, B., 273

sum

of edge sets, 23

of flows, 149

of thin families, 232

supergraph, 3

suppressing a vertex, 29

surface, 339, 342, 343, 361–367

surgery on, 364

surgery on surfaces, 364

capping, 364

cutting, 364

symmetric difference, 23, 34, 64

system of distinct representatives, 51

Szab´o, T., 192

Szekeres, G., 271

Szemer´edi, E., 176, 192, 194, 256, 272, 289, 291

see also regularity lemma

tail

of an edge, see initial vertex

of a ray, 196, 237

Tait, P.G., 137, 290–291

tangle, 353, 355

Tarsi, M., 137

teeth, 196

terminal vertex, 28

thick/thin end, 208–212, 238

thin

end, 208–212, 238

family, 232

sum, 232

Thomas, R., 53, 71, 81, 128, 137, 138, 162, 175, 193, 269, 270, 273, 291, 322, 325, 340, 341, 354, 355, 356

Thomason, A.G., 170, 192, 305

Thomass´e, S., 246

Thomassen, C., 80, 109, 122, 137, 138, 171, 193, 244, 247, 291, 355, 356, 365

three colour theorem, 113

three-flow conjecture, 157

threshold function, 305–312, 313, 314

Toft, B., 136, 162

topological

connectedness, 229, 236

cycle space, 232–235, 248, 249

edge, 226

end degree, 229

end space, 226–237, 242

Euler tour, 244

forest, 250

isomorphism, 93, 94, 104

spanning tree, 49, 231–237, 242, 243, 250, 385

topological minor, 20

$K _ { 3 , 3 }$ , 92, 97, 100, 101, 109, 191

$K ^ { 4 }$ , 59, 173–174, 191, 327

$K ^ { 5 }$ , 92, 97, 100, 101, 109, 174, 193, 352

$K ^ { 5 }$ and $K _ { 3 , 3 }$ , 92, 97, 100, 101, 107, 109

$K ^ { r }$ , 70, 165, 169–172, 175, 190, 191, 193–194, 252, 268, 340

$K ^ { \aleph _ { 0 } }$ , 197, 205, 238, 241, 341, 354

of all large 2-connected graphs, 269

forced by average degree, 70, 169–172

forced by chromatic number, 175

forced by girth, 172, 175

induced, 170

as order relation, 20

vs. ordinary minor, 20, 97

and planarity, 92, 96–101

tree (induced), 169

and WQO of general graphs, 350

and WQO of trees, 317

torso, 339–341

total chromatic number, 135

total colouring, 135

conjecture, 135, 138

total value of a flow, 142

touching sets, 322

$t$ -tough, 277–278, 290

toughness conjecture, 278, 289, 290, 291

tournament, 289

transfinite induction, 198–199, 359

transitive graph, 52

travelling salesman problem, 290

tree, 13 –16

binary, 203, 238

cover, 46–49

as forced substructure, 15, 169, 190

level of, 15

normal, 15–16, 31, 155, 160, 389

infinite, 205, 228, 232, 239, 242, 245, 341, 356

-order, 15

-packing, 46–48, 52, 53, 235, 249, 250

path-width of, 352

spanning, 14, 16, 198, 205

topological, 49, 231–237, 242, 243, 250, 385

threshold function for, 312

well-quasi-ordering of trees, 317–318

tree-decomposition, 193, 319–326, 340, 341, 351, 354–355

induced on minors, 320

induced on subgraphs, 320

lean, 325

obstructions, 322–324, 328–329, 354, 355

part of, 319

simplicial, 325, 339, 352, 355

width of, 321

tree-packing theorem, 46, 235

tree-width, 321–341

and brambles, 322–324, 353, 355

compactness theorem, 354

duality theorem, 322–324

finite, 341

and forbidden minors, 327–341

of grid, 324, 351, 354

of a minor, 321

obstructions to small, 322–324, 328– 329, 354, 355

of a subdivision, 351

triangle, 3

triangulated, see chordal

triangulation, see plane triangulation

trivial graph, 2

Trotter, W.T., 256, 272

Tur´an, P., 165

theorem, 165, 192, 256

graph, 165–169, 192, 379

Tutte, W.T., 39, 46, 53, 57, 58, 59, 80, 102, 109, 144, 147, 155, 161–162, 225, 235, 250, 278, 291

condition, 39–40

cycle basis theorem, 59, 249

decomposition of 2-connected graphs into 3-connected pieces, 57

1-factor theorem, 39, 53, 225

flow conjectures, 156–157, 162

planarity criterion, 102, 109

polynomial, 162

tree-packing theorem, 46, 53–54, 235, 250

wheel theorem, 58–59, 80

Tychonoff’s theorem, 201, 245, 381

ubiquitous, 207, 240, 246

conjecture, 207, 240, 246

unbalanced subgraph, 312, 313, 314

unfriendly partition conjecture, 202, 245

uniformity lemma, see regularity lemma

union, 3

unit circle $S ^ { 1 }$ , 84, 361

universal graphs, 212–216, 213, 240, 246

unmatched, 33

up (-closure), 15

upper

bound, 358

density, 189

Urquhart, A., 137

valency (degree), 5

value of a flow, 142

variance, 307

Veldman, H.J., 291

Vella, A., 249

vertex, 2

-chromatic number, 111

colouring, 111, 114–118

-connectivity, 11

cover, 34, 49–51

cut, see separator

duplication, 166

expansion, 129

of a plane graph, 86

space, 23

-transitive, 52, 215, 239

Vince, A., 314

Vizing, V.G., 119, 137, 138, 376, 377, 380

Voigt, M., 137–138

vortex, 340, 353

Vuˇskovi´c, K., 138

Wagner, K., 101, 109, 174, 190, 191, 193, 354–355

‘Wagner’s Conjecture’, see graph minor theorem

Wagner graph, 174, 325–326, 352

walk, 10

alternating, 64

closed, 10

length, 10

wave, 217, 241

large, 218

limit, 218

maximal, 218

proper, 218

small, 218

weakly perfect, 226, 242

well-founded set, 358

well-ordering, 358, 386

theorem, 358

well-quasi-ordering, 316–356

Welsh, D.J.A., 162

wheel, 59, 270

theorem, 58–59, 80

Whitney, H., 81, 96, 105

width of tree-decomposition, 321

Wilson, R.J., 32

Winkler, P., 314

Wollan, P., 71, 81

Woodrow, R.E., 215, 246

Yu, X., 54, 291

Zehavi, A., 54

Zorn’s lemma, 198, 237, 360

Zykov, A.A., 192

The entries in this index are divided into two groups. Entries involving only mathematical symbols (i.e. no letters except variables) are listed on the first page, grouped loosely by logical function. The entry ‘[ ]’, for example, refers to the definition of induced subgraphs $H \left[ U \right]$ on page 4 as well as to the definition of face boundaries $G \left[ f \right]$ on page 88.

Entries involving fixed letters as constituent parts are listed on the second page, in typographical groups ordered alphabetically by those letters. Letters standing as variables are ignored in the ordering.

$$
\begin{array}{l} \varnothing \qquad \qquad 2 \\ = \quad 3 \\ \simeq \quad 3 \\ \subseteq \quad 3 \\ \leqslant \quad 3 1 7, 3 5 7 \\ \preceq 2 0 \\ + \quad 4, 2 3, 1 4 4 \\ - \quad 4, 8 6, 1 4 4 \\ \in \quad 2 \\ \searrow \quad 8 6 \\ \bigcup \qquad \qquad \qquad \qquad 1 \\ \cup , \cap \quad 3 \\ \begin{array}{c c} * & 4 \end{array} \\ \end{array}
$$

$$
\begin{array}{l} \lfloor \rfloor 1, 1 5 \\ \left[ \begin{array}{c} 1, 1 5 \end{array} \right. \\ 2, 1 4 2, 2 2 6 \\ \| \quad \| \quad 2, 1 7 6 \\ [ ] \quad 4, 8 8, 2 2 6 \\ [ ] ^ {k}, [ ] ^ {<   \omega} \quad 1, 3 1 6 \\ \end{array}
$$

$$
\begin{array}{l} \langle \quad , \quad \rangle (23) \\ / \quad 1 8, 1 9, 2 9 \\ \mathcal {C} ^ {\perp}, \mathcal {F} ^ {\perp}, \dots 2 3 \\ \overline {{0}}, \overline {{1}}, \overline {{2}}, \dots \quad 1 \\ (n) _ {k}, \dots (298) \\ E (v), E ^ {\prime} (w), \dots 2 \\ E (X, Y), E ^ {\prime} (U, W), \dots 2 \\ (e, x, y), (u, v), \dots 1 4 0, 2 2 6 \\ \vec {E}, \vec {F}, \vec {C}, \dots 1 4 0, 1 5 2, 1 5 4 \\ \bar {e}, \bar {E}, \bar {F}, \dots 1 4 0 \\ f (X, Y), g (U, W), \dots 1 4 0 \\ G ^ {*}, F ^ {*}, \vec {e} ^ {*}, \dots \quad 1 0 3, 1 5 2 \\ G ^ {2}, H ^ {3}, \dots 2 8 1 \\ \overline {{G}}, \overline {{X}}, \overline {{\mathcal {P}}}, \dots 4, 1 4 0, 2 2 7, 3 2 7 \\ (S, \bar {S}), \dots 1 4 2 \\ x y, x _ {1} \dots x _ {k}, \dots 2, 7 \\ x P, \quad P x, \quad x P y, \quad x P y Q z, \dots 7 \\ \begin{array}{c} \mathring {P}, \mathring {x} Q, \mathring {F}, \dots \end{array} \quad 7, 8 4, 2 2 6 \\ x T y, \dots 1 4 \\ \end{array}
$$

F2 23

N 1, 357

$\mathbb { Z } _ { n }$ 1

CG 39

C(G) 23, 232

C∗(G) 25, 249

E(G) 23

G(n, p) 294

KP , KP(S) 341, 342

PH 308

$\mathcal { P } _ { i , j }$ 302

V(G) 23

Ck, C(S, ω), Cˆ(S, ω) 8, 227

E(G) 2

$E ( X )$ 297

$F ( G )$ 86

Forb 327

G(H1, H2) 259

Kn 3, 197

Kn1,...,nr 17

Krs 17

$L ( G )$ 4

MX 19

N (v), N (U ) 5

$N ^ { + } ( v )$ 124

P 295

$P ^ { k }$ 6

134

R(H) 255

$R ( H _ { 1 } , H _ { 2 } )$ 255

$R ( k , c , r )$ 255

$R ( r )$ 253

$R _ { s }$ 184

$S ^ { n }$ 85, 361

$T _ { 2 }$ 203

T X 20

$T ^ { r - 1 } ( n )$ 165

$V ( G )$ 2

$\operatorname { c h } ( G )$ 121

ch-(G) 121

$\operatorname { c o l } ( G )$ 114

$d ( G )$ 5

$d ( v )$ 5

$d ^ { + } ( v )$ 124

$d ( x , y )$ 8

$d ( X , Y )$ 176

diam G 8

$\operatorname { e x } ( n , H )$ 165

$f ^ { * } ( v )$ 103

$g ( G )$ 8

i 1

$\operatorname { i n i t } ( e )$ 28

log, ln 1

$\operatorname { p w } ( G )$ 352

$q ( G )$ 39

rad G 9

tr−1(n) 165

28

321

ve, vxy, vU 18, 19

v∗(f ) 103

5

126

5

363

11

68

12

68

307

85

147

σ2 307

147

363

112

135

358

Ω(G) 203

ℵ0, ℵ1 357

Reinhard Diestel received a PhD from the University of Cambridge, following research 1983–86 as a scholar of Trinity College under B´ela Bollob´as. He was a Fellow of St. John’s College, Cambridge, from 1986 to 1990. Research appointments and scholarships have taken him to Bielefeld (Germany), Oxford and the US. He became a professor in Chemnitz in 1994 and has held a chair at Hamburg since 1999.

Reinhard Diestel’s main area of research is graph theory, including infinite graph theory. He has published numerous papers and a research monograph, Graph Decompositions (Oxford 1990).

# A study of graph-based system for multi-view clustering✩

Hao Wang a,b, Yan Yang a,∗, Bing Liu b, Hamido Fujita c

a School of Information Science and Technology, Southwest Jiaotong University, Chengdu 611756, China   
b Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, USA   
c Faculty of Software and Information Science, Iwate Prefectural University, Takizawa 020-0693, Japan

![](images/47771cf4c0e9df6c1e65a81e0943233aa440c654db20f61eaff5608b15447049.jpg)

# a r t i c l e i n f o

Article history:

Received 21 June 2018

Received in revised form 11 October 2018

Accepted 13 October 2018

Available online 28 October 2018

Keywords:

Multi-view clustering

Graph-based technology

Data fusion

Laplacian matrix

Rank constraint

# a b s t r a c t

This paper studies clustering of multi-view data, known as multi-view clustering. Among existing multiview clustering methods, one representative category of methods is the graph-based approach. Despite its elegant and simple formulation, the graph-based approach has not been studied in terms of (a) the generalization of the approach or (b) the impact of different graph metrics on the clustering results. This paper extends this important approach by first proposing a general Graph-Based System (GBS) for multi-view clustering, and then discussing and evaluating the impact of different graph metrics on the multi-view clustering performance within the proposed framework. GBS works by extracting data feature matrix of each view, constructing graph matrices of all views, and fusing the constructed graph matrices to generate a unified graph matrix, which gives the final clusters. A novel multi-view clustering method that works in the GBS framework is also proposed, which can (1) construct data graph matrices effectively, (2) weight each graph matrix automatically, and (3) produce clustering results directly. Experimental results on benchmark datasets show that the proposed method outperforms state-of-the-art baselines significantly.

© 2018 Elsevier B.V. All rights reserved.

# 1. Introduction

In many real-world problems data are from different sources or different views. For instance, the same news may be reported by different news organizations, an image may be represented by different types of features, and a video may be encoded in different amounts of images and sounds. All these are referred to as multi-view data, which bred a new learning paradigm, called multi-view learning. This learning paradigm is nature because we humans seem to learn in a similar way. We often look at problems from different views. That is why we can approach a problem in a comprehensive manner. The learning paradigm has been studied extensively in the past [1,2]. This paper makes a focused contribution to multi-view unsupervised learning, particularly, multi-view clustering. Multi-view clustering considers the diversity of different views and fuses these views to produce a more accurate and robust partition than single-view clustering [3,4]. We will discuss related work in the next section.

Recently, graph-based multi-view clustering has produced several state-of-the-art multi-view clustering methods [5–14]. These methods typically work as follows: (1) constructing a data graph

of each view, (2) learning a fusion graph across the constructed graphs of all views, and (3) producing the clustering results on the fusion graph. In these methods, the data graph of each view is usually generated from a data similarity matrix, where each matrix entry is the similarity of two data points. We call this graph matrix the similarity-induced graph (SIG) matrix. Although such methods have achieved state-of-the-art performances, most are not comprehensive. First, there is not a general graph-based system for multi-view clustering. We propose a general approach in this paper. Second, the impact of different graph metrics on multi-view clustering has not been discussed. We will show in the experiment section (i.e., Section 6) that the performances of such clustering methods heavily depend on the constructed graphs. Additionally, the current methods, e.g., [5–7], do not give sufficient consideration to weights of different views in fusion or require an additional clustering step after fusion to produce the final clustering results. Most existing methods also need to tune parameters, e.g., [8,10,12,14]. Our proposals in GBS can tackle these problems. All these prompted us to make a new attempt to study multi-view graph-based clustering.

In this paper, we propose a general system for multi-view clustering, named Graph-Based System (GBS), as shown in Fig. 1. GBS first extracts features for data matrices of m views, constructs a SIG matrix $\pmb { S } ^ { v }$ for each view $( v )$ , and then generates a unified graph matrix U, which gives the clustering results directly in fusion without an additional clustering step. Specially, we propose a new

![](images/35f495dd1a579643cb9794e7fd5348b02a07729d56aceb81617e4dfe671c1ce9.jpg)  
Fig. 1. The proposed GBS for multi-view clustering. The SIG matrices $\pmb { S } = \{ \pmb { S } ^ { 1 } , \pmb { S } ^ { 2 } , \dots , \pmb { S } ^ { m } \}$ are constructed from data matrices of m views $\mathbf { X } = \{ \mathbf { X } ^ { 1 } , \mathbf { X } ^ { 2 } , \ldots , \mathbf { X } ^ { m } \}$ . The fused graph (i.e., the unified matrix U) is generated by using the proposed auto-weighted fusion technique, where the weights $\mathbf { w } = \{ w _ { 1 } , w _ { 2 } , \ldots , w _ { m } \}$ correspond to $\pmb { S } = \{ \pmb { S } ^ { 1 } , \pmb { S } ^ { 2 } , \dots , \pmb { S } ^ { m } \}$ . With a rank constraint on the Laplacian matrix of U, the learned unified matrix U has c connected components, where c is the user-specified number of clusters.

graph construction method based on manifold learning [15,16] and sparse representation [17]. A novel multi-view fusion approach can automatically weight each SIG matrix to generate the unified graph matrix. A rank constraint is imposed on the Laplacian matrix of the unified graph matrix, which helps partition the data points naturally into the required number of clusters. To the best of knowledge, this is the first attempt to propose a general graphbased system for multi-view clustering.

In summary, this paper makes the following contributions:

1. It proposes a general Graph-Based System (GBS) for multiview clustering and discusses the impact of graph metrics on multi-view clustering within the proposed GBS.   
2. It also proposes a novel multi-view clustering method on GBS to tackle the problems confronted in the existing methods. The proposed method can effectively construct SIG matrices, automatically weight each SIG to learn the unified graph and directly produce the final clusters on the unified graph.   
3. Extensive experiments show the robust of the proposed system and the effectiveness of the proposed clustering method. It outperforms state-of-the-art baseline methods markedly.

The rest of this paper is organized as follows. Section 2 describes some notations and gives a brief review on related works. Section 3 introduces the proposed multi-view clustering method for GBS. Section 5 presents our optimization algorithm and provides a convergence proof of our optimization scheme. Experimental results are given in Section 6. Finally, we give concluding remarks and future work in Section 7.

# 2. Related work

In this section, we first describe some notational conventions, and then discuss the related works to our proposed method.

# 2.1. Notational conventions

Throughout the paper, matrices, and vectors and scalars are written in boldface capital letters (e.g., X), boldface lowercase letters (e.g., x) and lowercase letters (e.g., x), respectively. I denotes the identity matrix, and 1 denotes a column vector with all the entries as one. For a matrix $\pmb { X } \in \mathbb { R } ^ { d \times n }$ , the jth column vector and the $i j \cdot$ -th entry are denoted by $\mathbf { x _ { j } }$ and $x _ { i j }$ , respectively. The trace and the Frobenius norm of $\mathbf { x }$ are denoted by $T r ( { \bf { X } } )$ and $\| \mathbf { X } \| _ { F }$ , respectively. For a vector $\mathbf { x } \in \mathbb { R } ^ { d \times 1 }$ , the jth entry is denoted by xj, and $l _ { p }$ -norm is denoted by $\| \mathbf { x } \| _ { p }$ .

# 2.2. Graph-based clustering

Given a data matrix $\pmb { X } \in \mathbb { R } ^ { d \times n }$ , where $d$ is the dimensionality and n is the number of data points, traditional graph-based clustering methods partition the n data points into c clusters as follows:

Step 1. Constructing the data graph matrix $\mathbf { G } \in \mathbb { R } ^ { n \times n }$ , where each entry $g _ { i j }$ in G represents the similarity between $\mathbf { x } _ { i }$ and $\mathbf { x } _ { j }$ ;

Step 2. Computing the graph Laplacian matrix $\mathbf { L } _ { G } = \mathbf { D } _ { G } - ( \mathbf { G } ^ { T } +$ G)/2, where $\mathbf { D } _ { G }$ is a diagonal matrix whose ith diagonal element is $\textstyle \sum _ { j } ( g _ { i j } + g _ { j i } ) / 2$ ;

Step 3. Computing the embedding matrix $\mathbf { E } \in \mathbb { R } ^ { n \times c }$ by solving

$$
\min  _ {\mathbf {E} \in \mathbb {R} ^ {n \times c}} T r (\mathbf {E} ^ {T} \mathbf {L} _ {G} \mathbf {E});
$$

Step 4. Clustering E into c groups with an additional clustering algorithm (e.g., K-means).

If we further constrain $\begin{array} { r } { \mathbf { E } ^ { T } \mathbf { D } _ { G } \mathbf { E } \ = \ \mathbf { I } , } \end{array}$ , it becomes the typical normalized cut [18]. Note that spectral clustering methods usually perform similar steps. Our method performs in a new paradigm, which produces clusters on the graph matrix (i.e., the unified graph matrix U, which will be given in the next section), not the embedding matrix. The theorem in graph theory [19] motivates us to constrain the rank of $\mathbf { L } _ { U }$ (the Laplacian matrix of U) to be equal to $n - c$ . In such a way, the connected components in the unified graph are exactly c, which are the final clusters.

# 2.3. Multi-view clustering

Based on the prior work, existing multi-view clustering methods can be classified into five categories: multi-view graph-based clustering, multi-view spectral clustering, co-training style clustering, multi-kernel clustering, and multi-view subspace clustering. Our work is clearly related to multi-view graph-based clustering [5,11,6–10,12–14]. [5] proposed a 3-stage graph-based multiview clustering method that utilizes a graph representation of subspaces. However, it does not consider the weights of different views, and also requires a hierarchical agglomerative clustering algorithm to produce the final clustering results. To address the first issue, weighted multi-view graph-based clustering was studied in [6,7]. These two methods first construct each view graph matrix, combine the constructed graph matrices to learn a unified graph matrix, and then employ K-means on the unified graph matrix to produce the final clusters. That is, they both require an additional clustering step to produce the final results. More

advanced methods were presented in [8–10,6,12–14]. Although these methods generate the final clusters after fusion without additional clustering steps, none of them is a general framework or has studied the impact of different graph metrics on multiview clustering. In addition, there are a number of parameters in [8,10,12,14], which are hard to set in practice. Proposals in this paper are made to handle these problems. In Section 6, we will compare with these methods experimentally.

Our work is also related to multi-view spectral clustering [20–27]. As discussed earlier, spectral clustering behaves similar to graph-based clustering. Multi-view spectral clustering methods typically find a low-dimensional embedding matrix of the data first and then employ an additional clustering algorithm on this embedding matrix to produce the final clusters. However, additional clustering steps may bring about additional PAC (Probably Approximately Correct) bounds [28]. Our method produces the final clusters on the graph matrix of the data without any additional clustering steps. We will also compare with representative methods of this category.

There are also some other multi-view clustering methods, [29–34,16,35–41] to name a few. More methods are surveyed in [4].

# 3. Clustering method for our graph-based system

This section presents the proposed multi-view clustering method on our system GBS. From Fig. 1, we can see that GBS consists of four components: feature extraction, graph construction, graph fusion and data clustering. The first component is not our main concern as we compare with existing methods on benchmark datasets. In practice, we can use Bag-of-Words, N-grams or Word2Vector for text data [42], HOG, SIFT or LBP for image data [43], and STFT or FFT for audio data [44]. This section introduces the last three components in detail. We also give some insights about our method.

# 3.1. Graph construction

For a multi-view dataset with m views, let $\mathbf { X } ^ { 1 } , \ldots , \mathbf { X } ^ { m }$ be the data matrices of the m views and ${ \bf X } ^ { v } = \{ { \bf x } _ { 1 } ^ { v } , \ldots , { \bf x } _ { n } ^ { v } \} \in \mathbb { R } ^ { d _ { v } \times n }$ be the data matrix of the vth view, where $d _ { v }$ is the dimensionality of the vth view and $n$ is the number of data points. The most commonly used methods for transforming the data matrix $\mathbf { X } ^ { v }$ to a graph are as follows:

1. Complete graph. For each data point $x _ { i } ^ { v }$ , it puts edges between $x _ { i } ^ { v }$ and all the other data points.   
2. k-nearest graph. For each data point $x _ { i } ^ { v }$ , it puts edges between $x _ { i } ^ { v }$ and its $k$ nearest neighbors.

Let $\pmb { S } ^ { v } \in \mathbb { R } ^ { n \times n }$ denote the constructed graph, where each node corresponds a data point. If nodes i and $j$ are connected by an edge, the similarity $\pmb { s } _ { i j } ^ { v }$ on the graph is usually defined as follows:

1. Binary (0–1) similarity: $\pmb { s } _ { i j } ^ { v } = 1$ .   
2. Cosine similarity: svij = i j∥xv ∥∥xv ∥ . $\begin{array} { r } { \pmb { s } _ { i j } ^ { v } = \frac { ( \boldsymbol { x } _ { i } ^ { v } ) ^ { T } \boldsymbol { x } _ { j } ^ { v } } { \lVert \boldsymbol { x } _ { i } ^ { v } \rVert \lVert \boldsymbol { x } _ { j } ^ { v } \rVert } } \end{array}$ ij ( xv )T xv   
3. Gaussian kernel similarity: $\begin{array} { r } { \pmb { \mathscr { s } } _ { i j } ^ { v } = e x p ( \frac { - \| x _ { i } ^ { v } - x _ { j } ^ { v } \| } { 2 \sigma } ) } \end{array}$ −∥ xvi − xvj ∥ 2σ , where $\sigma$ is a scaling parameter.

There are some limitations in these metrics. For example, binary similarity is the simplest method but the result is weak. Cosine similarity cannot consider the local geometric structure of the data. Gaussian kernel similarity is a distance based measure, which is sensitive to noise and outliers in the data. We will compare these similarity metrics on both complete graph and k-nearest graph in Section 6.

We now introduce a novel graph construction method based on manifold learning [15,16] and sparse representation [17]. An intuitive explanation of manifold learning is that if two data points are close, they are also close to each other in the embedding graph. Recent study [17] has shown that sparse representation is robust to noise and outliers. Specially, we compute SIG matrix of each view (e.g., v) by solving the following problem:

$$
\min  _ {\mathbf {s} ^ {v}} \sum_ {i, j = 1} ^ {n} \left\| \mathbf {x} _ {i} ^ {v} - \mathbf {x} _ {j} ^ {v} \right\| _ {2} ^ {2} \mathbf {s} _ {i j} ^ {v} + \alpha \sum_ {i = 1} ^ {n} \| \mathbf {s} _ {i} ^ {v} \| _ {1} \tag {1}
$$

$$
s. t. s _ {i i} ^ {v} = 0, s _ {i j} ^ {v} \geq 0.
$$

When we normalize $\pmb { s } _ { i } ^ { v }$ with $\mathbf { 1 } ^ { T } \mathbf { S } _ { i } ^ { v } \ = \ 1$ , it exactly makes the second term constant. That is, the normalization $\mathbf { 1 } ^ { T } \mathbf { S } _ { i } ^ { v } \ = \ \ 1$ is equivalent to the sparse constraint on $\pmb { s } _ { i } ^ { v }$ . Then, problem (1) is turned into

$$
\min  _ {\mathbf {s} ^ {v}} \sum_ {i, j = 1} ^ {n} \left\| \mathbf {x} _ {i} ^ {v} - \mathbf {x} _ {j} ^ {v} \right\| _ {2} ^ {2} s _ {i j} ^ {v} \tag {2}
$$

$$
s. t. s _ {i i} ^ {v} = 0, s _ {i j} ^ {v} \geq 0, \mathbf {1} ^ {T} \mathbf {s} _ {i} ^ {v} = 1.
$$

Here we denote that problem (2) has a trivial solution, i.e., only one data point with the smallest distance to $\mathbf { x } _ { i } ^ { v }$ has the value 1, while all the other data points have the value 0. Now we add a prior to problem (2), which is formulated as

$$
\min  _ {\mathbf {s} ^ {v}} \sum_ {i, j = 1} ^ {n} \left\| \mathbf {x} _ {i} ^ {v} - \mathbf {x} _ {j} ^ {v} \right\| _ {2} ^ {2} s _ {i j} ^ {v} + \beta \sum_ {i = 1} ^ {n} \| \mathbf {s} _ {i} ^ {v} \| _ {2} ^ {2} \tag {3}
$$

$$
s. t. s _ {i i} ^ {v} = 0, s _ {i j} ^ {v} \geq 0, \mathbf {1} ^ {T} \mathbf {s} _ {i} ^ {v} = 1.
$$

The prior can be seen as the similarity value of each data point to $\mathbf { x } _ { i } ^ { v }$ , which is $\frac { 1 } { n }$ , if we only focus on the second term of Eq. (3) (Note that we use the terms problem (·) and Eq. (·) interchangeably as each problem is modeled as an equation.). Now we can construct a SIG matrix for each view. In the next subsection, we present our proposed graph fusion method.

# 3.2. Graph fusion

This subsection presents the proposed fusion method, which can automatically weight each SIG matrix to find a unified graph matrix. Mathematically, we compute the unified matrix $\mathbf { U } \in \mathbb { R } ^ { n \times n }$ from the SIG matrices $\mathbf { S } ^ { 1 } , \ldots , \mathbf { S } ^ { m }$ by solving the problem below:

$$
\min  _ {\mathbf {U}} \sum_ {v = 1} ^ {m} w _ {v} \| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F} ^ {2} \tag {4}
$$

$$
s. t. u _ {i j} \geq 0, \mathbf {1} ^ {T} \mathbf {u} _ {i} = 1
$$

where $w _ { v }$ is the weight of the vth view. The weight $w _ { v }$ can be determined automatically according to the following theorem.

Theorem 1. The weight $w _ { v }$ is determined as $\frac { 1 } { 2 \sqrt { \| \mathbf { U } - \pmb { S } ^ { v } \| _ { F } ^ { 2 } } }$

Proof. We now define an auxiliary function of $\mathbf { U }$ as follows:

$$
\min  _ {\mathbf {U}} \sum_ {v = 1} ^ {m} \sqrt {\left\| \mathbf {U} - \mathbf {S} ^ {v} \right\| _ {F} ^ {2}} \tag {5}
$$

$$
s. t. u _ {i j} \geq 0, \mathbf {1} ^ {T} \mathbf {u} _ {i} = 1.
$$

Taking the derivative of the Lagrange function of Eq. (5) with respect to U and setting the derivative to zero, we have

$$
\sum_ {v = 1} ^ {m} \hat {w} _ {v} \frac {\partial \| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F} ^ {2}}{\partial \mathbf {U}} + \frac {\partial \Theta (\Lambda , \mathbf {U})}{\partial \mathbf {U}} = 0 \tag {6}
$$

where $\Theta ( A , { \bf U } )$ is the formalized term derived from the constraints $u _ { i j } \geq 0$ and $\mathbf { \mathbb { 1 } } ^ { T } \mathbf { \mathbb { u } } _ { i } = 1$ , Λ is the Lagrange multiplier, and

$$
\hat {w} _ {v} = \frac {1}{2 \sqrt {\left\| \mathbf {U} - \mathbf {S} ^ {v} \right\| _ {F} ^ {2}}}. \tag {7}
$$

If we also perform the same operations on Eq. (4), we can get the same result as shown in Eq. (6). Thus, the solution to weight $w _ { v }$ is s $\frac { 1 } { 2 \sqrt { \| \mathbf { U } - \pmb { S } ^ { v } \| _ { F } ^ { 2 } } }$ . □

Note that Eq. (6) is the derivative of the Lagrange function of Eq. (5) with respect to U. Plug $w _ { v }$ using Eq. (7) into Eq. (6), then Eq. (6) equals to the derivative of Eq. (5) with respect to U. As can be seen, Eqs. (4) and (5) are different and the main character of Theorem 1 is to determine the weights by solving Eq. (5).

# 3.3. Data clustering

As mentioned in the introduction section, our method can directly produce the clustering result on the unified graph matrix U without any additional clustering algorithms. So far, the unified graph matrix U obtained through Eq. (4) above cannot tackle this.

Below, we give an efficient solution to achieve this goal by imposing a rank constraint on the graph Laplacian matrix $\mathbf { L } _ { U }$ of the unified matrix U. According to graph theory [19,45], if graph matrix (our U in this case) is non-negative, then we have Theorem 2.

Theorem 2. The graph Laplacian matrix $\mathbf { L } = \mathbf { L } _ { U }$ of the graph matrix U has the following properties.

1. L is a symmetric positive semi-definite matrix. Thus all eigenvalues of L are real and non-negative, and L has a full set of n real and orthogonal eigenvectors.   
2. $\mathbf { L 1 } = \mathbf { 0 } $ , where $\mathbf { 1 } = [ 1 , \ldots , 1 ] ^ { T }$ . Thus 0 is an eigenvalue of L and 1 is the corresponding eigenvector.   
3. If the graph U has r connected components then L has r eigenvalues that equal 0.

The proof of each part in Theorem 2 is presented in [45]. As a conclusion, the part 3 in Theorem 2 says that if $r a n k ( { \bf L } _ { U } ) =$ $n - c$ as $c \ = \ r$ , the corresponding U can be partitioned into c groups directly. Inspired by Theorem 2, we add a rank constraint $r a n k ( { \bf L } _ { U } ) = n - c$ to problem (4).

We now detail the constraint $r a n k ( { \bf L } _ { U } ) = n - c .$ Let $\vartheta _ { i } ( \mathbf { L } _ { U } )$ be the ith smallest eigenvalue of $\mathbf { L } _ { U }$ . We know that $\vartheta _ { i } ( \mathbf { L } _ { U } ) \geq 0$ because $\mathbf { L } _ { U }$ is positive semi-definite (See part 1 in Theorem 2). Then, the constraint $r a n k ( \mathbf { L } _ { U } ) = n - c$ can be achieved if $\begin{array} { r } { \sum _ { i = 1 } ^ { c } \vartheta _ { i } ( \mathbf { L } _ { U } ) = 0 } \end{array}$ According to Ky Fan’s theorem [46], we have

$$
\sum_ {i = 1} ^ {c} \vartheta_ {i} (\mathbf {L} _ {U}) = \min  _ {\mathbf {F} \in \mathbb {R} ^ {n \times c}, \mathbf {F} ^ {T} \mathbf {F} = \mathbf {I}} T r \left(\mathbf {F} ^ {T} \mathbf {L} _ {U} \mathbf {F}\right) \tag {8}
$$

where $\mathbf { F } = \{ \mathbf { f } _ { 1 } , \dots . . \mathbf { f } _ { c } \}$ is the embedding matrix.

Plugging the item Eq. (8) into problem (4), formally, our objective function is formulated as

$$
\begin{array}{l} \min  _ {\mathbf {U}} \sum_ {v = 1} ^ {m} w _ {v} \| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F} ^ {2} + 2 \lambda T r (\mathbf {F} ^ {T} \mathbf {L} _ {U} \mathbf {F}) \tag {9} \\ s. t. s _ {i i} ^ {v} = 0, s _ {i j} ^ {v} \geq 0, \mathbf {1} ^ {T} \mathbf {s} _ {i} ^ {v} = 1, \\ u _ {i j} \geq 0, \mathbf {1} ^ {T} \mathbf {u} _ {i} = 1, \mathbf {F} ^ {T} \mathbf {F} = \mathbf {I}. \\ \end{array}
$$

When $\lambda$ is large enough, the solution to problem (9) will make $\begin{array} { r } { \sum _ { i = 1 } ^ { k } \vartheta _ { i } ( { \bf L } _ { U } ) = 0 } \end{array}$ hold. In practice, we simply increase or decrease the value of λ when the number of connected components is smaller or greater than c. Thus, the resulting unified graph matrix U contains c connected components exactly, which partitions the data points into c clusters.

# 4. Some insights about problem (9)

Now we give some insights of our method, i.e., problem (9). Theorems 3 and 4 reveal that problem (9) can be seen as a combination of K-means and spectral clustering. The parameter $\lambda$ adjusts the contributions of both methods.

Theorem 3. When $\lambda \ = \ 0$ and $\mathbf { \mathsf { \pmb { S } } } ^ { v } \ = \ ( \mathbf { \pmb { X } } ^ { v } ) ^ { T } \mathbf { \pmb { X } } ^ { v }$ , problem (9) is equivalent to K-means clustering.

Proof. Given the data matrix $\mathbf { X } ^ { v } = [ \mathbf { x } _ { 1 } , \ldots , \mathbf { x } _ { n } ] \in \mathbb { R } ^ { d _ { v } \times n }$ and the user-specified cluster number c, the objective function of K-means is

$$
\min  \sum_ {j = 1} ^ {c} \sum_ {\mathbf {x} _ {i} ^ {v} \in \mathbf {z} _ {j}} \| \mathbf {x} _ {i} ^ {v} - \mathbf {z} _ {j} \| _ {2} ^ {2} \tag {10}
$$

where $\mathbf { z } _ { j }$ is the cluster center of the jth cluster.

Let $\dot { { \bf Z } } = [ { \bf z } _ { 1 } , \dots , { \bf z } _ { k } ] \in \mathbb { R } ^ { d _ { v } \times c }$ and $\mathbf { Y } \in \mathbb { R } ^ { n \times c }$ be the clustering center matrix and the indicator matrix, respectively. For the indicator matrix Y, each element is in [0, 1], each column vector contain at least one non-zero element and the sum of each row vector is 1. Then, Eq. (10) can be rewritten as

$$
\begin{array}{l} \min  _ {\mathbf {Z}, \mathbf {Y}} \left\| \mathbf {X} ^ {v} - \mathbf {Z Y} ^ {T} \right\| _ {F} ^ {2} \\ \Leftrightarrow \min  _ {\mathbf {Z}, \mathbf {Y}} T r (\mathbf {Y} \mathbf {Z} ^ {T} \mathbf {Z} \mathbf {Y} ^ {T}) - 2 T r ((\mathbf {X} ^ {v}) ^ {T} \mathbf {Z} \mathbf {Y} ^ {T}). \tag {11} \\ \end{array}
$$

Since the solution to Z with respect to $\mathbf { X } ^ { v }$ and $\mathbf { Y }$ is ${ \textbf { Z } } =$ $\mathbf { X } ^ { v } \mathbf { Y } ^ { T } ( \mathbf { Y } \mathbf { Y } ^ { T } ) ^ { - 1 }$ , we have the equation $T r ( { \bf Y } { \bf Z } ^ { T } { \bf Z } { \bf Y } ^ { T } ) = T r ( ( { \bf X } ^ { v } ) ^ { T } { \bf Z } { \bf Y } ^ { T } ) ,$ . So, problem (11) becomes

$$
\begin{array}{l} \max  _ {\mathbf {Z}, \mathbf {B}} T r \left(\left(\mathbf {X} ^ {v}\right) ^ {T} \mathbf {Z} \mathbf {Y} ^ {T}\right) \\ \Leftrightarrow \max  _ {\mathbf {Y}} T r \left(\left(\mathbf {Y} ^ {T} \mathbf {Y}\right) ^ {- \frac {1}{2}} \mathbf {Y} ^ {T} \left(\mathbf {X} ^ {v}\right) ^ {T} \mathbf {X} ^ {v} \mathbf {Y} \left(\mathbf {Y} ^ {T} \mathbf {Y}\right) ^ {- \frac {1}{2}}\right) \tag {12} \\ \Leftrightarrow \max  _ {\mathbf {R}} T r \left(\mathbf {R} ^ {T} \left(\mathbf {X} ^ {v}\right) ^ {T} \mathbf {X} ^ {v} \mathbf {R}\right) \\ \end{array}
$$

where $\textbf { R } = \textbf { Y } ( \textbf Y ^ { T } \textbf Y ) ^ { - \frac { 1 } { 2 } }$ . Note that ${ \cal T } r ( { \bf R } { \bf R } ^ { T } { \bf R } { \bf R } ^ { T } ) ~ = ~ { \cal T } r ( { \bf R } { \bf R } ^ { T } ) ~ =$ $T r ( { \bf { Y } } ( { \bf { Y } } ^ { T } { \bf { Y } } ) ^ { - 1 } { \bf { Y } } ) = c$ . Then problem (12) can be transformed to the following problem

$$
\min  _ {\mathbf {R}} \left\| \mathbf {R} \mathbf {R} ^ {T} - \left(\mathbf {X} ^ {v}\right) ^ {T} \mathbf {X} ^ {v} \right\| _ {F} ^ {2}. \tag {13}
$$

Define ${ \bf U } = { \bf R } { \bf R } ^ { T }$ and $\pmb { S } ^ { v } = ( \mathbf { X } ^ { v } ) ^ { T } \mathbf { X } ^ { v }$ , Eq. (13) is rewritten as

$$
\min  _ {\mathbf {U}} \| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F} ^ {2} \tag {14}
$$

where U is a symmetry matrix. So, K-means clustering is to seek a matrix U by minimizing problem (14), which is the first term in problem (9). □

Theorem 4. When $\lambda \to \infty$ , problem (9) is equivalent to spectral clustering.

Proof. When $\lambda \to \infty$ , problem (9) can be seen as the following problem

$$
\min  _ {\mathbf {F} \in \mathbb {R} ^ {n \times c}, \mathbf {F} ^ {T} \mathbf {F} = \mathbf {I}} T r \left(\mathbf {F} ^ {T} \mathbf {L} _ {U} \mathbf {F}\right). \tag {15}
$$

As introduced in Section 2, spectral clustering also aims to solve the problem shown above. From this viewpoint, spectral clustering and our method are equivalent. □

Hereto, we presented our method in detail. In the next section, we propose our optimization algorithms to solve problem (3) and problem (9).

# 5. Optimization algorithms

# 5.1. Optimization algorithm for problem (3)

As can be seen, problem (3) is independent for each data point i, so we can solve the following problem separately for each data point i:

$$
\min  _ {\mathbf {s} _ {i} ^ {v}} \sum_ {j = 1} ^ {n} \left\| \mathbf {x} _ {i} ^ {v} - \mathbf {x} _ {j} ^ {v} \right\| _ {2} ^ {2} s _ {i j} ^ {v} + \beta \| \mathbf {s} _ {i} ^ {v} \| _ {2} ^ {2} \tag {16}
$$

$$
s. t. s _ {i i} ^ {v} = 0, s _ {i j} ^ {v} \geq 0, \mathbf {1} ^ {T} \mathbf {s} _ {i} ^ {v} = 1.
$$

We denote $d _ { i j } = \lVert \mathbf { x } _ { i } ^ { v } - \mathbf { x } _ { i } ^ { v } \rVert _ { 2 } ^ { 2 }$ and further denote ${ \bf d } _ { i }$ as a vector with the jth element as $d _ { i j }$ . Then we can formulate problem (17) in a vector form as follows:

$$
\min  _ {\mathbf {s} _ {i} ^ {v}} \left\| \mathbf {s} _ {i} ^ {v} + \frac {\mathbf {d} _ {i}}{2 \beta} \right\| _ {2} ^ {2} \tag {17}
$$

$$
s. t. s _ {i j} \geq 0, \mathbf {1} ^ {T} \mathbf {s} _ {i} ^ {v} = 1.
$$

In practice, we prefer a data point having similarities with its neighbors. That is, we aim to learn a $\pmb { s } _ { i } ^ { v }$ with k nonzero values, where $k$ is the number of neighbors. This problem can be solved with a closed form solution as in [47]. Here we give a summary of this algorithm, which is shown in Algorithm 1.

Algorithm 1: Algorithm for computing $\pmb { s } _ { i } ^ { v }$ in problem (17)   
Input: Data matrix $\mathbf{X}^v$ , and the number of neighbors $k$ . Output: Vector $\mathbf{s}_i^v$ in the SIG matrix $\mathbf{S}^v$ .  
1 begin  
2 Take the Lagrangian function of problem (17): $\ell (\mathbf{s}_i^v,\eta ,\xi) = \left\| \mathbf{s}_i^v +\frac{\mathbf{d}_i}{2\beta}\right\| _2^2 -\eta (\mathbf{1}^T\mathbf{s}_i^v -1) - \xi^T\mathbf{s}_i^v;$ 3 According to the Karush-Kuhn-Tucker conditions, we have the optimal solution $\hat{s}_{ij}^{v} = (-\frac{d_{ij}}{2\alpha} +\eta)_{+}$ , where $(a)_{+} = max(a,0)$ ;  
4 Order $d_{i1},\ldots,d_{in}$ from small to large;  
5 Due to the constraint $\mathbf{1}^T\mathbf{s}_i^v = 1$ , we have $\eta = \frac{1}{k} +\frac{1}{2k\beta}\sum_{j = 1}^{k}h_{ij};$ 6 As there are only $k$ nonzero values in $\mathbf{s}_i^v$ , we have the maximal $\beta$ , denoted as $\beta = \frac{k}{2} d_{i,k + 1} - \frac{1}{2}\sum j = 1^k d_{ij}$ ;  
7 Get the resulting $\mathbf{s}_i^v$ with $j$ th entry as $s_{ij}^{v} = \left\{ \begin{array}{cc}\frac{d_{i,k + 1} - d_{ij}}{kd_{i,k + 1} - \sum_{h = 1}^{k}b_{ih}} & j\leq k,\\ 0 & j > k; \end{array} \right.$ 8 end  
Result: The resulting $\mathbf{s}_i^v$ with $k$ nonzero values.

# 5.2. Optimization algorithm for problem (9)

Suppose we have obtained $\mathbf { S } ^ { 1 } , \ldots , \mathbf { S } ^ { m }$ by using Algorithm 1. Now we compute the unified matrix U by solving problem (9). Solving problem (9) to give every variable an optimized solution at once is still challenging because all the variables are coupled. Assume w and F are fixed, we can compute U via the augmented Lagrange multiplier scheme, which has been shown effective in many matrix learning problems [48]. Similarly, w and F can be updated when the other variables are fixed. These inspire us to

develop an alternating iterative algorithm to solve problem (9). The detailed updated rules are shown below:

Keep U and F fixed, update w: When U and F are fixed, the second term of problem (9) is a constant. Then we have problem (4). As discussed early, the value of $w _ { v }$ is updated by Eq. (7).

Keep w and F fixed, update U: When w and F are fixed, the optimization problem (9) is turned into

$$
\min  _ {\mathbf {U}} \sum_ {v = 1} ^ {m} \sum_ {i, j = 1} ^ {n} w _ {v} \left(u _ {i j} - s _ {i j} ^ {v}\right) ^ {2} + 2 \lambda \operatorname {T r} \left(\mathbf {F} ^ {T} \mathbf {L} _ {U} \mathbf {F}\right) \tag {18}
$$

$$
s. t. u _ {i j} \geq 0, \mathbf {1} ^ {T} \mathbf {u} _ {i} = 1.
$$

Since $\begin{array} { r } { T r ( \mathbf { F } ^ { T } \mathbf { L } _ { U } \mathbf { F } ) = \frac { 1 } { 2 } \sum _ { i , j } \| \mathbf { f } _ { i } - \mathbf { f } _ { j } \| _ { 2 } ^ { 2 } u _ { i j } } \end{array}$ , problem (18) is rewritten as

$$
\min  _ {\mathbf {u}} \sum_ {v = 1} ^ {m} \sum_ {i, j = 1} ^ {n} w _ {v} \left(u _ {i j} - s _ {i j} ^ {v}\right) ^ {2} + \lambda \sum_ {i, j = 1} ^ {n} \left\| \mathbf {f} _ {i} - \mathbf {f} _ {j} \right\| _ {2} ^ {2} u _ {i j} \tag {19}
$$

$$
s. t. u _ {i j} \geq 0, \mathbf {1} ^ {T} \mathbf {u} _ {i} = 1.
$$

Similar to $d _ { i j }$ and ${ \bf d } _ { i }$ , we denote $h _ { i j } = \| \mathbf { f } _ { i } - \mathbf { f } _ { j } \| _ { 2 } ^ { 2 }$ and further denote $\mathbf { h } _ { i }$ as a vector with the jth element as $h _ { i j }$ . Then we can formulate problem (19) in a vector form as follows

$$
\min  _ {\mathbf {u} _ {i}} \sum_ {v = 1} ^ {m} \left\| \mathbf {u} _ {i} - \mathbf {s} _ {i} ^ {v} + \frac {\lambda}{2 m w _ {v}} \mathbf {h} _ {i} \right\| _ {2} ^ {2} \tag {20}
$$

$$
s. t. u _ {i j} \geq 0, \mathbf {1} ^ {T} \mathbf {u} _ {i} = 1.
$$

For simplicity, the solution to problem (20) is presented in the next subsection, where we develop a simple and yet effective algorithm.

Keep w and U fixed, update F: With w and U fixed, optimizing F is to solve the following problem:

$$
\min  _ {\mathbf {F}} \operatorname {T r} \left(\mathbf {F} ^ {T} \mathbf {L} _ {U} \mathbf {F}\right) \tag {21}
$$

$$
s. t. \mathbf {F} ^ {T} \mathbf {F} = \mathbf {I}.
$$

The study in [19] indicates that the optimal solution to F is formed by the c eigenvectors of $\mathbf { L } _ { U }$ corresponding to the c smallest eigenvalues.

Hereto, all the variables have been updated. The procedure of the proposed algorithm to solve problem (9) is summarized in Algorithm 2. We denote that Algorithm 2 can be fed with different types of SIG matrices. If Algorithm 2 is fed with the SIG matrices generated from Algorithm 1, it forms our proposed multi-view clustering method. We briefly name it GBS-KO (GBS is fed with knearest graph defined by our SIG matrices.). Compared to single view graph-based clustering algorithm, the proposed algorithm needs to optimize Problem (18). The computational complexity of optimizing Problem (18) is $O ( t m n ^ { 2 } )$ in total. In practice, $t \ll n$ and $m \ll n$ . Thus, the proposed algorithm does not increase the computational complexity of graph-based clustering, i.e., $O ( n ^ { 3 } )$ . For large-scale data, the study on large-scale spectral clustering such as [49,50] can be used to speed up our algorithm as our algorithm exploits the key characters of spectral clustering.

# 5.3. Solution to problem (20)

We now give the solution to problem (20). When w and F are fixed, the second and third terms in problem (20) are constants. Denoting $\begin{array} { r } { \mathbf q ^ { v } = \pmb S _ { i } ^ { v } - \frac { \lambda } { 2 m w _ { v } } \mathbf e _ { i } } \end{array}$ 2mwv , then problem (20) is simplified to

$$
\min  _ {\mathbf {u} _ {i}} \sum_ {v = 1} ^ {m} \| \mathbf {u} _ {i} - \mathbf {q} ^ {v} \| _ {2} ^ {2} \tag {22}
$$

$$
s. t. u _ {i j} \geq 0, \mathbf {1} ^ {T} \mathbf {u} _ {i} = 1.
$$

Algorithm 2: Algorithm for solving problem (9)   
Input: SIG matrices with $m$ views $\mathbf{S}^1$ ..., $\mathbf{S}^m$ , the number of clusters $c$ , and initial parameter $\lambda$ Output: The learned unified matrix U.   
begin   
Initialize the weight for each view with $w_{v} = 1 / m$ .   
Initialize U by connecting $\mathbf{S}^1$ ,..., $\mathbf{S}^m$ with w, and then F is obtained by solving Eq. (21); while Property 2 or the maximum iteration reached do   
Keep F and U fixed, update $w_{v}$ by using Eq. (7); Keep w and F fixed, update U by solving problem (20); Keep w and U fixed, update F, which is formed by the c eigenvectors of $\mathbf{L}_U$ corresponding to the $c$ smallest eigenvalues;   
end   
end Result: The learned unified matrix U with exact $c$ connected components, which are the final clusters.

Let $\varphi$ and $\phi$ be the Lagrange multipliers for the constraints $u _ { i j } ~ \geq ~ 0$ and $\mathbf { \Omega } \mathbf { \mathsf { 1 } } ^ { T } \mathbf { u } _ { i } \ = \ 1$ respectively. The Lagrangian function of problem (22) is

$$
\ell \left(\mathbf {u} _ {i}, \phi , \varphi\right) = \frac {1}{2} \sum_ {v = 1} ^ {m} \left\| \mathbf {u} _ {i} - \mathbf {q} _ {v} \right\| _ {2} ^ {2} - \varphi^ {T} \mathbf {u} _ {i} - \phi \left(\mathbf {1} ^ {T} \mathbf {u} _ {i} - 1\right) \tag {23}
$$

where $\varphi$ is a Lagrangian coefficient vector and $\phi$ is a scalar.

Suppose the optimal solution to problem (22) is $\mathbf { u } _ { i } ^ { * }$ , and the corresponding Lagrange multipliers are $\phi ^ { * }$ and $\varphi ^ { * }$ respectively. According to the Karush–Kuhn–Tucker conditions, we have

$\begin{array}{l}\forall j,\sum_{v = 1}^{m}u_{ij}^{*} - \sum_{v = 1}^{m}q_{j}^{v} - \varphi_{j}^{*} - \phi^{*} = 0\\ \forall j,u_{ij}^{*}\geq 0\\ \forall j,\varphi_{j}^{*}\geq 0\\ \forall j,u_{ij}^{*}\varphi_{j}^{*} = 0 \end{array}$ (a) (b) (c) (d) (24)

(24)(a) in a vector form, ${ \textstyle \sum _ { v = 1 } ^ { m } \mathbf { u } _ { i } ^ { * } - \sum _ { v = 1 } ^ { m } \mathbf { q } ^ { v } - }$ $\varphi ^ { * } - \phi ^ { * } \mathbf { 1 } = \mathbf { 0 }$ $\mathbf { 1 } ^ { T } \mathbf { u } _ { i } ^ { * } = \mathbf { \ } 1$ $\phi ^ { * } =$ m−∑mv=1 1T qv −1T ϕ ∗ $\frac { m - \sum _ { v = 1 } ^ { m } \mathbf { 1 } ^ { T } \mathbf { q } ^ { v } - \mathbf { 1 } ^ { T } \varphi ^ { * } } { n }$

nThus, the optimal solution $\mathbf { u } _ { i } ^ { * }$ is formulated as

$$
\mathbf {u} _ {i} ^ {*} = \frac {\sum_ {v = 1} ^ {m} \mathbf {q} ^ {v}}{m} + \frac {\mathbf {1}}{n} - \frac {\sum_ {v = 1} ^ {m} \mathbf {1} ^ {T} q ^ {v} \mathbf {1}}{m n} - \frac {\mathbf {1} ^ {T} \varphi^ {*} \mathbf {1}}{m n} + \frac {\varphi^ {*}}{m}. \tag {25}
$$

We further denote $\begin{array} { r } { \mathbf p = \frac { \sum _ { v = 1 } ^ { m } q ^ { v } } { m } + \frac { 1 } { n } - \frac { \sum _ { v = 1 } ^ { m } \mathbf 1 ^ { T } q ^ { v } \mathbf 1 } { m n } } \end{array}$ m mn and $\begin{array} { r } { \hat { \varphi } ^ { * } = \frac { \pmb { 1 } ^ { T } \varphi ^ { * } } { m n } } \end{array}$ = mn . Then Eq. (25) is simplified to $\begin{array} { r } { \mathbf { u } _ { i } ^ { * } = \mathbf { p } - \hat { \varphi } ^ { * } \mathbf { 1 } + \frac { \varphi ^ { * } } { m } } \end{array}$ ϕ ∗ . As a result, for ∀j, we have

$$
u _ {i j} ^ {*} = p _ {j} - \hat {\varphi} ^ {*} + \frac {\varphi_ {j} ^ {*}}{m}. \tag {26}
$$

According to Eqs. (24)(b)–(d) and (26), we know that $p _ { j } - \hat { \varphi } ^ { \ast } +$ $\frac { \varphi _ { j } ^ { * } } { m } = ( p _ { j } - \hat { \varphi } ^ { * } ) _ { + }$ . That is, the optimal solution $u _ { i j } ^ { * }$ can be obtained if $\hat { \varphi } ^ { * }$ is known, which is formulated as

$$
u _ {i j} ^ {*} = \left(p _ {j} - \hat {\varphi} ^ {*}\right) _ {+}. \tag {27}
$$

In such a way, we get an adaptive neighbor graph because $( p _ { j } -$ $\hat { \varphi } ) _ { + } = m a x ( p _ { j } - \hat { \varphi } , 0 )$ . Furthermore, we can derive $\varphi _ { j } ^ { * } = m ( u _ { i j } ^ { * } +$ $\hat { \varphi } ^ { * } - p _ { j } )$ from Eq. (26). Similarly, we then have $\varphi _ { j } ^ { * } = m ( \hat { \varphi } ^ { * } - p _ { j } ) _ { + }$ according to Eqs. (24)(b)–(d). As denoted above $\begin{array} { r } { \hat { \varphi } ^ { * } = \frac { \pmb { 1 } ^ { T } \varphi ^ { * } } { m n } } \end{array}$ , the

optimal solution $\hat { \varphi } ^ { * }$ is represented as $\begin{array} { r } { \hat { \varphi } ^ { * } = \frac { 1 } { n } \sum _ { j = 1 } ^ { n } ( \hat { \varphi } ^ { * } - p _ { j } ) _ { + } } \end{array}$ . Now we define a function of $\hat { \varphi }$ as

$$
\mathcal {F} (\hat {\varphi}) = \frac {1}{n} \sum_ {j = 1} ^ {n} (\hat {\varphi} - p _ {j}) _ {+} - \hat {\varphi}. \tag {28}
$$

As can be seen, $\hat { \varphi } ^ { * }$ is determined by solving the root finding problem when $\mathcal { F } ( \hat { \varphi } ^ { * } ) = 0$ . Since $\hat { \varphi } \ge 0$ , $\mathcal { F } ^ { \prime } ( \hat { \varphi } _ { t } ) \leq 0$ and $\mathcal { F } ^ { \prime } ( \hat { \varphi } _ { t } ) \leq 0$ is a piece-wise linear and convex function, the root of $\mathcal { F } ( \hat { \boldsymbol { \varphi } } ) = 0$ can be computed via the Newton method efficiently, shown below

$$
\hat {\varphi} _ {t + 1} = \hat {\varphi} _ {t} - \frac {\mathcal {F} (\hat {\varphi} _ {t})}{\mathcal {F} ^ {\prime} (\hat {\varphi} _ {t})}. \tag {29}
$$

# 5.4. Convergence proof

As mentioned above, we solve problem (17) with a closed form solution (i.e., Algorithm 1) and solve problem (9) with an alternating iterative algorithm (i.e., Algorithm 2). We now analyze the convergence of these two problems. First, it is easy to check that Eq./problem (17) is a convex function because the twoorder derivative of this function with respect to $\pmb { s } _ { i } ^ { v }$ is a positive value. Thus, Eq. (17) decreases monotonically with the computing scheme of $\pmb { s } _ { i } ^ { v }$ . Second, proof of the convergence of Algorithm 2 is equivalent to proving each sub-problem is convex. If we can find the optimal solution of each sub-problem, Algorithm 2 converges obviously. The convergence proof of each sub-problem is as follows.

Update w. We have problem (4), which is a linear convex function. We also give a closed-form solution, i.e., Eq. (7), to each weight $w _ { v }$ .

Update U. We have problem (18). We now prove the convergence of problem (18) based on Lemma 1 from [51].

Lemma 1. For any non-zero matrices $\ b { \mathbf { A } } \in \mathbb { R } ^ { n \times n }$ and $\mathbf { B } \in \mathbb { R } ^ { n \times n }$ , the following inequality holds:

$$
\left\| \mathbf {A} \right\| _ {F} - \frac {\left\| \mathbf {A} \right\| _ {F} ^ {2}}{2 \| \mathbf {B} \| _ {F}} \leq \left\| \mathbf {B} \right\| _ {F} - \frac {\left\| \mathbf {B} \right\| _ {F} ^ {2}}{2 \| \mathbf {B} \| _ {F}}. \tag {30}
$$

Proof. Let $\begin{array} { r } { \varDelta ( \mathbf { U } ) = 2 \lambda T r ( \mathbf { F } ^ { T } \mathbf { L } _ { U } \mathbf { F } ) } \end{array}$ and $\tilde { \mathbf { U } }$ be the resulting solution in each iteration, we can derive

$$
\sum_ {v = 1} ^ {m} \frac {\left\| \tilde {\mathbf {U}} - \mathbf {S} ^ {v} \right\| _ {F} ^ {2}}{2 \left\| \mathbf {U} - \mathbf {S} ^ {v} \right\| _ {F}} + \Delta (\tilde {\mathbf {U}}) \leq \sum_ {v = 1} ^ {m} \frac {\left\| \mathbf {U} - \mathbf {S} ^ {v} \right\| _ {F} ^ {2}}{2 \left\| \mathbf {U} - \mathbf {S} ^ {v} \right\| _ {F}} + \Delta (\mathbf {U}). \tag {31}
$$

According to Lemma 1, we have

$$
\begin{array}{l} \sum_ {v = 1} ^ {m} \| \tilde {\mathbf {U}} - \mathbf {S} ^ {v} \| _ {F} - \sum_ {v = 1} ^ {m} \frac {\| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F} ^ {2}}{2 \| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F}} \tag {32} \\ \leq \sum_ {v = 1} ^ {m} \| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F} - \sum_ {v = 1} ^ {m} \frac {\| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F} ^ {2}}{2 \| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F}}. \\ \end{array}
$$

We now sum Eqs. (31) and (32) over both sides, which gives us

$$
\sum_ {v = 1} ^ {m} \| \tilde {\mathbf {U}} - \mathbf {S} ^ {v} \| _ {F} + \Delta (\tilde {\mathbf {U}}) \leq \sum_ {v = 1} ^ {m} \| \mathbf {U} - \mathbf {S} ^ {v} \| _ {F} + \Delta (\tilde {\mathbf {U}}). \tag {33}
$$

The inequality (33) says that the objective function of problem (18) decreases monotonically in each iteration until it converges. □

Update F. We have problem (21). The Hessian matrix of Eq. (21) (i.e., problem (21)) is

$$
\frac {\partial^ {2} \operatorname {T r} \left(\mathbf {F} ^ {T} \mathbf {L} _ {U} \mathbf {F}\right)}{\partial \mathbf {F} \partial \mathbf {F} ^ {T}} = \mathbf {L} _ {U} + \mathbf {L} _ {U} ^ {T}. \tag {34}
$$

Since the Laplacian matrix $\mathbf { L } _ { U }$ is positive semi-definite, the Hessian matrix of Eq. (21) is also positive semi-definite. Thus, Eq. (21) is a convex function with respect to F, which is updated by arg $\sin i n _ { \mathbf { F } ^ { T } \mathbf { F } = \mathbf { I } } T r ( \mathbf { F } ^ { T } \mathbf { L } _ { U } \mathbf { F } )$ in each iteration.

# 6. Experiments

In this section, we evaluate our system GBS for multi-view clustering on both toy data and real-world data. We performed experiments on Windows Server 2008 R2 having Intel Xeon processor, 24 GB RAM and MATLAB development environment. We used MATLAB development environment because all the baselines used it. Before going further, we introduce the settings of our method. As mentioned earlier, if GBS is fed with the SIG matrices generated from Algorithm 1, we have the proposed multi-view clustering method, denoted by GBS-KO. Following [13], we set the number of neighbors $k = 1 5$ . The parameter λ is set to 1 as an initial value, which is tuned automatically. In each iteration, we increase it $( \lambda = \lambda ^ { * } 2$ ) or decrease it $( \lambda = \lambda / 2 )$ when the connected components of the learned graph is smaller or greater than the number of clusters c, respectively. After this, the experiments are divided into two parts.

# 6.1. Experiments on toy data

We first give a visual illustration of the capability of the proposed GBS-KO on a toy dataset.

Specially, we generate a two-view dataset, named Two-Moon dataset, as shown in Figs. 2a and 2d. Each view is generated with a moon pattern with 0.12 percentage of random Gaussian noise adding. There are two clusters, i.e., the upper moon (red) and the lower moon (blue). Figs. 2b, 2c, 2e and 2f show the experimental results.

Among these figures, Figs. 2b and 2e show the constructed graphs with the SIG matrices. From the figures, we can see that the two clusters are connected and not easy to separate on both views. Figs. 2c and 2f show the constructed graphs with the learned unified graph matrix. As can be seen, the two clusters are partitioned very well because GBS-KO can exploit the complementary information from the two views. That is, the hard-to-separate data points in one view can be easily separated with the help of another view.

# 6.2. Experiments on real-world data

Now we study the impact of different graph metrics on multiview clustering and demonstrate the improvements of the proposed GBS-KO method for multi-view clustering. Experiments are conducted on eight real-world benchmark datasets. The clustering results are evaluated by comparing the obtained label of each instance with the provided label by the dataset. Two metrics, the accuracy (ACC) and the normalized mutual information (NMI), are used to measure the clustering performance [52,53]. In order to randomize the experiments, we run each algorithm 10 times and report the means and standard deviations of the performance measures.

Datasets. The statistics of these datasets are summarized in Table 1, where # instance, # view, and # cluster denote the number of instances, views, and clusters, respectively. # ${ \sf d } _ { v }$ denotes the dimensionality of the features in view v. Here we give a brief description of each dataset:

• 3 source dataset1 (3source) consists of 169 news reported by three online news organizations, i.e., BBC, Reuters, and The Guardian. Each news was manually annotated with one of six topical labels.

Table 1 Summary of the benchmark datasets.   

<table><tr><td>Dataset</td><td># instance</td><td># view</td><td># cluster</td><td># d1</td><td># d2</td><td># d3</td><td># d4</td><td># d5</td><td># d6</td></tr><tr><td>3sources</td><td>169</td><td>3</td><td>6</td><td>3560</td><td>3631</td><td>3068</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BBC</td><td>685</td><td>4</td><td>5</td><td>4659</td><td>4633</td><td>4665</td><td>4684</td><td>-</td><td>-</td></tr><tr><td>BBCSport</td><td>544</td><td>2</td><td>5</td><td>3183</td><td>3203</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NGs</td><td>500</td><td>3</td><td>5</td><td>2000</td><td>2000</td><td>2000</td><td>-</td><td>-</td><td>-</td></tr><tr><td>WebKB</td><td>203</td><td>3</td><td>4</td><td>1703</td><td>230</td><td>230</td><td>-</td><td>-</td><td>-</td></tr><tr><td>100leaves</td><td>1600</td><td>3</td><td>100</td><td>64</td><td>64</td><td>64</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HW</td><td>2000</td><td>6</td><td>10</td><td>216</td><td>76</td><td>64</td><td>6</td><td>240</td><td>47</td></tr><tr><td>HW2sources</td><td>2000</td><td>2</td><td>10</td><td>784</td><td>256</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></table>

• BBC dataset2 (BBC) [54] consists of 685 documents collected from the BBC news website. Each document was split into four segments and was manually annotated with one of five topical labels.   
• BBCSport dataset see footnote 2 (BBCSport) [54] consists of 544 documents collected from the BBC Sport website. Each document was split into two segments and was manually annotated with one of the five topical labels.   
• Newsgroups data set3 (NGs) [55] is a subset of the 20 Newsgroup datasets. It consists of 500 newsgroup documents. Each raw document was pre-processed with three different feature extraction methods (giving three views), and was annotated with one of five topical labels.   
• WebKB data set4 (WebKB) [56] consists of 203 web-pages of 4 classes. Each web-page is described by the content of the page, the anchor text of the hyper-link, and the text in its title.   
• One-hundred plant species leaves data set5 (100leaves) [57] is from the UCI repository. The dataset consists of 1600 samples with three views. Each sample is one of the one hundred plant species.   
• Handwritten digit data set6 (HW) [58] is from the UCI repository. The dataset consists of 2000 samples with six views. Each sample is one of the handwritten digits (0–9).   
• Handwritten digit 2 source dataset7 (HW2sources) consists of 2000 samples collected from two sources, i.e., MNIST Handwritten Digits (0–9) and USPS Handwritten Digits (0–9).

# 6.2.1. Comparisons of different graph metrics

To have a comparison of the impact of different graph metrics on multi-view clustering, GBS is fed with other five types of graphs below (Note that binary similarity cannot be used for complete graph because it gives 1 to all edges.):

(a) complete graph with Gaussian kernel similarity, denoted by GBS-CG;   
(b) complete graph with cosine similarity, denoted by GBS-CC;   
(c) k-nearest graph with binary similarity, denoted by GBS-KB;   
(d) k-nearest graph with Gaussian kernel similarity, denoted by GBS-KG;   
(e) k-nearest graph with cosine similarity, denoted by GBS-KC.

Fig. 3a shows the ACC scores of GBS with each graph metric on eight benchmark datasets. Fig. 3b shows the NMI scores of GBS with each graph metric on the eight benchmark datasets. From the figures, we can see that the clustering performances strongly depend on the graph metrics. Complete graph (dash line) is inferior to

2 http://mlg.ucd.ie/datasets/segment.html.   
3 http://lig-membres.imag.fr/grimal/data.html.   
4 https://linqs.soe.ucsc.edu/data.   
5 https://archive.ics.uci.edu/ml/datasets/One-hundred+plant+species+leaves+ data+set.   
6 http://archive.ics.uci.edu/ml/datasets/Multiple+Features.   
7 https://cs.nyu.edu/roweis/data.html.

![](images/a2f607438f3bb238e592ed63e45da3485f6148aad87f247d378a626c8d85dae0.jpg)  
(a)

![](images/c78c0fbe679d21fd813b303965010307647fbc0ea594d367f8ffe1fac23414d5.jpg)  
(b)

![](images/dd7a9b08174713e83bcbc9c0803b62c2d396bac1dac101e755b5ddc80b78f076.jpg)  
(c）

![](images/222481ac2508a1a6f3028f3f52aa9a452e65f0c0a2d3f10c888a530ffa4e35b1.jpg)  
(d)

![](images/418094d9e56fa3707c5d6bced30c27c14cc7aec2b0d25adde59dea3133c534b6.jpg)  
(e)

![](images/91beac1758c56516152e152859dff6150d013633fcddf7ba5de040f69c549250.jpg)  
(f)

![](images/7a3e44b7e94e85c86aef4d4ceb8922e210545f622d72189e74287c58734f81ac.jpg)  
Fig. 2. A small example of GBS-KO on a toy two-view dataset. The upper row is the first view. The lower row is the second view. Notes: 2a and 2d are the generated sample data points of the first view and the second view, respectively; 2b and 2e are the constructed graphs with the SIG matrices for the first view and the second view, respectively; and 2c and 2f are the constructed graphs with the unified graph matrix for the first view and the second view, respectively . (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

![](images/b617a112e236456e280aeef32d3332e70725d8bcedef877fae1e7b786e1ce4e7.jpg)  
Fig. 3. Performance comparison of GBS with different graph metrics on eight real-world datasets.

k-nearest graph (solid line). For the methods with complete graph, the clustering results of both GBS-CG and GBS-CC are dependent on the datasets. For the methods with k-nearest graph, GBS-KC is slight better than GBS-KB and GBS-KG, but worse than our method GBS-KO. This suggests the importance of the geometrical structure and the sparse representation in learning the SIG matrices. As a conclusion, k-nearest graph with our method is a better choice for multi-view clustering.

# 6.2.2. Improvements of GBS-KO for multi-view clustering

To demonstrate how the clustering performance can be improved by our method, we compared GBS-KO with the following baseline methods, where the first two are single-view clustering methods and the others are multi-view clustering methods.

• K-means (Kmeans): This baseline conducts the K-means algorithm on each data view and reports the results of the view that gives the best performance. We use the code available in the tool-box of MATLAB. The maximum number of iterations is set to 200.

• Normalized cut (Ncut) [18]: Normalized cut is a typical graphbased method. We employ it for each view and report the results of the view that gives the best performance. The code is obtained from the author J. Shi.8 The graph of each view is constructed by following the authors’ suggestion to compute the similarity matrix as the squared ${ \bf D } / \sigma$ , where D is the pairwise Euclidean distance matrix and $\boldsymbol { \sigma } = 0 . 0 5 * m a x ( \mathbf { D } )$ .   
• Co-regularized Spectral Clustering (CoregSC) [21]: This is a typical multi-view clustering method based on spectral clustering and kernel learning in a co-training style. We obtained the code from the author A. Kumar9 and used the default parameter settings.   
• Multi-view Spectral Clustering (MSC) [22]: This is a robust multi-view spectral clustering. We obtained the code from

Table 2 Clustering performance comparison of each method in terms of ACC on eight real-world datasets.   

<table><tr><td>ACC (%)</td><td>3sources</td><td>BBC</td><td>BBCSport</td><td>NGs</td><td>WebKB</td><td>100leaves</td><td>HW</td><td>HW2sources</td><td>Ave.</td></tr><tr><td>Kmeans</td><td>44.02 (4.27)</td><td>38.20 (6.21)</td><td>39.39 (3.81)</td><td>22.72 (1.52)</td><td>72.32 (5.12)</td><td>57.96 (1.41)</td><td>70.20 (8.31)</td><td>54.38 (3.95)</td><td>49.90 (4.33)</td></tr><tr><td>Ncut</td><td>40.24 (→0)</td><td>33.14 (→0)</td><td>35.85 (0.00)</td><td>23.60 (→0)</td><td>66.50 (0.00)</td><td>43.04 (1.29)</td><td>69.30 (→0)</td><td>38.80 (0.20)</td><td>43.26 (0.20)</td></tr><tr><td>CoregSC</td><td>54.79 (2.99)</td><td>47.01 (0.00)</td><td>43.44 (2.11)</td><td>27.68 (1.53)</td><td>59.70 (1.43)</td><td>77.06 (2.58)</td><td>75.56 (5.96)</td><td>81.50 (3.86)</td><td>58.34 (2.56)</td></tr><tr><td>MSC</td><td>47.51 (2.97)</td><td>62.32 (4.94)</td><td>35.39 (1.13)</td><td>31.12 (0.67)</td><td>47.34 (3.92)</td><td>73.79 (2.21)</td><td>79.18 (8.21)</td><td>68.78 (4.55)</td><td>55.68 (3.58)</td></tr><tr><td>MGL</td><td>67.51 (6.67)</td><td>53.96 (11.05)</td><td>53.90 (12.37)</td><td>82.18 (14.70)</td><td>73.84 (3.93)</td><td>69.04 (2.42)</td><td>74.40 (8.19)</td><td>68.93 (11.46)</td><td>67.97 (8.85)</td></tr><tr><td>MCGL</td><td>30.77 (→0)</td><td>35.33 (→0)</td><td>39.15 (0.00)</td><td>24.60 (0.00)</td><td>54.19 (0.00)</td><td>81.06 (→0)</td><td>85.30 (0.00)</td><td>97.95 (0.00)</td><td>56.04 (→0)</td></tr><tr><td>ASMV</td><td>33.73 (→0)</td><td>33.72 (0.00)</td><td>36.58 (0.00)</td><td>22.80 (→0)</td><td>72.41 (→0)</td><td>79.06 (→0)</td><td>57.45 (→0)</td><td>74.20 (0.00)</td><td>51.24 (→0)</td></tr><tr><td>GBS-KO</td><td>69.23 (→0)</td><td>69.34 (→0)</td><td>80.70 (→0)</td><td>98.20 (0.00)</td><td>74.38 (0.00)</td><td>82.44 (→0)</td><td>88.10 (0.00)</td><td>99.40 (0.00)</td><td>82.72 (→0)</td></tr></table>

Table 3 Clustering performance comparison of each method in terms of NMI on eight real-world datasets.   

<table><tr><td>NMI (%)</td><td>3sources</td><td>BBC</td><td>BBCSport</td><td>NGs</td><td>WebKB</td><td>100leaves</td><td>HW</td><td>HW2sources</td><td>Ave.</td></tr><tr><td>Kmeans</td><td>24.28 (4.39)</td><td>8.93 (9.96)</td><td>11.15 (6.09)</td><td>5.87 (3.63)</td><td>36.77 (6.96)</td><td>80.29 (0.66)</td><td>71.50 (3.66)</td><td>48.33 (1.90)</td><td>36.04 (4.65)</td></tr><tr><td>Ncut</td><td>10.45 (→0)</td><td>1.85 (→0)</td><td>1.76 (0.00)</td><td>7.39 (→0)</td><td>20.58 (→0)</td><td>74.86 (0.51)</td><td>83.02 (0.00)</td><td>36.43 (0.16)</td><td>28.98 (0.12)</td></tr><tr><td>CoregSC</td><td>52.38 (1.98)</td><td>28.63 (0.00)</td><td>22.80 (0.59)</td><td>8.80 (0.77)</td><td>31.39 (2.36)</td><td>91.65 (0.59)</td><td>74.21 (3.27)</td><td>70.63 (2.06)</td><td>47.56 (1.45)</td></tr><tr><td>MSC</td><td>38.50 (2.27)</td><td>55.31 (1.44)</td><td>10.10 (1.22)</td><td>9.72 (1.26)</td><td>22.37 (1.65)</td><td>90.14 (0.76)</td><td>75.60 (3.24)</td><td>62.77 (2.23)</td><td>45.56 (1.76)</td></tr><tr><td>MGL</td><td>57.68 (8.61)</td><td>36.97 (18.97)</td><td>41.14 (21.14)</td><td>83.04 (8.96)</td><td>43.62 (1.43)</td><td>87.53 (0.76)</td><td>82.64 (4.73)</td><td>84.79 (5.36)</td><td>64.71 (8.75)</td></tr><tr><td>MCGL</td><td>10.34 (→0)</td><td>7.41 (→0)</td><td>8.72 (→0)</td><td>10.72 (→0)</td><td>8.60 (→0)</td><td>91.30 (0.00)</td><td>90.55 (0.00)</td><td>95.55 (0.00)</td><td>40.33 (→0)</td></tr><tr><td>ASMV</td><td>8.96 (→0)</td><td>3.48 (0.00)</td><td>3.05 (→0)</td><td>6.30 (→0)</td><td>28.80 (→0)</td><td>90.09 (→0)</td><td>67.09 (→0)</td><td>87.38 (0.00)</td><td>36.89 (→0)</td></tr><tr><td>GBS-KO</td><td>62.16 (0.00)</td><td>56.27 (0.00)</td><td>76.00 (0.00)</td><td>93.92 (→0)</td><td>37.83 (0.00)</td><td>93.43 (→0)</td><td>90.11 (→0)</td><td>98.53 (0.00)</td><td>76.03 (→0)</td></tr></table>

the author Y. Pan.10 The parameter λ is set as 0.005 according to the authors’ setting.

• Multiple Graph Learning (MGL) [6]: This is a parameterfree graph-based framework for multi-view clustering and semi-supervised classification. We use its clustering setting because here we handle clustering problem. The code is obtained from the author F. Nie.11   
• Multi-view Clustering with Graph Learning (MCGL) [13]: This is a recently proposed multi-view graph-based clustering method. We obtained the code from the author K. Zhan.12 The number of neighbors is set to the default value 10.   
• Adaptive Structure-based Multi-view clustering (ASMV) [14]: This is a recent graph-based multi-view clustering approach. The code is provided by the author K. Zhan.13 According to the authors’ settings, the number of neighbors is set to 15, the scale factor LA is set to 10, the projection directions m is set to 50, and the parameters $\theta$ and $\gamma$ are set to the default values.

Results. The performance comparison results are shown in Tables 2 and 3, where we also record the average score (denoted by Ave.) over eight datasets for each method. Note that $ 0$ means that the value is close to zero, and 0.00 denotes zero. The numbers in the parentheses are the standard deviations. From the tables, we make the following observations:

• Our proposed GBS-KO method is markedly better than all baselines. GBS-KO gives the best performance on all the datasets, except the WebKB dataset in terms of NMI and the HW dataset in terms of NMI. The results clearly show that our GBS-KO method is a promising multi-view clustering method.

• All the graph-based methods, i.e, Ncut, MGL, MCGL, ASMV and GBS-KO, perform robustly except MGL. For MGL, it employs K-means on the learned unified representation to produce the final clusters. Since K-means is sensitive to the initial cluster centers, it results in a high standard deviation for MGL.   
• Compared with the recently proposed graph-based methods MCGL and ASMV, GBS-KO performs better or comparably. This suggests that by considering the geometrical structure of each view, the sparse representation, and the auto-weighting can achieve a better clustering result.   
• In some cases, single view baseline methods, i.e., SK-means and SNcut, are even slightly better than some multi-view baseline methods. This indicates that exploring multi-view data still needs good techniques.

# 6.2.3. Comparison among BestView, FeatConcat and GBS-KO

We observed that single view baseline methods perform slightly better than some multi-view baseline methods just now. To further evaluate the clustering performance of our method, we conduct GBS-KO on each view data of each multi-view dataset. We record the results on each view data and report the results of the view that gives the best performance, denoted by BestView. We also merge the features of all views into a single view. Then this merged single view data are fed into GBS-KO, denoted by FeatConcat. Fig. 4 shows the comparison results among them on all benchmark datasets. From the figure, we can clearly see that GBS-KO always results in the best performance on all datasets except for the dataset WebKB. The reason why the results on the dataset WebKB are similar may be that the clustering ability of each view of this dataset is similar, which leads to the results of these three methods are also similar. We also observe that BestView is slightly superior to FeatConcat in most cases. This shows that simply concatenating the features of all views cannot achieve a better performance. Weighting technique is needed as we did in this paper.

![](images/71122e1cd32bc366ae5172a29d8c9ad05e0adf8cf8c9ce835181c4c792747c4c.jpg)

![](images/10b50ad9f95012be1484ffc1a3eef29a0b85d49721feeb5a312d693f704a5595.jpg)  
Fig. 4. Performance comparison among BestView, FeatConcat and GBS-KO on eight real-world datasets.

![](images/879f8bbbc5dca6e6bbf49eb08695ac976cfa9054a3bdbf4fa35f3a64f2c9a8cf.jpg)  
Fig. 5. Performance comparison of each method in terms of running time on eight real-world datasets.

# 6.3. Running time

Since the computational complexity of the proposed algorithm has been analyzed in Section 5.2, here we further evaluate running time of the proposed GBS-KO in practice. The results are shown in Fig. 5. From the figure, we can see that single view baselines (i.e., Kmeans and NCut) often perform more efficient than multi-view baselines (i.e., CoregSC, MSC, MGL, MCGL and ASMV) including the proposed method GBS-KO. The reason is clear because multi-view clustering methods need to handle multi-view data. For all multi-view clustering methods, our method GBS-KO achieves the best performance in most cases. More precisely, GBS-KO is superior to ASMV on all eight datasets, and superior to MSC, MGL and MCGL on six datasets of all eight datasets. GBS-KO is inferior to CoregSC on five datasets of all datasets. All the above results show that the running time of our method GBS-KO is medium among all the methods.

# 7. Conclusions

This paper presented a general system for multi-view clustering, called Graph-Based System (GBS). We discussed the impact of different graph metrics on GBS and introduced a new graph construction method based on manifold learning and sparse representation. As a conclusion, k-nearest graph with our method is a better choice for multi-view clustering. A novel multi-view clustering method based on GBS is proposed to tackle the limitations of the existing graph-based multi-view clustering methods. The proposed method can automatically weight the constructed graph of each view to learn a unified graph, which gives the final clusters without any additional clustering steps. Extensive experiments on

both toy data and real-world data showed the superior performance of the proposed method.

Several questions remain to be investigated in our future work:

1. The proposed method constructs the graph of each view in isolation and keeps the constructed graph fixed during fusion. Instead of keeping them fixed, improving the constructed graph with the help of the learned unified graph during fusion may be more promising for multi-view clustering. This suggests a way to extend the proposed method.   
2. This paper concerns unsupervised multi-view clustering. In real-world problems, data are partially labeled, the unlabeled ones can be labeled according to learning the pairwise constraints (i.e., must-link and cannot-link). So, our another future work is to learn the pairwise constraints for semi-supervised multi-view clustering or classification, such as [12,59].   
3. Also, it would be very interesting to study incomplete multiview clustering based on graph theory. The incomplete multi-view clustering here means that parts of instances are not available in some views [60–62]. It is unclear how to model graph for incomplete multi-view clustering.

# Acknowledgments

This work was supported by the National Science Foundation of China (No. 61572407), and the China Scholarship Council (No. 201707000064). We would like to thank the authors of the baseline systems for their codes.

# References

[1] C. Xu, D. Tao, C. Xu, A survey on multi-view learning, 2013, CoRR abs/1304. 5634.   
[2] J. Zhao, X. Xie, X. Xu, S. Sun, Multi-view learning overview: Recent progress and new challenges, Inf. Fusion 38 (2017) 43–54.   
[3] G. Chao, S. Sun, J. Bi, A survey on multi-view clustering, 2017, CoRR abs/1712. 06246.   
[4] Y. Yang, H. Wang, Multi-view clustering: A survey, Big Data Min. Anal. 1 (2) (2018) 83–107.   
[5] M. Saha, A graph based approach to multiview clustering, in: Proceedings of the International Conference on Pattern Recognition and Machine Intelligence, 2013, pp. 128–133.   
[6] F. Nie, J. Li, X. Li, Parameter-free auto-weighted multiple graph learning: A framework for multiview clustering and semi-supervised classification, in: Proceedings of the International Joint Conference on Artificial Intelligence, 2016, pp. 1881–1887.   
[7] C. Hou, F. Nie, H. Tao, D. Yi, Multi-view unsupervised feature selection with adaptive similarity and view weight, IEEE Trans. Knowl. Data Eng. 29 (9) (2017) 1998–2011.   
[8] F. Nie, J. Li, X. Li, Self-weighted multiview clustering with multiple graphs, in: Proceedings of the International Joint Conference on Artificial Intelligence, 2017, pp. 2564–2570.   
[9] H. Tao, C. Hou, J. Zhu, D. Yi, Multi-view Clustering with adaptively learned graph, in: Proceedings of the Asian Conference on Machine Learning, 2017, pp. 113–128.   
[10] W. Zhuge, F. Nie, C. Hou, D. Yi, Unsupervised single and multiple views feature extraction with structured graph, IEEE Trans. Knowl. Data Eng. 29 (10) (2017) 2347–2359.   
[11] F. Nie, G. Cai, X. Li, Multi-view clustering and semi-supervised classification with adaptive neighbours, in: Proceedings of the AAAI Conference on Artificial Intelligence, 2017, pp. 2408–2414.   
[12] F. Nie, G. Cai, J. Li, X. Li, Auto-weighted multi-view learning for image clustering and semi-supervised classification, IEEE Trans. Image Process. 27 (3) (2018) 1501–1511.   
[13] K. Zhan, C. Zhang, J. Guan, J. Wang, Graph learning for multiview clustering, IEEE Trans. Cybern. 48 (10) (2018) 2887–2895.   
[14] K. Zhan, X. Chang, J. Guan, L. Chen, Z. Ma, Y. Yang, Adaptive structure discovery for multimedia analysis using multiple features, IEEE Trans. Cybern. PP (99) (2018) 1–9.   
[15] D. Cai, X. He, J. Han, T.S. Huang, Graph regularized nonnegative matrix factorization for data representation, IEEE Trans. Pattern Anal. Mach. Intell. 33 (8) (2011) 1548–1560.   
[16] H. Wang, Y. Yang, T. Li, Multi-view clustering via concept factorization with local manifold regularization, in: Proceedings of the IEEE International Conference on Data Mining, 2016, pp. 1245–1250.   
[17] J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, Y. Ma, Robust face recognition via sparse representation, IEEE Trans. Pattern Anal. Mach. Intell. 31 (2) (2009) 210–227.   
[18] J. Shi, J. Malik, Normalized cuts and image segmentation, IEEE Trans. Pattern Anal. Mach. Intell. 22 (8) (2000) 888–905.   
[19] B. Mohar, Y. Alavi, G. Chartrand, O. Oellermann, The Laplacian spectrum of graphs, Graph Theory Combin. Appl. 2 (12) (1991) 871–898.   
[20] T. Xia, D. Tao, T. Mei, Y. Zhang, Multiview spectral embedding, IEEE Trans. Syst. Man Cybern. B 40 (6) (2010) 1438–1446.   
[21] A. Kumar, P. Rai, H.D. III, Co-regularized multi-view spectral clustering, in: Proceedings of the Annual Conference on Neural Information Processing Systems, 2011, pp. 1413–1421.   
[22] R. Xia, Y. Pan, L. Du, J. Yin, Robust multi-view spectral clustering via lowrank and sparse decomposition, in: Proceedings of the AAAI International Conference on Artificial Intelligence, 2014, pp. 2149–2155.   
[23] C. Lu, S. Yan, Z. Lin, Convex sparse spectral clustering: Single-view to multiview, IEEE Trans. Image Process. 25 (6) (2016) 2833–2843.   
[24] L. Feng, L. Cai, Y. Liu, S. Liu, Multi-view spectral clustering via robust local subspace learning, Soft Comput. 21 (8) (2017) 1937–1948.   
[25] J.W. Son, J. Jeon, A. Lee, S. Kim, Spectral clustering with brainstorming process for multi-view data, in: Proceedings of the AAAI International Conference on Artificial Intelligence, 2017, pp. 2548–2554.   
[26] Y. Wang, L. Wu, X. Lin, J. Gao, Multiview spectral clustering via structured low-rank matrix factorization, IEEE Trans. Neural Netw. Learn. Syst. 29 (10) (2018) 4833–4843,.   
[27] L. Zong, X. Zhang, X. Liu, H. Yu, Weighted multi-view spectral clustering based on spectral perturbation, in: Proceedings of the AAAI Conference on Artificial Intelligence, 2018, pp. 4621–4628.   
[28] S. Sun, J. Shawe-Taylor, L. Mao, PAC-Bayes analysis of multi-view learning, Inf. Fusion 35 (2017) 117–131.   
[29] E. Eaton, M. Desjardins, S. Jacob, Multi-view constrained clustering with an incomplete mapping between views, Knowl. Inf. Syst. 38 (1) (2014) 231–257.   
[30] Y. Lu, L. Wang, J. Lu, J. Yang, C. Shen, Multiple kernel clustering based on centered kernel alignment, Pattern Recognit. 47 (11) (2014) 3656–3664.

[31] X. Zhang, L. Zong, X. Liu, H. Yu, Constrained NMF-based multi-View clustering on unmapped data, in: Proceedings of the AAAI Conference on Artificial Intelligence, 2015, pp. 3174–3180.   
[32] X. Zhang, X. Zhang, H. Liu, X. Liu, Multi-task multi-view clustering, IEEE Trans. Knowl. Data Eng. 18 (12) (2016) 3324–3338.   
[33] L. Zong, X. Zhang, H. Yu, Q. Zhao, F. Ding, Local linear neighbor reconstruction for multi-view data, Pattern Recognit. Lett. 84 (2016) 56–62.   
[34] X. Liu, M. Li, L. Wang, Y. Dou, J. Yin, E. Zhu, Multiple kernel K-means with incomplete kernels, in: Proceedings of the AAAI International Conference on Artificial Intelligence, 2017, pp. 2259–2265.   
[35] L. Zong, X. Zhang, L. Zhao, H. Yu, Q. Zhao, Multi-view clustering via multimanifold regularized non-negative matrix factorization, Neural Netw. 88 (2017) 74–89.   
[36] C. Zhang, Q. Hu, H. Fu, P. Zhu, X. Cao, Latent multi-view subspace clustering, in: Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, 2017, pp. 4333–4341.   
[37] S. Wang, E.K. Wang, X. Li, Y. Ye, R.Y.K. Lau, X. Du, Multi-view learning via multiple graph regularized generative model, Knowl.-Based Syst. 121 (2017) 153–162.   
[38] H. Zhao, Z. Ding, Y. Fu, Multi-view clustering via deep matrix factorization, in: Proceedings of the AAAI International Conference on Artificial Intelligence, 2017, pp. 2921–2927.   
[39] S. Huang, Z. Kang, Z. Xu, Self-weighted multi-view clustering with soft capped norm, Knowl.-Based Syst. (2018) 1–8.   
[40] S. Huang, Y. Ren, Z. Xu, Robust multi-view data clustering with multi-view capped-norm k-means, Neurocomputing (2018) 197–208.   
[41] F. Nie, L. Tian, X. Li, Multiview clustering via adaptively weighted procrustes, in: Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2018, pp. 2022–2030.   
[42] B. Liu, Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data, Springer Science & Business Media, 2007.   
[43] D. Tian, A review on image feature extraction and representation techniques, Int. J. Multimedia Ubiquitous Eng. 8 (4) (2013) 385–396.   
[44] O. Lartillot, P. Toiviainen, A matlab toolbox for musical feature extraction fromaudio.in: Proceedings of the International Conference on Digital Audio Effects, 2007, pp. 237–244.   
[45] I.S. Dhillon, Co-clustering documents and words using bipartite spectral graph partitioning, in: Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2001, pp. 269–274.   
[46] K. Fan, On a theorem of Weyl concerning eigenvalues of linear transformations I, Proc. Natl. Acad. Sci. USA 35 (11) (1949) 652–655.   
[47] F. Nie, X. Wang, M.I. Jordan, H. Huang, The constrained Laplacian rank algorithm for graph-based clustering, in: Proceedings of the AAAI Conference on Artificial Intelligence, 2016, pp. 1969–1976.   
[48] D.P. Bertsekas, Constrained Optimization and Lagrange Multiplier Methods, Academic Press, 2014.   
[49] Y. Li, F. Nie, H. Huang, J. Huang, Large-scale multi-view spectral clustering via bipartite graph, in: Proceedings of the AAAI International Conference on Artificial Intelligence, 2015, pp. 2750–2756.   
[50] X. Zhang, L. Zong, Q. You, X. Yong, Sampling for nyström extension-based spectral clustering: Incremental perspective and novel analysis, ACM Trans. Knowl. Discovery Data 11 (1) (2016) 7:1–7:25.   
[51] F. Nie, H. Huang, X. Cai, C.H.Q. Ding, Efficient and robust feature selection via joint l2,1-norms minimization, in: Proceedings of the Annual Conference on Neural Information Processing Systems, 2010, pp. 1813–1821.   
[52] D. Cai, X. He, J. Han, Document clustering using locality preserving indexing, IEEE Trans. Knowl. Data Eng. 17 (12) (2005) 1624–1637.   
[53] J. Hu, T. Li, C. Luo, H. Fujita, Y. Yang, Incremental fuzzy cluster ensemble learning based on rough set theory, Knowl.-Based Syst. 132 (2017) 144–155.   
[54] D. Greene, P. Cunningham, Practical solutions to the problem of diagonal dominance in kernel document clustering, in: Proceedings of the International Conference on Machine Learning, 2006, pp. 377–384.   
[55] S.F. Hussain, G. Bisson, C. Grimal, An improved co-similarity measure for document clustering, in: Proceedings of the International Conference on Machine Learning and Applications, 2010, pp. 190–197.   
[56] Q. Lu, L. Getoor, Link-based classification, in: Proceedings of the International Conference on Machine Learning, 2003, pp. 496–503.   
[57] C. Mallah, J. Cope, J. Orwell, Plant leaf classification using probabilistic integration of shape, texture and margin features, in: Proceedings of the IASTED International Conference Signal Processing, Pattern Recognition and Applications, 2013, pp. 279–286.   
[58] M. van Breukelen, R.P.W. Duin, D.M.J. Tax, J.E. den Hartog, Handwritten digit recognition by combined classifiers, Kybernetika 34 (4) (1998) 381–386.   
[59] Q. Qian, S. Chen, X. Zhou, Multi-view classification with cross-view must-link and cannot-link side information, Knowl.-Based Syst. 54 (2013) 137–146.   
[60] C. Xu, D. Tao, C. Xu, Multi-view learning with incomplete views, IEEE Trans. Image Process. 24 (12) (2015) 5812–5825.   
[61] Q. Yin, S. Wu, L. Wang, Unified subspace learning for incomplete and unlabeled multi-view data, Pattern Recognit. 67 (2017) 313–327.   
[62] J. Liu, Y. Jiang, Z. Li, Z.H. Zhou, Partially shared latent factor learning with multiview data, IEEE Trans. Neural Netw. Learn. Syst. 26 (6) (2014) 1233– 1246.

# A Tutorial on Spectral Clustering

Ulrike von Luxburg

Max Planck Institute for Biological Cybernetics

Spemannstr. 38, 72076 T¨ubingen, Germany

ulrike.luxburg@tuebingen.mpg.de

This article appears in Statistics and Computing, 17 (4), 2007.

The original publication is available at www.springer.com.

# Abstract

In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.

Keywords: spectral clustering; graph Laplacian

# 1 Introduction

Clustering is one of the most widely used techniques for exploratory data analysis, with applications ranging from statistics, computer science, biology to social sciences or psychology. In virtually every scientific field dealing with empirical data, people attempt to get a first impression on their data by trying to identify groups of “similar behavior” in their data. In this article we would like to introduce the reader to the family of spectral clustering algorithms. Compared to the “traditional algorithms” such as $k$ -means or single linkage, spectral clustering has many fundamental advantages. Results obtained by spectral clustering often outperform the traditional approaches, spectral clustering is very simple to implement and can be solved efficiently by standard linear algebra methods.

This tutorial is set up as a self-contained introduction to spectral clustering. We derive spectral clustering from scratch and present different points of view to why spectral clustering works. Apart from basic linear algebra, no particular mathematical background is required by the reader. However, we do not attempt to give a concise review of the whole literature on spectral clustering, which is impossible due to the overwhelming amount of literature on this subject. The first two sections are devoted to a step-by-step introduction to the mathematical objects used by spectral clustering: similarity graphs in Section 2, and graph Laplacians in Section 3. The spectral clustering algorithms themselves will be presented in Section 4. The next three sections are then devoted to explaining why those algorithms work. Each section corresponds to one explanation: Section 5 describes a graph partitioning approach, Section 6 a random walk perspective, and Section 7 a perturbation theory approach. In Section 8 we will study some practical issues related to spectral clustering, and discuss various extensions and literature related to spectral clustering in Section 9.

# 2 Similarity graphs

Given a set of data points $x _ { 1 } , \ldots , x _ { n }$ and some notion of similarity $s _ { i j } ~ \geq ~ 0$ between all pairs of data points $x _ { i }$ and $x _ { j }$ , the intuitive goal of clustering is to divide the data points into several groups such that points in the same group are similar and points in different groups are dissimilar to each other. If we do not have more information than similarities between data points, a nice way of representing the data is in form of the similarity graph $G = ( V , E )$ . Each vertex $v _ { i }$ in this graph represents a data point $x _ { i }$ . Two vertices are connected if the similarity $s _ { i j }$ between the corresponding data points $x _ { i }$ and $x _ { j }$ is positive or larger than a certain threshold, and the edge is weighted by $s _ { i j }$ . The problem of clustering can now be reformulated using the similarity graph: we want to find a partition of the graph such that the edges between different groups have very low weights (which means that points in different clusters are dissimilar from each other) and the edges within a group have high weights (which means that points within the same cluster are similar to each other). To be able to formalize this intuition we first want to introduce some basic graph notation and briefly discuss the kind of graphs we are going to study.

# 2.1 Graph notation

Let $G = ( V , E )$ be an undirected graph with vertex set $V = \{ v _ { 1 } , \ldots , v _ { n } \}$ . In the following we assume that the graph $G$ is weighted, that is each edge between two vertices $v _ { i }$ and $v _ { j }$ carries a non-negative weight $w _ { i j } \geq 0$ . The weighted adjacency matrix of the graph is the matrix $W = ( w _ { i j } ) _ { i , j = 1 , \dots , n }$ . If $w _ { i j } = 0$ this means that the vertices $v _ { i }$ and $v _ { j }$ are not connected by an edge. As $G$ is undirected we require $w _ { i j } = w _ { j i }$ . The degree of a vertex $v _ { i } \in V$ is defined as

$$
d _ {i} = \sum_ {j = 1} ^ {n} w _ {i j}.
$$

Note that, in fact, this sum only runs over all vertices adjacent to $v _ { i }$ , as for all other vertices $v _ { j }$ the weight $w _ { i j }$ is 0. The degree matrix $D$ is defined as the diagonal matrix with the degrees $d _ { 1 } , \ldots , d _ { n }$ on the diagonal. Given a subset of vertices $A \subset V$ , we denote its complement $V \setminus A$ by $\overline { { A } }$ . We define the indicator vector $\mathbb { 1 } _ { A } = ( f _ { 1 } , \ldots , f _ { n } ) ^ { \prime } \in \mathbb { R } ^ { n }$ as the vector with entries $f _ { i } = 1$ if $v _ { i } \in A$ and $f _ { i } = 0$ otherwise. For convenience we introduce the shorthand notation $i \in A$ for the set of indices $\{ i \mid v _ { i } \in A \}$ , in particular when dealing with a sum like $\textstyle \sum _ { i \in A } w _ { i j }$ . For two not necessarily disjoint sets $A , B \subset V$ we define

$$
W (A, B) := \sum_ {i \in A, j \in B} w _ {i j}.
$$

We consider two different ways of measuring the “size” of a subset $A \subset V$ :

$$
| A | := \text {t h e n u m b e r o f v e r t i c e s i n} A
$$

$$
\operatorname {v o l} (A) := \sum_ {i \in A} d _ {i}.
$$

Intuitively, $| A |$ measures the size of $A$ by its number of vertices, while $\operatorname { v o l } ( A )$ measures the size of $A$ by summing over the weights of all edges attached to vertices in $A$ . A subset $A \subset V$ of a graph is connected if any two vertices in $A$ can be joined by a path such that all intermediate points also lie in $A$ . A subset $A$ is called a connected component if it is connected and if there are no connections between vertices in $A$ and $\overline { { A } }$ . The nonempty sets $A _ { 1 } , \ldots , A _ { k }$ form a partition of the graph if $A _ { i } \cap A _ { j } = \varnothing$ and $A _ { 1 }$ . . . $A _ { k } = V$ .

# 2.2 Different similarity graphs

There are several popular constructions to transform a given set $x _ { 1 } , \ldots , x _ { n }$ of data points with pairwise similarities $s _ { i j }$ or pairwise distances $d _ { i j }$ into a graph. When constructing similarity graphs the goal is to model the local neighborhood relationships between the data points.

The $\varepsilon$ -neighborhood graph: Here we connect all points whose pairwise distances are smaller than $\varepsilon$ . As the distances between all connected points are roughly of the same scale (at most $\varepsilon$ ), weighting the edges would not incorporate more information about the data to the graph. Hence, the $\varepsilon$ -neighborhood graph is usually considered as an unweighted graph.

$k$ -nearest neighbor graphs: Here the goal is to connect vertex with vertex if is among $v _ { i }$ $v _ { j }$ $v _ { j }$ the $k$ -nearest neighbors of $v _ { i }$ . However, this definition leads to a directed graph, as the neighborhood relationship is not symmetric. There are two ways of making this graph undirected. The first way is to simply ignore the directions of the edges, that is we connect $v _ { i }$ and $v _ { j }$ with an undirected edge if $v _ { i }$ is among the $k$ -nearest neighbors of or if is among the $k$ -nearest neighbors of . The resulting $v _ { j }$ $v _ { j }$ $v _ { i }$ graph is what is usually called the $k$ -nearest neighbor graph. The second choice is to connect vertices and if both is among the $k$ -nearest neighbors of and is among the $k$ -nearest neighbors of $v _ { i }$ $v _ { j }$ $v _ { i }$ $v _ { j }$ $v _ { j }$ $v _ { i }$ . The resulting graph is called the mutual $k$ -nearest neighbor graph. In both cases, after connecting the appropriate vertices we weight the edges by the similarity of their endpoints.

The fully connected graph: Here we simply connect all points with positive similarity with each other, and we weight all edges by $s _ { i j }$ . As the graph should represent the local neighborhood relationships, this construction is only useful if the similarity function itself models local neighborhoods. An example for such a similarity function is the Gaussian similarity function $s ( x _ { i } , x _ { j } ) =$ $\exp ( - \| x _ { i } - x _ { j } \| ^ { 2 } / ( 2 \sigma ^ { 2 } ) )$ , where the parameter $\sigma$ controls the width of the neighborhoods. This parameter plays a similar role as the parameter $\varepsilon$ in case of the $\varepsilon$ -neighborhood graph.

All graphs mentioned above are regularly used in spectral clustering. To our knowledge, theoretical results on the question how the choice of the similarity graph influences the spectral clustering result do not exist. For a discussion of the behavior of the different graphs we refer to Section 8.

# 3 Graph Laplacians and their basic properties

The main tools for spectral clustering are graph Laplacian matrices. There exists a whole field dedicated to the study of those matrices, called spectral graph theory (e.g., see Chung, 1997). In this section we want to define different graph Laplacians and point out their most important properties. We will carefully distinguish between different variants of graph Laplacians. Note that in the literature there is no unique convention which matrix exactly is called “graph Laplacian”. Usually, every author just calls “his” matrix the graph Laplacian. Hence, a lot of care is needed when reading literature on graph Laplacians.

In the following we always assume that $G$ is an undirected, weighted graph with weight matrix $W$ , where $w _ { i j } = w _ { j i } \ge 0$ . When using eigenvectors of a matrix, we will not necessarily assume that they are normalized. For example, the constant vector 1 and a multiple $a \mathbb { 1 }$ for some $a \neq 0$ will be considered as the same eigenvectors. Eigenvalues will always be ordered increasingly, respecting multiplicities. By “the first $k$ eigenvectors” we refer to the eigenvectors corresponding to the $k$ smallest eigenvalues.

# 3.1 The unnormalized graph Laplacian

The unnormalized graph Laplacian matrix is defined as

$$
L = D - W.
$$

An overview over many of its properties can be found in Mohar (1991, 1997). The following proposition summarizes the most important facts needed for spectral clustering.

Proposition 1 (Properties of $L$ ) The matrix L satisfies the following properties:

1. For every vector $f \in \mathbb { R } ^ { n }$ we have

$$
f ^ {\prime} L f = \frac {1}{2} \sum_ {i, j = 1} ^ {n} w _ {i j} \left(f _ {i} - f _ {j}\right) ^ {2}.
$$

2. L is symmetric and positive semi-definite.   
3. The smallest eigenvalue of $L$ is $\boldsymbol { \mathit { 0 } }$ , the corresponding eigenvector is the constant one vector 1.   
4. L has n non-negative, real-valued eigenvalues $0 = \lambda _ { 1 } \leq \lambda _ { 2 } \leq . . . \leq \lambda _ { n }$ .

Proof.

Part (1): By the definition of $d _ { i }$ ,

$$
\begin{array}{l} f ^ {\prime} L f = f ^ {\prime} D f - f ^ {\prime} W f = \sum_ {i = 1} ^ {n} d _ {i} f _ {i} ^ {2} - \sum_ {i, j = 1} ^ {n} f _ {i} f _ {j} w _ {i j} \\ = \frac {1}{2} \left(\sum_ {i = 1} ^ {n} d _ {i} f _ {i} ^ {2} - 2 \sum_ {i, j = 1} ^ {n} f _ {i} f _ {j} w _ {i j} + \sum_ {j = 1} ^ {n} d _ {j} f _ {j} ^ {2}\right) = \frac {1}{2} \sum_ {i, j = 1} ^ {n} w _ {i j} (f _ {i} - f _ {j}) ^ {2}. \\ \end{array}
$$

Part (2): The symmetry of $L$ follows directly from the symmetry of $W$ and $D$ . The positive semidefiniteness is a direct consequence of Part (1), which shows that $f ^ { \prime } L f \geq 0$ for all $f \in \mathbb { R } ^ { n }$ .

Part (3): Obvious.

Part (4) is a direct consequence of Parts (1) - (3).

Note that the unnormalized graph Laplacian does not depend on the diagonal elements of the adjacency matrix $W$ . Each adjacency matrix which coincides with $W$ on all off-diagonal positions leads to the same unnormalized graph Laplacian $L$ . In particular, self-edges in a graph do not change the corresponding graph Laplacian.

The unnormalized graph Laplacian and its eigenvalues and eigenvectors can be used to describe many properties of graphs, see Mohar (1991, 1997). One example which will be important for spectral clustering is the following:

Proposition 2 (Number of connected components and the spectrum of $L$ ) Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue 0 of $L$ equals the number of connected components $A _ { 1 } , \ldots , A _ { k }$ in the graph. The eigenspace of eigenvalue 0 is spanned by the indicator vectors $\mathbb { 1 } _ { A _ { 1 } } , \ldots , \mathbb { 1 } _ { A _ { k } }$ of those components.

Proof. We start with the case $k = 1$ , that is the graph is connected. Assume that $f$ is an eigenvector with eigenvalue 0. Then we know that

$$
0 = f ^ {\prime} L f = \sum_ {i, j = 1} ^ {n} w _ {i j} \left(f _ {i} - f _ {j}\right) ^ {2}.
$$

As the weights $w _ { i j }$ are non-negative, this sum can only vanish if all terms $w _ { i j } ( f _ { i } - f _ { j } ) ^ { 2 }$ vanish. Thus, if two vertices $v _ { i }$ and $v _ { j }$ are connected (i.e., $w _ { i j } > 0$ ), then $f _ { i }$ needs to equal $f _ { j }$ . With this argument we can see that $f$ needs to be constant for all vertices which can be connected by a path in the graph. Moreover, as all vertices of a connected component in an undirected graph can be connected by a path, $f$ needs to be constant on the whole connected component. In a graph consisting of only one connected component we thus only have the constant one vector 1 as eigenvector with eigenvalue 0, which obviously is the indicator vector of the connected component.

Now consider the case of $k$ connected components. Without loss of generality we assume that the vertices are ordered according to the connected components they belong to. In this case, the adjacency matrix $W$ has a block diagonal form, and the same is true for the matrix $L$ :

$$
L = \left( \begin{array}{c c c c} L _ {1} & & & \\ & L _ {2} & & \\ & & \ddots & \\ & & & L _ {k} \end{array} \right)
$$

Note that each of the blocks $L _ { i }$ is a proper graph Laplacian on its own, namely the Laplacian corresponding to the subgraph of the $i$ -th connected component. As it is the case for all block diagonal matrices, we know that the spectrum of $L$ is given by the union of the spectra of $L _ { i }$ , and the corresponding eigenvectors of $L$ are the eigenvectors of $L _ { i }$ , filled with 0 at the positions of the other blocks. As each $L _ { i }$ is a graph Laplacian of a connected graph, we know that every $\boldsymbol { L } _ { i }$ has eigenvalue 0 with multiplicity 1, and the corresponding eigenvector is the constant one vector on the $_ i$ -th connected component. Thus, the matrix $L$ has as many eigenvalues 0 as there are connected components, and the corresponding eigenvectors are the indicator vectors of the connected components. 2

# 3.2 The normalized graph Laplacians

There are two matrices which are called normalized graph Laplacians in the literature. Both matrices are closely related to each other and are defined as

$$
L _ {\mathrm {s y m}} := D ^ {- 1 / 2} L D ^ {- 1 / 2} = I - D ^ {- 1 / 2} W D ^ {- 1 / 2}
$$

$$
L _ {\mathrm {r w}} := D ^ {- 1} L = I - D ^ {- 1} W.
$$

We denote the first matrix by $L _ { \mathrm { s y m } }$ as it is a symmetric matrix, and the second one by $L _ { \mathrm { r w } }$ as it is closely related to a random walk. In the following we summarize several properties of $L _ { \mathrm { s y m } }$ and $L _ { \mathrm { r w } }$ . The standard reference for normalized graph Laplacians is Chung (1997).

Proposition 3 (Properties of $L _ { \mathbf { s y m } }$ and $L _ { \mathbf { r w } }$ ) The normalized Laplacians satisfy the following properties:

1. For every $f \in \mathbb { R } ^ { n }$ we have

$$
f ^ {\prime} L _ {s y m} f = \frac {1}{2} \sum_ {i, j = 1} ^ {n} w _ {i j} \left(\frac {f _ {i}}{\sqrt {d _ {i}}} - \frac {f _ {j}}{\sqrt {d _ {j}}}\right) ^ {2}.
$$

2. $\lambda$ is an eigenvalue of $\scriptstyle L _ { r w }$ with eigenvector u if and only if λ is an eigenvalue of $L _ { s y m }$ with eigenvector $w = D ^ { 1 / 2 } u$ .   
3. λ is an eigenvalue of $\scriptstyle L _ { r w }$ with eigenvector u if and only if λ and u solve the generalized eigenproblem $L u = \lambda D u$ .

4. 0 is an eigenvalue of $\scriptstyle L _ { r w }$ with the constant one vector 1 as eigenvector. 0 is an eigenvalue of $L _ { s y m }$ with eigenvector $D ^ { 1 / 2 } \mathbb { 1 }$ .   
5. $L _ { s y m }$ and $L _ { r w }$ are positive semi-definite and have n non-negative real-valued eigenvalues $0 =$ $\lambda _ { 1 } \leq . . . \leq \lambda _ { n }$ .

Proof. Part (1) can be proved similarly to Part (1) of Proposition 1.

Part (2) can be seen immediately by multiplying the eigenvalue equation $L _ { \mathrm { s y m } } w = \lambda w$ with $D ^ { - 1 / 2 }$ from the left and substituting $u = D ^ { - 1 / 2 } w$ .   
Part (3) follows directly by multiplying the eigenvalue equation $L _ { \mathrm { r w } } u = \lambda u$ with $D$ from the left.   
Part (4): The first statement is obvious as $L _ { \mathrm { r w } } \mathbb { 1 } = 0$ , the second statement follows from (2).   
Part (5): The statement about $L _ { \mathrm { s y m } }$ follows from (1), and then the statement about $L _ { \mathrm { r w } }$ follows from (2). 2

As it is the case for the unnormalized graph Laplacian, the multiplicity of the eigenvalue 0 of the normalized graph Laplacian is related to the number of connected components:

Proposition 4 (Number of connected components and spectra of $\scriptstyle L _ { \mathbf { s y m } }$ and $L _ { \mathbf { r w } }$ ) Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue 0 of both $\scriptstyle L _ { r w }$ and $L _ { s y m }$ equals the number of connected components $A _ { 1 } , \ldots , A _ { k }$ in the graph. For $L _ { r w }$ , the eigenspace of 0 is spanned by the indicator vectors $\mathbb { 1 } _ { A _ { i } }$ of those components. For $L _ { s y m }$ , the eigenspace of 0 is spanned by the vectors $D ^ { 1 / 2 } \mathbb { 1 } _ { A _ { i } }$ .

Proof. The proof is analogous to the one of Proposition 2, using Proposition 3.

# 4 Spectral Clustering Algorithms

Now we would like to state the most common spectral clustering algorithms. For references and the history of spectral clustering we refer to Section 9. We assume that our data consists of $n$ “points” $x _ { 1 } , \ldots , x _ { n }$ which can be arbitrary objects. We measure their pairwise similarities $s _ { i j } ~ = ~ s ( x _ { i } , x _ { j } )$ by some similarity function which is symmetric and non-negative, and we denote the corresponding similarity matrix by ${ \cal { S } } = ( s _ { i j } ) _ { i , j = 1 \dots n }$ .

# Unnormalized spectral clustering

Input: Similarity matrix $S \in \mathbb { R } ^ { n \times n }$ , number $k$ of clusters to construct.

• Construct a similarity graph by one of the ways described in Section 2. Let W be its weighted adjacency matrix.   
• Compute the unnormalized Laplacian $L$   
• Compute the first $\boldsymbol { k }$ eigenvectors $u _ { 1 } , \ldots , u _ { k }$ of $\pmb { L }$   
• Let $U \in \mathbb { R } ^ { n \times k }$ be the matrix containing the vectors $u _ { 1 } , \ldots , u _ { k }$ as columns.   
• For $i = 1 , \ldots , n$ , let $y _ { i } \in \mathbb { R } ^ { k }$ be the vector corresponding to the $i$ -th row of $U$   
• Cluster the points $( y _ { i } ) _ { i = 1 , \ldots , n }$ in $\mathbb { R } ^ { k }$ with the $k$ -means algorithm into clusters $C _ { 1 } , \ldots , C _ { k }$ .

Output: Clusters $A _ { 1 } , \ldots , A _ { k }$ with $A _ { i } = \{ j | y _ { j } \in C _ { i } \}$ .

There are two different versions of normalized spectral clustering, depending which of the normalized

graph Laplacians is used. We name both algorithms after two popular papers, for more references and history please see Section 9.

# Normalized spectral clustering according to Shi and Malik (2000)

Input: Similarity matrix $S \in \mathbb { R } ^ { n \times n }$ , number $k$ of clusters to construct.

• Construct a similarity graph by one of the ways described in Section 2. Let W be its weighted adjacency matrix.   
• Compute the unnormalized Laplacian $L$   
• Compute the first $\boldsymbol { k }$ generalized eigenvectors $u _ { 1 } , \ldots , u _ { k }$ of the generalized eigenproblem $L u = \lambda D u$ .   
• Let $U \in \mathbb { R } ^ { n \times k }$ be the matrix containing the vectors $u _ { 1 } , \ldots , u _ { k }$ as columns.   
• For $i = 1 , \ldots , n$ , let $y _ { i } \in \mathbb { R } ^ { k }$ be the vector corresponding to the $i$ -th row of $U$   
• Cluster the points $( y _ { i } ) _ { i = 1 , \ldots , n }$ in $\mathbb { R } ^ { k }$ with the $k$ -means algorithm into clusters $C _ { 1 } , \ldots , C _ { k }$ .

Output: Clusters $A _ { 1 } , \ldots , A _ { k }$ with $A _ { i } = \{ j | y _ { j } \in C _ { i } \} .$

Note that this algorithm uses the generalized eigenvectors of $L$ , which according to Proposition 3 correspond to the eigenvectors of the matrix $L _ { \mathrm { r w } }$ . So in fact, the algorithm works with eigenvectors of the normalized Laplacian $L _ { \mathrm { r w } }$ , and hence is called normalized spectral clustering. The next algorithm also uses a normalized Laplacian, but this time the matrix $L _ { \mathrm { s y m } }$ instead of $L _ { \mathrm { r w } }$ . As we will see, this algorithm needs to introduce an additional row normalization step which is not needed in the other algorithms. The reasons will become clear in Section 7.

# Normalized spectral clustering according to $\mathbf { N g }$ , Jordan, and Weiss (2002)

Input: Similarity matrix $S \in \mathbb { R } ^ { n \times n }$ , number $k$ of clusters to construct.

• Construct a similarity graph by one of the ways described in Section 2. Let W be its weighted adjacency matrix.   
• Compute the normalized Laplacian $L _ { \mathrm { s y m } }$   
• Compute the first $\boldsymbol { k }$ eigenvectors $u _ { 1 } , \ldots , u _ { k }$ of $\pmb { L } _ { \mathbf { s y m } }$ .   
• Let $U \in \mathbb { R } ^ { n \times k }$ be the matrix containing the vectors $u _ { 1 } , \ldots , u _ { k }$ as columns.   
• Form the matrix $\ b { T } \in \mathbb { R } ^ { n \times k }$ from $U$ by normalizing the rows to norm 1, that is set $t _ { i j } = u _ { i j } / ( \sum _ { k } u _ { i k } ^ { 2 } ) ^ { 1 / 2 }$ .   
• For $i = 1 , \ldots , n$ , let $y _ { i } \in \mathbb { R } ^ { k }$ be the vector corresponding to the $i$ -th row of $T$   
• Cluster the points $( y _ { i } ) _ { i = 1 , \ldots , n }$ with the $k$ -means algorithm into clusters $C _ { 1 } , \ldots , C _ { k }$ .

Output: Clusters $A _ { 1 } , \ldots , A _ { k }$ with $A _ { i } = \{ j | y _ { j } \in C _ { i } \}$ .

All three algorithms stated above look rather similar, apart from the fact that they use three different graph Laplacians. In all three algorithms, the main trick is to change the representation of the abstract data points $x _ { i }$ to points $y _ { i } \in \mathbb { R } ^ { k }$ . It is due to the properties of the graph Laplacians that this change of representation is useful. We will see in the next sections that this change of representation enhances the cluster-properties in the data, so that clusters can be trivially detected in the new representation. In particular, the simple $k$ -means clustering algorithm has no difficulties to detect the clusters in this new representation. Readers not familiar with $k$ -means can read up on this algorithm in numerous

![](images/7e47bacd233182c0ab55c3a9d109d7efb3db461f7411291b56f62cda679c24a4.jpg)

![](images/c34692badc6cbf356f88671ee01b17528696991178286c27b33e8e5840ec0ad8.jpg)  
Eigenvalues

![](images/df8873ab31db3b8c8d151c37110bd74f671dbeb4da102bc93cda68afd5518f12.jpg)  
Eigenvalues

![](images/b18d9912e7c8f08c9553eef78f664355d1c410b5c23f0e384df42bd4e2c14e7d.jpg)  
Eigenvalues

![](images/ab05802c4dfdc82310072697a9e1ba440fb54aa7d20e88ffe8d8db0f8f32caa0.jpg)  
Eigenvalues

![](images/3254fbb6fe3955c5521b52e1248d14976a68c505235cb2640c693e0af2dda103.jpg)  
Eigenvector 1   
Eigenvector 3

![](images/d052bc3fb683927ae80e416ef632be5a7ecd44779211751558f73f37dcdf1117.jpg)

![](images/8d35ba0a1406e54df41fba46d41fc7a0a4f7949947ca67e47fe5151896b653c6.jpg)

![](images/14803b48a0c9c209595a03ce3cee94896a45a8afc6f2ae63d8606ee0713f9021.jpg)  
Eigenvector 4

![](images/5516eda1a2476c75affa5741519a18b7174abad9f9afc4e7a81b3b0fb5da8c00.jpg)  
Eigenvector 5

![](images/cf5722888583c827ca02baf23d49b8b2960690798b1c91a9c958627838908fce.jpg)  
Eigenvector 1

![](images/608fe11c516527aa09085505791ed90b1edbe0f4b6544b5d5bbe537df2b709d4.jpg)  
Eigenvector 3

![](images/4114c77301c33f99490cf939534fe3e929747907f0c6aeb2faddbd84db24aa3a.jpg)  
Eigenvector 4

Eigenvector 5

![](images/ec0fffff8adb0a10c920057603f320a0bd8e38d7c66287f3f132c7e8e7e02e37.jpg)  
Eigenvector 3   
Eigenvector 4   
Eigenvector 5

![](images/7085f86645e1302465895088ed0489b6eb6afa3f7e3dfc974601328f452244da.jpg)  
Eigenvector 1   
Eigenvector 2

![](images/b167a8dffa80d039509afa02737e93ef6e10746c12fbc24f8a8c544cb43960ee.jpg)

![](images/3c490e8875e6da8d2fd261b8c31ffb337733fa0ee2bbfd76561d753f345b8e48.jpg)

![](images/ef62275b95571a155ddda6afddb3dffd47a4130026104c8b11fe31003eb2775d.jpg)

![](images/7fef0818c95f1df15a370ac045638445043fcc7a2df9ef53986116af01c7eaba.jpg)

![](images/b497506257eac96c4a8d57a0d4f3accf89826f2d09bffa89a064e70c063b3d28.jpg)  
Eigenvector 1

![](images/27513276dc3a075526b083c39dccd739e05ed4d17b5bf86028bf8345c408d7a3.jpg)

![](images/0260f7d7646eb7fb7913d920046e50b716734a81ce62effb876ec543d5ca9cd7.jpg)

![](images/7ad9a68068c1591ee09f5a79b52e72a9801a7c3fca91790700a0cbb2c039fd28.jpg)  
Figure 1: Toy example for spectral clustering where the data points have been drawn from a mixture of four Gaussians on $\mathbb { R }$ . Left upper corner: histogram of the data. First and second row: eigenvalues and eigenvectors of $L _ { \mathrm { r w } }$ and $L$ based on the $k$ -nearest neighbor graph. Third and fourth row: eigenvalues and eigenvectors of $L _ { \mathrm { r w } }$ and $L$ based on the fully connected graph. For all plots, we used the Gaussian kernel with $\sigma = 1$ as similarity function. See text for more details.

text books, for example in Hastie, Tibshirani, and Friedman (2001).

Before we dive into the theory of spectral clustering, we would like to illustrate its principle on a very simple toy example. This example will be used at several places in this tutorial, and we chose it because it is so simple that the relevant quantities can easily be plotted. This toy data set consists of a random sample of 200 points $x _ { 1 } , \dotsc , x _ { 2 0 0 } \in \mathbb { R }$ drawn according to a mixture of four Gaussians. The first row of Figure 1 shows the histogram of a sample drawn from this distribution (the $x$ -axis represents the one-dimensional data space). As similarity function on this data set we choose the Gaussian similarity function $s ( x _ { i } , x _ { j } ) = \exp ( - | x _ { i } - x _ { j } | ^ { 2 } / ( 2 \sigma ^ { 2 } ) )$ with $\sigma = 1$ . As similarity graph we consider both the fully connected graph and the 10-nearest neighbor graph. In Figure 1 we show the first eigenvalues and eigenvectors of the unnormalized Laplacian $L$ and the normalized Laplacian $L _ { \mathrm { r w } }$ . That is, in the eigenvalue plot we plot $i$ vs. $\lambda _ { i }$ (for the moment ignore the dashed line and the different shapes of the eigenvalues in the plots for the unnormalized case; their meaning will be discussed in Section 8.5). In the eigenvector plots of an eigenvector $u = ( u _ { 1 } , \ldots , u _ { 2 0 0 } ) ^ { \prime }$ we plot $x _ { i }$ vs. $u _ { i }$ (note that in the example chosen $x _ { i }$ is simply a real number, hence we can depict it on the $x$ -axis). The first two rows of Figure 1 show the results based on the 10-nearest neighbor graph. We can see that the first four eigenvalues are $0$ , and the corresponding eigenvectors are cluster indicator vectors. The reason is that the clusters

form disconnected parts in the 10-nearest neighbor graph, in which case the eigenvectors are given as in Propositions 2 and 4. The next two rows show the results for the fully connected graph. As the Gaussian similarity function is always positive, this graph only consists of one connected component. Thus, eigenvalue 0 has multiplicity 1, and the first eigenvector is the constant vector. The following eigenvectors carry the information about the clusters. For example in the unnormalized case (last row), if we threshold the second eigenvector at 0, then the part below 0 corresponds to clusters 1 and 2, and the part above 0 to clusters 3 and 4. Similarly, thresholding the third eigenvector separates clusters 1 and 4 from clusters 2 and 3, and thresholding the fourth eigenvector separates clusters 1 and 3 from clusters 2 and 4. Altogether, the first four eigenvectors carry all the information about the four clusters. In all the cases illustrated in this figure, spectral clustering using $k$ -means on the first four eigenvectors easily detects the correct four clusters.

# 5 Graph cut point of view

The intuition of clustering is to separate points in different groups according to their similarities. For data given in form of a similarity graph, this problem can be restated as follows: we want to find a partition of the graph such that the edges between different groups have a very low weight (which means that points in different clusters are dissimilar from each other) and the edges within a group have high weight (which means that points within the same cluster are similar to each other). In this section we will see how spectral clustering can be derived as an approximation to such graph partitioning problems.

Given a similarity graph with adjacency matrix $W$ , the simplest and most direct way to construct a partition of the graph is to solve the mincut problem. To define it, please recall the notation $\textstyle W ( A , B ) : = \sum _ { i \in A , j \in B } w _ { i j }$ and $A$ for the complement of $A$ . For a given number $k$ of subsets, the mincut approach simply consists in choosing a partition $A _ { 1 } , \ldots , A _ { k }$ which minimizes

$$
\operatorname {c u t} (A _ {1}, \ldots , A _ {k}) := \frac {1}{2} \sum_ {i = 1} ^ {k} W (A _ {i}, \overline {{A}} _ {i}).
$$

Here we introduce the factor $1 / 2$ for notational consistency, otherwise we would count each edge twice in the cut. In particular for $k = 2$ , mincut is a relatively easy problem and can be solved efficiently, see Stoer and Wagner (1997) and the discussion therein. However, in practice it often does not lead to satisfactory partitions. The problem is that in many cases, the solution of mincut simply separates one individual vertex from the rest of the graph. Of course this is not what we want to achieve in clustering, as clusters should be reasonably large groups of points. One way to circumvent this problem is to explicitly request that the sets $A _ { 1 } , \ldots , A _ { k }$ are “reasonably large”. The two most common objective functions to encode this are RatioCut (Hagen and Kahng, 1992) and the normalized cut Ncut (Shi and Malik, 2000). In RatioCut, the size of a subset $A$ of a graph is measured by its number of vertices $| A |$ , while in Ncut the size is measured by the weights of its edges vol(A). The definitions are:

$$
\operatorname {R a t i o C u t} \left(A _ {1}, \dots , A _ {k}\right) := \frac {1}{2} \sum_ {i = 1} ^ {k} \frac {W \left(A _ {i} , \bar {A} _ {i}\right)}{\left| A _ {i} \right|} = \sum_ {i = 1} ^ {k} \frac {\operatorname {c u t} \left(A _ {i} , \bar {A} _ {i}\right)}{\left| A _ {i} \right|}
$$

$$
\operatorname {N c u t} (A _ {1}, \dots , A _ {k}) := \frac {1}{2} \sum_ {i = 1} ^ {k} \frac {W (A _ {i} , \overline {{A}} _ {i})}{\operatorname {v o l} (A _ {i})} = \sum_ {i = 1} ^ {k} \frac {\operatorname {c u t} (A _ {i} , \overline {{A}} _ {i})}{\operatorname {v o l} (A _ {i})}.
$$

Note that both objective functionsular, the minimum of the function value if the cluste is achieved if all $A _ { i }$ are not too small. In partic-oincide, and the minimum of $\textstyle \sum _ { i = 1 } ^ { k } ( 1 / | A _ { i } | )$ $| A _ { i } |$ $\textstyle \sum _ { i = 1 } ^ { k } ( 1 / \operatorname { v o l } ( A _ { i } ) )$ is achieved if all $\operatorname { v o l } ( A _ { i } )$ coincide. So what both objective functions try to achieve is that the clusters are “balanced”, as measured by the number of vertices or edge weights, respectively. Unfortunately, introducing balancing conditions makes the previously simple to solve mincut problem

become NP hard, see Wagner and Wagner (1993) for a discussion. Spectral clustering is a way to solve relaxed versions of those problems. We will see that relaxing Ncut leads to normalized spectral clustering, while relaxing RatioCut leads to unnormalized spectral clustering (see also the tutorial slides by Ding (2004)).

# 5.1 Approximating RatioCut for $k = 2$

Let us start with the case of RatioCut and $k = 2$ , because the relaxation is easiest to understand in this setting. Our goal is to solve the optimization problem

$$
\min  _ {A \subset V} \operatorname {R a t i o C u t} (A, \bar {A}). \tag {1}
$$

We first rewrite the problem in a more convenient form. Given a subset $A \subset V$ we define the vector $f = ( f _ { 1 } , \ldots , f _ { n } ) ^ { \prime } \in \mathbb { R } ^ { n }$ with entries

$$
f _ {i} = \left\{ \begin{array}{l l} \sqrt {| \bar {A} | / | A |} & \text {i f} v _ {i} \in A \\ - \sqrt {| A | / | \bar {A} |} & \text {i f} v _ {i} \in \bar {A}. \end{array} \right. \tag {2}
$$

Now the RatioCut objective function can be conveniently rewritten using the unnormalized graph Laplacian. This is due to the following calculation:

$$
\begin{array}{l} f ^ {\prime} L f = \frac {1}{2} \sum_ {i, j = 1} ^ {n} w _ {i j} (f _ {i} - f _ {j}) ^ {2} \\ = \frac {1}{2} \sum_ {i \in A, j \in \overline {{A}}} w _ {i j} \left(\sqrt {\frac {| \overline {{A}} |}{| A |}} + \sqrt {\frac {| A |}{| \overline {{A}} |}}\right) ^ {2} + \frac {1}{2} \sum_ {i \in \overline {{A}}, j \in A} w _ {i j} \left(- \sqrt {\frac {| \overline {{A}} |}{| A |}} - \sqrt {\frac {| A |}{| \overline {{A}} |}}\right) ^ {2} \\ = \operatorname {c u t} (A, \bar {A}) \left(\frac {| \bar {A} |}{| A |} + \frac {| A |}{| \bar {A} |} + 2\right) \\ = \operatorname {c u t} (A, \bar {A}) \left(\frac {| A | + | \bar {A} |}{| A |} + \frac {| A | + | \bar {A} |}{| \bar {A} |}\right) \\ = | V | \cdot \operatorname {R a t i o C u t} (A, \bar {A}). \\ \end{array}
$$

Additionally, we have

$$
\sum_ {i = 1} ^ {n} f _ {i} = \sum_ {i \in A} \sqrt {\frac {| \overline {{A}} |}{| A |}} - \sum_ {i \in \overline {{A}}} \sqrt {\frac {| A |}{| \overline {{A}} |}} = | A | \sqrt {\frac {| \overline {{A}} |}{| A |}} - | \overline {{A}} | \sqrt {\frac {| A |}{| \overline {{A}} |}} = 0.
$$

In other words, the vector $f$ as defined in Equation (2) is orthogonal to the constant one vector 1. Finally, note that $f$ satisfies

$$
\| f \| ^ {2} = \sum_ {i = 1} ^ {n} f _ {i} ^ {2} = | A | \frac {| \overline {{A}} |}{| A |} + | \overline {{A}} | \frac {| A |}{| \overline {{A}} |} = | \overline {{A}} | + | A | = n.
$$

Altogether we can see that the problem of minimizing (1) can be equivalently rewritten as

$$
\min  _ {A \subset V} f ^ {\prime} L f \text {s u b j e c t t o} f \perp \mathbb {1}, f _ {i} \text {a s d e f i n e d i n E q . (2) ,} \| f \| = \sqrt {n}. \tag {3}
$$

This is a discrete optimization problem as the entries of the solution vector $f$ are only allowed to take two particular values, and of course it is still NP hard. The most obvious relaxation in this setting is

to discard the discreteness condition and instead allow that $f _ { i }$ takes arbitrary values in $\mathbb { R }$ . This leads to the relaxed optimization problem

$$
\min  _ {f \in \mathbb {R} ^ {n}} f ^ {\prime} L f \text {s u b j e c t t o} f \perp \mathbb {1}, \| f \| = \sqrt {n}. \tag {4}
$$

By the Rayleigh-Ritz theorem (e.g., see Section 5.5.2. of L¨utkepohl, 1997) it can be seen immediately that the solution of this problem is given by the vector $f$ which is the eigenvector corresponding to the second smallest eigenvalue of $L$ (recall that the smallest eigenvalue of $L$ is 0 with eigenvector 1). So we can approximate a minimizer of RatioCut by the second eigenvector of $L$ . However, in order to obtain a partition of the graph we need to re-transform the real-valued solution vector $f$ of the relaxed problem into a discrete indicator vector. The simplest way to do this is to use the sign of $f$ as indicator function, that is to choose

$$
\left\{ \begin{array}{l l} v _ {i} \in A & \text {i f} f _ {i} \geq 0 \\ v _ {i} \in \overline {{A}} & \text {i f} f _ {i} <   0. \end{array} \right.
$$

However, in particular in the case of $k > 2$ treated below, this heuristic is too simple. What most spectral clustering algorithms do instead is to consider the coordinates $f _ { i }$ as points in $\mathbb { R }$ and cluster them into two groups $C , { \overline { { C } } }$ by the $k$ -means clustering algorithm. Then we carry over the resulting clustering to the underlying data points, that is we choose

$$
\left\{ \begin{array}{l l} v _ {i} \in A & \text {i f} f _ {i} \in C \\ v _ {i} \in \overline {{A}} & \text {i f} f _ {i} \in \overline {{C}}. \end{array} \right.
$$

This is exactly the unnormalized spectral clustering algorithm for the case of $k = 2$ .

# 5.2 Approximating RatioCut for arbitrary $\boldsymbol { k }$

The relaxation of the RatioCut minimization problem in the case of a general value $k$ follows a similar principle as the one above. Given a partition of $V$ into $k$ sets $A _ { 1 } , \ldots , A _ { k }$ , we define $k$ indicator vectors $h _ { j } = ( h _ { 1 , j } , \dotsc , h _ { n , j } ) ^ { \prime }$ by

$$
h _ {i, j} = \left\{ \begin{array}{l l} 1 / \sqrt {| A _ {j} |} & \text {i f} v _ {i} \in A _ {j} \\ 0 & \text {o t h e r w i s e} \end{array} \right. \quad (i = 1, \dots , n; j = 1, \dots , k). \tag {5}
$$

Then we set the matrix $H \in \mathbb { R } ^ { n \times k }$ as the matrix containing those $k$ indicator vectors as columns. Observe that the columns in $H$ are orthonormal to each other, that is $H ^ { \prime } H = I$ . Similar to the calculations in the last section we can see that

$$
h _ {i} ^ {\prime} L h _ {i} = \frac {\mathrm {c u t} (A _ {i} , \overline {{A}} _ {i})}{| A _ {i} |}.
$$

Moreover, one can check that

$$
h _ {i} ^ {\prime} L h _ {i} = (H ^ {\prime} L H) _ {i i}.
$$

Combining those facts we get

$$
\mathrm {R a t i o C u t} (A _ {1}, \ldots , A _ {k}) = \sum_ {i = 1} ^ {k} h _ {i} ^ {\prime} L h _ {i} = \sum_ {i = 1} ^ {k} (H ^ {\prime} L H) _ {i i} = \mathrm {T r} (H ^ {\prime} L H),
$$

where $\mathrm { T r }$ denotes the trace of a matrix. So the problem of minimizing RatioCut $( A _ { 1 } , \ldots , A _ { k } )$ can be rewritten as

$$
\min  _ {A _ {1}, \dots , A _ {k}} \operatorname {T r} \left(H ^ {\prime} L H\right) \text {s u b j e c t} H ^ {\prime} H = I, H \text {a s d e f i n e d i n E q . (5)}.
$$

Similar to above we now relax the problem by allowing the entries of the matrix $H$ to take arbitrary real values. Then the relaxed problem becomes:

$$
\min  _ {H \in \mathbb {R} ^ {n \times k}} \operatorname {T r} \left(H ^ {\prime} L H\right) \text {s u b j e c t} I.
$$

This is the standard form of a trace minimization problem, and again a version of the Rayleigh-Ritz theorem (e.g., see Section 5.2.2.(6) of L¨utkepohl, 1997) tells us that the solution is given by choosing $H$ as the matrix which contains the first $k$ eigenvectors of $L$ as columns. We can see that the matrix $H$ is in fact the matrix $U$ used in the unnormalized spectral clustering algorithm as described in Section 4. Again we need to re-convert the real valued solution matrix to a discrete partition. As above, the standard way is to use the $k$ -means algorithms on the rows of $U$ . This leads to the general unnormalized spectral clustering algorithm as presented in Section 4.

# 5.3 Approximating Ncut

Techniques very similar to the ones used for RatioCut can be used to derive normalized spectral clustering as relaxation of minimizing Ncut. In the case $k = 2$ we define the cluster indicator vector $f$ by

$$
f _ {i} = \left\{ \begin{array}{l l} \sqrt {\frac {\operatorname {v o l} (\bar {A})}{\operatorname {v o l} A}} & \text {i f} v _ {i} \in A \\ - \sqrt {\frac {\operatorname {v o l} (A)}{\operatorname {v o l} (\bar {A})}} & \text {i f} v _ {i} \in \bar {A}. \end{array} \right. \tag {6}
$$

Similar to above one can check that $( D f ) ^ { \prime } \mathbb { 1 } = 0$ , $f ^ { \prime } D f = \operatorname { v o l } ( V )$ , and $f ^ { \prime } L f = \operatorname { v o l } ( V ) \operatorname { N c u t } ( A , { \overline { { A } } } )$ . Thus we can rewrite the problem of minimizing Ncut by the equivalent problem

$$
\min  _ {A} f ^ {\prime} L f \text {s u b j e c t t o} f \text {a s i n (6) ,} D f \perp \mathbb {1}, f ^ {\prime} D f = \operatorname {v o l} (V). \tag {7}
$$

Again we relax the problem by allowing $f$ to take arbitrary real values:

$$
\min  _ {f \in \mathbb {R} ^ {n}} f ^ {\prime} L f \text {s u b j e c t t o} D f \perp \mathbb {1}, f ^ {\prime} D f = \operatorname {v o l} (V). \tag {8}
$$

Now we substitute $g : = D ^ { 1 / 2 } f$ . After substitution, the problem is

$$
\min  _ {g \in \mathbb {R} ^ {n}} g ^ {\prime} D ^ {- 1 / 2} L D ^ {- 1 / 2} g \text {s u b j e c t t o} g \perp D ^ {1 / 2} \mathbb {1}, \| g \| ^ {2} = \operatorname {v o l} (V). \tag {9}
$$

Observe that $D ^ { - 1 / 2 } L D ^ { - 1 / 2 } = L _ { \mathrm { { s y m } } }$ , $D ^ { 1 / 2 } \mathbb { 1 }$ is the first eigenvector of $L _ { \mathrm { s y m } }$ , and $\operatorname { v o l } ( V )$ is a constant. Hence, Problem (9) is in the form of the standard Rayleigh-Ritz theorem, and its solution $g$ is given by the second eigenvector of $L _ { \mathrm { s y m } }$ . Re-substituting $f = D ^ { - 1 / 2 } g$ and using Proposition 3 we see that $f$ is the second eigenvector of $L _ { \mathrm { r w } }$ , or equivalently the generalized eigenvector of $L u = \lambda D u$ .

For the case of finding $k > 2$ clusters, we define the indicator vectors $h _ { j } = ( h _ { 1 , j } , . . . , h _ { n , j } ) ^ { \prime }$ by

$$
h _ {i, j} = \left\{ \begin{array}{l l} 1 / \sqrt {\operatorname {v o l} \left(A _ {j}\right)} & \text {i f} v _ {i} \in A _ {j} \\ 0 & \text {o t h e r w i s e} \end{array} \quad (i = 1, \dots , n; j = 1, \dots , k). \right. \tag {10}
$$

![](images/8c448c260412084006667f2e547ad8a4e635b9c371137b006d82c6b225407dc8.jpg)  
Figure 2: The cockroach graph from Guattery and Miller (1998).

Then we set the matrix $H$ as the matrix containing those $k$ indicator vectors as columns. Observe that $H ^ { \prime } H = I$ , $h _ { i } ^ { \prime } D h _ { i } = 1$ , and $h _ { i } ^ { \prime } L h _ { i } = \operatorname { c u t } ( A _ { i } , { \overline { { A } } } _ { i } ) / \operatorname { v o l } ( A _ { i } )$ . So we can write the problem of minimizing Ncut as

$$
\min  _ {A _ {1}, \dots , A _ {k}} \operatorname {T r} \left(H ^ {\prime} L H\right) \text {s u b j e c t t o} H ^ {\prime} D H = I, \mathrm {H} \text {a s i n (1 0)}.
$$

Relaxing the discreteness condition and substituting $T = D ^ { 1 / 2 } H$ we obtain the relaxed problem

$$
\min  _ {T \in \mathbb {R} ^ {n \times k}} \operatorname {T r} \left(T ^ {\prime} D ^ {- 1 / 2} L D ^ {- 1 / 2} T\right) \text {s u b j e c t}
$$

Again this is the standard trace minimization problem which is solved by the matrix $T$ which contains the first $k$ eigenvectors of $L _ { \mathrm { s y m } }$ as columns. Re-substituting $H = D ^ { - 1 / 2 } T$ and using Proposition 3 we see that the solution $H$ consists of the first $k$ eigenvectors of the matrix $L _ { \mathrm { r w } }$ , or the first $k$ generalized eigenvectors of $L u = \lambda D u$ . This yields the normalized spectral clustering algorithm according to Shi and Malik (2000).

# 5.4 Comments on the relaxation approach

There are several comments we should make about this derivation of spectral clustering. Most importantly, there is no guarantee whatsoever on the quality of the solution of the relaxed problem compared to the exact solution. That is, if $A _ { 1 } , \ldots , A _ { k }$ is the exact solution of minimizing RatioCut, and $B _ { 1 } , \ldots , B _ { k }$ is the solution constructed by unnormalized spectral clustering, then $\mathrm { R a t i o C u t } ( B _ { 1 } , \ldots , B _ { k } ) -$ RatioCut $( A _ { 1 } , \ldots , A _ { k } )$ can be arbitrary large. Several examples for this can be found in Guattery and Miller (1998). For instance, the authors consider a very simple class of graphs called “cockroach graphs”. Those graphs essentially look like a ladder, with a few rimes removed, see Figure 2. Obviously, the ideal RatioCut for $k \ = \ 2$ just cuts the ladder by a vertical cut such that $A = \left\{ v _ { 1 } , \ldots , v _ { k } , v _ { 2 k + 1 } , \ldots , v _ { 3 k } \right\}$ and $\overline { { A } } = \{ v _ { k + 1 } , \ldots , v _ { 2 k } , v _ { 3 k + 1 } , \ldots , v _ { 4 k } \}$ . This cut is perfectly balanced with $| A | = | A | = 2 k$ and $\operatorname { c u t } ( A , { \overline { { A } } } ) = 2$ . However, by studying the properties of the second eigenvector of the unnormalized graph Laplacian of cockroach graphs the authors prove that unnormalized spectral clustering always cuts horizontally through the ladder, constructing the sets $B = \{ v _ { 1 } , \ldots , v _ { 2 k } \}$ and $\overline { { B } } = \{ v _ { 2 k + 1 } , \ldots , v _ { 4 k } \}$ . This also results in a balanced cut, but now we cut $k$ edges instead of just 2. So $\mathrm { R a t i o C u t } ( A , { \overline { { A } } } ) = 2 / k$ , while $\mathrm { R a t i o C u t } ( B , { \overline { { B } } } ) = 1$ . This means that compared to the optimal cut, the RatioCut value obtained by spectral clustering is $k / 2$ times worse, that is a factor in the order of $n$ . Several other papers investigate the quality of the clustering constructed by spectral clustering, for example Spielman and Teng (1996) (for unnormalized spectral clustering) and Kannan, Vempala, and Vetta (2004) (for normalized spectral clustering). In general it is known that efficient algorithms to approximate balanced graph cuts up to a constant factor do not exist. To the contrary, this approximation problem can be NP hard itself (Bui and Jones, 1992).

Of course, the relaxation we discussed above is not unique. For example, a completely different relaxation which leads to a semi-definite program is derived in Bie and Cristianini (2006), and there might be many other useful relaxations. The reason why the spectral relaxation is so appealing is not that it leads to particularly good solutions. Its popularity is mainly due to the fact that it results in a standard linear algebra problem which is simple to solve.

# 6 Random walks point of view

Another line of argument to explain spectral clustering is based on random walks on the similarity graph. A random walk on a graph is a stochastic process which randomly jumps from vertex to vertex. We will see below that spectral clustering can be interpreted as trying to find a partition of the graph such that the random walk stays long within the same cluster and seldom jumps between clusters. Intuitively this makes sense, in particular together with the graph cut explanation of the last section: a balanced partition with a low cut will also have the property that the random walk does not have many opportunities to jump between clusters. For background reading on random walks in general we refer to Norris (1997) and Br´emaud (1999), and for random walks on graphs we recommend Aldous and Fill (in preparation) and Lov´asz (1993). Formally, the transition probability of jumping in one step from vertex $v _ { i }$ to vertex $v _ { j }$ is proportional to the edge weight $w _ { i j }$ and is given by $p _ { i j } : = w _ { i j } / d _ { i }$ . The transition matrix $P = ( p _ { i j } ) _ { i , j = }$ 1,...,n of the random walk is thus defined by

$$
P = D ^ {- 1} W.
$$

If the graph is connected and non-bipartite, then the random walk always possesses a unique stationary distribution $\pi = ( \pi _ { 1 } , \ldots , \pi _ { n } ) ^ { \prime }$ , where $\pi _ { i } = d _ { i } / \mathrm { v o l } ( V )$ . Obviously there is a tight relationship between $L _ { \mathrm { r w } }$ and $P$ , as $L _ { \mathrm { r w } } = I - P$ . As a consequence, $\lambda$ is an eigenvalue of $L _ { \mathrm { r w } }$ with eigenvector $u$ if and only if $1 - \lambda$ is an eigenvalue of $P$ with eigenvector $u$ . It is well known that many properties of a graph can be expressed in terms of the corresponding random walk transition matrix $P$ , see Lov´asz (1993) for an overview. From this point of view it does not come as a surprise that the largest eigenvectors of $P$ and the smallest eigenvectors of $L _ { \mathrm { r w } }$ can be used to describe cluster properties of the graph.

# Random walks and Ncut

A formal equivalence between Ncut and transition probabilities of the random walk has been observed in Meila and Shi (2001).

Proposition 5 (Ncut via transition probabilities) Let $G$ be connected and non bi-partite. Assume that we run the random walk $( X _ { t } ) _ { t \in \mathbb { N } }$ starting with $X _ { 0 }$ in the stationary distribution $\pi$ . For disjoint subsets $A , B \subset V$ , denote by $P ( B | A ) : = P ( X _ { 1 } \in B | X _ { 0 } \in A )$ . Then:

$$
\operatorname {N c u t} (A, \overline {{A}}) = P (\overline {{A}} | A) + P (A | \overline {{A}}).
$$

Proof. First of all observe that

$$
\begin{array}{l} P \left(X _ {0} \in A, X _ {1} \in B\right) = \sum_ {i \in A, j \in B} P \left(X _ {0} = i, X _ {1} = j\right) = \sum_ {i \in A, j \in B} \pi_ {i} p _ {i j} \\ = \sum_ {i \in A, j \in B} \frac {d _ {i}}{\operatorname {v o l} (V)} \frac {w _ {i j}}{d _ {i}} = \frac {1}{\operatorname {v o l} (V)} \sum_ {i \in A, j \in B} w _ {i j}. \\ \end{array}
$$

Using this we obtain

$$
\begin{array}{l} P (X _ {1} \in B | X _ {0} \in A) = \frac {P (X _ {0} \in A , X _ {1} \in B)}{P (X _ {0} \in A)} \\ = \left(\frac {1}{\operatorname {v o l} (V)} \sum_ {i \in A, j \in B} w _ {i j}\right) \left(\frac {\operatorname {v o l} (A)}{\operatorname {v o l} (V)}\right) ^ {- 1} = \frac {\sum_ {i \in A , j \in B} w _ {i j}}{\operatorname {v o l} (A)}. \\ \end{array}
$$

Now the proposition follows directly with the definition of Ncut.

![](images/4fcc0185417ce5728fdf2f63b6a0ea731d956f88006ce37b524ef5f6e5ec9303.jpg)

This proposition leads to a nice interpretation of Ncut, and hence of normalized spectral clustering. It tells us that when minimizing Ncut, we actually look for a cut through the graph such that a random walk seldom transitions from $A$ to $A$ and vice versa.

# The commute distance

A second connection between random walks and graph Laplacians can be made via the commute distance on the graph. The commute distance (also called resistance distance) $c _ { i j }$ between two vertices $v _ { i }$ and $v _ { j }$ is the expected time it takes the random walk to travel from vertex $v _ { i }$ to vertex $v _ { j }$ and back (Lov´asz, 1993; Aldous and Fill, in preparation). The commute distance has several nice properties which make it particularly appealing for machine learning. As opposed to the shortest path distance on a graph, the commute distance between two vertices decreases if there are many different short ways to get from vertex $v _ { i }$ to vertex $v _ { j }$ . So instead of just looking for the one shortest path, the commute distance looks at the set of short paths. Points which are connected by a short path in the graph and lie in the same high-density region of the graph are considered closer to each other than points which are connected by a short path but lie in different high-density regions of the graph. In this sense, the commute distance seems particularly well-suited to be used for clustering purposes.

Remarkably, the commute distance on a graph can be computed with the help of the generalized inverse (also called pseudo-inverse or Moore-Penrose inverse) $L ^ { \dagger }$ of the graph Laplacian $L$ . In the following we denote $e _ { i } = ( 0 , \ldots 0 , 1 , 0 , \ldots , 0 ) ^ { \prime }$ as the $i$ -th unit vector. To define the generalized inverse of $L$ , recall that by Proposition 1 the matrix $L$ can be decomposed as $L = U \Lambda U ^ { \prime }$ where $U$ is the matrix containing all eigenvectors as columns and $\Lambda$ the diagonal matrix with the eigenvalues $\lambda _ { 1 } , \ldots , \lambda _ { n }$ on the diagonal. As at least one of the eigenvalues is $0$ , the matrix $L$ is not invertible. Instead, we define its generalized inverse as $L ^ { \dagger } : = U \Lambda ^ { \dagger } U ^ { \prime }$ where the matrix $\Lambda ^ { \dagger }$ is the diagonal matrix with diagonal entries $1 / \lambda _ { i }$ if $\lambda _ { i } \neq 0$ and 0 if $\lambda _ { i } = 0$ . The entries of $L ^ { \dagger }$ can be computed as $\begin{array} { r } { l _ { i j } ^ { \dag } = \sum _ { k = 2 } ^ { n } \frac { 1 } { \lambda _ { k } } u _ { i k } u _ { j k } } \end{array}$ . The matrix $L ^ { \dagger }$ is positive semi-definite and symmetric. For further properties of $L ^ { \dagger }$ see Gutman and Xiao (2004).

Proposition 6 (Commute distance) Let $G = ( V , E )$ a connected, undirected graph. Denote by $c _ { i j }$ the commute distance between vertex $v _ { i }$ and vertex $v _ { j }$ , and by $L ^ { \dagger } = ( l _ { i j } ^ { \dagger } ) _ { i , j = 1 , \dots , n }$ the generalized inverse of $L$ . Then we have:

$$
c _ {i j} = \mathrm {v o l} (V) (l _ {i i} ^ {\dagger} - 2 l _ {i j} ^ {\dagger} + l _ {j j} ^ {\dagger}) = \mathrm {v o l} (V) (e _ {i} - e _ {j}) ^ {\prime} L ^ {\dagger} (e _ {i} - e _ {j}).
$$

This result has been published by Klein and Randic (1993), where it has been proved by methods of electrical network theory. For a proof using first step analysis for random walks see Fouss, Pirotte, Renders, and Saerens (2007). There also exist other ways to express the commute distance with the help of graph Laplacians. For example a method in terms of eigenvectors of the normalized Laplacian $L _ { \mathrm { s y m } }$ can be found as Corollary 3.2 in Lov´asz (1993), and a method computing the commute distance with the help of determinants of certain sub-matrices of $L$ can be found in Bapat, Gutman, and Xiao (2003).

Proposition 6 has an important consequence. It shows that $\sqrt { c _ { i j } }$ can be considered as a Euclidean distance function on the vertices of the graph. This means that we can construct an embedding which

maps the vertices $v _ { i }$ of the graph on points $z _ { i } \in \mathbb { R } ^ { n }$ such that the Euclidean distances between the points $z _ { i }$ coincide with the commute distances on the graph. This works as follows. As the matrix $L ^ { \dagger }$ is positive semi-definite and symmetric, it induces an inner product on $\mathbb { R } ^ { n }$ (or to be more formal, it induces an inner product on the subspace of $\mathbb { R } ^ { n }$ which is perpendicular to the vector $\mathbb { 1 }$ ). Now choose $z _ { i }$ as the point in $\mathbb { R } ^ { n }$ corresponding to the $i$ -th row of the matrix $U ( \Lambda ^ { \dagger } ) ^ { 1 / 2 }$ . Then, by Proposition 6 and by the construction of $L ^ { \dagger }$ we have that $\langle z _ { i } , z _ { j } \rangle = e _ { i } ^ { \prime } L ^ { \dagger } e _ { j }$ and $c _ { i j } = \mathrm { v o l } ( V ) | | z _ { i } - z _ { j } | | ^ { 2 }$ .

The embedding used in unnormalized spectral clustering is related to the commute time embedding, but not identical. In spectral clustering, we map the vertices of the graph on the rows $y _ { i }$ of the matrix $U$ , while the commute time embedding maps the vertices on the rows $z _ { i }$ of the matrix $( \Lambda ^ { \dagger } ) ^ { 1 / 2 } U$ . That is, compared to the entries of $y _ { i }$ , the entries of $z _ { i }$ are additionally scaled by the inverse eigenvalues of $L$ . Moreover, in spectral clustering we only take the first $k$ columns of the matrix, while the commute time embedding takes all columns. Several authors now try to justify why $y _ { i }$ and $z _ { i }$ are not so different after all and state a bit hand-waiving that the fact that spectral clustering constructs clusters based on the Euclidean distances between the $y _ { i }$ can be interpreted as building clusters of the vertices in the graph based on the commute distance. However, note that both approaches can differ considerably. For example, in the optimal case where the graph consists of $k$ disconnected components, the first $k$ eigenvalues of $L$ are 0 according to Proposition 2, and the first $k$ columns of $U$ consist of the cluster indicator vectors. However, the first $k$ columns of the matrix $( \Lambda ^ { \dagger } ) ^ { 1 / 2 } U$ consist of zeros only, as the first $k$ diagonal elements of $\Lambda ^ { \dagger }$ are 0. In this case, the information contained in the first $k$ columns of $U$ is completely ignored in the matrix $( \Lambda ^ { \dagger } ) ^ { 1 / 2 } U$ , and all the non-zero elements of the matrix $( \Lambda ^ { \dagger } ) ^ { 1 / 2 } U$ which can be found in columns $k + 1$ to $n$ are not taken into account in spectral clustering, which discards all those columns. On the other hand, those problems do not occur if the underlying graph is connected. In this case, the only eigenvector with eigenvalue 0 is the constant one vector, which can be ignored in both cases. The eigenvectors corresponding to small eigenvalues $\lambda _ { i }$ of $L$ are then stressed in the matrix $( \Lambda ^ { \dagger } ) ^ { 1 / 2 } U$ as they are multiplied by $\lambda _ { i } ^ { \dagger } = 1 / \lambda _ { i }$ . In such a situation, it might be true that the commute time embedding and the spectral embedding do similar things.

All in all, it seems that the commute time distance can be a helpful intuition, but without making further assumptions there is only a rather loose relation between spectral clustering and the commute distance. It might be possible that those relations can be tightened, for example if the similarity function is strictly positive definite. However, we have not yet seen a precise mathematical statement about this.

# 7 Perturbation theory point of view

Perturbation theory studies the question of how eigenvalues and eigenvectors of a matrix $A$ change if we add a small perturbation $H$ , that is we consider the perturbed matrix ${ \tilde { A } } : = A + H$ . Most perturbation theorems state that a certain distance between eigenvalues or eigenvectors of $A$ and $\tilde { A }$ is bounded by a constant times a norm of $H$ . The constant usually depends on which eigenvalue we are looking at, and how far this eigenvalue is separated from the rest of the spectrum (for a formal statement see below). The justification of spectral clustering is then the following: Let us first consider the “ideal case” where the between-cluster similarity is exactly 0. We have seen in Section 3 that then the first $k$ eigenvectors of $L$ or $L _ { \mathrm { r w } }$ are the indicator vectors of the clusters. In this case, the points $y _ { i } \in \mathbb { R } ^ { k }$ constructed in the spectral clustering algorithms have the form $( 0 , \ldots , 0 , 1 , 0 , \ldots 0 ) ^ { \prime }$ where the position of the 1 indicates the connected component this point belongs to. In particular, all $y _ { i }$ belonging to the same connected component coincide. The $k$ -means algorithm will trivially find the correct partition by placing a center point on each of the points $( 0 , \ldots , 0 , 1 , 0 , \ldots 0 ) ^ { \prime } \in \mathbb { R } ^ { k }$ . In a “nearly ideal case” where we still have distinct clusters, but the between-cluster similarity is not exactly 0, we consider the Laplacian matrices to be perturbed versions of the ones of the ideal case. Perturbation theory then tells us that the eigenvectors will be very close to the ideal indicator vectors. The points $y _ { i }$ might not

completely coincide with $( 0 , \ldots , 0 , 1 , 0 , \ldots 0 ) ^ { \prime }$ , but do so up to some small error term. Hence, if the perturbations are not too large, then $k$ -means algorithm will still separate the groups from each other.

# 7.1 The formal perturbation argument

The formal basis for the perturbation approach to spectral clustering is the Davis-Kahan theorem from matrix perturbation theory. This theorem bounds the difference between eigenspaces of symmetric matrices under perturbations. We state those results for completeness, but for background reading we refer to Section V of Stewart and Sun (1990) and Section VII.3 of Bhatia (1997). In perturbation theory, distances between subspaces are usually measured using “canonical angles” (also called “principal angles”). To define principal angles, let $\nu _ { 1 }$ and $\nu _ { 2 }$ be two $p$ -dimensional subspaces of $\mathbb { R } ^ { d }$ , and $V _ { 1 }$ and $V _ { 2 }$ two matrices such that their columns form orthonormal systems for $\nu _ { 1 }$ and $\nu _ { 2 }$ , respectively. Then the cosines $\cos \Theta _ { i }$ of the principal angles $\Theta _ { i }$ are the singular values of $V _ { 1 } ^ { \prime } V _ { 2 }$ . For $p = 1$ , the so defined canonical angles coincide with the normal definition of an angle. Canonical angles can also be defined if $\nu _ { 1 }$ and $\nu _ { 2 }$ do not have the same dimension, see Section V of Stewart and Sun (1990), Section VII.3 of Bhatia (1997), or Section 12.4.3 of Golub and Van Loan (1996). The matrix $\sin \Theta ( \gamma _ { 1 } , \gamma _ { 2 } )$ will denote the diagonal matrix with the sine of the canonical angles on the diagonal.

Theorem 7 (Davis-Kahan) Let $A , H \in \mathbb { R } ^ { n \times n }$ be symmetric matrices, and let $\| \cdot \|$ be the Frobenius norm or the two-norm for matrices, respectively. Consider ${ \tilde { A } } : = A + H$ as a perturbed version of $A$ . Let $S _ { 1 } \subset \mathbb { R }$ be an interval. Denote by $\sigma _ { S _ { 1 } } ( A )$ the set of eigenvalues of $A$ which are contained in $S _ { 1 }$ , and by $V _ { 1 }$ the eigenspace corresponding to all those eigenvalues (more formally, $V _ { 1 }$ is the image of the spectral projection induced by $\sigma _ { S _ { 1 } } ( A )$ ). Denote by $\sigma _ { S _ { 1 } } ( \tilde { A } )$ and $\tilde { V _ { 1 } }$ the analogous quantities for $\tilde { A }$ . Define the distance between $S _ { 1 }$ and the spectrum of $A$ outside of $S _ { 1 }$ as

$$
\delta = \min  \left\{\left| \lambda - s \right|; \lambda e i g e n v a l u e o f A, \lambda \notin S _ {1}, s \in S _ {1} \right\}.
$$

Then the distance $d ( V _ { 1 } , \tilde { V } _ { 1 } ) : = \parallel \sin \Theta ( V _ { 1 } , \tilde { V } _ { 1 } ) \parallel$ between the two subspaces $V _ { 1 }$ and $\tilde { V _ { 1 } }$ is bounded by

$$
d (V _ {1}, \tilde {V} _ {1}) \leq \frac {\| H \|}{\delta}.
$$

For a discussion and proofs of this theorem see for example Section V.3 of Stewart and Sun (1990). Let us try to decrypt this theorem, for simplicity in the case of the unnormalized Laplacian (for the normalized Laplacian it works analogously). The matrix $A$ will correspond to the graph Laplacian $L$ in the ideal case where the graph has $k$ connected components. The matrix $\ddot { A }$ corresponds to a perturbed case, where due to noise the $k$ components in the graph are no longer completely disconnected, but they are only connected by few edges with low weight. We denote the corresponding graph Laplacian of this case by $\ddot { L }$ . For spectral clustering we need to consider the first $k$ eigenvalues and eigenvectors of $\ddot { L }$ . Denote the eigenvalues of $L$ by $\lambda _ { 1 } , \ldots \lambda _ { n }$ and the ones of the perturbed Laplacian $\tilde { L }$ by $\tilde { \lambda } _ { 1 } , \ldots , \tilde { \lambda } _ { n }$ . Choosing the interval $S _ { 1 }$ is now the crucial point. We want to choose it such that both the first $k$ eigenvalues of $\tilde { L }$ and the first $k$ eigenvalues of $L$ are contained in $S _ { 1 }$ . This is easier the smaller the perturbation $H = L - \ddot { L }$ and the larger the eigengap $| \lambda _ { k } - \lambda _ { k + 1 } |$ is. If we manage to find such a set, then the Davis-Kahan theorem tells us that the eigenspaces corresponding to the first $k$ eigenvalues of the ideal matrix $L$ and the first $k$ eigenvalues of the perturbed matrix $\tilde { L }$ are very close to each other, that is their distance is bounded by $\| H \| / \delta$ . Then, as the eigenvectors in the ideal case are piecewise constant on the connected components, the same will approximately be true in the perturbed case. How good “approximately” is depends on the norm of the perturbation $\lVert H \rVert$ and the distance $\delta$ between $S _ { 1 }$ and the $( k + 1 )$ st eigenvector of $L$ . If the set $S _ { 1 }$ has been chosen as the interval $[ 0 , \lambda _ { k } ]$ , then $\delta$ coincides with the spectral gap $| \lambda _ { k + 1 } - \lambda _ { k } |$ . We can see from the theorem that the larger this eigengap is, the closer the eigenvectors of the ideal case and the perturbed case are, and hence the better spectral clustering works. Below we will see that the size of the eigengap can also be used in a

different context as a quality criterion for spectral clustering, namely when choosing the number $k$ of clusters to construct.

If the perturbation $H$ is too large or the eigengap is too small, we might not find a set $S _ { 1 }$ such that both the first $k$ eigenvalues of $L$ and $\tilde { L }$ are contained in $S _ { 1 }$ . In this case, we need to make a compromise by choosing the set $S _ { 1 }$ to contain the first $k$ eigenvalues of $L$ , but maybe a few more or less eigenvalues of $\tilde { L }$ . The statement of the theorem then becomes weaker in the sense that either we do not compare the eigenspaces corresponding to the first $k$ eigenvectors of $L$ and $\tilde { L }$ , but the eigenspaces corresponding to the first $k$ eigenvectors of $L$ and the first $\tilde { k }$ eigenvectors of $\tilde { L }$ (where $\tilde { k }$ is the number of eigenvalues of $\ddot { L }$ contained in $S _ { 1 }$ ). Or, it can happen that $\delta$ becomes so small that the bound on the distance between $d ( V _ { 1 } , \tilde { V } _ { 1 } )$ blows up so much that it becomes useless.

# 7.2 Comments about the perturbation approach

A bit of caution is needed when using perturbation theory arguments to justify clustering algorithms based on eigenvectors of matrices. In general, any block diagonal symmetric matrix has the property that there exists a basis of eigenvectors which are zero outside the individual blocks and real-valued within the blocks. For example, based on this argument several authors use the eigenvectors of the similarity matrix $S$ or adjacency matrix $W$ to discover clusters. However, being block diagonal in the ideal case of completely separated clusters can be considered as a necessary condition for a successful use of eigenvectors, but not a sufficient one. At least two more properties should be satisfied:

First, we need to make sure that the order of the eigenvalues and eigenvectors is meaningful. In case of the Laplacians this is always true, as we know that any connected component possesses exactly one eigenvector which has eigenvalue 0. Hence, if the graph has $k$ connected components and we take the first $k$ eigenvectors of the Laplacian, then we know that we have exactly one eigenvector per component. However, this might not be the case for other matrices such as $S$ or $W$ . For example, it could be the case that the two largest eigenvalues of a block diagonal similarity matrix $S$ come from the same block. In such a situation, if we take the first $k$ eigenvectors of $S$ , some blocks will be represented several times, while there are other blocks which we will miss completely (unless we take certain precautions). This is the reason why using the eigenvectors of $S$ or $W$ for clustering should be discouraged.

The second property is that in the ideal case, the entries of the eigenvectors on the components should be “safely bounded away” from 0. Assume that an eigenvector on the first connected component has an entry $u _ { 1 , i } > 0$ at position $_ i$ . In the ideal case, the fact that this entry is non-zero indicates that the corresponding point $i$ belongs to the first cluster. The other way round, if a point $j$ does not belong to cluster 1, then in the ideal case it should be the case that $u _ { 1 , j } = 0$ . Now consider the same situation, but with perturbed data. The perturbed eigenvector $\ddot { u }$ will usually not have any non-zero component any more; but if the noise is not too large, then perturbation theory tells us that the entries $\ddot { u } _ { 1 , i }$ and $\bar { u } _ { 1 , j }$ are still “close” to their original values $u _ { 1 , i }$ and $u _ { 1 , j }$ . So both entries $\tilde { u } _ { 1 , i }$ and $\bar { u } _ { 1 , j }$ will take some small values, say $\varepsilon _ { 1 }$ and $\varepsilon _ { 2 }$ . In practice, if those values are very small it is unclear how we should interpret this situation. Either we believe that small entries in $\ddot { u }$ indicate that the points do not belong to the first cluster (which then misclassifies the first data point $i$ ), or we think that the entries already indicate class membership and classify both points to the first cluster (which misclassifies point $\jmath$ ).

For both matrices $L$ and $L _ { \mathrm { r w } }$ , the eigenvectors in the ideal situation are indicator vectors, so the second problem described above cannot occur. However, this is not true for the matrix $L _ { \mathrm { s y m } }$ , which is used in the normalized spectral clustering algorithm of Ng et al. (2002). Even in the ideal case, the eigenvectors of this matrix are given as $D ^ { 1 / 2 } \mathbb { 1 } _ { A _ { i } }$ . If the degrees of the vertices differ a lot, and in particular if there are vertices which have a very low degree, the corresponding entries in the eigenvectors are very small. To counteract the problem described above, the row-normalization step in the algorithm of Ng et al. (2002) comes into play. In the ideal case, the matrix $U$ in the algorithm has exactly one

non-zero entry per row. After row-normalization, the matrix $T$ in the algorithm of Ng et al. (2002) then consists of the cluster indicator vectors. Note however, that this might not always work out correctly in practice. Assume that we have $\tilde { u } _ { i , 1 } = \varepsilon _ { 1 }$ and $\tilde { u } _ { i , 2 } = \varepsilon _ { 2 }$ . If we now normalize the $_ i$ -th row of $U$ , both $\varepsilon _ { 1 }$ and $\varepsilon _ { 2 }$ will be multiplied by the factor of $1 / \sqrt { \varepsilon _ { 1 } ^ { 2 } + \varepsilon _ { 2 } ^ { 2 } }$ and become rather large. We now run into a similar problem as described above: both points are likely to be classified into the same cluster, even though they belong to different clusters. This argument shows that spectral clustering using the matrix $L _ { \mathrm { s y m } }$ can be problematic if the eigenvectors contain particularly small entries. On the other hand, note that such small entries in the eigenvectors only occur if some of the vertices have a particularly low degrees (as the eigenvectors of $L _ { \mathrm { s y m } }$ are given by $D ^ { 1 / 2 } \mathbb { 1 } _ { A _ { i } }$ ). One could argue that in such a case, the data point should be considered an outlier anyway, and then it does not really matter in which cluster the point will end up.

To summarize, the conclusion is that both unnormalized spectral clustering and normalized spectral clustering with $L _ { \mathrm { r w } }$ are well justified by the perturbation theory approach. Normalized spectral clustering with $L _ { \mathrm { s y m } }$ can also be justified by perturbation theory, but it should be treated with more care if the graph contains vertices with very low degrees.

# 8 Practical details

In this section we will briefly discuss some of the issues which come up when actually implementing spectral clustering. There are several choices to be made and parameters to be set. However, the discussion in this section is mainly meant to raise awareness about the general problems which an occur. For thorough studies on the behavior of spectral clustering for various real world tasks we refer to the literature.

# 8.1 Constructing the similarity graph

Constructing the similarity graph for spectral clustering is not a trivial task, and little is known on theoretical implications of the various constructions.

# The similarity function itself

Before we can even think about constructing a similarity graph, we need to define a similarity function on the data. As we are going to construct a neighborhood graph later on, we need to make sure that the local neighborhoods induced by this similarity function are “meaningful”. This means that we need to be sure that points which are considered to be “very similar” by the similarity function are also closely related in the application the data comes from. For example, when constructing a similarity function between text documents it makes sense to check whether documents with a high similarity score indeed belong to the same text category. The global “long-range” behavior of the similarity function is not so important for spectral clustering — it does not really matter whether two data points have similarity score 0.01 or 0.001, say, as we will not connect those two points in the similarity graph anyway. In the common case where the data points live in the Euclidean space $\mathbb { R } ^ { d }$ , a reasonable default candidate is the Gaussian similarity function $s ( x _ { i } , x _ { j } ) = \exp ( - \| x _ { i } - x _ { j } \| ^ { 2 } / ( 2 \sigma ^ { 2 } ) )$ (but of course we need to choose the parameter $\sigma$ here, see below). Ultimately, the choice of the similarity function depends on the domain the data comes from, and no general advice can be given.

# Which type of similarity graph

The next choice one has to make concerns the type of the graph one wants to use, such as the $k$ -nearest neighbor or the $\varepsilon$ -neighborhood graph. Let us illustrate the behavior of the different graphs using the toy example presented in Figure 3. As underlying distribution we choose a distribution on $\mathbb { R } ^ { 2 }$ with

![](images/d7fbf2bf1a2ce1f3ed54490105425585dbfc3624d6a5504f688d2b64623098d8.jpg)  
kNN graph, $\mathsf { k } = 5$

![](images/10647c392805247cc11608835aea1c9366d91734a4f7eb2692c641501787c2e2.jpg)  
Mutual kNN graph, $\mathtt { k } = 5$

![](images/6b6443d788c525af12c6d684ca9032b791adb8610b149c9cc63f079a81d01d6e.jpg)

![](images/42bb62ae1fc1d40adeb6dd5a25108467930d93408129816a008690d7c5f3c36b.jpg)  
Figure 3: Different similarity graphs, see text for details.

three clusters: two “moons” and a Gaussian. The density of the bottom moon is chosen to be larger than the one of the top moon. The upper left panel in Figure 3 shows a sample drawn from this distribution. The next three panels show the different similarity graphs on this sample.

In the $\varepsilon$ -neighborhood graph, we can see that it is difficult to choose a useful parameter $\varepsilon$ . With $\varepsilon = 0 . 3$ as in the figure, the points on the middle moon are already very tightly connected, while the points in the Gaussian are barely connected. This problem always occurs if we have data “on different scales”, that is the distances between data points are different in different regions of the space.

The $k$ -nearest neighbor graph, on the other hand, can connect points “on different scales”. We can see that points in the low-density Gaussian are connected with points in the high-density moon. This is a general property of $k$ -nearest neighbor graphs which can be very useful. We can also see that the $k$ -nearest neighbor graph can break into several disconnected components if there are high density regions which are reasonably far away from each other. This is the case for the two moons in this example.

The mutual $k$ -nearest neighbor graph has the property that it tends to connect points within regions of constant density, but does not connect regions of different densities with each other. So the mutual $k$ -nearest neighbor graph can be considered as being “in between” the $\varepsilon$ -neighborhood graph and the $k$ -nearest neighbor graph. It is able to act on different scales, but does not mix those scales with each other. Hence, the mutual $k$ -nearest neighbor graph seems particularly well-suited if we want to detect clusters of different densities.

The fully connected graph is very often used in connection with the Gaussian similarity function $s ( x _ { i } , x _ { j } ) = \exp ( - \| x _ { i } - x _ { j } \| ^ { 2 } / ( 2 \sigma ^ { 2 } ) )$ . Here the parameter $\sigma$ plays a similar role as the parameter $\varepsilon$ in the $\varepsilon$ -neighborhood graph. Points in local neighborhoods are connected with relatively high weights, while edges between far away points have positive, but negligible weights. However, the resulting

similarity matrix is not a sparse matrix.

As a general recommendation we suggest to work with the $k$ -nearest neighbor graph as the first choice. It is simple to work with, results in a sparse adjacency matrix $W$ , and in our experience is less vulnerable to unsuitable choices of parameters than the other graphs.

# The parameters of the similarity graph

Once one has decided for the type of the similarity graph, one has to choose its connectivity parameter $k$ or $\varepsilon$ , respectively. Unfortunately, barely any theoretical results are known to guide us in this task. In general, if the similarity graph contains more connected components than the number of clusters we ask the algorithm to detect, then spectral clustering will trivially return connected components as clusters. Unless one is perfectly sure that those connected components are the correct clusters, one should make sure that the similarity graph is connected, or only consists of “few” connected components and very few or no isolated vertices. There are many theoretical results on how connectivity of random graphs can be achieved, but all those results only hold in the limit for the sample size $n \longrightarrow \infty$ . For example, it is known that for $n$ data points drawn i.i.d. from some underlying density with a connected support in $\mathbb { R } ^ { d }$ , the $k$ -nearest neighbor graph and the mutual $k$ -nearest neighbor graph will be connected if we choose $k$ on the order of $\log ( n )$ (e.g., Brito, Chavez, Quiroz, and Yukich, 1997). Similar arguments show that the parameter $\varepsilon$ in the $\varepsilon$ -neighborhood graph has to be chosen as $( \log ( n ) / n ) ^ { d }$ to guarantee connectivity in the limit (Penrose, 1999). While being of theoretical interest, all those results do not really help us for choosing $k$ on a finite sample.

Now let us give some rules of thumb. When working with the $k$ -nearest neighbor graph, then the connectivity parameter should be chosen such that the resulting graph is connected, or at least has significantly fewer connected components than clusters we want to detect. For small or medium-sized graphs this can be tried out ”by foot”. For very large graphs, a first approximation could be to choose $k$ in the order of $\log ( n )$ , as suggested by the asymptotic connectivity results.

For the mutual $k$ -nearest neighbor graph, we have to admit that we are a bit lost for rules of thumb. The advantage of the mutual $k$ -nearest neighbor graph compared to the standard $k$ -nearest neighbor graph is that it tends not to connect areas of different density. While this can be good if there are clear clusters induced by separate high-density areas, this can hurt in less obvious situations as disconnected parts in the graph will always be chosen to be clusters by spectral clustering. Very generally, one can observe that the mutual $k$ -nearest neighbor graph has much fewer edges than the standard $k$ -nearest neighbor graph for the same parameter $k$ . This suggests to choose $k$ significantly larger for the mutual $k$ -nearest neighbor graph than one would do for the standard $k$ -nearest neighbor graph. However, to take advantage of the property that the mutual $k$ -nearest neighbor graph does not connect regions of different density, it would be necessary to allow for several “meaningful” disconnected parts of the graph. Unfortunately, we do not know of any general heuristic to choose the parameter $k$ such that this can be achieved.

For the $\varepsilon$ -neighborhood graph, we suggest to choose $\varepsilon$ such that the resulting graph is safely connected. To determine the smallest value of $\varepsilon$ where the graph is connected is very simple: one has to choose $\varepsilon$ as the length of the longest edge in a minimal spanning tree of the fully connected graph on the data points. The latter can be determined easily by any minimal spanning tree algorithm. However, note that when the data contains outliers this heuristic will choose $\varepsilon$ so large that even the outliers are connected to the rest of the data. A similar effect happens when the data contains several tight clusters which are very far apart from each other. In both cases, $\varepsilon$ will be chosen too large to reflect the scale of the most important part of the data.

Finally, if one uses a fully connected graph together with a similarity function which can be scaled

itself, for example the Gaussian similarity function, then the scale of the similarity function should be chosen such that the resulting graph has similar properties as a corresponding $k$ -nearest neighbor or $\varepsilon$ -neighborhood graph would have. One needs to make sure that for most data points the set of neighbors with a similarity significantly larger than 0 is “not too small and not too large”. In particular, for the Gaussian similarity function several rules of thumb are frequently used. For example, one can choose $\sigma$ in the order of the mean distance of a point to its $k$ -th nearest neighbor, where $k$ is chosen similarly as above (e.g., $k \sim \log ( n ) + 1 \ ;$ ). Another way is to determine $\varepsilon$ by the minimal spanning tree heuristic described above, and then choose $\sigma = \varepsilon$ . But note that all those rules of thumb are very ad-hoc, and depending on the given data at hand and its distribution of inter-point distances they might not work at all.

In general, experience shows that spectral clustering can be quite sensitive to changes in the similarity graph and to the choice of its parameters. Unfortunately, to our knowledge there has been no systematic study which investigates the effects of the similarity graph and its parameters on clustering and comes up with well-justified rules of thumb. None of the recommendations above is based on a firm theoretic ground. Finding rules which have a theoretical justification should be considered an interesting and important topic for future research.

# 8.2 Computing the eigenvectors

To implement spectral clustering in practice one has to compute the first $k$ eigenvectors of a potentially large graph Laplace matrix. Luckily, if we use the $k$ -nearest neighbor graph or the $\varepsilon$ -neighborhood graph, then all those matrices are sparse. Efficient methods exist to compute the first eigenvectors of sparse matrices, the most popular ones being the power method or Krylov subspace methods such as the Lanczos method (Golub and Van Loan, 1996). The speed of convergence of those algorithms depends on the size of the eigengap (also called spectral gap) $\gamma _ { k } = | \lambda _ { k } - \lambda _ { k + 1 } |$ . The larger this eigengap is, the faster the algorithms computing the first $k$ eigenvectors converge.

Note that a general problem occurs if one of the eigenvalues under consideration has multiplicity larger than one. For example, in the ideal situation of $k$ disconnected clusters, the eigenvalue 0 has multiplicity $k$ . As we have seen, in this case the eigenspace is spanned by the $k$ cluster indicator vectors. But unfortunately, the vectors computed by the numerical eigensolvers do not necessarily converge to those particular vectors. Instead they just converge to some orthonormal basis of the eigenspace, and it usually depends on implementation details to which basis exactly the algorithm converges. But this have the form is not so bad after all. Note that all vectors in the space spanned by the cluster indicator vectors $\textstyle u = \sum _ { i = 1 } ^ { k } a _ { i } \mathbb { 1 } _ { A _ { i } }$ for some coefficients $a _ { i }$ , that is, they are piecewise constant on the $\mathbb { 1 } _ { A _ { i } }$ clusters. So the vectors returned by the eigensolvers still encode the information about the clusters, which can then be used by the $k$ -means algorithm to reconstruct the clusters.

# 8.3 The number of clusters

Choosing the number $k$ of clusters is a general problem for all clustering algorithms, and a variety of more or less successful methods have been devised for this problem. In model-based clustering settings there exist well-justified criteria to choose the number of clusters from the data. Those criteria are usually based on the log-likelihood of the data, which can then be treated in a frequentist or Bayesian way, for examples see Fraley and Raftery (2002). In settings where no or few assumptions on the underlying model are made, a large variety of different indices can be used to pick the number of clusters. Examples range from ad-hoc measures such as the ratio of within-cluster and between-cluster similarities, over information-theoretic criteria (Still and Bialek, 2004), the gap statistic (Tibshirani, Walther, and Hastie, 2001), to stability approaches (Ben-Hur, Elisseeff, and Guyon, 2002; Lange, Roth,

![](images/0e57bd50809e16eba445c2f0e169d7c86576112986d656dcf263f6407edcce3b.jpg)  
Histogram of the sample 10 5 0 2 4 6 8 10 Eigenvalues   
Histogram of the sample 10 5 0 0 2 4 6 8 10 Eigenvalues   
Histogram of the sample 6 4 2 0 0 2 4 6 8 10 Eigenvalues

![](images/45c045871601e33b5ac48c74db3f092484f2bcd2ee3b0968353e2586f7f0ef8c.jpg)

![](images/3c6f4ddbf788ad476e22cba3088fcd77bf08de4776ab8ba27d8cf9b4fb044a3b.jpg)  
Figure 4: Three data sets, and the smallest 10 eigenvalues of $L _ { \mathrm { r w } }$ . See text for more details.

Braun, and Buhmann, 2004; Ben-David, von Luxburg, and P´al, 2006). Of course all those methods can also be used for spectral clustering. Additionally, one tool which is particularly designed for spectral clustering is the eigengap heuristic, which can be used for all three graph Laplacians. Here the goal is to choose the number $k$ such that all eigenvalues $\lambda _ { 1 } , \ldots , \lambda _ { k }$ are very small, but $\lambda _ { k + 1 }$ is relatively large. There are several justifications for this procedure. The first one is based on perturbation theory, where we observe that in the ideal case of $k$ completely disconnected clusters, the eigenvalue 0 has multiplicity $k$ , and then there is a gap to the $( k + 1 ) \mathrm { t h }$ eigenvalue $\lambda _ { k + 1 } > 0$ . Other explanations can be given by spectral graph theory. Here, many geometric invariants of the graph can be expressed or bounded with the help of the first eigenvalues of the graph Laplacian. In particular, the sizes of cuts are closely related to the size of the first eigenvalues. For more details on this topic we refer to Bolla (1991), Mohar (1997) and Chung (1997).

We would like to illustrate the eigengap heuristic on our toy example introduced in Section 4. For this purpose we consider similar data sets as in Section 4, but to vary the difficulty of clustering we consider the Gaussians with increasing variance. The first row of Figure 4 shows the histograms of the three samples. We construct the 10-nearest neighbor graph as described in Section 4, and plot the eigenvalues of the normalized Laplacian $L _ { \mathrm { r w } }$ on the different samples (the results for the unnormalized Laplacian are similar). The first data set consists of four well separated clusters, and we can see that the first 4 eigenvalues are approximately 0. Then there is a gap between the 4th and 5th eigenvalue, that is $| \lambda _ { 5 } - \lambda _ { 4 } |$ is relatively large. According to the eigengap heuristic, this gap indicates that the data set contains 4 clusters. The same behavior can also be observed for the results of the fully connected graph (already plotted in Figure 1). So we can see that the heuristic works well if the clusters in the data are very well pronounced. However, the more noisy or overlapping the clusters are, the less effective is this heuristic. We can see that for the second data set where the clusters are more “blurry”, there is still a gap between the 4th and 5th eigenvalue, but it is not as clear to detect as in the case before. Finally, in the last data set, there is no well-defined gap, the differences between all eigenvalues are approximately the same. But on the other hand, the clusters in this data set overlap so much that many non-parametric algorithms will have difficulties to detect the clusters, unless they make strong assumptions on the underlying model. In this particular example, even for a human looking at the histogram it is not obvious what the correct number of clusters should be. This illustrates that, as most methods for choosing the number of clusters, the eigengap heuristic usually works well if the data contains very well pronounced clusters, but in ambiguous cases it also returns ambiguous results.

Finally, note that the choice of the number of clusters and the choice of the connectivity parameters of the neighborhood graph affect each other. For example, if the connectivity parameter of the neighborhood graph is so small that the graph breaks into, say, $k _ { 0 }$ connected components, then choosing $k _ { 0 }$ as the number of clusters is a valid choice. However, as soon as the neighborhood graph is connected, it is not clear how the number of clusters and the connectivity parameters of the neighborhood graph interact. Both the choice of the number of clusters and the choice of the connectivity parameters of the graph are difficult problems on their own, and to our knowledge nothing non-trivial is known on their interactions.

# 8.4 The $k$ -means step

The three spectral clustering algorithms we presented in Section 4 use $k$ -means as last step to extract the final partition from the real valued matrix of eigenvectors. First of all, note that there is nothing principled about using the $k$ -means algorithm in this step. In fact, as we have seen from the various explanations of spectral clustering, this step should be very simple if the data contains well-expressed clusters. For example, in the ideal case if completely separated clusters we know that the eigenvectors of $L$ and $L _ { \mathrm { r w } }$ are piecewise constant. In this case, all points $x _ { i }$ which belong to the same cluster $C _ { s }$ are mapped to exactly the sample point $y _ { i }$ , namely to the unit vector $e _ { s } \in \mathbb { R } ^ { k }$ . In such a trivial case, any clustering algorithm applied to the points $y _ { i } \in \mathbb { R } ^ { k }$ will be able to extract the correct clusters.

While it is somewhat arbitrary what clustering algorithm exactly one chooses in the final step of spectral clustering, one can argue that at least the Euclidean distance between the points $y _ { i }$ is a meaningful quantity to look at. We have seen that the Euclidean distance between the points $y _ { i }$ is related to the “commute distance” on the graph, and in Nadler, Lafon, Coifman, and Kevrekidis (2006) the authors show that the Euclidean distances between the $y _ { i }$ are also related to a more general “diffusion distance”. Also, other uses of the spectral embeddings (e.g., Bolla (1991) or Belkin and Niyogi (2003)) show that the Euclidean distance in $\mathbb { R } ^ { d }$ is meaningful.

Instead of $k$ -means, people also use other techniques to construct he final solution from the real-valued representation. For example, in Lang (2006) the authors use hyperplanes for this purpose. A more advanced post-processing of the eigenvectors is proposed in Bach and Jordan (2004). Here the authors study the subspace spanned by the first $k$ eigenvectors, and try to approximate this subspace as good as possible using piecewise constant vectors. This also leads to minimizing certain Euclidean distances in the space $\mathbb { R } ^ { k }$ , which can be done by some weighted $k$ -means algorithm.

# 8.5 Which graph Laplacian should be used?

A fundamental question related to spectral clustering is the question which of the three graph Laplacians should be used to compute the eigenvectors. Before deciding this question, one should always look at the degree distribution of the similarity graph. If the graph is very regular and most vertices have approximately the same degree, then all the Laplacians are very similar to each other, and will work equally well for clustering. However, if the degrees in the graph are very broadly distributed, then the Laplacians differ considerably. In our opinion, there are several arguments which advocate for using normalized rather than unnormalized spectral clustering, and in the normalized case to use the eigenvectors of $L _ { \mathrm { r w } }$ rather than those of $L _ { \mathrm { s y m } }$ .

# Clustering objectives satisfied by the different algorithms

The first argument in favor of normalized spectral clustering comes from the graph partitioning point of view. For simplicity let us discuss the case $k = 2$ . In general, clustering has two different objectives:

1. We want to find a partition such that points in different clusters are dissimilar to each other, that is we want to minimize the between-cluster similarity. In the graph setting, this means to minimize $\operatorname { c u t } ( A , { \overline { { A } } } )$ .   
2. We want to find a partition such that points in the same cluster are similar to each other, that is we want to maximize the within-cluster similarities $W ( A , A )$ and $W ( { \overline { { A } } } , { \overline { { A } } } )$ .

Both RatioCut and Ncut directly implement the first objective by explicitly incorporating $\operatorname { c u t } ( A , { \overline { { A } } } )$ in the objective function. However, concerning the second point, both algorithms behave differently. Note that

$$
W (A, A) = W (A, V) - W (A, \overline {{A}}) = \mathrm {v o l} (A) - \mathrm {c u t} (A, \overline {{A}}).
$$

Hence, the within-cluster similarity is maximized if $\operatorname { c u t } ( A , A )$ is small and if $\operatorname { v o l } ( A )$ is large. As this is exactly what we achieve by minimizing Ncut, the Ncut criterion implements the second objective. This can be seen even more explicitly by considering yet another graph cut objective function, namely the MinMaxCut criterion introduced by Ding, He, Zha, Gu, and Simon (2001):

$$
\mathrm {M i n M a x C u t} (A _ {1}, \ldots , A _ {k}) := \sum_ {i = 1} ^ {k} \frac {\mathrm {c u t} (A _ {i} , \overline {{A}} _ {i})}{W (A _ {i} , A _ {i})}.
$$

Compared to Ncut, which has the terms $\operatorname { v o l } ( A ) = \operatorname { c u t } ( A , { \overline { { A } } } ) + W ( A , A )$ in the denominator, the MinMaxCut criterion only has $W ( A , A )$ in the denominator. In practice, Ncut and MinMaxCut are often minimized by similar cuts, as a good Ncut solution will have a small value of $\operatorname { c u t } ( A , { \overline { { A } } } )$ anyway and hence the denominators are not so different after all. Moreover, relaxing MinMaxCut leads to exactly the same optimization problem as relaxing Ncut, namely to normalized spectral clustering with the eigenvectors of $L _ { \mathrm { r w } }$ . So one can see by several ways that normalized spectral clustering incorporates both clustering objectives mentioned above.

Now consider the case of RatioCut. Here the objective is to maximize $| A |$ and $| { \overline { { A } } } |$ instead of $\operatorname { v o l } ( A )$ and $\mathrm { v o l } ( { \overline { { A } } } )$ . But $| A |$ and $| { \overline { { A } } } |$ are not necessarily related to the within-cluster similarity, as the within-cluster similarity depends on the edges and not on the number of vertices in $A$ . For instance, just think of a set $A$ which has very many vertices, all of which only have very low weighted edges to each other. Minimizing RatioCut does not attempt to maximize the within-cluster similarity, and the same is then true for its relaxation by unnormalized spectral clustering.

So this is our first important point to keep in mind: Normalized spectral clustering implements both clustering objectives mentioned above, while unnormalized spectral clustering only implements the first objective.

# Consistency issues

A completely different argument for the superiority of normalized spectral clustering comes from a statistical analysis of both algorithms. In a statistical setting one assumes that the data points $x _ { 1 } , \ldots , x _ { n }$ have been sampled i.i.d. according to some probability distribution $P$ on some underlying data space $\mathcal { X }$ . The most fundamental question is then the question of consistency: if we draw more and more data points, do the clustering results of spectral clustering converge to a useful partition of the underlying space $\mathcal { X }$ ?

For both normalized spectral clustering algorithms, it can be proved that this is indeed the case (von Luxburg, Bousquet, and Belkin, 2004, 2005; von Luxburg, Belkin, and Bousquet, to appear). Mathematically, one proves that as we take the limit $n \longrightarrow \infty$ , the matrix $L _ { \mathrm { s y m } }$ converges in a strong sense

![](images/17cb5dae6225997067571ea4d06f6a2ae2b5ab699ba769247507b7f88c36e230.jpg)  
Eigenvalues

![](images/266bb610b8f4575f60e199f200507962b15850539fd2edf6325367b429a1c592.jpg)  
Eigenvalues

![](images/0dafb5f2811f8914be04346c7193656bf28ef943979c9711c0c4133559494bfd.jpg)  
Eigenvector 1 Eigenvector 2 Eigenvector 2

![](images/172b1b385248668b2b1393dd27d9ab19c83a228042145cbea1bcfe30797d28fb.jpg)

![](images/02e11ab4b003b366205818f982efce62cb9de83761cfcaa7a09667576cb4bdd3.jpg)  
Eigenvector 3

![](images/329e63b37d3baec2ae462484fd64fe0e79008e0fd687df67d3fc8f2f8b6fb04a.jpg)  
Eigenvector 4

![](images/798e45e11e2abbee5300579bde6f16c4e6a947e890ef62ea68733cf1be70f468.jpg)  
Eigenvector 5

![](images/9a0e3f9be5f3e03b0cb2790d95ce0a3dbf8bebe5d72adf4de10ca5ac102a4012.jpg)  
Eigenvector 1 Eigenvector 2 Eigenvector 2

![](images/15a9a79b81041944eecc37e1b8b6a341f08d51a970dc0dc06e054fe24e19eb21.jpg)

![](images/77c7cc5e82eecf58265346617fedfc837de98c36c8e7d55b35529b7bd6acf060.jpg)  
Eigenvector 3

![](images/30358407ec192fb356e31b5b48cf5a6b98da24e22c6d12c6dce50810d8baac6d.jpg)  
Eigenvector 4

![](images/7d34fd9394d7a130242134bf4e4137a668d66b6ddee6c0eb69dd568b9e2fee7c.jpg)  
Eigenvector 5   
Figure 5: Consistency of unnormalized spectral clustering. Plotted are eigenvalues and eigenvectors of $L$ , for parameter $\sigma = 2$ (first row) and $\sigma = 5$ (second row). The dashed line indicates $\operatorname* { m i n } d _ { j }$ , the eigenvalues below $\operatorname* { m i n } d _ { j }$ are plotted as red diamonds, the eigenvalues above $\operatorname* { m i n } d _ { j }$ are plotted as blue stars. See text for more details.

to an operator $U$ on the space $C ( \mathcal { X } )$ of continuous functions on $\mathcal { X }$ . This convergence implies that the eigenvalues and eigenvectors of $L _ { \mathrm { s y m } }$ converge to those of $U$ , which in turn can be transformed to a statement about the convergence of normalized spectral clustering. One can show that the partition which is induced on $\mathcal { X }$ by the eigenvectors of $U$ can be interpreted similar to the random walks interpretation of spectral clustering. That is, if we consider a diffusion process on the data space $\mathcal { X }$ , then the partition induced by the eigenvectors of $U$ is such that the diffusion does not transition between the different clusters very often (von Luxburg et al., 2004). All consistency statements about normalized spectral clustering hold, for both $L _ { \mathrm { s y m } }$ and $L _ { \mathrm { r w } }$ , under very mild conditions which are usually satisfied in real world applications. Unfortunately, explaining more details about those results goes beyond the scope of this tutorial, so we refer the interested reader to von Luxburg et al. (to appear).

In contrast to the clear convergence statements for normalized spectral clustering, the situation for unnormalized spectral clustering is much more unpleasant. It can be proved that unnormalized spectral clustering can fail to converge, or that it can converge to trivial solutions which construct clusters consisting of one single point of the data space (von Luxburg et al., 2005, to appear). Mathematically, even though one can prove that the matrix $( 1 / n ) L$ itself converges to some limit operator $T$ on $C ( \mathcal { X } )$ as $n \longrightarrow \infty$ , the spectral properties of this limit operator $T$ can be so nasty that they prevent the convergence of spectral clustering. It is possible to construct examples which show that this is not only a problem for very large sample size, but that it can lead to completely unreliable results even for small sample size. At least it is possible to characterize the conditions when those problem do not occur: We have to make sure that the eigenvalues of $L$ corresponding to the eigenvectors used in unnormalized spectral clustering are significantly smaller than the minimal degree in the graph. This means that if we use the first $k$ eigenvectors for clustering, then $\lambda _ { i } \ll \operatorname* { m i n } _ { j = 1 , \dots , n } d _ { j }$ should hold for all $i = 1 , \ldots , k$ . The mathematical reason for this condition is that eigenvectors corresponding to eigenvalues larger than $\operatorname* { m i n } d _ { j }$ approximate Dirac functions, that is they are approximately 0 in all but one coordinate. If those eigenvectors are used for clustering, then they separate the one vertex where the eigenvector is non-zero from all other vertices, and we clearly do not want to construct such a partition. Again we refer to the literature for precise statements and proofs.

For an illustration of this phenomenon, consider again our toy data set from Section 4. We consider the first eigenvalues and eigenvectors of the unnormalized graph Laplacian based on the fully connected graph, for different choices of the parameter $\sigma$ of the Gaussian similarity function (see last row of Figure 1 and all rows of Figure 5). The eigenvalues above $\operatorname* { m i n } { d _ { j } }$ are plotted as blue stars, the eigenvalues below $\operatorname* { m i n } d _ { j }$ are plotted as red diamonds. The dashed line indicates $\operatorname* { m i n } d _ { j }$ . In general, we can see

that the eigenvectors corresponding to eigenvalues which are much below the dashed lines are “useful” eigenvectors. In case $\sigma = 1$ (plotted already in the last row of Figure 1), Eigenvalues 2, 3 and 4 are significantly below $\operatorname* { m i n } { d _ { j } }$ , and the corresponding Eigenvectors 2, 3, and 4 are meaningful (as already discussed in Section 4). If we increase the parameter $\sigma$ , we can observe that the eigenvalues tend to move towards $\operatorname* { m i n } d _ { j }$ . In case $\sigma = 2$ , only the first three eigenvalues are below $\operatorname* { m i n } d _ { j }$ (first row in Figure 5), and in case $\sigma = 5$ only the first two eigenvalues are below $\operatorname* { m i n } d _ { j }$ (second row in Figure 5). We can see that as soon as an eigenvalue gets close to or above $\operatorname* { m i n } d _ { j }$ , its corresponding eigenvector approximates a Dirac function. Of course, those eigenvectors are unsuitable for constructing a clustering. In the limit for $n  \infty$ , those eigenvectors would converge to perfect Dirac functions. Our illustration of the finite sample case shows that this behavior not only occurs for large sample size, but can be generated even on the small example in our toy data set.

It is very important to stress that those problems only concern the eigenvectors of the matrix $L$ , and they do not occur for $L _ { \mathrm { r w } }$ or $L _ { \mathrm { s y m } }$ . Thus, from a statistical point of view, it is preferable to avoid unnormalized spectral clustering and to use the normalized algorithms instead.

# Which normalized Laplacian?

Looking at the differences between the two normalized spectral clustering algorithms using $L _ { \mathrm { r w } }$ and $L _ { \mathrm { s y m } }$ , all three explanations of spectral clustering are in favor of $L _ { \mathrm { r w } }$ . The reason is that the eigenvectors of $L _ { \mathrm { r w } }$ are cluster indicator vectors $\mathbb { 1 } _ { A _ { i } }$ , while the eigenvectors of $L _ { \mathrm { s y m } }$ are additionally multiplied with $D ^ { 1 / 2 }$ , which might lead to undesired artifacts. As using $L _ { \mathrm { s y m } }$ also does not have any computational advantages, we thus advocate for using $L _ { \mathrm { r w } }$ .

# 9 Outlook and further reading

Spectral clustering goes back to Donath and Hoffman (1973), who first suggested to construct graph partitions based on eigenvectors of the adjacency matrix. In the same year, Fiedler (1973) discovered that bi-partitions of a graph are closely connected with the second eigenvector of the graph Laplacian, and he suggested to use this eigenvector to partition a graph. Since then, spectral clustering has been discovered, re-discovered, and extended many times in different communities, see for example Pothen, Simon, and Liou (1990), Simon (1991), Bolla (1991), Hagen and Kahng (1992), Hendrickson and Leland (1995), Van Driessche and Roose (1995), Barnard, Pothen, and Simon (1995), Spielman and Teng (1996), Guattery and Miller (1998). A nice overview over the history of spectral clustering can be found in Spielman and Teng (1996).

In the machine learning community, spectral clustering has been made popular by the works of Shi and Malik (2000), Ng et al. (2002), Meila and Shi (2001), and Ding (2004). Subsequently, spectral clustering has been extended to many non-standard settings, for example spectral clustering applied to the co-clustering problem (Dhillon, 2001), spectral clustering with additional side information (Joachims, 2003) connections between spectral clustering and the weighted kernel- $k$ -means algorithm (Dhillon, Guan, and Kulis, 2005), learning similarity functions based on spectral clustering (Bach and Jordan, 2004), or spectral clustering in a distributed environment (Kempe and McSherry, 2004). Also, new theoretical insights about the relation of spectral clustering to other algorithms have been found. A link between spectral clustering and the weighted kernel $k$ -means algorithm is described in Dhillon et al. (2005). Relations between spectral clustering and (kernel) principal component analysis rely on the fact that the smallest eigenvectors of graph Laplacians can also be interpreted as the largest eigenvectors of kernel matrices (Gram matrices). Two different flavors of this interpretation exist: while Bengio et al. (2004) interpret the matrix $D ^ { - 1 / 2 } W D ^ { - 1 / 2 }$ as kernel matrix, other authors (Saerens,

Fouss, Yen, and Dupont, 2004) interpret the Moore-Penrose inverses of $L$ or $L _ { \mathrm { s y m } }$ as kernel matrix. Both interpretations can be used to construct (different) out-of-sample extensions for spectral clustering. Concerning application cases of spectral clustering, in the last few years such a huge number of papers has been published in various scientific areas that it is impossible to cite all of them. We encourage the reader to query his favorite literature data base with the phrase “spectral clustering” to get an impression no the variety of applications.

The success of spectral clustering is mainly based on the fact that it does not make strong assumptions on the form of the clusters. As opposed to $k$ -means, where the resulting clusters form convex sets (or, to be precise, lie in disjoint convex sets of the underlying space), spectral clustering can solve very general problems like intertwined spirals. Moreover, spectral clustering can be implemented efficiently even for large data sets, as long as we make sure that the similarity graph is sparse. Once the similarity graph is chosen, we just have to solve a linear problem, and there are no issues of getting stuck in local minima or restarting the algorithm for several times with different initializations. However, we have already mentioned that choosing a good similarity graph is not trivial, and spectral clustering can be quite unstable under different choices of the parameters for the neighborhood graphs. So spectral clustering cannot serve as a “black box algorithm” which automatically detects the correct clusters in any given data set. But it can be considered as a powerful tool which can produce good results if applied with care.

In the field of machine learning, graph Laplacians are not only used for clustering, but also emerge for many other tasks such as semi-supervised learning (e.g., Chapelle, Sch¨olkopf, and Zien, 2006 for an overview) or manifold reconstruction (e.g., Belkin and Niyogi, 2003). In most applications, graph Laplacians are used to encode the assumption that data points which are “close” (i.e., $w _ { i j }$ is large) should have a “similar” label (i.e., $f _ { i } \approx f _ { j }$ ). A function $f$ satisfies this assumption if $w _ { i j } ( f _ { i } - f _ { j } ) ^ { 2 }$ is small for all $i , j$ , that is $f ^ { \prime } L f$ is small. With this intuition one can use the quadratic form $f ^ { \prime } L f$ as a regularizer in a transductive classification problem. One other way to interpret the use of graph Laplacians is by the smoothness assumptions they encode. A function $f$ which has a low value of $f ^ { \prime } L f$ has the property that it varies only “a little bit” in regions where the data points lie dense (i.e., the graph is tightly connected), whereas it is allowed to vary more (e.g., to change the sign) in regions of low data density. In this sense, a small value of $f ^ { \prime } L f$ encodes the so called “cluster assumption” in semi-supervised learning, which requests that the decision boundary of a classifier should lie in a region of low density.

An intuition often used is that graph Laplacians formally look like a continuous Laplace operator (and this is also where the name “graph Laplacian” comes from). To see this, transform a local similarity $w _ { i j }$ to a distance $d _ { i j }$ by the relationship $w _ { i j } = 1 / d _ { i j } ^ { 2 }$ and observe that

$$
w _ {i j} (f _ {i} - f _ {j}) ^ {2} \approx \left(\frac {f _ {i} - f _ {j}}{d _ {i j}}\right) ^ {2}
$$

looks like a difference quotient. As a consequence, the equation $\begin{array} { r } { f ^ { \prime } L f = \sum _ { i j } w _ { i j } ( f _ { i } - f _ { j } ) ^ { 2 } } \end{array}$ from Proposition 1 looks like a discrete version of the quadratic form associated to the standard Laplace operator $\mathcal { L }$ on $\mathbb { R } ^ { n }$ , which satisfies

$$
\langle g, \mathcal {L} g \rangle = \int | \nabla g | ^ {2} d x.
$$

This intuition has been made precise in the works of Belkin (2003), Lafon (2004), Hein, Audibert, and von Luxburg (2005); M., Audibert, and von Luxburg (2007), Belkin and Niyogi (2005), Hein (2006), Gin´e and Koltchinskii (2005). In general, it is proved that graph Laplacians are discrete versions of certain continuous Laplace operators, and that if the graph Laplacian is constructed on a similarity graph of randomly sampled data points, then it converges to some continuous Laplace operator (or

Laplace-Beltrami operator) on the underlying space. Belkin (2003) studied the first important step of the convergence proof, which deals with the convergence of a continuous operator related to discrete graph Laplacians to the Laplace-Beltrami operator. His results were generalized from uniform distributions to general distributions by Lafon (2004). Then in Belkin and Niyogi (2005), the authors prove pointwise convergence results for the unnormalized graph Laplacian using the Gaussian similarity function on manifolds with uniform distribution. At the same time, Hein et al. (2005) prove more general results, taking into account all different graph Laplacians $L$ , $L _ { \mathrm { r w } }$ , and $L _ { \mathrm { s y m } }$ , more general similarity functions, and manifolds with arbitrary distributions. In Gin´e and Koltchinskii (2005), distributional and uniform convergence results are proved on manifolds with uniform distribution. Hein (2006) studies the convergence of the smoothness functional induced by the graph Laplacians and shows uniform convergence results.

Apart from applications of graph Laplacians to partitioning problems in the widest sense, graph Laplacians can also be used for completely different purposes, for example for graph drawing (Koren, 2005). In fact, there are many more tight connections between the topology and properties of graphs and the graph Laplacian matrices than we have mentioned in this tutorial. Now equipped with an understanding for the most basic properties, the interested reader is invited to further explore and enjoy the huge literature in this field on his own.

# References

Aldous, D. and Fill, J. (in preparation). Reversible Markov Chains and Random Walks on Graphs. online version available at http://www.stat.berkeley.edu/users/aldous/RWG/book.html.   
Bach, F. and Jordan, M. (2004). Learning spectral clustering. In S. Thrun, L. Saul, and B. Sch¨olkopf (Eds.), Advances in Neural Information Processing Systems 16 (NIPS) (pp. 305 – 312). Cambridge, MA: MIT Press.   
Bapat, R., Gutman, I., and Xiao, W. (2003). A simple method for computing resistance distance. Z. Naturforsch., 58, 494 – 498.   
Barnard, S., Pothen, A., and Simon, H. (1995). A spectral algorithm for envelope reduction of sparse matrices. Numerical Linear Algebra with Applications, 2 (4), 317 – 334.   
Belkin, M. (2003). Problems of Learning on Manifolds. PhD Thesis, University of Chicago.   
Belkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15 (6), 1373 – 1396.   
Belkin, M. and Niyogi, P. (2005). Towards a theoretical foundation for Laplacian-based manifold methods. In P. Auer and R. Meir (Eds.), Proceedings of the 18th Annual Conference on Learning Theory (COLT) (pp. 486 – 500). Springer, New York.   
Ben-David, S., von Luxburg, U., and P´al, D. (2006). A sober look on clustering stability. In G. Lugosi and H. Simon (Eds.), Proceedings of the 19th Annual Conference on Learning Theory (COLT) (pp. 5 – 19). Springer, Berlin.   
Bengio, Y., Delalleau, O., Roux, N., Paiement, J., Vincent, P., and Ouimet, M. (2004). Learning eigenfunctions links spectral embedding and kernel PCA. Neural Computation, 16, 2197 – 2219.   
Ben-Hur, A., Elisseeff, A., and Guyon, I. (2002). A stability based method for discovering structure in clustered data. In Pacific Symposium on Biocomputing (pp. 6 – 17).   
Bhatia, R. (1997). Matrix Analysis. Springer, New York.   
Bie, T. D. and Cristianini, N. (2006). Fast SDP relaxations of graph cut clustering, transduction, and other combinatorial problems . JMLR, 7, 1409 – 1436.   
Bolla, M. (1991). Relations between spectral and classification properties of multigraphs (Technical Report No. DIMACS-91-27). Center for Discrete Mathematics and Theoretical Computer Science.   
Br´emaud, P. (1999). Markov chains: Gibbs fields, Monte Carlo simulation, and queues. New York: Springer-Verlag.

Brito, M., Chavez, E., Quiroz, A., and Yukich, J. (1997). Connectivity of the mutual k-nearest-neighbor graph in clustering and outlier detection. Statistics and Probability Letters, 35, $3 3 - 4 2$ .   
Bui, T. N. and Jones, C. (1992). Finding good approximate vertex and edge partitions is NP-hard. Inf. Process. Lett., 42 (3), 153 – 159.   
Chapelle, O., Sch¨olkopf, B., and Zien, A. (Eds.). (2006). Semi-Supervised Learning. MIT Press, Cambridge.   
Chung, F. (1997). Spectral graph theory (Vol. 92 of the CBMS Regional Conference Series in Mathematics). Conference Board of the Mathematical Sciences, Washington.   
Dhillon, I. (2001). Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (KDD) (pp. 269 – 274). New York: ACM Press.   
Dhillon, I., Guan, Y., and Kulis, B. (2005). A unified view of kernel $k$ -means, spectral clustering, and graph partitioning (Technical Report No. UTCS TR-04-25). University of Texas at Austin.   
Ding, C. (2004). A tutorial on spectral clustering. Talk presented at ICML. (Slides available at http://crd.lbl.gov/~cding/Spectral/)   
Ding, C., He, X., Zha, H., Gu, M., and Simon, H. (2001). A min-max cut algorithm for graph partitioning and data clustering. In Proceedings of the first IEEE International Conference on Data Mining (ICDM) (pp. 107 – 114). Washington, DC, USA: IEEE Computer Society.   
Donath, W. E. and Hoffman, A. J. (1973). Lower bounds for the partitioning of graphs. IBM J. Res. Develop., 17, 420 – 425.   
Fiedler, M. (1973). Algebraic connectivity of graphs. Czechoslovak Math. J., 23, 298 – 305.   
Fouss, F., Pirotte, A., Renders, J.-M., and Saerens, M. (2007). Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. IEEE Trans. Knowl. Data Eng, 19 (3), 355–369.   
Fraley, C. and Raftery, A. E. (2002). Model-based clustering, discriminant analysis, and density estimation. JASA, 97, 611 – 631.   
Gin´e, E. and Koltchinskii, V. (2005). Empirical graph Laplacian approximation of Laplace-Beltrami operators: large sample results. In Proceedings of the 4th International Conference on High Dimensional Probability (pp. 238 – 259).   
Golub, G. and Van Loan, C. (1996). Matrix computations. Baltimore: Johns Hopkins University Press.   
Guattery, S. and Miller, G. (1998). On the quality of spectral separators. SIAM Journal of Matrix Anal. Appl., 19 (3), 701 – 719.   
Gutman, I. and Xiao, W. (2004). Generalized inverse of the Laplacian matrix and some applications. Bulletin de l’Academie Serbe des Sciences at des Arts (Cl. Math. Natur.), 129, 15 – 23.   
Hagen, L. and Kahng, A. (1992). New spectral methods for ratio cut partitioning and clustering. IEEE Trans. Computer-Aided Design, 11 (9), 1074 – 1085.   
Hastie, T., Tibshirani, R., and Friedman, J. (2001). The elements of statistical learning. New York: Springer.   
Hein, M. (2006). Uniform convergence of adaptive graph-based regularization. In Proceedings of the 19th Annual Conference on Learning Theory (COLT) (pp. 50 – 64). Springer, New York.   
Hein, M., Audibert, J.-Y., and von Luxburg, U. (2005). From graphs to manifolds - weak and strong pointwise consistency of graph Laplacians. In P. Auer and R. Meir (Eds.), Proceedings of the 18th Annual Conference on Learning Theory (COLT) (pp. 470 – 485). Springer, New York.   
Hendrickson, B. and Leland, R. (1995). An improved spectral graph partitioning algorithm for mapping parallel computations. SIAM J. on Scientific Computing, 16, 452 – 469.   
Joachims, T. (2003). Transductive Learning via Spectral Graph Partitioning. In T. Fawcett and N. Mishra (Eds.), Proceedings of the 20th international conference on machine learning (ICML) (pp. 290 – 297). AAAI Press.   
Kannan, R., Vempala, S., and Vetta, A. (2004). On clusterings: Good, bad and spectral. Journal of the ACM, 51 (3), 497–515.   
Kempe, D. and McSherry, F. (2004). A decentralized algorithm for spectral analysis. In Proceedings

of the 36th Annual ACM Symposium on Theory of Computing (STOC) (pp. 561 – 568). New York, NY, USA: ACM Press.   
Klein, D. and Randic, M. (1993). Resistance distance. Journal of Mathematical Chemistry, 12, 81 – 95.   
Koren, Y. (2005). Drawing graphs by eigenvectors: theory and practice. Computers and Mathematics with Applications, 49, 1867 – 1888.   
Lafon, S. (2004). Diffusion maps and geometric harmonics. PhD Thesis, Yale University.   
Lang, K. (2006). Fixing two weaknesses of the spectral method. In Y. Weiss, B. Sch¨olkopf, and J. Platt (Eds.), Advances in Neural Information Processing Systems 18 (pp. 715 – 722). Cambridge, MA: MIT Press.   
Lange, T., Roth, V., Braun, M., and Buhmann, J. (2004). Stability-based validation of clustering solutions. Neural Computation, 16 (6), 1299 – 1323.   
Lov´asz, L. (1993). Random walks on graphs: a survey. In Combinatorics, Paul Erd¨os is eighty (pp. 353 – 397). Budapest: J´anos Bolyai Math. Soc.   
L¨utkepohl, H. (1997). Handbook of Matrices. Chichester: Wiley.   
M., Audibert, J.-Y., and von Luxburg, U. (2007). Graph laplacians and their convergence on random neighborhood graphs. JMLR, 8, 1325 – 1370.   
Meila, M. and Shi, J. (2001). A random walks view of spectral segmentation. In 8th International Workshop on Artificial Intelligence and Statistics (AISTATS).   
Mohar, B. (1991). The Laplacian spectrum of graphs. In Graph theory, combinatorics, and applications. Vol. 2 (Kalamazoo, MI, 1988) (pp. 871 – 898). New York: Wiley.   
Mohar, B. (1997). Some applications of Laplace eigenvalues of graphs. In G. Hahn and G. Sabidussi (Eds.), Graph Symmetry: Algebraic Methods and Applications (Vol. NATO ASI Ser. C 497, pp. 225 – 275). Kluwer.   
Nadler, B., Lafon, S., Coifman, R., and Kevrekidis, I. (2006). Diffusion maps, spectral clustering and eigenfunctions of Fokker-Planck operators. In Y. Weiss, B. Sch¨olkopf, and J. Platt (Eds.), Advances in Neural Information Processing Systems 18 (pp. 955 – 962). Cambridge, MA: MIT Press.   
Ng, A., Jordan, M., and Weiss, Y. (2002). On spectral clustering: analysis and an algorithm. In T. Dietterich, S. Becker, and Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14 (pp. 849 – 856). MIT Press.   
Norris, J. (1997). Markov Chains. Cambridge: Cambridge University Press.   
Penrose, M. (1999). A strong law for the longest edge of the minimal spanning tree. Ann. of Prob., 27 (1), 246 – 260.   
Pothen, A., Simon, H. D., and Liou, K. P. (1990). Partitioning sparse matrices with eigenvectors of graphs. SIAM Journal of Matrix Anal. Appl., 11, 430 – 452.   
Saerens, M., Fouss, F., Yen, L., and Dupont, P. (2004). The principal components analysis of a graph, and its relationships to spectral clustering. In Proceedings of the 15th European Conference on Machine Learning (ECML) (pp. 371 – 383). Springer, Berlin.   
Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22 (8), 888 – 905.   
Simon, H. (1991). Partitioning of unstructured problems for parallel processing. Computing Systems Engineering, 2, 135 – 148.   
Spielman, D. and Teng, S. (1996). Spectral partitioning works: planar graphs and finite element meshes. In 37th Annual Symposium on Foundations of Computer Science (Burlington, VT, 1996) (pp. 96 – 105). Los Alamitos, CA: IEEE Comput. Soc. Press. (See also extended technical report.)   
Stewart, G. and Sun, J. (1990). Matrix Perturbation Theory. New York: Academic Press.   
Still, S. and Bialek, W. (2004). How many clusters? an information-theoretic perspective. Neural Comput., 16 (12), 2483 – 2506.   
Stoer, M. and Wagner, F. (1997). A simple min-cut algorithm. J. ACM, 44 (4), 585 – 591.   
Tibshirani, R., Walther, G., and Hastie, T. (2001). Estimating the number of clusters in a dataset via the gap statistic. J. Royal. Statist. Soc. B, 63 (2), 411 – 423.

Van Driessche, R. and Roose, D. (1995). An improved spectral bisection algorithm and its application to dynamic load balancing. Parallel Comput., 21 (1), 29 – 48.   
von Luxburg, U., Belkin, M., and Bousquet, O. (to appear). Consistency of spectral clustering. Annals of Statistics. (See also Technical Report 134, Max Planck Institute for Biological Cybernetics, 2004)   
von Luxburg, U., Bousquet, O., and Belkin, M. (2004). On the convergence of spectral clustering on random samples: the normalized case. In J. Shawe-Taylor and Y. Singer (Eds.), Proceedings of the 17th Annual Conference on Learning Theory (COLT) (pp. 457 – 471). Springer, New York.   
von Luxburg, U., Bousquet, O., and Belkin, M. (2005). Limits of spectral clustering. In L. Saul, Y. Weiss, and L. Bottou (Eds.), Advances in Neural Information Processing Systems (NIPS) 17 (pp. 857 – 864). Cambridge, MA: MIT Press.   
Wagner, D. and Wagner, F. (1993). Between min cut and graph bisection. In Proceedings of the 18th International Symposium on Mathematical Foundations of Computer Science (MFCS) (pp. 744 – 750). London: Springer.