Peter J. Brockwell Richard A. Davis

# Introduction to Time Series and Forecasting

Third Edition

# Springer Texts in Statistics

Series Editors:

R. DeVeaux   
S. Fienberg   
I. Olkin

Peter J. Brockwell • Richard A. Davis

# Introduction to Time Series and Forecasting

Third Edition

Additional material to this book can be downloaded from http://extras.springer.com.

ISSN 1431-875X

ISSN 2197-4136 (electronic)

Springer Texts in Statistics

ISBN 978-3-319-29852-8

ISBN 978-3-319-29854-2 (eBook)

DOI 10.1007/978-3-319-29854-2

Library of Congress Control Number: 2016939116

$^ ©$ Springer International Publishing Switzerland 1996, 2002, 2016

This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.

The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made.

Printed on acid-free paper

This Springer imprint is published by Springer Nature

The registered company is Springer International Publishing AG Switzerland

To Pam and Patti

# Preface

This book is aimed at the reader who wishes to gain a working knowledge of time series and forecasting methods as applied in economics, engineering, and the natural and social sciences. Unlike our more advanced book, Time Series: Theory and Methods, Brockwell and Davis (1991), this one requires only a knowledge of basic calculus, matrix algebra and elementary statistics at the level, for example, of Mendenhall et al. (1990). It is intended for upper-level undergraduate students and beginning graduate students.

The emphasis is on methods and the analysis of data sets. The professional version of the time series package ITSM2000, for Windows-based PC, enables the reader to reproduce most of the calculations in the text (and to analyze further data sets of the reader’s own choosing). It is available for download, together with most of the data sets used in the book, from http://extras.springer.com. Appendix E contains a detailed introduction to the package.

Very little prior familiarity with computing is required in order to use the computer package. The book can also be used in conjunction with other computer packages for handling time series. Chapter 14 of the book by Venables and Ripley (2003) describes how to perform many of the calculations using S and R. The package ITSMR of Weigt (2015) can be used in R to reproduce many of the features of ITSM2000. The package Yuima, also for R, can be used for simulation and estimation of the Lévy-driven CARMA processes discussed in Section 11.5 (see Iacus and Mercuri (2015)). Both of these packages can be downloaded from https://cran.rproject.org/web/packages.

There are numerous problems at the end of each chapter, many of which involve use of the programs to study the data sets provided.

To make the underlying theory accessible to a wider audience, we have stated some of the key mathematical results without proof, but have attempted to ensure that the logical structure of the development is otherwise complete. (References to proofs are provided for the interested reader.)

There is sufficient material here for a full-year introduction to univariate and multivariate time series and forecasting. Chapters 1 through 6 have been used for several years in introductory one-semester courses in univariate time series at Columbia University, Colorado State University, and Royal Melbourne Institute of Technology. The chapter on spectral analysis can be excluded without loss of continuity by readers who are so inclined.

In view of the explosion of interest in financial time series in recent decades, the third edition includes a new chapter (Chapter 7) specifically devoted to this topic. Some of the basic tools required for an understanding of continuous-time financial time series models (Brownian motion, Lévy processes, and Itô calculus) have also been added as

Appendix D, and a new Section 11.5 provides an introduction to continuous parameter ARMA (or CARMA) processes.

The diskette containing the student version of the package ITSM2000 is no longer included with the book since the professional version (which places no limit on the length of the series to be studied) can now be downloaded from http://extras.springer. com as indicated above. A tutorial for the use of the package is provided as Appendix E and a searchable file, ITSM_HELP.pdf, giving more detailed instructions, is included with the package.

We are greatly indebted to the readers of the first and second editions of the book and especially to Matthew Calder, coauthor of the computer package ITSM2000 and to Anthony Brockwell, both of whom made many valuable comments and suggestions. We also wish to thank Colorado State University, Columbia University, the National Science Foundation, Springer-Verlag, and our families for their continuing support during the preparation of this third edition.

Fort Collins, CO, USA

New York, NY, USA

April, 2016

Peter J. Brockwell

Richard A. Davis

# Contents

# Preface vii

# 1. Introduction 1

1.1. Examples of Time Series 1   
1.2. Objectives of Time Series Analysis 5   
1.3. Some Simple Time Series Models 6

1.3.1. Some Zero-Mean Models 6   
1.3.2. Models with Trend and Seasonality 7   
1.3.3. A General Approach to Time Series Modeling 12

1.4. Stationary Models and the Autocorrelation Function 13

1.4.1. The Sample Autocorrelation Function 16   
1.4.2. A Model for the Lake Huron Data 18

1.5. Estimation and Elimination of Trend and Seasonal Components 20

1.5.1. Estimation and Elimination of Trend in the Absence of Seasonality 21   
1.5.2. Estimation and Elimination of Both Trend and Seasonality 26

1.6. Testing the Estimated Noise Sequence 30

Problems 34

# 2. Stationary Processes 39

2.1. Basic Properties 39  
2.2. Linear Processes 44   
2.3. Introduction to ARMA Processes 47

2.4. Properties of the Sample Mean and Autocorrelation Function 50

2.4.1. Estimation of $\mu$ 50   
2.4.2. Estimation of $\gamma ( \cdot )$ and $\rho ( \cdot )$ 51

2.5. Forecasting Stationary Time Series 55

2.5.1. Prediction of Second-Order Random Variables 57   
2.5.2. The Prediction Operator $P ( \cdot | \mathbf { W } )$ 58   
2.5.3. The Durbin–Levinson Algorithm 60   
2.5.4. The Innovations Algorithm 62   
2.5.5. Recursive Calculation of the $h$ -Step Predictors 65

2.5.6. Prediction of a Stationary Process in Terms of Infinitely Many Past Values 65   
2.5.7. Determination of $\tilde { P } _ { n } X _ { n + h }$ 66

2.6. The Wold Decomposition 67

Problems 68

# 3. ARMA Models 73

3.1. ARMA( p, q) Processes 73   
3.2. The ACF and PACF of an ARMA( p, q) Process 77

3.2.1. Calculation of the ACVF 78   
3.2.2. The Autocorrelation Function 82   
3.2.3. The Partial Autocorrelation Function 83   
3.2.4. Examples 84

3.3. Forecasting ARMA Processes 87

3.3.1. h-Step Prediction of an ARMA $( p , q )$ Process 91

Problems 94

# 4. Spectral Analysis 97

4.1. Spectral Densities 98   
4.2. The Periodogram 106   
4.3. Time-Invariant Linear Filters 111   
4.4. The Spectral Density of an ARMA Process 115

4.4.1. Rational Spectral Density Estimation 117

Problems 117

# 5. Modeling and Forecasting with ARMA Processes 121

5.1. Preliminary Estimation 122

5.1.1. Yule–Walker Estimation 123   
5.1.2. Burg’s Algorithm 129   
5.1.3. The Innovations Algorithm 132   
5.1.4. The Hannan–Rissanen Algorithm 137

5.2. Maximum Likelihood Estimation 139   
5.3. Diagnostic Checking 144

5.3.1. The Graph of $\{ \hat { R } _ { t } , t = 1 , \dots , n \}$ 145   
5.3.2. The Sample ACF of the Residuals 146   
5.3.3. Tests for Randomness of the Residuals 146

5.4. Forecasting 147   
5.5. Order Selection 149

5.5.1. The FPE Criterion 149   
5.5.2. The AICC Criterion 151

Problems 153

# 6. Nonstationary and Seasonal Time Series Models 157

6.1. ARIMA Models for Nonstationary Time Series 158   
6.2. Identification Techniques 164   
6.3. Unit Roots in Time Series Models 169

6.3.1. Unit Roots in Autoregressions 169   
6.3.2. Unit Roots in Moving Averages 171

6.4. Forecasting ARIMA Models 173   
6.4.1. The Forecast Function 175   
6.5. Seasonal ARIMA Models 177   
6.5.1. Forecasting SARIMA Processes 182   
6.6. Regression with ARMA Errors 184   
6.6.1. OLS and GLS Estimation 184   
6.6.2. ML Estimation 186   
Problems 191

# 7. Time Series Models for Financial Data 195

7.1. Historical Overview 196   
7.2. GARCH Models 197   
7.3. Modified GARCH Processes 204

7.3.1. EGARCH Models 205   
7.3.2. FIGARCH and IGARCH Models 207

7.4. Stochastic Volatility Models 209   
7.5. Continuous-Time Models 212

7.5.1. Lévy Processes 212   
7.5.2. The Geometric Brownian Motion (GBM) Model for Asset Prices 215   
7.5.3. A Continuous-Time SV Model 217

7.6. An Introduction to Option Pricing 221   
Problems 224

# 8. Multivariate Time Series 227

8.1. Examples 228   
8.2. Second-Order Properties of Multivariate Time Series 232

8.2.1. Second-Order Properties in the Frequency Domain 236   
8.3. Estimation of the Mean and Covariance Function 236

8.3.1. Estimation of $\mu$ 236   
8.3.2. Estimation of Ŵ(h) 238

8.3.3. Testing for Independence of Two Stationary Time Series 239   
8.3.4. Bartlett’s Formula 240

8.4. Multivariate ARMA Processes 243   
8.4.1. The Covariance Matrix Function of a Causal ARMA Process 245   
8.5. Best Linear Predictors of Second-Order Random Vectors 246

8.6. Modeling and Forecasting with Multivariate AR Processes 247

8.6.1. Estimation for Autoregressive Processes Using Whittle’s Algorithm 248   
8.6.2. Forecasting Multivariate Autoregressive Processes 250

8.7. Cointegration 254   
Problems 255

# 9. State-Space Models 259

9.1. State-Space Representations 260   
9.1.1. State-Space Models with $t \in \{ 0 , \pm 1 , . . . \}$ 262   
9.2. The Basic Structural Model 263

9.3. State-Space Representation of ARIMA Models 266   
9.4. The Kalman Recursions 270

9.4.1. h-Step Prediction of $\{ \mathbf { Y } _ { t } \}$ Using the Kalman Recursions

9.5. Estimation for State-Space Models 275   
9.5.1. Application to Structural Models 276   
9.6. State-Space Models with Missing Observations 280   
9.6.1. The Gaussian Likelihood of $\{ \mathbf { Y } _ { i _ { 1 } } , \ldots , \mathbf { Y } _ { i _ { r } } \}$ , $1 \leq i _ { 1 } < i _ { 2 } < \cdot \cdot \cdot < i _ { r } \leq n$ 281   
9.6.2. Estimation of Missing Values for State-Space Models 283

9.7. The EM Algorithm 285   
9.7.1. Missing Data 286   
9.8. Generalized State-Space Models 287   
9.8.1. Parameter-Driven Models 288   
9.8.2. Observation-Driven Models 294   
9.8.3. Exponential Family Models 296

Problems 303

# 10.Forecasting Techniques 309

10.1.The ARAR Algorithm 310

10.1.1.Memory Shortening 310   
10.1.2.Fitting a Subset Autoregression 311   
10.1.3.Forecasting 311   
10.1.4.Application of the ARAR Algorithm 312

10.2.The Holt–Winters Algorithm 314

10.2.1.The Algorithm 314   
10.2.2.Holt–Winters and ARIMA Forecasting 316

10.3.The Holt–Winters Seasonal Algorithm 317

10.3.1.The Algorithm 317   
10.3.2.Holt–Winters Seasonal and ARIMA Forecasting 318

10.4.Choosing a Forecasting Algorithm 318

Problems 320

# 11.Further Topics 323

11.1.Transfer Function Models 323   
11.1.1.Prediction Based on a Transfer Function Model 327   
11.2.Intervention Analysis 331   
11.3.Nonlinear Models 334

11.3.1.Deviations from Linearity 335   
11.3.2.Chaotic Deterministic Sequences 335   
11.3.3.Distinguishing Between White Noise and iid Sequences 337   
11.3.4.Three Useful Classes of Nonlinear Models 338

11.4.Long-Memory Models 338   
11.5.Continuous-Time ARMA Processes 342

11.5.1.The Gaussian CAR(1) Process, $\{ Y ( t ) , t \geq 0 \}$ 343   
11.5.2.The Gaussian CARMA $( p , q )$ Process, $\{ Y ( t ) , t \in \mathbb { R } \}$ 345   
11.5.3.Lévy-driven CARMA Processes, $\{ Y ( t ) , t \in \mathbb { R } \}$ 347

Problems 350

# A. Random Variables and Probability Distributions 353

A.1. Distribution Functions and Expectation 353  
A.1.1. Examples of Continuous Distributions 354   
A.1.2. Examples of Discrete Distributions 355   
A.1.3. Expectation, Mean, and Variance 356

A.2. Random Vectors 357

A.2.1. Means and Covariances 359

A.3. The Multivariate Normal Distribution 360

Problems 363

# B. Statistical Complements 365

B.1. Least Squares Estimation 365   
B.1.1. The Gauss–Markov Theorem 367   
B.1.2. Generalized Least Squares 367   
B.2. Maximum Likelihood Estimation 368   
B.2.1. Properties of Maximum Likelihood Estimators 369   
B.3. Confidence Intervals 369   
B.3.1. Large-Sample Confidence Regions 370   
B.4. Hypothesis Testing 370   
B.4.1. Error Probabilities 371   
B.4.2. Large-Sample Tests Based on Confidence Regions 371

# C. Mean Square Convergence 373

C.1. The Cauchy Criterion 373

# D. Lévy Processes, Brownian Motion and Itô Calculus 375

D.1. Lévy Processes 375   
D.2. Brownian Motion and the Itô Integral 377   
D.3. Itô Processes and Itô’s Formula 381   
D.4. Itô Stochastic Differential Equations 383

# E. An ITSM Tutorial 387

E.1. Getting Started 388   
E.1.1. Running ITSM 388   
E.2. Preparing Your Data for Modeling 388   
E.2.1. Entering Data 389   
E.2.2. Information 389   
E.2.3. Filing Data 389   
E.2.4. Plotting Data 390   
E.2.5. Transforming Data 390   
E.3. Finding a Model for Your Data 394   
E.3.1. Autofit 394   
E.3.2. The Sample ACF and PACF 394   
E.3.3. Entering a Model 396   
E.3.4. Preliminary Estimation 397   
E.3.5. The AICC Statistic 398   
E.3.6. Changing Your Model 399

E.3.7. Maximum Likelihood Estimation 399   
E.3.8. Optimization Results 400

E.4. Testing Your Model 401

E.4.1. Plotting the Residuals 401   
E.4.2. ACF/PACF of the Residuals 402   
E.4.3. Testing for Randomness of the Residuals 403

E.5. Prediction 404

E.5.1. Forecast Criteria 404   
E.5.2. Forecast Results 405

E.6. Model Properties 405

E.6.1. ARMA Models 406   
E.6.2. Model ACF, PACF 406   
E.6.3. Model Representations 408   
E.6.4. Generating Realizations of a Random Series 409   
E.6.5. Spectral Properties 409

E.7. Multivariate Time Series 409

References 411

Index 419

# 1

# Introduction

1.1 Examples of Time Series   
1.2 Objectives of Time Series Analysis   
1.3 Some Simple Time Series Models   
1.4 Stationary Models and the Autocorrelation Function   
1.5 Estimation and Elimination of Trend and Seasonal Components   
1.6 Testing the Estimated Noise Sequence

In this chapter we introduce some basic ideas of time series analysis and stochastic processes. Of particular importance are the concepts of stationarity and the autocovariance and sample autocovariance functions. Some standard techniques are described for the estimation and removal of trend and seasonality (of known period) from an observed time series. These are illustrated with reference to the data sets in Section 1.1. The calculations in all the examples can be carried out using the time series package ITSM, the professional version of which is available at http://extras. springer.com. The data sets are contained in files with names ending in .TSM. For example, the Australian red wine sales are filed as WINE.TSM. Most of the topics covered in this chapter will be developed more fully in later sections of the book. The reader who is not already familiar with random variables and random vectors should first read Appendix A, where a concise account of the required background is given.

# 1.1 Examples of Time Series

A time series is a set of observations $x _ { t }$ , each one being recorded at a specific time t. A discrete-time time series (the type to which this book is primarily devoted) is one in which the set $T _ { 0 }$ of times at which observations are made is a discrete set, as is the case, for example, when observations are made at fixed time intervals. Continuoustime time series are obtained when observations are recorded continuously over some time interval, e.g., when $T _ { 0 } = [ 0 , 1 ]$ .

The Australian red wine sales, Jan. 1980–Oct. 1991

![](images/471d1afcf5f59916aa093f16a54c0f211ed7d4a199e24268dbd26449b52e13b5.jpg)  
Figure 1-1

# Example 1.1.1 Australian Red Wine Sales; WINE.TSM

Figure 1-1 shows the monthly sales (in kiloliters) of red wine by Australian winemakers from January 1980 through October 1991. In this case the set $T _ { 0 }$ consists of the 142 times (Jan. 1980), (Feb. 1980), …,(Oct. 1991) . Given a set of $n$ observations made at uniformly spaced time intervals, it is often convenient to rescale the time axis in such a way that $T _ { 0 }$ becomes the set of integers $\{ 1 , 2 , \ldots , n \}$ . In the present example this amounts to measuring time in months with (Jan. 1980) as month 1. Then $T _ { 0 }$ is the set $\{ 1 , 2 , \ldots , 1 4 2 \}$ . It appears from the graph that the sales have an upward trend and a seasonal pattern with a peak in July and a trough in January. To plot the data using ITSM, run the program by double-clicking on the ITSM icon and then select the option File>Project>Open>Univariate, click OK, and select the file WINE.TSM. The graph of the data will then appear on your screen.

# Example 1.1.2 All-Star Baseball Games, 1933–1995

Figure 1-2 shows the results of the all-star games by plotting $x _ { t }$ , where

$$
x _ {t} = \left\{ \begin{array}{l l} 1 & \text {i f t h e N a t i o n a l L e a g u e w o n i n y e a r t ,} \\ - 1 & \text {i f t h e A m e r i c a n L e a g u e w o n i n y e a r t .} \end{array} \right.
$$

This is a series with only two possible values, $\pm 1$ . It also has some missing values, since no game was played in 1945, and two games were scheduled for each of the years 1959–1962.

# Example 1.1.3 Accidental Deaths, U.S.A., 1973–1978; DEATHS.TSM

Like the red wine sales, the monthly accidental death figures show a strong seasonal pattern, with the maximum for each year occurring in July and the minimum for each year occurring in February. The presence of a trend in Figure 1-3 is much less apparent than in the wine sales. In Section 1.5 we shall consider the problem of representing the data as the sum of a trend, a seasonal component, and a residual term.

![](images/3367e3a50f963cebe5215069b95023ee5547ffc6ddfe608054d3267bddfa6df8.jpg)

![](images/c98adde77613bcd19eb2027c30571280ad791401267f9a37d0c47f7a9f7e36d1.jpg)  
Figure 1-2 Results of the all-star baseball games, 1933–1995

Figure 1-3   
![](images/7bbd5b81582dcaf597eecce01fddeb2776ddec65644a3b0c3a93d0215aea4289.jpg)  
The monthly accidental deaths data, 1973–1978

# Example 1.1.4 A Signal Detection Problem; SIGNAL.TSM

Figure 1-4 shows simulated values of the series

$$
X _ {t} = \cos \left(\frac {t}{1 0}\right) + N _ {t}, \quad t = 1, 2, \dots , 2 0 0,
$$

where $\{ N _ { t } \}$ is a sequence of independent normal random variables, with mean 0 and variance 0.25. Such a series is often referred to as signal plus noise, the signal being the smooth function, $\begin{array} { r } { S _ { t } = \cos ( \frac { t } { 1 0 } ) } \end{array}$ in this case. Given only the data $X _ { t }$ , how can we determine the unknown signal component? There are many approaches to this general problem under varying assumptions about the signal and the noise. One simple approach is to smooth the data by expressing $X _ { t }$ as a sum of sine waves of various frequencies (see Section 4.2) and eliminating the high-frequency components. If we do this to the values of $\{ X _ { t } \}$ shown in Figure 1-4 and retain only the lowest $3 . 5 \%$ of the frequency components, we obtain the estimate of the signal also shown as the red dashed line in Figure 1-4. The waveform of the signal is quite close to that of the true signal in this case, although its amplitude is somewhat smaller.

Figure 1-4 The series $\{ X _ { t } \}$ o f Example 1.1.4   
Figure 1-5   
![](images/8bf9e5e98f42274d364086a414827158fa2c09dc2476d14b04746db13494b684.jpg)  
Population of the U.S.A. at 10-year intervals, 1790–1990

![](images/272a230350f032be01d0dcb41a2729f2dafd138436cf8194c0a7d79e9e9611a8.jpg)

# Example 1.1.5 Population of the U.S.A., 1790–1990; USPOP.TSM

The population of the U.S.A., measured at 10-year intervals, is shown in Figure 1-5. The graph suggests the possibility of fitting a quadratic or exponential trend to the data. We shall explore this further in Section 1.3.

# Example 1.1.6 Number of Strikes Per Year in the U.S.A., 1951–1980; STRIKES.TSM

The annual numbers of strikes in the U.S.A. for the years 1951–1980 are shown in Figure 1-6. They appear to fluctuate erratically about a slowly changing level.

![](images/7d503e16d6e612b9ee1cec05ba8393448a224c23924fd60af60032549051f3bb.jpg)  
Figure 1-6 Strikes in the U.S.A., 1951–1980

# 1.2 Objectives of Time Series Analysis

The examples considered in Section 1.1 are an extremely small sample from the multitude of time series encountered in the fields of engineering, science, sociology, and economics. Our purpose in this book is to study techniques for drawing inferences from such series. Before we can do this, however, it is necessary to set up a hypothetical probability model to represent the data. After an appropriate family of models has been chosen, it is then possible to estimate parameters, check for goodness of fit to the data, and possibly to use the fitted model to enhance our understanding of the mechanism generating the series. Once a satisfactory model has been developed, it may be used in a variety of ways depending on the particular field of application.

The model may be used simply to provide a compact description of the data. We may, for example, be able to represent the accidental deaths data of Example 1.1.3 as the sum of a specified trend, and seasonal and random terms. For the interpretation of economic statistics such as unemployment figures, it is important to recognize the presence of seasonal components and to remove them so as not to confuse them with long-term trends. This process is known as seasonal adjustment. Other applications of time series models include separation (or filtering) of noise from signals as in Example 1.1.4, prediction of future values of a series such as the red wine sales in Example 1.1.1 or the population data in Example 1.1.5, testing hypotheses such as global warming using recorded temperature data, predicting one series from observations of another, e.g., predicting future sales using advertising expenditure data, and controlling future values of a series by adjusting parameters. Time series models are also useful in simulation studies. For example, the performance of a reservoir depends heavily on the random daily inputs of water to the system. If these are modeled as a time series, then we can use the fitted model to simulate a large number of independent sequences of daily inputs. Knowing the size and mode of operation of the reservoir, we can determine the fraction of the simulated input sequences that cause the reservoir to run out of water in a given time period. This fraction will then be an estimate of the probability of emptiness of the reservoir at some time in the given period.

# 1.3 Some Simple Time Series Models

An important part of the analysis of a time series is the selection of a suitable probability model (or class of models) for the data. To allow for the possibly unpredictable nature of future observations it is natural to suppose that each observation $x _ { t }$ is a realized value of a certain random variable $X _ { t }$ .

# Definition 1.3.1

A time series model for the observed data $\{ x _ { t } \}$ is a specification of the joint distributions (or possibly only the means and covariances) of a sequence of random variables $\{ X _ { t } \}$ of which $\{ x _ { t } \}$ is postulated to be a realization.

Remark . We shall frequently use the term time series to mean both the data and the process of which it is a realization. -

A complete probabilistic time series model for the sequence of random variables $\{ X _ { 1 } , X _ { 2 } , \ldots \}$ would specify all of the joint distributions of the random vectors $( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ , $n = 1 , 2 , \ldots$ , or equivalently all of the probabilities

$$
P \left[ X _ {1} \leq x _ {1}, \dots , X _ {n} \leq x _ {n} \right], \quad - \infty <   x _ {1}, \dots , x _ {n} <   \infty , \quad n = 1, 2, \dots .
$$

Such a specification is rarely used in time series analysis (unless the data are generated by some well-understood simple mechanism), since in general it will contain far too many parameters to be estimated from the available data. Instead we specify only the first- and second-order moments of the joint distributions, i.e., the expected values $E X _ { t }$ and the expected products $E ( X _ { t + h } X _ { t } )$ $E ( X _ { t + h } X _ { t } ) , t \ = \ 1 , 2 , . . . , h \ = \ 0 , 1 , 2 , . . . ,$ , focusing on properties of the sequence $\{ X _ { t } \}$ that depend only on these. Such properties of $\{ X _ { t } \}$ are referred to as second-order properties. In the particular case where all the joint distributions are multivariate normal, the second-order properties of $\{ X _ { t } \}$ completely determine the joint distributions and hence give a complete probabilistic characterization of the sequence. In general we shall lose a certain amount of information by looking at time series “through second-order spectacles”; however, as we shall see in Chapter 2, the theory of minimum mean squared error linear prediction depends only on the second-order properties, thus providing further justification for the use of the second-order characterization of time series models.

Figure 1-7 shows one of many possible realizations of $\{ S _ { t } , t = 1 , \ldots , 2 0 0 \}$ , where $\{ S _ { t } \}$ is a sequence of random variables specified in Example 1.3.3 below. In most practical problems involving time series we see only one realization. For example, there is only one available realization of Fort Collins’s annual rainfall for the years 1900–1996, but we imagine it to be one of the many sequences that might have occurred. In the following examples we introduce some simple time series models. One of our goals will be to expand this repertoire so as to have at our disposal a broad range of models with which to try to match the observed behavior of given data sets.

# 1.3.1 Some Zero-Mean Models

# Example 1.3.1 iid Noise

Perhaps the simplest model for a time series is one in which there is no trend or seasonal component and in which the observations are simply independent and identically distributed (iid) random variables with zero mean. We refer to such a sequence

of random variables $X _ { 1 } , X _ { 2 } , \dots$ as iid noise. By definition we can write, for any positive integer $n$ and real numbers $x _ { 1 } , \ldots , x _ { n }$ ,

$$
P \left[ X _ {1} \leq x _ {1}, \dots , X _ {n} \leq x _ {n} \right] = P \left[ X _ {1} \leq x _ {1} \right] \dots P \left[ X _ {n} \leq x _ {n} \right] = F \left(x _ {1}\right) \dots F \left(x _ {n}\right),
$$

where $F ( \cdot )$ is the cumulative distribution function (see Section A.1) of each of the identically distributed random variables $X _ { 1 } , X _ { 2 } , \ldots$ In this model there is no dependence between observations. In particular, for all $h \geq 1$ and all $x , x _ { 1 } , \ldots , x _ { n }$ ,

$$
P \left[ X _ {n + h} \leq x \mid X _ {1} = x _ {1}, \dots , X _ {n} = x _ {n} \right] = P \left[ X _ {n + h} \leq x \right],
$$

showing that knowledge of $X _ { 1 } , \ldots , X _ { n }$ is of no value for predicting the behavior of $X _ { n + h }$ . Given the values of $X _ { 1 } , \ldots , X _ { n }$ , the function $f$ that minimizes the mean squared error $E { \big [ } ( X _ { n + h } - f ( X _ { 1 } , \dots , X _ { n } ) ) ^ { 2 } { \big ] }$ is in fact identically zero (see Problem 1.2). Although this means that iid noise is a rather uninteresting process for forecasters, it plays an important role as a building block for more complicated time series models.

![](images/54b9e4d307b0d734730dba08219af0e7d0f937c6e732ac58c453c7a139780007.jpg)

# Example 1.3.2 A Binary Process

As an example of iid noise, consider the sequence of iid random variables $\{ X _ { t } , t =$ $1 , 2 , \ldots , \}$ with

$$
P [ X _ {t} = 1 ] = p, \quad P [ X _ {t} = - 1 ] = 1 - p,
$$

where $\begin{array} { r } { p = \frac { 1 } { 2 } } \end{array}$ . The time series obtained by tossing a penny repeatedly and scoring $+ 1$ for each head and $^ { - 1 }$ for each tail is usually modeled as a realization of this process. A priori we might well consider the same process as a model for the all-star baseball games in Example 1.1.2. However, even a cursory inspection of the results from 1963– 1982, which show the National League winning 19 of 20 games, casts serious doubt on the hypothesis $\begin{array} { r } { P [ X _ { t } = 1 ] = \frac { 1 } { 2 } } \end{array}$ .

![](images/57f92dfb37cd1ae3b4d2a38e31bc1ac8c1c278b9ed4413ff6074a8d688dd8bc0.jpg)

# Example 1.3.3 Random Walk

The random walk $\{ S _ { t } , t = 0 , 1 , 2 , \ldots \}$ (starting at zero) is obtained by cumulatively summing (or “integrating”) iid random variables. Thus a random walk with zero mean is obtained by defining $S _ { 0 } = 0$ and

$$
S _ {t} = X _ {1} + X _ {2} + \dots + X _ {t}, \quad \text {f o r} t = 1, 2, \dots ,
$$

where $\{ X _ { t } \}$ is iid noise. If $\{ X _ { t } \}$ is the binary process of Example 1.3.2, then $\{ S _ { t } , t =$ $0 , 1 , 2 , \ldots , \}$ is called a simple symmetric random walk. This walk can be viewed as the location of a pedestrian who starts at position zero at time zero and at each integer time tosses a fair coin, stepping one unit to the right each time a head appears and one unit to the left for each tail. A realization of length 200 of a simple symmetric random walk is shown in Figure 1-7. Notice that the outcomes of the coin tosses can be recovered from $\{ S _ { t } , t = 0 , 1 , . . . \}$ by differencing. Thus the result of the tth toss can be found from $S _ { t } - S _ { t - 1 } = X _ { t }$ .

![](images/a54d3bf3adbdd1be206e1db75f32d9b7b23f6d4cd6b3d3ad454c03f534366dbb.jpg)

# 1.3.2 Models with Trend and Seasonality

In several of the time series examples of Section 1.1 there is a clear trend in the data. An increasing trend is apparent in both the Australian red wine sales (Figure 1-1) and

![](images/b5b442482846787cf19491604bec54004c48dcf33457c596bf85c64c79c903c1.jpg)  
Figure 1-7 One realization of a simple random walk $\{ S _ { t } , t =$ $0 , 1 , 2 , \ldots , 2 0 0 \}$ 4

the population of the U.S.A. (Figure 1-5). In both cases a zero-mean model for the data is clearly inappropriate. The graph of the population data, which contains no apparent periodic component, suggests trying a model of the form

$$
X _ {t} = m _ {t} + Y _ {t},
$$

where $m _ { t }$ is a slowly changing function known as the trend component and $Y _ { t }$ has zero mean. A useful technique for estimating $m _ { t }$ is the method of least squares (some other methods are considered in Section 1.5).

In the least squares procedure we attempt to fit a parametric family of functions, e.g.,

$$
m _ {t} = a _ {0} + a _ {1} t + a _ {2} t ^ {2}, \tag {1.3.1}
$$

to the data $\{ x _ { 1 } , \ldots , x _ { n } \}$ by choosing the parameters, in this illustration $a _ { 0 } , a _ { 1 }$ , and $a _ { 2 }$ , to minimize $\scriptstyle \sum _ { t = 1 } ^ { n } ( x _ { t } - m _ { t } ) ^ { 2 }$ . This method of curve fitting is called least squares regres-=sion and can be carried out using the program ITSM and selecting the Regression option.

# Example 1.3.4 Population of the U.S.A., 1790–1990

To fit a function of the form (1.3.1) to the population data shown in Figure 1-5 we relabel the time axis so that $t ~ = ~ 1$ corresponds to 1790 and $t = 2 1$ corresponds to 1990. Run ITSM, select File>Project $>$ Open>Univariate, and open the file USPOP.TSM. Then select Regression $. >$ Specify, choose Polynomial Regression with order equal to 2, and click OK. Finally, selecting the option Regression $. >$ Estimation>Least Squares, gives the following estimated parameter values in the model (1.3.1):

$$
\hat {a} _ {0} = 6. 9 5 7 9 \times 1 0 ^ {6},
$$

$$
\hat {a} _ {1} = - 2. 1 5 9 9 \times 1 0 ^ {6},
$$

and

$$
\hat {a} _ {2} = 6. 5 0 6 3 \times 1 0 ^ {5}.
$$

![](images/184353eddbc8680f245181c52fe0c9c8a980f7dbd2ce6caa54421d130120ef65.jpg)  
Figure 1-8 Population of the U.S.A. showing the quadratic trend fitted by least squares

A graph of the fitted function is shown with the original data in Figure 1-8. The estimated values of the noise process $Y _ { t } , 1 \le t \le 2 1$ , are the residuals obtained by subtraction of $\hat { m } _ { t } = \hat { a } _ { 0 } + \hat { a } _ { 1 } t + \hat { a } _ { 2 } t ^ { 2 }$ from $x _ { t }$ .

The estimated trend component $\hat { m } _ { t }$ furnishes us with a natural predictor of future values of $X _ { t }$ . For example, if we estimate the noise $Y _ { 2 2 }$ by its mean value, i.e., zero, then (1.3.1) gives the estimated U.S. population for the year 2000 as

$$
\hat {m} _ {2 2} = 6. 9 5 7 9 \times 1 0 ^ {6} - 2. 1 5 9 9 \times 1 0 ^ {6} \times 2 2 + 6. 5 0 6 3 \times 1 0 ^ {5} \times 2 2 ^ {2} = 2 7 4. 3 5 \times 1 0 ^ {6}.
$$

However, if the residuals $\{ Y _ { t } \}$ are highly correlated, we may be able to use their values to give a better estimate of $Y _ { 2 2 }$ and hence of the population $X _ { 2 2 }$ in the year 2000.

# Example 1.3.5 Level of Lake Huron 1875–1972; LAKE.DAT

A graph of the level in feet of Lake Huron (reduced by 570) in the years 1875–1972 is displayed in Figure 1-9. Since the lake level appears to decline at a roughly linear rate, ITSM was used to fit a model of the form

$$
X _ {t} = a _ {0} + a _ {1} t + Y _ {t}, \quad t = 1, \dots , 9 8 \tag {1.3.2}
$$

(with the time axis relabeled as in Example 1.3.4). The least squares estimates of the parameter values are

$$
\hat {a} _ {0} = 1 0. 2 0 2 \quad \text {a n d} \quad \hat {a} _ {1} = - 0. 0 2 4 2.
$$

(The resulting least squares line, $\hat { a } _ { 0 } { + } \hat { a } _ { 1 } t$ , is also displayed in Figure 1-9.) The estimates of the noise, $Y _ { t }$ , in the model (1.3.2) are the residuals obtained by subtracting the least squares line from $x _ { t }$ and are plotted in Figure 1-10. There are two interesting features of the graph of the residuals. The first is the absence of any discernible trend. The second is the smoothness of the graph. (In particular, there are long stretches of residuals that have the same sign. This would be very unlikely to occur if the residuals were observations of iid noise with zero mean.) Smoothness of the graph of a time series is generally indicative of the existence of some form of dependence among the observations.

Such dependence can be used to advantage in forecasting future values of the series. If we were to assume the validity of the fitted model with iid residuals $\{ Y _ { t } \}$ , then

Figure 1-9

Level of Lake Huron 1875–1972 showing the line fitted by least squares

Figure 1-10   
![](images/10b74f1c461829653dd05fe654bc33fbcfcbcb61ac9ebaaa4d5a6eef56d90995.jpg)  
Residuals from fitting a line to the Lake Huron data in Figure 1-9

![](images/bf50e0648b24a996826a07e85fe34c994af19fd45f7fdbb622b2873973a15b8f.jpg)

the minimum mean squared error predictor of the next residual $( Y _ { 9 9 } )$ would be zero (by Problem 1.2). However, Figure 1-10 strongly suggests that $Y _ { 9 9 }$ will be positive.

How then do we quantify dependence, and how do we construct models for forecasting that incorporate dependence of a particular type? To deal with these questions, Section 1.4 introduces the autocorrelation function as a measure of dependence, and stationary processes as a family of useful models exhibiting a wide variety of dependence structures.

# Harmonic Regression

Many time series are influenced by seasonally varying factors such as the weather, the effect of which can be modeled by a periodic component with fixed known period. For example, the accidental deaths series (Figure 1-3) shows a repeating annual pattern with peaks in July and troughs in February, strongly suggesting a seasonal factor with period 12. In order to represent such a seasonal effect, allowing for noise but assuming no trend, we can use the simple model,

$$
X _ {t} = s _ {t} + Y _ {t},
$$

where $s _ { t }$ is a periodic function of $t$ with period d $( s _ { t - d } = s _ { t }$ ). A convenient choice for $s _ { t }$ is a sum of harmonics (or sine waves) given by

$$
s _ {t} = a _ {0} + \sum_ {j = 1} ^ {k} \left(a _ {j} \cos \left(\lambda_ {j} t\right) + b _ {j} \sin \left(\lambda_ {j} t\right)\right), \tag {1.3.3}
$$

where $a _ { 0 } , a _ { 1 } , \ldots , a _ { k }$ and $b _ { 1 } , \ldots , b _ { k }$ are unknown parameters and $\lambda _ { 1 } , \ldots , \lambda _ { k }$ are fixed frequencies, each being some integer multiple of $2 \pi / d$ . To carry out harmonic regression using ITSM, select Regression $. >$ Specify, and check the two boxes, Include intercept term and Harmonic Regression. Then specify the number of harmonics $[ k$ in equation (1.3.3)] and enter $k$ integer-valued Fourier indices $f _ { 1 } , \ldots , f _ { k }$ . For a sine wave with period $d$ , set $f _ { 1 } = n / d$ , where $n$ is the number of observations in the time series. (If $n / d$ is not an integer, you will need to delete a few observations from the beginning of the series to make it so.) The other $k - 1$ Fourier indices should be positive integer multiples of the first, corresponding to harmonics of the fundamental sine wave with period $d$ . Thus to fit a single sine wave with period 365 to 365 daily observations we would choose $k = 1$ and $f _ { 1 } = 1$ . To fit a linear combination of sine waves with periods $3 6 5 / j , j = 1 , \ldots , 4$ , we would choose $k = 4$ and $f _ { j } = j , j = 1 , \ldots , 4$ . Once $k$ and the frequencies $f _ { 1 } , \ldots , f _ { k }$ have been specified, click OK and then select Regression $. >$ Estimation>Least Squares to obtain the required coefficients. To see how well the fitted function matches the data, select Regression>Show fit.

# Example 1.3.6 Accidental Deaths

To fit a sum of two harmonics with periods 12 months and 6 months to the monthly accidental deaths data $x _ { 1 } , \ldots , x _ { n }$ with $n = 7 2$ , we choose $k = 2$ , $f _ { 1 } = n / 1 2 = 6$ , and $f _ { 2 } = n / 6 = 1 2$ . Using ITSM as described above, we obtain the fitted function shown in Figure 1-11. As can be seen from the figure, the periodic character of the series is captured reasonably well by this fitted function. In practice, it is worth experimenting with several different combinations of harmonics in order to find a satisfactory estimate of the seasonal component. The program ITSM also allows fitting a linear combination

![](images/f44df9bb2e9580d76aef76aa3b2e9d5944c393681e3118d19accb93325a025b1.jpg)  
Figure 1-11 The estimated harmonic component of the accidental deaths data from ITSM

of harmonics and polynomial trend by checking both Harmonic Regression and Polynomial Regression in the Regression $. >$ Specificationdialog box. Other methods for dealing with seasonal variation in the presence of trend are described in Section 1.5.

![](images/e630012ff8066ea853cac0bfb97a9abbe0de0ea0b7e72a9685880b2f56722fa3.jpg)

# 1.3.3 A General Approach to Time Series Modeling

The examples of the previous section illustrate a general approach to time series analysis that will form the basis for much of what is done in this book. Before introducing the ideas of dependence and stationarity, we outline this approach to provide the reader with an overview of the way in which the various ideas of this chapter fit together.

• Plot the series and examine the main features of the graph, checking in particular whether there is

(a) a trend,   
(b) a seasonal component,   
(c) any apparent sharp changes in behavior,   
(d) any outlying observations.

• Remove the trend and seasonal components to get stationary residuals (as defined in Section 1.4). To achieve this goal it may sometimes be necessary to apply a preliminary transformation to the data. For example, if the magnitude of the fluctuations appears to grow roughly linearly with the level of the series, then the transformed series $\{ \ln X _ { 1 } , \ldots , \ln X _ { n } \}$ will have fluctuations of more constant magnitude. See, for example, Figures 1-1 and 1-17. (If some of the data are negative, add a positive constant to each of the data values to ensure that all values are positive before taking logarithms.) There are several ways in which trend and seasonality can be removed (see Section 1.5), some involving estimating the components and subtracting them from the data, and others depending on differencing the data, i.e., replacing the original series $\{ X _ { t } \}$ by $\{ Y _ { t } : = X _ { t } - X _ { t - d } \}$ for some positive integer $d$ . Whichever method is used, the aim is to produce a stationary series, whose values we shall refer to as residuals.   
Choose a model to fit the residuals, making use of various sample statistics including the sample autocorrelation function to be defined in Section 1.4.   
Forecasting will be achieved by forecasting the residuals and then inverting the transformations described above to arrive at forecasts of the original series $\{ X _ { t } \}$ .   
• An extremely useful alternative approach touched on only briefly in this book is to express the series in terms of its Fourier components, which are sinusoidal waves of different frequencies (cf. Example 1.1.4). This approach is especially important in engineering applications such as signal processing and structural design. It is important, for example, to ensure that the resonant frequency of a structure does not coincide with a frequency at which the loading forces on the structure have a particularly large component.

# 1.4 Stationary Models and the Autocorrelation Function

Loosely speaking, a time series $\{ X _ { t } , t = 0 , \pm 1 , . . . \}$ is said to be stationary if it has statistical properties similar to those of the “time-shifted” series $\{ X _ { t + h } , t = 0 , \pm 1 , . . . \}$ , for each integer $h$ . Restricting attention to those properties that depend only on the first- and second-order moments of $\{ X _ { t } \}$ , we can make this idea precise with the following definitions.

# Definition 1.4.1

Let $\{ X _ { t } \}$ be a time series with $E ( X _ { t } ^ { 2 } ) < \infty$ . The mean function of $\{ X _ { t } \}$ is

$$
\mu_ {X} (t) = E \left(X _ {t}\right).
$$

The covariance function of $\{ X _ { t } \}$ is

$$
\gamma_ {X} (r, s) = \operatorname {C o v} \left(X _ {r}, X _ {s}\right) = E \left[ \left(X _ {r} - \mu_ {X} (r)\right) \left(X _ {s} - \mu_ {X} (s)\right) \right]
$$

for all integers $r$ and $s$ .

# Definition 1.4.2

$\{ X _ { t } \}$ is (weakly) stationary if

(i) $\mu _ { X } ( t )$ is independent of $t$

and

(ii) $\gamma _ { X } ( t + h , t )$ is independent of t for each h.

Remark 1. Strict stationarity of a time series $\{ X _ { t } , t = 0 , \pm 1 , . . . \}$ is defined by the condition that $( X _ { 1 } , \ldots , X _ { n } )$ and $( X _ { 1 + h } , \ldots , X _ { n + h } )$ have the same joint distributions for all integers $h$ and $n > 0$ . It is easy to check that if $\{ X _ { t } \}$ is strictly stationary and $E X _ { t } ^ { 2 } <$ $\infty$ for all $t$ , then $\{ X _ { t } \}$ is also weakly stationary (Problem 1.3). Whenever we use the term stationary we shall mean weakly stationary as in Definition 1.4.2, unless we specifically indicate otherwise. -

Remark 2. In view of condition (ii), whenever we use the term covariance function with reference to a stationary time series $\{ X _ { t } \}$ we shall mean the function $\gamma _ { X }$ of one variable, defined by

$$
\gamma_ {X} (h) := \gamma_ {X} (h, 0) = \gamma_ {X} (t + h, t).
$$

The function $\gamma _ { X } ( \cdot )$ will be referred to as the autocovariance function and $\gamma _ { X } ( h )$ as its value at lag $h$ . -

# Definition 1.4.3

Let $\{ X _ { t } \}$ be a stationary time series. The autocovariance function (ACVF) of $\{ X _ { t } \}$ at lag $h$ is

$$
\gamma_ {X} (h) = \operatorname {C o v} \left(X _ {t + h}, X _ {t}\right).
$$

The autocorrelation function (ACF) of $\{ X _ { t } \}$ at lag $h$ is

$$
\rho_ {X} (h) \equiv \frac {\gamma_ {X} (h)}{\gamma_ {X} (0)} = \operatorname {C o r} (X _ {t + h}, X _ {t}).
$$

In the following examples we shall frequently use the easily verified linearity property of covariances, that if $\mathrm { E } X ^ { 2 } < \infty$ , $\mathrm { E } Y ^ { 2 } < \infty$ , $\mathrm { E } Z ^ { 2 } < \infty$ and a, b, and $c$ are any real constants, then

$$
\operatorname {C o v} (a X + b Y + c, Z) = a \operatorname {C o v} (X, Z) + b \operatorname {C o v} (Y, Z).
$$

# Example 1.4.1 iid Noise

If $\{ X _ { t } \}$ is iid noise and $E ( X _ { t } ^ { 2 } ) ~ = ~ \sigma ^ { 2 } ~ < ~ \infty$ , then the first requirement of Definition 1.4.2 is obviously satisfied, since $E ( X _ { t } ) ~ = ~ 0$ for all t. By the assumed independence,

$$
\gamma_ {X} (t + h, t) = \left\{ \begin{array}{l l} \sigma^ {2}, & \text {i f} h = 0, \\ 0, & \text {i f} h \neq 0, \end{array} \right.
$$

which does not depend on t. Hence iid noise with finite second moment is stationary. We shall use the notation

$$
\left\{X _ {t} \right\} \sim \operatorname {I I D} \left(0, \sigma^ {2}\right)
$$

to indicate that the random variables $X _ { t }$ are independent and identically distributed random variables, each with mean 0 and variance $\sigma ^ { 2 }$ .

![](images/640085f3361dd0d7defcf0eac5285fd72e57af1b0a48f578bad47c1a0a7541ac.jpg)

# Example 1.4.2 White Noise

If $\{ X _ { t } \}$ is a sequence of uncorrelated random variables, each with zero mean and variance $\sigma ^ { 2 }$ , then clearly $\{ X _ { t } \}$ is stationary with the same covariance function as the iid noise in Example 1.4.1. Such a sequence is referred to as white noise (with mean 0 and variance $\sigma ^ { 2 }$ ). This is indicated by the notation

$$
\left\{X _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

Clearly, every $\operatorname { I I D } ( 0 , \sigma ^ { 2 } )$ sequence is $\operatorname { W N } ( 0 , \sigma ^ { 2 } )$ but not conversely (see Problem 1.8 and the ARCH(1) process of Section 11.3).

![](images/f56ecebe51caf5d904cc92d5de177e31ec97a96a298466bfbef0b5752d4fdea2.jpg)

# Example 1.4.3 The Random Walk

If $\{ S _ { t } \}$ is the random walk defined in Example 1.3.3 with $\{ X _ { t } \}$ as in Example 1.4.1, then $E S _ { t } = 0$ , $E ( S _ { t } ^ { 2 } ) = t \sigma ^ { 2 } < \infty$ for all $t$ , and, for $h \geq 0$ ,

$$
\begin{array}{l} \gamma_ {S} (t + h, t) = \operatorname {C o v} \left(S _ {t + h}, S _ {t}\right) \\ = \operatorname {C o v} \left(S _ {t} + X _ {t + 1} + \dots + X _ {t + h}, S _ {t}\right) \\ = \operatorname {C o v} \left(S _ {t}, S _ {t}\right) \\ = t \sigma^ {2}. \\ \end{array}
$$

Since $\gamma _ { S } ( t + h , t )$ depends on $t$ , the series $\{ S _ { t } \}$ is not stationary.

![](images/590b1b238d1b8345276f05432df2599b971d55d447293360d8c6193b16e85987.jpg)

# Example 1.4.4 First-Order Moving Average or MA(1) Process

Consider the series defined by the equation

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1}, \quad t = 0, \pm 1, \dots , \tag {1.4.1}
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } \left( 0 , \sigma ^ { 2 } \right)$ and $\theta$ is a real-valued constant. From (1.4.1) we see that $E X _ { t } = 0$ , $E X _ { t } ^ { 2 } = \sigma ^ { 2 } ( 1 + \theta ^ { 2 } ) < \infty$ , and

$$
\gamma_ {X} (t + h, t) = \left\{ \begin{array}{l l} \sigma^ {2} \left(1 + \theta^ {2}\right), & \text {i f} h = 0, \\ \sigma^ {2} \theta , & \text {i f} h = \pm 1, \\ 0, & \text {i f} | h | > 1. \end{array} \right.
$$

Thus the requirements of Definition 1.4.2 are satisfied, and $\{ X _ { t } \}$ is stationary. The autocorrelation function of $\{ X _ { t } \}$ is

$$
\rho_ {X} (h) = \left\{ \begin{array}{l l} 1, & \text {i f} h = 0, \\ \theta / \left(1 + \theta^ {2}\right), & \text {i f} h = \pm 1, \\ 0, & \text {i f} | h | > 1. \end{array} \right. \tag {1.4.2}
$$

![](images/af79e83a13e6bf11267c8f76a7a0f486e521fdee0089ba8c7e1f75cccc45d281.jpg)

# Example 1.4.5 First-Order Autoregression or AR(1) Process

Let us assume now that $\{ X _ { t } \}$ is a stationary series satisfying the equations

$$
X _ {t} = \phi X _ {t - 1} + Z _ {t}, \quad t = 0, \pm 1, \dots , \tag {1.4.3}
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , \sigma ^ { 2 } )$ , $| \phi | < 1$ , and $Z _ { t }$ is uncorrelated with $X _ { s }$ for each $s < t$ . (We shall show in Section 2.2 that there is in fact exactly one such solution of (1.4.3).) By taking expectations on each side of (1.4.3) and using the fact that $E Z _ { t } = 0$ , we see at once that

$$
E X _ {t} = 0.
$$

To find the autocorrelation function of $\{ X _ { t } \}$ we multiply each side of (1.4.3) by $X _ { t - h }$ $( h > 0 )$ and then take expectations to get

$$
\begin{array}{l} \gamma_ {X} (h) = \operatorname {C o v} \left(X _ {t}, X _ {t - h}\right) \\ = \operatorname {C o v} \left(\phi X _ {t - 1}, X _ {t - h}\right) + \operatorname {C o v} \left(Z _ {t}, X _ {t - h}\right) \\ = \phi \gamma_ {X} (h - 1) + 0 = \dots = \phi^ {h} \gamma_ {X} (0). \\ \end{array}
$$

Observing that $\gamma ( h ) = \gamma ( - h )$ and using Definition 1.4.3, we find that

$$
\rho_ {X} (h) = \frac {\gamma_ {X} (h)}{\gamma_ {X} (0)} = \phi^ {| h |}, \quad h = 0, \pm 1, \dots .
$$

It follows from the linearity of the covariance function in each of its arguments and the fact that $Z _ { t }$ is uncorrelated with $X _ { t - 1 }$ that

$$
\gamma_ {X} (0) = \operatorname {C o v} \left(X _ {t}, X _ {t}\right) = \operatorname {C o v} \left(\phi X _ {t - 1} + Z _ {t}, \phi X _ {t - 1} + Z _ {t}\right) = \phi^ {2} \gamma_ {X} (0) + \sigma^ {2}
$$

and hence that $\gamma _ { X } ( 0 ) = \sigma ^ { 2 } / \left( 1 - \phi ^ { 2 } \right)$ .

![](images/521223e8abbae4c25b0809346d51c7e9205f1cd615ecd03a372b783bb52630a8.jpg)

# 1.4.1 The Sample Autocorrelation Function

Although we have just seen how to compute the autocorrelation function for a few simple time series models, in practical problems we do not start with a model, but with observed data $\{ x _ { 1 } , x _ { 2 } , \ldots , x _ { n } \}$ . To assess the degree of dependence in the data and to select a model for the data that reflects this, one of the important tools we use is the sample autocorrelation function (sample ACF) of the data. If we believe that the data are realized values of a stationary time series $\{ X _ { t } \}$ , then the sample ACF will provide us with an estimate of the ACF of $\{ X _ { t } \}$ . This estimate may suggest which of the many possible stationary time series models is a suitable candidate for representing the dependence in the data. For example, a sample ACF that is close to zero for all nonzero lags suggests that an appropriate model for the data might be iid noise. The following definitions are natural sample analogues of those for the autocovariance and autocorrelation functions given earlier for stationary time series models.

# Definition 1.4.4

Let x1, . . . , xn be observations of a time series. The sample mean of $x _ { 1 } , \ldots , x _ { n }$ is

$$
\bar {x} = \frac {1}{n} \sum_ {t = 1} ^ {n} x _ {t}.
$$

The sample autocovariance function is

$$
\hat {\gamma} (h) := n ^ {- 1} \sum_ {t = 1} ^ {n - | h |} (x _ {t + | h |} - \bar {x}) (x _ {t} - \bar {x}), \quad - n <   h <   n.
$$

The sample autocorrelation function is

$$
\hat {\rho} (h) = \frac {\hat {\gamma} (h)}{\hat {\gamma} (0)}, \quad - n <   h <   n.
$$

Remark 3. $\mathrm { F o r } h \ge 0 , \hat { \gamma } ( h )$ is approximately equal to the sample covariance of the $n -$ $h$ pairs of observations $( x _ { 1 } , x _ { 1 + h } )$ , (x2, x2 h), . . . , $( x _ { n - h } , x _ { n } )$ . The difference arises from use of the divisor $n$ instead of $n - h$ and the subtraction of the overall mean, $\bar { x }$ , from each factor of the summands. Use of the divisor $n$ ensures that the sample covariance matrix $\hat { \Gamma } _ { n } : = [ \hat { \gamma } ( i - j ) ] _ { i , j = 1 } ^ { n }$ is nonnegative definite (see Section 2.4.2).

Remark 4. Like the sample covariance matrix defined in Remark 3, the sample correlation matrix $\hat { R } _ { n } : = \hat { [ \rho ( i - j ) ] } _ { i , j = 1 } ^ { n }$ is nonnegative definite. Each of its diagonal elements is equal to 1, since $\hat { \rho } ( 0 ) = \mathrm { \bar { 1 } }$ . -

# Example 1.4.6

Figure 1-12 shows a simulated sequence of 200 iid normal random variables with mean 0 and variance 1 (called an IID $\mathrm { N } ( 0 , 1 )$ sequence). Figure 1-13 shows the corresponding sample autocorrelation function at lags 0, 1, . . . , 40. Since $\rho ( h ) = 0$ for $h > 0$ , one would also expect the corresponding sample autocorrelations to be near 0. It can be shown, in fact, that for iid noise with finite variance, the sample autocorrelations $\hat { \rho } ( h )$ , $h \ > \ 0$ , are approximately IID $\mathrm { N } ( 0 , 1 / n )$ for $n$ large (see Brockwell and Davis (1991) p. 222). Hence, approximately $9 5 \%$ of the sample autocorrelations should fall between the bounds $\pm 1 . 9 6 / \sqrt { n }$ (since 1.96 is the 0.975 quantile of the

![](images/07077b84adf6dd91ae26d5caf1d184a51c802781d2eac4b2f9c1fbedc7349058.jpg)  
Figure 1-12 200 simulated values of IID N(0,1) noise

![](images/6ab4f8fd18378a14c90ce19e667803cf40fd63737baad8e5c4326cad92c572e6.jpg)  
Figure 1-13 The sample autocorrelation function for the data of Figure 1-12 showing the bounds $\pm 1 . 9 6 { \bar { / } } { \sqrt { n } }$

standard normal distribution). Therefore, in Figure 1-13 we would expect roughly $4 0 ( 0 . 0 5 ) \ = \ 2$ values to fall outside the bounds. To simulate IID $\mathrm { N } ( 0 , 1 )$ noise in ITSM, select File>Project>New>Univariate then Model>Simulate. In the resulting dialog box, enter 200 for the required Number of Observations. (The remaining entries in the dialog box can be left as they are, since the model assumed by ITSM, until you enter another, is IID $\mathrm { \Delta N } ( 0 , 1 )$ noise. If you wish to reproduce exactly the same sequence at a later date, record the Random Number Seed for later use. By specifying different values for the random number seed you can generate independent realizations of your time series.) Click on OK and you will see the graph of your simulated series. To see its sample autocorrelation function together with the autocorrelation function of the model that generated it, click on the third yellow button at the top of the screen and you will see the two graphs superimposed (with the latter in red.) The horizontal lines on the graph are the bounds $\pm 1 . 9 6 / \sqrt { n }$ .

-

The sample autocorrelation function for the Australian red wine sales showing the bounds $\pm 1 . 9 6 { \bar { / } } { \sqrt { n } }$

![](images/ef690375e205df47e622c8ef226010149f1c0296cbdbe73b2a35ef58e0e228e4.jpg)  
Figure 1-14

Remark 5. The sample autocovariance and autocorrelation functions can be computed for any data set $\{ x _ { 1 } , \ldots , x _ { n } \}$ and are not restricted to observations from a stationary time series. For data containing a trend, $| \hat { \rho } ( h ) |$ will exhibit slow decay as $h$ increases, and for data with a substantial deterministic periodic component, $| \hat { \rho } ( h ) |$ will exhibit similar behavior with the same periodicity. (See the sample ACF of the Australian red wine sales in Figure 1-14 and Problem 1.9.) Thus $\hat { \rho } ( \cdot )$ can be useful as an indicator of nonstationarity (see also Section 6.1). -

# 1.4.2 A Model for the Lake Huron Data

As noted earlier, an iid noise model for the residuals $\{ y _ { 1 } , \dots , y _ { 9 8 } \}$ obtained by fitting a straight line to the Lake Huron data in Example 1.3.5 appears to be inappropriate. This conclusion is confirmed by the sample ACF of the residuals (Figure 1-15), which has three of the first 40 values well outside the bounds $\pm 1 . 9 6 / \sqrt { 9 8 }$ .

The roughly geometric decay of the first few sample autocorrelations (with $\hat { \rho } ( h + 1 ) / \hat { \rho } ( h ) \approx 0 . 7 )$ suggests that an AR(1) series (with $\phi ~ \approx ~ 0 . 7 )$ ) might provide a reasonable model for these residuals. (The form of the ACF for an AR(1) process was computed in Example 1.4.5.)

To explore the appropriateness of such a model, consider the points $( y _ { 1 } , y _ { 2 } )$ , $( y _ { 2 } , y _ { 3 } ) , \dots , ( y _ { 9 7 } , y _ { 9 8 } )$ plotted in Figure 1-16. The graph does indeed suggest a linear relationship between $y _ { t }$ and $y _ { t - 1 }$ . Using simple least squares estimation to fit a straight line of the form $y _ { t } = a y _ { t - 1 }$ , we obtain the model

$$
Y _ {t} = 0. 7 9 1 Y _ {t - 1} + Z _ {t}, \tag {1.4.4}
$$

where $\{ Z _ { t } \}$ is iid noise with variance $\textstyle \sum _ { t = 2 } ^ { 9 8 } ( y _ { t } - 0 . 7 9 1 y _ { t - 1 } ) ^ { 2 } / 9 7 = 0 . 5 0 2 4$ . The sample ACF of the estimated noise sequence $z _ { t } = y _ { t } - 0 . 7 9 1 y _ { t - 1 }$ , t  2, . . . , 98, is slightly outside the bounds $\pm 1 . 9 6 / \sqrt { 9 7 }$ at lag 1 $( \hat { \rho } ( 1 ) = 0 . 2 1 6 )$ , but it is inside the bounds for all other lags up to 40. This check that the estimated noise sequence is consistent with the iid assumption of (1.4.3) reinforces our belief in the fitted model. More goodness

Figure 1-15 The sample autocorrelation function for the Lake Huron residuals of Figure 1-10 showing the bounds $\pm 1 . 9 6 { \bar { / } } { \sqrt { n } }$   
Figure 1-16   
![](images/a5dee52f2b6d5e23fc8537adbf8c9b46f645ac84836dcfe6081e815b28f46592.jpg)  
Scatter plot of $( y _ { t - 1 } , y _ { t } ) ,$ $t = 2 , \ldots , 9 8 ,$ , for the data in Figure 1-10 showing the least squares regression line $\gamma = 0 . 7 9 1 x$

![](images/8c21e092b9f3f3cedd565194cbd331b7824fbb8a9caeec7a3373d39dae63a54c.jpg)

of fit tests for iid noise sequences are described in Section 1.6. The estimated noise sequence $\{ z _ { t } \}$ in this example passes them all, providing further support for the model (1.4.3).

A better fit to the residuals in equation (1.3.2) is provided by the second-order autoregression

$$
Y _ {t} = \phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + Z _ {t}, \tag {1.4.5}
$$

where $\{ Z _ { t } \}$ is iid noise with variance $\sigma ^ { 2 }$ . This is analogous to a linear model in which $Y _ { t }$ is regressed on the previous two values $Y _ { t - 1 }$ and $Y _ { t - 2 }$ of the time series. The least squares estimates of the parameters $\phi _ { 1 }$ and $\phi _ { 2 }$ , found by minimizing $\sum _ { t = 3 } ^ { 9 8 } ( y _ { t } \mathrm { ~ - ~ }$ $\phi _ { 1 } y _ { t - 1 } - \phi _ { 2 } y _ { t - 2 } ) ^ { 2 }$ , are $\hat { \phi } _ { 1 } ~ = ~ 1 . 0 0 2$ and $\hat { \phi } _ { 2 } ~ = ~ - 0 . 2 8 3 4$ . The estimate of $\sigma ^ { 2 }$ is $\begin{array} { r } { \hat { \sigma } ^ { 2 } = \sum _ { t = 3 } ^ { 9 8 } ( y _ { t } - \hat { \phi } _ { 1 } y _ { t - 1 } - \hat { \phi } _ { 2 } y _ { t - 2 } ) ^ { 2 } / 9 6 = 0 . 4 4 6 0 . } \end{array}$ , which is approximately $11 \%$ smaller than the estimate of the noise variance for the AR(1) model (1.4.3). The improved fit is indicated by the sample ACF of the estimated residuals, $y _ { t } - \hat { \phi } _ { 1 } y _ { t - 1 } - \hat { \phi } _ { 2 } y _ { t - 2 }$ , which falls well within the bounds $\pm 1 . 9 6 / \sqrt { 9 6 }$ for all lags up to 40.

# 1.5 Estimation and Elimination of Trend and Seasonal Components

The first step in the analysis of any time series is to plot the data. If there are any apparent discontinuities in the series, such as a sudden change of level, it may be advisable to analyze the series by first breaking it into homogeneous segments. If there are outlying observations, they should be studied carefully to check whether there is any justification for discarding them (as for example if an observation has been incorrectly recorded). Inspection of a graph may also suggest the possibility of representing the data as a realization of the process (the classical decomposition model)

$$
X _ {t} = m _ {t} + s _ {t} + Y _ {t}, \tag {1.5.1}
$$

where $m _ { t }$ is a slowly changing function known as a trend component, $s _ { t }$ is a function with known period $d$ referred to as a seasonal component, and $Y _ { t }$ is a random noise component that is stationary in the sense of Definition 1.4.2. If the seasonal and noise fluctuations appear to increase with the level of the process, then a preliminary transformation of the data is often used to make the transformed data more compatible with the model (1.5.1). Compare, for example, the red wine sales in Figure 1-1 with the transformed data, Figure 1-17, obtained by applying a logarithmic transformation. The transformed data do not exhibit the increasing fluctuation with increasing level that was apparent in the original data. This suggests that the model (1.5.1) is more appropriate for the transformed than for the original series. In this section we shall assume that the model (1.5.1) is appropriate (possibly after a preliminary transformation of the data) and examine some techniques for estimating the components mt, st, and $Y _ { t }$ in the model.

Our aim is to estimate and extract the deterministic components $m _ { t }$ and $s _ { t }$ in the hope that the residual or noise component $Y _ { t }$ will turn out to be a stationary time series. We can then use the theory of such processes to find a satisfactory probabilistic model for the process $Y _ { t }$ , to analyze its properties, and to use it in conjunction with $m _ { t }$ and $s _ { t }$ for purposes of prediction and simulation of $\{ X _ { t } \}$ .

Another approach, developed extensively by Box and Jenkins (1976), is to apply differencing operators repeatedly to the series $\{ X _ { t } \}$ until the differenced observations resemble a realization of some stationary time series $\{ W _ { t } \}$ . We can then use the theory

![](images/6e156daca23616663dd7b2877f2835fdf89dcdc473f5ae73d7e7f7a4449ce7c3.jpg)  
Figure 1-17 The natural logarithms of the red wine data

of stationary processes for the modeling, analysis, and prediction of $\{ W _ { t } \}$ and hence of the original process. The various stages of this procedure will be discussed in detail in Chapters 5 and 6.

The two approaches to trend and seasonality removal, (1) by estimation of $m _ { t }$ and $s _ { t }$ in (1.5.1) and (2) by differencing the series $\{ X _ { t } \}$ , will now be illustrated with reference to the data introduced in Section 1.1.

# 1.5.1 Estimation and Elimination of Trend in the Absence of Seasonality

In the absence of a seasonal component the model (1.5.1) becomes the following.

# Nonseasonal Model with Trend:

$$
X _ {t} = m _ {t} + Y _ {t}, \quad t = 1, \dots , n, \tag {1.5.2}
$$

where $E Y _ { t } = 0$ .

(If $E Y _ { t } \neq 0$ , then we can replace $m _ { t }$ and $Y _ { t }$ in (1.5.2) with $m _ { t } + E Y _ { t }$ and $Y _ { t } - E Y _ { t }$ , respectively.)

# Method 1: Trend Estimation

Moving average and spectral smoothing are essentially nonparametric methods for trend (or signal) estimation and not for model building. Special smoothing filters can also be designed to remove periodic components as described under Method S1 below. The choice of smoothing filter requires a certain amount of subjective judgment, and it is recommended that a variety of filters be tried in order to get a good idea of the underlying trend. Exponential smoothing, since it is based on a moving average of past values only, is often used for forecasting, the smoothed value at the present time being used as the forecast of the next value.

To construct a model for the data (with no seasonality) there are two general approaches, both available in ITSM. One is to fit a polynomial trend (by least squares) as described in Method 1(d) below, then to subtract the fitted trend from the data and to find an appropriate stationary time series model for the residuals. The other is to eliminate the trend by differencing as described in Method 2 and then to find an appropriate stationary model for the differenced series. The latter method has the advantage that it usually requires the estimation of fewer parameters and does not rest on the assumption of a trend that remains fixed throughout the observation period. The study of the residuals (or of the differenced series) is taken up in Section 1.6.

(a) Smoothing with a finite moving average filter. Let $q$ be a nonnegative integer and consider the two-sided moving average

$$
W _ {t} = (2 q + 1) ^ {- 1} \sum_ {j = - q} ^ {q} X _ {t - j} \tag {1.5.3}
$$

of the process $\{ X _ { t } \}$ defined by (1.5.2). Then for $q + 1 \leq t \leq n - q$

$$
W _ {t} = (2 q + 1) ^ {- 1} \sum_ {j = - q} ^ {q} m _ {t - j} + (2 q + 1) ^ {- 1} \sum_ {j = - q} ^ {q} Y _ {t - j} \approx m _ {t}, \tag {1.5.4}
$$

assuming that $m _ { t }$ is approximately linear over the interval $[ t - q , t + q ]$ and that the average of the error terms over this interval is close to zero (see Problem 1.11).

# Figure 1-18

Simple 5-term moving average $\hat { m } _ { t }$ of the strike data from Figure 1-6

![](images/39ce88c1b3ae81eb58d1566cd5aea6b71fe55ac654ba39398954b13a36ac7b79.jpg)

The moving average thus provides us with the estimates

$$
\hat {m} _ {t} = (2 q + 1) ^ {- 1} \sum_ {j = - q} ^ {q} X _ {t - j}, \quad q + 1 \leq t \leq n - q. \tag {1.5.5}
$$

Since $X _ { t }$ is not observed for $t ~ \leq ~ 0$ or $t \ > \ n$ , we cannot use (1.5.5) for $t ~ \le ~ q$ or $t ~ > ~ n - q$ . The program ITSM deals with this problem by defining $X _ { t } ~ : = { \cal X } _ { 1 }$ for $t < 1$ and $X _ { t } : = X _ { n }$ for $t > n$ .

Example 1.5.1 The result of applying the moving-average filter (1.5.5) with $q = 2$ to the strike data of Figure 1-6 is shown in Figure 1-18. The estimated noise terms $\hat { Y } _ { t } = X _ { t } - \hat { m } _ { t }$ are shown in Figure 1-19. As expected, they show no apparent trend. To apply this filter using ITSM, open the project STRIKES.TSM, select Smooth>Moving Average, specify 2 for the filter order, and enter the weights 1,1,1 for Theta(0), Theta(1), and Theta(2) (these are automatically normalized so that the sum of the weights is one). Then click OK.

It is useful to think of $\{ \hat { m } _ { t } \}$ in (1.5.5) as a process obtained from $\{ X _ { t } \}$ by application of a linear operator or linear filter $\begin{array} { r } { \hat { m } _ { t } ~ = ~ \sum _ { j = - \infty } ^ { \infty } a _ { j } X _ { t - j } } \end{array}$ with weights $a _ { j } = ( 2 q +$ $1 ) ^ { - 1 }$ , $- q \le j \le q$ . This particular filter is a low-pass filter in the sense that it takes the data $\{ X _ { t } \}$ and removes from it the rapidly fluctuating (or high frequency) component $\{ \hat { Y } _ { t } \}$ to leave the slowly varying estimated trend term $\{ \hat { m } _ { t } \}$ (see Figure 1-20).

The particular filter (1.5.5) is only one of many that could be used for smoothing. For large $q$ , provided that $\begin{array} { r } { ( 2 q + 1 ) ^ { - 1 } \sum _ { j = - q } ^ { q } Y _ { t - j } \approx 0 } \end{array}$ , it not only will attenuate noise but at the same time will allow linear trend functions $m _ { t } = c _ { 0 } + c _ { 1 } t$ to pass without distortion (see Problem 1.11). However, we must beware of choosing $q$ to be too large, since if $m _ { t }$ is not linear, the filtered process, although smooth, will not be a good estimate of $m _ { t }$ . By clever choice of the weights $\{ a _ { j } \}$ it is possible (see Problems 1.12– 1.14 and Section 4.3) to design a filter that will not only be effective in attenuating noise in the data, but that will also allow a larger class of trend functions (for example all polynomials of degree less than or equal to 3) to pass through without distortion. The Spencer 15-point moving average is a filter that passes polynomials of degree 3 without distortion. Its weights are

$$
a _ {j} = 0, \quad | j | > 7,
$$

![](images/29574e71f4b0d1dad15563a220492b72ac0dd93a4bfe7e7ea46c0f8e1a20eb9b.jpg)  
Figure 1-19 Residuals $\hat { Y } _ { t } = \bar { X _ { t } } - \hat { m } _ { t }$ after subtracting the 5-term moving average from the strike data   
Figure 1-20 Smoothing with a low-pass linear filter

![](images/8c076a0c594caf27512b8ebcd9cb8140f5690a7b02a0fa30a7c5538a0ed6e607.jpg)

with

$$
a _ {j} = a _ {- j}, \quad | j | \leq 7,
$$

and

$$
[ a _ {0}, a _ {1}, \dots , a _ {7} ] = \frac {1}{3 2 0} [ 7 4, 6 7, 4 6, 2 1, 3, - 5, - 6, - 3 ]. \tag {1.5.6}
$$

Applied to the process (1.5.2) with $m _ { t } = c _ { 0 } + c _ { 1 } t + c _ { 2 } t ^ { 2 } + c _ { 3 } t ^ { 3 }$ , it gives

$$
\sum_ {j = - 7} ^ {7} a _ {j} X _ {t - j} = \sum_ {j = - 7} ^ {7} a _ {j} m _ {t - j} + \sum_ {j = - 7} ^ {7} a _ {j} Y _ {t - j} \approx \sum_ {j = - 7} ^ {7} a _ {j} m _ {t - j} = m _ {t},
$$

where the last step depends on the assumed form of $m _ { t }$ (Problem 1.12). Further details regarding this and other smoothing filters can be found in Kendall and Stuart (1976, Chapter 46).

(b) Exponential smoothing. For any fixed $\alpha ~ \in ~ [ 0 , 1 ]$ , the one-sided moving averages $\hat { m } _ { t }$ , $t = 1 , \ldots , n$ , defined by the recursions

$$
\hat {m} _ {t} = \alpha X _ {t} + (1 - \alpha) \hat {m} _ {t - 1}, \quad t = 2, \dots , n, \tag {1.5.7}
$$

and

$$
\hat {m} _ {1} = X _ {1} \tag {1.5.8}
$$

can be computed using ITSM by selecting Smooth $. >$ Exponential and specifying the value of $\alpha$ . Application of (1.5.7) and (1.5.8) is often referred to as exponential smoothing, since the recursions imply that for $t \geq 2$ , $\begin{array} { r } { \hat { m } _ { t } = \sum _ { j = 0 } ^ { t - 2 } \alpha ( 1 - \hat { \alpha } ) ^ { j } X _ { t - j } + } \end{array}$ $( 1 - \alpha ) ^ { t - 1 } X _ { 1 }$ , a weighted moving average of $X _ { t } , X _ { t - 1 } , \ldots$ , with weights decreasing exponentially (except for the last one).

(c) Smoothing by elimination of high-frequency components. The option Smooth>FFT in the program ITSM allows us to smooth an arbitrary series

![](images/e2d4bcd21bfd668945d390a548babf4146e7756a7f8ff2e74fd765be07abe42d.jpg)  
Figure 1-21 Exponentially smoothed strike data with $\alpha = 0 . 4$

![](images/951e36c61d350fca82bc414af1a4fcf0d7e87d20bff116ccc69f4e93ac193a20.jpg)  
Figure 1-22 Strike data smoothed by elimination of high frequencies with $f = 0 . 4$

by elimination of the high-frequency components of its Fourier series expansion (see Section 4.2). This option was used in Example 1.1.4, where we chose to retain the fraction $\mathnormal { f } = 0 . 0 3 5$ of the frequency components of the series in order to estimate the underlying signal. (The choice $f = 1$ would have left the series unchanged.)

# Example 1.5.2

In Figures 1-21 and 1-22 we show the results of smoothing the strike data by exponential smoothing with parameter $\alpha \ = \ 0 . 4$ [see (1.5.7)] and by high-frequency elimination with $f = 0 . 4$ , i.e., by eliminating a fraction 0.6 of the Fourier components at the top of the frequency range. These should be compared with the simple 5-term moving average smoothing shown in Figure 1-18. Experimentation with different smoothing parameters can easily be carried out using the program ITSM. The exponentially smoothed value of the last observation is frequently used to forecast the next data value. The program automatically selects an optimal value of $\alpha$ for this purpose if $\alpha$ is specified as $^ { - 1 }$ in the exponential smoothing dialog box.

-

(d) Polynomial fitting. In Section 1.3.2 we showed how a trend of the form $m _ { t } = a _ { 0 } + a _ { 1 } t + a _ { 2 } t ^ { 2 }$ can be fitted to the data $\{ x _ { 1 } , \ldots , x _ { n } \}$ by choosing the parameters $a _ { 0 } , a _ { 1 }$ , and $a _ { 2 }$ to minimize the sum of squares, $\textstyle \sum _ { t = 1 } ^ { n } ( x _ { t } - m _ { t } ) ^ { 2 }$ (see Example 1.3.4). The method of least squares estimation can also be used to estimate higher-order polynomial trends in the same way. The Regression option of ITSM allows least squares fitting of polynomial trends of order up to 10 (together with up to four harmonic terms; see Example 1.3.6). It also allows generalized least squares estimation (see Section 6.6), in which correlation between the residuals is taken into account.

# 1.5.1.1 Method 2: Trend Elimination by Differencing

Instead of attempting to remove the noise by smoothing as in Method 1, we now attempt to eliminate the trend term by differencing. We define the lag-1 difference operator $\nabla$ by

$$
\nabla X _ {t} = X _ {t} - X _ {t - 1} = (1 - B) X _ {t}, \tag {1.5.9}
$$

where $B$ is the backward shift operator,

$$
B X _ {t} = X _ {t - 1}. \tag {1.5.10}
$$

Powers of the operators $B$ and $\nabla$ are defined in the obvious way, i.e., $B ^ { j } ( X _ { t } ) = X _ { t - j }$ and $\nabla ^ { j } ( X _ { t } ) = \nabla ( \nabla ^ { j - 1 } ( X _ { t } ) ) , j$ $j \geq 1$ , with $\nabla ^ { 0 } ( X _ { t } ) = X _ { t }$ . Polynomials in $B$ and $\nabla$ are manipulated in precisely the same way as polynomial functions of real variables. For example,

$$
\begin{array}{l} \nabla^ {2} X _ {t} = \nabla (\nabla (X _ {t})) = (1 - B) (1 - B) X _ {t} = (1 - 2 B + B ^ {2}) X _ {t} \\ = X _ {t} - 2 X _ {t - 1} + X _ {t - 2}. \\ \end{array}
$$

If the operator $\nabla$ is applied to a linear trend function $m _ { t } = c _ { 0 } + c _ { 1 } t$ , then we obtain the constant function $\nabla m _ { t } = m _ { t } - m _ { t - 1 } = c _ { 0 } + c _ { 1 } t - ( c _ { 0 } + c _ { 1 } ( t - 1 ) ) = c _ { 1 }$ . In the same way any polynomial trend of degree $k$ can be reduced to a constant by application of the operator $\nabla ^ { k }$ (Problem 1.10). For example, if $X _ { t } = m _ { t } + Y _ { t }$ , where $\begin{array} { r } { m _ { t } = \sum _ { j = 0 } ^ { k } c _ { j } t ^ { j } } \end{array}$ and $Y _ { t }$ is stationary with mean zero, application of $\nabla ^ { k }$ gives

$$
\nabla^ {k} X _ {t} = k! c _ {k} + \nabla^ {k} Y _ {t},
$$

a stationary process with mean $k ! c _ { k }$ . These considerations suggest the possibility, given any sequence $\{ x _ { t } \}$ of data, of applying the operator $\nabla$ repeatedly until we find a sequence $\left\{ \nabla ^ { k } x _ { t } \right\}$ that can plausibly be modeled as a realization of a stationary process. It is often found in practice that the order $k$ of differencing required is quite small, frequently one or two. (This relies on the fact that many functions can be well approximated, on an interval of finite length, by a polynomial of reasonably low degree.)

# Example 1.5.3

Applying the operator $\nabla$ to the population values $\{ x _ { t } , t = 1 , \ldots , 2 0 \}$ of Figure 1-5, we find that two differencing operations are sufficient to produce a series with no apparent trend. (To do the differencing in ITSM, select Transform>Difference, enter the value 1 for the differencing lag, and click OK.) This replaces the original series $\{ x _ { t } \}$ by the once-differenced series $\{ x _ { t } - x _ { t - 1 } \}$ . Repetition of these steps gives the twicedifferenced series $\nabla ^ { 2 } x _ { t } = x _ { t } - 2 x _ { t - 1 } + x _ { t - 2 }$ , plotted in Figure 1-23. Notice that the magnitude of the fluctuations in $\nabla ^ { 2 } x _ { t }$ increases with the value of $x _ { t }$ . This effect can be suppressed by first taking natural logarithms, $y _ { t } = \ln x _ { t }$ , and then applying the operator $\nabla ^ { 2 }$ to the series $\{ y _ { t } \}$ . (See also Figures 1-1 and 1-17.)

![](images/0a693c333743dcc5ff4a07dae4efa58a7e78b455083429682373b9684bf8fa8a.jpg)

![](images/ded67cd95ab865a0c152ed95595e75eb4f2175aefceb17151a094cb022d7df9f.jpg)  
Figure 1-23 The twice-differenced series derived from the population data of Figure 1-5

# 1.5.2 Estimation and Elimination of Both Trend and Seasonality

The methods described for the estimation and elimination of trend can be adapted in a natural way to eliminate both trend and seasonality in the general model, specified as follows.

# Classical Decomposition Model

$$
X _ {t} = m _ {t} + s _ {t} + Y _ {t}, \quad t = 1, \dots , n, \tag {1.5.11}
$$

where EYt = 0, st+d = st, and 	dj 1 sj = 0.

We shall illustrate these methods with reference to the accidental deaths data of Example 1.1.3, for which the period $d$ of the seasonal component is clearly 12.

# 1.5.2.1 Method S1: Estimation of Trend and Seasonal Components

The method we are about to describe is used in the Transform>Classical option of ITSM.

Suppose we have observations $\{ x _ { 1 } , \ldots , x _ { n } \}$ . The trend is first estimated by applying a moving average filter specially chosen to eliminate the seasonal component and to dampen the noise. If the period $d$ is even, say $d = 2 q$ , then we use

$$
\hat {m} _ {t} = \left(0. 5 x _ {t - q} + x _ {t - q + 1} + \dots + x _ {t + q - 1} + 0. 5 x _ {t + q}\right) / d, \quad q <   t \leq n - q. \tag {1.5.12}
$$

If the period is odd, say $d = 2 q + 1$ , then we use the simple moving average (1.5.5).

The second step is to estimate the seasonal component. For each $k = 1 , \ldots , d$ , we compute the average $w _ { k }$ of the deviations $\{ ( x _ { k + j d } - \hat { m } _ { k + j d } ) , q < k + j d \leq n - q \}$ . Since these average deviations do not necessarily sum to zero, we estimate the seasonal component $s _ { k }$ as

$$
\hat {s} _ {k} = w _ {k} - d ^ {- 1} \sum_ {i = 1} ^ {d} w _ {i}, \quad k = 1, \dots , d, \tag {1.5.13}
$$

and $\hat { s } _ { k } = \hat { s } _ { k - d } , k > d$ .

![](images/44f2e6469e90e687d965b29f1b107882533761b4eec6afc74a3edf974f46d061.jpg)  
Figure 1-24 The deseasonalized accidental deaths data from ITSM

The deseasonalized data is then defined to be the original series with the estimated seasonal component removed, i.e.,

$$
d _ {t} = x _ {t} - \hat {s} _ {t}, \quad t = 1, \dots , n. \tag {1.5.14}
$$

Finally, we reestimate the trend from the deseasonalized data $\{ d _ { t } \}$ using one of the methods already described. The program ITSM allows you to fit a least squares polynomial trend $\hat { m }$ to the deseasonalized series. In terms of this reestimated trend and the estimated seasonal component, the estimated noise series is then given by

$$
\hat {Y} _ {t} = x _ {t} - \hat {m} _ {t} - \hat {s} _ {t}, \quad t = 1, \ldots , n.
$$

The reestimation of the trend is done in order to have a parametric form for the trend that can be extrapolated for the purposes of prediction and simulation.

Example 1.5.4 Figure 1-24 shows the deseasonalized accidental deaths data obtained from ITSM by reading in the series DEATHS.TSM, selecting Transform>Classical, checking only the box marked Seasonal Fit, entering 12 for the period, and clicking OK. The estimated seasonal component $\hat { \boldsymbol { s } } _ { t }$ , shown in Figure 1-25, is obtained by selecting Transform>Show Classical Fit. (Except for having a mean of zero, this estimate is very similar to the harmonic regression function with frequencies $2 \pi / 1 2$ and $2 \pi / 6$ displayed in Figure 1-11.) The graph of the deseasonalized data suggests the presence of an additional quadratic trend function. In order to fit such a trend to the deseasonalized data, select Transform>Undo Classical to retrieve the original data and then select Transform>Classical and check the boxes marked Seasonal Fit and Polynomial Trend, entering 12 for the period and selecting Quadratic for the trend. Then click OK and you will obtain the trend function

$$
\hat {m} _ {t} = 9 9 5 2 - 7 1. 8 2 t + 0. 8 2 6 0 t ^ {2}, \quad 1 \leq t \leq 7 2.
$$

At this point the data stored in ITSM consists of the estimated noise

$$
\hat {Y} _ {t} = x _ {t} - \hat {m} _ {t} - \hat {s} _ {t}, t = 1, \dots , 7 2,
$$

obtained by subtracting the estimated seasonal and trend components from the original data.

![](images/5044d76df99f11836d2af12529f0d480be4df0acfc32cd2e2cdd5fd32b836720.jpg)

![](images/e40a8c5f879d8d3990f37bd81643c0601bb7171cae3bb95ca2c2bf0ffd48bcc4.jpg)  
Figure 1-25 The estimated seasonal component of the accidental deaths data from ITSM

# 1.5.2.2 Method S2: Elimination of Trend and Seasonal Components by Differencing

The technique of differencing that we applied earlier to nonseasonal data can be adapted to deal with seasonality of period $d$ by introducing the lag- $\mathbf { \nabla } \cdot d$ differencing operator $\nabla _ { d }$ defined by

$$
\nabla_ {d} X _ {t} = X _ {t} - X _ {t - d} = (1 - B ^ {d}) X _ {t}. \tag {1.5.15}
$$

(This operator should not be confused with the operator $\nabla ^ { d } = ( 1 - B ) ^ { d }$ defined earlier.) Applying the operator $\nabla _ { d }$ to the model

$$
X _ {t} = m _ {t} + s _ {t} + Y _ {t},
$$

where $\left\{ { { s } _ { t } } \right\}$ has period $d$ , we obtain

$$
\nabla_ {d} X _ {t} = m _ {t} - m _ {t - d} + Y _ {t} - Y _ {t - d},
$$

which gives a decomposition of the difference $\nabla _ { d } X _ { t }$ into a trend component $( m _ { t } - m _ { t - d } )$ and a noise term $( Y _ { t } - Y _ { t - d } )$ . The trend, $m _ { t } - m _ { t - d }$ , can then be eliminated using the methods already described, in particular by applying a power of the operator $\nabla$ .

Example 1.5.5 Figure 1-26 shows the result of applying the operator $\nabla _ { 1 2 }$ to the accidental deaths data. The graph is obtained from ITSM by opening DEATHS.TSM, selecting Transform>Difference, entering lag 12, and clicking OK. The seasonal component evident in Figure 1-3 is absent from the graph of $\nabla _ { 1 2 } x _ { t }$ , $1 3 \ \leq \ t \ \leq \ 7 2$ . However, there still appears to be a nondecreasing trend. If we now apply the operator $\nabla$ to $\{ \nabla _ { 1 2 } x _ { t } \}$ by again selecting Transform $. >$ Difference, this time with lag one, we obtain the graph of $\nabla \nabla _ { 1 2 } x _ { t }$ , $1 4 \leq t \leq 7 2$ , shown in Figure 1-27, which has no apparent trend or seasonal component. In Chapter 5 we show that this doubly differenced series can in fact be well represented by a stationary time series model.

In this section we have discussed a variety of methods for estimating and/or removing trend and seasonality. The particular method chosen for any given data set will depend on a number of factors including whether or not estimates of the components of the series are required and whether or not it appears that the data contain

# Figure 1-26

The differenced series $\{ \nabla _ { 1 2 } x _ { t } , t = 1 3 , \ldots , 7 2 \}$ $\{ \nabla _ { 1 2 } x _ { t }$ derived from the monthly accidental deaths $\{ x _ { t } , t = 1 , \ldots , 7 2 \}$

![](images/7308c716968f95b1d3a3f66a90bf925f7d52377b7383c0fd25b048c43231763e.jpg)

# Figure 1-27

The differenced series $\{ \nabla \nabla _ { 1 2 } x _ { t } , t = 1 4 , \ldots , 7 2 \}$ $\{ \nabla \nabla _ { 1 2 } x _ { t }$ derived from the monthly accidental deaths $\{ x _ { t } , t = 1 , \ldots , 7 2 \}$

![](images/9422f7ca833551b7eefa4db0c48a68e8378726a8048f1fa2aa5ebfdfa91288dd.jpg)

a seasonal component that does not vary with time. The program ITSM allows two options under the Transform menu:

1. “classical decomposition,” in which trend and/or seasonal components are estimated and subtracted from the data to generate a noise sequence, and   
2. “differencing,” in which trend and/or seasonal components are removed from the data by repeated differencing at one or more lags in order to generate a noise sequence.

A third option is to use the Regression menu, possibly after applying a Box–Cox transformation. Using this option we can (see Example 1.3.6)

3. fit a sum of harmonics and a polynomial trend to generate a noise sequence that consists of the residuals from the regression.

In the next section we shall examine some techniques for deciding whether or not the noise sequence so generated differs significantly from iid noise. If the noise sequence does have sample autocorrelations significantly different from zero, then we can take advantage of this serial dependence to forecast future noise values in terms of past values by modeling the noise as a stationary time series.

# 1.6 Testing the Estimated Noise Sequence

The objective of the data transformations described in Section 1.5 is to produce a series with no apparent deviations from stationarity, and in particular with no apparent trend or seasonality. Assuming that this has been done, the next step is to model the estimated noise sequence (i.e., the residuals obtained either by differencing the data or by estimating and subtracting the trend and seasonal components). If there is no dependence among between these residuals, then we can regard them as observations of independent random variables, and there is no further modeling to be done except to estimate their mean and variance. However, if there is significant dependence among the residuals, then we need to look for a more complex stationary time series model for the noise that accounts for the dependence. This will be to our advantage, since dependence means in particular that past observations of the noise sequence can assist in predicting future values.

In this section we examine some simple tests for checking the hypothesis that the residuals from Section 1.5 are observed values of independent and identically distributed random variables. If they are, then our work is done. If not, then we must use the theory of stationary processes to be developed in later chapters to find a more appropriate model.

(a) The sample autocorrelation function. For large $n$ , the sample autocorrelations of an iid sequence $Y _ { 1 } , \dots , Y _ { n }$ with finite variance are approximately iid with distribution $\mathrm { N } ( 0 , 1 / n )$ (see Brockwell and Davis (1991) p. 222). Hence, if $y _ { 1 } , \ldots , y _ { n }$ is a realization of such an iid sequence, about $9 5 \%$ of the sample autocorrelations should fall between the bounds $\pm 1 . 9 6 / \sqrt { n }$ . If we compute the sample autocorrelations up to lag 40 and find that more than two or three values fall outside the bounds, or that one value falls far outside the bounds, we therefore reject the iid hypothesis. The bounds $\pm 1 . 9 6 / \sqrt { n }$ are automatically plotted when the sample autocorrelation function is computed by the program ITSM.

(b) The portmanteau test. Instead of checking to see whether each sample autocorrelation $\hat { \rho } ( j )$ falls inside the bounds defined in (a) above, it is also possible to consider the single statistic

$$
Q = n \sum_ {j = 1} ^ {h} \hat {\rho} ^ {2} (j).
$$

If $Y _ { 1 } , \dots , Y _ { n }$ is a finite-variance iid sequence, then by the same result used in (a), $Q$ is approximately distributed as the sum of squares of the independent $\mathrm { N } ( 0 , 1 )$ random variables, $\sqrt { n } \hat { \rho } ( j ) , j = 1 , \ldots , h$ , i.e., as chi-squared with $h$ degrees of freedom. A large value of $Q$ suggests that the sample autocorrelations of the data are too large for the data to be a sample from an iid sequence. We therefore reject the iid hypothesis

at level $\alpha$ if $Q > \chi _ { 1 - \alpha } ^ { 2 } ( h )$ , where $\chi _ { 1 - \alpha } ^ { 2 } ( h )$ is the $1 - \alpha$ quantile of the chi-squared distribution with $h$ degrees of freedom. The program ITSM conducts a refinement of this test, formulated by Ljung and Box (1978), in which $Q$ is replaced by

$$
Q _ {\mathrm {L B}} = n (n + 2) \sum_ {j = 1} ^ {h} \hat {\rho} ^ {2} (j) / (n - j),
$$

whose distribution is better approximated by the chi-squared distribution with h degrees of freedom.

Another portmanteau test, formulated by McLeod and Li (1983), can be used as a further test for the iid hypothesis, since if the data are iid, then the squared data are also iid. It is based on the same statistic used for the Ljung–Box test, except that the sample autocorrelations of the data are replaced by the sample autocorrelations of the squared data, $\hat { \rho } _ { W W } ( h )$ , giving

$$
Q _ {\mathrm {M L}} = n (n + 2) \sum_ {k = 1} ^ {h} \hat {\rho} _ {W W} ^ {2} (k) / (n - k).
$$

The hypothesis of iid data is then rejected at level $\alpha$ if the observed value of $Q _ { \mathrm { M L } }$ is larger than the $1 - \alpha$ quantile of the $\chi ^ { 2 } ( h )$ distribution.

(c) The turning point test. If $y _ { 1 } , \ldots , y _ { n }$ is a sequence of observations, we say that there is a turning point at time $i , 1 \ < \ i \ < \ n$ , if $y _ { i - 1 } ~ < ~ y _ { i }$ and $y _ { i } ~ > ~ y _ { i + 1 }$ or if $y _ { i - 1 } ~ > ~ y _ { i }$ and $y _ { i } ~ < ~ y _ { i + 1 }$ . If $T$ is the number of turning points of an iid sequence of length $n$ , then, since the probability of a turning point at time $i$ is $\frac { 2 } { 3 }$ , the expected value of $T$ is

$$
\mu_ {T} = E (T) = 2 (n - 2) / 3.
$$

It can also be shown for an iid sequence that the variance of $T$ is

$$
\sigma_ {T} ^ {2} = \operatorname {V a r} (T) = (1 6 n - 2 9) / 9 0.
$$

A large value of $T \mathrm { ~ - ~ } \mu _ { T }$ indicates that the series is fluctuating more rapidly than expected for an iid sequence. On the other hand, a value of $T \mathrm { ~ - ~ } \mu _ { T }$ much smaller than zero indicates a positive correlation between neighboring observations. For an iid sequence with $n$ large, it can be shown that

$$
T \text {i s a p p r o x i m a t e l y} \mathrm {N} \left(\mu_ {T}, \sigma_ {T} ^ {2}\right).
$$

This means we can carry out a test of the iid hypothesis, rejecting it at level $\alpha$ if $| T - \mu _ { T } | / \sigma _ { T } > \Phi _ { 1 - \alpha / 2 }$ , where $\Phi _ { 1 - \alpha / 2 }$ is the $1 - \alpha / 2$ quantile of the standard normal distribution. (A commonly used value of $\alpha$ is 0.05, for which the corresponding value of $\Phi _ { 1 - \alpha / 2 }$ is 1.96.)

(d) The difference-sign test. For this test we count the number S of values of $i$ such that $y _ { i } > y _ { i - 1 }$ , $i = 2 , \ldots , n$ , or equivalently the number of times the differenced series $y _ { i } - y _ { i - 1 }$ is positive. For an iid sequence it is clear that

$$
\mu_ {S} = E S = \frac {1}{2} (n - 1).
$$

It can also be shown, under the same assumption, that

$$
\sigma_ {S} ^ {2} = \operatorname {V a r} (S) = (n + 1) / 1 2,
$$

and that for large $n$

$$
S \text {i s a p p r o x i m a t e l y} \mathrm {N} \left(\mu_ {S}, \sigma_ {S} ^ {2}\right).
$$

A large positive (or negative) value of $S - \mu _ { S }$ indicates the presence of an increasing (or decreasing) trend in the data. We therefore reject the assumption of no trend in the data if $| S - \mu _ { S } | / \sigma _ { S } > \Phi _ { 1 - \alpha / 2 }$ .

The difference-sign test must be used with caution. A set of observations exhibiting a strong cyclic component will pass the difference-sign test for randomness, since roughly half of the observations will be points of increase.

(e) The rank test. The rank test is particularly useful for detecting a linear trend in the data. Define $P$ to be the number of pairs $( i , j )$ such that $y _ { j } ~ > ~ y _ { i }$ and $j > i$ , $i = 1 , \ldots , n - 1$ . There is a total of $\begin{array} { r } { { \binom { n } { 2 } } = \frac { 1 } { 2 } n ( n - \mathrm { i } ) } \end{array}$ pairs $( i , j )$ such that $j > i$ . For an iid sequence $\{ Y _ { 1 } , \ldots , Y _ { n } \}$ , each event $\{ Y _ { j } > Y _ { i } \}$ has probability $\frac { 1 } { 2 }$ , and the mean of $P$ is therefore

$$
\mu_ {P} = \frac {1}{4} n (n - 1).
$$

It can also be shown for an iid sequence that the variance of $P$ is

$$
\sigma_ {P} ^ {2} = n (n - 1) (2 n + 5) / 7 2
$$

and that for large $n$ ,

$$
P \text {i s a p p r o x i m a t e l y} \mathrm {N} \left(\mu_ {P}, \sigma_ {P} ^ {2}\right)
$$

(see Kendall and Stuart 1976). A large positive (negative) value of $P - \mu _ { P }$ indicates the presence of an increasing (decreasing) trend in the data. The assumption that $\{ y _ { j } \}$ is a sample from an iid sequence is therefore rejected at level $\alpha = 0 . 0 5$ if $| P - \mu _ { P } | / \sigma _ { P } >$ $\Phi _ { 1 - \alpha / 2 } = 1 . 9 6$ .

( f ) Fitting an autoregressive model. A further test that can be carried out using the program ITSM is to fit an autoregressive model to the data using the Yule–Walker algorithm (discussed in Section 5.1.1) and choosing the order which minimizes the AICC statistic (see Section 5.5). A selected order equal to zero suggests that the data is white noise.   
(g) Checking for normality. If the noise process is Gaussian, i.e., if all of its joint distributions are normal, then stronger conclusions can be drawn when a model is fitted to the data. The following test enables us to check whether it is reasonable to assume that observations from an iid sequence are also Gaussian.

Let $Y _ { ( 1 ) } < Y _ { ( 2 ) } < \cdot \cdot \cdot < Y _ { ( n ) }$ be the order statistics of a random sample $Y _ { 1 } , \dots , Y _ { n }$ from the distribution $\mathrm { N } ( \mu , \sigma ^ { 2 } )$ . If $X _ { ( 1 ) } < X _ { ( 2 ) } < \cdots < X _ { ( n ) }$ are the order statistics from a $\mathrm { N } ( 0 , 1 )$ sample of size $n$ , then

$$
E Y _ {(j)} = \mu + \sigma m _ {j},
$$

where $m _ { j } = E X _ { ( j ) } , j = 1 , \dots , n$ . The graph of the points $\left( m _ { 1 } , Y _ { ( 1 ) } \right) , \ldots , \left( m _ { n } , Y _ { ( n ) } \right)$ is called a Gaussian qq plot) and can be displayed in ITSM by clicking on the yellow button labeled QQ. If the normal assumption is correct, the Gaussian qq plot should be approximately linear. Consequently, the squared correlation of the points $( m _ { i } , Y _ { ( i ) } )$ , $i = 1 , \ldots , n$ , should be near 1. The assumption of normality is therefore rejected if the squared correlation $R ^ { 2 }$ is sufficiently small. If we approximate $m _ { i }$ by $\Phi ^ { - 1 } ( ( i - 0 . 5 ) / n )$ (see Mage 1982 for some alternative approximations), then $R ^ { 2 }$ reduces to

$$
R ^ {2} = \frac {\left(\sum_ {i = 1} ^ {n} (Y _ {(i)} - \overline {{Y}}) \Phi^ {- 1} \left(\frac {i - 0 . 5}{n}\right)\right) ^ {2}}{\sum_ {i = 1} ^ {n} (Y _ {(i)} - \overline {{Y}}) ^ {2} \sum_ {i = 1} ^ {n} \left(\Phi^ {- 1} \left(\frac {i - 0 . 5}{n}\right)\right) ^ {2}},
$$

where ${ \overline { { Y } } } = n ^ { - 1 } ( Y _ { 1 } + \cdot \cdot \cdot + Y _ { n } )$ . Percentage points for the distribution of $R ^ { 2 }$ , assuming normality of the sample values, are given by Shapiro and Francia (1972) for sample

![](images/40ddbe34281e5bcf77f880a3ae22b22e062fe18e3ce3cfb433740c36257d25f0.jpg)  
Figure 1-28 The sample autocorrelation function for the data of Example 1.1.4 showing the bounds $\pm 1 . 9 6 / \check { \sqrt { n } }$ -

sizes $n < 1 0 0$ . For $n = 2 0 0$ , $P ( R ^ { 2 } < 0 . 9 8 7 ) = 0 . 0 5$ and $P ( R ^ { 2 } < 0 . 9 8 9 ) = 0 . 1 0$ . For larger values of $n$ the Jarque-Bera test (Jarque and Bera, 1980) for normality can be used (see Section 5.3.3).

Example 1.6.1 If we did not know in advance how the signal plus noise data of Example 1.1.4 were generated, we might suspect that they came from an iid sequence. We can check this hypothesis with the aid of the tests (a)–(f) introduced above.

(a) The sample autocorrelation function (Figure 1-28) is obtained from ITSM by opening the project SIGNAL.TSM and clicking on the second yellow button at the top of the ITSM window. Observing that $25 \%$ of the autocorrelations are outside the bounds $\pm 1 . 9 6 / \sqrt { 2 0 0 }$ , we reject the hypothesis that the series is iid.   
The remaining tests (b), (c), (d), (e), and (f) are performed by choosing the option Statistics>Residual Analysis $>$ Tests of Randomness. (Since no model has been fitted to the data, the residuals are the same as the data themselves.)   
(b) The sample value of the Ljung–Box statistic $Q _ { \mathrm { L B } }$ with $h = 2 0$ is 51.84. Since the corresponding $p$ -value (displayed by ITSM) is $0 . 0 0 0 1 2 < 0 . 0 5$ , we reject the iid hypothesis at level 0.05. The $p$ -value for the McLeod–Li statistic $Q _ { \mathrm { M L } }$ is 0.717. The McLeod–Li statistic does therefore not provide sufficient evidence to reject the iid hypothesis at level 0.05.   
(c) The sample value of the turning-point statistic $T$ is 138, and the asymptotic distribution under the iid hypothesis (with sample size $n = 2 0 0 \mathrm { \Omega }$ ) is N(132, 35.3). Thus $| T - \mu _ { T } | / \sigma _ { T } = 1 . 0 1$ , corresponding to a computed $p$ -value of 0.312. On the basis of the value of $T$ there is therefore not sufficient evidence to reject the iid hypothesis at level 0.05.   
(d) The sample value of the difference-sign statistic $S$ is 101, and the asymptotic distribution under the iid hypothesis (with sample size $n = 2 0 0$ ) is N(99.5, 16.7). Thus $| S - \mu _ { S } | / \sigma _ { S } = 0 . 3 8$ , corresponding to a computed $p$ -value of 0.714. On the basis of the value of $S$ there is therefore not sufficient evidence to reject the iid hypothesis at level 0.05.

(e) The sample value of the rank statistic $P$ is 10,310, and the asymptotic distribution under the iid hypothesis (with $n { = } 2 0 0$ ) is $\mathrm { N } \big ( 9 9 5 0 , 2 . 2 3 9 \times 1 0 ^ { 5 } \big )$ . The statistic $| P - \mu _ { P } | / \sigma _ { P }$ , is therefore equal to 0.76, corresponding to a $p$ -value of 0.447. On the basis of the value of $P$ there is therefore not sufficient evidence to reject the iid hypothesis at level 0.05.   
(f) The minimum-AICC Yule–Walker autoregressive model for the data is of order seven, supporting the evidence provided by the sample ACF and Ljung–Box tests against the iid hypothesis.

Thus, although not all of the tests detect significant deviation from iid behavior, the sample autocorrelation, the Ljung–Box statistic, and the fitted autoregression provide strong evidence against it, causing us to reject it (correctly) in this example.

The general strategy in applying the tests described in this section is to check them all and to proceed with caution if any of them suggests a serious deviation from the iid hypothesis. (Remember that as you increase the number of tests, the probability that at least one rejects the null hypothesis when it is true increases. You should therefore not necessarily reject the null hypothesis on the basis of one test result only.)

# Problems

1.1 Let $X$ and Y be two random variables with $E ( Y ) = \mu$ and $E Y ^ { 2 } < \infty$

a. Show that the constant $c$ that minimizes $E ( Y - c ) ^ { 2 }$ is $c = \mu$ .

b. Deduce that the random variable $f ( X )$ that minimizes $E { \big [ } ( Y - f ( X ) ) ^ { 2 } | X { \big ] }$ is

$$
f (X) = E [ Y | X ].
$$

c. Deduce that the random variable $f ( X )$ that minimizes $E ( Y - f ( X ) ) ^ { 2 }$ is also

$$
f (X) = E [ Y | X ].
$$

1.2 (Generalization of Problem 1.1.) Suppose that $X _ { 1 } , X _ { 2 } , \dots$ is a sequence of random variables with $E ( X _ { t } ^ { 2 } ) < \infty$ and $E ( X _ { t } ) = \mu$ .

a. Show that the random variable $f ( X _ { 1 } , \ldots , X _ { n } )$ that minimizes the conditional mean squared error, $E \big [ ( X _ { n + 1 } - f ( X _ { 1 } , \dots , X _ { n } ) ) ^ { 2 } | X _ { 1 } , \dots , X _ { n } \big ]$ , is

$$
f (X _ {1}, \dots , X _ {n}) = E [ X _ {n + 1} | X _ {1}, \dots , X _ {n} ].
$$

b. Deduce that the random variable $f ( X _ { 1 } , \ldots , X _ { n } )$ that minimizes the unconditional mean squared error, $E { \big [ } ( X _ { n + 1 } - f ( X _ { 1 } , \ldots , X _ { n } ) ) ^ { 2 } { \big ] }$ , is also

$$
f (X _ {1}, \dots , X _ {n}) = E [ X _ {n + 1} | X _ {1}, \dots , X _ {n} ].
$$

c. If $X _ { 1 } , X _ { 2 } , \dots$ is iid with $E ( X _ { i } ^ { 2 } ) < \infty$ and $E X _ { i } = \mu$ , where $\mu$ is known, what is the minimum mean squared error predictor of $X _ { n + 1 }$ in terms of $X _ { 1 } , \dots , X _ { n } ?$   
d. Under the conditions of part (c) show that the best linear unbiased estimator of $\mu$ in terms of $X _ { 1 } , \ldots , X _ { n }$ is $\begin{array} { r } { \bar { X } = \frac { 1 } { n } ( X _ { 1 } + \cdot \cdot \cdot + X _ { n } ) . } \end{array}$ $\hat { \mu }$ said to be an unbiased estimator of $\mu$ if $\operatorname { E } { \hat { \mu } } = \mu$ for all $\mu$ . )   
e. Under the conditions of part (c) show that $\bar { X }$ is the best linear predictor of $X _ { n + 1 }$ that is unbiased for $\mu$ .   
f. If $X _ { 1 } , X _ { 2 } , \dots$ is iid with $E \big ( X _ { i } ^ { 2 } \big ) \ < \ \infty$ and $E X _ { i } = \mu$ , and if $S _ { 0 } = 0$ , $S _ { n } ~ =$ $X _ { 1 } + \cdot \cdot \cdot + X _ { n } , n = 1 , 2 , . . . _ { ! }$ , what is the minimum mean squared error predictor of $S _ { n + 1 }$ in terms of $S _ { 1 } , \ldots , S _ { n } ?$

1.3 Show that a strictly stationary process with $E ( X _ { i } ^ { 2 } ) < \infty$ is weakly stationary.

1.4 Let $\{ Z _ { t } \}$ be a sequence of independent normal random variables, each with mean 0 and variance $\sigma ^ { 2 }$ , and let a, b, and $c$ be constants. Which, if any, of the following processes are stationary? For each stationary process specify the mean and autocovariance function.

a. $X _ { t } = a + b Z _ { t } + c Z _ { t - 2 }$   
b. $X _ { t } = Z _ { 1 } \cos ( c t ) + Z _ { 2 } \sin ( c t )$   
c. $X _ { t } = Z _ { t } \cos ( c t ) + Z _ { t - 1 } \sin ( c t )$   
d. $X _ { t } = a + b Z _ { 0 }$   
e. $X _ { t } = Z _ { 0 } \cos ( c t )$   
f. $X _ { t } = Z _ { t } Z _ { t - 1 }$

1.5 Let $\{ X _ { t } \}$ be the moving-average process of order 2 given by

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 2},
$$

where $\{ Z _ { t } \}$ is $\mathbf { W N } ( 0 , 1 )$ .

a. Find the autocovariance and autocorrelation functions for this process when $\theta = 0 . 8$ .   
b. Compute the variance of the sample mean $( X _ { 1 } + X _ { 2 } + X _ { 3 } + X _ { 4 } ) / 4$ when $\theta = 0 . 8$   
c. Repeat (b) when $\theta = - 0 . 8$ and compare your answer with the result obtained in (b).

1.6 Let $\{ X _ { t } \}$ be the AR(1) process defined in Example 1.4.5.

a. Compute the variance of the sample mean $( X _ { 1 } + X _ { 2 } + X _ { 3 } + X _ { 4 } ) / 4$ when $\phi = 0 . 9$ and $\sigma ^ { 2 } = 1$ .   
b. Repeat (a) when $\phi = - 0 . 9$ and compare your answer with the result obtained in (a).

1.7 If $\{ X _ { t } \}$ and $\{ Y _ { t } \}$ are uncorrelated stationary sequences, i.e., if $X _ { r }$ and $Y _ { s }$ are uncorrelated for every $r$ and $s$ , show that $\{ X _ { t } + Y _ { t } \}$ is stationary with autocovariance function equal to the sum of the autocovariance functions of $\{ X _ { t } \}$ and $\{ Y _ { t } \}$ .

1.8 Let $\{ Z _ { t } \}$ be IID $\mathrm { \Delta N } ( 0 , 1 )$ noise and define

$$
X _ {t} = \left\{ \begin{array}{l l} Z _ {t}, & \text {i f t i s e v e n ,} \\ (Z _ {t - 1} ^ {2} - 1) / \sqrt {2}, & \text {i f t i s o d d .} \end{array} \right.
$$

a. Show that $\{ X _ { t } \}$ is $\mathsf { W N } ( 0 , 1 )$ but not iid(0, 1) noise.   
b. Find $E ( X _ { n + 1 } | X _ { 1 } , \ldots , X _ { n } )$ for $n$ odd and $n$ even and compare the results.

1.9 Let $\{ x _ { 1 } , \ldots , x _ { n } \}$ be observed values of a time series at times $1 , \ldots , n$ , and let ${ \hat { \rho } } ( h )$ be the sample ACF at lag $h$ as in Definition 1.4.4.

a. If $x _ { t } = a + b t$ , where $a$ and $^ b$ are constants and $b \neq 0$ , show that for each fixed $h \geq 1$ ,

$$
\hat {\rho} (h) \rightarrow 1 \text {a s} n \rightarrow \infty .
$$

b. If $x _ { t } = c \cos ( \omega t )$ , where $c$ and $\omega$ are constants $( c \neq 0$ and $\omega \in ( - \pi , \pi ] )$ , show that for each fixed $h$ ,

$$
\hat {\rho} (h) \rightarrow \cos (\omega h) \text {a s} n \rightarrow \infty .
$$

1.10 If $\begin{array} { r } { m _ { t } = \sum _ { k = 0 } ^ { p } c _ { k } t ^ { k } , t = 0 , \pm 1 , . . . } \end{array}$ , show that $\nabla m _ { t }$ is a polynomial of degree $p - 1$ in $t$ = and hence that $\nabla ^ { p + 1 } m _ { t } = 0$ .

1.11 Consider the simple moving-average filter with weights $a _ { j } = ( 2 q + 1 ) ^ { - 1 }$ , $- q \leq$ $j \le q$ .

a. If $m _ { t } = c _ { 0 } + c _ { 1 } t$ , show that $\begin{array} { r } { \sum _ { j = - q } ^ { q } a _ { j } m _ { t - j } = m _ { t } } \end{array}$   
b. If $Z _ { t } , t = 0 , \pm 1 , \pm 2 , . . .$ , are independent random variables with mean 0 and variance $\sigma ^ { 2 }$ , show that the moving average $\begin{array} { r } { A _ { t } = \sum _ { j = - q } ^ { q } a _ { j } Z _ { t - j } } \end{array}$ is “small” for large $q$ in the sense that $E A _ { t } = 0$ and $\operatorname { V a r } ( A _ { t } ) = \sigma ^ { 2 } / ( 2 q + 1 )$ .

1.12 a. Show that a linear filter $\{ a _ { j } \}$ passes an arbitrary polynomial of degree $k$ without distortion, i.e., that

$$
m _ {t} = \sum_ {j} a _ {j} m _ {t - j}
$$

for all kth-degree polynomials $m _ { t } = c _ { 0 } + c _ { 1 } t + \cdot \cdot \cdot + c _ { k } t ^ { k }$ , if and only if

$$
\left\{ \begin{array}{l l} \sum_ {j} a _ {j} = 1 & \text {a n d} \\ \sum_ {j} j ^ {r} a _ {j} = 0, & \text {f o r} r = 1, \ldots , k. \end{array} \right.
$$

b. Deduce that the Spencer 15-point moving-average filter $\{ a _ { j } \}$ defined by (1.5.6) passes arbitrary third-degree polynomial trends without distortion.

1.13 Find a filter of the form $1 + \alpha B + \beta B ^ { 2 } + \gamma B ^ { 3 }$ (i.e., find $\alpha , \beta$ , and $\gamma$ ) that passes linear trends without distortion and that eliminates arbitrary seasonal components of period 2.   
1.14 Show that the filter with coefficients $[ a _ { - 2 } , a _ { - 1 } , a _ { 0 } , a _ { 1 } , a _ { 2 } ] = { \scriptstyle { \frac { 1 } { 9 } } } [ - 1 , 4 , 3 , 4 , - 1 ]$ passes third-degree polynomials and eliminates seasonal components with period 3.   
1.15 Let $\{ Y _ { t } \}$ be a stationary process with mean zero and let $a$ and $^ b$ be constants.

a. If $X _ { t } = a + b t + s _ { t } + Y _ { t }$ , where $s _ { t }$ is a seasonal component with period 12, show that $\nabla \nabla _ { 1 2 } X _ { t } = ( 1 - B ) ( 1 - B ^ { 1 2 } ) X _ { t }$ is stationary and express its autocovariance function in terms of that of $\{ Y _ { t } \}$ .   
b. If $X _ { t } = ( a + b t ) s _ { t } + Y _ { t }$ , where $s _ { t }$ is a seasonal component with period 12, show that $\nabla _ { 1 2 } ^ { 2 } X _ { t } = ( 1 - B ^ { 1 2 } ) ^ { 2 } X _ { t }$ is stationary and express its autocovariance function in terms of that of $\{ Y _ { t } \}$ .

1.16 (Using ITSM to smooth the strikes data.) Double-click on the ITSM icon, select File>Project>Open>Univariate, click OK, and open the file STRIKES. TSM. The graph of the data will then appear on your screen. For smoothing select either Smooth>Moving Ave,Smooth>Exponential, or Smooth>FFT. Try using each of these to reproduce the results shown in Figures 1-18, 1-21, and 1-22.

1.17 (Using ITSM to plot the deaths data.) In ITSM select File>Project>Open> Univariate, click OK, and open the project DEATHS.TSM. The graph of the data will then appear on your screen. To see a histogram of the data, click on the sixth yellow button at the top of the ITSM window. To see the sample autocorrelation function, click on the second yellow button. The presence of a strong seasonal component with period 12 is evident in the graph of the data and in the sample autocorrelation function.

1.18 (Using ITSM to analyze the deaths data.) Open the file DEATHS.TSM, select Transform>Classical, check the box marked Seasonal Fit, and enter 12 for the period. Make sure that the box labeled Polynomial Fit is not checked, and click, OK. You will then see the graph (Figure 1-24) of the deseasonalized data. This graph suggests the presence of an additional quadratic trend function. To fit such a trend, select Transform>Undo Classical to retrieve the original data. Then select Transform>Classical and check the boxes marked Seasonal Fit and Polynomial Trend, entering 12 for the period and Quadratic for the trend. Click OK to obtain the trend function

$$
\hat {m} _ {t} = 9 9 5 2 - 7 1. 8 2 t + 0. 8 2 6 0 t ^ {2}, \quad 1 \leq t \leq 7 2.
$$

At this point the data stored in ITSM consists of the estimated noise

$$
\hat {Y} _ {t} = x _ {t} - \hat {m} _ {t} - \hat {s} _ {t}, \quad t = 1, \dots , 7 2,
$$

obtained by subtracting the estimated seasonal and trend components from the original data. The sample autocorrelation function can be plotted by clicking on the second yellow button at the top of the ITSM window. Further tests for dependence can be carried out by selecting the options Statistics>Residual Analysis $>$ Tests of Randomness. These show clearly the substantial dependence in the series $\{ Y _ { t } \}$ .

To forecast the data without allowing for this dependence, select the option Forecasting>ARMA. Specify 24 for the number of values to be forecast, and the program will compute forecasts based on the assumption that the estimated seasonal and trend components are true values and that $\{ Y _ { t } \}$ is a white noise sequence with zero mean. (This is the default model assumed by ITSM until a more complicated stationary model is estimated or specified.) The original data are plotted with the forecasts appended. Later we shall see how to improve on these forecasts by taking into account the dependence in the series $\{ Y _ { t } \}$ .

1.19 Use a text editor to construct and save a text file named TEST.TSM, which consists of a single column of 30 numbers, $\{ x _ { 1 } , \ldots , x _ { 3 0 } \}$ , defined by

$$
x _ {1}, \dots , x _ {1 0}: 4 8 6, 4 7 4, 4 3 4, 4 4 1, 4 3 5, 4 0 1, 4 1 4, 4 1 4, 3 8 6, 4 0 5;
$$

$$
x _ {1 1}, \dots , x _ {2 0}: 4 1 1, 3 8 9, 4 1 4, 4 2 6, 4 1 0, 4 4 1, 4 5 9, 4 4 9, 4 8 6, 5 1 0;
$$

$$
x _ {2 1}, \dots , x _ {3 0}: 5 0 6, 5 4 9, 5 7 9, 5 8 1, 6 3 0, 6 6 6, 6 7 4, 7 2 9, 7 7 1, 7 8 5.
$$

This series is in fact the sum of a quadratic trend and a period-three seasonal component. Use the program ITSM to apply the filter in Problem 1.14 to this time series and discuss the results.

(Once the data have been typed, they can be imported directly into ITSM by highlighting the data to be imported, using the Windows command Select and Copy and then, in ITSM, selecting the option File>Project>New> Univariate, clicking on OK and selecting File>Import Clipboard.)

# 2

# Stationary Processes

2.1 Basic Properties   
2.2 Linear Processes   
2.3 Introduction to ARMA Processes   
2.4 Properties of the Sample Mean and Autocorrelation Function   
2.5 Forecasting Stationary Time Series   
2.6 The Wold Decomposition

A key role in time series analysis is played by processes whose properties, or some of them, do not vary with time. If we wish to make predictions, then clearly we must assume that something does not vary with time. In extrapolating deterministic functions it is common practice to assume that either the function itself or one of its derivatives is constant. The assumption of a constant first derivative leads to linear extrapolation as a means of prediction. In time series analysis our goal is to predict a series that typically is not deterministic but contains a random component. If this random component is stationary, in the sense of Definition 1.4.2, then we can develop powerful techniques to forecast its future values. These techniques will be developed and discussed in this and subsequent chapters.

# 2.1 Basic Properties

In Section 1.4 we introduced the concept of stationarity and defined the autocovariance function (ACVF) of a stationary time series $\{ X _ { t } \}$ as

$$
\gamma (h) = \operatorname {C o v} \left(X _ {t + h}, X _ {t}\right), \quad h = 0, \pm 1, \pm 2, \dots .
$$

The autocorrelation function (ACF) of $\{ X _ { t } \}$ was defined similarly as the function $\rho ( \cdot )$ whose value at lag $h$ is

$$
\rho (h) = \frac {\gamma (h)}{\gamma (0)}.
$$

The ACVF and ACF provide a useful measure of the degree of dependence among the values of a time series at different times and for this reason play an important role when we consider the prediction of future values of the series in terms of the past and present values. They can be estimated from observations of $X _ { 1 }$ , . . . , $X _ { n }$ by computing the sample ACVF and ACF as described in Section 1.4.1.

The role of the autocorrelation function in prediction is illustrated by the following simple example. Suppose that $\{ X _ { t } \}$ is a stationary Gaussian time series (see Definition A.3.2) and that we have observed $X _ { n }$ . We would like to find the function of $X _ { n }$ that gives us the best predictor of $X _ { n + h }$ , the value of the series after another $h$ time units have elapsed. To define the problem we must first say what we mean by “best.” A natural and computationally convenient definition is to specify our required predictor to be the function of $X _ { n }$ with minimum mean squared error. In this illustration, and indeed throughout the remainder of this book, we shall use this as our criterion for “best.” Now by Proposition A.3.1 the conditional distribution of $X _ { n + h }$ given that $X _ { n } = x _ { n }$ is

$$
\mathrm {N} \big (\mu + \rho (h) (x _ {n} - \mu), \sigma^ {2} \big (1 - \rho (h) ^ {2} \big) \big),
$$

where $\mu$ and $\sigma ^ { 2 }$ are the mean and variance of $\{ X _ { t } \}$ . It was shown in Problem 1.1 that the value of the constant $c$ that minimizes $E ( X _ { n + h } - c ) ^ { 2 }$ is $c = E ( X _ { n + h } ) $ and that the function $m$ of $X _ { n }$ that minimizes $E ( X _ { n + h } - m ( X _ { n } ) ) ^ { 2 }$ is the conditional mean

$$
m \left(X _ {n}\right) = E \left(X _ {n + h} \mid X _ {n}\right) = \mu + \rho (h) \left(X _ {n} - \mu\right). \tag {2.1.1}
$$

The corresponding mean squared error is

$$
E \left(X _ {n + h} - m \left(X _ {n}\right)\right) ^ {2} = \sigma^ {2} \left(1 - \rho (h) ^ {2}\right). \tag {2.1.2}
$$

This calculation shows that at least for stationary Gaussian time series, prediction of $X _ { n + h }$ in terms of $X _ { n }$ is more accurate as $| \rho ( h ) |$ becomes closer to 1, and in the limit as $\rho ( h ) \to \pm 1$ the best predictor approaches $\mu \pm ( X _ { n } - \mu )$ and the corresponding mean squared error approaches 0.

In the preceding calculation the assumption of joint normality of $X _ { n + h }$ and $X _ { n }$ played a crucial role. For time series with nonnormal joint distributions the corresponding calculations are in general much more complicated. However, if instead of looking for the best function of $X _ { n }$ for predicting $X _ { n + h }$ , we look for the best linear predictor, i.e., the best predictor of the form $\ell ( X _ { n } ) = a X _ { n } + b$ , then our problem becomes that of finding $a$ and $^ b$ to minimize $E ( X _ { n + h } - a X _ { n } - b ) ^ { 2 }$ . An elementary calculation (Problem 2.1), shows that the best predictor of this form is

$$
\ell \left(X _ {n}\right) = \mu + \rho (h) \left(X _ {n} - \mu\right) \tag {2.1.3}
$$

with corresponding mean squared error

$$
E \left(X _ {n + h} - \ell \left(X _ {n}\right)\right) ^ {2} = \sigma^ {2} \left(1 - \rho (h) ^ {2}\right). \tag {2.1.4}
$$

Comparison with (2.1.1) and (2.1.3) shows that for Gaussian processes, $\ell ( X _ { n } )$ and $m ( X _ { n } )$ are the same. In general, of course, $m ( X _ { n } )$ will give smaller mean squared error than $\ell ( X _ { n } )$ , since it is the best of a larger class of predictors (see Problem 1.8). However, the fact that the best linear predictor depends only on the mean and ACF of the series $\{ X _ { t } \}$ means that it can be calculated without more detailed knowledge of the joint distributions. This is extremely important in practice because of the difficulty of estimating all of the joint distributions and because of the difficulty of computing the required conditional expectations even if the distributions were known.

As we shall see later in this chapter, similar conclusions apply when we consider the more general problem of predicting $X _ { n + h }$ as a function not only of $X _ { n }$ , but also of Xn 1, Xn 2, . . . . Before pursuing this question we need to examine in more detail the properties of the autocovariance and autocorrelation functions of a stationary time series.

# Basic Properties of $\gamma ( \cdot )$ :

$$
\gamma (0) \geq 0,
$$

$$
\left| \gamma (h) \right| \leq \gamma (0) \text {f o r a l l} h,
$$

and $\gamma ( \cdot )$ is even, i.e.,

$$
\gamma (h) = \gamma (- h) \text {f o r a l l} h.
$$

Proof The first property is simply the statement that $\mathrm { V a r } ( X _ { t } ) \geq 0$ , the second is an immediate consequence of the fact that correlations are less than or equal to 1 in absolute value (or the Cauchy–Schwarz inequality), and the third is established by observing that

$$
\gamma (h) = \mathrm {C o v} (X _ {t + h}, X _ {t}) = \mathrm {C o v} (X _ {t}, X _ {t + h}) = \gamma (- h).
$$

Autocovariance functions have another fundamental property, namely that of nonnegative definiteness.

Definition 2.1.1 A real-valued function $\kappa$ defined on the integers is nonnegative definite if

$$
\sum_ {i, j = 1} ^ {n} a _ {i} \kappa (i - j) a _ {j} \geq 0 \tag {2.1.5}
$$

for all positive integers $n$ and vectors $\mathbf { a } = ( a _ { 1 } , \ldots , a _ { n } ) ^ { \prime }$ with real-valued components $a _ { i }$ .

Theorem 2.1.1 A real-valued function defined on the integers is the autocovariance function of a stationary time series if and only if it is even and nonnegative definite.

Proof To show that the autocovariance function $\gamma ( \cdot )$ of any stationary time series $\{ X _ { t } \}$ is nonnegative definite, let a be any $n \times 1$ vector with real components $a _ { 1 } , \ldots , a _ { n }$ and let $\mathbf { X } _ { n } = ( X _ { n } , \ldots , X _ { 1 } ) ^ { \prime }$ . Then by equation (A.2.5) and the nonnegativity of variances,

$$
\operatorname {V a r} \left(\mathbf {a} ^ {\prime} \mathbf {X} _ {n}\right) = \mathbf {a} ^ {\prime} \Gamma_ {n} \mathbf {a} = \sum_ {i, j = 1} ^ {n} a _ {i} \gamma (i - j) a _ {j} \geq 0,
$$

where $\Gamma _ { n }$ is the covariance matrix of the random vector $\mathbf { X } _ { n }$ . The last inequality, however, is precisely the statement that $\gamma ( \cdot )$ is nonnegative definite. The converse result, that there exists a stationary time series with autocovariance function $\kappa$ if $\kappa$ is even, real-valued, and nonnegative definite, is more difficult to establish (see Brockwell and Davis (1991), Theorem 1.5.1 for a proof). A slightly stronger statement

can be made, namely, that under the specified conditions there exists a stationary Gaussian time series $\{ X _ { t } \}$ with mean 0 and autocovariance function $\kappa ( \cdot )$ .

Remark 1. An autocorrelation function $\rho ( \cdot )$ has all the properties of an autocovariance function and satisfies the additional condition $\rho ( 0 ) = 1$ . In particular, we can say that $\rho ( \cdot )$ is the autocorrelation function of a stationary process if and only if $\rho ( \cdot )$ is an ACVF with $\rho ( 0 ) = 1$ . -

Remark 2. To verify that a given function is nonnegative definite it is often simpler to find a stationary process that has the given function as its ACVF than to verify the conditions (2.1.5) directly. For example, the function $\kappa ( h ) = \cos ( \omega h )$ is nonnegative definite, since (see Problem 2.2) it is the ACVF of the stationary process

$$
X _ {t} = A \cos (\omega t) + B \sin (\omega t),
$$

where $A$ and $B$ are uncorrelated random variables, both with mean 0 and variance 1. Another illustration is provided by the following example. -

Example 2.1.1 We shall show now that the function defined on the integers by

$$
\kappa (h) = \left\{ \begin{array}{l l} 1, & \text {i f} h = 0, \\ \rho , & \text {i f} h = \pm 1, \\ 0, & \text {o t h e r w i s e}, \end{array} \right.
$$

is the ACVF of a stationary time series if and only if $\begin{array} { r } { | \rho | \le \frac { 1 } { 2 } } \end{array}$ . Inspection of the ACVF of the MA(1) process of Example 1.4.4 shows that $\kappa$ is the ACVF of such a process if we can find real $\theta$ and nonnegative $\sigma ^ { 2 }$ such that

$$
\sigma^ {2} (1 + \theta^ {2}) = 1
$$

and

$$
\sigma^ {2} \theta = \rho .
$$

If $| \rho | \ \leq \ { \textstyle { \frac { 1 } { 2 } } }$ , these equations give solutions $\theta = ( 2 \rho ) ^ { - 1 } \big ( 1 \pm \sqrt { 1 - 4 \rho ^ { 2 } } \big )$ and $\sigma ^ { 2 } = $ $\left( 1 + \theta ^ { 2 } \right) ^ { - 1 }$ . However, if $\begin{array} { r } { | \rho | > \frac { 1 } { 2 } } \end{array}$ , there is no real solution for $\theta$ and hence no MA(1) process with ACVF $\kappa$ . To show that there is no stationary process with ACVF $\kappa$ , we need to show that $\kappa$ is not nonnegative definite. We shall do this directly from the definition (2.1.5). First, if $\rho > \frac { 1 } { 2 } , K = [ \kappa ( i - j ) ] _ { i , j = 1 } ^ { n }$ , and a is the $n$ -component vector $\mathbf { a } = ( 1 , - 1 , 1 , - 1 , . . . ) ^ { \prime }$ , then

$$
\mathbf {a} ^ {\prime} K \mathbf {a} = n - 2 (n - 1) \rho <   0 \text {f o r} n > 2 \rho / (2 \rho - 1),
$$

showing that $\kappa ( \cdot )$ is not nonnegative definite and therefore, by Theorem 2.1.1, is not an autocovariance function. If $\rho < - \frac { 1 } { 2 }$ , the same argument with $\mathbf { a } = ( 1 , 1 , 1 , 1 , 1 , . . . ) ^ { \prime }$ again shows that $\kappa ( \cdot )$ is not nonnegative definite.

If $\{ X _ { t } \}$ is a (weakly) stationary time series, then the vector $( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ and the time-shifted vector $( X _ { 1 + h } , \ldots , X _ { n + h } ) ^ { \prime }$ have the same mean vectors and covariance matrices for every integer $h$ and positive integer n. A strictly stationary sequence is one in which the joint distributions of these two vectors (and not just the means and covariances) are the same. The precise definition is given below.

# Definition 2.1.2

$\{ X _ { t } \}$ is a strictly stationary time series if

$$
(X _ {1}, \ldots , X _ {n}) ^ {\prime} \stackrel {d} {=} (X _ {1 + h}, \ldots , X _ {n + h}) ^ {\prime}
$$

for all integers $h$ and $n \geq 1$ . (Here $\circeq$ is used to indicate that the two random vectors have the same joint distribution function.)

For reference, we record some of the elementary properties of strictly stationary time series.

# Properties of a Strictly Stationary Time Series $\{ X _ { t } \}$ :

a. The random variables $X _ { t }$ are identically distributed.   
b. $\quad ( X _ { t } , X _ { t + h } ) ^ { \prime } \stackrel { d } { = } ( X _ { 1 } , X _ { 1 + h } ) ^ { \prime }$ for all integers $t$ and $h$   
c. $\{ X _ { t } \}$ is weakly stationary if $E ( X _ { t } ^ { 2 } ) < \infty$ for all $t$ .   
d. Weak stationarity does not imply strict stationarity.   
e. An iid sequence is strictly stationary.

Proof Properties (a) and (b) follow at once from Definition 2.1.2. If $E X _ { t } ^ { 2 } \ < \ \infty$ , then by (a) and (b) $E X _ { t }$ is independent of $t$ and $\operatorname { C o v } ( X _ { t } , X _ { t + h } ) \ = \ \operatorname { C o v } ( X _ { 1 } , X _ { 1 + h } )$ , which is also independent of $t$ , proving (c). For (d) see Problem 1.8. If $\{ X _ { t } \}$ is an iid sequence of random variables with common distribution function $F$ , then the joint distribution function of $( X _ { 1 + h } , \ldots , \ X _ { n + h } ) ^ { \prime }$ $X _ { n + h } ) ^ { \prime }$ evaluated at $( x _ { 1 } , \ldots , x _ { n } ) ^ { \prime }$ is $F ( x _ { 1 } ) \cdots F ( x _ { n } )$ , which is independent of $h$ . 

One of the simplest ways to construct a time series $\{ X _ { t } \}$ that is strictly stationary (and hence stationary if $E X _ { t } ^ { 2 } < \infty ,$ ) is to “filter” an iid sequence of random variables. Let $\{ Z _ { t } \}$ be an iid sequence, which by (e) is strictly stationary, and define

$$
X _ {t} = g \left(Z _ {t}, Z _ {t - 1}, \dots , Z _ {t - q}\right) \tag {2.1.6}
$$

for some real-valued function $g ( \cdot , \ldots , \cdot )$ . Then $\{ X _ { t } \}$ is strictly stationary, since $( Z _ { t + h } , \ldots , Z _ { t + h - q } ) ^ { \prime } \stackrel { d } { = } ( Z _ { t } , \ldots , Z _ { t - q } ) ^ { \prime }$ for all integers $h$ . It follows also from the defining equation (2.1.6) that $\{ X _ { t } \}$ is $\pmb q$ -dependent, i.e., that $X _ { s }$ and $X _ { t }$ are independent whenever $| t - s | > q$ . (An iid sequence is 0-dependent.) In the same way, adopting a second-order viewpoint, we say that a stationary time series is $\pmb q$ -correlated if $\gamma ( h ) = 0$ whenever $| h | \ > \ q$ . A white noise sequence is then 0-correlated, while the MA(1) process of Example 1.4.4 is 1-correlated. The moving-average process of order $q$ defined below is $q$ -correlated, and perhaps surprisingly, the converse is also true (Proposition 2.1.1).

# The MA(q) Process:

$\{ X _ { t } \}$ is a moving-average process of order $\pmb q$ if

$$
X _ {t} = Z _ {t} + \theta_ {1} Z _ {t - 1} + \dots + \theta_ {q} Z _ {t - q}, \tag {2.1.7}
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \big ( 0 , \sigma ^ { 2 } \big )$ and $\theta _ { 1 } , \ldots , \theta _ { q }$ are constants.

It is a simple matter to check that (2.1.7) defines a stationary time series that is strictly stationary if $\{ Z _ { t } \}$ is iid noise. In the latter case, (2.1.7) is a special case of (2.1.6) with g a linear function.

The importance of $\mathrm { M A } ( q )$ processes derives from the fact that every $q$ -correlated process is an $\mathrm { M A } ( q )$ process. This is the content of the following proposition, whose proof can be found in Brockwell and Davis (1991), Section 3.2. The extension of this result to the case $q = \infty$ is essentially Wold’s decomposition (see Section 2.6).

Proposition 2.1.1 If $\{ X _ { t } \}$ is a stationary $q$ -correlated time series with mean 0, then it can be represented as the MA(q) process in (2.1.7).

# 2.2 Linear Processes

The class of linear time series models, which includes the class of autoregressive moving-average (ARMA) models, provides a general framework for studying stationary processes. In fact, every second-order stationary process is either a linear process or can be transformed to a linear process by subtracting a deterministic component. This result is known as Wold’s decomposition and is discussed in Section 2.6.

# Definition 2.2.1

The time series $\{ X _ { t } \}$ is a linear process if it has the representation

$$
X _ {t} = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} Z _ {t - j}, \tag {2.2.1}
$$

for all $t$ , where $\{ Z _ { t } \} \sim \mathrm { ~ W N } \big ( 0 , \sigma ^ { 2 } \big )$ and $\{ \psi _ { j } \}$ is a sequence of constants with $\scriptstyle \sum _ { j = - \infty } ^ { \infty } | \psi _ { j } | < \infty$ .

In terms of the backward shift operator $B$ , (2.2.1) can be written more compactly as

$$
X _ {t} = \psi (B) Z _ {t}, \tag {2.2.2}
$$

where $\begin{array} { r } { \psi ( B ) = \sum _ { j = - \infty } ^ { \infty } \psi _ { j } B ^ { j } . } \end{array}$ . A linear process is called a moving average or $\mathbf { M A } ( \infty )$ if $\psi _ { j } = 0$ for all $j < 0$ , i.e., if

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j}.
$$

Remark 1. The condition $\textstyle \sum _ { j = - \infty } ^ { \infty } | \psi _ { j } | < \infty$ ensures that the infinite sum in (2.2.1) converges (with probability one), since $E | Z _ { t } | \le \sigma$ and

$$
E | X _ {t} | \leq \sum_ {j = - \infty} ^ {\infty} \left(| \psi_ {j} | E | Z _ {t - j} |\right) \leq \left(\sum_ {j = - \infty} ^ {\infty} | \psi_ {j} |\right) \sigma <   \infty .
$$

It also ensures that $\textstyle \sum _ { j = - \infty } ^ { \infty } \psi _ { j } ^ { 2 } < \infty$ and hence (see Appendix C, Example C.1.1) that =−∞the series in (2.2.1) converges in mean square, i.e., that $X _ { t }$ is the mean square limit of the partial sums $\scriptstyle \sum _ { j = - n } ^ { n } \psi _ { j } Z _ { t - j }$ . The condition $\textstyle \sum _ { j = - n } ^ { n } | \psi _ { j } | ~ < ~ \infty$ also ensures =− =−convergence in both senses of the more general series (2.2.3) considered in Proposition 2.2.1 below. In Section 11.4 we consider a more general class of linear

processes, the fractionally integrated ARMA processes, for which the coefficients are not absolutely summable but only square summable. -

The operator $\psi ( B )$ can be thought of as a linear filter, which when applied to the white noise “input” series $\{ Z _ { t } \}$ produces the “output” $\{ X _ { t } \}$ (see Section 4.3). As established in the following proposition, a linear filter, when applied to any stationary input series, produces a stationary output series.

Proposition 2.2.1 Let $\{ Y _ { t } \}$ be a stationary time series with mean 0 and covariance function $\gamma _ { Y }$ . If $\textstyle \sum _ { j = - \infty } ^ { \infty } | \psi _ { j } | < \infty$ , then the time series

$$
X _ {t} = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} Y _ {t - j} = \psi (B) Y _ {t} \tag {2.2.3}
$$

is stationary with mean 0 and autocovariance function

$$
\gamma_ {X} (h) = \sum_ {j = - \infty} ^ {\infty} \sum_ {k = - \infty} ^ {\infty} \psi_ {j} \psi_ {k} \gamma_ {Y} (h + k - j). \tag {2.2.4}
$$

In the special case where $\{ X _ { t } \}$ is the linear process (2.2.1),

$$
\gamma_ {X} (h) = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} \psi_ {j + h} \sigma^ {2}. \tag {2.2.5}
$$

Proof The argument used in Remark 1, with $\sigma$ replaced by $\sqrt { \gamma _ { Y } ( 0 ) }$ , shows that the series in (2.2.3) is convergent. Since $E Y _ { t } = 0$ , we have

$$
E (X _ {t}) = E \left(\sum_ {j = - \infty} ^ {\infty} \psi_ {j} Y _ {t - j}\right) = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} E (Y _ {t - j}) = 0
$$

and

$$
\begin{array}{l} E (X _ {t + h} X _ {t}) = E \left[ \left(\sum_ {j = - \infty} ^ {\infty} \psi_ {j} Y _ {t + h - j}\right) \left(\sum_ {k = - \infty} ^ {\infty} \psi_ {k} Y _ {t - k}\right) \right] \\ = \sum_ {j = - \infty} ^ {\infty} \sum_ {k = - \infty} ^ {\infty} \psi_ {j} \psi_ {k} E (Y _ {t + h - j} Y _ {t - k}) \\ = \sum_ {j = - \infty} ^ {\infty} \sum_ {k = - \infty} ^ {\infty} \psi_ {j} \psi_ {k} \gamma_ {Y} (h - j + k), \\ \end{array}
$$

which shows that $\{ X _ { t } \}$ is stationary with covariance function (2.2.4). (The interchange of summation and expectation operations in the above calculations can be justified by the absolute summability of $\psi _ { j }$ .) Finally, if $\{ Y _ { t } \}$ is the white noise sequence $\{ Z _ { t } \}$ in (2.2.1), then $\gamma _ { Y } ( h - j + k ) = { \overset { } { \sigma } } ^ { 2 }$ if $k = j - h$ and 0 otherwise, from which (2.2.5) follows. 

Remark 2. The absolute convergence of (2.2.3) implies (Problem 2.6) that filters of the form $\begin{array} { r } { \alpha ( B ) = \sum _ { j = - \infty } ^ { \infty } \alpha _ { j } B ^ { j } } \end{array}$ ∞j and $\begin{array} { r } { \beta ( B ) = \sum _ { j = - \infty } ^ { \infty } \beta _ { j } B ^ { j } } \end{array}$ with absolutely summable coefficients can be applied successively to a stationary series $\{ Y _ { t } \}$ to generate a new stationary series

$$
W _ {t} = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} Y _ {t - j},
$$

where

$$
\psi_ {j} = \sum_ {k = - \infty} ^ {\infty} \alpha_ {k} \beta_ {j - k} = \sum_ {k = - \infty} ^ {\infty} \beta_ {k} \alpha_ {j - k}. \tag {2.2.6}
$$

These relations can be expressed in the equivalent form

$$
W _ {t} = \psi (B) Y _ {t},
$$

where

$$
\psi (B) = \alpha (B) \beta (B) = \beta (B) \alpha (B), \tag {2.2.7}
$$

and the products are defined by (2.2.6) or equivalently by multiplying the series ∞ $\textstyle \sum _ { j = - \infty } ^ { \infty } \alpha _ { j } B ^ { j }$ and $\scriptstyle \sum _ { j = - \infty } ^ { \infty } \beta _ { j } B ^ { j }$ term by term and collecting powers of $B$ . It is clear from (2.2.6) and (2.2.7) that the order of application of the filters $\alpha ( B )$ and $\beta ( B )$ is immaterial. -

# Example 2.2.1 An AR(1) Process

In Example 1.4.5, an AR(1) process was defined as a stationary solution $\{ X _ { t } \}$ of the equations

$$
X _ {t} - \phi X _ {t - 1} = Z _ {t}, \tag {2.2.8}
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , \sigma ^ { 2 } )$ , $| \phi | < 1$ , and $Z _ { t }$ is uncorrelated with $X _ { s }$ for each $s < t$ . To show that such a solution exists and is the unique stationary solution of (2.2.8), we consider the linear process defined by

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \phi^ {j} Z _ {t - j}. \tag {2.2.9}
$$

(The coefficients $\phi ^ { j }$ for $j \geq 0$ are absolutely summable, since $| \phi | < 1 . )$ It is easy to verify directly that the process (2.2.9) is a solution of (2.2.8), and by Proposition 2.2.1 it is also stationary with mean 0 and ACVF

$$
\gamma_ {X} (h) = \sum_ {j = 0} ^ {\infty} \phi^ {j} \phi^ {j + h} \sigma^ {2} = \frac {\sigma^ {2} \phi^ {h}}{1 - \phi^ {2}},
$$

for $h \geq 0$ .

To show that (2.2.9) is the only stationary solution of (2.2.8) let $\{ Y _ { t } \}$ be any stationary solution. Then, iterating (2.2.8), we obtain

$$
\begin{array}{l} Y _ {t} = \phi Y _ {t - 1} + Z _ {t} \\ = Z _ {t} + \phi Z _ {t - 1} + \phi^ {2} Y _ {t - 2} \\ = \dots \\ = Z _ {t} + \phi Z _ {t - 1} + \dots + \phi^ {k} Z _ {t - k} + \phi^ {k + 1} Y _ {t - k - 1}. \\ \end{array}
$$

If $\{ Y _ { t } \}$ is stationary, then $E Y _ { t } ^ { 2 }$ is finite and independent of $t$ , so that

$$
\begin{array}{l} E (Y _ {t} - \sum_ {j = 0} ^ {k} \phi^ {j} Z _ {t - j}) ^ {2} = \phi^ {2 k + 2} E (Y _ {t - k - 1}) ^ {2} \\ \rightarrow 0 \text {a s} k \rightarrow \infty . \\ \end{array}
$$

This implies that $Y _ { t }$ is equal to the mean square limit $\scriptstyle \sum _ { j = 0 } ^ { \infty } \phi ^ { j } Z _ { t - j }$ and hence that the process defined by (2.2.9) is the unique stationary solution of equation (2.2.8).

It the case $| \phi | > 1$ , the series in (2.2.9) does not converge. However, we can rewrite (2.2.8) in the form

$$
X _ {t} = - \phi^ {- 1} Z _ {t + 1} + \phi^ {- 1} X _ {t + 1}. \tag {2.2.10}
$$

Iterating (2.2.10) gives

$$
\begin{array}{l} X _ {t} = - \phi^ {- 1} Z _ {t + 1} - \phi^ {- 2} Z _ {t + 2} + \phi^ {- 2} X _ {t + 2} \\ = \dots \\ = - \phi^ {- 1} Z _ {t + 1} - \dots - \phi^ {- k - 1} Z _ {t + k + 1} + \phi^ {- k - 1} X _ {t + k + 1}, \\ \end{array}
$$

which shows, by the same arguments used above, that

$$
X _ {t} = - \sum_ {j = 1} ^ {\infty} \phi^ {- j} Z _ {t + j} \tag {2.2.11}
$$

is the unique stationary solution of (2.2.8). This solution should not be confused with the nonstationary solution $\{ X _ { t } \}$ of (2.2.8) obtained when $X _ { 0 }$ is any specified random variable that is uncorrelated with $\{ Z _ { t } \}$ .

The solution (2.2.11) is frequently regarded as unnatural, since $X _ { t }$ as defined by (2.2.11) is correlated with future values of $Z _ { s }$ , contrasting with the solution (2.2.9), which has the property that $X _ { t }$ is uncorrelated with $Z _ { s }$ for all $s \ > \ t$ . It is customary therefore in modeling stationary time series to restrict attention to AR(1) processes with $| \phi | < 1$ . Then $X _ { t }$ has the representation (2.2.8) in terms of $\{ Z _ { s } , s \le t \}$ , and we say that $\{ X _ { t } \}$ is a causal or future-independent function of $\{ Z _ { t } \}$ , or more concisely that $\{ X _ { t } \}$ is a causal autoregressive process. It should be noted that every AR(1) process with $| \phi | > 1$ can be reexpressed as an AR(1) process with $| \phi | < 1$ and a new white noise sequence (Problem 3.8). From a second-order point of view, therefore, nothing is lost by eliminating AR(1) processes with $| \phi | > 1$ from consideration.

If $\phi = \pm 1$ , there is no stationary solution of (2.2.8) (see Problem 2.8).

Remark 3. It is worth remarking that when $| \phi | < 1$ the unique stationary solution (2.2.9) can be found immediately with the aid of (2.2.7). To do this let $\phi ( B ) = 1 - \phi B$ and $\begin{array} { r } { \pi ( B ) = \sum _ { j = 0 } ^ { \infty } \phi ^ { j } B ^ { j } } \end{array}$ . Then

$$
\psi (B) := \phi (B) \pi (B) = 1.
$$

Applying the operator $\pi ( B )$ to both sides of (2.2.8), we obtain

$$
X _ {t} = \pi (B) Z _ {t} = \sum_ {j = 0} ^ {\infty} \phi^ {j} Z _ {t - j}
$$

as claimed.

# 2.3 Introduction to ARMA Processes

In this section we introduce, through an example, some of the key properties of an important class of linear processes known as ARMA (autoregressive moving average) processes. These are defined by linear difference equations with constant coefficients.

As our example we shall consider the ARMA(1,1) process. Higher-order ARMA processes will be discussed in Chapter 3.

# Definition 2.3.1

The time series $\{ X _ { t } \}$ is an ARMA(1, 1) process if it is stationary and satisfies (for every t)

$$
X _ {t} - \phi X _ {t - 1} = Z _ {t} + \theta Z _ {t - 1}, \tag {2.3.1}
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \big ( 0 , \sigma ^ { 2 } \big )$ and $\phi + \theta \neq 0$ .

Using the backward shift operator $B$ , (2.3.1) can be written more concisely as

$$
\phi (B) X _ {t} = \theta (B) Z _ {t}, \tag {2.3.2}
$$

where $\phi ( B )$ and $\theta ( B )$ are the linear filters

$$
\phi (B) = 1 - \phi B \text {a n d} \theta (B) = 1 + \theta B,
$$

respectively.

We first investigate the range of values of $\phi$ and $\theta$ for which a stationary solution of (2.3.1) exists. If $| \phi | < 1$ , let $\chi \left( z \right)$ denote the power series expansion of $1 / \phi ( z )$ , i.e., $\textstyle \sum _ { j = 0 } ^ { \infty } \phi ^ { j } z ^ { j }$ | |, which has absolutely summable coefficients. Then from (2.2.7) we conclude that $\chi ( B ) \phi ( B ) = 1$ . Applying $\chi ( B )$ to each side of (2.3.2) therefore gives

$$
X _ {t} = \chi (B) \theta (B) Z _ {t} = \psi (B) Z _ {t},
$$

where

$$
\psi (B) = \sum_ {j = 0} ^ {\infty} \psi_ {j} B ^ {j} = \left(1 + \phi B + \phi^ {2} B ^ {2} + \dots\right) (1 + \theta B).
$$

By multiplying out the right-hand side or using (2.2.6), we find that

$$
\psi_ {0} = 1 \text {a n d} \psi_ {j} = (\phi + \theta) \phi^ {j - 1} \text {f o r} j \geq 1.
$$

As in Example 2.2.1, we conclude that the $\mathbf { M A } ( \infty )$ process

$$
X _ {t} = Z _ {t} + (\phi + \theta) \sum_ {j = 1} ^ {\infty} \phi^ {j - 1} Z _ {t - j} \tag {2.3.3}
$$

is the unique stationary solution of (2.3.1).

Now suppose that $| \phi | > 1$ . We first represent $1 / \phi ( z )$ as a series of powers of $z$ with absolutely summable coefficients by expanding in powers of $z ^ { - 1 }$ , giving (Problem 2.7)

$$
\frac {1}{\phi (z)} = - \sum_ {j = 1} ^ {\infty} \phi^ {- j} z ^ {- j}.
$$

Then we can apply the same argument as in the case where $| \phi | ~ < ~ 1$ to obtain the unique stationary solution of (2.3.1). We let $\begin{array} { r } { \chi ( B ) = - \sum _ { j = 1 } ^ { \infty } \phi ^ { - j } B ^ { - j } } \end{array}$ and apply $\chi \left( B \right)$ to each side of (2.3.2) to obtain

$$
X _ {t} = \chi (B) \theta (B) Z _ {t} = - \theta \phi^ {- 1} Z _ {t} - (\theta + \phi) \sum_ {j = 1} ^ {\infty} \phi^ {- j - 1} Z _ {t + j}. \tag {2.3.4}
$$

If $\phi = \pm 1$ , there is no stationary solution of (2.3.1). Consequently, there is no such thing as an ARMA(1,1) process with $\phi = \pm 1$ according to our definition.

We can now summarize our findings about the existence and nature of the stationary solutions of the ARMA(1,1) recursions (2.3.2) as follows:

• A stationary solution of the ARMA(1,1) equations exists if and only if $\phi \neq \pm 1$ .   
• If $| \phi | < 1$ , then the unique stationary solution is given by (2.3.3). In this case we say that $\{ X _ { t } \}$ is causal or a causal function of $\{ Z _ { t } \}$ , since $X _ { t }$ can be expressed in terms of the current and past values $Z _ { s } , s \le t$ .   
• If $| \phi | > 1$ , then the unique stationary solution is given by (2.3.4). The solution is noncausal, since $X _ { t }$ is then a function of $Z _ { s }$ , $s \geq t$ .

Just as causality means that $X _ { t }$ is expressible in terms of $Z _ { s }$ , $s \leq t$ , the dual concept of invertibility means that $Z _ { t }$ is expressible in terms of $X _ { s } , s \le t$ . We show now that the ARMA(1,1) process defined by (2.3.1) is invertible if $| \theta | < 1$ . To demonstrate this, let $\xi ( z )$ denote the power series expansion of $1 / \theta ( z )$ , i.e., $\textstyle \sum _ { j = 0 } ^ { \infty } ( - \theta ) ^ { j } z ^ { j }$ , which has absolutely summable coefficients. From (2.2.6) it therefore follows that $\xi ( B ) \theta ( B ) = 1$ , and applying $\xi ( B )$ to each side of (2.3.2) gives

$$
Z _ {t} = \xi (B) \phi (B) X _ {t} = \pi (B) X _ {t},
$$

where

$$
\pi (B) = \sum_ {j = 0} ^ {\infty} \pi_ {j} B ^ {j} = \left(1 - \theta B + (- \theta) ^ {2} B ^ {2} + \dots\right) (1 - \phi B).
$$

By multiplying out the right-hand side or using (2.2.6), we find that

$$
Z _ {t} = X _ {t} - (\phi + \theta) \sum_ {j = 1} ^ {\infty} (- \theta) ^ {j - 1} X _ {t - j}. \tag {2.3.5}
$$

Thus the ARMA(1,1) process is invertible, since $Z _ { t }$ can be expressed in terms of the present and past values of the process $X _ { s }$ , $s \leq t$ . An argument like the one used to show noncausality when $| \phi | > 1$ shows that the ARMA(1,1) process is noninvertible when $| \theta | > 1$ , since then

$$
Z _ {t} = - \phi \theta^ {- 1} X _ {t} + (\theta + \phi) \sum_ {j = 1} ^ {\infty} (- \theta) ^ {- j - 1} X _ {t + j}. \tag {2.3.6}
$$

We summarize these results as follows:

• If $| \theta | < 1$ , then the ARMA(1,1) process is invertible, and $Z _ { t }$ is expressed in terms of $X _ { s } , s \le t$ , by (2.3.5).   
• If $| \theta | > 1$ , then the ARMA(1,1) process is noninvertible, and $Z _ { t }$ is expressed in terms of $X _ { s }$ , $s \geq t$ , by (2.3.6).

Remark 1. In the cases $\theta = \pm 1$ , the ARMA(1,1) process is invertible in the more general sense that $Z _ { t }$ is a mean square limit of finite linear combinations of $X _ { s }$ , $s \leq t$ , although it cannot be expressed explicitly as an infinite linear combination of $X _ { s }$ , $s \leq$ t (see Section 4.4 of Brockwell and Davis (1991)). In this book the term invertible will always be used in the more restricted sense that $\begin{array} { r } { Z _ { t } ~ = ~ \sum _ { j = 0 } ^ { \infty } \pi _ { j } X _ { t - j } } \end{array}$ , where $\textstyle \sum _ { j = 0 } ^ { \infty } | \pi _ { j } | < \infty$ . -

Remark 2. If the ARMA(1,1) process $\{ X _ { t } \}$ is noncausal or noninvertible with $| \theta | > 1$ , then it is possible to find a new white noise sequence $\{ W _ { t } \}$ such that $\{ X _ { t } \}$ is a causal and noninvertible ARMA(1,1) process relative to $\{ W _ { t } \}$ (Problem 4.10). Therefore, from a second-order point of view, nothing is lost by restricting attention to causal and invertible ARMA(1,1) models. This last sentence is also valid for higher-order ARMA models. -

# 2.4 Properties of the Sample Mean and Autocorrelation Function

A stationary process $\{ X _ { t } \}$ is characterized, at least from a second-order point of view, by its mean $\mu$ and its autocovariance function $\gamma ( \cdot )$ . The estimation of $\mu , \gamma ( \cdot )$ , and the autocorrelation function $\rho ( \cdot ) = \gamma ( \cdot ) / \gamma ( 0 )$ from observations $X _ { 1 } , \ldots , X _ { n }$ therefore plays a crucial role in problems of inference and in particular in the problem of constructing an appropriate model for the data. In this section we examine some of the properties of the sample estimates $\bar { x }$ and $\hat { \rho } ( \cdot )$ of $\mu$ and $\rho ( \cdot )$ , respectively.

# 2.4.1 Estimation of $\pmb { \mu }$

The moment estimator of the mean $\mu$ of a stationary process is the sample mean

$$
\bar {X} _ {n} = n ^ {- 1} (X _ {1} + X _ {2} + \dots + X _ {n}). \tag {2.4.1}
$$

It is an unbiased estimator of $\mu$ , since

$$
E (\bar {X} _ {n}) = n ^ {- 1} (E X _ {1} + \dots + E X _ {n}) = \mu .
$$

The mean squared error of ${ \bar { X } } _ { n }$ is

$$
\begin{array}{l} E (\bar {X} _ {n} - \mu) ^ {2} = \operatorname {V a r} (\bar {X} _ {n}) \\ = n ^ {- 2} \sum_ {i = 1} ^ {n} \sum_ {j = 1} ^ {n} \operatorname {C o v} \left(X _ {i}, X _ {j}\right) \\ = n ^ {- 2} \sum_ {i - j = - n} ^ {n} (n - | i - j |) \gamma (i - j) \\ = n ^ {- 1} \sum_ {h = - n} ^ {n} \left(1 - \frac {| h |}{n}\right) \gamma (h). \tag {2.4.2} \\ \end{array}
$$

Now if $\gamma ( h ) \ \to \ 0$ as $h  \infty$ , the right-hand side of (2.4.2) converges to zero, so that ${ \bar { X } } _ { n }$ → → ∞converges in mean square to $\mu$ . If $\begin{array} { r } { \sum _ { h = - \infty } ^ { \infty } | \gamma ( h ) | ~ < ~ \infty } \end{array}$ , then (2.4.2) gives $\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty } n \mathrm { V a r } ( \bar { X } _ { n } ) \ = \ \sum _ { | h | < \infty } \gamma ( h ) } \end{array}$ . We record these results in the following proposition.

Proposition 2.4.1 If $\{ X _ { t } \}$ is a stationary time series with mean $\mu$ and autocovariance function $\gamma ( \cdot )$ , then as $n \to \infty$ ,

$$
\begin{array}{l} \mathrm {V a r} (\bar {X} _ {n}) = E (\bar {X} _ {n} - \mu) ^ {2} \rightarrow 0 \quad \mathrm {i f} \gamma (n) \rightarrow 0, \\ n E (\bar {X} _ {n} - \mu) ^ {2} \rightarrow \sum_ {| h | <   \infty} \gamma (h) \quad \text {i f} \sum_ {h = - \infty} ^ {\infty} | \gamma (h) | <   \infty . \\ \end{array}
$$

To make inferences about $\mu$ using the sample mean ${ \bar { X } } _ { n }$ , it is necessary to know the distribution or an approximation to the distribution of ${ \bar { X } } _ { n }$ . If the time series is Gaussian (see Definition A.3.2), then by Remark 2 of Section A.3 and (2.4.2),

$$
n ^ {1 / 2} (\bar {X} _ {n} - \mu) \sim \mathrm {N} \left(0, \sum_ {| h | <   n} \left(1 - \frac {| h |}{n}\right) \gamma (h)\right).
$$

It is easy to construct exact confidence bounds for $\mu$ using this result if $\gamma ( \cdot )$ is known, and approximate confidence bounds if it is necessary to estimate $\gamma ( \cdot )$ from the observations.

For many time series, in particular for linear and ARMA models, ${ \bar { X } } _ { n }$ is approximately normal with mean $\mu$ and variance $n ^ { - 1 } \sum _ { | h | < \infty } \gamma ( h )$ for large $n$ (see Brockwell and Davis (1991), p. 219). An approximate $9 5 \%$ confidence interval for $\mu$ is then

$$
\left(\bar {X} _ {n} - 1. 9 6 v ^ {1 / 2} / \sqrt {n}, \bar {X} _ {n} + 1. 9 6 v ^ {1 / 2} / \sqrt {n}\right), \tag {2.4.3}
$$

where $\begin{array} { r } { \nu = \sum _ { | h | < \infty } \gamma ( h ) } \end{array}$ . Of course, $\nu$ is not generally known, so it must be estimated from the data. The estimator computed in the program ITSM is $\begin{array} { r } { \hat { \nu } = \sum _ { | h | < \sqrt { n } } \bigl ( 1 - \qquad } \end{array}$ $| h | / \sqrt { n } \big ) \hat { \gamma } ( h )$ . For ARMA processes this is a good approximation to $\nu$ for large $n$ .

# Example 2.4.1 An AR(1) Model

Let $\{ X _ { t } \}$ be an AR(1) process with mean $\mu$ , defined by the equations

$$
X _ {t} - \mu = \phi \left(X _ {t - 1} - \mu\right) + Z _ {t},
$$

where $| \phi | ~ < ~ 1$ and $\{ Z _ { t } \} \ \sim \ \mathrm { W N } \big ( 0 , \sigma ^ { 2 } \big )$ . From Example 2.2.1 we have $\gamma ( h ) \ =$ $\phi ^ { | h | } \sigma ^ { 2 } / ( 1 - \phi ^ { 2 } )$ and hence $\textstyle \nu = \big ( 1 + 2 \sum _ { h = 1 } ^ { \infty } \phi ^ { h } \big ) \sigma ^ { 2 } / \big ( 1 - \phi ^ { 2 } \big ) = \sigma ^ { 2 } / ( 1 - \phi ) ^ { 2 }$ . Approximate $9 5 \%$ confidence bounds for $\mu$ are therefore given by $\bar { x } _ { n } \pm 1 . 9 6 \sigma n ^ { - 1 / 2 } / ( 1 - \phi )$ . Since $\phi$ and $\sigma$ are unknown in practice, they must be replaced in these bounds by estimated values.

-

# 2.4.2 Estimation of $\gamma ( \cdot )$ and $\rho ( \cdot )$

Recall from Section 1.4.1 that the sample autocovariance and autocorrelation functions are defined by

$$
\hat {\gamma} (h) = n ^ {- 1} \sum_ {t = 1} ^ {n - | h |} \left(X _ {t + | h |} - \bar {X} _ {n}\right) \left(X _ {t} - \bar {X} _ {n}\right) \tag {2.4.4}
$$

and

$$
\hat {\rho} (h) = \frac {\hat {\gamma} (h)}{\hat {\gamma} (0)}. \tag {2.4.5}
$$

Both the estimators $\hat { \gamma } ( h )$ and $\hat { \rho } ( h )$ are biased even if the factor $n ^ { - 1 }$ in (2.4.4) is replaced by $( n - h ) ^ { - 1 }$ . Nevertheless, under general assumptions they are nearly unbiased for large sample sizes. The sample ACVF has the desirable property that for each $k \geq 1$ the $k$ -dimensional sample covariance matrix

$$
\hat {\Gamma} _ {k} = \left[ \begin{array}{c c c c} \hat {\gamma} (0) & \hat {\gamma} (1) & \dots & \hat {\gamma} (k - 1) \\ \hat {\gamma} (1) & \hat {\gamma} (0) & \dots & \hat {\gamma} (k - 2) \\ \vdots & \vdots & \dots & \vdots \\ \hat {\gamma} (k - 1) & \hat {\gamma} (k - 2) & \dots & \hat {\gamma} (0) \end{array} \right] \tag {2.4.6}
$$

is nonnegative definite. To see this, first note that if $\hat { \Gamma } _ { m }$ is nonnegative definite, then $\hat { \Gamma } _ { k }$ is nonnegative definite for all $k < m$ . So assume $k \geq n$ and write

$$
\hat {\Gamma} _ {k} = n ^ {- 1} T T ^ {\prime},
$$

where $T$ is the $k \times 2 k$ matrix

$$
T = \left[ \begin{array}{c c c c c c c c} 0 & \dots & 0 & 0 & Y _ {1} & Y _ {2} & \dots & Y _ {k} \\ 0 & \dots & 0 & Y _ {1} & Y _ {2} & \dots & Y _ {k} & 0 \\ \vdots & & & & & & & \vdots \\ 0 & Y _ {1} & Y _ {2} & \dots & Y _ {k} & 0 \dots & 0 \end{array} \right],
$$

$Y _ { i } = X _ { i } - { \bar { X } } _ { n }$ , $i = 1 , \ldots , n$ , and $Y _ { i } = 0$ for $i = n + 1 , \ldots , k$ . Then for any real $k \times 1$ vector a we have

$$
\mathbf {a} ^ {\prime} \hat {\Gamma} _ {k} \mathbf {a} = n ^ {- 1} \left(\mathbf {a} ^ {\prime} T\right) \left(T ^ {\prime} \mathbf {a}\right) \geq 0, \tag {2.4.7}
$$

and consequently the sample autocovariance matrix $\hat { \Gamma } _ { k }$ and sample autocorrelation matrix

$$
\hat {R} _ {k} = \hat {\Gamma} _ {k} / \gamma (0) \tag {2.4.8}
$$

are nonnegative definite. Sometimes the factor $n ^ { - 1 }$ is replaced by $( n - h ) ^ { - 1 }$ in the definition of $\hat { \gamma } ( h )$ , but the resulting covariance and correlation matrices $\hat { \Gamma } _ { n }$ and ${ \hat { R } } _ { n }$ may not then be nonnegative definite. We shall therefore use the definitions (2.4.4) and (2.4.5) of $\hat { \gamma } ( h )$ and ${ \hat { \rho } } ( h )$ .

Remark 1. The matrices $\hat { \Gamma } _ { k }$ and $\hat { R } _ { k }$ are in fact nonsingular if there is at least one nonzero $Y _ { i }$ , or equivalently if $\hat { \gamma } ( 0 ) > 0$ . To establish this result, suppose that $\hat { \gamma } ( 0 ) > 0$ and $\hat { \Gamma } _ { k }$ is singular. Then there is equality in (2.4.7) for some nonzero vector a, implying that ${ \bf { a } } ^ { \prime } T = 0$ and hence that the rank of $T$ is less than $k$ . Let $Y _ { i }$ be the first nonzero value of $Y _ { 1 }$ $Y _ { 1 } , Y _ { 2 } , \ldots , Y _ { k }$ $Y _ { k }$ , and consider the $k \times k$ submatrix of $T$ consisting of columns $( i + 1 )$ through $( i + k )$ . Since this matrix is lower right triangular with each diagonal element equal to $Y _ { i }$ , its determinant has absolute value $| Y _ { i } | ^ { k } \neq 0$ . Consequently, the submatrix is nonsingular, and $T$ must have rank $k$ , a contradiction. -

Without further information beyond the observed data X1, . . . , $X _ { n }$ , it is impossible to give reasonable estimates of $\gamma ( h )$ and $\rho ( h )$ for $h \geq n$ . Even for $h$ slightly smaller than $n$ , the estimates $\hat { \gamma } ( h )$ and $\hat { \rho } ( h )$ are unreliable, since there are so few pairs $\left( X _ { t + h } , \ X _ { t } \right)$ available (only one if $h = n - 1$ ). A useful guide is provided by Jenkins (1976), p. 33 who suggest that $n$ should be at least about 50 and $h \leq n / 4$ .

The sample ACF plays an important role in the selection of suitable models for the data. We have already seen in Example 1.4.6 and Section 1.6 how the sample ACF can be used to test for iid noise. For systematic inference concerning $\rho ( h )$ , we need the sampling distribution of the estimator ${ \hat { \rho } } ( h )$ . Although the distribution of $\hat { \rho } ( h )$ is intractable for samples from even the simplest time series models, it can usually be well approximated by a normal distribution for large sample sizes. For linear models and in particular for ARMA models (see Theorem 7.2.2 of Brockwell and Davis (1991) for exact conditions) $\hat { \pmb { \rho } } _ { k } = ( \hat { \rho } ( 1 ) , \dots , \hat { \rho } ( k ) ) ^ { \prime }$ is approximately distributed for large $n$ as $N ( \pmb \rho _ { k } , n ^ { - 1 } W )$ , i.e.,

$$
\hat {\rho} \approx N (\rho , n ^ {- 1} W), \tag {2.4.9}
$$

where $\pmb { \rho } = ( \rho ( 1 ) , \ldots , \rho ( k ) ) ^ { \prime }$ , and $W$ is the covariance matrix whose $( i , j )$ element is given by Bartlett’s formula

$$
\begin{array}{l} w _ {i j} = \sum_ {k = - \infty} ^ {\infty} \left\{\rho (k + i) \rho (k + j) + \rho (k - i) \rho (k + j) + 2 \rho (i) \rho (j) \rho^ {2} (k) \right. \\ \left. - 2 \rho (i) \rho (k) \rho (k + j) - 2 \rho (j) \rho (k) \rho (k + i) \right\}. \\ \end{array}
$$

Simple algebra shows that

$$
\begin{array}{l} w _ {i j} = \sum_ {k = 1} ^ {\infty} \left\{\rho (k + i) + \rho (k - i) - 2 \rho (i) \rho (k) \right\} \\ \times \left\{\rho (k + j) + \rho (k - j) - 2 \rho (j) \rho (k) \right\}, \tag {2.4.10} \\ \end{array}
$$

which is a more convenient form of $w _ { i j }$ for computational purposes.

# Example 2.4.2 iid Noise

If $\{ X _ { t } \} \sim \operatorname { I I D } ( 0 , \sigma ^ { 2 } )$ , then $\rho ( h ) = 0$ for $| h | > 0$ , so from (2.4.10) we obtain

$$
w _ {i j} = \left\{ \begin{array}{l l} 1 & \text {i f} i = j, \\ 0 & \text {o t h e r w i s e .} \end{array} \right.
$$

For large $n$ , therefore, $\hat { \rho } ( 1 ) , \ldots , \hat { \rho } ( h )$ are approximately independent and identically distributed normal random variables with mean 0 and variance $n ^ { - 1 }$ . This result is the basis for the test that data are generated from iid noise using the sample ACF described in Section 1.6. (See also Example 1.4.6.)

![](images/0669b9c12a223728bd3142fde6fb28cdf860747244d63de37e7d7e6a6b660d1e.jpg)

# Example 2.4.3 An MA(1) Process

If $\{ X _ { t } \}$ is the MA(1) process of Example 1.4.4, i.e., if

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1}, \quad t = 0, \pm 1, \dots ,
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , \sigma ^ { 2 } )$ , then from (2.4.10)

$$
w _ {i i} = \left\{ \begin{array}{l l} 1 - 3 \rho^ {2} (1) + 4 \rho^ {4} (1), & \text {i f} i = 1, \\ 1 + 2 \rho^ {2} (1), & \text {i f} i > 1, \end{array} \right.
$$

is the approximate variance of $n ^ { - 1 / 2 } ( \hat { \rho } ( i ) - \rho ( i ) )$ for large $n$ . In Figure 2-1 we have plotted the sample autocorrelation function ${ \hat { \rho } } ( k )$ $\hat { \rho } ( k ) , k = 0 , \dots , 4 0$ , for 200 observations from the MA(1) model

$$
X _ {t} = Z _ {t} - . 8 Z _ {t - 1}, \tag {2.4.11}
$$

where $\{ Z _ { t } \}$ is a sequence of iid $\mathrm { \Delta N } ( 0 , 1 )$ random variables. Here $\rho ( 1 ) = - 0 . 8 / 1 . 6 4 =$ $- 0 . 4 8 7 8$ and $\rho ( h ) = 0$ for $h > 1$ . The lag-one sample ACF is found to be $\hat { \rho } ( 1 ) =$ $- 0 . 4 3 3 3 { = } - 6 . 1 2 8 n ^ { - 1 / 2 }$ , which would cause us (in the absence of our prior knowledge of $\{ X _ { t } \} )$ to reject the hypothesis that the data are a sample from an iid noise sequence. The fact that $| \hat { \rho } ( h ) | { \leq } 1 . 9 6 n ^ { - 1 / 2 }$ for $h { = } 2 , \ldots , 4 0$ strongly suggests that the data are from a model in which observations are uncorrelated past lag 1. Figure 2-1 shows the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 } ( 1 + 2 \rho ^ { 2 } ( 1 ) ) ^ { 1 / 2 }$ , indicating the compatibility of the data with the model (2.4.11). Since, however, $\rho ( 1 )$ is not normally known in advance, the autocorrelations $\hat { \rho } ( 2 ) , \ldots , \hat { \rho } ( 4 0 )$ $\hat { \rho } ( 4 0 )$ would in practice have been compared with the more

![](images/3bbbc27898a8b56d25793414ade8b492260d60c5dc96444cd82296be125fdfd5.jpg)  
Figure 2-1 The sample autocorrelation function of $n = 2 0 0$ observations of the MA(1) process in Example 2.4.3, showing the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 } ( 1 + 2 \hat { \rho } ^ { 2 } ( 1 ) ) ^ { 1 / 2 }$

stringent bounds $\pm 1 . 9 6 n ^ { - 1 / 2 }$ or with the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 } ( 1 + 2 \hat { \rho } ^ { 2 } ( 1 ) ) ^ { 1 / 2 }$ in order to check the hypothesis that the data are generated by a moving-average process of order 1. Finally, it is worth noting that the lag-one correlation $- 0 . 4 8 7 8$ is well inside the $95 \%$ confidence bounds for $\rho ( 1 )$ given by $\hat { \rho } ( 1 ) \pm 1 . 9 6 n ^ { - 1 / 2 } ( 1 - 3 \hat { \rho } ^ { 2 } ( 1 ) +$ $4 \hat { \rho } ^ { 4 } ( 1 ) ) ^ { 1 / 2 } = - 0 . 4 3 3 3 \pm 0 . 1 0 5 3$ . This further supports the compatibility of the data with the model $X _ { t } = Z _ { t } - 0 . 8 Z _ { t - 1 }$ .

# Example 2.4.4 An AR(1) Process

For the AR(1) process of Example 2.2.1,

$$
X _ {t} = \phi X _ {t - 1} + Z _ {t},
$$

where $\{ Z _ { t } \}$ is iid noise and $| \phi | < 1$ , we have, from (2.4.10) with $\rho ( h ) = \phi ^ { | h | }$ ,

$$
\begin{array}{l} w _ {i i} = \sum_ {k = 1} ^ {i} \phi^ {2 i} \big (\phi^ {- k} - \phi^ {k} \big) ^ {2} + \sum_ {k = i + 1} ^ {\infty} \phi^ {2 k} \big (\phi^ {- i} - \phi^ {i} \big) ^ {2} \\ = \left(1 - \phi^ {2 i}\right) \left(1 + \phi^ {2}\right) \left(1 - \phi^ {2}\right) ^ {- 1} - 2 i \phi^ {2 i}, \tag {2.4.12} \\ \end{array}
$$

$i = 1 , 2 , \dots .$ . In Figure 2-2 we have plotted the sample ACF of the Lake Huron residuals $y _ { 1 } , \ldots , y _ { 9 8 }$ from Figure 1-10 together with $9 5 \%$ confidence bounds for $\rho ( i ) , i = 1 , . . . , 4 0$ , assuming that data are generated from the AR(1) model

$$
Y _ {t} = 0. 7 9 1 Y _ {t - 1} + Z _ {t} \tag {2.4.13}
$$

[see equation (1.4.3)]. The confidence bounds are computed from $\hat { \rho } ( i ) \pm 1 . 9 6 n ^ { - 1 / 2 }$ $w _ { i i } ^ { 1 / 2 }$ wii where $w _ { i i }$ is given in (2.4.12) with $\phi ~ = ~ 0 . 7 9 1$ . The model ACF, $\begin{array} { r l } { \rho ( i ) } & { { } = } \end{array}$ (0.791)i, is also plotted in Figure 2-2. Notice that the model ACF just touches the confidence bounds at lags 2–4. This suggests some incompatibility of the data with the model (2.4.13). A much better fit to the residuals is provided by the second-order autoregression defined by (1.4.4).

-

![](images/5c2fcb10e459e50d2cb68f9b6f9c437937c474096a8e805e781ebf889a0adbe7.jpg)  
Figure 2-2 The sample autocorrelation function of the Lake Huron residuals of Figure 1-10 showing the bounds ρ(ˆ i)±1.96n−1/2w1/2ii $\hat { \rho } ( i ) { \pm } 1 . 9 6 n ^ { - 1 / 2 } w _ { i i } ^ { 1 / 2 }$ and the model ACF $\rho ( i ) = ( 0 . 7 9 1 ) ^ { i }$

# 2.5 Forecasting Stationary Time Series

We now consider the problem of predicting the values $X _ { n + h }$ , $h > 0$ , of a stationary time series with known mean $\mu$ and autocovariance function $\gamma$ in terms of the values $\{ X _ { n } , \ldots , X _ { 1 } \}$ , up to time $n$ . Our goal is to find the linear combination of 1, Xn, Xn 1, . . . , $X _ { 1 }$ , that forecasts $X _ { n + h }$ with minimum mean squared error. The best linear predictor in terms of 1, Xn, . . . , $X _ { 1 }$ will be denoted by $P _ { n } X _ { n + h }$ and clearly has the form

$$
^ {\prime \prime} P _ {n} X _ {n + h} = a _ {0} + a _ {1} X _ {n} + \dots + a _ {n} X _ {1}. \tag {2.5.1}
$$

It remains only to determine the coefficients $a _ { 0 } , a _ { 1 } , \ldots , a _ { n }$ , by finding the values that minimize

$$
S \left(a _ {0}, \dots , a _ {n}\right) = E \left(X _ {n + h} - a _ {0} - a _ {1} X _ {n} - \dots - a _ {n} X _ {1}\right) ^ {2}. \tag {2.5.2}
$$

(We already know from Problem 1.1 that $P _ { 0 } Y = E ( Y ) .$ .) Since $S$ is a quadratic function of $a _ { 0 } , \ldots , a _ { n }$ $a _ { n }$ and is bounded below by zero, it is clear that there is at least one value of $( a _ { 0 } , \ldots , a _ { n } )$ that minimizes $S$ and that the minimum $( a _ { 0 } , \ldots , a _ { n } )$ satisfies the equations

$$
\frac {\partial S \left(a _ {0} , \dots , a _ {n}\right)}{\partial a _ {j}} = 0, \quad j = 0, \dots , n. \tag {2.5.3}
$$

Evaluation of the derivatives in equation (2.5.3) gives the equivalent equations

$$
E \left[ X _ {n + h} - a _ {0} - \sum_ {i = 1} ^ {n} a _ {i} X _ {n + 1 - i} \right] = 0, \tag {2.5.4}
$$

$$
E \left[ \left(X _ {n + h} - a _ {0} - \sum_ {i = 1} ^ {n} a _ {i} X _ {n + 1 - i}\right) X _ {n + 1 - j} \right] = 0, \quad j = 1, \dots , n. \tag {2.5.5}
$$

These equations can be written more neatly in vector notation as

$$
a _ {0} = \mu \left(1 - \sum_ {i = 1} ^ {n} a _ {i}\right) \tag {2.5.6}
$$

and

$$
\Gamma_ {n} \mathbf {a} _ {n} = \gamma_ {n} (h), \tag {2.5.7}
$$

where

$$
\mathbf {a} _ {n} = (a _ {1}, \ldots , a _ {n}) ^ {\prime}, \qquad \Gamma_ {n} = \left[ \gamma (i - j) \right] _ {i, j = 1} ^ {n},
$$

and

$$
\gamma_ {n} (h) = (\gamma (h), \gamma (h + 1), \dots , \gamma (h + n - 1)) ^ {\prime}.
$$

Hence,

$$
P _ {n} X _ {n + h} = \mu + \sum_ {i = 1} ^ {n} a _ {i} \left(X _ {n + 1 - i} - \mu\right), \tag {2.5.8}
$$

where ${ \bf a } _ { n }$ satisfies (2.5.7). From (2.5.8) the expected value of the prediction error $X _ { n + h } - P _ { n } X _ { n + h }$ is zero, and the mean square prediction error is therefore

$$
\begin{array}{l} E \left(X _ {n + h} - P _ {n} X _ {n + h}\right) ^ {2} = \gamma (0) - 2 \sum_ {i = 1} ^ {n} a _ {i} \gamma (h + i - 1) + \sum_ {i = 1} ^ {n} \sum_ {j = 1} ^ {n} a _ {i} \gamma (i - j) a _ {j} \\ = \gamma (0) - \mathbf {a} _ {n} ^ {\prime} \gamma_ {n} (h), \tag {2.5.9} \\ \end{array}
$$

where the last line follows from (2.5.7).

Remark 1. To show that equations (2.5.4) and (2.5.5) determine $P _ { n } X _ { n + h }$ uniquely, let $\big \{ a _ { j } ^ { ( 1 ) } , j = 0 , \dots , n \big \}$ and $\big \{ a _ { j } ^ { ( 2 ) } , j = 0 , \dots , n \big \}$ be two solutions and let $Z$ be the difference between the corresponding predictors, i.e.,

$$
Z = a _ {0} ^ {(1)} - a _ {0} ^ {(2)} + \sum_ {j = 1} ^ {n} \left(a _ {j} ^ {(1)} - a _ {j} ^ {(2)}\right) X _ {n + 1 - j}.
$$

Then

$$
Z ^ {2} = Z \left(a _ {0} ^ {(1)} - a _ {0} ^ {(2)} + \sum_ {j = 1} ^ {n} \left(a _ {j} ^ {(1)} - a _ {j} ^ {(2)}\right) X _ {n + 1 - j}\right).
$$

But from (2.5.4) and (2.5.5) we have $E Z = 0$ and $E ( Z X _ { n + 1 - j } ) = 0$ for $j = 1 , \dots , n$ . Consequently, $E ( Z ^ { 2 } ) = 0$ and hence $Z = 0$ . -

# Properties of $ { \boldsymbol { P } } _ { n }  { \boldsymbol { X } } _ { n + h }$ :

1. $\scriptstyle P _ { n } X _ { n + h } = \mu + \sum _ { i = 1 } ^ { n } a _ { i } ( X _ { n + 1 - i } - \mu )$ , where $\mathbf { a } _ { n } = ( a _ { 1 } , \ldots , a _ { n } ) ^ { \prime }$ satisfies (2.5.7).   
2. $E ( X _ { n + h } - P _ { n } X _ { n + h } ) ^ { 2 } = \gamma ( 0 ) - \mathbf { a } _ { n } ^ { \prime } \gamma _ { n } ( h )$ , where $\gamma _ { n } ( h ) = ( \gamma ( h ) , \ldots , \gamma ( h + n -$ 1))′ .   
3. $E ( X _ { n + h } - P _ { n } X _ { n + h } ) = 0$   
4. $E [ ( X _ { n + h } - P _ { n } X _ { n + h } ) X _ { j } ] = 0 , j = 1 , \ldots .$ , n.

Remark 2. Notice that properties 3 and 4 are exactly equivalent to (2.5.4) and (2.5.5). They can be written more succinctly in the form

$$
E [ (\text {E r r o r}) \times (\text {P r e d i c t o r V a r i a b l e}) ] = 0. \tag {2.5.10}
$$

The equations (2.5.10), one for each predictor variable, therefore uniquely determine $P _ { n } X _ { n + h }$ . -

# Example 2.5.1 One-Step Prediction of an AR(1) Series

Consider now the stationary time series defined in Example 2.2.1 by the equations

$$
X _ {t} = \phi X _ {t - 1} + Z _ {t}, \quad t = 0, \pm 1, \dots ,
$$

where $| \phi | ~ < ~ 1$ and $\{ Z _ { t } \} \sim \mathrm { W N } \big ( 0 , \sigma ^ { 2 } \big )$ . From (2.5.7) and (2.5.8), the best linear predictor of $X _ { n + 1 }$ in terms of $\{ 1 , X _ { n } , \ldots , X _ { 1 } \}$ is (for $n \geq 1$ )

$$
P _ {n} X _ {n + 1} = \mathbf {a} _ {n} ^ {\prime} \mathbf {X} _ {n},
$$

where $\mathbf { X } _ { n } = ( X _ { n } , \ldots , X _ { 1 } ) ^ { \prime }$ and

$$
\left[ \begin{array}{c c c c c} 1 & \phi & \phi^ {2} & \dots & \phi^ {n - 1} \\ \phi & 1 & \phi & \dots & \phi^ {n - 2} \\ \vdots & \vdots & \vdots & \vdots & \vdots \\ \phi^ {n - 1} & \phi^ {n - 2} & \phi^ {n - 3} & \dots & 1 \end{array} \right] \left[ \begin{array}{l} a _ {1} \\ a _ {2} \\ \vdots \\ a _ {n} \end{array} \right] = \left[ \begin{array}{c} \phi \\ \phi^ {2} \\ \vdots \\ \phi^ {n} \end{array} \right]. \tag {2.5.11}
$$

A solution of (2.5.11) is clearly

$$
\mathbf {a} _ {n} = (\phi , 0, \dots , 0) ^ {\prime},
$$

and hence the best linear predictor of $X _ { n + 1 }$ in terms of $\{ X _ { 1 } , \ldots , X _ { n } \}$ is

$$
P _ {n} X _ {n + 1} = \mathbf {a} _ {n} ^ {\prime} \mathbf {X} _ {n} = \phi X _ {n},
$$

with mean squared error

$$
E (X _ {n + 1} - P _ {n} X _ {n + 1}) ^ {2} = \gamma (0) - \mathbf {a} _ {n} ^ {\prime} \boldsymbol {\gamma} _ {n} (1) = \frac {\sigma^ {2}}{1 - \phi^ {2}} - \phi \gamma (1) = \sigma^ {2}.
$$

A simpler approach to this problem is to guess, by inspection of the equation defining $X _ { n + 1 }$ , that the best predictor is $\phi X _ { n }$ . Then to verify this conjecture, it suffices to check (2.5.10) for each of the predictor variables $1 , X _ { n } , \ldots , X _ { 1 }$ . The prediction error of the predictor $\phi X _ { n }$ is clearly $X _ { n + 1 } - \phi X _ { n } = Z _ { n + 1 }$ . But $E ( Z _ { n + 1 } Y ) = 0$ for $Y = 1$ and for $Y = X _ { j }$ , $j = 1 , \dotsc , n$ . Hence, by (2.5.10), $\phi X _ { n }$ is the required best linear predictor in terms of 1 $, X _ { 1 } , \ldots , X _ { n }$ $X _ { 1 }$ $X _ { n }$ .

-

# 2.5.1 Prediction of Second-Order Random Variables

Suppose now that $Y$ and $W _ { n }$ , …, $W _ { 1 }$ are any random variables with finite second moments and that the means $\mu ~ = ~ E Y$ , $\mu _ { i } ~ = ~ E W _ { i }$ and covariances $\operatorname { C o v } ( Y , Y )$ , $\operatorname { C o v } ( Y , W _ { i } )$ , and $\operatorname { C o v } ( W _ { i } , W _ { j } )$ are all known. It is convenient to introduce the random vector $\mathbf { W } = ( W _ { n } , \ldots , W _ { 1 } ) ^ { \prime }$ , the corresponding vector of means $\pmb { \mu } _ { W } = ( \mu _ { n } , \ldots , \mu _ { 1 } ) ^ { \prime }$ , the vector of covariances

$$
\gamma = \operatorname {C o v} (Y, \mathbf {W}) = (\operatorname {C o v} (Y, W _ {n}), \operatorname {C o v} (Y, W _ {n - 1}), \dots , \operatorname {C o v} (Y, W _ {1})) ^ {\prime},
$$

and the covariance matrix

$$
\Gamma = \operatorname {C o v} (\mathbf {W}, \mathbf {W}) = \left[ \operatorname {C o v} \left(W _ {n + 1 - i}, W _ {n + 1 - j}\right) \right] _ {i, j = 1} ^ {n}.
$$

Then by the same arguments used in the calculation of $P _ { n } X _ { n + h }$ , the best linear predictor of Y in terms of $\{ 1 , W _ { n } , \ldots , W _ { 1 } \}$ is found to be

$$
P (Y | \mathbf {W}) = \mu_ {Y} + \mathbf {a} ^ {\prime} (\mathbf {W} - \boldsymbol {\mu} _ {W}), \tag {2.5.12}
$$

where $\mathbf { a } = ( a _ { 1 } , \ldots , a _ { n } ) ^ { \prime }$ is any solution of

$$
\Gamma \mathbf {a} = \gamma . \tag {2.5.13}
$$

The mean squared error of the predictor is

$$
E \left[ (Y - P (Y | \mathbf {W})) ^ {2} \right] = \operatorname {V a r} (Y) - \mathbf {a} ^ {\prime} \gamma . \tag {2.5.14}
$$

# Example 2.5.2 Estimation of a Missing Value

Consider again the stationary series defined in Example 2.2.1 by the equations

$$
X _ {t} = \phi X _ {t - 1} + Z _ {t}, \quad t = 0, \pm 1, \dots ,
$$

where $| \phi | < 1$ and $\{ Z _ { t } \} \sim \mathrm { W N } \big ( 0 , \sigma ^ { 2 } \big )$ . Suppose that we observe the series at times 1 and 3 and wish to use these observations to find the linear combination of $1 , X _ { 1 }$ , and $X _ { 3 }$ that estimates $X _ { 2 }$ with minimum mean squared error. The solution to this problem can be obtained directly from (2.5.12) and (2.5.13) by setting $Y = X _ { 2 }$ and $\mathbf { W } = ( X _ { 1 } , X _ { 3 } ) ^ { \prime }$ . This gives the equations

$$
\left[ \begin{array}{c c} 1 & \phi^ {2} \\ \phi^ {2} & 1 \end{array} \right] \mathbf {a} = \left[ \begin{array}{c} \phi \\ \phi \end{array} \right],
$$

with solution

$$
\mathbf {a} = \frac {1}{1 + \phi^ {2}} \left[ \begin{array}{c} \phi \\ \phi \end{array} \right].
$$

The best estimator of $X _ { 2 }$ is thus

$$
P (X _ {2} | \mathbf {W}) = \frac {\phi}{1 + \phi^ {2}} \left(X _ {1} + X _ {3}\right),
$$

with mean squared error

$$
E [ (X _ {2} - P (X _ {2} | \mathbf {W})) ^ {2} ] = \frac {\sigma^ {2}}{1 - \phi^ {2}} - \mathbf {a} ^ {\prime} \left[ \begin{array}{c} \frac {\phi \sigma^ {2}}{1 - \phi^ {2}} \\ \frac {\phi \sigma^ {2}}{1 - \phi^ {2}} \end{array} \right] = \frac {\sigma^ {2}}{1 + \phi^ {2}}.
$$

# 2.5.2 The Prediction Operator $P ( \cdot | W )$

For any given $\mathbf { W } = ( W _ { n } , \ldots , W _ { 1 } ) ^ { \prime }$ and Y with finite second moments, we have seen how to compute the best linear predictor $P ( Y | \mathbf { W } )$ of $Y$ in terms of 1, $W _ { n } , \ldots , W _ { 1 }$ $W _ { 1 }$ from (2.5.12) and (2.5.13). The function $P ( \cdot | \mathbf { W } )$ , which converts $Y$ into $P ( Y | \mathbf { W } )$ , is called a prediction operator. (The operator $P _ { n }$ defined by equations (2.5.7) and (2.5.8) is an example with $\mathbf { W } = ( X _ { n } , X _ { n - 1 } , \ldots , X _ { 1 } ) ^ { \prime }$ .) Prediction operators have a number of useful properties that can sometimes be used to simplify the calculation of best linear predictors. We list some of these below.

# Properties of the Prediction Operator $P ( \cdot | \mathbf { W } )$ :

Suppose that $E U ^ { 2 } < \infty$ , $E V ^ { 2 } < \infty$ , $\Gamma = \mathrm { C o v } ( \mathbf { W } , \mathbf { W } )$ , and $\beta , \alpha _ { 1 } , \ldots , \alpha _ { n }$ are constants.

1. $P ( U | \mathbf { W } ) = E U + \mathbf { a } ^ { \prime } ( \mathbf { W } - E \mathbf { W } )$ , where $\Gamma \mathbf { a } = \mathrm { C o v } ( U , \mathbf { W } )$ .   
2. $E [ ( U - P ( U | \mathbf { W } ) ) \mathbf { W } ] = \mathbf { 0 }$ and $E [ U - P ( U | \mathbf { W } ) ] = 0$   
3. $E [ ( U - P ( U | \mathbf { W } ) ) ^ { 2 } ] = \mathrm { V a r } ( U ) - \mathbf { a } ^ { \prime } \mathrm { C o v } ( U , \mathbf { W } )$ .   
4. $P ( \alpha _ { 1 } U + \alpha _ { 2 } V + \beta | \mathbf { W } ) = \alpha _ { 1 } P ( U | \mathbf { W } ) + \alpha _ { 2 } P ( V | \mathbf { W } ) +$ β .   
5. $\begin{array} { r } { P \big ( \sum _ { i = 1 } ^ { n } \alpha _ { i } W _ { i } + \beta | \mathbf { W } \big ) = \sum _ { i = 1 } ^ { n } \alpha _ { i } W _ { i } + \beta . } \end{array}$   
6. $P ( U | \mathbf { W } ) = E U$ if $\mathbf { C o v } ( U , \mathbf { W } ) = \mathbf { 0 }$   
7. $P ( U | \mathbf { W } ) = P ( P ( U | \mathbf { W } , \mathbf { V } ) | \mathbf { W } )$ if $\mathbf { V }$ is a random vector such that the components of $E ( \mathbf { V V } )$ are all finite.

# Example 2.5.3 One-Step Prediction of an $\operatorname { A R } ( p )$ Series

Suppose now that $\{ X _ { t } \}$ is a stationary time series satisfying the equations

$$
X _ {t} = \phi_ {1} X _ {t - 1} + \dots + \phi_ {p} X _ {t - p} + Z _ {t}, \quad t = 0, \pm 1, \dots ,
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } \big ( 0 , \sigma ^ { 2 } \big )$ and $Z _ { t }$ is uncorrelated with $X _ { s }$ for each $\textit { s } < \textit { t }$ . Then if $n > p$ , we can apply the prediction operator $P _ { n }$ to each side of the defining equations, using properties (4), (5), and (6) to get

$$
P _ {n} X _ {n + 1} = \phi_ {1} X _ {n} + \dots + \phi_ {p} X _ {n + 1 - p}.
$$

# Example 2.5.4 An AR(1) Series with Nonzero Mean

The time series $\{ Y _ { t } \}$ is said to be an AR(1) process with mean $\mu$ if $\{ X _ { t } = Y _ { t } - \mu \}$ is a zero-mean AR(1) process. Defining $\{ X _ { t } \}$ as in Example 2.5.1 and letting $Y _ { t } = X _ { t } + \mu$ , we see that $Y _ { t }$ satisfies the equation

$$
Y _ {t} - \mu = \phi \left(Y _ {t - 1} - \mu\right) + Z _ {t}. \tag {2.5.15}
$$

If $P _ { n } Y _ { n + h }$ is the best linear predictor of $Y _ { n + h }$ in terms of $\{ 1 , Y _ { n } , \ldots , Y _ { 1 } \}$ , then application of $P _ { n }$ to (2.5.15) with $t = n + 1 , n + 2 , . . .$ gives the recursions

$$
P _ {n} Y _ {n + h} - \mu = \phi \left(P _ {n} Y _ {n + h - 1} - \mu\right), \quad h = 1, 2, \dots .
$$

Noting that $\begin{array} { l l l } { P _ { n } Y _ { n } } & { = } & { Y _ { n } } \end{array}$ , we can solve these equations recursively for $P _ { n } Y _ { n + h }$ , $h = 1 , 2 , \ldots$ , to obtain

$$
P _ {n} Y _ {n + h} = \mu + \phi^ {h} \left(Y _ {n} - \mu\right). \tag {2.5.16}
$$

The corresponding mean squared error is [from (2.5.14)]

$$
E \left(Y _ {n + h} - P _ {n} Y _ {n + h}\right) ^ {2} = \gamma (0) \left[ 1 - \mathbf {a} _ {n} ^ {\prime} \boldsymbol {\rho} _ {n} (h) \right]. \tag {2.5.17}
$$

From Example 2.2.1 we know that $\gamma ( 0 ) = \sigma ^ { 2 } / \big ( 1 - \phi ^ { 2 } \big )$ and $\rho ( h ) = \phi ^ { h }$ , $h \geq 0$ . Hence, substituting $\mathbf { a } _ { n } = \left( \phi ^ { h } , 0 , \dots , 0 \right) ^ { \prime }$ [from (2.5.16)] into (2.5.17) gives

$$
E \left(Y _ {n + h} - P _ {n} Y _ {n + h}\right) ^ {2} = \sigma^ {2} \left(1 - \phi^ {2 h}\right) / \left(1 - \phi^ {2}\right). \tag {2.5.18}
$$

-

Remark 3. In general, if $\{ Y _ { t } \}$ is a stationary time series with mean $\mu$ and if $\{ X _ { t } \}$ is the zero-mean series defined by $X _ { t } = Y _ { t } - \mu$ , then since the collection of all linear combinations of 1, $Y _ { n } , \ldots , Y _ { 1 }$ $Y _ { n }$ $Y _ { 1 }$ is the same as the collection of all linear combinations of 1, $X _ { n }$ , . . . , $X _ { 1 }$ , the linear predictor of any random variable W in terms of 1, $Y _ { n } , \ldots , Y _ { 1 }$ is the same as the linear predictor in terms of $1 , X _ { n } , \ldots , X _ { 1 }$ . Denoting this predictor by $P _ { n } W$ and applying $P _ { n }$ to the equation $Y _ { n + h } = X _ { n + h } + \mu$ gives

$$
P _ {n} Y _ {n + h} = \mu + P _ {n} X _ {n + h}. \tag {2.5.19}
$$

Thus the best linear predictor of $Y _ { n + h }$ can be determined by finding the best linear predictor of $X _ { n + h }$ and then adding $\mu$ . Note from (2.5.8) that since $E ( X _ { t } ) = 0$ , $P _ { n } X _ { n + h }$ is the same as the best linear predictor of $X _ { n + h }$ in terms of $X _ { n } , \ldots , X _ { 1 }$ only. -

# 2.5.3 The Durbin–Levinson Algorithm

In view of Remark 3 above, we can restrict attention from now on to zero-mean stationary time series, making the necessary adjustments for the mean if we wish to predict a stationary series with nonzero mean. If $\{ X _ { t } \}$ is a zero-mean stationary series with autocovariance function $\gamma ( \cdot )$ , then in principle the equations (2.5.12) and (2.5.13) completely solve the problem of determining the best linear predictor $P _ { n } X _ { n + h }$ of $X _ { n + h }$ in terms of $\{ X _ { n } , \ldots , X _ { 1 } \}$ . However, the direct approach requires the determination of a solution of a system of $n$ linear equations, which for large $n$ may be difficult and time-consuming. In cases where the process is defined by a system of linear equations (as in Examples 2.5.2 and 2.5.3) we have seen how the linearity of $P _ { n }$ can be used to great advantage. For more general stationary processes it would be helpful if the one-step predictor $P _ { n } X _ { n + 1 }$ based on $n$ previous observations could be used to simplify the calculation of $P _ { n + 1 } X _ { n + 2 }$ , the one-step predictor based on $n + 1$ previous observations. Prediction algorithms that utilize this idea are said to be recursive. Two important examples are the Durbin–Levinson algorithm, discussed in this section, and the innovations algorithm, discussed in Section 2.5.4 below.

We know from (2.5.12) and (2.5.13) that if the matrix $\Gamma _ { n }$ is nonsingular, then

$$
P _ {n} X _ {n + 1} = \phi_ {n} ^ {\prime} \mathbf {X} _ {n} = \phi_ {n 1} X _ {n} + \dots + \phi_ {n n} X _ {1},
$$

where

$$
\phi_ {n} = \Gamma_ {n} ^ {- 1} \gamma_ {n},
$$

$\gamma _ { n } = ( \gamma ( 1 ) , \ldots , \gamma ( n ) ) ^ { \prime }$ , and the corresponding mean squared error is

$$
v _ {n} := E (X _ {n + 1} - P _ {n} X _ {n + 1}) ^ {2} = \gamma (0) - \phi_ {n} ^ {\prime} \boldsymbol {\gamma} _ {n}.
$$

A useful sufficient condition for nonsingularity of all the autocovariance matrices $\Gamma _ { 1 } , \Gamma _ { 2 } , \dots$ is $\gamma ( 0 ) > 0$ and $\gamma ( h ) \to 0$ as $h  \infty$ . (For a proof of this result see Brockwell and Davis (1991), Proposition 5.1.1.)

# The Durbin–Levinson Algorithm:

The coefficients $\phi _ { n 1 } , \ldots , \phi _ { n n }$ $\phi _ { n n }$ can be computed recursively from the equations

$$
\phi_ {n n} = \left[ \gamma (n) - \sum_ {j = 1} ^ {n - 1} \phi_ {n - 1, j} \gamma (n - j) \right] v _ {n - 1} ^ {- 1}, \tag {2.5.20}
$$

$$
\left[ \begin{array}{c} \phi_ {n 1} \\ \vdots \\ \phi_ {n, n - 1} \end{array} \right] = \left[ \begin{array}{c} \phi_ {n - 1, 1} \\ \vdots \\ \phi_ {n - 1, n - 1} \end{array} \right] - \phi_ {n n} \left[ \begin{array}{c} \phi_ {n - 1, n - 1} \\ \vdots \\ \phi_ {n - 1, 1} \end{array} \right] \tag {2.5.21}
$$

and

$$
v _ {n} = v _ {n - 1} \left[ 1 - \phi_ {n n} ^ {2} \right], \tag {2.5.22}
$$

where $\phi _ { 1 1 } = \gamma ( 1 ) / \gamma ( 0 )$ and $\nu _ { 0 } = \gamma ( 0 )$ .

Proofs 1 The definition of $\phi _ { 1 1 }$ ensures that the equation

$$
R _ {n} \phi_ {n} = \rho_ {n} \tag {2.5.23}
$$

(where $\pmb { \rho } _ { n } = ( \rho ( 1 ) , \ldots , \rho ( n ) ) ^ { \prime } )$ is satisfied for $n = 1$ . The first step in the proof is to show that $\phi _ { n }$ , defined recursively by (2.5.20) and (2.5.21), satisfies (2.5.23) for all $n$ . Suppose this is true for $n = k$ . Then, partitioning $R _ { k + 1 }$ and defining

$$
\boldsymbol {\rho} _ {k} ^ {(r)} := (\rho (k), \rho (k - 1), \dots , \rho (1)) ^ {\prime}
$$

and

$$
\phi_ {k} ^ {(r)} := \left(\phi_ {k k}, \phi_ {k, k - 1}, \dots , \phi_ {k 1}\right) ^ {\prime},
$$

we see that the recursions imply

$$
\begin{array}{l} \mathbf {R} _ {k + 1} \phi_ {k + 1}   =   \left[ \begin{array}{c c} \mathbf {R} _ {k} & \boldsymbol {\rho} _ {k} ^ {(r)} \\ \boldsymbol {\rho} _ {k} ^ {(r)}, & 1 \end{array} \right] \left[ \begin{array}{c} \phi_ {k} - \phi_ {k + 1, k + 1} \phi_ {k} ^ {(r)} \\ \phi_ {k + 1, k + 1} \end{array} \right] \\ = \left[ \begin{array}{c} \boldsymbol {\rho} _ {k} - \phi_ {k + 1, k + 1} \boldsymbol {\rho} _ {k} ^ {(r)} + \phi_ {k + 1, k + 1} \boldsymbol {\rho} _ {k} ^ {(r)} \\ \boldsymbol {\rho} _ {k} ^ {(r) \prime} \phi_ {k} - \phi_ {k + 1, k + 1} \boldsymbol {\rho} _ {k} ^ {(r) \prime} \phi_ {k} ^ {(r)} + \phi_ {k + 1, k + 1} \end{array} \right] \\ = \rho_ {k + 1}, \\ \end{array}
$$

as required. Here we have used the fact that if $R _ { k } \phi _ { k } = \pmb { \rho } _ { k }$ , then $R _ { k } \phi _ { k } ^ { \left( r \right) } = \rho _ { k } ^ { \left( r \right) }$ . This is easily checked by writing out the component equations in reverse order. Since (2.5.23) is satisfied for $n = 1$ , it follows by induction that the coefficient vectors $\phi _ { n }$ defined recursively by (2.5.20) and (2.5.21) satisfy (2.5.23) for all $n$ .

It remains only to establish that the mean squared errors

$$
v _ {n} := E (X _ {n + 1} - \phi_ {n} ^ {\prime} \mathbf {X} _ {n}) ^ {2}
$$

satisfy $\nu _ { 0 } = \gamma ( 0 )$ and (2.5.22). The fact that $\nu _ { 0 } = \gamma ( 0 )$ is an immediate consequence of the definition $P _ { 0 } X _ { 1 } : = E ( X _ { 1 } ) = 0$ . Since we have shown that $\phi _ { n } ^ { \prime } \mathbf { X } _ { n }$ is the best linear predictor of $X _ { n + 1 }$ , we can write, from (2.5.9) and (2.5.21),

$$
v _ {n} = \gamma (0) - \phi_ {n} ^ {\prime} \gamma_ {n} = \gamma (0) - \phi_ {n - 1} ^ {\prime} \gamma_ {n - 1} + \phi_ {n n} \phi_ {n - 1} ^ {(r) \prime} \gamma_ {n - 1} - \phi_ {n n} \gamma (n).
$$

Applying (2.5.9) again gives

$$
v _ {n} = v _ {n - 1} + \phi_ {n n} \left(\phi_ {n - 1} ^ {(r) ^ {\prime}} \gamma_ {n - 1} - \gamma (n)\right),
$$

and hence, by (2.5.20),

$$
v _ {n} = v _ {n - 1} - \phi_ {n n} ^ {2} (\gamma (0) - \phi_ {n - 1} ^ {\prime} \gamma_ {n - 1}) = v _ {n - 1} \left(1 - \phi_ {n n} ^ {2}\right).
$$

Remark 4. Under the conditions of the proposition, the function defined by $\alpha ( 0 ) =$ 1 and $\alpha ( n ) = \phi _ { n n }$ , $n = 1 , 2 , \ldots$ , is known as the partial autocorrelation function (PACF) of $\{ X _ { t } \}$ , discussed further in Section 3.2. Equation (2.5.22) shows the relation between $\alpha ( n )$ and the reduction in the one-step mean squared error as the number of predictors is increased from $n - 1$ to $n$ . -

# 2.5.4 The Innovations Algorithm

The recursive algorithm to be discussed in this section is applicable to all series with finite second moments, regardless of whether they are stationary or not. Its application, however, can be simplified in certain special cases.

Suppose then that $\{ X _ { t } \}$ is a zero-mean series with $E | X _ { t } | ^ { 2 } < \infty$ for each $t$ and

$$
E \left(X _ {i} X _ {j}\right) = \kappa (i, j). \tag {2.5.24}
$$

We denote the best one-step linear predictors and their mean squared errors by

$$
\hat {X} _ {n} = \left\{ \begin{array}{l l} 0, & \text {i f} n = 1, \\ P _ {n - 1} X _ {n}, & \text {i f} n = 2, 3, \ldots , \end{array} \right.
$$

and

$$
v _ {n} = E (X _ {n + 1} - P _ {n} X _ {n + 1}) ^ {2}.
$$

We shall also introduce the innovations, or one-step prediction errors,

$$
U _ {n} = X _ {n} - \hat {X} _ {n}.
$$

In terms of the vectors $\mathbf { U } _ { n } = ( U _ { 1 } , \ldots , U _ { n } ) ^ { \prime }$ and $\mathbf { X } _ { n } = ( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ the last equations can be written as

$$
\mathbf {U} _ {n} = A _ {n} \mathbf {X} _ {n}, \tag {2.5.25}
$$

where $A _ { n }$ has the form

$$
A _ {n} = \left[ \begin{array}{c c c c c} 1 & 0 & 0 & \dots & 0 \\ a _ {1 1} & 1 & 0 & \dots & 0 \\ a _ {2 2} & a _ {2 1} & 1 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & 0 \\ a _ {n - 1, n - 1} & a _ {n - 1, n - 2} & a _ {n - 1, n - 3} & \dots & 1 \end{array} \right].
$$

(If $\{ X _ { t } \}$ is stationary, then $a _ { i j } = - a _ { j }$ with $a _ { j }$ as in (2.5.7) with $h = 1 .$ .) This implies that $A _ { n }$ is nonsingular, with inverse $C _ { n }$ of the form

$$
C _ {n} = \left[ \begin{array}{c c c c c} 1 & 0 & 0 & \dots & 0 \\ \theta_ {1 1} & 1 & 0 & \dots & 0 \\ \theta_ {2 2} & \theta_ {2 1} & 1 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & 0 \\ \theta_ {n - 1, n - 1} & \theta_ {n - 1, n - 2} & \theta_ {n - 1, n - 3} & \dots & 1 \end{array} \right].
$$

The vector of one-step predictors ${ \hat { \bf X } } _ { n } : = ( X _ { 1 } , P _ { 1 } X _ { 2 } , . . . , P _ { n - 1 } X _ { n } ) ^ { \prime }$ can therefore be expressed as

$$
\hat {\mathbf {X}} _ {n} = \mathbf {X} _ {n} - \mathbf {U} _ {n} = C _ {n} \mathbf {U} _ {n} - \mathbf {U} _ {n} = \boldsymbol {\Theta} _ {n} \left(\mathbf {X} _ {n} - \hat {\mathbf {X}} _ {n}\right), \tag {2.5.26}
$$

where

$$
\boldsymbol {\Theta} _ {n} = \left[ \begin{array}{c c c c c} 0 & 0 & 0 & \dots & 0 \\ \theta_ {1 1} & 0 & 0 & \dots & 0 \\ \theta_ {2 2} & \theta_ {2 1} & 0 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & 0 \\ \theta_ {n - 1, n - 1} & \theta_ {n - 1, n - 2} & \theta_ {n - 1, n - 3} & \dots & 0 \end{array} \right]
$$

and $\mathbf { X } _ { n }$ itself satisfies

$$
\mathbf {X} _ {n} = C _ {n} \left(\mathbf {X} _ {n} - \hat {\mathbf {X}} _ {n}\right). \tag {2.5.27}
$$

Equation (2.5.26) can be rewritten as

$$
\hat {X} _ {n + 1} = \left\{ \begin{array}{l l} 0, & \text {i f} n = 0, \\ \sum_ {j = 1} ^ {n} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right), & \text {i f} n = 1, 2, \dots , \end{array} \right. \tag {2.5.28}
$$

from which the one-step predictors ${ \hat { X } } _ { 1 } , { \hat { X } } _ { 2 }$ , . . . can be computed recursively once the coefficients $\theta _ { i j }$ have been determined. The following algorithm generates these coefficients and the mean squared errors $\nu _ { i } = E \big ( X _ { i + 1 } - \hat { X } _ { i + 1 } \big ) ^ { 2 }$ , starting from the covariances $\kappa ( i , j )$ .

# The Innovations Algorithm:

The coefficients $\theta _ { n 1 } , \ldots , \theta _ { n n }$ can be computed recursively from the equations

$$
v _ {0} = \kappa (1, 1),
$$

$$
\theta_ {n, n - k} = v _ {k} ^ {- 1} \left(\kappa (n + 1, k + 1) - \sum_ {j = 0} ^ {k - 1} \theta_ {k, k - j} \theta_ {n, n - j} v _ {j}\right), \quad 0 \leq k <   n,
$$

and

$$
v _ {n} = \kappa (n + 1, n + 1) - \sum_ {j = 0} ^ {n - 1} \theta_ {n, n - j} ^ {2} v _ {j}.
$$

(It is a trivial matter to solve first for $\nu _ { 0 }$ , then successively for $\theta _ { 1 1 } , \nu _ { 1 } ; \theta _ { 2 2 }$ , $\theta _ { 2 1 } , \nu _ { 2 } ; \theta _ { 3 3 } , \theta _ { 3 2 } , \theta _ { 3 1 } , \nu _ { 3 } ; . . . . )$

Proof See Brockwell and Davis (1991), Proposition 5.2.2.

Remark 5. While the Durbin–Levinson recursion gives the coefficients of $X _ { n } , \ldots , X _ { 1 }$ in the representation $\begin{array} { r } { \hat { X } _ { n + 1 } \ = \ \sum _ { j = 1 } ^ { n } \phi _ { n j } X _ { n + 1 - j } } \end{array}$ , the innovations algorithm gives the coefficients of $\bigl ( X _ { n } \ - \ \hat { X } _ { n } \bigr ) , \ldots , ( X _ { 1 } \ - \ \hat { X } _ { 1 } )$ , in the alternative expansion $\hat { X } _ { n + 1 } \quad =$ $\begin{array} { r } { \sum _ { j = 1 } ^ { n } \theta _ { n j } \bigl ( X _ { n + 1 - j } - \hat { X } _ { n + 1 - j } \bigr ) , } \end{array}$ . The latter expansion has a number of advantages deriving =from the fact that the innovations are uncorrelated (see Problem 2.20). It can also be greatly simplified in the case of $\mathbf { A R M A } ( p , q )$ series, as we shall see in Section 3.3.

An immediate consequence of (2.5.28) is the innovations representation of $X _ { n + 1 }$ itself. Thus (defining $\theta _ { n 0 } : = 1$ ),

$$
X _ {n + 1} = X _ {n + 1} - \hat {X} _ {n + 1} + \hat {X} _ {n + 1} = \sum_ {j = 0} ^ {n} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right), \quad n = 0, 1, 2, \dots .
$$

![](images/3a490bf278b54828997d95c0b80840342bd6c1731e0ee9985bc35eb794c64408.jpg)

# Example 2.5.5 Recursive Prediction of an MA(1)

If $\{ X _ { t } \}$ is the time series defined by

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1}, \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right),
$$

then $\kappa ( i , j ) = 0$ for $| i - j | > 1 , \kappa ( i , i ) = \sigma ^ { 2 } \big ( 1 + \theta ^ { 2 } \big )$ , and $\kappa ( i , i { + } 1 ) = \theta \sigma ^ { 2 }$ . Application of the innovations algorithm leads at once to the recursions

$$
\begin{array}{l} \theta_ {n j} = 0, 2 \leq j \leq n, \\ \theta_ {n 1} = \nu_ {n - 1} ^ {- 1} \theta \sigma^ {2}, \\ v _ {0} = (1 + \theta^ {2}) \sigma^ {2}, \\ \end{array}
$$

and

$$
v _ {n} = \left[ 1 + \theta^ {2} - v _ {n - 1} ^ {- 1} \theta^ {2} \sigma^ {2} \right] \sigma^ {2}.
$$

For the particular case

$$
X _ {t} = Z _ {t} - 0. 9 Z _ {t - 1}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 1),
$$

the mean squared errors $\nu _ { n }$ of ${ \hat { X } } _ { n + 1 }$ and coefficients $\theta _ { n j }$ , $1 \leq j \leq n$ , in the innovations representation

$$
\hat {X} _ {n + 1} = \sum_ {j = 1} ^ {n} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right) = \theta_ {n 1} \left(X _ {n} - \hat {X} _ {n}\right)
$$

are found from the recursions to be as follows:

$$
\begin{array}{l} v _ {0} = 1. 8 1 0 0, \\ \theta_ {1 1} = - 0. 4 9 7 2, \quad v _ {1} = 1. 3 6 2 5, \\ \theta_ {2 1} = - 0. 6 6 0 6, \quad \theta_ {2 2} = 0, \quad \nu_ {2} = 1. 2 1 5 5, \\ \theta_ {3 1} = - 0. 7 4 0 4, \quad \theta_ {3 2} = 0, \quad \theta_ {3 3} = 0, \quad v _ {3} = 1. 1 4 3 6, \\ \theta_ {4 1} = - 0. 7 8 7 0, \quad \theta_ {4 2} = 0, \qquad \theta_ {4 3} = 0, \qquad \theta_ {4 4} = 0, \qquad v _ {4} = 1. 1 0 1 7. \\ \end{array}
$$

If we apply the Durbin–Levinson algorithm to the same problem, we find that the mean squared errors $\nu _ { n }$ of ${ \hat { X } } _ { n + 1 }$ and coefficients $\phi _ { n j }$ , $1 \leq j \leq n$ , in the representation

$$
\hat {X} _ {n + 1} = \sum_ {j = 1} ^ {n} \phi_ {n j} X _ {n + 1 - j}
$$

are as follows:

$$
\begin{array}{l} v _ {0} = 1. 8 1 0 0, \\ \phi_ {1 1} = - 0. 4 9 7 2, \quad v _ {1} = 1. 3 6 2 5, \\ \phi_ {2 1} = - 0. 6 6 0 6, \quad \phi_ {2 2} = - 0. 3 2 8 5, \quad v _ {2} = 1. 2 1 5 5, \\ \phi_ {3 1} = - 0. 7 4 0 4, \quad \phi_ {3 2} = - 0. 4 8 9 2, \quad \phi_ {3 3} = - 0. 2 4 3 3, \quad v _ {3} = 1. 1 4 3 6, \\ \phi_ {4 1} = - 0. 7 8 7 0, \quad \phi_ {4 2} = - 0. 5 8 2 8, \quad \phi_ {4 3} = - 0. 3 8 5 0, \quad \phi_ {4 4} = - 0. 1 9 1 4, \quad \nu_ {4} = 1. 1 0 1 7. \\ \end{array}
$$

Notice that as $n$ increases, $\nu _ { n }$ approaches the white noise variance and $\theta _ { n 1 }$ approaches $\theta$ . These results hold for any MA(1) process with $| \theta | < 1$ . The innovations algorithm is particularly well suited to forecasting $\mathrm { M A } ( q )$ processes, since for them $\theta _ { n j } = 0$ for $n - j > q$ . For $\operatorname { A R } ( p )$ processes the Durbin–Levinson algorithm is usually more convenient, since $\phi _ { n j } = 0$ for $n - j > p$ .

# 2.5.5 Recursive Calculation of the h-Step Predictors

For $h$ -step prediction we use the result

$$
P _ {n} \left(X _ {n + k} - P _ {n + k - 1} X _ {n + k}\right) = 0, \quad k \geq 1. \tag {2.5.29}
$$

This follows from (2.5.10) and the fact that

$$
E \left[ \left(X _ {n + k} - P _ {n + k - 1} X _ {n + k} - 0\right) X _ {n + j - 1} \right] = 0, \quad j = 1, \dots , n.
$$

Hence,

$$
\begin{array}{l} P _ {n} X _ {n + h} = P _ {n} P _ {n + h - 1} X _ {n + h} \\ = P _ {n} \hat {X} _ {n + h} \\ = P _ {n} \bigg (\sum_ {j = 1} ^ {n + h - 1} \theta_ {n + h - 1, j} \left(X _ {n + h - j} - \hat {X} _ {n + h - j}\right) \bigg). \\ \end{array}
$$

Applying (2.5.29) again and using the linearity of $P _ { n }$ we find that

$$
P _ {n} X _ {n + h} = \sum_ {j = h} ^ {n + h - 1} \theta_ {n + h - 1, j} \left(X _ {n + h - j} - \hat {X} _ {n + h - j}\right), \tag {2.5.30}
$$

where the coefficients $\theta _ { n j }$ are determined as before by the innovations algorithm. Moreover, the mean squared error can be expressed as

$$
\begin{array}{l} E (X _ {n + h} - P _ {n} X _ {n + h}) ^ {2} = E X _ {n + h} ^ {2} - E (P _ {n} X _ {n + h}) ^ {2} \\ = \kappa (n + h, n + h) - \sum_ {j = h} ^ {n + h - 1} \theta_ {n + h - 1, j} ^ {2} v _ {n + h - j - 1}. \tag {2.5.31} \\ \end{array}
$$

# 2.5.6 Prediction of a Stationary Process in Terms of Infinitely Many Past Values

It is often useful, when many past observations $X _ { m }$ , . . . , X0, X1, . . . , $X _ { n }$ $( m \ < \ 0$ ) are available, to evaluate the best linear predictor of $X _ { n + h }$ in terms of $1 , X _ { m } , \ldots , X _ { 0 }$ $1 , X _ { m }$ $X _ { 0 }$ , . . . , $X _ { n }$ . This predictor, which we shall denote by $P _ { m , n } X _ { n + h }$ , can easily be evaluated by the methods described above. If $| m |$ is large, this predictor can be approximated by the sometimes more easily calculated mean square limit

$$
\tilde {P} _ {n} X _ {n + h} = \lim  _ {m \rightarrow - \infty} P _ {m, n} X _ {n + h}.
$$

We shall refer to $\tilde { P } _ { n }$ as the prediction operator based on the infinite past, $\{ X _ { t }$ , $- \infty < t \leq n \}$ . Analogously we shall refer to $P _ { n }$ as the prediction operator based on the finite past, $\{ X _ { 1 } , \ldots , X _ { n } \}$ . (Mean square convergence of random variables is discussed in Appendix C.)

# 2.5.7 Determination of $\tilde { { P } } _ { n } { X } _ { n + h }$

If $\{ X _ { n } \}$ is a zero-mean stationary process with autocovariance function $\gamma ( \cdot )$ then, just as $P _ { n } X _ { n + h }$ is characterized by equation (2.5.10), $\tilde { P } _ { n } X _ { n + h }$ is characterized by the equations

$$
E \left[ \left(X _ {n + h} - \tilde {P} _ {n} X _ {n + h}\right) X _ {n + 1 - i} \right] = 0, \quad i = 1, 2, \dots .
$$

If we can find a solution to these equations, it will necessarily be the uniquely defined predictor $\tilde { P } _ { n } X _ { n + h }$ . An approach to this problem that is often effective is to assume that $ { \widetilde { P } } _ { n } X _ { n + h }$ can be expressed in the form

$$
\tilde {P} _ {n} X _ {n + h} = \sum_ {j = 1} ^ {\infty} \alpha_ {j} X _ {n + 1 - j},
$$

in which case the preceding equations reduce to

$$
E \left[ \left(X _ {n + h} - \sum_ {j = 1} ^ {\infty} \alpha_ {j} X _ {n + 1 - j}\right) X _ {n + 1 - i} \right] = 0, \quad i = 1, 2, \dots ,
$$

or equivalently,

$$
\sum_ {j = 1} ^ {\infty} \gamma (i - j) \alpha_ {j} = \gamma (h + i - 1), \quad i = 1, 2, \dots .
$$

This is an infinite set of linear equations for the unknown coefficients $\alpha _ { i }$ that determine $\tilde { P } _ { n } X _ { n + h }$ , provided that the resulting series converges.

# Properties of $\tilde { P } _ { n }$ :

Suppose that $E U ^ { 2 } ~ < ~ \infty$ $E U ^ { 2 } ~ < ~ \infty , E V ^ { 2 } ~ < ~ \infty , ~ a , b .$ $E V ^ { 2 } ~ < ~ \infty ,$ , and $c$ are constants, and $\Gamma =$ Cov(W, W).

1. $E [ ( U - \tilde { P } _ { n } ( U ) ) X _ { j } ] = 0$ , $j \leq n$ .   
2. $\tilde { P } _ { n } ( a U + b V + c ) = a \tilde { P } _ { n } ( U ) + b \tilde { P } _ { n } ( V ) + c .$   
3. ${ \tilde { P } } _ { n } ( U ) = U$ if $U$ is a limit of linear combinations of $X _ { j }$ , $j \le n$   
4. ${ \tilde { P } } _ { n } ( U ) = E U$ if $\mathrm { C o v } ( U , X _ { j } ) = 0$ for all $j \leq n$

These properties can sometimes be used to simplify the calculation of $\tilde { P } _ { n } X _ { n + h }$ , notably when the process $\{ X _ { t } \}$ is an ARMA process.

Example 2.5.6 Consider the causal invertible ARMA(1,1) process $\{ X _ { t } \}$ defined by

$$
X _ {t} - \phi X _ {t - 1} = Z _ {t} + \theta Z _ {t - 1}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, \sigma^ {2}).
$$

We know from (2.3.3) and (2.3.5) that we have the representations

$$
X _ {n + 1} = Z _ {n + 1} + (\phi + \theta) \sum_ {j = 1} ^ {\infty} \phi^ {j - 1} Z _ {n + 1 - j}
$$

and

$$
Z _ {n + 1} = X _ {n + 1} - (\phi + \theta) \sum_ {j = 1} ^ {\infty} (- \theta) ^ {j - 1} X _ {n + 1 - j}.
$$

Applying the operator ${ \tilde { P } } _ { n }$ to the second equation and using the properties of ${ \tilde { P } } _ { n }$ gives

$$
\tilde {P} _ {n} X _ {n + 1} = (\phi + \theta) \sum_ {j = 1} ^ {\infty} (- \theta) ^ {j - 1} X _ {n + 1 - j}.
$$

Applying the operator ${ \tilde { P } } _ { n }$ to the first equation and using the properties of ${ \tilde { P } } _ { n }$ gives

$$
\tilde {P} _ {n} X _ {n + 1} = (\phi + \theta) \sum_ {j = 1} ^ {\infty} \phi^ {j - 1} Z _ {n + 1 - j}.
$$

Hence,

$$
X _ {n + 1} - \tilde {P} _ {n} X _ {n + 1} = Z _ {n + 1},
$$

and so the mean squared error of the predictor $\tilde { P } _ { n } X _ { n + 1 }$ is $E Z _ { n + 1 } ^ { 2 } = \sigma ^ { 2 }$

![](images/011cbc1588ccf3e4a4d3d834984b78277ae048a26a0bb3ac2d200da2acdca293.jpg)

# 2.6 The Wold Decomposition

Consider the stationary process

$$
X _ {t} = A \cos (\omega t) + B \sin (\omega t),
$$

where $\omega \in ( 0 , \pi )$ is constant and A, $B$ are uncorrelated random variables with mean 0 and variance $\sigma ^ { 2 }$ . Notice that

$$
X _ {n} = (2 \cos \omega) X _ {n - 1} - X _ {n - 2} = \tilde {P} _ {n - 1} X _ {n}, \quad n = 0, \pm 1, \dots ,
$$

so that $X _ { n } - \tilde { P } _ { n - 1 } X _ { n } = 0$ for all $n$ . Processes with the latter property are said to be deterministic.

# The Wold Decomposition:

If $\{ X _ { t } \}$ is a nondeterministic stationary time series, then

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j} + V _ {t}, \tag {2.6.1}
$$

where

1. $\psi _ { 0 } = 1$ and $\textstyle \sum _ { j = 0 } ^ { \infty } \psi _ { j } ^ { 2 } < \infty$ ,   
2. $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \left( 0 , \sigma ^ { 2 } \right)$ ,   
3. $\operatorname { C o v } ( Z _ { s } , V _ { t } ) = 0$ for all $s$ and $t$   
4. $Z _ { t } = \tilde { P } _ { t } Z _ { t }$ for all $t$   
5. $V _ { t } = \tilde { P } _ { s } V _ { t }$ for all $s$ and $t$ , and   
6. $\{ V _ { t } \}$ is deterministic.

Here as in Section 2.5, $\tilde { P } _ { t } Y$ denotes the best predictor of $Y$ in terms of linear combinations, or limits of linear combinations of $1 , X _ { s } , - \infty \ < \ s \ \leq \ t$ . The sequences $\{ Z _ { t } \} , \ \{ \psi _ { j } \}$ , and $\{ V _ { t } \}$ are unique and can be written explicitly as $Z _ { t } = X _ { t } - \mathbf { \tilde { \tilde { P } } } _ { t - 1 } X _ { t }$ , $\psi _ { j } = E ( X _ { t } Z _ { t - j } ) / E \big ( Z _ { t } ^ { 2 } \big )$ , and $\begin{array} { r } { V _ { t } = X _ { t } - \sum _ { j = 0 } ^ { \infty } \psi _ { j } Z _ { t - j } } \end{array}$ . (See Brockwell and Davis (1991), p. 188.) For most of the zero-mean stationary time series dealt with in this book (in particular for all ARMA processes) the deterministic component $V _ { t }$ is 0 for all $t$ , and the series is then said to be purely nondeterministic.

Example 2.6.1 If $X _ { t } = U _ { t } + Y$ , where $\{ U _ { t } \} \sim \operatorname { W N } \left( 0 , \nu ^ { 2 } \right)$ , $E ( U _ { t } Y ) = 0$ for all $t$ , and Y has mean 0 and variance $\tau ^ { 2 }$ , then $\tilde { P } _ { t - 1 } X _ { t } = Y$ , since $Y$ is the mean square limit as $s  \infty$ of $[ X _ { t - 1 } + \cdot \cdot \cdot + X _ { t - s } ] / s$ , and $E [ ( X _ { t } - Y ) X _ { s } ] = 0$ for all $s \leq t - 1$ . Hence the sequences in the Wold decomposition of $\{ X _ { t } \}$ are given by $Z _ { t } = U _ { t }$ , $\psi _ { 0 } = 1$ , $\psi _ { j } = 0$ for $j > 0$ , and $V _ { t } = Y$ .

# Problems

2.1 Suppose that $X _ { 1 } , X _ { 2 } , . . . ,$ , is a stationary time series with mean $\mu$ and ACF $\rho ( \cdot )$ . Show that the best predictor of $X _ { n + h }$ of the form $a X _ { n } + b$ is obtained by choosing $a = \rho ( h )$ and $b = \mu ( 1 - \rho ( h ) )$ .   
2.2 Show that the process

$$
X _ {t} = A \cos (\omega t) + B \sin (\omega t), \quad t = 0, \pm 1, \dots
$$

(where $A$ and $B$ are uncorrelated random variables with mean 0 and variance 1 and $\omega$ is a fixed frequency in the interval $[ 0 , \pi ] )$ , is stationary and find its mean and autocovariance function. Deduce that the function $\kappa ( h ) = \cos ( \omega h ) , h =$ $0 , \pm 1 , . . . ,$ is nonnegative definite.

2.3 a. Find the ACVF of the time series $X _ { t } = Z _ { t } + 0 . 3 Z _ { t - 1 } - 0 . 4 Z _ { t - 2 }$ , where $\{ Z _ { t } \} \sim$ $\mathbf { W N } ( 0 , 1 )$ .   
b. Find the ACVF of the time series $Y _ { t } = \tilde { Z } _ { t } - 1 . 2 \tilde { Z } _ { t - 1 } - 1 . 6 \tilde { Z } _ { t - 2 }$ , where $\{ \tilde { Z } _ { t } \} \sim$ WN(0, 0.25). Compare with the answer found in (a).

2.4 It is clear that the function $\kappa ( h ) = 1 , h = 0 , \pm 1 , \ldots$ , is an autocovariance function, since it is the autocovariance function of the process $X _ { t } = Z$ $X _ { t } = Z , t = 0 , \pm 1 , . . . ,$ where $Z$ is a random variable with mean 0 and variance 1. By identifying appropriate sequences of random variables, show that the following functions are also autocovariance functions:

a. κ(h) = (−1)|h| $\kappa ( h ) = ( - 1 ) ^ { | h | }$   
b $\mathbf { \partial } \cdot \kappa ( h ) = 1 + \cos \left( { \frac { \pi h } { 2 } } \right) + \cos \left( { \frac { \pi h } { 4 } } \right)$   
$\mathsf { c . ~ } \kappa ( h ) = \left\{ { \begin{array} { l l } { 1 , } & { \mathrm { i f ~ } h = 0 , } \\ { 0 . 4 , } & { \mathrm { i f ~ } h = \pm 1 , } \\ { 0 , } & { \mathrm { o t h e r w i s e . } } \end{array} } \right.$

2.5 Suppose that $\{ X _ { t } , t = 0 , \pm 1 , . . . \}$ is stationary and that $| \theta | < 1$ . Show that for each fixed $n$ the sequence

$$
S _ {m} = \sum_ {j = 1} ^ {m} \theta^ {j} X _ {n - j}
$$

is convergent absolutely and in mean square (see Appendix C) as $m  \infty$

2.6 Verify the equations (2.2.6).

2.7 Show, using the geometric series $\textstyle 1 / ( 1 - x ) = \sum _ { j = 0 } ^ { \infty } x ^ { j }$ for $| x | < 1$ , that $1 / ( 1 -$ $\begin{array} { r } { \phi z ) = - \sum _ { j = 1 } ^ { \infty } \phi ^ { - j } z ^ { - j } } \end{array}$ for $| \phi | > 1$ and $| z | \geq 1$ .

2.8 Show that the autoregressive equations

$$
X _ {t} = \phi_ {1} X _ {t - 1} + Z _ {t}, \quad t = 0, \pm 1, \dots ,
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , \sigma ^ { 2 } )$ and $| \phi | ~ = ~ 1$ , have no stationary solution. HINT: Suppose there does exist a stationary solution $\{ X _ { t } \}$ and use the autoregressive equation to derive an expression for the variance of $X _ { t } - \phi _ { 1 } ^ { n + 1 } X _ { t - n - 1 }$ that contradicts the stationarity assumption.

2.9 Let $\{ Y _ { t } \}$ be the AR(1) plus noise time series defined by

$$
Y _ {t} = X _ {t} + W _ {t},
$$

where $\{ W _ { t } \} \sim \mathrm { W N } \big ( 0 , \sigma _ { w } ^ { 2 } \big )$ , $\{ X _ { t } \}$ is the AR(1) process of Example 2.2.1, i.e.,

$$
X _ {t} - \phi X _ {t - 1} = Z _ {t}, \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma_ {z} ^ {2}\right),
$$

and $E ( W _ { s } Z _ { t } ) = 0$ for all $s$ and $t$

a. Show that $\{ Y _ { t } \}$ is stationary and find its autocovariance function.   
b. Show that the time series $U _ { t } : = Y _ { t } - \phi Y _ { t - 1 }$ is 1-correlated and hence, by Proposition 2.1.1, is an MA(1) process.   
c. Conclude from (b) that $\{ Y _ { t } \}$ is an ARMA(1,1) process and express the three parameters of this model in terms of $\phi , \sigma _ { w } ^ { 2 }$ , and $\sigma _ { z } ^ { 2 }$ .

2.10 Use the program ITSM to compute the coefficients $\psi _ { j }$ and $\pi _ { j } , j = 1 , \ldots , 5 .$ , in the expansions

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j}
$$

and

$$
Z _ {t} = \sum_ {j = 0} ^ {\infty} \pi_ {j} X _ {t - j}
$$

for the ARMA(1,1) process defined by the equations

$$
X _ {t} - 0. 5 X _ {t - 1} = Z _ {t} + 0. 5 Z _ {t - 1}, \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

(Select File>Project>New>Univariate, then Model $>$ Specify. In the resulting dialog box enter 1 for the AR and MA orders, specify $\begin{array} { l c l c l } { \phi ( 1 ) } & { = } & { \theta ( 1 ) } & { = } & { 0 . 5 } \end{array}$ , and click OK. Finally, select Model>AR/MA Infinity>Default lag and the values of $\psi _ { j }$ and $\pi _ { j }$ will appear on the screen.) Check the results with those obtained in Section 2.3.

2.11 Suppose that in a sample of size 100 from an AR(1) process with mean $\mu , \phi = . 6$ , and $\sigma ^ { 2 } = 2$ we obtain $\bar { x } _ { 1 0 0 } = 0 . 2 7 1$ . Construct an approximate $95 \%$ confidence interval for $\mu$ . Are the data compatible with the hypothesis that $\mu = 0 ^ { \cdot }$ ?   
2.12 Suppose that in a sample of size 100 from an MA(1) process with mean $\mu$ , $\theta \ : = \ : - 0 . 6$ , and $\sigma ^ { 2 } = 1$ we obtain $\bar { x } _ { 1 0 0 } = 0 . 1 5 7$ . Construct an approximate $9 5 \%$ confidence interval for $\mu$ . Are the data compatible with the hypothesis that $\mu = 0 ?$   
2.13 Suppose that in a sample of size 100, we obtain $\hat { \rho } ( 1 ) = 0 . 4 3 8$ and $\hat { \rho } ( 2 ) = 0 . 1 4 5$ . a. Assuming that the data were generated from an AR(1) model, construct approximate $9 5 \%$ confidence intervals for both $\rho ( 1 )$ and $\rho ( 2 )$ . Based on these

two confidence intervals, are the data consistent with an AR(1) model with $\phi = 0 . 8 \mathrm { : }$

b. Assuming that the data were generated from an MA(1) model, construct approximate $9 5 \%$ confidence intervals for both $\rho ( 1 )$ and $\rho ( 2 )$ . Based on these two confidence intervals, are the data consistent with an MA(1) model with $\theta = 0 . 6 ?$

2.14 Let $\{ X _ { t } \}$ be the process defined in Problem 2.2.

a. Find $P _ { 1 } X _ { 2 }$ and its mean squared error.   
b. Find $P _ { 2 } X _ { 3 }$ and its mean squared error.   
c. Find $\tilde { P } _ { n } X _ { n + 1 }$ and its mean squared error.

2.15 Suppose that $\{ X _ { t } , t = 0 , \pm 1 , . . . \}$ is a stationary process satisfying the equations

$$
X _ {t} = \phi_ {1} X _ {t - 1} + \dots + \phi_ {p} X _ {t - p} + Z _ {t},
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } \big ( 0 , \sigma ^ { 2 } \big )$ and $Z _ { t }$ is uncorrelated with $X _ { s }$ for each $s \ < \ t$ . Show that the best linear predictor $P _ { n } X _ { n + 1 }$ of $X _ { n + 1 }$ in terms of $1 , X _ { 1 } , \ldots , X _ { n }$ , assuming $n > p$ , is

$$
P _ {n} X _ {n + 1} = \phi_ {1} X _ {n} + \dots + \phi_ {p} X _ {n + 1 - p}.
$$

What is the mean squared error of $P _ { n } X _ { n + 1 }$ ?

2.16 Use the program ITSM to plot the sample ACF and PACF up to lag 40 of the sunspot series $D _ { t } , t \ : = \ : 1 , 1 0 0$ , contained in the ITSM file SUNSPOTS.TSM. (Open the project SUNSPOTS.TSM and click on the second yellow button at the top of the screen to see the graphs. Repeated clicking on this button will toggle between graphs of the sample ACF, sample PACF, and both. To see the numerical values, right-click on the graph and select Info.) Fit an AR(2) model to the mean-corrected data by selecting Model>Estimation>Preliminary and click Yes to subtract the sample mean from the data. In the dialog box that follows, enter 2 for the AR order and make sure that the MA order is zero and that the Yule-Walker algorithm is selected without AICC minimization. Click OK and you will obtain a model of the form

$$
X _ {t} = \phi_ {1} X _ {t - 1} + \phi_ {2} X _ {t - 2} + Z _ {t}, \qquad \text {w h e r e} \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right),
$$

for the mean-corrected series $X _ { t } = D _ { t } – 4 6 . 9 3$ . Record the values of the estimated parameters $\phi _ { 1 }$ , $\phi _ { 2 }$ , and $\sigma ^ { 2 }$ . Compare the model and sample ACF and PACF by selecting the third yellow button at the top of the screen. Print the graphs by right-clicking and selecting Print.

2.17 Without exiting from ITSM, use the model found in the preceding problem to compute forecasts of the next ten values of the sunspot series. (Select Forecasting>ARMA,make sure that the number of forecasts is set to 10 and the box Add the mean to the forecasts is checked, and then click OK. You will see a graph of the original data with the ten forecasts appended. Right-click on the graph and then on Info to get the numerical values of the forecasts. Print the graph as described in Problem 2.16.) The details of the calculations will be taken up in Chapter 3 when we discuss ARMA models in detail.

2.18 Let $\{ X _ { t } \}$ be the stationary process defined by the equations

$$
X _ {t} = Z _ {t} - \theta Z _ {t - 1}, \quad t = 0, \pm 1, \dots ,
$$

where $| \theta | < 1$ and $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \big ( 0 , \sigma ^ { 2 } \big )$ . Show that the best linear predictor $\tilde { P } _ { n } X _ { n + 1 }$ of $X _ { n + 1 }$ based on $\{ X _ { j } , - \infty < j \leq n \}$ is

$$
\tilde {P} _ {n} X _ {n + 1} = - \sum_ {j = 1} ^ {\infty} \theta^ {j} X _ {n + 1 - j}.
$$

What is the mean squared error of the predictor $\boldsymbol { \tilde { P } _ { n } } \boldsymbol { X _ { n + 1 } } \boldsymbol { ? }$

2.19 If $\{ X _ { t } \}$ is defined as in Problem 2.18 and $\theta = 1$ , find the best linear predictor $P _ { n } X _ { n + 1 }$ of $X _ { n + 1 }$ in terms of $X _ { 1 }$ , . . . , $X _ { n }$ . What is the corresponding mean squared error?

2.20 In the innovations algorithm, show that for each $n \geq 2$ , the innovation $X _ { n } - { \hat { X } } _ { n }$ is uncorrelated with X1, . . . , $X _ { n - 1 }$ . Conclude that $X _ { n } - { \hat { X } } _ { n }$ is uncorrelated with the innovations $X _ { 1 } - \hat { X } _ { 1 } , \dots .$ , $X _ { n - 1 } - { \hat { X } } _ { n - 1 }$ .

2.21 Let $X _ { 1 }$ , $X _ { 2 }$ , $X _ { 4 }$ , $X _ { 5 }$ be observations from the MA(1) model

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

a. Find the best linear estimate of the missing value $X _ { 3 }$ in terms of $X _ { 1 }$ and $X _ { 2 }$   
b. Find the best linear estimate of the missing value $X _ { 3 }$ in terms of $X _ { 4 }$ and $X _ { 5 }$   
c. Find the best linear estimate of the missing value $X _ { 3 }$ in terms of $X _ { 1 } , X _ { 2 } , X _ { 4 }$ and $X _ { 5 }$ .   
d. Compute the mean squared errors for each of the estimates in (a)–(c).

2.22 Repeat parts (a)–(d) of Problem 2.21 assuming now that the observations $X _ { 1 } , X _ { 2 }$ , $X _ { 4 }$ , $X _ { 5 }$ are from the causal AR(1) model

$$
X _ {t} = \phi X _ {t - 1} + Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

# 3

# ARMA Models

3.1 ARMA( p, q) Processes   
3.2 The ACF and PACF of an ARMA( p, q) Process   
3.3 Forecasting ARMA Processes

In this chapter we introduce an important parametric family of stationary time series, the autoregressive moving-average, or ARMA, processes. For a large class of autocovariance functions $\gamma ( \cdot )$ it is possible to find an ARMA process $\{ X _ { t } \}$ with ACVF $\gamma _ { X } ( \cdot )$ such that $\gamma ( \cdot )$ is well approximated by $\gamma _ { X } ( \cdot )$ . In particular, for any positive integer $K$ , there exists an ARMA process $\{ X _ { t } \}$ such that $\gamma _ { X } ( h ) = \gamma ( h )$ for $h = 0 , 1 , \ldots , K$ . For this (and other) reasons, the family of ARMA processes plays a key role in the modeling of time series data. The linear structure of ARMA processes also leads to a substantial simplification of the general methods for linear prediction discussed earlier in Section 2.5.

# 3.1 ARMA $( p , q )$ Processes

In Section 2.3 we introduced an ARMA(1,1) process and discussed some of its key properties. These included existence and uniqueness of stationary solutions of the defining equations and the concepts of causality and invertibility. In this section we extend these notions to the general ARMA $( p , q )$ process.

# Definition 3.1.1

$\{ X _ { t } \}$ is an $\mathbf { A R M A } ( p , q )$ process if $\{ X _ { t } \}$ is stationary and if for every t,

$$
X _ {t} - \phi_ {1} X _ {t - 1} - \dots - \phi_ {p} X _ {t - p} = Z _ {t} + \theta_ {1} Z _ {t - 1} + \dots + \theta_ {q} Z _ {t - q}, \tag {3.1.1}
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } \left( 0 , \sigma ^ { 2 } \right)$ and the polynomials $\big ( 1 - \phi _ { 1 } z - \ldots - \phi _ { p } z ^ { p } \big )$ and $( 1 +$ $\theta _ { 1 } z + \ldots + \theta _ { q } z ^ { q } )$ have no common factors.

The process $\{ X _ { t } \}$ is said to be an ARMA $( p , q )$ process with mean $\mu$ if $\{ X _ { t } - \mu \}$ is an $\mathbf { A R M A } ( p , q )$ $( p , q )$ process.

It is convenient to use the more concise form of (3.1.1)

$$
\phi (B) X _ {t} = \theta (B) Z _ {t}, \tag {3.1.2}
$$

where $\phi ( \cdot )$ and $\theta ( \cdot )$ are the pth and qth-degree polynomials

$$
\phi (z) = 1 - \phi_ {1} z - \dots - \phi_ {p} z ^ {p}
$$

and

$$
\theta (z) = 1 + \theta_ {1} z + \dots + \theta_ {q} z ^ {q},
$$

and $B$ is the backward shift operator $( B ^ { j } X _ { t } ~ = ~ X _ { t - j }$ , $B ^ { j } Z _ { t } = Z _ { t - j }$ , $j = 0 , \pm 1 , . . . )$ . The time series $\{ X _ { t } \}$ is said to be an autoregressive process of order $\pmb { p }$ (or $\operatorname { A R } ( p ) ,$ ) if $\theta ( z ) \equiv 1$ , and a moving-average process of order $\pmb q$ (or $\operatorname { M A } ( q ) )$ if $\phi ( z ) \equiv 1$ .

An important part of Definition 3.1.1 is the requirement that $\{ X _ { t } \}$ be stationary. In Section 2.3 we showed, for the ARMA(1,1) equations (2.3.1), that a stationary solution exists (and is unique) if and only if $\phi _ { 1 } \neq \pm 1$ . The latter is equivalent to the condition that the autoregressive polynomial $\phi ( z ) = 1 - \phi _ { 1 } z \neq 0$ for $z = \pm 1$ . The analogous condition for the general ARMA $( p , q )$ process is $\phi ( z ) = 1 - \phi _ { 1 } z - \cdot \cdot \cdot -$ $\phi _ { p } z ^ { p } \neq 0$ for all complex z with $| z | = 1$ . (Complex z is used here, since the zeros of a polynomial of degree $p > 1$ may be either real or complex. The region defined by the set of complex z such that $| z | = 1$ is referred to as the unit circle.) If $\phi ( z ) \neq 0$ for all $z$ on the unit circle, then there exists $\delta > 0$ such that

$$
\frac {1}{\phi (z)} = \sum_ {j = - \infty} ^ {\infty} \chi_ {j} z ^ {j} \text {f o r} 1 - \delta <   | z | <   1 + \delta ,
$$

and $\scriptstyle \sum _ { j = - \infty } ^ { \infty } | \chi _ { j } | < \infty$ . We can then define $1 / \phi ( B )$ as the linear filter with absolutely summable coefficients

$$
\frac {1}{\phi (B)} = \sum_ {j = - \infty} ^ {\infty} \chi_ {j} B ^ {j}.
$$

Applying the operator $\chi ( B ) : = 1 / \phi ( B )$ to both sides of (3.1.2), we obtain

$$
X _ {t} = \chi (B) \phi (B) X _ {t} = \chi (B) \theta (B) Z _ {t} = \psi (B) Z _ {t} = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} Z _ {t - j}, \tag {3.1.3}
$$

where the A $\begin{array} { r } { \psi ( z ) = \chi ( z ) \theta ( z ) = \sum _ { j = - \infty } ^ { \infty } \psi _ { j } z ^ { j } } \end{array}$ .t he argument given in Section 2.3 foris the unique stationary solution of $\psi ( B ) Z _ { t }$ (3.1.1).

# Existence and Uniqueness:

A stationary solution $\{ X _ { t } \}$ of equation (3.1.1) exists (and is also the unique stationary solution) if and only if

$$
\phi (z) = 1 - \phi_ {1} z - \dots - \phi_ {p} z ^ {p} \neq 0 \quad \text {f o r a l l} | z | = 1. \tag {3.1.4}
$$

In Section 2.3 we saw that the ARMA(1,1) process is causal, i.e., that $X _ { t }$ can be expressed in terms of $Z _ { s }$ , $s \leq t$ , if and only if $| \phi _ { 1 } | < 1$ . For a general ARMA $( p , q )$ process the analogous condition is that $\phi ( z ) \neq 0$ for $| z | \leq 1$ , i.e., the zeros of the autoregressive polynomial must all be greater than 1 in absolute value.

# Causality:

An ARMA $( p , q )$ process $\{ X _ { t } \}$ is causal, or a causal function of $\{ Z _ { t } \}$ , if there exist constants $\{ \psi _ { j } \}$ such that $\textstyle \sum _ { j = 0 } ^ { \infty } | \psi _ { j } | < \infty$ and

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j} \text {f o r a l l} t. \tag {3.1.5}
$$

Causality is equivalent to the condition

$$
\phi (z) = 1 - \phi_ {1} z - \dots - \phi_ {p} z ^ {p} \neq 0 \text {f o r a l l} | z | \leq 1. \tag {3.1.6}
$$

The proof of the equivalence between causality and (3.1.6) follows from elementary properties of power series. From (3.1.3) we see that $\{ X _ { t } \}$ is causal if and only if $\begin{array} { r } { \chi ( z ) : = 1 / \phi ( z ) = \sum _ { j = 0 } ^ { \infty } \chi _ { j } z ^ { j } } \end{array}$ (assuming that $\phi ( z )$ and $\theta ( z )$ have no common factors). But this, in turn, is equivalent to (3.1.6).

The sequence $\{ \psi _ { j } \}$ in (3.1.5) is determined by the relation $\begin{array} { r } { \psi ( z ) = \sum _ { j = 0 } ^ { \infty } \psi _ { j } z ^ { j } = } \end{array}$ $\theta ( z ) / \phi ( z )$ , or equivalently by the identity

$$
\left(1 - \phi_ {1} z - \dots - \phi_ {p} z ^ {p}\right) (\psi_ {0} + \psi_ {1} z + \dots) = 1 + \theta_ {1} z + \dots + \theta_ {q} z ^ {q}.
$$

Equating coefficients of $z ^ { j } , j = 0 , 1 , . . . ,$ we find that

$$
1 = \psi_ {0},
$$

$$
\theta_ {1} = \psi_ {1} - \psi_ {0} \phi_ {1},
$$

$$
\theta_ {2} = \psi_ {2} - \psi_ {1} \phi_ {1} - \psi_ {0} \phi_ {2},
$$

or equivalently,

$$
\psi_ {j} - \sum_ {k = 1} ^ {p} \phi_ {k} \psi_ {j - k} = \theta_ {j}, j = 0, 1, \dots , \tag {3.1.7}
$$

where $\theta _ { 0 } : = 1 , \theta _ { j } : = 0$ for $j > q$ , and $\psi _ { j } : = 0$ for $j < 0$

Invertibility, which allows $Z _ { t }$ to be expressed in terms of $X _ { s }$ , $s \leq t$ , has a similar characterization in terms of the moving-average polynomial.

# Invertibility:

An ARMA( p, q) process $\{ X _ { t } \}$ is invertible if there exist constants $\{ \pi _ { j } \}$ such that $\textstyle \sum _ { j = 0 } ^ { \infty } | \pi _ { j } | < \infty$ and

$$
Z _ {t} = \sum_ {j = 0} ^ {\infty} \pi_ {j} X _ {t - j} \text {f o r a l l} t.
$$

Invertibility is equivalent to the condition

$$
\theta (z) = 1 + \theta_ {1} z + \dots + \theta_ {q} z ^ {q} \neq 0 \text {f o r a l l} | z | \leq 1.
$$

Interchanging the roles of the AR and MA polynomials, we find from (3.1.7) that the sequence $\{ \pi _ { j } \}$ is determined by the equations

$$
\pi_ {j} + \sum_ {k = 1} ^ {q} \theta_ {k} \pi_ {j - k} = - \phi_ {j}, j = 0, 1, \dots , \tag {3.1.8}
$$

where $\phi _ { 0 } : = - 1$ , $\phi _ { j } : = 0$ for $j > p$ , and $\pi _ { j } : = 0$ for $j < 0$ .

Example 3.1.1 An ARMA(1,1) Process Consider the ARMA(1,1) process $\{ X _ { t } \}$ satisfying the equations

$$
X _ {t} - 0. 5 X _ {t - 1} = Z _ {t} + 0. 4 Z _ {t - 1}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right). \tag {3.1.9}
$$

Since the autoregressive polynomial $\phi ( z ) = 1 - 0 . 5 z \quad$ has a zero at $z = 2$ , which is located outside the unit circle, we conclude from (3.1.4) and (3.1.6) that there exists a unique ARMA process satisfying (3.1.9) that is also causal. The coefficients $\{ \psi _ { j } \}$ in the MA(∞) representation of $\{ X _ { t } \}$ are found directly from (3.1.7):

$$
\psi_ {0} = 1,
$$

$$
\psi_ {1} = 0. 4 + 0. 5,
$$

$$
\psi_ {2} = 0. 5 (0. 4 + 0. 5),
$$

$$
\psi_ {j} = 0. 5 ^ {j - 1} (0. 4 + 0. 5), \quad j = 1, 2, \ldots .
$$

The MA polynomial $\theta ( z ) = 1 + 0 . 4 z$ has a zero at $z = - 1 / 0 . 4 = - 2 . 5$ , which is also located outside the unit circle. This implies that $\{ X _ { t } \}$ is invertible with coefficients $\{ \pi _ { j } \}$ given by [see (3.1.8)]

$$
\pi_ {0} = 1,
$$

$$
\pi_ {1} = - (0. 4 + 0. 5),
$$

$$
\pi_ {2} = - (0. 4 + 0. 5) (- 0. 4),
$$

$$
\pi_ {j} = - (0. 4 + 0. 5) (- 0. 4) ^ {j - 1}, \quad j = 1, 2, \dots .
$$

(A direct derivation of these formulas for $\{ \psi _ { j } \}$ and $\{ \pi _ { j } \}$ was given in Section 2.3 without appealing to the recursions (3.1.7) and (3.1.8).)

# Example 3.1.2 An AR(2) Process

Let $\{ X _ { t } \}$ be the AR(2) process

$$
X _ {t} = 0. 7 X _ {t - 1} - 0. 1 X _ {t - 2} + Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

The autoregressive polynomial for this process has the factorization $\phi ( z ) = 1 - 0 . 7 z +$ $0 . 1 z ^ { 2 } = ( 1 - 0 . 5 z ) ( 1 - 0 . 2 z )$ , and is therefore zero at $z = 2$ and $z = 5$ . Since these zeros lie outside the unit circle, we conclude that $\{ X _ { t } \}$ is a causal AR(2) process with coefficients $\{ \psi _ { j } \}$ given by

$$
\psi_ {0} = 1,
$$

$$
\psi_ {1} = 0. 7,
$$

$$
\psi_ {2} = 0. 7 ^ {2} - 0. 1,
$$

$$
\psi_ {j} = 0. 7 \psi_ {j - 1} - 0. 1 \psi_ {j - 2}, \quad j = 2, 3, \dots .
$$

While it is a simple matter to calculate $\psi _ { j }$ numerically for any $j$ , it is possible also to give an explicit solution of these difference equations using the theory of linear difference equations (see Brockwell and Davis (1991), Section 3.6).

The option Model>Specifyof the program ITSM allows the entry of any causal $\mathbf { A R M A } ( p , q )$ model with $p < 2 8$ and $q < 2 8$ . This option contains a causality check and will immediately let you know if the entered model is noncausal. (A causal model can be obtained by setting all the AR coefficients equal to 0.001.) Once a causal model has been entered, the coefficients $\psi _ { j }$ in the MA(∞) representation of the process can be computed by selecting Model>AR/MA Infinity. This option will also compute the $\mathbf { A R } ( \infty )$ coefficients $\pi _ { j }$ , provided that the model is invertible.

# Example 3.1.3 An ARMA(2,1) Process

Consider the ARMA(2,1) process defined by the equations

$$
X _ {t} - 0. 7 5 X _ {t - 1} + 0. 5 6 2 5 X _ {t - 2} = Z _ {t} + 1. 2 5 Z _ {t - 1}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

The AR polynomial $\phi ( z ) = 1 - 0 . 7 5 z + 0 . 5 6 2 5 z ^ { 2 }$ has zeros at $z = 2 \big ( 1 \pm i \sqrt { 3 } \big ) / 3$ , which lie outside the unit circle. The process is therefore causal. On the other hand, the MA polynomial $\theta ( z ) = 1 + 1 . 2 5 z$ has a zero at $z = - 0 . 8$ , and hence $\{ X _ { t } \}$ is not invertible.

![](images/b685539d3164c41a6094d01eddff47f1956e3312978072bab68a64c92e299fd6.jpg)

Remark 1. It should be noted that causality and invertibility are properties not of $\{ X _ { t } \}$ alone, but rather of the relationship between the two processes $\{ X _ { t } \}$ and $\{ Z _ { t } \}$ appearing in the defining ARMA equations (3.1.1). -

Remark 2. If $\{ X _ { t } \}$ is an ARMA process defined by $\phi ( B ) X _ { t } = \theta ( B ) Z _ { t }$ , where $\theta ( z ) \neq 0$ if $| z | = 1$ , then it is always possible (see Brockwell and Davis (1991), p. 127) to find polynomials $\tilde { \phi } ( z )$ and $ { \tilde { \theta } } ( z )$ and a white noise sequence $\{ W _ { t } \}$ such that $ { \tilde { \phi } } (  { \hat { B } } )  { \boldsymbol { X } } _ { t } =  { \tilde { \theta } } (  { \boldsymbol { B } } )  { \boldsymbol { W } } _ { t }$ and $\tilde { \theta } ( z )$ and $\tilde { \phi } ( z )$ are nonzero for $| z | \leq 1 .$ . However, if the original white noise sequence $\{ Z _ { t } \}$ is iid, then the new white noise sequence will not be iid unless $\{ Z _ { t } \}$ is Gaussian.

![](images/3a88a703e3b0492f469bb6002930e530f71919a6c4503e9f0216568ade1e231b.jpg)

In view of Remark 2, we will focus our attention principally on causal and invertible ARMA processes.

# 3.2 The ACF and PACF of an ARMA $( p , q )$ Process

In this section we discuss three methods for computing the autocovariance function $\gamma ( \cdot )$ of a causal ARMA process $\{ X _ { t } \}$ . The autocorrelation function is readily found

from the ACVF on dividing by $\gamma ( 0 )$ . The partial autocorrelation function (PACF) is also found from the function $\gamma ( \cdot )$ .

# 3.2.1 Calculation of the ACVF

First we determine the ACVF $\gamma ( \cdot )$ of the causal ARMA $( p , q )$ process defined by

$$
\phi (B) X _ {t} = \theta (B) Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right), \tag {3.2.1}
$$

where $\phi ( z ) = 1 - \phi _ { 1 } z - \cdot \cdot \cdot - \phi _ { p } z ^ { p }$ and $\theta ( z ) = 1 + \theta _ { 1 } z + \cdot \cdot \cdot + \theta _ { q } z ^ { q }$ . The causality assumption implies that

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j}, \tag {3.2.2}
$$

where $\textstyle \sum _ { j = 0 } ^ { \infty } \psi _ { j } z ^ { j } = \theta ( z ) / \phi ( z ) , \ | z |$ $| z | \leq 1$ . The calculation of the sequence $\{ \psi _ { j } \}$ was discussed in Section 3.1.

First Method. From Proposition 2.2.1 and the representation (3.2.2), we obtain

$$
\gamma (h) = E \left(X _ {t + h} X _ {t}\right) = \sigma^ {2} \sum_ {j = 0} ^ {\infty} \psi_ {j} \psi_ {j + | h |}. \tag {3.2.3}
$$

# Example 3.2.1 The ARMA(1,1) Process

Substituting from (2.3.3) into (3.2.3), we find that the ACVF of the process defined by

$$
X _ {t} - \phi X _ {t - 1} = Z _ {t} + \theta Z _ {t - 1}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right), \tag {3.2.4}
$$

with $| \phi | < 1$ is given by

$$
\begin{array}{l} \gamma (0) = \sigma^ {2} \sum_ {j = 0} ^ {\infty} \psi_ {j} ^ {2} \\ = \sigma^ {2} \left[ 1 + (\theta + \phi) ^ {2} \sum_ {j = 0} ^ {\infty} \phi^ {2 j} \right] \\ = \sigma^ {2} \left[ 1 + \frac {(\theta + \phi) ^ {2}}{1 - \phi^ {2}} \right], \\ \end{array}
$$

$$
\begin{array}{l} \gamma (1) = \sigma^ {2} \sum_ {j = 0} ^ {\infty} \psi_ {j + 1} \psi_ {j} \\ = \sigma^ {2} \left[ \theta + \phi + (\theta + \phi) ^ {2} \phi \sum_ {j = 0} ^ {\infty} \phi^ {2 j} \right] \\ = \sigma^ {2} \left[ \theta + \phi + \frac {(\theta + \phi) ^ {2} \phi}{1 - \phi^ {2}} \right], \\ \end{array}
$$

and

$$
\gamma (h) = \phi^ {h - 1} \gamma (1), \quad h \geq 2.
$$

# Example 3.2.2 The MA(q) Process

For the process

$$
X _ {t} = Z _ {t} + \theta_ {1} Z _ {t - 1} + \dots + \theta_ {q} Z _ {t - q}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right),
$$

Equation (3.2.3) immediately gives the result

$$
\gamma (h) = \left\{ \begin{array}{l l} \sigma^ {2} \sum_ {j = 0} ^ {q - | h |} \theta_ {j} \theta_ {j + | h |}, & \text {i f} | h | \leq q, \\ 0, & \text {i f} | h | > q, \end{array} \right.
$$

where $\theta _ { 0 }$ is defined to be 1. The ACVF of the $\mathrm { M A } ( q )$ process thus has the distinctive feature of vanishing at lags greater than $q$ . Data for which the sample ACVF is small for lags greater than $q$ therefore suggest that an appropriate model might be a moving average of order $q$ (or less). Recall from Proposition 2.1.1 that every zero-mean stationary process with correlations vanishing at lags greater than $q$ can be represented as a moving-average process of order $q$ or less.

![](images/d36214e64c89908ca9c9753d1e8619c1e8da76f5f8568a73247992d412340ed4.jpg)

Second Method. If we multiply each side of the equations

$$
X _ {t} - \phi_ {1} X _ {t - 1} - \dots - \phi_ {p} X _ {t - p} = Z _ {t} + \theta_ {1} Z _ {t - 1} + \dots + \theta_ {q} Z _ {t - q},
$$

by $X _ { t - k } , k = 0 , 1 , 2 , . . .$ $X _ { t - k }$ , and take expectations on each side, we find that

$$
\gamma (k) - \phi_ {1} \gamma (k - 1) - \dots - \phi_ {p} \gamma (k - p) = \sigma^ {2} \sum_ {j = 0} ^ {\infty} \theta_ {k + j} \psi_ {j}, \quad 0 \leq k <   m, \tag {3.2.5}
$$

and

$$
\gamma (k) - \phi_ {1} \gamma (k - 1) - \dots - \phi_ {p} \gamma (k - p) = 0, \quad k \geq m, \tag {3.2.6}
$$

where $m = \operatorname* { m a x } ( p , q + 1 )$ , $\psi _ { j } : = 0$ for $j < 0$ , $\theta _ { 0 } : = 1$ , and $\theta _ { j } : = 0$ for $j \not \in \{ 0 , \ldots , q \}$ . In calculating the right-hand side of (3.2.5) we have made use of the expansion (3.2.2). Equations (3.2.6) are a set of homogeneous linear difference equations with constant coefficients, for which the solution is well known (see, e.g., Brockwell and Davis (1991), Section 3.6) to be of the form

$$
\gamma (h) = \alpha_ {1} \xi_ {1} ^ {- h} + \alpha_ {2} \xi_ {2} ^ {- h} + \dots + \alpha_ {p} \xi_ {p} ^ {- h}, \quad h \geq m - p, \tag {3.2.7}
$$

where $\xi _ { 1 } , \ldots , \xi _ { p }$ are the roots (assumed to be distinct) of the equation $\phi ( z ) = 0$ , and $\alpha _ { 1 } , \ldots , \alpha _ { p }$ are arbitrary constants. (For further details, and for the treatment of the case where the roots are not distinct, see Brockwell and Davis (1991), Section 3.6.) Of course, we are looking for the solution of (3.2.6) that also satisfies (3.2.5). We therefore substitute the solution (3.2.7) into (3.2.5) to obtain a set of $m$ linear equations that then uniquely determine the constants $\alpha _ { 1 } , \ldots , \alpha _ { p }$ and the $m - p$ autocovariances $\gamma ( h ) , 0 \leq h < m - p$ .

# Example 3.2.3 The ARMA(1,1) Process

For the causal ARMA(1,1) process defined in Example 3.2.1, equations (3.2.5) are

$$
\gamma (0) - \phi \gamma (1) = \sigma^ {2} (1 + \theta (\theta + \phi)) \tag {3.2.8}
$$

and

$$
\gamma (1) - \phi \gamma (0) = \sigma^ {2} \theta . \tag {3.2.9}
$$

Equation (3.2.6) takes the form

$$
\gamma (k) - \phi \gamma (k - 1) = 0, \quad k \geq 2. \tag {3.2.10}
$$

The solution of (3.2.10) is

$$
\gamma (h) = \alpha \phi^ {h}, \quad h \geq 1.
$$

Substituting this expression for $\gamma ( h )$ into the two preceding equations (3.2.8) and (3.2.9) gives two linear equations for $\alpha$ and the unknown autocovariance $\gamma ( 0 )$ . These equations are easily solved, giving the autocovariances already found for this process in Example 3.2.1.

![](images/8fb55d16ecfb934a2397ac9388e1c4294d0883aa7e1c947c4dff216c4705b492.jpg)

# Example 3.2.4 The General AR(2) Process

For the causal AR(2) process defined by

$$
\left(1 - \xi_ {1} ^ {- 1} B\right) \left(1 - \xi_ {2} ^ {- 1} B\right) X _ {t} = Z _ {t}, \quad | \xi_ {1} |, | \xi_ {2} | > 1, \xi_ {1} \neq \xi_ {2},
$$

we easily find from (3.2.7) and (3.2.5) using the relations

$$
\phi_ {1} = \xi_ {1} ^ {- 1} + \xi_ {2} ^ {- 1}
$$

and

$$
\phi_ {2} = - \xi_ {1} ^ {- 1} \xi_ {2} ^ {- 1}
$$

that

$$
\gamma (h) = \frac {\sigma^ {2} \xi_ {1} ^ {2} \xi_ {2} ^ {2}}{(\xi_ {1} \xi_ {2} - 1) (\xi_ {2} - \xi_ {1})} \left[ (\xi_ {1} ^ {2} - 1) ^ {- 1} \xi_ {1} ^ {1 - h} - (\xi_ {2} ^ {2} - 1) ^ {- 1} \xi_ {2} ^ {1 - h} \right]. \tag {3.2.11}
$$

Figures 3-1, 3-2, 3-3, and 3-4 illustrate some of the possible forms of $\gamma ( \cdot )$ for different values of $\xi _ { 1 }$ and $\xi _ { 2 }$ . Notice that in the case of complex conjugate roots $\xi _ { 1 } = r e ^ { i \theta }$ and $\xi _ { 2 } = r e ^ { - i \theta }$ , $0 < \theta < \pi$ , we can write (3.2.11) in the more illuminating form

$$
\gamma (h) = \frac {\sigma^ {2} r ^ {4} \cdot r ^ {- h} \sin (h \theta + \psi)}{(r ^ {2} - 1) (r ^ {4} - 2 r ^ {2} \cos 2 \theta + 1) \sin \theta}, \tag {3.2.12}
$$

![](images/8502e77e0d3a6dc876c902e8ee812e2539ef47860b172bbdf3bf718663896d15.jpg)  
Figure 3-1

The model ACF of the AR(2) series of Example 3.2.4 with

$$
\xi_ {1} = 2 \text {a n d} \xi_ {2} = 5
$$

![](images/32d58b9ca3c1a93cc25ff7d0df0d50f0a192105563a7833bc9632fae0a88a3d1.jpg)  
Figure 3-2 The model ACF of the AR(2) series of Example 3.2.4 with $\xi _ { 1 } = 1 0 / 9$ and $\xi _ { 2 } = 2$

Figure 3-3   
![](images/e22b653c970a3f871e4ff88f7270cb8180bff59e2a1fc6e38222232289d978d6.jpg)  
The model ACF of the AR(2) series of Example 3.2.4 with $\xi _ { 1 } = - 1 0 / 9$ and $\xi _ { 2 } = 2$

where

$$
\tan \psi = \frac {r ^ {2} + 1}{r ^ {2} - 1} \tan \theta \tag {3.2.13}
$$

and cos $\psi$ has the same sign as cos $\theta$ . Thus in this case $\gamma ( \cdot )$ has the form of a damped sinusoidal function with damping factor $r ^ { - 1 }$ and period $2 \pi / \theta$ . If the roots are close to the unit circle, then $r$ is close to 1, the damping is slow, and we obtain a nearly sinusoidal autocovariance function.

Third Method. The autocovariances can also be found by solving the first $p + 1$ equations of (3.2.5) and (3.2.6) for $\gamma ( 0 ) \ldots , \gamma ( p )$ and then using the subsequent equations to solve successively for $\gamma ( p + 1 ) , \gamma ( p + 2 ) , \ldots .$ $\gamma ( p + 1 )$ This is an especially convenient method for numerical determination of the autocovariances $\gamma ( h )$ and is used in the option $\mathtt { M o d e l } \mathtt { > A C F / P A C F > M o d e l }$ of the program ITSM.

![](images/dc6a913c7f38621d1686b822f90268f5b15633914f5d72deffffe74d3d4300a4.jpg)  
Figure 3-4 The model ACF of the AR(2) series of Example 3.2.4 with $\xi _ { 1 } = 2 ( 1 \stackrel { \cdot } { + } i \sqrt { 3 } ) / 3$ and $\xi _ { 2 } = 2 ( 1 - i \sqrt { 3 } ) / 3$

Example 3.2.5 Consider again the causal ARMA(1,1) process of Example 3.2.1. To apply the third method we simply solve (3.2.8) and (3.2.9) for $\gamma ( 0 )$ and $\gamma ( 1 )$ . Then $\gamma ( 2 ) , \gamma ( 3 ) , \ldots$ can be found successively from (3.2.10). It is easy to check that this procedure gives the same results as those obtained in Examples 3.2.1 and 3.2.3.

![](images/4da7512d2e6229a1033ce4047d353c6c5cf342f45b3717faf45dc7632a40cda3.jpg)

# 3.2.2 The Autocorrelation Function

Recall that the ACF of an ARMA process $\{ X _ { t } \}$ is the function $\rho ( \cdot )$ found immediately from the ACVF $\gamma ( \cdot )$ as

$$
\rho (h) = \frac {\gamma (h)}{\gamma (0)}.
$$

Likewise, for any set of observations $\{ x _ { 1 } , \ldots , x _ { n } \}$ , the sample ACF $\hat { \rho } ( \cdot )$ is computed as

$$
\hat {\rho} (h) = \frac {\hat {\gamma} (h)}{\hat {\gamma} (0)}.
$$

The Sample ACF of an MA(q) Series. Given observations $\{ x _ { 1 } , \ldots , x _ { n } \}$ of a time series, one approach to the fitting of a model to the data is to match the sample ACF of the data with the ACF of the model. In particular, if the sample ACF ${ \hat { \rho } } ( h )$ is significantly different from zero for $0 ~ \leq ~ h ~ \leq ~ q$ and negligible for $h \ > \ q$ , Example 3.2.2 suggests that an $\mathrm { M A } ( q )$ model might provide a good representation of the data. In order to apply this criterion we need to take into account the random variation expected in the sample autocorrelation function before we can classify ACF values as “negligible.” To resolve this problem we can use Bartlett’s formula (Section 2.4), which implies that for a large sample of size $n$ from an $\mathrm { M A } ( q )$ process, the sample ACF values at lags $h$ greater than $q$ are approximately normally distributed with means 0 and variances $w _ { h h } / n ~ = ~ \bigl ( 1 + 2 \rho ^ { 2 } ( 1 ) + \cdots + 2 \rho ^ { 2 } ( q ) \bigr ) / n$ . This means that if the sample is from an $\mathrm { M A } ( q )$ process and if $h \ > \ q$ , then ${ \hat { \rho } } ( h )$ should fall between the bounds $\pm 1 . 9 6 \sqrt { w _ { h h } / n }$ with probability approximately 0.95. In practice we frequently use the more stringent values $\pm 1 . 9 6 / \sqrt { n }$ as the bounds between which sample autocovariances are considered “negligible.” A more effective and systematic approach to the problem of model selection, which also applies to ARMA $( p , q )$ models with $p > 0$ and $q > 0$ , will be discussed in Section 5.5.

# 3.2.3 The Partial Autocorrelation Function

The partial autocorrelation function (PACF) of an ARMA process $\{ X _ { t } \}$ is the function $\alpha ( \cdot )$ defined by the equations

$$
\alpha (0) = 1
$$

and

$$
\alpha (h) = \phi_ {h h}, \quad h \geq 1,
$$

where $\phi _ { h h }$ is the last component of

$$
\boldsymbol {\phi} _ {h} = \Gamma_ {h} ^ {- 1} \boldsymbol {\gamma} _ {h}, \tag {3.2.14}
$$

$\Gamma _ { h } = \left[ \gamma ( i - j ) \right] _ { i , j = 1 } ^ { h }$ , and $\pmb { \gamma } _ { h } = \left[ \gamma ( 1 ) , \gamma ( 2 ) , \dots , \gamma ( h ) \right] ^ { \prime }$

For any set of observations $\{ x _ { 1 } , \ldots , x _ { n } \}$ with $x _ { i } \neq x _ { j }$ for some $i$ and $j$ , the sample PACF ${ \hat { \alpha } } ( h )$ is given by

$$
\hat {\alpha} (0) = 1
$$

and

$$
\hat {\alpha} (h) = \hat {\phi} _ {h h}, \quad h \geq 1,
$$

where $\hat { \phi } _ { h h }$ is the last component of

$$
\hat {\boldsymbol {\phi}} _ {h} = \hat {\Gamma} _ {h} ^ {- 1} \hat {\boldsymbol {\gamma}} _ {h}. \tag {3.2.15}
$$

We show in the next example that the PACF of a causal $\operatorname { A R } ( p )$ process is zero for lags greater than $p$ . Both sample and model partial autocorrelation functions can be computed numerically using the program ITSM. Algebraic calculation of the PACF is quite complicated except when $q$ is zero or $p$ and $q$ are both small.

It can be shown (Brockwell and Davis (1991), p. 171) that $\phi _ { h h }$ is the correlation between the prediction errors $X _ { h } - P ( X _ { h } | X _ { 1 } , \dots , X _ { h - 1 } )$ and $X _ { 0 } - P ( X _ { 0 } | X _ { 1 } , \dots , X _ { h - 1 } )$ .

# Example 3.2.6 The PACF of an AR(p) Process

For the causal $\operatorname { A R } ( p )$ process defined by

$$
X _ {t} - \phi_ {1} X _ {t - 1} - \dots - \phi_ {p} X _ {t - p} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, \sigma^ {2}),
$$

we know (Problem 2.15) that for $h \geq p$ the best linear predictor of $X _ { h + 1 }$ in terms of $1 , X _ { 1 }$ , . . . , $X _ { h }$ is

$$
\hat {X} _ {h + 1} = \phi_ {1} X _ {h} + \phi_ {2} X _ {h - 1} + \dots + \phi_ {p} X _ {h + 1 - p}.
$$

Since the coefficient $\phi _ { h h }$ of $X _ { 1 }$ is $\phi _ { p }$ if $h = p$ and 0 if $h > p$ , we conclude that the PACF $\alpha ( \cdot )$ of the process $\{ X _ { t } \}$ has the properties

$$
\alpha (p) = \phi_ {p}
$$

and

$$
\alpha (h) = 0 \text {f o r} h > p.
$$

For $\textit { h } < \textit { p }$ the values of $\alpha ( h )$ can easily be computed from (3.2.14). For any specified ARMA model the PACF can be evaluated numerically using the option Model>ACF/PACF>Model of the program ITSM.

-

# Example 3.2.7 The PACF of an MA(1) Process

For the MA(1) process, it can be shown from (3.2.14) (see Problem 3.12) that the PACF at lag $h$ is

$$
\alpha (h) = \phi_ {h h} = - (- \theta) ^ {h} / \big (1 + \theta^ {2} + \dots + \theta^ {2 h} \big).
$$

The Sample PACF of an $A R ( p )$ Series. If $\{ X _ { t } \}$ is an $\operatorname { A R } ( p )$ series, then the sample PACF based on observations $\{ x _ { 1 } , \ldots , x _ { n } \}$ should reflect (with sampling variation) the properties of the PACF itself. In particular, if the sample PACF ${ \hat { \alpha } } ( h )$ is significantly different from zero for $0 \leq h \leq p$ and negligible for $h > p$ , Example 3.2.6 suggests that an $\operatorname { A R } ( p )$ model might provide a good representation of the data. To decide what is meant by “negligible” we can use the result that for an $\operatorname { A R } ( p )$ process the sample PACF values at lags greater than $p$ are approximately independent $N ( 0 , 1 / n )$ random variables. This means that roughly $95 \%$ of the sample PACF values beyond lag $p$ should fall within the bounds $\pm 1 . 9 6 / \sqrt { n }$ . If we observe a sample PACF satisfying $| \hat { \alpha } ( h ) | > 1 . 9 6 / \sqrt { n }$ for $0 \leq h \leq p$ and $| \hat { \alpha } ( h ) | \ < \ 1 . 9 6 / \sqrt { n }$ for $h > p$ , this suggests an $\operatorname { A R } ( p )$ model for the data. For a more systematic approach to model selection, see Section 5.5.

# 3.2.4 Examples

Example 3.2.8 The time series plotted in Figure 3-5 consists of 57 consecutive daily overshorts from an underground gasoline tank at a filling station in Colorado. If $y _ { t }$ is the measured amount of fuel in the tank at the end of the tth day and $a _ { t }$ is the measured amount sold minus the amount delivered during the course of the tth day, then the overshort at the end of day $t$ is defined as $x _ { t } = y _ { t } - y _ { t - 1 } + a _ { t }$ . Due to the error in measuring the current amount of fuel in the tank, the amount sold, and the amount delivered to the station, we view yt, at, and $x _ { t }$ as observed values from some set of random variables $Y _ { t } , A _ { t }$ , and $X _ { t }$ for $t = 1 , \ldots , 5 7$ . (In the absence of any measurement error and any leak in the tank, each $x _ { t }$ would be zero.) The data and their ACF are plotted in Figures 3-5 and 3-6. To check the plausibility of an MA(1) model, the bounds $\pm 1 . 9 6 \left( 1 + 2 \hat { \rho } ^ { 2 } ( 1 ) \right) ^ { 1 / 2 } / n ^ { 1 / 2 }$ are also plotted in Figure 3-6. Since ${ \hat { \rho } } ( h )$ is well within these bounds for $h > 1$ , the data

Figure 3-5   
![](images/6eee09e38561db0fa6b09334b8793a4e900fd5c1cea4e3b7b574292a248710e3.jpg)  
Time series of the overshorts   
in Example 3.2.8

![](images/c9d0bca114bbba4dccb2210ecdbeaffc077e2e41346547aac3f5d4dfda86f18a.jpg)  
Figure 3-6 The sample ACF of the data in Figure 3-5 showing the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 } \big ( 1 +$ $2 \hat { \rho } ^ { 2 } ( 1 ) \big ) ^ { 1 / 2 }$ assuming an MA(1) model for the data

appear to be compatible with the model

$$
X _ {t} = \mu + Z _ {t} + \theta Z _ {t - 1}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right). \tag {3.2.16}
$$

The mean $\mu$ may be estimated by the sample mean $\bar { x } _ { 5 7 } = - 4 . 0 3 5$ , and the parameters $\theta , \sigma ^ { 2 }$ may be estimated by equating the sample ACVF with the model ACVF at lags 0 and 1, and solving the resulting equations for $\theta$ and $\sigma ^ { 2 }$ . This estimation procedure is known as the method of moments, and in this case gives the equations

$$
\begin{array}{l} (1 + \theta^ {2}) \sigma^ {2} = \hat {\gamma} (0) = 3 4 1 5. 7 2, \\ \theta \sigma^ {2} = \hat {\gamma} (1) = - 1 7 1 9. 9 5. \\ \end{array}
$$

Using the approximate solution $\theta = - 1$ and $\sigma ^ { 2 } = 1 7 0 8$ , we obtain the noninvertible MA(1) model

$$
X _ {t} = - 4. 0 3 5 + Z _ {t} - Z _ {t - 1}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 1 7 0 8).
$$

Typically, in time series modeling we have little or no knowledge of the underlying physical mechanism generating the data, and the choice of a suitable class of models is entirely data driven. For the time series of overshorts, the data, through the graph of the ACF, lead us to the MA(1) model. Alternatively, we can attempt to model the mechanism generating the time series of overshorts using a structural model. As we will see, the structural model formulation leads us again to the MA(1) model. In the structural model setup, write $Y _ { t }$ , the observed amount of fuel in the tank at time t, as

$$
Y _ {t} = y _ {t} ^ {*} + U _ {t}, \tag {3.2.17}
$$

where $y _ { t } ^ { * }$ is the true (or actual) amount of fuel in the tank at time $t$ (not to be confused with $y _ { t }$ above) and $U _ { t }$ is the resulting measurement error. The variable $y _ { t } ^ { * }$ is an idealized quantity that in principle cannot be observed even with the most sophisticated measurement devices. Similarly, we assume that

$$
A _ {t} = a _ {t} ^ {*} + V _ {t}, \tag {3.2.18}
$$

where $a _ { t } ^ { * }$ is the actual amount of fuel sold minus the actual amount delivered during day $t$ , and $V _ { t }$ is the associated measurement error. We further assume that $\{ U _ { t } \} \sim$

WN $ \lceil \left( 0 , \sigma _ { U } ^ { 2 } \right) $ , $\{ V _ { t } \} \sim \mathrm { W N } \left( 0 , \sigma _ { V } ^ { 2 } \right)$ , and that the two sequences $\{ U _ { t } \}$ and $\{ V _ { t } \}$ are uncorrelated with one another $( E ( U _ { t } V _ { s } ) = 0$ for all $s$ and $t$ ). If the change of level per day due to leakage is $\mu$ gallons $( \mu < 0$ indicates leakage), then

$$
y _ {t} ^ {*} = \mu + y _ {t - 1} ^ {*} - a _ {t} ^ {*}. \tag {3.2.19}
$$

This equation relates the actual amounts of fuel in the tank at the end of days $t$ and $t - 1$ , adjusted for the actual amounts that have been sold and delivered during the day. Using (3.2.17)–(3.2.19), the model for the time series of overshorts is given by

$$
X _ {t} = Y _ {t} - Y _ {t - 1} + A _ {t} = \mu + U _ {t} - U _ {t - 1} + V _ {t}.
$$

This model is stationary and 1-correlated, since

$$
E X _ {t} = E \left(\mu + U _ {t} - U _ {t - 1} + V _ {t}\right) = \mu
$$

and

$$
\begin{array}{l} \gamma (h) = E [ (X _ {t + h} - \mu) (X _ {t} - \mu) ] \\ = E [ (U _ {t + h} - U _ {t + h - 1} + V _ {t + h}) (U _ {t} - U _ {t - 1} + V _ {t}) ] \\ = \left\{ \begin{array}{l l} 2 \sigma_ {U} ^ {2} + \sigma_ {V} ^ {2}, & \text {i f} h = 0, \\ - \sigma_ {U} ^ {2}, & \text {i f} | h | = 1, \\ 0, & \text {o t h e r w i s e}. \end{array} \right. \\ \end{array}
$$

It follows from Proposition 2.1.1 that $\{ X _ { t } \}$ is the MA(1) model (3.2.16) with

$$
\rho (1) = \frac {\theta_ {1}}{1 + \theta_ {1} ^ {2}} = \frac {- \sigma_ {U} ^ {2}}{2 \sigma_ {U} ^ {2} + \sigma_ {V} ^ {2}}.
$$

From this equation we see that the measurement error associated with the adjustment $\left\{ A _ { t } \right\}$ is zero (i.e., $\sigma _ { V } ^ { 2 } = 0 \mathrm { \bar { . } }$ ) if and only if $\rho ( 1 ) = - 0 . 5$ or, equivalently, if and only if $\theta _ { 1 } = - 1$ . From the analysis above, the moment estimator of $\theta _ { 1 }$ for the overshort data is in fact $^ { - 1 }$ , so that we conclude that there is relatively little measurement error associated with the amount of fuel sold and delivered.

We shall return to a more general discussion of structural models in Chapter 8.

# Example 3.2.9 The Sunspot Numbers

Figure 3-7 shows the sample PACF of the sunspot numbers $S _ { 1 } , \ldots , S _ { 1 0 0 }$ (for the years 1770–1869) as obtained from ITSM by opening the project SUNSPOTS.TSM and clicking on the second yellow button at the top of the screen. The graph also shows the bounds $\pm 1 . 9 6 / \sqrt { 1 0 0 }$ . The fact that all of the PACF values beyond lag 2 fall within the bounds suggests the possible suitability of an AR(2) model for the mean-corrected data set $X _ { t } = S _ { t } - 4 6 . 9 3$ . One simple way to estimate the parameters $\phi _ { 1 } , \phi _ { 2 }$ , and $\sigma ^ { 2 }$ of such a model is to require that the ACVF of the model at lags 0, 1, and 2 should match the sample ACVF at those lags. Substituting the sample ACVF values

$$
\hat {\gamma} (0) = 1 3 8 2. 2, \quad \hat {\gamma} (1) = 1 1 1 4. 4, \quad \hat {\gamma} (2) = 5 9 1. 7 3,
$$

for $\gamma ( 0 )$ , $\gamma ( 1 )$ , and $\gamma ( 2 )$ in the first three equations of (3.2.5) and (3.2.6) and solving for $\phi _ { 1 } , \phi _ { 2 }$ , and $\sigma ^ { 2 }$ gives the fitted model

$$
X _ {t} - 1. 3 1 8 X _ {t - 1} + 0. 6 3 4 X _ {t - 2} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 2 8 9. 2). \tag {3.2.20}
$$

(This method of model fitting is called Yule–Walker estimation and will be discussed more fully in Section 5.1.1.)

![](images/26e9befecf90624d540d9556440e8fc02cce6433522b81ace6ac2595f4e067eb.jpg)

The sample PACF of the sunspot numbers with the bounds $\pm 1 . 9 6 / \sqrt { 1 0 0 }$

![](images/58495eb13f5f958510471f283666b725c3c93ebfaef05fca754bcda2baacef7d.jpg)  
Figure 3-7

# 3.3 Forecasting ARMA Processes

The innovations algorithm (see Section 2.5.4) provided us with a recursive method for forecasting second-order zero-mean processes that are not necessarily stationary. For the causal ARMA process

$$
\phi (B) X _ {t} = \theta (B) Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right),
$$

it is possible to simplify the application of the algorithm drastically. The idea is to apply it not to the process $\{ X _ { t } \}$ itself, but to the transformed process [cf. Ansley (1979)]

$$
\left\{ \begin{array}{l l} W _ {t} = \sigma^ {- 1} X _ {t}, & t = 1, \dots , m, \\ W _ {t} = \sigma^ {- 1} \phi (B) X _ {t}, & t > m, \end{array} \right. \tag {3.3.1}
$$

where

$$
m = \max  (p, q). \tag {3.3.2}
$$

For notational convenience we define $\theta _ { 0 } : = 1$ and $\theta _ { j } : = 0$ for $j > q$ . We shall also assume that $p \geq 1$ and $q \geq 1$ . (There is no loss of generality in these assumptions, since in the analysis that follows we may take any of the coefficients $\phi _ { i }$ and $\theta _ { i }$ to be zero.)

The autocovariance function $\gamma _ { X } ( \cdot )$ of $\{ X _ { t } \}$ can easily be computed using any of the methods described in Section 3.2.1. The autocovariances $\kappa ( i , j ) = E ( W _ { i } W _ { j } ) ,$ i, j 1, are then found from

$$
\kappa (i, j) = \left\{ \begin{array}{l l} \sigma^ {- 2} \gamma_ {X} (i - j), & 1 \leq i, j \leq m \\ \sigma^ {- 2} \left[ \gamma_ {X} (i - j) - \sum_ {r = 1} ^ {p} \phi_ {r} \gamma_ {X} (r - | i - j |) \right], & \min  (i, j) \leq m <   \max  (i, j) \leq 2 m, \\ \sum_ {r = 0} ^ {q} \theta_ {r} \theta_ {r + | i - j |}, & \min  (i, j) > m, \\ 0, & \text {o t h e r w i s e .} \end{array} \right. \tag {3.3.3}
$$

Applying the innovations algorithm to the process $\{ W _ { t } \}$ we obtain

$$
\left\{ \begin{array}{l} \hat {W} _ {n + 1} = \sum_ {j = 1} ^ {n} \theta_ {n j} \left(W _ {n + 1 - j} - \hat {W} _ {n + 1 - j}\right), \quad 1 \leq n <   m, \\ \hat {W} _ {n + 1} = \sum_ {j = 1} ^ {q} \theta_ {n j} \left(W _ {n + 1 - j} - \hat {W} _ {n + 1 - j}\right), \quad n \geq m, \end{array} \right. \tag {3.3.4}
$$

where the coefficients $\theta _ { n j }$ and the mean squared errors $r _ { n } = E \left( W _ { n + 1 } - \hat { W } _ { n + 1 } \right) ^ { 2 }$ are found recursively from the innovations algorithm with $\kappa$ defined as in (3.3.3). The notable feature of the predictors (3.3.4) is the vanishing of $\theta _ { n j }$ when both $n \geq m$ and $j > q$ . This is a consequence of the innovations algorithm and the fact that $\kappa ( r , s ) = 0$ if $r > m$ and $| r - s | > q$ .

Observe now that the equations (3.3.1) allow each $X _ { n }$ , $n \geq 1$ , to be written as a linear combination of $W _ { j }$ , $1 \leq j \leq n$ , and, conversely, each $W _ { n }$ $W _ { n } , n \geq 1$ , to be written as a linear combination of $X _ { j }$ , $1 \leq j \leq n$ . This means that the best linear predictor of any random variable $Y$ in terms of $\{ 1 , X _ { 1 } , \ldots , X _ { n } \}$ is the same as the best linear predictor of $Y$ in terms of $\{ 1 , W _ { 1 } , \ldots , W _ { n } \}$ . We shall denote this predictor by $P _ { n } Y$ . In particular, the one-step predictors of $W _ { n + 1 }$ and $X _ { n + 1 }$ are given by

$$
\hat {W} _ {n + 1} = P _ {n} W _ {n + 1}
$$

and

$$
\hat {X} _ {n + 1} = P _ {n} X _ {n + 1}.
$$

Using the linearity of $P _ { n }$ and the equations (3.3.1) we see that

$$
\left\{ \begin{array}{l l} \hat {W} _ {t} = \sigma^ {- 1} \hat {X} _ {t}, & t = 1, \dots , m, \\ \hat {W} _ {t} = \sigma^ {- 1} \left[ \hat {X} _ {t} - \phi_ {1} X _ {t - 1} - \dots - \phi_ {p} X _ {t - p} \right], & t > m, \end{array} \right. \tag {3.3.5}
$$

which, together with (3.3.1), shows that

$$
X _ {t} - \hat {X} _ {t} = \sigma \left[ W _ {t} - \hat {W} _ {t} \right] \quad \text {f o r a l l} t \geq 1. \tag {3.3.6}
$$

Replacing $\left( W _ { j } - \hat { W } _ { j } \right)$ by $\sigma ^ { - 1 } \left( X _ { j } - \hat { X } _ { j } \right)$ in (3.3.3) and then substituting into (3.3.4), we finally obtain

$$
\hat {X} _ {n + 1} = \left\{ \begin{array}{l l} \sum_ {j = 1} ^ {n} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right), & 1 \leq n <   m, \\ \phi_ {1} X _ {n} + \dots + \phi_ {p} X _ {n + 1 - p} + \sum_ {j = 1} ^ {q} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right), & n \geq m, \end{array} \right. \tag {3.3.7}
$$

and

$$
E \left(X _ {n + 1} - \hat {X} _ {n + 1}\right) ^ {2} = \sigma^ {2} E \left(W _ {n + 1} - \hat {W} _ {n + 1}\right) ^ {2} = \sigma^ {2} r _ {n}, \tag {3.3.8}
$$

where $\theta _ { n j }$ and $r _ { n }$ are found from the innovations algorithm with $\kappa$ as in (3.3.3). Equations (3.3.7) determine the one-step predictors $\hat { X } _ { 2 } , \hat { X } _ { 3 }$ , . . . recursively.

Remark 1. It can be shown (see Brockwell and Davis (1991), Problem 5.6) that if $\{ X _ { t } \}$ is invertible, then as $n \to \infty$ ,

$$
E \left(X _ {n} - \hat {X} _ {n} - Z _ {n}\right) ^ {2} \rightarrow 0,
$$

$$
\theta_ {n j} \rightarrow \theta_ {j}, j = 1, \dots , q,
$$

and

$$
r _ {n} \rightarrow 1.
$$

Algebraic calculation of the coefficients $\theta _ { n j }$ and $r _ { n }$ is not feasible except for very simple models, such as those considered in the following examples. However, numerical implementation of the recursions is quite straightforward and is used to compute predictors in the program ITSM. -

Example 3.3.1 Prediction of an $\operatorname { A R } ( p )$ Process

Applying (3.3.7) to the ARMA $( p , 0 )$ process, we see at once that

$$
\hat {X} _ {n + 1} = \phi_ {1} X _ {n} + \dots + \phi_ {p} X _ {n + 1 - p}, \quad n \geq p.
$$

![](images/c0181718754ef993f3627961251f6519ed8f4943d9bdc0a38deb30f0873da127.jpg)

Example 3.3.2 Prediction of an $\mathrm { M A } ( q )$ Process

Applying (3.3.7) to the ARMA $( 1 , q )$ process with $\phi _ { 1 } = 0$ gives

$$
\hat {X} _ {n + 1} = \sum_ {j = 1} ^ {\min (n, q)} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right), \quad n \geq 1,
$$

where the coefficients $\theta _ { n j }$ are found by applying the innovations algorithm to the covariances $\kappa ( i , j )$ defined in (3.3.3). Since in this case the processes $\{ X _ { t } \}$ and $\{ \sigma ^ { - 1 } W _ { t } \}$ are identical, these covariances are simply

$$
\kappa (i, j) = \sigma^ {- 2} \gamma_ {X} (i - j) = \sum_ {r = 0} ^ {q - | i - j |} \theta_ {r} \theta_ {r + | i - j |}.
$$

![](images/03616ff52cc2df11191c4ea92abef147d123a5b8d049bcf2cd958fc9d2e2ad2a.jpg)

Example 3.3.3 Prediction of an ARMA(1,1) Process

If

$$
X _ {t} - \phi X _ {t - 1} = Z _ {t} + \theta Z _ {t - 1}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, \sigma^ {2}),
$$

and $| \phi | < 1$ , then equations (3.3.7) reduce to the single equation

$$
\hat {X} _ {n + 1} = \phi X _ {n} + \theta_ {n 1} (X _ {n} - \hat {X} _ {n}), \quad n \geq 1.
$$

To compute $\theta _ { n 1 }$ we use Example 3.2.1 to obtain $\gamma _ { X } ( 0 ) = \sigma ^ { 2 } ( 1 + 2 \theta \phi + \theta ^ { 2 } ) / \big ( 1 - \phi ^ { 2 } \big )$ . Substituting in (3.3.3) then gives, for $i , j \geq 1$ ,

$$
\kappa (i, j) = \left\{ \begin{array}{l l} \left(1 + 2 \theta \phi + \theta^ {2}\right) / \left(1 - \phi^ {2}\right), & i = j = 1, \\ 1 + \theta^ {2}, & i = j \geq 2, \\ \theta , & | i - j | = 1, i \geq 1, \\ 0, & \text {o t h e r w i s e}. \end{array} \right.
$$

With these values of $\kappa ( i , j )$ , the recursions of the innovations algorithm reduce to

$$
r _ {0} = \left(1 + 2 \theta \phi + \theta^ {2}\right) / \left(1 - \phi^ {2}\right),
$$

$$
\theta_ {n 1} = \theta / r _ {n - 1}, \tag {3.3.9}
$$

$$
r _ {n} = 1 + \theta^ {2} - \theta^ {2} / r _ {n - 1},
$$

which can be solved quite explicitly (see Problem 3.13).

![](images/3c365b042a7563b9b3b7b53a1c6a22ba974baaafdedb1d07172c9003262fac49.jpg)

# Example 3.3.4 Numerical Prediction of an ARMA(2,3) Process

In this example we illustrate the steps involved in numerical prediction of an ARMA(2,3) process. Of course, these steps are shown for illustration only. The calculations are all carried out automatically by ITSM in the course of computing predictors for any specified data set and ARMA model. The process we shall consider is the ARMA process defined by the equations

$$
X _ {t} - X _ {t - 1} + 0. 2 4 X _ {t - 2} = Z _ {t} + 0. 4 Z _ {t - 1} + 0. 2 Z _ {t - 2} + 0. 1 Z _ {t - 3}, \tag {3.3.10}
$$

where $\{ Z _ { t } \} \sim  { \mathrm { W N } } ( 0 , 1 )$ . Ten values of $X _ { 1 } , \ldots , X _ { 1 0 }$ simulated by the program ITSM are shown in Table 3.1. (These were produced using the option Model>Specify to specify the order and parameters of the model and then Model>Simulate to generate the series from the specified model.)

The first step is to compute the covariances $\gamma _ { X } ( h ) , h = 0 , 1 , 2$ , which are easily found from equations (3.2.5) with $k = 0 , 1 , 2$ to be

$$
\gamma_ {X} (0) = 7. 1 7 1 3 3, \quad \gamma_ {X} (1) = 6. 4 4 1 3 9, \quad \text {a n d} \quad \gamma_ {X} (2) = 5. 0 6 0 3.
$$

From (3.3.3) we find that the symmetric matrix $K = [ \boldsymbol { \kappa } ( i , j ) ] _ { i , j = 1 , 2 , \ldots }$ is given by

$$
K = \left[ \begin{array}{c c c c c c c c} 7. 1 7 1 3 & & & & & & \\ 6. 4 4 1 4 & 7. 1 7 1 3 & & & & & \\ 5. 0 6 0 3 & 6. 4 4 1 4 & 7. 1 7 1 3 & & & & \\ 0. 1 0 & 0. 3 4 & 0. 8 1 6 & 1. 2 1 & & & \\ 0 & 0. 1 0 & 0. 3 4 & 0. 5 0 & 1. 2 1 & & \\ 0 & 0 & 0. 1 0 & 0. 2 4 & 0. 5 0 & 1. 2 1 & \\ \cdot & 0 & 0 & 0. 1 0 & 0. 2 4 & 0. 5 0 & 1. 2 1 \\ \cdot & \cdot & 0 & 0 & 0. 1 0 & 0. 2 4 & 0. 5 0 & 1. 2 1 \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \end{array} \right].
$$

The next step is to solve the recursions of the innovations algorithm for $\theta _ { n j }$ and $r _ { n }$ using these values for $\kappa ( i , j )$ . Then

Table 3.1 $\hat { X } _ { n + 1 }$ for the ARMA(2,3) Process of Example 3.3.4   

<table><tr><td>n</td><td>Xn+1</td><td>rn</td><td>θn1</td><td>θn2</td><td>θn3</td><td>X̂n+1</td></tr><tr><td>0</td><td>1.704</td><td>7.1713</td><td>0</td><td></td><td></td><td></td></tr><tr><td>1</td><td>0.527</td><td>1.3856</td><td>0.8982</td><td>1.5305</td><td></td><td></td></tr><tr><td>2</td><td>1.041</td><td>1.0057</td><td>1.3685</td><td>0.7056</td><td>-0.1710</td><td></td></tr><tr><td>3</td><td>0.942</td><td>1.0019</td><td>0.4008</td><td>0.1806</td><td>0.0139</td><td>1.2428</td></tr><tr><td>4</td><td>0.555</td><td>1.0019</td><td>0.3998</td><td>0.2020</td><td>0.0732</td><td>0.7443</td></tr><tr><td>5</td><td>-1.002</td><td>1.0005</td><td>0.3992</td><td>0.1995</td><td>0.0994</td><td>0.3138</td></tr><tr><td>6</td><td>-0.585</td><td>1.0000</td><td>0.4000</td><td>0.1997</td><td>0.0998</td><td>-1.7293</td></tr><tr><td>7</td><td>0.010</td><td>1.0000</td><td>0.4000</td><td>0.2000</td><td>0.0998</td><td>-0.1688</td></tr><tr><td>8</td><td>-0.638</td><td>1.0000</td><td>0.4000</td><td>0.2000</td><td>0.0999</td><td>0.3193</td></tr><tr><td>9</td><td>0.525</td><td>1.0000</td><td>0.4000</td><td>0.2000</td><td>0.1000</td><td>-0.8731</td></tr><tr><td>10</td><td></td><td>1.0000</td><td>0.4000</td><td>0.2000</td><td>0.1000</td><td>1.0638</td></tr><tr><td>11</td><td></td><td>1.0000</td><td>0.4000</td><td>0.2000</td><td>0.1000</td><td></td></tr><tr><td>12</td><td></td><td>1.0000</td><td>0.4000</td><td>0.2000</td><td>0.1000</td><td></td></tr></table>

$$
\hat {X} _ {n + 1} = \left\{ \begin{array}{l l} \sum_ {j = 1} ^ {n} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right), & n = 1, 2, \\ X _ {n} - 0. 2 4 X _ {n - 1} + \sum_ {j = 1} ^ {3} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right), & n = 3, 4, \ldots , \end{array} \right.
$$

and

$$
E \left(X _ {n + 1} - \hat {X} _ {n + 1}\right) ^ {2} = \sigma^ {2} r _ {n} = r _ {n}.
$$

The results are shown in Table 3.1.

![](images/cc8234470d74ab819a618af782b2ed80a250e44d3b5998432fa295e3daa7eb6b.jpg)

# 3.3.1 h-Step Prediction of an ARMA $( p , q )$ Process

As in Section 2.5, we use $P _ { n } Y$ to denote the best linear predictor of $Y$ in terms of $X _ { 1 } , \ldots , X _ { n }$ (which, as pointed out after (3.3.4), is the same as the best linear predictor of $Y$ in terms of $W _ { 1 } , \ldots , W _ { n } )$ ). Then from (2.5.30) we have

$$
P _ {n} W _ {n + h} = \sum_ {j = h} ^ {n + h - 1} \theta_ {n + h - 1, j} \Big (W _ {n + h - j} - \hat {W} _ {n + h - j} \Big) = \sigma^ {2} \sum_ {j = h} ^ {n + h - 1} \theta_ {n + h - 1, j} \Big (X _ {n + h - j} - \hat {X} _ {n + h - j} \Big).
$$

Using this result and applying the operator $P _ { n }$ to each side of equation (3.3.1), we conclude that the $h$ -step predictors $P _ { n } X _ { n + h }$ satisfy

$$
P _ {n} X _ {n + h} = \left\{ \begin{array}{l l} \sum_ {j = h} ^ {n + h - 1} \theta_ {n + h - 1, j} \left(X _ {n + h - j} - \hat {X} _ {n + h - j}\right), & 1 \leq h \leq m - n, \\ \sum_ {i = 1} ^ {p} \phi_ {i} P _ {n} X _ {n + h - i} + \sum_ {j = h} ^ {n + h - 1} \theta_ {n + h - 1, j} \left(X _ {n + h - j} - \hat {X} _ {n + h - j}\right), & h > m - n. \end{array} \right. \tag {3.3.11}
$$

If, as is almost always the case, $n > m = \operatorname* { m a x } ( p , q )$ , then for all $h \geq 1$ ,

$$
P _ {n} X _ {n + h} = \sum_ {i = 1} ^ {p} \phi_ {i} P _ {n} X _ {n + h - i} + \sum_ {j = h} ^ {q} \theta_ {n + h - 1, j} \left(X _ {n + h - j} - \hat {X} _ {n + h - j}\right). \tag {3.3.12}
$$

Once the predictors $\hat { X } _ { 1 } , \dots \hat { X } _ { n }$ have been computed from (3.3.7), it is a straightforward calculation, with $n$ fixed, to determine the predictors PnXn 1, $P _ { n } X _ { n + 2 }$ , PnXn 3, . . . recursively from (3.3.12) (or (3.3.11) if $n ~ \leq ~ m )$ . The calculations are performed automatically in the Forecasting>ARMA option of the program ITSM.

# Example 3.3.5 $h$ -Step Prediction of an ARMA(2,3) Process

To compute $h$ -step predictors, $h \ = \ 1 , \ldots , 1 0$ , for the data of Example 3.3.4 and the model (3.3.10), open the project E334.TSM in ITSM and enter the model using the option Model>Specify. Then select Forecasting>ARMA and specify 10 for the number of forecasts required. You will notice that the white noise variance is automatically set by ITSM to an estimate based on the sample. To retain the model value of 1, you must reset the white noise variance to this value. Then click OK and you will see a graph of the original series with the ten predicted values appended. If you right-click on the graph and select Info, you will see the numerical results shown in the following table as well as prediction bounds based on the assumption that the series is Gaussian. (Prediction bounds are discussed in the last paragraph of

this chapter.) The mean squared errors are calculated as described below. Notice how the predictors converge fairly rapidly to the mean of the process (i.e., zero) as the lead time $h$ increases. Correspondingly, the one-step mean squared error increases from the white noise variance (i.e., 1) at $h = 1$ to the variance of $X _ { t }$ (i.e., 7.1713), which is virtually reached at $h = 1 0$ .

![](images/7dcd927a97db7b5cfe211eb1c93cd46a5f6fe3fbecef087b160565621fca5068.jpg)

The Mean Squared Error of $P _ { n } X _ { n + h }$

The mean squared error of $P _ { n } X _ { n + h }$ is easily computed by ITSM from the formula

$$
\sigma_ {n} ^ {2} (h) := E \left(X _ {n + h} - P _ {n} X _ {n + h}\right) ^ {2} = \sum_ {j = 0} ^ {h - 1} \left(\sum_ {r = 0} ^ {j} \chi_ {r} \theta_ {n + h - r - 1, j - r}\right) ^ {2} v _ {n + h - j - 1}, \tag {3.3.13}
$$

where the coefficients $\chi _ { j }$ are computed recursively from the equations $\chi _ { 0 } = 1$ and

$$
\chi_ {j} = \sum_ {k = 1} ^ {\min  (p, j)} \phi_ {k} \chi_ {j - k}, \quad j = 1, 2, \dots . \tag {3.3.14}
$$

# Example 3.3.6 $h$ -Step Prediction of an ARMA(2,3) Process

We now illustrate the use of (3.3.12) and (3.3.13) for the $h$ -step predictors and their mean squared errors by manually reproducing the output of ITSM shown in Table 3.2. From (3.3.12) and Table 3.1 we obtain

$$
\begin{array}{l} P _ {1 0} X _ {1 2} = \sum_ {i = 1} ^ {2} \phi_ {i} P _ {1 0} X _ {1 2 - i} + \sum_ {j = 2} ^ {3} \theta_ {1 1, j} \left(X _ {1 2 - j} - \hat {X} _ {1 2 - j}\right) \\ = \phi_ {1} \hat {X} _ {1 1} + \phi_ {2} X _ {1 0} + 0. 2 \left(X _ {1 0} - \hat {X} _ {1 0}\right) + 0. 1 \left(X _ {9} - \hat {X} _ {9}\right) \\ = 1. 1 2 1 7 \\ \end{array}
$$

and

Table 3.2 $h$ -step predictors for the ARMA(2,3)   
Series of Example 3.3.4   

<table><tr><td>h</td><td>P10X10+h</td><td>√MSE</td></tr><tr><td>1</td><td>1.0638</td><td>1.0000</td></tr><tr><td>2</td><td>1.1217</td><td>1.7205</td></tr><tr><td>3</td><td>1.0062</td><td>2.1931</td></tr><tr><td>4</td><td>0.7370</td><td>2.4643</td></tr><tr><td>5</td><td>0.4955</td><td>2.5902</td></tr><tr><td>6</td><td>0.3186</td><td>2.6434</td></tr><tr><td>7</td><td>0.1997</td><td>2.6648</td></tr><tr><td>8</td><td>0.1232</td><td>2.6730</td></tr><tr><td>9</td><td>0.0753</td><td>2.6761</td></tr><tr><td>10</td><td>0.0457</td><td>2.6773</td></tr></table>

$$
\begin{array}{l} P _ {1 0} X _ {1 3} = \sum_ {i = 1} ^ {2} \phi_ {i} P _ {1 0} X _ {1 3 - i} + \sum_ {j = 3} ^ {3} \theta_ {1 2, j} \left(X _ {1 3 - j} - \hat {X} _ {1 3 - j}\right) \\ = \phi_ {1} P _ {1 0} X _ {1 2} + \phi_ {2} \hat {X} _ {1 1} + 0. 1 \left(X _ {1 0} - \hat {X} _ {1 0}\right) \\ = 1. 0 0 6 2. \\ \end{array}
$$

For $k > 1 3$ , $P _ { 1 0 } X _ { k }$ is easily found recursively from

$$
P _ {1 0} X _ {k} = \phi_ {1} P _ {1 0} X _ {k - 1} + \phi_ {2} P _ {1 0} X _ {k - 2}.
$$

To find the mean squared errors we use (3.3.13) with $\chi _ { 0 } = 1$ , $\chi _ { 1 } = \phi _ { 1 } = 1$ , and $\chi _ { 2 } = \phi _ { 1 } \chi _ { 1 } + \phi _ { 2 } = 0 . 7 6$ . Using the values of $\theta _ { n j }$ and $\nu _ { j } ( = r _ { j } )$ in Table 3.1, we obtain

$$
\sigma_ {1 0} ^ {2} (2) = E \left(X _ {1 2} - P _ {1 0} X _ {1 2}\right) ^ {2} = 2. 9 6 0
$$

and

$$
\sigma_ {1 0} ^ {2} (3) = E (X _ {1 3} - P _ {1 0} X _ {1 3}) ^ {2} = 4. 8 1 0,
$$

in accordance with the results shown in Table 3.2.

![](images/ac509f51dd46e76cd08111e8606bf27fa41a55194b449fd289247f77a15f432b.jpg)

# Large-Sample Approximations

Assuming as usual that the $\mathbf { A R M A } ( p , q )$ process defined by $\phi ( B ) X _ { t } = \theta ( B ) Z _ { t }$ , $\{ Z _ { t } \} \sim$ $\operatorname { W N } \left( 0 , \sigma ^ { 2 } \right)$ , is causal and invertible, we have the representations

$$
X _ {n + h} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {n + h - j} \tag {3.3.15}
$$

and

$$
Z _ {n + h} = X _ {n + h} + \sum_ {j = 1} ^ {\infty} \pi_ {j} X _ {n + h - j}, \tag {3.3.16}
$$

where $\{ \psi _ { j } \}$ and $\{ \pi _ { j } \}$ are uniquely determined by equations (3.1.7) and (3.1.8), respectively. Let $\tilde { P } _ { n } Y$ denote the best (i.e., minimum mean squared error) approximation to Y that is a linear combination or limit of linear combinations of $X _ { t }$ , $- \infty < t \leq n$ , or equivalently [by (3.3.15) and (3.3.16)] of $Z _ { t }$ , $- \infty < t \leq n$ . The properties of the operator ${ \tilde { P } } _ { n }$ were discussed in Section 2.5.6. Applying $\tilde { P } _ { n }$ to each side of equations (3.3.15) and (3.3.16) gives

$$
\tilde {P} _ {n} X _ {n + h} = \sum_ {j = h} ^ {\infty} \psi_ {j} Z _ {n + h - j} \tag {3.3.17}
$$

and

$$
\tilde {P} _ {n} X _ {n + h} = - \sum_ {j = 1} ^ {\infty} \pi_ {j} \tilde {P} _ {n} X _ {n + h - j}. \tag {3.3.18}
$$

For $h \ = \ 1$ the jth term on the right of (3.3.18) is just $X _ { n + 1 - j }$ . Once $\tilde { P } _ { n } X _ { n + 1 }$ has been evaluated, $\tilde { P } _ { n } X _ { n + 2 }$ can then be computed from (3.3.18). The predictors $\tilde { P } _ { n } X _ { n + 3 }$ , $\tilde { P } _ { n } X _ { n + 4 }$ , . . . can then be computed successively in the same way. Subtracting (3.3.17) from (3.3.15) gives the $h$ -step prediction error as

$$
X _ {n + h} - \tilde {P} _ {n} X _ {n + h} = \sum_ {j = 0} ^ {h - 1} \psi_ {j} Z _ {n + h - j},
$$

from which we see that the mean squared error is

$$
\tilde {\sigma} ^ {2} (h) = \sigma^ {2} \sum_ {j = 0} ^ {h - 1} \psi_ {j} ^ {2}. \tag {3.3.19}
$$

The predictors obtained in this way have the form

$$
\tilde {P} _ {n} X _ {n + h} = \sum_ {j = 0} ^ {\infty} c _ {j} X _ {n - j}. \tag {3.3.20}
$$

In practice, of course, we have only observations $X _ { 1 } , \ldots , X _ { n }$ available, so we must truncate the series (3.3.20) after $n$ terms. The resulting predictor is a useful approximation to $P _ { n } X _ { n + h }$ if $n$ is large and the coefficients $c _ { j }$ converge to zero rapidly as $j$ increases. It can be shown that the mean squared error (3.3.19) of $\tilde { P } _ { n } X _ { n + h }$ can also be obtained by letting $n  \infty$ in the expression (3.3.13) for the mean squared error of $P _ { n } X _ { n + h }$ , so that $\tilde { \sigma } ^ { 2 } ( h )$ is an easily calculated approximation to $\sigma _ { n } ^ { 2 } ( h )$ for large $n$ .

# Prediction Bounds for Gaussian Processes

If the ARMA process $\{ X _ { t } \}$ is driven by Gaussian white noise (i.e., if $\begin{array} { r l } { \{ Z _ { t } \} } & { { } \sim } \end{array}$ II $) \mathrm { N } \left( 0 , \sigma ^ { 2 } \right) )$ , then for each $h \geq 1$ the prediction error $X _ { n + h } - P _ { n } X _ { n + h }$ is normally distributed with mean 0 and variance $\sigma _ { n } ^ { 2 } ( h )$ given by (3.3.19).

Consequently, if $\Phi _ { 1 - \alpha / 2 }$ denotes the $( 1 - \alpha / 2 )$ quantile of the standard normal distribution function, it follows that $X _ { n + h }$ lies between the bounds $P _ { n } X _ { n + h } \pm \Phi _ { 1 - \alpha / 2 } \sigma _ { n } ( h )$ with probability $( 1 - \alpha )$ . These bounds are therefore called $( 1 - \alpha )$ prediction bounds for $X _ { n + h }$ .

# Problems

3.1 Determine which of the following ARMA processes are causal and which of them are invertible. (In each case $\{ Z _ { t } \}$ denotes white noise.)

(a) $X _ { t } + 0 . 2 X _ { t - 1 } - 0 . 4 8 X _ { t - 2 } = Z _ { t }$   
(b) $X _ { t } + 1 . 9 X _ { t - 1 } + 0 . 8 8 X _ { t - 2 } = Z _ { t } + 0 . 2 Z _ { t - 1 } + 0 . 7 Z _ { t - 2 } .$   
(c) $X _ { t } + 0 . 6 X _ { t - 1 } = Z _ { t } + 1 . 2 Z _ { t - 1 }$   
(d) $X _ { t } + 1 . 8 X _ { t - 1 } + 0 . 8 1 X _ { t - 2 } = Z _ { t } .$   
(e) $X _ { t } + 1 . 6 X _ { t - 1 } = Z _ { t } - 0 . 4 Z _ { t - 1 } + 0 . 0 4 Z _ { t - 2 } .$

3.2 For those processes in Problem 3.1 that are causal, compute and graph their ACF and PACF using the program ITSM.   
3.3 For those processes in Problem 3.1 that are causal, compute the first six coefficients $\psi _ { 0 } , \psi _ { 1 } , \ldots , \psi _ { 5 }$ $\psi _ { 5 }$ in the causal representation $\begin{array} { r } { X _ { t } = \sum _ { j = 0 } ^ { \infty } \psi _ { j } Z _ { t - j } } \end{array}$ of $\{ X _ { t } \}$ .   
3.4 Compute the ACF and PACF of the AR(2) process

$$
X _ {t} = 0. 8 X _ {t - 2} + Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

3.5 Let $\{ Y _ { t } \}$ be the ARMA plus noise time series defined by

$$
Y _ {t} = X _ {t} + W _ {t},
$$

where $\{ W _ { t } \} \sim \mathrm { W N } \left( 0 , \sigma _ { w } ^ { 2 } \right)$ , $\{ X _ { t } \}$ is the ARMA $( p , q )$ process satisfying

$$
\phi (B) X _ {t} = \theta (B) Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma_ {z} ^ {2}\right),
$$

and $E ( W _ { s } Z _ { t } ) = 0$ for all $s$ and $t$ .

(a) Show that $\{ Y _ { t } \}$ is stationary and find its autocovariance function in terms of $\sigma _ { W } ^ { 2 }$ and the ACVF of $\{ X _ { t } \}$ .   
(b) Show that the process $U _ { t } : = \phi ( B ) Y _ { t }$ is $r$ -correlated, where $r = \operatorname* { m a x } ( p , q )$ and hence, by Proposition 2.1.1, is an $\mathbf { M A } ( r )$ process. Conclude that $\{ Y _ { t } \}$ is an $\mathbf { A R M A } ( p , r )$ process.

3.6 Show that the two MA(1) processes

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right)
$$

$$
Y _ {t} = \tilde {Z} _ {t} + \frac {1}{\theta} \tilde {Z} _ {t - 1}, \quad \{\tilde {Z} _ {t} \} \sim \mathrm {W N} (0, \sigma^ {2} \theta^ {2}),
$$

where $0 < | \theta | < 1$ , have the same autocovariance functions.

3.7 Suppose that $\{ X _ { t } \}$ is the noninvertible MA(1) process

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right),
$$

where $| \theta | > 1$ . Define a new process $\{ W _ { t } \}$ as

$$
W _ {t} = \sum_ {j = 0} ^ {\infty} (- \theta) ^ {- j} X _ {t - j}
$$

and show that $\{ W _ { t } \} \sim \mathrm { W N } \left( 0 , \sigma _ { W } ^ { 2 } \right)$ . Express $\sigma _ { W } ^ { 2 }$ in terms of $\theta$ and $\sigma ^ { 2 }$ and show that $\{ X _ { t } \}$ has the invertible representation (in terms of $\{ W _ { t } \} .$ )

$$
X _ {t} = W _ {t} + \frac {1}{\theta} W _ {t - 1}.
$$

3.8 Let $\{ X _ { t } \}$ denote the unique stationary solution of the autoregressive equations

$$
X _ {t} = \phi X _ {t - 1} + Z _ {t}, \quad t = 0, \pm 1, \dots ,
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , \sigma ^ { 2 } )$ and $| \phi | ~ > ~ 1$ . Then $X _ { t }$ is given by the expression (2.2.11). Define the new sequence

$$
W _ {t} = X _ {t} - \frac {1}{\phi} X _ {t - 1},
$$

show that $\{ W _ { t } \} \sim \mathrm { ~ W N } \left( 0 , \sigma _ { W } ^ { 2 } \right)$ , and express $\sigma _ { W } ^ { 2 }$ in terms of $\sigma ^ { 2 }$ and $\phi$ . These calculations show that $\{ X _ { t } \}$ is the (unique stationary) solution of the causal AR equations

$$
X _ {t} = \frac {1}{\phi} X _ {t - 1} + W _ {t}, \quad t = 0, \pm 1, \dots .
$$

3.9 (a) Calculate the autocovariance function $\gamma ( \cdot )$ of the stationary time series

$$
Y _ {t} = \mu + Z _ {t} + \theta_ {1} Z _ {t - 1} + \theta_ {1 2} Z _ {t - 1 2}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

(b) Use the program ITSM to compute the sample mean and sample autocovariances $\hat { \gamma } ( h ) , 0 \leq h \leq 2 0$ , of $\{ \nabla \nabla _ { 1 2 } X _ { t } \}$ , where $\{ X _ { t } , t = 1 , \ldots , 7 2 \}$ is the accidental deaths series DEATHS.TSM of Example 1.1.3.   
(c) By equating γ (ˆ 1), γ (ˆ 11), and $\hat { \gamma } ( 1 2 )$ from part (b) to $\gamma ( 1 )$ , $\gamma ( 1 1 )$ , and $\gamma ( 1 2 )$ , respectively, from part (a), find a model of the form defined in (a) to represent $\{ \nabla \nabla _ { 1 2 } X _ { t } \}$ .

3.10 By matching the autocovariances and sample autocovariances at lags 0 and 1, fit a model of the form

$$
X _ {t} - \mu = \phi (X _ {t - 1} - \mu) + Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} \big (0, \sigma^ {2} \big),
$$

to the data STRIKES.TSM of Example 1.1.6. Use the fitted model to compute the best predictor of the number of strikes in 1981. Estimate the mean squared error of your predictor and construct $9 5 \%$ prediction bounds for the number of strikes in 1981 assuming that $\{ Z _ { t } \} \sim$ iid $\mathrm { N } ( 0 , \sigma ^ { 2 } )$ .

3.11 Show that the value at lag 2 of the partial ACF of the MA(1) process

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1}, \quad t = 0, \pm 1, \dots ,
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \big ( 0 , \sigma ^ { 2 } \big )$ , is

$$
\alpha (2) = - \theta^ {2} / \left(1 + \theta^ {2} + \theta^ {4}\right).
$$

3.12 For the MA(1) process of Problem 3.11, the best linear predictor of $X _ { n + 1 }$ based on $X _ { 1 } , \ldots , X _ { n }$ is

$$
\hat {X} _ {n + 1} = \phi_ {n, 1} X _ {n} + \dots + \phi_ {n, n} X _ {1},
$$

where $\phi _ { n } = \left( \phi _ { n 1 } , \ldots , \phi _ { n n } \right) ^ { \prime }$ satisfies $R _ { n } \phi _ { n } \ : = \ : \rho _ { n }$ [equation (2.5.23)]. By substituting the appropriate correlations into $R _ { n }$ and $\rho _ { n }$ and solving the resulting equations (starting with the last and working up), show that for $1 \ \leq \ j \ < \ n$ , $\bar { \phi _ { n , n - j } } = ( - \theta ) ^ { - j } \bigl ( \bar { 1 } + \theta ^ { 2 } + \cdot \cdot \cdot + \theta ^ { 2 j } \bigr ) \phi _ { n n }$ and hence that the PACF $\alpha ( n ) : = \phi _ { n n } =$ $- ( - \theta ) ^ { n } / \big ( 1 + \theta ^ { 2 } + \cdot \cdot \cdot + \theta ^ { 2 n } \big )$ .

3.13 The coefficients $\theta _ { n j }$ and one-step mean squared errors $\nu _ { n } = r _ { n } \sigma ^ { 2 }$ for the general causal ARMA(1,1) process in Example 3.3.3 can be found as follows:

(a) Show that if $y _ { n } : = r _ { n } / ( r _ { n } - 1 )$ , then the last of equation (3.3.9) can be rewritten in the form

$$
y _ {n} = \theta^ {- 2} y _ {n - 1} + 1, \quad n \geq 1.
$$

(b) Deduce that $y _ { n } { = } { \theta } ^ { { - } 2 n } y _ { 0 } { + } \textstyle { \sum _ { j = 1 } ^ { n } \theta } ^ { { - } 2 \left( j - 1 \right) }$ and hence determine $r _ { n }$ and $\theta _ { n 1 }$ $_ 1 , n = 1 , 2 , \ldots$ .   
(c) Evaluate the limits as $n  \infty$ of $r _ { n }$ and $\theta _ { n 1 }$ in the two cases $| \theta | < 1$ and $| \theta | \geq 1$ .

4.1 Spectral Densities   
4.2 The Periodogram   
4.3 Time-Invariant Linear Filters   
4.4 The Spectral Density of an ARMA Process

This chapter can be omitted without any loss of continuity. The reader with no background in Fourier or complex analysis should go straight to Chapter 5. The spectral representation of a stationary time series $\{ X _ { t } \}$ essentially decomposes $\{ X _ { t } \}$ into a sum of sinusoidal components with uncorrelated random coefficients. In conjunction with this decomposition there is a corresponding decomposition into sinusoids of the autocovariance function of $\{ X _ { t } \}$ . The spectral decomposition is thus an analogue for stationary processes of the more familiar Fourier representation of deterministic functions. The analysis of stationary processes by means of their spectral representation is often referred to as the “frequency domain analysis” of time series or “spectral analysis.” It is equivalent to “time domain” analysis based on the autocovariance function, but provides an alternative way of viewing the process, which for some applications may be more illuminating. For example, in the design of a structure subject to a randomly fluctuating load, it is important to be aware of the presence in the loading force of a large sinusoidal component with a particular frequency to ensure that this is not a resonant frequency of the structure. The spectral point of view is also particularly useful in the analysis of multivariate stationary processes and in the analysis of linear filters. In Section 4.1 we introduce the spectral density of a stationary process $\{ X _ { t } \}$ , which specifies the frequency decomposition of the autocovariance function, and the closely related spectral representation (or frequency decomposition) of the process $\{ X _ { t } \}$ itself. Section 4.2 deals with the periodogram, a sample-based function

from which we obtain estimators of the spectral density. In Section 4.3 we discuss time-invariant linear filters from a spectral point of view and in Section 4.4 we use the results to derive the spectral density of an arbitrary ARMA process.

# 4.1 Spectral Densities

Suppose that $\{ X _ { t } \}$ is a zero-mean stationary time series with autocovariance function $\gamma ( \cdot )$ satisfying $\begin{array} { r } { \sum _ { h = - \infty } ^ { \infty } \vert \gamma ( h ) \vert < \infty } \end{array}$ . The spectral density of $\{ X _ { t } \}$ is the function $f ( \cdot )$ defined by

$$
f (\lambda) = \frac {1}{2 \pi} \sum_ {h = - \infty} ^ {\infty} e ^ {- i h \lambda} \gamma (h), \quad - \infty <   \lambda <   \infty , \tag {4.1.1}
$$

where $e ^ { i \lambda } = \cos ( \lambda ) + i \sin ( \lambda )$ and $i = \sqrt { - 1 }$ . The summability of $| \gamma ( \cdot ) |$ implies that the series in (4.1.1) converges absolutely (since $\left| e ^ { i h \lambda } \right| ^ { 2 } = \cos ^ { 2 } ( h \lambda ) + \sin ^ { 2 } ( h \lambda ) = 1 \rangle$ ). Since cos and sin have period $2 \pi$ , so also does $f$ , and it suffices to confine attention to the values of $f$ , on the interval $( - \pi , \pi ]$ .

# Basic Properties of $f$ :

(a) $f$ is even, i.e., $f ( \lambda ) = f ( - \lambda )$ , (4.1.2)   
(b) $f ( \lambda ) \geq 0$ for all $\lambda \in ( - \pi , \pi ]$ , and (4.1.3)   
(c) $\gamma ( k ) = \int _ { - \pi } ^ { \pi } e ^ { i k \lambda } f ( \lambda ) d \lambda = \int _ { - \pi } ^ { \pi } \cos ( k \lambda ) f ( \lambda ) d \lambda .$ (4.1.4)

Proof Since $\sin ( \cdot )$ is an odd function and cos(·) and $\gamma ( \cdot )$ are even functions, we have

$$
\begin{array}{l} f (\lambda) = \frac {1}{2 \pi} \sum_ {h = - \infty} ^ {\infty} (\cos (h \lambda) - i \sin (h \lambda)) \gamma (h) \\ = \frac {1}{2 \pi} \sum_ {h = - \infty} ^ {\infty} \cos (- h \lambda) \gamma (h) + 0 \\ = f (- \lambda). \\ \end{array}
$$

For each positive integer $N$ define

$$
\begin{array}{l} f _ {N} (\lambda) = \frac {1}{2 \pi N} E \left(\left| \sum_ {r = 1} ^ {N} X _ {r} e ^ {- i r \lambda} \right| ^ {2}\right) \\ = \frac {1}{2 \pi N} E \left(\sum_ {r = 1} ^ {N} X _ {r} e ^ {- i r \lambda} \sum_ {s = 1} ^ {N} X _ {s} e ^ {i s \lambda}\right) \\ = \frac {1}{2 \pi N} \sum_ {| h | <   N} (N - | h |) e ^ {- i h \lambda} \gamma (h). \\ \end{array}
$$

Clearly, the function $f _ { N }$ is nonnegative for each $N$ , and since $\begin{array} { r } { f _ { N } ( \lambda ) \ \to \ \frac { 1 } { 2 \pi } \sum _ { h = - \infty } ^ { \infty } } \end{array}$ 2 π 	∞h =−∞ $e ^ { - i h \lambda } \gamma ( h ) = f ( \lambda )$ as $N \to \infty$ , $f$ must also be nonnegative. This proves (4.1.3). Turning to (4.1.4),

$$
\begin{array}{l} \int_ {- \pi} ^ {\pi} e ^ {i k \lambda} f (\lambda) d \lambda = \int_ {- \pi} ^ {\pi} \frac {1}{2 \pi} \sum_ {h = - \infty} ^ {\infty} e ^ {i (k - h) \lambda} \gamma (h) d \lambda \\ = \frac {1}{2 \pi} \sum_ {h = - \infty} ^ {\infty} \gamma (h) \int_ {- \pi} ^ {\pi} e ^ {i (k - h) \lambda} d \lambda \\ = \gamma (k), \\ \end{array}
$$

since the only nonzero summand in the second line is the one for which $h = k$ (see Problem 4.1).

Equation (4.1.4) expresses the autocovariances of a stationary time series with absolutely summable ACVF as the Fourier coefficients of the nonnegative even function on $( - \pi , \pi ]$ defined by (4.1.1). However, even if $\begin{array} { r } { \sum _ { h = - \infty } ^ { \infty } | \gamma ( h ) | = \infty } \end{array}$ , there may exist a corresponding spectral density defined as follows.

# Definition 4.1.1

A function $f$ is the spectral density of a stationary time series $\{ X _ { t } \}$ with ACVF $\gamma ( \cdot )$ if

(i) $f ( \lambda ) \geq 0$ for all $\lambda \in ( - \pi , \pi ]$ , and   
(ii) $\gamma ( h ) = \int _ { - \pi } ^ { \pi } e ^ { i h \lambda } f ( \lambda ) d \lambda { \mathrm { ~ f o r ~ a l l ~ i n t e g e r s ~ } } h .$ $\gamma ( h ) = \int _ { - \pi } ^ { \pi } e ^ { i h \lambda } f ( \lambda ) d \lambda$ eihλf (λ) dλ for all integers h.

Remark 1. Spectral densities are essentially unique. That is, if $f$ and $g$ are two spectral densities corresponding to the autocovariance function $\gamma ( \cdot )$ , i.e., $\gamma ( h ) \ =$ $\begin{array} { r } { \int _ { - \pi } ^ { \pi } e ^ { i h \lambda } f ( \lambda ) d \lambda = \int _ { - \pi } ^ { \pi } e ^ { i \bar { h } \lambda } g ( \lambda ) \bar { d } \lambda } \end{array}$ for all integers $h$ , then $f$ and $g$ have the same Fourier coefficients and hence are equal (see, for example, Brockwell and Davis (1991), Section 2.8). -

The following proposition characterizes spectral densities.

Proposition 4.1.1 A real-valued function $f$ defined on $( - \pi , \pi ]$ is the spectral density of a real-valued stationary process if and only if

(i) $f ( \lambda ) = f ( - \lambda )$ ,   
(ii) $f ( \lambda ) \geq 0$ , and   
(iii) $\textstyle \int _ { - \pi } ^ { \pi } f ( \lambda ) d \lambda < \infty$ .

Proof If $\gamma ( \cdot )$ is absolutely summable, then (i)–(iii) follow from the basic properties of $f$ , (4.1.2)–(4.1.4). For the argument in the general case, see Brockwell and Davis (1991), Section 4.3.

Conversely, suppose $f$ satisfies (i)–(iii). Then it is easy to check, using (i), that the function defined by

$$
\gamma (h) = \int_ {- \pi} ^ {\pi} e ^ {i h \lambda} f (\lambda) d \lambda
$$

is even. Moreover, if $\textstyle { a _ { r } \in \mathbb { R } }$ , $r = 1 , \ldots , n$ , then

$$
\begin{array}{l} \sum_ {r, s = 1} ^ {n} a _ {r} \gamma (r - s) a _ {s} = \int_ {- \pi} ^ {\pi} \sum_ {r, s = 1} ^ {n} a _ {r} a _ {s} e ^ {i \lambda (r - s)} f (\lambda) d \lambda \\ = \int_ {- \pi} ^ {\pi} \left| \sum_ {r = 1} ^ {n} a _ {r} e ^ {i \lambda r} \right| ^ {2} f (\lambda) d \lambda \\ \geq 0, \\ \end{array}
$$

so that $\gamma ( \cdot )$ is also nonnegative definite and therefore, by Theorem 2.1.1, is an autocovariance function. 

Corollary 4.1.1 An absolutely summable function $\gamma ( \cdot )$ is the autocovariance function of a stationary time series if and only if it is even and

$$
f (\lambda) = \frac {1}{2 \pi} \sum_ {h = - \infty} ^ {\infty} e ^ {- i h \lambda} \gamma (h) \geq 0, \quad f o r a l l \lambda \in (- \pi , \pi ], \tag {4.1.5}
$$

in which case $f ( \cdot )$ is the spectral density of $\gamma ( \cdot )$ .

Proof We have already established the necessity of (4.1.5). Now suppose (4.1.5) holds. Applying Proposition 4.1.1 (the assumptions are easily checked) we conclude that $f$ is the spectral density of some autocovariance function. But this ACVF must be $\gamma ( \cdot )$ , since $\begin{array} { r } { \bar { \gamma ( k ) } = \int _ { - \pi } ^ { \pi } e ^ { i k \lambda } f ( \lambda ) d \lambda } \end{array}$ for all integers $k$ . 

Example 4.1.1 Using Corollary 4.1.1, it is a simple matter to show that the function defined by

$$
\kappa (h) = \left\{ \begin{array}{l l} 1, & \text {i f} h = 0, \\ \rho , & \text {i f} h = \pm 1, \\ 0, & \text {o t h e r w i s e}, \end{array} \right.
$$

is the ACVF of a stationary time series if and only if $| \rho | \le { \frac { 1 } { 2 } }$ (see Example 2.1.1). Since $\kappa ( \cdot )$ is even and nonzero only at lags $0 , \pm 1$ , it follows from the corollary that $\kappa$ is an ACVF if and only if the function

$$
f (\lambda) = \frac {1}{2 \pi} \sum_ {h = - \infty} ^ {\infty} e ^ {- i h \lambda} \gamma (h) = \frac {1}{2 \pi} [ 1 + 2 \rho \cos \lambda ]
$$

is nonnegative for all $\lambda \in ( - \pi , \pi ]$ . But this occurs if and only if $\begin{array} { r } { | \rho | \le { \frac { 1 } { 2 } } } \end{array}$

As illustrated in the previous example, Corollary 4.1.1 provides us with a powerful tool for checking whether or not an absolutely summable function on the integers is an autocovariance function. It is much simpler and much more informative than direct verification of nonnegative definiteness as required in Theorem 2.1.1.

Not all autocovariance functions have a spectral density. For example, the stationary time series

$$
X _ {t} = A \cos (\omega t) + B \sin (\omega t), \tag {4.1.6}
$$

where $A$ and $B$ are uncorrelated random variables with mean 0 and variance 1, has ACVF $\gamma ( h ) \ : = \ : \cos ( \omega h )$ (Problem 2.2), which is not expressible as $\textstyle { \int _ { - \pi } ^ { \pi } e ^ { i h \lambda } f ( \lambda ) d \lambda }$ , with $f$ a function on $( - \pi , \pi ]$ . Nevertheless, $\gamma ( \cdot )$ can be written as the Fourier transform of the discrete distribution function

$$
F (\lambda) = \left\{ \begin{array}{l l} 0 & \text {i f} \lambda <   - \omega , \\ 0. 5 & \text {i f} - \omega \leq \lambda <   \omega , \\ 1. 0 & \text {i f} \lambda \geq \omega , \end{array} \right.
$$

i.e.,

$$
\cos (\omega h) = \int_ {(- \pi , \pi ]} e ^ {i h \lambda} d F (\lambda),
$$

where the integral is as defined in Section A.1. As the following theorem states (see Brockwell and Davis (1991), p. 117), every ACVF is the Fourier transform of a (generalized) distribution function on $[ - \pi , \pi ]$ . This representation is called the spectral representation of the ACVF.

Theorem 4.1.1 (Spectral Representation of the ACVF) A function $\gamma ( \cdot )$ defined on the integers is the ACVF of a stationary time series if and only if there exists a right-continuous, nondecreasing, bounded function $F$ on $[ - \pi , \pi ]$ with $F ( - \pi ) = 0$ such that

$$
\gamma (h) = \int_ {(- \pi , \pi ]} e ^ {i h \lambda} d F (\lambda) \tag {4.1.7}
$$

for all integers h. (For real-valued time series, $F$ is symmetric in the sense that $\begin{array} { r } { \int _ { ( a , b ] } d F ( x ) = \int _ { [ - b , - a ) } d F ( x ) } \end{array}$ for all a and b such that $0 < a < b .$ .)

Remark 2. The function $F$ is a generalized distribution function on $[ - \pi , \pi ]$ in the sense that $G ( \lambda ) = F ( \lambda ) / F ( \pi )$ is a probability distribution function on $[ - \pi , \pi ]$ . Note that since $F ( \pi ) = \gamma ( 0 ) = \mathrm { V a r } ( X _ { 1 } )$ , the ACF of $\{ X _ { t } \}$ has spectral representation

$$
\rho (h) = \int_ {(- \pi , \pi ]} e ^ {i h \lambda} d G (\lambda).
$$

The function $F$ in (4.1.7) is called the spectral distribution function of $\gamma ( \cdot )$ . If $F ( \lambda )$ can be expressed as $\begin{array} { r } { F ( \lambda ) = \int _ { - \pi } ^ { \lambda } f ( y ) d y } \end{array}$ for all $\lambda \in [ - \pi , \pi ]$ , then $f$ is the spectral density function and the time series is said to have a continuous spectrum. If $F$ is a discrete distribution function (i.e., if $G$ is a discrete probability distribution function), then the time series is said to have a discrete spectrum. The time series (4.1.6) has a discrete spectrum. -

# Example 4.1.2 Linear Combination of Sinusoids

Consider now the process obtained by adding uncorrelated processes of the type defined in (4.1.6), i.e.,

$$
X _ {t} = \sum_ {j = 1} ^ {k} \left(A _ {j} \cos \left(\omega_ {j} t\right) + B _ {j} \sin \left(\omega_ {j} t\right)\right), \quad 0 <   \omega_ {1} <   \dots <   \omega_ {k} <   \pi , \tag {4.1.8}
$$

where $A _ { 1 } , B _ { 1 } , \ldots , A _ { k } , B _ { k }$ are uncorrelated random variables with $E ( A _ { j } ) = E ( B _ { j } ) = 0$ and $\operatorname { V a r } ( A _ { j } ) = \operatorname { V a r } ( B _ { j } ) = \sigma _ { j } ^ { 2 } , j = 1 , \ldots , k .$ By Problem 4.5, the ACVF of this time series is $\begin{array} { r } { \gamma ( h ) \ = \ \sum _ { j = 1 } ^ { k } \sigma _ { j } ^ { 2 } \cos ( \omega _ { j } h ) } \end{array}$ and its spectral distribution function is $F ( \lambda ) =$ $\begin{array} { r } { \sum _ { j = 1 } ^ { k } \sigma _ { j } ^ { 2 } F _ { j } ( \lambda ) } \end{array}$ , where

$$
F _ {j} (\lambda) = \left\{ \begin{array}{l l} 0 & \text {i f} \lambda <   - \omega_ {j}, \\ 0. 5 & \text {i f} - \omega_ {j} \leq \lambda <   \omega_ {j}, \\ 1. 0 & \text {i f} \lambda \geq \omega_ {j}. \end{array} \right.
$$

Figure 4-1 A sample path of size 100 from the time series in Example 4.1.2   
Figure 4-2   
![](images/7fd41be4c1e5cf156921e51b6d23af93a47debf3d7daf982c06aefa086b9e1e1.jpg)  
The spectral distribution function $F ( \lambda )$ , $- \pi \leq \lambda \leq \pi ,$ , of the time series in Example 4.1.2

![](images/e0c58e095b227cfb231b1c1c24f6a920211c26d21772cfc19c6afe480ff5a9a1.jpg)

A sample path of this time series with $k = 2$ , $\omega _ { 1 } = \pi / 4$ , $\omega _ { 2 } = \pi / 6$ , σ 2 9, and σ 22 $\sigma _ { 2 } ^ { 2 } = 1$ is plotted in Figure 4-1. Not surprisingly, the sample path closely approximates a sinusoid with frequency $\omega _ { 1 } = \pi / 4$ (and period $2 \pi / \omega _ { 1 } = 8 $ ). The general features of this sample path could have been deduced from the spectral distribution function (see Figure 4-2), which places $90 \%$ of its total mass at the frequencies $\pm \pi / 4$ . This means that $90 \%$ of the variance of $X _ { t }$ is contributed by the term $A _ { 1 } \cos ( \omega _ { 1 } t ) + B _ { 1 } \cos ( \omega _ { 1 } t )$ , which is a sinusoid with period 8.

The remarkable feature of Example 4.1.2 is that every zero-mean stationary process can be expressed as a superposition of uncorrelated sinusoids with frequencies $\omega \in [ 0 , \pi ]$ . In general, however, a stationary process is a superposition of infinitely many sinusoids rather than a finite number as in (4.1.8). The required generalization of (4.1.8) that allows for this is called a stochastic integral, written as

$$
X _ {t} = \int_ {(- \pi , \pi ]} e ^ {i h \lambda} d Z (\lambda), \tag {4.1.9}
$$

where $\{ Z ( \lambda ) , - \pi \ : < \ : \lambda \ : \le \ : \pi \}$ is a complex-valued process with orthogonal (or uncorrelated) increments. The representation (4.1.9) of a zero-mean stationary process $\{ X _ { t } \}$ is called the spectral representation of the process and should be compared with the corresponding spectral representation (4.1.7) of the autocovariance function $\gamma ( \cdot )$ . The underlying technical aspects of stochastic integration are beyond the scope of this book; however, in the simple case of the process (4.1.8) it is not difficult to see that it can be reexpressed in the form (4.1.9) by choosing

$$
d Z (\lambda) = \left\{ \begin{array}{l l} \frac {A _ {j} + i B _ {j}}{2}, & \text {i f} \lambda = - \omega_ {j} \text {a n d} j \in \{1, \ldots , k \}, \\ \frac {A _ {j} - i B _ {j}}{2}, & \text {i f} \lambda = \omega_ {j} \text {a n d} j \in \{1, \ldots , k \}, \\ 0, & \text {o t h e r w i s e}. \end{array} \right.
$$

For this example it is also clear that

$$
E (d Z (\lambda) \overline {{d Z (\lambda)}}) = \left\{ \begin{array}{l l} \frac {\sigma_ {j} ^ {2}}{2}, & \text {i f} \lambda = \pm \omega_ {j}, \\ 0, & \text {o t h e r w i s e}. \end{array} \right.
$$

In general, the connection between $d Z ( \lambda )$ and the spectral distribution function of the process can be expressed symbolically as

$$
E (d Z (\lambda) \overline {{d Z (\lambda)}}) = \left\{ \begin{array}{l l} F (\lambda) - F (\lambda -), & \text {f o r a d i s c r e t e s p e c t r u m ,} \\ f (\lambda) d \lambda , & \text {f o r a c o n t i n u o u s s p e c t r u m .} \end{array} \right. \tag {4.1.10}
$$

These relations show that a large jump in the spectral distribution function (or a large peak in the spectral density) at frequency $\pm \omega$ indicates the presence in the time series of strong sinusoidal components with frequencies at (or near) $\omega$ radians per unit time. The period of a sinusoid with frequency $\omega$ radians per unit time is $2 \pi / \omega$ .

# Example 4.1.3 White Noise

If $\{ X _ { t } \} \sim \mathrm { W N } \left( 0 , \sigma ^ { 2 } \right)$ , then $\gamma ( 0 ) = \sigma ^ { 2 }$ and $\gamma ( h ) = 0$ for all $| h | > 0$ . This process has a flat spectral density (see Problem 4.2)

$$
f (\lambda) = \frac {\sigma^ {2}}{2 \pi}, - \pi \leq \lambda \leq \pi .
$$

A process with this spectral density is called white noise, since each frequency in the spectrum contributes equally to the variance of the process.

# Example 4.1.4 The Spectral Density of an AR(1) Process

If $\{ X _ { t } \}$ is a causal AR(1) process satisfying the equation,

$$
X _ {t} = \phi X _ {t - 1} + Z _ {t},
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \big ( 0 , \sigma ^ { 2 } \big )$ , then from (4.1.1), $\{ X _ { t } \}$ has spectral density

$$
f (\lambda) = \frac {\sigma^ {2}}{2 \pi (1 - \phi^ {2})} \left(1 + \sum_ {h = 1} ^ {\infty} \phi^ {h} (e ^ {- i h \lambda} + e ^ {i h \lambda})\right)
$$

Figure 4-3

The spectral density $f ( \lambda )$ , $0 \leq \lambda \leq \pi$ , of $X _ { t } = 0 . 7 X _ { t - 1 } + Z _ { t } , $ where $\{ Z _ { t } \} \sim W N \big ( 0 , \sigma ^ { 2 } \big )$

Figure 4-4   
![](images/24466950f1224b75e2cfda1d345fcdf950f8f603e192088da26f9fdcedb00398.jpg)  
The spectral density $f ( \lambda )$ , $0 \leq \lambda \leq \pi .$ , of $X _ { t } = - 0 . 7 X _ { t - 1 } + Z _ { t } , $ where $\{ Z _ { t } \} \sim W N \big ( 0 , \sigma ^ { 2 } \big )$

![](images/ada1dcc946bd4e40b34285e3fd96230d52be7ef8fdd15db63cda374cce685047.jpg)

$$
\begin{array}{l} = \frac {\sigma^ {2}}{2 \pi (1 - \phi^ {2})} \left(1 + \frac {\phi e ^ {i \lambda}}{1 - \phi e ^ {i \lambda}} + \frac {\phi e ^ {- i \lambda}}{1 - \phi e ^ {- i \lambda}}\right) \\ = \frac {\sigma^ {2}}{2 \pi} \left(1 - 2 \phi \cos \lambda + \phi^ {2}\right) ^ {- 1}. \\ \end{array}
$$

Graphs of $f ( \lambda )$ , $0 \leq \lambda \leq \pi$ , are displayed in Figures 4-3 and 4-4 for $\phi = 0 . 7$ and $\phi = - 0 . 7$ . Observe that for $\phi = 0 . 7$ the density is large for low frequencies and small for high frequencies. This is not unexpected, since when $\phi = 0 . 7$ the process has a positive ACF with a large value at lag one (see Figure 4-5), making the series smooth with relatively few high-frequency components. On the other hand, for $\phi = - 0 . 7$ the ACF has a large negative value at lag one (see Figure 4-6), producing a series that

Figure 4-5

The ACF of the AR(1) process $X _ { t } = 0 . 7 X _ { t - 1 } + Z _ { t }$

Figure 4-6   
![](images/6a69b30198d15015523762748386b53f5948311b35d18452a07797a3fa278b79.jpg)  
The ACF of the AR(1) process $X _ { t } = - 0 . 7 X _ { t - 1 } + Z _ { t }$

![](images/247d218a9fb06a72fb6e5989d739ed45b182fb73ef959e4f6b5aa2659cee23fe.jpg)

fluctuates rapidly about its mean value. In this case the series has a large contribution from high-frequency components as reflected by the size of the spectral density near frequency $\pi$ .

# Example 4.1.5 Spectral Density of an MA(1) Process

If

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1},
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \big ( 0 , \sigma ^ { 2 } \big )$ , then from (4.1.1),

$$
f (\lambda) = \frac {\sigma^ {2}}{2 \pi} \left(1 + \theta^ {2} + \theta \left(e ^ {- i \lambda} + e ^ {i \lambda}\right)\right) = \frac {\sigma^ {2}}{2 \pi} \left(1 + 2 \theta \cos \lambda + \theta^ {2}\right).
$$

This function is shown in Figures 4-7 and 4-8 for the values $\theta \quad = \quad 0 . 9$ and $\theta ~ = ~ - 0 . 9$ . Interpretations of the graphs analogous to those in Example 4.1.4 can again be made.

![](images/8b838fa5baa5c9598a14357b96f6ff4b65f72b4b634380bdd6cc91531c93ffad.jpg)  
Figure 4-7 The spectral density $f ( \lambda )$ , $0 \leq \lambda \leq \pi .$ , of $X _ { t } = Z _ { t } + 0 . 9 Z _ { t - 1 }$ where $\{ Z _ { t } \} \sim \dot { W } N \big ( 0 , \sigma ^ { 2 } \big )$

![](images/3024caf116917d31835f3d1b839f1b119403877517435d7d2b220d8b6c597c9a.jpg)  
Figure 4-8 The spectral density $f ( \lambda )$ , $0 \leq \lambda \leq \pi$ , of $X _ { t } = Z _ { t } - 0 . 9 Z _ { t - 1 }$ where $\{ Z _ { t } \} \sim W N \big ( 0 , \sigma ^ { 2 } \big )$

# 4.2 The Periodogram

If $\{ X _ { t } \}$ is a stationary time series $\{ X _ { t } \}$ with $\mathsf { A C V F } \gamma ( \cdot )$ and spectral density $f ( \cdot )$ , then just as the sample ACVF $\hat { \gamma } ( \cdot )$ of the observations $\{ x _ { 1 } , \ldots , x _ { n } \}$ can be regarded as a sample analogue of $\gamma ( \cdot )$ , so also can the periodogram $I _ { n } ( \cdot )$ of the observations be regarded as a sample analogue of $2 \pi f ( \cdot )$ .

To introduce the periodogram, we consider the vector of complex numbers

$$
\mathbf {x} = \left[ \begin{array}{c} x _ {1} \\ x _ {2} \\ \vdots \\ x _ {n} \end{array} \right] \in \mathbb {C} ^ {n},
$$

where $\mathbb { C } ^ { n }$ denotes the set of all column vectors with complex-valued components. Now let $\omega _ { k } = 2 \pi k / n$ , where $k$ is any integer between $- ( n - 1 ) / 2$ and $n / 2$ (inclusive), i.e.,

$$
\omega_ {k} = \frac {2 \pi k}{n}, \quad k = - \left[ \frac {n - 1}{2} \right], \dots , \left[ \frac {n}{2} \right], \tag {4.2.1}
$$

where [ y] denotes the largest integer less than or equal to y. We shall refer to the set $F _ { n }$ of these values as the Fourier frequencies associated with sample size $n$ , noting that $F _ { n }$ is a subset of the interval $( - \pi , \pi ]$ . Correspondingly, we introduce the $n$ vectors

$$
\mathbf {e} _ {k} = \frac {1}{\sqrt {n}} \left[ \begin{array}{c} e ^ {i \omega_ {k}} \\ e ^ {2 i \omega_ {k}} \\ \vdots \\ e ^ {n i \omega_ {k}} \end{array} \right], \quad k = - \left[ \frac {n - 1}{2} \right], \dots , \left[ \frac {n}{2} \right]. \tag {4.2.2}
$$

Now $\mathbf { e } _ { 1 } , \ldots , \mathbf { e } _ { n }$ are orthonormal in the sense that

$$
\mathbf {e} _ {j} ^ {*} \mathbf {e} _ {k} = \left\{ \begin{array}{l l} 1, & \text {i f} j = k, \\ 0, & \text {i f} j \neq k, \end{array} \right. \tag {4.2.3}
$$

where $\mathbf { e } _ { j } { } ^ { \ast }$ denotes the row vector whose kth component is the complex conjugate of the kth component of $\mathbf { e } _ { j }$ (see Problem 4.3). This implies that $\{ \mathbf { e } _ { 1 } , \ldots , \mathbf { e } _ { n } \}$ is a basis for $\mathbb { C } ^ { n }$ , so that any $\mathbf { x } \in \mathbb { C } ^ { n }$ can be expressed as the sum of $n$ components,

$$
\mathbf {x} = \sum_ {k = - [ (n - 1) / 2 ]} ^ {[ n / 2 ]} a _ {k} \mathbf {e} _ {k}. \tag {4.2.4}
$$

The coefficients $a _ { k }$ are easily found by multiplying (4.2.4) on the left by ${ { \bf e } _ { k } } ^ { * }$ and using (4.2.3). Thus,

$$
a _ {k} = \mathbf {e} _ {k} ^ {*} \mathbf {x} = \frac {1}{\sqrt {n}} \sum_ {t = 1} ^ {n} x _ {t} e ^ {- i t \omega_ {k}}. \tag {4.2.5}
$$

The sequence $\{ a _ { k } \}$ is called the discrete Fourier transform of the sequence $\{ x _ { 1 } , \ldots , x _ { n } \}$ .

Remark 1. The tth component of (4.2.4) can be written as

$$
x _ {t} = \sum_ {k = - [ (n - 1) / 2 ]} ^ {[ n / 2 ]} a _ {k} \left[ \cos \left(\omega_ {k} t\right) + i \sin \left(\omega_ {k} t\right) \right], \quad t = 1, \dots , n, \tag {4.2.6}
$$

showing that (4.2.4) is just a way of representing $x _ { t }$ as a linear combination of sine waves with frequencies $\omega _ { k } \in F _ { n }$ . -

# Definition 4.2.1

The periodogram of $\{ x _ { 1 } , \ldots , x _ { n } \}$ is the function

$$
I _ {n} (\lambda) = \frac {1}{n} \left| \sum_ {t = 1} ^ {n} x _ {t} e ^ {- i t \lambda} \right| ^ {2}. \tag {4.2.7}
$$

Remark 2. If $\lambda$ is one of the Fourier frequencies $\omega _ { k }$ , then $I _ { n } ( \omega _ { k } ) = | a _ { k } | ^ { 2 }$ , and so from (4.2.4) and (4.2.3) we find at once that the squared length of $\mathbf { X }$ is

$$
\sum_ {t = 1} ^ {n} \left| x _ {t} \right| ^ {2} = \mathbf {x} ^ {*} \mathbf {x} = \sum_ {k = - [ (n - 1) / 2 ]} ^ {[ n / 2 ]} \left| a _ {k} \right| ^ {2} = \sum_ {k = - [ (n - 1) / 2 ]} ^ {[ n / 2 ]} I _ {n} (\omega_ {k}).
$$

The value of the periodogram at frequency $\omega _ { k }$ is thus the contribution to this sum of squares from the “frequency $\omega _ { k } \mathbf { \overrightarrow { \Omega } }$ term $a _ { k } \mathbf { e } _ { k }$ in (4.2.4). -

The next proposition shows that $I _ { n } ( \lambda )$ can be regarded as a sample analogue of $2 \pi f ( \lambda )$ . Recall that if $\begin{array} { r } { \sum _ { h = - \infty } ^ { \infty } \vert \gamma ( h ) \vert < \infty } \end{array}$ , then

$$
2 \pi f (\lambda) = \sum_ {h = - \infty} ^ {\infty} \gamma (h) e ^ {- i h \lambda}, \quad \lambda \in (- \pi , \pi ]. \tag {4.2.8}
$$

Proposition 4.2.1 If $x _ { 1 } , \ldots , x _ { n }$ are any real numbers and $\omega _ { k }$ is any of the nonzero Fourier frequencies $2 \pi k / n$ in $( - \pi , \pi ] ,$ , then

$$
I _ {n} \left(\omega_ {k}\right) = \sum_ {| h | <   n} \hat {\gamma} (h) e ^ {- i h \omega_ {k}}, \tag {4.2.9}
$$

where $\hat { \gamma } ( h )$ is the sample ACVF of $x _ { 1 } , \ldots , x _ { n }$ .

Proof Since $\begin{array} { r } { \sum _ { t = 1 } ^ { n } e ^ { - i t \omega _ { k } } = 0 } \end{array}$ if $\omega _ { k } \neq 0$ , we can subtract the sample mean $\bar { x }$ from $x _ { t }$ in the defining equation (4.2.7) of $I _ { n } ( \omega _ { k } )$ . Hence,

$$
\begin{array}{l} I _ {n} (\omega_ {k}) = n ^ {- 1} \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} (x _ {s} - \bar {x}) (x _ {t} - \bar {x}) e ^ {- i (s - t) \omega_ {k}} \\ = \sum_ {| h | <   n} \hat {\gamma} (h) e ^ {- i h \omega_ {k}}. \\ \end{array}
$$

In view of the similarity between (4.2.8) and (4.2.9), a natural estimate of the spectral density $f ( \lambda )$ is $I _ { n } ( \lambda ) / ( 2 \pi )$ . For a very large class of stationary time series $\{ X _ { t } \}$ with strictly positive spectral density, it can be shown that for any fixed frequencies $\lambda _ { 1 } , \ldots , \lambda _ { m }$ such that $0 < \lambda _ { 1 } < \cdots < \lambda _ { m } < \pi$ , the joint distribution function $F _ { n } ( x _ { 1 } , \ldots , x _ { m } )$ of the periodogram values $( I _ { n } ( \lambda _ { 1 } ) , \dots , I _ { n } ( \lambda _ { m } ) )$ converges, as $n \to \infty$ , to $F ( x _ { 1 } , \dots , x _ { m } )$ , where

$$
F \left(x _ {1}, \dots , x _ {m}\right) = \left\{ \begin{array}{l l} \prod_ {i = 1} ^ {m} \left(1 - \exp \left\{\frac {- x _ {i}}{2 \pi f \left(\lambda_ {i}\right)} \right\}\right), & \text {i f} x _ {1}, \dots , x _ {m} > 0, \\ 0, & \text {o t h e r w i s e .} \end{array} \right. \tag {4.2.10}
$$

Thus for large $n$ the periodogram ordinates $( I _ { n } ( \lambda _ { 1 } ) , \dots , I _ { n } ( \lambda _ { m } ) )$ are approximately distributed as independent exponential random variables with means $2 \pi f ( \lambda _ { 1 } )$ , …, $2 \pi f ( \lambda _ { m } )$ , respectively. In particular, for each fixed $\lambda \in ( 0 , \pi )$ and $\epsilon > 0$ ,

$$
P \left[\left| I _ {n} (\lambda) - 2 \pi f (\lambda) \right| > \epsilon \right]\rightarrow p > 0, \text {a s} n \rightarrow \infty ,
$$

so the probability of an estimation error larger than $\epsilon$ cannot be made arbitrarily small by choosing a sufficiently large sample size $n$ . Thus, $I _ { n } ( \lambda )$ is not a consistent estimator of $2 \pi f ( \lambda )$ .

Since for large $n$ the periodogram ordinates at fixed frequencies are approximately independent with variances changing only slightly over small frequency intervals, we might hope to construct a consistent estimator of $f ( \lambda )$ by averaging the periodogram estimates in a small frequency interval containing $\lambda$ , provided that we can choose the interval in such a way that its width decreases to zero while at the same time the number

of Fourier frequencies in the interval increases to $\infty$ as $n  \infty$ . This can indeed be done, since the number of Fourier frequencies in any fixed frequency interval increases approximately linearly with $n$ . Consider, for example, the estimator

$$
\tilde {f} (\lambda) = \frac {1}{2 \pi} \sum_ {| j | \leq m} (2 m + 1) ^ {- 1} I _ {n} (g (n, \lambda) + 2 \pi j / n), \tag {4.2.11}
$$

where $m \ = \ { \sqrt { n } }$ and $g ( n , \lambda )$ is the multiple of $2 \pi / n$ closest to $\lambda$ . The number of periodogram ordinates being averaged is approximately $2 \sqrt { n }$ , and the width of the frequency interval over which the average is taken is approximately $4 \pi / { \sqrt { n } }$ . It can be shown (see Brockwell and Davis (1991), Section 11.4) that this estimator is consistent for the spectral density $f$ . The argument in fact establishes the consistency of a whole class of estimators defined as follows.

# Definition 4.2.2

A discrete spectral average estimator of the spectral density $f ( \lambda )$ has the form

$$
\hat {f} (\lambda) = \frac {1}{2 \pi} \sum_ {| j | \leq m _ {n}} W _ {n} (j) I _ {n} (g (n, \lambda) + 2 \pi j / n), \tag {4.2.12}
$$

where the bandwidths $m _ { n }$ satisfy

$$
m _ {n} \rightarrow \infty \text {a n d} m _ {n} / n \rightarrow 0 \text {a s} n \rightarrow \infty , \tag {4.2.13}
$$

and the weight functions $W _ { n } ( \cdot )$ satisfy

$$
W _ {n} (j) = W _ {n} (- j), W _ {n} (j) \geq 0 \text {f o r a l l} j, \tag {4.2.14}
$$

$$
\sum_ {| j | \leq m _ {n}} W _ {n} (j) = 1, \tag {4.2.15}
$$

and

$$
\sum_ {| j | \leq m _ {n}} W _ {n} ^ {2} (j) \rightarrow 0 \text {a s} n \rightarrow \infty . \tag {4.2.16}
$$

Remark 3. The conditions imposed on the sequences $\{ m _ { n } \}$ and $\{ W _ { n } ( \cdot ) \}$ ensure consistency of ${ \hat { f } } ( \lambda )$ for $f ( \lambda )$ for a very large class of stationary processes (see Brockwell and Davis (1991), Theorem 10.4.1) including all the ARMA processes considered in this book. The conditions (4.2.13) simply mean that the number of terms in the weighted average (4.2.12) goes to $\infty$ as $n  \infty$ while at the same time the width of the frequency interval over which the average is taken goes to zero. The conditions on $\{ W _ { n } ( \cdot ) \}$ ensure that the mean and variance of $\hat { \ b { f } } ( \lambda )$ converge as $n  \infty$ to $f ( \lambda )$ and 0, respectively. Under the conditions of Brockwell and Davis (1991), Theorem 10.4.1, it can be shown, in fact, that

$$
\lim  _ {n \rightarrow \infty} E \hat {f} (\lambda) = f (\lambda)
$$

and

$$
\lim  _ {n \to \infty} \left(\sum_ {| j | \leq m _ {n}} W _ {n} ^ {2} (j)\right) ^ {- 1} \operatorname {C o v} (\hat {f} (\lambda), \hat {f} (\nu)) = \left\{ \begin{array}{l l} 2 f ^ {2} (\lambda) & \text {i f \lambda = \nu = 0 o r \pi}, \\ f ^ {2} (\lambda) & \text {i f 0 <   \lambda = \nu <   \pi}, \\ 0 & \text {i f \lambda \neq \nu}. \end{array} \right.
$$

-

The spectral density estimate, $I _ { 1 0 0 } ( \lambda ) / ( 2 \pi ) ,$ , $0 < \lambda \leq \pi$ , of the sunspot numbers, 1770–1869

![](images/c124a5d7d46903926b12800042c9bbc7a62851ab58b31705a0f2a4ec02b365ea.jpg)  
Figure 4-9

Example 4.2.1 For the simple moving average estimator with $m _ { n } = { \sqrt { n } }$ and $W _ { n } ( j ) = ( 2 m _ { n } + 1 ) ^ { - 1 }$ , $| j | \leq m _ { n }$ , Remark 3 gives

$$
\left(2 \sqrt {n} + 1\right) \operatorname {V a r} \left(\hat {f} (\lambda)\right) \to \left\{ \begin{array}{l l} 2 f ^ {2} (\lambda) & \text {i f} \lambda = 0 \text {o r} \pi , \\ f ^ {2} (\lambda) & \text {i f} 0 <   \lambda <   \pi . \end{array} \right.
$$

![](images/fb8df8163cc5da501055843d36ea70c2c285083d0b03f5a5f1b0570215454571.jpg)

In practice, when the sample size $n$ is a fixed finite number, the choice of $m$ and $\{ W ( \cdot ) \}$ involves a compromise between achieving small bias and small variance for the estimator $\hat { \ b { f } } ( \lambda )$ . A weight function that assigns roughly equal weights to a broad band of frequencies will produce an estimate of $f ( \lambda )$ that, although smooth, may have a large bias, since the estimate of $f ( \lambda )$ depends on the values of $I _ { n }$ at frequencies distant from λ. On the other hand, a weight function that assigns most of its weight to a narrow frequency band centered at zero will give an estimator with relatively small bias, but with a larger variance. In practice it is advisable to experiment with a range of weight functions and to select the one that appears to strike a satisfactory balance between bias and variance.

The option Spectrum>Smoothed Periodogram in the program ITSM allows the user to apply up to 50 successive discrete spectral average filters with weights $W ( j ) = 1 / ( 2 m + 1 )$ , $j = - m , - m + 1 , \ldots , m$ , to the periodogram. The value of $m$ for each filter can be specified arbitrarily, and the weights of the filter corresponding to the combined effect (the convolution of the component filters) is displayed by the program. The program computes the corresponding discrete spectral average estimators $\tilde { f } ( \lambda ) , 0 \le \lambda \le \overline { { \pi } }$ .

# Example 4.2.2 The Sunspot Numbers, 1770–1869

Figure 4-9 displays a plot of $( 2 \pi ) ^ { - 1 }$ times the periodogram of the annual sunspot numbers (obtained by opening the project SUNSPOTS.TSM in ITSM and selecting Spectrum>Periodogram). Figure 4-10 shows the result of applying the discrete spectral weights $\left\{ \begin{array} { l l } { { \frac { 1 } { 3 } } , { \frac { 1 } { 3 } } , { \frac { 1 } { 3 } } } \end{array} \right\}$ (corresponding to $m = 1$ , $W ( j ) = 1 / ( 2 m + 1 ) , | j | \leq m )$ $| j | \leq m )$ . It is obtained from ITSM by selecting Spectrum $! >$ Smoothed Periodogram, entering 1 for the number of Daniell filters, 1 for the order $m$ , and clicking on Apply. As expected, with such a small value of $m$ , not much smoothing of the periodogram occurs. If we change the number of Daniell filters to 2 and set the order of the first

![](images/536d2a4060609c344fa7e1f381b388666aa2f9c9e34ffe1240ece0289bffefca.jpg)  
Figure 4-10 The spectral density estimate, $\hat { \tilde { f } } ( \lambda )$ , $0 < \lambda \leq \pi$ of the sunspot numbers, 1770–1869, with weights $\left\{ { \frac { 1 } { 3 } } , { \frac { 1 } { 3 } } , { \frac { 1 } { 3 } } \right\}$

![](images/fed90cefc0722632667556f13466bef1e6b19d2c123ec73a71fa79dee68ea3a9.jpg)  
Figure 4-11 The spectral density estimate, $\hat { f } ( \lambda ) , 0 < \lambda \leq \pi$ , of the sunspot numbers, 1770–1869, with weights $\left\{ \frac { 1 } { 1 5 } , \frac { 2 } { 1 5 } , \frac { 3 } { 1 5 } , \frac { 3 } { 1 5 } , \frac { 3 } { 1 5 } , \frac { 2 } { 1 5 } , \frac { 1 } { 1 5 } \right\}$

filter to 1 and the order of the second filter to 2, we obtain a combined filter with a more dispersed set of weights, $\begin{array} { r } { W ( 0 ) ~ = ~ W ( 1 ) ~ = ~ \frac { 3 } { 1 5 } } \end{array}$ $W ( 0 ) \ = \ W ( 1 ) \ = \ { \textstyle { \frac { 3 } { 1 5 } } } , W ( 2 ) \ = \ { \textstyle { \frac { 2 } { 1 5 } } } , W ( 3 ) \ = \ { \textstyle { \frac { 1 } { 1 5 } } }$ $W ( 2 ) ~ = ~ { \frac { 2 } { 1 5 } }$ 215 , W (3) = 115 . $W ( 3 ) ~ = ~ { \frac { 1 } { 1 5 } }$ Clicking on Apply will then give the smoother spectral estimate shown in Figure 4-11. When you are satisfied with the smoothed estimate click OK, and the dialog box will close. All three spectral density estimates show a well-defined peak at the frequency $\omega _ { 1 0 } = 2 \pi / 1 0$ radians per year, in keeping with the suggestion from the graph of the data itself that the sunspot series contains an approximate cycle with period around 10 or 11 years.

# 4.3 Time-Invariant Linear Filters

In Section 1.5 we saw the utility of time-invariant linear filters for smoothing the data, estimating the trend, eliminating the seasonal and/or trend components of the data, etc. A linear process is the output of a time-invariant linear filter (TLF) applied to a white noise input series. More generally, we say that the process $\{ Y _ { t } \}$ is the output of a linear filter $C = \{ c _ { t , k } , t , k = 0 \pm 1 , . . . \}$ applied to an input process $\{ X _ { t } \}$ if

$$
Y _ {t} = \sum_ {k = - \infty} ^ {\infty} c _ {t, k} X _ {k}, \quad t = 0, \pm 1, \dots . \tag {4.3.1}
$$

The filter is said to be time-invariant if the weights $c _ { t , t - k }$ are independent of $t$ , i.e., if

$$
c _ {t, t - k} = \psi_ {k}.
$$

In this case,

$$
Y _ {t} = \sum_ {k = - \infty} ^ {\infty} \psi_ {k} X _ {t - k}
$$

and

$$
Y _ {t - s} = \sum_ {k = - \infty} ^ {\infty} \psi_ {k} X _ {t - s - k},
$$

so that the time-shifted process $\{ Y _ { t - s } , t = 0 , \pm 1 , . . . \}$ is obtained from $\{ X _ { t - s } , t \ =$ $0 , \pm 1 , \ldots \}$ by application of the same linear filter $\Psi = \{ \psi _ { j } , j = 0 , \pm 1 , . . . \}$ . The TLF $\psi$ is said to be causal if

$$
\psi_ {j} = 0 \text {f o r} j <   0,
$$

since then $Y _ { t }$ is expressible in terms only of $X _ { s }$ , $s \leq t$ .

Example 4.3.1 The filter defined by

$$
Y _ {t} = a X _ {- t}, \quad t = 0, \pm 1, \dots ,
$$

is linear but not time-invariant, since $c _ { t , \ t - k } = 0$ except when $2 t = k$ . Thus, $c _ { t , \ t - k }$ depends on the value of $t$ .

![](images/53692bc7e156e6da9007ebddce1e1ef9a6cf5cb7f3303808d8c43c42855edcb9.jpg)

Example 4.3.2 The Simple Moving Average

The filter

$$
Y _ {t} = (2 q + 1) ^ {- 1} \sum_ {| j | \leq q} X _ {t - j}
$$

is a TLF with $\psi _ { j } = ( 2 q + 1 ) ^ { - 1 } , j = - q , \dots , q$ , and $\psi _ { j } = 0$ otherwise.

Spectral methods are particularly valuable in describing the behavior of timeinvariant linear filters as well as in designing filters for particular purposes such as the suppression of high-frequency components. The following proposition shows how the spectral density of the output of a TLF is related to the spectral density of the input—a fundamental result in the study of time-invariant linear filters.

Proposition 4.3.1 Let $\{ X _ { t } \}$ be a stationary time series with mean zero and spectral density $f _ { X } ( \lambda )$ . Suppose that $\Psi ~ = ~ \{ \psi _ { j } , j ~ = ~ 0 , \pm 1 , . . . \}$ is an absolutely summable TLF (i.e., $\textstyle \sum _ { j = - \infty } ^ { \infty } | \psi _ { j } | < \infty$ ). Then the time series

$$
Y _ {t} = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} X _ {t - j}
$$

is stationary with mean zero and spectral density

$$
f _ {Y} (\lambda) = \left| \Psi (e ^ {- i \lambda}) \right| ^ {2} f _ {X} (\lambda) = \Psi (e ^ {- i \lambda}) \Psi (e ^ {i \lambda}) f _ {X} (\lambda),
$$

where $\begin{array} { r c l } { \Psi ( e ^ { - i \lambda } ) } & { = } & { \sum _ { j = - \infty } ^ { \infty } \psi _ { j } e ^ { - i j \lambda } } \end{array}$ . (The function $\Psi \left( e ^ { - i \cdot } \right)$ is called the transfer function of the filter, and the squared modulus $\left| \Psi \left( e ^ { - i \cdot } \right) \right| ^ { 2 }$ is referred to as the power transfer function of the filter.)

Proof Applying Proposition 2.2.1, we see that $\{ Y _ { t } \}$ is stationary with mean 0 and ACVF

$$
\gamma_ {Y} (h) = \sum_ {j, k = - \infty} ^ {\infty} \psi_ {j} \psi_ {k} \gamma_ {X} (h + k - j). \tag {4.3.2}
$$

Since $\{ X _ { t } \}$ has spectral density $f _ { X } ( \lambda )$ , we have

$$
\gamma_ {X} (h + k - j) = \int_ {- \pi} ^ {\pi} e ^ {i (h - j + k) \lambda} f _ {X} (\lambda) d \lambda , \tag {4.3.3}
$$

which, when substituted into (4.3.2), gives

$$
\begin{array}{l} \gamma_ {Y} (h) = \sum_ {j, k = - \infty} ^ {\infty} \psi_ {j} \psi_ {k} \int_ {- \pi} ^ {\pi} e ^ {i (h - j + k) \lambda} f _ {X} (\lambda) d \lambda \\ = \int_ {- \pi} ^ {\pi} \left(\sum_ {j = - \infty} ^ {\infty} \psi_ {j} e ^ {- i j \lambda}\right) \left(\sum_ {k = - \infty} ^ {\infty} \psi_ {k} e ^ {i k \lambda}\right) e ^ {i h \lambda} f _ {X} (\lambda) d \lambda \\ = \int_ {- \pi} ^ {\pi} e ^ {i h \lambda} \left| \sum_ {j = - \infty} ^ {\infty} \psi_ {j} e ^ {- i j \lambda} \right| ^ {2} f _ {X} (\lambda) d \lambda . \\ \end{array}
$$

The last expression immediately identifies the spectral density function of $\{ Y _ { t } \}$ as

$$
f _ {Y} (\lambda) = \left| \psi (e ^ {- i \lambda}) \right| ^ {2} f _ {X} (\lambda) = \psi (e ^ {- i \lambda}) \psi (e ^ {i \lambda}) f _ {X} (\lambda).
$$

Remark 4. Proposition 4.3.1 allows us to analyze the net effect of applying one or more filters in succession. For example, if the input process $\{ X _ { t } \}$ with spectral density $f _ { X }$ is operated on sequentially by two absolutely summable TLFs $\Psi _ { 1 }$ and $\Psi _ { 2 }$ , then the net effect is the same as that of a TLF with transfer function $\psi _ { 1 } \big ( e ^ { - i \lambda } \big ) \Psi _ { 2 } \big ( e ^ { - i \lambda } \big )$ and the spectral density of the output process

$$
W _ {t} = \psi_ {1} (B) \psi_ {2} (B) X _ {t}
$$

is $\left| \psi _ { 1 } \left( e ^ { - i \lambda } \right) \psi _ { 2 } \left( e ^ { - i \lambda } \right) \right| ^ { 2 } f _ { X } ( \lambda )$ . (See also Remark 2 of Section 2.2.)

As we saw in Section 1.5, differencing at lag s is one method for removing a seasonal component with period $s$ from a time series. The transfer function for this filter is $1 - e ^ { - i s \lambda }$ , which is zero for all frequencies that are integer multiples of $2 \pi / s$ radians per unit time. Consequently, this filter has the desired effect of removing all components with period $s$ .

The simple moving-average filter in Example 4.3.2 has transfer function

$$
\psi \left(e ^ {- i \lambda}\right) = D _ {q} (\lambda),
$$

![](images/a77b3f31605ef31ff2921c45c167f29c1fe124f73e6087813ad2b6f122a4aa13.jpg)  
Figure 4-12 The transfer function $D _ { 1 0 } ( \lambda )$ for the simple moving-average filter

where $D _ { q } ( \lambda )$ is the Dirichlet kernel

$$
D_{q}(\lambda) = (2q + 1)^{-1}\sum_{|j|\leq q}e^{-ij\lambda} = \left\{ \begin{array}{ll}\frac{\sin[(q + 0.5)\lambda]}{(2q + 1)\sin(\lambda / 2)}, & \text{if}\lambda \neq 0,\\ 1, & \text{if}\lambda = 0. \end{array} \right.
$$

A graph of $D _ { q }$ is given in Figure 4-12. Notice that $| D _ { q } ( \lambda ) |$ is near 1 in a neighborhood of 0 and tapers off to 0 for large frequencies. This is an example of a low-pass filter. The ideal low-pass filter would have a transfer function of the form

$$
\psi (e ^ {- i \lambda}) = \left\{ \begin{array}{l l} 1, & \text {i f} | \lambda | \leq \omega_ {c}, \\ 0, & \text {i f} | \lambda | > \omega_ {c}, \end{array} \right.
$$

where $\omega _ { c }$ is a predetermined cutoff value. To determine the corresponding linear filter, we expand $\Psi \left( e ^ { - i \lambda } \right)$ as a Fourier series,

$$
\psi \left(e ^ {- i \lambda}\right) = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} e ^ {- i j \lambda}, \tag {4.3.4}
$$

with coefficients

$$
\psi_ {j} = \frac {1}{2 \pi} \int_ {- \omega_ {c}} ^ {\omega_ {c}} e ^ {i j \lambda} d \lambda = \left\{ \begin{array}{l l} \frac {\omega_ {c}}{\pi}, & \mathrm {i f} j = 0, \\ \frac {\sin (j \omega_ {c})}{j \pi}, & \mathrm {i f} | j | > 0. \end{array} \right.
$$

We can approximate the ideal low-pass filter by truncating the series in (4.3.4) at some large value $q$ , which may depend on the length of the observed input series. In Figure 4-13 the transfer function of the ideal low-pass filter with $w _ { c } { = } \pi / 4$ is plotted with the approximations $\begin{array} { r } { \Psi ^ { ( q ) } \big ( e ^ { - i \lambda } \big ) { = } \sum _ { j { = - } q } ^ { q } \psi _ { j } e ^ { - i j \lambda } } \end{array}$ for $q { = } 2$ and $q { = } 1 0$ . As can be seen in the figure, the approximations do not mirror $\Psi$ very well near the cutoff value $\omega _ { c }$ and behave like damped sinusoids for frequencies greater than $\omega _ { c }$ . The poor approximation in the neighborhood of $\omega _ { c }$ is typical of Fourier series approximations to functions with discontinuities, an effect known as the Gibbs phenomenon. Convergence factors may

![](images/fcd14618ce34a5888d0c543a4d7e66bac8424a22200ba0168836a10c775a0742.jpg)  
Figure 4-13 The transfer function for the ideal low-pass filter and truncated Fourier approximations (q) for $\Psi ^ { ( q ) }$ q  2, 10 $q = 2 , 1 0$

be employed to help mitigate the overshoot problem at $\omega _ { c }$ and to improve the overall approximation of $\bar { \Psi } ^ { ( q ) } \bigl ( e ^ { - i \cdot } \bigr )$ to $\Psi \left( e ^ { - i \cdot } \right)$ (see Bloomfield 2000).

# 4.4 The Spectral Density of an ARMA Process

In Section 4.1 the spectral density was computed for an MA(1) and for an AR(1) process. As an application of Proposition 4.3.1, we can now easily derive the spectral density of an arbitrary $\mathbf { A R M A } ( p , q )$ process.

Spectral Density of an ARMA(p,q) Process: If $\{ X _ { t } \}$ is a causal ARMA(p, q) process satisfying $\phi ( B ) X _ { t } = \theta ( B ) Z _ { t }$ , then

$$
f _ {X} (\lambda) = \frac {\sigma^ {2}}{2 \pi} \frac {\left| \theta \left(e ^ {- i \lambda}\right) \right| ^ {2}}{\left| \phi \left(e ^ {- i \lambda}\right) \right| ^ {2}}, \quad - \pi \leq \lambda \leq \pi . \tag {4.4.1}
$$

Because the spectral density of an ARMA process is a ratio of trigonometric polynomials, it is often called a rational spectral density.

Proof From (3.1.3), $\{ X _ { t } \}$ is obtained from $\{ Z _ { t } \}$ by application of the TLF with transfer function

$$
\psi \left(e ^ {- i \lambda}\right) = \frac {\theta \left(e ^ {- i \lambda}\right)}{\phi \left(e ^ {- i \lambda}\right)}.
$$

Since $\{ Z _ { t } \}$ has spectral density $f _ { Z } ( \lambda ) = \sigma ^ { 2 } / ( 2 \pi )$ , the result now follows from Proposition 4.3.1. 

For any specified values of the parameters $\phi _ { 1 } , \ldots , \phi _ { p } , \theta _ { 1 } , \ldots , \theta _ { q }$ and $\sigma ^ { 2 }$ , the Spectrum>Model option of ITSM can be used to plot the model spectral density.

# Figure 4-14

The spectral density $f _ { X } ( \lambda )$ , $0 \leq \lambda \leq \pi$ of the AR(2) model (3.2.20) fitted to the mean-corrected sunspot series

![](images/9e2bf60b6faee38fd5fb372f6712997735894d458559590f816d9ef535362664.jpg)

# Example 4.4.1 The Spectral Density of an AR(2) Process

For an AR(2) process (4.4.1) becomes

$$
\begin{array}{l} f _ {X} (\lambda) = \frac {\sigma^ {2}}{2 \pi \left(1 - \phi_ {1} e ^ {- i \lambda} - \phi_ {2} e ^ {- 2 i \lambda}\right) \left(1 - \phi_ {1} e ^ {i \lambda} - \phi_ {2} e ^ {2 i \lambda}\right)} \\ = \frac {\sigma^ {2}}{2 \pi \left(1 + \phi_ {1} ^ {2} + 2 \phi_ {2} + \phi_ {2} ^ {2} + 2 \left(\phi_ {1} \phi_ {2} - \phi_ {1}\right) \cos \lambda - 4 \phi_ {2} \cos^ {2} \lambda\right)}. \\ \end{array}
$$

Figure 4-14 shows the spectral density, found from the Spectrum>Model option of ITSM, for the model (3.2.20) fitted to the mean-corrected sunspot series. Notice the well-defined peak in the model spectral density. The frequency at which this peak occurs can be found by differentiating the denominator of the spectral density with respect to cos λ and setting the derivative equal to zero. This gives

$$
\cos \lambda = \frac {\phi_ {1} \phi_ {2} - \phi_ {1}}{4 \phi_ {2}} = 0. 8 4 9.
$$

The corresponding frequency is $\lambda { \mathrm { ~  ~ { ~ \alpha ~ } ~ } } = 0 . 5 5 6$ radians per year, or equivalently $c = \lambda / ( 2 \pi ) = 0 . 0 8 8 5$ cycles per year, and the corresponding period is therefore $1 / 0 . 0 8 8 5 = 1 1 . 3$ years. The model thus reflects the approximate cyclic behavior of the data already pointed out in Example 4.2.2. The model spectral density in Figure 4-14 should be compared with the rescaled periodogram of the data and the nonparametric spectral density estimates of Figures 4-9, 4-10, and 4-11.

# Example 4.4.2 The ARMA(1,1) Process

In this case the expression (4.4.1) becomes

$$
\begin{array}{l} f _ {X} (\lambda) = \frac {\sigma^ {2} (1 + \theta e ^ {i \lambda}) (1 + \theta e ^ {- i \lambda})}{2 \pi (1 - \phi e ^ {i \lambda}) (1 - \phi e ^ {- i \lambda})} \\ = \frac {\sigma^ {2} (1 + \theta^ {2} + 2 \theta \cos \lambda)}{2 \pi (1 + \phi^ {2} - 2 \phi \cos \lambda)}. \\ \end{array}
$$

# 4.4.1 Rational Spectral Density Estimation

An alternative to the spectral density estimator of Definition 4.2.2 is the estimator obtained by fitting an ARMA model to the data and then computing the spectral density of the fitted model. The spectral density shown in Figure 4-14 can be regarded as such an estimate, obtained by fitting an AR(2) model to the mean-corrected sunspot data.

Provided that there is an ARMA model that fits the data satisfactorily, this procedure has the advantage that it can be made systematic by selecting the model according (for example) to the AICC criterion (see Section 5.5.2). For further information see Brockwell and Davis (1991), Section 10.6.

# Problems

# 4.1 Show that

$$
\int_ {- \pi} ^ {\pi} e ^ {i (k - h) \lambda} d \lambda = \left\{ \begin{array}{l l} 2 \pi , & \text {i f} k = h, \\ 0, & \text {o t h e r w i s e}. \end{array} \right.
$$

4.2 If $\{ Z _ { t } \} \sim \mathrm { W N } \big ( 0 , \sigma ^ { 2 } \big )$ , apply Corollary 4.1.1 to compute the spectral density of $\{ Z _ { t } \}$ .   
4.3 Show that the vectors $\mathbf { e } _ { 1 } , \ldots , \mathbf { e } _ { n }$ are orthonormal in the sense of (4.2.3).   
4.4 Use Corollary 4.1.1 to establish whether or not the following function is the autocovariance function of a stationary process $\{ X _ { t } \}$ :

$$
\gamma (h) = \left\{ \begin{array}{l l} 1 & \text {i f} h = 0, \\ - 0. 5 & \text {i f} h = \pm 2, \\ - 0. 2 5 & \text {i f} h = \pm 3, \\ 0 & \text {o t h e r w i s e .} \end{array} \right.
$$

4.5 If $\{ X _ { t } \}$ and $\{ Y _ { t } \}$ are uncorrelated stationary processes with autocovariance functions $\gamma _ { X } ( \cdot )$ and $\gamma _ { Y } ( \cdot )$ and spectral distribution functions $F _ { X } ( \cdot )$ and $F _ { Y } ( \cdot )$ , respectively, show that the process $\{ Z _ { t } = X _ { t } + Y _ { t } \}$ is stationary with autocovariance function $\gamma _ { Z } = \gamma _ { X } + \gamma _ { Y }$ and spectral distribution function $F _ { Z } = F _ { X } + F _ { Y }$ .   
4.6 Let $\{ X _ { t } \}$ be the process defined by

$$
X _ {t} = A \cos (\pi t / 3) + B \sin (\pi t / 3) + Y _ {t},
$$

where $Y _ { t } = Z _ { t } + 2 . 5 Z _ { t - 1 }$ , $\{ Z _ { t } \} \sim \mathrm { W N } \big ( 0 , \sigma ^ { 2 } \big )$ , A and $B$ are uncorrelated with mean 0 and variance $\nu ^ { 2 }$ , and $Z _ { t }$ is uncorrelated with $A$ and $B$ for each $t$ . Find the autocovariance function and spectral distribution function of $\{ X _ { t } \}$ .

4.7 Let $\{ X _ { t } \}$ denote the sunspot series filed as SUNSPOTS.TSM and let $\{ Y _ { t } \}$ denote the mean-corrected series $Y _ { t } = X _ { t } - 4 6 . 9 3$ , $t = 1 , \ldots , 1 0 0$ . Use ITSM to find the Yule–Walker AR(2) model

$$
Y _ {t} = \phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + Z _ {t}, \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right),
$$

i.e., find $\phi _ { 1 } , \phi _ { 2 }$ , and $\sigma ^ { 2 }$ . Use ITSM to plot the spectral density of the fitted model and find the frequency at which it achieves its maximum value. What is the corresponding period?

4.8 (a) Use ITSM to compute and plot the spectral density of the stationary series $\{ X _ { t } \}$ satisfying

$$
X _ {t} - 0. 9 9 X _ {t - 3} = Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 1).
$$

(b) Does the spectral density suggest that the sample paths of $\{ X _ { t } \}$ will exhibit approximately oscillatory behavior? If so, then with what period?   
(c) Use ITSM to simulate a realization of $X _ { 1 } , \ldots , X _ { 6 0 }$ and plot the realization. Does the graph of the realization support the conclusion of part (b)? Save the generated series as X.TSM by clicking on the window displaying the graph, then on the red EXP button near the top of the screen. Select Time Series and File in the resulting dialog box and click OK. You will then be asked to provide the file name, X.TSM.   
(d) Compute the spectral density of the filtered process

$$
Y _ {t} = \frac {1}{3} \left(X _ {t - 1} + X _ {t} + X _ {t + 1}\right)
$$

and compare the numerical values of the spectral densities of $\{ X _ { t } \}$ and $\{ Y _ { t } \}$ at frequency $\omega = 2 \pi / 3$ radians per unit time. What effect would you expect the filter to have on the oscillations of $\{ X _ { t } \} \ 2$

(e) Open the project X.TSM and use the option Smooth>Moving Ave. to apply the filter of part (d) to the realization generated in part (c). Comment on the result.

4.9 The spectral density of a real-valued time series $\{ X _ { t } \}$ is defined on $[ 0 , \pi ]$ by

$$
f (\lambda) = \left\{ \begin{array}{l l} 1 0 0, & \text {i f} \pi / 6 - 0. 0 1 <   \lambda <   \pi / 6 + 0. 0 1, \\ 0, & \text {o t h e r w i s e}, \end{array} \right.
$$

and on $[ - \pi , 0 ]$ by $f ( \lambda ) = f ( - \lambda )$ .

(a) Evaluate the ACVF of $\{ X _ { t } \}$ at lags 0 and 1.   
(b) Find the spectral density of the process $\{ Y _ { t } \}$ defined by

$$
Y _ {t} := \nabla_ {1 2} X _ {t} = X _ {t} - X _ {t - 1 2}.
$$

(c) What is the variance of $Y _ { t }$ ?   
(d) Sketch the power transfer function of the filter $\nabla _ { 1 2 }$ and use the sketch to explain the effect of the filter on sinusoids with frequencies (i) near zero and (ii) near $\pi / 6$ .

4.10 Suppose that $\{ X _ { t } \}$ is the noncausal and noninvertible ARMA(1,1) process satisfying

$$
X _ {t} - \phi X _ {t - 1} = Z _ {t} + \theta Z _ {t - 1}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right),
$$

where $| \phi | > 1$ and $| \theta | > 1$ . Define $\begin{array} { r } { \tilde { \phi } ( B ) = 1 - \frac { 1 } { \phi } B } \end{array}$ and $\begin{array} { r } { \tilde { \theta } ( B ) = 1 + \frac { 1 } { \theta } B } \end{array}$ and let $\{ W _ { t } \}$ be the process given by

$$
W _ {t} := \tilde {\theta} ^ {- 1} (B) \tilde {\phi} (B) X _ {t}.
$$

(a) Show that $\{ W _ { t } \}$ has a constant spectral density function.   
(b) Conclude that $\{ W _ { t } \} \sim \mathrm { W N } \big ( 0 , \sigma _ { w } ^ { 2 } \big )$ . Give an explicit formula for $\sigma _ { w } ^ { 2 }$ in terms of $\phi , \theta$ , and $\sigma ^ { 2 }$ .   
(c) Deduce that $\tilde { \phi } ( B ) X _ { t } ~ = ~ \tilde { \theta } ( B ) W _ { t }$ , so that $\{ X _ { t } \}$ is a causal and invertible ARMA(1,1) process relative to the white noise sequence $\{ W _ { t } \}$ .

# 5

# Modeling and Forecasting with ARMA Processes

5.1 Preliminary Estimation   
5.2 Maximum Likelihood Estimation   
5.3 Diagnostic Checking   
5.4 Forecasting   
5.5 Order Selection

The determination of an appropriate $\mathbf { A R M A } ( p , q )$ model to represent an observed stationary time series involves a number of interrelated problems. These include the choice of $p$ and $q$ (order selection) and estimation of the mean, the coefficients $\{ \phi _ { i } , i ~ = ~ 1 , \ldots , p \}$ , $\{ \theta _ { i } , i ~ = ~ 1 , \ldots , q \}$ , and the white noise variance $\sigma ^ { 2 }$ . Final selection of the model depends on a variety of goodness of fit tests, although it can be systematized to a large degree by use of criteria such as minimization of the AICC statistic as discussed in Section 5.5. (A useful option in the program ITSM is Model>Estimation>Autofit, which automatically minimizes the AICC statistic over all $\mathbf { A R M A } ( p , q )$ processes with $p$ and $q$ in a specified range.)

This chapter is primarily devoted to the problem of estimating the parameters $\boldsymbol { \phi } = ( \phi _ { i } , \ldots , \phi _ { p } )$ , $\pmb { \theta } = ( \theta _ { i } , \ldots , \theta _ { q } )$ , and $\sigma ^ { 2 }$ when $p$ and $q$ are assumed to be known, but the crucial issue of order selection is also considered. It will be assumed throughout (unless the mean is believed a priori to be zero) that the data have been “meancorrected” by subtraction of the sample mean, so that it is appropriate to fit a zero-mean ARMA model to the adjusted data $x _ { 1 } , \ldots , x _ { n }$ . If the model fitted to the mean-corrected data is

$$
\phi (B) X _ {t} = \theta (B) Z _ {t}, \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right),
$$

then the corresponding model for the original stationary series $\{ Y _ { t } \}$ is found on replacing $X _ { t }$ for each $t$ by $Y _ { t } - \overline { { y } }$ , where $\begin{array} { r } { \overline { { y } } = n ^ { - 1 } \sum _ { j = 1 } ^ { n } y _ { j } } \end{array}$ { } is the sample mean of the original data, treated as a fixed constant.

When $p$ and $q$ are known, good estimators of $\phi$ and $\pmb \theta$ can be found by imagining the data to be observations of a stationary Gaussian time series and maximizing the likelihood with respect to the $p \ + \ q \ + \ 1$ parameters $\phi _ { 1 } , \ldots , \phi _ { p } , \theta _ { 1 } , \ldots , \theta _ { q }$

and $\sigma ^ { 2 }$ . The estimators obtained by this procedure are known as maximum likelihood (or maximum Gaussian likelihood) estimators. Maximum likelihood estimation is discussed in Section 5.2 and can be carried out in practice using the ITSM option Model>Estimation>Max likelihood, after first specifying a preliminary model to initialize the maximization algorithm. Maximization of the likelihood and selection of the minimum AICC model over a specified range of $p$ and $q$ values can also be carried out using the option Model $>$ Estimation>Autofit.

The maximization is nonlinear in the sense that the function to be maximized is not a quadratic function of the unknown parameters, so the estimators cannot be found by solving a system of linear equations. They are found instead by searching numerically for the maximum of the likelihood surface. The algorithm used in ITSM requires the specification of initial parameter values with which to begin the search. The closer the preliminary estimates are to the maximum likelihood estimates, the faster the search will generally be.

To provide these initial values, a number of preliminary estimation algorithms are available in the option Model $>$ Estimation>Preliminary of ITSM. They are described in Section 5.1. For pure autoregressive models the choice is between Yule-Walker and Burg estimation, while for models with $q > 0$ it is between the innovations and Hannan–Rissanen algorithms. It is also possible to begin the search with an arbitrary causal ARMA model by using the option Model $>$ Specify and entering the desired parameter values. The initial values are chosen automatically in the option Model>Estimation>Autofit.

Calculation of the exact Gaussian likelihood for an ARMA model (and in fact for any second-order model) is greatly simplified by use of the innovations algorithm. In Section 5.2 we take advantage of this simplification in discussing maximum likelihood estimation and consider also the construction of confidence intervals for the estimated coefficients.

Section 5.3 deals with goodness of fit tests for the chosen model and Section 5.4 with the use of the fitted model for forecasting. In Section 5.5 we discuss the theoretical basis for some of the criteria used for order selection.

For an overview of the general strategy for model-fitting see Section 6.2.

# 5.1 Preliminary Estimation

In this section we shall consider four techniques for preliminary estimation of the parameters $\phi = ( \phi _ { 1 } , \ldots , \phi _ { p } ) ^ { \prime }$ , $\pmb { \theta } = ( \theta _ { 1 } , \ldots , \phi _ { p } ) ^ { \prime }$ , and $\sigma ^ { 2 }$ from observations $x _ { 1 } , . . . . , x _ { n }$ of the causal ARMA $( p , q )$ process defined by

$$
\phi (B) X _ {t} = \theta (B) Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma^ {2}\right). \tag {5.1.1}
$$

The Yule–Walker and Burg procedures apply to the fitting of pure autoregressive models. (Although the former can be adapted to models with $q > 0$ , its performance is less efficient than when $q = 0$ .) The innovation and Hannan–Rissanen algorithms are used in ITSM to provide preliminary estimates of the ARMA parameters when $q > 0$ .

For pure autoregressive models Burg’s algorithm usually gives higher likelihoods than the Yule–Walker equations. For pure moving-average models the innovations algorithm frequently gives slightly higher likelihoods than the Hannan–Rissanen algorithm (we use only the first two steps of the latter for preliminary estimation). For mixed models (i.e., those with $p > 0$ and $q > 0$ ) the Hannan–Rissanen algorithm is usually more successful in finding causal models (which are required for initialization of the likelihood maximization).

# 5.1.1 Yule–Walker Estimation

For a pure autoregressive model the moving-average polynomial $\theta ( z )$ is identically 1, and the causality assumption in (5.1.1) allows us to write $X _ { t }$ in the form

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j}, \tag {5.1.2}
$$

where, from Section 3.1, $\begin{array} { r } { \psi ( z ) = \sum _ { j = 0 } ^ { \infty } \psi _ { j } z ^ { j } = 1 / \phi ( z ) , } \end{array}$ . Multiplying each side of (5.1.1) by $X _ { t - j }$ $\mathbf { \Phi } _ { - j } , j = 0 , 1 , 2 , \ldots , p$ , taking expectations, and using (5.1.2) to evaluate the righthand side of the first equation, we obtain the Yule–Walker equations

$$
\Gamma_ {p} \phi = \gamma_ {p} \tag {5.1.3}
$$

and

$$
\sigma^ {2} = \gamma (0) - \phi^ {\prime} \gamma_ {p}, \tag {5.1.4}
$$

where $\Gamma _ { p }$ is the covariance matrix $[ \gamma ( i - j ) ] _ { i , j = 1 } ^ { p }$ and $\gamma _ { p } = ( \gamma ( 1 ) , \dots , \gamma ( p ) ) ^ { \prime }$ . These equations can be used to determine $\gamma ( 0 ) , \ldots , \gamma ( p )$ from $\sigma ^ { 2 }$ and $\phi$ .

On the other hand, if we replace the covariances $\gamma ( j ) , j = 0 , \ldots , p$ , appearing in (5.1.3) and (5.1.4) by the corresponding sample covariances $\hat { \gamma } ( j )$ , we obtain a set of equations for the so-called Yule–Walker estimators $\hat { \phi }$ and $\hat { \sigma } ^ { 2 }$ of $\phi$ and $\sigma ^ { 2 }$ , namely,

$$
\hat {\Gamma} _ {p} \hat {\phi} = \hat {\gamma} _ {p} \tag {5.1.5}
$$

and

$$
\hat {\sigma} ^ {2} = \hat {\gamma} (0) - \hat {\phi} ^ {\prime} \hat {\gamma} _ {p}, \tag {5.1.6}
$$

where $\hat { \Gamma } _ { p } = \big [ \hat { \gamma } ( i - j ) \big ] _ { i , j = 1 } ^ { p }$ and $\hat { \gamma } _ { p } = \left( \hat { \gamma } ( 1 ) , \ldots , \hat { \gamma } ( p ) \right) ^ { \prime }$

If $\hat { \gamma } ( 0 ) > 0$ , then $\hat { \Gamma } _ { m }$ is nonsingular for every $m = 1 , 2 , \ldots$ (see Brockwell and Davis (1991), Problem 7.11), so we can rewrite equations (5.1.5) and (5.1.6) in the following form:

# Sample Yule–Walker Equations:

$$
\hat {\phi} = \left(\hat {\phi} _ {1}, \dots , \hat {\phi} _ {p}\right) ^ {\prime} = \hat {R} _ {p} ^ {- 1} \hat {\rho} _ {p} \tag {5.1.7}
$$

and

$$
\hat {\sigma} ^ {2} = \hat {\gamma} (0) \left[ 1 - \hat {\rho} _ {p} ^ {\prime} \hat {R} _ {p} ^ {- 1} \hat {\rho} _ {p} \right], \tag {5.1.8}
$$

where $\begin{array} { r } { \hat { \pmb { \rho } } _ { p } = \left( \hat { \rho } ( 1 ) , \ldots , \hat { \rho } ( p ) \right) ^ { \prime } = \hat { \gamma } _ { p } / \hat { \gamma } ( 0 ) . } \end{array}$

With $\hat { \phi }$ as defined by (5.1.7), it can be shown that $1 - \hat { \phi } _ { 1 } z - \cdot \cdot \cdot - \hat { \phi } _ { p } z ^ { p } \neq 0$ for $| z | \leq 1$ (see Brockwell and Davis (1991), Problem 8.3). Hence the fitted model

$$
X _ {t} - \hat {\phi} _ {1} X _ {t - 1} - \dots - \hat {\phi} _ {p} X _ {t - p} = Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \hat {\sigma} ^ {2}\right)
$$

is causal. The autocovariances $\gamma _ { \scriptscriptstyle F } ( h ) , h = 0 , \ldots , p$ , of the fitted model therefore satisfy the $p + 1$ linear equations

$$
\gamma_ {F} (h) - \hat {\phi} _ {1} \gamma_ {F} (h - 1) - \dots - \hat {\phi} _ {p} \gamma_ {F} (h - p) = \left\{ \begin{array}{l l} 0, & h = 1, \ldots , p, \\ \hat {\sigma} ^ {2}, & h = 0. \end{array} \right.
$$

However, from (5.1.5) and (5.1.6) we see that the solution of these equations is $\gamma _ { F } ( h ) =$ $\hat { \gamma } ( h ) , h = 0 , \ldots , p$ , so that the autocovariances of the fitted model at lags $0 , 1 , \ldots , p$ coincide with the corresponding sample autocovariances.

The argument of the preceding paragraph shows that for every nonsingular covariance matrix of the form $\Gamma _ { p + 1 } = [ \gamma ( i - j ) ] _ { i , j = 1 } ^ { p + 1 }$ there is an $\operatorname { A R } ( p )$ process whose autocovariances at lags $0 , \ldots , p$ are $\gamma ( 0 ) , \ldots , \gamma ( p )$ . (The required coefficients and white noise variance are found from (5.1.7) and (5.1.8) on replacing $\hat { \rho } ( j )$ by $\gamma ( j ) / \gamma ( 0 )$ , $j = 0 , \ldots , p$ , and $\hat { \gamma } ( 0 )$ by $\gamma ( 0 )$ .) There may not, however, be an $\mathbf { M A } ( p )$ process with this property. For example, if $\gamma ( 0 ) = 1$ and $\gamma ( 1 ) = \gamma ( - 1 ) = \beta$ , the matrix $\Gamma _ { 2 }$ is a nonsingular covariance matrix for all $\beta \in ( - 1 , 1 )$ . Consequently, there is an AR(1) process with autocovariances 1 and $\beta$ at lags 0 and 1 for all $\beta \in ( - 1 , 1 )$ . However, there is an MA(1) process with autocovariances 1 and $\beta$ at lags 0 and 1 if and only if $\begin{array} { r } { | \beta | \le { \frac { 1 } { 2 } } } \end{array}$ . (See Example 2.1.1).

It is often the case that moment estimators, i.e., estimators that (like $\hat { \phi }$ ) are obtained by equating theoretical and sample moments, have much higher variances than estimators obtained by alternative methods such as maximum likelihood. However, the Yule–Walker estimators of the coefficients $\phi _ { 1 } , . . . , \phi _ { p }$ of an $\operatorname { A R } ( p )$ process have approximately the same distribution for large samples as the corresponding maximum likelihood estimators. For a precise statement of this result see Brockwell and Davis (1991), Section 8.10. For our purposes it suffices to note the following:

# Large-Sample Distribution of Yule–Walker Estimators:

For a large sample from an $\operatorname { A R } ( p )$ process,

$$
\hat {\phi} \approx N \left(\phi , n ^ {- 1} \sigma^ {2} \Gamma_ {p} ^ {- 1}\right).
$$

If we replace $\sigma ^ { 2 }$ and $\Gamma _ { p }$ by their estimates $\hat { \sigma } ^ { 2 }$ and $\hat { \Gamma } _ { p }$ , we can use this result to find large-sample confidence regions for $\phi$ and each of its components as in (5.1.12) and (5.1.13) below.

# Order Selection

In practice we do not know the true order of the model generating the data. In fact, it will usually be the case that there is no true AR model, in which case our goal is simply to find one that represents the data optimally in some sense. Two useful techniques for selecting an appropriate AR model are given below. The second is more systematic and extends beyond the narrow class of pure autoregressive models.

Some guidance in the choice of order is provided by a large-sample result (see Brockwell and Davis (1991), Section 8.10), which states that if $\{ X _ { t } \}$ is the causal $\operatorname { A R } ( p )$ process defined by (5.1.1) with $\{ Z _ { t } \} \sim \operatorname { i i d } ( 0 , \sigma ^ { 2 } )$ and if we fit a model with order $m > p$ using the Yule–Walker equations, i.e., if we fit a model with coefficient vector

$$
\hat {\phi} _ {m} = \hat {R} _ {m} ^ {- 1} \hat {\rho} _ {m}, \quad m > p,
$$

then the last component, $\hat { \phi } _ { m m }$ , of the vector $\hat { \phi } _ { m }$ is approximately normally distributed with mean 0 and variance $1 / n$ . Notice that $\hat { \phi } _ { m m }$ is exactly the sample partial autocorrelation at lag m as defined in Section 3.2.3.

Now, we already know from Example 3.2.6 that for an $\operatorname { A R } ( p )$ , process the partial autocorrelations $\phi _ { m m }$ , $m \ > \ p$ , are zero. By the result of the previous paragraph,

if an $\operatorname { A R } ( p )$ model is appropriate for the data, then the values $\hat { \phi } _ { k k }$ , $k > p$ , should be compatible with observations from the distribution $\mathrm { N } ( 0 , 1 / n )$ . In particular, for $k > p$ , $\hat { \phi } _ { k k }$ will fall between the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 }$ with probability close to 0.95. This suggests using as a preliminary estimator of $p$ the smallest value $m$ such that $\left| \hat { \phi } _ { k k } \right| < \overline { { 1 . 9 6 n ^ { - 1 / 2 } } }$ for $k > m$ .

The program ITSM plots the sample PACF $\bigl \{ \hat { \phi } _ { m m } , m = 1 , 2 , \ldots \bigr \}$ together with the bounds $\pm 1 . 9 6 / \sqrt { n }$ . From this graph it is easy to read off the preliminary estimator of $p$ defined above.

• A more systematic approach to order selection is to find the values of $p$ and $\phi _ { p }$ that minimize the AICC statistic (see Section 5.5.2 below)

$$
\mathrm {A I C C} = - 2 \ln L (\phi_ {p}, S (\phi_ {p}) / n) + 2 (p + 1) n / (n - p - 2),
$$

where $L$ is the Gaussian likelihood defined in (5.2.9) and $S$ is defined in (5.2.11). The Preliminary Estimation dialog box of ITSM (opened by pressing the blue PRE button) allows you to search for the minimum AICC Yule–Walker (or Burg) models by checking Find AR model with min AICC. This causes the program to fit autoregressions of orders $0 , 1 , \ldots , 2 7$ and to return the model with smallest AICC value.

# Definition 5.1.1

# The fitted Yule–Walker AR(m) model is

$$
X _ {t} - \hat {\phi} _ {m 1} X _ {t - 1} - \dots - \hat {\phi} _ {m m} X _ {t - m} = Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \hat {\nu} _ {m}\right), \tag {5.1.9}
$$

where

$$
\hat {\phi} _ {m} = \left(\hat {\phi} _ {m 1}, \dots , \hat {\phi} _ {m m}\right) ^ {\prime} = \hat {R} _ {m} ^ {- 1} \hat {\rho} _ {m} \tag {5.1.10}
$$

and

$$
\hat {v} _ {m} = \hat {\gamma} (0) \left[ 1 - \hat {\rho} _ {m} ^ {\prime} \hat {R} _ {m} ^ {- 1} \hat {\rho} _ {m} \right]. \tag {5.1.11}
$$

For both approaches to order selection we need to fit AR models of gradually increasing order to our given data. The problem of solving the Yule–Walker equations with gradually increasing orders has already been encountered in a slightly different context in Section 2.5.3, where we derived a recursive scheme for solving the equations (5.1.3) and (5.1.4) with $p$ successively taking the values 1, 2, . . . . Here we can use exactly the same scheme (the Durbin–Levinson algorithm) to solve the Yule– Walker equations (5.1.5) and (5.1.6), the only difference being that the covariances in (5.1.3) and (5.1.4) are replaced by their sample counterparts. This is the algorithm used by ITSM to perform the necessary calculations.

# Confidence Regions for the Coefficients

Under the assumption that the order $p$ of the fitted model is the correct value, we can use the asymptotic distribution of $\hat { \phi } _ { p }$ to derive approximate large-sample confidence regions for the true coefficient vector $\phi _ { p }$ and for its individual components $\phi _ { p j }$ . Thus, if $\chi _ { 1 - \alpha } ^ { 2 } ( p )$ denotes the $( 1 - \alpha )$ quantile of the chi-squared distribution with $p$ degrees of freedom, then for large sample-size $n$ the region

$$
\left\{\phi \in \mathbf {R} ^ {p}: \left(\hat {\phi} _ {p} - \phi\right) ^ {\prime} \hat {\Gamma} _ {p} \left(\hat {\phi} _ {p} - \phi\right) \leq n ^ {- 1} \hat {v} _ {p} \chi_ {1 - \alpha} ^ {2} (p) \right\} \tag {5.1.12}
$$

contains $\phi _ { p }$ with probability close to $( 1 - \alpha )$ . (This follows from Problem A.7 and the fact that $\sqrt { n } \big ( \hat { \phi } _ { p } - \phi _ { p } \big )$ is approximately normally distributed with mean 0 and covariance matrix $\hat { \nu } _ { p } \hat { \Gamma } _ { p } ^ { - 1 }$ .) Similarly, if $\Phi _ { 1 - \alpha }$ denotes the $( 1 - \alpha )$ quantile of the standard normal distribution and $\hat { \nu } _ { j j }$ is the jth diagonal element of $\hat { \nu } _ { p } \hat { \Gamma } _ { p } ^ { - 1 }$ , then for large $n$ the interval bounded by

$$
\hat {\phi} _ {p j} \pm \Phi_ {1 - \alpha / 2} n ^ {- 1 / 2} \hat {v} _ {j j} ^ {1 / 2} \tag {5.1.13}
$$

contains $\phi _ { p j }$ with probability close to $( 1 - \alpha )$ .

# Example 5.1.1 The Dow Jones Utilities Index, Aug. 28–Dec. 18, 1972; DOWJ.TSM

The very slowly decaying positive sample ACF of the time series contained in the file DOWJ.TSM this time series suggests differencing at lag 1 before attempting to fit a stationary model. One application of the operator $( 1 - B )$ produces a new series $\{ Y _ { t } \}$ with no obvious deviations from stationarity. We shall therefore try fitting an AR process to this new series

$$
Y _ {t} = D _ {t} - D _ {t - 1}
$$

using the Yule–Walker equations. There are 77 values of $Y _ { t }$ , which we shall denote by $Y _ { 1 } , \ldots , Y _ { 7 7 }$ . (We ignore the unequal spacing of the original data resulting from the five-day working week.) The sample autocovariances of the series $y _ { 1 } , \ldots , y _ { 7 7 }$ are $\hat { \gamma } ( 0 ) = 0 . 1 7 9 9 2$ , $\hat { \gamma } ( 1 ) = 0 . 0 7 5 9 0$ , $\hat { \gamma } ( 2 ) = 0 . 0 4 8 8 5$ , etc.

Applying the Durbin–Levinson algorithm to fit successively higher-order autoregressive processes to the data, we obtain

$$
\hat {\phi} _ {1 1} = \hat {\rho} (1) = 0. 4 2 1 9,
$$

$$
\hat {v} _ {1} = \hat {\gamma} (0) [ 1 - \hat {\rho} ^ {2} (1) ] = 0. 1 4 7 9,
$$

$$
\hat {\phi} _ {2 2} = \left[ \hat {\gamma} (2) - \hat {\phi} _ {1 1} \hat {\gamma} (1) \right] / \hat {\nu} _ {1} = 0. 1 1 3 8,
$$

$$
\hat {\phi} _ {2 1} = \hat {\phi} _ {1 1} - \hat {\phi} _ {1 1} \hat {\phi} _ {2 2} = 0. 3 7 3 9,
$$

$$
\hat {v} _ {2} = \hat {v} _ {1} \left[ 1 - \hat {\phi} _ {2 2} ^ {2} \right] = 0. 1 4 6 0.
$$

The sample ACF and PACF of the data can be displayed by pressing the second yellow button at the top of the ITSM window. They are shown in Figures 5-1 and 5-2, respectively. Also plotted are the bounds $\pm 1 . 9 6 / \sqrt { 7 7 }$ . Since the PACF values at lags greater than 1 all lie between the bounds, the first order-selection criterion described above indicates that we should fit an AR(1) model to the data set $\{ Y _ { t } \}$ . Unless we wish to assume that $\{ Y _ { t } \}$ is a zero-mean process, we should subtract the sample mean from the data before attempting to fit a (zero-mean) AR(1) model. When the blue PRE (preliminary estimation) button at the top of the ITSM window is pressed, you will be given the option of subtracting the mean from the data. In this case (as in most) click Yes to obtain the new series

$$
X _ {t} = Y _ {t} - 0. 1 3 3 6.
$$

You will then see the Preliminary Estimation dialog box. Enter 1 for the AR order, zero for the MA order, select Yule-Walker, and click OK. We have already computed $\hat { \phi } _ { 1 1 }$ and $\hat { \nu } _ { 1 }$ above using the Durbin–Levinson algorithm. The Yule–Walker AR(1) model obtained by ITSM for $\{ X _ { t } \}$ is therefore (not surprisingly)

$$
X _ {t} - 0. 4 2 1 9 X _ {t - 1} = Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \mathrm {W N} (0, 0. 1 4 7 9), \tag {5.1.14}
$$

Figure 5-1 The sample ACF of the differenced series $\{ Y _ { t } \}$ in Example 5.1.1   
Figure 5-2   
![](images/b070bdce13f69ed9c3fa9c9741f3795050ff1de22292d477e3be6cac126624be.jpg)  
The sample PACF of the differenced series $\{ Y _ { t } \}$ in Example 5.1.1

![](images/440f5e4d1ac2829d15ea44338e795f22ace768b3d0a5f1b585d1241ae0b1b6a5.jpg)

and the corresponding model for $\{ Y _ { t } \}$ is

$$
Y _ {t} - 0. 1 3 3 6 - 0. 4 2 1 9 \left(Y _ {t - 1} - 0. 1 3 3 6\right) = Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \mathrm {W N} (0, 0. 1 4 7 9). \tag {5.1.15}
$$

Assuming that our observed data really are generated by an AR process with $p = 1$ , (5.1.13) gives us approximate $9 5 \%$ confidence bounds for the autoregressive coefficient $\phi$ ,

$$
0. 4 2 1 9 \pm \frac {(1 . 9 6) (0 . 1 4 7 9) ^ {1 / 2}}{(0 . 1 7 9 9 2) ^ {1 / 2} \sqrt {7 7}} = (0. 2 1 9 4, 0. 6 2 4 4).
$$

Besides estimating the autoregressive coefficients, ITSM computes and prints out the ratio of each coefficient to 1.96 times its estimated standard deviation. From these numbers large-sample $9 5 \%$ confidence intervals for each of the coefficients are easily

obtained. In this particular example there is just one coefficient estimate, $\hat { \phi } _ { 1 } = 0 . 4 2 1 9$ , with ratio of coefficient to $1 . 9 6 \times$ standard error equal to 2.0832. Hence the required $9 5 \%$ confidence bounds are $0 . 4 2 1 9 \pm 0 . 4 2 1 9 / 2 . 0 8 3 2 = ( 0 . 2 1 9 4 , 0 . 6 2 4 4 )$ , as found above.

A useful technique for preliminary autoregressive estimation that incorporates automatic model selection (i.e., choice of $p$ ) is to minimize the AICC [see equation (5.5.4)] over all fitted autoregressions of orders 0 through 27. This is achieved by selecting both Yule-Walker and Find AR model with min AICC in the Preliminary Estimation dialog box. (The MA order must be set to zero, but the AR order setting is immaterial.) Click OK, and the program will search through all the Yule–Walker $\operatorname { A R } ( p )$ models, $p = 0$ , 1, . . . , 27, selecting the one with smallest AICC value. The minimum-AICC Yule–Walker AR model turns out to be the one defined by (5.1.14) with $p = 1$ and AICC value 74.541.

![](images/ac812b254e5ba0bad652781f9a899247bc99123317db4f40fd34d68504404dca.jpg)

Yule–Walker Estimation with $q > 0$ ; Moment Estimators

The Yule–Walker estimates for the parameters in an $\operatorname { A R } ( p )$ model are examples of moment estimators: The autocovariances at lags $0 , 1 , \ldots , p$ are replaced by the corresponding sample estimates in the Yule–Walker equations (5.1.3), which are then solved for the parameters $\phi ~ = ~ ( \phi _ { 1 } , \ldots , \phi _ { p } ) ^ { \prime }$ and $\sigma ^ { 2 }$ . The analogous procedure for $\mathbf { A R M A } ( \boldsymbol { p } , \boldsymbol { q } )$ models with $q \ > \ 0$ is easily formulated, but the corresponding equations are nonlinear in the unknown coefficients, leading to possible nonexistence and nonuniqueness of solutions for the required estimators.

From (3.2.5), the equations to be solved for $\phi _ { 1 } , \ldots , \phi _ { p } , \theta _ { 1 } , \ldots , \theta _ { q }$ and $\sigma ^ { 2 }$ are

$$
\hat {\gamma} (k) - \phi_ {1} \hat {\gamma} (k - 1) - \dots - \phi_ {p} \hat {\gamma} (k - p) = \sigma^ {2} \sum_ {j = k} ^ {q} \theta_ {j} \psi_ {j - k}, \quad 0 \leq k \leq p + q, \tag {5.1.16}
$$

where $\psi _ { j }$ must first be expressed in terms of $\phi$ and $\pmb \theta$ using the identity $\psi ( z ) =$ $\theta ( z ) / \phi ( z )$ $( \theta _ { 0 } : = 1$ and $\theta _ { j } = \psi _ { j } = 0$ for $j < 0$ ).

Example 5.1.2 For the MA(1) model the equation (5.1.16) are equivalent to

$$
\hat {\gamma} (0) = \hat {\sigma} ^ {2} \left(1 + \hat {\theta} _ {1} ^ {2}\right), \tag {5.1.17}
$$

$$
\hat {\rho} (1) = \frac {\hat {\theta} _ {1}}{1 + \hat {\theta} _ {1} ^ {2}}. \tag {5.1.18}
$$

If $\left| \hat { \rho } ( 1 ) \right| > 0 . 5$ , there is no real solution, so we define $ { \hat { \theta } _ { 1 } } =  { \hat { \rho } } ( 1 ) /  { \left| \hat { \rho } ( 1 ) \right| }$ . If $\left| \hat { \rho } ( 1 ) \right| \leq$ 0.5, then the solution of (5.1.17)–(5.1.18) (with $| { \hat { \theta } } | \leq 1$ ) is

$$
\hat {\theta} _ {1} = \left(1 - \left(1 - 4 \hat {\rho} ^ {2} (1)\right) ^ {1 / 2}\right) / \left(2 \hat {\rho} (1)\right),
$$

$$
\hat {\sigma} ^ {2} = \hat {\gamma} (0) / \left(1 + \hat {\theta} _ {1} ^ {2}\right).
$$

For the overshort data of Example 3.2.8, $\hat { \rho } ( 1 ) = - 0 . 5 0 3 5$ and $\hat { \gamma } ( 0 ) = 3 4 1 6$ , so the fitted MA(1) model has parameters $\widehat { \theta } _ { 1 } = - 1 . 0$ and $\hat { \sigma } ^ { 2 } = 1 7 0 8$ .

![](images/cafffd3a45afde30e49d8f1059afd5917c24feeace50f553b1b8743f6c5d1cf3.jpg)

Relative Efficiency of Estimators

The performance of two competing estimators is often measured by computing their asymptotic relative efficiency. In a general statistics estimation problem, suppose $\hat { \theta } _ { n } ^ { ( 1 ) }$ and ${ \hat { \theta } } _ { n } ^ { ( 2 ) }$ are two estimates of the parameter $\theta$ in the parameter space $\Theta$ based on the observations $X _ { 1 } , \ldots , X _ { n }$ . If $\hat { \theta } _ { n } ^ { ( i ) }$ is approximately $\mathrm { N } \big ( \theta , \sigma _ { i } ^ { 2 } ( \theta ) \big )$ for large $n , i = 1 , 2$ , then the asymptotic efficiency of $\hat { \theta } _ { n } ^ { ( 1 ) }$ relative to ${ \hat { \theta } } _ { n } ^ { ( 2 ) }$ is defined to be

$$
e \left(\theta , \hat {\theta} ^ {(1)}, \hat {\theta} ^ {(2)}\right) = \frac {\sigma_ {2} ^ {2} (\theta)}{\sigma_ {1} ^ {2} (\theta)}.
$$

If $e \big ( \theta , \hat { \theta } ^ { ( 1 ) } , \hat { \theta } ^ { ( 2 ) } \big ) \leq 1$ for all $\theta \in \Theta$ , then we say that ${ \hat { \theta } } _ { n } ^ { ( 2 ) }$ is a more efficient estimator of $\theta$ than $\hat { \theta } _ { n } ^ { ( 1 ) }$ (strictly more efficient if in addition, $e \big ( \theta , \hat { \theta } ^ { ( 1 ) } , \hat { \theta } ^ { ( 2 ) } \big ) \ : < \ : 1$ for some $\theta \in$ $\Theta$ ). For the MA(1) process the moment estimator $\theta _ { n } ^ { ( 1 ) }$ discussed in Example 5.1.2 is approximately $\mathrm { N } \big ( \theta _ { 1 } , \sigma _ { 1 } ^ { 2 } ( \theta _ { 1 } ) / n \big )$ with

$$
\sigma_ {1} ^ {2} \left(\theta_ {1}\right) = \left(1 + \theta_ {1} ^ {2} + 4 \theta_ {1} ^ {4} + \theta_ {1} ^ {6} + \theta_ {1} ^ {8}\right) / \left(1 - \theta_ {1} ^ {2}\right) ^ {2}
$$

(see Brockwell and Davis (1991), p. 254). On the other hand, the innovations estimator ${ \hat { \theta } } _ { n } ^ { ( 2 ) }$ discussed in the next section is distributed approximately as $\mathrm { N } \big ( \theta _ { 1 } , n ^ { - 1 } \big )$ . Thus, $e \big ( \theta _ { 1 } , \hat { \theta } ^ { ( 1 ) } , \hat { \theta } ^ { ( 2 ) } \big ) = \sigma _ { 1 } ^ { - 2 } ( \theta _ { 1 } ) \leq 1$ for all $| \theta _ { 1 } | < 1$ , with strict inequality when $\theta \neq 1$ . In particular,

$$
e \left(\theta_ {1}, \hat {\theta} ^ {(1)}, \hat {\theta} ^ {(2)}\right) = \left\{ \begin{array}{l l} 0. 8 2, & \theta_ {1} = 0. 2 5, \\ 0. 3 7, & \theta_ {1} = 0. 5 0, \\ 0. 0 6, & \theta_ {1} = 0. 7 5, \end{array} \right.
$$

demonstrating the superiority, at least in terms of asymptotic relative efficiency, of ${ \hat { \theta } } _ { n } ^ { ( 2 ) }$ over $\hat { \theta } _ { n } ^ { ( 1 ) }$ . On the other hand (Section 5.2), the maximum likelihood estimator $\hat { \theta } _ { n } ^ { ( 3 ) }$ of $\theta _ { 1 }$ is approximately $\Nu ( \theta _ { 1 } , ( 1 - \theta _ { 1 } ^ { 2 } ) / n )$ . Hence,

$$
e \left(\theta_ {1}, \hat {\theta} ^ {(2)}, \hat {\theta} ^ {(3)}\right) = \left\{ \begin{array}{l l} 0. 9 4, & \theta_ {1} = 0. 2 5, \\ 0. 7 5, & \theta_ {1} = 0. 5 0, \\ 0. 4 4, & \theta_ {1} = 0. 7 5. \end{array} \right.
$$

While $\hat { \theta } _ { n } ^ { ( 3 ) }$ is more efficient, ${ \hat { \theta } } _ { n } ^ { ( 2 ) }$ has reasonably good efficiency, except when $| \theta _ { 1 } |$ is close to 1, and can serve as initial value for the nonlinear optimization procedure in computing the maximum likelihood estimator.

While the method of moments is an effective procedure for fitting autoregressive models, it does not perform as well for ARMA models with $q > 0$ . From a computational point of view, it requires as much computing time as the more efficient estimators based on either the innovations algorithm or the Hannan–Rissanen procedure and is therefore rarely used except when $q = 0$ .

# 5.1.2 Burg’s Algorithm

The Yule–Walker coefficients $\hat { \phi } _ { p 1 } , \hdots , \hat { \phi } _ { p p }$ are precisely the coefficients of the best linear predictor of $X _ { p + 1 }$ in terms of $\{ X _ { p } , \ldots , X _ { 1 } \}$ under the assumption that the ACF of $\{ X _ { t } \}$ coincides with the sample ACF at lags $1 , \ldots , p$ .

Burg’s algorithm estimates the PACF $\{ \phi _ { 1 1 } , \phi _ { 2 2 } , . . . \}$ by successively minimizing sums of squares of forward and backward one-step prediction errors with respect to the coefficients $\phi _ { i i }$ . Given observations $\{ x _ { 1 } , \ldots , x _ { n } \}$ of a stationary zero-mean time series $\{ X _ { t } \}$ we define $u _ { i } ( t ) , t = i + 1 , . . . , n$ , $0 ~ \leq ~ i ~ < ~ n$ , to be the difference between

$x _ { n + 1 + i - t }$ and the best linear estimate of $x _ { n + 1 + i - t }$ in terms of the preceding i observations. Similarly, we define $\nu _ { i } ( t ) , t = i + 1 , \ldots , n , 0 \leq i < n$ $\nu _ { i } ( t )$ $0 \leq i < n$ , to be the difference between $x _ { n + 1 - t }$ and the best linear estimate of $x _ { n + 1 - t }$ in terms of the subsequent i observations. Then it can be shown (see Problem 5.6) that the forward and backward prediction errors $\{ u _ { i } ( t ) \}$ and $\{ \nu _ { i } ( t ) \}$ satisfy the recursions

$$
u _ {0} (t) = v _ {0} (t) = x _ {n + 1 - t},
$$

$$
u _ {i} (t) = u _ {i - 1} (t - 1) - \phi_ {i i} v _ {i - 1} (t), \tag {5.1.19}
$$

and

$$
v _ {i} (t) = v _ {i - 1} (t) - \phi_ {i i} u _ {i - 1} (t - 1). \tag {5.1.20}
$$

Burg’s estimate $\phi _ { 1 1 } ^ { ( B ) }$ of $\phi _ { 1 1 }$ is found by minimizing

$$
\sigma_ {1} ^ {2} := \frac {1}{2 (n - 1)} \sum_ {t = 2} ^ {n} \left[ u _ {1} ^ {2} (t) + v _ {1} ^ {2} (t) \right]
$$

with respect to $\phi _ { 1 1 }$ . This gives corresponding numerical values for $u _ { 1 } ( t )$ and $\nu _ { 1 } ( t )$ and $\sigma _ { 1 } ^ { 2 }$ that can then be substituted into (5.1.19) and (5.1.20) with $i = 2$ . Then we minimize

$$
\sigma_ {2} ^ {2} := \frac {1}{2 (n - 2)} \sum_ {t = 3} ^ {n} \left[ u _ {2} ^ {2} (t) + v _ {2} ^ {2} (t) \right]
$$

of with respect to $u _ { 2 } ( t ) , \nu _ { 2 } ( t )$ , and $\phi _ { 2 2 }$ $\sigma _ { 2 } ^ { 2 }$ to obtain the Burg estimate 22 . This process can clearly be continued to obtain estimates $\phi _ { 2 2 } ^ { ( B ) }$ of $\phi _ { 2 2 }$ and corresponding values $\phi _ { p p } ^ { ( B ) }$ and corresponding minimum values, $\sigma _ { p } ^ { ( B ) 2 } , p \le n - 1$ . Estimates of the coefficients $\phi _ { p j }$ , $1 \leq j \leq p - 1$ , in the best linear predictor

$$
P _ {p} X _ {p + 1} = \phi_ {p 1} X _ {p} + \dots + \phi_ {p p} X _ {1}
$$

are then found by substituting the estimates $\phi _ { i i } ^ { ( B ) }$ , $i = 1 , \ldots , p$ , for $\phi _ { i i }$ in the recursions (2.5.20)–(2.5.22). The resulting estimates of $\phi _ { p j }$ , $j = 1 , \dotsc , p$ , are the coefficient estimates of the Burg $\operatorname { A R } ( p )$ model for the data $\{ x _ { 1 } , \ldots , x _ { n } \}$ . The Burg estimate of the white noise variance is the minimum value $\sigma _ { p } ^ { ( B ) 2 }$ found in the determination of $\phi _ { p p } ^ { ( B ) }$ . The calculation of the estimates of $\phi _ { p p }$ and $\sigma _ { p } ^ { 2 }$ described above is equivalent (Problem 5.7) to solving the following recursions:

# Burg’s Algorithm:

$$
d (1) = \sum_ {t = 2} ^ {n} (u _ {0} ^ {2} (t - 1) + v _ {0} ^ {2} (t)),
$$

$$
\phi_ {i i} ^ {(B)} = \frac {2}{d (i)} \sum_ {t = i + 1} ^ {n} v _ {i - 1} (t) u _ {i - 1} (t - 1),
$$

$$
d (i + 1) = \left(1 - \phi_ {i i} ^ {(B) 2}\right) d (i) - v _ {i} ^ {2} (i + 1) - u _ {i} ^ {2} (n),
$$

$$
\sigma_ {i} ^ {(B) 2} = \left[ \left(1 - \phi_ {i i} ^ {(B) 2}\right) d (i) \right] / [ 2 (n - i) ].
$$

The large-sample distribution of the estimated coefficients for the Burg estimators of the coefficients of an $\operatorname { A R } ( p )$ process is the same as for the Yule–Walker estimators, namely, $\mathrm { N } \bigl ( \phi , n ^ { - 1 } \sigma ^ { 2 } \Gamma _ { p } ^ { - 1 } \bigr )$ . Approximate large-sample confidence intervals for the coefficients can be found as in Section 5.1.1 by substituting estimated values for $\sigma ^ { 2 }$ and $\Gamma _ { p }$ .

# Example 5.1.3 The Dow Jones Utilities Index

The fitting of AR models using Burg’s algorithm in the program ITSM is completely analogous to the use of the Yule–Walker equations. Applying the same transformations as in Example 5.1.1 to the Dow Jones Utilities Index and selecting Burg instead of Yule-Walker in the Preliminary Estimation dialog box, we obtain the minimum AICC Burg model

$$
X _ {t} - 0. 4 3 7 1 X _ {t - 1} = Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \mathrm {W N} (0, 0. 1 4 2 3), \tag {5.1.21}
$$

with $\mathrm { A I C C } = 7 4 . 4 9 2$ . This is slightly different from the Yule–Walker AR(1) model fitted in Example 5.1.1, and it has a larger likelihood $L$ , i.e., a smaller value of $- 2 \ln L$ (see Section 5.2). Although the two methods give estimators with the same large-sample distributions, for finite sample sizes the Burg model usually has smaller estimated white noise variance and larger Gaussian likelihood. From the ratio of the estimated coefficient to $1 . 9 6 \times$ standard error) displayed by ITSM, we obtain the $95 \%$ confidence bounds for $\phi \colon 0 . 4 3 7 1 \pm 0 . 4 3 7 1 / 2 . 1 6 6 8 = ( 0 . 2 3 5 4 , 0 . 6 3 8 8 )$ .

![](images/7c9e262d6f49c14e34d43f7dac79dd15df2ae0d8d7a3ddfa96d11a242eaf9164.jpg)

# Example 5.1.4 The Lake Data

This series $\{ Y _ { t } , t = 1 , \ldots , 9 8 \}$ has already been studied in Example 1.3.5. In this example we shall consider the problem of fitting an AR process directly to the data without first removing any trend component. A graph of the data was displayed in Figure 1-9. The sample ACF and PACF are shown in Figures 5-3 and 5-4, respectively.

The sample PACF shown in Figure 5-4 strongly suggests fitting an AR(2) model to the mean-corrected data $X _ { t } = Y _ { t } - 9 . 0 0 4 1$ . After clicking on the blue preliminary estimation button of ITSM select Yes to subtract the sample mean from $\{ Y _ { t } \}$ . Then

Figure 5-3   
![](images/a4e87436e2d042cf7b322556a93ebf6fdfb49352f3e3d429a628290da6c7bc98.jpg)  
The sample ACF of the lake data in Example 5.1.4

The sample PACF of the lake data in Example 5.1.4

![](images/810c3d081fd2a2505adbae53f25954a4125b4df9fa62566dadb8448d0cc05245.jpg)  
Figure 5-4

specify 2 for the AR order, 0 for the MA order, and Burg for estimation. Click OK to obtain the model

$$
X _ {t} - 1. 0 4 4 9 X _ {t - 1} + 0. 2 4 5 6 X _ {t - 2} = Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 0. 4 7 0 6),
$$

with AICC value 213.55 and $9 5 \%$ confidence bounds

$$
\phi_ {1}: 1. 0 4 4 9 \pm 1. 0 4 4 9 / 5. 5 2 9 5 = (0. 8 5 5 9, 1. 2 3 3 9),
$$

$$
\phi_ {2}: - 0. 2 4 5 6 \pm 0. 2 4 5 6 / 1. 2 9 9 7 = (- 0. 4 3 4 6, - 0. 0 5 6 6).
$$

Selecting the Yule–Walker method for estimation, we obtain the model

$$
X _ {t} - 1. 0 5 3 8 X _ {t - 1} + 0. 2 6 6 8 X _ {t - 2} = Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 0. 4 9 2 0),
$$

with AICC value 213.57 and $9 5 \%$ confidence bounds

$$
\phi_ {1}: 1. 0 5 3 8 \pm 1. 0 5 3 8 / 5. 5 2 2 7 = (0. 8 6 3 0, 1. 2 4 4 6),
$$

$$
\phi_ {2}: - 0. 2 6 6 8 \pm 0. 2 6 6 8 / 1. 3 9 8 0 = (- 0. 4 5 7 6, - 0. 0 7 6 0).
$$

We notice, as in Example 5.1.3, that the Burg model again has smaller white noise variance and larger Gaussian likelihood than the Yule–Walker model.

If we determine the minimum AICC Yule–Walker and Burg models, we find that they are both of order 2. Thus the order suggested by the sample PACF coincides again with the order obtained by AICC minimization.

![](images/9d9ed54720410a96f0eedf86dc37d8fd10cece01de8cba5154bd7feaab61b6ef.jpg)

# 5.1.3 The Innovations Algorithm

Just as we can fit autoregressive models of orders 1, 2, . . . to the data $\{ x _ { 1 } , \ldots , x _ { n } \}$ by applying the Durbin–Levinson algorithm to the sample autocovariances, we can also fit moving average models

$$
X _ {t} = Z _ {t} + \hat {\theta} _ {m 1} Z _ {t - 1} + \dots + \hat {\theta} _ {m m} Z _ {t - m}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \hat {v} _ {m}\right) \tag {5.1.22}
$$

of orders $m = 1 , 2 , \ldots$ by means of the innovations algorithm (Section 2.5.4). The estimated coefficient vectors $\hat { \pmb { \theta } } _ { m } : = \left( \hat { \theta } _ { m 1 } , \dots , \hat { \theta } _ { m m } \right) ^ { \prime }$ and white noise variances $\hat { \nu } _ { m }$ ,

$m = 1 , 2 , \ldots$ , are specified in the following definition. (The justification for using estimators defined in this way is contained in Remark 1 following the definition.)

# Definition 5.1.2

# The fitted innovations MA(m) model is

$$
X _ {t} = Z _ {t} + \hat {\theta} _ {m 1} Z _ {t - 1} + \dots + \hat {\theta} _ {m m} Z _ {t - m}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, \hat {\nu} _ {m}),
$$

where $\hat { \pmb { \theta } } _ { m }$ and $\hat { \nu } _ { m }$ are obtained from the innovations algorithm with the ACVF replaced by the sample ACVF.

Remark 1. It can be shown (see Brockwell and Davis 1988) that if $\{ X _ { t } \}$ is an invertible $\mathrm { M A } ( q )$ process

$$
X _ {t} = Z _ {t} + \theta_ {1} Z _ {t - 1} + \dots + \theta_ {q} Z _ {t - q}, \quad \{Z _ {t} \} \sim \operatorname {I I D} \left(0, \sigma^ {2}\right),
$$

with $E Z _ { t } ^ { 4 } < \infty$ , and if we define $\theta _ { 0 } = 1$ and $\theta _ { j } = 0$ for $j > q$ , then the innovation estimates have the following large-sample properties. If $n  \infty$ and $m ( n )$ is any sequence of positive integers such that $m ( n ) \to \infty$ but $n ^ { - 1 / 3 } m ( n )  0$ , then for each positive integer $k$ the joint distribution function of

$$
n ^ {1 / 2} \left(\hat {\theta} _ {m 1} - \theta_ {1}, \hat {\theta} _ {m 2} - \theta_ {2}, \dots , \hat {\theta} _ {m k} - \theta_ {k}\right) ^ {\prime}
$$

converges to that of the multivariate normal distribution with mean 0 and covariance matrix $\bar { \boldsymbol { A } } = [ a _ { i j } ] _ { i , j = 1 } ^ { k }$ , where

$$
a _ {i j} = \sum_ {r = 1} ^ {\min  (i, j)} \theta_ {i - r} \theta_ {j - r}. \tag {5.1.23}
$$

This result enables us to find approximate large-sample confidence intervals for the moving-average coefficients from the innovation estimates as described in the examples below. Moreover, the estimator $\hat { \nu } _ { m }$ is consistent for $\sigma ^ { 2 }$ in the sense that for every $\mathsf { \bar { \epsilon } } > 0 , P \bigl ( \bigl | \hat { \nu } _ { m } - \sigma ^ { 2 } \bigr | > \epsilon \bigr ) \to 0$ as $m  \infty$ . -

Remark 2. Although the recursive fitting of moving-average models using the innovations algorithm is closely analogous to the recursive fitting of autoregressive models using the Durbin–Levinson algorithm, there is one important distinction. For an $\operatorname { A R } ( p )$ process the Yule–Walker and Burg estimators $\hat { \phi } _ { p }$ are consistent estimators of $( \phi _ { 1 } , \ldots , \phi _ { p } ) ^ { \prime }$ as the sample size $n \to \infty$ . However, for an $\mathrm { M A } ( q )$ process the estimator $\hat { \pmb { \theta } } _ { q } = ( \theta _ { q 1 } , \ldots , \theta _ { q q } ) ^ { \prime }$ is not consistent for $( \theta _ { 1 } , \ldots , \theta _ { q } ) ^ { \prime }$ . For consistency it is necessary to use the estimators $( \theta _ { m 1 } , \ldots , \theta _ { m q } ) ^ { \prime }$ with $m ( n )$ satisfying the conditions of Remark 1. The choice of $m$ for any fixed sample size can be made by increasing $m$ until the vector $( \theta _ { m 1 } , \ldots , \theta _ { m q } ) ^ { \prime }$ stabilizes. It is found in practice that there is a large range of values of $m$ for which the fluctuations in $\theta _ { m j }$ are small compared with the estimated asymptotic standard deviation n−1/2	j−1i=0 θˆ2mi1/2 a $n ^ { - 1 / 2 } \bigl ( \sum _ { i = 0 } ^ { j - 1 } \hat { \theta } _ { m i } ^ { 2 } \bigr ) ^ { 1 / 2 }$ s found from (5.1.23) when the coefficients $\theta _ { j }$ are replaced by their estimated values $\hat { \theta } _ { m j }$ . -

# Order Selection

Three useful techniques for selecting an appropriate MA model are given below. The third is more systematic and extends beyond the narrow class of pure moving-average models.

We know from Section 3.2.2 that for an $\mathrm { M A } ( q )$ process the autocorrelations $\rho ( m )$ , $m > q$ , are zero. Moreover, we know from Bartlett’s formula (Section 2.4) that the sample autocorrelation $\hat { \rho } ( m )$ , $m > q$ , is approximately normally distributed with mean $\rho ( m ) = 0$ and variance $n ^ { - 1 } \big [ 1 + 2 \rho ^ { 2 } ( 1 ) + \cdot \cdot \cdot + 2 \rho ^ { 2 } ( q ) \big ]$ . This result enables us to use the graph of $\hat { \rho } ( m )$ , $m = 1 , 2 , \ldots$ , both to decide whether or not a given data set can be plausibly modeled by a moving-average process and also to obtain a preliminary estimate of the order $q$ as the smallest value of $m$ such that ${ \hat { \rho } } ( k )$ is not significantly different from zero for all $k > m$ . For practical purposes “significantly different from zero” is often interpreted as “larger than $1 . 9 6 / { \sqrt { n } }$ in absolute value” (cf. the corresponding approach to order selection for AR models based on the sample PACF and described in Section 5.1.1).

If in addition to examining $\hat { \rho } ( m ) , m = 1 , 2 , \ldots$ , we examine the coefficient vectors $\hat { \theta } _ { m }$ , $m = 1 , 2 , \ldots$ , we are able not only to assess the appropriateness of a movingaverage model and estimate its order $q$ , but at the same time to obtain preliminary estimates $\hat { \theta } _ { m 1 } , \ldots , \hat { \theta } _ { m q }$ $\hat { \theta } _ { m q }$ of the coefficients. By inspecting the estimated coefficients $\hat { \theta } _ { m 1 } , \ldots , \hat { \theta } _ { m m }$ for $m = 1 , 2 , \ldots$ and the ratio of each coefficient estimate $\hat { \theta } _ { m j }$ to 1.96 times its approximate standard deviation $\begin{array} { r } { \sigma _ { j } = n ^ { - 1 / 2 } \big [ \sum _ { i = 0 } ^ { j - 1 } \hat { \theta } _ { m i } ^ { 2 } \big ] ^ { 1 / 2 } } \end{array}$ , we can see which of the coefficient estimates are most significantly different from zero, estimate the order of the model to be fitted as the largest lag $j$ for which the ratio is larger than 1 in absolute value, and at the same time read off estimated values for each of the coefficients. A default value of $m$ is set by the program, but it may be altered manually. As m is increased the values $\hat { \theta } _ { m 1 } , \ldots , \hat { \theta } _ { m m } ^ { \ }$ stabilize in the sense that the fluctuations in each component are of order $n ^ { - 1 / 2 }$ , the asymptotic standard deviation of $\theta _ { m 1 }$ .

• As for autoregressive models, a more systematic approach to order selection for moving-average models is to find the values of $q$ and $\hat { \pmb { \theta } } _ { q } = \left( \hat { \theta } _ { m 1 } , \dots , \hat { \theta } _ { m q } \right) ^ { \prime }$ that minimize the AICC statistic

$$
\operatorname {A I C C} = - 2 \ln L (\boldsymbol {\theta} _ {q}, S (\boldsymbol {\theta} _ {q}) / n) + 2 (q + 1) n / (n - q - 2),
$$

where $L$ is the Gaussian likelihood defined in (5.2.9) and $S$ is defined in (5.2.11). (See Section 5.5 for further details.)

# Confidence Regions for the Coefficients

Asymptotic confidence regions for the coefficient vector $\theta _ { q }$ and for its individual components can be found with the aid of the large-sample distribution specified in Remark 1. For example, approximate $95 \%$ confidence bounds for $\theta _ { j }$ are given by

$$
\hat {\theta} _ {m j} \pm 1. 9 6 n ^ {- 1 / 2} \left(\sum_ {i = 0} ^ {j - 1} \hat {\theta} _ {m i} ^ {2}\right) ^ {1 / 2}. \tag {5.1.24}
$$

# Example 5.1.5 The Dow Jones Utilities Index

In Example 5.1.1 we fitted an AR(1) model to the differenced Dow Jones Utilities Index. The sample ACF of the differenced data shown in Figure 5-1 suggests that an MA(2) model might also provide a good fit to the data. To apply the innovation technique for preliminary estimation, we proceed as in Example 5.1.1 to difference the series DOWJ.TSM to obtain observations of the differenced series $\{ Y _ { t } \}$ . We then select preliminary estimation by clicking on the blue PRE button and subtract the mean of the differences to obtain observations of the differenced and mean-corrected series $\{ X _ { t } \}$ . In the Preliminary Estimation dialog box enter 0 for the AR order and

2 for the MA order, and select Innovations as the estimation method. We must then specify a value of m, which is set by default in this case to 17. If we accept the default value, the program will compute $\hat { \theta } _ { 1 7 , 1 } , \dots , \hat { \theta } _ { 1 7 , 1 7 }$ and print out the first two values as the estimates of $\theta _ { 1 }$ and $\theta _ { 2 }$ , together with the ratios of the estimated values to their estimated standard deviations. These are

# MA COEFFICIENT

0.4269 0.2704

# COEFFICIENT/(1.96*STANDARD ERROR)

1.9114 1.1133

The remaining parameter in the model is the white noise variance, for which two estimates are given:

# WN VARIANCE ESTIMATE $=$ (RESID SS)/N

0.1470

# INNOVATION WN VARIANCE ESTIMATE

0.1122

The first of these is the average of the squares of the rescaled one-step prediction errors under the fitted MA(2) model, i.e., $\textstyle { \frac { 1 } { 7 7 } } \sum _ { j = 1 } ^ { 7 7 } \bigl ( X _ { j } - \hat { X } _ { j } \bigr ) ^ { 2 } / r _ { j - 1 }$ . The second value is the innovation estimate, $\hat { \nu } _ { 1 7 }$ . (By default ITSM retains the first value. If you wish instead to use the innovation estimate, you must change the white noise variance by selecting Model>Specify and setting the white noise value to the desired value.) The fitted model for $X _ { t } ( = Y _ { t } - 0 . 1 3 3 6 )$ is thus

$$
X _ {t} = Z _ {t} + 0. 4 2 6 9 Z _ {t - 1} + 0. 2 7 0 4 Z _ {t - 2}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 0. 1 4 7 0),
$$

with $\mathrm { A I C C } = 7 7 . 4 6 7$ .

To see all 17 estimated coefficients $\widehat { \theta } _ { 1 7 , j } , j = 1 , \ldots , 1 7$ , we repeat the preliminary estimation, this time fitting an MA(17) model with $m = 1 7$ . The coefficients and ratios for the resulting model are found to be as follows:

MA COEFFICIENT   

<table><tr><td>0.4269</td><td>0.2704</td><td>0.1183</td><td>0.1589</td><td>0.1355</td><td>0.1568</td><td>0.1284</td><td>-0.0060</td></tr><tr><td>0.0148</td><td>-0.0017</td><td>0.1974</td><td>-0.0463</td><td>0.2023</td><td>0.1285</td><td>-0.0213</td><td>-0.2575</td></tr><tr><td>0.0760</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="8">EFFICIENT/(1.96*STANDARD ERROR)</td></tr><tr><td>1.9114</td><td>1.1133</td><td>0.4727</td><td>0.6314</td><td>0.5331</td><td>0.6127</td><td>0.4969</td><td>-0.0231</td></tr><tr><td>0.0568</td><td>-0.0064</td><td>0.7594</td><td>-0.1757</td><td>0.7667</td><td>0.4801</td><td>-0.0792</td><td>-0.9563</td></tr><tr><td>0.2760</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>

The ratios indicate that the estimated coefficients most significantly different from zero are the first and second, reinforcing our original intention of fitting an MA(2) model to the data. Estimated coefficients $\hat { \theta } _ { m j } ^ { \setminus }$ for other values of m can be examined in the same way, and it is found that the values obtained for $m > 1 7$ change only slightly from the values tabulated above.

By fitting $\mathrm { M A } ( q )$ models of orders 0, 1, 2, . . . , 26 using the innovations algorithm with the default settings for m, we find that the minimum AICC model is the one with $q = 2$ found above. Thus the model suggested by the sample ACF again coincides with the more systematically chosen minimum AICC model.

-

Innovations Algorithm Estimates when $p > 0$ and $q > 0$ The causality assumption (Section 3.1) ensures that

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j},
$$

where the coefficients $\psi _ { j }$ satisfy

$$
\psi_ {j} = \theta_ {j} + \sum_ {i = 1} ^ {\min  (j, p)} \phi_ {i} \psi_ {j - i}, \quad j = 0, 1, \dots , \tag {5.1.25}
$$

and we define $\theta _ { 0 } : = 1$ and $\theta _ { j } : = 0$ for $j > q$ . To estimate $\psi _ { 1 }$ , . . . , $\psi _ { p + q }$ we can use the innovation estimates $\hat { \theta } _ { m 1 } , \ldots , \hat { \theta } _ { m , p + q }$ , whose large-sample behavior is specified in Remark 1. Replacing $\psi _ { j }$ by $\hat { \theta } _ { m j }$ in (5.1.25) and solving the resulting equations

$$
\hat {\theta} _ {m j} = \theta_ {j} + \sum_ {i = 1} ^ {\min  (j, p)} \phi_ {i} \hat {\theta} _ {m, j - i}, \quad j = 1, \dots , p + q, \tag {5.1.26}
$$

for $\phi$ and $\pmb \theta$ , we obtain initial parameter estimates $\hat { \phi }$ and $\hat { \pmb { \theta } }$ . To solve (5.1.26) we first find $\phi$ from the last $q$ equations:

$$
\left[ \begin{array}{c} \hat {\theta} _ {m, q + 1} \\ \hat {\theta} _ {m, q + 2} \\ \vdots \\ \hat {\theta} _ {m, q + p} \end{array} \right] = \left[ \begin{array}{c c c c} \hat {\theta} _ {m q} & \hat {\theta} _ {m, q - 1} & \dots & \hat {\theta} _ {m, q + 1 - p} \\ \hat {\theta} _ {m, q + 1} & \hat {\theta} _ {m, q} & \dots & \hat {\theta} _ {m, q + 2 - p} \\ \vdots & \vdots & & \vdots \\ \hat {\theta} _ {m, q + p - 1} & \hat {\theta} _ {m, q + p - 2} & \dots & \hat {\theta} _ {m, q} \end{array} \right] \left[ \begin{array}{c} \phi_ {1} \\ \phi_ {2} \\ \vdots \\ \phi_ {p} \end{array} \right]. \tag {5.1.27}
$$

Having solved (5.1.27) for $\hat { \phi }$ (which may not be causal), we can easily determine the estimate of $\pmb \theta$ from

$$
\hat {\theta} _ {j} = \hat {\theta} _ {m j} - \sum_ {i = 1} ^ {\min  (j, p)} \hat {\phi} _ {i} \hat {\theta} _ {m, j - i}, \quad j = 1, \dots , q.
$$

Finally, the white noise variance $\sigma ^ { 2 }$ is estimated by

$$
\hat {\sigma} ^ {2} = n ^ {- 1} \sum_ {t = 1} ^ {n} \left(X _ {t} - \hat {X} _ {t}\right) ^ {2} / r _ {t - 1},
$$

where $\hat { X } _ { t }$ is the one-step predictor of $X _ { t }$ computed from the fitted coefficient vectors $\hat { \phi }$ and $\hat { \pmb { \theta } }$ , and $r _ { t - 1 }$ is defined in (3.3.8).

The above calculations can all be carried out by selecting the ITSM option $\mathsf { M o d e l } >$ Estimation>Preliminary. This option also computes, if $p \ : = \ : q$ , the ratio of each estimated coefficient to 1.96 times its estimated standard deviation. Approximate $9 5 \%$ confidence intervals can therefore easily be obtained in this case. If the fitted model is noncausal, it cannot be used to initialize the search for the maximum likelihood estimators, and so the autoregressive coefficients should be set to some causal values (e.g., all equal to 0.001) using the Model>Specify option. If both the innovation and Hannan–Rissanen algorithms give noncausal models, it is an indication (but not a conclusive one) that the assumed values of $p$ and $q$ may not be appropriate for the data.

# Order Selection for Mixed Models

For models with $p > 0$ and $q > 0$ , the sample ACF and PACF are difficult to recognize and are of far less value in order selection than in the special cases where $p = 0$ or $q = 0$ . A systematic approach, however, is still available through minimization of the AICC statistic

$$
\mathrm {A I C C} = - 2 \ln L (\phi_ {p}, \theta_ {q}, S (\phi_ {p}, \theta_ {q}) / n) + 2 (p + q + 1) n / (n - p - q - 2),
$$

which is discussed in more detail in Section 5.5. For fixed $p$ and $q$ it is clear from the definition that the AICC value is minimized by the parameter values that maximize the likelihood. Hence, final decisions regarding the orders $p$ and $q$ that minimize AICC must be based on maximum likelihood estimation as described in Section 5.2.

# Example 5.1.6 The Lake Data

In Example 5.1.4 we fitted AR(2) models to the mean-corrected lake data using the Yule–Walker equations and Burg’s algorithm. If instead we fit an ARMA(1,1) model using the innovations method in the option Model>Estimation>Preliminary of ITSM (with the default value $m = 1 7$ ), we obtain the model

$$
X _ {t} - 0. 7 2 3 4 X _ {t - 1} = Z _ {t} + 0. 3 5 9 6 Z _ {t - 1}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 0. 4 7 5 7),
$$

for the mean-corrected series $X _ { t } ~ = ~ Y _ { t } - 9 . 0 0 4 1$ . The ratio of the two coefficient estimates $\hat { \phi }$ and $\hat { \theta }$ to 1.96 times their estimated standard deviations are given by ITSM as 3.2064 and 1.8513, respectively. The corresponding $95 \%$ confidence intervals are therefore

$$
\begin{array}{l} \phi : 0. 7 2 3 4 \pm 0. 7 2 3 4 / 3. 2 0 6 4 = (0. 4 9 7 8, 0. 9 4 9 0), \\ \theta : 0. 3 5 9 6 \pm 0. 3 5 9 6 / 1. 8 5 1 3 = (0. 1 6 5 4, 0. 5 5 3 8). \\ \end{array}
$$

It is interesting to note that the value of AICC for this model is 212.89, which is smaller than the corresponding values for the Burg and Yule–Walker AR(2) models in Example 5.1.4. This suggests that an ARMA(1,1) model may be superior to a pure autoregressive model for these data. Preliminary estimation of a variety of ARMA $( p , q )$ models shows that the minimum AICC value does in fact occur when $\begin{array} { l } { p } \end{array} = \begin{array} { l } { q } \ = \begin{array} { l } { 1 } \end{array} \end{array}$ . (Before committing ourselves to this model, however, we need to compare AICC values for the corresponding maximum likelihood models. We shall do this in Section 5.2.)

![](images/fcedd0967e71174fc055f54a9d40ab62df6c04a0a6e7fb1b75218015453720ac.jpg)

# 5.1.4 The Hannan–Rissanen Algorithm

The defining equations for a causal $\operatorname { A R } ( p )$ model have the form of a linear regression model with coefficient vector $\phi = ( \phi _ { 1 } , \ldots , \phi _ { p } ) ^ { \prime }$ . This suggests the use of simple least squares regression for obtaining preliminary parameter estimates when $q = 0$ . Application of this technique when $q \ > \ 0$ is complicated by the fact that in the general $\mathbf { A R M A } ( p , q )$ equations $X _ { t }$ is regressed not only on $X _ { t - 1 } , \dotsc , X _ { t - p }$ $X _ { t - p }$ , but also on the unobserved quantities $Z _ { t - 1 } , \dotsc , Z _ { t - q }$ . Nevertheless, it is still possible to apply least squares regression to the estimation of $\phi$ and $\pmb \theta$ by first replacing the unobserved quantities $Z _ { t - 1 } , \dotsc , Z _ { t - q }$ in (5.1.1) by estimated values $\hat { Z } _ { t - 1 } , \dotsc , \hat { Z } _ { t - q }$ . The parameters $\phi$ and $\pmb \theta$ are then estimated by regressing $X _ { t }$ onto $X _ { t - 1 } , \ldots , X _ { t - p } , \hat { Z } _ { t - 1 } , \ldots , \hat { Z } _ { t - q }$ . These are the main steps in the Hannan–Rissanen estimation procedure, which we now describe in more detail.

Step 1. A high-order $\operatorname { A R } ( m )$ model (with $m > \operatorname* { m a x } ( p , q ) )$ is fitted to the data using the Yule–Walker estimates of Section 5.1.1. If $\bigl ( \hat { \phi } _ { m 1 } , \ldots , \hat { \phi } _ { m m } \bigr ) ^ { \prime }$ is the vector of estimated coefficients, then the estimated residuals are computed from the equations

$$
\hat {Z} _ {t} = X _ {t} - \hat {\phi} _ {m 1} X _ {t - 1} - \dots - \hat {\phi} _ {m m} X _ {t - m}, t = m + 1, \ldots , n.
$$

Step 2. Once the estimated residuals $\hat { Z } _ { t }$ , $t = m + 1 , \ldots , n$ , have been computed as in Step 1, the vector of parameters, $\beta = \left( \phi ^ { \prime } , \theta ^ { \prime } \right) ^ { \prime }$ ′ is estimated by least squares linear regression of $X _ { t }$ onto $\left( X _ { t - 1 } , \ldots , X _ { t - p } , { \hat { Z } } _ { t - 1 } , \ldots , { \hat { Z } } _ { t - q } \right)$ , $t = m + 1 + q , \ldots , n$ , i.e., by minimizing the sum of squares

$$
S (\beta) = \sum_ {t = m + 1 + q} ^ {n} \left(X _ {t} - \phi_ {1} X _ {t - 1} - \dots - \phi_ {p} X _ {t - p} - \theta_ {1} \hat {Z} _ {t - 1} - \dots - \theta_ {q} \hat {Z} _ {t - q}\right) ^ {2}
$$

with respect to $\beta$ . This gives the Hannan–Rissanen estimator

$$
\hat {\boldsymbol {\beta}} = (Z ^ {\prime} Z) ^ {- 1} Z ^ {\prime} \mathbf {X} _ {n},
$$

where $\mathbf { X } _ { n } = ( X _ { m + 1 + q } , \ldots , X _ { n } ) ^ { \prime }$ and $Z$ is the $( n - m - q ) \times ( p + q )$ matrix

$$
Z = \left[ \begin{array}{c c c c c c c c} X _ {m + q} & X _ {m + q - 1} & \dots & X _ {m + q + 1 - p} & \hat {Z} _ {m + q} & \hat {Z} _ {m + q - 1} & \dots & \hat {Z} _ {m + 1} \\ X _ {m + q + 1} & X _ {m + q} & \dots & X _ {m + q + 2 - p} & \hat {Z} _ {m + q + 1} & \hat {Z} _ {m + q} & \dots & \hat {Z} _ {m + 2} \\ \vdots & \vdots & \dots & \vdots & \vdots & \vdots & \dots & \vdots \\ X _ {n - 1} & X _ {n - 2} & \dots & X _ {n - p} & \hat {Z} _ {n - 1} & \hat {Z} _ {n - 2} & \dots & \hat {Z} _ {n - q} \end{array} \right].
$$

(If $p = 0$ , Z contains only the last $q$ columns.) The Hannan–Rissanen estimate of the white noise variance is

$$
\hat {\sigma} _ {\mathrm {H R}} ^ {2} = \frac {S (\hat {\boldsymbol {\beta}})}{n - m - q}.
$$

# Example 5.1.7 The Lake Data

In Example 5.1.6 an ARMA(1,1) model was fitted to the mean corrected lake data using the innovations algorithm. We can fit an ARMA(1,1) model to these data using the Hannan–Rissanen estimates by selecting Hannan-Rissanen in the Preliminary Estimation dialog box of ITSM. The fitted model is

$$
X _ {t} - 0. 6 9 6 1 X _ {t - 1} = Z _ {t} + 0. 3 7 8 8 Z _ {t - 1}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 0. 4 7 7 4),
$$

for the mean-corrected series $X _ { t } = Y _ { t } - 9 . 0 0 4 1$ . (Two estimates of the white noise variance are computed in ITSM for the Hannan–Rissanen procedure, $ { \hat { \sigma } } _ { \mathrm { H R } } ^ { 2 }$ and $\sum _ { j = 1 } ^ { n } ( X _ { t } -$ $\hat { X } _ { t - 1 } ) ^ { 2 } / n$ . The latter is the one retained by the program.) The ratios of the two coefficient estimates to 1.96 times their standard deviation are 4.5289 and 1.3120, respectively. The corresponding $9 5 \%$ confidence bounds for $\phi$ and $\theta$ are

$$
\phi : 0. 6 9 6 1 \pm 0. 6 9 6 1 / 4. 5 2 8 9 = (0. 5 4 2 4, 0. 8 4 9 8),
$$

$$
\theta : 0. 3 7 8 8 \pm 0. 3 7 8 8 / 1. 3 1 2 0 = (0. 0 9 0 1, 0. 6 6 7 5).
$$

Clearly, there is little difference between this model and the one fitted using the innovations method in Example 5.1.6. (The AICC values are 213.18 for the current model and 212.89 for the model fitted in Example 5.1.6.)

-

Hannan and Rissanen include a third step in their procedure to improve the estimates.

Step 3. Using the estimate $\hat { \beta } = ( \hat { \phi } _ { 1 } , \ldots , \hat { \phi } _ { p } , \hat { \theta } _ { 1 } , \ldots , \hat { \theta } _ { q } ) ^ { \prime }$ from Step 2, set

$$
\tilde {Z} _ {t} = \left\{ \begin{array}{l l} 0, & \text {i f} t \leq \max  (p, q), \\ X _ {t} - \sum_ {j = 1} ^ {p} \hat {\phi} _ {j} X _ {t - j} - \sum_ {j = 1} ^ {q} \hat {\theta} _ {j} \tilde {Z} _ {t - j}, & \text {i f} t > \max  (p, q). \end{array} \right.
$$

Now for $t = 1 , \ldots , n$ put

$$
V _ {t} = \left\{ \begin{array}{l l} 0, & \text {i f} t \leq \max  (p, q), \\ \sum_ {j = 1} ^ {p} \hat {\phi} _ {j} V _ {t - j} + \tilde {Z} _ {t}, & \text {i f} t > \max  (p, q), \end{array} \right.
$$

and

$$
W _ {t} = \left\{ \begin{array}{l l} 0, & \text {i f} t \leq \max  (p, q), \\ - \sum_ {j = 1} ^ {p} \hat {\theta} _ {j} W _ {t - j} + \tilde {Z} _ {t}, & \text {i f} t > \max  (p, q). \end{array} \right.
$$

(Observe that both $V _ { t }$ and $W _ { t }$ satisfy the AR recursions $\hat { \phi } ( B ) V _ { t } = \tilde { Z } _ { t }$ and $\hat { \theta } ( B ) W _ { t } = \tilde { Z } _ { t }$ for $t = 1 , \ldots , n . )$ If $\hat { \beta } ^ { \dagger }$ is the regression estimate of $\beta$ found by regressing $\tilde { Z } _ { t }$ on (Vt 1, . . . , Vt p, Wt 1, . . . , Wt q), i.e., if $\hat { \beta } ^ { \dagger }$ minimizes

$$
S ^ {\dagger} (\beta) = \sum_ {t = \max (p, q) + 1} ^ {n} \left(\tilde {Z} _ {t} - \sum_ {j = 1} ^ {p} \beta_ {j} V _ {t - j} - \sum_ {k = 1} ^ {q} \beta_ {k + p} W _ {t - k}\right) ^ {2},
$$

then the improved estimate of $\beta$ is $\tilde { \boldsymbol { \beta } } = \hat { \boldsymbol { \beta } } ^ { \dagger } + \hat { \boldsymbol { \beta } }$ . The new estimator $\tilde { \boldsymbol { \beta } }$ then has the same asymptotic efficiency as the maximum likelihood estimator. In ITSM, however, we eliminate Step 3, using the model produced by Step 2 as the initial model for the calculation (by numerical maximization) of the maximum likelihood estimator itself.

# 5.2 Maximum Likelihood Estimation

Suppose that $\{ X _ { t } \}$ is a Gaussian time series with mean zero and autocovariance function $\kappa ( i , j ) = E ( X _ { i } X _ { j } )$ . Let $\mathbf { X } _ { n } = ( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ and let $\hat { \mathbf { X } } _ { n } = ( \hat { X } _ { 1 } , \ldots , \hat { X } _ { n } ) ^ { \prime }$ , where $\hat { X } _ { 1 } = 0$ and $\hat { X } _ { j } = E ( X _ { j } | X _ { 1 } , \dots , X _ { j - 1 } ) = P _ { j - 1 } X _ { j } , j \ge 2 .$ . Let $\Gamma _ { n }$ denote the covariance matrix $\Gamma _ { n } = E ( \mathbf { X } _ { n } \mathbf { X } _ { n } ^ { \prime } )$ , and assume that $\Gamma _ { n }$ is nonsingular.

The likelihood of $\mathbf { X } _ { n }$ is

$$
L \left(\Gamma_ {n}\right) = (2 \pi) ^ {- n / 2} \left(\det \Gamma_ {n}\right) ^ {- 1 / 2} \exp \left(- \frac {1}{2} \mathbf {X} _ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \mathbf {X} _ {n}\right). \tag {5.2.1}
$$

As we shall now show, the direct calculation of det $\Gamma _ { n }$ and $ { \Gamma _ { n } } ^ { - 1 }$ can be avoided by expressing this in terms of the one-step prediction errors $X _ { j } - { \hat { X } } _ { j }$ and their variances $\nu _ { j - 1 } , j = 1 , \dots , n$ , both of which are easily calculated recursively from the innovations algorithm (Section 2.5.4).

Let $\theta _ { i j } , j ~ = ~ 1 , \ldots , i ; i ~ = ~ 1 , 2 , \ldots$ , denote the coefficients obtained when the innovations algorithm is applied to the autocovariance function $\kappa$ of $\{ X _ { t } \}$ , and let $C _ { n }$ be the $n \times n$ lower triangular matrix defined in Section 2.5.4. From (2.5.27) we have the identity

$$
\mathbf {X} _ {n} = C _ {n} \left(\mathbf {X} _ {n} - \hat {\mathbf {X}} _ {n}\right). \tag {5.2.2}
$$

We also know from Remark 5 of Section 2.5.4 that the components of $\mathbf { X } _ { n } - \hat { \mathbf { X } } _ { n }$ are uncorrelated. Consequently, by the definition of vj, ${ \bf X } _ { n } - \hat { \bf X } _ { n }$ has the diagonal covariance matrix

$$
D _ {n} = \operatorname {d i a g} \left\{v _ {0}, \dots , v _ {n - 1} \right\}.
$$

From (5.2.2) and (A.2.5) we conclude that

$$
\Gamma_ {n} = C _ {n} D _ {n} C _ {n} ^ {\prime}. \tag {5.2.3}
$$

From (5.2.2) and (5.2.3) we see that

$$
\mathbf {X} _ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \mathbf {X} _ {n} = \left(\mathbf {X} _ {n} - \hat {\mathbf {X}} _ {n}\right) ^ {\prime} D _ {n} ^ {- 1} \left(\mathbf {X} _ {n} - \hat {\mathbf {X}} _ {n}\right) = \sum_ {j = 1} ^ {n} \left(X _ {j} - \hat {X} _ {j}\right) ^ {2} / v _ {j - 1} \tag {5.2.4}
$$

and

$$
\det \Gamma_ {n} = (\det C _ {n}) ^ {2} (\det D _ {n}) = v _ {0} v _ {1} \dots v _ {n - 1}. \tag {5.2.5}
$$

The likelihood (5.2.1) of the vector $\mathbf { X } _ { n }$ therefore reduces to

$$
L \left(\Gamma_ {n}\right) = \frac {1}{\sqrt {\left(2 \pi\right) ^ {n} v _ {0} \cdots v _ {n - 1}}} \exp \left\{- \frac {1}{2} \sum_ {j = 1} ^ {n} \left(X _ {j} - \hat {X} _ {j}\right) ^ {2} / v _ {j - 1} \right\}. \tag {5.2.6}
$$

If $\Gamma _ { n }$ is expressible in terms of a finite number of unknown parameters $\beta _ { 1 } , \ldots , \beta _ { r }$ (as is the case when $\{ X _ { t } \}$ is an $\mathbf { A R M A } ( \boldsymbol { p } , \boldsymbol { q } )$ process), the maximum likelihood estimators of the parameters are those values that maximize $L$ for the given data set. When $X _ { 1 } , X _ { 2 } , \ldots , X _ { n }$ are iid, it is known, under mild assumptions and for $n$ large, that maximum likelihood estimators are approximately normally distributed with variances that are at least as small as those of other asymptotically normally distributed estimators (see, e.g., Lehmann 1983).

Even if $\{ X _ { t } \}$ is not Gaussian, it still makes sense to regard (5.2.6) as a measure of goodness of fit of the model to the data, and to choose the parameters $\beta _ { 1 } , \ldots , \beta _ { r }$ in such a way as to maximize (5.2.6). We shall always refer to the estimators $\hat { \beta } _ { 1 } , \ldots , \hat { \beta } _ { r }$ so obtained as “maximum likelihood” estimators, even when $\{ X _ { t } \}$ is not Gaussian. Regardless of the joint distribution of $X _ { 1 } , \ldots , X _ { n }$ , we shall refer to (5.2.1) and its algebraic equivalent (5.2.6) as the “likelihood” (or “Gaussian likelihood”) of $X _ { 1 } , \ldots , X _ { n }$ . A justification for using maximum Gaussian likelihood estimators of ARMA coefficients is that the large-sample distribution of the estimators is the same for $\{ Z _ { t } \} \sim \mathrm { I I D } \big ( 0 , \sigma ^ { 2 } \big )$ , regardless of whether or not $\{ Z _ { t } \}$ is Gaussian (see Brockwell and Davis (1991), Section 10.8).

The likelihood for data from an ARMA $( p , q )$ process is easily computed from the innovations form of the likelihood (5.2.6) by evaluating the one-step predictors $\hat { X } _ { i + 1 }$ and the corresponding mean squared errors $\nu _ { i }$ . These can be found from the recursions (Section 3.3)

$$
\hat {X} _ {n + 1} = \left\{ \begin{array}{l l} \sum_ {j = 1} ^ {n} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right), & 1 \leq n <   m, \\ \phi_ {1} X _ {n} + \dots + \phi_ {p} X _ {n + 1 - p} + \sum_ {j = 1} ^ {q} \theta_ {n j} \left(X _ {n + 1 - j} - \hat {X} _ {n + 1 - j}\right), & n \geq m, \end{array} \right. \tag {5.2.7}
$$

and

$$
E \left(X _ {n + 1} - \hat {X} _ {n + 1}\right) ^ {2} = \sigma^ {2} E \left(W _ {n + 1} - \hat {W} _ {n + 1}\right) ^ {2} = \sigma^ {2} r _ {n}, \tag {5.2.8}
$$

where $\theta _ { n j }$ and $r _ { n }$ are determined by the innovations algorithm with $\kappa$ as in (3.3.3) and $m = \operatorname* { m a x } ( p , q )$ . Substituting in the general expression (5.2.6), we obtain the following:

# The Gaussian Likelihood for an ARMA Process:

$$
L \left(\boldsymbol {\phi}, \boldsymbol {\theta}, \sigma^ {2}\right) = \frac {1}{\sqrt {\left(2 \pi \sigma^ {2}\right) ^ {n} r _ {0} \cdots r _ {n - 1}}} \exp \left\{- \frac {1}{2 \sigma^ {2}} \sum_ {j = 1} ^ {n} \frac {\left(X _ {j} - \hat {X} _ {j}\right) ^ {2}}{r _ {j - 1}} \right\}. \tag {5.2.9}
$$

Differentiating $\ln L \left( \phi , \theta , \sigma ^ { 2 } \right)$ partially with respect to $\sigma ^ { 2 }$ and noting that $\hat { X } _ { j }$ and $r _ { j }$ are independent of $\sigma ^ { 2 }$ , we find that the maximum likelihood estimators $\hat { \phi } , \hat { \theta }$ , and $\hat { \sigma } ^ { 2 }$ satisfy the following equations (Problem 5.8):

# Maximum Likelihood Estimators:

$$
\hat {\sigma} ^ {2} = n ^ {- 1} S (\hat {\boldsymbol {\phi}}, \hat {\boldsymbol {\theta}}), \tag {5.2.10}
$$

where

$$
S (\hat {\boldsymbol {\phi}}, \hat {\boldsymbol {\theta}}) = \sum_ {j = 1} ^ {n} \left(X _ {j} - \hat {X} _ {j}\right) ^ {2} / r _ {j - 1}, \tag {5.2.11}
$$

and $\hat { \phi } , \hat { \theta }$ are the values of $\phi$ , θ that minimize

$$
\ell (\boldsymbol {\phi}, \boldsymbol {\theta}) = \ln (n ^ {- 1} S (\boldsymbol {\phi}, \boldsymbol {\theta})) + n ^ {- 1} \sum_ {j = 1} ^ {n} \ln r _ {j - 1}. \tag {5.2.12}
$$

Minimization of $\ell ( \pmb \phi , \pmb \theta )$ must be done numerically. Initial values for $\phi$ and $\pmb \theta$ can be obtained from ITSM using the methods described in Section 5.1. The program then searches systematically for the values of $\phi$ and $\pmb \theta$ that minimize the reduced likelihood (5.2.12) and computes the corresponding maximum likelihood estimate of $\sigma ^ { 2 }$ from (5.2.10).

# Least Squares Estimation for Mixed Models

The least squares estimates $\tilde { \phi }$ and $\tilde { \pmb { \theta } }$ of $\phi$ and $\pmb \theta$ are obtained by minimizing the function S as defined in (5.2.11) rather than $\ell$ as defined in (5.2.12), subject to the constraints that the model be causal and invertible. The least squares estimate of $\sigma ^ { 2 }$ is

$$
\tilde {\sigma} ^ {2} = \frac {S (\tilde {\phi} , \tilde {\theta})}{n - p - q}.
$$

# Order Selection

In Section 5.1 we introduced minimization of the AICC value as a major criterion for the selection of the orders $p$ and $q$ . This criterion is applied as follows:

# AICC Criterion:

Choose p, q, $\phi _ { p }$ , and $\theta _ { q }$ to minimize

$$
\mathrm {A I C C} = - 2 \ln L (\pmb {\phi} _ {p}, \pmb {\theta} _ {q}, S (\pmb {\phi} _ {p}, \pmb {\theta} _ {q}) / n) + 2 (p + q + 1) n / (n - p - q - 2).
$$

For any fixed $p$ and $q$ it is clear that the AICC is minimized when $\phi _ { p }$ and $\theta _ { q }$ are the vectors that minimize $- 2 \ln L ( \phi _ { p } , \theta _ { q } , S ( \phi _ { p } , \theta _ { q } ) / n )$ , i.e., the maximum likelihood estimators. Final decisions with respect to order selection should therefore be made on the basis of maximum likelihood estimators (rather than the preliminary estimators of Section 5.1, which serve primarily as a guide). The AICC statistic and its justification are discussed in detail in Section 5.5.

One of the options in the program ITSM is Model>Estimation>Autofit. Selection of this option allows you to specify a range of values for both $p$ and $q$ , after which the program will automatically fit maximum likelihood $\mathbf { A R M A } ( p , q )$ values for all $p$ and $q$ in the specified range, and select from these the model with smallest AICC value. This may be slow if a large range is selected (the maximum range is from 0 through 27 for both $p$ and $q$ ), and once the model has been determined, it should be checked by preliminary estimation followed by maximum likelihood estimation to minimize the risk of the fitted model corresponding to a local rather than a global maximum of the likelihood. (For more details see Section E.3.1.)

# Confidence Regions for the Coefficients

For large sample size the maximum likelihood estimator $\hat { \boldsymbol \beta }$ of $\beta : = ( \phi _ { 1 } , . . . , \phi _ { p }$ , $\theta _ { 1 } , \ldots , \theta _ { q } ) ^ { \prime }$ is approximately normally distributed with mean $\beta$ and covariance matrix $\left[ n ^ { - 1 } V ( \beta ) \right]$ which can be approximated by $2 H ^ { - 1 } ( \beta )$ , where $H$ is the Hessian matrix $\left[ \partial ^ { 2 } \ell ( \beta ) / \partial \beta _ { i } \partial \beta _ { j } \right] _ { i , j = 1 } ^ { p + q }$ . ITSM prints out the approximate standard deviations and correlations of the coefficient estimators based on the Hessian matrix evaluated numerically at $\hat { \boldsymbol \beta }$ unless this matrix is not positive definite, in which case ITSM instead computes the theoretical asymptotic covariance matrix in Section 9.8 of Brockwell and Davis (1991). The resulting covariances can be used to compute confidence bounds for the parameters.

# Large-Sample Distribution of Maximum Likelihood Estimators:

For a large sample from an ARMA $( p , q )$ process,

$$
\hat {\beta} \approx \mathrm {N} (\beta , n ^ {- 1} V (\beta)).
$$

The general form of $V ( \beta )$ can be found in Brockwell and Davis (1991), Section 9.8. The following are several special cases.

# Example 5.2.1 An $\operatorname { A R } ( p )$ Model

The asymptotic covariance matrix in this case is the same as that for the Yule–Walker estimates given by

$$
V (\phi) = \sigma^ {2} \Gamma_ {p} ^ {- 1}.
$$

In the special cases $p = 1$ and $p = 2$ , we have

$$
\operatorname {A R} (1): V (\phi) = \left(1 - \phi_ {1} ^ {2}\right),
$$

$$
\operatorname {A R} (2): V (\phi) = \left[ \begin{array}{c c} 1 - \phi_ {2} ^ {2} & - \phi_ {1} (1 + \phi_ {2}) \\ - \phi_ {1} (1 + \phi_ {2}) & 1 - \phi_ {2} ^ {2} \end{array} \right].
$$

# Example 5.2.2 An MA(q) Model

Let $\Gamma _ { q } ^ { * }$ be the covariance matrix of $Y _ { 1 } , \dots , Y _ { q }$ , where $\{ Y _ { t } \}$ is the autoregressive process with autoregressive polynomial $\theta ( z )$ , i.e.,

$$
Y _ {t} + \theta_ {1} Y _ {t - 1} + \dots + \theta_ {q} Y _ {t - q} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 1).
$$

Then it can be shown that

$$
V (\theta) = \Gamma_ {q} ^ {* - 1}.
$$

Inspection of the results of Example 5.2.1 and replacement of $\phi _ { i }$ by $- \theta _ { i }$ yields

$$
\operatorname {M A} (1): V (\boldsymbol {\theta}) = \left(1 - \theta_ {1} ^ {2}\right),
$$

$$
\operatorname {M A} (2): V (\boldsymbol {\theta}) = \left[ \begin{array}{c c} 1 - \theta_ {2} ^ {2} & \theta_ {1} (1 - \theta_ {2}) \\ \theta_ {1} (1 - \theta_ {2}) & 1 - \theta_ {2} ^ {2} \end{array} \right].
$$

# Example 5.2.3 An ARMA(1, 1) Model

For a causal and invertible ARMA(1,1) process with coefficients $\phi$ and $\theta$ .

$$
V (\phi , \theta) = \frac {1 + \phi \theta}{(\phi + \theta) ^ {2}} \left[ \begin{array}{c c} (1 - \phi^ {2}) (1 + \phi \theta) & - (1 - \theta^ {2}) (1 - \phi^ {2}) \\ - (1 - \theta^ {2}) (1 - \phi^ {2}) & (1 - \theta^ {2}) (1 + \phi \theta) \end{array} \right].
$$

# Example 5.2.4 The Dow Jones Utilities Index

For the Burg and Yule–Walker AR(1) models derived for the differenced and meancorrected series in Examples 5.1.1 and 5.1.3, the Model>Estimation> Preliminary option of ITSM gives $- 2 \ln ( L ) = 7 0 . 3 3 0$ for the Burg model and $- 2 \ln ( L ) = 7 0 . 3 7 8$ for the Yule–Walker model. Since maximum likelihood estimation attempts to minimize $- 2 \ln L$ , the Burg estimate appears to be a slightly better initial estimate of $\phi$ . We therefore retain the Burg AR(1) model and then select Model> Estimation>Max Likelihood and click OK. The Burg coefficient estimates provide initial parameter values to start the search for the minimizing values. The model found on completion of the minimization is

$$
Y _ {t} - 0. 4 4 7 1 Y _ {t - 1} = Z _ {t}, \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 0 2 1 1 7). \tag {5.2.13}
$$

This model is different again from the Burg and Yule–Walker models. It has $- 2 \ln ( L ) ~ = ~ 7 0 . 3 2 1$ , corresponding to a slightly higher likelihood. The standard error (or estimated standard deviation) of the estimator $\mathcal { \bar { \phi } }$ is found from the program to be 0.1050. This is close to the estimated standard deviation $\sqrt { ( 1 - ( 0 . 4 4 7 1 ) ^ { 2 } ) / 7 7 } =$ 0.1019, based on the large-sample approximation given in Example 5.2.1. Using the value computed from ITSM, approximate $95 \%$ confidence bounds for $\phi$ are $0 . 4 4 7 1 \pm 1 . 9 6 \times 0 . 1 0 5 0 = ( 0 . 2 4 1 3 , 0 . 6 5 2 9 )$ . These are quite close to the bounds based on the Yule–Walker and Burg estimates found in Examples 5.1.1 and 5.1.3. To find the minimum-AICC model for the series $\{ Y _ { t } \}$ using ITSM, choose the option Model>Estimation>Autofit. Using the default range for both $p$ and

q, and clicking on Start, we quickly find that the minimum AICC $\mathbf { A R M A } ( \boldsymbol { p } , \boldsymbol { q } )$ model with $\boldsymbol { p } ~ \le ~ 5$ and $q \ \leq \ 5$ is the AR(1) model defined by (5.2.13). The corresponding AICC value is 74.483. If we increase the upper limits for $p$ and $q$ , we obtain the same result.

# Example 5.2.5 The Lake Data

Using the option Model>Estimation>Autofit as in the previous example, we find that the minimum-AICC ARMA $( p , q )$ model for the mean-corrected lake data, $X _ { t } = Y _ { t } - 9 . 0 0 4 1$ , of Examples 5.1.6 and 5.1.7 is the ARMA(1,1) model

$$
X _ {t} - 0. 7 4 4 6 X _ {t - 1} = Z _ {t} + 0. 3 2 1 3 Z _ {t - 1}, \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 4 7 5 0). \tag {5.2.14}
$$

The estimated standard deviations of the two coefficient estimates $\hat { \phi }$ and $\hat { \theta }$ are found from ITSM to be 0.0773 and 0.1123, respectively. (The respective estimated standard deviations based on the large-sample approximation given in Example 5.2.3 are 0.0788 and 0.1119.) The corresponding $9 5 \%$ confidence bounds are therefore

$$
\phi : 0. 7 4 4 6 \pm 1. 9 6 \times 0. 0 7 7 3 = (0. 5 9 4 1, 0. 8 9 6 1),
$$

$$
\theta : 0. 3 2 0 8 \pm 1. 9 6 \times 0. 1 1 2 3 = (0. 1 0 0 7, 0. 5 4 0 9).
$$

The value of AICC for this model is 212.77, improving on the values for the preliminary models of Examples 5.1.4, 5.1.6, and 5.1.7.

# 5.3 Diagnostic Checking

Typically, the goodness of fit of a statistical model to a set of data is judged by comparing the observed values with the corresponding predicted values obtained from the fitted model. If the fitted model is appropriate, then the residuals should behave in a manner that is consistent with the model.

When we fit an $\mathbf { A R M A } ( p , q )$ $( p , q )$ model to a given series we determine the maximum likelihood estimators $\hat { \phi } , \hat { \pmb { \theta } }$ , and $\hat { \sigma } ^ { 2 }$ of the parameters $\phi , \theta$ , and $\sigma ^ { 2 }$ . In the course of this procedure the predicted values $\hat { X } _ { t } \big ( \hat { \phi } , \hat { \pmb \theta } \big )$ of $X _ { t }$ based on $X _ { 1 } , \dots , X _ { t - 1 }$ are computed for the fitted model. The residuals are then defined, in the notation of Section 3.3, by

$$
\hat {W} _ {t} = \left(X _ {t} - \hat {X} _ {t} (\hat {\phi}, \hat {\theta})\right) / \left(r _ {t - 1} (\hat {\phi}, \hat {\theta})\right) ^ {1 / 2}, \quad t = 1, \dots , n. \tag {5.3.1}
$$

If we were to assume that the maximum likelihood $\mathbf { A R M A } ( p , q )$ model is the true process generating $\{ X _ { t } \}$ , then we could say that $\left\{ \hat { W } _ { t } \right\} \ \sim \ \mathrm { \overrightarrow { W N } } \left( 0 , \hat { \sigma } ^ { 2 } \right)$ . However, to check the appropriateness of an $\mathbf { A R M A } ( p , q )$ model for the data we should assume only that $X _ { 1 } , \ldots , X _ { n }$ are generated by an $\mathbf { A R M A } ( p , q )$ process with unknown parameters $\phi , \theta$ , and $\sigma ^ { 2 }$ , whose maximum likelihood estimators are $\hat { \phi } , \hat { \pmb { \theta } }$ , and $\hat { \sigma } ^ { 2 }$ , respectively. Then it is not true that $\left\{ \hat { W } _ { t } \right\}$ is white noise. Nonetheless $\hat { W } _ { t }$ $\mathbf { \omega } _ { t } ^ { \prime } , t = 1 , \ldots , n$ , should have properties that are similar to those of the white noise sequence

$$
W _ {t} (\phi , \pmb {\theta}) = (X _ {t} - \hat {X} _ {t} (\phi , \pmb {\theta})) / (r _ {t - 1} (\phi , \pmb {\theta})) ^ {1 / 2}, \quad t = 1, \ldots , n.
$$

Moreover, $W _ { t } ( \phi , \pmb \theta )$ approximates the white noise term in the defining equation (5.1.1) in the sense that $E ( W _ { t } ( \phi , \pmb { \theta } ) - Z _ { t } ) ^ { 2 }  0$ as $t  \infty$ (Brockwell and Davis (1991), Section 8.11). Consequently, the properties of the residuals $\left\{ \hat { W } _ { t } \right\}$ should reflect those of the white noise sequence $\{ Z _ { t } \}$ generating the underlying $\mathbf { A R M A } ( p , q )$ process. In

particular, the sequence $\left\{ \hat { W } _ { t } \right\}$ should be approximately (1) uncorrelated if $\left\{ Z _ { t } \right\} \sim$ $\operatorname { W N } ( 0 , \sigma ^ { 2 } )$ , (2) independent if $\{ Z _ { t } \} \sim \ \mathrm { I I D } { \left( 0 , \sigma ^ { 2 } \right) }$ , and (3) normally distributed if $Z _ { t } \sim \mathbf { N } ( 0 , \sigma ^ { 2 } )$ .

The rescaled residuals $\hat { R } _ { t } , t = 1 , \dots , n$ , are obtained by dividing the residuals $\hat { W } _ { t } , t = 1 , \ldots , n$ , by the estimate $\hat { \sigma } = \sqrt { \big ( \sum _ { t = 1 } ^ { n } \hat { W } _ { t } ^ { 2 } \big ) / n }$ of the white noise standard deviation. Thus,

$$
\hat {R} _ {t} = \hat {W} _ {t} / \hat {\sigma}. \tag {5.3.2}
$$

If the fitted model is appropriate, the rescaled residuals should have properties similar to those of a $\mathbf { W N } ( 0 , 1 )$ sequence or of an iid(0,1) sequence if we make the stronger assumption that the white noise $\{ Z _ { t } \}$ driving the ARMA process is independent white noise.

The following diagnostic checks are all based on the expected properties of the residuals or rescaled residuals under the assumption that the fitted model is correct and that $\{ Z _ { t } \} \sim \mathrm { I I D } \left( 0 , \sigma ^ { 2 } \right)$ . They are the same tests introduced in Section 1.6.

# 5.3.1 The Graph of $\{ \hat { R } _ { t } , t = 1 , \dots , n \}$

If the fitted model is appropriate, then the graph of the rescaled residuals $\left\{ \hat { R } _ { t } , t = \right.$ $1 , \ldots , n \big \}$ should resemble that of a white noise sequence with variance one. While it is difficult to identify the correlation structure of $\left\{ \hat { R } _ { t } \right\}$ (or any time series for that matter) from its graph, deviations of the mean from zero are sometimes clearly indicated by a trend or cyclic component and nonconstancy of the variance by fluctuations in $\hat { R _ { t } }$ , whose magnitude depends strongly on t.

The rescaled residuals obtained from the ARMA(1,1) model fitted to the meancorrected lake data in Example 5.2.5 are displayed in Figure 5-5. The graph gives no indication of a nonzero mean or nonconstant variance, so on this basis there is no reason to doubt the compatibility of $\hat { R } _ { 1 } , \dots , \hat { R } _ { n }$ with unit-variance white noise.

The rescaled residuals after fitting the ARMA(1,1) model of Example 5.2.5 to the lake data

![](images/8f15320668fa8082e0a5b8c48f739f2691378d7b6b637b13eb14590a09dd76b0.jpg)  
Figure 5-5

The sample ACF of the residuals after fitting the ARMA(1,1) model of Example 5.2.5 to the lake data

![](images/206ccde4cdef0db053cc7389760f9c8ccb1d047409d3578b46b150b02cdad9d5.jpg)  
Figure 5-6

The next step is to check that the sample autocorrelation function of $\left\{ \hat { W } _ { t } \right\}$ (or equivalently of $\left\{ \hat { R } _ { t } \right\} )$ behaves as it should under the assumption that the fitted model is appropriate.

# 5.3.2 The Sample ACF of the Residuals

We know from Section 1.6 that for large $n$ the sample autocorrelations of an iid sequence $Y _ { 1 } , \dots , Y _ { n }$ with finite variance are approximately iid with distribution $\mathrm { N } ( 0 , 1 / n )$ . We can therefore test whether or not the observed residuals are consistent with iid noise by examining the sample autocorrelations of the residuals and rejecting the iid noise hypothesis if more than two or three out of 40 fall outside the bounds $\pm 1 . 9 6 / \sqrt { n }$ or if one falls far outside the bounds. (As indicated above, our estimated residuals will not be precisely iid even if the true model generating the data is as assumed. To correct for this the bounds $\pm 1 . 9 6 / \sqrt { n }$ should be modified to give a more precise test as in Box and Pierce (1970) and Brockwell and Davis (1991), Section 9.4.) The sample ACF and PACF of the residuals and the bounds $\pm 1 . 9 6 / \sqrt { n }$ can be viewed by pressing the second green button (Plot ACF/PACF of residuals) at the top of the ITSM window. Figure 5-6 shows the sample ACF of the residuals after fitting the ARMA(1,1) of Example 5.2.5 to the lake data. As can be seen from the graph, there is no cause to reject the fitted model on the basis of these autocorrelations.

# 5.3.3 Tests for Randomness of the Residuals

The tests (b), (c), (d), (e), and (f) of Section 1.6 can be carried out using the program ITSM by selecting Statistics $>$ Residual Analysis $>$ Tests of Randomness.

Applying these tests to the residuals from the ARMA(1,1) model for the meancorrected lake data (Example 5.2.5), and using the default value $h = 2 2$ suggested for the portmanteau tests, we obtain the following results:

RANDOMNESS TEST STATISTICS

LJUNG-BOX PORTM. = 10.23 CHISQUR(20) p=0.964

MCLEOD-LI PORTM. = 16.55 CHISQUR(22) p=0.788

TURNING POINTS $= 6 9$ ANORMAL(64.0, 4.14**2) p=0.227

DIFFERENCE-SIGN = 50 ANORMAL(48.5, 2.87**2) $_ { \mathrm { p = 0 } . 6 0 2 }$

RANK TEST = 2083 ANORMAL(2376, 488.7**2) $_ { \mathrm { p = 0 . 0 7 2 } }$

JARQUE-BER $\mathord { \mathrm { \Omega } } = 0 . 2 8 5$ CHISQUR(2) $\mathrm { p { = } } 0 . 8 6 7$

ORDER OF MIN AICC YW MODEL FOR RESIDUALS = 0

This table shows the observed values of the statistics defined in Section 1.6, with each followed by its large-sample distribution under the null hypothesis of iid residuals, and the corresponding $p$ -values. The observed values can thus be checked easily for compatibility with their distributions under the null hypothesis. Since all of the $p$ - values are greater than 0.05, none of the test statistics leads us to reject the null hypothesis at this level. The order of the minimum AICC autoregressive model for the residuals also suggests the compatibility of the residuals with white noise.

A rough check for normality is provided by visual inspection of the histogram of the rescaled residuals, obtained by selecting the third green button at the top of the ITSM window. A Gaussian qq-plot of the residuals can also be plotted by selecting Statistics $>$ Residual Analysis $>$ QQ-Plot (normal). No obvious deviation from normality is apparent in either the histogram or the qq-plot. The Jarque-Bera statistic, $n [ m _ { 3 } ^ { 2 } / ( 6 m _ { 2 } ^ { 3 } ) + ( m _ { 4 } / m _ { 2 } ^ { 3 } - 3 ) ^ { 2 } / 2 4 ]$ , where $\begin{array} { r } { m _ { r } = \sum _ { j = 1 } ^ { n } ( \bar { Y } _ { j } - \bar { Y } ) ^ { r } / n } \end{array}$ , is distributed asymptotically as $\chi ^ { 2 } ( 2 )$ if $\{ Y _ { t } \} \sim \operatorname { I I D } \mathrm { N } ( \mu , \sigma ^ { 2 } )$ . This hypothesis is rejected if the statistic is sufficiently large (at level $\alpha$ if the $p$ -value of the test is less than $\alpha$ ). In this case the large $p$ -value computed by ITSM provides no evidence for rejecting the normality hypothesis.

# 5.4 Forecasting

Once a model has been fitted to the data, forecasting future values of the time series can be carried out using the method described in Section 3.3. We illustrate this method with one of the examples from Section 3.2.

Example 5.4.1 For the overshort data $\{ X _ { t } \}$ of Example 3.2.8, selection of the options Model> Estimation >Preliminary, the innovations algorithm, and then Model> Estimation>Max likelihood, gives the maximum likelihood MA(1) model for $\{ X _ { t } \}$ ,

$$
X _ {t} + 4. 0 3 5 = Z _ {t} - 0. 8 1 8 Z _ {t - 1}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 2 0 4 0. 7 5). \tag {5.4.1}
$$

To predict the next 7 days of overshorts, we treat (5.4.1) as the true model for the data, and use the results of Example 3.3.3 with $\phi = 0$ . From (3.3.11), the predictors are given by

$$
\begin{array}{l} P _ {5 7} X _ {5 7 + h} = - 4. 0 3 5 + \sum_ {j = h} ^ {1} \theta_ {5 7 + h - 1, j} \left(X _ {5 7 + h - j} - \hat {X} _ {5 7 + h - j}\right) \\ = \left\{ \begin{array}{l l} - 4. 0 3 5 + \theta_ {5 7, 1} \left(X _ {5 7} - \hat {X} _ {5 7}\right), & \text {i f} h = 1, \\ - 4. 0 3 5, & \text {i f} h > 1, \end{array} \right. \\ \end{array}
$$

Table 5.1 Forecasts of the next seven observations of the overshort data of Example 3.2.8 using model (5.4.1)   

<table><tr><td>#</td><td>XHAT</td><td>SQRT (MSE)</td><td>XHAT + MEAN</td></tr><tr><td>58</td><td>1.0097</td><td>45.1753</td><td>-3.0254</td></tr><tr><td>59</td><td>0.0000</td><td>58.3602</td><td>-4.0351</td></tr><tr><td>60</td><td>0.0000</td><td>58.3602</td><td>-4.0351</td></tr><tr><td>61</td><td>0.0000</td><td>58.3602</td><td>-4.0351</td></tr><tr><td>62</td><td>0.0000</td><td>58.3602</td><td>-4.0351</td></tr><tr><td>63</td><td>0.0000</td><td>58.3602</td><td>-4.0351</td></tr><tr><td>64</td><td>0.0000</td><td>58.3602</td><td>-4.0351</td></tr></table>

with mean squared error

$$
E (X _ {5 7 + h} - P _ {5 7} X _ {5 7 + h}) ^ {2} = \left\{ \begin{array}{l l} 2 0 4 0. 7 5 r _ {5 7}, & \text {i f} h = 1, \\ 2 0 4 0. 7 5 (1 + (- 0. 8 1 8) ^ {2}), & \text {i f} h > 1, \end{array} \right.
$$

where $\theta _ { 5 7 , 1 }$ and $r _ { 5 7 }$ are computed recursively from (3.3.9) with $\theta = - 0 . 8 1 8$ .

These calculations are performed with ITSM by fitting the maximum likelihood model (5.4.1), selecting Forecasting>ARMA, and specifying the number of forecasts required. The 1-step, 2-step, . . . , and 7-step forecasts of $X _ { t }$ are shown in Table 5.1. Notice that the predictor of $X _ { t }$ for $t \geq 5 9$ is equal to the sample mean, since under the MA(1) model $\{ X _ { t } , t \ge 5 9 \}$ is uncorrelated with $\{ X _ { t } , t \le 5 7 \}$ }.

Assuming that the innovations $\{ Z _ { t } \}$ are normally distributed, an approximate $95 \%$ prediction interval for $X _ { 6 4 }$ is given by

$$
- 4. 0 3 5 1 \pm 1. 9 6 \times 5 8. 3 6 0 2 = (- 1 1 8. 4 2, 1 1 0. 3 5).
$$

The mean squared errors of prediction, as computed in Section 3.3 and the example above, are based on the assumption that the fitted model is in fact the true model for the data. As a result, they do not reflect the variability in the estimation of the model parameters. To illustrate this point, suppose the data $X _ { 1 }$ , . . . , $X _ { n }$ are generated from the causal AR(1) model

$$
X _ {t} = \phi X _ {t - 1} + Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {i i d} \left(0, \sigma^ {2}\right).
$$

If $\hat { \phi }$ is the maximum likelihood estimate of $\phi$ , based on $X _ { 1 } , \ldots , X _ { n }$ , then the one-step ahead forecast of $X _ { n + 1 }$ is $\hat { \phi } X _ { n }$ , which has mean squared error

$$
E \left(X _ {n + 1} - \hat {\phi} X _ {n}\right) ^ {2} = E \left(\left(\phi - \hat {\phi}\right) X _ {n} + Z _ {n + 1}\right) ^ {2} = E \left(\left(\phi - \hat {\phi}\right) X _ {n}\right) ^ {2} + \sigma^ {2}. \tag {5.4.2}
$$

The second equality follows from the independence of $Z _ { n + 1 }$ and $\left( { \hat { \phi } } , X _ { n } \right) ^ { \prime } .$ To evaluate the first term in (5.4.2), first condition on $X _ { n }$ and then use the approximations

$$
E \left(\left(\phi - \hat {\phi}\right) ^ {2} \mid X _ {n}\right) \approx E \left(\phi - \hat {\phi}\right) ^ {2} \approx \left(1 - \phi^ {2}\right) / n,
$$

where the second relation comes from the formula for the asymptotic variance of $\hat { \phi }$ given by $\sigma ^ { 2 } \Gamma _ { 1 } ^ { - 1 } = \left( 1 - \phi ^ { 2 } \right)$ (see Example 5.2.1). The one-step mean squared error is then approximated by

$$
E \left(\phi - \hat {\phi}\right) ^ {2} E X _ {n} ^ {2} + \sigma^ {2} \approx n ^ {- 1} \left(1 - \phi^ {2}\right) \left(1 - \phi^ {2}\right) ^ {- 1} \sigma^ {2} + \sigma^ {2} = \frac {n + 1}{n} \sigma^ {2}.
$$

Thus, the error in parameter estimation contributes the term $\sigma ^ { 2 } / n$ to the mean squared error of prediction. If the sample size is large, this factor is negligible, and so for the purpose of mean squared error computation, the estimated parameters can be treated as the true model parameters. On the other hand, for small sample sizes, ignoring parameter variability can lead to a severe underestimate of the actual mean squared error of the forecast.

# 5.5 Order Selection

Once the data have been transformed (e.g., by some combination of Box–Cox and differencing transformations or by removal of trend and seasonal components) to the point where the transformed series $\{ X _ { t } \}$ can potentially be fitted by a zero-mean ARMA model, we are faced with the problem of selecting appropriate values for the orders $p$ and $q$ .

It is not advantageous from a forecasting point of view to choose $p$ and $q$ arbitrarily large. Fitting a very high order model will generally result in a small estimated white noise variance, but when the fitted model is used for forecasting, the mean squared error of the forecasts will depend not only on the white noise variance of the fitted model but also on errors arising from estimation of the parameters of the model (see the paragraphs following Example 5.4.1). These will be larger for higherorder models. For this reason we need to introduce a “penalty factor” to discourage the fitting of models with too many parameters.

Many criteria based on such penalty factors have been proposed in the literature, since the problem of model selection arises frequently in statistics, particularly in regression analysis. We shall restrict attention here to a brief discussion of the FPE, AIC, and BIC criteria of Akaike and a bias-corrected version of the AIC known as the AICC.

# 5.5.1 The FPE Criterion

The FPE criterion was developed by Akaike (1969) to select the appropriate order of an AR process to fit to a time series $\{ X _ { 1 } , \ldots , X _ { n } \}$ . Instead of trying to choose the order $p$ to make the estimated white noise variance as small as possible, the idea is to choose the model for $\{ X _ { t } \}$ in such a way as to minimize the one-step mean squared error when the model fitted to $\{ X _ { t } \}$ is used to predict an independent realization $\{ Y _ { t } \}$ of the same process that generated $\{ X _ { t } \}$ .

Suppose then that $\{ X _ { 1 } , \ldots , X _ { n } \}$ is a realization of an $\operatorname { A R } ( p )$ process with coefficients $\bar { \phi } _ { 1 } , \ldots , \phi _ { p } , p < n$ , and that $\{ Y _ { 1 } , \ldots , Y _ { n } \}$ is an independent realization of the same process. If $\hat { \phi } _ { 1 } , \ldots , \hat { \phi } _ { p }$ , are the maximum likelihood estimators of the coefficients based on $\{ X _ { 1 } , \ldots , X _ { n } \}$ and if we use these to compute the one-step predictor $\hat { \phi } _ { 1 } Y _ { n } +$ $\cdots + \hat { \phi } _ { p } Y _ { n + 1 - p }$ of $Y _ { n + 1 }$ , then the mean square prediction error is

$$
\begin{array}{l} E \left(Y _ {n + 1} - \hat {\phi} _ {1} Y _ {n} - \dots - \hat {\phi} _ {p} Y _ {n + 1 - p}\right) ^ {2} \\ = E \left[ Y _ {n + 1} - \phi_ {1} Y _ {n} - \dots - \phi_ {p} Y _ {n + 1 - p} - \left(\hat {\phi} _ {1} - \phi_ {1}\right) Y _ {n} - \dots - \left(\hat {\phi} _ {p} - \phi_ {p}\right) Y _ {n + 1 - p} \right] ^ {2} \\ = \sigma^ {2} + E \left[ \left(\hat {\phi} _ {p} - \phi_ {p}\right) ^ {\prime} \left[ Y _ {n + 1 - i} Y _ {n + 1 - j} \right] _ {i, j = 1} ^ {p} \left(\hat {\phi} _ {p} - \phi\right) \right], \\ \end{array}
$$

Table 5.2 $\hat { \sigma } _ { p } ^ { 2 }$ and $\mathsf { F P E } _ { p }$ for $\mathsf { A R } ( p )$ models fitted to the lake data   

<table><tr><td>p</td><td>σp2</td><td>FPEp</td></tr><tr><td>0</td><td>1.7203</td><td>1.7203</td></tr><tr><td>1</td><td>0.5097</td><td>0.5202</td></tr><tr><td>2</td><td>0.4790</td><td>0.4989</td></tr><tr><td>3</td><td>0.4728</td><td>0.5027</td></tr><tr><td>4</td><td>0.4708</td><td>0.5109</td></tr><tr><td>5</td><td>0.4705</td><td>0.5211</td></tr><tr><td>6</td><td>0.4705</td><td>0.5318</td></tr><tr><td>7</td><td>0.4679</td><td>0.5399</td></tr><tr><td>8</td><td>0.4664</td><td>0.5493</td></tr><tr><td>9</td><td>0.4664</td><td>0.5607</td></tr><tr><td>10</td><td>0.4453</td><td>0.5465</td></tr></table>

where $\phi _ { p } ^ { \prime } = ( \phi _ { 1 } , \ldots , \phi _ { p } ) ^ { \prime }$ , $\hat { \phi } _ { p } ^ { \prime } = \left( \hat { \phi } _ { 1 } , \ldots , \hat { \phi } _ { p } \right) ^ { \prime }$ , and $\sigma ^ { 2 }$ is the white noise variance of the $\operatorname { A R } ( p )$ model. Writing the last term in the preceding equation as the expectation of the conditional expectation given $X _ { 1 } , \ldots , X _ { n }$ , and using the independence of $\{ X _ { 1 } , \ldots , X _ { n } \}$ and $\{ Y _ { 1 } , \ldots , Y _ { n } \}$ , we obtain

$$
E \left(Y _ {n + 1} - \hat {\phi} _ {1} Y _ {n} - \dots - \hat {\phi} _ {p} Y _ {n + 1 - p}\right) ^ {2} = \sigma^ {2} + E \left[ \left(\hat {\phi} _ {p} - \phi_ {p}\right) ^ {\prime} \Gamma_ {p} \left(\hat {\phi} _ {p} - \phi\right) \right],
$$

where $\Gamma _ { p } ~ = ~ E [ Y _ { i } Y _ { j } ] _ { i , j = 1 } ^ { p }$ . We can approximate the last term by assuming that the random variable $n ^ { - 1 / 2 } \left( \hat { \phi } _ { p } - \phi _ { p } \right)$ has its large-sample distribution $\mathrm { N } \big (                             0 , \sigma ^ { 2 } \Gamma _ { p } ^ { - 1 } \big )$ as given in Example 5.21. Using Problem 5.13, we then find that

$$
E \left(Y _ {n + 1} - \hat {\phi} _ {1} Y _ {n} - \dots - \hat {\phi} _ {p} Y _ {n + 1 - p}\right) ^ {2} \approx \sigma^ {2} \left(1 + \frac {p}{n}\right). \tag {5.5.1}
$$

If $\hat { \sigma } ^ { 2 }$ is the maximum likelihood estimator of $\sigma ^ { 2 }$ , then for large $n , n \hat { \sigma } ^ { 2 } / \sigma ^ { 2 }$ is distributed approximately as chi-squared with $( n - p )$ degrees of freedom (see Brockwell and Davis (1991), Section 8.9). We therefore replace $\sigma ^ { 2 }$ in (5.5.1) by the estimator $n \hat { \sigma } ^ { 2 } / ( n - p )$ to get the estimated mean square prediction error of $Y _ { n + 1 }$ ,

$$
\mathrm {F P E} _ {p} = \hat {\sigma} ^ {2} \frac {n + p}{n - p}. \tag {5.5.2}
$$

To apply the FPE criterion for autoregressive order selection we therefore choose the value of $p$ that minimizes $\mathrm { F P E } _ { p }$ as defined in (5.5.2).

# Example 5.5.1 FPE-Based Selection of an AR Model for the Lake Data

In Example 5.1.4 we fitted AR(2) models to the mean-corrected lake data, the order 2 being suggested by the sample PACF shown in Figure 5-4. To use the FPE criterion to select $p$ , we have shown in Table 5.2 the values of FPE for values of $p$ from 0 to 10. These values were found using ITSM by fitting maximum likelihood AR models with the option Model>Estimation>Max likelihood. Also shown in the table are the values of the maximum likelihood estimates of $\sigma ^ { 2 }$ for the same values of $p$ . Whereas $\hat { \sigma } _ { p } ^ { 2 }$ decreases steadily with $p$ , the values of $\mathrm { F P E } _ { p }$ have a clear minimum at $p = 2$ , confirming our earlier choice of $p = 2$ as the most appropriate for this data set.

# 5.5.2 The AICC Criterion

A more generally applicable criterion for model selection than the FPE is the information criterion of Akaike (1973), known as the AIC. This was designed to be an approximately unbiased estimate of the Kullback–Leibler index of the fitted model relative to the true model (defined below). Here we use a bias-corrected version of the AIC, referred to as the AICC, suggested by Hurvich and Tsai (1989).

If $\mathbf { X }$ is an $n$ -dimensional random vector whose probability density belongs to the family $\{ f ( \cdot ; \psi ) , \psi \in \Psi \}$ , the Kullback–Leibler discrepancy between $f ( \cdot ; \psi )$ and $f ( \cdot ; \theta )$ is defined as

$$
d (\psi | \theta) = \Delta (\psi | \theta) - \Delta (\theta | \theta),
$$

where

$$
\Delta (\psi | \theta) = E _ {\theta} (- 2 \ln f (\mathbf {X}; \psi)) = \int_ {\mathbb {R} ^ {n}} - 2 \ln (f (\mathbf {x}; \psi)) f (\mathbf {x}; \theta) d \mathbf {x}
$$

is the Kullback–Leibler index of $f ( \cdot ; \psi )$ relative to $f ( \cdot ; \theta )$ . (Note that in general, $\Delta ( \psi | \theta ) \neq \Delta ( \theta | \psi )$ .) By Jensen’s inequality (see, e.g., Mood et al., 1974),

$$
\begin{array}{l} d (\psi | \theta) = \int_ {\mathbb {R} ^ {n}} - 2 \ln \left(\frac {f (\mathbf {x} ; \psi)}{f (\mathbf {x} ; \theta)}\right) f (\mathbf {x}; \theta) d \mathbf {x} \\ \geq - 2 \ln \left(\int_ {\mathbb {R} ^ {n}} \frac {f (\mathbf {x} ; \psi)}{f (\mathbf {x} ; \theta)} f (\mathbf {x}; \theta) d \mathbf {x}\right) \\ = - 2 \ln \left(\int_ {\mathbb {R} ^ {n}} f (\mathbf {x}; \psi) d \mathbf {x}\right) \\ = 0, \\ \end{array}
$$

with equality holding if and only if $f ( \mathbf { x } ; \psi ) = f ( \mathbf { x } ; \theta )$ .

Given observations $X _ { 1 } , \ldots , X _ { n }$ of an ARMA process with unknown parameters $\theta \ : = \ : \left( { \boldsymbol { \beta } } , \sigma ^ { 2 } \right)$ , the true model could be identified if it were possible to compute the Kullback–Leibler discrepancy between all candidate models and the true model. Since this is not possible, we estimate the Kullback–Leibler discrepancies and choose the model whose estimated discrepancy (or index) is minimum. In order to do this, we assume that the true model and the alternatives are all Gaussian. Then for any given $\theta = { \bigl ( } \beta , \sigma ^ { 2 } { \bigr ) } , f ( \cdot ; \theta )$ is the probability density of $( Y _ { 1 } , \ldots , Y _ { n } ) ^ { \prime }$ , where $\{ Y _ { t } \}$ is a Gaussian $\mathbf { A R M A } ( p , q )$ process with coefficient vector $\beta$ and white noise variance $\sigma ^ { 2 }$ . (The dependence of $\theta$ on $p$ and $q$ is through the dimension of the autoregressive and movingaverage coefficients in $\beta$ .)

Suppose, therefore, that our observations $X _ { 1 } , \ldots , X _ { n }$ are from a Gaussian ARMA process with parameter vector $\theta = \left( { \boldsymbol { \beta } } , \sigma ^ { 2 } \right)$ and assume for the moment that the true order is $( p , q )$ . Let $\hat { \theta } = \left( \hat { \beta } , \hat { \sigma } ^ { 2 } \right)$ be the maximum likelihood estimator of $\theta$ based on $X _ { 1 }$ , . . . , $X _ { n }$ and let $Y _ { 1 } , \dots , Y _ { n }$ $Y _ { n }$ be an independent realization of the true process (with parameter $\theta$ ). Then

$$
- 2 \ln L _ {Y} (\hat {\beta}, \hat {\sigma} ^ {2}) = - 2 \ln L _ {X} (\hat {\beta}, \hat {\sigma} ^ {2}) + \hat {\sigma} ^ {- 2} S _ {Y} (\hat {\beta}) - n,
$$

where $L _ { X } , L _ { Y } , S _ { X }$ , and $S _ { Y }$ are defined as in (5.2.9) and (5.2.11). Hence,

$$
E _ {\theta} (\Delta (\hat {\theta} | \theta)) = E _ {\beta , \sigma^ {2}} \left(- 2 \ln L _ {Y} (\hat {\beta}, \hat {\sigma} ^ {2})\right)
$$

$$
= E _ {\beta , \sigma^ {2}} \left(- 2 \ln L _ {X} (\hat {\beta}, \hat {\sigma} ^ {2})\right) + E _ {\beta , \sigma^ {2}} \left(\frac {S _ {Y} (\hat {\beta})}{\hat {\sigma} ^ {2}}\right) - n. \tag {5.5.3}
$$

It can be shown using large-sample approximations (see Brockwell and Davis (1991), Section 10.3 for details) that

$$
E _ {\beta , \sigma^ {2}} \left(\frac {S _ {Y} (\hat {\boldsymbol {\beta}})}{\hat {\boldsymbol {\sigma}} ^ {2}}\right) \approx \frac {2 (p + q + 1) n}{n - p - q - 2},
$$

from which we see that $- 2 \ln L _ { X } \left( \hat { \beta } , \hat { \sigma } ^ { 2 } \right) + 2 ( p + q + 1 ) n / ( n - p - q - 2 )$ is an approximately unbiased estimator of the expected Kullback–Leibler index $E _ { \theta } \big ( \Delta \big ( \hat { \theta } | \theta \big ) \big )$ in (5.5.3). Since the preceding calculations (and the maximum likelihood estimators $\hat { \boldsymbol \beta }$ and $\hat { \sigma } ^ { 2 }$ ) are based on the assumption that the true order is $( p , q )$ , we therefore select the values of $p$ and $q$ for our fitted model to be those that minimize $\operatorname { A I C C } ( { \hat { \boldsymbol { \beta } } } )$ , where

$$
\operatorname {A I C C} (\beta) := - 2 \ln L _ {X} (\beta , S _ {X} (\beta) / n) + 2 (p + q + 1) n / (n - p - q - 2). \tag {5.5.4}
$$

The AIC statistic, defined as

$$
\operatorname {A I C} (\beta) := - 2 \ln L _ {X} (\beta , S _ {X} (\beta) / n) + 2 (p + q + 1),
$$

can be used in the same way. Both $\operatorname { A I C C } ( \beta , \sigma ^ { 2 } )$ and $\operatorname { A I C } ( \beta , \sigma ^ { 2 } )$ can be defined for arbitrary $\sigma ^ { 2 }$ by replacing $S _ { X } ( \beta ) / n$ in the preceding definitions by $\sigma ^ { 2 }$ . The value $S _ { X } ( \beta ) / n$ is used in (5.5.4), since $\operatorname { \ u c c } ( \beta , \sigma ^ { 2 } )$ (like $\operatorname { A I C } ( \beta , \sigma ^ { 2 } ) )$ ) is minimized for any given $\beta$ by setting $\sigma ^ { 2 } = S _ { X } ( \beta ) / n$ .

For fitting autoregressive models, Monte Carlo studies (Jones 1975; Shibata 1976) suggest that the AIC has a tendency to overestimate $p$ . The penalty factors $2 ( p + q +$ $1 ) n / ( n - p - q - 2 )$ and $2 ( p + q + 1 )$ for the AICC and AIC statistics are asymptotically equivalent as $n  \infty$ . The AICC statistic, however, has a more extreme penalty for large-order models, which counteracts the overfitting tendency of the AIC. The BIC is another criterion that attempts to correct the overfitting nature of the AIC. For a zero-mean causal invertible $\mathbf { A R M A } ( p , q )$ process, it is defined (Akaike 1978) to be

$$
\begin{array}{l} \mathrm {B I C} = (n - p - q) \ln \left[ n \hat {\sigma} ^ {2} / (n - p - q) \right] + n \left(1 + \ln \sqrt {2 \pi}\right) \\ + (p + q) \ln \left[ \left(\sum_ {t = 1} ^ {n} X _ {t} ^ {2} - n \hat {\sigma} ^ {2}\right) / (p + q) \right], \tag {5.5.5} \\ \end{array}
$$

where $\hat { \sigma } ^ { 2 }$ is the maximum likelihood estimate of the white noise variance.

The BIC is a consistent order-selection criterion in the sense that if the data $\{ X _ { 1 } , \ldots , X _ { n } \}$ are in fact observations of an $\mathbf { A R M A } ( p , q )$ process, and if $\hat { p }$ and $\hat { \boldsymbol { q } }$ are the estimated orders found by minimizing the BIC, then $\hat { p }  p$ and $\hat { q }  q$ with probability 1 as $n  \infty$ (Hannan 1980). This property is not shared by the AICC or AIC. On the other hand, order selection by minimization of the AICC, AIC, or FPE is asymptotically efficient for autoregressive processes, while order selection by BIC minimization is not (Shibata 1980; Hurvich and Tsai 1989). Efficiency is a desirable property defined in terms of the one-step mean square prediction error achieved by the fitted model. For more details see Brockwell and Davis (1991), Section 10.3.

In the modeling of real data there is rarely such a thing as the “true order.” For the process $\begin{array} { r } { X _ { t } = \sum _ { j = 0 } ^ { \infty } \psi _ { j } Z _ { t - j } } \end{array}$ there may be many polynomials $\theta ( z ) , \phi ( z )$ such that the coefficients of $z ^ { j }$ in $\theta ( z ) / \phi ( z )$ closely approximate $\psi _ { j }$ for moderately small values of $j$ . Correspondingly, there may be many ARMA processes with properties similar to $\{ X _ { t } \}$ . This problem of identifiability becomes much more serious for multivariate processes. The AICC criterion does, however, provide us with a rational criterion for choosing among competing models. It has been suggested (Duong 1984) that models with AIC values within $c$ of the minimum value should be considered competitive (with $c = 2$ as a typical value). Selection from among the competitive models can then be based on such factors as whiteness of the residuals (Section 5.3) and model simplicity.

We frequently need, particularly in analyzing seasonal data, to fit $\mathbf { A R M A } ( p , q )$ models in which all except $m ( \leq p + q )$ of the coefficients are constrained to be zero. In such cases the definition (5.5.4) is replaced by

$$
\operatorname {A I C C} (\beta) := - 2 \ln L _ {X} (\beta , S _ {X} (\beta) / n) + 2 (m + 1) n / (n - m - 2). \tag {5.5.6}
$$

# Example 5.5.2 Models for the Lake Data

In Example 5.2.4 we found that the minimum-AICC ARMA $( p , q )$ model for the meancorrected lake data is the ARMA(1,1) model (5.2.14). For this model ITSM gives the values $\mathrm { A I C C } = 2 1 2 . 7 7$ and $\mathrm { B I C } = 2 1 6 . 8 6$ . A systematic check on $\mathbf { A R M A } ( p , q )$ models for other values of $p$ and $q$ shows that the model (5.2.14) also minimizes the BIC statistic. The minimum-AICC $\operatorname { A R } ( p )$ model is found to be the AR(2) model satisfying

$$
X _ {t} - 1. 0 4 4 1 X _ {t - 1} + 0. 2 5 0 3 X _ {t - 2} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 4 7 8 9),
$$

with $\mathrm { A I C C } = 2 1 3 . 5 4$ and $\mathrm { B I C } = 2 1 7 . 6 3$ . Both the AR(2) and ARMA(1,1) models pass the diagnostic checks of Section 5.3, and in view of the small difference between the AICC values there is no strong reason to prefer one model or the other.

# Problems

5.1 The sunspot numbers $\{ X _ { t } , t ~ = ~ 1 , \ldots , 1 0 0 \}$ , filed as SUNSPOTS.TSM, have sample autocovariances $\hat { \gamma } ( 0 ) = 1 3 8 2 . 2$ , $\hat { \gamma } ( 1 ) = 1 1 1 4 . 4$ , $\hat { \gamma } ( 2 ) = 5 9 1 . 7 3$ , and $\hat { \gamma } ( 3 ) = 9 6 . 2 1 6$ . Use these values to find the Yule–Walker estimates of $\phi _ { 1 }$ , φ2, and $\sigma ^ { 2 }$ in the model

$$
Y _ {t} = \phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} \big (0, \sigma^ {2} \big),
$$

for the mean-corrected series $Y _ { t } ~ = ~ X _ { t } - 4 6 . 9 3 , t ~ = ~ 1 , \dots , 1 0 0$ . Assuming that the data really are a realization of an AR(2) process, find $9 5 \%$ confidence intervals for $\phi _ { 1 }$ and $\phi _ { 2 }$ .

5.2 From the information given in the previous problem, use the Durbin–Levinson algorithm to compute the sample partial autocorrelations $\hat { \phi } _ { 1 1 } , \hat { \phi } _ { 2 2 }$ , and $\hat { \phi } _ { 3 3 }$ of the sunspot series. Is the value of $\hat { \phi } _ { 3 3 }$ compatible with the hypothesis that the data are generated by an AR(2) process? (Use significance level 0.05.)

5.3 Consider the AR(2) process $\{ X _ { t } \}$ satisfying

$$
X _ {t} - \phi X _ {t - 1} - \phi^ {2} X _ {t - 2} = Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

a. For what values of $\phi$ is this a causal process?   
b. The following sample moments were computed after observing $X _ { 1 } , . . . , X _ { 2 0 0 }$ :

$$
\hat {\gamma} (0) = 6. 0 6, \quad \hat {\rho} (1) = 0. 6 8 7.
$$

Find estimates of $\phi$ and $\sigma ^ { 2 }$ by solving the Yule–Walker equations. (If you find more than one solution, choose the one that is causal.)

5.4 Two hundred observations of a time series, $X _ { 1 } , \ldots , X _ { 2 0 0 }$ $X _ { 2 0 0 }$ , gave the following sample statistics:

sample mean: $\overline { { x } } _ { 2 0 0 } = 3 . 8 2 ; $

sample variance: $\hat { \gamma } ( 0 ) = 1 . 1 5 ;$ ;

sample ACF: ${ \hat { \rho } } ( 1 ) = 0 . 4 2 7 ;$

$$
\hat {\rho} (2) = 0. 4 7 5;
$$

$$
\hat {\rho} (3) = 0. 1 6 9.
$$

a. Based on these sample statistics, is it reasonable to suppose that $\{ X _ { t } - \mu \}$ is white noise?   
b. Assuming that $\{ X _ { t } - \mu \}$ can be modeled as the AR(2) process

$$
X _ {t} - \mu - \phi_ {1} \left(X _ {t - 1} - \mu\right) - \phi_ {2} \left(X _ {t - 2} - \mu\right) = Z _ {t},
$$

where $\{ Z _ { t } \} \sim \mathrm { I I D } \left( 0 , \sigma ^ { 2 } \right)$ , find estimates of $\mu , \phi _ { 1 } , \phi _ { 2 }$ , and $\sigma ^ { 2 }$

c. Would you conclude that $\mu = 0$ ?   
d. Construct $9 5 \%$ confidence intervals for $\phi _ { 1 }$ and $\phi _ { 2 }$   
e. Assuming that the data were generated from an AR(2) model, derive estimates of the PACF for all lags $h \geq 1$ .

5.5 Use the program ITSM to simulate and file 20 realizations of length 200 of the Gaussian MA(1) process

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 1),
$$

with $\theta = 0 . 6$ .

a. For each series find the moment estimate of $\theta$ as defined in Example 5.1.2.   
b. For each series use the innovations algorithm in the ITSM option Model> Estimation>Preliminary to find an estimate of $\theta$ . (Use the default value of the parameter m.) As soon as you have found this preliminary estimate for a particular series, select Model>Estimation>Max likelihood to find the maximum likelihood estimate of $\theta$ for the series.   
c. Compute the sample means and sample variances of your three sets of estimates.   
d. Use the asymptotic formulae given at the end of Section 5.1.1 (with $n \ =$ 200) to compute the variances of the moment, innovation, and maximum likelihood estimators ofθ. Compare with the corresponding sample variances found in (c).   
e. What do the results of (c) suggest concerning the relative merits of the three estimators?

5.6 Establish the recursions (5.1.19) and (5.1.20) for the forward and backward prediction errors $u _ { i } ( t )$ and $\nu _ { i } ( t )$ in Burg’s algorithm.   
5.7 Derive the recursions for the Burg estimates $\phi _ { i i } ^ { ( B ) }$ φi and $\boldsymbol { \sigma } _ { i } ^ { ( B ) 2 }$ .   
5.8 From the innovation form of the likelihood (5.2.9) derive the equations (5.2.10), (5.2.11), and (5.2.12) for the maximum likelihood estimators of the parameters of an ARMA process.   
5.9 Use equation (5.2.9) to show that for $n > p$ , the likelihood of the observations $\{ X _ { 1 } , \ldots , X _ { n } \}$ of the causal $\operatorname { A R } ( p )$ process defined by

$$
X _ {t} = \phi_ {1} X _ {t - 1} + \dots + \phi_ {p} X _ {t - p} + Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, \sigma^ {2}),
$$

is

$$
\begin{array}{l} L \left(\phi , \sigma^ {2}\right) = \left(2 \pi \sigma^ {2}\right) ^ {- n / 2} (\det  G _ {p}) ^ {- 1 / 2} \\ \times \exp \left\{- \frac {1}{2 \sigma^ {2}} \left[ \mathbf {X} _ {p} ^ {\prime} G _ {p} ^ {- 1} \mathbf {X} _ {p} + \sum_ {t = p + 1} ^ {n} (X _ {t} - \phi_ {1} X _ {t - 1} - \dots - \phi_ {p} X _ {t - p}) ^ {2} \right] \right\}, \\ \end{array}
$$

where $\mathbf { X } _ { p } = ( X _ { 1 } , \ldots , X _ { p } ) ^ { \prime }$ and $G _ { p } = \sigma ^ { - 2 } \Gamma _ { p } = \sigma ^ { - 2 } E ( \mathbf { X } _ { p } \mathbf { X } _ { p } ^ { \prime } )$ .

5.10 Use the result of Problem 5.9 to derive a pair of linear equations for the least squares estimates of $\phi _ { 1 }$ and $\phi _ { 2 }$ for a causal AR(2) process (with mean zero). Compare your equations with those for the Yule–Walker estimates. (Assume that the mean is known to be zero in writing down the latter equations, so that the sample autocovariances are $\begin{array} { r } { \hat { \gamma } ( h ) = \frac { 1 } { n } \sum _ { t = 1 } ^ { n - h } X _ { t + h } X _ { t } } \end{array}$ for $h \geq 0$ .)   
5.11 Given two observations $x _ { 1 }$ and $x _ { 2 }$ from the causal AR(1) process satisfying

$$
X _ {t} = \phi X _ {t - 1} + Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right),
$$

and assuming that $| x _ { 1 } | \ \neq \ | x _ { 2 } |$ , find the maximum likelihood estimates of $\phi$ and $\sigma ^ { 2 }$ .

5.12 Derive a cubic equation for the maximum likelihood estimate of the coefficient $\phi$ of a causal AR(1) process based on the observations $X _ { 1 } , \ldots , X _ { n }$ $X _ { 1 }$ $X _ { n }$ .   
5.13 Use the result of Problem A.7 and the approximate large-sample normal distribution of the maximum likelihood estimator $\hat { \phi } _ { p }$ to establish the approximation (5.5.1).

# Nonstationary and Seasonal Time Series Models

6.1 ARIMA Models for Nonstationary Time Series   
6.2 Identification Techniques   
6.3 Unit Roots in Time Series Models   
6.4 Forecasting ARIMA Models   
6.5 Seasonal ARIMA Models   
6.6 Regression with ARMA Errors

In this chapter we shall examine the problem of finding an appropriate model for a given set of observations $\{ x _ { 1 } , \ldots , x _ { n } \}$ that are not necessarily generated by a stationary time series. If the data (a) exhibit no apparent deviations from stationarity and (b) have a rapidly decreasing autocovariance function, we attempt to fit an ARMA model to the mean-corrected data using the techniques developed in Chapter 5. Otherwise, we look first for a transformation of the data that generates a new series with the properties (a) and (b). This can frequently be achieved by differencing, leading us to consider the class of ARIMA (autoregressive integrated moving-average) models, defined in Section 6.1. We have in fact already encountered ARIMA processes. The model fitted in Example 5.1.1 to the Dow Jones Utilities Index was obtained by fitting an AR model to the differenced data, thereby effectively fitting an ARIMA model to the original series. In Section 6.1 we shall give a more systematic account of such models.

In Section 6.2 we discuss the problem of finding an appropriate transformation for the data and identifying a satisfactory $\mathbf { A R M A } ( p , q )$ $( p , q )$ model for the transformed data. The latter can be handled using the techniques developed in Chapter 5. The sample ACF and PACF and the preliminary estimators $\hat { \phi } _ { m }$ and $\widehat { \pmb { \theta } } _ { m }$ of Section 5.1 can provide useful guidance in this choice. However, our prime criterion for model selection will be the AICC statistic discussed in Section 5.5.2. To apply this criterion we compute maximum likelihood estimators of $\phi , \theta$ , and $\sigma ^ { 2 }$ for a variety of competing $p$ and $q$ values and choose the fitted model with smallest AICC value. Other techniques, in particular those that use the $R$ and $S$ arrays of Gray et al. (1978), are discussed in the survey of model identification by de Gooijer et al. (1985). If the fitted model is

satisfactory, the residuals (see Section 5.3) should resemble white noise. Tests for this were described in Section 5.3 and should be applied to the minimum AICC model to make sure that the residuals are consistent with their expected behavior under the model. If they are not, then competing models (models with AICC value close to the minimum) should be checked until we find one that passes the goodness of fit tests. In some cases a small difference in AICC value (say less than 2) between two satisfactory models may be ignored in the interest of model simplicity. In Section 6.3 we consider the problem of testing for a unit root of either the autoregressive or moving-average polynomial. An autoregressive unit root suggests that the data require differencing, and a moving-average unit root suggests that they have been overdifferenced. Section 6.4 considers the prediction of ARIMA processes, which can be carried out using an extension of the techniques developed for ARMA processes in Sections 3.3 and 5.4. In Section 6.5 we examine the fitting and prediction of seasonal ARIMA (SARIMA) models, whose analysis, except for certain aspects of model identification, is quite analogous to that of ARIMA processes. Finally, we consider the problem of regression, allowing for dependence between successive residuals from the regression. Such models are known as regression models with time series residuals and often occur in practice as natural representations for data containing both trend and serially dependent errors.

# 6.1 ARIMA Models for Nonstationary Time Series

We have already discussed the importance of the class of ARMA models for representing stationary series. A generalization of this class, which incorporates a wide range of nonstationary series, is provided by the ARIMA processes, i.e., processes that reduce to ARMA processes when differenced finitely many times.

# Definition 6.1.1

$$
\begin{array}{l} \text {I f d i s a n a n o n n e g a t i v e i n t e g e r , t h e n \{X _ {t} \} i s a n A R I M A (p , d , q) p r o c e s s i f Y _ {t} : =} \\ (1 - B) ^ {d} X _ {t} \text {i s a c a u s a l A R M A (p , q) p r o c e s s .} \end{array}
$$

This definition means that $\{ X _ { t } \}$ satisfies a difference equation of the form

$$
\phi^ {*} (B) X _ {t} \equiv \phi (B) (1 - B) ^ {d} X _ {t} = \theta (B) Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma^ {2}\right), \tag {6.1.1}
$$

where $\phi ( z )$ and $\theta ( z )$ are polynomials of degrees $p$ and $q$ , respectively, and $\phi ( z ) \neq 0$ for $| z | \leq 1$ . The polynomial $\phi ^ { * } ( z )$ has a zero of order $d$ at $z = 1$ . The process $\{ X _ { t } \}$ is stationary if and only if $d = 0$ , in which case it reduces to an $\mathbf { A R M A } ( p , q )$ process.

Notice that if $d \digamma \geq \digamma 1$ , we can add an arbitrary polynomial trend of degree $( d - 1 )$ to $\{ X _ { t } \}$ without violating the difference equation (6.1.1). ARIMA models are therefore useful for representing data with trend (see Sections 1.5 and 6.2). It should be noted, however, that ARIMA processes can also be appropriate for modeling series with no trend. Except when $d \ : = \ : 0$ , the mean of $\{ X _ { t } \}$ is not determined by equation (6.1.1), and it can in particular be zero (as in Example 1.3.3). Since for $d \geq 1$ , equation (6.1.1) determines the second-order properties of $\{ ( 1 - B ) ^ { d } X _ { t } \}$ but not those of $\{ X _ { t } \}$ (Problem 6.1), estimation of $\phi , \theta$ , and $\sigma ^ { 2 }$ will be based on the observed differences $( 1 - B ) ^ { d } X _ { t }$ . Additional assumptions are needed for prediction (see Section 6.4).

Example 6.1.1 $\{ X _ { t } \}$ is an ARIMA(1,1,0) process if for some $\phi \in ( - 1 , 1 )$ ,

$$
(1 - \phi B) (1 - B) X _ {t} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right).
$$

![](images/8f3f4addb0885e31999ce4ef6ad57b19a8184940f8d9625b7918d4edac5ceaf3.jpg)  
Figure 6-1 200 observations of the ARIMA(1,1,0) series $X _ { t }$ of Example 6.1.1

![](images/61623eaa90cfefac3c85d568a71c786ff00f59a65174cddfe4208d2b6be0e2cd.jpg)  
Figure 6-2 The sample ACF of the data in Figure 6-1

We can then write

$$
X _ {t} = X _ {0} + \sum_ {j = 1} ^ {t} Y _ {j}, \quad t \geq 1,
$$

where

$$
Y _ {t} = (1 - B) X _ {t} = \sum_ {j = 0} ^ {\infty} \phi^ {j} Z _ {t - j}.
$$

A realization of $\{ X _ { 1 } , \ldots , X _ { 2 0 0 } \}$ with $X _ { 0 } ~ = ~ 0$ , $\phi \ : = \ : 0 . 8$ , and $\sigma ^ { 2 } = 1$ is shown in Figure 6-1, with the corresponding sample autocorrelation and partial autocorrelation functions in Figures 6-2 and 6-3, respectively.

A distinctive feature of the data that suggests the appropriateness of an ARIMA model is the slowly decaying positive sample autocorrelation function in Figure 6-2.

![](images/305800b12cd0906f6e642995674afe485cf7e96a897eb2aa9dc9ec072cf1e0a3.jpg)  
Figure 6-3 The sample PACF of the data in Figure 6-1

![](images/742b4adae5a20f4b22c4ef174f6a9f892506e537ac1678ac902c9648ec00bd84.jpg)  
Figure 6-4 199 observations of the series $Y _ { t } = \nabla X _ { t }$ with $\{ X _ { t } \}$ as in Figure 6-1

If, therefore, we were given only the data and wished to find an appropriate model, it would be natural to apply the operator $\nabla = 1 - B$ repeatedly in the hope that for some $j , \{ \nabla ^ { j } X _ { t } \}$ will have a rapidly decaying sample autocorrelation function compatible with that of an ARMA process with no zeros of the autoregressive polynomial near the unit circle. For this particular time series, one application of the operator $\nabla$ produces the realization shown in Figure 6-4, whose sample ACF and PACF (Figures 6-5 and 6-6) suggest an AR(1) [or possibly AR(2)] model for $\{ \nabla X _ { t } \}$ . The maximum likelihood estimates of $\phi$ and $\sigma ^ { 2 }$ obtained from ITSM under the assumption that $E ( \nabla X _ { t } ) = 0$ (found by not subtracting the mean after differencing the data) are 0.808 and 0.978, respectively, giving the model

$$
(1 - 0. 8 0 8 B) (1 - B) X _ {t} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 9 7 8), \tag {6.1.2}
$$

which bears a close resemblance to the true underlying process,

$$
(1 - 0. 8 B) (1 - B) X _ {t} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 1). \tag {6.1.3}
$$

![](images/c343d46a29dfad8fa5cdf85bf4582a7b963e7ca4463a01b3872a683962a69a4e.jpg)  
Figure 6-5 The sample ACF of the series $\{ Y _ { t } \}$ in Figure 6-4

![](images/2ff4342b45c25da3c692cec35bd22e944680e49aa07081463082a7d819b18059.jpg)  
Figure 6-6 The sample PACF of the series $\{ Y _ { t } \}$ in Figure 6-4

Instead of differencing the series in Figure 6-1 we could proceed more directly by attempting to fit an AR(2) process as suggested by the sample PACF of the original series in Figure 6-3. Maximum likelihood estimation, carried out using ITSM after fitting a preliminary model with Burg’s algorithm and assuming that $E X _ { t } = 0$ , gives the model

$$
(1 - 1. 8 0 8 B + 0. 8 1 1 B ^ {2}) X _ {t} = (1 - 0. 8 2 5 B) (1 - 0. 9 8 3 B) X _ {t} = Z _ {t},
$$

$$
\left\{Z _ {t} \right\} \sim \operatorname {W N} (0, 0. 9 7 0), \tag {6.1.4}
$$

which, although stationary, has coefficients closely resembling those of the true nonstationary process (6.1.3). (To obtain the model (6.1.4), two optimizations were carried out using the Model>Estimation>Max likelihood option of ITSM, the first with the default settings and the second after setting the accuracy parameter to 0.00001.) From a sample of finite length it will be extremely difficult to distinguish between a nonstationary process such as (6.1.3), for which $\phi ^ { * } ( 1 ) = 0$ , and a process such as (6.1.4), which has very similar coefficients but for which $\phi ^ { * }$ has all of its

![](images/a07346de6a8305209d09c2ea6d4a85c6fac1a8465713ba6d99193040f3eb08e5.jpg)  
Figure 6-7 200 observations of the AR(2) process defined by (6.1.6) with $r = 1 . 0 0 5$ and $\omega = \pi / 3$

zeros outside the unit circle. In either case, however, if it is possible by differencing to generate a series with rapidly decaying sample ACF, then the differenced data set can be fitted by a low-order ARMA process whose autoregressive polynomial $\phi ^ { * }$ has zeros that are comfortably outside the unit circle. This means that the fitted parameters will be well away from the boundary of the allowable parameter set. This is desirable for numerical computation of parameter estimates and can be quite critical for some methods of estimation. For example, if we apply the Yule–Walker equations to fit an AR(2) model to the data in Figure 6-1, we obtain the model

$$
(1 - 1. 2 8 2 B + 0. 2 9 0 B ^ {2}) X _ {t} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 6. 4 3 5), \tag {6.1.5}
$$

which bears little resemblance to either the maximum likelihood model (6.1.4) or the true model (6.1.3). In this case the matrix $\hat { R } _ { 2 }$ appearing in (5.1.7) is nearly singular.

An obvious limitation in fitting an $\mathrm { A R I M A } ( p , d , q )$ process $\{ X _ { t } \}$ to data is that $\{ X _ { t } \}$ is permitted to be nonstationary only in a very special way, i.e., by allowing the polynomial $\phi ^ { * } ( B )$ in the representation $\phi ^ { * } ( B ) X _ { t } = Z _ { t }$ to have a zero of multiplicity $d$ at the point 1 on the unit circle. Such models are appropriate when the sample ACF is a slowly decaying positive function as in Figure 6-2, since sample autocorrelation functions of this form are associated with models $\phi ^ { * } ( B ) X _ { t } = \theta ( B ) Z _ { t }$ in which $\phi ^ { * }$ has a zero either at or close to 1.

Sample autocorrelations with slowly decaying oscillatory behavior as in Figure 6-8 are associated with models $\phi ^ { * } ( B ) X _ { t } = \theta ( B ) Z _ { t }$ in which $\phi ^ { * }$ has a zero close to $e ^ { i \omega }$ for some $\omega \in ( - \pi , \pi ]$ other than 0. Figure 6-8 is the sample ACF of the series of 200 observations in Figure 6-7, obtained from ITSM by simulating the AR(2) process

$$
X _ {t} - (2 r ^ {- 1} \cos \omega) X _ {t - 1} + r ^ {- 2} X _ {t - 2} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 1), \tag {6.1.6}
$$

with $r = 1 . 0 0 5$ and $\omega = \pi / 3$ , i.e.,

$$
X _ {t} - 0. 9 9 5 0 X _ {t - 1} + 0. 9 9 0 1 X _ {t - 2} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 1).
$$

The autocorrelation function of the model (6.1.6) can be derived by noting that

![](images/bbb9f35756ebba203c77ec8cad3eb8d0f87f25b25ec0a606166bb5c248fe289f.jpg)  
Figure 6-8 The sample ACF of the data in Figure 6-7

$$
1 - \left(2 r ^ {- 1} \cos \omega\right) B + r ^ {- 2} B ^ {2} = \left(1 - r ^ {- 1} e ^ {i \omega} B\right) \left(1 - r ^ {- 1} e ^ {- i \omega} B\right) \tag {6.1.7}
$$

and using (3.2.12). This gives

$$
\rho (h) = r ^ {- h} \frac {\sin (h \omega + \psi)}{\sin \psi}, \quad h \geq 0, \tag {6.1.8}
$$

where

$$
\tan \psi = \frac {r ^ {2} + 1}{r ^ {2} - 1} \tan \omega . \tag {6.1.9}
$$

It is clear from these equations that

$$
\rho (h) \rightarrow \cos (h \omega) \text {a s} r \downarrow 1. \tag {6.1.10}
$$

With $r = 1 . 0 0 5$ and $\omega = \pi / 3$ as in the model generating Figure 6-7, the model ACF (6.1.8) is a damped sine wave with damping ratio 1/1.005 and period 6. These properties are reflected in the sample ACF shown in Figure 6-8. For values of $r$ closer to 1, the damping will be even slower as the model ACF approaches its limiting form (6.1.10).

If we were simply given the data shown in Figure 6-7, with no indication of the model from which it was generated, the slowly damped sinusoidal sample ACF with period 6 would suggest trying to make the sample ACF decay more rapidly by applying the operator (6.1.7) with $r = 1$ and $\omega = \pi / 3$ , i.e., $\left( 1 - B + B ^ { 2 } \right)$ . If it happens, as in this case, that the period $2 \pi / \omega$ is close to some integer $s$ (in this case 6), then the operator $1 - B ^ { s }$ can also be applied to produce a series with more rapidly decaying autocorrelation function (see also Section 6.5). Figures 6-9 and 6-10 show the sample autocorrelation functions obtained after applying the operators $1 - B + B ^ { 2 }$ and $1 - B ^ { 6 }$ , respectively, to the data shown in Figure 6-7. For either one of these two differenced series, it is then not difficult to fit an ARMA model $\phi ( B ) X _ { t } = \theta ( B ) Z _ { t }$ for which the zeros of $\phi$ are well outside the unit circle. Techniques for identifying and determining such ARMA models have already been introduced in Chapter 5. For convenience we shall collect these together in the following sections with a number of illustrative examples.

![](images/d5b30b14b00bad685ab34ca3b5d854e77fee8dfb0520d58aa495e70fa4307bcf.jpg)  
Figure 6-9 The sample ACF of $( 1 - B + \dot { B } ^ { 2 } ) X _ { t }$ with $\{ X _ { t } \}$ as in Figure 6-7

![](images/a712f417360e12d0f73545113c0d51a4815bf1471ef3c745569b9cb6cdccf023.jpg)  
Figure 6-10 The sample ACF of $( 1 - B ^ { 6 } ) X _ { t }$ with $\{ X _ { t } \}$ as in Figure 6-7

# 6.2 Identification Techniques

(a) Preliminary Transformations. The estimation methods of Chapter 5 enable us to find, for given values of $p$ and $q$ , an $\mathbf { A R M A } ( p , q )$ model to fit a given series of data. For this procedure to be meaningful it must be at least plausible that the data are in fact a realization of an ARMA process and in particular a realization of a stationary process. If the data display characteristics suggesting nonstationarity (e.g., trend and seasonality), then it may be necessary to make a transformation so as to produce a new series that is more compatible with the assumption of stationarity.

Deviations from stationarity may be suggested by the graph of the series itself or by the sample autocorrelation function or both.

![](images/d8b63b0c42ed05c17ecdbf8b55ea98ba93d2c6ce9ddcc0c2a1bb335ce22e8c15.jpg)  
Figure 6-11 The Australian red wine data after taking natural logarithms and removing a seasonal component of period 12 and a linear trend

Inspection of the graph of the series will occasionally reveal a strong dependence of variability on the level of the series, in which case the data should first be transformed to reduce or eliminate this dependence. For example, Figure 1-1 shows the Australian monthly red wine sales from January 1980 through October 1991, and Figure 1-17 shows how the increasing variability with sales level is reduced by taking natural logarithms of the original series. The logarithmic transformation $V _ { t } = \ln U _ { t }$ used here is in fact appropriate whenever $\{ U _ { t } \}$ is a series whose standard deviation increases linearly with the mean. For a systematic account of a general class of variance-stabilizing transformations, we refer the reader to Box and Cox (1964). The defining equation for the general Box–Cox transformation $f _ { \lambda }$ is

$$
f _ {\lambda} (U _ {t}) = \left\{ \begin{array}{l l} \lambda^ {- 1} (U _ {t} ^ {\lambda} - 1), & U _ {t} \geq 0, \lambda > 0, \\ \ln U _ {t}, & U _ {t} > 0, \lambda = 0, \end{array} \right.
$$

and the program ITSM provides the option (Transform>Box-Cox) of applying $f _ { \lambda }$ (with $0 \leq \lambda \leq 1 . 5 )$ ) prior to the elimination of trend and/or seasonality from the data. In practice, if a Box–Cox transformation is necessary, it is often the case that either $f _ { 0 }$ or $f _ { 0 . 5 }$ is adequate.

Trend and seasonality are usually detected by inspecting the graph of the (possibly transformed) series. However, they are also characterized by autocorrelation functions that are slowly decaying and nearly periodic, respectively. The elimination of trend and seasonality was discussed in Section 1.5, where we described two methods:

(i) “classical decomposition” of the series into a trend component, a seasonal component, and a random residual component, and   
(ii) differencing.

The program ITSM (in the Transform option) offers a choice between these techniques. The results of applying methods (i) and (ii) to the transformed red wine data $V _ { t } = \ln U _ { t }$ in Figure 1-17 are shown in Figures 6-11 and 6-12, respectively. Figure 6-11 was obtained from ITSM by estimating and removing from $\{ V _ { t } \}$ a linear trend component and a seasonal component with period 12. Figure 6-12 was obtained by applying the operator $\left( 1 - B ^ { 1 2 } \right)$ to $\{ V _ { t } \}$ . Neither of the two resulting series displays

![](images/c7db440e292b9ea829eb618d9aef2a4ec5dbda3612ce56660e872bc9b67c8383.jpg)  
Figure 6-12 The Australian red wine data after taking natural logarithms and differencing at lag 12

any apparent deviations from stationarity, nor do their sample autocorrelation functions. The sample ACF and PACF of $\big \{ ( 1 - B ^ { 1 2 } ) V _ { t } \big \}$ are shown in Figures 6-13 and 6-14, respectively.

After the elimination of trend and seasonality, it is still possible that the sample autocorrelation function may appear to be that of a nonstationary (or nearly nonstationary) process, in which case further differencing may be carried out.

In Section 1.5 we also mentioned a third possible approach:

(iii) fitting a sum of harmonics and a polynomial trend to generate a noise sequence that consists of the residuals from the regression.

In Section 6.6 we discuss the modifications to classical least squares regression analysis that allow for dependence among the residuals from the regression. These modifications are implemented in the ITSM option Regression>Estimation> Generalized LS.

(b) Identification and Estimation. Let $\{ X _ { t } \}$ be the mean-corrected transformed series found as described in (a). The problem now is to find the most satisfactory ARMA( p, q) model to represent $\{ X _ { t } \}$ . If $p$ and $q$ were known in advance, this would be a straightforward application of the estimation techniques described in Chapter 5. However, this is usually not the case, so it becomes necessary also to identify appropriate values for $p$ and $q$ .

It might appear at first sight that the higher the values chosen for $p$ and $q$ , the better the resulting fitted model will be. However, as pointed out in Section 5.5, estimation of too large a number of parameters introduces estimation errors that adversely affect the use of the fitted model for prediction as illustrated in Section 5.4. We therefore minimize one of the model selection criteria discussed in Section 5.5 in order to choose the values of $p$ and $q$ . Each of these criteria includes a penalty term to discourage the fitting of too many parameters. We shall base our choice of $p$ and $q$ primarily on the minimization of the AICC statistic, defined as

$$
\operatorname {A I C C} (\phi , \theta) = - 2 \ln L (\phi , \theta , S (\phi , \theta) / n) + 2 (p + q + 1) n / (n - p - q - 2), \tag {6.2.1}
$$

![](images/ef256454742a2eddaa56e81724504eff98eb741fd18602c25d98fac764422648.jpg)  
Figure 6-13 The sample ACF of the data in Figure 6-12

![](images/e28e4e17012d6f7f422e7bf1fdf89dc3ad5d8c32813b47ec80b1f7c12e5ea6cf.jpg)  
Figure 6-14 The sample PACF of the data in Figure 6-12

where $L ( \phi , \theta , \sigma ^ { 2 } )$ is the likelihood of the data under the Gaussian ARMA model with parameters $\left( \phi , \theta , \sigma ^ { 2 } \right)$ , and $S ( \phi , \theta )$ is the residual sum of squares defined in (5.2.11). Once a model has been found that minimizes the AICC value, it is then necessary to check the model for goodness of fit (essentially by checking that the residuals are like white noise) as discussed in Section 5.3.

For any fixed values of $p$ and $q$ , the maximum likelihood estimates of $\phi$ and $\pmb \theta$ are the values that minimize the AICC. Hence, the minimum AICC model (over any given range of $p$ and $q$ values) can be found by computing the maximum likelihood estimators for each fixed $p$ and $q$ and choosing from these the maximum likelihood model with the smallest value of AICC. This can be done with the program ITSM by using the option Model>Estimation>Autofit. When this option is selected and upper and lower bounds for $p$ and $q$ are specified, the program fits maximum likelihood models for each pair $( p , q )$ in the range specified and

selects the model with smallest AICC value. If some of the coefficient estimates are small compared with their estimated standard deviations, maximum likelihood subset models (with those coefficients set to zero) can also be explored.

The steps in model identification and estimation can be summarized as follows:

• After transforming the data (if necessary) to make the fitting of an $\mathbf { A R M A } ( p , q )$ model reasonable, examine the sample ACF and PACF to get some idea of potential $p$ and $q$ values. Preliminary estimation using the ITSM option Model>Estimation>Preliminary is also useful in this respect. Burg’s algorithm with AICC minimization rapidly fits autoregressions of all orders up to 27 and selects the one with minimum AICC value. For preliminary estimation of models with $q > 0$ , each pair $( p , q )$ must be considered separately.   
Select the option Model>Estimation>Autofit of ITSM. Specify the required limits for $p$ and $q$ , and the program will then use maximum likelihood estimation to find the minimum AICC model with $p$ and $q$ in the range specified.   
• Examination of the fitted coefficients and their standard errors may suggest that some of them can be set to zero. If this is the case, then a subset model can be fitted by clicking on the button Constrain optimization in the Maximum Likelihood Estimation dialog box and setting the selected coefficients to zero. Optimization will then give the maximum likelihood model with the chosen coefficients constrained to be zero. The constrained model is assessed by comparing its AICC value with those of the other candidate models.   
• Check the candidate model(s) for goodness of fit as described in Section 5.3. These tests can be performed by selecting the option Statistics>Residual Analysis.

# Example 6.2.1 The Australian Red Wine Data

Let $\{ X _ { 1 } , \ldots , X _ { 1 3 0 } \}$ denote the series obtained from the red wine data of Example 1.1.1 after taking natural logarithms, differencing at lag 12, and subtracting the mean (0.0681) of the differences. The data prior to mean correction are shown in Figure 6-12. The sample PACF of $\{ X _ { t } \}$ , shown in Figure 6-14, suggests that an AR(12) model might be appropriate for this series. To explore this possibility we use the ITSM option Model>Estimation $. >$ Preliminary with Burg’s algorithm and AICC minimization. As anticipated, the fitted Burg models do indeed have minimum AICC when $p = 1 2$ . The fitted model is

$$
\begin{array}{l} \left(1 - 0. 2 4 5 B - 0. 0 6 9 B ^ {2} - 0. 0 1 2 B ^ {3} - 0. 0 2 1 B ^ {4} - 0. 2 0 0 B ^ {5} + 0. 0 2 5 B ^ {6} + 0. 0 0 4 B ^ {7} \right. \\ \left. - 0. 1 3 3 B ^ {8} + 0. 0 1 0 B ^ {9} - 0. 0 9 5 B ^ {1 0} + 0. 1 1 8 B ^ {1 1} + 0. 3 8 4 B ^ {1 2}\right) X _ {t} = Z _ {t}, \\ \end{array}
$$

with $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , 0 . 0 1 3 5 )$ and AICC value 158.77. Selecting the option Model> Estimation>Max likelihood then gives the maximum likelihood AR(12) model, which is very similar to the Burg model and has AICC value $- 1 5 8 . 8 7$ . Inspection of the standard errors of the coefficient estimators suggests the possibility of setting those at lags 2,3,4,6,7,9,10, and 11 equal to zero. If we do this by clicking on the Constrain optimization button in the Maximum Likelihood Estimation dialog box and then reoptimize, we obtain the model,

$$
\left(1 - 0. 2 7 0 B - 0. 2 2 4 B ^ {5} - 0. 1 4 9 B ^ {8} + 0. 0 9 9 B ^ {1 1} + 0. 3 5 3 B ^ {1 2}\right) X _ {t} = Z _ {t},
$$

with $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , 0 . 0 1 3 8 )$ and AICC value 172.49.

In order to check more general $\mathbf { A R M A } ( p , q )$ models, select the option Model> Estimation>Autofit and specify the minimum and maximum values of $p$ and

$q$ to be zero and 15, respectively. (The sample ACF and PACF suggest that these limits should be more than adequate to include the minimum AICC model.) In a few minutes (depending on the speed of your computer) the program selects an ARMA(1,12) model with AICC value $- 1 7 2 . 7 4$ , which is slightly better than the subset AR(12) model just found. Inspection of the estimated standard deviations of the MA coefficients at lags 1, 3, 4, 6, 7, 9, and 11 suggests setting them equal to zero and reestimating the values of the remaining coefficients. If we do this by clicking on the Constrain optimization button in the Maximum Likelihood Estimation dialog box, setting the required coefficients to zero and then reoptimizing, we obtain the model,

$$
(1 - 0. 2 8 6 B) X _ {t} = \left(1 + 0. 1 2 7 B ^ {2} + 0. 1 8 3 B ^ {5} + 0. 1 7 7 B ^ {8} + 0. 1 8 1 B ^ {1 0} - 0. 5 5 4 B ^ {1 2}\right) Z _ {t},
$$

with $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , 0 . 0 1 2 0 )$ and AICC value −184.09.

The subset ARMA(1,12) model easily passes all the goodness of fit tests in the Statistics>Residual Analysis option. In view of this and its small AICC value, we accept it as a plausible model for the transformed red wine series.

# Example 6.2.2 The Lake Data

Let $\{ Y _ { t } , t = 1 , \ldots , 9 9 \}$ denote the lake data of Example 1.3.5. We have seen already in Example 5.2.5 that the ITSM option Model>Estimation $. >$ Autofit gives the minimum-AICC model

$$
X _ {t} - 0. 7 4 4 6 X _ {t - 1} = Z _ {t} + 0. 3 2 1 3 Z _ {t - 1}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 4 7 5 0),
$$

for the mean-corrected series $X _ { t } = Y _ { t } - 9 . 0 0 4 1$ . The corresponding AICC value is 212.77. Since the model passes all the goodness of fit tests, we accept it as a reasonable model for the data.

# 6.3 Unit Roots in Time Series Models

The unit root problem in time series arises when either the autoregressive or movingaverage polynomial of an ARMA model has a root on or near the unit circle. A unit root in either of these polynomials has important implications for modeling. For example, a root near 1 of the autoregressive polynomial suggests that the data should be differenced before fitting an ARMA model, whereas a root near 1 of the moving-average polynomial indicates that the data were overdifferenced. In this section, we consider inference procedures for detecting the presence of a unit root in the autoregressive and moving-average polynomials.

# 6.3.1 Unit Roots in Autoregressions

In Section 6.1 we discussed the use of differencing to transform a nonstationary time series with a slowly decaying sample ACF and values near 1 at small lags into one with a rapidly decreasing sample ACF. The degree of differencing of a time series $\{ X _ { t } \}$ was largely determined by applying the difference operator repeatedly until the sample ACF of $\left\{ \nabla ^ { d } X _ { t } \right\}$ decays quickly. The differenced time series could then be modeled by a low-order ARMA $( p , q )$ process, and hence the resulting ARIMA $( p , d , q )$ model

for the original data has an autoregressive polynomial $\big ( 1 - \phi _ { 1 } z - \cdot \cdot \cdot - \phi _ { p } z ^ { p } \big ) ( 1 - z ) ^ { d }$ [see (6.1.1)] with $d$ roots on the unit circle. In this subsection we discuss a more systematic approach to testing for the presence of a unit root of the autoregressive polynomial in order to decide whether or not a time series should be differenced. This approach was pioneered by Dickey and Fuller (1979).

Let $X _ { 1 }$ , . . . , $X _ { n }$ be observations from the AR(1) model

$$
X _ {t} - \mu = \phi_ {1} \left(X _ {t - 1} - \mu\right) + Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma^ {2}\right), \tag {6.3.1}
$$

where $| \phi _ { 1 } | < 1$ and $\mu = E X _ { t }$ . For large $n$ , the maximum likelihood estimator $\hat { \phi } _ { 1 }$ of $\phi _ { 1 }$ is approximately $\mathrm { N } \big ( \phi _ { 1 } , \big ( 1 - \phi _ { 1 } ^ { 2 } \big ) / n \big )$ . For the unit root case, this normal approximation is no longer applicable, even asymptotically, which precludes its use for testing the unit root hypothesis $H _ { 0 } : \phi _ { 1 } = 1$ vs. $H _ { 1 } : \phi _ { 1 } < 1$ . To construct a test of $H _ { 0 }$ , write the model (6.3.1) as

$$
\nabla X _ {t} = X _ {t} - X _ {t - 1} = \phi_ {0} ^ {*} + \phi_ {1} ^ {*} X _ {t - 1} + Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right), \tag {6.3.2}
$$

where $\phi _ { 0 } ^ { * } = \mu ( 1 - \phi _ { 1 } )$ and $\phi _ { 1 } ^ { * } = \phi _ { 1 } - 1$ . Now let $\hat { \phi } _ { 1 } ^ { * }$ be the ordinary least squares (OLS) estimator of $\phi _ { 1 } ^ { * }$ found by regressing $\nabla X _ { t }$ on 1 and $X _ { t - 1 }$ . The estimated standard error of $\hat { \phi } _ { 1 } ^ { * }$ is

$$
\widehat {\mathrm {S E}} \left(\widehat {\phi} _ {1} ^ {*}\right) = S \left(\sum_ {t = 2} ^ {n} \left(X _ {t - 1} - \bar {X}\right) ^ {2}\right) ^ {- 1 / 2},
$$

where $\begin{array} { r l r } { S ^ { 2 } } & { { } = } & { \sum _ { t = 2 } ^ { n } \left( \nabla X _ { t } - \hat { \phi } _ { 0 } ^ { * } - \hat { \phi } _ { 1 } ^ { * } X _ { t - 1 } \right) ^ { 2 } / ( n - 3 ) } \end{array}$ and $\bar { X }$ is the sample mean of $X _ { 1 } , \dots , X _ { n - 1 }$ . Dickey and Fuller derived the limit distribution as $n  \infty$ of the $t \cdot$ - ratio

$$
\hat {\tau} _ {\mu} := \hat {\phi} _ {1} ^ {*} / \widehat {\mathrm {S E}} \left(\hat {\phi} _ {1} ^ {*}\right) \tag {6.3.3}
$$

under the unit root assumption $\phi _ { 1 } ^ { * } = 0$ , from which a test of the null hypothesis $H _ { 0 } : \phi _ { 1 } = 1$ can be constructed. The 0.01, 0.05, and 0.10 quantiles of the limit distribution of $\hat { \tau } _ { \mu }$ (see Table 8.5.2 of Fuller 1976) are 3.43, $- 2 . 8 6$ , and $- 2 . 5 7$ , respectively. The augmented Dickey–Fuller test then rejects the null hypothesis of a unit root, at say, level 0.05 if $\widehat { \tau } _ { \mu } ~ < ~ - 2 . 8 6$ . Notice that the cutoff value for this test statistic is much smaller than the standard cutoff value of $- 1 . 6 4 5$ obtained from the normal approximation to the $t$ -distribution, so that the unit root hypothesis is less likely to be rejected using the correct limit distribution.

The above procedure can be extended to the case where $\{ X _ { t } \}$ follows the $\operatorname { A R } ( p )$ model with mean $\mu$ given by

$$
X _ {t} - \mu = \phi_ {1} \left(X _ {t - 1} - \mu\right) + \dots + \phi_ {p} \left(X _ {t - p} - \mu\right) + Z _ {t}, \qquad \left\{Z _ {t} \right\} \sim \mathrm {W N} \big (0, \sigma^ {2} \big).
$$

This model can be rewritten as (see Problem 6.2)

$$
\nabla X _ {t} = \phi_ {0} ^ {*} + \phi_ {1} ^ {*} X _ {t - 1} + \phi_ {2} ^ {*} \nabla X _ {t - 1} + \dots + \phi_ {p} ^ {*} \nabla X _ {t - p + 1} + Z _ {t}, \tag {6.3.4}
$$

where $\phi _ { 0 } = \mu \big ( 1 - \phi _ { 1 } - \cdot \cdot \cdot - \phi _ { p } \big )$ , $\begin{array} { r } { \phi _ { 1 } ^ { * } = \sum _ { i = 1 } ^ { p } \phi _ { i } - 1 } \end{array}$ , and $\begin{array} { r } { \phi _ { j } ^ { * } = - \sum _ { i = j } ^ { p } \phi _ { i } } \end{array}$ , j = $2 , \ldots , p$ . If the autoregressive polynomial has a unit root at 1, then $0 = \phi \left( 1 \right) = - \phi _ { 1 } ^ { * }$ , and the differenced series $\{ \nabla X _ { t } \}$ is an $\Delta \mathsf { R } ( p - 1 )$ process. Consequently, testing the hypothesis of a unit root at 1 of the autoregressive polynomial is equivalent to testing $\phi _ { 1 } ^ { * } = 0$ . As in the AR(1) example, $\phi _ { 1 } ^ { * }$ can be estimated as the coefficient of $X _ { t - 1 }$ in the OLS regression of $\nabla X _ { t }$ onto $1 , X _ { t - 1 }$ , $\nabla X _ { t - 1 }$ , . . . , $\nabla X _ { t - p + 1 }$ . For large $n$ the $t$ -ratio

$$
\hat {\tau} _ {\mu} := \hat {\phi} _ {1} ^ {*} / \widehat {\operatorname {S E}} \left(\hat {\phi} _ {1} ^ {*}\right), \tag {6.3.5}
$$

where $\widehat { \mathrm { S E } } \left( \hat { \phi } _ { 1 } ^ { * } \right)$ is the estimated standard error of $\hat { \phi } _ { 1 } ^ { * }$ , has the same limit distribution as the test statistic in (6.3.3). The augmented Dickey–Fuller test in this case is applied in exactly the same manner as for the AR(1) case using the test statistic (6.3.5) and the cutoff values given above.

Example 6.3.1 Consider testing the time series of Example 6.1.1 (see Figure 6-1) for the presence of a unit root in the autoregressive operator. The sample PACF in Figure 6-3 suggests fitting an AR(2) or possibly an AR(3) model to the data. Regressing $\nabla X _ { t }$ on $1 , X _ { t - 1 }$ , $\nabla X _ { t - 1 }$ , $\nabla X _ { t - 2 }$ for $t = 4 , \ldots , 2 0 0$ $t = 4$ using OLS gives

$$
\nabla X _ {t} = 0. 1 5 0 3 - 0. 0 0 4 1 X _ {t - 1} + 0. 9 3 3 5 \nabla X _ {t - 1} - 0. 1 5 4 8 \nabla X _ {t - 2} + Z _ {t},
$$

$$
(0. 1 1 3 5) (0. 0 0 2 8) \quad (0. 0 7 0 7) \quad (0. 0 7 0 8)
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , 0 . 9 6 3 9 )$ . The test statistic for testing the presence of a unit root is

$$
\hat {\tau} _ {\mu} = \frac {- 0 . 0 0 4 1}{0 . 0 0 2 8} = - 1. 4 6 4.
$$

Since $- 1 . 4 6 4 \ > \ - 2 . 5 7$ , the unit root hypothesis is not rejected at level 0.10. In contrast, if we had mistakenly used the $t$ -distribution with 193 degrees of freedom as an approximation to $\hat { \tau } _ { \mu }$ , then we would have rejected the unit root hypothesis at the 0.10 level $\dot { p }$ -value is 0.074). The $t$ -ratios for the other coefficients, $\phi _ { 0 } ^ { * } , \phi _ { 2 } ^ { * }$ , and $\phi _ { 3 } ^ { * }$ , have an approximate $t$ -distribution with 193 degrees of freedom. Based on these $t$ -ratios, the intercept should be 0, while the coefficient of $\nabla X _ { t - 2 }$ is barely significant. The evidence is much stronger in favor of a unit root if the analysis is repeated without a mean term. The fitted model without a mean term is

$$
\nabla X _ {t} = 0. 0 0 1 2 X _ {t - 1} + 0. 9 3 9 5 \nabla X _ {t - 1} - 0. 1 5 8 5 \nabla X _ {t - 2} + Z _ {t},
$$

$$
(0. 0 0 1 8) \quad (0. 0 7 0 7) \quad (0. 0 7 0 9)
$$

where $\begin{array} { r l r } { \{ Z _ { t } \} } & { { } \sim } & { \mathrm { W N } ( 0 , 0 . 9 6 7 7 ) } \end{array}$ . The 0.01, 0.05, and 0.10 cutoff values for the corresponding test statistic when a mean term is excluded from the model are $- 2 . 5 8$ , $- 1 . 9 5$ , and $- 1 . 6 2$ (see Table 8.5.2 of Fuller 1976). In this example, the test statistic is

$$
\hat {\tau} = \frac {- 0 . 0 0 1 2}{0 . 0 0 1 8} = - 0. 6 6 7,
$$

which is substantially larger than the 0.10 cutoff value of −1.62.

Further extensions of the above test to AR models with $p \ = \ O \big ( n ^ { 1 / 3 } \big )$ and to ARMA $( p , q )$ models can be found in Said and Dickey (1984). However, as reported in Schwert (1987) and Pantula (1991), this test must be used with caution if the underlying model orders are not correctly specified.

# 6.3.2 Unit Roots in Moving Averages

A unit root in the moving-average polynomial can have a number of interpretations depending on the modeling application. For example, let $\{ X _ { t } \}$ be a causal and invertible $\mathbf { A R M A } ( p , q )$ process satisfying the equations

$$
\phi (B) X _ {t} = \theta (B) Z _ {t}, \qquad \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right).
$$

Then the differenced series $Y _ { t } : = \nabla X _ { t }$ is a noninvertible $\mathbf { A R M A } ( p , q + 1 )$ process with moving-average polynomial $\theta ( z ) ( 1 - z )$ . Consequently, testing for a unit root in the moving-average polynomial is equivalent to testing that the time series has been overdifferenced.

As a second application, it is possible to distinguish between the competing models

$$
\nabla^ {k} X _ {t} = a + V _ {t}
$$

and

$$
X _ {t} = c _ {0} + c _ {1} t + \dots + c _ {k} t ^ {k} + W _ {t},
$$

where $\{ V _ { t } \}$ and $\{ W _ { t } \}$ are invertible ARMA processes. For the former model the differenced series $\left\{ \nabla ^ { k } X _ { t } \right\}$ has no moving-average unit roots, while for the latter model $\{ \nabla ^ { k } X _ { t } \}$ has a multiple moving-average unit root of order $k$ . We can therefore distinguish between the two models by using the observed values of $\big \{ \nabla ^ { k } X _ { t } \big \}$ to test for the presence of a moving-average unit root.

We confine our discussion of unit root tests to first-order moving-average models, the general case being considerably more complicated and not fully resolved. Let $X _ { 1 }$ , . . . , $X _ { n }$ be observations from the MA(1) model

$$
X _ {t} = Z _ {t} + \theta Z _ {t - 1}, \qquad \{Z _ {t} \} \sim \operatorname {I I D} \left(0, \sigma^ {2}\right).
$$

Davis and Dunsmuir (1996) showed that under the assumption $\theta = - 1$ , $n ( { \hat { \theta } } + 1 )$ ） $\hat { \theta }$ is the maximum likelihood estimator) converges in distribution. A test of $H _ { 0 } : \theta = - 1$ vs. $H _ { 1 } : \theta > - 1$ can be fashioned on this limiting result by rejecting $H _ { 0 }$ when

$$
\hat {\theta} > - 1 + c _ {\alpha} / n,
$$

where $c _ { \alpha }$ is the $( 1 ~ - ~ \alpha )$ quantile of the limit distribution of $n ( { \hat { \theta } } + 1 )$ . (From Table 3.2 of Davis et al. (1995), $c _ { 0 . 0 1 } = 1 1 . 9 3$ , $c _ { 0 . 0 5 } = 6 . 8 0$ , and $\begin{array} { r l } { c _ { 0 . 1 0 } } & { { } = } \end{array}$ 4.90.) In particular, if $n = 5 0$ , then the null hypothesis is rejected at level 0.05 if $\hat { \theta } > - 1 + 6 . 8 0 / 5 0 = - 0 . 8 6 4$ .

The likelihood ratio test can also be used for testing the unit root hypothesis. The likelihood ratio for this problem is $L ( - 1 , S ( - 1 ) / n ) / L \left( \hat { \theta } , \hat { \sigma } ^ { 2 } \right)$ , where $L \left( \theta , \sigma ^ { 2 } \right)$ is the Gaussian likelihood of the data based on an MA(1) model, $S ( - 1 )$ is the sum of squares given by (5.2.11) when $\theta = - 1$ , and $\hat { \theta }$ and $\hat { \sigma } ^ { 2 }$ are the maximum likelihood estimators of $\theta$ and $\sigma ^ { 2 }$ . The null hypothesis is rejected at level $\alpha$ if

$$
\lambda_ {n} := - 2 \ln \left(\frac {L (- 1 , S (- 1) / n)}{L (\hat {\theta} , \hat {\sigma} ^ {2})}\right) > c _ {\mathrm {L R}, \alpha}
$$

where the cutoff value is chosen such that $ P _ { \theta = - 1 } [ \lambda _ { n } > c _ { \mathrm { L R } , \alpha } ] = \alpha$ . The limit distribution of $\lambda _ { n }$ was derived by Davis et al. (1995), who also gave selected quantiles of the limit. It was found that these quantiles provide a good approximation to their finite-sample counterparts for time series of length $n \geq 5 0$ . The limiting quantiles for $\lambda _ { n }$ under $H _ { 0 }$ are $c _ { \mathrm { L R } , 0 . 0 1 } = 4 . 4 1 $ , $c _ { \mathrm { L R } , 0 . 0 5 } = 1 . 9 4$ , and $c _ { \mathrm { L R } , 0 . 1 0 } = 1 . 0 0$ .

Example 6.3.2 For the overshort data $\{ X _ { t } \}$ of Example 3.2.8, the maximum likelihood MA(1) model for the mean corrected data $\{ Y _ { t } = X _ { t } + 4 . 0 3 5 \}$ was (see Example 5.4.1)

$$
Y _ {t} = Z _ {t} - 0. 8 1 8 Z _ {t - 1}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 2 0 4 0. 7 5).
$$

In the structural formulation of this model given in Example 3.2.8, the moving-average parameter $\theta$ was related to the measurement error variances $\sigma _ { U } ^ { 2 }$ and $\sigma _ { V } ^ { 2 }$ through the equation

$$
\frac {\theta}{1 + \theta^ {2}} = \frac {- \sigma_ {U} ^ {2}}{2 \sigma_ {U} ^ {2} + \sigma_ {V} ^ {2}}.
$$

(These error variances correspond to the daily measured amounts of fuel in the tank and the daily measured adjustments due to sales and deliveries.) A value of $\theta = - 1$ indicates that there is no appreciable measurement error due to sales and deliveries (i.e., $\sigma _ { V } ^ { 2 } = 0 \mathrm { \bar { . } }$ ), and hence testing for a unit root in this case is equivalent to testing that $\sigma _ { U } ^ { 2 } = 0$ . Assuming that the mean is known, the unit root hypothesis is rejected at $\alpha = 0 . 0 5$ , since $- 0 . 8 1 8 > - 1 + 6 . 8 0 / 5 7 = - 0 . 8 8 1$ . The evidence against $H _ { 0 }$ is stronger using the likelihood ratio statistic. Using ITSM and entering the MA(1) model $\theta \ : = \ : - 1$ and $\sigma ^ { 2 } = 2 2 0 3 . 1 2$ , we find that $- 2 \ln L ( - 1 , 2 2 0 3 . 1 2 ) \ : = \ : 6 0 4 . 5 8 4$ , while $- 2 \ln { \cal L } ( \hat { \theta } , \hat { \sigma } ^ { 2 } ) = 5 9 7 . 2 6 7$ . Comparing the likelihood ratio statistic $\lambda _ { n } = 6 0 4 . 5 8 4 -$ $5 9 7 . 2 6 7 = 7 . 3 1 7$ with the cutoff value $c _ { \mathrm { L R } , 0 . 0 1 }$ , we reject $H _ { 0 }$ at level $\alpha = 0 . 0 1$ and conclude that the measurement error associated with sales and deliveries is nonzero.

In the above example it was assumed that the mean was known. In practice, these tests should be adjusted for the fact that the mean is also being estimated.

Tanaka (1990) proposed a locally best invariant unbiased (LBIU) test for the unit root hypothesis. It was found that the LBIU test has slightly greater power than the likelihood ratio test for alternatives close to $\theta = - 1$ but has less power for alternatives further away from $^ { - 1 }$ (see Davis et al. 1995). The LBIU test has been extended to cover more general models by Tanaka (1990) and Tam and Reinsel (1995). Similar extensions to tests based on the maximum likelihood estimator and the likelihood ratio statistic have been explored in Davis et al. (1996).

# 6.4 Forecasting ARIMA Models

In this section we demonstrate how the methods of Sections 3.3 and 5.4 can be adapted to forecast the future values of an $\mathrm { A R I M A } ( p , d , q )$ process $\{ X _ { t } \}$ . (The required numerical calculations can all be carried out using the program ITSM.)

If $d \geq 1$ , the first and second moments $E X _ { t }$ and $E ( X _ { t + h } X _ { t } )$ are not determined by the difference equations (6.1.1). We cannot expect, therefore, to determine best linear predictors for $\{ X _ { t } \}$ without further assumptions.

For example, suppose that $\{ Y _ { t } \}$ is a causal ARMA $( p , q )$ process and that $X _ { 0 }$ is any random variable. Define

$$
X _ {t} = X _ {0} + \sum_ {j = 1} ^ {t} Y _ {j}, \quad t = 1, 2, \ldots .
$$

Then $\{ X _ { t } , t \geq 0 \}$ is an $\mathbf { A R I M A } ( p , 1 , q )$ process with mean $E X _ { t } = E X _ { 0 }$ and autocovariances $E ( X _ { t + h } X _ { t } ) - ( E X _ { 0 } ) ^ { 2 }$ that depend on $\mathrm { V a r } ( X _ { 0 } )$ and $\operatorname { C o v } ( X _ { 0 } , Y _ { j } ) , j = 1 , 2 , \dotsc .$ The best linear predictor of $X _ { n + 1 }$ based on $\{ 1 , X _ { 0 } , X _ { 1 } , \ldots , X _ { n } \}$ is the same as the best linear predictor in terms of the set $\{ 1 , X _ { 0 } , Y _ { 1 } , \ldots , Y _ { n } \}$ , since each linear combination of the latter is a linear combination of the former and vice versa. Hence, using $P _ { n }$ to denote best linear predictor in terms of either set and using the linearity of $P _ { n }$ , we can write

$$
P _ {n} X _ {n + 1} = P _ {n} \left(X _ {0} + Y _ {1} + \dots + Y _ {n + 1}\right) = P _ {n} \left(X _ {n} + Y _ {n + 1}\right) = X _ {n} + P _ {n} Y _ {n + 1}.
$$

To evaluate $P _ { n } Y _ { n + 1 }$ it is necessary (see Section 2.5) to know $E ( X _ { 0 } Y _ { j } ) , j = 1 , \dots , n + 1$ , and $E X _ { 0 } ^ { 2 }$ . However, if we assume that $X _ { 0 }$ is uncorrelated with $\{ Y _ { t } , t ~ \geq ~ 1 \}$ , then $P _ { n } Y _ { n + 1 }$ is the same (Problem 6.5) as the best linear predictor $\hat { Y } _ { n + 1 }$ of $Y _ { n + 1 }$ in terms of $\{ 1 , Y _ { 1 } , \ldots , Y _ { n } \}$ , which can be calculated as described in Section 3.3. The assumption

that $X _ { 0 }$ is uncorrelated with $Y _ { 1 } , Y _ { 2 } , \ldots$ is therefore sufficient to determine the best linear predictor $P _ { n } X _ { n + 1 }$ in this case.

Turning now to the general case, we shall assume that our observed process $\{ X _ { t } \}$ satisfies the difference equations

$$
(1 - B) ^ {d} X _ {t} = Y _ {t}, \quad t = 1, 2, \dots ,
$$

where $\{ Y _ { t } \}$ is a causal $\mathbf { A R M A } ( p , q )$ $( p , q )$ process, and that the random vector $( X _ { 1 - d } , . . . , X _ { 0 } )$ is uncorrelated with $Y _ { t }$ , $t > 0$ . The difference equations can be rewritten in the form

$$
X _ {t} = Y _ {t} - \sum_ {j = 1} ^ {d} \binom {d} {j} (- 1) ^ {j} X _ {t - j}, \quad t = 1, 2, \dots . \tag {6.4.1}
$$

It is convenient, by relabeling the time axis if necessary, to assume that we observe $X _ { 1 - d } , X _ { 2 - d } , \ldots , X _ { n }$ . (The observed values of $\{ Y _ { t } \}$ are then $Y _ { 1 } , \ldots , Y _ { n } . )$ As usual, we shall use $P _ { n }$ to denote best linear prediction in terms of the observations up to time $n$ (in this case $1 , X _ { 1 - d } , \ldots , X _ { n }$ or equivalently $1 , X _ { 1 - d } , \ldots , X _ { 0 } , Y _ { 1 } , \ldots , Y _ { n } )$ .

Our goal is to compute the best linear predictors $P _ { n } X _ { n + h }$ . This can be done by applying the operator $P _ { n }$ to each side of (6.4.1) (with $t = n + h )$ and using the linearity of $P _ { n }$ to obtain

$$
P _ {n} X _ {n + h} = P _ {n} Y _ {n + h} - \sum_ {j = 1} ^ {d} \binom {d} {j} (- 1) ^ {j} P _ {n} X _ {n + h - j}. \tag {6.4.2}
$$

Now the assumption that $( X _ { 1 - d } , \ldots , X _ { 0 } )$ is uncorrelated with $Y _ { t } , t > 0$ , enables us to identify $P _ { n } Y _ { n + h }$ with the best linear predictor of $Y _ { n + h }$ in terms of $\{ 1 , Y _ { 1 } , \ldots , Y _ { n } \}$ , and this can be calculated as described in Section 3.3. The predictor $P _ { n } X _ { n + 1 }$ is obtained directly from (6.4.2) by noting that $P _ { n } X _ { n + 1 - j } = X _ { n + 1 - j }$ for each $j \geq 1$ . The predictor $P _ { n } X _ { n + 2 }$ can then be found from (6.4.2) using the previously calculated value of $P _ { n } X _ { n + 1 }$ . The predictors $P _ { n } X _ { n + 3 }$ , $P _ { n } X _ { n + 4 }$ , . . . can be computed recursively in the same way.

To find the mean squared error of prediction it is convenient to express $P _ { n } Y _ { n + h }$ in terms of $\{ X _ { j } \}$ . For $n \geq 0$ we denote the one-step predictors by $\hat { Y } _ { n + 1 } = P _ { n } Y _ { n + 1 }$ and $\hat { X } _ { n + 1 } = P _ { n } \dot { X } _ { n + 1 }$ . Then from (6.4.1) and (6.4.2) we have

$$
X _ {n + 1} - \hat {X} _ {n + 1} = Y _ {n + 1} - \hat {Y} _ {n + 1}, \quad n \geq 1,
$$

and hence from (3.3.12), if $n > m = \operatorname* { m a x } ( p , q )$ and $h \geq 1$ , we can write

$$
P _ {n} Y _ {n + h} = \sum_ {i = 1} ^ {p} \phi_ {i} P _ {n} Y _ {n + h - i} + \sum_ {j = h} ^ {q} \theta_ {n + h - 1, j} \left(X _ {n + h - j} - \hat {X} _ {n + h - j}\right). \tag {6.4.3}
$$

Setting $\phi ^ { * } ( z ) = ( 1 - z ) ^ { d } \phi ( z ) = 1 - \phi _ { 1 } ^ { * } z - \cdot \cdot \cdot - \phi _ { p + d } ^ { * } z ^ { p + d }$ , we find from (6.4.2) and (6.4.3) that

$$
P _ {n} X _ {n + h} = \sum_ {j = 1} ^ {p + d} \phi_ {j} ^ {*} P _ {n} X _ {n + h - j} + \sum_ {j = h} ^ {q} \theta_ {n + h - 1, j} \left(X _ {n + h - j} - \hat {X} _ {n + h - j}\right), \tag {6.4.4}
$$

which is analogous to the $h$ -step prediction formula (3.3.12) for an ARMA process. As in (3.3.13), the mean squared error of the $h$ -step predictor is

$$
\sigma_ {n} ^ {2} (h) = E \left(X _ {n + h} - P _ {n} X _ {n + h}\right) ^ {2} = \sum_ {j = 0} ^ {h - 1} \left(\sum_ {r = 0} ^ {j} \chi_ {r} \theta_ {n + h - r - 1, j - r}\right) ^ {2} v _ {n + h - j - 1}, \tag {6.4.5}
$$

where $\theta _ { n 0 } = 1$

$$
\chi (z) = \sum_ {r = 0} ^ {\infty} \chi_ {r} z ^ {r} = \left(1 - \phi_ {1} ^ {*} z - \dots - \phi_ {p + d} ^ {*} z ^ {p + d}\right) ^ {- 1},
$$

and

$$
v _ {n + h - j - 1} = E \left(X _ {n + h - j} - \hat {X} _ {n + h - j}\right) ^ {2} = E \left(Y _ {n + h - j} - \hat {Y} _ {n + h - j}\right) ^ {2}.
$$

The coefficients $\chi _ { j }$ can be found from the recursions (3.3.14) with $\phi _ { j } ^ { * }$ replacing $\phi _ { j }$ . For large $n$ we can approximate (6.4.5), provided that $\theta ( \cdot )$ is invertible, by

$$
\sigma_ {n} ^ {2} (h) = \sum_ {j = 0} ^ {h - 1} \psi_ {j} ^ {2} \sigma^ {2}, \tag {6.4.6}
$$

where

$$
\psi (z) = \sum_ {j = 0} ^ {\infty} \psi_ {j} z ^ {j} = (\phi^ {*} (z)) ^ {- 1} \theta (z).
$$

# 6.4.1 The Forecast Function

Inspection of equation (6.4.4) shows that for fixed $n > m = \operatorname* { m a x } ( p , q )$ , the $h$ -step predictors

$$
g (h) := P _ {n} X _ {n + h},
$$

satisfy the homogeneous linear difference equations

$$
g (h) - \phi_ {1} ^ {*} g (h - 1) - \dots - \phi_ {p + d} ^ {*} g (h - p - d) = 0, \quad h > q, \tag {6.4.7}
$$

where $\boldsymbol { \phi } _ { 1 } ^ { * } , \ldots , \boldsymbol { \phi } _ { p + d } ^ { * }$ are the coefficients of $z , \ldots , z ^ { p + d }$ $z _ { \cdot } ^ { p + d }$ in

$$
\phi^ {*} (z) = (1 - z) ^ {d} \phi (z).
$$

The solution of (6.4.7) is well known from the theory of linear difference equations (see Brockwell and Davis (1991), Section 3.6). If we assume that the zeros of $\phi ( z )$ (denoted by $\xi _ { 1 } , \ldots , \xi _ { p } )$ are all distinct, then the solution is

$$
g (h) = a _ {0} + a _ {1} h + \dots + a _ {d - 1} h ^ {d - 1} + b _ {1} \xi_ {1} ^ {- h} + \dots + b _ {p} \xi_ {p} ^ {- h}, \quad h > q - p - d, \tag {6.4.8}
$$

where the coefficients $a _ { 0 } , \ldots , a _ { d - 1 }$ and $b _ { 1 } , \ldots , b _ { p }$ can be determined from the $p + d$ equations obtained by equating the right-hand side of (6.4.8) for $q - p - d < h \leq q$ with the corresponding value of $g ( h )$ computed numerically (for $h \leq 0$ , $P _ { n } X _ { n + h } \ =$ $X _ { n + h }$ , and for $1 \leq h \leq q , P _ { n } X _ { n + h }$ $P _ { n } X _ { n + h }$ can be computed from (6.4.4) as already described). Once the constants $a _ { i }$ and $b _ { i }$ have been evaluated, the algebraic expression (6.4.8) gives the predictors for all $h > q - p - d$ . In the case $q = 0$ , the values of $g ( h )$ in the equations for $a _ { 0 } , \dotsc , a _ { d - 1 } , b _ { 1 } , \dotsc , b _ { p }$ are simply the observed values $g ( h ) = X _ { n + h }$ , $- p - d \leq h \leq 0$ , and the expression (6.4.6) for the mean squared error is exact.

The calculation of the forecast function is easily generalized to deal with more complicated ARIMA processes. For example, if the observations X 13, X 12, . . . , $X _ { n }$ are differenced at lags 12 and 1, and $( 1 - B ) \bar { ( 1 - B ^ { 1 2 } ) } X _ { t }$ is modeled as a causal invertible ARMA $( p , q )$ process with mean $\mu$ and $\operatorname* { m a x } ( p , q ) < n$ , then $\{ X _ { t } \}$ satisfies an equation of the form

$$
\phi (B) [ (1 - B) (1 - B ^ {1 2}) X _ {t} - \mu ] = \theta (B) Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right), \tag {6.4.9}
$$

and the forecast function $g ( h ) = P _ { n } X _ { n + h }$ satisfies the analogue of (6.4.7), namely,

$$
\phi (B) (1 - B) \left(1 - B ^ {1 2}\right) g (h) = \phi (1) \mu , h > q. \tag {6.4.10}
$$

To find the general solution of these inhomogeneous linear difference equations, it suffices (see Brockwell and Davis (1991), Section 3.6) to find one particular solution of (6.4.10) and then add to it the general solution of the same equations with the righthand side set equal to zero. A particular solution is easily found (by trial and error) to be

$$
g (h) = \frac {\mu h ^ {2}}{2 4},
$$

and the general solution is therefore

$$
\begin{array}{l} g (h) = \frac {\mu h ^ {2}}{2 4} + a _ {0} + a _ {1} h + \sum_ {j = 1} ^ {1 1} c _ {j} e ^ {i j \pi / 6} + b _ {1} \xi_ {1} ^ {- h} + \dots + b _ {p} \xi_ {p} ^ {- h}, \\ h > q - p - 1 3. \qquad (6. 4. 1 1) \\ \end{array}
$$

(The terms $a _ { 0 }$ and $a _ { 1 } h$ correspond to the double root $z = 1$ of the equation $\phi ( z ) ( 1 -$ $z ) ( 1 - z ^ { 1 2 } ) = 0$ , and the subsequent terms to each of the other roots, which we assume to be distinct.) For $q - p - 1 3 < h \leq 0$ , $g ( h ) = X _ { n + h }$ , and for $1 \leq h \leq q$ , the values of $g ( h ) = P _ { n } X _ { n + h }$ can be determined recursively from the equations

$$
P _ {n} X _ {n + h} = \mu + P _ {n} X _ {n - 1} + P _ {n} X _ {n - 1 2} - P _ {n} X _ {n - 1 3} + P _ {n} Y _ {n + h},
$$

where $\{ Y _ { t } \}$ is the ARMA process $Y _ { t } = ( 1 - B ) \bigl ( 1 - B ^ { 1 2 } \bigr ) X _ { t } - \mu$ . Substituting these values of $g ( h )$ into (6.4.11), we obtain a set of $p + 1 3$ equations for the coefficients $a _ { i } , b _ { j }$ , and $c _ { k }$ . Solving these equations then completes the determination of $g ( h )$ .

The large-sample approximation to the mean squared error is again given by (6.4.6), with $\psi _ { j }$ redefined as the coefficient of $z ^ { j }$ in the power series expansion of $\theta ( z ) / \big [ ( 1 - z ) \big ( \mathrm { i } - z ^ { 1 2 } \big ) \phi ( z ) \big ]$ .

# Example 6.4.1 An ARIMA(1,1,0) Model

In Example 5.2.4 we found the maximum likelihood AR(1) model for the meancorrected differences $X _ { t }$ of the Dow Jones Utilities Index (August 28–December 18, 1972). The model was

$$
X _ {t} - 0. 4 4 7 1 X _ {t - 1} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 1 4 5 5), \tag {6.4.12}
$$

where $X _ { t } = D _ { t } - D _ { t - 1 } - 0 . 1 3 3 6$ , $t = 1 , \ldots , 7 7$ , and $\{ D _ { t } , t = 0 , 1 , 2 , \ldots , 7 7 \}$ is the original series. The model for $\{ D _ { t } \}$ is thus

$$
(1 - 0. 4 4 7 1 B) [ (1 - B) D _ {t} - 0. 1 3 3 6 ] = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 1 4 5 5).
$$

The recursions for $g ( h )$ therefore take the form

$$
(1 - 0. 4 4 7 1 B) (1 - B) g (h) = 0. 5 5 2 9 \times 0. 1 3 3 6 = 0. 0 7 3 8 7, h > 0. \tag {6.4.13}
$$

A particular solution of these equations is $g ( h ) = 0 . 1 3 3 6 h$ , so the general solution is

$$
g (h) = 0. 1 3 3 6 h + a + b \left(0. 4 4 7 1\right) ^ {h}, h > - 2. \tag {6.4.14}
$$

Substituting $g ( - 1 ) = D _ { 7 6 } = 1 2 2$ and $g ( 0 ) = D _ { 7 7 } = 1 2 1 . 2 3$ in the equations with $h = - 1$ and $h = 0$ , and solving for $a$ and $^ b$ gives

$$
g (h) = 0. 1 3 6 6 h + 1 2 0. 5 0 + 0. 7 3 3 1 (0. 4 4 7 1) ^ {h}.
$$

Setting $h = 1$ and $h = 2$ gives

$$
P _ {7 7} D _ {7 8} = 1 2 0. 9 7 \text {a n d} P _ {7 7} D _ {7 9} = 1 2 0. 9 4.
$$

From (6.4.5) we find that the corresponding mean squared errors are

$$
\sigma_ {7 7} ^ {2} (1) = \nu_ {7 7} = \sigma^ {2} = 0. 1 4 5 5
$$

and

$$
\sigma_ {7 7} ^ {2} (2) = v _ {7 8} + \phi_ {1} ^ {* 2} v _ {7 7} = \sigma^ {2} \left(1 + 1. 4 4 7 1 ^ {2}\right) = 0. 4 5 0 2.
$$

(Notice that the approximation (6.4.6) is exact in this case.) The predictors and their mean squared errors are easily obtained from the program ITSM by opening the file DOWJ.TSM, differencing at lag 1, fitting a preliminary AR(1) model to the meancorrected data with Burg’s algorithm, and selecting Model>Estimation>Max likelihood to find the maximum likelihood AR(1) model. Predicted values and their mean squared errors are then found using the option Forecasting>ARMA.

![](images/1d1ad8f47ee35ba74b121b55fb3ab8b182d0ce901dd6f8d64ae72af808036f7c.jpg)

# 6.5 Seasonal ARIMA Models

We have already seen how differencing the series $\{ X _ { t } \}$ at lag $s$ is a convenient way of eliminating a seasonal component of period $s$ . If we fit an $\mathbf { A R M A } ( p , q )$ model $\phi ( B ) Y _ { t } = \theta ( B ) Z _ { t }$ to the differenced series $Y _ { t } = ( 1 - B ^ { s } ) X _ { t }$ , then the model for the original series is $\phi ( B ) \left( 1 - B ^ { s } \right) X _ { t } \ = \ \theta ( B ) Z _ { t }$ . This is a special case of the general seasonal ARIMA (SARIMA) model defined as follows.

# Definition 6.5.1

If $d$ and $D$ are nonnegative integers, then $\{ X _ { t } \}$ is a seasonal $\mathbf { A R I M A } ( p , d , q ) \xleftarrow { } \mathbf { x }$ $( P , D , Q ) _ { s }$ process with period $s$ if the differenced series $Y _ { t } = ( 1 - B ) ^ { d } ( 1 - B ^ { s } ) ^ { D } X _ { t }$ is a causal ARMA process defined by

$$
\phi (B) \Phi \left(B ^ {s}\right) Y _ {t} = \theta (B) \Theta \left(B ^ {s}\right) Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right), \tag {6.5.1}
$$

where $\phi ( z ) = 1 - \phi _ { 1 } z - \cdot \cdot \cdot - \phi _ { p } z ^ { p }$ , (z)  1  1z  PzP, θ (z) $1 + \theta _ { 1 } z + \cdot \cdot \cdot + \theta _ { q } z ^ { q }$ , and $\Theta ( z ) = 1 + \Theta _ { 1 } z + \cdot \cdot \cdot + \Theta _ { Q } z ^ { Q }$ .

Remark 1. Note that the process $\{ Y _ { t } \}$ is causal if and only if $\phi ( z ) \neq 0$ and $\Phi ( z ) \neq 0$ for $| z | \leq 1$ . In applications $D$ is rarely more than one, and $P$ and $Q$ are typically less than three. -

Remark 2. Equation (6.5.1) satisfied by the differenced process $\{ Y _ { t } \}$ can be rewritten in the equivalent form

$$
\phi^ {*} (B) Y _ {t} = \theta^ {*} (B) Z _ {t}, \tag {6.5.2}
$$

where $\phi ^ { \ast } ( \cdot ) , \theta ^ { \ast } ( \cdot )$ are polynomials of degree $p + s P$ and $q + s Q$ , respectively, whose coefficients can all be expressed in terms of $\phi _ { 1 } , . . . , \phi _ { p }$ , $\Phi _ { 1 } , \ldots , \Phi _ { P } , \theta _ { 1 } , \ldots , \theta _ { q }$ $\Phi _ { 1 }$ , and

$\Theta _ { 1 } , \ldots , \Theta _ { Q }$ $\Theta _ { Q }$ . Provided that $p < s$ and $q < s$ , the constraints on the coefficients of $\phi ^ { \ast } ( \cdot )$ and $\theta ^ { * } ( \cdot )$ can all be expressed as multiplicative relations

$$
\phi_ {i s + j} ^ {*} = \phi_ {i s} ^ {*} \phi_ {j} ^ {*}, \quad i = 1, 2, \ldots ; \quad j = 1, \ldots , s - 1,
$$

and

$$
\theta_ {i s + j} ^ {*} = \theta_ {i s} ^ {*} \theta_ {j} ^ {*}, i = 1, 2, \ldots ; j = 1, \ldots , s - 1.
$$

In Section 1.5 we discussed the classical decomposition model incorporating trend, seasonality, and random noise, namely, $X _ { t } = m _ { t } + s _ { t } + Y _ { t }$ . In modeling real data it might not be reasonable to assume, as in the classical decomposition model, that the seasonal component $s _ { t }$ repeats itself precisely in the same way cycle after cycle. Seasonal ARIMA models allow for randomness in the seasonal pattern from one cycle to the next. -

Example 6.5.1 Suppose we have $r$ years of monthly data, which we tabulate as follows:

<table><tr><td>Year/Month</td><td>1</td><td>2</td><td>...</td><td>12</td></tr><tr><td>1</td><td>Y1</td><td>Y2</td><td>...</td><td>Y12</td></tr><tr><td>2</td><td>Y13</td><td>Y14</td><td>...</td><td>Y24</td></tr><tr><td>3</td><td>Y25</td><td>Y26</td><td>...</td><td>Y36</td></tr><tr><td>\( \vdots \)</td><td>\( \vdots \)</td><td>\( \vdots \)</td><td>\( \vdots \)</td><td></td></tr><tr><td>r</td><td>Y1+12(r-1)</td><td>Y2+12(r-1)</td><td>...</td><td>Y12+12(r-1)</td></tr></table>

Each column in this table may itself be viewed as a realization of a time series. Suppose that each one of these twelve time series is generated by the same ARMA $( P , Q )$ model, or more specifically, that the series corresponding to the jth month, $Y _ { j + 1 2 t }$ , $t = 0 , \ldots , r - 1$ , satisfies a difference equation of the form

$$
\begin{array}{l} Y _ {j + 1 2 t} = \Phi_ {1} Y _ {j + 1 2 (t - 1)} + \dots + \Phi_ {P} Y _ {j + 1 2 (t - P)} + U _ {j + 1 2 t} \\ + \Theta_ {1} U _ {j + 1 2 (t - 1)} + \dots + \Theta_ {Q} U _ {j + 1 2 (t - Q)}, \tag {6.5.3} \\ \end{array}
$$

where

$$
\left\{U _ {j + 1 2 t}, t = \dots , - 1, 0, 1, \dots \right\} \sim \operatorname {W N} \left(0, \sigma_ {U} ^ {2}\right). \tag {6.5.4}
$$

Then since the same $\operatorname { A R M A } ( P , Q )$ model is assumed to apply to each month, (6.5.3) holds for each $j = 1 , \dots , 1 2$ . (Notice, however, that $E ( U _ { t } U _ { t + h } )$ is not necessarily zero except when $h$ is an integer multiple of 12.) We can thus write (6.5.3) in the compact form

$$
\Phi \left(B ^ {1 2}\right) Y _ {t} = \Theta \left(B ^ {1 2}\right) U _ {t}, \tag {6.5.5}
$$

where $\Phi ( z ) = 1 - \Phi _ { 1 } z - \cdot \cdot \cdot - \Phi _ { P } z ^ { P }$ , $\Theta ( z ) = 1 + \Theta _ { 1 } z + \cdot \cdot \cdot + \Theta _ { Q } z ^ { Q }$ , and $\{ U _ { j + 1 2 t }$ , t $\dots , - 1 , 0 , 1 , \dots \} \sim \mathrm { W N } \left( 0 , \sigma _ { U } ^ { 2 } \right)$ for each $j$ . We refer to the model (6.5.5) as the between-year model.

Example 6.5.2 Suppose $P = 0$ , $Q = 1$ , and $\Theta _ { 1 } = - 0 . 4$ in (6.5.5). Then the series for any particular month is a moving-average of order 1. If $E ( U _ { t } U _ { t + h } ) { = } 0$ for all $h$ , i.e., if the white noise sequences for different months are uncorrelated with each other, then the columns themselves are uncorrelated. The correlation function for such a process is shown in Figure 6-15.

![](images/bcf8892b85cf27004eb4b124bbd732405b7a4502e48129422cd9aa0ca355cd8c.jpg)  
Figure 6-15 The ACF of the model $X _ { t } = U _ { t } - 0 . 4 U _ { t - 1 2 }$ of Example 6.5.2

![](images/94ed0b29ce5bd5fb4ea20c15c9f864cb7c49dd1b2cfae3c754ce9f3b135d2ada.jpg)  
Figure 6-16 The ACF of the model $X _ { t } - 0 . 7 X _ { t - 1 2 } = U _ { t }$ of Example 6.5.3

# Example 6.5.3

Suppose $P = 1$ , $Q = 0$ , and $\Phi _ { 1 } = 0 . 7$ in (6.5.5). In this case the 12 series (one for each month) are AR(1) processes that are uncorrelated if the white noise sequences for different months are uncorrelated. A graph of the autocorrelation function of this process is shown in Figure 6-16.

In each of the Examples 6.5.1–6.5.3, the 12 series corresponding to the different months are uncorrelated. To incorporate dependence between these series we allow the process $\{ U _ { t } \}$ in (6.5.5) to follow an ARMA $( p , q )$ model,

$$
\phi (B) U _ {t} = \theta (B) Z _ {t}, \qquad \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right). \tag {6.5.6}
$$

This assumption implies possible nonzero correlation not only between consecutive values of $U _ { t }$ , but also within the 12 sequences $\{ U _ { j + 1 2 t } , t = \dots , - 1 , 0 , 1 , \dots \}$ $\{ U _ { j + 1 2 t }$ , each of which was assumed to be uncorrelated in the preceding examples. In this case (6.5.4)

may no longer hold; however, the coefficients in (6.5.6) will frequently have values such that $E ( U _ { t } U _ { t + 1 2 j } )$ is small for $j = \pm 1$ , $\pm 2$ , . . . . Combining the two models (6.5.5) and (6.5.6) and allowing for possible differencing leads directly to Definition 6.5.1 of the general SARIMA model as given above.

The first steps in identifying SARIMA models for a (possibly transformed) data set are to find $d$ and $D$ so as to make the differenced observations

$$
Y _ {t} = (1 - B) ^ {d} \left(1 - B ^ {s}\right) ^ {D} X _ {t}
$$

stationary in appearance (see Sections 6.1–6.3). Next we examine the sample ACF and PACF of $\{ Y _ { t } \}$ at lags that are multiples of $s$ for an indication of the orders $P$ and $Q$ in the model (6.5.5). If $\hat { \rho } ( \cdot )$ is the sample ACF of $\{ Y _ { t } \}$ , then $P$ and $Q$ should be chosen such that $\hat { \rho } ( k s )$ , k  1, 2, . . ., is compatible with the ACF of an ARMA $( P , Q )$ process. The orders $p$ and $q$ are then selected by trying to match $\hat { \rho } ( 1 ) , \ldots , \hat { \rho } ( s - 1 )$ $\hat { \rho } ( s - 1 )$ with the ACF of an ARMA $( p , q )$ process. Ultimately, the AICC criterion (Section 5.5) and the goodness of fit tests (Section 5.3) are used to select the best SARIMA model from competing alternatives.

For given values of $p , d , q , P , D$ , and $Q$ , the parameters $\phi , \theta , \Phi , \Theta$ , and $\sigma ^ { 2 }$ can be found using the maximum likelihood procedure of Section 5.2. The differences $Y _ { t } = ( 1 - B ) ^ { \bar { d } } \bigl ( 1 - B ^ { s } \bigr ) ^ { D } X _ { t }$ constitute an $\mathrm { A R M A } ( p + s P , q + s Q )$ process in which some of the coefficients are zero and the rest are functions of the $( p + P + q + Q )$ - dimensional vector $\beta ^ { \prime } = ( \phi ^ { \prime } , \Phi ^ { \prime } , \theta ^ { \prime } , \Theta ^ { \prime } )$ . For any fixed $\beta$ the reduced likelihood $\ell ( \beta )$ of the differences $Y _ { t + d + s D }$ , . . . , $Y _ { n }$ is easily computed as described in Section 5.2. The maximum likelihood estimator of $\beta$ is the value that minimizes $\ell ( \beta )$ , and the maximum likelihood estimate of $\sigma ^ { 2 }$ is given by (5.2.10). The estimates can be found using the program ITSM by specifying the required multiplicative relationships among the coefficients as given in Remark 2 above.

A more direct approach to modeling the differenced series $\{ Y _ { t } \}$ is simply to fit a subset ARMA model of the form (6.5.2) without making use of the multiplicative form of $\phi ^ { \ast } ( \cdot )$ and $\theta ^ { * } ( \cdot )$ in (6.5.1).

# Example 6.5.4 Monthly Accidental Deaths

In Figure 1-27 we showed the series $\left\{ Y _ { t } = \left( 1 { - } B ^ { 1 2 } \right) ( 1 { - } B ) X _ { t } \right\}$ obtained by differencing the accidental deaths series $\{ X _ { t } \}$ once at lag 12 and once at lag 1. The sample ACF of $\{ Y _ { t } \}$ is shown in Figure 6-17.

The values $\hat { \rho } ( 1 2 ) = - 0 . 3 3 3$ , $\hat { \rho } ( 2 4 ) = - 0 . 0 9 9$ , and $\hat { \rho } ( 3 6 ) = 0 . 0 1 3$ suggest a moving-average of order 1 for the between-year model (i.e., $P = 0$ and $Q \ : = \ : 1 $ ). Moreover, inspection of $\hat { \rho } ( 1 ) , \dotsc , \hat { \rho } ( 1 1 )$ suggests that $\rho ( 1 )$ is the only short-term correlation different from zero, so we also choose a moving-average of order 1 for the between-month model (i.e., $p = 0$ and $q = 1 \AA$ ). Taking into account the sample mean (28.831) of the differences $\{ Y _ { t } \}$ , we therefore arrive at the model

$$
Y _ {t} = 2 8. 8 3 1 + \left(1 + \theta_ {1} B\right) \left(1 + \Theta_ {1} B ^ {1 2}\right) Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right), \tag {6.5.7}
$$

for the series $\{ Y _ { t } \}$ . The maximum likelihood estimates of the parameters are obtained from ITSM by opening the file DEATHS.TSM and proceeding as follows. After differencing (at lags 1 and 12) and then mean-correcting the data, choose the option Model $>$ Specify. In the dialog box enter an MA(13) model with $\theta _ { 1 } ~ = ~ - 0 . 3$ , $\theta _ { 1 2 } = - 0 . 3 , \theta _ { 1 3 } = 0 . 0 9$ $\theta _ { 1 3 } = 0 . 0 9$ , and all other coefficients zero. (This corresponds to the initial

![](images/9ef2acb1f03f7eeb446c5f5dc82893cba51ef79c6ac04aff635286fbe06f2e5c.jpg)  
Figure 6-17 The sample ACF of the differenced accidental deaths $\{ \nabla \nabla _ { 1 2 } X _ { t } \}$

guess $Y _ { t } = ( 1 - 0 . 3 B ) \big ( 1 - 0 . 3 B ^ { 1 2 } \big ) Z _ { t }$ .) Then choose Model>Estimation>Max likelihood and click on the button Constrain optimization. Specify the number of multiplicative relations (one in this case) in the box provided and define the relationship by entering 1, 12, 13 to indicate that $\theta _ { 1 } \times \theta _ { 1 2 } = \theta _ { 1 3 }$ . Click OK to return to the Maximum Likelihood dialog box. Click OK again to obtain the parameter estimates

$$
\hat {\theta} _ {1} = - 0. 4 7 8,
$$

$$
\Theta_ {1} = - 0. 5 9 1,
$$

and

$$
\hat {\sigma} ^ {2} = 9 4, 2 5 5,
$$

with AICC value 855.53. The corresponding fitted model for $\{ X _ { t } \}$ is thus the SARIMA $( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ process

$$
\nabla \nabla_ {1 2} X _ {t} = 2 8. 8 3 1 + (1 - 0. 4 7 8 B) \left(1 - 0. 5 9 1 B ^ {1 2}\right) Z _ {t}, \tag {6.5.8}
$$

where $\{ Z _ { t } \} \sim \mathrm { { W N } } ( 0 , 9 4 3 9 0 )$ .

If we adopt the alternative approach of fitting a subset ARMA model to $\{ Y _ { t } \}$ without seeking a multiplicative structure for the operators $\phi ^ { * } ( B )$ and $\theta ^ { * } ( B )$ in (6.5.2), we begin by fitting a preliminary MA(13) model (as suggested by Figure 6-17) to the series $\{ Y _ { t } \}$ . We then fit a maximum likelihood MA(13) model and examine the standard errors of the coefficient estimators. This suggests setting the coefficients at lags 2, 3, 8, 10, and 11 equal to zero, since these are all less than one standard error from zero. To do this select Model>Estimation>Max likelihood and click on the button Constrain optimization. Then highlight the coefficients to be set to zero and click on the button Set to zero. Click OK to return to the Maximum Likelihood Estimation dialog box and again to carry out the constrained optimization. The coefficients that have been set to zero will be held at that value, and the optimization will be with respect to the remaining coefficients. This gives a model with substantially smaller AICC than the unconstrained MA(13) model. Examining the standard errors again we see that the coefficients at lags 4, 5, and 7 are promising

candidates to be set to zero, since each of them is less than one standard error from zero. Setting these coefficients to zero in the same way and reoptimizing gives a further reduction in AICC. Setting the coefficient at lag 9 to zero and reoptimizing again gives a further reduction in AICC (to 855.61) and the fitted model

$$
\nabla \nabla_ {1 2} X _ {t} = 2 8. 8 3 1 + Z _ {t} - 0. 5 9 6 Z _ {t - 1} - 0. 4 0 7 Z _ {t - 6} - 0. 6 8 5 Z _ {t - 1 2} + 0. 4 6 0 Z _ {t - 1 3},
$$

$$
\left\{Z _ {t} \right\} \sim \mathrm {W N} (0, 7 1 2 4 0). \tag {6.5.9}
$$

The AICC value 855.61 is quite close to the value 855.53 for the model (6.5.8). The residuals from the two models are also very similar, the randomness tests (with the exception of the difference-sign test) yielding high $p$ -values for both.

# 6.5.1 Forecasting SARIMA Processes

Forecasting SARIMA processes is completely analogous to the forecasting of ARIMA processes discussed in Section 6.4. Expanding out the operator $\left( 1 - B \right) ^ { d } \left( 1 - B ^ { s } \right) ^ { D }$ in powers of $B$ , rearranging the equation

$$
(1 - B) ^ {d} (1 - B ^ {s}) ^ {D} X _ {t} = Y _ {t},
$$

and setting $t = n + h$ gives the analogue

$$
X _ {n + h} = Y _ {n + h} + \sum_ {j = 1} ^ {d + D s} a _ {j} X _ {n + h - j} \tag {6.5.10}
$$

of equation (6.4.2). Under the assumption that the first $d + D s$ observations $X _ { - d - D s + 1 }$ , $\dots , X _ { 0 }$ are uncorrelated with $\{ Y _ { t } , t \ge 1 \}$ , we can determine the best linear predictors $P _ { n } X _ { n + h }$ of $X _ { n + h }$ based on $\{ 1 , X _ { - d - D s + 1 } , \ldots , X _ { n } \}$ by applying $P _ { n }$ to each side of (6.5.10) to obtain

$$
P _ {n} X _ {n + h} = P _ {n} Y _ {n + h} + \sum_ {j = 1} ^ {d + D s} a _ {j} P _ {n} X _ {n + h - j}. \tag {6.5.11}
$$

The first term on the right is just the best linear predictor of the (possibly nonzeromean) ARMA process $\{ Y _ { t } \}$ in terms of $\{ 1 , Y _ { 1 } , \ldots , Y _ { n } \}$ , which can be calculated as described in Section 3.3. The predictors $P _ { n } X _ { n + h }$ can then be computed recursively for $h = 1 , 2 , \ldots$ from (6.5.11), if we note that $P _ { n } X _ { n + 1 - j } = X _ { n + 1 - j }$ for each $j \geq 1$ .

An argument analogous to the one leading to (6.4.5) gives the prediction mean squared error as

$$
\sigma_ {n} ^ {2} (h) = E \left(X _ {n + h} - P _ {n} X _ {n + h}\right) ^ {2} = \sum_ {j = 0} ^ {h - 1} \left(\sum_ {r = 0} ^ {j} \chi_ {r} \theta_ {n + h - r - 1, j - r}\right) ^ {2} v _ {n + h - j - 1}, \tag {6.5.12}
$$

where $\theta _ { n j }$ and $\nu _ { n }$ are obtained by applying the innovations algorithm to the differenced series $\{ Y _ { t } \}$ and

$$
\chi (z) = \sum_ {r = 0} ^ {\infty} \chi_ {r} z ^ {r} = \left[ \phi (z) \Phi (z ^ {s}) (1 - z) ^ {d} (1 - z ^ {s}) ^ {D} \right] ^ {- 1}, | z | <   1.
$$

Table 6.1 Predicted values of the Accidental Deaths series for $t = 7 3 , \dots , 7 8$ , the standard deviations $\sigma _ { t }$ of the prediction errors, and the corresponding observed values of $X _ { t }$ f r the same period   

<table><tr><td>t</td><td>73</td><td>74</td><td>75</td><td>76</td><td>77</td><td>78</td></tr><tr><td colspan="7">Model (6.5.8)</td></tr><tr><td>Predictors</td><td>8441</td><td>7704</td><td>8549</td><td>8885</td><td>9843</td><td>10279</td></tr><tr><td>σt</td><td>308</td><td>348</td><td>383</td><td>415</td><td>445</td><td>474</td></tr><tr><td colspan="7">Model (6.5.9)</td></tr><tr><td>Predictors</td><td>8345</td><td>7619</td><td>8356</td><td>8742</td><td>9795</td><td>10179</td></tr><tr><td>σt</td><td>292</td><td>329</td><td>366</td><td>403</td><td>442</td><td>486</td></tr><tr><td colspan="7">Observed values</td></tr><tr><td>Xt</td><td>7798</td><td>7406</td><td>8363</td><td>8460</td><td>9217</td><td>9316</td></tr></table>

For large $n$ we can approximate (6.5.12), if $\theta ( z ) \Theta \left( z ^ { s } \right)$ is nonzero for all $| z | \leq 1$ , by

$$
\sigma_ {n} ^ {2} (h) = \sum_ {j = 0} ^ {h - 1} \psi_ {j} ^ {2} \sigma^ {2}, \tag {6.5.13}
$$

where

$$
\psi (z) = \sum_ {j = 0} ^ {\infty} \psi_ {j} z ^ {j} = \frac {\theta (z) \Theta (z ^ {s})}{\phi (z) \Phi (z ^ {s}) (1 - z) ^ {d} (1 - z ^ {s}) ^ {D}}, | z | <   1.
$$

The required calculations can all be carried out with the aid of the program ITSM. The mean squared errors are computed from the large-sample approximation (6.5.13) if the fitted model is invertible. If the fitted model is not invertible, ITSM computes the mean squared errors by converting the model to the equivalent (in terms of Gaussian likelihood) invertible model and then using (6.5.13).

# Example 6.5.5 Monthly Accidental Deaths

Continuing with Example 6.5.4, we next use ITSM to predict six future values of the Accidental Deaths series using the fitted models (6.5.8) and (6.5.9). First fit the desired model as described in Example 6.5.4 or enter the data and model directly by opening the file DEATHS.TSM, differencing at lags 12 and 1, subtracting the mean, and then entering the MA(13) coefficients and white noise variance using the option Model $>$ Specify. Select Forecasting>ARMA, and you will see the ARMA Forecast dialog box. Enter 6 for the number of predicted values required. You will notice that the default options in the dialog box are set to generate predictors of the original series by reversing the transformations applied to the data. If for some reason you wish to predict the transformed data, these check marks can be removed. If you wish to include prediction bounds in the graph of the predictors, check the appropriate box and specify the desired coefficient (e.g., $95 \%$ ). Click OK, and you will see a graph of the data with the six predicted values appended. For numerical values of the predictors and prediction bounds, right-click on the graph and then on Info. The prediction bounds are computed under the assumption that the white noise sequence in the ARMA model for the transformed data is Gaussian. Table 6.1 shows the predictors and standard deviations of the prediction errors under both models (6.5.8) and (6.5.9) for the Accidental Deaths series.

# 6.6 Regression with ARMA Errors

# 6.6.1 OLS and GLS Estimation

In standard linear regression, the errors (or deviations of the observations from the regression function) are assumed to be independent and identically distributed. In many applications of regression analysis, however, this assumption is clearly violated, as can be seen by examination of the residuals from the fitted regression and their sample autocorrelations. It is often more appropriate to assume that the errors are observations of a zero-mean second-order stationary process. Since many autocorrelation functions can be well approximated by the autocorrelation function of a suitably chosen ARMA $( p , q )$ process, it is of particular interest to consider the model

$$
Y _ {t} = \mathbf {x} _ {t} ^ {\prime} \boldsymbol {\beta} + W _ {t}, t = 1, \dots , n, \tag {6.6.1}
$$

or in matrix notation,

$$
\mathbf {Y} = X \boldsymbol {\beta} + \mathbf {W}, \tag {6.6.2}
$$

where $\textbf { Y } = ~ ( Y _ { 1 } , \ldots , Y _ { n } ) ^ { \prime }$ is the vector of observations at times $t ~ = ~ 1 , \dots , n , X$ is the design matrix whose tth row, $\mathbf { x } _ { t } ^ { \prime } ~ = ~ ( x _ { t 1 } , \ldots , x _ { t k } )$ , consists of the values of the explanatory variables at time t, $\beta = ( \beta _ { 1 } , \ldots , \beta _ { k } ) ^ { \prime }$ is the vector of regression coefficients, and the components of $\mathbf { W } = ( W _ { 1 } , \ldots , W _ { n } ) ^ { \prime }$ are values of a causal zeromean ARMA $( p , q )$ process satisfying

$$
\phi (B) W _ {t} = \theta (B) Z _ {t}, \left\{Z _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma^ {2}\right). \tag {6.6.3}
$$

The model (6.6.1) arises naturally in trend estimation for time series data. For example, the explanatory variables $x _ { t 1 } = 1 , x _ { t 2 } = t$ , and $x _ { t 3 } ~ = ~ t ^ { 2 }$ can be used to estimate a quadratic trend, and the variables $x _ { t 1 } = 1 , x _ { t 2 } = \cos ( \omega t )$ , and $x _ { t 3 } = \sin ( \omega t )$ can be used to estimate a sinusoidal trend with frequency $\omega$ . The columns of $X$ are not necessarily simple functions of $t$ as in these two examples. Any specified column of relevant variables, e.g., temperatures at times $t = 1 , \ldots , n$ , can be included in the design matrix $X$ , in which case the regression is conditional on the observed values of the variables included in the matrix.

The ordinary least squares (OLS) estimator of $\beta$ is the value, $\hat { \beta } _ { \mathrm { O L S } }$ , which minimizes the sum of squares

$$
(\mathbf {Y} - X \boldsymbol {\beta}) ^ {\prime} (\mathbf {Y} - X \boldsymbol {\beta}) = \sum_ {t = 1} ^ {n} \left(Y _ {t} - \mathbf {x} _ {t} ^ {\prime} \boldsymbol {\beta}\right) ^ {2}.
$$

Equating to zero the partial derivatives with respect to each component of $\beta$ and assuming (as we shall) that $X ^ { \prime } X$ is nonsingular, we find that

$$
\hat {\boldsymbol {\beta}} _ {\mathrm {O L S}} = \left(X ^ {\prime} X\right) ^ {- 1} X ^ {\prime} \mathbf {Y}. \tag {6.6.4}
$$

(If $X ^ { \prime } X$ is singular, $\hat { \beta } _ { \mathrm { O L S } }$ is not uniquely determined but still satisfies (6.6.4) with $( X ^ { \prime } X ) ^ { - 1 }$ any generalized inverse of $X ^ { \prime } X .$ .) The OLS estimate also maximizes the likelihood of the observations when the errors $W _ { 1 } , \ldots , W _ { n }$ are iid and Gaussian. If the design matrix $X$ is nonrandom, then even when the errors are non-Gaussian and dependent, the OLS estimator is unbiased (i.e., $E ( \hat { \beta } _ { \mathrm { O L S } } ) ~ = ~ \beta )$ and its covariance matrix is

$$
\operatorname {C o v} \left(\hat {\beta} _ {\mathrm {O L S}}\right) = \left(X ^ {\prime} X\right) ^ {- 1} X ^ {\prime} \Gamma_ {n} X \left(X ^ {\prime} X\right) ^ {- 1}, \tag {6.6.5}
$$

where $\Gamma _ { n } = E \big ( \mathbf { W } \mathbf { W } ^ { \prime } \big )$ is the covariance matrix of $\mathbf { W }$ .

The generalized least squares (GLS) estimator of $\beta$ is the value $\hat { \beta } _ { G L S }$ that minimizes the weighted sum of squares

$$
\left(\mathbf {Y} - X \beta\right) ^ {\prime} \Gamma_ {n} ^ {- 1} \left(\mathbf {Y} - X \beta\right). \tag {6.6.6}
$$

Differentiating partially with respect to each component of $\beta$ and setting the derivatives equal to zero, we find that

$$
\hat {\beta} _ {\mathrm {G L S}} = \left(X ^ {\prime} \Gamma_ {n} ^ {- 1} X\right) ^ {- 1} X ^ {\prime} \Gamma_ {n} ^ {- 1} \mathbf {Y}. \tag {6.6.7}
$$

If the design matrix $X$ is nonrandom, the GLS estimator is unbiased and has covariance matrix

$$
\operatorname {C o v} \left(\hat {\beta} _ {\mathrm {G L S}}\right) = \left(X ^ {\prime} \Gamma_ {n} ^ {- 1} X\right) ^ {- 1}. \tag {6.6.8}
$$

It can be shown that the GLS estimator is the best linear unbiased estimator of $\beta$ , i.e., for any $k$ -dimensional vector c and for any unbiased estimator $\hat { \boldsymbol \beta }$ of $\beta$ that is a linear function of the observations $Y _ { 1 } , \dots , Y _ { n }$ ,

$$
\operatorname {V a r} \left(\mathbf {c} ^ {\prime} \hat {\boldsymbol {\beta}} _ {\mathrm {G L S}}\right) \leq \operatorname {V a r} \left(\mathbf {c} ^ {\prime} \hat {\boldsymbol {\beta}}\right).
$$

In this sense the GLS estimator is therefore superior to the OLS estimator. However, it can be computed only if $\phi$ and $\pmb \theta$ are known.

Let $V ( \phi , \theta )$ denote the matrix $\sigma ^ { - 2 } \Gamma _ { n }$ and let $T ( \phi , \theta )$ be any square root of $V ^ { - 1 }$ (i.e., a matrix such that $T ^ { \prime } T = V ^ { - 1 }$ ). Then we can multiply each side of (6.6.2) by $T$ to obtain

$$
T \mathbf {Y} = T X \boldsymbol {\beta} + T \mathbf {W}, \tag {6.6.9}
$$

a regression equation with coefficient vector $\beta$ , data vector TY, design matrix TX, and error vector TW. Since the latter has uncorrelated, zero-mean components, each with variance $\sigma ^ { 2 }$ , the best linear estimator of $\beta$ in terms of TY (which is clearly the same as the best linear estimator of $\beta$ in terms of Y, i.e., $\hat { \beta } _ { \mathrm { G L S } } )$ ) can be obtained by applying OLS estimation to the transformed regression equation (6.6.9). This gives

$$
\hat {\beta} _ {\mathrm {G L S}} = \left(X ^ {\prime} T ^ {\prime} T X\right) ^ {- 1} X ^ {\prime} T ^ {\prime} T \mathbf {Y}, \tag {6.6.10}
$$

which is clearly the same as (6.6.7). Cochran and Orcutt (1949) pointed out that if $\{ W _ { t } \}$ is an $\operatorname { A R } ( p )$ process satisfying

$$
\phi (B) W _ {t} = Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right),
$$

then application of $\phi ( B )$ to each side of the regression equations (6.6.1) transforms them into regression equations with uncorrelated, zero-mean, constant-variance errors, so that ordinary least squares can again be used to compute best linear unbiased estimates of the components of $\beta$ in terms of $Y _ { t } ^ { * } = \phi ( B ) Y _ { t }$ , $t = p + 1 , \ldots , n$ . This approach eliminates the need to compute the matrix $T$ but suffers from the drawback that $\mathbf { Y ^ { * } }$ does not contain all the information in Y. Cochrane and Orcutt’s transformation can be improved, and at the same generalized to ARMA errors, as follows.

Instead of applying the operator $\phi ( B )$ to each side of the regression equations (6.6.1), we multiply each side of equation (6.6.2) by the matrix $T ( \phi , \theta )$ that maps $\{ W _ { t } \}$ into the residuals [see (5.3.1)] of $\{ W _ { t } \}$ from the ARMA model (6.6.3). We have already seen how to calculate these residuals using the innovations algorithm in Section 3.3. To see that $T$ is a square root of the matrix $V$ as defined in the previous paragraph, we simply recall that the residuals are uncorrelated with zero mean and variance $\sigma ^ { 2 }$ , so that

$$
\operatorname {C o v} (T \mathbf {W}) = T \Gamma_ {n} T ^ {\prime} = \sigma^ {2} I,
$$

where $I$ is the $n \times n$ identity matrix. Hence

$$
T ^ {\prime} T = \sigma^ {2} \Gamma_ {n} ^ {- 1} = V ^ {- 1}.
$$

GLS estimation of $\beta$ can therefore be carried out by multiplying each side of (6.6.2) by $T$ and applying ordinary least squares to the transformed regression model. It remains only to compute TY and TX.

Any data vector $\mathbf { d } = ( d _ { 1 } , \ldots , d _ { n } ) ^ { \prime }$ can be left-multiplied by $T$ simply by reading it into ITSM, entering the model (6.6.3), and pressing the green button labeled RES, which plots the residuals. (The calculations are performed using the innovations algorithm as described in Section 3.3.) The GLS estimator $\hat { \beta } _ { \mathrm { G L S } }$ is computed as follows. The data vector $\mathbf { Y }$ is left-multiplied by $T$ to generate the transformed data vector $\mathbf { Y ^ { * } }$ , and each column of the design matrix $X$ is left-multiplied by $T$ to generate the corresponding column of the transformed design matrix $X ^ { * }$ . Then

$$
\hat {\beta} _ {\mathrm {G L S}} = \left(X ^ {* ^ {\prime}} X ^ {*}\right) ^ {- 1} X ^ {* ^ {\prime}} \mathbf {Y} ^ {*}. \tag {6.6.11}
$$

The calculations of $\mathbf { Y ^ { * } } , X ^ { * }$ , and hence of $\hat { \beta } _ { \mathrm { G L S } }$ , are all carried out by the program ITSM in the option Regression $. >$ Estimation $. >$ Generalized LS.

# 6.6.2 ML Estimation

If (as is usually the case) the parameters of the $\mathbf { A R M A } ( p , q )$ model for the errors are unknown, they can be estimated together with the regression coefficients by maximizing the Gaussian likelihood

$$
L (\boldsymbol {\beta}, \phi , \boldsymbol {\theta}, \sigma^ {2}) = (2 \pi) ^ {- n / 2} (\det  \Gamma_ {n}) ^ {- 1 / 2} \exp \left\{- \frac {1}{2} (\mathbf {Y} - X \boldsymbol {\beta}) ^ {\prime} \Gamma_ {n} ^ {- 1} (\mathbf {Y} - X \boldsymbol {\beta}) \right\},
$$

where $\Gamma _ { n } \left( \phi , \theta , \sigma ^ { 2 } \right)$ is the covariance matrix of $\boldsymbol { \mathbf { W } } \ = \ \boldsymbol { \mathbf { Y } } - \boldsymbol { X } \boldsymbol { \beta } $ . Since $\{ W _ { t } \}$ is an ARMA $( p , q )$ process with parameters $\left( \phi , \theta , \sigma ^ { 2 } \right)$ , the maximum likelihood estimators $\hat { \boldsymbol { \beta } } , \hat { \boldsymbol { \phi } }$ , and $\hat { \pmb { \theta } }$ are found (as in Section 5.2) by minimizing

$$
\ell (\beta , \phi , \theta) = \ln (n ^ {- 1} S (\beta , \phi , \theta)) + n ^ {- 1} \sum_ {t = 1} ^ {n} \ln r _ {t - 1}, \tag {6.6.12}
$$

where

$$
S (\boldsymbol {\beta}, \phi , \boldsymbol {\theta}) = \sum_ {t = 1} ^ {n} \left(W _ {t} - \hat {W} _ {t}\right) ^ {2} / r _ {t - 1},
$$

$\hat { W } _ { t }$ is the best one-step predictor of $W _ { t }$ , and $r _ { t - 1 } \sigma ^ { 2 }$ is its mean squared error. The function $\ell ( \beta , \phi , \theta )$ can be expressed in terms of the observations $\{ Y _ { t } \}$ and the parameters $\beta$ , $\phi$ , and $\pmb \theta$ using the innovations algorithm (see Section 3.3) and minimized numerically to give the maximum likelihood estimators, βˆ , $\hat { \phi }$ , and $\hat { \pmb { \theta } }$ . The maximum likelihood estimator of $\sigma ^ { 2 }$ is then given, as in Section 5.2, by $\hat { \sigma } ^ { 2 } = S \left( \hat { \beta } , \hat { \phi } , \hat { \pmb { \theta } } \right) / n$ .

An extension of an iterative scheme, proposed by Cochran and Orcutt (1949) for the case $q = 0$ , simplifies the minimization considerably. It is based on the observation that for fixed $\phi$ and $\pmb \theta$ , the value of $\beta$ that minimizes $\ell ( \beta , \phi , \theta )$ is $\hat { \beta } _ { \mathrm { G L S } } ( \phi , \theta )$ , which can be computed algebraically from (6.6.11) instead of by searching numerically for the minimizing value. The scheme is as follows.

(i) Compute $\hat { \boldsymbol { \beta } } _ { \mathrm { O L S } }$ and the estimated residuals $Y _ { t } - \mathbf { x } _ { t } ^ { \prime } \hat { \boldsymbol { \beta } } _ { \mathrm { O L S } }$ , $t = 1 , \ldots , n$ .

(ii) Fit an ARMA(p.q) model by maximum Gaussian likelihood to the estimated residuals.   
(iii) For the fitted ARMA model compute the corresponding estimator $\hat { \beta } _ { \mathrm { G L S } }$ from (6.6.11).   
(iv) Compute the residuals $Y _ { t } - \mathbf { x } _ { t } ^ { \prime } \hat { \boldsymbol { \beta } } _ { \mathrm { G L S } }$ , $t = 1 , \ldots , n$ , and return to (ii), stopping when the estimators have stabilized.

If $\{ W _ { t } \}$ is a causal and invertible ARMA process, then under mild conditions on the explanatory variables $\mathbf { X } _ { t }$ , the maximum likelihood estimates are asymptotically multivariate normal (see Fuller 1976). In addition, the estimated regression coefficients are asymptotically independent of the estimated ARMA parameters.

The large-sample covariance matrix of the ARMA parameter estimators, suitably normalized, has a complicated form that involves both the regression variables $\mathbf { X } _ { t }$ and the covariance function of $\{ W _ { t } \}$ . It is therefore convenient to estimate the covariance matrix as $- H ^ { - 1 }$ , where $H$ is the Hessian matrix of the observed log-likelihood evaluated at its maximum.

The OLS, GLS, and maximum likelihood estimators of the regression coefficients all have the same asymptotic covariance matrix, so in this sense the dependence does not play a major role. However, the asymptotic covariance of both the OLS and GLS estimators can be very inaccurate if the appropriate covariance matrix $\Gamma _ { n }$ is not used in the expressions (6.6.5) and (6.6.8). This point is illustrated in the following examples.

Remark 1. The use of the innovations algorithm for GLS and ML estimation extends to regression with ARIMA errors (see Example 6.6.3 below) and FARIMA errors (FARIMA processes are defined in Section 10.5). -

# Example 6.6.1 The Overshort Data

The analysis of the overshort data in Example 3.2.8 suggested the model

$$
Y _ {t} = \beta + W _ {t},
$$

where $- \beta$ is interpreted as the daily leakage from the underground storage tank and $\{ W _ { t } \}$ is the MA(1) process

$$
W _ {t} = Z _ {t} + \theta Z _ {t - 1}, \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right).
$$

(Here $k = 1$ and $x _ { t 1 } = 1 .$ .) The OLS estimate of $\beta$ is simply the sample mean $\hat { \beta } _ { \mathrm { O L S } } =$ $\bar { Y } _ { n } ~ = ~ - 4 . 0 3 5$ . Under the assumption that $\{ W _ { t } \}$ is iid noise, the estimated variance of the OLS estimator of $\beta$ is $\hat { \gamma } _ { \scriptscriptstyle Y } ( 0 ) / 5 7 = 5 9 . 9 2$ . However, since this estimate of the variance fails to take dependence into account, it is not reliable.

To find maximum Gaussian likelihood estimates of $\beta$ and the parameters of $\{ W _ { t } \}$ using ITSM, open the file OSHORTS.TSM, select the option Regression $. >$ Specify and check the box marked Include intercept term only. Then press the blue GLS button and you will see the estimated value of $\beta$ . (This is in fact the same as the OLS estimator since the default model in ITSM is WN(0,1).) Then select Model>Estimation>Autofit and press Start. The autofit option selects the minimum AICC model for the residuals,

$$
W _ {t} = Z _ {t} - 0. 8 1 8 Z _ {t - 1}, \{Z _ {t} \} \sim \mathrm {W N} (0, 2 0 4 1), \tag {6.6.13}
$$

and displays the estimated MA coefficient $\widehat { \theta } _ { 1 } ^ { ( 0 ) } = - 0 . 8 1 8$ and the corresponding GLS estimate $\hat { \beta } _ { \mathrm { G L S } } ^ { ( 1 ) } = - 4 . 7 4 5$ 1 = −, with a standard error of 1.188, in the Regression estimates window. (If we reestimate the variance of the OLS estimator, using

(6.6.5) with $\Gamma _ { 5 7 }$ computed from the model (6.6.13), we obtain the value 2.214, a drastic reduction from the value 59.92 obtained when dependence is ignored. For a positively correlated time series, ignoring the dependence would lead to underestimation of the variance.)

Pressing the blue MLE button will reestimate the MA parameters using the residuals from the updated regression and at the same time reestimate the regression coefficient, printing the new parameters in the Regression estimates window. After this operation has been repeated several times, the parameters will stabilize, as shown in Table 6.2. Estimated $9 5 \%$ confidence bounds for $\beta$ using the GLS estimate are $- 4 . 7 5 \pm 1 . 9 6 ( 1 . 4 0 8 ) ^ { 1 / 2 } = ( - 7 . 0 7 , - 2 . 4 3 )$ , strongly suggesting that the storage tank has a leak. Such a conclusion would not have been reached without taking into account the dependence in the data.

![](images/77bb03fd85f202c92c350230b2e77f7a2385b75220f2d970cc9af7106484d73e.jpg)

Table 6.2 Estimates of $\beta$ and $\theta _ { 1 }$ for the overshort data of Example 6.6.1   

<table><tr><td>Iteration i</td><td>ˆ(i)</td><td>β1(i)</td></tr><tr><td>0</td><td>0</td><td>-4.035</td></tr><tr><td>1</td><td>-0.818</td><td>-4.745</td></tr><tr><td>2</td><td>-0.848</td><td>-4.780</td></tr><tr><td>3</td><td>-0.848</td><td>-4.780</td></tr></table>

# Example 6.6.2 The Lake Data

In Examples 5.2.4 and 5.5.2 we found maximum likelihood ARMA(1,1) and AR(2) models for the mean-corrected lake data. Now let us consider fitting a linear trend to the data with AR(2) noise. The choice of an AR(2) model was suggested by an analysis of the residuals obtained after removing a linear trend from the data using OLS. Our model now takes the form

$$
Y _ {t} = \beta_ {0} + \beta_ {1} t + W _ {t},
$$

where $\{ W _ { t } \}$ is the AR(2) process satisfying

$$
W _ {t} = \phi_ {1} W _ {t - 1} + \phi_ {2} W _ {t - 2} + Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

From Example 1.3.5, we find that the OLS estimate of $\beta$ is $\scriptstyle { \hat { \beta } } _ { \mathrm { O L S } } = ( 1 0 . 2 0 2 , - 0 . 0 2 4 2 ) ^ { \prime }$ . If we ignore the correlation structure of the noise, the estimated covariance matrix $\Gamma _ { n }$ of W is $\hat { \gamma } ( 0 ) I$ (where I is the identity matrix). The corresponding estimated covariance matrix of $\hat { \beta } _ { \mathrm { O L S } }$ is (from (6.6.5))

$$
\hat {\gamma} _ {Y} (0) \left(X ^ {\prime} X\right) ^ {- 1} = \hat {\gamma} _ {Y} (0) \left[ \begin{array}{c c} n & \sum_ {t = 1} ^ {n} t \\ \sum_ {t = 1} ^ {n} t & \sum_ {t = 1} ^ {n} t ^ {2} \end{array} \right] ^ {- 1} = \left[ \begin{array}{c c} 0. 0 7 2 0 3 & - 0. 0 0 1 1 0 \\ - 0. 0 0 1 1 0 & 0. 0 0 0 0 2 \end{array} \right]. \tag {6.6.14}
$$

However, the estimated model for the noise process, found by fitting an AR(2) model to the residuals $Y _ { t } - \hat { \beta } _ { \mathrm { O L S } } ^ { \prime } \mathbf { x } _ { t }$ , is

$$
W _ {t} = 1. 0 0 8 W _ {t - 1} - 0. 2 9 5 W _ {t - 2} + Z _ {t}, \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 4 5 7 1).
$$

Table 6.3 Estimates of $\beta$ and $\phi$ for the lake data after 3 iterations   

<table><tr><td>Iteration i</td><td>ˆφ1(i)</td><td>ˆφ2(i)</td><td>ˆβ1(i)</td><td>ˆβ2(i)</td></tr><tr><td>0</td><td>0</td><td>0</td><td>10.20</td><td>-0.0242</td></tr><tr><td>1</td><td>1.008</td><td>-0.295</td><td>10.09</td><td>-0.0216</td></tr><tr><td>2</td><td>1.005</td><td>-0.291</td><td>10.09</td><td>-0.0216</td></tr></table>

Assuming that this is the true model for $\{ W _ { t } \}$ , the GLS estimate is found to be (10.091, −0.0216)′, in close agreement with the OLS estimate. The estimated covariance matrices for the OLS and GLS estimates are given by

$$
\operatorname {C o v} \left(\hat {\boldsymbol {\beta}} _ {\mathrm {O L S}}\right) = \left[ \begin{array}{c c} 0. 2 2 1 7 7 & - 0. 0 0 3 3 5 \\ - 0. 0 0 3 3 5 & 0. 0 0 0 0 7 \end{array} \right]
$$

and

$$
\operatorname {C o v} \left(\hat {\boldsymbol {\beta}} _ {\mathrm {G L S}}\right) = \left[ \begin{array}{c c} 0. 2 1 3 9 2 & - 0. 0 0 3 2 1 \\ - 0. 0 0 3 2 1 & 0. 0 0 0 0 6 \end{array} \right].
$$

Notice how the estimated variances of the OLS and GLS estimators are nearly three times the magnitude of the corresponding variance estimates of the OLS calculated under the independence assumption [see (6.6.14)]. Estimated $95 \%$ confidence bounds for the slope $\beta _ { 1 }$ using the GLS estimate are $- 0 . 0 2 1 6 \pm 1 . 9 6 ( 0 . 0 0 0 0 6 ) ^ { 1 / 2 } = - 0 . 0 2 1 6 \pm$ .0048, indicating a significant decreasing trend in the level of Lake Huron during the years 1875–1972.

The iterative procedure described above was used to produce maximum likelihood estimates of the parameters. The calculations using ITSM are analogous to those in Example 6.6.1. The results from each iteration are summarized in Table 6.3. As in Example 6.6.1, the convergence of the estimates is very rapid.

![](images/7c91e30e614a11b8d5870c887ace33309a2080df9fe254c8675fb1b5a33b3b59.jpg)

# Example 6.6.3 Seat-Belt Legislation; SBL.TSM

Figure 6-18 shows the numbers of monthly deaths and serious injuries $Y _ { t }$ , $t \_ =$ 1, . . . , 120, on UK roads for 10 years beginning in January 1975. They are filed as SBL.TSM. Seat-belt legislation was introduced in February 1983 in the hope of reducing the mean number of monthly “deaths and serious injuries,” (from $t = 9 9$ onwards). In order to study whether or not there was a drop in mean from that time onwards, we consider the regression,

$$
Y _ {t} = a + b f (t) + W _ {t}, t = 1, \dots , 1 2 0, \tag {6.6.15}
$$

where $f _ { t } ~ = ~ 0$ for $1 ~ \leq ~ t ~ \leq ~ 9 8$ , and $f _ { t } ~ = ~ 1$ for $t ~ \geq ~ 9 9$ . The seat-belt legislation will be considered effective if the estimated value of the regression coefficient $^ b$ is significantly negative. This problem also falls under the heading of intervention analysis (see Section 11.2).

OLS regression based on the model (6.6.15) suggests that the error sequence $\{ W _ { t } \}$ is highly correlated with a strong seasonal component of period 12. (To do the regression using ITSM open the file SBL.TSM, select Regression $. >$ Specify, check only Include intercept term and Include auxiliary variables, press the Browse button, and select the file SBLIN.TSM, which contains the function $f _ { t }$ of (6.6.15) and enter 1 for the number of columns. Then select the

![](images/1cf9c18d0dae005fd9dc300c906d96465a1e08f30682939065028e591e09cdf4.jpg)  
Figure 6-18 Monthly deaths and serious injuries $\{ Y _ { t } \}$ on UK roads, January 1975–December 1984

option Regression $. >$ Estimation>Generalized LS. The estimates of the coefficients a and $^ b$ are displayed in the Regression estimates window, and the data become the estimates of the residuals $\{ W _ { t } \}$ .) The graphs of the data and sample ACF clearly suggest a strong seasonal component with period 12. In order to transform the model (6.6.15) into one with stationary residuals, we therefore consider the differenced data $X _ { t } = Y _ { t } - Y _ { t - 1 2 }$ , which satisfy

$$
X _ {t} = b g _ {t} + N _ {t}, t = 1 3, \dots , 1 2 0, \tag {6.6.16}
$$

where $g _ { t } \ = \ 1$ for $9 8 \leq t \leq 1 1 0$ , $g _ { t } \ = \ 0$ otherwise, and $\{ N _ { t } = W _ { t } - W _ { t - 1 2 } \}$ is a stationary sequence to be represented by a suitably chosen ARMA model. The series $\{ X _ { t } \}$ is contained in the file SBLD.TSM, and the function $g _ { t }$ is contained in the file SBLDIN.TSM.

The next step is to perform ordinary least squares regression of $X _ { t }$ on $g _ { t }$ following steps analogous to those of the previous paragraph (but this time checking only the box marked Include auxiliary variables in the Regression Trend Function dialog box) and again using the option Regression>Estimation> Generalized LS or pressing the blue GLS button. The model

$$
X _ {t} = - 3 4 6. 9 2 g _ {t} + N _ {t}, \tag {6.6.17}
$$

is then displayed in the Regression estimates window together with the assumed noise model (white noise in this case). Inspection of the sample ACF of the residuals suggests an MA(13) or AR(13) model for $\{ N _ { t } \}$ . Fitting AR and MA models of order up to 13 (with no mean-correction) using the option Model>Estimation>Autofit gives an MA(12) model as the minimum AICC fit for the residuals. Once this model has been fitted, the model in the Regression estimates window is automatically updated to

$$
X _ {t} = - 3 2 8. 4 5 g _ {t} + N _ {t}, \tag {6.6.18}
$$

with the fitted MA(12) model for the residuals also displayed. After several iterations (each iteration is performed by pressing the MLE button) we arrive at the model

$$
X _ {t} = - 3 2 8. 4 5 g _ {t} + N _ {t}, \tag {6.6.19}
$$

![](images/6bccb94516d6278cac00191455e5a7e5e95e51514a5c492ed975b4970ca96a2c.jpg)  
Figure 6-19 The differenced deaths and serious injuries on UK roads $\{ X _ { t } = Y _ { t } - Y _ { t - 1 2 } \} _ { } .$ , showing the fitted GLS regression line

with

$$
\begin{array}{l} N _ {t} = Z _ {t} + 0. 2 1 9 Z _ {t - 1} + 0. 0 9 8 Z _ {t - 2} + 0. 0 3 1 Z _ {t - 3} + 0. 0 6 4 Z _ {t - 4} + 0. 0 6 9 Z _ {t - 5} + 0. 1 1 1 Z _ {t - 6} \\ + 0. 0 8 1 Z _ {t - 7} + 0. 0 5 7 Z _ {t - 8} + 0. 0 9 2 Z _ {t - 9} - 0. 0 2 8 Z _ {t - 1 0} + 0. 1 8 3 Z _ {t - 1 1} - 0. 6 2 7 Z _ {t - 1 2}, \\ \end{array}
$$

where $\{ Z _ { t } \} \sim \mathrm { { W N } } ( 0 , 1 2 , 5 8 1 )$ . The estimated standard deviation of the regression coefficient estimator is 49.41, so the estimated coefficient, $- 3 2 8 . 4 5$ , is very significantly negative, indicating the effectiveness of the legislation. The differenced data are shown in Figure 6-19 with the fitted regression function.

# Problems

6.1 Suppose that $\{ X _ { t } \}$ is an $\mathbf { A R I M A } ( p , d , q )$ process satisfying the difference equations

$$
\phi (B) (1 - B) ^ {d} X _ {t} = \theta (B) Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right).
$$

Show that these difference equations are also satisfied by the process $W _ { t } = X _ { t } +$ $A _ { 0 } + A _ { 1 } t + \cdot \cdot \cdot + A _ { d - 1 } t ^ { d - 1 }$ , where $A _ { 0 } , \ldots , A _ { d - 1 }$ are arbitrary random variables.

6.2 Verify the representation given in (6.3.4).   
6.3 Test the data in Example 6.3.1 for the presence of a unit root in an AR(2) model using the augmented Dickey–Fuller test.   
6.4 Apply the augmented Dickey–Fuller test to the levels of Lake Huron data (LAKE.TSM). Perform two analyses assuming AR(1) and AR(2) models.   
6.5 If $\{ Y _ { t } \}$ is a causal ARMA process (with zero mean) and if $X _ { 0 }$ is a random variable with finite second moment such that $X _ { 0 }$ is uncorrelated with $Y _ { t }$ for each $\ t \ = \ 1 , 2 , \ldots$ , show that the best linear predictor of $Y _ { n + 1 } \mathrm { i n }$ terms of 1, $X _ { 0 } , Y _ { 1 } , \ldots , Y _ { n }$ is the same as the best linear predictor of $Y _ { n + 1 }$ in terms of 1 $, Y _ { 1 } , \ldots , Y _ { n }$ .

6.6 Let $\{ X _ { t } \}$ be the ARIMA(2,1,0) process satisfying

$$
\left(1 - 0. 8 B + 0. 2 5 B ^ {2}\right) \nabla X _ {t} = Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \mathrm {W N} (0, 1).
$$

(a) Determine the forecast function $g ( h ) = P _ { n } X _ { n + h }$ for $h > 0$   
(b) Assuming that $n$ is large, compute $\sigma _ { n } ^ { 2 } ( h )$ for $h = 1 , \ldots , 5$

6.7 Use a text editor to create a new data set ASHORT.TSM that consists of the data in AIRPASS.TSM with the last 12 values deleted. Use ITSM to find an ARIMA model for the logarithms of the data in ASHORT.TSM. Your analysis should include

(a) a logical explanation of the steps taken to find the chosen model,   
(b) approximate $9 5 \%$ bounds for the components of $\phi$ and $\pmb \theta$   
(c) an examination of the residuals to check for whiteness as described in Section 1.6,   
(d) a graph of the series ASHORT.TSM showing forecasts of the next 12 values and $9 5 \%$ prediction bounds for the forecasts,   
(e) numerical values for the 12-step ahead forecast and the corresponding $95 \%$ prediction bounds,   
(f) a table of the actual forecast errors, i.e.„ the true value (deleted from AIRPASS.TSM) minus the forecast value, for each of the 12 forecasts.

Does the last value of AIRPASS.TSM lie within the corresponding $95 \%$ prediction bounds?

6.8 Repeat Problem 6.7, but instead of differencing, apply the classical decomposition method to the logarithms of the data in ASHORT.TSM by deseasonalizing, subtracting a quadratic trend, and then finding an appropriate ARMA model for the residuals. Compare the 12 forecast errors found from this approach with those found in Problem 6.7.   
6.9 Repeat Problem 6.7 for the series BEER.TSM, deleting the last 12 values to create a file named BSHORT.TSM.   
6.10 Repeat Problem 6.8 for the series BEER.TSM and the shortened series BSHORT.TSM.   
6.11 A time series $\{ X _ { t } \}$ is differenced at lag 12, then at lag 1 to produce a zero-mean series $\{ Y _ { t } \}$ with the following sample ACF:

$$
\hat {\rho} (1 2 j) \approx (0. 8) ^ {j}, \quad \quad \quad \quad \quad j = 0, \pm 1, \pm 2, \dots ,
$$

$$
\hat {\rho} (1 2 j \pm 1) \approx (0. 4) (0. 8) ^ {j}, \quad j = 0, \pm 1, \pm 2, \dots ,
$$

$$
\hat {\rho} (h) \approx 0, \quad \text {o t h e r w i s e},
$$

and $\hat { \gamma } ( 0 ) = 2 5$

(a) Suggest a SARIMA model for $\{ X _ { t } \}$ specifying all parameters.   
(b) For large $n$ , express the one- and twelve-step linear predictors $P _ { n } X _ { n + 1 }$ and $P _ { n } X _ { n + 1 2 }$ in terms of $X _ { t }$ , $t = - 1 2$ $t = - 1 2 , - 1 1 , \ldots , n$ , and $\hat { Y _ { t } } - \hat { Y } _ { t }$ , $t = 1 , \ldots , n$ .   
(c) Find the mean squared errors of the predictors in (b).

6.12 Use ITSM to verify the calculations of Examples 6.6.1–6.6.3.

6.13 The file TUNDRA.TSM contains the average maximum temperature over the month of February for the years 1895-1993 in an area of the USA whose vegetation is characterized as tundra.

(a) Fit a straight line to the data using OLS. Is the slope of the line significantly different from zero?   
(b) Find an appropriate ARMA model to the residuals from the OLS fit in (a).   
(c) Calculate the MLE estimates of the intercept and the slope of the line and the ARMA parameters in (a). Is the slope of the line significantly different from zero?   
(d) Use your model to forecast the average maximum temperature for the years 1994–2004.

# Time Series Models for Financial Data

7.1 Historical Overview   
7.2 GARCH Models   
7.3 Modified GARCH Processes   
7.4 Stochastic Volatility Models   
7.5 Continuous-Time Models   
7.6 An Introduction to Option Pricing

In this chapter we discuss some of the time series models which have been found useful in the analysis of financial data. These include both discrete-time and continuoustime models, the latter being used widely, following the celebrated work of Black, Merton and Scholes, for the pricing of stock options. The closing price on trading day $t$ , say $P _ { t }$ , of a particular stock or stock-price index, typically appears to be nonstationary while the log asset price, $X _ { t } : = \log ( P _ { t } )$ , has observed sample-paths like those of a random walk with stationary uncorrelated increments, i.e., the differenced log asset price, $Z _ { t } ~ : = ~ X _ { t } - X _ { t - 1 }$ , known as the log return (or simply return) for day t, has sample-paths resembling those of white noise. Although the sequence $Z _ { t }$ appears to be white noise, there is strong evidence to suggest that it is not independent white noise. Much of the analysis of financial time series is devoted to representing and exploiting this dependence, which is not visible in the sample autocorrelation function of $\{ Z _ { t } \}$ . The continuous time analogue of a random walk with independent and identically distributed increments is known as a Lévy process, the most familiar examples of which are the Poisson process and Brownian motion. Lévy processes play a key role in the continuous-time modeling of financial data, both as models for the log asset price itself and as building blocks for more complex models. We give a brief introduction to these processes and some of the continuous-time models constructed from them. Finally we consider the pricing of European stock options using the geometric Brownian motion model for stock prices, a model which, in spite of its limitations, has been found useful in practice.

# 7.1 Historical Overview

For more than 30 years now, discrete-time models (including stochastic volatility, ARCH, GARCH and their many generalizations) have been developed to reflect the so-called stylized features of financial time series. These properties, which include tail heaviness, asymmetry, volatility clustering and serial dependence without correlation, cannot be captured with traditional linear time series models such as the ARMA models considered earlier in this book. If $P _ { t }$ denotes the price of a stock or other financial asset at time t, $t \in \mathbb { Z }$ , then the series of log returns, $\{ Z _ { t } : = \log P _ { t } - \log P _ { t - 1 } \}$ , is typically modeled as a stationary time series. An ARMA model for the series $\{ Z _ { t } \}$ would have the property that the conditional variance $h _ { t }$ of $Z _ { t }$ given $\{ Z _ { s } , s ~ < ~ t \}$ is independent of $t$ and of $\{ Z _ { s } , s ~ < ~ t \}$ . However even a cursory inspection of most empirical log return series (see e.g., Figure 7-4) strongly suggests that this is not the case in practice. The fundamental idea of the ARCH (autoregressive conditional heteroscedasticity) model (Engle 1982) is to incorporate the sequence $\{ h _ { t } \}$ into the model by postulating that

$$
Z _ {t} = \sqrt {h _ {t}} e _ {t}, \text {w h e r e} \{e _ {t} \} \sim \mathrm {I I D N} (0, 1)
$$

and $h _ { t }$ (known as the volatility) is related to the past values of $Z _ { t } ^ { 2 }$ via a relation of the form,

$$
h _ {t} = \alpha_ {0} + \sum_ {i = 1} ^ {p} \alpha_ {i} Z _ {t - i} ^ {2},
$$

for some positive integer $p$ , where $\alpha _ { 0 } > 0$ and $\alpha _ { i } \geq 0 , \ i = 1 , \ldots , p$ . The GARCH (generalized ARCH) model of Bollerslev (1986) postulates a more general relation,

$$
h _ {t} = \alpha_ {0} + \sum_ {i = 1} ^ {p} \alpha_ {i} Z _ {t - i} ^ {2} + \sum_ {i = 1} ^ {q} \beta_ {i} h _ {t - i},
$$

with $\alpha _ { 0 } > 0$ , $\alpha _ { i } \ge 0 , i = 1 , \ldots , p$ , and $\beta _ { i } \ge 0 , i = 1 , \ldots , q$ . These models have been studied intensively since their introduction and a variety of parameter estimation techniques have been developed. They will be discussed in Section 7.2 and some of their extensions in Section 7.3.

An alternative approach to modeling the changing variability of log returns, due to Taylor (1982), is to suppose that $Z _ { t } ~ = ~ \sqrt { h _ { t } } e _ { t }$ , where $\{ e _ { t } \} \sim \mathrm { \Pi { I I D } } ( 0 , 1 )$ and the volatility sequence $\{ h _ { t } \}$ is independent of $\{ e _ { t } \}$ . (Taylor originally allowed $\{ e _ { t } \}$ to be an autoregression, but it is now customary to use the more restrictive definition just given.) A critical difference from the ARCH and GARCH models is the fact that the conditional distribution of $h _ { t }$ given $\{ h _ { s } , s < t \}$ is independent of $\{ e _ { s } , s < t \}$ . A widely used special case of this model is the so-called log-normal stochastic volatility (SV) model in which $\{ e _ { t } \} \sim \mathrm { I I D } \mathrm { N } ( 0 , 1 )$ , $\ln h _ { t } = \gamma _ { 0 } + \gamma _ { 1 } \ln h _ { t - 1 } + \eta _ { t }$ , $\{ \eta _ { t } \} \sim \mathrm { I I D } \mathrm { N } ( 0 , \sigma ^ { 2 } )$ and $\{ \eta _ { t } \}$ and $\{ e _ { t } \}$ are independent. We shall discuss this model in Section 7.4.

Continuous-time models for financial time series have a long history, going back at least to Bachelier (1900), who used Brownian motion to represent the prices $\{ P ( t ) , t \geq 0 \}$ of a stock in the Paris stock exchange. This model had the unfortunate feature of permitting negative stock prices, a shortcoming which was eliminated in the geometric Brownian motion model of Samuelson (1965), according to which $P ( t )$ satisfies an Itô stochastic differential equation of the form,

$$
\mathrm {d} P (t) = \mu P (t) \mathrm {d} t + \sigma P (t) \mathrm {d} B (t),
$$

where $\mu \in \mathbb { R }$ , $\sigma > 0$ and $B$ is standard Brownian motion. For any fixed positive value of $P ( 0 )$ the solution (see Section 7.5.2 and Appendix D.4) is

$$
P (t) = P (0) \exp \left[ (\mu - \sigma^ {2} / 2) t + \sigma B (t) \right], t \geq 0,
$$

so that the log asset price, $X ( t ) : = \log P ( t )$ , is Brownian motion and the log return over the time-interval $( t , t + \varDelta )$ is

$$
X (t + \varDelta) - X (t) = (\mu - \frac {1}{2} \sigma^ {2}) \varDelta + \sigma (B (t + \varDelta) - B (t)).
$$

For disjoint intervals of length $\varDelta$ the log returns are therefore independent normally distributed random variables with mean $( \mu - \sigma ^ { 2 } / 2 ) \varDelta$ and variance $\sigma ^ { 2 } \varDelta$ . The normality is a conclusion which can easily be checked against observed log returns, and it is found that although the observed values are approximately normally distributed for intervals $\varDelta$ greater than 1 day, the deviations from normality are substantial for shorter time intervals. This is one of the reasons for developing the more realistic models described in Section 7.5. The parameter $\sigma ^ { 2 }$ is called the volatility parameter of the geometric Brownian motion model and plays a key role in the celebrated option pricing results (see Section 7.6) developed for this model by Black, Scholes and Merton, earning the Nobel Economics Prize for Merton and Scholes in 1997 (unfortunately Black died before the award was made). These results inspired an explosion of interest, not only in the pricing of more complicated financial derivatives, but also in the development of new continuous-time models which, like the discrete-time ARCH, GARCH and stochastic volatility models, better reflect the observed properties of financial time series.

# 7.2 GARCH Models

For modeling changing volatility as discussed above, Engle (1982) introduced the $\mathbf { A R C H } ( p )$ process $\{ Z _ { t } \}$ as a stationary solution of the equations

$$
Z _ {t} = \sqrt {h _ {t}} e _ {t}, \{e _ {t} \} \sim \text {I I D N} (0, 1), \tag {7.2.1}
$$

where $h _ { t }$ is the (positive) function of $\{ Z _ { s } , s < t \}$ , defined by

$$
h _ {t} = \alpha_ {0} + \sum_ {i = 1} ^ {p} \alpha_ {i} Z _ {t - i} ^ {2}, \tag {7.2.2}
$$

with $\alpha _ { 0 } ~ > ~ 0$ and $\alpha _ { j } \ge 0 , j = 1 , \ldots , p$ . The name ARCH signifies autoregressive conditional heteroscedasticity and $h _ { t }$ is the conditional variance of $Z _ { t }$ given $\{ Z _ { s } , s < t \}$ .

The simplest such process is the ARCH(1) process. In this case the recursions (7.2.1) and (7.2.2) give

$$
\begin{array}{l} Z _ {t} ^ {2} = \alpha_ {0} e _ {t} ^ {2} + \alpha_ {1} Z _ {t - 1} ^ {2} e _ {t} ^ {2} \\ = \alpha_ {0} e _ {t} ^ {2} + \alpha_ {1} \alpha_ {0} e _ {t} ^ {2} e _ {t - 1} ^ {2} + \alpha_ {1} ^ {2} Z _ {t - 2} ^ {2} e _ {t} ^ {2} e _ {t - 1} ^ {2} \\ = \dots \\ = \alpha_ {0} \sum_ {j = 0} ^ {n} \alpha_ {1} ^ {j} e _ {t} ^ {2} e _ {t - 1} ^ {2} \dots e _ {t - j} ^ {2} + \alpha_ {1} ^ {n + 1} Z _ {t - n - 1} ^ {2} e _ {t} ^ {2} e _ {t - 1} ^ {2} \dots e _ {t - n} ^ {2}. \\ \end{array}
$$

If $\alpha _ { 1 } < 1$ and $\{ Z _ { t } \}$ is stationary and causal (i.e., $Z _ { t }$ is a function of $\{ e _ { s } , s \le t \} ,$ ), then the last term has expectation $\alpha ^ { n + 1 } E Z _ { t } ^ { 2 }$ and converges to zero as $n \to \infty$ . The first term converges as $n \to \infty$ since it is non-decreasing in $n$ and its expected value is bounded above by $\alpha _ { 0 } / ( 1 - \alpha _ { 1 } )$ . Hence

$$
Z _ {t} ^ {2} = \alpha_ {0} \sum_ {j = 0} ^ {\infty} \alpha_ {1} ^ {j} e _ {t} ^ {2} e _ {t - 1} ^ {2} \dots e _ {t - j} ^ {2} \tag {7.2.3}
$$

and

$$
E Z _ {t} ^ {2} = \alpha_ {0} / (1 - \alpha_ {1}). \tag {7.2.4}
$$

Since

$$
Z _ {t} = e _ {t} \sqrt {\alpha_ {1} \left(1 + \sum_ {j = 1} ^ {\infty} \alpha_ {1} ^ {j} e _ {t - 1} ^ {2} \cdots e _ {t - j} ^ {2}\right)}, \tag {7.2.5}
$$

it is clear that $\{ Z _ { t } \}$ is strictly stationary and hence, since $E Z _ { t } ^ { 2 } \ < \ \infty$ , also stationary in the weak sense. We have now established the following result.

# Solution of the ARCH(1) Equations:

If $\alpha _ { 1 } < 1$ , the unique causal stationary solution of the ARCH(1) equations is given by (7.2.5). It has the properties

$$
E \left(Z _ {t}\right) = E \left(E \left(Z _ {t} \mid e _ {s}, s <   t\right)\right) = 0,
$$

$$
\operatorname {V a r} \left(Z _ {t}\right) = \alpha_ {0} / \left(1 - \alpha_ {1}\right),
$$

and

$$
E (Z _ {t + h} Z _ {t}) = E (E (Z _ {t + h} Z _ {t} | e _ {s}, s <   t + h)) = 0 \mathrm {f o r} h > 0.
$$

Thus the ARCH(1) process with $\alpha _ { 1 } < 1$ is strictly stationary white noise. However, it is not an iid sequence, since from (7.2.1) and (7.2.2),

$$
E (Z _ {t} ^ {2} | Z _ {t - 1}) = (\alpha_ {0} + \alpha_ {1} Z _ {t - 1} ^ {2}) E (e _ {t} ^ {2} | Z _ {t - 1}) = \alpha_ {0} + \alpha_ {1} Z _ {t - 1} ^ {2}.
$$

This also shows that $\{ Z _ { t } \}$ is not Gaussian, since strictly stationary Gaussian white noise is necessarily iid. From (7.2.5) it is clear that the distribution of $Z _ { t }$ is symmetric, i.e., that $Z _ { t }$ and $- Z _ { t }$ have the same distribution. From (7.2.3) it is easy to calculate $E \big ( Z _ { t } ^ { 4 } \big )$ (Problem 7.1) and hence to show that $E \big ( Z _ { t } ^ { 4 } \big )$ is finite if and only if $3 \alpha _ { 1 } ^ { 2 } \ < \ 1$ . More generally (see Engle 1982), it can be shown that for every $\alpha _ { 1 }$ in the interval $( 0 , 1 )$ , $\dot { E } \big ( Z ^ { 2 k } \big ) \stackrel { \cdot } { = } \infty$ for some positive integer $k$ . This indicates the “heavy-tailed” nature of the marginal distribution of $Z _ { t }$ . If $E Z _ { t } ^ { 4 } < \infty$ , the squared process $Y _ { t } = Z _ { t } ^ { 2 }$ has the same ACF as the AR(1) process $W _ { t } = \alpha _ { 1 } W _ { t - 1 } + e _ { t }$ , a result that extends also to $\operatorname { A R C H } ( p )$ processes (see Problem 7.3).

The $\operatorname { A R C H } ( p )$ process is conditionally Gaussian, in the sense that for given values of $\{ Z _ { s } , s = t - 1 , t - 2 , \ldots , t - p \}$ , $Z _ { t }$ is Gaussian with known distribution. This makes it easy to write down the likelihood of $Z _ { p + 1 } , \ldots , Z _ { n }$ conditional on $\{ Z _ { 1 } , \ldots , Z _ { p } \}$ and hence, by numerical maximization, to compute conditional maximum likelihood estimates of the parameters. For example, the conditional likelihood of observations $\{ z _ { 2 } , \ldots , z _ { n } \}$ of the ARCH(1) process given $Z _ { 1 } = z _ { 1 }$ is

![](images/63d8caf99c139a0dabf7e5b2311c41a74e3bb3e18a7f6915fc257fa00fb1abcc.jpg)  
Figure 7-1 A realization of the process $Z _ { t } = \mathbf { e } _ { t } \sqrt { 1 + 0 . 5 Z _ { t - 1 } ^ { 2 } }$

![](images/d33af1b1968dc99669eafcb6d4ada5b0a7d7a6ea5479cfc91d24583c064770b4.jpg)  
Figure 7-2 The sample autocorrelation function of the series in Figure 7-1

$$
L = \prod_ {t = 2} ^ {n} \frac {1}{\sqrt {2 \pi (\alpha_ {0} + \alpha_ {1} z _ {t - 1} ^ {2})}} \exp \left\{- \frac {z _ {t} ^ {2}}{2 (\alpha_ {0} + \alpha_ {1} z _ {t - 1} ^ {2})} \right\}.
$$

# Example 7.2.1 An ARCH(1) Series

Figure 7-1 shows a realization of the ARCH(1) process with $\alpha _ { 0 } = 1$ and $\alpha _ { 1 } = 0 . 5$ . The graph of the realization and the sample autocorrelation function shown in Figure 7-2 suggest that the process is white noise. This conclusion is correct from a second-order point of view.

However, the fact that the series is not a realization of iid noise is very strongly indicated by Figure 7-3, which shows the sample autocorrelation function of the series $\left\{ Z _ { t } ^ { 2 } \right\}$ . (The sample ACF of $\{ \vert Z _ { t } \vert \}$ and that of $\{ Z _ { t } ^ { 2 } \}$ can be plotted in ITSM by selecting Statistics>Residual Analysis>ACF abs values/Squares.)

The sample autocorrelation function of the squares of the data shown in Figure 7-1

![](images/2f3bb432af273533df600c7bf763eb73f2270fcf28e2929d3e7e32eac2d9575c.jpg)  
Figure 7-3

It is instructive to apply the Ljung–Box and McLeod–Li portmanteau tests for white noise to this series (see Section 1.6). To do this using ITSM, open the file ARCH.TSM, and then select Statistics>Residual Analysis $>$ Tests of Randomness. We find (with $h = 2 0$ ) that the Ljung–Box test (and all the others except for the McLeod–Li test) are passed comfortably at level 0.05. However, the McLeod–Li test gives a $p$ -value of 0 to five decimal places, clearly rejecting the hypothesis that the series is iid.

The $\mathbf { G A R C H } ( p , \ q )$ process (see Bollerslev 1986) is a generalization of the $\operatorname { A R C H } ( p )$ process in which the variance equation (7.2.2) is replaced by

$$
h _ {t} = \alpha_ {0} + \sum_ {i = 1} ^ {p} \alpha_ {i} Z _ {t - i} ^ {2} + \sum_ {j = 1} ^ {q} \beta_ {j} h _ {t - j}, \tag {7.2.6}
$$

with $\alpha _ { 0 } > 0$ and $\alpha _ { j }$ , $\beta _ { j } \ge 0 , j = 1 , 2 , \ldots .$

In the analysis of empirical financial data such as percentage daily stock returns (defined as $1 0 0 \ln ( P _ { t } / P _ { t - 1 } )$ , where $P _ { t }$ is the closing price on trading day t), it is usually found that better fits to the data are obtained by relaxing the Gaussian assumption in (7.2.1) and supposing instead that the distribution of $Z _ { t }$ given $\{ Z _ { s } , s < t \}$ has a heaviertailed zero-mean distribution such as Student’s $t$ -distribution. To incorporate such distributions we can define a general ${ \mathrm { G A R C H } } ( p , q )$ process as a stationary process $\{ Z _ { t } \}$ satisfying (7.2.6) and the generalized form of (7.2.1),

$$
Z _ {t} = \sqrt {h _ {t}} e _ {t}, \quad \left\{e _ {t} \right\} \sim \operatorname {I I D} (0, 1). \tag {7.2.7}
$$

For modeling purposes it is usually assumed in addition that either

$$
e _ {t} \sim N (0, 1), \tag {7.2.8}
$$

(as in (7.2.1)) or that

$$
\sqrt {\frac {\nu}{\nu - 2}} e _ {t} \sim t _ {\nu}, \quad \nu > 2, \tag {7.2.9}
$$

![](images/e1a4b3b96c269f1f10d6ead526a76f6895742151872590420cb9e02ffe748ffc.jpg)  
Figure 7-4 The daily percentage returns of the Dow Jones Industrial Index (E1032.TSM) from July 1, 1997, through April 9, 1999 (above), and the estimates of $\sigma _ { t } = \sqrt { h _ { t } }$ for the conditional Gaussian GARCH(1,1) model of Example 7.2.2

![](images/2d17af8643a3dfadc3b292ec4cf8f3c6b4d1ddb077dcf2f6ac9decada2ef2699.jpg)

where $t _ { \nu }$ denotes Student’s $t$ -distribution with $\nu$ degrees of freedom. (The scale factor on the left of (7.2.9) is introduced to make the variance of $e _ { t }$ equal to 1.) Other distributions for $e _ { t }$ can also be used.

One of the striking features of stock return data that is reflected by GARCH models is the “persistence of volatility,” or the phenomenon that large (small) fluctuations in the data tend to be followed by fluctuations of comparable magnitude. GARCH models reflect this by incorporating correlation in the sequence $\{ h _ { t } \}$ of conditional variances.

# Example 7.2.2 Fitting GARCH Models to Stock Data

The top graph in Figure 7-4 shows the percentage daily returns of the Dow Jones Industrial Index for the period July 1st, 1997, through April 9th, 1999, contained in the file E1032.TSM. The graph suggests that there are sustained periods of both high volatility (in October, 1997, and August, 1998) and of low volatility. The sample autocorrelation function of this series, like that in Example 7.2.1, has very small values, however the sample autocorrelations of the absolute values and squares of the data (like those in Example 7.2.1) are significantly different from zero, indicating dependence in spite of the lack of autocorrelation. (The sample autocorrelations of the absolute values and squares of the residuals (or of the data if no transformations have been made and no model fitted) can be seen by clicking on the third green button at the top of the ITSM window.) These properties suggest that an ARCH or GARCH model might be appropriate for this series.

The model

$$
Y _ {t} = a + Z _ {t}, \tag {7.2.10}
$$

where $\{ Z _ { t } \}$ is the ${ \mathrm { G A R C H } } ( p , q )$ process defined by (7.2.6)–(7.2.8), can be fitted using ITSM as follows. Open the project E1032.TSM and click on the red button labeled GAR at the top of the ITSM screen. In the resulting dialog box enter the desired values of $p$ and $q$ , e.g., 1 and 1 if you wish to fit a GARCH(1,1) model. You may also enter

initial values for the coefficients $\alpha _ { 0 } , \ldots , \alpha _ { p }$ , and $\beta _ { 1 } , \ldots , \beta _ { q }$ , or alternatively use the default values specified by the program. Make sure that Use normal noise is selected, click on OK and then click on the red MLE button. You will be advised to subtract the sample mean (unless you wish to assume that the parameter $a$ in (7.2.10) is zero). If you subtract the sample mean it will be used as the estimate of $a$ in the model (7.2.10). The GARCH Maximum Likelihood Estimation box will then open. When you click on OK the optimization will proceed. Denoting by $\{ \tilde { Z } _ { t } \}$ the (possibly) mean-corrected observations, the GARCH coefficients are estimated by numerically maximizing the likelihood of $\tilde { Z } _ { p + 1 } , \ldots , \tilde { Z } _ { n }$ conditional on the known values $\tilde { Z } _ { 1 } , \ldots , \tilde { Z } _ { p }$ , and with assumed values 0 for each $\tilde { Z } _ { t }$ , $t \leq 0$ , and $\hat { \sigma } ^ { 2 }$ for each $h _ { t }$ , $t \leq 0$ , where $\hat { \sigma } ^ { 2 }$ is the sample variance of $\{ \tilde { Z } _ { 1 } , \dots , \tilde { Z } _ { n } \}$ . In other words the program maximizes

$$
L (\alpha_ {0}, \ldots , \alpha_ {p}, \beta_ {1}, \ldots , \beta_ {q}) = \prod_ {t = p + 1} ^ {n} \frac {1}{\sigma_ {t}} \phi \bigg (\frac {\tilde {Z} _ {t}}{\sigma_ {t}} \bigg), \tag {7.2.11}
$$

with respect to the coefficients $\alpha _ { 0 } , \ldots , \alpha _ { p }$ and $\beta _ { 1 } , \ldots , \beta _ { q }$ , where $\phi$ denotes the standard normal density, and the standard deviations $\sigma _ { t } = \sqrt { h _ { t } } , t \ge 1$ , are computed recursively from (7.2.6) with $Z _ { t }$ replaced by $\tilde { Z } _ { t }$ , and with $\tilde { Z } _ { t } = 0$ and $h _ { t } \ = \ \hat { \sigma } ^ { 2 }$ for $t \leq 0$ . To find the minimum of $- 2 \mathrm { l n } ( L )$ it is advisable to repeat the optimization by clicking on the red MLE button and then on OK several times until the result stabilizes. It is also useful to try other initial values for $\alpha _ { 0 } , \ldots , \alpha _ { p }$ , and $\beta _ { 1 } , \ldots , \beta _ { q }$ , to minimize the chance of finding only a local minimum of $- 2 \mathrm { l n } ( L )$ . Note that the optimization is constrained so that the estimated parameters are all non-negative with

$$
\hat {\alpha} _ {1} + \dots + \hat {\alpha} _ {p} + \hat {\beta} _ {1} + \dots + \hat {\beta} _ {q} <   1, \tag {7.2.12}
$$

and $\hat { \alpha } _ { 0 } \ > \ 0$ . Condition (7.2.12) is necessary and sufficient for the corresponding GARCH equations to have a causal weakly stationary solution.

Comparison of models with different orders $p$ and $q$ can be made with the aid of the AICC, which is defined in terms of the conditional likelihood $L$ as

$$
\mathrm {A I C C} := - 2 \frac {n}{n - p} \ln L + 2 (p + q + 2) n / (n - p - q - 3). \tag {7.2.13}
$$

The factor $n / ( n - p )$ multiplying the first term on the right has been introduced to correct for the fact that the number of factors in (7.2.11) is only $n - p$ . Notice also that the $\mathrm { G A R C H } ( p , q )$ model has $p + q + 1$ coefficients.

The estimated mean is $\hat { a } = 0 . 0 6 0 8$ and the minimum-AICC GARCH model (with Gaussian noise) for the residuals, $\tilde { Z } _ { t } = Y _ { t } - \hat { a }$ , is found to be the GARCH(1,1) with estimated parameter values

$$
\hat {\alpha} _ {0} = 0. 1 3 0 0, \hat {\alpha} _ {1} = 0. 1 2 6 6, \hat {\beta} _ {1} = 0. 7 9 2 2,
$$

and an AICC value [defined by (7.2.13)] of 1469.02. The bottom graph in Figure 7-4 shows the corresponding estimated conditional standard deviations, $\hat { \sigma } _ { t }$ , which clearly reflect the changing volatility of the series $\{ Y _ { t } \}$ . This graph is obtained from ITSM by clicking on the red SV (stochastic volatility) button. Under the model defined by (7.2.6)–(7.2.8) and (7.2.10), the GARCH residuals, $\left\{ \tilde { Z } _ { t } / \hat { \sigma } _ { t } \right\}$ , should be approximately IID N(0,1). A check on the independence is provided by the sample ACF of the absolute values and squares of the residuals, which is obtained by clicking on the fifth red button at the top of the ITSM window. These are found to be not significantly different from zero. To check for normality, select Garch $. >$ Garch residuals>QQ-Plot(normal). If the model is appropriate the resulting graph should approximate a straight line through the origin with slope 1. It is found that the

deviations from the expected line are quite large for large values of $\left| \tilde { Z } _ { t } \right|$ , suggesting the need for a heavier-tailed model, e.g., a model with conditional $t$ -distribution as defined by (7.2.9).

To fit the GARCH model defined by (7.2.6), (7.2.7), (7.2.9) and (7.2.10) (i.e., with conditional $t$ -distribution), we proceed in the same way, but with the conditional likelihood replaced by

$$
L \left(\alpha_ {0}, \dots , \alpha_ {p}, \beta_ {1}, \dots , \beta_ {q}, v\right) = \prod_ {t = p + 1} ^ {n} \frac {\sqrt {v}}{\sigma_ {t} \sqrt {v - 2}} t _ {v} \left(\frac {\tilde {Z} _ {t} \sqrt {v}}{\sigma_ {t} \sqrt {v - 2}}\right). \tag {7.2.14}
$$

Maximization is now carried out with respect to the coefficients, $\alpha _ { 0 } , \ldots , \alpha _ { p }$ , $\beta _ { 1 } , \ldots , \beta _ { q }$ and the degrees of freedom $\nu$ of the t-density, $t _ { \nu }$ . The optimization can be performed using ITSM in exactly the same way as described for the GARCH model with Gaussian noise, except that the option Use t-distribution for noise should be checked in each of the dialog boxes where it appears. In order to locate the minimum of $- 2 \mathrm { l n } ( L )$ it is often useful to initialize the coefficients of the model by first fitting a GARCH model with Gaussian noise and then carrying out the optimization using $t$ -distributed noise.

The estimated mean is $\hat { a } = 0 . 0 6 0 8$ as before and the minimum-AICC GARCH model for the residuals, $\tilde { Z } _ { t } = Y _ { t } - \hat { a }$ , is the GARCH(1,1) with estimated parameter values

$$
\hat {\alpha} _ {0} = 0. 1 3 2 4, \quad \hat {\alpha} _ {1} = 0. 0 6 7 2, \quad \hat {\beta} _ {1} = 0. 8 4 0 0, \quad \hat {\nu} = 5. 7 1 4,
$$

and an AICC value (as in (7.2.13) with $q$ replaced by $q + 1 )$ ) of 1437.89. Thus from the point of view of AICC, the model with conditional $t$ -distribution is substantially better than the conditional Gaussian model. The sample ACF of the absolute values and squares of the GARCH residuals are much the same as those found using Gaussian noise, but the qq plot (obtained by clicking on the red QQ button and based on the tdistribution with 5.714 degrees of freedom) is closer to the expected line than was the case for the model with Gaussian noise.

There are many important and interesting theoretical questions associated with the existence and properties of stationary solutions of the GARCH equations and their moments and of the sampling properties of these processes. As indicated above, in maximizing the conditional likelihood, ITSM constrains the GARCH coefficients to be non-negative and to satisfy the condition (7.2.12) with $\hat { \alpha } _ { 0 } > 0$ . These conditions are sufficient for the process defined by the GARCH equations to be stationary. It is frequently found in practice that the estimated values of $\alpha _ { 1 } , \ldots , \alpha _ { p }$ and $\beta _ { 1 } , \ldots , \beta _ { q }$ have a sum which is very close to 1. A GARCH(p,q) model with $\alpha _ { 1 } + \cdots + \alpha _ { p } +$ $\beta _ { 1 } + \cdots \beta _ { q } = 1$ is called I-GARCH (or integrated GARCH). Many generalizations of GARCH processes (ARCH-M, E-GARCH, I-GARCH, T-GARCH, FI-GARCH, etc., as well as ARMA models driven by GARCH noise, and regression models with GARCH errors) can now be found in the econometrics literature see Andersen et al. (2009).

ITSM can be used to fit ARMA and regression models with GARCH noise by using the procedures described in Example 7.2.2 to fit a GARCH model to the residuals $\{ \tilde { Z } _ { t } \}$ from the ARMA (or regression) fit.

# Example 7.2.3 Fitting ARMA Models Driven by GARCH Noise

If we open the data file SUNSPOTS.TSM, subtract the mean and use the option Model>Estimation>Autofit with the default ranges for $p$ and $q$ , we obtain an

ARMA(3,4) model for the mean-corrected data. Clicking on the second green button at the top of the ITSM window, we see that the sample ACF of the ARMA residuals is compatible with iid noise. However the sample autocorrelation functions of the absolute values and squares of the residuals (obtained by clicking on the third green button) indicate that they are not independent. To fit a Gaussian GARCH(1,1) model to the ARMA residuals click on the red GAR button, enter the value 1 for both $p$ and $q$ and click OK. Then click on the red MLE button, click OK in the dialog box, and the GARCH ML Estimates window will open, showing the estimated parameter values. Repeat the steps in the previous sentence two more times and the window will display the following ARMA(3,4) model for the mean-corrected sunspot data and the fitted GARCH model for the ARMA noise process $\{ Z _ { t } \}$ ,

$$
\begin{array}{l} X _ {t} = 2. 4 6 3 X _ {t - 1} - 2. 2 4 8 X _ {t - 2} + 0. 7 5 7 X _ {t - 3} + Z _ {t} - 0. 9 4 8 Z _ {t - 1} \\ - 0. 2 9 6 Z _ {t - 2} + 0. 3 1 3 Z _ {t - 3} + 0. 1 3 6 Z _ {t - 4}, \\ \end{array}
$$

where

$$
Z _ {t} = \sqrt {h _ {t}} e _ {t}
$$

and

$$
h _ {t} = 3 1. 1 5 2 + 0. 2 2 3 Z _ {t - 1} ^ {2} + 0. 5 9 6 h _ {t - 1}.
$$

The AICC value for the GARCH fit (805.12) should be used for comparing alternative GARCH models for the ARMA residuals. The AICC value adjusted for the ARMA fit (821.70) should be used for comparison with alternative ARMA models (with or without GARCH noise). Standard errors of the estimated coefficients are also displayed.

Simulation using the fitted ARMA(3,4) model with GARCH (1,1) noise can be carried out by selecting Garch>Simulate Garch process. If you retain the settings in the ARMA Simulation dialog box and click OK you will see a simulated realization of the model for the original data in SUNSPOTS.TSM.

Some useful references for extensions and further properties of GARCH models are Weiss (1986), Engle (1995), Shephard (1996), Gourieroux (1997), Lindner (2009) and Francq and Zakoian (2010).

# 7.3 Modified GARCH Processes

The following are so-called “stylized features” associated with observed time series of financial returns:

(i) the marginal distributions have heavy tails,   
(ii) there is persistence of volatility,   
(iii) the returns exhibit aggregational Gaussianity,   
(iv) there is asymmetry with respect to negative and positive disturbances and   
(v) the volatility frequently exhibits long-range dependence.

The properties (i), (ii) and (iii) are well accounted for by the GARCH models of Section 7.2. Property (iii) means that the sum, $\begin{array} { r } { S _ { n } \ = \ \sum _ { t = 1 } ^ { n } Z _ { t } } \end{array}$ , of the daily returns,

$Z _ { t } = \ln P _ { t } { - } \ln P _ { t - 1 }$ , is approximately normally distributed if $\mathbf { \xi } _ { n }$ is large. For the GARCH model with $E Z _ { t } ^ { 2 } = \sigma ^ { 2 } < \infty$ it follows from the martingale central limit theorem (see e.g. Billingsley (1995)) that $\begin{array} { r } { n ^ { - 1 / 2 } ( \ln P _ { n } - \ln P _ { 0 } ) = n ^ { - 1 / 2 } \sum _ { t = 1 } ^ { n } Z _ { t } } \end{array}$ is asymptotically $\mathrm { N } ( 0 , \sigma ^ { 2 } )$ , in accordance with (iii).

To account for properties (iv) and (v) the EGARCH and FIGARCH models were devised.

# 7.3.1 EGARCH Models

To allow negative and positive values of $e _ { t }$ in the definition of the GARCH process to have different impacts on the subsequent volatilities, $h _ { s }$ , $\mathbf { \boldsymbol { \mathscr { s } } } > t \mathbf { \boldsymbol { \mathscr { t } } }$ , Nelson (1991) introduced EGARCH models, illustrated in the following simple example.

# Example 7.3.1 EGARCH(1,1)

Consider the process $\{ Z _ { t } \}$ defined by the equations,

$$
Z _ {t} = \sqrt {h _ {t}} e _ {t}, \{e _ {t} \} \sim \operatorname {I I D} (0, 1), \tag {7.3.1}
$$

where $\{ \ell _ { t } : = \ln h _ { t } \}$ is the weakly and strictly stationary solution of

$$
\ell_ {t} = c + \alpha_ {1} g \left(e _ {t - 1}\right) + \gamma_ {1} \ell_ {t - 1}, \tag {7.3.2}
$$

$$
c \in \mathbb {R}, \alpha_ {1} \in \mathbb {R}, | \gamma_ {1} | <   1,
$$

$$
g \left(e _ {t}\right) = e _ {t} + \lambda \left(\left| e _ {t} \right| - E \left| e _ {t} \right|\right), \tag {7.3.3}
$$

and $e _ { t }$ has a distribution symmetric about zero, i.e., $e _ { t } \overset { \mathrm { ~ d ~ } } { = } - e _ { t }$

The process is defined in terms of $\ell _ { t }$ to ensure that $h _ { t } ( = e ^ { \ell _ { t } } ) > 0$ . Equation (7.3.3) can be rewritten as

$$
g (e _ {t}) = \left\{ \begin{array}{l l} (1 + \lambda) e _ {t} - \lambda E | e _ {t} | & \text {i f} e _ {t} \geq 0, \\ (1 - \lambda) e _ {t} - \lambda E | e _ {t} | & \text {i f} e _ {t} <   0. \end{array} \right.
$$

showing that the function $g$ is piecewise linear with slope $( 1 + \lambda )$ on $( 0 , \infty )$ and slope $( 1 - \lambda )$ on $( - \infty , 0 )$ . This asymmetry in $g$ allows $\ell _ { t }$ , to respond differently to positive and negative shocks $e _ { t - 1 }$ of the same magnitude. If $\lambda = 0$ there is no asymmetry.

When fitting EGARCH models to stock prices it is usually found that the estimated value of $\lambda$ is negative, corresponding to large negative shocks having greater impact on volatility than positive ones of the same magnitude.

Properties of $\{ g ( e _ { t } ) \}$ : (i) $\{ g ( e _ { t } ) \}$ is iid.

(ii) Eg(et)  0.   
$\begin{array} { r } { \mathrm { ( i i i ) } \mathrm { V a r } ( g ( e _ { t } ) ) = 1 + \lambda ^ { 2 } \mathrm { V a r } ( | e _ { t } | ) . } \end{array}$

(The symmetry of $e _ { t }$ implies that $e _ { t }$ and $| e _ { t } | - E | e _ { t } |$ are uncorrelated.)

More generally, the $\mathbf { E G A R C H } ( p , q )$ process is obtained by replacing the equation (7.3.2) for $l _ { t } : = \ln h _ { t }$ by

$$
\ell_ {t} = c + \alpha (B) g \left(e _ {t}\right) + \gamma (B) \ell_ {t}, \tag {7.3.4}
$$

where

$$
\alpha (B) = \sum_ {i = 1} ^ {p} \alpha_ {i} B ^ {i}, \gamma (B) = \sum_ {i = 1} ^ {q} \gamma_ {i} B ^ {i}.
$$

Clearly $\{ \ell _ { t } \} , \{ h _ { t } \}$ and $\{ Z _ { t } \}$ are all strictly stationary and causal if $1 - \gamma ( z )$ is non-zero for all complex z such that $| z | \leq 1$ .

Nelson also proposed the use of the generalized error distribution (GED) for $e _ { t }$ , with density

$$
f (x) = \frac {\nu \exp [ (- 1 / 2) | x / \xi | ^ {\nu} ]}{\xi \cdot 2 ^ {1 + 1 / \nu} \Gamma (1 / \nu)},
$$

where

$$
\xi = \left\{\frac {2 ^ {(- 2 / \nu)} \Gamma (1 / \nu)}{\Gamma (3 / \nu)} \right\} ^ {1 / 2}
$$

and $\nu > 0$ . The value of $\xi$ ensures that $\mathrm { V a r } ( e _ { t } ) = 1$ and the parameter $\nu$ determines the tail heaviness. For $\nu = 2$ , $e _ { t } \sim \Nu ( 0 , 1 )$ . Tail heaviness increases as $\nu$ decreases.

Properties of the GED: (i) $f$ is symmetric and $\scriptstyle { \frac { 1 } { 2 } } | e _ { t } / \xi | ^ { \nu }$ has the gamma distribution with parameters $1 / \nu$ and 1 (see Appendix A.1, Example (d)).

(ii) The specified value of $\xi$ ensures that $\mathrm { V a r } ( e _ { t } ) = 1$ .   
$\begin{array} { r } { E | e _ { t } | ^ { k } = \frac { \Gamma ( ( k + 1 ) / \nu ) } { \Gamma ( 1 / \nu ) } } \end{array}$ · # Ŵ(1/ν)Ŵ(3/ν) $k/2 .

# Inference via Conditional Maximum Likelihood

As in Section 7.2 we initialize the recursions (7.3.1) and (7.3.4) by supposing that

(i)   
(ii) $e _ { t } = 0 , t \leq 0$

Then $h _ { 1 }$ , $e _ { 1 }$ $\begin{array} { r l } { \mathrm { ~  ~ \omega ~ } ( = } & { { } Z _ { 1 } / \sqrt { h _ { 1 } } ) } \end{array}$ , $h _ { 2 }$ , $e _ { 2 } , . . .$ , can be computed recursively from the observations $Z _ { 1 } , Z _ { 2 } , . . . .$ , and the recursions defining the process.

The conditional likelihood is then computed as

$$
L = \prod_ {t = 1} ^ {n} \frac {1}{\sqrt {h _ {t}}} f \left(\frac {Z _ {t}}{\sqrt {h _ {t}}}\right).
$$

We therefore need to minimize

$$
- 2 \ln L = \sum_ {t = 1} ^ {n} \ln h _ {t} + \sum_ {t = 1} ^ {n} \left| \frac {Z _ {t}}{\xi \sqrt {h _ {t}}} \right| ^ {\nu} + 2 n \ln \left(\frac {2 \xi}{\nu} \cdot 2 ^ {1 / \nu} \Gamma (1 / \nu)\right)
$$

with respect to

$$
c, \lambda , \nu , \alpha_ {1}, \dots , \alpha_ {p}, \gamma_ {1}, \dots , \gamma_ {q}.
$$

Since $h _ { t }$ is automatically positive, the only constraints in this optimization are the conditions

$$
\nu > 0
$$

and

$$
1 - \gamma (z) \neq 0 \text {f o r a l l c o m p l e x} z \text {s u c h t h a t} | z | \leq 1.
$$

# 7.3.2 FIGARCH and IGARCH Models

To allow for the very slow decay of the sample ACF frequently observed in long daily squared return series, the FIGARCH (fractionally integrated GARCH) models were developed. Before introducing them we first give a very brief account of fractionally integrated ARMA processes. (For more details see Section 11.4 and Brockwell and Davis (1991), Section 13.2.)

# Fractionally Integrated ARMA Processes and “Long Memory”

The autocorrelation function $\rho ( \cdot )$ of an ARMA process at lag $h$ converges rapidly to zero as $h  \infty$ in the sense that there exists $r > 1$ such that

$$
r ^ {h} \rho (h) \rightarrow 0, \text {a s} h \rightarrow \infty .
$$

The fractionally integrated ARMA (or ARFIMA) process of order $( p , d , q )$ , where $p$ and $q$ are non-negative integers and $0 < d < 0 . 5$ , is a stationary time series with an autocorrelation function which for large lags decays at a much slower rate. It is defined to be the zero-mean stationary solution $\{ X _ { t } \}$ of the difference equations

$$
(1 - B) ^ {d} \phi (B) X _ {t} = \theta (B) Z _ {t}, \tag {7.3.5}
$$

where $\phi ( z )$ and $\theta ( z )$ are polynomials of degrees $p$ and $q$ respectively, with no common zeroes, satisfying

$$
\phi (z) \neq 0 \text {a n d} \theta (z) \neq 0 \quad \text {f o r a l l c o m p l e x} z \text {s u c h t h a t} | z | \leq 1,
$$

$\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , \sigma ^ { 2 } )$ , $B$ is the backward shift operator, and $( 1 - B ) ^ { r }$ , is defined via the power series expansion,

$$
(1 - z) ^ {r} := 1 + \sum_ {j = 1} ^ {\infty} \frac {r (r - 1) \dots (r - j + 1)}{j !} (- z) ^ {j}, | z | <   1, r \in \mathbb {R}.
$$

The zero-mean stationary process $\{ X _ { t } \}$ defined by (7.3.5) has the mean-square convergent $\mathbf { M A } ( \infty )$ representation,

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j},
$$

where $\psi _ { j }$ is the coefficient of $z ^ { j }$ in the power series expansion,

$$
\psi (z) = (1 - z) ^ {- d} \theta (z) / \phi (z), | z | <   1.
$$

The autocorrelations $\rho ( j )$ of $\{ X _ { t } \}$ at $\log j$ and the coefficients $\psi _ { j }$ both converge to zero at hyperbolic rates as $j \to \infty$ ; specifically, there exist non-zero constants $\gamma$ and $\delta$ such that

$$
j ^ {1 - d} \psi_ {j} \rightarrow \gamma \text {a n d} j ^ {1 - 2 d} \rho (j) \rightarrow \delta .
$$

Thus $\psi _ { j }$ and $\rho ( j )$ converge to zero as $j ~  ~ \infty$ at much slower rates than the corresponding coefficients and autocorrelations of an ARMA process. Consequently fractionally integrated ARMA processes are said to have “long memory". The spectral density of $\{ X _ { t } \}$ is given by

$$
f (\lambda) = \frac {\sigma^ {2}}{2 \pi} \frac {| \theta (e ^ {- i \lambda}) | ^ {2}}{| \phi (e ^ {- i \lambda}) | ^ {2}} | 1 - e ^ {- i \lambda} | ^ {- 2 d}.
$$

The exact Gaussian likelihood $L$ of observations $\mathbf { x } _ { n } = ( x _ { 1 } , \ldots , x _ { n } ) ^ { \prime }$ of a fractionally integrated ARMA process is given by

$$
- 2 \ln (L) = n \ln (2 \pi) + \ln \det \Gamma_ {n} + \mathbf {x} _ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \mathbf {x} _ {n},
$$

where $\Gamma _ { n } = E ( \mathbf { X } _ { n } \mathbf { X } _ { n } ^ { \prime } )$ . Calculation and maximization with respect to the parameters $d , \phi _ { 1 } , \ldots , \phi _ { p } , \theta _ { 1 } , \ldots , \theta _ { q }$ $\theta _ { 1 } , \ldots , \theta _ { q }$ and $\sigma ^ { 2 }$ is difficult. It is much easier to maximize the Whittle approximation $L _ { W }$ (see (11.4.10)), i.e. to minimize

$$
- 2 \ln (L _ {W}) = n \ln (2 \pi) + \sum_ {j} \ln (2 \pi f (\omega_ {j})) + \sum_ {j} \frac {I _ {n} (\omega_ {j})}{2 \pi f (\omega_ {j})},
$$

where $I _ { n }$ is the periodogram, and $\textstyle \sum _ { j }$ denotes the sum over all nonzero Fourier frequencies, $\omega _ { j } ~ = ~ 2 \pi j / n ~ \in ~ ( - \pi , \bar { \pi } ]$ . The program ITSM allows estimation of parameters for $\mathbf { A R I M A } ( p , d , q )$ models either by minimizing $- 2 \ln ( L _ { W } )$ , or by the slower and more computationally intensive process of minimizing $- 2 \ln ( L )$ .

# Fractionally Integrated GARCH Processes

In order to incorporate long memory into the family of GARCH models, (Baillie et al. 1996) defined a fractionally integrated GARCH (FIGARCH) process as a causal strictly stationary solution of the difference equations (7.3.9) and (7.3.10) specified below.

To motivate the definition, we recall that the ${ \mathrm { G A R C H } } ( p , q )$ process is the causal stationary solution of the equations,

$$
Z _ {t} = \sqrt {h _ {t}} e _ {t}, h _ {t} = \alpha_ {0} + \sum_ {i = 1} ^ {p} \alpha_ {i} Z _ {t - i} ^ {2} + \sum_ {i = 1} ^ {q} \beta_ {i} h _ {t - i}, \tag {7.3.6}
$$

where $\alpha _ { 0 } > 0 , \alpha _ { 1 } , \ldots , \alpha _ { p } \geq 0$ and $\beta _ { 1 } , \ldots , \beta _ { q } \geq 0$ . It follows (Problem 7.5) that

$$
(1 - \alpha (B) - \beta (B)) Z _ {t} ^ {2} = \alpha_ {0} + (1 - \beta (B)) W _ {t}, \tag {7.3.7}
$$

where $\{ W _ { t } : = Z _ { t } ^ { 2 } - h _ { t } \}$ is white noise, $\begin{array} { r } { \alpha ( B ) = \sum _ { i = 1 } ^ { p } \alpha _ { i } B ^ { i } } \end{array}$ and $\begin{array} { r } { \beta ( B ) = \sum _ { i = 1 } ^ { q } \beta _ { i } B ^ { i } } \end{array}$ . There is a causal weakly stationary solution for $\{ Z _ { t } \}$ if and only if the zeroes of $1 - \alpha ( z ) -$ $\beta ( z )$ have absolute value greater than 1 and there is then exactly one such solution (Bollerslev 1986).

In order to define the $\mathbf { I G A R C H } ( p , q )$ (integrated ${ \mathrm { G A R C H } } ( p , q ) )$ process, Engle and Bollerslev (1986) supposed that the polynomial $( 1 - \alpha ( z ) - \beta ( z ) )$ has a simple zero at $z = 1$ , and that the other zeroes all fall outside the closed unit disc as in (7.3.6). Under these assumptions we can write

$$
(1 - \beta (z) - \alpha (z)) = (1 - z) \phi (z),
$$

where $\phi ( z )$ is a polynomial with all of its zeroes outside the unit circle. We then say [cf. (7.3.6)] that $\{ Z _ { t } \}$ is an $\operatorname { I G A R C H } ( p , q )$ process if it satisfies

$$
\phi (B) (1 - B) Z _ {t} ^ {2} = \alpha_ {0} + (1 - \beta (B)) W _ {t}, \tag {7.3.8}
$$

with $Z _ { t } = \sqrt { h _ { t } } e _ { t }$ , $W _ { t } = Z _ { t } ^ { 2 } - h _ { t }$ and $\{ e _ { t } \} \sim \mathrm { { I I D } } ( 0 , 1 )$ . Bougerol and Picard (1992) showed that if the distribution of $e _ { t }$ has unbounded support and no atom at zero then there is a unique strictly stationary causal solution of these equations for $\{ Z _ { t } \}$ . The

solution has the property that $E Z _ { t } ^ { 2 } = \infty$ . In practice, for GARCH models fitted to empirical data, it is often found that $\alpha ( 1 ) / + \beta ( 1 ) \approx 1$ , supporting the practical relevance of the IGARCH model even though $E Z _ { t } ^ { 2 } = \infty$ .

Baillie et al. (1996) defined the $\mathbf { F I G A R C H } ( p , d , q )$ process $\{ Z _ { t } \}$ to be a causal strictly stationary solution of the equations,

$$
Z _ {t} = \sqrt {h _ {t}} e _ {t}, \tag {7.3.9}
$$

and [cf. (7.3.8)]

$$
\phi (B) \left(1 - B\right) ^ {d} Z _ {t} ^ {2} = \alpha_ {0} + \left(1 - \beta (B)\right) W _ {t}, 0 <   d <   1, \tag {7.3.10}
$$

where $W _ { t } = Z _ { t } ^ { 2 } - h _ { t }$ , $\{ e _ { t } \} \sim \mathrm { { I I D } } ( 0 , 1 )$ and the polynomials $\phi ( z )$ and $1 - \beta ( z )$ are non-zero for all complex $z$ such that $| z | \leq 1$ . Substituting $W _ { t } = Z _ { t } ^ { 2 } - h _ { t }$ in (7.3.10) we see that (7.3.10) is equivalent to the equation,

$$
h _ {t} = \frac {\alpha_ {0}}{1 - \beta (1)} + \left[ 1 - (1 - \beta (B)) ^ {- 1} \phi (B) (1 - B) ^ {d} \right] Z _ {t} ^ {2}, \tag {7.3.11}
$$

which means that the $\operatorname { F I G A R C H } ( p , q )$ process can be regarded as a special case of the $\mathbf { I A R C H } ( \infty )$ process defined by (7.3.9) and

$$
h _ {t} = a _ {0} + \sum_ {j = 1} ^ {\infty} a _ {j} Z _ {t - j} ^ {2}, \tag {7.3.12}
$$

with $a _ { 0 } > 0$ and $\textstyle \sum _ { j = 1 } ^ { \infty } a _ { j } = 1$ . The questions of existence and uniqueness of causal strictly stationary solutions of the ${ \mathrm { I A R C H } } ( \infty )$ (including FIGARCH) equations have not yet been fully resolved. Any strictly stationary solution must have infinite variance since if $\sigma ^ { 2 } : = E Z _ { t } ^ { 2 } = E h _ { t } < \infty$ then, since $\textstyle \sum _ { j = 1 } ^ { \infty } a _ { j } = 1$ , it follows from (7.3.12) that $\sigma ^ { 2 } = a _ { 0 } + \sigma ^ { 2 }$ , contradicting the finiteness of $\sigma ^ { 2 }$ . Sufficient conditions for the existence of causal strictly stationary solution of the IARCH) $( \infty )$ , and in particular of the FIGARCH equations, have been given by Douc et al. (2008).

Other models, based on changing volatility levels, have been proposed to explain the “long-memory” effect in stock and exchange rate returns. Fractionally integrated E-GARCH models have also been introduced (Bollerslev and Mikkelsen 1996) in order to account for both long memory and asymmetry of the effects of positive and negative shocks $e _ { t }$ .

# 7.4 Stochastic Volatility Models

The general discrete-time stochastic volatility (SV) model for the log return sequence $\{ Z _ { t } \}$ defined in Section 7.1 is [cf. (7.2.1)]

$$
Z _ {t} = \sqrt {h _ {t}} e _ {t}, t \in \mathbb {Z}, \tag {7.4.1}
$$

where $\{ e _ { t } \} \sim \mathrm { I I D } ( 0 , 1 )$ , $\{ h _ { t } \}$ is a strictly stationary sequence of non-negative random variables, independent of $\{ e _ { t } \}$ , and $h _ { t }$ is known, like the corresponding quantity in the GARCH models, as the volatility at time t. Note however that in the GARCH models, the sequences $\{ h _ { t } \}$ and $\{ e _ { t } \}$ are not independent since $h _ { t }$ depends on $e _ { s }$ , $s < t$ through the defining equation (7.2.6).

The independence of $\{ h _ { t } \}$ and $\{ e _ { t } \}$ in the SV model (7.4.1) allows us to model the volatility process with any non-negative strictly stationary sequence we may wish to choose. This contrasts with the GARCH models in which the processes $\{ Z _ { t } \}$ and $\{ h _ { t } \}$

are inextricably linked. Inference for the GARCH models, based on observations of $Z _ { 1 } , \ldots , Z _ { n }$ , can be carried out using the conditional likelihood, which is easily written down, as in (7.2.14), in terms of the marginal probability density of the sequence $\{ e _ { t } \}$ . Inference for an SV model based on observations of $\{ Z _ { t } \}$ however is considerably more difficult since the process is driven by two independent random sequences rather than one and only $\{ Z _ { t } \}$ is observed. The unobserved sequence $\{ h _ { t } \}$ is said to be latent.

A general account of the probabilistic properties of SV models can be found in Davis and Mikosch (2009) and an extensive history and overview of both discrete-time and continuous-time SV models in Shephard and Andersen (2009). In this section we shall focus attention on an early, but still widely used, special case of the SV model due to Taylor (1982, 1986) known as the lognormal SV model.

The lognormal SV process $\{ Z _ { t } \}$ is defined as,

$$
Z _ {t} = \sqrt {h _ {t}} e _ {t}, \{e _ {t} \} \sim \mathrm {I I D N} (0, 1), \tag {7.4.2}
$$

where $h _ { t } = e ^ { \ell _ { t } }$ , $\{ \ell _ { t } \}$ is a (strictly and weakly) stationary solution of the equations

$$
\ell_ {t} = \gamma_ {0} + \gamma_ {1} \ell_ {t - 1} + \eta_ {t}, \{\eta_ {t} \} \sim \mathrm {I I D N} (0, \sigma^ {2}), \tag {7.4.3}
$$

$| \gamma _ { 1 } | < 1$ and the sequences $\{ \boldsymbol { e } _ { t } \}$ and $\{ \eta _ { t } \}$ are independent. The sequence $\{ \ell _ { t } \}$ is clearly a Gaussian AR(1) process with mean

$$
\mu_ {\ell} := E \ell_ {t} = \frac {\gamma_ {0}}{1 - \gamma_ {1}} \tag {7.4.4}
$$

and variance

$$
v _ {\ell} := \operatorname {V a r} \left(\ell_ {t}\right) = \frac {\sigma^ {2}}{1 - \gamma_ {1} ^ {2}}. \tag {7.4.5}
$$

# Properties of $\{ Z _ { t } \}$ .

(i) $\{ Z _ { t } \}$ is strictly stationary.   
(ii) Moments:

$$
\begin{array}{l} E Z _ {t} ^ {r} = E \left(e _ {t} ^ {r}\right) E \exp \left(r \ell_ {t} / 2\right) \\ = \left\{ \begin{array}{l l} 0, & \text {i f r i s o d d}, \\ {[ \prod_ {i = 1} ^ {m} (2 i - 1) ] \exp \left(\frac {m \gamma_ {0}}{1 - \gamma_ {1}} + \frac {m ^ {2} \sigma^ {2}}{2 (1 - \gamma_ {1} ^ {2})}\right),} & \text {i f r = 2 m}. \end{array} \right. \\ \end{array}
$$

(iii) Kurtosis:

$$
\frac {E Z _ {t} ^ {4}}{(E Z _ {t} ^ {2}) ^ {2}} = 3 \exp \left(\frac {\sigma^ {2}}{1 - \gamma_ {1} ^ {2}}\right) \geq 3.
$$

Kurtosis (defined by the ratio on the left) is a standard measure of tail heaviness. For a normally distributed random variable it has the value 3, so, as measured by kurtosis, the tails of the marginal distribution of the lognormal SV process are heavier than those of a normally distributed random variable.

(iv) The autocovariance function of $\{ Z _ { t } ^ { 2 } \}$ :

We first observe that if $t > s$

$$
E (Z _ {t} ^ {2} Z _ {s} ^ {2} | e _ {u}, \eta_ {u}, u <   t) = h _ {s} h _ {t} e _ {s} ^ {2} E (e _ {t} ^ {2} | e _ {u}, \eta_ {u}, u <   t) = h _ {s} h _ {t} e _ {s} ^ {2},
$$

since $h _ { s } , h _ { t }$ and $e _ { s } ^ { 2 }$ are each functions of $\{ e _ { u } , \eta _ { u } , u < t \}$ and $e _ { t } ^ { 2 }$ is independent of $\{ e _ { u } , \eta _ { u } , u < t \}$ . Taking expectations on both sides of the last equation and using the independence of $\{ h _ { t } \}$ and $\{ e _ { t } \}$ and the relation $h _ { t } = \exp ( l _ { t } )$ gives

$$
E (Z _ {t} ^ {2} Z _ {s} ^ {2}) = E \exp (\ell_ {t} + \ell_ {s}).
$$

Hence, for $h > 0$ ,

$$
\begin{array}{l} \operatorname {C o v} \left(Z _ {t + h} ^ {2}, Z _ {t} ^ {2}\right) = E \exp \left(\ell_ {t + h} + \ell_ {t}\right) - E \exp \left(\ell_ {t + h}\right) E \exp \left(\ell_ {t}\right) \\ = \exp [ 2 \mu_ {\ell} + v _ {\ell} (1 + \gamma_ {1} ^ {2}) ] - \exp [ 2 \mu_ {\ell} + v _ {\ell} ]. \\ \end{array}
$$

Here we have used the facts that $\ell _ { t + h }$ is normally distributed with mean and variance which are easily computed from (7.2.17) and that for a normally distributed random variable $X$ with mean $\mu$ and variance $\nu , E \exp ( X ) = \exp ( \mu +$ $\nu / 2 )$ ). From (ii) we also have

$$
\operatorname {V a r} \left(Z _ {t} ^ {2}\right) = E Z _ {t} ^ {4} - \left(E Z _ {t} ^ {2}\right) ^ {2} = 3 \exp \left(2 \mu_ {\ell} + 2 v _ {l}\right) - \exp \left(2 \mu_ {\ell} + v _ {l}\right).
$$

Hence, for $h > 0$

$$
\rho_ {Z _ {t} ^ {2}} (h) = \frac {\operatorname {C o v} \left(Z _ {t + h} ^ {2} , Z _ {t} ^ {2}\right)}{\operatorname {V a r} \left(Z _ {t} ^ {2}\right)} = \frac {\exp \left(v _ {\ell} \gamma_ {1} ^ {h}\right) - 1}{3 \exp \left(v _ {\ell}\right) - 1} \sim \frac {v _ {\ell}}{3 \exp \left(v _ {\ell}\right) - 1} \gamma_ {1} ^ {h}, \text {a s} \gamma_ {1} \rightarrow 0,
$$

suggesting the approximation of the autocorrelation function of $\{ Z _ { t } ^ { 2 } \}$ by that of an ARMA(1,1) process. (Recall from Example 3.2.1 that the autocorrelation function of an ARMA(1,1) process has the form $\rho ( h ) ~ = ~ c \phi ^ { h } , h ~ \ge ~ 1$ , with $\rho ( 0 ) = 1 .$ ) There is a similarity here to the autocovariance function of the squared GARCH(1,1) process which (see Problem 7.3) has the autocovariance function of an ARMA(1,1) process.

(v) The process $\{ \ln Z _ { t } ^ { 2 } \}$ :

$$
\ln Z _ {t} ^ {2} = \ell_ {t} + \ln e _ {t} ^ {2}. \tag {7.4.6}
$$

If $e _ { t } \sim \mathrm { N } ( 0 , 1 )$ then $E \ln e _ { t } ^ { 2 } = - 1 . 2 7$ and $\operatorname { V a r } ( \ln e _ { t } ^ { 2 } ) = 4 . 9 3 .$ . From (7.4.6) we find at once that $\operatorname { V a r } ( \ln Z _ { t } ^ { 2 } ) = \nu _ { l } + 4 . 9 3$ and $\mathrm { C o v } ( Z _ { t + h } ^ { 2 } , Z _ { t } ) = \nu _ { l } \gamma _ { 1 } ^ { | h | }$ for $h \neq 0$ . Hence the process $\{ \ln Z _ { t } ^ { 2 } \}$ has the autocovariance function of an ARMA(1,1) process with autocorrelation function

$$
\rho_ {\ln Z _ {l} ^ {2}} (h) = \frac {v _ {l} \gamma_ {1} ^ {| h |}}{v _ {l} + 4 . 9 3}, h \neq 0.
$$

# Estimation for the lognormal SV model

The parameters to be estimated in the defining equations (7.4.2) and (7.4.3) are $\sigma ^ { 2 }$ , $\gamma _ { 0 }$ and $\gamma _ { 1 }$ . They can be estimated by maximization of the Gaussian likelihood which can be calculated, for any specified values of the parameters, as follows.

By property (v) above, the process $\{ Y _ { t } : = \ln Z _ { t } ^ { 2 } - E \ln Z _ { t } ^ { 2 } \}$ satisfies the ARMA(1,1) equations,

$$
Y _ {t} - \phi Y _ {t - 1} = Z _ {t} + \theta Z _ {t - 1}, \left\{Z _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma_ {Z} ^ {2}\right), \tag {7.4.7}
$$

for some coefficients $\phi$ and $\theta$ in the interval $( - 1 , 1 )$ and white-noise variance $\sigma _ { Z } ^ { 2 }$ . Comparing the autocorrelation function of (7.4.7) with the autocorrelation function of $\{ \ln Z _ { t } ^ { 2 } \}$ given above in Property (v), we find that

$$
\gamma_ {1} = \phi \tag {7.4.8}
$$

and

$$
\frac {v _ {\ell}}{v _ {\ell} + 4 . 9 3} = \frac {(\theta + \phi) (1 + \theta \phi)}{1 + 2 \theta \phi + \theta^ {2}}. \tag {7.4.9}
$$

To ensure that the right-hand side falls in the interval (0, 1) it is necessary and sufficient (assuming that $\phi ~ \in ~ ( - 1 . 1 )$ and $\theta ~ \in ~ ( - 1 , 1 ) \rangle$ ) that $\phi + \theta \ > \ 0$ . The maximum

Gaussian likelihood estimators $\hat { \phi }$ and $\hat { \theta }$ can be found using the program ITSM and the corresponding estimators $\hat { \gamma } _ { 1 }$ and $\hat { \nu } _ { \ell }$ on replacing $\phi$ and $\theta$ by their estimators in (7.4.8) and (7.4.9) respectively. From (7.4.5) the corresponding estimator of $\sigma ^ { 2 }$ is

$$
\hat {\sigma} ^ {2} = (1 - \hat {\gamma_ {1}} ^ {2}) \hat {v} _ {\ell},
$$

where $\hat { \gamma } _ { 1 } = \hat { \phi }$ and, from (7.4.4) and (7.4.6), the corresponding estimator of $\gamma _ { 0 }$ is

$$
\hat {\gamma_ {0}} = (1 - \hat {\gamma_ {1}}) (\overline {{\ln Z _ {t} ^ {2}}} + 1. 2 7),
$$

where $\overline { { \ln Z _ { t } ^ { 2 } } }$ denotes the sample mean of the observations of $\ln Z _ { t } ^ { 2 }$ . If it turns out that the estimators $\hat { \phi }$ and $\hat { \theta }$ satisfy $\hat { \phi } + \hat { \theta } \leq 0$ then, from (7.4.9), $\hat { \nu } _ { \ell } \leq 0$ , suggesting that the lognormal SV model is not appropriate in this case.

# Forecasting the log volatility

The minimum mean-squared error predictor of $\ell _ { t + h }$ conditional on $\{ \ell _ { s } , s \le t \}$ is easily found from (7.4.3) to be

$$
P _ {t} \ell_ {t + h} = \gamma_ {1} ^ {h} \ell_ {t} + \gamma_ {0} \frac {1 - \gamma_ {1} ^ {h}}{1 - \gamma_ {1}}, \tag {7.4.10}
$$

with mean-squared error,

$$
E \left(\ell_ {t + h} - P _ {t} \ell_ {t + h}\right) ^ {2} = \sigma^ {2} \frac {1 - \gamma_ {1} ^ {2 h}}{1 - \gamma_ {1} ^ {2}}. \tag {7.4.11}
$$

We have seen how to estimate $\gamma _ { 0 } , \gamma _ { 1 }$ and $\sigma ^ { 2 }$ , but unfortunately $\ell _ { t }$ is not observed. In order to forecast $\ell _ { t + h }$ using the observations $\{ Z _ { s } , s \le t \}$ , we can however use the Kalman recursions as described in Section 9.4, Example 9.4.2

# 7.5 Continuous-Time Models

# 7.5.1 Lévy Processes

Continuous-time models for asset prices have a long history, going back to Bachelier (1900) who used Brownian motion to represent the movement of asset prices in the Paris stock exchange. Continuous-time models have since moved to a central place in mathematical finance, largely because of their use in the field of optionpricing, initiated by the Nobel-Prize-winning work of Black, Scholes and Merton, and partly also because of the current availability of high-frequency and irregularly-spaced transaction data which are represented most naturally by continuous-time models.

We earlier defined the daily return on day t of a stock whose closing price is $P _ { t }$ as

$$
Z _ {t} = X _ {t} - X _ {t - 1}, \tag {7.5.1}
$$

wh

$$
X _ {t} = \log P _ {t} \tag {7.5.2}
$$

is the log asset price at the close of day t. If the daily returns were iid this would mean that the process $\{ X _ { t } \}$ is a random walk (Example 1.4.3). This is an over-simplified model for daily asset prices as there is very strong evidence suggesting that the daily returns, although exhibiting little or no autocorrelation, are not independent.

Nevertheless it will be a useful starting point, in the construction of continuoustime models to introduce the continuous-time analogue of a random walk, known as a Lévy process. Like iid noise in discrete time, it is the building block for the construction of a large family of more complex models for financial data.

# Definition 7.5.1

A Lévy process, $\{ L ( t ) , t \in \mathbb { R } \}$ is a process with the following properties:

(i) $L ( 0 ) = 0 .$   
(ii) $L ( t ) - L ( s )$ has the same distribution as $L ( t - s )$ for all $s$ and $t$ such that $s \leq t$   
(iii) If $( s , t )$ and $( u , \nu )$ are disjoint intervals then $L ( t ) - L ( s )$ and $L ( \nu ) - L ( u )$ are independent.   
(v) $\{ L ( t ) \}$ is continuous in probability, i.e. for all $\epsilon > 0$ and for all $t \in \mathbb { R }$

$$
\lim  _ {s \rightarrow t} P (| L (t) - L (s) | > \epsilon) = 0.
$$

The essential properties of Lévy processes are discussed in Appendix D. For thorough accounts of Lévy processes and their properties see the books of Applebaum (2004), Protter (2010) and Sato (1999) and for an extensive account of their applications to finance see Schoutens (2003) and Andersen et al. (2009). For now we restrict attention to two of the most familiar examples of Lévy processes, Brownian motion, whose sample-paths are continuous, and the compound Poisson process, whose sample-paths are constant except for jumps.

# Example 7.5.1 Brownian Motion

This is a Lévy process for which $L ( t ) \sim N ( \mu t , \sigma ^ { 2 } t )$ , $t \geq 0$ , with parameters $\mu \in \mathbb { R }$ and $\sigma > 0$ . The sample-paths are continuous and the characteristic function of $L ( t )$ for $t > 0$ is

$$
E e ^ {i \theta L (t)} = e ^ {t \xi (\theta)}, \quad \theta \in \mathbb {R}, \tag {7.5.3}
$$

where

$$
\xi (\theta) = i \theta \mu - \theta^ {2} \sigma^ {2} / 2.
$$

The defining properties (ii) and (iii) imply that for any finite collection of times $t _ { 1 } ~ <$ $t _ { 2 } < \cdots < t _ { n }$ , the increments $\varDelta _ { i } : = L ( t _ { i + 1 } ) - L ( t _ { i } )$ , $i = 1 , \ldots , n$ , are independent random variables satisfying $\mathbf { \delta } _ { \lambda _ { i } } \sim \mathrm { N } ( \mu ( t _ { i + 1 } - t _ { i } ) , \sigma ^ { 2 } ( t _ { i + 1 } - t _ { i } ) )$ . Brownian motion with $\mu = 0$ and $\sigma = 1$ is known as standard Brownian motion. We shall denote it henceforth as $\{ B ( t ) , \ t \in \mathbb { R } \}$ . A realization of $B ( t ) , 0 \leq t \leq 1 0$ , is shown in Figure 7-5.

# Example 7.5.2 The Poisson Process

The Poisson process $\{ N ( t ) , t \in \mathbb { R } \}$ with intensity or jump-rate $\lambda$ is a Lévy process such that $N ( t )$ , for $t \geq 0$ , has the Poisson distribution with mean λt. Its sample paths are right-continuous functions which are constant except for jumps of size 1, the number of jumps occurring in any time interval of length ℓ having the Poisson distribution with mean $\lambda \ell$ . The characteristic function of $N ( t )$ for $t > 0$ is given by (7.5.3) with

$$
\xi (\theta) = e ^ {i \theta} - 1.
$$

![](images/442ab98b9dd1a8a4a1d20b8df644281306686eac9a9a8b3903d3ef6a5d98f08f.jpg)  
Figure 7-5 A realization of standard Brownian motion $B ( t ) , 0 \leq t \leq 1 0$

![](images/2f40fffc698bab2cc480e25d85163b6af2794f0217e1171649ebe7dd82903d72.jpg)  
Figure 7-6 A realization of a Poisson process $N ( t ) , 0 \leq t \leq 1 0$ , with jump-rate 5 per unit time

A sample-path of a Poisson process with $\lambda = 5$ on the time-interval 0, 10 is shown in Figure 7-6.

# Example 7.5.3 The Compound Poisson Process

The compound Poisson process $\{ X ( t ) , t ~ \in ~ \mathbb { R } \}$ with jump-rate $\lambda$ and jump-size distribution function $F$ is a Lévy process with sample-paths which are constant except for jumps. The jump-times are those of a Poisson process $\{ N ( t ) \}$ with jump-rate $\lambda$ and the sizes of the jumps are independent random variables, independent of the process $\{ N ( t ) \}$ , with a distribution function $F$ assigning probability zero to the value zero. The characteristic function of $L ( t )$ for $t > 0$ is again given by (7.5.3) but now with

$$
\xi (\theta) = i \theta c + \int_ {\mathbb {R}} \left(e ^ {i \theta x} - 1 - i \theta x I _ {(- 1, 1)} (x)\right) \lambda d F (x), \tag {7.5.4}
$$

where $\begin{array} { r } { c } { = \ \lambda \int _ { | x | < 1 } x d F ( x ) } \end{array}$ and $I _ { ( - 1 , 1 ) } ( x ) ~ = ~ 1$ if $| x | \ < \ 1$ and zero otherwise. | | A realization of a compound Poisson process on the interval [0,10] is shown in Figure 7-7

The above examples give some idea of the immense variety in the class of Lévy processes. The Lévy-Itô decomposition implies that every Lévy process $L$ can be expressed as the sum of a Brownian motion and an independent pure-jump process. The marginal distribution of $L ( t )$ can be any distribution from the class of infinitely divisible distributions (which includes the gamma, Gaussian, Student’s t, stable, compound Poisson and many additional well-known distributions). See Appendix D and the references given there for more details.

![](images/4fd54e36fedfddaa5750c6278dcab5153e3d528cc63d847e3ceaedcbec2f76aa.jpg)  
Figure 7-7 A realization of a compound Poisson process X(t), $0 \leq t \leq 1 0$ , with jump-rate 5 per unit time and jump-size distribution normal with mean 0 and variance 1

# 7.5.2 The Geometric Brownian Motion (GBM) Model for Asset Prices

In his pioneering mathematical analysis of stock prices, contained in his doctoral thesis, Théorie de la speculation, Bachelier (1900) introduced a model in which the price of an asset $\{ P ( t ) \}$ is Brownian motion with parameters $\mu$ and $\sigma$ (see Example 7.5.1). Measuring time in units of 1 day, this implies in particular that the daily closing prices, $P ( t ) , t = 0 , 1 , 2 , . . .$ , constitute a random walk with increments $P ( t ) - P ( t - 1 )$ which are independent and normally distributed with mean $\mu$ and variance $\sigma ^ { 2 }$ . The normality of these increments and the fact that $P ( t )$ takes negative values with positive probability clearly limit the value of this model as a realistic approximation to observed daily prices. However, interest in the work of Bachelier and his use of the Brownian motion model to solve problems in mathematical finance led (Samuelson 1965) to develop and apply the more realistic geometric Brownian motion model for asset prices. A fascinating account of Bachelier’s work, including an English translation of his thesis and comments on its place in the history of both probability theory and mathematical finance is contained in the book of Davis and Etheridge (2006). The geometric Brownian motion model is the one for which the celebrated option-pricing formulae of Black, Scholes and Merton were first derived.

In the Brownian motion model the asset price $\{ P ( t ) , t \geq 0 \}$ satisfies the stochastic differential equation,

$$
d P (t) = \mu d t + \sigma d B (t), \tag {7.5.5}
$$

where $\{ B ( t ) \}$ is standard Brownian motion, i.e., Brownian motion with $E B ( t ) = 0$ and $\mathrm { V a r } B ( t ) = t , t \geq 0$ . Equation (7.5.5) is shorthand for the integrated form,

$$
P (t) - P (0) = \mu t + \sigma B (t).
$$

In addition to the obvious flaw that $P ( t )$ will take negative values for some values of $t$ , the increments $P ( t ) - P ( t - 1 )$ are normally distributed, while in practice it is observed that these increments have marginal distributions with heavier tails than the normal distribution. The geometric Brownian motion model addresses both of these shortcomings.

The geometric Brownian motion model for $\{ P ( t ) , \ t \ \geq \ 0 \}$ is defined by the Itô stochastic differential equation,

$$
d P (t) = P (t) [ \mu d t + \sigma d B (t) ], \text {w i t h} P (0) > 0. \tag {7.5.6}
$$

Solution of this equation requires knowledge of Itô calculus, a brief introduction to which is given in Appendix D. A more extensive and very readable account with financial applications can be found in the book of Mikosch (1998). The solution of (7.5.6) satisfies (see Appendix D)

![](images/c02461932aaed96bf1374593a07121c9fe28a8c87bb5f7e1832883f98d35c201.jpg)  
Figure 7-8 A realization of GBM, P( $t ) , 0 \leq t \leq 1 0$ , with $P ( 0 ) = 1$ . $\mu = 0$ and $\sigma = 0 . 0 1$

$$
P (t) = P (0) \exp \left[ \left(\mu - \frac {\sigma^ {2}}{2}\right) t + \sigma B (t) \right], \tag {7.5.7}
$$

from which it follows at once that the log asset price $X ( t ) = \log P ( t )$ satisfies

$$
X (t) = X (0) + \left(\mu - \frac {\sigma^ {2}}{2}\right) t + \sigma B (t), \tag {7.5.8}
$$

or equivalently

$$
d X (t) = \left(\mu - \frac {\sigma^ {2}}{2}\right) d t + \sigma d B (t). \tag {7.5.9}
$$

A realization of the process $P ( t ) , 0 \leq t \leq 1 0$ , with $P ( 0 ) = 1$ , $\mu = 0$ and $\sigma = 0 . 0 1$ is shown in Figure 7-8.

The return for the time interval $( t - \varDelta , t )$ is

$$
Z _ {\Delta} (t) = X (t) - X (t - \Delta) = (\mu - \frac {\sigma^ {2}}{2}) \Delta + \sigma [ B (t) - B (t - \Delta) ]. \qquad (7. 5. 1 0)
$$

For disjoint intervals of length $\varDelta$ the returns are therefore independent normally distributed random variables with mean $( \mu - \sigma ^ { 2 } / 2 ) \varDelta$ and variance $\sigma ^ { 2 } \varDelta$ . The normality of the returns implied by this model is a property which can easily be checked against observed returns. It is found from empirically observed returns that the deviations from normality are substantial for time intervals of the order of a day or less, becoming less apparent as $\varDelta$ increases. This is one of the reasons for developing the more complex models described in later sections.

Remark 1. An asset-price model which overcomes the normality constraint is the socalled Lévy market model (LMM), in which the log asset price $X$ is assumed to be a Lévy process, not necessarily Brownian motion as in the GBM model. For a discussion of such models see Eberlein (2009).

The parameter $\sigma ^ { 2 }$ in the GBM model is called the volatility parameter. It plays a key role in the option pricing analysis of Black and Scholes (1973) and Merton (1973) to be discussed in Section 7.6. Although $\sigma ^ { 2 }$ cannot be determined from discrete observations of a GBM process it can be estimated from closely-spaced discrete observations $X ( i / N ) , i ~ = ~ 1 , \dots , N$ , with large $N$ , as described in the following paragraph.

From (7.5.8) we can write

$$
(\Delta_ {i} X) ^ {2} := [ X (i / N) - X ((i - 1) / N) ] ^ {2} = (c / N + \sigma \Delta_ {i} B) ^ {2}, \tag {7.5.11}
$$

where $\varDelta _ { i } B = B ( i / N ) - B ( ( i - 1 ) / N )$ and $c = \mu - \sigma ^ { 2 } / 2$ . A simple calculation then gives

$$
\mathbf {E} [ (\Delta_ {i} X) ^ {2} ] = \frac {\sigma^ {2}}{N} + \frac {c ^ {2}}{N ^ {2}},
$$

and

$$
\mathrm {V a r} [ (\varDelta_ {i} X) ^ {2} ] = \frac {4 \sigma^ {2} c ^ {2}}{N ^ {3}} + \frac {2 \sigma^ {4}}{N ^ {2}}.
$$

By the independence of the summands, $\textstyle \sum _ { i = 1 } ^ { N } ( \varDelta _ { i } X ) ^ { 2 }$ has mean $\sigma ^ { 2 } + c ^ { 2 } / N$ and variance $2 \sigma ^ { 4 } / N + \bar { 4 \sigma ^ { 2 } } c ^ { 2 } / N ^ { 2 }$ , showing that, as $N \to \infty$ ,

$$
\sum_ {i = 1} ^ {N} \left(\Delta_ {i} X\right) ^ {2} \xrightarrow {\text {m . s .}} \sigma^ {2} = \int_ {0} ^ {1} \sigma^ {2} d t. \tag {7.5.12}
$$

This calculation shows that, for the GBM process, the sum on the left is a consistent estimator of $\sigma ^ { 2 }$ as $N \to \infty$ . The sum (for suitably large $N$ ) is known as the realized volatility for the time interval 0, 1 and the integral on the right is known as the integrated volatility for the same interval. $\sigma ^ { 2 }$ itself is known as the spot volatility. The realized volatility is widely used as an estimator of the integrated volatility and is consistent for a wide class of models in which the spot volatility is not necessarily constant as it is in the GBM model. For a discussion of realized volatility in a more general context see the article of Andersen and Benzoni (2009).

We shall denote the realized volatility, computed for day n $, n = 1 , 2 , 3 , . . . ,$ by $\hat { \sigma } _ { n } ^ { 2 }$ . It is found in practice to vary significantly from 1 day to the next. The sequence $\{ \hat { \sigma } _ { n } ^ { 2 } \}$ of realized volatilities exhibits clustering, i.e., periods of low values interrupted by bursts of large values, and has the appearance of a positively correlated stationary sequence, reinforcing the view that volatility is not constant as in the GBM model and suggesting the need for a model in which volatility is stochastic. Such observations are precisely those which led to the development in discrete time of stochastic volatility, ARCH, and GARCH models, and suggest the need for analogous models with continuous time parameter.

# 7.5.3 A Continuous-Time SV Model

In the discrete-time modeling of asset prices we have seen how both the GARCH and SV models allow for the variation of the volatility with time by modeling $\{ h _ { t } \}$ as a random process. A continuous-time analogue of this idea was introduced by Barndorff-Niesen and Shephard (2001) in their celebrated continuous-time SV model for the log asset price $X ( t )$ [cf. (7.5.9)],

$$
d X (t) = [ m + b h (t) ] d t + \sqrt {h (t)} d B (t), t \geq 0, \text {w i t h} X (0) = 0, \tag {7.5.13}
$$

where $\textbf { \textit { m } } \in \textbf { \textit { R } }$ , $b \in \mathbb { R }$ , $\{ B ( t ) \}$ is standard Brownian motion and $\{ h ( t ) \}$ is a stationary subordinator-driven Ornstein-Uhlenbeck process independent of $\{ B ( t ) \}$ . The connection with discrete-time SV models is clear if we set $m = b = 0$ in (7.5.13) and compare with (7.4.1). Notice also that (7.5.13) has the same form as the GBM equation (7.5.9) except that the constant volatility parameter $\sigma ^ { 2 }$ has been replaced by the random volatility $h ( t )$ .

A subordinator is a Lévy process with non-decreasing sample paths. The simplest example of a subordinator is the Poisson process of Example 7.5.2. If the compound Poisson process in Example 7.5.3 has non-negative jumps, i.e., if the jump-size

distribution function $F$ satisfies $F ( 0 ) = 0$ , then it too is a subordinator. Other examples of subordinators are the gamma process (see Appendix D), whose increments on disjoint intervals have a gamma distribution, and the stable subordinators, whose increments on disjoint intervals are independent non-negative stable random variables.

An Ornstein-Uhlenbeck process driven by the subordinator $L$ satisfies the stochastic differential equation,

$$
d h (t) = \lambda h (t) d t + d L (t), t \in \mathbb {R}, \tag {7.5.14}
$$

where $\lambda ~ < ~ 0$ . If $E L ( 1 ) ^ { r } \ < \ \infty$ for some $r > 0$ this equation has a unique strictly stationary causal solution

$$
h (t) = \int_ {- \infty} ^ {t} e ^ {\lambda (t - u)} d L (u). \tag {7.5.15}
$$

(Causal here means that $h ( t )$ is independent of the increments $\{ L ( u ) - L ( t ) : u > t \}$ for every t.) A crucial feature of (7.5.15) is the non-negativity of $h ( t )$ which follows from the non-decreasing sample-paths of the subordinator $\{ L ( t ) \}$ and the non-negativity of the integrand. Non-negativity is clearly a necessary property if $h ( t )$ is to represent volatility. For a detailed account of Lévy-driven stochastic differential equations and integrals with respect to Lévy processes, see Protter (2010). In the case when $L$ is a subordinator, (7.5.15) has the very simple interpretation as a pathwise integral with respect to the non-decreasing sample-path of $L$ .

Quantities associated with the model (7.5.13) which are of particular interest are the returns over time intervals of length $\varDelta > 0$ , i.e.

$$
Y _ {n} := X (n \Delta) - X ((n - 1) \Delta), n \in \mathbb {N},
$$

and the integrated volatilities,

$$
I _ {n} = \int_ {(n - 1) \Delta} ^ {n \Delta} h (t) d t, n \in \mathbb {N}.
$$

The interval $\varDelta$ is frequently one trading day. The return for the day is an observable quantity and the integrated volatility, although not directly observable, can be estimated from high-frequency within-day observations of $X ( t )$ , as discussed in Section 7.5.2 for the GBM model.

For the model (7.5.13) with any second-order stationary non-negative volatility process $h$ which is independent of $B$ and has the properties,

$$
E h (t) = \xi , \operatorname {V a r} (h (t)) = \omega^ {2}
$$

and

$$
\operatorname {C o v} (h (t), h (t + s)) = \omega^ {2} \rho (s), s \in \mathbb {R},
$$

it can be shown (Problem 7.8) that the stationary sequence $\left\{ { { I } _ { n } } \right\}$ has mean,

$$
E I _ {n} = \xi \Delta . \tag {7.5.16}
$$

and autocovariance function,

$$
\gamma_ {I} (k) = \left\{ \begin{array}{l l} 2 \omega^ {2} r (\Delta), & \text {i f} k = 0, \\ \omega^ {2} \left[ r ((k + 1) \Delta) - 2 r (k \Delta) + r ((k - 1) \Delta) \right], & \text {i f} k \geq 1. \end{array} \right. \tag {7.5.17}
$$

where

$$
r (t) := \int_ {0} ^ {t} \int_ {0} ^ {y} \rho (u) d u d y. \tag {7.5.18}
$$

The stationary sequence of log returns $\left\{ Y _ { n } \right\}$ has mean $m + b \xi \delta$ and autocovariance function,

$$
\gamma_ {Y} (k) = \left\{ \begin{array}{l l} b ^ {2} \gamma_ {I} (0) + \xi \Delta , & \text {i f} k = 0, \\ b ^ {2} \gamma_ {I} (k), & \text {i f} k \geq 1. \end{array} \right. \tag {7.5.19}
$$

If in addition $m = b = 0$ then the log returns $\left\{ Y _ { n } \right\}$ are uncorrelated while the squared sequence $\left\{ Y _ { n } \right\}$ (see Problem 7.11) has mean,

$$
E Y _ {n} ^ {2} = \xi \Delta \tag {7.5.20}
$$

and autocovariance function,

$$
\gamma_ {Y ^ {2}} (k) = \left\{ \begin{array}{l l} \omega^ {2} \left[ 6 r (\Delta) + 2 \Delta^ {2} \xi^ {2} / \omega^ {2} \right], & \text {i f} k = 0, \\ \omega^ {2} \left[ r ((k + 1) \Delta) - 2 r (k \Delta) + r ((k - 1) \Delta) \right], & \text {i f} k \geq 1. \end{array} \right. \tag {7.5.21}
$$

Thus, under these assumptions, the log returns, $Y _ { n }$ , calculated from the model are uncorrelated while the squares, $Y _ { n } ^ { 2 }$ , are correlated, showing that the log returns are uncorrelated but not independent, in keeping with the '“stylized facts” associated with empirically observed log returns.

# Example 7.5.4. The Ornstein-Uhlenbeck SV Model with $m = b = 0$

We can use the results (7.5.16)–(7.5.21) to determine properties of the sequences $\left\{ Y _ { n } \right\}$ , $\{ Y _ { n } ^ { 2 } \}$ and $\left\{ { { I } _ { n } } \right\}$ associated with the Ornstein-Uhlenbeck SV model,

$$
d X (t) = \sqrt {h (t)} d B (t), t \geq 0, \text {w i t h} X (0) = 0, \tag {7.5.22}
$$

where

$$
h (t) = \int_ {- \infty} ^ {t} e ^ {\lambda (t - u)} d L (u), \tag {7.5.23}
$$

$\lambda < 0$ and $E L ( 1 ) ^ { 2 } < \infty$ .

In order to apply (7.5.16)–(7.5.21) we need to determine $\xi \ = \ E h ( t )$ , $\omega ^ { 2 } \ = \quad$ $\mathrm { V a r } ( h ( t ) )$ and the autocorrelation function $\rho$ of $h$ . To this end we rewrite (7.5.23) as

$$
h (t) = \int_ {- \infty} ^ {\infty} g (t - u) d L (u), \tag {7.5.24}
$$

where

$$
g (x) := \left\{ \begin{array}{l l} e ^ {\lambda x}, & \text {i f} x \geq 0, \\ 0, & \text {o t h e r w i s e} \end{array} \right. \tag {7.5.25}
$$

The function $g$ in the representation (7.5.24) is called a kernel function. If $E L ( 1 ) ^ { 2 } <$ $\infty$ , as we shall assume from now on, and if $f$ and $g$ are integrable and square-integrable functions on $\mathbb { R }$ , we have (see Appendix D),

$$
E \int_ {- \infty} ^ {\infty} f (t - u) d L (u) = \mu \int_ {- \infty} ^ {\infty} f (u) d u \tag {7.5.26}
$$

and

$$
\operatorname {C o v} \left(\int_ {- \infty} ^ {\infty} f (t - u) d L (u), \int_ {- \infty} ^ {\infty} g (t - u) d L (u)\right) = \sigma^ {2} \int_ {- \infty} ^ {\infty} f (u) g (u) d u, \tag {7.5.27}
$$

where $\mu = E L ( 1 )$ and $\sigma ^ { 2 } = \mathrm { V a r } ( L ( 1 ) )$ . Taking $g$ as in (7.5.25) and $f ( x ) = g ( s { + } x )$ , $x \in$ $\mathbb { R }$ , we find from these equations that the mean and autocovariance function of the volatility process $\{ h ( t ) \}$ defined by (7.5.23) are given by

$$
\xi = E h (t) = \frac {\mu}{| \lambda |}
$$

and

$$
\operatorname {C o v} (h (t + s), h (t)) = \frac {\sigma^ {2}}{2 | \lambda |} e ^ {\lambda s} = \omega^ {2} \rho (s), \quad s \geq 0,
$$

where $\omega ^ { 2 } = \mathrm { V a r } ( h ( t ) ) = \sigma ^ { 2 } / ( 2 | \lambda | )$ and $\rho ( s ) = e ^ { \lambda s }$ , $s \geq 0$ . Substituting for $\rho$ into (7.5.17) gives

$$
r (t) = \frac {1}{\lambda^ {2}} \left(e ^ {\lambda t} - 1 - \lambda t\right).
$$

We can now substitute for $\xi$ , $\omega ^ { 2 }$ , $\rho$ and $r$ in equations (7.5.16)–(7.5.21) to get the second-order properties of the sequences $\{ Y _ { n } \} , \{ Y _ { n } ^ { 2 } \}$ and $\left\{ { { I } _ { n } } \right\}$ . In particular we find that

$$
\left\{Y _ {n} \right\} \sim \operatorname {W N} (0, | \lambda | ^ {- 1} \mu \Delta),
$$

$$
E Y _ {n} ^ {2} = E I _ {n} = | \lambda | ^ {- 1} \mu \Delta
$$

and

$$
\gamma_ {Y ^ {2}} (k) = \gamma_ {I} (k) = \frac {1}{2} | \lambda | ^ {- 3} \sigma^ {2} e ^ {(k - 1) \lambda \Delta} (1 - e ^ {\lambda \Delta}) ^ {2}, k \geq 1.
$$

The validity of the latter expressions for $k \geq 1$ and not for $k = 0$ indicates that both the squared return sequence $\{ Y _ { n } ^ { 2 } \}$ and the integrated volatility sequence $\left\{ { { I } _ { n } } \right\}$ have the autocovariances of ARMA(1, 1) processes. This demonstrates, for this particular model, the covariance structure of the sequence $\{ Y _ { n } ^ { 2 } \}$ and the consequent dependence of the white-noise returns sequence $\left\{ Y _ { n } \right\}$ .

![](images/9e50fe12739c6c4aa6f93c959322c9a054b06e07891028d6d4840a547e25fab8.jpg)

Remark 2. Since equations (7.5.16)–(7.5.19) (derived by Barndorff-Niesen and Shephard 2001) apply to any second-order stationary non-negative stochastic volatility process, $h$ , independent of $B$ in (7.5.13), they can be used to calculate the second order properties of $\left\{ Y _ { n } \right\}$ and $\left\{ { { I } _ { n } } \right\}$ for more general models than the Ornstein-Uhlenbeck model defined by (7.5.13) and (7.5.15). If $m = b = 0$ the second-order properties of $\{ Y _ { n } ^ { 2 } \}$ can also be calculated using equations (7.5.20) and (7.5.21). In particular we can replace the Ornstein-Uhlenbeck process, $h$ , in Example 7.5.4 by a non-negative CARMA process (see Section 11.5) to allow a more general class of autocovariance functions for the sequences $\left\{ { { I } _ { n } } \right\}$ and $\{ Y _ { n } ^ { 2 } \}$ in order to better represent empirically observed financial data.

Remark 3. Continuous-time generalizations of the GARCH process have also been developed (see Klüppelberg et al. (2004) and Brockwell et al. 2006). Details however are beyond the scope of this book.

# 7.6 An Introduction to Option Pricing

We saw in Section 7.5.2 that, under the geometric Brownian motion model, the asset price $P ( t )$ satisfies the Itô equation,

$$
\mathrm {d} P (t) = P (t) \left[ \mu \mathrm {d} t + \sigma \mathrm {d} B (t) \right] \text {w i t h} P (0) > 0, \tag {7.6.1}
$$

which leads to the relation,

$$
P (t) = P (0) \exp \left[ \left(\mu - \sigma^ {2} / 2\right) t + \sigma B (t) \right]. \tag {7.6.2}
$$

In this section we shall determine the value of a European call option on an asset whose price satisfies (7.6.2). The result, derived by Black and Scholes (1973) and Merton (1973), clearly demonstrates the key role played by the volatility parameter $\sigma ^ { 2 }$ .

A European call option, if sold at time 0, gives the buyer the right, but not the obligation, to buy one unit of the stock at the strike time $T$ for the strike price $K$ . At time $T$ the option has the cash value $h ( P ( t ) ) = \operatorname* { m a x } ( P ( T ) - K , 0 )$ since the option will be exercised only if $P ( T ) > K$ , in which case the holder of the option can buy the stock at the price $K$ and resell it instantly for $P ( T )$ . However it is not clear at time 0, since $P ( T )$ is random, what price the buyer should pay for this privilege. Assuming

(i) the existence of a risk-free asset with price process,

$$
D (t) = D (0) \exp (r t), r > 0, \tag {7.6.3}
$$

(ii) the ability to buy and sell arbitrary (positive or negative) amounts of the stock and the risk-free asset continuously with no transaction costs, and   
(iii) an arbitrage-free market ( i.e., a market in which it is impossible to make a profit which is non-negative with probability one and strictly positive with probability greater than zero).

Black, Scholes and Merton showed that there is a unique value for the option in the sense that both higher and lower prices introduce demonstrable arbitrage opportunities. Details of the derivation can be found in most books dealing with mathematical finance (e.g., Campbell et al. 1996; Mikosch 1998; Klebaner 2005). In the following paragraphs we give a sketch of two arguments, following Mikosch (1998), which determine this value under the assumption that the asset price follows the GBM model.

In the first argument, we attempt to construct a self-financing portfolio, consisting at time $t$ of $a _ { t }$ shares of the stock and $b _ { t }$ shares of the risk-free asset, where $a _ { t }$ and $b _ { t }$ are random variables which, for each t are functions of $\{ B ( s ) , s \le t \}$ . We require the value of this portfolio at time t, namely

$$
V (t) = a _ {t} P (t) + b _ {t} D (t), \tag {7.6.4}
$$

to satisfy the self-financing condition,

$$
\mathrm {d} V (t) = a _ {t} \mathrm {d} P (t) + b _ {t} \mathrm {d} D (t), \tag {7.6.5}
$$

and to match the value of the option at time $T$ , i.e.,

$$
V (T) = h (P (T)) = \max  (P (T) - K, 0). \tag {7.6.6}
$$

If such an investment strategy, $\{ ( a _ { t } , b _ { t } ) , 0 \leq t \leq T \}$ can be found, then $V ( 0 )$ must be the value of the option at the purchase time $t = 0$ . A higher price for the option would allow the seller to pocket the difference $\delta$ and invest the amount $V ( 0 )$ in such

a way as to match the value of the option at time $T$ . Then at time $T$ , if $P ( T ) < K$ the option will not be exercised and the portfolio and the option will both have value zero. If $P ( T ) > K$ the seller sells the portfolio for $P ( T ) - K$ , then buys one stock for $P ( T )$ and receives $K$ for it from the holder of the option. Since there is no loss involved in this transaction, the seller is left with a net profit of δ. The seller of the option therefore makes a profit which is certainly non-negative and strictly positive with nonzero probability, in violation of the no arbitrage assumption. Similarly a lower price than $V ( 0 )$ would create an arbitrage opportunity for the buyer. In order to determine $V ( t )$ , $a _ { t }$ and $b _ { t }$ we look for a smooth function $\nu ( t , x )$ , $t \in [ 0 , T ]$ , $x > 0$ , such that

$$
V (t) = v (t, P (t)), t \in [ 0, T ], \tag {7.6.7}
$$

satisfies the conditions (7.6.4)–(7.6.6).

Writing $x$ for $P ( t )$ in $\nu ( t , P ( t ) )$ and applying Itô’s formula (see Appendix D) gives

$$
d v = \frac {\partial v}{\partial t} d t + \frac {\partial v}{\partial x} d x + \frac {1}{2} \frac {\partial^ {2} v}{\partial x ^ {2}} (d x) ^ {2} \tag {7.6.8}
$$

where, from (7.6.1),

$$
d x = x (\mu d t + \sigma d B (t)) \tag {7.6.9}
$$

and

$$
(d x) ^ {2} = x ^ {2} \sigma^ {2} d t. \tag {7.6.10}
$$

Applying Itô’s formula to (7.6.5) and using (7.6.3) and (7.6.4) gives

$$
d v = a _ {t} (\mu d t + \sigma d B (t)) + r (v - a _ {t} x) d t. \tag {7.6.11}
$$

Substituting (7.6.9) and (7.6.10) into (7.6.8) and comparing with (7.6.11), we find that

$$
a _ {t} = \frac {\partial v}{\partial x} (t, P (t)) \tag {7.6.12}
$$

and that $\nu ( t , x )$ satisfies the equation,

$$
\frac {\partial v}{\partial t} + \frac {1}{2} \sigma^ {2} x ^ {2} \frac {\partial^ {2} v}{\partial x ^ {2}} + r x \frac {\partial v}{\partial x} = r v. \tag {7.6.13}
$$

The condition (7.6.6) yields the boundary condition,

$$
v (T, x) = h (x) = \max  (x - K, 0), \tag {7.6.14}
$$

which, with (7.6.13), uniquely determines the function $\nu$ and hence $V ( t )$ , $a _ { t }$ and $b _ { t } = ( V ( t ) - a _ { t } P ( t ) ) / D ( t )$ for each $t \in [ 0 , T ]$ . The corresponding investment strategy $\{ ( a _ { t } , b _ { t } ) , 0 \leq t \leq T \}$ satisfies (7.6.5) and (7.6.6) and can, under the assumed idealized trading conditions, be implemented in practice. Since at time $T$ this portfolio has the same value as the option, $V ( 0 )$ must be the fair value of the option at time $t = 0$ , otherwise an arbitrage opportunity would arise. The option is said to be hedged by the investment strategy $\{ ( a _ { t } , b _ { t } ) \}$ . A key feature of this solution [apparent from (7.6.12)– (7.6.14)] is that both the strategy and the fair price of the option are independent of $\mu$ , depending on the price process $P$ only through the volatility parameter $\sigma ^ { 2 }$ .

Instead of attempting to solve (7.6.13) directly we now outline the martingale argument which leads to the explicit solution for $\nu ( x , t )$ , $a _ { t }$ and $b _ { t }$ . It is based on the fact that for the GBM model with $B ( t )$ defined on the probability space $( \Omega , { \mathcal { F } } , \Pi )$ , there is a unique probability measure $Q$ on $( \Omega , { \mathcal { F } } )$ which is equivalent to  (i.e., it has the same null sets) and which, when substituted for $\Pi$ , causes the discounted price process

$\tilde { P } ( t ) : = e ^ { - r t } P ( t )$ , $0 \leq t \leq T$ , to be a $B$ -martingale, i.e., to satisfy the conditions that $E _ { Q } \tilde { P } ( t ) < \infty$ and

$$
E _ {Q} (\tilde {P} (t) | B (u), u \leq s) = \tilde {P} (s) \text {f o r a l l} 0 \leq s \leq t \leq T. \tag {7.6.15}
$$

The measure $Q$ and the relation (7.6.15) can be derived as follows. Applying Itô’s formula to the expression $\tilde { P } ( t ) = e ^ { - r t } P ( t )$ and using (7.6.1) gives

$$
\frac {\mathrm {d} \tilde {P} (t)}{\tilde {P} (t)} = (\mu - r) \mathrm {d} t + \sigma \mathrm {d} B (t) = \sigma \mathrm {d} \tilde {B} (t), \tag {7.6.16}
$$

where $\tilde { B } ( t ) : = ( \mu - r ) t / \sigma + B ( t )$ . The solution of (7.6.16) satisfies

$$
\tilde {P} (t) = \tilde {P} (0) e ^ {\sigma \tilde {B} (t) - \sigma^ {2} t / 2}. \tag {7.6.17}
$$

By Girsanov’s theorem (see Mikosch 1998), if we define $Q$ by

$$
Q (A) = \int_ {A} \exp \left(- \frac {\mu - r}{\sigma} B (T) - \frac {(\mu - r) ^ {2}}{2 \sigma^ {2}} T\right) d \Pi , \tag {7.6.18}
$$

then, on the new probability space $( \Omega , { \mathcal { F } } , Q ) ,$ B˜ is standard Brownian motion. A simple calculation using (7.6.17) then shows that the discounted price process $\tilde { P }$ is a $B$ -martingale on $( \Omega , { \mathcal { F } } , Q )$ , i.e. $E _ { Q } \tilde { P } ( t ) < \infty$ and (7.6.15) holds.

Assuming the existence of a portfolio (7.6.4) which satisfies the self-financing condition (7.6.5) and the boundary condition (7.6.6), the discounted portfolio value is

$$
\tilde {V} (t) = e ^ {- r t} V (t). \tag {7.6.19}
$$

Applying Itô’s formula to this expression we obtain

$$
\mathrm {d} \tilde {V} (t) = e ^ {- r t} (- r V (t) d t + \mathrm {d} V (t)) = a _ {t} e ^ {- r t} (- r P (t) \mathrm {d} t + \mathrm {d} P (t)) = a _ {t} \mathrm {d} \tilde {P} (t),
$$

and hence, from (7.6.16),

$$
\tilde {V} (t) = \tilde {V} (0) + \int_ {0} ^ {t} a _ {s} \mathrm {d} \tilde {P} (s) = V (0) + \sigma \int_ {0} ^ {t} a _ {s} \tilde {P} (s) \mathrm {d} \tilde {B} (s). \tag {7.6.20}
$$

Since $a _ { t } \tilde { P } ( t )$ is a function of $\{ B ( s ) , s \ \leq \ t \}$ for each $t \in [ 0 , T ]$ and since, under the probability measure $Q , \tilde { B }$ is Brownian motion and $\tilde { B } ( t )$ is a function of $\{ B ( s ) , s \le t \}$ for each $t \in [ 0 , T ]$ , we conclude that $\tilde { V }$ is a $B$ -martingale. Hence

$$
\tilde {V} (t) = E _ {Q} [ \tilde {V} (T) | B (s), s \leq t ], t \in [ 0, T ],
$$

and

$$
V (t) = e ^ {r t} \tilde {V} (t) = E _ {Q} \left[ e ^ {- r (T - t)} h (P (T)) | B (s), s \leq t \right], \tag {7.6.21}
$$

where $h ( P ( T ) )$ is the value of the option at time $T$ . For the European call option $h ( P ( T ) ) = \operatorname* { m a x } ( P ( T ) - K , 0 )$ .

It only remains to calculate $\nu ( t , x )$ from (7.6.21). To do this we define $\theta : = T - t$ . Then, expressing $P ( T )$ in terms of $P ( t )$ ,

$$
V (t) = E _ {Q} [ e ^ {- r \theta} h (P (t) e ^ {(r - \frac {\sigma^ {2}}{2}) \theta + \sigma (\tilde {B} (T) - \tilde {B} (t))}) | B (s), s \leq t ] = v (t, P (t)),
$$

where

$$
v (t, x) = e ^ {- r \theta} \int h \left(x e ^ {\left(r - \frac {\sigma^ {2}}{2}\right) \theta + \sigma y \theta^ {1 / 2}}\right) \phi (y) d y \tag {7.6.22}
$$

and $\phi$ is the standard normal density function,

$$
\phi (y) = \frac {1}{\sqrt {2 \pi}} \exp (- y ^ {2} / 2).
$$

Substituting $\operatorname* { m a x } ( x - K , 0 )$ for $h ( x )$ in (7.6.22) gives

$$
v (t, x) = x \Phi (z _ {1}) - K e ^ {- r (T - t)} \Phi (z _ {2}), \tag {7.6.23}
$$

where $\Phi$ is the standard normal cumulative distribution function, $\begin{array} { r } { \Phi ( x ) = \int _ { - \infty } ^ { x } \phi ( u ) \mathrm { d } u } \end{array}$ ,

$$
z _ {1} = \frac {\log (x / K) + (r + \sigma^ {2} / 2) (T - t)}{\sigma \sqrt {T - t}} \mathrm {a n d} z _ {2} = z _ {1} - \sigma \sqrt {T - t}.
$$

The value of the option at time 0 is $V ( 0 ) = \nu ( 0 , P ( 0 ) )$ and the investment strategy $\{ a _ { t } , b _ { t } , 0 \leq t \leq T \}$ = required to hedge it is determined by the relations $\begin{array} { r } { a _ { t } = \frac { \partial \nu } { \partial x } ( t , P ( t ) ) } \end{array}$ and $b _ { t } = ( \nu ( t , P ( t ) - a _ { t } P ( t ) ) / D ( t )$ . It can be verified by direct substitution (Problem 7.12) that the function $\nu$ given by (7.6.23) satisfies the partial differential equation (7.6.13) and the boundary condition (7.6.14).

The quantity $m = ( \mu - r ) / \sigma$ which appears in the integrand in (7.6.18) is called the market price of risk and represents the excess, in units of $\sigma$ , of the instantaneous rate of return $\mu$ of the risky asset $S$ over that of the risk-free asset $D$ . If $m = 0$ then $Q = \Pi$ and the model is said to be risk-neutral.

Although the model (7.6.1) has many shortcomings as a representation of asset prices, the remarkable achievement of Black, Scholes and Merton in using it to derive a unique arbitrage-free option price has inspired enormous interest and progress in the field of financial mathematics. As a result of their pioneering work, research in continuous-time financial models has blossomed, with much of it directed at the construction, estimation and analysis of more realistic continuous-time models for the evolution of stock prices, and the pricing of options based on such models. A nice account of option-pricing for a broad class of Lévy-driven stock-price models can be found in the book of Schoutens (2003).

# Problems

7.1 Evaluate $E Z _ { t } ^ { 4 }$ for the ARCH(1) process (7.2.5) with $0 \ < \ \alpha _ { 1 } \ < \ 1$ and $\{ e _ { t } \} \sim$ IID $\mathrm { N } ( 0 , 1 )$ . Deduce that $E X _ { t } ^ { 4 } < \infty$ if and only if $3 \alpha _ { 1 } ^ { 2 } < 1$ .   
7.2 Let $\{ Z _ { t } \}$ be a causal stationary solution of the $\operatorname { A R C H } ( p )$ equations (7.2.1) and (7.2.2) with $E Z _ { t } ^ { 4 } \ < \ \infty$ . Assuming that such a process exists, show that $Y _ { t } ~ =$ $Z _ { t } ^ { 2 } / \alpha _ { 0 }$ satisfies the equations

$$
Y _ {t} = e _ {t} ^ {2} \left(1 + \sum_ {i = 1} ^ {p} \alpha_ {i} Y _ {t - i}\right)
$$

and deduce that $\{ Y _ { t } \}$ has the same autocorrelation function as the $\operatorname { A R } ( p )$ process

$$
W _ {t} = \sum_ {i = 1} ^ {p} \alpha_ {i} W _ {t - i} + e _ {t}, \{e _ {t} \} \sim \mathrm {W N} (0, 1).
$$

(In the case $p = 1$ , a necessary and sufficient condition for existence of a causal stationary solution of (7.2.1) and (7.2.2) with $E Z _ { t } ^ { 4 } < \infty$ is $3 \alpha _ { 1 } ^ { 2 } < 1$ , as shown by the results of Section 7.2 and Problem 7.1.)

7.3 Suppose that $\{ Z _ { t } \}$ is a causal stationary $\mathrm { G A R C H } ( p , q )$ process $Z _ { t } = \sqrt { h _ { t } } e _ { t }$ , where $\{ e _ { t } \} \sim \mathrm { I I D } ( 0 , 1 )$ , $\textstyle \sum _ { i = 1 } ^ { p } a _ { i } + \sum _ { j = 1 } ^ { q } B _ { j } < 1$ and

$$
h _ {t} = \alpha_ {0} + \alpha_ {1} Z _ {t - 1} ^ {2} + \dots + \alpha_ {p} Z _ {t - p} ^ {2} + \beta_ {1} h _ {t - 1} + \dots + \beta_ {q} h _ {t - q}.
$$

a. Show that $E ( Z _ { t } ^ { 2 } | Z _ { t - 1 } ^ { 2 } , Z _ { t - 2 } ^ { 2 } , . ~ . ~ . ) = h _ { t }$   
b. Show that the squared process $\{ Z _ { t } ^ { 2 } \}$ is an ARMA(m, q) process satisfying the equations

$$
\begin{array}{l} Z _ {t} ^ {2} = \alpha_ {0} + (\alpha_ {1} + \beta_ {1}) Z _ {t - 1} ^ {2} + \dots + (\alpha_ {m} + \beta_ {m}) Z _ {t - m} ^ {2} \\ + U _ {t} - \beta_ {1} U _ {t - 1} - \dots - \beta_ {q} U _ {t - q}, \\ \end{array}
$$

where $m = \operatorname* { m a x } \{ p , q \}$ , $\alpha _ { j } = 0$ for $j > p$ , $\beta _ { j } = 0$ for $j > q$ , and $U _ { t } = Z _ { t } ^ { 2 } - h _ { t }$ is white noise if $E Z _ { t } ^ { 4 } < \infty$ .

c. For $p \geq 1$ , show that the conditional variance process $\{ h _ { t } \}$ is an ARMA $( m , p - 1 )$ process satisfying the equations

$$
\begin{array}{l} h _ {t} = \alpha_ {0} + (\alpha_ {1} + \beta_ {1}) h _ {t - 1} + \dots + (\alpha_ {m} + \beta_ {m}) h _ {t - m} \\ + V _ {t} + \alpha_ {1} ^ {*} V _ {t - 1} + \dots + \alpha_ {p} ^ {*} V _ {t - p - 1}, \\ \end{array}
$$

where $V _ { t } = \alpha _ { 1 } ^ { - 1 } U _ { t - 1 }$ and $\alpha _ { j } ^ { * } = \alpha _ { j + 1 } / \alpha _ { 1 } \ : \mathrm { f o r } \ : j = 1 , \ldots , p - 1 .$

7.4 To each of the seven components of the multivariate time series filed as STOCK7.TSM, fit an ARMA model driven by GARCH noise. Compare the fitted models for the various series and comment on the differences. (For exporting components of a multivariate time series to a univariate project, see the topic Getting started in the PDF file ITSM_HELP which is included in the ITSM software package.   
7.5 Verify equation (7.3.7).   
7.6 Show that the return, $Z _ { \Delta } ( t ) ~ : = ~ \log P ( t ) - \log P ( t - \Delta )$ , approximates the fractional gain, $F _ { \varDelta } ( t ) : = ( P ( t ) - P ( t - \varDelta ) ) / P ( t - \varDelta )$ , in the sense that

$$
\frac {Z _ {\Delta} (t)}{F _ {\Delta} (t)} \rightarrow 1 \text {a s} F _ {\Delta} (t) \rightarrow 0.
$$

7.7 For the GBM model (7.5.7) with $P ( 0 ) = 1$ , evaluate the mean and variance of $P ( t )$ and the mean and variance of the return, $Z _ { \varDelta } ( t )$ .   
7.8 If $h$ is any second-order stationary non-negative volatility process with mean $\xi$ , variance $\omega ^ { 2 }$ and autocorrelation function $\rho$ , verify the relations (7.5.16)–(7.5.18).   
7.9 Use (7.5.26) and (7.5.27) to evaluate the mean and autocovariance function of the stationary Ornstein-Uhlenbeck process (7.5.23).   
7.10 If $h$ is the stationary Ornstein-Uhlenbeck process (7.5.23) and $s$ is any fixed value in $[ 0 , \varDelta ]$ , show that application of the operator $\phi ( B ) : = ( 1 - e ^ { \lambda A } B )$ to the sequence $\{ h ( n \Delta + s ) , n \in \mathbb { Z } \}$ gives

$$
\phi (B) h (n \Delta + s) = W _ {n} (s),
$$

where $\{ W _ { n } ( s ) , n \in \mathbb { Z } \}$ is the iid sequence,

$$
W _ {n} (s) = \int_ {(n - 1) \Delta + s} ^ {n \Delta + s} e ^ {\lambda (n \Delta + s - u)} d L (u).
$$

Deduce that the integrated volatility sequence, $\begin{array} { r } { I _ { n } = \int _ { - \varDelta } ^ { 0 } h ( n \varDelta + s ) d s } \end{array}$ , satisfies

$$
(1 - e ^ {\lambda \Delta} B) I _ {n} = \int_ {- \Delta} ^ {0} W _ {n} (s) d s.
$$

Since the right-hand side is 1-correlated, it follows from Proposition 2.1.1 that it is an MA(1) process and hence that the integrated volatility sequence is an ARMA(1,1) process.

7.11 For the stochastic volatility model (7.5.13) with $m = b = 0$ and second-order stationary volatility process $h$ independent of W, establish (7.5.20) and (7.5.21).   
7.12 Verify that the expression (7.6.23) for $\nu ( t , s )$ satisfies (7.6.13) and (7.6.14) and use it to write down the value of the option at time $t = 0$ and the corresponding investment strategy $\{ ( a _ { t } , b _ { t } ) , 0 \leq t \leq T \}$ .

# Multivariate Time Series

8.1 Examples   
8.2 Second-Order Properties of Multivariate Time Series   
8.3 Estimation of the Mean and Covariance Function   
8.4 Multivariate ARMA Processes   
8.5 Best Linear Predictors of Second-Order Random Vectors   
8.6 Modeling and Forecasting with Multivariate AR Processes   
8.7 Cointegration

Many time series arising in practice are best considered as components of some vectorvalued (multivariate) time series $\{ { \mathbf X } _ { t } \}$ having not only serial dependence within each component series $\{ X _ { t i } \}$ but also interdependence between the different component series $\{ X _ { t i } \}$ and $\{ X _ { t j } \}$ , $i \neq j$ . Much of the theory of univariate time series extends in a natural way to the multivariate case; however, new problems arise. In this chapter we introduce the basic properties of multivariate series and consider the multivariate extensions of some of the techniques developed earlier. In Section 8.1 we introduce two sets of bivariate time series data for which we develop multivariate models later in the chapter. In Section 8.2 we discuss the basic properties of stationary multivariate time series, namely, the mean vector ${ \pmb { \mu } } = E { \pmb { \mathrm { X } } } _ { t }$ and the covariance matrices $\Gamma ( h ) =$ $E ( \mathbf { X } _ { t + h } \mathbf { X } _ { t } ^ { \prime } ) \ - \ \pmb { \mu } \pmb { \mu } ^ { \prime } , h \ = \ 0 , \pm 1 , \pm 2 , . . .$ , with reference to some simple examples, including multivariate white noise. Section 8.3 deals with estimation of $\pmb { \mu }$ and $\Gamma ( \cdot )$ and the question of testing for serial independence on the basis of observations of $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ . In Section 8.4 we introduce multivariate ARMA processes and illustrate the problem of multivariate model identification with an example of a multivariate AR(1) process that also has an MA(1) representation. (Such examples do not exist in the univariate case.) The identification problem can be avoided by confining attention to multivariate autoregressive (or VAR) models. Forecasting multivariate time series with known second-order properties is discussed in Section 8.5, and in Section 8.6 we consider the modeling and forecasting of multivariate time series using the multivariate Yule–Walker equations and Whittle’s generalization of the

Durbin–Levinson algorithm. Section 8.7 contains a brief introduction to the notion of cointegrated time series.

# 8.1 Examples

In this section we introduce two examples of bivariate time series. A bivariate time series is a series of two-dimensional vectors $( X _ { t 1 } , \ X _ { t 2 } ) ^ { \prime }$ observed at times $t$ (usually $t = 1 , 2 , 3 , \ldots )$ . The two component series $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ could be studied independently as univariate time series, each characterized, from a second-order point of view, by its own mean and autocovariance function. Such an approach, however, fails to take into account possible dependence between the two component series, and such crossdependence may be of great importance, for example in predicting future values of the two component series.

We therefore consider the series of random vectors $\mathbf { X } _ { t } = ( X _ { t 1 } , \ X _ { t 2 } ) ^ { \prime }$ and define the mean vector

$$
\boldsymbol {\mu} _ {t} := E \mathbf {X} _ {t} = \left[ \begin{array}{c} E X _ {t 1} \\ E X _ {t 2} \end{array} \right]
$$

and covariance matrices

$$
\Gamma (t + h, t) := \operatorname {C o v} \bigl (\mathbf {X} _ {t + h}, \mathbf {X} _ {t} \bigr) = \left[ \begin{array}{c c} \operatorname {c o v} (X _ {t + h, 1}, X _ {t 1}) & \operatorname {c o v} (X _ {t + h, 1}, X _ {t 2}) \\ \operatorname {c o v} (X _ {t + h, 2}, X _ {t 1}) & \operatorname {c o v} (X _ {t + h, 2}, X _ {t 2}) \end{array} \right].
$$

The bivariate series $\left\{ { \bf X } _ { t } \right\}$ is said to be (weakly) stationary if the moments ${ \pmb { \mu } } _ { t }$ and $\Gamma ( t + h , t )$ are both independent of $t$ , in which case we use the notation

$$
\boldsymbol {\mu} = E \mathbf {X} _ {t} = \left[ \begin{array}{c} E X _ {t 1} \\ E X _ {t 2} \end{array} \right]
$$

and

$$
\Gamma (h) = \operatorname {C o v} \bigl (\mathbf {X} _ {t + h}, \mathbf {X} _ {t} \bigr) = \left[ \begin{array}{c c} \gamma_ {1 1} (h) & \gamma_ {1 2} (h) \\ \gamma_ {2 1} (h) & \gamma_ {2 2} (h) \end{array} \right].
$$

The diagonal elements are the autocovariance functions of the univariate series $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ as defined in Chapter 2, while the off-diagonal elements are the covariances between $X _ { t + h , i }$ and $X _ { t j }$ , $i \neq j$ . Notice that $\gamma _ { 1 2 } ( h ) = \gamma _ { 2 1 } ( - h )$ .

A natural estimator of the mean vector $\pmb { \mu }$ in terms of the observations $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ is the vector of sample means

$$
\overline {{\mathbf {X}}} _ {n} = \frac {1}{n} \sum_ {t = 1} ^ {n} \mathbf {X} _ {t},
$$

and a natural estimator of $\Gamma ( h )$ is

$$
\hat {\Gamma} (h) = \left\{ \begin{array}{l l} n ^ {- 1} \sum_ {t = 1} ^ {n - h} \left(\mathbf {X} _ {t + h} - \overline {{\mathbf {X}}} _ {n}\right) \left(\mathbf {X} _ {t} - \overline {{\mathbf {X}}} _ {n}\right) ^ {\prime} & \text {f o r} 0 \leq h \leq n - 1, \\ \hat {\Gamma} (- h) ^ {\prime} & \text {f o r} - n + 1 \leq h <   0. \end{array} \right.
$$

The correlation $\rho _ { i j } ( h )$ between $X _ { t + h , i }$ and $X _ { t , j }$ is estimated by

$$
\hat {\rho} _ {i j} (h) = \hat {\gamma} _ {i j} (h) (\hat {\gamma} _ {i i} (0) \hat {\gamma} _ {j j} (0)) ^ {- 1 / 2}.
$$

If $i \ : = \ : j$ , then $\hat { \rho } _ { i j }$ reduces to the sample autocorrelation function of the ith series. These estimators will be discussed in more detail in Section 8.2.

![](images/e21abd98a53391a5c6fa062ad45cde49b47238c3a58c415dd6cb152f57735247.jpg)  
Figure 8-1 The Dow Jones Index (top) and Australian All Ordinaries Index (bottom) at closing on 251 trading days ending August 26th, 1994

# Example 8.1.1 Dow Jones and All Ordinaries Indices; DJAO2.TSM

Figure 8-1 shows the closing values $D _ { 0 } , \ldots , D _ { 2 5 0 }$ of the Dow Jones Index of stocks on the New York Stock Exchange and the closing values $A _ { 0 } , \ldots , A _ { 2 5 0 }$ of the Australian All Ordinaries Index of Share Prices, recorded at the termination of trading on 251 successive trading days up to August 26th, 1994. (Because of the time difference between Sydney and New York, the markets do not close simultaneously in both places; however, in Sydney the closing price of the Dow Jones index for the previous day is known before the opening of the market on any trading day.) The efficient market hypothesis suggests that these processes should resemble random walks with uncorrelated increments. In order to model the data as a stationary bivariate time series we first reexpress them as percentage relative price changes or percentage returns (filed as DJAOPC2.TSM)

$$
X _ {t 1} = 1 0 0 \frac {\left(D _ {t} - D _ {t - 1}\right)}{D _ {t - 1}}, \quad t = 1, \dots , 2 5 0,
$$

and

$$
X _ {t 2} = 1 0 0 \frac {\left(A _ {t} - A _ {t - 1}\right)}{A _ {t - 1}}, \quad t = 1, \dots , 2 5 0.
$$

The estimators ${ \hat { \rho } } _ { 1 1 } ( h )$ and $\hat { \rho } _ { 2 2 } ( h )$ of the autocorrelations of the two univariate series are shown in Figures 8-2 and 8-3. They are not significantly different from zero.

To compute the sample cross-correlations ${ \hat { \rho } } _ { 1 2 } ( h )$ and $\hat { \rho } _ { 2 1 } ( h )$ using ITSM, select File>Project>Open>Multivariate. Then click OK and double-click on the file name DJAOPC2.TSM. You will see a dialog box in which Number of columns should be set to 2 (the number of components of the observation vectors). Then click OK, and the graphs of the two component series will appear. To see the correlations, press the middle yellow button at the top of the ITSM window. The correlation functions are plotted as a $2 \times 2$ array of graphs with ${ \hat { \rho } } _ { 1 1 } ( h )$ , ${ \hat { \rho } } _ { 1 2 } ( h )$ in the top row and $\hat { \rho } _ { 2 1 } ( h )$ , $\hat { \rho } _ { 2 2 } ( h )$ in the second row. We see from these graphs (shown in Figure 8-4) that although the autocorrelations $\hat { \rho } _ { i i } ( h ) , i = 1 , 2$ , are all small, there is a much larger correlation between $X _ { t - 1 , 1 }$ and $X _ { t , 2 }$ . This indicates the importance of considering the two series jointly as components of a bivariate time series. It also suggests that the value of $X _ { t - 1 , 1 }$ , i.e., the Dow Jones return on day $t - 1$ , may be of assistance in predicting the value of $X _ { t , 2 }$ , the All Ordinaries return on day t. This last

![](images/6c1d499f61dc2ca0fac54a5f144d06d07720ee01f87e8d1b2e509204a965dd24.jpg)  
Figure 8-2 The sample ACF $\bar { \rho } _ { 1 1 }$ of the observed values of $\{ X _ { t 1 } \}$ i n Example 8.1.1, showing the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 }$

![](images/63700d5795e784547ebf69a298506b3a24cd1c655a9c4b0f801ac958e9dad2d4.jpg)  
Figure 8-3 The sample ACF $\bar { \rho } _ { 2 2 }$ of the observed values of $\{ X _ { t 2 } \}$ in Example 8.1.1, showing the bounds $\pm 1 . 9 6 n ^ { - 1 / \bigstar }$

observation is supported by the scatterplot of the points $( x _ { t - 1 , 1 } , x _ { t , 2 } )$ , $t = 2 , \ldots , 2 5 0$ , shown in Figure 8-5.

# Example 8.1.2 Sales with a leading indicator; LS2.TSM

In this example we consider the sales data $\{ Y _ { t 2 } , t = 1 , \ldots , 1 5 0 \}$ with leading indicator $\{ Y _ { t 1 } , t = 1 , \ldots , 1 5 0 \}$ given by Box and Jenkins (1976, p. 537). The two series are stored in the ITSM data files SALES.TSM and LEAD.TSM, respectively, and in bivariate format as LS2.TSM. The graphs of the two series and their sample autocorrelation functions strongly suggest that both series are nonstationary. Application of the operator $( 1 - B )$ yields the two differenced series $\{ D _ { t 1 } \}$ and $\{ D _ { t 2 } \}$ , whose properties are compatible with those of low-order ARMA processes. Using ITSM, we find that the models

![](images/dbf2f3be36fbb237b79f71cc6a003a782fcc8bfb79086291978ab24925e9decc.jpg)

Figure 8-4   
![](images/5e0f22736bb2ab7177d5a103b31a74c523fe2adbd888e431adc818e54e4db561.jpg)  
The sample correlations $\hat { \rho } _ { i j } ( h )$ between $X _ { t + h , i }$ and $X _ { t , j }$ for Example 8.1.1. $( \rho _ { i j } ( h )$ i s plotted as the jth graph in the ith row, $i , j = 1 , 2$ . Series 1 and 2 consist of the daily Dow Jones and All Ordinaries percentage returns, respectively.)

![](images/1086d7e5f2d865691458adbbb1b9b562b45c3c8d1c64fda28f654536e6083f8e.jpg)

Figure 8-5   
![](images/e3dab0be7189c8f21ae27d81678a302a15078f8a7f745e724f46a0579f26d903.jpg)  
Scatterplot of $( x _ { t - 1 , 1 } , x _ { t , 2 } )$ , $t = 2 , \ldots , 2 5 0$ , for the data in Example 8.1.1

![](images/1bc30878c56cdee47068c22a8a7ba75315467f000d17eb7725982e14c95e0268.jpg)

$$
D _ {t 1} - 0. 0 2 2 8 = Z _ {t 1} - 0. 4 7 4 Z _ {t - 1, 1}, \quad \left\{Z _ {t 1} \right\} \sim \mathrm {W N} (0, 0. 0 7 7 9), \tag {8.1.1}
$$

$$
D _ {t 2} - 0. 8 3 8 D _ {t - 1, 2} - 0. 0 6 7 6 = Z _ {t 2} - 0. 6 1 0 Z _ {t - 1, 2},
$$

$$
\left\{Z _ {t 2} \right\} \sim \mathrm {W N} (0, 1. 7 5 4), \tag {8.1.2}
$$

provide good fits to the series $\{ D _ { t 1 } \}$ and $\{ D _ { t 2 } \}$

The sample autocorrelations and cross-correlations of $\{ D _ { t 1 } \}$ and $\{ D _ { t 2 } \}$ , are computed by opening the bivariate ITSM file LS2.TSM (as described in Example 8.1.1). The option Transform>Difference, with differencing lag equal to 1, generates

![](images/b816ede7658537d8eb269add75232d5166e1e4f578a37850f09c6c4cd4138cdf.jpg)

Figure 8-6   
![](images/0b9745648f485f0ca3903324821cec65b04acdd9c4a2c1c0a0e8c537be5b862e.jpg)  
The sample correlations $\hat { \rho } _ { i j } ( h )$ of the series $\{ D _ { t 1 } \}$ and $\{ D _ { t 2 } \}$ of Example 8.1.2, showing the bounds $\pm 1 . 9 6 n ^ { - \check { 1 } / 2 }$ . $( \hat { \rho } _ { i j } ( h )$ i s plotted as the jth graph in the ith row, $i , j = 1 , 2 . )$ )

![](images/5a9247af206e0c8d94a15e1b9a002f1e3890929b2bbb091f7c38a17a73f37be3.jpg)

![](images/1207dbeed6bed7304064724e6db83efc126814337fc0227c5c73af27fb0d12bd.jpg)

the bivariate differenced series $\{ ( D _ { t 1 } , D _ { t 2 } ) \}$ , and the correlation functions are then obtained as in Example 8.1.1 by clicking on the middle yellow button at the top of the ITSM screen. The sample auto- and cross-correlations $\hat { \rho } _ { i j } ( h ) , i , j = 1 , 2$ , are shown in Figure 8-6. As we shall see in Section 8.3, care must be taken in interpreting the cross-correlations without first taking into account the autocorrelations of $\{ D _ { t 1 } \}$ and $\{ D _ { t 2 } \}$ .

# 8.2 Second-Order Properties of Multivariate Time Series

Consider $m$ time series $\{ X _ { t i } , t = 0 , \pm 1 , \ldots , \} , i = 1 , \ldots , m$ , with $E X _ { t i } ^ { 2 } < \infty$ for all $t$ and $i$ . If all the finite-dimensional distributions of the random variables $\{ X _ { t i } \}$ were multivariate normal, then the distributional properties of $\{ X _ { t i } \}$ would be completely determined by the means

$$
\mu_ {t i} := E X _ {t i} \tag {8.2.1}
$$

and the covariances

$$
\gamma_ {i j} (t + h, t) := E \left[ \left(X _ {t + h, i} - \mu_ {t i}\right) \left(X _ {t j} - \mu_ {t j}\right) \right]. \tag {8.2.2}
$$

Even when the observations $\{ X _ { t i } \}$ do not have joint normal distributions, the quantities $\mu _ { t i }$ and $\gamma _ { i j } ( t + h , t )$ specify the second-order properties, the covariances providing us with a measure of the dependence, not only between observations in the same series, but also between the observations in different series.

It is more convenient in dealing with $m$ interrelated series to use vector notation. Thus we define

$$
\mathbf {X} _ {t} := \left[ \begin{array}{c} X _ {t 1} \\ \vdots \\ X _ {t m} \end{array} \right], \quad t = 0, \pm 1, \dots . \tag {8.2.3}
$$

The second-order properties of the multivariate time series $\{ { \mathbf { X } } _ { t } \}$ are then specified by the mean vectors

$$
\boldsymbol {\mu} _ {t} := E \mathbf {X} _ {t} = \left[ \begin{array}{c} \mu_ {t 1} \\ \vdots \\ \mu_ {t m} \end{array} \right] \tag {8.2.4}
$$

and covariance matrices

$$
\Gamma (t + h, t) := \left[ \begin{array}{c c c} \gamma_ {1 1} (t + h, t) & \dots & \gamma_ {1 m} (t + h, t) \\ \vdots & \ddots & \vdots \\ \gamma_ {m 1} (t + h, t) & \dots & \gamma_ {m m} (t + h, t) \end{array} \right], \tag {8.2.5}
$$

where

$$
\gamma_ {i j} (t + h, t) := \operatorname {C o v} \left(X _ {t + h, i}, X _ {t, j}\right).
$$

Remark 1. The matrix $\Gamma ( t + h , t )$ can also be expressed as

$$
\Gamma (t + h, t) := E \left[ \left(\mathbf {X} _ {t + h} - \boldsymbol {\mu} _ {t + h}\right) \left(\mathbf {X} _ {t} - \boldsymbol {\mu} _ {t}\right) ^ {\prime} \right],
$$

where as usual, the expected value of a random matrix $A$ is the matrix whose components are the expected values of the components of A. -

As in the univariate case, a particularly important role is played by the class of multivariate stationary time series, defined as follows.

# Definition 8.2.1

The m-variate series $\{ { \mathbf { X } } _ { t } \}$ is (weakly) stationary if

(i) $\pmb { \mu } _ { X } ( t )$ is independent of t

and

(ii) $\Gamma _ { X } ( t + h , t )$ is independent of t for each h.

For a stationary time series we shall use the notation

$$
\boldsymbol {\mu} := E \mathbf {X} _ {t} = \left[ \begin{array}{c} \mu_ {1} \\ \vdots \\ \mu_ {m} \end{array} \right] \tag {8.2.6}
$$

and

$$
\Gamma (h) := E \left[ \left(\mathbf {X} _ {t + h} - \boldsymbol {\mu}\right) \left(\mathbf {X} _ {t} - \boldsymbol {\mu}\right) ^ {\prime} \right] = \left[ \begin{array}{c c c} \gamma_ {1 1} (h) & \dots & \gamma_ {1 m} (h) \\ \vdots & \ddots & \vdots \\ \gamma_ {m 1} (h) & \dots & \gamma_ {m m} (h) \end{array} \right]. \tag {8.2.7}
$$

We shall refer to $\pmb { \mu }$ as the mean of the series and to $\Gamma ( h )$ as the covariance matrix at lag $h$ . Notice that if $\{ { \mathbf { X } } _ { t } \}$ is stationary with covariance matrix function $\Gamma ( \cdot )$ , then for each i, $\{ X _ { t i } \}$ is stationary with covariance function $\gamma _ { i i } ( \cdot )$ . The function $\gamma _ { i j } ( \cdot ) , i \neq j$ , is called the cross-covariance function of the two series $\{ X _ { t i } \}$ and $\{ X _ { t j } \}$ . It should be noted

that $\gamma _ { i j } ( \cdot )$ is not in general the same as $\gamma _ { j i } ( \cdot )$ . The correlation matrix function $R ( \cdot )$ is defined by

$$
R (h) := \left[ \begin{array}{c c c} \rho_ {1 1} (h) & \dots & \rho_ {1 m} (h) \\ \vdots & \ddots & \vdots \\ \rho_ {m 1} (h) & \dots & \rho_ {m m} (h) \end{array} \right], \tag {8.2.8}
$$

where $\rho _ { i j } ( h ) \ = \ \gamma _ { i j } ( h ) / [ \gamma _ { i i } ( 0 ) \gamma _ { j j } ( 0 ) ] ^ { 1 / 2 }$ . The function $R ( \cdot )$ is the covariance matrix function of the normalized series obtained by subtracting $\pmb { \mu }$ from $\mathbf { X } _ { t }$ and then dividing each component by its standard deviation.

Example 8.2.1 Consider the bivariate stationary process $\{ { \mathbf X } _ { t } \}$ defined by

$$
X _ {t 1} = Z _ {t},
$$

$$
X _ {t 2} = Z _ {t} + 0. 7 5 Z _ {t - 1 0},
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , 1 )$ . Elementary calculations yield ${ \pmb \mu } = { \bf 0 }$ ,

$$
\Gamma (- 1 0) = \left[ \begin{array}{c c} 0 & 0. 7 5 \\ 0 & 0. 7 5 \end{array} \right], \quad \Gamma (0) = \left[ \begin{array}{c c} 1 & 1 \\ 1 & 1. 5 6 2 5 \end{array} \right], \quad \Gamma (1 0) = \left[ \begin{array}{c c} 0 & 0 \\ 0. 7 5 & 0. 7 5 \end{array} \right],
$$

and $\Gamma ( j ) = 0$ otherwise. The correlation matrix function is given by

$$
R (- 1 0) = \left[ \begin{array}{c c} 0 & 0. 6 0 \\ 0 & 0. 4 8 \end{array} \right], \quad R (0) = \left[ \begin{array}{c c} 1 & 0. 8 \\ 0. 8 & 1 \end{array} \right], \quad R (1 0) = \left[ \begin{array}{c c} 0 & 0 \\ 0. 6 0 & 0. 4 8 \end{array} \right],
$$

and $R ( j ) = 0$ otherwise.

# Basic Properties of $\Gamma ( \cdot )$ :

1. $\Gamma ( h ) = \Gamma ^ { \prime } ( - h )$ ,   
2. $| \gamma _ { i j } ( h ) | \leq [ \gamma _ { i i } ( 0 ) \gamma _ { j j } ( 0 ) ] ^ { 1 / 2 }$ , i, j, = 1, . . . , m,   
3. $\gamma _ { i i } ( \cdot )$ is an autocovariance function, $i = 1 , \ldots , m$ , and   
4. $\begin{array} { r } { \sum _ { j , k = 1 } ^ { n } \mathbf { a } _ { j } ^ { \prime } \Gamma ( j - k ) \mathbf { a } _ { k } \geq 0 } \end{array}$ for all $n \in \{ 1 , 2 , . . . \}$ and $\mathbf { a } _ { 1 } , \ldots , \mathbf { a } _ { n } \in \mathbb { R } ^ { m }$

Proof The first property follows at once from the definition, the second from the fact that correlations cannot be greater than one in absolute value, and the third from the observation that $\gamma _ { i i } ( \cdot )$ is the autocovariance function of the stationary series $\{ X _ { t i } , t =$ $0 , \pm 1 , \ldots \}$ . Property 4 is a statement of the obvious fact that

$$
E \left(\sum_ {j = 1} ^ {n} \mathbf {a} _ {j} ^ {\prime} (\mathbf {X} _ {j} - \boldsymbol {\mu})\right) ^ {2} \geq 0.
$$

Remark 2. The basic properties of the matrices $\Gamma ( h )$ are shared also by the corresponding matrices of correlations $R ( h ) = [ \rho _ { i j } ( h ) ] _ { i , j = 1 } ^ { m }$ , which have the additional property

$$
\rho_ {i i} (0) = 1 \quad \text {f o r a l l} i.
$$

The correlation $\rho _ { i j } ( 0 )$ is the correlation between $X _ { t i }$ and $X _ { t j }$ , which is generally not equal to 1 if $i \neq j$ (see Example 8.2.1). It is also possible that $| \gamma _ { i j } ( h ) | > | \gamma _ { i j } ( 0 ) |$ if $i \neq j$ (see Problem 7.1). -

The simplest multivariate time series is multivariate white noise, the definition of which is quite analogous to that of univariate white noise.

# Definition 8.2.2.

The m-variate series $\{ \mathbf { Z } _ { t } \}$ is called white noise with mean 0 and covariance matrix $\Sigma$ , written

$$
\left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {W N} (\mathbf {0}, \Sigma), \tag {8.2.9}
$$

if $\{ \mathbf { Z } _ { t } \}$ is stationary with mean vector 0 and covariance matrix function

$$
\Gamma (h) = \left\{ \begin{array}{l l} \Sigma , & \text {i f} h = 0, \\ 0, & \text {o t h e r w i s e .} \end{array} \right. \tag {8.2.10}
$$

# Definition 8.2.3.

The $m$ -variate series $\{ \mathbf { Z } _ { t } \}$ is called iid noise with mean 0 and covariance matrix $\Sigma$ , written

$$
\left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {i i d} (\mathbf {0}, \Sigma), \tag {8.2.11}
$$

if the random vectors $\{ \mathbf { Z } _ { t } \}$ are independent and identically distributed with mean 0 and covariance matrix $\Sigma$ .

Multivariate white noise $\{ \mathbf { Z } _ { t } \}$ is used as a building block from which can be constructed an enormous variety of multivariate time series. The linear processes are generated as follows.

# Definition 8.2.4.

The m-variate series $\{ { \mathbf { X } } _ { t } \}$ is a linear process if it has the representation

$$
\mathbf {X} _ {t} = \sum_ {j = - \infty} ^ {\infty} C _ {j} \mathbf {Z} _ {t - j}, \quad \left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {W N} (\mathbf {0}, \boldsymbol {\Sigma}), \tag {8.2.12}
$$

where $\{ C _ { j } \}$ is a sequence of $m \times m$ matrices whose components are absolutely summable.

The linear process (8.2.12) is stationary (Problem 7.2) with mean 0 and covariance function

$$
\Gamma (h) = \sum_ {j = - \infty} ^ {\infty} C _ {j + h} \nabla C _ {j} ^ {\prime}, \quad h = 0, \pm 1, \dots . \tag {8.2.13}
$$

An $\mathbf { M A } ( \infty )$ process is a linear process with $C _ { j } = 0$ for $j < 0$ . Thus $\{ { \mathbf X } _ { t } \}$ is an $\mathbf { M A } ( \infty )$ process if and only if there exists a white noise sequence $\{ \mathbf { Z } _ { t } \}$ and a sequence of matrices $C _ { j }$ with absolutely summable components such that

$$
\mathbf {X} _ {t} = \sum_ {j = 0} ^ {\infty} C _ {j} \mathbf {Z} _ {t - j}.
$$

Multivariate ARMA processes will be discussed in Section 8.4, where it will be shown in particular that any causal ARMA $( p , q )$ process can be expressed as an $\mathbf { M A } ( \infty )$

process, while any invertible $\mathbf { A R M A } ( p , q )$ process can be expressed as an $\mathbf { A R } ( \infty )$ process, i.e., a process satisfying equations of the form

$$
\mathbf {X} _ {t} + \sum_ {j = 1} ^ {\infty} A _ {j} \mathbf {X} _ {t - j} = \mathbf {Z} _ {t},
$$

in which the matrices $A _ { j }$ have absolutely summable components.

# 8.2.1 Second-Order Properties in the Frequency Domain

varia, then e matrix function  has a matrix-va $\Gamma ( \cdot )$ have the property spectral density $\begin{array} { r } { \sum _ { h = - \infty } ^ { \infty } | \gamma _ { i j } ( h ) | < \infty , i , j = 1 , \ldots , m } \end{array}$ $\Gamma$

$$
f (\lambda) = \frac {1}{2 \pi} \sum_ {h = - \infty} ^ {\infty} e ^ {- i \lambda h} \Gamma (h), \quad - \pi \leq \lambda \leq \pi ,
$$

and $\Gamma$ can be expressed in terms of $f$ as

$$
\Gamma (h) = \int_ {- \pi} ^ {\pi} e ^ {i \lambda h} f (\lambda) d \lambda .
$$

The second-order properties of the stationary process $\{ { \mathbf X } _ { t } \}$ can therefore be described equivalently in terms of $f ( \cdot )$ rather than $\Gamma ( \cdot )$ . Similarly, $\{ { \mathbf X } _ { t } \}$ has a spectral representation

$$
\mathbf {X} _ {t} = \int_ {- \pi} ^ {\pi} e ^ {i \lambda t} d \mathbf {Z} (\lambda),
$$

where $\{ \mathbf { Z } ( \lambda ) , - \pi \ \leq \ \lambda \ \leq \ \pi \}$ is a process whose components are complex-valued processes satisfying

$$
E \left(d Z _ {j} (\lambda) d \overline {{Z}} _ {k} (\mu)\right) = \left\{ \begin{array}{l l} f _ {j k} (\lambda) d \lambda & \text {i f} \lambda = \mu , \\ 0 & \text {i f} \lambda \neq \mu , \end{array} \right.
$$

and $\overline { { Z } } _ { k }$ denotes the complex conjugate of $Z _ { k }$ . We shall not go into the spectral representation in this book. For details see Brockwell and Davis (1991).

# 8.3 Estimation of the Mean and Covariance Function

As in the univariate case, the estimation of the mean vector and covariances of a stationary multivariate time series plays an important role in describing and modeling the dependence structure of the component series. In this section we introduce estimators, for a stationary m-variate time series $\{ { \mathbf X } _ { t } \}$ , of the components μj, $\gamma _ { i j } ( h )$ , and $\rho _ { i j } ( h )$ of $\pmb { \mu }$ , $\Gamma ( h )$ , and $R ( h )$ , respectively. We also examine the large-sample properties of these estimators.

# 8.3.1 Estimation of $\pmb { \mu }$

A natural unbiased estimator of the mean vector $\pmb { \mu }$ based on the observations $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ is the vector of sample means

$$
\overline {{\mathbf {X}}} _ {n} = \frac {1}{n} \sum_ {t = 1} ^ {n} \mathbf {X} _ {t}.
$$

The resulting estimate of the mean of the jth time series is then the univariate sample mean $( 1 / n ) \textstyle \sum _ { t = 1 } ^ { n } X _ { t j }$ . If each of the univariate autocovariance functions $\gamma _ { i i } ( \cdot ) , i \ =$ $1 , \ldots , m$ , satisfies the conditions of Proposition 2.4.1, then the consistency of the estimator $\overline { { \mathbf { X } } } _ { n }$ can be established by applying the proposition to each of the component time series $\{ X _ { t i } \}$ . This immediately gives the following result.

Proposition 8.3.1. If $\{ { \mathbf X } _ { t } \}$ is a stationary multivariate time series with mean μ and covariance function $\Gamma ( \cdot )$ , then as $n \to \infty$ ,

$$
E \left(\overline {{\mathbf {X}}} _ {n} - \boldsymbol {\mu}\right) ^ {\prime} \left(\overline {{\mathbf {X}}} _ {n} - \boldsymbol {\mu}\right)\rightarrow 0 \quad \text {i f} \gamma_ {i i} (n) \rightarrow 0, \quad 1 \leq i \leq m,
$$

and

$$
n E \left(\overline {{\mathbf {X}}} _ {n} - \boldsymbol {\mu}\right) ^ {\prime} (\overline {{\mathbf {X}}} _ {n} - \boldsymbol {\mu}) \rightarrow \sum_ {i = 1} ^ {m} \sum_ {h = - \infty} ^ {\infty} \gamma_ {i i} (h) \quad \text {i f} \quad \sum_ {h = - \infty} ^ {\infty} | \gamma_ {i i} (h) | <   \infty , \quad 1 \leq i \leq m.
$$

Under more restrictive assumptions on the process $\{ { \mathbf { X } } _ { t } \}$ it can also be shown that $\overline { { \mathbf { X } } } _ { n }$ is approximately normally distributed for large $n$ . Determination of the covariance matrix of this distribution would allow us to obtain confidence regions for $\pmb { \mu }$ . However, this is quite complicated, and the following simple approximation is useful in practice.

For each i we construct a confidence interval for $\mu _ { i }$ based on the sample mean ${ \overline { { X } } } _ { i }$ of the univariate series $X _ { 1 i } , \dots , X _ { t i }$ and combine these to form a confidence region for $\pmb { \mu }$ . If $f _ { i } ( \omega )$ is the spectral density of the ith process $\{ X _ { t i } \}$ and if the sample size $n$ is large, then we know, under the same conditions as in Section 2.4, that ${ \sqrt { n } } \left( { \overline { { X } } } _ { i } - \mu _ { i } \right)$ is approximately normally distributed with mean zero and variance

$$
2 \pi f _ {i} (0) = \sum_ {k = - \infty} ^ {\infty} \gamma_ {i i} (k).
$$

It can also be shown (see, e.g., Anderson 1971) that

$$
2 \pi \hat {f} _ {i} (0) := \sum_ {| h | \leq r} \left(1 - \frac {| h |}{r}\right) \hat {\gamma} _ {i i} (h)
$$

is a consistent estimator of $2 \pi f _ { i } ( 0 )$ , provided that $r = r _ { n }$ is a sequence of numbers depending on $n$ in such a way that $r _ { n } \to \infty$ and $r _ { n } / n \to 0$ as $n  \infty$ . Thus if ${ \overline { { X } } } _ { i }$ denotes the sample mean of the ith process and $\varPhi _ { \alpha }$ is the $\alpha$ -quantile of the standard normal distribution, then the bounds

$$
\bar {X} _ {i} \pm \Phi_ {1 - \alpha / 2} \left(2 \pi \hat {f} _ {i} (0) / n\right) ^ {1 / 2}
$$

are asymptotic $( 1 - \alpha )$ confidence bounds for $\mu _ { i }$ . Hence

$$
\begin{array}{l} P \left(| \mu_ {i} - \bar {X} _ {i} | \leq \Phi_ {1 - \alpha / 2} \left(2 \pi \hat {f} _ {i} (0) / n\right) ^ {1 / 2}, i = 1, \dots , m\right) \\ \geq 1 - \sum_ {i = 1} ^ {m} P \left(\left| \mu_ {i} - \bar {X} _ {i} \right| > \Phi_ {1 - \alpha / 2} \left(2 \pi \hat {f} _ {i} (0) / n\right) ^ {1 / 2}\right), \\ \end{array}
$$

where the right-hand side converges to $1 - m \alpha$ as $n \to \infty$ . Consequently, as $n \to \infty$ , the set of $m$ -dimensional vectors bounded by

$$
\left\{x _ {i} = \bar {X} _ {i} \pm \Phi_ {1 - (\alpha / (2 m))} \left(2 \pi \hat {f} _ {i} (0) / n\right) ^ {1 / 2}, i = 1, \dots , m \right\} \tag {8.3.1}
$$

has a confidence coefficient that converges to a value greater than or equal to $1 - \alpha$ (and substantially greater if $m$ is large). Nevertheless, the region defined by (8.3.1) is easy to determine and is of reasonable size, provided that $m$ is not too large.

# 8.3.2 Estimation of $\Gamma ( h )$

As in the univariate case, a natural estimator of the covariance $\Gamma ( h ) = E \big [ ( { \mathbf { X } } _ { t + h } -$ $\pmb { \mu } ) \big ( \mathbf { X } _ { t } - \pmb { \mu } \big ) ^ { \prime } \big ]$ is

$$
\hat {\Gamma} (h) = \left\{ \begin{array}{l l} n ^ {- 1} \sum_ {t = 1} ^ {n - h} \left(\mathbf {X} _ {t + h} - \overline {{\mathbf {X}}} _ {n}\right) \left(\mathbf {X} _ {t} - \overline {{\mathbf {X}}} _ {n}\right) ^ {\prime} & \text {f o r} 0 \leq h \leq - 1, \\ \hat {\Gamma} ^ {\prime} (- h) & \text {f o r} - n + 1 \leq h <   0. \end{array} \right.
$$

Writing $\hat { \gamma } _ { i j } ( h )$ for the $( i , j )$ -component of $\hat { \Gamma } ( h ) , i , j = 1 , 2 , \ldots$ , we estimate the crosscorrelations by

$$
\hat {\rho} _ {i j} (h) = \hat {\gamma} _ {i j} (h) (\hat {\gamma} _ {i i} (0) \hat {\gamma} _ {j j} (0)) ^ {- 1 / 2}.
$$

If $i = j$ , then $\hat { \rho } _ { i j }$ reduces to the sample autocorrelation function of the ith series.

Derivation of the large-sample properties of $\hat { \gamma } _ { i j }$ and $\hat { \rho } _ { i j }$ is quite complicated in general. Here we shall simply note one result that is of particular importance for testing the independence of two component series. For details of the proof of this and related results, see Brockwell and Davis (1991).

Theorem 8.3.1. Let $\{ { \mathbf X } _ { t } \}$ be the bivariate time series whose components are defined by

$$
X _ {t 1} = \sum_ {k = - \infty} ^ {\infty} \alpha_ {k} Z _ {t - k, 1}, \quad \left\{Z _ {t 1} \right\} \sim \operatorname {I I D} \left(0, \sigma_ {1} ^ {2}\right),
$$

and

$$
X _ {t 2} = \sum_ {k = - \infty} ^ {\infty} \beta_ {k} Z _ {t - k, 2}, \quad \left\{Z _ {t 2} \right\} \sim \text {I I D} \left(0, \sigma_ {2} ^ {2}\right),
$$

where the two sequences $\{ Z _ { t 1 } \}$ and $\{ Z _ { t 2 } \}$ are independent, $\begin{array} { r } { \sum _ { k } | \alpha _ { k } | ~ < ~ \infty } \end{array}$ , and $\textstyle \sum _ { k } | \beta _ { k } | < \infty$ .

Then for all integers $h$ and $k$ with $\begin{array} { l l l } { h } & { \neq } & { k } \end{array}$ , the random variables $n ^ { 1 / 2 } \hat { \rho } _ { 1 2 } ( h )$ $n ^ { 1 / 2 } \hat { \rho } _ { 1 2 } ( k )$ e approximately, and covariance ean 0, variance, for n large. $\begin{array} { r } { \sum _ { j = - \infty } ^ { \infty } \rho _ { 1 1 } ( j ) \rho _ { 2 2 } ( j ) } \end{array}$ $\begin{array} { r } { \sum _ { j = - \infty } ^ { \infty } \rho _ { 1 1 } ( j ) \rho _ { 2 2 } ( j + k - h ) } \end{array}$

[For a related result that does not require the independence of the two series $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ see Bartlett’s Formula, Section 8.3.4 below.]

Theorem 8.3.1 is useful in testing for correlation between two time series. If one of the two processes in the theorem is white noise, then it follows at once from the theorem that ${ \hat { \rho } } _ { 1 2 } ( h )$ is approximately normally distributed with mean 0 and variance

$1 / n$ , in which case it is straightforward to test the hypothesis that $\rho _ { 1 2 } ( h ) = 0$ . However, if neither process is white noise, then a value of ${ \hat { \rho } } _ { 1 2 } ( h )$ that is large relative to $n ^ { - 1 / 2 }$ does not necessarily indicate that $\rho _ { 1 2 } ( h )$ is different from zero. For example, suppose that $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ are two independent AR(1) processes with $\rho _ { 1 1 } ( h ) = \rho _ { 2 2 } ( h ) = 0 . 8 ^ { | h | }$ . Then the large-sample variance of ${ \hat { \rho } } _ { 1 2 } ( h )$ is $\textstyle n ^ { - 1 } \left( 1 + 2 \sum _ { k = 1 } ^ { \infty } ( 0 . 6 4 ) ^ { k } \right) = 4 . 5 5 6 n ^ { - 1 }$ . It would therefore not be surprising to observe a value of ${ \hat { \rho } } _ { 1 2 } ( h )$ as large as $3 n ^ { - 1 / 2 }$ even though $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ are independent. If on the other hand, $\rho _ { 1 1 } ( h ) ~ = ~ 0 . 8 ^ { | h | }$ and $\rho _ { 2 2 } ( h ) \ = \ ( - 0 . 8 ) ^ { | h | }$ , then the large-sample variance of ${ \hat { \rho } } _ { 1 2 } ( h )$ is $0 . 2 1 9 5 n ^ { - 1 }$ , and an observed value of $3 n ^ { - 1 / 2 }$ for ${ \hat { \rho } } _ { 1 2 } ( h )$ would be very unlikely.

# 8.3.3 Testing for Independence of Two Stationary Time Series

Since by Theorem 8.3.1 the large-sample distribution of ${ \hat { \rho } } _ { 1 2 } ( h )$ depends on both $\rho _ { 1 1 } ( \cdot )$ and $\rho _ { 2 2 } ( \cdot )$ , any test for independence of the two component series cannot be based solely on estimated values of $\rho _ { 1 2 } ( h ) , h = 0 , \pm 1 , . . .$ , without taking into account the nature of the two component series.

This difficulty can be circumvented by “prewhitening” the two series before computing the cross-correlations ${ \hat { \rho } } _ { 1 2 } ( h )$ , i.e., by transforming the two series to white noise by application of suitable filters. If $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ are invertible ARMA $( p , q )$ processes, this can be achieved by the transformations

$$
Z _ {t i} = \sum_ {j = 0} ^ {\infty} \pi_ {j} ^ {(i)} X _ {t - j, i},
$$

where $\textstyle \sum _ { j = 0 } ^ { \infty } \pi _ { j } ^ { ( i ) } z ^ { j } = \phi ^ { ( i ) } ( z ) / \theta ^ { ( i ) } ( z )$ and $\phi ^ { ( i ) }$ , $\theta ^ { ( i ) }$ are the autoregressive and movingaverage polynomials of the ith series, $i = 1 , 2$ .

Since in practice the true model is nearly always unknown and since the data $X _ { t j }$ , $t \leq 0$ , are not available, it is convenient to replace the sequences $\{ Z _ { t i } \}$ by the residuals $\left\{ \hat { W } _ { t i } \right\}$ after fitting a maximum likelihood ARMA model to each of the component series (see (5.3.1)). If the fitted ARMA models were in fact the true models, the series $\left\{ \hat { W } _ { t i } \right\}$ would be white noise sequences for $i = 1 , 2$ .

To test the hypothesis $H _ { 0 }$ that $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ are independent series, we observe that under $H _ { 0 }$ , the corresponding two prewhitened series $\{ Z _ { t 1 } \}$ and $\{ Z _ { t 2 } \}$ are also independent. Theorem 8.3.1 then implies that the sample cross-correlations $\hat { \rho } _ { 1 2 } ( h ) , \hat { \rho } _ { 1 2 } ( k )$ , $h \neq k$ , of $\{ Z _ { t 1 } \}$ and $\{ Z _ { t 2 } \}$ are for large $n$ approximately independent and normally distributed with means 0 and variances $n ^ { - 1 }$ . An approximate test for independence can therefore be obtained by comparing the values of $| \hat { \rho } _ { 1 2 } ( h ) |$ with $1 . 9 6 n ^ { - 1 / 2 }$ , exactly as in Section 5.3.2. If we prewhiten only one of the two original series, say $\{ X _ { t 1 } \}$ , then under $H _ { 0 }$ Theorem 8.3.1 implies that the sample cross-correlations $\tilde { \rho } _ { 1 2 } ( h )$ , $\tilde { \rho } _ { 1 2 } ( k )$ , $h \neq k$ , of $\{ Z _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ are for large $n$ approximately normal with means 0, variances $n ^ { - 1 }$ and covariance $n ^ { - 1 } \rho _ { 2 2 } ( k - h )$ , where $\rho _ { 2 2 } ( \cdot )$ is the autocorrelation function of $\{ X _ { t 2 } \}$ . Hence, for any fixed $h$ , $\tilde { \rho } _ { 1 2 } ( h )$ also falls (under $H _ { 0 }$ ) between the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 }$ with a probability of approximately 0.95.

Example 8.3.1. The sample correlation functions $\hat { \rho } _ { i j } ( \cdot ) , ~ i , j ~ = ~ 1 , 2$ , of the bivariate time series E731A.TSM (of length $n ~ = ~ 2 0 0 )$ ) are shown in Figure 8-7. Without taking into account the autocorrelations $\hat { \rho } _ { i i } ( \cdot ) , i = 1 , 2$ , it is impossible to decide on the basis of the cross-correlations whether or not the two component processes are independent

![](images/4f3d552ca9ba6d64a5351e44a4ae11d05b852f41a36e456f7eb2468c0c388639.jpg)  
Figure 8-7 The sample correlations of the bivariate series E731A.TSM of Example 8.3.1, showing the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 }$

![](images/da764600767fad1d286cd4a9bd56350ea4123d8864d3a8efa53666b89f2d15ec.jpg)

![](images/a800ce9ef857e2cb4a2ebf9f8f1fe4d133efe506126f1f50284545ea24d28803.jpg)

![](images/4cf56697c2b53aa65308178515d9179ba876a1dc283345c30c716c08773250fd.jpg)

of each other. Notice that many of the sample cross-correlations $\hat { \rho } _ { i j } ( h )$ , $i \neq j$ , lie outside the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 } ~ = ~ \pm 0 . 1 3 \bar { 9 }$ . However, these bounds are relevant only if at least one of the component series is white noise. Since this is clearly not the case, a whitening transformation must be applied to at least one of the two component series. Analysis using ITSM leads to AR(1) models for each. The residuals from these maximum likelihood models are stored as a bivariate series in the file E731B.TSM, and their sample correlations, obtained from ITSM, are shown in Figure 8-8. All but two of the cross-correlations are between the bounds $\pm 0 . 1 3 9$ , suggesting by Theorem 8.3.1 that the two residual series (and hence the two original series) are uncorrelated. The data for this example were in fact generated as two independent AR(1) series with $\phi = 0 . 8$ and $\sigma ^ { 2 } = 1$ .

![](images/edc1500b0ffb4390731cce5d61d066926f1c87b42f36e547e7156640c77643d8.jpg)

# 8.3.4 Bartlett’s Formula

In Section 2.4 we gave Bartlett’s formula for the large-sample distribution of the sample autocorrelation vector $\hat { \pmb { \rho } } = \left( \hat { \rho } ( 1 ) , \ldots , \hat { \rho } ( k ) \right) ^ { \top }$ of a univariate time series. The following theorem gives a large-sample approximation to the covariances of the sample cross-correlations ${ \hat { \rho } } _ { 1 2 } ( h )$ and ${ \hat { \rho } } _ { 1 2 } ( k )$ of the bivariate time series $\{ { \mathbf X } _ { t } \}$ under the assumption that $\{ { \mathbf X } _ { t } \}$ is Gaussian. However, it is not assumed (as in Theorem 8.3.1) that $\{ X _ { t 1 } \}$ is independent of $\{ X _ { t 2 } \}$ .

![](images/2a4f57d1037133abdb586c3fa58c30b40e558149731ee50898ea3f9ff21900d2.jpg)  
Figure 8-8 The sample correlations of the bivariate series of residuals E731B.TSM, whose components are the residuals from the AR(1) models fitted to each of the component series in E731A.TSM

![](images/510e51b0d12ae07b4ea602d4b8c10c3dacc913b6b5994eba9ea64f98618d6701.jpg)

![](images/eb83399562c197bd6a28517ccc733073ab003df591369eec792fef53be47422d.jpg)

![](images/7059ee5d3bc5c720cb87de6fce78fcb70fb80277fddf1a9f67b51ebc61990212.jpg)

![](images/bc1797e76bb85a4003c6a3b5cfe83fec8d30408197622b9c80fedd89fc8f7c76.jpg)

![](images/89ec6ce6c7d9c51b63023337160f02c002ed4927c809b3b1c601e86f2d1262f1.jpg)

![](images/2e7b77223d91718b89f36a9ec06e59d0c69585ce6f8a313f9071d0b6dce8adf2.jpg)

Figure 8-9   
![](images/b55ad329275ef21a367d66d5f446c1b0b1e539c919f0172bf7ad202100005e02.jpg)  
The sample correlations of the whitened series $\hat { W } _ { t + h , 1 }$ and $\hat { W } _ { t 2 }$ of +Example 8.3.2, showing the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 }$

# Bartlett’s Formula:

If $\{ { \mathbf X } _ { t } \}$ is a bivariate Gaussian time series with covariances satisfying $\begin{array} { r } { \sum _ { h = - \infty } ^ { \infty } | \gamma _ { i j } ( h ) | < \infty , i , j = } \end{array}$ 1, 2, then

$$
\begin{array}{l} \lim  _ {n \rightarrow \infty} n \operatorname {C o v} \left(\hat {\rho} _ {1 2} (h), \hat {\rho} _ {1 2} (k)\right) = \sum_ {j = - \infty} ^ {\infty} \left[ \rho_ {1 1} (j) \rho_ {2 2} (j + k - h) + \rho_ {1 2} (j + k) \rho_ {2 1} (j - h) \right. \\ - \rho_ {1 2} (h) \left\{\rho_ {1 1} (j) \rho_ {1 2} (j + k) + \rho_ {2 2} (j) \rho_ {2 1} (j - k) \right\} \\ - \rho_ {1 2} (k) \left\{\rho_ {1 1} (j) \rho_ {1 2} (j + h) + \rho_ {2 2} (j) \rho_ {2 1} (j - h) \right\} \\ \left. + \rho_ {1 2} (h) \rho_ {1 2} (k) \left\{\frac {1}{2} \rho_ {1 1} ^ {2} (j) + \rho_ {1 2} ^ {2} (j) + \frac {1}{2} \rho_ {2 2} ^ {2} (j) \right\} \right] \\ \end{array}
$$

Corollary 8.3.1. If $\{ { \mathbf X } _ { t } \}$ satisfies the conditions for Bartlett’s formula, if either $\{ X _ { t 1 } \}$ or $\{ X _ { t 2 } \}$ is white noise, and if

$$
\rho_ {1 2} (h) = 0, \quad h \notin [ a, b ],
$$

then

$$
\lim  _ {n \to \infty} n \operatorname {V a r} \left(\hat {\rho} _ {1 2} (h)\right) = 1, \quad h \notin [ a, b ].
$$

Example 8.3.2. Sales with a leading indicator

We consider again the differenced series $\{ D _ { t 1 } \}$ and $\{ D _ { t 2 } \}$ of Example 8.1.2, for which we found the maximum likelihood models (8.1.1) and (8.1.2) using ITSM. The residuals from the two models (which can be filed by ITSM) are the two “whitened” series $\left\{ \hat { W } _ { t 1 } \right\}$ and $\left\{ \hat { W } _ { t 2 } \right\}$ with sample variances 0.0779 and 1.754, respectively. This bivariate series is contained in the file E732.TSM.

The sample auto- and cross-correlations of $\{ D _ { t 1 } \}$ and $\{ D _ { t 2 } \}$ were shown in Figure 8-6. Without taking into account the autocorrelations, it is not possible to draw any conclusions about the dependence between the two component series from the cross-correlations.

Examination of the sample cross-correlation function of the whitened series $\left\{ \hat { W } _ { t 1 } \right\}$ and $\left\{ \hat { W } _ { t 2 } \right\}$ , on the other hand, is much more informative. From Figure 8-9 it is apparent that there is one large-sample cross-correlation (between $\hat { W } _ { t + 3 , 2 }$ and $\hat { W } _ { t , 1 } )$ , while the others are all between $\pm 1 . { \bar { 9 } } 6 n ^ { - 1 / 2 }$ .

If $\left\{ \hat { W } _ { t 1 } \right\}$ and $\left\{ \hat { W } _ { t 2 } \right\}$ are assumed to be jointly Gaussian, Corollary 8.3.1 indicates the compatibility of the cross-correlations with a model for which

$$
\rho_ {1 2} (- 3) \neq 0
$$

and

$$
\rho_ {1 2} (h) = 0, \quad h \neq - 3.
$$

The value $\hat { \rho } _ { 1 2 } ( - 3 ) = 0 . 9 6 9$ suggests the model

$$
\hat {W} _ {t 2} = 4. 7 4 \hat {W} _ {t - 3, 1} + N _ {t}, \tag {8.3.2}
$$

where the stationary noise $\{ N _ { t } \}$ has small variance compared with $\left\{ \hat { W } _ { t 2 } \right\}$ and $\left\{ \hat { W } _ { t 1 } \right\}$ , and the coefficient 4.74 is the square root of the ratio of sample variances of $\left\{ \hat { W } _ { t 2 } \right\}$ and

$\left\{ \hat { W } _ { t 1 } \right\}$ . A study of the sample values of $\left\{ \hat { W } _ { t 2 } - 4 . 7 4 \hat { W } _ { t - 3 , 1 } \right\}$ suggests the model

$$
(1 + 0. 3 4 5 B) N _ {t} = U _ {t}, \{U _ {t} \} \sim \mathrm {W N} (0, 0. 0 7 8 2) \tag {8.3.3}
$$

for $\{ N _ { t } \}$ . Finally, replacing $\hat { W } _ { t 2 }$ and $\hat { W } _ { t - 3 , 1 }$ in (8.3.2) by $Z _ { t 2 }$ and $Z _ { t - 3 , 1 }$ , respectively, and then using (8.1.1) and (8.1.2) to express $Z _ { t 2 }$ and $Z _ { t - 3 , 1 }$ in terms of $\{ D _ { t 2 } \}$ and $\{ D _ { t 1 } \}$ , we obtain a model relating $\{ D _ { t 1 } \}$ , $\{ D _ { t 2 } \}$ , and $\{ U _ { t 1 } \}$ , namely,

$$
\begin{array}{l} D _ {t 2} + 0. 0 7 7 3 = (1 - 0. 6 1 0 B) (1 - 0. 8 3 8 B) ^ {- 1} [ 4. 7 4 (1 - 0. 4 7 4 B) ^ {- 1} D _ {t - 3, 1} \\ + (1 + 0. 3 4 5 B) ^ {- 1} U _ {t} ]. \\ \end{array}
$$

This model should be compared with the one derived later in Section 11.1 by the more systematic technique of transfer function modeling.

# 8.4 Multivariate ARMA Processes

As in the univariate case, we can define an extremely useful class of multivariate stationary processes $\{ { \mathbf X } _ { t } \}$ by requiring that $\{ { \mathbf X } _ { t } \}$ should satisfy a set of linear difference equations with constant coefficients. Multivariate white noise $\{ \mathbf { Z } _ { t } \}$ (see Definition 8.2.2) is a fundamental building block from which these ARMA processes are constructed.

# Definition 8.4.1.

$\{ { \mathbf { X } } _ { t } \}$ is an ARMA( p, q) process if $\{ { \mathbf { X } } _ { t } \}$ is stationary and if for every t,

$$
\mathbf {X} _ {t} - \Phi_ {1} \mathbf {X} _ {t - 1} - \dots - \Phi_ {p} \mathbf {X} _ {t - p} = \mathbf {Z} _ {t} + \Theta_ {1} \mathbf {Z} _ {t - 1} + \dots + \Theta_ {q} \mathbf {Z} _ {t - q}, \tag {8.4.1}
$$

where $\{ { \mathbf { Z } } _ { t } \} \sim \mathrm { W N } ( { \mathbf { 0 } } , { \boldsymbol { \Sigma } } ) . ( \{ { \mathbf { X } } _ { t } \}$ $\{ { \bf X } _ { t } \}$ is an ARMA $( p , q )$ process with mean $\pmb { \mu }$ if $\{ \mathbf { X } _ { t } - \pmb { \mu } \}$ is an ARMA $( p , q )$ process.)

Equations (8.4.1) can be written in the more compact form

$$
\Phi (B) \mathbf {X} _ {t} = \Theta (B) \mathbf {Z} _ {t}, \quad \left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {W N} (\mathbf {0}, \Sigma), \tag {8.4.2}
$$

where $\phi ( z ) : = I - \phi _ { 1 } z - \cdot \cdot \cdot - \phi _ { p } z ^ { p }$ and $\Theta ( z ) : = I + \Theta _ { 1 } z + \cdot \cdot \cdot + \Theta _ { q } z ^ { q }$ are matrixvalued polynomials, $I$ is the $m \times m$ identity matrix, and $B$ as usual denotes the backward shift operator. (Each component of the matrices $\varPhi ( z ) , \varTheta ( z )$ is a polynomial with real coefficients and degree less than or equal to $p , q$ , respectively.)

Example 8.4.1. The multivariate AR(1) process

Setting $p = 1$ and $q = 0$ in (8.4.1) gives the defining equations

$$
\mathbf {X} _ {t} = \Phi \mathbf {X} _ {t - 1} + \mathbf {Z} _ {t}, \quad \left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {W N} (\mathbf {0}, \Sigma), \tag {8.4.3}
$$

for the multivariate AR(1) series $\{ { \mathbf { X } } _ { t } \}$ . By exactly the same argument as used in Example 2.2.1, we can express $\mathbf { X } _ { t }$ as

$$
\mathbf {X} _ {t} = \sum_ {j = 0} ^ {\infty} \Phi^ {j} \mathbf {Z} _ {t - j}, \tag {8.4.4}
$$

provided that all the eigenvalues of $\varPhi$ are less than 1 in absolute value, i.e., provided that

$$
\det  (I - z \Phi) \neq 0 \quad \text {f o r a l l} z \in \mathbb {C} \text {s u c h t h a t} | z | \leq 1. \tag {8.4.5}
$$

If this condition is satisfied, then the coefficients $\varPhi ^ { j }$ are absolutely summable, and hence the series in (8.4.4) converges; i.e., each component of the matrix $\textstyle \sum _ { j = 0 } ^ { n } \phi ^ { j } \mathbf { Z } _ { t - j }$ converges (see Remark 1 of Section 2.2). The same argument as in Example 2.2.1 also shows that (8.4.4) is the unique stationary solution of (8.4.3). The condition that all the eigenvalues of $\varPhi$ should be less than 1 in absolute value (or equivalently (8.4.5)) is just the multivariate analogue of the condition $| \phi | < 1$ required for the existence of a causal stationary solution of the univariate AR(1) equations (2.2.8).

Causality and invertibility of a multivariate $\mathbf { A R M A } ( p , q )$ process are defined precisely as in Section 3.1, except that the coefficients $\begin{array} { r } { X _ { t } ~ = ~ \sum _ { j = 0 } ^ { \infty } \psi _ { j } Z _ { t - j } } \end{array}$ and $\begin{array} { r } { Z _ { t } ~ = ~ \sum _ { j = 0 } ^ { \infty } \pi _ { j } X _ { t - j } } \end{array}$ are replaced by $\psi _ { j }$ , $\pi _ { j }$ in the representations $m \ \times \ m$ matrices $\psi _ { j }$ and $\Pi _ { j }$ whose components are required to be absolutely summable. The following two theorems (proofs of which can be found in Brockwell and Davis (1991)) provide us with criteria for causality and invertibility analogous to those of Section 3.1.

# Causality:

An ARMA $( p , q )$ process $\{ { \mathbf X } _ { t } \}$ is causal, or a causal function of $\{ \mathbf { Z } _ { t } \}$ , if there exist matrices $\{ \psi _ { j } \}$ with absolutely summable components such that

$$
\mathbf {X} _ {t} = \sum_ {j = 0} ^ {\infty} \Psi_ {j} \mathbf {Z} _ {t - j} \quad \text {f o r a l l} t. \tag {8.4.6}
$$

Causality is equivalent to the condition

$$
\det  \Phi (z) \neq 0 \text {f o r a l l} z \in \mathbb {C} \text {s u c h t h a t} | z | \leq 1. \tag {8.4.7}
$$

The matrices $\psi _ { j }$ are found recursively from the equations

$$
\Psi_ {j} = \Theta_ {j} + \sum_ {k = 1} ^ {\infty} \Phi_ {k} \Psi_ {j - k}, \quad j = 0, 1, \dots , \tag {8.4.8}
$$

where we define $\Theta _ { 0 } = I$ , $\Theta _ { j } = 0$ for $j > q$ , $\varPhi _ { j } = 0$ for $j > p$ , and ${ \varPsi } _ { j } = 0$ for $j < 0$ .

# Invertibility:

An ARMA( p, q) process $\{ { \mathbf X } _ { t } \}$ is invertible if there exist matrices $\{ \Pi _ { j } \}$ with absolutely summable components such that

$$
\mathbf {Z} _ {t} = \sum_ {j = 0} ^ {\infty} \Pi_ {j} \mathbf {X} _ {t - j} \text {f o r a l l} t. \tag {8.4.9}
$$

Invertibility is equivalent to the condition

$$
\det \Theta (z) \neq 0 \text {f o r a l l} z \in \mathbb {C} \text {s u c h t h a t} | z | \leq 1. \tag {8.4.10}
$$

The matrices $\Pi _ { j }$ are found recursively from the equations

$$
\Pi_ {j} = - \Phi_ {j} - \sum_ {k = 1} ^ {\infty} \Theta_ {k} \Pi_ {j - k}, \quad j = 0, 1, \dots , \tag {8.4.11}
$$

where we define Φ0 I, $\varPhi _ { j } = 0$ for $j > p$ , $\Theta _ { j } = 0$ for $j > q$ , and $\Pi _ { j } = 0$ for $j < 0$ .

Example 8.4.2. For the multivariate AR(1) process defined by (8.4.3), the recursions (8.4.8) give

$$
\psi_ {0} = I,
$$

$$
\psi_ {1} = \Phi \psi_ {0} = \Phi ,
$$

$$
\psi_ {2} = \Phi \psi_ {1} = \Phi^ {2},
$$

$$
\psi_ {j} = \Phi \psi_ {j - 1} = \Phi^ {j}, j \geq 3,
$$

as already found in Example 8.4.1.

![](images/78f5e2472b1ba1e48a4137c2662a7ec9610ca68d257b11962b4677b80e2dc75c.jpg)

Remark 3. For the bivariate AR(1) process (8.4.3) with

$$
\varPhi = \left[ \begin{array}{c c} 0 & 0. 5 \\ 0 & 0 \end{array} \right]
$$

it is easy to check that $\varPsi _ { j } = \varPhi ^ { j } = 0$ for $j > 1$ and hence that $\{ { \mathbf X } _ { t } \}$ has the alternative representation

$$
\mathbf {X} _ {t} = \mathbf {Z} _ {t} + \Phi \mathbf {Z} _ {t - 1}
$$

as an MA(1) process. This example shows that it is not always possible to distinguish between multivariate ARMA models of different orders without imposing further restrictions. If, for example, attention is restricted to pure AR processes, the problem does not arise. For detailed accounts of the identification problem for general ARMA $( p , q )$ models see Hannan and Deistler (1988) and Lütkepohl (1993). -

# 8.4.1 The Covariance Matrix Function of a Causal ARMA Process

From (8.2.13) we can express the covariance matrix $\Gamma ( h ) = E ( \mathbf { X } _ { t + h } \mathbf { X } _ { t } ^ { \prime } )$ of the causal process (8.4.6) as

$$
\Gamma (h) = \sum_ {j = 0} ^ {\infty} \Psi_ {h + j} \Sigma \Psi_ {j} ^ {\prime}, \quad h = 0, \pm 1, \dots , \tag {8.4.12}
$$

where the matrices $\psi _ { j }$ are found from (8.4.8) and $\psi _ { j } : = 0$ for $j < 0$ .

The covariance matrices $\Gamma ( h ) , h = 0 , \pm 1 , . . .$ , can also be found by solving the Yule–Walker equations

$$
\Gamma (j) - \sum_ {r = 1} ^ {p} \Phi_ {r} \Gamma (j - r) = \sum_ {j \leq r \leq q} \Theta_ {r} \Xi \Psi_ {r - j}, \quad j = 0, 1, 2, \dots , \tag {8.4.13}
$$

obtained by postmultiplying (8.4.1) by $\mathbf { X } _ { t - j } ^ { \prime }$ and taking expectations. The first $p + 1$ of the equation (8.4.13) can be solved for the components of $\Gamma ( 0 ) , \ldots , \Gamma ( p )$ using the fact that $\Gamma ( - h ) = \Gamma ^ { \prime } ( h )$ . The remaining equations then give $\Gamma ( p + 1 ) , \Gamma ( p + 2 ) , . . .$ $\Gamma ( p + 1 )$ recursively. An explicit form of the solution of these equations can be written down by making use of Kronecker products and the vec operator (see e.g., Lütkepohl 1993).

Remark 4. If $z _ { 0 }$ is the root of det $\varPhi ( z ) = 0$ with smallest absolute value, then it can be shown from the recursions (8.4.8) that $\psi _ { j } / r ^ { j } \to 0$ as $j  \infty$ for all $r$ such that $| z _ { 0 } | ^ { - 1 } < r < 1$ . Hence, there is a constant $C$ such that each component of $\psi _ { j }$ is smaller

in absolute value than $C r ^ { j }$ . This implies in turn that there is a constant $K$ such that each component of the matrix $\psi _ { h + j } \pounds \psi _ { j } ^ { \prime }$ on the right of (8.4.12) is bounded in absolute value by $K r ^ { 2 j }$ . Provided that $| z _ { 0 } |$ is not very close to 1, this means that the series (8.4.12) converges rapidly, and the error incurred in each component by truncating the series after the term with $j = k - 1$ is smaller in absolute value than $\textstyle \sum _ { j = k } ^ { \infty } K r ^ { 2 j } = \bar { K } r ^ { 2 k } / \left( 1 - r ^ { 2 } \right)$ .

# 8.5 Best Linear Predictors of Second-Order Random Vectors

Let $\big \{ \mathbf { X } _ { t } = ( X _ { t 1 } , \ldots , X _ { t m } ) ^ { \prime } \big \}$ be an $m$ -variate time series with means $E \mathbf { X } _ { t } = \pmb { \mu } _ { t }$ and covariance function given by the $m \times m$ matrices

$$
K (i, j) = E \left(\mathbf {X} _ {i} \mathbf {X} _ {j} ^ {\prime}\right) - \boldsymbol {\mu} _ {i} \boldsymbol {\mu} _ {j} ^ {\prime}.
$$

If $\mathbf { Y } = ( Y _ { 1 } , \ldots , Y _ { m } ) ^ { \prime }$ is a random vector with finite second moments and $E \mathbf { Y } = \pmb { \mu }$ , we define

$$
P _ {n} (\mathbf {Y}) = \left(P _ {n} Y _ {1}, \dots , P _ {n} Y _ {m}\right) ^ {\prime}, \tag {8.5.1}
$$

where $P _ { n } Y _ { j }$ is the best linear predictor of the component $Y _ { j }$ of $\mathbf { Y }$ in terms of all of the components of the vectors $\mathbf { X } _ { t } , t = 1 , \ldots , n$ , and the constant 1. It follows immediately from the properties of the prediction operator (Section 2.5) that

$$
P _ {n} (\mathbf {Y}) = \boldsymbol {\mu} + A _ {1} \left(\mathbf {X} _ {n} - \boldsymbol {\mu} _ {n}\right) + \dots + A _ {n} \left(\mathbf {X} _ {1} - \boldsymbol {\mu} _ {1}\right) \tag {8.5.2}
$$

for some matrices $A _ { 1 } , \ldots , A _ { n }$ , and that

$$
\mathbf {Y} - P _ {n} (\mathbf {Y}) \perp \mathbf {X} _ {n + 1 - i}, i = 1, \dots , n, \tag {8.5.3}
$$

where we say that two m-dimensional random vectors $\mathbf { X }$ and Y are orthogonal (written $\mathbf { X } \perp \mathbf { Y } )$ if $E ( \mathbf { X Y ^ { \prime } } )$ is a matrix of zeros. The vector of best predictors (8.5.1) is uniquely determined by (8.5.2) and (8.5.3), although it is possible that there may be more than one possible choice for $A _ { 1 } , \ldots , A _ { n }$ .

As a special case of the above, if $\{ { \mathbf X } _ { t } \}$ is a zero-mean time series, the best linear predictor $\hat { \mathbf { X } } _ { n + 1 }$ of ${ \bf X } _ { n + 1 }$ in terms of $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ is obtained on replacing Y by ${ \bf X } _ { n + 1 }$ in (8.5.1). Thus

$$
\hat {\mathbf {X}} _ {n + 1} = \left\{ \begin{array}{l l} \mathbf {0}, & \text {i f} n = 0, \\ P _ {n} (\mathbf {X} _ {n + 1}), & \text {i f} n \geq 1. \end{array} \right.
$$

Hence, we can write

$$
\hat {\mathbf {X}} _ {n + 1} = \Phi_ {n 1} \mathbf {X} _ {n} + \dots + \Phi_ {n n} \mathbf {X} _ {1}, n = 1, 2, \dots , \tag {8.5.4}
$$

where, from (8.5.3), the coefficients $\varPhi _ { n j }$ , $j = 1 , \ldots , n$ , are such that

$$
E \left(\hat {\mathbf {X}} _ {n + 1} \mathbf {X} _ {n + 1 - i} ^ {\prime}\right) = E \left(\mathbf {X} _ {n + 1} \mathbf {X} _ {n + 1 - i} ^ {\prime}\right), \quad i = 1, \dots , n, \tag {8.5.5}
$$

i.e.,

$$
\sum_ {j = 1} ^ {n} \Phi_ {n j} K (n + 1 - j, n + 1 - i) = K (n + 1, n + 1 - i), \quad i = 1, \dots , n.
$$

In the case where $\{ { \mathbf X } _ { t } \}$ is stationary with $K ( i , j ) = \Gamma ( i - j )$ , the prediction equations simplify to the $m$ -dimensional analogues of (2.5.7), i.e.,

$$
\sum_ {j = 1} ^ {n} \Phi_ {n j} \Gamma (i - j) = \Gamma (i), \quad i = 1, \dots , n. \tag {8.5.6}
$$

Provided that the covariance matrix of the nm components of $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ is nonsingular for every $n \geq 1$ , the coefficients $\{ \varPhi _ { n j } \}$ can be determined recursively using a multivariate version of the Durbin–Levinson algorithm given by Whittle (1963) (for details see Brockwell and Davis (1991), Proposition 11.4.1). Whittle’s recursions also determine the covariance matrices of the one-step prediction errors, namely, $V _ { 0 } = \Gamma ( 0 )$ and, for $n \geq 1$ ,

$$
\begin{array}{l} V _ {n} = E (\mathbf {X} _ {n + 1} - \hat {\mathbf {X}} _ {n + 1}) (\mathbf {X} _ {n + 1} - \hat {\mathbf {X}} _ {n + 1}) ^ {\prime} \\ = \Gamma (0) - \Phi_ {n 1} \Gamma (- 1) - \dots - \Phi_ {n n} \Gamma (- n). \tag {8.5.7} \\ \end{array}
$$

Remark 5. The innovations algorithm also has a multivariate version that can be used for prediction in much the same way as the univariate version described in Section 2.5.4 (for details see Brockwell and Davis (1991), Proposition 11.4.2). -

# 8.6 Modeling and Forecasting with Multivariate AR Processes

If $\{ { \mathbf { X } } _ { t } \}$ is any zero-mean second-order multivariate time series, it is easy to show from the results of Section 8.5 (Problem 8.4) that the one-step prediction errors $\mathbf { X } _ { j } - \hat { \mathbf { X } } _ { j }$ , $j = 1 , \dots , n$ , have the property

$$
E \left(\mathbf {X} _ {j} - \hat {\mathbf {X}} _ {j}\right) \left(\mathbf {X} _ {k} - \hat {\mathbf {X}} _ {k}\right) ^ {\prime} = 0 \text {f o r} j \neq k. \tag {8.6.1}
$$

Moreover, the matrix $M$ such that

$$
\left[ \begin{array}{c} \mathbf {X} _ {1} - \hat {\mathbf {X}} _ {1} \\ \mathbf {X} _ {2} - \hat {\mathbf {X}} _ {2} \\ \mathbf {X} _ {3} - \hat {\mathbf {X}} _ {3} \\ \vdots \\ \mathbf {X} _ {n} - \hat {\mathbf {X}} _ {n} \end{array} \right] = M \left[ \begin{array}{c} \mathbf {X} _ {1} \\ \mathbf {X} _ {2} \\ \mathbf {X} _ {3} \\ \vdots \\ \mathbf {X} _ {n} \end{array} \right] \tag {8.6.2}
$$

is lower triangular with ones on the diagonal and therefore has determinant equal to 1.

If the series $\{ { \mathbf X } _ { t } \}$ is also Gaussian, then (8.6.1) implies that the prediction errors $\mathbf { U } _ { j } = \mathbf { X } _ { j } - \hat { \mathbf { X } } _ { j } , j = \hat { 1 } , \ldots , n$ , are independent with covariance matrices $V _ { 0 } , \ldots , V _ { n - 1 }$ , respectively (as specified in (8.5.7)). Consequently, the joint density of the prediction errors is the product

$$
f \left(\mathbf {u} _ {1}, \dots , \mathbf {u} _ {n}\right) = (2 \pi) ^ {- n m / 2} \left(\prod_ {j = 1} ^ {n} \det V _ {j - 1}\right) ^ {- 1 / 2} \exp \left[ - \frac {1}{2} \sum_ {j = 1} ^ {n} \mathbf {u} _ {j} ^ {\prime} V _ {j - 1} ^ {- 1} \mathbf {u} _ {j} \right].
$$

Since the determinant of the matrix $M$ in (8.6.2) is equal to 1, the joint density of the observations $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ at $\mathbf { x } _ { 1 } , \ldots , \mathbf { x } _ { n }$ is obtained on replacing $\mathbf { u } _ { 1 } , \ldots , \mathbf { u } _ { n }$ in the last expression by the values of $\mathbf { X } _ { j } - \hat { \mathbf { X } } _ { j }$ corresponding to the observations $\mathbf { x } _ { 1 } , \ldots , \mathbf { x } _ { n }$ .

If we suppose that $\{ { \mathbf { X } } _ { t } \}$ is a zero-mean $m$ -variate $\operatorname { A R } ( p )$ process with coefficient matrices $\pmb { \phi } = \{ \phi _ { 1 } , \ldots , \phi _ { p } \}$ and white noise covariance matrix $\Sigma$ , we can therefore express the likelihood of the observations $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ as

$$
L (\boldsymbol {\Phi}, \boldsymbol {\Sigma}) = (2 \pi) ^ {- n m / 2} \left(\prod_ {j = 1} ^ {n} \det V _ {j - 1}\right) ^ {- 1 / 2} \exp \left[ - \frac {1}{2} \sum_ {j = 1} ^ {n} \mathbf {U} _ {j} ^ {\prime} V _ {j - 1} ^ {- 1} \mathbf {U} _ {j} \right],
$$

where $\mathbf { U } _ { j } = \mathbf { X } _ { j } - { \hat { \mathbf { X } } } _ { j } , j = 1 , \ldots , n $ , and $\hat { \mathbf { X } } _ { j }$ and $V _ { j }$ are found from (8.5.4), (8.5.6), and (8.5.7).

Maximization of the Gaussian likelihood is much more difficult in the multivariate than in the univariate case because of the potentially large number of parameters involved and the fact that it is not possible to compute the maximum likelihood estimator of $\pmb { \phi }$ independently of $\Sigma$ as in the univariate case. In principle, maximum likelihood estimators can be computed with the aid of efficient nonlinear optimization algorithms, but it is important to begin the search with preliminary estimates that are reasonably close to the maximum. For pure AR processes good preliminary estimates can be obtained using Whittle’s algorithm or a multivariate version of Burg’s algorithm given by Jones (1978). We shall restrict our discussion here to the use of Whittle’s algorithm (the multivariate option AR-Model $>$ Estimation>Yule-Walker in ITSM), but Jones’s multivariate version of Burg’s algorithm is also available (AR-Model>Estimation>Burg). Other useful algorithms can be found in Lütkepohl (1993), in particular the method of conditional least squares and the method of Hannan and Rissanen (1982), the latter being useful also for preliminary estimation in the more difficult problem of fitting ARMA $( p , q )$ models with $q > 0$ . Spectral methods of estimation for multivariate ARMA processes are also frequently used. A discussion of these (as well as some time-domain methods) is given in Anderson (1980).

Order selection for multivariate autoregressive models can be made by minimizing a multivariate analogue of the univariate AICC statistic

$$
\mathrm {A I C C} = - 2 \ln L \left(\Phi_ {1}, \dots , \Phi_ {p}, \Sigma\right) + \frac {2 (p m ^ {2} + 1) n m}{n m - p m ^ {2} - 2}. \tag {8.6.3}
$$

# 8.6.1 Estimation for Autoregressive Processes Using Whittle’s Algorithm

If $\{ { \mathbf X } _ { t } \}$ is the (causal) multivariate $\operatorname { A R } ( p )$ process defined by the difference equations

$$
\mathbf {X} _ {t} = \Phi_ {1} \mathbf {X} _ {t - 1} + \dots + \Phi_ {p} \mathbf {X} _ {t - p} + \mathbf {Z} _ {t}, \quad \left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {W N} (\mathbf {0}, \Sigma), \tag {8.6.4}
$$

then postmultiplying by $\mathbf { X } _ { t - j } ^ { \prime } , j = 0 , \ldots , p$ , and taking expectations gives the equations

$$
\Sigma = \Gamma (0) - \sum_ {j = 1} ^ {p} \Phi_ {j} \Gamma (- j) \tag {8.6.5}
$$

and

$$
\Gamma (i) = \sum_ {j = 1} ^ {n} \Phi_ {j} \Gamma (i - j), \quad i = 1, \dots , p. \tag {8.6.6}
$$

Given the matrices $\Gamma ( 0 ) , \ldots , \Gamma ( p )$ , equation (8.6.6) can be used to determine the coefficient matrices $\mathcal { \hat { P } } _ { 1 } , \ldots , \mathcal { \hat { P } } _ { p }$ . The white noise covariance matrix $\Sigma$ can then be found from (8.6.5). The solution of these equations for $\mathcal { \bar { P } } _ { 1 } , \ldots , \mathcal { \bar { P } } _ { p }$ , and $\Sigma$ is identical to the solution of (8.5.6) and (8.5.7) for the prediction coefficient matrices $\varPhi _ { p 1 } , \ldots , \varPhi _ { p p }$ and the corresponding prediction error covariance matrix $V _ { p }$ . Consequently, Whittle’s algorithm can be used to carry out the algebra.

The Yule–Walker estimators $\hat { \phi } _ { 1 } , \ldots , \hat { \phi } _ { p }$ , and $\hat { \Sigma }$ for the model (8.6.4) fitted to the data $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ are obtained by replacing $\Gamma ( j )$ in (8.6.5) and (8.6.6) by $\hat { \Gamma } ( j )$ , $j ~ = ~ 0 , \ldots , p$ , and solving the resulting equations for $\mathcal { \bar { P } } _ { 1 } , \ldots , \mathcal { \bar { P } } _ { p }$ , and $\Sigma$ . The solution of these equations is obtained from ITSM by selecting the multivariate option AR-Model>Estimation>Yule-Walker. The mean vector of the fitted model is the sample mean of the data, and Whittle’s algorithm is used to solve the

equations (8.6.5) and (8.6.6) for the coefficient matrices and the white noise covariance matrix. The fitted model is displayed by ITSM in the form

$$
\mathbf {X} _ {t} = \phi_ {0} + \Phi_ {1} \mathbf {X} _ {t - 1} + \dots + \Phi_ {p} \mathbf {X} _ {t - p} + \mathbf {Z} _ {t}, \{\mathbf {Z} _ {t} \} \sim \mathrm {W N} (\mathbf {0}, \boldsymbol {\Sigma}).
$$

Note that the mean $\pmb { \mu }$ of this model is not the vector $\phi _ { 0 }$ , but

$$
\boldsymbol {\mu} = (I - \Phi_ {1} - \dots - \Phi_ {p}) ^ {- 1} \phi_ {0}.
$$

In fitting multivariate autoregressive models using ITSM, check the box Find minimum AICC model to find the $\operatorname { A R } ( p )$ model with $0 ~ \leq ~ p ~ \leq ~ 2 0$ that minimizes the AICC value as defined in (8.6.3).

Analogous calculations using Jones’s multivariate version of Burg’s algorithm can be carried out by selecting AR-Model $>$ Estimation>Burg.

# Example 8.6.1 The Dow Jones and All Ordinaries Indices

To find the minimum AICC Yule–Walker model (of order less than or equal to 20) for the bivariate series $\{ ( X _ { t 1 } , X _ { t 2 } ) ^ { \prime }$ , $t = 1 , \ldots , 2 5 0 \}$ of Example 8.1.1, proceed as follows. Select File>Project>Open> Multivariate, click OK, and then double-click on the file name, DJAOPC2.TSM. Check that Number of columns is set to 2, the dimension of the observation vectors, and click OK again to see graphs of the two component time series. No differencing is required (recalling from Example 8.1.1 that $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ are the daily percentage price changes of the original Dow Jones and All Ordinaries Indices). Select AR-Model $. >$ Estimation>Yule-Walker, check the box Find minimum AICC Model, click OK, and you will obtain the model

$$
\left[ \begin{array}{c} X _ {t 1} \\ X _ {t 2} \end{array} \right] = \left[ \begin{array}{c} 0. 0 2 8 8 \\ 0. 0 0 8 3 6 \end{array} \right] + \left[ \begin{array}{c c} - 0. 0 1 4 8 & 0. 0 3 5 7 \\ 0. 6 5 8 9 & 0. 0 9 9 8 \end{array} \right] \left[ \begin{array}{c} X _ {t - 1, 1} \\ X _ {t - 1, 2} \end{array} \right] + \left[ \begin{array}{c} Z _ {t 1} \\ Z _ {t 2} \end{array} \right],
$$

where

$$
\left[ \begin{array}{c} Z _ {t 1} \\ Z _ {t 2} \end{array} \right] \sim \operatorname {W N} \left(\left[ \begin{array}{c} 0 \\ 0 \end{array} \right], \left[ \begin{array}{c c} 0. 3 6 5 3 & 0. 0 2 2 4 \\ 0. 0 2 2 4 & 0. 6 0 1 6 \end{array} \right]\right).
$$

![](images/67c439c4e4b595be53502e868c8f9a55adea26011b08fd8aa3a51e4ebe660ca5.jpg)

# Example 8.6.2 Sales with a leading indicator

The series $\{ Y _ { t 1 } \}$ (leading indicator) and $\{ Y _ { t 2 } \}$ (sales) are stored in bivariate form $\boldsymbol { Y } _ { t 1 }$ in column 1 and $Y _ { t 2 }$ in column 2) in the file LS2.TSM. On opening this file in ITSM you will see the graphs of the two component time series. Inspection of the graphs immediately suggests, as in Example 8.2.2, that the differencing operator $\nabla = 1 - B$ should be applied to the data before a stationary AR model is fitted. Select Transform>Difference and specify 1 for the differencing lag. Click OK and you will see the graphs of the two differenced series. Inspection of the series and their correlation functions (obtained by pressing the second yellow button at the top of the ITSM window) suggests that no further differencing is necessary. The next step is to select AR-model>Estimation>Yule-Walker with the option Find minimum AICC model. The resulting model has order $p \ = \ 5$ and parameters $\phi _ { 0 } = ( 0 . 0 3 2 8 \ 0 . 0 1 5 6 ) ^ { \prime }$ ,

$$
\hat {\varPhi} _ {1} = \left[ \begin{array}{c c} - 0. 5 1 7 & 0. 0 2 4 \\ - 0. 0 1 9 & - 0. 0 5 1 \end{array} \right], \hat {\varPhi} _ {2} = \left[ \begin{array}{c c} - 0. 1 9 2 & - 0. 0 1 8 \\ 0. 0 4 7 & 0. 2 5 0 \end{array} \right], \hat {\varPhi} _ {3} = \left[ \begin{array}{c c} - 0. 0 7 3 & 0. 0 1 0 \\ 4. 6 7 8 & 0. 2 0 7 \end{array} \right],
$$

$$
\hat {\varPhi} _ {4} = \left[ \begin{array}{c c} - 0. 0 3 2 & - 0. 0 0 9 \\ 3. 6 6 4 & 0. 0 0 4 \end{array} \right], \hat {\varPhi} _ {5} = \left[ \begin{array}{c c} 0. 0 2 2 & 0. 0 1 1 \\ 1. 3 0 0 & 0. 0 2 9 \end{array} \right], \hat {\mathbf {\Sigma}} = \left[ \begin{array}{c c} 0. 0 7 6 & - 0. 0 0 3 \\ - 0. 0 0 3 & 0. 0 9 5 \end{array} \right],
$$

with $\operatorname { A I C C } = 1 0 9 . 4 9$ . (Analogous calculations using Burg’s algorithm give an AR(8) model for the differenced series.) The sample cross-correlations of the residual vectors $\hat { \mathbf { Z } } _ { t }$ can be plotted by clicking on the last blue button at the top of the ITSM window. These are nearly all within the bounds $\pm 1 . 9 6 / \sqrt { n }$ , suggesting that the model is a good fit. The components of the residual vectors themselves are plotted by selecting AR Model>Residual Analysis>Plot Residuals. Simulated observations from the fitted model can be generated using the option AR Model>Simulate. The fitted model has the interesting property that the upper right component of each of the co- efficient matrices is close to zero. This suggests that $\{ X _ { t 1 } \}$ can be effectively modeled independently of $\{ X _ { t 2 } \}$ . In fact, the MA(1) model

$$
X _ {t 1} = (1 - 0. 4 7 4 B) U _ {t}, \quad \left\{U _ {t} \right\} \sim \mathrm {W N} (0, 0. 0 7 7 9), \tag {8.6.7}
$$

provides an adequate fit to the univariate series $\{ X _ { t 1 } \}$ . Inspecting the bottom rows of the coefficient matrices and deleting small entries, we find that the relation between $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ can be expressed approximately as

$$
X _ {t 2} = 0. 2 5 0 X _ {t - 2, 2} + 0. 2 0 7 X _ {t - 3, 2} + 4. 6 7 8 X _ {t - 3, 1} + 3. 6 6 4 X _ {t - 4, 1} + 1. 3 0 0 X _ {t - 5, 1} + W _ {t},
$$

or equivalently,

$$
X _ {t 2} = \frac {4 . 6 7 8 B ^ {3} \left(1 + 0 . 7 8 3 B + 0 . 2 7 8 B ^ {2}\right)}{1 - 0 . 2 5 0 B ^ {2} - 0 . 2 0 7 B ^ {3}} X _ {t 1} + \frac {W _ {t}}{1 - 0 . 2 5 0 B ^ {2} - 0 . 2 0 7 B ^ {3}}, \tag {8.6.8}
$$

where $\{ W _ { t } \} \sim \mathrm { W N } ( 0 , 0 . 0 9 5 )$ . Moreover, since the estimated noise covariance matrix is essentially diagonal, it follows that the two sequences $\{ X _ { t 1 } \}$ and $\{ W _ { t } \}$ are uncorrelated. This reduced model defined by (8.6.7) and (8.6.8) is an example of a transfer function model that expresses the “output” series $\{ X _ { t 2 } \}$ as the output of a linear filter with “input” $\{ X _ { t 1 } \}$ plus added noise. A more direct approach to the fitting of transfer function models is given in Section 11.1 and applied to this same data set.

![](images/01ae031406475853c8a8bc946237d57248e1966a3c778ddca4f587514674ac9b.jpg)

# 8.6.2 Forecasting Multivariate Autoregressive Processes

The technique developed in Section 8.5 allows us to compute the minimum mean squared error one-step linear predictors $\hat { \mathbf { X } } _ { n + 1 }$ for any multivariate stationary time series from the mean $\pmb { \mu }$ and autocovariance matrices $\Gamma ( h )$ by recursively determining the coefficients $\varPhi _ { n i }$ , $i = 1 , \ldots , n$ , and evaluating

$$
\hat {\mathbf {X}} _ {n + 1} = \boldsymbol {\mu} + \Phi_ {n 1} \left(\mathbf {X} _ {n} - \boldsymbol {\mu}\right) + \dots + \Phi_ {n n} \left(\mathbf {X} _ {1} - \boldsymbol {\mu}\right). \tag {8.6.9}
$$

The situation is simplified when $\{ { \mathbf { X } } _ { t } \}$ is the causal $\operatorname { A R } ( p )$ process defined by (8.6.4), since for $n \geq p$ (as is almost always the case in practice)

$$
\hat {\mathbf {X}} _ {n + 1} = \Phi_ {1} \mathbf {X} _ {n} + \dots + \Phi_ {p} \mathbf {X} _ {n + 1 - p}. \tag {8.6.10}
$$

To verify (8.6.10) it suffices to observe that the right-hand side has the required form (8.5.2) and that the prediction error

$$
\mathbf {X} _ {n + 1} - \boldsymbol {\Phi} _ {1} \mathbf {X} _ {n} - \dots - \boldsymbol {\Phi} _ {p} \mathbf {X} _ {n + 1 - p} = \mathbf {Z} _ {n + 1}
$$

is orthogonal to $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ in the sense of (8.5.3). (In fact, the prediction error is orthogonal to all $\mathbf { X } _ { j }$ , $- \infty < j \leq n$ , showing that if $n \geq p$ , then (8.6.10) is also the best linear predictor of ${ \bf X } _ { n + 1 }$ in terms of all components of $\mathbf { X } _ { j }$ , $- \infty < j \leq n .$ ) The covariance matrix of the one-step prediction error is clearly $E ( \mathbf { Z } _ { n + 1 } \mathbf { Z } _ { n + 1 } ^ { \prime } ) = \mathfrak { T }$ .

To compute the best $h$ -step linear predictor $P _ { n } \mathbf { X } _ { n + h }$ based on all the components of $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ we apply the linear operator $P _ { n }$ to (8.6.4) to obtain the recursions

$$
P _ {n} \mathbf {X} _ {n + h} = \Phi_ {1} P _ {n} \mathbf {X} _ {n + h - 1} + \dots + \Phi_ {p} P _ {n} \mathbf {X} _ {n + h - p}. \tag {8.6.11}
$$

These equations are easily solved recursively, first for $P _ { n } \mathbf { X } _ { n + 1 }$ , then for $P _ { n } \mathbf { X } _ { n + 2 }$ , $P _ { n } \mathbf { X } _ { n + 3 }$ , . . ., etc. If $n \ \geq \ p$ , then the $h$ -step predictors based on all components of $\mathbf { X } _ { j }$ , $- \infty \ < \ j \ \leq \ n$ , also satisfy (8.6.11) and are therefore the same as the $h$ -step predictors based on $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ .

To compute the $h$ -step error covariance matrices, recall from (8.4.6) that

$$
\mathbf {X} _ {n + h} = \sum_ {j = 0} ^ {\infty} \Psi_ {j} \mathbf {Z} _ {n + h - j}, \tag {8.6.12}
$$

where the coefficient matrices $\psi _ { j }$ are found from the recursions (8.4.8) with $q = 0$ . From (8.6.12) we find that for $n \geq p$ ,

$$
P _ {n} \mathbf {X} _ {n + h} = \sum_ {j = h} ^ {\infty} \Psi_ {j} \mathbf {Z} _ {n + h - j}. \tag {8.6.13}
$$

Subtracting (8.6.13) from (8.6.12) gives the $h$ -step prediction error

$$
\mathbf {X} _ {n + h} - P _ {n} \mathbf {X} _ {n + h} = \sum_ {j = 0} ^ {h - 1} \Psi_ {j} \mathbf {Z} _ {n + h - j}, \tag {8.6.14}
$$

with covariance matrix

$$
E \left[ \left(\mathbf {X} _ {n + h} - P _ {n} \mathbf {X} _ {n + h}\right) \left(\mathbf {X} _ {n + h} - P _ {n} \mathbf {X} _ {n + h}\right) ^ {\prime} \right] = \sum_ {j = 0} ^ {h - 1} \Psi_ {j} \Sigma_ {j} \Psi_ {j} ^ {\prime}, n \geq p. \tag {8.6.15}
$$

For the (not necessarily zero-mean) causal $\operatorname { A R } ( p )$ process defined by

$$
\mathbf {X} _ {t} = \phi_ {0} + \Phi_ {1} \mathbf {X} _ {t - 1} + \dots + \Phi_ {p} \mathbf {X} _ {t - p} + \mathbf {Z} _ {t}, \quad \left\{\mathbf {Z} _ {t} \right\} \sim \mathrm {W N} (\mathbf {0}, \boldsymbol {\Sigma}),
$$

Equations (8.6.10) and (8.6.11) remain valid, provided that $\phi _ { 0 }$ is added to each of their right-hand sides. The error covariance matrices are the same as in the case $\phi _ { 0 } = \mathbf { 0 }$ .

The above calculations are all based on the assumption that the $\operatorname { A R } ( p )$ model for the series is known. However, in practice, the parameters of the model are usually estimated from the data, and the uncertainty in the predicted values of the series will be larger than indicated by (8.6.15) because of parameter estimation errors. See Lütkepohl (1993).

# Example 8.6.3 The Dow Jones and All Ordinaries Indices

The VAR(1) model fitted to the series $\{ \mathbf { X } _ { t } , t = 1 , \ldots , 2 5 0 \}$ in Example 8.6.1 was

$$
\left[ \begin{array}{c} X _ {t 1} \\ X _ {t 2} \end{array} \right] = \left[ \begin{array}{c} 0. 0 2 8 8 \\ 0. 0 0 8 3 6 \end{array} \right] + \left[ \begin{array}{c c} - 0. 0 1 4 8 & 0. 0 3 5 7 \\ 0. 6 5 8 9 & 0. 0 9 9 8 \end{array} \right] \left[ \begin{array}{c} X _ {t - 1, 1} \\ X _ {t - 1, 2} \end{array} \right] + \left[ \begin{array}{c} Z _ {t 1} \\ Z _ {t 2} \end{array} \right],
$$

where

$$
\left[ \begin{array}{c} Z _ {t 1} \\ Z _ {t 2} \end{array} \right] \sim \operatorname {W N} \left(\left[ \begin{array}{c} 0 \\ 0 \end{array} \right], \left[ \begin{array}{c c} 0. 3 6 5 3 & 0. 0 2 2 4 \\ 0. 0 2 2 4 & 0. 6 0 1 6 \end{array} \right]\right).
$$

The one-step mean squared error for prediction of $X _ { t 2 }$ , assuming the validity of this model, is thus 0.6016. This is a substantial reduction from the estimated mean squared error $\hat { \gamma } _ { 2 2 } ( 0 ) = 0 . 7 7 1 2$ when the sample mean $\hat { \mu } _ { 2 } = 0 . 0 3 0 9$ is used as the one-step predictor.

If we fit a univariate model to the series $\{ X _ { t 2 } \}$ using ITSM, we find that the autoregression with minimum AICC value (645.0) is

$$
X _ {t 2} = 0. 0 2 7 3 + 0. 1 1 8 0 X _ {t - 1, 2} + Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 7 6 0 4).
$$

Assuming the validity of this model, we thus obtain a mean squared error for onestep prediction of 0.7604, which is slightly less than the estimated mean squared error (0.7712) incurred when the sample mean is used for one-step prediction.

The preceding calculations suggest that there is little to be gained from the point of view of one-step prediction by fitting a univariate model to $\{ X _ { t 2 } \}$ , while there is a substantial reduction achieved by the bivariate AR(1) model for $\begin{array} { r l } { \{ \mathbf { X } _ { t } } & { { } = } \end{array}$ $( X _ { t 1 } , X _ { t 2 } ) ^ { \prime } \}$ .

To test the models fitted above, we consider the next forty values $\{ \mathbf { X } _ { t } , t \ =$ 251, . . . , 290 , which are stored in the file DJAOPCF.TSM. We can use these values, in conjunction with the bivariate and univariate models fitted to the data for $t = 1 , \ldots , 2 5 0$ , to compute one-step predictors of $X _ { t 2 }$ , $t = 2 5 1$ , . . . , 290. The results are as follows:

Predictor Average Squared Error   

<table><tr><td>μ = 0.0309</td><td>0.4706</td></tr><tr><td>AR(1)</td><td>0.4591</td></tr><tr><td>VAR(1)</td><td>0.3962</td></tr></table>

It is clear from these results that the sample variance of the series $\{ X _ { t 2 } , t = 2 5 1 , . . . ,$ , 290} is rather less than that of the series $\{ X _ { t 2 } , t = 1 , \ldots , 2 5 0 \}$ , and consequently, the average squared errors of all three predictors are substantially less than expected from the models fitted to the latter series. Both the AR(1) and VAR(1) models show an improvement in one-step average squared error over the sample mean $\hat { \mu }$ , but the improvement shown by the bivariate model is much more pronounced.

The calculation of predictors and their error covariance matrices for multivariate ARIMA and SARIMA processes is analogous to the corresponding univariate calculation, so we shall simply state the pertinent results. Suppose that $\{ \mathbf { Y } _ { t } \}$ is a nonstationary process satisfying $D ( B ) \mathbf { Y } _ { t } = \mathbf { U } _ { t }$ where $D ( z ) = 1 - d _ { 1 } z - \cdot \cdot \cdot - d _ { r } z ^ { r }$ is a polynomial with $D ( 1 ) = 0$ and $\{ \mathbf { U } _ { t } \}$ is a causal invertible ARMA process with mean $\pmb { \mu }$ . Then $\mathbf { X } _ { t } = \mathbf { U } _ { t } - \pmb { \mu }$ satisfies

$$
\Phi (B) \mathbf {X} _ {t} = \Theta (B) \mathbf {Z} _ {t}, \quad \left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {W N} (\mathbf {0}, \boldsymbol {\Sigma}). \tag {8.6.16}
$$

Under the assumption that the random vectors $\mathbf { Y } _ { - r + 1 } , \ldots , \mathbf { Y } _ { 0 }$ are uncorrelated with the sequence $\{ \mathbf { Z } _ { t } \}$ , the best linear predictors $\tilde { P } _ { n } { \bf Y } _ { j }$ of $\mathbf { Y } _ { j } , j > n > 0$ , based on 1 and the components of $\mathbf { Y } _ { j }$ $\mathbf { Y } _ { j } , - r + 1 , \le j \le n$ , are found as follows. Compute the observed values of $\mathbf { U } _ { t } = D ( B ) \mathbf { Y } _ { t }$ , $t = 1 , \ldots , n$ , and use the ARMA model for $\mathbf { X } _ { t } = \mathbf { U } _ { t } - \pmb { \mu }$ to compute predictors $P _ { n } \mathbf { U } _ { n + h }$ . Then use the recursions

$$
\tilde {P} _ {n} \mathbf {Y} _ {n + h} = P _ {n} \mathbf {U} _ {n + h} + \sum_ {j = 1} ^ {r} d _ {j} \tilde {P} _ {n} \mathbf {Y} _ {n + h - j} \tag {8.6.17}
$$

to compute successively $\tilde { P } _ { n } \mathbf { Y } _ { n + 1 }$ , $\tilde { P } _ { n } \mathbf { Y } _ { n + 2 }$ , $\tilde { P } _ { n } \mathbf { Y } _ { n + 3 }$ , etc. The error covariance matrices are approximately (for large $n$ )

$$
E \left[ \left(\mathbf {Y} _ {n + h} - \tilde {P} _ {n} \mathbf {Y} _ {n + h}\right) \left(\mathbf {Y} _ {n + h} - \tilde {P} _ {n} \mathbf {Y} _ {n + h}\right) ^ {\prime} \right] = \sum_ {j = 0} ^ {h - 1} \Psi_ {j} ^ {*} \Sigma \Psi_ {j} ^ {* \prime}, \tag {8.6.18}
$$

where $\boldsymbol { \varPsi } _ { j } ^ { * }$ is the coefficient of $z ^ { j }$ in the power series expansion

$$
\sum_ {j = 0} ^ {\infty} \Psi_ {j} ^ {*} z ^ {j} = D (z) ^ {- 1} \Phi^ {- 1} (z) \Theta (z), \quad | z | <   1.
$$

The matrices $\boldsymbol { \varPsi } _ { j } ^ { * }$ are most readily found from the recursions (8.4.8) after replacing $\phi _ { j } , j ~ = ~ 1 , \ldots , p$ , by $\phi _ { j } ^ { * } , j = 1 , . . . , p + r$ , where $\varPhi _ { j } ^ { * }$ is the coefficient of $z ^ { j }$ in $D ( z ) \phi ( z )$ .

Remark 6. In the special case where $\Theta ( z ) = I$ (i.e., in the purely autoregressive case) the expression (8.6.18) for the $h$ -step error covariance matrix is exact for all $n \geq p$ (i.e., if there are at least $p + r$ observed vectors). The program ITSM allows differencing transformations and subtraction of the mean before fitting a multivariate autoregression. Predicted values for the original series and the standard deviations of the prediction errors can be determined using the multivariate option Forecasting>AR Model. -

Remark 7. In the multivariate case, simple differencing of the type discussed in this section where the same operator $D ( B )$ is applied to all components of the random vectors is rather restrictive. It is useful to consider more general linear transformations of the data for the purpose of generating a stationary series. Such considerations lead to the class of cointegrated models discussed briefly in Section 8.7 below. -

# Example 8.6.4 Sales with a leading indicator

Assume that the model fitted to the bivariate series $\{ \mathbf { Y } _ { t } , t = 0 , \ldots , 1 4 9 \}$ in Example 8.6.2 is correct, i.e., that

$$
\Phi (B) \mathbf {X} _ {t} = \mathbf {Z} _ {t}, \quad \left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {W N} \left(\mathbf {0}, \hat {\mathbf {\Sigma}}\right),
$$

where

$$
\mathbf {X} _ {t} = (1 - B) \mathbf {Y} _ {t} - (0. 0 2 2 8, 0. 4 2 0) ^ {\prime}, \quad t = 1, \dots , 1 4 9,
$$

$\phi ( B ) \ = \ I - \hat { \phi } _ { 1 } B - \cdot \cdot \cdot - \hat { \phi } _ { 5 } B ^ { 5 }$ , and $\hat { \phi } _ { 1 } , \ldots , \hat { \phi } _ { 5 } , \hat { \Sigma }$ are the matrices found in Example 8.6.2. Then the one- and two-step predictors of $\mathbf { X } _ { 1 5 0 }$ and $\mathbf { X } _ { 1 5 1 }$ are obtained from (8.6.11) as

$$
P _ {1 4 9} \mathbf {X} _ {1 5 0} = \hat {\varPhi} _ {1} \mathbf {X} _ {1 4 9} + \dots + \hat {\varPhi} _ {5} \mathbf {X} _ {1 4 5} = \left[ \begin{array}{c} 0. 1 6 3 \\ - 0. 2 1 7 \end{array} \right]
$$

and

$$
P _ {1 4 9} \mathbf {X} _ {1 5 1} = \hat {\varPhi} _ {1} P _ {1 4 9} \mathbf {X} _ {1 5 0} + \hat {\varPhi} _ {2} \mathbf {X} _ {1 4 9} + \dots + \hat {\varPhi} _ {5} \mathbf {X} _ {1 4 6} = \left[ \begin{array}{c} - 0. 0 2 7 \\ 0. 8 1 6 \end{array} \right]
$$

with error covariance matrices, from (8.6.15),

$$
\Sigma = \left[ \begin{array}{c c} 0. 0 7 6 & - 0. 0 0 3 \\ - 0. 0 0 3 & 0. 0 9 5 \end{array} \right]
$$

and

$$
\boldsymbol {\Sigma} + \hat {\boldsymbol {\Phi}} _ {1} \boldsymbol {\Sigma} \hat {\boldsymbol {\Phi}} _ {1} ^ {\prime} = \left[ \begin{array}{l l} 0. 0 9 6 & - 0. 0 0 2 \\ - 0. 0 0 2 & 0. 0 9 5 \end{array} \right],
$$

respectively.

Similarly, the one- and two-step predictors of $\mathbf { Y } _ { 1 5 0 }$ and $\mathbf { Y } _ { 1 5 1 }$ are obtained from (8.6.17) as

$$
\tilde {P} _ {1 4 9} \mathbf {Y} _ {1 5 0} = \left[ \begin{array}{c} 0. 0 2 2 8 \\ 0. 4 2 0 \end{array} \right] + P _ {1 4 9} \mathbf {X} _ {1 5 0} + \mathbf {Y} _ {1 4 9} = \left[ \begin{array}{c} 1 3. 5 9 \\ 2 6 2. 9 0 \end{array} \right]
$$

and

$$
\tilde {P} _ {1 4 9} \mathbf {Y} _ {1 5 1} = \left[ \begin{array}{l} 0. 0 2 2 8 \\ 0. 4 2 0 \end{array} \right] + P _ {1 4 9} \mathbf {X} _ {1 5 1} + \tilde {P} _ {1 4 9} \mathbf {Y} _ {1 5 0} = \left[ \begin{array}{l} 1 3. 5 9 \\ 2 6 4. 1 4 \end{array} \right]
$$

with error covariance matrices, from (8.6.18),

$$
\Sigma = \left[ \begin{array}{c c} 0. 0 7 6 & - 0. 0 0 3 \\ - 0. 0 0 3 & 0. 0 9 5 \end{array} \right]
$$

and

$$
\Xi + \left(I + \hat {\varPhi} _ {1}\right) \Xi \left(I + \hat {\varPhi} _ {1}\right) ^ {\prime} = \left[ \begin{array}{c c} 0. 0 9 4 & - 0. 0 0 3 \\ - 0. 0 0 3 & 0. 1 8 1 \end{array} \right],
$$

respectively. The predicted values and the standard deviations of the predictors can easily be verified with the aid of the program ITSM. It is also of interest to compare the results with those obtained by fitting a transfer function model to the data as described in Section 11.1 below.

-

# 8.7 Cointegration

We have seen that nonstationary univariate time series can frequently be made stationary by applying the differencing operator $\nabla = 1 - B$ repeatedly. If $\{ \nabla ^ { d } X _ { t } \}$ is stationary for some positive integer $d$ but $\left\{ \nabla ^ { d - 1 } X _ { t } \right\}$ is nonstationary, we say that $\{ X _ { t } \}$ is integrated of order $\pmb { d }$ , or more concisely, $\{ X _ { t } \} ~ \sim ~ I ( d )$ . Many macroeconomic time series are found to be integrated of order 1.

If $\{ { \mathbf { X } } _ { t } \}$ is a $k$ -variate time series, we define $\left\{ \nabla ^ { d } \mathbf { X } _ { t } \right\}$ to be the series whose jth component is obtained by applying the operator $( 1 - B ) ^ { d }$ to the jth component of $\{ { \mathbf X } _ { t } \}$ , $j = 1 , \dots , k$ . The idea of a cointegrated multivariate time series was introduced by Granger (1981) and developed by Engle and Granger (1987). Here we use the slightly different definition of Lütkepohl (1993). We say that the $k$ -dimensional time series $\{ { \mathbf { X } } _ { t } \}$ is integrated of order $d$ (or $\{ \mathbf { X } _ { t } \} \sim I ( d ) )$ if $d$ is a positive integer, $\left\{ \nabla ^ { d } \mathbf { X } _ { t } \right\}$ is stationary, and $\left\{ \nabla ^ { d - 1 } \mathbf { X } _ { t } \right\}$ is nonstationary. The $I ( d )$ process $\{ { \mathbf X } _ { t } \}$ is said to be cointegrated with cointegration vector $\pmb { \alpha }$ if $_ \alpha$ is a $k \times 1$ vector such that $\{ { \pmb { \alpha } } ^ { \prime } { \pmb { \mathrm { X } } } _ { t } \}$ is of order less than $d$ .

Example 8.7.1 A simple example is provided by the bivariate process whose first component is the random walk

$$
X _ {t} = \sum_ {j = 1} ^ {t} Z _ {j}, \quad t = 1, 2, \dots , \quad \left\{Z _ {t} \right\} \sim \operatorname {I I D} \left(0, \sigma^ {2}\right),
$$

and whose second component consists of noisy observations of the same random walk,

$$
Y _ {t} = X _ {t} + W _ {t}, \quad t = 1, 2, \ldots , \quad \{W _ {t} \} \sim \operatorname {I I D} \left(0, \tau^ {2}\right),
$$

where $\{ W _ { t } \}$ is independent of $\{ Z _ { t } \}$ . Then $\{ ( X _ { t } , Y _ { t } ) ^ { \prime } \}$ is integrated of order 1 and cointegrated with cointegration vector $\alpha = ( 1 , - 1 ) ^ { \prime }$ .

The notion of cointegration captures the idea of univariate nonstationary time series “moving together.” Thus, even though $\{ X _ { t } \}$ and $\{ Y _ { t } \}$ in Example 8.7.1 are both nonstationary, they are linked in the sense that they differ only by the stationary sequence $\{ W _ { t } \}$ . Series that behave in a cointegrated manner are often encountered in economics. Engle and Granger (1991) give as an illustrative example the prices of tomatoes $U _ { t }$ and $V _ { t }$ in Northern and Southern California. These are linked by the fact that if one were to increase sufficiently relative to the other, the profitability of buying in one market and selling for a profit in the other would tend to push the prices $( U _ { t } , V _ { t } ) ^ { \prime }$ toward the straight line $\nu = u$ in $\mathbb { R } ^ { 2 }$ . This line is said to be an attractor for $( U _ { t } , V _ { t } ) ^ { \prime }$ , since although $U _ { t }$ and $V _ { t }$ may both vary in a nonstationary manner as $t$ increases, the points $( U _ { t } , V _ { t } ) ^ { \prime }$ will exhibit relatively small random deviations from the line $\nu = u$ .

Example 8.7.2 If we apply the operator $\nabla = 1 - B$ to the bivariate process defined in Example 8.7.1 in order to render it stationary, we obtain the series $( U _ { t } , V _ { t } ) ^ { \prime }$ , where

$$
U _ {t} = Z _ {t}
$$

and

$$
V _ {t} = Z _ {t} + W _ {t} - W _ {t - 1}.
$$

The series $\{ ( U _ { t } , V _ { t } ) ^ { \prime } \}$ is clearly a stationary multivariate MA(1) process

$$
\left[ \begin{array}{c} U _ {t} \\ V _ {t} \end{array} \right] = \left[ \begin{array}{c c} 1 & 0 \\ 0 & 1 \end{array} \right] \left[ \begin{array}{c} Z _ {t} \\ Z _ {t} + W _ {t} \end{array} \right] - \left[ \begin{array}{c c} 0 & 0 \\ - 1 & 1 \end{array} \right] \left[ \begin{array}{c} Z _ {t - 1} \\ Z _ {t - 1} + W _ {t - 1} \end{array} \right].
$$

However, the process $\{ ( U _ { t } , V _ { t } ) ^ { \prime } \}$ cannot be represented as an $\mathbf { A R } ( \infty )$ process, since the matrix $\left[ { \begin{array} { l l } { 1 } & { 0 } \\ { 0 } & { 1 } \end{array} } \right] - z \left[ { \begin{array} { l l } { 0 } & { 0 } \\ { - 1 } & { 1 } \end{array} } \right]$ has zero determinant when $z = 1$ , thus violating condition the closely related error-correction models). We shall not go into the details here but refer the reader to Engle and Granger (1987) and Lütkepohl (1993).

# Problems

8.1 Let $\{ Y _ { t } \}$ be a stationary process and define the bivariate process $X _ { t 1 } = Y _ { t }$ , $X _ { t 2 } =$ $Y _ { t - d }$ , where $d \ne 0$ . Show that $\{ ( X _ { t 1 } , X _ { t 2 } ) ^ { \prime } \}$ is stationary and express its crosscorrelation function in terms of the autocorrelation function of $\{ Y _ { t } \}$ . If $\rho _ { Y } ( h )  0$ as $h  \infty$ , show that there exists a lag $k$ for which $\rho _ { 1 2 } ( k ) > \rho _ { 1 2 } ( 0 )$ .   
8.2 Show that the covariance matrix function of the multivariate linear process defined by (8.2.12) is as specified in (8.2.13).   
8.3 Let $\{ { \mathbf { X } } _ { t } \}$ be the bivariate time series whose components are the MA(1) processes defined by

$$
X _ {t 1} = Z _ {t, 1} + 0. 8 Z _ {t - 1, 1}, \quad \left\{Z _ {t 1} \right\} \sim \operatorname {I I D} \left(0, \sigma_ {1} ^ {2}\right),
$$

and

$$
X _ {t 2} = Z _ {t, 2} - 0. 6 Z _ {t - 1, 2}, \quad \left\{Z _ {t 2} \right\} \sim \operatorname {I I D} \left(0, \sigma_ {2} ^ {2}\right),
$$

where the two sequences $\{ Z _ { t 1 } \}$ and $\{ Z _ { t 2 } \}$ are independent.

a. Find a large-sample approximation to the variance of $n ^ { 1 / 2 } \hat { \rho } _ { 1 2 } ( h )$   
b. Find a large-sample approximation to the covariance of $n ^ { 1 / 2 } \hat { \rho } _ { 1 2 } ( h )$ and $n ^ { 1 / 2 } \hat { \rho } _ { 1 2 } ( k )$ for $h \neq k$ .

8.4 Use the characterization (8.5.3) of the multivariate best linear predictor of $\mathbf { Y }$ in terms of $\{ \mathbf { X } _ { 1 } , \ldots . . . \mathbf { X } _ { n } \}$ to establish the orthogonality of the one-step prediction errors $\mathbf { X } _ { j } - \hat { \mathbf { X } } _ { j }$ and $\ddot { \mathbf X } _ { k } - \hat { \mathbf X } _ { k } , j \neq k$ , as asserted in (8.6.1).

8.5 Determine the covariance matrix function of the ARMA(1,1) process satisfying

$$
\mathbf {X} _ {t} - \varPhi \mathbf {X} _ {t - 1} = \mathbf {Z} _ {t} + \Theta \mathbf {Z} _ {t - 1}, \quad \left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {W N} (\mathbf {0}, I _ {2}),
$$

where $I _ { 2 }$ is the $2 \times 2$ identity matrix and $\phi = \Theta ^ { \prime } = \left[ ^ { 0 . 5 \ 0 . 5 } _ { 0 . 5 } \right]$ .

8.6 a. Let $\{ { \mathbf X } _ { t } \}$ be a causal $\operatorname { A R } ( p )$ process satisfying the recursions

$$
\mathbf {X} _ {t} = \Phi_ {1} \mathbf {X} _ {t - 1} + \dots + \Phi_ {p} \mathbf {X} _ {t - p} + \mathbf {Z} _ {t}, \quad \left\{\mathbf {Z} _ {t} \right\} \sim \operatorname {W N} (\mathbf {0}, \Sigma).
$$

For $n \ \geq \ p$ write down recursions for the predictors $P _ { n } \mathbf { X } _ { n + h }$ , $h ~ \geq ~ 0$ , and find explicit expressions for the error covariance matrices in terms of the AR coefficients and $\Sigma$ when $h = 1 , 2$ , and 3.

b. Suppose now that $\{ \mathbf { Y } _ { t } \}$ is the multivariate ARIMA $( p , 1 , 0 )$ process satisfying $\nabla { \mathbf { Y } } _ { t } = { \mathbf { X } } _ { t }$ , where $\{ { \mathbf X } _ { t } \}$ is the AR process in (a). Assuming that $E ( \mathbf { Y } _ { 0 } \mathbf { X } _ { t } ^ { \prime } ) = 0$ , for $t \geq 1$ , show (using (8.6.17) with $r = 1$ and $d = 1$ ) that

$$
\tilde {P} _ {n} (\mathbf {Y} _ {n + h}) = \mathbf {Y} _ {n} + \sum_ {j = 1} ^ {h} P _ {n} \mathbf {X} _ {n + j},
$$

and derive the error covariance matrices when $h = 1 , 2$ , and 3. Compare these results with those obtained in Example 8.6.4.

8.7 Use the program ITSM to find the minimum AICC AR model of order less than or equal to 20 for the bivariate series $\{ ( X _ { t 1 } , X _ { t 2 } ) ^ { \prime } , t \ = \ 1 , \ldots , 2 0 0 \}$ with components filed as APPJK2.TSM. Use the fitted model to predict $( X _ { t 1 } , X _ { t 2 } ) ^ { \prime }$ , t 201, 202, 203 and estimate the error covariance matrices of the predictors (assuming that the fitted model is appropriate for the data).

8.8 Let $\{ X _ { t 1 } , t = 1 , \ldots , 6 3 \}$ and $\{ X _ { t 2 } , t = 1 , \ldots , 6 3 \}$ denote the differenced series $\{ \nabla \ln Y _ { t 1 } \}$ and $\{ \nabla \ln Y _ { t 2 } \}$ , where $\{ Y _ { t 1 } \}$ and $\{ Y _ { t 2 } \}$ are the annual mink and muskrat trappings filed as APPH.TSM and APPI.TSM, respectively).

a. Use ITSM to construct and save the series $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ as univariate data files X1.TSM and X2.TSM, respectively. (After making the required transformations press the red EXP button and save each transformed series to a file with the appropriate name.) To enter X1 and X2 as a bivariate series in ITSM, open X1 as a multivariate series with Number of columns equal to 1. Then open X2 as a univariate series. Click the project editor button (at the top left of the ITSM window), click on the plus signs next to the projects X1.TSM and X2.TSM, then click on the series that appears just below X2.TSM and drag it to the first line of the project X1.TSM. It will then be added as a second component, making X1.TSM a bivariate project consisting of the two component series X1 and X2. Click OK to close the project editor and close

the ITSM window labeled X2.TSM. You will then see the graphs of X1 and X2. Press the second yellow button to see the correlation functions of $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ . For more information on the project editor in ITSM consult the Project Editor section of the PDF file ITSM_HELP.

b. Conduct a test for independence of the two series $\{ X _ { t 1 } \}$ and $\{ X _ { t 1 } \}$

8.9 Use ITSM to open the data file STOCK7.TSM, which contains the daily returns on seven different stock market indices from April 27th, 1998, through April 9th, 1999. (Consult the Data Sets section of the PDF file ITSM_HELP for more information.) Fit a multivariate autoregression to the trivariate series consisting of the returns on the Dow Jones Industrials, All Ordinaries, and Nikkei indices. Check the model for goodness of fit and interpret the results.

# 9

# State-Space Models

9.1 State-Space Representations   
9.2 The Basic Structural Model   
9.3 State-Space Representation of ARIMA Models   
9.4 The Kalman Recursions   
9.5 Estimation for State-Space Models   
9.6 State-Space Models with Missing Observations   
9.7 The EM Algorithm   
9.8 Generalized State-Space Models

In recent years state-space representations and the associated Kalman recursions have had a profound impact on time series analysis and many related areas. The techniques were originally developed in connection with the control of linear systems (for accounts of this subject see Davis and Vinter 1985; Hannan and Deistler 1988). An extremely rich class of models for time series, including and going well beyond the linear ARIMA and classical decomposition models considered so far in this book, can be formulated as special cases of the general state-space model defined below in Section 9.1. In econometrics the structural time series models developed by Harvey (1990) are formulated (like the classical decomposition model) directly in terms of components of interest such as trend, seasonal component, and noise. However, the rigidity of the classical decomposition model is avoided by allowing the trend and seasonal components to evolve randomly rather than deterministically. An introduction to these structural models is given in Section 9.2, and a state-space representation is developed for a general ARIMA process in Section 9.3. The Kalman recursions, which play a key role in the analysis of state-space models, are derived in Section 9.4. These recursions allow a unified approach to prediction and estimation for all processes that can be given a state-space representation. Following the development of the Kalman recursions we discuss estimation with structural models (Section 9.5) and the formulation of state-space models to deal with missing values (Section 9.6). In Section 9.7 we introduce the EM algorithm, an iterative procedure for maximizing the

likelihood when only a subset of the complete data set is available. The EM algorithm is particularly well suited for estimation problems in the state-space framework. Generalized state-space models are introduced in Section 9.8. These are Bayesian models that can be used to represent time series of many different types, as demonstrated by two applications to time series of count data. Throughout the chapter we shall use the notation

$$
\left\{\mathbf {W} _ {t} \right\} \sim \operatorname {W N} (\mathbf {0}, \left\{R _ {t} \right\})
$$

to indicate that the random vectors $\mathbf { W } _ { t }$ have mean 0 and that

$$
E \left(\mathbf {W} _ {s} \mathbf {W} _ {t} ^ {\prime}\right) = \left\{ \begin{array}{l l} R _ {t}, & \text {i f} s = t, \\ 0, & \text {o t h e r w i s e .} \end{array} \right.
$$

# 9.1 State-Space Representations

A state-space model for a (possibly multivariate) time series $\{ { \bf Y } _ { t } , t ~ = ~ 1 , 2 , . . . \}$ consists of two equations. The first, known as the observation equation, expresses the $w$ -dimensional observation $\mathbf { Y } _ { t }$ as a linear function of a $\nu$ -dimensional state variable $\mathbf { X } _ { t }$ plus noise. Thus

$$
\mathbf {Y} _ {t} = G _ {t} \mathbf {X} _ {t} + \mathbf {W} _ {t}, \quad t = 1, 2, \dots , \tag {9.1.1}
$$

where $\{ \mathbf { W } _ { t } \} \sim \mathrm { W N } ( \mathbf { 0 } , \{ R _ { t } \} )$ and $\{ G _ { t } \}$ is a sequence of $w \times \nu$ matrices. The second equation, called the state equation, determines the state $\mathbf { X } _ { t + 1 }$ at time $t + 1$ in terms of the previous state $\mathbf { X } _ { t }$ and a noise term. The state equation is

$$
\mathbf {X} _ {t + 1} = F _ {t} \mathbf {X} _ {t} + \mathbf {V} _ {t}, \quad t = 1, 2, \dots , \tag {9.1.2}
$$

where $\{ F _ { t } \}$ is a sequence of $\nu ~ \times ~ \nu$ matrices, $\{ { \bf V } _ { t } \} \ \sim \ \mathrm { W N } ( \mathbf { 0 } , \{ Q _ { t } \} )$ , and $\left\{ \mathbf { V } _ { t } \right\}$ is uncorrelated with $\left\{ \mathbf { W } _ { t } \right\}$ (i.e., $E ( \mathbf { W } _ { t } \mathbf { V } _ { s } ^ { \prime } ) = 0$ for all $s$ and $t$ ). To complete the specification, it is assumed that the initial state $\mathbf { X } _ { 1 }$ is uncorrelated with all of the noise terms $\{ \mathbf { V } _ { t } \}$ and $\left\{ \mathbf { W } _ { t } \right\}$ .

Remark 1. A more general form of the state-space model allows for correlation between $\mathbf { V } _ { t }$ and $\mathbf { W } _ { t }$ (see Brockwell and Davis (1991), Chapter 12) and for the addition of a control term $H _ { t } \mathbf { u } _ { t }$ in the state equation. In control theory, $H _ { t } \mathbf { u } _ { t }$ represents the effect of applying a “control” $\mathbf { u } _ { t }$ at time $t$ for the purpose of influencing $\mathbf { X } _ { t + 1 }$ . However, the system defined by (9.1.1) and (9.1.2) with $E \big ( \mathbf { W } _ { t } \mathbf { V } _ { s } ^ { \prime } \big ) = 0$ for all $s$ and $t$ will be adequate for our purposes. -

Remark 2. In many important special cases, the matrices $F _ { t } , G _ { t } , \mathcal { Q } _ { t }$ , and $R _ { t }$ will be independent of $t$ , in which case the subscripts will be suppressed. -

Remark 3. It follows from the observation equation (9.1.1) and the state equation (9.1.2) that $\mathbf { X } _ { t }$ and $\mathbf { Y } _ { t }$ have the functional forms, for $t = 2 , 3 , \dots$ ,

$$
\begin{array}{l} \mathbf {X} _ {t} = F _ {t - 1} \mathbf {X} _ {t - 1} + \mathbf {V} _ {t - 1} \\ = F _ {t - 1} \left(F _ {t - 2} \mathbf {X} _ {t - 2} + \mathbf {V} _ {t - 2}\right) + \mathbf {V} _ {t - 1} \\ \begin{array}{c} \bullet \\ \bullet \\ \bullet \end{array} \\ = \left(F _ {t - 1} \dots F _ {1}\right) \mathbf {X} _ {1} + \left(F _ {t - 1} \dots F _ {2}\right) \mathbf {V} _ {1} + \dots + F _ {t - 1} \mathbf {V} _ {t - 2} + \mathbf {V} _ {t - 1} \\ = f _ {t} \left(\mathbf {X} _ {1}, \mathbf {V} _ {1}, \dots , \mathbf {V} _ {t - 1}\right) \tag {9.1.3} \\ \end{array}
$$

and

$$
\mathbf {Y} _ {t} = g _ {t} \left(\mathbf {X} _ {1}, \mathbf {V} _ {1}, \dots , \mathbf {V} _ {t - 1}, \mathbf {W} _ {t}\right). \tag {9.1.4}
$$

Remark 4. From Remark 3 and the assumptions on the noise terms, it is clear that

$$
E \left(\mathbf {V} _ {t} \mathbf {X} _ {s} ^ {\prime}\right) = 0, \qquad E \left(\mathbf {V} _ {t} \mathbf {Y} _ {s} ^ {\prime}\right) = 0, \quad 1 \leq s \leq t,
$$

and

$$
E \left(\mathbf {W} _ {t} \mathbf {X} _ {s} ^ {\prime}\right) = 0, \quad 1 \leq s \leq t, \quad E (\mathbf {W} _ {t} \mathbf {Y} _ {s} ^ {\prime}) = 0, \quad 1 \leq s <   t.
$$

# Definition 9.1.1

A time series $\{ { \mathbf Y } _ { t } \}$ has a state-space representation if there exists a state-space model for $\{ { \mathbf Y } _ { t } \}$ as specified by equations (9.1.1) and (9.1.2).

As already indicated, it is possible to find a state-space representation for a large number of time-series (and other) models. It is clear also from the definition that neither $\{ { \mathbf X } _ { t } \}$ nor $\{ \mathbf { Y } _ { t } \}$ is necessarily stationary. The beauty of a state-space representation, when one can be found, lies in the simple structure of the state equation (9.1.2), which permits relatively simple analysis of the process $\{ { \mathbf X } _ { t } \}$ . The behavior of $\{ { \mathbf Y } _ { t } \}$ is then easy to determine from that of $\{ { \mathbf { X } } _ { t } \}$ using the observation equation (9.1.1). If the sequence $\{ \mathbf { X } _ { 1 } , \mathbf { V } _ { 1 } , \mathbf { V } _ { 2 } , \ldots \}$ is independent, then $\{ { \mathbf X } _ { t } \}$ has the Markov property; i.e., the distribution of $\mathbf { X } _ { t + 1 }$ given $\mathbf { X } _ { t } , \ldots , \mathbf { X } _ { 1 }$ is the same as the distribution of $\mathbf { X } _ { t + 1 }$ given $\mathbf { X } _ { t }$ . This is a property possessed by many physical systems, provided that we include sufficiently many components in the specification of the state $\mathbf { X } _ { t }$ (for example, we may choose the state vector in such a way that $\mathbf { X } _ { t }$ includes components of $\mathbf { X } _ { t - 1 }$ for each t).

# Example 9.1.1 An AR(1) Process

Let $\{ Y _ { t } \}$ be the causal AR(1) process given by

$$
Y _ {t} = \phi Y _ {t - 1} + Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right). \tag {9.1.5}
$$

In this case, a state-space representation for $\{ Y _ { t } \}$ is easy to construct. We can, for example, define a sequence of state variables $X _ { t }$ by

$$
X _ {t + 1} = \phi X _ {t} + V _ {t}, \quad t = 1, 2, \dots , \tag {9.1.6}
$$

where $\begin{array} { r } { X _ { 1 } = Y _ { 1 } = \sum _ { j = 0 } ^ { \infty } \phi ^ { j } Z _ { 1 - j } } \end{array}$ and $V _ { t } = Z _ { t + 1 }$ . The process $\{ Y _ { t } \}$ then satisfies the observation equation

$$
Y _ {t} = X _ {t},
$$

which has the form (9.1.1) with $G _ { t } = 1$ and $W _ { t } = 0$ .

![](images/f2613cd34c3b93b1b5dde279e13d5e14865f29c5e8de3cf199831c8a5da8f7ac.jpg)

# Example 9.1.2 An ARMA(1,1) Process

Let $\{ Y _ { t } \}$ be the causal and invertible ARMA(1,1) process satisfying the equations

$$
Y _ {t} = \phi Y _ {t - 1} + Z _ {t} + \theta Z _ {t - 1}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right). \tag {9.1.7}
$$

Although the existence of a state-space representation for $\{ Y _ { t } \}$ is not obvious, we can find one by observing that

$$
Y _ {t} = \theta (B) X _ {t} = \left[ \begin{array}{l l} \theta & 1 \end{array} \right] \left[ \begin{array}{c} X _ {t - 1} \\ X _ {t} \end{array} \right], \tag {9.1.8}
$$

where $\{ X _ { t } \}$ is the causal AR(1) process satisfying

$$
\phi (B) X _ {t} = Z _ {t},
$$

or the equivalent equation

$$
\left[ \begin{array}{l} X _ {t} \\ X _ {t + 1} \end{array} \right] = \left[ \begin{array}{l l} 0 & 1 \\ 0 & \phi \end{array} \right] \left[ \begin{array}{l} X _ {t - 1} \\ X _ {t} \end{array} \right] + \left[ \begin{array}{l} 0 \\ Z _ {t + 1}. \end{array} \right]. \tag {9.1.9}
$$

Noting that $\begin{array} { r } { X _ { t } = \sum _ { j = 0 } ^ { \infty } \phi ^ { j } Z _ { t - j } } \end{array}$ , we see that equations (9.1.8) and (9.1.9) for $t = 1 , 2 , \ldots$ furnish a state-space representation of $\{ Y _ { t } \}$ with

$$
\mathbf {X} _ {t} = \left[ \begin{array}{c} X _ {t - 1} \\ X _ {t} \end{array} \right] \text {a n d} \mathbf {X} _ {1} = \left[ \begin{array}{c} \sum_ {j = 0} ^ {\infty} \phi^ {j} Z _ {- j} \\ \sum_ {j = 0} ^ {\infty} \phi^ {j} Z _ {1 - j} \end{array} \right].
$$

The extension of this state-space representation to general ARMA and ARIMA processes is given in Section 9.3.

![](images/d724c09dd14d7101726bbd021510a9c98a7318e2d14d07c694c70aa5c0c5f6df.jpg)

In subsequent sections we shall give examples that illustrate the versatility of statespace models. (More examples can be found in Aoki 1987; Hannan and Deistler 1988; Harvey 1990; West and Harrison 1989.) Before considering these, we need a slight modification of (9.1.1) and (9.1.2), which allows for series in which the time index runs from $- \infty$ to $\infty$ . This is a more natural formulation for many time series models.

# 9.1.1 State-Space Models with $t \in \{ 0 , \pm 1 , \ldots \}$ }

Consider the observation and state equations

$$
\mathbf {Y} _ {t} = G \mathbf {X} _ {t} + \mathbf {W} _ {t}, \quad t = 0, \pm 1, \dots , \tag {9.1.10}
$$

$$
\mathbf {X} _ {t + 1} = F \mathbf {X} _ {t} + \mathbf {V} _ {t}, \quad t = 0, \pm 1, \dots , \tag {9.1.11}
$$

where $F$ and $G$ are $\nu \times \nu$ and $w \times \nu$ matrices, respectively, $\{ \mathbf { V } _ { t } \} \sim \mathrm { W N } ( \mathbf { 0 } , \boldsymbol { Q } ) , \{ \mathbf { W } _ { t } \} \sim$ ${ \bf W N } ( { \bf 0 } , R )$ , and $E ( \mathbf { V } _ { s } \mathbf { W } _ { t } ^ { \prime } ) = 0$ for all $s$ , and $t$ .

The state equation (9.1.11) is said to be stable if the matrix $F$ has all its eigenvalues in the interior of the unit circle, or equivalently if $\operatorname* { d e t } ( I - F z ) \neq 0$ for all z complex such that $| z | \leq 1$ . The matrix $F$ is then also said to be stable.

In the stable case equation (9.1.11) has the unique stationary solution (Problem 9.1) given by

$$
\mathbf {X} _ {t} = \sum_ {j = 0} ^ {\infty} F ^ {j} \mathbf {V} _ {t - j - 1}.
$$

The corresponding sequence of observations

$$
\mathbf {Y} _ {t} = \mathbf {W} _ {t} + \sum_ {j = 0} ^ {\infty} G F ^ {j} \mathbf {V} _ {t - j - 1}
$$

is also stationary.

# 9.2 The Basic Structural Model

A structural time series model, like the classical decomposition model defined by (1.5.1), is specified in terms of components such as trend, seasonality, and noise, which are of direct interest in themselves. The deterministic nature of the trend and seasonal components in the classical decomposition model, however, limits its applicability. A natural way in which to overcome this deficiency is to permit random variation in these components. This can be very conveniently done in the framework of a state-space representation, and the resulting rather flexible model is called a structural model. Estimation and forecasting with this model can be encompassed in the general procedure for state-space models made possible by the Kalman recursions of Section 9.4.

# Example 9.2.1 The Random Walk Plus Noise Model

One of the simplest structural models is obtained by adding noise to a random walk. It is suggested by the nonseasonal classical decomposition model

$$
Y _ {t} = M _ {t} + W _ {t}, \quad \text {w h e r e} \left\{W _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma_ {w} ^ {2}\right), \tag {9.2.1}
$$

and $M _ { t } ~ = ~ m _ { t }$ , the deterministic “level” or “signal” at time t. We now introduce randomness into the level by supposing that $M _ { t }$ is a random walk satisfying

$$
M _ {t + 1} = M _ {t} + V _ {t}, \quad \text {a n d} \quad \left\{V _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma_ {v} ^ {2}\right), \tag {9.2.2}
$$

with initial value $M _ { 1 } = m _ { 1 }$ . Equations (9.2.1) and (9.2.2) constitute the “local level” or “random walk plus noise” model. Figure 9-1 shows a realization of length 100 of this model with $M _ { 1 } = 0$ , $\sigma _ { \nu } ^ { 2 } = 4$ , and $\sigma _ { w } ^ { 2 } = 8$ . (The realized values $m _ { t }$ of $M _ { t }$ are plotted as a solid line, and the observed data are plotted as square boxes.) The differenced data

$$
D _ {t} := \nabla Y _ {t} = Y _ {t} - Y _ {t - 1} = V _ {t - 1} + W _ {t} - W _ {t - 1}, \quad t \geq 2,
$$

constitute a stationary time series with mean 0 and ACF

$$
\rho_ {D} (h) = \left\{ \begin{array}{l l} \frac {- \sigma_ {w} ^ {2}}{2 \sigma_ {w} ^ {2} + \sigma_ {v} ^ {2}}, & \text {i f | h | = 1}, \\ 0, & \text {i f | h | > 1}. \end{array} \right.
$$

![](images/1a9cc02c3f5a8dfbef2dc65897030dc47333ece4e5451e89930114ae77deb73c.jpg)  
Figure 9-1

Realization from a random walk plus noise model.

The random walk is represented by the solid line and the data are represented by boxes

![](images/e8c43e17f59688dc89fc6acd2f3465285c43a1587ce5b934b138889a19c2326a.jpg)  
Figure 9-2 Sample ACF of the series obtained by differencing the data in Figure 9-1

Since $\{ D _ { t } \}$ is 1-correlated, we conclude from Proposition 2.1.1 that $\{ D _ { t } \}$ is an MA(1) process and hence that $\{ Y _ { t } \}$ is an ARIMA(0,1,1) process. More specifically,

$$
D _ {t} = Z _ {t} + \theta Z _ {t - 1}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma^ {2}\right), \tag {9.2.3}
$$

where $\theta$ and $\sigma ^ { 2 }$ are found by solving the equations

$$
\frac {\theta}{1 + \theta^ {2}} = \frac {- \sigma_ {w} ^ {2}}{2 \sigma_ {w} ^ {2} + \sigma_ {v} ^ {2}} \quad \text {a n d} \quad \theta \sigma^ {2} = - \sigma_ {w} ^ {2}.
$$

For the process $\{ Y _ { t } \}$ generating the data in Figure 9-1, the parameters $\theta$ and $\sigma ^ { 2 }$ of the differenced series $\{ D _ { t } \}$ satisfy $\theta / ( 1 + \theta ^ { 2 } ) = - 0 . 4$ and $\theta \sigma ^ { 2 } = - 8$ . Solving these equations for $\theta$ and $\sigma ^ { 2 }$ , we find that $\theta = - 0 . 5$ and $\sigma ^ { 2 } = 1 6$ (or $\theta = - 2$ and $\sigma ^ { 2 } = 4$ ). The sample ACF of the observed differences $D _ { t }$ of the realization of $\{ Y _ { t } \}$ in Figure 9-1 is shown in Figure 9-2.

The local level model is often used to represent a measured characteristic of the output of an industrial process for which the unobserved process level $\{ M _ { t } \}$ is intended to be within specified limits (to meet the design specifications of the manufactured product). To decide whether or not the process requires corrective attention, it is important to be able to test the hypothesis that the process level $\{ M _ { t } \}$ is constant. From the state equation, we see that $\{ M _ { t } \}$ is constant (and equal to $m _ { 1 }$ ) when $V _ { t } ~ = ~ 0$ or equivalently when $\sigma _ { \nu } ^ { 2 } = 0$ . This in turn is equivalent to the moving-average model (9.2.3) for $\{ D _ { t } \}$ being noninvertible with $\theta = - 1$ (see Problem 8.2). Tests of the unit root hypothesis $\theta = - 1$ were discussed in Section 6.3.2.

The local level model can easily be extended to incorporate a locally linear trend with slope $\beta _ { t }$ at time t. Equation (9.2.2) is replaced by

$$
M _ {t} = M _ {t - 1} + B _ {t - 1} + V _ {t - 1}, \tag {9.2.4}
$$

where $B _ { t - 1 } = \beta _ { t - 1 }$ . Now if we introduce randomness into the slope by replacing it with the random walk

$$
B _ {t} = B _ {t - 1} + U _ {t - 1}, \quad \text {w h e r e} \left\{U _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma_ {u} ^ {2}\right), \tag {9.2.5}
$$

we obtain the “local linear trend” model.

To express the local linear trend model in state-space form we introduce the state vector

$$
\mathbf {X} _ {t} = \left(M _ {t}, B _ {t}\right) ^ {\prime}.
$$

Then (9.2.4) and (9.2.5) can be written in the equivalent form

$$
\mathbf {X} _ {t + 1} = \left[ \begin{array}{l l} 1 & 1 \\ 0 & 1 \end{array} \right] \mathbf {X} _ {t} + \mathbf {V} _ {t}, \quad t = 1, 2, \dots , \tag {9.2.6}
$$

where $\mathbf { V } _ { t } = ( V _ { t } , U _ { t } ) ^ { \prime }$ . The process $\{ Y _ { t } \}$ is then determined by the observation equation

$$
Y _ {t} = \left[ \begin{array}{l l} 1 & 0 \end{array} \right] \mathbf {X} _ {t} + W _ {t}. \tag {9.2.7}
$$

If $\{ { \bf X } _ { 1 } , U _ { 1 } , V _ { 1 } , W _ { 1 } , U _ { 2 } , V _ { 2 } , W _ { 2 } , \ldots \}$ $U _ { 2 }$ is an uncorrelated sequence, then equations (9.2.6) and (9.2.7) constitute a state-space representation of the process $\{ Y _ { t } \}$ , which is a model for data with randomly varying trend and added noise. For this model we have $\nu =$ 2, $w = 1$ ,

$$
F = \left[ \begin{array}{c c} 1 & 1 \\ 0 & 1, \end{array} \right] \quad G = [ 1 \quad 0 ], \quad Q = \left[ \begin{array}{c c} \sigma_ {v} ^ {2} & 0 \\ 0 & \sigma_ {u} ^ {2} \end{array} \right], \quad \text {a n d} R = \sigma_ {w} ^ {2}.
$$

# Example 9.2.2 A Seasonal Series with Noise

The classical decomposition (1.5.11) expressed the time series $\{ X _ { t } \}$ as a sum of trend, seasonal, and noise components. The seasonal component (with period $d$ ) was a sequence $\left\{ { { s } _ { t } } \right\}$ with the properties $s _ { t + d } ~ = ~ s _ { t }$ and $\textstyle \sum _ { t = 1 } ^ { d } s _ { t } = 0$ . Such a sequence can be generated, for any values of $s _ { 1 } , s _ { 0 } , \ldots , s _ { - d + 3 }$ $s _ { 1 }$ , by means of the recursions

$$
s _ {t + 1} = - s _ {t} - \dots - s _ {t - d + 2}, \quad t = 1, 2, \dots . \tag {9.2.8}
$$

A somewhat more general seasonal component $\{ Y _ { t } \}$ , allowing for random deviations from strict periodicity, is obtained by adding a term $S _ { t }$ to the right side of (9.2.8), where $\{ V _ { t } \}$ is white noise with mean zero. This leads to the recursion relations

$$
Y _ {t + 1} = - Y _ {t} - \dots - Y _ {t - d + 2} + S _ {t}, \quad t = 1, 2, \dots . \tag {9.2.9}
$$

To find a state-space representation for $\{ Y _ { t } \}$ we introduce the $( d - 1 )$ -dimensional state vector

$$
\mathbf {X} _ {t} = \left(Y _ {t}, Y _ {t - 1}, \dots , Y _ {t - d + 2}\right) ^ {\prime}.
$$

The series $\{ Y _ { t } \}$ is then given by the observation equation

$$
Y _ {t} = \left[ \begin{array}{l l l l l} 1 & 0 & 0 & \dots & 0 \end{array} \right] \mathbf {X} _ {t}, \quad t = 1, 2, \dots , \tag {9.2.10}
$$

where $\{ { \mathbf X } _ { t } \}$ satisfies the state equation

$$
\mathbf {X} _ {t + 1} = F \mathbf {X} _ {t} + \mathbf {V} _ {t}, \quad t = 1, 2 \dots , \tag {9.2.11}
$$

$\mathbf { V } _ { t } = ( S _ { t } , 0 , \ldots , 0 ) ^ { \prime }$ , and

$$
F = \left[ \begin{array}{c c c c c} - 1 & - 1 & \dots & - 1 & - 1 \\ 1 & 0 & \dots & 0 & 0 \\ 0 & 1 & \dots & 0 & 0 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & 1 & 0 \end{array} \right]. \tag {9.2.12}
$$

Example 9.2.3 A Randomly Varying Trend with Random Seasonality and Noise

A series with randomly varying trend, random seasonality and noise can be constructed by adding the two series in Examples 9.2.1 and 9.2.2. (Addition of series with statespace representations is in fact always possible by means of the following construction. See Problem 9.9.) We introduce the state vector

$$
\mathbf {X} _ {t} = \left[ \begin{array}{c} \mathbf {X} _ {t} ^ {1} \\ \mathbf {X} _ {t} ^ {2} \end{array} \right],
$$

where $\mathbf { X } _ { t } ^ { 1 }$ and $\mathbf { X } _ { t } ^ { 2 }$ are the state vectors in (9.2.6) and (9.2.11). We then have the following representation for $\{ Y _ { t } \}$ , the sum of the two series whose state-space representations were given in (9.2.6)–(9.2.7) and (9.2.10)–(9.2.11). The state equation is

$$
\mathbf {X} _ {t + 1} = \left[ \begin{array}{l l} F _ {1} & 0 \\ 0 & F _ {2} \end{array} \right] \mathbf {X} _ {t} + \left[ \begin{array}{l} \mathbf {V} _ {t} ^ {1} \\ \mathbf {V} _ {t} ^ {2} \end{array} \right], \tag {9.2.13}
$$

where $F _ { 1 } , F _ { 2 }$ are the coefficient matrices and {V1t }, $\{ \mathbf { V } _ { t } ^ { 2 } \}$ are the noise vectors in the state equations (9.2.6) and (9.2.11), respectively. The observation equation is

$$
Y _ {t} = \left[ \begin{array}{l l l l l l} 1 & 0 & 1 & 0 & \dots & 0 \end{array} \right] \mathbf {X} _ {t} + W _ {t}, \tag {9.2.14}
$$

where $\{ W _ { t } \}$ is the noise sequence in (9.2.7). If the sequence of random vectors $\{ \mathbf { X } _ { 1 } , \mathbf { V } _ { 1 } ^ { 1 } , \mathbf { V } _ { 1 } ^ { 2 } , W _ { 1 } , \mathbf { V } _ { 2 } ^ { 1 } , \mathbf { V } _ { 2 } ^ { 2 } , W _ { 2 } , \ldots \}$ is uncorrelated, then equations (9.2.13) and (9.2.14) constitute a state-space representation for $\{ Y _ { t } \}$ .

![](images/ea4431fa58c29b080d3b9c478c2c918db4e956096f28e1da5c40186951c6dba1.jpg)

# 9.3 State-Space Representation of ARIMA Models

We begin by establishing a state-space representation for the causal $\operatorname { A R } ( p )$ process and then build on this example to find representations for the general ARMA and ARIMA processes.

Example 9.3.1 State-Space Representation of a Causal $\operatorname { A R } ( p )$ Process

Consider the $\operatorname { A R } ( p )$ process defined by

$$
Y _ {t + 1} = \phi_ {1} Y _ {t} + \phi_ {2} Y _ {t - 1} + \dots + \phi_ {p} Y _ {t - p + 1} + Z _ {t + 1}, \quad t = 0, \pm 1, \dots , \tag {9.3.1}
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \big ( 0 , \sigma ^ { 2 } \big )$ , and $\phi ( z ) : = 1 - \phi _ { 1 } z - \cdot \cdot \cdot - \phi _ { p } z ^ { p }$ is nonzero for $| z | \leq 1$ . To express $\{ Y _ { t } \}$ in state-space form we simply introduce the state vectors

$$
\mathbf {X} _ {t} = \left[ \begin{array}{c} Y _ {t - p + 1} \\ Y _ {t - p + 2} \\ \vdots \\ Y _ {t}, \end{array} \right], \quad t = 0, \pm 1, \dots . \tag {9.3.2}
$$

From (9.3.1) and (9.3.2) the observation equation is

$$
Y _ {t} = \left[ \begin{array}{l l l l l} 0 & 0 & 0 & \dots & 1 \end{array} \right] \mathbf {X} _ {t}, \quad t = 0, \pm 1, \dots , \tag {9.3.3}
$$

while the state equation is given by

$$
\mathbf {X} _ {t + 1} = \left[ \begin{array}{c c c c c} 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 1 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & 1 \\ \phi_ {p} & \phi_ {p - 1} & \phi_ {p - 2} & \dots & \phi_ {1} \end{array} \right] \mathbf {X} _ {t} + \left[ \begin{array}{l} 0 \\ 0 \\ \vdots \\ 0 \\ 1 \end{array} \right] Z _ {t + 1}, \quad t = 0, \pm 1, \dots . \tag {9.3.4}
$$

These equations have the required forms (9.1.10) and (9.1.11) with $\mathbf { W } _ { t } = \mathbf { 0 }$ and $\mathbf { V } _ { t } =$ $( 0 , 0 , \ldots , Z _ { t + 1 } ) ^ { \prime }$ , $t = 0$ , ±1, . . . .

![](images/3d1a50773273b9a553172c4737fe7ddb135d91f2016b1b458178a5d709593811.jpg)

Remark 1. In Example 9.3.1 the causality condition $\phi ( z ) \neq 0 \mathrm { f o r } | z | \leq 1$ is equivalent to the condition that the state equation (9.3.4) is stable, since the eigenvalues of the coefficient matrix in (9.3.4) are simply the reciprocals of the zeros of $\phi ( z )$ (Problem 9.3). -

Remark 2. If equations (9.3.3) and (9.3.4) are postulated to hold only for $t \_ =$ 1, 2, . . . , and if $\mathbf { X } _ { 1 }$ is a random vector such that $\{ \mathbf { X } _ { 1 } , Z _ { 1 } , Z _ { 2 } , . . . \}$ is an uncorrelated sequence, then we have a state-space representation for $\{ Y _ { t } \}$ of the type defined earlier by (9.1.1) and (9.1.2). The resulting process $\{ Y _ { t } \}$ is well-defined, regardless of whether or not the state equation is stable, but it will not in general be stationary. It will be stationary if the state equation is stable and if $\mathbf { X } _ { 1 }$ is defined by (9.3.2) with $\begin{array} { r } { Y _ { t } = \sum _ { j = 0 } ^ { \infty } \psi _ { j } Z _ { t - j } } \end{array}$ , $t = 1 , 0 , \ldots , 2 - p$ , and $\psi ( z ) = 1 / \phi ( z )$ , $| z | \leq 1$ . -

# Example 9.3.2 State-Space Form of a Causal ARMA $( p , q )$ Process

State-space representations are not unique. Here we shall give one of the (infinitely many) possible representations of a causal ARMA(p,q) process that can easily be derived from Example 9.3.1. Consider the ARMA $_ { ( p , q ) }$ process defined by

$$
\phi (B) Y _ {t} = \theta (B) Z _ {t}, \quad t = 0, \pm 1, \dots , \tag {9.3.5}
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \big ( 0 , \sigma ^ { 2 } \big )$ and $\phi ( z ) \neq 0$ for $| z | \leq 1$ . Let

$$
r = \max  (p, q + 1), \quad \phi_ {j} = 0 \quad \text {f o r} j > p, \quad \theta_ {j} = 0 \quad \text {f o r} j > q, \quad \text {a n d} \quad \theta_ {0} = 1.
$$

If $\{ U _ { t } \}$ is the causal $\operatorname { A R } ( p )$ process satisfying

$$
\phi (B) U _ {t} = Z _ {t}, \tag {9.3.6}
$$

then $Y _ { t } = \theta ( B ) U _ { t }$ , since

$$
\phi (B) Y _ {t} = \phi (B) \theta (B) U _ {t} = \theta (B) \phi (B) U _ {t} = \theta (B) Z _ {t}.
$$

Consequently,

$$
Y _ {t} = \left[ \begin{array}{l l l l} \theta_ {r - 1} & \theta_ {r - 2} & \dots & \theta_ {0} \end{array} \right] \mathbf {X} _ {t}, \tag {9.3.7}
$$

where

$$
\mathbf {X} _ {t} = \left[ \begin{array}{c} U _ {t - r + 1} \\ U _ {t - r + 2} \\ \vdots \\ U _ {t} \end{array} \right]. \tag {9.3.8}
$$

But from Example 9.3.1 we can write

$$
\mathbf {X} _ {t + 1} = \left[ \begin{array}{c c c c c} 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 1 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & 1 \\ \phi_ {r} & \phi_ {r - 1} & \phi_ {r - 2} & \dots & \phi_ {1} \end{array} \right] \mathbf {X} _ {t} + \left[ \begin{array}{l} 0 \\ 0 \\ \vdots \\ 0 \\ 1 \end{array} \right] Z _ {t + 1}, \quad t = 0, \pm 1, \dots . \tag {9.3.9}
$$

Equations (9.3.7) and (9.3.9) are the required observation and state equations. As in Example 9.3.1, the observation and state noise vectors are again $\mathbf { W } _ { t } = \mathbf { 0 }$ and $\mathbf { V } _ { t } =$ $( 0 , 0 , \ldots , Z _ { t + 1 } ) ^ { \prime }$ , t  0, 1, ….

![](images/cbb46441a1d90e284cf2f0e7de4d746792cd9e597d08daee9ce3a822f98432c5.jpg)

Example 9.3.3 State-Space Representation of an ARIMA $( p , d , q )$ Process

If $\left\{ Y _ { t } \right\}$ is an $\mathbf { A R I M A } ( p , d , q )$ process with $\{ \nabla ^ { d } Y _ { t } \}$ satisfying (9.3.5), then by the preceding example $\{ \nabla ^ { d } Y _ { t } \}$ has the representation

$$
\nabla^ {d} Y _ {t} = G \mathbf {X} _ {t}, \quad t = 0, \pm 1, \dots , \tag {9.3.10}
$$

where $\{ { \mathbf X } _ { t } \}$ is the unique stationary solution of the state equation

$$
\mathbf {X} _ {t + 1} = F \mathbf {X} _ {t} + \mathbf {V} _ {t},
$$

$F$ and $G$ are the coefficients of $\mathbf { X } _ { t }$ in (9.3.9) and (9.3.7), respectively, and $\begin{array} { r l } { \mathbf { V } _ { t } } & { { } = } \end{array}$ $( 0 , 0 , \ldots , Z _ { t + 1 } ) ^ { \prime }$ . Let $A$ and $B$ be the $d \times 1$ and $d \times d$ matrices defined by $A = B = 1$ if $d = 1$ and

$$
A = \left[ \begin{array}{l} 0 \\ 0 \\ \vdots \\ 0 \\ 1 \end{array} \right], \quad B = \left[ \begin{array}{l l l l l} 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 1 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & 1 \\ (- 1) ^ {d + 1} {\binom {d} {d}} & (- 1) ^ {d} {\binom {d} {d - 1}} & (- 1) ^ {d - 1} {\binom {d} {d - 2}} & \dots & d \end{array} \right]
$$

if $d > 1$ . Then since

$$
Y _ {t} = \nabla^ {d} Y _ {t} - \sum_ {j = 1} ^ {d} \binom {d} {j} (- 1) ^ {j} Y _ {t - j}, \tag {9.3.11}
$$

the vector

$$
\mathbf {Y} _ {t - 1} := \left(Y _ {t - d}, \dots , Y _ {t - 1}\right) ^ {\prime}
$$

satisfies the equation

$$
\mathbf {Y} _ {t} = A \nabla^ {d} Y _ {t} + B \mathbf {Y} _ {t - 1} = A G \mathbf {X} _ {t} + B \mathbf {Y} _ {t - 1}.
$$

Defining a new state vector $\mathbf { T } _ { t }$ by stacking $\mathbf { X } _ { t }$ and $\mathbf { Y } _ { t - 1 }$ , we therefore obtain the state equation

$$
\mathbf {T} _ {t + 1} := \left[ \begin{array}{l} \mathbf {X} _ {t + 1} \\ \mathbf {Y} _ {t} \end{array} \right] = \left[ \begin{array}{l l} F & 0 \\ A G B \end{array} \right] \mathbf {T} _ {t} + \left[ \begin{array}{l} \mathbf {V} _ {t} \\ \mathbf {0} \end{array} \right], \quad t = 1, 2, \dots , \tag {9.3.12}
$$

and the observation equation, from (9.3.10) and (9.3.11),

$$
\begin{array}{l} Y _ {t} = \left[ G (- 1) ^ {d + 1} \binom {d} {d} (- 1) ^ {d} \binom {d} {d - 1} (- 1) ^ {d - 1} \binom {d} {d - 2} \dots d \right] \quad \left[ \begin{array}{c} \mathbf {X} _ {t} \\ \mathbf {Y} _ {t - 1} \end{array} \right], \\ t = 1, 2, \dots , \tag {9.3.13} \\ \end{array}
$$

with initial condition

$$
\mathbf {T} _ {1} = \left[ \begin{array}{c} \mathbf {X} _ {1} \\ \mathbf {Y} _ {0} \end{array} \right] = \left[ \begin{array}{c} \sum_ {j = 0} ^ {\infty} F ^ {j} \mathbf {V} _ {- j} \\ \mathbf {Y} _ {0} \end{array} \right], \tag {9.3.14}
$$

and the assumption

$$
E \left(\mathbf {Y} _ {0} Z _ {t} ^ {\prime}\right) = 0, \quad t = 0, \pm 1, \dots , \tag {9.3.15}
$$

where $\mathbf { Y } _ { 0 } ~ = ~ ( Y _ { 1 - d } , Y _ { 2 - d } , \ldots , Y _ { 0 } ) ^ { \prime }$ . The conditions (9.3.15), which are satisfied in particular if ${ \bf Y } _ { 0 }$ is considered to be nonrandom and equal to the vector of observed values $( y _ { 1 - d } , y _ { 2 - d } , . . . , y _ { 0 } ) ^ { \prime }$ , are imposed to ensure that the assumptions of a statespace model given in Section 9.1 are satisfied. They also imply that $E \left( \mathbf { X } _ { 1 } \mathbf { Y } _ { 0 } ^ { \prime } \right) = 0$ and $E ( \mathbf { Y } _ { 0 } \nabla ^ { d } Y _ { t } ^ { \prime } ) = 0$ , $t \geq 1$ , as required earlier in Section 6.4 for prediction of ARIMA processes.

State-space models for more general ARIMA processes (e.g., $\{ Y _ { t } \}$ such that $\{ \nabla \nabla _ { 1 2 } Y _ { t } \}$ is an ARMA $( p , q )$ process) can be constructed in the same way. See Problem 9.4.

![](images/42d21b2239415038330f728279fecd640ac9617be543c79ea815212684c8b114.jpg)

For the ARIMA(1, 1, 1) process defined by

$$
(1 - \phi B) (1 - B) Y _ {t} = (1 + \theta B) Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right),
$$

the vectors $\mathbf { X } _ { t }$ and $\mathbf { Y } _ { t - 1 }$ reduce to $\mathbf { X } _ { t } = ( X _ { t - 1 } , X _ { t } ) ^ { \prime }$ and $\mathbf { Y } _ { t - 1 } = Y _ { t - 1 }$ . From (9.3.12) and (9.3.13) the state-space representation is therefore (Problem 9.8)

$$
Y _ {t} = \left[ \theta 1 1 \right] \left[ \begin{array}{c} X _ {t - 1} \\ X _ {t} \\ Y _ {t - 1} \end{array} \right], \tag {9.3.16}
$$

where

$$
\left[ \begin{array}{l} X _ {t} \\ X _ {t + 1} \\ Y _ {t} \end{array} \right] = \left[ \begin{array}{c c c} 0 & 1 & 0 \\ 0 & \phi & 0 \\ \theta & 1 & 1 \end{array} \right] \left[ \begin{array}{l} X _ {t - 1} \\ X _ {t} \\ Y _ {t - 1} \end{array} \right] + \left[ \begin{array}{l} 0 \\ Z _ {t + 1} \\ 0 \end{array} \right], \quad t = 1, 2, \dots , \tag {9.3.17}
$$

and

$$
\left[ \begin{array}{c} X _ {0} \\ X _ {1} \\ Y _ {0} \end{array} \right] = \left[ \begin{array}{c} \sum_ {j = 0} ^ {\infty} \phi^ {j} Z _ {- j} \\ \sum_ {j = 0} ^ {\infty} \phi^ {j} Z _ {1 - j} \\ Y _ {0} \end{array} \right]. \tag {9.3.18}
$$

# 9.4 The Kalman Recursions

In this section we shall consider three fundamental problems associated with the statespace model defined by (9.1.1) and (9.1.2) in Section 9.1. These are all concerned with finding best (in the sense of minimum mean square error) linear estimates of the statevector $\mathbf { X } _ { t }$ in terms of the observations $\mathbf { Y } _ { 1 } , \mathbf { Y } _ { 2 } , \ldots$ , and a random vector ${ \bf Y } _ { 0 }$ that is orthogonal to $\mathbf { V } _ { t }$ and $\mathbf { W } _ { t }$ for all $t \geq 1$ . In many cases ${ \bf Y } _ { 0 }$ will be the constant vector $( 1 , 1 , \ldots , 1 ) ^ { \prime }$ . Estimation of $\mathbf { X } _ { t }$ in terms of:

a. $\mathbf { Y } _ { 0 } , \ldots , \mathbf { Y } _ { t - 1 }$ defines the prediction problem,   
b. $\mathbf { Y } _ { 0 } , \ldots , \mathbf { Y } _ { t }$ defines the filtering problem,   
c. $\mathbf { Y } _ { 0 } , \ldots , \mathbf { Y } _ { n }$ $( n > t )$ ) defines the smoothing problem.

Each of these problems can be solved recursively using an appropriate set of Kalman recursions, which will be established in this section.

In the following definition of best linear predictor (and throughout this chapter) it should be noted that we do not automatically include the constant 1 among the predictor variables as we did in Sections 2.5 and 8.5. (It can, however, be included by choosing $\mathbf { Y } _ { 0 } = ( 1 , 1 , \ldots , 1 ) ^ { \prime } .$ .)

# Definition 9.4.1

For the random vector $\mathbf { X } = ( X _ { 1 } , \ldots , X _ { \nu } ) ^ { \prime }$ ,

$$
P _ {t} (\mathbf {X}) := \left(P _ {t} \left(X _ {1}\right), \dots , P _ {t} \left(X _ {v}\right)\right) ^ {\prime},
$$

where $P _ { t } ( X _ { i } ) : = P ( X _ { i } | \mathbf { Y } _ { 0 } , \mathbf { Y } _ { 1 } , \ldots , \mathbf { Y } _ { t } )$ , is the best linear predictor of $X _ { i }$ in terms of all components of $\mathbf { Y } _ { 0 } , \mathbf { Y } _ { 1 } , \ldots , \mathbf { Y } _ { t }$ .

Remark 1. By the definition of the best predictor of each component $X _ { i }$ of $\mathbf { X }$ , $P _ { t } ( \mathbf { X } )$ is the unique random vector of the form

$$
P _ {t} (\mathbf {X}) = A _ {0} \mathbf {Y} _ {0} + \dots + A _ {t} \mathbf {Y} _ {t}
$$

with $\nu \times w$ matrices $A _ { 0 } , \ldots , A _ { t }$ such that

$$
[ \mathbf {X} - P _ {t} (\mathbf {X}) ] \perp \mathbf {Y} _ {s}, \quad s = 0, \dots , t
$$

[cf. (8.5.2) and (8.5.3)]. Recall that two random vectors $\mathbf { X }$ and $\mathbf { Y }$ are orthogonal (written $\mathbf { X } \perp \mathbf { Y } )$ if $E ( \mathbf { X Y ^ { \prime } } )$ is a matrix of zeros. -

Remark 2. If all the components of $\mathbf { X } , \mathbf { Y } _ { 1 } , \ldots , \mathbf { Y } _ { t }$ are jointly normally distributed and $\mathbf { Y } _ { 0 } = ( 1 , \ldots , 1 ) ^ { \prime }$ , then

$$
P _ {t} (\mathbf {X}) = E (\mathbf {X} | \mathbf {Y} _ {1}, \dots , \mathbf {Y} _ {t}), \quad t \geq 1.
$$

Remark 3. $P _ { t }$ is linear in the sense that if $A$ is any $k \times \nu$ matrix and $\mathbf { X } , \mathbf { V }$ are two $\nu$ -variate random vectors with finite second moments, then (Problem 9.10)

$P _ { t } ( A \mathbf { X } ) = A P _ { t } ( \mathbf { X } )$

and

$$
P _ {t} (\mathbf {X} + \mathbf {V}) = P _ {t} (\mathbf {X}) + P _ {t} (\mathbf {V}).
$$

-

Remark 4. If $\mathbf { X }$ and $\mathbf { Y }$ are random vectors with $\nu$ and $w$ components, respectively, each with finite second moments, then

$$
P (\mathbf {X} | \mathbf {Y}) = M \mathbf {Y},
$$

where $M$ is a $\nu \times w$ matrix, $M = E ( \mathbf { X Y ^ { \prime } } ) [ E ( \mathbf { Y Y ^ { \prime } } ) ] ^ { - 1 }$ with $[ E ( \mathbf { Y } \mathbf { Y } ^ { \prime } ) ] ^ { - 1 }$ any generalized inverse of $E ( \mathbf { Y } \mathbf { Y } ^ { \prime } )$ . (A generalized inverse of a matrix $S$ is a matrix $S ^ { - 1 }$ such that $S S ^ { - 1 } S = S$ . Every matrix has at least one. See Problem 9.11.)

In the notation just developed, the prediction, filtering, and smoothing problems (a), (b), and (c) formulated above reduce to the determination of $P _ { t - 1 } ( \mathbf { X } _ { t } ) , P _ { t } ( \mathbf { X } _ { t } )$ , and $P _ { n } ( \mathbf { X } _ { t } )$ $( n > t )$ , respectively. We deal first with the prediction problem. -

# Kalman Prediction:

For the state-space model (9.1.1)–(9.1.2), the one-step predictors $\hat { \mathbf { X } } _ { t } : = P _ { t - 1 } ( \mathbf { X } _ { t } )$ and their error covariance matrices $\Omega _ { t } = E \big [ \big ( \mathbf { X } _ { t } - \hat { \mathbf { X } } _ { t } \big ) \big ( \mathbf { X } _ { t } - \hat { \mathbf { X } } _ { t } \big ) ^ { \prime } \big ]$ − are uniquely determined by the initial conditions

$$
\hat {\mathbf {X}} _ {1} = P \left(\mathbf {X} _ {1} \mid \mathbf {Y} _ {0}\right), \quad \Omega_ {1} = E \left[ \left(\mathbf {X} _ {1} - \hat {\mathbf {X}} _ {1}\right) \left(\mathbf {X} _ {1} - \hat {\mathbf {X}} _ {1}\right) ^ {\prime} \right]
$$

and the recursions, for $t = 1 , \ldots$ ,

$$
\hat {\mathbf {X}} _ {t + 1} = F _ {t} \hat {\mathbf {X}} _ {t} + \Theta_ {t} \Delta_ {t} ^ {- 1} \left(\mathbf {Y} _ {t} - G _ {t} \hat {\mathbf {X}} _ {t}\right), \tag {9.4.1}
$$

$$
\Omega_ {t + 1} = F _ {t} \Omega_ {t} F _ {t} ^ {\prime} + Q _ {t} - \Theta_ {t} \Delta_ {t} ^ {- 1} \Theta_ {t} ^ {\prime}, \tag {9.4.2}
$$

where

$$
\varDelta_ {t} = G _ {t} \varOmega_ {t} G _ {t} ^ {\prime} + R _ {t},
$$

$$
\Theta_ {t} = F _ {t} \Omega_ {t} G _ {t} ^ {\prime},
$$

and $\varDelta _ { t } ^ { - 1 }$ is any generalized inverse of $\varDelta _ { t }$ .

Proof. We shall make use of the innovations $\mathbf { I } _ { t }$ defined by $\mathbf { I } _ { 0 } = \mathbf { Y } _ { 0 }$ and

$$
\mathbf {I} _ {t} = \mathbf {Y} _ {t} - P _ {t - 1} \mathbf {Y} _ {t} = \mathbf {Y} _ {t} - G _ {t} \hat {\mathbf {X}} _ {t} = G _ {t} \left(\mathbf {X} _ {t} - \hat {\mathbf {X}} _ {t}\right) + \mathbf {W} _ {t}, \quad t = 1, 2, \dots .
$$

The sequence $\left\{ \mathbf { I } _ { t } \right\}$ is orthogonal by Remark 1. Using Remarks 3 and 4 and the relation

$$
P _ {t} (\cdot) = P _ {t - 1} (\cdot) + P (\cdot | \mathbf {I} _ {t}) \tag {9.4.3}
$$

(see Problem 9.12), we find that

$$
\begin{array}{l} \hat {\mathbf {X}} _ {t + 1} = P _ {t - 1} (\mathbf {X} _ {t + 1}) + P (\mathbf {X} _ {t + 1} | \mathbf {I} _ {t}) = P _ {t - 1} (F _ {t} \mathbf {X} _ {t} + \mathbf {V} _ {t}) + \Theta_ {t} \Delta_ {t} ^ {- 1} \mathbf {I} _ {t} \\ = F _ {t} \hat {\mathbf {X}} _ {t} + \Theta_ {t} \Delta_ {t} ^ {- 1} \mathbf {I} _ {t}, \tag {9.4.4} \\ \end{array}
$$

where

$$
\Delta_ {t} = E \left(\mathbf {I} _ {t} \mathbf {I} _ {t} ^ {\prime}\right) = G _ {t} \Omega_ {t} G _ {t} ^ {\prime} + R _ {t},
$$

$$
\Theta_ {t} = E (\mathbf {X} _ {t + 1} \mathbf {I} _ {t} ^ {\prime}) = E \left[ \left(F _ {t} \mathbf {X} _ {t} + \mathbf {V} _ {t}\right) \left(\left[ \mathbf {X} _ {t} - \hat {\mathbf {X}} _ {t} \right] ^ {\prime} G _ {t} ^ {\prime} + \mathbf {W} _ {t} ^ {\prime}\right) \right] = F _ {t} \Omega_ {t} G _ {t} ^ {\prime}.
$$

To verify (9.4.2), we observe from the definition of $\boldsymbol { \varOmega } _ { t + 1 }$ that

$$
\Omega_ {t + 1} = E \left(\mathbf {X} _ {t + 1} \mathbf {X} _ {t + 1} ^ {\prime}\right) - E \left(\hat {\mathbf {X}} _ {t + 1} \hat {\mathbf {X}} _ {t + 1} ^ {\prime}\right).
$$

With (9.1.2) and (9.4.4) this gives

$$
\begin{array}{l} \varOmega_ {t + 1} = F _ {t} E (\mathbf {X} _ {t} \mathbf {X} _ {t} ^ {\prime}) F _ {t} ^ {\prime} + Q _ {t} - F _ {t} E (\hat {\mathbf {X}} _ {t} \hat {\mathbf {X}} _ {t} ^ {\prime}) F _ {t} ^ {\prime} - \Theta_ {t} \varDelta_ {t} ^ {- 1} \Theta_ {t} ^ {\prime} \\ = F _ {t} \Omega_ {t} F _ {t} ^ {\prime} + Q _ {t} - \Theta_ {t} \varDelta_ {t} ^ {- 1} \Theta_ {t} ^ {\prime}. \\ \end{array}
$$

# 9.4.1 h-Step Prediction of $\{ \pmb { \Upsilon } _ { t } \}$ Using the Kalman Recursions

The Kalman prediction equations lead to a very simple algorithm for recursive calculation of the best linear mean square predictors $P _ { t } \mathbf { Y } _ { t + h } , h = 1 , 2 , \ldots .$ $P _ { t } \mathbf { Y } _ { t + h }$ From (9.4.4), (9.1.1), (9.1.2), and Remark 3 in Section 9.1, we find that

$$
\begin{array}{l} P _ {t} \mathbf {X} _ {t + 1} = F _ {t} P _ {t - 1} \mathbf {X} _ {t} + \Theta_ {t} \Delta_ {t} ^ {- 1} \left(\mathbf {Y} _ {t} - P _ {t - 1} \mathbf {Y} _ {t}\right), (9.4.5) \\ P _ {t} \mathbf {X} _ {t + h} = F _ {t + h - 1} P _ {t} \mathbf {X} _ {t + h - 1} \\ \begin{array}{c} \vdots \\ \vdots \\ \vdots \end{array} \\ = \left(F _ {t + h - 1} F _ {t + h - 2} \dots F _ {t + 1}\right) P _ {t} \mathbf {X} _ {t + 1}, \quad h = 2, 3, \dots , (9.4.6) \\ \end{array}
$$

and

$$
P _ {t} \mathbf {Y} _ {t + h} = G _ {t + h} P _ {t} \mathbf {X} _ {t + h}, \quad h = 1, 2, \dots . \tag {9.4.7}
$$

From the relation

$$
\mathbf {X} _ {t + h} - P _ {t} \mathbf {X} _ {t + h} = F _ {t + h - 1} \left(\mathbf {X} _ {t + h - 1} - P _ {t} \mathbf {X} _ {t + h - 1}\right) + \mathbf {V} _ {t + h - 1}, \quad h = 2, 3, \dots ,
$$

we find that $\Omega _ { t } ^ { ( h ) } : = E [ ( \mathbf { X } _ { t + h } - P _ { t } \mathbf { X } _ { t + h } ) ( \mathbf { X } _ { t + h } - P _ { t } \mathbf { X } _ { t + h } ) ^ { \prime } ]$ satisfies the recursions

$$
\Omega_ {t} ^ {(h)} = F _ {t + h - 1} \Omega_ {t} ^ {(h - 1)} F _ {t + h - 1} ^ {\prime} + Q _ {t + h - 1}, \quad h = 2, 3, \dots , \tag {9.4.8}
$$

with $\boldsymbol { \Omega } _ { t } ^ { ( 1 ) } = \boldsymbol { \Omega } _ { t + 1 }$ . Then from (9.1.1) and (9.4.7), $\boldsymbol { \varDelta } _ { t } ^ { ( h ) } : = E [ ( \mathbf { Y } _ { t + h } - P _ { t } \mathbf { Y } _ { t + h } ) ( \mathbf { Y } _ { t + h } -$ $P _ { t } \mathbf { Y } _ { t + h } ) ^ { \prime } ]$ is given by

$$
\Delta_ {t} ^ {(h)} = G _ {t + h} \Omega_ {t} ^ {(h)} G _ {t + h} ^ {\prime} + R _ {t + h}, \quad h = 1, 2, \dots . \tag {9.4.9}
$$

Example 9.4.1. Consider the random walk plus noise model of Example 9.2.1 defined by

$$
Y _ {t} = X _ {t} + W _ {t}, \quad \{W _ {t} \} \sim \mathrm {W N} \left(0, \sigma_ {w} ^ {2}\right),
$$

where the local level $X _ { t }$ follows the random walk

$$
X _ {t + 1} = X _ {t} + V _ {t}, \quad \{V _ {t} \} \sim \mathrm {W N} \left(0, \sigma_ {\nu} ^ {2}\right).
$$

Applying the Kalman prediction equations with $Y _ { 0 } : = 1 , R = \sigma _ { w } ^ { 2 }$ , and $Q = \sigma _ { \nu } ^ { 2 }$ , we obtain

$$
\begin{array}{l} \hat {Y} _ {t + 1} = P _ {t} Y _ {t + 1} = \hat {X} _ {t} + \frac {\Theta_ {t}}{\varDelta_ {t}} \left(Y _ {t} - \hat {Y} _ {t}\right) \\ = (1 - a _ {t}) \hat {Y} _ {t} + a _ {t} Y _ {t} \\ \end{array}
$$

where

$$
a _ {t} = \frac {\Theta_ {t}}{\varDelta_ {t}} = \frac {\varOmega_ {t}}{\varOmega_ {t} + \sigma_ {w} ^ {2}}.
$$

For a state-space model (like this one) with time-independent parameters, the solution of the Kalman recursions (9.4.2) is called a steady-state solution if $\varOmega _ { t }$ is independent of t. If $\boldsymbol { \varOmega } _ { t } = \boldsymbol { \varOmega }$ for all $t$ , then from (9.4.2)

$$
\Omega_ {t + 1} = \Omega = \Omega + \sigma_ {v} ^ {2} - \frac {\Omega^ {2}}{\Omega + \sigma_ {w} ^ {2}} = \frac {\Omega \sigma_ {w} ^ {2}}{\Omega + \sigma_ {w} ^ {2}} + \sigma_ {v} ^ {2}.
$$

Solving this quadratic equation for $\varOmega$ and noting that $\varOmega \geq 0$ , we find that

$$
\Omega = \frac {1}{2} \left(\sigma_ {v} ^ {2} + \sqrt {\sigma_ {v} ^ {4} + 4 \sigma_ {v} ^ {2} \sigma_ {w} ^ {2}}\right)
$$

Since $\Omega _ { t + 1 } - \Omega _ { t }$ is a continuous function of $\varOmega _ { t }$ on $\varOmega _ { t } \ \geq \ 0$ , positive at $\varOmega _ { t } \ = \ 0$ , negative for large $\varOmega _ { t }$ , and zero only at $\boldsymbol { \varOmega } _ { t } = \boldsymbol { \varOmega }$ , it is clear that $\Omega _ { t + 1 } - \Omega _ { t }$ is negative for $\varOmega _ { t } > \varOmega$ and positive for $\varOmega _ { t } < \varOmega$ . A similar argument shows (Problem 9.14) that $( \Omega _ { t + 1 } - \Omega ) ( \Omega _ { t } - \Omega ) \geq 0$ for all $\boldsymbol { \varOmega } _ { t } \geq 0$ . These observations imply that $\Omega _ { t + 1 }$ always falls between $\varOmega$ and $\varOmega _ { t }$ . Consequently, regardless of the value of $\varOmega _ { 1 }$ , $\varOmega _ { t }$ converges to $\varOmega$ , the unique solution of $\varOmega _ { t + 1 } = \varOmega _ { t }$ . For any initial predictors $\hat { Y } _ { 1 } = \hat { X } _ { 1 }$ and any initial mean squared error $\Omega _ { 1 } = E { \left( X _ { 1 } - \hat { X } _ { 1 } \right) } ^ { 2 }$ , the coefficients $a _ { t } : = \varOmega _ { t } / \left( \varOmega _ { t } + \sigma _ { w } ^ { 2 } \right)$ converge to

$$
a = \frac {\Omega}{\Omega + \sigma_ {w} ^ {2}},
$$

and the mean squared errors of the predictors defined by

$$
\hat {Y} _ {t + 1} = \left(1 - a _ {t}\right) \hat {Y} _ {t} + a _ {t} Y _ {t}
$$

converge to $\varOmega + \sigma _ { w } ^ { 2 }$

If, as is often the case, we do not know $\varOmega _ { 1 }$ , then we cannot determine the sequence $\{ a _ { t } \}$ . It is natural, therefore, to consider the behavior of the predictors defined by

$$
\hat {Y} _ {t + 1} = (1 - a) \hat {Y} _ {t} + a Y _ {t}
$$

with $a$ as above and arbitrary $\hat { Y } _ { 1 }$ . It can be shown (Problem 9.16) that this sequence of predictors is also asymptotically optimal in the sense that the mean squared error converges to $\varOmega + \sigma _ { w } ^ { 2 }$ as $t \to \infty$ .

As shown in Example 9.2.1, the differenced process $D _ { t } = Y _ { t } - Y _ { t - 1 }$ is the MA(1) process

$$
D _ {t} = Z _ {t} + \theta Z _ {t - 1}, \left\{Z _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma^ {2}\right),
$$

where $\theta / \left( 1 + \theta ^ { 2 } \right) = - \sigma _ { w } ^ { 2 } / \left( 2 \sigma _ { w } ^ { 2 } + \sigma _ { \nu } ^ { 2 } \right)$ . Solving this equation for $\theta$ (Problem 9.15), we find that

$$
\theta = - \frac {1}{2 \sigma_ {w} ^ {2}} \left(2 \sigma_ {w} ^ {2} + \sigma_ {v} ^ {2} - \sqrt {\sigma_ {v} ^ {4} + 4 \sigma_ {v} ^ {2} \sigma_ {w} ^ {2}}\right)
$$

and that $\theta = a - 1$ .

It is instructive to derive the exponential smoothing formula for $\hat { Y } _ { t }$ directly from the ARIMA(0,1,1) structure of $\{ Y _ { t } \}$ . For $t \geq 2$ , we have from Section 6.5 that

$$
\hat {Y} _ {t + 1} = Y _ {t} + \theta_ {t 1} (Y _ {t} - \hat {Y} _ {t}) = - \theta_ {t 1} \hat {Y} _ {t} + (1 + \theta_ {t 1}) Y _ {t}
$$

for $t \geq 2$ , where $\theta _ { t 1 }$ is found by application of the innovations algorithm to an MA(1) process with coefficient $\theta$ . It follows that $1 - a _ { t } \ = \ - \theta _ { t 1 }$ , and since $\theta _ { t 1 } \  \ \theta$

(see Remark 1 of Section 3.3) and $a _ { t }$ converges to the steady-state solution a, we conclude that

$$
1 - a = \lim  _ {t \rightarrow \infty} (1 - a _ {t}) = - \lim  _ {t \rightarrow \infty} \theta_ {t 1} = - \theta .
$$

# Example 9.4.2. The lognormal stochastic volatility model

We can rewrite the defining equations (7.4.2) and (7.4.3) of the lognormal SV process $\{ Z _ { t } \}$ in the following state-space form

$$
X _ {t} = \gamma_ {1} X _ {t - 1} + \eta_ {t}, \tag {9.4.10}
$$

and

$$
Y _ {t} = X _ {t} + \varepsilon_ {t}, \tag {9.4.11}
$$

where the (one-dimensional) state and observation vectors are

$$
X _ {t} = \ell_ {t} - \frac {\gamma_ {0}}{1 - \gamma_ {1}}, \tag {9.4.12}
$$

and

$$
Y _ {t} = \ln Z _ {t} ^ {2} + 1. 2 7 - \frac {\gamma_ {0}}{2 \left(1 - \gamma_ {1}\right)} \tag {9.4.13}
$$

respectively. The independent white-noise sequences $\{ \eta _ { t } \}$ and $\left\{ \varepsilon _ { t } \right\}$ have zero means and variances $\sigma ^ { 2 }$ and 4.93 respectively.

Taking

$$
\hat {X} _ {0} = E X _ {0} = 0 \tag {9.4.14}
$$

and

$$
\hat {\Omega} _ {0} = \operatorname {V a r} \left(X _ {0}\right) = \sigma^ {2} / \left(1 - \gamma_ {1} ^ {2}\right), \tag {9.4.15}
$$

and we can directly apply the Kalman prediction recursions (9.4.1), (9.4.2), (9.4.6) and (9.4.8), to compute recursively the best linear predictor of $X _ { t + h }$ in terms of $\{ Y _ { s } , s \le t \}$ , or equivalently of the log volatility $\ell _ { t + h }$ in terms of the observations $\{ \ln Z _ { s } ^ { 2 } , s \le t \}$ .

# Kalman Filtering:

The filtered estimates ${ \bf X } _ { t \mid t } = P _ { t } ( { \bf X } _ { t } )$ and their error covariance matrices $\begin{array} { r } { \Omega _ { t | t } = } \end{array}$ $E [ ( \mathbf { X } _ { t } - \mathbf { X } _ { t | t } ) ( \mathbf { X } _ { t } - \mathbf { X } _ { t | t } ) ^ { \prime } ]$ are determined by the relations

$$
P _ {t} \mathbf {X} _ {t} = P _ {t - 1} \mathbf {X} _ {t} + \Omega_ {t} G _ {t} ^ {\prime} \Delta_ {t} ^ {- 1} \left(\mathbf {Y} _ {t} - G _ {t} \hat {\mathbf {X}} _ {t}\right) \tag {9.4.16}
$$

and

$$
\Omega_ {t | t} = \Omega_ {t} - \Omega_ {t} G _ {t} ^ {\prime} \Delta_ {t} ^ {- 1} G _ {t} \Omega_ {t} ^ {\prime}. \tag {9.4.17}
$$

Proof. From (9.4.3) it follows that

$$
P _ {t} \mathbf {X} _ {t} = P _ {t - 1} \mathbf {X} _ {t} + M \mathbf {I} _ {t},
$$

where

$$
M = E \left(\mathbf {X} _ {t} \mathbf {I} _ {t} ^ {\prime}\right) \left[ E \left(\mathbf {I} _ {t} \mathbf {I} _ {t} ^ {\prime}\right) \right] ^ {- 1} = E \left[ \mathbf {X} _ {t} \left(G _ {t} \left(\mathbf {X} _ {t} - \hat {\mathbf {X}} _ {t}\right) + W _ {t}\right) ^ {\prime} \right] \Delta_ {t} ^ {- 1} = \Omega_ {t} G _ {t} ^ {\prime} \Delta_ {t} ^ {- 1}. \tag {9.4.18}
$$

To establish (9.4.17) we write

$$
\mathbf {X} _ {t} - P _ {t - 1} \mathbf {X} _ {t} = \mathbf {X} _ {t} - P _ {t} \mathbf {X} _ {t} + P _ {t} \mathbf {X} _ {t} - P _ {t - 1} \mathbf {X} _ {t} = \mathbf {X} _ {t} - P _ {t} \mathbf {X} _ {t} + M \mathbf {I} _ {t}.
$$

Using (9.4.18) and the orthogonality of $\mathbf { X } _ { t } - P _ { t } \mathbf { X } _ { t }$ and $M \mathbf { I } _ { t }$ , we find from the last equation that

$$
\Omega_ {t} = \Omega_ {t | t} + \Omega_ {t} G _ {t} ^ {\prime} \Delta_ {t} ^ {- 1} G _ {t} \Omega_ {t} ^ {\prime},
$$

as required.

# Kalman Fixed-Point Smoothing:

The smoothed estimates ${ \bf X } _ { t \mid n } = P _ { n } { \bf X } _ { t }$ and the error covariance matrices $\varOmega _ { t | n } =$ $E [ ( \mathbf { X } _ { t } - \mathbf { X } _ { t \mid n } ) ( \mathbf { X } _ { t } - \mathbf { X } _ { t \mid n } ) ^ { \prime } ]$ are determined for fixed $t$ by the following recursions, which can be solved successively for $n = t , t + 1 , . . .$ :

$$
P _ {n} \mathbf {X} _ {t} = P _ {n - 1} \mathbf {X} _ {t} + \Omega_ {t, n} G _ {n} ^ {\prime} \Delta_ {n} ^ {- 1} \left(\mathbf {Y} _ {n} - G _ {n} \hat {\mathbf {X}} _ {n}\right), \tag {9.4.19}
$$

$$
\Omega_ {t, n + 1} = \Omega_ {t, n} \left[ F _ {n} - \Theta_ {n} \Delta_ {n} ^ {- 1} G _ {n} \right], \tag {9.4.20}
$$

$$
\Omega_ {t \mid n} = \Omega_ {t \mid n - 1} - \Omega_ {t, n} G _ {n} ^ {\prime} \Delta_ {n} ^ {- 1} G _ {n} \Omega_ {t, n} ^ {\prime}, \tag {9.4.21}
$$

with initial conditions $P _ { t - 1 } \mathbf { X } _ { t } = \hat { \mathbf { X } } _ { t }$ and $\Omega _ { t , t } = \Omega _ { t | t - 1 } = \Omega _ { t }$ (found from Kalman prediction).

Proof. Using (9.4.3) we can write $P _ { n } \mathbf { X } _ { t } = P _ { n - 1 } \mathbf { X } _ { t } + C \mathbf { I } _ { n }$ , where $\mathbf { I } _ { n } = G _ { n } \big ( \mathbf { X } _ { n } - \hat { \mathbf { X } } _ { n } \big ) + \mathbf { W } _ { n }$ . By Remark 4 above,

$$
C = E \left[ \mathbf {X} _ {t} \left(G _ {n} \left(\mathbf {X} _ {n} - \hat {\mathbf {X}} _ {n}\right) + \mathbf {W} _ {n}\right) ^ {\prime} \right] \left[ E \left(\mathbf {I} _ {n} \mathbf {I} _ {n} ^ {\prime}\right) \right] ^ {- 1} = \Omega_ {t, n} G _ {n} ^ {\prime} \Delta_ {n} ^ {- 1}, \tag {9.4.22}
$$

where $\Omega _ { t , n } : = E \big [ \big ( \mathbf { X } _ { t } - \hat { \mathbf { X } } _ { t } \big ) \big ( \mathbf { X } _ { n } - \hat { \mathbf { X } } _ { n } \big ) ^ { \prime } \big ]$ . It follows now from (9.1.2), (9.4.5), the orthogonality of $\mathbf { V } _ { n }$ and $\mathbf { W } _ { n }$ with $\mathbf { X } _ { t } - \hat { \mathbf { X } } _ { t }$ , and the definition of $\Omega _ { t , n }$ that

$$
\Omega_ {t, n + 1} = E \left[ \left(\mathbf {X} _ {t} - \hat {\mathbf {X}} _ {t}\right) \left(\mathbf {X} _ {n} - \hat {\mathbf {X}} _ {n}\right) ^ {\prime} \left(F _ {n} - \Theta_ {n} \Delta_ {n} ^ {- 1} G _ {n}\right) ^ {\prime} \right] = \Omega_ {t, n} \left[ F _ {n} - \Theta_ {n} \Delta_ {n} ^ {- 1} G _ {n} \right] ^ {\prime},
$$

thus establishing (9.4.20). To establish (9.4.21) we write

$$
\mathbf {X} _ {t} - P _ {n} \mathbf {X} _ {t} = \mathbf {X} _ {t} - P _ {n - 1} \mathbf {X} _ {t} - C \mathbf {I} _ {n}.
$$

Using (9.4.22) and the orthogonality of $\mathbf { X } _ { t } - P _ { n } \mathbf { X } _ { t }$ and ${ \mathbf I } _ { n }$ , the last equation then gives

$$
\Omega_ {t | n} = \Omega_ {t | n - 1} - \Omega_ {t, n} G _ {n} ^ {\prime} \Delta_ {n} ^ {- 1} G _ {n} \Omega_ {t, n} ^ {\prime}, \quad n = t, t + 1, \dots ,
$$

as required.

# 9.5 Estimation for State-Space Models

Consider the state-space model defined by equations (9.1.1) and (9.1.2) and suppose that the model is completely parameterized by the components of the vector $\pmb \theta$ . The maximum likelihood estimate of $\pmb \theta$ is found by maximizing the likelihood of the observations $\mathbf { Y } _ { 1 } , \ldots , \mathbf { Y } _ { n }$ with respect to the components of the vector $\pmb \theta$ . If the conditional probability density of $\mathbf { Y } _ { t }$ given $\mathbf { Y } _ { t - 1 } = \mathbf { y } _ { t - 1 } , \ldots , \mathbf { Y } _ { 0 } = \mathbf { y } _ { 0 }$ is $f _ { t } ( \cdot | \mathbf { y } _ { t - 1 } , \ldots , \mathbf { y } _ { 0 } )$ , then the likelihood of $\mathbf { Y } _ { t }$ $\mathbf { \Phi } _ { t } ^ { \prime } , t = 1 , \dots , n$ (conditional on ${ \bf Y } _ { 0 }$ ), can immediately be written as

$$
L (\boldsymbol {\theta}; \mathbf {Y} _ {1}, \dots , \mathbf {Y} _ {n}) = \prod_ {t = 1} ^ {n} f _ {t} \left(\mathbf {Y} _ {t} \mid \mathbf {Y} _ {t - 1}, \dots , \mathbf {Y} _ {0}\right). \tag {9.5.1}
$$

The calculation of the likelihood for any fixed numerical value of $\pmb \theta$ is extremely complicated in general, but is greatly simplified if $\mathbf { Y } _ { 0 } , \mathbf { X } _ { 1 }$ and Wt, Vt, $t = 1 , 2 , \dots$ , are assumed to be jointly Gaussian. The resulting likelihood is called the Gaussian likelihood and is widely used in time series analysis (cf. Section 5.2) whether the time series is truly Gaussian or not. As before, we shall continue to use the term likelihood to mean Gaussian likelihood.

If $\mathbf { Y } _ { 0 } , \mathbf { X } _ { 1 }$ and $\mathbf { W } _ { t } , \mathbf { V } _ { t } , t = 1 , 2 , \ldots$ , are jointly Gaussian, then the conditional densities in (9.5.1) are given by

$$
f _ {t} \left(\mathbf {Y} _ {t} \mid \mathbf {Y} _ {t - 1}, \dots , \mathbf {Y} _ {0}\right) = (2 \pi) ^ {- w / 2} (\det  \Delta_ {t}) ^ {- 1 / 2} \exp \left[ - \frac {1}{2} \mathbf {I} _ {t} ^ {\prime} \Delta_ {t} ^ {- 1} \mathbf {I} _ {t} \right],
$$

where $\mathbf { I } _ { t } = \mathbf { Y } _ { t } - P _ { t - 1 } \mathbf { Y } _ { t } = \mathbf { Y } _ { t } - G \hat { \mathbf { X } } _ { t }$ , $P _ { t - 1 } \mathbf { Y } _ { t }$ , and $\Delta _ { t } , t \geq 1$ , are the one-step predictors and error covariance matrices found from the Kalman prediction recursions. The likelihood of the observations $\mathbf { Y } _ { 1 } , \ldots , \mathbf { Y } _ { n }$ (conditional on ${ \bf Y } _ { 0 }$ ) can therefore be expressed as

$$
L \left(\boldsymbol {\theta}; \mathbf {Y} _ {1}, \dots , \mathbf {Y} _ {n}\right) = (2 \pi) ^ {- n w / 2} \left(\prod_ {j = 1} ^ {n} \det  \Delta_ {j}\right) ^ {- 1 / 2} \exp \left[ - \frac {1}{2} \sum_ {j = 1} ^ {n} \mathbf {I} _ {j} ^ {\prime} \Delta_ {j} ^ {- 1} \mathbf {I} _ {j} \right]. \tag {9.5.2}
$$

Given the observations $\mathbf { Y } _ { 1 } , \ldots , \mathbf { Y } _ { n }$ , the distribution of ${ \bf Y } _ { 0 }$ (see Section 9.4), and a particular parameter value $\pmb \theta$ , the numerical value of the likelihood $L$ can be computed from the previous equation with the aid of the Kalman recursions of Section 9.4. To find maximum likelihood estimates of the components of $\pmb \theta$ , a nonlinear optimization algorithm must be used to search for the value of $\pmb \theta$ that maximizes the value of $L$ .

Having estimated the parameter vector $\pmb \theta$ , we can compute forecasts based on the fitted state-space model and estimated mean squared errors by direct application of equations (9.4.7) and (9.4.9).

# 9.5.1 Application to Structural Models

The general structural model for a univariate time series $\{ Y _ { t } \}$ of which we gave examples in Section 9.2 has the form

$$
Y _ {t} = G \mathbf {X} _ {t} + W _ {t}, \quad \left\{W _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma_ {w} ^ {2}\right), \tag {9.5.3}
$$

$$
\mathbf {X} _ {t + 1} = F \mathbf {X} _ {t} + \mathbf {V} _ {t}, \quad \left\{\mathbf {V} _ {t} \right\} \sim \operatorname {W N} (0, Q), \tag {9.5.4}
$$

for $t = 1 , 2 , \ldots$ , where $F$ and $G$ are assumed known. We set $Y _ { 0 } ~ = ~ 1$ in order to include constant terms in our predictors and complete the specification of the model by prescribing the mean and covariance matrix of the initial state $\mathbf { X } _ { 1 }$ . A simple and convenient assumption is that $\mathbf { X } _ { 1 }$ is equal to a deterministic but unknown parameter $\pmb { \mu }$ and that $\hat { { \bf X } } _ { 1 } = \hat { { \bf \mu } } { \bf \Pi } $ , so that $\varOmega _ { 1 } = 0$ . The parameters of the model are then $\mu , Q$ , and $\sigma _ { w } ^ { 2 }$ .

Direct maximization of the likelihood (9.5.2) is difficult if the dimension of the state vector is large. The maximization can, however, be simplified by the following stepwise procedure. For fixed $Q$ we find $\hat { \pmb { \mu } } ( Q )$ and $\sigma _ { w } ^ { 2 } ( { Q } )$ that maximize the likelihood $L \left( \mu , Q , \sigma _ { w } ^ { 2 } \right)$ . We then maximize the “reduced likelihood” $L \left( \hat { \pmb { \mu } } ( Q ) , Q , \hat { \sigma } _ { w } ^ { 2 } ( Q ) \right)$ with respect to $Q$ .

To achieve this we define the mean-corrected state vectors, $\mathbf { X } _ { t } ^ { * } = \mathbf { X } _ { t } - F ^ { t - 1 } { \boldsymbol { \mu } }$ , and apply the Kalman prediction recursions to $\{ \mathbf { X } _ { t } ^ { * } \}$ with initial condition $\mathbf { X } _ { 1 } ^ { * } = \mathbf { 0 }$ . This gives, from (9.4.1),

$$
\hat {\mathbf {X}} _ {t + 1} ^ {*} = F \hat {\mathbf {X}} _ {t} ^ {*} + \Theta_ {t} \Delta_ {t} ^ {- 1} \left(Y _ {t} - G \hat {\mathbf {X}} _ {t} ^ {*}\right), \quad t = 1, 2, \dots , \tag {9.5.5}
$$

with $\hat { \mathbf { X } } _ { 1 } ^ { * } = \mathbf { 0 }$ . Since $\hat { \mathbf { X } } _ { t }$ also satisfies (9.5.5), but with initial condition $\hat { \mathbf X } _ { t } = { \boldsymbol \mu }$ , it follows that

$$
\hat {\mathbf {X}} _ {t} = \hat {\mathbf {X}} _ {t} ^ {*} + C _ {t} \boldsymbol {\mu} \tag {9.5.6}
$$

for some $\nu \times \nu$ matrices $C _ { t }$ . (Note that although $\hat { \mathbf { X } } _ { t } = P ( \mathbf { X } _ { t } | Y _ { 0 } , Y _ { 1 } , \ldots , Y _ { t } )$ , the quantity $\hat { \mathbf { X } } _ { t } ^ { * }$ is not the corresponding predictor of $\mathbf { X } _ { t } ^ { * }$ .) The matrices $C _ { t }$ can be determined recursively from (9.5.5), (9.5.6), and (9.4.1). Substituting (9.5.6) into (9.5.5) and using (9.4.1), we have

$$
\begin{array}{l} \hat {\mathbf {X}} _ {t + 1} ^ {*} = F \left(\hat {\mathbf {X}} _ {t} - C _ {t} \boldsymbol {\mu}\right) + \Theta_ {t} \Delta_ {t} ^ {- 1} \left(Y _ {t} - G \left(\hat {\mathbf {X}} _ {t} - C _ {t} \boldsymbol {\mu}\right)\right) \\ = F \hat {\mathbf {X}} _ {t} + \Theta_ {t} \Delta_ {t} ^ {- 1} \left(Y _ {t} - G \hat {\mathbf {X}} _ {t}\right) - \left(F - \Theta_ {t} \Delta_ {t} ^ {- 1} G\right) C _ {t} \boldsymbol {\mu} \\ = \hat {\mathbf {X}} _ {t + 1} - \left(F - \Theta_ {t} \Delta_ {t} ^ {- 1} G\right) C _ {t} \boldsymbol {\mu}, \\ \end{array}
$$

so that

$$
C _ {t + 1} = \left(F - \Theta_ {t} \Delta_ {t} ^ {- 1} G\right) C _ {t} \tag {9.5.7}
$$

with $C _ { 1 }$ equal to the identity matrix. The quadratic form in the likelihood (9.5.2) is therefore

$$
\begin{array}{l} S (\boldsymbol {\mu}, Q, \sigma_ {w} ^ {2}) = \sum_ {t = 1} ^ {n} \frac {\left(Y _ {t} - G \hat {\mathbf {X}} _ {t}\right) ^ {2}}{\Delta_ {t}} (9.5.8) \\ = \sum_ {t = 1} ^ {n} \frac {\left(Y _ {t} - G \hat {\mathbf {X}} _ {t} ^ {*} - G C _ {t} \boldsymbol {\mu}\right) ^ {2}}{\Delta_ {t}}. (9.5.9) \\ \end{array}
$$

Now let $Q ^ { * } : = \sigma _ { w } ^ { - 2 } Q$ and define $L ^ { * }$ to be the likelihood function with this new parameterization, i.e., $L ^ { * } \left( \pmb { \mu } , \mathcal { Q } ^ { * } , \sigma _ { w } ^ { 2 } \right) = L \left( \pmb { \mu } , \sigma _ { w } ^ { 2 } \pmb { \mathcal { Q } } ^ { * } , \sigma _ { w } ^ { 2 } \right)$ . Writing $\varDelta _ { t } ^ { * } = \sigma _ { w } ^ { - 2 } \varDelta _ { t }$ and $\varOmega _ { t } ^ { * } = \sigma _ { w } ^ { - 2 } \varOmega _ { t }$ , we see that the predictors $\hat { X } _ { t } ^ { * }$ and the matrices $C _ { t }$ in (9.5.7) depend on the parameters only through $Q ^ { * }$ . Thus,

$$
S (\boldsymbol {\mu}, Q, \sigma_ {w} ^ {2}) = \sigma_ {w} ^ {- 2} S (\boldsymbol {\mu}, Q ^ {*}, 1),
$$

so that

$$
\begin{array}{l} - 2 \ln L ^ {*} (\boldsymbol {\mu}, Q ^ {*}, \sigma_ {w} ^ {2}) = n \ln (2 \pi) + \sum_ {t = 1} ^ {n} \ln \Delta_ {t} + \sigma_ {w} ^ {- 2} S (\boldsymbol {\mu}, Q ^ {*}, 1) \\ = n \ln (2 \pi) + \sum_ {t = 1} ^ {n} \ln \Delta_ {t} ^ {*} + n \ln \sigma_ {w} ^ {2} + \sigma_ {w} ^ {- 2} S (\boldsymbol {\mu}, Q ^ {*}, 1). \\ \end{array}
$$

For $Q ^ { * }$ fixed, it is easy to show (see Problem 9.18) that this function is minimized when

$$
\hat {\boldsymbol {\mu}} = \hat {\boldsymbol {\mu}} \left(Q ^ {*}\right) = \left[ \sum_ {t = 1} ^ {n} \frac {C _ {t} ^ {\prime} G ^ {\prime} G C _ {t}}{\Delta_ {t} ^ {*}} \right] ^ {- 1} \sum_ {t = 1} ^ {n} \frac {C _ {t} ^ {\prime} G ^ {\prime} \left(Y _ {t} - G \hat {\mathbf {X}} _ {t} ^ {*}\right)}{\Delta_ {t} ^ {*}} \tag {9.5.10}
$$

and

$$
\hat {\sigma} _ {w} ^ {2} = \hat {\sigma} _ {w} ^ {2} (Q ^ {*}) = n ^ {- 1} \sum_ {t = 1} ^ {n} \frac {\left(Y _ {t} - G \hat {\mathbf {X}} _ {t} ^ {*} - G C _ {t} \hat {\boldsymbol {\mu}}\right) ^ {2}}{\Delta_ {t} ^ {*}}. \tag {9.5.11}
$$

Replacing $\pmb { \mu }$ and $\sigma _ { w } ^ { 2 }$ by these values in $- 2 \ln { \cal L } ^ { * }$ and ignoring constants, the reduced likelihood becomes

$$
\ell \left(Q ^ {*}\right) = \ln \left(n ^ {- 1} \sum_ {t = 1} ^ {n} \frac {\left(Y _ {t} - G \hat {\mathbf {X}} _ {t} ^ {*} - G C _ {t} \hat {\boldsymbol {\mu}}\right) ^ {2}}{\Delta_ {t} ^ {*}}\right) + n ^ {- 1} \sum_ {t = 1} ^ {n} \ln \left(\det \Delta_ {t} ^ {*}\right). \tag {9.5.12}
$$

If ${ \hat { Q } } ^ { * }$ denotes the minimizer of (9.5.12), then the maximum likelihood estimator of the parameters $\mu , Q , \sigma _ { w } ^ { 2 }$ are $\hat { \pmb { \mu } }$ , $\hat { \sigma } _ { w } ^ { 2 } \hat { Q } ^ { * }$ , σˆ 2w, where $\hat { \pmb { \mu } }$ and $\hat { \sigma } _ { w } ^ { 2 }$ are computed from (9.5.10) and (9.5.11) with $Q ^ { * }$ replaced by $\hat { Q } ^ { * }$ .

We can now summarize the steps required for computing the maximum likelihood estimators of $\mu , Q$ , and $\sigma _ { w } ^ { 2 }$ for the model (9.5.3)–(9.5.4).

1. For a fixed $Q ^ { * }$ , apply the Kalman prediction recursions with $\hat { \mathbf { X } } _ { 1 } ^ { * } = \mathbf { 0 }$ , $\varOmega _ { 1 } = 0$ , $Q = Q ^ { * }$ , and $\sigma _ { w } ^ { 2 } = 1$ to obtain the predictors $\hat { \mathbf { X } } _ { t } ^ { * }$ . Let $\varDelta _ { t } ^ { * }$ denote the one-step prediction error produced by these recursions.   
2. Set $\begin{array} { r } { \hat { \pmb { \mu } } = \hat { \pmb { \mu } } ( Q ^ { * } ) = \left[ \sum _ { t = 1 } ^ { n } C _ { t } ^ { \prime } G ^ { \prime } G C _ { t } / \Delta _ { t } \right] ^ { - 1 } \sum _ { t = 1 } ^ { n } C _ { t } ^ { \prime } G ^ { \prime } ( Y _ { t } - G \hat { \bf X } _ { t } ^ { * } ) / \Delta _ { t } ^ { * } . } \end{array}$   
3. Let ${ \hat { Q } } ^ { * }$ be the minimizer of (9.5.12).   
4. The maximum likelihood estimators of $\mu , Q$ , and $\sigma _ { w } ^ { 2 }$ are then given by $\hat { \pmb { \mu } } , \hat { \sigma } _ { w } ^ { 2 } \hat { Q } ^ { * }$ , and $\hat { \sigma } _ { w } ^ { 2 }$ , respectively, where $\hat { \pmb { \mu } }$ and $\hat { \sigma } _ { w } ^ { 2 }$ are found from (9.5.10) and (9.5.11) evaluated at $\hat { Q } ^ { * }$ .

# Example 9.5.1. Random Walk Plus Noise Model

In Example 9.2.1, 100 observations were generated from the structural model

$$
Y _ {t} = M _ {t} + W _ {t}, \quad \left\{W _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma_ {w} ^ {2}\right),
$$

$$
M _ {t + 1} = M _ {t} + V _ {t}, \quad \{V _ {t} \} \sim \operatorname {W N} \left(0, \sigma_ {v} ^ {2}\right),
$$

with initial values $\mu = M _ { 1 } = 0$ , $\sigma _ { w } ^ { 2 } = 8$ , and $\sigma _ { \nu } ^ { 2 } = 4$ . The maximum likelihood estimates of the parameters are found by first minimizing (9.5.12) with $\hat { \mu }$ given by (9.5.10). Substituting these values into (9.5.11) gives $\hat { \sigma } _ { w } ^ { 2 }$ . The resulting estimates are $\hat { \mu } = 0 . 9 0 6$ , $\hat { \sigma } _ { \nu } ^ { 2 } = 5 . 3 5 1$ , and $\hat { \sigma } _ { w } ^ { 2 } = 8 . 2 3 3$ , which are in reasonably close agreement with the true values.

# Example 9.5.2. International Airline Passengers, 1949–1960; AIRPASS.TSM

The monthly totals of international airline passengers from January 1949 to December 1960 (Box and Jenkins 1976) are displayed in Figure 9-3. The data exhibit both a strong seasonal pattern and a nearly linear trend. Since the variability of the data $Y _ { 1 } , \dots , Y _ { 1 4 4 }$ increases for larger values of $Y _ { t }$ , it may be appropriate to consider a logarithmic transformation of the data. For the purpose of this illustration, however, we will fit a structural model incorporating a randomly varying trend and seasonal and noise components (see Example 9.2.3) to the raw data. This model has the form

![](images/ef497ab4661e6c7d082d5ea3c28c1c750a86814e382e2c6215a863f4f440835b.jpg)  
Figure 9-3 International airline passengers; monthly totals from January 1949 to December 1960

$$
Y _ {t} = G \mathbf {X} _ {t} + W _ {t}, \quad \{W _ {t} \} \sim \operatorname {W N} \left(0, \sigma_ {w} ^ {2}\right),
$$

$$
\mathbf {X} _ {t + 1} = F \mathbf {X} _ {t} + \mathbf {V} _ {t}, \quad \left\{\mathbf {V} _ {t} \right\} \sim \operatorname {W N} (0, Q),
$$

where $\mathbf { X } _ { t }$ is a 13-dimensional state-vector,

$$
F = \left[ \begin{array}{c c c c c c c} 1 & 1 & 0 & 0 & \dots & 0 & 0 \\ 0 & 1 & 0 & 0 & \dots & 0 & 0 \\ 0 & 0 & - 1 & - 1 & \dots & - 1 & - 1 \\ 0 & 0 & 1 & 0 & \dots & 0 & 0 \\ 0 & 0 & 0 & 1 & \dots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & 0 & 0 & \dots & 1 & 0 \end{array} \right],
$$

$$
G = \left[ \begin{array}{c c c c} 1 & 0 & 1 & 0 \end{array} \dots & 0 \right],
$$

and

$$
Q = \left[ \begin{array}{c c c c c c} \sigma_ {1} ^ {2} & 0 & 0 & 0 & \dots & 0 \\ 0 & \sigma_ {2} ^ {2} & 0 & 0 & \dots & 0 \\ 0 & 0 & \sigma_ {3} ^ {2} & 0 & \dots & 0 \\ 0 & 0 & 0 & 0 & \dots & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & \dots & 0 \end{array} \right].
$$

The parameters of the model are $\mu , \sigma _ { 1 } ^ { 2 } , \sigma _ { 2 } ^ { 2 } , \sigma _ { 3 } ^ { 2 }$ , and $\sigma _ { w } ^ { 2 }$ , where $\pmb { \mu } = \mathbf { X } _ { 1 }$ . Minimizing (9.5.12) with respect to $Q ^ { * }$ we find from (9.5.11) and (9.5.12) that

$$
\left(\hat {\sigma} _ {1} ^ {2}, \hat {\sigma} _ {2} ^ {2}, \hat {\sigma} _ {3} ^ {2}, \hat {\sigma} _ {w} ^ {2}\right) = (1 7 0. 6 3, . 0 0 0 0 0, 1 1. 3 3 8, . 0 1 4 1 7 9)
$$

and from (9.5.10) that μ (146.9, 2.171, 34.92, 34.12, 47.00, 16.98, 22.99, 53.99, 58.34, 33.65, 2.204, 4.053, 6.894)′. The first component, $X _ { t 1 }$ , of the state vector corresponds to the local linear trend with slope $X _ { t 2 }$ . Since $\hat { \sigma } _ { 2 } ^ { 2 } = 0$ , the slope at time $t$ , which satisfies

$$
X _ {t 2} = X _ {t - 1, 2} + V _ {t 2},
$$

Figure 9-4 The one-step predictors $\left( \hat { X } _ { t 1 } , \hat { X } _ { t 2 } , \hat { X } _ { t 3 } \right) ^ { \prime }$ for the airline passenger data in Example 9.5.2   
Figure 9-5   
![](images/982dab5908a986b01cfba8ce74efdaf26eb3c880a861a8404b3c044c9c3dc791.jpg)  
The one-step predictors $\hat { Y } _ { t }$ for the airline passenger data (solid line) and the actual data (square boxes)

![](images/2b63353b348dddec25f9cd45a69f9179afe4bf293a11bcc25fd81545dc033980.jpg)

must be nearly constant and equal to $\hat { X } _ { 1 2 } = 2 . 1 7 1$ . The first three components of the predictors $\hat { \mathbf { X } } _ { t }$ are plotted in Figure 9-4. Notice that the first component varies like a random walk around a straight line, while the second component is nearly constant as a result of $\hat { \sigma } _ { 2 } ^ { 2 } \approx 0$ . The third component, corresponding to the seasonal component, exhibits a clear seasonal cycle that repeats roughly the same pattern throughout the 12 years of data. The one-step predictors $\hat { X } _ { t 1 } + \check { X } _ { t 3 }$ of $Y _ { t }$ are plotted in Figure 9-5 (solid line) together with the actual data (square boxes). For this model the predictors follow the movement of the data quite well.

# 9.6 State-Space Models with Missing Observations

State-space representations and the associated Kalman recursions are ideally suited to the analysis of data with missing values, as was pointed out by Jones (1980) in the context of maximum likelihood estimation for ARMA processes. In this section we shall deal with two missing-value problems for state-space models. The first is the

evaluation of the (Gaussian) likelihood based on $\{ \mathbf { Y } _ { i _ { 1 } } , \ldots , \mathbf { Y } _ { i _ { r } } \}$ , where $i _ { 1 } , i _ { 2 } , \ldots , i _ { r }$ are positive integers such that $1 ~ \leq ~ i _ { 1 } ~ < ~ i _ { 2 } ~ < ~ \cdots ~ < ~ i _ { r } ~ \leq ~ n$ . (This allows for observation of the process $\{ \mathbf { Y } _ { t } \}$ at irregular intervals, or equivalently for the possibility that $( n - r )$ observations are missing from the sequence $\{ \mathbf { Y } _ { 1 } , \ldots , \mathbf { Y } _ { n } \}$ .) The solution of this problem will, in particular, enable us to carry out maximum likelihood estimation for ARMA and ARIMA processes with missing values. The second problem to be considered is the minimum mean squared error estimation of the missing values themselves.

# 9.6.1 The Gaussian Likelihood of $\{ \mathsf { Y } _ { i _ { 1 } } , \hdots , \mathsf { Y } _ { i _ { r } } \} , \ 1 \leq i _ { 1 } < i _ { 2 } < \cdots < i _ { r } \leq n$

Consider the state-space model defined by equations (9.1.1) and (9.1.2) and suppose that the model is completely parameterized by the components of the vector $\pmb \theta$ . If there are no missing observations, i.e., if $r = n$ and $i _ { j } = j , j = 1 , \ldots , n$ , then the likelihood of the observations $\{ \mathbf { Y } _ { 1 } , \ldots , \mathbf { Y } _ { n } \}$ is easily found as in Section 9.5 to be

$$
L (\boldsymbol {\theta}; \mathbf {Y} _ {1}, \dots , \mathbf {Y} _ {n}) = (2 \pi) ^ {- n w / 2} \left(\prod_ {j = 1} ^ {n} \det  \Delta_ {j}\right) ^ {- 1 / 2} \exp \left[ - \frac {1}{2} \sum_ {j = 1} ^ {n} \mathbf {I} _ {j} ^ {\prime} \Delta_ {j} ^ {- 1} \mathbf {I} _ {j} \right],
$$

where ${ \bf I } _ { j } \ = \ { \bf Y } _ { j } - P _ { j - 1 } { \bf Y } _ { j }$ and $\begin{array} { l l l } { \Delta _ { j } , j } & { \geq } & { 1 } \end{array}$ , are the one-step predictors and error covariance matrices found from (9.4.7) and (9.4.9) with $\mathbf { Y } _ { 0 } = \mathbf { 1 }$ .

To deal with the more general case of possibly irregularly spaced observations $\{ \mathbf { Y } _ { i _ { 1 } } , \ldots , \mathbf { Y } _ { i _ { r } } \}$ , we introduce a new series $\{ \mathbf { Y } _ { t } ^ { * } \}$ , related to the process $\{ { \mathbf { X } } _ { t } \}$ by the modified observation equation

$$
\mathbf {Y} _ {t} ^ {*} = G _ {t} ^ {*} \mathbf {X} _ {t} + \mathbf {W} _ {t} ^ {*}, \quad t = 1, 2, \dots , \tag {9.6.1}
$$

where

$$
G _ {t} ^ {*} = \left\{ \begin{array}{l l} G _ {t} & \text {i f} t \in \left\{i _ {1}, \dots , i _ {r} \right\}, \\ 0 & \text {o t h e r w i s e}, \end{array} \quad \mathbf {W} _ {t} ^ {*} = \left\{ \begin{array}{l l} \mathbf {W} _ {t} & \text {i f} t \in \left\{i _ {1}, \dots , i _ {r} \right\}, \\ \mathbf {N} _ {t} & \text {o t h e r w i s e}, \end{array} \right. \right. \tag {9.6.2}
$$

and $\{ \mathbf { N } _ { t } \}$ is iid with

$$
\mathbf {N} _ {t} \sim \mathrm {N} (\mathbf {0}, I _ {w \times w}), \quad \mathbf {N} _ {s} \perp \mathbf {X} _ {1}, \quad \mathbf {N} _ {s} \perp \left[ \begin{array}{l} \mathbf {V} _ {t} \\ \mathbf {W} _ {t} \end{array} \right], \quad s, t = 0, \pm 1, \dots . \tag {9.6.3}
$$

Equations (9.6.1) and (9.1.2) constitute a state-space representation for the new series $\{ \mathbf { Y } _ { t } ^ { * } \}$ , which coincides with $\{ { \mathbf Y } _ { t } \}$ at each $t \in \{ i _ { 1 } , i _ { 2 } , \ldots , i _ { r } \}$ , and at other times takes random values that are independent of $\{ \mathbf { Y } _ { t } \}$ with a distribution independent of $\pmb \theta$ .

Let $L _ { 1 } \left( \pmb \theta ; \mathbf { y } _ { i _ { 1 } } , \ldots , \mathbf { y } _ { i _ { r } } \right)$ be the Gaussian likelihood based on the observed values ${ \bf y } _ { i _ { 1 } } , \ldots , { \bf y } _ { i _ { r } }$ of $\mathbf { Y } _ { i _ { 1 } } , \ldots , \mathbf { Y } _ { i _ { r } }$ under the model defined by (9.1.1) and (9.1.2). Corresponding to these observed values, we define a new sequence, ${ \bf y } _ { 1 } ^ { * } , \ldots , { \bf y } _ { n } ^ { * }$ , by

$$
\mathbf {y} _ {t} ^ {*} = \left\{ \begin{array}{l l} \mathbf {y} _ {t} & \text {i f} t \in \left\{i _ {1}, \dots , i _ {r} \right\}, \\ \mathbf {0} & \text {o t h e r w i s e .} \end{array} \right. \tag {9.6.4}
$$

Then it is clear from the preceding paragraph that

$$
L _ {1} \left(\boldsymbol {\theta}; \mathbf {y} _ {i _ {1}}, \dots , \mathbf {y} _ {i _ {r}}\right) = (2 \pi) ^ {(n - r) w / 2} L _ {2} \left(\boldsymbol {\theta}; \mathbf {y} _ {1} ^ {*}, \dots , \mathbf {y} _ {n} ^ {*}\right), \tag {9.6.5}
$$

where $L _ { 2 }$ denotes the Gaussian likelihood under the model defined by (9.6.1) and (9.1.2).

In view of (9.6.5) we can now compute the required likelihood $L _ { 1 }$ of the realized values $\{ \mathbf { y } _ { t } , t = i _ { 1 } , \ldots , i _ { r } \}$ as follows:

i. Define the sequence $\{ \mathbf { y } _ { t } ^ { * }$ $\{ \mathbf { y } _ { t } ^ { * } , t = 1 , \ldots , n \}$ as in (9.6.4).   
ii. Find the one-step predictors $\hat { \mathbf { Y } } _ { t } ^ { * }$ of $\mathbf { Y } _ { t } ^ { * }$ , and their error covariance matrices $\boldsymbol { \varDelta } _ { t } ^ { * }$ , using Kalman prediction and equations (9.4.7) and (9.4.9) applied to the statespace representation (9.6.1) and (9.1.2) of $\{ \mathbf { Y } _ { t } ^ { * } \}$ . Denote the realized values of the predictors, based on the observation sequence $\left\{ \mathbf { y } _ { t } ^ { * } \right\}$ , by $\left\{ \hat { \mathbf { y } } _ { t } ^ { * } \right\}$ .

iii. The required Gaussian likelihood of the irregularly spaced observations $\{ \mathbf { y } _ { i _ { 1 } } , \ldots$ , $\mathbf { y } _ { i _ { r } } \}$ is then, by (9.6.5),

$$
L _ {1} (\boldsymbol {\theta}; \mathbf {y} _ {i _ {1}}, \dots , \mathbf {y} _ {i _ {r}}) = (2 \pi) ^ {- r w / 2} \left(\prod_ {j = 1} ^ {n} \det  \Delta_ {j} ^ {*}\right) ^ {- 1 / 2} \exp \left\{- \frac {1}{2} \sum_ {j = 1} ^ {n} \mathbf {i} _ {j} ^ {* \prime} \Delta_ {j} ^ {* - 1} \mathbf {i} _ {j} ^ {*} \right\},
$$

where $\mathbf { i } _ { j } ^ { * }$ denotes the observed innovation $\mathbf { y } _ { j } ^ { * } - \hat { \mathbf { y } } _ { j } ^ { * } , j = 1 , \ldots , n$ .

Example 9.6.1. An AR(1) Series with One Missing Observation

Let $\{ Y _ { t } \}$ be the causal AR(1) process defined by

$$
Y _ {t} - \phi Y _ {t - 1} = Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right).
$$

To find the Gaussian likelihood of the observations $y _ { 1 } , y _ { 3 } , y _ { 4 }$ , and $y _ { 5 }$ of Y1, Y3, Y4, and $Y _ { 5 }$ we follow the steps outlined above.

i. Set $y _ { i } ^ { * } = y _ { i } , i = 1 , 3 , 4 , 5$ and $y _ { 2 } ^ { * } = 0$ .   
ii. We start with the state-space model for $\{ Y _ { t } \}$ from Example 9.1.1, i.e., $\begin{array} { r l } { Y _ { t } } & { { } = } \end{array}$ $X _ { t }$ , $X _ { t + 1 } = \phi X _ { t } + Z _ { t + 1 }$ . The corresponding model for $\{ Y _ { t } ^ { * } \}$ is then, from (9.6.1),

$$
Y _ {t} ^ {*} = G _ {t} ^ {*} X _ {t} + W _ {t} ^ {*}, t = 1, 2, \ldots ,
$$

where

$$
X _ {t + 1} = F _ {t} X _ {t} + V _ {t}, t = 1, 2, \dots ,
$$

$$
F _ {t} = \phi , \quad G _ {t} ^ {*} = \left\{ \begin{array}{l l} 1 & \text {i f} t \neq 2, \\ 0 & \text {i f} t = 2, \end{array} \right. \quad V _ {t} = Z _ {t + 1}, \quad W _ {t} ^ {*} = \left\{ \begin{array}{l l} 0 & \text {i f} t \neq 2, \\ N _ {t} & \text {i f} t = 2, \end{array} \right.
$$

$$
Q _ {t} = \sigma^ {2}, \qquad R _ {t} ^ {*} = \left\{ \begin{array}{l l} 0 & \text {i f} t \neq 2, \\ 1 & \text {i f} t = 2, \end{array} \right. \qquad S _ {t} ^ {*} = 0,
$$

and $\begin{array} { r } { X _ { 1 } = \sum _ { j = 0 } ^ { \infty } \phi ^ { j } Z _ { 1 - j } } \end{array}$ . Starting from the initial conditions

$$
\hat {X} _ {1} = 0, \qquad \Omega_ {1} = \sigma^ {2} / \left(1 - \phi^ {2}\right),
$$

and applying the recursions (9.4.1) and (9.4.2), we find (Problem 9.19) that

$$
\Theta_ {t} \varDelta_ {t} ^ {- 1} = \left\{ \begin{array}{l l} \phi & \text {i f} t = 1, 3, 4, 5, \\ 0 & \text {i f} t = 2, \end{array} \right. \quad \varOmega_ {t} = \left\{ \begin{array}{l l} \sigma^ {2} / \left(1 - \phi^ {2}\right) & \text {i f} t = 1, \\ \sigma^ {2} \left(1 + \phi^ {2}\right) & \text {i f} t = 3, \\ \sigma^ {2} & \text {i f} t = 2, 4, 5, \end{array} \right.
$$

and

$$
\hat {X} _ {1} = 0, \quad \hat {X} _ {2} = \phi Y _ {1}, \quad \hat {X} _ {3} = \phi^ {2} Y _ {1}, \quad \hat {X} _ {4} = \phi Y _ {3}, \quad \hat {X} _ {5} = \phi Y _ {4}.
$$

From (9.4.7) and (9.4.9) with $h = 1$ , we find that

$$
\hat {Y} _ {1} ^ {*} = 0, \quad \hat {Y} _ {2} ^ {*} = 0, \quad \hat {Y} _ {3} ^ {*} = \phi^ {2} Y _ {1}, \quad \hat {Y} _ {4} ^ {*} = \phi Y _ {3}, \quad \hat {Y} _ {5} ^ {*} = \phi Y _ {4},
$$

with corresponding mean squared errors

$$
\Delta_ {1} ^ {*} = \sigma^ {2} / \left(1 - \phi^ {2}\right), \quad \Delta_ {2} ^ {*} = 1, \quad \Delta_ {3} ^ {*} = \sigma^ {2} \left(1 + \phi^ {2}\right), \quad \Delta_ {4} ^ {*} = \sigma^ {2}, \quad \Delta_ {5} ^ {*} = \sigma^ {2}.
$$

iii. From the preceding calculations we can now write the likelihood of the original data as

$$
\begin{array}{l} L _ {1} \left(\phi , \sigma^ {2}; y _ {1}, y _ {3}, y _ {4}, y _ {5}\right) = \sigma^ {- 4} (2 \pi) ^ {- 2} \left[ \left(1 - \phi^ {2}\right) / \left(1 + \phi^ {2}\right) \right] ^ {1 / 2} \\ \times \exp \left\{- \frac {1}{2 \sigma^ {2}} \left[ y _ {1} ^ {2} (1 - \phi^ {2}) + \frac {(y _ {3} - \phi^ {2} y _ {1}) ^ {2}}{1 + \phi^ {2}} + (y _ {4} - \phi y _ {3}) ^ {2} + (y _ {5} - \phi y _ {4}) ^ {2} \right] \right\}. \\ \end{array}
$$

![](images/f311ace184c6f93054e76b26fabeb494571915a6555d987952daa686371a8355.jpg)

Remark 1. If we are given observations $y _ { 1 - d } , y _ { 2 - d } , \ldots , y _ { 0 } , y _ { i _ { 1 } } , y _ { i _ { 2 } } , \ldots , y _ { i _ { r } }$ of an $\mathrm { A R I M A } ( p , d , q )$ process at times $1 - d , 2 - d , \ldots , 0 , i _ { 1 } , \ldots , i _ { r }$ , where $1 ~ \leq ~ i _ { 1 } ~ <$ $i _ { 2 } < \cdots < i _ { r } \leq n$ , a similar argument can be used to find the Gaussian likelihood of $y _ { i _ { 1 } } , \ldots , y _ { i _ { r } }$ conditional on $Y _ { 1 - d } = y _ { 1 - d }$ , $Y _ { 2 - d } = y _ { 2 - d } , \dots , Y _ { 0 } = y _ { 0 }$ . Missing values among the first $d$ observations $y _ { 1 - d } , y _ { 2 - d } , . . . , y _ { 0 }$ can be handled by treating them as unknown parameters for likelihood maximization. For more on ARIMA series with missing values see Brockwell and Davis (1991) and Ansley and Kohn (1985). -

# 9.6.2 Estimation of Missing Values for State-Space Models

Given that we observe only $\mathbf { Y } _ { i _ { 1 } }$ , Yi2 , . . . , Yir , $1 \leq i _ { 1 } < i _ { 2 } < \cdot \cdot \cdot < i _ { r } \leq n$ , where $\{ { \mathbf Y } _ { t } \}$ has the state-space representation (9.1.1) and (9.1.2), we now consider the problem of finding the minimum mean squared error estimators $P \left( \mathbf { Y } _ { t } | \mathbf { Y } _ { 0 } , \mathbf { Y } _ { i _ { 1 } } , \ldots , \mathbf { Y } _ { i _ { r } } \right)$ of $\mathbf { Y } _ { t }$ , $1 \leq t \leq n$ , where $\mathbf { Y } _ { 0 } = \mathbf { 1 }$ . To handle this problem we again use the modified process $\{ \mathbf { Y } _ { t } ^ { * } \}$ defined by (9.6.1) and (9.1.2) with $\mathbf { Y } _ { 0 } ^ { * } = \mathbf { 1 }$ . Since $\mathbf { Y } _ { s } ^ { * } = \mathbf { Y } _ { s }$ for $s \in \{ i _ { 1 } , \ldots , i _ { r } \}$ and $\mathbf { Y } _ { s } ^ { * } \perp \mathbf { X } _ { t }$ , ${ \bf Y } _ { 0 }$ for $1 \leq t \leq n$ and $s \notin \{ 0 , i _ { 1 } , \ldots , i _ { r } \}$ , we immediately obtain the minimum mean squared error state estimators

$$
P \left(\mathbf {X} _ {t} | \mathbf {Y} _ {0}, \mathbf {Y} _ {i _ {1}}, \dots , \mathbf {Y} _ {i _ {r}}\right) = P \left(\mathbf {X} _ {t} | \mathbf {Y} _ {0} ^ {*}, \mathbf {Y} _ {1} ^ {*}, \dots , \mathbf {Y} _ {n} ^ {*}\right), \quad 1 \leq t \leq n. \tag {9.6.6}
$$

The right-hand side can be evaluated by application of the Kalman fixed-point smoothing algorithm to the state-space model (9.6.1) and (9.1.2). For computational purposes the observed values of $\mathbf { Y } _ { t } ^ { * }$ , $t \notin \{ 0 , i _ { 1 } , \ldots , i _ { r } \}$ , are quite immaterial. They may, for example, all be set equal to zero, giving the sequence of observations of $\mathbf { Y } _ { t } ^ { * }$ defined in (9.6.4).

To evaluate $P \big ( \mathbf { Y } _ { t } | \mathbf { Y } _ { 0 } , \mathbf { Y } _ { i _ { 1 } } , \ldots , \mathbf { Y } _ { i _ { r } } \big ) , 1 \leq$ $1 \leq t \leq n$ , we use (9.6.6) and the relation

$$
\mathbf {Y} _ {t} = G _ {t} \mathbf {X} _ {t} + \mathbf {W} _ {t}. \tag {9.6.7}
$$

Since $E \left( \mathbf { V } _ { t } \mathbf { W } _ { t } ^ { \prime } \right) = S _ { t } = 0 , \quad t = 1 , \ldots , n$ , we find from (9.6.7) that

$$
P \left(\mathbf {Y} _ {t} \mid \mathbf {Y} _ {0}, \mathbf {Y} _ {i _ {1}}, \dots , \mathbf {Y} _ {i _ {r}}\right) = G _ {t} P \left(\mathbf {X} _ {t} \mid \mathbf {Y} _ {0} ^ {*}, \mathbf {Y} _ {1} ^ {*}, \dots , \mathbf {Y} _ {n} ^ {*}\right). \tag {9.6.8}
$$

# Example 9.6.2. An AR(1) Series with One Missing Observation

Consider the problem of estimating the missing value $Y _ { 2 }$ in Example 9.6.1 in terms of $Y _ { 0 } = 1$ , Y1, Y3, Y4, and $Y _ { 5 }$ . We start from the state-space model $X _ { t + 1 } = \phi X _ { t } + Z _ { t + 1 }$ , $Y _ { t } = X _ { t }$ , for $\{ Y _ { t } \}$ . The corresponding model for $\{ Y _ { t } ^ { * } \}$ is the one used in Example 9.6.1. Applying the Kalman smoothing equations to the latter model, we find that

$$
P _ {1} X _ {2} = \phi Y _ {1}, \quad P _ {2} X _ {2} = \phi Y _ {1}, \quad P _ {3} X _ {2} = \frac {\phi (Y _ {1} + Y _ {3})}{(1 + \phi^ {2})},
$$

$$
P _ {4} X _ {2} = P _ {3} X _ {2}, \quad P _ {5} X _ {2} = P _ {3} X _ {2},
$$

$$
\Omega_ {2, 2} = \sigma^ {2}, \qquad \Omega_ {2, 3} = \phi \sigma^ {2}, \qquad \Omega_ {2, t} = 0, \quad t \geq 4,
$$

and

$$
\varOmega_ {2 | 1} = \sigma^ {2}, \quad \varOmega_ {2 | 2} = \sigma^ {2}, \quad \varOmega_ {2 | t} = \frac {\sigma^ {2}}{(1 + \phi^ {2})}, \quad t \geq 3,
$$

where $P _ { t } ( \cdot )$ here denotes $P \left( \cdot | Y _ { 0 } ^ { * } , \ldots , Y _ { t } ^ { * } \right)$ and $\Omega _ { t , n }$ , $\Omega _ { t | n }$ are defined correspondingly. We deduce from (9.6.8) that the minimum mean squared error estimator of the missing value $Y _ { 2 }$ is

$$
P _ {5} Y _ {2} = P _ {5} X _ {2} = \frac {\phi \left(Y _ {1} + Y _ {3}\right)}{\left(1 + \phi^ {2}\right)},
$$

with mean squared error

$$
\Omega_ {2 | 5} = \frac {\sigma^ {2}}{\left(1 + \phi^ {2}\right)}.
$$

![](images/05a12500a9a2811d0efffe331b95d797905650a9dc3d29289020f172db5bbc40.jpg)

Remark 2. Suppose we have observations $Y _ { 1 - d }$ , Y2 d, . . . , Y0, Yi , . . . , Yi $( 1 \leq i _ { 1 } <$ $i _ { 2 } \cdots < i _ { r } \leq n )$ of an $\mathbf { A R I M A } ( p , d , q )$ process. Determination of the best linear estimates of the missing values $Y _ { t } , ~ t ~ \notin ~ \{ i _ { 1 } , \ldots , i _ { r } \}$ , in terms of $Y _ { t } , ~ t ~ \in ~ \{ i _ { 1 } , \ldots , i _ { r } \}$ , and the components of $\begin{array} { r c l } { \mathbf { Y } _ { 0 } } & { : = } & { ( Y _ { 1 - d } , Y _ { 2 - d } , . . . , Y _ { 0 } ) ^ { \prime } } \end{array}$ can be carried out as in Example 9.6.2 using the state-space representation of the ARIMA series $\{ Y _ { t } \}$ from Example 9.3.3 and the Kalman recursions for the corresponding state-space model for $\{ Y _ { t } ^ { * } \}$ defined by (9.6.1) and (9.1.2). See Brockwell and Davis (1991) for further details. -

We close this section with a brief discussion of a direct approach to estimating missing observations. This approach is often more efficient than the methods just described, especially if the number of missing observations is small and we have a simple (e.g., autoregressive) model. Consider the general problem of computing $E ( \mathbf { X } | \mathbf { Y } )$ when the random vector $( { \bf X } ^ { \prime } , { \bf Y } ^ { \prime } ) ^ { \prime }$ has a multivariate normal distribution with mean 0 and covariance matrix $\Sigma$ . (In the missing observation problem, think of X as the vector of the missing observations and Y as the vector of observed values.) Then the joint probability density function of $\mathbf { X }$ and Y can be written as

$$
f _ {\mathbf {X}, \mathbf {Y}} (\mathbf {x}, \mathbf {y}) = f _ {\mathbf {X} | \mathbf {Y}} (\mathbf {x} | \mathbf {y}) f _ {\mathbf {Y}} (\mathbf {y}), \tag {9.6.9}
$$

where $f _ { \mathbf { X } | \mathbf { Y } } ( \mathbf { x } | \mathbf { y } )$ is a multivariate normal density with mean $E ( \mathbf { X } | \mathbf { Y } )$ and covariance matrix $\Sigma _ { \mathbf { x } | \mathbf { Y } }$ (see Proposition A.3.1). In particular,

$$
f _ {\mathbf {x} | \mathbf {Y}} (\mathbf {x} | \mathbf {y}) = \frac {1}{\sqrt {(2 \pi) ^ {q} \det \Sigma_ {\mathbf {X} | \mathbf {Y}}}} \exp \left\{- \frac {1}{2} (\mathbf {x} - E (\mathbf {X} | \mathbf {y})) ^ {\prime} \Sigma_ {\mathbf {X} | \mathbf {Y}} ^ {- 1} (\mathbf {x} - E (\mathbf {X} | \mathbf {y})) \right\}, \tag {9.6.10}
$$

where $q \ = \ \dim ( \mathbf { X } )$ . It is clear from (9.6.10) that $f _ { \mathbf { X } | \mathbf { Y } } ( \mathbf { x } | \mathbf { y } )$ (and also $f _ { \mathbf { X } , \mathbf { Y } } ( \mathbf { x } , \mathbf { y } ) )$ is maximum when $\mathbf { x } = E ( \mathbf { X } | \mathbf { y } )$ . Thus, the best estimator of $\mathbf { X }$ in terms of $\mathbf { Y }$ can be found by maximizing the joint density of X and Y with respect to x. For autoregressive processes it is relatively straightforward to carry out this optimization, as shown in the following example.

# Example 9.6.3. Estimating Missing Observations in an AR Process

Suppose $\{ Y _ { t } \}$ is the $\operatorname { A R } ( p )$ process defined by

$$
Y _ {t} = \phi_ {1} Y _ {t - 1} + \dots + \phi_ {p} Y _ {t - p} + Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} \left(0, \sigma^ {2}\right),
$$

and $\mathbf { Y } = ( Y _ { i _ { 1 } } , \ldots , Y _ { i _ { r } } ) ^ { \prime }$ , with $1 \leq i _ { 1 } < \cdots < i _ { r } \leq n$ , are the observed values. If there are no missing observations in the first $p$ observations, then the best estimates of the missing values are found by minimizing

$$
\sum_ {t = p + 1} ^ {n} \left(Y _ {t} - \phi_ {1} Y _ {t - 1} - \dots - \phi_ {p} Y _ {t - p}\right) ^ {2} \tag {9.6.11}
$$

with respect to the missing values (see Problem 9.20). For the AR(1) model in Example 9.6.2, minimization of (9.6.11) is equivalent to minimizing

$$
\left(Y _ {2} - \phi Y _ {1}\right) ^ {2} + \left(Y _ {3} - \phi Y _ {2}\right) ^ {2}
$$

with respect to $Y _ { 2 }$ . Setting the derivative of this expression with respect to $Y _ { 2 }$ equal to 0 and solving for $Y _ { 2 }$ we obtain $E ( Y _ { 2 } | Y _ { 1 } , Y _ { 3 } , Y _ { 4 } , Y _ { 5 } ) = \phi ( Y _ { 1 } + Y _ { 3 } ) / \left( 1 + \phi ^ { 2 } \right)$ .

# 9.7 The EM Algorithm

The expectation-maximization (EM) algorithm is an iterative procedure for computing the maximum likelihood estimator when only a subset of the complete data set is available. Dempster et al. (1977) demonstrated the wide applicability of the EM algorithm and are largely responsible for popularizing this method in statistics. Details regarding the convergence and performance of the EM algorithm can be found in $\mathrm { { W u } }$ (1983).

In the usual formulation of the EM algorithm, the “complete” data vector W is made up of “observed” data Y (sometimes called incomplete data) and “unobserved” data X. In many applications, X consists of values of a “latent” or unobserved process occurring in the specification of the model. For example, in the state-space model of Section 9.1, Y could consist of the observed vectors $\mathbf { Y } _ { 1 } , \ldots , \mathbf { Y } _ { n }$ and $\mathbf { X }$ of the unobserved state vectors $\mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n }$ . The EM algorithm provides an iterative procedure for computing the maximum likelihood estimator based only on the observed data Y. Each iteration of the EM algorithm consists of two steps. If $\theta ^ { ( i ) }$ denotes the estimated value of the parameter $\theta$ after $i$ iterations, then the two steps in the $( i + 1 )$ th iteration are

E-step. Calcul $\operatorname { a t e } Q ( \theta | \theta ^ { ( i ) } ) = E _ { \theta ^ { ( i ) } } \left[ \ell ( \theta ; { \bf X } , { \bf Y } ) | { \bf Y } \right]$

and

M-step. Maximize $Q ( \theta | \theta ^ { ( i ) } )$ with respect to $\theta$ .

Then $\theta ^ { ( i + 1 ) }$ is set equal to the maximizer of $Q$ in the M-step. In the E-step, $\ell ( \theta ; { \mathbf x } , { \mathbf y } ) =$ $\ln f ( \mathbf { x } , \mathbf { y } ; { \boldsymbol { \theta } } )$ , and $E _ { \theta ^ { ( i ) } } ( \cdot | \mathbf { Y } )$ denotes the conditional expectation relative to the conditional density $f \bigl ( \mathbf { x } | \mathbf { y } ; \boldsymbol { \theta } ^ { ( i ) } \bigr ) = f \bigl ( \mathbf { x } , \mathbf { y } ; \boldsymbol { \theta } ^ { ( i ) } \bigr ) / f \bigl ( \mathbf { y } ; \boldsymbol { \theta } ^ { ( i ) } \bigr )$ .

It can be shown that $\ell \big ( \theta ^ { ( i ) } ; \mathbf { Y } \big )$ is nondecreasing in $i$ , and a simple heuristic argument shows that if $\theta ^ { ( i ) }$ has a limit $\hat { \theta }$ then $\hat { \theta }$ must be a solution of the likelihood equations $\ell ^ { \prime } ( \hat { \theta } ; { \bf Y } ) = 0$ . To see this, observe that $\ln f ( \mathbf { x } , \mathbf { y } ; \theta ) = \ln f ( \mathbf { x } | \mathbf { y } ; \theta ) + \ell ( \theta ; \mathbf { y } )$ , from which we obtain

$$
Q (\theta | \theta^ {(i)}) = \int (\ln f (\mathbf {x} | \mathbf {Y}; \theta)) f (\mathbf {x} | \mathbf {Y}; \theta^ {(i)}) d \mathbf {x} + \ell (\theta ; \mathbf {Y})
$$

and

$$
Q ^ {\prime} (\theta | \boldsymbol {\theta} ^ {(i)}) = \int \left[ \frac {\partial}{\partial \theta^ {\prime}} f (\mathbf {x} | \mathbf {Y}; \theta) \right] / f (\mathbf {x} | \mathbf {Y}; \theta) f (\mathbf {x} | \mathbf {Y}; \theta^ {(i)}) d \mathbf {x} + \ell^ {\prime} (\theta ; \mathbf {Y}).
$$

Now replacing $\theta$ with $\theta ^ { ( i + 1 ) }$ , noticing that $Q ^ { \prime } ( \theta ^ { ( i + 1 ) } | \theta ^ { ( i ) } ) = 0$ , and letting $i \to \infty$ , we find that

$$
0 = \int \frac {\partial}{\partial \theta} \left[ f (\mathbf {x} | \mathbf {Y}; \theta) \right] _ {\theta = \hat {\theta}} d \mathbf {x} + \ell^ {\prime} \left(\hat {\theta}; \mathbf {Y}\right) = \ell^ {\prime} \left(\hat {\theta}; \mathbf {Y}\right).
$$

The last equality follows from the fact that

$$
0 = \frac {\partial}{\partial \theta} (1) = \frac {\partial}{\partial \theta} \left[ \int (f (\mathbf {x} | \mathbf {Y}; \theta) d \mathbf {x} \right] _ {\theta = \hat {\theta}} = \int \left[ \frac {\partial}{\partial \theta} f (\mathbf {x} | \mathbf {Y}; \theta) \right] _ {\theta = \hat {\theta}} d \mathbf {x}.
$$

The computational advantage of the EM algorithm over direct maximization of the likelihood is most pronounced when the calculation and maximization of the exact likelihood is difficult as compared with the maximization of $Q$ in the $M$ -step. (There are some applications in which the maximization of $Q$ can easily be carried out explicitly.)

# 9.7.1 Missing Data

The EM algorithm is particularly useful for estimation problems in which there are missing observations. Suppose the complete data set consists of $Y _ { 1 } , \dots , Y _ { n }$ of which $r$ are observed and $n - r$ are missing. Denote the observed and missing data by $\mathbf { Y } =$ $( Y _ { i _ { 1 } } , \ldots , Y _ { i _ { r } } ) ^ { \prime }$ and $\mathbf { X } = ( Y _ { j _ { 1 } } , \ldots , Y _ { j _ { n - r } } ) ^ { \prime }$ , respectively. Assuming that $\mathbf { W } = ( \mathbf { X } ^ { \prime } , \mathbf { Y } ^ { \prime } ) ^ { \prime }$ has a multivariate normal distribution with mean 0 and covariance matrix $\Sigma$ , which depends on the parameter $\pmb \theta$ , the log-likelihood of the complete data is given by

$$
\ell (\pmb {\theta}; \mathbf {W}) = - \frac {n}{2} \ln (2 \pi) - \frac {1}{2} \ln \det (\Sigma) - \frac {1}{2} \mathbf {W} ^ {\prime} \Sigma \mathbf {W}.
$$

The E-step requires that we compute the expectation of $\ell ( \pmb \theta ; \mathbf W )$ with respect to the conditional distribution of $\mathbf { W }$ given $\mathbf { Y }$ with $\pmb { \theta } \mathrm { = } \pmb { \theta } ^ { ( i ) }$ . Writing $\Sigma ( \pmb \theta )$ as the block matrix

$$
\Sigma = \left[ \begin{array}{c c} \Sigma_ {1 1} & \Sigma_ {1 2} \\ \Sigma_ {2 1} & \Sigma_ {2 2} \end{array} \right],
$$

which is conformable with $\mathbf { X }$ and $\mathbf { Y }$ , the conditional distribution of $\mathbf { W }$ given $\mathbf { Y }$ is multivariate normal with mean $[ \hat { \bf { Y } } ]$ and covariance matrix $\left[ \begin{array} { c c } { { \Sigma _ { 1 1 } } _ { | 2 } ( \pmb { \theta } ) } & { { 0 } } \\ { { 0 } } & { { 0 } } \end{array} \right]$ , where $\hat { \textbf { X } } =$ $E _ { \theta } ( \mathbf { X } | \mathbf { Y } ) = \Sigma _ { 1 2 } \Sigma _ { 2 2 } ^ { - 1 } \mathbf { Y }$ and $\Sigma _ { 1 1 | 2 } ( \pmb { \theta } ) = \Sigma _ { 1 1 } - \Sigma _ { 1 2 } \Sigma _ { 2 2 } ^ { - 1 } \Sigma _ { 2 1 }$ (see Proposition A.3.1). Using Problem A.8, we have

$$
E _ {\boldsymbol {\theta} ^ {(i)}} \left[ \left(\mathbf {X} ^ {\prime}, \mathbf {Y} ^ {\prime}\right) \boldsymbol {\Sigma} ^ {- 1} (\boldsymbol {\theta}) \left(\mathbf {X} ^ {\prime}, \mathbf {Y} ^ {\prime}\right) ^ {\prime} | \mathbf {Y} \right] = \operatorname {t r a c e} \left(\boldsymbol {\Sigma} _ {1 1 | 2} \left(\boldsymbol {\theta} ^ {(i)}\right) \boldsymbol {\Sigma} _ {1 1 | 2} ^ {- 1} (\boldsymbol {\theta})\right) + \hat {\mathbf {W}} ^ {\prime} \boldsymbol {\Sigma} ^ {- 1} (\boldsymbol {\theta}) \hat {\mathbf {W}},
$$

where $\hat { \bf W } = \left( \hat { \bf X } ^ { \prime } , { \bf Y } ^ { \prime } \right) ^ { \prime }$ . It follows that

$$
\mathcal {Q} \left(\boldsymbol {\theta} | \boldsymbol {\theta} ^ {(i)}\right) = \ell \left(\boldsymbol {\theta}, \hat {\mathbf {W}}\right) - \frac {1}{2} \operatorname {t r a c e} \left(\Sigma_ {1 1 | 2} \left(\boldsymbol {\theta} ^ {(i)}\right) \Sigma_ {1 1 | 2} ^ {- 1} (\boldsymbol {\theta})\right).
$$

The first term on the right is the log-likelihood based on the complete data, but with X replaced by its “best estimate” $\hat { \mathbf { X } }$ calculated from the previous iteration. If the increments $\pmb { \theta } ^ { ( i + 1 ) } - \pmb { \theta } ^ { ( i ) }$ are small, then the second term on the right is nearly constant $( \approx n - r )$ and can be ignored. For ease of computation in this application we shall use the modified version

$$
\tilde {Q} (\boldsymbol {\theta} | \boldsymbol {\theta} ^ {(i)}) = \ell (\boldsymbol {\theta}; \hat {\mathbf {W}}).
$$

With this adjustment, the steps in the EM algorithm are as follows:

E-step. Calculate $E _ { \theta ^ { ( i ) } } ( \mathbf { X } | \mathbf { Y } )$ (e.g., with the Kalman fixed-point smoother) and form $\ell \big ( \pmb { \theta } ; \hat { \mathbf { W } } \big )$ .

M-step. Find the maximum likelihood estimator for the “complete” data problem, i.e., maximize $\ell ( \pmb \theta : \hat { \mathbf W } )$ . For ARMA processes, ITSM can be used directly, with the missing values replaced with their best estimates computed in the E-step.

# Example 9.7.1. The Lake Data

It was found in Example 5.2.5 that the AR(2) model

$$
W _ {t} - 1. 0 4 1 5 W _ {t - 1} + 0. 2 4 9 4 W _ {t - 2} = Z _ {t}, \{Z _ {t} \} \sim \mathrm {W N} (0, . 4 7 9 0)
$$

was a good fit to the mean-corrected lake data $\{ W _ { t } \}$ . To illustrate the use of the EM algorithm for missing data, consider fitting an AR(2) model to the mean-corrected data assuming that there are 10 missing values at times $t = 1 7$ , 24, 31, 38, 45, 52, 59, 66, 73, and 80. We start the algorithm at iteration 0 with $\hat { \phi } _ { 1 } ^ { ( 0 ) } = \hat { \phi } _ { 2 } ^ { ( 0 ) } = 0$ φˆ . Since this initial model represents white noise, the first E-step gives, in the notation used above, $\hat { W } _ { 1 7 } = \cdot \cdot \cdot = \hat { W } _ { 8 0 } = 0 .$ . Replacing the “missing” values of the mean-corrected lake data with 0 and fitting a mean-zero AR(2) model to the resulting complete data set using the maximum likelihood option in ITSM, we find that $\hat { \phi } _ { 1 } ^ { ( 1 ) } \stackrel {  } { = } 0 . 7 \dot { 2 } 5 2$ , $\hat { \phi } _ { 2 } ^ { ( 1 ) } = 0 . 0 2 3 \dot { 6 }$ . (Examination of the plots of the ACF and PACF of this new data set suggests an AR(1) as a better model. This is also borne out by the small estimated value of $\phi _ { 2 }$ .) The updated missing values at times $t = 1 7 , 2 4 , \dots , 8 0$ are found (see Section 9.6 and Problem 9.21) by minimizing

$$
\sum_ {j = 0} ^ {2} \left(W _ {t + j} - \hat {\phi} _ {1} ^ {(1)} W _ {t + j - 1} - \hat {\phi} _ {2} ^ {(1)} W _ {t + j - 2}\right) ^ {2}
$$

with respect to $W _ { t }$ . The solution is given by

$$
\hat {W} _ {t} = \frac {\hat {\phi} _ {2} ^ {(1)} (W _ {t - 2} + W _ {t + 2}) + (\hat {\phi} _ {1} ^ {(1)} - \hat {\phi} _ {1} ^ {(1)} \hat {\phi} _ {2} ^ {(1)}) (W _ {t - 1} + W _ {t + 1})}{1 + (\hat {\phi} _ {1} ^ {(1)}) ^ {2} + (\hat {\phi} _ {2} ^ {(1)}) ^ {2}}.
$$

The M-step of iteration 1 is then carried out by fitting an AR(2) model using ITSM applied to the updated data set. As seen in the summary of the results reported in Table 9.1, the EM algorithm converges in four iterations with the final parameter estimates reasonably close to the fitted model based on the complete data set. (In Table 9.1, estimates of the missing values are recorded only for the first three.) Also notice how $- 2 \ell \left( \pmb { \theta } ^ { ( i ) } , \mathbf { W } \right)$ decreases at every iteration. The standard errors of the parameter estimates produced from the last iteration of ITSM are based on a “complete” data set and, as such, underestimate the true sampling errors. Formulae for adjusting the standard errors to reflect the true sampling error based on the observed data can be found in Dempster et al. (1977).

# 9.8 Generalized State-Space Models

As in Section 9.1, we consider a sequence of state variables $\{ X _ { t } , \ t \geq 1 \}$ and a sequence of observations $\{ Y _ { t } , \ t \geq 1 \}$ . For simplicity, we consider only one-dimensional state and observation variables, since extensions to higher dimensions can be carried out with

Table 9.1 Estimates of the missing observations at times $\begin{array} { r l r } { t } & { { } = } & { 1 7 , } \end{array}$ , 24, 31 and the AR estimates using the EM algorithm in Example 9.7.1   

<table><tr><td>Iteration i</td><td>W17</td><td>W24</td><td>W31</td><td>φ(i)1</td><td>φ(i)2</td><td>-2ℓ(θ(i),W)</td></tr><tr><td>0</td><td></td><td></td><td></td><td>0</td><td>0</td><td>322.60</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0.7252</td><td>0.0236</td><td>244.76</td></tr><tr><td>2</td><td>0.534</td><td>0.205</td><td>0.746</td><td>1.0729</td><td>-0.2838</td><td>203.57</td></tr><tr><td>3</td><td>0.458</td><td>0.393</td><td>0.821</td><td>1.0999</td><td>-0.3128</td><td>202.25</td></tr><tr><td>4</td><td>0.454</td><td>0.405</td><td>0.826</td><td>1.0999</td><td>-0.3128</td><td>202.25</td></tr></table>

little change. Throughout this section it will be convenient to write $\mathbf { Y } ^ { ( t ) }$ and $\mathbf { X } ^ { ( t ) }$ for the $t$ dimensional column vectors $\mathbf { Y } ^ { ( t ) } = ( Y _ { 1 } , Y _ { 2 } , \ldots , Y _ { t } ) ^ { \prime }$ and $\mathbf { X } ^ { ( t ) } = ( X _ { 1 } , X _ { 2 } , \ldots , X _ { t } ) ^ { \prime }$ .

There are two important types of state-space models, “parameter driven” and “observation driven,” both of which are frequently used in time series analysis. The observation equation is the same for both, but the state vectors of a parameter-driven model evolve independently of the past history of the observation process, while the state vectors of an observation-driven model depend on past observations.

# 9.8.1 Parameter-Driven Models

In place of the observation and state equations (9.1.1) and (9.1.2), we now make the assumptions that $Y _ { t }$ given $\left( X _ { t } , \mathbf { X } ^ { ( t - 1 ) } , \bar { \mathbf { Y } } ^ { ( t - 1 ) } \right)$ is independent of $\left( \mathbf { X } ^ { ( t - 1 ) } , \mathbf { Y } ^ { ( t - 1 ) } \right)$ with conditional probability density

$$
p \left(y _ {t} \mid x _ {t}\right) := p \left(y _ {t} \mid x _ {t}, \mathbf {x} ^ {(t - 1)}, \mathbf {y} ^ {(t - 1)}\right), \quad t = 1, 2, \dots , \tag {9.8.1}
$$

and that $X _ { t + 1 }$ given $\left( X _ { t } , \mathbf { X } ^ { ( t - 1 ) } , \mathbf { Y } ^ { ( t ) } \right)$ is independent of $\left( \mathbf { X } ^ { \left( t - 1 \right) } , \mathbf { Y } ^ { \left( t \right) } \right)$ with conditional density function

$$
p \left(x _ {t + 1} \mid x _ {t}\right) := p \left(x _ {t + 1} \mid x _ {t}, \mathbf {x} ^ {(t - 1)}, \mathbf {y} ^ {(t)}\right) \quad t = 1, 2, \dots . \tag {9.8.2}
$$

We shall also assume that the initial state $X _ { 1 }$ has probability density $p _ { 1 }$ . The joint density of the observation and state variables can be computed directly from (9.8.1)– (9.8.2) as

$$
\begin{array}{l} p \left(y _ {1}, \dots , y _ {n}, x _ {1}, \dots , x _ {n}\right) = p \left(y _ {n} \mid x _ {n}, \mathbf {x} ^ {(n - 1)}, \mathbf {y} ^ {(n - 1)}\right) p \left(x _ {n}, \mathbf {x} ^ {(n - 1)}, \mathbf {y} ^ {(n - 1)}\right) \\ = p \left(y _ {n} \mid x _ {n}\right) p \left(x _ {n} \mid \mathbf {x} ^ {(n - 1)}, \mathbf {y} ^ {(n - 1)}\right) p \left(\mathbf {y} ^ {(n - 1)}, \mathbf {x} ^ {(n - 1)}\right) \\ = p \left(y _ {n} \mid x _ {n}\right) p \left(x _ {n} \mid x _ {n - 1}\right) p \left(\mathbf {y} ^ {(n - 1)}, \mathbf {x} ^ {(n - 1)}\right) \\ = \dots \\ = \left(\prod_ {j = 1} ^ {n} p \left(y _ {j} \mid x _ {j}\right)\right) \left(\prod_ {j = 2} ^ {n} p \left(x _ {j} \mid x _ {j - 1}\right)\right) p _ {1} \left(x _ {1}\right), \\ \end{array}
$$

and since (9.8.2) implies that $\{ X _ { t } \}$ is Markov (see Problem 9.22),

$$
p \left(y _ {1}, \dots , y _ {n} \mid x _ {1}, \dots , x _ {n}\right) = \left(\prod_ {j = 1} ^ {n} p \left(y _ {j} \mid x _ {j}\right)\right). \tag {9.8.3}
$$

We conclude that $Y _ { 1 } , \dots , Y _ { n }$ are conditionally independent given the state variables $X _ { 1 } , \ldots , X _ { n }$ , so that the dependence structure of $\{ Y _ { t } \}$ is inherited from that of the state process $\{ X _ { t } \}$ . The sequence of state variables $\{ X _ { t } \}$ is often referred to as the hidden or latent generating process associated with the observed process.

In order to solve the filtering and prediction problems in this setting, we shall determine the conditional densities $p \left( x _ { t } | \mathbf { y } ^ { ( t ) } \right)$ of $X _ { t }$ given $\mathbf { Y } ^ { ( t ) }$ , and $p \left( x _ { t } | \mathbf { y } ^ { ( t - 1 ) } \right)$ of $X _ { t }$ given $\mathbf { Y } ^ { ( t - 1 ) }$ , respectively. The minimum mean squared error estimates of $X _ { t }$ based on $\mathbf { Y } ^ { ( t ) }$ and $\mathbf { Y } ^ { ( t - 1 ) }$ can then be computed as the conditional expectations, $E \left( X _ { t } | \mathbf { Y } ^ { ( t ) } \right)$ and $E \left( X _ { t } | \mathbf { Y } ^ { ( t - 1 ) } \right)$ .

An application of Bayes’s theorem, using the assumption that the distribution of $Y _ { t }$ given $\bar { \left( { { X } _ { t } } , { { \mathbf { X } } ^ { ( t - 1 ) } } , { { \mathbf { Y } } ^ { ( t - 1 ) } } \right) }$ does not depend on $\left( \mathbf { X } ^ { ( t - 1 ) } , \bar { \mathbf { Y } } ^ { ( t - 1 ) } \right)$ , yields

$$
p \left(x _ {t} | \mathbf {y} ^ {(t)}\right) = p \left(y _ {t} \mid x _ {t}\right) p \left(x _ {t} \mid \mathbf {y} ^ {(t - 1)}\right) / p \left(y _ {t} \mid \mathbf {y} ^ {(t - 1)}\right) \tag {9.8.4}
$$

and

$$
p \left(x _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) = \int p \left(x _ {t} \mid \mathbf {y} ^ {(t)}\right) p \left(x _ {t + 1} \mid x _ {t}\right) d \mu \left(x _ {t}\right). \tag {9.8.5}
$$

(The integral relative to $d \mu ( x _ { t } )$ in (9.8.4) is interpreted as the integral relative to $d x _ { t }$ in the continuous case and as the sum over all values of $x _ { t }$ in the discrete case.) The initial condition needed to solve these recursions is

$$
p \left(x _ {1} \mid \mathbf {y} ^ {(0)}\right) := p _ {1} \left(x _ {1}\right). \tag {9.8.6}
$$

The factor $p \left( y _ { t } | \mathbf { y } ^ { ( t - 1 ) } \right)$ appearing in the denominator of (9.8.4) is just a scale factor, determined by the condition $\begin{array} { r c l } { \int p \left( x _ { t } | \mathbf { y } ^ { ( t ) } \right) d \mu ( x _ { t } ) } & { = } & { 1 } \end{array}$ . In the generalized statespace setup, prediction of a future state variable is less important than forecasting a future value of the observations. The relevant forecast density can be computed from (9.8.5) as

$$
p \left(y _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) = \int p \left(y _ {t + 1} \mid x _ {t + 1}\right) p \left(x _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) d \mu \left(x _ {t + 1}\right). \tag {9.8.7}
$$

Equations (9.8.1)–(9.8.2) can be regarded as a Bayesian model specification. A classical Bayesian model has two key assumptions. The first is that the data $Y _ { 1 } , \dots , Y _ { t }$ , given an unobservable parameter $\mathbf { X } ^ { ( t ) }$ in our case), are independent with specified conditional distribution. This corresponds to (9.8.3). The second specifies a prior distribution for the parameter value. This corresponds to (9.8.2). The posterior distribution is then the conditional distribution of the parameter given the data. In the present setting the posterior distribution of the component $X _ { t }$ of $\mathbf { X } ^ { ( t ) }$ is determined by the solution (9.8.4) of the filtering problem.

Example 9.8.1. Consider the simplified version of the linear state-space model of Section 9.1,

$$
Y _ {t} = G X _ {t} + W _ {t}, \quad \{W _ {t} \} \sim \operatorname {i i d} \mathrm {N} (0, R), \tag {9.8.8}
$$

$$
X _ {t + 1} = F X _ {t} + V _ {t}, \quad \{V _ {t} \} \sim \operatorname {i i d} \mathrm {N} (0, Q), \tag {9.8.9}
$$

where the noise sequences $\{ W _ { t } \}$ and $\{ V _ { t } \}$ are independent of each other. For this model the probability densities in (9.8.1)–(9.8.2) become

$$
p _ {1} \left(x _ {1}\right) = n \left(x _ {1}; E X _ {1}, \operatorname {V a r} \left(X _ {1}\right)\right), \tag {9.8.10}
$$

$$
p \left(y _ {t} \mid x _ {t}\right) = n \left(y _ {t}; G x _ {t}, R\right), \tag {9.8.11}
$$

$$
p \left(x _ {t + 1} \mid x _ {t}\right) = n \left(x _ {t + 1}; F x _ {t}, Q\right), \tag {9.8.12}
$$

where $n \left( x ; \mu , \sigma ^ { 2 } \right)$ is the normal density with mean $\mu$ and variance $\sigma ^ { 2 }$ defined in Example (a) of Section A.1.

To solve the filtering and prediction problems in this new framework, we first observe that the filtering and prediction densities in (9.8.4) and (9.8.5) are both normal. We shall write them, using the notation of Section 9.4, as

$$
p \left(x _ {t} \mid \mathbf {Y} ^ {(t)}\right) = n \left(x _ {t}; X _ {t \mid t}, \Omega_ {t \mid t}\right) \tag {9.8.13}
$$

and

$$
p \left(x _ {t + 1} \mid \mathbf {Y} ^ {(t)}\right) = n \left(x _ {t + 1}; \hat {X} _ {t + 1}, \Omega_ {t + 1}\right). \tag {9.8.14}
$$

From (9.8.5), (9.8.12), (9.8.13), and (9.8.14), we find that

$$
\begin{array}{l} \hat {X} _ {t + 1} = \int_ {- \infty} ^ {\infty} x _ {t + 1} p (x _ {t + 1} | \mathbf {Y} ^ {(t)}) d x _ {t + 1} \\ = \int_ {- \infty} ^ {\infty} x _ {t + 1} \int_ {- \infty} ^ {\infty} p \left(x _ {t} \mid \mathbf {Y} ^ {(t)}\right) p \left(x _ {t + 1} \mid x _ {t}\right) d x _ {t} d x _ {t + 1} \\ = \int_ {- \infty} ^ {\infty} p (x _ {t} | \mathbf {Y} ^ {(t)}) \left[ \int_ {- \infty} ^ {\infty} x _ {t + 1} p (x _ {t + 1} | x _ {t}) d x _ {t + 1} \right] d x _ {t} \\ = \int_ {- \infty} ^ {\infty} F x _ {t} p (x _ {t} | \mathbf {Y} ^ {(t)}) d x _ {t} \\ = F X _ {t | t} \\ \end{array}
$$

and (see Problem 9.23)

$$
\Omega_ {t + 1} = F ^ {2} \Omega_ {t | t} + Q.
$$

Substituting the corresponding densities (9.8.11) and (9.8.14) into (9.8.4), we find by equating the coefficient of $x _ { t } ^ { 2 }$ on both sides of (9.8.4) that

$$
\Omega_ {t | t} ^ {- 1} = G ^ {2} R ^ {- 1} + \Omega_ {t} ^ {- 1} = G ^ {2} R ^ {- 1} + (F ^ {2} \Omega_ {t - 1 | t - 1} + Q) ^ {- 1}
$$

and

$$
X _ {t | t} = \hat {X} _ {t} + \Omega_ {t | t} G R ^ {- 1} \left(Y _ {t} - G \hat {X} _ {t}\right).
$$

Also, from (9.8.4) with $p \left( x _ { 1 } | \mathbf { y } ^ { ( 0 ) } \right) = n ( x _ { 1 } ; E X _ { 1 } , \Omega _ { 1 } )$ we obtain the initial conditions

$$
X _ {1 | 1} = E X _ {1} + \Omega_ {1 | 1} G R ^ {- 1} (Y _ {1} - G E X _ {1})
$$

and

$$
\Omega_ {1 | 1} ^ {- 1} = G ^ {2} R ^ {- 1} + \Omega_ {1} ^ {- 1}.
$$

The Kalman prediction and filtering recursions of Section 9.4 give the same results for $\hat { X } _ { t }$ and $X _ { t \mid t }$ , since for Gaussian systems best linear mean square estimation is equivalent to best mean square estimation.

# Example 9.8.2. A non-Gaussian Example

In general, the solution of the recursions (9.8.4) and (9.8.5) presents substantial computational problems. Numerical methods for dealing with non-Gaussian models are discussed by Sorenson and Alspach (1971) and Kitagawa (1987). Here we shall illustrate the recursions (9.8.4) and (9.8.5) in a very simple special case. Consider the state equation

$$
X _ {t} = a X _ {t - 1}, \tag {9.8.15}
$$

with observation density

$$
p \left(y _ {t} \mid x _ {t}\right) = \frac {\left(\pi x _ {t}\right) ^ {y _ {t}} e ^ {- \pi x _ {t}}}{y _ {t} !}, \quad y _ {t} = 0, 1, \dots , \tag {9.8.16}
$$

where $\pi$ is a constant between 0 and 1. The relationship in (9.8.15) implies that the transition density [in the discrete sense—see the comment after (9.8.5)] for the state variables is

$$
p (x _ {t + 1} | x _ {t}) = \left\{ \begin{array}{l l} 1, & \text {i f} x _ {t + 1} = a x _ {t}, \\ 0, & \text {o t h e r w i s e}. \end{array} \right.
$$

We shall assume that $X _ { 1 }$ has the gamma density function

$$
p _ {1} (x _ {1}) = g (x _ {1}; \alpha , \lambda) = \frac {\lambda^ {\alpha} x _ {1} ^ {\alpha - 1} e ^ {- \lambda x _ {1}}}{\Gamma (\alpha)}, \quad x _ {1} > 0.
$$

(This is a simplified model for the evolution of the number $X _ { t }$ of individuals at time t infected with a rare disease, in which $X _ { t }$ is treated as a continuous rather than an integer-valued random variable. The observation $Y _ { t }$ represents the number of infected individuals observed in a random sample consisting of a small fraction $\pi$ of the population at time t.) Because the transition distribution of $\{ X _ { t } \}$ is not continuous, we use the integrated version of (9.8.5) to compute the prediction density. Thus,

$$
\begin{array}{l} P \left(X _ {t} \leq x | \mathbf {y} ^ {(t - 1)}\right) = \int_ {0} ^ {\infty} P \left(X _ {t} \leq x \mid x _ {t - 1}\right) p \left(x _ {t - 1} \mid \mathbf {y} ^ {(t - 1)}\right) d x _ {t - 1} \\ = \int_ {0} ^ {x / a} p \left(x _ {t - 1} | \mathbf {y} ^ {(t - 1)}\right) d x _ {t - 1}. \\ \end{array}
$$

Differentiation with respect to $x$ gives

$$
p \left(x _ {t} | \mathbf {y} ^ {(t - 1)}\right) = a ^ {- 1} p _ {X _ {t - 1} | \mathbf {Y} ^ {(t - 1)}} \left(a ^ {- 1} x _ {t} | \mathbf {y} ^ {(t - 1)}\right). \tag {9.8.17}
$$

Now applying (9.8.4), we find that

$$
\begin{array}{l} p \left(x _ {1} \mid y _ {1}\right) = p \left(y _ {1} \mid x _ {1}\right) p _ {1} \left(x _ {1}\right) / p \left(y _ {1}\right) \\ = \left(\frac {(\pi x _ {1}) ^ {y _ {1}} e ^ {- \pi x _ {1}}}{y _ {1} !}\right) \left(\frac {\lambda^ {\alpha} x _ {1} ^ {\alpha - 1} e ^ {- \lambda x _ {1}}}{\Gamma (\alpha)}\right) \left(\frac {1}{p (y _ {1})}\right) \\ = c \left(y _ {1}\right) x _ {1} ^ {\alpha + y _ {1} - 1} e ^ {- (\pi + \lambda) x _ {1}}, \quad x _ {1} > 0, \\ \end{array}
$$

where $c ( y _ { 1 } )$ is an integration factor ensuring that $p ( \cdot | y _ { 1 } )$ integrates to 1. Since $p ( \cdot | y _ { 1 } )$ has the form of a gamma density, we deduce (see Example (d) of Section A.1) that

$$
p \left(x _ {1} \mid y _ {1}\right) = g \left(x _ {1}; \alpha_ {1}, \lambda_ {1}\right), \tag {9.8.18}
$$

where $\alpha _ { 1 } = \alpha + y _ { 1 }$ and $\lambda _ { 1 } = \lambda + \pi$ . The prediction density, calculated from (9.8.5) and (9.8.18), is

$$
\begin{array}{l} p \left(x _ {2} | \mathbf {y} ^ {(1)}\right) = a ^ {- 1} p _ {X _ {1} | \mathbf {Y} ^ {(1)}} \left(a ^ {- 1} x _ {2} | \mathbf {y} ^ {(1)}\right) \\ = a ^ {- 1} g \left(a ^ {- 1} x _ {2}; \alpha_ {1}, \lambda_ {1}\right) \\ = g \left(x _ {2}; \alpha_ {1}, \lambda_ {1} / a\right). \\ \end{array}
$$

Iterating the recursions (9.8.4) and (9.8.5) and using (9.8.17), we find that for $t \geq 1$ ,

$$
p \left(x _ {t} \mid \mathbf {y} ^ {(t)}\right) = g \left(x _ {t}; \alpha_ {t}, \lambda_ {t}\right) \tag {9.8.19}
$$

and

$$
\begin{array}{l} p \left(x _ {t + 1} | \mathbf {y} ^ {(t)}\right) = a ^ {- 1} g \left(a ^ {- 1} x _ {t + 1}; \alpha_ {t}, \lambda_ {t}\right) \\ = g \left(x _ {t + 1}; \alpha_ {t}, \lambda_ {t} / a\right), \tag {9.8.20} \\ \end{array}
$$

where $\alpha _ { t } = \alpha _ { t - 1 } + y _ { t } = \alpha + y _ { 1 } + \cdot \cdot \cdot + y _ { t }$ and $\lambda _ { t } = \lambda _ { t - 1 } / a + \pi = \lambda a ^ { 1 - t } +$ $\pi \left( 1 - a ^ { - t } \right) / ( 1 - a ^ { - 1 } )$ . In particular, the minimum mean squared error estimate of $x _ { t }$ based on $\mathbf { y } ^ { ( t ) }$ is the conditional expectation $\alpha _ { t } / \lambda _ { t }$ with conditional variance $\alpha _ { t } / \lambda _ { t } ^ { 2 }$ . From (9.8.7) the probability density of $Y _ { t + 1 }$ given $\mathbf { Y } ^ { ( t ) }$ is

$$
\begin{array}{l} p \left(y _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) = \int_ {0} ^ {\infty} \left(\frac {\left(\pi x _ {t + 1}\right) ^ {y _ {t + 1}} e ^ {- \pi x _ {t + 1}}}{y _ {t + 1} !}\right) g \left(x _ {t + 1}; \alpha_ {t}, \lambda_ {t} / a\right) d x _ {t + 1} \\ = \frac {\Gamma (\alpha_ {t} + y _ {t + 1})}{\Gamma (\alpha_ {t}) \Gamma (y _ {t + 1} + 1)} \left(1 - \frac {\pi}{\lambda_ {t + 1}}\right) ^ {\alpha_ {t}} \left(\frac {\pi}{\lambda_ {t + 1}}\right) ^ {y _ {t + 1}} \\ = n b \left(y _ {t + 1}; \alpha_ {t}, 1 - \pi / \lambda_ {t + 1}\right), \quad y _ {t + 1} = 0, 1, \dots , \\ \end{array}
$$

where $n b ( \boldsymbol { y } ; \boldsymbol { \alpha } , p )$ is the negative binomial density defined in example (i) of Section A.1. Conditional on $\mathbf { Y } ^ { ( t ) }$ , the best one-step predictor of $Y _ { t + 1 }$ is therefore the mean, $\alpha _ { t } \pi / ( \lambda _ { t + 1 } - \pi )$ , of this negative binomial distribution. The conditional mean squared error of the predictor is $\operatorname { V a r } \bigl ( Y _ { t + 1 } | \mathbf { Y } ^ { ( t ) } \bigr ) = \alpha _ { t } \pi \lambda _ { t + 1 } / ( \lambda _ { t + 1 } - \pi ) ^ { 2 }$ (see Problem 9.25).

# Example 9.8.3. A Model for Time Series of Counts

We often encounter time series in which the observations represent count data. One such example is the monthly number of newly recorded cases of poliomyelitis in the U.S. for the years 1970–1983 plotted in Figure 9-6. Unless the actual counts are large and can be approximated by continuous variables, Gaussian and linear time series models are generally inappropriate for analyzing such data. The parameter-driven specification provides a flexible class of models for modeling count data. We now discuss a specific model based on a Poisson observation density. This model is similar to the one presented by Zeger (1988) for analyzing the polio data. The observation density is assumed to be Poisson with mean $\exp \{ x _ { t } \}$ , i.e.,

$$
p \left(y _ {t} \mid x _ {t}\right) = \frac {e ^ {x _ {t} y _ {t}} e ^ {- e ^ {x _ {t}}}}{y _ {t} !}, \quad y _ {t} = 0, 1, \dots , \tag {9.8.21}
$$

while the state variables are assumed to follow a regression model with Gaussian AR(1) noise. If $\mathbf { u } _ { t } = ( u _ { t 1 } , \ldots , u _ { t k } ) ^ { \prime }$ are the regression variables, then

$$
X _ {t} = \beta^ {\prime} \mathbf {u} _ {t} + W _ {t}, \tag {9.8.22}
$$

where $\beta$ is a $k$ -dimensional regression parameter and

$$
W _ {t} = \phi W _ {t - 1} + Z _ {t}, \quad \{Z _ {t} \} \sim \text {I I D N} (0, \sigma^ {2}).
$$

The transition density function for the state variables is then

$$
p \left(x _ {t + 1} \mid x _ {t}\right) = n \left(x _ {t + 1}; \beta^ {\prime} \mathbf {u} _ {t + 1} + \phi \left(x _ {t} - \beta^ {\prime} \mathbf {u} _ {t}\right), \sigma^ {2}\right). \tag {9.8.23}
$$

The case $\sigma ^ { 2 } = 0$ corresponds to a log-linear model with Poisson noise.

Estimation of the parameters $\pmb { \theta } = \left( \beta ^ { \prime } , \phi , \sigma ^ { 2 } \right) ^ { \prime }$ in the model by direct numerical maximization of the likelihood function is difficult, since the likelihood cannot be written down in closed form. (From (9.8.3) the likelihood is the $n$ -fold integral,

$$
\int_ {- \infty} ^ {\infty} \dots \int_ {- \infty} ^ {\infty} \exp \left\{\sum_ {t = 1} ^ {n} \left(x _ {t} y _ {t} - e ^ {x _ {t}}\right) \right\} L (\boldsymbol {\theta}; \mathbf {x} ^ {(n)}) (d x _ {1} \dots d x _ {n}) / \prod_ {i = 1} ^ {n} (y _ {i}!),
$$

![](images/df58ea363197e24139434faeac1deec985ebea37cc46b9f0b86f39cccca2477d.jpg)  
Figure 9-6 Monthly number of U.S. cases of polio, January 1970–December 1983

where $L ( \pmb \theta ; \mathbf x )$ is the likelihood based on $X _ { 1 } , \dots , X _ { n } .$ .) To overcome this difficulty, Chan and Ledolter (1995) proposed an algorithm, called Monte Carlo EM (MCEM), whose iterates $\theta ^ { ( i ) }$ converge to the maximum likelihood estimate. To apply this algorithm, first note that the conditional distribution of $\mathbf { Y } ^ { ( n ) }$ given $\mathbf { X } ^ { ( n ) }$ does not depend on $\pmb \theta$ , so that the likelihood based on the complete data $\left( \mathbf { X } ^ { ( n ) \prime } , \mathbf { Y } ^ { ( n ) \prime } \right) ^ { \prime }$ is given by

$$
L \left(\boldsymbol {\theta}; \mathbf {X} ^ {(n)}, \mathbf {Y} ^ {(n)}\right) = f \left(\mathbf {Y} ^ {(n)} | \mathbf {X} ^ {(n)}\right) L \left(\boldsymbol {\theta}; \mathbf {X} ^ {(n)}\right).
$$

The E-step of the algorithm (see Section 9.7) requires calculation of

$$
\begin{array}{l} Q (\boldsymbol {\theta} | \boldsymbol {\theta} ^ {(i)}) = E _ {\boldsymbol {\theta} ^ {(i)}} \left(\ln L (\boldsymbol {\theta}; \mathbf {X} ^ {(n)}, \mathbf {Y} ^ {(n)}) | \mathbf {Y} ^ {(n)}\right) \\ = E _ {\boldsymbol {\theta} ^ {(i)}} \left(\ln f (\mathbf {Y} ^ {(n)} | \mathbf {X} ^ {(n)}) | \mathbf {Y} ^ {(n)}\right) + E _ {\boldsymbol {\theta} ^ {(i)}} \left(\ln L (\boldsymbol {\theta}; \mathbf {X} ^ {(n)}) | \mathbf {Y} ^ {(n)}\right). \\ \end{array}
$$

We delete the first term from the definition of $Q$ , since it is independent of $\pmb \theta$ and hence plays no role in the M-step of the EM algorithm. The new $Q$ is redefined as

$$
Q (\boldsymbol {\theta} | \boldsymbol {\theta} ^ {(i)}) = E _ {\boldsymbol {\theta} ^ {(i)}} \left(\ln L (\boldsymbol {\theta}; \mathbf {X} ^ {(n)}) | \mathbf {Y} ^ {(n)}\right). \tag {9.8.24}
$$

Even with this simplification, direct calculation of $Q$ is still intractable. Suppose for the moment that it is possible to generate replicates of $\mathbf { X } ^ { ( n ) }$ from the conditional distribution of $\mathbf { X } ^ { ( n ) }$ given $\mathbf { Y } ^ { ( n ) }$ when $\pmb \theta = \pmb \theta ^ { ( i ) }$ . If we denote $m$ independent replicates of $\mathbf { X } ^ { ( n ) }$ by $\mathbf { X } _ { 1 } ^ { ( n ) } , \ldots , \bar { \mathbf { X } _ { m } ^ { ( n ) } }$ , then a Monte Carlo approximation to $Q$ in (9.8.24) is given by

$$
Q _ {m} \left(\boldsymbol {\theta} | \boldsymbol {\theta} ^ {(i)}\right) = \frac {1}{m} \sum_ {j = 1} ^ {m} \ln L \left(\boldsymbol {\theta}; \mathbf {X} _ {j} ^ {(n)}\right).
$$

The M-step is easy to carry out using $Q _ { m }$ in place of $Q$ (especially if we condition on $X _ { 1 } = 0$ in all the simulated replicates), since $L$ is just the Gaussian likelihood of the regression model with AR(1) noise treated in Section 6.6. The difficult steps in the algorithm are the generation of replicates of $\mathbf { X } ^ { ( n ) }$ given $\mathbf { Y } ^ { ( n ) }$ and the choice of m. Chan and Ledolter (1995) discuss the use of the Gibb’s sampler for generating the desired replicates and give some guidelines on the choice of $m$ .

In their analyses of the polio data, Zeger (1988) and Chan and Ledolter (1995) included as regression components an intercept, a slope, and harmonics at periods of 6 and 12 months. Specifically, they took

$$
\mathbf {u} _ {t} = (1, t / 1 0 0 0, \cos (2 \pi t / 1 2), \sin (2 \pi t / 1 2), \cos (2 \pi t / 6), \sin (2 \pi t / 6)) ^ {\prime}.
$$

![](images/3ecb4a1899162455c0bbb6d4e1b3778d516042fa0e96068e202e57a2efbf5d61.jpg)  
Figure 9-7 Trend estimate for the monthly number of U.S. cases of polio, January 1970–December 1983

The implementation of Chan and Ledolter’s MCEM method by Kuk and Cheng (1994) gave estimates $\hat { \beta } = ( 0 . 2 4 7 , - 3 . 8 7 1 , 0 . 1 6 2 , - 0 . 4 8 2 , 0 . 4 1 4 , - 0 . 0 1 1 ) ^ { \prime } ,$ ， $\hat { \phi } = 0 . 6 4 8$ , and $\hat { \sigma } ^ { 2 } = 0 . 2 8 1$ . The estimated trend function $\hat { \boldsymbol { \beta } } ^ { \prime } \mathbf { u } _ { t }$ is displayed in Figure 9-7. The negative coefficient of $t / 1 0 0 0$ indicates a slight downward trend in the monthly number of polio cases.

![](images/1f22411bf0a65df2bc75c0e2dbe2e54588ba2f858209702ebf1663eb0aa4ba76.jpg)

# 9.8.2 Observation-Driven Models

Again we assume that $Y _ { t }$ , conditional on $\left( X _ { t } , \mathbf { X } ^ { ( t - 1 ) } , \mathbf { Y } ^ { ( t - 1 ) } \right)$ , is independent of $\left( \mathbf { X } ^ { ( t - 1 ) } , \mathbf { Y } ^ { ( t - 1 ) } \right)$ . These models are specified by the conditional densities

$$
p \left(y _ {t} \mid x _ {t}\right) = p \left(y _ {t} \mid \mathbf {x} ^ {(t)}, \mathbf {y} ^ {(t - 1)}\right), \quad t = 1, 2, \dots , \tag {9.8.25}
$$

$$
p \left(x _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) = p _ {X _ {t + 1} \mid \mathbf {Y} ^ {(t)}} \left(x _ {t + 1} \mid \mathbf {y} ^ {(t)}\right), \quad t = 0, 1, \dots , \tag {9.8.26}
$$

where $p \big ( x _ { 1 } | \mathbf { y } ^ { ( 0 ) } \big ) : = p _ { 1 } ( x _ { 1 } )$ for some prespecified initial density $p _ { 1 } ( x _ { 1 } )$ . The advantage of the observation-driven state equation (9.8.26) is that the posterior distribution of $X _ { t }$ given $\mathbf { Y } ^ { ( t ) }$ can be computed directly from (9.8.4) without the use of the updating formula (9.8.5). This then allows for easy computation of the forecast function in (9.8.7) and hence of the joint density function of $( Y _ { 1 } , \ldots , Y _ { n } ) ^ { \prime }$ ,

$$
p \left(y _ {1}, \dots , y _ {n}\right) = \prod_ {t = 1} ^ {n} p \left(y _ {t} \mid \mathbf {y} ^ {(t - 1)}\right). \tag {9.8.27}
$$

On the other hand, the mechanism by which the state $X _ { t - 1 }$ makes the transition to $X _ { t }$ is not explicitly defined. In fact, without further assumptions there may be state sequences $\{ X _ { t } \}$ and $\{ X _ { t } ^ { * } \}$ with different distributions for which both (9.8.25) and (9.8.26) hold (see Example 9.8.6). Both sequences, however, lead to the same joint distribution, given by (9.8.27), for $Y _ { 1 } , \dots , Y _ { n }$ $Y _ { n }$ . The ambiguity in the specification of the distribution of the state variables can be removed by assuming that $X _ { t + 1 }$ given $\left( \mathbf { X } ^ { ( t ) } , \mathbf { Y } ^ { ( t ) } \right)$ is independent of $\mathbf { X } ^ { ( t ) }$ , with conditional distribution (9.8.26), i.e.,

$$
p \left(x _ {t + 1} | \mathbf {x} ^ {(t)}, \mathbf {y} ^ {(t)}\right) = p _ {X _ {t + 1} | \mathbf {Y} ^ {(t)}} \left(x _ {t + 1} | \mathbf {y} ^ {(t)}\right). \tag {9.8.28}
$$

With this modification, the joint density of $\mathbf { Y } ^ { ( n ) }$ and $\mathbf { X } ^ { ( n ) }$ is given by (cf. (9.8.3))

$$
\begin{array}{l} p \left(\mathbf {y} ^ {(n)}, \mathbf {x} ^ {(n)}\right) = p \left(y _ {n} \mid x _ {n}\right) p \left(x _ {n} \mid \mathbf {y} ^ {(n - 1)}\right) p \left(\mathbf {y} ^ {(n - 1)}, \mathbf {x} ^ {(n - 1)}\right) \\ = \dots \\ = \prod_ {t = 1} ^ {n} \left(p \left(y _ {t} \mid x _ {t}\right) p \left(x _ {t} \mid \mathbf {y} ^ {(t - 1)}\right)\right). \\ \end{array}
$$

# Example 9.8.4. An AR(1) Process

An AR(1) process with iid noise can be expressed as an observation driven model. Suppose $\{ Y _ { t } \}$ is the AR(1) process

$$
Y _ {t} = \phi Y _ {t - 1} + Z _ {t},
$$

where $\{ Z _ { t } \}$ is an iid sequence of random variables with mean 0 and some probability density function $f ( x )$ . Then with $X _ { t } : = Y _ { t - 1 }$ we have

$$
p \left(y _ {t} \mid x _ {t}\right) = f \left(y _ {t} - \phi x _ {t}\right)
$$

and

$$
p \left(x _ {t + 1} | \mathbf {y} ^ {(t)}\right) = \left\{ \begin{array}{l l} 1, & \text {i f} x _ {t + 1} = y _ {t}, \\ 0, & \text {o t h e r w i s e}. \end{array} \right.
$$

![](images/50d1e321bdaafdf0aedbf7f2ab65ff576c282690dbdae57b079100e0c1e89a24.jpg)

Example 9.8.5. Suppose the observation-equation density is given by

$$
p \left(y _ {t} \mid x _ {t}\right) = \frac {x _ {t} ^ {y _ {t}} e ^ {- x _ {t}}}{y _ {t} !}, \quad y _ {t} = 0, 1, \dots , \tag {9.8.29}
$$

and the state equation (9.8.26) is

$$
p \left(x _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) = g \left(x _ {t}; \alpha_ {t}, \lambda_ {t}\right), \tag {9.8.30}
$$

where $\alpha _ { t } = \alpha + y _ { 1 } + \cdot \cdot \cdot + y _ { t }$ and $\lambda _ { t } = \lambda + t$ . It is possible to give a parameterdriven specification that gives rise to the same state equation (9.8.30). Let $\{ X _ { t } ^ { * } \}$ be the parameter-driven state variables, where $X _ { t } ^ { * } = X _ { t - 1 } ^ { * }$ and $X _ { 1 } ^ { * }$ has a gamma distribution with parameters $\alpha$ and λ. (This corresponds to the model in Example 9.8.2 with $\pi =$ $a = 1 .$ ) Then from (9.8.19) we see that $p \left( x _ { t } ^ { * } | \mathbf { y } ^ { ( t ) } \right) = g ( x _ { t } ^ { * } ; \alpha _ { t } , \lambda _ { t } )$ , which coincides with the state equation (9.8.30). If $\{ X _ { t } \}$ are the state variables whose joint distribution is specified through (9.8.28), then $\{ X _ { t } \}$ and $\{ X _ { t } ^ { * } \}$ cannot have the same joint distributions. To see this, note that

$$
p \left(x _ {t + 1} ^ {*} | x _ {t} ^ {*}\right) = \left\{ \begin{array}{l l} 1, & \text {i f} x _ {t + 1} ^ {*} = x _ {t} ^ {*}, \\ 0, & \text {o t h e r w i s e}, \end{array} \right.
$$

while

$$
p \left(x _ {t + 1} | \mathbf {x} ^ {(t)}, \mathbf {y} ^ {(t)}\right) = p \left(x _ {t + 1} | \mathbf {y} ^ {(t)}\right) = g \left(x _ {t}; \alpha_ {t}, \lambda_ {t}\right).
$$

If the two sequences had the same joint distribution, then the latter density could take only the values 0 and 1, which contradicts the continuity (as a function of $x _ { t }$ ) of this density.

![](images/22826885f798e7077441ba049c015086eed2cfa2352e0af067a5e4600b8cca2e.jpg)

# 9.8.3 Exponential Family Models

The exponential family of distributions provides a large and flexible class of distributions for use in the observation equation. The density in the observation equation is said to belong to an exponential family (in natural parameterization) if

$$
p \left(y _ {t} \mid x _ {t}\right) = \exp \left\{y _ {t} x _ {t} - b \left(x _ {t}\right) + c \left(y _ {t}\right) \right\}, \tag {9.8.31}
$$

where $b ( \cdot )$ is a twice continuously differentiable function and $c ( y _ { t } )$ does not depend on $x _ { t }$ . This family includes the normal, exponential, gamma, Poisson, binomial, and many other distributions frequently encountered in statistics. Detailed properties of the exponential family can be found in Barndorff-Nielsen (1978), and an excellent treatment of its use in the analysis of linear models is given by McCullagh and Nelder (1989). We shall need only the following important facts:

$$
e ^ {b \left(x _ {t}\right)} = \int \exp \left\{y _ {t} x _ {t} + c \left(y _ {t}\right) \right\} v \left(d y _ {t}\right), \tag {9.8.32}
$$

$$
b ^ {\prime} \left(x _ {t}\right) = E \left(Y _ {t} \mid x _ {t}\right), \tag {9.8.33}
$$

$$
b ^ {\prime \prime} \left(x _ {t}\right) = \operatorname {V a r} \left(Y _ {t} \mid x _ {t}\right) := \int y _ {t} ^ {2} p \left(y _ {t} \mid x _ {t}\right) v \left(d y _ {t}\right) - \left[ b ^ {\prime} \left(x _ {t}\right) \right] ^ {2}, \tag {9.8.34}
$$

where integration with respect to $\nu ( d y _ { t } )$ means integration with respect to $d y _ { t }$ in the continuous case and summation over all values of $y _ { t }$ in the discrete case.

Proof. The first relation is simply the statement that $p ( y _ { t } | x _ { t } )$ integrates to 1. The second relation is established by differentiating both sides of (9.8.32) with respect to $x _ { t }$ and then multiplying through by $e ^ { - b ( x _ { t } ) }$ (for justification of the differentiation under the integral sign see Barndorff-Nielsen 1978). The last relation is obtained by differentiating (9.8.32) twice with respect to $x _ { t }$ and simplifying. 

# Example 9.8.6. The Poisson Case

If the observation $Y _ { t }$ , given $X _ { t } = x _ { t }$ , has a Poisson distribution of the form (9.8.21), then

$$
p \left(y _ {t} \mid x _ {t}\right) = \exp \left\{y _ {t} x _ {t} - e ^ {x _ {t}} - \ln y _ {t}!\right\}, \quad y _ {t} = 0, 1, \dots , \tag {9.8.35}
$$

which has the form (9.8.31) with $b ( x _ { t } ) ~ = ~ e ^ { x _ { t } }$ and $c ( y _ { t } ) = - \ln { y _ { t } } !$ . From (9.8.33) we easily find that $E ( Y _ { t } | x _ { t } ) = b ^ { \prime } ( x _ { t } ) = e ^ { x _ { t } }$ . This parameterization is slightly different from the one used in Examples 9.8.2 and 9.8.5, where the conditional mean of $Y _ { t }$ given $x _ { t }$ was $\pi x _ { t }$ and not $e ^ { x _ { t } }$ . For this observation equation, define the family of densities

$$
f (x; \alpha , \lambda) = \exp \left\{\alpha x - \lambda b (x) + A (\alpha , \lambda) \right\}, \quad - \infty <   x <   \infty , \tag {9.8.36}
$$

where $\alpha > 0$ and $\lambda > 0$ are parameters and $A ( \alpha , \lambda ) = - \ln { \Gamma ( \alpha ) } + \alpha \ln { \lambda }$ . Now consider state densities of the form

$$
p \left(x _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) = f \left(x _ {t + 1}; \alpha_ {t + 1 | t}, \lambda_ {t + 1 | t}\right), \tag {9.8.37}
$$

where $\alpha _ { t + 1 | t }$ and $\lambda _ { t + 1 | t }$ are, for the moment, unspecified functions of $\mathbf { y } ^ { ( t ) }$ . (The subscript $t + 1 | t$ on the parameters is a shorthand way to indicate dependence on the conditional distribution of $X _ { t + 1 }$ given $\mathbf { Y } ^ { ( t ) }$ .) With this specification of the state densities, the parameters $\alpha _ { t + 1 | t }$ are related to the best one-step predictor of $Y _ { t }$ through the formula

$$
\alpha_ {t + 1 | t} / \lambda_ {t + 1 | t} = \hat {Y} _ {t + 1} := E \left(Y _ {t + 1} \mid \mathbf {y} ^ {(t)}\right). \tag {9.8.38}
$$

Proof. We have from (9.8.7) and (9.8.33) that

$$
\begin{array}{l} E \left(Y _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) = \sum_ {y _ {t + 1} = 0} ^ {\infty} \int_ {- \infty} ^ {\infty} y _ {t + 1} p \left(y _ {t + 1} \mid x _ {t + 1}\right) p \left(x _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) d x _ {t + 1} \\ = \int_ {- \infty} ^ {\infty} b ^ {\prime} (x _ {t + 1}) p \left(x _ {t + 1} | \mathbf {y} ^ {(t)}\right) d x _ {t + 1}. \\ \end{array}
$$

Addition and subtraction of $\alpha _ { t + 1 | t } / \lambda _ { t + 1 | t }$ then gives

$$
\begin{array}{l} E \left(Y _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) = \int_ {- \infty} ^ {\infty} \left(b ^ {\prime} \left(x _ {t + 1}\right) - \frac {\alpha_ {t + 1 \mid t}}{\lambda_ {t + 1 \mid t}}\right) p \left(x _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) d x _ {t + 1} + \frac {\alpha_ {t + 1 \mid t}}{\lambda_ {t + 1 \mid t}} \\ = \int_ {- \infty} ^ {\infty} - \lambda_ {t + 1 | t} ^ {- 1} p ^ {\prime} (x _ {t + 1} | \mathbf {y} ^ {(t)}) d x _ {t + 1} + \frac {\alpha_ {t + 1 | t}}{\lambda_ {t + 1 | t}} \\ = \left[ - \lambda_ {t + 1 | t} ^ {- 1} p \left(x _ {t + 1} | \mathbf {y} ^ {(t)}\right) \right] _ {x _ {t + 1} = - \infty} ^ {x _ {t + 1} = \infty} + \frac {\alpha_ {t + 1 | t}}{\lambda_ {t + 1 | t}} \\ = \frac {\alpha_ {t + 1 | t}}{\lambda_ {t + 1 | t}}. \\ \end{array}
$$

Letting $A _ { t | t - 1 } = A ( \alpha _ { t | t - 1 } , \lambda _ { t | t - 1 } )$ , we can write the posterior density of $X _ { t }$ given $\mathbf { Y } ^ { ( t ) }$ as

$$
\begin{array}{l} p \left(x _ {t} \mid \mathbf {y} ^ {(t)}\right) = \exp \left\{y _ {t} x _ {t} - b \left(x _ {t}\right) + c \left(y _ {t}\right) \right\} \exp \left\{\alpha_ {t | t - 1} x _ {t} - \lambda_ {t | t - 1} b \left(x _ {t}\right) \right. \\ + A _ {t \mid t - 1} \} / p \left(y _ {t} \mid \mathbf {y} ^ {(t - 1)}\right) \\ = \exp \left\{\lambda_ {t | t} \left(\alpha_ {t | t} x _ {t} - b (x _ {t})\right) - A _ {t | t} \right\}, \\ = f \left(x _ {t}; \alpha_ {t}, \lambda_ {t}\right), \\ \end{array}
$$

where we find, by equating coefficients of $x _ { t }$ and $b ( x _ { t } )$ , that the coefficients $\lambda _ { t }$ and $\alpha _ { t }$ are determined by

$$
\lambda_ {t} = 1 + \lambda_ {t | t - 1}, \tag {9.8.39}
$$

$$
\alpha_ {t} = y _ {t} + \alpha_ {t | t - 1}. \tag {9.8.40}
$$

The family of prior densities in (9.8.37) is called a conjugate family of priors for the observation equation (9.8.35), since the resulting posterior densities are again members of the same family.

As mentioned earlier, the parameters $\alpha _ { t \mid t - 1 }$ and $\lambda _ { t \mid t - 1 }$ can be quite arbitrary: Any nonnegative functions of $\mathbf { y } ^ { ( t - 1 ) }$ will lead to a consistent specification of the state densities. One convenient choice is to link these parameters with the corresponding parameters of the posterior distribution at time $t - 1$ through the relations

$$
\lambda_ {t + 1 \mid t} = \delta \lambda_ {t} (= \delta (1 + \lambda_ {t \mid t - 1})) \tag {9.8.41}
$$

$$
\alpha_ {t + 1 \mid t} = \delta \alpha_ {t} (= \delta \left(y _ {t} + \alpha_ {t \mid t - 1}\right)), \tag {9.8.42}
$$

where $0 < \delta < 1$ (see Remark 4 below). Iterating the relation (9.8.41), we see that

$$
\begin{array}{l} \lambda_ {t + 1 \mid t} = \delta (1 + \lambda_ {t \mid t - 1}) = \delta + \delta \lambda_ {t \mid t - 1} \\ = \delta + \delta (\delta + \delta \lambda_ {t - 2 | t - 2}) \\ = \dots \\ \end{array}
$$

$$
\begin{array}{l} = \delta + \delta^ {2} + \dots + \delta^ {t} \lambda_ {1 | 0} \tag {9.8.43} \\ \rightarrow \delta / (1 - \delta) \\ \end{array}
$$

as $t \to \infty$ . Similarly,

$$
\begin{array}{l} \alpha_ {t + 1 \mid t} = \delta y _ {t} + \delta \alpha_ {t \mid t - 1} \\ = \dots \\ = \delta y _ {t} + \delta^ {2} y _ {t - 1} + \dots + \delta^ {t} y _ {1} + \delta^ {t} \alpha_ {1 | 0}. \tag {9.8.44} \\ \end{array}
$$

For large t, we have the approximations

$$
\lambda_ {t + 1 \mid t} = \delta / (1 - \delta) \tag {9.8.45}
$$

and

$$
\alpha_ {t + 1 \mid t} = \delta \sum_ {j = 0} ^ {t - 1} \delta^ {j} y _ {t - j}, \tag {9.8.46}
$$

which are exact if $\lambda _ { 1 | 0 } = \delta / ( 1 - \delta )$ and $\alpha _ { 1 | 0 } = 0$ . From (9.8.38) the one-step predictors are linear and given by

$$
\hat {Y} _ {t + 1} = \frac {\alpha_ {t + 1 | t}}{\lambda_ {t + 1 | t}} = \frac {\sum_ {j = 0} ^ {t - 1} \delta^ {j} y _ {t - j} + \delta^ {t - 1} \alpha_ {1 | 0}}{\sum_ {j = 0} ^ {t - 1} \delta^ {j} + \delta^ {t - 1} \lambda_ {1 | 0}}. \tag {9.8.47}
$$

Replacing the denominator with its limiting value, or starting with $\lambda _ { 1 | 0 } = \delta / ( 1 - \delta )$ , we find that $\hat { Y } _ { t + 1 }$ is the solution of the recursions

$$
\hat {Y} _ {t + 1} = (1 - \delta) y _ {t} + \delta \hat {Y} _ {t}, \quad t = 1, 2, \dots , \tag {9.8.48}
$$

with initial condition $\hat { Y } _ { 1 } ~ = ~ ( 1 - \delta ) \delta ^ { - 1 } \alpha _ { 1 | 0 }$ . In other words, under the restrictions of (9.8.41) and (9.8.42), the best one-step predictors can be found by exponential smoothing.

![](images/80ef8da7d66c205c9aa08d1bce0bd58cebfeebf9d9fe4b550e5699c28a2d813b.jpg)

Remark 1. The preceding analysis for the Poisson-distributed observation equation holds, almost verbatim, for the general family of exponential densities (9.8.31). (One only needs to take care in specifying the correct range for $x$ and the allowable parameter space for $\alpha$ and $\lambda$ in (9.8.37).) The relations (9.8.43)–(9.8.44), as well as the exponential smoothing formula (9.8.48), continue to hold even in the more general setting, provided that the parameters $\alpha _ { t \mid t - 1 }$ and $\lambda _ { t \mid t - 1 }$ satisfy the relations (9.8.41)– (9.8.42). -

Remark 2. Equations (9.8.41)–(9.8.42) are equivalent to the assumption that the prior density of $X _ { t }$ given $\mathbf { y } ^ { ( t - 1 ) }$ is proportional to the $\delta$ -power of the posterior distribution of $X _ { t - 1 }$ given $\mathbf { Y } ^ { ( t - 1 ) }$ , or more succinctly that

$$
\begin{array}{l} f \left(x _ {t}; \alpha_ {t | t - 1}, \lambda_ {t | t - 1}\right) = f \left(x _ {t}; \delta \alpha_ {t - 1 | t - 1}, \delta \lambda_ {t - 1 | t - 1}\right) \\ \propto f ^ {\delta} \left(x _ {t}; \alpha_ {t - 1 | t - 1}, \lambda_ {t - 1 | t - 1}\right). \\ \end{array}
$$

This power relationship is sometimes referred to as the power steady model (Grunwald et al. 1993; Smith 1979). -

Remark 3. The transformed state variables $W _ { t } \ = \ e ^ { X _ { t } }$ have a gamma state density given by

$$
p \left(w _ {t + 1} | \mathbf {y} ^ {(t)}\right) = g \left(w _ {t + 1}; \alpha_ {t + 1 | t}, \lambda_ {t + 1 | t}\right)
$$

(see Problem 9.26). The mean and variance of this conditional density are

$$
E \left(W _ {t + 1} | \mathbf {y} ^ {(t)}\right) = \alpha_ {t + 1 | t} \quad \text {a n d} \quad \operatorname {V a r} \left(W _ {t + 1} | \mathbf {y} ^ {(t)}\right) = \alpha_ {t + 1 | t} / \lambda_ {t + 1 | t} ^ {2}.
$$

Remark 4. If we regard the random walk plus noise model of Example 9.2.1 as the prototypical state-space model, then from the calculations in Example 9.8.1 with $G =$ $F = 1$ , we have

$$
E \left(X _ {t + 1} | \mathbf {Y} ^ {(t)}\right) = E \left(X _ {t} | \mathbf {Y} ^ {(t)}\right)
$$

and

$$
\operatorname {V a r} \left(X _ {t + 1} | \mathbf {Y} ^ {(t)}\right) = \operatorname {V a r} \left(X _ {t} | \mathbf {Y} ^ {(t)}\right) + Q > \operatorname {V a r} \left(X _ {t} | \mathbf {Y} ^ {(t)}\right).
$$

The first of these equations implies that the best estimate of the next state is the same as the best estimate of the current state, while the second implies that the variance increases. Under the conditions (9.8.41), and (9.8.42), the same is also true for the state variables in the above model (see Problem 9.26). This was, in part, the rationale behind these conditions given in Harvey and Fernandes (1989). -

Remark 5. While the calculations work out neatly for the power steady model, Grunwald et al. (1994) have shown that such processes have degenerate sample paths for large t. In the Poisson example above, they argue that the observations $Y _ { t }$ converge to 0 as $t ~  ~ \infty$ (see Figure 9-12). Although such models may still be useful in practice for modeling series of moderate length, the efficacy of using such models for describing long-term behavior is doubtful. -

# Example 9.8.7. Goals Scored by England Against Scotland

The time series of the number of goals scored by England against Scotland in soccer matches played at Hampden Park in Glasgow is graphed in Figure 9-8. The matches have been played nearly every second year, with interruptions during the war years. We will treat the data $y _ { 1 } , \ldots , y _ { 5 2 }$ as coming from an equally spaced time series model $\{ Y _ { t } \}$ . Since the number of goals scored is small (see the frequency histogram in Figure 9-9), a model based on the Poisson distribution might be deemed appropriate. The observed relative frequencies and those based on a Poisson distribution with mean equal to $\bar { y } _ { 5 2 } = 1 . 2 6 9$ are contained in Table 9.2. The standard chi-squared goodness of fit test, comparing the observed frequencies with expected frequencies based on a Poisson model, has a $p$ -value of 0.02. The lack of fit with a Poisson distribution is hardly unexpected, since the sample variance (1.652) is much larger than the sample mean, while the mean and variance of the Poisson distribution are equal. In this case the data are said to be overdispersed in the sense that there is more variability in the data than one would expect from a sample of independent Poisson-distributed variables. Overdispersion can sometimes be explained by serial dependence in the data.

Dependence in count data can often be revealed by estimating the probabilities of transition from one state to another. Table 9.3 contains estimates of these probabilities, computed as the average number of one-step transitions from state $y _ { t }$ to state $y _ { t + 1 }$ . If the data were independent, then in each column the entries should be nearly the same. This is certainly not the case in Table 9.3. For example, England is very unlikely to be shut out or score 3 or more goals in the next match after scoring at least three goals in the previous encounter.

![](images/ed868bf27a0df1c35c07b89a3b04577d3d0c13f56aea914d0dd5954a25aa2d2a.jpg)  
Figure 9-8 Goals scored by England against Scotland at Hampden Park, Glasgow, 1872–1987

![](images/631f18ab6afe56770a296d486ef239cbb17f51452f4ec65fcda858c8e7c74ddc.jpg)  
Figure 9-9 Histogram of the data in Figure 9-8

Table 9.2 Relative frequency and fitted Poisson distribution of goals scored by England against Scotland   

<table><tr><td rowspan="2"></td><td colspan="6">Number of goals</td></tr><tr><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>Relative frequency</td><td>0.288</td><td>0.423</td><td>0.154</td><td>0.019</td><td>0.096</td><td>0.019</td></tr><tr><td>Poisson distribution</td><td>0.281</td><td>0.356</td><td>0.226</td><td>0.096</td><td>0.030</td><td>0.008</td></tr></table>

Harvey and Fernandes (1989) model the dependence in this data using an observation-driven model of the type described in Example 9.8.6. Their model assumes a Poisson observation equation and a log-gamma state equation:

$$
p (y _ {t} | x _ {t}) = \frac {\exp \left\{y _ {t} x _ {t} - e ^ {x _ {t}} \right\}}{y _ {t} !}, \quad y _ {t} = 0, 1, \dots ,
$$

$$
p \left(x _ {t} | \mathbf {y} ^ {(t - 1)}\right) = f \left(x _ {t}; \alpha_ {t | t - 1}, \lambda_ {t | t - 1}\right), \quad - \infty <   x <   \infty ,
$$

Table 9.3 Transition probabilities for the number of goals scored by England against Scotland   

<table><tr><td></td><td colspan="5">yt+1</td></tr><tr><td></td><td>p(yt+1|yt)</td><td>0</td><td>1</td><td>2</td><td>≥3</td></tr><tr><td rowspan="4">yt</td><td>0</td><td>0.214</td><td>0.500</td><td>0.214</td><td>0.072</td></tr><tr><td>1</td><td>0.409</td><td>0.272</td><td>0.136</td><td>0.182</td></tr><tr><td>2</td><td>0.250</td><td>0.375</td><td>0.125</td><td>0.250</td></tr><tr><td>≥3</td><td>0</td><td>0.857</td><td>0.143</td><td>0</td></tr></table>

Table 9.4 Prediction density of $Y _ { 5 3 }$ given $\mathbf { \pmb { Y } } ^ { ( 5 2 ) }$ for data in Figure 9-7   

<table><tr><td rowspan="2"></td><td colspan="6">Number of goals</td></tr><tr><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>p(y53|y(52))</td><td>0.472</td><td>0.326</td><td>0.138</td><td>0.046</td><td>0.013</td><td>0.004</td></tr></table>

for $t = 1 , 2 , \ldots$ , where $f$ is given by (9.8.36) and $\alpha _ { 1 | 0 } = 0 , \lambda _ { 1 | 0 } = 0$ . The power steady conditions (9.8.41)–(9.8.42) are assumed to hold for $\alpha _ { t \mid t - 1 }$ and $\lambda _ { t \mid t - 1 }$ . The only unknown parameter in the model is δ. The log-likelihood function for $\delta$ based on the conditional distribution of $y _ { 1 } , \ldots , y _ { 5 2 }$ given $y _ { 1 }$ is given by [see (9.8.27)]

$$
\ell (\delta , \mathbf {y} ^ {(n)}) = \sum_ {t = 1} ^ {n - 1} \ln p (y _ {t + 1} | \mathbf {y} ^ {(t)}), \tag {9.8.49}
$$

where $p \left( y _ { t + 1 } | \mathbf { y } ^ { ( t ) } \right)$ is the negative binomial density [see Problem 9.25(c)]

$$
p \left(y _ {t + 1} | \mathbf {y} ^ {(t)}\right) = n b \left(y _ {t + 1}; \alpha_ {t + 1 | t}, (1 + \lambda_ {t + 1 | t}) ^ {- 1}\right),
$$

with $\alpha _ { t + 1 | t }$ and $\lambda _ { t + 1 | t }$ as defined in (9.8.44) and (9.8.43). (For the goal data, $y _ { 1 } = 0$ , which implies $\alpha _ { 2 | 1 } = 0$ and hence that $p \left( y _ { 2 } | y ^ { ( 1 ) } \right)$ is a degenerate density with unit mass at $y _ { 2 } = 0$ . Harvey and Fernandes avoid this complication by conditioning the likelihood on $y ^ { ( \tau ) }$ , where $\tau$ is the time of the first nonzero data value.)

Maximizing this likelihood with respect to δ, we obtain $\hat { \delta } \ = \ 0 . 8 4 4$ . (Starting equations (9.8.43)–(9.8.44) with $\alpha _ { 1 | 0 } ~ = ~ 0$ and $\lambda _ { 1 | 0 } ~ = ~ \delta / ( 1 ~ - ~ \delta )$ , we obtain $\hat { \delta } \ = \ 0 . 7 3 2 .$ ) With 0.844 as our estimate of $\delta$ , the prediction density of the next observation $Y _ { 5 3 }$ given $\mathbf { y } ^ { ( 5 2 ) }$ is $n b ( y _ { 5 3 } ; \alpha _ { 5 3 | 5 2 }$ , $( 1 + \lambda _ { 5 3 | 5 2 } ) ^ { - 1 }$ . The first five values of this distribution are given in Table 9.4. Under this model, the probability that England will be held scoreless in the next match is 0.471. The one-step predictors, $\tilde { \hat { Y } } _ { 1 } ~ =$ $0 , \hat { Y } _ { 2 } , \ldots , \hat { Y } _ { 5 2 }$ $\hat { Y } _ { 5 2 }$ are graphed in Figure 9-10. (This graph can be obtained by using the ITSM option Smooth $. >$ Exponential with $\alpha = 0 . 1 5 4 .$ )

Figures 9-11 and 9-12 contain two realizations from the fitted model for the goal data. The general appearance of the first realization is somewhat compatible with the goal data, while the second realization illustrates the convergence of the sample path to 0 in accordance with the result of Grunwald et al. (1994).

![](images/c9f81fcf3b170b443d2c6d02a4dc2d43fde2d73db2621679b9926c2c7102ac3d.jpg)

![](images/efed41558ad1218f3e15f9e86efd2d9b1f4e26c6c1676475ad552b150ca93bab.jpg)  
Figure 9-10 One-step predictors of the goal data

![](images/2ff7a41de2cabdeacfa81b86975402d31958b304a257047ee405bb68a87225cc.jpg)  
Figure 9-11 A simulated time series from the fitted model to the goal data

# Example 9.8.8. The Exponential Case

Suppose $Y _ { t }$ given $X _ { t }$ has an exponential density with mean $- 1 / X _ { t }$ $( X _ { t } ~ < ~ 0 )$ ). The observation density is given by

$$
p (y _ {t} | x _ {t}) = \exp \{y _ {t} x _ {t} + \ln (- x _ {t}) \}, \quad y _ {t} > 0,
$$

which has the form (9.8.31) with $b ( x ) = - \ln ( - x )$ and $c ( y ) = 0$ . The state densities corresponding to the family of conjugate priors (see (9.8.37)) are given by

$$
p \left(x _ {t + 1} | \mathbf {y} ^ {(t)}\right) = \exp \left\{\alpha_ {t + 1 | t} x _ {t + 1} - \lambda_ {t + 1 | t} b \left(x _ {t + 1}\right) + A _ {t + 1 | t} \right\}, \quad - \infty <   x <   0.
$$

(Here $p ( x _ { t + 1 } | \mathbf { y } ^ { ( t ) } )$ is a probability density when $\alpha _ { t + 1 \mid t } ~ > ~ 0$ and $\lambda _ { t + 1 | t } ~ > ~ - 1 . )$ The one-step prediction density is

![](images/6a34272f3f7a7ee01ca912e88c06011fb63415ce3aa1bb597c5b9cd8951fbbba.jpg)  
Figure 9-12 A second simulated time series from the fitted model to the goal data

$$
\begin{array}{l} p \left(y _ {t + 1} | \mathbf {y} ^ {(t)}\right) = \int_ {- \infty} ^ {0} e ^ {x _ {t + 1} y _ {t + 1} + \ln (- x _ {t + 1}) + \alpha_ {t + 1 | t} x - \lambda_ {t + 1 | t} b (x) + A _ {t + 1 | t}} d x _ {t + 1} \\ = \left(\lambda_ {t + 1 | t} + 1\right) \alpha_ {t + 1 | t} ^ {\lambda_ {t + 1 | t} + 1} \left(y _ {t + 1} + \alpha_ {t + 1 | t}\right) ^ {- \lambda_ {t + 1 | t} - 2}, \quad y _ {t + 1} > 0 \\ \end{array}
$$

(see Problem 9.28). While $E ( Y _ { t + 1 } | \mathbf { y } ^ { ( t ) } ) = \alpha _ { t + 1 | t } / \lambda _ { t + 1 | t }$ , the conditional variance is finite if and only if $\lambda _ { t + 1 | t } > 1$ . Under assumptions (9.8.41)–(9.8.42), and starting with $\lambda _ { 1 | 0 } =$ $\delta / ( 1 - \delta )$ , the exponential smoothing formula (9.8.48) remains valid.

# Problems

9.1 Show that if all the eigenvalues of $F$ are less than 1 in absolute value (or equivalently that $F ^ { k } \to 0$ as $k \to \infty$ ), the unique stationary solution of equation (9.1.11) is given by the infinite series

$$
\mathbf {X} _ {t} = \sum_ {j = 0} ^ {\infty} F ^ {j} V _ {t - j - 1}
$$

and that the corresponding observation vectors are

$$
\mathbf {Y} _ {t} = \mathbf {W} _ {t} + \sum_ {j = 0} ^ {\infty} G F ^ {j} \mathbf {V} _ {t - j - 1}.
$$

Deduce that $\{ ( \mathbf { X } _ { t } ^ { \prime } , \mathbf { Y } _ { t } ^ { \prime } ) ^ { \prime } \}$ is a multivariate stationary process. (Hint: Use a vector analogue of the argument in Example 2.2.1.)

9.2 In Example 9.2.1, show that $\theta = - 1$ if and only if $\sigma _ { \nu } ^ { 2 } = 0$ , which in turn is equivalent to the signal $M _ { t }$ being constant.   
9.3 Let $F$ be the coefficient of $\mathbf { X } _ { t }$ in the state equation (9.3.4) for the causal $\operatorname { A R } ( p )$ process

$$
X _ {t} - \phi_ {1} X _ {t - 1} - \dots - \phi_ {p} X _ {t - p} = Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

Establish the stability of (9.3.4) by showing that

$$
\det  (z I - F) = z ^ {p} \phi \left(z ^ {- 1}\right),
$$

and hence that the eigenvalues of $F$ are the reciprocals of the zeros of the autoregressive polynomial $\phi ( z ) = 1 - \phi _ { 1 } z - \cdot \cdot \cdot - \phi _ { p } z ^ { p }$ .

9.4 By following the argument in Example 9.3.3, find a state-space model for $\{ Y _ { t } \}$ when $\{ \nabla \nabla _ { 1 2 } Y _ { t } \}$ is an $\mathbf { A R M A } ( p , q )$ process.

9.5 For the local linear trend model defined by equations (9.2.6)–(9.2.7), show that $\nabla ^ { 2 } Y _ { t } = ( 1 - B ) ^ { 2 } Y _ { t }$ is a 2-correlated sequence and hence, by Proposition 2.1.1, is an MA(2) process. Show that this MA(2) process is noninvertible if $\sigma _ { u } ^ { 2 } = 0$ .

9.6 a. For the seasonal model of Example 9.2.2, show that $\nabla _ { d } Y _ { t } = Y _ { t } - Y _ { t - d }$ is an MA(1) process.

b. Show that $\nabla \nabla _ { d } Y _ { t }$ is an $\mathrm { M A } ( d + 1 )$ process where $\{ Y _ { t } \}$ follows the seasonal model with a local linear trend as described in Example 9.2.3.

9.7 Let $\{ Y _ { t } \}$ be the MA(1) process

$$
Y _ {t} = Z _ {t} + \theta Z _ {t - 1}, \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right).
$$

Show that $\{ Y _ { t } \}$ has the state-space representation

$$
Y _ {t} = \left[ \begin{array}{c c} 1 & 0 \end{array} \right] \mathbf {X} _ {t},
$$

where $\{ { \mathbf { X } } _ { t } \}$ is the unique stationary solution of

$$
\mathbf {X} _ {t + 1} = \left[ \begin{array}{c c} 0 & 1 \\ 0 & 0 \end{array} \right] \mathbf {X} _ {t} + \left[ \begin{array}{c} 1 \\ \theta \end{array} \right] Z _ {t + 1}.
$$

In particular, show that the state vector $\mathbf { X } _ { t }$ can written as

$$
\mathbf {X} _ {t} = \left[ \begin{array}{l l} 1 & \theta \\ \theta & 0 \end{array} \right] \left[ \begin{array}{l} Z _ {t} \\ Z _ {t - 1} \end{array} \right].
$$

9.8 Verify equations (9.3.16)–(9.3.18) for an ARIMA(1,1,1) process.

9.9 Consider the two state-space models

$$
\left\{ \begin{array}{l} \mathbf {X} _ {t + 1, 1} = F _ {1} \mathbf {X} _ {t 1} + \mathbf {V} _ {t 1}, \\ \mathbf {Y} _ {t 1} = G _ {1} \mathbf {X} _ {t 1} + \mathbf {W} _ {t 1}, \end{array} \right.
$$

and

$$
\left\{ \begin{array}{l} \mathbf {X} _ {t + 1, 2} = F _ {2} \mathbf {X} _ {t 2} + \mathbf {V} _ {t 2}, \\ \mathbf {Y} _ {t 2} = G _ {2} \mathbf {X} _ {t 2} + \mathbf {W} _ {t 2}, \end{array} \right.
$$

where $\{ ( \mathbf { V } _ { t 1 } ^ { \prime } , \mathbf { W } _ { t 1 } ^ { \prime } , \mathbf { V } _ { t 2 } ^ { \prime } , \mathbf { W } _ { t 2 } ^ { \prime } ) ^ { \prime } \}$ is white noise. Derive a state-space representation for $\{ ( \mathbf { Y } _ { t 1 } ^ { \prime } , \mathbf { Y } _ { t 2 } ^ { \prime } ) ^ { \prime } \}$ .

9.10 Use Remark 1 of Section 9.4 to establish the linearity properties of the operator $P _ { t }$ stated in Remark 3.

9.11 a. Show that if the matrix equation $X S { = } B$ can be solved for $X$ , then $X { = } B S ^ { - 1 }$ is a solution for any generalized inverse $S ^ { - 1 }$ of $S$ .

b. Use the result of (a) to derive the expression for $P ( \mathbf { X } | \mathbf { Y } )$ in Remark 4 of Section 9.4.

9.12 In the notation of the Kalman prediction equations, show that every vector of the form

$$
\mathbf {Y} = A _ {1} \mathbf {X} _ {1} + \dots + A _ {t} \mathbf {X} _ {t}
$$

can be expressed as

$$
\mathbf {Y} = B _ {1} \mathbf {X} _ {1} + \dots + B _ {t - 1} \mathbf {X} _ {t - 1} + C _ {t} \mathbf {I} _ {t},
$$

where $B _ { 1 } , \ldots , B _ { t - 1 }$ and $C _ { t }$ are matrices that depend on the matrices $A _ { 1 } , \ldots , A _ { t }$ . Show also that the converse is true. Use these results and the fact that $E ( \mathbf { X } _ { s } \mathbf { I } _ { t } ) =$ 0 for all $s < t$ to establish (9.4.3).

9.13 In Example 9.4.1, verify that the steady-state solution of the Kalman recursions (9.1.2) is given by $\Omega _ { t } = \left( \sigma _ { \nu } ^ { 2 } + \sqrt { \sigma _ { \nu } ^ { 4 } + 4 \sigma _ { \nu } ^ { 2 } \sigma _ { w } ^ { 2 } } \right) / 2$ .   
9.14 Show from the difference equations for $\varOmega _ { t }$ in Example 9.4.1 that $( \Omega _ { t + 1 } -$ $\varOmega ) ( \varOmega _ { t } \varOmega ) \geq 0$ for all $\boldsymbol { \varOmega } _ { t } \geq 0$ , where $\varOmega$ is the steady-state solution for $\varOmega _ { t }$ given in Problem 9.13.   
9.15 Show directly that for the MA(1) model (9.2.3), the parameter $\theta$ is equal to $- \left( 2 \sigma _ { w } ^ { 2 } + \sigma _ { \nu } ^ { 2 } - \sqrt { \sigma _ { \nu } ^ { 4 } + 4 \sigma _ { \nu } ^ { 2 } \sigma _ { w } ^ { 2 } } \right) / \left( 2 \sigma _ { w } ^ { 2 } \right)$ , which in turn is equal to $- \sigma _ { w } ^ { 2 } / ( \varOmega +$ $\sigma _ { w } ^ { 2 } )$ , where $\varOmega$ is the steady-state solution for $\varOmega _ { t }$ given in Problem 9.13.   
9.16 Use the ARMA(0,1,1) representation of the series $\{ Y _ { t } \}$ in Example 9.4.1 to show that the predictors defined by

$$
\hat {Y} _ {n + 1} = a Y _ {n} + (1 - a) \hat {Y} _ {n}, \quad n = 1, 2, \dots ,
$$

where $a = \varOmega / ( \varOmega + \sigma _ { w } ^ { 2 } )$ , satisfy

$$
Y _ {n + 1} - \hat {Y} _ {n + 1} = Z _ {n + 1} + (1 - a) ^ {n} \left(Y _ {0} - Z _ {0} - \hat {Y} _ {1}\right).
$$

Deduce that if $0 < a < 1$ , the mean squared error of $\hat { Y } _ { n + 1 }$ converges to $\varOmega + \sigma _ { w } ^ { 2 }$ for any initial predictor $\hat { Y } _ { 1 }$ with finite mean squared error.

9.17 a. Using equations (9.4.1) and (9.4.16), show that $\hat { \mathbf { X } } _ { t + 1 } = F _ { t } \mathbf { X } _ { t \mid t }$

b. From (a) and (9.4.16) show that $\mathbf { X } _ { t \mid t }$ satisfies the recursions

$$
\mathbf {X} _ {t \mid t} = F _ {t - 1} \mathbf {X} _ {t - 1 \mid t - 1} + \Omega_ {t} G _ {t} ^ {\prime} \Delta_ {t} ^ {- 1} (\mathbf {Y} _ {t} - G _ {t} F _ {t - 1} \mathbf {X} _ {t - 1 \mid t - 1})
$$

$$
\text {f o r} t = 2, 3, \dots , \text {w i t h} \mathbf {X} _ {1 | 1} = \hat {\mathbf {X}} _ {1} + \Omega_ {1} G _ {1} ^ {\prime} \Delta_ {1} ^ {- 1} \left(\mathbf {Y} _ {1} - G _ {1} \hat {\mathbf {X}} _ {1}\right).
$$

9.18 In Section 9.5, show that for fixed $Q ^ { * }$ , $- 2 \ln L \left( \pmb { \mu } , Q ^ { * } , \sigma _ { w } ^ { 2 } \right)$ is minimized when $\pmb { \mu }$ and $\sigma _ { w } ^ { 2 }$ are given by (9.5.10) and (9.5.11), respectively.   
9.19 Verify the calculation of $\Theta _ { t } \varDelta _ { t } ^ { - 1 }$ and $\varOmega _ { t }$ in Example 9.6.1.   
9.20 Verify that the best estimates of missing values in an $\operatorname { A R } ( p )$ process are found by minimizing (9.6.11) with respect to the missing values.   
9.21 Suppose that $\{ Y _ { t } \}$ is the AR(2) process

$$
Y _ {t} = \phi_ {1} Y _ {t - 1} + \phi_ {2} Y _ {t - 2} + Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right),
$$

and that we observe $Y _ { 1 } , Y _ { 2 } , Y _ { 4 } , Y _ { 5 } , Y _ { 6 } , Y _ { 7 }$ . Show that the best estimator of $Y _ { 3 }$ is

$$
\left. \left(\phi_ {2} \left(Y _ {1} + Y _ {5}\right) + \left(\phi_ {1} - \phi_ {1} \phi_ {2}\right) \left(Y _ {2} + Y _ {4}\right)\right) / \left(1 + \phi_ {1} ^ {2} + \phi_ {2} ^ {2}\right). \right.
$$

9.22 Let $X _ { t }$ be the state at time $t$ of a parameter-driven model (see (9.8.2)). Show that $\{ X _ { t } \}$ is a Markov chain and that (9.8.3) holds.   
9.23 For the generalized state-space model of Example 9.8.1, show that $\begin{array} { r l } { \mathscr { Q } _ { t + 1 } } & { { } = } \end{array}$ $F ^ { 2 } \Omega _ { t | t } + Q$ .   
9.24 If $Y$ and $X$ are random variables, show that

$$
\operatorname {V a r} (Y) = E (\operatorname {V a r} (Y | X)) + \operatorname {V a r} (E (Y | X)).
$$

9.25 Suppose that $Y$ and $X$ are two random variables such that the distribution of $Y$ given $X$ is Poisson with mean $\pi X$ , $0 < \pi \leq 1$ , and $X$ has the gamma density $g ( x ; \alpha , \lambda )$ .

a. Show that the posterior distribution of $X$ given $Y$ also has a gamma density and determine its parameters.   
b. Compute $E ( X | Y )$ and $\operatorname { V a r } ( X | Y )$ .   
c. Show that $Y$ has a negative binomial density and determine its parameters.   
d. Use (c) to compute $E ( Y )$ and $\mathrm { V a r } ( Y )$ .   
e. Verify in Example 9.8.2 that $\begin{array} { r l r } { E \left( Y _ { t + 1 } | \mathbf { Y } ^ { ( t ) } \right) } & { { } = } & { \alpha _ { t } \pi / ( \lambda _ { t + 1 } \ - \ \pi ) } \end{array}$ and $\operatorname { V a r } \bigl ( Y _ { t + 1 } | \mathbf { Y } ^ { ( t ) } \bigr ) = \alpha _ { t } \pi \lambda _ { t + 1 } / ( \lambda _ { t + 1 } - \bar { \pi } ) ^ { 2 }$ .

9.26 For the model of Example 9.8.6, show that

a. $E \left( X _ { t + 1 } | \mathbf { Y } ^ { ( t ) } \right) = E \left( \hat { X _ { t } } | \mathbf { Y } ^ { ( t ) } \right) , \operatorname { V a r } \bigl ( X _ { t + 1 } | \mathbf { Y } ^ { ( t ) } \bigr ) > \operatorname { V a r } \bigl ( X _ { t } | \mathbf { Y } ^ { ( t ) } \bigr ) .$ , and   
+b. the transformed sequence $W _ { t } = e ^ { X _ { t } }$ +has a gamma state density.

9.27 Let $\{ V _ { t } \}$ be a sequence of independent exponential random variables with $E V _ { t } =$ $t ^ { - 1 }$ and suppose that $\{ X _ { t } , \ t \geq 1 \}$ and $\{ Y _ { t } , \ t \geq 1 \}$ are the state and observation random variables, respectively, of the parameter-driven state-space system

$$
X _ {1} = V _ {1},
$$

$$
X _ {t} = X _ {t - 1} + V _ {t}, \quad t = 2, 3, \dots ,
$$

where the distribution of the observation $Y _ { t }$ , conditional on the random variables $Y _ { 1 } , Y _ { 2 } , \dots , Y _ { t - 1 } , X _ { t }$ , is Poisson with mean $X _ { t }$ .

a. Determine the observation and state transition density functions $p ( y _ { t } | x _ { t } )$ and $p ( x _ { t + 1 } | x _ { t } )$ in the parameter-driven model for $\{ Y _ { t } \}$ .   
b. Show, using (9.8.4)–(9.8.6), that

$$
p \left(x _ {1} \mid y _ {1}\right) = g \left(x _ {1}; y _ {1} + 1, 2\right)
$$

and

$$
p \left(x _ {2} \mid y _ {1}\right) = g \left(x _ {2}; y _ {1} + 2, 2\right),
$$

where $g ( x ; \alpha , \lambda )$ is the gamma density function (see Example (d) of Section A.1).

c. Show that

$$
p \left(x _ {t} | \mathbf {y} ^ {(t)}\right) = g \left(x _ {t}; \alpha_ {t} + t, t + 1\right)
$$

and

$$
p \left(x _ {t + 1} | \mathbf {y} ^ {(t)}\right) = g \left(x _ {t + 1}; \alpha_ {t} + t + 1, t + 1\right),
$$

where $\alpha _ { t } = y _ { 1 } + \cdot \cdot \cdot + y _ { t }$

d. Conclude from (c) that the minimum mean squared error estimates of $X _ { t }$ and $X _ { t + 1 }$ based on $Y _ { 1 } , \dots , Y _ { t }$ $Y _ { t }$ are

$$
X _ {t | t} = \frac {t + Y _ {1} + \cdots + Y _ {t}}{t + 1}
$$

and

$$
\hat {X} _ {t + 1} = \frac {t + 1 + Y _ {1} + \cdots + Y _ {t}}{t + 1},
$$

respectively.

9.28 Let $Y$ and $X$ be two random variables such that $Y$ given $X$ is exponential with mean $1 / X$ , and $X$ has the gamma density function with

$$
g (x; \lambda + 1, \alpha) = \frac {\alpha^ {\lambda + 1} x ^ {\lambda} \exp \{- \alpha x \}}{\Gamma (\lambda + 1)}, \quad x > 0,
$$

where $\lambda > - 1$ and $\alpha > 0$ .

a. Determine the posterior distribution of $X$ given $Y$   
b. Show that $Y$ has a Pareto distribution

$$
p (y) = (\lambda + 1) \alpha^ {\lambda + 1} (y + \alpha) ^ {- \lambda - 2}, \quad y > 0.
$$

c. Find the mean and variance of Y. Under what conditions on $\alpha$ and $\lambda$ does the latter exist?   
d. Verify the calculation of $p \left( y _ { t + 1 } | \mathbf { y } ^ { ( t ) } \right)$ and $E \left( Y _ { t + 1 } | \mathbf { y } ^ { ( t ) } \right)$ for the model in Example 9.8.8.

9.29 Consider an observation-driven model in which $Y _ { t }$ given $X _ { t }$ is binomial with parameters $n$ and $X _ { t }$ , i.e.,

$$
p (y _ {t} | x _ {t}) = \binom {n} {y _ {t}} x _ {t} ^ {y _ {t}} (1 - x _ {t}) ^ {n - y _ {t}}, \quad y _ {t} = 0, 1, \ldots , n.
$$

a. Show that the observation equation with state variable transformed by the logit transformation $W _ { t } = \ln ( X _ { t } / ( 1 - X _ { t } ) )$ follows an exponential family

$$
p \left(y _ {t} \mid w _ {t}\right) = \exp \left\{y _ {t} w _ {t} - b \left(w _ {t}\right) + c \left(y _ {t}\right) \right\}.
$$

Determine the functions $b ( \cdot )$ and $c ( \cdot )$

b. Suppose that the state $X _ { t }$ has the beta density

$$
p \left(x _ {t + 1} \mid \mathbf {y} ^ {(t)}\right) = f \left(x _ {t + 1}; \alpha_ {t + 1 | t}, \lambda_ {t + 1 | t}\right),
$$

where

$$
f (x; \alpha , \lambda) = [ B (\alpha , \lambda) ] ^ {- 1} x ^ {\alpha - 1} (1 - x) ^ {\lambda - 1}, \quad 0 <   x <   1,
$$

$B ( \alpha , \lambda ) : = { \varGamma } ( \alpha ) { \varGamma } ( \lambda ) / { \varGamma } ( \alpha + \lambda )$ is the beta function, and $\alpha , \lambda > 0$ . Show that the posterior distribution of $X _ { t }$ given $Y _ { t }$ is also beta and express its parameters in terms of $y _ { t }$ and $\alpha _ { t | t - 1 } , \lambda _ { t | t - 1 }$ .

| − | −c. Under the assumptions made in (b), show that $E { \big ( } X _ { t } | \mathbf { Y } ^ { ( t ) } { \big ) } ~ = ~ E { \big ( } X _ { t + 1 } | \mathbf { Y } ^ { ( t ) } { \big ) }$ and $\mathrm { V a r } \big ( X _ { t } | \mathbf { Y } ^ { ( t ) } \big ) < \mathrm { V a r } \big ( X _ { t + 1 } | \mathbf { Y } ^ { ( t ) } \big )$ .

d. Assuming that the parameters in (b) satisfy (9.8.41)–(9.8.42), show that the onestep prediction density $p \big ( y _ { t + 1 } | \mathbf { y } ^ { ( t ) } \big )$ is beta-binomial,

$$
p (y _ {t + 1} | \mathbf {y} ^ {(t)}) = \frac {B (\alpha_ {t + 1 | t} + y _ {t + 1} , \lambda_ {t + 1 | t} + n - y _ {t + 1})}{(n + 1) B (y _ {t + 1} + 1 , n - y _ {t + 1} + 1) B (\alpha_ {t + 1 | t} , \lambda_ {t + 1 | t})},
$$

and verify that $\hat { Y } _ { t + 1 }$ is given by (9.8.47).

# 10 Forecasting Techniques

10.1 The ARAR Algorithm   
10.2 The Holt–Winters Algorithm   
10.3 The Holt–Winters Seasonal Algorithm   
10.4 Choosing a Forecasting Algorithm

We have focused until now on the construction of time series models for stationary and nonstationary series and the determination, assuming the appropriateness of these models, of minimum mean squared error predictors. If the observed series had in fact been generated by the fitted model, this procedure would give minimum mean squared error forecasts. In this chapter we discuss three forecasting techniques that have less emphasis on the explicit construction of a model for the data. Each of the three selects, from a limited class of algorithms, the one that is optimal according to specified criteria.

The three techniques have been found in practice to be effective on wide ranges of real data sets (for example, the economic time series used in the forecasting competition described by Makridakis et al. 1984).

The ARAR algorithm described in Section 10.1 is an adaptation of the ARARMA algorithm (Newton and Parzen 1984; Parzen 1982) in which the idea is to apply automatically selected “memory-shortening” transformations (if necessary) to the data and then to fit an ARMA model to the transformed series. The ARAR algorithm we describe is a version of this in which the ARMA fitting step is replaced by the fitting of a subset AR model to the transformed data.

The Holt–Winters (HW) algorithm described in Section 10.2 uses a set of simple recursions that generalize the exponential smoothing recursions of Section 1.5.1 to generate forecasts of series containing a locally linear trend.

The Holt–Winters seasonal (HWS) algorithm extends the HW algorithm to handle data in which there are both trend and seasonal variation of known period. It is described in Section 10.3.

Each of these three algorithms can be applied to specific data sets with the aid of the ITSM options Forecasting>ARAR, Forecasting>Holt-Winters and Forecasting>Seasonal Holt-Winters.

# 10.1 The ARAR Algorithm

# 10.1.1 Memory Shortening

Given a data set $\{ Y _ { t } , t = 1 , 2 , \ldots , n \}$ , the first step is to decide whether the underlying process is “long-memory,” and if so to apply a memory-shortening transformation before attempting to fit an autoregressive model. The differencing operations permitted under the option Transform of ITSM are examples of memory-shortening transformations; however, the ones used by the option Forecasting>ARAR selects are members of a more general class. There are two types allowed:

$$
\tilde {Y} _ {t} = Y _ {t} - \hat {\phi} (\hat {\tau}) Y _ {t - \hat {\tau}} \tag {10.1.1}
$$

and

$$
\tilde {Y} _ {t} = Y _ {t} - \hat {\phi} _ {1} Y _ {t - 1} - \hat {\phi} _ {2} Y _ {t - 2}. \tag {10.1.2}
$$

With the aid of the five-step algorithm described below, we classify $\{ Y _ { t } \}$ and take one of the following three courses of action:

• L. Declare $\{ Y _ { t } \}$ to be long-memory and form $\left\{ \tilde { Y } _ { t } \right\}$ using (10.1.1).   
• M. Declare $\{ Y _ { t } \}$ to be moderately long-memory and form $\left\{ \tilde { Y } _ { t } \right\}$ using (10.1.2).   
• S. Declare $\{ Y _ { t } \}$ to be short-memory.

If the alternative L or M is chosen, then the transformed series $\left\{ \tilde { Y } _ { t } \right\}$ is again checked. If it is found to be long-memory or moderately long-memory, then a further transformation is performed. The process continues until the transformed series is classified as short-memory. At most three memory-shortening transformations are performed, but it is very rare to require more than two. The algorithm for deciding among L, M, and S can be described as follows:

1. For each $\tau = 1 , 2 , \ldots , 1 5$ , we find the value $\hat { \phi } ( \tau )$ of $\phi$ that minimizes

$$
\operatorname {E R R} (\phi , \tau) = \frac {\sum_ {t = \tau + 1} ^ {n} \left[ Y _ {t} - \phi Y _ {t - \tau} \right] ^ {2}}{\sum_ {t = \tau + 1} ^ {n} Y _ {t} ^ {2}}.
$$

We then define

$$
\operatorname {E r r} (\tau) = \operatorname {E R R} \left(\hat {\phi} (\tau), \tau\right)
$$

and choose the lag $\hat { \tau }$ to be the value of $\tau$ that minimizes $\mathrm { E r r } ( \tau )$

2. If $\mathrm { E r r } \big ( \hat { \tau } \big ) \leq 8 / n$ , go to L.   
3. If $\hat { \phi } \mathopen { } \mathclose \bgroup \left( \hat { \tau } \aftergroup \egroup \right) \geq 0 . 9 3$ and $\hat { \tau } > 2$ , go to L.  
4. If $\hat { \phi } \left( \hat { \tau } \right) \geq 0 . 9 3$ and $\hat { \tau } = 1$ or 2, determine the values $\hat { \phi } _ { 1 }$ and $\hat { \phi } _ { 2 }$ of $\phi _ { 1 }$ and $\phi _ { 2 }$ that minimize ${ \scriptstyle \sum _ { t = 3 } ^ { n } } [ Y _ { t } - \phi _ { 1 } Y _ { t - 1 } - \phi _ { 2 } Y _ { t - 2 } ] ^ { 2 } $ ; then go to M.   
5. If $\hat { \phi } \mathopen { } \mathclose \bgroup \left( \hat { \tau } \aftergroup \egroup \right) < 0 . 9 3$ , go to S.

# 10.1.2 Fitting a Subset Autoregression

Let $\{ S _ { t } , t = k + 1 , \ldots , n \}$ denote the memory-shortened series derived from $\{ Y _ { t } \}$ by the algorithm of the previous section and let $\overline { S }$ denote the sample mean of $S _ { k + 1 } , \ldots , S _ { n }$ .

The next step in the modeling procedure is to fit an autoregressive process to the mean-corrected series

$$
X _ {t} = S _ {t} - \overline {{S}}, \quad t = k + 1, \ldots , n.
$$

The fitted model has the form

$$
X _ {t} = \phi_ {1} X _ {t - 1} + \phi_ {l _ {1}} X _ {t - l _ {1}} + \phi_ {l _ {2}} X _ {t - l _ {2}} + \phi_ {l _ {3}} X _ {t - l _ {3}} + Z _ {t},
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \left( 0 , \sigma ^ { 2 } \right)$ , and for given lags, $l _ { 1 } , l _ { 2 }$ , and $l _ { 3 }$ , the coefficients $\phi _ { j }$ and the white noise variance $\sigma ^ { 2 }$ are found from the Yule–Walker equations

$$
\left[ \begin{array}{c c c c} 1 & \hat {\rho} (l _ {1} - 1) & \hat {\rho} (l _ {2} - 1) & \hat {\rho} (l _ {3} - 1) \\ \hat {\rho} (l _ {1} - 1) & 1 & \hat {\rho} (l _ {2} - l _ {1}) & \hat {\rho} (l _ {3} - l _ {1}) \\ \hat {\rho} (l _ {2} - 1) & \hat {\rho} (l _ {2} - l _ {1}) & 1 & \hat {\rho} (l _ {3} - l _ {2}) \\ \hat {\rho} (l _ {3} - 1) & \hat {\rho} (l _ {3} - l _ {1}) & \hat {\rho} (l _ {3} - l _ {2}) & 1 \end{array} \right] \left[ \begin{array}{c} \phi_ {1} \\ \phi_ {l _ {1}} \\ \phi_ {l _ {2}} \\ \phi_ {l _ {3}} \end{array} \right] = \left[ \begin{array}{c} \hat {\rho} (1) \\ \hat {\rho} (l _ {1}) \\ \hat {\rho} (l _ {2}) \\ \hat {\rho} (l _ {3}) \end{array} \right]
$$

and

$$
\sigma^ {2} = \hat {\gamma} (0) \left[ 1 - \phi_ {1} \hat {\rho} (1) - \phi_ {l _ {1}} \hat {\rho} (l _ {1}) - \phi_ {l _ {2}} \hat {\rho} (l _ {2}) - \phi_ {l _ {3}} \hat {\rho} (l _ {3}) \right],
$$

where $\hat { \gamma } ( j )$ and $\hat { \rho } ( j ) , j = 0 , 1 , 2 , . . .$ , are the sample autocovariances and autocorrelations of the series $\{ X _ { t } \}$ .

The program computes the coefficients $\phi _ { j }$ for each set of lags such that

$$
1 <   l _ {1} <   l _ {2} <   l _ {3} \leq m,
$$

where m can be chosen to be either 13 or 26. It then selects the model for which the Yule–Walker estimate $\sigma ^ { 2 }$ is minimal and prints out the lags, coefficients, and white noise variance for the fitted model.

A slower procedure chooses the lags and coefficients (computed from the Yule– Walker equations as above) that maximize the Gaussian likelihood of the observations. For this option the maximum lag m is 13.

The options are displayed in the ARAR Forecasting dialog box, which appears on the screen when the option Forecasting>ARAR is selected. It allows you also to bypass memory shortening and fit a subset AR to the original (meancorrected) data.

# 10.1.3 Forecasting

If the memory-shortening filter found in the first step has coefficients $\psi _ { 0 } ( = ~ 1 )$ , ψ1, . . . , $\psi _ { k }$ $( k \geq 0 )$ , then the memory-shortened series can be expressed as

$$
S _ {t} = \psi (B) Y _ {t} = Y _ {t} + \psi_ {1} Y _ {t - 1} + \dots + \psi_ {k} Y _ {t - k}, \tag {10.1.3}
$$

where $\psi ( B )$ is the polynomial in the backward shift operator,

$$
\psi (B) = 1 + \psi_ {1} B + \dots + \psi_ {k} B ^ {k}.
$$

Similarly, if the coefficients of the subset autoregression found in the second step are $\phi _ { 1 } , \phi _ { l _ { 1 } } , \phi _ { l _ { 2 } }$ , and $\phi _ { l _ { 3 } }$ , then the subset AR model for the mean-corrected series $\left\{ X _ { t } \right. =$ $S _ { t } - \overline { { S } } \}$ is

$$
\phi (B) X _ {t} = Z _ {t}, \tag {10.1.4}
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \left( 0 , \sigma ^ { 2 } \right)$ and

$$
\phi (B) = 1 - \phi_ {1} B - \phi_ {l _ {1}} B ^ {l _ {1}} - \phi_ {l _ {2}} B ^ {l _ {2}} - \phi_ {l _ {3}} B ^ {l _ {3}}.
$$

From (10.1.3) and (10.1.4) we obtain the equations

$$
\xi (B) Y _ {t} = \phi (1) \bar {S} + Z _ {t}, \tag {10.1.5}
$$

where

$$
\xi (B) = \psi (B) \phi (B) = 1 + \xi_ {1} B + \dots + \xi_ {k + l _ {3}} B ^ {k + l _ {3}}.
$$

Assuming that the fitted model (10.1.5) is appropriate and that the white noise term $Z _ { t }$ is uncorrelated with $\{ Y _ { j } , ~ j < ~ t \}$ for each t, we can determine the minimum mean squared error linear predictors $P _ { n } Y _ { n + h }$ of $Y _ { n + h }$ in terms of $\{ 1 , Y _ { 1 } , \ldots , Y _ { n } \}$ , for $n > k + l _ { 3 }$ , from the recursions

$$
P _ {n} Y _ {n + h} = - \sum_ {j = 1} ^ {k + l _ {3}} \xi_ {j} P _ {n} Y _ {n + h - j} + \phi (1) \bar {S}, \quad h \geq 1, \tag {10.1.6}
$$

with the initial conditions

$$
P _ {n} Y _ {n + h} = Y _ {n + h}, \quad \text {f o r} h \leq 0. \tag {10.1.7}
$$

The mean squared error of the predictor $P _ { n } Y _ { n + h }$ is found to be (Problem 10.1)

$$
E \left[ \left(Y _ {n + h} - P _ {n} Y _ {n + h}\right) ^ {2} \right] = \sum_ {j = 0} ^ {h - 1} \tau_ {j} ^ {2} \sigma^ {2}, \tag {10.1.8}
$$

where $\textstyle \sum _ { j = 0 } ^ { \infty } \tau _ { j } z ^ { j }$ is the Taylor expansion of $1 / \xi ( z )$ in a neighborhood of $z ~ = ~ 0$ Equivalently the sequence $\{ \tau _ { j } \}$ can be found from the recursion

$$
\tau_ {0} = 1, \sum_ {j = 0} ^ {n} \tau_ {j} \xi_ {n - j} = 0, \quad n = 1, 2, \dots . \tag {10.1.9}
$$

# 10.1.4 Application of the ARAR Algorithm

To determine an ARAR model for a given data set $\{ Y _ { t } \}$ using ITSM, select Forecasting>ARAR and choose the appropriate options in the resulting dialog box. These include specification of the number of forecasts required, whether or not you wish to include the memory-shortening step, whether you require prediction bounds, and which of the optimality criteria is to be used. Once you have made these selections, click OK, and the forecasts will be plotted with the original data. Right-click on the graph and then Info to see the coefficients $1 , \psi _ { 1 } , \ldots , \psi _ { k }$ $\psi _ { 1 }$ $\psi _ { k }$ of the memory-shortening filter $\psi ( B )$ , the lags and coefficients of the subset autoregression

$$
X _ {t} - \phi_ {1} X _ {t - 1} - \phi_ {l _ {1}} X _ {t - l _ {1}} - \phi_ {l _ {2}} X _ {t - l _ {2}} - \phi_ {l _ {3}} X _ {t - l _ {3}} = Z _ {t},
$$

and the coefficients $\xi _ { j }$ of $B ^ { j }$ in the overall whitening filter

$$
\xi (B) = \left(1 + \psi_ {1} B + \dots + \psi_ {k} B ^ {k}\right) \left(1 - \phi_ {1} B - \phi_ {l _ {1}} B ^ {l _ {1}} - \phi_ {l _ {2}} B ^ {l _ {2}} - \phi_ {l _ {3}} B ^ {l _ {3}}\right).
$$

The numerical values of the predictors, their root mean squared errors, and the prediction bounds are also printed.

Example 10.1.1 To use the ARAR algorithm to predict 24 values of the accidental deaths data, open the file DEATHS.TSM and proceed as described above. Selecting Minimize WN variance [max $\mathtt { l a g } = 2 6 .$ ] gives the graph of the data and

![](images/4d7d6c1b9e702b9268a4db44f19c77a92e52c1fad42f7e4689a11854b10d90f4.jpg)  
Figure 10-1 The data set DEATHS.TSM with 24 values predicted by the ARAR algorithm

predictors shown in Figure 10-1. Right-clicking on the graph and then Info, we find that the selected memory-shortening filter is $\left( 1 - 0 . 9 7 7 9 B ^ { 1 2 } \right)$ . The fitted subset autoregression and the coefficients $\xi _ { j }$ of the overall whitening filter $\xi ( B )$ are shown below: -

Optimal lags 1 3 12 13 Optimal coeffs 0.5915 0.3822 0.3022 0.2970 WN Variance: 0.12314E+06   

<table><tr><td colspan="5">COEFFICIENTS OF OVERALL WHITENING FILTER:</td></tr><tr><td>1.0000</td><td>-0.5915</td><td>0.0000</td><td>-0.2093</td><td>0.0000</td></tr><tr><td>0.0000</td><td>0.0000</td><td>0.0000</td><td>0.0000</td><td>0.0000</td></tr><tr><td>0.0000</td><td>0.0000</td><td>-0.6757</td><td>0.2814</td><td>0.0000</td></tr><tr><td>0.2047</td><td>0.0000</td><td>0.0000</td><td>0.0000</td><td>0.0000</td></tr><tr><td>0.0000</td><td>0.0000</td><td>0.0000</td><td>0.0000</td><td>-0.2955</td></tr><tr><td>0.2904</td><td></td><td></td><td></td><td></td></tr></table>

In Table 10.1 we compare the predictors of the next six values of the accidental deaths series with the actual observed values. The predicted values obtained from ARAR as described in the example are shown together with the predictors obtained by fitting ARIMA models as described in Chapter 6 (see Table 10.1). The observed root mean squared errors (i.e., $\scriptstyle \sqrt { \sum _ { h = 1 } ^ { 6 } ( Y _ { 7 2 + h } - P _ { 7 2 } Y _ { 7 2 + h } ) ^ { 2 } / 6 } .$ ) for the three prediction methods are easily calculated to be 253 for ARAR, 583 for the ARIMA model (6.5.8), and 501 for the ARIMA model (6.5.9). The ARAR algorithm thus performs very well here. Notice that in this particular example the ARAR algorithm effectively fits a causal AR model to the data, but this is not always the case.

# 10.2 The Holt–Winters Algorithm

# 10.2.1 The Algorithm

Given observations $Y _ { 1 } , Y _ { 2 } , \ldots , Y _ { n }$ from the “trend plus noise” model (1.5.2), the exponential smoothing recursions (1.5.7) allowed us to compute estimates $\hat { m } _ { t }$ of the trend at times $t = 1 , 2 , \ldots , n .$ . If the series is stationary, then $m _ { t }$ is constant and the exponential smoothing forecast of $Y _ { n + h }$ based on the observations $Y _ { 1 } , \dots , Y _ { n }$ is

$$
P _ {n} Y _ {n + h} = \hat {m} _ {n}, \quad h = 1, 2, \dots . \tag {10.2.1}
$$

If the data have a (nonconstant) trend, then a natural generalization of the forecast function (10.2.1) that takes this into account is

$$
P _ {n} Y _ {n + h} = \hat {a} _ {n} + \hat {b} _ {n} h, \quad h = 1, 2, \dots , \tag {10.2.2}
$$

where $\hat { a } _ { n }$ and $\hat { b } _ { n }$ can be thought of as estimates of the “level” $a _ { n }$ and “slope” $b _ { n }$ of the trend function at time $n$ . Holt (1957) suggested a recursive scheme for computing the quantities $\hat { a } _ { n }$ and $\hat { b } _ { n }$ in (10.2.2). Denoting by $\hat { Y } _ { n + 1 }$ the one-step forecast $P _ { n } Y _ { n + 1 }$ , we have from (10.2.2)

$$
\hat {Y} _ {n + 1} = \hat {a} _ {n} + \hat {b} _ {n}.
$$

Now, as in exponential smoothing, we suppose that the estimated level at time $n + 1$ is a linear combination of the observed value at time $n + 1$ and the forecast value at time $n + 1$ . Thus,

$$
\hat {a} _ {n + 1} = \alpha Y _ {n + 1} + (1 - \alpha) (\hat {a} _ {n} + \hat {b} _ {n}). \tag {10.2.3}
$$

We can then estimate the slope at time $n + 1$ as a linear combination of $\hat { a } _ { n + 1 } - \hat { a } _ { n }$ and the estimated slope $\hat { b } _ { n }$ at time $n$ . Thus,

$$
\hat {b} _ {n + 1} = \beta (\hat {a} _ {n + 1} - \hat {a} _ {n}) + (1 - \beta) \hat {b} _ {n}. \tag {10.2.4}
$$

In order to solve the recursions (10.2.3) and (10.2.4) we need initial conditions. A natural choice is to set

$$
\hat {a} _ {2} = Y _ {2} \tag {10.2.5}
$$

and

$$
\hat {b} _ {2} = Y _ {2} - Y _ {1}. \tag {10.2.6}
$$

Then (10.2.3) and (10.2.4) can be solved successively for $\hat { a } _ { i }$ and $\hat { b } _ { i }$ $\hat { b } _ { i } , i = 3 , \ldots , n$ , and the predictors $P _ { n } Y _ { n + h }$ found from (10.2.2).

The forecasts depend on the “smoothing parameters” $\alpha$ and $\beta$ . These can either be prescribed arbitrarily (with values between 0 and 1) or chosen in a more systematic way to minimize the sum of squares of the one-step errors $\scriptstyle \sum _ { i = 3 } ^ { n } ( Y _ { i } - P _ { i - 1 } Y _ { i } ) ^ { 2 }$ , obtained

Table 10.1 Predicted and observed values of the accidental deaths series for $t = 7 3 , \dots , 7 8$   

<table><tr><td>t</td><td>73</td><td>74</td><td>75</td><td>76</td><td>77</td><td>78</td></tr><tr><td>Observed Yt</td><td>7798</td><td>7406</td><td>8363</td><td>8460</td><td>9217</td><td>9316</td></tr><tr><td>Predicted by ARAR</td><td>8168</td><td>7196</td><td>7982</td><td>8284</td><td>9144</td><td>9465</td></tr><tr><td>Predicted by (6.5.8)</td><td>8441</td><td>7704</td><td>8549</td><td>8885</td><td>9843</td><td>10,279</td></tr><tr><td>Predicted by (6.5.9)</td><td>8345</td><td>7619</td><td>8356</td><td>8742</td><td>9795</td><td>10,179</td></tr></table>

when the algorithm is applied to the already observed data. Both choices are available in the ITSM option Forecasting>Holt-Winters.

Before illustrating the use of the Holt–Winters forecasting procedure, we discuss the connection between the recursions (10.2.3) and (10.2.4) and the steady-state solution of the Kalman filtering equations for a local linear trend model. Suppose $\{ Y _ { t } \}$ follows the local linear structural model with observation equation

$$
Y _ {t} = M _ {t} + W _ {t}
$$

and state equation

$$
\left[ \begin{array}{c} M _ {t + 1} \\ B _ {t + 1} \end{array} \right] = \left[ \begin{array}{c c} 1 & 1 \\ 0 & 1 \end{array} \right] \left[ \begin{array}{c} M _ {t} \\ B _ {t} \end{array} \right] + \left[ \begin{array}{c} V _ {t} \\ U _ {t} \end{array} \right]
$$

[see (9.2.4)–(9.2.7)]. Now define $\hat { a } _ { n }$ and $\hat { b } _ { n }$ to be the filtered estimates of $M _ { n }$ and $B _ { n }$ , respectively, i.e.,

$$
\hat {a} _ {n} = M _ {n | n} := P _ {n} M _ {n},
$$

$$
\hat {b} _ {n} = B _ {n \mid n} := P _ {n} B _ {n}.
$$

Using Problem 9.17 and the Kalman recursion (9.4.16), we find that

$$
\left[ \begin{array}{l} \hat {a} _ {n + 1} \\ \hat {b} _ {n + 1} \end{array} \right] = \left[ \begin{array}{c} \hat {a} _ {n} + \hat {b} _ {n} \\ \hat {b} _ {n} \end{array} \right] + \Delta_ {n} ^ {- 1} \Omega_ {n} G ^ {\prime} \left(Y _ {n} - \hat {a} _ {n} - \hat {b} _ {n}\right), \tag {10.2.7}
$$

where $G \ = \ [ 1 \ 0 ]$ . Assuming that $\varOmega _ { n } { = } \varOmega _ { 1 } { = } [ \varOmega _ { i j } ] _ { i , j = 1 } ^ { 2 }$ is the steady-state solution of (9.4.2) for this model, then $\scriptstyle \Delta _ { n } = \Omega _ { 1 1 } + \sigma _ { w } ^ { 2 }$ for all $n$ , so that (10.2.7) simplifies to the equations

$$
\hat {a} _ {n + 1} = \hat {a} _ {n} + \hat {b} _ {n} + \frac {\Omega_ {1 1}}{\Omega_ {1 1} + \sigma_ {w} ^ {2}} \left(Y _ {n} - \hat {a} _ {n} - \hat {b} _ {n}\right) \tag {10.2.8}
$$

and

$$
\hat {b} _ {n + 1} = \hat {b} _ {n} + \frac {\Omega_ {1 2}}{\Omega_ {1 1} + \sigma_ {w} ^ {2}} \left(Y _ {n} - \hat {a} _ {n} - \hat {b} _ {n}\right). \tag {10.2.9}
$$

Solving (10.2.8) for $\left( Y _ { n } - { \hat { a } } _ { n } - { \hat { b } } _ { n } \right)$ and substituting into (10.2.9), we find that

$$
\hat {a} _ {n + 1} = \alpha Y _ {n + 1} + (1 - \alpha) (\hat {a} _ {n} + \hat {b} _ {n}), \tag {10.2.10}
$$

$$
\hat {b} _ {n + 1} = \beta (\hat {a} _ {n + 1} - \hat {a} _ {n}) + (1 - \beta) \hat {b} _ {n} \tag {10.2.11}
$$

with $\alpha = \Omega _ { 1 1 } / \left( \Omega _ { 1 1 } + \sigma _ { w } ^ { 2 } \right)$ and $\beta = \Omega _ { 2 1 } / \Omega _ { 1 1 }$ . These equations coincide with the Holt–Winters recursions (10.2.3) and (10.2.4). Equations relating $\alpha$ and $\beta$ to the variances $\sigma _ { u } ^ { 2 } , \sigma _ { \nu } ^ { 2 }$ , and $\sigma _ { w } ^ { 2 }$ can be found in Harvey (1990).

Example 10.2.1 To predict 24 values of the accidental deaths series using the Holt–Winters algorithm, open the file DEATHS.TSM and select Forecasting>Holt-Winters. In the resulting dialog box specify 24 for the number of predictors and check the box marked Optimize coefficients for automatic selection of the smoothing coefficients $\alpha$ and $\beta$ . Click OK, and the forecasts will be plotted with the original data as shown in Figure 10-2. Right-click on the graph and then Info to see the numerical values of the predictors, their root mean squared errors, and the optimal values of $\alpha$ and $\beta$ . The predicted and observed values are shown in Table 10.2.

The root mean squared error $\begin{array} { r l } {  { \big ( \sqrt { \sum _ { h = 1 } ^ { 6 } ( Y _ { 7 2 + h } - P _ { 7 2 } Y _ { 7 2 + h } ) ^ { 2 } / 6 } \big ) } } \end{array}$ for the nonseasonal = Holt–Winters forecasts is found to be 1143. Not surprisingly, since we have not taken

seasonality into account, this is a much larger value than for the three sets of forecasts shown in Table 10.1. In the next section we show how to modify the Holt–Winters algorithm to allow for seasonality.

# 10.2.2 Holt–Winters and ARIMA Forecasting

The one-step forecasts obtained by exponential smoothing with parameter $\alpha$ (defined by (1.5.7) and (10.2.1)) satisfy the relations

$$
P _ {n} Y _ {n + 1} = Y _ {n} - (1 - \alpha) \left(Y _ {n} - P _ {n - 1} Y _ {n}\right), \quad n \geq 2. \tag {10.2.12}
$$

But these are the same relations satisfied by the large-sample minimum mean squared error forecasts of the invertible ARIMA(0,1,1) process

$$
Y _ {t} = Y _ {t - 1} + Z _ {t} - (1 - \alpha) Z _ {t - 1}, \{Z _ {t} \} \sim \operatorname {W N} \left(0, \sigma^ {2}\right). \tag {10.2.13}
$$

Forecasting by exponential smoothing with optimal $\alpha$ can therefore be viewed as fitting a member of the two-parameter family of ARIMA processes (10.2.13) to the data and using the corresponding large-sample forecast recursions initialized by $P _ { 0 } Y _ { 1 } = Y _ { 1 }$ . In ITSM, the optimal $\alpha$ is found by minimizing the average squared error of the one-step forecasts of the observed data $Y _ { 2 } , \dots , Y _ { n }$ , and the parameter $\sigma ^ { 2 }$ is estimated by this average squared error. This algorithm could easily be modified to minimize other error measures such as average absolute one-step error and average 12-step squared error.

In the same way it can be shown that Holt–Winters forecasting can be viewed as fitting a member of the three-parameter family of ARIMA processes,

$$
(1 - B) ^ {2} Y _ {t} = Z _ {t} - (2 - \alpha - \alpha \beta) Z _ {t - 1} + (1 - \alpha) Z _ {t - 2}, \tag {10.2.14}
$$

where $\left\{ Z _ { t } \right\} \sim \mathrm { W N } ( 0 , \sigma ^ { 2 } )$ . The coefficients $\alpha$ and $\beta$ are selected as described after (10.2.6), and the estimate of $\sigma ^ { 2 }$ is the average squared error of the one-step forecasts of $Y _ { 3 } , \ldots , Y _ { n }$ obtained from the large-sample forecast recursions corresponding to (10.2.14).

Figure 10-2   
![](images/ed7013031f964ede9cc044eb97b40bd48b3e2e30ec1824378d1eb464ba699edb.jpg)  
The data set DEATHS.TSM with 24 values predicted by the nonseasonal Holt–Winters algorithm

Table 10.2 Predicted and observed values of the accidental deaths series for $t = 7 3 , \dots , 7 8$ from the (nonseasonal) Holt–Winters algorithm   

<table><tr><td>t</td><td>73</td><td>74</td><td>75</td><td>76</td><td>77</td><td>78</td></tr><tr><td>Observed Yt</td><td>7798</td><td>7406</td><td>8363</td><td>8460</td><td>9217</td><td>9316</td></tr><tr><td>Predicted by HW</td><td>9281</td><td>9322</td><td>9363</td><td>9404</td><td>9445</td><td>9486</td></tr></table>

# 10.3 The Holt–Winters Seasonal Algorithm

# 10.3.1 The Algorithm

If the series $Y _ { 1 }$ $Y _ { 1 } , Y _ { 2 } , \ldots , Y _ { n }$ $Y _ { 2 }$ $Y _ { n }$ contains not only trend, but also seasonality with period $d$ [as in the model (1.5.11)], then a further generalization of the forecast function (10.2.2) that takes this into account is

$$
P _ {n} Y _ {n + h} = \hat {a} _ {n} + \hat {b} _ {n} h + \hat {c} _ {n + h}, \quad h = 1, 2, \dots , \tag {10.3.1}
$$

where $\hat { a } _ { n } , \hat { b } _ { n }$ $\hat { b } _ { n }$ , and $\hat { c } _ { n }$ can be thought of as estimates of the “trend level” $a _ { n }$ , “trend slope” $b _ { n }$ , and “seasonal component” $c _ { n }$ at time $n$ . If $k$ is the smallest integer such that $n + h - k d \leq n$ , then we set

$$
\hat {c} _ {n + h} = \hat {c} _ {n + h - k d}, \quad h = 1, 2, \dots , \tag {10.3.2}
$$

while the values of $\hat { a } _ { i } , \hat { b } _ { i }$ , and $\hat { c } _ { i }$ , $i = d { + } 2 , \ldots , n$ , are found from recursions analogous to (10.2.3) and (10.2.4), namely,

$$
\hat {a} _ {n + 1} = \alpha \left(Y _ {n + 1} - \hat {c} _ {n + 1 - d}\right) + (1 - \alpha) \left(\hat {a} _ {n} + \hat {b} _ {n}\right), \tag {10.3.3}
$$

$$
\hat {b} _ {n + 1} = \beta (\hat {a} _ {n + 1} - \hat {a} _ {n}) + (1 - \beta) \hat {b} _ {n}, \tag {10.3.4}
$$

and

$$
\hat {c} _ {n + 1} = \gamma \left(Y _ {n + 1} - \hat {a} _ {n + 1}\right) + (1 - \gamma) \hat {c} _ {n + 1 - d}, \tag {10.3.5}
$$

with initial conditions

$$
\hat {a} _ {d + 1} = Y _ {d + 1}, \tag {10.3.6}
$$

$$
\hat {b} _ {d + 1} = \left(Y _ {d + 1} - Y _ {1}\right) / d, \tag {10.3.7}
$$

and

$$
\hat {c} _ {i} = Y _ {i} - \left(Y _ {1} + \hat {b} _ {d + 1} (i - 1)\right), \quad i = 1, \dots , d + 1. \tag {10.3.8}
$$

Then (10.3.3)–(10.3.5) can be solved successively for aˆi, $\hat { b } _ { i }$ , and $\hat { c } _ { i }$ , $i = d + 1 , \ldots , n$ and the predictors $P _ { n } Y _ { n + h }$ found from (10.3.1).

As in the nonseasonal case of Section 10.2, the forecasts depend on the parameters $\alpha , \beta$ , and γ . These can either be prescribed arbitrarily (with values between 0 and 1) or chosen in a more systematic way to minimize the sum of squares of the one-step errors ${ \textstyle \sum _ { i = d + 2 } ^ { n } } ( Y _ { i } - P _ { i - 1 } Y _ { i } ) ^ { 2 }$ , obtained when the algorithm is applied to the already observed data. Seasonal Holt–Winters forecasts can be computed by selecting the ITSM option Forecasting>Seasonal Holt-Winters.

Example 10.3.1 As in Example 10.2.1, open the file DEATHS.TSM, but this time select Forecasting>Seasonal Holt-Winters. Specify 24 for the number of predicted

values required, 12 for the period of the seasonality, and check the box marked Optimize Coefficients. Click OK, and the graph of the data and predicted values shown in Figure 10-3 will appear. Right-click on the graph and then on Info and you will see the numerical values of the predictors and the optimal values of the coefficients α, β, and $\gamma$ (minimizing the observed one-step average squared error $\textstyle \sum _ { i = 1 4 } ^ { 7 2 } ( Y _ { i } - P _ { i - 1 } ) Y _ { i } ) ^ { 2 } / 5 9 )$ . Table 10.3 compares the predictors of $Y _ { 7 3 } , \dots , Y _ { 7 8 }$ $Y _ { 7 3 }$ $Y _ { 7 8 }$ with the corresponding observed values.

The root mean squared error $( \sqrt { \sum _ { h = 1 } ^ { 6 } ( Y _ { 7 2 + h } - P _ { 7 2 } Y _ { 7 2 + h } ) ^ { 2 } / 6 }$ ) for the seasonal Holt–Winters forecasts is found to be 401. This is not as good as the value 253 achieved by the ARAR model for this example but is substantially better than the values achieved by the nonseasonal Holt–Winters algorithm (1143) and the ARIMA models (6.5.8) and (6.5.9) (583 and 501, respectively).

# 10.3.2 Holt–Winters Seasonal and ARIMA Forecasting

As in Section 10.2.2, the Holt–Winters seasonal recursions with seasonal period $d$ correspond to the large-sample forecast recursions of an ARIMA process, in this case defined by

$$
\begin{array}{l} (1 - B) (1 - B ^ {d}) Y _ {t} = Z _ {t} + \dots + Z _ {t - d + 1} + \gamma (1 - \alpha) (Z _ {t - d} - Z _ {t - d - 1}) \\ - (2 - \alpha - \alpha \beta) \left(Z _ {t - 1} + \dots + Z _ {t - d}\right) \\ + (1 - \alpha) \left(Z _ {t - 2} + \dots + Z _ {t - d - 1}\right), \\ \end{array}
$$

where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \big ( 0 , \sigma ^ { 2 } \big )$ . Holt–Winters seasonal forecasting with optimal α, β, and $\gamma$ can therefore be viewed as fitting a member of this four-parameter family of ARIMA models and using the corresponding large-sample forecast recursions.

Table 10.3 Predicted and observed values of the accidental deaths series for $t = 7 3 , \dots , 7 8$ from the seasonal Holt–Winters algorithm   

<table><tr><td>t</td><td>73</td><td>74</td><td>75</td><td>76</td><td>77</td><td>78</td></tr><tr><td>Observed Yt</td><td>7798</td><td>7406</td><td>8363</td><td>8460</td><td>9217</td><td>9316</td></tr><tr><td>Predicted by HWS</td><td>8039</td><td>7077</td><td>7750</td><td>7941</td><td>8824</td><td>9329</td></tr></table>

# 10.4 Choosing a Forecasting Algorithm

Real data are rarely if ever generated by a simple mathematical model such as an ARIMA process. Forecasting methods that are predicated on the assumption of such a model are therefore not necessarily the best, even in the mean squared error sense. Nor is the measurement of error in terms of mean squared error necessarily always the most appropriate one in spite of its mathematical convenience. Even within the framework of minimum mean squared-error forecasting, we may ask (for example) whether we wish to minimize the one-step, two-step, or twelve-step mean squared error.

![](images/927b35ed4f00437cbd0e9d34fef690ffb90515f9ce719632706406e307e33c0b.jpg)  
Figure 10-3 The data set DEATHS.TSM with 24 values predicted by the seasonal Holt–Winters algorithm

The use of more heuristic algorithms such as those discussed in this chapter is therefore well worth serious consideration in practical forecasting problems. But how do we decide which method to use? A relatively simple solution to this problem, given the availability of a substantial historical record, is to choose among competing algorithms by comparing the relevant errors when the algorithms are applied to the data already observed (e.g., by comparing the mean absolute percentage errors of the 12-step predictors of the historical data if 12-step prediction is of primary concern).

It is extremely difficult to make general theoretical statements about the relative merits of the various techniques we have discussed (ARIMA modeling, exponential smoothing, ARAR, and HW methods). For the series DEATHS.TSM we found on the basis of average mean squared error for predicting the series at times 73–78 that the ARAR method was best, followed by the seasonal Holt–Winters algorithm, and then the ARIMA models fitted in Chapter 6. This ordering is by no means universal. For example, if we consider the natural logarithms $\{ Y _ { t } \}$ of the first 130 observations in the series WINE.TSM (Figure 1-1) and compare the average mean squared errors of the forecasts of $Y _ { 1 3 1 } , \dots , Y _ { 1 4 2 }$ $Y _ { 1 4 2 }$ , we find (Problem 10.2 that an MA(12) model fitted to the mean corrected differenced series $\{ Y _ { t } ~ - ~ Y _ { t - 1 2 } \}$ does better than seasonal Holt–Winters (with period 12), which in turn does better than ARAR and (not surprisingly) dramatically better than nonseasonal Holt–Winters. An interesting empirical comparison of these and other methods applied to a variety of economic time series is contained in Makridakis et al. (1984).

The versions of the Holt–Winters algorithms we have discussed in Sections 10.2 and 10.3 are referred to as “additive,” since the seasonal and trend components enter the forecasting function in an additive manner. “Multiplicative” versions of the algorithms can also be constructed to deal directly with processes of the form

$$
Y _ {t} = m _ {t} s _ {t} Z _ {t}, \tag {10.4.1}
$$

where mt, st, and $Z _ { t }$ are trend, seasonal, and noise factors, respectively (see, e.g., Makridakis et al. 1997). An alternative approach (provided that $Y _ { t } ~ > ~ 0$ for all $t$ ) is to apply the linear Holt–Winters algorithms to $\{ \ln { Y _ { t } } \}$ (as in the case of WINE.TSM in the preceding paragraph). Because of the rather general memory shortening permitted by the ARAR algorithm, it gives reasonable results when applied directly to series of the form (10.4.1), even without preliminary transformations. In particular, if we

![](images/e486dc77fb825b378d961686f08dd806eb91a0c57367954385b6cf3d6226bc94.jpg)  
Figure 10-4 The first 132 values of the data set AIRPASS.TSM and predictors of the last 12 values obtained by direct application of the ARAR algorithm

consider the first 132 observations in the series AIRPASS.TSM and apply the ARAR algorithm to predict the last 12 values in the series, we obtain (Problem 10.4) an observed root mean squared error of 18.22. On the other hand if we use the same data take logarithms, difference at lag 12, subtract the mean and then fit an AR(13) model by maximum likelihood using ITSM and use it to predict the last 12 values, we obtain an observed root mean squared error of 21.17. The data and predicted values from the ARAR algorithm are shown in Figure 10-4.

# Problems

10.1 Establish the formula (10.1.8) for the mean squared error of the $h$ -step forecast based on the ARAR algorithm.

10.2 Let $\{ X _ { 1 } , \dots , X _ { 1 4 2 } \}$ denote the data in the file WINE.TSM and let $\{ Y _ { 1 } , \ldots , Y _ { 1 4 2 } \}$ denote their natural logarithms. Denote by $m$ the sample mean of the differenced series $\{ Y _ { t } - Y _ { t - 1 2 }$ , $t = 1 3 , \dots , 1 3 0 \}$ .

(a) Use the program ITSM to find the maximum likelihood MA(12) model for the differenced and mean-corrected series $\{ Y _ { t } - Y _ { t - 1 2 } - m$ , t  13, . . . , 130 .   
(b) Use the model in (a) to compute forecasts of $\{ X _ { 1 3 1 } , \ldots , X _ { 1 4 2 } \}$ .   
(c) Tabulate the forecast errors $\{ X _ { t } - P _ { 1 3 0 } X _ { t }$ , $t = 1 3 1$ , . . . , 142}.   
(d) Compute the average squared error for the 12 forecasts.   
(e) Repeat steps (b), (c), and (d) for the corresponding forecasts obtained by applying the ARAR algorithm to the series $\{ X _ { t } , t = 1 , \ldots , 1 3 0 \}$ .   
(f) Repeat steps (b), (c), and (d) for the corresponding forecasts obtained by applying the seasonal Holt–Winters algorithm (with period 12) to the logged data $\{ Y _ { t } , t \quad = \quad 1 , \ldots , 1 3 0 \}$ . (Open the file WINE.TSM, select Transform>Box-Coxwith parameter $\lambda = 0$ , then select Forecasting> Seasonal Holt-Winters, and check Apply to original data in the dialog box.)

(g) Repeat steps (b), (c), and (d) for the corresponding forecasts obtained by applying the nonseasonal Holt–Winters algorithm to the logged data $\{ Y _ { t } , t =$ $1 , \ldots , 1 3 0 \}$ . (The procedure is analogous to that described in part (f).)   
(h) Compare the average squared errors obtained by the four methods.

10.3 In equations (10.2.10) and (10.2.11), show that $\alpha { = } \Omega _ { 1 1 } / \big ( \Omega _ { 1 1 } ~ + ~ \sigma _ { w } ^ { 2 } \big )$ and $\scriptstyle \beta = \Omega _ { 2 1 } / \Omega _ { 1 1 }$ .

10.4 Verify the assertions made in the last paragraph of Section 10.4, comparing the forecasts of the last 12 values of the series AIRPASS.TSM obtained from the ARAR algorithm (with no log transformation) and the corresponding forecasts obtained by taking logarithms of the original series, then differencing at lag 12, mean-correcting, and fitting an AR(13) model to the transformed series.

# Further Topics

11.1 Transfer Function Models   
11.2 Intervention Analysis   
11.3 Nonlinear Models   
11.4 Long-Memory Models   
11.5 Continuous-Time ARMA Processes

In this chapter we touch on a variety of topics of special interest. In Section 11.1 we consider transfer function models, designed to exploit for predictive purposes the relationship between two time series when one acts as a leading indicator for the other. Section 11.2 deals with intervention analysis, which allows for possible changes in the mechanism generating a time series, causing it to have different properties over different time intervals. In Section 11.3 we introduce the very fast growing area of nonlinear time series analysis, and in Section 11.4 we discuss fractionally integrated ARMA processes, sometimes called “long-memory” processes on account of the slow rate of convergence of their autocorrelation functions to zero as the lag increases. In Section 11.5 we discuss continuous-time ARMA processes which, for continuously evolving processes, play a role analogous to that of ARMA processes in discrete time. Besides being of interest in their own right, they have proved a useful class of models in the representation of financial time series and in the modeling of irregularly spaced data.

# 11.1 Transfer Function Models

In this section we consider the problem of estimating the transfer function of a linear filter when the output includes added uncorrelated noise. Suppose that $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ are, respectively, the input and output of the transfer function model

$$
X _ {t 2} = \sum_ {j = 0} ^ {\infty} \tau_ {j} X _ {t - j, 1} + N _ {t}, \tag {11.1.1}
$$

where $T = \{ \tau _ { j } , j = 0 , 1 , . . . \}$ is a causal time-invariant linear filter and $\{ N _ { t } \}$ is a zero-mean stationary process, uncorrelated with the input process $\{ X _ { t 1 } \}$ . We further assume that $\{ X _ { t 1 } \}$ is a zero-mean stationary time series. Then the bivariate process $\{ ( X _ { t 1 } , X _ { t 2 } ) ^ { \prime } \}$ is also stationary. Multiplying each side of (11.1.1) by $X _ { t - k , 1 }$ and then taking expectations gives the equation

$$
\gamma_ {2 1} (k) = \sum_ {j = 0} ^ {\infty} \tau_ {j} \gamma_ {1 1} (k - j). \tag {11.1.2}
$$

Equation (11.1.2) simplifies a great deal if the input process happens to be white noise. For example, if $\{ X _ { t 1 } \} \sim \mathrm { W N } ( 0 , \sigma _ { 1 } ^ { 2 } )$ , then we can immediately identify $t _ { k }$ from (11.1.2) as

$$
\tau_ {k} = \gamma_ {2 1} (k) / \sigma_ {1} ^ {2}. \tag {11.1.3}
$$

This observation suggests that “prewhitening” of the input process might simplify the identification of an appropriate transfer function model and at the same time provide simple preliminary estimates of the coefficients $t _ { k }$ .

If $\{ X _ { t 1 } \}$ can be represented as an invertible ARMA $( p , q )$ process

$$
\phi (B) X _ {t 1} = \theta (B) Z _ {t}, \quad \left\{Z _ {t} \right\} \sim \operatorname {W N} \left(0, \sigma_ {\mathrm {Z}} ^ {2}\right), \tag {11.1.4}
$$

then application of the filter $\pi ( B ) = \phi ( B ) \theta ^ { - 1 } ( B )$ to $\{ X _ { t 1 } \}$ will produce the whitened series $\{ Z _ { t } \}$ . Now applying the operator $\pi ( B )$ to each side of (11.1.1) and letting $Y _ { t } =$ $\pi ( B ) X _ { t 2 }$ , we obtain the relation

$$
Y _ {t} = \sum_ {j = 0} ^ {\infty} \tau_ {j} Z _ {t - j} + N _ {t} ^ {\prime},
$$

where

$$
N _ {t} ^ {\prime} = \pi (B) N _ {t},
$$

and $\{ N _ { t } ^ { \prime } \}$ is a zero-mean stationary process, uncorrelated with $\{ Z _ { t } \}$ . The same arguments that led to (11.1.3) therefore yield the equation

$$
\tau_ {j} = \rho_ {Y Z} (j) \sigma_ {Y} / \sigma_ {Z}, \tag {11.1.5}
$$

where $\rho _ { Y Z }$ is the cross-correlation function of $\{ Y _ { t } \}$ and $\{ Z _ { t } \} , ~ \sigma _ { Z } ^ { 2 } { \mathrm { ~ \scriptsize ~ = V a r } } ( Z _ { t } )$ , and $\sigma _ { Y } ^ { 2 } = \mathrm { V a r } ( Y _ { t } )$ .

Given the observations $\{ ( X _ { t 1 } , X _ { t 2 } ) ^ { \prime } , t \ = \ 1 , \ldots , n \}$ , the results of the previous paragraph suggest the following procedure for estimating $\{ \tau _ { j } \}$ and analyzing the noise $\{ N _ { t } \}$ in the model (11.1.1):

1. Fit an ARMA model to $\{ X _ { t 1 } \}$ and file the residuals $( { \hat { Z } } _ { 1 } , \dots , { \hat { Z } } _ { n } )$ (using the Export button in ITSM to copy them to the clipboard and then pasting them into the first column of an Excel file). Let $\hat { \phi }$ and $\hat { \pmb { \theta } }$ denote the maximum likelihood estimates of the autoregressive and moving-average parameters and let $\hat { \sigma } _ { z } ^ { 2 }$ be the maximum likelihood estimate of the variance of $\{ Z _ { t } \}$ .   
2. Apply the operator $\hat { \pi } ( B ) = \hat { \phi } ( B ) \hat { \theta } ^ { - 1 } ( \stackrel { \cdot } { B } )$ to $\{ X _ { t 2 } \}$ to obtain the series $\left( { \hat { Y } } _ { 1 } , \ldots , { \hat { Y } } _ { n } \right)$ . (After fitting the ARMA model as in Step 1 above, highlight the window containing the graph of $\{ X _ { t } \}$ and replace $\{ X _ { t } \}$ by $\{ Y _ { t } \}$ using the option $\mathtt { F i l e } { > } \mathtt { I m p o r t }$ . The residuals are then automatically replaced by the residuals of $\{ Y _ { t } \}$ under the model already fitted to $\{ X _ { t } \}$ .) Export the new residuals to the clipboard, paste them into the second column of the Excel file created in Step 1, and save this as a text file, FNAME.TSM. The file FNAME.TSM then contains the bivariate series $\{ ( Z _ { t } , Y _ { t } ) \}$ . Let $\hat { \sigma } _ { _ { Y } } ^ { 2 }$ denote the sample variance of $\hat { Y } _ { t }$ .

3. Compute the sample auto- and cross-correlation functions of $\{ Z _ { t } \}$ and $\{ Y _ { t } \}$ by opening the bivariate project FNAME.TSM in ITSM and clicking on the second yellow button at the top of the ITSM window. Comparison of $\hat { \rho } _ { _ { Y Z } } ( h )$ with the bounds $\pm 1 . 9 6 n ^ { - 1 / 2 }$ gives a preliminary indication of the lags $h$ at which $\rho _ { _ { Y Z } } ( h )$ is significantly different from zero. A more refined check can be carried out by using Bartlett’s formula in Section 8.3.4 for the asymptotic variance of $\hat { \rho } _ { _ { Y Z } } ( h )$ . Under the assumptions that $\left\{ \hat { Z } _ { t } \right\} \sim \mathrm { W N } \left( 0 , \hat { \pmb { \sigma } } _ { \mathrm { z } } ^ { 2 } \right)$ and $\left\{ \left( \hat { Y } _ { t } , \hat { Z } _ { t } \right) ^ { \prime } \right\}$ is a stationary Gaussian process,

$$
\begin{array}{l} n \mathrm {V a r} (\hat {\rho} _ {Y Z} (h)) \sim 1 - \rho_ {Y Z} ^ {2} (h) \left[ 1. 5 - \sum_ {k = - \infty} ^ {\infty} (\rho_ {Y Z} ^ {2} (k) + \rho_ {Y Y} ^ {2} (k) / 2) \right] \\ + \sum_ {k = - \infty} ^ {\infty} \left[ \rho_ {Y Z} (h + k) \rho_ {Y Z} (h - k) - 2 \rho_ {Y Z} (h) \rho_ {Y Z} (k + h) \rho_ {Y Y} ^ {2} (k) \right]. \\ \end{array}
$$

In order to check the hypothesis $H _ { 0 }$ that $\rho _ { Y Z } ( h ) = 0 , h \notin [ a , b ]$ , where $a$ and $^ b$ are integers, we note from Corollary 8.3.1 that under $H _ { 0 }$ ,

$$
\operatorname {V a r} \left(\hat {\rho} _ {Y Z} (h)\right) \sim n ^ {- 1} \qquad \text {f o r} h \notin [ a, b ].
$$

We can therefore check the hypothesis $H _ { 0 }$ by comparing $\hat { \rho } _ { Y Z } , h \notin [ a , b ]$ , with the bounds $\pm \ : 1 . 9 6 n ^ { - 1 / 2 }$ . Observe that $\rho _ { Z Y } ( h )$ should be zero for $h > 0$ if the model (11.1.1) is valid.

4. Preliminary estimates of $\tau _ { h }$ for the lags $h$ at which $\hat { \rho } _ { _ { Y Z } } ( h )$ is significantly different from zero are

$$
\hat {\tau} _ {h} = \hat {\rho} _ {Y Z} (h) \hat {\sigma} _ {Y} / \hat {\sigma} _ {Z}.
$$

For other values of $h$ the preliminary estimates are $\hat { \tau } _ { h } = 0$ . The numerical values of the cross-correlations $\hat { \rho } _ { Y Z } ( h )$ are found by right-clicking on the graphs of the sample correlations plotted in Step 3 and then on Info. The values of $\hat { \sigma } _ { Z }$ and $\hat { \sigma } _ { Y }$ are found by doing the same with the graphs of the series themselves. Let $m \geq 0$ be the largest value of $j$ such that $\hat { \tau } _ { j }$ is nonzero and let $b \geq 0$ be the smallest such value. Then $^ b$ is known as the delay parameter of the filter $\{ \hat { \tau } _ { j } \}$ . If $m$ is very large and if the coefficients $\left\{ \hat { \tau } _ { j } \right\}$ are approximately related by difference equations of the form

$$
\hat {\tau} _ {j} - v _ {1} \hat {\tau} _ {j - 1} - \dots - v _ {p} \hat {\tau} _ {j - p} = 0, \qquad j \geq b + p,
$$

then $\begin{array} { r } { { \hat { T } } ( B ) = \sum _ { j = b } ^ { m } { \hat { \tau } } _ { j } B ^ { j } } \end{array}$ can be represented approximately, using fewer parameters, as

$$
\hat {T} (B) = w _ {0} (1 - v _ {1} B - \dots - v _ {p} B _ {p}) ^ {- 1} B ^ {b}.
$$

In particular, if $\hat { \tau } _ { j } = 0$ , $j < b$ , and $\hat { \tau } _ { j } = w _ { 0 } \nu _ { 1 } ^ { j - b }$ , $j \geq b$ , then

$$
\hat {T} (B) = w _ {0} \left(1 - v _ {1} B\right) ^ {- 1} B ^ {b}. \tag {11.1.6}
$$

Box and Jenkins (1976) recommend choosing $\hat { T } ( B )$ to be a ratio of two polynomials. However, the degrees of the polynomials are often difficult to estimate from $\left\{ \hat { \tau } _ { j } \right\}$ . The primary objective at this stage is to find a parametric function that provides an adequate approximation to $\hat { T } ( B )$ without introducing too large a number of parameters. If $\hat { \hat { T } } ( B )$ is represented as ${ \hat { T } } ( B ) ~ = ~ B ^ { b } w ( B ) \nu ^ { - 1 } ( B ) ~ =$ $B ^ { b } \left( w _ { 0 } + w _ { 1 } B + \cdot \cdot \cdot + w _ { q } B ^ { q } \right)$ $\left( 1 - \nu _ { 1 } B - \cdot \cdot \cdot - \nu _ { p } B ^ { p } \right) ^ { - 1 }$ with $\nu ( z ) \neq 0$ for $| z | \leq 1$ , then we define $m = \operatorname* { m a x } ( q + b , p )$ .

5. The noise sequence $\{ N _ { t } , t = m + 1 , \ldots , n \}$ is estimated as

$$
\hat {N} _ {t} = X _ {t 2} - \hat {T} (B) X _ {t 1}.
$$

(We set $\hat { N } _ { t } = 0$ , t  m, in order to compute $\hat { N } _ { t }$ $, t > m = \operatorname* { m a x } ( b + q , p ) )$ . The calculations are done in ITSM by opening the bivariate file containing $\{ ( X _ { t 1 } , X _ { t 2 } ) \}$ , selecting Transfer>Specify Model, and entering the preliminary model found in Step 4. Click on the fourth green button to see a graph of the residuals $\{ N _ { t } \}$ . These should then be filed as, say, NOISE.TSM.

6. Preliminary identification of a suitable model for the noise sequence is carried out by fitting a causal invertible ARMA model

$$
\phi^ {(N)} (B) N _ {t} = \theta^ {(N)} (B) W _ {t}, \quad \left\{W _ {t} \right\} \sim \mathrm {W N} \left(0, \sigma_ {\mathrm {W}} ^ {2}\right), \tag {11.1.7}
$$

to the estimated noise $\hat { N } _ { m + 1 } , \hdots , \hat { N } _ { n }$ filed as NOISE.TSM in Step 5.

7. At this stage we have the preliminary model

$$
\phi^ {(N)} (B) v (B) X _ {t 2} = B ^ {b} \phi^ {(N)} (B) w (B) X _ {t 1} + \theta^ {(N)} (B) v (B) W _ {t},
$$

where ${ \hat { T } } ( B ) ~ = ~ B ^ { b } w ( B ) \nu ^ { - 1 } ( B )$ as in step (4). For this model we can compute $\hat { W } _ { t } \left( \mathbf { w } , \mathbf { v } , \phi ^ { ( N ) } , \theta ^ { ( N ) } \right)$ , $t \ > \ m ^ { * } = \ \mathrm { m a x } ( p _ { 2 } + p , b + p _ { 2 } + q )$ , by setting $\hat { W } _ { t } \stackrel { \cdot } { = } 0$ for $t \leq m ^ { * }$ . The parameters ${ \mathbf w } , { \mathbf v } , \phi ^ { ( N ) }$ , and ${ \pmb \theta } ^ { ( N ) }$ can then be reestimated (more efficiently) by minimizing the sum of squares

$$
\sum_ {t = m ^ {*} + 1} ^ {n} \hat {W} _ {t} ^ {2} \left(\mathbf {w}, \mathbf {v}, \boldsymbol {\phi} ^ {(N)}, \boldsymbol {\theta} ^ {(N)}\right).
$$

(The calculations are performed in ITSM by opening the bivariate project $\{ ( X _ { t 1 }$ , $X _ { t 2 } ,$ ) , selecting Transfer $\cdot >$ Specify model, entering the preliminary model, and clicking OK. Then choose Transfer>Estimation, click OK, and the least squares estimates of the parameters will be computed. Pressing the fourth green button at the top of the screen will give a graph of the estimated residuals $\hat { W } _ { t }$ .)

8. To test for goodness of fit, the estimated residuals $\left\{ \hat { W } _ { t } , t > m ^ { * } \right\}$ and $\left\{ \hat { Z } _ { t } . t > m ^ { * } \right\}$ should be filed as a bivariate series and the auto- and cross correlations compared with the bounds $\pm 1 . 9 6 / \sqrt { n }$ in order to check the hypothesis that the two series are uncorrelated white noise sequences. Alternative models can be compared using the AICC value that is printed with the estimated parameters in Step 7. It is computed from the exact Gaussian likelihood, which is computed using a state-space representation of the model, described in Brockwell and Davis (1991), Section 13.1.

# Example 11.1.1 Sales with a Leading Indicator

In this example we fit a transfer function model to the bivariate time series of Example 8.1.2. Let

$$
X _ {t 1} = (1 - B) Y _ {t 1} - 0. 0 2 2 8, \quad t = 2, \dots , 1 5 0,
$$

$$
X _ {t 2} = (1 - B) Y _ {t 2} - 0. 4 2 0, \quad t = 2, \dots , 1 5 0,
$$

where $\{ Y _ { t 1 } \}$ and $\{ Y _ { t 2 } \}$ , $t = 1 , \ldots , 1 5 0$ , are the leading indicator and sales data, respectively. It was found in Example 8.1.2 that $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ can be modeled as low-order zero-mean ARMA processes. In particular, we fitted the model

$$
X _ {t 1} = (1 - 0. 4 7 4 B) Z _ {t}, \quad \{Z _ {t} \} \sim \mathrm {W N} (0, 0. 0 7 7 9),
$$

to the series $\{ X _ { t 1 } \}$ . We can therefore whiten the series by application of the filter $\hat { \pi } ( B ) = ( 1 - 0 . 4 7 4 B ) ^ { - 1 }$ . Applying ${ \hat { \pi } } ( B )$ to both $\{ X _ { t 1 } \}$ and $\{ X _ { t 2 } \}$ we obtain

$$
\hat {Z} _ {t} = (1 - 0. 4 7 4 B) ^ {- 1} X _ {t 1}, \quad \hat {\sigma} _ {Z} ^ {2} = 0. 0 7 7 9,
$$

$$
\hat {Y} _ {t} = (1 - 0. 4 7 4 B) ^ {- 1} X _ {t 2}, \quad \hat {\sigma} _ {Y} ^ {2} = 4. 0 2 1 7.
$$

These calculations and the filing of the series $\bigl \{ \hat { Z } _ { t } \bigr \}$ and $\left\{ \hat { Y } _ { t } \right\}$ were carried out using ITSM as described in steps (1) and (2). Their sample auto- and cross-correlations, found as described in step (3), are shown in Figure 11-1. The cross-correlations $\hat { \rho } _ { Z Y } ( h )$ (top right) and $\hat { \rho } _ { Y Z } ( h )$ (bottom left), when compared with the upper and lower bounds $\pm 1 . 9 6 ( 1 4 9 ) ^ { - 1 / 2 } = \pm 0 . 1 6 1$ , strongly suggest a transfer function model for $\{ X _ { t 2 } \}$ in terms of $\{ X _ { t 1 } \}$ with delay parameter 3. Since $\hat { \tau } _ { j } = \hat { \rho } _ { Y Z } ( j ) \hat { \sigma } _ { Y } / \hat { \sigma } _ { Z }$ is decreasing approximately geometrically for $j \geq 3$ , we take $T ( B )$ to have the form (11.1.6), i.e.,

$$
T (B) = w _ {0} (1 - \nu_ {1} B) ^ {- 1} B ^ {3}.
$$

The preliminary estimates of $w _ { 0 }$ and $\nu _ { 1 }$ are $\hat { w } _ { 0 } = \hat { \tau } _ { 3 } = 4 . 8 6$ and $\hat { \nu } _ { 1 } = \hat { \tau } _ { 4 } / \hat { \tau } _ { 3 } = 0 . 6 9 8$ , the coefficients $\tau _ { j }$ being estimated as described in step (4). The estimated noise sequence is determined and filed using ITSM as described in step (5). It satisfies the equations

$$
\hat {N} _ {t} = X _ {t 2} - 4. 8 6 B ^ {3} (1 - 0. 6 9 8 B) ^ {- 1} X _ {t 1}, \quad t = 5, 6, \dots , 1 5 0.
$$

Analysis of this univariate series with ITSM gives the MA(1) model

$$
N _ {t} = (1 - 0. 3 6 4 B) W _ {t}, \quad \left\{W _ {t} \right\} \sim \operatorname {W N} (0, 0. 0 5 9 0).
$$

Substituting these preliminary noise and transfer function models into equation (11.1.1) then gives

$$
X _ {t 2} = 4. 8 6 B ^ {3} (1 - 0. 6 9 8 B) ^ {- 1} X _ {t 1} + (1 - 0. 3 6 4 B) W _ {t}, \quad \{W _ {t} \} \sim \mathrm {W N} (0, 0. 0 5 9 0).
$$

Now minimizing the sum of squares (11.1.7) with respect to the parameters $\left( w _ { 0 } , \nu _ { 1 } \right.$ , $\theta _ { 1 } ^ { ( N ) } )$ as described in step (7), we obtain the least squares model

$$
X _ {t 2} = 4. 7 1 7 B ^ {3} (1 - 0. 7 2 4 B) ^ {- 1} X _ {t 1} + (1 - 0. 5 8 2 B) W _ {t}, \tag {11.1.8}
$$

where $\{ W _ { t } \} \sim \mathrm { W N } ( 0 , 0 . 0 4 8 6 )$ and

$$
X _ {t 1} = (1 - 0. 4 7 4 B) Z _ {t}, \quad \{Z _ {t} \} \sim \operatorname {W N} (0, 0. 0 7 7 9).
$$

Notice the reduced white noise variance of $\{ W _ { t } \}$ in the least squares model as compared with the preliminary model.

The sample auto- and cross-correlation functions of the series $\hat { Z } _ { t }$ and $\hat { W } _ { t }$ , $t = 5 , \ldots , 1 5 0$ , are shown in Figure 11-2. All of the correlations lie between the bounds

$\pm 1 . 9 6 / \sqrt { 1 4 4 }$ , supporting the assumption underlying the fitted model that the residuals are uncorrelated white noise sequences.

# 11.1.1 Prediction Based on a Transfer Function Model

When predicting $X _ { n + h , 2 }$ on the basis of the transfer function model defined by (11.1.1), (11.1.4), and (11.1.7), with observations of $X _ { t 1 }$ and $X _ { t 2 }$ , $t = 1 , \ldots , n$ , our aim is to find the linear combination of $1 , X _ { 1 1 } , \ldots , X _ { n 1 } , X _ { 1 2 } , \ldots , X _ { n 2 }$ that predicts $X _ { n + h , 2 }$ with minimum mean squared error. The exact solution of this problem can be found with the help of the Kalman recursions (see Brockwell and Davis (1991), Section 13.1 for details). The program ITSM uses these recursions to compute the predictors and their mean squared errors.

In order to provide a little more insight, we give here the predictors $\tilde { P } _ { n } X _ { n + h }$ and mean squared errors based on infinitely many past observations $X _ { t 1 }$ and $X _ { t 2 }$ ,

$- \infty < t \leq n$ . These predictors and their mean squared errors will be close to those based on $X _ { t 1 }$ and $X _ { t 2 }$ , $1 \leq t \leq n$ , if $n$ is sufficiently large.

The transfer function model defined by (11.1.1), (11.1.4), and (11.1.7) can be rewritten as

$$
X _ {t 2} = T (B) X _ {t 1} + \beta (B) W _ {t}, \tag {11.1.9}
$$

$$
X _ {t 1} = \theta (B) \phi^ {- 1} (B) Z _ {t}, \tag {11.1.10}
$$

where $\beta ( B ) = \theta ^ { ( N ) } ( B ) / \phi ^ { ( N ) } ( B )$ . Eliminating $X _ { t 1 }$ gives

$$
X _ {t 2} = \sum_ {j = 0} ^ {\infty} \alpha_ {j} Z _ {t - j} + \sum_ {j = 0} ^ {\infty} \beta_ {j} W _ {t - j}, \tag {11.1.11}
$$

where $\alpha ( B ) = T ( B ) \theta ( B ) / \phi ( B )$ .

Noting that each limit of linear combinations of $\{ X _ { t 1 } , X _ { t 2 } , - \infty \mathrm { ~ < ~ } t \mathrm { ~ \leq ~ } n \}$ is a limit of linear combinations of $\{ Z _ { t } , W _ { t } , - \infty < t \leq n \}$ and conversely and that $\{ Z _ { t } \}$ and $\{ W _ { t } \}$ are uncorrelated, we see at once from (11.1.11) that

$$
\tilde {P} _ {n} X _ {n + h, 2} = \sum_ {j = h} ^ {\infty} \alpha_ {j} Z _ {n + h - j} + \sum_ {j = h} ^ {\infty} \beta_ {j} W _ {n + h - j}. \tag {11.1.12}
$$

Setting $t = n + h$ in (11.1.11) and subtracting (11.1.12) gives the mean squared error

$$
E \left(X _ {n + h, 2} - \tilde {P} _ {n} X _ {n + h, 2}\right) ^ {2} = \sigma_ {Z} ^ {2} \sum_ {j = 0} ^ {h - 1} \alpha_ {j} ^ {2} + \sigma_ {W} ^ {2} \sum_ {j = 0} ^ {h - 1} \beta_ {j} ^ {2}. \tag {11.1.13}
$$

To compute the predictors $\tilde { P } _ { n } X _ { n + h , 2 }$ we proceed as follows. Rewrite (11.1.9) as

$$
A (B) X _ {t 2} = B ^ {b} U (B) X _ {t 1} + V (B) W _ {t}, \tag {11.1.14}
$$

![](images/c724ed4a9aba46df4151e378050729bf4c2c2f62e14b4d3bf9f5dfe19aaf4776.jpg)

![](images/4b1b6f796479e33920c6ae7b6e0c79b9a2e1be02b75b9f71e0fcaff1cb8d98fe.jpg)

![](images/29c3e4e87a6d025c2a874102b54df5ad35be8d88e9c6676c5cd0312c983d6381.jpg)

![](images/06c553e4ac158ffbbf9d98ead2aae165f4ff340a6a6e62b1e2c307e3754e937e.jpg)  
Figure 11-1 The sample correlation functions $\hat { \rho } _ { i j } ( h )$ , of Example 11.1.1. Series 1 is $\{ \hat { Z } _ { t } \}$ and Series 2 is $\{ \hat { Y } _ { t } \}$

![](images/9932d0b24d6dd2de11c540e65aa1b06d94013fd15857b5c2d839a46f85edd2f8.jpg)

![](images/917bdf114718f6afafc82f443ed178f42710603d0d3e896ed4ad80420c3bb60b.jpg)

![](images/2dc6d2848afc5623430fa3f9366c737fd9686536b72c0d64601989a31fdea3ab.jpg)  
Figure 11-2 The sample correlation functions of the estimated residuals from the model fitted in Example 11.1.1. Series 1 is $\{ \hat { Z } _ { t } \}$ and Series 2 is $\{ \hat { W } _ { t } \}$

![](images/e70a5ee713851f9ee16c5da4551482d8bd8fc9a32973071d73e01fd216668a9b.jpg)

where A, $U$ , and $V$ are polynomials of the form

$$
A (B) = 1 - A _ {1} B - \dots - A _ {a} B ^ {a},
$$

$$
U (B) = U _ {0} + U _ {1} B + \dots + U _ {u} B ^ {u},
$$

$$
V (B) = 1 + V _ {1} B + \dots + V _ {\nu} B ^ {\nu}.
$$

Applying the operator $\tilde { P } _ { n }$ to equation (11.1.14) with $t = n + h$ , we obtain

$$
\tilde {P} _ {n} X _ {n + h, 2} = \sum_ {j = 1} ^ {a} A _ {j} \tilde {P} _ {n} X _ {n + h - j, 2} + \sum_ {j = 0} ^ {u} U _ {j} \tilde {P} _ {n} X _ {n + h - b - j, 1} + \sum_ {j = h} ^ {\nu} V _ {j} W _ {n + h - j}, \tag {11.1.15}
$$

where the last sum is zero if $h > \nu$ .

Since $\{ X _ { t 1 } \}$ is uncorrelated with $\{ W _ { t } \}$ , the predictors appearing in the second sum in (11.1.15) are therefore obtained by predicting the univariate series $\{ X _ { t 1 } \}$ as described in Section 3.3 using the model (11.1.10). In keeping with our assumption that $n$ is large, we can replace $\tilde { P } _ { n } X _ { j 1 }$ for each $j$ by the finite-past predictor obtained from the program ITSM. The values $W _ { j } , j \le n$ , are replaced by their estimated values $\hat { W } _ { j }$ from the least squares estimation in step (7) of the modeling procedure.

Equation (11.1.15) can now be solved recursively for the predictors $\tilde { P } _ { n } X _ { n + 1 , 2 }$ , $\tilde { P } _ { n } X _ { n + 2 , 2 } , \tilde { P } _ { n } X _ { n + 3 , 2 } , . . .$ .

# Example 11.1.2 Sales with a Leading Indicator

Applying the preceding results to the series $\{ X _ { t 1 } , X _ { t 2 } , 2 \leq t \leq 1 5 0 \}$ of Example 11.1.1, and using the values X148,1 0.093, $X _ { 1 5 0 , 2 } ~ = ~ 0 . 0 8$ , $\hat { W } _ { 1 5 0 } ~ = ~ - 0 . 0 7 0 6$ , $\hat { W } _ { 1 4 9 } ~ =$ 0.1449, we find from (11.1.8) and (11.1.15) that

$$
\tilde {P} _ {1 5 0} X _ {1 5 1, 2} = 0. 7 2 4 X _ {1 5 0, 2} + 4. 7 1 7 X _ {1 4 8, 1} - 1. 3 0 6 W _ {1 5 0} + 0. 4 2 1 W _ {1 4 9} = - 0. 2 2 8
$$

and, using the value $X _ { 1 4 9 , 1 } = 0 . 2 3 7$ , that

$$
\tilde {P} _ {1 5 0} X _ {1 5 2, 2} = 0. 7 2 4 \tilde {P} _ {1 5 0} X _ {1 5 1, 2} + 4. 7 1 7 X _ {1 4 9, 1} + 0. 4 2 1 W _ {1 5 0} = 0. 9 2 3.
$$

In terms of the original sales data $\{ Y _ { t 2 } \}$ we have $Y _ { 1 4 9 , 2 } = 2 6 2 . 7$ and

$$
Y _ {t 2} = Y _ {t - 1, 2} + X _ {t 2} + 0. 4 2 0.
$$

Hence the predictors of actual sales are

$$
P _ {1 5 0} ^ {*} Y _ {1 5 1, 2} = 2 6 2. 7 0 - 0. 2 2 8 + 0. 4 2 0 = 2 6 2. 8 9,
$$

$$
P _ {1 5 0} ^ {*} Y _ {1 5 2, 2} = 2 6 2. 8 9 + 0. 9 2 3 + 0. 4 2 0 = 2 6 4. 2 3,
$$

where $P _ { 1 4 9 } ^ { * }$ is based on $\{ 1 , Y _ { 1 1 } , Y _ { 1 2 } , X _ { s 1 } , X _ { s 2 } , - \infty < s \leq 1 5 0 \}$ $- \infty < s \le 1 5 0 \}$ , and it is assumed that $Y _ { 1 1 }$ and $Y _ { 1 2 }$ are uncorrelated with $\{ X _ { s 1 } \}$ and with $\{ X _ { s 2 } \}$ . The predicted values are in close agreement with those based on the finite number of available observations that are computed by ITSM. Since our model for the sales data is

$$
(1 - B) Y _ {t 2} = 0. 4 2 0 + 4. 7 1 7 B ^ {3} (1 - 0. 4 7 4 B) (1 - 0. 7 2 4 B) ^ {- 1} Z _ {t} + (1 - 0. 5 8 2 B) W _ {t},
$$

it can be shown, using an argument analogous to that which gave (11.1.13), that the mean squared errors are given by

$$
E (Y _ {1 5 0 + h, 2} - P _ {1 5 0} Y _ {1 5 0 + h, 2}) ^ {2} = \sigma_ {Z} ^ {2} \sum_ {j = 0} ^ {h - 1} \alpha_ {j} ^ {* 2} + \sigma_ {W} ^ {2} \sum_ {j = 0} ^ {h - 1} \beta_ {j} ^ {* 2},
$$

where

$$
\sum_ {j = 0} ^ {\infty} \alpha_ {j} ^ {*} z ^ {j} = 4. 7 1 7 z ^ {3} (1 - 0. 4 7 4 z) (1 - 0. 7 2 4 z) ^ {- 1} (1 - z) ^ {- 1}
$$

and

$$
\sum_ {j = 0} ^ {\infty} \beta_ {j} ^ {*} z ^ {j} = (1 - 0. 5 8 2 z) (1 - z) ^ {- 1}.
$$

For $h = 1$ and 2 we obtain

$$
E \left(Y _ {1 5 1, 2} - P _ {1 5 0} ^ {*} Y _ {1 5 1, 2}\right) ^ {2} = 0. 0 4 8 6,
$$

$$
E (Y _ {1 5 2, 2} - P _ {1 5 0} ^ {*} Y _ {1 5 2, 2}) ^ {2} = 0. 0 5 7 0,
$$

in close agreement with the finite-past mean squared errors obtained by ITSM.

It is interesting to examine the improvement obtained by using the transfer function model rather than fitting a univariate model to the sales data alone. If we adopt the latter course, we obtain the model

$$
X _ {t 2} - 0. 2 4 9 X _ {t - 1, 2} - 0. 1 9 9 X _ {t - 2, 2} = U _ {t},
$$

where $\{ U _ { t } \} \sim \ \mathrm { W N } ( 0 , 1 . 7 9 4 )$ and $X _ { t 2 } = Y _ { t 2 } - Y _ { t - 1 , 2 } - 0 . 4 2 0$ . The corresponding predictors of $Y _ { 1 5 1 , 2 }$ and $Y _ { 1 5 2 , 2 }$ are easily found from the program ITSM to be 263.14 and 263.58 with mean squared errors 1.794 and 4.593, respectively. These mean squared errors are much worse than those obtained using the transfer function model.

# 11.2 Intervention Analysis

During the period for which a time series is observed, it is sometimes the case that a change occurs that affects the level of the series. A change in the tax laws may, for example, have a continuing effect on the daily closing prices of shares on the stock market. In the same way construction of a dam on a river may have a dramatic effect on the time series of streamflows below the dam. In the following we shall assume that the time $T$ at which the change (or “intervention”) occurs is known.

To account for such changes, Box and Tiao (1975) introduced a model for intervention analysis that has the same form as the transfer function model

$$
Y _ {t} = \sum_ {j = 0} ^ {\infty} \tau_ {j} X _ {t - j} + N _ {t}, \tag {11.2.1}
$$

except that the input series $\{ X _ { t } \}$ is not a random series but a deterministic function of $t$ . {It is clear from (11.2.1) that $\scriptstyle \sum _ { j = 0 } ^ { \infty } \tau _ { j } X _ { t - j }$ is then the mean of $Y _ { t }$ . The function $\{ X _ { t } \}$ and observations of the coefficients $\{ \tau _ { j } \}$ $\{ Y _ { t } \}$ are therefore chosen in such a way that the changing level of the is well represented by the sequence $\scriptstyle \sum _ { j = 0 } ^ { \infty } \tau _ { j } X _ { t - j }$ . For a series $\{ Y _ { t } \}$ with $E Y _ { t } = 0$ for $t \leq T$ and $E Y _ { t }  0$ as $t \to \infty$ , a suitable input series is

$$
X _ {t} = I _ {t} (T) = \left\{ \begin{array}{l l} 1 & \text {i f} t = T, \\ 0 & \text {i f} t \neq T. \end{array} \right. \tag {11.2.2}
$$

For a series $\{ Y _ { t } \}$ with $E Y _ { t } = 0$ for $t \leq T$ and $E Y _ { t }  a \neq 0$ as $t \to \infty$ , a suitable input series is

$$
X _ {t} = H _ {t} (T) = \sum_ {k = T} ^ {\infty} I _ {t} (k) = \left\{ \begin{array}{l l} 1 & \text {i f} t \geq T, \\ 0 & \text {i f} t <   T. \end{array} \right. \tag {11.2.3}
$$

(Other deterministic input functions $\{ X _ { t } \}$ can also be used, for example when interventions occur at more than one time.) The function $\left\{ X _ { t } \right\}$ having been selected by inspection of the data, the determination of the coefficients $\{ \tau _ { j } \}$ in (11.2.1) then reduces to a regression problem in which the errors $\{ N _ { t } \}$ constitute an ARMA process. This problem can be solved using the program ITSM as described below.

The goal of intervention analysis is to estimate the effect of the intervention as indicated by the term $\scriptstyle \sum _ { j = 0 } ^ { \infty } \tau _ { j } X _ { t - j }$ and to use the resulting model (11.2.1) for forecasting. For example, Wichern and Jones (1978) used intervention analysis to investigate the effect of the American Dental Association’s endorsement of Crest toothpaste on Crest’s market share. Other applications of intervention analysis can be found in Box and Tiao (1975), Atkins (1979), and Bhattacharyya and Layton (1979). A more general approach can also be found in West and Harrison (1989), Harvey (1990), and Pole et al. (1994).

As in the case of transfer function modeling, once $\{ X _ { t } \}$ has been chosen (usually as either (11.2.2) or (11.2.3)), estimation of the linear filter $\{ \tau _ { j } \}$ in (11.2.1) is simplified by approximating the operator $\begin{array} { r } { T ( B ) \ = \ \sum _ { j = 0 } ^ { \infty } \tau _ { j } B ^ { j } } \end{array}$ with a rational operator of the form

$$
T (B) = \frac {B ^ {b} W (B)}{V (B)}, \tag {11.2.4}
$$

where $^ b$ is the delay parameter and $W ( B )$ and $V ( B )$ are polynomials of the form

$$
W (B) = w _ {0} + w _ {1} B + \dots + w _ {q} B ^ {q}
$$

and

$$
V (B) = 1 - v _ {1} B - \dots - v _ {p} B ^ {p}.
$$

By suitable choice of the parameters $b , \ q , \ p$ and the coefficients $w _ { i }$ and $\nu _ { j }$ , the intervention term $T ( B ) X _ { t }$ can made to take a great variety of functional forms.

For example, if $T ( B ) \ = \ w B ^ { 2 } / ( 1 \ - \ \nu B )$ and $X _ { t } ~ = ~ I _ { t } ( T )$ as in (11.2.2), the resulting intervention term is

$$
\frac {w B ^ {2}}{(1 - v B)} I _ {t} (T) = \sum_ {j = 0} ^ {\infty} v ^ {j} w I _ {t - j - 2} (T) = \sum_ {j = 0} ^ {\infty} v ^ {j} w I _ {t} (T + 2 + j),
$$

a series of pulses of sizes $\nu ^ { j } w$ at times $T + 2 + j , j = 0 , 1 , 2 , \ldots .$ If $| \nu | < 1$ , the effect of the intervention is to add a series of pulses with size $w$ at time $T + 2$ , decreasing to zero at a geometric rate depending on $\nu$ as $t  \infty$ . Similarly, with $X _ { t } = H _ { t } ( T )$ as in (11.2.3),

$$
\frac {w B ^ {2}}{(1 - v B)} H _ {t} (T) = \sum_ {j = 0} ^ {\infty} v ^ {j} w H _ {t - j - 2} (T) = \sum_ {j = 0} ^ {\infty} (1 + v + \dots + v ^ {j}) w I _ {t} (T + 2 + j),
$$

a series of pulses of sizes $( 1 + \nu + \cdot \cdot \cdot + \nu ^ { j } ) w$ at times $\displaystyle { T + 2 + j , j = 0 , 1 , 2 , . . . }$ If $| \nu | < 1$ , the effect of the intervention is to bring about a shift in level of the series $X _ { t }$ , the size of the shift converging to $w / ( 1 - \nu )$ as $t \to \infty$ .

An appropriate form for $X _ { t }$ and possible values of $^ b$ , q, and $p$ having been chosen by inspection of the data, the estimation of the parameters in (11.2.4) and the fitting of the model for $\{ N _ { t } \}$ can be carried out using steps (6)–(8) of the transfer function modeling procedure described in Section 11.1. Start with step (7) and assume that $\{ N _ { t } \}$ is white noise to get preliminary estimates of the coefficients $w _ { i }$ and $\nu _ { j }$ by least squares. The residuals are filed and used as estimates of $\{ N _ { t } \}$ . Then go to step (6) and continue exactly as for transfer function modeling with input series $\{ X _ { t } \}$ and output series $\{ Y _ { t } \}$ .

# Figure 11-3

The differenced series of Example 11.2.1 (showing also the fitted intervention term accounting for the seat-belt legislation of 1983)

![](images/ea515c66f98670f0f16b4822814645aac8288c99cd745618b970c40308c4dd7d.jpg)

# Example 11.2.1 Seat-Belt Legislation

In this example we reanalyze the seat-belt legislation data, SBL.TSM of Example 6.6.3 from the point of view of intervention analysis. For this purpose the bivariate series $\{ ( f _ { t } , Y _ { t } ) \}$ consisting of the series filed as SBLIN.TSM and SBL.TSM respectively has been saved in the file SBL2.TSM. The input series $\{ f _ { t } \}$ is the deterministic stepfunction defined in Example 6.6.3 and $Y _ { t }$ is the number of deaths and serious injuries on UK roads in month t $, t = 1 , \ldots , 1 2 0$ , corresponding to the 10 years beginning with January 1975.

To account for the seat-belt legislation, we use the same model (6.6.15) as in Example 6.6.3 and, because of the apparent non-stationarity of the residuals, we again difference both $\{ f _ { t } \}$ and $\{ Y _ { t } \}$ at lag 12 to obtain the model (6.6.15), i.e.,

$$
X _ {t} = b g _ {t} + N _ {t}, \tag {11.2.4}
$$

where $X _ { t } = \nabla _ { 1 2 } Y _ { t }$ , $g _ { t } = \nabla _ { 1 2 } f _ { t }$ , and $\{ N _ { t } \}$ is a zero-mean stationary time series. This is a particularly simple example of the general intervention model (11.2.1) for the series $\{ X _ { t } \}$ with intervention $\{ b g _ { t } \}$ . Our aim is to find a suitable model for $\{ N _ { t } \}$ and at the same time to estimate $^ b$ , taking into account the autocorrelation function of the model for $\{ N _ { t } \}$ . To apply intervention analysis to this problem using ITSM, we proceed as follows:

(1) Open the bivariate project SBL2.TSM and difference the series at lag 12.   
(2) Select Transfer $>$ Specify model and you will see that the default input and noise are white noise, while the default transfer model relating the input $g _ { t }$ to the output $X _ { t }$ is $X _ { t } ~ = ~ b g _ { t }$ with $b \ = \ 1$ . Click OK, leaving these settings as they are. The input model is irrelevant for intervention analysis and estimation of the transfer function with the default noise model will give us the ordinary least squares estimate of $^ b$ in the model (10.2.4), with the residuals providing estimates of $N _ { t }$ . Now select Transfer>Estimation and click OK. You will then see the estimated value 346.9 for $^ b$ . Finally, press the red Export button (top right in the ITSM window) to export the residuals (estimated values of $N _ { t }$ ) to a file and call it, say, NOISE.TSM.   
(3) Without closing the bivariate project, open the univariate project NOISE.TSM. The sample ACF and PACF of the series suggests either an MA(13) or AR(13) model. Fitting AR and MA models of order up to 13 (with no mean-correction) using the option Model>Estimation>Autofit gives an MA(12) model as the minimum AICC fit.   
(4) Return to the bivariate project by highlighting the window labeled SBL2.TSM and select Transfer $>$ Specify model. The transfer model will now show the estimated value 346.9 for b. Click on the Residual Model tab, enter 12 for the MA order and click OK. Select Transfer $>$ Estimationand again click OK. The parameters in both the noise and transfer models will then be estimated and printed on the screen. Repeating the minimization with decreasing step-sizes, 0.1, 0.01 and then 0.001, gives the model,

$$
X _ {t} = - 3 6 2. 5 g _ {t} + N _ {t},
$$

where $N _ { t } = W _ { t } + 0 . 2 0 7 W _ { t - 1 } + 0 . 3 1 1 W _ { t - 2 } + 0 . 1 0 5 W _ { t - 3 } + 0 . 0 4 0 W _ { t - 4 } + 0 . 1 9 4 W _ { t - 5 } +$ $) . 1 0 0 W _ { t - 6 } ~ + ~ 0 . 2 9 9 W _ { t - 7 } + 0 . 0 8 0 W _ { t - 8 } + 0 . 1 2 5 W _ { t - 9 } ~ + ~ 0 . 2 1 0 W _ { t - 1 0 } + 0 . 1 0 9 W _ { t - 1 1 } +$ $0 . 5 0 1 W _ { t - 1 2 }$ , and $\{ W _ { t } \} \sim \mathrm { W N } ( 0 , 1 7 2 8 9 )$ . File the residuals (which are now estimates of $\{ W _ { t } \} .$ ) as RES.TSM. The differenced series $\{ X _ { t } \}$ and the fitted intervention term, $- 3 6 2 . 5 g _ { t }$ , are shown in Figure 11-3.

![](images/58021d4346919f051bf38ad419ad82f43d7a26be174bf7a253d6fd89e3b59b5c.jpg)  
Figure 11-4 The sample ACF of the residuals from the model in Example 11.2.1

(5) Open the univariate project RES.TSM and apply the usual tests for randomness by selecting Statistics>Residual Analysis. The tests are all passed at level 0.05, leading us to conclude that the model found in step (4) is satisfactory. The sample ACF of the residuals is shown in Figure 11-4.

# 11.3 Nonlinear Models

A time series of the form

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j}, \left\{Z _ {t} \right\} \sim \operatorname {I I D} \left(0, \sigma^ {2}\right), \tag {11.3.1}
$$

where $Z _ { t }$ is expressible as a mean square limit of linear combinations of $\{ X _ { s }$ $X _ { s } , \infty < s \leq$ $t \}$ , has the property that the best mean square predictor $E ( X _ { t + h } | X _ { s } , - \infty < s \leq t )$ and the best linear predictor $\tilde { P } _ { t } X _ { t + h }$ in terms of $\{ X _ { s } , - \infty < s \leq t \}$ are identical. It can be shown that if iid is replaced by WN in (11.3.1), then the two predictors are identical if and only if $\{ Z _ { t } \}$ is a martingale difference sequence relative to $\{ X _ { t } \}$ , i.e., if and only if $E ( Z _ { t } | X _ { s } , - \infty < s < t ) = 0$ for all $t$ .

The Wold decomposition (Section 2.6) ensures that every purely nondeterministic stationary process can be expressed in the form (11.3.1) with $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \left( 0 , \sigma ^ { 2 } \right)$ . The process $\{ Z _ { t } \}$ in the Wold decomposition, however, is generally not an iid sequence, and the best mean square predictor of $X _ { t + h }$ may be quite different from the best linear predictor.

In the case where $\{ X _ { t } \}$ is a purely nondeterministic Gaussian stationary process, the sequence $\{ Z _ { t } \}$ in the Wold decomposition is Gaussian and therefore iid. Every stationary purely nondeterministic Gaussian process can therefore be generated by applying a causal linear filter to an iid Gaussian sequence. We shall therefore refer to such a process as a Gaussian linear process.

In this section we shall use the term linear process to mean a process $\{ X _ { t } \}$ of the form (11.3.1). This is a more restrictive use of the term than in Definition 2.2.1.

# 11.3.1 Deviations from Linearity

Many of the time series encountered in practice exhibit characteristics not shown by linear processes, and so to obtain good models and predictors it is necessary to look to models more general than those satisfying (11.3.1) with iid noise. As indicated above, this will mean that the minimum mean squared error predictors are not, in general, linear functions of the past observations.

Gaussian linear processes have a number of properties that are often found to be violated by observed time series. The former are reversible in the sense that $\left( X _ { t _ { 1 } } , \cdots , X _ { t _ { n } } \right) ^ { \prime }$ has the same distribution as $\left( X _ { t _ { n } } , \cdots , X _ { t _ { 1 } } \right) ^ { \prime }$ . (Except in a few special cases, ARMA processes are reversible if and only if they are Gaussian (Breidt and Davis 1992).) Deviations from this property by observed time series are suggested by sample paths that rise to their maxima and fall away at different rates (see, for example, the sunspot numbers filed as SUNSPOTS.TSM). Bursts of outlying values are frequently observed in practical time series and are seen also in the sample paths of nonlinear (and infinite-variance) models. They are rarely seen, however, in the sample paths of Gaussian linear processes. Other characteristics suggesting deviation from a Gaussian linear model are discussed by Tong (1990).

Many observed time series, particularly financial time series, exhibit periods during which they are “less predictable” (or “more volatile”), depending on the past history of the series. This dependence of the predictability (i.e., the size of the prediction mean squared error) on the past of the series cannot be modeled with a linear time series, since for a linear process the minimum $h$ -step mean squared error is independent of the past history. Linear models thus fail to take account of the possibility that certain past histories may permit more accurate forecasting than others, and cannot identify the circumstances under which more accurate forecasts can be expected. Nonlinear models, on the other hand, do allow for this. The ARCH and GARCH models considered in Section 7.2 are in fact constructed around the dependence of the conditional variance of the process on its past history.

# 11.3.2 Chaotic Deterministic Sequences

To distinguish between linear and nonlinear processes, we need to be able to decide in particular when a white noise sequence is also iid. Sequences generated by nonlinear deterministic difference equations can exhibit sample correlation functions that are very close to those of samples from a white noise sequence. However, the deterministic nature of the recursions implies the strongest possible dependence between successive observations. For example, the celebrated logistic equation (see May, 1976, and Tong 1990) defines a sequence $\{ x _ { n } \}$ , for any given $x _ { 0 }$ , via the equations

$$
x _ {n} = 4 x _ {n - 1} (1 - x _ {n - 1}), \quad 0 <   x _ {0} <   1.
$$

The values of $x _ { n }$ are, for even moderately large values of $n$ , extremely sensitive to small changes in $x _ { 0 }$ . This is clear from the fact that the sequence can be expressed explicitly as

$$
x _ {n} = \sin^ {2} \left(2 ^ {n} \arcsin \left(\sqrt {x _ {0}}\right)\right), \quad n = 0, 1, 2, \dots .
$$

A very small change $\delta$ in arcsin $\left( \sqrt { x _ { 0 } } \right)$ leads to a change $2 ^ { n } \delta$ in the argument of the sine function defining $x _ { n }$ . If we generate a sequence numerically, the generated sequence will, for most values of $x _ { 0 }$ in the interval (0,1), be random in appearance, with a

![](images/ce61847633a5657d37c83b22666139f20ea43989548764fa0cc19f9433b96dbf.jpg)  
Figure 11-5 A sequence generated by the recursions $x _ { n } = 4 x _ { n - 1 } ( 1 - x _ { n - 1 } )$

![](images/8998e6010319fd938bc5197be483a3d203ad7a77d3ab2867c6a552434f13fc13.jpg)

# Figure 11-6

The sample autocorrelation function of the sequence in Figure 11-5

sample autocorrelation function similar to that of a sample from white noise. The data file CHAOS.TSM contains the sequence $x _ { 1 } , \ldots , x _ { 2 0 0 }$ (correct to nine decimal places) generated by the logistic equation with $x _ { 0 } = \pi / 1 0$ . The calculation requires specification of $x _ { 0 }$ to at least 70 decimal places and the use of correspondingly high precision arithmetic. The series and its sample autocorrelation function are shown in Figures 11-5 and 11-6. The sample ACF and the AICC criterion both suggest white noise with mean 0.4954 as a model for the series. Under this model the best linear predictor of $X _ { 2 0 1 }$ would be 0.4954. However, the best predictor of $X _ { 2 0 1 }$ to nine decimal places is, in fact, $4 x _ { 2 0 0 } ( 1 - x _ { 2 0 0 } ) = 0 . 0 1 6 2 8 6 6 6 9$ , with zero mean squared error.

Distinguishing between iid and non-iid white noise is clearly not possible on the basis of second-order properties. For insight into the dependence structure we can examine sample moments of order higher than two. For example, the dependence in the

data in CHAOS.TSM is reflected by a significantly nonzero sample autocorrelation at lag 1 of the squared data. In the following paragraphs we consider several approaches to this problem.

# 11.3.3 Distinguishing Between White Noise and iid Sequences

If $\{ X _ { t } \} \sim \mathrm { W N } \left( 0 , \sigma ^ { 2 } \right)$ and $E | X _ { t } | ^ { 4 } \ < \ \infty$ , a useful tool for deciding whether or not $\{ X _ { t } \}$ is iid is the ACF $\rho _ { X ^ { 2 } } ( h )$ of the process $\left\{ X _ { t } ^ { 2 } \right\}$ . If $\{ X _ { t } \}$ is iid, then $\rho _ { X ^ { 2 } } ( h ) = 0$ for all $h \neq 0$ , whereas this is not necessarily the case otherwise. This is the basis for the test of McLeod and Li described in Section 1.6.

Now suppose that $\{ X _ { t } \}$ is a strictly stationary time series such that $E | X _ { t } | ^ { k } \leq K < \infty$ for some integer $k \ \geq \ 3$ . The kth-order cumulant $C _ { k } ( r _ { 1 } , \ldots , r _ { k - 1 } )$ of $\{ X _ { t } \}$ is then defined as the joint cumulant of the random variables, $X _ { t } , X _ { t + r _ { 1 } } , \ldots , X _ { t + r _ { k - 1 } }$ , i.e., as the coefficient of $i ^ { k } z _ { 1 } z _ { 2 } \cdots z _ { k }$ in the Taylor expansion about $( 0 , \ldots , 0 )$ of

$$
\chi \left(z _ {1}, \dots , z _ {k}\right) := \ln E \left[ \exp \left(i z _ {1} X _ {t} + i z _ {2} X _ {t + r _ {1}} + \dots + i z _ {k} X _ {t + r _ {k - 1}}\right) \right]. \tag {11.3.2}
$$

(Since $\{ X _ { t } \}$ is strictly stationary, this quantity does not depend on $t .$ .) In particular, the third-order cumulant function $C _ { 3 }$ of $\{ X _ { t } \}$ coincides with the third-order central moment function, i.e.,

$$
C _ {3} (r, s) = E \left[ \left(X _ {t} - \mu\right) \left(X _ {t + r} - \mu\right) \left(X _ {t + s} - \mu\right) \right], \quad r, s \in \{0, \pm 1, \dots \},
$$

where $\mu \ : = \ : E X _ { t }$ . If $\begin{array} { r } { \sum _ { r } \sum _ { s } | C _ { 3 } ( r , s ) | < \infty } \end{array}$ , we define the third-order polyspectral density (or bispectral density) of $\{ X _ { t } \}$ to be the Fourier transform

$$
f _ {3} \left(\omega_ {1}, \omega_ {2}\right) = \frac {1}{(2 \pi) ^ {2}} \sum_ {r = - \infty} ^ {\infty} \sum_ {s = - \infty} ^ {\infty} C _ {3} (r, s) e ^ {- i r \omega_ {1} - i s \omega_ {2}}, \quad - \pi \leq \omega_ {1}, \omega_ {2} \leq \pi ,
$$

in which case

$$
C _ {3} (r, s) = \int_ {- \pi} ^ {\pi} \int_ {- \pi} ^ {\pi} e ^ {i r \omega_ {1} + i s \omega_ {2}} f _ {3} (\omega_ {1}, \omega_ {2}) d \omega_ {1} d \omega_ {2}.
$$

[More generally, if the kth order cumulants $C _ { k } ( r _ { 1 } , \cdot \cdot \cdot , r _ { k - 1 } )$ , of $\{ X _ { t } \}$ are absolutely summable, we define the kth order polyspectral density as the Fourier transform of $C _ { k }$ . For details see Rosenblatt (1985) and Priestley (1988).]

If $\{ X _ { t } \}$ is a Gaussian linear process, it follows from Problem 10.3 that the cumulant function $C _ { 3 }$ of $\{ X _ { t } \}$ is identically zero. (The same is also true of all the cumulant functions $C _ { k }$ with $k > 3 .$ .) Consequently, $f _ { 3 } ( \omega _ { 1 } , \omega _ { 2 } ) = 0$ for all $\omega _ { 1 }$ , $\omega _ { 2 } \in [ - \pi , \pi ]$ . Appropriateness of a Gaussian linear model for a given data set can therefore be checked by using the data to test the null hypothesis $f _ { 3 } = 0$ . For details of such a test, see Subba-Rao and Gabr (1984).

If $\{ X _ { t } \}$ is a linear process of the form (11.3.1) with $E | Z _ { t } | ^ { 3 } < \infty$ , $E Z _ { t } ^ { 3 } = \eta$ , and $\textstyle \sum _ { j = 0 } ^ { \infty } | \psi _ { j } | < \infty$ , it can be shown from (11.3.2) (see Problem 11.3) that the third-order cumulant function of $\{ X _ { t } \}$ is given by

$$
C _ {3} (r, s) = \eta \sum_ {i = - \infty} ^ {\infty} \psi_ {i} \psi_ {i + r} \psi_ {i + s} \tag {11.3.3}
$$

(with $\psi _ { j } = 0$ for $j < 0$ ), and hence that $\{ X _ { t } \}$ has bispectral density

$$
f _ {3} \left(\omega_ {1}, \omega_ {2}\right) = \frac {\eta}{4 \pi^ {2}} \psi \left(e ^ {i \left(\omega_ {1} + \omega_ {2}\right)}\right) \psi \left(e ^ {- i \omega_ {1}}\right) \psi \left(e ^ {- i \omega_ {2}}\right), \tag {11.3.4}
$$

where $\textstyle \psi ( z ) : = \sum _ { j = 0 } ^ { \infty } \psi _ { j } z ^ { j }$ . By Proposition 4.3.1, the spectral density of $\{ X _ { t } \}$ is

$$
f (\omega) = \frac {\sigma^ {2}}{2 \pi} \left| \psi \left(e ^ {- i \omega}\right) \right| ^ {2}.
$$

Hence,

$$
\phi (\omega_ {1}, \omega_ {2}) := \frac {| f _ {3} (\omega_ {1} , \omega_ {2}) | ^ {2}}{f (\omega_ {1}) f (\omega_ {2}) f (\omega_ {1} + \omega_ {2})} = \frac {\eta^ {2}}{2 \pi \sigma^ {6}}.
$$

Appropriateness of the linear process (11.3.1) for modeling a given data set can therefore be checked by using the data to test for constancy of $\phi ( \omega _ { 1 } , \omega _ { 2 } )$ (Subba-Rao and Gabr 1984).

# 11.3.4 Three Useful Classes of Nonlinear Models

If it is decided that a linear Gaussian model is not appropriate, there is a choice of several families of nonlinear processes that have been found useful for modeling purposes. These include bilinear models, autoregressive models with random coefficients, and threshold models. Excellent accounts of these are available in Subba-Rao and Gabr (1984), Nicholls and Quinn (1982), and Tong (1990), respectively.

The bilinear model of order $( p , q , r , s )$ is defined by the equations

$$
X _ {t} = Z _ {t} + \sum_ {i = 1} ^ {p} a _ {i} X _ {t - i} + \sum_ {j = 1} ^ {q} b _ {j} Z _ {t - j} + \sum_ {i = 1} ^ {r} \sum_ {j = 1} ^ {s} c _ {i j} X _ {t - i} Z _ {t - j}, \tag {11.3.5}
$$

where $\{ Z _ { t } \} \sim \operatorname { i i d } \left( 0 , \sigma ^ { 2 } \right)$ . A sufficient condition for the existence of a strictly stationary solution of these equations is given by Liu and Brockwell (1988).

A random coefficient autoregressive process $\{ X _ { t } \}$ of order $p$ satisfies an equation of the form

$$
X _ {t} = \sum_ {i = 1} ^ {p} \left(\phi_ {i} + U _ {t} ^ {(i)}\right) X _ {t - i} + Z _ {t},
$$

where $\{ Z _ { t } \} \sim \ \mathrm { I I D } \left( 0 , \sigma ^ { 2 } \right)$ , $\big \{ U _ { t } ^ { ( i ) } \big \} \sim \mathrm { I I D } \left( 0 , \nu ^ { 2 } \right)$ , $\{ Z _ { t } \}$ is independent of $\left\{ U _ { t } \right\}$ , and $\phi _ { 1 } , \ldots , \phi _ { p } \in \mathbb { R }$ .

Threshold models can be regarded as piecewise linear models in which the linear relationship varies with the values of the process. For example, if $R ^ { ( i ) }$ , $i = 1 , \ldots , k$ , is a partition of $\mathbb { R } ^ { p }$ , and $\{ Z _ { t } \} \sim \mathrm { { I I D } } ( 0 , 1 )$ , then the $k$ difference equations

$$
X _ {t} = \sigma^ {(i)} Z _ {t} + \sum_ {j = 1} ^ {p} \phi_ {j} ^ {(i)} X _ {t - j}, \quad \left(X _ {t - 1}, \dots , X _ {t - p}\right) \in R ^ {(i)}, \quad i = 1, \dots , k, \tag {11.3.6}
$$

define a threshold $\operatorname { A R } ( p )$ model. Model identification and parameter estimation for threshold models can be carried out in a manner similar to that for linear models using maximum likelihood and the AIC criterion.

# 11.4 Long-Memory Models

The autocorrelation function $\rho ( \cdot )$ of an ARMA process at lag $h$ converges rapidly to zero as $h  \infty$ in the sense that there exists $r > 1$ such that

$$
r ^ {h} \rho (h) \rightarrow 0 \quad \text {a s} \quad h \rightarrow \infty . \tag {11.4.1}
$$

Stationary processes with much more slowly decreasing autocorrelation function, known as fractionally integrated ARMA processes, or more precisely as ARIMA $( p , d , q )$ processes with $0 < | d | < 0 . 5$ , satisfy difference equations of the form

$$
(1 - B) ^ {d} \phi (B) X _ {t} = \theta (B) Z _ {t}, \tag {11.4.2}
$$

where $\phi ( z )$ and $\theta ( z )$ are polynomials of degrees $p$ and $q$ , respectively, satisfying

$$
\phi (z) \neq 0 \quad \text {a n d} \quad \theta (z) \neq 0 \quad \text {f o r a l l} z \text {s u c h t h a t} | z | \leq 1,
$$

$B$ is the backward shift operator, and $\{ Z _ { t } \}$ is a white noise sequence with mean 0 and variance $\sigma ^ { 2 }$ . The operator $( 1 - B ) ^ { d }$ is defined by the binomial expansion

$$
(1 - B) ^ {d} = \sum_ {j = 0} ^ {\infty} \pi_ {j} B ^ {j},
$$

where $n _ { 0 } = 1$ and

$$
\pi_ {j} = \prod_ {0 <   k \leq j} \frac {k - 1 - d}{k}, \qquad j = 1, 2, \dots .
$$

The autocorrelation $\rho ( h )$ at lag $h$ of an ARIMA $( p , d , q )$ process with $0 < | d | < 0 . 5$ has the property

$$
\rho (h) h ^ {1 - 2 d} \rightarrow c \neq 0 \quad \text {a s} \quad h \rightarrow \infty . \tag {11.4.3}
$$

This implies (see (11.4.1)) that $\rho ( h )$ converges to zero as $h  \infty$ at a much slower rate than $\rho ( h )$ for an ARMA process. Consequently, fractionally integrated ARMA processes are said to have “long memory.” In contrast, stationary processes whose ACF converges to 0 rapidly, such as ARMA processes, are said to have “short memory.”

A fractionally integrated $\mathrm { A R I M A } ( p , d , q )$ process can be regarded as an ARMA $( p , q )$ process driven by fractionally integrated noise; i.e., we can replace equation (11.4.2) by the two equations

$$
\phi (B) X _ {t} = \theta (B) W _ {t} \tag {11.4.4}
$$

and

$$
(1 - B) ^ {d} W _ {t} = Z _ {t}. \tag {11.4.5}
$$

The process $\{ W _ { t } \}$ is called fractionally integrated white noise and can be shown (see, e.g., Brockwell and Davis (1991), Section 13.2) to have variance and autocorrelations given by

$$
\gamma_ {W} (0) = \sigma^ {2} \frac {\Gamma (1 - 2 d)}{\Gamma^ {2} (1 - d)} \tag {11.4.6}
$$

and

$$
\rho_ {W} (h) = \frac {\Gamma (h + d) \Gamma (1 - d)}{\Gamma (h - d + 1) \Gamma (d)} = \prod_ {0 <   k \leq h} \frac {k - 1 + d}{k - d}, \quad h = 1, 2, \dots , \tag {11.4.7}
$$

where $\Gamma ( \cdot )$ is the gamma function (see Example $( d )$ of Section A.1). The exact autocovariance function of the ARIMA $( p , d , q )$ process $\{ X _ { t } \}$ defined by (11.4.2) can therefore be expressed, by Proposition 2.2.1, as

$$
\gamma_ {X} (h) = \sum_ {j = 0} ^ {\infty} \sum_ {k = 0} ^ {\infty} \psi_ {j} \psi_ {k} \gamma_ {W} (h + j - k), \tag {11.4.8}
$$

where $\begin{array} { r } { \sum _ { i = 0 } ^ { \infty } \psi _ { i } z ^ { i } = \theta ( z ) / \phi ( z ) , | z | \le } \end{array}$ 1, and $\gamma _ { W } ( \cdot )$ is the autocovariance function of fractionally integrated white noise with parameters $d$ and $\sigma ^ { 2 }$ , i.e.,

$$
\gamma_ {W} (h) = \gamma_ {W} (0) \rho_ {W} (h),
$$

with $\gamma _ { W } ( 0 )$ and $\rho _ { W } ( h )$ as in (11.4.6) and (11.4.7). The series (11.4.8) converges rapidly as long as $\phi ( z )$ does not have zeros with absolute value close to 1.

The spectral density of $\{ X _ { t } \}$ is given by

$$
f (\lambda) = \frac {\sigma^ {2}}{2 \pi} \frac {\left| \theta \left(e ^ {- i \lambda}\right) \right| ^ {2}}{\left| \phi \left(e ^ {- i \lambda}\right) \right| ^ {2}} \left| 1 - e ^ {- i \lambda} \right| ^ {- 2 d}. \tag {11.4.9}
$$

Calculation of the exact Gaussian likelihood of observations $\{ x _ { 1 } , \ldots , x _ { n } \}$ of a fractionally integrated ARMA process is very slow and demanding in terms of computer memory. Instead of estimating the parameters $d , \phi _ { 1 } , \ldots , \phi _ { p } , \theta _ { 1 } , \ldots , \theta _ { q }$ , and $\sigma ^ { 2 }$ by maximizing the exact Gaussian likelihood, it is much simpler to maximize the Whittle approximation $L _ { W }$ , defined by

$$
- 2 \ln (L _ {W}) = n \ln (2 \pi) + 2 n \ln \sigma + \sigma^ {- 2} \sum_ {j} \frac {I _ {n} (\omega_ {j})}{g (\omega_ {j})} + \sum_ {j} \ln g (\omega_ {j}), \tag {11.4.10}
$$

where $I _ { n }$ is the periodogram, $\sigma ^ { 2 } g / ( 2 \pi ) ( = f )$ is the model spectral density, and $\textstyle \sum _ { j }$ denotes the sum over all nonzero Fourier frequencies $\omega _ { j } = 2 \pi j / n \in ( - \pi , \pi ]$ . The program ITSM estimates parameters for $\mathbf { A R I M A } ( p , d , q )$ models in this way. It can also be used to predict and simulate fractionally integrated ARMA series and to compute the autocovariance function of any specified fractionally integrated ARMA model.

# Example 11.4.1 Annual Minimum Water Levels; NILE.TSM

The data file NILE.TSM consists of the annual minimum water levels of the Nile river as measured at the Roda gauge near Cairo for the years 622–871. These values are plotted in Figure 11-7 with the corresponding sample autocorrelations shown in Figure 11-8. The rather slow decay of the sample autocorrelation function suggests the possibility of a fractionally intergrated model for the mean-corrected series $Y _ { t } =$ $X _ { t } - 1 1 1 9$ .

![](images/9c3be8e6f5c1ffcdcaa383648dc198f474e56741d2138cc69107c6a3e5ef9187.jpg)  
Figure 11-7 Annual minimum water levels of the Nile river for the years 622–871

![](images/b9b2f8ccf2cd618e2b551ab1c0641ffb52c75916389737f4a48d35f06afd992d.jpg)  
Figure 11-8 The sample correlation function of the data in Figure 11-7

The ARMA model with minimum (exact) AICC value for the mean-corrected series $\{ Y _ { t } \}$ is found, using Model $>$ Estimation>Autofit, to be

$$
\begin{array}{l} Y _ {t} = - 0. 3 2 3 Y _ {t - 1} - 0. 0 6 0 Y _ {t - 2} + 0. 6 3 3 Y _ {t - 3} + 0. 0 6 9 Y _ {t - 4} + 0. 2 4 8 Y _ {t - 5} \\ + Z _ {t} + 0. 7 0 2 Z _ {t - 1} + 0. 3 5 0 Z _ {t - 2} - 0. 4 1 9 Z _ {t - 3}, \tag {11.4.11} \\ \end{array}
$$

with $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , 5 6 6 3 . 6 )$ and $\mathsf { A I C C } = 2 8 8 9 . 9$ .

To fit a fractionally integrated ARMA model to this series, select the option Model $>$ Specify, check the box marked Fractionally integrated model, and click on OK. Then select Model>Estimation>Autofit, and click on Start. This estimation procedure is relatively slow so the specified ranges for $p$ and $q$ should be small (the default is from 0 to 2). When models have been fitted for each value of $( p , q )$ , the fractionally integrated model with the smallest modified AIC value is found to be

$$
(1 - B) ^ {0. 3 8 3 0} (1 - 0. 1 6 9 4 B + 0. 9 7 0 4 B ^ {2}) Y _ {t} = (1 - 0. 1 8 0 0 B + 0. 9 2 7 8 B ^ {2}) Z _ {t}, \tag {11.4.12}
$$

with $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , 5 8 2 7 . 4 )$ and modified $\mathrm { A I C } = 2 8 8 4 . 9 4$ . (The modified AIC statistic for estimating the parameters of a fractionally integrated $\mathbf { A R M A } ( p , q )$ process is defined in terms of the Whittle likelihood $L _ { W }$ as $- 2 \mathrm { l n } \ L _ { W } + 2 ( p + q + 2 )$ if $d$ is estimated, and $- 2 \mathrm { l n } L _ { W } + 2 ( p + q + 1 )$ otherwise. The Whittle likelihood was defined in (11.4.10).)

In order to compare the models (11.4.11) and (11.4.12), the modified AIC value for (11.4.11) is found as follows. After fitting the model as described above, select Model>Specify, check the box marked Fractionally integrated model, set $d \ = \ 0$ and click on OK. Next choose Model>Estimation>Max likelihood, check No optimization and click on OK. You will then see the modified AIC value, 2884.58, displayed in the ML estimates window together with the value 2866.58 of $- 2 \mathrm { l n } L _ { W }$ .

The ARMA(5,3) model is slightly better in terms of modified AIC than the fractionally integrated model and its ACF is closer to the sample ACF of the data than is the ACF of the fractionally integrated model. (The sample and model autocorrelation

functions can be compared by clicking on the third yellow button at the top of the ITSM window.) The residuals from both models pass all of the ITSM tests for randomness.

Figure 11-9 shows the graph of $\{ x _ { 2 0 0 } , \ldots , x _ { 2 5 0 } \}$ with predictors of the next 20 values obtained from the model (11.4.12) for the mean-corrected series.

![](images/191a47142285d4943c63d64478f710f7323cf108cc917f835a59fd39ab757f1c.jpg)

# 11.5 Continuous-Time ARMA Processes

Time series frequently consist of observations of a continuous-time process $\{ Y ( t ) , t \geq$ 0 or $\{ Y ( t ) , t \in \mathbb { R } \}$ at a discrete sequence of observation times. It is then natural, even though the observations are made at discrete times, to model the data by fitting the underlying continuous-time process.

Even if there is no underlying continuous-time process, it may still be advantageous to model the data as observations of a continuous-time process sampled at discrete times. For example, the analysis of time series data observed at irregularly spaced times can be handled very conveniently by regarding the data as sampled values of a continuous-time process (see Jones 1980 and equation (11.5.6) below).

Continuous-time models also provide a unifying framework for data collected when a time series is observed at different frequencies, i.e., with different spacings between the observation times. Instead of requiring different discrete-time models to represent observations collected at different frequencies, continuous-time modelling provides a single model which can be sampled at any frequency whatsoever.

When very high-frequency observations are available (as in many financial and turbulence studies), the relation between the high-frequency sequence and the underlying continuous-time process is also of interest since the high-frequency observations provide a natural source of information regarding the continuous-time process of which the discrete observations are a sample.

Stationarity of a continuous-time process $\{ Y ( t ) \}$ (cf. Definition 1.4.2) means that $E Y ( t )$ and $\operatorname { C o v } ( Y ( t + h ) , Y ( t ) )$ are defined and independent of $t$ for all $h \geq 0$ . Strict stationarity means that $( Y ( t _ { 1 } ) , \ldots , Y ( t _ { n } ) )$ and $( Y ( t _ { 1 } + h ) , \ldots , Y ( t _ { n } + h ) )$ $Y ( t _ { n } + h ) ,$ have the same joint distributions for all $t _ { 1 } , \ldots , t _ { n }$ , all $h \geq 0$ and all positive integers $n$ .

![](images/b347c78b8a8582dfbfa29b511a73988045e5471bfd18a3cb5b6302fc078faf1e.jpg)  
Figure 11-9 The minimum annual Nile river levels for the years 821–871, with 20 forecasts based on the model (11.4.12)

Continuous-time ARMA (or CARMA) processes are defined as stationary solutions of stochastic differential equations analogous to the difference equations that are used to define discrete-time ARMA processes. They play a role in continuous-time modelling analogous to that of ARMA processes in discrete time.

We shall begin with the Gaussian continuous-time AR(1) process, also known as the stationary Gaussian Ornstein-Uhlenbeck process.

# 11.5.1 The Gaussian CAR(1) Process, $\{ Y ( t ) , t \ge 0 \}$

The Gaussian CAR(1) process $\{ Y ( t ) , t \geq 0 \}$ is defined as a strictly stationary solution of the first-order stochastic differential equation,

$$
D Y (t) + a Y (t) = \sigma D B (t) + c, t > 0, \tag {11.5.1}
$$

where the operator $D$ denotes differentiation with respect to t, $\{ B ( t ) , \ t \ \in \ \mathbb { R } \}$ is standard Brownian motion (see Example 7.5.1), $a , c$ , and $\sigma$ are parameters and $Y ( 0 )$ is a normally distributed random variable independent of $\{ B ( t ) - B ( s ) , 0 \le s \le t <$ $\infty \}$ . The derivative $D B ( t )$ does not exist in the usual sense, so equation (11.5.1) is interpreted as the Itô differential equation (see Appendix D.4),

$$
d Y (t) + a Y (t) d t = \sigma d B (t) + c d t, \quad t > 0, \tag {11.5.2}
$$

with $d Y ( t )$ and $d B ( t )$ denoting the increments of Y and $B$ in the time interval $( t , t + d t )$ .

Standard theory of deterministic linear differential equations suggests multiplying this equation by $e ^ { a t }$ in which case the left-hand side would become $d ( e ^ { a t } Y ( t ) )$ . We therefore apply Itô’s formula (Appendix, equation (D.3.7)) to $d ( e ^ { a t } Y ( t ) )$ with $g ( t , x ) : =$ $e ^ { a t } x$ and we obtain exactly the same result since the second partial derivative ${ g } _ { x x }$ is zero. Hence we can rewrite (11.5.2) as

$$
d \left(e ^ {a t} Y (t)\right) = \sigma e ^ {a t} d B (t) + c e ^ {a t} d t,
$$

or equivalently,

$$
e ^ {a t} Y (t) - Y (0) = \sigma \int_ {0} ^ {t} e ^ {a u} d B (u) + c \int_ {0} ^ {t} e ^ {a u} d u.
$$

Thus

$$
Y (t) = e ^ {- a t} Y (0) + \sigma \int_ {0} ^ {t} e ^ {- a (t - u)} d B (u) + c \int_ {0} ^ {t} e ^ {- a (t - u)} d u. \tag {11.5.3}
$$

Remark 1. The Itô integral $\textstyle \int _ { 0 } ^ { t } e ^ { - a ( t - u ) } d B ( u )$ in (11.5.3) is of a special type in which the integrand is deterministic. This permits the application of integration by parts to obtain a pathwise representation of $Y$ as

$$
Y (t) = e ^ {- a t} Y (0) + \sigma B (t) - \sigma \int_ {0} ^ {t} a e ^ {- a (t - u)} B (u) d u + c \int_ {0} ^ {t} e ^ {- a (t - u)} d u.
$$

If $a \ > \ 0$ and $Y ( 0 )$ has mean $c / a$ and variance $\sigma ^ { 2 } / ( 2 a )$ , it is easy to check, using the properties of $\begin{array} { r } { I _ { 0 , t } ( f ) = \int _ { 0 } ^ { t } f ( u ) d B ( u ) } \end{array}$ in Remark 3 of Appendix D.3 and the independence of $Y ( 0 )$ and $\{ B ( t ) - B ( s ) , 0 \leq s \leq t < \infty \}$ (Problem 11.4), that $\{ Y ( t ) \}$ as defined by (11.5.3) is stationary with

$$
E (Y (t)) = \frac {c}{a} \quad \text {a n d} \quad \operatorname {C o v} (Y (t + h), Y (t)) = \frac {\sigma^ {2}}{2 a} e ^ {- a h}, \quad t, h \geq 0. \tag {11.5.4}
$$

Since $\{ Y ( t ) \}$ is Gaussian it is also strictly stationary. Conversely, if $\{ Y ( t ) \}$ is strictly stationary, then by equating the variances of both sides of (11.5.3), we find that $\begin{array} { r } { \left( 1 - e ^ { - 2 { \bar { a } } t } \right) { \mathrm { V a r } } ( Y ( 0 ) ) { \mathrm { \Sigma } } = \sigma ^ { \bar { 2 } } \int _ { 0 } ^ { t } e ^ { - 2 a u } d u } \end{array}$ for all $t ~ \geq ~ 0$ , and hence that $a \ > \ 0$ and $\operatorname { V a r } ( Y ( 0 ) ) ~ = ~ \sigma ^ { 2 } / ( 2 a )$ . Equating the means of both sides of (11.5.3) then gives $E ( Y ( 0 ) ) = c / a$ . Necessary and sufficient conditions for $\{ Y ( t ) \}$ to be strictly stationary are therefore $a > 0$ , $E ( Y ( 0 ) ) = c / a$ , and $\operatorname { V a r } ( Y ( 0 ) ) = \sigma ^ { 2 } / ( 2 a )$ .

If $a > 0$ and $0 \leq s \leq t$ , it follows from (11.5.3) that $Y ( t )$ satisfies the relation

$$
Y (t) = e ^ {- a (t - s)} Y (s) + \frac {c}{a} \left(1 - e ^ {- a (t - s)}\right) + \sigma \int_ {s} ^ {t} e ^ {- a (t - u)} d B (u), t \geq s \geq 0. \tag {11.5.5}
$$

This shows that the process is Markovian, i.e., that the distribution of $Y ( t )$ given $Y ( u ) , u \ \leq \ s$ , is the same as the distribution of $Y ( t )$ given $Y ( s )$ . It also shows that the conditional mean and variance of $Y ( t )$ given $Y ( s )$ are

$$
E (Y (t) | Y (s)) = e ^ {- a (t - s)} Y (s) + c / a \left(1 - e ^ {- a (t - s)}\right)
$$

and

$$
\operatorname {V a r} (Y (t) | Y (s)) = \frac {\sigma^ {2}}{2 a} \left[ 1 - e ^ {- 2 a (t - s)} \right].
$$

We can now use the Markov property and the moments of the stationary distribution to write down the likelihood of observations $y ( t _ { 1 } ) , \ldots , y ( t _ { n } )$ at times $t _ { 1 } , \ldots , t _ { n }$ of the Gaussian CAR(1) process. This is just the joint density of $( Y ( t _ { 1 } ) , \ldots , Y ( t _ { n } ) ) ^ { \prime }$ at $( y ( t _ { 1 } ) , \ldots , y ( t _ { n } ) ) ^ { \prime }$ , which can be expressed as the product of the stationary density at $y ( t _ { 1 } )$ and the transition densities of $Y ( t _ { i } )$ given $Y ( t _ { i - 1 } ) = y ( t _ { i - 1 } )$ , $i = 2 , \ldots , n$ . The joint density $g$ is therefore given by

$$
g \left(y \left(t _ {1}\right), \dots , y \left(t _ {n}\right); a, c, \sigma^ {2}\right) = \prod_ {i = 1} ^ {n} \frac {1}{\sqrt {v _ {i}}} f \left(\frac {y \left(t _ {i}\right) - m _ {i}}{\sqrt {v _ {i}}}\right), \tag {11.5.6}
$$

where $f ( y ) = n ( y ; 0 , 1 )$ is the standard normal density, $m _ { 1 } = c / a$ , $\nu _ { 1 } = \sigma ^ { 2 } / ( 2 a )$ , and for $i > 1$ ,

$$
m _ {i} = e ^ {- a \left(t _ {i} - t _ {i - 1}\right)} y \left(t _ {i - 1}\right) + \frac {c}{a} \left(1 - e ^ {- a \left(t _ {i} - t _ {i - 1}\right)}\right)
$$

and

$$
v _ {i} = \frac {\sigma^ {2}}{2 a} \left[ 1 - e ^ {- 2 a (t _ {i} - t _ {i - 1})} \right].
$$

The maximum likelihood estimators of $a , c$ , and $\sigma ^ { 2 }$ are the values that maximize $g \left( y ( t _ { 1 } ) , \dots , y ( t _ { n } ) ; a , c , \sigma ^ { 2 } \right)$ . These can be found with the aid of a nonlinear maximization algorithm. Notice that the times $t _ { i }$ appearing in (11.5.6) are quite arbitrarily spaced. It is this feature that makes the CAR(1) process so useful for modeling irregularly spaced data.

If the observations are regularly spaced, say $t _ { i } = i , i = 1 , \ldots , n$ $t _ { i } = i$ , then the joint density $g$ is exactly the same as the joint density of observations of the discrete-time Gaussian AR(1) process

$$
Y _ {n} - \frac {c}{a} = e ^ {- a} \left(Y _ {n - 1} - \frac {c}{a}\right) + Z _ {n}, \quad \{Z _ {t} \} \sim \mathrm {I I D N} \left(0, \frac {\sigma^ {2} (1 - e ^ {- 2 a})}{2 a}\right).
$$

This shows that the “embedded” (or sampled) discrete-time process $\{ Y ( i ) , i \ =$ $1 , 2 , \ldots \}$ of the CAR(1) process is a discrete-time AR(1) process with coefficient $e ^ { - a }$ .

This coefficient is clearly positive, immediately raising the question of whether there is a continuous-time ARMA process for which the embedded process is a discrete-time AR(1) process with negative coefficient. It can be shown (Chan and Tong 1987) that the answer is yes and that given a discrete-time AR(1) process with negative coefficient, it can always be embedded in a suitably chosen continuous-time ARMA(2,1) process.

# 11.5.2 The Gaussian CARMA $( p , q )$ Process, $\{ Y ( t ) , t \in \mathbb { R } \}$

We define a zero-mean Gaussian CARMA $( p , q )$ process $\{ Y ( t ) , t \in \mathbb { R } \}$ (with $0 \leq q <$ $p$ ) to be a strictly stationary Gaussian process satisfying the $p$ th-order linear differential equation,

$$
\begin{array}{l} D ^ {p} Y (t) + a _ {1} D ^ {p - 1} Y (t) + \dots + a _ {p} Y (t) \\ = b _ {0} D B (t) + b _ {1} D ^ {2} B (t) + \dots + b _ {q} D ^ {q + 1} B (t), \tag {11.5.7} \\ \end{array}
$$

where $D ^ { j }$ denotes $j$ -fold differentiation with respect to t, $\{ B ( t ) , \ t \in \mathbb { R } \}$ is standard Brownian motion, and $a _ { 1 } , \dotsc , a _ { p }$ $a _ { p }$ and $b _ { 0 } , \ldots , b _ { q }$ are constants. We assume that $b _ { q } \neq 0$ and define $b _ { j } : = 0$ for $j > q$ . We shall also assume that the polynomials, $a ( z ) : =$ $z ^ { p } + a _ { 1 } z ^ { p - 1 } + \cdot \cdot \cdot + a _ { p }$ and $b ( z ) : = b _ { 0 } + b _ { 1 } z + \cdot \cdot \cdot + b _ { q } z ^ { q }$ , have no common zeroes. Since the derivatives $\bar { D ^ { j } } B ( t ) , j > 0$ , do not exist in the usual sense, we interpret (11.5.7) as being equivalent (see Remark 2 below) to the observation and state equations

$$
Y (t) = \mathbf {b} ^ {\prime} \mathbf {X} (t), \tag {11.5.8}
$$

and

$$
d \mathbf {X} (t) = A \mathbf {X} (t) d t + \mathbf {e} d B (t), \tag {11.5.9}
$$

where

$$
A = \left[ \begin{array}{c c c c c} 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 1 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & 1 \\ - a _ {p} & - a _ {p - 1} & - a _ {p - 2} & \dots & - a _ {1}, \end{array} \right],
$$

$\mathbf { e } = { \left[ 0 \quad 0 \quad \cdots \quad 0 \quad 1 \right] } ^ { \prime } , \mathbf { b } = { \left[ b _ { 0 } \quad b _ { 1 } \quad \cdots \quad b _ { p - 2 } \quad b _ { p - 1 } \right] } ^ { \prime }$ , and (11.5.9) is an Itô differential equation for the state vector $\mathbf { X } ( t )$ (see Appendix D.4).

Remark 2. Denoting the components of $\mathbf { X } ( t )$ by $X _ { j } ( t ) , j = 0 , \ldots , p - 1$ , the first $p - 1$ component equations of (11.5.9) are

$$
d X _ {j} (t) = X _ {j + 1} (t) d t, j = 0, \dots , p - 2,
$$

showing that $X _ { j } ( t )$ is just the $j ^ { \mathrm { t h } }$ derivative of $X _ { 0 } ( t ) , ~ j ~ = ~ 1 , \dots , p ~ - ~ 1$ . The last component equation of (11.5.9) is

$$
d X _ {p - 1} (t) = - \left(a _ {1} X _ {p - 1} + a _ {2} X _ {p - 2} + \dots + a _ {p} X _ {0} (t)\right) d t + d B (t),
$$

which is the Itô form of the stochastic differential equation,

$$
D ^ {p} X _ {0} (t) + a _ {1} D ^ {p - 1} X _ {0} (t) + \dots + a _ {p} X _ {0} (t) = D B (t). \tag {11.5.10}
$$

The state equation (11.5.9) is thus the Itô equation for the vector whose first component $X _ { 0 } ( t )$ satisfies the CARMA $( p , 0 )$ (or CAR(p)) equation (11.5.10) and whose remaining components are successively higher derivatives, up to order $p - 1$ , of $X _ { 0 } ( t )$ . It is clear

from the linearity of equation (11.5.7) that if $\mathbf { X } ( t )$ satisfies (11.5.9) then $X _ { 0 } ( t )$ satisfies (11.5.10) and the linear combination,

$$
Y (t) = b _ {0} X _ {0} (t) + b _ {1} X _ {1} (t) + \dots + b _ {p - 1} X _ {p - 1} (t) = \mathbf {b} ^ {\prime} \mathbf {X} (t),
$$

of $X _ { 0 } ( t )$ and its derivatives satisfies (11.5.7). This explains the replacement of (11.5.7) by the observation and state equations (11.5.8) and (11.5.9). -

If $\mathbf { X } ( 0 )$ is a normally distributed random vector independent of $\{ B ( t ) - B ( s ) , 0 \leq$ $s \leq t < \infty \}$ , then equation (11.5.9) is simply a vector form of equation (11.5.2) and its unique solution for $t \geq 0$ , as specified in Appendix D.4, Theorem D.4.1, satisfies

$$
\mathbf {X} (t) = e ^ {A t} \mathbf {X} (0) + \int_ {0} ^ {t} e ^ {A (t - u)} \mathbf {e} d B (u), 0 \leq t <   \infty ,
$$

$e ^ { A t }$ $\begin{array} { r } { e ^ { \boldsymbol { A } t } : = \sum _ { j = 0 } ^ { \infty } \frac { \boldsymbol { A } ^ { j } } { j ! } t ^ { j } } \end{array}$

More generally (see Appendix D, equation (D.4.6)), if for each $S \in \mathbb R$ , $\mathbf { X } ( S )$ has finite second moments and is independent of $\{ B ( t ) - B ( s )$ , $S \le s \le t < \infty \}$ , then the unique solution of (11.5.9) specified by Theorem D.4.1 satisfies

$$
\mathbf {X} (t) = e ^ {A (t - S)} \mathbf {X} (S) + \int_ {S} ^ {t} e ^ {A (t - u)} \mathbf {e} d B (u), t \geq S, \text {f o r a l l} S \in \mathbb {R}. \tag {11.5.11}
$$

If the real parts of the eigenvalues $\lambda _ { 1 } , \ldots , \lambda _ { p }$ of $A$ (which are also the zeroes of the autoregressive polynomial $a ( z ) )$ ) satisfy

$$
\mathcal {R e} \left(\lambda_ {r}\right) <   0, r = 1, \dots , p, \tag {11.5.12}
$$

and if $\{ { \bf X } ( t ) \}$ is a stationary solution of (11.5.11) then, taking mean square limits as $S \to - \infty$ in (11.5.11), we see that it must satisfy

$$
\mathbf {X} (t) = \int_ {- \infty} ^ {t} e ^ {A (t - u)} \mathbf {e} d B (u), t \in \mathbb {R}. \tag {11.5.13}
$$

Conversely, if $\{ { \bf X } ( t ) \}$ is given by (11.5.13) then it is a stationary solution of (11.5.11) and for each $\textit { S } \in \mathbb { R }$ , X(S) has finite second moments and is independent of $\{ B ( t ) - B ( s )$ , $S ~ \le ~ s ~ \le ~ t ~ < ~ \infty \}$ . Hence it is the unique solution of the type specified in Theorem D.4.1 with these properties. The property that, for each S, $\mathbf { X } ( S )$ is independent of $\{ B ( t ) - B ( s )$ , $S \leq s \leq t < \infty \}$ , corresponds to the discrete-time concept of causality introduced in Section 3.1.

Assuming that condition (11.5.12) is satisfied, we define the zero-mean causal Gaussian CARMA $( p , q )$ process $\{ Y ( t ) , \ t \ \in \ \mathbb { R } \}$ , with parameters $( a _ { 1 } , \ldots , a _ { p } , b _ { 0 }$ , $\dots , b _ { q } )$ , by

$$
Y (t) = \mathbf {b} ^ {\prime} \mathbf {X} (t), \tag {11.5.14}
$$

where $\{ { \bf X } ( t ) \}$ is given by (11.5.13). A Gaussian CARMA process with mean m is obtained by simply adding the constant value $m$ to Y .

Remark 3. For the zero-mean causal Gaussian CAR(1) process defined by (11.5.1), with $c = 0$ and with index set $\mathbb { R }$ instead of $\lbrack 0 , \infty )$ as in Section 11.5.1, we have ${ \bf b } = \sigma$ and $A = - a$ , so that

$$
Y (t) = \sigma \int_ {- \infty} ^ {t} e ^ {- a (t - u)} d B (u), t \in \mathbb {R}.
$$

The autocovariance function of the process $\mathbf { X } ( t )$ at lag $h$ is easily found from (11.5.13) to be

$$
\operatorname {C o v} (\mathbf {X} (t + h), \mathbf {X} (t)) = e ^ {A | h |} \Sigma ,
$$

where

$$
\Sigma := \int_ {0} ^ {\infty} e ^ {A y} \mathbf {e} e ^ {\prime} e ^ {A ^ {\prime} y} d y.
$$

The mean and autocovariance function of the CARMA $( p , q )$ process $\{ Y ( t ) \}$ are therefore given by

$$
E Y (t) = 0
$$

and

$$
\operatorname {C o v} (Y (t + h), Y (t)) = \mathbf {b} ^ {\prime} e ^ {A | h |} \boldsymbol {\Sigma} \mathbf {b}.
$$

Inference for a CARMA $( p , q )$ process with $p > 1$ is more complicated than for a CAR(1) process because the former is not Markovian, so the simple argument that led to (11.5.6) no longer holds. However, the Gaussian likelihood of observations at times $t _ { 1 } , \ldots , t _ { n }$ can still easily be computed using the discrete-time Kalman recursions as pointed out by Jones (1980).

Simulation and estimation, not only for Gaussian, but also for Lévy-driven CARMA processes (as introduced in the following subsection) can be carried out using the Yuima package, a package for use in the R environment, which can be downloaded from https://cran.r-project.org/web/packages/yuima. A detailed account of its application to CARMA processes is contained in the paper of Iacus and Mercuri (2015). A simulated Gaussian (3,2) process and the components of its state-vector, generated in R by the Yuima package, is shown in Figure 11-10.

Rather than examining Gaussian CARMA processes in more detail, we next introduce the more general class of Lévy-driven $\mathbf { C A R M A } ( p , q )$ processes, whose marginal distributions can be both heavy-tailed and asymmetric and whose samplepaths are continuous if $q < p - 1$ and have the same jumps as the driving Lévy process if $q = p - 1$ .

# 11.5.3 Lévy-driven CARMA Processes, $\{ Y ( t ) , t \in \mathbb { R } \}$

In Section 11.5.2, under the assumption (11.5.12), we defined the zero-mean causal Gaussian CARMA process $\{ Y ( t ) , t \in \mathbb { R } \}$ as the strictly stationary linear combination (11.5.14) of components of the state-vector $\mathbf { X } ( t )$ given by (11.5.13). In this section we wish to extend the definition by replacing the driving process $B$ by a Lévy process $L$ in order to allow a much broader class of possible marginal distributions for $Y ( t )$ . As in Section 11.5.2 we shall assume that the polynomials $a ( z )$ and $b ( z )$ in the defining stochastic differential equation,

$$
a (D) Y (t) = b (D) D L (t),
$$

have no common zeroes. We use the same state-space representation of the process as in Section 11.5.2 to obtain a rigorous interpretation of this equation.

![](images/c4dcfc1db393e392a8cdb049776e346730d9ab426362e77cbc4dc8bad5fb4425.jpg)  
Figure 11-10 Simulated CARMA(3,2) process y and state-vector X driven by standard Brownian motion with $a _ { 1 } = 4$ , $a _ { 2 } = 4 . 5$ , $a _ { 3 } = 1 . 5$ , $b _ { 0 } = 1$ , $b _ { 1 } = . 2 3$ , $b _ { 2 } = . 3 5$

Replacing the Brownian motion $\{ B ( t ) \}$ in (11.5.11) by the Lévy process $\{ L ( t ) \}$ gives

$$
\mathbf {X} (t) = e ^ {A (t - S)} \mathbf {X} (S) + \int_ {(S, t ]} e ^ {A (t - S)} d L (u), t \geq S, \text {f o r a l l} S \in \mathbb {R}. \tag {11.5.15}
$$

where the integral is now interpreted in the sense of Protter (2010). We then define the CARMA $( p , q )$ process driven by $L$ , with coefficients $( a _ { 1 } , \hdots , a _ { p } , b _ { 1 } , \hdots , b _ { q } )$ , to be a strictly stationary solution $\{ Y ( t ) \}$ of the equation (11.5.15) and

$$
Y (t) = \mathbf {b} ^ {\prime} \mathbf {X} (t). \tag {11.5.16}
$$

The matrix A and the vector b are defined as in Section 11.5.2, except that we now define $b _ { q } : = 1$ since there is no variance constraint on $L ( 1 )$ as there was on $B ( 1 )$ in the definition of the Gaussian special cases in Sections 11.5.1 and 11.5.2. In the case when $L$ is a subordinator (i.e., a Lévy process with non-decreasing sample paths), the integral in (11.5.15) can also be interpreted as a pathwise Stieltjes integral with respect to $L$ .

Brockwell and Lindner (2009) have established necessary and sufficient conditions for the existence of a strictly stationary solution $\{ Y ( t ) , \ t \ \in \ \mathbb { R } \}$ of (11.5.15) and (11.5.16). If we assume that $a ( z )$ and $b ( z )$ have no common zeroes and $L$ is not deterministic, then the necessary and sufficient conditions are

$$
E \max  (0. \log | L (1) |) <   \infty \tag {11.5.17}
$$

and

$$
\mathcal {R e} \left(\lambda_ {r}\right) \neq 0, r = 1, \dots , p, \tag {11.5.18}
$$

where $\lambda _ { 1 } , \ldots , \lambda _ { p }$ are the eigenvalues of $A$ (which are also the zeroes of the autoregressive polynomial $a ( z )$ ).

The strictly stationary solution is unique, and under the stronger conditions,

$$
\mathcal {R e} \left(\lambda_ {r}\right) <   0, r = 1, \dots , p, \tag {11.5.19}
$$

it is causal, i.e., for every s, t and $u$ such that $s \leq t \leq u , Y ( s$ ) is independent of $L ( u ) - L ( t )$ and can be expressed as

$$
Y (t) = \int_ {(- \infty , t ]} \mathbf {b} ^ {\prime} e ^ {A (t - u)} \mathbf {e} d L (u). \tag {11.5.20}
$$

(The representation (11.5.20) is easily obtained formally by letting $s  - \infty$ in (11.5.15) and substituting the resulting expression for $\mathbf { X } ( t )$ in (11.5.16).) Thus, under the causality condition (11.5.19), equations (11.5.15) and (11.5.16) have the unique strictly stationary solution (11.5.20). This solution is the causal CARMA $( p , q )$ process with parameters $( a _ { 1 } , \ldots , a _ { p } , b _ { 1 } , \ldots , b _ { q } : = 1 )$ driven by the Lévy process $L$ .

From equation (11.5.20) we find that, if $E ( L ( 1 ) ^ { 2 } ) < \infty$ , $E Y ( t ) = \mu b _ { 0 } / a _ { p }$ , where $\mu = E L ( 1 )$ , and

$$
\gamma_ {Y} (h) := \operatorname {C o v} [ Y (t + h), Y (t) ] = \sigma^ {2} \mathbf {b} ^ {\prime} e ^ {A | h |} \Sigma \mathbf {b}, \tag {11.5.21}
$$

where $\sigma ^ { 2 } = \mathrm { V a r } ( L ( 1 ) )$ and $\begin{array} { r } { \Sigma = \int _ { 0 } ^ { \infty } e ^ { A y } \mathbf { e } \mathbf { e } ^ { \prime } e ^ { A ^ { \prime } y } d y } \end{array}$ .

Remark 4. A result of Eller (1987) was used by Brockwell and Lindner (2009) to rewrite (11.5.20) as

$$
Y (t) = \int_ {(- \infty , t ]} \sum_ {\lambda} \sum_ {k = 0} ^ {m (\lambda) - 1} c _ {\lambda k} (t - u) ^ {k} e ^ {\lambda (t - u)} d L (u), \tag {11.5.22}
$$

where $\Sigma _ { \lambda }$ denotes summation over the zeroes $\lambda$ of $a ( z )$ , $m ( \lambda )$ is the multiplicity of the zero λ and 	m(λ)k 0 $\lambda$ ${ \begin{array} { r } { \sum _ { k = 0 } ^ { m ( \lambda ) - 1 } c _ { \lambda k } t ^ { k } e ^ { \lambda t } } \end{array} }$ is the residue at $\lambda$ of the mapping $z \mapsto e ^ { z t } b ( z ) / a ( z )$ . If the zeroes, $\lambda _ { 1 } , \ldots , \lambda _ { p }$ , of $a ( z )$ each have multiplicity one, then the expression (11.5.22) simplifies to

$$
Y (t) = \int_ {(- \infty , t ]} \sum_ {r = 1} ^ {p} \alpha_ {r} e ^ {\lambda_ {r} (t - u)} d L (u), \tag {11.5.23}
$$

where $\alpha _ { r } = b ( \lambda _ { r } ) / a ^ { \prime } ( \lambda _ { r } )$ . Hence $\{ Y ( t ) \}$ has a corresponding canonical representation as a linear combination of (possibly complex-valued) CAR(1) processes,

$$
Y (t) = \sum_ {r = 1} ^ {p} \alpha_ {r} Y _ {r} (t), \tag {11.5.24}
$$

where $\begin{array} { r } { Y _ { r } ( t ) = \int _ { ( - \infty , t ] } e ^ { \lambda _ { r } ( t - u ) } d L ( u ) } \end{array}$ . Notice that the driving process $L$ is the same for −∞ ]each of the component processes $\{ Y _ { r } ( t ) \}$ , so they are not independent. Corresponding to the canonical decomposition (11.5.24), if $E ( L ( 1 ) ^ { 2 } ) \ < \ \infty$ , there is an analogous representation of the autocovariance function when $E ( L ( 1 ) ^ { 2 } ) < \infty$ , namely

$$
\gamma (h) = \sum_ {r = 1} ^ {p} \beta_ {r} e ^ {\lambda_ {r} | h |}, \tag {11.5.25}
$$

where $\beta _ { r } = \sigma ^ { 2 } b ( \lambda _ { r } ) b ( - \lambda _ { r } ) / [ a ( - \lambda _ { r } ) a ^ { \prime } ( \lambda _ { r } ) ] .$

# Example 11.5.1. Stochastic Volatility

The stochastic volatility process, $h$ , appearing in Example 7.5.4 was defined as

$$
h (t) = \int_ {(- \infty . t ]} e ^ {\lambda (t - u)} d L (u), \text {w h e r e} \lambda <   0, \tag {11.5.26}
$$

i.e., as a Lévy-driven CARMA(1, 0) process with $a ( z ) = z - \lambda$ and $b ( z ) = 1$ . For non-negativity of $h$ , the Lévy process $L$ is required to be a subordinator, for example, a gamma process with characteristic function $E e ^ { i \theta L ( t ) } = ( 1 - i \theta / \beta ) ^ { - \alpha t }$ , $E L ( t ) \ =$ $\alpha t / \beta$ and $\mathrm { V a r } ( L ( 1 ) ) ~ = ~ \alpha t / \beta ^ { 2 }$ . Then $E h ( t ) ~ = ~ \alpha t / ( \beta | \lambda | )$ and the autocovariance function of $h$ is, from (11.5.27), $\gamma _ { h } ( s ) = e ^ { \lambda | s | } \alpha / ( 2 \beta ^ { 2 } | \lambda | )$ . For any finite-variance Lévydriven CARMA(1,0) model for stochastic volatility, the autocorrelation function is necessarily of the form $\rho ( s ) = e ^ { \lambda | s | }$ for some negative $\lambda$ . In order to relax this constraint a non-negative CARMA $( p , q )$ model for $h$ can be employed (see e.g., Brockwell and Lindner 2012).

![](images/d529eec96f894ad4b5d1d2f30d96efeb5f4a500628bf9bab7f81030fc07d11c0.jpg)

Remark 5. Marginal distributions. The condition (11.5.17) clearly does not require $L ( 1 )$ (and consequently $Y ( t ) _ { , }$ ) to have finite variance. In fact the condition $E | L ( 1 ) | ^ { r } \ < \ \infty$ for some $r ~ > ~ 0$ is sufficient to ensure that (11.5.17) holds. Given a $\mathbf { \boldsymbol { C } A R M A } ( p , q )$ process $Y$ driven by a Lévy process $L$ with characteristic function $E ( e ^ { i \theta L ( t ) } ) = \exp ( t \xi ( \theta ) ) , \theta \in \mathbb { R }$ , (see Appendix D.1), the joint characteristic function of $Y ( t _ { 1 } ) , \dots , Y ( t _ { n } )$ can be expressed in terms of the coefficients of the polynomials a and $^ b$ and the characteristic exponent $\xi ( \cdot )$ of $L$ (see Brockwell (2014)). In particular the logarithm of the marginal characteristic function of $Y ( t )$ is

$$
\ln E e ^ {i \theta Y (t)} = \int_ {0} ^ {\infty} \xi \left(\theta \mathbf {b} ^ {\prime} e ^ {A u} \mathbf {e}\right) d u, \theta \in \mathbb {R}. \tag {11.5.27}
$$

For the CAR(1) process $h$ defined by (11.5.26) this simplifies to

$$
1 \mathrm {n} E e ^ {i \theta h (t)} = \int_ {0} ^ {\infty} \xi (\theta e ^ {\lambda u}) d u = | \lambda | ^ {- 1} \int_ {0} ^ {\theta} y ^ {- 1} \xi (y) d y, \tag {11.5.28}
$$

(where $\textstyle { \int _ { 0 } ^ { \theta } : = - \int _ { \theta } ^ { 0 } }$ if $\theta < 0$ ). If, for example, $L ( 1 )$ has a symmetric stable distribution with $1 \mathrm { n } E e ^ { i \theta L ( 1 ) } = - c | \theta | ^ { \alpha } , c > 0 , 0 < \alpha \leq 2$ , then $E | L ( 1 ) | ^ { r } < \infty$ for all $r \in ( 0 , \alpha )$ and from (11.5.28) we find at once (Problem 11.8) that,

$$
1 \mathrm {n} E e ^ {i \theta h (t)} = - \frac {c}{\alpha | \lambda |} | \theta | ^ {\alpha}, \tag {11.5.29}
$$

in other words $h ( t )$ has a symmetric stable distribution with the same exponent $\alpha$ as $L ( 1 )$ but with the parameter c replaced by $c / ( \alpha | \lambda | )$ .

# Problems

11.1 Find a transfer function model relating the input and output series $X _ { t 1 }$ and $X _ { t 2 }$ , $t = 1 , \ldots , 2 0 0$ , contained in the ITSM data files APPJ.TSM and APPK.TSM, respectively. Use the fitted model to predict $X _ { 2 0 1 , 2 } , X _ { 2 0 2 , 2 }$ , and $X _ { 2 0 3 , 2 }$ . Compare the predictors and their mean squared errors with the corresponding predictors and mean squared errors obtained by modeling $\{ X _ { t 2 } \}$ as a univariate ARMA process and with the results of Problem 8.7.   
11.2 Verify the calculations of Example 11.2.1 to fit an intervention model to the series SB.TSM.   
11.3 If $\{ X _ { t } \}$ is the linear process (11.3.1) with $\{ Z _ { t } \} \sim \mathrm { I I D } \left( 0 , \sigma ^ { 2 } \right)$ and $\eta = E Z _ { t } ^ { 3 }$ , show that the third-order cumulant function of $\{ X _ { t } \}$ is given by

$$
C _ {3} (r, s) = \eta \sum_ {i = - \infty} ^ {\infty} \psi_ {i} \psi_ {i + r} \psi_ {i + s}.
$$

Use this result to establish equation (11.3.4). Conclude that if $\{ X _ { t } \}$ is a Gaussian linear process, then $C _ { 3 } ( r , s ) \equiv 0$ and $f _ { 3 } ( \omega _ { 1 } , \omega _ { 2 } ) \equiv 0$ .

11.4 If $a ~ > ~ 0$ and $Y ( 0 )$ has mean $b / a$ and variance $\sigma ^ { 2 } / ( 2 a )$ , show that the process defined by (11.5.3) is stationary and evaluate its mean and autocovariance function.   
11.5 The file TRINGS.TSM contains normalized tree-ring widths of a Colorado pine for the years 525–774 (Donald Graybill 1984) from the file CO522.DAT at http:// www-personal.buseco.monash.edu.au/hyndman/TSDL/.

a. Use exact maximum likelihood estimation to fit a fractionally integrated ARMA model to the first 230 tree-ring widths and use the model to generate forecasts and $9 5 \%$ prediction bounds for the last 20 observations (corresponding to $t ~ = ~ 2 3 1 , \dots , 2 5 0 )$ . Plot the entire data set with the forecasts and prediction bounds superposed on the graph of the data.   
b. Repeat part (a), but this time fitting an appropriate ARMA model. Compare the performance of the two sets of predictors.

11.6 The tent map with parameter $s \in ( 1 , \infty )$ is the function

$$
g (x) = s x I _ {[ 0, 1 / s)} (x) + \frac {s}{s - 1} (1 - x) I _ {[ 1 / s, 1 ]} (x), \quad x \in [ 0, 1 ],
$$

where $I _ { A }$ denotes the indicator function of the set $A$ . If $X _ { 0 }$ has the uniform distribution on 0, 1 (written more concisely as $X _ { 0 } ~ \sim ~ U )$ and if $\{ X _ { n } \}$ is the sequence defined by $X _ { n } = g ( X _ { n - 1 } )$ , $n = 1 , 2 , \ldots$ , then $\{ X _ { n } \}$ is a Markov chain and $X _ { n } ~ \sim ~ U$ for all $n ~ \in ~ \{ 0 , 1 , 2 , \ldots \}$ , so that $\{ X _ { n } \}$ is strictly (and weakly) stationary.

a. Show that in the symmetric case $( s = 2 )$ ), $\{ X _ { n } \} \sim \mathrm { W N } ( 0 , 1 / 1 2 )$ .   
b. In the general case, $X _ { n } - 0 . 5 = \phi ( X _ { n - 1 } - 0 . 5 ) + Z _ { n }$ , $n = 1 , 2 , \ldots$ , where $\phi = ( 2 / s ) - 1$ and $\{ Z _ { n } \}$ is an uncorrelated (but strongly dependent) sequence of random variables with mean zero and variance $( 1 - \phi ^ { 2 } ) / 1 2$ . (See Sakai and Tokumaru 1980.)

11.7 A Lévy-driven CARMA(2,1) process is defined by the stochastic differential equation,

$$
\left(D ^ {2} + 1. 5 D +. 5\right) Y (t) = \left(D +. 2\right) D L (t), t \in \mathbb {R},
$$

where $L$ is a Poisson process with jump-rate $\rho$

a. Calculate $E Y ( t )$ .   
b. Use (11.5.23) to determine the canonical decomposition of $\{ Y ( t ) \}$   
c. Use (11.5.25) to determine the autocovariance function of $\{ Y ( t ) \}$

11.8 Use (11.5.27) to verify (11.5.28) and (11.5.29). If ${ \bf \ddot { \cal L } } ( 1 ) \sim { \bf N } ( 0 , \sigma ^ { 2 } )$ , use (11.5.29) to determine the distribution of $h ( t )$ , as defined by (11.5.26).

# Random Variables and Probability Distributions

A.1 Distribution Functions and Expectation   
A.2 Random Vectors   
A.3 The Multivariate Normal Distribution

# A.1 Distribution Functions and Expectation

The distribution function $F$ of a random variable $X$ is defined by

$$
F (x) = P [ X \leq x ] \tag {A.1.1}
$$

for all real $x$ . The following properties are direct consequences of (A.1.1):

1. $F$ is nondecreasing, i.e., $F ( x ) \leq F ( y )$ if $x \leq y$ .   
2. $F$ is right continuous, i.e., $F ( y ) \downarrow F ( x )$ as $y \downarrow x$   
3. $F ( x ) \to 1$ and $F ( y ) \to 0$ as $x \to \infty$ and $y  - \infty$ , respectively.

Conversely, any function that satisfies properties 1–3 is the distribution function of some random variable.

Most of the commonly encountered distribution functions $F$ can be expressed either as

$$
F (x) = \int_ {- \infty} ^ {x} f (y) d y \tag {A.1.2}
$$

or

$$
F (x) = \sum_ {j: x _ {j} \leq x} p \left(x _ {j}\right), \tag {A.1.3}
$$

where $\{ x _ { 0 } , x _ { 1 } , x _ { 2 } , \ldots \}$ is a finite or countably infinite set. In the case (A.1.2) we shall say that the random variable $X$ is continuous. The function $f$ is called the probability density function (pdf) of $X$ and can be found from the relation

$$
f (x) = F ^ {\prime} (x).
$$

In case (A.1.3), the possible values of $X$ are restricted to the set $\{ x _ { 0 } , x _ { 1 } , \ldots \}$ , and we shall say that the random variable $X$ is discrete. The function $p$ is called the probability mass function (pmf) of $X$ , and $F$ is constant except for upward jumps of size $p ( x _ { j } )$ at the points $x _ { j }$ . Thus $p ( x _ { j } )$ is the size of the jump in $F$ at $x _ { j }$ , i.e.,

$$
p (x _ {j}) = F (x _ {j}) - F (x _ {j} ^ {-}) = P [ X = x _ {j} ],
$$

where $F ( x _ { j } ^ { - } ) = \mathrm { l i m } _ { y \uparrow x _ { j } } F ( y )$ .

# A.1.1 Examples of Continuous Distributions

(a) The normal distribution with mean $\mu$ and variance $\sigma ^ { 2 }$ . We say that a random variable $X$ has the normal distribution with mean $\mu$ and variance $\sigma ^ { 2 }$ written more concisely as $X \sim N ( \mu , \sigma ^ { 2 } ) )$ if $X$ has the pdf given by

$$
n \left(x; \mu , \sigma^ {2}\right) = (2 \pi) ^ {- 1 / 2} \sigma^ {- 1} e ^ {- (x - \mu) ^ {2} / (2 \sigma^ {2})} \qquad - \infty <   x <   \infty .
$$

It follows then that $Z = ( X - \mu ) / \sigma \sim N ( 0 , 1 )$ and that

$$
P [ X \leq x ] = P \left[ Z \leq \frac {x - \mu}{\sigma} \right] = \Phi \left(\frac {x - \mu}{\sigma}\right),
$$

where $\begin{array} { r } { \Phi ( x ) = \int _ { - \infty } ^ { x } ( 2 \pi ) ^ { - 1 / 2 } e ^ { - \frac { 1 } { 2 } z ^ { 2 } } d z } \end{array}$ is known as the standard normal distribu-−∞tion function. The significance of the terms mean and variance for the parameters $\mu$ and $\sigma ^ { 2 }$ is explained below (see Example A.1.1).

(b) The uniform distribution on $[ a , b ]$ . The pdf of a random variable uniformly distributed on the interval $[ a , b ]$ is given by

$$
u (x; a, b) = \left\{ \begin{array}{l l} \frac {1}{b - a}, & \text {i f} a \leq x \leq b, \\ 0, & \text {o t h e r w i s e .} \end{array} \right.
$$

(c) The exponential distribution with parameter λ. The pdf of an exponentially distributed random variable with parameter $\lambda > 0$ is

$$
e (x; \lambda) = \left\{ \begin{array}{l l} 0, & \text {i f} x <   0, \\ \lambda e ^ {- \lambda x}, & \text {i f} x \geq 0. \end{array} \right.
$$

The corresponding distribution function is

$$
F (x) = \left\{ \begin{array}{l l} 0, & \text {i f} x <   0, \\ 1 - e ^ {- \lambda x}, & \text {i f} x \geq 0. \end{array} \right.
$$

(d) The gamma distribution with parameters α and λ. The pdf of a gamma-distributed random variable is

$$
g (x; \alpha , \lambda) = \left\{ \begin{array}{l l} 0, & \text {i f} x <   0, \\ x ^ {\alpha - 1} \lambda^ {\alpha} e ^ {- \lambda x} / \Gamma (\alpha), & \text {i f} x \geq 0, \end{array} \right.
$$

where the parameters $\alpha$ and $\lambda$ are both positive and $\Gamma$ is the gamma function defined as

$$
\Gamma (\alpha) = \int_ {0} ^ {\infty} x ^ {\alpha - 1} e ^ {- x} d x.
$$

Note that $f$ is the exponential pdf when $\alpha = 1$ and that when $\alpha$ is a positive integer

$$
\Gamma (\alpha) = (\alpha - 1)! \text {w i t h} 0! \text {d e f i n e d t o b e} 1.
$$

(e) The chi-squared distribution with ν degrees of freedom. For each positive integer ν, the chi-squared distribution with $\nu$ degrees of freedom is defined to be the distribution of the sum

$$
X = Z _ {1} ^ {2} + \dots + Z _ {\nu} ^ {2},
$$

where $Z _ { 1 } , \ldots , Z _ { \nu }$ are independent normally distributed random variables with mean 0 and variance 1. This distribution is the same as the gamma distribution with parameters $\alpha = \nu / 2$ and $\begin{array} { r } { \lambda = \frac { 1 } { 2 } } \end{array}$ .

# A.1.2 Examples of Discrete Distributions

(f) The binomial distribution with parameters n and $p$ . The pmf of a binomially distributed random variable $X$ with parameters $n$ and $p$ is

$$
b (j; n, p) = P [ X = j ] = \binom {n} {j} p ^ {j} (1 - p) ^ {n - j}, \quad j = 0, 1, \ldots , n,
$$

where $n$ is a positive integer and $0 \leq p \leq 1$ .

(g) The uniform distribution on $\{ 1 , 2 , \ldots , k \}$ . The pmf of a random variable $X$ uniformly distributed on $\{ 1 , 2 , \ldots , k \}$ is

$$
p (j) = P [ X = j ] = \frac {1}{k}, \quad j = 1, 2 \ldots , k,
$$

where $k$ is a positive integer.

(h) The Poisson distribution with parameter λ. A random variable $X$ is said to have a Poisson distribution with parameter $\lambda > 0$ if

$$
p (j; \lambda) = P [ X = j ] = \frac {\lambda^ {j}}{j !} e ^ {- \lambda}, \quad j = 0, 1, \dots .
$$

We shall see in Example A.1.2 below that $\lambda$ is the mean of $X$ .

(i) The negative binomial distribution with parameters α and $p$ . The random variable $X$ is said to have a negative binomial distribution with parameters $\alpha \ : > \ : 0$ and $p \in [ 0 , 1 ]$ if it has pmf

$$
n b (j; \alpha , p) = \left(\prod_ {k = 1} ^ {j} \frac {k - 1 + \alpha}{k}\right) (1 - p) ^ {j} p ^ {\alpha}, \quad j = 0, 1, \dots ,
$$

where the product is defined to be 1 if $j = 0$ .

Not all random variables can be neatly categorized as either continuous or discrete. For example, consider the time you spend waiting to be served at a checkout counter and suppose that the probability of finding no customers ahead of you is $\frac { 1 } { 2 }$ . Then the time you spend waiting for service can be expressed as

$$
W = \left\{ \begin{array}{l l} 0, & \text {w i t h p r o b a b i l i t y} \frac {1}{2}, \\ W _ {1}, & \text {w i t h p r o b a b i l i t y} \frac {1}{2}, \end{array} \right.
$$

where $W _ { 1 }$ is a continuous random variable. If the distribution of $W _ { 1 }$ is exponential with parameter 1, then the distribution function of W is

$$
F (x) = \left\{ \begin{array}{l l} 0, & \text {i f} x <   0, \\ \frac {1}{2} + \frac {1}{2} \left(1 - e ^ {- x}\right) = 1 - \frac {1}{2} e ^ {- x}, & \text {i f} x \geq 0. \end{array} \right.
$$

This distribution function is neither continuous (since it has a discontinuity at $x = 0$ ) nor discrete (since it increases continuously for $x > 0$ ). It is expressible as a mixture,

$$
F = p F _ {\mathrm {d}} + (1 - p) F _ {\mathrm {c}},
$$

with $\begin{array} { r } { p = \frac { 1 } { 2 } } \end{array}$ , of a discrete distribution function

$$
F _ {\mathrm {d}} = \left\{ \begin{array}{l l} 0, & x <   0, \\ 1, & x \geq 0, \end{array} \right.
$$

and a continuous distribution function

$$
F _ {\mathrm {c}} = \left\{ \begin{array}{l l} 0, & x <   0, \\ 1 - e ^ {- x}, & x \geq 0. \end{array} \right.
$$

Every distribution function can in fact be expressed in the form

$$
F = p _ {1} F _ {\mathrm {d}} + p _ {2} F _ {\mathrm {c}} + p _ {3} F _ {\mathrm {s c}},
$$

where $0 \leq p _ { 1 } , p _ { 2 } , p _ { 3 } \leq 1 , p _ { 1 } + p _ { 2 } + p _ { 3 } = 1$ , $F _ { \mathrm { d } }$ is discrete, $F _ { \mathrm { c } }$ is continuous, and $F _ { \mathrm { s c } }$ is singular continuous (continuous but not of the form A.1.2). Distribution functions with a singular continuous component are rarely encountered.

# A.1.3 Expectation, Mean, and Variance

The expectation of a function $g$ of a random variable $X$ is defined by

$$
E \left(g (X)\right) = \int g (x) d F (x),
$$

where

$$
\int g (x) d F (x) := \left\{ \begin{array}{l l} \int_ {- \infty} ^ {\infty} g (x) f (x) d x & \text {i n t h e c o n t i n u o u s c a s e}, \\ \sum_ {j = 0} ^ {\infty} g (x _ {j}) p (x _ {j}) & \text {i n t h e d i s c r e t e c a s e}, \end{array} \right.
$$

and $g$ is any function such that $E | g ( x ) | < \infty$ . (If $F$ is the mixture $F = p F _ { \mathrm { c } } + ( 1 - p ) F _ { \mathrm { d } }$ , then $\begin{array} { r } { E ( g ( X ) ) = p \int g ( x ) d F _ { \mathrm { c } } ( x ) + ( 1 - p ) \int g ( x ) d F _ { \mathrm { d } } ( x ) . } \end{array}$ ) The mean and variance of $X$ are defined as $\mu = E X$ and $\sigma ^ { 2 } = E ( X - \mu ) ^ { 2 }$ , respectively. They are evaluated by setting $g ( x ) = x$ and $g ( x ) = ( x - \mu ) ^ { 2 }$ in the definition of $E ( g ( X ) )$ .

It is clear from the definition that expectation has the linearity property

$$
E (a X + b) = a E (X) + b
$$

for any real constants $a$ and $^ b$ (provided that $E | X | < \infty$ ).

# Example A.1.1 The Normal Distribution

If $X$ has the normal distribution with pdf $n \left( x ; \mu , \sigma ^ { 2 } \right)$ as defined in Example (a) above, then

$$
E (X - \mu) = \int_ {- \infty} ^ {\infty} (x - \mu) n (x; \mu , \sigma^ {2}) d x = - \sigma^ {2} \int_ {- \infty} ^ {\infty} n ^ {\prime} (x: \mu , \sigma^ {2}) d x = 0.
$$

This shows, with the help of the linearity property of $E$ , that

$$
E (X) = \mu ,
$$

i.e., that the parameter $\mu$ is in fact the mean of the normal distribution defined in Example (a). Similarly,

$$
E (X - \mu) ^ {2} = \int_ {- \infty} ^ {\infty} (x - \mu) ^ {2} n \left(x; \mu , \sigma^ {2}\right) d x = - \sigma^ {2} \int_ {- \infty} ^ {\infty} (x - \mu) n ^ {\prime} \left(x; \mu , \sigma^ {2}\right) d x.
$$

Integrating by parts and using the fact that $f$ is a pdf, we find that the variance of $X$ is

$$
E (X - \mu) ^ {2} = \sigma^ {2} \int_ {- \infty} ^ {\infty} n (x; \mu , \sigma^ {2}) d x = \sigma^ {2}.
$$

# Example A.1.2 The Poisson Distribution

The mean of the Poisson distribution with parameter λ (see Example (h) above) is given by

$$
\mu = \sum_ {j = 0} ^ {\infty} \frac {j \lambda^ {j}}{j !} e ^ {- \lambda} = \sum_ {j = 1} ^ {\infty} \frac {\lambda \lambda^ {j - 1}}{(j - 1) !} e ^ {- \lambda} = \lambda e ^ {\lambda} e ^ {- \lambda} = \lambda .
$$

A similar calculation shows that the variance is also equal to $\lambda$ (see Problem A.2).

Remark . Functions and parameters associated with a random variable $X$ will be labeled with the subscript $X$ whenever it is necessary to identify the particular random variable to which they refer. For example, the distribution function, pdf, mean, and variance of $X$ will be written as FX, fX, $\mu _ { X }$ , and $\sigma _ { X } ^ { 2 }$ , respectively, whenever it is necessary to distinguish them from the corresponding quantities $F _ { Y } , f _ { Y } , \mu _ { Y }$ , and $\sigma _ { Y } ^ { 2 }$ associated with a different random variable Y.

# A.2 Random Vectors

An $n$ -dimensional random vector is a column vector $\mathbf { X } = ( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ each of whose components is a random variable. The distribution function $F$ of $\mathbf { X }$ , also called the joint distribution of $X _ { 1 } , \ldots , X _ { n }$ , is defined by

$$
F \left(x _ {1}, \dots , x _ {n}\right) = P \left[ X _ {1}, \leq x _ {1}, \dots , X _ {n} \leq x _ {n} \right] \tag {A.2.1}
$$

for all real numbers $x _ { 1 } , \ldots , x _ { n }$ . This can be expressed in a more compact form as

$$
F (\mathbf {x}) = P [ \mathbf {X} \leq \mathbf {x} ], \quad \mathbf {x} = (x _ {1}, \dots , x _ {n}) ^ {\prime},
$$

for all real vectors $\mathbf { x } ~ = ~ ( x _ { 1 } , \ldots , x _ { n } ) ^ { \prime }$ . The joint distribution of any subcollection $X _ { i _ { 1 } } , \ldots , X _ { i _ { k } }$ of these random variables can be obtained from $F$ by setting $x _ { j } = \infty$

in (A.2.1) for all $j \not \in \{ i _ { 1 } , \ldots , i _ { k } \}$ . In particular, the distributions of $X _ { 1 }$ and $( X _ { 1 } , X _ { n } ) ^ { \prime }$ are given by

$$
F _ {X _ {1}} (x _ {1}) = P [ X _ {1} \leq x _ {1} ] = F (x _ {1}, \infty , \dots , \infty)
$$

and

$$
F _ {X _ {1}, X _ {n}} \left(x _ {1}, x _ {n}\right) = P \left[ X _ {1} \leq x _ {1}, X _ {n} \leq x _ {n} \right] = F \left(x _ {1}, \infty , \dots , \infty , x _ {n}\right).
$$

As in the univariate case, a random vector with distribution function $F$ is said to be continuous if $F$ has a density function, i.e., if

$$
F (x _ {1}, \dots , x _ {n}) = \int_ {- \infty} ^ {x _ {n}} \dots \int_ {- \infty} ^ {x _ {2}} \int_ {- \infty} ^ {x _ {1}} f (y _ {1}, \dots , y _ {n}) d y _ {1} d y _ {2} \dots d y _ {n}.
$$

The probability density of $\mathbf { X }$ is then found from

$$
f (x _ {1}, \ldots , x _ {n}) = \frac {\partial^ {n} F (x _ {1} , \ldots , x _ {n})}{\partial x _ {1} \cdots \partial x _ {n}}.
$$

The random vector $\mathbf { X }$ is said to be discrete if there exist real-valued vectors $\mathbf { X } _ { 0 } , \mathbf { X } _ { 1 } , \ldots$ and a probability mass function $p ( \mathbf { x } _ { j } ) = P [ \mathbf { X } = \mathbf { x } _ { j } ]$ such that

$$
\sum_ {j = 0} ^ {\infty} p (\mathbf {x} _ {j}) = 1.
$$

The expectation of a function $g$ of a random vector $\mathbf { X }$ is defined by

$$
E \left(g (\mathbf {X})\right) = \int g (\mathbf {x}) d F (\mathbf {x}) = \int g (x _ {1}, \ldots , x _ {n}) d F (x _ {1}, \ldots , x _ {n}),
$$

where

$$
\begin{array}{l} \int g \left(x _ {1}, \dots , x _ {n}\right) d F \left(x _ {1}, \dots , x _ {n}\right) \\ = \left\{ \begin{array}{l l} \int \dots \int g (x _ {1}, \dots , x _ {n}) f (x _ {1}, \dots , x _ {n}) d x _ {1} \dots d x _ {n}, & \text {i n t h e c o n t i n u o u s c a s e}, \\ \sum_ {j _ {1}} \dots \sum_ {j _ {n}} g (x _ {j _ {1}}, \dots , x _ {j _ {n}}) p (x _ {j _ {1}}, \dots , x _ {j _ {n}}), & \text {i n t h e d i s c r e t e c a s e}, \end{array} \right. \\ \end{array}
$$

and $g$ is any function such that $E | g ( \mathbf { X } ) | < \infty$ .

The random variables $X _ { 1 } , \ldots , X _ { n }$ are said to be independent if

$$
P \left[ X _ {1} \leq x _ {1}, \dots , X _ {n} \leq x _ {n} \right] = P \left[ X _ {1} \leq x _ {1} \right] \dots P \left[ X _ {n} \leq x _ {n} \right],
$$

i.e.,

$$
F (x _ {1}, \dots , x _ {n}) = F _ {X _ {1}} (x _ {1}) \dots F _ {X _ {n}} (x _ {n})
$$

for all real numbers $x _ { 1 } , \ldots , x _ { n }$ . In the continuous and discrete cases, independence is equivalent to the factorization of the joint density function or probability mass function into the product of the respective marginal densities or mass functions, i.e.,

$$
f \left(x _ {1}, \dots , x _ {n}\right) = f _ {X _ {1}} \left(x _ {1}\right) \dots f _ {X _ {n}} \left(x _ {n}\right) \tag {A.2.2}
$$

or

$$
p \left(x _ {1}, \dots , x _ {n}\right) = p _ {X _ {1}} \left(x _ {1}\right) \dots p _ {X _ {n}} \left(x _ {n}\right). \tag {A.2.3}
$$

For two random vectors $\mathbf { X } \ = \ ( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ and $\mathbf { Y } = ( Y _ { 1 } , \ldots , Y _ { m } ) ^ { \prime }$ with joint density function $f _ { \mathbf { X } , \mathbf { Y } }$ , the conditional density of $\mathbf { Y }$ given $\mathbf { X } = \mathbf { x }$ is

$$
f _ {\mathbf {Y} | \mathbf {X}} (\mathbf {y} | \mathbf {x}) = \left\{ \begin{array}{l l} \frac {f _ {\mathbf {X} , \mathbf {Y}} (\mathbf {x} , \mathbf {y})}{f _ {\mathbf {X}} (\mathbf {x})}, & \text {i f} f _ {\mathbf {X}} (\mathbf {x}) > 0, \\ f _ {\mathbf {Y}} (\mathbf {y}), & \text {i f} f _ {\mathbf {X}} (\mathbf {x}) = 0. \end{array} \right.
$$

The conditional expectation of $g \left( \mathbf { Y } \right)$ given $\mathbf { X } = \mathbf { x }$ is then

$$
E (g (\mathbf {Y}) | \mathbf {X} = \mathbf {x}) = \int_ {- \infty} ^ {\infty} g (\mathbf {y}) f _ {\mathbf {Y} | \mathbf {X}} (\mathbf {y} | \mathbf {x}) d \mathbf {y}.
$$

If $\mathbf { X }$ and $\mathbf { Y }$ are independent, then $f _ { \mathbf { Y | X } } ( \mathbf { y | x } ) = f _ { \mathbf { Y } } ( \mathbf { y } )$ by (A.2.2), and so the conditional expectation of $g ( \mathbf { Y } )$ given $\mathbf { X } = \mathbf { x }$ is

$$
E (g (\mathbf {Y}) | \mathbf {X} = \mathbf {x}) = E (g (\mathbf {Y})),
$$

which, as expected, does not depend on x. The same ideas hold in the discrete case with the probability mass function assuming the role of the density function.

# A.2.1 Means and Covariances

If $E | X _ { i } | \ < \ \infty$ for each i, then we define the mean or expected value of ${ \textbf { X } } =$ $( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ to be the column vector

$$
\boldsymbol {\mu} _ {X} = E \mathbf {X} = \left(E X _ {1}, \dots , E X _ {n}\right) ^ {\prime}.
$$

In the same way we define the expected value of any array whose elements are random variables (e.g., a matrix of random variables) to be the same array with each random variable replaced by its expected value (if the expectation exists).

If $\mathbf { X } = ( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ and $\mathbf { Y } = ( Y _ { 1 } , \ldots , Y _ { m } ) ^ { \prime }$ are random vectors such that each $X _ { i }$ and $Y _ { j }$ has a finite variance, then the covariance matrix of $\mathbf { X }$ and $\mathbf { Y }$ is defined to be the matrix

$$
\begin{array}{l} \Sigma_ {\mathbf {X Y}} = \mathrm {C o v} (\mathbf {X}, \mathbf {Y}) = E [ (\mathbf {X} - E \mathbf {X}) (\mathbf {Y} - E \mathbf {Y}) ^ {\prime} ] \\ = E \left(\mathbf {X} \mathbf {Y} ^ {\prime}\right) - (E \mathbf {X}) \left(E \mathbf {Y}\right) ^ {\prime}. \\ \end{array}
$$

The $( i , j )$ element of $\Sigma _ { \mathbf { X Y } }$ is the covariance $\operatorname { C o v } ( X _ { i } , Y _ { j } ) = E ( X _ { i } Y _ { j } ) - E ( X _ { i } ) E ( Y _ { j } )$ . In the special case where $\mathbf { Y } = \mathbf { X }$ , $\operatorname { C o v } ( \mathbf { X } , \mathbf { Y } )$ reduces to the covariance matrix of the random vector $\mathbf { X }$ .

Now suppose that $\mathbf { Y }$ and $\mathbf { X }$ are linearly related through the equation

$$
\mathbf {Y} = \mathbf {a} + B \mathbf {X},
$$

where a is an $m$ -dimensional column vector and $B$ is an $m \times n$ matrix. Then Y has mean

$$
E \mathbf {Y} = \mathbf {a} + B E \mathbf {X} \tag {A.2.4}
$$

and covariance matrix

$$
\Sigma_ {\mathbf {Y Y}} = B \Sigma_ {\mathbf {X X}} B ^ {\prime} \tag {A.2.5}
$$

(see Problem A.3).

Proposition A.2.1 The covariance matrix $\Sigma _ { \mathbf { X } \mathbf { X } }$ of a random vector $\mathbf { X }$ is symmetric and nonnegative definite, i.e., ${ \bf { b } } ^ { \prime } \Sigma _ { \bf { X X } } { \bf { b } } \geq 0$ for all vectors $\mathbf { b } = ( b _ { 1 } , \ldots , b _ { n } ) ^ { \prime }$ with real components.

Proof Since the $( i , j )$ element of $\Sigma _ { \mathbf { X } \mathbf { X } }$ is $\operatorname { C o v } ( X _ { i } , X _ { j } ) = \operatorname { C o v } ( X _ { j } , X _ { i } )$ , it is clear that $\Sigma _ { \mathbf { X } \mathbf { X } }$ is symmetric. To prove nonnegative definiteness, let $\mathbf { b } = ( b _ { 1 } , \ldots , b _ { n } ) ^ { \prime }$ be an arbitrary

vector. Then applying (A.2.5) with ${ \bf a } = { \bf 0 }$ and $B = \mathbf { b }$ , we have

$$
\mathbf {b} ^ {\prime} \Sigma_ {\mathbf {X X}} \mathbf {b} = \operatorname {V a r} (\mathbf {b} ^ {\prime} \mathbf {X}) = \operatorname {V a r} \left(b _ {1} X _ {1} + \dots + b _ {n} X _ {n}\right) \geq 0.
$$

Proposition A.2.2 Every $n \times n$ covariance matrix  can be factorized as

$$
\Sigma = P \Lambda P ^ {\prime}
$$

where $P$ is an orthogonal matrix (i.e., $P ^ { \prime } = P ^ { - 1 }$ ) whose columns are an orthonormal set of right eigenvectors corresponding to the (nonnegative) eigenvalues $\lambda _ { 1 } , \ldots , \lambda _ { n }$ of $\Sigma$ , and & is the diagonal matrix

$$
\Lambda = \left[ \begin{array}{c c c c} \lambda_ {1} & 0 & \dots & 0 \\ 0 & \lambda_ {2} & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda_ {n} \end{array} \right].
$$

In particular, $\Sigma$ is nonsingular if and only if all the eigenvalues are strictly positive.

Proof Every covariance matrix is symmetric and nonnegative definite by Proposition A.2.1, and for such matrices the specified factorization is a standard result (see Graybill 1983 for a proof). The determinant of an orthogonal matrix is 1 or $^ { - 1 }$ , so that $\operatorname* { d e t } ( \Sigma ) =$ $\operatorname* { d e t } ( P ) \operatorname* { d e t } ( \Lambda ) \operatorname* { d e t } ( P ) = \lambda _ { 1 } \cdot \cdot \cdot \lambda _ { n }$ . It follows that $\Sigma$ is nonsingular if and only if $\lambda _ { i } > 0$ for all i. 

Remark 1. Given a covariance matrix $\Sigma$ , it is sometimes useful to be able to find a square root $A = \Sigma ^ { 1 / 2 }$ with the property that $A A ^ { \prime } = \Sigma$ . It is clear from Proposition A.2.2 and the orthogonality of $P$ that one such matrix is given by

$$
A = \Sigma^ {1 / 2} = P \Lambda^ {1 / 2} P ^ {\prime}.
$$

If $\Sigma$ is nonsingular, then we can define

$$
\Sigma^ {s} = P \Lambda^ {s} P ^ {\prime}, \quad - \infty <   s <   \infty .
$$

The matrix $\Sigma ^ { - 1 / 2 }$ defined in this way is then a square root of $\Sigma ^ { - 1 }$ and also the inverse of $\Sigma ^ { 1 / 2 }$ .

# A.3 The Multivariate Normal Distribution

The multivariate normal distribution is one of the most commonly encountered and important distributions in statistics. It plays a key role in the modeling of time series data. Let $\mathbf { X } = ( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ be a random vector.

# Definition A.3.1

X has a multivariate normal distribution with mean $\pmb { \mu }$ and nonsingular covariance matrix $\boldsymbol { \Sigma } = \boldsymbol { \Sigma } _ { \mathbf { X } \mathbf { X } }$ , written as $\mathbf { X } \sim \mathrm { N } ( { \pmb { \mu } } , { \pmb { \Sigma } } )$ , if

$$
f _ {\mathbf {X}} (\mathbf {x}) = (2 \pi) ^ {- n / 2} (\det \Sigma) ^ {- 1 / 2} \exp \left\{- \frac {1}{2} (\mathbf {x} - \boldsymbol {\mu}) ^ {\prime} \Sigma^ {- 1} (\mathbf {x} - \boldsymbol {\mu}) \right\}.
$$

If $\mathbf { X } \sim \mathrm { N } ( { \pmb { \mu } } , { \pmb { \Sigma } } )$ , we can define a standardized random vector $\mathbf { Z }$ by applying the linear transformation

$$
\mathbf {Z} = \Sigma^ {- 1 / 2} (\mathbf {X} - \boldsymbol {\mu}), \tag {A.3.1}
$$

where $\Sigma ^ { - 1 / 2 }$ is defined as in the remark of Section A.2. Then by (A.2.4) and (A.2.5), Z has mean 0 and covariance matrix $\Sigma _ { \mathbf { Z } \mathbf { Z } } = \Sigma ^ { - 1 / 2 } \Sigma \Sigma ^ { - 1 / 2 } = I _ { n }$ , where $I _ { n }$ is the $n \times n$ identity matrix. Using the change of variables formula for probability densities (see Mood et al. 1974), we find that the probability density of $\mathbf { Z }$ is

$$
\begin{array}{l} f _ {\mathbf {Z}} (\mathbf {z}) = \left(\det \Sigma\right) ^ {1 / 2} f _ {\mathbf {X}} \left(\Sigma^ {1 / 2} \mathbf {z} + \boldsymbol {\mu}\right) \\ = (\det \Sigma) ^ {1 / 2} (2 \pi) ^ {- n / 2} (\det \Sigma) ^ {- 1 / 2} \exp \left\{- \frac {1}{2} (\Sigma^ {- 1 / 2} \mathbf {z}) ^ {\prime} \Sigma^ {- 1} \Sigma^ {- 1 / 2} \mathbf {z} \right\} \\ = (2 \pi) ^ {- n / 2} \exp \left\{- \frac {1}{2} {\bf z} ^ {\prime} {\bf z} \right\} \\ = \left((2 \pi) ^ {- 1 / 2} \exp \left\{- \frac {1}{2} z _ {1} ^ {2} \right\}\right) \dots \left((2 \pi) ^ {- 1 / 2} \exp \left\{- \frac {1}{2} z _ {n} ^ {2} \right\}\right), \\ \end{array}
$$

showing, by (A.2.2), that $Z _ { 1 } , \ldots , Z _ { n }$ are independent $\mathrm { \Delta N } ( 0 , 1 )$ random variables. Thus the standardized random vector $\mathbf { Z }$ defined by (A.3.1) has independent standard normal random components. Conversely, given any $n \times 1$ mean vector $\pmb { \mu }$ , a nonsingular $n \times n$ covariance matrix $\Sigma$ , and an $n \times 1$ vector of standard normal random variables, we can construct a normally distributed random vector with mean $\pmb { \mu }$ and covariance matrix $\Sigma$ by defining

$$
\mathbf {X} = \Sigma^ {1 / 2} \mathbf {Z} + \boldsymbol {\mu}. \tag {A.3.2}
$$

(See Problem A.4.)

Remark 1. The multivariate normal distribution with mean $\pmb { \mu }$ and covariance matrix $\Sigma$ can be defined, even when $\Sigma$ is singular, as the distribution of the vector X in (A.3.2). The singular multivariate normal distribution does not have a joint density, since the possible values of $\mathbf X - \pmb \mu$ are constrained to lie in a subspace of $\mathbb { R } ^ { n }$ with dimension equal to rank $( \Sigma )$ . -

Remark 2. If $\mathbf { X } \sim \mathrm { N } ( { \pmb { \mu } } , { \pmb { \Sigma } } ) , I$ is an $m \times n$ matrix, and a is a real $m \times 1$ vector, then the random vector

$$
\mathbf {Y} = \mathbf {a} + B \mathbf {X}
$$

is also multivariate normal (see Problem A.5). Note that from (A.2.4) and (A.2.5), Y has mean $\mathbf { a } + B \pmb { \mu }$ and covariance matrix $B \Sigma B ^ { \prime }$ . In particular, by taking $B$ to be the row vector $\mathbf { b } ^ { \prime } = ( b _ { 1 } , \ldots , b _ { n } )$ , we see that any linear combination of the components of a multivariate normal random vector is normal. Thus $\mathbf { b } ^ { \prime } \mathbf { X } = b _ { 1 } X _ { 1 } + \cdots + b _ { n } X _ { n } \sim$ $\mathrm { N } ( \mathbf { b } ^ { \prime } \pmb { \mu } _ { \mathrm { X } } , \mathbf { b } ^ { \prime } \pmb { \Sigma } _ { \mathbf { X } \mathbf { X } } \mathbf { b } )$ . -

# Example A.3.1. The Bivariate Normal Distribution

Suppose that $\mathbf { X } \ = \ ( X _ { 1 } , X _ { 2 } ) ^ { \prime }$ is a bivariate normal random vector with mean $\mu \mathbf { \Sigma } =$ $( \mu _ { 1 } , \mu _ { 2 } ) ^ { \prime }$ and covariance matrix

$$
\Sigma = \left[ \begin{array}{c c} \sigma_ {1} ^ {2} & \rho \sigma_ {1} \sigma_ {2} \\ \rho \sigma_ {1} \sigma_ {2} & \sigma_ {2} ^ {2} \end{array} \right], \quad \sigma > 0, \sigma_ {2} > 0, - 1 <   \rho <   1. \tag {A.3.3}
$$

The parameters $\sigma _ { 1 }$ , $\sigma _ { 2 }$ , and $\rho$ are the standard deviations and correlation of the components $X _ { 1 }$ and $X _ { 2 }$ . Every nonsingular 2-dimensional covariance matrix can be expressed in the form (A.3.3). The inverse of $\Sigma$ is

$$
\Sigma^ {- 1} = \left(1 - \rho^ {2}\right) ^ {- 1} \left[ \begin{array}{c c} \sigma_ {1} ^ {- 2} & - \rho \sigma_ {1} ^ {- 1} \sigma_ {2} ^ {- 1} \\ - \rho \sigma_ {1} ^ {- 1} \sigma_ {2} ^ {- 1} & \sigma_ {2} ^ {- 2} \end{array} \right],
$$

and so the pdf of $\mathbf { X }$ is given by

$$
\begin{array}{l} f _ {\mathbf {X}} (\mathbf {x}) = \left(2 \pi \sigma_ {1} \sigma_ {2} \left(1 - \rho^ {2}\right) ^ {1 / 2}\right) ^ {- 1} \\ \times \exp \left\{\frac {- 1}{2 (1 - \rho^ {2})} \left[ \left(\frac {x _ {1} - \mu_ {1}}{\sigma_ {1}}\right) ^ {2} \right. \right. \\ \left. - 2 \rho \left(\frac {x _ {1} - \mu_ {1}}{\sigma_ {1}}\right) \left(\frac {x _ {2} - \mu_ {2}}{\sigma_ {2}}\right) + \left(\frac {x _ {2} - \mu_ {2}}{\sigma_ {2}}\right) ^ {2} \right] \Bigg \}. \\ \end{array}
$$

![](images/3ae438b31dd30f0e7ec6e6ec78b42164822a4cb41a6c4b46e50ae8bd9260ce0d.jpg)

Multivariate normal random vectors have the important property that the conditional distribution of any set of components, given any other set, is again multivariate normal. In the following proposition we shall suppose that the nonsingular normal random vector X is partitioned into two subvectors

$$
\mathbf {X} = \left[ \begin{array}{c} \mathbf {X} ^ {(1)} \\ \mathbf {X} ^ {(2)} \end{array} \right].
$$

Correspondingly, we shall write the mean and covariance matrix of $\mathbf { X }$ as

$$
\boldsymbol {\mu} = \left[ \begin{array}{c} \boldsymbol {\mu} ^ {(1)} \\ \boldsymbol {\mu} ^ {(2)} \end{array} \right] \quad \text {a n d} \quad \boldsymbol {\Sigma} = \left[ \begin{array}{c c} \Sigma_ {1 1} & \Sigma_ {1 2} \\ \Sigma_ {2 1} & \Sigma_ {2 2} \end{array} \right],
$$

where $\pmb { \mu } ^ { ( i ) } = E \mathbf { X } ^ { ( i ) }$ and $\begin{array} { r } { \Sigma _ { i j } = E \left( \mathbf { X } ^ { ( i ) } - { \pmb \mu } ^ { ( i ) } \right) \left( \mathbf { X } ^ { ( j ) } - { \pmb \mu } ^ { ( i ) } \right) ^ { \prime } . } \end{array}$

Proposition A.3.1. i. $\mathbf { X } ^ { ( 1 ) }$ and $\mathbf { X } ^ { ( 2 ) }$ are independent if and only if $\Sigma _ { 1 2 } = 0$

ii. The conditional distribution of $\mathbf { X } ^ { ( 1 ) }$ given $\mathbf { X } ^ { ( 2 ) } = \mathbf { X } ^ { ( 2 ) }$ is $\mathrm { N } \big ( { \pmb \mu } ^ { ( 1 ) } + \Sigma _ { 1 2 } \Sigma _ { 2 2 } ^ { - 1 } \big ( \mathbf { x } ^ { ( 2 ) } -$ $\pmb { \mu } ^ { ( 2 ) } )$ , $\Sigma _ { 1 1 } - \Sigma _ { 1 2 } \Sigma _ { 2 2 } ^ { - 1 } \Sigma _ { 2 1 } \Big )$ . In particular,

$$
E \left(\mathbf {X} ^ {(1)} \mid \mathbf {X} ^ {(2)} = \mathbf {x} ^ {(2)}\right) = \boldsymbol {\mu} ^ {(1)} + \Sigma_ {1 2} \Sigma_ {2 2} ^ {- 1} \left(\mathbf {x} ^ {(2)} - \boldsymbol {\mu} ^ {(2)}\right).
$$

The proof of this proposition involves routine algebraic manipulations of the multivariate normal density function and is left as an exercise (see Problem A.6).

Example A.3.2. For the bivariate normal random vector $\mathbf { X }$ in Example A.3.1, we immediately deduce from Proposition A.3.1 that $X _ { 1 }$ and $X _ { 2 }$ are independent if and only if $\rho \sigma _ { 1 } \sigma _ { 2 } = 0$ (or $\rho = 0$ , since $\sigma _ { 1 }$ and $\sigma _ { 2 }$ are both positive). The conditional distribution of $X _ { 1 }$ given $X _ { 2 } = x _ { 2 }$ is normal with mean

$$
E (X _ {1} | X _ {2} = x _ {2}) = \mu_ {1} + \rho \sigma_ {1} \sigma_ {2} ^ {- 1} (x _ {2} - \mu_ {2})
$$

and variance

$$
\operatorname {V a r} \left(X _ {1} \mid X _ {2} = x _ {2}\right) = \sigma_ {1} ^ {2} \left(1 - \rho^ {2}\right).
$$

![](images/e53c421285d5316b822583529df842fe8e0615da10392a3161ea7c7ef4231279.jpg)

# Definition A.3.2.

$\{ X _ { t } \}$ is a Gaussian time series if all of its joint distributions are multivariate normal, i.e., if for any collection of integers $i _ { 1 } , \ldots , i _ { n }$ , the random vector $( X _ { i _ { 1 } } , \ldots , X _ { i _ { n } } ) ^ { \prime }$ has a multivariate normal distribution.

Remark 3. If $\{ X _ { t } \}$ is a Gaussian time series, then all of its joint distributions are completely determined by the mean function $\mu ( t ) \ = \ E X _ { t }$ and the autocovariance function $\kappa ( s , t ) ~ = ~ \mathrm { C o v } ( X _ { s } , X _ { t } )$ . If the process also happens to be stationary, then the mean function is constant $\mathbf { \nabla } _ { \mu _ { t } } = \mu _ { \mathbf { \nabla } } _ { \mathbf { \mu } }$ for all $t$ ) and $\kappa ( t + h , t ) ~ = ~ \gamma ( h )$ for all $t$ . In this case, the joint distribution of $X _ { 1 } , \ldots , X _ { n }$ is the same as that of $X _ { 1 + h } , \dots , X _ { n + h }$ for all integers $h$ and $n > 0$ . Hence for a Gaussian time series strict stationarity is equivalent to weak stationarity (see Section 2.1). -

# Problems

A.1 Let $X$ have a negative binomial distribution with parameters $\alpha$ and $p$ , where $\alpha > 0$ and $0 \leq p < 1$ .

a. Show that the probability generating function of $X$ defined as $M ( s ) = E { \bigl ( } s ^ { X } { \bigr ) } { \bigr ) }$ is

$$
M (s) = p ^ {\alpha} (1 - s + s p) ^ {- \alpha}, \quad 0 \leq s \leq 1.
$$

b. Using the property that $M ^ { \prime } ( 1 ) = E ( X )$ and $M ^ { \prime \prime } ( 1 ) = E ( X ^ { 2 } ) - E ( X )$ , show that

$$
E (X) = \alpha (1 - p) / p \quad \text {a n d} \quad \operatorname {V a r} (X) = \alpha (1 - p) / p ^ {2}.
$$

A.2 If X has the Poisson distribution with mean $\lambda$ , show that the variance of $X$ is also $\lambda$ .   
A.3 Use the linearity of the expectation operator for real-valued random variables to establish (A.2.4) and (A.2.5).   
A.4 If $\Sigma$ is an $n \times n$ covariance matrix, $\Sigma ^ { 1 / 2 }$ is the square root of $\Sigma$ defined in the remark of Section A.2, and $\mathbf { Z }$ is an $n$ -vector whose components are independent normal random variables with mean 0 and variance 1, show that

$$
X = \Sigma^ {1 / 2} \mathbf {Z} + \boldsymbol {\mu}
$$

is a normally distributed random vector with mean $\pmb { \mu }$ and covariance matrix $\Sigma$ .

A.5 Show that if $\mathbf { X }$ is an $n$ -dimensional random vector such that $\mathbf { X } \sim \mathrm { \ N } ( { \pmb { \mu } } , { \pmb { \Sigma } } ) , B$ is a real $m \times n$ matrix, and a is a real-valued $m$ -vector, then

$$
\mathbf {Y} = \mathbf {a} + B \mathbf {X}
$$

is a multivariate normal random vector. Specify the mean and covariance matrix of Y.

A.6 Prove Proposition A.3.1.

A.7 Suppose that $\bf { X } _ { \nu } = \Gamma ( X _ { 1 } , \dots , X _ { n } ) ^ { \prime } \sim \Gamma { \cal N } ( { \bf 0 } , \Sigma )$ with $\Sigma$ nonsingular. Using the fact that $\mathbf { Z }$ , as defined in (A.3.1), has independent standard normal components, show that $( \mathbf { X } - { \pmb { \mu } } ) ^ { \prime } { \Sigma } ^ { - 1 } ( \mathbf { X } - { \pmb { \mu } } )$ has the chi-squared distribution with $n$ degrees of freedom (Section A.1, Example (e)).

A.8 Suppose that $\mathrm { ~ \bf ~ X ~ } = \mathrm { ~ \bf ~ ( } X _ { 1 } , \mathrm { ~ \ldots ~ } , X _ { n } \mathrm { ) ^ { \prime } ~ } \sim \mathrm { ~ \bf ~ N } ( \pmb { \mu } , \Sigma )$ with $\Sigma$ nonsingular. If $A$ is a symmetric $n \times n$ matrix, show that $E ( \mathbf { X } ^ { \prime } A \mathbf { X } ) = \operatorname { t r a c e } ( A { \Sigma } ) + \mu ^ { \prime } { \Sigma } \pmb { \mu }$ .   
A.9 Suppose that $\{ X _ { t } \}$ is a stationary Gaussian time series with mean 0 and autocovariance function $\gamma ( h )$ . Find $E ( X _ { t } | X _ { s } )$ and $\mathrm { V a r } ( X _ { t } | X _ { s } )$ , $s \neq t$ .

# B

# Statistical Complements

B.1 Least Squares Estimation   
B.2 Maximum Likelihood Estimation   
B.3 Confidence Intervals   
B.4 Hypothesis Testing

# B.1 Least Squares Estimation

Consider the problem of finding the “best” straight line

$$
y = \theta_ {0} + \theta_ {1} x
$$

to approximate observations $y _ { 1 } , \ldots , y _ { n }$ of a dependent variable $y$ taken at fixed values $x _ { 1 } , \ldots , x _ { n }$ of the independent variable $x$ . The (ordinary) least squares estimates $\widehat { \theta } _ { 0 }$ , $\widehat { \theta } _ { 1 }$ are defined to be values of $\theta _ { 0 } , \theta _ { 1 }$ that minimize the sum

$$
S (\theta_ {0}, \theta_ {1}) = \sum_ {i = 1} ^ {n} (y _ {i} - \theta_ {0} - \theta_ {1} x _ {i}) ^ {2}
$$

of squared deviations of the observations $y _ { i }$ from the fitted values $\theta _ { 0 } + \theta _ { 1 } x _ { i }$ . (The “sum of squares” $S ( \theta _ { 0 } , \theta _ { 1 } )$ is identical to the Euclidean squared distance between $\mathbf { y }$ and $\theta _ { 0 } \mathbf { 1 } + \theta _ { 1 } \mathbf { x }$ , i.e.,

$$
S (\theta_ {0}, \theta_ {1}) = \| \mathbf {y} - \theta_ {0} \mathbf {1} - \theta_ {1} \mathbf {x} \| ^ {2},
$$

where $\mathbf { x } = ( x _ { 1 } , \ldots , x _ { n } ) ^ { \prime }$ , $\mathbf { 1 } = ( 1 , \ldots , 1 ) ^ { \prime }$ , and $\mathbf { y } = ( y _ { 1 } , \ldots , y _ { n } ) ^ { \prime }$ .) Setting the partial derivatives of $S$ with respect to $\theta _ { 0 }$ and $\theta _ { 1 }$ both equal to zero shows that the vector $\widehat { \pmb { \theta } } = ( \widehat { \theta } _ { 0 } , \widehat { \theta } _ { 1 } ) ^ { \prime }$ satisfies the “normal equations”

$$
X ^ {\prime} X \hat {\pmb {\theta}} = X ^ {\prime} \mathbf {y},
$$

where $X$ is the $n \times 2$ matrix $X = [ \mathbf { 1 } , \mathbf { x } ]$ . Since $0 \leq S ( \pmb \theta )$ and $S ( { \pmb \theta } )  \infty$ as $\| \pmb \theta \|  \infty$ , the normal equations have at least one solution. If ˆθ( $\hat { \pmb { \theta } } ^ { ( 1 ) }$ 1) and ˆθ (2) a $\hat { \pmb { \theta } } ^ { ( 2 ) }$ re two solutions of the normal equations, then a simple calculation shows that

$$
\left(\hat {\boldsymbol {\theta}} ^ {(1)} - \hat {\boldsymbol {\theta}} ^ {(2)}\right) ^ {\prime} X ^ {\prime} X \left(\hat {\boldsymbol {\theta}} ^ {(1)} - \hat {\boldsymbol {\theta}} ^ {(2)}\right) = 0,
$$

i.e., that X ˆθ (1) $\boldsymbol { X } \hat { \pmb { \theta } } ^ { ( 1 ) } = \boldsymbol { X } \hat { \pmb { \theta } } ^ { ( 2 ) }$ . The solution of the normal equations is unique if and only if the matrix $X ^ { \prime } X$ is nonsingular. But the preceding calculations show that even if $X ^ { \prime } X$ is singular, the vector $\hat { \mathbf { y } } = X \hat { \pmb { \theta } }$ of fitted values is the same for any solution $\hat { \pmb { \theta } }$ of the normal equations.

The argument just given applies equally well to least squares estimation for the general linear model. Given a set of data points

$$
\left(x _ {i 1}, x _ {i 2}, \dots , x _ {i m}, y _ {i}\right), \quad i = 1, \dots , n \text {w i t h} m \leq n,
$$

the least squares estimate, $\hat { \pmb { \theta } } = \left( \hat { \theta } _ { 1 } , \dots , \hat { \theta } _ { m } \right) ^ { \prime }$ of $\pmb \theta = ( \theta _ { 1 } , \dots , \theta _ { m } ) ^ { \prime }$ minimizes

$$
S (\pmb {\theta}) = \sum_ {i = 1} ^ {n} (y _ {i} - \theta_ {1} x _ {i 1} - \dots - \theta_ {m} x _ {i m}) ^ {2} = \left\| \mathbf {y} - \theta_ {1} \mathbf {x} ^ {(1)} - \dots - \theta_ {m} \mathbf {x} ^ {(m)} \right\| ^ {2},
$$

where $\mathbf { y } = ( y _ { 1 } , \ldots , y _ { n } ) ^ { \prime }$ and $\mathbf { x } ^ { ( j ) } = ( x _ { 1 j } , \dots , x _ { n j } ) ^ { \prime } , j = 1 , \dots , m$ . As in the previous special case, $\hat { \pmb { \theta } }$ satisfies the equations

$$
X ^ {\prime} X \hat {\boldsymbol {\theta}} = X ^ {\prime} \mathbf {y},
$$

where $X$ is the $n \times m$ matrix $X = \left[ \mathbf { x } ^ { ( 1 ) } , \ldots , \mathbf { x } ^ { ( m ) } \right]$ . The solution of this equation is unique if and only if $X ^ { \prime } X$ nonsingular, in which case

$$
\hat {\boldsymbol {\theta}} = (X ^ {\prime} X) ^ {- 1} X ^ {\prime} \mathbf {y}.
$$

If $X ^ { \prime } X$ is singular, there are infinitely many solutions $\hat { \pmb { \theta } }$ , but the vector of fitted values $X { \hat { \pmb { \theta } } }$ is the same for all of them.

Example B.1.1. To illustrate the general case, let us fit a quadratic function

$$
y = \theta_ {0} + \theta_ {1} x + \theta_ {2} x ^ {2}
$$

to the data

$$
\begin{array}{c c c c c c} \text {x} & 0 & 1 & 2 & 3 & 4 \\ \text {y} & 1 & 0 & 3 & 5 & 8 \end{array}
$$

The matrix $X$ for this problem is

$$
X = \left[ \begin{array}{c c c} 1 & 0 & 0 \\ 1 & 1 & 1 \\ 1 & 2 & 4 \\ 1 & 3 & 9 \\ 1 & 4 & 1 6 \end{array} \right], \text {g i v i n g} (X ^ {\prime} X) ^ {- 1} = \frac {1}{1 4 0} \left[ \begin{array}{c c c} 1 2 4 & - 1 0 8 & 2 0 \\ - 1 0 8 & 1 7 4 & - 4 0 \\ 2 0 & - 4 0 & 1 0 \end{array} \right].
$$

The least squares estimate $\hat { \pmb { \theta } } = \left( \hat { \theta } _ { 0 } , \hat { \theta } _ { 1 } , \hat { \theta } _ { 2 } \right) ^ { \prime }$ is therefore unique and given by

$$
\hat {\boldsymbol {\theta}} = (X ^ {\prime} X) ^ {- 1} X ^ {\prime} \mathbf {y} = \left[ \begin{array}{c} 0. 6 \\ - 0. 1 \\ 0. 5 \end{array} \right].
$$

The vector of fitted values is given by

$$
\hat {\mathbf {y}} = X \hat {\boldsymbol {\theta}} = (0. 6, 1, 2. 4, 4. 8, 8. 2) ^ {\prime}
$$

as compared with the observed values

$$
\mathbf {y} = (1, 0, 3, 5, 8) ^ {\prime}.
$$

![](images/f452fe62f0f380454201be08a558417be986cb1c47a2aeeeb906425066e33cae.jpg)

# B.1.1 The Gauss–Markov Theorem

Suppose now that the observations $y _ { 1 } , \ldots , y _ { n }$ are realized values of random variables $Y _ { 1 } , \dots , Y _ { n }$ $Y _ { n }$ satisfying

$$
Y _ {i} = \theta_ {1} x _ {i 1} + \dots + \theta_ {m} x _ {i m} + Z _ {i},
$$

where $Z _ { i } \sim \mathrm { \normalfont ~ W N } \left( 0 , \sigma ^ { 2 } \right)$ . Letting $\mathbf { Y } = ( Y _ { 1 } , \ldots , Y _ { n } ) ^ { \prime }$ and $\mathbf { Z } = ( Z _ { 1 } , \ldots , Z _ { n } ) ^ { \prime }$ , we can write these equations as

$$
\mathbf {Y} = X \boldsymbol {\theta} + \mathbf {Z}.
$$

Assume for simplicity that the matrix $X ^ { \prime } X$ is nonsingular (for the general case see, e.g., Silvey 1975). Then the least squares estimator of $\pmb \theta$ is, as above,

$$
\hat {\boldsymbol {\theta}} = (X ^ {\prime} X) ^ {- 1} X ^ {\prime} \mathbf {Y},
$$

and the least squares estimator of the parameter $\sigma ^ { 2 }$ is the unbiased estimator

$$
\hat {\sigma} ^ {2} = \frac {1}{n - m} \left\| \mathbf {Y} - X \hat {\boldsymbol {\theta}} \right\| ^ {2}.
$$

It is easy to see that $\hat { \pmb { \theta } }$ is also unbiased, i.e., that

$$
E (\hat {\boldsymbol {\theta}}) = \boldsymbol {\theta}.
$$

It follows at once that if $\mathbf { c } ^ { \prime } { \pmb \theta }$ is any linear combination of the parameters $\theta _ { i } , i \ =$ $1 , \ldots , m$ , then $\mathbf { c } ^ { \prime } { \hat { \pmb { \theta } } }$ is an unbiased estimator of $\mathbf { c } ^ { \prime } { \pmb \theta }$ . The Gauss–Markov theorem says that of all unbiased estimators of $\mathbf { c } ^ { \prime } { \pmb \theta }$ of the form $\sum _ { i = 1 } ^ { n } a _ { i } Y _ { i }$ , the estimator $\mathbf { c } ^ { \prime } { \hat { \pmb { \theta } } }$ has the smallest variance.

In the special case where $Z _ { 1 } , \ldots , Z _ { n }$ are IID $\mathsf { N } \big ( 0 , \sigma ^ { 2 } \big )$ , the least squares estimator $\hat { \pmb { \theta } }$ has the distribution $\mathrm { N } \big ( \pmb \theta , \sigma ^ { 2 } ( X ^ { \prime } X ) ^ { - 1 } \big )$ , and $( n - m ) \hat { \sigma } ^ { 2 } / \sigma ^ { 2 }$ has the $\chi ^ { 2 }$ distribution with $n - m$ degrees of freedom.

# B.1.2 Generalized Least Squares

The Gauss–Markov theorem depends on the assumption that the errors $Z _ { 1 } , \ldots , Z _ { n }$ are uncorrelated with constant variance. If, on the other hand, $\mathbf { Z } = ( Z _ { 1 } , \ldots , Z _ { n } ) ^ { \prime }$ has mean 0 and nonsingular covariance matrix $\sigma ^ { 2 } \Sigma$ where $\Sigma \neq I$ , we consider the transformed observation vector $U = R ^ { - 1 } { \mathbf { Y } }$ , where $R$ is a nonsingular matrix such that $R R ^ { \prime } = \Sigma$ . Then

$$
\mathbf {U} = R ^ {- 1} X \boldsymbol {\theta} + \mathbf {W} = M \boldsymbol {\theta} + \mathbf {W},
$$

where $M = R ^ { - 1 } X$ and W has mean 0 and covariance matrix $\sigma ^ { 2 } I$ . The Gauss–Markov theorem now implies that the best linear estimate of any linear combination $\mathbf { c } ^ { \prime } { \pmb \theta }$ is $\mathbf { c } ^ { \prime } { \hat { \pmb { \theta } } }$ , where $\hat { \pmb { \theta } }$ is the generalized least squares estimator, which minimizes

$$
\left\| \mathbf {U} - M \boldsymbol {\theta} \right\| ^ {2}.
$$

In the special case where $Z _ { 1 } , \ldots , Z _ { n }$ are uncorrelated and $Z _ { i }$ has mean 0 and variance $\sigma ^ { 2 } r _ { i } ^ { 2 }$ , the generalized least squares estimator minimizes the weighted sum of squares

$$
\sum_ {i = 1} ^ {n} \frac {1}{r _ {i} ^ {2}} (Y _ {i} - \theta_ {1} x _ {i 1} - \dots - \theta_ {m} x _ {i m}) ^ {2}.
$$

In the general case, if $X ^ { \prime } X$ and $\Sigma$ are both nonsingular, the generalized least squares estimator is given by

$$
\hat {\boldsymbol {\theta}} = (M ^ {\prime} M) ^ {- 1} M ^ {\prime} \mathbf {U}.
$$

Although the least squares estimator $( X ^ { \prime } X ) ^ { - 1 } X ^ { \prime } \mathbf { Y }$ is unbiased if $E ( \mathbf { Z } ) = \mathbf { 0 }$ , even when the covariance matrix of $\mathbf { Z }$ is not equal to $\sigma ^ { 2 } I$ , the variance of the corresponding estimate of any linear combination of $\theta _ { 1 } , \ldots , \theta _ { m }$ is greater than or equal to the estimator based on the generalized least squares estimator.

# B.2 Maximum Likelihood Estimation

The method of least squares has an appealing intuitive interpretation. Its application depends on knowledge only of the means and covariances of the observations. Maximum likelihood estimation depends on the assumption of a particular distributional form for the observations, known apart from the values of parameters $\theta _ { 1 } , \ldots , \theta _ { m }$ . We can regard the estimation problem as that of selecting the most appropriate value of a parameter vector $\pmb \theta$ , taking values in a subset $\Theta$ of $\mathbb { R } ^ { m }$ . We suppose that these distributions have probability densities $p ( \mathbf { x } ; \pmb { \theta } ) , \pmb { \theta } \in \Theta$ . For a fixed vector of observations $\mathbf { X }$ , the function $L ( \pmb \theta ) = p ( \mathbf x ; \pmb \theta )$ on $\Theta$ is called the likelihood function. A maximum likelihood estimate $\widehat { \pmb { \theta } } ( \mathbf { x } )$ of $\pmb \theta$ is a value of $\pmb \theta \in \Theta$ that maximizes the value of $L ( \pmb \theta )$ for the given observed value $\mathbf { X }$ , i.e.,

$$
L \big (\hat {\boldsymbol {\theta}} \big) = p \big (x; \hat {\boldsymbol {\theta}} (\mathbf {x}) \big) = \max  _ {\boldsymbol {\theta} \in \Theta} p (\mathbf {x}; \boldsymbol {\theta}).
$$

Example B.2.1. If $\mathbf { x } ~ = ~ ( x _ { 1 } , \ldots , x _ { n } ) ^ { \prime }$ is a vector of observations of independent $\mathrm { N } ( \mu , \sigma ^ { 2 } )$ random variables, the likelihood function is

$$
L \left(\mu , \sigma^ {2}\right) = \frac {1}{\left(2 \pi \sigma^ {2}\right) ^ {n / 2}} \exp \left[ - \frac {1}{2 \sigma^ {2}} \sum_ {i = 1} ^ {n} (x _ {i} - \mu) ^ {2} \right], \quad - \infty <   \mu <   \infty , \quad \sigma > 0.
$$

Maximization of $L$ with respect to $\mu$ and $\sigma$ is equivalent to minimization of

$$
- 2 \ln L (\mu , \sigma^ {2}) = n \ln (2 \pi) + 2 n \ln (\sigma) + \frac {1}{\sigma^ {2}} \sum_ {i = 1} ^ {n} (x _ {i} - \mu) ^ {2}.
$$

Setting the partial derivatives of $- 2 \ln L$ with respect to $\mu$ and $\sigma$ both equal to zero gives the maximum likelihood estimates

$$
\hat {\mu} = \overline {{x}} = \frac {1}{n} \sum_ {i = 1} ^ {n} x _ {i} \qquad \mathrm {a n d} \qquad \hat {\sigma} ^ {2} = \frac {1}{n} \sum_ {i = 1} ^ {n} (x _ {i} - \overline {{x}}) ^ {2}.
$$

-

# B.2.1 Properties of Maximum Likelihood Estimators

The Gauss–Markov theorem lent support to the use of least squares estimation by showing its property of minimum variance among unbiased linear estimators. Maximum likelihood estimators are not generally unbiased, but in particular cases they can be shown to have small mean squared error relative to other competing estimators. Their main justification, however, lies in their good large-sample behavior.

For independent and identically distributed observations with true probability density $p ( \cdot ; \pmb { \theta } _ { 0 } )$ satisfying certain regularity conditions, it can be shown that the maximum likelihood estimator $\hat { \pmb { \theta } }$ of $\pmb { \theta } _ { 0 }$ converges in probability to $\pmb { \theta } _ { 0 }$ and that the distribution of $\sqrt { n } (  { \hat { \theta } } - \pmb { \theta } _ { 0 } )$ is approximately normal with mean 0 and covariance matrix $I ( \pmb \theta _ { 0 } ) ^ { - 1 }$ , where $I ( \pmb \theta )$ is Fisher’s information matrix with $( i , j )$ component

$$
E _ {\boldsymbol {\theta}} \left[ \frac {\partial \ln p (X ; \boldsymbol {\theta})}{\partial \theta_ {i}} \frac {\partial \ln p (X ; \boldsymbol {\theta})}{\partial \theta_ {j}} \right].
$$

In time series analysis the situation is rather more complicated than in the case of iid observations. “Likelihood” in the time series context is almost always used in the sense of Gaussian likelihood, i.e., the likelihood computed under the (possibly false) assumption that the series is Gaussian. Nevertheless, estimators of ARMA coefficients computed by maximization of the Gaussian likelihood have good largesample properties analogous to those described in the preceding paragraph. For details see Brockwell and Davis (1991), Section 10.8.

# B.3 Confidence Intervals

Estimation of a parameter or parameter vector by least squares or maximum likelihood leads to a particular value, often referred to as a point estimate. It is clear that this will rarely be exactly equal to the true value, and so it is important to convey some idea of the probable accuracy of the estimator. This can be done using the notion of confidence interval, which specifies a random set covering the true parameter value with some specified (high) probability.

Example B.3.1. If $\mathbf { X } = ( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ is a vector of independent $\mathrm { N } \big ( \mu , \sigma ^ { 2 } \big )$ random variables, we saw in Section B.2 that the random variable $\begin{array} { r } { \overline { { X } } _ { n } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } X _ { i } } \end{array}$ is the maximum likelihood estimator of $\mu$ . This is a point estimator of $\mu$ =. To construct a confidence interval for $\mu$ from ${ \overline { { X } } } _ { n }$ , we observe that the random variable

$$
\frac {\overline {{X}} _ {n} - \mu}{S / \sqrt {n}}
$$

has Student’s $t$ -distribution with $n - 1$ degrees of freedom, where S is the sample standard deviation, i.e., $\begin{array} { r } { S ^ { 2 } = \frac { 1 } { n - 1 } \sum _ { i = 1 } ^ { n } \left( X _ { i } - \overline { { X } } _ { n } \right) ^ { 2 } } \end{array}$ . Hence,

$$
P \left[ - t _ {1 - \alpha / 2} <   \frac {\overline {{X}} _ {n} - \mu}{S / \sqrt {n}} <   t _ {1 - \alpha / 2} \right] = 1 - \alpha ,
$$

where $t _ { 1 - \alpha / 2 }$ denotes the $( 1 - \alpha / 2 )$ quantile of the $t$ -distribution with $n - 1$ degrees of freedom. This probability statement can be expressed in the form

$$
P \left[ \overline {{X}} _ {n} - t _ {1 - \alpha / 2} S / \sqrt {n} <   \mu <   \overline {{X}} _ {n} + t _ {1 - \alpha / 2} S / \sqrt {n} \right] = 1 - \alpha ,
$$

which shows that the random interval bounded by $\overline { { X } } _ { n } \pm t _ { 1 - \alpha / 2 } S / \sqrt { n }$ includes the true value $\mu$ with probability $1 - \alpha$ . This interval is called a $( 1 - \alpha )$ confidence interval for the mean $\mu$ .

![](images/374d45e66c23751a046496f0a9d13ad4b1de2395625aff9d742da7c70337f937.jpg)

# B.3.1 Large-Sample Confidence Regions

Many estimators of a vector-valued parameter $\pmb \theta$ are approximately normally distributed when the sample size $n$ is large. For example, under mild regularity conditions, the maximum likelihood estimator ${ \hat { \pmb { \theta } } } ( \mathbf { X } )$ of $\pmb { \theta } ~ = ~ ( \theta _ { 1 } , \ldots , \theta _ { m } ) ^ { \prime }$ is approximately $\mathrm { N } \big ( { \bf 0 } , { \frac { 1 } { n } } I ( \hat { \pmb { \theta } } ) ^ { - 1 } \big )$ , where $I ( \pmb \theta )$ is the Fisher information defined in Section B.2. Consequently,

$$
n \left(\hat {\boldsymbol {\theta}} - \boldsymbol {\theta}\right) ^ {\prime} I \left(\hat {\boldsymbol {\theta}}\right) \left(\hat {\boldsymbol {\theta}} - \boldsymbol {\theta}\right)
$$

is approximately distributed as $\chi ^ { 2 }$ with $m$ degrees of freedom, and the random set of $\pmb \theta$ -values defined by

$$
n \left(\boldsymbol {\theta} - \hat {\boldsymbol {\theta}}\right) ^ {\prime} I \left(\hat {\boldsymbol {\theta}}\right) \left(\boldsymbol {\theta} - \hat {\boldsymbol {\theta}}\right) \leq \chi_ {1 - \alpha} ^ {2} (m)
$$

covers the true value of $\pmb \theta$ with probability approximately equal to $1 - \alpha$

Example B.3.2. For iid observations $X _ { 1 } , \ldots , X _ { n }$ from $\mathrm { N } \big ( \mu , \sigma ^ { 2 } \big )$ , a straightforward calculation gives, for $\pmb \theta = \left( \mu , \sigma ^ { 2 } \right) ^ { \prime }$ ,

$$
I (\theta) = \left[ \begin{array}{c c} \sigma^ {- 2} & 0 \\ 0 & \sigma^ {- 4} / 2 \end{array} \right].
$$

Thus we obtain the large-sample confidence region for $\left( \mu , \sigma ^ { 2 } \right) ^ { \prime }$ ,

$$
n \left(\mu - \bar {X} _ {n}\right) ^ {2} / \hat {\sigma} ^ {2} + n \left(\sigma^ {2} - \hat {\sigma} ^ {2}\right) ^ {2} / \left(2 \hat {\sigma} ^ {4}\right) \leq \chi_ {1 - \alpha} ^ {2} (2),
$$

which covers the true value of $\pmb \theta$ with probability approximately equal to $1 - \alpha$ . This region is an ellipse centered at $\left( \overline { { X } } _ { n } , \hat { \sigma } ^ { \hat { 2 } } \right)$ .

![](images/347ca3b1e06291276368c410c9d59a72cf498ca7b0f632e102123c6b83bbe8fc.jpg)

# B.4 Hypothesis Testing

Parameter estimation can be regarded as choosing one from infinitely many possible decisions regarding the value of a parameter vector θ. Hypothesis testing, on the other hand, involves a choice between two alternative hypotheses, a “null” hypothesis $\mathrm { H } _ { 0 }$ and an “alternative” hypothesis $\mathrm { H } _ { 1 }$ , regarding the parameter vector $\pmb \theta$ . The hypotheses $\mathrm { H } _ { 0 }$ and $\mathrm { H } _ { 1 }$ correspond to subsets $\Theta _ { 0 }$ and $\Theta _ { 1 }$ of the parameter set $\Theta$ . The problem is to decide, on the basis of an observed data vector $\mathbf { X }$ , whether or not we should reject the null hypothesis $\mathrm { H } _ { 0 }$ . A statistical test of $\mathrm { H } _ { 0 }$ can therefore be regarded as a partition of the sample space into one set of values of X for which we reject $\mathrm { H } _ { 0 }$ and another for which we do not. The problem is to specify a test (i.e., a subset of the sample space called the “rejection region”) for which the corresponding decision rule performs well in practice.

Example B.4.1. If $\mathbf { X } = ( X _ { 1 } , \ldots , X _ { n } ) ^ { \prime }$ is a vector of independent $\mathrm { \Delta N } ( \mu , 1 )$ random variables, we may wish to test the null hypothesis $\mathrm { H } _ { 0 } \colon \mu \ = \ 0$ against the alternative $\mathrm { H } _ { 1 } \colon \mu \ \ne \ 0$ . A plausible choice of rejection region in this case is the set of all samples X for which $\left. \overline { { X } } _ { n } \right. > c$ for some suitably chosen constant $c$ . We shall return to this example after considering those factors that should be taken into account in the systematic selection of a “good” rejection region.

![](images/9fb8d98b039647918dcb418e967e96837bc5464d9db25ef959022d03a37ffec3.jpg)

# B.4.1 Error Probabilities

There are two types of error that may be incurred in the application of a statistical test:

• type I error is the rejection of $\mathrm { H } _ { 0 }$ when it is true.   
• type II error is the acceptance of $\mathrm { H } _ { 0 }$ when it is false.

For a given test (i.e., for a given rejection region $R$ ), the probabilities of error can both be found from the power function of the test, defined as

$$
P _ {\boldsymbol {\theta}} (R), \quad \boldsymbol {\theta} \in \Theta ,
$$

where $P _ { \pmb { \theta } }$ is the distribution of $\mathbf { X }$ when the true parameter value is $\pmb \theta$ . The probabilities of a type I error are

$$
\alpha (\boldsymbol {\theta}) = P _ {\boldsymbol {\theta}} (R), \quad \boldsymbol {\theta} \in \Theta_ {0},
$$

and the probabilities of a type II error are

$$
\beta (\boldsymbol {\theta}) = 1 - P _ {\boldsymbol {\theta}} (R), \quad \boldsymbol {\theta} \in \Theta_ {1}.
$$

It is not generally possible to find a test that simultaneously minimizes $\alpha ( \pmb \theta )$ and $\beta ( \pmb \theta )$ for all values of their arguments. Instead, therefore, we seek to limit the probability of type I error and then, subject to this constraint, to minimize the probability of type II error uniformly on $\Theta _ { 1 }$ . Given a significance level $\alpha$ , an optimum level- $\mathbf { \nabla } \cdot \alpha$ test is a test satisfying

$$
\alpha (\boldsymbol {\theta}) \leq \alpha , \quad \text {f o r a l l} \boldsymbol {\theta} \in \Theta_ {0},
$$

that minimizes $\beta ( \pmb \theta )$ for every $\pmb \theta \in \Theta _ { 1 }$ . Such a test is called a uniformly most powerful (U.M.P.) test of level $\pmb { \alpha }$ . The quantity $\operatorname { s u p } _ { \pmb { \theta } \in \Theta _ { 0 } } \alpha ( \pmb { \theta } )$ is called the size of the test.

In the special case of a simple hypothesis vs. a simple hypothesis, e.g., $\mathrm { H } _ { 0 } \colon \pmb { \theta } = \pmb { \theta } _ { 0 }$ vs. $\mathrm { H } _ { 1 }$ : $\pmb { \theta } \mathrm { = } \pmb { \theta } _ { 1 }$ , an optimal test based on the likelihood ratio statistic can be constructed (see Silvey 1975). Unfortunately, it is usually not possible to find a uniformly most powerful test of a simple hypothesis against a composite (more than one value of $\pmb \theta$ ) alternative. This problem can sometimes be solved by searching for uniformly most powerful tests within the smaller classes of unbiased or invariant tests. For further information see Lehmann (1986).

# B.4.2 Large-Sample Tests Based on Confidence Regions

There is a natural link between the testing of a simple hypothesis $\mathrm { H } _ { 0 } \colon \pmb { \theta } \ = \ \pmb { \theta } _ { 0 }$ vs. $\mathrm { H } _ { 1 } \colon \pmb { \theta } ~ \neq ~ \pmb { \theta } _ { 0 }$ and the construction of confidence regions. To illustrate this connection, suppose that $\hat { \pmb { \theta } }$ is an estimator of $\pmb \theta$ whose distribution is approximately $\mathrm { N } \big ( \pmb \theta , n ^ { - 1 } I ^ { - 1 } ( \pmb \theta ) \big )$ , where $I ( \pmb \theta )$ is a positive definite matrix. This is usually the case, for example, when $\hat { \pmb { \theta } }$ is a maximum likelihood estimator and $I ( \pmb \theta )$ is the Fisher information.

As in Section B.3.1, we have

$$
P _ {\boldsymbol {\theta}} \left(n \left(\boldsymbol {\theta} - \hat {\boldsymbol {\theta}}\right) ^ {\prime} I (\hat {\boldsymbol {\theta}}) \left(\boldsymbol {\theta} - \hat {\boldsymbol {\theta}}\right) \leq \chi_ {1 - \alpha} ^ {2} (m)\right) \approx 1 - \alpha .
$$

Consequently, an approximate $\alpha$ -level test is to reject $\mathrm { H } _ { 0 }$ if

$$
n \left(\boldsymbol {\theta} _ {0} - \hat {\boldsymbol {\theta}}\right) ^ {\prime} I \left(\hat {\boldsymbol {\theta}}\right) \left(\boldsymbol {\theta} _ {0} - \hat {\boldsymbol {\theta}}\right) > \chi_ {1 - \alpha} ^ {2} (m),
$$

or equivalently, if the confidence region determined by those $\pmb \theta$ ’s satisfying

$$
n \left(\boldsymbol {\theta} - \hat {\boldsymbol {\theta}}\right) ^ {\prime} I \left(\hat {\boldsymbol {\theta}}\right) \left(\boldsymbol {\theta} - \hat {\boldsymbol {\theta}}\right) \leq \chi_ {1 - \alpha} ^ {2} (m)
$$

does not include $\pmb { \theta } _ { 0 }$ .

Example B.4.2. Consider again the problem described in Example B.4.1. Since $\overline { { X } } _ { n } \ \sim \mathbf { N } \big ( \mu , n ^ { - 1 } \big )$ , the hypothesis $\mathrm { H } _ { 0 }$ : $\mu = 0$ is rejected at level $\alpha$ if

$$
n \left(\bar {X} _ {n}\right) ^ {2} > \chi_ {1 - \alpha , 1} ^ {2},
$$

or equivalently, if

$$
\left| \bar {X} _ {n} \right| > \frac {\Phi_ {1 - \alpha / 2}}{n ^ {1 / 2}}.
$$

-

# Mean Square Convergence

# C.1 The Cauchy Criterion

The sequence $S _ { n }$ of random variables is said to converge in mean square to the random variable $S$ if

$$
E \left(S _ {n} - S\right) ^ {2} \rightarrow 0 \text {a s} n \rightarrow \infty .
$$

In particular, we say that the sum $\scriptstyle \sum _ { k = 1 } ^ { n } X _ { k }$ converges (in mean square) if there exists a random variable S such that $\begin{array} { r } { E \bigl ( \sum _ { k = 1 } ^ { n } X _ { k } - S \bigr ) ^ { 2 } \to 0 } \end{array}$ as $n  \infty$ . If this is the case, then we use the notation $S = \textstyle \sum _ { k = 1 } ^ { \infty } X _ { k }$ 1 .

# C.1 The Cauchy Criterion

For a given sequence $S _ { n }$ of random variables to converge in mean square to some random variable, it is necessary and sufficient that

$$
E \left(S _ {m} - S _ {n}\right) ^ {2} \rightarrow 0 \text {a s} m, n \rightarrow \infty
$$

(for a proof of this see Brockwell and Davis (1991), Chapter 2). The point of the criterion is that it permits checking for mean square convergence without having to identify the limit of the sequence.

Example C.1.1. Consider the sequence of partial sums $\begin{array} { r } { S _ { n } = \sum _ { t = - n } ^ { n } a _ { t } Z _ { t } } \end{array}$ , $n = 1 , 2 , \ldots$ , where $\{ Z _ { t } \} \sim$ $\mathbf { W N } \left( 0 , \sigma ^ { 2 } \right)$ =−. Under what conditions on the coefficients $a _ { i }$ does this sequence converge in mean square? To answer this question we apply the Cauchy criterion as follows. For $n > m > 0$ ,

$$
E \left(S _ {n} - S _ {m}\right) ^ {2} = E \left(\sum_ {m <   | i | \leq n} a _ {i} Z _ {i}\right) ^ {2} = \sigma^ {2} \sum_ {m <   | i | \leq n} a _ {i} ^ {2}.
$$

Consequently, $E ( S _ { n } - S _ { m } ) ^ { 2 } \to 0$ if and only if $\textstyle \sum _ { m < | i | \leq n } a _ { i } ^ { 2 } \to 0$ . Since the Cauchy criterion applies also to real-valued sequences, this last condition is equivalent to convergence of the sequence $\scriptstyle \sum _ { i = - n } ^ { n } a _ { i } ^ { 2 }$ , or equivalently to the condition

$$
\sum_ {i = - \infty} ^ {\infty} a _ {i} ^ {2} <   \infty . \tag {C.1.1}
$$

# Properties of Mean Square Convergence:

If $X _ { n } \to X$ and $Y _ { n }  Y$ , in mean square as $n \to \infty$ , then

(a) $E \left( X _ { n } ^ { 2 } \right) \to E \left( X ^ { 2 } \right)$   
(b) $E ( X _ { n } ) \to E ( X ) ,$ ,

and

(c) $E ( X _ { n } Y _ { n } )  E ( X Y ) .$

Proof. See Brockwell and Davis (1991), Proposition 2.1.2.

# D D

# Lévy Processes, Brownian Motion and Itô Calculus

D.1 Lévy Processes   
D.2 Brownian Motion and the Itô Integral   
D.3 Itô Processes and Itô’s Formula   
D.4 Itô Stochastic Differential Equations

# D.1 Lévy Processes

Just as ARMA processes were defined as stationary solutions of stochastic difference equations driven by white noise, the so-called CARMA (continuous-time ARMA) models arise as stationary solutions of stochastic differential equations driven by Lévy processes. In order to discuss these equations in more detail we first present a few essential facts concerning Lévy processes. (For detailed accounts see Protter 2010; Applebaum 2004; Bertoin 1996; Sato 1999.) They have already been introduced in Definition 7.5.1, but for ease of reference we repeat the definition here.

# Definition D.1.1.

A Lévy process, $\{ L ( t ) , t \in \mathbb { R } \}$ is a process with the following properties:

(i) $L ( 0 ) = 0$   
(ii) $L ( t ) - L ( s )$ has the same distribution as $L ( t - s )$ for all $s$ and $t$ such that $s \leq t$   
(iii) If $( s , t )$ and $( u , \nu )$ are disjoint intervals then $L ( t ) - L ( s )$ and $L ( \nu ) - L ( u )$ are independent.   
(iv) $\{ L ( t ) \}$ is continuous in probability, i.e. for all $\epsilon > 0$ and for all $t \in \mathbb { R }$ ,

$$
\lim  _ {s \rightarrow t} P (| L (t) - L (s) | > \epsilon) = 0.
$$

It is known that every Lévy process has a version with sample-paths which are right continuous with left limits (càdlàg for short). We shall therefore assume that our Lévy processes have this property.

The characteristic function of L(t), $\phi _ { t } ( \theta ) : = E ( \exp ( i \theta L ( t ) ) )$ , has the celebrated Lévy-Khinchin representation, for $t \geq 0$ ,

$$
\phi_ {t} (\theta) = \exp (t \xi (\theta)), \theta \in \mathbb {R},
$$

where

$$
\xi (\theta) = \mathrm {i} \theta \mu - \frac {1}{2} \theta^ {2} \sigma^ {2} + \int_ {\mathbb {R}} (e ^ {\mathrm {i} \theta x} - 1 - \mathrm {i} \theta x I _ {(- 1, 1)} (x)) \nu (d x),
$$

for some $\mu \in \mathbb { R }$ , $\sigma \geq 0$ , and measure $\nu . I _ { ( - 1 , 1 ) }$ is the indicator function of the set $( - 1 , 1 )$ . The measure $\nu$ is known as the Lévy measure of the process $L$ and satisfies the conditions

$$
\nu (\{0 \}) = 0
$$

and

$$
\int_ {\mathbb {R}} \min  (1, | u | ^ {2}) v (d u) <   \infty .
$$

The triplet $( \sigma ^ { 2 } , \nu , \mu )$ is often referred to as the characteristic triplet of the Lévy process and completely determines all of its finite-dimensional distributions.

The measure $\nu$ characterizes the distribution of the jumps of the process. If, in particular, $\nu$ is the zero measure then the characteristic function of $L ( t )$ for $t \geq 0$ , is that of a normal random variable with $E ( L ( t ) ) = \mu t$ and $\operatorname { V a r } ( L ( t ) ) = \sigma ^ { 2 } t$ and the process $\{ L ( t ) , t \in \mathbb { R } \}$ is Brownian motion (Example 7.5.1) with sample-paths which are continuous (but nowhere differentiable).

If $\lambda : = \nu ( \mathbb { R } ) < \infty$ then the expected number of jumps in any time-interval of length $t$ is $\lambda t$ and the expected number of jumps with size in $( - \infty , x ]$ in the same time interval is $t \nu ( ( - \infty , x ] ) = \lambda t F ( x )$ where $F$ is a probability distribution function. The distribution function $F$ is known as the jump-size distribution and $\lambda$ is known as the mean jump-rate. If $\sigma ^ { 2 } = 0$ and $\begin{array} { r } { m = \lambda \int _ { ( - 1 , 1 ) } x d F ( x ) } \end{array}$ , then $\{ L ( t ) \}$ is a compound Poisson process with parameters $\lambda$ and $F$ −(Example 7.5.2) and with sample paths which are constant except for jumps.

If $\lambda = \infty$ then the expected number of jumps in every interval of positive length is infinite and the process $\{ L ( t ) \}$ is said to have infinite activity. The gamma process of Example 11.5.1 is such a process with characteristic triplet $( 0 , \nu , \alpha ( 1 - e ^ { - \beta } ) / \beta )$ , where $\nu$ is the measure defined on subsets of $( 0 , \infty )$ by,

$$
\nu (d x) = \alpha x ^ {- 1} e ^ {- \beta x} I _ {(0, \infty)} (x) d x.
$$

The Lévy-Khinchin representation of the characteristic function of $L ( t )$ shows that the distribution of $L ( t )$ can, by appropriate choice of the characteristic triplet, be any infinitely divisible distribution. This family includes a vast array of distributions such as the normal distributions, compound Poisson distributions, Student’s t-distributions, the stable distributions and many others. In particular it includes distributions which have heavy tails and which are not necessarily symmetric. These features allow for great flexibility when modelling observed phenomena in both financial and physical contexts.

In this appendix we shall restrict attention to Lévy processes for which $E L ( 1 ) ^ { 2 } < \infty$ . This constraint is not serious for most applications in finance where

it is generally believed that second moments exist while higher moments (those of order four or more) may not. For Lévy processes with $E L ( 1 ) ^ { 2 } < \infty$ it follows from the definition that there are finite constants $m$ and $s \geq 0$ such that

$$
E L (t) = m t \text {a n d} \operatorname {V a r} (L (t)) = s ^ {2} t \text {f o r a l l} t \geq 0.
$$

In the following sections we shall focus on Brownian motion and stochastic differential equations driven by Brownian motion.

In order to develop the necessary tools we introduce the Itô stochastic integral, Itô processes and Itô’s formula. Following this we shall outline some results concerning the solution of stochastic differential equations and use them to expand on the treatment of Gaussian CARMA processes and their Lévy-driven generalizations in Section 11.5.

# D.2 Brownian Motion and the Itô Integral

Robert Brown (1828) observed the erratic motion of pollen particles in a liquid which was later explained by the irregular bombardment of the particles by the molecules of the liquid. In order to provide a mathematical model for the one-dimensional version of this process, Einstein (1905) postulated the existence of a process satisfying conditions (i)–(iii) of Definition D.1.1 with $L ( t )$ normally distributed for every t. Bachelier (1900) had in fact already proposed such a model for the prices of stocks on the Paris stock exchange. It was later shown by Wiener that there is a process with continuous samplepaths satisfying these conditions, a process which has come to be known as a Brownian motion or Wiener process. It is in fact the only Lévy process with continuous samplepaths, a feature which adds to its plausibility as a model for the physical process originally observed by Brown. Although the sample-paths are continuous they are far from smooth in the sense that they are nowhere differentiable. We shall not attempt to prove these properties here but refer to the books of Mikosch (1998), Klebaner (2005) and Oksendal (2013) for further details. In the following sections we shall give an outline of the essentials of Itô calculus adapted from the more extensive treatment of Øksendal.

For modelling more complex physical phenomena it is often appropriate to suppose that the increment $d X ( t )$ of the observed process $\{ X ( t ) \}$ in the infinitesimally small time interval $( t , t + d t )$ satisfies an equation of the form

$$
d X (t) = b (t, X (t)) d t + \sigma (t, X (t)) d B (t), \quad S \leq t \leq T, \tag {D.2.1}
$$

where $d B ( t )$ denotes the increment of a standard Brownian motion in the same time interval. In order to attach a precise meaning to (D.2.1) we first consider the following discrete approximation. For any fixed positive integer $n$ , consider the grid of time points $\{ 2 ^ { - n } k , k \in \mathbb { Z } \}$ and define

$$
t _ {k} = \left\{ \begin{array}{l l} 2 ^ {- n} k, & \text {i f} S \leq 2 ^ {- n} k \leq T, \\ S, & \text {i f} 2 ^ {- n} k <   S, \\ T, & \text {i f} 2 ^ {- n} k > T. \end{array} \right. \tag {D.2.2}
$$

A discrete approximation to (D.2.1) is then

$$
X _ {j + 1} ^ {n} = X _ {j} ^ {n} + b \left(t _ {j}, X _ {j} ^ {n}\right) \Delta t _ {j} + \sigma \left(t _ {j}, X _ {j} ^ {n}\right) \Delta B _ {j}, [ 2 ^ {n} S ] \leq j \leq [ 2 ^ {n} T ], \tag {D.2.3}
$$

where $X _ { j } ^ { n } : = X ( t _ { j } )$ , $\Delta t _ { j } : = t _ { j + 1 } - t _ { j }$ , and $\Delta B _ { j } : = B ( t _ { j + 1 } ) - B ( t _ { j } )$ . For given functions $^ b$ and $\sigma$ and for any given initial condition, $X _ { [ 2 ^ { n } S ] } ^ { n } = X ( S )$ , and values of $B ( t _ { j } ) , j \le k$ , equation (D.2.3) can be solved recursively for $X _ { j } ^ { n } , j \leq k$ . The solution satisfies

$$
X _ {j + 1} ^ {n} = X (S) + \sum_ {k \leq j} b \left(t _ {k}, X _ {k} ^ {n}\right) \Delta t _ {k} + \sum_ {k \leq j} \sigma \left(t _ {k}, X _ {k} ^ {n}\right) \Delta B _ {k}, [ 2 ^ {n} S ] \leq j \leq [ 2 ^ {n} T ]. \tag {D.2.4}
$$

This suggests that, under suitable conditions, as $n  \infty$ , the random variables $X _ { j } ^ { n }$ $^ { \ i } , [ 2 ^ { n } S ] \le j \le [ 2 ^ { n } T ] + 1$ , approximate (in a sense to be specified) a random process $\{ \dot { X } ( t ) , S \le t \le T \}$ satisfying

$$
X (t) = X (S) + \int_ {S} ^ {t} b (u, X (u)) d u + \int_ {S} ^ {t} \sigma (u, X (u)) d B (u), S \leq t \leq T, \tag {D.2.5}
$$

In order to make sense of these statements, and to solve equations of the form (D.2.5) we must first define what is meant by the integrals on the right-hand side. We shall do this for non-anticipating integrands. The random process $\{ X ( t ) \}$ is said to be a non-anticipating function of $\{ B ( t ) \}$ if, for each t, $X ( t )$ is a function of $\{ B ( s ) , s \leq t \}$ . This property is the continuous-time analogue of causality, which we introduced in connection with ARMA processes in Chapter 3. We shall use the notation $\mathcal { F } _ { t }$ to denote the class of random variables on $( \Omega , { \mathcal { F } } , P )$ (the probability space on which $\{ B ( t ) \}$ is defined) which are functions of $\{ B ( s ) , s \ \leq \ t \}$ . In this terminology $\{ X ( t ) \}$ is a nonanticipating function of $\{ B ( t ) \}$ if $X ( t ) \in \mathcal { F } _ { t }$ for all $t$ .

To deal with the first integral in (D.2.5) we consider integrals of the form

$$
\int_ {S} ^ {T} m (u) d u, S <   T, \tag {D.2.6}
$$

for functions $m$ on $\mathbb { R } \times \Omega$ belonging to the family ${ \mathcal { M } } ( S , T )$ defined by the properties (i)–(iii) below. For clarity we have suppressed the dependence on $\omega \in \Omega$ in (D.2.5) and (D.2.6), but in fact $X$ and $m$ are both functions on $\mathbb { R } \times \Omega$ with values $X ( u , \omega )$ and $m ( u , \omega )$ respectively.

# Defining properties of $m \in { \mathcal { M } } ( S , T )$ :

(i) $m ( \cdot , \cdot )$ is a measurable function on $\mathbb { R } \times \Omega$ .   
(ii) $m ( t , \cdot ) \in \mathscr { F } _ { t }$ for each $t \in \mathbb { R }$ .   
(iii) $\begin{array} { r } { P \left[ \int _ { S } ^ { T } | m ( u , \omega ) | d u < \infty \right] = 1 . } \end{array}$

For $m \ \in \ { \mathcal { M } } ( S , T )$ the integrals $\begin{array} { r } { \int _ { S } ^ { t } m ( u ) d u , t \ \in \ [ S , T ] } \end{array}$ , can be defined for all $\omega$ outside a set of probability zero as straightforward Lebesgue integrals, continuous in t. Specifying them to be zero on the exceptional subset of $\Omega$ defines $\begin{array} { r } { \int _ { S } ^ { t } m ( u ) d u , t \in } \end{array}$ $[ S , T ]$ , as a continuous function of $t$ for each $\omega$ .

In order to attach a meaning to the second integral in (D.2.5) we need to define integrals of the form

$$
\int_ {S} ^ {T} f (u) d B (u), S <   T, \tag {D.2.7}
$$

where the random variables $f ( u )$ , defined on the same probability space $( \Omega , { \mathcal { F } } , P )$ as $\{ B ( t ) \}$ , satisfy the properties (i)–(iii) specified below. We shall denote the class of such functions as ${ \mathcal { N } } ( S , T )$ and an integral of the form (D.2.7) as an Itô integral.

# Defining properties of $f \in { \mathcal { N } } ( S , T )$ :

(i) $f ( \cdot , \cdot )$ is a measurable function on $\mathbb { R } \times \Omega$ .   
(ii) $f ( t , \cdot ) \in \mathcal { F } _ { t }$ for each $t \in \mathbb { R }$ .   
(iii) $\begin{array} { r } { E \left[ \int _ { S } ^ { T } f ( t , \omega ) ^ { 2 } d t \right] < \infty . } \end{array}$

The construction of the integral (D.2.7) is achieved by defining it for elementary functions and then extending the definition to all functions $f \in { \mathcal { N } } ( S , T )$ . The function $e$ is an elementary function if for some positive integer $n$ ,

$$
e (u, \omega) = \sum_ {j = - \infty} ^ {\infty} e _ {j} (\omega) I _ {(2 ^ {- n} j, 2 ^ {- n} (j + 1) ]} (u), u \in \mathbb {R}, \omega \in \Omega , \tag {D.2.8}
$$

where the random variables $e _ { j }$ belong to $\mathcal { F } _ { t _ { j } }$ for all $j$ and the times $t _ { j }$ are defined as in (D.2.2). Since the function $e ( u , \omega )$ is independent of $u$ on the interval $( 2 ^ { - n } j , 2 ^ { - n } ( j + 1 ) ]$ , and since $B$ increases on that interval by $\Delta B _ { j } : = B ( t _ { j + 1 } ) - B ( t _ { j } )$ , it is natural to define (suppressing $\omega$ as in (D.2.7)),

$$
I _ {S, T} (e) = \int_ {S} ^ {T} e (u) d B (u) := \sum_ {j = - \infty} ^ {\infty} e _ {j} \Delta B _ {j}, S <   T. \tag {D.2.9}
$$

Proposition D.2.1. If e is bounded and elementary then

$$
E \left(\int_ {S} ^ {T} e (u) d B (u)\right) ^ {2} = E \left(\int_ {S} ^ {T} e (u) ^ {2} d u\right), \quad S <   T. \tag {D.2.10}
$$

Proof. Observing that $E ( e _ { i } e _ { j } \Delta B _ { i } \Delta B _ { j } ) = \delta _ { i j } E ( e _ { j } ^ { 2 } ) \Delta t _ { j }$ , where $\delta _ { i j } = 1$ if $i = j$ and 0 otherwise, we can rewrite the left-hand side of (D.2.10) as

$$
E \sum_ {i = - \infty} ^ {\infty} \sum_ {j = - \infty} ^ {\infty} (e _ {i} e _ {j} \Delta B _ {i} \Delta B _ {j}) = \sum_ {j = - \infty} ^ {\infty} E (e _ {j} ^ {2}) \Delta t _ {j} = E \sum_ {j = - \infty} ^ {\infty} e _ {j} ^ {2} \Delta t _ {j} = E \int_ {S} ^ {T} e (t) ^ {2} d t.
$$

Remark 1. The left-hand side of (D.2.10) is the squared norm of the random variable $I _ { S , T } ( e )$ defined on $( \Omega , { \mathcal { F } } , P )$ . The right-hand side is the squared norm of the function

$$
e ^ {*} (u, \omega) := \left\{ \begin{array}{l l} e (u, \omega), & \text {i f} (u, \omega) \in [ S, T ] \times \Omega , \\ 0, & \text {o t h e r w i s e}, \end{array} \right.
$$

a square integrable function on the product space $[ S , T ] \times \Omega$ with respect to the product measure $\ell \times P$ , where $\ell$ denotes Lebesgue measure. The mapping $e \mapsto I _ { S , T } ( e )$ thus determines an isometry from the restrictions $e ^ { * }$ of the bounded elementary functions $e$ to $[ S , T ] \times \Omega$ into the space of square integrable random variables on $( \Omega , { \mathcal { F } } , P )$ .

It can be shown (see e.g., Oksendal 2013) that for every function $f \in { \mathcal { N } } ( S , T )$ there is a sequence of bounded elementary functions $\{ e _ { n } \}$ such that

$$
E \int_ {S} ^ {T} \left(e _ {n} (u) - f (u)\right) ^ {2} d u \rightarrow 0 \text {a s} n \rightarrow \infty . \tag {D.2.11}
$$

This implies that $\begin{array} { r } { E \int _ { S } ^ { T } ( e _ { n } ( u ) - e _ { m } ( u ) ) ^ { 2 } d u  0 } \end{array}$ as $m$ and $n$ both go to $\infty$ and, by the isometry, that

$$
E (I _ {S, T} (e _ {n} - e _ {m})) ^ {2} = E \left(I _ {S, T} (e _ {n}) - I _ {S, T} (e _ {m})\right) ^ {2} \to 0.
$$

By the Cauchy property of mean square convergence (Appendix C.1) it follows that $\{ I _ { S , T } ( e _ { n } ) \}$ has a mean square limit.

If $\left\{ g _ { n } \right\}$ is another sequence of bounded elementary functions with the property (D.2.11) then $\begin{array} { r } { E \int _ { S } ^ { T } ( e _ { n } ( u ) ^ { - } g _ { n } ( u ) ) ^ { 2 } d u  0 } \end{array}$ as $n \to \infty$ so that

$$
E \left(I _ {S, T} \left(\left(e _ {n} - g _ {n}\right)\right) ^ {2} = E \left(I _ {S, T} \left(e _ {n}\right) - I _ {S, T} \left(g _ {n}\right)\right) ^ {2} \rightarrow 0.
$$

Hence the mean square limit of $I _ { S , T } ( e _ { n } )$ is the same for all sequences of bounded elementary functions satisfying (D.2.11) and the common limit is defined to be $I _ { S , T } ( f )$ . Thus $I _ { S , T } ( f )$ can be defined unambiguously as

$$
I _ {S, T} (f) := \lim  _ {m. s.} I _ {S, T} \left(e _ {n}\right), \tag {D.2.12}
$$

where $\{ e _ { n } \}$ is any sequence of bounded elementary functions satisfying (D.2.11).

Moreover if $f \in { \mathcal { N } } ( S , T )$ and $\{ e _ { n } \}$ satisfies (D.2.11), then

$$
E \left(I _ {S, T} (f) ^ {2}\right) = \lim  _ {n \rightarrow \infty} E \left(I _ {S, T} \left(e _ {n}\right) ^ {2}\right) = \lim  _ {n \rightarrow \infty} E \int_ {S} ^ {T} e _ {n} ^ {2} (u) d u = E \int_ {S} ^ {T} f ^ {2} (u) d u,
$$

showing that the isometry of the restrictions $e ^ { * }$ of bounded elementary functions extends to the corresponding restrictions $f ^ { * }$ of all functions in ${ \mathcal { N } } ( S , T )$ .

This means that, in principle, $I _ { S , T } ( f )$ can be evaluated as the mean-square limit of $\textstyle \int _ { S } ^ { T } x _ { n } ( u ) d B ( u )$ where $\{ x _ { n } \}$ is any (not necessarily bounded) sequence of elementary functions such that $\begin{array} { r } { E \int _ { S } ^ { T } ( x _ { n } ( u ) - f ( u ) ) ^ { 2 } d u  0 } \end{array}$ as $n  \infty$ . In particular it can be shown in this way that

$$
\int_ {S} ^ {T} B (u) d B (u) = \frac {1}{2} (B ^ {2} (T) - B ^ {2} (S)) - \frac {1}{2} (T - S).
$$

We shall not go into the details as we shall derive this result in a much simpler way using the tools of Itô calculus to be discussed in the following section. -

Remark 2. If $\boldsymbol { f } ~ \in \mathrm { \mathcal { N } } ( S , T )$ then for each $t ~ \in ~ [ S , T ]$ so also is the function, $\{ f ( \omega , u ) \mathbf { 1 } _ { [ S , t ] } ( u )$ , ω ∈ , u ∈ R}, where $\mathbf { 1 } _ { [ S , t ] }$ is the indicator function of the set $[ S , t ]$ . This enables us to define

$$
\int_ {S} ^ {t} f (u) d B (u) := \int_ {S} ^ {T} f (u) \mathbf {1} _ {[ S, t ]} (u) d u
$$

for each $t \in [ S , T ]$ and each $f \in { \mathcal { N } } ( S , T )$ .

Remark 3. If $\cdot f \in \mathcal { N } : = \cap \mathcal { N } ( S , T )$ , where $\cap$ denotes the intersection over all $S \in \mathbb R$ and $T \in \mathbb { R }$ such that $S \leq T$ , then $I _ { s , t } ( f )$ is defined for all real-valued $s$ and $t$ such that $s \leq t$ and the integral has the properties,

(i) $E I _ { s , t } ( f ) = 0$   
(ii) $I _ { s , u } ( f ) = I _ { s , t } ( f ) + I _ { t , u } ( f )$ , $s \leq t \leq u$   
(iii) $I _ { s , t } ( a f + b g ) = a I _ { s , t } ( f ) + b I _ { s , t } ( g )$ for all $a , b \in \mathbb { R }$ and $g \in \mathcal { N }$   
(iv) $\begin{array} { r } { E \left[ I _ { s , t } ( f ) I _ { s , t } ( g ) \right] = E \int _ { s } ^ { t } f ( u ) g ( u ) d u } \end{array}$ for all $g \in \mathcal { N }$

(v) For each fixed $s \in \mathbb { R }$ , $\{ I _ { s , t } ( f ) , t \geq s \}$ is an $\mathcal { F } _ { t }$ -martingale, i.e. $E | I _ { s , t } ( f ) | < \infty$ and

$$
E (I _ {s, u} (f) | B (y), y \leq t) = I _ {s, t} (f), u \geq t \geq s.
$$

(vi) For each fixed $s \in \mathbb { R }$ and for each fixed $T \geq s$ there is a version of $\{ I _ { s , t } ( f ) , s \le$ $t \leq T \}$ which is continuous in t. In other words there is a process $\{ X _ { t } , s \leq t \leq T \}$ with continuous sample-paths such that

$$
P (X _ {t} = \int_ {s} ^ {t} f (u) d B (u)) = 1 \mathrm {f o r a l l} t \in [ s, T ].
$$

Properties (i)–(iv) are clearly true for bounded elementary functions $f$ and $g$ . Their validity for functions in $\mathcal { N }$ can be established by taking limits. Property (v) follows from (ii) and the independence of the increments of $\{ B ( t ) \}$ . The proof of property (vi) is beyond the scope of this book [see, e.g., Oksendal (2013) for details]. -

# D.3 Itô Processes and Itô’s Formula

Direct evaluation of Itô stochastic integrals from the definition (D.2.12) is very messy. For example, it can be shown by a lengthy calculation from the definition that

$$
\int_ {0} ^ {t} B (u) d B (u) = \frac {1}{2} B (t) ^ {2} - \frac {1}{2} t.
$$

Itô’s formula provides a chain rule for evaluating such integrals. It is clear from this example that the classic rule for Riemann integration does not apply. If, for example, we apply it in this particular case we find, from the rule $d ( x ^ { 2 } ) = 2 x d x$ , that the integral is ${ \scriptstyle { \frac { 1 } { 2 } } } B ( t ) ^ { 2 }$ instead of the correct expression above. Before we can derive the appropriate rule however we first need to define what is meant by an Itô process.

# Itô Process

This is a process which satisfies (suppressing the argument $\omega$ as before)

$$
X (t) = X (s) + \int_ {s} ^ {t} m (u) d u + \int_ {s} ^ {t} f (u) d B (u), s \leq t \in \mathbb {R}, \tag {D.3.1}
$$

where

$$
X (t) \in \mathcal {F} _ {t} \text {f o r a l l} t \in \mathbb {R}, \tag {D.3.2}
$$

$$
m \in \mathcal {M} (S, T) \text {f o r a l l} S \leq T \in \mathbb {R} \tag {D.3.3}
$$

and

$$
f \in \mathcal {N} ^ {*} (S, T) \text {f o r a l l} S \leq T \in \mathbb {R}, \tag {D.3.4}
$$

with ${ \mathcal { M } } ( S , T )$ defined as in Section E.2 and $\mathcal { N } ^ { * } ( S , T )$ defined like ${ \mathcal { N } } ( S , T )$ in Section E.2 except for the replacement of property (iii), $E \left[ \int _ { S } ^ { T } f ( u ) ^ { 2 } d u \right] < \infty$ , by the weaker condition,

$$
\left(\text {i i i}\right) ^ {*} P \left[ \int_ {S} ^ {T} f (u) ^ {2} d u <   \infty \right] = 1.
$$

It can be shown that, under this weaker condition, the integrals $I _ { s , t } ( f )$ , $s \leq t \in \mathbb { R }$ , can still be defined, retaining all of the properties in Remark 3 of Section E.2 with the exception of the martingale property (v).

Definition (D.3.1) is often written in the shorthand notation,

$$
d X (t) = m (t) d t + f (t) d B (t). \tag {D.3.5}
$$

Both of the integrals in (D.3.1) are assumed to be continuous versions so that the Itô process $\{ X ( t ) \}$ is also continuous. The first integral is usually referred to as the drift component of $\{ X ( t ) \}$ and the second as the Brownian component.

# Itô’s Formula

Itô’s formula is concerned with smooth functions of Itô processes. Specifically it states that if $\{ X ( t ) \}$ is an Itô process satisfying (D.3.5) and $\{ g ( t , x ) \}$ is a function on $\mathbb { R } \times \mathbb { R }$ with continuous partial derivatives $\partial g / \partial t$ and $\partial ^ { 2 } g / \partial x ^ { 2 }$ then

(i) $Y ( t ) : = g ( t , X ( t ) )$ is an Itô process and

(ii)

$$
d Y (t) = \frac {\partial g}{\partial t} (t, X (t)) d t + \frac {\partial g}{\partial x} (t, X (t)) d X (t) + \frac {1}{2} \frac {\partial^ {2} g}{\partial x ^ {2}} (t, X (t)) (d X (t)) ^ {2}, \tag {D.3.6}
$$

where $d X ( t ) = m d t + f d B ( t )$ and $( d X ( t ) ) ^ { 2 } = f ^ { 2 } d t$ .

Writing $g _ { t } , g _ { x }$ and $g _ { x x }$ for the corresponding partial derivatives of $g$ evaluated at $( t , X ( t ) )$ , and substituting for $d X ( t )$ and $d X ( t ) ^ { 2 }$ as indicated in (ii), we can write the increment of $Y ( t )$ explicitly in the form (D.3.5) as

$$
d Y (t) = \left(g _ {t} + m g _ {x} + \frac {1}{2} v ^ {2} g _ {x x}\right) d t + f g _ {x} d B (t). \tag {D.3.7}
$$

Example D.3.1. $\textstyle \int _ { 0 } ^ { t } B ( u ) d B ( u )$

Inspection of (D.3.7) suggests that in order to find a process with increments $B ( u ) d B ( u )$ we should start with the Itô process $X ( t ) = B ( t )$ , for which $m = 0$ and $f = 1$ , and define $Y ( t ) = g ( t , X ( t ) )$ where $g _ { x } ( t , x ) = x$ . Taking $g ( t , x ) = x ^ { 2 } / 2$ we obtain, from (D.3.7),

$$
d Y (t) = \frac {1}{2} d t + B (t) d B (t),
$$

which gives

$$
\int_ {0} ^ {t} B (u) d B (u) = Y (t) - Y (0) - \frac {1}{2} t = \frac {1}{2} B (t) ^ {2} - \frac {1}{2} t.
$$

# Multivariate Itô Processes

An $n$ -dimensional Itô process $\{ { \bf X } ( t ) \}$ is defined to be an $n$ -dimensional vector-valued process satisfying an equation (cf. (D.3.5),

$$
d \mathbf {X} (t) = \mathbf {m} (t) d t + F (t) d \mathbf {B} (t), \tag {D.3.8}
$$

where $\{ { \bf B } ( t ) \}$ is $m$ -dimensional standard Brownian motion, i.e. an $m$ -dimensional random process with components which are independent one-dimensional standard Brownian motions, the components of the $n$ -vectors $\mathbf { X } ( t )$ and $\mathbf { m } ( t )$ satisfy (D.3.2) and (D.3.3) respectively, and each component $f _ { i j }$ of the $n \times m$ matrix $F ( t )$ satisfies (D.3.4). The more explicit form of (D.3.8), corresponding to (D.3.1), is

$$
\mathbf {X} (t) = \mathbf {X} (s) + \int_ {s} ^ {t} \mathbf {m} (u) d u + \int_ {s} ^ {t} F (u) d \mathbf {B} (u), s \leq t \in \mathbb {R}. \tag {D.3.9}
$$

# The Multidimensional Itô Formula

The multidimensional version of Itô’s formula states that if $\{ { \bf X } ( t ) \}$ is an $n$ -dimensional Itô process satisfying (D.3.9) and $\{ g ( t , { \bf x } ) \}$ is a function on $\mathbb { R } \times \mathbb { R } ^ { n }$ with values in $\mathbb { R } ^ { p }$ and with continuous second partial derivatives, then

(i) $\{ \mathbf { Y } ( t ) : = g ( t , \mathbf { X } ( t ) )$ is a $p$ -dimensional Itô process and (ii)

$$
d Y _ {i} (t) = \frac {\partial g _ {i}}{\partial t} d t + \sum_ {j = 1} ^ {n} \frac {\partial g _ {i}}{\partial x _ {j}} d X _ {j} (t) + \frac {1}{2} \sum_ {j = 1} ^ {n} \sum_ {k = 1} ^ {n} \frac {\partial^ {2} g _ {i}}{\partial x _ {j} \partial x _ {k}} d X _ {j} (t) d X _ {k} (t), \tag {D.3.10}
$$

where $X _ { i }$ , $Y _ { i }$ and $g _ { i }$ are the components of X, Y and $g$ respectively, and the partial derivatives of $g$ are all evaluated at $( t , \mathbf { X } ( t ) )$ . The increments $d X _ { j }$ satisfy the relations $\begin{array} { r } { d X _ { j } ( t ) = m _ { j } d t + \sum _ { r = 1 } ^ { n } f _ { j r } d B _ { r } ( t ) } \end{array}$ and $\begin{array} { r } { d X _ { j } ( t ) d X _ { k } ( t ) = \sum _ { r = 1 } ^ { n } f _ { j r } f _ { k r } d t } \end{array}$ , where $m _ { j }$ and $f _ { i j }$ are the components of $\mathbf { m } ( t )$ and $F ( t )$ respectively.

In the following section we shall consider solutions of stochastic differential equations of the form

$$
d \mathbf {X} (t) = b (t, \mathbf {X} (t)) d t + \sigma (t, \mathbf {X} (t)) d \mathbf {B} (t), S <   t <   T, \mathbf {X} _ {S} = Z, \tag {D.3.11}
$$

where $\{ { \bf B } ( t ) \}$ is $m$ -dimensional standard Brownian motion. Conditions on the functions $^ b$ and $\sigma$ and the initial random variable $Z$ which guarantee existence and uniqueness of solutions will be specified in Theorem D.4.1, a proof of which can be found in Oksendal (2013).

# D.4 Itô Stochastic Differential Equations

The equation (D.3.11) is known as an Itô stochastic differential equation for the $\mathbb { R } ^ { n }$ - valued random process $\{ { \bf X } ( t ) \}$ . Equations (7.5.6), for geometric Brownian motion, (11.5.2), for the CAR(1) process, and (11.5.9), for the state vector of a CARMA process, are special cases. It is trivial to check, in each of these cases, that the conditions on $^ b$ and $\sigma$ given in the following theorem are satisfied for all S and $T \in \mathbb { R }$ with $T > S$ . Provided the conditions on the initial random vector Z are satisfied, these guarantee the existence and uniqueness of a continuous solution of (D.3.11). After stating the theorem we shall use Itô’s formula to derive solutions of the particular Itô equations (7.5.6) and (11.5.9). The solution of (11.5.2) was discussed in Section 11.5.1.

Theorem D.4.1. Suppose that $S \ < \ T \ \in \ \mathbb { R }$ and that the measurable functions b $[ S , T ] \times \mathbb { R } ^ { n } \mapsto \mathbb { R } ^ { n }$ and $\sigma : [ S , T ] \times \mathbb { R } ^ { n } \mapsto \mathbb { R } ^ { n } \times \mathbb { R } ^ { m }$ in (D.3.11) have the properties

$$
\left| b (t, \mathbf {x}) \right| + \left| \sigma (t, \mathbf {x}) \right| <   C (1 + | \mathbf {x} |), \mathbf {x} \in \mathbb {R} ^ {n}, t \in [ S, T ]
$$

and

$$
\left| b (t, \mathbf {x} - b (t, \mathbf {y}) \right| + \left| \sigma (t, \mathbf {x} - \sigma (t, \mathbf {y}) \right| <   D | \mathbf {x} - \mathbf {y} |,
$$

where $C$ and $D$ are finite positive constants and $| M |$ denotes the (positive) square root of the sum of squares of the components of the matrix or vector M. If Z is a random variable independent of $\{ B ( t ) - B ( s ) , S \le s < t \le T \}$ such that $E | Z | ^ { 2 } < \infty$ , then the

stochastic differential equation (D.3.11) has a unique continuous (in t) solution, each component of which belongs to $\mathcal { N } ^ { * } [ S , T ]$ as defined in (D.3.4).

# Geometric Brownian Motion

Geometric Brownian motion was introduced in Section 7.5.2 as a continuous-time model for asset prices and was the basis for the derivations by Black and Scholes (1973) and Merton (1973) of the option-pricing formula discussed in Section 7.6. Here we shall use Itô’s formula to find the solution $\{ P ( t ) , \ t \geq 0 \}$ of the defining differential equation,

$$
d P (t) = P (t) \left[ \mu d t + \sigma d B (t) \right], t \geq 0, \tag {D.4.1}
$$

where $P ( 0 )$ is a strictly positive random variable, independent of $\{ B ( t ) - B ( s ) , 0 \leq$ $s \leq t < \infty \}$ . The standard calculus identity, $d ( \log ( y ) ) = d y / y$ , suggests that we try applying Itô’s formula with $X ( t ) ~ = ~ P ( t )$ and $g ( x , t ) = \log ( x )$ . The function $g$ has continuous partial derivatives, $\partial g / \partial t = 0$ , $\partial g / \partial x = 1 / x$ and $\partial ^ { 2 } g / \partial x ^ { 2 } = - 1 / x ^ { 2 }$ on the set where $x > 0$ . Substituting in (D.3.6) and using (D.4.1) we obtain -

$$
d (\log P (t)) = \frac {1}{P (t)} d P (t) - \frac {1}{2 P (t) ^ {2}} (d P (t)) ^ {2} = \mu d t + \sigma d B (t) - \frac {\sigma^ {2}}{2} d t, \tag {D.4.2}
$$

whence

$$
\log (P (t)) - \log (P (0)) = (\mu - \frac {\sigma^ {2}}{2}) t + \sigma B (t).
$$

This is equivalent to the solution (7.5.7) given earlier.

# Gaussian CARMA Processes

The state equation (11.5.9) for the Gaussian CARMA $( p , q )$ process, i.e.

$$
d \mathbf {X} (t) = A \mathbf {X} (t) d t + \mathbf {e} d B (t), \tag {D.4.3}
$$

where $\mathbf { X } ( 0 )$ is independent of $\{ B ( t ) - B ( s ) , 0 \le s \le t \le T \}$ and $E | \mathbf { X } ( \mathbf { 0 } ) | ^ { 2 } < \infty$ , clearly satisfies the conditions of Theorem D.4.1 and therefore has a unique solution which is continuous in t. In order to find the solution we multiply both sides by the integrating factor $e ^ { - A t }$ , as we would if $\{ B ( t ) \}$ were deterministic. Since $e ^ { - A t }$ is non-singular the state equation is equivalent to the equation

$$
e ^ {- A t} d \mathbf {X} (t) - e ^ {- A t} A \mathbf {X} (t) d t = e ^ {- A t} \mathbf {e} d B (t). \tag {D.4.4}
$$

This form of the equation suggests applying the multivariate Itô formula with $\begin{array} { r } { \mathbf { g } ( t , \mathbf { x } ) = } \end{array}$ $e ^ { - A t } \mathbf { x }$ . The second derivatives of $g$ are all continuous and satisfy

$$
\frac {\partial^ {2} g _ {i}}{\partial x _ {j} \partial x _ {k}} = 0 \text {f o r a l l} i, j \text {a n d} k,
$$

$$
\frac {\partial g _ {i}}{\partial x _ {j}} = \mathbf {e} _ {i} ^ {\prime} e ^ {- A t} \mathbf {e} _ {j},
$$

and

$$
\frac {\partial \mathbf {g}}{\partial t} = - \mathbf {e} _ {t} ^ {\prime} A e ^ {- A t} \mathbf {x}.
$$

where $\mathbf { e } _ { r }$ , $r ~ \in ~ \{ 1 , \ldots , p \}$ , denotes a $p$ -component column vector, all of whose components are zero except for the rth, which is one. Substituting these derivatives into (D.3.10) and writing the resulting equations in vector form we obtain

$$
d \left(e ^ {- A t} \mathbf {X} (t)\right) = - A e ^ {- A t} \mathbf {X} (t) d t + e ^ {- A t} d \mathbf {X} (t).
$$

Substituting this expression in (D.4.4) gives

$$
d \left(e ^ {- A t} \mathbf {X} (t)\right) = e ^ {- A t} d B (t),
$$

which implies that

$$
e ^ {- A t} \mathbf {X} (t) - \mathbf {X} (0) = \int_ {0} ^ {t} e ^ {- A u} \mathbf {e} d B (u),
$$

or equivalently

$$
\mathbf {X} (t) = e ^ {A t} \mathbf {X} (0) + \int_ {0} ^ {t} e ^ {A (t - u)} \mathbf {e} d B (u), 0 \leq t \leq T. \tag {D.4.5}
$$

Since equation (D.4.1), with $\mathbf { X } ( S )$ independent of $\{ B ( t ) - B ( s ) , S \le s \le t \le T \}$ and $E | \mathbf { X } ( \mathbf { S } ) | ^ { 2 } < \infty$ , satisfies the conditions of Theorem D.4.1 for all $S \in \mathbb R$ and $T \in \mathbb { R }$ such that $S < T$ , exactly the same arguments give the more general relation,

$$
\mathbf {X} (t) = e ^ {A (t - S)} \mathbf {X} (S) + \int_ {S} ^ {t} e ^ {A (t - u)} \mathbf {e} d B (u), t \geq S, \text {f o r a l l} S \in \mathbb {R}. \tag {D.4.6}
$$

This is equation (11.5.11) for which we showed (in Section 11.5.2) that the unique causal stationary solution is

$$
\mathbf {X} (t) = \int_ {- \infty} ^ {t} e ^ {A (t - u)} \mathbf {e} d B (u), t \in \mathbb {R}.
$$

This led, with (11.5.8), to the definition of the zero-mean causal CARMA $( p , q )$ process $\{ Y ( t ) , t \in \mathbb { R } \}$ as

$$
Y (t) = \int_ {- \infty} ^ {t} \mathbf {b} ^ {\prime} e ^ {A (t - u)} \mathbf {e} d B (u)
$$

and, more generally in Section 11.5.3, to the second-order Lévy-driven CARMA $( p , q )$ process,

$$
Y (t) = \int_ {(- \infty , t ]} \mathbf {b} ^ {\prime} e ^ {A (t - u)} \mathbf {e} d L (u).
$$

E.1 Getting Started   
E.2 Preparing Your Data for Modeling   
E.3 Finding a Model for Your Data   
E.4 Testing Your Model   
E.5 Prediction   
E.6 Model Properties   
E.7 Multivariate Time Series

The package ITSM2000 requires an IBM-compatible PC operating under Windows XP or any subsequent Windows operating system. To install the package, go to http:// extras.springer.com and locate the extras for this book either by entering the ISBN number or by choosing the year 2016. Choose the option Download Entire Contents and you will receive a zip file containing ITSM.EXE, the data files, an introduction called README.PDF and a searchable document of Help files, ITSM_HELP.PDF. For a quick and easy introduction to the use of the package we recommend following the instructions in README.PDF. For detailed help on each of the functions of the program refer to ITSM_HELP.PDF. Under older Windows operating systems the Help files can be accessed from the Help menu within ITSM itself, but this feature is not yet supported by all versions of Windows so you may need to open ITSM_HELP.PDF in a separate window while running the program.

When you unzip the downloaded zip file it will create a folder called ITSM2000 which contains all the necessary files for running the program. Double-click on the ITSM icon or the ITSM-Shortcut icon to open the ITSM window. (You may wish to copy and paste the ITSM-Shortcut icon to the desktop or some other convenient location from which it can also be accessed.). The package ITSM2000 supersedes the versions of the package ITSM2000 distributed with earlier editions of this book.

# E.1 Getting Started

# E.1.1 Running ITSM

Double-clicking on the ITSM or the ITSM-Shortcut icon will open the ITSM window. To analyze one of the data sets provided, select File>Project>Open at the top left corner of the ITSM window.

There are several distinct functions of the program ITSM. The first is to analyze and display the properties of time series data, the second is to compute and display the properties of time series models, and the third is to combine these functions in order to fit models to data. The last of these includes checking that the properties of the fitted model match those of the data in a suitable sense. Having found an appropriate model, we can (for example) then use it in conjunction with the data to forecast future values of the series. Sections E.2–E.5 of this appendix deal with the modeling and analysis of data, while Section E.6 is concerned with model properties. Section E.7 explains how to open multivariate projects in ITSM. Examples of the analysis of multivariate time series are given in Chapter 8.

It is important to keep in mind the distinction between data and model properties and not to confuse the data with the model. In any one project ITSM stores one data set and one model (which can be identified by highlighting the project window and pressing the red INFO button at the top of the ITSM window). Until a model is entered by the user, ITSM stores the default model of white noise with variance 1. If the data are transformed (e.g., differenced and mean-corrected), then the data are replaced in ITSM by the transformed data. (The original data can, however, be restored by inverting the transformations.) Rarely (if ever) is a real time series generated by a model as simple as those used for fitting purposes. In model fitting the objective is to develop a model that mimics important features of the data, but is still simple enough to be used with relative ease.

The following sections constitute a tutorial that illustrates the use of some of the features of ITSM by leading you through a complete analysis of the well-known airline passenger series of Box and Jenkins (1976) filed as AIRPASS.TSM in the ITSM2000 folder.

# E.2 Preparing Your Data for Modeling

The observed values of your time series should be available in a single-column ASCII file (or two columns for a bivariate series). The file, like those provided with the package, should be given a name with suffix .TSM. You can then begin model fitting with ITSM. The program will read your data from the file, plot it on the screen, compute sample statistics, and allow you to make a number of transformations designed to make your transformed data representable as a realization of a zero-mean stationary process.

Example E.2.1. To illustrate the analysis we shall use the file AIRPASS.TSM, which contains the number of international airline passengers (in thousands) for each month from January, 1949, through December, 1960.

-

# E.2.1 Entering Data

Once you have opened the ITSM window as described above under Getting Started, select the options File>Project>Open,and you will see a dialog box in which you can check either Univariate or Multivariate. Since the data set for this example is univariate, make sure that the univariate option is checked and then click OK. A window labeled Open File will then appear, in which you can either type the name AIRPASS.TSM and click Open, or else locate the icon for AIRPASS.TSM in the Open File window and double-click on it. You will then see a graph of the monthly international airline passenger totals (measured in thousands) $X _ { 1 } , \ldots , X _ { n }$ , with $~ n ~ = ~ 1 4 4$ . Directly behind the graph is a window containing data summary statistics.

An additional, second, project can be opened by repeating the procedure described in the preceding paragraph. Alternatively, the data can be replaced in the current project using the option File>Import File. This option is useful if you wish to examine how well a fitted model represents a different data set. (See the entry ProjectEditor in the ITSM_HELP Files for information on multiple project management. Each ITSM project has its own data set and model.) For the purpose of this introduction we shall open only one project.

# E.2.2 Information

If, with the window labeled AIRPASS.TSM highlighted, you press the red INFO button at the top of the ITSM window, you will see the sample mean, sample variance, estimated standard deviation of the sample mean, and the current model (white noise with variance 1).

Example E.2.2. Go through the steps in Entering Data to open the project AIRPASS.TSM and use the INFO button to determine the sample mean and variance of the series.

# E.2.3 Filing Data

You may wish to transform your data using ITSM and then store it in another file. At any time before or after transforming the data in ITSM, the data can be exported to a file by clicking on the red Export button, selecting Time Series and File, clicking OK, and specifying a new file name. The numerical values of the series can also be pasted to the clipboard (and from there into another document) in the same way by choosing Clipboard instead of File. Other quantities computed by the program (e.g., the residuals from the current model) can be filed or pasted to the clipboard in the same way by making the appropriate selection in the Export dialog box. Graphs can also be pasted to the clipboard by right-clicking on them and selecting Copy to Clipboard.

Example E.2.3. Copy the series AIRPASS.TSM to the clipboard, open Wordpad or some convenient screen editor, and choose Edit>Paste to insert the series into your new document. Then copy the graph of the series to the clipboard and insert it into your document in the same way.

# E.2.4 Plotting Data

A time series graph is automatically plotted when you open a data file (with time measured in units of the interval between observations, i.e., $t = 1 , 2 , 3 , \ldots )$ . To see a histogram of the data press the rightmost yellow button at the top of the ITSM screen. If you wish to adjust the number of bins in the histogram, select Statistics $>$ Histogram>Set Bin Count and specify the number of bins required. The histogram will then be replotted accordingly.

To insert any of the ITSM graphs into a text document, right-click on the graph concerned, select Copy to Clipboard, and the graph will be copied to the clipboard. It can then be pasted into a document opened by any standard text editor such as MS-Word or Wordpad using the Edit>Paste option in the screen editor. The graph can also be sent directly to a printer by right-clicking on the graph and selecting Print. Another useful graphics feature is provided by the white Zoom buttons at the top of the ITSM screen. The first and second of these enable you to enlarge a designated segment or box, respectively, of any of the graphs. The third button restores the original graph.

Example E.2.4. Continuing with our analysis of AIRPASS.TSM, press the yellow histogram button to see a histogram of the data. Replot the histogram with 20 bins by selecting Statistics>Histogram>Set Bin Count.

![](images/e97e6f24189109da1c4e0f4aa531d63c709a30ebb8d0f2fdc85110642b067226.jpg)

# E.2.5 Transforming Data

Transformations are applied in order to produce data that can be successfully modeled as “stationary time series.” In particular, they are used to eliminate trend and cyclic components and to achieve approximate constancy of level and variability with time.

Example E.2.5. The airline passenger data (see Figure 10-4) are clearly not stationary. The level and variability both increase with time, and there appears to be a large seasonal component (with period 12). They must therefore be transformed in order to be represented as a realization of a stationary time series using one or more of the transformations available for this purpose in ITSM.

![](images/2a2dc660b16b5eec38bb8aa6d8c47073ab543dce337beb28d709bddb0ee0cb8e.jpg)

# Box–Cox Transformations

Box–Cox transformations are performed by selecting Transform>Box-Cox and specifying the value of the Box–Cox parameter λ. If the original observations are $Y _ { 1 }$ $Y _ { 1 } , Y _ { 2 } , \ldots , Y _ { n }$ , the Box–Cox transformation $f _ { \lambda }$ converts them to $f _ { \lambda } ( Y _ { 1 } ) , f _ { \lambda } ( Y _ { 2 } ) , \dots ,$ , $f _ { \lambda } ( Y _ { n } )$ , where

$$
f _ {\lambda} (y) = \left\{ \begin{array}{l l} \frac {y ^ {\lambda} - 1}{\lambda}, & \lambda \neq 0, \\ \log (y), & \lambda = 0. \end{array} \right.
$$

These transformations are useful when the variability of the data increases or decreases with the level. By suitable choice of $\lambda$ , the variability can often be made nearly constant. In particular, for positive data whose standard deviation increases linearly with level, the variability can be stabilized by choosing $\lambda = 0$ .

The choice of $\lambda$ can be made visually by watching the graph of the data when you click on the pointer in the Box–Cox dialog box and drag it back and forth along

the scale, which runs from zero to 1.5. Very often it is found that no transformation is needed or that the choice $\lambda = 0$ is satisfactory.

# Example E.2.6.

For the series AIRPASS.TSM, the variability increases with level, and the data are strictly positive. Taking natural logarithms (i.e., choosing a Box–Cox transformation with $\lambda = 0$ ) gives the transformed data shown in Figure E-1.

Notice how the amplitude of the fluctuations no longer increases with the level of the data. However, the seasonal effect remains, as does the upward trend. These will be removed shortly. The data stored in ITSM now consist of the natural logarithms of the original data.

![](images/cab439a76a75488dee6b82f718c7c536d50d8b0d46ec711409f1967f135970d2.jpg)

# Classical Decomposition

There are two methods provided in ITSM for the elimination of trend and seasonality. These are:

i. “classical decomposition” of the series into a trend component, a seasonal component, and a random residual component, and   
ii. differencing.

![](images/b5a5a2f712f14c15e34a8d927ff1bd359f24ff131c3c43304a0b934cc7a88e26.jpg)  
Classical decomposition of the series $\{ X _ { t } \}$ is based on the model   
Figure E-1 The series AIRPASS.TSM after taking logs

$$
X _ {t} = m _ {t} + s _ {t} + Y _ {t},
$$

where $X _ { t }$ is the observation at time t, mt is a “trend component,” $s _ { t }$ is a “seasonal component,” and $Y _ { t }$ is a “random noise component,” which is stationary with mean zero. The objective is to estimate the components $m _ { t }$ and $s _ { t }$ and subtract them from the data to generate a sequence of residuals (or estimated noise) that can then be modeled as a stationary time series.

To achieve this, select Transform>Classical and you will see the Classical Decomposition dialog box. To remove a seasonal component and trend, check the

The logged AIRPASS.TSM series after removal of trend and seasonal components by classical decomposition

![](images/adbcd05ef350dfd50904f97a57f581df6d5f7932b2f0c558fb78dc652190ee2e.jpg)  
Figure E-2

Seasonal Fit and Polynomial Fit boxes, enter the period of the seasonal component, and choose between the alternatives Quadratic Trend and Linear Trend. Click OK, and the trend and seasonal components will be estimated and removed from the data, leaving the estimated noise sequence stored as the current data set.

The estimated noise sequence automatically replaces the previous data stored in ITSM.

# Example E.2.7.

The logged airline passenger data have an apparent seasonal component of period 12 (corresponding to the month of the year) and an approximately quadratic trend. Remove these using the option Transform>Classical as described above. (An alternative approach is to use the option Regression, which allows the specification and fitting of polynomials of degree up to 10 and a linear combination of up to 4 sine waves.)

Figure E-2 shows the transformed data (or residuals) $Y _ { t }$ , obtained by removal of trend and seasonality from the logged AIRPASS.TSM series by classical decomposition. $\{ Y _ { t } \}$ shows no obvious deviations from stationarity, and it would now be reasonable to attempt to fit a stationary time series model to this series. To see how well the estimated seasonal and trend components fit the data, select Transform>Show Classical Fit. We shall not pursue this approach any further here, but turn instead to the differencing approach. (You should have no difficulty in later returning to this point and completing the classical decomposition analysis by fitting a stationary time series model to $\{ Y _ { t } \}$ .)

![](images/32db77df845dcf1750af4772f564e58368682e0cf15196d7680fab07f1339861.jpg)

# Differencing

Differencing is a technique that can also be used to remove seasonal components and trends. The idea is simply to consider the differences between pairs of observations with appropriate time separations. For example, to remove a seasonal component of period 12 from the series $\{ X _ { t } \}$ , we generate the transformed series

$$
Y _ {t} = X _ {t} - X _ {t - 1 2}.
$$

It is clear that all seasonal components of period 12 are eliminated by this transformation, which is called differencing at lag 12. A linear trend can be eliminated by differencing at lag 1, and a quadratic trend by differencing twice at lag 1 (i.e., differencing once to get a new series, then differencing the new series to get a second new series). Higher-order polynomials can be eliminated analogously. It is worth noting that differencing at lag 12 eliminates not only seasonal components with period 12 but also any linear trend.

Data are differenced in ITSM by selecting Transform>Difference and entering the required lag in the resulting dialog box.

# Example E.2.8.

Restore the original airline passenger data using the option File>Import File and selecting AIRPASS.TSM. We take natural logarithms as in Example E.2.6 by selecting Transform>Box-Cox and setting $\lambda \quad = \quad 0$ . The transformed series can now be deseasonalized by differencing at lag 12. To do this select Transform>Difference, enter the lag 12 in the dialog box, and click OK. Inspection of the graph of the deseasonalized series suggests a further differencing at lag 1 to eliminate the remaining trend. To do this, repeat the previous step with lag equal to 1 and you will see the transformed and twice-differenced series shown in Figure E-3.

![](images/385d2b9730e12a7fefacf628dc45bae3ece49da532bc960fda9a492550becc32.jpg)

# Subtracting the Mean

The term ARMA model is used in ITSM to denote a zero-mean ARMA process (see Definition 3.1.1). To fit such a model to data, the sample mean of the data should therefore be small. Once the apparent deviations from stationarity of the data have been removed, we therefore (in most cases) subtract the sample mean of the transformed data from each observation to generate a series to which we then fit a zero-mean stationary model. Effectively we are estimating the mean of the model by the sample mean, then fitting a (zero-mean) ARMA model to the “mean-corrected” transformed data. If we know a priori that the observations are from a process with zero mean, then this process of mean correction is omitted. ITSM keeps track of all the transformations

![](images/af9d71185196a33359df767929d17e76ea722bed5145b9454f835b65f3853223.jpg)  
Figure E-3 The series AIRPASS.TSM after taking logs and differencing at lags 12 and 1

(including mean correction) that are made. When it comes time to predict the original series, ITSM will invert all these transformations automatically.

# Example E.2.9.

Subtract the mean of the transformed and twice-differenced series AIRPASS.TSM by selecting Transform>Subtract Mean. To check the current model status press the red INFO button, and you will see that the current model is white noise with variance 1, since no model has yet been entered.

![](images/5f9419016557c6d3a5ea3e515fbbba98485850acea73aa7346fe6fd08dd55d7e.jpg)

# E.3 Finding a Model for Your Data

After transforming the data (if necessary) as described above, we are now in a position to fit an ARMA model. ITSM uses a variety of tools to guide us in the search for an appropriate model. These include the sample ACF (autocorrelation function), the sample PACF (partial autocorrelation function), and the AICC statistic, a biascorrected form of Akaike’s AIC statistic (see Section 5.5.2).

# E.3.1 Autofit

Before discussing the considerations that go into the selection, fitting, and checking of a stationary time series model, we first briefly describe an automatic feature of ITSM that searches through $\mathbf { A R M A } ( p , q )$ models with $p$ and $q$ between specified limits (less than or equal to 27) and returns the model with smallest AICC value (see Sections 5.5.2 and E.3.5). Once the data set is judged to be representable by a stationary model, select Model>Estimation>Autofit. A dialog box will appear in which you must specify the upper and lower limits for $p$ and $q$ . Since the number of maximum likelihood models to be fitted is the product of the number of $p$ -values and the number of $q$ -values, these ranges should not be chosen to be larger than necessary. Once the limits have been specified, press Start, and the search will begin. You can watch the progress of the search in the dialog box that continually updates the values of $p$ and $q$ and the best model found so far. This option does not consider models in which the coefficients are required to satisfy constraints (other than causality) and consequently does not always lead to the optimal representation of the data. However, like the tools described below, it provides valuable information on which to base the selection of an appropriate model.

# E.3.2 The Sample ACF and PACF

Pressing the second yellow button at the top of the ITSM window will produce graphs of the sample ACF and PACF for values of the lag h from 1 up to 40. For higher lags choose Statistics>ACF/PACF>Specify Lag, enter the maximum lag required, and click OK. Pressing the second yellow button repeatedly then rotates the display through ACF, PACF, and side-by-side graphs of both. Values of the ACF that decay rapidly as $h$ increases indicate short-term dependency in the time series, while slowly decaying values indicate long-term dependency. For ARMA fitting it is desirable to have a sample ACF that decays fairly rapidly. A sample ACF that is positive and very slowly decaying suggests that the data may have a trend. A sample ACF with very slowly damped periodicity suggests the presence of a periodic seasonal

component. In either of these two cases you may need to transform your data before continuing.

As a rule of thumb, the sample ACF and PACF are good estimates of the ACF and PACF of a stationary process for lags up to about a third of the sample size. It is clear from the definition of the sample ACF, $\hat { \rho } ( h )$ , that it will be a very poor estimator of $\rho ( h )$ for $h$ close to the sample size $n$ .

The horizontal lines on the graphs of the sample ACF and PACF are the bounds $\pm 1 . 9 6 / \sqrt { n }$ . If the data constitute a large sample from an independent white noise sequence, approximately $9 5 \%$ of the sample autocorrelations should lie between these bounds. Large or frequent excursions from the bounds suggest that we need a model to explain the dependence and sometimes to suggest the kind of model we need (see below). To obtain numerical values of the sample ACF and PACF, right-click on the graphs and select Info.

The graphs of the sample ACF and PACF sometimes suggest an appropriate ARMA model for the data. As a rough guide, if the sample ACF falls between the plotted bounds $\pm 1 . 9 6 / \sqrt { n }$ for lags $h > q$ , then an $\mathrm { M A } ( q )$ model is suggested, while if the sample PACF falls between the plotted bounds $\pm 1 . 9 6 / \sqrt { n }$ for lags $h > p$ , then an $\operatorname { A R } ( p )$ model is suggested.

If neither the sample ACF nor PACF “cuts off” as in the previous paragraph, a more refined model selection technique is required (see the discussion of the AICC statistic in Section 5.5.2). Even if the sample ACF or PACF does cut off at some lag, it is still advisable to explore models other than those suggested by the sample ACF and PACF values.

# Example E.3.1.

Figure E-4 shows the sample ACF of the AIRPASS.TSM series after taking logarithms, differencing at lags 12 and 1, and subtracting the mean. Figure E-5 shows the corresponding sample PACF. These graphs suggest that we consider an MA model of order 12 (or perhaps 23) with a large number of zero coefficients, or alternatively an AR model of order 12.

Figure E-4   
![](images/bc3735aa917280abcc44b59b7a89e55bb2298345caaf684dfbaa2e646bdef6c3.jpg)  
The sample ACF of the transformed AIRPASS. TSM series

![](images/88c34162763b226ee55455ff9aea39b6803e63c7acce5f811cdebe86bd406bdc.jpg)  
Figure E-5 The sample PACF of the transformed AIRPASS. TSM series

# E.3.3 Entering a Model

A major function of ITSM is to find an ARMA model whose properties reflect to a high degree those of an observed (and possibly transformed) time series. Any particular causal $\mathbf { A R M A } ( p , q )$ model with $\textit { p } \leq \ 2 7$ and $q \ \leq \ 2 7$ can be entered directly by choosing Model $>$ Specify, entering the values of $p , \ q$ , the coefficients, and the white noise variance, and clicking OK. If there is a data set already open in ITSM, a quick way of entering a reasonably appropriate model is to use the option Model>Estimation>Preliminary, which estimates the coefficients and white noise variance of an ARMA model after you have specified the orders $p$ and $q$ and selected one of the four preliminary estimation algorithms available. An optimal preliminary AR model can also be fitted by checking Find AR model with min AICC in the Preliminary Estimation dialog box. If no model is entered or estimated, ITSM assumes the default ARMA(0,0), or white noise, model

$$
X _ {t} = Z _ {t},
$$

where $\{ Z _ { t } \}$ is an uncorrelated sequence of random variables with mean zero and variance 1.

If you have data and no particular ARMA model in mind, it is advisable to use the option Model>Estimation>Preliminary or equivalently to press the blue PRE button at the top of the ITSM window.

Sometimes you may wish to try a model found in a previous session or a model suggested by someone else. In that case choose Model $>$ Specify and enter the required model. You can save both the model and data from any project by selecting File>Project>Save as and specifying the name for the new file. When the new file is opened, both the model and the data will be imported. To create a project with this model and a new data set select File>Import File and enter the name of the file containing the new data. (This file must contain data only. If it also contains a model, then the model will be imported with the data and the model previously in ITSM will be overwritten.)

# E.3.4 Preliminary Estimation

The option Model $>$ Estimation>Preliminary contains fast (but not the most efficient) model-fitting algorithms. They are useful for suggesting the most promising models for the data, but should be followed by maximum likelihood estimation using Model>Estimation>Max likelihood. The fitted preliminary model is generally used as an initial approximation with which to start the nonlinear optimization carried out in the course of maximizing the (Gaussian) likelihood.

To fit an ARMA model of specified order, first enter the values of $p$ and $q$ (see Section 2.6.1). For pure AR models $q = 0$ , and the preliminary estimation option offers a choice between the Burg and Yule–Walker estimates. (The Burg estimates frequently give higher values of the Gaussian likelihood than the Yule–Walker estimates.) If $q =$ 0, you can also check the box Find AR model with min AICC to allow the program to fit AR models of orders $0 , 1 , \ldots , 2 7$ and select the one with smallest AICC value (Section 5.5.2). For models with $q > 0$ , ITSM provides a choice between two preliminary estimation methods, one based on the Hannan–Rissanen procedure and the other on the innovations algorithm. If you choose the innovations option, a default value of m will be displayed on the screen. This parameter was defined in Section 5.1.3. The standard choice is the default value computed by ITSM. The Hannan–Rissanen algorithm is recommended when $p$ and $q$ are both greater than 0, since it tends to give causal models more frequently than the innovations method. The latter is recommended when $p = 0$ .

Once the required entries in the Preliminary Estimation dialog box have been completed, click OK, and ITSM will quickly estimate the parameters of the selected model and display a number of diagnostic statistics. (If $p$ and $q$ are both greater than 0, it is possible that the fitted model may be noncausal, in which case ITSM sets all the coefficients to .001 to ensure the causality required for subsequent maximum likelihood estimation. It will also give you the option of fitting a model of different order.)

Provided that the fitted model is causal, the estimated parameters are given with the ratio of each estimate to 1.96 times its standard error. The denominator ${ \ : 1 . 9 6 \times }$ standard error) is the critical value (at level .05) for the coefficient. Thus, if the ratio is greater than 1 in absolute value, we may conclude (at level .05) that the corresponding coefficient is different from zero. On the other hand, a ratio less than 1 in absolute value suggests the possibility that the corresponding coefficient in the model may be zero. (If the innovations option is chosen, the ratios of estimates to $1 . 9 6 \times$ standard error are displayed only when $p = q$ or $p = 0 .$ .) In the Preliminary Estimates window you will also see one or more estimates of the white noise variance (the residual sum of squares divided by the sample size is the estimate retained by ITSM) and some further diagnostic statistics. These are $- 2 \ln L \big ( \hat { \phi } , \hat { \theta } , \hat { \sigma } ^ { 2 } \big )$ , where $L$ denotes the Gaussian likelihood (5.2.9), and the AICC statistic

$$
- 2 \ln L + 2 (p + q + 1) n / (n - p - q - 2)
$$

(see Section 5.5.2).

Our eventual aim is to find a model with as small an AICC value as possible. Smallness of the AICC value computed in the preliminary estimation phase is indicative of a good model, but should be used only as a rough guide. Final decisions between models should be based on maximum likelihood estimation, carried out using the option Model>Estimation>Max likelihood, since for fixed $p$ and $q$ , the values of $\phi , \theta$ , and $\sigma ^ { 2 }$ that minimize the AICC statistic are the maximum likelihood estimates, not the preliminary estimates. After completing preliminary estimation, ITSM stores

the estimated model coefficients and white noise variance. The stored estimate of the white noise variance is the sum of squares of the residuals (or one-step prediction errors) divided by the number of observations.

A variety of models should be explored using the preliminary estimation algorithms, with a view to finding the most likely candidates for minimizing AICC when the parameters are reestimated by maximum likelihood.

# Example E.3.2.

To find the minimum-AICC Burg AR model for the logged, differenced, and meancorrected series AIRPASS.TSM currently stored in ITSM, press the blue PRE button, set the MA order equal to zero, select Burg and Find AR model with min AICC, and then click OK. The minimum-AICC AR model is of order 12 with an AICC value of 458.13. To fit a preliminary MA(25) model to the same data, press the blue PRE button again, but this time set the AR order to 0, the MA order to 25, select Innovations, and click OK.

The ratios (estimated coefficient)/( $1 . 9 6 \times$ standard error) indicate that the coefficients at lags 1 and 12 are nonzero, as suggested by the sample ACF. The estimated coefficients at lags 3 and 23 also look substantial even though the corresponding ratios are less than 1 in absolute value. The displayed values are as follows:

<table><tr><td colspan="5">MA COEFFICIENTS</td></tr><tr><td>-0.3568</td><td>0.0673</td><td>-0.1629</td><td>-0.0415</td><td>0.1268</td></tr><tr><td>0.0264</td><td>0.0283</td><td>-0.0648</td><td>0.1326</td><td>-0.0762</td></tr><tr><td>-0.0066</td><td>-0.4987</td><td>0.1789</td><td>-0.0318</td><td>0.1476</td></tr><tr><td>-0.1461</td><td>0.0440</td><td>-0.0226</td><td>-0.0749</td><td>-0.0456</td></tr><tr><td>-0.0204</td><td>-0.0085</td><td>0.2014</td><td>-0.0767</td><td>-0.0789</td></tr><tr><td colspan="5">RATIO OF COEFFICIENTS TO (1.96*STANDARD ERROR)</td></tr><tr><td>-2.0833</td><td>0.3703</td><td>-0.8941</td><td>-0.2251</td><td>0.6875</td></tr><tr><td>0.1423</td><td>0.1522</td><td>-0.3487</td><td>0.7124</td><td>-0.4061</td></tr><tr><td>-0.0353</td><td>-2.6529</td><td>0.8623</td><td>-0.1522</td><td>0.7068</td></tr><tr><td>-0.6944</td><td>0.2076</td><td>-0.1065</td><td>-0.3532</td><td>-0.2147</td></tr><tr><td>-0.0960</td><td>-0.0402</td><td>0.9475</td><td>-0.3563</td><td>-0.3659</td></tr></table>

The estimated white noise variance is 0.00115 and the AICC value is 440.93, which is not as good as that of the AR(12) model. Later we shall find a subset MA(25) model that has a smaller AICC value than both of these models.

![](images/f3eceb1921cbae7de75918c2ed5086ffe49fb6246e9c6d7621869934e7f749bb.jpg)

# E.3.5 The AICC Statistic

The AICC statistic for the model with parameters $p , q , \phi$ , and $\pmb \theta$ is defined (see Section 5.5.2) as

$$
\operatorname {A I C C} (\phi , \theta) = - 2 \ln L (\phi , \theta , S (\phi , \theta) / n) + 2 (p + q + 1) n / (n - p - q - 2),
$$

and a model chosen according to the AICC criterion minimizes this statistic.

Model-selection statistics other than AICC are also available in ITSM. A Bayesian modification of the AIC statistic known as the BIC statistic is also computed in the option Model>Estimation>Max likelihood. It is used in the same way as the AICC.

An exhaustive search for a model with minimum AICC or BIC value can be very slow. For this reason the sample ACF and PACF and the preliminary estimation

techniques described above are useful in narrowing down the range of models to be considered more carefully in the maximum likelihood estimation stage of model fitting.

# E.3.6 Changing Your Model

The model currently stored by the program can be checked at any time by selecting Model>Specify. Any parameter can be changed in the resulting dialog box, including the white noise variance. The model can be filed together with the data for later use by selecting File>Project>Save as and specifying a file name with suffix .TSM.

# Example E.3.3.

We shall now set some of the coefficients in the current model to zero. To do this choose Model $>$ Specify and click on the box containing the value $- 0 . 3 5 6 7 6$ of Theta(1). Press Enter, and the value of Theta(2) will appear in the box. Set this to zero. Press Enter again, and the value of Theta(3) will appear. Continue to work through the coefficients, setting all except Theta(1), Theta(3), Theta(12), and Theta(23) equal to zero. When you have reset the parameters, click OK, and the new model stored in ITSM will be the subset MA(23) model

$$
X _ {t} = Z _ {t} - 0. 3 5 7 Z _ {t - 1} - 0. 1 6 3 Z _ {t - 3} - 0. 4 9 9 Z _ {t - 1 2} + 0. 2 0 1 Z _ {t - 2 3},
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , 0 . 0 0 1 1 5 )$

-

# E.3.7 Maximum Likelihood Estimation

Once you have specified values of $p$ and $q$ and possibly set some coefficients to zero, you can carry out efficient parameter estimation by selecting Model>Estimation> Max likelihood or equivalently by pressing the blue MLE button.

The resulting dialog box displays the default settings, which in most cases will not need to be modified. However, if you wish to compute the likelihood without maximizing it, check the box labeled No optimization. The remaining information concerns the optimization settings. (With the default settings, any coefficients that are set to zero will be treated as fixed values and not as parameters. Coefficients to be optimized must therefore not be set exactly to zero. If you wish to impose further constraints on the optimization, press the Constrain optimization button. This allows you to fix certain coefficients or to impose multiplicative relationships on the coefficients during optimization.)

To find the maximum likelihood estimates of your parameters, click OK, and the estimated parameters will be displayed. To refine the estimates, repeat the estimation, specifying a smaller value of the accuracy parameter in the Maximum Likelihood dialog box.

# Example E.3.4.

To find the maximum likelihood estimates of the parameters in the model for the logged, differenced, and mean-corrected airline passenger data currently stored in ITSM, press the blue MLE button and click OK. The following estimated parameters and diagnostic statistics will then be displayed:

ARMA MODEL:

$$
\begin{array}{l} X (t) = Z (t) + (-. 3 5 5) * Z (t - 1) + (-. 2 0 1) * Z (t - 3) + (-. 5 2 3) * Z (t - 1 2) \\ + (. 2 4 2) * Z (t - 2 3) \\ \end{array}
$$

WN Variance $=$ .001250

MA Coefficients

$$
\mathrm {T H E T A} (1) = -. 3 5 5 0 7 8 \mathrm {T H E T A} (3) = -. 2 0 1 1 2 5
$$

$$
\mathrm {T H E T A} (1 2) = -. 5 2 3 4 2 3 \mathrm {T H E T A} (2 3) = . 2 4 1 5 2 7
$$

Standard Error of MA Coefficients

THETA( 1): .059385 THETA( 3): .059297

THETA(12): .058011 THETA(23): .055828

(Residual SS)/N $=$ .125024E−02

$$
\mathrm {A I C C} = -. 4 8 6 0 3 7 \mathrm {E} + 0 3
$$

$$
\mathrm {B I C} = -. 4 8 7 6 2 2 \mathrm {E} + 0 3
$$

$$
- 2 \operatorname {L n} (\text {L i k e l i h o o d}) = -. 4 9 6 5 1 7 E + 0 3
$$

$$
\text {A c c u r a c y p a r a m e t e r} = . 0 0 2 0 5 0 0 0
$$

$$
\text {N u m b e r o f i t e r a t i o n s} = 5
$$

Number of function evaluations $= 4 6$

Optimization stopped within accuracy level.

The last message indicates that the minimum of $- 2 \ln L$ has been located with the specified accuracy. If you see the message

Iteration limit exceeded,

then the minimum of $- 2 \ln L$ could not be located with the number of iterations (50) allowed. You can continue the search (starting from the point at which the iterations were interrupted) by pressing the MLE button to continue the minimization and possibly increasing the maximum number of iterations from 50 to 100.

![](images/4a116c57e0d31f45d7f35c8acfe280c2bb4fe5ed6149fdd30c87e9631cacddf7.jpg)

# E.3.8 Optimization Results

After maximizing the Gaussian likelihood, ITSM displays the model parameters (coefficients and white noise variance), the values of $- 2 \ln L$ , AICC, BIC, and information regarding the computations.

# Example E.3.5.

The next stage of the analysis is to consider a variety of competing models and to select the most suitable. The following table shows the AICC statistics for a variety of subset moving average models of order less than 24.

<table><tr><td></td><td></td><td colspan="4">Lags</td><td>AICC</td></tr><tr><td>1</td><td>3</td><td colspan="3">12</td><td>23</td><td>-486.04</td></tr><tr><td>1</td><td>3</td><td colspan="2">12</td><td>13</td><td>23</td><td>-485.78</td></tr><tr><td>1</td><td>3</td><td>5</td><td>12</td><td></td><td>23</td><td>-489.95</td></tr><tr><td>1</td><td>3</td><td></td><td>12</td><td>13</td><td></td><td>-482.62</td></tr><tr><td>1</td><td></td><td></td><td>12</td><td></td><td></td><td>-475.91</td></tr></table>

The best of these models from the point of view of AICC value is the one with nonzero coefficients at lags 1, 3, 5, 12, and 23. To obtain this model from the one currently stored in ITSM, select Model $>$ Specify, change the value of THETA(5) from zero to .001, and click OK. Then reoptimize by pressing the blue MLE button and clicking OK. You should obtain the noninvertible model

$$
X _ {t} = Z _ {t} - 0. 4 3 4 Z _ {t - 1} - 0. 3 0 5 Z _ {t - 3} + 0. 2 3 8 Z _ {t - 5} - 0. 6 5 6 Z _ {t - 1 2} + 0. 3 5 1 Z _ {t - 2 3},
$$

where $\{ Z _ { t } \} \sim \mathrm { W N } ( 0 , 0 . 0 0 1 0 3 )$ . For future reference, file the model and data as AIR-PASS2.TSM using the option File>Project $>$ Save as.

The next step is to check our model for goodness of fit.

# E.4 Testing Your Model

Once we have a model, it is important to check whether it is any good or not. Typically this is judged by comparing observations with corresponding predicted values obtained from the fitted model. If the fitted model is appropriate then the prediction errors should behave in a manner that is consistent with the model. The residuals are the rescaled one-step prediction errors,

$$
\hat {W} _ {t} = (X _ {t} - \hat {X} _ {t}) / \sqrt {r _ {t - 1}},
$$

where $\hat { X } _ { t }$ is the best linear mean-square predictor of $X _ { t }$ based on the observations up to time $t - 1$ , $r _ { t - 1 } = E ( X _ { t } - \hat { X } _ { t } ) ^ { 2 } / \dot { \sigma } ^ { 2 }$ and $\sigma ^ { 2 }$ is the white noise variance of the fitted model.

If the data were truly generated by the fitted ARMA $( p , q )$ model with white noise sequence $\{ Z _ { t } \}$ , then for large samples the properties of $\{ \hat { W } _ { t } \}$ should reflect those of $\{ Z _ { t } \}$ . To check the appropriateness of the model we therefore examine the residual series $\{ \hat { W } _ { t } \}$ , and check that it resembles a realization of a white noise sequence.

ITSM provides a number of tests for doing this in the Residuals Menu, which is obtained by selecting the option Statistics>Residual Analysis. Within this option are the suboptions

Plot

QQ-Plot (normal)

QQ-Plot (t-distr)

Histogram

ACF/PACF

ACF Abs vals/Squares

Tests of randomness

# E.4.1 Plotting the Residuals

Select Statistics>Residual Analysis $>$ Histogram, and you will see a histogram of the rescaled residuals, defined as

$$
\hat {R} _ {t} = \hat {W} _ {t} / \hat {\sigma},
$$

where $n \hat { \sigma } ^ { 2 }$ is the sum of the squared residuals. If the fitted model is appropriate, the histogram of the rescaled residuals should have mean close to zero. If in addition the data are Gaussian, this will be reflected in the shape of the histogram, which should then resemble a normal density with mean zero and variance 1.

Select Statistics $>$ Residual Analysis $>$ Plot and you will see a graph of $\hat { R } _ { t }$ vs. t. If the fitted model is appropriate, this should resemble a realization of a white noise sequence. Look for trends, cycles, and nonconstant variance, any of which suggest that the fitted model is inappropriate. If substantially more than $5 \%$ of

![](images/550f33f5ead6fcc8b4d23305bc9ce602b55ebf6fb9ef9d126e8d3a5a1b57c094.jpg)  
Figure E-6 Histogram of the rescaled residuals from AIRPASS.MOD

the rescaled residuals lie outside the bounds $\pm 1 . 9 6$ or if there are rescaled residuals far outside these bounds, then the fitted model should not be regarded as Gaussian.

Compatibility of the distribution of the residuals with either the normal distribution or the $t$ -distribution can be checked by inspecting the corresponding qq plots and checking for approximate linearity. To test for normality, the Jarque–Bera statistic is also computed.

# Example E.4.1.

The histogram of the rescaled residuals from our model for the logged, differenced, and mean-corrected airline passenger series is shown in Figure E-6. The mean is close to zero, and the shape suggests that the assumption of Gaussian white noise is not unreasonable in our proposed model.

The graph of $\hat { R _ { t } }$ vs. t is shown in Figure E-7. A few of the rescaled residuals are greater in magnitude than 1.96 (as is to be expected), but there are no obvious indications here that the model is inappropriate. The approximate linearity of the normal qq plot and the Jarque–Bera test confirm the approximate normality of the residuals.

![](images/ce85b7e388a3eb70eb5f3bcac218095f20fd3786600621a88c9a8186e151fc21.jpg)

# E.4.2 ACF/PACF of the Residuals

If we were to assume that our fitted model is the true process generating the data, then the observed residuals would be realized values of a white noise sequence.

In particular, the sample ACF and PACF of the observed residuals should lie within the bounds $\pm 1 . 9 6 / \sqrt { n }$ roughly $9 5 \%$ of the time. These bounds are displayed on the graphs of the ACF and PACF. If substantially more than $5 \%$ of the correlations are outside these limits, or if there are a few very large values, then we should look for a better-fitting model. (More precise bounds, due to Box and Pierce, can be found in Brockwell and Davis (1991) Section 10.4.)

# Example E.4.2.

Choose Statistics>Residual Analysis>ACF/PACF, or equivalently press the middle green button at the top of the ITSM window. The sample ACF and PACF of the residuals will then appear as shown in Figures E-8 and E-9. No correlations

![](images/85c231683b789af72d95ea6e6e045311f905d35c231161bfe2e374f8dccb7ce5.jpg)  
Figure E-7 Time plot of the rescaled residuals from AIRPASS.MOD

![](images/348f9882e8be511392bc23bb1dee3562010687a5c9b85d9e579cd8f7443ed2e8.jpg)  
Figure E-8 Sample ACF of the residuals from AIRPASS.MOD

are outside the bounds in this case. They appear to be compatible with the hypothesis that the residuals are in fact observations of a white noise sequence. To check for independence of the residuals, the sample autocorrelation functions of their absolute values and squares can be plotted by clicking on the third green button.

# E.4.3 Testing for Randomness of the Residuals

The option Statistics>Residual Analysis>Tests of Randomness carries out the six tests for randomness of the residuals described in Section 5.3.3.

# Example E.4.3.

The residuals from our model for the logged, differenced, and mean-corrected series AIRPASS.TSM are checked by selecting the option indicated above and selecting the parameter $h$ for the portmanteau tests. Adopting the value $h = 2 5$ suggested by ITSM, we obtain the following results:

![](images/12c2a24ec1e0be136eea4703de7120ec0615abb39bbede83ff26ac9ab04b866f.jpg)  
Figure E-9 Sample PACF of the residuals from AIRPASS.MOD

RANDOMNESS TEST STATISTICS (see Section 5.3.3)   

<table><tr><td>LJUNG-BOX PORTM. = 13.76</td><td>CHISQUR(20),</td><td>p-value = 0.843</td></tr><tr><td>MCLEOD-LI PORTM. = 17.39</td><td>CHISQUR(25),</td><td>p-value = 0.867</td></tr><tr><td>TURNING POINTS = 87.</td><td>ANORMAL(86.00, 4.79**2),</td><td>p-value = 0.835</td></tr><tr><td>DIFFERENCE-SIGN = 65.</td><td>ANORMAL(65.00, 3.32**2),</td><td>p-value = 1.000</td></tr><tr><td>RANK TEST = 3934.</td><td>ANORMAL(4257.50, 251.3**2),</td><td>p-value = 0.198</td></tr><tr><td>JARQUE-BERA = 4.33</td><td>CHISQUR(2)</td><td>p-value = 0.115</td></tr><tr><td>ORDER OF MIN AICC</td><td>YW MODEL FOR RESIDUALS = 0</td><td></td></tr></table>

Every test is easily passed by our fitted model (with significance level $\alpha = 0 . 0 5$ ), and the order of the minimum-AICC AR model for the residuals supports the compatibility of the residuals with white noise. For later use, file the residuals by pressing the red EXP button and exporting the residuals to a file with the name AIRRES.TSM.

# E.5 Prediction

One of the main purposes of time series modeling is the prediction of future observations. Once you have found a suitable model for your data, you can predict future values using the option Forecasting>ARMA. (The other options listed under Forecasting refer to the methods of Chapter 10.)

# E.5.1 Forecast Criteria

Given observations $X _ { 1 } , \ldots , X _ { n }$ of a series that we assume to be appropriately modeled as an $\mathbf { A R M A } ( p , q )$ process, ITSM predicts future values of the series $X _ { n + h }$ from the data and the model by computing the linear combination $P _ { n } ( X _ { n + h } )$ of $X _ { 1 } , \ldots , X _ { n }$ that minimizes the mean squared error $E ( X _ { n + h } - P _ { n } ( X _ { n + h } ) ) ^ { 2 }$ .

# E.5.2 Forecast Results

Assuming that the current data set has been adequately fitted by the current ARMA $( p , q )$ model, choose Forecasting>ARMA, and you will see the ARMA Forecast dialog box.

You will be asked for the number of forecasts required, which of the transformations you wish to invert (the default settings are to invert all of them so as to obtain forecasts of the original data), whether or not you wish to plot prediction bounds (assuming normality), and if so, the confidence level required, e.g., $95 \%$ . After providing this information, click OK, and the data will be plotted with the forecasts (and possibly prediction bounds) appended. As is to be expected, the separation of the prediction bounds increases with the lead time $h$ of the forecast.

Right-click on the graph, select Info, and the numerical values of the predictors and prediction bounds will be printed.

# Example E.5.1.

We left our logged, differenced, and mean-corrected airline passenger data stored in ITSM with the subset MA(23) model found in Example D.3.5. To predict the next 24 values of the original series, select Forecasting>ARMA and accept the default settings in the dialog box by clicking OK. You will then see the graph shown in Figure E-10. Numerical values of the forecasts are obtained by right-clicking on the graph and selecting Info. The ARMA Forecast dialog box also permits using a model constructed from a subset of the data to obtain forecasts and prediction bounds for the remaining observed values of the series.

# E.6 Model Properties

ITSM can be used to analyze the properties of a specified ARMA process without reference to any data set. This enables us to explore and compare the properties of different ARMA models in order to gain insight into which models might best represent particular features of a given data set.

![](images/3802cdf5ab7ac9f315eb21de2463c64cebae6ba086357f7967b8b1ffee019f4b.jpg)  
Figure E-10 The original AIRPASS data with 24 forecasts appended

For any $\mathbf { A R M A } ( p , q )$ process or fractionally integrated $\mathbf { A R M A } ( p , q )$ process with $p \ \leq \ 2 7$ and $q \ \leq \ 2 7$ , ITSM allows you to compute the autocorrelation and partial autocorrelation functions, the spectral density and distribution functions, and the $\mathbf { M A } ( \infty )$ and $\mathbf { A R } ( \infty )$ representations of the process. It also allows you to generate simulated realizations of the process driven by either Gaussian or non-Gaussian noise. The use of these options is described in this section.

Example E.6.1. We shall illustrate the use of ITSM for model analysis using the model for the transformed series AIRPASS.TSM that is currently stored in the program.

![](images/3588ac6522ccdad81113ce3d90f0b3398f93705a8452bef35b9515c1a1c11146.jpg)

# E.6.1 ARMA Models

For modeling zero-mean stationary time series, ITSM uses the class of ARMA (and fractionally integrated ARMA) processes. ITSM enables you to compute characteristics of the causal ARMA model defined by

$$
X _ {t} = \phi_ {1} X _ {t - 1} + \phi_ {2} X _ {t - 2} + \dots + \phi_ {p} X _ {t - p} + Z _ {t} + \theta_ {1} Z _ {t - 1} + \theta_ {2} Z _ {t - 2} + \dots + \theta_ {q} Z _ {t - q},
$$

or more concisely $\phi ( B ) X _ { t } = \theta ( B ) Z _ { t }$ , where $\{ Z _ { t } \} \sim \mathbf { W } \mathbf { N } \left( 0 , \sigma ^ { 2 } \right)$ and the parameters are all specified. (Characteristics of the fractionally integrated $\mathbf { A R I M A } ( p , d , q )$ process defined by

$$
(1 - B) ^ {d} \phi (B) X _ {t} = \theta (B) Z _ {t}, | d | <   0. 5,
$$

can also be computed.)

ITSM works exclusively with causal models. It will not permit you to enter a model for which $1 - \phi _ { 1 } z - \cdot \cdot \cdot - \phi _ { p } z ^ { p }$ has a zero inside or on the unit circle, nor does it generate fitted models with this property. From the point of view of second-order properties, this represents no loss of generality (Section 3.1). If you are trying to enter an ARMA $( p , q )$ model manually, the simplest way to ensure that your model is causal is to set all the autoregressive coefficients close to zero (e.g., .001). ITSM will not accept a noncausal model.

ITSM does not restrict models to be invertible. You can check whether or not the current model is invertible by choosing Model $>$ Specify and pressing the button labeled Causal/Invertible in the resulting dialog box. If the model is noninvertible, i.e., if the moving-average polynomial $1 + \theta _ { 1 } z + \cdot \cdot \cdot + \theta _ { q } z ^ { q }$ has a zero inside or on the unit circle, the message Non-invertible will appear beneath the box containing the moving-average coefficients. (A noninvertible model can be converted to an invertible model with the same autocovariance function by choosing Model>Switch to invertible. If the model is already invertible, the program will tell you.)

# E.6.2 Model ACF, PACF

The model ACF and PACF are plotted using Model>ACF/PACF>Model. If you wish to change the maximum lag from the default value of 40, select Model>ACF/PACF> Specify Lag and enter the required maximum lag. (It can be much larger than 40, e.g., 10,000). The graph will then be modified, showing the correlations up to the specified maximum lag.

If there is a data file open as well as a model in ITSM, the model ACF and PACF can be compared with the sample ACF and PACF by pressing the third yellow button

at the top of the ITSM window. The model correlations will then be plotted in red, with the corresponding sample correlations shown in the same graph but plotted in green.

Figure E-11   
![](images/e3e2c8c450df883ac122d9e782a11200de5f9779809f27df1e6c3fb0d6029aa9.jpg)  
The ACF of the model in Example E.3.5 together with the sample ACF of the transformed AIRPASS.TSM series

Figure E-12   
![](images/7c46d6977f3190f8e300a2fbdd28c04bc2546292012359316c0f7060ad7009aa.jpg)  
The PACF of the model in Example E.3.5 together with the sample PACF of the transformed AIRPASS.TSM series

# Example E.6.2.

The sample and model ACF and PACF for the current model and transformed series AIRPASS.TSM are shown in Figures E-11 and E-12. They are obtained by pressing the third yellow button at the top of the ITSM window. The vertical lines represent the model values, and the squares are the sample ACF/PACF. The graphs show that the data and the model ACF both have large values at lag 12, while the sample and model partial autocorrelation functions both tend to die away geometrically after the peak at lag 12. The similarities between the graphs indicate that the model is capturing some of the important features of the data.

# E.6.3 Model Representations

As indicated in Section 3.1, if $\{ X _ { t } \}$ is a causal ARMA process, then it has an $\mathbf { M A } ( \infty )$ representation

$$
X _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} Z _ {t - j}, \quad t = 0, \pm 1, \pm 2, \ldots ,
$$

where $\sum _ { j = 0 } ^ { \infty } | \psi _ { j } | < \infty$ and $\psi _ { 0 } = 1$ .

Similarly, if $\{ X _ { t } \}$ is an invertible ARMA process, then it has an $\mathbf { A R } ( \infty )$ representation

$$
Z _ {t} = \sum_ {j = 0} ^ {\infty} \pi_ {j} X _ {t - j}, \quad t = 0, \pm 1, \pm 2, \ldots ,
$$

where $\textstyle \sum _ { j = 0 } ^ { \infty } | \pi _ { j } | < \infty$ and $\pi _ { 0 } = 1$

For any specified causal ARMA model you can determine the coefficients in these representations by selecting the option Model>AR/MA Infinity. (If the model is not invertible, you will see only the $\mathbf { M A } ( \infty )$ coefficients, since the $\mathbf { A R } ( \infty )$ representation does not exist in this case.)

Example E.6.3. The current subset MA(23) model for the transformed series AIRPASS.TSM does not have an $\mathbf { A R } ( \infty )$ representation, since it is not invertible. However, we can replace the model with an invertible one having the same autocovariance function by selecting Model>Switch to Invertible. For this model we can then find an $\mathbf { A R } ( \infty )$ representation by selecting Model>AR Infinity. This gives 50 coefficients, the first 20 of which are shown below.

<table><tr><td>MA – Infinity</td><td>AR – Infinity</td><td></td></tr><tr><td>j</td><td>psi(j)</td><td>pi(j)</td></tr><tr><td>0</td><td>1.00000</td><td>1.00000</td></tr><tr><td>1</td><td>-0.36251</td><td>0.36251</td></tr><tr><td>2</td><td>0.01163</td><td>0.11978</td></tr><tr><td>3</td><td>-0.26346</td><td>0.30267</td></tr><tr><td>4</td><td>-0.06924</td><td>0.27307</td></tr><tr><td>5</td><td>0.15484</td><td>-0.00272</td></tr><tr><td>6</td><td>-0.02380</td><td>0.05155</td></tr><tr><td>7</td><td>-0.06557</td><td>0.16727</td></tr><tr><td>8</td><td>-0.04487</td><td>0.10285</td></tr><tr><td>9</td><td>0.01921</td><td>0.01856</td></tr><tr><td>10</td><td>-0.00113</td><td>0.07947</td></tr><tr><td>11</td><td>0.01882</td><td>0.07000</td></tr><tr><td>12</td><td>-0.57008</td><td>0.58144</td></tr><tr><td>13</td><td>0.00617</td><td>0.41683</td></tr><tr><td>14</td><td>0.00695</td><td>0.23490</td></tr><tr><td>15</td><td>0.03188</td><td>0.37200</td></tr><tr><td>16</td><td>0.02778</td><td>0.38961</td></tr><tr><td>17</td><td>0.01417</td><td>0.10918</td></tr><tr><td>18</td><td>0.02502</td><td>0.08776</td></tr><tr><td>19</td><td>0.00958</td><td>0.22791</td></tr></table>

# E.6.4 Generating Realizations of a Random Series

ITSM can be used to generate realizations of a random time series defined by the currently stored model.

To generate such a realization, select the option Model>Simulate, and you will see the ARMA Simulation dialog box. You will be asked to specify the number of observations required, the white noise variance (if you wish to change it from the current value), and an integer-valued random number seed (by specifying and recording this integer with up to nine digits you can reproduce the same realization at a later date by reentering the same seed). You will also have the opportunity to add a specified mean to the simulated ARMA values. If the current model has been fitted to transformed data, then you can also choose to apply the inverse transformations to the simulated ARMA to generate a simulated version of the original series. The default distribution for the white noise is Gaussian. However, by pressing the button Change noise distribution you can select from a variety of alternative distributions or by checking the box Use Garch model for noise process you can generate an ARMA process driven by GARCH noise. Finally, you can choose whether the simulated data will overwrite the data set in the current project or whether they will be used to create a new project. Once you are satisfied with your choices, click OK, and the simulated series will be generated.

Example E.6.4. To generate a simulated realization of the series AIRPASS.TSM using the current model and transformed data set, select the option Model>Simulate. The default options in the dialog box are such as to generate a realization of the original series as a new project, so it suffices to click OK. You will then see a graph of the simulated series that should resemble the original series AIRPASS.TSM.

![](images/b5ddd1d676015fbacb16f7699bc87dee289585d3cf89f8de6a88c9a0816b3d36.jpg)

# E.6.5 Spectral Properties

Spectral properties of both data and fitted ARMA models can also be computed and plotted with the aid of ITSM. The spectral density of the model is determined by selecting the option Spectrum>Model. Estimation of the spectral density from observations of a stationary series can be carried out in two ways, either by fitting an ARMA model as already described and computing the spectral density of the fitted model (Section 4.4) or by computing the periodogram of the data and smoothing (Section 4.2). The latter method is applied by selecting the option Spectrum>Smoothed Periodogram. Examples of both approaches are given in Chapter 4.

# E.7 Multivariate Time Series

Observations $\left\{ \mathbf { x } _ { 1 } , \ldots , \mathbf { x } _ { n } \right\}$ of an m-component time series must be stored as an ASCII file with $n$ rows and m columns, with at least one space between entries in the same row. To open a multivariate series for analysis, select File>Project>Open>Multivariate and click OK. Then double-click on the file containing the data, and you will be asked to enter the number of columns (m) in the data file. After doing this, click OK, and you will see graphs of each component of the series, with the multivariate tool bar at the top of the ITSM screen. For examples of the application of ITSM to the analysis of multivariate series, see Chapter 8.

# References

Akaike, H.(1969). Fitting autoregressive models for prediction. Annals of the Institute of Statistical Mathematics, 21, 243–247.   
Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In B. N. Petrov & F. Csaki (Eds.), 2nd International Symposium on Information Theory (pp. 267–281). Budapest: Akademiai Kiado.   
Akaike, H. (1978). Time series analysis and control through parametric models. In D. F. Findley (Ed.), Applied time series analysis. New York: Academic.   
Andersen, T. G., & Benzoni, L. (2009). Realized volatility. In T. G. Andersen, R. A. Davis, J.-P. Kreiss, & T. V. Mikosch (Eds.), Handbook of financial time series (pp. 555–576). Berlin, Heidelberg: Springer.   
Andersen, T. G., Davis, R. A., Kreiss, J.-P., & Mikosch, T. V. (Eds.) (2009). Handbook of financial time series. Berlin: Springer.   
Anderson, T. W. (1971). The statistical analysis of time series. New York: Wiley.   
Anderson, T. W. (1980). Maximum likelihood estimation for vector autoregressive moving-average models. In D. R. Brillinger & G. C. Tiao (Eds.), Directions in time series (pp. 80–111). Beachwood: Institute of Mathematical Statistics.   
Ansley, C. F. (1979). An algorithm for the exact likelihood of a mixed autoregressivemoving-average process. Biometrika, 66, 59–65.   
Ansley, C. F., & Kohn, R. (1985). On the estimation of ARIMA models with missing values. In E. Parzen (Ed.), Time series analysis of irregularly observed data. Springer lecture notes in statistics (Vol. 25, pp. 9–37), Springer-Verlag, Berlin, Heidelberg, New York.   
Aoki, M. (1987). State space modeling of time series. Berlin: Springer.   
Applebaum, D. Lévy processes and stochastic calculus. Cambridge: Cambridge University Press.   
Atkins, S. M. (1979). Case study on the use of intervention analysis applied to traffic accidents. Journal of the Operations Research Society, 30(7), 651–659.   
Bachelier, L. (1900). Théorie de la spéculation. Annales de lÉcole Normale Supérieure, 17, 21–86.   
Baillie, R. T., Bollerslev, T., & Mikkelsen, H. O. (1996). Fractionally integrated generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, 74, 3–30.   
Barndorff-Nielsen, O. E. (1978). Information and exponential families in statistical theory. New York: Wiley.

Barndorff-Niesen, O. E., & Shephard, N. (2001). Non-Gaussian Ornstein–Uhlenbeck based models and some of their uses in financial economics (with discussion). Journal of the Royal Statistical Society Series B, 63, 167–241.   
Bertoin, J. (1996). Lévy processes. Cambridge: Cambridge University Press.   
Bhattacharyya, M. N., & Layton, A. P. (1979). Effectiveness of seat belt legislation on the Queensland road toll—An Australian case study in intervention analysis. Journal of the American Statistical Association, 74, 596–603.   
Billingsley, P. (1995). Probability and measure (3rd ed.). New York: Wiley.   
Black, F., & Scholes, M. (1973). The pricing of options and corporate liabilities. Journal of Political Economy, 81, 637–654.   
Bloomfield, P. (2000). Fourier analysis of time series: An introduction (2nd ed.). New York: Wiley.   
Bollerslev, T. (1986), Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, 31, 307–327.   
Bollerslev, T., & Mikkelsen, H. O. (1996). Modeling and pricing long memory in stock market volatility. Journal of Econometrics, 73, 151–184.   
Box, G. E. P., & Cox, D. R. (1964). An analysis of transformations (with discussion). Journal of the Royal Statistical Society B, 26, 211–252.   
Box, G. E. P., & Jenkins, G. M. (1976). Time series analysis: Forecasting and control (revised edition). San Francisco: Holden-Day.   
Box, G. E. P., & Pierce, D. A. (1970). Distribution of residual autocorrelations in autoregressive-integrated moving-average time series models. Journal of the American Statistical Association, 65, 1509–1526.   
Box, G. E. P., & Tiao, G. C. (1975). Intervention analysis with applications to economic and environmental problems. Journal of the American Statistical Association, 70, 70–79.   
Breidt, F. J., & Davis, R. A. (1992). Time reversibility, identifiability and independence of innovations for stationary time series. Journal of Time Series Analysis, 13, 377–390.   
Brockwell, P.J. (2014), Recent results in the theory and applications of CARMA processes, Ann. Inst. Stat. Math. 66, 637–685.   
Brockwell, P. J., Chadraa, E., & Lindner, A. (2006). Continuous-time GARCH processes. Annals of Applied Probability, 16, 790–826.   
Brockwell, P. J., & Davis, R. A. (1988). Applications of innovation representations in time series analysis. In J. N. Srivastava (Ed.), Probability and statistics, essays in honor of Franklin A. Graybill (pp. 61–84). Amsterdam: Elsevier.   
Brockwell, P. J., & Davis, R. A. (1991). Time series: Theory and methods (2nd ed.). New York: Springer.   
Brockwell, P. J., & Lindner, A. (2009). Existence and uniqueness of stationary Lévydriven CARMA processes. Stochastic Processes and Their Applications, 119, 2660–2681.   
Brockwell, P. J., & Lindner, A. (2012). Integration of CARMA processes and spot volatility modelling. Journal of Time Series Analysis, 34, 156–167.   
Campbell, J., Lo, A., & McKinlay, C. (1996). The econometrics of financial markets. Princeton, NJ: Princeton University Press.   
Chan, K. S., & Ledolter, J. (1995). Monte Carlo EM estimation for time series models involving counts. Journal of the American Statistical Association, 90, 242–252.   
Chan, K. S., & Tong, H. (1987). A note on embedding a discrete parameter ARMA model in a continuous parameter ARMA model. Journal of Time Series Analysis, 8, 277–281.

Cochran, D., & Orcutt, G. H. (1949). Applications of least squares regression to relationships containing autocorrelated errors. Journal of the American Statistical Association, 44, 32–61.   
Davis, M., & Etheridge, A. (2006). Louis Bachelier’s theory of speculation: The origins of modern finance. Princeton, NJ: Princeton University Press.   
Davis, M. H. A., & Vinter, R. B. (1985). Stochastic modelling and control. London: Chapman and Hall.   
Davis, R. A., Chen, M., & Dunsmuir, W. T. M. (1995). Inference for MA(1) processes with a root on or near the unit circle. Probability and Mathematical Statistics, 15, 227–242.   
Davis, R. A., Chen, M., & Dunsmuir, W. T. M. (1996). Inference for seasonal movingaverage models with a unit root. In Athens conference on applied probability and time series, volume 2: Time series analysis. Lecture notes in statistics (Vol. 115, pp. 160–176). Berlin: Springer.   
Davis, R. A., & Dunsmuir, W. T. M. (1996). Maximum likelihood estimation for MA(1) processes with a root on or near the unit circle. Econometric Theory, 12, 1–29.   
Davis, R. A., & Mikosch, T. V. (2009). Probabilistic properties of stochastic volatility models. In T. G. Andersen, R. A. Davis, J.-P. Kreiss, & T. V. Mikosch (Eds.), Handbook of financial time series (pp. 255–268). Berlin: Springer.   
de Gooijer, J. G., Abraham, B., Gould, A., & Robinson, L. (1985). Methods of determining the order of an autoregressive-moving-average process: A survey. International Statistical Review, 53, 301–329.   
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39, 1–38.   
Dickey, D. A., & Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series with a unit root. Journal of American Statistical Association, 74, 427–431.   
Douc, R., Roueff, F., & Soulier, P. (2008). On the existence of some ARCH(∞) processes. Stochastic Processes and Their Applications, 118, 755–761.   
Duong, Q. P. (1984). On the choice of the order of autoregressive models: a ranking and selection approach. Journal of Time Series Analysis, 5, 145–157.   
Eberlein, E. (2009). Jump-type Lévy processes. In T. G. Andersen, R. A. Davis, J.-P. Kreiss, & T. V. Mikosch (Eds.), Handbook of financial time series (pp. 439–456). Berlin: Springer.   
Eller, J. (1987). On functions of companion matrices. Linear Algebra and Applications, 96, 191–210.   
Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of UK inflation. Econometrica, 50, 987–1007.   
Engle, R. F. (1995). ARCH: Selected readings. Advanced texts in econometrics. Oxford: Oxford University Press.   
Engle, R. F., & Bollerslev, T. (1986). Modelling the persistence of conditional variances. Economic Review, 5, 1–50.   
Engle, R. F., & Granger, C. W. J. (1987). Co-integration and error correction: Representation, estimation and testing. Econometrica, 55, 251–276.   
Engle, R. F., & Granger, C. W. J. (1991). Long-run economic relationships. Advanced texts in econometrics. Oxford: Oxford University Press.   
Francq, C., & Zakoian, J.-M. (2010). GARCH models: Structure, statistical inference and financial applications. New York: Wiley.   
Fuller, W. A. (1976). Introduction to statistical time series. New York: Wiley.

Gourieroux, C. (1997). ARCH models and financial applications. New York: Springer.   
Granger, C. W. J. (1981). Some properties of time series data and their use in econometric model specification. Journal of Econometrics, 16, 121–130.   
Gray, H. L., Kelley, G. D., & McIntire, D. D. (1978). A new approach to ARMA modeling. Communications in Statistics, B7, 1–77.   
Graybill, F. A. (1983). Matrices with applications in statistics. Belmont, CA: Wadsworth.   
Grunwald, G. K., Hyndman, R. J., & Hamza, K. (1994). Some Properties and Generalizations of Nonnegative Bayesian Time Series Models, Technical Report. Statistics Dept., Melbourne University, Parkville, Australia.   
Grunwald, G. K., Raftery, A. E., & Guttorp, P. (1993). Prediction rule for exponential family state space models. Journal of the Royal Statistical Society B, 55, 937–943.   
Hannan, E. J. (1980). The estimation of the order of an ARMA process. Annals of Applied Statistics, 8, 1071–1081.   
Hannan, E. J., & Deistler, M. (1988). The statistical theory of linear systems. New York: Wiley.   
Hannan, E. J., & Rissanen, J. (1982). Recursive estimation of mixed autoregressive moving-average order. Biometrika, 69, 81–94.   
Harvey, A. C. (1990). Forecasting, structural time series models and the Kalman filter. Cambridge: Cambridge University Press.   
Harvey, A. C., & Fernandes, C. (1989). Time Series models for count data of qualitative observations. Journal of Business and Economic Statistics, 7, 407–422.   
Holt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted moving averages. ONR research memorandum (Vol. 52). Pittsburgh, PA: Carnegie Institute of Technology.   
Hurvich, C. M., & Tsai, C. L. (1989). Regression and time series model selection in small samples. Biometrika, 76, 297–307.   
Iacus, S.M. and Mercuri, L. (2015), Implementation of Lévy CARMA model in Yuima package, Comput. Stat., 30, 1111–1141.   
Jarque, C. M., & Bera, A. K. (1980). Efficient tests for normality, heteroscedasticity and serial independence of regression residuals. Economics Letters, 6, 255–259.   
Jones, R. H. (1975). Fitting autoregressions, Journal of American Statistical Association, 70, 590–592.   
Jones, R. H. (1978). Multivariate autoregression estimation using residuals. In D. F. Findley (Ed.), Applied time series analysis (pp. 139–162). New York: Academic.   
Jones, R. H. (1980). Maximum likelihood fitting of ARMA models to time series with missing observations. Technometrics, 22, 389–395.   
Kendall, M. G., & Stuart, A. (1976). The advanced theory of statistics (Vol. 3). London: Griffin.   
Kitagawa, G. (1987). Non-Gaussian state-space modeling of non-stationary time series. Journal of the American Statistical Association, 82 (with discussion), 1032–1063.   
Klebaner, F. (2005). Introduction to stochastic calculus with applications. London: Imperial College Press.   
Klüppelberg, C., Lindner, A., & Maller, R. (2004). A continuous-time GARCH process driven by a Lévy process: stationarity and second-order behaviour. Journal of Applied Probability, 41, 601–622.   
Kuk, A. Y. C., & Cheng, Y. W. (1994). The Monte Carlo Newton-Raphson Algorithm, Technical Report S94-10. Department of Statistics, U. New South Wales, Sydney, Australia.   
Lehmann, E. L. (1983). Theory of point estimation. New York: Wiley.

Lehmann, E. L. (1986). Testing statistical hypotheses (2nd ed.). New York: Wiley.   
Lindner, A. (2009). Stationarity, mixing, distributional properties and moments of GARCH(p,q)-processes. In T. G. Andersen, R. A. Davis, J.-P. Kreiss, & T. V. Mikosch (Eds.), Handbook of financial time series (pp. 233–254). Berlin: Springer.   
Liu, J. & Brockwell, P. J. (1988). The general bilinear time series model. Journal of Applied Probability, 25, 553–564.   
Ljung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in time series models. Biometrika, 65, 297–303.   
Lütkepohl, H. (1993). Introduction to multiple time series analysis (2nd ed.). Berlin: Springer.   
Mage, D. T. (1982). An objective graphical method for testing normal distributional assumptions using probability plots. American Statistician, 36, 116–120.   
Makridakis, S., Andersen, A., Carbone, R., Fildes, R., Hibon, M., Lewandowski, R., Newton, J., Parzen, E., & Winkler, R. (1984). The forecasting accuracy of major time series methods. New York: Wiley.   
Makridakis, S., Wheelwright, S. C., & Hyndman, R. J. (1997). Forecasting: Methods and applications. New York: Wiley.   
May, R. M. (1976). Simple mathematical models with very complicated dynamics. Nature, 261, 459–467.   
McCullagh, P., & Nelder, J. A. (1989). Generalized linear models (2nd ed.). London: Chapman and Hall.   
McLeod, A. I., & Li, W. K. (1983). Diagnostic checking ARMA time series models using squared-residual autocorrelations. Journal of Time Series Analysis, 4, 269– 273.   
Mendenhall, W., Wackerly, D. D., and Scheaffer, D. L. (1990). Mathematical statistics with applications (4th ed.). Belmont: Duxbury.   
Merton, R. (1973). The theory of rational option pricing. Bell Journal of Economics and Management Science, 4, 141–183.   
Mikosch, T. (1998), Elementary stochastic calculus with finance in view. Singapore: World Scientific.   
Mood, A.M., Graybill, F.A. and Boes, D.C. (1974), Introduction to the Theory of Statistics, McGraw-Hill, New York.   
Nelson, D. (1991). Conditional heteroskedasticity in asset returns: A new approach. Econometrica, 59, 347–370.   
Newton, H. J., & Parzen, E. (1984). Forecasting and time series model types of 111 economic time series. In S. Makridakis, et al. (Eds.), The forecasting accuracy of major time series methods. New York: Wiley.   
Nicholls, D. F., & Quinn, B. G. (1982). Random coefficient autoregressive models: An introduction. Springer lecture notes in statistics (Vol. 11), Springer-Verlag, Berlin, Heidelberg, New York.   
Oksendal, B. (2013). Stochastic differential equations: An introduction with applications (6th ed.). New York: Springer.   
Pantula, S. (1991). Asymptotic distributions of unit-root tests when the process is nearly stationary. Journal of Business and Economic Statistics, 9, 63–71.   
Parzen, E. (1982), ARARMA models for time series analysis and forecasting. Journal of Forecasting, 1, 67–82.   
Pole, A., West, M., & Harrison, J. (1994). Applied Bayesian forecasting and time series analysis. New York: Chapman and Hall.   
Priestley, M. B. (1988). Non-linear and non-stationary time series analysis. London: Academic.

Protter, P. E. (2010). Stochastic integration and differential equations (2nd ed.). New York: Springer.   
Rosenblatt, M. (1985). Stationary sequences and random fields. Boston: Birkhäuser.   
Said, S. E., & Dickey, D. A. (1984). Testing for unit roots in autoregressive movingaverage models with unknown order. Biometrika, 71, 599–607.   
Sakai, H., & Tokumaru, H. (1980). Autocorrelations of a certain chaos. In IEEE Transactions on Acoustics, Speech and Signal Processing (Vol. 28, pp. 588–590).   
Samuelson, P. A. (1965). Rational theory of warrant pricing. Industrial Management Review, 6, 13–31.   
Sato, K. (1999). Lévy processes and infinitely divisible distributions . Cambridge: Cambridge University Press.   
Schoutens, W. (2003). Lévy processes in finance. New York: Wiley.   
Schwert, G. W. (1987). Effects of model specification on tests for unit roots in macroeconomic data. Journal of Monetary Economics, 20, 73–103.   
Shapiro, S. S., & Francia, R. S. (1972). An approximate analysis of variance test for normality. Journal of the American Statistical Association, 67, 215–216.   
Shephard, N. (1996). Statistical aspects of ARCH and stochastic volatility. In D. R. Cox, D. V. Hinkley, & O. E. Barndorff-Nielsen (Eds.), Time series models in econometrics, finance and other fields (pp. 1–67). London: Chapman and Hall.   
Shephard, N., & Andersen, T. G. (2009). Stochastic volatility: Origins and overview. In T. G. Andersen, R. A. Davis, J.-P. Kreiss, & T. V. Mikosch (Eds.), Handbook of financial time series (pp. 233–254). Berlin, Heidelberg: Springer.   
Shibata, R. (1976), Selection of the order of an autoregressive model by Akaike’s information criterion. Biometrika, 63, 117–126.   
Shibata, R. (1980), Asymptotically efficient selection of the order of the model for estimating parameters of a linear process. Annals of Statistics, 8, 147–164.   
Silvey, S. D. (1975). Statistical inference. New York: Halsted.   
Smith, J. Q. (1979). A generalization of the Bayesian steady forecasting model. Journal of the Royal Statistical Society B, 41, 375–387.   
Sorenson, H. W., & Alspach, D. L. (1971). Recursive Bayesian estimation using Gaussian sums. Automatica, 7, 465–479.   
Subba-Rao, T., & Gabr, M. M. (1984). An introduction to bispectral analysis and bilinear time series models. Springer lecture notes in statistics (Vol. 24), Springer-Verlag, Berlin, Heidelberg, New York.   
Tam, W. K., & Reinsel, G. C. (1997). Tests for seasonal moving-average unit root in ARIMA models. Journal of the American Statistical Association, 92, 725–738.   
Tanaka, K. (1990). Testing for a moving-average unit root. Econometric Theory, 9, 433–444.   
Taylor, S. J. (1982). Financial returns modelled by the product of two stochastic processes-a study of the daily sugar prices 1961–75. Time Series Analysis: Theory and Practice, 1, 203–226.   
Taylor, S. J (1986). Modelling financial time series. New York: Wiley.   
Tong, H. (1990). Non-linear time series: A dynamical systems approach. Oxford: Oxford University Press.   
Venables, W. N., Ripley, B. D. (2003). Modern applied statistics with S (4th ed.). New York: Springer.   
Weigt, G. (2015), ITSM-R Reference Manual. The manual can be downloaded from http://eigenmath.sourceforge.net/itsmr-refman.pdf.   
Weiss, A. A. (1986). Asymptotic theory for ARCH models: Estimation and testing. Econometric Theory, 2, 107–131.

West, M., & Harrison, P. J. (1989). Bayesian forecasting and dynamic models. New York: Springer.   
Whittle, P. (1963). On the fitting of multivariate autoregressions and the approximate canonical factorization of a spectral density matrix. Biometrika, 40, 129–134.   
Wichern, D., & Jones, R. H. (1978). Assessing the impact of market disturbances using intervention analysis. Management Science, 24, 320–337.   
Wu, C. F. J. (1983). On the convergence of the EM algorithm. Annals of Statistics, 11, 95–103.   
Zeger, S. L. (1988). A regression model for time series of counts. Biometrika, 75, 621–629.

# Index

# A

Accidental deaths (DEATHS.TSM), 2, 11, 27, 28, 96, 180, 183, 313, 315, 317

ACF. See Autocorrelation function (ACF)

AIC, 149, 152

AICC, 125, 134, 137, 142, 151, 153, 167, 248, 396

Airline passenger data (AIRPASS.TSM), 192, 278, 320, 386

All Ordinaries index, 228

All-star baseball games, 2, 7

Alternative hypothesis, 368

APPH.TSM, 256

APPI.TSM, 256

APPJ.TSM, 350

APPJK2.TSM, 256

APPK.TSM, 350

Arbitrage, 221, 222, 224

ARAR algorithm, 310–313

forecasting, 311–312

application of, 312–313

ARCH(1) process, 197–199

ARCH(p) process, 197, 224

ARCH.TSM, 200

AR(1) process, 15, 46, 54, 295

ACVF of, 46

causal, 47

confidence regions for coefficients, 142

estimation of mean, 51

estimation of missing value, 58, 282

observation driven model of, 295

plus noise, 69

prediction of, 57, 59

sample ACF of, 54

spectral density of, 103

state-space representation of, 261

with missing data, 58, 285, 287

with non-zero mean, 59–60

AR(2) process, 19, 76–77

ACVF of, 80–82

AR(p) process. See Autoregressive (AR(p)) process

ARIMA(1,1,0) process, 158

forecast of, 176–177

ARIMA process

definition, 158

forecasting, 173–177

seasonal (see Seasonal ARIMA models)

state-space representation of, 269

with missing observations, 280

with regression, 184, 203

ARIMA(p, d, q) process with $( - . 5 < d < . 5$ ). See Fractionally integrated ARMA process

ARMA(1, 1) process, 48–50, 66, 76

ACVF of, 78

causal, 49, 66

invertible, 49, 76

noncausal, 49, 118

noninvertible, 49, 118

prediction of, 89

spectral density of, 116

state-space representation of, 261–262

ARMA(p, q) process

ACVF of, 78–82

coefficients in AR representation, 77

coefficients in MA representation, 76

causal, 75

definition, 74

estimation

Hannan-Rissanen, 137–139

innovations algorithm, 132–137

least squares, 141

maximum likelihood, 139–140

existence and uniqueness of, 75

invertible, 76

multivariate (see Multivariate ARMA processes)

order selection, 124–129, 151

prediction, 91–96

seasonal (see Seasonal ARIMA models)

spectral density of, 115

state-space representation of, 267–268

with mean $\mu$ , 74

Asymptotic relative efficiency, 129

Australian red wine sales (WINE.TSM), 2, 7, 18, 319, 320

Autocorrelation function (ACF)

definition, 16

sample ACF

of absolute values, 202–204, 396, 401

of squares, 200, 202–204, 401

approximate distribution of, 51

of MA(q), 79, 82

Autocovariance function (ACVF)

basic properties of, 39

Autocovariance function (ACVF) (cont.)

characterization of, 41

definition, 13, 16, 45

nonnegative definite, 41

of ARMA processes, 78–87

of ARMA(1, 1) process, 80

of AR(2) process, 81

of MA(q) process, 79

of MA(1) process, 15, 42

sample, 59–60

spectral representation of, 101

Autofit

for ARMA fitting, 122, 142, 143, 167–169, 187, 203, 341, 392

for fractionally integrated ARMA, 341

Autoregressive integrated moving-average. See ARIMA process

Autoregressive moving-average. See ARMA process

Autoregressive polynomial, 74–77, 143, 160, 162, 169, 170, 304, 346

Autoregressive (AR(p)) process, 74

estimation of parameters Burg, 122

maximum likelihood, 142–143, 149

with missing observations, 284

Yule-Walker, 123–124

large-sample distributions, 124

confidence intervals, 131

one-step prediction of, 59

order selection, 134, 149, 150

minimum AICC model, 153

multivariate (see Multivariate AR models)

partial autocorrelation function of, 83

prediction of, 89

state-space representation, 266–267

subset models, 309, 311

unit roots in, 170

Yule-Walker equations, 124, 128

Autoregressive process of infinite order $( \mathrm { A R } ( \infty ) )$ ), 236

B

Backward prediction errors, 130

Backward shift operator, 25, 44, 48, 74, 207, 311, 339

Bandwidth, 109

Bartlett’s formula, 53

AR(1), 54

independent white noise, 53

MA(1), 53

multivariate, 240

Bayesian state-space model, 289

BEER.TSM, 192

Best linear predictor, 40, 246–247

Beta function, 307

Beta-binomial distribution, 307

BIC criterion, 149, 152

Bilinear model, 338

Binary process, 7

Binomial distribution, 353

Bispectral density, 337

Bivariate normal distribution, 359–360

Bivariate time series

covariance matrix, 228

mean vector, 228

(weakly) stationary, 228

Box-Cox transformation, 165, 388, 389

Brownian motion, 213, 375–379

Burg’s algorithm, 129–132

C

CAR(1) process, 343–345

estimation of, 344

CARMA(p, q) process, 345

autocovariance function of, 347

mean of, 346, 347

with thresholds, 347

Cauchy criterion, 371–372

Causal

ARCH(1) process, 198, 199

ARMA process, 75

GARCH process, 208, 209

multivariate ARMA process, 244

time-invariant linear filter, 112

Chaotic deterministic sequence, 335–337

Checking for normality, 32

Chi-squared distribution, 31, 125, 353, 361

Classical decomposition, 20, 26, 29, 165, 389, 390

Cochran and Orcutt procedure, 186

Cointegration, 254–255

Cointegration vector, 254, 255

Compound Poisson process, 214–215

Conditional density, 289, 294, 299, 356

Conditional expectation, 40, 149, 285, 289, 357

Confidence interval, 367–368

large-sample confidence region, 368

Conjugate family of priors, 297

Consistent estimator, 108, 133, 237

Continuous distributions

chi-squared, 353

exponential, 352

gamma, 352–353

normal, 352

uniform, 352

Continuous spectrum, 101

Continuous-time ARMA process. See CARMA(p, q) process

Continuous-time models, 212–220

CAR(1), 343

Covariance function, 13. See also Autocovariance function (ACVF)

Covariance matrix, 376

factorization of, 358

properties of, 358

square root of, 358

Cumulant, 337, 350

kth-order, 337

Delay parameter, 325, 327, 331

Design matrix, 184–186

Deterministic, 67

Diagnostic checking, 144–147. See also Residuals

Difference operator

first-order, 25

with positive lag d, 28

with real lag d, 28, 343

Differencing to generate stationary data, 12 at lag d, 28

Dirichlet kernel, 114

Discrete distributions binomial, 353

negative binomial, 353–354, 361

Poisson, 353

uniform, 353

Discrete Fourier transform, 107

Discrete spectral average. See Spectral density function

Distribution function, 351–355. See also Continuous distributions; Discrete distributions properties of, 351

Dow-Jones Utilities Index (DOWJ.TSM), 126–128, 131, 134–137, 143, 176, 177

Dow-Jones and All ordinaries Indices, (DJAO2.TSM, DJAOPC2.TSM), 228–229, 249, 251–253

Durbin-Levinson algorithm, 60–62, 64, 65, 125, 126, 132, 133, 247

# E

EGARCH (p, q) process, 205

Elementary function, 377–379

EM algorithm, 260, 285–288

Monte Carlo (MCEM), 293

Embedded discrete-time process, 344

Equivalent martingale measure, 223

Error probabilities, 389–390 type I, 369

type II, 369

Estimation of missing values

in an ARIMA process, 284

in an AR(p) process, 285

in a state-space model, 283

Estimation of the white noise variance least squares, 140

maximum likelihood, 141

using Burg’s algorithm, 130

using the Hannan-Rissanen algorithm, 138

using the innovations algorithm, 136

using the Yule-Walker equations, 124

European call option, 221, 223

Expectation, 351–355

Exponential distribution, 352

Exponential family models, 296–303

Exponential smoothing, 21, 23, 316

# F

FIGARCH (p, d, q) process, 209

Filter. See Linear filter

Financial data, 195–226

Fisher information matrix, 367

Forecasting, 12, 55–67, 147–148. See also Prediction

Forecasting ARIMA processes, 173–177 forecast function, 182–183

h-step predictor, 175

mean square error of, 174

forecast density, 289

forward prediction errors, 130

Fourier frequencies, 107, 109

Fourier indices, 11

Fractionally integrated ARMA process, 339 estimation of, 340

spectral density of, 340

Whittle likelihood approximation, 340

Fractionally integrated white noise, 339

autocovariance of, 339

variance of, 339

Frequency domain, 97, 236

# G

Gamma distribution, 206, 218, 295, 352

Gamma function, 339, 352

$\mathrm { G A R C H } ( p , q )$ process, 200

ARMA model with GARCH noise, 203–204

fitting GARCH models, 201–203

Gaussian-driven, 203

generalizations, 203

regression with GARCH noise, 203

t driven, 203

Gaussian likelihood

in time series context, 367

of a CAR(1) process, 343–344

of a multivariate AR process, 248

of an ARMA(p, q) process, 140

with missing observations, 281–283

of GARCH model, 340

of regression with ARMA errors, 186, 187

Gaussian linear process, 334, 335, 337

Gaussian time series, 40, 42, 139, 242, 361

Gauss-Markov theorem, 365, 367

Generalized distribution function, 101

Generalized error distribution (GED), 206

Generalized least squares (GLS) estimation, 184–186

Generalized inverse, 184, 271, 304

Generalized state-space models

Bayesian, 289

filtering, 289, 290

forecast density, 289

observation-driven, 294–295

parameter-driven, 288–294

prediction, 289, 290

Geometric Brownian motion (GBM), 195, 196, 215–217, 381, 382

Gibbs phenomenon, 114

Goals scored by England against Scotland, 299–302

Goodness of fit based on ACF, 18–19. See also Tests of randomness

# H

Hannan-Rissanen algorithm, 122, 136, 137–139

Harmonic regression, 10–12

Hessian matrix, 142, 187

Hidden process, 289

Holt-Winters algorithm, 314–317

seasonal, 317–318

Hypothesis testing, 368–370

large-sample tests based on confidence regions, 369–370

uniformly most powerful test, 369

# I

${ \mathrm { I A R C H } } ( \infty )$ process, 209

$\mathrm { I G A R C H } ( p , q )$ process, 208

Independent random variables, 30, 36, 214

Identification techniques, 163–169

for ARMA processes, 164

Identification techniques (cont.)

for AR(p) processes, 142

for MA(q) processes, 153

for seasonal ARIMA processes, 177

$\operatorname { I G A R C H } ( p , q )$ process, 208, 209

iid noise, 6–7, 14

sample ACF of, 53

multivariate, 235

Innovations, 62, 271

Innovations algorithm, 62–65, 132–137

fitted innovations MA(m) model, 133

multivariate, 247

Input, 45, 112, 333

Integrated volatility, 217, 218, 220, 226

Intervention analysis, 331–334

Invertible

ARMA process, 76

multivariate ARMA process, 244

Investment strategy, 221, 222, 224

Itô calculus, 373

Itô integral, 343, 375–379

Itô process, 379–380

Itô’s formula, 380

Itô stochastic differential equation, 381–383

ITSM, 27–33, 37, 122, 125–127, 165, 327, 329,

385–407

J

Joint distributions of a time series, 6

Joint distribution of a random vector, 355

K

Kalman recursions

filtering, 271, 274

prediction, 271

h step, 272–275

smoothing, 271, 275

Kullback-Leibler discrepancy, 151

Kullback-Leibler index, 151, 152

L

Lake Huron (LAKE.TSM), 9–10, 18–19, 54, 189, 191

Latent process, 289

Large-sample tests based on confidence regions, 369–370

Least squares estimation

for ARMA processes, 141

for regression model, 186

for transfer function models, 326

of trend, 8

Lévy process, 195, 212–218, 347–350, 375–377

Lévy-Itô decomposition, 214

Lévy-Khinchin representation, 374

Lévy market model (LMM), 216

Lévy measure, 374

Likelihood function, 277, 292, 366. See also

Gaussian likelihood

Linear combination of sinusoids, 101–103

Linear difference equations, 47, 175

Linear filter, 36, 45, 48, 74

input, 45

low-pass, 23, 114

moving-average, 22, 36

output, 45

simple moving-average, 112–114

Linear process, 44, 335

ACVF of, 46

Gaussian, 334

multivariate, 235

Linear regression. See Regression

Local level model, 264

Local linear trend model, 264, 265, 304, 315

Log asset price, 195, 197, 212, 216

Log return, 195–197, 209, 219

Logistic equation, 335, 336

Lognormal SV process, 210, 211–212, 274

Long memory, 207–209, 310, 323

Long-memory model, 338–342

M

MA(1) process

ACF of, 42

estimation of missing values, 71

moment estimation, 128–129

noninvertible, 85, 95

order selection, 128–129

PACF of, 84

sample ACF of, 53

spectral density of, 105–106

state-space representation of, 273

MA(q). See Moving average (MA(q)) process

MA(∞), 44

multivariate, 235

Market price of risk, 224

Martingale difference sequence, 334

Maximum likelihood estimation, 366–367

ARMA processes, 140

large-sample distribution of, 142

confidence regions for, 142–144

Mean

of a multivariate time series, 236

estimation of, 236–243

of a random variable, 352, 354

of a random vector, 357

estimation of, 50

sample, 50

large-sample properties of, 51, 236

Mean square convergence, 65, 371–372

properties of, 372, 378

Measurement error, 84–86, 172

Memory shortening, 309, 310

Method of moments estimation, 85, 129

Minimum AICC AR model, 147, 256, 396, 402

Mink trappings (APPH.TSM), 256

Missing values in ARMA processes

estimation of, 283–285

likelihood calculation with, 281–283

Mixture distribution, 354

Monte Carlo EM algorithm (MCEM), 293

Moving average (MA(q)) process, 43

ACF of, 79

sample, 82

ACVF of, 79

estimation

confidence intervals, 143

Hannan-Rissanen, 137

innovations, 133

maximum likelihood, 140, 142

order selection, 133, 134

partial autocorrelation of, 83

unit roots in, 171–173

Multivariate AR process

estimation, 247–249

Burg’s algorithm, 248

maximum likelihood, 248

Whittle’s algorithm, 248

forecasting, 250–254

error covariance matrix of prediction, 251

Multivariate ARMA process, 243–246

causal, 244

covariance matrix function of, 245–246

estimation

maximum likelihood, 248

invertible, 244

prediction, 246–247

error covariance matrix of prediction, 252

Multivariate innovations algorithm, 247

Multivariate normal distribution

bivariate, 359–360

conditional distribution, 360

conditional expectation, 357

density function, 356

definition, 358

singular, 359

standardized, 359

Multivariate time series, 223

covariance matrices of, 228, 233, 235

mean vectors of, 228, 233, 235

second-order properties of, 232–236

stationary, 233

Multivariate white noise, 227, 235, 243

Muskrat trappings (APPI.TSM), 256

# N

Negative binomial distribution, 292, 353, 361

NILE.TSM, 340–342

NOISE.TSM, 326, 333

Non-anticipating integrand, 376

Nonlinear models, 334–338

Nonnegative definite matrix, 357, 358

Nonnegative definite function, 41

Normal distribution, 352, 355

Normal equations, 363, 364

Null hypothesis, 34, 147, 170, 172, 337, 368, 369

# O

Observation equation, 260

of CARMA(p, q) model, 345

Ordinary least squares (OLS) estimators, 170, 184, 185, 363

One-step predictors, 60, 63, 88, 136, 140, 149, 174, 252, 271, 280, 281, 302

Order selection, 124, 133, 137, 141, 149–153

AIC, 149, 152

AICC, 149, 151–153

BIC, 149, 152

consistent, 152

efficient, 152

FPE, 149–150

Ornstein-Uhlenbeck process, 217, 218, 220, 225, 343

Ornstein-Uhlenbeck SV model, 219

Orthogonal increment process, 103

Orthonormal set, 107

Overdifferencing, 169, 171

Overdispersed, 299

Overshorts (OSHORTS.TSM), 84–86, 128, 147, 148, 172, 187–188

structural model for, 85

# P

Partial autocorrelation function (PACF), 62, 83–84

estimation of, 85

of an AR(p) process, 83

of an MA(1) process, 84

sample, 83, 84

Periodogram, 106–111, 208, 340

approximate distribution of, 108

Point estimate, 367

Poisson distribution, 296, 299, 300, 355

Poisson exponential family model, 296

Poisson process, 195, 213–215, 217, 351, 374

Polynomial fitting, 24–25

Population of USA (USPOP.TSM), 4, 8–9, 25–26

Portmanteau test for residuals. See Tests of randomness

Posterior distribution, 289, 294, 297, 298, 306, 307

Power function, 369

Power steady model, 298, 299

Prediction of stationary processes. See also Recursive prediction

AR(p) processes, 89

ARIMA processes, 173–177

ARMA processes, 87–94

based on infinite past, 65

best linear predictor, 40

Gaussian processes, 94

prediction bounds, 94

large-sample approximations, 93

MA(q) processes, 89

multivariate AR processes, 250–254

one-step predictors, 57

mean squared error of, 92

seasonal ARIMA processes, 182–183

Prediction operator, 58–60

properties of, 59

Preliminary transformations, 12, 20, 163

Prewhitening, 239, 324

Prior distribution, 289

Probability density function (pdf), 354

Probability generating function, 361

Probability mass function (pmf), 354

Purely nondeterministic, 67, 334

# Q

q dependent, 43

q-correlated, 43, 44

qq plot, 32, 147, 202–203, 401–402

# R

R and S arrays, 157

Random noise component, 20, 389

Random variable

continuous, 353, 354

discrete, 354, 355

Randomly varying trend and seasonality with noise, 266, 317

Random vector, 355–358

covariance matrix of, 357

joint distribution of, 355

mean of, 357

probability density of, 356

Random walk, 7, 14

simple symmetric, 7, 14

with noise, 263, 272, 278, 299

Rational spectral density. See Spectral density function

Realization of a time series, 6, 409

Realized volatility, 217

Recursive prediction

Durbin-Levinson algorithm, 60, 247

Innovations algorithm, 64–65

Kalman prediction (see Kalman recursions)

multivariate processes

Durbin-Levinson algorithm, 247

innovations algorithm, 247

Regression

with ARMA errors, 184–191

best linear unbiased estimator, 185

Cochrane and Orcutt procedure, 185, 186

GLS estimation, 185

OLS estimation, 184

Rejection region, 368, 369

RES.TSM, 333, 334

Residuals, 30, 144

check for normality, 33, 147

graph of, 145

rescaled, 145

sample ACF of, 146

tests of randomness for, 146–147

Risk-neutral, 224

# S

Sales with leading indicator (LS2.TSM, SALES.TSM, LEAD.TSM), 230– 232, 242–243, 249–250, 253–254, 326–327, 329–330

Sample

autocorrelation function, 16–18 MA(q), 82 of residuals, 146

autocovariance function, 16

covariance matrix, 16

mean, 16

large-sample properties of, 50

multivariate, 228

partial autocorrelation, 83

SARIMA. See Seasonal ARIMA process

Seasonal adjustment, 5

Seasonal ARIMA process, 177–183 forecasting, 182–183

mean squared error of, 183

maximum likelihood estimation, 180, 181

Seasonal component, 20

estimation of, 21–25

method S1, 26–27

elimination of, 28

method S2, 28–30

Seat-belt legislation (SBL.TSM, SBL2.TSM), 189–191, 333–334

Second-order properties, 6

in frequency domain, 236

Self-financing condition, 221

Short memory, 313, 339

SIGNAL.TSM, 3, 33

Signal detection, 3

Significance level, 153, 369, 402

Size of a test, 369

Smoothing

by elimination of high-frequency components, 23–24

with a moving average filter, 21–23

exponential, 21, 23, 314, 316, 319

the periodogram (see Spectral density estimation)

using a simple moving average, 112

Spectral density estimation

discrete spectral average, 109

large-sample properties of, 110

rational, 117

Spectral density function, 98–106

characterization of, 99

of an ARMA(1, 1), 116

of an ARMA process, 115

of an AR(1), 103–105

of an AR(2), 116

of an MA(1), 105–106

of white noise, 103

properties of, 98

rational, 115

Spectral density matrix function, 236

Spectral distribution function, 101–103, 117

Spectral representation

of an autocovariance function, 101

of a covariance matrix function, 233

of a stationary multivariate time series, 233

of a stationary time series, 97

Spencer’s 15-point moving average, 22–23, 36

Spot volatility, 217

State equation, 260

of CARMA(p, q) model, 345

stable, 262, 267

State-space model, 259–307

estimation for, 275–280

stable, 262

stationary, 262

with missing observations, 280–285

State-space representation, 261

causal AR(p), 266–267

causal ARMA(p, q), 267–268

ARIMA(p, d, q), 268–269

Stationarity

multivariate, 227

strict, 13, 43, 361

weak, 13, 361

Steady-state solution, 273, 274, 305, 315

Stochastic differential equation, 196, 215, 218, 343, 345, 348, 383–385

first-order, 343

pth-order, 345

Stochastic volatility model, 197, 209–212, 217–220, 226, 274

Stock market indices (STOCK7.TSM), 225, 257

Strictly stationary series, 13, 43

properties of, 43

Strike price, 221

Strike time, 221

Strikes in the U.S.A. (STRIKES.TSM), 4, 22, 36, 96

Structural time series models, 85, 263

level model, 263–265

local linear trend model, 264, 265, 315

randomly varying trend and seasonality with noise, 266, 278

estimation of, 275–280

seasonal series with noise, 265

Stylized features, 196, 204

Subordinator, 217

Sunspot numbers (SUNSPOTS.TSM), 70, 86–87, 110–111, 117, 153, 203, 204, 335

# T

Testing for the independence of two stationary time series, 239–240

Test for normality, 33, 147, 400

Tests of randomness

based on sample ACF, 30

based on turning points, 31–33, 146

difference-sign test, 32, 146

Jarque-Bera normality test, 33, 146

minimum AICC AR model, 147

portmanteau tests

Ljung-Box, 31, 146, 402

McLeod-Li, 31, 146, 402

rank test, 32, 146

Third-order central moment, 337

Third-order cumulant function, 337, 350 of linear process, 337, 350

Threshold model, 338

AR(p), 338

Time domain, 97, 248

Time-invariant linear filter (TLF), 111–115

causal, 112

transfer function, 113

Time series, 6

continuous-time, 1

discrete-time, 1

Gaussian, 40, 42, 361

Time series model, 6

Time series of counts, 292–294

Transfer function, 113–115

Transfer function model, 323–330

estimation of, 324–326

prediction of, 327–330

Transformations, 20, 163–164, 388

variance-stabilizing, 165

Tree-ring widths (TRINGS.TSM), 351

Trend component, 7–10, 20

elimination of

in absence of seasonality, 21–25

by differencing, 25–26

estimation of

by elimination of high-frequency components, 23

by exponential smoothing, 23

by least squares, 10

by polynomial fitting, 24–25

by smoothing with a moving average, 21, 26

# U

Uniform distribution, 352, 353

discrete, 353–354

Uniformly most powerful (UMP) test, 369

Unit roots

augmented Dickey-Fuller test, 170

Dickey-Fuller test, 170

in autoregression, 169–171

in moving-averages, 171–173

likelihood ratio test, 172

locally best invariant unbiased (LBIU) test, 173

# V

Variance, 352, 354

Volatility, 196, 209, 216, 349

# W

Weight function, 109, 110

White noise, 14

multivariate, 235

spectral density of, 103

Whittle approximation to likelihood, 340

Wold decomposition, 44, 67, 334

# Y

Yule-Walker estimation, 86, 123–124. See also

Autoregressive process and multivariate AR process

for $q > 0$ , 128–129

#

Zoom buttons, 388