# Introduction to Time Series Analysis and Forecasting

# Second Edition

Douglas C. Montgomery

Cheryl L. Jennings

Murat Kulahci

INTRODUCTION TO TIME SERIES ANALYSIS AND FORECASTING

WILEY SERIES IN PROBABILITY AND STATISTICS

Established by WALTER A. SHEWHART and SAMUEL S. WILKS

Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice
Geof H. Givens, Harvey Goldstein, Geert Molenberghs, David W. Scott,

Adrian F. M. Smith, Ruey S. Tsay, Sanford Weisberg

Editors Emeriti: J. Stuart Hunter, Iain M. Johnstone, Joseph B. Kadane,

Jozef L. Teugels

A complete list of the titles in this series appears at the end of this volume.

# INTRODUCTION TO TIME SERIES ANALYSIS AND FORECASTING

Second Edition

# DOUGLAS C. MONTGOMERY

Arizona State University

Tempe, Arizona, USA

# CHERYL L. JENNINGS

Arizona State University

Tempe, Arizona, USA

# MURAT KULAHCI

Technical University of Denmark

Lyngby, Denmark

and

Lulea University of Technology

Lulea, Sweden

# CONTENTS

# PREFACE xi

# 1 INTRODUCTION TO FORECASTING 1

1.1 The Nature and Uses of Forecasts / 1   
1.2 Some Examples of Time Series / 6   
1.3 The Forecasting Process / 13   
1.4 Data for Forecasting / 16

1.4.1 The Data Warehouse / 16   
1.4.2 Data Cleaning / 18   
1.4.3 Imputation / 18

1.5 Resources for Forecasting / 19

Exercises / 20

# 2 STATISTICS BACKGROUND FOR FORECASTING 25

2.1 Introduction / 25   
2.2 Graphical Displays / 26

2.2.1 Time Series Plots / 26   
2.2.2 Plotting Smoothed Data / 30

2.3 Numerical Description of Time Series Data / 33

2.3.1 Stationary Time Series / 33

2.3.2 Autocovariance and Autocorrelation Functions / 36   
2.3.3 The Variogram / 42

2.4 Use of Data Transformations and Adjustments / 46

2.4.1 Transformations / 46   
2.4.2 Trend and Seasonal Adjustments / 48

2.5 General Approach to Time Series Modeling and

Forecasting / 61

2.6 Evaluating and Monitoring Forecasting Model

Performance / 64

2.6.1 Forecasting Model Evaluation / 64   
2.6.2 Choosing Between Competing Models / 74   
2.6.3 Monitoring a Forecasting Model / 77

2.7 R Commands for Chapter 2 / 84

Exercises / 96

# 3 REGRESSION ANALYSIS AND FORECASTING

107

3.1 Introduction / 107   
3.2 Least Squares Estimation in Linear Regression Models / 110   
3.3 Statistical Inference in Linear Regression / 119

3.3.1 Test for Signi-cance of Regression / 120   
3.3.2 Tests on Individual Regression Coef-cients and

Groups of Coef-cients / 123

3.3.3 Con-dence Intervals on Individual Regression   
Coef-cients / 130   
3.3.4 Con-dence Intervals on the Mean Response / 131

3.4 Prediction of New Observations / 134   
3.5 Model Adequacy Checking / 136

3.5.1 Residual Plots / 136   
3.5.2 Scaled Residuals and PRESS / 139   
3.5.3 Measures of Leverage and Inuence / 144

3.6 Variable Selection Methods in Regression / 146   
3.7 Generalized and Weighted Least Squares / 152

3.7.1 Generalized Least Squares / 153   
3.7.2 Weighted Least Squares / 156   
3.7.3 Discounted Least Squares / 161

3.8 Regression Models for General Time Series Data / 177

3.8.1 Detecting Autocorrelation: The Durbin鈥揥atson Test / 178   
3.8.2 Estimating the Parameters in Time Series Regression Models / 184

3.9 Econometric Models / 205   
3.10 R Commands for Chapter 3 / 209

Exercises / 219

# 4 EXPONENTIAL SMOOTHING METHODS

233

4.1 Introduction / 233   
4.2 First-Order Exponential Smoothing / 239

4.2.1 The Initial Value, $\tilde { y } _ { 0 }$ / 241   
4.2.2 The Value of $\lambda ~ / ~ 2 4 1$

4.3 Modeling Time Series Data / 245   
4.4 Second-Order Exponential Smoothing / 247   
4.5 Higher-Order Exponential Smoothing / 257   
4.6 Forecasting / 259

4.6.1 Constant Process / 259   
4.6.2 Linear Trend Process / 264   
4.6.3 Estimation of $\sigma _ { e } ^ { 2 }$ / 273   
4.6.4 Adaptive Updating of the Discount Factor / 274   
4.6.5 Model Assessment / 276

4.7 Exponential Smoothing for Seasonal Data / 277

4.7.1 Additive Seasonal Model / 277   
4.7.2 Multiplicative Seasonal Model / 280

4.8 Exponential Smoothing of Biosurveillance Data / 286   
4.9 Exponential Smoothers and Arima Models / 299   
4.10 R Commands for Chapter 4 / 300

Exercises / 311

# 5 AUTOREGRESSIVE INTEGRATED MOVING AVERAGE (ARIMA) MODELS

327

5.1 Introduction / 327   
5.2 Linear Models for Stationary Time Series / 328

5.2.1 Stationarity / 329   
5.2.2 Stationary Time Series / 329

5.3 Finite Order Moving Average Processes / 333

5.3.1 The First-Order Moving Average Process, MA(1) / 334   
5.3.2 The Second-Order Moving Average Process, MA(2) / 336

5.4 Finite Order Autoregressive Processes / 337

5.4.1 First-Order Autoregressive Process, AR(1) / 338   
5.4.2 Second-Order Autoregressive Process, AR(2) / 341   
5.4.3 General Autoregressive Process, $\operatorname { A R } ( p )$ / 346   
5.4.4 Partial Autocorrelation Function, PACF / 348

5.5 Mixed Autoregressive鈥揗oving Average Processes / 354

5.5.1 Stationarity of $\mathbf { A R M A } ( p , q )$ Process / 355   
5.5.2 Invertibility of $\mathbf { A R M A } ( p , q )$ Process / 355   
5.5.3 ACF and PACF of ARMA(p, q) Process / 356

5.6 Nonstationary Processes / 363

5.6.1 Some Examples of ARIMA $( p , d , q )$ Processes / 363

5.7 Time Series Model Building / 367

5.7.1 Model Identi-cation / 367   
5.7.2 Parameter Estimation / 368   
5.7.3 Diagnostic Checking / 368   
5.7.4 Examples of Building ARIMA Models / 369

5.8 Forecasting Arima Processes / 378   
5.9 Seasonal Processes / 383   
5.10 Arima Modeling of Biosurveillance Data / 393   
5.11 Final Comments / 399   
5.12 R Commands for Chapter 5 / 401

Exercises / 412

# 6 TRANSFER FUNCTIONS AND INTERVENTION MODELS

427

6.1 Introduction / 427   
6.2 Transfer Function Models / 428   
6.3 Transfer Function鈥揘oise Models / 436   
6.4 Cross-Correlation Function / 436   
6.5 Model Speci-cation / 438   
6.6 Forecasting with Transfer Function鈥揘oise Models / 456

6.7 Intervention Analysis / 462   
6.8 R Commands for Chapter 6 / 473

Exercises / 486

# 7 SURVEY OF OTHER FORECASTING METHODS 493

7.1 Multivariate Time Series Models and Forecasting / 493

7.1.1 Multivariate Stationary Process / 494   
7.1.2 Vector ARIMA Models / 494   
7.1.3 Vector AR (VAR) Models / 496

7.2 State Space Models / 502   
7.3 Arch and Garch Models / 507   
7.4 Direct Forecasting of Percentiles / 512   
7.5 Combining Forecasts to Improve Prediction Performance / 518   
7.6 Aggregation and Disaggregation of Forecasts / 522   
7.7 Neural Networks and Forecasting / 526   
7.8 Spectral Analysis / 529   
7.9 Bayesian Methods in Forecasting / 535   
7.10 Some Comments on Practical Implementation and Use of Statistical Forecasting Procedures / 542   
7.11 R Commands for Chapter 7 / 545

Exercises / 550

# APPENDIX A STATISTICAL TABLES 561

# APPENDIX B DATA SETS FOR EXERCISES 581

# APPENDIX C INTRODUCTION TO R 627

# BIBLIOGRAPHY 631

# INDEX 639

# PREFACE

Analyzing time-oriented data and forecasting future values of a time series are among the most important problems that analysts face in many -elds, ranging from -nance and economics to managing production operations, to the analysis of political and social policy sessions, to investigating the impact of humans and the policy decisions that they make on the environment. Consequently, there is a large group of people in a variety of -elds, including -nance, economics, science, engineering, statistics, and public policy who need to understand some basic concepts of time series analysis and forecasting. Unfortunately, most basic statistics and operations management books give little if any attention to time-oriented data and little guidance on forecasting. There are some very good high level books on time series analysis. These books are mostly written for technical specialists who are taking a doctoral-level course or doing research in the -eld. They tend to be very theoretical and often focus on a few speci-c topics or techniques. We have written this book to -ll the gap between these two extremes.

We have made a number of changes in this revision of the book. New material has been added on data preparation for forecasting, including dealing with outliers and missing values, use of the variogram and sections on the spectrum, and an introduction to Bayesian methods in forecasting. We have added many new exercises and examples, including new data sets in Appendix B, and edited many sections of the text to improve the clarity of the presentation.

Like the -rst edition, this book is intended for practitioners who make real-world forecasts. We have attempted to keep the mathematical level modest to encourage a variety of users for the book. Our focus is on shortto medium-term forecasting where statistical methods are useful. Since many organizations can improve their effectiveness and business results by making better short- to medium-term forecasts, this book should be useful to a wide variety of professionals. The book can also be used as a textbook for an applied forecasting and time series analysis course at the advanced undergraduate or -rst-year graduate level. Students in this course could come from engineering, business, statistics, operations research, mathematics, computer science, and any area of application where making forecasts is important. Readers need a background in basic statistics (previous exposure to linear regression would be helpful, but not essential), and some knowledge of matrix algebra, although matrices appear mostly in the chapter on regression, and if one is interested mainly in the results, the details involving matrix manipulation can be skipped. Integrals and derivatives appear in a few places in the book, but no detailed working knowledge of calculus is required.

Successful time series analysis and forecasting requires that the analyst interact with computer software. The techniques and algorithms are just not suitable to manual calculations. We have chosen to demonstrate the techniques presented using three packages: Minitab庐, $\mathbf { J } \mathbf { M P } ^ { \mathbb { ( B ) } }$ , and R, and occasionally $\mathbf { S A S } ^ { \mathbb { ( B ) } }$ . We have selected these packages because they are widely used in practice and because they have generally good capability for analyzing time series data and generating forecasts. Because R is increasingly popular in statistics courses, we have included a section in each chapter showing the R code necessary for working some of the examples in the chapter. We have also added a brief appendix on the use of R. The basic principles that underlie most of our presentation are not speci-c to any particular software package. Readers can use any software that they like or have available that has basic statistical forecasting capability. While the text examples do utilize these particular software packages and illustrate some of their features and capability, these features or similar ones are found in many other software packages.

There are three basic approaches to generating forecasts: regressionbased methods, heuristic smoothing methods, and general time series models. Because all three of these basic approaches are useful, we give an introduction to all of them. Chapter 1 introduces the basic forecasting problem, de-nes terminology, and illustrates many of the common features of time series data. Chapter 2 contains many of the basic statistical tools used in analyzing time series data. Topics include plots, numerical

summaries of time series data including the autocovariance and autocorrelation functions, transformations, differencing, and decomposing a time series into trend and seasonal components. We also introduce metrics for evaluating forecast errors and methods for evaluating and tracking forecasting performance over time. Chapter 3 discusses regression analysis and its use in forecasting. We discuss both crosssection and time series regression data, least squares and maximum likelihood model -tting, model adequacy checking, prediction intervals, and weighted and generalized least squares. The -rst part of this chapter covers many of the topics typically seen in an introductory treatment of regression, either in a stand-alone course or as part of another applied statistics course. It should be a reasonable review for many readers. Chapter 4 presents exponential smoothing techniques, both for time series with polynomial components and for seasonal data. We discuss and illustrate methods for selecting the smoothing constant(s), forecasting, and constructing prediction intervals. The explicit time series modeling approach to forecasting that we have chosen to emphasize is the autoregressive integrated moving average (ARIMA) model approach. Chapter 5 introduces ARIMA models and illustrates how to identify and -t these models for both nonseasonal and seasonal time series. Forecasting and prediction interval construction are also discussed and illustrated. Chapter 6 extends this discussion into transfer function models and intervention modeling and analysis. Chapter 7 surveys several other useful topics from time series analysis and forecasting, including multivariate time series problems, ARCH and GARCH models, and combinations of forecasts. We also give some practical advice for using statistical approaches to forecasting and provide some information about realistic expectations. The last two chapters of the book are somewhat higher in level than the -rst -ve.

Each chapter has a set of exercises. Some of these exercises involve analyzing the data sets given in Appendix B. These data sets represent an interesting cross section of real time series data, typical of those encountered in practical forecasting problems. Most of these data sets are used in exercises in two or more chapters, an indication that there are usually several approaches to analyzing, modeling, and forecasting a time series. There are other good sources of data for practicing the techniques given in this book. Some of the ones that we have found very interesting and useful include the U.S. Department of Labor鈥擝ureau of Labor Statistics (http:// www.bls.gov/data/home.htm), the U.S. Department of Agriculture鈥?National Agricultural Statistics Service, Quick Stats Agricultural Statistics Data (http://www.nass.usda.gov/Data_and_Statistics/Quick_Stats/index. asp), the U.S. Census Bureau (http://www.census.gov), and the U.S.

Department of the Treasury (http://www.treas.gov/of-ces/domestic--nance/debt-management/interest-rate/). The time series data library created by Rob Hyndman at Monash University (http://www-personal. buseco.monash.edu.au/鈭糷yndman/TSDL/index.htm) and the time series data library at the Mathematics Department of the University of York (http://www.york.ac.uk/depts/maths/data/ts/) also contain many excellent data sets. Some of these sources provide links to other data. Data sets and other materials related to this book can be found at ftp://ftp.wiley.com/ public/scitechmed/ timeseries.

We would like to thank the many individuals who provided feedback and suggestions for improvement to the -rst edition. We found these suggestions most helpful. We are indebted to Clifford Long who generously provided the R codes he used with his students when he taught from the book. We found his codes very helpful in putting the end-of-chapter R code sections together. We also have placed a premium in the book on bridging the gap between theory and practice. We have not emphasized proofs or technical details and have tried to give intuitive explanations of the material whenever possible. The result is a book that can be used with a wide variety of audiences, with different interests and technical backgrounds, whose common interests are understanding how to analyze time-oriented data and constructing good short-term statistically based forecasts.

We express our appreciation to the individuals and organizations who have given their permission to use copyrighted material. These materials are noted in the text. Portions of the output contained in this book are printed with permission of Minitab Inc. All material remains the exclusive property and copyright of Minitab Inc. All rights reserved.

Douglas C. Montgomery

Cheryl L. Jennings

Murat Kulahci

# INTRODUCTION TO FORECASTING

It is dif-cult to make predictions, especially about the future

NEILS BOHR, Danish physicist

# 1.1 THE NATURE AND USES OF FORECASTS

A forecast is a prediction of some future event or events. As suggested by Neils Bohr, making good predictions is not always easy. Famously 鈥渂ad鈥?forecasts include the following from the book Bad Predictions:

- 鈥淭he population is constant in size and will remain so right up to the end of mankind.鈥?L鈥橢ncyclopedie, 1756.   
- 鈥?930 will be a splendid employment year.鈥?U.S. Department of Labor, New Year鈥檚 Forecast in 1929, just before the market crash on October 29.   
- 鈥淐omputers are multiplying at a rapid rate. By the turn of the century there will be 220,000 in the U.S.鈥?Wall Street Journal, 1966.

Forecasting is an important problem that spans many -elds including business and industry, government, economics, environmental sciences, medicine, social science, politics, and -nance. Forecasting problems are often classi-ed as short-term, medium-term, and long-term. Short-term forecasting problems involve predicting events only a few time periods (days, weeks, and months) into the future. Medium-term forecasts extend from 1 to 2 years into the future, and long-term forecasting problems can extend beyond that by many years. Short- and medium-term forecasts are required for activities that range from operations management to budgeting and selecting new research and development projects. Long-term forecasts impact issues such as strategic planning. Short- and medium-term forecasting is typically based on identifying, modeling, and extrapolating the patterns found in historical data. Because these historical data usually exhibit inertia and do not change dramatically very quickly, statistical methods are very useful for short- and medium-term forecasting. This book is about the use of these statistical methods.

Most forecasting problems involve the use of time series data. A time series is a time-oriented or chronological sequence of observations on a variable of interest. For example, Figure 1.1 shows the market yield on US Treasury Securities at 10-year constant maturity from April 1953 through December 2006 (data in Appendix B, Table B.1). This graph is called a time

![](images/62c0afc57c8c3341f5eee25b3059e46493bee6b08880e4e263af61847e118b09.jpg)  
FIGURE 1.1 Time series plot of the market yield on US Treasury Securities at 10-year constant maturity. Source: US Treasury.

series plot. The rate variable is collected at equally spaced time periods, as is typical in most time series and forecasting applications. Many business applications of forecasting utilize daily, weekly, monthly, quarterly, or annual data, but any reporting interval may be used. Furthermore, the data may be instantaneous, such as the viscosity of a chemical product at the point in time where it is measured; it may be cumulative, such as the total sales of a product during the month; or it may be a statistic that in some way reects the activity of the variable during the time period, such as the daily closing price of a speci-c stock on the New York Stock Exchange.

The reason that forecasting is so important is that prediction of future events is a critical input into many types of planning and decision-making processes, with application to areas such as the following:

1. Operations Management. Business organizations routinely use forecasts of product sales or demand for services in order to schedule production, control inventories, manage the supply chain, determine staf-ng requirements, and plan capacity. Forecasts may also be used to determine the mix of products or services to be offered and the locations at which products are to be produced.   
2. Marketing. Forecasting is important in many marketing decisions. Forecasts of sales response to advertising expenditures, new promotions, or changes in pricing polices enable businesses to evaluate their effectiveness, determine whether goals are being met, and make adjustments.   
3. Finance and Risk Management. Investors in -nancial assets are interested in forecasting the returns from their investments. These assets include but are not limited to stocks, bonds, and commodities; other investment decisions can be made relative to forecasts of interest rates, options, and currency exchange rates. Financial risk management requires forecasts of the volatility of asset returns so that the risks associated with investment portfolios can be evaluated and insured, and so that -nancial derivatives can be properly priced.   
4. Economics. Governments, -nancial institutions, and policy organizations require forecasts of major economic variables, such as gross domestic product, population growth, unemployment, interest rates, ination, job growth, production, and consumption. These forecasts are an integral part of the guidance behind monetary and -scal policy, and budgeting plans and decisions made by governments. They are also instrumental in the strategic planning decisions made by business organizations and -nancial institutions.

5. Industrial Process Control. Forecasts of the future values of critical quality characteristics of a production process can help determine when important controllable variables in the process should be changed, or if the process should be shut down and overhauled. Feedback and feedforward control schemes are widely used in monitoring and adjustment of industrial processes, and predictions of the process output are an integral part of these schemes.

6. Demography. Forecasts of population by country and regions are made routinely, often strati-ed by variables such as gender, age, and race. Demographers also forecast births, deaths, and migration patterns of populations. Governments use these forecasts for planning policy and social service actions, such as spending on health care, retirement programs, and antipoverty programs. Many businesses use forecasts of populations by age groups to make strategic plans regarding developing new product lines or the types of services that will be offered.

These are only a few of the many different situations where forecasts are required in order to make good decisions. Despite the wide range of problem situations that require forecasts, there are only two broad types of forecasting techniques鈥攓ualitative methods and quantitative methods.

Qualitative forecasting techniques are often subjective in nature and require judgment on the part of experts. Qualitative forecasts are often used in situations where there is little or no historical data on which to base the forecast. An example would be the introduction of a new product, for which there is no relevant history. In this situation, the company might use the expert opinion of sales and marketing personnel to subjectively estimate product sales during the new product introduction phase of its life cycle. Sometimes qualitative forecasting methods make use of marketing tests, surveys of potential customers, and experience with the sales performance of other products (both their own and those of competitors). However, although some data analysis may be performed, the basis of the forecast is subjective judgment.

Perhaps the most formal and widely known qualitative forecasting technique is the Delphi Method. This technique was developed by the RAND Corporation (see Dalkey [1967]). It employs a panel of experts who are assumed to be knowledgeable about the problem. The panel members are physically separated to avoid their deliberations being impacted either by social pressures or by a single dominant individual. Each panel member responds to a questionnaire containing a series of questions and returns the information to a coordinator. Following the -rst questionnaire, subsequent

questions are submitted to the panelists along with information about the opinions of the panel as a group. This allows panelists to review their predictions relative to the opinions of the entire group. After several rounds, it is hoped that the opinions of the panelists converge to a consensus, although achieving a consensus is not required and justi-ed differences of opinion can be included in the outcome. Qualitative forecasting methods are not emphasized in this book.

Quantitative forecasting techniques make formal use of historical data and a forecasting model. The model formally summarizes patterns in the data and expresses a statistical relationship between previous and current values of the variable. Then the model is used to project the patterns in the data into the future. In other words, the forecasting model is used to extrapolate past and current behavior into the future. There are several types of forecasting models in general use. The three most widely used are regression models, smoothing models, and general time series models. Regression models make use of relationships between the variable of interest and one or more related predictor variables. Sometimes regression models are called causal forecasting models, because the predictor variables are assumed to describe the forces that cause or drive the observed values of the variable of interest. An example would be using data on house purchases as a predictor variable to forecast furniture sales. The method of least squares is the formal basis of most regression models. Smoothing models typically employ a simple function of previous observations to provide a forecast of the variable of interest. These methods may have a formal statistical basis, but they are often used and justi-ed heuristically on the basis that they are easy to use and produce satisfactory results. General time series models employ the statistical properties of the historical data to specify a formal model and then estimate the unknown parameters of this model (usually) by least squares. In subsequent chapters, we will discuss all three types of quantitative forecasting models.

The form of the forecast can be important. We typically think of a forecast as a single number that represents our best estimate of the future value of the variable of interest. Statisticians would call this a point estimate or point forecast. Now these forecasts are almost always wrong; that is, we experience forecast error. Consequently, it is usually a good practice to accompany a forecast with an estimate of how large a forecast error might be experienced. One way to do this is to provide a prediction interval (PI) to accompany the point forecast. The PI is a range of values for the future observation, and it is likely to prove far more useful in decision-making than a single number. We will show how to obtain PIs for most of the forecasting methods discussed in the book.

Other important features of the forecasting problem are the forecast horizon and the forecast interval. The forecast horizon is the number of future periods for which forecasts must be produced. The horizon is often dictated by the nature of the problem. For example, in production planning, forecasts of product demand may be made on a monthly basis. Because of the time required to change or modify a production schedule, ensure that suf-cient raw material and component parts are available from the supply chain, and plan the delivery of completed goods to customers or inventory facilities, it would be necessary to forecast up to 3 months ahead. The forecast horizon is also often called the forecast lead time. The forecast interval is the frequency with which new forecasts are prepared. For example, in production planning, we might forecast demand on a monthly basis, for up to 3 months in the future (the lead time or horizon), and prepare a new forecast each month. Thus the forecast interval is 1 month, the same as the basic period of time for which each forecast is made. If the forecast lead time is always the same length, say, T periods, and the forecast is revised each time period, then we are employing a rolling or moving horizon forecasting approach. This system updates or revises the forecasts for $T { - } 1$ of the periods in the horizon and computes a forecast for the newest period T. This rolling horizon approach to forecasting is widely used when the lead time is several periods long.

# 1.2 SOME EXAMPLES OF TIME SERIES

Time series plots can reveal patterns such as random, trends, level shifts, periods or cycles, unusual observations, or a combination of patterns. Patterns commonly found in time series data are discussed next with examples of situations that drive the patterns.

The sales of a mature pharmaceutical product may remain relatively at in the absence of unchanged marketing or manufacturing strategies. Weekly sales of a generic pharmaceutical product shown in Figure 1.2 appear to be constant over time, at about $1 0 , 4 0 0 \times 1 0 ^ { 3 }$ units, in a random sequence with no obvious patterns (data in Appendix B, Table B.2).

To assure conformance with customer requirements and product speci-- cations, the production of chemicals is monitored by many characteristics. These may be input variables such as temperature and ow rate, and output properties such as viscosity and purity.

Due to the continuous nature of chemical manufacturing processes, output properties often are positively autocorrelated; that is, a value above the long-run average tends to be followed by other values above the

![](images/067eea4677c5c520a3ad789f02c3eeebf0ed7652a5c5bc32c101e9fcff805289.jpg)  
FIGURE 1.2 Pharmaceutical product sales.

average, while a value below the average tends to be followed by other values below the average.

The viscosity readings plotted in Figure 1.3 exhibit autocorrelated behavior, tending to a long-run average of about 85 centipoises (cP), but with a structured, not completely random, appearance (data in Appendix B, Table B.3). Some methods for describing and analyzing autocorrelated data will be described in Chapter 2.

![](images/319fc35d51e14e59d2c49b1aa485241afd12cbedda9d801de4bdddd6a43c4102.jpg)  
FIGURE 1.3 Chemical process viscosity readings.

The USDA National Agricultural Statistics Service publishes agricultural statistics for many commodities, including the annual production of dairy products such as butter, cheese, ice cream, milk, yogurt, and whey. These statistics are used for market analysis and intelligence, economic indicators, and identi-cation of emerging issues.

Blue and gorgonzola cheese is one of 32 categories of cheese for which data are published. The annual US production of blue and gorgonzola cheeses (in $1 0 ^ { 3 }$ lb) is shown in Figure 1.4 (data in Appendix B, Table B.4). Production quadrupled from 1950 to 1997, and the linear trend has a constant positive slope with random, year-to-year variation.

The US Census Bureau publishes historic statistics on manufacturers鈥?shipments, inventories, and orders. The statistics are based on North American Industry Classi-cation System (NAICS) code and are utilized for purposes such as measuring productivity and analyzing relationships between employment and manufacturing output.

The manufacture of beverage and tobacco products is reported as part of the nondurable subsector. The plot of monthly beverage product shipments (Figure 1.5) reveals an overall increasing trend, with a distinct cyclic pattern that is repeated within each year. January shipments appear to be the lowest, with highs in May and June (data in Appendix B, Table B.5). This monthly, or seasonal, variation may be attributable to some cause

![](images/89de766ee531ec6275a88e46221279a496234cc21de8600eff3fe087589c45b8.jpg)  
FIGURE 1.4 The US annual production of blue and gorgonzola cheeses. Source: USDA鈥揘ASS.

![](images/061921153ee4f4d3d80ff99c74a52e77b77b34fda232a89a6cc4612b6fc3b3d5.jpg)  
FIGURE 1.5 The US beverage manufacturer monthly product shipments, unadjusted. Source: US Census Bureau.

such as the impact of weather on the demand for beverages. Techniques for making seasonal adjustments to data in order to better understand general trends will be discussed in Chapter 2.

To determine whether the Earth is warming or cooling, scientists look at annual mean temperatures. At a single station, the warmest and the coolest temperatures in a day are averaged. Averages are then calculated at stations all over the Earth, over an entire year. The change in global annual mean surface air temperature is calculated from a base established from 1951 to 1980, and the result is reported as an 鈥渁nomaly.鈥?
The plot of the annual mean anomaly in global surface air temperature (Figure 1.6) shows an increasing trend since 1880; however, the slope, or rate of change, varies with time periods (data in Appendix B, Table B.6). While the slope in earlier time periods appears to be constant, slightly increasing, or slightly decreasing, the slope from about 1975 to the present appears much steeper than the rest of the plot.

Business data such as stock prices and interest rates often exhibit nonstationary behavior; that is, the time series has no natural mean. The daily closing price adjusted for stock splits of Whole Foods Market (WFMI) stock in 2001 (Figure 1.7) exhibits a combination of patterns for both mean level and slope (data in Appendix B, Table B.7).

While the price is constant in some short time periods, there is no consistent mean level over time. In other time periods, the price changes

![](images/b678e839e4d8b8df2471248863b6d375aed805cefcdb63635122d9e5b98f6a99.jpg)  
FIGURE 1.6 Global mean surface air temperature annual anomaly. Source: NASA-GISS.

at different rates, including occasional abrupt shifts in level. This is an example of nonstationary behavior, which will be discussed in Chapter 2.

The Current Population Survey (CPS) or 鈥渉ousehold survey鈥?prepared by the US Department of Labor, Bureau of Labor Statistics, contains national data on employment, unemployment, earnings, and other labor market topics by demographic characteristics. The data are used to report

![](images/cdde7123386ce6c0180d05c2fda2513cc69abac007c7ec92f719cf5c31442614.jpg)  
FIGURE 1.7 Whole foods market stock price, daily closing adjusted for splits.

![](images/7794be6e885e7172c49f87d2b7262186d2d81be965f78df215730deacadc551c.jpg)  
FIGURE 1.8 Monthly unemployment rate鈥攆ull-time labor force, unadjusted. Source: US Department of Labor-BLS.

on the employment situation, for projections with impact on hiring and training, and for a multitude of other business planning activities. The data are reported unadjusted and with seasonal adjustment to remove the effect of regular patterns that occur each year.

The plot of monthly unadjusted unemployment rates (Figure 1.8) exhibits a mixture of patterns, similar to Figure 1.5 (data in Appendix B, Table B.8). There is a distinct cyclic pattern within a year; January, February, and March generally have the highest unemployment rates. The overall level is also changing, from a gradual decrease, to a steep increase, followed by a gradual decrease. The use of seasonal adjustments as described in Chapter 2 makes it easier to observe the nonseasonal movements in time series data.

Solar activity has long been recognized as a signi-cant source of noise impacting consumer and military communications, including satellites, cell phone towers, and electric power grids. The ability to accurately forecast solar activity is critical to a variety of -elds. The International Sunspot Number $R$ is the oldest solar activity index. The number incorporates both the number of observed sunspots and the number of observed sunspot groups. In Figure 1.9, the plot of annual sunspot numbers reveals cyclic patterns of varying magnitudes (data in Appendix B, Table B.9).

In addition to assisting in the identi-cation of steady-state patterns, time series plots may also draw attention to the occurrence of atypical events. Weekly sales of a generic pharmaceutical product dropped due to limited

![](images/ae43c932927343119adb5ab53a81d050e2c52488c03a26636073c8db9f75a683.jpg)  
FIGURE 1.9 The international sunspot number. Source: SIDC.

availability resulting from a -re at one of the four production facilities. The 5-week reduction is apparent in the time series plot of weekly sales shown in Figure 1.10.

Another type of unusual event may be the failure of the data measurement or collection system. After recording a vastly different viscosity reading at time period 70 (Figure 1.11), the measurement system was

![](images/9ccb4c7bdeb521894c70842648d279d2590def6a4a694cf1803508a003d8fd85.jpg)  
FIGURE 1.10 Pharmaceutical product sales.

![](images/98dbee68188230eaa861c08b68da21c2c920987fb315cb102ef27c5e725f4021.jpg)  
FIGURE 1.11 Chemical process viscosity readings, with sensor malfunction.

checked with a standard and determined to be out of calibration. The cause was determined to be a malfunctioning sensor.

# 1.3 THE FORECASTING PROCESS

A process is a series of connected activities that transform one or more inputs into one or more outputs. All work activities are performed in processes, and forecasting is no exception. The activities in the forecasting process are:

1. Problem de-nition   
2. Data collection   
3. Data analysis   
4. Model selection and -tting   
5. Model validation   
6. Forecasting model deployment   
7. Monitoring forecasting model performance

These activities are shown in Figure 1.12.

Problem de-nition involves developing understanding of how the forecast will be used along with the expectations of the 鈥渃ustomer鈥?(the user of

![](images/bd734a573d37ce7ceb0506a282fed4eba62062b0d4dd67b4654159b796e2bf94.jpg)  
FIGURE 1.12 The forecasting process.

the forecast). Questions that must be addressed during this phase include the desired form of the forecast (e.g., are monthly forecasts required), the forecast horizon or lead time, how often the forecasts need to be revised (the forecast interval), and what level of forecast accuracy is required in order to make good business decisions. This is also an opportunity to introduce the decision makers to the use of prediction intervals as a measure of the risk associated with forecasts, if they are unfamiliar with this approach. Often it is necessary to go deeply into many aspects of the business system that requires the forecast to properly de-ne the forecasting component of the entire problem. For example, in designing a forecasting system for inventory control, information may be required on issues such as product shelf life or other aging considerations, the time required to manufacture or otherwise obtain the products (production lead time), and the economic consequences of having too many or too few units of product available to meet customer demand. When multiple products are involved, the level of aggregation of the forecast (e.g., do we forecast individual products or families consisting of several similar products) can be an important consideration. Much of the ultimate success of the forecasting model in meeting the customer expectations is determined in the problem de-nition phase.

Data collection consists of obtaining the relevant history for the variable(s) that are to be forecast, including historical information on potential predictor variables.

The key here is 鈥渞elevant鈥? often information collection and storage methods and systems change over time and not all historical data are useful for the current problem. Often it is necessary to deal with missing values of some variables, potential outliers, or other data-related problems that have occurred in the past. During this phase, it is also useful to begin planning how the data collection and storage issues in the future will be handled so that the reliability and integrity of the data will be preserved.

Data analysis is an important preliminary step to the selection of the forecasting model to be used. Time series plots of the data should be constructed and visually inspected for recognizable patterns, such as trends and seasonal or other cyclical components. A trend is evolutionary movement, either upward or downward, in the value of the variable. Trends may

be long-term or more dynamic and of relatively short duration. Seasonality is the component of time series behavior that repeats on a regular basis, such as each year. Sometimes we will smooth the data to make identi-- cation of the patterns more obvious (data smoothing will be discussed in Chapter 2). Numerical summaries of the data, such as the sample mean, standard deviation, percentiles, and autocorrelations, should also be computed and evaluated. Chapter 2 will provide the necessary background to do this. If potential predictor variables are available, scatter plots of each pair of variables should be examined. Unusual data points or potential outliers should be identi-ed and agged for possible further study. The purpose of this preliminary data analysis is to obtain some 鈥渇eel鈥?for the data, and a sense of how strong the underlying patterns such as trend and seasonality are. This information will usually suggest the initial types of quantitative forecasting methods and models to explore.

Model selection and -tting consists of choosing one or more forecasting models and -tting the model to the data. By -tting, we mean estimating the unknown model parameters, usually by the method of least squares. In subsequent chapters, we will present several types of time series models and discuss the procedures of model -tting. We will also discuss methods for evaluating the quality of the model -t, and determining if any of the underlying assumptions have been violated. This will be useful in discriminating between different candidate models.

Model validation consists of an evaluation of the forecasting model to determine how it is likely to perform in the intended application. This must go beyond just evaluating the 鈥?t鈥?of the model to the historical data and must examine what magnitude of forecast errors will be experienced when the model is used to forecast 鈥渇resh鈥?or new data. The -tting errors will always be smaller than the forecast errors, and this is an important concept that we will emphasize in this book. A widely used method for validating a forecasting model before it is turned over to the customer is to employ some form of data splitting, where the data are divided into two segments鈥攁 -tting segment and a forecasting segment. The model is -t to only the -tting data segment, and then forecasts from that model are simulated for the observations in the forecasting segment. This can provide useful guidance on how the forecasting model will perform when exposed to new data and can be a valuable approach for discriminating between competing forecasting models.

Forecasting model deployment involves getting the model and the resulting forecasts in use by the customer. It is important to ensure that the customer understands how to use the model and that generating timely forecasts from the model becomes as routine as possible. Model maintenance,

including making sure that data sources and other required information will continue to be available to the customer is also an important issue that impacts the timeliness and ultimate usefulness of forecasts.

Monitoring forecasting model performance should be an ongoing activity after the model has been deployed to ensure that it is still performing satisfactorily. It is the nature of forecasting that conditions change over time, and a model that performed well in the past may deteriorate in performance. Usually performance deterioration will result in larger or more systematic forecast errors. Therefore monitoring of forecast errors is an essential part of good forecasting system design. Control charts of forecast errors are a simple but effective way to routinely monitor the performance of a forecasting model. We will illustrate approaches to monitoring forecast errors in subsequent chapters.

# 1.4 DATA FOR FORECASTING

# 1.4.1 The Data Warehouse

Developing time series models and using them for forecasting requires data on the variables of interest to decision-makers. The data are the raw materials for the modeling and forecasting process. The terms data and information are often used interchangeably, but we prefer to use the term data as that seems to reect a more raw or original form, whereas we think of information as something that is extracted or synthesized from data. The output of a forecasting system could be thought of as information, and that output uses data as an input.

In most modern organizations data regarding sales, transactions, company -nancial and business performance, supplier performance, and customer activity and relations are stored in a repository known as a data warehouse. Sometimes this is a single data storage system; but as the volume of data handled by modern organizations grows rapidly, the data warehouse has become an integrated system comprised of components that are physically and often geographically distributed, such as cloud data storage. The data warehouse must be able to organize, manipulate, and integrate data from multiple sources and different organizational information systems. The basic functionality required includes data extraction, data transformation, and data loading. Data extraction refers to obtaining data from internal sources and from external sources such as third party vendors or government entities and -nancial service organizations. Once the data are extracted, the transformation stage involves applying rules to prevent duplication of records and dealing with problems such as missing information. Sometimes we refer to the transformation activities as data

cleaning. We will discuss some of the important data cleaning operations subsequently. Finally, the data are loaded into the data warehouse where they are available for modeling and analysis.

Data quality has several dimensions. Five important ones that have been described in the literature are accuracy, timeliness, completeness, representativeness, and consistency. Accuracy is probably the oldest dimension of data quality and refers to how close that data conform to its 鈥渞eal鈥?values. Real values are alternative sources that can be used for veri-cation purposes. For example, do sales records match payments to accounts receivable records (although the -nancial records may occur in later time periods because of payment terms and conditions, discounts, etc.)? Timeliness means that the data are as current as possible. Infrequent updating of data can seriously impact developing a time series model that is going to be used for relatively short-term forecasting. In many time series model applications the time between the occurrence of the real-world event and its entry into the data warehouse must be as short as possible to facilitate model development and use. Completeness means that the data content is complete, with no missing data and no outliers. As an example of representativeness, suppose that the end use of the time series model is to forecast customer demand for a product or service, but the organization only records booked orders and the date of ful-llment. This may not accurately reect demand, because the orders can be booked before the desired delivery period and the date of ful-llment can take place in a different period than the one required by the customer. Furthermore, orders that are lost because of product unavailability or unsatisfactory delivery performance are not recorded. In these situations demand can differ dramatically from sales. Data cleaning methods can often be used to deal with some problems of completeness. Consistency refers to how closely data records agree over time in format, content, meaning, and structure. In many organizations how data are collected and stored evolves over time; de-nitions change and even the types of data that are collected change. For example, consider monthly data. Some organizations de-ne 鈥渕onths鈥?that coincide with the traditional calendar de-nition. But because months have different numbers of days that can induce patterns in monthly data, some organizations prefer to de-ne a year as consisting of 13 鈥渕onths鈥?each consisting of 4 weeks.

It has been suggested that the output data that reside in the data warehouse are similar to the output of a manufacturing process, where the raw data are the input. Just as in manufacturing and other service processes, the data production process can bene-t by the application of quality management and control tools. Jones-Farmer et al. (2014) describe how statistical quality control methods, speci-cally control charts, can be used to enhance data quality in the data production process.

# 1.4.2 Data Cleaning

Data cleaning is the process of examining data to detect potential errors, missing data, outliers or unusual values, or other inconsistencies and then correcting the errors or problems that are found. Sometimes errors are the result of recording or transmission problems, and can be corrected by working with the original data source to correct the problem. Effective data cleaning can greatly improve the forecasting process.

Before data are used to develop a time series model, it should be subjected to several different kinds of checks, including but not necessarily limited to the following:

1. Is there missing data?   
2. Does the data fall within an expected range?   
3. Are there potential outliers or other unusual values?

These types of checks can be automated fairly easily. If this aspect of data cleaning is automated, the rules employed should be periodically evaluated to ensure that they are still appropriate and that changes in the data have not made some of the procedures less effective. However, it is also extremely useful to use graphical displays to assist in identifying unusual data. Techniques such as time series plots, histograms, and scatter diagrams are extremely useful. These and other graphical methods will be described in Chapter 2.

# 1.4.3 Imputation

Data imputation is the process of correcting missing data or replacing outliers with an estimation process. Imputation replaces missing or erroneous values with a 鈥渓ikely鈥?value based on other available information. This enables the analysis to work with statistical techniques which are designed to handle the complete data sets.

Mean value imputation consists of replacing a missing value with the sample average calculated from the nonmissing observations. The big advantage of this method is that it is easy, and if the data does not have any speci-c trend or seasonal pattern, it leaves the sample mean of the complete data set unchanged. However, one must be careful if there are trends or seasonal patterns, because the sample mean of all of the data may not reect these patterns. A variation of this is stochastic mean value imputation, in which a random variable is added to the mean value to capture some of the noise or variability in the data. The random variable could be assumed to

follow a normal distribution with mean zero and standard deviation equal to the standard deviation of the actual observed data. A variation of mean value imputation is to use a subset of the available historical data that reects any trend or seasonal patterns in the data. For example, consider the time series $y _ { 1 } , y _ { 2 } , \ldots , y _ { T }$ and suppose that one observation $y _ { j }$ is missing. We can impute the missing value as

$$
y _ {j} ^ {*} = \frac {1}{2 k} \left(\sum_ {t = j - k} ^ {j - 1} y _ {t} + \sum_ {t - j + 1} ^ {j + k} y _ {t}\right),
$$

where $k$ would be based on the seasonal variability in the data. It is usually chosen as some multiple of the smallest seasonal cycle in the data. So, if the data are monthly and exhibit a monthly cycle, $k$ would be a multiple of 12. Regression imputation is a variation of mean value imputation where the imputed value is computed from a model used to predict the missing value. The prediction model does not have to be a linear regression model. For example, it could be a time series model.

Hot deck imputation is an old technique that is also known as the last value carried forward method. The term 鈥渉ot deck鈥?comes from the use of computer punch cards. The deck of cards was 鈥渉ot鈥?because it was currently in use. Cold deck imputation uses information from a deck of cards not currently in use. In hot deck imputation, the missing values are imputed by using values from similar complete observations. If there are several variables, sort the data by the variables that are most related to the missing observation and then, starting at the top, replace the missing values with the value of the immediately preceding variable. There are many variants of this procedure.

# 1.5 RESOURCES FOR FORECASTING

There are a variety of good resources that can be helpful to technical professionals involved in developing forecasting models and preparing forecasts. There are three professional journals devoted to forecasting:

- Journal of Forecasting   
- International Journal of Forecasting   
- Journal of Business Forecasting Methods and Systems

These journals publish a mixture of new methodology, studies devoted to the evaluation of current methods for forecasting, and case studies and

applications. In addition to these specialized forecasting journals, there are several other mainstream statistics and operations research/management science journals that publish papers on forecasting, including:

- Journal of Business and Economic Statistics   
- Management Science   
- Naval Research Logistics   
- Operations Research   
- International Journal of Production Research   
- Journal of Applied Statistics

This is by no means a comprehensive list. Research on forecasting tends to be published in a variety of outlets.

There are several books that are good complements to this one. We recommend Box, Jenkins, and Reinsel (1994); Chat-eld (1996); Fuller (1995); Abraham and Ledolter (1983); Montgomery, Johnson, and Gardiner (1990); Wei (2006); and Brockwell and Davis (1991, 2002). Some of these books are more specialized than this one, in that they focus on a speci-c type of forecasting model such as the autoregressive integrated moving average [ARIMA] model, and some also require more background in statistics and mathematics.

Many statistics software packages have very good capability for -tting a variety of forecasting models. Minitab庐 Statistical Software, $\mathbf { J M P ^ { \otimes } }$ , the Statistical Analysis System (SAS) and R are the packages that we utilize and illustrate in this book. At the end of most chapters we provide R code for working some of the examples in the chapter. Matlab and S-Plus are also two packages that have excellent capability for solving forecasting problems.

# EXERCISES

1.1 Why is forecasting an essential part of the operation of any organization or business?   
1.2 What is a time series? Explain the meaning of trend effects, seasonal variations, and random error.   
1.3 Explain the difference between a point forecast and an interval forecast.   
1.4 What do we mean by a causal forecasting technique?

1.5 Everyone makes forecasts in their daily lives. Identify and discuss a situation where you employ forecasts.

a. What decisions are impacted by your forecasts?   
b. How do you evaluate the quality of your forecasts?   
c. What is the value to you of a good forecast?   
d. What is the harm or penalty associated with a bad forecast?

1.6 What is meant by a rolling horizon forecast?   
1.7 Explain the difference between forecast horizon and forecast interval.   
1.8 Suppose that you are in charge of capacity planning for a large electric utility. A major part of your job is ensuring that the utility has suf-cient generating capacity to meet current and future customer needs. If you do not have enough capacity, you run the risks of brownouts and service interruption. If you have too much capacity, it may cost more to generate electricity.

a. What forecasts do you need to do your job effectively?   
b. Are these short-range or long-range forecasts?   
c. What data do you need to be able to generate these forecasts?

1.9 Your company designs and manufactures apparel for the North American market. Clothing and apparel is a style good, with a relatively limited life. Items not sold at the end of the season are usually sold through off-season outlet and discount retailers. Items not sold through discounting and off-season merchants are often given to charity or sold abroad.

a. What forecasts do you need in this business to be successful?   
b. Are these short-range or long-range forecasts?   
c. What data do you need to be able to generate these forecasts?   
d. What are the implications of forecast errors?

1.10 Suppose that you are in charge of production scheduling at a semiconductor manufacturing plant. The plant manufactures about 20 different types of devices, all on 8-inch silicon wafers. Demand for these products varies randomly. When a lot or batch of wafers is started into production, it can take from 4 to 6 weeks before the batch is -nished, depending on the type of product. The routing of each batch of wafers through the production tools can be different depending on the type of product.

a. What forecasts do you need in this business to be successful?   
b. Are these short-range or long-range forecasts?   
c. What data do you need to be able to generate these forecasts?   
d. Discuss the impact that forecast errors can potentially have on the ef-ciency with which your factory operates, including workin-process inventory, meeting customer delivery schedules, and the cycle time to manufacture product.

1.11 You are the administrator of a large metropolitan hospital that operates the only 24-hour emergency room in the area. You must schedule attending physicians, resident physicians, nurses, laboratory, and support personnel to operate this facility effectively.

a. What measures of effectiveness do you think patients use to evaluate the services that you provide?   
b. How are forecasts useful to you in planning services that will maximize these measures of effectiveness?   
c. What planning horizon do you need to use? Does this lead to short-range or long-range forecasts?

1.12 Consider an airline that operates a network of ights that serves 200 cities in the continental United States. What long-range forecasts do the operators of the airline need to be successful? What forecasting problems does this business face on a daily basis? What are the consequences of forecast errors for the airline?   
1.13 Discuss the potential dif-culties of forecasting the daily closing price of a speci-c stock on the New York Stock Exchange. Would the problem be different (harder, easier) if you were asked to forecast the closing price of a group of stocks, all in the same industry (say, the pharmaceutical industry)?   
1.14 Explain how large forecast errors can lead to high inventory levels at a retailer; at a manufacturing plant.   
1.15 Your company manufactures and distributes soft drink beverages, sold in bottles and cans at retail outlets such as grocery stores, restaurants and other eating/drinking establishments, and vending machines in of-ces, schools, stores, and other outlets. Your product line includes about 25 different products, and many of these are produced in different package sizes.

a. What forecasts do you need in this business to be successful?

b. Is the demand for your product likely to be seasonal? Explain why or why not?   
c. Does the shelf life of your product impact the forecasting problem?   
d. What data do you think that you would need to be able to produce successful forecasts?

# STATISTICS BACKGROUNDFOR FORECASTING

The future ain鈥檛 what it used to be.

YOGI BERRA, New York Yankees catcher

# 2.1 INTRODUCTION

This chapter presents some basic statistical methods essential to modeling, analyzing, and forecasting time series data. Both graphical displays and numerical summaries of the properties of time series data are presented. We also discuss the use of data transformations and adjustments in forecasting and some widely used methods for characterizing and monitoring the performance of a forecasting model. Some aspects of how these performance measures can be used to select between competing forecasting techniques are also presented.

Forecasts are based on data or observations on the variable of interest. These data are usually in the form of a time series. Suppose that there are $T$ periods of data available, with period $T$ being the most recent. We will let the observation on this variable at time period $t$ be denoted by $y _ { t }$ , $t = 1$ , 2, 鈥?, T. This variable can represent a cumulative quantity, such as the

total demand for a product during period $t$ , or an instantaneous quantity, such as the daily closing price of a speci-c stock on the New York Stock Exchange.

Generally, we will need to distinguish between a forecast or predicted value of $y _ { t }$ that was made at some previous time period, say, $t - \tau$ , and a -tted value of $y _ { t }$ that has resulted from estimating the parameters in a time series model to historical data. Note that $\tau$ is the forecast lead time. The forecast made at time period $t - \tau$ is denoted by $\hat { y } _ { t } ( t - \tau )$ . There is a lot of interest in the lead 鈭?1 forecast, which is the forecast of the observation in period t, $y _ { t }$ , made one period prior, $\hat { y } _ { t } ( t - 1 )$ . We will denote the -tted value of $y _ { t }$ by $\hat { y } _ { t }$ .

We will also be interested in analyzing forecast errors. The forecast error that results from a forecast of $y _ { t }$ that was made at time period $t - \tau$ i s the lead 鈭?$\pmb { \tau }$ forecast error

$$
e _ {t} (\tau) = y _ {t} - \hat {y} _ {t} (t - \tau). \tag {2.1}
$$

For example, the lead 鈭?1 forecast error is

$$
e _ {t} (1) = y _ {t} - \hat {y} _ {t} (t - 1).
$$

The difference between the observation $y _ { t }$ and the value obtained by -tting a time series model to the data, or a -tted value $\hat { y } _ { t }$ de-ned earlier, is called a residual, and is denoted by

$$
e _ {t} = y _ {t} - \hat {y} _ {t}. \tag {2.2}
$$

The reason for this careful distinction between forecast errors and residuals is that models usually -t historical data better than they forecast. That is, the residuals from a model--tting process will almost always be smaller than the forecast errors that are experienced when that model is used to forecast future observations.

# 2.2 GRAPHICAL DISPLAYS

# 2.2.1 Time Series Plots

Developing a forecasting model should always begin with graphical display and analysis of the available data. Many of the broad general features of a time series can be seen visually. This is not to say that analytical tools are

not useful, because they are, but the human eye can be a very sophisticated data analysis tool. To paraphrase the great New York Yankees catcher Yogi Berra, 鈥淵ou can observe a lot just by watching.鈥?
The basic graphical display for time series data is the time series plot, illustrated in Chapter 1. This is just a graph of $y _ { t }$ versus the time period, t, for $t = 1$ , 2, 鈥?, T. Features such as trend and seasonality are usually easy to see from the time series plot. It is interesting to observe that some of the classical tools of descriptive statistics, such as the histogram and the stem-and-leaf display, are not particularly useful for time series data because they do not take time order into account.

Example 2.1 Figures 2.1 and 2.2 show time series plots for viscosity readings and beverage production shipments (originally shown in Figures 1.3 and 1.5, respectively). At the right-hand side of each time series plot is a histogram of the data. Note that while the two time series display very different characteristics, the histograms are remarkably similar. Essentially, the histogram summarizes the data across the time dimension, and in so doing, the key time-dependent features of the data are lost. Stem-andleaf plots and boxplots would have the same issues, losing time-dependent features.

![](images/d281dbc596968110b85dbcf16717a358ed75e283c4aac8a3dc129422951ad041.jpg)

![](images/8b32412ea84e8693b89921bc47c28fe33249f83c89e8bf555c9d2e068c3f1eb4.jpg)  
FIGURE 2.1 Time series plot and histogram of chemical process viscosity readings.

![](images/7fd99941b7d744818dcde1238dda0579330a20601513a2b027df162cacf38496.jpg)

![](images/153315ee12fef007d0955a0ea7d8d2e884d0974697739295e917b330d300e4d1.jpg)  
FIGURE 2.2 Time series plot and histogram of beverage production shipments.

When there are two or more variables of interest, scatter plots can be useful in displaying the relationship between the variables. For example, Figure 2.3 is a scatter plot of the annual global mean surface air temperature anomaly -rst shown in Figure 1.6 versus atmospheric $\mathrm { C O } _ { 2 }$ concentrations. The scatter plot clearly reveals a relationship between the two variables:

![](images/0e676be1f72c963f327680df0822461d896ce6b5b99996a25f0c575ed33c06b9.jpg)  
FIGURE 2.3 Scatter plot of temperature anomaly versus $\mathrm { C O } _ { 2 }$ concentrations. Sources: NASA鈥揋ISS (anomaly), DOE鈥揇IAC $\left( \mathbf { C O } _ { 2 } \right)$ ).

low concentrations of $\mathrm { C O } _ { 2 }$ are usually accompanied by negative anomalies, and higher concentrations of $\mathrm { C O } _ { 2 }$ tend to be accompanied by positive anomalies. Note that this does not imply that higher concentrations of $\mathrm { C O } _ { 2 }$ actually cause higher temperatures. The scatter plot cannot establish a causal relationship between two variables (neither can naive statistical modeling techniques, such as regression), but it is useful in displaying how the variables have varied together in the historical data set.

There are many variations of the time series plot and other graphical displays that can be constructed to show speci-c features of a time series. For example, Figure 2.4 displays daily price information for Whole Foods Market stock during the -rst quarter of 2001 (the trading days from January 2, 2001 through March 30, 2001). This chart, created in Excel庐, shows the opening, closing, highest, and lowest prices experienced within a trading day for the -rst quarter. If the opening price was higher than the closing price, the box is -lled, whereas if the closing price was higher than the opening price, the box is open. This type of plot is potentially more useful than a time series plot of just the closing (or opening) prices, because it shows the volatility of the stock within a trading day. The volatility of an asset is often of interest to investors because it is a measure of the inherent risk associated with the asset.

![](images/ff030e70a72f3a7b7185787de297c2248fd54e8dd37f15b70f601ec83b63721e.jpg)  
FIGURE 2.4 Open-high/close-low chart of Whole Foods Market stock price. Source: finance.yahoo.com.

# 2.2.2 Plotting Smoothed Data

Sometimes it is useful to overlay a smoothed version of the original data on the original time series plot to help reveal patterns in the original data. There are several types of data smoothers that can be employed. One of the simplest and most widely used is the ordinary or simple moving average. A simple moving average of span $N$ assigns weights 1/N to the most recent $N$ observations $y _ { T } , y _ { T - 1 } , \dots , y _ { T - N + 1 }$ , and weight zero to all other observations. If we let $M _ { T }$ be the moving average, then the $N$ -span moving average at time period $T$ is

$$
M _ {T} = \frac {y _ {T} + y _ {T - 1} + \cdots + y _ {T - N + 1}}{N} = \frac {1}{N} \sum_ {t = T - N + 1} ^ {T} y _ {t} \tag {2.3}
$$

Clearly, as each new observation becomes available it is added into the sum from which the moving average is computed and the oldest observation is discarded. The moving average has less variability than the original observations; in fact, if the variance of an individual observation $y _ { t }$ is $\sigma ^ { 2 }$ , then assuming that the observations are uncorrelated the variance of the moving average is

$$
\operatorname {V a r} (M _ {T}) = \operatorname {V a r} \left(\frac {1}{N} \sum_ {t = T - N + 1} ^ {N} y _ {t}\right) = \frac {1}{N ^ {2}} \sum_ {t = T - N + 1} ^ {N} \operatorname {V a r} (y _ {t}) = \frac {\sigma^ {2}}{N}
$$

Sometimes a 鈥渃entered鈥?version of the moving average is used, such as in

$$
M _ {t} = \frac {1}{S + 1} \sum_ {i = - S} ^ {S} y _ {t - i} \tag {2.4}
$$

where the span of the centered moving average is $N = 2 S + 1$ .

Example 2.2 Figure 2.5 plots the annual global mean surface air temperature anomaly data along with a -ve-period (a period is 1 year) moving average of the same data. Note that the moving average exhibits less variability than found in the original series. It also makes some features of the data easier to see; for example, it is now more obvious that the global air temperature decreased from about 1940 until about 1975.

Plots of moving averages are also used by analysts to evaluate stock price trends; common MA periods are 5, 10, 20, 50, 100, and 200 days. A time series plot of Whole Foods Market stock price with a 50-day moving

![](images/0f5522ac1818e59460eceeedcb73b25fce2f24ceb0b91be836a66d45ceed0e7d.jpg)  
FIGURE 2.5 Time series plot of global mean surface air temperature anomaly, with -ve-period moving average. Source: NASA鈥揋ISS.

![](images/db586b336917179867a47f690b521fa829bdf64f61e839570ffa40501ea2cf41.jpg)  
FIGURE 2.6 Time series plot of Whole Foods Market stock price, with 50-day moving average. Source: finance.yahoo.com.

average is shown in Figure 2.6. The moving average plot smoothes the day-to-day noise and shows a generally increasing trend.

The simple moving average is a linear data smoother, or a linear -lter, because it replaces each observation $y _ { t }$ with a linear combination of the other data points that are near to it in time. The weights in the linear combination are equal, so the linear combination here is an average. Of

course, unequal weights could be used. For example, the Hanning -lter is a weighted, centered moving average

$$
M _ {t} ^ {\mathrm {H}} = 0. 2 5 y _ {t + 1} + 0. 5 y _ {t} + 0. 2 5 y _ {t - 1}
$$

Julius von Hann, a nineteenth century Austrian meteorologist, used this -lter to smooth weather data.

An obvious disadvantage of a linear -lter such as a moving average is that an unusual or erroneous data point or an outlier will dominate the moving averages that contain that observation, contaminating the moving averages for a length of time equal to the span of the -lter. For example, consider the sequence of observations

15, 18, 13, 12, 16, 14, 16, 17, 18, 15, 18, 200, 19, 14, 21, 24, 19, 25

which increases reasonably steadily from 15 to 25, except for the unusual value 200. Any reasonable smoothed version of the data should also increase steadily from 15 to 25 and not emphasize the value 200. Now even if the value 200 is a legitimate observation, and not the result of a data recording or reporting error (perhaps it should be 20!), it is so unusual that it deserves special attention and should likely not be analyzed along with the rest of the data.

Odd-span moving medians (also called running medians) are an alternative to moving averages that are effective data smoothers when the time series may be contaminated with unusual values or outliers. The moving median of span $N$ is de-ned as

$$
m _ {t} ^ {[ N ]} = \operatorname {m e d} \left(y _ {t - u}, \dots , y _ {t}, \dots , y _ {t + u}\right), \tag {2.5}
$$

where $N = 2 u + 1$ . The median is the middle observation in rank order (or order of value). The moving median of span 3 is a very popular and effective data smoother, where

$$
m _ {t} ^ {[ 3 ]} = m e d (y _ {t - 1}, y _ {t}, y _ {t + 1}).
$$

This smoother would process the data three values at a time, and replace the three original observations by their median. If we apply this smoother to the data above, we obtain

, 15, 13, 13, 14, 16, 17, 17, 18, 18, 19, 19, 19, 21, 21, 24,

This smoothed data are a reasonable representation of the original data, but they conveniently ignore the value 200. The end values are lost when using the moving median, and they are represented by

In general, a moving median will pass monotone sequences of data unchanged. It will follow a step function in the data, but it will eliminate a spike or more persistent upset in the data that has duration of at most u consecutive observations. Moving medians can be applied more than once if desired to obtain an even smoother series of observations. For example, applying the moving median of span 3 to the smoothed data above results in

, 13, 13, 14, 16, 17, 17, 18, 18, 19, 19, 19, 21, 21,

These data are now as smooth as it can get; that is, repeated application of the moving median will not change the data, apart from the end values.

If there are a lot of observations, the information loss from the missing end values is not serious. However, if it is necessary or desirable to keep the lengths of the original and smoothed data sets the same, a simple way to do this is to 鈥渃opy on鈥?or add back the end values from the original data. This would result in the smoothed data:

15, 18, 13, 13, 14, 16, 17, 17, 18, 18, 19, 19, 19, 21, 21, 19, 25

There are also methods for smoothing the end values. Tukey (1979) is a basic reference on this subject and contains many other clever and useful techniques for data analysis.

Example 2.3 The chemical process viscosity readings shown in Figure 1.11 are an example of a time series that bene-ts from smoothing to evaluate patterns. The selection of a moving median over a moving average, as shown in Figure 2.7, minimizes the impact of the invalid measurements, such as the one at time period 70.

# 2.3 NUMERICAL DESCRIPTION OF TIME SERIES DATA

# 2.3.1 Stationary Time Series

A very important type of time series is a stationary time series. A time series is said to be strictly stationary if its properties are not affected

![](images/4d806ec65904dd051456748effd15f6d52cba530875acccb7055cd59f41b1b62.jpg)  
(a)

![](images/b41678369aa628da3b6472cc772118b8bc9d482a0577c1432de513cf97a21dcc.jpg)  
(b)   
FIGURE 2.7 Viscosity readings with (a) moving average and (b) moving median.

by a change in the time origin. That is, if the joint probability distribution of the observations $y _ { t } , y _ { t + 1 } , \ldots , y _ { t + n }$ is exactly the same as the joint probability distribution of the observations $y _ { t + k } , y _ { t + k + 1 } , \dots , y _ { t + k + n }$ then the time series is strictly stationary. When $n = 0$ the stationarity assumption means that the probability distribution of $y _ { t }$ is the same for all time periods

![](images/fc41380c86f53c86118fe1d640d9d389db735a31fa03d67235f7d731cf99771a.jpg)  
FIGURE 2.8 Pharmaceutical product sales.

and can be written as $f ( y )$ . The pharmaceutical product sales and chemical viscosity readings time series data originally shown in Figures 1.2 and 1.3, respectively, are examples of stationary time series. The time series plots are repeated in Figures 2.8 and 2.9 for convenience. Note that both time series seem to vary around a -xed level. Based on the earlier de-nition, this is a characteristic of stationary time series. On the other hand, the Whole

![](images/0b2c059e34dd950f877949fd77735cb596b67ba51351c091d33b9cfca9425f37.jpg)  
FIGURE 2.9 Chemical process viscosity readings.

Foods Market stock price data in Figure 1.7 tends to wander around or drift, with no obvious -xed level. This is behavior typical of a nonstationary time series.

Stationary implies a type of statistical equilibrium or stability in the data. Consequently, the time series has a constant mean de-ned in the usual way as

$$
\mu_ {y} = E (y) = \int_ {- \infty} ^ {\infty} y f (y) d y \tag {2.6}
$$

and constant variance de-ned as

$$
\sigma_ {y} ^ {2} = \operatorname {V a r} (y) = \int_ {- \infty} ^ {\infty} \left(y - \mu_ {y}\right) ^ {2} f (y) d y. \tag {2.7}
$$

The sample mean and sample variance are used to estimate these parameters. If the observations in the time series are $y _ { 1 } , y _ { 2 } , \ldots , y _ { T }$ , then the sample mean is

$$
\bar {y} = \hat {\mu} _ {y} = \frac {1}{T} \sum_ {t = 1} ^ {T} y _ {t} \tag {2.8}
$$

and the sample variance is

$$
s ^ {2} = \hat {\sigma} _ {y} ^ {2} = \frac {1}{T} \sum_ {t = 1} ^ {T} (y _ {t} - \bar {y}) ^ {2}. \tag {2.9}
$$

Note that the divisor in Eq. (2.9) is $T$ rather than the more familiar $T - 1$ . This is the common convention in many time series applications, and because $T$ is usually not small, there will be little difference between using $T$ instead of $T - 1$ .

# 2.3.2 Autocovariance and Autocorrelation Functions

If a time series is stationary this means that the joint probability distribution of any two observations, say, $y _ { t }$ and $y _ { t + k }$ , is the same for any two time periods $t$ and $t + k$ that are separated by the same interval $k$ . Useful information about this joint distribution, and hence about the nature of the time series, can be obtained by plotting a scatter diagram of all of the data pairs $y _ { t } , y _ { t + k }$ that are separated by the same interval $k$ . The interval $k$ is called the lag.

![](images/3e358c16b1741746ddb773e0dddfaea12ca980cbfd09027f9c401775c0b9e9b1.jpg)  
FIGURE 2.10 Scatter diagram of pharmaceutical product sales at lag $k = 1$ .

Example 2.4 Figure 2.10 is a scatter diagram for the pharmaceutical product sales for lag $k = 1$ and Figure 2.11 is a scatter diagram for the chemical viscosity readings for lag $k = 1$ . Both scatter diagrams were constructed by plotting $y _ { t + 1 }$ versus $y _ { t }$ . Figure 2.10 exhibits little structure; the plotted pairs of adjacent observations $y _ { t } , y _ { t + 1 }$ seem to be uncorrelated. That is, the value of $y$ in the current period does not provide any useful information about the value of $y$ that will be observed in the next period. A different story is revealed in Figure 2.11, where we observe that the

![](images/d2a1e05b4fa589857744e2cfd93ecd76686d60a6f33a1f7db9de4a706df97995.jpg)  
FIGURE 2.11 Scatter diagram of chemical viscosity readings at lag $k = 1$ .

pairs of adjacent observations $y _ { t + 1 } , y _ { t }$ are positively correlated. That is, a small value of $y$ tends to be followed in the next time period by another small value of $y$ , and a large value of $y$ tends to be followed immediately by another large value of y. Note from inspection of Figures 2.10 and 2.11 that the behavior inferred from inspection of the scatter diagrams is reected in the observed time series.

The covariance between yt and its value at another time period, say, yt+k $y _ { t }$ $y _ { t + k }$ is called the autocovariance at lag $k$ , de-ned by

$$
\gamma_ {k} = \operatorname {C o v} \left(y _ {t}, y _ {t + k}\right) = E \left[ \left(y _ {t} - \mu\right) \left(y _ {t + k} - \mu\right) \right]. \tag {2.10}
$$

The collection of the values of $\gamma _ { k } , k = 0 , 1 , 2 , \ldots$ is called the autocovariance function. Note that the autocovariance at lag $k = 0$ is just the variance of the time series; that is, $\gamma _ { 0 } = \sigma _ { \mathrm { y } } ^ { 2 }$ ,which is constant for a stationary time series. The autocorrelation coef-cient at lag $k$ for a stationary time series is

$$
\rho_ {k} = \frac {E \left[ \left(y _ {t} - \mu\right) \left(y _ {t + k} - \mu\right) \right]}{\sqrt {E \left[ \left(y _ {t} - \mu\right) ^ {2} \right] E \left[ \left(y _ {t + k} - \mu\right) ^ {2} \right]}} = \frac {\operatorname {C o v} \left(y _ {t} , y _ {t + k}\right)}{\operatorname {V a r} \left(y _ {t}\right)} = \frac {\gamma_ {k}}{\gamma_ {0}}. \tag {2.11}
$$

The collection of the values of $\rho _ { k }$ , $k = 0 , 1 , 2 , \ldots$ is called the autocorrelation function (ACF). Note that by de-nition $\rho _ { 0 } = 1$ . Also, the ACF is independent of the scale of measurement of the time series, so it is a dimensionless quantity. Furthermore, $\rho _ { k } = \rho _ { - k }$ ; that is, the ACF is symmetric around zero, so it is only necessary to compute the positive (or negative) half.

If a time series has a -nite mean and autocovariance function it is said to be second-order stationary (or weakly stationary of order 2). If, in addition, the joint probability distribution of the observations at all times is multivariate normal, then that would be suf-cient to result in a time series that is strictly stationary.

It is necessary to estimate the autocovariance and ACFs from a time series of -nite length, say, $y _ { 1 } , y _ { 2 } , \ldots , y _ { T }$ . The usual estimate of the autocovariance function is

$$
c _ {k} = \hat {\gamma} _ {k} = \frac {1}{T} \sum_ {t = 1} ^ {T - k} \left(y _ {t} - \bar {y}\right) \left(y _ {t + k} - \bar {y}\right), \quad k = 0, 1, 2, \dots , K \tag {2.12}
$$

and the ACF is estimated by the sample autocorrelation function (or sample ACF)

$$
r _ {k} = \hat {\rho} _ {k} = \frac {c _ {k}}{c _ {0}}, \quad k = 0, 1, \dots , K \tag {2.13}
$$

A good general rule of thumb is that at least 50 observations are required to give a reliable estimate of the ACF, and the individual sample autocorrelations should be calculated up to lag $K$ , where $K$ is about $T / 4$ .

Often we will need to determine if the autocorrelation coef-cient at a particular lag is zero. This can be done by comparing the sample autocorrelation coef-cient at lag $k$ , $r _ { k }$ , to its standard error. If we make the assumption that the observations are uncorrelated, that is, $\rho _ { k } = 0$ for all $k$ , then the variance of the sample autocorrelation coef-cient is

$$
\operatorname {V a r} \left(r _ {k}\right) \cong \frac {1}{T} \tag {2.14}
$$

and the standard error is

$$
s e \left(r _ {k}\right) \cong \frac {1}{\sqrt {T}} \tag {2.15}
$$

Example 2.5 Consider the chemical process viscosity readings plotted in Figure 2.9; the values are listed in Table 2.1.

The sample ACF at lag $k = 1$ is calculated as

$$
\begin{array}{l} c _ {0} = \frac {1}{1 0 0} \sum_ {t = 1} ^ {1 0 0 - 0} (y _ {t} - \bar {y}) (y _ {t + 0} - \bar {y}) \\ = \frac {1}{1 0 0} [ (8 6. 7 4 1 8 - 8 4. 9 1 5 3) (8 6. 7 4 1 8 - 8 4. 9 1 5 3) + \dots \\ + (8 5. 0 5 7 2 - 8 4. 9 1 5 3) (8 5. 0 5 7 2 - 8 4. 9 1 5 3) ] \\ = 2 8 0. 9 3 3 2 \\ \end{array}
$$

$$
\begin{array}{l} c _ {1} = \frac {1}{1 0 0} \sum_ {t = 1} ^ {1 0 0 - 1} (y _ {t} - \bar {y}) (y _ {t + 1} - \bar {y}) \\ = \frac {1}{1 0 0} [ (8 6. 7 4 1 8 - 8 4. 9 1 5 3) (8 5. 3 1 9 5 - 8 4. 9 1 5 3) + \dots \\ + (8 7. 0 0 4 8 - 8 4. 9 1 5 3) (8 5. 0 5 7 2 - 8 4. 9 1 5 3) ] \\ = 2 2 0. 3 1 3 7 \\ \end{array}
$$

$$
r _ {1} = \frac {c _ {1}}{c _ {0}} = \frac {2 2 0 . 3 1 3 7}{2 8 0 . 9 3 3 2} = 0. 7 8 4 2
$$

A plot and listing of the sample ACFs generated by Minitab for the -rst 25 lags are displayed in Figures 2.12 and 2.13, respectively.

TABLE 2.1 Chemical Process Viscosity Readings   

<table><tr><td>Time 
Period</td><td>Reading</td><td>Time 
Period</td><td>Reading</td><td>Time 
Period</td><td>Reading</td><td>Time 
Period</td><td>Reading</td></tr><tr><td>1</td><td>86.7418</td><td>26</td><td>87.2397</td><td>51</td><td>85.5722</td><td>76</td><td>84.7052</td></tr><tr><td>2</td><td>85.3195</td><td>27</td><td>87.5219</td><td>52</td><td>83.7935</td><td>77</td><td>83.8168</td></tr><tr><td>3</td><td>84.7355</td><td>28</td><td>86.4992</td><td>53</td><td>84.3706</td><td>78</td><td>82.4171</td></tr><tr><td>4</td><td>85.1113</td><td>29</td><td>85.6050</td><td>54</td><td>83.3762</td><td>79</td><td>83.0420</td></tr><tr><td>5</td><td>85.1487</td><td>30</td><td>86.8293</td><td>55</td><td>84.9975</td><td>80</td><td>83.6993</td></tr><tr><td>6</td><td>84.4775</td><td>31</td><td>84.5004</td><td>56</td><td>84.3495</td><td>81</td><td>82.2033</td></tr><tr><td>7</td><td>84.6827</td><td>32</td><td>84.1844</td><td>57</td><td>85.3395</td><td>82</td><td>82.1413</td></tr><tr><td>8</td><td>84.6757</td><td>33</td><td>85.4563</td><td>58</td><td>86.0503</td><td>83</td><td>81.7961</td></tr><tr><td>9</td><td>86.3169</td><td>34</td><td>86.1511</td><td>59</td><td>84.8839</td><td>84</td><td>82.3241</td></tr><tr><td>10</td><td>88.0006</td><td>35</td><td>86.4142</td><td>60</td><td>85.4176</td><td>85</td><td>81.5316</td></tr><tr><td>11</td><td>86.2597</td><td>36</td><td>86.0498</td><td>61</td><td>84.2309</td><td>86</td><td>81.7280</td></tr><tr><td>12</td><td>85.8286</td><td>37</td><td>86.6642</td><td>62</td><td>83.5761</td><td>87</td><td>82.5375</td></tr><tr><td>13</td><td>83.7500</td><td>38</td><td>84.7289</td><td>63</td><td>84.1343</td><td>88</td><td>82.3877</td></tr><tr><td>14</td><td>84.4628</td><td>39</td><td>85.9523</td><td>64</td><td>82.6974</td><td>89</td><td>82.4159</td></tr><tr><td>15</td><td>84.6476</td><td>40</td><td>86.8473</td><td>65</td><td>83.5454</td><td>90</td><td>82.2102</td></tr><tr><td>16</td><td>84.5751</td><td>41</td><td>88.4250</td><td>66</td><td>86.4714</td><td>91</td><td>82.7673</td></tr><tr><td>17</td><td>82.2473</td><td>42</td><td>89.6481</td><td>67</td><td>86.2143</td><td>92</td><td>83.1234</td></tr><tr><td>18</td><td>83.3774</td><td>43</td><td>87.8566</td><td>68</td><td>87.0215</td><td>93</td><td>83.2203</td></tr><tr><td>19</td><td>83.5385</td><td>44</td><td>88.4997</td><td>69</td><td>86.6504</td><td>94</td><td>84.4510</td></tr><tr><td>20</td><td>85.1620</td><td>45</td><td>87.0622</td><td>70</td><td>85.7082</td><td>95</td><td>84.9145</td></tr><tr><td>21</td><td>83.7881</td><td>46</td><td>85.1973</td><td>71</td><td>86.1504</td><td>96</td><td>85.7609</td></tr><tr><td>22</td><td>84.0421</td><td>47</td><td>85.0767</td><td>72</td><td>85.8032</td><td>97</td><td>85.2302</td></tr><tr><td>23</td><td>84.1023</td><td>48</td><td>84.4362</td><td>73</td><td>85.6197</td><td>98</td><td>86.7312</td></tr><tr><td>24</td><td>84.8495</td><td>49</td><td>84.2112</td><td>74</td><td>84.2339</td><td>99</td><td>87.0048</td></tr><tr><td>25</td><td>87.6416</td><td>50</td><td>85.9952</td><td>75</td><td>83.5737</td><td>100</td><td>85.0572</td></tr></table>

![](images/87f2a4b4d099e8acbf8bfcffaa07edfdd2df80385455b6378156e32b4e2bd6ad.jpg)  
FIGURE 2.12 Sample autocorrelation function for chemical viscosity readings, with $5 \%$ signi-cance limits.

Autocorrelation function: reading   

<table><tr><td>Lag</td><td>ACF</td><td>T</td><td>LBQ</td></tr><tr><td>1</td><td>0.784221</td><td>7.84</td><td>63.36</td></tr><tr><td>2</td><td>0.628050</td><td>4.21</td><td>104.42</td></tr><tr><td>3</td><td>0.491587</td><td>2.83</td><td>129.83</td></tr><tr><td>4</td><td>0.362880</td><td>1.94</td><td>143.82</td></tr><tr><td>5</td><td>0.304554</td><td>1.57</td><td>153.78</td></tr><tr><td>6</td><td>0.208979</td><td>1.05</td><td>158.52</td></tr><tr><td>7</td><td>0.164320</td><td>0.82</td><td>161.48</td></tr><tr><td>8</td><td>0.144789</td><td>0.72</td><td>163.80</td></tr><tr><td>9</td><td>0.103625</td><td>0.51</td><td>165.01</td></tr><tr><td>10</td><td>0.066559</td><td>0.33</td><td>165.51</td></tr><tr><td>11</td><td>0.003949</td><td>0.02</td><td>165.51</td></tr><tr><td>12</td><td>-0.077226</td><td>-0.38</td><td>166.20</td></tr><tr><td>13</td><td>-0.051953</td><td>-0.25</td><td>166.52</td></tr><tr><td>14</td><td>0.020525</td><td>0.10</td><td>166.57</td></tr><tr><td>15</td><td>0.072784</td><td>0.36</td><td>167.21</td></tr><tr><td>16</td><td>0.070753</td><td>0.35</td><td>167.81</td></tr><tr><td>17</td><td>0.001334</td><td>0.01</td><td>167.81</td></tr><tr><td>18</td><td>-0.057435</td><td>-0.28</td><td>168.22</td></tr><tr><td>19</td><td>-0.123122</td><td>-0.60</td><td>170.13</td></tr><tr><td>20</td><td>-0.180546</td><td>-0.88</td><td>174.29</td></tr><tr><td>21</td><td>-0.162466</td><td>-0.78</td><td>177.70</td></tr><tr><td>22</td><td>-0.145979</td><td>-0.70</td><td>180.48</td></tr><tr><td>23</td><td>-0.087420</td><td>-0.42</td><td>181.50</td></tr><tr><td>24</td><td>-0.011579</td><td>-0.06</td><td>181.51</td></tr><tr><td>25</td><td>0.063170</td><td>0.30</td><td>182.06</td></tr></table>

FIGURE 2.13 Listing of sample autocorrelation functions for -rst 25 lags of chemical viscosity readings, Minitab session window output (the de-nition of T and LBQ will be given later).

Note the rate of decrease or decay in ACF values in Figure 2.12 from 0.78 to 0, followed by a sinusoidal pattern about 0. This ACF pattern is typical of stationary time series. The importance of ACF estimates exceeding the $5 \%$ signi-cance limits will be discussed in Chapter 5. In contrast, the plot of sample ACFs for a time series of random values with constant mean has a much different appearance. The sample ACFs for pharmaceutical product sales plotted in Figure 2.14 appear randomly positive or negative, with values near zero.

While the ACF is strictly speaking de-ned only for a stationary time series, the sample ACF can be computed for any time series, so a logical question is: What does the sample ACF of a nonstationary time series look like? Consider the daily closing price for Whole Foods Market stock in Figure 1.7. The sample ACF of this time series is shown in Figure 2.15. Note that this sample ACF plot behaves quite differently than the ACF plots in Figures 2.12 and 2.14. Instead of cutting off or tailing off near zero after a few lags, this sample ACF is very persistent; that is, it decays very slowly and exhibits sample autocorrelations that are still rather large even at long lags. This behavior is characteristic of a nonstationary time series. Generally, if the sample ACF does not dampen out within about 15 to 20 lags, the time series is nonstationary.

![](images/0c142e4185b86af64ff6048f70fa2c22a08b19990ad548e99b77209e0e0b5128.jpg)  
FIGURE 2.14 Autocorrelation function for pharmaceutical product sales, with $5 \%$ signi-cance limits.

# 2.3.3 The Variogram

We have discussed two techniques for determining if a time series is nonstationary, plotting a reasonable long series of the data to see if it drifts or wanders away from its mean for long periods of time, and computing the sample ACF. However, often in practice there is no clear demarcation

![](images/c778bd00991621fc4d08fb0ba713e36efd2bf59c5dcb89aa02b8e7a46bff5f1e.jpg)  
FIGURE 2.15 Autocorrelation function for Whole Foods Market stock price, with $5 \%$ signi-cance limits.

between a stationary and a nonstationary process for many real-world time series. An additional diagnostic tool that is very useful is the variogram.

Suppose that the time series observations are represented by $y _ { t }$ . The variogram $G _ { k }$ measures variances of the differences between observations that are $k$ lags apart, relative to the variance of the differences that are one time unit apart (or at lag 1). The variogram is de-ned mathematically as

$$
G _ {k} = \frac {\operatorname {V a r} \left(y _ {t + k} - y _ {t}\right)}{\operatorname {V a r} \left(y _ {t + 1} - y _ {t}\right)} \quad k = 1, 2, \dots \tag {2.16}
$$

and the values of $G _ { k }$ are plotted as a function of the lag $k$ . If the time series is stationary, it turns out that

$$
G _ {k} = \frac {1 - \rho_ {k}}{1 - \rho_ {1}},
$$

but for a stationary time series $\rho _ { k } \to 0$ as $k$ increases, so when the variogram is plotted against lag $k$ , $G _ { k }$ will reach an asymptote $1 / ( 1 - \rho _ { 1 } )$ . However, if the time series is nonstationary, $G _ { k }$ will increase monotonically.

Estimating the variogram is accomplished by simply applying the usual sample variance to the differences, taking care to account for the changing sample sizes when the differences are taken (see Haslett (1997)). Let

$$
d _ {t} ^ {k} = y _ {t + k} - y _ {t}
$$

$$
\bar {d} ^ {k} = \frac {1}{T - k} \sum d _ {i} ^ {k}.
$$

Then an estimate of Var $( y _ { t + k } - y _ { t } )$ is

$$
s _ {k} ^ {2} = \frac {\sum_ {t = 1} ^ {T - k} \left(d _ {t} ^ {k} - \bar {d} ^ {k}\right) ^ {2}}{T - k - 1}.
$$

Therefore the sample variogram is given by

$$
\hat {G} _ {k} = \frac {s _ {k} ^ {2}}{s _ {1} ^ {2}} k = 1, 2, \dots \tag {2.17}
$$

To illustrate the use of the variogram, consider the chemical process viscosity data plotted in Figure 2.9. Both the data plot and the sample ACF in

<table><tr><td>Lag</td><td>Variogram</td><td>Plot Variogram</td></tr><tr><td>1</td><td>1.0000</td><td></td></tr><tr><td>2</td><td>1.7238</td><td></td></tr><tr><td>3</td><td>2.3562</td><td></td></tr><tr><td>4</td><td>2.9527</td><td></td></tr><tr><td>5</td><td>3.2230</td><td></td></tr><tr><td>6</td><td>3.6659</td><td></td></tr><tr><td>7</td><td>3.8729</td><td></td></tr><tr><td>8</td><td>3.9634</td><td></td></tr><tr><td>9</td><td>4.1541</td><td></td></tr><tr><td>10</td><td>4.3259</td><td></td></tr><tr><td>11</td><td>4.6161</td><td></td></tr><tr><td>12</td><td>4.9923</td><td></td></tr><tr><td>13</td><td>4.8752</td><td></td></tr><tr><td>14</td><td>4.5393</td><td></td></tr><tr><td>15</td><td>4.2971</td><td></td></tr><tr><td>16</td><td>4.3065</td><td></td></tr><tr><td>17</td><td>4.6282</td><td></td></tr><tr><td>18</td><td>4.9006</td><td></td></tr><tr><td>19</td><td>5.2050</td><td></td></tr><tr><td>20</td><td>5.4711</td><td></td></tr><tr><td>21</td><td>5.3873</td><td></td></tr><tr><td>22</td><td>5.3109</td><td></td></tr><tr><td>23</td><td>5.0395</td><td></td></tr><tr><td>24</td><td>4.6880</td><td></td></tr><tr><td>25</td><td>4.3416</td><td></td></tr></table>

FIGURE 2.16 JMP output for the sample variogram of the chemical process viscosity data from Figure 2.19.

Figures 2.12 and 2.13 suggest that the time series is stationary. Figure 2.16 is the variogram. Many software packages do not offer the variogram as a standard pull-down menu selection, but the JMP package does. Without software, it is still fairly easy to compute.

Start by computing the successive differences of the time series for a number of lags and then -nd their sample variances. The ratios of these sample variances to the sample variance of the -rst differences will produce the sample variogram. The JMP calculations of the sample variogram are shown in Figure 2.16 and a plot is given in Figure 2.17. Notice that the sample variogram generally converges to a stable level and then uctuates around it. This is consistent with a stationary time series, and it provides additional evidence that the chemical process viscosity data are stationary.

Now let us see what the sample variogram looks like for a nonstationary time series. The Whole Foods Market stock price data from Appendix Table B.7 originally shown in Figure 1.7 are apparently nonstationary, as it wanders about with no obvious -xed level. The sample ACF in Figure 2.15 decays very slowly and as noted previously, gives the impression that the time series is nonstationary. The calculations for the variogram from JMP are shown in Figure 2.18 and the variogram is plotted in Figure 2.19.

![](images/38c66a15936808e49887808abcf8fc9abdb7bf96d9879f08d4a2f7308b4d915d.jpg)  
FIGURE 2.18 JMP output for the sample variogram of the Whole Foods Market stock price data from Figure 1.7 and Appendix Table B.7.

FIGURE 2.17 JMP sample variogram of the chemical process viscosity data from Figure 2.9.

<table><tr><td>Lag</td><td>Variogram</td><td>Plot Variogram</td></tr><tr><td>1</td><td>1.0000</td><td></td></tr><tr><td>2</td><td>2.0994</td><td></td></tr><tr><td>3</td><td>3.2106</td><td></td></tr><tr><td>4</td><td>4.3960</td><td></td></tr><tr><td>5</td><td>5.4982</td><td></td></tr><tr><td>6</td><td>6.5810</td><td></td></tr><tr><td>7</td><td>7.5690</td><td></td></tr><tr><td>8</td><td>8.5332</td><td></td></tr><tr><td>9</td><td>9.4704</td><td></td></tr><tr><td>10</td><td>10.4419</td><td></td></tr><tr><td>11</td><td>11.4154</td><td></td></tr><tr><td>12</td><td>12.3452</td><td></td></tr><tr><td>13</td><td>13.3759</td><td></td></tr><tr><td>14</td><td>14.4411</td><td></td></tr><tr><td>15</td><td>15.6184</td><td></td></tr><tr><td>16</td><td>16.9601</td><td></td></tr><tr><td>17</td><td>18.2442</td><td></td></tr><tr><td>18</td><td>19.3782</td><td></td></tr><tr><td>19</td><td>20.3934</td><td></td></tr><tr><td>20</td><td>21.3618</td><td></td></tr><tr><td>21</td><td>22.4010</td><td></td></tr><tr><td>22</td><td>23.4788</td><td></td></tr><tr><td>23</td><td>24.5450</td><td></td></tr><tr><td>24</td><td>25.5906</td><td></td></tr><tr><td>25</td><td>26.6620</td><td></td></tr></table>

![](images/3d97de8ca6a07ce878ad85d483da6534feda7e314a710152948b354ba43ed895.jpg)  
FIGURE 2.19 Sample variogram of the Whole Foods Market stock price data from Figure 1.7 and Appendix Table B.7.

Notice that the sample variogram in Figure 2.19 increases monotonically for all 25 lags. This is a strong indication that the time series is nonstationary.

# 2.4 USE OF DATA TRANSFORMATIONS AND ADJUSTMENTS

# 2.4.1 Transformations

Data transformations are useful in many aspects of statistical work, often for stabilizing the variance of the data. Nonconstant variance is quite common in time series data. For example, the International Sunspot Numbers plotted in Figure 2.20a show cyclic patterns of varying magnitudes. The variability from about 1800 to 1830 is smaller than that from about 1830 to 1880; other small periods of constant, but different, variances can also be identi-ed.

A very popular type of data transformation to deal with nonconstant variance is the power family of transformations, given by

$$
y ^ {(\lambda)} = \left\{ \begin{array}{l l} \frac {y ^ {\lambda} - 1}{\lambda \dot {y} ^ {\lambda - 1}}, & \lambda \neq 0 \\ \dot {y} \ln y, & \lambda = 0 \end{array} , \right. \tag {2.18}
$$

![](images/e187e380e2e3f4e539fc87db76ca0457cf9394cfe647a564467dbfa45388fdff.jpg)  
(a)

![](images/6909bf06c5ea461d7a4fa04202c2d3e0d0f498f3fc3d5e9a4e33e71b2b8b962c.jpg)  
(b)   
FIGURE 2.20 Yearly International Sunspot Number, (a) untransformed and (b) natural logarithm transformation. Source: SIDC.

where $\begin{array} { r } { \dot { y } = \exp [ ( 1 / T ) \sum _ { t = 1 } ^ { T } \ln y _ { t } ] } \end{array}$ is the geometric mean of the observations. If $\lambda = 1$ , there is no transformation. Typical values of $\lambda$ used with time series data are $\lambda = 0 . 5$ (a square root transformation), $\lambda = 0$ (the log transformation), $\lambda = - 0 . 5$ (reciprocal square root transformation), and $\lambda = - 1$ (inverse transformation). The divisor $\displaystyle { \dot { y } } ^ { \lambda - 1 }$ is simply a scale factor that ensures that when different models are -t to investigate the utility of different transformations (values of $\lambda$ ), the residual sum of squares for these models can be meaningfully compared. The reason that $\lambda = 0$ implies a log transformation is that $( y ^ { \lambda } - 1 ) / \lambda$ approaches the log of $y$ as $\lambda$ approaches zero. Often an appropriate value of $\lambda$ is chosen empirically by -tting a model to $y ^ { ( \lambda ) }$ for various values of $\lambda$ and then selecting the transformation that produces the minimum residual sum of squares.

The log transformation is used frequently in situations where the variability in the original time series increases with the average level of the series. When the standard deviation of the original series increases linearly with the mean, the log transformation is in fact an optimal variancestabilizing transformation. The log transformation also has a very nice physical interpretation as percentage change. To illustrate this, let the time series be $y _ { 1 } , y _ { 2 }$ , 鈥?, yT and suppose that we are interested in the percentage change in $y _ { t }$ , say,

$$
x _ {t} = \frac {1 0 0 (y _ {t} - y _ {t - 1})}{y _ {t - 1}},
$$

The approximate percentage change in $y _ { t }$ can be calculated from the differences of the log-transformed time series $x _ { t } \cong 1 0 0 [ \ln ( y _ { t } ) - \ln ( y _ { t - 1 } ) ]$ because

$$
\begin{array}{l} 1 0 0 [ \ln (y _ {t}) - \ln (y _ {t - 1}) ] = 1 0 0 \ln \left(\frac {y _ {t}}{y _ {t - 1}}\right) = 1 0 0 \ln \left(\frac {y _ {t - 1} + (y _ {t} - y _ {t - 1})}{y _ {t - 1}}\right) \\ = 1 0 0 \ln \left(1 + \frac {x _ {t}}{1 0 0}\right) \cong x _ {t} \\ \end{array}
$$

since $\ln ( 1 + z ) \cong z$ when z is small.

The application of a natural logarithm transformation to the International Sunspot Number, as shown in Figure 2.20b, tends to stabilize the variance and leaves just a few unusual values.

# 2.4.2 Trend and Seasonal Adjustments

In addition to transformations, there are also several types of adjustments that are useful in time series modeling and forecasting. Two of the most widely used are trend adjustments and seasonal adjustments. Sometimes these procedures are called trend and seasonal decomposition.

A time series that exhibits a trend is a nonstationary time series. Modeling and forecasting of such a time series is greatly simpli-ed if we can eliminate the trend. One way to do this is to -t a regression model describing the trend component to the data and then subtracting it out of the original observations, leaving a set of residuals that are free of trend. The trend models that are usually considered are the linear trend, in which the mean of $y _ { t }$ is expected to change linearly with time as in

$$
E \left(y _ {t}\right) = \beta_ {0} + \beta_ {1} t \tag {2.19}
$$

or as a quadratic function of time

$$
E \left(y _ {t}\right) = \beta_ {0} + \beta_ {1} t + \beta_ {2} t ^ {2} \tag {2.20}
$$

or even possibly as an exponential function of time such as

$$
E \left(y _ {t}\right) = \beta_ {0} e ^ {\beta_ {1} t}. \tag {2.21}
$$

The models in Eqs. (2.19)鈥?2.21) are usually -t to the data by using ordinary least squares.

Example 2.6 We will show how least squares can be used to -t regression models in Chapter 3. However, it would be useful at this point to illustrate how trend adjustment works. Minitab can be used to perform trend adjustment. Consider the annual US production of blue and gorgonzola cheeses

![](images/02b3b2982c863e28cf5d960395b3f87e3084738b20771c8a3af4aaf9fdb66abf.jpg)  
FIGURE 2.21 Blue and gorgonzola cheese production, with -tted regression line. Source: USDA鈥揘ASS.

shown in Figure 1.4. There is clearly a positive, nearly linear trend. The trend analysis plot in Figure 2.21 shows the original time series with the -tted line.

Plots of the residuals from this model indicate that, in addition to an underlying trend, there is additional structure. The normal probability plot (Figure 2.22a) and histogram (Figure 2.22c) indicate the residuals are

![](images/8334db47a1dab6d454ca4e4108d01ca384da100f81a32c3a0911391685d9f908.jpg)

![](images/c3adabc1aae4c5bbdf062eab6f460f81fdd7298bcc2ce8fb271d9e078c5c4a8b.jpg)  
(b)

![](images/0239ce0d28e11c821b901bf5e12d88e62c9652b79df41001bf3c56c90e9e3ea1.jpg)  
(c)

![](images/813e5ab4982de58e303823bebaca667bdc512734b275a7f323cfbd5b716c2704.jpg)  
(d)   
FIGURE 2.22 Residual plots for simple linear regression model of blue and gorgonzola cheese production.

approximately normally distributed. However, the plots of residuals versus -tted values (Figure 2.22b) and versus observation order (Figure 2.22d) indicate nonconstant variance in the last half of the time series. Analysis of model residuals is discussed more fully in Chapter 3.

Another approach to removing trend is by differencing the data; that is, applying the difference operator to the original time series to obtain a new time series, say,

$$
x _ {t} = y _ {t} - y _ {t - 1} = \nabla y _ {t}, \tag {2.22}
$$

where $\nabla$ is the (backward) difference operator. Another way to write the differencing operation is in terms of a backshift operator $B$ , de-ned as $B y _ { t } = y _ { t - 1 }$ , so

$$
x _ {t} = (1 - B) y _ {t} = \nabla y _ {t} = y _ {t} - y _ {t - 1} \tag {2.23}
$$

with $\nabla = ( 1 - B )$ . Differencing can be performed successively if necessary until the trend is removed; for example, the second difference is

$$
x _ {t} = \nabla^ {2} y _ {t} = \nabla (\nabla y _ {t}) = (1 - B) ^ {2} y _ {t} = (1 - 2 B + B ^ {2}) = y _ {t} - 2 y _ {t - 1} + y _ {t - 2} \tag {2.24}
$$

In general, powers of the backshift operator and the backward difference operator are de-ned as

$$
\begin{array}{l} B ^ {d} y _ {t} = y _ {t - d} \\ \nabla^ {d} = (1 - B) ^ {d} \tag {2.25} \\ \end{array}
$$

Differencing has two advantages relative to -tting a trend model to the data. First, it does not require estimation of any parameters, so it is a more parsimonious (i.e., simpler) approach; and second, model -tting assumes that the trend is -xed throughout the time series history and will remain so in the (at least immediate) future. In other words, the trend component, once estimated, is assumed to be deterministic. Differencing can allow the trend component to change through time. The -rst difference accounts for a trend that impacts the change in the mean of the time series, the second difference accounts for changes in the slope of the time series, and so forth. Usually, one or two differences are all that is required in practice to remove an underlying trend in the data.

Example 2.7 Reconsider the blue and gorgonzola cheese production data. A difference of one applied to this time series removes the increasing trend (Figure 2.23) and also improves the appearance of the residuals plotted versus -tted value and observation order when a linear model is -tted to the detrended time series (Figure 2.24). This illustrates that differencing may be a very good alternative to detrending a time series by using a regression model.

![](images/dd672879b57078f0247cc220b9ece45bfd0b6fe38ff00a6840f19f97cf0ad492.jpg)  
FIGURE 2.23 Blue and gorgonzola cheese production, with one difference. Source: USDA鈥揘ASS.

![](images/ad0de78746cff961b1399418b3800193011583877cd3f3d16fbc7b12e071035b.jpg)  
(a)

![](images/5f8f3f64541e30dcf95020bc288620fb3ced25e6849d19da1e04dc2bfb27c787.jpg)

![](images/d897a9a2be0f8832df691508f9f839d67309d0b8203c85d9b6b1914ff7122589.jpg)  
(c)

![](images/934902f6a52de02b240056c2c3d9dfdf3de29177cd99745ccddcd9b4ae6d7cc7.jpg)  
(d)   
FIGURE 2.24 Residual plots for one difference of blue and gorgonzola cheese production.

Seasonal, or both trend and seasonal, components are present in many time series. Differencing can also be used to eliminate seasonality. De-ne a lag鈥攄 seasonal difference operator as

$$
\nabla_ {d} y _ {t} = \left(1 - B ^ {d}\right) = y _ {t} - y _ {t - d}. \tag {2.26}
$$

For example, if we had monthly data with an annual season (a very common situation), we would likely use $d = 1 2$ , so the seasonally differenced data would be

$$
\nabla_ {1 2} y _ {t} = (1 - B ^ {1 2}) y _ {t} = y _ {t} - y _ {t - 1 2}.
$$

When both trend and seasonal components are simultaneously present, we can sequentially difference to eliminate these effects. That is, -rst seasonally difference to remove the seasonal component and then difference one or more times using the regular difference operator to remove the trend.

Example 2.8 The beverage shipment data shown in Figure 2.2 appear to have a strong monthly pattern鈥擩anuary consistently has the lowest shipments in a year while the peak shipments are in May and June. There is also an overall increasing trend from year to year that appears to be the same regardless of month.

A seasonal difference of twelve followed by a trend difference of one was applied to the beverage shipments, and the results are shown in Figure 2.25. The seasonal differencing removes the monthly pattern (Figure 2.25a), and the second difference of one removes the overall increasing trend (Figure 2.25b). The -tted linear trend line in Figure 2.25b has a slope of virtually zero. Examination of the residual plots in Figure 2.26 does not reveal any problems with the linear trend model -t to the differenced data.

Regression models can also be used to eliminate seasonal (or trend and seasonal components) from time series data. A simple but useful model is

$$
E \left(y _ {t}\right) = \beta_ {0} + \beta_ {1} \sin \frac {2 \pi}{d} t + \beta_ {2} \cos \frac {2 \pi}{d} t, \tag {2.27}
$$

where $d$ is the period (or length) of the season and $2 \pi / d$ is expressed in radians. For example, if we had monthly data and an annual season, then $d = 1 2$ . This model describes a simple, symmetric seasonal pattern that

![](images/b74ca0c99b919a5ae882f54e443d8fafdfd6b1e03b3abcbaefcb177098c3123b.jpg)

![](images/39aef94cc5e0667155575757067c1e1578a3cc5f2ee96bfd92beb32c67351cde.jpg)  
(a)   
(b)   
FIGURE 2.25 Time series plots of seasonal- and trend-differenced beverage data.

repeats every 12 periods. The model is actually a sine wave. To see this, recall that a sine wave with amplitude $\beta$ , phase angle or origin $\theta$ , and period or cycle length $\omega$ can be written as

$$
E \left(y _ {t}\right) = \beta \sin \omega (t + \theta). \tag {2.28}
$$

![](images/18d93f43b41cf44edc3912f941a8ab40b3b1b08e61598fc8d9e7a87f39af9dd0.jpg)

![](images/8db134983ed31021cd5c2499f080a8bd1185e7f6610fd2c2f18049d1f1c33901.jpg)  
(b)

![](images/d019c4d3259f775d65cc179db43b02d7196aba6b11e7108ba7846850864b07f3.jpg)  
(c)

![](images/0a78f028dae0b420d5a6e4b964fa07458cfbd02a877f94f95afec0de80b638c4.jpg)  
(d)   
FIGURE 2.26 Residual plots for linear trend model of differenced beverage shipments.

Equation (2.27) was obtained by writing Eq. (2.28) as a sine鈥揷osine pair using the trigonometric identity $\sin ( u + \nu ) = \cos u \sin \nu + \sin u \cos \nu$ and adding an intercept term $\beta _ { 0 }$ :

$$
\begin{array}{l} E (y _ {t}) = \beta \sin \omega (t + \theta) \\ = \beta \cos \omega \theta \sin \omega t + \beta \sin \omega \theta \cos \omega t \\ = \beta_ {1} \sin \omega t + \beta_ {2} \cos \omega t \\ \end{array}
$$

where $\beta _ { 1 } = \beta \cos \omega \theta$ and $\beta _ { 2 } = \beta \sin \omega \theta$ . Setting $\omega = 2 \pi / 1 2$ and adding the intercept term $\beta _ { 0 }$ produces Eq. (2.27). This model is very exible; for example, if we set $\omega = 2 \pi / 5 2$ we can model a yearly seasonal pattern that is observed weekly, if we set $\omega = 2 \pi / 4$ we can model a yearly seasonal pattern observed quarterly, and if we set $\omega = 2 \pi / 1 3$ we can model an annual seasonal pattern observed in 13 four-week periods instead of the usual months.

Equation (2.27) incorporates a single sine wave at the fundamental frequency $\omega = 2 \pi / 1 2$ . In general, we could add harmonics of the fundamental frequency to the model in order to model more complex seasonal patterns. For example, a very general model for monthly data and

an annual season that uses the fundamental frequency and the -rst three harmonics is

$$
E \left(y _ {t}\right) = \beta_ {0} + \sum_ {j = 1} ^ {4} \left(\beta_ {j} \sin \frac {2 \pi j}{1 2} t + \beta_ {4 + j} \cos \frac {2 \pi j}{1 2} t\right). \tag {2.29}
$$

If the data are observed in 13 four-week periods, the model would be

$$
E \left(y _ {t}\right) = \beta_ {0} + \sum_ {j = 1} ^ {4} \left(\beta_ {j} \sin \frac {2 \pi j}{1 3} t + \beta_ {4 + j} \cos \frac {2 \pi j}{1 3} t\right). \tag {2.30}
$$

There is also a 鈥渃lassical鈥?approach to decomposition of a time series into trend and seasonal components (actually, there are a lot of different decomposition algorithms; here we explain a very simple but useful approach). The general mathematical model for this decomposition is

$$
y _ {t} = f (S _ {t}, T _ {t}, \varepsilon_ {t}),
$$

where $S _ { t }$ is the seasonal component, $T _ { t }$ is the trend effect (sometimes called the trend-cycle effect), and $\varepsilon _ { t }$ is the random error component. There are usually two forms for the function $f$ ; an additive model

$$
y _ {t} = S _ {t} + T _ {t} + \varepsilon_ {t}
$$

and a multiplicative model

$$
y _ {t} = S _ {t} T _ {t} \varepsilon_ {t}.
$$

The additive model is appropriate if the magnitude (amplitude) of the seasonal variation does not vary with the level of the series, while the multiplicative version is more appropriate if the amplitude of the seasonal uctuations increases or decreases with the average level of the time series.

Decomposition is useful for breaking a time series down into these component parts. For the additive model, it is relatively easy. First, we would model and remove the trend. A simple linear model could be used to do this, say, $T _ { t } = \beta _ { 0 } + \beta _ { 1 } t .$ . Other methods could also be used. Moving averages can be used to isolate a trend and remove it from the original data, as could more sophisticated regression methods. These techniques might be appropriate when the trend is not a straight line over the history of the

time series. Differencing could also be used, although it is not typically in the classical decomposition approach.

Once the trend or trend-cycle component is estimated, the series is detrended:

$$
y _ {t} - T _ {t} = S _ {t} + \varepsilon_ {t}.
$$

Now a seasonal factor can be calculated for each period in the season. For example, if the data are monthly and an annual season is anticipated, we would calculate a season effect for each month in the data set. Then the seasonal indices are computed by taking the average of all of the seasonal factors for each period in the season. In this example, all of the January seasonal factors are averaged to produce a January season index; all of the February seasonal factors are averaged to produce a February season index; and so on. Sometimes medians are used instead of averages. In multiplicative decomposition, ratios are used, so that the data are detrended by

$$
\frac {y _ {t}}{T _ {t}} = S _ {t} \varepsilon_ {t}.
$$

The seasonal indices are estimated by taking the averages over all of the detrended values for each period in the season.

Example 2.9 The decomposition approach can be applied to the beverage shipment data. Examining the time series plot in Figure 2.2, there is both a strong positive trend as well as month-to-month variation, so the model should include both a trend and a seasonal component. It also appears that the magnitude of the seasonal variation does not vary with the level of the series, so an additive model is appropriate.

Results of a time series decomposition analysis from Minitab of the beverage shipments are in Figure 2.27, showing the original data (labeled 鈥淎ctual鈥?; along with the -tted trend line (鈥淭rend鈥? and the predicted values (鈥淔its鈥? from the additive model with both the trend and seasonal components.

Details of the seasonal analysis are shown in Figure 2.28. Estimates of the monthly variation from the trend line for each season (seasonal indices) are in Figure 2.28a with boxplots of the actual differences in Figure 2.28b. The percent of variation by seasonal period is in Figure 2.28c, and model residuals by seasonal period are in Figure 2.28d.

![](images/7e74b8c627112ce9c3d189d92a4166722b487809a2fffa56b3ccc7ba254881d2.jpg)  
FIGURE 2.27 Time series plot of decomposition model for beverage shipments.

Additional details of the component analysis are shown in Figure 2.29. Figure 2.29a is the original time series, Figure 2.29b is a plot of the time series with the trend removed, Figure 2.29c is a plot of the time series with the seasonality removed, and Figure 2.29d is essentially a residual plot of the detrended and seasonally adjusted data. The wave-like pattern in Figure 2.29d suggests a potential issue with the assumption of constant variance over time.

![](images/312c2da9a3598787241c4dd79b2e38cc45a21cb1ff5bc7e35af7630666c78e4b.jpg)

![](images/4cacf5a087d1d646cf40b9bfd57565d9c55fe54697c2e2ea3ab159c57cbafeea.jpg)  
(b)

![](images/b6dee1c20c715d763dc7686926190da6dda72802d352c8eb03baf494b104d08f.jpg)  
(c)

![](images/523665f3ffa028e655890bbda7532fae3a750858c640dfc31deb177ed6a71a7a.jpg)  
(d)   
FIGURE 2.28 Seasonal analysis for beverage shipments.

![](images/9b12e218bd45e74fed76ac494ae77992cef89c43193dfffcda7cfe3ff03b3815.jpg)  
(a) Original data

![](images/9f0b3aaa12311a60d99999f5ee608abaf03b7aa05b8f7395ee01625c999f77fe.jpg)  
(b) Detrended data

![](images/248325a6dc87a928e7bd80055fbea92ce9a18f939d378cb8478817e298254078.jpg)  
(c) Seasonally adjusted data

![](images/3064e61c7b1873ac7d1edd7cfd7c49cdf7ec3c48e54cfeb4f174d6b1f48d9c95.jpg)  
(d) Seasonally Adj. and detrended data   
FIGURE 2.29 Component analysis of beverage shipments.

Looking at the normal probability plot and histogram of residuals (Figure 2.30a,c), there does not appear to be an issue with the normality assumption. Figure 2.30d is the same plot as Figure 2.29d. However, variance does seem to increase as the predicted value increases; there is a funnel shape to the residuals plotted in Figure 2.30b. A natural logarithm transformation of the data may stabilize the variance and allow a useful decomposition model to be -t.

Results from the decomposition analysis of the natural log-transformed beverage shipment data are plotted in Figure 2.31, with the transformed data, -tted trend line, and predicted values. Figure 2.32a shows the transformed data, Figure 2.32b the transformed data with the trend removed, Figure 2.32c the transformed data with seasonality removed, and Figure 2.32d the residuals plot of the detrended and seasonally adjusted transformed data. The residual plots in Figure 2.33 indicate that the variance over the range of the predicted values is now stable (Figure 2.33b), and there are no issues with the normality assumption (Figures 2.33a,c). However, there is still a wave-like pattern in the plot of residuals versus time,

![](images/a41792de1fd4ac91f5729a6b4d17901b9a75f45b3c95e63dc7d2c0a02689334d.jpg)  
(a)

![](images/71768f04b872ff34ff37ca37a41256b7637bbcc900195c5adcffefc953e60216.jpg)  
(b)

![](images/5dd46fcdd5622e87f14b05f1769158c7a0bf18add8a5efd64da660ee1ecc681b.jpg)  
(c)

![](images/b5b74364be86785d97d3f033a15b5abf6160c3a9cdda0b213ed1d22324b14d8b.jpg)  
(d)   
FIGURE 2.30 Residual plots for additive model of beverage shipments.

both Figures 2.32d and 2.33d, indicating that some other structure in the transformed data over time is not captured by the decomposition model. This was not an issue with the model based on seasonal and trend differencing (Figures 2.25 and 2.26), which may be a more appropriate model for monthly beverage shipments.

![](images/946ecde07997b2696c6c8b7c30567f1e660f9313bb29c69478a8ce8092dc299b.jpg)  
FIGURE 2.31 Time series plot of decomposition model for transformed beverage data.

![](images/b9ec8e75c79faa296f0efad438b15861461acc9290e5afc77a3d6199b87ddd6a.jpg)

![](images/866e5c448d58f67061b949e216299d8c62170fb1328ea21a9c8724c536c07fc7.jpg)

![](images/39d48b7ac7c774022b84dd637d613659c4140eb3473e4b20f76471821b48b8c8.jpg)

![](images/5cc7ef88871371e7f95807dfbc1016db481aa11f93ac5eeeaccd2554829e049d.jpg)  
  
FIGURE 2.32 Component analysis of transformed beverage data.

![](images/9ec70f3892e4831d60945381df853027e62cbb7896c8b9aedf732fd7c04d5c27.jpg)  
(a)

![](images/0a40fc7bbb2e47c6ab966d5bcb8c37720a9918b34c35fc4604a502bb6e0ddfe2.jpg)  
(b)

![](images/a4f24366e676f034fb52a6806f3593de21601d7c6be3b98259d2356d60560580.jpg)  
(c)

![](images/7d8be342af3a09c7cf4a6a8d3e0d98974b75ce01ba7f7222a0eb038be90ec01a.jpg)  
(d)   
FIGURE 2.33 Residual plots from decomposition model for transformed beverage data.

Another technique for seasonal adjustment that is widely used in modeling and analyzing economic data is the X-11 method. Much of the development work on this method was done by Julian Shiskin and others at the US Bureau of the Census beginning in the mid-1950s and culminating into the X-11 Variant of the Census Method II Seasonal Adjustment Program. References for this work during this period include Shiskin (1958), and Marris (1961). Authoritative documentation for the X-11 procedure is in Shiskin, Young, and Musgrave (1967). The X-11 method uses symmetric moving averages in an iterative approach to estimating the trend and seasonal components. At the end of the series, however, these symmetric weights cannot be applied. Asymmetric weights have to be used.

JMP (V12 and higher) provides the X-11 technique. Figure 2.34 shows the JMP X-11 output for the beverage shipment data from Figure 2.2. The upper part of the output contains a plot of the original time series, followed by the sample ACF and PACF. Then Display D10 in the -gure shows the -nal estimates of the seasonal factors by month followed in Display D13 by the irregular or deseasonalized series. The -nal display is a plot of the original and adjusted time series.

While different variants of the X-11 technique have been proposed, the most important method to date has been the X-11-ARIMA method developed at Statistics Canada. This method uses Box鈥揓enkins autoregressive integrated moving average models (which are discussed in Chapter 5) to extend the series. The use of ARIMA models will result in differences in the -nal component estimates. Details of this method are in Dagum (1980, 1983, 1988).

# 2.5 GENERAL APPROACH TO TIME SERIES MODELINGAND FORECASTING

The techniques that we have been describing form the basis of a general approach to modeling and forecasting time series data. We now give a broad overview of the approach. This should give readers a general understanding of the connections between the ideas we have presented in this chapter and guidance in understanding how the topics in subsequent chapters form a collection of useful techniques for modeling and forecasting time series.

The basic steps in modeling and forecasting a time series are as follows:

1. Plot the time series and determine its basic features, such as whether trends or seasonal behavior or both are present. Look for possible outliers or any indication that the time series has changed with respect

![](images/75397f5b7d558a9d006c02ee60ae78f6f3229daa7bcbd3d7af2d380251954103.jpg)

<table><tr><td>Mean</td><td>5238.6611</td></tr><tr><td>Std</td><td>782.42158</td></tr><tr><td>N</td><td>180</td></tr><tr><td>Zero Mean ADF</td><td>-0.793136</td></tr><tr><td>Single Mean ADF</td><td>-8.334716</td></tr><tr><td>Trend ADF</td><td>-8.501149</td></tr></table>

![](images/9841804c3ae3ffd343507d29235cf6e3082030d1a1daa65f4ccdb6977e13100f.jpg)

![](images/a922331a082ab7a80e3c0bdbcae46d05359572c5c20981bbd8ab0fa7bc2b7b2d.jpg)  
FIGURE 2.34 JMP output for the X-11 procedure.

to its basic features (such as trends or seasonality) over the time period history.

2. Eliminate any trend or seasonal components, either by differencing or by -tting an appropriate model to the data. Also consider using data transformations, particularly if the variability in the time series seems to be proportional to the average level of the series. The objective of these operations is to produce a set of stationary residuals.

![](images/6aa499f0d23d6b945d9a0abe5399519a02d22f82a2da5bcc68bfe18570b6c44f.jpg)

![](images/66fec0a0d3f13f3ee6761ee2a0f36bdcc9b520cadcff1a7149eb8af35b1a26ee.jpg)

![](images/fc7c611c9f22a4a74a96e68472db6ea01e333165af39c90a8dcae9f5b770d592.jpg)  
FIGURE 2.34 (Continued)

3. Develop a forecasting model for the residuals. It is not unusual to -nd that there are several plausible models, and additional analysis will have to be performed to determine the best one to deploy. Sometimes potential models can be eliminated on the basis of their -t to the historical data. It is unlikely that a model that -ts poorly will produce good forecasts.

4. Validate the performance of the model (or models) from the previous step. This will probably involve some type of split-sample or crossvalidation procedure. The objective of this step is to select a model to use in forecasting. We will discuss this more in the next section and illustrate these techniques throughout the book.   
5. Also of interest are the differences between the original time series $y _ { t }$ and the values that would be forecast by the model on the original scale. To forecast values on the scale of the original time series $y _ { t }$ , reverse the transformations and any differencing adjustments made to remove trends or seasonal effects.   
6. For forecasts of future values in period $T + \tau$ on the original scale, if a transformation was used, say, $x _ { t } = \ln y _ { t }$ , then the forecast made at the end of period $T$ for $T + \tau$ would be obtained by reversing the transformation. For the natural log this would be

$$
\hat {y} _ {T + \tau} (T) = \exp [ \hat {x} _ {T + \tau} (T) ].
$$

7. If prediction intervals are desired for the forecast (and we recommend doing this), construct prediction intervals for the residuals and then reverse the transformations made to produce the residuals as described earlier. We will discuss methods for -nding prediction intervals for most of the forecasting methods presented in this book.

8. Develop and implement a procedure for monitoring the forecast to ensure that deterioration in performance will be detected reasonably quickly. Forecast monitoring is usually done by evaluating the stream of forecast errors that are experienced. We will present methods for monitoring forecast errors with the objective of detecting changes in performance of the forecasting model.

# 2.6 EVALUATING AND MONITORING FORECASTING MODEL PERFORMANCE

# 2.6.1 Forecasting Model Evaluation

We now consider how to evaluate the performance of a forecasting technique for a particular time series or application. It is important to carefully de-ne the meaning of performance. It is tempting to evaluate performance on the basis of the -t of the forecasting or time series model to historical data. There are many statistical measures that describe how well a model -ts a given sample of data, and several of these will be described in

subsequent chapters. This goodness-of--t approach often uses the residuals and does not really reect the capability of the forecasting technique to successfully predict future observations. The user of the forecasts is very concerned about the accuracy of future forecasts, not model goodness of -t, so it is important to evaluate this aspect of any recommended technique. Sometimes forecast accuracy is called 鈥渙ut-of-sample鈥?forecast error, to distinguish it from the residuals that arise from a model--tting process.

Measure of forecast accuracy should always be evaluated as part of a model validation effort (see step 4 in the general approach to forecasting in the previous section). When more than one forecasting technique seems reasonable for a particular application, these forecast accuracy measures can also be used to discriminate between competing models. We will discuss this more in Section 2.6.2.

It is customary to evaluate forecasting model performance using the one-step-ahead forecast errors

$$
e _ {t} (1) = y _ {t} - \hat {y} _ {t} (t - 1), \tag {2.31}
$$

where $\hat { y } _ { t } ( t - 1 )$ is the forecast of $y _ { t }$ that was made one period prior. Forecast errors at other lags, or at several different lags, could be used if interest focused on those particular forecasts. Suppose that there are $n$ observations for which forecasts have been made and $n$ one-step-ahead forecast errors, $e _ { t } ( 1 ) , t = 1 , 2 , \ldots , n$ . Standard measures of forecast accuracy are the average error or mean error

$$
\mathrm {M E} = \frac {1}{n} \sum_ {t = 1} ^ {n} e _ {t} (1), \tag {2.32}
$$

the mean absolute deviation (or mean absolute error)

$$
\mathrm {M A D} = \frac {1}{n} \sum_ {t = 1} ^ {n} \left| e _ {t} (1) \right|, \tag {2.33}
$$

and the mean squared error

$$
\mathrm {M S E} = \frac {1}{n} \sum_ {t = 1} ^ {n} \left[ e _ {t} (1) \right] ^ {2}. \tag {2.34}
$$

The mean forecast error in Eq. (2.32) is an estimate of the expected value of forecast error, which we would hope to be zero; that is, the forecasting

technique produces unbiased forecasts. If the mean forecast error differs appreciably from zero, bias in the forecast is indicated. If the mean forecast error drifts away from zero when the forecasting technique is in use, this can be an indication that the underlying time series has changed in some fashion, the forecasting technique has not tracked this change, and now biased forecasts are being generated.

Both the mean absolute deviation (MAD) in Eq. (2.33) and the mean squared error (MSE) in Eq. (2.34) measure the variability in forecast errors. Obviously, we want the variability in forecast errors to be small. The MSE is a direct estimator of the variance of the one-step-ahead forecast errors:

$$
\hat {\sigma} _ {e (1)} ^ {2} = \mathrm {M S E} = \frac {1}{n} \sum_ {t = 1} ^ {n} \left[ e _ {t} (1) \right] ^ {2}. \tag {2.35}
$$

If the forecast errors are normally distributed (this is usually not a bad assumption, and one that is easily checked), the MAD is related to the standard deviation of forecast errors by

$$
\hat {\sigma} _ {e (1)} = \sqrt {\frac {\pi}{2}} \mathrm {M A D} \cong 1. 2 5 \mathrm {M A D} \tag {2.36}
$$

The one-step-ahead forecast error and its summary measures, the ME, MAD, and MSE, are all scale-dependent measures of forecast accuracy; that is, their values are expressed in terms of the original units of measurement (or in the case of MSE, the square of the original units). So, for example, if we were forecasting demand for electricity in Phoenix during the summer, the units would be megawatts (MW). If the MAD for the forecast error during summer months was 5 MW, we might not know whether this was a large forecast error or a relatively small one. Furthermore, accuracy measures that are scale dependent do not facilitate comparisons of a single forecasting technique across different time series, or comparisons across different time periods. To accomplish this, we need a measure of relative forecast error.

De-ne the relative forecast error (in percent) as

$$
r e _ {t} (1) = \left(\frac {y _ {t} - \hat {y} _ {t} (t - 1)}{y _ {t}}\right) 1 0 0 = \left(\frac {e _ {t} (1)}{y _ {t}}\right) 1 0 0. \tag {2.37}
$$

This is customarily called the percent forecast error. The mean percent forecast error (MPE) is

$$
\mathrm {M P E} = \frac {1}{n} \sum_ {t = 1} ^ {n} r e _ {t} (1) \tag {2.38}
$$

and the mean absolute percent forecast error (MAPE) is

$$
\mathrm {M A P E} = \frac {1}{n} \sum_ {t = 1} ^ {n} | r e _ {t} (1) |. \tag {2.39}
$$

Knowing that the relative or percent forecast error or the MAPE is $3 \%$ (say) can be much more meaningful than knowing that the MAD is 5 MW. Note that the relative or percent forecast error only makes sense if the time series $y _ { t }$ does not contain zero values.

Example 2.10 Table 2.2 illustrates the calculation of the one-step-ahead forecast error, the absolute errors, the squared errors, the relative (percent) error, and the absolute percent error from a forecasting model for 20 time periods. The last row of columns (3) through (7) display the sums required to calculate the ME, MAD, MSE, MPE, and MAPE.

From Eq. (2.32), the mean (or average) forecast error is

$$
\mathrm {M E} = \frac {1}{n} \sum_ {t = 1} ^ {n} e _ {t} (1) = \frac {1}{2 0} (- 1 1. 6) = - 0. 5 8,
$$

the MAD is computed from Eq. (2.33) as

$$
\mathrm {M A D} = \frac {1}{n} \sum_ {t = 1} ^ {n} | e _ {t} (1) | = \frac {1}{2 0} (8 6. 6) = 4. 3 3,
$$

and the MSE is computed from Eq. (2.34) as

$$
\mathrm {M S E} = \frac {1}{n} \sum_ {t = 1} ^ {n} [ e _ {t} (1) ] ^ {2} = \frac {1}{2 0} (4 7 1. 8) = 2 3. 5 9.
$$

TABLE 2.2 Calculation of Forecast Accuracy Measures   

<table><tr><td>Time Period</td><td>(1) 
Observed Value 
yt</td><td>(2) 
Forecast 
hat y_t(t-1)</td><td>(3) 
Forecast Error 
et(1)</td><td>(4) 
Absolute Error 
|et(1)|</td><td>(5) 
Squared Error 
[e_t(1)]^2</td><td>(6) 
Relative (%) 
Error 
(e_t(1)/y_t) 100</td><td>(6) 
Absolute (%) 
Error 
| (e_t(1)/y_t) 100|</td></tr><tr><td>1</td><td>47</td><td>51.1</td><td>-4.1</td><td>4.1</td><td>16.81</td><td>-8.7234</td><td>8.723404</td></tr><tr><td>2</td><td>46</td><td>52.9</td><td>-6.9</td><td>6.9</td><td>47.61</td><td>-15</td><td>15</td></tr><tr><td>3</td><td>51</td><td>48.8</td><td>2.2</td><td>2.2</td><td>4.84</td><td>4.313725</td><td>4.313725</td></tr><tr><td>4</td><td>44</td><td>48.1</td><td>-4.1</td><td>4.1</td><td>16.81</td><td>-9.31818</td><td>9.318182</td></tr><tr><td>5</td><td>54</td><td>49.7</td><td>4.3</td><td>4.3</td><td>18.49</td><td>7.962963</td><td>7.962963</td></tr><tr><td>6</td><td>47</td><td>47.5</td><td>-0.5</td><td>0.5</td><td>0.25</td><td>-1.06383</td><td>1.06383</td></tr><tr><td>7</td><td>52</td><td>51.2</td><td>0.8</td><td>0.8</td><td>0.64</td><td>1.538462</td><td>1.538462</td></tr><tr><td>8</td><td>45</td><td>53.1</td><td>-8.1</td><td>8.1</td><td>65.61</td><td>-18</td><td>18</td></tr><tr><td>9</td><td>50</td><td>54.4</td><td>-4.4</td><td>4.4</td><td>19.36</td><td>-8.8</td><td>8.8</td></tr><tr><td>10</td><td>51</td><td>51.2</td><td>-0.2</td><td>0.2</td><td>0.04</td><td>-0.39216</td><td>0.392157</td></tr><tr><td>11</td><td>49</td><td>53.3</td><td>-4.3</td><td>4.3</td><td>18.49</td><td>-8.77551</td><td>8.77551</td></tr><tr><td>12</td><td>41</td><td>46.5</td><td>-5.5</td><td>5.5</td><td>30.25</td><td>-13.4146</td><td>13.41463</td></tr><tr><td>13</td><td>48</td><td>53.1</td><td>-5.1</td><td>5.1</td><td>26.01</td><td>-10.625</td><td>10.625</td></tr><tr><td>14</td><td>50</td><td>52.1</td><td>-2.1</td><td>2.1</td><td>4.41</td><td>-4.2</td><td>4.2</td></tr><tr><td>15</td><td>51</td><td>46.8</td><td>4.2</td><td>4.2</td><td>17.64</td><td>8.235294</td><td>8.235294</td></tr><tr><td>16</td><td>55</td><td>47.7</td><td>7.3</td><td>7.3</td><td>53.29</td><td>13.27273</td><td>13.27273</td></tr><tr><td>17</td><td>52</td><td>45.4</td><td>6.6</td><td>6.6</td><td>43.56</td><td>12.69231</td><td>12.69231</td></tr><tr><td>18</td><td>53</td><td>47.1</td><td>5.9</td><td>5.9</td><td>34.81</td><td>11.13208</td><td>11.13208</td></tr><tr><td>19</td><td>48</td><td>51.8</td><td>-3.8</td><td>3.8</td><td>14.44</td><td>-7.91667</td><td>7.916667</td></tr><tr><td>20</td><td>52</td><td>45.8</td><td>6.2</td><td>6.2</td><td>38.44</td><td>11.92308</td><td>11.92308</td></tr><tr><td></td><td colspan="2">Totals</td><td>-11.6</td><td>86.6</td><td>471.8</td><td>-35.1588</td><td>177.3</td></tr></table>

Because the MSE estimates the variance of the one-step-ahead forecast errors, we have

$$
\hat {\sigma} _ {e (1)} ^ {2} = \mathrm {M S E} = 2 3. 5 9
$$

and an estimate of the standard deviation of forecast errors is the square root of this quantity, or ${ \hat { \sigma } } _ { e ( 1 ) } = { \sqrt { \mathrm { M S E } } } = 4 . 8 6$ . We can also obtain an estimate of the standard deviation of forecasts errors from the MAD using Eq. (2.36)

$$
\hat {\sigma} _ {e (1)} \cong 1. 2 5 \mathrm {M A D} = 1. 2 5 (4. 3 3) = 5. 4 1.
$$

These two estimates are reasonably similar. The mean percent forecast error, MPE, is computed from Eq. (2.38) as

$$
\mathrm{MPE} = \frac{1}{n}\sum_{t = 1}^{n}re_{t}(1) = \frac{1}{20} (-35.1588) = -1.76\%
$$

and the mean absolute percent error is computed from Eq. (2.39) as

$$
\mathrm{MAPE} = \frac{1}{n}\sum_{t = 1}^{n}|re_{t}(1)| = \frac{1}{20} (177.3) = 8.87\%.
$$

There is much empirical evidence (and even some theoretical justi-- cation) that the distribution of forecast errors can be well approximated by a normal distribution. This can easily be checked by constructing a normal probability plot of the forecast errors in Table 2.2, as shown in Figure 2.35. The forecast errors deviate somewhat from the straight line, indicating that the normal distribution is not a perfect model for the distribution of forecast errors, but it is not unreasonable. Minitab calculates the Anderson鈥揇arling statistic, a widely used test statistic for normality. The $P$ -value is 0.088, so the hypothesis of normality of the forecast errors would not be rejected at the 0.05 level. This test assumes that the observations (in this case the forecast errors) are uncorrelated. Minitab also reports the standard deviation of the forecast errors to be 4.947, a slightly larger value than we computed from the MSE, because Minitab uses the standard method for calculating sample standard deviations.

Note that Eq. (2.31) could have been written as

![](images/7e107e97292b769a78c899997f74118e344aa2655c02e9803e243b40bf437153.jpg)  
FIGURE 2.35 Normal probability plot of forecast errors from Table 2.2.

Hopefully, the forecasts do a good job of describing the structure in the observations. In an ideal situation, the forecasts would adequately model all of the structure in the data, and the sequence of forecast errors would be structureless. If they are, the sample ACF of the forecast error should look like the ACF of random data; that is, there should not be any large 鈥渟pikes鈥?on the sample ACF at low lag. Any systematic or nonrandom pattern in the forecast errors will tend to show up as signi-cant spikes on the sample ACF. If the sample ACF suggests that the forecast errors are not random, then this is evidence that the forecasts can be improved by re-ning the forecasting model. Essentially, this would consist of taking the structure out of the forecast errors and putting it into the forecasts, resulting in forecasts that are better prediction of the data.

Example 2.11 Table 2.3 presents a set of 50 one-step-ahead errors from a forecasting model, and Table 2.4 shows the sample ACF of these forecast errors. The sample ACF is plotted in Figure 2.36. This sample ACF was obtained from Minitab. Note that sample autocorrelations for the -rst 13 lags are computed. This is consistent with our guideline indicating that for $T$ observations only the -rst T/4 autocorrelations should be computed. The sample ACF does not provide any strong evidence to support a claim that there is a pattern in the forecast errors.

TABLE 2.3 One-Step-Ahead Forecast Errors   

<table><tr><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td></tr><tr><td>1</td><td>-0.62</td><td>11</td><td>-0.49</td><td>21</td><td>2.90</td><td>31</td><td>-1.88</td><td>41</td><td>-3.98</td></tr><tr><td>2</td><td>-2.99</td><td>12</td><td>4.13</td><td>22</td><td>0.86</td><td>32</td><td>-4.46</td><td>42</td><td>-4.28</td></tr><tr><td>3</td><td>0.65</td><td>13</td><td>-3.39</td><td>23</td><td>5.80</td><td>33</td><td>-1.93</td><td>43</td><td>1.06</td></tr><tr><td>4</td><td>0.81</td><td>14</td><td>2.81</td><td>24</td><td>4.66</td><td>34</td><td>-2.86</td><td>44</td><td>0.18</td></tr><tr><td>5</td><td>-2.25</td><td>15</td><td>-1.59</td><td>25</td><td>3.99</td><td>35</td><td>0.23</td><td>45</td><td>3.56</td></tr><tr><td>6</td><td>-2.63</td><td>16</td><td>-2.69</td><td>26</td><td>-1.76</td><td>36</td><td>-1.82</td><td>46</td><td>-0.24</td></tr><tr><td>7</td><td>3.57</td><td>17</td><td>3.41</td><td>27</td><td>2.31</td><td>37</td><td>0.64</td><td>47</td><td>-2.98</td></tr><tr><td>8</td><td>0.11</td><td>18</td><td>4.35</td><td>28</td><td>-2.24</td><td>38</td><td>-1.55</td><td>48</td><td>2.47</td></tr><tr><td>9</td><td>0.59</td><td>19</td><td>-4.37</td><td>29</td><td>2.95</td><td>39</td><td>0.78</td><td>49</td><td>0.66</td></tr><tr><td>10</td><td>-0.63</td><td>20</td><td>2.79</td><td>30</td><td>6.30</td><td>40</td><td>2.84</td><td>50</td><td>0.32</td></tr></table>

TABLE 2.4 Sample ACF of the One-Step-Ahead Forecast Errors in Table 2.3   

<table><tr><td>Lag</td><td>Sample ACF, rk</td><td>Z-Statistic</td><td>Ljung-Box Statistic, QLB</td></tr><tr><td>1</td><td>0.004656</td><td>0.03292</td><td>0.0012</td></tr><tr><td>2</td><td>-0.102647</td><td>-0.72581</td><td>0.5719</td></tr><tr><td>3</td><td>0.136810</td><td>0.95734</td><td>1.6073</td></tr><tr><td>4</td><td>-0.033988</td><td>-0.23359</td><td>1.6726</td></tr><tr><td>5</td><td>0.118876</td><td>0.81611</td><td>2.4891</td></tr><tr><td>6</td><td>0.181508</td><td>1.22982</td><td>4.4358</td></tr><tr><td>7</td><td>-0.039223</td><td>-0.25807</td><td>4.5288</td></tr><tr><td>8</td><td>-0.118989</td><td>-0.78185</td><td>5.4053</td></tr><tr><td>9</td><td>0.003400</td><td>0.02207</td><td>5.4061</td></tr><tr><td>10</td><td>0.034631</td><td>0.22482</td><td>5.4840</td></tr><tr><td>11</td><td>-0.151935</td><td>-0.98533</td><td>7.0230</td></tr><tr><td>12</td><td>-0.207710</td><td>-1.32163</td><td>9.9749</td></tr><tr><td>13</td><td>0.089387</td><td>0.54987</td><td>10.5363</td></tr></table>

If a time series consists of uncorrelated observations and has constant variance, we say that it is white noise. If, in addition, the observations in this time series are normally distributed, the time series is Gaussian white noise. Ideally, forecast errors are Gaussian white noise. The normal probability plot of the one-step-ahead forecast errors from Table 2.3 are shown in Figure 2.37. This plot does not indicate any serious problem, with the normality assumption, so the forecast errors in Table 2.3 are Gaussian white noise.

![](images/ad2ca035c1b85acfeead792d02f1840319c1c1f7a2973ca4c98a3b9809da4fa1.jpg)  
FIGURE 2.36 Sample ACF of forecast errors from Table 2.4.

If a time series is white noise, the distribution of the sample autocorrelation coef-cient at lag $k$ in large samples is approximately normal with mean zero and variance $1 / T$ ; that is,

$$
r _ {k} \sim N \left(0, \frac {1}{T}\right).
$$

![](images/771683965a41cdfd2c3af3bd9b49bed522cb9d5747deaedaf82d2c54ac73dde4.jpg)  
FIGURE 2.37 Normal probability plot of forecast errors from Table 2.3.

Therefore we could test the hypothesis $H _ { 0 } : \rho _ { k } = 0$ using the test statistic

$$
Z _ {0} = \frac {r _ {k}}{\sqrt {\frac {1}{T}}} = r _ {k} \sqrt {T}. \tag {2.40}
$$

Minitab calculates this Z-statistic (calling it a $t$ -statistic), and it is reported in Table 2.4 for the one-step-ahead forecast errors of Table 2.3 (this is the $t \cdot$ -statistic reported in Figure 2.13 for the ACF of the chemical viscosity readings). Large values of this statistic (say, $| Z _ { 0 } | > Z _ { \alpha / 2 }$ , where $Z _ { \alpha / 2 }$ is the upper $\alpha / 2$ percentage point of the standard normal distribution) would indicate that the corresponding autocorrelation coef-cient does not equal zero. Alternatively, we could calculate a $P$ -value for this test statistic. Since none of the absolute values of the $Z$ -statistics in Table 2.4 exceeds $Z _ { \alpha / 2 } = Z _ { 0 . 0 2 5 } = 1 . 9 6$ , we cannot conclude at signi-cance level $\alpha = 0 . 0 5$ that any individual autocorrelation coef-cient differs from zero.

This procedure is a one-at-a-time test; that is, the signi-cance level applies to the autocorrelations considered individually. We are often interested in evaluating a set of autocorrelations jointly to determine if they indicate that the time series is white noise. Box and Pierce (1970) have suggested such a procedure. Consider the square of the test statistic $Z _ { 0 }$ in Eq. (2.40). The distribution of $Z _ { 0 } ^ { 2 } = r _ { k } ^ { 2 } T$ is approximately chi-square with one degree of freedom. The Box鈥揚ierce statistic

$$
Q _ {\mathrm {B P}} = T \sum_ {k = 1} ^ {K} r _ {k} ^ {2} \tag {2.41}
$$

is distributed approximately as chi-square with Kdegrees of freedom under the null hypothesis that the time series is white noise. Therefore, if $Q _ { \mathrm { B P } } >$ $\chi _ { \alpha , K } ^ { 2 }$ we would reject the null hypothesis and conclude that the time series is not white noise because some of the autocorrelations are not zero. A $P -$ -value approach could also be used. When this test statistic is applied to a set of residual autocorrelations the statistic ${ \mathcal { Q } } _ { \mathrm { B P } } \sim { \chi } _ { \alpha , K - p } ^ { 2 }$ , where $p$ is the number of parameters in the model, so the number of degrees of freedom in the chi-square distribution becomes $K - p$ . Box and Pierce call this procedure a 鈥淧ortmanteau鈥?or general goodness-of--t statistic (it is testing the goodness of -t of the ACF to the ACF of white noise). A modi-cation of this test that works better for small samples was devised by Ljung and Box (1978). The Ljung鈥揃ox goodness-of--t statistic is

$$
Q _ {\mathrm {L B}} = T (T + 2) \sum_ {k = 1} ^ {K} \left(\frac {1}{T - k}\right) r _ {k} ^ {2}. \tag {2.42}
$$

Note that the Ljung鈥揃ox goodness-of--t statistic is very similar to the original Box鈥揚ierce statistic, the difference being that the squared sample autocorrelation at lag $k$ is weighted by $( T + 2 ) / ( T - k )$ . For large values of $T$ , these weights will be approximately unity, and so the $Q _ { \mathrm { L B } }$ and $Q _ { \mathrm { B P } }$ statistics will be very similar.

Minitab calculates the Ljung鈥揃ox goodness-of--t statistic $Q _ { \mathrm { L B } }$ , and the values for the -rst 13 sample autocorrelations of the one-step-ahead forecast errors oflag 13, the value $Q _ { \mathrm { L B } } = 1 0 . 5 3 6 3$ own in the, and since $\chi _ { 0 . 0 5 , 1 3 } ^ { 2 } = 2 2 . 3 6$ Table 2.4. At, there is no strong evidence to indicate that the -rst 13 autocorrelations of the forecast errors considered jointly differ from zero. If we calculate the $P$ -value for this test statistic, we -nd that $P = 0 . 6 5$ . This is a good indication that the forecast errors are white noise. Note that Figure 2.13 also gave values for the Ljung鈥揃ox statistic.

# 2.6.2 Choosing Between Competing Models

There are often several competing models that can be used for forecasting a particular time series. For example, there are several ways to model and forecast trends. Consequently, selecting an appropriate forecasting model is of considerable practical importance. In this section we discuss some general principles of model selection. In subsequent chapters, we will illustrate how these principles are applied in speci-c situations.

Selecting the model that provides the best -t to historical data generally does not result in a forecasting method that produces the best forecasts of new data. Concentrating too much on the model that produces the best historical -t often results in over-tting, or including too many parameters or terms in the model just because these additional terms improve the model -t. In general, the best approach is to select the model that results in the smallest standard deviation (or mean squared error) of the one-step-ahead forecast errors when the model is applied to data that were not used in the -tting process. Some authors refer to this as an outof-sample forecast error standard deviation (or mean squared error). A standard way to measure this out-of-sample performance is by utilizing some form of data splitting; that is, divide the time series data into two segments鈥攐ne for model -tting and the other for performance testing. Sometimes data splitting is called cross-validation. It is somewhat arbitrary as to how the data splitting is accomplished. However, a good rule of thumb is to have at least 20 or 25 observations in the performance testing data set.

When evaluating the -t of the model to historical data, there are several criteria that may be of value. The mean squared error of the residuals is

$$
s ^ {2} = \frac {\sum_ {t = 1} ^ {T} e _ {t} ^ {2}}{T - p} \tag {2.43}
$$

where $T$ periods of data have been used to -t a model with $p$ parameters and $e _ { t }$ is the residual from the model--tting process in period $t$ . The mean squared error $s ^ { 2 }$ is just the sample variance of the residuals and it is an estimator of the variance of the model errors.

Another criterion is the $R$ -squared statistic

$$
R ^ {2} = 1 - \frac {\sum_ {t = 1} ^ {T} e _ {t} ^ {2}}{\sum_ {t = 1} ^ {T} (y _ {t} - \bar {y}) ^ {2}}. \tag {2.44}
$$

The denominator of Eq. (2.44) is just the total sum of squares of the observations, which is constant (not model dependent), and the numerator is just the residual sum of squares. Therefore, selecting the model that maximizes $R ^ { 2 }$ is equivalent to selecting the model that minimizes the sum of the squared residuals. Large values of $R ^ { 2 }$ suggest a good -t to the historical data. Because the residual sum of squares always decreases when parameters are added to a model, relying on $R ^ { 2 }$ to select a forecasting model encourages over-tting or putting in more parameters than are really necessary to obtain good forecasts. A large value of $R ^ { 2 }$ does not ensure that the out-of-sample one-step-ahead forecast errors will be small.

A better criterion is the 鈥渁djusted鈥?$R ^ { 2 }$ statistic, de-ned as

$$
R _ {\text {A d j}} ^ {2} = 1 - \frac {\sum_ {t = 1} ^ {T} e _ {t} ^ {2} / (T - p)}{\sum_ {t = 1} ^ {T} (y _ {t} - \bar {y}) ^ {2} / (T - 1)} = 1 - \frac {s ^ {2}}{\sum_ {t = 1} ^ {T} (y _ {t} - \bar {y}) ^ {2} / (T - 1)}. \tag {2.45}
$$

The adjustment is a 鈥渟ize鈥?adjustment鈥攖hat is, adjust for the number of parameters in the model. Note that a model that maximizes the adjusted $R ^ { 2 }$ statistic is also the model that minimizes the residual mean square.

Two other important criteria are the Akaike Information Criterion (AIC) (see Akaike (1974)) and the Schwarz Bayesian Information Criterion (abbreviated as BIC or SIC by various authors) (see Schwarz (1978)):

$$
\mathrm {A I C} = \ln \left(\frac {\sum_ {t = 1} ^ {T} e _ {t} ^ {2}}{T}\right) + \frac {2 p}{T} \tag {2.46}
$$

and

$$
\mathrm {B I C} = \ln \left(\frac {\sum_ {t = 1} ^ {T} e _ {t} ^ {2}}{T}\right) + \frac {p \ln (T)}{T}. \tag {2.47}
$$

These two criteria penalize the sum of squared residuals for including additional parameters in the model. Models that have small values of the AIC or BIC are considered good models.

One way to evaluate model selection criteria is in terms of consistency. A model selection criterion is consistent if it selects the true model when the true model is among those considered with probability approaching unity as the sample size becomes large, and if the true model is not among those considered, it selects the best approximation with probability approaching unity as the sample size becomes large. It turns out that $s ^ { 2 }$ , the adjusted $R ^ { 2 }$ , and the AIC are all inconsistent, because they do not penalize for adding parameters heavily enough. Relying on these criteria tends to result in over-tting. The BIC, which caries a heavier 鈥渟ize adjustment鈥?penalty, is consistent.

Consistency, however, does not tell the complete story. It may turn out that the true model and any reasonable approximation to it are very complex. An asymptotically ef-cient model selection criterion chooses a sequence of models as T(the amount of data available) gets large for which the one-step-ahead forecast error variances approach the one-stepahead forecast error variance for the true model at least as fast as any other criterion. The AIC is asymptotically ef-cient but the BIC is not.

There are a number of variations and extensions of these criteria. The AIC is a biased estimator of the discrepancy between all candidate

models and the true model. This has led to developing a 鈥渃orrected鈥?version of AIC:

$$
\mathrm {A I C c} = \ln \left(\frac {\sum_ {t = 1} ^ {T} e _ {t} ^ {2}}{T}\right) + \frac {2 T (p + 1)}{T - p - 2}. \tag {2.48}
$$

Sometimes we see the -rst term in the AIC, AICc, or BIC written as $- 2 \ln L ( \beta , \sigma ^ { 2 } )$ , where $L ( \beta , \sigma ^ { 2 } )$ is the likelihood function for the -tted model evaluated at the maximum likelihood estimates of the unknown parameters $\beta$ and $\sigma ^ { 2 }$ . In this context, AIC, AICc, and SIC are called penalized likelihood criteria.

Many software packages evaluate and print model selection criteria, such as those discussed here. When both AIC and SIC are available, we prefer using SIC. It generally results in smaller, and hence simpler, models, and so its use is consistent with the time-honored model-building principle of parsimony (all other things being equal, simple models are preferred to complex ones). We will discuss and illustrate model selection criteria again in subsequent chapters. However, remember that the best way to evaluate a candidate model鈥檚 potential predictive performance is to use data splitting. This will provide a direct estimate of the one-step-ahead forecast error variance, and this method should always be used, if possible, along with the other criteria that we have discussed here.

# 2.6.3 Monitoring a Forecasting Model

Developing and implementing procedures to monitor the performance of the forecasting model is an essential component of good forecasting system design. No matter how much effort has been expended in developing the forecasting model, and regardless of how well the model works initially, over time it is likely that its performance will deteriorate. The underlying pattern of the time series may change, either because the internal inertial forces that drive the process may evolve through time, or because of external events such as new customers entering the market. For example, a level change or a slope change could occur in the variable that is being forecasted. It is also possible for the inherent variability in the data to increase. Consequently, performance monitoring is important.

The one-step-ahead forecast errors $e _ { t } ( 1 )$ are typically used for forecast monitoring. The reason for this is that changes in the underlying time series

will also typically be reected in the forecast errors. For example, if a level change occurs in the time series, the sequence of forecast errors will no longer uctuate around zero; that is, a positive or negative bias will be introduced.

There are several ways to monitor forecasting model performance. The simplest way is to apply Shewhart control charts to the forecast errors. A Shewhart control chart is a plot of the forecast errors versus time containing a center line that represents the average (or the target value) of the forecast errors and a set of control limits that are designed to provide an indication that the forecasting model performance has changed. The center line is usually taken as either zero (which is the anticipated forecast error for an unbiased forecast) or the average forecast error (ME from Eq. (2.32)), and the control limits are typically placed at three standard deviations of the forecast errors above and below the center line. If the forecast errors plot within the control limits, we assume that the forecasting model performance is satisfactory (or in control), but if one or more forecast errors exceed the control limits, that is a signal that something has happened and the forecast errors are no longer uctuating around zero. In control chart terminology, we would say that the forecasting process is out of control and some analysis is required to determine what has happened.

The most familiar Shewhart control charts are those applied to data that have been collected in subgroups or samples. The one-step-ahead forecast errors $e _ { t } ( 1 )$ are individual observations. Therefore the Shewhart control chart for individuals would be used for forecast monitoring. On this control chart it is fairly standard practice to estimate the standard deviation of the individual observations using a moving range method. The moving range is de-ned as the absolute value of the difference between any two successive one-step-ahead forecast errors, say, $| e _ { t } ( 1 ) - e _ { t - 1 } ( 1 ) |$ , and the moving range based on $n$ observations is

$$
M R = \sum_ {t = 2} ^ {n} \left| e _ {t} (1) - e _ {t - 1} (1) \right|. \tag {2.49}
$$

The estimate of the standard deviation of the one-step-ahead forecast errors is based on the average of the moving ranges

$$
\hat {\sigma} _ {e (1)} = \frac {0 . 8 8 6 5 M R}{n - 1} = \frac {0 . 8 8 6 5 \sum_ {t = 2} ^ {n} \left| e _ {t} (1) - e _ {t - 1} (1) \right|}{n - 1} = 0. 8 8 6 5 \overline {{M R}}, \tag {2.50}
$$

where $\overline { { M R } }$ is the average of the moving ranges. This estimate of the standard deviation would be used to construct the control limits on the control chart

for forecast errors. For more details on constructing and interpreting control charts, see Montgomery (2013).

Example 2.12 Minitab can be used to construct Shewhart control charts for individuals. Figure 2.38 shows the Minitab control charts for the onestep-ahead forecast errors in Table 2.3. Note that both an individuals control chart of the one-step-ahead forecast errors and a control chart of the moving ranges of these forecast errors are provided. On the individuals control chart the center line is taken to be the average of the forecast errors ME de-ned in Eq. (2.30) (denoted $\overline { { X } }$ in Figure 2.38) and the upper and lower three-sigma control limits are abbreviated as UCL and LCL, respectively. The center line on the moving average control chart is at the average of the moving ranges $\overline { { M R } } = M R / ( n - 1 )$ , the three-sigma upper control limit UCL is at $3 . 2 6 7 M R / ( n - 1 )$ , and the lower control limit is at zero (for details on how the control limits are derived, see Montgomery (2013)). All of the one-step-ahead forecast errors plot within the control limits (and the moving range also plot within their control limits). Thus there is no reason to suspect that the forecasting model is performing inadequately, at least from the statistical stability viewpoint. Forecast errors that plot outside the control limits would indicate model inadequacy, or possibly the presence of unusual observations such as outliers in the data. An investigation would be required to determine why these forecast errors exceed the control limits.

![](images/3c4b5b05d42f155364f7a80d39b72494ec755d46d4b8381e86f6062af10ef793.jpg)

![](images/777bb297a6b368f51c50861946bbcc9a418245629377e3ae31933ed3c1c37bb3.jpg)  
FIGURE 2.38 Individuals and moving range control charts of the one-stepahead forecast errors in Table 2.3.

Because the control charts in Figure 2.38 exhibit statistical control, we would conclude that there is no strong evidence of statistical inadequacy in the forecasting model. Therefore, these control limits would be retained and used to judge the performance of future forecasts (in other words, we do not recalculate the control limits with each new forecast). However, the stable control chart does not imply that the forecasting performance is satisfactory in the sense that the model results in small forecast errors. In the quality control literature, these two aspects of process performance are referred to as control and capability, respectively. It is possible for the forecasting process to be stable or in statistical control but not capable鈥?that is, produce forecast errors that are unacceptably large.

Two other types of control charts, the cumulative sum (or CUSUM) control chart and the exponentially weighted moving average (or EWMA) control chart, can also be useful for monitoring the performance of a forecasting model. These charts are more effective at detecting smaller changes or disturbances in the forecasting model performance than the individuals control chart. The CUSUM is very effective in detecting level changes in the monitored variable. It works by accumulating deviations of the forecast errors that are above the desired target value $T _ { 0 }$ (usually either zero or the average forecast error) with one statistic $C ^ { + }$ and deviations that are below the target with another statistic $C ^ { - }$ . The statistics $C ^ { + }$ and $C ^ { - }$ are called the upper and lower CUSUMs, respectively. They are computed as follows:

$$
C _ {t} ^ {+} = \max  \left[ 0, e _ {t} (1) - \left(T _ {0} + K\right) + C _ {t - 1} ^ {+} \right], \tag {2.51}
$$

$$
C _ {t} ^ {-} = \min  \left[ 0, e _ {t} (1) - \left(T _ {0} - K\right) + C _ {t - 1} ^ {-} \right] ^ {\prime}
$$

where the constant $K$ , usually called the reference value, is usually chosen as $K = 0 . 5 \sigma _ { e ( 1 ) }$ and $\sigma _ { e ( 1 ) }$ is the standard deviation of the one-step-ahead forecast errors. The logic is that if the forecast errors begin to systematically fall on one side of the target value (or zero), one of the CUSUMs in Eq. (2.51) will increase in magnitude. When this increase becomes large enough, an out-of-control signal is generated. The decision rule is to signal if the statistic $C ^ { + }$ exceeds a decision interval $H = 5 \sigma _ { e ( 1 ) }$ or if $C ^ { - }$ exceeds $- H$ . The signal indicates that the forecasting model is not performing satisfactorily (Montgomery (2013) discusses the choice of $H$ and $K$ in detail).

Example 2.13 The CUSUM control chart for the forecast errors shown in Table 2.3 is shown in Figure 2.39. This CUSUM chart was constructed

![](images/02f88c3cc740bd4088e734e23d8d0ec6a2f69f3a8c6a38d5cdc0bb1797242a91.jpg)  
FIGURE 2.39 CUSUM control chart of the one-step-ahead forecast errors in Table 2.3.

using Minitab with a target value of $T = 0$ and $\sigma _ { e ( 1 ) }$ was estimated using the moving range method described previously, resulting in $H = 5 \hat { \sigma } _ { e ( 1 ) } =$ $5 ( 0 . 8 8 6 5 ) M R / ( T - 1 ) = 5 ( 0 . 8 8 6 5 ) 3 . 2 4 = 1 4 . 3 6 .$ Minitab labels $H$ and $- H$ as UCL and LCL, respectively. The CUSUM control chart reveals no obvious forecasting model inadequacies.

A control chart based on the EWMA is also useful for monitoring forecast errors. The EWMA applied to the one-step-ahead forecast errors is

$$
\bar {e} _ {t} (1) = \lambda e _ {t} (1) + (1 - \lambda) \bar {e} _ {t - 1} (1), \tag {2.52}
$$

where $0 < \lambda < 1$ is a constant (usually called the smoothing constant) and the starting value of the EWMA (required at the -rst observation) is either $\bar { e } _ { 0 } ( 1 ) = 0$ or the average of the forecast errors. Typical values of the smoothing constant for an EWMA control chart are $0 . 0 5 < \lambda < 0 . 2$ .

The EWMA is a weighted average of all current and previous forecast errors, and the weights decrease geometrically with the 鈥渁ge鈥?of the forecast error. To see this, simply substitute recursively for $\bar { e } _ { t - 1 } ( 1 )$ , then $\bar { e } _ { t - 2 } ( 1 )$ , then $\bar { \boldsymbol { e } } _ { t - j } ( 1 ) _ { j }$ for $j = 3 , 4 , \dots$ , until we obtain

$$
\bar {e} _ {n} (1) = \lambda \sum_ {j = 0} ^ {n - 1} (1 - \lambda) ^ {j} e _ {T - j} (1) + (1 - \lambda) ^ {n} \bar {e} _ {0} (1)
$$

and note that the weights sum to unity because

$$
\lambda \sum_ {j = 0} ^ {n - 1} (1 - \lambda) ^ {j} = 1 - (1 - \lambda) ^ {n}.
$$

The standard deviation of the EWMA is

$$
\sigma_ {\tilde {e} _ {t} (1)} = \sigma_ {e (1)} \sqrt {\frac {\lambda}{2 - \lambda} [ 1 - (1 - \lambda) ^ {2 t} ]}.
$$

So an EWMA control chart for the one-step-ahead forecast errors with a center line of $T$ (the target for the forecast errors) is de-ned as follows:

$$
\mathrm {U C L} = T + 3 \sigma_ {e (1)} \sqrt {\frac {\lambda}{2 - \lambda} [ 1 - (1 - \lambda) ^ {2 t} ]}
$$

Center line = T (2.53)

$$
\mathrm {L C L} = T - 3 \sigma_ {e (1)} \sqrt {\frac {\lambda}{2 - \lambda} [ 1 - (1 - \lambda) ^ {2 t} ]}
$$

Example 2.14 Minitab can be used to construct EWMA control charts. Figure 2.40 is the EWMA control chart of the forecast errors in Table 2.3. This chart uses the mean forecast error as the center line, $\sigma _ { e ( 1 ) }$ was estimated using the moving range method, and we chose $\lambda = 0 . 1$ . None of the forecast

![](images/2f16194c42fa08c165daf423bc348a32ea1b1ba94a74c2595bbb48a127a319a9.jpg)  
FIGURE 2.40 EWMA control chart of the one-step-ahead forecast errors in Table 2.3.

errors exceeds the control limits so there is no indication of a problem with the forecasting model.

Note from Eq. (2.51) and Figure 2.40 that the control limits on the EWMA control chart increase in width for the -rst few observations and then stabilize at a constant value because the term $[ 1 - ( 1 - \lambda ) ^ { 2 t } ]$ approaches unity as $t$ increases. Therefore steady-state limits for the EWMA control chart are

$$
\mathrm {U C L} = T _ {0} + 3 \sigma_ {e (1)} \sqrt {\frac {\lambda}{2 - \lambda}}
$$

Center line = T (2.54)

$$
\mathrm {L C L} = T _ {0} - 3 \sigma_ {e (1)} \sqrt {\frac {\lambda}{2 - \lambda}}.
$$

In addition to control charts, other statistics have been suggested for monitoring the performance of a forecasting model. The most common of these are tracking signals. The cumulative error tracking signal (CETS) is based on the cumulative sum of all current and previous forecast errors, say,

$$
Y (n) = \sum_ {t = 1} ^ {n} e _ {t} (1) = Y (n - 1) + e _ {n} (1).
$$

If the forecasts are unbiased, we would expect $Y ( n )$ to uctuate around zero. If it differs from zero by very much, it could be an indication that the forecasts are biased. The standard deviation of $Y ( n )$ , say, $\sigma _ { Y ( n ) }$ , will provide a measure of how far $Y ( n )$ can deviate from zero due entirely to random variation. Therefore, we would conclude that the forecast is biased if $| Y ( n ) |$ exceeds some multiple of its standard deviation. To operationalize this, suppose that we have an estimate ${ \hat { \sigma } } _ { Y ( n ) }$ of $\sigma _ { Y ( n ) }$ and form the cumulative error tracking signal

$$
\text {C E T S} = \left| \frac {Y (n)}{\hat {\sigma} _ {Y (n)}} \right|. \tag {2.55}
$$

If the CETS exceeds a constant, say, $K _ { 1 }$ , we would conclude that the forecasts are biased and that the forecasting model may be inadequate.

It is also possible to devise a smoothed error tracking signal based on the smoothed one-step-ahead forecast errors in Eq. (2.52). This would lead to a ratio

$$
\text {S E T S} = \left| \frac {\bar {e} _ {n} (1)}{\hat {\sigma} _ {\bar {e} _ {n} (1)}} \right|. \tag {2.56}
$$

If the SETS exceeds a constant, say, $K _ { 2 }$ , this is an indication that the forecasts are biased and that there are potentially problems with the forecasting model.

Note that the CETS is very similar to the CUSUM control chart and that the SETS is essentially equivalent to the EWMA control chart. Furthermore, the CUSUM and EWMA are available in standard statistics software (such as Minitab) and the tracking signal procedures are not. So, while tracking signals have been discussed extensively and recommended by some authors, we are not going to encourage their use. Plotting and periodically visually examining a control chart of forecast errors is also very informative, something that is not typically done with tracking signals.

# 2.7 R COMMANDS FOR CHAPTER 2

Example 2.15 The data are in the second column of the array called gms.data in which the -rst column is the year. For moving averages, we use functions from package 鈥渮oo.鈥?
plot(gms.data,type $= 1$ "l",xlab $\equiv$ 'Year',ylab $\equiv$ 'Average Amount of   
Anomaly, $^\circ \mathrm{C})$ points(gms.data,pch $= 16$ ,cex $= .5$ lines(gms.data[5:125,1],rollmean(gms.data[,2],5),col $\equiv$ "red")   
points(gms.data[5:125,1],rollmean(gms.data[,2],5),col $\equiv$ "red",pch $= 15$ -   
cex $= .5$ legend(1980锛?.3,c("Actual","Fits"),pch=c(16,15),lwd=c(.5锛?5)锛?  
cex $= .55$ ,col $=$ c("black","red"))

![](images/c5f3a96941a592a29c45b823fac76c68fb529d3a68f2a97f15eaefceb0184ea3.jpg)

# Example 2.16 The data are in the second column of the array called vis.data in which the -rst column is the time period (or index).

# Moving Average plot vis.data,type $= 1$ ,xlab $\equiv$ 'Time Period',ylab $\equiv$ 'Viscosity, cP') points (vis.data,pch $= 16$ ,cex=.5) lines (vis.data[5:100,1], rollmean (vis.data[,2],5),col="red") points (vis.data[5:100,1], rollmean (vis.data[,2],5),col="red", pch $= 15$ ,cex=.5) legend(1,61,c("Actual","Fits"),pch=c(16,15),lwd=c(.5,.5),cex=.55, col=c("black","red"))

![](images/ea85a81eb2d028009f69539835ecf4135ab737288f6ce0374e1ca30402b9c082.jpg)

# Moving Median plot(vis.data,type $= 1$ "l",xlab $\equiv$ 'Time Period',ylab $\equiv$ 'Viscosity,cP') points vis.data,pch $= 16$ ,cex $= .5$ lines (vis.data[5:100,1], rollmedian(vis.data[,2],5),col="red") points vis.data[5:100,1], rollmedian(vis.data[,2],5),col $=$ "red", pch $= 15$ ,cex $= .5$ legend(1,61,c("Actual","Fits"),pch=c(16,15),lwd=c(.5,.5),cex=.55, col=c("black","red"))

![](images/a388ebea8f8a35743dd1d39e984ecc51e9723258f873fb337237912b1782416b.jpg)

Example 2.17 The pharmaceutical sales data are in the second column of the array called pharma.data in which the -rst column is the week.

The viscosity data are in the second column of the array called vis.data in which the -rst column is the year (Note that the 70th observation is corrected).

nrp<-dim(pharma.data)[1]

nrv<-dim(vis.data)[1]

plot(pharma.data[1:(nrp-1),2], pharma.data[2:nrp,2],type $: =$ "p", xlab $^ { 1 = }$ 'Sales, Week t',ylab $^ { \prime = }$ ' Sales, Week t+1',pch $^ { - 2 0 }$ ,cex $^ { \cdot = 1 }$ )

plot(vis.data[1:(nrv-1),2], vis.data[2:nrv,2],type $: =$ "p", xlab $\nu =$ 'Reading, Time Period t',ylab $^ { \ast = }$ ' Reading, Time Period t+1',pch $\scriptstyle = 2 0$ , cex $^ { = 1 }$ )

![](images/1871a03ff639ed4d849f9b75dce0145552099161182c99352334b8819e4163e0.jpg)

![](images/e7f110b165b87d555968d917f3a059ef2f4fd311b3baed0c5cbf873d0e7ff26b.jpg)

Example 2.18 The viscosity data are in the second column of the array called vis.data in which the -rst column is the year (Note that the 70th observation is corrected).

acf(vis.data[,2], lag.max $^ { = 2 5 }$ ,type $: =$ "correlation",main $. =$ "ACF of viscosity readings")

![](images/2ce1aeec61601b0b9198f5c365c8998034a3e27cb6795704cddc34614c3b8494.jpg)  
ACF of viscosity readings

Example 2.19 The cheese production data are in the second column of the array called cheese.data in which the -rst column is the year.

fit.cheese<-lm(cheese.data[,2]~cheese.data[,1]) plot(cheese.data,type $=$ "l",xlab $i = 1$ Year',ylab $^ { 1 = }$ 'Production, 10000lb') points(cheese.data,pch $_ { . = 1 6 }$ , $\mathtt { C e x } = . 5$ ) lines(cheese.data[,1], fit.cheese$fit,col $=$ "red",lt $^ { \cdot } = 2$ ) legend(1990,12000,c("Actual","Fits"), pch $\boldsymbol { \mathbf { \mathit { \varepsilon } } } = \mathbf { \mathit { \varepsilon } } _ { \mathbf { \mathit { \mathbf { C } } } }$ (16,NA),lwd $\boldsymbol { \mathbf { \mathit { \varepsilon } } } = \mathbf { \mathit { \varepsilon } } _ { \mathbf { \mathit { \mathbf { C } } } }$ (.5,.5),lty $\scriptscriptstyle \sum C$ (1,2),cex $\displaystyle =$ .55,col ${ } = { \mathsf { C } }$ ("black","red"))

![](images/08c2f22d7b3d7b65ee5246cac39c031827ac2b2061db1644fd59c5bf82709cb3.jpg)

```txt
par(mfrow=c(2,2),oma=c(0,0,0,0))  
qqnorm.fit.cheese\\(res,datrix \)\equiv\( TRUE,pch \(= 16\) xlab \(\equiv\) 'Residual'锛宮ain \(\equiv\) '')  
qqline.fit.cheese\\)res,datrix \(\equiv\) TRUE)  
plot.fit.cheese\\(fit,fit.cheese\\)res,pch \(= 16\) 锛寈lab \(\equiv\) 'Fitted Value',ylab \(\equiv\) 'Residual')  
abline(h=0)  
hist.fit.cheese\\)res,col \(\equiv\) "gray",xlab \(\equiv\) 'Residual'锛宮ain \(\equiv\) '')  
plot.fit.cheese\\)res,type \(\equiv\) "l"锛寈lab \(\equiv\) 'Observation Order'锛寉lab \(\equiv\) 'Residual')  
points(fit.cheese\\)res,pch \(= 16\) ,cex=.5)  
abline(h=0) 
```

![](images/14a79e1c68e8a33e07dde810dc0df594a3ba60d2965f593479dc64b76de403f9.jpg)

![](images/5d2df300f762a8f53cf6f39389793c2ae43c31c5e0a68a39dd1ec81ef66b2ec7.jpg)

![](images/6a86a1489a83b31823bf78c869bf68a741e4827a5ffd70c653c78c62db03ea40.jpg)

![](images/49433de6cc6edfb8d4329e84bfc2c0a081bf3726ac3be03b4719bb80a6c27d87.jpg)

Example 2.20 The cheese production data are in the second column of the array called cheese.data in which the -rst column is the year.   
nrc<-dim(cheese.data) [1]  
dcheese.data<-cbind(cheese.data[2:nrc,1],diff(cheese.data[,2]))  
fit.dcheese<-lm(dcheese.data[,2]~dcheese.data[,1])  
plot(dcheese.data,type $= "l"$ ,xlab $= "$ ,ylab $=$ 'Production锛宒=1')  
points(dcheese.data,pch=16,cex=.5)  
lines(dcheese.data[,1]锛宖it.dcheese\$fit,col $= "red"$ ,lty $= 2$ )  
legend(1952,-2200,c("Actual","Fits")锛? 
pch=c(16,NA),lwd=c(.5,.5)锛宭ty=c(1,2)锛? 
cex=.75,col=c("black","red"))

![](images/325f8270745b82f6d58f0e49022c95771efe3a38f302bb8d4056419f1d87e2fa.jpg)

par(mfrow ${ \bf \Pi } = { \bf C }$ (2,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

qqnorm(fit.dcheese$res,datax $: =$ TRUE,pch=16,xla $^ { 1 = }$ 'Residual',main $. = \cdot \cdot$ )

qqline(fit.dcheese$res,datax $: =$ TRUE)

plot(fit.dcheese$fit,fit.dcheese$res,pch $_ { . = 1 6 }$ , xlab $^ { 1 = }$ 'Fitted Value', ylab $^ { 1 = }$ 'Residual')

abline $\scriptstyle \mathrm { \mathrm { h } = 0 }$ )

hist(fit.dcheese$res,col $=$ "gray",xlab $i = 1$ 'Residual',main $. = 1$ ')

plot(fit.dcheese$res,typ $: =$ "l",xlab $i = 1$ Observation Order', ylab $^ { 1 = }$ 'Residual')

points(fit.dcheese$res,pch=16,cex .5)

abline $\mathtt { h } = 0$ )

![](images/b69a08c8b66a4897570ad87cfedb6e8734e36efbc66aad28b021848309010496.jpg)

![](images/2fe96ef44c14df31d1d552e470de6762048d42b93cdf1dd2d4ec9476174c0314.jpg)

![](images/11dc45c4b9aed406c94cb4509e12a905755239e5e9f07c967e7ed954e396a2d8.jpg)

![](images/688c2c776921d7bcd78f682e9cd83ac07706b584d5a87ca1ca9af11d70546e73.jpg)

# Example 2.21 The beverage sales data are in the second column of the array called bev.data in which the -rst column is the month of the year.

```txt
nrb<-dim(bev.data) [1]  
tt<-1:nrb  
dsbev.data<-bev.data  
dsbev.data[,2] <- c(array(NA, dim=c(12, 1)), diff(bev.data[,2], 12)) 
```

```python
plot(tt, dsbev.data[,2], type="l", xlab='', ylab='Seasonal d=12', xaxt='n') axis(1, seq(1, nrb, 24), labels=dsbev.data[seq(1, nrb, 24), 1]) points(tt, dsbev.data[,2], pch=16, cex=.5) 
```

![](images/5be5447121d6f5926bee7a8857e449421e98cac18f56186ccbfe3e5f93439ec8.jpg)

```r
dstbev.data<-dsbev.data  
dstbev.data[,2] <- c(NA, diff(dstbev.data[,2],1))  
fit.dstbev<-lm(dstbev.data[,2]~tt)  
plot(tt,dstbev.data[,2],type="l",xlab='',ylab='Seasonal d=12 with Trend d=1',xaxt='n')  
axis(1,seq(1,nrb,24),labels=dsbev.data[seq(1,nrb,24),1])  
points(tt,dstbev.data[,2],pch=16,cex=.5)  
lines(c(array(NA, dim=c(12,1)),fit.dstbev\$fit),col="red",lty=2)  
legend(2,-300,c("Actual","Fits"),  
pch=c(16,NA),lwd=c(.5,.5),lty=c(1,2),cex=.75,col=c("black","red")) 
```

![](images/5bc875e986f604f46c4d2ef804adc312140a4a6d96b9eb86a17e4e2cf497495a.jpg)

```txt
par(mfrow=c(2,2),oma=c(0,0,0,0))  
qqnorm(fit.dstbev$res,datrix=TRUE,pch=16,xlab='Residual',main='')  
qqline(fit.dstbev$res,datrix=TRUE)  
plot(fit.dstbev$fit,fit.dstbev$res,pch=16,xlab='Fitted Value',ylab='Residual')  
abline(h=0)  
hist(fit.dstbev$res,col="gray",xlab='Residual',main='')  
plot(fit.dstbev$res,type="1",xlab='Observation Order',ylab='Residual')  
points(fit.dstbev$res,pch=16,cex=.5)  
abline(h=0) 
```

![](images/c392bf0f8a9ba4d465d4e76c234226d8288608ede7681b7ed213c217a8d18308.jpg)

![](images/0a3add296a91683d5534fa3dc4844250c11b61d8ef9f36e56559f1f94392cff1.jpg)

![](images/6a958158dfee53f6b01998aae8c55a70660df0e3ecbe586f90e1b87dab5aff15.jpg)

![](images/5b1208f57762a97c45bfd23afcd5642d83be4f5644d64da94e132a7de350ad91.jpg)

Example 2.22 The beverage sales data are in the second column of the array called bev.data in which the -rst column is the month of the year.

Software packages use different methods for decomposing a time series. Below we provide the code of doing it in R without using these functions. Note that we use the additive model.

nrb<-dim(bev.data) [1]   
# De-trend the data   
tt<-1:nrb   
fit.tbev<-lm(bev.data[,2]~tt)   
bev.data.dt<-fit.tbev\$res   
# Obtain seasonal medians for each month, seasonal period is sp=12 sp<-12 smed<-apply matrix(bev.data.dt,nrow $\coloneqq$ sp),1,median)   
# Adjust the medians so that their sum is zero smed<-smed-mean(smed)   
# Data without the trend and seasonal components   
bev.data.dts<-bev.data.dt-rep(smed,nrb/sp)   
# Note that we can also reverse the order, i.e. first take the seasonality out smed2<-apply (matrix(bev.data[,2],nrow $\equiv$ sp),1,median) smed2<-smed2-mean(smed2)   
bev.data.ds<-bev.data[,2]-rep(smed2,nrb/sp)   
# To reproduce Figure 2.25   
par(mfrow=c(2,2),oma=c(0,0,0,0)) plot(tt,bev.data[,2],type $=$ "l",xlab $=$ '(a) Original Data',ylab= 'Data',xaxt $=$ 'n') axis(1,seq(1,nrb,24),labels $\equiv$ bev.data[seq(1,nrb,24),1]) points(tt,bev.data[,2],pch=16,cex=.75)   
plot(tt,bev.data.dt,type $=$ "l",xlab $=$ '(b) Detrended Data',ylab $=$ 'Detr. Data',xaxt $=$ 'n') axis(1,seq(1,nrb,24),labels $\equiv$ bev.data[seq(1,nrb,24),1])   
points(tt,bev.data.dt,pch=16,cex=.75)   
plot(tt,bev.data.ds,type $=$ "l",xlab $=$ '(c) Seasonally Adjusted Data', ylab $=$ 'Seas.   
Adj.Data',xaxt $=$ 'n')   
axis(1,seq(1,nrb,24),labels $\equiv$ bev.data[seq(1,nrb,24),1])   
points(tt,bev.data.ds,pch=16,cex=.75)

plot(tt, bev.data.dts,type $: =$ "l",xlab $= ^ { \mathsf { I } }$ (c) Seasonally Adj. and Detrended Data',ylab $=$ 'Seas. Adj. and Detr. Data',xaxt $= \ l ^ { 1 } \mathrm { ~ n ~ } ^ { \prime }$ ) axis(1,seq(1,nrb,24),labels $=$ bev.data[seq(1,nrb,24),1]) points(tt, bev.data.dts, pch=16,cex $\ l =$ .75)

![](images/404e2d81e70670857bedf53fbfc20816bd0771e1a207a482aed55f9567dbb9e1.jpg)  
(a) Original data

![](images/45af78f39703102b86eca824b88428be43654872aa8a5b679da15f4741a5e3f4.jpg)  
(b) Detrended data

![](images/85c71fdb381a5fc335781dac542dab49b84654e50a58d99f77a67a08d6671a2b.jpg)  
(c) Seasonally adjusted data

![](images/3aeaf80c1b34a4595f8c05085d3f8b064e310d7751f9f100344d1e99ce4d4328.jpg)  
(c) Seasonally Adj. and detrended data

Example 2.23 Functions used to -t a time series model often also provide summary statistics. However, in this example we provide some calculations for a given set of forecast errors as provided in the text.

# original data and forecast errors

yt<-c(47,46,51,44,54,47,52,45,50,51,49,41,48,50,51,55,52,53,48,52)

fe<-c(-4.1,-6.9,2.2,-4.1,4.3,-.5,.8,-8.1,-4.4,-.2,-4.3,-5.5,-5.1,

-2.1,4.2,7.3,6.6,5.9,-3.8,6.2)

ME<-mean(fe)

MAD<-mean(abs(fe))

MSE<-mean(fe藛2)

ret1<-(fe/yt)*100

MPE<-mean(ret1)

MAPE<-mean(abs(ret1))

> ME

[1] -0.58

> MAD

[1] 4.33

> MSE

[1] 23.59

> MPE

[1] -1.757938

> MAPE

[1] 8.865001

Example 2.24 The forecast error data are in the second column of the array called fe2.data in which the -rst column is the period.

acf.fe2<-acf(fe2.data[,2],main $. = \cdot$ ACF of Forecast Error (Ex 2.11)')

![](images/6bfd0df977a2dba7fa73e9de8776c9df0163818a48a1c8660f5f240790b760e5.jpg)

# To get the $\mathsf { Q } _ { \mathtt { L B } }$ statistic, we first define the lag K

K<-13

T<-dim(fe2.data)[1]

QLB<-T* $\mathbf { T } + 2$ )*sum((1/(T-1:K))*(acf.fe2$acf[2:(K+1)]藛2))

# Upper $5 \%$ of $\chi ^ { 2 }$ distribution with K degrees of freedom qchisq(.95,K)

Example 2.25 The forecast error data are in the second column of the array called fe2.data in which the -rst column is the period.

# The following function can be found in qcc package

# Generating the chart for individuals

qcc(fe2.data[,2],type $: =$ "xbar.one",title $=$ "Individuals Chart for the Forecast Error")

![](images/572f1f18a5dd64cec171398963adb682cc28b9771587c01c9c203c3ee6dd78bf.jpg)  
Example 2.26 The forecast error data are in the second column of the array called fe2.data in which the -rst column is the period.

# The following function can be found in qcc package # Generating the cusum chart

cusum(fe2.data[,2], title $=$ 'Cusum Chart for the Forecast Error', sizes $^ { = 1 }$ )

![](images/db32fb7df010093f0690d1a7ca0b71ede4e070688bb663e6b330fff6046787ae.jpg)

Example 2.27 The forecast error data are in the second column of the array called fe2.data in which the -rst column is the period.

# The following function can be found in qcc package # Generating the EWMA chart ewma(fe2.data[,2], title='EWMA Chart for the Forecast Error', lambda $=$ .1,sizes $^ { = 1 }$ )

![](images/f9a78c717e69f740bb7a405ed3aca73d4a83afe21194d21c9211328b6508414d.jpg)

# EXERCISES

2.1 Consider the US Treasury Securities rate data in Table B.1 (Appendix B). Find the sample autocorrelation function and the variogram for these data. Is the time series stationary or nonstationary?   
2.2 Consider the data on US production of blue and gorgonzola cheeses in Table B.4.

a. Find the sample autocorrelation function and the variogram for these data. Is the time series stationary or nonstationary?   
b. Take the -rst difference of the time series, then -nd the sample autocorrelation function and the variogram. What conclusions can you draw about the structure and behavior of the time series?

2.3 Table B.5 contains the US beverage product shipments data. Find the sample autocorrelation function and the variogram for these data. Is the time series stationary or nonstationary?   
2.4 Table B.6 contains two time series: the global mean surface air temperature anomaly and the global $\mathrm { C O } _ { 2 }$ concentration. Find the sample autocorrelation function and the variogram for both of these time series. Is either one of the time series stationary?   
2.5 Reconsider the global mean surface air temperature anomaly and the global $\mathrm { C O } _ { 2 }$ concentration time series from Exercise 2.4. Take the -rst difference of both time series. Find the sample autocorrelation function and variogram of these new time series. Is either one of these differenced time series stationary?   
2.6 Find the closing stock price for a stock that interests you for the last 200 trading days. Find the sample autocorrelation function and the variogram for this time series. Is the time series stationary?   
2.7 Reconsider the Whole Foods Market stock price data from Exercise 2.6. Take the -rst difference of the data. Find the sample autocorrelation function and the variogram of this new time series. Is this differenced time series stationary?   
2.8 Consider the unemployment rate data in Table B.8. Find the sample autocorrelation function and the variogram for this time series. Is the time series stationary or nonstationary? What conclusions can you draw about the structure and behavior of the time series?   
2.9 Table B.9 contains the annual International Sunspot Numbers. Find the sample autocorrelation function and the variogram for this time series. Is the time series stationary or nonstationary?   
2.10 Table B.10 contains data on the number of airline miles own in the United Kingdom. This is strongly seasonal data. Find the sample autocorrelation function for this time series.

a. Is the seasonality apparent in the sample autocorrelation function?   
b. Is the time series stationary or nonstationary?

2.11 Reconsider the data on the number of airline miles own in the United Kingdom from Exercise 2.10. Take the natural logarithm of the data and plot this new time series.

a. What impact has the log transformation had on the time series?

b. Find the autocorrelation function for this time series.   
c. Interpret the sample autocorrelation function.

2.12 Reconsider the data on the number of airline miles own in the United Kingdom from Exercises 2.10 and 2.11. Take the -rst difference of the natural logarithm of the data and plot this new time series.

a. What impact has the log transformation had on the time series?   
b. Find the autocorrelation function for this time series.   
c. Interpret the sample autocorrelation function.

2.13 The data on the number of airline miles own in the United Kingdom in Table B.10 are seasonal. Difference the data at a season lag of 12 months and also apply a -rst difference to the data. Plot the differenced series. What effect has the differencing had on the time series? Find the sample autocorrelation function and the variogram. What does the sample autocorrelation function tell you about the behavior of the differenced series?   
2.14 Table B.11 contains data on the monthly champagne sales in France. This is strongly seasonal data. Find the sample autocorrelation function and variogram for this time series.

a. Is the seasonality apparent in the sample autocorrelation function?   
b. Is the time series stationary or nonstationary?

2.15 Reconsider the champagne sales data from Exercise 2.14. Take the natural logarithm of the data and plot this new time series.

a. What impact has the log transformation had on the time series?   
b. Find the autocorrelation function and variogram for this time series.   
c. Interpret the sample autocorrelation function and variogram.

2.16 Table B.13 contains data on ice cream and frozen yogurt production. Plot the data and calculate both the sample autocorrelation function and variogram. Is there an indication of nonstationary behavior in the time series? Now plot the -rst difference of the time series and compute the sample autocorrelation function and variogram of the -rst differences. What impact has differencing had on the time series?

2.17 Table B.14 presents data on $\mathrm { C O } _ { 2 }$ readings from the Mauna Loa Observatory. Plot the data, then calculate the sample autocorrelation

function and variogram. Is there an indication of nonstationary behavior in the time series? Now plot the -rst difference of the time series and compute the sample autocorrelation function and the variogram of the -rst differences. What impact has differencing had on the time series?

2.18 Data on violent crime rates are given in Table B.15. Plot the data and calculate the sample autocorrelation function and variogram. Is there an indication of nonstationary behavior in the time series? Now plot the -rst difference of the time series and compute the sample autocorrelation function and variogram of the -rst differences. What impact has differencing had on the time series?   
2.19 Table B.16 presents data on the US Gross Domestic Product (GDP). Plot the GDP data and calculate the sample autocorrelation function and variogram. Is there an indication of nonstationary behavior in the time series? Now plot the -rst difference of the GDP time series and compute the sample autocorrelation function and variogram of the -rst differences. What impact has differencing had on the time series?   
2.20 Table B.17 contains information on total annual energy consumption. Plot the energy consumption data and calculate the sample autocorrelation function and variogram. Is there an indication of nonstationary behavior in the time series? Now plot the -rst difference of the time series and compute the sample autocorrelation function and variogram of the -rst differences. What impact has differencing had on the time series?   
2.21 Data on US coal production are given in Table B.18. Plot the coal production data and calculate the sample autocorrelation function and variogram. Is there an indication of nonstationary behavior in the time series? Now plot the -rst difference of the time series and compute the sample autocorrelation function and variogram of the -rst differences. What impact has differencing had on the time series?   
2.22 Consider the $\mathrm { C O } _ { 2 }$ readings from Mauna Loa in Table B.14. Use a sixperiod moving average to smooth the data. Plot both the smoothed data and the original $\mathrm { C O } _ { 2 }$ readings on the same axes. What has the moving average done? Repeat the procedure with a three-period moving average. What is the effect of changing the span of the moving average?

2.23 Consider the violent crime rate data in Table B.15. Use a ten-period moving average to smooth the data. Plot both the smoothed data and the original $\mathrm { C O } _ { 2 }$ readings on the same axes. What has the moving average done? Repeat the procedure with a four-period moving average. What is the effect of changing the span of the moving average?   
2.24 Table B.21 contains data from the US Energy Information Administration on monthly average price of electricity for the residential sector in Arizona. Plot the data and comment on any features that you observe from the graph. Calculate and plot the sample ACF and variogram. Interpret these graphs.   
2.25 Reconsider the residential electricity price data from Exercise 2.24.

a. Plot the -rst difference of the data and comment on any features that you observe from the graph. Calculate and plot the sample ACF and variogram for the differenced data. Interpret these graphs. What impact did differencing have?   
b. Now difference the data again at a seasonal lag of 12. Plot the differenced data and comment on any features that you observe from the graph. Calculate and plot the sample ACF and variogram for the differenced data. Interpret these graphs. What impact did regular differencing combined with seasonal differencing have?

2.26 Table B.22 contains data from the Danish Energy Agency on Danish crude oil production. Plot the data and comment on any features that you observe from the graph. Calculate and plot the sample ACF and variogram. Interpret these graphs.   
2.27 Reconsider the Danish crude oil production data from Exercise 2.26. Plot the -rst difference of the data and comment on any features that you observe from the graph. Calculate and plot the sample ACF and variogram for the differenced data. Interpret these graphs. What impact did differencing have?   
2.28 Use a six-period moving average to smooth the -rst difference of the Danish crude oil production data that you computed in Exercise 2.27. Plot both the smoothed data and the original data on the same axes. What has the moving average done? Does the moving average look like a reasonable forecasting technique for the differenced data?   
2.29 Weekly data on positive laboratory test results for inuenza are shown in Table B.23. Notice that these data have a number of missing

values. Construct a time series plot of the data and comment on any relevant features that you observe.

a. What is the impact of the missing observations on your ability to model and analyze these data?   
b. Develop and implement a scheme to estimate the missing values

2.30 Climate data collected from Remote Automated Weather Stations (RAWS) are used to monitor the weather and to assist land management agencies with projects such as monitoring air quality, rating -re danger, and other research purposes. Data from the Western Regional Climate Center for the mean daily solar radiation (in Langleys) at the Zion Canyon, Utah, station are shown in Table B.24.

a. Plot the data and comment on any features that you observe.   
b. Calculate and plot the sample ACF and variogram. Comment on the plots.   
c. Apply seasonal differencing to the data, plot the data, and construct the sample ACF and variogram. What was the impact of seasonal differencing?

2.31 Table B.2 contains annual US motor vehicle traf-c fatalities along with other information. Plot the data and comment on any features that you observe from the graph. Calculate and plot the sample ACF and variogram. Interpret these graphs.   
2.32 Reconsider the motor vehicle fatality data from Exercise 2.31.

a. Plot the -rst difference of the data and comment on any features that you observe from the graph. Calculate and plot the sample ACF and variogram for the differenced data. Interpret these graphs. What impact did differencing have?   
b. Compute a six-period moving average for the differenced data. Plot the moving average and the original data on the same axes. Does it seem that the six-period moving average would be a good forecasting technique for the differenced data?

2.33 Apply the X-11 seasonal decomposition method (or any other seasonal adjustment technique for which you have software) to the mean daily solar radiation in Table B.24.   
2.34 Consider the $N$ -span moving average applied to data that are uncorrelated with mean $\mu$ and variance $\sigma ^ { 2 }$ .

a. Show that the variance of the moving average is Var $( M _ { t } ) = \sigma ^ { 2 } / N$

b. Show that $\begin{array} { r } { \mathrm { C o v } ( M _ { t } , M _ { t + k } ) = \sigma ^ { 2 } \sum _ { j = 1 } ^ { N - k } { ( 1 / N ) ^ { 2 } } } \end{array}$ , for $k < N$

c. Show that the autocorrelation function is

$$
\rho_ {k} = \left\{ \begin{array}{l l} 1 - \frac {| k |}{N}, & k = 1, 2, \ldots , N - 1 \\ 0, & k \geq N \end{array} \right.
$$

2.35 Consider an $N .$ -span moving average where each observation is weighted by a constant, say, $a _ { j } \geq 0$ . Therefore the weighted moving average at the end of period $T$ is

$$
M _ {T} ^ {w} = \sum_ {t = T - N + 1} ^ {T} a _ {T + 1 - t} y _ {t}.
$$

a. Why would you consider using a weighted moving average?

b. Show that the variance of the weighted moving average is Var $\begin{array} { r } { ( M _ { T } ^ { w } ) = \sigma ^ { 2 } \sum _ { j = i } ^ { N } a _ { j } ^ { 2 } } \end{array}$ .

c. Show that $\begin{array} { r } { \mathrm { C o v } ( M _ { T } ^ { w } , M _ { T + k } ^ { w } ) = \sigma ^ { 2 } \sum _ { j = 1 } ^ { N - k } a _ { j } a _ { j + k } , | k | < N } \end{array}$ 2 鈭慛鈭択 ajaj+k, k < N .

d. Show that the autocorrelation function is

$$
\rho_ {k} = \left\{ \begin{array}{l l} \left(\sum_ {j = 1} ^ {N - k} a _ {j} a _ {j + k}\right) \Bigg / \left(\sum_ {j = 1} ^ {N} a _ {j} ^ {2}\right), & k = 1, 2, \ldots , N - 1 \\ 0, & k \geq N \end{array} \right.
$$

2.36 Consider the Hanning -lter. This is a weighted moving average.

a. Find the variance of the weighted moving average for the Hanning -lter. Is this variance smaller than the variance of a simple span-3 moving average with equal weights?

b. Find the autocorrelation function for the Hanning -lter. Compare this with the autocorrelation function for a simple span-3 moving average with equal weights.

2.37 Suppose that a simple moving average of span $N$ is used to forecast a time series that varies randomly around a constant, that is, $y _ { t } =$ $\mu + \varepsilon _ { t }$ , where the variance of the error term is $\sigma ^ { 2 }$ . The forecast error at lead one is $e _ { T + 1 } ( 1 ) = y _ { T + 1 } - M _ { T }$ . What is the variance of this lead-one forecast error?

2.38 Suppose that a simple moving average of span $N$ is used to forecast a time series that varies randomly around a constant, that is,

$y _ { t } = \mu + \varepsilon _ { t }$ , where the variance of the error term is $\sigma ^ { 2 }$ . You are interested in forecasting the cumulative value of $y$ over a lead time of $L$ periods, say, $y _ { T + 1 } + y _ { T + 2 } + \dots + y _ { T + L }$ .

a. The forecast of this cumulative demand is $L M _ { T }$ . Why?   
b. What is the variance of the cumulative forecast error?

2.39 Suppose that a simple moving average of span $N$ is used to forecast a time series that varies randomly around a constant mean, that is, $y _ { t } = \mu + \varepsilon _ { t }$ . At the start of period $t _ { 1 }$ the process shifts to a new mean level, say, $\mu + \delta$ . Show that the expected value of the moving average is

$$
E (M _ {T}) = \left\{ \begin{array}{l l} \mu , & T \leq t _ {1} - 1 \\ \mu + \frac {T - t _ {1} + 1}{N} \delta , & t _ {1} \leq T \leq t _ {1} + N - 2. \\ \mu + \delta , & T \geq t _ {1} + N - 1 \end{array} \right..
$$

2.40 Suppose that a simple moving average of span $N$ is used to forecast a time series that varies randomly around a constant mean, that is, $y _ { t } = \mu + \varepsilon _ { t }$ . At the start of period $t _ { 1 }$ the process experiences a transient; that is, it shifts to a new mean level, say, $\mu + \delta$ , but it reverts to its original level $\mu$ at the start of period $t _ { 1 } + 1$ . Show that the expected value of the moving average is

$$
E (M _ {T}) = \left\{ \begin{array}{l l} \mu , & T \leq t _ {1} - 1 \\ \mu + \frac {\delta}{N}, & t _ {1} \leq T \leq t _ {1} + N - 1. \\ \mu , & T \geq t _ {1} + N \end{array} \right.
$$

2.41 If a simple $N -$ span moving average is applied to a time series that has a linear trend, say, $y _ { t } = \beta _ { 0 } + \beta _ { 1 } t + \varepsilon _ { t }$ , the moving average will lag behind the observations. Assume that the observations are uncorrelated and have constant variance. Show that at time $T$ the expected value of the moving average is

$$
E (M _ {T}) = \beta_ {0} + \beta_ {1} T - \frac {N - 1}{2} \beta_ {1}.
$$

2.42 Use a three-period moving average to smooth the champagne sales data in Table B.11. Plot the moving average on the same axes as the original data. What impact has this smoothing procedure had on the data?

TABLE E2.1 One-Step-Ahead Forecast Errors for Exercise 2.44   

<table><tr><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td></tr><tr><td>1</td><td>1.83</td><td>11</td><td>-2.30</td><td>21</td><td>3.30</td><td>31</td><td>-0.07</td></tr><tr><td>2</td><td>-1.80</td><td>12</td><td>0.65</td><td>22</td><td>1.036</td><td>32</td><td>0.57</td></tr><tr><td>3</td><td>0.09</td><td>13</td><td>-0.01</td><td>23</td><td>2.042</td><td>33</td><td>2.92</td></tr><tr><td>4</td><td>-1.53</td><td>14</td><td>-1.11</td><td>24</td><td>1.04</td><td>34</td><td>1.99</td></tr><tr><td>5</td><td>-0.58</td><td>15</td><td>0.13</td><td>25</td><td>-0.87</td><td>35</td><td>1.74</td></tr><tr><td>6</td><td>0.21</td><td>16</td><td>-1.07</td><td>26</td><td>-0.39</td><td>36</td><td>-0.76</td></tr><tr><td>7</td><td>1.25</td><td>17</td><td>0.80</td><td>27</td><td>-0.29</td><td>37</td><td>2.35</td></tr><tr><td>8</td><td>-1.22</td><td>18</td><td>-1.98</td><td>28</td><td>2.08</td><td>38</td><td>-1.91</td></tr><tr><td>9</td><td>1.32</td><td>19</td><td>0.02</td><td>29</td><td>3.36</td><td>39</td><td>2.22</td></tr><tr><td>10</td><td>3.63</td><td>20</td><td>0.25</td><td>30</td><td>-0.53</td><td>40</td><td>2.57</td></tr></table>

2.43 Use a 12-period moving average to smooth the champagne sales data in Table B.11. Plot the moving average on the same axes as the original data. What impact has this smoothing procedure had on the data?   
2.44 Table E2.1 contains 40 one-step-ahead forecast errors from a forecasting model.

a. Find the sample ACF of the forecast errors. Interpret the results.   
b. Construct a normal probability plot of the forecast errors. Is there evidence to support a claim that the forecast errors are normally distributed?   
c. Find the mean error, the mean squared error, and the mean absolute deviation. Is it likely that the forecasting technique produces unbiased forecasts?

2.45 Table E2.2 contains 40 one-step-ahead forecast errors from a forecasting model.

a. Find the sample ACF of the forecast errors. Interpret the results.   
b. Construct a normal probability plot of the forecast errors. Is there evidence to support a claim that the forecast errors are normally distributed?   
c. Find the mean error, the mean squared error, and the mean absolute deviation. Is it likely that the forecasting method produces unbiased forecasts?

2.46 Exercises 2.44 and 2.45 present information on forecast errors. Suppose that these two sets of forecast errors come from two different

TABLE E2.2 One-Step-Ahead Forecast Errors for Exercise 2.45   

<table><tr><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td><td>Period, t</td><td>et(1)</td></tr><tr><td>1</td><td>-4.26</td><td>11</td><td>3.62</td><td>21</td><td>-6.24</td><td>31</td><td>-6.42</td></tr><tr><td>2</td><td>-3.12</td><td>12</td><td>-5.08</td><td>22</td><td>-0.25</td><td>32</td><td>-8.94</td></tr><tr><td>3</td><td>-1.87</td><td>13</td><td>-1.35</td><td>23</td><td>-3.64</td><td>33</td><td>-1.76</td></tr><tr><td>4</td><td>0.98</td><td>14</td><td>3.46</td><td>24</td><td>5.49</td><td>34</td><td>-0.57</td></tr><tr><td>5</td><td>-5.17</td><td>15</td><td>-0.19</td><td>25</td><td>-2.01</td><td>35</td><td>-10.32</td></tr><tr><td>6</td><td>0.13</td><td>16</td><td>-7.48</td><td>26</td><td>-4.24</td><td>36</td><td>-5.64</td></tr><tr><td>7</td><td>1.85</td><td>17</td><td>-3.61</td><td>27</td><td>-4.61</td><td>37</td><td>-1.45</td></tr><tr><td>8</td><td>-2.83</td><td>18</td><td>-4.21</td><td>28</td><td>3.24</td><td>38</td><td>-5.67</td></tr><tr><td>9</td><td>0.95</td><td>19</td><td>-6.49</td><td>29</td><td>-8.66</td><td>39</td><td>-4.45</td></tr><tr><td>10</td><td>7.56</td><td>20</td><td>4.03</td><td>30</td><td>-1.32</td><td>40</td><td>-10.23</td></tr></table>

forecasting methods applied to the same time series. Which of these two forecasting methods would you recommend for use? Why?

2.47 Consider the forecast errors in Exercise 2.44. Construct individuals and moving range control charts for these forecast errors. Does the forecasting system exhibit stability over this time period?   
2.48 Consider the forecast errors in Exercise 2.44. Construct a cumulative sum control chart for these forecast errors. Does the forecasting system exhibit stability over this time period?   
2.49 Consider the forecast errors in Exercise 2.45. Construct individuals and moving range control charts for these forecast errors. Does the forecasting system exhibit stability over this time period?   
2.50 Consider the forecast errors in Exercise 2.45. Construct a cumulative sum control chart for these forecast errors. Does the forecasting system exhibit stability over this time period?   
2.51 Ten additional forecast errors for the forecasting model in Exercise 2.44 are as follows: 5.5358, 鈥?.6183, 0.0130, 1.3543, 12.6980, 2.9007, 0.8985, 2.9240, 2.6663, and 鈥?.6710. Plot these additional 10 forecast errors on the individuals and moving range control charts constructed in Exercise 2.47. Is the forecasting system still working satisfactorily?   
2.52 Plot the additional 10 forecast errors from Exercise 2.51 on the cumulative sum control chart constructed in Exercise 2.38. Is the forecasting system still working satisfactorily?

# REGRESSION ANALYSIS AND FORECASTING

Weather forecast for tonight: dark

GEORGE CARLIN, American comedian

# 3.1 INTRODUCTION

Regression analysis is a statistical technique for modeling and investigating the relationships between an outcome or response variable and one or more predictor or regressor variables. The end result of a regression analysis study is often to generate a model that can be used to forecast or predict future values of the response variable, given speci-ed values of the predictor variables.

The simple linear regression model involves a single predictor variable and is written as

$$
y = \beta_ {0} + \beta_ {1} x + \varepsilon , \tag {3.1}
$$

where y is the response, $x$ is the predictor variable, $\beta _ { 0 }$ and $\beta _ { 1 }$ are unknown parameters, and $\varepsilon$ is an error term. The model parameters or regression

coef-cients $\beta _ { 0 }$ and $\beta _ { 1 }$ have a physical interpretation as the intercept and slope of a straight line, respectively. The slope $\beta _ { 1 }$ measures the change in the mean of the response variable $y$ for a unit change in the predictor variable $x$ . These parameters are typically unknown and must be estimated from a sample of data. The error term $\varepsilon$ accounts for deviations of the actual data from the straight line speci-ed by the model equation. We usually think of $\varepsilon$ as a statistical error, so we de-ne it as a random variable and will make some assumptions about its distribution. For example, we typically assume that $\varepsilon$ is normally distributed with mean zero and variance $\sigma ^ { 2 }$ , abbreviated $N ( 0 , \sigma ^ { 2 } )$ . Note that the variance is assumed constant; that is, it does not depend on the value of the predictor variable (or any other variable).

Regression models often include more than one predictor or regressor variable. If there are $k$ predictors, the multiple linear regression model is

$$
y = \beta_ {0} + \beta_ {1} x _ {1} + \beta_ {2} x _ {2} + \dots + \beta_ {k} x _ {k} + \varepsilon . \tag {3.2}
$$

The parameters $\beta _ { 0 } , \beta _ { 1 } , \ldots , \beta _ { k }$ in this model are often called partial regression coef-cients because they convey information about the effect on y of the predictor that they multiply, given that all of the other predictors in the model do not change.

The regression models in Eqs. (3.1) and (3.2) are linear regression models because they are linear in the unknown parameters (the $\beta$ 鈥檚), and not because they necessarily describe linear relationships between the response and the regressors. For example, the model

$$
y = \beta_ {0} + \beta_ {1} x + \beta_ {2} x ^ {2} + \varepsilon
$$

is a linear regression model because it is linear in the unknown parameters $\beta _ { 0 } , \beta _ { 1 }$ , and $\beta _ { 2 }$ , although it describes a quadratic relationship between $y$ and $x$ . As another example, consider the regression model

$$
y _ {t} = \beta_ {0} + \beta_ {1} \sin \frac {2 \pi}{d} t + \beta_ {2} \cos \frac {2 \pi}{d} t + \varepsilon_ {t}, \tag {3.3}
$$

which describes the relationship between a response variable y that varies cyclically with time (hence the subscript t) and the nature of this cyclic variation can be described as a simple sine wave. Regression models such as Eq. (3.3) can be used to remove seasonal effects from time series data (refer to Section 2.4.2 where models like this were introduced). If the period $d$ of the cycle is speci-ed (such as $d = 1 2$ for monthly data with

an annual cycle), then sin $( 2 \pi / d ) t$ and cos $( 2 \pi / d ) t$ are just numbers for each observation on the response variable and Eq. (3.3) is a standard linear regression model.

We will discuss the use of regression models for forecasting or making predictions in two different situations. The -rst of these is the situation where all of the data are collected on $y$ and the regressors in a single time period (or put another way, the data are not time oriented). For example, suppose that we wanted to develop a regression model to predict the proportion of consumers who will redeem a coupon for purchase of a particular brand of milk (y) as a function of the amount of the discount or face value of the coupon $( x )$ . These data are collected over some speci-ed study period (such as a month) and the data do not explicitly vary with time. This type of regression data is called cross-section data. The regression model for cross-section data is written as

$$
y _ {i} = \beta_ {0} + \beta_ {1} x _ {i 1} + \beta_ {2} x _ {i 2} + \dots + \beta_ {k} x _ {i k} + \varepsilon_ {i}, \quad i = 1, 2, \dots , n, \tag {3.4}
$$

where the subscript $i$ is used to denote each individual observation (or case) in the data set and $n$ represents the number of observations. In the other situation the response and the regressors are time series, so the regression model involves time series data. For example, the response variable might be hourly $\mathrm { C O } _ { 2 }$ emissions from a chemical plant and the regressor variables might be the hourly production rate, hourly changes in the concentration of an input raw material, and ambient temperature measured each hour. All of these are time-oriented or time series data.

The regression model for time series data is written as

$$
y _ {t} = \beta_ {0} + \beta_ {1} x _ {t 1} + \beta_ {2} x _ {t 2} + \dots + \beta_ {k} x _ {t k} + \varepsilon_ {t}, \quad t = 1, 2, \dots , T \tag {3.5}
$$

In comparing Eq. (3.5) to Eq. (3.4), note that we have changed the observation or case subscript from $i$ to $t$ to emphasize that the response and the predictor variables are time series. Also, we have used $T$ instead of n to denote the number of observations in keeping with our convention that, when a time series is used to build a forecasting model, $T$ represents the most recent or last available observation. Equation (3.3) is a speci-c example of a time series regression model.

The unknown parameters $\beta _ { 0 } , \beta _ { 1 } , \ldots , \beta _ { k }$ in a linear regression model are typically estimated using the method of least squares. We illustrated least squares model -tting in Chapter 2 for removing trend and seasonal effects from time series data. This is an important application of regression models in forecasting, but not the only one. Section 3.1 gives a formal description

of the least squares estimation procedure. Subsequent sections deal with statistical inference about the model and its parameters, and with model adequacy checking. We will also describe and illustrate several ways in which regression models are used in forecasting.

# 3.2 LEAST SQUARES ESTIMATION IN LINEAR REGRESSION MODELS

We begin with the situation where the regression model is used with crosssection data. The model is given in Eq. (3.4). There are $n > k$ observations on the response variable available, say, $y _ { 1 }$ , y , 鈥?, $y _ { n }$ . Along with each observed response $y _ { i }$ , we will have an observation on each regressor or predictor variable and $x _ { i j }$ denotes the ith observation or level of variable $x _ { j }$ . The data will appear as in Table 3.1. We assume that the error term $\varepsilon$ in the model has expected value $E ( \varepsilon ) = 0$ and variance Var $( \varepsilon ) = \sigma ^ { 2 }$ , and that the errors $\varepsilon _ { i }$ , $i = 1$ $i = 1 , 2 , \dots , n$ are uncorrelated random variables.

The method of least squares chooses the model parameters (the 饾浗鈥檚) in Eq. (3.4) so that the sum of the squares of the errors, $\varepsilon _ { i }$ , is minimized. The least squares function is

$$
\begin{array}{l} L = \sum_ {i = 1} ^ {n} \varepsilon_ {i} ^ {2} = \sum_ {i = 1} ^ {n} (y _ {i} - \beta_ {0} - \beta_ {1} x _ {i 1} - \beta_ {2} x _ {i 2} - \dots - \beta_ {k} x _ {i k}) ^ {2} \\ = \sum_ {i = 1} ^ {n} \left(y _ {i} - \beta_ {0} - \sum_ {j = 1} ^ {k} \beta_ {j} x _ {i j}\right) ^ {2}. \tag {3.6} \\ \end{array}
$$

This function is to be minimized with respect to $\beta _ { 0 } , \beta _ { 1 } , \ldots , \beta _ { k }$ . Therefore the least squares estimators, say, $\hat { \beta } _ { 0 } , \hat { \beta } _ { 1 } , \ldots , \hat { \beta } _ { k }$ , must satisfy

$$
\left. \frac {\partial L}{\partial \beta_ {0}} \right| _ {\beta_ {0}, \beta_ {1}, \dots , \beta_ {k}} = - 2 \sum_ {i = 1} ^ {n} \left(y _ {i} - \hat {\beta} _ {0} - \sum_ {j = 1} ^ {k} \hat {\beta} _ {j} x _ {i j}\right) = 0 \tag {3.7}
$$

TABLE 3.1 Cross-Section Data for Multiple Linear Regression   

<table><tr><td>Observation</td><td>Response, y</td><td>x1</td><td>x2</td><td>...</td><td>xk</td></tr><tr><td>1</td><td>y1</td><td>x11</td><td>x12</td><td>...</td><td>x1k</td></tr><tr><td>2</td><td>y2</td><td>x21</td><td>x22</td><td>...</td><td>x2k</td></tr><tr><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td></tr><tr><td>n</td><td>yn</td><td>xn1</td><td>xn2</td><td>...</td><td>xnk</td></tr></table>

and

$$
\left. \frac {\partial L}{\partial \beta_ {j}} \right| _ {\beta_ {0}, \beta_ {1}, \dots , \beta_ {k}} = - 2 \sum_ {i = 1} ^ {n} \left(y _ {i} - \hat {\beta} _ {0} - \sum_ {j = 1} ^ {k} \hat {\beta} _ {j} x _ {i j}\right) x _ {i j} = 0, \quad j = 1, 2, \dots , k \tag {3.8}
$$

Simplifying Eqs. (3.7) and (3.8), we obtain

$$
\begin{array}{l} n \hat {\beta} _ {0} + \hat {\beta} _ {1} \sum_ {i = 1} ^ {n} x _ {i 1} + \hat {\beta} _ {2} \sum_ {i = 1} ^ {n} x _ {i 2} + \dots + \hat {\beta} _ {k} \sum_ {i = 1} ^ {n} x _ {i k} = \sum_ {i = 1} ^ {n} y _ {i} (3.9) \\ \hat {\beta} _ {0} \sum_ {i = 1} ^ {n} x _ {i 1} + \hat {\beta} _ {1} \sum_ {i = 1} ^ {n} x _ {i 1} ^ {2} + \hat {\beta} _ {2} \sum_ {i = 1} ^ {n} x _ {i 2} x _ {i 1} + \dots + \hat {\beta} _ {k} \sum_ {i = 1} ^ {n} x _ {i k} x _ {i 1} = \sum_ {i = 1} ^ {n} y _ {i} x _ {i 1} (3.10) \\ \hat {\beta} _ {0} \sum_ {i = 1} ^ {n} x _ {i k} + \hat {\beta} _ {1} \sum_ {i = 1} ^ {n} x _ {i 1} x _ {i k} + \hat {\beta} _ {2} \sum_ {i = 1} ^ {n} x _ {i 2} x _ {i k} + \dots + \hat {\beta} _ {k} \sum_ {i = 1} ^ {n} x _ {i k} ^ {2} = \sum_ {i = 1} ^ {n} y _ {i} x _ {i k} \\ \end{array}
$$

These equations are called the least squares normal equations. Note that there are $p = k + 1$ normal equations, one for each of the unknown regression coef-cients. The solutions to the normal equations will be the least squares estimators of the model regression coef-cients.

It is simpler to solve the normal equations if they are expressed in matrix notation. We now give a matrix development of the normal equations that parallels the development of Eq. (3.10). The multiple linear regression model may be written in matrix notation as

$$
\mathbf {y} = \mathbf {X} \boldsymbol {\beta} + \varepsilon , \tag {3.11}
$$

where

$$
\mathbf {y} = \left[ \begin{array}{l} y _ {1} \\ y _ {2} \\ \vdots \\ y _ {n} \end{array} \right], \quad \mathbf {X} = \left[ \begin{array}{l l l l} 1 & x _ {1 1} & x _ {1 2} \dots & x _ {1 k} \\ 1 & x _ {2 1} & x _ {2 2} \dots & x _ {2 k} \\ \vdots & \vdots & \vdots & \vdots \\ 1 & x _ {n 1} & x _ {n 2} \dots & x _ {n k} \end{array} \right], \quad \boldsymbol {\beta} = \left[ \begin{array}{l} \beta_ {0} \\ \beta_ {1} \\ \vdots \\ \beta_ {k} \end{array} \right], \quad \mathrm {a n d} \quad \boldsymbol {\varepsilon} = \left[ \begin{array}{l} \varepsilon_ {1} \\ \varepsilon_ {2} \\ \vdots \\ \varepsilon_ {n} \end{array} \right]
$$

In general, $\mathbf { y }$ is an $( n \times 1 )$ vector of the observations, $\mathbf { X }$ is an $( n \times p )$ matrix of the levels of the regressor variables, $\beta$ is a $( p \times 1 )$ vector of the regression

coef-cients, and $\varepsilon$ is an $( n \times 1 )$ vector of random errors. X is usually called the model matrix, because it is the original data table for the problem expanded to the form of the regression model that you desire to -t.

The vector of least squares estimators minimizes

$$
L = \sum_ {i = 1} ^ {n} \varepsilon_ {i} ^ {2} = \varepsilon^ {\prime} \varepsilon = (\mathbf {y} - \mathbf {X} \boldsymbol {\beta}) ^ {\prime} (\mathbf {y} - \mathbf {X} \boldsymbol {\beta})
$$

We can expand the right-hand side of $L$ and obtain

$$
L = \mathbf {y} ^ {\prime} \mathbf {y} - \beta^ {\prime} \mathbf {X} ^ {\prime} \mathbf {y} - \mathbf {y} ^ {\prime} \mathbf {X} \boldsymbol {\beta} + \beta^ {\prime} \mathbf {X} ^ {\prime} \mathbf {X} \boldsymbol {\beta} = \mathbf {y} ^ {\prime} \mathbf {y} - 2 \beta^ {\prime} \mathbf {X} ^ {\prime} \mathbf {y} + \beta^ {\prime} \mathbf {X} ^ {\prime} \mathbf {X} \boldsymbol {\beta},
$$

because $\beta ^ { \prime } \mathbf { X } ^ { \prime } \mathbf { y }$ is a $( 1 \times 1 )$ matrix, or a scalar, and its transpose $( \beta ^ { \prime } \mathbf { X } ^ { \prime } \mathbf { y } ) ^ { \prime } =$ $\mathbf { y } ^ { \prime } \mathbf { X } \beta$ is the same scalar. The least squares estimators must satisfy

$$
\left. \frac {\partial L}{\partial \boldsymbol {\beta}} \right| _ {\hat {\boldsymbol {\beta}}} = - 2 \mathbf {X} ^ {\prime} \mathbf {y} + 2 (\mathbf {X} ^ {\prime} \mathbf {X}) \hat {\boldsymbol {\beta}} = \mathbf {0},
$$

which simpli-es to

$$
\left(\mathbf {X} ^ {\prime} \mathbf {X}\right) \hat {\boldsymbol {\beta}} = \mathbf {X} ^ {\prime} \mathbf {y} \tag {3.12}
$$

In Eq. (3.12) $\mathbf { X } ^ { \prime } \mathbf { X }$ is a $( p \times p )$ symmetric matrix and $\mathbf { X ^ { \prime } y }$ is a $( p \times$ 1) column vector. Equation (3.12) is just the matrix form of the least squares normal equations. It is identical to Eq. (3.10). To solve the normal equations, multiply both sides of Eq. (3.12) by the inverse of $\mathbf { X } ^ { \prime } \mathbf { X }$ (we assume that this inverse exists). Thus the least squares estimator of $\hat { \boldsymbol { \beta } }$ is

$$
\hat {\boldsymbol {\beta}} = \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\prime} \mathbf {y} \tag {3.13}
$$

The -tted values of the response variable from the regression model are computed from

$$
\hat {\mathbf {y}} = \mathbf {X} \hat {\boldsymbol {\beta}} \tag {3.14}
$$

or in scalar notation,

$$
\hat {y} _ {i} = \hat {\beta} _ {0} + \hat {\beta} _ {1} x _ {i 1} + \hat {\beta} _ {2} x _ {i 2} + \dots + \hat {\beta} _ {k} x _ {i k}, i = 1, 2, \ldots , n (3. 1 5)
$$

The difference between the actual observation $y _ { i }$ and the corresponding -tted value is the residual $e _ { i } = y _ { i } - \hat { y } _ { i } , i = 1 , 2 , \ldots , n$ . The $n$ residuals can be written as an $( n \times 1 )$ vector denoted by

$$
\mathbf {e} = \mathbf {y} - \hat {\mathbf {y}} = \mathbf {y} - \mathbf {X} \hat {\boldsymbol {\beta}} \tag {3.16}
$$

In addition to estimating the regression coef-cients $\beta _ { 0 } , \beta _ { 1 } , \ldots , \beta _ { k }$ , it is also necessary to estimate the variance of the model errors, $\sigma ^ { 2 }$ . The estimator of this parameter involves the sum of squares of the residuals

$$
S S _ {\mathrm {E}} = (\mathbf {y} - \mathbf {X} \hat {\boldsymbol {\beta}}) ^ {\prime} (\mathbf {y} - \mathbf {X} \hat {\boldsymbol {\beta}})
$$

We can show that $E ( S S _ { \mathrm { E } } ) = ( n - p ) \sigma ^ { 2 }$ , so the estimator of $\sigma ^ { 2 }$ is the residual or mean square error

$$
\hat {\sigma} ^ {2} = \frac {S S _ {\mathrm {E}}}{n - p} \tag {3.17}
$$

The method of least squares is not the only way to estimate the parameters in a linear regression model, but it is widely used, and it results in estimates of the model parameters that have nice properties. If the model is correct (it has the right form and includes all of the relevant predictors), the least squares estimator $\hat { \boldsymbol { \beta } }$ is an unbiased estimator of the model parameters $\beta$ ; that is,

$$
E (\hat {\boldsymbol {\beta}}) = \boldsymbol {\beta}.
$$

The variances and covariances of the estimators $\hat { \boldsymbol { \beta } }$ are contained in a $( p \times p )$ covariance matrix

$$
\operatorname {V a r} (\hat {\boldsymbol {\beta}}) = \sigma^ {2} \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \tag {3.18}
$$

The variances of the regression coef-cients are on the main diagonal of this matrix and the covariances are on the off-diagonals.

Example 3.1 A hospital is implementing a program to improve quality and productivity. As part of this program, the hospital is attempting to measure and evaluate patient satisfaction. Table 3.2 contains some of the data that have been collected for a random sample of 25 recently discharged patients. The 鈥渟everity鈥?variable is an index that measures the severity of

TABLE 3.2 Patient Satisfaction Survey Data   

<table><tr><td>Observation</td><td>Age (x1)</td><td>Severity (x2)</td><td>Satisfaction (y)</td></tr><tr><td>1</td><td>55</td><td>50</td><td>68</td></tr><tr><td>2</td><td>46</td><td>24</td><td>77</td></tr><tr><td>3</td><td>30</td><td>46</td><td>96</td></tr><tr><td>4</td><td>35</td><td>48</td><td>80</td></tr><tr><td>5</td><td>59</td><td>58</td><td>43</td></tr><tr><td>6</td><td>61</td><td>60</td><td>44</td></tr><tr><td>7</td><td>74</td><td>65</td><td>26</td></tr><tr><td>8</td><td>38</td><td>42</td><td>88</td></tr><tr><td>9</td><td>27</td><td>42</td><td>75</td></tr><tr><td>10</td><td>51</td><td>50</td><td>57</td></tr><tr><td>11</td><td>53</td><td>38</td><td>56</td></tr><tr><td>12</td><td>41</td><td>30</td><td>88</td></tr><tr><td>13</td><td>37</td><td>31</td><td>88</td></tr><tr><td>14</td><td>24</td><td>34</td><td>102</td></tr><tr><td>15</td><td>42</td><td>30</td><td>88</td></tr><tr><td>16</td><td>50</td><td>48</td><td>70</td></tr><tr><td>17</td><td>58</td><td>61</td><td>52</td></tr><tr><td>18</td><td>60</td><td>71</td><td>43</td></tr><tr><td>19</td><td>62</td><td>62</td><td>46</td></tr><tr><td>20</td><td>68</td><td>38</td><td>56</td></tr><tr><td>21</td><td>70</td><td>41</td><td>59</td></tr><tr><td>22</td><td>79</td><td>66</td><td>26</td></tr><tr><td>23</td><td>63</td><td>31</td><td>52</td></tr><tr><td>24</td><td>39</td><td>42</td><td>83</td></tr><tr><td>25</td><td>49</td><td>40</td><td>75</td></tr></table>

the patient鈥檚 illness, measured on an increasing scale (i.e., more severe illnesses have higher values of the index), and the response satisfaction is also measured on an increasing scale, with larger values indicating greater satisfaction.

We will -t a multiple linear regression model to the patient satisfaction data. The model is

$$
y = \beta_ {0} + \beta_ {1} x _ {1} + \beta_ {2} x _ {2} + \varepsilon ,
$$

where $y =$ patient satisfaction, $x _ { 1 } =$ patient age, and $x _ { 2 } =$ illness severity. To solve the least squares normal equations, we will need to set up the $\mathbf { X } ^ { \prime } \mathbf { X }$

matrix and the $\mathbf { X } ^ { \prime } \mathbf { y }$ vector. The model matrix X and observation vector y are

$$
\mathbf {X} = \left[ \begin{array}{l l l} 1 & 5 5 & 5 0 \\ 1 & 4 6 & 2 4 \\ 1 & 3 0 & 4 6 \\ 1 & 3 5 & 4 8 \\ 1 & 5 9 & 5 8 \\ 1 & 6 1 & 6 0 \\ 1 & 7 4 & 6 5 \\ 1 & 3 8 & 4 2 \\ 1 & 2 7 & 4 2 \\ 1 & 5 1 & 5 0 \\ 1 & 5 3 & 3 8 \\ 1 & 4 1 & 3 0 \\ 1 & 3 7 & 3 1 \\ 1 & 2 4 & 3 4 \\ 1 & 4 2 & 3 0 \\ 1 & 5 0 & 4 8 \\ 1 & 5 8 & 6 1 \\ 1 & 6 0 & 7 1 \\ 1 & 6 2 & 6 2 \\ 1 & 6 8 & 3 8 \\ 1 & 7 0 & 4 1 \\ 1 & 7 9 & 6 6 \\ 1 & 6 3 & 3 1 \\ 1 & 3 9 & 4 2 \\ 1 & 4 9 & 4 0 \end{array} \right],
$$

$$
\mathbf {y} = \left[ \begin{array}{l} 6 8 \\ 7 7 \\ 9 6 \\ 8 0 \\ 4 3 \\ 4 4 \\ 2 6 \\ 8 8 \\ 7 5 \\ 5 7 \\ 5 6 \\ 8 8 \\ 8 8 \\ 1 0 2 \\ 8 8 \\ 7 0 \\ 5 2 \\ 4 3 \\ 4 6 \\ 5 6 \\ 5 9 \\ 2 6 \\ 5 2 \\ 8 3 \\ 7 5 \end{array} \right]
$$

The $\mathbf { X } ^ { \prime } \mathbf { X }$ matrix and the $\mathbf { X ^ { \prime } y }$ vector are

$$
\mathbf {X} ^ {\prime} \mathbf {X} = \left[ \begin{array}{l l l l} 1 & 1 & \dots & 1 \\ 5 5 & 4 6 & \dots & 4 9 \\ 5 0 & 2 4 & \dots & 4 0 \end{array} \right] \left[ \begin{array}{l l l} 1 & 5 5 & 5 0 \\ 1 & 4 6 & 2 4 \\ \vdots & \vdots & \vdots \\ 1 & 4 9 & 4 0 \end{array} \right] = \left[ \begin{array}{l l l} 2 5 & 1 2 7 1 & 1 1 4 8 \\ 1 2 7 1 & 6 9 8 8 1 & 6 0 8 1 4 \\ 1 1 4 8 & 6 0 8 1 4 & 5 6 7 9 0 \end{array} \right]
$$

and

$$
\mathbf {X} ^ {\prime} \mathbf {y} = \left[ \begin{array}{l l l l} 1 & 1 & \dots & 1 \\ 5 5 & 4 6 & \dots & 4 9 \\ 5 0 & 2 4 & \dots & 4 0 \end{array} \right] \left[ \begin{array}{l} 6 8 \\ 7 7 \\ \vdots \\ 7 5 \end{array} \right] = \left[ \begin{array}{l} 1 6 3 8 \\ 7 6 4 8 7 \\ 7 0 4 2 6 \end{array} \right]
$$

Using Eq. (3.13), we can -nd the least squares estimates of the parameters in the regression model as

$$
\begin{array}{l} \hat {\boldsymbol {\beta}} = \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\prime} \mathbf {y} \\ = \left[ \begin{array}{c c c} 2 5 & 1 2 7 1 & 1 1 4 8 \\ 1 2 7 1 & 6 9 8 8 1 & 6 0 8 1 4 \\ 1 1 4 8 & 6 0 8 1 4 & 5 6 7 9 0 \end{array} \right] ^ {- 1} \left[ \begin{array}{c} 1 6 3 8 \\ 7 6 4 8 7 \\ 7 0 4 2 6 \end{array} \right] \\ = \left[ \begin{array}{c c c} 0. 6 9 9 9 4 6 0 9 7 & - 0. 0 0 6 1 2 8 0 8 6 & - 0. 0 0 7 5 8 6 9 8 2 \\ - 0. 0 0 6 1 2 8 0 8 6 & 0. 0 0 0 2 6 3 8 3 & - 0. 0 0 0 1 5 8 6 4 6 \\ - 0. 0 0 7 5 8 6 9 8 2 & - 0. 0 0 0 1 5 8 6 4 6 & 0. 0 0 0 3 4 0 8 6 6 \end{array} \right] \left[ \begin{array}{l} 1 6 3 8 \\ 7 6 4 8 7 \\ 7 0 4 2 6 \end{array} \right] \\ = \left[ \begin{array}{l} 1 4 3. 4 7 2 0 1 1 8 \\ - 1. 0 3 1 0 5 3 4 1 4 \\ - 0. 5 5 6 0 3 7 8 1 \end{array} \right] \\ \end{array}
$$

Therefore the regression model is

$$
\hat {y} = 1 4 3. 4 7 2 - 1. 0 3 1 x _ {1} - 0. 5 5 6 x _ {2},
$$

where $x _ { 1 } =$ patient age and $x _ { 2 } =$ severity of illness, and we have reported the regression coef-cients to three decimal places.

Table 3.3 shows the output from the JMP regression routine for the patient satisfaction data. At the top of the table JMP displays a plot of the actual satisfaction data points versus the -tted values from the regression. If the -t is 鈥減erfect鈥?then the actual-predicted and the plotted points would lie on a straight $4 5 ^ { \circ }$ line. The points do seem to scatter closely along the $4 5 ^ { \circ }$ line, suggesting that the model is a reasonably good -t to the data. Note that, in addition to the -tted regression model, JMP provides a list of the residuals computed from Eq. (3.16) along with other output that will provide information about the quality of the regression model. This output will be explained in subsequent sections, and we will frequently refer back to Table 3.3.

Example 3.2 Trend Adjustment One way to forecast time series data that contain a linear trend is with a trend adjustment procedure. This involves -tting a model with a linear trend term in time, subtracting the -tted values from the original observations to obtain a set of residuals that are trend-free, then forecast the residuals, and compute the forecast by adding the forecast of the residual value(s) to the estimate of trend. We

# Actual by Predicted Plot

![](images/8fbd335a08be126d926e1f3f02a45268eb1bb0a2a68541e1851a20ec50bcb24a.jpg)  
P < .0001 RSq = 0.90 RMSE $=$ 7.1177

# Summary of Fit

RSquare 0.896593

RSquare Adj 0.887192

Root mean square error 7.117667

Mean of response 65.52

Observations (or Sum Wgts) 25

# Analysis of Variance

TABLE 3.3 JMP Output for the Patient Satisfaction Data in Table 3.2   

<table><tr><td>Source</td><td>DF</td><td>Sum of Squares</td><td>Mean Square</td><td>F Ratio</td></tr><tr><td>Model</td><td>2</td><td>9663.694</td><td>4831.85</td><td>95.3757</td></tr><tr><td>Error</td><td>22</td><td>1114.546</td><td>50.66</td><td>Prob &gt; F</td></tr><tr><td>C. Total</td><td>24</td><td>10778.240</td><td></td><td>&lt;.0001*</td></tr></table>

# Parameter Estimates

<table><tr><td>Term</td><td>Estimate</td><td>Std Error</td><td>t Ratio</td><td>Prob&gt; |t|</td></tr><tr><td>Intercept</td><td>143.47201</td><td>5.954838</td><td>24.09</td><td>&lt;.0001*</td></tr><tr><td>Age</td><td>-1.031053</td><td>0.115611</td><td>-8.92</td><td>&lt;.0001*</td></tr><tr><td>Severity</td><td>-0.556038</td><td>0.13141</td><td>-4.23</td><td>0.0003*</td></tr></table>

(continued)

TABLE 3.3 (Continued)   

<table><tr><td>Observation</td><td>Age</td><td>Severity</td><td>Satisfaction</td><td>Residual</td></tr><tr><td>1</td><td>55</td><td>50</td><td>68</td><td>9.03781647</td></tr><tr><td>2</td><td>46</td><td>24</td><td>77</td><td>-5.6986473</td></tr><tr><td>3</td><td>30</td><td>46</td><td>96</td><td>9.03732988</td></tr><tr><td>4</td><td>35</td><td>48</td><td>80</td><td>-0.6953274</td></tr><tr><td>5</td><td>59</td><td>58</td><td>43</td><td>-7.3896674</td></tr><tr><td>6</td><td>61</td><td>60</td><td>44</td><td>-3.2154849</td></tr><tr><td>7</td><td>74</td><td>65</td><td>26</td><td>-5.0316015</td></tr><tr><td>8</td><td>38</td><td>42</td><td>88</td><td>7.06160595</td></tr><tr><td>9</td><td>27</td><td>42</td><td>75</td><td>-17.279982</td></tr><tr><td>10</td><td>51</td><td>50</td><td>57</td><td>-6.0863972</td></tr><tr><td>11</td><td>53</td><td>38</td><td>56</td><td>-11.696744</td></tr><tr><td>12</td><td>41</td><td>30</td><td>88</td><td>3.48231247</td></tr><tr><td>13</td><td>37</td><td>31</td><td>88</td><td>-0.0858634</td></tr><tr><td>14</td><td>24</td><td>34</td><td>102</td><td>2.17855567</td></tr><tr><td>15</td><td>42</td><td>30</td><td>88</td><td>4.51336588</td></tr><tr><td>16</td><td>50</td><td>48</td><td>70</td><td>4.77047378</td></tr><tr><td>17</td><td>58</td><td>61</td><td>52</td><td>2.24739262</td></tr><tr><td>18</td><td>60</td><td>71</td><td>43</td><td>0.86987755</td></tr><tr><td>19</td><td>62</td><td>62</td><td>46</td><td>0.92764409</td></tr><tr><td>20</td><td>68</td><td>38</td><td>56</td><td>3.76905713</td></tr><tr><td>21</td><td>70</td><td>41</td><td>59</td><td>10.4992774</td></tr><tr><td>22</td><td>79</td><td>66</td><td>26</td><td>0.67970337</td></tr><tr><td>23</td><td>63</td><td>31</td><td>52</td><td>-9.2784746</td></tr><tr><td>24</td><td>39</td><td>42</td><td>83</td><td>3.09265936</td></tr><tr><td>25</td><td>49</td><td>40</td><td>75</td><td>4.29111788</td></tr></table>

described and illustrated trend adjustment in Section 2.4.2, and the basic trend adjustment model introduced there was

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \varepsilon , \quad t = 1, 2, \dots , T.
$$

The least squares normal equations for this model are

$$
T \hat {\beta} _ {0} + \hat {\beta} _ {1} \frac {T (T + 1)}{2} = \sum_ {t = 1} ^ {T} y _ {t}
$$

$$
\hat {\beta} _ {0} \frac {T (T + 1)}{2} + \hat {\beta} _ {1} \frac {T (T + 1) (2 T + 1)}{6} = \sum_ {t = 1} ^ {T} t y _ {t}
$$

Because there are only two parameters, it is easy to solve the normal equations directly, resulting in the least squares estimators

$$
\hat {\beta} _ {0} = \frac {2 (2 T + 1)}{T (T - 1)} \sum_ {t = 1} ^ {T} y _ {t} - \frac {6}{T (T - 1)} \sum_ {t = 1} ^ {T} t y _ {t}
$$

$$
\hat {\beta} _ {1} = \frac {1 2}{T (T ^ {2} - 1)} \sum_ {t = 1} ^ {T} t y _ {t} - \frac {6}{T (T - 1)} \sum_ {t = 1} ^ {T} y _ {t}
$$

Minitab computes these parameter estimates in its trend adjustment procedure, which we illustrated in Example 2.6. The least squares estimates obtained from this trend adjustment model depend on the point in time at which they were computed, that is, T. Sometimes it may be convenient to keep track of the period of computation and denote the estimates as functions of time, say, $\hat { \beta } _ { 0 } ( T )$ and $\hat { \beta } _ { 1 } ( T )$ . The model can be used to predict the next observation by predicting the point on the trend line in period $T + 1$ , which is $\hat { \beta } _ { 0 } ( T ) \stackrel { \cdot } { + } \hat { \beta } _ { 1 } ( T ) ( T + 1 )$ , and adding to the trend a forecast of the next residual, say, $\hat { \boldsymbol e } _ { T + 1 } ( 1 )$ . If the residuals are structureless and have average value zero, the forecast of the next residual would be zero. Then the forecast of the next observation would be

$$
\hat {y} _ {T + 1} (T) = \hat {\beta} _ {0} (T) + \hat {\beta} _ {1} (T) (T + 1)
$$

When a new observation becomes available, the parameter estimates $\hat { \beta } _ { 0 } ( T )$ and $\hat { \beta } _ { 1 } ( T )$ could be updated to reect the new information. This could be done by solving the normal equations again. In some situations it is possible to devise simple updating equations so that new estimates $\hat { \beta } _ { 0 } ( T + 1 )$ and $\hat { \beta } _ { 1 } ( T + 1 )$ can be computed directly from the previous ones $\hat { \beta } _ { 0 } ( T )$ and $\hat { \beta } _ { 1 } ( T )$ without having to directly solve the normal equations. We will show how to do this later.

# 3.3 STATISTICAL INFERENCE IN LINEAR REGRESSION

In linear regression problems, certain tests of hypotheses about the model parameters and con-dence interval estimates of these parameters are helpful in measuring the usefulness of the model. In this section, we describe several important hypothesis-testing procedures and a con-dence interval estimation procedure. These procedures require that the errors $\varepsilon _ { i }$ in the model are normally and independently distributed with mean zero

and variance $\sigma ^ { 2 }$ , abbreviated ${ \mathrm { N I D } } ( 0 , \sigma ^ { 2 } )$ . As a result of this assumption, the observations $y _ { i }$ are normally and independently distributed with mean $\begin{array} { r } { \beta _ { 0 } + \sum _ { j = 1 } ^ { k } \beta _ { j } x _ { i j } } \end{array}$ and variance $\sigma ^ { 2 }$ .

# 3.3.1 Test for Significance of Regression

The test for signi-cance of regression is a test to determine whether there is a linear relationship between the response variable $y$ and a subset of the predictor or regressor variables $x _ { 1 } , x _ { 2 } , \ldots , x _ { k }$ . The appropriate hypotheses are

$$
\begin{array}{l} \begin{array}{l} H _ {0}: \beta_ {1} = \beta_ {2} = \dots = \beta_ {k} = 0 \\ H _ k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: k + 1: \end{array} \tag {3.19} \\ H _ {1}: \text {a t l e a s t o n e} \beta_ {j} \neq 0 \\ \end{array}
$$

Rejection of the null hypothesis $H _ { 0 }$ in Eq. (3.19) implies that at least one of the predictor variables $x _ { 1 } , x _ { 2 } , \ldots , x _ { k }$ contributes signi-cantly to the model. The test procedure involves an analysis of variance partitioning of the total sum of squares

$$
S S _ {\mathrm {T}} = \sum_ {i = 1} ^ {n} \left(y _ {i} - \bar {y}\right) ^ {2} \tag {3.20}
$$

into a sum of squares due to the model (or to regression) and a sum of squares due to residual (or error), say,

$$
S S _ {\mathrm {T}} = S S _ {\mathrm {R}} + S S _ {\mathrm {E}} \tag {3.21}
$$

Now if the null hypothesis in Eq. (3.19) is true and the model errors are normally and independently distributed with constant variance as assumed, then the test statistic for signi-cance of regression is

$$
F _ {0} = \frac {S S _ {\mathrm {R}} / k}{S S _ {\mathrm {E}} / (n - p)} \tag {3.22}
$$

and one rejects $H _ { 0 }$ if the test statistic $F _ { 0 }$ exceeds the upper tail point of the $F$ distribution with $k$ numerator degrees of freedom and $n - p$ denominator degrees of freedom, $F _ { \alpha , k , n - p }$ . Table A.4 in Appendix A contains these upper tail percentage points of the $F$ distribution.

Alternatively, we could use the $P$ -value approach to hypothesis testing and thus reject the null hypothesis if the $P$ -value for the statistic $F _ { 0 }$ is

TABLE 3.4 Analysis of Variance for Testing Signi-cance of Regression   

<table><tr><td>Source of Variation</td><td>Sum of Squares</td><td>Degrees of Freedom</td><td>Mean Square</td><td>Test Statistic, F0</td></tr><tr><td>Regression</td><td>SSR</td><td>k</td><td>SSR/k</td><td>F0 = SSR/k/SSR/(n-p)</td></tr><tr><td>Residual (error)</td><td>SSE</td><td>n-p</td><td>SSE/n-p</td><td></td></tr><tr><td>Total</td><td>SST</td><td>n-1</td><td></td><td></td></tr></table>

less than $\alpha$ . The quantities in the numerator and denominator of the test statistic $F _ { 0 }$ are called mean squares. Recall that the mean square for error or residual estimates $\sigma ^ { 2 }$ .

The test for signi-cance of regression is usually summarized in an analysis of variance (ANOVA) table such as Table 3.4. Computational formulas for the sums of squares in the ANOVA are

$$
S S _ {\mathrm {T}} = \sum_ {i = 1} ^ {n} (y _ {i} - \bar {y}) ^ {2} = \mathbf {y} ^ {\prime} \mathbf {y} - n \bar {y} ^ {2}
$$

$$
S S _ {\mathrm {R}} = \hat {\boldsymbol {\beta}} ^ {\prime} \mathrm {X} ^ {\prime} \mathrm {y} - n \bar {\mathrm {y}} ^ {2} \tag {3.23}
$$

$$
S S _ {\mathrm {E}} = \mathbf {y} ^ {\prime} \mathbf {y} - \hat {\boldsymbol {\beta}} ^ {\prime} \mathbf {X} ^ {\prime} \mathbf {y}
$$

Regression model ANOVA computations are almost always performed using a computer software package. The JMP output in Table 3.3 shows the ANOVA test for signi-cance of regression for the regression model for the patient satisfaction data. The hypotheses in this problem are

$$
H _ {0}: \beta_ {1} = \beta_ {2} = 0
$$

$$
H _ {1}: \text {a t l e a s t o n e} \beta_ {j} \neq 0
$$

The reported value of the $F$ -statistic from Eq. (3.22) is

$$
F _ {0} = \frac {9 6 6 3 . 6 9 4 / 2}{1 1 1 4 . 5 4 6 / 2 2} = \frac {4 8 3 1 . 8 5}{5 0 . 6 6} = 9 5. 3 8.
$$

and the $P$ -value is reported as ${ < } 0 . 0 0 0 1$ . The actual $P$ -value is approximately $1 . 4 4 \times 1 0 ^ { - 1 1 }$ , a very small value, so there is strong evidence to reject the

null hypothesis and we conclude that either patient age or severity are useful predictors for patient satisfaction.

Table 3.3 also reports the coef-cient of multiple determination $R ^ { 2 }$ , -rst introduced in Section 2.6.2 in the context of choosing between competing forecasting models. Recall that

$$
R ^ {2} = \frac {S S _ {\mathrm {R}}}{S S _ {\mathrm {T}}} = 1 - \frac {S S _ {\mathrm {E}}}{S S _ {\mathrm {T}}} \tag {3.24}
$$

For the regression model for the patient satisfaction data, we have

$$
R ^ {2} = \frac {S S _ {\mathrm {R}}}{S S _ {\mathrm {T}}} = \frac {9 6 6 3 . 6 9 4}{1 0 7 7 8 . 2 4} = 0. 8 9 6 6
$$

So this model explains about $8 9 . 7 \%$ of the variability in the data.

The statistic $R ^ { 2 }$ is a measure of the amount of reduction in the variability of y obtained by using the predictor variables $x _ { 1 } , x _ { 2 } , \ldots , x _ { k }$ in the model. It is a measure of how well the regression model -ts the data sample. However, as noted in Section 2.6.2, a large value of $R ^ { 2 }$ does not necessarily imply that the regression model is a good one. Adding a variable to the model will never cause a decrease in $R ^ { 2 }$ , even in situations where the additional variable is not statistically signi-cant. In almost all cases, when a variable is added to the regression model $R ^ { 2 }$ increases. As a result, over reliance on $R ^ { 2 }$ as a measure of model adequacy often results in over-tting; that is, putting too many predictors in the model. In Section 2.6.2 we introduced the adjusted $R ^ { 2 }$ statistic

$$
R _ {\text {A d j}} ^ {2} = 1 - \frac {S S _ {\mathrm {E}} / (n - p)}{S S _ {\mathrm {T}} / (n - 1)} \tag {3.25}
$$

In general, the adjusted $R ^ { 2 }$ statistic will not always increase as variables are added to the model. In fact, if unnecessary regressors are added, the value of the adjusted $R ^ { 2 }$ statistic will often decrease. Consequently, models with a large value of the adjusted $R ^ { 2 }$ statistic are usually considered good regression models. Furthermore, the regression model that maximizes the adjusted $R ^ { 2 }$ statistic is also the model that minimizes the residual mean square.

JMP reports both $R ^ { 2 }$ and $R _ { \mathrm { A d j } } ^ { 2 }$ in Table 3.4. The value of $R ^ { 2 } = 0 . 8 9 7$ (or $8 9 . 7 \%$ , and the adjusted $R ^ { 2 }$ statistic is

$$
\begin{array}{l} R _ {\text {A d j}} ^ {2} = 1 - \frac {S S _ {\mathrm {E}} / (n - p)}{S S _ {\mathrm {T}} / (n - 1)} \\ = 1 - \frac {1 1 1 4 . 5 4 6 / (2 5 - 3)}{1 0 7 7 8 . 2 4 / (2 5 - 1)} \\ = 0. 8 8 7. \\ \end{array}
$$

Both $R ^ { 2 }$ and $R _ { \mathrm { A d j } } ^ { 2 }$ are very similar, usually a good sign that the regression model does not contain unnecessary predictor variables. It seems reasonable to conclude that the regression model involving patient age and severity accounts for between about $8 8 \%$ and $90 \%$ of the variability in the patient satisfaction data.

# 3.3.2 Tests on Individual Regression Coefficients and Groups of Coefficients

Tests on Individual Regression Coefficients We are frequently interested in testing hypotheses on the individual regression coef-cients. These tests would be useful in determining the value or contribution of each predictor variable in the regression model. For example, the model might be more effective with the inclusion of additional variables or perhaps with the deletion of one or more of the variables already in the model.

Adding a variable to the regression model always causes the sum of squares for regression to increase and the error sum of squares to decrease. We must decide whether the increase in the regression sum of squares is suf-cient to warrant using the additional variable in the model. Furthermore, adding an unimportant variable to the model can actually increase the mean squared error, thereby decreasing the usefulness of the model.

The hypotheses for testing the signi-cance of any individual regression coef-cient, say, $\beta _ { j }$ , are

$$
\begin{array}{l} H _ {0}: \beta_ {j} = 0 \tag {3.26} \\ H _ {1}: \beta_ {j} \neq 0 \\ \end{array}
$$

If the null hypothesis $H _ { 0 } : \beta _ { j } = 0$ is not rejected, then this indicates that the predictor variable $x _ { j }$ can be deleted from the model.

The test statistic for this hypothesis is

$$
t _ {0} = \frac {\hat {\beta} _ {j}}{\sqrt {\hat {\sigma} ^ {2} C _ {j j}}}, \tag {3.27}
$$

where $C _ { j j }$ is the diagonal element of the $( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 }$ matrix corresponding to the regression coef-cient $\hat { \beta } _ { j }$ (in numbering the elements of the matrix $\mathbf { C } = ( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 }$ , it is necessary to number the -rst row and column as zero so that the -rst diagonal element $C _ { 0 0 }$ will correspond to the subscript number on the intercept). The null hypothesis $H _ { 0 } : \beta _ { j } = 0$ is rejected if the absolute value of the test statistic $| t _ { 0 } | > t _ { \alpha / 2 , n - p }$ , where $t _ { \alpha / 2 , n - p }$ is the upper 饾浖/2 percentage point of the $t$ distribution with $n - p$ degrees of freedom. Table A.3 in Appendix A contains these upper tail points of the $t$ distribution. A $P -$ -value approach could also be used. This $t \cdot$ -test is really a partial or marginal test because the regression coef-cient $\hat { \beta } _ { j }$ depends on all the other regressor variables $x _ { i } ( i \neq j )$ that are in the model.

The denominator of Eq. (3.27), $\sqrt { \hat { \sigma } ^ { 2 } C _ { j j } }$ , is usually called the standard error of the regression coef-cient. That is,

$$
s e \left(\hat {\beta} _ {j}\right) = \sqrt {\hat {\sigma} ^ {2} C _ {j j}}. \tag {3.28}
$$

Therefore an equivalent way to write the $t$ -test statistic in Eq. (3.27) is

$$
t _ {0} = \frac {\hat {\beta} _ {j}}{s e (\hat {\beta} _ {j})}. \tag {3.29}
$$

Most regression computer programs provide the $t$ -test for each model parameter. For example, consider Table 3.3, which contains the JMP output for Example 3.1. The upper portion of this table gives the least squares estimate of each parameter, the standard error, the $t$ statistic, and the corresponding $P$ -value. To illustrate how these quantities are computed, suppose that we wish to test the hypothesis that $x _ { 1 } =$ patient age contributes significantly to the model, given that $x _ { 2 } =$ severity is included in the regression equation. Stated formally, the hypotheses are

$$
\begin{array}{l} H _ {0}: \beta_ {1} = 0 \\ H _ {1}: \beta_ {1} \neq 0 \\ \end{array}
$$

The regression coef-cient for $x _ { 1 } =$ patient age is ${ \widehat { \beta } } _ { 1 } = - 1 . 0 3 1 1 .$ . The standard error of this estimated regression coef-cient is

$$
s e (\hat {\beta} _ {1}) = \sqrt {\hat {\sigma} ^ {2} C _ {1 1}} = \sqrt {(5 0 . 6 6) (0 . 0 0 0 2 6 3 8 3)} = 0. 1 1 5 6.
$$

which when rounded agrees with the JMP output. (Often manual calculations will differ slightly from those reported by the computer, because the computer carries more decimal places. For instance, in this example if the mean squared error is computed to four decimal places as $M S _ { E } = S S _ { E } / ( n - p ) = 1 1 1 4 . 5 4 6 / ( 2 5 - 3 ) = 5 0 . 6 6 1 2$ instead of the two places reported in the JMP output, and this value of the $M S _ { E }$ is used as the estimate $\hat { \sigma } ^ { 2 }$ in calculating the standard error, then the standard error of $\hat { \beta } _ { 1 }$ will match the JMP output.) The test statistic is computed from Eq. (3.29) as

$$
t _ {0} = \frac {\hat {\beta} _ {1}}{s e (\hat {\beta} _ {1})} = \frac {- 1 . 0 3 1 0 5 3}{0 . 1 1 5 6 1 1} = - 8. 9 2
$$

This is agrees with the results reported by JMP. Because the $P$ -value reported is small, we would conclude that patient age is statistically signi-cant; that is, it is an important predictor variable, given that severity is also in the model. Similarly, because the $t \cdot$ -test statistic for $x _ { 2 } =$ severity is large, we would conclude that severity is a signi-cant predictor, given that patient age is in the model.

Tests on Groups of Coefficients We may also directly examine the contribution to the regression sum of squares for a particular predictor, say, $x _ { j }$ , or a group of predictors, given that other predictors $x _ { i }$ $( i \neq j )$ are included in the model. The procedure for doing this is the general regression signi-cance test or, as it is more often called, the extra sum of squares method. This procedure can also be used to investigate the contribution of a subset involving several regressor or predictor variables to the model. Consider the regression model with $k$ regressor variables

$$
\mathbf {y} = \mathbf {X} \boldsymbol {\beta} + \varepsilon , \tag {3.30}
$$

where y is $( n \times 1 )$ , X is $( n \times p )$ , $\beta$ is $( p \times 1 )$ , $\varepsilon$ is $( n \times 1 )$ , and $p = k + 1$ . We would like to determine if a subset of the predictor variables $x _ { 1 } , x _ { 2 } , \ldots , x _ { r }$

$( r < k )$ contributes signi-cantly to the regression model. Let the vector of regression coef-cients be partitioned as follows:

$$
\beta = \left[ \begin{array}{c} \beta_ {1} \\ \beta_ {2} \end{array} \right],
$$

where $\beta _ { 1 }$ is $( r \times 1 )$ and $\beta _ { 2 }$ is $[ ( p - r ) \times 1 ]$ . We wish to test the hypotheses

$$
H _ {0}: \boldsymbol {\beta} _ {1} = \mathbf {0} \tag {3.31}
$$

$$
H _ {1}: \beta_ {1} \neq \mathbf {0}
$$

The model may be written as

$$
\mathbf {y} = \mathbf {X} \boldsymbol {\beta} + \varepsilon = \mathbf {X} _ {1} \boldsymbol {\beta} _ {1} + \mathbf {X} _ {2} \boldsymbol {\beta} _ {2} + \varepsilon , \tag {3.32}
$$

where $\mathbf { X } _ { 1 }$ represents the columns of $\mathbf { X }$ (or the predictor variables) associated with $\beta _ { 1 }$ and $\mathbf { X } _ { 2 }$ represents the columns of $\mathbf { X }$ (predictors) associated with $\beta _ { 2 }$ .

For the full model (including both $\beta _ { 1 }$ and $\beta _ { 2 }$ ), we know that ${ \hat { \boldsymbol { \beta } } } =$ ${ \bf ( X ^ { \prime } X ) } ^ { - 1 } { \bf X ^ { \prime } y }$ . Also, the regression sum of squares for all predictor variables including the intercept is

$$
S S _ {\mathrm {R}} (\boldsymbol {\beta}) = \hat {\boldsymbol {\beta}} ^ {\prime} \mathbf {X} ^ {\prime} \mathbf {y} \quad (p \text {d e g r e e s o f f r e e d o m}) \tag {3.33}
$$

and the estimate of $\sigma ^ { 2 }$ based on this full model is

$$
\hat {\sigma} ^ {2} = \frac {\mathbf {y} ^ {\prime} \mathbf {y} - \hat {\beta} ^ {\prime} \mathbf {X} ^ {\prime} \mathbf {y}}{n - p} \tag {3.34}
$$

$S S _ { \mathrm { R } } ( \beta )$ is called the regression sum of squares due to $\beta$ . To -nd the contribution of the terms in $\beta _ { 1 }$ to the regression, we -t the model assuming that the null hypothesis H $: \beta _ { 1 } = 0$ is true. The reduced model is found from Eq. (3.32) with $\beta _ { 1 } = \mathbf { 0 }$ :

$$
\mathbf {y} = \mathbf {X} _ {2} \boldsymbol {\beta} _ {2} + \varepsilon \tag {3.35}
$$

The least squares estimator of $\beta _ { 2 }$ is $\hat { \pmb { \beta } } _ { 2 } = ( \mathbf { X } ^ { \prime } { } _ { 2 } \mathbf { X } _ { 2 } ) ^ { - 1 } \mathbf { X } ^ { \prime } { } _ { 2 } \mathbf { y }$ and the regression sum of squares for the reduced model is

$$
S S _ {\mathrm {R}} \left(\boldsymbol {\beta} _ {2}\right) = \hat {\boldsymbol {\beta}} _ {2} ^ {\prime} \mathbf {X} _ {2} ^ {\prime} \mathbf {y} (p - r \text {d e g r e e s o f f r e e d o m}) \tag {3.36}
$$

The regression sum of squares due to $\beta _ { 1 }$ , given that $\beta _ { 2 }$ is already in the model is

$$
S S _ {\mathrm {R}} \left(\boldsymbol {\beta} _ {1} \mid \boldsymbol {\beta} _ {2}\right) = S S _ {\mathrm {R}} (\boldsymbol {\beta}) - S S _ {\mathrm {R}} \left(\boldsymbol {\beta} _ {2}\right) = \hat {\boldsymbol {\beta}} ^ {\prime} \mathbf {X} ^ {\prime} \mathbf {y} - \hat {\boldsymbol {\beta}} _ {2} ^ {\prime} \mathbf {X} _ {2} ^ {\prime} \mathbf {y} \tag {3.37}
$$

This sum of squares has $r$ degrees of freedom. It is the 鈥渆xtra sum of squares鈥?due to $\beta _ { 1 }$ . Note that $S S _ { \mathrm { R } } ( \beta _ { 1 } | \beta _ { 2 } )$ is the increase in the regression sum of squares due to including the predictor variables $x _ { 1 } , x _ { 2 } , \ldots , x _ { r }$ in the model. Now $S S _ { \mathrm { R } } ( \beta _ { 1 } | \ \beta _ { 2 } )$ is independent of the estimate of $\sigma ^ { 2 }$ based on the full model from Eq. (3.34), so the null hypothesis $H _ { 0 }$ : ${ \boldsymbol { \beta } } _ { 1 } = \mathbf { 0 }$ may be tested by the statistic

$$
F _ {0} = \frac {S S _ {\mathrm {R}} \left(\boldsymbol {\beta} _ {1} \mid \boldsymbol {\beta} _ {2}\right) / r}{\hat {\sigma} ^ {2}}, \tag {3.38}
$$

where $\hat { \sigma } ^ { 2 }$ is computed from Eq. (3.34). If $F _ { 0 } > F _ { \alpha , r , n - p }$ we reject $H _ { 0 }$ , concluding that at least one of the parameters in $\beta _ { 1 }$ is not zero, and, consequently, at least one of the predictor variables $x _ { 1 }$ ${ \mathfrak { s } } , x _ { 2 } , \ldots , x _ { r }$ in $\mathbf { X } _ { 1 }$ contributes signi-cantly to the regression model. A $P$ -value approach could also be used in testing this hypothesis. Some authors call the test in Eq. (3.38) a partial $F$ test.

The partial $F$ test is very useful. We can use it to evaluate the contribution of an individual predictor or regressor $x _ { j }$ as if it were the last variable added to the model by computing

$$
S S _ {\mathrm {R}} (\beta_ {j} | \boldsymbol {\beta} _ {i}; i \neq j)
$$

This is the increase in the regression sum of squares due to adding $x _ { j }$ to a model that already includes $x _ { 1 } , \ldots , x _ { j - 1 } , x _ { j + 1 } , \ldots , x _ { k } .$ $x _ { 1 } , \ldots , x _ { j - 1 }$ . The partial $F$ test on a single variable $x _ { j }$ is equivalent to the $t$ -test in Equation (3.27). The computed value of $F _ { 0 }$ will be exactly equal to the square of the $t \cdot$ -test statistic $t _ { 0 }$ . However, the partial $F$ test is a more general procedure in that we can evaluate simultaneously the contribution of more than one predictor variable to the model.

Example 3.3 To illustrate this procedure, consider again the patient satisfaction data from Table 3.2. Suppose that we wish to consider -tting a more elaborate model to this data; speci-cally, consider the second-order polynomial

$$
y = \beta_ {0} + \beta_ {1} x _ {1} + \beta_ {2} x _ {2} + \beta_ {1 2} x _ {1} x _ {2} + \beta_ {1 1} x _ {1} ^ {2} + \beta_ {2 2} x _ {2} ^ {2} + \varepsilon
$$

TABLE 3.5 JMP Output for the Second-Order Model for the Patient Satisfaction Data   

<table><tr><td colspan="5">Summary of Fit</td></tr><tr><td>RSquare</td><td>0.900772</td><td></td><td></td><td></td></tr><tr><td>RSquare Adj</td><td>0.874659</td><td></td><td></td><td></td></tr><tr><td>Root mean square error</td><td>7.502639</td><td></td><td></td><td></td></tr><tr><td>Mean of response</td><td>65.52</td><td></td><td></td><td></td></tr><tr><td>Observations (or Sum Wgts)</td><td>25</td><td></td><td></td><td></td></tr><tr><td colspan="5">Analysis of Variance</td></tr><tr><td></td><td></td><td rowspan="2">Sum of Squares</td><td rowspan="2">Mean Square</td><td rowspan="2">F Ratio</td></tr><tr><td>Source</td><td>DF</td></tr><tr><td>Model</td><td>5</td><td>9708.738</td><td>1941.75</td><td>34.4957</td></tr><tr><td>Error</td><td>19</td><td>1069.502</td><td>56.29</td><td>Prob &gt; F</td></tr><tr><td>C. Total</td><td>24</td><td>10,778.240</td><td></td><td>&lt;.0001*</td></tr><tr><td colspan="5">Parameter Estimates</td></tr><tr><td>Term</td><td>Estimate</td><td>Std Error</td><td>t Ratio</td><td>Prob&gt; |t|</td></tr><tr><td>Intercept</td><td>143.74009</td><td>6.774622</td><td>21.22</td><td>&lt;0.0001*</td></tr><tr><td>Age</td><td>-0.986524</td><td>0.135366</td><td>-7.29</td><td>&lt;0.0001*</td></tr><tr><td>Severity</td><td>-0.571637</td><td>0.158928</td><td>-3.60</td><td>0.0019*</td></tr><tr><td>(Severity-45.92)*(Age-50.84)</td><td>0.0064566</td><td>0.016546</td><td>0.39</td><td>0.7007</td></tr><tr><td>(Age-50.84)*(Age-50.84)</td><td>-0.00283</td><td>0.008588</td><td>-0.33</td><td>0.7453</td></tr><tr><td>(Severity-45.92)*(Severity-45.92)</td><td>-0.011368</td><td>0.013533</td><td>-0.84</td><td>0.4113</td></tr></table>

where $x _ { 1 } =$ patient age and $x _ { 2 } =$ severity. To -t the model, the model matrix would need to be expanded to include columns for the second-order terms $x _ { 1 } x _ { 2 } , x _ { 1 } ^ { 2 }$ , and $x _ { 2 } ^ { 2 }$ . The results of -tting this model using JMP are shown in Table 3.5.

Suppose that we want to test the signi-cance of the additional secondorder terms. That is, the hypotheses are

$$
H _ {0}: \beta_ {1 2} = \beta_ {1 1} = \beta_ {2 2} = 0
$$

$H _ { 1 }$ : at least one of the parameters $\beta _ { 1 2 } , \beta _ { 1 1 }$ , or $\beta _ { 2 2 } \neq 0$

In the notation used in this section, these second-order terms are the parameters in the vector $\beta _ { 1 }$ . Since the quadratic model is the full model, we can -nd $S S _ { \mathrm { R } } ( \beta )$ directly from the JMP output in Table 3.5 as

$$
S S _ {\mathrm {R}} (\boldsymbol {\beta}) = 9 7 0 8. 7 3 8
$$

with 5 degrees of freedom (because there are -ve predictors in this model). The reduced model is the model with all of the predictors in the vector $\beta _ { 1 }$ equal to zero. This reduced model is the original regression model that we -t to the data in Table 3.3. From Table 3.3, we can -nd the regression sum of squares for the reduced model as

$$
S S _ {\mathrm {R}} \left(\beta_ {2}\right) = 9 6 6 3. 6 9 4
$$

and this sum of squares has 2 degrees of freedom (the model has two predictors).

Therefore the extra sum of squares for testing the signi-cance of the quadratic terms is just the difference between the regression sums of squares for the full and reduced models, or

$$
\begin{array}{l} S S _ {\mathrm {R}} \left(\boldsymbol {\beta} _ {1} \mid \boldsymbol {\beta} _ {2}\right) = S S _ {\mathrm {R}} (\boldsymbol {\beta}) - S S _ {\mathrm {R}} \left(\boldsymbol {\beta} _ {2}\right) \\ = 9 7 0 8. 7 3 8 - 9 6 6 3. 6 9 4 \\ = 4 5. 0 4 4 \\ \end{array}
$$

with $5 - 2 = 3$ degrees of freedom. These three degrees of freedom correspond to the three additional terms in the second-order model. The test statistic from Eq. (3.38) is

$$
\begin{array}{l} F _ {0} = \frac {S S _ {\mathrm {R}} (\boldsymbol {\beta} _ {1} | \boldsymbol {\beta} _ {2}) / r}{\hat {\sigma} ^ {2}} \\ = \frac {4 5 . 0 4 4 / 3}{5 6 . 2 9} \\ = 0. 2 6 7. \\ \end{array}
$$

This $F$ -statistic is very small, so there is no evidence against the null hypothesis.

Furthermore, from Table 3.5, we observe that the individual $t$ -statistics for the second-order terms are very small and have large $P$ -values, so there is no reason to believe that the model would be improved by adding any of the second-order terms.

It is also interesting to compare the $R ^ { 2 }$ and $R _ { \mathrm { A d j } } ^ { 2 }$ statistics for the two models. From Table 3.3, we -nd that $R ^ { 2 } = 0 . 8 9 7$ and $R _ { \mathrm { A d j } } ^ { 2 } = 0 . 8 8 7$ for the original two-variable model, and from Table 3.5, we -nd that $R ^ { 2 } = 0 . 9 0 1$ and $R _ { \mathrm { A d j } } ^ { 2 } = 0 . 8 7 5$ for the quadratic model. Adding the quadratic terms caused the ordinary $R ^ { 2 }$ to increase slightly (it will never decrease when

additional predictors are inserted into the model), but the adjusted $R ^ { 2 }$ statistic decreased. This decrease in the adjusted $R ^ { 2 }$ is an indication that the additional variables did not contribute to the explanatory power of the model.

# 3.3.3 Confidence Intervals on Individual Regression Coefficients

It is often necessary to construct con-dence interval (CI) estimates for the parameters in a linear regression and for other quantities of interest from the regression model. The procedure for obtaining these con-dence intervals requires that we assume that the model errors are normally and independently distributed with mean zero and variance $\sigma ^ { 2 }$ , the same assumption made in the two previous sections on hypothesis testing.

Because the least squares estimator $\hat { \boldsymbol { \beta } }$ is a linear combination of the observations, it follows that $\hat { \boldsymbol { \beta } }$ is normally distributed with mean vector $\beta$ and covariance matrix $V ( \hat { \pmb { \beta } } ) = \sigma ^ { 2 } ( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 }$ .Then each of the statistics

$$
\frac {\hat {\beta} _ {j} - \beta_ {j}}{\sqrt {\hat {\sigma} ^ {2} C _ {j j}}}, \quad j = 0, 1, \dots , k \tag {3.39}
$$

is distributed as $t$ with $n - p$ degrees of freedom, where $C _ { j j }$ is the $( j j ) \mathrm { t h }$ element of the $( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 }$ matrix, and $\hat { \sigma } ^ { 2 }$ is the estimate of the error variance, obtained from Eq. (3.34). Therefore a $1 0 0 ( 1 - \alpha )$ percent con-dence interval for an individual regression coef-cient $\beta _ { j } , j = 0 , 1 , \ldots , k$ , is

$$
\hat {\beta} _ {j} - t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} C _ {j j}} \leq \beta_ {j} \leq \hat {\beta} _ {j} + t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} C _ {j j}}. \tag {3.40}
$$

This CI could also be written as

$$
\hat {\beta} _ {j} - t _ {\alpha / 2, n - p} s e (\hat {\beta} _ {j}) \leq \beta_ {j} \leq \hat {\beta} _ {j} + t _ {\alpha / 2, n - p} s e (\hat {\beta} _ {j})
$$

because $s e ( \hat { \beta } _ { j } ) = \sqrt { \hat { \sigma } ^ { 2 } C _ { j j } }$

Example 3.4 We will -nd a $9 5 \%$ CI on the regression for patient age in the patient satisfaction data regression model. From the JMP output in

Table 3.3, we -nd that $\hat { \beta } _ { 1 } = - 1 . 0 3 1 1$ and $s e ( \hat { \beta } _ { 1 } ) = 0 . 1 1 5 6$ . Therefore the $9 5 \%$ CI is

$$
\begin{array}{l} \hat {\beta} _ {j} - t _ {\alpha / 2, n - p} s e (\hat {\beta} _ {j}) \leq \beta_ {j} \leq \hat {\beta} _ {j} + t _ {\alpha / 2, n - p} s e (\hat {\beta} _ {j}) \\ - 1. 0 3 1 1 - (2. 0 7 4) (0. 1 1 5 6) \leq \beta_ {1} \leq - 1. 0 3 1 1 + (2. 0 7 4) (0. 1 1 5 6) \\ - 1. 2 7 0 9 \leq \beta_ {1} \leq - 0. 7 9 1 3. \\ \end{array}
$$

This con-dence interval does not include zero; this is equivalent to rejecting (at the 0.05 level of signi-cance) the null hypothesis that the regression coef-cient $\beta _ { 1 } = 0$ .

# 3.3.4 Confidence Intervals on the Mean Response

We may also obtain a con-dence interval on the mean response at a particular combination of the predictor or regressor variables, say, $x _ { 0 1 }$ $, x _ { 0 2 } , \ldots ,$ $x _ { 0 k }$ . We -rst de-ne a vector that represents this point expanded to model form. Since the standard multiple linear regression model contains the $k$ predictors and an intercept term, this vector is

$$
\mathbf {x} _ {0} = \left[ \begin{array}{c} 1 \\ x _ {0 1} \\ \vdots \\ x _ {0 k} \end{array} \right]
$$

The mean response at this point is

$$
E [ y (\mathbf {x} _ {0}) ] = \mu_ {y | \mathbf {x} _ {0}} = \mathbf {x} _ {0} ^ {\prime} \boldsymbol {\beta}.
$$

The estimator of the mean response at this point is found by substituting $\hat { \boldsymbol { \beta } }$ for $\beta$

$$
\hat {y} \left(\mathbf {x} _ {0}\right) = \hat {\mu} _ {y \mid \mathbf {x} _ {0}} = \mathbf {x} ^ {\prime} _ {0} \hat {\boldsymbol {\beta}} \tag {3.41}
$$

This estimator is normally distributed because $\hat { \boldsymbol { \beta } }$ is normally distributed and it is also unbiased because $\hat { \boldsymbol { \beta } }$ is an unbiased estimator of $\beta$ . The variance of $\hat { y } ( \mathbf x _ { 0 } )$ is

$$
\operatorname {V a r} \left[ \hat {y} \left(\mathbf {x} _ {0}\right) \right] = \sigma^ {2} \mathbf {x} _ {0} ^ {\prime} \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \mathbf {x} _ {0}. \tag {3.42}
$$

Therefore, a $1 0 0 ( 1 - \alpha )$ percent CI on the mean response at the point $x _ { 0 1 }$ , $x _ { 0 2 } , \ldots , x _ { 0 k }$ is

$$
\hat {y} \left(\mathbf {x} _ {0}\right) - t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} \mathbf {x} _ {0} ^ {\prime} \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \mathbf {x} _ {0}} \leq \mu_ {y | \mathbf {x} _ {0}} \leq \hat {y} \left(\mathbf {x} _ {0}\right) + t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} \mathbf {x} _ {0} ^ {\prime} \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \mathbf {x} _ {0}}, \tag {3.43}
$$

where $\hat { \sigma } ^ { 2 }$ is the estimate of the error variance, obtained from Eq. (3.34). Note that the length of this con-dence interval will depend on the location of the point $\mathbf { x } _ { 0 }$ through the term $\mathbf { x } ^ { \prime } { } _ { 0 } ( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 } \mathbf { x } _ { 0 }$ in the con-dence interval formula. Generally, the length of the CI will increase as the point $\mathbf { x } _ { 0 }$ moves further from the center of the predictor variable data.

The quantity

$$
\sqrt {\mathrm {V a r} [ \hat {y} (\mathbf {x} _ {0}) ]} = \sqrt {\hat {\sigma} ^ {2} \mathbf {x} _ {0} ^ {\prime} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {x} _ {0}}
$$

used in the con-dence interval calculations in Eq. (3.43) is sometimes called the standard error of the -tted response. JMP will calculate and display these standard errors for each individual observation in the sample used to -t the model and for other non-sample points of interest. The next-to-last column of Table 3.6 displays the standard error of the -tted response for the patient satisfaction data. These standard errors can be used to compute the CI in Eq. (3.43).

Example 3.5 Suppose that we want to -nd a con-dence interval on mean patient satisfaction for the point where $x _ { 1 } =$ patient $\tt a g e = 5 5$ and $x _ { 2 } = \mathrm { s e v e r i t y } = 5 0$ $= 5 0$ . This is the -rst observation in the sample, so refer to Table 3.6, the JMP output for the patient satisfaction regression model. For this observation, JMP reports that the 鈥淪E Fit鈥?is 1.51 rounded to two decimal places, or in our notation, $\sqrt { \mathrm { V a r } \left[ \hat { y } ( \mathbf { x } _ { 0 } ) \right] } = 1 . 5 1$ . Therefore, if we want to -nd a $9 5 \%$ CI on the mean patient satisfaction for the case where $x _ { 1 } =$ patient age $= 5 5$ and $x _ { 2 } = \mathrm { s e v e r i t y } = 5 0$ $= 5 0$ , we would proceed as follows:

$$
\hat {y} (\mathbf {x} _ {0}) - t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} \mathbf {x} _ {0} ^ {\prime} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {x} _ {0}} \leq \mu_ {y | \mathbf {x} _ {0}} \leq \hat {y} (\mathbf {x} _ {0}) + t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} \mathbf {x} _ {0} ^ {\prime} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {x} _ {0}}
$$

$$
5 8. 9 6 - 2. 0 7 4 (1. 5 1) \leq \mu_ {y | \mathbf {x} _ {0}} \leq 5 8. 9 6 + 2. 0 7 4 (1. 5 1)
$$

$$
5 5. 8 3 \leq \mu_ {y | \mathbf {x} _ {0}} \leq 6 2. 0 9.
$$

From inspection of Table 3.6, note that the standard errors for each observation are different. This reects the fact that the length of the CI on

TABLE 3.6 JMP Calculations of the Standard Errors of the Fitted Values and Predicted Responses for the Patient Satisfaction Data   

<table><tr><td>Observation</td><td>Age</td><td>Severity</td><td>Satisfaction</td><td>Predicted</td><td>Residual</td><td>SE (Fit)</td><td>SE (Predicted)</td></tr><tr><td>1</td><td>55</td><td>50</td><td>68</td><td>58.96218</td><td>9.037816</td><td>1.507444</td><td>7.275546</td></tr><tr><td>2</td><td>46</td><td>24</td><td>77</td><td>82.69865</td><td>-5.69865</td><td>2.988566</td><td>7.719631</td></tr><tr><td>3</td><td>30</td><td>46</td><td>96</td><td>86.96267</td><td>9.03733</td><td>2.803259</td><td>7.6498</td></tr><tr><td>4</td><td>35</td><td>48</td><td>80</td><td>80.69533</td><td>-0.69533</td><td>2.446294</td><td>7.526323</td></tr><tr><td>5</td><td>59</td><td>58</td><td>43</td><td>50.38967</td><td>-7.38967</td><td>1.962621</td><td>7.383296</td></tr><tr><td>6</td><td>61</td><td>60</td><td>44</td><td>47.21548</td><td>-3.21548</td><td>2.128407</td><td>7.429084</td></tr><tr><td>7</td><td>74</td><td>65</td><td>26</td><td>31.0316</td><td>-5.0316</td><td>2.89468</td><td>7.683772</td></tr><tr><td>8</td><td>38</td><td>42</td><td>88</td><td>80.93839</td><td>7.061606</td><td>1.919979</td><td>7.372076</td></tr><tr><td>9</td><td>27</td><td>42</td><td>75</td><td>92.27998</td><td>-17.28</td><td>2.895873</td><td>7.684221</td></tr><tr><td>10</td><td>51</td><td>50</td><td>57</td><td>63.0864</td><td>-6.0864</td><td>1.517813</td><td>7.277701</td></tr><tr><td>11</td><td>53</td><td>38</td><td>56</td><td>67.69674</td><td>-11.6967</td><td>1.856609</td><td>7.355826</td></tr><tr><td>12</td><td>41</td><td>30</td><td>88</td><td>84.51769</td><td>3.482312</td><td>2.275784</td><td>7.472641</td></tr><tr><td>13</td><td>37</td><td>31</td><td>88</td><td>88.08586</td><td>-0.08586</td><td>2.260863</td><td>7.468111</td></tr><tr><td>14</td><td>24</td><td>34</td><td>102</td><td>99.82144</td><td>2.178556</td><td>2.994326</td><td>7.721863</td></tr><tr><td>15</td><td>42</td><td>30</td><td>88</td><td>83.48663</td><td>4.513366</td><td>2.277152</td><td>7.473058</td></tr><tr><td>16</td><td>50</td><td>48</td><td>70</td><td>65.22953</td><td>4.770474</td><td>1.462421</td><td>7.266351</td></tr><tr><td>17</td><td>58</td><td>61</td><td>52</td><td>49.75261</td><td>2.247393</td><td>2.214287</td><td>7.454143</td></tr><tr><td>18</td><td>60</td><td>71</td><td>43</td><td>42.13012</td><td>0.869878</td><td>3.21204</td><td>7.808866</td></tr><tr><td>19</td><td>62</td><td>62</td><td>46</td><td>45.07236</td><td>0.927644</td><td>2.296</td><td>7.478823</td></tr><tr><td>20</td><td>68</td><td>38</td><td>56</td><td>52.23094</td><td>3.769057</td><td>3.038105</td><td>7.738945</td></tr><tr><td>21</td><td>70</td><td>41</td><td>59</td><td>48.50072</td><td>10.49928</td><td>2.97766</td><td>7.715416</td></tr><tr><td>22</td><td>79</td><td>66</td><td>26</td><td>25.3203</td><td>0.679703</td><td>3.24021</td><td>7.820495</td></tr><tr><td>23</td><td>63</td><td>31</td><td>52</td><td>61.27847</td><td>-9.27847</td><td>3.28074</td><td>7.837374</td></tr><tr><td>24</td><td>39</td><td>42</td><td>83</td><td>79.90734</td><td>3.092659</td><td>1.849178</td><td>7.353954</td></tr><tr><td>25</td><td>49</td><td>40</td><td>75</td><td>70.70888</td><td>4.291118</td><td>1.58171</td><td>7.291295</td></tr><tr><td>鈥?/td><td>75</td><td>60</td><td>鈥?/td><td>32.78074</td><td>鈥?/td><td>2.78991</td><td>7.644918</td></tr><tr><td>鈥?/td><td>60</td><td>60</td><td>鈥?/td><td>48.24654</td><td>鈥?/td><td>2.120899</td><td>7.426937</td></tr></table>

the mean response depends on the location of the observation. Generally, the standard error increases as the distance of the point from the center of the predictor variable data increases.

In the case where the point of interest $\mathbf { x } _ { 0 }$ is not one of the observations in the sample, it is necessary to calculate the standard error for that point $\sqrt { \mathrm { V a r } \left[ \hat { y } ( \mathbf { x } _ { 0 } ) \right] } = \sqrt { \hat { \sigma } ^ { 2 } \mathbf { x } ^ { \prime } { } _ { 0 } ( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 } \mathbf { x } _ { 0 } }$ , which involves -nding $\mathbf { x ^ { \prime } } _ { 0 } ( \mathbf { X ^ { \prime } X } ) ^ { - 1 } \mathbf { x } _ { 0 }$ for the observation $\mathbf { x } _ { 0 }$ . This is not too dif-cult (you can do it in Excel), but it is not necessary, because JMP will provide the CI at any point that you specify. For example, if you want to -nd a $9 5 \%$ CI on the mean patient satisfaction for the point where $x _ { 1 } =$ patient $\tt a g e = 6 0$ and $x _ { 2 } = \mathrm { s e v e r i t y } =$ $=$ 60 (this is not a sample observation), then in the last row of Table 3.6 JMP reports that the estimate of the mean patient satisfaction at the point $x _ { 1 } =$ patient age $= 6 0$ and x = severity $= 6 0$ as $\hat { y } ( \mathbf x _ { 0 } ) = 4 8 . 2 5$ , and the standard error of the -tted response as $\sqrt { \mathrm { V a r } [ \hat { y } ( \mathbf { x } _ { 0 } ) ] } = \sqrt { \hat { \sigma } ^ { 2 } \mathbf { x } ^ { \prime } { } _ { 0 } ( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 } \mathbf { x } _ { 0 } } = 2 . 1 2$ . Consequently, the $9 5 \%$ CI on the mean patient satisfaction at that point is

$$
4 3. 8 5 \leq \mu_ {y | \mathbf {x} _ {0}} \leq 5 2. 6 5.
$$

# 3.4 PREDICTION OF NEW OBSERVATIONS

A regression model can be used to predict future observations on the response $y$ corresponding to a particular set of values of the predictor or regressor variables, say, $x _ { 0 1 }$ , x02, 鈥?, $x _ { 0 k }$ . Let $\mathbf { x } _ { 0 }$ represent this point, expanded to model form. That is, if the regression model is the standard multiple regression model, then $\mathbf { x } _ { 0 }$ contains the coordinates of the point of interest and unity to account for the intercept term, so $\mathbf { x } _ { 0 } ^ { \prime } = [ 1 , x _ { 0 1 }$ , $x _ { 0 2 } , \ldots , x _ { 0 k } ]$ . A point estimate of the future observation $y ( \mathbf { x } _ { 0 } )$ at the point x , x , 鈥?, x is computed from $x _ { 0 1 } , x _ { 0 2 } , \ldots , x _ { 0 k }$

$$
\hat {y} (\mathbf {x} _ {0}) = \mathbf {x} _ {0} ^ {\prime} \hat {\boldsymbol {\beta}} \tag {3.44}
$$

The prediction error in using $\hat { y } ( \mathbf x _ { 0 } )$ to estimate $y ( \mathbf { x } _ { 0 } )$ is $y ( \mathbf x _ { 0 } ) - \hat { y } ( \mathbf x _ { 0 } )$ . Because $\hat { y } ( \mathbf x _ { 0 } )$ and $y ( \mathbf { x } _ { 0 } )$ are independent, the variance of this prediction error is

$$
\begin{array}{l} \mathrm {V a r} [ y (\mathbf {x} _ {0}) - \hat {y} (\mathbf {x} _ {0}) ] = \mathrm {V a r} [ y (\mathbf {x} _ {0}) ] + \mathrm {V a r} [ \hat {y} (\mathbf {x} _ {0}) ] = \sigma^ {2} + \sigma^ {2} \mathbf {x ^ {\prime}} _ {0} (\mathbf {X ^ {\prime} X}) ^ {- 1} \mathbf {x} _ {0} \\ = \sigma^ {2} \left[ 1 + {\bf x ^ {\prime}} _ {0} ({\bf X ^ {\prime}} {\bf X}) ^ {- 1} {\bf x} _ {0} \right]. (3. 4 5) \\ \end{array}
$$

If we use $\hat { \sigma } ^ { 2 }$ from Eq. (3.34) to estimate the error variance $\sigma ^ { 2 }$ , then the ratio

$$
\frac {y (\mathbf {x} _ {0}) - \hat {y} (\mathbf {x} _ {0})}{\sqrt {\hat {\sigma} ^ {2} \left[ 1 + \mathbf {x} _ {0} ^ {\prime} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {x} _ {0} \right]}}
$$

has a t distribution with $n - p$ degrees of freedom. Consequently, we can write the following probability statement:

$$
P \left(- t _ {\alpha / 2, n - p} \leq \frac {y (\mathbf {x} _ {0}) - \hat {y} (\mathbf {x} _ {0})}{\sqrt {\hat {\sigma} ^ {2} \left[ 1 + \mathbf {x} _ {0} ^ {\prime} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {x} _ {0} \right]}} \leq t _ {\alpha / 2, n - p}\right) = 1 - \alpha
$$

This probability statement can be rearranged as follows:

$$
P \left( \begin{array}{l} \hat {y} (\mathbf {x} _ {0}) - t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} [ 1 + \mathbf {x} _ {0} ^ {\prime} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {x} _ {0} ]} \leq y (\mathbf {x} _ {0}) \\ \leq \hat {y} (\mathbf {x} _ {0}) + t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} [ 1 + \mathbf {x} _ {0} ^ {\prime} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {x} _ {0} ]} \end{array} \right) = 1 - \alpha .
$$

Therefore, the probability is $1 - \alpha$ that the future observation falls in the interval

$$
\begin{array}{l} \hat {y} (\mathbf {x} _ {0}) - t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} \left[ 1 + \mathbf {x} _ {0} ^ {\prime} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {x} _ {0} \right]} \leq y (\mathbf {x} _ {0}) \\ \leq \hat {y} (\mathbf {x} _ {0}) + t _ {\alpha / 2, n - p} \sqrt {\hat {\sigma} ^ {2} \left[ 1 + \mathbf {x} _ {0} ^ {\prime} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {x} _ {0} \right]} \tag {3.46} \\ \end{array}
$$

This statement is called a $1 0 0 ( 1 - \alpha )$ percent prediction interval (PI) for the future observation $y ( \mathbf { x } _ { 0 } )$ at the point $x _ { 0 1 }$ , x02, 鈥?, $x _ { 0 k }$ . The expression in the square tool in Eq. (3.46) is often called the standard error of the predicted response.

The PI formula in Eq. (3.46) looks very similar to the formula for the CI on the mean, Eq. (3.43). The difference is the 鈥?鈥?in the variance of the prediction error under the square root. This will make PI longer than the corresponding CI at the same point. It is reasonable that the PI should be longer, as the CI is an interval estimate on the mean of the response distribution at a speci-c point, while the PI is an interval estimate on a single future observation from the response distribution at that point. There should be more variability associated with an individual observation

than with an estimate of the mean, and this is reected in the additional length of the PI.

Example 3.6 JMP will compute the standard errors of the predicted response so it is easy to construct the prediction interval in Eq. (3.46). To illustrate, suppose that we want a $9 5 \%$ PI on a future observation of patient satisfaction for a patient whose age is 75 and with severity of illness 60. In the next to last row of Table 3.6 JMP predicted value of satisfaction at this new observation as $\hat { y } ( \mathbf { x } _ { 0 } ) = 3 2 . 7 8$ , and the standard error of the predicted response is 7.65. Then from Eq. (3.46) the prediction interval is

$$
1 6. 9 3 \leq y \left(\mathbf {x} _ {0}\right) \leq 4 8. 6 4.
$$

This example provides us with an opportunity to compare prediction and con-dence intervals. First, note that from Table 3.6 the standard error of the -t at this point is smaller than the standard error of the prediction. Therefore, the PI is longer than the corresponding CI. Now compare the length of the CI and the PI for this point with the length of the CI and the PI for the point $x _ { 1 } =$ patient age $= 6 0$ and $x _ { 2 } = \mathrm { s e v e r i t y } = 6 0$ $= 6 0$ from Example 3.4. The intervals are longer for the point in this example because this point with $x _ { 1 } =$ patient age $= 7 5$ and $x _ { 2 } = \mathrm { s e v e r i t y } = 6 0$ $= 6 0$ is further from the center of the predictor variable data than the point in Example 3.4, where $x _ { 1 } =$ patient age $= 6 0$ and $x _ { 2 } =$ severity $= 6 0$ .

# 3.5 MODEL ADEQUACY CHECKING

# 3.5.1 Residual Plots

An important part of any data analysis and model-building procedure is checking the adequacy of the model. We know that all models are wrong, but a model that is a reasonable -t to the data used to build it and that does not seriously ignore or violate any of the underlying model-building assumptions can be quite useful. Model adequacy checking is particularly important in building regression models for purposes of forecasting, because forecasting will almost always involve some extrapolation or projection of the model into the future, and unless the model is reasonable the forecasting process is almost certainly doomed to failure.

Regression model residuals, originally de-ned in Eq. (2.2), are very useful in model adequacy checking and to get some sense of how well the

regression model assumptions of normally and independently distributed model errors with constant variance are satis-ed. Recall that if $y _ { i }$ is the observed value of the response variable and if the corresponding -tted value from the model is $\hat { y } _ { i }$ , then the residuals are

$$
e _ {i} = y _ {i} - \hat {y} _ {i}, \quad i = 1, 2, \ldots , n.
$$

Residual plots are the primary approach to model adequacy checking. The simplest way to check the adequacy of the normality assumption on the model errors is to construct a normal probability plot of the residuals. In Section 2.6.1 we introduced and used the normal probability plot of forecast errors to check for the normality of forecast errors. The use of the normal probability plot for regression residuals follows the same approach. To check the assumption of constant variance, plot the residuals versus the -tted values from the model. If the constant variance assumption is satis-ed, this plot should exhibit a random scatter of residuals around zero. Problems with the equal variance assumption usually show up as a pattern on this plot. The most common pattern is an outward-opening funnel or megaphone pattern, indicating that the variance of the observations is increasing as the mean increases. Data transformations (see Section 2.4.1) are useful in stabilizing the variance. The log transformation is frequently useful in forecasting applications. It can also be helpful to plot the residuals against each of the predictor or regressor variables in the model. Any deviation from random scatter on these plots can indicate how well the model -ts a particular predictor.

When the data are a time series, it is also important to plot the residuals versus time order. As usual, the anticipated pattern on this plot is random scatter. Trends, cycles, or other patterns in the plot of residuals versus time indicate model inadequacies, possibly due to missing terms or some other model speci-cation issue. A funnel-shaped pattern that increases in width with time is an indication that the variance of the time series is increasing with time. This happens frequently in economic time series data, and in data that span a long period of time. Log transformations are often useful in stabilizing the variance of these types of time series.

Example 3.7 Table 3.3 presents the residuals for the regression model for the patient satisfaction data from Example 3.1. Figure 3.1 presents plots of these residuals. The plot in the upper left-hand portion of the display is a normal probability plot of the residuals. The residuals lie generally along a straight line, so there is no obvious reason to be concerned with

![](images/8aa82cdb90c6ca0b7c6f5555407ce9b5151333b3805b654ad6bcf14137ab6507.jpg)

![](images/d0f63f1aca6ac92b1eff54bdc0c43f2b7c7b300b934e277f8831fbd2668a4e27.jpg)

![](images/9b8c5d90c64b584acf286e3217dbee57e50ed9503cbba7d6eb701698d642e50f.jpg)

![](images/2633f588628e800484afc2ffae4f0782875e6087513228f8626731285c1ebee6.jpg)  
FIGURE 3.1 Plots of residuals for the patient satisfaction model.

the normality assumption. There is a very mild indication that one of the residuals (in the lower tail) may be slightly larger than expected, so this could be an indication of an outlier (a very mild one). The lower left plot is a histogram of the residuals. Histograms are more useful for large samples of data than small ones, so since there are only 25 residuals, this display is probably not as reliable as the normal probability plot. However, the histogram does not give any serious indication of nonnormality. The upper right is a plot of residuals versus the -tted values. This plot indicates essentially random scatter in the residuals, the ideal pattern. If this plot had exhibited a funnel shape, it could indicate problems with the equality of variance assumption. The lower right is a plot of the observations in the order of the data. If this was the order in which the data were collected, or if the data were a time series, this plot could reveal information about how the data may be changing over time. For example, a funnel shape on this plot might indicate that the variability of the observations was changing with time.

In addition to residual plots, other model diagnostics are frequently useful in regression. The following sections introduce and briey illustrate some of these procedures. For more complete presentations, see Montgomery, Peck, and Vining (2012) and Myers (1990).

# 3.5.2 Scaled Residuals and PRESS

Standardized Residuals Many regression model builders prefer to work with scaled residuals in contrast to the ordinary least squares (OLS) residuals. These scaled residuals frequently convey more information than do the ordinary residuals. One type of scaled residual is the standardized residual,

$$
d _ {i} = \frac {e _ {i}}{\hat {\sigma}}, \tag {3.47}
$$

where we generally use $\hat { \sigma } = \sqrt { M S _ { \mathrm { E } } }$ in the computation. The standardized residuals have mean zero and approximately unit variance; consequently, they are useful in looking for outliers. Most of the standardized residuals should lie in the interval $- 3 \leq d _ { i } \leq + 3$ , and any observation with a standardized residual outside this interval is potentially unusual with respect to its observed response. These outliers should be carefully examined because they may represent something as simple as a data-recording error or something of more serious concern, such as a region of the predictor or regressor variable space where the -tted model is a poor approximation to the true response.

Studentized Residuals The standardizing process in Eq. (3.47) scales the residuals by dividing them by their approximate average standard deviation. In some data sets, residuals may have standard deviations that differ greatly. We now present a scaling that takes this into account. The vector of -tted values $\hat { y } _ { i }$ that corresponds to the observed values $y _ { i }$ is

$$
\hat {\mathbf {y}} = \mathbf {X} \hat {\boldsymbol {\beta}} = \mathbf {X} \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\prime} \mathbf {y} = \mathbf {H} \mathbf {y}. \tag {3.48}
$$

The $n \times n$ matrix $\mathbf { H } = \mathbf { X } ( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 } \mathbf { X } ^ { \prime }$ is usually called the 鈥渉at鈥?matrix because it maps the vector of observed values into a vector of -tted values. The hat matrix and its properties play a central role in regression analysis.

The residuals from the -tted model may be conveniently written in matrix notation as

$$
\mathbf {e} = \mathbf {y} - \hat {\mathbf {y}} = \mathbf {y} - \mathbf {H} \mathbf {y} = (\mathbf {I} - \mathbf {H}) \mathbf {y} \tag {3.49}
$$

and the covariance matrix of the residuals is

$$
\operatorname {C o v} (\mathbf {e}) = V [ (\mathbf {I} - \mathbf {H}) \mathbf {y} ] = \sigma^ {2} (\mathbf {I} - \mathbf {H}).
$$

The matrix I 鈭?H is in general not diagonal, so the residuals from a linear regression model have different variances and are correlated. The variance of the ith residual is

$$
V \left(e _ {i}\right) = \sigma^ {2} \left(1 - h _ {i i}\right), \tag {3.50}
$$

where $h _ { i i }$ is the ith diagonal element of the hat matrix H. Because $0 \leq$ $h _ { i i } \leq 1$ using the mean squared error $M S _ { \mathrm { E } }$ to estimate the variance of the residuals actually overestimates the true variance. Furthermore, it turns out that $h _ { i i }$ is a measure of the location of the ith point in the predictor variable or $x$ -space; the variance of the residual $e _ { i }$ depends on where the point $\mathbf { x } _ { i }$ lies. As $h _ { i i }$ increases, the observation $\mathbf { x } _ { i }$ lies further from the center of the region containing the data. Therefore residuals near the center of the x-space have larger variance than do residuals at more remote locations. Violations of model assumptions are more likely at remote points, so these violations may be hard to detect from inspection of the ordinary residuals $e _ { i }$ (or the standardized residuals $d _ { i . }$ ) because their residuals will usually be smaller.

We recommend taking this inequality of variance into account when scaling the residuals. We suggest plotting the studentized residuals as:

$$
r _ {i} = \frac {e _ {i}}{\sqrt {\hat {\sigma} ^ {2} \left(1 - h _ {i i}\right)}}, \tag {3.51}
$$

with $\hat { \sigma } ^ { 2 } = M S _ { \mathrm { E } } $ instead of the ordinary residuals or the standardized residuals. The studentized residuals have unit variance (i.e., $V ( r _ { i } ) = 1 $ ) regardless of the location of the observation $\mathbf { x } _ { i }$ when the form of the regression model is correct. In many situations the variance of the residuals stabilizes, particularly for large data sets. In these cases, there may be little difference between the standardized and studentized residuals. Thus standardized and studentized residuals often convey equivalent information. However, because any point with a large residual and a large hat diagonal $h _ { i i }$ is potentially highly inuential on the least squares -t, examination of the studentized residuals is generally recommended.

Table 3.7 displays the residuals, the studentized residuals, hat diagonals $h _ { i i }$ , and several other diagnostics for the regression model for the patient satisfaction data in Example 3.1. These quantities were computer generated using JMP. To illustrate the calculations, consider the -rst observation.

TABLE 3.7 Residuals and Other Diagnostics for the Regression Model for the Patient Satisfaction Data in Example 3.1   

<table><tr><td>Observation</td><td>Residuals</td><td>Studentized Residuals</td><td>R-Student</td><td>hii</td><td>Cook&#x27;s Distance</td></tr><tr><td>1</td><td>9.0378</td><td>1.29925</td><td>1.32107</td><td>0.044855</td><td>0.026424</td></tr><tr><td>2</td><td>-5.6986</td><td>-0.88216</td><td>-0.87754</td><td>0.176299</td><td>0.055521</td></tr><tr><td>3</td><td>9.0373</td><td>1.38135</td><td>1.41222</td><td>0.155114</td><td>0.116772</td></tr><tr><td>4</td><td>-0.6953</td><td>-0.10403</td><td>-0.10166</td><td>0.118125</td><td>0.000483</td></tr><tr><td>5</td><td>-7.3897</td><td>-1.08009</td><td>-1.08440</td><td>0.076032</td><td>0.031999</td></tr><tr><td>6</td><td>-3.2155</td><td>-0.47342</td><td>-0.46491</td><td>0.089420</td><td>0.007337</td></tr><tr><td>7</td><td>-5.0316</td><td>-0.77380</td><td>-0.76651</td><td>0.165396</td><td>0.039553</td></tr><tr><td>8</td><td>7.0616</td><td>1.03032</td><td>1.03183</td><td>0.072764</td><td>0.027768</td></tr><tr><td>9</td><td>-17.2800</td><td>-2.65767</td><td>-3.15124</td><td>0.165533</td><td>0.467041</td></tr><tr><td>10</td><td>-6.0864</td><td>-0.87524</td><td>-0.87041</td><td>0.045474</td><td>0.012165</td></tr><tr><td>11</td><td>-11.6967</td><td>-1.70227</td><td>-1.78483</td><td>0.068040</td><td>0.070519</td></tr><tr><td>12</td><td>3.4823</td><td>0.51635</td><td>0.50757</td><td>0.102232</td><td>0.010120</td></tr><tr><td>13</td><td>-0.0859</td><td>-0.01272</td><td>-0.01243</td><td>0.100896</td><td>0.000006</td></tr><tr><td>14</td><td>2.1786</td><td>0.33738</td><td>0.33048</td><td>0.176979</td><td>0.008159</td></tr><tr><td>15</td><td>4.5134</td><td>0.66928</td><td>0.66066</td><td>0.102355</td><td>0.017026</td></tr><tr><td>16</td><td>4.7705</td><td>0.68484</td><td>0.67634</td><td>0.042215</td><td>0.006891</td></tr><tr><td>17</td><td>2.2474</td><td>0.33223</td><td>0.32541</td><td>0.096782</td><td>0.003942</td></tr><tr><td>18</td><td>0.8699</td><td>0.13695</td><td>0.13386</td><td>0.203651</td><td>0.001599</td></tr><tr><td>19</td><td>0.9276</td><td>0.13769</td><td>0.13458</td><td>0.104056</td><td>0.000734</td></tr><tr><td>20</td><td>3.7691</td><td>0.58556</td><td>0.57661</td><td>0.182192</td><td>0.025462</td></tr><tr><td>21</td><td>10.4993</td><td>1.62405</td><td>1.69133</td><td>0.175015</td><td>0.186511</td></tr><tr><td>22</td><td>0.6797</td><td>0.10725</td><td>0.10481</td><td>0.207239</td><td>0.001002</td></tr><tr><td>23</td><td>-9.2785</td><td>-1.46893</td><td>-1.51118</td><td>0.212456</td><td>0.194033</td></tr><tr><td>24</td><td>3.0927</td><td>0.44996</td><td>0.44165</td><td>0.067497</td><td>0.004885</td></tr><tr><td>25</td><td>4.2911</td><td>0.61834</td><td>0.60945</td><td>0.049383</td><td>0.006621</td></tr></table>

The studentized residual is calculated as follows:

$$
\begin{array}{l} r _ {1} = \frac {e _ {1}}{\sqrt {\hat {\sigma} ^ {2} (1 - h _ {1 1})}} \\ = \frac {e _ {1}}{\hat {\sigma} \sqrt {(1 - h _ {1 1})}} \\ = \frac {9 . 0 3 7 8}{7 . 1 1 7 6 7 \sqrt {1 - 0 . 0 4 4 8 5 5}} \\ = 1. 2 9 9 2 \\ \end{array}
$$

which agrees approximately with the value reported by JMP in Table 3.7. Large values of the studentized residuals are usually an indication of potential unusual values or outliers in the data. Absolute values of the studentized residuals that are larger than three or four indicate potentially problematic observations. Note that none of the studentized residuals in Table 3.7 is this large. The largest studentized residual, $- 2 . 6 5 7 6 7$ , is associated with observation 9. This observation does show up on the normal probability plot of residuals in Figure 3.1 as a very mild outlier, but there is no indication of a signi-cant problem with this observation.

PRESS Another very useful residual scaling can be based on the prediction error sum of squares or PRESS. To calculate PRESS, we select an observation鈥攆or example, i. We -t the regression model to the remaining $n - 1$ observations and use this equation to predict the withheld observation $y _ { i }$ . Denoting this predicted value by $\hat { y } _ { ( i ) }$ , we may now -nd the prediction error for the ith observation as

$$
e _ {(i)} = y _ {i} - \hat {y} _ {(i)} \tag {3.52}
$$

The prediction error is often called the ith PRESS residual. Note that the prediction error for the ith observation differs from the ith residual because observation $i$ was not used in calculating the ith prediction value $\hat { y } _ { ( i ) }$ . This procedure is repeated for each observation $i = 1 , 2 , \dots , n$ , producing a set of $n$ PRESS residuals $e _ { ( 1 ) }$ , $e _ { ( 2 ) } , \ldots , e _ { ( n ) }$ . Then the PRESS statistic is de-ned as the sum of squares of the $n$ PRESS residuals or

$$
\text {P R E S S} = \sum_ {i = 1} ^ {n} e _ {(i)} ^ {2} = \sum_ {i = 1} ^ {n} \left[ y _ {i} - \hat {y} _ {(i)} \right] ^ {2} \tag {3.53}
$$

Thus PRESS is a form of data splitting (discussed in Chapter 2), since it uses each possible subset of $n - 1$ observations as an estimation data set, and every observation in turn is used to form a prediction data set. Generally, small values of PRESS imply that the regression model will be useful in predicting new observations. To get an idea about how well the model will predict new data, we can calculate an $R ^ { 2 }$ -like statistic called the $R ^ { 2 }$ for prediction

$$
R _ {\text {P r e d i c t i o n}} ^ {2} = 1 - \frac {\text {P R E S S}}{S S _ {\mathrm {T}}} \tag {3.54}
$$

Now PRESS will always be larger than the residual sum of squares and, because the ordinary $R ^ { 2 } = 1 - ( S S _ { \mathrm { E } } / S S _ { \mathrm { T } } )$ , if the value of the $R _ { \mathrm { P r e d i c t i o n } } ^ { 2 }$ is not much smaller than the ordinary $R ^ { 2 }$ , this is a good indication about potential model predictive performance.

It would initially seem that calculating PRESS requires -tting $n$ different regressions. However, it is possible to calculate PRESS from the results of a single least squares -t to all $n$ observations. It turns out that the ith PRESS residual is

$$
e _ {(i)} = \frac {e _ {i}}{1 - h _ {i i}}, \tag {3.55}
$$

where $e _ { i }$ is the OLS residual. The hat matrix diagonals are directly calculated as a routine part of solving the least squares normal equations. Therefore PRESS is easily calculated as

$$
\text {P R E S S} = \sum_ {i = 1} ^ {n} \frac {e _ {i} ^ {2}}{1 - h _ {i i}} \tag {3.56}
$$

JMP will calculate the PRESS statistic for a regression model and the $R ^ { 2 }$ for prediction based on PRESS from Eq. (3.54). The value of PRESS is PRES $\mathrm { S } = 1 4 8 4 . 9 3$ and the $R ^ { 2 }$ for prediction is

$$
\begin{array}{l} R _ {\text {P r e d i c t i o n}} ^ {2} = 1 - \frac {\text {P R E S S}}{S S _ {\mathrm {T}}} \\ = 1 - \frac {1 4 8 4 . 9 3}{1 0 7 7 8 . 2} \\ = 0. 8 6 2 2. \\ \end{array}
$$

That is, this model would be expected to account for about $8 6 . 2 2 \%$ of the variability in new data.

R-Student The studentized residual $r _ { i }$ discussed earlier is often considered an outlier diagnostic. It is customary to use the mean squared error $M S _ { \mathrm { E } }$ as an estimate of $\sigma ^ { 2 }$ in computing $r _ { i }$ . This is referred to as internal scaling of the residual because $M S _ { \mathrm { E } }$ is an internally generated estimate of $\sigma ^ { 2 }$ obtained from -tting the model to all $n$ observations. Another approach

would be to use an estimate of vation removed. We denote the $\sigma ^ { 2 }$ based oimate of data set with tso obtained by th obser-. We can $\sigma ^ { 2 }$ $S _ { ( i ) } ^ { 2 }$ show that

$$
S _ {(i)} ^ {2} = \frac {(n - p) M S _ {\mathrm {E}} - e _ {i} ^ {2} / (1 - h _ {i i})}{n - p - 1} \tag {3.57}
$$

The estimate of $\sigma ^ { 2 }$ in Eq. (3.57) is used instead of $M S _ { \mathrm { E } }$ to produce an externally studentized residual, usually called R-student, given by

$$
t _ {i} = \frac {e _ {i}}{\sqrt {S _ {(i)} ^ {2} \left(1 - h _ {i i}\right)}} \tag {3.58}
$$

In many situations, $t _ { i }$ will differ little from the studentized residual $r _ { i }$ However, if the ith observation is inuential, then $S _ { ( i ) } ^ { 2 }$ can differ signi-cantly from $M S _ { \mathrm { E } }$ , and consequently the $R$ -student residual will be more sensitive to this observation. Furthermore, under the standard assumptions, the $R$ - student residual $t _ { i }$ has a $t$ -distribution with $n - p - 1$ degrees of freedom. Thus $R$ -student offers a more formal procedure for investigating potential outliers by comparing the absolute magnitude of the residual $t _ { i }$ to an appropriate percentage point of $t _ { n - p - 1 }$ .

JMP will compute the $R$ -student residuals. They are shown in Table 3.7 for the regression model for the patient satisfaction data. The largest value of $R$ -student is for observation 9, $t _ { 9 } = - 3 . 1 5 1 2 4$ . This is another indication that observation 9 is a very mild outlier.

# 3.5.3 Measures of Leverage and Influence

In building regression models, we occasionally -nd that a small subset of the data exerts a disproportionate inuence on the -tted model. That is, estimates of the model parameters or predictions may depend more on the inuential subset than on the majority of the data. We would like to locate these inuential points and assess their impact on the model. If these inuential points really are 鈥渂ad鈥?values, they should be eliminated. On the other hand, there may be nothing wrong with these points, but if they control key model properties, we would like to know it because it could affect the use of the model. In this section we describe and illustrate some useful measures of inuence.

The disposition of points in the predictor variable space is important in determining many properties of the regression model. In particular, remote

observations potentially have disproportionate leverage on the parameter estimates, predicted values, and the usual summary statistics.

The hat matrix $\mathbf { H } = \mathbf { X } ( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 } \mathbf { X } ^ { \prime }$ is very useful in identifying inuential observations. As noted earlier, H determines the variances and covariances of the predicted response and the residuals because

$$
\operatorname {V a r} (\hat {\mathbf {y}}) = \sigma^ {2} \mathbf {H} \quad \text {a n d} \quad \operatorname {V a r} (\mathbf {e}) = \sigma^ {2} (\mathbf {I} - \mathbf {H})
$$

The elements $h _ { i j }$ of the hat matrix H may be interpreted as the amount of leverage exerted by the observation $y _ { j }$ on the predicted value $\hat { y } _ { i }$ . Thus inspection of the elements of $\mathbf { H }$ can reveal points that are potentially inuential by virtue of their location in $x$ -space.

Attention is usually focused on the diagonal elements of the hat matrix $h _ { i i }$ . It can be shown that $\begin{array} { r } { \sum _ { i = 1 } ^ { n } h _ { i i } = \mathrm { r a n k } ( \mathbf { H } ) = \mathrm { r a n k } ( \mathbf { X } ) = p } \end{array}$ , so the average size of the diagonal elements of the H matrix is $p / n$ . A widely used rough guideline is to compare the diagonal elements $h _ { i i } \mathrm { t o }$ twice their average value $2 p / n$ , and if any hat diagonal exceeds this value to consider that observation as a high-leverage point.

JMP will calculate and save the values of the hat diagonals. Table 3.7 displays the hat diagonals for the regression model for the patient satisfaction data in Example 3.1. Since there are $p = 3$ parameters in the model and $n = 2 5$ observations, twice the average size of a hat diagonal for this problem is

$$
2 p / n = 2 (3) / 2 5 = 0. 2 4.
$$

The largest hat diagonal, 0.212456, is associated with observation 23. This does not exceed twice the average size of a hat diagonal, so there are no high-leverage observations in these data.

The hat diagonals will identify points that are potentially inuential due to their location in $x$ -space. It is desirable to consider both the location of the point and the response variable in measuring inuence. Cook (1977, 1979) has suggested using a measure of the squared distance between the least squares estimate based on all $n$ points $\hat { \boldsymbol { \beta } }$ and the estimate obtained by deleting the ith point, say, $\hat { \beta } _ { ( i ) }$ . This distance measure can be expressed as

$$
D _ {i} = \frac {\left(\hat {\boldsymbol {\beta}} - \hat {\boldsymbol {\beta}} _ {(i)}\right) ^ {\prime} \mathbf {X} ^ {\prime} \mathbf {X} \left(\hat {\boldsymbol {\beta}} - \hat {\boldsymbol {\beta}} _ {(i)}\right)}{p M S _ {\mathrm {E}}}, \quad i = 1, 2, \dots , n \tag {3.59}
$$

A reasonable cutoff for $D _ { i }$ is unity. That is, we usually consider observations for which $D _ { i } > 1$ to be inuential. Cook鈥檚 distance statistic $D _ { i }$ is actually calculated from

$$
D _ {i} = \frac {r _ {i} ^ {2}}{p} \frac {V [ \hat {y} (\mathbf {x} _ {i}) ]}{V (e _ {i})} = \frac {r _ {i} ^ {2}}{p} \frac {h _ {i i}}{1 - h _ {i i}} \tag {3.60}
$$

Note that, apart from the constant $p , D _ { i }$ is the product of the square of the ith studentized residual and the ratio $h _ { i i } / ( 1 - h _ { i i } )$ . This ratio can be shown to be the distance from the vector $\mathbf { x } _ { i }$ to the centroid of the remaining data. Thus $D _ { i }$ is made up of a component that reects how well the regression model -ts the ith observation $y _ { i }$ and a component that measures how far that point is from the rest of the data. Either component (or both) may contribute to a large value of $D _ { i }$ .

JMP will calculate and save the values of Cook鈥檚 distance statistic $D _ { i }$ Table 3.7 displays the values of Cook鈥檚 distance statistic for the regression model for the patient satisfaction data in Example 3.1. The largest value, 0.467041, is associated with observation 9. This value was calculated from Eq. (3.60) as follows:

$$
\begin{array}{l} D _ {i} = \frac {r _ {i} ^ {2}}{p} \frac {h _ {i i}}{1 - h _ {i i}} \\ = \frac {(- 2 . 6 5 7 6 7) ^ {2}}{3} \frac {0 . 1 6 5 5 3 3}{1 - 0 . 1 6 5 5 3 3} \\ = 0. 4 6 7 0 4 1. \\ \end{array}
$$

This does not exceed twice the cutoff of unity, so there are no inuential observations in these data.

# 3.6 VARIABLE SELECTION METHODS IN REGRESSION

In our treatment of regression we have concentrated on -tting the full regression model. Actually, in most applications of regression the analyst will have a very good idea about the general form of the model he/she wishes to -t, but there may be uncertainty about the exact structure of the model. For example, we may not know if all of the predictor variables are really necessary. These applications of regression frequently involve a moderately large or large set of candidate predictors, and the objective

of the analyst here is to -t a regression model to the 鈥渂est subset鈥?of these candidates. This can be a complex problem, as these data sets frequently have outliers, strong correlations between subsets of the variables, and other complicating features.

There are several techniques that have been developed for selecting the best subset regression model. Generally, these methods are either stepwisetype variable selection methods or all possible regressions. Stepwise-type methods build a regression model by either adding or removing a predictor variable to the basic model at each step. The forward selection version of the procedure begins with a model containing none of the candidate predictor variables and sequentially inserts variables into the model oneat-a-time until a -nal equation is produced. The criterion for entering a variable into the equation is that the $t \cdot$ -statistic for that variable must be signi-cant. The process is continued until there are no remaining candidate predictors that qualify for entry into the equation. In backward elimination, the procedure begins with all of the candidate predictor variables in the equation, and then variables are removed one-at-a-time to produce a -nal equation. The criterion for removing a variable is usually based on the $t \cdot$ -statistic, with the variable having the smallest $t { \cdot }$ -statistic considered for removal -rst. Variables are removed until all of the predictors remaining in the model have signi-cant $t$ -statistics. Stepwise regression usually consists of a combination of forward and backward stepping. There are many variations of the basic procedures.

In all possible regressions with $K$ candidate predictor variables, the analyst examines all $2 ^ { K }$ possible regression equations to identify the ones with potential to be a useful model. Obviously, as $K$ becomes even moderately large, the number of possible regression models quickly becomes formidably large. Ef-cient algorithms have been developed that implicitly rather than explicitly examine all of these equations. Typically, only the equations that are found to be 鈥渂est鈥?according to some criterion (such as minimum $M S _ { \mathrm { E } }$ or AICc) at each subset size are displayed. For more discussion of variable selection methods, see textbooks on regression such as Montgomery, Peck, and Vining (2012) or Myers (1990).

Example 3.8 Table 3.8 contains an expanded set of data for the hospital patient satisfaction data introduced in Example 3.1. In addition to the patient age and illness severity data, there are two additional regressors, an indicator of whether the patent is a surgical patient (1) or a medical patient (0), and an index indicating the patient鈥檚 anxiety level. We will use these data to illustrate how variable selection methods in regression can be used to help the analyst build a regression model.

TABLE 3.8 Expanded Patient Satisfaction Data   

<table><tr><td>Observation</td><td>Age</td><td>Severity</td><td>Surgical-Medical</td><td>Anxiety</td><td>Satisfaction</td></tr><tr><td>1</td><td>55</td><td>50</td><td>0</td><td>2.1</td><td>68</td></tr><tr><td>2</td><td>46</td><td>24</td><td>1</td><td>2.8</td><td>77</td></tr><tr><td>3</td><td>30</td><td>46</td><td>1</td><td>3.3</td><td>96</td></tr><tr><td>4</td><td>35</td><td>48</td><td>1</td><td>4.5</td><td>80</td></tr><tr><td>5</td><td>59</td><td>58</td><td>0</td><td>2.0</td><td>43</td></tr><tr><td>6</td><td>61</td><td>60</td><td>0</td><td>5.1</td><td>44</td></tr><tr><td>7</td><td>74</td><td>65</td><td>1</td><td>5.5</td><td>26</td></tr><tr><td>8</td><td>38</td><td>42</td><td>1</td><td>3.2</td><td>88</td></tr><tr><td>9</td><td>27</td><td>42</td><td>0</td><td>3.1</td><td>75</td></tr><tr><td>10</td><td>51</td><td>50</td><td>1</td><td>2.4</td><td>57</td></tr><tr><td>11</td><td>53</td><td>38</td><td>1</td><td>2.2</td><td>56</td></tr><tr><td>12</td><td>41</td><td>30</td><td>0</td><td>2.1</td><td>88</td></tr><tr><td>13</td><td>37</td><td>31</td><td>0</td><td>1.9</td><td>88</td></tr><tr><td>14</td><td>24</td><td>34</td><td>0</td><td>3.1</td><td>102</td></tr><tr><td>15</td><td>42</td><td>30</td><td>0</td><td>3.0</td><td>88</td></tr><tr><td>16</td><td>50</td><td>48</td><td>1</td><td>4.2</td><td>70</td></tr><tr><td>17</td><td>58</td><td>61</td><td>1</td><td>4.6</td><td>52</td></tr><tr><td>18</td><td>60</td><td>71</td><td>1</td><td>5.3</td><td>43</td></tr><tr><td>19</td><td>62</td><td>62</td><td>0</td><td>7.2</td><td>46</td></tr><tr><td>20</td><td>68</td><td>38</td><td>0</td><td>7.8</td><td>56</td></tr><tr><td>21</td><td>70</td><td>41</td><td>1</td><td>7.0</td><td>59</td></tr><tr><td>22</td><td>79</td><td>66</td><td>1</td><td>6.2</td><td>26</td></tr><tr><td>23</td><td>63</td><td>31</td><td>1</td><td>4.1</td><td>52</td></tr><tr><td>24</td><td>39</td><td>42</td><td>0</td><td>3.5</td><td>83</td></tr><tr><td>25</td><td>49</td><td>40</td><td>1</td><td>2.1</td><td>75</td></tr></table>

We will illustrate the forward selection procedure -rst. The JMP output that results from applying forward selection to these data is shown in Table 3.9. We used the AICc criterion for selecting the best model. The forward selection algorithm inserted the predictor patient age -rst, then severity, then anxiety, and -nally surg-med was inserted into the equation. The best model based on the minimum value of AICc contained age and severity.

Table 3.10 presents the results of applying the JMP backward elimination procedure to the patient satisfaction data. Once again the AICc criterion was chosen to select the -nal model. The procedure begins with all four predictors in the model, then the surgical鈥搈edical indicator variable was removed, followed by the anxiety predictor, followed by severity.

TABLE 3.9 JMP Forward Selection for the Patient Satisfaction Data in Table 3.8   

<table><tr><td colspan="10">Stepwise Fit for Satisfaction</td></tr><tr><td colspan="10">Stepwise Regression Control</td></tr><tr><td colspan="10">Stopping rule: Minimum AICc</td></tr><tr><td colspan="10">Direction: Forward</td></tr><tr><td>SSE</td><td>DFE</td><td>RMSE</td><td>RSquare</td><td>RSquare Adj</td><td>Cp</td><td>p</td><td>AICc</td><td>BIC</td><td></td></tr><tr><td>1114.5459</td><td>22</td><td>7.1176667</td><td>0.8966</td><td>0.8872</td><td>2.4553073</td><td>3</td><td>175.8801</td><td>178.7556</td><td></td></tr><tr><td colspan="9">Current Estimates</td><td></td></tr><tr><td>Lock</td><td>Entered</td><td>Parameter</td><td>Estimate</td><td>nDF</td><td>SS</td><td>&quot;F Ratio&quot;</td><td>&quot;Prob&gt;F&quot;</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Intercept</td><td>143.472012</td><td>1</td><td>0</td><td>0.000</td><td>1</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Age</td><td>-1.0310534</td><td>1</td><td>4029.379</td><td>79.536</td><td>9.28e-9</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Severity</td><td>-0.5560378</td><td>1</td><td>907.0377</td><td>17.904</td><td>0.00034</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Surg-Med</td><td>0</td><td>1</td><td>0.162962</td><td>0.003</td><td>0.95633</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Anxiety</td><td>0</td><td>1</td><td>74.611</td><td>1.507</td><td>0.23323</td><td></td><td></td></tr><tr><td colspan="9">StepHistory</td><td></td></tr><tr><td>Step</td><td>Parameter</td><td>Action</td><td>&quot;Sig Prob&quot;</td><td>Seq SS</td><td>RSquare</td><td>Cp</td><td>p</td><td>AICc</td><td>BIC</td></tr><tr><td>1</td><td>Age</td><td>Entered</td><td>0.0000</td><td>8756.656</td><td>0.8124</td><td>17.916</td><td>2</td><td>187.909</td><td>190.423</td></tr><tr><td>2</td><td>Severity</td><td>Entered</td><td>0.0003</td><td>907.0377</td><td>0.8966</td><td>2.4553</td><td>3</td><td>175.88</td><td>178.756</td></tr><tr><td>3</td><td>Anxiety</td><td>Entered</td><td>0.2332</td><td>74.611</td><td>0.9035</td><td>3.019</td><td>4</td><td>177.306</td><td>180.242</td></tr><tr><td>4</td><td>Surg-Med</td><td>Entered</td><td>0.8917</td><td>0.988332</td><td>0.9036</td><td>5</td><td>5</td><td>180.791</td><td>183.437</td></tr><tr><td>5</td><td>Best</td><td>Specific</td><td>.</td><td>.</td><td>0.8966</td><td>2.4553</td><td>3</td><td>175.88</td><td>178.756</td></tr></table>

However, removing severity causes an increase in AICc so it is added back to the model. The algorithm concluded with both patient age and severity in the model. Note that in this example, the forward selection procedure produced the same model as the backward elimination procedure. This does not always happen, so it is usually a good idea to investigate different model-building techniques for a problem.

Table 3.11 is the JMP stepwise regression algorithm applied to the patient satisfaction data, JMP calls the stepwise option 鈥渕ixed鈥?variable selection. The default signi-cance levels of 0.25 to enter or remove variables from the model were used. At the -rst step, patient age is entered in the model. Then severity is entered as the second variable. This is followed by anxiety as the third variable. At that point, none of the remaining predictors met the 0.25 signi-cance level criterion to enter the model, so stepwise regression terminated with age, severity and anxiety as the model predictors. This is not the same model found by backwards elimination and forward selection.

TABLE 3.10 JMP Backward Elimination for the Patient Satisfaction Data in Table 3.8   

<table><tr><td colspan="10">Stepwise Fit for Satisfaction</td></tr><tr><td colspan="10">Stepwise Regression Control</td></tr><tr><td colspan="10">Stopping rule: Minimum AICc</td></tr><tr><td colspan="10">Direction: Backward</td></tr><tr><td>SSE</td><td>DFE</td><td>RMSE</td><td>RSquare</td><td>RSquare Adj</td><td>Cp</td><td>p</td><td>AICc</td><td>BIC</td><td></td></tr><tr><td>1114.5459</td><td>22</td><td>7.1176667</td><td>0.8966</td><td>0.8872</td><td>2.4553073</td><td>3</td><td>175.8801</td><td>178.7556</td><td></td></tr><tr><td colspan="9">Current Estimates</td><td></td></tr><tr><td>Lock</td><td>Entered</td><td>Parameter</td><td>Estimate</td><td>nDF</td><td>SS</td><td>&quot;F Ratio&quot;</td><td>&quot;Prob&gt;F&quot;</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Intercept</td><td>143.472012</td><td>1</td><td>0</td><td>0.000</td><td>1</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Age</td><td>-1.0310534</td><td>1</td><td>4029.379</td><td>79.536</td><td>9.28e-9</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Severity</td><td>-0.5560378</td><td>1</td><td>907.0377</td><td>17.904</td><td>0.00034</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Surg-Med</td><td>0</td><td>1</td><td>0.162962</td><td>0.003</td><td>0.95633</td><td></td><td></td></tr><tr><td>鈽?/td><td>鈽?/td><td>Anxiety</td><td>0</td><td>1</td><td>74.611</td><td>1.507</td><td>0.23323</td><td></td><td></td></tr><tr><td colspan="9">Step History</td><td></td></tr><tr><td>Step</td><td>Parameter</td><td>Action</td><td>&quot;Sig Prob&quot;</td><td>Seq SS</td><td>RSquare</td><td>Cp</td><td>p</td><td>AICc</td><td>BIC</td></tr><tr><td>1</td><td>Age</td><td>Entered</td><td>0.0000</td><td>8756.656</td><td>0.8124</td><td>17.916</td><td>2</td><td>187.909</td><td>190.423</td></tr><tr><td>2</td><td>Severity</td><td>Entered</td><td>0.0003</td><td>907.0377</td><td>0.8966</td><td>2.4553</td><td>3</td><td>175.88</td><td>178.756</td></tr><tr><td>3</td><td>Surg-Med</td><td>Entered</td><td>0.9563</td><td>0.162962</td><td>0.8966</td><td>4.4522</td><td>4</td><td>179.034</td><td>181.971</td></tr><tr><td>4</td><td>Anxiety</td><td>Entered</td><td>0.2422</td><td>75.43637</td><td>0.9036</td><td>5</td><td>5</td><td>180.791</td><td>183.437</td></tr><tr><td>5</td><td>Surg-Med</td><td>Removed</td><td>0.8917</td><td>0.988332</td><td>0.9035</td><td>3.019</td><td>4</td><td>177.306</td><td>180.242</td></tr><tr><td>6</td><td>Anxiety</td><td>Removed</td><td>0.2332</td><td>74.611</td><td>0.8966</td><td>2.4553</td><td>3</td><td>175.88</td><td>178.756</td></tr><tr><td>7</td><td>Severity</td><td>Removed</td><td>0.0003</td><td>907.0377</td><td>0.8124</td><td>17.916</td><td>2</td><td>187.909</td><td>190.423</td></tr><tr><td>8</td><td>Best</td><td>Specific</td><td>.</td><td>.</td><td>0.8966</td><td>2.4553</td><td>3</td><td>175.88</td><td>178.756</td></tr></table>

Table 3.12 shows the results of applying the JMP all possible regressions algorithm to the patient satisfaction data. Since there are $k = 4$ predictors, there are 16 possible regression equations. JMP shows the best four of each subset size, along with the full (four-variable) model. For each model, JMP presents the value of $R ^ { 2 }$ , the square root of the mean squared error (RMSE), and the AICc and BIC statistics.

The model with the smallest value of AICc and BIC is the two-variable model with age and severity. The model with the smallest value of the mean squared error (or its square root, RMSE) is the three-variable model with age, severity, and anxiety. Both of these models were found using the stepwise-type algorithms. Either one of these models is likely to be a good regression model describing the effects of the predictor variables on patient satisfaction.

TABLE 3. 1 1 JMP Stepwise (Mixed) Variable Selection for the Patient Satisfaction Data in Table 3.8   

<table><tr><td colspan="9">Stepwise Fit for Satisfaction</td><td></td></tr><tr><td colspan="9">Stepwise Regression Control</td><td></td></tr><tr><td>Stopping rule:</td><td colspan="8">P-Value Threshold</td><td></td></tr><tr><td></td><td colspan="8">Prob to Enter 0.25</td><td></td></tr><tr><td></td><td colspan="8">Prob to Leave 0.25</td><td></td></tr><tr><td colspan="9">Direction: Mixed</td><td></td></tr><tr><td>SSE</td><td>DFE</td><td>RMSE</td><td>RSquare</td><td>RSquare Adj</td><td>Cp</td><td>p</td><td>AICc</td><td>BIC</td><td></td></tr><tr><td>1039.935</td><td>21</td><td>7.0370954</td><td>0.9035</td><td>0.8897</td><td>3.0190257</td><td>4</td><td>177.3058</td><td>180.2422</td><td></td></tr><tr><td colspan="9">Current Estimates</td><td></td></tr><tr><td>Lock</td><td>Entered</td><td>Parameter</td><td>Estimate</td><td>nDF</td><td>SS</td><td>&quot;F Ratio&quot;</td><td>&quot;Prob&gt;F&quot;</td><td></td><td></td></tr><tr><td>[X]</td><td>[X]</td><td>Intercept</td><td>143.895206</td><td>1</td><td>0</td><td>0.000</td><td>1</td><td></td><td></td></tr><tr><td>[ ]</td><td>[X]</td><td>Age</td><td>-1.1135376</td><td>1</td><td>3492.683</td><td>70.530</td><td>3.75e-8</td><td></td><td></td></tr><tr><td>[ ]</td><td>[X]</td><td>Severity</td><td>-0.5849193</td><td>1</td><td>971.8361</td><td>19.625</td><td>0.00023</td><td></td><td></td></tr><tr><td>[ ]</td><td>[ ]</td><td>Surg-Med</td><td>0</td><td>1</td><td>0.988332</td><td>0.019</td><td>0.89167</td><td></td><td></td></tr><tr><td>[ ]</td><td>[X]</td><td>Anxiety</td><td>1.2961695</td><td>1</td><td>74.611</td><td>1.507</td><td>0.23323</td><td></td><td></td></tr><tr><td colspan="9">Step History</td><td></td></tr><tr><td>Step</td><td>Parameter</td><td>Action</td><td>&quot;Sig Prob&quot;</td><td>Seq SS</td><td>RSquare</td><td>Cp</td><td>p</td><td>AICc</td><td>BIC</td></tr><tr><td>1</td><td>Age</td><td>Entered</td><td>0.0000</td><td>8756.656</td><td>0.8124</td><td>17.916</td><td>2</td><td>187.909</td><td>190.423 ()</td></tr><tr><td>2</td><td>Severity</td><td>Entered</td><td>0.0003</td><td>907.0377</td><td>0.8966</td><td>2.4553</td><td>3</td><td>175.88</td><td>178.756 ()</td></tr><tr><td>3</td><td>Anxiety</td><td>Entered</td><td>0.2332</td><td>74.611</td><td>0.9035</td><td>3.019</td><td>4</td><td>177.306</td><td>180.242 ()</td></tr></table>

TABLE 3.12 JMP All Possible Models Regression for the Patient Satisfaction Data in Table 3.8   

<table><tr><td colspan="7">All Possible Models</td></tr><tr><td colspan="7">Ordered up to best 4 models up to 4 terms per model.</td></tr><tr><td>Model</td><td>Number</td><td>RSquare</td><td>RMSE</td><td>AICc</td><td>BIC</td><td></td></tr><tr><td>Age</td><td>1</td><td>0.8124</td><td>9.3752</td><td>187.909</td><td>190.423</td><td>鈼?/td></tr><tr><td>Severity</td><td>1</td><td>0.5227</td><td>14.9549</td><td>211.257</td><td>213.771</td><td>鈼?/td></tr><tr><td>Anxiety</td><td>1</td><td>0.2876</td><td>18.2709</td><td>221.271</td><td>223.785</td><td>鈼?/td></tr><tr><td>Surg-Med</td><td>1</td><td>0.0547</td><td>21.0469</td><td>228.343</td><td>230.857</td><td>鈼?/td></tr><tr><td>Age, severity</td><td>2</td><td>0.8966</td><td>7.1177</td><td>175.880</td><td>178.756</td><td>鈼?/td></tr><tr><td>Age, anxiety</td><td>2</td><td>0.8133</td><td>9.5626</td><td>190.644</td><td>193.520</td><td>鈼?/td></tr><tr><td>Age, surg-Med</td><td>2</td><td>0.8126</td><td>9.5817</td><td>190.744</td><td>193.619</td><td>鈼?/td></tr><tr><td>Severity, anxiety</td><td>2</td><td>0.5795</td><td>14.3537</td><td>210.951</td><td>213.827</td><td>鈼?/td></tr><tr><td>Age, severity, anxiety</td><td>3</td><td>0.9035</td><td>7.0371</td><td>177.306</td><td>180.242</td><td>鈼?/td></tr><tr><td>Age, severity, surg-Med</td><td>3</td><td>0.8966</td><td>7.2846</td><td>179.034</td><td>181.971</td><td>鈼?/td></tr><tr><td>Age, surg-Med, anxiety</td><td>3</td><td>0.8135</td><td>9.7844</td><td>193.785</td><td>196.722</td><td>鈼?/td></tr><tr><td>Severity, surg-Med, anxiety</td><td>3</td><td>0.5893</td><td>14.5186</td><td>213.518</td><td>216.454</td><td>鈼?/td></tr><tr><td>Age, severity, surg-Med, anxiety</td><td>4</td><td>0.9036</td><td>7.2074</td><td>180.791</td><td>183.437</td><td>鈼?/td></tr><tr><td>RMSE</td><td>+</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>18+</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>16+</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>14+</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>12+</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>10+</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>8+</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>6+</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>+Age, severity</td><td>+Age, severity</td><td>+Age, severity, surg-Med, anxiety</td><td></td><td></td><td></td></tr><tr><td></td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td></tr><tr><td></td><td colspan="6">p = Number of Terms</td></tr></table>

# 3.7 GENERALIZED AND WEIGHTED LEAST SQUARES

In Section 3.4 we discussed methods for checking the adequacy of a linear regression model. Analysis of the model residuals is the basic methodology. A common defect that shows up in -tting regression models is nonconstant variance. That is, the variance of the observations is not constant but changes in some systematic way with each observation. This problem is

often identi-ed from a plot of residuals versus the -tted values. Transformation of the response variable is a widely used method for handling the inequality of variance problem.

Another technique for dealing with nonconstant error variance is to -t the model using the method of weighted least squares (WLS). In this method of estimation the deviation between the observed and expected values of $y _ { i }$ is multiplied by a weight $w _ { i }$ that is inversely proportional to the variance of $y _ { i }$ . For the case of simple linear regression, the WLS function is

$$
L = \sum_ {i = 1} ^ {n} w _ {i} \left(y _ {i} - \beta_ {0} - \beta_ {1} x _ {i}\right) ^ {2}, \tag {3.61}
$$

where $w _ { i } = 1 / \sigma _ { i } ^ { 2 }$ and $\sigma _ { i } ^ { 2 }$ is the variance of the ith observation $y _ { i }$ . The resulting least squares normal equations are

$$
\hat {\beta} _ {0} \sum_ {i = 1} ^ {n} w _ {i} + \hat {\beta} _ {1} \sum_ {i = 1} ^ {n} w _ {i} x _ {i} = \sum_ {i = 1} ^ {n} w _ {i} y _ {i} \tag {3.62}
$$

$$
\hat {\beta} _ {0} \sum_ {i = 1} ^ {n} w _ {i} x _ {i} + \hat {\beta} _ {1} \sum_ {i = 1} ^ {n} w _ {i} x _ {i} ^ {2} = \sum_ {i = 1} ^ {n} w _ {i} x _ {i} y _ {i}
$$

Solving Eq. (3.62) will produce WLS estimates of the model parameters $\beta _ { 0 }$ and $\beta _ { 1 }$ .

In this section we give a development of WLS for the multiple regression model. We begin by considering a slightly more general situation concerning the structure of the model errors.

# 3.7.1 Generalized Least Squares

The assumptions that we have made concerning the linear regression model $\mathbf { y } = \mathbf { X } \beta + \varepsilon$ are that $E ( \varepsilon ) = \mathbf { 0 }$ and Var $\mathbf { \Pi } ( \varepsilon ) = \sigma ^ { 2 } \mathbf { I }$ ; that is, the errors have expected value zero and constant variance, and they are uncorrelated. For testing hypotheses and constructing con-dence and prediction intervals we also assume that the errors are normally distributed, in which case they are also independent. As we have observed, there are situations where these assumptions are unreasonable. We will now consider the modi-cations that are necessary to the OLS procedure when $E ( \varepsilon ) = \mathbf { 0 }$ and Var ${ \bf \Pi } ( \varepsilon ) = \sigma ^ { 2 } { \bf V }$ , where V is a known $n \times n$ matrix. This situation has a simple interpretation; if V is diagonal but with unequal diagonal elements, then the observations

y are uncorrelated but have unequal variances, while if some of the offdiagonal elements of V are nonzero, then the observations are correlated. When the model is

$$
\begin{array}{l} \mathbf {y} = \mathbf {X} \boldsymbol {\beta} + \varepsilon \\ E (\varepsilon) = \mathbf {0} \quad \text {a n d} \quad \operatorname {V a r} (\varepsilon) = \sigma^ {2} \mathbf {I} \tag {3.63} \\ \end{array}
$$

the OLS estimator ${ \hat { \boldsymbol { \beta } } } = ( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 } \mathbf { X } ^ { \prime } \mathbf { y }$ is no longer appropriate. The OLS estimator is unbiased because

$$
E (\hat {\pmb {\beta}}) = E [ (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {X} ^ {\prime} \mathbf {y} ] = (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {X} ^ {\prime} E (\mathbf {y}) = (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {X} ^ {\prime} \pmb {\beta} = \pmb {\beta}
$$

but the covariance matrix of $\hat { \boldsymbol { \beta } }$ is not $\sigma ^ { 2 } ( \mathbf { X } ^ { \prime } \mathbf { X } ) ^ { - 1 }$ . Instead, the covariance matrix is

$$
\begin{array}{l} \operatorname {V a r} (\hat {\boldsymbol {\beta}}) = \operatorname {V a r} \left[ \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\prime} \mathbf {y} \right] \\ = \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\prime} V (\mathbf {y}) \mathbf {X} \left(\mathbf {X} ^ {\prime} \mathbf {X}\right) ^ {- 1} \\ = \sigma^ {2} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \mathbf {X} ^ {\prime} \mathbf {X} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1} \\ \end{array}
$$

Practically, this implies that the variances of the regression coef-cients are larger than we expect them to be.

This problem can be avoided if we estimate the model parameters with a technique that takes the correct variance structure in the errors into account. We will develop this technique by transforming the model to a new set of observations that satisfy the standard least squares assumptions. Then we will use OLS on the transformed observations.

Because $\sigma ^ { 2 } \mathbf { V }$ is the covariance matrix of the errors, V must be nonsingular and positive de-nite, so there exists an $n \times n$ nonsingular symmetric matrix K de-ned such that

$$
\mathbf {K} ^ {\prime} \mathbf {K} = \mathbf {K K} = \mathbf {V}
$$

The matrix $\mathbf { K }$ is often called the square root of V. Typically, the error variance $\sigma ^ { 2 }$ is unknown, in which case V represents the known (or assumed) structure of the variances and covariances among the random errors apart from the constant $\sigma ^ { 2 }$ .

De-ne the new variables

$$
\mathbf {z} = \mathbf {K} ^ {- 1} \mathbf {y}, \quad \mathbf {B} = \mathbf {K} ^ {- 1} \mathbf {X}, \quad \text {a n d} \quad \delta = \mathbf {K} ^ {- 1} \varepsilon \tag {3.64}
$$

so that the regression model $\mathbf { y } = \mathbf { X } \beta + \varepsilon$ becomes, upon multiplication by ${ \bf K } ^ { - 1 }$ ,

$$
\mathbf {K} ^ {- 1} \mathbf {y} = \mathbf {K} ^ {- 1} \mathbf {X} \boldsymbol {\beta} + \mathbf {K} ^ {- 1} \varepsilon
$$

or

$$
\mathbf {z} = \mathbf {B} \boldsymbol {\beta} + \boldsymbol {\delta} \tag {3.65}
$$

The errors in the transformed model Eq. (3.65) have zero expectation because $E ( \pmb \delta ) = E ( \mathbf { K } ^ { - 1 } \pmb \varepsilon ) = \mathbf { K } ^ { - 1 } E ( \pmb \varepsilon ) = \mathbf { 0 } .$ . Furthermore, the covariance matrix of $\pmb { \delta }$ is

$$
\begin{array}{l} \operatorname {V a r} (\delta) = V \left(\mathbf {K} ^ {- 1} \varepsilon\right) \\ = \mathbf {K} ^ {- 1} V (\boldsymbol {\varepsilon}) \mathbf {K} ^ {- 1} \\ = \sigma^ {2} \mathbf {K} ^ {- 1} \mathbf {V} \mathbf {K} ^ {- 1} \\ = \sigma^ {2} \mathbf {K} ^ {- 1} \mathbf {K} \mathbf {K} \mathbf {K} ^ {- 1} \\ \mathbf {\sigma} = \sigma^ {2} \mathbf {I} \\ \end{array}
$$

Thus the elements of the vector of errors $\pmb { \delta }$ have mean zero and constant variance and are uncorrelated. Since the errors $\pmb { \delta }$ in the model in Eq. (3.65) satisfy the usual assumptions, we may use OLS to estimate the parameters. The least squares function is

$$
\begin{array}{l} L = \pmb {\delta} ^ {\prime} \pmb {\delta} \\ = (\mathbf {K} ^ {- 1} \varepsilon) ^ {\prime} \mathbf {K} ^ {- 1} \varepsilon \\ = \varepsilon^ {\prime} \mathrm {K} ^ {- 1} \mathrm {K} ^ {- 1} \varepsilon \\ = \varepsilon^ {\prime} \mathrm {V} ^ {- 1} \varepsilon \\ = (\mathbf {y} - \mathbf {X} \boldsymbol {\beta}) ^ {\prime} \mathbf {V} ^ {- 1} (\mathbf {y} - \mathbf {X} \boldsymbol {\beta}) \\ \end{array}
$$

The corresponding normal equations are

$$
\left(\mathbf {X} ^ {\prime} \mathbf {V} ^ {- 1} \mathbf {X}\right) \hat {\boldsymbol {\beta}} _ {\text {G L S}} = \mathbf {X} ^ {\prime} \mathbf {V} ^ {- 1} \mathbf {y} \tag {3.66}
$$

In Equation (3.66) $\hat { \beta } _ { \mathrm { G L S } }$ is the generalized least squares (GLS) estimator of the model parameters $\beta$ . The solution to the GLS normal equations is

$$
\hat {\boldsymbol {\beta}} _ {\mathrm {G L S}} = \left(\mathbf {X} ^ {\prime} \mathbf {V} ^ {- 1} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\prime} \mathbf {V} ^ {- 1} \mathbf {y} \tag {3.67}
$$

The GLS estimator is an unbiased estimator for the model parameters $\beta$ , and the covariance matrix of $\hat { \beta } _ { \mathrm { G L S } }$ is

$$
\operatorname {V a r} \left(\hat {\boldsymbol {\beta}} _ {\mathrm {G L S}}\right) = \sigma^ {2} \left(\mathbf {X} ^ {\prime} \mathbf {V} ^ {- 1} \mathbf {X}\right) ^ {- 1} \tag {3.68}
$$

The GLS estimator is a best linear unbiased estimator of the model parameters $\beta$ , where 鈥渂est鈥?means minimum variance.

# 3.7.2 Weighted Least Squares

Weighted least squares or WLS is a special case of GLS where the $n$ response observations $y _ { i }$ do not have the same variances but are uncorrelated. Therefore the matrix V is

$$
\mathbf {V} = \left[ \begin{array}{c c c c} \sigma_ {1} ^ {2} & 0 & \dots & 0 \\ 0 & \sigma_ {2} ^ {2} & 0 & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \sigma_ {n} ^ {2} \end{array} \right],
$$

where $\sigma _ { i } ^ { 2 }$ is the variance of the ith observation $y _ { i }$ , $i = 1$ , 2, 鈥?, n. Because the weight for each observation should be the reciprocal of the variance of that observation, it is convenient to de-ne a diagonal matrix of weights $\mathbf { W } = \mathbf { V } ^ { - 1 }$ . Clearly, the weights are the main diagonals of the matrix W. Therefore the WLS criterion is

$$
L = (\mathbf {y} - \mathbf {X} \boldsymbol {\beta}) ^ {\prime} \mathbf {W} (\mathbf {y} - \mathbf {X} \boldsymbol {\beta}) \tag {3.69}
$$

and the WLS normal equations are

$$
\left(\mathbf {X} ^ {\prime} \mathbf {W X}\right) \hat {\boldsymbol {\beta}} _ {\mathrm {W L S}} = \mathbf {X} ^ {\prime} \mathbf {W y}. \tag {3.70}
$$

The WLS estimator is

$$
\hat {\boldsymbol {\beta}} _ {\mathrm {W L S}} = \left(\mathbf {X} ^ {\prime} \mathbf {W} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\prime} \mathbf {W} \mathbf {y}. \tag {3.71}
$$

The WLS estimator is an unbiased estimator for the model parameters $\beta$ and the covariance matrix of $\hat { \beta } _ { \mathrm { W L S } }$ is

$$
\operatorname {V a r} \left(\hat {\boldsymbol {\beta}} _ {\mathrm {W L S}}\right) = \left(\mathbf {X} ^ {\prime} \mathbf {W X}\right) ^ {- 1}. \tag {3.72}
$$

To use WLS, the weights $w _ { i }$ must be known. Sometimes prior knowledge or experience or information from an underlying theoretical model can be used to determine the weights. For example, suppose that a significant source of error is measurement error and different observations are measured by different instruments of unequal but known or well-estimated accuracy. Then the weights could be chosen inversely proportional to the variances of measurement error.

In most practical situations, however, the analyst learns about the inequality of variance problem from the residual analysis for the original model that was -t using OLS. For example, the plot of the OLS residuals $e _ { i }$ versus the -tted values $\hat { y } _ { i }$ may exhibit an outward-opening funnel shape, suggesting that the variance of the observations is increasing with the mean of the response variable y. Plots of the OLS residuals versus the predictor variables may indicate that the variance of the observations is a function of one of the predictors. In these situations we can often use estimates of the weights. There are several approaches that could be used to estimate the weights. We describe two of the most widely used methods.

Estimation of a Variance Equation In the -rst method, suppose that analysis of the OLS residuals indicates that the variance of the ith observation is a function of one or more predictors or the mean of y. The squared OLS residual $e _ { i } ^ { 2 }$ is an estimator of the variance of the ith observation $\sigma _ { i } ^ { 2 }$ if the form of the regression model is correct. Furthermore, the absolute value of the residual $| e _ { i } |$ is an estimator of the standard deviation $\sigma _ { i }$ (because $\sigma _ { i } = | \sqrt { \sigma _ { i } ^ { 2 } } | )$ . Consequently, we can -nd a variance equation or a regression model relating $\sigma _ { i } ^ { 2 }$ to appropriate predictor variables by the following process:

1. Fit the model relating $y$ to the predictor variables using OLS and -nd the OLS residuals.   
2. Use residual analysis to determine potential relationships between $\sigma _ { i } ^ { 2 }$ and either the mean of $y$ or some of the predictor variables.   
3. Regress the squared OLS residuals on the appropriate predictors to obtain an equation for predicting the variance of each observation, say, $\hat { s } _ { i } ^ { 2 } = f ( x )$ or $\hat { s } _ { i } ^ { 2 } = f ( y )$ .   
4. Use the -tted values from the estimated variance function to obtain estimates of the weights, $w _ { i } = 1 / \hat { s } _ { i } ^ { 2 }$ , $i = 1 , 2 , \dots , n$ .   
5. Use the estimated weights as the diagonal elements of the matrix W in the WLS procedure.

As an alternative to estimating a variance equation in step 3 above, we could use the absolute value of the OLS residual and -t an equation that relates the standard deviation of each observation to the appropriate regressors. This is the preferred approach if there are potential outliers in the data, because the absolute value of the residuals is less affected by outliers than the squared residuals.

When using the -ve-step procedure outlined above, it is a good idea to compare the estimates of the model parameters obtained from the WLS -t to those obtained from the original OLS -t. Because both methods produce unbiased estimators, we would expect to -nd that the point estimates of the parameters from both analyses are very similar. If the WLS estimates differ signi-cantly from their OLS counterparts, it is usually a good idea to use the new WLS residuals and reestimate the variance equation to produce a new set of weights and a revised set of WLS estimates using these new weights. This procedure is called iteratively reweighted least squares (IRLS). Usually one or two iterations are all that is required to produce stable estimates of the model parameters.

Using Replicates or Nearest Neighbors The second approach to estimating the weights makes use of replicate observations or nearest neighbors. Exact replicates are sample observations that have exactly the same values of the predictor variables. Suppose that there are replicate observations at each of the combination of levels of the predictor variables. The weights $w _ { i }$ can be estimated directly as the reciprocal of the sample variances at each combination of these levels. Each observation in a replicate group would receive the same weight. This method works best when there are a moderately large number of observations in each replicate group, because small samples do not produce reliable estimates of the variance.

Unfortunately, it is fairly unusual to -nd groups of replicate observations in most regression-modeling situations. It is especially unusual to -nd them in time series data. An alternative is to look for observations with similar $x$ -levels, which can be thought of as a nearest-neighbor group of observations. The observations in a nearest-neighbor group can be considered as pseudoreplicates and the sample variance for all of the observations in each nearest-neighbor group can be computed. The reciprocal of a sample variance would be used as the weight for all observations in the nearestneighbor group.

Sometimes these nearest-neighbor groups can be identi-ed visually by inspecting the scatter plots of y versus the predictor variables or from plots of the predictor variables versus each other. Analytical methods can also be

used to -nd these nearest-neighbor groups. One nearest-neighbor algorithm is described in Montgomery, Peck, and Vining (2012). These authors also present a complete example showing how the nearest-neighbor approach can be used to estimate the weights for a WLS analysis.

Statistical Inference in WLS In WLS the variances $\sigma _ { i } ^ { 2 }$ are almost always unknown and must be estimated. Since statistical inference on the model parameters as well as con-dence intervals and prediction intervals on the response are usually necessary, we should consider the effect of using estimated weights on these procedures. Recall that the covariance matrix of the model parameters in WLS was given in Eq. (3.72). This covariance matrix plays a central role in statistical inference. Obviously, when estimates of the weights are substituted into Eq. (3.72) an estimated covariance matrix is obtained. Generally, the impact of using estimated weights is modest, provided that the sample size is not very small. In these situations, statistical tests, con-dence intervals, and prediction intervals should be considered as approximate rather than exact.

Example 3.9 Table 3.13 contains 28 observations on the strength of a connector and the age in weeks of the glue used to bond the components of the connector together. A scatter plot of the strength versus age, shown in Figure 3.2, suggests that there may be a linear relationship between strength and age, but there may also be a problem with nonconstant variance in the data. The regression model that was -t to these data is

$$
\hat {y} = 2 5. 9 3 6 + 0. 3 7 5 9 x,
$$

where $x =$ weeks.

The residuals from this model are shown in Table 3.13. Figure 3.3 is a plot of the residuals versus weeks. The pronounced outward-opening funnel shape on this plot con-rms the inequality of variance problem. Figure 3.4 is a plot of the absolute value of the residuals from this model versus week. There is an indication that a linear relationship may exist between the absolute value of the residuals and weeks, although there is evidence of one outlier in the data. Therefore it seems reasonable to -t a model relating the absolute value of the residuals to weeks. Since the absolute value of a residual is the residual standard deviation, the predicted values from this equation could be used to determine weights for the regression model relating strength to weeks. This regression model is

$$
\hat {s} _ {i} = - 5. 8 5 4 + 0. 2 9 8 5 2 x.
$$

TABLE 3.13 Connector Strength Data   

<table><tr><td>Observation</td><td>Weeks</td><td>Strength</td><td>Residual</td><td>Absolute Residual</td><td>Weights</td></tr><tr><td>1</td><td>20</td><td>34</td><td>0.5454</td><td>0.5454</td><td>73.9274</td></tr><tr><td>2</td><td>21</td><td>35</td><td>1.1695</td><td>1.1695</td><td>5.8114</td></tr><tr><td>3</td><td>23</td><td>33</td><td>-1.5824</td><td>1.5824</td><td>0.9767</td></tr><tr><td>4</td><td>24</td><td>36</td><td>1.0417</td><td>1.0417</td><td>0.5824</td></tr><tr><td>5</td><td>25</td><td>35</td><td>-0.3342</td><td>0.3342</td><td>0.3863</td></tr><tr><td>6</td><td>28</td><td>34</td><td>-2.4620</td><td>2.4620</td><td>0.1594</td></tr><tr><td>7</td><td>29</td><td>37</td><td>0.1621</td><td>0.1621</td><td>0.1273</td></tr><tr><td>8</td><td>30</td><td>34</td><td>-3.2139</td><td>3.2139</td><td>0.1040</td></tr><tr><td>9</td><td>32</td><td>42</td><td>4.0343</td><td>4.0343</td><td>0.0731</td></tr><tr><td>10</td><td>33</td><td>35</td><td>-3.3416</td><td>3.3416</td><td>0.0626</td></tr><tr><td>11</td><td>35</td><td>33</td><td>-6.0935</td><td>6.0935</td><td>0.0474</td></tr><tr><td>12</td><td>37</td><td>46</td><td>6.1546</td><td>6.1546</td><td>0.0371</td></tr><tr><td>13</td><td>38</td><td>43</td><td>2.7787</td><td>2.7787</td><td>0.0332</td></tr><tr><td>14</td><td>40</td><td>32</td><td>-8.9731</td><td>8.9731</td><td>0.0270</td></tr><tr><td>15</td><td>41</td><td>37</td><td>-4.3491</td><td>4.3491</td><td>0.0245</td></tr><tr><td>16</td><td>43</td><td>50</td><td>7.8991</td><td>7.8991</td><td>0.0205</td></tr><tr><td>17</td><td>44</td><td>34</td><td>-8.4769</td><td>8.4769</td><td>0.0189</td></tr><tr><td>18</td><td>45</td><td>54</td><td>11.1472</td><td>11.1472</td><td>0.0174</td></tr><tr><td>19</td><td>46</td><td>49</td><td>5.7713</td><td>5.7713</td><td>0.0161</td></tr><tr><td>20</td><td>48</td><td>55</td><td>11.0194</td><td>11.0194</td><td>0.0139</td></tr><tr><td>21</td><td>50</td><td>40</td><td>-4.7324</td><td>4.7324</td><td>0.0122</td></tr><tr><td>22</td><td>51</td><td>33</td><td>-12.1084</td><td>12.1084</td><td>0.0114</td></tr><tr><td>23</td><td>52</td><td>56</td><td>10.5157</td><td>10.5157</td><td>0.0107</td></tr><tr><td>24</td><td>55</td><td>58</td><td>11.3879</td><td>11.3879</td><td>0.0090</td></tr><tr><td>25</td><td>56</td><td>45</td><td>-1.9880</td><td>1.9880</td><td>0.0085</td></tr><tr><td>26</td><td>57</td><td>33</td><td>-14.3639</td><td>14.3639</td><td>0.0080</td></tr><tr><td>27</td><td>59</td><td>60</td><td>11.8842</td><td>11.8842</td><td>0.0072</td></tr><tr><td>28</td><td>60</td><td>35</td><td>-13.4917</td><td>13.4917</td><td>0.0069</td></tr></table>

The weights would be equal to the inverse of the square of the -tted value for each $s _ { i }$ . These weights are shown in Table 3.13. Using these weights to -t a new regression model to strength using WLS results in

$$
\hat {y} = 2 7. 5 4 5 + 0. 3 2 3 8 3 x
$$

Note that the weighted least squares model does not differ very much from the OLS model. Because the parameter estimates did not change very much, this is an indication that it is not necessary to iteratively reestimate the standard deviation model and obtain new weights.

![](images/87528f8f9e703f5baf5c62c6dacb15771c81a05a4fdcb9b725fdd3b310730ad5.jpg)  
FIGURE 3.2 Scatter diagram of connector strength versus age from Table 3.12.

# 3.7.3 Discounted Least Squares

Weighted least squares is typically used in situations where the variance of the observations is not constant. We now consider a different situation where a WLS-type procedure is also appropriate. Suppose that the predictor variables in the regression model are only functions of time. As

![](images/656ebaec4f67e73cbdcbb33f00ea563467713200c0782a32a4fe3132a657f6fc.jpg)  
FIGURE 3.3 Plot of residuals versus weeks.

![](images/5db54271101ba7af4a0af97e3cbee72f01d091f5cf1ef0084d8301159b622c43.jpg)  
FIGURE 3.4 Scatter plot of absolute residuals versus weeks.

an illustration, consider the linear regression model with a linear trend in time:

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \varepsilon , \quad t = 1, 2, \dots , T \tag {3.73}
$$

This model was introduced to illustrate trend adjustment in a time series in Section 2.4.2 and Example 3.2. As another example, the regression model

$$
y _ {t} = \beta_ {0} + \beta_ {1} \sin \frac {2 \pi}{d} t + \beta_ {2} \cos \frac {2 \pi}{d} t + \varepsilon \tag {3.74}
$$

describes the relationship between a response variable $y$ that varies cyclically or periodically with time where the cyclic variation is modeled as a simple sine wave. A very general model for these types of situations could be written as

$$
y _ {t} = \beta_ {0} + \beta_ {1} x _ {1} (t) + \dots + \beta_ {k} x _ {k} (t) + \varepsilon_ {t}, \quad t = 1, 2, \ldots , T, \tag {3.75}
$$

where the predictors $x _ { 1 } ( t ) , x _ { 2 } ( t ) , \ldots , x _ { k } ( t )$ are mathematical functions of time, t. In these types of models it is often logical to believe that older observations are of less value in predicting the future observations at periods $T + 1 , T + 2 , \dots$ , than are the observations that are close to the current time period, $T$ . In other words, if you want to predict the value of y at time

$T + 1$ given that you are at the end of time period $T$ (or $\hat { y } _ { T + 1 } ( T ) )$ , it is logical to assume that the more recent observations such as $y _ { T } , y _ { T - 1 }$ , and $y _ { T - 2 }$ carry much more useful information than do older observations such as $y _ { T - 2 0 }$ . Therefore it seems reasonable to weight the observations in the regression model so that recent observations are weighted more heavily than older observations. A very useful variation of WLS, called discounted least squares, can be used to do this. Discounted least squares also lead to a relatively simple way to update the estimates of the model parameters after each new observation in the time series.

Suppose that the model for observation $y _ { t }$ is given by Eq. (3.75):

$$
\begin{array}{l} y _ {t} = \beta_ {1} x _ {1} (t) + \dots + \beta_ {p} x _ {p} (t) + \varepsilon_ {t} \\ = \mathbf {x} (t) ^ {\prime} \boldsymbol {\beta}, \quad t = 1, 2, \dots , T, \\ \end{array}
$$

where $\mathbf { x } ( t ) ^ { \prime } = [ x _ { 1 } ( t ) , x _ { 2 } ( t ) , \ldots , x _ { p } ( t ) ]$ and $\beta ^ { \prime } = [ \beta _ { 1 } , \beta _ { 2 } , \dots , \beta _ { p } ]$ . This model could have an intercept term, in which case $x _ { 1 } ( t ) = 1$ and the -nal model term could be written as $\beta _ { k } x _ { k } ( t )$ as in Eq. (3.75). In matrix form, Eq. (3.75) is

$$
\mathbf {y} = \mathbf {X} (T) \boldsymbol {\beta} + \varepsilon , \tag {3.76}
$$

where $\mathbf { y }$ is a $T \times 1$ vector of the observations, $\beta$ is a $p \times 1$ vector of the model parameters, $\varepsilon$ is a $T \times 1$ vector of the errors, and $\mathbf X ( T )$ is the $T \times p$ matrix

$$
\mathbf {X} (T) = \left[ \begin{array}{c c c c} x _ {1} (1) & x _ {2} (1) & \dots & x _ {p} (1) \\ x _ {1} (2) & x _ {2} (2) & \dots & x _ {p} (2) \\ \vdots & \vdots & \vdots & \vdots \\ x _ {1} (T) & x _ {2} (T) & \dots & x _ {p} (T) \end{array} \right]
$$

Note that the tth row of $\mathbf X ( T )$ contains the values of the predictor variables that correspond to the tth observation of the response, $y _ { t }$ .

We will estimate the parameters in Eq. (3.76) using WLS. However, we are going to choose the weights so that they decrease in magnitude with time. Speci-cally, let the weight for observation $y _ { T - j }$ be $\theta ^ { j }$ , where $0 < \theta <$ 1. We are also going to shift the origin of time with each new observation

so that $T$ is the current time period. Therefore the WLS criterion is

$$
\begin{array}{l} L = \sum_ {j = 0} ^ {T - 1} w _ {j} \left[ y _ {T - j} - \left(\beta_ {1} (T) x _ {1} (- j) + \dots + \beta_ {p} (T) x _ {k} (- j)\right) \right] ^ {2} \tag {3.77} \\ = \sum_ {j = 0} ^ {T - 1} w _ {j} \left[ y _ {T - j} - \mathbf {x} (- j) \boldsymbol {\beta} (T) \right] ^ {2}, \\ \end{array}
$$

where $\beta ( T )$ indicates that the vector of regression coef-cients is estimated at the end of time period $T$ , and $\mathbf { x } ( - j )$ indicates that the predictor variables, which are just mathematical functions of time, are evaluated at $- j$ . This is just WLS with a $T \times T$ diagonal weight matrix

$$
\mathbf {W} = \left[ \begin{array}{c c c c c} \theta^ {T - 1} & 0 & 0 & \dots & 0 \\ 0 & \theta^ {T - 2} & 0 & \dots & 0 \\ \vdots & & \ddots & \vdots & \vdots \\ 0 & \dots & & \theta & 0 \\ 0 & 0 & \dots & 0 & 1 \end{array} \right]
$$

By analogy with Eq. (3.70), the WLS normal equations are

$$
\mathbf {X} (T) ^ {\prime} \mathbf {W} \mathbf {X} (T) \hat {\boldsymbol {\beta}} (T) = \mathbf {X} (T) ^ {\prime} \mathbf {W} \mathbf {y}
$$

or

$$
\mathbf {G} (T) \hat {\boldsymbol {\beta}} (T) = \mathbf {g} (T), \tag {3.78}
$$

where

$$
\begin{array}{l} \mathbf {G} (\mathrm {T}) = \mathbf {X} (T) ^ {\prime} \mathbf {W} \mathbf {X} (T) \tag {3.79} \\ \mathbf {g} (T) = \mathbf {X} (T) ^ {\prime} \mathbf {W y} \\ \end{array}
$$

The solution to the WLS normal equations is

$$
\hat {\boldsymbol {\beta}} (T) = \mathbf {G} (T) ^ {- 1} \mathbf {g} (T), \tag {3.80}
$$

${ \hat { \boldsymbol { \beta } } } ( { \boldsymbol { T } } )$ is called the discounted least squares estimator of $\beta$

In many important applications, the discounted least squares estimator can be simpli-ed considerably. Assume that the predictor variables $x _ { i } ( t )$ in the model are functions of time that have been chosen so that their values

at time period $t + 1$ are linear combinations of their values at the previous time period. That is,

$$
x _ {i} (t + 1) = L _ {i 1} x _ {1} (t) + L _ {i 2} x _ {2} (t) + \dots + L _ {i p} x _ {p} (t), \quad i = 1, 2, \ldots , p \tag {3.81}
$$

In matrix form,

$$
\mathbf {x} (t + 1) = \mathbf {L x} (t), \tag {3.82}
$$

where L is the $p \times p$ matrix of the constants $L _ { i j }$ in Eq. (3.81). The transition property in Eq. (3.81) holds for polynomial, trigonometric, and certain exponential functions of time. This transition relationship implies that

$$
\mathbf {x} (t) = \mathbf {L} ^ {t} \mathbf {x} (0) \tag {3.83}
$$

Consider the matrix $\mathbf G ( T )$ in the normal equations (3.78). We can write

$$
\begin{array}{l} \mathbf {G} (\mathrm {T}) = \sum_ {j = 0} ^ {T - 1} \theta^ {j} \mathbf {x} (- j) \mathbf {x} (- j) ^ {\prime} \\ = \mathbf {G} (T - 1) + \theta^ {T - 1} \mathbf {x} (- (T - 1)) \mathbf {x} (- (T - 1)) ^ {\prime} \\ \end{array}
$$

If the predictor variables $x _ { i } ( t )$ in the model are polynomial, trigonometric, or certain exponential functions of time, the matrix $\mathbf { G } ( T )$ approaches a steady-state limiting value G,where

$$
\mathbf {G} = \sum_ {j = 0} ^ {\infty} \theta^ {j} \mathbf {x} (- j) \mathbf {x} (- j) ^ {\prime} \tag {3.84}
$$

Consequently, the inverse of G would only need to be computed once. The right-hand side of the normal equations can also be simpli-ed. We can write

$$
\begin{array}{l} \mathbf {g} (\mathrm {T}) = \sum_ {j = 0} ^ {T - 1} \theta^ {j} y _ {T - j} \mathbf {x} (- j) \\ = y _ {T} \mathbf {x} (0) + \sum_ {j = 1} ^ {T - 1} \theta^ {j} y _ {T - j} \mathbf {x} (- j) \\ = y _ {T} \mathbf {x} (0) + \theta \sum_ {j = 1} ^ {T - 1} \theta^ {j - 1} y _ {T - j} \mathbf {L} ^ {- 1} \mathbf {x} (- j + 1) \\ = y _ {T} \mathbf {x} (0) + \theta \mathbf {L} ^ {- 1} \sum_ {k = 0} ^ {T - 2} \theta^ {k} y _ {T - 1 - k} \mathbf {x} (- k) \\ = y _ {T} \mathbf {x} (0) + \theta \mathbf {L} ^ {- 1} \mathbf {g} (T - 1) \\ \end{array}
$$

So the discounted least squares estimator can be written as

$$
\hat {\pmb {\beta}} (T) = \mathbf {G} ^ {- 1} \mathbf {g} (T)
$$

This can also be simpli-ed. Note that

$$
\begin{array}{l} \hat {\boldsymbol {\beta}} (T) = \mathbf {G} ^ {- 1} \mathbf {g} (T) \\ = \mathbf {G} ^ {- 1} \left[ y _ {T} \mathbf {x} (0) + \theta \mathbf {L} ^ {- 1} \mathbf {g} (T - 1) \right] \\ = \mathbf {G} ^ {- 1} \left[ y _ {T} \mathbf {x} (0) + \theta \mathbf {L} ^ {- 1} \mathbf {G} \hat {\boldsymbol {\beta}} (T - 1) \right] \\ = y _ {T} \mathbf {G} ^ {- 1} \mathbf {x} (0) + \theta \mathbf {G} ^ {- 1} \mathbf {L} ^ {- 1} \mathbf {G} \hat {\boldsymbol {\beta}} (T - 1) \\ \end{array}
$$

or

$$
\hat {\boldsymbol {\beta}} (T) = \mathbf {h} \mathbf {y} _ {T} + \mathbf {Z} \hat {\boldsymbol {\beta}} (T - 1) \tag {3.85}
$$

where

$$
\mathbf {h} = \mathbf {G} ^ {- 1} \mathbf {x} (0) \tag {3.86}
$$

and

$$
\mathbf {Z} = \theta \mathbf {G} ^ {- 1} \mathbf {L} ^ {- 1} \mathbf {G} \tag {3.87}
$$

The right-hand side of Eq. (3.85) can still be simpli-ed because

$$
\begin{array}{l} \mathbf {L} ^ {- 1} \mathbf {G} = \mathbf {L} ^ {- 1} \mathbf {G} (\mathbf {L} ^ {\prime}) ^ {- 1} \mathbf {L} ^ {\prime} \\ = \sum_ {j = 0} ^ {\infty} \theta^ {j} \mathbf {L} ^ {- 1} \mathbf {x} (- j) \mathbf {x} (- j) ^ {\prime} (\mathbf {L} ^ {\prime}) ^ {- 1} \mathbf {L} ^ {\prime} \\ = \sum_ {j = 0} ^ {\infty} \theta^ {j} [ \mathbf {L} ^ {- 1} \mathbf {x} (- j) ] [ \mathbf {L} ^ {- 1} \mathbf {x} (- j) ] ^ {\prime} \mathbf {L} ^ {\prime} \\ = \sum_ {j = 0} ^ {\infty} \theta^ {j} \mathbf {x} (- j - 1) \mathbf {x} (- j - 1) ^ {\prime} \mathbf {L} ^ {\prime} \\ \end{array}
$$

and letting $k = j + 1$ ,

$$
\begin{array}{l} \mathbf {L} ^ {- 1} \mathbf {G} = \theta^ {- 1} \sum_ {k = 1} ^ {\infty} \theta^ {k} \mathbf {x} (- k) \mathbf {x} (- k) ^ {\prime} \mathbf {L} ^ {\prime} \\ = \theta^ {- 1} [ \mathbf {G} - \mathbf {x} (0) \mathbf {x} (0) ^ {\prime} ] \mathbf {L} ^ {\prime} \\ \end{array}
$$

Substituting for $\mathbf { L } ^ { - 1 } \mathbf { G }$ on the right-hand side of Eq. (3.87) results in

$$
\begin{array}{l} \mathbf {Z} = \theta \mathbf {G} ^ {- 1} \theta^ {- 1} [ \mathbf {G} - \mathbf {x} (0) \mathbf {x} (0) ^ {\prime} ] \mathbf {L} ^ {\prime} \\ = [ \mathbf {I} - \mathbf {G} ^ {- 1} \mathbf {x} (0) \mathbf {x} (0) ^ {\prime} ] \mathbf {L} ^ {\prime} \\ = \mathbf {L} ^ {\prime} - \mathbf {h x} (0) \mathbf {L} ^ {\prime} \\ = \mathbf {L} ^ {\prime} - \mathbf {h} [ \mathbf {L} \mathbf {x} (0) ] ^ {\prime} \\ = \mathbf {L} ^ {\prime} - \mathbf {h x} (1) ^ {\prime} \\ \end{array}
$$

Now the vector of discounted least squares parameter estimates at the end of time period $T$ in Eq. (3.85) is

$$
\begin{array}{l} \hat {\boldsymbol {\beta}} (T) = \mathbf {h} y _ {T} + \mathbf {Z} \hat {\boldsymbol {\beta}} (T - 1) \\ = \mathbf {h} y _ {T} + \left[ \mathbf {L} ^ {\prime} - \mathbf {h x} (1) ^ {\prime} \right] \hat {\boldsymbol {\beta}} (T - 1) \\ = \mathbf {L} ^ {\prime} \hat {\boldsymbol {\beta}} (T - 1) + \mathbf {h} [ y _ {T} - \mathbf {x} (1) ^ {\prime} \hat {\boldsymbol {\beta}} (T - 1) ]. \\ \end{array}
$$

But $\mathbf { x } ( 1 ) ^ { \prime } { \pmb \beta } ( T - 1 ) = \hat { y } _ { T } ( T - 1 )$ is the forecast of $y _ { T }$ computed at the end of the previous time period, $T - 1$ , so the discounted least squares vector of parameter estimates computed at the end of time period $t$ is

$$
\begin{array}{l} \hat {\boldsymbol {\beta}} (\mathrm {T}) = \mathbf {L} ^ {\prime} \hat {\boldsymbol {\beta}} (T - 1) + \mathbf {h} \left[ y _ {T} - \hat {y} _ {T} (T - 1) \right] \tag {3.88} \\ = \mathbf {L} ^ {\prime} \hat {\boldsymbol {\beta}} (T - 1) + \mathbf {h} e _ {t} (1). \\ \end{array}
$$

The last line in Eq. (3.88) is an extremely important result; it states that in discounted least squares the vector of parameter estimates computed at the end of time period $T$ can be computed as a simple linear combination of the estimates made at the end of the previous time period $T - 1$ and the one-step-ahead forecast error for the observation in period T. Note that there are really two things going on in estimating $\beta$ by discounted least squares: the origin of time is being shifted to the end of the current period, and the estimates of the model parameters are being modi-ed to reect the forecast error in the current time period. The -rst and second terms on the right-hand side of Eq. (3.88) accomplish these objectives, respectively.

When discounted least squares estimation is started up, an initial estimate of the parameters is required at time period zero, say, $\mathbf { \hat { \boldsymbol { \beta } } } ( 0 )$ . This could be found by a standard least squares (or WLS) analysis of historical data.

Because the origin of time is shifted to the end of the current time period, forecasting is easy with discounted least squares. The forecast of

the observation at a future time period $T + \tau$ , made at the end of time period $T$ , is

$$
\begin{array}{l} \hat {y} _ {T + \tau} (T) = \hat {\beta} (T) ^ {\prime} \mathbf {x} (\tau) \\ = \sum_ {j = 1} ^ {p} \hat {\beta} _ {j} (T) x _ {j} (\tau). \tag {3.89} \\ \end{array}
$$

Example 3.10 Discounted Least Squares and the Linear Trend Model To illustrate the discounted least squares procedure, let us consider the linear trend model:

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \varepsilon_ {t}, \quad t = 1, 2, \dots , T
$$

To write the parameter estimation equations in Eq. (3.88), we need the transition matrix L. For the linear trend model, this matrix is

$$
\mathbf {L} = \left[ \begin{array}{l l} 1 & 0 \\ 1 & 1 \end{array} \right]
$$

Therefore the parameter estimation equations are

$$
\hat {\boldsymbol {\beta}} (T) = \mathbf {L} ^ {\prime} \hat {\boldsymbol {\beta}} (T - 1) + \mathbf {h e} _ {T} (1)
$$

$$
\left[ \begin{array}{l} \hat {\beta} _ {0} (T) \\ \hat {\beta} _ {1} (T) \end{array} \right] = \left[ \begin{array}{l l} 1 & 1 \\ 0 & 1 \end{array} \right] \left[ \begin{array}{l} \hat {\beta} _ {0} (T - 1) \\ \hat {\beta} _ {1} (T - 1) \end{array} \right] + \left[ \begin{array}{l} h _ {1} \\ h _ {2} \end{array} \right] e _ {T} (1)
$$

or

$$
\hat {\beta} _ {0} (T) = \hat {\beta} _ {0} (T - 1) + \hat {\beta} _ {1} (T - 1) + h _ {1} e _ {1} (T) \tag {3.90}
$$

$$
\hat {\beta} _ {1} (T) = \hat {\beta} _ {1} (T - 1) + h _ {2} e _ {T} (1)
$$

The elements of the vector h are found from Eq. (3.86):

$$
\begin{array}{l} \mathbf {h} = \mathbf {G} ^ {- 1} \mathbf {x} (0) \\ = \mathbf {G} ^ {- 1} \left[ \begin{array}{c} 1 \\ 0 \end{array} \right] \\ \end{array}
$$

The steady-state matrix G is found as follows:

$$
\begin{array}{l} \mathbf {G} (T) = \sum_ {j = 0} ^ {T - 1} \theta^ {j} \mathbf {x} (- j) \mathbf {x} (- j) ^ {\prime} \\ = \sum_ {j = 0} ^ {T - 1} \theta^ {j} \left[ \begin{array}{c} 1 \\ - j \end{array} \right] \left[ \begin{array}{c c} 1 & - j \end{array} \right] \\ = \sum_ {j = 0} ^ {T - 1} \theta^ {j} \left[ \begin{array}{c c} 1 & - j \\ - j & + j ^ {2} \end{array} \right] \\ = \left[ \begin{array}{c c} \sum_ {j = 0} ^ {T - 1} \theta^ {j} & - \sum_ {j = 0} ^ {T - 1} j \theta^ {j} \\ - \sum_ {j = 0} ^ {T - 1} j \theta^ {j} & \sum_ {j = 0} ^ {T - 1} j ^ {2} \theta^ {j} \end{array} \right] \\ = \left[ \begin{array}{c c} \frac {1 - \theta^ {T}}{1 - \theta} & - \frac {\theta (1 - \theta^ {T})}{1 - \theta} \\ - \frac {\theta (1 - \theta^ {T})}{1 - \theta} & \frac {\theta (1 + \theta) (1 - \theta^ {T})}{(1 - \theta) ^ {3}} \end{array} \right] \\ \end{array}
$$

The steady-state value of $\mathbf G ( T )$ is found by taking the limit as $T \to \infty$ , which results in

$$
\begin{array}{l} \mathbf {G} = \lim  _ {T \to \infty} \mathbf {G} (T) \\ = \left[ \begin{array}{c c} \frac {1}{1 - \theta} & - \frac {\theta}{1 - \theta} \\ - \frac {\theta}{1 - \theta} & \frac {\theta (1 + \theta)}{(1 - \theta) ^ {3}} \end{array} \right] \\ \end{array}
$$

The inverse of $\mathbf { G }$ is

$$
\mathbf {G} ^ {- 1} = \left[ \begin{array}{c c} 1 - \theta^ {2} & (1 - \theta) ^ {2} \\ (1 - \theta) ^ {2} & \frac {(1 - \theta) ^ {2}}{\theta} \end{array} \right].
$$

Therefore, the vector h is

$$
\begin{array}{l} \mathbf {h} = \mathbf {G} ^ {- 1} \mathbf {x} (0) \\ = \mathbf {G} ^ {- 1} \left[ \begin{array}{c} 1 \\ 0 \end{array} \right] \\ = \left[ \begin{array}{c c} 1 - \theta^ {2} & (1 - \theta) ^ {2} \\ (1 - \theta) ^ {2} & \frac {(1 - \theta) ^ {2}}{\theta} \end{array} \right] \left[ \begin{array}{c} 1 \\ 0 \end{array} \right] \\ = \left[ \begin{array}{c} 1 - \theta^ {2} \\ (1 - \theta) ^ {2} \end{array} \right]. \\ \end{array}
$$

Substituting the elements of the vector h into Eq. (3.90) we obtain the parameter estimating equations for the linear trend model as

$$
\begin{array}{l} \hat {\beta} _ {0} (T) = \hat {\beta} _ {0} (T - 1) + \hat {\beta} _ {1} (T - 1) + (1 - \theta^ {2}) e _ {T} (1) \\ \hat {\beta} _ {1} (T) = \hat {\beta} _ {1} (T - 1) + (1 - \theta) ^ {2} e _ {T} (1) \\ \end{array}
$$

Inspection of these equations illustrates the twin aspects of discounted least squares; shifting the origin of time, and updating the parameter estimates. In the -rst equation, the updated intercept at time $T$ consists of the old intercept plus the old slope (this shifts the origin of time to the end of the current period $T$ ), plus a fraction of the current forecast error (this revises or updates the estimate of the intercept). The second equation revises the slope estimate by adding a fraction of the current period forecast error to the previous estimate of the slope.

To illustrate the computations, suppose that we are forecasting a time series with a linear trend and we have initial estimates of the slope and intercept at time $t = 0$ as

$$
\hat {\beta} _ {0} (0) = 5 0 \quad \text {a n d} \quad \hat {\beta} _ {1} (0) = 1. 5
$$

These estimates could have been obtained by regression analysis of historical data.

Assume that $\theta = 0 . 9$ , so that $1 - \theta ^ { 2 } = 1 - ( 0 . 9 ) ^ { 2 } = 0 . 1 9$ and $( 1 -$ $\theta ) ^ { 2 } = ( 1 - 0 . 9 ) ^ { 2 } = 0 . 0 1$ . The forecast for time period $t = 1$ , made at the

end of time period $t = 0$ , is computed from Eq. (3.89):

$$
\begin{array}{l} \hat {y} _ {1} (0) = \hat {\beta} (0) ^ {\prime} \mathbf {x} (1) \\ = \hat {\beta} _ {0} (0) + \hat {\beta} _ {1} (0) \\ = 5 0 + 1. 5 \\ = 5 1. 5 \\ \end{array}
$$

Suppose that the actual observation in time period 1 is $y _ { 1 } = 5 2$ . The forecast error in time period 1 is

$$
\begin{array}{l} e _ {1} (1) = y _ {1} - \hat {y} _ {1} (0) \\ = 5 2 - 5 1. 5 \\ = 0. 5. \\ \end{array}
$$

The updated estimates of the model parameter computed at the end of time period 1 are now

$$
\begin{array}{l} \hat {\beta} _ {0} (1) = \hat {\beta} _ {0} (0) + \hat {\beta} _ {1} (0) + 0. 1 9 e _ {1} (0) \\ = 5 0 + 1. 5 + 0. 1 9 (0. 5) \\ = 5 1. 6 0 \\ \end{array}
$$

and

$$
\begin{array}{l} \hat {\beta} _ {1} (1) = \hat {\beta} _ {1} (0) + 0. 0 1 e _ {1} (0) \\ = 1. 5 + 0. 0 1 (0. 5) \\ = 1. 5 5 \\ \end{array}
$$

The origin of time is now $T = 1$ . Therefore the forecast for time period 2 made at the end of period 1 is

$$
\begin{array}{l} \hat {y} _ {2} (1) = \hat {\beta} _ {0} (1) + \hat {\beta} _ {1} (1) \\ = 5 1. 6 + 1. 5 5 \\ = 5 3. 1 5. \\ \end{array}
$$

If the observation in period 2 is $y _ { 2 } = 5 5$ , we would update the parameter estimates exactly as we did at the end of time period 1. First, calculate the forecast error:

$$
\begin{array}{l} e _ {2} (1) = y _ {2} - \hat {y} _ {2} (1) \\ = 5 5 - 5 3. 1 5 \\ = 1. 8 5 \\ \end{array}
$$

Second, revise the estimates of the model parameters:

$$
\begin{array}{l} \hat {\beta} _ {0} (2) = \hat {\beta} _ {0} (1) + \hat {\beta} _ {1} (1) + 0. 1 9 e _ {2} (1) \\ = 5 1. 6 + 1. 5 5 + 0. 1 9 (1. 8 5) \\ = 5 3. 5 0 \\ \end{array}
$$

and

$$
\begin{array}{l} \hat {\beta} _ {1} (2) = \hat {\beta} _ {1} (1) + 0. 0 1 e _ {2} (1) \\ = 1. 5 5 + 0. 0 1 (1. 8 5) \\ = 1. 5 7 \\ \end{array}
$$

The forecast for period 3, made at the end of period 2, is

$$
\begin{array}{l} \hat {y} _ {3} (2) = \hat {\beta} _ {0} (2) + \hat {\beta} _ {1} (2) \\ = 5 3. 5 0 + 1. 5 7 \\ = 5 5. 0 7. \\ \end{array}
$$

Suppose that a forecast at a longer lead time than one period is required. If a forecast for time period 5 is required at the end of time period 2, then because the forecast lead time is $\tau = 5 - 2 = 3$ , the desired forecast is

$$
\begin{array}{l} \hat {y} _ {5} (2) = \hat {\beta} _ {0} (2) + \hat {\beta} _ {1} (2) 3 \\ = 5 3. 5 0 + 1. 5 7 (3) \\ = 5 8. 2 1. \\ \end{array}
$$

In general, the forecast for any lead time $\tau$ , computed at the current origin of time (the end of time period 2), is

$$
\begin{array}{l} \hat {y} _ {5} (2) = \hat {\beta} _ {0} (2) + \hat {\beta} _ {1} (2) \tau \\ = 5 3. 5 0 + 1. 5 7 \tau . \\ \end{array}
$$

When the discounted least squares procedure is applied to a linear trend model as in Example 3.9, the resulting forecasts are equivalent to the forecasts produced by a method called double exponential smoothing. Exponential smoothing is a popular and very useful forecasting technique and will be discussed in detail in Chapter 4.

Discounted least squares can be applied to more complex models. For example, suppose that the model is a polynomial of degree $k$ . The transition

matrix for this model is a square $( k + 1 ) \times ( k + 1 )$ matrix in which the diagonal elements are unity, the elements immediately to the left of the diagonal are also unity, and all other elements are zero. In this polynomial, the term of degree $r$ is written as

$$
\beta_ {r} \left( \begin{array}{c} t \\ r \end{array} \right) = \beta_ {r} \frac {t !}{(t - r) ! r !}
$$

In the next example we illustrate discounted least squares for a simple seasonal model.

Example 3.11 A Simple Seasonal Model Suppose that a time series can be modeled as a linear trend with a superimposed sine wave to represent a seasonal pattern that is observed monthly. The model is a variation of the one shown in Eq. (3.3):

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \beta_ {2} \sin \frac {2 \pi}{d} t + \beta_ {3} \cos \frac {2 \pi}{d} t + \varepsilon \tag {3.91}
$$

Since this model represents monthly data, $d = 1 2$ , Eq. (3.91) becomes

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \beta_ {2} \sin \frac {2 \pi}{1 2} t + \beta_ {3} \cos \frac {2 \pi}{1 2} t + \varepsilon \tag {3.92}
$$

The transition matrix L for this model, which contains a mixture of polynomial and trigonometric terms, is

$$
\mathbf {L} = \left[ \begin{array}{l l l l} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & \cos \frac {2 \pi}{1 2} & \sin \frac {2 \pi}{1 2} \\ 0 & 0 & - \sin \frac {2 \pi}{1 2} & \cos \frac {2 \pi}{1 2} \end{array} \right].
$$

Note that L has a block diagonal structure, with the -rst block containing the elements for the polynomial portion of the model and the second block containing the elements for the trigonometric terms, and the remaining

elements of the matrix are zero. The parameter estimation equations for this model are

$$
\hat {\boldsymbol {\beta}} (T) = L ^ {\prime} \hat {\boldsymbol {\beta}} (T - 1) + h e _ {T} (1)
$$

$$
\left[ \begin{array}{l} \hat {\beta} _ {0} (T) \\ \hat {\beta} _ {1} (T) \\ \hat {\beta} _ {2} (T) \\ \hat {\beta} _ {3} (T) \end{array} \right] = \left[ \begin{array}{l l l l} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & \cos \frac {2 \pi}{1 2} & \sin \frac {2 \pi}{1 2} \\ 0 & 0 & - \sin \frac {2 \pi}{1 2} & \cos \frac {2 \pi}{1 2} \end{array} \right] \left[ \begin{array}{l} \hat {\beta} _ {0} (T - 1) \\ \hat {\beta} _ {1} (T - 1) \\ \hat {\beta} _ {2} (T - 1) \\ \hat {\beta} _ {3} (T - 1) \end{array} \right] + \left[ \begin{array}{l} h _ {1} \\ h _ {2} \\ h _ {3} \\ h _ {4} \end{array} \right] e _ {T} (1)
$$

or

$$
\hat {\beta} _ {0} (T) = \hat {\beta} _ {0} (T - 1) + \hat {\beta} _ {1} (T - 1) + h _ {1} e _ {T} (1)
$$

$$
\hat {\beta} _ {1} (T) = \hat {\beta} _ {1} (T - 1) + h _ {2} e _ {T} (1)
$$

$$
\hat {\beta} _ {2} (T) = \cos \frac {2 \pi}{1 2} \hat {\beta} _ {2} (T - 1) - \sin \frac {2 \pi}{1 2} \hat {\beta} _ {3} (T - 1) + h _ {3} e _ {T} (1)
$$

$$
\hat {\beta} _ {3} (T) = \sin \frac {2 \pi}{1 2} \hat {\beta} _ {2} (T - 1) + \cos \frac {2 \pi}{1 2} \hat {\beta} _ {3} (T - 1) + h _ {4} e _ {T} (1)
$$

and since $2 \pi / 1 2 = 3 0 ^ { \circ }$ , these equations become

$$
\hat {\beta} _ {0} (T) = \hat {\beta} _ {0} (T - 1) + \hat {\beta} _ {1} (T - 1) + h _ {1} e _ {T} (1)
$$

$$
\hat {\beta} _ {1} (T) = \hat {\beta} _ {1} (T - 1) + h _ {2} e _ {T} (1)
$$

$$
\hat {\beta} _ {2} (T) = 0. 8 6 6 \hat {\beta} _ {2} (T - 1) - 0. 5 \hat {\beta} _ {3} (T - 1) + h _ {3} e _ {T} (1)
$$

$$
\hat {\beta} _ {3} (T) = 0. 5 \hat {\beta} _ {2} (T - 1) + 0. 8 6 6 \hat {\beta} _ {3} (T - 1) + h _ {4} e _ {T} (1)
$$

The steady-state G matrix for this model is

$$
\mathbf {G} = \left[ \begin{array}{c c c c} \sum_ {k = 0} ^ {\infty} \theta^ {k} & - \sum_ {k = 0} ^ {\infty} k \theta^ {k} & - \sum_ {k = 0} ^ {\infty} \theta^ {k} \sin \omega k & \sum_ {k = 0} ^ {\infty} \theta^ {k} \cos \omega k \\ & \sum_ {k = 0} ^ {\infty} k ^ {2} \theta^ {k} & \sum_ {k = 0} ^ {\infty} k \theta^ {k} \sin \omega k & - \sum_ {k = 0} ^ {\infty} k \theta^ {k} \cos \omega k \\ & & \sum_ {k = 0} ^ {\infty} \theta^ {k} \sin \omega k \sin \omega k & - \sum_ {k = 0} ^ {\infty} \theta^ {k} \sin \omega k \cos \omega k \\ & & & \sum_ {k = 0} ^ {\infty} \theta^ {k} \cos \omega k \cos \omega k \end{array} \right]
$$

where we have let $\omega = 2 \pi / 1 2$ . Because G is symmetric, we only need to show the upper half of the matrix. It turns out that there are closed-form expressions for all of the entries in G. We will evaluate these expressions for $\theta = 0 . 9$ . This gives the following:

$$
\sum_ {k = 0} ^ {\infty} \theta^ {k} = \frac {1}{1 - \theta} = \frac {1}{1 - 0 . 9} = 1 0
$$

$$
\sum_ {k = 0} ^ {\infty} k \theta^ {k} = \frac {\theta}{(1 - \theta) ^ {2}} = \frac {0 . 9}{(1 - 0 . 9) ^ {2}} = 9 0
$$

$$
\sum_ {k = 0} ^ {\infty} k ^ {2} \theta^ {k} = \frac {\theta (1 + \theta)}{(1 - \theta) ^ {3}} = \frac {0 . 9 (1 + 0 . 9)}{(1 - 0 . 9) ^ {3}} = 1 7 1 0
$$

for the polynomial terms and

$$
\sum_ {k = 0} ^ {\infty} \theta^ {k} \sin \omega k = \frac {\theta \sin \omega}{1 - 2 \theta \cos \omega + \theta^ {2}} = \frac {(0 . 9) 0 . 5}{1 - 2 (0 . 9) 0 . 8 6 6 + (0 . 9) ^ {2}} = 1. 7 9
$$

$$
\sum_ {k = 0} ^ {\infty} \theta^ {k} \cos \omega k = \frac {1 - \theta \cos \omega}{1 - 2 \theta \cos \omega + \theta^ {2}} = \frac {1 - (0 . 9) 0 . 8 6 6}{1 - 2 (0 . 9) 0 . 8 6 6 + (0 . 9) ^ {2}} = 0. 8 8 2 4
$$

$$
\begin{array}{l} \sum_ {k = 0} ^ {\infty} k \theta^ {k} \sin \omega k = \frac {\theta (1 - \theta^ {2}) \sin \omega}{(1 - 2 \theta \cos \omega + \theta^ {2}) ^ {2}} = \frac {0 . 9 [ 1 - (0 . 9) ^ {2} ] 0 . 5}{[ 1 - 2 (0 . 9) 0 . 8 6 6 + (0 . 9) ^ {2} ] ^ {2}} \\ = 1. 3 6 8 \\ \end{array}
$$

$$
\begin{array}{l} \sum_ {k = 0} ^ {\infty} k \theta^ {k} \cos \omega k = \frac {2 \theta^ {2} - \theta (1 + \theta^ {2}) \cos \omega}{(1 - 2 \theta \cos \omega + \theta^ {2}) ^ {2}} = \frac {2 (0 . 9) ^ {2} - 0 . 9 [ 1 + (0 . 9) ^ {2} ] 0 . 8 6 6}{[ 1 - 2 (0 . 9) 0 . 8 6 6 + (0 . 9) ^ {2} ] ^ {2}} \\ = 3. 3 4 8 6 \\ \end{array}
$$

$$
\begin{array}{l} \sum_ {k = 0} ^ {\infty} \theta^ {k} \sin \omega k \sin \omega k = - \frac {1}{2} \left[ \frac {1 - \theta \cos (2 \omega)}{1 - 2 \theta \cos (2 \omega) + \theta^ {2}} - \frac {1 - \theta \cos (0)}{1 - 2 \theta \cos (0) + \theta^ {2}} \right] \\ - \frac {1}{2} \left[ \frac {1 - 0 . 9 (0 . 5)}{1 - 2 (0 . 9) 0 . 5 + (0 . 9) ^ {2}} - \frac {1 - 0 . 9 (1)}{1 - 2 (0 . 9) (1) + (0 . 9) ^ {2}} \right] \\ = 4. 7 5 2 8 \\ \end{array}
$$

$$
\begin{array}{l} \sum_ {k = 0} ^ {\infty} \theta^ {k} \sin \omega k \cos \omega k = \frac {1}{2} \left[ \frac {\theta \sin (2 \omega)}{1 - 2 \theta \cos (2 \omega) + \theta^ {2}} - \frac {\theta \sin (0)}{1 - 2 \theta \cos (0) + \theta^ {2}} \right] \\ = \frac {1}{2} \left[ \frac {0 . 9 (0 . 8 6 6)}{1 - 2 (0 . 9) 0 . 5 + (0 . 9) ^ {2}} + \frac {0 . 9 (0)}{1 - 2 (0 . 9) 1 + (0 . 9) ^ {2}} \right] \\ = 0. 4 2 8 4 \\ \end{array}
$$

$$
\begin{array}{l} \sum_ {k = 0} ^ {\infty} \theta^ {k} \cos \omega k \cos \omega k = \frac {1}{2} \left[ \frac {1 - \theta \cos (2 \omega)}{1 - 2 \theta \cos (2 \omega) + \theta^ {2}} + \frac {1 - \theta \cos (0)}{1 - 2 \theta \cos (0) + \theta^ {2}} \right] \\ = \frac {1}{2} \left[ \frac {1 - 0 . 9 (0 . 5)}{1 - 2 (0 . 9) 0 . 5 + (0 . 9) ^ {2}} + \frac {1 - 0 . 9 (1)}{1 - 2 (0 . 9) (1) + (0 . 9) ^ {2}} \right] \\ = 5. 3 0 2 2 \\ \end{array}
$$

for the trignometric terms. Therefore the G matrix is

$$
\mathbf {G} = \left[ \begin{array}{c c c c} 1 0 & - 9 0 & - 1. 7 9 & 0. 8 8 2 4 \\ & 1 7 4 0 & 1. 3 6 8 & - 3. 3 4 8 6 \\ & & 4. 7 5 2 8 & - 0. 4 2 8 4 \\ & & & 5. 3 0 2 2 \end{array} \right]
$$

and $\mathbf { G } ^ { - 1 }$ is

$$
\mathbf {G} ^ {- 1} = \left[ \begin{array}{c c c c} 0. 2 1 4 4 0 1 & 0. 0 1 9 8 7 & 0. 0 7 5 5 4 5 & - 0. 0 2 2 6 4 \\ 0. 0 1 9 8 7 & 0. 0 0 1 1 3 8 & 0. 0 0 3 7 3 7 & - 0. 0 0 0 8 1 \\ 0. 0 7 5 5 4 5 & 0. 0 0 3 7 3 7 & 0. 2 3 8 5 9 5 & 0. 0 0 9 0 6 6 \\ - 0. 0 2 2 6 4 & - 0. 0 0 0 8 1 & 0. 0 0 9 0 6 6 & 0. 1 9 2 5 9 1 \end{array} \right]
$$

where we have shown the entire matrix. The h vector is

$$
\begin{array}{l} \mathbf {h} = \mathbf {G} ^ {- 1} \mathbf {x} (0) \\ = \left[ \begin{array}{c c c c} 0. 2 1 4 4 0 1 & 0. 0 1 9 8 7 & 0. 0 7 5 5 4 5 & - 0. 0 2 2 6 4 \\ 0. 0 1 9 8 7 & 0. 0 0 1 1 3 8 & 0. 0 0 3 7 3 7 & - 0. 0 0 0 8 1 \\ 0. 0 7 5 5 4 5 & 0. 0 0 3 7 3 7 & 0. 2 3 8 5 9 5 & 0. 0 0 9 0 6 6 \\ - 0. 0 2 2 6 4 & - 0. 0 0 0 8 1 & 0. 0 0 9 0 6 6 & 0. 1 9 2 5 9 1 \end{array} \right] \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 1 \end{array} \right] \\ = \left[ \begin{array}{l} 0. 1 9 1 7 6 2 \\ 0. 0 1 0 1 7 9 \\ 0. 0 8 4 6 1 1 \\ 0. 1 6 9 9 5 3 \end{array} \right] \\ \end{array}
$$

Therefore the discounted least squares parameter estimation equations are

$$
\hat {\beta} _ {0} (T) = \hat {\beta} _ {0} (T - 1) + \hat {\beta} _ {1} (T - 1) + 0. 1 9 1 7 6 2 e _ {T} (1)
$$

$$
\hat {\beta} _ {1} (T) = \hat {\beta} _ {1} (T - 1) + 0. 0 1 0 1 7 9 e _ {T} (1)
$$

$$
\hat {\beta} _ {2} (T) = \cos \frac {2 \pi}{1 2} \hat {\beta} _ {2} (T - 1) - \sin \frac {2 \pi}{1 2} \hat {\beta} _ {3} (T - 1) + 0. 0 8 4 6 1 1 e _ {T} (1)
$$

$$
\hat {\beta} _ {3} (T) = \sin \frac {2 \pi}{1 2} \hat {\beta} _ {2} (T - 1) + \cos \frac {2 \pi}{1 2} \hat {\beta} _ {3} (T - 1) + 0. 1 6 9 9 5 3 e _ {T} (1)
$$

# 3.8 REGRESSION MODELS FOR GENERAL TIME SERIES DATA

Many applications of regression in forecasting involve both predictor and response variables that are time series. Regression models using time series data occur relatively often in economics, business, and many -elds of engineering. The assumption of uncorrelated or independent errors that is typically made for cross-section regression data is often not appropriate for time series data. Usually the errors in time series data exhibit some type of autocorrelated structure. You might -nd it useful at this point to review the discussion of autocorrelation in time series data from Chapter 2.

There are several sources of autocorrelation in time series regression data. In many cases, the cause of autocorrelation is the failure of the analyst to include one or more important predictor variables in the model. For example, suppose that we wish to regress the annual sales of a product in a particular region of the country against the annual advertising expenditures for that product. Now the growth in the population in that region over the period of time used in the study will also inuence the product sales. If population size is not included in the model, this may cause the errors in the model to be positively autocorrelated, because if the per capita demand for the product is either constant or increasing with time, population size is positively correlated with product sales.

The presence of autocorrelation in the errors has several effects on the OLS regression procedure. These are summarized as follows:

1. The OLS regression coef-cients are still unbiased, but they are no longer minimum-variance estimates. We know this from our study of GLS in Section 3.7.   
2. When the errors are positively autocorrelated, the residual mean square may seriously underestimate the error variance $\sigma ^ { 2 }$

Consequently, the standard errors of the regression coef-cients may be too small. As a result, con-dence and prediction intervals are shorter than they really should be, and tests of hypotheses on individual regression coef-cients may be misleading, in that they may indicate that one or more predictor variables contribute signi-cantly to the model when they really do not. Generally, underestimating the error variance $\sigma ^ { 2 }$ gives the analyst a false impression of precision of estimation and potential forecast accuracy.

3. The con-dence intervals, prediction intervals, and tests of hypotheses based on the $t$ and $F$ distributions are, strictly speaking, no longer exact procedures.

There are three approaches to dealing with the problem of autocorrelation. If autocorrelation is present because of one or more omitted predictors and if those predictor variable(s) can be identi-ed and included in the model, the observed autocorrelation should disappear. Alternatively, the WLS or GLS methods discussed in Section 3.7 could be used if there were suf-cient knowledge of the autocorrelation structure. Finally, if these approaches cannot be used, the analyst must turn to a model that speci-cally incorporates the autocorrelation structure. These models usually require special parameter estimation techniques. We will provide an introduction to these procedures in Section 3.8.2.

# 3.8.1 Detecting Autocorrelation: The Durbin鈥揥atson Test

Residual plots can be useful for the detection of autocorrelation. The most useful display is the plot of residuals versus time. If there is positive autocorrelation, residuals of identical sign occur in clusters: that is, there are not enough changes of sign in the pattern of residuals. On the other hand, if there is negative autocorrelation, the residuals will alternate signs too rapidly.

Various statistical tests can be used to detect the presence of autocorrelation. The test developed by Durbin and Watson (1950, 1951, 1971) is a very widely used procedure. This test is based on the assumption that the errors in the regression model are generated by a -rst-order autoregressive process observed at equally spaced time periods; that is,

$$
\varepsilon_ {t} = \phi \varepsilon_ {t - 1} + a _ {t}, \tag {3.93}
$$

where $\varepsilon _ { t }$ is the error term in the model at time period t, $a _ { t }$ is an $\mathrm { N I D } ( 0 , \sigma _ { a } ^ { 2 } )$ random variable, and $\phi$ is a parameter that de-nes the relationship between

successive values of the model errors $\varepsilon _ { t }$ and $\varepsilon _ { t - 1 }$ . We will require that $| \phi | < 1$ , so that the model error term in time period $t$ is equal to a fraction of the error experienced in the immediately preceding period plus a normally and independently distributed random shock or disturbance that is unique to the current period. In time series regression models $\phi$ is sometimes called the autocorrelation parameter. Thus a simple linear regression model with -rst-order autoregressive errors would be

$$
y _ {t} = \beta_ {0} + \beta_ {1} x _ {t} + \varepsilon_ {t}, \quad \varepsilon_ {t} = \phi \varepsilon_ {t - 1} + a _ {t}, \tag {3.94}
$$

where $y _ { t }$ and $x _ { t }$ are the observations on the response and predictor variables at time period $t$ .

When the regression model errors are generated by the -rst-order autoregressive process in Eq. (3.93), there are several interesting properties of these errors. By successively substituting for $\varepsilon _ { t } , \varepsilon _ { t - 1 } , \ldots$ on the right-hand side of Eq. (3.93) we obtain

$$
\varepsilon_ {t} = \sum_ {j = 0} ^ {\infty} \phi^ {j} a _ {t - j}
$$

In other words, the error term in the regression model for period $t$ is just a linear combination of all of the current and previous realizations of the $\mathrm { N I D } ( 0 , \sigma ^ { 2 } )$ random variables $a _ { t }$ . Furthermore, we can show that

$$
E (\varepsilon_ {t}) = 0
$$

$$
\operatorname {V a r} \left(\varepsilon_ {t}\right) = \sigma^ {2} = \sigma_ {a} ^ {2} \left(\frac {1}{1 - \phi^ {2}}\right) \tag {3.95}
$$

$$
\operatorname {C o v} \left(\varepsilon_ {t}, \varepsilon_ {t \pm j}\right) = \phi^ {j} \sigma_ {a} ^ {2} \left(\frac {1}{1 - \phi^ {2}}\right)
$$

That is, the errors have zero mean and constant variance but have a nonzero covariance structure unless $\phi = 0$ .

The autocorrelation between two errors that are one period apart, or the lag one autocorrelation, is

$$
\rho_ {1} = \frac {\mathrm {C o v} (\varepsilon_ {t} , \varepsilon_ {t + 1})}{\sqrt {\mathrm {V a r} (\varepsilon_ {t})} \sqrt {\mathrm {V a r} (\varepsilon_ {t})}}
$$

$$
\begin{array}{l} = \frac {\phi \sigma_ {a} ^ {2} \left(\frac {1}{1 - \phi^ {2}}\right)}{\sqrt {\sigma_ {a} ^ {2} \left(\frac {1}{1 - \phi^ {2}}\right)} \sqrt {\sigma_ {a} ^ {2} \left(\frac {1}{1 - \phi^ {2}}\right)}} \\ = \phi \\ \end{array}
$$

The autocorrelation between two errors that are $k$ periods apart is

$$
\rho_ {k} = \phi^ {k}, \quad i = 1, 2, \ldots
$$

This is called the autocorrelation function (refer to Section 2.3.2). Recall that we have required that $| \phi | < 1$ . When $\phi$ is positive, all error terms are positively correlated, but the magnitude of the correlation decreases as the errors grow further apart. Only if $\phi = 0$ are the model errors uncorrelated.

Most time series regression problems involve data with positive autocorrelation. The Durbin鈥揥atson test is a statistical test for the presence of positive autocorrelation in regression model errors. Speci-cally, the hypotheses considered in the Durbin鈥揥atson test are

$$
\begin{array}{l} H _ {0}: \phi = 0 \tag {3.96} \\ H _ {1}: \phi > 0 \\ \end{array}
$$

The Durbin鈥揥atson test statistic is

$$
d = \frac {\sum_ {t = 2} ^ {T} (e _ {t} - e _ {t - 1}) ^ {2}}{\sum_ {t = 1} ^ {T} e _ {t} ^ {2}} = \frac {\sum_ {t = 2} ^ {T} e _ {t} ^ {2} + \sum_ {t = 2} ^ {T} e _ {t - 1} ^ {2} - 2 \sum_ {t = 2} ^ {T} e _ {t} e _ {t - 1}}{\sum_ {t = 1} ^ {T} e _ {t} ^ {2}} \approx 2 (1 - r _ {1}), (3. 9 7)
$$

where the $e _ { t }$ , $t = 1 , 2 , \dots , T$ are the residuals from an OLS regression of $y _ { t }$ on $x _ { t }$ . In Eq. (3.97) $r _ { 1 }$ is the lag one autocorrelation between the residuals, so for uncorrelated errors the value of the Durbin鈥揥atson statistic should be approximately 2. Statistical testing is necessary to determine just how far away from 2 the statistic must fall in order for us to conclude that the assumption of uncorrelated errors is violated. Unfortunately, the distribution of the Durbin鈥揥atson test statistic $d$ depends on the X matrix, and this makes critical values for a statistical test dif-cult to obtain. However, Durbin and Watson (1951) show that $d$ lies between lower and upper

bounds, say, $d _ { \mathrm { L } }$ and $d _ { \mathrm { U } }$ , such that if $d$ is outside these limits, a conclusion regarding the hypotheses in Eq. (3.96) can be reached. The decision procedure is as follows:

$$
\begin{array}{l} \text {I f} d <   d _ {\mathrm {L}} \operatorname {r e j e c t} H _ {0}: \phi = 0 \\ \text {I f} d > d _ {\mathrm {U}} \text {d o n o t r e j e c t} H _ {0}: \phi = 0 \\ \end{array}
$$

If $d _ { \mathrm { L } } \leq d \leq d _ { \mathrm { U } }$ the test is inconclusive

Table A.5 in Appendix A gives the bounds $d _ { \mathrm { L } }$ and $d _ { \mathrm { U } }$ for a range of sample sizes, various numbers of predictors, and three type I error rates $\langle \alpha = 0 . 0 5$ , $\alpha = 0 . 0 2 5$ , and $\alpha = 0 . 0 1 $ ). It is clear that small values of the test statistic $d$ imply that $H _ { 0 } : \phi = 0$ should be rejected because positive autocorrelation indicates that successive error terms are of similar magnitude, and the differences in the residuals $e _ { t } - e _ { t - 1 }$ will be small. Durbin and Watson suggest several procedures for resolving inconclusive results. A reasonable approach in many of these inconclusive situations is to analyze the data as if there were positive autocorrelation present to see if any major changes in the results occur.

Situations where negative autocorrelation occurs are not often encountered. However, if a test for negative autocorrelation is desired, one can use the statistic $4 - d$ , where $d$ is de-ned in Eq. (3.97). Then the decision rules for testing the hypotheses $H _ { 0 } : \phi = 0$ versus $H _ { 1 } : \phi < 0$ are the same as those used in testing for positive autocorrelation. It is also possible to test a two-sided alternative hypothesis $( H _ { 0 } : \phi = 0$ versus $H _ { 1 } : \phi \neq 0$ ) by using both of the one-sided tests simultaneously. If this is done, the two-sided procedure has type I error $2 \alpha$ , where $\alpha$ is the type I error used for each individual one-sided test.

Example 3.12 Montgomery, Peck, and Vining (2012) present an example of a regression model used to relate annual regional advertising expenses to annual regional concentrate sales for a soft drink company. Table 3.14 presents the 20 years of these data used by Montgomery, Peck, and Vining (2012). The authors assumed that a straight-line relationship was appropriate and -t a simple linear regression model by OLS. The Minitab output for this model is shown in Table 3.15 and the residuals are shown in the last column of Table 3.14. Because these are time series data, there is a possibility that autocorrelation may be present. The plot of residuals versus time, shown in Figure 3.5, has a pattern indicative of potential autocorrelation; there is a de-nite upward trend in the plot, followed by a downward trend.

TABLE 3.14 Soft Drink Concentrate Sales Data   

<table><tr><td>Year</td><td>Sales (Units)</td><td>Expenditures (103 Dollars)</td><td>Residuals</td></tr><tr><td>1</td><td>3083</td><td>75</td><td>-32.3298</td></tr><tr><td>2</td><td>3149</td><td>78</td><td>-26.6027</td></tr><tr><td>3</td><td>3218</td><td>80</td><td>2.2154</td></tr><tr><td>4</td><td>3239</td><td>82</td><td>-16.9665</td></tr><tr><td>5</td><td>3295</td><td>84</td><td>-1.1484</td></tr><tr><td>6</td><td>3374</td><td>88</td><td>-2.5123</td></tr><tr><td>7</td><td>3475</td><td>93</td><td>-1.9671</td></tr><tr><td>8</td><td>3569</td><td>97</td><td>11.6691</td></tr><tr><td>9</td><td>3597</td><td>99</td><td>-0.5128</td></tr><tr><td>10</td><td>3725</td><td>104</td><td>27.0324</td></tr><tr><td>11</td><td>3794</td><td>109</td><td>-4.4224</td></tr><tr><td>12</td><td>3959</td><td>115</td><td>40.0318</td></tr><tr><td>13</td><td>4043</td><td>120</td><td>23.5770</td></tr><tr><td>14</td><td>4194</td><td>127</td><td>33.9403</td></tr><tr><td>15</td><td>4318</td><td>135</td><td>-2.7874</td></tr><tr><td>16</td><td>4493</td><td>144</td><td>-8.6060</td></tr><tr><td>17</td><td>4683</td><td>153</td><td>0.5753</td></tr><tr><td>18</td><td>4850</td><td>161</td><td>6.8476</td></tr><tr><td>19</td><td>5005</td><td>170</td><td>-18.9710</td></tr><tr><td>20</td><td>5236</td><td>182</td><td>-29.0625</td></tr></table>

We will use the Durbin鈥揥atson test for

$$
H _ {0}: \phi = 0
$$

$$
H _ {1}: \phi > 0
$$

The test statistic is calculated as follows:

$$
\begin{array}{l} d = \frac {\sum_ {t = 2} ^ {2 0} (e _ {t} - e _ {t - 1}) ^ {2}}{\sum_ {t = 1} ^ {2 0} e _ {t} ^ {2}} \\ = \frac {\left[ - 2 6 . 6 0 2 7 - (- 3 2 . 3 2 9 8) \right] ^ {2} + \left[ 2 . 2 1 5 4 - (- 2 6 . 6 0 2 7) \right] ^ {2} + \cdots + \left[ - 2 9 . 0 6 2 5 - (- 1 8 . 9 7 1 0) \right] ^ {2}}{\left(- 3 2 . 3 2 9 8\right) ^ {2} + \left(- 2 6 . 6 0 2 7\right) ^ {2} + \cdots + \left(- 2 9 . 0 6 2 5\right) ^ {2}} \\ = 1. 0 8 \\ \end{array}
$$

Minitab will also calculate and display the Durbin鈥揥atson statistic. Refer to the Minitab output in Table 3.15. If we use a signi-cance level

# Regression Analysis: Sales Versus Expenditures

The regression equation is

Sales $=$ 1609 + 20.1 Expenditures

TABLE 3.15 Minitab Output for the Soft Drink Concentrate Sales Data   

<table><tr><td>Predictor</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P</td></tr><tr><td>Constant</td><td>1608.51</td><td>17.02</td><td>94.49</td><td>0.000</td></tr><tr><td>Expenditures</td><td>20.0910</td><td>0.1428</td><td>140.71</td><td>0.000</td></tr></table>

S = 20.5316 R-Sq = 99.9% R-Sq(adj) = 99.9%

Analysis of Variance

<table><tr><td>Source</td><td>DF</td><td>SS</td><td>MS</td><td>F</td><td>P</td></tr><tr><td>Regression</td><td>1</td><td>8346283</td><td>8346283</td><td>19799.11</td><td>0.000</td></tr><tr><td>Residual Error</td><td>18</td><td>7588</td><td>422</td><td></td><td></td></tr><tr><td>Total</td><td>19</td><td>8353871</td><td></td><td></td><td></td></tr></table>

Unusual Observations

<table><tr><td>Obs</td><td>Expenditures</td><td>Sales</td><td>Fit</td><td>SE Fit</td><td>Residual</td><td>St Resid</td></tr><tr><td>12</td><td>115</td><td>3959.00</td><td>3918.97</td><td>4.59</td><td>40.03</td><td>2.00R</td></tr></table>

R denotes an observation with a large standardized residual.

Durbin-Watson statistic = 1.08005

![](images/1c43e8b9ee7f06414d67fbb88759d79d8985d00d79e9e0aab8dfd4d08de9a0b4.jpg)  
FIGURE 3.5 Plot of residuals versus time for the soft drink concentrate sales model.

of 0.05, Table A.5 in Appendix A gives the critical values corresponding to one predictor variable and 20 observations as $d _ { \mathrm { L } } = 1 . 2 0$ and $d _ { \mathrm { U } } = 1 . 4 1$ . Since the calculated value of the Durbin鈥揥atson statistic $d = 1 . 0 8$ is less than $d _ { \mathrm { L } } = 1 . 2 0$ , we reject the null hypothesis and conclude that the errors in the regression model are positively autocorrelated.

# 3.8.2 Estimating the Parameters in Time Series Regression Models

A signi-cant value of the Durbin鈥揥atson statistic or a suspicious residual plot indicates a potential problem with auto correlated model errors. This could be the result of an actual time dependence in the errors or an 鈥渁rti-cial鈥?time dependence caused by the omission of one or more important predictor variables. If the apparent autocorrelation results from missing predictors and if these missing predictors can be identi-ed and incorporated into the model, the apparent autocorrelation problem may be eliminated. This is illustrated in the following example.

Example 3.13 Table 3.16 presents an expanded set of data for the soft drink concentrate sales problem introduced in Example 3.12. Because it is reasonably likely that regional population affects soft drink sales, Montgomery, Peck, and Vining (2012) provided data on regional population for each of the study years. Table 3.17 is the Minitab output for a regression model that includes as the predictor variables advertising expenditures and population. Both of these predictor variables are highly signi-cant. The last column of Table 3.16 shows the residuals from this model. Minitab calculates the Durbin鈥揥atson statistic for this model as $d = 3 . 0 5 9 3 2$ , and the $5 \%$ critical values are $d _ { \mathrm { L } } = 1 . 1 0$ and $d _ { \mathrm { U } } = 1 . 5 4$ , and since $d$ is greater than $d _ { \mathrm { U } }$ , we conclude that there is no evidence to reject the null hypothesis. That is, there is no indication of autocorrelation in the errors.

Figure 3.6 is a plot of the residuals from this regression model in time order. This plot shows considerable improvement when compared to the plot of residuals from the model using only advertising expenditures as the predictor. Therefore, we conclude that adding the new predictor population size to the original model has eliminated an apparent problem with autocorrelation in the errors.

The Cochrane鈥揙rcutt Method When the observed autocorrelation in the model errors cannot be removed by adding one or more new predictor variables to the model, it is necessary to take explicit account of the autocorrelative structure in the model and use an appropriate parameter

TABLE 3.16 Expanded Soft Drink Concentrate Sales Data for Example 3.13   

<table><tr><td>Year</td><td>Sales (Units)</td><td>Expenditures (103 Dollars)</td><td>Population</td><td>Residuals</td></tr><tr><td>1</td><td>3083</td><td>75</td><td>825,000</td><td>-4.8290</td></tr><tr><td>2</td><td>3149</td><td>78</td><td>830,445</td><td>-3.2721</td></tr><tr><td>3</td><td>3218</td><td>80</td><td>838,750</td><td>14.9179</td></tr><tr><td>4</td><td>3239</td><td>82</td><td>842,940</td><td>-7.9842</td></tr><tr><td>5</td><td>3295</td><td>84</td><td>846,315</td><td>5.4817</td></tr><tr><td>6</td><td>3374</td><td>88</td><td>852,240</td><td>0.7986</td></tr><tr><td>7</td><td>3475</td><td>93</td><td>860,760</td><td>-4.6749</td></tr><tr><td>8</td><td>3569</td><td>97</td><td>865,925</td><td>6.9178</td></tr><tr><td>9</td><td>3597</td><td>99</td><td>871,640</td><td>-11.5443</td></tr><tr><td>10</td><td>3725</td><td>104</td><td>877,745</td><td>14.0362</td></tr><tr><td>11</td><td>3794</td><td>109</td><td>886,520</td><td>-23.8654</td></tr><tr><td>12</td><td>3959</td><td>115</td><td>894,500</td><td>17.1334</td></tr><tr><td>13</td><td>4043</td><td>120</td><td>900,400</td><td>-0.9420</td></tr><tr><td>14</td><td>4194</td><td>127</td><td>904,005</td><td>14.9669</td></tr><tr><td>15</td><td>4318</td><td>135</td><td>908,525</td><td>-16.0945</td></tr><tr><td>16</td><td>4493</td><td>144</td><td>912,160</td><td>-13.1044</td></tr><tr><td>17</td><td>4683</td><td>153</td><td>917,630</td><td>1.8053</td></tr><tr><td>18</td><td>4850</td><td>161</td><td>922,220</td><td>13.6264</td></tr><tr><td>19</td><td>5005</td><td>170</td><td>925,910</td><td>-3.4759</td></tr><tr><td>20</td><td>5236</td><td>182</td><td>929,610</td><td>0.1025</td></tr></table>

estimation method. A very good and widely used approach is the procedure devised by Cochrane and Orcutt (1949).

We will describe the Cochrane鈥揙rcutt method for the simple linear regression model with -rst-order autocorrelated errors given in Eq. (3.94). The procedure is based on transforming the response variable so that $y _ { t } ^ { \prime } = y _ { t } - \phi y _ { t - 1 }$ . Substituting for $y _ { t }$ and $y _ { t - 1 }$ , the model becomes

$$
\begin{array}{l} y _ {t} ^ {\prime} = y _ {t} - \phi y _ {t - 1} \\ = \beta_ {0} + \beta_ {1} x _ {t} + \varepsilon_ {t} - \phi \left(\beta_ {0} + \beta_ {1} x _ {t - 1} + \varepsilon_ {t - 1}\right) \tag {3.98} \\ = \beta_ {0} (1 - \phi) + \beta_ {1} (x _ {t} - \phi x _ {t - 1}) + \varepsilon_ {t} - \phi \varepsilon_ {t - 1} \\ = \beta_ {0} ^ {\prime} + \beta_ {1} x _ {t} ^ {\prime} + a _ {t}, \\ \end{array}
$$

where $\beta _ { 0 } ^ { \prime } = \beta _ { 0 } ( 1 - \phi )$ and $x _ { t } ^ { \prime } = x _ { t } - \phi x _ { t - 1 }$ . Note that the error terms $a _ { t }$ in the transformed or reparameterized model are independent random variables. Unfortunately, this new reparameterized model contains an unknown

TABLE 3.17 Minitab Output for the Soft Drink Concentrate Data in Example 3.13   
Regression Analysis: Sales Versus Expenditures, Population   

<table><tr><td colspan="7">The regression equation is
Sales = 320 + 18.4 Expenditures + 0.00168 Population</td></tr><tr><td>Predictor</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P</td><td></td><td></td></tr><tr><td>Constant</td><td>320.3</td><td>217.3</td><td>1.47</td><td>0.159</td><td></td><td></td></tr><tr><td>Expenditures</td><td>18.4342</td><td>0.2915</td><td>63.23</td><td>0.000</td><td></td><td></td></tr><tr><td>Population</td><td>0.0016787</td><td>0.0002829</td><td>5.93</td><td>0.000</td><td></td><td></td></tr><tr><td>S = 12.0557</td><td>R-Sq = 100.0%</td><td colspan="3">R-Sq(adj) = 100.0%</td><td></td><td></td></tr><tr><td colspan="7">Analysis of Variance</td></tr><tr><td>Source</td><td>DF</td><td>SS</td><td>MS</td><td>F</td><td>P</td><td></td></tr><tr><td>Regression</td><td>2</td><td>8351400</td><td>4175700</td><td>28730.40</td><td>0.000</td><td></td></tr><tr><td>Residual Error</td><td>17</td><td>2471</td><td>145</td><td></td><td></td><td></td></tr><tr><td>Total</td><td>19</td><td>8353871</td><td></td><td></td><td></td><td></td></tr><tr><td>Source</td><td>DF</td><td>Seq SS</td><td></td><td></td><td></td><td></td></tr><tr><td>Expenditures</td><td>1</td><td>8346283</td><td></td><td></td><td></td><td></td></tr><tr><td>Population</td><td>1</td><td>5117</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="7">Unusual Observations</td></tr><tr><td>Obs</td><td>Expenditures</td><td>Sales</td><td>Fit</td><td>SE Fit</td><td>Residual</td><td>St Resid</td></tr><tr><td>11</td><td>109</td><td>3794.00</td><td>3817.87</td><td>4.27</td><td>-23.87</td><td>-2.12R</td></tr><tr><td colspan="7">R denotes an observation with a large standardized residual.</td></tr><tr><td colspan="7">Durbin-Watson statistic = 3.05932</td></tr></table>

parameter $\phi$ and it is also no longer linear in the unknown parameters because it involves products of $\phi , \beta _ { 0 }$ , and $\beta _ { 1 }$ . However, the -rst-order autoregressive process $\varepsilon _ { t } = \phi \varepsilon _ { t - 1 } + a _ { t }$ can be viewed as a simple linear regression through the origin and the parameter $\phi$ can be estimated by obtaining the residuals of an OLS regression of $y _ { t }$ on $x _ { t }$ and then regressing $e _ { t }$ on $e _ { t - 1 }$ . The OLS regression of $e _ { t }$ on $e _ { t - 1 }$ results in

$$
\hat {\phi} = \frac {\sum_ {t = 2} ^ {T} e _ {t} e _ {t - 1}}{\sum_ {y = 1} ^ {T} e _ {t} ^ {2}} \tag {3.99}
$$

![](images/e838e0509ffb0405a65a09265c2990d5723a01222dacacc7a7714958da412eb0.jpg)  
FIGURE 3.6 Plot of residuals versus time for the soft drink concentrate sales model in Example 3.13.

Using $\hat { \phi }$ as an estimate of $\phi$ , we can calculate the transformed response and predictor variables as

$$
y _ {t} ^ {\prime} = y _ {t} - \hat {\phi} y _ {t - 1}
$$

$$
x _ {t} ^ {\prime} = x _ {t} - \hat {\phi} x _ {t - 1}
$$

Now apply OLS to the transformed data. This will result in estimates of the transformed slope $\hat { \beta } _ { 0 } ^ { \prime }$ , the intercept $\hat { \beta } _ { 1 }$ , and a new set of residuals. The Durbin鈥揥atson test can be applied to these new residuals from the reparameterized model. If this test indicates that the new residuals are uncorrelated, then no additional analysis is required. However, if positive autocorrelation is still indicated, then another iteration is necessary. In the second iteration $\phi$ is estimated with new residuals that are obtained by using the regression coef-cients from the reparameterized model with the original regressor and response variables. This iterative procedure may be continued as necessary until the residuals indicate that the error terms in the reparameterized model are uncorrelated. Usually only one or two iterations are suf-cient to produce uncorrelated errors.

Example 3.14 Montgomery, Peck, and Vining (2012) give data on the market share of a particular brand of toothpaste for 30 time periods and the corresponding selling price per pound. These data are shown in

TABLE 3.18 Toothpaste Market Share Data   

<table><tr><td>Time</td><td>Market Share</td><td>Price</td><td>Residuals</td><td>y鈥瞭</td><td>x鈥瞭</td><td>Residuals</td></tr><tr><td>1</td><td>3.63</td><td>0.97</td><td>0.281193</td><td></td><td></td><td></td></tr><tr><td>2</td><td>4.20</td><td>0.95</td><td>0.365398</td><td>2.715</td><td>0.553</td><td>-0.189435</td></tr><tr><td>3</td><td>3.33</td><td>0.99</td><td>0.466989</td><td>1.612</td><td>0.601</td><td>0.392201</td></tr><tr><td>4</td><td>4.54</td><td>0.91</td><td>-0.266193</td><td>3.178</td><td>0.505</td><td>-0.420108</td></tr><tr><td>5</td><td>2.89</td><td>0.98</td><td>-0.215909</td><td>1.033</td><td>0.608</td><td>-0.013381</td></tr><tr><td>6</td><td>4.87</td><td>0.90</td><td>-0.179091</td><td>3.688</td><td>0.499</td><td>-0.058753</td></tr><tr><td>7</td><td>4.90</td><td>0.89</td><td>-0.391989</td><td>2.908</td><td>0.522</td><td>-0.268949</td></tr><tr><td>8</td><td>5.29</td><td>0.86</td><td>-0.730682</td><td>3.286</td><td>0.496</td><td>-0.535075</td></tr><tr><td>9</td><td>6.18</td><td>0.85</td><td>-0.083580</td><td>4.016</td><td>0.498</td><td>0.244473</td></tr><tr><td>10</td><td>7.20</td><td>0.82</td><td>0.207727</td><td>4.672</td><td>0.472</td><td>0.256348</td></tr><tr><td>11</td><td>7.25</td><td>0.79</td><td>-0.470966</td><td>4.305</td><td>0.455</td><td>-0.531811</td></tr><tr><td>12</td><td>6.09</td><td>0.83</td><td>-0.659375</td><td>3.125</td><td>0.507</td><td>-0.423560</td></tr><tr><td>13</td><td>6.80</td><td>0.81</td><td>-0.435170</td><td>4.309</td><td>0.471</td><td>-0.131426</td></tr><tr><td>14</td><td>8.65</td><td>0.77</td><td>0.443239</td><td>5.869</td><td>0.439</td><td>0.635804</td></tr><tr><td>15</td><td>8.43</td><td>0.76</td><td>-0.019659</td><td>4.892</td><td>0.445</td><td>-0.192552</td></tr><tr><td>16</td><td>8.29</td><td>0.80</td><td>0.811932</td><td>4.842</td><td>0.489</td><td>0.847507</td></tr><tr><td>17</td><td>7.18</td><td>0.83</td><td>0.430625</td><td>3.789</td><td>0.503</td><td>0.141344</td></tr><tr><td>18</td><td>7.90</td><td>0.79</td><td>0.179034</td><td>4.963</td><td>0.451</td><td>0.027093</td></tr><tr><td>19</td><td>8.45</td><td>0.76</td><td>0.000341</td><td>5.219</td><td>0.437</td><td>-0.063744</td></tr><tr><td>20</td><td>8.23</td><td>0.78</td><td>0.266136</td><td>4.774</td><td>0.469</td><td>0.284026</td></tr></table>

Table 3.18. A simple linear regression model is -t to these data, and the resulting Minitab output is in Table 3.19. The residuals are shown in Table 3.18. The Durbin鈥揥atson statistic for the residuals from this model is $d = 1 . 1 3 5 8 2$ (see the Minitab output), and the $5 \%$ critical values are $d _ { \mathrm { L } } =$ 1.20 and $d _ { \mathrm { U } } = 1 . 4 1$ , so there is evidence to support the conclusion that the residuals are positively autocorrelated.

We will use the Cochrane鈥揙rcutt method to estimate the model parameters. The autocorrelation coef-cient can be estimated using the residuals in Table 3.18 and Eq. (3.99) as follows:

$$
\begin{array}{l} \hat {\phi} = \frac {\sum_ {t = 2} ^ {T} e _ {t} e _ {t - 1}}{\sum_ {y = 1} ^ {T} e _ {t} ^ {2}} \\ = \frac {1 . 3 5 4 7}{3 . 3 0 8 3} \\ = 0. 4 0 9 \\ \end{array}
$$

TABLE 3.19 Minitab Regression Results for the Toothpaste Market Share Data   
Regression Analysis: Market Share Versus Price   

<table><tr><td colspan="6">The regression equation is
Market Share = 26.9 - 24.3 Price</td></tr><tr><td>Predictor</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P</td><td></td></tr><tr><td>Constant</td><td>26.910</td><td>1.110</td><td>24.25</td><td>0.000</td><td></td></tr><tr><td>Price</td><td>-24.290</td><td>1.298</td><td>-18.72</td><td>0.000</td><td></td></tr><tr><td>S = 0.428710</td><td colspan="2">R-Sq = 95.1%</td><td colspan="2">R-Sq(adj) = 94.8%</td><td></td></tr><tr><td colspan="6">Analysis of Variance</td></tr><tr><td>Source</td><td>DF</td><td>SS</td><td>MS</td><td>F</td><td>P</td></tr><tr><td>Regression</td><td>1</td><td>64.380</td><td>64.380</td><td>350.29</td><td>0.000</td></tr><tr><td>Residual Error</td><td>18</td><td>3.308</td><td>0.184</td><td></td><td></td></tr><tr><td>Total</td><td>19</td><td>67.688</td><td></td><td></td><td></td></tr><tr><td colspan="6">Durbin-Watson statistic = 1.13582</td></tr></table>

The transformed variables are computed according to

$$
y _ {t} ^ {\prime} = y _ {t} - 0. 4 0 9 y _ {t - 1}
$$

$$
x _ {t} ^ {\prime} = x _ {t} - 0. 4 0 9 x _ {t - 1}
$$

for t = 2, 3, 鈥?, 20. These transformed variables are also shown in Table 3.18. The Minitab results for -tting a regression model to the transformed data are summarized in Table 3.20. The residuals from the transformed model are shown in the last column of Table 3.18. The Durbin鈥揥atson statistic for the transformed model is $d = 2 . 1 5 6 7 1$ , and the $5 \%$ critical values from Table A.5 in Appendix A are $d _ { \mathrm { L } } = 1 . 1 8$ and $d _ { \mathrm { U } } = 1 . 4 0$ , so we conclude that there is no problem with autocorrelated errors in the transformed model. The Cochrane鈥揙rcutt method has been effective in removing the autocorrelation.

The slope in the transformed model $\beta _ { 1 } ^ { \prime }$ is equal to the slope in the original model, $\beta _ { 1 }$ . A comparison of the slopes in the two models in Tables 3.19 and 3.20 shows that the two estimates are very similar. However, if the standard errors are compared, the Cochrane鈥揙rcutt method produces an estimate of the slope that has a larger standard error than the standard error of the OLS estimate. This reects the fact that if the errors are autocorrelated and

TABLE 3.20 Minitab Regression Results for Fitting the Transformed Model to the Toothpaste Sales Data   

<table><tr><td colspan="7">Regression Analysis: y&#x27; Versus x&#x27;</td></tr><tr><td colspan="7">The regression equation is
y-prime = 16.1 - 24.8 x-prime</td></tr><tr><td>Predictor</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P</td><td></td><td></td></tr><tr><td>Constant</td><td>16.1090</td><td>0.9610</td><td>16.76</td><td>0.000</td><td></td><td></td></tr><tr><td>x-prime</td><td>-24.774</td><td>1.934</td><td>-12.81</td><td>0.000</td><td></td><td></td></tr><tr><td colspan="7">S = 0.390963 R-Sq = 90.6% R-Sq(adj) = 90.1%</td></tr><tr><td colspan="7">Analysis of Variance</td></tr><tr><td>Source</td><td>DF</td><td>SS</td><td>MS</td><td>F</td><td>P</td><td></td></tr><tr><td>Regression</td><td>1</td><td>25.080</td><td>25.080</td><td>164.08</td><td>0.000</td><td></td></tr><tr><td>Residual Error</td><td>17</td><td>2.598</td><td>0.153</td><td></td><td></td><td></td></tr><tr><td>Total</td><td>18</td><td>27.679</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="7">Unusual Observations</td></tr><tr><td>Obs</td><td>x-prime</td><td>y-prime</td><td>Fit</td><td>SE Fit</td><td>Residual</td><td>St Resid</td></tr><tr><td>2</td><td>0.601</td><td>1.6120</td><td>1.2198</td><td>0.2242</td><td>0.3922</td><td>1.22 X</td></tr><tr><td>4</td><td>0.608</td><td>1.0330</td><td>1.0464</td><td>0.2367</td><td>-0.0134</td><td>-0.04 X</td></tr><tr><td>15</td><td>0.489</td><td>4.8420</td><td>3.9945</td><td>0.0904</td><td>0.8475</td><td>2.23R</td></tr><tr><td colspan="7">R denotes an observation with a large standardized residual.
X denotes an observation whose X value gives it large influence.
Durbin-Watson statistic = 2.15671</td></tr></table>

OLS is used, the standard errors of the model coef-cients are likely to be underestimated.

The Maximum Likelihood Approach There are other alternatives to the Cochrane鈥揙rcutt method. A popular approach is to use the method of maximum likelihood to estimate the parameters in a time series regression model. We will concentrate on the simple linear regression model with -rst-order autoregressive errors

$$
y _ {t} = \beta_ {0} + \beta_ {1} x _ {t} + \varepsilon_ {t}, \quad \varepsilon_ {t} = \phi \varepsilon_ {t - 1} + a _ {t} \tag {3.100}
$$

One reason that the method of maximum likelihood is so attractive is that, unlike the Cochrane鈥揙rcutt method, it can be used in situations where the autocorrelative structure of the errors is more complicated than -rst-order autoregressive.

For readers unfamiliar with maximum likelihood estimation, we will present a simple example. Consider the time series model

$$
y _ {t} = \mu + a _ {t}, \tag {3.101}
$$

where $a _ { t }$ is $N ( 0 , \sigma ^ { 2 } )$ and $\mu$ is unknown. This is a time series model for a process that varies randomly around a -xed level $( \mu )$ and for which there is no autocorrelation. We will estimate the unknown parameter $\mu$ using the method of maximum likelihood.

Suppose that there are $T$ observations available, $y _ { 1 } , y _ { 2 } , \ldots , y _ { T }$ . The probability distribution of any observation is normal, that is,

$$
\begin{array}{l} f (y _ {t}) = \frac {1}{\sigma \sqrt {2 \pi}} e ^ {- [ (y _ {t} - \mu) / \sigma ] ^ {2} / 2} \\ = \frac {1}{\sigma \sqrt {2 \pi}} e ^ {- (a _ {t} / \sigma) ^ {2} / 2} \\ \end{array}
$$

The likelihood function is just the joint probability density function of the sample. Because the observations $y _ { 1 }$ $y _ { 1 } , y _ { 2 } , \ldots , y _ { T }$ are independent, the likelihood function is just the product of the individual density functions, or

$$
\begin{array}{l} l (y _ {t}, \mu) = \prod_ {t = 1} ^ {T} f (y _ {t}) \\ = \prod_ {t = 1} ^ {T} \frac {1}{\sigma \sqrt {2 \pi}} e ^ {- \left(a _ {t} / \sigma\right) ^ {2} / 2} \tag {3.102} \\ = \left(\frac {1}{\sigma \sqrt {2 \pi}}\right) ^ {T} \exp \left(- \frac {1}{2 \sigma^ {2}} \sum_ {t = 1} ^ {T} a _ {t} ^ {2}\right) \\ \end{array}
$$

The maximum likelihood estimator of $\mu$ is the value of the parameter that maximizes the likelihood function. It is often easier to work with the log-likelihood, and this causes no problems because the value of $\mu$ that maximizes the likelihood function also maximizes the log-likelihood.

The log-likelihood is

$$
\ln l (y _ {t} \mu) = - \frac {T}{2} \ln (2 \pi) - T \ln \sigma - \frac {1}{2 \sigma^ {2}} \sum_ {t = 1} ^ {T} a _ {t} ^ {2}
$$

Suppose that $\sigma ^ { 2 }$ is known. Then to maximize the log-likelihood we would choose the estimate of $\mu$ that minimizes

$$
\sum_ {t = 1} ^ {T} a _ {t} ^ {2} = \sum_ {t = 1} ^ {T} (y _ {t} - \mu) ^ {2}
$$

Note that this is just the error sum of squares from the model in Eq. (3.101). So, in the case of normally distributed errors, the maximum likelihood estimator of $\mu$ is identical to the least squares estimator of $\mu$ . It is easy to show that this estimator is just the sample average; that is,

$$
\hat {\mu} = \bar {y}
$$

Suppose that the mean of the model in Eq. (3.101) is a linear regression function of time, say,

$$
\mu = \beta_ {0} + \beta_ {1} t
$$

so that the model is

$$
y _ {t} = \mu + a _ {t} = \beta_ {0} + \beta_ {1} t + a _ {t}
$$

with independent and normally distributed errors. The likelihood function for this model is identical to Eq. (3.102), and, once again, the maximum likelihood estimators of the model parameters $\beta _ { 0 }$ and $\beta _ { 1 }$ are found by minimizing the error sum of squares from the model. Thus when the errors are normally and independently distributed, the maximum likelihood estimators of the model parameters $\beta _ { 0 }$ and $\beta _ { 1 }$ in the linear regression model are identical to the least squares estimators.

Now let us consider the simple linear regression model with -rst-order autoregressive errors, -rst introduced in Eq. (3.94), and repeated for convenience below:

$$
y _ {t} = \beta_ {0} + \beta_ {1} x _ {t} + \varepsilon_ {t}, \quad \varepsilon_ {t} = \phi \varepsilon_ {t - 1} + a _ {t}
$$

Recall that the $a$ 鈥檚 are normally and independently distributed with mean zero and variance $\sigma _ { a } ^ { 2 }$ and $\phi$ is the autocorrelation parameter. Write this equation for $y _ { t - 1 }$ and subtract $\phi y _ { t - 1 }$ from $y _ { t }$ . This results in

$$
y _ {t} - \phi y _ {t - 1} = (1 - \phi) \beta_ {0} + \beta_ {1} (x _ {t} - \phi x _ {t - 1}) + a _ {t}
$$

or

$$
\begin{array}{l} y _ {t} = \phi y _ {t - 1} + (1 - \phi) \beta_ {0} + \beta_ {1} \left(x _ {t} - \phi x _ {t - 1}\right) + a _ {t} \tag {3.103} \\ = \mu (\mathbf {z} _ {t}, \boldsymbol {\theta}) + a _ {t}, \\ \end{array}
$$

where $\mathbf { z } _ { t } ^ { \prime } = [ y _ { t - 1 } , x _ { t } ]$ and $\pmb { \theta } ^ { \prime } = [ \phi , \beta _ { 0 } , \beta _ { 1 } ]$ . We can think of $\mathbf { z } _ { t }$ as a vector of predictor variables and $\pmb \theta$ as the vector of regression model parameters. Since $y _ { t - 1 }$ appears on the right-hand side of the model in Eq. (3.103), the index of time must run from 2, 3, 鈥?, T. At time period $t = 2$ , we treat $y _ { 1 }$ as an observed predictor.

Because the a鈥檚 are normally and independently distributed, the joint probability density of the $a$ 鈥檚 is

$$
\begin{array}{l} f (a _ {2}, a _ {3}, \dots , a _ {T}) = \prod_ {t = 2} ^ {T} \frac {1}{\sigma_ {a} \sqrt {2 \pi}} e ^ {- (a _ {t} / \sigma_ {a}) ^ {2} / 2} \\ = \left(\frac {1}{\sigma_ {a} \sqrt {2 \pi}}\right) ^ {T - 1} \exp \left(- \frac {1}{2 \sigma_ {a} ^ {2}} \sum_ {t = 1} ^ {T} a _ {t} ^ {2}\right) \\ \end{array}
$$

and the likelihood function is obtained from this joint distribution by substituting for the a鈥檚:

$$
\begin{array}{l} l (y _ {t}, \phi , \beta_ {0}, \beta_ {1}) = \left(\frac {1}{\sigma_ {a} \sqrt {2 \pi}}\right) ^ {T - 1} \\ \exp \left(- \frac {1}{2 \sigma_ {a} ^ {2}} \sum_ {t = 2} ^ {T} \left\{y _ {t} - \left[ \phi y _ {t - 1} + (1 - \phi) \beta_ {0} + \beta_ {1} \left(x _ {t} - \phi x _ {t - 1}\right) \right] \right\} ^ {2}\right) \\ \end{array}
$$

The log-likelihood is

$$
\begin{array}{l} \ln l (y _ {t}, \phi , \beta_ {0}, \beta_ {1}) = - \frac {T - 1}{2} \ln (2 \pi) - (T - 1) \ln \sigma_ {a} \\ - \frac {1}{2 \sigma_ {a} ^ {2}} \sum_ {t = 2} ^ {T} \left\{y _ {t} - [ \phi y _ {t - 1} + (1 - \phi) \beta_ {0} + \beta_ {1} (x _ {t} - \phi x _ {t - 1}) ] \right\} ^ {2} \\ \end{array}
$$

This log-likelihood is maximized with respect to the parameters $\phi , \beta _ { 0 }$ , and $\beta _ { 1 }$ by minimizing the quantity

$$
S S _ {\mathrm {E}} = \sum_ {t = 2} ^ {T} \left\{y _ {t} - \left[ \phi y _ {t - 1} + (1 - \phi) \beta_ {0} + \beta_ {1} \left(x _ {t} - \phi x _ {t - 1}\right) \right] \right\} ^ {2}, \tag {3.104}
$$

which is the error sum of squares for the model. Therefore the maximum likelihood estimators of $\phi , \beta _ { 0 }$ , and $\beta _ { 1 }$ are also least squares estimators.

There are two important points about the maximum likelihood (or least squares) estimators. First, the sum of squares in Eq. (3.104) is conditional on the initial value of the time series, $y _ { 1 }$ . Therefore the maximum likelihood (or least squares) estimators found by minimizing this conditional sum of squares are conditional maximum likelihood (or conditional least squares) estimators. Second, because the model involves products of the parameters $\phi$ and $\beta _ { 0 }$ , the model is no longer linear in the unknown parameters. That is, it is not a linear regression model and consequently we cannot give an explicit closed-form solution for the parameter estimators. Iterative methods for -tting nonlinear regression models must be used. These procedures work by linearizing the model about a set of initial guesses for the parameters, solving the linearized model to obtain improved parameter estimates, then using the improved estimates to de-ne a new linearized model, which leads to new parameter estimates and so on. The details of -tting nonlinear models by least squares are discussed in Montgomery, Peck, and Vining (2012).

Suppose that we have obtained a set of parameter estimates, say, $\hat { \theta } ^ { \prime } = \mathbf { \Phi }$ $[ \hat { \phi } , \hat { \beta } _ { 0 } , \hat { \beta } _ { 1 } ]$ . The maximum likelihood estimate of $\sigma _ { a } ^ { 2 }$ is computed as

$$
\hat {\sigma} _ {a} ^ {2} = \frac {S S _ {\mathrm {E}} (\hat {\boldsymbol {\theta}})}{n - 1}, \tag {3.105}
$$

where $S S _ { \mathrm { E } } ( \hat { \boldsymbol { \theta } } )$ is the error sum of squares in Eq. (3.104) evaluated at the conditional maximum likelihood (or conditional least squares) parameter estimates $\hat { \pmb { \theta } } ^ { \prime } = [ \hat { \phi } , \hat { \beta } _ { 0 } , \hat { \beta } _ { 1 } ]$ . Some authors (and computer programs) use an adjusted number of degrees of freedom in the denominator to account for the number of parameters that have been estimated. If there are $k$ predictors, then the number of estimated parameters will be $p = k + 3$ , and the formula for estimating $\sigma _ { a } ^ { 2 }$ is

$$
\hat {\sigma} _ {a} ^ {2} = \frac {S S _ {\mathrm {E}} (\hat {\theta})}{n - p - 1} = \frac {S S _ {\mathrm {E}} (\hat {\theta})}{n - k - 4} \tag {3.106}
$$

In order to test hypotheses about the model parameters and to -nd con-dence intervals, standard errors of the model parameters are needed. The standard errors are usually found by expanding the nonlinear model in a -rst-order Taylor series around the -nal estimates of the parameters $\hat { \pmb { \theta } } ^ { \prime } = [ \hat { \phi } , \hat { \beta } _ { 0 } , \hat { \beta } _ { 1 } ]$ . This results in

$$
y _ {t} \approx \mu (\mathbf {z} _ {t}, \hat {\boldsymbol {\theta}}) + (\boldsymbol {\theta} - \hat {\boldsymbol {\theta}}) ^ {\prime} \frac {\partial \mu (\mathbf {z} _ {t} , \boldsymbol {\theta})}{\partial \boldsymbol {\theta}} \bigg | _ {\boldsymbol {\theta} = \hat {\boldsymbol {\theta}}} + a _ {t}
$$

The column vector of derivatives, $\partial \mu ( \mathbf { z } _ { t } , \pmb { \theta } ) / \partial \pmb { \theta }$ , is found by differentiating the model with respect to each parameter in the vector $\theta ^ { \prime } = [ \phi , \beta _ { 0 } , \beta _ { 1 } ]$ . This vector of derivatives is

$$
\frac {\partial \mu (\mathbf {z} _ {t} , \boldsymbol {\theta})}{\partial \boldsymbol {\theta}} = \left[ \begin{array}{c} 1 - \phi \\ x _ {t} - \phi x _ {t - 1} \\ y _ {t - 1} - \beta_ {0} - \beta_ {1} x _ {t - 1} \end{array} \right]
$$

This vector is evaluated for each observation at the set of conditional maximum likelihood parameter estimates $\hat { \pmb { \theta } } ^ { \prime } = [ \hat { \phi } , \hat { \beta } _ { 0 } , \hat { \beta } _ { 1 } ]$ and assembled into an X matrix. Then the covariance matrix of the parameter estimates is found from

$$
\operatorname {C o v} (\hat {\boldsymbol {\theta}}) = \sigma_ {a} ^ {2} (\mathbf {X} ^ {\prime} \mathbf {X}) ^ {- 1}
$$

When $\sigma _ { a } ^ { 2 }$ is replaced by the estimate $\hat { \sigma } _ { a } ^ { 2 }$ from Eq. (3.106) an estimate of the covariance matrix results, and the standard errors of the model parameters are the main diagonals of the covariance matrix.

Example 3.15 We will -t the regression model with time series errors in Eq. (3.104) to the toothpaste market share data originally analyzed in Example 3.14. We will use a widely available software package, SAS (the Statistical Analysis System). The SAS procedure for -tting regression models with time series errors is SAS PROC AUTOREG. Table 3.21 contains the output from this software program for the toothpaste market share data. Note that the autocorrelation parameter (or the lag one autocorrelation) is estimated to be 0.4094, which is very similar to the value obtained by the Cochrane鈥揙rcutt method. The overall $R ^ { 2 }$ for this model is 0.9601, and we can show that the residuals exhibit no autocorrelative structure, so this is likely a reasonable model for the data.

There is, of course, some possibility that a more complex autocorrelation structure than -rst-order may exist. SAS PROC AUTOREG can -t

TABLE 3.21 SAS PROC AUTOREG Output for the Toothpaste Market Share Data, Assuming First-Order Autoregressive Errors   

<table><tr><td colspan="11">The SAS System
The AUTOREG Procedure
Dependent Variable y</td><td></td></tr><tr><td colspan="11">Ordinary Least Squares Estimates</td><td></td></tr><tr><td>SSE</td><td>3.30825739</td><td>DFE</td><td></td><td></td><td></td><td></td><td>18</td><td></td><td></td><td></td><td></td></tr><tr><td>MSE</td><td>0.18379</td><td>Root MSE</td><td></td><td></td><td></td><td></td><td>0.42871</td><td></td><td></td><td></td><td></td></tr><tr><td>SBC</td><td>26.762792</td><td>AIC</td><td></td><td></td><td></td><td></td><td>24.7713275</td><td></td><td></td><td></td><td></td></tr><tr><td>Regress R-Square</td><td>0.9511</td><td>Total R-Square</td><td></td><td></td><td></td><td></td><td>0.9511</td><td></td><td></td><td></td><td></td></tr><tr><td>Durbin-Watson</td><td>1.1358</td><td>Pr &lt; DW</td><td></td><td></td><td></td><td></td><td>0.0098</td><td></td><td></td><td></td><td></td></tr><tr><td>Pr &gt; DW</td><td>0.9902</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="11">NOTE: Pr&lt;DW is the p-value for testing positive autocorrelation, and Pr&gt;DW is the p-value for testing negative autocorrelation.</td><td></td></tr><tr><td>Standard</td><td></td><td></td><td></td><td></td><td></td><td></td><td>Approx</td><td colspan="3">Variable</td><td></td></tr><tr><td>Variable</td><td>DF</td><td>Estimate</td><td>Error</td><td>t Value</td><td></td><td></td><td>Pr &gt; |t|</td><td colspan="3">Label</td><td></td></tr><tr><td>Intercept</td><td>1</td><td>26.9099</td><td>1.1099</td><td>24.25</td><td></td><td></td><td>&lt;.0001</td><td></td><td></td><td></td><td></td></tr><tr><td>x</td><td>1</td><td>-24.2898</td><td>1.2978</td><td>-18.72</td><td></td><td></td><td>&lt;.0001</td><td>x</td><td></td><td></td><td></td></tr><tr><td colspan="11">Estimates of Autocorrelations</td><td></td></tr><tr><td>Lag</td><td>Covariance</td><td>Correlation</td><td>-1</td><td>9</td><td>8</td><td>7</td><td>6</td><td>5</td><td>4</td><td>3</td><td>2</td></tr><tr><td>0</td><td>0.1654</td><td>1.000000</td><td>|</td><td></td><td></td><td></td><td></td><td colspan="3">**********</td><td></td></tr><tr><td>1</td><td>0.0677</td><td>0.409437</td><td>|</td><td></td><td></td><td></td><td></td><td colspan="3">**********</td><td></td></tr><tr><td colspan="11">Preliminary MSE 0.1377</td><td></td></tr><tr><td colspan="11">Estimates of Autoregressive Parameters</td><td></td></tr><tr><td>Standard</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Lag</td><td>Coefficient</td><td>Error</td><td>t Value</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>-0.409437</td><td>0.221275</td><td>-1.85</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="11">Algorithm converged.</td><td></td></tr></table>

more complex patterns. Since there is obviously -rst-order autocorrelation present, an obvious possibility is that the autocorrelation might be second-order autoregressive, as in

$$
\varepsilon_ {t} = \phi_ {1} \varepsilon_ {t - 1} + \phi_ {2} \varepsilon_ {t - 2} + a _ {t},
$$

where the parameters $\phi _ { 1 }$ and $\phi _ { 2 }$ are autocorrelations at lags one and two, respectively. The output from SAS PROC AUTOREG for this model is in Table 3.22. The $t$ -statistic for the lag two autocorrelation is not signi-cant so there is no reason to believe that this more complex autocorrelative

TABLE 3.21 (Continued)   

<table><tr><td colspan="7">The SAS System
The AUTOREG Procedure
Maximum Likelihood Estimates</td></tr><tr><td>SSE</td><td>2.69864377</td><td>DFE</td><td></td><td>17</td><td></td><td></td></tr><tr><td>MSE</td><td>0.15874</td><td>Root MSE</td><td></td><td>0.39843</td><td></td><td></td></tr><tr><td>SBC</td><td>25.8919447</td><td>AIC</td><td></td><td>22.9047479</td><td></td><td></td></tr><tr><td>Regress R-Square</td><td>0.9170</td><td>Total R-Square</td><td></td><td>0.9601</td><td></td><td></td></tr><tr><td>Durbin-Watson</td><td>1.8924</td><td>Pr &lt; DW</td><td></td><td>0.3472</td><td></td><td></td></tr><tr><td>Pr &gt; DW</td><td>0.6528</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="7">NOTE: Pr&lt;DW is the p-value for testing positive autocorrelation, and Pr&gt;DW is the p-value for testing negative autocorrelation.</td></tr><tr><td>Standard</td><td></td><td></td><td></td><td></td><td>Approx</td><td>Variable</td></tr><tr><td>Variable</td><td>DF</td><td>Estimate</td><td>Error</td><td>t Value</td><td>Pr &gt; |t|</td><td>Label</td></tr><tr><td>Intercept</td><td>1</td><td>26.3322</td><td>1.4777</td><td>17.82</td><td>&lt;.0001</td><td></td></tr><tr><td>x</td><td>1</td><td>-23.5903</td><td>1.7222</td><td>-13.70</td><td>&lt;.0001</td><td>x</td></tr><tr><td>AR1</td><td>1</td><td>-0.4323</td><td>0.2203</td><td>-1.96</td><td>0.0663</td><td></td></tr><tr><td colspan="7">Autoregressive parameters assumed given.</td></tr><tr><td>Standard</td><td></td><td>Approx</td><td>Variable</td><td></td><td></td><td></td></tr><tr><td>Variable</td><td>DF</td><td>Estimate</td><td>Error</td><td>t Value</td><td>Pr &gt; |t|</td><td>Label</td></tr><tr><td>Intercept</td><td>1</td><td>26.3322</td><td>1.4776</td><td>17.82</td><td>&lt;.0001</td><td></td></tr><tr><td>x</td><td>1</td><td>-23.5903</td><td>1.7218</td><td>-13.70</td><td>&lt;.0001</td><td>x</td></tr></table>

structure is necessary to adequately model the data. The model with -rstorder autoregressive errors is satisfactory.

Forecasting and Prediction Intervals We now consider how to obtain forecasts at any lead time using a time series model. It is very tempting to ignore the autocorrelation in the data when forecasting, and simply substitute the conditional maximum likelihood estimates into the regression equation:

$$
\hat {y} _ {t} = \hat {\beta} _ {0} + \hat {\beta} _ {1} x _ {t}
$$

Now suppose that we are at the end of the current time period, $T$ , and we wish to obtain a forecast for period $T + 1$ . Using the above equation, this results in

$$
\hat {y} _ {T + 1} (T) = \hat {\beta} _ {0} + \hat {\beta} _ {1} x _ {T + 1},
$$

assuming that the value of the predictor variable in the next time period $x _ { T + 1 }$ is known.

TABLE 3.22 SAS PROC AUTOREG Output for the Toothpaste Market Share Data, Assuming Second-Order Autoregressive Errors   

<table><tr><td colspan="21">The SAS System</td><td></td><td></td><td></td></tr><tr><td colspan="21">The AUTOREG Procedure</td><td></td><td></td><td></td></tr><tr><td colspan="21">Dependent Variable y</td><td></td><td></td><td></td></tr><tr><td colspan="21">y</td><td></td><td></td><td></td></tr><tr><td colspan="21">Ordinary Least Squares Estimates</td><td></td><td></td><td></td></tr><tr><td>SSE</td><td colspan="3">3.30825739</td><td>DFE</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MSE</td><td colspan="3">0.18379</td><td>Root MSE</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SBC</td><td colspan="3">26.762792</td><td>AIC</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Regress R-Square</td><td colspan="3">0.9511</td><td>Total R-Square</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Durbin-Watson</td><td colspan="3">1.1358</td><td>Pr &lt; DW</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Pr &gt; DW</td><td colspan="3">0.9902</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="21">NOTE: Pr&lt;DW is the p-value for testing positive autocorrelation, and Pr&gt;DW is the p-value for testing negative autocorrelation.</td><td></td><td></td><td></td></tr><tr><td>Standard</td><td>Approx</td><td colspan="19">Variable</td><td></td><td></td><td></td></tr><tr><td>Variable</td><td>DF</td><td>Estimate</td><td>Error</td><td>t Value</td><td colspan="5">Pr &gt; |t|</td><td colspan="11">Label</td><td></td><td></td><td></td></tr><tr><td>Intercept</td><td>1</td><td>26.9099</td><td>1.1099</td><td>24.25</td><td colspan="5">&lt;.0001</td><td colspan="11"></td><td></td><td></td><td></td></tr><tr><td>x</td><td>1</td><td>-24.2898</td><td>1.2978</td><td>-18.72</td><td colspan="5">&lt;.0001</td><td colspan="11">x</td><td></td><td></td><td></td></tr><tr><td colspan="21">Estimates of Autocorrelations</td><td></td><td></td><td></td></tr><tr><td>Lag</td><td>Covariance</td><td>Correlation</td><td>-1</td><td>9</td><td>8</td><td>7</td><td>6</td><td>5</td><td>4</td><td>3</td><td>2</td><td>1</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>1</td></tr><tr><td>0</td><td>0.1654</td><td>1.000000</td><td>|</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>0.0677</td><td>0.409437</td><td>|</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>2</td><td>0.0223</td><td>0.134686</td><td>|</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="21">Preliminary MSE 0.1375</td><td></td><td></td><td></td></tr><tr><td colspan="21">Estimates of Autoregressive Parameters</td><td></td><td></td><td></td></tr><tr><td>Standard</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Lag</td><td>Coefficient</td><td>Error</td><td>t Value</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>-0.425646</td><td>0.249804</td><td>-1.70</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>2</td><td>0.039590</td><td>0.249804</td><td>0.16</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="21">Algorithm converged.</td><td></td><td></td><td></td></tr></table>

TABLE 3.22 (Continued)   

<table><tr><td colspan="7">The SAS System
The AUTOREG Procedure
Maximum Likelihood Estimates</td></tr><tr><td>SSE</td><td>2.69583958</td><td>DFE</td><td></td><td>16</td><td></td><td></td></tr><tr><td>MSE</td><td>0.16849</td><td>Root MSE</td><td></td><td>0.41048</td><td></td><td></td></tr><tr><td>SBC</td><td>28.8691217</td><td>AIC</td><td></td><td>24.8861926</td><td></td><td></td></tr><tr><td>Regress R-Square</td><td>0.9191</td><td>Total R-Square</td><td></td><td>0.9602</td><td></td><td></td></tr><tr><td>Durbin-Watson</td><td>1.9168</td><td>Pr &lt; DW</td><td></td><td>0.3732</td><td></td><td></td></tr><tr><td>Pr &gt; DW</td><td>0.6268</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="7">NOTE: Pr&lt;DW is the p-value for testing positive autocorrelation, and Pr&gt;DW is the p-value for testing negative autocorrelation.</td></tr><tr><td>Standard</td><td>Approx</td><td colspan="5">Variable</td></tr><tr><td>Variable</td><td>DF</td><td>Estimate</td><td>Error</td><td>t Value</td><td>Pr &gt; |t|</td><td>Label</td></tr><tr><td>Intercept</td><td>1</td><td>26.3406</td><td>1.5493</td><td>17.00</td><td>&lt;.0001</td><td></td></tr><tr><td>x</td><td>1</td><td>-23.6025</td><td>1.8047</td><td>-13.08</td><td>&lt;.0001</td><td>x</td></tr><tr><td>AR1</td><td>1</td><td>-0.4456</td><td>0.2562</td><td>-1.74</td><td>0.1012</td><td></td></tr><tr><td>AR2</td><td>1</td><td>0.0297</td><td>0.2617</td><td>0.11</td><td>0.9110</td><td></td></tr><tr><td colspan="7">Autoregressive parameters assumed given.</td></tr><tr><td>Standard</td><td>Approx</td><td colspan="5">Variable</td></tr><tr><td>Variable</td><td>DF</td><td>Estimate</td><td>Error</td><td>t Value</td><td>Pr &gt; |t|</td><td>Label</td></tr><tr><td>Intercept</td><td>1</td><td>26.3406</td><td>1.5016</td><td>17.54</td><td>&lt;.0001</td><td></td></tr><tr><td>x</td><td>1</td><td>-23.6025</td><td>1.7502</td><td>-13.49</td><td>&lt;.0001</td><td>x</td></tr></table>

Unfortunately, this naive approach is not correct. From Eq. (3.103), we know that the observation at time period $t$ is

$$
y _ {t} = \phi y _ {t - 1} + (1 - \phi) \beta_ {0} + \beta_ {1} \left(x _ {t} - \phi x _ {t - 1}\right) + a _ {t} \tag {3.107}
$$

So at the end of the current time period $T$ the next observation is

$$
y _ {T + 1} = \phi y _ {T} + (1 - \phi) \beta_ {0} + \beta_ {1} (x _ {T + 1} - \phi x _ {T}) + a _ {T + 1}
$$

Assume that the future value of the regressor variable $x _ { T + 1 }$ is known. Obviously, at the end of the current time period, both $y _ { T }$ and $x _ { T }$ are known. The random error at time $T { + } 1$ , $a _ { T + 1 }$ , has not been observed yet, and because we have assumed that the expected value of the errors is zero, the best estimate we can make of $a _ { T + 1 }$ is $a _ { T + 1 } = 0$ . This suggests that a

reasonable forecast of the observation in time period $T { + } 1$ that we can make at the end of the current time period $T$ is

$$
\hat {y} _ {T + 1} (T) = \hat {\phi} y _ {T} + (1 - \hat {\phi}) \hat {\beta} _ {0} + \hat {\beta} _ {1} \left(x _ {T + 1} - \hat {\phi} x _ {T}\right) \tag {3.108}
$$

Note that this forecast is likely to be very different from the naive forecast obtained by ignoring the autocorrelation.

To -nd a prediction interval on the forecast, we need to -nd the variance of the prediction error. The one-step-ahead forecast error is

$$
y _ {T + 1} - \hat {y} _ {T + 1} (T) = a _ {T + 1},
$$

assuming that all of the parameters in the forecasting model are known. The variance of the one-step-ahead forecast error is

$$
\mathrm {V a r} \left(a _ {T + 1}\right) = \sigma_ {a} ^ {2}
$$

Using the variance of the one-step-ahead forecast error, we can construct a $1 0 0 ( 1 - \alpha )$ percent prediction interval for the lead-one forecast from Eq. (3.107). The PI is

$$
\hat {y} _ {T + 1} (T) \pm z _ {\alpha / 2} \sigma_ {a},
$$

where $z _ { \alpha / 2 }$ is the upper $\alpha / 2$ percentage point of the standard normal distribution. To actually compute an interval, we must replace $\sigma _ { a }$ by an estimate, resulting in

$$
\hat {y} _ {T + 1} (T) \pm z _ {\alpha / 2} \hat {\sigma} _ {a} \tag {3.109}
$$

as the PI. Because $\sigma _ { a }$ and the model parameters in the forecasting equation have been replaced by estimates, the probability level on the PI in Eq. (3.109) is only approximate.

Now suppose that we want to forecast two periods ahead assuming that we are at the end of the current time period, T. Using Eq. (3.107), we can write the observation at time period $T + 2$ as

$$
\begin{array}{l} y _ {T + 2} = \phi y _ {T + 1} + (1 - \phi) \beta_ {0} + \beta_ {1} \left(x _ {T + 2} - \phi x _ {T + 1}\right) + a _ {T + 2} \\ = \phi [ \phi y _ {T} + (1 - \phi) \beta_ {0} + \beta_ {1} (x _ {T + 1} - \phi x _ {T}) + a _ {T + 1} ] + (1 - \phi) \beta_ {0} \\ + \beta_ {1} \left(x _ {T + 2} - \phi x _ {T + 1}\right) + a _ {T + 2} \\ \end{array}
$$

Assume that the future value of the regressor variables $x _ { T + 1 }$ and $x _ { T + 2 }$ are known. At the end of the current time period, both $y _ { T }$ and $x _ { T }$ are known. The random errors at time $T + 1$ and $T + 2$ have not been observed yet, and because we have assumed that the expected value of the errors is zero, the best estimate we can make of both $a _ { T + 1 }$ and $a _ { T + 2 }$ is zero. This suggests that the forecast of the observation in time period $T + 2$ made at the end of the current time period $T$ is

$$
\begin{array}{l} \hat {y} _ {T + 2} (T) = \hat {\phi} \hat {y} _ {T + 1} (T) + (1 - \hat {\phi}) \hat {\beta} _ {0} + \hat {\beta} _ {1} (x _ {T + 2} - \hat {\phi} x _ {T + 1}) \\ = \hat {\phi} \left[ \hat {\phi} y _ {T} + (1 - \hat {\phi}) \hat {\beta} _ {0} + \hat {\beta} _ {1} \left(x _ {T + 1} - \hat {\phi} x _ {T}\right) \right] \tag {3.110} \\ + (1 - \hat {\phi}) \hat {\beta} _ {0} + \hat {\beta} _ {1} \left(x _ {T + 2} - \hat {\phi} x _ {T + 1}\right) \\ \end{array}
$$

The two-step-ahead forecast error is

$$
y _ {T + 2} - \hat {y} _ {T + 2} (T) = a _ {T + 2} + \phi a _ {T + 1},
$$

assuming that all estimated parameters are actually known. The variance of the two-step-ahead forecast error is

$$
\begin{array}{l} \operatorname {V a r} \left(a _ {T + 2} + \phi a _ {T + 1}\right) = \sigma_ {a} ^ {2} + \phi^ {2} \sigma_ {a} ^ {2} \\ = (1 + \phi^ {2}) \sigma_ {a} ^ {2} \\ \end{array}
$$

Using the variance of the two-step-ahead forecast error, we can construct a $1 0 0 ( 1 - \alpha )$ percent PI for the lead-one forecast from Eq. (3.107):

$$
\hat {y} _ {T + 2} (T) \pm z _ {\alpha / 2} (1 + \phi^ {2}) ^ {1 / 2} \sigma_ {a}
$$

To actually compute the PI, both $\sigma _ { a }$ and $\phi$ must be replaced by estimates, resulting in

$$
\hat {y} _ {T + 2} (T) \pm z _ {\alpha / 2} \left(1 + \hat {\phi} ^ {2}\right) ^ {1 / 2} \hat {\sigma} _ {a} \tag {3.111}
$$

as the PI. Because $\sigma _ { a }$ and $\phi$ have been replaced by estimates, the probability level on the PI in Eq. (3.111) is only approximate.

In general, if we want to forecast $\tau$ periods ahead, the forecasting equation is

$$
\hat {y} _ {T + \tau} (T) = \hat {\phi} \hat {y} _ {T + \tau - 1} (T) + (1 - \hat {\phi}) \hat {\beta} _ {0} + \hat {\beta} _ {1} \left(x _ {T + \tau} - \hat {\phi} x _ {T + \tau - 1}\right) \tag {3.112}
$$

The $\tau$ -step-ahead forecast error is (assuming that the estimated model parameters are known)

$$
y _ {T + \tau} - \hat {y} _ {T + \tau} (T) = a _ {T + \tau} + \phi a _ {T + \tau - 1} + \dots + \phi^ {\tau - 1} a _ {T + 1}
$$

and the variance of the $\tau$ -step-ahead forecast error is

$$
\begin{array}{l} V \left(a _ {T + \tau} + \phi a _ {T + \tau - 1} + \dots + \phi^ {\tau - 1} a _ {T + 1}\right) = \left(1 + \phi^ {2} + \dots + \phi^ {2 (\tau - 1)}\right) \sigma_ {a} ^ {2} \\ = \frac {1 - \phi^ {2 \tau}}{1 + \phi^ {2}} \sigma_ {a} ^ {2} \\ \end{array}
$$

A $1 0 0 ( 1 - \alpha )$ percent PI for the lead- $\tau$ forecast from Eq. (3.112) is

$$
\hat {y} _ {T + \tau} (T) \pm z _ {\alpha / 2} \left(\frac {1 - \phi^ {2 \tau}}{1 + \phi^ {2}}\right) ^ {1 / 2} \sigma_ {a}
$$

Replacing $\sigma _ { a }$ and $\phi$ by estimates, the approximate $1 0 0 ( 1 - \alpha )$ percent PI is actually computed from

$$
\hat {y} _ {T + \tau} (T) \pm z _ {\alpha / 2} \left(\frac {1 - \hat {\phi} ^ {2 \tau}}{1 + \hat {\phi} ^ {2}}\right) ^ {1 / 2} \hat {\sigma} _ {a} \tag {3.113}
$$

The Case Where the Predictor Variable Must Also Be Forecast In the preceding discussion, we assumed that in order to make forecasts, any necessary values of the predictor variable in future time periods $T + \tau$ are known. This is often (probably usually) an unrealistic assumption. For example, if you are trying to forecast how many new vehicles will be registered in the state of Arizona in some future year $T + \tau$ as a function of the state population in year $T + \tau$ , it is pretty unlikely that you will know the state population in that future year.

A straightforward solution to this problem is to replace the required future values of the predictor variable in future time periods $T + \tau$ by forecasts of these values. For example, suppose that we are forecasting one period ahead. From Eq. (3.108) we know that the forecast for $y _ { T + 1 }$ is

$$
\hat {y} _ {T + 1} (T) = \hat {\phi} y _ {T} + (1 - \hat {\phi}) \hat {\beta} _ {0} + \hat {\beta} _ {1} (x _ {T + 1} - \hat {\phi} x _ {T})
$$

But the future value of $x _ { T + 1 }$ is not known. Let $\hat { x } _ { T + 1 } ( T )$ be an unbiased forecast of $x _ { T + 1 }$ , made at the end of the current time period $T$ . Now the forecast for $y _ { T + 1 }$ is

$$
\hat {y} _ {T + 1} (T) = \hat {\phi} y _ {T} + (1 - \hat {\phi}) \hat {\beta} _ {0} + \hat {\beta} _ {1} [ \hat {x} _ {T + 1} (T) - \hat {\phi} x _ {T} ] \tag {3.114}
$$

If we assume that the model parameters are known, the one-step-ahead forecast error is

$$
y _ {T + 1} - \hat {y} _ {T + 1} (T) = a _ {T + 1} + \beta_ {1} [ x _ {T + 1} - \hat {x} _ {T + 1} (T) ]
$$

and the variance of this forecast error is

$$
\operatorname {V a r} \left(a _ {T + 1}\right) = \sigma_ {a} ^ {2} + \beta_ {1} ^ {2} \sigma_ {x} ^ {2} (1), \tag {3.115}
$$

where $\sigma _ { x } ^ { 2 } ( 1 )$ is the variance of the one-step-ahead forecast error for the predictor variable $x$ and we have assumed that the random error $a _ { T + 1 }$ in period $T { + } 1$ is independent of the error in forecasting the predictor variable. Using the variance of the one-step-ahead forecast error from Eq. (3.115), we can construct a $1 0 0 ( 1 - \alpha )$ percent prediction interval for the lead-one forecast from Eq. (3.114). The PI is

$$
\hat {y} _ {T + 1} (T) \pm z _ {\alpha / 2} \left[ \sigma_ {a} ^ {2} + \beta_ {1} ^ {2} \sigma_ {x} ^ {2} (1) \right] ^ {1 / 2},
$$

where $z _ { \alpha / 2 }$ is the upper $\alpha / 2$ percentage point of the standard normal distribution. To actually compute an interval, we must replace the parameters $\beta _ { 1 } , \sigma _ { a } ^ { 2 }$ , and $\sigma _ { x } ^ { 2 } ( 1 )$ by estimates, resulting in

$$
\hat {y} _ {T + 1} (T) \pm z _ {\alpha / 2} \left[ \hat {\sigma} _ {a} ^ {2} + \hat {\beta} _ {1} ^ {2} \hat {\sigma} _ {x} ^ {2} (1) \right] ^ {1 / 2} \tag {3.116}
$$

as the PI. Because the parameters have been replaced by estimates, the probability level on the PI in Eq. (3.116) is only approximate.

In general, if we want to forecast $\tau$ periods ahead, the forecasting equation is

$$
\hat {y} _ {T + \tau} (T) = \hat {\phi} \hat {y} _ {T + \tau - 1} (T) + (1 - \hat {\phi}) \hat {\beta} _ {0} + \hat {\beta} _ {1} [ \hat {x} _ {T + \tau} (T) - \hat {\phi} \hat {x} _ {T + \tau - 1} (T) ] \tag {3.117}
$$

The $\tau$ -step-ahead forecast error is, assuming that the model parameters are known,

$$
y _ {T + \tau} - \hat {y} _ {T + \tau} (T) = a _ {T + \tau} + \phi a _ {T + \tau - 1} + \dots + \phi^ {\tau - 1} a _ {T + 1} + \beta_ {1} [ x _ {T + \tau} - \hat {x} _ {T + \tau} (T) ]
$$

and the variance of the $\tau$ -step-ahead forecast error is

$$
\begin{array}{l} \operatorname {V a r} \left(a _ {T + \tau} + \phi a _ {T + \tau - 1} + \dots + \phi^ {\tau - 1} a _ {T + 1} + \beta_ {1} \left[ x _ {T + \tau} - \hat {x} _ {T + \tau} (t) \right]\right) \\ = (1 + \phi^ {2} + \dots + \phi^ {2 (\tau - 1)}) \sigma_ {a} ^ {2} + \beta_ {1} ^ {2} \sigma_ {x} ^ {2} (\tau) \\ = \frac {1 - \phi^ {2 \tau}}{1 + \phi^ {2}} \sigma_ {a} ^ {2} + \beta_ {1} ^ {2} \sigma_ {x} ^ {2} (\tau), \\ \end{array}
$$

where $\sigma _ { x } ^ { 2 } ( \tau )$ is the variance of the $\tau$ -step-ahead forecast error for the predictor variable $x$ . A $1 0 0 ( 1 - \alpha )$ percent PI for the lead- $\tau$ forecast from Eq. (3.117) is

$$
\hat {y} _ {T + \tau} (T) \pm z _ {\alpha / 2} \left(\frac {1 - \phi^ {2 \tau}}{1 + \phi^ {2}} \sigma_ {a} ^ {2} + \beta_ {1} ^ {2} \sigma_ {x} ^ {2} (\tau)\right) ^ {1 / 2}
$$

Replacing all of the unknown parameters by estimates, the approximate $1 0 0 ( 1 - \alpha )$ percent PI is actually computed from

$$
\hat {y} _ {T + \tau} (T) \pm z _ {\alpha / 2} \left(\frac {1 - \hat {\phi} ^ {2 \tau}}{1 + \hat {\phi} ^ {2}} \hat {\sigma} _ {a} ^ {2} + \hat {\beta} _ {1} ^ {2} \hat {\sigma} _ {x} ^ {2} (\tau)\right) ^ {1 / 2} \tag {3.118}
$$

Alternate Forms of the Model The regression model with autocorrelated errors

$$
y _ {t} = \phi y _ {t - 1} + (1 - \phi) \beta_ {0} + \beta_ {1} (x _ {t} - \phi x _ {t - 1}) + a _ {t}
$$

is a very useful model for forecasting time series regression data. However, when using this model there are two alternatives that should be considered. The -rst of these is

$$
y _ {t} = \phi y _ {t - 1} + \beta_ {0} + \beta_ {1} x _ {t} + \beta_ {2} x _ {t - 1} + a _ {t} \tag {3.119}
$$

This model removes the requirement that the regression coef-cient for the lagged predictor variable $x _ { t - 1 }$ be equal to $- \beta _ { 1 } \phi$ . An advantage of this model is that it can be -t by OLS. Another alternative model to consider is to simply drop the lagged value of the predictor variable from Eq. (3.119), resulting in

$$
y _ {t} = \phi y _ {t - 1} + \beta_ {0} + \beta_ {1} x _ {t} + a _ {t} \tag {3.120}
$$

Often just including the lagged value of the response variable is suf-cient and Eq. (3.120) will be satisfactory.

The choice between models should always be a data-driven decision. The different models can be -t to the available data, and model selection can be based on the criteria that we have discussed previously, such as model adequacy checking and residual analysis, and (if enough data are available to do some data splitting) forecasting performance over a test or trial period of data.

Example 3.16 Reconsider the toothpaste market share data originally presented in Example 3.14 and modeled with a time series regression model with -rst-order autoregressive errors in Example 3.15. First we will try -tting the model in Eq. (3.119). This model simply relaxes the restriction that the regression coef-cient for the lagged predictor variable $x _ { t - 1 }$ (price in this example) be equal to $- \beta _ { 1 } \phi$ . Since this is just a linear regression model, we can -t it using Minitab. Table 3.23 contains the Minitab results.

This model is a good -t to the data. The Durbin鈥揥atson statistic is $d =$ 2.04203, which indicates no problems with autocorrelation in the residuals. However, note that the $t \cdot$ -statistic for the lagged predictor variable (price) is not signi-cant $( P = 0 . 2 1 7 $ ), indicating that this variable could be removed from the model. If $x _ { t - 1 }$ is removed, the model becomes the one in Eq. (3.120). The Minitab output for this model is in Table 3.24.

This model is also a good -t to the data. Both predictors, the lagged variable $y _ { t - 1 }$ and $x _ { t }$ , are signi-cant. The Durbin鈥揥atson statistic does not indicate any signi-cant problems with autocorrelation. It seems that either of these models would be reasonable for the toothpaste market share data. The advantage of these models relative to the time series regression model with autocorrelated errors is that they can be -t by OLS. In this example, including a lagged response variable and a lagged predictor variable has essentially eliminated any problems with autocorrelated errors.

# 3.9 ECONOMETRIC MODELS

The -eld of econometrics involves the uni-ed study of economics, economic data, mathematics, and statistical models. The term econometrics is generally credited to the Norwegian economist Ragnar Frisch (1895鈥?1973) who was one of the founders of the Econometric Society and the founding editor of the important journal Econometrica in 1933. Frisch was a co-winner of the -rst Nobel Prize in Economic Sciences in 1969. For

TABLE 3.23 Minitab Results for Fitting Model (3.119) to the Toothpaste Market Share Data   

<table><tr><td colspan="6">Regression Analysis: y Versus yt-1, x,xt-1</td></tr><tr><td colspan="6">The regression equation is y = 16.1 + 0.425 y(t-1) - 22.2 x + 7.56 x(t-1)</td></tr><tr><td>Predictor</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P</td><td></td></tr><tr><td>Constant</td><td>16.100</td><td>6.095</td><td>2.64</td><td>0.019</td><td></td></tr><tr><td>y(t-1)</td><td>0.4253</td><td>0.2239</td><td>1.90</td><td>0.077</td><td></td></tr><tr><td>x</td><td>-22.250</td><td>2.488</td><td>-8.94</td><td>0.000</td><td></td></tr><tr><td>x(t-1)</td><td>7.562</td><td>5.872</td><td>1.29</td><td>0.217</td><td></td></tr><tr><td colspan="6">S = 0.402205 R-Sq = 96.0% R-Sq(adj) = 95.2%</td></tr><tr><td colspan="6">Analysis of Variance</td></tr><tr><td>Source</td><td>DF</td><td>SS</td><td>MS</td><td>F</td><td>P</td></tr><tr><td>Regression</td><td>3</td><td>58.225</td><td>19.408</td><td>119.97</td><td>0.000</td></tr><tr><td>Residual Error</td><td>15</td><td>2.427</td><td>0.162</td><td></td><td></td></tr><tr><td>Total</td><td>18</td><td>60.651</td><td></td><td></td><td></td></tr><tr><td>Source</td><td>DF</td><td>Seq SS</td><td></td><td></td><td></td></tr><tr><td>y(t-1)</td><td>1</td><td>44.768</td><td></td><td></td><td></td></tr><tr><td>x</td><td>1</td><td>13.188</td><td></td><td></td><td></td></tr><tr><td>x(t-1)</td><td>1</td><td>0.268</td><td></td><td></td><td></td></tr><tr><td colspan="6">Durbin-Watson statistic = 2.04203</td></tr></table>

introductory books on econometrics, see Greene (2011) and Woodridge (2011).

Econometric models assume that the quantities being studied are random variables and regression modeling techniques are widely used in the -eld to describe the relationships between these quantities. Typically, an analyst may want to quantify the impact of one set of variables on another variable. For example, one may want to investigate the effect of education on income; that is, what is the change in earnings that result from increasing a worker鈥檚 education, while holding other variables such as age and gender constant. Large-scale, comprehensive econometric models of macroeconomic relationships are used by government agencies and central banks to evaluate economic activity and to provide guidance on economic

TABLE 3.24 Minitab Results for Fitting Model (3.120) to the Toothpaste Market Share Data   

<table><tr><td colspan="6">Regression Analysis: y Versusyt-1,x</td></tr><tr><td colspan="6">The regression equation is y = 23.3 + 0.162 y(t-1) - 21.2 x</td></tr><tr><td>Predictor</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P</td><td></td></tr><tr><td>Constant</td><td>23.279</td><td>2.515</td><td>9.26</td><td>0.000</td><td></td></tr><tr><td>y(t-1)</td><td>0.16172</td><td>0.09238</td><td>1.75</td><td>0.099</td><td></td></tr><tr><td>x</td><td>-21.181</td><td>2.394</td><td>-8.85</td><td>0.000</td><td></td></tr><tr><td>S = 0.410394</td><td colspan="2">R-Sq = 95.6%</td><td colspan="2">R-Sq(adj) = 95.0%</td><td></td></tr><tr><td colspan="6">Analysis of Variance</td></tr><tr><td>Source</td><td>DF</td><td>SS</td><td>MS</td><td>F</td><td>P</td></tr><tr><td>Regression</td><td>2</td><td>57.956</td><td>28.978</td><td>172.06</td><td>0.000</td></tr><tr><td>Residual Error</td><td>16</td><td>2.695</td><td>0.168</td><td></td><td></td></tr><tr><td>Total</td><td>18</td><td>60.651</td><td></td><td></td><td></td></tr><tr><td>Source</td><td>DF</td><td>Seq SS</td><td></td><td></td><td></td></tr><tr><td>y(t-1)</td><td>1</td><td>44.768</td><td></td><td></td><td></td></tr><tr><td>x</td><td>1</td><td>13.188</td><td></td><td></td><td></td></tr><tr><td colspan="6">Durbin-Watson statistic = 1.61416</td></tr></table>

policies. For example, the United States Federal Reserve Bank has maintained macroeconometric models for forecasting and quantitative policy and macroeconomic analysis for over 40 years. The Fed focuses on both the US economy and the global economy.

There are several types of data used in econometric modeling. Timeseries data are used in many applications. Typical examples include aggregates of economic quantities, such as GDP, asset or commodity prices, and interest rates. As we have discussed earlier in this chapter, time series such as these are characterized by serial correlation. A lot of aggregate economic data are only available at a relatively low sampling frequency, such as monthly, quarterly, or in some cases annually. One exception is -nancial data, which may be available at very high frequency, such as hourly, daily, or even by individual transaction. Cross-sectional data consist of observations taken at the same point in time. In econometric work, surveys are a typical source of cross-sectional data. In typical applications, the surveys

are conducted on individuals, households, business organizations, or other economic entities. The early part of this chapter described regression modeling of cross-section data. Panel data typically contain both cross-section and time-series data. These data sets consist of a collection of individuals, households, or corporations that are surveyed repeatedly over time. As an example of a simple econometric model involving time series data, suppose that we wish to develop a model for forecasting monthly consumer spending. A plausible model might be

$$
y _ {t} = \beta_ {0} + \beta_ {1} x _ {t - 1} + \beta_ {2} y _ {t - 1} + \varepsilon_ {t},
$$

where $y _ { t }$ is consumer spending in month t, $x _ { t - 1 }$ is income in month $t - 1$ , $y _ { t - 1 }$ is consumer spending in month $t - 1$ , and $\varepsilon _ { t }$ is the random error term. This is a lagged-variable regression model of the type discussed earlier in this chapter.

The consumer spending example above is an example of a very simple single-equation econometric model. Many econometric models involve several equations and the predictor variables in these models can be involved in complex interrelationships with each other and with the dependent variables. For example, consider the following econometric model:

Sales $= f _ { 1 }$ $= f _ { 1 } ( \mathrm { G N P } ,$ , price, number of competitors, advertising expenditures)

However, price is likely to be a function of other variables, say

Price $= f _ { 2 }$ (production costs, distribution costs, overhead costs, material cost, packaging costs)

and

Production costs $= f _ { 3 }$ (production volume, labor costs, material costs, inventory costs)

Advertising expenditures $= f _ { 4 }$ (sales, number of competitors)

Notice the interrelationships between these variables. Advertising expenditures certainly inuence sales, but the level of sales and the number of competitors will inuence the money spent on advertising. Furthermore, different levels of sales will have an impact on production costs.

Constructing and maintaining these models is a complex task. One could (theoretically at least) write a very large number of interrelated equations, but data availability and model estimation issues are practical restrictions. The $\mathbf { S A S } ^ { \mathbb { C } }$ software package has good capability for this type of simultaneous equation modeling and is widely used in econometrics. However,

forecast accuracy does not necessarily increase with the complexity of the models. Often, a relatively simple time series model will outperform a complex econometric model from a pure forecast accuracy point of view. Econometric models are most useful for providing understanding about the way an economic system works, and for evaluating in a broad sense how different economic policies will perform, and the effect that will have on the economy. This is why their use is largely con-ned to government entities and some large corporations. There are commercial services that offer econometric models that could be useful to smaller organizations, and free alternatives are available from central banks and other government organizations.

# 3.10 R COMMANDS FOR CHAPTER 3

Example 3.1 The patient satisfaction data are in the sixth column of the array called patsat.data in which the second and third columns are the age and the severity. Note that we can use the 鈥渓m鈥?function to -t the linear model. But as in the example, we will show how to obtain the regression coef-cients using the matrix notation.

```txt
nrow<-dim(patsat.data)[1]  
X<-cbindmatrix(1,nrow,1)锛宲atsat.data[锛?:3])  
y<-patsat.data[,6]  
beta<-solve(t(X)\*%X)%\*t(X)\*%y  
beta[1,1]  
143.4720118  
Age -1.0310534  
Severity -0.5560378
```

Example 3.3 For this example we will use the 鈥渓m鈥?function.

satisfaction2.fit<-lm(Satisfaction $\sim$ Age $^+$ Severity $^+$ Age:Severity $^+$ I(Age^2)+ I(Severity^2)锛宒ata $\equiv$ patsat)   
summary(satisfaction2.fit)   
Call:   
1m(formula $=$ Satisfaction \~ Age $^+$ Severity $^+$ Age:Severity $^+$ I(Age^2)+ I(Severity^2)锛宒ata $=$ patsat)   
Residuals: Min 1Q Median 3Q Max -16.915 -3.642 2.015 4.000 9.677

```txt
Coefficients: Estimate Std. Error t value Pr(> |t|) (Intercept) 127.527542 27.912923 4.569 0.00021 *** Age -0.995233 0.702072 -1.418 0.17251 Severity 0.144126 0.922666 0.156 0.87752 I(Age^2) -0.002830 0.008588 -0.330 0.74534 I(Severity^2) -0.011368 0.013533 -0.840 0.41134 Age:Severity 0.006457 0.016546 0.390 0.70071 Signif. codes: 0 '***' 0.001 '**' 0.01 '**' 0.05 '.' 0.1 ' ' 1 Residual standard error: 7.503 on 19 degrees of freedom Multiple R-squared: 0.9008, Adjusted R-squared: 0.8747 F-statistic: 34.5 on 5 and 19 DF, p-value: 6.76e-09 anova(satisfaction2.fit) 
```

```txt
Analysis of Variance Table Response: Satisfaction Df Sum Sq Mean Sq F value Pr(>F) Age 1 8756.7 8756.7 155.5644 1.346e-10 \*\* Severity 1 907.0 907.0 16.1138 0.0007417 \*\* I(Age^2) 1 1.4 1.4 0.0252 0.8756052 I(Severity^2) 1 35.1 35.1 0.6228 0.4397609 Age:Severity 1 8.6 8.6 0.1523 0.7007070 Residuals 19 1069.5 56.3 Signif. codes: O'***' 0.001 '**' 0.01 '**' 0.05 '.' 0.1 ' ' 1 
```

Example 3.4 We use the 鈥渓m鈥?function again and obtain the linear model. Then we use 鈥渃on-nt鈥?function to obtain the con-dence intervals of the model parameters. Note that the default con-dence level is $9 5 \%$ .

```batch
satisfaction1.fit<-lm(Satisfaction Age+Severity, data=patsat)  
confint(satisfaction1.fit, level=.95)  
2.5 % 97.5 %  
(Intercept) 131.122434 155.8215898  
Age -1.270816 -0.7912905  
Severity -0.828566 -0.2835096 
```

Example 3.5 This example refers to the linear model (Satisfaction1.-t). We obtain the con-dence and prediction intervals for a new data point for which age $= 6 0$ and severity $= 6 0$ .

new <- data.frame(Age = 60, Severity=60)  
pred.sat1.clim<-predict(Satisfaction1.fit, newdata=new, se.fit = TRUE, interval = "confidence")  
pred.sat1.plim<-predict(Satisfaction1.fit, newdata=new, se.fit = TRUE, interval = "prediction")  
pred.sat1.clim\(fit  
    fit lwr upr  
    1 48.24654 43.84806 52.64501  
pred.sat1.plim\)fit  
    fit lwr upr  
    1 48.24654 32.84401 63.64906

Example 3.6 We simply repeat Example 3.5 for age $= 7 5$ and severity $=$ 60.

new <- data.frame(Age = 75, Severity=60)  
pred.sat1.clim<-predict(Satisfaction1.fit, newdata=new, se.fit = TRUE, interval = "confidence")  
pred.sat1.plim<-predict(Satisfaction1.fit, newdata=new, se.fit = TRUE, interval = "prediction")  
pred.sat1.clim\(fit  
    fit lwr upr  
1 32.78074 26.99482 38.56666  
pred.sat1.plim\)fit  
    fit lwr upr  
1 32.78074 16.92615 48.63533

Example 3.7 The residual plots for Satisfaction1.-t can be obtained using the following commands:

```matlab
par(mfrow=c(2,2),oma=c(0,0,0,0))  
qqnorm(Satisfaction1.fit\\(res,datrix \)=\mathrm{TRUE}\( ,pch=16,xlab \(=\) 'Residual',main \(=\) "锛? 
qqline(Satisfaction1.fit\\)res,datrix \(\equiv\) TRUE)  
plot(Satisfaction1.fit\\(fit,Satisfaction1.fit\\)res,pch=16, xlab \(=\) Fitted Value',ylab \(=\) 'Residual')  
abline(h=0)  
hist(Satisfaction1.fit\\)res,col \(=\) "gray",xlab \(=\) 'Residual',main \(=\) ") plot(Satisfaction1.fit\\)res,type \(=\) "l",xlab \(=\) 'Observation Order',ylab \(=\) 'Residual')  
points(Satisfaction1.fit\\)res,pch=16,cex=.5)  
abline(h=0) 
```

![](images/dffa75d676cec83c88e2e1f3c3ce3c87da974d337ba79cddbd62d9276b47a465.jpg)

![](images/04e60db89319b8c25104b1a72929906eeb709b19890858de24ba5cb354bec58f.jpg)

![](images/17a71d80abb1617b0f97c343656c3a98da51ea6a4b04a5151757865a1dc25e01.jpg)

![](images/b8e64dbcaf60fac69613138cabb0d1ca97e2b620a0fca339722c16a3520c45ab.jpg)

Example 3.8 In R, one can do stepwise regression using step function which allows for stepwise selection of variables in forward, backward, or both directions. Note that step needs to be applied to a model with 4 input variables as indicated in the example. Therefore we -rst -t that model and apply the step function. Also note that the variable selection is done based on AIC and therefore we get in the forward selection slightly different results than the one provided in the textbook.

satisfaction3.fit<-lm(Satisfaction~Age+Severity+Surg.Med+Anxiety, data $\equiv$ patsat)

```txt
step.for<-step(satisfaction3.fit,direction='forward') 
```

```txt
Start: AIC=103.18  
Satisfaction~Age + Severity + Surg.Med + Anxiety 
```

```txt
step.back<-step(satisfaction3.fit,direction='backward') 
```

```txt
Start: AIC=103.18  
Satisfaction~Age + Severity + Surg.Med + Anxiety 
```

```txt
Df Sum of Sq RSS AIC  
- Surg.Med 1 1.0 1039.9 101.20  
- Anxiety 1 75.4 1114.4 102.93  
<none> 1038.9 103.18  
- Severity 1 971.5 2010.4 117.68  
- Age 1 3387.7 4426.6 137.41 
```

```txt
Step: AIC=101.2 Satisfaction~Age + Severity + Anxiety 
```

```txt
Df Sum of Sq RSS AIC
- Anxiety 1 74.6 1114.5 100.93
<none> 1 1039.9 101.20
- Severity 1 971.8 2011.8 115.70
- Age 1 3492.7 4532.6 136.00
Step: AIC=100.93
Satisfaction~Age + Severity
Df Sum of Sq RSS AIC
<none> 1 1114.5 100.93
- Severity 1 907.0 2021.6 113.82
- Age 1 4029.4 5143.9 137.17
step both<-step(satisfaction3.fit, direction='both') 
```

In R, one can do best subset regression using leaps function from leaps package. We -rst upload the leaps package.

library(leaps)

step.best<-regsubsets(Satisfaction ~ Age+Severity+Surg.Med+Anxiety, data $=$ patsat)

summary(step.best)

Subset selection object Call: regsubsets.formula(Satisfaction ~ Age + Severity + Surg.Med + Anxiety, data $=$ patsat)

4 Variables (and intercept)

Forced in Forced out

Age FALSE FALSE

Severity FALSE FALSE

Surg.Med FALSE FALSE

Anxiety FALSE FALSE

1 subsets of each size up to 4

Selection Algorithm: exhaustive

Age Severity Surg.Med Anxiety

1 ( 1 ) 鈥?鈥?鈥?鈥?  
2 ( 1 ) 鈥?鈥?鈥?鈥?  
3 ( 1 ) 鈥?鈥?鈥?鈥?"*=   
4 ( 1 ) 鈥?鈥?鈥?鈥?"*" "*"

Example 3.9 The connector strength data are in the second column of the array called strength.data in which the -rst column is the Weeks. We start with -tting the linear model and plot the residuals vs. Weeks.

strength1.fit<-lm(Strength ~ Weeks, data $=$ strength.data) summary(strength1.fit)

Call:

lm(formula $=$ Strength ~ Weeks, data $=$ strength.data)

Residuals:

Min 1Q Median 3Q Max

-14.3639 -4.4449 -0.0861 5.8671 11.8842

Coefficients:

Estimate Std. Error t value Pr(> |t|)

(Intercept) 25.9360 5.1116 5.074 2.76e-05 ***

Weeks 0.3759 0.1221 3.078 0.00486 **

Signif. codes: 0 鈥?**鈥?0.001 鈥?*鈥?0.01 鈥?鈥?0.05 鈥?鈥?0.1 鈥?鈥?1

Residual standard error: 7.814 on 26 degrees of freedom

Multiple R-squared: 0.2671, Adjusted R-squared: 0.2389

F-statistic: 9.476 on 1 and 26 DF, p-value: 0.004863

plot(strength.data[,1],strength1.fit$res, pch $_ { . = 1 6 }$ ,ce $: =$ .5, xlab $_ { 1 } = { \mathbf { \Omega } } ^ { \prime }$ Weeks鈥?ylab $_ { 1 } = { \prime }$ Residual鈥? abline $\mathtt { h } = 0$ )

![](images/bde7d278bd380dd884fd0fef28aa6c0a652b078fc95f3e913672a9489a9a7a20.jpg)

We then -t a linear model to the absolute value of the residuals and obtain the weights as the inverse of the square of the -tted values.

res.fit<-lm(abs(strength1.fit$res) ~ Weeks, data $=$ strength.data) weights.strength<-1/(res.fit$fitted藛2)

We then -t a linear model to the absolute value of the residuals and obtain the weights as the inverse of the square of the -tted values.

strength2.fit<-lm(Strength~Weeks, data $\equiv$ strength.data,   
weights=weightsstrength)   
summary(strength2.fit)   
Call: lm(formula $=$ Strength~Weeks, data $=$ strength.data, weights $=$ weights.strength)   
Weighted Residuals: Min 1Q Median 3Q Max -1.9695 -1.0450 -0.1231 1.1507 1.5785   
Coefficients: Estimate Std. Error t value Pr(> |t|) (Intercept) 27.54467 1.38051 19.953 < 2e-16 *** Weeks 0.32383 0.06769 4.784 5.94e-05 *** Signif. codes: 0 \*\*\* 0.001 \*\* 0.01 \*\* 0.05 .' 0.1 ' ' 1 Residual standard error: 1.119 on 26 degrees of freedom Multiple R-squared: 0.4682, Adjusted R-squared: 0.4477 F-statistic: 22.89 on 1 and 26 DF, p-value: 5.942e-05

Example 3.12 Different packages such as car, lmtest, and bstats offer functions for Durbin鈥揥atson test. We will use function 鈥渄wt鈥?in package car. Note that dwt function allows for two-sided or one-sided tests. As in the example we will test for positive autocorrelation. The data are given in softsales.data where the columns are Year, Sales, Expenditures and Population (to be used in the next example).

```txt
library(car)  
soft1.fit<-lm(Sales~Expenditures, data=softsales.data)  
dwt(soft1.fit, alternative="positive")  
lag Autocorrelation D-W Statistic p-value  
1 0.3354445 1.08005 0.007  
Alternative hypothesis: rho > 0 
```

Since the $p$ -value is too small the null hypothesis is rejected concluding that the errors are positively correlated.

Example 3.13 We repeat Example 3.12 with the model expanded to include Population as well.

```txt
soft2.fit<-lm(Sales~Expenditures+Population锛宒ata=softsales.data)  
dwt(soft2.fit, alternative="positive")  
lag Autocorrelation D-W Statistic p-value  
1 -0.534382 3.059322 0.974  
Alternative hypothesis: rho > 0 
```

As concluded in the example, adding the input variable Population seems to resolve the autocorrelation issue resulting in large $p$ -value for the test for autocorrelation.

Example 3.14 The Cochrane鈥揙rcutt method can be found in package orcutt. The function to be used is 鈥渃ochran.orcutt鈥? The data are given in toothsales.data where the columns are Share and Price.

```r
library(orcutt)  
tooth1.fit<-lm(Share~Price, data=toothsales.data)  
dwt(tooth1.fit, alternative="positive")  
lag Autocorrelation D-W Statistic p-value 1 0.4094368 1.135816 0.005  
Alternative hypothesis: rho > 0  
tooth2.fit<-cochrane.orcutt(tooth1.fit) 
```

```txt
tooth2.fit
$Cochrane.Orcutt
Call:
lm(formula = YB~XB - 1)
Residuals:
Min 1Q Median 3Q Max
-0.55508 -0.25069 -0.05506 0.25007 0.83017
Coefficients:
Estimate Std. Error t value Pr(> |t|)
XB (Intercept) 26.722 1.633 16.36 7.71e-12 *** 
XBPrice -24.084 1.938 -12.42 5.90e-10 *** 
Signif. codes: 0 '***' 0.001 '**' 0.01 '''' 0.05 '.' 0.1 ' ' 1
Residual standard error: 0.3955 on 17 degrees of freedom
Multiple R-squared: 0.991, Adjusted R-squared: 0.9899
F-statistic: 932.7 on 2 and 17 DF, p-value: < 2.2e-16
$rho
[1] 0.4252321
$numberinteraction
[1] 8 
```

The results are not exactly the same as the ones given in the example. This should be due to the difference in the approaches. Where the book uses a two-step procedure, the function Cochrane.Orcutt 鈥渆stimates both autocorrelation and beta coef-cients recursively until we reach the convergence (8th decimal)鈥?

Example 3.15 For this example we will use 鈥済ls鈥?function in package nlme. From Example 3.14 we know that there is autocorrelation in the residuals of the linear model. We -rst assume that the -rst-order model will be suf-cient to model the autocorrelation as shown in the book.

```txt
tooth3.fit <- gls(Share~Price, data = toothsales.data, correlation=corARMA(p=1), method="ML")  
summary(tooth3.fit)  
Generalized least squares fit by maximum likelihood  
Model: Share~Price  
Data: toothsales.data  
AIC BIC logLik 24.90475 28.88767 -8.452373  
Correlation Structure: AR(1)  
Formula: ~ 1 
```

Parameter estimate(s): Phi 0.4325871   
Coefficients: Value Std.Error t-value p-value (Intercept) 26.33218 1.436305 18.33328 0 Price -23.59030 1.673740 -14.09436 0 Correlation: (Intr) Price -0.995   
Standardized residuals: Min Q1 Med Q3 Max -1.85194806 -0.85848738 0.08945623 0.69587678 2.03734437 Residual standard error: 0.4074217 Degrees of freedom: 20 total; 18 residual intervals(tooth3.fit) Approximate $95\%$ confidence intervals Coefficients: lower est. upper (Intercept) 23.31462 26.33218 29.34974 Price -27.10670 -23.59030 -20.07390 attr("label") [1] "Coefficients:" Correlation structure: lower est. upper Phi -0.04226294 0.4325871 0.7480172 attr("label") [1] "Correlation structure:" Residual standard error: lower est. upper 0.2805616 0.4074217 0.5916436 predict(tooth3.fit)

The second-order autoregressive model for the errors can be -tted using,

```txt
tooth4.fit <- gls(Share ~ Price, data = toothsales.data, correlation = corARMA(p=2), method="ML") 
```

Example 3.16 To create the lagged version of the variables and also adjust for the number of observations, we use the following commands:

```txt
T<-length(toothsales.data\$Share)  
yt<-toothsales.data\$Share[2:T]  
yt.lag1<- toothsales.data\$Share[1:(T-1)]  
xt<-toothsales.data\$Price[2:T]  
xt.lag1<- toothsales.data\$Price[1:(T-1)]  
tooth5.fit<-lm(yt~yt.lag1+xt+xt.lag1)  
summary(tooth5.fit)  
Call:  
lm(formula = yt~yt.lag1 + xt + xt.lag1)  
Residuals:  
Min 1Q Median 3Q Max -0.59958 -0.23973 -0.02918 0.26351 0.66532  
Coefficients: Estimate Std. Error t value Pr(> |t|) (Intercept) 16.0675 6.0904 2.638 0.0186 * yt.lag1 0.4266 0.2237 1.907 0.0759 . xt -22.2532 2.4870 -8.948 2.11e-07 *** xt.lag1 7.5941 5.8697 1.294 0.2153  
Signif. codes: 0 '***' 0.001 '**' 0.01 '''' 0.05 '.' 0.1 ' ' 1  
Residual standard error: 0.402 on 15 degrees of freedom  
Multiple R-squared: 0.96, Adjusted R-squared: 0.952  
F-statistic: 120.1 on 3 and 15 DF, p-value: 1.037e-10 
```

# EXERCISES

3.1 An article in the journal Air and Waste (Update on Ozone Trends in California鈥檚 South Coast Air Basin, Vol. 43, 1993) investigated the ozone levels in the South Coast Air Basin of California for the years 1976鈥?991. The author believes that the number of days the ozone levels exceeded $0 . 2 0 ~ \mathrm { p p m }$ (the response) depends on the seasonal meteorological index, which is the seasonal average 850-millibar Temperature (the predictor). Table E3.1 gives the data.

a. Construct a scatter diagram of the data.   
b. Estimate the prediction equation.   
c. Test for signi-cance of regression.   
d. Calculate the $9 5 \%$ CI and PI on for a seasonal meteorological index value of 17. Interpret these quantities.   
e. Analyze the residuals. Is there evidence of model inadequacy?   
f. Is there any evidence of autocorrelation in the residuals?

TABLE E3.1 Days that Ozone Levels Exceed 20 ppm and Seasonal Meteorological Index   

<table><tr><td>Year</td><td>Days</td><td>Index</td></tr><tr><td>1976</td><td>91</td><td>16.7</td></tr><tr><td>1977</td><td>105</td><td>17.1</td></tr><tr><td>1978</td><td>106</td><td>18.2</td></tr><tr><td>1979</td><td>108</td><td>18.1</td></tr><tr><td>1980</td><td>88</td><td>17.2</td></tr><tr><td>1981</td><td>91</td><td>18.2</td></tr><tr><td>1982</td><td>58</td><td>16.0</td></tr><tr><td>1983</td><td>82</td><td>17.2</td></tr><tr><td>1984</td><td>81</td><td>18.0</td></tr><tr><td>1985</td><td>65</td><td>17.2</td></tr><tr><td>1986</td><td>61</td><td>16.9</td></tr><tr><td>1987</td><td>48</td><td>17.1</td></tr><tr><td>1988</td><td>61</td><td>18.2</td></tr><tr><td>1989</td><td>43</td><td>17.3</td></tr><tr><td>1990</td><td>33</td><td>17.5</td></tr><tr><td>1991</td><td>36</td><td>16.6</td></tr></table>

3.2 Montgomery, Peck, and Vining (2012) present data on the number of pounds of steam used per month at a plant. Steam usage is thought to be related to the average monthly ambient temperature. The past year鈥檚 usages and temperatures are shown in Table E3.2.

TABLE E3.2 Monthly Steam Usage and Average Ambient Temperature   

<table><tr><td>Month</td><td>Temperature (掳F)</td><td>Usage/1000</td><td>Month</td><td>Temperature (掳F)</td><td>Usage/1000</td></tr><tr><td>January</td><td>21</td><td>185.79</td><td>July</td><td>68</td><td>621.55</td></tr><tr><td>February</td><td>24</td><td>214.47</td><td>August</td><td>74</td><td>675.06</td></tr><tr><td>March</td><td>32</td><td>288.03</td><td>September</td><td>62</td><td>562.03</td></tr><tr><td>April</td><td>47</td><td>424.84</td><td>October</td><td>50</td><td>452.93</td></tr><tr><td>May</td><td>50</td><td>454.68</td><td>November</td><td>41</td><td>369.95</td></tr><tr><td>June</td><td>59</td><td>539.03</td><td>December</td><td>30</td><td>273.98</td></tr></table>

a. Fit a simple linear regression model to the data.   
b. Test for signi-cance of regression.   
c. Analyze the residuals from this model.

d. Plant management believes that an increase in average ambient temperature of one degree will increase average monthly steam consumption by 10,000 lb. Do the data support this statement?   
e. Construct a $9 9 \%$ prediction interval on steam usage in a month with average ambient temperature of $5 8 ^ { \circ } \mathrm { F } .$ .

3.3 On March 1, 1984, the Wall Street Journal published a survey of television advertisements conducted by Video Board Tests, Inc., a New York ad-testing company that interviewed 4000 adults. These people were regular product users who were asked to cite a commercial they had seen for that product category in the past week. In this case, the response is the number of millions of retained impressions per week. The predictor variable is the amount of money spent by the -rm on advertising. The data are in Table E3.3.

TABLE E3.3 Number of Retained Impressions and Advertising Expenditures   

<table><tr><td>Firm</td><td>Amount Spent (Millions)</td><td>Retained Impressions per Week (Millions)</td></tr><tr><td>Miller Lite</td><td>50.1</td><td>32.1</td></tr><tr><td>Pepsi</td><td>74.1</td><td>99.6</td></tr><tr><td>Stroh&#x27;s</td><td>19.3</td><td>11.7</td></tr><tr><td>Federal Express</td><td>22.9</td><td>21.9</td></tr><tr><td>Burger King</td><td>82.4</td><td>60.8</td></tr><tr><td>Coca-Cola</td><td>40.1</td><td>78.6</td></tr><tr><td>McDonald&#x27;s</td><td>185.9</td><td>92.4</td></tr><tr><td>MCI</td><td>26.9</td><td>50.7</td></tr><tr><td>Diet Cola</td><td>20.4</td><td>21.4</td></tr><tr><td>Ford</td><td>166.2</td><td>40.1</td></tr><tr><td>Levi&#x27;s</td><td>27</td><td>40.8</td></tr><tr><td>Bud Lite</td><td>45.6</td><td>10.4</td></tr><tr><td>ATT Bell</td><td>154.9</td><td>88.9</td></tr><tr><td>Calvin Klein</td><td>5</td><td>12</td></tr><tr><td>Wendy&#x27;s</td><td>49.7</td><td>29.2</td></tr><tr><td>Polaroid</td><td>26.9</td><td>38</td></tr><tr><td>Shasta</td><td>5.7</td><td>10</td></tr><tr><td>Meow Mix</td><td>7.6</td><td>12.3</td></tr><tr><td>Oscar Meyer</td><td>9.2</td><td>23.4</td></tr><tr><td>Crest</td><td>32.4</td><td>71.1</td></tr><tr><td>Kibbles N Bits</td><td>6.1</td><td>4.4</td></tr></table>

a. Fit the simple linear regression model to these data.   
b. Is there a signi-cant relationship between the amount that a company spends on advertising and retained impressions? Justify your answer statistically.   
c. Analyze the residuals from this model.   
d. Construct the $9 5 \%$ con-dence intervals on the regression coef-- cients.   
e. Give the $9 5 \%$ con-dence and prediction intervals for the number of retained impressions for MCI.

3.4 Suppose that we have -t the straight-line regression model $\hat { y } =$ $\hat { \beta } _ { 0 } + \hat { \beta } _ { 1 } x _ { 1 }$ , but the response is affected by a second variable $x _ { 2 }$ such that the true regression function is

$$
E (y) = \beta_ {0} + \beta_ {1} x _ {1} + \beta_ {2} x _ {2}
$$

a. Is the least squares estimator of the slope in the original simple linear regression model unbiased?   
b. Show the bias in $\hat { \beta } _ { 1 }$ .

3.5 Suppose that we are -tting a straight line and wish to make the standard error of the slope as small as possible. Suppose that the 鈥渞egion of interest鈥?for $x$ is $- 1 \leq x \leq 1$ . Where should the observations $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ be taken? Discuss the practical aspects of this data collection plan.

3.6 Consider the simple linear regression model

$$
y = \beta_ {0} + \beta_ {1} x + \varepsilon ,
$$

where the intercept $\beta _ { 0 }$ is known.

a. Find the least squares estimator of $\beta _ { 1 }$ for this model. Does this answer seem reasonable?   
b. What is the variance of the slope $( { \hat { \beta } } _ { 1 } )$ for the least squares estimator found in part a?   
c. Find a $1 0 0 ( 1 - \alpha )$ percent CI for $\beta _ { 1 }$ . Is this interval narrower than the estimator for the case where both slope and intercept are unknown?

3.7 The quality of Pinot Noir wine is thought to be related to the properties of clarity, aroma, body, avor, and oakiness. Data for 38 wines are given in Table E3.4.

TABLE E3.4 Wine Quality Dataa (Found in Minitab)   

<table><tr><td>Clarity, x1</td><td>Aroma, x2</td><td>Body, x3</td><td>Flavor, x4</td><td>Oakiness, x5</td><td>Quality, y</td><td>Region</td></tr><tr><td>1</td><td>3.3</td><td>2.8</td><td>3.1</td><td>4.1</td><td>9.8</td><td>1</td></tr><tr><td>1</td><td>4.4</td><td>4.9</td><td>3.5</td><td>3.9</td><td>12.6</td><td>1</td></tr><tr><td>1</td><td>3.9</td><td>5.3</td><td>4.8</td><td>4.7</td><td>11.9</td><td>1</td></tr><tr><td>1</td><td>3.9</td><td>2.6</td><td>3.1</td><td>3.6</td><td>11.1</td><td>1</td></tr><tr><td>1</td><td>5.6</td><td>5.1</td><td>5.5</td><td>5.1</td><td>13.3</td><td>1</td></tr><tr><td>1</td><td>4.6</td><td>4.7</td><td>5</td><td>4.1</td><td>12.8</td><td>1</td></tr><tr><td>1</td><td>4.8</td><td>4.8</td><td>4.8</td><td>3.3</td><td>12.8</td><td>1</td></tr><tr><td>1</td><td>5.3</td><td>4.5</td><td>4.3</td><td>5.2</td><td>12</td><td>1</td></tr><tr><td>1</td><td>4.3</td><td>4.3</td><td>3.9</td><td>2.9</td><td>13.6</td><td>3</td></tr><tr><td>1</td><td>4.3</td><td>3.9</td><td>4.7</td><td>3.9</td><td>13.9</td><td>1</td></tr><tr><td>1</td><td>5.1</td><td>4.3</td><td>4.5</td><td>3.6</td><td>14.4</td><td>3</td></tr><tr><td>0.5</td><td>3.3</td><td>5.4</td><td>4.3</td><td>3.6</td><td>12.3</td><td>2</td></tr><tr><td>0.8</td><td>5.9</td><td>5.7</td><td>7</td><td>4.1</td><td>16.1</td><td>3</td></tr><tr><td>0.7</td><td>7.7</td><td>6.6</td><td>6.7</td><td>3.7</td><td>16.1</td><td>3</td></tr><tr><td>1</td><td>7.1</td><td>4.4</td><td>5.8</td><td>4.1</td><td>15.5</td><td>3</td></tr><tr><td>0.9</td><td>5.5</td><td>5.6</td><td>5.6</td><td>4.4</td><td>15.5</td><td>3</td></tr><tr><td>1</td><td>6.3</td><td>5.4</td><td>4.8</td><td>4.6</td><td>13.8</td><td>3</td></tr><tr><td>1</td><td>5</td><td>5.5</td><td>5.5</td><td>4.1</td><td>13.8</td><td>3</td></tr><tr><td>1</td><td>4.6</td><td>4.1</td><td>4.3</td><td>3.1</td><td>11.3</td><td>1</td></tr><tr><td>0.9</td><td>3.4</td><td>5</td><td>3.4</td><td>3.4</td><td>7.9</td><td>2</td></tr><tr><td>0.9</td><td>6.4</td><td>5.4</td><td>6.6</td><td>4.8</td><td>15.1</td><td>3</td></tr><tr><td>1</td><td>5.5</td><td>5.3</td><td>5.3</td><td>3.8</td><td>13.5</td><td>3</td></tr><tr><td>0.7</td><td>4.7</td><td>4.1</td><td>5</td><td>3.7</td><td>10.8</td><td>2</td></tr><tr><td>0.7</td><td>4.1</td><td>4</td><td>4.1</td><td>4</td><td>9.5</td><td>2</td></tr><tr><td>1</td><td>6</td><td>5.4</td><td>5.7</td><td>4.7</td><td>12.7</td><td>3</td></tr><tr><td>1</td><td>4.3</td><td>4.6</td><td>4.7</td><td>4.9</td><td>11.6</td><td>2</td></tr><tr><td>1</td><td>3.9</td><td>4</td><td>5.1</td><td>5.1</td><td>11.7</td><td>1</td></tr><tr><td>1</td><td>5.1</td><td>4.9</td><td>5</td><td>5.1</td><td>11.9</td><td>2</td></tr><tr><td>1</td><td>3.9</td><td>4.4</td><td>5</td><td>4.4</td><td>10.8</td><td>2</td></tr><tr><td>1</td><td>4.5</td><td>3.7</td><td>2.9</td><td>3.9</td><td>8.5</td><td>2</td></tr><tr><td>1</td><td>5.2</td><td>4.3</td><td>5</td><td>6</td><td>10.7</td><td>2</td></tr><tr><td>0.8</td><td>4.2</td><td>3.8</td><td>3</td><td>4.7</td><td>9.1</td><td>1</td></tr><tr><td>1</td><td>3.3</td><td>3.5</td><td>4.3</td><td>4.5</td><td>12.1</td><td>1</td></tr><tr><td>1</td><td>6.8</td><td>5</td><td>6</td><td>5.2</td><td>14.9</td><td>3</td></tr><tr><td>0.8</td><td>5</td><td>5.7</td><td>5.5</td><td>4.8</td><td>13.5</td><td>1</td></tr><tr><td>0.8</td><td>3.5</td><td>4.7</td><td>4.2</td><td>3.3</td><td>12.2</td><td>1</td></tr><tr><td>0.8</td><td>4.3</td><td>5.5</td><td>3.5</td><td>5.8</td><td>10.3</td><td>1</td></tr><tr><td>0.8</td><td>5.2</td><td>4.8</td><td>5.7</td><td>3.5</td><td>13.2</td><td>1</td></tr></table>

a The wine here is Pinot Noir. Region refers to distinct geographic regions.

a. Fit a multiple linear regression model relating wine quality to these predictors. Do not include the 鈥淩egion鈥?variable in the model.   
b. Test for signi-cance of regression. What conclusions can you draw?   
c. Use $t \cdot$ -tests to assess the contribution of each predictor to the model. Discuss your -ndings.   
d. Analyze the residuals from this model. Is the model adequate?   
e. Calculate $R ^ { 2 }$ and the adjusted $R ^ { 2 }$ for this model. Compare these values to the $R ^ { 2 }$ and adjusted $R ^ { 2 }$ for the linear regression model relating wine quality to only the predictors 鈥淎roma鈥?and 鈥淔lavor.鈥?Discuss your results.   
f. Find a $9 5 \%$ CI for the regression coef-cient for 鈥淔lavor鈥?for both models in part e. Discuss any differences.

3.8 Reconsider the wine quality data in Table E3.4. The 鈥淩egion鈥?predictor refers to three distinct geographical regions where the wine was produced. Note that this is a categorical variable.

a. Fit the model using the 鈥淩egion鈥?variable as it is given in Table E3.4. What potential dif-culties could be introduced by including this variable in the regression model using the three levels shown in Table E3.4?   
b. An alternative way to include the categorical variable 鈥淩egion鈥?would be to introduce two indicator variables $x _ { 1 }$ and $x _ { 2 }$ as follows:

<table><tr><td>Region</td><td>x1</td><td>x2</td></tr><tr><td>1</td><td>0</td><td>0</td></tr><tr><td>2</td><td>1</td><td>0</td></tr><tr><td>3</td><td>0</td><td>1</td></tr></table>

Why is this approach better than just using the codes 1, 2, and 3?

c. Rework Exercise 3.7 using the indicator variables de-ned in part b for 鈥淩egion.鈥?
3.9 Table B.6 in Appendix B contains data on the global mean surface air temperature anomaly and the global $\mathrm { C O } _ { 2 }$ concentration. Fit a regression model to these data, using the global $\mathrm { C O } _ { 2 }$ concentration as the predictor. Analyze the residuals from this model. Is there

evidence of autocorrelation in these data? If so, use one iteration of the Cochrane鈥揙rcutt method to estimate the parameters.

3.10 Table B.13 in Appendix B contains hourly yield measurements from a chemical process and the process operating temperature. Fit a regression model to these data, using the temperature as the predictor. Analyze the residuals from this model. Is there evidence of autocorrelation in these data?   
3.11 The data in Table E3.5 give the percentage share of market of a particular brand of canned peaches $( y _ { t } )$ for the past 15 months and the relative selling price $( x _ { t } )$ .

TABLE E3.5 Market Share and Price of Canned Peaches   

<table><tr><td>t</td><td>xt</td><td>yt</td><td>t</td><td>xt</td><td>yt</td></tr><tr><td>1</td><td>100</td><td>15.93</td><td>9</td><td>85</td><td>16.60</td></tr><tr><td>2</td><td>98</td><td>16.26</td><td>10</td><td>83</td><td>17.16</td></tr><tr><td>3</td><td>100</td><td>15.94</td><td>11</td><td>81</td><td>17.77</td></tr><tr><td>4</td><td>89</td><td>16.81</td><td>12</td><td>79</td><td>18.05</td></tr><tr><td>5</td><td>95</td><td>15.67</td><td>13</td><td>90</td><td>16.78</td></tr><tr><td>6</td><td>87</td><td>16.47</td><td>14</td><td>77</td><td>18.17</td></tr><tr><td>7</td><td>93</td><td>15.66</td><td>15</td><td>78</td><td>17.25</td></tr><tr><td>8</td><td>82</td><td>16.94</td><td></td><td></td><td></td></tr></table>

a. Fit a simple linear regression model to these data. Plot the residuals versus time. Is there any indication of autocorrelation?   
b. Use the Durbin鈥揥atson test to determine if there is positive autocorrelation in the errors. What are your conclusions?   
c. Use one iteration of the Cochrane鈥揙rcutt procedure to estimate the regression coef-cients. Find the standard errors of these regression coef-cients.   
d. Is there positive autocorrelation remaining after the -rst iteration? Would you conclude that the iterative parameter estimation technique has been successful?

3.12 The data in Table E3.6 give the monthly sales for a cosmetics manufacturer $( y _ { t } )$ and the corresponding monthly sales for the entire industry $( x _ { t } )$ . The units of both variables are millions of dollars.

a. Build a simple linear regression model relating company sales to industry sales. Plot the residuals against time. Is there any indication of autocorrelation?

TABLE E3.6 Cosmetic Sales Data for Exercise 3.12   

<table><tr><td>t</td><td>xt</td><td>yt</td><td>t</td><td>xt</td><td>yt</td></tr><tr><td>1</td><td>5.00</td><td>0.318</td><td>10</td><td>6.16</td><td>0.650</td></tr><tr><td>2</td><td>5.06</td><td>0.330</td><td>11</td><td>6.22</td><td>0.655</td></tr><tr><td>3</td><td>5.12</td><td>0.356</td><td>12</td><td>6.31</td><td>0.713</td></tr><tr><td>4</td><td>5.10</td><td>0.334</td><td>13</td><td>6.38</td><td>0.724</td></tr><tr><td>5</td><td>5.35</td><td>0.386</td><td>14</td><td>6.54</td><td>0.775</td></tr><tr><td>6</td><td>5.57</td><td>0.455</td><td>15</td><td>6.68</td><td>0.78</td></tr><tr><td>7</td><td>5.61</td><td>0.460</td><td>16</td><td>6.73</td><td>0.796</td></tr><tr><td>8</td><td>5.80</td><td>0.527</td><td>17</td><td>6.89</td><td>0.859</td></tr><tr><td>9</td><td>6.04</td><td>0.598</td><td>18</td><td>6.97</td><td>0.88</td></tr></table>

b. Use the Durbin鈥揥atson test to determine if there is positive autocorrelation in the errors. What are your conclusions?   
c. Use one iteration of the Cochrane鈥揙rcutt procedure to estimate the model parameters. Compare the standard error of these regression coef-cients with the standard error of the least squares estimates.   
d. Test for positive autocorrelation following the -rst iteration. Has the procedure been successful?

3.13 Reconsider the data in Exercise 3.12. De-ne a new set of transformed variables as the -rst difference of the original variables, $y _ { t } ^ { \prime } = y _ { t } -$ $y _ { t - 1 }$ and $x _ { t } ^ { \prime } = x _ { t } - x _ { t - 1 } .$ . Regress $y _ { t } ^ { \prime }$ on $x _ { t } ^ { \prime }$ through the origin. Compare the estimate of the slope from this -rst-difference approach with the estimate obtained from the iterative method in Exercise 3.12.   
3.14 Show that an equivalent way to perform the test for signi-cance of regression in multiple linear regression is to base the test on $R ^ { 2 }$ as follows. To test $H _ { 0 } : \beta _ { 1 } = \beta _ { 2 } = \cdots = \beta _ { k }$ versus $H _ { 1 }$ : at least one $\beta _ { j } \neq 0$ , calculate

$$
F _ {0} = \frac {R ^ {2} (n - p)}{k (1 - R ^ {2})}
$$

and reject $H _ { 0 }$ if the computed value of $F _ { 0 }$ exceeds $F _ { a , k , n - p }$ , where $p = k + 1$ .

3.15 Suppose that a linear regression model with $k = 2$ regressors has been -t to $n = 2 5$ observations and $R ^ { 2 } = 0 . 9 0$ .

a. Test for signi-cance of regression at $\alpha = 0 . 0 5$ . Use the results of the Exercise 3.14.   
b. What is the smallest value of $R ^ { 2 }$ that would lead to the conclusion of a signi-cant regression if $\alpha = 0 . 0 5 ?$ Are you surprised at how small this value of $R ^ { 2 }$ is?

3.16 Consider the simple linear regression model $y _ { t } = \beta _ { 0 } + \beta _ { 1 } x + \varepsilon _ { t }$ , where the errors are generated by the second-order autoregressive process

$$
\varepsilon_ {t} = \rho_ {1} \varepsilon_ {t - 1} + \rho_ {2} \varepsilon_ {t - 2} + a _ {t}
$$

Discuss how the Cochrane鈥揙rcutt iterative procedure could be used in this situation. What transformations would be used on the variables $y _ { t }$ and $x _ { t }$ ? How would you estimate the parameters $\rho _ { 1 }$ and $\rho _ { 2 }$ ?

3.17 Show that an alternate computing formula for the regression sum of squares in a linear regression model is

$$
S S _ {\mathrm {R}} = \sum_ {i = 1} ^ {n} \hat {y} _ {i} ^ {2} - n \bar {y} ^ {2}
$$

3.18 An article in Quality Engineering (The Catapult Problem: Enhanced Engineering Modeling Using Experimental Design, Vol. 4, 1992) conducted an experiment with a catapult to determine the effects of hook $( x _ { 1 } )$ , arm length $( x _ { 2 } )$ , start angle $\left( x _ { 3 } \right)$ , and stop angle $( x _ { 4 } )$ on the distance that the catapult throws a ball. They threw the ball three times for each setting of the factors. Table E3.7 summarizes the experimental results.

TABLE E3.7 Catapult Experiment Data for Exercise 3.18   

<table><tr><td>x1</td><td>x2</td><td>x3</td><td>x4</td><td colspan="3">y</td></tr><tr><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>28.0</td><td>27.1</td><td>26.2</td></tr><tr><td>-1</td><td>-1</td><td>1</td><td>1</td><td>46.5</td><td>43.5</td><td>46.5</td></tr><tr><td>-1</td><td>1</td><td>-1</td><td>1</td><td>21.9</td><td>21.0</td><td>20.1</td></tr><tr><td>-1</td><td>1</td><td>1</td><td>-1</td><td>52.9</td><td>53.7</td><td>52.0</td></tr><tr><td>1</td><td>-1</td><td>-1</td><td>1</td><td>75.0</td><td>73.1</td><td>74.3</td></tr><tr><td>1</td><td>-1</td><td>1</td><td>-1</td><td>127.7</td><td>126.9</td><td>128.7</td></tr><tr><td>1</td><td>1</td><td>-1</td><td>-1</td><td>86.2</td><td>86.5</td><td>87.0</td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>195.0</td><td>195.9</td><td>195.7</td></tr></table>

a. Fit a regression model to the data and perform a residual analysis for the model.   
b. Use the sample variances as the basis for WLS estimation of the original data (not the sample means).   
c. Fit an appropriate model to the sample variances. Use this model to develop the appropriate weights and repeat part b.

3.19 Consider the simple linear regression model $y _ { i } = \beta _ { 0 } + \beta _ { 1 } x _ { i } + \varepsilon _ { i }$ where the variance of $\varepsilon _ { i }$ is proportional to $x _ { i } ^ { 2 }$ ; that is, $\mathrm { V a r } \left( \varepsilon _ { i } \right) = \sigma ^ { 2 } x _ { i } ^ { 2 }$

a. Suppose that we use the transformations $y ^ { \prime } = y / x$ and $x ^ { \prime } = 1 / x$ . Is this a variance-stabilizing transformation?   
b. What are the relationships between the parameters in the original and transformed models?   
c. Suppose we use the method of WLS with $w _ { i } = 1 / x _ { i } ^ { 2 }$ . Is this equivalent to the transformation introduced in part a?

3.20 Consider the WLS normal equations for the case of simple linear regression where time is the predictor variable, Eq. (3.62). Suppose that the variances of the errors are proportional to the index of time such that $w _ { t } = 1 / t$ . Simplify the normal equations for this situation. Solve for the estimates of the model parameters.

3.21 Consider the simple linear regression model where time is the predictor variable. Assume that the errors are uncorrelated and have constant variance $\sigma ^ { 2 }$ . Show that the variances of the model parameter estimates are

$$
V (\hat {\beta} _ {0}) = \sigma^ {2} \frac {2 (2 T + 1)}{T (T - 1)}
$$

and

$$
V (\hat {\beta} _ {1}) = \sigma^ {2} \frac {1 2}{T (T ^ {2} - 1)}
$$

3.22 Analyze the regression model in Exercise 3.1 for leverage and inuence. Discuss your results.   
3.23 Analyze the regression model in Exercise 3.2 for leverage and inuence. Discuss your results.   
3.24 Analyze the regression model in Exercise 3.3 for leverage and inuence. Discuss your results.

3.25 Analyze the regression model for the wine quality data in Exercise 3.7 for leverage and inuence. Discuss your results.   
3.26 Consider the wine quality data in Exercise 3.7. Use variable selection techniques to determine an appropriate regression model for these data.   
3.27 Consider the catapult data in Exercise 3.18. Use variable selection techniques to determine an appropriate regression model for these data. In determining the candidate variables, consider all of the two-factor cross-products of the original four variables.   
3.28 Table B.10 in Appendix B presents monthly data on airline miles own in the United Kingdom. Fit an appropriate regression model to these data. Analyze the residuals and comment on model adequacy.   
3.29 Table B.11 in Appendix B presents data on monthly champagne sales. Fit an appropriate regression model to these data. Analyze the residuals and comment on model adequacy.   
3.30 Consider the data in Table E3.5. Fit a time series regression model with autocorrected errors to these data. Compare this model with the results you obtained in Exercise 3.11 using the Cochrane鈥揙rcutt procedure.   
3.31 Consider the data in Table E3.5. Fit the lagged variables regression models shown in Eqs. (3.119) and (3.120) to these data. Compare these models with the results you obtained in Exercise 3.11 using the Cochrane鈥揙rcutt procedure, and with the time series regression model from Exercise 3.30.   
3.32 Consider the data in Table E3.5. Fit a time series regression model with autocorrected errors to these data. Compare this model with the results you obtained in Exercise 3.13 using the Cochrane鈥揙rcutt procedure.   
3.33 Consider the data in Table E3.6. Fit the lagged variables regression models shown in Eqs. (3.119) and (3.120) to these data. Compare these models with the results you obtained in Exercise 3.13 using the Cochrane鈥揙rcutt procedure, and with the time series regression model from Exercise 3.32.   
3.34 Consider the global surface air temperature anomaly data and the $\mathrm { C O } _ { 2 }$ concentration data in Table B.6 in Appendix B. Fit a time series regression model to these data, using global surface air temperature

anomaly as the response variable. Is there any indication of autocorrelation in the residuals? What corrective action and modeling strategies would you recommend?

3.35 Table B.20 in Appendix B contains data on tax refund amounts and population. Fit an OLS regression model to these data.

a. Analyze the residuals and comment on model adequacy.   
b. Fit the lagged variables regression models shown in Eqs. (3.119) and (3.120) to these data. How do these models compare with the OLS model in part a?

3.36 Table B.25 contains data from the National Highway Traf-c Safety Administration on motor vehicle fatalities from 1966 to 2012, along with several other variables. These data are used by a variety of governmental and industry groups, as well as research organizations.

a. Plot the fatalities data. Comment on the graph.   
b. Construct a scatter plot of fatalities versus number of licensed drivers. Comment on the apparent relationship between these two factors.   
c. Fit a simple linear regression model to the fatalities data, using the number of licensed drivers as the predictor variable. Discuss the summary statistics from this model.   
d. Analyze the residuals from the model in part c. Discuss the adequacy of the -tted model.   
e. Calculate the Durbin鈥揥atson test statistic for the model in part c. Is there evidence of autocorrelation in the residuals? Is a time series regression model more appropriate than an OLS model for these data?

3.37 Consider the motor vehicle fatalities data in Appendix Table B.25 and the simple linear regression model from Exercise 3.36. There are several candidate predictors that could be added to the model. Add the number of registered motor vehicles to the model that you -t in Exercise 3.36. Has the addition of another predictor improved the model?

3.38 Consider the motor vehicle fatalities data in Appendix Table B.25. There are several candidate predictors that could be added to the model. Use stepwise regression to -nd an appropriate subset of predictors for the fatalities data. Analyze the residuals from the model, including the Durbin鈥揥atson test, and comment on model adequacy.

3.39 Consider the motor vehicle fatalities data in Appendix Table B.25. There are several candidate predictors that could be added to the model. Use an all-possible-models approach to -nd an appropriate subset of predictors for the fatalities data. Analyze the residuals from the model, including the Durbin鈥揥atson test, and comment on model adequacy. Compare this model to the one you obtained through stepwise model -tting in Exercise 3.38.

3.40 Appendix Table B.26 contains data on monthly single-family residential new home sales from 1963 through 2014. The number of building permits issued is also given in the table.

a. Plot the home sales data. Comment on the graph.   
b. Construct a scatter plot of home sales versus number of building permits. Comment on the apparent relationship between these two factors.   
c. Fit a simple linear regression model to the home sales data, using the number of building permits as the predictor variable. Discuss the summary statistics from this model.   
d. Analyze the residuals from the model in part c. Discuss the adequacy of the -tted model.   
e. Calculate the Durbin鈥揥atson test statistic for the model in part c. Is there evidence of autocorrelation in the residuals? Is a time series regression model more appropriate than an OLS model for these data?

# EXPONENTIAL SMOOTHINGMETHODS

If you have to forecast, forecast often.

EDGAR R. FIEDLER, American economist

# 4.1 INTRODUCTION

We can often think of a data set as consisting of two distinct components: signal and noise. Signal represents any pattern caused by the intrinsic dynamics of the process from which the data are collected. These patterns can take various forms from a simple constant process to a more complicated structure that cannot be extracted visually or with any basic statistical tools. The constant process, for example, is represented as

$$
y _ {t} = \mu + \varepsilon_ {t}, \tag {4.1}
$$

where $\mu$ represents the underlying constant level of system response and $\varepsilon _ { t }$ is the noise at time t. The $\varepsilon _ { t }$ is often assumed to be uncorrelated with mean 0 and constant variance $\sigma _ { \epsilon } ^ { 2 }$ .

We have already discussed some basic data smoothers in Section 2.2.2. Smoothing can be seen as a technique to separate the signal and the noise

![](images/6de58f16529f027c1843bfa87fa6df38a8958f42ff5de9855f58208d32a13da1.jpg)  
FIGURE 4.1 The process of smoothing a data set.

as much as possible and in that a smoother acts as a -lter to obtain an 鈥渆stimate鈥?for the signal. In Figure 4.1, we give various types of signals that with the help of a smoother can be 鈥渞econstructed鈥?and the underlying pattern of the signal is to some extent recovered. The smoothers that we will discuss in this chapter achieve this by simply relating the current observation to the previous ones. For a given data set, one can devise forward and/or backward looking smoothers but in this chapter we will only consider backward looking smoothers. That is, at any given T, the observation $y _ { T }$ will be replaced by a combination of observations at and before T. It does then intuitively make sense to use some sort of an 鈥渁verage鈥?of the current and the previous observations to smooth the data. An obvious choice is to replace the current observation with the average of the observations at $T , T { \mathrm { - } } 1 , \ldots , 1 .$ . In fact this is the 鈥渂est鈥?choice in the least squares sense for a constant process given in Eq. (4.1).

A constant process can be smoothed by replacing the current observation with the best estimate for $\mu$ . Using the least squares criterion, we de-ne the error sum of squares, ${ \bf S } { \bf S } _ { \mathrm { E } }$ , for the constant process as

$$
S S _ {E} = \sum_ {t = 1} ^ {T} (y _ {t} - \mu) ^ {2}.
$$

![](images/e5adb886ebb879bb1e3029c9bd3fb4f51535c0f7ae0823a32056684b328e067e.jpg)  
FIGURE 4.2 The Dow Jones Index from June 1999 to June 2001.

The least squares estimate of $\mu$ can be found by setting the derivative of SS with respect to $\mu$ to 0. This gives

$$
\hat {\mu} = \frac {1}{T} \sum_ {t = 1} ^ {T} y _ {t}, \tag {4.2}
$$

where $\widehat { \mu }$ is the least squares estimate of $\mu$ . Equation (4.2) shows that the least squares estimate of $\mu$ is indeed the average of observations up to time $T$ .

Figure 4.2 shows the monthly data for the Dow Jones Index from June 1999 to June 2001. Visual inspection suggests that a constant model can be used to describe the general pattern of the data.1 To further con-rm this claim, we use the smoother described in Eq. (4.2) for each data point by taking the average of the available data up to that point in time. The smoothed observations are shown by the line segments in Figure 4.2. It can be seen that the smoother in Eq. (4.2) indeed extracts the main pattern

![](images/c9316d19203ae0912839e4c0882da84dbd6a5a596ff0fb54f33beda71234662a.jpg)  
FIGURE 4.3 The Dow Jones Index from June 1999 to June 2006.

in the data and leads to the conclusion that during the 2-year period from June 1999 to June 2001, the Dow Jones Index was quite stable.

As we can see, for the constant process the smoother in Eq. (4.2) is quite effective in providing a clear picture of the underlying pattern. What happens if the process is not constant but exhibits a more complicated pattern? Consider again, for example, the Dow Jones Index from June 1999 to June 2006 given in Figure 4.3 (the complete data set is in Table 4.1). It is clear that the data do not follow the behavior typical of a constant behavior during this period. In Figure 4.3, we can also see the pattern that the smoother in Eq. (4.2) extracts for the same period. As the process changes, this smoother is having trouble keeping up with the process. What could be the reason for the poor performance after June 2001? The answer is quite simple: the constant process assumption is no longer valid. However, as time goes on, the smoother in Eq. (4.2) accumulates more and more data points and gains some sort of 鈥渋nertia鈥? So when there is a change in the process, it becomes increasingly more dif-cult for this smoother to react to it.

How often is the constant process assumption violated? The answer to this question is provided by the Second Law of Thermodynamics, which in the most simplistic way states that if left on its own (free of external inuences) any system will deteriorate. Thus the constant process is not

TABLE 4.1 Dow Jones Index at the End of the Month from June 1999 to June 2006   

<table><tr><td>Date</td><td>Dow Jones</td><td>Date</td><td>Dow Jones</td><td>Date</td><td>Dow Jones</td><td>Date</td><td>Dow Jones</td></tr><tr><td>Jun-99</td><td>10,970.8</td><td>Apr-01</td><td>10,735</td><td>Feb-03</td><td>7891.08</td><td>Dec-04</td><td>10,783</td></tr><tr><td>Jul-99</td><td>10,655.2</td><td>May-01</td><td>10,911.9</td><td>Mar-03</td><td>7992.13</td><td>Jan-05</td><td>10,489.9</td></tr><tr><td>Aug-99</td><td>10,829.3</td><td>Jun-01</td><td>10,502.4</td><td>Apr-03</td><td>8480.09</td><td>Feb-05</td><td>10,766.2</td></tr><tr><td>Sep-99</td><td>10,337</td><td>Jul-01</td><td>10,522.8</td><td>May-03</td><td>8850.26</td><td>Mar-05</td><td>10,503.8</td></tr><tr><td>Oct-99</td><td>10,729.9</td><td>Aug-01</td><td>9949.75</td><td>Jun-03</td><td>8985.44</td><td>Apr-05</td><td>10,192.5</td></tr><tr><td>Nov-99</td><td>10,877.8</td><td>Sep-01</td><td>8847.56</td><td>Jul-03</td><td>9233.8</td><td>May-05</td><td>10,467.5</td></tr><tr><td>Dec-99</td><td>11,497.1</td><td>Oct-01</td><td>9075.14</td><td>Aug-03</td><td>9415.82</td><td>Jun-05</td><td>10,275</td></tr><tr><td>Jan-00</td><td>10,940.5</td><td>Nov-01</td><td>9851.56</td><td>Sep-03</td><td>9275.06</td><td>Jul-05</td><td>10,640.9</td></tr><tr><td>Feb-00</td><td>10,128.3</td><td>Dec-01</td><td>10,021.6</td><td>Oct-03</td><td>9801.12</td><td>Aug-05</td><td>10,481.6</td></tr><tr><td>Mar-00</td><td>10,921.9</td><td>Jan-02</td><td>9920</td><td>Nov-03</td><td>9782.46</td><td>Sep-05</td><td>10,568.7</td></tr><tr><td>Apr-00</td><td>10,733.9</td><td>Feb-02</td><td>10,106.1</td><td>Dec-03</td><td>10,453.9</td><td>Oct-05</td><td>10,440.1</td></tr><tr><td>May-00</td><td>10,522.3</td><td>Mar-02</td><td>10,403.9</td><td>Jan-04</td><td>10488.1</td><td>Nov-05</td><td>10,805.9</td></tr><tr><td>Jun-00</td><td>10,447.9</td><td>Apr-02</td><td>9946.22</td><td>Feb-04</td><td>10,583.9</td><td>Dec-05</td><td>10,717.5</td></tr><tr><td>Jul-00</td><td>10,522</td><td>May-02</td><td>9925.25</td><td>Mar-04</td><td>10,357.7</td><td>Jan-06</td><td>10,864.9</td></tr><tr><td>Aug-00</td><td>11,215.1</td><td>Jun-02</td><td>9243.26</td><td>Apr-04</td><td>10,225.6</td><td>Feb-06</td><td>10,993.4</td></tr><tr><td>Sep-00</td><td>10,650.9</td><td>Jul-02</td><td>8736.59</td><td>May-04</td><td>10,188.5</td><td>Mar-06</td><td>11,109.3</td></tr><tr><td>Oct-00</td><td>10,971.1</td><td>Aug-02</td><td>8663.5</td><td>Jun-04</td><td>10,435.5</td><td>Apr-06</td><td>11,367.1</td></tr><tr><td>Nov-00</td><td>10,414.5</td><td>Sep-02</td><td>7591.93</td><td>Jul-04</td><td>10,139.7</td><td>May-06</td><td>11,168.3</td></tr><tr><td>Dec-00</td><td>10,788</td><td>Oct-02</td><td>8397.03</td><td>Aug-04</td><td>10,173.9</td><td>Jun-06</td><td>11,247.9</td></tr><tr><td>Jan-01</td><td>10,887.4</td><td>Nov-02</td><td>8896.09</td><td>Sep-04</td><td>10,080.3</td><td></td><td></td></tr><tr><td>Feb-01</td><td>10,495.3</td><td>Dec-02</td><td>8341.63</td><td>Oct-04</td><td>10,027.5</td><td></td><td></td></tr><tr><td>Mar-01</td><td>9878.78</td><td>Jan-03</td><td>8053.81</td><td>Nov-04</td><td>10,428</td><td></td><td></td></tr></table>

the norm but at best an exception. So what can we do to deal with this issue? Recall that the problem with the smoother in Eq. (4.2) was that it reacted too slowly to process changes because of its inertia. In fact, when there is a change in the process, earlier data no longer carry the information about the change in the process, yet they contribute to this inertia at an equal proportion compared to the more recent (and probably more useful) data. The most obvious choice is to somehow discount the older data. Also recall that in a simple average, as in Eq. (4.2), all the observations are weighted equally and hence have the same amount of inuence on the average. Thus, if the weights of each observation are changed so that earlier observations are weighted less, a faster reacting smoother should be obtained. As mentioned in Section 2.2.2, a common solution is to use the simple moving average given in Eq. (2.3):

$$
M _ {T} = \frac {y _ {T} + y _ {T - 1} + \cdots + y _ {T - N + 1}}{N} = \frac {1}{N} \sum_ {t = T - N + 1} ^ {N} y _ {t}.
$$

The most crucial issue in simple moving averages is the choice of the span, N. A simple moving average will react faster to the changes if $N$ is small. However, we know from Section 2.2.2 that the variance of the simple moving average with uncorrelated observations with variance $\sigma ^ { 2 }$ is given as

$$
\mathrm {V a r} (M _ {T}) = \frac {\sigma^ {2}}{N}.
$$

This means that as $N$ gets small, the variance of the moving average gets bigger. This creates a dilemma in the choice of $N .$ . If the process is expected to be constant, a large $N$ can be used whereas a small $N$ is preferred if the process is changing. In Figure 4.4, we show the effect of going from a span of 10 observations to 5 observations. While the latter exhibits a more jittery behavior, it nevertheless follows the actual data more closely. A more thorough analysis on the choice of N can be performed based on the prediction error. We will explore this for exponential smoothers in Section 4.6.1, where we will discuss forecasting using exponential smoothing.

A -nal note on the moving average is that even if the individual observations are independent, the moving averages will be autocorrelated as two successive moving averages contain the same $N { - } 1$ observations. In fact,

![](images/413db723ce348e34f12f963749a694bced37925efed02aa3c237ab652d008177.jpg)  
FIGURE 4.4 The Dow Jones Index from June 1999 to June 2006 with moving averages of span 5 and 10.

the autocorrelation function (ACF) of the moving averages that are $k$ -lags apart is given as

$$
\rho k = \left\{ \begin{array}{l l} 1 - \frac {| k |}{N}, & k <   N \\ 0, & k \geq N \end{array} \right..
$$

# 4.2 FIRST-ORDER EXPONENTIAL SMOOTHING

Another approach to obtain a smoother that will react to process changes faster is to give geometrically decreasing weights to the past observations. Hence an exponentially weighted smoother is obtained by introducing a discount factor $\theta$ as

$$
\sum_ {t = 0} ^ {T - 1} \theta^ {t} y _ {T - t} = y _ {T} + \theta y _ {T - 1} + \theta^ {2} y _ {T - 2} + \dots + \theta^ {T - 1} y _ {1}. \tag {4.3}
$$

Please note that if the past observations are to be discounted in a geometrically decreasing manner, then we should have $| \theta | < 1$ . However, the smoother in Eq. (4.3) is not an average as the sum of the weights is

$$
\sum_ {t = 0} ^ {T - 1} \theta^ {t} = \frac {1 - \theta^ {T}}{1 - \theta} \tag {4.4}
$$

and hence does not necessarily add up to 1. For that we can adjust the smoother in Eq. (4.3) by multiplying it by $( 1 - \theta ) / ( 1 - \theta ^ { T } )$ . However, for large $T$ values, $\theta ^ { T }$ goes to zero and so the exponentially weighted average will have the following form:

$$
\begin{array}{l} \widetilde {y} _ {T} = (1 - \theta) \sum_ {t = 0} ^ {T - 1} \theta^ {t} y _ {T - t} \tag {4.5} \\ = (1 - \theta) \left(y _ {T} + \theta y _ {T - 1} + \theta^ {2} y _ {T - 2} + \dots + \theta^ {T - 1} y _ {1}\right) \\ \end{array}
$$

This is called a simple or -rst-order exponential smoother. There is an extensive literature on exponential smoothing. For example, see the books by Brown (1963), Abraham and Ledolter (1983), and Montgomery et al. (1990), and the papers by Brown and Meyer (1961), Chat-eld and Yar (1988), Cox (1961), Gardner (1985), Gardner and Dannenbring (1980), and Ledolter and Abraham (1984).

An alternate expression in a recursive form for simple exponential smoothing is given by

$$
\begin{array}{l} \widetilde {y} _ {T} = (1 - \theta) y _ {T} + (1 - \theta) \left(\theta y _ {T - 1} + \theta^ {2} y _ {T - 2} + \dots + \theta^ {T - 1} y _ {1}\right) \\ = (1 - \theta) y _ {T} + \theta \underbrace {(1 - \theta) \left(y _ {T - 1} + \theta^ {1} y _ {T - 2} + \cdots + \theta^ {T - 2} y _ {1}\right)} _ {\tilde {y} _ {T - 1}} \tag {4.6} \\ = (1 - \theta) y _ {T} + \theta \tilde {y} _ {T - 1}. \\ \end{array}
$$

The recursive form in Eq. (4.6) shows that -rst-order exponential smoothing can also be seen as the linear combination of the current observation and the smoothed observation at the previous time unit. As the latter contains the data from all previous observations, the smoothed observation at time $T$ is in fact the linear combination of the current observation and the discounted sum of all previous observations. The simple exponential smoother is often represented in a different form by setting $\lambda = 1 { - } \theta$ ,

$$
\tilde {y} _ {T} = \lambda y _ {T} + (1 - \lambda) \tilde {y} _ {T - 1} \tag {4.7}
$$

In this representation the discount factor, $\lambda$ , represents the weight put on the last observation and $( 1 - \lambda )$ represents the weight put on the smoothed value of the previous observations.

Analogous to the size of the span in moving average smoothers, an important issue for the exponential smoothers is the choice of the discount factor, 饾渾. Moreover, from Eq. (4.7), we can see that the calculation of $\tilde { y } _ { 1 }$ would require us to know $\tilde { y } _ { 0 }$ . We will discuss these issues in the next two sections.

# 4.2.1 The Initial Value, $\tilde { y } _ { 0 }$

Since $\tilde { y } _ { 0 }$ is needed in the recursive calculations that start with $\tilde { y } _ { 1 } = \lambda y _ { 1 } +$ $( 1 - \lambda ) \tilde { y } _ { 0 }$ , its value needs to be estimated. But from Eq. (4.7) we have

$$
\begin{array}{l} \tilde {y} _ {1} = \lambda y _ {1} + (1 - \lambda) \tilde {y} _ {0} \\ \tilde {y} _ {2} = \lambda y _ {2} + (1 - \lambda) \tilde {y} _ {1} = \lambda y _ {2} + (1 - \lambda) \left(\lambda y _ {1} + (1 - \lambda) \tilde {y} _ {0}\right) \\ = \lambda \left(y _ {2} + (1 - \lambda) y _ {1}\right) + (1 - \lambda) ^ {2} \tilde {y} _ {0} \\ \end{array}
$$

$$
\begin{array}{l} \tilde {y} _ {3} = \lambda (y _ {3} + (1 - \lambda) y _ {2} + (1 - \lambda) ^ {2} y _ {1}) + (1 - \lambda) ^ {3} \tilde {y} _ {0} \\ \vdots \\ \tilde {y} _ {T} = \lambda (y _ {T} + (1 - \lambda) y _ {T - 1} + \dots + (1 - \lambda) ^ {T - 1} y _ {1}) + (1 - \lambda) ^ {T} \tilde {y} _ {0}, \\ \end{array}
$$

which means that as $T$ gets large and hence $( 1 ~ - ~ \lambda ) ^ { T }$ gets small, the contribution of $\tilde { y } _ { 0 }$ to $\tilde { y } _ { T }$ becomes negligible. Thus for large data sets, the estimation of $\tilde { y } _ { 0 }$ has little relevance. Nevertheless, two commonly used estimates for $\tilde { y } _ { 0 }$ are the following.

1. Set $\tilde { y } _ { 0 } = y _ { 1 }$ . If the changes in the process are expected to occur early and fast, this choice for the starting value for $\tilde { y } _ { T }$ is reasonable.   
2. Take the average of the available data or a subset of the available data, $\tilde { y }$ , and set $\tilde { y } _ { 0 } = \bar { y }$ . If the process is at least at the beginning locally constant, this starting value may be preferred.

# 4.2.2 The Value of 饾潃

In Figures 4.5 and 4.6, respectively, we have two simple exponential smoothers for the Dow Jones Index data with $\lambda = 0 . 2$ and $\lambda = 0 . 4$ . It can be seen that in the latter the smoothed values follow the original observations more closely. In general, as $\lambda$ gets closer to 1, and more emphasis is put on the last observation, the smoothed values will approach the original observations. Two extreme cases will be when $\lambda = 0$ and $\lambda = 1$ . In the former, the smoothed values will all be equal to a constant, namely, $y _ { 0 }$ .

![](images/284fae2f1d30fc6630bb699e2f7512508d42b9fdf0babc79b8e32d0c82022d91.jpg)  
FIGURE 4.5 The Dow Jones Index from June 1999 to June 2006 with -rst-order exponential smoothing with $\lambda = 0 . 2$ .

![](images/ea1c7a854657d9ae23f8d6881af9687717c139bdeec6f2adac2410cd501840ea.jpg)  
FIGURE 4.6 The Dow Jones Index from June 1999 to June 2006 with -rst-order exponential smoothing with $\lambda = 0 . 4$ .

We can think of the constant line as the 鈥渟moothest鈥?version of whatever pattern the actual time series follows. For $\lambda = 1$ , we have $\tilde { y } _ { T } = y _ { T }$ and this will represent the 鈥渓east鈥?smoothed (or unsmoothed) version of the original time series. We can accordingly expect the variance of the simple exponential smoother to vary between 0 and the variance of the original time series based on the choice of 饾渾. Note that under the independence and constant variance assumptions we have

$$
\begin{array}{l} \operatorname {V a r} \left(\tilde {y} _ {T}\right) = \operatorname {V a r} \left(\lambda \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} y _ {T - t}\right) \\ = \lambda^ {2} \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {2 t} \operatorname {V a r} \left(y _ {T - t}\right) \\ = \lambda^ {2} \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {2 t} \operatorname {V a r} \left(y _ {T}\right) \tag {4.8} \\ = \operatorname {V a r} \left(y _ {T}\right) \lambda^ {2} \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {2 t} \\ = \frac {\lambda}{(2 - \lambda)} \operatorname {V a r} \left(y _ {T}\right). \\ \end{array}
$$

Thus the question will be how much smoothing is needed. In the literature, 饾渾 values between 0.1 and 0.4 are often recommended and do indeed perform well in practice. A more rigorous method of -nding the right 饾渾 value will be discussed in Section 4.6.1.

Example 4.1 Consider the Dow Jones Index from June 1999 to June 2006 given in Figure 4.3. For -rst-order exponential smoothing we would need to address two issues as stated in the previous sections: how to pick the initial value $y _ { 0 }$ and the smoothing constant 饾渾. Following the recommendation in Section 4.2.2, we will consider the smoothing constants 0.2 and 0.4. As for the initial value, we will consider the -rst recommendation in Section 4.2.1 and set $\tilde { y } _ { 0 } = y _ { 1 }$ . Figures 4.5 and 4.6 show the smoothed and actual data obtained from Minitab with smoothing constants 0.2 and 0.4, respectively.

Note that Minitab reports several measures of accuracy; MAPE, MAD, and MSD. Mean absolute percentage error (MAPE) is the average absolute percentage change between the predicted value that is $\tilde { y } _ { t - 1 }$ for a one-stepahead forecast and the true value, given as

$$
\mathrm {M A P E} = \frac {\sum_ {t = 1} ^ {T} | (y _ {t} - \tilde {y} _ {t - 1}) / y _ {t} |}{T} \times 1 0 0 (y _ {t} \neq 0).
$$

Mean absolute deviation (MAD) is the average absolute difference between the predicted and the true values, given as

$$
\mathrm {M A D} = \frac {\sum_ {t = 1} ^ {T} | (y _ {t} - \tilde {y} _ {t - 1}) |}{T}.
$$

Mean squared deviation (MSD) is the average squared difference between the predicted and the true values, given as

$$
\mathrm {M S D} = \frac {\sum_ {t = 1} ^ {T} (y _ {t} - \tilde {y} _ {t - 1}) ^ {2}}{T}.
$$

It should also be noted that the smoothed data with $\lambda = 0 . 4$ follows the actual data closer. However, in both cases, when there is an apparent linear trend in the data (e.g., from February 2003 to February 2004) the smoothed values consistently underestimate the actual data. We will discuss this issue in greater detail in Section 4.3.

As an alternative estimate for the initial value, we can also use the average of the data between June 1999 and June 2001, since during this period the time series data appear to be stable. Figures 4.7 and 4.8 show

![](images/db4f2ed385ccb51ffe7f28a21b8f6ae8f56a7108894a9efff68af9518b38d888.jpg)  
FIGURE 4.7 The Dow Jones Index from June 1999 to June 2006 with -rst-order exponential smoothing with $\lambda = 0 . 2$ and $\textstyle { \tilde { y } } _ { 0 } = ( \sum _ { t = 1 } ^ { 2 5 } y _ { t } / 2 5 )$ (i.e., initial value equal to the average of the -rst 25 observations).

![](images/da54f75fb56feec8c84cd96fd068fe75c7e292df68f90d98eeb47fc65eab6cc3.jpg)  
FIGURE 4.8 The Dow Jones Index from June 1999 to June 2006 with -rst-order exponential smoothing with $\lambda = 0 . 4$ and $\textstyle { \tilde { y } } _ { 0 } = ( \sum _ { t = 1 } ^ { 2 5 } y _ { t } / 2 5 )$ (i.e., initial value equal to the average of the -rst 25 observations).

the single exponential smoothing with the initial value equal to the average of the -rst 25 observations corresponding to the period between June 1999 and June 2001. Note that the choice of the initial value has very little effect on the smoothed values as time goes on.

# 4.3 MODELING TIME SERIES DATA

In Section 4.1, we considered the constant process where the time series data are expected to vary around a constant level with random uctuations, which are usually characterized by uncorrelated errors with mean 0 and constant variance $\sigma _ { \varepsilon } ^ { 2 }$ . In fact the constant process represents a very special case in a more general set of models often used in modeling time series data as a function of time. The general class of models can be represented as

$$
y _ {t} = f (t; \beta) + \varepsilon_ {\mathrm {t}}, \tag {4.9}
$$

where $\beta$ is the vector of unknown parameters and $\varepsilon _ { t }$ represents the uncorrelated errors. Thus as a member of this general class of models, the constant process can be represented as

$$
y _ {t} = \beta_ {0} + \varepsilon_ {\mathrm {t}}, \tag {4.10}
$$

where $\beta _ { O }$ is equal to $\mu$ in Eq. (4.1). We have seen in Chapter 3 how to estimate and make inferences about the regression coef-cients. The same principles apply to the class of models in Eq. (4.9). However, we have seen in Section 4.1 that the least squares estimates for $\beta _ { 0 }$ at any given time $T$ will be very slow to react to changes in the level of the process. For that, we suggested to use either the moving average or simple exponential smoothing.

As mentioned earlier, smoothing techniques are effective in illustrating the underlying pattern in the time series data. We have so far focused particularly on exponential smoothing techniques. For the class of models given in Eq. (4.9), we can -nd another use for the exponential smoothers: model estimation. Indeed for the constant process, we can see the simple exponential smoother as the estimate of the process level, or in regards to Eq. (4.10) an estimate of $\beta _ { 0 }$ . To show this in greater detail we need to introduce the sum of weighted squared errors for the constant process. Remember that the sum of squared errors for the constant process is given by

$$
S S _ {E} = \sum_ {t = 1} ^ {T} (y _ {t} - \mu) ^ {2}.
$$

If we argue that not all observations should have equal inuence on the sum and decide to introduce a string of weights that are geometrically decreasing in time, the sum of squared errors becomes

$$
S S _ {E} ^ {*} = \sum_ {t = 0} ^ {T - 1} \theta^ {t} \left(y _ {T - 1} - \beta_ {0}\right) ^ {2}, \tag {4.11}
$$

where $| \theta | \ 1 \textless \ 1$ . To -nd the least squares estimate for $\beta _ { 0 }$ , we take the derivative of Eq. (4.11) with respect to $\beta _ { 0 }$ and set it to zero:

$$
\left. \frac {d S S _ {E} ^ {*}}{d \beta_ {0}} \right| _ {\beta_ {0}} = - 2 \sum_ {t = 0} ^ {T - 1} \theta^ {t} \left(y _ {T - t} - \hat {\beta} _ {0}\right) = 0. \tag {4.12}
$$

The solution to Eq. (4.12), $\hat { \beta } _ { 0 }$ , which is the least squares estimate of $\beta _ { 0 }$ , is

$$
\hat {\beta} _ {0} \sum_ {t = 0} ^ {T - 1} \theta^ {t} = \sum_ {t = 0} ^ {T - 1} \theta^ {t} y _ {T - t}. \tag {4.13}
$$

From Eq. (4.4), we have

$$
\hat {\beta} _ {0} = \frac {1 - \theta}{1 - \theta^ {T}} \sum_ {t = 0} ^ {T - 1} \theta^ {t} y _ {T - t}. \tag {4.14}
$$

Once again for large T, $\theta ^ { T }$ goes to zero. We then have

$$
\hat {\beta} _ {0} = (1 - \theta) \sum_ {t = 0} ^ {T - 1} \theta^ {t} y _ {T - t}. \tag {4.15}
$$

We can see from Eqs. (4.5) and (4.15) that $\beta _ { 0 } = \widetilde { y } _ { T }$ . Thus the simple exponential smoothing procedure does in fact provide a weighted least squares estimate of $\beta _ { 0 }$ in the constant process with weights that are exponentially decreasing in time.

Now we return to our general class of models given in Eq. (4.9) and note that $f ( t ; \beta )$ can in fact be any function of t. For practical purposes it is usually more convenient to consider the polynomial family for nonseasonal time series. For seasonal time series, we will consider other forms of $f ( t ; \beta )$ that -t the data and exhibit a certain periodicity better. In the polynomial family, the constant process is indeed the simplest model we can consider. We will now consider the next obvious choice: the linear trend model.

# 4.4 SECOND-ORDER EXPONENTIAL SMOOTHING

We will now return to our Dow Jones Index data but consider only the subset of the data from February 2003 to February 2004 as given in Figure 4.9. Evidently for that particular time period it was a bullish market and correspondingly the Dow Jones Index exhibits an upward linear trend as indicated with the dashed line.

For this time period, an appropriate model in time from the polynomial family should be the linear trend model given as

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \varepsilon_ {t}, \tag {4.16}
$$

where the $\varepsilon _ { t }$ is once again assumed to be uncorrelated with mean 0 and constant variance $\boldsymbol { \sigma ^ { 2 } } _ { \varepsilon }$ . Based on what we have learned so far, we may attempt to smooth/model this linear trend using the simple exponential smoothing procedure. The actual and -tted values for the simple exponential smoothing procedure are given in Figure 4.10. For the exponential

![](images/a073c78e8800d9a07524cbc6dbd2dd930436b491e59ec0c3c9c9b0dd3927f0c2.jpg)  
FIGURE 4.9 The Dow Jones Index from February 2003 to February 2004.

smoother, without any loss of generality, we used $\tilde { y } _ { 0 } = y _ { 1 }$ and $\lambda { = } 0 . 3$ . From Figure 4.10, we can see that while the simple exponential smoother was to some extent able to capture the slope of the linear trend, it also exhibits some bias. That is, the -tted values based on the exponential smoother are consistently underestimating the actual data. More interestingly, the amount of underestimation is more or less constant for all observations.

![](images/896160567d6419b020b128f535954049c9af3657086bec9fb8b25d236fc4524c.jpg)  
FIGURE 4.10 The Dow Jones Index from February 2003 to February 2004 with simple exponential smoothing with $\lambda = 0 . 3$ .

In fact similar behavior for the simple exponential smoother can be observed in Figure 4.5 for the entire data from June 1999 to June 2006. Whenever the data exhibit a linear trend, the simple exponential smoother seems to over- or underestimates the actual data consistently. To further explore this, we will consider the expected value of $\tilde { y } _ { T }$ ,

$$
\begin{array}{l} E (\tilde {y} _ {T}) = E \left(\lambda \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} y _ {T - t}\right) \\ = \lambda \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} E \left(y _ {T - t}\right). \\ \end{array}
$$

For the linear trend model in Eq. (4.16), $E \left( y _ { t } \right) = \beta _ { 0 } + \beta _ { 1 } t$ . So we have

$$
\begin{array}{l} E (\tilde {y} _ {T}) = \lambda \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} (\beta_ {0} + \beta_ {1} (T - t)) \\ = \lambda \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} (\beta_ {0} + \beta_ {1} T) - \lambda \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} (\beta_ {1} t) \\ = (\beta_ {0} + \beta_ {1} T) \lambda \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} - \lambda \beta_ {1} \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} t. \\ \end{array}
$$

But for the in-nite sums we have

$$
\sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} = \frac {1}{1 - (1 - \lambda)} = \frac {1}{\lambda} \mathrm {a n d} \sum_ {t = 0} ^ {\infty} (1 - \lambda) ^ {t} t = \frac {1 - \lambda}{\lambda^ {2}}.
$$

Hence the expected value of the simple exponential smoother for the linear trend model is

$$
\begin{array}{l} E \left(\tilde {y} _ {T}\right) = \left(\beta_ {0} + \beta_ {1} T\right) - \frac {1 - \lambda}{\lambda} \beta_ {1} \tag {4.17} \\ = E (y _ {T}) - \frac {1 - \lambda}{\lambda} \beta_ {1}. \\ \end{array}
$$

This means that the simple exponential smoother is a biased estimator for the linear trend model and the amount of bias is $- [ ( 1 - \lambda ) / \lambda ] \beta _ { 1 }$ . This indeed explains the underestimation in Figure 4.10. One solution will be to use a large $\lambda$ value since $( 1 - \lambda ) / \lambda \to 0$ as $\lambda \to 1$ . In Figure 4.11, we show two simple exponential smoothers with $\lambda = 0 . 3$ and $\lambda = 0 . 9 9$ . It can be

![](images/436e33fbb78f32599ba1ba0d2c2efb36043b8b01dfd710aef47f5ddb91021ce4.jpg)  
FIGURE 4.11 The Dow Jones Index from June 1999 to June 2006 using exponential smoothing with $\lambda = 0 . 3$ and 0.99.

seen that the latter does a better job in capturing the linear trend. However, it should also be noted that as the smoother with $\lambda = 0 . 9 9$ follows the actual observations very closely, it fails to smooth out the constant pattern during the -rst 2 years of the data. A method based on adaptive updating of the discount factor, 饾渾, following the changes in the process is given in Section 4.6.4. In this section to model a linear trend model we will instead introduce the second-order exponential smoothing by applying simple exponential smoothing on $\tilde { y } _ { T }$ as

$$
\tilde {y} _ {T} ^ {(2)} = \lambda \tilde {y} _ {T} ^ {(1)} + (1 - \lambda) \tilde {y} _ {T - 1} ^ {(2)}, \tag {4.18}
$$

where $\tilde { y } _ { T } ^ { ( 1 ) }$ and $\tilde { y } _ { T } ^ { ( 2 ) }$ denote the -rst- and second-order smoothed exponentials, respectively. Of course, in Eq. (4.18) we can use a different $\lambda$ than in Eq. (4.7). However, for the derivations that follow, we will assume that the same $\lambda$ is used in the calculations of both $\tilde { y } _ { T } ^ { ( 1 ) }$ and $\tilde { y } _ { T } ^ { ( 2 ) }$ .

From Eq. (4.17), we can see that the -rst-order exponential smoother introduces bias in estimating a linear trend. It can also be seen in Figure 4.7 that the -rst-order exponential smoother for the linear trend model exhibits a linear trend as well. Hence the second-order smoother鈥攖hat is,

a -rst-order exponential smoother of the original -rst-order exponential smoother鈥攕hould also have a bias. We can represent this as

$$
E \left(\tilde {y} _ {T} ^ {(2)}\right) = E \left(\tilde {y} _ {T} ^ {(1)}\right) - \frac {1 - \lambda}{\lambda} \beta_ {1}. \tag {4.19}
$$

From Eq. (4.19), an estimate for $\beta _ { 1 }$ at time $T$ is

$$
\hat {\beta} _ {1, T} = \frac {\lambda}{1 - \lambda} \left(\tilde {y} _ {T} ^ {1} - \tilde {y} _ {T} ^ {2}\right) \tag {4.20}
$$

and for an estimate of $\beta _ { 0 }$ at time $T$ , we have from Eq. (4.17)

$$
\begin{array}{l} \tilde {y} _ {T} ^ {(1)} = \left(\hat {\beta} _ {0, T} + \hat {\beta} _ {1, T} T\right) - \frac {1 - \lambda}{\lambda} \hat {\beta} _ {1, T} \tag {4.21} \\ \Rightarrow \hat {\beta} _ {0, T} = \tilde {y} _ {T} ^ {(1)} - T \hat {\beta} _ {1, T} + \frac {1 - \lambda}{\lambda} \hat {\beta} _ {1, T}. \\ \end{array}
$$

In terms of the -rst- and second-order exponential smoothers, we have

$$
\begin{array}{l} \hat {\beta} _ {0, T} = \tilde {y} _ {T} ^ {(1)} - T \frac {\lambda}{1 - \lambda} \left(\tilde {y} _ {T} ^ {(1)} - \tilde {y} _ {T} ^ {(2)}\right) + \frac {1 - \lambda}{\lambda} \left(\frac {\lambda}{1 - \lambda} \left(\tilde {y} _ {T} ^ {(1)} - \tilde {y} _ {T} ^ {(2)}\right)\right) \\ = \tilde {y} _ {T} ^ {(1)} - T \frac {\lambda}{1 - \lambda} \left(\tilde {y} _ {T} ^ {(1)} - \tilde {y} _ {T} ^ {(2)}\right) + \left(\tilde {y} _ {T} ^ {(1)} - \tilde {y} _ {T} ^ {(2)}\right) \tag {4.22} \\ = \left(2 - T \frac {\lambda}{1 - \lambda}\right) \tilde {y} _ {T} ^ {(1)} - \left(1 - T \frac {\lambda}{1 - \lambda}\right) \tilde {y} _ {T} ^ {(2)}. \\ \end{array}
$$

Finally, combining Eq. (4.20) and (4.22), we have a predictor for $y _ { T }$ as

$$
\begin{array}{l} \tilde {y} _ {T} = \hat {\beta} _ {0, T} + \hat {\beta} _ {1, T} T \tag {4.23} \\ = 2 \tilde {y} _ {T} ^ {(1)} - \tilde {y} _ {T} ^ {(2)}. \\ \end{array}
$$

It can easily be shown that $\hat { y } _ { T }$ is an unbiased predictor of $y _ { T }$ . In Figure 4.12, we use Eq. (4.23) to estimate the Dow Jones Index from February 2003 to February 2004. From Figures 4.10 and 4.12, we can clearly see that the second-order exponential smoother is doing a much better job in modeling the linear trend compared to the simple exponential smoother.

As in the simple exponential smoothing, we have the same two issues to deal with: initial values for the smoothers and the discount factors. The

![](images/3d34518b0ca75f41d5d361be3c8350edc8b9bd789f29fc42a66a537d6b143976.jpg)  
FIGURE 4.12 The Dow Jones Index from February 2003 to February 2004 with second-order exponential smoother with discount factor of 0.3.

latter will be discussed in Section 4.6.1. For the former we will combine Eqs. (4.17) and (4.19) as the following:

$$
\tilde {y} _ {0} ^ {(1)} = \hat {\beta} _ {0, 0} - \frac {1 - \lambda}{\lambda} \hat {\beta} _ {1, 0} \tag {4.24}
$$

$$
\tilde {y} _ {0} ^ {(2)} = \hat {\beta} _ {0, 0} - 2 \left(\frac {1 - \lambda}{\lambda}\right) \hat {\beta} _ {1, 0}.
$$

The initial estimates of the model parameters are usually obtained by -tting the linear trend model to the entire or a subset of the available data. The least squares estimates of the parameter estimates are then used for $\hat { \beta } _ { 0 , 0 }$ and $\hat { \beta } _ { 1 , 0 }$ .

Example 4.2 Consider the US Consumer Price Index (CPI) from January 1995 to December 2004 in Table 4.2. Figure 4.13 clearly shows that the data exhibits a linear trend. To smooth the data, following the recommendation in Section 4.2, we can use single exponential smoothing with $\lambda = 0 . 3$ as given in Figure 4.14.

As we expected, the exponential smoother does a very good job in capturing the general trend in the data and provides a less jittery (smooth) version of it. However, we also notice that the smoothed values are

TABLE 4.2 Consumer Price Index from January 1995 to December 2004   

<table><tr><td>Month-Year</td><td>CPI</td><td>Month-Year</td><td>CPI</td><td>Month-Year</td><td>CPI</td><td>Month-Year</td><td>CPI</td><td>Month-Year</td><td>CPI</td></tr><tr><td>Jan-1995</td><td>150.3</td><td>Jan-1997</td><td>159.1</td><td>Jan-1999</td><td>164.3</td><td>Jan-2001</td><td>175.1</td><td>Jan-2003</td><td>181.7</td></tr><tr><td>Feb-1995</td><td>150.9</td><td>Feb-1997</td><td>159.6</td><td>Feb-1999</td><td>164.5</td><td>Feb-2001</td><td>175.8</td><td>Feb-2003</td><td>183.1</td></tr><tr><td>Mar-1995</td><td>151.4</td><td>Mar-1997</td><td>160</td><td>Mar-1999</td><td>165</td><td>Mar-2001</td><td>176.2</td><td>Mar-2003</td><td>184.2</td></tr><tr><td>Apr-1995</td><td>151.9</td><td>Apr-1997</td><td>160.2</td><td>Apr-1999</td><td>166.2</td><td>Apr-2001</td><td>176.9</td><td>Apr-2003</td><td>183.8</td></tr><tr><td>May-1995</td><td>152.2</td><td>May-1997</td><td>160.1</td><td>May-1999</td><td>166.2</td><td>May-2001</td><td>177.7</td><td>May-2003</td><td>183.5</td></tr><tr><td>Jun-1995</td><td>152.5</td><td>Jun-1997</td><td>160.3</td><td>Jun-1999</td><td>166.2</td><td>Jun-2001</td><td>178</td><td>Jun-2003</td><td>183.7</td></tr><tr><td>Jul-1995</td><td>152.5</td><td>Jul-1997</td><td>160.5</td><td>Jul-1999</td><td>166.7</td><td>Jul-2001</td><td>177.5</td><td>Jul-2003</td><td>183.9</td></tr><tr><td>Aug-1995</td><td>152.9</td><td>Aug-1997</td><td>160.8</td><td>Aug-1999</td><td>167.1</td><td>Aug-2001</td><td>177.5</td><td>Aug-2003</td><td>184.6</td></tr><tr><td>Sep-1995</td><td>153.2</td><td>Sep-1997</td><td>161.2</td><td>Sep-1999</td><td>167.9</td><td>Sep-2001</td><td>178.3</td><td>Sep-2003</td><td>185.2</td></tr><tr><td>Oct-1995</td><td>153.7</td><td>Oct-1997</td><td>161.6</td><td>Oct-1999</td><td>168.2</td><td>Oct-2001</td><td>177.7</td><td>Oct-2003</td><td>185</td></tr><tr><td>Nov-1995</td><td>153.6</td><td>Nov-1997</td><td>161.5</td><td>Nov-1999</td><td>168.3</td><td>Nov-2001</td><td>177.4</td><td>Nov-2003</td><td>184.5</td></tr><tr><td>Dec-1995</td><td>153.5</td><td>Dec-1997</td><td>161.3</td><td>Dec-1999</td><td>168.3</td><td>Dec-2001</td><td>176.7</td><td>Dec-2003</td><td>184.3</td></tr><tr><td>Jan-1996</td><td>154.4</td><td>Jan-1998</td><td>161.6</td><td>Jan-2000</td><td>168.8</td><td>Jan-2002</td><td>177.1</td><td>Jan-2004</td><td>185.2</td></tr><tr><td>Feb-1996</td><td>154.9</td><td>Feb-1998</td><td>161.9</td><td>Feb-2000</td><td>169.8</td><td>Feb-2002</td><td>177.8</td><td>Feb-2004</td><td>186.2</td></tr><tr><td>Mar-1996</td><td>155.7</td><td>Mar-1998</td><td>162.2</td><td>Mar-2000</td><td>171.2</td><td>Mar-2002</td><td>178.8</td><td>Mar-2004</td><td>187.4</td></tr><tr><td>Apr-1996</td><td>156.3</td><td>Apr-1998</td><td>162.5</td><td>Apr-2000</td><td>171.3</td><td>Apr-2002</td><td>179.8</td><td>Apr-2004</td><td>188</td></tr><tr><td>May-1996</td><td>156.6</td><td>May-1998</td><td>162.8</td><td>May-2000</td><td>171.5</td><td>May-2002</td><td>179.8</td><td>May-2004</td><td>189.1</td></tr><tr><td>Jun-1996</td><td>156.7</td><td>Jun-1998</td><td>163</td><td>Jun-2000</td><td>172.4</td><td>Jun-2002</td><td>179.9</td><td>Jun-2004</td><td>189.7</td></tr><tr><td>Jul-1996</td><td>157</td><td>Jul-1998</td><td>163.2</td><td>Jul-2000</td><td>172.8</td><td>Jul-2002</td><td>180.1</td><td>Jul-2004</td><td>189.4</td></tr><tr><td>Aug-1996</td><td>157.3</td><td>Aug-1998</td><td>163.4</td><td>Aug-2000</td><td>172.8</td><td>Aug-2002</td><td>180.7</td><td>Aug-2004</td><td>189.5</td></tr><tr><td>Sep-1996</td><td>157.8</td><td>Sep-1998</td><td>163.6</td><td>Sep-2000</td><td>173.7</td><td>Sep-2002</td><td>181</td><td>Sep-2004</td><td>189.9</td></tr><tr><td>Oct-1996</td><td>158.3</td><td>Oct-1998</td><td>164</td><td>Oct-2000</td><td>174</td><td>Oct-2002</td><td>181.3</td><td>Oct-2004</td><td>190.9</td></tr><tr><td>Nov-1996</td><td>158.6</td><td>Nov-1998</td><td>164</td><td>Nov-2000</td><td>174.1</td><td>Nov-2002</td><td>181.3</td><td>Nov-2004</td><td>191</td></tr><tr><td>Dec-1996</td><td>158.6</td><td>Dec-1998</td><td>163.9</td><td>Dec-2000</td><td>174</td><td>Dec-2002</td><td>180.9</td><td>Dec-2004</td><td>190.3</td></tr></table>

![](images/1eb8545ba1f86dfc3630ba24e46e92f14e981aa65c043f1e3c117c9f24e684f4.jpg)  
FIGURE 4.13 US Consumer Price Index from January 1995 to December 2004.

consistently below the actual values. Hence there is an apparent bias in our smoothing. To -x this problem we have two choices: use a bigger 饾渾 or second-order exponential smoothing. The former will lead to less smooth estimates and hence defeat the purpose. For the latter, however, we can use $\lambda = 0 . 3$ to calculate and $\tilde { y } _ { T } ^ { ( 1 ) }$ and $\tilde { y } _ { T } ^ { ( 2 ) }$ as given in Table 4.3.

![](images/fb4aa21ed3c8386c41c4b4be31e05bc6e23d3e25d085a8f307c5c7f82a35f41e.jpg)  
FIGURE 4.14 Single exponential smoothing of the US Consumer Price Index (with $\tilde { y } _ { 0 } = y _ { 1 } .$ ).

TABLE 4.3 SecPrice Index (with $\lambda = 0 . 3 , \tilde { y } _ { \mathbf { 0 } } ^ { ( 1 ) } = y _ { 1 }$ entia, and 虄(2) $\tilde { y } _ { \mathbf { 0 } } ^ { ( 2 ) } = \tilde { y } _ { \mathbf { 0 } } ^ { ( 1 ) }$ = ng of the US Consumer   

<table><tr><td>Date</td><td>yt</td><td>yT(1)</td><td>yT(2)</td><td>yT=2yT(1)-yT(2)</td></tr><tr><td>Jan-1995</td><td>150.3</td><td>150.300</td><td>150.300</td><td>150.300</td></tr><tr><td>Feb-1995</td><td>150.9</td><td>150.480</td><td>150.354</td><td>150.606</td></tr><tr><td>Mar-1995</td><td>151.4</td><td>150.756</td><td>150.475</td><td>151.037</td></tr><tr><td>Apr-1995</td><td>151.9</td><td>151.099</td><td>150.662</td><td>151.536</td></tr><tr><td>May-1995</td><td>152.2</td><td>151.429</td><td>150.892</td><td>151.967</td></tr><tr><td>Nov-2004</td><td>191.0</td><td>190.041</td><td>188.976</td><td>191.106</td></tr><tr><td>Dec-2004</td><td>190.3</td><td>190.119</td><td>189.319</td><td>190.919</td></tr></table>

Note that we used $\tilde { y } _ { 0 } ^ { ( 1 ) } = y _ { 1 }$ , and $\tilde { y } _ { 0 } ^ { ( 2 ) } = \tilde { y } _ { 0 } ^ { ( 1 ) }$ as the initial values of $\tilde { y } _ { T } ^ { ( 1 ) }$ and $\tilde { y } _ { T } ^ { ( 2 ) }$ . A more rigorous approach would involve -tting a linear regression model in time to the available data that give

$$
\begin{array}{l} \hat {y} _ {t} = \hat {\beta} _ {0, T} + \hat {\beta} _ {1, T} t \\ = 1 4 9. 8 9 + 0. 3 3 t, \\ \end{array}
$$

where t goes from 1 to 120. Then from Eq. (4.24) we have

$$
\begin{array}{l} \tilde {y} _ {0} ^ {(1)} = \hat {\beta} _ {0, 0} - \frac {1 - \lambda}{\lambda} \hat {\beta} _ {1, 0} \\ = 1 4 9. 8 9 - \frac {1 - 0 . 3}{0 . 3} 0. 3 3 = 1 4 6. 2 2 \\ \end{array}
$$

$$
\begin{array}{l} \tilde {y} _ {0} ^ {(2)} = \hat {\beta} _ {0, 0} - 2 \left(\frac {1 - \lambda}{\lambda}\right) \hat {\beta} _ {1, 0} \\ = 1 4 9. 8 9 - 2 \left(\frac {1 - 0 . 3}{0 . 3}\right) 0. 3 3 = 1 4 2. 5 6. \\ \end{array}
$$

Figure 4.15 shows the second-order exponential smoothing of the CPI. As we can see, the second-order exponential smoothing not only captures the trend in the data but also does not exhibit any bias.

The calculations for the second-order smoothing for the CPI data are performed using Minitab. We -rst obtained the -rst-order exponential smoother for the CPI, $\tilde { y } _ { T } ^ { ( 1 ) }$ , using $\lambda = 0 . 3$ and $\tilde { y } _ { 0 } ^ { ( 1 ) } = y _ { 1 }$ . Then we obtained $\tilde { y } _ { T } ^ { ( 2 ) }$ $\tilde { y } _ { 0 } ^ { ( 2 ) } = \tilde { y } _ { 1 } ^ { ( 1 ) }$ by taking the -rst-order exponential smoother . Then using Eq. (4.23) we have $\hat { y } _ { T } = 2 \tilde { y } _ { T } ^ { ( 1 ) } - \tilde { y } _ { T } ^ { ( 2 ) }$ $\tilde { y } _ { T } ^ { ( 1 ) }$ y虄 using . $\lambda = 0 . 3$ and

![](images/b5f5815fc7f4ad1b6dd1c96cacd9f2a4308017350b60d92c908641c06ee429e0.jpg)  
FIGURE 4.15 Second-order exponential smoothing of the US Consumer Price Index (with $\lambda = 0 . 3 , \tilde { y } _ { 0 } ^ { ( 1 ) } = y _ { 1 }$ , and $\tilde { y } _ { 0 } ^ { ( 2 ) } = \tilde { y } _ { 1 } ^ { ( 1 ) } .$ ).

The 鈥淒ouble Exponential Smoothing鈥?option available in Minitab is a slightly different approach based on Holt鈥檚 method (Holt, 1957). This method divides the time series data into two components: the level, $L _ { t }$ , and the trend, $T _ { t }$ . These two components can be calculated from

$$
L _ {t} = \alpha y _ {t} + (1 - \alpha) (L _ {t - 1} + T _ {t - 1})
$$

$$
T _ {t} = \gamma (L _ {t} - L _ {t - 1}) + (1 - \gamma) T _ {t - 1}
$$

Hence for a given set of $\alpha$ and 饾浘, these two components are calculated and $L _ { t }$ is used to obtain the double exponential smoothing of the data at time t. Furthermore, the sum of the level and trend components at time $t$ can be used as the one-step-ahead $( t + 1 )$ forecast. Figure 4.16 shows the actual and smoothed data using the double exponential smoothing option in Minitab with $\alpha = 0 . 3$ and $\gamma = 0 . 3$ .

In general, the initial values for the level and the trend terms can be obtained by -tting a linear regression model to the CPI data with time as

![](images/19ac85c2c46740e7b774669eb000c9db6eb2b35e196c9422602037d27827e260.jpg)  
FIGURE 4.16 The double exponential smoothing of the US Consumer Price Index (with $\alpha = 0 . 3$ and $\gamma = 0 . 3 $ ).

the regressor. Then the intercept and the slope can be used as the initial values of $L _ { t }$ and $T _ { t }$ respectively.

Example 4.3 For the Dow Jones Index data, we observed that -rst-order exponential smoothing with low values of 饾渾 showed some bias when there were linear trends in the data. We may therefore decide to use the secondorder exponential smoothing approach for this data as shown in Figure 4.17. Note that the bias present with -rst-order exponential smoothing has been eliminated. The calculations for second-order exponential smoothing for the Dow Jones Index are given in Table 4.4.

# 4.5 HIGHER-ORDER EXPONENTIAL SMOOTHING

So far we have discussed the use of exponential smoothers in estimating the constant and linear trend models. For the former we employed the simple or -rst-order exponential smoother and for the latter the secondorder exponential smoother. It can further be shown that for the general nth-degree polynomial model of the form

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \frac {\beta_ {2}}{2 !} t ^ {2} + \dots + \frac {\beta_ {n}}{n !} t ^ {n} + \varepsilon_ {t}, \tag {4.25}
$$

![](images/ada7c1f756665663b320a2a107963fde43cfa47f9e73e13e06e353dba7ba5b13.jpg)  
FIGURE 4.17 The second-order exponential smoothing of the Dow Jones Index (with $\lambda = 0 . 3 , \tilde { y } _ { 0 } ^ { ( 1 ) } = y _ { 1 }$ , and $\tilde { y } _ { 0 } ^ { ( 2 ) } = \tilde { y } _ { 1 } ^ { ( \bar { 1 } ) } .$ ).

where the $\varepsilon _ { t }$ is assumed to be independent with mean 0 and constant tvariance 饾湈饾渶2, w ${ \sigma _ { \varepsilon } } ^ { 2 }$ e employ $( n + 1 )$ -order exponential smoothers

$$
\begin{array}{l} \tilde {y} _ {T} ^ {(2)} = \lambda y _ {T} + (1 - \lambda) \tilde {y} _ {T - 1} ^ {(1)} \\ \tilde {y} _ {T} ^ {(2)} = \lambda \tilde {y} _ {T} ^ {(1)} + (1 - \lambda) \tilde {y} _ {T - 1} ^ {(2)} \\ \begin{array}{c} \vdots \\ \vdots \end{array} \\ \tilde {y} _ {T} ^ {(n)} = \lambda \tilde {y} _ {T} ^ {(n - 1)} + (1 - \lambda) \tilde {y} _ {T - 1} ^ {(n)} \\ \end{array}
$$

TABLE 4.4 Second-Order Exponential Smoothing of the Dow Jones Index (with 饾潃 = 0.3, y虄(1 $\lambda = 0 . 3 , \tilde { y } _ { 0 } ^ { ( 1 ) } = y _ { 1 }$ ) = y1, and y虄(2 $\tilde { y } _ { 0 } ^ { ( 2 ) } = \tilde { y } _ { 1 } ^ { ( 1 ) } ,$ ) = y虄 (1) )   

<table><tr><td>Date</td><td>\( \tilde{y}_t \)</td><td>\( \tilde{y}_T^1 \)</td><td>\( \tilde{y}_T^2 \)</td><td>\( \hat{y}_T = 2\tilde{y}_T^{(1)} - \tilde{y}_T^{(2)} \)</td></tr><tr><td>Jun-1999</td><td>10,970.8</td><td>10,970.8</td><td>10,970.8</td><td>10,970.8</td></tr><tr><td>Jul-1999</td><td>10,655.2</td><td>10,876.1</td><td>10,942.4</td><td>10,809.8</td></tr><tr><td>Aug-1999</td><td>10,829.3</td><td>10,862.1</td><td>10,918.3</td><td>10,805.8</td></tr><tr><td>Sep-1999</td><td>10,337.0</td><td>10,704.6</td><td>10,854.2</td><td>10,554.9</td></tr><tr><td>Oct-1999</td><td>10,729.9</td><td>10,712.2</td><td>10,811.6</td><td>10,612.7</td></tr><tr><td>May-2006</td><td>11,168.3</td><td>11,069.4</td><td>10,886.5</td><td>11,252.3</td></tr><tr><td>Jun-2006</td><td>11,247.9</td><td>11,123.0</td><td>10,957.4</td><td>11,288.5</td></tr></table>

to estimate the model parameters. For even the quadratic model (seconddegree polynomial), the calculations get quite complicated. Refer to Montgomery et al. (1990), Brown (1963), and Abraham and Ledolter (1983) for the solutions to higher-order exponential smoothing problems. If a highorder polynomial does seem to be required for the time series, the autoregressive integrated moving average (ARIMA) models and techniques discussed in Chapter 5 can instead be considered.

# 4.6 FORECASTING

We have so far considered exponential smoothing techniques as either visual aids to point out the underlying patterns in the time series data or to estimate the model parameters for the class of models given in Eq. (4.9). The latter brings up yet another use of exponential smoothing鈥攆orecasting future observations. At time $T _ { \mathrm { { : } } }$ , we may wish to forecast the observation in the next time unit, $T + 1$ , or further into the future. For that, we will denote the $\tau$ -step-ahead forecast made at time T as $\hat { y } _ { T + \tau }$ (T). In the next two sections and without any loss of generality, we will once again consider -rst- and second-order exponential smoothers as examples for forecasting time series data from the constant and linear trend processes.

# 4.6.1 Constant Process

In Section 4.2 we discussed -rst-order exponential smoothing for the constant process in Eq. (4.1) as

$$
\tilde {y} _ {T} = \lambda y _ {T} + (1 - \lambda) \tilde {y} _ {T - 1}.
$$

In Section 4.3 we further showed that the constant level in Eq. (4.1), $\beta _ { 0 }$ , can be estimated by $\tilde { y } _ { T }$ . Since the constant model consists of two parts鈥?$\cdot \beta _ { 0 }$ that can be estimated by the -rst-order exponential smoother and the random error that cannot be predicted鈥攐ur forecast for the future observation is simply equal to the current value of the exponential smoother

$$
\hat {y} _ {T + \tau} (T) = \tilde {y} _ {T} = \tilde {y} _ {T}. \tag {4.26}
$$

Please note that, for the constant process, the forecast in Eq. (4.26) is the same for all future values. Since there may be changes in the level of the constant process, forecasting all future observations with the same value

will most likely be misleading. However, as we start accumulating more observations, we can update our forecast. For example, if the data at $T + 1$ become available, our forecast for the future observations becomes

$$
\tilde {y} _ {T + 1} = \lambda y _ {T + 1} + (1 - \lambda) \tilde {y} _ {T}
$$

or

$$
\hat {y} _ {T + 1 + \tau} (T + 1) = \lambda y _ {T + 1} + (1 - \lambda) \hat {y} _ {T + \tau} (T) \tag {4.27}
$$

We can rewrite Eq. (4.27) for $\tau = 1$ as

$$
\begin{array}{l} \hat {y} _ {T + 2} (T + 1) = \hat {y} _ {T + 1} (T) + \lambda \left(y _ {T + 1} - \hat {y} _ {T + 1} (T)\right) \\ = \hat {y} _ {T + 1} (T) + \lambda e _ {T + 1} (1), \\ \end{array}
$$

where $e _ { T + 1 } \left( 1 \right) = y _ { T + 1 } - \hat { y } _ { T + 1 } \left( T \right)$ is called the one-step-ahead forecast or prediction error. The interpretation of Eq. (4.28) makes it easier to understand the forecasting process using exponential smoothing: our forecast for the next observation is simply our previous forecast for the current observation plus a fraction of the forecast error we made in forecasting the current observation. The fraction in this summation is determined by 饾渾. Hence how fast our forecast will react to the forecast error depends on the discount factor. A large discount factor will lead to fast reaction to the forecast error but it may also make our forecast react fast to random uctuations. This once again brings up the issue of the choice of the discount factor.

Choice of 饾潃 We will de-ne the sum of the squared one-step-ahead forecast errors as

$$
S S _ {E} (\lambda) = \sum_ {t = 1} ^ {T} e _ {t} ^ {2} (1). \tag {4.29}
$$

For a given historic data, we can in general calculate $S S _ { E }$ values for various values of $\lambda$ and pick the value of $\lambda$ that gives the smallest sum of the squared forecast errors.

Prediction Intervals Another issue in forecasting is the uncertainty associated with it. That is, we may be interested not only in the 鈥減oint estimates鈥?but also in the quanti-cation of the prediction uncertainty. This

is usually achieved by providing the prediction intervals that are expected at a speci-c con-dence level to contain the future observations. Calculations of the prediction intervals will require the estimation of the variance of the forecast errors. We will discuss two different techniques in estimating prediction error variance in Section 4.6.3. For the constant process, the 100 $( 1 - \alpha / 2 )$ percent prediction intervals for any lead time $\tau$ are given as

$$
\tilde {y} _ {T} \pm Z _ {\alpha / 2} \hat {\sigma} _ {e},
$$

where $\tilde { y } _ { T }$ is the -rst-order exponential smoother, $Z _ { \alpha / 2 }$ is the $1 0 0 ( 1 - \alpha / 2 )$ percentile of the standard normal distribution, and $\hat { \sigma } _ { e }$ is the estimate of the standard deviation of the forecast errors.

It should be noted that the prediction interval is constant for all lead times. This of course can be (and probably is in most cases) quite unrealistic. As it will be more likely that the process goes through some changes as time goes on, we would correspondingly expect to be less and less 鈥渟ure鈥?about our predictions for large lead times (or large $\tau$ values). Hence we would anticipate prediction intervals that are getting wider and wider for increasing lead times. We propose a remedy for this in Section 4.6.3. We will discuss this issue further in Chapter 6.

Example 4.4 We are interested in the average speed on a speci-c stretch of a highway during nonrush hours. For the past year and a half (78 weeks), we have available weekly averages of the average speed in miles/hour between 10 AM and 3 PM. The data are given in Table 4.5. Figure 4.18 shows that the time series data follow a constant process. To smooth out the excessive variation, however, -rst-order exponential smoothing can be used. The 鈥渂est鈥?smoothing constant can be determined by -nding the smoothing constant value that minimizes the sum of the squared one-stepahead prediction errors.

The sum of the squared one-step-ahead prediction errors for various 饾渾 values is given in Table 4.6. Furthermore, Figure 4.19 shows that the minimum $S S _ { E }$ is obtained for $\lambda = 0 . 4$ .

Let us assume that we are also asked to make forecasts for the next 12 weeks at week 78. Figure 4.20 shows the smoothed values for the -rst 78 weeks together with the forecasts for weeks 79鈥?0 with prediction intervals. It also shows the actual weekly speed during that period. Note that since the constant process is assumed, the forecasts for the next 12 weeks are the same. Similarly, the prediction intervals are constant for that period.

TABLE 4.5 The Weekly Average Speed During Nonrush Hours   

<table><tr><td>Week</td><td>Speed</td><td>Week</td><td>Speed</td><td>Week</td><td>Speed</td><td>Week</td><td>Speed</td></tr><tr><td>1</td><td>47.12</td><td>26</td><td>46.74</td><td>51</td><td>45.71</td><td>76</td><td>45.69</td></tr><tr><td>2</td><td>45.01</td><td>27</td><td>46.62</td><td>52</td><td>43.84</td><td>77</td><td>44.59</td></tr><tr><td>3</td><td>44.69</td><td>28</td><td>45.31</td><td>53</td><td>45.09</td><td>78</td><td>43.45</td></tr><tr><td>4</td><td>45.41</td><td>29</td><td>44.69</td><td>54</td><td>44.16</td><td>79</td><td>44.75</td></tr><tr><td>5</td><td>45.45</td><td>30</td><td>46.39</td><td>55</td><td>46.21</td><td>80</td><td>45.46</td></tr><tr><td>6</td><td>44.77</td><td>31</td><td>43.79</td><td>56</td><td>45.11</td><td>81</td><td>43.73</td></tr><tr><td>7</td><td>45.24</td><td>32</td><td>44.28</td><td>57</td><td>46.16</td><td>82</td><td>44.15</td></tr><tr><td>8</td><td>45.27</td><td>33</td><td>46.04</td><td>58</td><td>46.50</td><td>83</td><td>44.05</td></tr><tr><td>9</td><td>46.93</td><td>34</td><td>46.45</td><td>59</td><td>44.88</td><td>84</td><td>44.83</td></tr><tr><td>10</td><td>47.97</td><td>35</td><td>46.31</td><td>60</td><td>45.68</td><td>85</td><td>43.93</td></tr><tr><td>11</td><td>45.27</td><td>36</td><td>45.65</td><td>61</td><td>44.40</td><td>86</td><td>44.40</td></tr><tr><td>12</td><td>45.10</td><td>37</td><td>46.28</td><td>62</td><td>44.17</td><td>87</td><td>45.25</td></tr><tr><td>13</td><td>43.31</td><td>38</td><td>44.11</td><td>63</td><td>45.18</td><td>88</td><td>44.80</td></tr><tr><td>14</td><td>44.97</td><td>39</td><td>46.00</td><td>64</td><td>43.73</td><td>89</td><td>44.75</td></tr><tr><td>15</td><td>45.31</td><td>40</td><td>46.70</td><td>65</td><td>45.14</td><td>90</td><td>44.50</td></tr><tr><td>16</td><td>45.23</td><td>41</td><td>47.84</td><td>66</td><td>47.98</td><td>91</td><td>45.12</td></tr><tr><td>17</td><td>42.92</td><td>42</td><td>48.24</td><td>67</td><td>46.52</td><td>92</td><td>45.28</td></tr><tr><td>18</td><td>44.99</td><td>43</td><td>45.59</td><td>68</td><td>46.89</td><td>93</td><td>45.15</td></tr><tr><td>19</td><td>45.12</td><td>44</td><td>46.56</td><td>69</td><td>46.01</td><td>94</td><td>46.24</td></tr><tr><td>20</td><td>46.67</td><td>45</td><td>45.02</td><td>70</td><td>44.98</td><td>95</td><td>46.15</td></tr><tr><td>21</td><td>44.62</td><td>46</td><td>43.67</td><td>71</td><td>45.76</td><td>96</td><td>46.57</td></tr><tr><td>22</td><td>45.11</td><td>47</td><td>44.53</td><td>72</td><td>45.38</td><td>97</td><td>45.51</td></tr><tr><td>23</td><td>45.18</td><td>48</td><td>44.37</td><td>73</td><td>45.33</td><td>98</td><td>46.98</td></tr><tr><td>24</td><td>45.91</td><td>49</td><td>44.62</td><td>74</td><td>44.07</td><td>99</td><td>46.64</td></tr><tr><td>25</td><td>48.39</td><td>50</td><td>46.71</td><td>75</td><td>44.02</td><td>100</td><td>44.31</td></tr></table>

![](images/603d72d07b8581d4e0d3ca435b55619a3ccf4d9df1b614424bd8196479f10fcb.jpg)  
FIGURE 4.18 The weekly average speed during nonrush hours.

TABLE 4.6 $S S _ { E }$ for Different 饾潃 Values for the Average Speed Data   

<table><tr><td colspan="2">位</td><td colspan="2">0.1</td><td colspan="2">0.2</td><td colspan="2">0.3</td><td colspan="2">0.4</td><td colspan="2">0.5</td><td colspan="2">0.9</td></tr><tr><td>Week</td><td>Speed</td><td>Forecast</td><td>e(t)</td><td>Forecast</td><td>e(t)</td><td>Forecast</td><td>e(t)</td><td>Forecast</td><td>e(t)</td><td>Forecast</td><td>e(t)</td><td>Forecast</td><td>e(t)</td></tr><tr><td>1</td><td>47.12</td><td>47.12</td><td>0.00</td><td>47.12</td><td>0.00</td><td>47.12</td><td>0.00</td><td>47.12</td><td>0.00</td><td>47.12</td><td>0.00</td><td>47.12</td><td>0.00</td></tr><tr><td>2</td><td>45.01</td><td>47.12</td><td>-2.11</td><td>47.12</td><td>-2.11</td><td>47.12</td><td>-2.11</td><td>47.12</td><td>-2.11</td><td>47.12</td><td>-2.U</td><td>47.12</td><td>-2.11</td></tr><tr><td>3</td><td>44.69</td><td>46.91</td><td>-2.22</td><td>46.70</td><td>-2.01</td><td>46.49</td><td>-1.80</td><td>46.28</td><td>-1.59</td><td>46.07</td><td>-1.38</td><td>45.22</td><td>-0.53</td></tr><tr><td>4</td><td>45.41</td><td>46.69</td><td>-1.28</td><td>46.30</td><td>-0.89</td><td>45.95</td><td>-0.54</td><td>45.64</td><td>-0.23</td><td>45.38</td><td>0.03</td><td>44.74</td><td>0.67</td></tr><tr><td>5</td><td>45.45</td><td>46.56</td><td>-1.11</td><td>46.12</td><td>-0.67</td><td>45.79</td><td>-0.34</td><td>45.55</td><td>-0.10</td><td>45.39</td><td>0.06</td><td>45.34</td><td>0.11</td></tr><tr><td>6</td><td>44.77</td><td>46.45</td><td>-1.68</td><td>45.99</td><td>-1.22</td><td>45.69</td><td>-0.92</td><td>45.51</td><td>-0.74</td><td>45.42</td><td>-0.65</td><td>45.44</td><td>-0.67</td></tr><tr><td>7</td><td>45.24</td><td>46.28</td><td>-1.04</td><td>45.74</td><td>-0.50</td><td>45.41</td><td>-0.17</td><td>45.21</td><td>0.03</td><td>45.10</td><td>0.14</td><td>44.84</td><td>0.40</td></tr><tr><td>8</td><td>45.27</td><td>46.18</td><td>-0.91</td><td>45.64</td><td>-0.37</td><td>45.36</td><td>-0.09</td><td>45.22</td><td>0.05</td><td>45.17</td><td>0.10</td><td>45.20</td><td>0.07</td></tr><tr><td>9</td><td>46.93</td><td>46.09</td><td>0.84</td><td>45.57</td><td>1.36</td><td>45.33</td><td>1.60</td><td>45.24</td><td>1.69</td><td>45.22</td><td>1.71</td><td>45.26</td><td>1.67</td></tr><tr><td>10</td><td>47.97</td><td>46.17</td><td>1.80</td><td>45.84</td><td>2.13</td><td>45.81</td><td>2.16</td><td>45.92</td><td>2.05</td><td>46.07</td><td>1.90</td><td>46.76</td><td>1.21</td></tr><tr><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td></tr><tr><td>75</td><td>44.02</td><td>45.42</td><td>-1.40</td><td>45.30</td><td>-1.28</td><td>45.12</td><td>-1.10</td><td>44.93</td><td>-0.91</td><td>44.75</td><td>-0.73</td><td>44.20</td><td>-0.18</td></tr><tr><td>76</td><td>45.69</td><td>45.28</td><td>0.41</td><td>45.05</td><td>0.64</td><td>44.79</td><td>0.90</td><td>44.56</td><td>1.13</td><td>44.39</td><td>1.30</td><td>44.04</td><td>1.65</td></tr><tr><td>77</td><td>44.59</td><td>45.32</td><td>-0.73</td><td>45.18</td><td>-0.59</td><td>45.06</td><td>-0.47</td><td>45.01</td><td>-0.42</td><td>45.04</td><td>-0.45</td><td>45.52</td><td>-0.93</td></tr><tr><td>78</td><td>43.45</td><td>45.25</td><td>-1.80</td><td>45.06</td><td>-1.61</td><td>44.92</td><td>-1.47</td><td>44.84</td><td>-1.39</td><td>44.81</td><td>-1.36</td><td>44.68</td><td>-1.23</td></tr><tr><td>SS_E</td><td></td><td></td><td>124.14</td><td></td><td>118.88</td><td></td><td>117.27</td><td></td><td>116.69</td><td></td><td>116.95</td><td></td><td>128.98</td></tr></table>

# SSE vs. lambda

SSE $\operatorname* { m i n } = 1 1 6 . 6 9$

lambda $= 0 . 4$

![](images/68425b5c4f869143a2d14a539e469d912ae3ccf0d368a5f071233c2133a06d7d.jpg)  
FIGURE 4.19 Plot of $S S _ { E }$ for various $\lambda$ values for average speed data.

# 4.6.2 Linear Trend Process

The $t \cdot$ -step-ahead forecast for the linear trend model is given by

$$
\begin{array}{l} \hat {y} _ {T + \tau} (T) = \hat {\beta} _ {0, T} + \hat {\beta} _ {1, T} (T + \tau) \\ = \hat {\beta} _ {0, T} + \hat {\beta} _ {1, T} T + \hat {\beta} _ {1, T} \tau \tag {4.30} \\ = \hat {y} _ {T} + \hat {\beta} _ {1, T} \tau . \\ \end{array}
$$

![](images/7bb9a1209d42378f4f9d46a8f096f5a0a677d680a79908f00854e522e3715985.jpg)  
FIGURE 4.20 Forecasts for the weekly average speed data for weeks 79鈥?0.

In terms of the exponential smoothers, we can rewrite Eq. (4.30) as

$$
\begin{array}{l} \hat {y} _ {T + \tau} (\tau) = \left(2 \tilde {y} _ {T} ^ {(1)} - \tilde {y} _ {T} ^ {(2)}\right) + \tau \frac {\lambda}{1 - \lambda} \left(\tilde {y} _ {T} ^ {(1)} - \tilde {y} _ {T} ^ {(2)}\right) \tag {4.31} \\ = \left(2 + \frac {\lambda}{1 - \lambda} \tau\right) \tilde {y} _ {T} ^ {(1)} - \left(1 + \frac {\lambda}{1 - \lambda} \tau\right) \tilde {y} _ {T} ^ {(2)}. \\ \end{array}
$$

It should be noted that the predictions for the trend model depend on the lead time and, as opposed to the constant model, will be different for different lead times. As we collect more data, we can improve our forecasts by updating our parameter estimates using

$$
\begin{array}{l} \hat {\beta} _ {0, T + 1} = \lambda (1 + \lambda) y _ {T + 1} + (1 - \lambda) ^ {2} (\hat {\beta} _ {0, T} + \hat {\beta} _ {1, T}) \\ \hat {\beta} _ {1, T + 1} = \frac {\lambda}{(2 - \lambda)} \left(\hat {\beta} _ {0, T + 1} - \hat {\beta} _ {0, T}\right) + \frac {2 (1 - \lambda)}{(2 - \lambda)} \hat {\beta} _ {1, T} \tag {4.32} \\ \end{array}
$$

Subsequently, we can update our $\tau$ -step-ahead forecasts based on Eq. (4.32). As in the constant process, the discount factor, $\lambda$ , can be estimated by minimizing the sum of the squared one-step-ahead forecast errors given in Eq. (4.29).

In this case, the $1 0 0 ( 1 - \alpha / 2 )$ percent prediction interval for any lead time $\tau$ is

$$
\left(2 + \frac {\lambda}{1 - \lambda} \tau\right) \hat {y} _ {T} ^ {(1)} - \left(1 + \frac {\lambda}{1 - \lambda} \tau\right) \hat {y} _ {T} ^ {(2)} \pm Z _ {\alpha / 2} \frac {c _ {\tau}}{c _ {1}} \hat {\sigma} _ {e},
$$

where

$$
c _ {i} ^ {2} = 1 + \frac {\lambda}{(2 - \lambda) ^ {3}} [ (1 0 - 1 4 \lambda + 5 \lambda^ {2}) + 2 i \lambda (4 - 3 \lambda) + 2 i ^ {2} \lambda^ {2} ].
$$

Example 4.5 Consider the CPI data in Example 4.2. Assume that we are currently in December 2003 and would like to make predictions of the CPI for the following year. Although the data from January 1995 to December 2003 clearly exhibit a linear trend, we may still like to consider -rst-order exponential smoothing -rst. We will then calculate the 鈥渂est鈥?饾渾 value that minimizes the sum of the squared one-step-ahead prediction errors. The predictions and prediction errors for various $\lambda$ values are given in Table 4.7.

Figure 4.21 shows the sum of the squared one-step-ahead prediction errors $( S S _ { E } )$ for various values of 饾渾.

TABLE 4.7 The Predictions and Prediction Errors for Various 饾潃 Values for CPI Data   

<table><tr><td rowspan="2">Month-Year</td><td rowspan="2">CPI</td><td colspan="2">位 = 0.1</td><td colspan="2">位 = 0.2</td><td colspan="2">位 = 0.3</td><td colspan="2">位 = 0.9</td><td colspan="2">位 = 0.99</td></tr><tr><td>Prediction</td><td>Error</td><td>Prediction</td><td>Error</td><td>Prediction</td><td>Error</td><td>Prediction</td><td>Error</td><td>Prediction</td><td>Error</td></tr><tr><td>Jan-1995</td><td>150.3</td><td>150.30</td><td>0.00</td><td>150.30</td><td>0.00</td><td>150.30</td><td>0.00</td><td>...</td><td>150.30</td><td>0.00</td><td>150.30</td></tr><tr><td>Feb-1995</td><td>150.9</td><td>150.30</td><td>0.60</td><td>150.30</td><td>0.60</td><td>150.30</td><td>0.60</td><td>...</td><td>150.30</td><td>0.60</td><td>150.30</td></tr><tr><td>Mar-1995</td><td>151.4</td><td>150.36</td><td>1.04</td><td>150.42</td><td>0.98</td><td>150.48</td><td>0.92</td><td>...</td><td>150.84</td><td>0.56</td><td>150.89</td></tr><tr><td>Apr-1995</td><td>151.9</td><td>150.46</td><td>1.44</td><td>150.62</td><td>1.28</td><td>150.76</td><td>1.14</td><td>...</td><td>151.34</td><td>0.56</td><td>151.39</td></tr><tr><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td></tr><tr><td>Nov-2003</td><td>184.5</td><td>182.29</td><td>2.21</td><td>183.92</td><td>0.58</td><td>184.45</td><td>0.05</td><td>...</td><td>185.01</td><td>-0.51</td><td>185.00</td></tr><tr><td>Dec-2003</td><td>184.3</td><td>182.51</td><td>1.79</td><td>184.03</td><td>0.27</td><td>184.46</td><td>-0.16</td><td>...</td><td>184.55</td><td>-0.25</td><td>184.51</td></tr><tr><td>SS_E</td><td></td><td></td><td>1061.50</td><td></td><td>309.14</td><td></td><td>153.71</td><td></td><td>31.90</td><td></td><td>28.62</td></tr></table>

![](images/477bd89abde00fba7f0ab33f78e7e1687667c4db8aede68953bf4892197382e8.jpg)  
FIGURE 4.21 Scatter plot of the sum of the squared one-step-ahead prediction errors versus $\lambda$ .

We notice that the $S S _ { E }$ keeps on getting smaller as $\lambda$ gets bigger. This suggests that the data are highly autocorrelated. This can be clearly seen in the ACF plot in Figure 4.22. In fact if the 鈥渂est鈥?饾渾 value (i.e., $\lambda$ value that minimizes $S S _ { E }$ ) turns out to be high, it may indeed be better to switch to a higher-order smoothing or use an ARIMA model as discussed in Chapter 5.

![](images/abb82a02550cfab0f377c82338fa1341d4d863f33447a252dfc3c952b9bbb24f.jpg)  
FIGURE 4.22 ACF plot for the CPI data (with $5 \%$ signi-cance limits for the autocorrelations).

![](images/88dbe0c9fa3e30f32b7db2e5eb307093edca98d2981fa886454b9fadf14ca5ea.jpg)  
FIGURE 4.23 The 1- to 12-step-ahead forecasts of the CPI data for 2004.

Since the -rst-order exponential smoothing is deemed inadequate, we will now try the second-order exponential smoothing to forecast next year鈥檚 monthly CPI values. Usually we have two options:

1. On December 2003, make forecasts for the entire 2004 year; that is, 1-step-ahead, 2-step-ahead, 鈥?, 12-step-ahead forecasts. For that we can use Eq. (4.30) or equivalently Eq. (4.31). Using the double exponential smoothing option in Minitab with $\lambda = 0 . 3$ , we obtain the forecasts given in Figure 4.23.

Note that the forecasts further in the future (for the later part of 2004) are quite a bit off. To remedy this we may instead use the following strategy.

2. In December 2003, make the one-step-ahead forecast for January 2004. When the data for January 2004 becomes available, then make the one-step-ahead forecast for February 2004, and so on. We can see from Figure 4.24 that forecasts when only one-step-ahead forecasts are used and adjusted as actual data becomes available perform better than in the previous case where, for December 2003, forecasts are made for the entire following year.

The JMP software package also has an excellent forecasting capability. Table 4.8 shows output from JMP for the CPI data for double

![](images/02b4b16d71d2c18a3067e6cbaf2518e35cedf765166ee5286a35e88058059c0c.jpg)  
FIGURE 4.24 The one-step-ahead forecasts of the CPI data for 2004.

exponential smoothing. JMP uses the double smoothing procedure that employs a single smoothing constant. The JMP output shows the time series plot and summary statistics including the sample ACF. It also provides a sample partial ACF, which we will discuss in Chapter 5. Then an optimal smoothing constant is chosen by -nding the value of 饾渾 that

TABLE 4.8 JMP Output for the CPI Data   

<table><tr><td colspan="9">Time series CPI</td></tr><tr><td>190</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>180</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>170</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>160</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>150</td><td>0</td><td>20</td><td>40</td><td>60</td><td>80</td><td>100</td><td>120</td><td>140</td></tr><tr><td colspan="9">Row</td></tr><tr><td>Mean</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Std</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>N</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Zero Mean ADF</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Single Mean ADF</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Trend ADF</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>

(continued )

TABLE 4.8 (Continued)   

<table><tr><td colspan="5">Time series basic diagnostics</td></tr><tr><td>Lag</td><td>AutoCorr</td><td>Plot autocorr</td><td>Ljung-box Q</td><td>p-Value</td></tr><tr><td>0</td><td>1.0000</td><td></td><td></td><td></td></tr><tr><td>1</td><td>0.9743</td><td></td><td>116.774</td><td>&lt;.0001</td></tr><tr><td>2</td><td>0.9472</td><td></td><td>228.081</td><td>&lt;.0001</td></tr><tr><td>3</td><td>0.9203</td><td></td><td>334.053</td><td>&lt;.0001</td></tr><tr><td>4</td><td>0.8947</td><td></td><td>435.091</td><td>&lt;.0001</td></tr><tr><td>5</td><td>0.8694</td><td></td><td>531.310</td><td>&lt;.0001</td></tr><tr><td>6</td><td>0.8436</td><td></td><td>622.708</td><td>&lt;.0001</td></tr><tr><td>7</td><td>0.8166</td><td></td><td>709.101</td><td>&lt;.0001</td></tr><tr><td>8</td><td>0.7899</td><td></td><td>790.659</td><td>&lt;.0001</td></tr><tr><td>9</td><td>0.7644</td><td></td><td>867.721</td><td>&lt;.0001</td></tr><tr><td>10</td><td>0.7399</td><td></td><td>940.580</td><td>&lt;.0001</td></tr><tr><td>11</td><td>0.7161</td><td></td><td>1009.46</td><td>&lt;.0001</td></tr><tr><td>Lag</td><td>AutoCorr</td><td>Plot autocorr</td><td>Ljung-box Q</td><td>p-Value</td></tr><tr><td>12</td><td>0.6924</td><td></td><td>1074.46</td><td>&lt;.0001</td></tr><tr><td>13</td><td>0.6699</td><td></td><td>1135.85</td><td>&lt;.0001</td></tr><tr><td>14</td><td>0.6469</td><td></td><td>1193.64</td><td>&lt;.0001</td></tr><tr><td>15</td><td>0.6235</td><td></td><td>1247.84</td><td>&lt;.0001</td></tr><tr><td>16</td><td>0.6001</td><td></td><td>1298.54</td><td>&lt;.0001</td></tr><tr><td>17</td><td>0.5774</td><td></td><td>1345.93</td><td>&lt;.0001</td></tr><tr><td>18</td><td>0.5550</td><td></td><td>1390.14</td><td>&lt;.0001</td></tr><tr><td>19</td><td>0.5324</td><td></td><td>1431.24</td><td>&lt;.0001</td></tr><tr><td>20</td><td>0.5098</td><td></td><td>1469.29</td><td>&lt;.0001</td></tr><tr><td>21</td><td>0.4870</td><td></td><td>1504.36</td><td>&lt;.0001</td></tr><tr><td>22</td><td>0.4637</td><td></td><td>1536.48</td><td>&lt;.0001</td></tr><tr><td>23</td><td>0.4416</td><td></td><td>1565.91</td><td>&lt;.0001</td></tr><tr><td>24</td><td>0.4205</td><td></td><td>1592.87</td><td>&lt;.0001</td></tr><tr><td>25</td><td>0.4000</td><td></td><td>1617.54</td><td>0.0000</td></tr><tr><td>Lag</td><td colspan="4">Partial plot partial</td></tr><tr><td>0</td><td>1.0000</td><td></td><td></td><td></td></tr><tr><td>1</td><td>0.9743</td><td></td><td></td><td></td></tr><tr><td>2</td><td>-0.0396</td><td></td><td></td><td></td></tr><tr><td>3</td><td>-0.0095</td><td></td><td></td><td></td></tr><tr><td>4</td><td>0.0128</td><td></td><td></td><td></td></tr><tr><td>5</td><td>-0.0117</td><td></td><td></td><td></td></tr><tr><td>6</td><td>-0.0212</td><td></td><td></td><td></td></tr><tr><td>7</td><td>-0.0379</td><td></td><td></td><td></td></tr><tr><td>8</td><td>-0.0070</td><td></td><td></td><td></td></tr><tr><td>9</td><td>0.0074</td><td></td><td></td><td></td></tr><tr><td>10</td><td>0.0033</td><td></td><td></td><td></td></tr><tr><td>11</td><td>-0.0001</td><td></td><td></td><td></td></tr><tr><td>12</td><td>-0.0116</td><td></td><td></td><td></td></tr><tr><td>13</td><td>0.0090</td><td></td><td></td><td></td></tr><tr><td>14</td><td>-0.0224</td><td></td><td></td><td></td></tr><tr><td>15</td><td>-0.0220</td><td></td><td></td><td></td></tr><tr><td>16</td><td>-0.0139</td><td></td><td></td><td></td></tr><tr><td>17</td><td>-0.0022</td><td></td><td></td><td></td></tr><tr><td>18</td><td>-0.0089</td><td></td><td></td><td></td></tr><tr><td>19</td><td>-0.0174</td><td></td><td></td><td></td></tr><tr><td>20</td><td>-0.0137</td><td></td><td></td><td></td></tr><tr><td>21</td><td>-0.0186</td><td></td><td></td><td></td></tr><tr><td>22</td><td>-0.0234</td><td></td><td></td><td></td></tr><tr><td>23</td><td>0.0074</td><td></td><td></td><td></td></tr><tr><td>24</td><td>0.0030</td><td></td><td></td><td></td></tr><tr><td>25</td><td>-0.0036</td><td></td><td></td><td></td></tr></table>

TABLE 4.8 (Continued)   

<table><tr><td colspan="7">Model Comparison</td></tr><tr><td colspan="3">Model</td><td>DF</td><td>Variance</td><td>AIC</td><td></td></tr><tr><td colspan="2">Double (Brown)</td><td>Exponential Smoothing</td><td>117</td><td>0.247119</td><td>171.05558</td><td></td></tr><tr><td>SBC</td><td>RSquare</td><td>-2LogLH</td><td>AIC</td><td>Rank</td><td>SBC Rank</td><td>MAPE MAE</td></tr><tr><td>173.82626</td><td>0.998</td><td>169.05558</td><td>0</td><td>0</td><td>0.216853</td><td>0.376884</td></tr><tr><td colspan="2">Model: Double (Brown)</td><td colspan="5">Exponential Smoothing</td></tr><tr><td colspan="7">Model Summary</td></tr><tr><td colspan="5">DF</td><td colspan="2">117</td></tr><tr><td colspan="5">Sum of Squared Errors</td><td colspan="2">28.9129264</td></tr><tr><td colspan="5">Variance Estimate</td><td colspan="2">0.24711903</td></tr><tr><td colspan="5">Standard Deviation</td><td colspan="2">0.49711068</td></tr><tr><td colspan="5">Akaike&#x27;s &#x27;A&#x27; Information Criterion</td><td colspan="2">171.055579</td></tr><tr><td colspan="5">Schwarz&#x27;s Bayesian Criterion</td><td colspan="2">173.826263</td></tr><tr><td colspan="5">RSquare</td><td colspan="2">0.99812888</td></tr><tr><td colspan="5">RSquare Adj</td><td colspan="2">0.99812888</td></tr><tr><td colspan="5">MAPE</td><td colspan="2">0.21685285</td></tr><tr><td colspan="5">MAE</td><td colspan="2">0.37688362</td></tr><tr><td colspan="5">-2LogLikelihood</td><td colspan="2">169.055579</td></tr><tr><td colspan="7">Stable Yes</td></tr><tr><td colspan="7">Invertible Yes</td></tr><tr><td colspan="7">Parameter Estimates</td></tr><tr><td colspan="2">Term</td><td>Estimate</td><td>Std Error</td><td>t</td><td>Ratio</td><td>Prob&gt;|t|</td></tr><tr><td colspan="2">Level Smoothing Weight</td><td>0.81402446</td><td>0.0919040</td><td></td><td>8.86</td><td>&lt;.0001</td></tr></table>

![](images/b28e17e9b40a01bdbf68579ffc87c8de001f0f5673fa1c31f8a53937a87aa8a7.jpg)  
Forecast

![](images/5247d4de3f8ffd214e1ae8dd8c67a0c172d56ec4bad310a12fb45400af199b8e.jpg)  
Residuals

TABLE 4.8 (Continued)   

<table><tr><td>Lag</td><td>AutoCorr plot autocorr</td><td>Ljung-box Q</td><td>p-Value</td></tr><tr><td>0</td><td>1.0000</td><td></td><td></td></tr><tr><td>1</td><td>0.0791</td><td>0.7574</td><td>0.3841</td></tr><tr><td>2</td><td>-0.3880</td><td>19.1302</td><td>&lt;.0001</td></tr><tr><td>3</td><td>-0.2913</td><td>29.5770</td><td>&lt;.0001</td></tr><tr><td>4</td><td>-0.0338</td><td>29.7189</td><td>&lt;.0001</td></tr><tr><td>5</td><td>0.1064</td><td>31.1383</td><td>&lt;.0001</td></tr><tr><td>6</td><td>0.1125</td><td>32.7373</td><td>&lt;.0001</td></tr><tr><td>7</td><td>0.1867</td><td>37.1819</td><td>&lt;.0001</td></tr><tr><td>8</td><td>-0.1157</td><td>38.9063</td><td>&lt;.0001</td></tr><tr><td>9</td><td>-0.3263</td><td>52.7344</td><td>&lt;.0001</td></tr><tr><td>10</td><td>-0.1033</td><td>54.1324</td><td>&lt;.0001</td></tr><tr><td>11</td><td>0.2149</td><td>60.2441</td><td>&lt;.0001</td></tr><tr><td>12</td><td>0.2647</td><td>69.6022</td><td>&lt;.0001</td></tr><tr><td>13</td><td>-0.0773</td><td>70.4086</td><td>&lt;.0001</td></tr><tr><td>14</td><td>0.0345</td><td>70.5705</td><td>&lt;.0001</td></tr><tr><td>15</td><td>-0.1243</td><td>72.6937</td><td>&lt;.0001</td></tr><tr><td>16</td><td>-0.1429</td><td>75.5304</td><td>&lt;.0001</td></tr><tr><td>17</td><td>0.0602</td><td>76.0384</td><td>&lt;.0001</td></tr><tr><td>18</td><td>0.1068</td><td>77.6533</td><td>&lt;.0001</td></tr><tr><td>19</td><td>0.0370</td><td>77.8497</td><td>&lt;.0001</td></tr><tr><td>20</td><td>-0.0917</td><td>79.0656</td><td>&lt;.0001</td></tr><tr><td>21</td><td>-0.0363</td><td>79.2579</td><td>&lt;.0001</td></tr><tr><td>22</td><td>-0.0995</td><td>80.7177</td><td>&lt;.0001</td></tr><tr><td>23</td><td>-0.0306</td><td>80.8570</td><td>&lt;.0001</td></tr><tr><td>24</td><td>0.2602</td><td>91.0544</td><td>&lt;.0001</td></tr><tr><td>25</td><td>0.1728</td><td>95.6007</td><td>&lt;.0001</td></tr><tr><td>Lag</td><td>Partial plot partial</td><td></td><td></td></tr><tr><td>0</td><td>1.0000</td><td></td><td></td></tr><tr><td>1</td><td>0.0791</td><td></td><td></td></tr><tr><td>2</td><td>-0.3967</td><td></td><td></td></tr><tr><td>3</td><td>-0.2592</td><td></td><td></td></tr><tr><td>4</td><td>-0.1970</td><td></td><td></td></tr><tr><td>5</td><td>-0.1435</td><td></td><td></td></tr><tr><td>6</td><td>-0.0775</td><td></td><td></td></tr><tr><td>7</td><td>0.1575</td><td></td><td></td></tr><tr><td>8</td><td>-0.1144</td><td></td><td></td></tr><tr><td>9</td><td>-0.2228</td><td></td><td></td></tr><tr><td>10</td><td>-0.1482</td><td></td><td></td></tr><tr><td>Lag</td><td>AutoCorr plot autocorr</td><td>Ljung-box Q</td><td>p-Value</td></tr><tr><td>11</td><td>-0.0459</td><td></td><td></td></tr><tr><td>12</td><td>0.0368</td><td></td><td></td></tr><tr><td>13</td><td>-0.1335</td><td></td><td></td></tr><tr><td>14</td><td>0.2308</td><td></td><td></td></tr><tr><td>15</td><td>-0.0786</td><td></td><td></td></tr><tr><td>16</td><td>0.0050</td><td></td><td></td></tr><tr><td>17</td><td>0.0390</td><td></td><td></td></tr><tr><td>18</td><td>-0.0903</td><td></td><td></td></tr><tr><td>19</td><td>-0.0918</td><td></td><td></td></tr><tr><td>20</td><td>0.0012</td><td></td><td></td></tr><tr><td>21</td><td>-0.0077</td><td></td><td></td></tr><tr><td>22</td><td>-0.1935</td><td></td><td></td></tr><tr><td>23</td><td>-0.0665</td><td></td><td></td></tr><tr><td>24</td><td>0.1783</td><td></td><td></td></tr><tr><td>25</td><td>0.0785</td><td></td><td></td></tr></table>

minimizes the error sum of squares. The value selected is $\lambda = 0 . 8 1 4$ . This relatively large value is not unexpected, because there is a very strong linear trend in the data and considerable autocorrelation. Values of the forecast for the next 12 periods at origin December 2004 and the associated prediction interval are also shown. Finally, the residuals from the model -t are shown along with the sample ACF and sample partial ACF plots of the residuals. The sample ACF indicates that there may be a small amount of structure in the residuals, but it is not enough to cause concern.

# 4.6.3 Estimation of $\sigma _ { e } ^ { 2 }$

In the estimation of the variance of the forecast errors, $\sigma _ { e } ^ { 2 }$ , it is often assumed that the model (e.g., constant, linear trend) is correct and constant in time. With these assumptions, we have two different ways of estimating $\sigma _ { e } ^ { 2 }$ :

1. We already de-ned the one-step-ahead forecast error as $e _ { T } ( 1 ) = y _ { T } -$ $\hat { y } _ { T } ( T - 1 )$ . The idea is to apply the model to the historic data and obtain the forecast errors to calculate:

$$
\begin{array}{l} \hat {\sigma} _ {e} ^ {2} = \frac {1}{T} \sum_ {\substack {t = 1 \\ T}} ^ {T} e _ {t} ^ {2} (1) \tag{4.33} \\ = \frac {1}{T} \sum_ {t = 1} ^ {\cdot} (y _ {t} - \hat {y} _ {t} (t - 1)) ^ {2} \\ \end{array}
$$

It should be noted that in the variance calculations the mean adjustment was not needed, since for the correct model the forecasts are unbiased; that is, the expected value of the forecast errors is 0.

As more data are collected, the variance of the forecast errors can be updated as

$$
\hat {\sigma} _ {e T + 1} ^ {2} = \frac {1}{T + 1} \left(T \hat {\sigma} _ {e, T} ^ {2} + e _ {T + 1} ^ {2} (1)\right). \tag {4.34}
$$

As discussed in Section 4.6.1, it may be counterintuitive to have a constant forecast error variance for all lead times. We can instead de-ne $\sigma _ { e } ^ { 2 } ( \tau )$ as the $\tau$ -step-ahead forecast error variance and estimate it by

$$
\hat {\sigma} _ {e} ^ {2} (\tau) = \frac {1}{T - \tau + 1} \sum_ {t = \tau} ^ {T} e _ {1} ^ {2} (\tau). \tag {4.35}
$$

Hence the estimate in Eq. (4.35) can instead be used in the calculations of the prediction interval for the $\tau$ -step-ahead forecast.

2. For the second method of estimating $\sigma _ { e } ^ { 2 }$ we will -rst de-ne the mean absolute deviation $\Delta$ as

$$
\Delta = E (| e - E (e) |) \tag {4.36}
$$

and, assuming that the model is correct, calculate its estimate by

$$
\hat {\Delta} _ {T} = \delta \left| e _ {T} (1) \right| + (1 - \delta) \hat {\Delta} _ {T - 1}. \tag {4.37}
$$

Then the estimate of the $\sigma _ { e } ^ { 2 }$ is given by

$$
\hat {\sigma} _ {e, T} = 1. 2 5 \hat {\Delta} _ {T}. \tag {4.38}
$$

For further details, see Montgomery et al. (1990).

# 4.6.4 Adaptive Updating of the Discount Factor

In the previous sections we discussed estimation of the 鈥渂est鈥?discount factor, $\bar { \lambda }$ , by minimizing the sum of the squared one-step-ahead forecasts errors. However, as we have seen with the Dow Jones Index data, changes in the underlying time series model will make it dif-cult for the exponential smoother with -xed discount factor to follow these changes. Hence a need for monitoring and, if necessary, modifying the discount factor arises. By doing so, the discount factor will adapt to the changes in the time series model. For that we will employ the procedure originally described by Trigg and Leach (1967) for single discount factor. As an example we will consider the -rst-order exponential smoother and modify it as

$$
\hat {y} _ {T} = \lambda_ {\bar {T} y T} + (1 - \lambda_ {T}) \bar {y} _ {T - 1}. \tag {4.39}
$$

Please note that in Eq. (4.39), the discount factor $\lambda _ { T }$ is given as a function of time and hence it is allowed to adapt to changes in the time series model. We also de-ne the smoothed error as

$$
Q _ {T} = \delta e _ {T} (1) + (1 - \delta) Q _ {T - 1}, \tag {4.40}
$$

where $\delta$ is a smoothing parameter.

Finally, we de-ne the tracking signal as

$$
\frac {Q _ {T}}{\hat {\Delta} _ {T}}, \tag {4.41}
$$

where $\hat { \Delta } _ { T }$ is given in Eq. (4.37). This ratio is expected to be close to 0 when the forecasting system performs well and to approach $\pm 1$ as it starts to fail. In fact, Trigg and Leach (1967) suggest setting the discount factor to

$$
\lambda_ {T} = \left| \frac {Q _ {T}}{\hat {\Delta} _ {T}} \right| \tag {4.42}
$$

Equation (4.42) will allow for automatic updating of the discount factor.

Example 4.6 Consider the Dow Jones Index from June 1999 to June 2006 given in Table 4.1. Figure 4.2 shows that the data do not exhibit a single regime of constant or linear trend behavior. Hence a single exponential smoother with adaptive discount factor as given in Eq. (4.42) can be used. Figure 4.25 shows two simple exponential smoothers for the Dow Jones Index: one with -xed $\lambda = 0 . 3$ and another one with adaptive updating based on the Trigg鈥揕each method given in Eq. (4.42).

![](images/03b1a18203283a0f665139333cb89f3910853b42eb2678246b83b828912fbc79.jpg)  
FIGURE 4.25 Time series plot of the Dow Jones Index from June 1999 to June 2006, the simple exponential smoother with $\lambda = 0 . 3$ , and the Trigg鈥揕each (TL) smoother with $\delta = 0 . 3$ .

TABLE 4.9 The Trigg鈥揕each Smoother for the Dow Jones Index   

<table><tr><td>Date</td><td>Dow Jones</td><td>Smoothed</td><td>位</td><td>Error</td><td>Qt</td><td>Dt</td></tr><tr><td>Jun-99</td><td>10,970.8</td><td>10,970.8</td><td>1</td><td></td><td>0</td><td>0</td></tr><tr><td>Jul-99</td><td>10,655.2</td><td>10,655.2</td><td>1</td><td>-315.6</td><td>-94.68</td><td>94.68</td></tr><tr><td>Aug-99</td><td>10,829.3</td><td>10,675.835</td><td>0.11853</td><td>174.1</td><td>-14.046</td><td>118.506</td></tr><tr><td>Sep-99</td><td>10,337</td><td>10,471.213</td><td>0.6039</td><td>-338.835</td><td>-111.483</td><td>184.605</td></tr><tr><td>Oct-99</td><td>10,729.9</td><td>10,471.753</td><td>0.00209</td><td>258.687</td><td>-0.43178</td><td>206.83</td></tr><tr><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td></tr><tr><td>May-06</td><td>11,168.3</td><td>11,283.962</td><td>0.36695</td><td>-182.705</td><td>68.0123</td><td>185.346</td></tr><tr><td>Jun-06</td><td>11,247.9</td><td>11,274.523</td><td>0.26174</td><td>-36.0619</td><td>36.79</td><td>140.561</td></tr></table>

This plot shows that a better smoother can be obtained by making automatic updates to the discount factor. The calculations for the Trigg鈥?Leach smoother are given in Table 4.9.

The adaptive smoothing procedure suggested by Trigg and Leach is a useful technique. For other approaches to adaptive adjustment of exponential smoothing parameters, see Chow (1965), Roberts and Reed (1969), and Montgomery (1970).

# 4.6.5 Model Assessment

If the forecast model performs as expected, the forecast errors should not exhibit any pattern or structure; that is, they should be uncorrelated. Therefore it is always a good idea to verify this. As noted in Chapter 2, we can do so by calculating the sample ACF of the forecast errors from

$$
r k = \frac {\sum_ {t = k} ^ {T - 1} \left[ e _ {t} (1) - \bar {e} \right] \left[ e _ {t - k} (1) - \bar {e} \right]}{\sum_ {T = 0} ^ {T - 1} \left[ e _ {t} (1) - \bar {e} \right] ^ {2}}, \tag {4.43}
$$

where

$$
\bar {e} = \frac {1}{n} \sum_ {t = 1} ^ {T} e _ {t} (1).
$$

If the one-step-ahead forecast errors are indeed uncorrelated, the sample autocorrelations for any lag $k$ should be around 0 with a standard error $1 / \sqrt { T }$ . Hence a sample autocorrelation for any lag $k$ that lies outside the $\pm 2 / \sqrt { T }$ limits will require further investigation of the model.

# 4.7 EXPONENTIAL SMOOTHING FOR SEASONAL DATA

Some time series data exhibit cyclical or seasonal patterns that cannot be effectively modeled using the polynomial model in Eq. (4.25). Several approaches are available for the analysis of such data. In this chapter we will discuss exponential smoothing techniques that can be used in modeling seasonal time series. The methodology we will focus on was originally introduced by Holt (1957) and Winters (1960) and is generally known as Winters鈥?method, where a seasonal adjustment is made to the linear trend model. Two types of adjustments are suggested鈥攁dditive and multiplicative.

# 4.7.1 Additive Seasonal Model

Consider the US clothing sales data given in Figure 4.26. Clearly, for certain months of every year we have high (or low) sales. Hence we can conclude that the data exhibit seasonality. The data also exhibit a linear trend as the sales tend to get higher for the same month as time goes on. As the -nal observation, we note that the amplitude of the seasonal pattern, that is, the range of the periodic behavior within a year, remains more or

![](images/9bc313c4050b27dfd92752b37d7179dc4db0ad11e044ae5fa5a121877faea3ae.jpg)  
FIGURE 4.26 Time series plot of US clothing sales from January 1992 to December 2003.

less constant in time and remains independent of the average level within a year.

We will for this case assume that the seasonal time series can be represented by the following model:

$$
y _ {t} = L _ {t} + S _ {t} + \varepsilon_ {t}, \tag {4.44}
$$

where $L _ { t }$ represents the level or linear trend component and can in turn be represented by $\beta _ { 0 } + \beta _ { 1 } t ; S _ { t }$ represents the seasonal adjustment with $S _ { t } = S _ { t + s } = S _ { t + 2 s } = . . .$ for $t = 1 , \ldots , s - 1$ , where $s$ is the length of the season (period) of the cycles; and the $\varepsilon _ { t }$ are assumed to be uncorrelated with mean 0 and constant variance $\sigma _ { \varepsilon } ^ { 2 }$ . Sometimes the level is called the permanent component. One usual restriction on this model is that the seasonal adjustments add to zero during one season,

$$
\sum_ {t = 1} ^ {s} S _ {t} = 0. \tag {4.45}
$$

In the model given in Eq. (4.44), for forecasting the future observations, we will employ -rst-order exponential smoothers with different discount factors. The procedure for updating the parameter estimates once the current observation $y _ { T }$ is obtained is as follows.

Step 1. Update the estimate of $L _ { T }$ using

$$
\hat {L} _ {T} = \lambda_ {1} \left(y _ {T} - \hat {S} _ {T - s}\right) + \left(1 - \lambda_ {1}\right) \left(\hat {L} _ {T - 1} + \hat {\beta} _ {1, T - 1}\right), \tag {4.46}
$$

where $0 < \lambda _ { 1 } < 1$ . It should be noted that in Eq. (4.46), the -rst part can be seen as the 鈥渃urrent鈥?value for $L _ { T }$ and the second part as the forecast of $L _ { T }$ based on the estimates at $T - 1$ .

Step 2. Update the estimate of $\beta _ { 1 }$ using

$$
\hat {\beta} _ {1, T} = \lambda_ {2} \left(\hat {L} _ {T} - \hat {L} _ {T - 1}\right) + (1 - \lambda_ {2}) \hat {\beta} _ {1, T - 1}, \tag {4.47}
$$

where $0 < \lambda _ { 2 } < 1$ . As in Step 1, the estimate of $\beta _ { 1 }$ in Eq. (4.47) can be seen as the linear combination of the 鈥渃urrent鈥?value of $\beta _ { 1 }$ and its 鈥渇orecast鈥?at $T - 1$ .

Step 3. Update the estimate of $S _ { t }$ using

$$
\hat {S} _ {T} = \lambda_ {3} \left(y _ {T} - \hat {L} _ {T}\right) + \left(1 - \lambda_ {3}\right) \hat {S} _ {T - s}, \tag {4.48}
$$

where $0 < \lambda _ { 3 } < 1$ .

Step 4. Finally, the $\tau$ -step-ahead forecast, $\hat { y } _ { T + \tau } ( T )$ , is

$$
\hat {y} _ {T + \tau} (T) = \hat {L} _ {T} + \hat {\beta} _ {1, T} \tau + \hat {S} _ {T} (\tau - s). \tag {4.49}
$$

As before, estimating the initial values of the exponential smoothers is important. For a given set of historic data with $n$ seasons (hence ns observations), we can use the least squares estimates of the following model:

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \sum_ {i = 1} ^ {s - 1} \gamma_ {i} \left(I _ {t, i} - I _ {t, s}\right) + \varepsilon_ {t}, \tag {4.50}
$$

where

$$
I _ {t, i} = \left\{ \begin{array}{l l} 1, & t = i, i + s, i + 2 s, \dots \\ 0, & \text {o t h e r w i s e} \end{array} . \right. \tag {4.51}
$$

The least squares estimates of the parameters in Eq. (4.50) are used to obtain the initial values as

$$
\begin{array}{l} \hat {\beta} _ {0, 0} = \hat {L} _ {0} = \hat {\beta} _ {0} \\ \hat {\beta} _ {1, 0} = \hat {\beta} _ {1} \\ \hat {S} _ {j - s} = \hat {Y} _ {\mathrm {j}} \quad \text {f o r} 1 \leq j \leq \mathrm {s} - 1 \\ \hat {S} _ {0} = - \sum_ {j = 1} ^ {s - 1} \hat {y} _ {j} \\ \end{array}
$$

These are initial values of the model parameters at the original origin of time, $t = 0$ . To make forecasts from the correct origin of time the permanent component must be shifted to time $T$ by computing $\hat { L } _ { T } = \hat { L } _ { 0 } + \hat { n s \beta _ { 1 } }$ . Alternatively, one could smooth the parameters using equations (4.46)鈥?4.48) for time periods $t = 1$ , 2,鈥?T.

Prediction Intervals As in the nonseasonal smoothing case, the calculations of the prediction intervals would require an estimate for the prediction error variance. The most common approach is to use the relationship between the exponential smoothing techniques and the ARIMA models of Chapter 5 as discussed in Section 4.8, and estimate the prediction error variance accordingly. It can be shown that the seasonal exponential smoothing using the three parameter Holt鈥揥inters method is optimal for an ARIMA $( 0 , 1 , s + 1 ) \times ( 0 , 1 , 0 ) _ { s }$ , process, where s represents the length of

the period of the seasonal cycles. For further details, see Yar and Chat-eld (1990) and McKenzie (1986).

An alternate approach is to recognize that the additive seasonal model is just a linear regression model and to use the ordinary least squares (OLS) regression procedure for constructing prediction intervals as discussed in Chapter 3. If the errors are correlated, the regression methods for autocorrelated errors could be used instead of OLS.

Example 4.7 Consider the clothing sales data given in Table 4.10. To obtain the smoothed version of this data, we can use the Winters鈥?method option in Minitab. Since the amplitude of the seasonal pattern is constant over time, we decide to use the additive model. Two issues we have encountered in previous exponential smoothers have to be addressed in this case as well鈥攊nitial values and the choice of smoothing constants. Similar recommendations as in the previous exponential smoothing options can also be made in this case. Of course, the choice of the smoothing constant, in particular, is a bit more concerning since it involves the estimation of three smoothing constants. In this example, we follow our usual recommendation and choose smoothing constants that are all equal to 0.2. For more complicated cases, we recommend seasonal ARIMA models, which we will discuss in Chapter 5.

Figure 4.27 shows the smoothed version of the seasonal clothing sales data. To use this model for forecasting, let us assume that we are currently in December 2002 and we are asked to make forecasts for the following year. Figure 4.28 shows the forecasted sales for 2003 together with the actual data and the $9 5 \%$ prediction limits. Note that the forecast for December 2003 is the 12-step-ahead forecast made in December 2002. Even though the forecast is made further in the future, it still performs well since in the 鈥渟easonal鈥?sense it is in fact a one-step-ahead forecast.

# 4.7.2 Multiplicative Seasonal Model

If the amplitude of the seasonal pattern is proportional to the average level of the seasonal time series, as in the liquor store sales data given in Figure 4.29, the following multiplicative seasonal model will be more appropriate:

$$
y _ {t} = L _ {t} S _ {t} + \varepsilon_ {t}, \tag {4.52}
$$

where $L _ { t }$ once again represents the permanent component (i.e., $\beta _ { 0 } + \beta _ { 1 } t )$ ; $S _ { t }$ represents the seasonal adjustment with $S _ { t } = S _ { t + s } = S _ { t + 2 s } = \cdots$ for

TABLE 4.10 US Clothing Sales from January 1992 to December 2003   

<table><tr><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td></tr><tr><td>Jan-92</td><td>4889</td><td>Aug-94</td><td>7824</td><td>Mar-97</td><td>7695</td><td>Oct-99</td><td>9481</td><td>May-02</td><td>9906</td></tr><tr><td>Feb-92</td><td>5197</td><td>Sep-94</td><td>7229</td><td>Apr-97</td><td>7161</td><td>Nov-99</td><td>10577</td><td>Jun-02</td><td>9530</td></tr><tr><td>Mar-92</td><td>6061</td><td>Oct-94</td><td>7772</td><td>May-97</td><td>7978</td><td>Dec-99</td><td>15552</td><td>Jul-02</td><td>9298</td></tr><tr><td>Apr-92</td><td>6720</td><td>Nov-94</td><td>8873</td><td>Jun-97</td><td>7506</td><td>Jan-00</td><td>6726</td><td>Aug-02</td><td>10,755</td></tr><tr><td>May-92</td><td>6811</td><td>Dec-94</td><td>13397</td><td>Jul-97</td><td>7602</td><td>Feb-00</td><td>7514</td><td>Sep-02</td><td>9128</td></tr><tr><td>Jun-92</td><td>6579</td><td>Jan-95</td><td>5377</td><td>Aug-97</td><td>8877</td><td>Mar-00</td><td>9330</td><td>Oct-02</td><td>10,408</td></tr><tr><td>Jul-92</td><td>6598</td><td>Feb-95</td><td>5516</td><td>Sep-97</td><td>7859</td><td>Apr-00</td><td>9472</td><td>Nov-02</td><td>11,618</td></tr><tr><td>Aug-92</td><td>7536</td><td>Mar-95</td><td>6995</td><td>Oct-97</td><td>8500</td><td>May-00</td><td>9551</td><td>Dec-02</td><td>16,721</td></tr><tr><td>Sep-92</td><td>6923</td><td>Apr-95</td><td>7131</td><td>Nov-97</td><td>9594</td><td>Jun-00</td><td>9203</td><td>Jan-03</td><td>7891</td></tr><tr><td>Oct-92</td><td>7566</td><td>May-95</td><td>7246</td><td>Dec-97</td><td>13952</td><td>Jul-00</td><td>8910</td><td>Feb-03</td><td>7892</td></tr><tr><td>Nov-92</td><td>8257</td><td>Jun-95</td><td>7140</td><td>Jan-98</td><td>6282</td><td>Aug-00</td><td>10378</td><td>Mar-03</td><td>9874</td></tr><tr><td>Dec-92</td><td>12,804</td><td>Jul-95</td><td>6863</td><td>Feb-98</td><td>6419</td><td>Sep-00</td><td>9731</td><td>Apr-03</td><td>9920</td></tr><tr><td>Jan-93</td><td>5480</td><td>Aug-95</td><td>7790</td><td>Mar-98</td><td>7795</td><td>Oct-00</td><td>9868</td><td>May-03</td><td>10,431</td></tr><tr><td>Feb-93</td><td>5322</td><td>Sep-95</td><td>7618</td><td>Apr-98</td><td>8478</td><td>Nov-00</td><td>11512</td><td>Jun-03</td><td>9758</td></tr><tr><td>Mar-93</td><td>6390</td><td>Oct-95</td><td>7484</td><td>May-98</td><td>8501</td><td>Dec-00</td><td>16422</td><td>Jul-03</td><td>10,003</td></tr><tr><td>Apr-93</td><td>7155</td><td>Nov-95</td><td>9055</td><td>Jun-98</td><td>8044</td><td>Jan-01</td><td>7263</td><td>Aug-03</td><td>11,055</td></tr></table>

(continued )

TABLE 4.10 (Continued)   

<table><tr><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td></tr><tr><td>May-93</td><td>7175</td><td>Dec-95</td><td>13,201</td><td>Jul-98</td><td>8272</td><td>Feb-01</td><td>7866</td><td>Sep-03</td><td>9941</td></tr><tr><td>Jun-93</td><td>6770</td><td>Jan-96</td><td>5375</td><td>Aug-98</td><td>9189</td><td>Mar-01</td><td>9535</td><td>Oct-03</td><td>10,763</td></tr><tr><td>Jul-93</td><td>6954</td><td>Feb-96</td><td>6105</td><td>Sep-98</td><td>8099</td><td>Apr-01</td><td>9710</td><td>Nov-03</td><td>12058</td></tr><tr><td>Aug-93</td><td>7438</td><td>Mar-96</td><td>7246</td><td>Oct-98</td><td>9054</td><td>May-01</td><td>9711</td><td>Dec-03</td><td>17535</td></tr><tr><td>Sep-93</td><td>7144</td><td>Apr-96</td><td>7335</td><td>Nov-98</td><td>10,093</td><td>Jun-01</td><td>9324</td><td></td><td></td></tr><tr><td>Oct-93</td><td>7585</td><td>May-96</td><td>7712</td><td>Dec-98</td><td>14668</td><td>Jul-01</td><td>9063</td><td></td><td></td></tr><tr><td>Nov-93</td><td>8558</td><td>Jun-96</td><td>7337</td><td>Jan-99</td><td>6617</td><td>Aug-01</td><td>10,584</td><td></td><td></td></tr><tr><td>Dec-93</td><td>12,753</td><td>Jul-96</td><td>7059</td><td>Feb-99</td><td>6928</td><td>Sep-01</td><td>8928</td><td></td><td></td></tr><tr><td>Jan-94</td><td>5166</td><td>Aug-96</td><td>8374</td><td>Mar-99</td><td>8734</td><td>Oct-01</td><td>9843</td><td></td><td></td></tr><tr><td>Feb-94</td><td>5464</td><td>Sep-96</td><td>7554</td><td>Apr-99</td><td>8973</td><td>Nov-01</td><td>11,211</td><td></td><td></td></tr><tr><td>Mar-94</td><td>7145</td><td>Oct-96</td><td>8087</td><td>May-99</td><td>9237</td><td>Dec-01</td><td>16,470</td><td></td><td></td></tr><tr><td>Apr-94</td><td>7062</td><td>Nov-96</td><td>9180</td><td>Jun-99</td><td>8689</td><td>Jan-02</td><td>7508</td><td></td><td></td></tr><tr><td>May-94</td><td>6993</td><td>Dec-96</td><td>13109</td><td>Jul-99</td><td>8869</td><td>Feb-02</td><td>8002</td><td></td><td></td></tr><tr><td>Jun-94</td><td>6995</td><td>Jan-97</td><td>5833</td><td>Aug-99</td><td>9764</td><td>Mar-02</td><td>10,203</td><td></td><td></td></tr><tr><td>Jul-94</td><td>6886</td><td>Feb-97</td><td>5949</td><td>Sep-99</td><td>8970</td><td>Apr-02</td><td>9548</td><td></td><td></td></tr></table>

![](images/3cf34861fe6d29fe035ed4a8c8dcd9e98375da87c56143193d0513e18269e1fc.jpg)  
FIGURE 4.27 Smoothed data for the US clothing sales from January 1992 to December 2003 using the additive model.

![](images/f0e5674fbd82346fb585440c10541685bafa869631705eb038b10daa17c11fcf.jpg)  
FIGURE 4.28 Forecasts for 2003 for the US clothing sales.

![](images/8d7833134e0ff6dbaa7b2ae740cff29282957aefe3dadabd7aa3cab0c795ea40.jpg)  
FIGURE 4.29 Time series plot of liquor store sales data from January 1992 to December 2004.

$t = i , . . . , s - 1$ , where $s$ is the length of the period of the cycles; and the $\varepsilon _ { t }$ are assumed to be uncorrelated with mean 0 and constant variance $\sigma _ { \varepsilon } ^ { 2 }$ . The restriction for the seasonal adjustments in this case becomes

$$
\sum_ {t} ^ {s} S _ {t} = s. \tag {4.53}
$$

As in the additive model, we will employ three exponential smoothers to estimate the parameters in Eq. (4.52).

Step 1. Update the estimate of $L _ { T }$ using

$$
\hat {L} _ {T} = \lambda_ {1} \frac {y _ {T}}{\hat {S} _ {T - s}} + (1 - \lambda_ {1}) (\hat {L} _ {T - 1} + \hat {\beta} _ {1, T - 1}), \qquad (4. 5 4)
$$

where $0 < \lambda _ { 1 } < 1$ . Similar interpretation as in the additive model can be made for the exponential smoother in Eq. (4.54).

Step 2. Update the estimate of $\beta _ { 1 }$ using

$$
\hat {\beta} _ {1, T} = \lambda_ {2} (\hat {L} _ {T} - \hat {L} _ {T - 1}) + (1 - \lambda_ {2}) \hat {\beta} _ {1, T - 1}, \tag {4.55}
$$

where $0 < \lambda _ { 2 } < 1$ .

Step 3. Update the estimate of $S _ { t }$ using

$$
\hat {S} _ {T} = \lambda_ {3} \frac {y _ {T}}{\hat {L} _ {T}} + (1 - \lambda_ {3}) \hat {S} _ {T - s}, \tag {4.56}
$$

where $0 < \lambda _ { 3 } < 1$

Step 4. The $\tau$ -step-ahead forecast, $\hat { y } _ { T + \tau } ( T )$ , is

$$
\hat {y} _ {T + \tau} (T) = (\hat {L} _ {T} + \hat {\beta} _ {1, T} \tau) \hat {S} _ {T} (\tau - s). \tag {4.57}
$$

It will almost be necessary to obtain starting values of the model parameters. Suppose that a record consisting of $n$ seasons of data is available. From this set of historical data, the initial values, $\hat { \beta } _ { 0 , 0 } , \hat { \beta } _ { 1 , 0 }$ , and $\hat { \boldsymbol { S } } _ { 0 }$ , can be calculated as

$$
\hat {\beta} _ {0, 0} = \hat {L} _ {0} = \frac {\bar {y} _ {n} - \bar {y} _ {1}}{(n - 1) s},
$$

where

$$
\bar {y} _ {i} = \frac {1}{s} \sum_ {t = (i - 1) s + 1} ^ {i s} y _ {t}
$$

and

$$
\hat {\beta} _ {1, 0} = \bar {y} _ {1} - \frac {s}{2} \hat {\beta} _ {0, 0}
$$

$$
\hat{S}_{j - s} = s\frac{\hat{S}_{j}^{*}}{\sum_{i = 1}^{s}\hat{S}_{i}^{*}}\text{for} 1\leq j\leq s,
$$

where

$$
\hat {S} _ {j} ^ {*} = \frac {1}{n} \sum_ {t = 1} ^ {\mathrm {n}} \frac {y _ {(t - 1) s + j}}{\bar {y} _ {t} - ((s + 1) / 2 - j) \hat {\beta} _ {0}}.
$$

For further details, please see Montgomery et al. (1990) and Abraham and Ledolter (1983).

Prediction Intervals Constructing prediction intervals for the multiplicative model is much harder than the additive model as the former is

nonlinear. Several authors have considered this problem, including Chat--eld and Yar (1991), Sweet (1985), and Gardner (1988). Chat-eld and Yar (1991) propose an empirical method in which the length of the prediction interval depends on the point of origin of the forecast and may decrease in length near the low points of the seasonal cycle. They also discuss the case where the error is assumed to be proportional to the seasonal effect rather than constant, which is the standard assumption in Winters鈥?method. Another approach would be to obtain a 鈥渓inearized鈥?version of Winters鈥?model by expanding it in a -rst-order Taylor series and use this to -nd an approximate variance of the predicted value (statisticians call this the delta method). Then this prediction variance could be used to construct prediction intervals much as is done in the linear regression model case.

Example 4.8 Consider the liquor store data given in Table 4.11. In Figure 4.29, we can see that the amplitude of the periodic behavior gets larger as the average level of the seasonal data gets larger due to a linear trend. Hence the multiplicative model will be more appropriate. Figures 4.30 and 4.31 show the smoothed data with additive and multiplicative models, respectively. Based on the performance of the smoothers, it should therefore be clear that the multiplicative model should indeed be preferred.

As for forecasting using the multiplicative model, we can assume as usual that we are currently in December 2003 and are asked to forecast the sales in 2004. Figure 4.32 shows the forecasts together with the actual values and the prediction intervals.

# 4.8 EXPONENTIAL SMOOTHING OF BIOSURVEILLANCE DATA

Bioterrorism is the use of biological agents in a campaign of aggression. The use of biological agents in warfare is not new; many centuries ago plague and other contagious diseases were employed as weapons. Their use today is potentially catastrophic, so medical and public health of-cials are designing and implementing biosurveillance systems to monitor populations for potential disease outbreaks. For example, public health of-cials collect syndrome data from sources such as hospital emergency rooms, outpatient clinics, and over-the-counter medication sales to detect disease outbreaks, such as the onset of the u season. For an excellent and highly readable introduction to statistical techniques for biosurveillance and syndromic surveillance, see Fricker (2013). Monitoring of syndromic data is also a type of epidemiologic surveillance in a biosurveillance process,

TABLE 4.1 1 Liquor Store Sales from January 1992 to December 2004   

<table><tr><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td></tr><tr><td>Jan-92</td><td>1519</td><td>Aug-94</td><td>1870</td><td>Mar-97</td><td>1862</td><td>Oct-99</td><td>2264</td><td>May-02</td><td>2661</td></tr><tr><td>Feb-92</td><td>1551</td><td>Sep-94</td><td>1834</td><td>Apr-97</td><td>1826</td><td>Nov-99</td><td>2321</td><td>Jun-02</td><td>2579</td></tr><tr><td>Mar-92</td><td>1606</td><td>Oct-94</td><td>1817</td><td>May-97</td><td>2071</td><td>Dec-99</td><td>3336</td><td>Jul-02</td><td>2667</td></tr><tr><td>Apr-92</td><td>1686</td><td>Nov-94</td><td>1857</td><td>Jun-97</td><td>2012</td><td>Jan-00</td><td>1963</td><td>Aug-02</td><td>2698</td></tr><tr><td>May-92</td><td>1834</td><td>Dec-94</td><td>2593</td><td>Jul-97</td><td>2109</td><td>Feb-00</td><td>2022</td><td>Sep-02</td><td>2392</td></tr><tr><td>Jun-92</td><td>1786</td><td>Jan-95</td><td>1565</td><td>Aug-97</td><td>2092</td><td>Mar-00</td><td>2242</td><td>Oct-02</td><td>2504</td></tr><tr><td>Jul-92</td><td>1924</td><td>Feb-95</td><td>1510</td><td>Sep-97</td><td>1904</td><td>Apr-00</td><td>2184</td><td>Nov-02</td><td>2719</td></tr><tr><td>Aug-92</td><td>1874</td><td>Mar-95</td><td>1736</td><td>Oct-97</td><td>2063</td><td>May-00</td><td>2415</td><td>Dec-02</td><td>3647</td></tr><tr><td>Sep-92</td><td>1781</td><td>Apr-95</td><td>1709</td><td>Nov-97</td><td>2096</td><td>Jun-00</td><td>2473</td><td>Jan-03</td><td>2228</td></tr><tr><td>Oct-92</td><td>1894</td><td>May-95</td><td>1818</td><td>Dec-97</td><td>2842</td><td>Jul-00</td><td>2524</td><td>Feb-03</td><td>2153</td></tr><tr><td>Nov-92</td><td>1843</td><td>Jun-95</td><td>1873</td><td>Jan-98</td><td>1863</td><td>Aug-00</td><td>2483</td><td>Mar-03</td><td>2395</td></tr><tr><td>Dec-92</td><td>2527</td><td>Jul-95</td><td>1898</td><td>Feb-98</td><td>1786</td><td>Sep-00</td><td>2419</td><td>Apr-03</td><td>2460</td></tr><tr><td>Jan-93</td><td>1623</td><td>Aug-95</td><td>1872</td><td>Mar-98</td><td>1913</td><td>Oct-00</td><td>2413</td><td>May-03</td><td>2718</td></tr><tr><td>Feb-93</td><td>1539</td><td>Sep-95</td><td>1856</td><td>Apr-98</td><td>1985</td><td>Nov-00</td><td>2615</td><td>Jun-03</td><td>2570</td></tr><tr><td>Mar-93</td><td>1688</td><td>Oct-95</td><td>1800</td><td>May-98</td><td>2164</td><td>Dec-00</td><td>3464</td><td>Jul-03</td><td>2758</td></tr><tr><td>Apr-93</td><td>1725</td><td>Nov-95</td><td>1892</td><td>Jun-98</td><td>2084</td><td>Jan-01</td><td>2165</td><td>Aug-03</td><td>2809</td></tr></table>

(continued )

TABLE 4.1 1 (Continued)   

<table><tr><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td><td>Date</td><td>Sales</td></tr><tr><td>May-93</td><td>1807</td><td>Dec-95</td><td>2616</td><td>Jul-98</td><td>2237</td><td>Feb-01</td><td>2107</td><td>Sep-03</td><td>2597</td></tr><tr><td>Jun-93</td><td>1804</td><td>Jan-96</td><td>1690</td><td>Aug-98</td><td>2146</td><td>Mar-01</td><td>2390</td><td>Oct-03</td><td>2785</td></tr><tr><td>Jul-93</td><td>1962</td><td>Feb-96</td><td>1662</td><td>Sep-98</td><td>2058</td><td>Apr-01</td><td>2292</td><td>Nov-03</td><td>2803</td></tr><tr><td>Aug-93</td><td>1788</td><td>Mar-96</td><td>1849</td><td>Oct-98</td><td>2193</td><td>May-01</td><td>2538</td><td>Dec-03</td><td>3849</td></tr><tr><td>Sep-93</td><td>1717</td><td>Apr-96</td><td>1810</td><td>Nov-98</td><td>2186</td><td>Jun-01</td><td>2596</td><td>Jan-04</td><td>2406</td></tr><tr><td>Oct-93</td><td>1769</td><td>May-96</td><td>1970</td><td>Dec-98</td><td>3082</td><td>Jul-01</td><td>2553</td><td>Feb-04</td><td>2324</td></tr><tr><td>Nov-93</td><td>1794</td><td>Jun-96</td><td>1971</td><td>Jan-99</td><td>1897</td><td>Aug-01</td><td>2590</td><td>Mar-04</td><td>2509</td></tr><tr><td>Dec-93</td><td>2459</td><td>Jul-96</td><td>2047</td><td>Feb-99</td><td>1838</td><td>Sep-01</td><td>2384</td><td>Apr-04</td><td>2670</td></tr><tr><td>Jan-94</td><td>1557</td><td>Aug-96</td><td>2075</td><td>Mar-99</td><td>2021</td><td>Oct-01</td><td>2481</td><td>May-04</td><td>2809</td></tr><tr><td>Feb-94</td><td>1514</td><td>Sep-96</td><td>1791</td><td>Apr-99</td><td>2136</td><td>Nov-01</td><td>2717</td><td>Jun-04</td><td>2764</td></tr><tr><td>Mar-94</td><td>1724</td><td>Oct-96</td><td>1870</td><td>May-99</td><td>2250</td><td>Dec-01</td><td>3648</td><td>Jul-04</td><td>2995</td></tr><tr><td>Apr-94</td><td>1769</td><td>Nov-96</td><td>2003</td><td>Jun-99</td><td>2186</td><td>Jan-02</td><td>2182</td><td>Aug-04</td><td>2745</td></tr><tr><td>May-94</td><td>1842</td><td>Dec-96</td><td>2562</td><td>Jul-99</td><td>2383</td><td>Feb-02</td><td>2180</td><td>Sep-04</td><td>2742</td></tr><tr><td>Jun-94</td><td>1869</td><td>Jan-97</td><td>1716</td><td>Aug-99</td><td>2182</td><td>Mar-02</td><td>2447</td><td>Oct-04</td><td>2863</td></tr><tr><td>Jul-94</td><td>1994</td><td>Feb-97</td><td>1629</td><td>Sep-99</td><td>2169</td><td>Apr-02</td><td>2380</td><td>Nov-04</td><td>2912</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Dec-04</td><td>4085</td></tr></table>

![](images/58d3de217457ccaed312272df385c18860a343ecba3aa56f202249de2610abcb.jpg)  
FIGURE 4.30 Smoothed data for the liquor store sales from January 1992 to December 2004 using the additive model.

![](images/99c7fa594e83a11344bac380deafb0aa1ceeb7a981688a979ffbea5f86d399e2.jpg)  
FIGURE 4.31 Smoothed data for the liquor store sales from January 1992 to December 2004 using the multiplicative model.

![](images/de550ee6cb6e2f801879d370d8c5112b4a91be5b71d22a77d48285997213ef36.jpg)  
FIGURE 4.32 Forecasts for the liquor store sales for 2004 using the multiplicative model.

where signi-cantly higher than anticipated counts of inuenza-like illness might signal a potential bioterrorism attack.

As an example of such syndromic data, Fricker (2013) describes daily counts of respiratory and gastrointestinal complaints for more than $2 \ { } ^ { 1 } / _ { 2 }$ years at several hospitals in a large metropolitan area. Table 4.12 presents the respiratory count data from one of these hospitals. There are 980 observations. Fifty observations were missing from the original data set. The missing values were replaced with the last value that was observed on the same day of the week. This type of data imputation is a variation of 鈥淗ot Deck Imputation鈥?discussed in Section 1.4.3 and in Fricker (2013). It is also sometimes called last observation (or Value) carried forward (LOCF). For additional discussion see the web site: http://missingdata.lshtm.ac.uk/.

Figure 4.33 is a time series plot of the respiratory syndrome count data in Table 4.12. This plot was constructed using the Graph Builder feature in JMP. This software package overlays a smoothed curve on the data. The curve is -tted using locally weighted regression, often called loess. This is a variation of kernel regression that uses a weighted average of the data in a local neighborhood around a speci-c location to determine the value to plot at that location. Loess usually uses either -rst-order linear regression or a quadratic regression model for the weighted least squares -t. For more information on kernel regression and loess see Montgomery, et al. (2012).

TABLE 4.12 Counts of Respiratory Complaints at a Metropolitan Hospital   

<table><tr><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td></tr><tr><td>1</td><td>17</td><td>101</td><td>30</td><td>201</td><td>31</td><td>301</td><td>12</td><td>401</td><td>28</td><td>501</td><td>35</td><td>601</td><td>26</td><td>701</td><td>19</td><td>801</td><td>41</td><td>901</td><td>29</td></tr><tr><td>2</td><td>29</td><td>102</td><td>21</td><td>202</td><td>23</td><td>302</td><td>16</td><td>402</td><td>26</td><td>502</td><td>27</td><td>602</td><td>31</td><td>702</td><td>12</td><td>802</td><td>50</td><td>902</td><td>26</td></tr><tr><td>3</td><td>31</td><td>103</td><td>32</td><td>203</td><td>13</td><td>303</td><td>24</td><td>403</td><td>28</td><td>503</td><td>33</td><td>603</td><td>23</td><td>703</td><td>17</td><td>803</td><td>42</td><td>903</td><td>36</td></tr><tr><td>4</td><td>34</td><td>104</td><td>32</td><td>204</td><td>18</td><td>304</td><td>21</td><td>404</td><td>29</td><td>504</td><td>30</td><td>604</td><td>24</td><td>704</td><td>22</td><td>804</td><td>56</td><td>904</td><td>31</td></tr><tr><td>5</td><td>18</td><td>105</td><td>43</td><td>205</td><td>36</td><td>305</td><td>14</td><td>405</td><td>33</td><td>505</td><td>30</td><td>605</td><td>27</td><td>705</td><td>20</td><td>805</td><td>36</td><td>905</td><td>25</td></tr><tr><td>6</td><td>43</td><td>106</td><td>25</td><td>206</td><td>23</td><td>306</td><td>15</td><td>406</td><td>36</td><td>506</td><td>29</td><td>606</td><td>24</td><td>706</td><td>22</td><td>306</td><td>51</td><td>906</td><td>31</td></tr><tr><td>7</td><td>34</td><td>107</td><td>32</td><td>207</td><td>22</td><td>307</td><td>23</td><td>407</td><td>62</td><td>507</td><td>30</td><td>607</td><td>31</td><td>707</td><td>21</td><td>807</td><td>40</td><td>907</td><td>32</td></tr><tr><td>8</td><td>23</td><td>108</td><td>31</td><td>208</td><td>23</td><td>308</td><td>10</td><td>403</td><td>31</td><td>508</td><td>22</td><td>608</td><td>29</td><td>708</td><td>24</td><td>808</td><td>29</td><td>908</td><td>30</td></tr><tr><td>9</td><td>23</td><td>109</td><td>33</td><td>209</td><td>26</td><td>309</td><td>16</td><td>409</td><td>30</td><td>509</td><td>40</td><td>609</td><td>36</td><td>709</td><td>16</td><td>809</td><td>61</td><td>909</td><td>31</td></tr><tr><td>10</td><td>39</td><td>110</td><td>40</td><td>210</td><td>22</td><td>310</td><td>11</td><td>410</td><td>31</td><td>510</td><td>40</td><td>610</td><td>31</td><td>710</td><td>14</td><td>810</td><td>42</td><td>910</td><td>29</td></tr><tr><td>11</td><td>25</td><td>111</td><td>37</td><td>211</td><td>21</td><td>311</td><td>16</td><td>411</td><td>27</td><td>511</td><td>41</td><td>611</td><td>30</td><td>711</td><td>14</td><td>811</td><td>56</td><td>911</td><td>30</td></tr><tr><td>12</td><td>15</td><td>112</td><td>34</td><td>212</td><td>25</td><td>312</td><td>16</td><td>412</td><td>35</td><td>512</td><td>34</td><td>612</td><td>27</td><td>712</td><td>30</td><td>812</td><td>60</td><td>912</td><td>35</td></tr><tr><td>13</td><td>29</td><td>113</td><td>29</td><td>213</td><td>20</td><td>313</td><td>12</td><td>413</td><td>45</td><td>513</td><td>30</td><td>613</td><td>27</td><td>713</td><td>24</td><td>813</td><td>38</td><td>913</td><td>24</td></tr><tr><td>14</td><td>20</td><td>114</td><td>50</td><td>214</td><td>18</td><td>314</td><td>23</td><td>414</td><td>37</td><td>514</td><td>33</td><td>614</td><td>25</td><td>714</td><td>25</td><td>814</td><td>52</td><td>914</td><td>27</td></tr><tr><td>15</td><td>21</td><td>115</td><td>27</td><td>215</td><td>26</td><td>315</td><td>10</td><td>415</td><td>23</td><td>515</td><td>17</td><td>615</td><td>34</td><td>715</td><td>17</td><td>815</td><td>32</td><td>915</td><td>22</td></tr><tr><td>16</td><td>22</td><td>116</td><td>28</td><td>216</td><td>32</td><td>315</td><td>15</td><td>416</td><td>31</td><td>516</td><td>32</td><td>616</td><td>33</td><td>716</td><td>27</td><td>816</td><td>43</td><td>916</td><td>33</td></tr><tr><td>17</td><td>24</td><td>117</td><td>23</td><td>217</td><td>41</td><td>317</td><td>11</td><td>417</td><td>33</td><td>517</td><td>40</td><td>617</td><td>36</td><td>717</td><td>25</td><td>817</td><td>54</td><td>917</td><td>29</td></tr><tr><td>18</td><td>19</td><td>118</td><td>27</td><td>218</td><td>30</td><td>318</td><td>17</td><td>418</td><td>27</td><td>518</td><td>30</td><td>618</td><td>26</td><td>718</td><td>14</td><td>818</td><td>36</td><td>913</td><td>37</td></tr><tr><td>19</td><td>28</td><td>119</td><td>27</td><td>219</td><td>34</td><td>319</td><td>13</td><td>419</td><td>28</td><td>519</td><td>27</td><td>619</td><td>20</td><td>719</td><td>25</td><td>819</td><td>51</td><td>919</td><td>29</td></tr><tr><td>20</td><td>29</td><td>120</td><td>41</td><td>220</td><td>38</td><td>320</td><td>14</td><td>420</td><td>46</td><td>520</td><td>30</td><td>620</td><td>27</td><td>720</td><td>25</td><td>820</td><td>57</td><td>920</td><td>32</td></tr><tr><td>21</td><td>26</td><td>121</td><td>29</td><td>221</td><td>22</td><td>321</td><td>20</td><td>421</td><td>39</td><td>521</td><td>38</td><td>621</td><td>25</td><td>721</td><td>26</td><td>821</td><td>48</td><td>921</td><td>27</td></tr><tr><td>22</td><td>22</td><td>122</td><td>26</td><td>222</td><td>35</td><td>322</td><td>10</td><td>422</td><td>53</td><td>522</td><td>22</td><td>622</td><td>36</td><td>722</td><td>20</td><td>822</td><td>70</td><td>922</td><td>22</td></tr><tr><td>23</td><td>21</td><td>123</td><td>28</td><td>223</td><td>36</td><td>323</td><td>15</td><td>423</td><td>33</td><td>523</td><td>27</td><td>623</td><td>30</td><td>723</td><td>21</td><td>823</td><td>48</td><td>923</td><td>33</td></tr></table>

(continued )

TABLE 4.12 (Continued)   

<table><tr><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td></td><td></td></tr><tr><td>24</td><td>29</td><td>124</td><td>30</td><td>224</td><td>37</td><td>324</td><td>14</td><td>424</td><td>32</td><td>524</td><td>19</td><td>624</td><td>39</td><td>724</td><td>29</td><td>824</td><td>54</td><td>924</td><td>29</td></tr><tr><td>25</td><td>25</td><td>125</td><td>49</td><td>225</td><td>27</td><td>325</td><td>6</td><td>425</td><td>45</td><td>525</td><td>19</td><td>625</td><td>26</td><td>725</td><td>16</td><td>825</td><td>36</td><td>925</td><td>37</td></tr><tr><td>26</td><td>20</td><td>126</td><td>43</td><td>226</td><td>23</td><td>326</td><td>17</td><td>426</td><td>21</td><td>526</td><td>33</td><td>626</td><td>20</td><td>726</td><td>24</td><td>826</td><td>51</td><td>926</td><td>29</td></tr><tr><td>27</td><td>20</td><td>127</td><td>27</td><td>227</td><td>31</td><td>327</td><td>17</td><td>427</td><td>47</td><td>527</td><td>45</td><td>627</td><td>27</td><td>727</td><td>42</td><td>827</td><td>52</td><td>927</td><td>20</td></tr><tr><td>28</td><td>29</td><td>128</td><td>32</td><td>228</td><td>39</td><td>328</td><td>17</td><td>423</td><td>23</td><td>528</td><td>34</td><td>628</td><td>36</td><td>728</td><td>44</td><td>828</td><td>48</td><td>928</td><td>13</td></tr><tr><td>29</td><td>29</td><td>129</td><td>13</td><td>229</td><td>39</td><td>329</td><td>23</td><td>429</td><td>39</td><td>529</td><td>27</td><td>629</td><td>43</td><td>729</td><td>34</td><td>829</td><td>70</td><td>929</td><td>27</td></tr><tr><td>30</td><td>32</td><td>130</td><td>26</td><td>230</td><td>31</td><td>330</td><td>9</td><td>430</td><td>32</td><td>530</td><td>31</td><td>630</td><td>46</td><td>730</td><td>33</td><td>830</td><td>48</td><td>930</td><td>23</td></tr><tr><td>31</td><td>16</td><td>131</td><td>34</td><td>231</td><td>43</td><td>331</td><td>21</td><td>431</td><td>27</td><td>531</td><td>19</td><td>631</td><td>33</td><td>731</td><td>26</td><td>831</td><td>57</td><td>931</td><td>17</td></tr><tr><td>32</td><td>25</td><td>132</td><td>27</td><td>232</td><td>35</td><td>332</td><td>13</td><td>432</td><td>29</td><td>532</td><td>22</td><td>632</td><td>26</td><td>732</td><td>29</td><td>832</td><td>38</td><td>932</td><td>26</td></tr><tr><td>33</td><td>20</td><td>133</td><td>33</td><td>233</td><td>41</td><td>333</td><td>13</td><td>433</td><td>37</td><td>533</td><td>23</td><td>633</td><td>33</td><td>733</td><td>33</td><td>833</td><td>44</td><td>933</td><td>23</td></tr><tr><td>34</td><td>22</td><td>134</td><td>42</td><td>234</td><td>24</td><td>334</td><td>14</td><td>434</td><td>32</td><td>534</td><td>13</td><td>634</td><td>24</td><td>734</td><td>34</td><td>834</td><td>34</td><td>934</td><td>27</td></tr><tr><td>35</td><td>27</td><td>135</td><td>29</td><td>235</td><td>39</td><td>335</td><td>25</td><td>435</td><td>28</td><td>535</td><td>29</td><td>635</td><td>23</td><td>735</td><td>42</td><td>835</td><td>50</td><td>935</td><td>28</td></tr><tr><td>36</td><td>32</td><td>136</td><td>29</td><td>236</td><td>44</td><td>336</td><td>15</td><td>436</td><td>42</td><td>536</td><td>13</td><td>636</td><td>51</td><td>736</td><td>43</td><td>836</td><td>39</td><td>936</td><td>21</td></tr><tr><td>37</td><td>23</td><td>137</td><td>29</td><td>237</td><td>35</td><td>337</td><td>18</td><td>437</td><td>33</td><td>537</td><td>20</td><td>637</td><td>35</td><td>737</td><td>33</td><td>837</td><td>65</td><td>937</td><td>20</td></tr><tr><td>38</td><td>31</td><td>138</td><td>28</td><td>238</td><td>30</td><td>338</td><td>21</td><td>438</td><td>36</td><td>538</td><td>20</td><td>638</td><td>26</td><td>738</td><td>31</td><td>838</td><td>55</td><td>938</td><td>25</td></tr><tr><td>39</td><td>22</td><td>139</td><td>35</td><td>239</td><td>29</td><td>339</td><td>18</td><td>439</td><td>25</td><td>539</td><td>23</td><td>639</td><td>32</td><td>739</td><td>30</td><td>839</td><td>46</td><td>939</td><td>30</td></tr><tr><td>40</td><td>21</td><td>140</td><td>33</td><td>240</td><td>13</td><td>340</td><td>12</td><td>440</td><td>19</td><td>540</td><td>17</td><td>640</td><td>29</td><td>740</td><td>35</td><td>840</td><td>57</td><td>940</td><td>13</td></tr><tr><td>41</td><td>27</td><td>141</td><td>3S</td><td>241</td><td>23</td><td>341</td><td>10</td><td>441</td><td>34</td><td>541</td><td>31</td><td>641</td><td>24</td><td>741</td><td>34</td><td>841</td><td>43</td><td>941</td><td>19</td></tr><tr><td>42</td><td>37</td><td>142</td><td>23</td><td>242</td><td>19</td><td>342</td><td>10</td><td>442</td><td>34</td><td>542</td><td>21</td><td>642</td><td>18</td><td>742</td><td>43</td><td>842</td><td>50</td><td>942</td><td>20</td></tr><tr><td>43</td><td>28</td><td>143</td><td>28</td><td>243</td><td>24</td><td>343</td><td>17</td><td>443</td><td>33</td><td>543</td><td>29</td><td>643</td><td>36</td><td>743</td><td>21</td><td>843</td><td>39</td><td>943</td><td>27</td></tr><tr><td>44</td><td>41</td><td>144</td><td>23</td><td>244</td><td>19</td><td>344</td><td>12</td><td>444</td><td>26</td><td>544</td><td>20</td><td>644</td><td>15</td><td>744</td><td>42</td><td>844</td><td>55</td><td>944</td><td>14</td></tr><tr><td>45</td><td>45</td><td>145</td><td>31</td><td>245</td><td>27</td><td>345</td><td>24</td><td>445</td><td>43</td><td>545</td><td>21</td><td>645</td><td>33</td><td>745</td><td>30</td><td>845</td><td>38</td><td>945</td><td>21</td></tr><tr><td>46</td><td>40</td><td>146</td><td>29</td><td>246</td><td>20</td><td>345</td><td>22</td><td>446</td><td>31</td><td>546</td><td>25</td><td>646</td><td>21</td><td>746</td><td>29</td><td>846</td><td>29</td><td>946</td><td>32</td></tr><tr><td>47</td><td>32</td><td>147</td><td>24</td><td>247</td><td>19</td><td>347</td><td>14</td><td>447</td><td>30</td><td>547</td><td>35</td><td>647</td><td>25</td><td>747</td><td>29</td><td>347</td><td>32</td><td>947</td><td>18</td></tr><tr><td>48</td><td>45<
/td><td>148</td><td>22</td><td>248</td><td>28</td><td>348</td><td>14</td><td>448</td><td>41</td><td>548</td><td>24</td><td>648</td><td>25</td><td>748</td><td>41</td><td>848</td><td>27</td><td>948</td><td>25</td></tr></table>

29   

<table><tr><td>49</td><td>48</td><td>149</td><td>30</td><td>249</td><td>19</td><td>349</td><td>9</td><td>449</td><td>15</td><td>549</td><td>25</td><td>649</td><td>19</td><td>749</td><td>35</td><td>849</td><td>22</td><td>949</td><td>13</td></tr><tr><td>50</td><td>51</td><td>150</td><td>21</td><td>250</td><td>29</td><td>350</td><td>19</td><td>450</td><td>23</td><td>550</td><td>23</td><td>650</td><td>23</td><td>750</td><td>29</td><td>850</td><td>23</td><td>950</td><td>25</td></tr><tr><td>51</td><td>51</td><td>151</td><td>24</td><td>251</td><td>24</td><td>351</td><td>15</td><td>451</td><td>25</td><td>551</td><td>27</td><td>651</td><td>18</td><td>751</td><td>37</td><td>851</td><td>25</td><td>951</td><td>19</td></tr><tr><td>52</td><td>52</td><td>152</td><td>21</td><td>252</td><td>33</td><td>352</td><td>9</td><td>452</td><td>27</td><td>552</td><td>35</td><td>652</td><td>26</td><td>752</td><td>31</td><td>852</td><td>19</td><td>952</td><td>27</td></tr><tr><td>53</td><td>43</td><td>153</td><td>30</td><td>254</td><td>29</td><td>354</td><td>17</td><td>454</td><td>40</td><td>554</td><td>36</td><td>653</td><td>27</td><td>753</td><td>24</td><td>853</td><td>29</td><td>953</td><td>27</td></tr><tr><td>54</td><td>42</td><td>154</td><td>25</td><td>254</td><td>29</td><td>354</td><td>17</td><td>454</td><td>40</td><td>554</td><td>33</td><td>654</td><td>11</td><td>754</td><td>47</td><td>854</td><td>34</td><td>954</td><td>18</td></tr><tr><td>55</td><td>56</td><td>155</td><td>17</td><td>255</td><td>17</td><td>355</td><td>15</td><td>455</td><td>34</td><td>555</td><td>27</td><td>655</td><td>20</td><td>755</td><td>3</td><td>855</td><td>27</td><td>955</td><td>25</td></tr><tr><td>56</td><td>56</td><td>155</td><td>22</td><td>255</td><td>19</td><td>355</td><td>21</td><td>455</td><td>42</td><td>555</td><td>33</td><td>656</td><td>13</td><td>755</td><td>34</td><td>355</td><td>30</td><td>956</td><td>26</td></tr><tr><td>57</td><td>51</td><td>157</td><td>18</td><td>257</td><td>23</td><td>357</td><td>22</td><td>457</td><td>12</td><td>557</td><td>25</td><td>657</td><td>20</td><td>757</td><td>35</td><td>857</td><td>30</td><td>957</td><td>39</td></tr><tr><td>58</td><td>60</td><td>158</td><td>19</td><td>258</td><td>26</td><td>358</td><td>17</td><td>458</td><td>24</td><td>558</td><td>32</td><td>658</td><td>23</td><td>758</td><td>39</td><td>858</td><td>24</td><td>958</td><td>59</td></tr><tr><td>59</td><td>35</td><td>159</td><td>20</td><td>259</td><td>25</td><td>359</td><td>21</td><td>459</td><td>20</td><td>559</td><td>23</td><td>659</td><td>19</td><td>759</td><td>29</td><td>859</td><td>33</td><td>959</td><td>34</td></tr><tr><td>60</td><td>43</td><td>160</td><td>22</td><td>260</td><td>32</td><td>360</td><td>26</td><td>460</td><td>26</td><td>560</td><td>42</td><td>660</td><td>21</td><td>760</td><td>41</td><td>860</td><td>29</td><td>960</td><td>34</td></tr><tr><td>61</td><td>42</td><td>161</td><td>39</td><td>261</td><td>21</td><td>361</td><td>23</td><td>461</td><td>46</td><td>561</td><td>25</td><td>661</td><td>21</td><td>761</td><td>36</td><td>861</td><td>36</td><td>961</td><td>24</td></tr><tr><td>62</td><td>55</td><td>162</td><td>35</td><td>262</td><td>15</td><td>362</td><td>20</td><td>462</td><td>35</td><td>562</td><td>33</td><td>662</td><td>29</td><td>762</td><td>50</td><td>862</td><td>29</td><td>962</td><td>25</td></tr><tr><td>63</td><td>46</td><td>163</td><td>29</td><td>263</td><td>20</td><td>363</td><td>28</td><td>463</td><td>46</td><td>563</td><td>19</td><td>663</td><td>18</td><td>763</td><td>33</td><td>863</td><td>27</td><td>963</td><td>40</td></tr><tr><td>64</td><td>49</td><td>164</td><td>24</td><td>264</td><td>19</td><td>364</td><td>34</td><td>464</td><td>33</td><td>564</td><td>40</td><td>664</td><td>25</td><td>764</td><td>38</td><td>864</td><td>32</td><td>964</td><td>19</td></tr><tr><td>65</td><td>40</td><td>165</td><td>22</td><td>265</td><td>13</td><td>365</td><td>23</td><td>465</td><td>27</td><td>565</td><td>35</td><td>665</td><td>24</td><td>765</td><td>40</td><td>865</td><td>30</td><td>965</td><td>35</td></tr><tr><td>66</td><td>33</td><td>166</td><td>26</td><td>266</td><td>25</td><td>366</td><td>20</td><td>466</td><td>35</td><td>566</td><td>36</td><td>666</td><td>19</td><td>766</td><td>41</td><td>866</td><td>23</td><td>966</td><td>34</td></tr><tr><td>67</td><td>45</td><td>167</td><td>27</td><td>267</td><td>267</td><td>37</td><td>467</td><td>33</td><td>567</td><td>33</td><td>667</td><td>15</td><td>767</td><td>34</td><td>867</td><td>25</td><td>967</td><td>33</td><td></td></tr><tr><td>68</td><td>37</td><td>168</td><td>28</td><td>268</td><td>9</td><td>368</td><td>22</td><td>468</td><td>29</td><td>568</td><td>25</td><td>668</td><td>23</td><td>768</td><td>42</td><td>868</td><td>23</td><td>968</td><td>29</td></tr><tr><td>69</td><td>44</td><td>169</td><td>36</td><td>269</td><td>20</td><td>369</td><td>32</td><td>469</td><td>45</td><td>569</td><td>33</td><td>669</td><td>14</td><td>769</td><td>40</td><td>869</td><td>29</td><td>969</td><td>23</td></tr><tr><td>70</td><td>50</td><td>170</td><td>31</td><td>270</td><td>20</td><td>370</td><td>41</td><td>470</td><td>18</td><td>570</td><td>33</td><td>670</td><td>16</td><td>770</td><td>50</td><td>870</td><td>26</td><td>970</td><td>29</td></tr><tr><td>71</td><td>37</td><td>171</td><td>31</td><td>271</td><td>21</td><td>371</td><td>35</td><td>471</td><td>21</td><td>571</td><td>27</td><td>671</td><td>16</td><td>771</td><td>30</td><td>871</td><td>29</td><td>971</td><td>25</td></tr><tr><td>72</td><td>36</td><td>172</td><td>34</td><td>272</td><td>21</td><td>372</td><td>41</td><td>472</td><td>35</td><td>572</td><td>33</td><td>672</td><td>22</td><td>772</td><td>34</td><td>872</td><td>22</td><td>972</td><td>19</td></tr><tr><td>73</td><td>43</td><td>173</td><td>19</td><td>273</td><td>20</td><td>373</td><td>43</td><td>473</td><td>39</td><td>573</td><td>33</td><td>673</td><td>13</td><td>773</td><td>28</td><td>873</td><td>16</td><td>973</td><td>34</td></tr><tr><td>74</td><td>49</td><td>174</td><td>37</td>
<td>274</td><td>13</td><td>374</td><td>33</td><td>474</td><td>40</td><td>574</td><td>39</td><td>674</td><td>19</td><td>774</td><td>21</td><td>874</td><td>25</td><td>974</td><td>37</td></tr><tr><td>75</td><td>4C</td><td>175</td><td>39</td><td>275</td><td>25</td><td>375</td><td>32</td><td>475</td><td>33</td><td>575</td><td>30</td><td>675</td><td>23</td><td>775</td><td>24</td><td>875</td><td>26</td><td>975</td><td>34</td></tr></table>

(continued )

TABLE 4.12 (Continued)   

<table><tr><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td><td>Day</td><td>Count</td></tr><tr><td>76</td><td>65</td><td>176</td><td>32</td><td>276</td><td>29</td><td>376</td><td>28</td><td>476</td><td>35</td><td>576</td><td>33</td><td>676</td><td>14</td><td>776</td><td>37</td><td>876</td><td>25</td><td>976</td><td>29</td></tr><tr><td>77</td><td>49</td><td>177</td><td>36</td><td>277</td><td>16</td><td>377</td><td>42</td><td>477</td><td>20</td><td>577</td><td>26</td><td>677</td><td>16</td><td>777</td><td>44</td><td>877</td><td>24</td><td>977</td><td>27</td></tr><tr><td>78</td><td>49</td><td>178</td><td>42</td><td>278</td><td>18</td><td>378</td><td>27</td><td>478</td><td>36</td><td>578</td><td>26</td><td>678</td><td>10</td><td>778</td><td>39</td><td>878</td><td>29</td><td>978</td><td>18</td></tr><tr><td>79</td><td>34</td><td>179</td><td>31</td><td>279</td><td>32</td><td>379</td><td>25</td><td>479</td><td>34</td><td>579</td><td>23</td><td>679</td><td>14</td><td>779</td><td>37</td><td>879</td><td>34</td><td>979</td><td>26</td></tr><tr><td>80</td><td>33</td><td>180</td><td>28</td><td>280</td><td>32</td><td>380</td><td>32</td><td>480</td><td>35</td><td>580</td><td>24</td><td>680</td><td>13</td><td>780</td><td>35</td><td>880</td><td>35</td><td>980</td><td>28</td></tr><tr><td>81</td><td>29</td><td>181</td><td>35</td><td>281</td><td>19</td><td>381</td><td>27</td><td>481</td><td>36</td><td>581</td><td>32</td><td>681</td><td>15</td><td>781</td><td>32</td><td>881</td><td>29</td><td></td><td></td></tr><tr><td>82</td><td>32</td><td>182</td><td>36</td><td>282</td><td>24</td><td>382</td><td>35</td><td>482</td><td>29</td><td>582</td><td>24</td><td>682</td><td>15</td><td>782</td><td>41</td><td>882</td><td>39</td><td></td><td></td></tr><tr><td>83</td><td>57</td><td>183</td><td>35</td><td>283</td><td>18</td><td>383</td><td>26</td><td>483</td><td>19</td><td>583</td><td>32</td><td>683</td><td>11</td><td>783</td><td>41</td><td>883</td><td>31</td><td></td><td></td></tr><tr><td>84</td><td>43</td><td>184</td><td>32</td><td>284</td><td>20</td><td>384</td><td>32</td><td>484</td><td>36</td><td>584</td><td>41</td><td>634</td><td>11</td><td>784</td><td>51</td><td>884</td><td>26</td><td></td><td></td></tr><tr><td>85</td><td>40</td><td>185</td><td>26</td><td>285</td><td>20</td><td>385</td><td>42</td><td>485</td><td>35</td><td>585</td><td>26</td><td>685</td><td>18</td><td>785</td><td>43</td><td>885</td><td>24</td><td></td><td></td></tr><tr><td>86</td><td>46</td><td>186</td><td>29</td><td>286</td><td>20</td><td>386</td><td>38</td><td>486</td><td>31</td><td>586</td><td>28</td><td>686</td><td>16</td><td>786</td><td>35</td><td>886</td><td>31</td><td></td><td></td></tr><tr><td>87</td><td>33</td><td>187</td><td>25</td><td>287</td><td>24</td><td>387</td><td>36</td><td>487</td><td>23</td><td>587</td><td>25</td><td>687</td><td>18</td><td>787</td><td>33</td><td>887</td><td>24</td><td></td><td></td></tr><tr><td>88</td><td>30</td><td>188</td><td>23</td><td>288</td><td>15</td><td>388</td><td>26</td><td>488</td><td>31</td><td>588</td><td>29</td><td>688</td><td>15</td><td>788</td><td>33</td><td>888</td><td>29</td><td></td><td></td></tr><tr><td>89</td><td>41</td><td>189</td><td>29</td><td>289</td><td>22</td><td>389</td><td>26</td><td>489</td><td>29</td><td>589</td><td>40</td><td>689</td><td>16</td><td>789</td><td>31</td><td>889</td><td>26</td><td></td><td></td></tr><tr><td>90</td><td>38</td><td>190</td><td>29</td><td>290</td><td>16</td><td>390</td><td>24</td><td>490</td><td>44</td><td>590</td><td>34</td><td>690</td><td>11</td><td>790</td><td>43</td><td>890</td><td>45</td><td></td><td></td></tr><tr><td>91</td><td>29</td><td>191</td><td>26</td><td>291</td><td>14</td><td>391</td><td>30</td><td>491</td><td>42</td><td>591</td><td>41</td><td>691</td><td>11</td><td>791</td><td>45</td><td>891</td><td>36</td><td></td><td></td></tr><tr><td>92</td><td>41</td><td>192</td><td>18</td><td>292</td><td>17</td><td>392</td><td>32</td><td>492</td><td>31</td><td>592</td><td>37</td><td>692</td><td>23</td><td>792</td><td>43</td><td>892</td><td>29</td><td></td><td></td></tr><tr><td>93</td><td>28</td><td>193</td><td>19</td><td>293</td><td>15</td><td>393</td><td>14</td><td>493</td><td>31</td><td>593</td><td>36</td><td>693</td><td>20</td><td>793</td><td>42</td><td>893</td><td>22</td><td></td><td></td></tr><tr><td>94</td><td>47</td><td>194</td><td>17</td><td>294</td><td>8</td><td>394</td><td>27</td><td>494</td><td>24</td><td>594</td><td>26</td><td>694</td><td>18</td><td>794</td><td>36</td><td>894</td><td>31</td><td></td><td></td></tr><tr><td>95</td><td>42</td><td>195</td><td>22</td><td>295</td><td>23</td><td>395</td><td>26</td><td>495</td><td>30</td><td>595</td><td>42</td><td>695</td><td>24</td><td>795</td><td>34</td><td>895</td><td>38</td><td></td><td></td></tr><tr><td>96</td><td>34</td><td>196</td><td>25</td><td>296</td><td>17</td><td>396</td><td>25</td><td>496</td><td>26</td><td>596</td><td>40</td><td>696</td><td>14</td><td>796</td><td>30</td><td>896</td><td>36</td><td></td><td></td></tr><tr><td>97</td><td>40</td><td>197</td><td>33</td><td>297</td><td>13</td><td>397</td><td>23</td><td>497</td><td>26</td><td>597</td><td>34</td><td>697</td><td>22</td><td>797</td><td>46</td><td>897</td><td>33</td><td></td><td></td></tr><tr><td>98</td><td>35</td><td>198</td><td>10</td><td>298</td><td>15</td><td>398</td><td>27</td><td>498</td><td>39</td><td>598</td><td>41</td><td>698</td><td>16</td><td>798</td><td>54</td><td>898</td><td>34</td><td></td><td></td></tr><tr><td>99</td><td>40</td><td>199</td><td>25</td><td>299</td><td>15</td><td>399</td><td>36</td><td>499</td><td>35</td><td>599</td><td>37</td><td>699</td><td>26</td><td>799</td><td>52</td><td>899</td><td>34</td><td></td><td></td></tr><tr><td>100</td><td>24</td><td>200</td><td>25</td><td>300</td><td>13</td><td>400</td><td>40</td><td>500</td>
<td>34</td><td>600</td><td>36</td><td>700</td><td>17</td><td>800</td><td>39</td><td>900</td><td>25</td><td></td><td></td></tr></table>

![](images/fed9586f6d495fce19094931bec9b7b223292b71dddc9a412ed307cc7af98ce1.jpg)  
FIGURE 4.33 Time series plot of daily respiratory syndrome count, with kernelsmoothed -tted line. $( \alpha = 0 . 1 $ ).

Over the $2 ~ \%$ year period, the daily counts of the respiratory syndrome appear to follow a weak seasonal pattern, with the highest peak in November鈥揇ecember (late fall), a secondary peak in March鈥揂pril, and then decreasing to the lowest counts in June鈥揂ugust (summer). The amplitude, or range within a year, seems to vary, but counts do not appear to be increasing or decreasing over time.

Not immediately evident from the time series plots is a potential day effect. The box plots of the residuals from the loess smoothed line in Figure 4.33 are plotted in Figure 4.34 versus day of the week. These plots exhibit variation that indicates slightly higher-than-expected counts on Monday and slightly lower-than-expected counts on Thursday, Friday, and Saturday.

The exponential smoothing procedure in JMP was applied to the respiratory syndrome data. The results of -rst-order or simple exponential smoothing are summarized in Table 4.13 and Figure 4.35, which plots only the last 100 observations along with the smoothed values. JMP reported the value of the smoothing constant that produced the minimum value of the error sum of squares as $\lambda = 0 . 2 1$ . This value also minimizes the AIC and BIC criteria, and results in the smallest values of the mean absolute

![](images/f37d64801ada8201e0520f2672f9b3bfd88e4f329989ccfd2670ba11b295e1fb.jpg)  
FIGURE 4.34 Box plots of residuals from the kernel-smoothed line -t to daily respiratory syndrome count.

prediction error and the mean absolute, although there is very little difference between the optimal value of $\lambda = 0 . 2 1$ and the values $\lambda = 0 . 1$ and $\lambda = 0 . 4$ .

The results of using second-order exponential smoothing are summarized in Table 4.14 and illustrated graphically for the last 100 observations in Figure 4.36. There is not a lot of difference between the two procedures, although the optimal -rst-order smoother does perform slightly better and the larger smoothing parameters in the double smoother perform more poorly.

Single and double exponential smoothing do not account for the apparent mild seasonality observed in the original time series plot of the data.

TABLE 4.13 First-Order Simple Exponential Smoothing Applied to the Respiratory Data   

<table><tr><td>Model</td><td>Variance</td><td>AIC</td><td>BIC</td><td>MAPE</td><td>MAE</td></tr><tr><td>First-Order Exponential (min SSE, 位=0.21)</td><td>52.66</td><td>6660.81</td><td>6665.70</td><td>21.43</td><td>5.67</td></tr><tr><td>First-Order Exponential (位=0.1)</td><td>55.65</td><td>6714.67</td><td>6714.67</td><td>22.23</td><td>5.85</td></tr><tr><td>First-Order Exponential (位=0.4)</td><td>55.21</td><td>6705.63</td><td>6705.63</td><td>21.87</td><td>5.82</td></tr></table>

FIGURE 4.35 Respiratory syndrome counts using -rst-order exponential smoothing with $\lambda = 0 . 1$ , $\lambda = 0 . 2 1$ (min SSE), and $\lambda = 0 . 4$ .   
![](images/9dcb2862cade85f11358dad56d12d67ac434e27ce6ee2682279daf44a889b766.jpg)  
Actual count 位 = 0.1 位 = 0.21 位 = 0.4

We used JMP to -t Winters鈥?additive seasonal model to the respiratory syndrome count data. Because the seasonal patterns are not strong, we investigated seasons of length 3, 7, and 12 periods. The results are summarized in Table 4.15 and illustrated graphically for the last 100 observations in Figure 4.37. The 7-period season works best, probably reecting the daily seasonal pattern that we observed in Figure 4.34. This is also the best smoother of all the techniques that were investigated. The values of $\lambda = 0$ for the trend and seasonal components in this model are an indication that there is not a signi-cant linear trend in the data and that the seasonal pattern is relatively stable over the period of available data.

TABLE 4.14 Second-Order Simple Exponential Smoothing Applied to the Respiratory Data   

<table><tr><td>Model</td><td>Variance</td><td>AIC</td><td>BIC</td><td>MAPE</td><td>MAE</td></tr><tr><td>Second-Order Exponential (min SSE, 位 = 0.10)</td><td>54.37</td><td>6690.98</td><td>6695.86</td><td>21.71</td><td>5.78</td></tr><tr><td>Second-Order Exponential (位 = 0.2)</td><td>58.22</td><td>6754.37</td><td>6754.37</td><td>22.44</td><td>5.98</td></tr><tr><td>Second-Order Exponential (位 = 0.4)</td><td>74.46</td><td>6992.64</td><td>6992.64</td><td>25.10</td><td>6.74</td></tr></table>

FIGURE 4.36 Respiratory syndrome counts using second-order exponential smoothing with $\lambda = 0 . 1 0$ (min SSE), $\lambda = 0 . 2$ , and $\lambda = 0 . 4$ .   
![](images/c22d0f7deb9f58e5693643d47659fc0206155279b203eec812380d12fc897503.jpg)  
Actual count 位 = 0.10 位 = 0.2 位 = 0.4

TABLE 4.15 Winters鈥?Additive Seasonal Exponential Smoothing Applied to the Respiratory Data   

<table><tr><td>Model</td><td>Variance</td><td>AIC</td><td>BIC</td><td>MAPE</td><td>MAE</td></tr><tr><td>S=3</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Winters Additive (min SSE, 位1=0.21, 位2=0, 位3=0)</td><td>52.75</td><td>6662.75</td><td>6677.40</td><td>21.70</td><td>5.72</td></tr><tr><td>Winters Additive (位1=0.2, 位2=0.1, 位3=0.1)</td><td>57.56</td><td>6731.59</td><td>6731.59</td><td>22.38</td><td>5.94</td></tr><tr><td>S=7</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Winters Additive (min SSE, 位1=0.22, 位2=0, 位3=0)</td><td>49.77</td><td>6593.83</td><td>6608.47</td><td>21.10</td><td>5.56</td></tr><tr><td>Winters Additive (位1=0.2, 位2=0.1, 位3=0.1)</td><td>54.27</td><td>6652.57</td><td>6652.57</td><td>21.47</td><td>5.70</td></tr><tr><td>S=12</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Winters Additive (min SSE, 位1=0.21, 位2=0, 位3=0)</td><td>52.74</td><td>6635.58</td><td>6650.21</td><td>22.13</td><td>5.84</td></tr><tr><td>Winters Additive (位1=0.2, 位2=0.1, 位3=0.1)</td><td>58.76</td><td>6703.79</td><td>6703.79</td><td>22.77</td><td>6.08</td></tr></table>

![](images/aa9e33573b8ce8927c8569bcfc5aee9b3732376d3b57a6c65e516b52e169f469.jpg)

![](images/0957c3eea1e78e6211bbdbc97abe96bcd7e651f976c1c1db1e3c9591818af7ac.jpg)  
FIGURE 4.37 Respiratory syndrome counts using winters鈥?additive seasonal exponential smoothing with $S = 3$ , $S = 7$ , and $S = 1 2$ , and smoothing parameters that minimize SSE.

# 4.9 EXPONENTIAL SMOOTHERS AND ARIMA MODELS

The -rst-order exponential smoother presented in Section 4.2 is a very effective model in forecasting. The discount factor, $\lambda$ , makes this smoother fairly exible in handling time series data with various characteristics. The -rst-order exponential smoother is particularly good in forecasting time series data with certain speci-c characteristics.

Recall that the -rst-order exponential smoother is given as

$$
\tilde {y} _ {T} = \lambda y _ {T} + (1 - \lambda) \tilde {y} _ {T - 1} \tag {4.58}
$$

and the forecast error is de-ned as

$$
e _ {T} = y _ {T} - \hat {y} _ {T - 1}. \tag {4.59}
$$

Similarly, we have

$$
e _ {T - 1} = y _ {T - 1} - \hat {y} _ {T - 2}. \tag {4.60}
$$

By multiplying Eq. (4.60) by $( 1 - \lambda )$ and subtracting it from Eq. (4.59), we obtain

$$
\begin{array}{l} e _ {T} - (1 - \lambda) e _ {T - 1} = (y _ {T} - \hat {y} _ {T - 1}) - (1 - \lambda) (y _ {T - 1} - \hat {y} _ {T - 2}) \\ = y _ {T} - y _ {T - 1} - \hat {y} _ {T - 1} + \underbrace {\lambda y _ {T - 1} + (1 - \lambda) \hat {y} _ {T - 2}} _ {= \hat {y} _ {T - 1}} \tag {4.61} \\ = y _ {T} - y _ {T - 1} - \hat {y} _ {T - 1} + \hat {y} _ {T - 1} \\ = y _ {T} - y _ {T - 1}. \\ \end{array}
$$

We can rewrite Eq. (4.61) as

$$
y _ {T} - y _ {T - 1} = e _ {T} - \theta e _ {T - 1}, \tag {4.62}
$$

where $\theta = 1 - \lambda$ . Recall from Chapter 2 the backshift operator, B, de-ned as $B ( y _ { t } ) = y _ { t - 1 }$ . Thus Eq. (4.62) becomes

$$
(1 - B) y _ {T} = (1 - \theta B) e _ {T}. \tag {4.63}
$$

We will see in Chapter 5 that the model in Eq. (4.63) is called the integrated moving average model denoted as IMA(1,1), for the backshift operator is used only once on $y _ { T }$ and only once on the error. It can be shown that if the process exhibits the dynamics de-ned in Eq. (4.63), that is an IMA(1,1) process, the -rst-order exponential smoother provides minimum mean squared error (MMSE) forecasts (see Muth (1960), Box and Luceno (1997), and Box, Jenkins, and Reinsel (1994)). For more discussion of the equivalence between exponential smoothing techniques and the ARIMA models, see Abraham and Ledolter (1983), Cogger (1974), Goodman (1974), Pandit and Wu (1974), and McKenzie (1984).

# 4.10 R COMMANDS FOR CHAPTER 4

Example 4.1 The Dow Jones index data are in the second column of the array called dji.data in which the -rst column is the month of the year. We can use the following simple function to obtain the -rst-order exponential smoothing

```txt
firstsmooth<-function(y, lambda, start=y[1]) {
    ytilde<-y
    ytilde[1]<-lambda*y[1] + (1-lambda)*start
    for (i in 2:length(y)) {
        ytilde[i]<-lambda*y[i] + (1-lambda)*ytilde[i-1]
    }
} 
```

Note that this function uses the -rst observation as the starting value by default. One can change this by providing a speci-c start value when calling the function.

We can then obtain the smoothed version of the data for a speci-ed lambda value and plot the -tted value as the following:

```python
dji.smooth1<-firstsmooth(y=dji.data[,2],lambda=0.4) plot(dji.data[,2],type="p",pch=16,cex=.5,xlab='Date',ylab='Dow Jones',xaxt='n') axis(1,seq(1,85,12),dji.data[seq(1,85,12),1]) lines(dji.smooth1) 
```

![](images/3a3c9e931eaa743f406e3dea133f905ce56c715b7dbf1bb45e45de77665ee68e.jpg)

For the -rst-order exponential smoothing, measures of accuracy such as MAPE, MAD, and MSD can be obtained from the following function:

```r
measacc.fs<- function(y, lambda){ out<- firstsmooth(y, lambda) T<-length(y) #Smoothed version of the original is the one step ahead prediction #Hence the predictions (forecasts) are given as pred<-c(y[1],out[1:(T-1)]) prederr<- y-pred SSE<-sum(prederr^2) MAPE<-100\*sum(abs(prederr/y))/T MAD<-sum(abs(prederr))/T MSD<-sum(prederr^2)/T ret1<-c(SSE,MAPE,MAD,MSD) names(ret1)<-c("SSE","MAPE","MAD","MSD") return(ret1)   
} 
```

```txt
measacc.fs(dji.data[,2],0.4) 
```

SSE

MAPE

MAD

MSD

```python
1.665968e+07 3.461342e+00 3.356325e+02 1.959962e+05 
```

Note that alternatively we could use the Holt鈥揥inters function from the stats package. The function requires three parameters (alpha, beta, and gamma) to be de-ned. Providing a speci-c value for alpha and setting beta and gamma to 鈥淔ALSE鈥?give the -rst-order exponential as the following

dji1.fit<-HoltWinters(dji.data[,2],alpha ${ } = { }$ .4, beta FALSE, gamm $\cdot ^ { = }$ FALSE)

Beta corresponds to the second-order smoothing (or the trend term) and gamma is for the seasonal effect.

Example 4.2 The US CPI data are in the second column of the array called cpi.data in which the -rst column is the month of the year. For this case we use the -rstsmooth function twice to obtain the double exponential smoothing as

```txt
cpi.smooth1<-firstsmooth(y=cpi.data[,2],lambda=0.3)  
cpi.smooth2<-firstsmooth(y=cpi.smooth1, lambda=0.3)  
cpi.hat<-2*cpi.smooth1-cpi.smooth2 #Equation 4.23  
plot(cpi.data[,2], type="p", pch=16, cex=.5, xlab='Date', ylab='CPI', xaxt='n')  
axis(1, seq(1,120,24), cpi.data[seq(1,120,24),1])  
lines(cpi.hat) 
```

![](images/88d56d84d79ab65687fdbaf96edf250c55c9906b136b29f5cad831c4da0e7d51.jpg)

Note that the -tted values are obtained using Eq. (4.23). Also the corresponding command using Holt鈥揥inters function is

HoltWinters(cpi.data[,2],alpha=0.3, beta=0.3, gamma $=$ FALSE)

Example 4.3 In this example we use the -rstsmooth function twice for the Dow Jones Index data to obtain the double exponential smoothing as in the previous example.

```python
dji.smooth1<-firstsmooth(y=dji.data[,2],lambda=0.3)  
dji.smooth2<-firstsmooth(y=dji.smooth1, lambda=0.3)  
dji.hat<-2*dji.smooth1-dji.smooth2 #Equation 4.23  
plot(dji.data[,2], type="p", pch=16, cex=.5, xlab='Date', ylab='Dow Jones', xaxt='n')  
axis(1, seq(1,85,12), cpi.data[seq(1,85,12),1])  
lines(dji.hat) 
```

![](images/e85517bde13f491ce8ffd66e78cee6d4756f9d05c06e66b6db4bfba400b158f1.jpg)

Example 4.4 The average speed data are in the second column of the array called speed.data in which the -rst column is the index for the week. To -nd the 鈥渂est鈥?smoothing constant, we will use the -rstsmooth function for various lambda values and obtain the sum of squared one-step-ahead prediction error $( S S _ { E } )$ for each. The lambda value that minimizes the sum of squared prediction errors is deemed the 鈥渂est鈥?lambda. The obvious option is to apply -rstsmooth function in a for loop to obtain $S S _ { E }$ for various lambda values. Even though in this case this may not be an issue, in many cases for loops can slow down the computations in R and are to be avoided if possible. We will do that using sapply function.

```r
lambda vec<-seq(0.1, 0.9, 0.1)  
sse.speed<-function(sc){measacc.fs鐨勯€熷害.data[1:78,2],sc)[1]}  
sse vec<-sapply(lambda vec, SSE.speed)  
opt.lambda<-lambda vec[sse vec == min(sse vec)]  
plot(lambda.va, SSE.va, type="b", main = "SSE vs. lambda\n", xlab='lambda\n', ylab='SSE')  
abline(v=opt.lambda, col = 'red')  
mtext(text = paste("SSE min = ", round(min(sse.va), 2), "\n lambda = ", opt.lambda)) 
```

# SSE vs. lambda

SSE $\operatorname* { m i n } = 1 1 6 . 6 9$

lambda $= 0 . 4$

![](images/ea3687f0b6c2c0d868a152138cc37d60c3709f61cce3d4a6df4b6930ae3e23c9.jpg)

Note that we can also use Holt鈥揥inters function to -nd the 鈥渂est鈥?value for the smoothing constant by not specifying the appropriate parameter as the following:

HoltWinters(speed.data[,2], beta $. =$ FALSE, gamma ${ } . = { }$ FALSE)

Example 4.5 We will -rst try to -nd the best lambda for the CPI data using -rst-order exponential smoothing. We will also plot ACF of the data. Note that we will use the data up to December 2003.

lambda.va<-c(seq(0.1,0.9,0.1)锛?95锛?99)   
sse.cpi<-function(sc){measacc.fs(cpi.data[1:108,2],sc)[1]}   
sse.va<-sapply( lambda.va,sse.cpi)   
opt.lambda<-lambda.va[sse.va==min(sse.va)]   
plot( lambda.va,sse.va,type $= \mathrm{"b"}$ 锛宮ain $=$ "SSE vs. lambda\n", xlab $=$ 'lambda\n'锛寉lab $=$ 'SSE'锛宲ch $= 16$ ,cex $= .5$ 锛?  
acf(cpi.data[1:108,2]锛宭ag.max $= 25$ 锛?
# SSE vs. lambda

![](images/3a05ff305e5a0e317f396b80f1993b7874afff67c5292fa4d7d8a384ebdb1342.jpg)

![](images/c7aa55a32286f6591f0fd46424e079af23098d7ab39ea235bcca58e26064eb51.jpg)

We now use the second-order exponential smoothing with lambda of 0.3. We calculate the forecasts using Eq. (4.31) for the two options suggested in the Example 4.5.

Option 1: On December 2003, make the forecasts for the entire 2004 (1- to 12-step-ahead forecasts).

```r
lcpi<-0.3  
cpi.smooth1<-firstsmooth(y=cpi.data[1:108,2], lambda=1cpi)  
cpi.smooth2<-firstsmooth(y=cpi.smooth1, lambda=1cpi)  
cpi.hat<-2*cpi.smooth1-cpi.smooth2  
tau<-1:12  
T<-length(cpi.smooth1)  
cpi_forecast<- (2+tau*(1cpi/(1-lcpi)))*cpi.smooth1[T]-(1+tau*(1cpi/ (1-lcpi)))*cpi.smooth2[T]  
ctau<-sqrt(1+(1cpi/((2-1cpi)^3))*(10-14*1cpi+5*(1cpi^2)+2*tau*1cpi *(4-3*1cpi)+2*(tau^2)*(1cpi^2)))  
alpha.lev<- .05  
sig.est<- sqrt(var(cpi.data[2:108,2]- cpi.hat[1:107]))  
cl<-qnorm(1-alpha.lev/2)*(ctau/ctau[1])*ig.est  
plot(cpi.data[1:108,2],type="p",pch=16,cex=.5,xlab='Date', ylab='CPI',xaxt='n',xlim=c(1,120),ylim=c(150,192))  
axis(1,seq(1,120,24),cpi.data[seq(1,120,24),1])  
points(109:120,cpi.data[109:120,2])  
lines(109:120,cpi_forecast)  
lines(109:120,cpi_forecast+cl)  
lines(109:120,cpi_forecast-cl) 
```

![](images/2435961a964e4b6c36013c33016731493bb12a2d5b089fa5aa43cda94381e310.jpg)

Option 2: On December 2003, make the forecast for January 2004. Then when January 2004 data are available, make the forecast for February 2004 (only one-step-ahead forecasts).

```txt
lcpi<-0.3   
T<-108   
tau<-12   
alpha.lev<- .05   
cpi_forecast<-rep(0,tau)   
cl<-rep(0,tau)   
cpi.smooth1<-rep(0,T+tau)   
cpi.smooth2<-rep(0,T+tau)   
for (i in 1:tau) { cpi.smooth1[1:(T+i-1)]<-firstsmooth(y=cpi.data[1:(T+i-1),2], lambda=1cpi) cpi.smooth2[1:(T+i-1)]<-firstsmooth(y=cpi.smooth1[1:(T+i-1)], lambda=1cpi) cpi_forecast[i] <- (2+(1cpi/(1-lcpi)))*cpi.smooth1[T+i-1]-(1+(1cpi/(1-lcpi)))*cpi.smooth2[T+i-1] cpi.hat<-2*cpi.smooth1[1:(T+i-1)]-cpi.smooth2[1:(T+i-1)] sig.est<-sqrt(var(cpi.data[2:(T+i-1),2]-cpi.hat[1:(T+i-2)])) cl[i]<-qnorm(1-alpha.lev/2)*sig.est }   
plot(cpi.data[1:T,2],type="p",pch=16,cex=.5,xlab='Date',ylab='CPI', xaxt='n',xlim=c(1,T+tau),ylim=c(150,192)) axis(1,seq(1,T+tau,24),cpi.data[seq(1,T+tau,24),1]) points((T+1): (T+tau),cpi.data[(T+1): (T+tau),2], cex=.5) lines((T+1): (T+tau),cpi_forecast) lines((T+1): (T+tau),cpi_forecast+cl) lines((T+1): (T+tau),cpi_forecast-cl) 
```

![](images/524fbc99db70318d0ad8e1480f79714e2f9b239bf799745decc217f8f397127a.jpg)

# Example 4.6 The function for the Trigg鈥揕each smoother is given as:

```r
tlsmooth<-function(y,gamma,y.tilde.start=y[1],lambda.start=1){ T<-length(y)   
#Initialize the vectors Qt<-vector() Dt<-vector() y.tilde<-vector() lambda<-vector() err<-vector()   
#Set the starting values for the vectors lambda[1]=lambda.start y.tilde[1]=y.tilde.start Qt[1]=-0 Dt[1]=-0 err[1]=-0 for (i in 2:T) { err[i]=-y[i]-y.tilde[i-1] Qt[i]=-gamma*err[i]+(1-gamma)*Qt[i-1] Dt[i]=-gamma*abs(err[i])+(1-gamma)*Dt[i-1] lambda[i]=-abs(Qt[i]/Dt[i]) y.tilde[i]=lambda[i]*y[i]+(1-lambda[i])*y.tilde[i-1] } return(cbind(y.tilde,lambda,err,Qt,Dt)) }   
#Obtain the TL smoother for Dow Jones Index out.tl.dji<-tlsmooth(dji.data[,2],0.3) 
```

![](images/f041b60ef75a04619c0df47ab6962093b66d499255b19c9f62464ca1db571c33.jpg)

```txt
Obtain the exponential smoother for Dow Jones Index  
dji.smooth1<-firstsmooth(y=dji.data[,2], lambda=0.4)  
#Plot the data together with TL and exponential smoother for comparison  
plot(dji.data[,2], type="p", pch=16, cex=.5, xlab='Date', ylab='Dow Jones', xaxt='n')  
axis(1, seq(1,85,12), cpi.data[seq(1,85,12), 1])  
lines(out.tl.dji[,1])  
lines(dji.smooth1, col="grey40")  
legend(60,8000, c("Dow Jones","TL Smoother","Exponential Smoother"), pch=c(16, NA, NA), lwd=c(NA, .5, .5), cex=.55, col=c("black", "black", "grey40")) 
```

Example 4.7 The clothing sales data are in the second column of the array called closales.data in which the -rst column is the month of the year. We will use the data up to December 2002 to -t the model and make forecasts for the coming year (2003). We will use Holt鈥揥inters function given in stats package. The model is additive seasonal model with all parameters equal to 0.2.

```txt
dat.ts = ts(closales.data[,2], start = c(1992,1), freq = 12)  
y1<-closales.data[1:132,]  
# convert data to ts object  
y1.ts<-ts(y1[,2], start = c(1992,1), freq = 12)  
clo.hwl<-HoltWinters(y1.ts,alpha=0.2,beta=0.2,gamma=0.2,seasonal="additive")  
plot(y1.ts,type="p", pch=16,cex=.5,xlab='Date', ylab='Sales')  
lines(clo.hwl\$fitted[,1]) 
```

![](images/e3170c3a16652aceb23ed5460cd5c184f70a90c5b81a51034746dfb611d97ee3.jpg)

Forecast the the sales for 2003   
y2<-closales.data[133:144,]   
y2.ts<-ts(y2[,2],start=c(2003,1),freq=12)   
y2 forecast<-predict(clo.hw1锛宯.ahoad=12锛宲rediction.interval =TRUE)   
plot(y1.ps,type $=$ "p",pch=16,cex=.5,xlab $\equiv$ 'Date'锛寉lab $\equiv$ 'Sales', xlim=c(1992,2004))   
points(y2.ps)   
lines(y2.forecast[,1])   
lines(y2.forecast[,2])   
lines(y2.forecast[,3])

![](images/6808d99459bcb4765b5645a064c4f4c68b387bc968a77f3e9f5fb747c8ffb3c9.jpg)

Example 4.8 The liquor store sales data are in the second column of the array called liqsales.data in which the -rst column is the month of the year. We will -rst -t additive and multiplicative seasonal models to the entire data to see the difference in the -ts. Then we will use the data up to December 2003 to -t the multiplicative model and make forecasts for the coming year (2004). We will once again use Holt鈥揥inters function given in stats package. In all cases we set all parameters to 0.2.

```lua
y.ts<- ts(liqsales.data[,2], start = c(1992,1), freq = 12)  
liq.hw.add<-HoltWinters(y.ts,alpha=0.2,beta=0.2,gamma=0.2, seasonal="additive")  
plot(y.ts,type="p", pch=16,cex=.5,xlab='Date', ylab='Sales', main="Additive Model")  
lines(liq.hw.add$fitted[,1]) 
```

![](images/ae5485c71550fde459e1c4c02b16c93b79aeab6122cdb2cc7f84abd4d22a0b74.jpg)  
liq.hw.mult<-HoltWinters(y.ts,alpha=0.2,beta=0.2,gamma=0.2, seasonal $=$ "multiplicative") plot(y.ts,type $=$ "p", pch $_ { . = 1 6 }$ ,cex $\ l =$ .5,xlab $^ { \ast = }$ 'Date',ylab $^ { 1 = }$ 'Sales', main $. =$ "Multiplicative Model") lines(liq.hw.mult$fitted[,1])

![](images/cf9b9e062daccf5de33f3d796f10af6ce9b4ed5f1112984c91ba00b1bb804dd8.jpg)

```r
y1<-liqsales.data[1:144,]
y1.ts<-ts(y1[,2], start = c(1992,1), freq = 12)
liq.hwl<-HoltWinters(y1.ts,alpha=0.2,beta=0.2,gamma=0.2,
		seasonal="multiplicative")
y2<-liqsales.data[145:156,]
y2.ts<-ts(y2[,2],start=c(2004,1),freq=12)
y2_forecast<-predict(liq.hwl, n.ahead=12, prediction.interval =
TRUE)
plot(y1.ts,type="p", pch=16,cex=.5,xlab='Date',ylab='Sales',
xlim=c(1992,2005))
points(y2.ts)
lines(y2_forecast[,1])
lines(y2_forecast[,2])
lines(y2_forecast[,3]) 
```

![](images/6b9ab3d64fcad9a8ea51cd36df4f0eed00004ad3a8829d9edc2fe8805cb86e3d.jpg)

# EXERCISES

4.1 Consider the time series data shown in Table E4.1.

a. Make a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 2$ to smooth the -rst 40 time periods of this data. How well does this smoothing procedure work?   
c. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors.

TABLE E4.1 Data for Exercise 4.1   

<table><tr><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td></tr><tr><td>1</td><td>48.7</td><td>11</td><td>49.1</td><td>21</td><td>45.3</td><td>31</td><td>50.8</td><td>41</td><td>47.9</td></tr><tr><td>2</td><td>45.8</td><td>12</td><td>46.7</td><td>22</td><td>43.3</td><td>32</td><td>46.4</td><td>42</td><td>49.5</td></tr><tr><td>3</td><td>46.4</td><td>13</td><td>47.8</td><td>23</td><td>44.6</td><td>33</td><td>52.3</td><td>43</td><td>44.0</td></tr><tr><td>4</td><td>46.2</td><td>14</td><td>45.8</td><td>24</td><td>47.1</td><td>34</td><td>50.5</td><td>44</td><td>53.8</td></tr><tr><td>5</td><td>44.0</td><td>15</td><td>45.5</td><td>25</td><td>53.4</td><td>35</td><td>53.4</td><td>45</td><td>52.5</td></tr><tr><td>6</td><td>53.8</td><td>16</td><td>49.2</td><td>26</td><td>44.9</td><td>36</td><td>53.9</td><td>46</td><td>52.0</td></tr><tr><td>7</td><td>47.6</td><td>17</td><td>54.8</td><td>27</td><td>50.5</td><td>37</td><td>52.3</td><td>47</td><td>50.6</td></tr><tr><td>8</td><td>47.0</td><td>18</td><td>44.7</td><td>28</td><td>48.1</td><td>38</td><td>53.0</td><td>48</td><td>48.7</td></tr><tr><td>9</td><td>47.6</td><td>19</td><td>51.1</td><td>29</td><td>45.4</td><td>39</td><td>48.6</td><td>49</td><td>51.4</td></tr><tr><td>10</td><td>51.1</td><td>20</td><td>47.3</td><td>30</td><td>51.6</td><td>40</td><td>52.4</td><td>50</td><td>47.7</td></tr></table>

4.2 Reconsider the time series data shown in Table E4.1.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the -rst 40 time periods of this data (you can -nd the optimum value from Minitab). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.1.   
b. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors. Compare these forecast errors with those from Exercise 4.1. How much has using the optimum value of the smoothing constant improved the forecasts?

4.3 Find the sample ACF for the time series in Table E4.1. Does this give you any insight about the optimum value of the smoothing constant that you found in Exercise 4.2?

4.4 Consider the time series data shown in Table E4.2.

a. Make a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 2$ to smooth the -rst 40 time periods of this data. How well does this smoothing procedure work?   
c. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors.

4.5 Reconsider the time series data shown in Table E4.2.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the -rst 40 time periods of this data (you can -nd the optimum value from Minitab). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.4.

TABLE E4.2 Data for Exercise 4.4   

<table><tr><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td></tr><tr><td>1</td><td>43.1</td><td>11</td><td>41.8</td><td>21</td><td>47.7</td><td>31</td><td>52.9</td><td>41</td><td>48.3</td></tr><tr><td>2</td><td>43.7</td><td>12</td><td>50.7</td><td>22</td><td>51.1</td><td>32</td><td>47.3</td><td>42</td><td>45.0</td></tr><tr><td>3</td><td>45.3</td><td>13</td><td>55.8</td><td>23</td><td>67.1</td><td>33</td><td>50.0</td><td>43</td><td>55.2</td></tr><tr><td>4</td><td>47.3</td><td>14</td><td>48.7</td><td>24</td><td>47.2</td><td>34</td><td>56.7</td><td>44</td><td>63.7</td></tr><tr><td>5</td><td>50.6</td><td>15</td><td>48.2</td><td>25</td><td>50.4</td><td>35</td><td>42.3</td><td>45</td><td>64.4</td></tr><tr><td>6</td><td>54.0</td><td>16</td><td>46.9</td><td>26</td><td>44.2</td><td>36</td><td>52.0</td><td>46</td><td>66.8</td></tr><tr><td>7</td><td>46.2</td><td>17</td><td>47.4</td><td>27</td><td>52.0</td><td>37</td><td>48.6</td><td>47</td><td>63.3</td></tr><tr><td>8</td><td>49.3</td><td>18</td><td>49.2</td><td>28</td><td>35.5</td><td>38</td><td>51.5</td><td>48</td><td>60.0</td></tr><tr><td>9</td><td>53.9</td><td>19</td><td>50.9</td><td>29</td><td>48.4</td><td>39</td><td>49.5</td><td>49</td><td>60.9</td></tr><tr><td>10</td><td>42.5</td><td>20</td><td>55.3</td><td>30</td><td>55.4</td><td>40</td><td>51.4</td><td>50</td><td>56.1</td></tr></table>

b. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors. Compare these forecast errors with those from Exercise 4.4. How much has using the optimum value of the smoothing constant improved the forecasts?

4.6 Find the sample ACF for the time series in Table E4.2. Does this give you any insight about the optimum value of the smoothing constant that you found in Exercise 4.5?   
4.7 Consider the time series data shown in Table E4.3.

a. Make a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 1$ to smooth the -rst 30 time periods of this data. How well does this smoothing procedure work?

TABLE E4.3 Data for Exercise 4.7   

<table><tr><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td></tr><tr><td>1</td><td>275</td><td>11</td><td>297</td><td>21</td><td>231</td><td>31</td><td>255</td><td>41</td><td>293</td></tr><tr><td>2</td><td>245</td><td>12</td><td>235</td><td>22</td><td>238</td><td>32</td><td>255</td><td>42</td><td>284</td></tr><tr><td>3</td><td>222</td><td>13</td><td>237</td><td>23</td><td>251</td><td>33</td><td>229</td><td>43</td><td>276</td></tr><tr><td>4</td><td>169</td><td>14</td><td>203</td><td>24</td><td>253</td><td>34</td><td>286</td><td>44</td><td>290</td></tr><tr><td>5</td><td>236</td><td>15</td><td>238</td><td>25</td><td>283</td><td>35</td><td>236</td><td>45</td><td>250</td></tr><tr><td>6</td><td>259</td><td>16</td><td>232</td><td>26</td><td>283</td><td>36</td><td>194</td><td>46</td><td>235</td></tr><tr><td>7</td><td>268</td><td>17</td><td>206</td><td>27</td><td>245</td><td>37</td><td>228</td><td>47</td><td>275</td></tr><tr><td>8</td><td>225</td><td>18</td><td>295</td><td>28</td><td>234</td><td>38</td><td>244</td><td>48</td><td>350</td></tr><tr><td>9</td><td>246</td><td>19</td><td>247</td><td>29</td><td>273</td><td>39</td><td>241</td><td>49</td><td>290</td></tr><tr><td>10</td><td>263</td><td>20</td><td>227</td><td>30</td><td>293</td><td>40</td><td>284</td><td>50</td><td>269</td></tr></table>

c. Make one-step-ahead forecasts of the last 20 observations. Determine the forecast errors.   
d. Plot the forecast errors on a control chart for individuals. Use a moving range chart to estimate the standard deviation of the forecast errors in constructing this chart. What conclusions can you draw about the forecasting procedure and the time series?

4.8 The data in Table E4.4 exhibit a linear trend.

a. Verify that there is a trend by plotting the data.   
b. Using the -rst 12 observations, develop an appropriate procedure for forecasting.   
c. Forecast the last 12 observations and calculate the forecast errors. Does the forecasting procedure seem to be working satisfactorily?

TABLE E4.4 Data for Exercise 4.8   

<table><tr><td>Period</td><td>yt</td><td>Period</td><td>yt</td></tr><tr><td>1</td><td>315</td><td>13</td><td>460</td></tr><tr><td>2</td><td>195</td><td>14</td><td>395</td></tr><tr><td>3</td><td>310</td><td>15</td><td>390</td></tr><tr><td>4</td><td>316</td><td>16</td><td>450</td></tr><tr><td>5</td><td>325</td><td>17</td><td>458</td></tr><tr><td>6</td><td>335</td><td>18</td><td>570</td></tr><tr><td>7</td><td>318</td><td>19</td><td>520</td></tr><tr><td>8</td><td>355</td><td>20</td><td>400</td></tr><tr><td>9</td><td>420</td><td>21</td><td>420</td></tr><tr><td>10</td><td>410</td><td>22</td><td>580</td></tr><tr><td>11</td><td>485</td><td>23</td><td>475</td></tr><tr><td>12</td><td>420</td><td>24</td><td>560</td></tr></table>

4.9 Reconsider the linear trend data in Table E4.4. Take the -rst difference of this data and plot the time series of -rst differences. Has differencing removed the trend? Use exponential smoothing on the -rst 11 differences. Instead of forecasting the original data, forecast the -rst differences for the remaining data using exponential smoothing and use these forecasts of the -rst differences to obtain forecasts for the original data.

4.10 Table B.1 in Appendix B contains data on the market yield on US Treasury Securities at 10-year constant maturity.

a. Make a time series plot of the data.

b. Use simple exponential smoothing with $\lambda = 0 . 2$ to smooth the data, excluding the last 20 observations. How well does this smoothing procedure work?   
c. Make one-step-ahead forecasts of the last 20 observations. Determine the forecast errors.

4.11 Reconsider the US Treasury Securities data shown in Table B.1.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data, excluding the last 20 observations (you can -nd the optimum value from Minitab). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.10.   
b. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors. Compare these forecast errors with those from Exercise 4.10. How much has using the optimum value of the smoothing constant improved the forecasts?

4.12 Table B.2 contains data on pharmaceutical product sales.

a. Make a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 1$ to smooth this data. How well does this smoothing procedure work?   
c. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors.

4.13 Reconsider the pharmaceutical sales data shown in Table B.2.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data (you can -nd the optimum value from either Minitab or JMP). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.12.   
b. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors. Compare these forecast errors with those from Exercise 4.12. How much has using the optimum value of the smoothing constant improved the forecasts?   
c. Construct the sample ACF for these data. Does this give you any insight regarding the optimum value of the smoothing constant?

4.14 Table B.3 contains data on chemical process viscosity.

a. Make a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 1$ to smooth this data. How well does this smoothing procedure work?

c. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors.

4.15 Reconsider the chemical process data shown in Table B.3.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data (you can -nd the optimum value from either Minitab or JMP). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.14.   
b. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors. Compare these forecast errors with those from Exercise 4.14.How much has using the optimum value of the smoothing constant improved the forecasts?   
c. Construct the sample ACF for these data. Does this give you any insight regarding the optimum value of the smoothing constant?

4.16 Table B.4 contains data on the annual US production of blue and gorgonzola cheeses. This data have a strong trend.

a. Verify that there is a trend by plotting the data.   
b. Develop an appropriate exponential smoothing procedure for forecasting.   
c. Forecast the last 10 observations and calculate the forecast errors. Does the forecasting procedure seem to be working satisfactorily?

4.17 Reconsider the blue and gorgonzola cheese data in Table B.4 and Exercise 4.16. Take the -rst difference of this data and plot the time series of -rst differences. Has differencing removed the trend? Use exponential smoothing on the -rst differences. Instead of forecasting the original data, develop a procedure for forecasting the -rst differences and explain how you would use these forecasts of the -rst differences to obtain forecasts for the original data.   
4.18 Table B.5 shows data for US beverage manufacturer product shipments. Develop an appropriate exponential smoothing procedure for forecasting these data.   
4.19 Table B.6 contains data on the global mean surface air temperature anomaly.

a. Make a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 2$ to smooth the data. How well does this smoothing procedure work? Do you think this would be a reliable forecasting procedure?

4.20 Reconsider the global mean surface air temperature anomaly data shown in Table B.6 and used in Exercise 4.19.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data (you can -nd the optimum value from either Minitab or JMP). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.19.   
b. Do you think using the optimum value of the smoothing constant would result in improved forecasts from exponential smoothing?   
c. Take the -rst difference of this data and plot the time series of -rst differences. Use exponential smoothing on the -rst differences. Instead of forecasting the original data, develop a procedure for forecasting the -rst differences and explain how you would use these forecasts of the -rst differences to obtain forecasts for the original global mean surface air temperature anomaly.

4.21 Table B.7 contains daily closing stock prices for the Whole Foods Market.

a. Make a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 1$ to smooth the data. How well does this smoothing procedure work? Do you think this would be a reliable forecasting procedure?

4.22 Reconsider the Whole Foods Market data shown in Table B.7 and used in Exercise 4.21.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data (you can -nd the optimum value from either Minitab or JMP). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.21.   
b. Do you think that using the optimum value of the smoothing constant would result in improved forecasts from exponential smoothing?   
c. Use an exponential smoothing procedure for trends on this data. Is this an apparent improvement over the use of simple exponential smoothing with the optimum smoothing constant?   
d. Take the -rst difference of this data and plot the time series of -rst differences. Use exponential smoothing on the -rst differences. Instead of forecasting the original data, develop a procedure for forecasting the -rst differences and explain how you would use these forecasts of the -rst differences to obtain forecasts for the stock price.

4.23 Unemployment rate data are given in Table B.8.

a. Make a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 2$ to smooth the data. How well does this smoothing procedure work? Do you think that simple exponential smoothing should be used to forecast this data?

4.24 Reconsider the unemployment rate data shown in Table B.8 and used in Exercise 4.23.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data (you can -nd the optimum value from either Minitab or JMP). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.23.   
b. Do you think that using the optimum value of the smoothing constant would result in improved forecasts from exponential smoothing?   
c. Use an exponential smoothing procedure for trends on this data. Is this an apparent improvement over the use of simple exponential smoothing with the optimum smoothing constant?   
d. Take the -rst difference of this data and plot the time series of -rst differences. Use exponential smoothing on the -rst differences. Is this a reasonable procedure for forecasting the -rst differences?

4.25 Table B.9 contains yearly data on the international sunspot numbers.

a. Construct a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 1$ to smooth the data. How well does this smoothing procedure work? Do you think that simple exponential smoothing should be used to forecast this data?

4.26 Reconsider the sunspot data shown in Table B.9 and used in Exercise 4.25.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data (you can -nd the optimum value from either Minitab or JMP). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.25.   
b. Do you think that using the optimum value of the smoothing constant would result in improved forecasts from exponential smoothing?

c. Use an exponential smoothing procedure for trends on this data. Is this an apparent improvement over the use of simple exponential smoothing with the optimum smoothing constant?

4.27 Table B.10 contains 7 years of monthly data on the number of airline miles own in the United Kingdom. This is seasonal data.

a. Make a time series plot of the data and verify that it is seasonal.   
b. Use Winters鈥?multiplicative method for the -rst 6 years to develop a forecasting method for this data. How well does this smoothing procedure work?   
c. Make one-step-ahead forecasts of the last 12 months. Determine the forecast errors. How well did your procedure work in forecasting the new data?

4.28 Reconsider the airline mileage data in Table B.10 and used in Exercise 4.27.

a. Use the additive seasonal effects model for the -rst 6 years to develop a forecasting method for this data. How well does this smoothing procedure work?   
b. Make one-step-ahead forecasts of the last 12 months. Determine the forecast errors. How well did your procedure work in forecasting the new data?   
c. Compare these forecasts with those found using Winters鈥?multiplicative method in Exercise 4.27.

4.29 Table B.11 contains 8 years of monthly champagne sales data. This is seasonal data.

a. Make a time series plot of the data and verify that it is seasonal. Why do you think seasonality is present in these data?   
b. Use Winters鈥?multiplicative method for the -rst 7 years to develop a forecasting method for this data. How well does this smoothing procedure work?   
c. Make one-step-ahead forecasts of the last 12 months. Determine the forecast errors. How well did your procedure work in forecasting the new data?

4.30 Reconsider the monthly champagne sales data in Table B.11 and used in Exercise 4.29.

a. Use the additive seasonal effects model for the -rst 7 years to develop a forecasting method for this data. How well does this smoothing procedure work?

b. Make one-step-ahead forecasts of the last 12 months. Determine the forecast errors. How well did your procedure work in forecasting the new data?   
c. Compare these forecasts with those found using Winters鈥?multiplicative method in Exercise 4.29.

4.31 Montgomery et al. (1990) give 4 years of data on monthly demand for a soft drink. These data are given in Table E4.5.

a. Make a time series plot of the data and verify that it is seasonal. Why do you think seasonality is present in these data?   
b. Use Winters鈥?multiplicative method for the -rst 3 years to develop a forecasting method for this data. How well does this smoothing procedure work?   
c. Make one-step-ahead forecasts of the last 12 months. Determine the forecast errors. How well did your procedure work in forecasting the new data?

TABLE E4.5 Soft Drink Demand Data   

<table><tr><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td></tr><tr><td>1</td><td>143</td><td>13</td><td>189</td><td>25</td><td>359</td><td>37</td><td>332</td></tr><tr><td>2</td><td>191</td><td>14</td><td>326</td><td>26</td><td>264</td><td>38</td><td>244</td></tr><tr><td>3</td><td>195</td><td>15</td><td>289</td><td>27</td><td>315</td><td>39</td><td>320</td></tr><tr><td>4</td><td>225</td><td>16</td><td>293</td><td>28</td><td>362</td><td>40</td><td>437</td></tr><tr><td>5</td><td>175</td><td>17</td><td>279</td><td>29</td><td>414</td><td>41</td><td>544</td></tr><tr><td>6</td><td>389</td><td>18</td><td>552</td><td>30</td><td>647</td><td>42</td><td>830</td></tr><tr><td>7</td><td>454</td><td>19</td><td>674</td><td>31</td><td>836</td><td>43</td><td>1011</td></tr><tr><td>8</td><td>618</td><td>20</td><td>827</td><td>32</td><td>901</td><td>44</td><td>1081</td></tr><tr><td>9</td><td>770</td><td>21</td><td>1000</td><td>33</td><td>1104</td><td>45</td><td>1400</td></tr><tr><td>10</td><td>564</td><td>22</td><td>502</td><td>34</td><td>874</td><td>46</td><td>1123</td></tr><tr><td>11</td><td>327</td><td>23</td><td>512</td><td>35</td><td>683</td><td>47</td><td>713</td></tr><tr><td>12</td><td>235</td><td>24</td><td>300</td><td>36</td><td>352</td><td>48</td><td>487</td></tr></table>

4.32 Reconsider the soft drink demand data in Table E4.5 and used in Exercise 4.31.

a. Use the additive seasonal effects model for the -rst 3 years to develop a forecasting method for this data. How well does this smoothing procedure work?   
b. Make one-step-ahead forecasts of the last 12 months. Determine the forecast errors. How well did your procedure work in forecasting the new data?

c. Compare these forecasts with those found using Winters鈥?multiplicative method in Exercise 4.31.

4.33 Table B.12 presents data on the hourly yield from a chemical process and the operating temperature. Consider only the yield data in this exercise.

a. Construct a time series plot of the data.   
b. Use simple exponential smoothing with $\lambda = 0 . 2$ to smooth the data. How well does this smoothing procedure work? Do you think that simple exponential smoothing should be used to forecast this data?

4.34 Reconsider the chemical process yield data shown in Table B.12.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data (you can -nd the optimum value from either Minitab or JMP). How well does this smoothing procedure work? Compare the results with those obtained in Exercise 4.33.   
b. How much has using the optimum value of the smoothing constant improved the forecasts?

4.35 Find the sample ACF for the chemical process yield data in Table B.12. Does this give you any insight about the optimum value of the smoothing constant that you found in Exercise 4.34?   
4.36 Table B.13 presents data on ice cream and frozen yogurt sales. Develop an appropriate exponential smoothing forecasting procedure for this time series.   
4.37 Table B.14 presents the $\mathrm { C O } _ { 2 }$ readings from Mauna Loa.

a. Use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data (you can -nd the optimum value from either Minitab or JMP).   
b. Use simple exponential smoothing with $\lambda = 0 . 1$ to smooth the data. How well does this smoothing procedure work? Compare the results with those obtained using the optimum smoothing constant. How much has using the optimum value of the smoothing constant improved the exponential smoothing procedure?

4.38 Table B.15 presents data on the occurrence of violent crimes. Develop an appropriate exponential smoothing forecasting procedure for this time series.

4.39 Table B.16 presents data on the US. gross domestic product (GDP). Develop an appropriate exponential smoothing forecasting procedure for the GDP time series.   
4.40 Total annual energy consumption is shown in Table B.17. Develop an appropriate exponential smoothing forecasting procedure for the energy consumption time series.   
4.41 Table B.18 contains data on coal production. Develop an appropriate exponential smoothing forecasting procedure for the coal production time series.   
4.42 Table B.19 contains data on the number of children 0鈭? years old who drowned in Arizona.

a. Plot the data. What type of forecasting model seems appropriate?   
b. Develop a forecasting model for this data?

4.43 Data on tax refunds and population are shown in Table B.20. Develop an appropriate exponential smoothing forecasting procedure for the tax refund time series.   
4.44 Table B.21 contains data from the US Energy Information Administration on monthly average price of electricity for the residential sector in Arizona. This data have a strong seasonal component. Use the data from 2001鈥?010 to develop a multiplicative Winterstype exponential smoothing model for this data. Use this model to simulate one-month-ahead forecasts for the remaining years. Calculate the forecast errors. Discuss the reasonableness of the forecasts.   
4.45 Use the electricity price data in Table B.21 from 2010鈥?010 and an additive Winters-type exponential smoothing procedure to develop a forecasting model.

a. Use this model to simulate one-month-ahead forecasts for the remaining years. Calculate the forecast errors. Discuss the reasonableness of the forecasts.   
b. Compare the performance of this model with the multiplicative model you developed in Exercise 4.44.

4.46 Table B.22 contains data from the Danish Energy Agency on Danish crude oil production.

a. Plot the data and comment on any features that you observe from the graph. Calculate and plot the sample ACF and variogram. Interpret these graphs.

b. Use -rst-order exponential smoothing to develop a forecasting model for crude oil production. Plot the smoothed statistic on the same axes with the original data. How well does -rst-order exponential smoothing seem to work?   
c. Use double exponential smoothing to develop a forecasting model for crude oil production. Plot the smoothed statistic on the same axes with the original data. How well does double exponential smoothing seem to work?   
d. Compare the two smoothing models from parts b and c. Which approach seems preferable?

4.47 Apply a -rst difference to the Danish crude oil production data in Table B.22.

a. Plot the data and comment on any features that you observe from the graph. Calculate and plot the sample ACF and variogram. Interpret these graphs.   
b. Use -rst-order exponential smoothing to develop a forecasting model for crude oil production. Plot the smoothed statistic on the same axes with the original data. How well does -rst-order exponential smoothing seem to work? How does this compare to the -rst-order exponential smoothing model you developed in Exercise 4.46 for the original (undifferenced) data?

4.48 Table B.23 shows weekly data on positive laboratory test results for inuenza. Notice that these data have a number of missing values. In exercise you were asked to develop and implement a scheme to estimate the missing values. This data have a strong seasonal component. Use the data from 1997鈥?010 to develop a multiplicative Winters-type exponential smoothing model for this data. Use this model to simulate one-week-ahead forecasts for the remaining years. Calculate the forecast errors. Discuss the reasonableness of the forecasts.   
4.49 Repeat Exercise 4.48 using an additive Winters-type model. Compare the performance of the additive and the multiplicative model from Exercise 4.48.   
4.50 Data from the Western Regional Climate Center for the monthly mean daily solar radiation (in Langleys) at the Zion Canyon, Utah, station are shown in Table B.24. This data have a strong seasonal component. Use the data from 2003鈥?012 to develop a multiplicative Winters-type exponential smoothing model for this data. Use this

model to simulate one-month-ahead forecasts for the remaining years. Calculate the forecast errors. Discuss the reasonableness of the forecasts.

4.51 Repeat Exercise 4.50 using an additive Winters-type model. Compare the performance of the additive and the multiplicative model from Exercise 4.50.   
4.52 Table B.25 contains data from the National Highway Traf-c Safety Administration on motor vehicle fatalities from 1966 to 2012. This data are used by a variety of governmental and industry groups, as well as research organizations.

a. Plot the fatalities data and comment on any features of the data that you see.   
b. Develop a forecasting procedure using -rst-order exponential smoothing. Use the data from 1966鈥?006 to develop the model, and then simulate one-year-ahead forecasts for the remaining years. Compute the forecasts errors. How well does this method seem to work?   
c. Develop a forecasting procedure using based on double exponential smoothing. Use the data from 1966鈥?006 to develop the model, and then simulate one-year-ahead forecasts for the remaining years. Compute the forecasts errors. How well does this method seem to work in comparison to the method based on -rst-order exponential smoothing?

4.53 Apply a -rst difference to the motor vehicle fatalities data in Table B.25.

a. Plot the differenced data and comment on any features of the data that you see.   
b. Develop a forecasting procedure for the -rst differences based on -rst-order exponential smoothing. Use the data from 1966鈥?006 to develop the model, and then simulate one-year-ahead forecasts for the remaining years. Compute the forecasts errors. How well does this method seem to work?   
c. Compare this approach with the two smoothing methods used in Exercise 4.52.

4.54 Appendix Table B.26 contains data on monthly single-family residential new home sales from 1963 through 2014.

a. Plot the home sales data. Comment on the graph.

b. Develop a forecasting procedure using -rst-order exponential smoothing. Use the data from1963鈥?000 to develop the model, and then simulate one-year-ahead forecasts for the remaining years. Compute the forecasts errors. How well does this method seem to work?   
c. Can you explain the unusual changes in sales observed in the data near the end of the graph?

4.55 Appendix Table B.27 contains data on the airline鈥檚 best on-time arrival and airport performance. The data are given by month from January 1995 through February 2013.

a. Plot the data and comment on any features of the data that you see.   
b. Construct the sample ACF and variogram. Comment on these displays.   
c. Develop an appropriate exponential smoothing model for these data.

4.56 Data from the US Census Bureau on monthly domestic automobile manufacturing shipments (in millions of dollars) are shown in Table B.28.

a. Plot the data and comment on any features of the data that you see.   
b. Construct the sample ACF and variogram. Comment on these displays.   
c. Develop an appropriate exponential smoothing model for these data. Note that there is some apparent seasonality in the data. Why does this seasonal behavior occur?   
d. Plot the -rst difference of the data. Now compute the sample ACF and variogram for the differenced data. What impact has differencing had? Is there still some apparent seasonality in the differenced data?

4.57 Suppose that simple exponential smoothing is being used to forecast a process. At the start of period $t ^ { * }$ , the mean of the process shifts to a new level $\mu + \delta$ . The mean remains at this new level for subsequent time periods. Show that the expected value of the exponentially smoothed statistic is

$$
E (\hat {y} _ {t}) = \left\{ \begin{array}{l l} \mu , & T \leq t ^ {*} \\ \mu + \delta - \delta (1 - \lambda) ^ {T - t ^ {*} + 1}, & T \geq t ^ {*}. \end{array} \right.
$$

4.58 Using the results of Exercise 4.44, determine the number of periods following the step change for the expected value of the exponential smoothing statistic to be within $0 . 1 0 \delta$ of the new time series level $\mu + \delta$ . Plot the number of periods as a function of the smoothing constant. What conclusions can you draw?   
4.59 Suppose that simple exponential smoothing is being used to forecast the process $y _ { t } = \mu + \varepsilon _ { t }$ . At the start of period $t ^ { * }$ , the mean of the process experiences a transient; that is, it shifts to a new level $\mu + \delta$ , but reverts to its original level y at the start of the next period $t ^ { * } + 1$ . The mean remains at this level for subsequent time periods. Show that the expected value of the exponentially smoothed statistic is

$$
E (\hat {y} _ {t}) = \left\{ \begin{array}{l l} \mu , & T \leq t ^ {*} \\ \mu + \delta \lambda (1 - \lambda) ^ {T - t ^ {*}}, & T \geq t ^ {*}. \end{array} \right.
$$

4.60 Using the results of Exercise 4.46, determine the number of periods that it will take following the impulse for the expected value of the exponential smoothing statistic to return to within $0 . 1 0 \delta$ of the original time series level $\mu$ . Plot the number of periods as a function of the smoothing constant. What conclusions can you draw?

# AUTOREGRESSIVE INTEGRATED MOVING AVERAGE (ARIMA) MODELS

All models are wrong, some are useful.

GEORGE E. P. BOX, British statistician

# 5.1 INTRODUCTION

In the previous chapter, we discussed forecasting techniques that, in general, were based on some variant of exponential smoothing. The general assumption for these models was that any time series data can be represented as the sum of two distinct components: deterministic and stochastic (random). The former is modeled as a function of time whereas for the latter we assumed that some random noise that is added on to the deterministic signal generates the stochastic behavior of the time series. One very important assumption is that the random noise is generated through independent shocks to the process. In practice, however, this assumption is often violated. That is, usually successive observations show serial dependence. Under these circumstances, forecasting methods based on exponential smoothing may be inef-cient and sometimes inappropriate

because they do not take advantage of the serial dependence in the observations in the most effective way. To formally incorporate this dependent structure, in this chapter we will explore a general class of models called autoregressive integrated moving average (MA) models or ARIMA models (also known as Box鈥揓enkins models).

# 5.2 LINEAR MODELS FOR STATIONARY TIME SERIES

In statistical modeling, we are often engaged in an endless pursuit of -nding the ever elusive true relationship between certain inputs and the output. As cleverly put by the quote of this chapter, these efforts usually result in models that are nothing but approximations of the 鈥渢rue鈥?relationship. This is generally due to the choices the analyst makes along the way to ease the modeling efforts. A major assumption that often provides relief in modeling efforts is the linearity assumption. A linear -lter, for example, is a linear operation from one time series $x _ { t }$ to another time series $y _ { t }$ ,

$$
y _ {t} = L \left(x _ {t}\right) = \sum_ {i = - \infty} ^ {+ \infty} \psi_ {i} x _ {t - i} \tag {5.1}
$$

with $t = \ldots , - 1 , 0 , 1 , \ldots$ In that regard the linear -lter can be seen as a 鈥減rocess鈥?that converts the input, $x _ { t }$ , into an output, $y _ { t }$ , and that conversion is not instantaneous but involves all (present, past, and future) values of the input in the form of a summation with different 鈥渨eights鈥? $\left\{ \psi _ { i } \right\}$ , on each $x _ { t }$ . Furthermore, the linear -lter in Eq. (5.1) is said to have the following properties:

1. Time-invariant as the coef-cients $\left\{ \psi _ { i } \right\}$ do not depend on time.   
2. Physically realizable if $\psi _ { i } = 0$ for $i < 0$ ; that is, the output $y _ { t }$ is a linear function of the current and past values of the input: $y _ { t } =$ $\psi _ { 0 } x _ { t } + \psi _ { 1 } x _ { t - 1 } + \cdots$ .   
3. Stable if $\begin{array} { r } { \sum _ { i = - \infty } ^ { + \infty } \left| \psi _ { i } \right| < \infty } \end{array}$

In linear -lters, under certain conditions, some properties such as stationarity of the input time series are also reected in the output. We discussed stationarity previously in Chapter 2. We will now give a more formal description of it before proceeding further with linear models for time series.

# 5.2.1 Stationarity

The stationarity of a time series is related to its statistical properties in time. That is, in the more strict sense, a stationary time series exhibits similar 鈥渟tatistical behavior鈥?in time and this is often characterized as a constant probability distribution in time. However, it is usually satisfactory to consider the -rst two moments of the time series and de-ne stationarity (or weak stationarity) as follows: (1) the expected value of the time series does not depend on time and (2) the autocovariance function de-ned as $\mathrm { C o v } ( y _ { t } , y _ { t + k } )$ for any lag $k$ is only a function of $k$ and not time; that is, $\gamma _ { y } ( k ) = \mathrm { C o v } ( y _ { t } , y _ { t + k } )$ .

In a crude way, the stationarity of a time series can be determined by taking arbitrary 鈥渟napshots鈥?of the process at different points in time and observing the general behavior of the time series. If it exhibits 鈥渟imilar鈥?behavior, one can then proceed with the modeling efforts under the assumption of stationarity. Further preliminary tests also involve observing the behavior of the autocorrelation function. A strong and slowly dying ACF will also suggest deviations from stationarity. Better and more methodological tests of stationarity also exist and we will discuss some of them later in this chapter. Figure 5.1 shows examples of stationary and nonstationary time series data.

# 5.2.2 Stationary Time Series

For a time-invariant and stable linear -lter and a stationary input time series $x _ { t }$ with $\mu _ { x } = E ( x _ { t } )$ and $\gamma _ { x } \left( k \right) = \mathrm { C o v } ( x _ { t } , x _ { t + k } )$ , the output time series $y _ { t }$ given in Eq. (5.1) is also a stationary time series with

$$
E (y _ {t}) = \mu_ {y} = \sum_ {- \infty} ^ {\infty} \psi_ {i} \mu_ {x}
$$

and

$$
\operatorname {C o v} (y _ {t}, y _ {t + k}) = \gamma_ {y} (k) = \sum_ {i = - \infty} ^ {\infty} \sum_ {j = - \infty} ^ {\infty} \psi_ {i} \psi_ {j} \gamma_ {x} (i - j + k)
$$

It is then easy to show that the following stable linear process with white noise time series, $\varepsilon _ { t }$ , is also stationary:

$$
y _ {t} = \mu + \sum_ {i = 0} ^ {\infty} \psi_ {i} \varepsilon_ {t - i} \tag {5.2}
$$

![](images/ed7de1931231d14185c8044883f3b944e1aac770c4019588fafc971dc120c37f.jpg)

![](images/1c8aec4a9517f718cb37f0d0296aca50cfce0ee643cda241ba0db82d454e213b.jpg)  
$( \mathsf { a } ) y _ { 1 , t } = 1 0 + 0 . 7 5 y _ { 1 , t - 1 } + \varepsilon _ { t }$

![](images/35fd4cae60583c0c111806fcc9c6715030ee1010b7e33b432427845e77f85fc7.jpg)

![](images/aebffc3db67baf8c994d4f990f75c298774caee312b7bd63c84ff9368445f682.jpg)  
$( \mathsf { b } ) y _ { 2 , t } = 2 + 0 . 9 5 y _ { 2 , t - 1 } + \varepsilon _ { t }$

![](images/f7b6481b6483fb24fe6707df1fb61875af84bac956209d58885550658617a6e8.jpg)

![](images/33abcfefffacf604ae2a5f7732be5b78dbe60b6d20656294451c9eb8a0531239.jpg)  
$( \mathsf { c } ) y _ { 3 , t } = 2 0 + y _ { 3 , t - 1 } + \varepsilon _ { t }$   
FIGURE 5.1 Realizations of (a) stationary, (b) near nonstationary, and (c) nonstationary processes.

with $E ( \varepsilon _ { t } ) = 0$ , and

$$
\gamma_ {\varepsilon} (h) = \left\{ \begin{array}{l l} \sigma^ {2} & \mathrm {i f} h = 0 \\ 0 & \mathrm {i f} h \neq 0 \end{array} \right.
$$

So for the autocovariance function of $y _ { t }$ , we have

$$
\begin{array}{l} \gamma_ {y} (k) = \sum_ {i = 0} ^ {\infty} \sum_ {j = 0} ^ {\infty} \psi_ {i} \psi_ {j} \gamma_ {\varepsilon} (i - j + k) \tag {5.3} \\ = \sigma^ {2} \sum_ {i = 0} ^ {\infty} \psi_ {i} \psi_ {i + k} \\ \end{array}
$$

We can rewrite the linear process in Eq. (5.2) in terms of the backshift operator, $B$ , as

$$
\begin{array}{l} y _ {t} = \mu + \psi_ {0} \varepsilon_ {t} + \psi_ {1} \varepsilon_ {t - 1} + \psi_ {2} \varepsilon_ {t - 2} + \dots \\ = \mu + \sum_ {i = 0} ^ {\infty} \psi_ {i} B ^ {i} \varepsilon_ {t} \\ = \mu + \underbrace {\left(\sum_ {i = 0} ^ {\infty} \psi_ {i} B ^ {i}\right)} _ {= \Psi (\mathrm {B})} \varepsilon_ {t} \tag {5.4} \\ = \mu + \Psi (B) \varepsilon_ {t} \\ \end{array}
$$

This is called the in-nite moving average and serves as a general class of models for any stationary time series. This is due to a theorem by Wold (1938) and basically states that any nondeterministic weakly stationary time series $y _ { t }$ can be represented as in Eq. (5.2), where $\left\{ \psi _ { i } \right\}$ satisfy $\textstyle \sum _ { i = 0 } ^ { \infty } \psi _ { i } ^ { 2 } < \infty$ . A more intuitive interpretation of this theorem is that a stationary time series can be seen as the weighted sum of the present and past random 鈥渄isturbances.鈥?For further explanations see Yule (1927) and Bisgaard and Kulahci (2005, 2011).

The theorem by Wold requires that the random shocks in (5.4) to be white noise which we de-ned as uncorrelated random shocks with constant variance. Some textbooks discuss independent or strong white noise for random shocks. It should be noted that there is a difference between correlation and independence. Independent random variables are also uncorrelated but the opposite is not always true. Independence between two random variables refers their joint probability distribution function being equal to the product of the marginal distributions. That is, two random variables $X$ and Y are said to be independent if

$$
f (X, Y) = f _ {X} (X) f _ {Y} (Y)
$$

This can also be loosely interpreted as if $X$ and Y are independent, knowing the value of $X$ for example does not provide any information about what the value of Y might be.

For two uncorrelated random variables $X$ and $Y$ , we have their correlation and their covariance equal to zero. That is,

$$
\begin{array}{l} C o v (X, Y) = E \left[ \left(X - \mu_ {X}\right) \left(Y - \mu_ {Y}\right) \right] \\ = E [ X Y ] - E [ X ] E [ Y ] \\ = 0 \\ \end{array}
$$

This implies that if $X$ and $Y$ are uncorrelated, $E [ X Y ] = E [ X ] E [ Y ]$

Clearly if two random variables are independent, they are also uncorrelated since under independence we always have

$$
\begin{array}{l} E [ X Y ] = \iint x y f (x, y) d x d y \\ = \iint x y f (x) f (y) d x d y \\ = \left\{\int x f (x) d x \right\} \left\{\int y f (y) d x \right\} \\ = E [ X ] E [ Y ] \\ \end{array}
$$

As we mentioned earlier, the opposite is not always true. To illustrate this with an example, consider $X$ , a random variable with a symmetric probability density function around 0, i.e., $E [ X ] = 0$ . Assume that the second variable $Y$ is equal to $| X |$ . Since knowing the value of $X$ also determines the value of $Y$ , these two variables are clearly not independent. However we can show that $\begin{array} { r } { E [ Y ] = 2 \int _ { 0 } ^ { \infty } x f ( x ) d x } \end{array}$ and $\begin{array} { r } { E [ X Y ] = \int _ { 0 } ^ { \infty } x ^ { 2 } f ( x ) d x - \int _ { - \infty } ^ { 0 } x ^ { 2 } f ( x ) d x = 0 } \end{array}$ and hence $E [ X Y ] = E [ X ] E [ Y ]$ This shows that $X$ and $Y$ are uncorrected but not independent.

Wold鈥檚 decomposition theorem practically forms the foundation of the models we discuss in this chapter. This means that the strong assumption of independence is not necessarily needed except for the discussion on forecasting using ARIMA models in Section 5.8 where we assume the random shocks to be independent.

It can also be seen from Eq. (5.3) that there is a direct relation between the weights $\left\{ \psi _ { i } \right\}$ and the autocovariance function. In modeling a stationary time series as in Eq. (5.4), it is obviously impractical to attempt to estimate the in-nitely many weights given in $\left. { \psi _ { i } } \right.$ . Although very powerful in providing a general representation of any stationary time series, the in-nite moving average model given in Eq. (5.2) is useless in practice except for certain special cases:

1. Finite order moving average (MA) models where, except for a -nite number of the weights in $\left\{ \psi _ { i } \right\}$ , they are set to 0.   
2. Finite order autoregressive (AR) models, where the weights in $\left\{ \psi _ { i } \right\}$ are generated using only a -nite number of parameters.   
3. A mixture of -nite order autoregressive and moving average models (ARMA).

We shall now discuss each of these classes of models in great detail.

# 5.3 FINITE ORDER MOVING AVERAGE PROCESSES

In -nite order moving average or MA models, conventionally $\psi _ { 0 }$ is set to 1 and the weights that are not set to 0 are represented by the Greek letter $\theta$ with a minus sign in front. Hence a moving average process of order $q$ $\left( \mathrm { M A } ( q ) \right)$ is given as

$$
y _ {t} = \mu + \varepsilon_ {t} - \theta_ {1} \varepsilon_ {t - 1} - \dots - \theta_ {q} \varepsilon_ {t - q} \tag {5.5}
$$

where $\left\{ \varepsilon _ { t } \right\}$ is white noise. Since Eq. (5.5) is a special case of Eq. (5.4) with only -nite weights, an $\mathrm { M A } ( q )$ process is always stationary regardless of values of the weights. In terms of the backward shift operator, the $\mathrm { M A } ( q )$ process is

$$
\begin{array}{l} y _ {t} = \mu + \left(1 - \theta_ {1} B - \dots - \theta_ {q} B ^ {q}\right) \varepsilon_ {t} \\ = \mu + \left(1 - \sum_ {i = 1} ^ {q} \theta_ {i} B ^ {i}\right) \varepsilon_ {t} \tag {5.6} \\ = \mu + \Theta (B) \varepsilon_ {t} \\ \end{array}
$$

where $\begin{array} { r } { \Theta \left( B \right) = 1 - \sum _ { i = 1 } ^ { q } \theta _ { i } B ^ { i } } \end{array}$ .

Furthermore, since $\left\{ \varepsilon _ { t } \right\}$ is white noise, the expected value of the MA(q) process is simply

$$
\begin{array}{l} E \left(y _ {t}\right) = E \left(\mu + \varepsilon_ {t} - \theta_ {1} \varepsilon_ {t - 1} - \dots - \theta_ {q} \varepsilon_ {t - q}\right) \tag {5.7} \\ = \mu \\ \end{array}
$$

and its variance is

$$
\begin{array}{l} \operatorname {V a r} \left(y _ {t}\right) = \gamma_ {y} (0) = \operatorname {V a r} \left(\mu + \varepsilon_ {t} - \theta_ {1} \varepsilon_ {t - 1} - \dots - \theta_ {q} \varepsilon_ {t - q}\right) \tag {5.8} \\ = \sigma^ {2} \left(1 + \theta_ {1} ^ {2} + \dots + \theta_ {q} ^ {2}\right) \\ \end{array}
$$

Similarly, the autocovariance at lag $k$ can be calculated from

$$
\begin{array}{l} \gamma_ {y} (k) = \mathrm {C o v} (y _ {t}, y _ {t + k}) \\ = E \left[ \left(\varepsilon_ {t} - \theta_ {1} \varepsilon_ {t - 1} - \dots - \theta_ {q} \varepsilon_ {t - q}\right) \left(\varepsilon_ {t + k} - \theta_ {1} \varepsilon_ {t + k - 1} - \dots - \theta_ {q} \varepsilon_ {t + k - q}\right) \right] (5.9) \\ = \left\{ \begin{array}{l} \sigma^ {2} \left(- \theta_ {k} + \theta_ {1} \theta_ {k + 1} + \dots + \theta_ {q - k} \theta_ {q}\right), \quad k = 1, 2, \dots , q \\ 0, \quad k > q \end{array} \right. (5.9) \\ \end{array}
$$

From Eqs. (5.8) and (5.9), the autocorrelation function of the MA(q) process is

$$
\rho_ {y} (k) = \frac {\gamma_ {y} (k)}{\gamma_ {y} (0)} = \left\{ \begin{array}{l} - \theta_ {k} + \theta_ {1} \theta_ {k + 1} + \dots + \theta_ {q - k} \theta_ {q}, \\ 1 + \theta_ {1} ^ {2} + \dots + \theta_ {q} ^ {2}, \\ 0, \quad k > q \end{array} , \quad k = 1, 2, \dots , q \right. \tag {5.10}
$$

This feature of the ACF is very helpful in identifying the MA model and its appropriate order as it 鈥渃uts off鈥?after lag $q$ . In real life applications, however, the sample ACF, $r ( k )$ , will not necessarily be equal to zero after lag q. It is expected to become very small in absolute value after lag $q$ . For a data set of $N$ observations, this is often tested against $\pm 2 / \sqrt { N }$ limits, where $1 / \sqrt { N }$ is the approximate value for the standard deviation of the ACF for any lag under the assumption $\rho ( k ) = 0$ for all $k$ 鈥檚 as discussed in Chapter 2.

Note that a more accurate formula for the standard error of the kth sample autocorrelation coef-cient is provided by Bartlett (1946) as

$$
s. e. (r (k)) = N ^ {- 1 / 2} \left(1 + 2 \sum_ {j = 1} ^ {k - 1} r (j) ^ {* 2}\right) ^ {1 / 2}
$$

where

$$
r (j) ^ {*} = \left\{ \begin{array}{l l} r (j) & \text {f o r} \rho (j) \neq 0 \\ 0 & \text {f o r} \rho (j) = 0 \end{array} \right.
$$

A special case would be white noise data for which $\rho ( j ) = 0$ for all j鈥檚. Hence for a white noise process (i.e., no autocorrelation), a reasonable interval for the sample autocorrelation coef-cients to fall in would be $\pm 2 / \sqrt { N }$ and any indication otherwise may be considered as evidence for serial dependence in the process.

# 5.3.1 The First-Order Moving Average Process, MA(1)

The simplest -nite order MA model is obtained when $q = 1$ in Eq. (5.5):

$$
y _ {t} = \mu + \varepsilon_ {t} - \theta_ {1} \varepsilon_ {t - 1} \tag {5.11}
$$

For the -rst-order moving average or MA(1) model, we have the autocovariance function as

$$
\gamma_ {y} (0) = \sigma^ {2} \left(1 + \theta_ {1} ^ {2}\right)
$$

$$
\gamma_ {y} (1) = - \theta_ {1} \sigma^ {2} \tag {5.12}
$$

$$
\gamma_ {y} (k) = 0, \quad k > 1
$$

Similarly, we have the autocorrelation function as

$$
\rho_ {y} (1) = \frac {- \theta_ {1}}{1 + \theta_ {1} ^ {2}} \tag {5.13}
$$

$$
\rho_ {y} (k) = 0, \quad k > 1
$$

From Eq. (5.13), we can see that the -rst lag autocorrelation in MA(1) is bounded as

$$
\left| \rho_ {y} (1) \right| = \frac {\left| \theta_ {1} \right|}{1 + \theta_ {1} ^ {2}} \leq \frac {1}{2} \tag {5.14}
$$

and the autocorrelation function cuts off after lag 1.

Consider, for example, the following MA(1) model:

$$
y _ {t} = 4 0 + \varepsilon_ {t} + 0. 8 \varepsilon_ {t - 1}
$$

A realization of this model with its sample ACF is given in Figure 5.2. A visual inspection reveals that the mean and variance remain stable while there are some short runs where successive observations tend to follow each other for very brief durations, suggesting that there is indeed some positive autocorrelation in the data as revealed in the sample ACF plot.

![](images/901347cb2989b5b5543347fe23da29de93230836aec5cbe798e65688ce910b34.jpg)

![](images/0efefe6733eba9d289237dd01ba47aa65fc9198cc8b4564cba7654ba703397da.jpg)  
FIGURE 5.2 A realization of the MA(1) process, $y _ { t } = 4 0 + \varepsilon _ { t } + 0 . 8 \varepsilon _ { t - 1 }$

![](images/52549a740e00b861fcda4a692315a5a3c7dcc2869d377e46cb6c15929773e150.jpg)

![](images/287cb87ab6dd9dc94556c891c67eb1c48fbf2854d579ec82c54e1d8d7417fa77.jpg)  
FIGURE 5.3 A realization of the MA(1) process, $y _ { t } = 4 0 + \varepsilon _ { t } - 0 . 8 \varepsilon _ { t - 1 }$

We can also consider the following model:

$$
y _ {t} = 4 0 + \varepsilon_ {t} - 0. 8 \varepsilon_ {t - 1}
$$

A realization of this model is given in Figure 5.3. We can see that observations tend to oscillate successively. This suggests a negative autocorrelation as con-rmed by the sample ACF plot.

# 5.3.2 The Second-Order Moving Average Process, MA(2)

Another useful -nite order moving average process is MA(2), given as

$$
\begin{array}{l} y _ {t} = \mu + \varepsilon_ {t} - \theta_ {1} \varepsilon_ {t - 1} - \theta_ {2} \varepsilon_ {t - 2} \tag {5.15} \\ = \mu + \left(1 - \theta_ {1} B - \theta_ {2} B ^ {2}\right) \varepsilon_ {t} \\ \end{array}
$$

The autocovariance and autocorrelation functions for the MA(2) model are given as

$$
\gamma_ {y} (0) = \sigma^ {2} \left(1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2}\right)
$$

$$
\gamma_ {y} (1) = \sigma^ {2} \left(- \theta_ {1} + \theta_ {1} \theta_ {2}\right) \tag {5.16}
$$

$$
\gamma_ {y} (2) = \sigma^ {2} (- \theta_ {2})
$$

$$
\gamma_ {y} (k) = 0, \quad k > 2
$$

and

$$
\rho_ {y} (1) = \frac {- \theta_ {1} + \theta_ {1} \theta_ {2}}{1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2}}
$$

$$
\rho_ {y} (2) = \frac {- \theta_ {2}}{1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2}} \tag {5.17}
$$

$$
\rho_ {y} (k) = 0, \quad k > 2
$$

![](images/0afb7e271401614a53feed034130f2d5eec6ff5e25ac8d5936797de518a0c9c9.jpg)

![](images/bf1b35ae0ce5b82c0d6d033ef7a1241ed935856c2cef18c9f12b27434eb7b9a9.jpg)  
FIGURE 5.4 A realization of the MA(2) process, $y _ { t } = 4 0 + \varepsilon _ { t } + 0 . 7 \varepsilon _ { t - 1 } -$ 0.28饾渶t鈭?. $0 . 2 8 \varepsilon _ { t - 2 }$

Figure 5.4 shows the time series plot and the autocorrelation function for a realization of the MA(2) model:

$$
y _ {t} = 4 0 + \varepsilon_ {t} + 0. 7 \varepsilon_ {t - 1} - 0. 2 8 \varepsilon_ {t - 2}
$$

Note that the sample ACF cuts off after lag 2.

# 5.4 FINITE ORDER AUTOREGRESSIVE PROCESSES

As mentioned in Section 5.1, while it is quite powerful and important, Wold鈥檚 decomposition theorem does not help us much in our modeling and forecasting efforts as it implicitly requires the estimation of the in-nitely many weights, $\left\{ \psi _ { i } \right\}$ . In Section 5.2 we discussed a special case of this decomposition of the time series by assuming that it can be adequately modeled by only estimating a -nite number of weights and setting the rest equal to 0. Another interpretation of the -nite order MA processes is that at any given time, of the in-nitely many past disturbances, only a -nite number of those disturbances 鈥渃ontribute鈥?to the current value of the time series and that the time window of the contributors 鈥渕oves鈥?in time, making the 鈥渙ldest鈥?disturbance obsolete for the next observation. It is indeed not too far fetched to think that some processes might have these intrinsic dynamics. However, for some others, we may be required to consider the 鈥渓ingering鈥?contributions of the disturbances that happened back in the past. This will of course bring us back to square one in terms of our efforts in estimating in-nitely many weights. Another solution to this problem is through the autoregressive models in which the in-nitely many weights are assumed to follow a distinct pattern and can be successfully represented with only a handful of parameters. We shall now consider some special cases of autoregressive processes.

# 5.4.1 First-Order Autoregressive Process, AR(1)

Let us -rst consider again the time series given in Eq. (5.2):

$$
\begin{array}{l} y _ {t} = \mu + \sum_ {i = 0} ^ {\infty} \psi_ {i} \varepsilon_ {t - i} \\ = \mu + \sum_ {i = 0} ^ {\infty} \psi_ {i} B ^ {i} \varepsilon_ {t} \\ = \mu + \Psi (B) \varepsilon_ {t} \\ \end{array}
$$

where $\begin{array} { r } { \Psi ( B ) = \sum _ { i = 0 } ^ { \infty } \psi _ { i } B ^ { i } } \end{array}$ . As in the -nite order MA processes, one approach to modeling this time series is to assume that the contributions of the disturbances that are way in the past should be small compared to the more recent disturbances that the process has experienced. Since the disturbances are independently and identically distributed random variables, we can simply assume a set of in-nitely many weights in descending magnitudes reecting the diminishing magnitudes of contributions of the disturbances in the past. A simple, yet intuitive set of such weights can be created following an exponential decay pattern. For that we will set $\psi _ { i } = \phi ^ { i }$ , where $| \phi | < 1$ to guarantee the exponential 鈥渄ecay.鈥?In this notation, the weights on the disturbances starting from the current disturbance and going back in past will be $1 , \phi , \phi ^ { 2 } , \phi ^ { 3 } ,$ 鈥?Hence Eq. (5.2) can be written as

$$
\begin{array}{l} y _ {t} = \mu + \varepsilon_ {t} + \phi \varepsilon_ {t - 1} + \phi^ {2} \varepsilon_ {t - 2} + \dots \\ = \mu + \sum_ {i = 0} ^ {\infty} \phi^ {i} \varepsilon_ {t - i} \tag {5.18} \\ \end{array}
$$

From Eq. (5.18), we also have

$$
y _ {t - 1} = \mu + \varepsilon_ {t - 1} + \phi \varepsilon_ {t - 2} + \phi^ {2} \varepsilon_ {t - 3} + \dots \tag {5.19}
$$

We can then combine Eqs. (5.18) and (5.19) as

$$
\begin{array}{l} y _ {t} = \mu + \varepsilon_ {t} + \underbrace {\phi \varepsilon_ {t - 1} + \phi^ {2} \varepsilon_ {t - 2} + \cdots} _ {= \phi y _ {t - 1} - \phi \mu} \\ = \underbrace {\mu - \phi \mu} _ {= \delta} + \phi y _ {t - 1} + \varepsilon_ {t} \tag {5.20} \\ = \delta + \phi y _ {t - 1} + \varepsilon_ {t} \\ \end{array}
$$

where $\delta = \left( 1 - \phi \right) \mu$ . The process in Eq. (5.20) is called a -rst-order autoregressive process, AR(1), because Eq. (5.20) can be seen as a regression of $y _ { t }$ on $y _ { t - 1 }$ and hence the term autoregressive process.

The assumption of $| \phi | < 1$ results in the weights that decay exponen-| |tially in time and also guarantee $\textstyle \sum _ { i = 0 } ^ { + \infty } | \psi _ { i } | < \infty$ . This means that $| \phi | < 1$ $\vert \phi \vert > 1$ will get exponentially increasing weights as time goes on and the resulting time series will be explosive. Box et al. (2008) argue that this type of processes are of little practical interest and therefore only consider cases where $| \phi | = 1$ and $| \phi | < 1$ . The solution in (5.18) does indeed not converge for $\vert \phi \vert > 1$ . We can however rewrite the AR(1) process for $y _ { t + 1 }$

$$
y _ {t + 1} = \phi y _ {t} + a _ {t + 1} \tag {5.21}
$$

For $y _ { t }$ , we then have

$$
\begin{array}{l} y _ {t} = - \phi^ {- 1} \mu + \phi^ {- 1} y _ {t + 1} - \phi^ {- 1} \varepsilon_ {t + 1} \\ = - \phi^ {- 1} \mu + \phi^ {- 1} \left(- \phi^ {- 1} \mu + \phi^ {- 1} y _ {t + 2} - \phi^ {- 1} \varepsilon_ {t + 2}\right) - \phi^ {- 1} \varepsilon_ {t + 1} \\ = - \left(\phi^ {- 1} + \phi^ {- 2}\right) \mu + \phi^ {- 2} y _ {t + 2} - \phi^ {- 1} \varepsilon_ {t + 1} - \phi^ {- 2} \varepsilon_ {t + 2} \tag {5.22} \\ \end{array}
$$

鈰?
$$
= - \mu \sum_ {i = 1} ^ {\infty} \phi^ {- 1} - \sum_ {i = 1} ^ {\infty} \phi^ {- 1} \varepsilon_ {t + 1}
$$

For $\vert \phi \vert > 1$ we have $| \phi ^ { - 1 } | < 1$ and therefore the solution for $y _ { t }$ given in (5.22) is stationary. The only problem is that it involves future values of disturbances. This of course is impractical as this type of models requires knowledge about the future to make forecasts about it. These are called non-causal models. Therefore there exists a stationary solution for an AR(1) process when $\vert \phi \vert > 1$ , however, it results in a non-causal model. Throughout the book when we discuss the stationary autoregressive models, we implicitly refer to the causal autoregressive models. We can in fact show that an AR(I) process is nonstationary if and only if $| \phi | = 1$ .

|The mean of a stationary AR(1) process is

$$
E \left(y _ {t}\right) = \mu = \frac {\delta}{1 - \phi} \tag {5.23}
$$

The autocovariance function of a stationary AR(1) can be calculated from Eq. (5.18) as

$$
\gamma (k) = \sigma^ {2} \phi^ {k} \frac {1}{1 - \phi^ {2}} \quad \text {f o r} k = 0, 1, 2, \dots \tag {5.24}
$$

The covariance is then given as

$$
\gamma (0) = \sigma^ {2} \frac {1}{1 - \phi^ {2}} \tag {5.25}
$$

Correspondingly, the autocorrelation function for a stationary AR(1) process is given as

$$
\rho (k) = \frac {\gamma (k)}{\gamma (0)} = \phi^ {k} \quad \text {f o r} k = 0, 1, 2, \dots \tag {5.26}
$$

Hence the ACF for an AR(1) process has an exponential decay form.

A realization of the following AR(1) model,

$$
y _ {t} = 8 + 0. 8 y _ {t - 1} + \varepsilon_ {t}
$$

is shown in Figure 5.5. As in the MA(1) model with $\theta = - 0 . 8$ , we can observe some short runs during which observations tend to move in the upward or downward direction. As opposed to the MA(1) model, however, the duration of these runs tends to be longer and the trend tends to linger. This can also be observed in the sample ACF plot.

Figure 5.6 shows a realization of the AR(1) model $y _ { t } = 8 - 0 . 8 y _ { t - 1 } + \varepsilon _ { t }$ . We observe that instead of lingering runs, the observations exhibit jittery up/down movements because of the negative $\phi$ value.

![](images/0582b253b1085b48ded026ba04f0fb5c32ce07946df38a51d08de02a6daf3b1e.jpg)

![](images/e76fe3a76e58873c524906e18417b48240fd98d22aafb60fada4f60293a59a50.jpg)  
FIGURE 5.5 A realization of the AR(1) process, $y _ { t } = 8 + 0 . 8 y _ { t - 1 } + \varepsilon _ { t }$ .

![](images/810d47ad7d10eec19dcb285a90d18225a134e9dbca6cff77800dba99fe4d1386.jpg)

![](images/730f5c9dfcd91d8e6b9f3339ed5090597b0839065b9336fbf8aa4e55a615caa1.jpg)  
FIGURE 5.6 A realization of the AR(1) process, $y _ { t } = 8 - 0 . 8 y _ { t - 1 } + \varepsilon _ { t }$

# 5.4.2 Second-Order Autoregressive Process, AR(2)

In this section, we will -rst start with the obvious extension of Eq. (5.20) to include the observation $y _ { t - 2 }$ as

$$
y _ {t} = \delta + \phi_ {1} y _ {t - 1} + \phi_ {2} y _ {t - 2} + \varepsilon_ {t} \tag {5.27}
$$

We will then show that Eq. (5.27) can be represented in the in-nite MA form and provide the conditions of stationarity for $y _ { t }$ in terms of $\phi _ { 1 }$ and $\phi _ { 2 }$ . For that we will rewrite Eq. (5.27) as

$$
\left(1 - \phi_ {1} B - \phi_ {2} B ^ {2}\right) y _ {t} = \delta + \varepsilon_ {t} \tag {5.28}
$$

or

$$
\Phi (B) y _ {t} = \delta + \varepsilon_ {t} \tag {5.29}
$$

Furthermore, applying $\Phi ( B ) ^ { - 1 }$ to both sides, we obtain

$$
\begin{array}{l} y _ {t} = \underbrace {\Phi (B) ^ {- 1} \delta} _ {= \mu} + \underbrace {\Phi (B) ^ {- 1}} _ {= \Psi (B)} \varepsilon_ {t} \\ = \mu + \Psi (B) \varepsilon_ {t} \tag {5.30} \\ = \mu + \sum_ {i = 0} ^ {\infty} \psi_ {i} \varepsilon_ {t - i} \\ = \mu + \sum_ {i = 0} ^ {\infty} \psi_ {i} B ^ {i} \varepsilon_ {t} \\ \end{array}
$$

where

$$
\mu = \Phi (B) ^ {- 1} \delta \tag {5.31}
$$

and

$$
\Phi (B) ^ {- 1} = \sum_ {i = 0} ^ {\infty} \psi_ {i} B ^ {i} = \Psi (B) \tag {5.32}
$$

We can use Eq. (5.32) to obtain the weights in Eq. (5.30) in terms of $\phi _ { 1 }$ and $\phi _ { 2 }$ . For that, we will use

$$
\Phi (B) \Psi (B) = 1 \tag {5.33}
$$

That is,

$$
(1 - \phi_ {1} B - \phi_ {2} B ^ {2}) (\psi_ {0} + \psi_ {1} B + \psi_ {2} B ^ {2} + \dots) = 1
$$

or

$$
\begin{array}{l} \psi_ {0} + (\psi_ {1} - \phi_ {1} \psi_ {0}) B + (\psi_ {2} - \phi_ {1} \psi_ {1} - \phi_ {2} \psi_ {0}) B ^ {2} \\ + \dots + \left(\psi_ {j} - \phi_ {1} \psi_ {j - 1} - \phi_ {2} \psi_ {j - 2}\right) B ^ {j} + \dots = 1 \tag {5.34} \\ \end{array}
$$

Since on the right-hand side of the Eq. (5.34) there are no backshift operators, for $\Phi ( B ) \Psi ( B ) = 1$ , we need

$$
\psi_ {0} = 1
$$

$$
\left(\psi_ {1} - \phi_ {1} \psi_ {0}\right) = 0 \tag {5.35}
$$

$$
\left(\psi_ {j} - \phi_ {1} \psi_ {j - 1} - \phi_ {2} \psi_ {j - 2}\right) = 0 \quad \text {f o r a l l} j = 2, 3, \dots
$$

The equations in (5.35) can indeed be solved for each $\psi _ { j }$ in a futile attempt to estimate in-nitely many parameters. However, it should be noted that the $\psi _ { j }$ in Eq. (5.35) satisfy the second-order linear difference equation and that they can be expressed as the solution to this equation in terms of the two roots $m _ { 1 }$ and $m _ { 2 }$ of the associated polynomial

$$
m ^ {2} - \phi_ {1} m - \phi_ {2} = 0 \tag {5.36}
$$

If the roots obtained by

$$
m _ {1}, m _ {2} = \frac {\phi_ {1} \pm \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{2}
$$

satisfy $\left| m _ { 1 } \right| , \left| m _ { 2 } \right| < 1$ , then we have $\begin{array} { r } { \sum _ { i = 0 } ^ { + \infty } \left| \psi _ { i } \right| < \infty } \end{array}$ . Hence if the roots $m _ { 1 }$ and $m _ { 2 }$ | | | | are both less than 1 in absolute value, then the AR(2) model is causal and stationary. Note that if the roots of Eq. (5.36) are complex conjugates of the form $a \pm i b$ , the condition for stationarity is that $\sqrt { a ^ { 2 } + b ^ { 2 } } < 1$ Furthermore, under the condition that $\left| m _ { 1 } \right| , \left| m _ { 2 } \right| < 1$ , the AR(2) time series, $\left\{ y _ { t } \right\}$ | | | | , has an in-nite MA representation as in Eq. (5.30).

This implies that for the second-order autoregressive process to be stationary, the parameters $\phi _ { 1 }$ and $\phi _ { 2 }$ must satisfy.

$$
\phi_ {1} + \phi_ {2} <   1
$$

$$
\phi_ {2} - \phi_ {1} <   1
$$

$$
| \phi_ {2} | <   1
$$

Now that we have established the conditions for the stationarity of an AR(2) time series, let us now consider its mean, autocovariance, and autocorrelation functions. From Eq. (5.27), we have

$$
\begin{array}{l} E (y _ {t}) = \delta + \phi_ {1} E (y _ {t - 1}) + \phi_ {2} E (y _ {t - 2}) + 0 \\ \mu = \delta + \phi_ {1} \mu + \phi_ {2} \mu \\ \Rightarrow \mu = \frac {\delta}{1 - \phi_ {1} - \phi_ {2}} \tag {5.37} \\ \end{array}
$$

Note that for $1 - \phi _ { 1 } - \phi _ { 2 } = 0$ , $m = 1$ is one of the roots for the associated polynomial in Eq. (5.36) and hence the time series is deemed nonstationary. The autocovariance function is

$$
\begin{array}{l} \gamma (k) = \operatorname {C o v} (y _ {t}, y _ {t - k}) \\ = \operatorname {C o v} \left(\delta + \phi_ {1} y _ {t - 1} + \phi_ {2} y _ {t - 2} + \varepsilon_ {t}, y _ {t - k}\right) \\ = \phi_ {1} \operatorname {C o v} \left(y _ {t - 1}, y _ {t - k}\right) + \phi_ {2} \operatorname {C o v} \left(y _ {t - 2}, y _ {t - k}\right) + \operatorname {C o v} \left(\varepsilon_ {t}, y _ {t - k}\right) \tag {5.38} \\ = \phi_ {1} \gamma (k - 1) + \phi_ {2} \gamma (k - 2) + \left\{ \begin{array}{l l} \sigma^ {2} & \mathrm {i f} k = 0 \\ 0 & \mathrm {i f} k > 0 \end{array} \right. \\ \end{array}
$$

Thus $\gamma ( 0 ) = \phi _ { 1 } \gamma ( 1 ) + \phi _ { 2 } \gamma ( 2 ) + \sigma ^ { 2 }$ and

$$
\gamma (k) = \phi_ {1} \gamma (k - 1) + \phi_ {2} \gamma (k - 2), \quad k = 1, 2, \dots \tag {5.39}
$$

The equations in (5.39) are called the Yule鈥揥alker equations for $\gamma ( k )$ . Similarly, we can obtain the autocorrelation function by dividing Eq. (5.39) by 饾浘 (0):

$$
\rho (k) = \phi_ {1} \rho (k - 1) + \phi_ {2} \rho (k - 2), \quad k = 1, 2, \dots \tag {5.40}
$$

The Yule鈥揥alker equations for $\rho ( k )$ in Eq. (5.40) can be solved recursively as

$$
\rho (1) = \phi_ {1} \underbrace {\rho (0)} _ {= 1} + \phi_ {2} \underbrace {\rho (- 1)} _ {= \rho (1)}
$$

$$
= \frac {\phi_ {1}}{1 - \phi_ {2}}
$$

$$
\rho (2) = \phi_ {1} \rho (1) + \phi_ {2}
$$

$$
\rho (3) = \phi_ {1} \rho (2) + \phi_ {2} \rho (1)
$$

A general solution can be obtained through the roots $m _ { 1 }$ and $m _ { 2 }$ of the associated polynomial $m ^ { 2 } - \phi _ { 1 } m - \phi _ { 2 } = 0$ . There are three cases.

Case 1. If $m _ { 1 }$ and $m _ { 2 }$ are distinct, real roots, we then have

$$
\rho (k) = c _ {1} m _ {1} ^ {k} + c _ {2} m _ {2} ^ {k}, \quad k = 0, 1, 2, \dots \tag {5.41}
$$

where $c _ { 1 }$ and $c _ { 2 }$ are particular constants and can, for example, be obtained from $\rho ( 0 )$ and $\rho ( 1 )$ . Moreover, since for stationarity we have $\left| m _ { 1 } \right|$ $\left| m _ { 1 } \right| , \left| m _ { 2 } \right| < 1$ , in this case, the autocorrelation function is a | | | |mixture of two exponential decay terms.

Case 2. If $m _ { 1 }$ and $m _ { 2 }$ are complex conjugates in the form of $a \pm i b$ , we then have

$$
\rho (k) = R ^ {k} \left[ c _ {1} \cos (\lambda k) + c _ {2} \sin (\lambda k) \right], \quad k = 0, 1, 2, \dots \tag {5.42}
$$

where $R = | m _ { i } | = \sqrt { a ^ { 2 } + b ^ { 2 } }$ and $\lambda$ is determined by cos $( \lambda ) = a / R$ , $\sin \left( \lambda \right) = b / R$ . Hence we have $a \pm i b = R$ $a \pm i b = R \left[ \cos \left( \lambda \right) \pm i \sin \left( \lambda \right) \right]$ . Once

again $c _ { 1 }$ and $c _ { 2 }$ are particular constants. The ACF in this case has the form of a damped sinusoid, with damping factor $R$ and frequency $\lambda$ ; that is, the period is $2 \pi / \lambda$ .

Case 3. If there is one real root $m _ { 0 }$ , $m _ { 1 } = m _ { 2 } = m _ { 0 }$ , we then have

$$
\rho (k) = \left(c _ {1} + c _ {2} k\right) m _ {0} ^ {k} \quad k = 0, 1, 2, \dots \tag {5.43}
$$

In this case, the ACF will exhibit an exponential decay pattern.

In case 1, for example, an AR(2) model can be seen as an 鈥渁djusted鈥?AR(1) model for which a single exponential decay expression as in the AR(1) model is not enough to describe the pattern in the ACF, and hence an additional exponential decay expression is 鈥渁dded鈥?by introducing the second lag term, $y _ { t - 2 }$ .

Figure 5.7 shows a realization of the AR(2) process

$$
y _ {t} = 4 + 0. 4 y _ {t - 1} + 0. 5 y _ {t - 2} + \varepsilon_ {t}
$$

Note that the roots of the associated polynomial of this model are real. Hence the ACF is a mixture of two exponential decay terms.

Similarly, Figure 5.8 shows a realization of the following AR(2) process

$$
y _ {t} = 4 + 0. 8 y _ {t - 1} - 0. 5 y _ {t - 2} + \varepsilon_ {t}.
$$

For this process, the roots of the associated polynomial are complex conjugates. Therefore the ACF plot exhibits a damped sinusoid behavior.

![](images/cb8aeca869357496ec835fa072fde1761f8ba7556707bd5af1ab5ee641e3fa1d.jpg)

![](images/46349663ae6d521cf29376f622e583ad67715a29a096cd3dc4ac64a68a98cb7c.jpg)  
FIGURE 5.7 A realization of the AR(2) process, $y _ { t } = 4 + 0 . 4 y _ { t - 1 } + 0 . 5 y _ { t - 2 } +$ $\varepsilon _ { t }$ .

![](images/55ea24e0744241c63024c0484e069e4aa883f9d3bbb0a6b78ca5c1b9cf5763a4.jpg)

![](images/5fd45d47a38e1200dac7623e0ae5e28cde05cbc8f581055b7ade54bbe3d3e8ce.jpg)  
FIGURE 5.8 A realization of the AR(2) process, $y _ { t } = 4 + 0 . 8 y _ { t - 1 } - 0 . 5 y _ { t - 2 } +$ $\varepsilon _ { t }$ .

# 5.4.3 General Autoregressive Process, AR(p)

From the previous two sections, a general, $p$ th-order AR model is given as

$$
y _ {t} = \delta + \phi_ {1} y _ {t - 1} + \phi_ {2} y _ {t - 2} + \dots + \phi_ {p} y _ {t - p} + \varepsilon_ {t} \tag {5.44}
$$

where $\varepsilon _ { t }$ is white noise. Another representation of Eq. (5.44) can be given as

$$
\Phi (B) y _ {t} = \delta + \varepsilon_ {t} \tag {5.45}
$$

where $\Phi ( B ) = 1 - \phi _ { 1 } B - \phi _ { 2 } B ^ { 2 } - \cdots - \phi _ { p } B ^ { p }$ .

The $\operatorname { A R } ( p )$ time series $\left\{ y _ { t } \right\}$ in Eq. (5.44) is causal and stationary if the roots of the associated polynomial

$$
m ^ {p} - \phi_ {1} m ^ {p - 1} - \phi_ {2} m ^ {p - 2} - \dots - \phi_ {p} = 0 (5. 4 6)
$$

are less than one in absolute value. Furthermore, under this condition, the $\operatorname { A R } ( p )$ time series $\left\{ { { y } _ { t } } \right\}$ is also said to have an absolutely summable in-nite MA representation

$$
y _ {t} = \mu + \Psi (B) \varepsilon_ {t} = \mu + \sum_ {i = 0} ^ {\infty} \psi_ {i} \varepsilon_ {t - i} \tag {5.47}
$$

where $\Psi ( B ) = \Phi ( B ) ^ { - 1 }$ with $\textstyle \sum _ { i = 0 } ^ { \infty } \left| \psi _ { i } \right| < \infty$ .

As in AR(2), the weights of the random shocks in Eq. (5.47) can be obtained from $\Phi ( B ) \Psi ( B ) = 1$ as

$$
\psi_ {j} = 0, \quad j <   0
$$

$$
\psi_ {0} = 1 \tag {5.48}
$$

$$
\psi_ {j} - \phi_ {1} \psi_ {j - 1} - \phi_ {2} \psi_ {j - 2} - \dots - \phi_ {p} \psi_ {j - p} = 0 \quad \text {f o r a l l} j = 1, 2, \dots
$$

We can easily show that, for the stationary $\operatorname { A R } ( p )$ process

$$
E (y _ {t}) = \mu = \frac {\delta}{1 - \phi_ {1} - \phi_ {2} - \cdots - \phi_ {p}}
$$

and

$$
\begin{array}{l} \gamma (k) = \operatorname {C o v} \left(y _ {t}, y _ {t - k}\right) \\ = \operatorname {C o v} \left(\delta + \phi_ {1} y _ {t - 1} + \phi_ {2} y _ {t - 2} + \dots + \phi_ {p} y _ {t - p} + \varepsilon_ {t}, y _ {t - k}\right) \tag {5.49} \\ = \sum_ {i = 1} ^ {p} \phi_ {i} \mathrm {C o v} (y _ {t - i}, y _ {t - k}) + \mathrm {C o v} (\varepsilon_ {t}, y _ {t - k}) \\ = \sum_ {i = 1} ^ {p} \phi_ {i} \gamma (k - i) + \left\{ \begin{array}{l l} \sigma^ {2} & \text {i f} k = 0 \\ 0 & \text {i f} k > 0 \end{array} \right. \\ \end{array}
$$

Thus we have

$$
\begin{array}{l} \gamma (0) = \sum_ {i = 1} ^ {p} \phi_ {i} \gamma (i) + \sigma^ {2} (5.50) \\ \Rightarrow \gamma (0) \left[ 1 - \sum_ {i = 1} ^ {p} \phi_ {i} \rho (i) \right] = \sigma^ {2} (5.51) \\ \end{array}
$$

By dividing Eq. (5.49) by 饾浘 (0) for $k > 0$ , it can be observed that the ACF of an $\operatorname { A R } ( p )$ process satis-es the Yule鈥揥alker equations

$$
\rho (k) = \sum_ {i = 1} ^ {p} \phi_ {i} \rho (k - i), \quad k = 1, 2, \dots \tag {5.52}
$$

The equations in (5.52) are $p$ th-order linear difference equations, implying that the ACF for an $\operatorname { A R } ( p )$ model can be found through the $p$ roots of

the associated polynomial in Eq. (5.46). For example, if the roots are all distinct and real, we have

$$
\rho (k) = c _ {1} m _ {1} ^ {k} + c _ {2} m _ {2} ^ {k} + \dots + c _ {p} m _ {p} ^ {k}, \quad k = 1, 2, \dots \tag {5.53}
$$

where $c _ { 1 } , c _ { 2 } , \ldots , c _ { p }$ are particular constants. However, in general, the roots may not all be distinct or real. Thus the ACF of an $\operatorname { A R } ( p )$ process can be a mixture of exponential decay and damped sinusoid expressions depending on the roots of Eq. (5.46).

# 5.4.4 Partial Autocorrelation Function, PACF

In Section 5.2, we saw that the ACF is an excellent tool in identifying the order of an $\mathrm { M A } ( q )$ process, because it is expected to 鈥渃ut off鈥?after lag q. However, in the previous section, we pointed out that the ACF is not as useful in the identi-cation of the order of an $\operatorname { A R } ( p )$ process for which it will most likely have a mixture of exponential decay and damped sinusoid expressions. Hence such behavior, while indicating that the process might have an AR structure, fails to provide further information about the order of such structure. For that, we will de-ne and employ the partial autocorrelation function (PACF) of the time series. But before that, we discuss the concept of partial correlation to make the interpretation of the PACF easier.

Partial Correlation Consider three random variables X, Y, and Z. Then consider simple linear regression of $X$ on $Z$ and $Y$ on $Z$ as

$$
\hat {X} = a _ {1} + b _ {1} Z \quad \mathrm {w h e r e} b _ {1} = \frac {\operatorname {C o v} (Z , X)}{\operatorname {V a r} (Z)}
$$

and

$$
\hat {Y} = a _ {2} + b _ {2} Z \quad \mathrm {w h e r e} b _ {2} = \frac {\operatorname {C o v} (Z , Y)}{\operatorname {V a r} (Z)}
$$

Then the errors can be obtained from

$$
X ^ {*} = X - \hat {X} = X - (a _ {1} + b _ {1} Z)
$$

and

$$
Y ^ {*} = Y - \hat {Y} = Y - (a _ {2} + b _ {2} Z)
$$

Then the partial correlation between $X$ and $Y$ after adjusting for $Z$ is de-ned as the correlation between $X ^ { * }$ and $Y ^ { * }$ ; $\operatorname { c o r r } ( X ^ { * } , Y ^ { * } ) =$ $\operatorname { c o r r } ( X - { \hat { X } } , Y - { \hat { Y } } )$ . That is, partial correlation can be seen as the correlation between two variables after being adjusted for a common factor that may be affecting them. The generalization is of course possible by allowing for adjustment for more than just one factor.

Partial Autocorrelation Function Following the above de-nition, the PACF between $y _ { t }$ and $y _ { t - k }$ is the autocorrelation between $y _ { t }$ and $y _ { t - k }$ after adjusting for $y _ { t - 1 } , y _ { t - 2 } , \ldots , y _ { t - k + 1 }$ . Hence for an $\operatorname { A R } ( p )$ model the PACF between $y _ { t }$ and $y _ { t - k }$ for $k > p$ should be equal to zero. A more formal de-nition can be found below.

Consider a stationary time series model $\left\{ { y } _ { t } \right\}$ that is not necessarily an AR process. Further consider, for any -xed value of $k$ , the Yule鈥揥alker equations for the ACF of an $\operatorname { A R } ( p )$ process given in Eq. (5.52) as

$$
\rho (j) = \sum_ {i = 1} ^ {k} \phi_ {i k} \rho (j - i), \quad j = 1, 2, \dots , k \tag {5.54}
$$

or

$$
\begin{array}{l} \rho (1) = \phi_ {1 k} + \phi_ {2 k} \rho (1) + \dots + \phi_ {k k} \rho (k - 1) \\ \rho (2) = \phi_ {1 k} \rho (1) + \phi_ {2 k} + \dots + \phi_ {k k} \rho (k - 2) \\ \begin{array}{c} \bullet \\ \bullet \\ \bullet \end{array} \\ \end{array}
$$

$$
\rho (k) = \phi_ {1 k} \rho (k -) + \phi_ {2 k} \rho (k - 2) + \dots + \phi_ {k k}
$$

Hence we can write the equations in (5.54) in matrix notation as

$$
\left[ \begin{array}{c c c c c} 1 & \rho (1) & \rho (2) & \dots & \rho (k - 1) \\ \rho (1) & 1 & \rho (3) & \dots & \rho (k - 2) \\ \rho (2) & \rho (1) & 1 & \dots & \rho (k - 3) \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \rho (k - 1) & \rho (k - 2) & \rho (k - 3) & \dots & 1 \end{array} \right] \left[ \begin{array}{l} \phi_ {1 k} \\ \phi_ {2 k} \\ \phi_ {3 k} \\ \vdots \\ \phi_ {k k} \end{array} \right] = \left[ \begin{array}{l} \rho (1) \\ \rho (2) \\ \rho (3) \\ \vdots \\ \rho (k) \end{array} \right] \tag {5.55}
$$

or

$$
\mathbf {P} _ {k} \phi_ {k} = \rho_ {k} \tag {5.56}
$$

where

$$
\begin{array}{l} \mathbf {P} _ {k} = \left[ \begin{array}{c c c c c} 1 & \rho (1) & \rho (2) & \dots & \rho (k - 1) \\ \rho (1) & 1 & \rho (3) & \dots & \rho (k - 2) \\ \rho (2) & \rho (1) & 1 & \dots & \rho (k - 3) \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \rho (k - 1) & \rho (k - 2) & \rho (k - 3) & \dots & 1 \end{array} \right], \\ \phi_ {k} = \left[ \begin{array}{l} \phi_ {1 k} \\ \phi_ {2 k} \\ \phi_ {3 k} \\ \vdots \\ \phi_ {k k} \end{array} \right], \quad \mathrm {a n d} \quad \rho_ {k} = \left[ \begin{array}{l} \rho (1) \\ \rho (2) \\ \rho (3) \\ \vdots \\ \rho (k) \end{array} \right]. \\ \end{array}
$$

Thus to solve for $\phi _ { k }$ , we have

$$
\phi_ {k} = \mathbf {P} _ {k} ^ {- 1} \rho_ {k} \tag {5.57}
$$

For any given k, k = 1, 2, 鈥?, the last coef-cient $\phi _ { k k }$ is called the partial autocorrelation of the process at lag $k$ . Note that for an $\operatorname { A R } ( p )$ process $\phi _ { k k } = 0$ for $k > p$ . Hence we say that the PACF cuts off after lag $p$ for an $\operatorname { A R } ( p )$ . This suggests that the PACF can be used in identifying the order of an AR process similar to how the ACF can be used for an MA process.

For sample calculations, $\hat { \phi } _ { k k }$ , the sample estimate of $\phi _ { k k }$ , is obtained by using the sample ACF, $r ( k )$ . Furthermore, in a sample of $N$ observations from an $\operatorname { A R } ( p )$ process, $\hat { \phi } _ { k k } \operatorname { f o r } k > p$ $\hat { \phi } _ { k k }$ is approximately normally distributed with

$$
E \left(\hat {\phi} _ {k k}\right) \approx 0 \quad \text {a n d} \quad \operatorname {V a r} \left(\hat {\phi} _ {k k}\right) \approx \frac {1}{N} \tag {5.58}
$$

Hence the $9 5 \%$ limits to judge whether any $\hat { \phi } _ { k k }$ is statistically signi-cantly different from zero are given by $\pm 2 / \sqrt { N }$ . For further detail see Quenouille (1949), Jenkins (1954, 1956), and Daniels (1956).

Figure 5.9 shows the sample PACFs of the models we have considered so far. In Figure 5.9a we have the sample PACF of the realization of the MA(1) model with $\theta = 0 . 8$ given in Figure 5.3. It exhibits an exponential decay pattern. Figure 5.9b shows the sample PACF of the realization of the MA(2) model in Figure 5.4 and it also has an exponential decay pattern in absolute value since for this model the roots of the associated polynomial are real. Figures $5 . 9 \mathrm { c }$ and 5.9d show the sample PACFs of the realization of the AR(1) model with $\phi = 0 . 8$ and $\phi = - 0 . 8$ , respectively. In both

![](images/1732cad634e02e9d5a4aa445c02732154a9f163b15abfc6dc0bf23c7d6c977ee.jpg)  
(a)

![](images/9873b3f4a5a5c9601917264301425eb0c52452f1af93875d48a377585ed6d858.jpg)  
(b)

![](images/0bd44a593874f627a9fa1870709b51ac7fe2fdf37b49421d24bc3fc001995f11.jpg)  
(c)

![](images/2c9d12ba8e5bf565346ba580bc640aecc4f9f4af7598a64537ab99f9e0b5d47b.jpg)

![](images/4913f52dbb4b301a4669abeeb74f1d27419344827d83a962d27a28beba82a5da.jpg)  
(e)

![](images/eccdf6be28eef80d02a8b946aa53fdc4f681445ba82bc202d2d40d56815363cb.jpg)  
(f )   
FIGURE 5.9 Partial autocorrelation functions for the realizations of (a) MA(1) process, $y _ { t } = 4 0 + \varepsilon _ { t } - 0 . 8 \varepsilon _ { t - 1 }$ ; (b) MA(2) process, $y _ { t } = 4 0 + \varepsilon _ { t } + 0 . 7 \varepsilon _ { t - 1 } -$ $0 . 2 8 \varepsilon _ { t - 2 }$ ; (c) AR(1) process, $y _ { t } = 8 + 0 . 8 y _ { t - 1 } + \varepsilon _ { t }$ ; (d) AR(1) process, $y _ { t } =$ $8 - 0 . 8 y _ { t - 1 } + \varepsilon _ { t }$ ; (e) AR(2) process, $y _ { t } = 4 + 0 . 4 y _ { t - 1 } + 0 . 5 y _ { t - 2 } + \varepsilon _ { t }$ ; and (f) AR(2) process, $y _ { t } = 4 + 0 . 8 y _ { t - 1 } - 0 . 5 y _ { t - 2 } + \varepsilon _ { t }$ .

cases the PACF 鈥渃uts off鈥?after the -rst lag. That is, the only signi-cant sample PACF value is at lag 1, suggesting that the AR(1) model is indeed appropriate to -t the data. Similarly, in Figures 5.9e and 5.9f, we have the sample PACFs of the realizations of the AR(2) model. Note that the sample PACF cuts off after lag 2.

As we discussed in Section 5.3, -nite order MA processes are stationary. On the other hand as in the causality concept we discussed for the autoregressive processes, we will impose some restrictions on the parameters of the MA models as well. Consider for example the MA(1) model in (5.11)

$$
y _ {t} = \varepsilon_ {t} - \theta_ {1} \varepsilon_ {t - 1} \tag {5.59}
$$

Note that for the sake of simplicity, in (5.59) we consider a centered process, i.e. $E ( y _ { t } ) = 0$ .

We can then rewrite (5.59) as

$$
\begin{array}{l} \varepsilon_ {t} = y _ {t} + \theta_ {1} \varepsilon_ {t - 1} \\ = y _ {t} + \theta_ {1} \left[ y _ {t - 1} + \theta_ {1} \varepsilon_ {t - 2} \right] \\ = y _ {t} + \theta_ {1} y _ {t - 1} + \theta_ {1} ^ {2} \varepsilon_ {t - 2} \tag {5.60} \\ \begin{array}{c} \vdots \\ \vdots \end{array} \\ = \sum_ {i = 0} ^ {\infty} \theta_ {1} ^ {i} y _ {t - i} \\ \end{array}
$$

It can be seen from (5.60) that for $| \theta _ { 1 } | < 1$ , $\varepsilon _ { t }$ is a convergent series of current and past observations and the process is called an invertible moving average process. Similar to the causality argument, for $| \theta _ { 1 } | > 1$ , $\varepsilon _ { t }$ can be written as a convergent series of future observations and is called noninvertible. When $| \theta _ { 1 } | = 1$ , the MA(1) process is considered noninvertible in a more restricted sense (Brockwell and Davis (1991)).

The direct implication of invertibility becomes apparent in model identi-cation. Consider the MA(1) process as an example. The -rst lag autocorrelation for that process is given as

$$
\rho (1) = \frac {- \theta_ {1}}{1 + \theta_ {1} ^ {2}} \tag {5.61}
$$

This allows for the calculation of $\theta _ { 1 }$ for a given $\rho ( 1 )$ by rearranging (5.61) as

$$
\theta_ {1} ^ {2} - \frac {\theta_ {1}}{\rho (1)} + 1 = 0 \tag {5.62}
$$

and solving for $\theta _ { 1 }$ . Except for the case of a repeated root, this equation has two solutions. Consider for example $\rho ( 1 ) = 0 . 4$ for which both $\theta _ { 1 } = 0 . 5$ and $\theta _ { 1 } = 2$ are the solutions for (5.62). Following the above argument, only $\theta _ { 1 } = 0 . 5$ yields the invertible MA(1) process. It can be shown that when there are multiple solutions for possible values of MA parameters, there

is only one solution that will satisfy the invertibility condition (Box et al. (2008), Section 6.4.1).

Consider the $\mathrm { M A } ( q )$ process

$$
\begin{array}{l} y _ {t} = \mu + \left(1 - \sum_ {i = 1} ^ {q} \theta_ {i} B ^ {i}\right) \varepsilon_ {t} \\ = \mu + \Theta (B) \varepsilon_ {t} \\ \end{array}
$$

After multiplying both sides with $\Theta ( B ) ^ { - 1 }$ , we have

$$
\begin{array}{l} \Theta (B) ^ {- 1} y _ {t} = \Theta (B) ^ {- 1} \mu + \varepsilon_ {t} \tag {5.63} \\ \Pi (B) y _ {t} = \delta + \varepsilon_ {t} \\ \end{array}
$$

where $\begin{array} { r } { \Pi ( B ) = 1 - \sum _ { i = 1 } ^ { \infty } \pi _ { i } B ^ { i } = \Theta ( B ) ^ { - 1 } } \end{array}$ and $\Theta ( B ) ^ { - 1 } \mu = \delta$ . Hence the in-- nite AR representation of an $\mathrm { M A } ( q )$ process is given as

$$
y _ {t} - \sum_ {i = 1} ^ {\infty} \pi_ {i} y _ {t - i} = \delta + \varepsilon_ {t} \tag {5.64}
$$

with $\textstyle \sum _ { i = 1 } ^ { \infty } \left| \pi _ { i } \right| < \infty$ . The $\pi _ { i }$ can be determined from

$$
(1 - \theta_ {1} B - \theta_ {2} B ^ {2} - \dots - \theta_ {q} B ^ {q}) (1 - \pi_ {1} B - \pi_ {2} B ^ {2} + \dots) = 1 \tag {5.65}
$$

which in turn yields

$$
\pi_ {1} + \theta_ {1} = 0
$$

$$
\pi_ {2} - \theta_ {1} \pi_ {1} + \theta_ {2} = 0 \tag {5.66}
$$

鈰?
$$
\pi_ {j} - \theta_ {1} \pi_ {j - 1} - \dots - \theta_ {q} \pi_ {j - q} = 0
$$

with $\pi _ { 0 } = - 1$ and $\pi _ { j } = 0$ for $j < 0$ . Hence as in the previous arguments for the stationarity of $\operatorname { A R } ( p )$ models, the $\pi _ { i }$ are the solutions to the qth-order linear difference equations and therefore the condition for the invertibility of an $\mathrm { M A } ( q )$ process turns out to be very similar to the stationarity condition of an $\operatorname { A R } ( p )$ process: the roots of the associated polynomial given in Eq. (5.66) should be less than 1 in absolute value,

$$
m ^ {q} - \theta_ {1} m ^ {q - 1} - \theta_ {2} m ^ {q - 2} - \dots - \theta_ {q} = 0 \tag {5.67}
$$

An invertible $\mathrm { M A } ( q )$ process can then be written as an in-nite AR process.

Correspondingly, for such a process, adjusting for $y _ { t - 1 } , y _ { t - 2 } , \ldots , y _ { t - k + 1 }$ does not necessarily eliminate the correlation between $y _ { t }$ and $y _ { t - k }$ and therefore its PACF will never 鈥渃ut off.鈥?In general, the PACF of an $\mathrm { M A } ( q )$ process is a mixture of exponential decay and damped sinusoid expressions.

The ACF and the PACF do have very distinct and indicative properties for MA and AR models, respectively. Therefore, in model identi-cation, we strongly recommend the use of both the sample ACF and the sample PACF simultaneously.

# 5.5 MIXED AUTOREGRESSIVE鈥揗OVING AVERAGE PROCESSES

In the previous sections we have considered special cases of Wold鈥檚 decomposition of a stationary time series represented as a weighted sum of in-nite random shocks. In an AR(1) process, for example, the weights in the in-- nite sum are forced to follow an exponential decay form with $\phi$ as the rate of decay. Since there are no restrictions apart from $\textstyle \sum _ { i = 0 } ^ { \infty } \psi _ { i } ^ { 2 } < \infty$ on the weights $( \psi _ { i } )$ , it may not be possible to approximate them by an exponential decay pattern. For that, we will need to increase the order of the AR model to approximate any pattern that these weights may in fact be exhibiting. On some occasions, however, it is possible to make simple adjustments to the exponential decay pattern by adding only a few terms and hence to have a more parsimonious model. Consider, for example, that the weights $\psi _ { i }$ do indeed exhibit an exponential decay pattern with a constant rate except for the fact that $\psi _ { 1 }$ is not equal to this rate of decay as it would be in the case of an AR(1) process. Hence instead of increasing the order of the AR model to accommodate for this 鈥渁nomaly,鈥?we can add an MA(1) term that will simply adjust $\psi _ { 1 }$ while having no effect on the rate of exponential decay pattern of the rest of the weights. This results in a mixed autoregressive moving average or ARMA(1,1) model. In general, an ARMA $( p , q )$ model is given as

$$
\begin{array}{l} y _ {t} = \delta + \phi_ {1} y _ {t - 1} + \phi_ {2} y _ {t - 2} + \dots + \phi_ {p} y _ {t - p} + \varepsilon_ {t} - \theta_ {1} \varepsilon_ {t - 1} \\ - \theta_ {2} \varepsilon_ {t - 2} - \dots - \theta_ {q} \varepsilon_ {t - q} \\ = \delta + \sum_ {i = 1} ^ {p} \phi_ {i} y _ {t - i} + \varepsilon_ {t} - \sum_ {i = 1} ^ {q} \theta_ {i} \varepsilon_ {t - i} \tag {5.68} \\ \end{array}
$$

or

$$
\Phi (B) y _ {t} = \delta + \Theta (B) \varepsilon_ {t} \tag {5.69}
$$

where $\varepsilon _ { t }$ is a white noise process.

# 5.5.1 Stationarity of ARMA(p, q) Process

The stationarity of an ARMA process is related to the AR component in the model and can be checked through the roots of the associated polynomial

$$
m ^ {p} - \phi_ {1} m ^ {p - 1} - \phi_ {2} m ^ {p - 2} - \dots - \phi_ {p} = 0. \tag {5.70}
$$

If all the roots of Eq. (5.70) are less than one in absolute value, then $\mathbf { A R M A } ( p , q )$ is stationary. This also implies that, under this condition, $\mathbf { A R M A } ( p , q )$ has an in-nite MA representation as

$$
y _ {t} = \mu + \sum_ {i = 0} ^ {\infty} \psi_ {i} \varepsilon_ {t - i} = \mu + \Psi (B) \varepsilon_ {t} \tag {5.71}
$$

with $\Psi ( B ) = \Phi ( B ) ^ { - 1 } \Theta ( B )$ . The coef-cients in $\Psi ( B )$ can be found from

$$
\psi_ {i} - \phi_ {1} \psi_ {i - 1} - \phi_ {2} \psi_ {i - 2} - \dots - \phi_ {p} \psi_ {i - p} = \left\{ \begin{array}{l l} - \theta_ {i}, & i = 1, \dots , q \\ 0, & i > q \end{array} \right. \tag {5.72}
$$

and $\psi _ { 0 } = 1$ .

# 5.5.2 Invertibility of ARMA(p, q) Process

Similar to the stationarity condition, the invertibility of an ARMA process is related to the MA component and can be checked through the roots of the associated polynomial

$$
m ^ {q} - \theta_ {1} m ^ {q - 1} - \theta_ {2} m ^ {q - 2} - \dots - \theta_ {q} = 0 \tag {5.73}
$$

If all the roots of Eq. (5.71) are less than one in absolute value, then $\mathbf { A R M A } ( p , q )$ is said to be invertible and has an in-nite AR representation,

$$
\Pi (B) y _ {t} = \alpha + \varepsilon_ {t} \tag {5.74}
$$

where $\alpha = \Theta ( B ) ^ { - 1 } \delta$ and $\Pi ( B ) = \Theta ( B ) ^ { - 1 } \Phi ( B )$ . The coef-cients in $\Pi ( B )$ can be found from

$$
\pi_ {i} - \theta_ {1} \pi_ {i - 1} - \theta_ {2} \pi_ {i - 2} - \dots - \theta_ {q} \pi_ {i - q} = \left\{ \begin{array}{l l} \phi_ {i}, & i = 1, \dots , p \\ 0, & i > p \end{array} \right. \tag {5.75}
$$

and $\pi _ { 0 } = - 1$

In Figure 5.10 we provide realizations of two ARMA(1,1) models:

$$
y _ {t} = 1 6 + 0. 6 y _ {t - 1} + \varepsilon_ {t} + 0. 8 \varepsilon_ {t - 1} \quad \text {a n d} \quad y _ {t} = 1 6 - 0. 7 y _ {t - 1} + \varepsilon_ {t} - 0. 6 \varepsilon_ {t - 1}.
$$

Note that the sample ACFs and PACFs exhibit exponential decay behavior (sometimes in absolute value depending on the signs of the AR and MA coef-cients).

# 5.5.3 ACF and PACF of ARMA(p, q) Process

As in the stationarity and invertibility conditions, the ACF and PACF of an ARMA process are determined by the AR and MA components, respectively. It can therefore be shown that the ACF and PACF of an $\mathbf { A R M A } ( p , q )$ both exhibit exponential decay and/or damped sinusoid patterns, which makes the identi-cation of the order of the $\mathbf { A R M A } ( p , q )$ model relatively more dif-cult. For that, additional sample functions such as the Extended Sample ACF (ESACF), the Generalized Sample PACF (GPACF), the Inverse ACF (IACF), and canonical correlations can be used. For further information see Box, Jenkins, and Reinsel (2008), Wei (2006), Tiao and Box (1981), Tsay and Tiao (1984), and Abraham and Ledolter (1984). However, the availability of sophisticated statistical software packages such as Minitab JMP and SAS makes it possible for the practitioner to consider several different models with various orders and compare them based on the model selection criteria such as AIC, AICC, and BIC as described in Chapter 2 and residual analysis.

The theoretical values of the ACF and PACF for stationary time series are summarized in Table 5.1. The summary of the sample ACFs and PACFs of the realizations of some of the models we have covered in this chapter are given in Table 5.2, Table 5.3, and Table 5.4 for MA, AR, and ARMA models, respectively.

![](images/c9f20965bdc5e996946897410fe14e56817fa004b20ab39bd419c7ba2aac3b49.jpg)  
(a)

![](images/fc8872c849093330745165d73edf7b3687d00599d2df6c66d235e7d43842bd7b.jpg)  
(b)

![](images/d530f54161eec8e34f7b38bf44adb24072d9449613ac9d7b119202628449bac4.jpg)  
(c)

![](images/9944020be5b76eb72fa6533d490c01d20010068bbb648fa09263940a30163b41.jpg)

![](images/661706772cfd00ac8e9c361d5ede400d34379a4eb47c41364385bfd497cd2a08.jpg)  
(e)

![](images/edb5192cac4c5738610793ce5083abc5c3a4cd2688f87a39845b079d301ab12f.jpg)  
(f)   
FIGURE 5.10 Two realizations of the ARMA(1,1) model: (a) $y _ { t } = 1 6 +$ $0 . 6 y _ { t - 1 } + \varepsilon _ { t } + 0 . 8 \varepsilon _ { t - 1 }$ and (b) $y _ { t } = 1 6 - 0 . 7 y _ { t - 1 } + \varepsilon _ { t } - 0 . 6 \varepsilon _ { t - 1 }$ . (c) The ACF of (a), (d) the ACF of (b), (e) the PACF of (a), and (f) the PACF of (b).

TABLE 5.1 Behavior of Theoretical ACF and PACF for Stationary Processes   

<table><tr><td>Model</td><td>ACF</td><td>PACF</td></tr><tr><td>MA(q)</td><td>Cuts off after lag q</td><td>Exponential decay and/or damped sinusoid</td></tr><tr><td>AR(p)</td><td>Exponential decay and/or damped sinusoid</td><td>Cuts off after lag p</td></tr><tr><td>ARMA(p,q)</td><td>Exponential decay and/or damped sinusoid</td><td>Exponential decay and/or damped sinusoid</td></tr></table>

TABLE 5.2 Sample ACFs and PACFs for Some Realizations of MA(1) and MA(2) Models   

<table><tr><td>Model</td><td>Sample ACF</td><td>Sample PACF</td></tr><tr><td rowspan="2">MA(1)
yt=40+蔚t-0.8蔚t-1</td><td>Autocorrelation function
(with 5% significance limits for the autocorrelations)</td><td>Partial autocorrelation function
(with 5% significance limits for the partial autocorrelations)</td></tr><tr><td>Lag</td><td>Partial autocorrelation
Lag</td></tr><tr><td rowspan="2">yt=40+蔚t+0.8蔚t-1</td><td>Autocorrelation function
(with 5% significance limits for the autocorrelations)</td><td>Partial autocorrelation function
(with 5% significance limits for the partial autocorrelations)</td></tr><tr><td>Lag</td><td>Partial autocorrelation
Lag</td></tr></table>

MA(2)

$$
y _ {t} = 4 0 + \varepsilon_ {t} + 0. 7 \varepsilon_ {t - 1} - 0. 2 8 \varepsilon_ {t - 2}
$$

![](images/5040afb1a6ad08964f9b8a9ca7f855d93476dd97255c9650468d19860115de89.jpg)  
Autocorrelation fu nction   
(with 5% si g n ificance l i m its fo r the autoco rre lations)

![](images/9b01761da44c231620836837ec6aac4c6172e6414dd3234603e4f442d6a8255a.jpg)  
Partial autocorrelation fu nction   
(w i t h 5 % s i g n i f i ca n ce l i m i ts fo r t h e p a rt i a l a u toco r re l at i o n s )

$$
y _ {t} = 4 0 + \varepsilon_ {t} - 1. 1 \varepsilon_ {t - 1} + 0. 8 \varepsilon_ {t - 2}
$$

![](images/26af2c1d5b7ad59b42d1be6c89aa24196c2e59fa9c678cbdeaa3583784608168.jpg)  
Autocorrelation fu nction   
(with 5% si g n ificance l i m its fo r the autoco rre lations)

![](images/9ae87684b6d09e0d138e49b477857e2ac68dbaad2d8ca538d4e2fe932604ac2a.jpg)  
Partial autocorrelation fu nction   
(with 5% si g n ificance l i m its fo r the partial autoco rre lations)

TABLE 5.3 Sample ACFs and PACFs for Some Realizations of AR(1) and AR(2) Models   

<table><tr><td>Model</td><td>Sample ACF</td><td>Sample PACF</td></tr><tr><td rowspan="2">AR(1)
yt=8+0.8yt-1+蔚t</td><td>Autocorrelation function
(with 5% significance limits for the autocorrelations)</td><td>Partial autocorrelation function
(with 5% significance limits for the partial autocorrelations)</td></tr><tr><td>Lag</td><td>Partial autocorrelation
Lag</td></tr><tr><td rowspan="2">yt=8-0.8yt-1+蔚t</td><td>Autocorrelation function
(with 5% significance limits for the autocorrelations)</td><td>Partial autocorrelation function
(with 5% significance limits for the partial autocorrelations)</td></tr><tr><td>Lag</td><td>Partial autocorrelation
Lag</td></tr></table>

AR(2)

$$
y _ {t} = 4 + 0. 4 y _ {t - 1} + 0. 5 y _ {t - 2} + \varepsilon_ {t}
$$

![](images/e72c8be2781a1546d5b47744f4e8449c833ac13ad2088406d3d7de77880c3c09.jpg)  
Autocorrelation fu nction   
(with 5% si g n ificance l i m its fo r the autoco rre lations)

![](images/43dafc378b685e2960f92b07fd1043513872ca136f49ad4addaac9a82507bdad.jpg)  
Partial autocorrelation fu nction   
(w i t h 5 % s i g n i f i ca n ce l i m i ts fo r t h e p a rt i a l a u toco r re l at i o n s )

$$
y _ {t} = 4 + 0. 8 y _ {t - 1} - 0. 5 y _ {t - 2} + \varepsilon_ {t}
$$

![](images/8da0c24fc1309f6cd5a7ed27dc6cc40e6db6ae049ad1cc5b7793d60866affc64.jpg)  
Autocorrelation fu nction   
(w i t h 5 % s i g n i f i ca n ce l i m i ts fo r t h e a u toco r re l at i o n s )

![](images/5f34016f54b4555769468dffde3eb7d31369e0dd18ef13851711d96d27fa8dc9.jpg)  
Partial autocorrelation fu nction   
(with 5% si g n ificance l i m its fo r the partial autoco rre lations)

TABLE 5.4 Sample ACFs and PACFs for Some Realizations of ARMA(1,1) Models   

<table><tr><td>Model</td><td>Sample ACF</td><td>Sample PACF</td></tr><tr><td rowspan="2">ARMA(1.1)
yt=16+0.6yt-1+蔚t+0.8蔚t-1</td><td>Autocorrelation function
(with 5% significance limits for the autocorrelations)</td><td>Partial autocorrelation function
(with 5% significance limits for the partial autocorrelations)</td></tr><tr><td>Lag</td><td>Partial autocorrelation
Lag</td></tr><tr><td rowspan="2">yt=16-0.7yt-1+蔚t-0.6蔚t-1</td><td>Autocorrelation function
(with 5% significance limits for the autocorrelations)</td><td>Partial autocorrelation function
(with 5% significance limits for the partial autocorrelations)</td></tr><tr><td>Lag</td><td>Partial autocorrelation
Lag</td></tr></table>

# 5.6 NONSTATIONARY PROCESSES

It is often the case that while the processes may not have a constant level, they exhibit homogeneous behavior over time. Consider, for example, the linear trend process given in Figure 5.1c. It can be seen that different snapshots taken in time do exhibit similar behavior except for the mean level of the process. Similarly, processes may show nonstationarity in the slope as well. We will call a time series, $y _ { t }$ , homogeneous nonstationary if it is not stationary but its -rst difference, that is, $w _ { t } = y _ { t } - y _ { t - 1 } = ( 1 - B ) y _ { t }$ , or higher-order differences, $w _ { t } = ( 1 - B ) ^ { d } y _ { t }$ , produce a stationary time series. We will further call $y _ { t }$ an autoregressive integrated moving average (ARIMA) process of orders $p , d$ , and $q$ 鈥攖hat is, $\mathrm { A R I M A } ( p , d , q )$ 鈥攊f its dth difference, denoted by $w _ { t } = ( 1 - B ) ^ { d } y _ { t }$ , produces a stationary $\mathbf { A R M A } ( p , q )$ process. The term integrated is used since, for $d = 1$ , for example, we can write $y _ { t }$ as the sum (or 鈥渋ntegral鈥? of the $w _ { t }$ process as

$$
\begin{array}{l} y _ {t} = w _ {t} + y _ {t - 1} \\ = w _ {t} + w _ {t - 1} + y _ {t - 2} \tag {5.76} \\ = w _ {t} + w _ {t - 1} + \dots + w _ {1} + y _ {0} \\ \end{array}
$$

Hence an ARIMA $( p , d , q )$ can be written as

$$
\Phi (B) (1 - B) ^ {d} y _ {t} = \delta + \Theta (B) \varepsilon_ {t} \tag {5.77}
$$

Thus once the differencing is performed and a stationary time series $w _ { t } = ( 1 - B ) ^ { d } y _ { t }$ is obtained, the methods provided in the previous sections can be used to obtain the full model. In most applications -rst differencing $( d = 1 )$ and occasionally second differencing $( d = 2 )$ ) would be enough to achieve stationarity. However, sometimes transformations other than differencing are useful in reducing a nonstationary time series to a stationary one. For example, in many economic time series the variability of the observations increases as the average level of the process increases; however, the percentage of change in the observations is relatively independent of level. Therefore taking the logarithm of the original series will be useful in achieving stationarity.

# 5.6.1 Some Examples of ARIMA(p, d, q) Processes

The random walk process, ARIMA(0, 1, 0) is the simplest nonstationary model. It is given by

$$
(1 - B) y _ {t} = \delta + \varepsilon_ {t} \tag {5.78}
$$

![](images/64e90e3d585b145c4e7fbcec9c75f37723ca24a8e2d60636a395a0ac85a0d6fe.jpg)  
(a)

![](images/857de860a2a92c37bb441dec1e3fd26ba2731150bed54b3f80b7b33083df546a.jpg)  
(b)

![](images/fd64454a2d3291213eb309055840e5b849db19416356da1e6a803df3c62df20a.jpg)  
(c)

![](images/fb6f21b6010aa49ced573611bbb894dc600bbdda7f112341463932f63284a56e.jpg)  
(d)

![](images/d3bb43ebe26289f87ef4e7acf85f3649c93210638ca4543981064c14723b4b80.jpg)  
(e)

![](images/afe57dbc6f8889a63956e6b91d8eef95d877d36f08a6aed5abce95350187a322.jpg)  
  
FIGURE 5.11 A realization of the ARIMA(0, 1, 0) model, $y _ { t }$ , its -rst difference, $w _ { t }$ , and their sample ACFs and PACFs.

suggesting that -rst differencing eliminates all serial dependence and yields a white noise process.

Consider the process $y _ { t } = 2 0 + y _ { t - 1 } + \varepsilon _ { t }$ . A realization of this process together with its sample ACF and PACF are given in Figure 5.11a鈥揷. We can see that the sample ACF dies out very slowly, while the sample PACF is only signi-cant at the -rst lag. Also note that the PACF value at the -rst lag is very close to one. All this evidence suggests that the process

is not stationary. The -rst difference, $w _ { t } = y _ { t } - y _ { t - 1 }$ , and its sample ACF and PACF are shown in Figure 5.11d鈥揻. The time series plot of $w _ { t }$ implies that the -rst difference is stationary. In fact, the sample ACF and PACF do not show any signi-cant values. This further suggests that differencing the original data once 鈥渃lears out鈥?the autocorrelation. Hence the data can be modeled using the random walk model given in Eq. (5.78).

The ARIMA(0, 1, 1) process is given by

$$
(1 - B) y _ {t} = \delta + (1 - \theta B) \varepsilon_ {t} \tag {5.79}
$$

The in-nite AR representation of Eq. (5.79) can be obtained from Eq. (5.75)

$$
\pi_ {i} - \theta \pi_ {i - 1} = \left\{ \begin{array}{l l} 1, & i = 1 \\ 0, & i > 1 \end{array} \right. \tag {5.80}
$$

with $\pi _ { 0 } = - 1$ . Thus we have

$$
\begin{array}{l} y _ {t} = \alpha + \sum_ {i = 1} ^ {\infty} \pi_ {i} y _ {t - i} + \varepsilon_ {t} \\ = \alpha + (1 - \theta) \left(y _ {t - 1} + \theta y _ {t - 2} + \dots\right) + \varepsilon_ {t} \tag {5.81} \\ \end{array}
$$

This suggests that an ARIMA(0, 1, 1) (a.k.a. IMA(1, 1)) can be written as an exponentially weighted moving average (EWMA) of all past values.

Consider the time series data in Figure 5.12a. It looks like the mean of the process is changing (moving upwards) in time. Yet the change in the mean (i.e., nonstationarity) is not as obvious as in the previous example. The sample ACF plot of the data in Figure 5.12b dies down relatively slowly and the sample PACF of the data in Figure 5.12c shows two signi-cant values at lags 1 and 2. Hence we might be tempted to model this data using an AR(2) model because of the exponentially decaying ACF and signi-cant PACF at the -rst two lags. Indeed, we might even have a good -t using an AR(2) model. We should nevertheless check the roots of the associated polynomial given in Eq. (5.36) to make sure that its roots are less than 1 in absolute value. Also note that a technically stationary process will behave more and more nonstationary as the roots of the associated polynomial approach unity. For that, observe the realization of the near nonstationary process, $y _ { t } = 2 + 0 . 9 5 y _ { t - 1 } + \varepsilon _ { t }$ , given in Figure 5.1b. Based on the visual inspection, however, we may deem the process nonstationary and proceed with taking the -rst difference of the data. This is because the $\phi$ value of the AR(1) model is close to 1. Under these circumstances, where the nonstationarity

![](images/0a7388d2f6c534b49ace1201c5b8ae1649861f563f83404ebfa31e6ca2690d53.jpg)  
(a)

![](images/a118d5a649d02d1f6a6d72760034045491501b1bc1ca73fd9a9db88c6212727c.jpg)  
(b)

![](images/5fb774cf8dd9c0669a8ceaf35f107bdf77495eb4b63ba984b2a3379bad02a18a.jpg)  
(c)

![](images/d0bce1bdf0a72b80e4be735c6c15514c0b1d7f95a21b5434172e0fa0bf7f4aca.jpg)  
(d)

![](images/9c3a2937b85922101b928c71f7b972c8c9580528804456a722c9f83ad45ac435.jpg)  
(e)

![](images/d3bff2eade610aa3297d80202a3dbc93eab88d5b1469c6771436c94050f3c2ce.jpg)  
(f)   
FIGURE 5.12 A realization of the ARIMA(0, 1, 1) model, $y _ { t }$ , its -rst difference, $w _ { t }$ , and their sample ACFs and PACFs.

of the process is dubious, we strongly recommend that the analyst refer back to basic underlying process knowledge. If, for example, the process mean is expected to wander off as in some -nancial data, assuming that the process is nonstationary and proceeding with differencing the data would be more appropriate. For the data given in Figure 5.12a, its -rst difference given in Figure 5.12d looks stationary. Furthermore, its sample ACF and

PACF given in Figures 5.12e and 5.12f, respectively, suggest that an MA(1) model would be appropriate for the -rst difference since its ACF cuts off after the -rst lag and the PACF exhibits an exponential decay pattern. Hence the ARIMA(0, 1, 1) model given in Eq. (5.79) can be used for this data.

# 5.7 TIME SERIES MODEL BUILDING

A three-step iterative procedure is used to build an ARIMA model. First, a tentative model of the ARIMA class is identi-ed through analysis of historical data. Second, the unknown parameters of the model are estimated. Third, through residual analysis, diagnostic checks are performed to determine the adequacy of the model, or to indicate potential improvements. We shall now discuss each of these steps in more detail.

# 5.7.1 Model Identification

Model identi-cation efforts should start with preliminary efforts in understanding the type of process from which the data is coming and how it is collected. The process鈥?perceived characteristics and sampling frequency often provide valuable information in this preliminary stage of model identi-cation. In today鈥檚 data rich environments, it is often expected that the practitioners would be presented with 鈥渆nough鈥?data to be able to generate reliable models. It would nevertheless be recommended that 50 or preferably more observations should be initially considered. Before engaging in rigorous statistical model-building efforts, we also strongly recommend the use of 鈥渃reative鈥?plotting of the data, such as the simple time series plot and scatter plots of the time series data $y _ { t }$ versus $y _ { t - 1 } , y _ { t - 2 }$ , and so on. For the $y _ { t }$ versus $y _ { t - 1 }$ scatter plot, for example, this can be achieved in a data set of $N$ observations by plotting the -rst $N - 1$ observations versus the last $N - 1$ . Simple time series plots should be used as the preliminary assessment tool for stationarity. The visual inspection of these plots should later be con-rmed as described earlier in this chapter. If nonstationarity is suspected, the time series plot of the -rst (or dth) difference should also be considered. The unit root test by Dickey and Fuller (1979) can also be performed to make sure that the differencing is indeed needed. Once the stationarity of the time series can be presumed, the sample ACF and PACF of the time series of the original time series (or its dth difference if necessary) should be obtained. Depending on the nature of the autocorrelation, the -rst 20鈥?5 sample autocorrelations and partial autocorrelations should be suf-cient. More care should be taken of course if the process

exhibits strong autocorrelation and/or seasonality, as we will discuss in the following sections. Table 5.1 together with the $\pm 2 / \sqrt { N }$ limits can be used as a guide for identifying AR or MA models. As discussed earlier, the identi-cation of ARMA models would require more care, as both the ACF and PACF will exhibit exponential decay and/or damped sinusoid behavior.

We have already discussed that the differenced series $\left\{ w _ { t } \right\}$ may have a nonzero mean, say, $\mu _ { w }$ . At the identi-cation stage we may obtain an indication of whether or not a nonzero value of $\mu _ { w }$ is needed by comparing the sample mean of the differenced series, say, $\begin{array} { r } { \bar { w } = \sum _ { t = 1 } ^ { n - d } \left[ w / ( n - d ) \right] } \end{array}$ , with its approximate standard error. Box, Jenkins, and Reinsel (2008) give the approximate standard error of $\bar { w }$ for several useful $\mathrm { A R I M A } ( p , d , q )$ ) models.

Identi-cation of the appropriate ARIMA model requires skills obtained by experience. Several excellent examples of the identi-cation process are given in Box et al. (2008, Chap. 6), Montgomery et al. (1990), and Bisgaard and Kulahci (2011).

# 5.7.2 Parameter Estimation

There are several methods such as the methods of moments, maximum likelihood, and least squares that can be employed to estimate the parameters in the tentatively identi-ed model. However, unlike the regression models of Chapter 2, most ARIMA models are nonlinear models and require the use of a nonlinear model -tting procedure. This is usually automatically performed by sophisticated software packages such as Minitab JMP, and SAS. In some software packages, the user may have the choice of estimation method and can accordingly choose the most appropriate method based on the problem speci-cations.

# 5.7.3 Diagnostic Checking

After a tentative model has been -t to the data, we must examine its adequacy and, if necessary, suggest potential improvements. This is done through residual analysis. The residuals for an $\mathbf { A R M A } ( p , q )$ process can be obtained from

$$
\hat {\varepsilon} _ {t} = y _ {t} - \left(\hat {\delta} + \sum_ {i = 1} ^ {p} \hat {\phi} _ {i} y _ {t - i} - \sum_ {i = 1} ^ {q} \hat {\theta} _ {i} \hat {\varepsilon} _ {t - i}\right) \tag {5.82}
$$

If the speci-ed model is adequate and hence the appropriate orders $p$ and $q$ are identi-ed, it should transform the observations to a white noise process. Thus the residuals in Eq. (5.82) should behave like white noise.

Let the sample autocorrelation function of the residuals be denoted by $\left\{ r _ { e } \left( k \right) \right\}$ . If the model is appropriate, then the residual sample autocorrelation function should have no structure to identify. That is, the autocorrelation should not differ signi-cantly from zero for all lags greater than one. If the form of the model were correct and if we knew the true parameter values, then the standard error of the residual autocorrelations would be $N ^ { - 1 / 2 }$ .

Rather than considering the $r _ { e } ( k )$ terms individually, we may obtain an indication of whether the -rst $K$ residual autocorrelations considered together indicate adequacy of the model. This indication may be obtained through an approximate chi-square test of model adequacy. The test statistic is

$$
Q = (N - d) \sum_ {k = 1} ^ {K} r _ {e} ^ {2} (k) \tag {5.83}
$$

which is approximately distributed as chi-square with $K - p - q$ degrees of freedom if the model is appropriate. If the model is inadequate, the calculated value of $Q$ will be too large. Thus we should reject the hypothesis of model adequacy if $Q$ exceeds an approximate small upper tail point of the chi-square distribution with $K - p - q$ degrees of freedom. Further details of this test are in Chapter 2 and in the original reference by Box and Pierce (1970). The modi-cation of this test by Ljung and Box (1978) presented in Chapter 2 is also useful in assessing model adequacy.

# 5.7.4 Examples of Building ARIMA Models

In this section we shall present two examples of the identi-cation, estimation, and diagnostic checking process. One example presents the analysis for a stationary time series, while the other is an example of modeling a nonstationary series.

Example 5.1 Table 5.5 shows the weekly total number of loan applications in a local branch of a national bank for the last 2 years. It is suspected that there should be some relationship (i.e., autocorrelation) between the number of applications in the current week and the number of loan applications in the previous weeks. Modeling that relationship will help the management to proactively plan for the coming weeks through reliable forecasts. As always, we start our analysis with the time series plot of the data, shown in Figure 5.13.

TABLE 5.5 Weekly Total Number of Loan Applications for the Last 2 Years   

<table><tr><td>Week</td><td>Applications</td><td>Week</td><td>Applications</td><td>Week</td><td>Applications</td><td>Week</td><td>Applications</td></tr><tr><td>1</td><td>71</td><td>27</td><td>62</td><td>53</td><td>66</td><td>79</td><td>63</td></tr><tr><td>2</td><td>57</td><td>28</td><td>77</td><td>54</td><td>71</td><td>80</td><td>61</td></tr><tr><td>3</td><td>62</td><td>29</td><td>76</td><td>55</td><td>59</td><td>81</td><td>73</td></tr><tr><td>4</td><td>64</td><td>30</td><td>88</td><td>56</td><td>57</td><td>82</td><td>72</td></tr><tr><td>5</td><td>65</td><td>31</td><td>71</td><td>57</td><td>66</td><td>83</td><td>65</td></tr><tr><td>6</td><td>67</td><td>32</td><td>72</td><td>58</td><td>51</td><td>84</td><td>70</td></tr><tr><td>7</td><td>65</td><td>33</td><td>66</td><td>59</td><td>59</td><td>85</td><td>54</td></tr><tr><td>8</td><td>82</td><td>34</td><td>65</td><td>60</td><td>56</td><td>86</td><td>63</td></tr><tr><td>9</td><td>70</td><td>35</td><td>73</td><td>61</td><td>57</td><td>87</td><td>62</td></tr><tr><td>10</td><td>74</td><td>36</td><td>76</td><td>62</td><td>55</td><td>88</td><td>60</td></tr><tr><td>11</td><td>75</td><td>37</td><td>81</td><td>63</td><td>53</td><td>89</td><td>67</td></tr><tr><td>12</td><td>81</td><td>38</td><td>84</td><td>64</td><td>74</td><td>90</td><td>59</td></tr><tr><td>13</td><td>71</td><td>39</td><td>68</td><td>65</td><td>64</td><td>91</td><td>74</td></tr><tr><td>14</td><td>75</td><td>40</td><td>63</td><td>66</td><td>70</td><td>92</td><td>61</td></tr><tr><td>15</td><td>82</td><td>41</td><td>66</td><td>67</td><td>74</td><td>93</td><td>61</td></tr><tr><td>16</td><td>74</td><td>42</td><td>71</td><td>68</td><td>69</td><td>94</td><td>52</td></tr><tr><td>17</td><td>78</td><td>43</td><td>67</td><td>69</td><td>64</td><td>95</td><td>55</td></tr><tr><td>18</td><td>75</td><td>44</td><td>69</td><td>70</td><td>68</td><td>96</td><td>61</td></tr><tr><td>19</td><td>73</td><td>45</td><td>63</td><td>71</td><td>64</td><td>97</td><td>56</td></tr><tr><td>20</td><td>76</td><td>46</td><td>61</td><td>72</td><td>70</td><td>98</td><td>61</td></tr><tr><td>21</td><td>66</td><td>47</td><td>68</td><td>73</td><td>73</td><td>99</td><td>60</td></tr><tr><td>22</td><td>69</td><td>48</td><td>75</td><td>74</td><td>59</td><td>100</td><td>65</td></tr><tr><td>23</td><td>63</td><td>49</td><td>66</td><td>75</td><td>68</td><td>101</td><td>55</td></tr><tr><td>24</td><td>76</td><td>50</td><td>81</td><td>76</td><td>59</td><td>102</td><td>61</td></tr><tr><td>25</td><td>65</td><td>51</td><td>72</td><td>77</td><td>66</td><td>103</td><td>59</td></tr><tr><td>26</td><td>73</td><td>52</td><td>77</td><td>78</td><td>63</td><td>104</td><td>63</td></tr></table>

Figure 5.13 shows that the weekly data tend to have short runs and that the data seem to be indeed autocorrelated. Next, we visually inspect the stationarity. Although there might be a slight drop in the mean for the second year (weeks 53鈥?04), in general, it seems to be safe to assume stationarity.

We now look at the sample ACF and PACF plots in Figure 5.14. Here are possible interpretations of the ACF plot:

1. It cuts off after lag 2 (or maybe even 3), suggesting an MA(2) (or MA(3)) model.   
2. It has an (or a mixture of) exponential decay(s) pattern suggesting an $\operatorname { A R } ( p )$ model.

To resolve the conict, consider the sample PACF plot. For that, we have only one interpretation; it cuts off after lag 2. Hence we use the second

![](images/0c98cae4065a36e31c6380b58c86819d8e197f0fd42dcaef8500cbe47b0a2839.jpg)  
FIGURE 5.13 Time series plot of the weekly total number of loan applications.

interpretation of the sample ACF plot and assume that the appropriate model to -t is the AR(2) model.

Table 5.6 shows the Minitab output for the AR(2) model. The parameter estimates are $\hat { \phi } _ { 1 } = 0 . 2 7$ and $\hat { \phi } _ { 2 } = \bar { 0 } . 4 2$ , and they turn out to be signi-cant (see the $P$ -values).

MSE is calculated to be 39.35. The modi-ed Box鈥揚ierce test suggests that there is no autocorrelation left in the residuals. We can also see this in the ACF and PACF plots of the residuals in Figure 5.15.

As the last diagnostic check, we have the 4-in-1 residual plots in Figure 5.16 provided by Minitab: Normal Probability Plot, Residuals versus

![](images/74f2faf43a84f25c6d73bb9b8098b625eee1522d7494b0ab387aa7bf3757125e.jpg)  
(a)

![](images/78b065ce941512dc97338aa186d5348ea9a79c48f99fb1b5cf6a53193626420d.jpg)  
  
FIGURE 5.14 ACF and PACF for the weekly total number of loan applications.

TABLE 5.6 Minitab Output for the AR(2) Model for the Loan Application Data   

<table><tr><td colspan="6">Final Estimates of Parameters</td></tr><tr><td>Type</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P</td><td></td></tr><tr><td>AR 1</td><td>0.2682</td><td>0.0903</td><td>2.97</td><td>0.004</td><td></td></tr><tr><td>AR 2</td><td>0.4212</td><td>0.0908</td><td>4.64</td><td>0.000</td><td></td></tr><tr><td>Constant</td><td>20.7642</td><td>0.6157</td><td>33.73</td><td>0.000</td><td></td></tr><tr><td>Mean</td><td>66.844</td><td>1.982</td><td></td><td></td><td></td></tr><tr><td colspan="6">Number of observations: 104</td></tr><tr><td colspan="6">Residuals: SS = 3974.30 (backforecasts excluded)</td></tr><tr><td colspan="6">MS = 39.35 DF = 101</td></tr><tr><td colspan="6">Modified Box-Pierce (Ljung-Box) Chi-Square statistic</td></tr><tr><td>Lag</td><td>12</td><td>24</td><td>36</td><td>48</td><td></td></tr><tr><td>Chi-Square</td><td>6.2</td><td>16.0</td><td>24.9</td><td>32.0</td><td></td></tr><tr><td>DF</td><td>9</td><td>21</td><td>33</td><td>45</td><td></td></tr><tr><td>P-Value</td><td>0.718</td><td>0.772</td><td>0.843</td><td>0.927</td><td></td></tr></table>

Fitted Value, Histogram of the Residuals, and Time Series Plot of the Residuals. They indicate that the -t is indeed acceptable.

Figure 5.17 shows the actual data and the -tted values. It looks like the -tted values smooth out the highs and lows in the data.

Note that, in this example, we often and deliberately used 鈥渧ague鈥?words such as 鈥渟eems鈥?or 鈥渓ooks like.鈥?It should be clear by now that

![](images/99d43fdfc73ba722ebecb4de7f3b415d308940dd0c0a6213aad5ba5a91d11d93.jpg)  
(a)

![](images/84848349aace607f7db9e44e8ffef8afde0bb7b23dd28a9d52f0b1bb5d3ee993.jpg)  
(b)   
FIGURE 5.15 The sample ACF and PACF of the residuals for the AR(2) model in Table 5.6.

![](images/48a3e0e544b6bba52fbb646bb1fd83b67a88d506b641a140c59d238ac842e3fe.jpg)

![](images/6c6d540f19725a94412e75fd01c6a38694d6a0dd8eba734c59f24e1dfa150ae1.jpg)

![](images/fd428588abad6ad6b3d3b18d54ab99cb356289648ab473412154e9b684e48947.jpg)

![](images/06ff7ce48d41ea0dacde35edc667e7dc7cde908e8fbb26234158e7da0330c821.jpg)  
FIGURE 5.16 Residual plots for the AR(2) model in Table 5.6.

![](images/590f08a66455da215fc5f7bbd6dad3f959f35d0ac64a444157351829eb6f167b.jpg)  
FIGURE 5.17 Time series plot of the actual data and -tted values for the AR(2) model in Table 5.6.

the methodology presented in this chapter has a very sound theoretical foundation. However, as in any modeling effort, we should also keep in mind the subjective component of model identi-cation. In fact, as we mentioned earlier, time series model -tting can be seen as a mixture of science and art and can best be learned by practice and experience. The next example will illustrate this point further.

Example 5.2 Consider the Dow Jones Index data from Chapter 4. A time series plot of the data is given in Figure 5.18. The process shows signs of nonstationarity with changing mean and possibly variance.

Similarly, the slowly decreasing sample ACF and sample PACF with signi-cant value at lag 1, which is close to 1 in Figure 5.19, con-rm that indeed the process can be deemed nonstationary. On the other hand, one might argue that the signi-cant sample PACF value at lag 1 suggests that the AR(1) model might also -t the data well. We will consider this interpretation -rst and -t an AR(1) model to the Dow Jones Index data.

Table 5.7 shows the Minitab output for the AR(1) model. Although it is close to 1, the AR(1) model coef-cient estimate $\hat { \phi } = 0 . 9 0 4 5$ turns out to be quite signi-cant and the modi-ed Box鈥揚ierce test suggests that there is no autocorrelation left in the residuals. This is also con-rmed by the sample ACF and PACF plots of the residuals given in Figure 5.20.

![](images/33b8a33bbe89a13df6d1ebe094863c1ffc0377bc949fb7d19fa4acda620ce9b4.jpg)  
FIGURE 5.18 Time series plot of the Dow Jones Index from June 1999 to June 2006.

![](images/b95e8be69a90a0b6931aa244a9cc985c0be176a64fd824581e1cdaacc84d12f0.jpg)

![](images/34d129ff3aead73911c27d4190a3e0cbba6b99a6c911f5ba174562986da9e788.jpg)  
FIGURE 5.19 Sample ACF and PACF of the Dow Jones Index.

TABLE 5.7 Minitab Output for the AR(1) Model for the Dow Jones Index   

<table><tr><td colspan="5">Final Estimates of Parameters</td></tr><tr><td>Type</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P</td></tr><tr><td>AR 1</td><td>0.9045</td><td>0.0500</td><td>18.10</td><td>0.000</td></tr><tr><td>Constant</td><td>984.94</td><td>44.27</td><td>22.25</td><td>0.000</td></tr><tr><td>Mean</td><td>10309.9</td><td>463.4</td><td></td><td></td></tr><tr><td colspan="5">Number of observations: 85</td></tr><tr><td colspan="5">Residuals: SS = 13246015 (backforecasts excluded)</td></tr><tr><td colspan="5">MS = 159591 DF = 83</td></tr><tr><td colspan="5">Modified Box-Pierce (Ljung-Box) Chi-Square statistic</td></tr><tr><td>Lag</td><td>12</td><td>24</td><td>36</td><td>48</td></tr><tr><td>Chi-Square</td><td>2.5</td><td>14.8</td><td>21.4</td><td>29.0</td></tr><tr><td>DF</td><td>10</td><td>22</td><td>34</td><td>46</td></tr><tr><td>P-Value</td><td>0.991</td><td>0.872</td><td>0.954</td><td>0.977</td></tr></table>

![](images/fd3aa8c9b111a7e0771341d626ca13bc9c06d6af364c791ad3544548bd181375.jpg)

![](images/3a65b2ded4086df369aebab6a6f53dce785d874567556cf148abea86ff3625d7.jpg)  
FIGURE 5.20 Sample ACF and PACF of the residuals from the AR(1) model for the Dow Jones Index data.

![](images/04c39d38da594f942885b1da4ce71100b397af11b8b2113a63464eb1b9014153.jpg)

![](images/71104d15eccaf4937cf647ea2bafd99660b8893dac8593b0ea4a89913a31b856.jpg)

![](images/8dd9f30ae78be7824a0020f6d3817b611f27a7749e93a16f89b0f34e1d6a95dd.jpg)

![](images/65a56e0ad174f5a9bb83bcadf9791a458db4ffe493d41b3e3b058669c4194f1b.jpg)  
FIGURE 5.21 Residual plots from the AR(1) model for the Dow Jones Index data.

The only concern in the residual plots in Figure 5.21 is in the changing variance observed in the time series plot of the residuals. This is indeed a very important issue since it violates the constant variance assumption. We will discuss this issue further in Section 7.3 but for illustration purposes we will ignore it in this example.

Overall it can be argued that an AR(1) model provides a decent -t to the data. However, we will now consider the earlier interpretation and assume that the Dow Jones Index data comes from a nonstationary process. We then take the -rst difference of the data as shown in Figure 5.22. While there are once again some serious concerns about changing variance, the level of the -rst difference remains the same. If we ignore the changing variance and look at the sample ACF and PACF plots given in Figure 5.23, we may conclude that the -rst difference is in fact white noise. That is, since these plots do not show any sign of signi-cant autocorrelation, a model we may consider for the Dow Jones Index data would be the random walk model, ARIMA(0, 1, 0).

Now the analyst has to decide between the two models: AR(1) and ARIMA(0, 1, 0). One can certainly use some of the criteria we discussed in Section 2.6.2 to choose one of these models. Since these two models are fundamentally quite different, we strongly recommend that the analyst use the subject matter/process knowledge as much as possible. Do we expect

![](images/a7d4799469c16dc067cd28cf337e186b9833210e75b8edf3355b36947866ccdd.jpg)  
FIGURE 5.22 Time series plot of the -rst difference $w ( t )$ of the Dow Jones Index data.

a -nancial index such as the Dow Jones Index to wander about a -xed mean as implied by the AR(1)? In most cases involving -nancial data, the answer would be no. Hence a model such as ARIMA(0, 1, 0) that takes into account the inherent nonstationarity of the process should be preferred. However, we do have a problem with the proposed model. A random walk model means that the price changes are random and cannot be predicted. If we have a higher price today compared to yesterday, that would have no bearing on the forecasts tomorrow. That is, tomorrow鈥檚 price can be higher or lower than today鈥檚 and we would have no way to forecast it effectively. This further suggests that the best forecast for tomorrow鈥檚 price is in fact the price we have today. This is obviously not a reliable and effective forecasting model. This very same issue of the random walk models for

![](images/ba9564627dba24af0c22c8b5d40198a74ad6bb1348ca58a9cff5c3efacfc2e78.jpg)

![](images/9977c7bf58c66737d664b9100c57d213c3e897b7475c7066b8cfec1fd41d75db.jpg)  
FIGURE 5.23 Sample ACF and PACF plots of the -rst difference of the Dow Jones Index data.

-nancial data has been discussed in great detail in the literature. We simply used this data to illustrate that in time series model -tting we can end up with fundamentally different models that will -t the data equally well. At this point, process knowledge can provide the needed guidance in picking the 鈥渞ight鈥?model.

It should be noted that, in this example, we tried to keep the models simple for illustration purposes. Indeed, a more thorough analysis would (and should) pay close attention to the changing variance issue. In fact, this is a very common concern particularly when dealing with -nancial data. For that, we once again refer the reader to Section 7.3.

# 5.8 FORECASTING ARIMA PROCESSES

Once an appropriate time series model has been -t, it may be used to generate forecasts of future observations. If we denote the current time by $T$ , the forecast for $y _ { T + \tau }$ is called the $\tau$ -period-ahead forecast and denoted by $\hat { y } _ { T + \tau } ( T )$ . The standard criterion to use in obtaining the best forecast is the mean squared error for which the expected value of the squared forecast errors, $E [ ( y _ { T + \tau } - \hat { y } _ { T + \tau } ( T ) ) ^ { 2 } ] = E [ e _ { T } ( \tau ) ^ { 2 } ]$ , is minimized. It can be shown that the best forecast in the mean square sense is the conditional expectation of $y _ { T + \tau }$ given current and previous observations, that is, $y _ { T }$ , $y _ { T - 1 } , \dots$

$$
\hat {y} _ {T + \tau} (T) = E \left[ y _ {T + \tau} \mid y _ {T}, y _ {T - 1}, \dots \right] \tag {5.84}
$$

Consider, for example, an ARIMA $( p , d , q )$ process at time $T + \tau$ (i.e., $\tau$ period in the future):

$$
y _ {T + \tau} = \delta + \sum_ {i = 1} ^ {p + d} \phi_ {i} y _ {T + \tau - i} + \varepsilon_ {T + \tau} - \sum_ {i = 1} ^ {q} \theta_ {i} \varepsilon_ {T + \tau - i} \tag {5.85}
$$

Further consider its in-nite MA representation,

$$
y _ {T + \tau} = \mu + \sum_ {i = 0} ^ {\infty} \psi_ {i} \varepsilon_ {T + \tau - i} \tag {5.86}
$$

We can partition Eq. (5.86) as

$$
y _ {T + \tau} = \mu + \sum_ {i = 0} ^ {\tau - 1} \psi_ {i} \varepsilon_ {T + \tau - i} + \sum_ {i = \tau} ^ {\infty} \psi_ {i} \varepsilon_ {T + \tau - i} \tag {5.87}
$$

In this partition, we can clearly see that the $\textstyle \sum _ { i = 0 } ^ { \tau - 1 } { \psi _ { i } \varepsilon _ { T + \tau - i } }$ component involves the future errors, whereas the $\textstyle \sum _ { i = \tau } ^ { \infty } \psi _ { i } \varepsilon _ { T + \tau - i }$ i T+饾湉鈭抜 component involves the present and past errors. From the relationship between the current and past observations and the corresponding random shocks as well as the fact that the random shocks are assumed to have mean zero and to be independent, we can show that the best forecast in the mean square sense is

$$
\hat {y} _ {T + \tau} (T) = E [ y _ {T + \tau} | y _ {T}, y _ {T - 1}, \dots ] = \mu + \sum_ {i = \tau} ^ {\infty} \psi_ {i} \varepsilon_ {T + \tau - i} \tag {5.88}
$$

since

$$
E \left[ \varepsilon_ {T + \tau - i} \big | y _ {T}, y _ {T - 1}, \ldots \right] = \left\{ \begin{array}{l l} 0 & \text {i f} \quad i <   \tau \\ \varepsilon_ {T + \tau - i} & \text {i f} \quad i \geq \tau \end{array} \right.
$$

Subsequently, the forecast error is calculated from

$$
e _ {T} (\tau) = y _ {T + \tau} - \hat {y} _ {T + \tau} (T) = \sum_ {i = 0} ^ {\tau - 1} \psi_ {i} \varepsilon_ {T + \tau - i} \tag {5.89}
$$

Since the forecast error in Eq. (5.89) is a linear combination of random shocks, we have

$$
E \left[ e _ {T} (\tau) \right] = 0 \tag {5.90}
$$

$$
\begin{array}{l} \mathrm {V a r} \left[ e _ {T} (\tau) \right] = \mathrm {V a r} \left[ \sum_ {i = 0} ^ {\tau - 1} \psi_ {i} \varepsilon_ {T + \tau - i} \right] = \sum_ {i = 0} ^ {\tau - 1} \psi_ {i} ^ {2} \mathrm {V a r} (\varepsilon_ {T + \tau - i}) \\ = \sigma^ {2} \sum_ {i = 0} ^ {\tau - 1} \psi_ {i} ^ {2} \tag {5.91} \\ = \sigma^ {2} (\tau), \quad \tau = 1, 2, \ldots \\ \end{array}
$$

It should be noted that the variance of the forecast error gets bigger with increasing forecast lead times $\tau$ . This intuitively makes sense as we should expect more uncertainty in our forecasts further into the future. Moreover, if the random shocks are assumed to be normally distributed, $N ( 0 , \sigma ^ { 2 } )$ , then the forecast errors will also be normally distributed with $N ( 0 , \sigma ^ { 2 } ( \tau ) )$ . We

can then obtain the $1 0 0 ( 1 - \alpha )$ percent prediction intervals for the future observations from

$$
P \left(\hat {y} _ {T + \tau} (T) - z _ {\alpha / 2} \sigma (\tau) <   y _ {T + \tau} <   \hat {y} _ {T + \tau} (T) + z _ {\alpha / 2} \sigma (\tau)\right) = 1 - \alpha \tag {5.92}
$$

where $z _ { \alpha / 2 }$ is the upper $\alpha / 2$ percentile of the standard normal distribution, $N ( 0 , 1 )$ . Hence the $1 0 0 ( 1 - \alpha )$ percent prediction interval for $y _ { T + \tau }$ is

$$
\hat {y} _ {T + \tau} (T) \pm z _ {\alpha / 2} \sigma (\tau) \tag {5.93}
$$

There are two issues with the forecast equation in (5.88). First, it involves in-nitely many terms in the past. However, in practice, we will only have a -nite amount of data. For a suf-ciently large data set, this can be overlooked. Second, Eq. (5.88) requires knowledge of the magnitude of random shocks in the past, which is unrealistic. A solution to this problem is to 鈥渆stimate鈥?the past random shocks through one-step-ahead forecasts. For the ARIMA model we can calculate

$$
\hat {\varepsilon} _ {t} = y _ {t} - \left[ \delta + \sum_ {i = 1} ^ {p + d} \phi_ {i} y _ {t - i} - \sum_ {i = 1} ^ {q} \theta_ {i} \hat {\varepsilon} _ {t - i} \right] \tag {5.94}
$$

recursively by setting the initial values of the random shocks to zero for $t < p + d + 1 .$ . For more accurate results, these initial values together with the $y _ { t }$ for $t \leq 0$ can also be obtained using back-forecasting. For further details, see Box, Jenkins, and Reinsel (2008).

As an illustration consider forecasting the ARIMA(1, 1, 1) process

$$
(1 - \phi B) (1 - B) y _ {T + \tau} = (1 - \theta B) \varepsilon_ {T + \tau} \tag {5.95}
$$

We will consider two of the most commonly used approaches:

1. As discussed earlier, this approach involves the in-nite MA representation of the model in Eq. (5.95), also known as the random shock form of the model:

$$
\begin{array}{l} y _ {T + \tau} = \sum_ {i = 0} ^ {\infty} \psi_ {i} \varepsilon_ {T + \tau - i} \tag {5.96} \\ = \psi_ {0} \varepsilon_ {T + \tau} + \psi_ {1} \varepsilon_ {T + \tau - 1} + \psi_ {2} \varepsilon_ {T + \tau - 2} + \dots \\ \end{array}
$$

Hence the $\tau$ -step-ahead forecast can be calculated from

$$
\hat {y} _ {T + \tau} (T) = \psi_ {\tau} \varepsilon_ {T} + \psi_ {\tau + 1} \varepsilon_ {T - 1} + \dots \tag {5.97}
$$

The weights $\psi _ { i }$ can be calculated from

$$
\left(\psi_ {0} + \psi_ {1} B + \dots\right) (1 - \phi B) (1 - B) = (1 - \theta B) \tag {5.98}
$$

and the random shocks can be estimated using the one-step-ahead forecast error; for example, $\varepsilon _ { T }$ can be replaced by $e _ { T - 1 } ( 1 ) = y _ { T } -$ $\hat { y } _ { T } ( T - 1 )$ .

2. Another approach that is often employed in practice is to use difference equations as given by

$$
y _ {T + \tau} = (1 + \phi) y _ {T + \tau - 1} - \phi y _ {T + \tau - 2} + \varepsilon_ {T + \tau} - \theta \varepsilon_ {T + \tau - 1} \tag {5.99}
$$

For $\tau = 1$ , the best forecast in the mean squared error sense is

$$
\hat {y} _ {T + 1} (T) = E \left[ y _ {T + 1} \mid y _ {T}, y _ {T - 1}, \dots \right] = (1 + \phi) y _ {T} - \phi y _ {T - 1} - \theta e _ {T} (1) \tag {5.100}
$$

We can further show that for lead times $\tau > 2$ , the forecast is

$$
\hat {y} _ {T + \tau} (T) = (1 - \phi) \hat {y} _ {T + \tau - 1} (T) - \phi \hat {y} _ {T + \tau - 2} (T) \tag {5.101}
$$

Prediction intervals for forecasts of future observations at time period $T + \tau$ are found using equation 5.87. However, in using Equation 5.87 the $\psi$ weights must be found in order to compute the variance (or standard deviation) of the $\tau$ -step ahead forecast error. The $\psi$ weights for the general $\mathrm { A R I M A } ( p , d , q )$ model may be obtained by equating like powers of $B$ i n the expansion of

$$
\begin{array}{l} (\psi_ {0} + \psi_ {1} B + \psi_ {2} B ^ {2} + \dots) (1 - \phi_ {1} B - \phi_ {2} B ^ {2} - \dots - \phi_ {p} B ^ {p}) (1 - B) ^ {d} \\ = \left(1 - \theta_ {1} B - \theta_ {2} B ^ {2} - \dots - \theta_ {q} B ^ {q}\right) \\ \end{array}
$$

and solving for the $\psi$ weights. We now illustrate this with three examples.

Example 5.3 The ARMA(1, 1) Model For the ARMA(1, 1) model the product of the required polynomials is

$$
(\psi_ {0} + \psi_ {1} B + \psi_ {2} B ^ {2} + \dots) (1 - \phi B) = (1 - \theta B)
$$

Equating like power of $B$ we -nd that

$$
\begin{array}{l} B ^ {0} \colon \psi_ {0} = 1 \\ B ^ {1} \colon \psi_ {1} - \phi = - \theta , \text {o r} \psi_ {1} = \phi - \theta \\ B ^ {2} \colon \psi_ {2} - \phi \psi_ {1} = 0, \text {o r} \psi_ {2} = \phi (\phi - \theta) \\ \end{array}
$$

In general, we can show for the ARMA(1,1) model that $\psi _ { j } = \phi ^ { j - 1 } ( \phi - \theta )$ .

Example 5.4 The AR(2) Model For the AR(2) model the product of the required polynomials is

$$
(\psi_ {0} + \psi_ {1} B + \psi_ {2} B ^ {2} + \dots) (1 - \phi_ {1} B - \phi_ {2} B ^ {2}) = 1
$$

Equating like power of $B$ , we -nd that

$$
\begin{array}{l} B ^ {0} \colon \psi_ {0} = 1 \\ B ^ {1} \colon \psi_ {1} - \phi_ {1} = 0, \text {o r} \psi_ {1} = \phi_ {1} \\ B ^ {2} \colon \psi_ {2} - \phi_ {1} \psi_ {1} - \phi_ {2} = 0, \text {o r} \psi_ {2} = \phi_ {1} \psi_ {1} + \phi_ {2} \\ \end{array}
$$

In general, we can show for the AR(2) model that $\psi _ { j } = \phi _ { 1 } \psi _ { j - 1 } + \phi _ { 2 } \psi _ { j - 2 }$

Example 5.5 The ARIMA(0, 1, 1) or IMA(1,1) Model Now consider a nonstationary model, the IMA(1, 1) model. The product of the required polynomials for this model is

$$
\left(\psi_ {0} + \psi_ {1} B + \psi_ {2} B ^ {2} + \dots\right) (1 - B) = (1 - \theta B)
$$

It is straightforward to show that the $\psi$ weights for this model are

$$
\begin{array}{l} \psi_ {0} = 1 \\ \psi_ {1} = 1 - \theta \\ \psi_ {j} = \psi_ {j - 1}, j = 2, 3, \dots \\ \end{array}
$$

Notice that the prediction intervals will increase in length rapidly as the forecast lead time increases. This is typical of nonstationary ARIMA models. It implies that these models may not be very effective in forecasting more than a few periods ahead.

![](images/95f1ab66585e0219046610f1e4ef00dfa21b0fe546ca1012eb8dcaa353ba8902.jpg)  
FIGURE 5.24 Time series plot and forecasts for the weekly loan application data.

Example 5.6 Consider the loan applications data given in Table 5.5. Now assume that the manager wants to make forecasts for the next 3 months (12 weeks) using the AR(2) model from Example 5.1. Hence at the 104th week we need to make 1-step, 2-step, 鈥?, 12-step-ahead predictions, which are obtained and plotted using Minitab in Figure 5.24 together with the $9 5 \%$ prediction interval.

Table 5.8 shows the output from JMP for -tting an AR(2) model to the weekly loan application data. In addition to the sample ACF and PACF, JMP provides the model -tting information including the estimates of the model parameters, the forecasts for 10 periods into the future and the associated prediction intervals, and the residual autocorrelation and PACF. The AR(2) model is an excellent -t to the data.

# 5.9 SEASONAL PROCESSES

Time series data may sometimes exhibit strong periodic patterns. This is often referred to as the time series having a seasonal behavior. This mostly

occurs when data is taken in speci-c intervals鈥攎onthly, weekly, and so on. One way to represent such data is through an additive model where the process is assumed to be composed of two parts,

$$
y _ {t} = S _ {t} + N _ {t} \tag {5.102}
$$

where $S _ { t }$ is the deterministic component with periodicity $s$ and $N _ { t }$ is the stochastic component that may be modeled as an ARMA process. In that, $y _ { t }$ can be seen as a process with predictable periodic behavior with some noise sprinkled on top of it. Since the $S _ { t }$ is deterministic and has periodicity $s$ , we have $S _ { t } = S _ { t + s }$ or

$$
S _ {t} - S _ {t - s} = \left(1 - B ^ {s}\right) S _ {t} = 0 \tag {5.103}
$$

Applying the $( 1 - B ^ { s } )$ operator to Eq. (5.102), we have

$$
\underbrace {(1 - B ^ {s}) y _ {t}} _ {\equiv w _ {t}} = \underbrace {(1 - B ^ {s}) S _ {t}} _ {= 0} + (1 - B ^ {s}) N _ {t} \tag {5.104}
$$

$$
w _ {t} = (1 - B ^ {s}) N _ {t}
$$

![](images/1cf9b6d27aaca339cf7bf02bdefe5f5bc3bd28e1c4e5503f2c899e9a2056ca35.jpg)  
TABLE 5.8 JMP AR(2) Output for the Loan Application Data   
Time series y(t)

Mean 67.067308

Std 7.663932

N 104

Zero Mean ADF -0.695158

Single Mean ADF -6.087814

Trend ADF -7.396174

TABLE 5.8 (Continued)   

<table><tr><td colspan="4">Time series basic diagnostics</td></tr><tr><td>Lag</td><td>AutoCorr plot autocorr</td><td>Ljung-Box Q</td><td>p-Value</td></tr><tr><td>0</td><td>1.0000</td><td></td><td></td></tr><tr><td>1</td><td>0.4617</td><td>22.8186</td><td>&lt;.0001</td></tr><tr><td>2</td><td>0.5314</td><td>53.3428</td><td>&lt;.0001</td></tr><tr><td>3</td><td>0.2915</td><td>62.6167</td><td>&lt;.0001</td></tr><tr><td>4</td><td>0.2682</td><td>70.5487</td><td>&lt;.0001</td></tr><tr><td>5</td><td>0.2297</td><td>76.4252</td><td>&lt;.0001</td></tr><tr><td>6</td><td>0.1918</td><td>80.5647</td><td>&lt;.0001</td></tr><tr><td>7</td><td>0.2484</td><td>87.5762</td><td>&lt;.0001</td></tr><tr><td>8</td><td>0.1162</td><td>89.1255</td><td>&lt;.0001</td></tr><tr><td>9</td><td>0.1701</td><td>92.4847</td><td>&lt;.0001</td></tr><tr><td>10</td><td>0.0565</td><td>92.8587</td><td>&lt;.0001</td></tr><tr><td>11</td><td>0.0716</td><td>93.4667</td><td>&lt;.0001</td></tr><tr><td>12</td><td>0.1169</td><td>95.1040</td><td>&lt;.0001</td></tr><tr><td>13</td><td>0.1151</td><td>96.7080</td><td>&lt;.0001</td></tr><tr><td>14</td><td>0.2411</td><td>103.829</td><td>&lt;.0001</td></tr><tr><td>15</td><td>0.1137</td><td>105.430</td><td>&lt;.0001</td></tr><tr><td>16</td><td>0.2540</td><td>113.515</td><td>&lt;.0001</td></tr><tr><td>17</td><td>0.1279</td><td>115.587</td><td>&lt;.0001</td></tr><tr><td>18</td><td>0.2392</td><td>122.922</td><td>&lt;.0001</td></tr><tr><td>19</td><td>0.1138</td><td>124.603</td><td>&lt;.0001</td></tr><tr><td>20</td><td>0.1657</td><td>128.206</td><td>&lt;.0001</td></tr><tr><td>21</td><td>0.0745</td><td>128.944</td><td>&lt;.0001</td></tr><tr><td>22</td><td>0.1320</td><td>131.286</td><td>&lt;.0001</td></tr><tr><td>23</td><td>0.0708</td><td>131.968</td><td>&lt;.0001</td></tr><tr><td>24</td><td>0.0338</td><td>132.125</td><td>&lt;.0001</td></tr><tr><td>25</td><td>0.0057</td><td>132.130</td><td>&lt;.0001</td></tr><tr><td>Lag</td><td colspan="3">Partial plot partial</td></tr><tr><td>Lag</td><td colspan="3">AutoCorr plot autocorr</td></tr><tr><td>0</td><td>1.0000</td><td rowspan="26">Ljung-Box Q</td><td rowspan="26">p-Value</td></tr><tr><td>1</td><td>0.4617</td></tr><tr><td>2</td><td>0.4045</td></tr><tr><td>3</td><td>-0.0629</td></tr><tr><td>4</td><td>-0.0220</td></tr><tr><td>5</td><td>0.0976</td></tr><tr><td>6</td><td>0.0252</td></tr><tr><td>7</td><td>0.1155</td></tr><tr><td>8</td><td>-0.1017</td></tr><tr><td>9</td><td>0.0145</td></tr><tr><td>10</td><td>-0.0330</td></tr><tr><td>11</td><td>-0.0250</td></tr><tr><td>12</td><td>0.1349</td></tr><tr><td>13</td><td>0.0488</td></tr><tr><td>14</td><td>0.1489</td></tr><tr><td>15</td><td>-0.0842</td></tr><tr><td>16</td><td>0.1036</td></tr><tr><td>17</td><td>0.0105</td></tr><tr><td>18</td><td>0.0830</td></tr><tr><td>19</td><td>-0.0938</td></tr><tr><td>20</td><td>0.0052</td></tr><tr><td>21</td><td>-0.0927</td></tr><tr><td>22</td><td>0.1149</td></tr><tr><td>23</td><td>-0.0645</td></tr><tr><td>24</td><td>-0.0473</td></tr><tr><td>25</td><td>-0.0742</td></tr></table>

(continued)

TABLE 5.8 (Continued)   

<table><tr><td colspan="8">Model Comparison</td></tr><tr><td>Model</td><td>DF</td><td>Variance</td><td>AIC</td><td>SBC</td><td>RSquare</td><td>-2LogLH</td><td></td></tr><tr><td>AR(2)</td><td>101</td><td>39.458251</td><td>680.92398</td><td>688.85715</td><td>0.343</td><td>674.92398</td><td></td></tr><tr><td colspan="8">Model: AR(2)</td></tr><tr><td colspan="8">Model Summary</td></tr><tr><td>DF</td><td></td><td></td><td></td><td></td><td></td><td>101</td><td></td></tr><tr><td>Sum of Squared Errors</td><td></td><td></td><td></td><td></td><td>3985.28336</td><td></td><td></td></tr><tr><td>Variance Estimate</td><td></td><td></td><td></td><td></td><td>39.4582511</td><td></td><td></td></tr><tr><td>Standard Deviation</td><td></td><td></td><td></td><td></td><td>6.2815803</td><td></td><td></td></tr><tr><td>Akaike&#x27;s &#x27;A&#x27; Information Criterion</td><td></td><td></td><td></td><td></td><td>680.923978</td><td></td><td></td></tr><tr><td>Schwarz&#x27;s Bayesian Criterion</td><td></td><td></td><td></td><td></td><td>688.857151</td><td></td><td></td></tr><tr><td>RSquare</td><td></td><td></td><td></td><td></td><td>0.34278547</td><td></td><td></td></tr><tr><td>RSquare Adj</td><td></td><td></td><td></td><td></td><td>0.32977132</td><td></td><td></td></tr><tr><td>MAPE</td><td></td><td></td><td></td><td></td><td>7.37857799</td><td></td><td></td></tr><tr><td>MAE</td><td></td><td></td><td></td><td></td><td>4.91939717</td><td></td><td></td></tr><tr><td>-2LogLikelihood</td><td></td><td></td><td></td><td></td><td>674.923978</td><td></td><td></td></tr><tr><td>Stable</td><td colspan="7">Yes</td></tr><tr><td>Invertible</td><td colspan="7">Yes</td></tr><tr><td colspan="8">Parameter Estimates</td></tr><tr><td>Term</td><td>Lag</td><td>Estimate</td><td>Std Error</td><td>t Ratio</td><td>Prob&gt;|t|</td><td>Constant</td><td>Estimate</td></tr><tr><td>AR1</td><td>1</td><td>0.265885</td><td>0.089022</td><td>2.99</td><td>0.0035</td><td></td><td>21.469383</td></tr><tr><td>AR2</td><td>2</td><td>0.412978</td><td>0.090108</td><td>4.58</td><td>&lt;.0001</td><td></td><td></td></tr><tr><td>Intercept</td><td>0</td><td>66.854262</td><td>1.833390</td><td>36.46</td><td>&lt;.0001</td><td></td><td></td></tr></table>

![](images/24df00bc2594339c7b9fea0443ee5436efbfeb4ee4f48e4c7437464ecb74b51f.jpg)  
Forecast

![](images/b8e2575a3e720ba9b6d5ecfcf042fcf2e10ec622a8b1b1c3ea7301aeba5ecb5c.jpg)  
Residuals

TABLE 5.8 (Continued)   

<table><tr><td>Lag</td><td>AutoCorr plot autocorr</td><td>Ljung-Box Q</td><td>p-Value</td></tr><tr><td>0</td><td>1.0000</td><td>.</td><td>.</td></tr><tr><td>1</td><td>0.0320</td><td>0.1094</td><td>0.7408</td></tr><tr><td>2</td><td>0.0287</td><td>0.1986</td><td>0.9055</td></tr><tr><td>3</td><td>-0.0710</td><td>0.7489</td><td>0.8617</td></tr><tr><td>4</td><td>-0.0614</td><td>1.1647</td><td>0.8839</td></tr><tr><td>5</td><td>-0.0131</td><td>1.1839</td><td>0.9464</td></tr><tr><td>6</td><td>0.0047</td><td>1.1864</td><td>0.9776</td></tr><tr><td>7</td><td>0.1465</td><td>3.6263</td><td>0.8217</td></tr><tr><td>8</td><td>-0.0309</td><td>3.7358</td><td>0.8801</td></tr><tr><td>9</td><td>0.0765</td><td>4.4158</td><td>0.8820</td></tr><tr><td>10</td><td>-0.0938</td><td>5.4479</td><td>0.8593</td></tr><tr><td>11</td><td>-0.0698</td><td>6.0251</td><td>0.8717</td></tr><tr><td>12</td><td>0.0019</td><td>6.0255</td><td>0.9148</td></tr><tr><td>13</td><td>0.0223</td><td>6.0859</td><td>0.9430</td></tr><tr><td>14</td><td>0.1604</td><td>9.2379</td><td>0.8155</td></tr><tr><td>15</td><td>-0.0543</td><td>9.6028</td><td>0.8440</td></tr><tr><td>16</td><td>0.1181</td><td>11.3501</td><td>0.7874</td></tr><tr><td>17</td><td>-0.0157</td><td>11.3812</td><td>0.8361</td></tr><tr><td>18</td><td>0.1299</td><td>13.5454</td><td>0.7582</td></tr><tr><td>19</td><td>-0.0059</td><td>13.5499</td><td>0.8093</td></tr><tr><td>20</td><td>0.0501</td><td>13.8788</td><td>0.8366</td></tr><tr><td>21</td><td>-0.0413</td><td>14.1056</td><td>0.8650</td></tr><tr><td>22</td><td>0.0937</td><td>15.2870</td><td>0.8496</td></tr><tr><td>23</td><td>0.0409</td><td>15.5146</td><td>0.8752</td></tr><tr><td>24</td><td>-0.0035</td><td>15.5163</td><td>0.9047</td></tr><tr><td>25</td><td>-0.0335</td><td>15.6731</td><td>0.9242</td></tr><tr><td>Lag</td><td>Partial plot partial</td><td></td><td></td></tr><tr><td>0</td><td>1.0000</td><td></td><td></td></tr><tr><td>1</td><td>0.0320</td><td></td><td></td></tr><tr><td>2</td><td>0.0277</td><td></td><td></td></tr><tr><td>3</td><td>-0.0729</td><td></td><td></td></tr><tr><td>4</td><td>-0.0580</td><td></td><td></td></tr><tr><td>5</td><td>-0.0053</td><td></td><td></td></tr><tr><td>6</td><td>0.0038</td><td></td><td></td></tr><tr><td>7</td><td>0.1399</td><td></td><td></td></tr><tr><td>8</td><td>-0.0454</td><td></td><td></td></tr><tr><td>9</td><td>0.0715</td><td></td><td></td></tr><tr><td>10</td><td>-0.0803</td><td></td><td></td></tr><tr><td>Lag</td><td>AutoCorr plot autocorr</td><td>Ljung-Box Q</td><td>p-Value</td></tr><tr><td>11</td><td>-0.0586</td><td></td><td></td></tr><tr><td>12</td><td>0.0201</td><td></td><td></td></tr><tr><td>13</td><td>0.0211</td><td></td><td></td></tr><tr><td>14</td><td>0.1306</td><td></td><td></td></tr><tr><td>15</td><td>-0.0669</td><td></td><td></td></tr><tr><td>16</td><td>0.1024</td><td></td><td></td></tr><tr><td>17</td><td>0.0256</td><td></td><td></td></tr><tr><td>18</td><td>0.1477</td><td></td><td></td></tr><tr><td>19</td><td>-0.0027</td><td></td><td></td></tr><tr><td>20</td><td>0.0569</td><td></td><td></td></tr><tr><td>21</td><td>-0.0823</td><td></td><td></td></tr><tr><td>22</td><td>0.1467</td><td></td><td></td></tr><tr><td>23</td><td>-0.0124</td><td></td><td></td></tr><tr><td>24</td><td>0.0448</td><td></td><td></td></tr><tr><td>25</td><td>-0.0869</td><td></td><td></td></tr></table>

The process $w _ { t }$ can be seen as seasonally stationary. Since an ARMA process can be used to model $N _ { t }$ , in general, we have

$$
\Phi (B) w _ {t} = \left(1 - B ^ {s}\right) \Theta (B) \varepsilon_ {t} \tag {5.105}
$$

where $\varepsilon _ { t }$ is white noise.

We can also consider $S _ { t }$ as a stochastic process. We will further assume that after seasonal differencing, $( 1 - B ^ { s } ) , ( 1 - B ^ { s } ) y _ { t } = w _ { t }$ becomes stationary. This, however, may not eliminate all seasonal features in the process. That is, the seasonally differenced data may still show strong autocorrelation at lags s, 2s, 鈥?. So the seasonal ARMA model is

$$
\left(1 - \phi_ {1} ^ {*} B ^ {s} - \phi_ {2} ^ {*} B ^ {2 s} - \dots - \phi_ {P} ^ {*} B ^ {P s}\right) w _ {t} = \left(1 - \theta_ {1} ^ {*} B ^ {s} - \theta_ {2} ^ {*} B ^ {2 s} - \dots - \theta_ {Q} ^ {*} B ^ {Q s}\right) \varepsilon_ {t} \tag {5.106}
$$

This representation, however, only takes into account the autocorrelation at seasonal lags s, 2s, 鈥?. Hence a more general seasonal ARIMA model of orders $( p , d , q ) \times ( P , D , Q )$ with period $s$ is

$$
\Phi^ {*} (B ^ {s}) \Phi (B) (1 - B) ^ {d} (1 - B ^ {s}) ^ {D} y _ {t} = \delta + \Theta^ {*} (B ^ {s}) \Theta (B) \varepsilon_ {t} \tag {5.107}
$$

In practice, although it is case speci-c, it is not expected to have $P$ , $D$ , and $Q$ greater than 1. The results for regular ARIMA processes that we discussed in previous sections apply to the seasonal models given in Eq. (5.107).

As in the nonseasonal ARIMA models, the forecasts for the seasonal ARIMA models can be obtained from the difference equations as illustrated for example in Eq. (5.101) for a nonseasonal ARIMA(1,1,1) process. Similarly the weights in the random shock form given in Eq. (5.96) can be estimated as in Eq. (5.98) to obtain the estimate for the variance of the forecast errors as well as the prediction intervals given in Eqs. (5.91) and (5.92), respectively.

Example 5.7 The ARIMA $( 0 , 1 , 1 ) \times ( 0 , 1 , 1 )$ model with $s = 1 2$ is

$$
\underbrace {(1 - B) (1 - B ^ {1 2}) y _ {t}} _ {w _ {t}} = \left(1 - \theta_ {1} B - \theta_ {1} ^ {*} B ^ {1 2} + \theta_ {1} \theta_ {1} ^ {*} B ^ {1 3}\right) \varepsilon_ {t}
$$

For this process, the autocovariances are calculated as

$$
\begin{array}{l} \gamma (0) = \operatorname {V a r} \left(w _ {t}\right) = \sigma^ {2} \left(1 + \theta_ {1} ^ {2} + \theta_ {1} ^ {* 2} + \left(- \theta_ {1} \theta_ {1} ^ {*}\right) ^ {2}\right) \\ = \sigma^ {2} \left(1 + \theta_ {1} ^ {2}\right) \left(1 + \theta_ {1} ^ {* 2}\right) \\ \end{array}
$$

$$
\begin{array}{l} \gamma (1) = \operatorname {C o v} \left(w _ {t}, w _ {t - 1}\right) = \sigma^ {2} \left(- \theta_ {1} + \theta_ {1} ^ {*} \left(- \theta_ {1} \theta_ {1} ^ {*}\right)\right) \\ = - \theta_ {1} \sigma^ {2} \left(1 + \theta_ {1} ^ {*}\right) \\ \end{array}
$$

$$
\begin{array}{l} \gamma (2) = \gamma (3) = \dots = \gamma (1 0) = 0 \\ \gamma (1 1) = \sigma^ {2} \theta_ {1} \theta_ {1} ^ {*} \\ \gamma (1 2) = - \sigma^ {2} \theta_ {1} ^ {*} \left(1 + \theta_ {1} ^ {2}\right) \\ \gamma (1 3) = \sigma^ {2} \theta_ {1} \theta_ {1} ^ {*} \\ \gamma (j) = 0, \quad j > 1 3 \\ \end{array}
$$

Example 5.8 Consider the US clothing sales data in Table 4.9. The data obviously exhibit some seasonality and upward linear trend. The sample ACF and PACF plots given in Figure 5.25 indicate a monthly seasonality, $s = 1 2$ , as ACF values at lags 12, 24, 36 are signi-cant and slowly decreasing, and there is a signi-cant PACF value at lag 12 that is close to 1. Moreover, the slowly decreasing ACF in general, also indicates a nonstationarity that can be remedied by taking the -rst difference. Hence we would now consider $w _ { t } = ( 1 - B ) ( 1 - B ^ { 1 2 } ) y _ { t }$ .

Figure 5.26 shows that -rst difference together with seasonal differencing鈥攖hat is, $w _ { t } = ( 1 - B ) ( 1 - B ^ { 1 2 } ) y _ { t }$ 鈥攈elps in terms of stationarity and eliminating the seasonality, which is also con-rmed by sample ACF and PACF plots given in Figure 5.27. Moreover, the sample ACF with a signi-cant value at lag 1 and the sample PACF with exponentially

![](images/902101fde22ccffcf6873fa30b30f70bc98ad1748434aecbd86c4dd100736388.jpg)

![](images/692789e0e562bc8e6b397129361135376cfe4bf2ae1747e9b8167c8ab250be04.jpg)  
FIGURE 5.25 Sample ACF and PACF plots of the US clothing sales data.

![](images/43ba19be778eec7b077f932cf6798d032bc67d2876c3f84c453595b9fa8997c5.jpg)  
FIGURE 5.26 Time series plot of $w _ { t } = ( 1 - B ) ( 1 - B ^ { 1 2 } ) y _ { t }$ for the US clothing sales data.

decaying values at the -rst 8 lags suggest that a nonseasonal MA(1) model should be used.

The interpretation of the remaining seasonality is a bit more dif-cult. For that we should focus on the sample ACF and PACF values at lags 12, 24, 36, and so on. The sample ACF at lag 12 seems to be signi-cant and the sample PACF at lags 12, 24, 36 (albeit not signi-cant) seems to be alternating in sign. That suggests that a seasonal MA(1) model can be used as well. Hence an $\mathrm { A R I M A } ( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ model is used to model the data, $y _ { t }$ . The output from Minitab is given in Table 5.9. Both MA(1) and seasonal MA(1) coef-cient estimates are signi-cant. As we can see from the sample ACF and PACF plots in Figure 5.28, while there are still some

![](images/e2e86c8ccdbfef327549c71ecd25db9468210e2168c69aff2eb69a185c22a316.jpg)

![](images/d769f91fcfc60a947abe34dcd18ffe885a2a60496db22b08173b8788474ed634.jpg)  
FIGURE 5.27 Sample ACF and PACF plots of $w _ { t } = ( 1 - B ) ( 1 - B ^ { 1 2 } ) y _ { t }$

TABLE 5.9 Minitab Output for the ARIMA $( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ Model for the US Clothing Sales Data   

<table><tr><td colspan="6">Final Estimates of Parameters</td></tr><tr><td>Type</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P</td><td></td></tr><tr><td>MA 1</td><td>0.7626</td><td>0.0542</td><td>14.06</td><td>0.000</td><td></td></tr><tr><td>SMA 12</td><td>0.5080</td><td>0.0771</td><td>6.59</td><td>0.000</td><td></td></tr><tr><td colspan="6">Differencing: 1 regular, 1 seasonal of order 12</td></tr><tr><td colspan="6">Number of observations: Original series 155, after differencing 142</td></tr><tr><td colspan="6">Residuals: SS = 10033560 (backforecasts excluded)</td></tr><tr><td colspan="6">MS = 71668 DF = 140</td></tr><tr><td colspan="6">Modified Box-Pierce (Ljung-Box) Chi-Square statistic</td></tr><tr><td>Lag</td><td>12</td><td>24</td><td>36</td><td>48</td><td></td></tr><tr><td>Chi-Square</td><td>15.8</td><td>37.7</td><td>68.9</td><td>92.6</td><td></td></tr><tr><td>DF</td><td>10</td><td>22</td><td>34</td><td>46</td><td></td></tr><tr><td>P-Value</td><td>0.107</td><td>0.020</td><td>0.000</td><td>0.000</td><td></td></tr></table>

small signi-cant values, as indicated by the modi-ed Box pierce statistic most of the autocorrelation is now modeled out.

The residual plots in Figure 5.29 provided by Minitab seem to be acceptable as well.

Finally, the time series plot of the actual and -tted values in Figure 5.30 suggests that the ARIMA $( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ model provides a reasonable -t to this highly seasonal and nonstationary time series data.

![](images/bc410438ebd57bb7bf3a6da4f537f1509de1c19611dd310f47e77c42ea9a32e5.jpg)

![](images/54cada76fbd009fef979e95cb54a5b74c5c21f5a1e433669145fb98cf8ceff84.jpg)  
FIGURE 5.28 Sample ACF and PACF plots of residuals from the ARIMA(0, $1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ model.

![](images/5c41c6de4cdbde02eaa64c0d72dff4e631ba03f82060bfb8b55a24a8e657a736.jpg)

![](images/d0aa8d248324fa8fa3bdb2289c04070324eb72f99b545390fcecee8f95fe4a94.jpg)

![](images/c03608fd78ffcfab3fd383a8193a77d9c5db63e0dc3643aa378278b4313887f3.jpg)

![](images/9936819823d9615345d2ffa68656d4926db70395b94b9f615880e097181e5b03.jpg)  
FIGURE 5.29 Residual plots from the ARIMA $\left( 0 , 1 , 1 \right) \times \left( 0 , 1 , 1 \right) _ { 1 2 }$ model for the US clothing sales data.

![](images/8ce2146daca6192ee623deac0692b5697c9a27abc3ebaa13beb70c19c98c3939.jpg)  
FIGURE 5.30 Time series plot of the actual data and -tted values from the A $\mathrm { \Delta \Omega ^ { \mathrm { 3 I M A } } ( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 } }$ model for the US clothing sales data.

# 5.10 ARIMA MODELING OF BIOSURVEILLANCE DATA

In Section 4.8 we introduced the daily counts of respiratory and gastrointestinal complaints for more than 2- $\cdot ^ { 1 } / _ { 2 }$ years at several hospitals in a large metropolitan area from Fricker (2013). Table 4.12 presents the 980 observations from one of these hospitals. Section 4.8 described modeling the respiratory count data with exponential smoothing. We now present an ARIMA modeling approach. Figure 5.31 presents the sample ACF, PACF, and the variogram from JMP for these data. Examination of the original time series plot in Figure 4.35 and the ACF and variogram indicate that the daily respiratory syndrome counts may be nonstationary and that the data should be differenced to obtain a stationary time series for ARIMA modeling.

The ACF for the differenced series $( d = 1$ ) shown in Figure 5.32 cuts off after lag 1 while the PACF appears to be a mixture of exponential decays. This suggests either an ARIMA(1, 1, 1) or ARIMA(2, 1, 1) model.

The Time Series Modeling platform in JMP allows a group of ARIMA models to be -t by specifying ranges for the AR, difference, and MA terms. Table 5.10 summarizes the -ts obtained for a constant difference $( \mathrm { d } = 1 )$ ), and both AR (p) and MA (q) parameters ranging from 0 to 2.

![](images/576f06213a0412f3bd6cad342f5d61b89672af65ae7db3484bc7bc17c1cb43d3.jpg)  
FIGURE 5.31 ACF, PACF, and variogram for daily respiratory syndrome counts.

![](images/7eebfb1db228f2719a0e414da9e1d1f540a13b8988ca761d3bb5c64a60cc47ac.jpg)  
FIGURE 5.32 ACF, PACF, and variogram for the -rst difference of the daily respiratory syndrome counts.

TABLE 5.10 Summary of Models -t to the Respiratory Syndrome Count Data   

<table><tr><td>Model</td><td>Variance</td><td>AIC</td><td>BIC</td><td>RSquare</td><td>MAPE</td><td>MAE</td></tr><tr><td>AR(1)</td><td>65.7</td><td>6885.5</td><td>6895.3</td><td>0.4</td><td>24.8</td><td>6.3</td></tr><tr><td>AR(2)</td><td>57.1</td><td>6748.2</td><td>6762.9</td><td>0.5</td><td>22.9</td><td>5.9</td></tr><tr><td>MA(1)</td><td>81.6</td><td>7096.9</td><td>7106.6</td><td>0.2</td><td>28.5</td><td>6.9</td></tr><tr><td>MA(2)</td><td>69.3</td><td>6937.6</td><td>6952.3</td><td>0.3</td><td>26.2</td><td>6.4</td></tr><tr><td>ARMA(1, 1)</td><td>52.2</td><td>6661.2</td><td>6675.9</td><td>0.5</td><td>21.6</td><td>5.6</td></tr><tr><td>ARMA(1, 2)</td><td>52.1</td><td>6661.2</td><td>6680.7</td><td>0.5</td><td>21.6</td><td>5.6</td></tr><tr><td>ARMA(2, 1)</td><td>52.1</td><td>6660.7</td><td>6680.3</td><td>0.5</td><td>21.6</td><td>5.6</td></tr><tr><td>ARMA(2, 2)</td><td>52.3</td><td>6664.3</td><td>6688.7</td><td>0.5</td><td>21.6</td><td>5.6</td></tr><tr><td>ARIMA(0, 0, 0)</td><td>104.7</td><td>7340.4</td><td>7345.3</td><td>0.0</td><td>33.2</td><td>8.0</td></tr><tr><td>ARIMA(0, 1, 0)*</td><td>81.6</td><td>7088.2</td><td>7093.1</td><td>0.2</td><td>26.2</td><td>7.0</td></tr><tr><td>ARIMA(0, 1, 1)*</td><td>52.7</td><td>6662.8</td><td>6672.6</td><td>0.5</td><td>21.4</td><td>5.7</td></tr><tr><td>ARIMA(0, 1, 2)</td><td>52.6</td><td>6662.1</td><td>6676.7</td><td>0.5</td><td>21.4</td><td>5.7</td></tr><tr><td>ARIMA(1, 1, 0)*</td><td>62.2</td><td>6824.4</td><td>6834.2</td><td>0.4</td><td>23.2</td><td>6.2</td></tr><tr><td>ARIMA(1, 1, 1)</td><td>52.6</td><td>6661.4</td><td>6676.1</td><td>0.5</td><td>21.4</td><td>5.7</td></tr><tr><td>ARIMA(1, 1, 2)</td><td>52.6</td><td>6661.9</td><td>6681.5</td><td>0.5</td><td>21.4</td><td>5.7</td></tr><tr><td>ARIMA(2, 1, 0)</td><td>59.6</td><td>6783.5</td><td>6798.1</td><td>0.4</td><td>22.7</td><td>6.1</td></tr><tr><td>ARIMA(2, 1, 1)</td><td>52.3</td><td>6657.1</td><td>6676.6</td><td>0.5</td><td>21.4</td><td>5.6</td></tr><tr><td>ARIMA(2, 1, 2)</td><td>52.3</td><td>6657.8</td><td>6682.2</td><td>0.5</td><td>21.3</td><td>5.6</td></tr></table>

鈭桰ndicates that objective function failed during parameter estimation.

![](images/401546112da21b397231a389fe7bab15febab9b494567c04b35039b1de42c364.jpg)

![](images/adeb964367f968c614cbd32d1158ef13aa14aa29f6acf83753573d884fc12d2f.jpg)

![](images/aba9edcb5a3690157892e54ae9d431eb08745b7ebd1456ec9d27b40fdb92e821.jpg)  
FIGURE 5.33 ACF, PACF, and variogram for the residuals of ARIMA(1, 1, 1) -t to daily respiratory syndrome counts.

In terms of the model summary statistics variance of the errors, AIC and mean absolute prediction error (MAPE) several models look potentially reasonable. For the ARIMA(1, 1, 1) we obtained the following results from JMP:

<table><tr><td colspan="7">Parameter estimates</td></tr><tr><td>Term</td><td>Lag</td><td>Estimate</td><td>Std error</td><td>t Ratio</td><td>Prob&gt; |t|</td><td>Constant estimate</td></tr><tr><td>AR1</td><td>1</td><td>0.07307009</td><td>0.0394408</td><td>1.85</td><td>0.0642</td><td>0.00069557</td></tr><tr><td>MA1</td><td>1</td><td>0.81584055</td><td>0.0223680</td><td>36.47</td><td>&lt;.0001*</td><td></td></tr><tr><td>Intercept</td><td>0</td><td>0.00075040</td><td>0.0036018</td><td>0.21</td><td>0.8350</td><td></td></tr></table>

Figure 5.33 presents the ACF, PACF, and variogram of the residuals from this model. Other residual plots are in Figure 5.34.

For comparison purposes we also -t the ARIMA(2, 1, 1) model. The parameter estimates obtained from JMP are:

<table><tr><td colspan="7">Parameter Estimates</td></tr><tr><td>Term</td><td>Lag</td><td>Estimate</td><td>Std Error</td><td>t Ratio</td><td>Prob&gt;|t|</td><td>Constant Estimate</td></tr><tr><td>AR1</td><td>1</td><td>0.09953471</td><td>0.0402040</td><td>2.48</td><td>0.0135*</td><td>0.00097496</td></tr><tr><td>AR2</td><td>2</td><td>0.09408008</td><td>0.0375486</td><td>2.51</td><td>0.0124*</td><td></td></tr><tr><td>MA1</td><td>1</td><td>0.84755625</td><td>0.0231814</td><td>36.56</td><td>&lt;.0001*</td><td></td></tr><tr><td>Intercept</td><td>0</td><td>0.00120905</td><td>0.0088678</td><td>0.14</td><td>0.8916</td><td></td></tr></table>

![](images/5304af14ae8decdcc88796e730886a42afbcf5a28bff642e84024765feae8545.jpg)

![](images/5def3c0857d50a1c888ada0e659bc9cf20cd3e1988dfd00c13ab433e0accb41b.jpg)  
FIGURE 5.34 Plots of residuals from ARIMA(1, 1, 1) -t to daily respiratory syndrome counts.

![](images/97f9bcfac80382c4eb2bb3557b0a9af2b1e51e18251d8dd33241c4dbe874d707.jpg)

![](images/e204b704f4bf63a04a439977404181dd86f96654c56b83a32e291040616ab36c.jpg)

![](images/f800bc68ac8082ec371f9f2a55a3a956c7946d1d7b262c809492c02e9db68810.jpg)  
FIGURE 5.35 ACF, PACF, and variogram for residuals of ARIMA(2, 1, 1) -t to daily respiratory syndrome counts.

The lag 2 AR parameter is highly signi-cant. Figure 5.35 presents the plots of the ACF, PACF, and variogram of the residuals from ARIMA(2, 1, 1). Other residual plots are shown in Figure 5.36. Based on the signi-cant lag 2 AR parameter, this model is preferable to the ARIMA(1, 1, 1) model -t previously.

Considering the variation in counts by day of week that was observed previously, a seasonal ARIMA model with a seasonal period of 7 days may be appropriate. The resulting model has an error variance of 50.9, smaller than for the ARIMA(1, 1, 1) and ARIMA(2, 1, 1) models. The AIC is also smaller. Notice that all of the model parameters are highly signi-cant. The residual ACF, PACF, and variogram shown in Figure 5.37 do not suggest any remaining structure. Other residual plots are in Figure 5.38.

<table><tr><td>Model</td><td>Variance</td><td>AIC</td><td>BIC</td><td>RSquare</td><td>MAPE</td><td>MAE</td></tr><tr><td>ARIMA(2,1,1)(0,0,1)7</td><td>50.9</td><td>6631.9</td><td>6656.4</td><td>0.5</td><td>21.1</td><td>5.6</td></tr></table>

![](images/3e19198ca41d3df6306bf066e1900d7b782b876f20d17d3a742d4e203938c64a.jpg)

![](images/15e25e87d47c23b33c1c64bd4089fce6fc006189062d10cd0cc9e3b1b673e4ab.jpg)  
FIGURE 5.36 Plots of residuals from ARIMA(2, 1, 1) -t to daily respiratory syndrome counts.

Parameter estimates   

<table><tr><td>Term</td><td>Factor</td><td>Lag</td><td>Estimate</td><td>Std error</td><td>t Ratio</td><td>Prob&gt;|t|</td><td>Constant estimate</td></tr><tr><td>AR1,1</td><td>1</td><td>1</td><td>0.1090685</td><td>0.0395388</td><td>2.76</td><td>0.0059*</td><td rowspan="5">0.00083453</td></tr><tr><td>AR1,2</td><td>1</td><td>2</td><td>0.1186083</td><td>0.0376471</td><td>3.15</td><td>0.0017*</td></tr><tr><td>MA1,1</td><td>1</td><td>1</td><td>0.8730535</td><td>0.0225127</td><td>38.78</td><td>&lt;.0001*</td></tr><tr><td>MA2,7</td><td>2</td><td>7</td><td>-0.1744415</td><td>0.0328363</td><td>-5.31</td><td>&lt;.0001*</td></tr><tr><td>Intercept</td><td>1</td><td>0</td><td>0.0010805</td><td>0.0051887</td><td>0.21</td><td>0.8351</td></tr></table>

![](images/2ebebd31fcdd1cb7c4b0cc29a7352ab6ab5584daf2d19c929d2e0c2b458e78c8.jpg)

![](images/2dcc0b344bd69932044e140e3690ae14aa6562f62aa857a749df0a5732ad967d.jpg)

![](images/f37959c157f99b8aff31891154777f73eb9b4973869956e97904c9e2534b28ef.jpg)  
FIGURE 5.37 ACF, PACF, and variogram of residuals from ARIMA(2, 1, 1) 脳 $( 0 , 0 , 1 ) _ { 7 }$ -t to daily respiratory syndrome counts.

# 5.11 FINAL COMMENTS

ARIMA models (a.k.a. Box鈥揓enkins models) present a very powerful and exible class of models for time series analysis and forecasting. Over the years, they have been very successfully applied to many problems in research and practice. However, there might be certain situations where they may fall short on providing the 鈥渞ight鈥?answers. For example, in ARIMA models, forecasting future observations primarily relies on the past data and implicitly assumes that the conditions at which the data is collected will remain the same in the future as well. In many situations this assumption may (and most likely will) not be appropriate. For those cases, the transfer function鈥搉oise models, where a set of input variables that may have an effect on the time series are added to the model, provide suitable options. We shall discuss these models in the next chapter. For an excellent discussion of this matter and of time series analysis and forecasting in general, see Jenkins (1979).

![](images/b3f96fc10bf536b3f4fbe14df5a714909fb0690254db94567703788a794bb6b9.jpg)

![](images/f2df649867d0ea9bfb580cf354d3fa4f064237e887a8e3ca8761fad49c8a2729.jpg)  
FIGURE 5.38 Plots of residuals from ARIM $\phantom { - } \ i ( 2 , 1 , 1 ) \times ( 0 , 0 , 1 ) _ { 7 }$ -t to daily respiratory syndrome counts.

# 5.12 R COMMANDS FOR CHAPTER 5

Example 5.1 The loan applications data are in the second column of the array called loan.data in which the -rst column is the number of weeks. We -rst plot the data as well as the ACF and PACF.

plot(loan.data[,2],type $=$ "o",pch=16,cex $\displaystyle =$ .5,xla $i = 1$ 'Week',ylab $= ^ { 1 }$ 'Loan Applications')

![](images/939c82b566b1307a8dd3439ce76a71de0c97f9b10786d75cbf7c0d38076980ad.jpg)

par(mfrow=c(1,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

acf(loan.data[,2],lag.ma $_ { : = 2 5 }$ ,type $: =$ "correlation",main $. =$ "ACF for the Number \nof Loan Applications")

acf(loan.data[,2], lag.max $^ { = 2 5 }$ ,type $=$ "partial",main $. =$ "PACF for the Number \nof Loan Applications")

![](images/be3325f98cae42d15df42a8b01564b0989b0f8dd211eeca55471cf4c4e78f5bf.jpg)

![](images/e9bbc3cc30e789a3a9a56b3ff37655037c1049709037c40d56e44fa2f537348d.jpg)

Fit an ARIMA(2,0,0) model to the data using arima function in the stats package.

```txt
loan.fit.ar2<-arima(loan.data[,2],order=c(2,0锛?)) loan.fit.ar2 
```

```txt
Call:  
arima(x = loan.data[, 2], order = c(2, 0, 0)) 
```

```txt
Coefficients: ar1 ar2 intercept 0.2659 0.4130 66.8538 s.e. 0.0890 0.0901 1.8334 
```

```txt
sigma^2 estimated as 38.32: log likelihood = -337.46,  
aic = 682.92 
```

```txt
res_loan.ar2<-as.vector(residuals(loan.fit.ar2)) #to obtain the fitted values we use the function fitted() from #the forecast package library Forecast) fit_loan.ar2<-as.vector(fitted(loan.fit.ar2)) 
```

Box.test(res_loan.ar2,lag $= 48$ ,fitdf $= 3$ ,type $\equiv$ "Ljung") Box-Ljung test

```txt
data: res_loan.ar2  
X-squared = 31.8924, df = 45, p-value = 0.9295 
```

```txt
ACF and PACF of the Residuals par(mfrow=c(1,2), oma=c(0,0,0,0)) 
```

acf(res.loan.ar2,lag.max=25,type $=$ "correlation",mai $. =$ "ACF of the Residuals \nof AR(2) Model")

acf(res.loan.ar2, lag.max=25,type $=$ "partial",main $. =$ "PACF of the Residuals \nof AR(2) Model")

![](images/75443f94da0cef86784711dd06d09c0df07e001a3702aaabe0be40718b7cf094.jpg)

![](images/b6f46c01d6826186d1e7f317fe875584feb2bd7822bb980376ec1ecfbec7ab71.jpg)

4-in-1 plot of the residuals   
par(mfrow=c(2,2),oma=c(0,0,0,0))   
qqnorm(res_loan.ar2,datrix $\equiv$ TRUE,pch $= 16$ xlab $=$ 'Residual',main $= ^{\prime \prime}$ )   
qqline(res_loan.ar2,datrix $\equiv$ TRUE)   
plot(fit_loan.ar2,res_loan.ar2,pch $= 16$ ,xlab $=$ 'Fitted Value', ylab $=$ 'Residual')   
abline(h=0)   
hist(res_loan.ar2,col $=$ "gray",xlab $=$ 'Residual',main $= ^{\prime \prime}$ 1   
plot(res_loan.ar2,type $=$ "1",xlab $=$ 'Observation Order', ylab $=$ 'Residual')   
points(res_loan.ar2,pch $= 16$ ,cex=.5)   
abline(h=0)

![](images/5b96efcf0f23b744427ea27230d30b40a0aba5a64fb5ec040188a3aec2415e23.jpg)

![](images/af00854d21b62886dd15f012439c54d6c986f4074b6af166bbda627cde125134.jpg)

![](images/22d77c1ed3c345f47a2f252de76d8cfdfcdf82bf84068429396375b92df90673.jpg)

![](images/16b2306c2732bfb4e557013d1cab5bd892cf31f269af2bc89c2a99949235c509.jpg)

# Plot -tted values

plot(loan.data[,2],type $=$ "p",pch $= 16$ ,cex $= .5$ ,xlab $\equiv$ 'Week',ylab $\equiv$ Loan Applications') lines (fit.loan.ar2) legend(95,88,c("y(t)","yhat(t)"),pch=c(16锛孨A),lwd=c(NA,.5)锛?cex $= .55)$

![](images/1580bc9ccb25bbb0da1c89ceec5d171e33c26fbe9255d85b3e098ca281bcc7bf.jpg)

Example 5.2 The Dow Jones index data are in the second column of the array called dji.data in which the -rst column is the month of the year. We -rst plot the data as well as the ACF and PACF.

plot(dji.data[,2],type $=$ "o",pch $= 16$ ,cex $= .5$ ,xlab $\equiv$ 'Date'锛寉lab $\equiv$ 'DJI'锛?xaxt $\equiv$ 'n') axis(1锛宻eq(1,85,12)锛宒ji.data[seq(1,85,12),1])

![](images/c105de6d69ad76a93f2f8f8281e881dafae6b31d965ba9e761311376c3ccdace.jpg)

par(mfrow ${ \bf \Pi } = { \bf C }$ (1,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

acf(dji.data[,2],lag.max $_ { : = 2 5 }$ ,type $=$ "correlation",mai $. =$ "ACF for the Number \nof Dow Jones Index")

acf(dji.data[,2], lag.max $^ { = 2 5 }$ ,type $=$ "partial",main $. =$ "PACF for the Number \nof Dow Jones Index ")

![](images/ed4642477303f8336cd0fb7e3d6230175251794ef60dd00ca93c05b2cf4e7c1c.jpg)

![](images/4a84859b7a9d37f549b2144988b743ccdb8936bdabadcc481ef786d74aa3bc71.jpg)

We -rst -t an ARIMA(1,0,0) model to the data using arima function in the stats package.

dji.fit.ar1<-arima(dji.data[,2],order ${ \bf \bar { \Psi } } = { \bf C }$ (1, 0, 0))

dji.fit.ar1

Call: arima(x = dji.data[, 2], order = c(1, 0, 0))

Coefficients: ar1 intercept 0.8934 10291.2984 s.e. 0.0473 373.8723

sigma藛2 estimated as 156691: log likelihood $=$ -629.8, aic $=$ 1265.59

res.dji.ar1<-as.vector(residuals(dji.fit.ar1))

#to obtain the fitted values we use the function fitted() from

#the forecast package

library(forecast)

fit.dji.ar1<-as.vector(fitted(dji.fit.ar1))

Box.test(res.dji.ar1,lag $^ { \cdot } = 4 8$ ,fitdf $^ { = 3 }$ ,type $: =$ "Ljung")

Box-Ljung test

data: res.dji.ar1

X-squared $=$ 29.9747, $\ d \boldsymbol { \mathsf { { f } } } \ = \ \ \boldsymbol { \mathsf { 4 } } \boldsymbol { 5 }$ , p-value $=$ 0.9584

#ACF and PACF of the Residuals par(mfrow ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (1,2),oma ${ \bf \Psi } = { \bf C }$ (0,0,0,0)) acf(res.dji.ar1,lag.max $_ { : = 2 5 }$ ,type $=$ "correlation",main $\cdot ^ { = }$ "ACF of the Residuals \nof AR(1) Model")

acf(res.dji.ar1, lag.max=25,type $: =$ "partial",mai $. =$ "PACF of the Residuals \nof AR(1) Model")

![](images/26fa1fd5291e15f0da1c373d070ad6da23515eaa9fa4d9ce935f6b2d9a2a22b5.jpg)

![](images/86a8f85eb5ac1dbf97b36d2fecf3beb69694c1e13a870213b5ff36d9b4cce9f3.jpg)

#4-in-1 plot of the residuals par(mfrow ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (2,2),oma ${ \bf \Psi } = { \bf C }$ (0,0,0,0)) qqnorm(res.dji.ar1,datax $\displaystyle =$ TRUE,pch=16,xlab $^ { 1 = }$ 'Residual',main='') qqline(res.dji.ar1,datax $: =$ TRUE) plot(fit.dji.ar1,res.dji.ar1,pch $_ { . = 1 6 }$ , xlab $^ { 1 = }$ 'Fitted Value', ylab $^ { 1 = }$ 'Residual') abline $\scriptstyle \mathrm { { h = 0 } }$ ) hist(res.dji.ar1,col $=$ "gray",xlab $^ { 1 = }$ 'Residual',main='') plot(res.dji.ar1,type $: =$ "l",xlab $^ { 1 = }$ 'Observation Order', ylab $= ^ { 1 }$ Residual') points(res.dji.ar1,pch=16,cex $: =$ .5) abline $\scriptstyle \mathrm { { h = 0 } }$ )

![](images/e4bbc8361b9a22d29d7508ff666b1e8f7544fa7c32fe5763238e103960e23973.jpg)

![](images/c75546bebb3f89539708feedce8eb78b7586f177c43a5e119afb3be68ff8a9d9.jpg)

![](images/101472d0fab1829a64bd5e8b8c9b3d1ada5e553a37c8397715eae8b9725dc989.jpg)

![](images/96b9be0af5a9f564b3e0d5cf73b9276b2950f02fcb91f5d16df09bdacf70cc32.jpg)

We now consider the -rst difference of the Dow Jones index.

wt.dji<-diff(dji.data[,2])

plot(wt.dji,type $=$ "o",pch=16,cex .5,xlab $i = 1$ 'Date',ylab $= ^ { \mathsf { I } }$ w(t)',

xaxt $= \cdot \mathrm { ~ n ~ } ^ { \prime }$

axis(1, seq(1,85,12), dji.data[seq(1,85,12),1])

![](images/3500587cb5b1a28a09aef720bec12ecd5f8e379690162e4a062e72cc58924d86.jpg)

par(mfrow ${ \bf \Pi } = { \bf C }$ (1,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

acf(wt.dji,lag.max $_ { : = 2 5 }$ ,type $: =$ "correlation",mai $. =$ "ACF for the Number \nof w(t)")

acf(wt.dji, lag.max=25,type $=$ "partial",main $. =$ "PACF for the Number \nof w(t)")

![](images/40d8872781650b9ae030e4679df7f637ab7792a0a167aa450c4d2a059ea64e23.jpg)

![](images/88626f28fe5dc5a1062870c26d23369ca315bc4e3004b49ead3f80a7f35153e1.jpg)

Example 5.6 The loan applications data are in the second column of the array called loan.data in which the -rst column is the number of weeks. We use the AR(2) model to make the forecasts.

```txt
loan.fit.ar2<-arima(loan.data[,2],order=c(2,0,0)) #to obtain the 1- to 12-step ahead forecasts, we use the #function forecast() from the forecast package library Forecast) loan.ar2.forecast<-as.array( forecast(loan.fit.ar2,h=12)) loan.ar2.forecast 
```

<table><tr><td>Point</td><td>Forecast</td><td>Lo 80</td><td>Hi 80</td><td>Lo 95</td><td>Hi 95</td></tr><tr><td>105</td><td>62.58571</td><td>54.65250</td><td>70.51892</td><td>50.45291</td><td>74.71851</td></tr><tr><td>106</td><td>64.12744</td><td>55.91858</td><td>72.33629</td><td>51.57307</td><td>76.68180</td></tr><tr><td>107</td><td>64.36628</td><td>55.30492</td><td>73.42764</td><td>50.50812</td><td>78.22444</td></tr><tr><td>108</td><td>65.06647</td><td>55.80983</td><td>74.32312</td><td>50.90965</td><td>79.22330</td></tr><tr><td>109</td><td>65.35129</td><td>55.86218</td><td>74.84039</td><td>50.83895</td><td>79.86362</td></tr><tr><td>110</td><td>65.71617</td><td>56.13346</td><td>75.29889</td><td>51.06068</td><td>80.37167</td></tr><tr><td>111</td><td>65.93081</td><td>56.27109</td><td>75.59054</td><td>51.15754</td><td>80.70409</td></tr><tr><td>112</td><td>66.13857</td><td>56.43926</td><td>75.83789</td><td>51.30475</td><td>80.97240</td></tr><tr><td>113</td><td>66.28246</td><td>56.55529</td><td>76.00962</td><td>51.40605</td><td>81.15887</td></tr><tr><td>114</td><td>66.40651</td><td>56.66341</td><td>76.14961</td><td>51.50572</td><td>81.30730</td></tr><tr><td>115</td><td>66.49892</td><td>56.74534</td><td>76.25249</td><td>51.58211</td><td>81.41572</td></tr><tr><td>116</td><td>66.57472</td><td>56.81486</td><td>76.33458</td><td>51.64830</td><td>81.50114</td></tr></table>

Note that forecast function provides a list with forecasts as well as $80 \%$ and $9 5 \%$ prediction limits. To see the elements of the list, we can do

ls(loan.ar2.forecast)   
```txt
[1] "fitted" "level" "lower" "mean" "method" "model"  
[7] "residuals" "upper" "x" "xlabel" 
```

In this list, 鈥渕ean鈥?stands for the forecasts while 鈥渓ower鈥?and 鈥渦pper鈥?provide the 80 and $9 5 \%$ lower and upper prediction limits, respectively. To plot the forecasts and the prediction limits, we have

```python
plot(loan.data[,2],type="p",pch=16, cex=.5,xlab='Date',ylab='Loan Applications',xaxt='n', xlim=c(1,120)) axis(1, seq(1,120,24), dji.data[seq(1,120,24),1]) lines(105:116,loan.ar2_forecast\\(mean,col="grey40") lines(105:116,loan.ar2_forecast\\)lower[,2]) lines(105:116,loan.ar2_forecast\\)upper[,2]) legend(72,88,c("y","Forecast","95% LPL","95% UPL"),pch=c(16,NA, NA,NA),lwd=c (NA,.5,.5,.5), cex=.55,col=c("black","grey40","black","black")) 
```

![](images/5631e8cccbc77650e908a4bc8895db352c5c4c6da87d438932fa346edabf66c7.jpg)

Example 5.8 The clothing sales data are in the second column of the array called closales.data in which the -rst column is the month of the year. We -rst plot the data and its ACF and PACF.

par(mfrow ${ \bf \Pi } = { \bf C }$ (1,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

acf(closales.data[,2],lag.max $\mathtt { : = 5 0 }$ ,type $: =$ "correlation",mai $. =$ "ACF for the \n Clothing Sales")

acf(closales.data[,2], lag.max $\mathtt { . = 5 0 }$ ,type $=$ "partial",main $. =$ "PACF for the \n Clothing Sales")

![](images/6d650463944125328acd89b07a208a2aab29ea72a6079b103fed98ecb1b189c4.jpg)

![](images/336e96fdbe2e3c93b4c2d5e0102d2ab9dbcd297cb5c499f59235559c559689a5.jpg)

We now take the seasonal and non-seasonal difference of the data.

wt.closales<-diff(diff(closales.data[,2],lag=1), $\mathtt { l a g } = 1 2$ )

#Note that the same result would have been obtained with the #following command when the order of differencing is reversed #wt.closales<-diff(diff(closales.data[,2], $\mathtt { l a g } = 1 2$ ),lag=1)

plot(wt.closales,type $: =$ "o",pch=16,cex $: =$ .5,xlab $=$ 'Date',ylab $^ { \prime = }$ 'w(t)', xaxt $= \ l ^ { 1 } \mathrm { ~ n ~ } ^ { \ l }$ )

axis(1, seq(1,144,24), closales.data[seq(13,144,24),1])

![](images/109984359d40e207fa9981cfeb6dfb4988fd869a492762c78f468489e7234ab8.jpg)

par(mfrow ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (1,2),oma ${ \bf \Psi } = { \bf C }$ (0,0,0,0))

acf(wt.closales,lag.max $_ { : = 5 0 }$ ,type $=$ "correlation",main $\cdot ^ { = }$ "ACF for w(t)")

acf(wt.closales, lag.max $\mathtt { \_ 5 0 }$ ,type $: =$ "partial",mai $\cdot =$ "PACF for w(t)")

![](images/dc29d305855c564fbd9b4bdf8f5f5d49652cab52c8284d8dd00dd2f9f7ddccb1.jpg)

![](images/c00fc1e5901455fa8d0509fdd8c7d034350a9bb667483e0fa58266993390a8aa.jpg)

We now -t a seasonal $\mathrm { A R I M A } ( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ model to the data. We then plot the residuals plots including ACF and PACF of the residuals. In the end we plot the true and -tted values.

closales.fit.sar<-arima(closales.data[,2],orde ${ \bf \Phi } = { \bf C }$ (0,1,1), seasonal $=$ list(order $=$ c(0,1,1),period $^ { = 1 2 }$ ),)

res.closales.sar<-as.vector(residuals(closales.fit.sar)) #to obtain the fitted values we use the function fitted() from the forecast package library(forecast) fit.closales.sar<-as.vector(fitted(closales.fit.sar))

#ACF and PACF of the Residuals

par(mfrow ${ \bf \Pi } = { \bf C }$ (1,2),oma $\boldsymbol { \mathbf { \mathit { \Pi } } } = \boldsymbol { \mathbf { \check { \mathbf { C } } } }$ (0,0,0,0))

acf(res.closales.sar,lag.max $\mathtt { . = 5 0 }$ ,type $=$ "correlation",main $\cdot ^ { = }$ "ACF of the Residuals")

acf(res.closales.sar,lag.max=50,type $: =$ "partial",mai $. =$ "PACF of the Residuals")

![](images/de2939013888f22d235ddb4f8256e4f087775881c948a4f0d16ac3ffe998baa6.jpg)  
ACF for the residuals

![](images/51d4798bcfba50b2e30845a328563f4ce2cbcb1acb6920a567a150e25c07fb7e.jpg)  
PACF for the residuals

#4-in-1 plot of the residuals

par(mfrow ${ \bf \Pi } = { \bf C }$ (2,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

qqnorm(res.closales.sar,datax $\displaystyle =$ TRUE,pch=16,xla $^ { 1 = }$ 'Residual',main='') qqline(res.closales.sar,datax $\displaystyle =$ TRUE)

plot(fit.closales.sar,res.closales.sar,pch $_ { . = 1 6 }$ , xla $^ { 1 = }$ 'Fitted Value',ylab $=$ 'Residual')

abline $\scriptstyle \mathrm { \mathrm { h } = 0 }$ )

hist(res.closales.sar,co $=$ "gray",xlab $1 = 1$ 'Residual',main='')

plot(res.closales.sar,type $: =$ "l",xlab $^ { 1 = }$ 'Observation Order', ylab $=$ 'Residual')

points(res.closales.sar,pch=16,cex $: =$ .5) abline $\mathtt { h } = 0$ )

![](images/8b19b23d5c4bcf30624cd854d4ae7c863394d79227ab30534cab03ff4c069eaf.jpg)

![](images/e9c15f55d7fa3039fa246983597128f9b2c4b15d343477ab0a0ae07bf0fff01f.jpg)

![](images/2720245a4c7011effec7019f960d59b196338c4bd90fe75b39d7518647b0cb86.jpg)

![](images/a1a7138a4f3f7907967d60d5e24cfe51c085a5f461b8f51c41026caf3b4d7630.jpg)

plot(closales.data[,2],type $=$ "p",pch $= 16$ ,cex $= .5$ ,xlab $\equiv$ 'Date' ylab $\equiv$ Clothing Sales',xaxt $\equiv$ n') axis(1,seq(1,144,24),closales.data[seq(1,144,24),1]) lines(1:144锛宖it.closales.sar) legend(2,17500,c("US Clothing Sales","Fitted")锛宲ch=c(16锛孨A)锛?lwd=c(NA锛?5)锛宑ex $= .55$ ,col=c("black","black"))

![](images/c7f417fd304b5e5d541eec37a0558293494f06af5e5dc6bf377bfd0e595e94f2.jpg)

# EXERCISES

5.1 Consider the time series data shown in Chapter 4, Table E4.2.

a. Fit an appropriate ARIMA model to the -rst 40 observations of this time series.   
b. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors.   
c. In Exercise 4.4 you used simple exponential smoothing with $\lambda = 0 . 2$ to smooth the -rst 40 time periods of this data and make forecasts of the last 10 observations. Compare the ARIMA forecasts with the exponential smoothing forecasts. How well do both of these techniques work?

5.2 Consider the time series data shown in Table E5.1.

a. Make a time series plot of the data.   
b. Calculate and plot the sample autocorrelation and PACF. Is there signi-cant autocorrelation in this time series?

TABLE E5.1 Data for Exercise 5.2   

<table><tr><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td></tr><tr><td>1</td><td>29</td><td>11</td><td>29</td><td>21</td><td>31</td><td>31</td><td>28</td><td>41</td><td>36</td></tr><tr><td>2</td><td>20</td><td>12</td><td>28</td><td>22</td><td>30</td><td>32</td><td>30</td><td>42</td><td>35</td></tr><tr><td>3</td><td>25</td><td>13</td><td>28</td><td>23</td><td>37</td><td>33</td><td>29</td><td>43</td><td>33</td></tr><tr><td>4</td><td>29</td><td>14</td><td>26</td><td>24</td><td>30</td><td>34</td><td>34</td><td>44</td><td>29</td></tr><tr><td>5</td><td>31</td><td>15</td><td>27</td><td>25</td><td>33</td><td>35</td><td>30</td><td>45</td><td>25</td></tr><tr><td>6</td><td>33</td><td>16</td><td>26</td><td>26</td><td>31</td><td>36</td><td>20</td><td>46</td><td>27</td></tr><tr><td>7</td><td>34</td><td>17</td><td>30</td><td>27</td><td>27</td><td>37</td><td>17</td><td>47</td><td>30</td></tr><tr><td>8</td><td>27</td><td>18</td><td>28</td><td>28</td><td>33</td><td>38</td><td>23</td><td>48</td><td>29</td></tr><tr><td>9</td><td>26</td><td>19</td><td>26</td><td>29</td><td>37</td><td>39</td><td>24</td><td>49</td><td>28</td></tr><tr><td>10</td><td>30</td><td>20</td><td>30</td><td>30</td><td>29</td><td>40</td><td>34</td><td>50</td><td>32</td></tr></table>

c. Identify and -t an appropriate ARIMA model to these data. Check for model adequacy.   
d. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors.

5.3 Consider the time series data shown in Table E5.2.

a. Make a time series plot of the data.   
b. Calculate and plot the sample autocorrelation and PA. Is there signi-cant autocorrelation in this time series?   
c. Identify and -t an appropriate ARIMA model to these data. Check for model adequacy.   
d. Make one-step-ahead forecasts of the last 10 observations. Determine the forecast errors.

TABLE E5.2 Data for Exercise 5.3   

<table><tr><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td><td>Period</td><td>yt</td></tr><tr><td>1</td><td>500</td><td>11</td><td>508</td><td>21</td><td>475</td><td>31</td><td>639</td><td>41</td><td>637</td></tr><tr><td>2</td><td>496</td><td>12</td><td>510</td><td>22</td><td>485</td><td>32</td><td>679</td><td>42</td><td>606</td></tr><tr><td>3</td><td>450</td><td>13</td><td>512</td><td>23</td><td>495</td><td>33</td><td>674</td><td>43</td><td>610</td></tr><tr><td>4</td><td>448</td><td>14</td><td>503</td><td>24</td><td>500</td><td>34</td><td>677</td><td>44</td><td>620</td></tr><tr><td>5</td><td>456</td><td>15</td><td>505</td><td>25</td><td>541</td><td>35</td><td>700</td><td>45</td><td>613</td></tr><tr><td>6</td><td>458</td><td>16</td><td>494</td><td>26</td><td>555</td><td>36</td><td>704</td><td>46</td><td>593</td></tr><tr><td>7</td><td>472</td><td>17</td><td>491</td><td>27</td><td>565</td><td>37</td><td>727</td><td>47</td><td>578</td></tr><tr><td>8</td><td>495</td><td>18</td><td>487</td><td>28</td><td>601</td><td>38</td><td>736</td><td>48</td><td>581</td></tr><tr><td>9</td><td>491</td><td>19</td><td>491</td><td>29</td><td>610</td><td>39</td><td>693</td><td>49</td><td>598</td></tr><tr><td>10</td><td>488</td><td>20</td><td>486</td><td>30</td><td>605</td><td>40</td><td>65</td><td>50</td><td>613</td></tr></table>

5.4 Consider the time series model

$$
y _ {t} = 2 0 0 + 0. 7 y _ {t - 1} + \varepsilon_ {t}
$$

a. Is this a stationary time series process?   
b. What is the mean of the time series?   
c. If the current observation is $y _ { 1 0 0 } = 7 5 0$ , would you expect the next observation to be above or below the mean?

5.5 Consider the time series model

$$
y _ {t} = 1 5 0 - 0. 5 y _ {t - 1} + \varepsilon_ {t}
$$

a. Is this a stationary time series process?   
b. What is the mean of the time series?   
c. If the current observation is $y _ { 1 0 0 } = 8 5$ , would you expect the next observation to be above or below the mean?

5.6 Consider the time series model

$$
y _ {t} = 5 0 + 0. 8 y _ {t - 1} - 0. 1 5 + \varepsilon_ {t}
$$

a. Is this a stationary time series process?   
b. What is the mean of the time series?   
c. If the current observation is $y _ { 1 0 0 } = 1 6 0$ , would you expect the next observation to be above or below the mean?

5.7 Consider the time series model

$$
y _ {t} = 2 0 + \varepsilon_ {t} + 0. 2 \varepsilon_ {t - 1}
$$

a. Is this a stationary time series process?   
b. Is this an invertible time series?   
c. What is the mean of the time series?   
d. If the current observation is $y _ { 1 0 0 } = 2 3$ , would you expect the next observation to be above or below the mean? Explain your answer.

5.8 Consider the time series model

$$
y _ {t} = 5 0 + 0. 8 y _ {t - 1} + \varepsilon_ {t} - 0. 2 \varepsilon_ {t - 1}
$$

a. Is this a stationary time series process?

b. What is the mean of the time series?   
c. If the current observation is $y _ { 1 0 0 } = 2 7 0$ , would you expect the next observation to be above or below the mean?

5.9 The data in Chapter 4, Table E4.4, exhibits a linear trend. Difference the data to remove the trend.

a. Fit an ARIMA model to the -rst differences.   
b. Explain how this model would be used for forecasting.

5.10 Table B.1 in Appendix B contains data on the market yield on US Treasury Securities at 10-year constant maturity.

a. Fit an ARIMA model to this time series, excluding the last 20 observations. Investigate model adequacy. Explain how this model would be used for forecasting.   
b. Forecast the last 20 observations.   
c. In Exercise 4.10, you were asked to use simple exponential smoothing with $\lambda = 0 . 2$ to smooth the data, and to forecast the last 20 observations. Compare the ARIMA and exponential smoothing forecasts. Which forecasting method do you prefer?

5.11 Table B.2 contains data on pharmaceutical product sales.

a. Fit an ARIMA model to this time series, excluding the last 10 observations. Investigate model adequacy. Explain how this model would be used for forecasting.   
b. Forecast the last 10 observations.   
c. In Exercise 4.12, you were asked to use simple exponential smoothing with $\lambda = 0 . 1$ to smooth the data, and to forecast the last 10 observations. Compare the ARIMA and exponential smoothing forecasts. Which forecasting method do you prefer?   
d. How would prediction intervals be obtained for the ARIMA forecasts?

5.12 Table B.3 contains data on chemical process viscosity.

a. Fit an ARIMA model to this time series, excluding the last 20 observations. Investigate model adequacy. Explain how this model would be used for forecasting.   
b. Forecast the last 20 observations.   
c. Show how to obtain prediction intervals for the forecasts in part b above.

5.13 Table B.4 contains data on the annual US production of blue and gorgonzola cheeses.

a. Fit an ARIMA model to this time series, excluding the last 10 observations. Investigate model adequacy. Explain how this model would be used for forecasting.   
b. Forecast the last 10 observations.   
c. In Exercise 4.16, you were asked to use exponential smoothing methods to smooth the data, and to forecast the last 10 observations. Compare the ARIMA and exponential smoothing forecasts. Which forecasting method do you prefer?   
d. How would prediction intervals be obtained for the ARIMA forecasts?

5.14 Reconsider the blue and gorgonzola cheese data in Table B.4 and Exercise 5.13. In Exercise 4.17 you were asked to take the -rst difference of this data and develop a forecasting procedure based on using exponential smoothing on the -rst differences. Compare this procedure with the ARIMA model of Exercise 5.13.   
5.15 Table B.5 shows US beverage manufacturer product shipments. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.16 Table B.6 contains data on the global mean surface air temperature anomaly. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.17 Reconsider the global mean surface air temperature anomaly data shown in Table B.6 and used in Exercise 5.16. In Exercise 4.20 you were asked to use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data. Compare the results with those obtained with the ARIMA model in Exercise 5.16.   
5.18 Table B.7 contains daily closing stock prices for the Whole Foods Market. Develop an appropriate ARIMA model and a procedure for these data. Explain how prediction intervals would be computed.   
5.19 Reconsider the Whole Foods Market data shown in Table B.7 and used in Exercise 5.18. In Exercise 4.22 you used simple exponential smoothing with the optimum value of $\lambda$ to smooth the data.

Compare the results with those obtained from the ARIMA model in Exercise 5.18.

5.20 Unemployment rate data is given in Table B.8. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.21 Reconsider the unemployment rate data shown in Table B.8 and used in Exercise 5.21. In Exercise 4.24 you used simple exponential smoothing with the optimum value of $\lambda$ to smooth the data. Compare the results with those obtained from the ARIMA model in Exercise 5.20.   
5.22 Table B.9 contains yearly data on the international sunspot numbers. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.23 Reconsider the sunspot data shown in Table B.9 and used in Exercise 5.22.

a. In Exercise 4.26 you were asked to use simple exponential smoothing with the optimum value of $\lambda$ to smooth the data, and to use an exponential smoothing procedure for trends. How do these procedures compare to the ARIMA model from Exercise 5.22? Compare the results with those obtained in Exercise 4.26.   
b. Do you think that using either exponential smoothing procedure would result in better forecasts than those from the ARIMA model?

5.24 Table B.10 contains 7 years of monthly data on the number of airline miles own in the United Kingdom. This is seasonal data.

a. Using the -rst 6 years of data, develop an appropriate ARIMA model and a procedure for these data.   
b. Explain how prediction intervals would be computed.   
c. Make one-step-ahead forecasts of the last 12 months. Determine the forecast errors. How well did your procedure work in forecasting the new data?

5.25 Reconsider the airline mileage data in Table B.10 and used in Exercise 5.24.

a. In Exercise 4.27 you used Winters鈥?method to develop a forecasting model using the -rst 6 years of data and you made forecasts

for the last 12 months. Compare those forecasts with the ones you made using the ARIMA model from Exercise 5.24.

b. Which forecasting method would you prefer and why?

5.26 Table B.11 contains 8 years of monthly champagne sales data. This is seasonal data.

a. Using the -rst 7 years of data, develop an appropriate ARIMA model and a procedure for these data.   
b. Explain how prediction intervals would be computed.   
c. Make one-step-ahead forecasts of the last 12 months. Determine the forecast errors. How well did your procedure work in forecasting the new data?

5.27 Reconsider the monthly champagne sales data in Table B.11 and used in Exercise 5.26.

a. In Exercise 4.29 you used Winters鈥?method to develop a forecasting model using the -rst 7 years of data and you made forecasts for the last 12 months. Compare those forecasts with the ones you made using the ARIMA model from Exercise 5.26.   
b. Which forecasting method would you prefer and why?

5.28 Montgomery et al. (1990) give 4 years of data on monthly demand for a soft drink. These data are given in Chapter 4, Table E4.5.

a. Using the -rst three years of data, develop an appropriate ARIMA model and a procedure for these data.   
b. Explain how prediction intervals would be computed.   
c. Make one-step-ahead forecasts of the last 12 months. Determine the forecast errors. How well did your procedure work in forecasting the new data?

5.29 Reconsider the soft drink demand data in Table E4.5 and used in Exercise 5.28.

a. In Exercise 4.31 you used Winters鈥?method to develop a forecasting model using the -rst 7 years of data and you made forecasts for the last 12 months. Compare those forecasts with the ones you made using the ARIMA model from the previous exercise.   
b. Which forecasting method would you prefer and why?

5.30 Table B.12 presents data on the hourly yield from a chemical process and the operating temperature. Consider only the yield data in this exercise. Develop an appropriate ARIMA model and a procedure for

forecasting for these data. Explain how prediction intervals would be computed.

5.31 Table B.13 presents data on ice cream and frozen yogurt sales. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.32 Table B.14 presents the $\mathrm { C O } _ { 2 }$ readings from Mauna Loa. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.33 Table B.15 presents data on the occurrence of violent crimes. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.34 Table B.16 presents data on the US gross domestic product (GDP). Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.35 Total annual energy consumption is shown in Table B.17. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.36 Table B.18 contains data on coal production. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.37 Table B.19 contains data on the number of children 0鈥? years old who drowned in Arizona. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.38 Data on tax refunds and population are shown in Table B.20. Develop an appropriate ARIMA model and a procedure for forecasting for these data. Explain how prediction intervals would be computed.   
5.39 Table B.21 contains data from the US Energy Information Administration on monthly average price of electricity for the residential sector in Arizona. This data has a strong seasonal component. Use the data from 2001鈥?010 to develop an ARIMA model for this data. Use this model to simulate one-month-ahead forecasts for the

remaining years. Calculate the forecast errors. Discuss the reasonableness of the forecasts.

5.40 In Exercise 4.44 you were asked to develop a smoothing-type model for the data in Table B.21. Compare the performance of that mode with the performance of the ARIMA model from the previous exercise.   
5.41 Table B.22 contains data from the Danish Energy Agency on Danish crude oil production. Develop an appropriate ARIMA model for this data. Compare this model with the smoothing models developed in Exercises 4.46 and 4.47.   
5.42 Table B.23 shows Weekly data on positive laboratory test results for inuenza are shown in Table B.23. Notice that these data have a number of missing values. In exercise you were asked to develop and implement a scheme to estimate the missing values. This data has a strong seasonal component. Use the data from 1997 to 2010 to develop an appropriate ARIMA model for this data. Use this model to simulate one-week-ahead forecasts for the remaining years. Calculate the forecast errors. Discuss the reasonableness of the forecasts.   
5.43 In Exercise 4.48 you were asked to develop a smoothing鈥搕ype model for the data in Table B.23. Compare the performance of that mode with the performance of the ARIMA model from the previous exercise.   
5.44 Data from the Western Regional Climate Center for the monthly mean daily solar radiation (in Langleys) at the Zion Canyon, Utah, station are shown in Table B.24. This data has a strong seasonal component. Use the data from 2003 to 2012 to develop an appropriate ARIMA model for this data. Use this model to simulate one-monthahead forecasts for the remaining years. Calculate the forecast errors. Discuss the reasonableness of the forecasts.   
5.45 In Exercise 4.50 you were asked to develop a smoothing-type model for the data in Table B.24. Compare the performance of that mode with the performance of the ARIMA model from the previous exercise.   
5.46 Table B.25 contains data from the National Highway Traf-c Safety Administration on motor vehicle fatalities from 1966 to 2012. This data is used by a variety of governmental and industry groups, as well

as research organizations. Develop an ARIMA model for forecasting fatalities using the data from 1966 to 2006 to develop the model, and then simulate one-year-ahead forecasts for the remaining years. Compute the forecasts errors. How well does this method seem to work?

5.47 Appendix Table B.26 contains data on monthly single-family residential new home sales from 1963 through 2014. Develop an ARIMA model for forecasting new home sales using the data from 1963 to 2006 to develop the model, and then simulate one-yearahead forecasts for the remaining years. Compute the forecasts errors. How well does this method seem to work?   
5.48 Appendix Table B.27 contains data on the airline best on-time arrival and airport performance. The data is given by month from January 1995 through February 2013. Develop an ARIMA model for forecasting on-time arrivals using the data from 1995 to 2008 to develop the model, and then simulate one-year-ahead forecasts for the remaining years. Compute the forecasts errors. How well does this method seem to work?   
5.49 Data from the US Census Bureau on monthly domestic automobile manufacturing shipments (in millions of dollars) are shown in Table B.28. Develop an ARIMA model for forecasting shipments. Note that there is some apparent seasonality in the data. Why does this seasonal behavior occur?   
5.50 An ARIMA model has been -t to a time series, resulting in

$$
\hat {y} _ {t} = 2 5 + 0. 3 5 y _ {t - 1} + \varepsilon_ {t}
$$

a. Suppose that we are at time period $T = 1 0 0$ and $y _ { 1 0 0 } = 3 1$ . Determine forecasts for periods 101, 102, 103, 鈥?from this model at origin 100.   
b. What is the shape of the forecast function from this model?   
c. Suppose that the observation for time period 101 turns out to be $y _ { 1 0 1 } = 3 3$ . Revise your forecasts for periods 102, 103, 鈥?using period 101 as the new origin of time.   
d. If your estimate $\hat { \sigma } ^ { 2 } = 2$ , -nd a $9 5 \%$ prediction interval on the forecast of period 101 made at the end of period 100.

5.51 The following ARIMA model has been -t to a time series:

$$
\hat {y} _ {t} = 2 5 + 0. 8 y _ {t - 1} - 0. 3 y _ {t - 2} + \varepsilon_ {t}
$$

a. Suppose that we are at the end of time period $T = 1 0 0$ and we know that $y _ { 1 0 0 } = 4 0$ and $y _ { 9 9 } = 3 8 .$ . Determine forecasts for periods 101, 102, 103, 鈥?from this model at origin 100.   
b. What is the shape of the forecast function from this model?   
c. Suppose that the observation for time period 101 turns out to be $y _ { 1 0 1 } = 3 5$ . Revise your forecasts for periods 102, 103, 鈥?using period 101 as the new origin of time.   
d. If your estimate $\hat { \sigma } ^ { 2 } = 1$ , -nd a $9 5 \%$ prediction interval on the forecast of period 101 made at the end of period 100.

5.52 The following ARIMA model has been -t to a time series:

$$
\hat {y} _ {t} = 2 5 + 0. 8 y _ {t - 1} - 0. 2 \varepsilon_ {t - 1} + \varepsilon_ {t}
$$

a. Suppose that we are at the end of time period $T = 1 0 0$ and we know that the forecast for period 100 was 130 and the actual observed value was $y _ { 1 0 0 } = 1 4 0$ . Determine forecasts for periods 101, 102, 103, 鈥?from this model at origin 100.   
b. What is the shape of the forecast function from this model?   
c. Suppose that the observation for time period 101 turns out to be $y _ { 1 0 1 } = 1 3 2$ . Revise your forecasts for periods 102, 103, 鈥?using period 101 as the new origin of time.   
d. If your estimate $\widehat { \sigma } ^ { 2 } = 1 . 5$ , -nd a $9 5 \%$ prediction interval on the forecast of period 101 made at the end of period 100.

5.53 The following ARIMA model has been -t to a time series:

$$
\hat {y} _ {t} = 2 0 + \varepsilon_ {t} + 0. 4 5 \varepsilon_ {t - 1} - 0. 3 \varepsilon_ {t - 2}
$$

a. Suppose that we are at the end of time period $T = 1 0 0$ and we know that the observed forecast error for period 100 was 0.5 and for period 99 we know that the observed forecast error was 鈭?.8. Determine forecasts for periods 101, 102, 103, 鈥?from this model at origin 100.   
b. What is the shape of the forecast function that evolves from this model?   
c. Suppose that the observations for the next four time periods turn out to be 17.5, 21.25, 18.75, and 16.75. Revise your forecasts for periods 102, 103, 鈥?using a rolling horizon approach.   
d. If your estimate $\hat { \sigma } = 0 . 5$ , -nd a $9 5 \%$ prediction interval on the forecast of period 101 made at the end of period 100.

5.54 The following ARIMA model has been -t to a time series:

$$
\hat {y} _ {t} = 5 0 + \varepsilon_ {t} + 0. 5 \varepsilon_ {t - 1}
$$

a. Suppose that we are at the end of time period $T = 1 0 0$ and we know that the observed forecast error for period 100 was 2. Determine forecasts for periods 101, 102, 103, 鈥?from this model at origin 100.   
b. What is the shape of the forecast function from this model?   
c. Suppose that the observations for the next four time periods turn out to be 53, 55, 46, and 50. Revise your forecasts for periods 102, 103, 鈥?using a rolling horizon approach.   
d. If your estimate $\hat { \sigma } = 1$ , -nd a $9 5 \%$ prediction interval on the forecast of period 101 made at the end of period 100.

5.55 For each of the ARIMA models shown below, give the forecasting equation that evolves for lead times $\tau = 1 , 2 , \dots , L$ . In each case, explain the shape of the resulting forecast function over the forecast lead time.

a. AR(1)   
b. AR(2)  
c. MA(1)   
d. MA(2)   
e. ARMA(1, 1)   
f. IMA(1, 1)   
g. ARIMA(1, 1, 0)

5.56 Use a random number generator and generate 100 observations from the AR(1) model $y _ { t } = 2 5 + 0 . 8 y _ { t - 1 } + \varepsilon _ { t }$ . Assume that the errors are normally and independently distributed with mean zero and variance $\sigma ^ { 2 } = 1$ .

a. Verify that your time series is AR(1).   
b. Generate 100 observations for a $N ( 0 , 1 )$ process and add these random numbers to the 100 AR(1) observations in part a to create a new time series that is the sum of AR(1) and 鈥渨hite noise.鈥?  
c. Find the sample autocorrelation and partial autocorrelation functions for the new time series created in part b. Can you identify the new time series?   
d. Does this give you any insight about how the new time series might arise in practical settings?

5.57 Assume that you have -t the following model:

$$
\hat {y} _ {t} = y _ {t - 1} + 0. 7 \varepsilon_ {t - 1} + \varepsilon_ {t}
$$

a. Suppose that we are at the end of time period $T = 1 0 0$ . What is the equation for forecasting the time series in period 101?   
b. What does the forecast equation look like for future periods 102, 103, 鈥??   
c. Suppose that we know that the observed value of $y _ { 1 0 0 }$ was 250 and forecast error in period 100 was 12. Determine forecasts for periods 101, 102, 103, 鈥?from this model at origin 100.   
d. If your estimate $\hat { \sigma } = 1$ , -nd a $9 5 \%$ prediction interval on the forecast of period 101 made at the end of period 100.   
e. Show the behavior of this prediction interval for future lead times beyond period 101. Are you surprised at how wide the interval is? Does this tell you something about the reliability of forecasts from this model at long lead times?

5.58 Consider the AR(1) model $y _ { t } = 2 5 + 0 . 7 5 y _ { t - 1 } + \varepsilon _ { t }$ . Assume that the variance of the white noise process is $\sigma ^ { 2 } = 1$ .

a. Sketch the theoretical ACF and PACF for this model.   
b. Generate 50 realizations of this AR(1) process and compute the sample ACF and PACF. Compare the sample ACF and the sample PACF to the theoretical ACF and PACF. How similar to the theoretical values are the sample values?   
c. Repeat part b using 200 realizations. How has increasing the sample size impacted the agreement between the sample and theoretical ACF and PACF? Does this give you any insight about the sample sizes required for model building, or the reliability of models built to short time series?

5.59 Consider the AR(1) model $y _ { t } = 2 5 + 0 . 7 5 y _ { t - 1 } + \varepsilon _ { t }$ . Assume that the variance of the white noise process is $\sigma ^ { 2 } = 1 0 $ .

a. Sketch the theoretical ACF and PACF for this model.   
b. Generate 50 realizations of this AR(1) process and compute the sample ACF and PACF. Compare the sample ACF and the sample PACF to the theoretical ACF and PACF. How similar to the theoretical values are the sample values?   
c. Compare the results from part b with the results from part b of Exercise 5.47. How much has changing the variance of the white noise process impacted the results?

d. Repeat part b using 200 realizations. How has increasing the sample size impacted the agreement between the sample and theoretical ACF and PACF? Does this give you any insight about the sample sizes required for model building, or the reliability of models built to short time series?   
e. Compare the results from part d with the results from part c of Exercise 5.47. How much has changing the variance of the white noise process impacted the results?

5.60 Consider the AR(2) model $y _ { t } = 2 5 + 0 . 6 y _ { t - 1 } + 0 . 2 5 y _ { t - 2 } + \varepsilon _ { t }$ Assume that the variance of the white noise process is $\sigma ^ { 2 } = 1$ .

a. Sketch the theoretical ACF and PACF for this model.   
b. Generate 50 realizations of this AR(1) process and compute the sample ACF and PACF. Compare the sample ACF and the sample PACF to the theoretical ACF and PACF. How similar to the theoretical values are the sample values?   
c. Repeat part b using 200 realizations. How has increasing the sample size impacted the agreement between the sample and theoretical ACF and PACF? Does this give you any insight about the sample sizes required for model building, or the reliability of models built to short time series?

5.61 Consider the MA(1) model $y _ { t } = 4 0 + 0 . 4 \varepsilon _ { t - 1 } + \varepsilon _ { t }$ . Assume that the variance of the white noise process is $\sigma ^ { 2 } = 2$ .

a. Sketch the theoretical ACF and PACF for this model.   
b. Generate 50 realizations of this AR(1) process and compute the sample ACF and PACF. Compare the sample ACF and the sample PACF to the theoretical ACF and PACF. How similar to the theoretical values are the sample values?   
c. Repeat part b using 200 realizations. How has increasing the sample size impacted the agreement between the sample and theoretical ACF and PACF? Does this give you any insight about the sample sizes required for model building, or the reliability of models built to short time series?

5.62 Consider the ARMA(1, 1) model $y _ { t } = 5 0 - 0 . 7 y _ { t - 1 } + 0 . 5 \varepsilon _ { t - 1 } + \varepsilon _ { t }$ Assume that the variance of the white noise process is $\sigma ^ { 2 } = 2$ .

a. Sketch the theoretical ACF and PACF for this model.   
b. Generate 50 realizations of this AR(1) process and compute the sample ACF and PACF. Compare the sample ACF and the sample

PACF to the theoretical ACF and PACF. How similar to the theoretical values are the sample values?

c. Repeat part b using 200 realizations. How has increasing the sample size impacted the agreement between the sample and theoretical ACF and PACF? Does this give you any insight about the sample sizes required for model building, or the reliability of models built to short time series?

# TRANSFER FUNCTIONS AND INTERVENTION MODELS

He uses statistics as a drunken man uses lamp posts 鈥?For support rather than illumination

Andrew Lang, Scottish poet

# 6.1 INTRODUCTION

The ARIMA models discussed in the previous chapter represent a general class of models that can be used very effectively in time series modeling and forecasting problems. An implicit assumption in these models is that the conditions under which the data for the time series process is collected remain the same. If, however, these conditions change over time, ARIMA models can be improved by introducing certain inputs re-ecting these changes in the process conditions. This will lead to what is known as transfer function鈥搉oise models. These models can be seen as regression models in Chapter 3 with serially dependent response, inputs, and the error term. The identication and the estimation of these models can be challenging. Furthermore, not all standard statistical software packages possess the capability to t such models. So far in this book, we have used

the Minitab and JMP software packages to illustrate time series model tting. However, Minitab (version 16) lacks the capability of tting transfer function鈥搉oise models. Therefore for Chapters 6 and 7, we will use JMP and R instead.

# 6.2 TRANSFER FUNCTION MODELS

In Section 5.2, we discussed the linear -lter and dened it as

$$
\begin{array}{l} y _ {t} = L \left(x _ {t}\right) = \sum_ {i = - \infty} ^ {+ \infty} v _ {i} x _ {t - i} \tag {6.1} \\ \mathbf {\Sigma} = \nu (B) x _ {t}, \\ \end{array}
$$

where v(B) = 鈭?鈭?$\begin{array} { r } { \nu ( B ) = \sum _ { i = - \infty } ^ { + \infty } \nu _ { i } B ^ { i } } \end{array}$ i=鈭掆垶 is called the transfer function. Following the denition of a linear lter, Eq. (6.1) is:

1. Time-invariant as the coefcients $\{ \nu _ { i } \}$ do not depend on time.   
2. Physically realizable if $\nu _ { i } = 0$ for $i < 0$ ; that is, the output $y _ { t }$ is a linear function of the current and past values of the input:

$$
\begin{array}{l} y _ {t} = v _ {0} x _ {t} + v _ {1} x _ {t - 1} + \dots \\ = \sum_ {i = 0} ^ {\infty} v _ {i} x _ {t - i}. \tag {6.2} \\ \end{array}
$$

3. Stable if $\textstyle \sum _ { i = - \infty } ^ { + \infty } | \nu _ { i } | < \infty$ 鈭?鈭?
There are two interesting special cases for the input $x _ { t }$ :

Impulse Response Function. If $x _ { t }$ is a unit impulse at time $t = 0$ , that is,

$$
x _ {t} = \left\{ \begin{array}{l l} 1, & t = 0 \\ 0, & t \neq 0 \end{array} \right. \tag {6.3}
$$

then the output $y _ { t }$ is

$$
y _ {t} = \sum_ {i = 0} ^ {\infty} v _ {i} x _ {t - i} = v _ {t} \tag {6.4}
$$

Therefore the coefcients $\nu _ { i }$ in Eq. (6.2) are also called the impulse response function.

Step Response Function. If $x _ { t }$ is a unit step, that is,

$$
x _ {t} = \left\{ \begin{array}{l l} 0, & t <   0 \\ 1, & t \geq 0 \end{array} \right. \tag {6.5}
$$

then the output $y _ { t }$ is

$$
\begin{array}{l} y _ {t} = \sum_ {\substack {i = 0 \\ t}} ^ {\infty} v _ {i} x _ {t - i} \tag{6.6} \\ = \sum_ {i = 0} ^ {t} v _ {i}, \\ \end{array}
$$

which is also called the step response function.

A generalization of the step response function is obtained when Eq. (6.5) is modied so that $x _ { t }$ is kept at a certain target value $X$ after $t \geq 0$ ; that is,

$$
x _ {t} = \left\{ \begin{array}{l l} 0, & t <   0 \\ X, & t \geq 0. \end{array} \right. \tag {6.7}
$$

Hence we have

$$
\begin{array}{l} y _ {t} = \sum_ {i = 0} ^ {\infty} v _ {i} x _ {t - i} \\ = \left(\sum_ {i = 0} ^ {t} v _ {i}\right) X \tag {6.8} \\ = g X, \\ \end{array}
$$

where $g$ is called the steady-state gain.

A more realistic representation of the response is obtained by adding a noise or disturbance term to Eq. (6.2) to account for unanticipated and/or ignored factors that may have an effect on the response as well. Hence the 鈥渁dditive鈥?model representation of the dynamic systems is given as

$$
y _ {t} = v (B) x _ {t} + N _ {t}, \tag {6.9}
$$

where $N _ { t }$ represents the unobservable noise process. In Eq. (6.9), $x _ { t }$ and $N _ { t }$ are assumed to be independent. The model representation in Eq. (6.9) is also called the transfer function鈥搉oise model.

Since the noise process is unobservable, the predictions of the response can be made by estimating the impulse response function $\{ \nu _ { t } \}$ . Similar to our discussion about the estimation of the coefcients in Wold鈥檚 decomposition theorem in Chapter 5, attempting to estimate the innitely many coefcients in $\{ \nu _ { t } \}$ is a futile exercise. Therefore also parallel to the arguments we made in Chapter 5, we will make assumptions about these innitely many coefcients to be able to represent them with only a handful of parameters. Following the derivations we had for the ARMA models, we will assume that the coefcients in $\{ \nu _ { t } \}$ have a structure and can be represented as

$$
\begin{array}{l} v (B) = \sum_ {i = 0} ^ {\infty} v _ {i} B ^ {i} = \frac {w (B)}{\delta (B)} \tag {6.10} \\ = \frac {w _ {0} - w _ {1} B - \cdots - w _ {s} B ^ {s}}{1 - \delta_ {1} B - \cdots - \delta_ {r} B ^ {r}} \\ \end{array}
$$

The interpretation of Eq. (6.10) is quite similar to the one we had for ARMA models; the denominator summarizes the innitely many coef- cients with a certain structure determined by $\{ \delta _ { i } \}$ as in the AR part of the ARMA model and the numerator represents the adjustment we may like to make to the strictly structured innitely many coefcients as in the MA part of the ARMA model.

So the transfer function鈥搉oise model in Eq. (6.9) can be rewritten as

$$
y _ {t} = \frac {w (B)}{\delta (B)} x _ {t} + N _ {t},
$$

where $\begin{array} { r } { w ( B ) / \delta ( B ) = \delta ( B ) ^ { - 1 } w ( B ) = \sum _ { i = 0 } ^ { + \infty } \nu _ { i } B ^ { i } } \end{array}$ . For some processes, there may also be a delay before a change in the input $x _ { t }$ shows its effect on the response $y _ { t }$ . If we assume that there is $^ b$ time units of delay between the response and the input, a more general representation for the transfer function鈥搉oise models can be obtained as

$$
\begin{array}{l} y _ {t} = \frac {w (B)}{\delta (B)} x _ {t - b} + N _ {t} \\ = \frac {w (B)}{\delta (B)} B ^ {b} x _ {t} + N _ {t} \\ = v (B) x _ {t} + N _ {t}, \tag {6.11} \\ \end{array}
$$

Since the denominator $\delta ( B )$ in Eq. (6.11) determines the structure of the innitely many coefcients, the stability of $\nu ( B )$ depends on the coef-cients in $\delta ( B )$ . In fact $\nu ( B )$ is said to be stable if all the roots of $m ^ { r } - \delta _ { 1 } m ^ { r - 1 } - \cdots - \delta _ { r }$ are less than 1 in absolute value.

Once the nite number of parameters in $w ( B )$ and $\delta ( B )$ are estimated, $\nu ( B )$ can be computed recursively from

$$
\delta (B) \nu (B) = w (B) B ^ {b}
$$

or

$$
v _ {j} - \delta_ {1} v _ {j - 1} - \delta_ {2} v _ {j - 2} - \dots - \delta_ {r} v _ {j - r} = \left\{ \begin{array}{l l} - w _ {j - b}, & j = b + 1, \dots , b + s \\ 0, & j > b + s \end{array} \right. \tag {6.12}
$$

with $\nu _ { b } = w _ { 0 }$ and $\nu _ { j } = 0$ for $j < b$ .

The characteristics of the impulse response function are determined by the values of b, r, and s. Recall that in univariate ARIMA modeling, we matched sample autocorrelation and partial autocorrelation functions computed from a time series to theoretical autocorrelation and partial autocorrelation functions of specic ARIMA models to tentatively identify an appropriate model. Thus by knowing the theoretical patterns in the autocorrelation and partial autocorrelation functions for an AR(1) process, for example, we can tentatively identify the AR(1) model when sample autocorrelation and partial autocorrelation functions exhibit the same behavior for an observed time series. The same approach is used in transfer function modeling. However, the primary identication tool is the impulse response function. Consequently, it is necessary that we investigate the nature of the impulse response function and determine what various patterns in the weights imply about the parameters b, r, and $s$ .

Example 6.1 For illustration, we will consider cases for $b = 2$ , $r \leq 2$ and $s \leq 2$ .

Case 1. $r = 0$ and $s = 2$ .

We have

$$
y _ {t} = (w _ {0} - w _ {1} B - w _ {2} B ^ {2}) x _ {t - 2}
$$

From Eq. (6.12), we have

$$
\begin{array}{l} v _ {0} = v _ {1} = 0 \\ v _ {2} = w _ {0} \\ v _ {3} = - w _ {1} \\ v _ {4} = - w _ {2} \\ v _ {j} = 0, \quad j > 4 \\ \end{array}
$$

Hence $\nu _ { t }$ will only be nonzero for $t = 2 , 3$ , and 4.

Case 2. $r = 1$ and $s = 2$ .

We have

$$
y _ {t} = \frac {\left(w _ {0} - w _ {1} B - w _ {2} B ^ {2}\right)}{1 - \delta_ {1} B} x _ {t - 2}
$$

As in the AR(1) model, the stability of the transfer function is achieved for $| \delta _ { 1 } | < 1$ . Once again from Eq. (6.12), we have

$$
\begin{array}{l} v _ {0} = v _ {1} = 0 \\ v _ {2} = w _ {0} \\ v _ {3} = \delta_ {1} w _ {0} - w _ {1} \\ v _ {4} = \delta_ {1} ^ {2} w _ {0} - \delta_ {1} w _ {1} - w _ {2} \\ v _ {j} = \delta_ {1} v _ {j - 1}, j > 4 \\ \end{array}
$$

Since $| \delta _ { 1 } | < 1$ , the impulse response function will approach zero asymp-|totically.

Case 3. $r = 2$ and $s = 2$ .

We have

$$
y _ {t} = \frac {(w _ {0} - w _ {1} B - w _ {2} B ^ {2})}{1 - \delta_ {1} B - \delta_ {2} B ^ {2}} x _ {t - 2}
$$

The stability of the transfer function depends on the roots of the associated polynomial $m ^ { 2 } - \delta _ { 1 } m ^ { 1 } - \delta _ { 2 }$ . For stability, the roots obtained by

$$
m _ {1}, m _ {2} = \frac {\delta_ {1} \pm \sqrt {\delta_ {1} ^ {2} + 4 \delta_ {2}}}{2}
$$

must satisfy $| m _ { 1 } | , | m _ { 2 } | < 1$ . This also means that

$$
\delta_ {2} - \delta_ {1} <   1
$$

$$
\delta_ {2} + \delta_ {1} <   1
$$

$$
- 1 <   \delta_ {2} <   1
$$

or

$$
| \delta_ {1} | <   1 - \delta_ {2}
$$

$$
- 1 <   \delta_ {2} <   1.
$$

This set of two equations implies that the stability is achieved with the triangular region given in Figure 6.1. Within that region we might have two real roots or two complex conjugates. For the latter, we need $\delta _ { 1 } ^ { 2 } + 4 \delta _ { 2 } < 0$ , which occurs in the area under the curve within the triangle in Figure 6.1. Hence for the values of $\delta _ { 1 }$ and $\delta _ { 2 }$ within that curve, the impulse response function would exhibit a damped sinusoid behavior. Everywhere else in the triangle, however, it will have an exponential decay pattern.

Note that when $\delta _ { 2 } = 0$ (i.e., $r = 1$ ), stability is achieved when $| \delta _ { 1 } | < 1$ as expected.

Table 6.1 summarizes the impulse response functions for the cases we have just discussed with specic values for the parameters.

![](images/86504209327e90cbd658209f90278201f886bb614bb94633bc5d549054c92e81.jpg)  
FIGURE 6.1 The stable region for the impulse response function for $r = 2$ .

TABLE 6.1 Impulse Response Function with $b = 2 , r \leq 2$ , and $s \leq 2$   

<table><tr><td>b r s</td><td>Model</td><td colspan="5">Impulse response function</td></tr><tr><td rowspan="6">200</td><td rowspan="6">yt=w0xt-2</td><td rowspan="6">w0=0.5</td><td>0.5</td><td>0.5</td><td>w0=-0.5</td><td>0.5</td></tr><tr><td>0.4</td><td>0.4</td><td>-0.1</td><td>-0.1</td></tr><tr><td>0.3</td><td>0.3</td><td>-0.2</td><td>-0.2</td></tr><tr><td>0.2</td><td>0.2</td><td>-0.3</td><td>-0.3</td></tr><tr><td>0.1</td><td>0.1</td><td>-0.4</td><td>-0.4</td></tr><tr><td>0</td><td>0.5</td><td>-0.5</td><td>-0.5</td></tr><tr><td rowspan="4">201</td><td rowspan="4">yt=(w0-w1B)xt-2</td><td>w0=0.5</td><td>0.5</td><td>0.5</td><td>w0=0.5</td><td>0.5</td></tr><tr><td rowspan="3">w1=-0.4</td><td>0.4</td><td>0.4</td><td>w1=0.4</td><td>0.4</td></tr><tr><td>0.3</td><td>0.3</td><td>-0.25</td><td>-0.25</td></tr><tr><td>0.2</td><td>0.2</td><td>-0.50</td><td>-0.50</td></tr><tr><td rowspan="3">202</td><td rowspan="3">yt=(w0-w1B-w2B2)xt-2</td><td>w0=0.5</td><td>0.5</td><td>0.5</td><td>w0=0.5</td><td>0.75</td></tr><tr><td>w1=-0.4</td><td>0.4</td><td>0.4</td><td>w1=0.4</td><td>0.5</td></tr><tr><td>w2=-0.6</td><td>0.2</td><td>0.2</td><td>w2=-0.6</td><td>0.5</td></tr><tr><td rowspan="10">210</td><td rowspan="5">yt=w0/1-未1Bxt-2</td><td rowspan="5">w0=0.5</td><td>0.5</td><td>0.5</td><td>未1=0.6</td><td>0.5</td></tr><tr><td>0.4</td><td>0.4</td><td>-0.1</td><td>-0.1</td></tr><tr><td>0.3</td><td>0.3</td><td>-0.2</td><td>-0.2</td></tr><tr><td>0.2</td><td>0.2</td><td>-0.3</td><td>-0.3</td></tr><tr><td>0.1</td><td>0.1</td><td>-0.4</td><td>-0.4</td></tr><tr><td rowspan="5">yt=w0-w1B/1-未1Bxt-2</td><td>w0=0.5</td><td>0.7</td><td>0.7</td><td>未1=0.6</td><td>0.5</td></tr><tr><td rowspan="4">w1=-0.4</td><td>0.5</td><td>0.5</td><td>w0=0.5</td><td>0.5</td></tr><tr><td>0.4</td><td>0.4</td><td>w1=0.4</td><td>0.5</td></tr><tr><td>0.3</td><td>0.3</td><td>-0.1</td><td>-0.1</td></tr><tr><td>0.2</td><td>0.2</td><td>-0.5</td><td>-0.5</td></tr></table>

$y _ { t } = \frac { w _ { 0 } - w _ { 1 } B - w _ { 2 } B ^ { 2 } } { 1 - \delta _ { 1 } B } x _ { t - 2 }$ w0 鈭?w1B 鈭?w2B2 2 1 2

$$
w _ {0} = 0. 5
$$

$$
w _ {1} = - 0. 4
$$

$$
w _ {2} = - 0. 6
$$

![](images/7b11b3867271817a442968cfaf18253ae130f794b6f1323613ba9c889d3e33ab.jpg)

$\delta _ { 1 } = 0 . 6$

$$
w _ {0} = 0. 5
$$

$$
w _ {1} = - 0. 4
$$

$$
w _ {2} = - 0. 6
$$

![](images/afed6dde13938445424f831e03f5e829b9267e9a4d5a6e08c32db70cf0326798.jpg)

$\delta _ { 1 } = 0 . 6$

2 2 0 $y _ { t } = \frac { w _ { 0 } } { 1 - \delta _ { 1 } B - \delta _ { 2 } B ^ { 2 } } x _ { t - 2 }$

$$
w _ {0} = 0. 5
$$

![](images/d83eb9e1aa6df829f5cd8dbdab5dddfdfac89466f37678c42182d3fd25192c0e.jpg)

$$
\delta_ {1} = 0. 6
$$

$$
\delta_ {2} = 0. 3
$$

$$
w _ {0} = 0. 5
$$

![](images/4b438299d4e38f2c847691b1234a3630194a06c48c757c23da4592839fdf2483.jpg)

$$
\delta_ {1} = 0. 6
$$

$$
\delta_ {2} = - 0. 7
$$

2 2 1 $y _ { t } = \frac { w _ { 0 } - w _ { 1 } B } { 1 - \delta _ { 1 } B - \delta _ { 2 } B ^ { 2 } } x _ { t - 2 }$

$$
w _ {0} = 0. 5
$$

$$
w _ {1} = - 0. 4
$$

![](images/373f329c7bcc1d1446ce53d82216853f616d64ffdca8923d57c87db1dfa21eb0.jpg)

$$
\delta_ {1} = 0. 6
$$

$$
\delta_ {2} = 0. 3
$$

$$
w _ {0} = 0. 5
$$

$$
w _ {1} = - 0. 4
$$

![](images/ccf5c8e615566477f119af72712791a588ce59c07be68ad00eb2010bde9fbd92.jpg)

$$
\delta_ {1} = 0. 6
$$

$$
\delta_ {2} = - 0. 7
$$

2 2 2 $y _ { t } = \frac { w _ { 0 } - w _ { 1 } B - w _ { 2 } B ^ { 2 } } { 1 - \delta _ { 1 } B - \delta _ { 2 } B ^ { 2 } } x _ { t - 2 }$

$$
w _ {0} = 0. 5
$$

$$
w _ {1} = - 0. 4
$$

$$
w _ {2} = - 0. 6
$$

![](images/69c4ce8b6d59fa0d8e3b9e771fca7880cac50921eb9d6afe1483bd24ef1267d6.jpg)

$$
\delta_ {1} = 0. 6
$$

$$
\delta_ {2} = 0. 3
$$

$$
w _ {0} = 0. 5
$$

$$
w _ {1} = - 0. 4
$$

$$
w _ {2} = - 0. 6
$$

![](images/0c72dd93e5304527ed1925564953a8b718c562466e1c0a9a4a557bf19a0ffb0c.jpg)

$$
\delta_ {1} = 0. 6
$$

$$
\delta_ {2} = - 0. 7
$$

# 6.3 TRANSFER FUNCTION鈥揘OISE MODELS

As mentioned in the previous section, in the transfer function鈥搉oise model in Eq. (6.11) $x _ { t }$ and $N _ { t }$ are assumed to be independent. Moreover, we will assume that the noise $N _ { t }$ can be represented by an ARIMA $( p , d , q )$ model,

$$
\underbrace {\phi (B) (1 - B) ^ {d}} _ {= \varphi (B)} N _ {t} = \theta (B) \varepsilon_ {t}, \tag {6.13}
$$

where $\left\{ \varepsilon _ { t } \right\}$ is white noise with $E ( \varepsilon _ { t } ) = 0 .$ . Hence the transfer function鈥搉oise model can be written as

$$
\begin{array}{l} y _ {t} = v (B) x _ {t} + \psi (B) \varepsilon_ {t} \tag {6.14} \\ = \frac {w (B)}{\delta (B)} x _ {t - b} + \frac {\theta (B)}{\varphi (B)} \varepsilon_ {t} \\ \end{array}
$$

After rearranging Eq. (6.14), we have

$$
\begin{array}{l} \underbrace {\delta (B) \varphi (B)} _ {= \delta^ {*} (B)} y _ {t} = \underbrace {\varphi (B) w (B)} _ {= w ^ {*} (B)} x _ {t - b} + \underbrace {\delta (B) \theta (B)} _ {= \theta^ {*} (B)} \varepsilon_ {t} (6.15) \\ \delta^ {*} (B) y _ {t} = w ^ {*} (B) x _ {t - b} + \theta^ {*} (B) \varepsilon_ {t} (6.15) \\ \end{array}
$$

or

$$
y _ {t} - \sum_ {i = 1} ^ {r ^ {*}} \delta_ {i} ^ {*} y _ {t - i} = w _ {0} ^ {*} x _ {t - b} - \sum_ {i = 1} ^ {s ^ {*}} w _ {i} ^ {*} x _ {t - b - i} + \varepsilon_ {t} - \sum_ {i = 1} ^ {q ^ {*}} \theta_ {i} ^ {*} \varepsilon_ {t - i}. (6. 1 6)
$$

Ignoring the terms involving $x _ { t }$ , Eq. (6.16) is the ARMA representation of the response $y _ { t }$ . Due to the addition of $x _ { t }$ , the model in Eq. (6.16) is also called an ARMAX model. Hence the transfer function鈥搉oise model as given in Eq. (6.16) can be interpreted as an ARMA model for the response with the additional exogenous factor $x _ { t }$ .

# 6.4 CROSS-CORRELATION FUNCTION

For the bivariate time series $( x _ { t } , y _ { t } )$ , we dene the cross-covariance function as

$$
\gamma_ {x y} (t, s) = \operatorname {C o v} \left(x _ {t}, y _ {s}\right) \tag {6.17}
$$

Assuming that $( x _ { t } , y _ { t } )$ is (weakly) stationary, we have

$$
E \left(x _ {t}\right) = \mu_ {x}, \quad \text {c o n s t a n t f o r a l l} t
$$

$$
E \left(y _ {t}\right) = \mu_ {\mathrm {y}}, \quad \text {c o n s t a n t f o r a l l} t
$$

$$
\operatorname {C o v} \left(x _ {t}, x _ {t + j}\right) = \gamma_ {x} (j), \quad \text {d e p e n d s o n l y o n} j
$$

$$
\operatorname {C o v} \left(y _ {t}, y _ {t + j}\right) = \gamma_ {y} (j), \quad \text {d e p e n d s o n l y o n} j
$$

and

$$
\operatorname {C o v} \left(x _ {t}, y _ {t + j}\right) = \gamma_ {x y} (j), \quad \text {d e p e n d s o n l y o n} j \text {f o r} j = 0, \pm 1, \pm 2, \dots
$$

Hence the cross-correlation function (CCF) is dened as

$$
\rho_ {x y} (j) = \operatorname {c o r r} \left(x _ {t}, y _ {t + j}\right) = \frac {\gamma_ {x y} (j)}{\sqrt {\gamma_ {x} (0) \gamma_ {y} (0)}} \quad \text {f o r} \quad j = 0, \pm 1, \pm 2, \dots \tag {6.18}
$$

It should be noted that $\rho _ { x y } ( j ) \neq \rho _ { x y } ( - j )$ but $\rho _ { x y } ( j ) = \rho _ { y x } ( - j )$ .

We then dene the correlation matrix at lag $j$ as

$$
\begin{array}{l} \rho (j) = \left[ \begin{array}{l l} \rho_ {x} (j) & \rho_ {x y} (j) \\ \rho_ {y x} (j) & \rho_ {y} (j) \end{array} \right] \tag {6.19} \\ = \operatorname {c o r r} \left[ \left( \begin{array}{l} x _ {t} \\ y _ {t} \end{array} \right), (x _ {t + j} y _ {t + j}) \right] \\ \end{array}
$$

For a given sample of $N$ observations, the sample cross covariance is estimated from

$$
\hat {\gamma} _ {x y} (j) = \frac {1}{N} \sum_ {t = 1} ^ {N - j} \left(x _ {t} - \bar {x}\right) \left(y _ {t + j} - \bar {y}\right) \quad \text {f o r} \quad j = 0, 1, 2, \dots \tag {6.20}
$$

and

$$
\hat {\gamma} _ {x y} (j) = \frac {1}{N} \sum_ {t = 1} ^ {N + j} (x _ {t - j} - \bar {x}) (y _ {t} - \bar {y}) \quad \mathrm {f o r} \quad j = - 1, - 2, \dots \tag {6.21}
$$

Similarly, the sample cross correlations are estimated from

$$
r _ {x y} (j) = \hat {\rho} _ {x y} (j) = \frac {\hat {\gamma} _ {x y} (j)}{\sqrt {\hat {\gamma} _ {x} (0) \hat {\gamma} _ {y} (0)}} \quad \text {f o r} \quad j = 0, \pm 1, \pm 2, \dots \tag {6.22}
$$

where

$$
\hat {\gamma} _ {x} (0) = \frac {1}{N} \sum_ {t = 1} ^ {N} (x _ {t} - \bar {x}) ^ {2} \quad \mathrm {a n d} \quad \hat {\gamma} _ {y} (0) = \frac {1}{N} \sum_ {t = 1} ^ {N} (y _ {t} - \bar {y}) ^ {2}
$$

Sampling properties such as the mean and variance of the sample CCF are quite complicated. For a few special cases, however, we have the following.

1. For large data sets, $E ( r _ { x y } ( j ) ) \approx \rho _ { x y } ( j )$ but the variance is still complicated.

2. If $x _ { t }$ and $y _ { t }$ are autocorrelated but un(cross)correlated at all lags, that is, $\rho _ { x y } ( j ) = 0$ , we then have $E ( r _ { x y } ( j ) ) \approx 0$ and $\mathrm { v a r } ( r _ { x y } ( j ) ) \approx$ $\textstyle ( 1 / N ) \sum _ { i = - \infty } ^ { \infty } \rho _ { x } ( i ) \rho _ { y } ( i )$ .

3. If $\rho _ { x y } ( j ) = 0$ for all lags $j$ but also $x _ { t }$ is white noise, that is, $\rho _ { x } ( j ) = 0$ for $j \neq 0$ , then we have $\mathrm { v a r } ( r _ { x y } ( j ) ) \approx 1 / N \mathrm { f o r } j = 0 , \pm 1 , \pm 2 , . . .$ $( r _ { x y } ( j ) ) \approx 1 / N$ .

4. If $\rho _ { x y } ( j ) = 0$ for all lags $j$ but also both $x _ { t }$ and $y _ { t }$ are white noise, then we have $\mathrm { c o r r } ( r _ { x y } ( i ) , r _ { x y } ( j ) ) \approx 0$ for $i \neq j$ .

# 6.5 MODEL SPECIFICATION

In this section, we will discuss the issues regarding the specication of the model order in a transfer function鈥搉oise model. Further discussion can be found in Bisgaard and Kulahci (2006a,b, 2011).

We will rst consider the general form of the transfer function鈥搉oise model with time delay given as

$$
\begin{array}{l} y _ {t} = v (B) x _ {t} + N _ {t} \\ = \frac {w (B)}{\delta (B)} x _ {t - b} + \frac {\theta (B)}{\varphi (B)} \varepsilon_ {t}. \tag {6.23} \\ \end{array}
$$

The six-step model specication process is outlined next.

Step 1. Obtaining the preliminary estimates of the coefcients in $\nu ( B )$ .

One approach is to assume that the coefcients in $\nu ( B )$ are zero except for the rst $k$ lags:

$$
y _ {t} \cong \sum_ {i = 0} ^ {k} v _ {i} x _ {t - i} + N _ {t}.
$$

We can then attempt to obtain the initial estimates for $\nu _ { 1 }$ , $\nu _ { 2 } , \ldots , \nu _ { k }$ through ordinary least squares. However, this approach can lead to highly inaccurate estimates as $x _ { t }$ may have strong autocorrelation. Therefore a method called prewhitening of the input is generally preferred.

Method of Prewhitening For the transfer function鈥搉oise model in Eq. (6.23), suppose that $x _ { t }$ follows an ARIMA model as

$$
\underbrace {\phi_ {x} (B) (1 - B) ^ {d}} _ {= \varphi_ {x} (B)} x _ {t} = \theta_ {x} (B) \alpha_ {t}, \tag {6.24}
$$

where $\alpha _ { t }$ is white noise with variance $\sigma _ { \alpha } ^ { 2 }$ . Equivalently, we have

$$
\alpha_ {t} = \theta_ {x} (B) ^ {- 1} \varphi_ {x} (B) x _ {t}. \tag {6.25}
$$

In this notation, $\theta _ { x } ( B ) ^ { - 1 } \varphi _ { x } ( B )$ can be seen as a lter that when applied to $x _ { t }$ generates a white noise time series, hence the name 鈥減rewhitening.鈥?
When we apply this lter to the transfer function鈥搉oise model in Eq. (6.23), we obtain

$$
\begin{array}{l} \underbrace {\theta_ {x} (B) ^ {- 1} \varphi_ {x} (B) y _ {t}} _ {= \beta_ {t}} = \theta_ {x} (B) ^ {- 1} \varphi_ {x} (B) v (B) x _ {t} + \underbrace {\theta_ {x} (B) ^ {- 1} \varphi_ {x} (B) N _ {t}} _ {= N _ {t} ^ {*}} \tag {6.26} \\ \beta_ {t} = v (B) \alpha_ {t} + N _ {t} ^ {*} \\ \end{array}
$$

The cross covariance between the ltered series $\alpha _ { t }$ and $\beta _ { t }$ is given by

$$
\begin{array}{l} \gamma_ {\alpha \beta} (j) = \operatorname {C o v} \left(\alpha_ {t}, \beta_ {t + j}\right) = \operatorname {C o v} \left(\alpha_ {t}, v (B) \alpha_ {t + j} + N _ {t + j} ^ {*}\right) \\ = \operatorname {C o v} \left(\alpha_ {t}, \sum_ {i = 0} ^ {\infty} v _ {i} \alpha_ {t + j - i} + N _ {t + j} ^ {*}\right) \\ = \operatorname {C o v} \left(\alpha_ {t}, \sum_ {i = 0} ^ {\infty} v _ {i} \alpha_ {t + j - i}\right) + \underbrace {\operatorname {C o v} \left(\alpha_ {t} , N _ {t + j} ^ {*}\right)} _ {= 0} \tag {6.27} \\ = \sum_ {i = 0} ^ {\infty} v _ {i} \operatorname {C o v} \left(\alpha_ {t}, \alpha_ {t + j - i}\right) \\ = v _ {j} \operatorname {V a r} \left(\alpha_ {t}\right). \\ \end{array}
$$

Note that $\mathrm { C o v } ( \alpha _ { t } , N _ { t + j } ^ { * } ) = 0$ since $x _ { t }$ and $N _ { t }$ are assumed to be independent.

From Eq. (6.27), we have $\gamma _ { \alpha \beta } = \nu _ { j } \sigma _ { \alpha } ^ { 2 }$ and hence

$$
\begin{array}{l} v _ {j} = \frac {\gamma_ {\alpha \beta} (j)}{\sigma_ {\alpha} ^ {2}} = \frac {\rho_ {\alpha \beta} (j) \sigma_ {\alpha} \sigma_ {\beta}}{\sigma_ {\alpha} ^ {2}} \tag {6.28} \\ = \rho_ {\alpha \beta} (j) \frac {\sigma_ {\beta}}{\sigma_ {\alpha}}, \\ \end{array}
$$

where $\rho _ { \alpha \beta } ( j ) = \mathrm { c o r r } ( \alpha _ { t } , \beta _ { t + j } )$ is the CCF between $\alpha _ { t }$ and $\beta _ { t }$ . So through the sample estimates we can obtain the initial estimates for the $\nu _ { j }$ :

$$
\hat {v} _ {j} = r _ {\alpha \beta} (j) \frac {\hat {\sigma} _ {\beta}}{\hat {\sigma} _ {\alpha}}. \tag {6.29}
$$

Equation (6.29) implies that there is a simple relationship between the impulse response function, $\nu ( B )$ , and the cross-correlation function of the prewhitened response and input series. Hence the estimation of the coef-cients in $\nu ( B )$ is possible through this relationship as summarized in Eq. (6.29). A similar relationship exists when the response and the input are not prewhitened (see Box et al., 2008). However, the calculations become fairly complicated when the series are not prewhitened. Therefore we strongly recommend the use of prewhitening in model identication and estimation of transfer function鈥搉oise models.

Moreover, since $\alpha _ { t }$ is white noise, the variance of $r _ { \alpha \beta } ( j )$ is relatively easier to obtain than that of $r _ { x y } ( j )$ . In fact, from the special case 3 in the previous section, we have

$$
\operatorname {V a r} \left[ r _ {\alpha \beta} (j) \right] \approx \frac {1}{N}, \tag {6.30}
$$

if $\rho _ { \alpha \beta } ( j ) = 0$ for all lags $j$ . We can then use $\pm 2 / \sqrt { N }$ as the approximate $9 5 \%$ condence interval to judge the signicance of $r _ { \alpha \beta } ( j )$ .

Step 2. Specications of the orders $r$ and $s$ .

Once the initial estimates of the $\nu _ { j }$ from Eq. (6.29) are obtained, we can use them to specify the orders $r$ and $s$ in

$$
\begin{array}{l} v (B) = \frac {w (B)}{\delta (B)} B ^ {b} \\ = \frac {w _ {0} - w _ {1} B - \cdots - w _ {s} B ^ {s}}{1 - \delta_ {1} B - \cdots - \delta_ {r} B ^ {r}} B ^ {b} \\ \end{array}
$$

The specication of the orders $r$ and $s$ can be accomplished by plotting the $\nu _ { j }$ . In Figure 6.2, we have an example of the plot of the initial estimates for the $\nu _ { j }$ in which we can see that $\hat { \nu } _ { 0 } \approx 0$ , implying that there might be a

![](images/813b32379923b319cf9008cf232074ebf6865972fca3f0e21b54f939f9c1c170.jpg)  
FIGURE 6.2 Example of an impulse response function.

time delay (i.e., $b = 1$ ). However, for $j > 1$ , we have an exponential decay pattern, suggesting that we may have $r = 1$ , which implies

$$
v _ {j} - \delta v _ {j - 1} = 0 \quad \mathrm {f o r} \quad j > 1
$$

and

$$
s = 0.
$$

Hence for this example, our initial attempt in specifying the order of the transfer function noise model will be

$$
y _ {t} = \frac {w _ {0}}{1 - \delta B} x _ {t - 1} + N _ {t}. \tag {6.31}
$$

Caution: In model specication, one should be acutely aware of overparameterization as for an arbitrary $\eta$ , the model in Eq. (6.31) can also be written as

$$
\begin{array}{l} y _ {t} = \frac {w _ {0} (1 - \eta B)}{(1 - \delta B) (1 - \eta B)} x _ {t - 1} + N _ {t} \tag {6.32} \\ = \frac {w _ {0} - w _ {1} B}{1 - \delta_ {1} B - \delta_ {2} B ^ {2}} x _ {t - 1} + N _ {t}. \\ \end{array}
$$

But the parameters in Eq. (6.32) are not identiable, since $\eta$ can arbitrarily take any value.

Step 3. Obtain the estimates of $\delta _ { i }$ and $w _ { i }$ .

From $\hat { \delta } ( B ) \hat { \nu } ( B ) = \hat { w } ( B )$ , we can recursively estimate $\delta _ { i }$ and $w _ { i }$ using Eq. (6.12),

$$
v _ {j} - \delta_ {1} v _ {j - 1} - \delta_ {2} v _ {j - 2} - \ldots - \delta_ {r} v _ {j - r} = \left\{ \begin{array}{l l} - w _ {j - b}, & j = b + 1, \ldots , b + s \\ 0, & j > b + s \end{array} \right.
$$

with $\nu _ { b } = w _ { 0 }$ and $\nu _ { j } = 0$ for $j < b$ . Hence for the example in Step 2, we have

$$
\hat {v} _ {1} = \hat {w} _ {0}
$$

$$
\hat {v} _ {2} - \hat {\delta} \hat {v} _ {1} = 0
$$

鈰?
Step 4. Model the noise.

Once the initial estimates of the model parameters are obtained, the estimated noise can be obtained as

$$
\hat {N} _ {t} = y _ {t} - \frac {\hat {w} (B)}{\hat {\delta} (B)} x _ {t - \hat {b}}, \tag {6.33}
$$

To obtain the estimated noise, we dene $\hat { y } _ { t } = ( \hat { w } ( B ) / \hat { \delta } ( B ) ) x _ { t - \hat { b } }$ . We can then calculate $\hat { y } _ { t }$ recursively. To model the estimated noise, we observe its ACF and PACF and determine the orders of the ARIMA model, $\phi ( B ) ( 1 - B ) ^ { d } N _ { t } = \theta ( B ) \varepsilon _ { t }$ .

Step 5. Fitting the overall model.

Steps 1 through 4 provide us with the model specications and the initial estimates of the parameters in the transfer function鈥搉oise model,

$$
y _ {t} = \frac {w (B)}{\delta (B)} x _ {t - b} + \frac {\theta (B)}{\phi (B) (1 - B) ^ {d}} \varepsilon_ {t}.
$$

The nal estimates of the model parameters are then obtained by a nonlinear model t. Model selection criteria such as AIC and BIC can be used to pick the 鈥渂est鈥?model among competing models.

Step 6. Model adequacy checks.

At this step, we check the validity of the two assumptions in the tted model:

1. The assumption that the noise $\varepsilon _ { t }$ is white noise requires the examination of the residuals $\hat { \varepsilon } _ { t }$ . We perform the usual checks through analysis of the sample ACF and PACF of the residuals.   
2. We should also check the independence between $\varepsilon _ { t }$ and $x _ { t }$ . For that, we observe the sample cross-correlation function between $\hat { \varepsilon } _ { t }$ and $\hat { x } _ { t }$ . Alternatively, we can examine $r _ { \hat { \alpha } \hat { \varepsilon } } ( j )$ , where $\alpha _ { t } = \hat { \theta } _ { x } ( B ) ^ { - 1 } \hat { \varphi } _ { x } \overset { \cdot } { ( } B ) x _ { t }$ . Under the assumption the model is adequate, $r _ { \hat { \alpha } \hat { \varepsilon } } ( j )$ will have 0 mean, $1 / \sqrt { N }$ standard deviation, and be independent for different lags $j$ . Hence we can use $\pm 2 / \sqrt { N }$ as the limit to check the independence assumption.

TABLE 6.2 The viscosity, $y ( t )$ and temperature, $\boldsymbol { x } ( t )$   

<table><tr><td>x(t)</td><td>y(t)</td><td>x(t)</td><td>y(t)</td><td>x(t)</td><td>y(t)</td><td>x(t)</td><td>y(t)</td></tr><tr><td>0.17</td><td>0.30</td><td>0.08</td><td>0.53</td><td>0.00</td><td>0.34</td><td>-0.04</td><td>-0.12</td></tr><tr><td>0.13</td><td>0.18</td><td>0.17</td><td>0.54</td><td>0.02</td><td>0.13</td><td>0.11</td><td>-0.26</td></tr><tr><td>0.19</td><td>0.09</td><td>0.20</td><td>0.42</td><td>-0.08</td><td>0.21</td><td>0.19</td><td>0.20</td></tr><tr><td>0.09</td><td>0.06</td><td>0.20</td><td>0.37</td><td>-0.08</td><td>0.06</td><td>-0.07</td><td>0.18</td></tr><tr><td>0.03</td><td>0.30</td><td>0.27</td><td>0.34</td><td>-0.26</td><td>0.04</td><td>-0.10</td><td>0.32</td></tr><tr><td>0.11</td><td>0.44</td><td>0.23</td><td>0.27</td><td>-0.06</td><td>-0.06</td><td>0.13</td><td>0.50</td></tr><tr><td>0.15</td><td>0.46</td><td>0.23</td><td>0.34</td><td>-0.06</td><td>-0.16</td><td>0.10</td><td>0.40</td></tr><tr><td>-0.02</td><td>0.44</td><td>0.20</td><td>0.35</td><td>-0.09</td><td>-0.47</td><td>-0.10</td><td>0.41</td></tr><tr><td>0.07</td><td>0.34</td><td>0.08</td><td>0.43</td><td>-0.14</td><td>-0.50</td><td>-0.05</td><td>0.47</td></tr><tr><td>0.00</td><td>0.23</td><td>-0.16</td><td>0.63</td><td>-0.10</td><td>-0.60</td><td>-0.12</td><td>0.37</td></tr><tr><td>-0.08</td><td>0.07</td><td>-0.08</td><td>0.61</td><td>-0.25</td><td>-0.49</td><td>0.00</td><td>0.04</td></tr><tr><td>-0.15</td><td>0.21</td><td>0.14</td><td>0.52</td><td>-0.23</td><td>-0.27</td><td>0.03</td><td>-0.10</td></tr><tr><td>-0.15</td><td>0.03</td><td>0.17</td><td>0.06</td><td>-0.11</td><td>-0.18</td><td>-0.06</td><td>-0.34</td></tr><tr><td>0.04</td><td>-0.20</td><td>0.27</td><td>-0.11</td><td>-0.01</td><td>-0.37</td><td>0.03</td><td>-0.41</td></tr><tr><td>0.08</td><td>-0.39</td><td>0.19</td><td>-0.01</td><td>-0.17</td><td>-0.34</td><td>0.04</td><td>-0.33</td></tr><tr><td>0.10</td><td>-0.70</td><td>0.10</td><td>0.02</td><td>-0.23</td><td>-0.34</td><td>0.09</td><td>-0.25</td></tr><tr><td>0.07</td><td>-0.22</td><td>0.13</td><td>0.34</td><td>-0.28</td><td>-0.18</td><td>-0.25</td><td>-0.18</td></tr><tr><td>-0.01</td><td>-0.08</td><td>-0.05</td><td>0.21</td><td>-0.26</td><td>-0.26</td><td>-0.25</td><td>-0.06</td></tr><tr><td>0.06</td><td>0.16</td><td>0.13</td><td>0.18</td><td>-0.19</td><td>-0.51</td><td>-0.40</td><td>0.15</td></tr><tr><td>0.07</td><td>0.13</td><td>-0.02</td><td>0.19</td><td>-0.26</td><td>-0.65</td><td>-0.30</td><td>-0.32</td></tr><tr><td>0.17</td><td>0.07</td><td>0.04</td><td>0.05</td><td>-0.20</td><td>-0.71</td><td>-0.18</td><td>-0.32</td></tr><tr><td>-0.01</td><td>0.23</td><td>0.00</td><td>0.15</td><td>-0.08</td><td>-0.82</td><td>-0.09</td><td>-0.81</td></tr><tr><td>0.09</td><td>0.33</td><td>0.08</td><td>0.10</td><td>0.03</td><td>-0.70</td><td>-0.05</td><td>-0.87</td></tr><tr><td>0.22</td><td>0.72</td><td>0.08</td><td>0.28</td><td>-0.08</td><td>-0.63</td><td>0.09</td><td>-0.84</td></tr><tr><td>0.09</td><td>0.45</td><td>0.07</td><td>0.20</td><td>0.01</td><td>-0.29</td><td>0.18</td><td>-0.73</td></tr></table>

Example 6.2 In a chemical process it is expected that changes in temperature affect viscosity, a key quality characteristic. It is therefore of great importance to learn more about this relationship. The data are collected every 10 seconds and given in Table 6.2 (Note that for each variable the data are centered by subtracting the respective averages). Figure 6.3 shows the time series plots of the two variables.

Since the data are taken in time and at frequent intervals, we expect the variables to exhibit some autocorrelation and decide to t a transfer function-noise model following the steps provided earlier.

![](images/0a126a5a016344bb70a905168ad785c829d4660ed393ae298cde8b6d5927d2d5.jpg)

![](images/8373256f8e370e8301cabdbd7afef37353b5a632e3120bfe7ddba0b10a88d2a7.jpg)  
FIGURE 6.3 Time series plots of the viscosity, $y ( t )$ and temperature, $x ( t )$ .

![](images/06daaf2f313a1aae1f2a0c48cf82cfeaa22b11cbb663787b609acf174ec31d2c.jpg)

![](images/3602de1f94786245563bbae0fb131dddf2eaa964c1735d39dfce73918c17a087.jpg)  
FIGURE 6.4 Sample ACF and PACF of the temperature.

Step 1. Obtaining the preliminary estimates of the coefcients in $\nu ( B )$

In this step we use the prewhitening method. First we t an ARIMA model to the temperature. Since the time series plot in Figure 6.3 shows that the process is changing around a constant mean and has a constant variance, we will assume that it is stationary.

Sample ACF and PACF plots in Figure 6.4 suggest that an AR(1) model should be used to t the temperature data. Table 6.3 shows that $\hat { \phi } \cong 0 . 7 3$ .

TABLE 6.3 AR(1) Model for Temperature, $\boldsymbol { x } ( t )$   

<table><tr><td colspan="5">Parameter Estimate of the AR(1) model for x(t)</td></tr><tr><td>Term</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P-value</td></tr><tr><td>AR 1</td><td>0.7292</td><td>0.0686</td><td>10.63</td><td>&lt;0.0001</td></tr><tr><td colspan="5">Number of degrees of freedom: 99</td></tr><tr><td colspan="5">MSE = 0.01009</td></tr><tr><td colspan="5">AIC = -171.08</td></tr></table>

![](images/64dad61f246060ad0475145cee97696ee9147345290067205495a8aa7437d1cf.jpg)

![](images/4d9fcea29da774642f10c1318b002721d81526cf53391eed042078e8993003ee.jpg)  
FIGURE 6.5 Sample ACF and PACF of the residuals from the AR(1) model for the Temperature, $x ( t )$ .

The sample ACF and PACF plots in Figure 6.5 as well as the additional residuals plots in Figure 6.6 reveal that no autocorrelation is left in the data and the model gives a reasonable t.

Hence we dene

$$
\alpha_ {t} = (1 - 0. 7 3 B) x _ {t}
$$

and

$$
\beta_ {t} = (1 - 0. 7 3 B) y _ {t}
$$

We then compute the sample cross-correlation of $\alpha _ { t }$ and $\beta _ { t }$ , $r _ { \alpha \beta }$ given in Figure 6.7. Since the cross correlation at lags 0, 1 and 2 do not seem to be signicant, we conclude that there is a delay of 3 lags (30 seconds) in the system, that is, $b = 3$ .

![](images/c769a3d2bb32a410212e1a0e05630791a57da08bc2c3b322cafb21f43a583326.jpg)

![](images/5420f919ee57336c95ae87e0de2ffc052140b3e5d0c9acf9edd0f42a1963f48e.jpg)

![](images/f3cb7abdf70f5db186caa7645909f1fccb3195f8702472ae7b45d1345e636030.jpg)

![](images/52df9cf63c099f313bed5ad287333fd0867489ae6ff17c25fd6013b095a78175.jpg)  
FIGURE 6.6 Residual plots from the AR(1) model for the temperature.

![](images/269770bbb670400a3a55fb8b1f2476e86fc4e2514b7548851b9eff71d0719573.jpg)  
FIGURE 6.7 Sample cross-correlation function between $\alpha _ { t }$ and $\beta _ { t }$ t .

From Eq. (6.34), we have

$$
\begin{array}{l} \hat {v} _ {j} = r _ {\alpha \beta} (j) \frac {\hat {\sigma} _ {\beta}}{\hat {\sigma} _ {\alpha}} \\ = r _ {\alpha \beta} (j) \frac {0 . 1 8 8 1}{0 . 1 0 0 8}, \tag {6.34} \\ \end{array}
$$

where $\hat { \sigma } _ { \alpha }$ and $\hat { \sigma } _ { \beta }$ are the sample standard deviations of $\alpha _ { t }$ and $\beta _ { t }$ . The plot of $\hat { \nu } _ { j }$ is given in Figure 6.8.

![](images/daedf94feabfe990e32d0253f68b5fd7d4a6f5e31b04916e0c27594bddd6370e.jpg)  
FIGURE 6.8 Plot of the impulse response function for the viscosity data.

Step 2. Specications of the orders $r$ and s.

To identify the pattern in Figures 6.7 and 6.8, we can refer back to Table 6.1. From the examples of impulse response functions given in that table, we may conclude the denominator of the transfer function is a second order polynomial in B. That is, $r = 2$ so we have $1 - \delta _ { 2 } B - \delta _ { 2 } , B ^ { 2 }$ for the denominator. For the numerator, it seems that $s = 0$ or $w _ { 0 }$ would be appropriate. Hence our tentative impulse response function is the following

$$
\nu_ {t} = \frac {w _ {0}}{1 - \delta_ {1} B - \delta_ {2} B ^ {2}} B ^ {3}.
$$

Step 3. Obtain the estimates of the $\delta _ { i }$ and $\mathrm { w } _ { i }$ .

To obtain the estimates of $\delta _ { i }$ and $\mathrm { w } _ { i }$ , we refer back to Eq. (6.12) which implies that we have

$$
\begin{array}{l} \hat {v} _ {0} \approx 0 \\ \hat {v} _ {1} \approx 0 \\ \hat {v} _ {2} \approx 0 \\ \hat {v} _ {3} = 1. 2 1 = \hat {w} _ {0} \\ \hat {\nu} _ {4} = 0. 3 7 = 1. 2 1 \hat {\delta} _ {1} \\ \hat {\nu} _ {5} = 0. 6 9 = 0. 3 7 \hat {\delta} _ {1} + 1. 2 1 \hat {\delta} _ {2} \\ \end{array}
$$

The parameter estimates are then

$$
\begin{array}{l} \hat {w} _ {0} = 1. 2 1 \\ \hat {\delta} _ {1} = 0. 3 1 \\ \hat {\delta} _ {2} = 0. 4 8 \\ \end{array}
$$

or

$$
\hat {\nu} _ {t} = \frac {1 . 2 1}{1 - 0 . 3 1 B - 0 . 4 8 B ^ {2}} B.
$$

Step 4. Model the noise.

To model the noise, we rst dene $\hat { y } _ { t } = \frac { \hat { w } ( B ) } { \hat { \delta } ( B ) } x _ { t - 3 }$ o r

$$
\begin{array}{l} \hat {\delta} (B) \hat {y} _ {t} = \hat {w} (B) x _ {t - 3} \\ (1 - 0. 3 1 B - 0. 4 8 B ^ {2}) \hat {y} _ {t} = 1. 2 1 x _ {t - 3} \\ \hat {y} _ {t} = 0. 3 1 \hat {y} _ {t - 1} + 0. 4 8 \hat {y} _ {t - 2} + 1. 2 1 x _ {t - 3}. \\ \end{array}
$$

![](images/7ada9f7c8475834c886c7299fac340de3cf3c647c2d90ad43883991138b7a75d.jpg)  
FIGURE 6.9 Time series plot of $\hat { N } _ { t }$ .

We then dene

$$
\hat {N} _ {t} = y _ {t} - \hat {y} _ {t}.
$$

Figures 6.9 and 6.10 show the time series plot of $\hat { N } _ { t }$ and its sample ACF/PACF plots respectively which indicate an AR model. Note that partial autocorrelation at lag 3 is borderline signicant. However when an AR(3) model is tted, both $\phi _ { 2 }$ and $\phi _ { 3 }$ are found to be insignicant. Therefore AR(1) model is considered to be the appropriate model.

The parameter estimates for the AR(1) model for $\bar { \hat { N } } _ { t }$ are given in Table 6.4. Diagnostic checks of the residuals through sample ACF and PACF plots in Figure 6.11 and residuals plots in Figure 6.12 imply that we have a good t.

![](images/e8da276632870df8b08c6daddcc582d461be1bb396f0c0992dc86e1f7a3e4d08.jpg)

![](images/96b2d88e982b3214cdd11b04fd8390894e55fe2aa0f08080a7368c174da47ae8.jpg)  
FIGURE 6.10 Sample ACF and PACF of $\hat { N } _ { t }$ .

TABLE 6.4 AR(1) Model for $N _ { t }$   

<table><tr><td colspan="5">Parameter Estimate of the AR(1) model for x(t)</td></tr><tr><td>Term</td><td>Coef</td><td>SE Coef</td><td>T</td><td>P-value</td></tr><tr><td>AR 1</td><td>0.9426</td><td>0.0300</td><td>31.42</td><td>&lt;0.0001</td></tr><tr><td colspan="5">Number of degrees of freedom: 96</td></tr><tr><td colspan="5">MSE = 0.0141</td></tr><tr><td colspan="5">AIC = -131.9</td></tr></table>

![](images/5d260ef8dcc5b481eaa3c20c712ffb494d43386c660f32abe0ef660515dc7199.jpg)

![](images/d16aa1ff01ce316f8045850962bc432076c758492dc89bb21062544b3e388789.jpg)  
FIGURE 6.11 Sample ACF and PACF of the residuals of the AR(1) model for $\hat { N } _ { t }$ .

![](images/cfdc153d0c85bb90794cbc055ed59fe7d4e5e420bd4e034c4fb2af96f957a3c2.jpg)

![](images/926d9135861a815da5f94a3c250426973c131d2d3b157368d28365d76202ff0b.jpg)

![](images/7dd8225e85e13787e85a67cd0a7787f05d97126dbaef43e99b4c54f5f453c916.jpg)

![](images/a63e79a345fb617405c6c2ee66895c06a27c6c82e12e3b61816d9bebd59ba603.jpg)  
FIGURE 6.12 Residual plots of the AR(l) model for $\hat { N } _ { t }$ .

Note that we do not necessarily need the coefcients estimates as they will be re-estimated in the next step. Thus at this step all we need is a sensible model for $\hat { N } _ { t }$ to put into the overall model.

Step 5. Fitting the overall model.

From Step 4, we have the tentative overall model as

$$
y _ {t} = \frac {w _ {0}}{1 - \delta_ {1} B - \delta_ {2} B ^ {2}} x _ {t - 3} + \frac {1}{1 - \phi_ {1} B} \varepsilon_ {t}.
$$

The calculations that were made so far could have been performed practically in any statistical package. However as we mentioned at the beginning of the Chapter, unfortunately only a few software packages have the capability to t the overall transfer function-noise model described above. In the following we provide the output from JMP with which such a model can be tted. At the end of the chapter, we also provide the R code that can be used to t the transfer function-noise model.

JMP output for the overall transfer function-noise model is provided in Table 6.5. The estimated coefcients are

$$
\hat {w} _ {0} = 1. 3 2 7 6, \quad \hat {\delta} _ {1} = 0. 3 4 1 4, \quad \hat {\delta} _ {2} = 0. 2 6 6 7, \quad \hat {\phi} _ {1} = 0. 8 2 9 5,
$$

and they are all signicant.

Step 6. Model adequacy checks

The sample ACF and PACF of the residuals provided in Table 6.5 show no indication of leftover autocorrelation. We further check the cross correlation function between $\alpha _ { t } = ( 1 - 0 . 7 3 B ) x _ { t }$ and the residuals as given in Figure 6.13. There is a borderline signicant cross correlation at lag 5. However we believe that it is at this point safe to claim that the current tted model is adequate.

Example 6.2 illustrates transfer function modeling with a single input series where both the input and output time series were stationary. It is often necessary to incorporate multiple input time series into the model. A simple generalization of the single-input transfer function is to form an additive model for the inputs, say

$$
y _ {t} = \sum_ {j = 1} ^ {m} \frac {\omega_ {j} (B)}{\delta_ {j} (B)} x _ {j, t - b _ {j}} \tag {6.35}
$$

TABLE 6.5 JMP Output for the Viscosity-Temperature Transfer Function-Noise Model   

<table><tr><td colspan="2">Transfer Function Analysis</td></tr><tr><td colspan="2">Time Series y(t)</td></tr><tr><td rowspan="6">y(t)</td><td>Mean 0.0001</td></tr><tr><td>Std 0.3860375</td></tr><tr><td>N 100</td></tr><tr><td>Zero Mean ADF -2.038514</td></tr><tr><td>Single Mean ADF -2.020093</td></tr><tr><td>Trend ADF -2.548358</td></tr><tr><td>0</td><td>20</td></tr><tr><td>-0.5</td><td>40</td></tr><tr><td>-1</td><td>0</td></tr><tr><td>Row</td><td>60</td></tr><tr><td>80</td><td>100</td></tr></table>

Transfer Function Model (1)   

<table><tr><td colspan="2">Model Summary</td></tr><tr><td>DF</td><td>93</td></tr><tr><td>Sum of Squared Errors</td><td>1.19314545</td></tr><tr><td>Variance Estimate</td><td>0.01282952</td></tr><tr><td>Standard Deviation</td><td>0.11326746</td></tr><tr><td>Akaike&#x27;s A&#x27; Information Criterion</td><td>-142.17826</td></tr><tr><td>Schwarz&#x27;s Bayesian Criterion</td><td>-131.87942</td></tr><tr><td>RSquare</td><td>0.91993657</td></tr><tr><td>RSquare Adj</td><td>0.91743459</td></tr><tr><td>MAPE</td><td>56.3367497</td></tr><tr><td>MAE</td><td>0.08684777</td></tr><tr><td>-2LogLikelihood</td><td>-150.17826</td></tr></table>

![](images/ed239f6f42ab62e7c06b6fd27c1e5e065ecc444b643daf9ea671e9f1c0e76f2c.jpg)  
Residuals

<table><tr><td>Lag</td><td>AutoCorr</td><td colspan="2">-.8-.6-.4-.2 0 .2 .4 .6 .8</td><td>Ljung-Box Q</td><td>p-Value</td><td>Lag</td><td>Partial</td><td>-.8-.6-.4-.2 0 .2 .4 .6 .8</td></tr><tr><td>0</td><td>1.0000</td><td></td><td></td><td></td><td></td><td>0</td><td>1.0000</td><td></td></tr><tr><td>1</td><td>0.0797</td><td></td><td></td><td>0.6542</td><td>0.4186</td><td>1</td><td>0.0797</td><td></td></tr><tr><td>2</td><td>0.1208</td><td></td><td></td><td>2.1735</td><td>0.3373</td><td>2</td><td>0.1152</td><td></td></tr><tr><td>3</td><td>-0.0019</td><td></td><td></td><td>2.1739</td><td>0.5371</td><td>3</td><td>-0.0200</td><td></td></tr><tr><td>4</td><td>-0.1254</td><td></td><td></td><td>3.8440</td><td>0.4275</td><td>4</td><td>-0.1405</td><td></td></tr><tr><td>5</td><td>0.0867</td><td></td><td></td><td>4.6516</td><td>0.4599</td><td>5</td><td>0.1126</td><td></td></tr><tr><td>6</td><td>-0.1280</td><td></td><td></td><td>6.4301</td><td>0.3768</td><td>6</td><td>-0.1157</td><td></td></tr><tr><td>7</td><td>-0.0722</td><td></td><td></td><td>7.0020</td><td>0.4287</td><td>7</td><td>-0.0844</td><td></td></tr><tr><td>8</td><td>-0.1709</td><td></td><td></td><td>10.2410</td><td>0.2485</td><td>8</td><td>-0.1526</td><td></td></tr><tr><td>9</td><td>-0.1556</td><td></td><td></td><td>12.9565</td><td>0.1646</td><td>9</td><td>-0.0960</td><td></td></tr><tr><td>10</td><td>-0.1092</td><td></td><td></td><td>14.3080</td><td>0.1594</td><td>10</td><td>-0.1090</td><td></td></tr><tr><td>11</td><td>0.0132</td><td></td><td></td><td>14.3280</td><td>0.2154</td><td>11</td><td>0.0585</td><td></td></tr><tr><td>12</td><td>0.1584</td><td></td><td></td><td>17.2361</td><td>0.1409</td><td>12</td><td>0.1468</td><td></td></tr><tr><td>13</td><td>0.0907</td><td></td><td></td><td>18.2009</td><td>0.1500</td><td>13</td><td>0.0546</td><td></td></tr><tr><td>14</td><td>0.0066</td><td></td><td></td><td>18.2061</td><td>0.1976</td><td>14</td><td>-0.0824</td><td></td></tr><tr><td>15</td><td>-0.0355</td><td></td><td></td><td>18.3577</td><td>0.2443</td><td>15</td><td>-0.0690</td><td></td></tr><tr><td>16</td><td>0.0755</td><td></td><td></td><td>19.0505</td><td>0.2661</td><td>16</td><td>0.0674</td><td></td></tr><tr><td>17</td><td>0.1032</td><td></td><td></td><td>20.3586</td><td>0.2563</td><td>17</td><td>0.0600</td><td></td></tr><tr><td>18</td><td>0.0639</td><td></td><td></td><td>20.8663</td><td>0.2862</td><td>18</td><td>0.0078</td><td></td></tr><tr><td>19</td><td>-0.1056</td><td></td><td></td><td>22.2705</td><td>0.2710</td><td>19</td><td>-0.1365</td><td></td></tr><tr><td>20</td><td>0.0298</td><td></td><td></td><td>22.3837</td><td>0.3201</td><td>20</td><td>0.1179</td><td></td></tr><tr><td>21</td><td>-0.0848</td><td></td><td></td><td>23.3115</td><td>0.3276</td><td>21</td><td>-0.0129</td><td></td></tr><tr><td>22</td><td>-0.1518</td><td></td><td></td><td>26.3261</td><td>0.2379</td><td>22</td><td>-0.1502</td><td></td></tr><tr><td>23</td><td>-0.1162</td><td></td><td></td><td>28.1143</td><td>0.2115</td><td>23</td><td>-0.1532</td><td></td></tr><tr><td>24</td><td>-0.1560</td><td></td><td></td><td>31.3818</td><td>0.1431</td><td>24</td><td>-0.0949</td><td></td></tr><tr><td>25</td><td>0.0399</td><td></td><td></td><td>31.5985</td><td>0.1700</td><td>25</td><td>0.0526</td><td></td></tr></table>

![](images/66bf86b800f7a19f4e76c3628b5fcd7d7a2e73de0306ae79d4063d8cf5c640fe.jpg)  
FIGURE 6.13 Sample cross-correlation function between $\alpha _ { 1 }$ and the residuals of the transfer function-noise model.

where each input series has a transfer function representation including a potential delay. This is an appropriate approach so long as the input series are uncorrelated with each other. If any of the original series are nonstationary, then differencing may be required. In general, differencing of a higher order than one may be required, and inputs and outputs need not be identically differenced. We now present an example from Montgomery and Weatherby (1980) where two inputs are used in the model.

Example 6.3 Montgomery and Weatherby (1980) present an example of modeling the output viscosity of a chemical process as a function of two inputs, the incoming raw material viscosity $x _ { 1 , t }$ and the reaction temperature $x _ { 2 , t }$ . Readings are recorded hourly. Figure 6.14 is a plot of the last 100 readings. All three variables appear to be nonstationary.

Standard univariate ARIMA modeling techniques indicate that the input raw material viscosity can be modeled by an ARIMA(0, 1, 2) or IMA(1,2) process

$$
(1 - B) x _ {1, t} = (1 + 0. 5 9 B + 0. 3 2 B ^ {2}) \alpha_ {1, t},
$$

which is then used to prewhiten the output nal viscosity. Similarly, an IMA(1,1) model

$$
(1 - B) x _ {2, t} = (1 + 0. 4 5 B) \alpha_ {2, t}
$$

was used to prewhiten the temperature input. Table 6.6 contains the impulse response functions between the prewhitened inputs and outputs.

Both impulse response functions in Table 6.6 exhibit approximate exponential decay beginning with lag 2 for the initial viscosity input and lag 3

![](images/49e06af9b508f55759709e8a82e4d7bde9ff71fd0d2068beb313ef51cd3188e4.jpg)  
FIGURE 6.14 Hourly readings of nal product viscosity $y _ { t }$ , incoming raw material viscosity $x _ { r 1 , t }$ , and reaction temperature $x _ { 2 , t }$ .

for the temperature input. This is consistent with a tentative model identi-cation of $r = 1 , s = 0 , b = 2$ for the transfer function relating initial and nal viscosities and $r = 1 , s = 0 , b = 3$ for the transfer function relating temperature and nal viscosity. The preliminary parameter estimates for these models are

$$
\begin{array}{l} (1 - 0. 6 2 B) y _ {t} = 0. 3 4 x _ {1, t - 2} \\ (1 - 0. 6 8 B) y _ {t} = 0. 4 2 x _ {2, t - 3}. \\ \end{array}
$$

TABLE 6.6 Impulse Response Function for Example 6.3   

<table><tr><td colspan="3">Impulse Response Functions</td></tr><tr><td>Lag</td><td>Initial and Final Viscosity</td><td>Temperature and Final Viscosity</td></tr><tr><td>0</td><td>-0.422</td><td>-0.0358</td></tr><tr><td>1</td><td>-0.1121</td><td>-0.1835</td></tr><tr><td>2</td><td>0.3446</td><td>0.0892</td></tr><tr><td>3</td><td>0.2127</td><td>0.4205</td></tr><tr><td>4</td><td>0.1327</td><td>0.2849</td></tr><tr><td>5</td><td>0.2418</td><td>0.3102</td></tr><tr><td>6</td><td>0.0851</td><td>0.0899</td></tr><tr><td>7</td><td>0.1491</td><td>0.1712</td></tr><tr><td>8</td><td>0.1402</td><td>0.0051</td></tr></table>

The noise time series is then computed from

$$
\hat {N} _ {t} = y _ {t} - \frac {0 . 3 4}{1 - 0 . 6 2 B} x _ {1, t - 2} - \frac {0 . 4 2}{1 - 0 . 6 8 B} x _ {2, t - 3}.
$$

The sample ACF and PACF of the noise series indicate that it can be modeled as

$$
(1 - B) (1 - 0. 6 4 B) \hat {N} _ {t} = \varepsilon_ {t}.
$$

Therefore, the combined transfer function plus noise model is

$$
(1 - B) y _ {t} = \frac {\omega_ {1 0}}{1 - \delta_ {1 1} B} (1 - B) x _ {1, t - 2} + \frac {\omega_ {2 0}}{1 - \delta_ {2 1} B} (1 - B) x _ {2, t - 3} + \frac {\varepsilon_ {t}}{1 - \phi_ {1} B}.
$$

The estimates of the parameters in this model are shown in Table 6.7 along with other model summary statistics. The $t$ -test statistics indicate that all model parameters are different from zero.

Residual checks did not reveal any problems with model adequacy. The chi-square statistic for the rst 25 residual autocorrelations was 10.412. The rst 20 cross-correlations between the residuals and the prewhitened input viscosity produced a chi-square test statistic of 11.028 and the rst 20 cross-correlations between the residuals and the prewhitened input temperature produced a chi-square test statistic of 15.109. These chi-square

TABLE 6.7 Model Summary Statistics for the Two-Input Transfer Function Model in Example 6.3   

<table><tr><td rowspan="2">Parameter</td><td rowspan="2">Estimate</td><td rowspan="2">Standard Error</td><td rowspan="2">t-statistic for H0: Parameter = 0</td><td colspan="2">95% Confidence Limits</td></tr><tr><td>Lower</td><td>Upper</td></tr><tr><td>未11</td><td>0.789</td><td>0.110</td><td>7.163</td><td>0.573</td><td>1.005</td></tr><tr><td>蠅10</td><td>0.328</td><td>0.103</td><td>3.189</td><td>0.127</td><td>0.530</td></tr><tr><td>未21</td><td>0.677</td><td>0.186</td><td>3.643</td><td>0.313</td><td>1.041</td></tr><tr><td>蠅20</td><td>0.455</td><td>0.124</td><td>3.667</td><td>0.212</td><td>0.698</td></tr><tr><td>蠁1</td><td>0.637</td><td>0.082</td><td>7.721</td><td>0.475</td><td>0.799</td></tr></table>

test statistics are not signicant at the $25 \%$ level, so we conclude that the model is adequate. The correlation matrix of the parameter estimates is

<table><tr><td></td><td>未11</td><td>蠅10</td><td>未21</td><td>蠅20</td><td>蠁1</td></tr><tr><td>未11</td><td>1.00</td><td>-0.37</td><td>-0.06</td><td>0.01</td><td>0.02</td></tr><tr><td>蠅10</td><td>-0.37</td><td>1.00</td><td>-0.22</td><td>0.00</td><td>0.12</td></tr><tr><td>未21</td><td>-0.06</td><td>-0.22</td><td>1.00</td><td>-0.14</td><td>-0.07</td></tr><tr><td>蠅20</td><td>-0.01</td><td>0.00</td><td>-0.14</td><td>1.00</td><td>-0.08</td></tr><tr><td>蠁1</td><td>-0.02</td><td>0.07</td><td>-0.07</td><td>-0.08</td><td>1.00</td></tr></table>

Notice that a complex relationship between two input variables and one output has been modeled with only ve parameters and the small off-diagonal elements in the covariance matrix above imply that these parameter estimates are essentially uncorrelated.

# 6.6 FORECASTING WITH TRANSFERFUNCTION鈥揘OISE MODELS

In this section we discuss making $\tau$ -step-ahead forecasts using the transfer function鈥搉oise model in Eq. (6.23). We can rearrange Eq. (6.23) and rewrite it in the difference equation form as

$$
\delta (B) \varphi (B) y _ {t} = w (B) \varphi (B) x _ {t - b} + \theta (B) \delta (B) \varepsilon_ {t} \tag {6.36}
$$

or

$$
\delta^ {*} (B) y _ {t} = w ^ {*} (B) x _ {t - b} + \theta^ {*} (B) \varepsilon_ {t}. \tag {6.37}
$$

Then at time $t + \tau$ , we will have

$$
y _ {t + \tau} = \sum_ {i = 1} ^ {r + p ^ {*}} \delta_ {i} ^ {*} y _ {t + \tau - i} + w _ {0} ^ {*} x _ {t + \tau - b} - \sum_ {i = 1} ^ {s + p ^ {*}} w _ {i} ^ {*} x _ {t + \tau - b - i} + \varepsilon_ {t + \tau} - \sum_ {i = 1} ^ {q + r} \theta_ {i} ^ {*} \varepsilon_ {t + \tau - i}, \tag {6.38}
$$

where $r$ is the order of $\delta ( B ) , p ^ { * }$ is the order of $\varphi ( B ) ( = \phi ( B ) ( 1 - B ) ^ { d } )$ , and $s$ is the order of $\omega ( B )$ , and $q$ is the order of $\theta ( B )$ .

The $\tau$ -step ahead MSE forecasts are obtained from

$$
\begin{array}{l} \hat {y} _ {t + \tau} (\tau) = E [ y _ {t + \tau} | y _ {t}, y _ {t - 1}, \dots , x _ {t}, x _ {t - 1}, \dots ] \\ = \sum_ {i = 1} ^ {r + p ^ {*}} \delta_ {i} ^ {*} \hat {y} _ {t + \tau - i} (t) + w _ {0} ^ {*} \hat {x} _ {t + \tau - b} (t) \tag {6.39} \\ - \sum_ {i = 1} ^ {s + p ^ {*}} w _ {i} ^ {*} \hat {x} _ {t + \tau - b - i} (t) - \sum_ {i = 1} ^ {q + r} \theta_ {i} ^ {*} \varepsilon_ {t + \tau - i} \quad \mathrm {f o r} \tau = 1, 2, \ldots , q. \\ \end{array}
$$

Note that the MA terms will vanish for $\tau > q + r$ . We obtain Eq. (6.39) using

$$
E (\varepsilon_ {t + \tau - i} | y _ {t}, y _ {t - 1}, \ldots , x _ {t}, x _ {t - 1}, \ldots) = \left\{ \begin{array}{l l} \varepsilon_ {t + \tau - i}, & i \geq \tau \\ 0, & i <   \tau \end{array} \right.
$$

and

$$
\begin{array}{l} \hat {x} _ {t} (l) = E \left(x _ {t + l} \mid y _ {t}, y _ {t - 1}, \dots , x _ {t}, x _ {t - 1}, \dots\right) \tag {6.40} \\ = E (x _ {t + l} | x _ {t}, x _ {t - 1}, \ldots). \\ \end{array}
$$

Equation (6.40) implies that the relationship between $x _ { t }$ and $y _ { t }$ is unidirectional and that $\hat { x } _ { t } ( l )$ is the forecast from the univariate ARIMA model, $\phi _ { x } ( B ) ( 1 - B ) ^ { d } x _ { t } = \theta _ { x } ( B ) \alpha _ { t }$ .

So forecasts $\hat { y } _ { t + 1 } ( t ) , \ \hat { y } _ { t + 2 } ( t ) , \ldots$ $\hat { y } _ { t + 1 } ( t )$ can be computed recursively from Eqs. (6.39) and (6.40).

The variance of the forecast errors can be obtained from the innite MA representations for $x _ { t }$ and $N _ { t }$ given as

$$
\begin{array}{l} x _ {t} = \varphi_ {x} (B) ^ {- 1} \theta_ {x} (B) \alpha_ {t} \tag {6.41} \\ = \psi_ {x} (B) \alpha_ {t} \\ \end{array}
$$

and

$$
\begin{array}{l} N _ {t} = \varphi (B) ^ {- 1} \theta (B) \varepsilon_ {t} \\ = \psi (B) \varepsilon_ {t} \tag {6.42} \\ = \sum_ {i = 0} ^ {\infty} \psi_ {i} \varepsilon_ {t - i}. \\ \end{array}
$$

Hence the innite MA form of the transfer function鈥搉oise model is given as

$$
\begin{array}{l} y_{t} = \underbrace{v(B)\psi_{x}(B)}_{= v^{*}(B)}\alpha_{t - b} + \psi (B)\varepsilon_{t} \\ = \sum_ {i = 0} ^ {\infty} v _ {i} ^ {*} \alpha_ {t - b - i} + \sum_ {i = 0} ^ {\infty} \psi_ {i} \varepsilon_ {t - i.} \tag {6.43} \\ \end{array}
$$

Thus the minimum MSE forecast can be represented as

$$
\hat {y} _ {t + \tau} (t) = \sum_ {i = \tau - b} ^ {\infty} v _ {i} ^ {*} \alpha_ {t + \tau - b - i} + \sum_ {i = \tau} ^ {\infty} \psi_ {i} \varepsilon_ {t + \tau - i} \tag {6.44}
$$

and the $\tau$ -step-ahead forecast error is

$$
\begin{array}{l} e _ {t} (\tau) = y _ {t + \tau} - \hat {y} _ {t + \tau} (t) \quad \tag {6.45} \\ = \sum_ {i = 0} ^ {\tau - b - 1} v _ {i} ^ {*} \alpha_ {t + \tau - b - i} + \sum_ {i = 0} ^ {\tau - 1} \psi_ {i} \varepsilon_ {t + \tau - i}. \\ \end{array}
$$

As we can see in Eq. (6.45), the forecast error has two components that are assumed to be independent: forecast errors in forecasting $\textstyle \sum _ { i = 0 } ^ { \tau - b - 1 } \nu _ { i } ^ { * } \alpha _ { t + \tau - b - i }$ ; and forecast errors in forecasting $N _ { t }$ , $\textstyle \sum _ { i = 0 } ^ { \tau - 1 } { \psi _ { i } \varepsilon _ { t + \tau - i } }$ $x _ { t }$ , . The forecast variance is simply the sum of the two variances:

$$
\begin{array}{l} \sigma^ {2} (\tau) = \operatorname {V a r} \left[ e _ {t} (\tau) \right] \tag {6.46} \\ = \sigma_ {\alpha} ^ {2} \sum_ {i = 0} ^ {\tau - b - 1} \left(v _ {i} ^ {*}\right) ^ {2} + \sigma_ {\varepsilon} ^ {2} \sum_ {i = 0} ^ {\tau - 1} \psi_ {i} ^ {2}. \\ \end{array}
$$

To check the effect of adding $x _ { t }$ in the model when forecasting, it may be appealing to compare the forecast errors between the transfer function鈥?noise model and the univariate ARIMA model for $y _ { t }$ . Let the forecast error variances for the former and the latter be denoted by $\sigma _ { \mathrm { T F N } } ^ { 2 } \left( \tau \right)$ and $\sigma _ { \mathrm { U M } } ^ { 2 } \left( \tau \right)$ , respectively. We may then consider

$$
\begin{array}{l} R ^ {2} (\tau) = 1 - \frac {\sigma_ {\mathrm {T F N}} ^ {2} (\tau)}{\sigma_ {\mathrm {U M}} ^ {2} (\tau)} \\ = \frac {\sigma_ {\mathrm {U M}} ^ {2} (\tau) - \sigma_ {\mathrm {T F N}} ^ {2} (\tau)}{\sigma_ {\mathrm {U M}} ^ {2} (\tau)}. \tag {6.47} \\ \end{array}
$$

This quantity is expected to go down signicantly if the introduction of $x _ { t }$ were indeed appropriate.

Example 6.4 Suppose we need to make forecasts for the next minute (6 observations) for the viscosity data in Example 6.2. We rst consider the nal model suggested in Example 6.2

$$
y _ {t} = \frac {w _ {0}}{1 - \delta_ {1} B - \delta_ {2} B ^ {2}} x _ {t - 3} + \frac {1}{1 - \phi_ {1} B} \varepsilon_ {t}.
$$

After some rearrangement, we have

$$
\begin{array}{l} y _ {t} = \left(\delta_ {1} + \phi_ {1}\right) y _ {t - 1} + \left(\delta_ {2} - \delta_ {1} \phi_ {1}\right) y _ {t - 2} - \delta_ {2} \phi_ {1} y _ {t - 3} + w _ {0} x _ {t - 3} \\ - w _ {0} \phi_ {1} x _ {t - 4} + \varepsilon_ {t} - \delta_ {1} \varepsilon_ {t - 1} - \delta_ {2} \varepsilon_ {t - 2}. \\ \end{array}
$$

From Eq. (6.38), we have the $\tau$ -step ahead prediction as

$$
\begin{array}{l} \hat {y} _ {1 + \tau} (t) = (\hat {\delta} _ {1} + \hat {\phi} _ {1}) [ y _ {t + \tau - 1} ] + (\hat {\delta} _ {2} - \hat {\delta} _ {1} \hat {\phi} _ {1}) [ y _ {t + \tau - 2} ] - \hat {\delta} _ {2} \hat {\phi} _ {1} [ y _ {t + \tau - 3} ] \\ + \hat {w} _ {0} [ x _ {t + \tau - 3} ] - \hat {w} _ {0} \hat {\phi} _ {1} [ x _ {t + \tau - 4} ] + [ \varepsilon_ {t + \tau} ] - \hat {\delta} _ {1} [ \varepsilon_ {t + \tau - 1} ] - \hat {\delta} _ {2} [ \varepsilon_ {t + \tau - 2} ], \\ \end{array}
$$

where

$$
\begin{array}{l} [ y _ {t + j} ] = \left\{ \begin{array}{l l} y _ {t + j}, & j \leq 0 \\ \hat {y} _ {t + j} (t), & j > 0 \end{array} \right. \\ [ x _ {t + j} ] = \left\{ \begin{array}{l l} x _ {t + j}, & j \leq 0 \\ \hat {x} _ {t + j} (t), & j > 0 \end{array} \right. \\ \end{array}
$$

and

$$
[ \varepsilon_ {t + j} ] = \left\{ \begin{array}{c c} \hat {\varepsilon} _ {t + j}, & j \leq 0 \\ 0, & j > 0. \end{array} \right.
$$

Hence for the current and past response and input values, we can use the actual data. For the future response and input values we will instead use their respective forecasts. To forecast the input variable $x _ { t }$ , we will use the AR(1) model, $( 1 - 0 . 7 3 B ) x _ { t } = \alpha _ { t }$ from Example 6.2. As for the error estimates, we can use the residuals from the transfer function-noise model or for $b \geq 1$ , the one-step-ahead forecast errors for the current and past values of the errors, and set the error estimates equal to zero for future values.

We can obtain the variance of the prediction error from Eq. (6.45). The estimates of $\sigma _ { \alpha } ^ { 2 }$ and $\sigma _ { \varepsilon } ^ { 2 }$ in Eq. (6.45) can be obtained from the univariate AR(1) model for $x _ { t }$ , and the transfer function-noise model from Example 6.2, respectively. Hence for this example we have $\hat { \sigma } _ { \alpha } ^ { 2 } = 0 . 0 1 0 2$ and $\hat { \sigma } _ { \varepsilon } ^ { 2 } = 0 . 0 1 2 8$ . The coefcients in $\nu ^ { * } ( B )$ and $\psi ( B )$ can be calculated from

$$
v ^ {*} (B) = \sum_ {i = 0} ^ {\infty} v _ {i} ^ {*} B ^ {i} = v (B) \psi_ {x} (B)
$$

$$
(v _ {0} ^ {*} + v _ {1} ^ {*} B + v _ {2} ^ {*} B ^ {2} + \ldots) = \frac {w _ {0}}{(1 - \delta_ {1} B - \delta_ {2} B ^ {2})} (1 - \phi_ {x} B) ^ {- 1}
$$

or

$$
(\nu_ {0} ^ {*} + \nu_ {1} ^ {*} B + \nu_ {2} ^ {*} B ^ {2} + \ldots) (1 - \delta_ {1} B - \delta_ {2} B ^ {2}) (1 - \phi_ {x} B) = w _ {0}
$$

which means

$$
v _ {0} ^ {*} = w _ {0}
$$

$$
v _ {1} ^ {*} = (\delta_ {1} + \phi_ {x}) v _ {0} ^ {*} = (\delta_ {1} + \phi_ {x}) w _ {0}
$$

$$
v _ {2} ^ {*} = \left(\delta_ {1} + \phi_ {x}\right) v _ {1} ^ {*} = \left(\delta_ {2} - \delta_ {1} \phi_ {x}\right) v _ {0} ^ {*}
$$

$$
= [ (\delta_ {1} + \phi_ {x}) ^ {2} + (\delta_ {2} - \delta_ {1} \phi_ {x}) ] w _ {0}
$$

and

$$
\psi_ {i} = \phi_ {1} ^ {i} \qquad \mathrm {f o r} i = 0, 1, 2 \dots
$$

Hence the estimates of the coefcients in $\nu ^ { * } ( B )$ and $\psi ( B )$ can be obtained by using the estimates of the parameters given in Example 6.2. Note that

![](images/28b5787c336a95942571a52b9715c178df0a5e20c176ad1b109d600ded5e5862.jpg)  
FIGURE 6.15 The time series plots of the actual and 1- to 6-step ahead forecasts for the viscosity data.

for up to 6-step-ahead forecasts, from Eq. (6.45), we will only need to calculate $\nu _ { 0 } ^ { * } , \nu _ { 1 } ^ { * }$ and $\nu _ { 2 } ^ { * }$ .

The time series plot of the forecasts is given in Figure 6.15 together with the approximate $9 5 \%$ prediction limits calculate.by $\pm 2 \hat { \sigma } ( \tau )$ .

For comparison purposes we t a univariate ARIMA model for $y _ { t }$ . Following the model identication procedure given in Chapter 5, an AR(3) model is deemed a good t. The estimated standard deviations of the prediction error for the transfer function-noise model and the univariate model are given in Table 6.8. It can be seen that adding the exogenous variable $x _ { t }$ helps to reduce the prediction error standard deviation.

TABLE 6.8 Estimated standard deviations of the prediction error for the transfer function-noise model (TFM) and the univariate model (UM)   

<table><tr><td rowspan="2">Observation</td><td colspan="2">Estimated Standard Deviation of the Prediction Error</td></tr><tr><td>TFM</td><td>UM</td></tr><tr><td>101</td><td>0.111</td><td>0.167</td></tr><tr><td>102</td><td>0.137</td><td>0.234</td></tr><tr><td>103</td><td>0.149</td><td>0.297</td></tr><tr><td>104</td><td>0.205</td><td>0.336</td></tr><tr><td>105</td><td>0.251</td><td>0.362</td></tr><tr><td>106</td><td>0.296</td><td>0.376</td></tr></table>

# 6.7 INTERVENTION ANALYSIS

In some cases, the response $y _ { t }$ can be affected by a known event that happens at a specic time such as scal policy changes, introduction of new regulatory laws, or switching suppliers. Since these interventions do not have to be quantitative variables, we can represent them with indicator variables. Consider, for example, the transfer function鈥搉oise model as the following:

$$
\begin{array}{l} y _ {t} = \frac {w (B)}{\delta (B)} \xi_ {t} ^ {(T)} + \frac {\theta (B)}{\varphi (B)} \varepsilon_ {t} \tag {6.48} \\ = \nu (B) \xi_ {t} ^ {(T)} + N _ {t}, \\ \end{array}
$$

where $\boldsymbol { \xi } _ { t } ^ { ( T ) }$ 饾湁t is a deterministic indicator variable, taking only the values 0 and 1 to indicate nonoccurrence and occurrence of some event. The model in Eq. (6.48) is called the intervention model. Note that this model has only one intervention event. Generalization of this model with several intervention events is also possible.

The most common indicator variables are the pulse and step variables,

$$
P _ {t} ^ {(T)} = \left\{ \begin{array}{l l} 0 & \text {i f} t \neq T \\ 1 & \text {i f} t = T \end{array} \right. \tag {6.49}
$$

and

$$
S _ {t} ^ {(T)} = \left\{ \begin{array}{l l} 0 & \text {i f} t <   T \\ 1 & \text {i f} t \geq T \end{array} , \right. \tag {6.50}
$$

where $T$ is a specied occurrence time of the intervention event. The transfer function operator $\nu ( B ) = w ( B ) / \delta ( B )$ in Eq. (6.48) usually has a fairly simple and intuitive form.

# Examples of Responses to Pulse and Step Inputs

1. We will rst consider the pulse indicator variable. We will further assume a simple transfer function鈥搉oise model as

$$
y _ {t} = \frac {w _ {0}}{1 - \delta B} P _ {t} ^ {(T)}. \tag {6.51}
$$

After rearranging Eq. (6.51), we have

$$
(1 - \delta B) y _ {t} = w _ {0} P _ {t} ^ {(T)} = \left\{ \begin{array}{l l} 0 & \mathrm {i f} t \neq T \\ w _ {0} & \mathrm {i f} t = T \end{array} \right.
$$

or

$$
y _ {t} = \delta y _ {t - 1} + w _ {0} P _ {t} ^ {(T)}
$$

So we have

$$
{y _ {T}} {= w _ {0}}
$$

$$
y _ {T + 1} = \delta y _ {T} = \delta w _ {0}
$$

$$
y _ {T + 2} = \delta y _ {T + 1} = \delta^ {2} y _ {T} = \delta^ {2} w _ {0}
$$

鈰?
$$
y _ {T + k} = \dots = \delta^ {k} y _ {T} = \delta^ {k} w _ {0},
$$

which means

$$
y _ {t} = \left\{ \begin{array}{l l} 0 & \mathrm {i f} t <   T \\ w _ {0} s ^ {t - T} & \mathrm {i f} t \geq T. \end{array} \right.
$$

2. For the step indicator variable with the same transfer function鈥搉oise model as in the previous case, we have

$$
y _ {t} = \frac {w _ {0}}{1 - \delta B} S _ {t} ^ {(T)}.
$$

Solving the difference equation

$$
(1 - \delta B) y _ {t} = w _ {0} S _ {t} ^ {(T)} = \left\{ \begin{array}{l l} 0 & \mathrm {i f} t <   T \\ w _ {0} & \mathrm {i f} t \geq T \end{array} \right.
$$

we have

$$
y _ {T} = w _ {0}
$$

$$
y _ {T + 1} = \delta y _ {T} + w _ {0} = w _ {0} (1 + \delta)
$$

$$
y _ {T + 2} = \delta y _ {T + 1} + w _ {0} = w _ {0} (1 + \delta + \delta^ {2})
$$

$$
y _ {T + k} = \delta y _ {T + k - 1} + w _ {0} = w _ {0} (1 + \delta + \dots + \delta^ {k})
$$

or

$$
y _ {t} = w _ {0} (1 + \delta + \dots + \delta^ {t - T}) \mathrm {f o r} t \geq T
$$

In intervention analysis, one of the things we could be interested in may be how permanent the effect of the event will be. Generally, for yt = (w(B)鈭曫潧?B))饾湁(T)t $y _ { t } \dot { = } ( w ( B ) / \delta ( \mathbf { \bar { \boldsymbol { B } } } ) ) \xi _ { t } ^ { ( T ) }$ with stable $\delta ( B )$ , if the intervention event is a pulse, we will then have a transient (short-lived) effect. On the other hand, if the intervention event is a step, we will have a permanent effect. In general, depending on the form of the transfer function, there are many possible responses to the step and pulse inputs. Table 6.9 displays the output

TABLE 6.9 Output responses to step and pulse inputs.   

<table><tr><td></td><td>Step input</td><td>Pulse input</td></tr><tr><td rowspan="2">蠅(B)/未(B)</td><td>STtT</td><td>PTt</td></tr><tr><td>Output response</td><td>Output response</td></tr><tr><td>蠅0</td><td>蠅0T</td><td>蠅0T</td></tr><tr><td>蠅0/1-未1B</td><td>蠅01-未1T蠅0T</td><td>蠅0T</td></tr><tr><td>蠅0/1-B</td><td>蠅0T蠅0</td><td>蠅0T</td></tr><tr><td>蠅0/1-未1B+蠅11-B</td><td>1</td><td>蠅0T蠅1</td></tr></table>

TABLE 6.10 Weekly Cereal Sales Data   

<table><tr><td>Week</td><td>Sales</td><td>Week</td><td>Sales</td><td>Week</td><td>Sales</td><td>Week</td><td>Sales</td></tr><tr><td>1</td><td>102,450</td><td>27</td><td>114,980</td><td>53</td><td>167,170</td><td>79</td><td>181,560</td></tr><tr><td>2</td><td>98,930</td><td>28</td><td>130,250</td><td>54</td><td>161,200</td><td>80</td><td>202,130</td></tr><tr><td>3</td><td>91,550</td><td>29</td><td>128,070</td><td>55</td><td>166,710</td><td>81</td><td>183,740</td></tr><tr><td>4</td><td>111,940</td><td>30</td><td>135,970</td><td>56</td><td>156,430</td><td>82</td><td>191,880</td></tr><tr><td>5</td><td>103,380</td><td>31</td><td>142,370</td><td>57</td><td>162,440</td><td>83</td><td>197,950</td></tr><tr><td>6</td><td>112,120</td><td>32</td><td>121,300</td><td>58</td><td>177,260</td><td>84</td><td>209,040</td></tr><tr><td>7</td><td>105,780</td><td>33</td><td>121,380</td><td>59</td><td>163,920</td><td>85</td><td>203,990</td></tr><tr><td>8</td><td>103,000</td><td>34</td><td>128,790</td><td>60</td><td>166,040</td><td>86</td><td>201,220</td></tr><tr><td>9</td><td>111,920</td><td>35</td><td>139,290</td><td>61</td><td>182,790</td><td>87</td><td>202,370</td></tr><tr><td>10</td><td>106,170</td><td>36</td><td>128,530</td><td>62</td><td>169,510</td><td>88</td><td>201,100</td></tr><tr><td>11</td><td>106,350</td><td>37</td><td>139,260</td><td>63</td><td>173,940</td><td>89</td><td>203,210</td></tr><tr><td>12</td><td>113,920</td><td>38</td><td>157,960</td><td>64</td><td>179,350</td><td>90</td><td>198,770</td></tr><tr><td>13</td><td>126,860</td><td>39</td><td>145,310</td><td>65</td><td>177,980</td><td>91</td><td>171,570</td></tr><tr><td>14</td><td>115,680</td><td>40</td><td>150,340</td><td>66</td><td>180,180</td><td>92</td><td>184,320</td></tr><tr><td>15</td><td>122,040</td><td>41</td><td>158,980</td><td>67</td><td>188,070</td><td>93</td><td>182,460</td></tr><tr><td>16</td><td>134,350</td><td>42</td><td>152,690</td><td>68</td><td>191,930</td><td>94</td><td>173,430</td></tr><tr><td>17</td><td>131,200</td><td>43</td><td>157,440</td><td>69</td><td>186,070</td><td>95</td><td>177,680</td></tr><tr><td>18</td><td>132,990</td><td>44</td><td>144,500</td><td>70</td><td>171,860</td><td>96</td><td>186,460</td></tr><tr><td>19</td><td>126,020</td><td>45</td><td>156,340</td><td>71</td><td>180,240</td><td>97</td><td>185,140</td></tr><tr><td>20</td><td>152,220</td><td>46</td><td>137,440</td><td>72</td><td>180,910</td><td>98</td><td>183,970</td></tr><tr><td>21</td><td>137,350</td><td>47</td><td>166,750</td><td>73</td><td>185,420</td><td>99</td><td>154,630</td></tr><tr><td>22</td><td>132,240</td><td>48</td><td>171,640</td><td>74</td><td>195,470</td><td>100</td><td>174,720</td></tr><tr><td>23</td><td>144,550</td><td>49</td><td>170,830</td><td>75</td><td>183,680</td><td>101</td><td>169,580</td></tr><tr><td>24</td><td>128,730</td><td>50</td><td>174,250</td><td>76</td><td>190,200</td><td>102</td><td>180,310</td></tr><tr><td>25</td><td>137,040</td><td>51</td><td>178,480</td><td>77</td><td>186,970</td><td>103</td><td>154,080</td></tr><tr><td>26</td><td>136,830</td><td>52</td><td>178,560</td><td>78</td><td>182,330</td><td>104</td><td>163,560</td></tr></table>

responses to the unit step and pulse inputs for several transfer function model structures. This table is helpful in model formulation.

Example 6.5 The weekly sales data of a cereal brand for the last two years are given in Table 6.10. As can be seen from Figure 6.16, the sales were showing a steady increase during most of the two-year period. At the end of the summer of the second year (Week 88), the rival company introduced a similar product into the market. Using intervention analysis, we want to study whether that had an effect on the sales. For that, we will rst t an ARIMA model to the preintervention data from Week 1 to Week 87. The sample ACF and PACF of the data for that time period in Figure 6.17

![](images/d33ccf6cd18dc86fc8748009d059e98be9e61ba310dfe44acac17d856e15b915.jpg)  
FIGURE 6.16 Time series plot of the weekly sales data.

show that the process is nonstationary. The sample ACF and PACF of the rst difference given in Figure 6.18 suggest that an ARIMA(0,1,1) model is appropriate. Then the intervention model has the following form:

$$
y _ {t} = w _ {0} S _ {t} ^ {(8 8)} + \frac {1 - \theta B}{1 - B} \varepsilon_ {t},
$$

where

$$
S _ {t} ^ {(8 8)} = \left\{ \begin{array}{l l} 0 & \mathrm {i f} t <   8 8 \\ 1 & \mathrm {i f} t \geq 8 8. \end{array} \right.
$$

![](images/0a35f9aaecce37989bc00b52340ca12d66c10ccb276402a9f75575ffd95c3897.jpg)

![](images/9f65410ab25fb531c552d6032d116af9810268294710b38bbf9fa4d199058a4d.jpg)  
FIGURE 6.17 Sample ACF and PACF pulse of the sales data for weeks 1鈥?7.

![](images/90f9a084682c5fb9b5c2de9500867bc12fabac36667e441b617f5675b2dd10cf.jpg)

![](images/fbf098935194b369aec94d0b81de16fead280c19d0ec6742ecafb09c187d82b5.jpg)  
FIGURE 6.18 Sample ACF and PACF plots of the rst difference of the sales data for weeks 1鈥?7.

This means that for the intervention analysis we assume that the competition simply slows down (or reverses) the rate of increase in the sales. To t the model we use the transfer function model option in JMP with S(88) a $S _ { t } ^ { ( 8 8 ) }$ s the input. The output in Table 6.11 shows that there was indeed a signicant effect on sales due to the introduction of a similar product in the market. The coefcient estimate $\hat { w } _ { 0 } = - 2 3 6 9 . 9$ further suggests that if no appropriate action is taken, the sales will most likely continue to go down.

# Example 6.6 Electricity Consumption and the 1973 Arab Oil Embargo

The natural logarithm of monthly electric energy consumption in megawatt hours (MWh) for a regional utility from January 1951 to April 1977 is shown in Figure 6.19. The original data exhibited considerable inequality

![](images/34e022416723479209c075540b6d4a5eea8a9e03944d91dc0d6f02da7efacf22.jpg)  
FIGURE 6.19 Natural logarithm of monthly electric energy consumption in megawatt hours (MWh) from January 1951 to April 1977.

![](images/54a53bb12d7b76bcddd97e3b377de4317ca52a9457f2416a7c9a633479e17965.jpg)  
Transfer Function Analysis Time Series Sales

TABLE 6.11 JMP Output for the Intervention Analysis in Example 6.5   

<table><tr><td>Mean</td><td>157540.87</td></tr><tr><td>Std</td><td>29870.833</td></tr><tr><td>N</td><td>104</td></tr><tr><td>Zero Mean ADF</td><td>0.0886817</td></tr><tr><td>Single Mean ADF</td><td>-2.381355</td></tr><tr><td>Trend ADF</td><td>-3.770646</td></tr></table>

Transfer Function Model (1) Model Summary   

<table><tr><td>DF</td><td>101</td></tr><tr><td>Sum of Squared Errors</td><td>9554575810</td></tr><tr><td>Variance Estimate</td><td>94599755.5</td></tr><tr><td>Standard Deviation</td><td>9726.24056</td></tr><tr><td>Akaike&#x27;s &#x27;A&#x27; Information Criterion</td><td>2186.26524</td></tr><tr><td>Schwarz&#x27;s Bayesian Criterion</td><td>2191.5347</td></tr><tr><td>RSquare</td><td>0.26288323</td></tr><tr><td>RSquare Adj</td><td>0.25558504</td></tr><tr><td>MAPE</td><td>4.84506071</td></tr><tr><td>MAE</td><td>7399.35196</td></tr><tr><td>-2LogLikelihood</td><td>2182.26524</td></tr></table>

Parameter Estimates   

<table><tr><td>Variable</td><td>Term</td><td>Factor</td><td>Lag</td><td>Estimate</td><td>Std Error</td><td>t Ratio</td><td>Prob&gt;|t|</td></tr><tr><td>Step</td><td>Num0.0</td><td>0</td><td>0</td><td>-2359.900</td><td>1104.753</td><td>-2.15</td><td></td></tr><tr><td>Sales</td><td>MA1,1</td><td>1</td><td>1</td><td>0.557</td><td>0.076</td><td>7.36</td><td></td></tr></table>

$$
(1 - B) * S a l e s _ {t} = - (2 3 6 9. 8 9 9 5 * \text {S t e p} _ {t}) + (1 - 0. 5 5 7 1 * B) * e _ {t}
$$

![](images/2c2d6e8a25c0825957b3bb2c08b5f7c674e377e7921c33dd30d3b2498dc62e68.jpg)  
Interactive Forecasting

of variance over this time period and the natural logarithm stabilizes the variance. In November of 1973 the Arab oil embargo affected the supply of petroleum products to the United States. Following this event, it is hypothesized that the rate of growth of electricity consumption is smaller than in the pre-embargo period. Montgomery and Weatherby (1980) tested this hypothesis using an intervention model.

Although the embargo took effect in November of 1973, we will assume that rst impact of this was not felt until the following month, December, 1973. Therefore, the period from January of 1951 until November of 1973 is assumed to have no intervention effect. These 275 months are analyzed to produce a univariate ARIMA model for the noise model. Both regular and seasonal differencing are required to achieve stationary and a multiplicative $( 0 , 1 , 2 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ was t to the pre-intervention data. The model is

$$
(1 - B) (1 - B ^ {1 2}) \ln N _ {t} = (1 - 0. 4 0 B - 0. 2 8 B ^ {2}) (1 - 0. 6 4 B ^ {1 2}) \varepsilon_ {t}.
$$

To develop the intervention, it is necessary to hypothesize the effect that the oil embargo may have had on electricity consumption. Perhaps the simplest scenario in which the level of the consumption time series is hypothesized to be permanently changed by a constant amount. Logarithms can be thought of as percent change, so this is equivalent to hypothesizing that the intervention effect is a change in the percent growth of electricity consumption. Thus the input series would be a step function

$$
S _ {t} = \left\{ \begin{array}{l} 0, t = 1, 2, \ldots , 2 7 5 \\ 1, t = 2 7 5, 2 7 6, 3 2 6. \end{array} \right.
$$

The intervention model is then

$$
(1 - B) (1 - B ^ {1 2}) \ln y _ {t} = \omega_ {0} S _ {t} + (1 - \theta_ {1} B - \theta_ {2} B ^ {2}) (1 - \Theta_ {1 2} B ^ {1 2}) \varepsilon_ {t},
$$

where $\Theta _ { 1 2 }$ is the seasonal MA parameter at lag 12. Table 6.12 gives the model parameter estimates and the corresponding $9 5 \%$ condence intervals. Since the $9 5 \%$ condence intervals do not contain zero, we conclude that all model parameters are statistically signicant. The residual standard error for this model is 0.029195. Since the model was built to the natural logarithm of electricity consumption, the standard error has a simple, direct interpretation: namely, the standard deviation of one-step-ahead forecast errors is 2.9195 percent of the level of the time series.

TABLE 6.12 Model Summary Statistics for Example 6.6   

<table><tr><td rowspan="2">Parameter</td><td rowspan="2">Point Estimate</td><td colspan="2">95% Confidence Limits</td></tr><tr><td>Lower</td><td>Upper</td></tr><tr><td>蠅0</td><td>-0.07303</td><td>-0.11605</td><td>-0.03000</td></tr><tr><td>胃1</td><td>0.40170</td><td>0.28964</td><td>0.51395</td></tr><tr><td>胃2</td><td>0.27739</td><td>0.16448</td><td>0.39030</td></tr><tr><td>螛12</td><td>0.64225</td><td>0.54836</td><td>0.73604</td></tr></table>

It is also possible to draw conclusions about the intervention effect. This effect is a level change of magnitude $\hat { \omega } _ { 0 } = - 0 . 0 7 3 0 3$ , expressed in the natural logarithm metric. The estimate of the intervention effect in the original MWh metric is $e ^ { \hat { \omega } _ { 0 } } = e ^ { - 0 . 0 7 3 0 3 } = 0 . 9 2 9 6$ . That is, the postintervention level of electricity consumption is 92.96 percent of the preintervention level. The effect of the Arab oil embargo has been to reduce the increase in electricity consumption by 7.04 percent. This is a statistically signicant effect.

In this example, there are 275 pre-intervention observations and 41 post-intervention observations. Generally, we would like to have as many observations as possible in the post-intervention period to ensure that the power of the test for the intervention effect is high. However, an extremely long post-intervention period may allow other unidentied factors to affect the output, leading to potential confounding of effects. The ability of the procedure to detect an intervention effect is a function of the number of preand post-intervention observations, the size of the intervention effect, the form of the noise model, and the parameter values of the process. In many cases, however, an intervention effect can be identied with relatively short record of post-intervention observations.

It is interesting to consider alternative hypotheses regarding the impact of the Arab oil embargo. For example, it may be more reasonable to suspect that the effect of the oil embargo is not to cause an immediate level change in electricity consumption, but a gradual one. This would suggest a model

$$
(1 - B) (1 - B ^ {1 2}) \ln y _ {t} = \frac {\omega_ {0}}{1 - \delta_ {1} B} S _ {t} + (1 - \theta_ {1} B - \theta_ {2} B ^ {2}) (1 - \Theta_ {1 2} B ^ {1 2}) \varepsilon_ {t}.
$$

The results of tting this model to the data are shown in Table 6.13. Note that the $9 5 \%$ condence interval for $\delta _ { 1 }$ includes zero, implying that we can

TABLE 6.13 Model Summary Statistics for the Alternate Intervention Model for Example 6.6   

<table><tr><td rowspan="2">Parameter</td><td rowspan="2">Point Estimate</td><td colspan="2">95% Confidence Limits</td></tr><tr><td>Lower</td><td>Upper</td></tr><tr><td>蠅0</td><td>-0.06553</td><td>-0.11918</td><td>-0.01187</td></tr><tr><td>未1</td><td>0.18459</td><td>-0.51429</td><td>0.88347</td></tr><tr><td>胃1</td><td>0.40351</td><td>0.29064</td><td>0.51637</td></tr><tr><td>胃2</td><td>0.27634</td><td>0.16002</td><td>0.38726</td></tr><tr><td>螛12</td><td>0.63659</td><td>0.54201</td><td>0.73117</td></tr></table>

drop this parameter from the model. This would leave us with the original intervention model that was t in Example 6.6. Consequently, we conclude that the Arab oil embargo induced an immediate permanent change in the level of electricity consumption.

In some problems there may be multiple intervention effects. Generally, one indicator variable must be used for each intervention effect. For example, suppose that in the electricity consumption example, we think that the oil embargo had two separate effects: the initial impact beginning in month 276, and a second impact beginning three months later. The intervention model to incorporate these effects is

$$
\begin{array}{l} (1 - B) (1 - B ^ {1 2}) \ln y _ {t} = \omega_ {1 0} S _ {1, t} + \omega_ {2 0} S _ {2, t} + (1 - \theta_ {1} B - \theta_ {2} B ^ {2}) \\ (1 - \Theta_ {1 2} B ^ {1 2}) \varepsilon_ {t}, \\ \end{array}
$$

where

$$
S _ {1, t} = \left\{ \begin{array}{l l} 0, & t = 1, 2, 3, \ldots , 2 7 5 \\ 1, & t = 2 7 6, 2 7 7, \ldots , 3 1 6 \end{array} \right.
$$

and

$$
S _ {2, t} = \left\{ \begin{array}{l l} 0, & t = 1, 2, 3, \ldots , 2 7 8 \\ 1, & t = 2 7 9, 2 7 7, \ldots , 3 1 6 \end{array} \right..
$$

In this model the parameters $\omega _ { 1 0 }$ and $\omega _ { 2 0 }$ represent the initial and secondary effects of the oil embargo and $\omega _ { 1 0 } + \omega _ { 2 0 }$ represents the long-term total impact.

There have been many other interesting applications of intervention analysis. For some very good examples, see the following references:

- Box and Tiao (1975) investigate the effects on ozone $( \mathbf { O } _ { 3 } )$ ) concentration in downtown Los Angeles of a new law that restricted the amount of reactive hydrocarbons in locally sold gasoline, regulations that mandated automobile engine design changes, and the diversion of trafc by opening of the Golden State Freeway. They showed that these interventions did indeed lead to reductions in ozone levels.   
- Wichern and Jones (1977) analyzed the impact of the endorsement by the American Dental Association of Crest toothpaste as an effective aid in reducing cavities on the market shares of Crest and Colgate toothpaste. The endorsement led to a signicant increase in market share for Crest. See Bisgaard and Kulahci (2011) for a detailed analysis of that example.   
- Atkins (1979) used intervention analysis to investigate the effect of compulsory automobile insurance, a company strike, and a change in insurance companies鈥?policies on the number of highway accidents on freeways in British Columbia.   
- Izenman and Zabell (1981) study the effect of the 9 November, 1965, blackout in New York City that resulted from a widespread power failure, on the birth rate nine months later. An article in The New York Times in August 1966 noted that births were up, but subsequent medical and demographic articles appeared with con-icting statements. Using the weekly birth rate from 1961 to 1966, the authors show that there is no statistically signicant increase in the birth rate.   
- Ledolter and Chan (1996) used intervention analysis to study the effect of a speed change on rural interstate highways in Iowa on the occurrence of trafc accidents.

Another important application of intervention analysis is in the detection of time series outliers. Time series observations are often in-uenced by external disruptive events, such as strikes, social/political events, economic crises, or wars and civil disturbances. The consequences of these events are observations that are not consistent with the other observations in the time series. These inconsistent observations are called outliers. In addition to the external events identied above, outliers can also be caused by more mundane forces, such as data recording or transmission errors. Outliers can have a very disruptive effect on model identication, parameter estimation, and forecasting, so it is important to be able to detect their presence so that they can be removed. Intervention analysis can be useful for this.

There are two kinds of time series outliers: additive outliers and innovation outliers. An additive outlier affects only the level of the $t ^ { * }$ observation, while an innovation outlier affects all observations y 鈭? y 鈭?, y 鈭?,鈥?$y _ { t ^ { * } }$ $y _ { t ^ { * } + 1 }$ $y _ { t ^ { * } + 2 } , \ldots$ beyond time $t ^ { * }$ where the original outlier effect occurred. An additive outlier can be modeled as

$$
z _ {t} = \frac {\theta (B)}{\phi (B)} \varepsilon_ {t} + \omega I _ {t} ^ {(t ^ {*})},
$$

where $I _ { t } ^ { ( t ^ { * } ) }$ is an indicator time series dened as

$$
I _ {t} ^ {(t ^ {*})} = \left\{ \begin{array}{l l} 1 & \text {i f} t = t ^ {*} \\ 0 & \text {i f} t \neq t ^ {*} \end{array} \right..
$$

An innovation outlier is modeled as

$$
z _ {t} = \frac {\theta (B)}{\phi (B)} (\varepsilon_ {t} + \omega I _ {t} ^ {(t ^ {*})}).
$$

When the timing of the outlier is known, it is relatively straightforward to t the intervention model. Then the presence of the outlier can be tested by comparing the estimate of the parameter $\omega$ , say, $\hat { \omega }$ , to its standard error. When the timing of the outlier is not known, an iterative procedure is required. This procedure is described in Box, Jenkins, and Reinsel (1994) and in Wei (2006). The iterative procedure is capable of identifying multiple outliers in the time series.

# 6.8 R COMMANDS FOR CHAPTER 6

Example 6.2 The data for this example are in the array called vistemp.data of which the two columns represent the viscosity and the temperature respectively.

Below we rst start with the prewhitening step.

xt<-vistemp.data[,1]   
yt<-vistemp.data[,2]   
par(mfrow=c(2,1),oma=c(0,0,0,0))   
plot(xt,type $=$ "o",pch $= 16$ ,cex $= .5$ ,xlab $\equiv$ 'Time',ylab $\equiv$ expression (italic(x[italic(t)])))   
plot(yt,type $=$ "o",pch $= 16$ ,cex $= .5$ ,xlab $\equiv$ 'Time',ylab $\equiv$ expression (italic(y[italic(t)])))

![](images/776e967ea72ed5a467263af17313a72c7bd6d1fb8f745a889b56865d8db6228c.jpg)

![](images/fd3471a0016de0e9ee02b34df1860d72497d8db2889e4f60bad82626423524e5.jpg)

```txt
#Prewhtening
#Model identification for xt
par(mfrow=c(1,2), om = c(0,0,0,0))
acf(xt, lag.max=25, type="correlation", main="ACF for Temperature")
acf(xt, lag.max=25, type="partial", main="PACF for Temperature", ylab="PACF") 
```

![](images/8f6e5f3bc8f5d563c0bd9b1fd37e3d53a3719251f59d7261c999f93cc052a044.jpg)

![](images/59721bd3ce23f7c88179412b6c3e6a18f480f1e7b5e52ca57f222953b20cb9e5.jpg)

# Fit an AR(1) model to xt.

xt.ar1<-arima(xt,order=c(1,0,0)锛宨nclude.mean $\equiv$ FALSE)   
xt.ar1 Call: arima $(x = x t$ 锛宱rder $= c(1,0,0)$ 锛宨nclude.mean $=$ FALSE) Coefficients: ar1 0.7292 s.e. 0.0686 sigma^2 estimated as 0.01009: log likelihood $= 87.54$ aic $= -171.08$

# We perform the residual analysis

```txt
res.xt.ar1(-as.vector(residuals(xt.ar1)) #to obtain the fitted values we use the function fitted() from the forecast package 
```

library(forecast)

fit.xt.ar1<-as.vector(fitted(xt.ar1))

# ACF and PACF of the Residuals

par(mfrow ${ \bf \Pi } = { \bf C }$ (1,2),oma $\boldsymbol { \mathbf { \mathit { \Pi } } } = \boldsymbol { \mathbf { \check { \mathbf { C } } } }$ (0,0,0,0))

acf(res.xt.ar1,lag.max $^ { = 2 5 }$ ,type $=$ "correlation",main $. =$ "ACF of the Residuals for x(t)")

acf(res.xt.ar1, lag.max $^ { = 2 5 }$ ,type $: =$ "partial",mai $\cdot ^ { = }$ "PACF of the Residuals for x(t)")

![](images/ff1dfc747dc0af14ed6bdd76b776aa627476b930494ec3231a6810c5d0bfd291.jpg)

![](images/e28e4fc5becf8d436fd831a7b3b40d5ba54af0312144ce3575d37f4a70ecf9eb.jpg)

# 4-in-1 plot of the residuals

par(mfrow ${ \bf \Pi } = { \bf C }$ (2,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

qqnorm(res.xt.ar1,datax $: =$ TRUE,pch=16,xlab $=$ 'Residual',mai $. = "$ )

qqline(res.xt.ar1,datax $: =$ TRUE)

plot(fit.xt.ar1,res.xt.ar1,pch $_ { . = 1 6 }$ , xlab $^ { \prime = }$ 'Fitted Value', ylab $=$ 'Residual')

abline $\scriptstyle \mathrm { \mathrm { h } = 0 }$

hist(res.xt.ar1,col $=$ "gray",xlab $i = 1$ 'Residual',main $= "$

plot(res.xt.ar1,type $=$ "l",xlab='Observation Order',ylab $=$ 'Residual')

points(res.xt.ar1,pch $_ { . = 1 6 }$ ,cex $=$ .5)

abline $\scriptstyle \mathrm { \mathrm { h } = 0 }$

![](images/ab57eec68194b1df99a42fc7db56f0fcdb367ad8f66c046b9128334ea33c35db.jpg)

![](images/36589dac9d1fd5f5022c4fe9d0d55f47a739b3eeda4e0efcf44d71d657f4ff3e.jpg)

![](images/832908cf5354185b4d53b2a4140a8424f4d658ff64f215935e7163d9933c083e.jpg)

![](images/31261971418b460133d9f7cde24c5603369b4496c703cd7fa4194b948d5b9ae3.jpg)

Prewhitening both series using AR(1) coefcients of 0.73.

```r
T<-length(xt)  
alphat<-xt[2:T]-0.73*xt[1:(T-1)]  
betat<-yt[2:T]-0.73*yt[1:(T-1)]  
ralbe<-ccf(betat,alphat,main='CCF of alpha(t) and beta(t)', ylab='CCF')  
abline(v=0,col='blue') 
```

![](images/78451cf60f02a2360bbe2764d23e3d35037904fb1422fce7089794d634f65816.jpg)

Obtain the estimates of $v_t$ vhat<-sqrt(var(betat)/var(alphat)) * ralbe$acf  
nl<-length(vhat)  
plot(seq(- (nl-1)/2, (nl-1)/2, 1), vhat, type='h', xlab='Lag', ylab=expression(italic(hat(v) [italic(j)])))  
abline(v=0, col='blue')  
abline(h=0)

![](images/183c45f114bcb5044070caca846e428c9f100b7269643f535958fbbc71e7bfcb.jpg)

#Model the noise using the estimates given in the example

```txt
Nhat<-array(0, dim=c(1,T))  
for (i in 4:T)  
Nhat[i] <- yt[i] + 0.31*(Nhat[i-1] - yt[i-1]) + 0.48*(Nhat[i-2] - yt[i-2]) + 1.21*xt[i-3]  
}  
Nhat<-Nhat[4:T]  
plot(Nhat,type="o",pch=16, cex=.5, xlab='Time', ylab='expression' (italic(hat(N) [italic(t)]))) 
```

![](images/2d14dc2a9a9772a56730e75ae91438307d313faae2914db5f32c7ece8238256e.jpg)

par(mfrow ${ \bf \Pi } = { \bf C }$ (1,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

acf(Nhat,lag.max $^ { = 2 5 }$ ,type $: =$ "correlation",main $\cdot =$ "ACF of Nhat")

acf(Nhat, lag.max=25,type $=$ "partial",main $\displaystyle =$ "PACF of Nhat")

![](images/a89f0c58322de6cd1a3b70ccf61fc5d14a61856e66a31622752f68e2af58b726.jpg)

![](images/3930229cd91ae7a1b3b6d2c3a7caee5d38c14017a4768db2edcef444492085c1.jpg)

#Fit AR(1) and AR(3) models for Nhat

Nhat.ar1<-arima(Nhat,order ${ \bf \bar { \Psi } } = { \bf C }$ (1, 0, 0),include.mea $\cdot =$ FALSE)

Nhat.ar3<-arima(Nhat,order ${ \bf \bar { \Psi } } = { \bf C }$ (3, 0, 0),include.mea $\cdot =$ FALSE)

res.Nhat.ar1<-as.vector(residuals(Nhat.ar1))

library( forecast)   
fit.Nhat.ar1<-as.vector(fitted(Nhat.ar1))   
# ACF and PACF of the Residuals   
par(mfrow=c(1,2),oma=c(0,0,0,0))   
acf(res.Nhat.ar1,lag.max=25,type $=$ "correlation",main $=$ "ACF of the Residuals for Nhat")   
acf(res.Nhat.ar1,lag.max=25,type $=$ "partial",main $=$ "PACF of the Residuals for Nhat")

![](images/de1b39924a3fa76f7b122aa3149c1eb1351edbd910a862ea30f67bd3dddf375e.jpg)

![](images/770d26bdcd2653ce803ce6637842aaee58790a907c16ef7d9c6f4457f7f557ca.jpg)

4-in-1 plot of the residuals   
par(mfrow=c(2,2),oma=c(0,0,0,0))   
qqnorm(res.Nhat.ar1,datrix $\equiv$ TRUE,pch $= 16$ ,xlab $\equiv$ 'Residual',main $=$ "   
qqline(res.Nhat.ar1,datrix $\equiv$ TRUE)   
plot(fit.xt.ar1,res.xt.ar1,pch $= 16$ ,xlab $\equiv$ 'Fitted Value' ylab $\equiv$ 'Residual')   
abline(h=0)   
hist(res.Nhat.ar1,col $\equiv$ "gray",xlab $\equiv$ 'Residual',main $=$ "   
plot(res.Nhat.ar1,type $\equiv$ "l",xlab $\equiv$ 'Observation Order',ylab $\equiv$ 'Residual')   
points(res.Nhat.ar1,pch $= 16$ ,cex=.5)   
abline(h=0)

![](images/8ce80317073f244e4a73055be842458061e6993d8928829c6b5f237616e1d12a.jpg)

![](images/af6ebce3f2f07b7d22604a3e8d515df366cd8111cc40ac087a2163bcd08d9364.jpg)

![](images/37111b88d6e77a5254992a8d4ebad8119feef1e9f9691d5981711b7429a506d7.jpg)

![](images/96091b5138753feff40e2b65ba92e69af33023f44dd906326269432ed4ab1135.jpg)

We now t the following transfer function鈥搉oise model

$$
y _ {t} = \frac {w _ {0}}{1 - \delta_ {1} B - \delta_ {2} B ^ {2}} x _ {t - 3} + \frac {1}{1 - \phi_ {1} B} \varepsilon_ {t}.
$$

For that we will use the 鈥渁rimax鈥?function in TSA package.

library(TSA)  
ts.xt<-ts(xt)  
lag3.x<-lag(ts.xt,-3)  
ts.yt<-ts(yt)  
dat3<-cbind(ts.xt,lag3.x,ts.yt)  
dimnames(mat3)[[2]]<-c("xt","lag3x","yt")  
data2<-na.omit(as.data.frame(mat3))  
#Input arguments  
#order: determines the model for the error component, i.e. the  
#order of the ARIMA model for y(t)  
#if there were no x(t)  
#xtransf: x(t)  
#transfer: the orders (r and s) of the transfer function  
visc.tf<-arimax(data2\ $yt, order=c(1,0,0), xtransf=data.frame(data2\$ lag3x), transfer=list(c(2,0)), include.mean = FALSE)  
visc.tf

```txt
Call:  
arimax(x = data2$yt, order = c(1, 0, 0), include.mean = FALSE, xtransf = data.frame(data2$lag3x), transfer = list(c(2, 0))) 
```

```txt
Coefficients:  
ar1 data2.lag3x-AR1 data2.lag3x-AR2 data2.lag3x-MA0  
0.8295 0.3414 0.2667 1.3276  
s.e. 0.0642 0.0979 0.0934 0.1104 
```

```txt
sigma^2 estimated as 0.0123: log likelihood = 75.09, aic = -142.18 
```

```r
res.visc=tf<-as.vector(residuals(visc=tf))  
library(forecast)  
fit.visc=tf<-as.vector(fitted(visc=tf))  
# ACF and PACF of the Residuals  
par(mfrow=c(1,2), oma=c(0,0,0,0))  
acf(res.visc=tf, lag.max=25, type="correlation", main="ACF of the Residuals\nfor TF-N Model")  
acf(res.visc=tf, lag.max=25, type="partial", main="PACF of the Residuals\nfor TF-N Model") 
```

![](images/e4207cae1db3774d2173abc6dea9640f2682677ba3d476d81bafe596ba57caee.jpg)

![](images/98ee37604f0caede3f739850a65daeef38849a7dc78c2465903e6f9850673012.jpg)

4-in-1 plot of the residuals   
par(mfrow=c(2,2),oma=c(0,0,0,0))   
qqnorm(res.visc=tf,datax $\equiv$ TRUE,pch $= 16$ xlab $=$ 'Residual',main $=$ "   
qqline(res.visc(tf,dadata $\equiv$ TRUE)   
plot(fit.visc(tf,res.visc(tf,pch $= 16$ ,xlab $=$ 'Fitted Value' ylab $=$ 'Residual')   
abline(h=0)   
hist(res.visc(tf,col $=$ "gray",xlab $=$ 'Residual',main $=$ "   
plot(res.visc(tf,type $=$ "l",xlab $=$ 'Observation Order',ylab $=$ 'Residual')   
points(res.visc(tf,pch $= 16$ ,cex=.5)   
abline(h=0)

![](images/ebb1cfdb2e21e4429ebe7c86dfe9a3b81abbbb035f26ef2e6a73a65f61263fb8.jpg)

![](images/d4714c966938345cfc84e0d3227dd70b9469c848678114fef34684cf1282ab98.jpg)

![](images/b5e23ef73b4270b6e64bb9a95b128c75d19831a587804d191c9d6ceab97e85c4.jpg)

![](images/6efab505bfa74148d35092510ba51ea3c444446e05a86919f1061021c76fd4ac.jpg)

```r
T<-length(res.visc=tf)   
Ta<-length(alphat)   
ccf(res.visc=tf,alphat[(Ta-T+1):Ta],main='CCF of alpha(t) and \nResiduals of TF-N Model',ylab='CCF')   
abline(v=0,col='blue') 
```

![](images/72049c820dcb8c0a568f2b7c42fb9a7be34b9b4b052db130e039fcc501b5ab40.jpg)  
CCF of alpha(t) and Residuals of TF-N model

Example 6.4 Note that this is a continuation of Example 6.2. Below we assume that the reader followed the modeling efforts required in Example 6.2. For variable and model names, please refer to R-code for Example 6.2.

For forecasting we use the formula given in the example. Before we proceed, we rst make forecasts for x(t) based on the AR(1) model. We will only make 6-step-ahead forecasts for x(t) even if not all of them are needed due to delay.

```txt
tau<-6  
xt.ar1_forecast<-forecast(xt.ar1,h=tau) 
```

To make the recursive calculations given in the example simpler, we will simply extend the xt, yt and residuals vectors as the following.

```r
xt.new<-c(xt, xt.ar1.forecast\\(mean)   
res=tf.new<-c(rep(0,3),res.visc=tf,rep(0,tau))   
yt.new<-c(yt,rep(0,tau)) 
```

```txt
Note that 3 0's are added to the beginning to compensate for the misalignment between xt and the residuals of transfer function noise model due to the delay of 3 lags. Last 6 0's are added since the future values of the error are assumed to be 0. 
```

We now get the parameter estimates for the transfer function鈥搉oise model

```txt
phil<-visc=tf[[1]][1]  
d1<-visc=tf[[1]][2]  
d2<-visc=tf[[1]][3]  
w0<-visc=tf[[1]][4] 
```

# The forecasts are then obtained using:

T<-length(yt)   
for (i in $(\mathrm{T} + 1):(T + \mathrm{tau}))$ { yt.new[i] <- (d1+phi1)*yt.new[i-1] + (d2-d1*phi1)*yt.new[i-2] -d2*phi1*yt.new[i-3] +w0*xt.new[i-3] -w0*phi1*xt.new[i-4] +res=tf.new[i] -d1*res=tf.new[i-1] - d2*res=tf.new[i-1]   
}

To calculate the prediction limits, we need to rst calculate the estimate of forecast error variance given in (6.45). As mentioned in the example, since we only need up to six-step ahead forecasts, we need to calculate only $\nu _ { 0 } ^ { * } , \nu _ { 1 } ^ { * }$ and $\nu _ { 2 } ^ { * }$ and $\psi _ { 0 }$ through $\psi _ { 5 }$ .

```txt
phix<-xt.ar1[[1]][1]  
v0star<-w0  
vlstar<- (d1+phix) *w0  
v2star<- (((d1+phix)^2)+(d2-d1*phix)) *w0  
vstar<-c(v0star,v1star,v2star)  
psi<-phix^(0:(tau-1))  
sig2.alpha<-xt.ar1$sigma2  
sig2.err<-visc.tf$sigma2  
sig2.tfn<-rep(0,6)  
b<-3  
for (i in 1:6) {  
    if ((i-b) <= 0) {  
        sig2.tfn[i] <- sig2.err*sum(psi[1:i]^2)  
    }  
else {  
        sig2.tfn[i] <- sig2.alpha*sum(vstar[1:(i-b)]^2) + sig2.err*sum(psi[1:i]^2)  
    }  
} 
```

For comparison purposes, we also t a univariate ARIMA model to y(t). An AR(3) model is considered even though the AR coefcient at third lag is borderline signicant. The model is given below.

yt.ar3<-arima(yt,order=c(3,0,0)锛宨nclude.mean $\equiv$ FALSE) >yt.ar3 Series:x ARIMA(3,0,0锛墂ith zero mean

```txt
Coefficients: ar1 ar2 ar3 0.9852 0.1298 -0.2700 s.e. 0.0954 0.1367 0.0978 sigma^2 estimated as 0.02779: log likelihood=36.32 AIC=-66.65 AIC=-66.23 BIC=-56.23 
```

To calculate the prediction limits, we need to rst calculate the estimate of forecast error variance given in (5.83). To estimate $\psi _ { 0 }$ through $\psi _ { 5 }$ we use the formula given in (5.46).

```txt
psi.yt<-vector()   
psi.yt[1:4]<-c(0,0,0,1)   
sig2.yt<-yt.ar3\$sigma2   
for (i in 5:(4+tau-1)){ psi.yt[i]<-yt.ar3[[1]][1]*psi.yt[i-1]+yt.ar3[[1]][2] \*psi.yt[i-2]+yt.ar3[[1]][3]*psi.yt[i-3]   
}   
psi.yt<-psi.yt[4:(4+tau-1)]   
psi.yt   
sig2.um<-rep(0,6)   
b<-3   
for (i in 1:6){ sig2.um[i]<-sig2.yt\*sum(psi.yt[1:i]^2) 
```

Thus for the transfer function-noise model and the univariate model we have the following prediction error variances.

```python
cbind(sig2.tfn,sig2.um sig2.tfn sig2.um [1,] 0.01230047 0.02778995 [2,] 0.01884059 0.05476427 [3,] 0.02231796 0.08841901 [4,] 0.04195042 0.11308279 [5,] 0.06331720 0.13109029 [6,] 0.08793347 0.14171137 
```

We can see that adding the exogenous variable $\mathbf { \boldsymbol { x } } ( \mathbf { \boldsymbol { t } } )$ helps to reduce the prediction error variance by half.

# To plot the forecasts and the prediction limits, we have

plot(yt.new[1:T],type $=$ "p",pch $= 16$ ,cex $= .5$ ,xlab $\equiv$ 'Time' ylab $\equiv$ 'Viscosity',xlim=c(1,110)) lines(101:106,yt.new[101:106],col $\equiv$ "grey40") lines(101:106锛寉t.new[101:106] $+2^{*}$ sqrt(sig2.tfn)) lines(101:106锛寉t.new[101:106]-2\*sqrt(sig2.tfn)) legend(20,-.4,c("y(t)","Forecast","95%LPL","95%UPL")锛宲ch=c(16, NA锛孨A,NA),lwd=c(NA锛?5锛?5锛?5)锛宑ex $= .55$ ,col $\equiv$ c("black","grey40", "black","black"))

![](images/30621f2e62c96d3340e81849b89f7217dc87286c1b8d2756f38b029e471dfefc.jpg)

# Example 6.5 The data for this example are in the array called cerealsales.data of which the two columns represent the week and the sales respectively. We rst start with the plot of the data

yt.sales<-cerealsales.data[,2] plot(yt.sales,type $=$ "o",pch $= 16$ ,cex $= .5$ ,xlab $\equiv$ 'Week',ylab $\equiv$ 'Sales') abline(v=88) mtext("Week 88",side $= 3$ 锛宎t $= 88$

![](images/29725178e1ce4571874dfc57241ab7b01d1638f17f2808dc5666510ca7b3992b.jpg)

We then try to identify the ARIMA model for the pre-intervention data (up to week 87)

par(mfrow ${ \bf \Pi } = { \bf C }$ (1,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

acf(yt.sales[1:87],lag.max $^ { = 2 5 }$ ,type $=$ "correlation",main $. =$ "ACF for Sales \nWeeks 1-87")

acf(yt.sales[1:87], lag.max $^ { = 2 5 }$ ,type $: =$ "partial",main $. =$ "PACF for Sales \nWeeks 1-87",ylab $^ { 1 = }$ "PACF")

![](images/c0948ffddb75fce2679929bbfb9bbc66a80954b60eeb73cd058b6ce2d22ee081.jpg)

![](images/8f21c1f5d9dbf70f6b237a30aad3f5da7820cd1c8e12270c96f1c8dc415c284f.jpg)

The series appears to be non-stationary. We try the rst differences

par(mfrow ${ \bf \Pi } = { \bf C }$ (1,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

acf(diff(yt.sales[1:87],1),lag.max $^ { \cdot = 2 5 }$ ,type $: =$ "correlation",main $. =$ "ACF for First Differences \nWeeks 1-87")

acf(diff(yt.sales[1:87],1), lag.max $_ { : = 2 5 }$ ,type $=$ "partial",mai $. =$ "PACF for First Differences \nWeeks 1-87",ylab $^ { \prime = }$ "PACF")

![](images/d29bf60ca84b1469670b00918d574ae640ee25a35fefed8e17cec65074a04e3c.jpg)

![](images/15ea749aa9d592770179a0bb95c8e28d9b043c1fe5577a44bd39a826d794e40f.jpg)

ARIMA(0,0,1) model for the rst differences seems to be appropriate. We now t the transfer function鈥搉oise model with the step input. First we dene the step indicator variable.

library(TSA)

T<-length(yt.sales)

St<-c(rep(0,87),rep(1,(T-87)))

```txt
sales=tf<-arimax(diff(yt.sales), order=c(0,0,1), xtransf=St[2:T], transfer=list(c(0,0)), include.mean = FALSE) #Note that we adjusted the step function for the differencing we # did on the yt.sales 
```

```txt
sales=tf   
Series:diff(yt.sales) ARIMA(0,0,1) with zero mean   
Coefficients: ma1 T1-MA0 -0.5571 -2369.888 s.e. 0.0757 1104.542 sigma^2 estimated as 92762871: log likelihood=-1091.13 AIC=2186.27 AICc=2186.51 BIC=2194.17 
```

# EXERCISES

6.1 An input and output time series consists of 300 observations. The prewhitened input series is well modeled by an AR(2) model $y _ { t } =$ $0 . 5 y _ { t - 1 } + 0 . 2 y _ { t - 2 } + \alpha _ { t }$ . We have estimated $\hat { \sigma } _ { \alpha } = 0 . 2$ and $\hat { \sigma } _ { \beta } = 0 . 4$ . The estimated cross-correlation function between the prewhitened input and output time series is shown below.

<table><tr><td>Lag,j</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>r伪尾(j)</td><td>0.01</td><td>0.03</td><td>-0.03</td><td>-0.25</td><td>-0.35</td><td>-0.51</td><td>-0.30</td><td>-0.15</td><td>-0.02</td><td>0.09</td><td>-0.01</td></tr></table>

a. Find the approximate standard error of the cross-correlation function. Which spikes on the cross-correlation function appear to be signicant?   
b. Estimate the impulse response function. Tentatively identify the form of the transfer function model.

6.2 Find initial estimates of the parameters of the transfer function model for the situation in Exercise 6.1.   
6.3 An input and output time series consists of 200 observations. The prewhitened input series is well modeled by an MA(1) model $y _ { t } = 0 . 8 \alpha _ { t - 1 } + \alpha _ { t }$ . We have estimated $\hat { \sigma } _ { \alpha } = 0 . 4$ and $\hat { \sigma } _ { \beta } = 0 . 6$ . The estimated cross-correlation function between the prewhitened input and output time series is shown below.

<table><tr><td>Lag,j</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>r伪尾(j)</td><td>0.01</td><td>0.55</td><td>0.40</td><td>0.28</td><td>0.20</td><td>0.07</td><td>0.02</td><td>0.01</td><td>-0.02</td><td>0.01</td><td>-0.01</td></tr></table>

a. Find the approximate standard error of the cross-correlation function. Which spikes on the cross-correlation function appear to be signicant?   
b. Estimate the impulse response function. Tentatively identify the form of the transfer function model.

6.4 Find initial estimates of the parameters of the transfer function model for the situation in Exercise 6.3.   
6.5 Write the equations that must be solved in order to obtain initial estimates of the parameters in a transfer function model with $b = 2$ $r = 1$ , and $s = 0$ .   
6.6 Write the equations that must be solved in order to obtain initial estimates of the parameters in a transfer function model with $b = 2$ , $r = 2$ , and $s = 1$ .   
6.7 Write the equations that must be solved in order to obtain initial estimates of the parameters in a transfer function model with $b = 2$ $r = 1$ , and $s = 1$ .   
6.8 Consider a transfer function model with $b = 2$ , $r = 1$ , and $s = 0$ . Assume that the noise model is AR(1). Find the forecasts in terms of the transfer function and noise model parameters.   
6.9 Consider the transfer function model in Exercise 6.8 with $b = 2$ $r = 1$ , and $s = 0$ . Now assume that the noise model is AR(2). Find the forecasts in terms of the transfer function and noise model parameters. What difference does this noise model make on the forecasts?   
6.10 Consider the transfer function model in the Exercise 6.8 with $b =$ 2, $r = 1$ , and $s = 0$ . Now assume that the noise model is MA(1). Find the forecasts in terms of the transfer function and noise model parameters. What difference does this noise model make on the forecasts?   
6.11 Consider the transfer function model

$$
y _ {t} = \frac {- 0 . 5 - 0 . 4 B - 0 . 2 B ^ {2}}{1 - 0 . 5 B} x _ {t - 2} + \frac {1}{1 - 0 . 5 B} \varepsilon_ {t}.
$$

Find the forecasts that are generated from this model.

6.12 Sketch a graph of the impulse response function for the following transfer function:

$$
y _ {t} = \frac {2 B}{1 - 0 . 6 B} x _ {t}.
$$

6.13 Sketch a graph of the impulse response function for the following transfer function:

$$
y _ {t} = \frac {1 - 0 . 2 B}{1 - 0 . 8 B} x _ {t}.
$$

6.14 Sketch a graph of the impulse response function for the following transfer function:

$$
y _ {t} = \frac {1}{1 - 1 . 2 B + 0 . 4 B ^ {2}} x _ {t}.
$$

6.15 Box, Jenkins, and Reinsel (1994) t a transfer function model to data from a gas furnace. The input variable is the volume of methane entering the chamber in cubic feet per minute and the output is the concentration of carbon dioxide emitted. The transfer function model is

$$
y _ {t} = \frac {- (0 . 5 3 + 0 . 3 7 B + 0 . 5 1 B ^ {2})}{1 - 0 . 5 7 B} x _ {t} + \frac {1}{1 - 0 . 5 3 B + 0 . 6 3 B ^ {2}} \varepsilon_ {t},
$$

where the input and output variables are measured every nine seconds.

a. What are the values of b, s,and $r$ for this model?   
b. What is the form of the ARIMA model for the errors?   
c. If the methane input was increased, how long would it take before the carbon dioxide concentration in the output is impacted?

6.16 Consider the global mean surface air temperature anomaly and global $\mathrm { C O } _ { 2 }$ concentration data in Table B.6 in Appendix B. Fit an appropriate transfer function model to this data, assuming that $\mathrm { C O } _ { 2 }$ concentration is the input variable.

6.17 Consider the chemical process yield and uncontrolled operating temperature data in Table B.12. Fit an appropriate transfer function model to these data, assuming that temperature is the input variable. Does including the temperature data improve your ability to forecast the yield data?

6.18 Consider the U.S. Internal Revenue tax refunds data in Table B.20. Fit an appropriate transfer function model to these data, assuming that population is the input variable. Does including the population data improve your ability to forecast the tax refund data?

6.19 Find time series data of interest to you where a transfer function鈥?noise model would be appropriate.

a. Identify and t the appropriate transfer function鈥搉oise model.   
b. Use an ARIMA model to t only the $y _ { t }$ series.   
c. Compare the forecasting performance of the two models from parts a and b.

6.20 Find a time series of interest to you that you think may be impacted by an outlier. Fit an appropriate ARIMA model to the time series and use either the additive outlier or innovation outlier model to see if the potential outlier is statistically signicant.

6.21 Table E6.1 provides 100 observations on a time series.

TABLE E6.1 Time Series Data for Exercise 6.21 (100 observations, read down then across)   

<table><tr><td>86.74</td><td>83.79</td><td>88.42</td><td>84.23</td><td>82.20</td></tr><tr><td>85.32</td><td>84.04</td><td>89.65</td><td>83.58</td><td>82.14</td></tr><tr><td>84.74</td><td>84.10</td><td>97.85</td><td>84.13</td><td>81.80</td></tr><tr><td>85.11</td><td>84.85</td><td>88.50</td><td>82.70</td><td>82.32</td></tr><tr><td>85.15</td><td>87.64</td><td>87.06</td><td>83.55</td><td>81.53</td></tr><tr><td>84.48</td><td>87.24</td><td>85.20</td><td>86.47</td><td>81.73</td></tr><tr><td>84.68</td><td>87.52</td><td>85.08</td><td>86.21</td><td>82.54</td></tr><tr><td>84.68</td><td>86.50</td><td>84.44</td><td>87.02</td><td>82.39</td></tr><tr><td>86.32</td><td>85.61</td><td>84.21</td><td>86.65</td><td>82.42</td></tr><tr><td>88.00</td><td>86.83</td><td>86.00</td><td>85.71</td><td>82.21</td></tr><tr><td>86.26</td><td>84.50</td><td>85.57</td><td>86.15</td><td>82.77</td></tr><tr><td>85.83</td><td>84.18</td><td>83.79</td><td>85.80</td><td>83.12</td></tr><tr><td>83.75</td><td>85.46</td><td>84.37</td><td>85.62</td><td>83.22</td></tr><tr><td>84.46</td><td>86.15</td><td>83.38</td><td>84.23</td><td>84.45</td></tr><tr><td>84.65</td><td>86.41</td><td>85.00</td><td>83.57</td><td>84.91</td></tr><tr><td>84.58</td><td>86.05</td><td>84.35</td><td>84.71</td><td>85.76</td></tr><tr><td>82.25</td><td>86.66</td><td>85.34</td><td>83.82</td><td>85.23</td></tr><tr><td>83.38</td><td>84.73</td><td>86.05</td><td>82.42</td><td>86.73</td></tr><tr><td>83.54</td><td>85.95</td><td>84.88</td><td>83.04</td><td>87.00</td></tr><tr><td>85.16</td><td>86.85</td><td>85.42</td><td>83.70</td><td>85.06</td></tr></table>

a. Plot the data.   
b. There is an apparent outlier in the data. Use intervention analysis to investigate the presence of this outlier.

6.22 Table E6.2 provides 100 observations on a time series. These data represent weekly shipments of a product.

a. Plot the data.   
b. Note that there is an apparent increase in the level of the time series at about observation 80. Management suspects that this increase in shipments may be due to a strike at a competitor鈥檚 plant. Build an appropriate intervention model for these data. Do you think that the impact of this intervention is likely to be permanent?

TABLE E6.2 Time Series Data for Exercise 6.22 (100 observations, read down then across)   

<table><tr><td>1551</td><td>1556</td><td>1613</td><td>1552</td><td>1838</td></tr><tr><td>1548</td><td>1557</td><td>1595</td><td>1558</td><td>1838</td></tr><tr><td>1554</td><td>1564</td><td>1601</td><td>1543</td><td>1834</td></tr><tr><td>1557</td><td>1592</td><td>1587</td><td>1552</td><td>1840</td></tr><tr><td>1552</td><td>1588</td><td>1568</td><td>1581</td><td>1832</td></tr><tr><td>1555</td><td>1591</td><td>1567</td><td>1578</td><td>1834</td></tr><tr><td>1556</td><td>1581</td><td>1561</td><td>1587</td><td>1842</td></tr><tr><td>1574</td><td>1572</td><td>1558</td><td>1583</td><td>1840</td></tr><tr><td>1591</td><td>1584</td><td>1576</td><td>1573</td><td>1840</td></tr><tr><td>1575</td><td>1561</td><td>1572</td><td>1578</td><td>1838</td></tr><tr><td>1571</td><td>1558</td><td>1554</td><td>1574</td><td>1844</td></tr><tr><td>1551</td><td>1571</td><td>1560</td><td>1573</td><td>1848</td></tr><tr><td>1558</td><td>1578</td><td>1550</td><td>1559</td><td>1849</td></tr><tr><td>1561</td><td>1580</td><td>1566</td><td>1552</td><td>1861</td></tr><tr><td>1560</td><td>1577</td><td>1560</td><td>1563</td><td>1865</td></tr><tr><td>1537</td><td>1583</td><td>1570</td><td>1555</td><td>1874</td></tr><tr><td>1549</td><td>1564</td><td>1577</td><td>1541</td><td>1869</td></tr><tr><td>1551</td><td>1576</td><td>1565</td><td>1547</td><td>1884</td></tr><tr><td>1567</td><td>1585</td><td>1571</td><td>1553</td><td>1886</td></tr><tr><td>1553</td><td>1601</td><td>1559</td><td>1538</td><td>1867</td></tr></table>

6.23 Table B.23 contains data on Danish crude oil production. Historically, oil production increased steadily from 1972 up to about 2000, when the Danish government presented an energy strategy

containing a number of ambitious goals for national energy policy up through 2025. The aim is to reduce Denmark鈥檚 dependency on coal, oil and natural gas. The data exhibit a marked downturn in oil production starting in 2005. Fit and analyze an appropriate intervention model to these data.

6.24 Table B.25 contains data on annual US motor vehicle fatalities from 1966 through 2012, along with data on several other factors. Fit a transfer function model to these data using the number of licensed drivers as the input time series. Compare this transfer function model to a univariate ARIMA model for the annual fatalities data.   
6.25 Table B.25 contains data on annual US motor vehicle fatalities from 1966 through 2012, along with data on several other factors. Fit a transfer function model to these data using the annual unemployment rate as the input time series. Compare this transfer function model to a univariate ARIMA model for the annual fatalities data. Why do you think that the annual unemployment rate might be a good predictor of fatalities?   
6.26 Table B.25 contains data on annual US motor vehicle fatalities from 1966 through 2012, along with data on several other factors. Fit a transfer function model to these data using both the number of licensed drivers and the annual unemployment rate as the input time series. Compare this two-input transfer function model to a univariate ARIMA model for the annual fatalities data, and to the two univariate transfer function models from Exercises 6.24 and 6.25.

# SURVEY OF OTHER FORECASTING METHODS

I always avoid prophesying beforehand, because it is a much better policy to prophesy after the event has already taken place.

SIR WINSTON CHURCHILL, British Prime Minister

# 7.1 MULTIVARIATE TIME SERIES MODELS AND FORECASTING

In many forecasting problems, it may be the case that there are more than just one variable to consider. Attempting to model each variable individually may at times work. However, in these situations, it is often the case that these variables are somehow cross-correlated, and that structure can be effectively taken advantage of in forecasting. In the previous chapter we explored this for the 鈥渦nidirectional鈥?case, where it is assumed that certain inputs have impact on the variable of interest but not the other way around. Multivariate time series models involve several variables that are not only serially but also cross-correlated. As in the univariate case, multivariate or vector ARIMA models can often be successfully used in forecasting multivariate time series. Many of the concepts we have seen in Chapter 5

will be directly applicable in the multivariate case as well. We will -rst start with the property of stationarity.

# 7.1.1 Multivariate Stationary Process

Suppose that the vector time series $Y _ { t } = ( y _ { 1 t } , y _ { 2 t } , \dots , y _ { m t } )$ consists of m univariate time series. Then $Y _ { t }$ with -nite -rst and second order moments is said to be weakly stationary if

(i) $E ( Y _ { t } ) = E ( Y _ { t + s } ) = \mu$ , constant for all $s$   
(ii) $\operatorname { C o v } ( Y _ { t } ) = E [ ( Y _ { t } - \mu ) ( Y _ { t } - \mu ) ^ { \prime } ] = \Gamma ( 0 )$   
(iii) $\operatorname { C o v } ( Y _ { t } , Y _ { t + s } ) = \Gamma ( s )$ depends only on s

Note that the diagonal elements of $\Gamma ( s )$ give the autocovariance function of the individual time series, $\gamma _ { i i } ( s )$ . Similarly, the autocorrelation matrix is given by

$$
\boldsymbol {\rho} (s) = \left[ \begin{array}{c c c c} \rho_ {1 1} (s) & \rho_ {1 2} (s) & \dots & \rho_ {1 m} (s) \\ \rho_ {2 1} (s) & \rho_ {2 2} (s) & \dots & \rho_ {2 m} (s) \\ \vdots & \vdots & \ddots & \vdots \\ \rho_ {m 1} (s) & \rho_ {m 2} (s) & \dots & \rho_ {m m} (s) \end{array} \right] \tag {7.1}
$$

which can also be obtained by de-ning

$$
\begin{array}{l} \mathbf {V} = \operatorname {d i a g} \left\{\gamma_ {1 1} (0), \gamma_ {2 2} (0), \dots , \gamma_ {m m} (0) \right\} \\ = \left[ \begin{array}{c c c c} \gamma_ {1 1} (0) & 0 & \dots & 0 \\ 0 & \gamma_ {2 2} (0) & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \gamma_ {m m} (0) \end{array} \right] \tag {7.2} \\ \end{array}
$$

We then have

$$
\rho (s) = \mathbf {V} ^ {- 1 / 2} \boldsymbol {\Gamma} (s) \mathbf {V} ^ {- 1 / 2} \tag {7.3}
$$

We can further show that $\boldsymbol { \Gamma } ( s ) = \boldsymbol { \Gamma } ( - s ) ^ { \prime }$ and $\mathbf { \boldsymbol { \rho } } ( s ) = \mathbf { \boldsymbol { \rho } } ( - s ) ^ { \prime }$ .

# 7.1.2 Vector ARIMA Models

The stationary vector time series can be represented with a vector ARMA model given by

$$
\Phi (B) Y _ {t} = \boldsymbol {\delta} + \Theta (B) \varepsilon_ {t} \tag {7.4}
$$

where $\Phi ( B ) = \mathbf { I } - \Phi _ { 1 } B - \Phi _ { 2 } B ^ { 2 } - \dots - \Phi _ { p } B ^ { p }$ , $\Theta ( B ) = { \bf I } - \Theta _ { 1 } B - \Theta _ { 2 } B ^ { 2 } -$ $\cdots - \Theta _ { q } B ^ { q }$ , and $\mathbf { \delta } \mathbf { \varepsilon } _ { \varepsilon _ { t } }$ represents the sequence of independent random vectors with $E ( \dot { \bf { e } } _ { t } ) = { \bf 0 }$ and $\mathrm { C o v } ( \mathbf { \varepsilon } _ { t } ) = \Sigma$ . Since the random vectors are independent, we have $\Gamma _ { \varepsilon } ( s ) = 0$ for all $s \neq 0$ .

The process $Y _ { t }$ in Eq. (7.4) is stationary if the roots of

$$
\det  [ \Phi (B) ] = \det  [ \mathbf {I} - \Phi_ {1} B - \Phi_ {2} B ^ {2} - \dots - \Phi_ {p} B ^ {p} ] = 0 \tag {7.5}
$$

are all greater than one in absolute value. Then the process $Y _ { t }$ is also said to have in-nite MA representation given as

$$
\begin{array}{l} Y _ {t} = \boldsymbol {\mu} + \boldsymbol {\Psi} (B) \boldsymbol {\varepsilon} _ {t} \\ = \boldsymbol {\mu} + \sum_ {i = 0} ^ {\infty} \Psi_ {i} \boldsymbol {\varepsilon} _ {t - i} \tag {7.6} \\ \end{array}
$$

where $\pmb { \Psi } ( B ) = \pmb { \Phi } ( B ) ^ { - 1 } \pmb { \Theta } ( B )$ , $\mu = \Phi ( B ) ^ { - 1 } \delta$ , and $\textstyle \sum _ { i = 0 } ^ { \infty } \| \Psi _ { i } \| ^ { 2 } < \infty$

Similarly, if the roots of de $\mathbf { \nabla } [ \Theta ( B ) ] = \operatorname* { d e t } [ \mathbf { I } - \Theta _ { 1 } B - \Theta _ { 2 } B ^ { 2 } -$ $\dots - \Theta _ { q } B ^ { q } ] = 0$ are greater than unity in absolute value the process $Y _ { t }$ in Eq. (7.4) is invertible.

To illustrate the vector ARMA model given in Eq. (7.4), consider the bivariate ARMA(1,1) model with

$$
\begin{array}{l} \boldsymbol {\Phi} (B) = \mathbf {I} - \boldsymbol {\Phi} _ {1} B \\ = \left[ \begin{array}{c c} 1 & 0 \\ 0 & 1 \end{array} \right] - \left[ \begin{array}{c c} \phi_ {1 1} & \phi_ {1 2} \\ \phi_ {2 1} & \phi_ {2 2} \end{array} \right] B \\ \end{array}
$$

and

$$
\begin{array}{l} \boldsymbol {\Theta} (B) = \mathbf {I} - \boldsymbol {\Theta} _ {1} B \\ = \left[ \begin{array}{c c} 1 & 0 \\ 0 & 1 \end{array} \right] - \left[ \begin{array}{c c} \theta_ {1 1} & \theta_ {1 2} \\ \theta_ {2 1} & \theta_ {2 2} \end{array} \right] B \\ \end{array}
$$

Hence the model can be written as

$$
\left[ \left[ \begin{array}{c c} 1 & 0 \\ 0 & 1 \end{array} \right] - \left[ \begin{array}{c c} \phi_ {1 1} & \phi_ {1 2} \\ \phi_ {2 1} & \phi_ {2 2} \end{array} \right] B \right] Y _ {t} = \left[ \begin{array}{c} \delta_ {1} \\ \delta_ {2} \end{array} \right] + \left[ \left[ \begin{array}{c c} 1 & 0 \\ 0 & 1 \end{array} \right] - \left[ \begin{array}{c c} \theta_ {1 1} & \theta_ {1 2} \\ \theta_ {2 1} & \theta_ {2 2} \end{array} \right] B \right] \left[ \begin{array}{c} \varepsilon_ {1, t} \\ \varepsilon_ {2, t} \end{array} \right]
$$

or

$$
\begin{array}{l} y _ {1, t} = \delta_ {1} + \phi_ {1 1} y _ {1, t - 1} + \phi_ {1 2} y _ {2, t - 1} + \varepsilon_ {1, t} - \theta_ {1 1} \varepsilon_ {1, t - 1} - \theta_ {1 2} \varepsilon_ {2, t - 1} \\ y _ {2, t} = \delta_ {2} + \phi_ {2 1} y _ {1, t - 1} + \phi_ {2 2} y _ {2, t - 1} + \varepsilon_ {2, t} - \theta_ {2 1} \varepsilon_ {1, t - 1} - \theta_ {2 2} \varepsilon_ {2, t - 1} \\ \end{array}
$$

As in the univariate case, if nonstationarity is present, through an appropriate degree of differencing a stationary vector time series may be achieved. Hence the vector ARIMA model can be represented as

$$
\boldsymbol {\Phi} (B) \mathbf {D} (B) Y _ {t} = \boldsymbol {\delta} + \boldsymbol {\Theta} (B) \boldsymbol {\varepsilon} _ {t}
$$

where

$$
\begin{array}{l} \mathbf {D} (B) = \operatorname {d i a g} \left\{\left(1 - B\right) ^ {d _ {1}}, \left(1 - B\right) ^ {d _ {2}}, \dots , \left(1 - B\right) ^ {d _ {m}} \right\} \\ = \left[ \begin{array}{c c c c} (1 - B) ^ {d _ {1}} & 0 & \ldots & 0 \\ 0 & (1 - B) ^ {d _ {2}} & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & (1 - B) ^ {d _ {m}} \end{array} \right] \\ \end{array}
$$

However, the degree of differencing is usually quite complicated and has to be handled with care (Reinsel (1997)).

The identi-cation of the vector ARIMA model can indeed be fairly dif-cult. Therefore in the next section we will concentrate on the more commonly used and intuitively appealing vector autoregressive models. For a more general discussion see Reinsel (1997), Lutkepohl (2005), Tiao 篓 and Box (1981), Tiao and Tsay (1989), Tsay (1989), and Tjostheim and Paulsen (1982).

# 7.1.3 Vector AR (VAR) Models

The vector $\operatorname { A R } ( p )$ model is given by

$$
\boldsymbol {\Phi} (B) Y _ {t} = \boldsymbol {\delta} + \boldsymbol {\varepsilon} _ {t} \tag {7.7}
$$

or

$$
Y _ {t} = \pmb {\delta} + \sum_ {i = 1} ^ {p} \pmb {\Phi} _ {i} Y _ {t - i} + \pmb {\varepsilon} _ {t}
$$

For a stationary vector AR process, the in-nite MA representation is given as

$$
Y _ {t} = \boldsymbol {\mu} + \boldsymbol {\Psi} (B) \boldsymbol {\varepsilon} _ {t} \tag {7.8}
$$

where $\Psi ( B ) = { \bf I } + \Psi _ { 1 } B + \Psi _ { 2 } B ^ { 2 } + \cdots$ and ${ \bf \mu } = { \bf \Phi } ( B ) ^ { - 1 } \delta$ . Hence we have $E ( Y _ { t } ) = \mu$ and $\mathrm { C o v } ( \mathfrak { e } _ { t } , Y _ { t - s } ) = 0$ for any $s > 0$ since $Y _ { t - s }$ is only concerned

with $\mathbf { \delta } _ { \mathbf { \varepsilon } _ { t - s } }$ , 饾泦t鈭抯鈭?, 鈥? which are not correlated with $\mathbf { \delta } \mathbf { \varepsilon } _ { \varepsilon _ { t } }$ . Moreover, we also have

$$
\begin{array}{l} \operatorname {C o v} \left(\varepsilon_ {t}, Y _ {t}\right) = \operatorname {C o v} \left(\varepsilon_ {t}, \varepsilon_ {t} + \boldsymbol {\Psi} _ {1} \varepsilon_ {t - 1} + \boldsymbol {\Psi} _ {2} \varepsilon_ {t - 2} + \dots\right) \\ = \operatorname {C o v} \left(\boldsymbol {\varepsilon} _ {t}, \boldsymbol {\varepsilon} _ {t}\right) \\ = \Sigma \\ \end{array}
$$

and

$$
\begin{array}{l} \boldsymbol {\Gamma} (s) = \operatorname {C o v} \left(Y _ {t - s}, Y _ {t}\right) = \operatorname {C o v} \left(Y _ {t - s}, \boldsymbol {\delta} + \sum_ {i = 1} ^ {p} \boldsymbol {\Phi} _ {i} Y _ {t - i} + \boldsymbol {\varepsilon} _ {t}\right) \\ = \operatorname {C o v} \left(Y _ {t - s}, \sum_ {i = 1} ^ {p} \boldsymbol {\Phi} _ {i} Y _ {t - i}\right) + \underbrace {\operatorname {C o v} (Y _ {t - s} , \boldsymbol {\varepsilon} _ {t})} _ {= 0 \text {f o r} s > 0} \\ = \sum_ {i = 1} ^ {p} \operatorname {C o v} \left(Y _ {t - s}, \boldsymbol {\Phi} _ {i} Y _ {t - i}\right) \tag {7.9} \\ = \sum_ {i = 1} ^ {p} \operatorname {C o v} \left(Y _ {t - s}, Y _ {t - i}\right) \boldsymbol {\Phi} _ {i} ^ {\prime} \\ \end{array}
$$

Hence we have

$$
\boldsymbol {\Gamma} (s) = \sum_ {i = 1} ^ {p} \boldsymbol {\Gamma} (s - i) \boldsymbol {\Phi} _ {i} ^ {\prime} \tag {7.10}
$$

and

$$
\boldsymbol {\Gamma} (0) = \sum_ {i = 1} ^ {p} \boldsymbol {\Gamma} (- i) \boldsymbol {\Phi} _ {i} ^ {\prime} + \boldsymbol {\Sigma} \tag {7.11}
$$

As in the univariate case, the Yule鈥揥alker equations can be obtained from the -rst $p$ equations as

$$
\left[ \begin{array}{c} \mathbf {\Gamma} (1) \\ \mathbf {\Gamma} (2) \\ \vdots \\ \mathbf {\Gamma} (p) \end{array} \right] = \left[ \begin{array}{c c c c} \mathbf {\Gamma} (0) & \mathbf {\Gamma} (1) ^ {\prime} & \dots & \mathbf {\Gamma} (p - 1) ^ {\prime} \\ \mathbf {\Gamma} (1) & \mathbf {\Gamma} (0) & \dots & \mathbf {\Gamma} (p - 2) ^ {\prime} \\ \vdots & \vdots & \ddots & \vdots \\ \mathbf {\Gamma} (p - 1) & \mathbf {\Gamma} (p - 2) & \dots & \mathbf {\Gamma} (0) \end{array} \right] \left[ \begin{array}{c} \mathbf {\Phi} _ {1} ^ {\prime} \\ \mathbf {\Phi} _ {2} ^ {\prime} \\ \vdots \\ \mathbf {\Phi} _ {p} ^ {\prime} \end{array} \right] \tag {7.12}
$$

The model parameters in $\Phi$ and $\pmb { \Sigma }$ can be estimated from Eqs. (7.11) and (7.12).

For the $\operatorname { V A R } ( p )$ , the autocorrelation matrix in Eq. (7.3) will exhibit a decaying behavior following a mixture of exponential decay and damped sinusoid.

Example 7.1 VAR(1) Model The autocovariance matrix for VAR(1) is given as

$$
\boldsymbol {\Gamma} (s) = \boldsymbol {\Gamma} (s - 1) \boldsymbol {\Phi} ^ {\prime} = (\boldsymbol {\Gamma} (s - 2) \boldsymbol {\Phi} ^ {\prime}) \boldsymbol {\Phi} ^ {\prime} = \dots = \boldsymbol {\Gamma} (0) \left(\boldsymbol {\Phi} ^ {\prime}\right) ^ {s} \tag {7.13}
$$

and

$$
\begin{array}{l} \boldsymbol {\rho} (s) = \mathbf {V} ^ {- 1 / 2} \boldsymbol {\Gamma} (s) \mathbf {V} ^ {- 1 / 2} \\ = \mathbf {V} ^ {- 1 / 2} \boldsymbol {\Gamma} (0) \left(\boldsymbol {\Phi} ^ {\prime}\right) ^ {s} \mathbf {V} ^ {- 1 / 2} \\ = \mathbf {V} ^ {- 1 / 2} \boldsymbol {\Gamma} (0) \mathbf {V} ^ {- 1 / 2} \mathbf {V} ^ {1 / 2} \left(\boldsymbol {\Phi} ^ {\prime}\right) ^ {s} \mathbf {V} ^ {- 1 / 2} \\ = \rho (0) \mathbf {V} ^ {1 / 2} \left(\boldsymbol {\Phi} ^ {\prime}\right) ^ {s} \mathbf {V} ^ {- 1 / 2} \tag {7.14} \\ \end{array}
$$

where $\mathbf { V } = \mathrm { d i a g } \{ \gamma _ { 1 1 } ( 0 ) , \gamma _ { 2 2 } ( 0 ) , \dots , \gamma _ { m m } ( 0 ) \}$ . The eigenvalues of $\Phi$ determine the behavior of the autocorrelation matrix. In fact, if the eigenvalues of $\Phi$ are real and/or complex conjugates, the behavior will be a mixture of the exponential decay and damped sinusoid, respectively.

Example 7.2 The pressure readings at two ends of an industrial furnace are taken every 10 minutes and given in Table 7.1. It is expected the individual time series are not only autocorrelated but also cross-correlated. Therefore it is decided to -t a multivariate time series model to this data. The time series plots of the data are given in Figure 7.1. To identify the model we consider the sample ACF plots as well as the cross correlation of the time series given in Figure 7.2. These plots exhibit an exponential decay pattern, suggesting that an autoregressive model may be appropriate. It is further conjectured that a VAR(1) or VAR(2) model may provide a good -t. Another approach to model identi-cation would be to -t ARIMA models to the individual time series and consider the cross correlation of the residuals. For that, we -t an AR(1) model to both time series. The cross-correlation plot of the residuals given in Figure 7.3 further suggests that the VAR(1) model may indeed provide an appropriate -t. Using the SAS ARIMA procedure given in Table 7.2, we -t a VAR(1) model. The SAS output in Table 7.3 con-rms that the VAR(1) model provides an appropriate -t for the data. The time series plots of the residuals and the -tted values are given in Figures 7.3, 7.4, and 7.5.

TABLE 7.1 Pressure Readings at Both Ends of the Furnace   

<table><tr><td rowspan="2">Index</td><td colspan="2">Pressure</td><td colspan="2">Pressure</td><td>Index</td><td rowspan="2" colspan="2">Pressure</td><td rowspan="2">Index</td><td colspan="2">Pressure</td><td rowspan="2">Index</td><td colspan="2">Pressure</td><td></td></tr><tr><td>Front</td><td>Back</td><td>Index</td><td>Front</td><td>Back</td><td>Front</td><td>Back</td><td>Front</td><td>Back</td><td></td></tr><tr><td>1</td><td>7.98</td><td>20.1</td><td>39</td><td>10.23</td><td>22.2</td><td>77</td><td>9.23</td><td>21.19</td><td>115</td><td>12.73</td><td>19.88</td><td>153</td><td>5.45</td><td>19.46</td></tr><tr><td>2</td><td>8.64</td><td>20.37</td><td>40</td><td>11.27</td><td>18.86</td><td>78</td><td>10.18</td><td>18.52</td><td>116</td><td>13.86</td><td>22.36</td><td>154</td><td>6.5</td><td>18.33</td></tr><tr><td>3</td><td>10.06</td><td>19.99</td><td>41</td><td>9.57</td><td>21.16</td><td>79</td><td>8.6</td><td>21.79</td><td>117</td><td>12.38</td><td>19.04</td><td>155</td><td>5.23</td><td>19.22</td></tr><tr><td>4</td><td>8.13</td><td>19.62</td><td>42</td><td>10.6</td><td>17.89</td><td>80</td><td>9.66</td><td>18.4</td><td>118</td><td>12.51</td><td>23.32</td><td>156</td><td>4.97</td><td>17.7</td></tr><tr><td>5</td><td>8.84</td><td>20.26</td><td>43</td><td>9.22</td><td>20.55</td><td>81</td><td>8.66</td><td>19.17</td><td>119</td><td>14.32</td><td>20.42</td><td>157</td><td>4.3</td><td>18.42</td></tr><tr><td>6</td><td>10.28</td><td>19.46</td><td>44</td><td>8.91</td><td>20.47</td><td>82</td><td>7.55</td><td>18.86</td><td>120</td><td>13.47</td><td>20.88</td><td>158</td><td>4.47</td><td>17.85</td></tr><tr><td>7</td><td>9.63</td><td>20.21</td><td>45</td><td>10.15</td><td>20</td><td>83</td><td>9.21</td><td>19.42</td><td>121</td><td>12.96</td><td>20.25</td><td>159</td><td>5.48</td><td>19.16</td></tr><tr><td>8</td><td>10.5</td><td>19.72</td><td>46</td><td>11.32</td><td>20.07</td><td>84</td><td>9.45</td><td>19.54</td><td>122</td><td>11.65</td><td>20.69</td><td>160</td><td>5.7</td><td>17.91</td></tr><tr><td>9</td><td>7.91</td><td>19.5</td><td>47</td><td>12.41</td><td>20.82</td><td>85</td><td>10.61</td><td>19.79</td><td>123</td><td>11.99</td><td>19.7</td><td>161</td><td>3.78</td><td>19.36</td></tr><tr><td>10</td><td>9.71</td><td>18.97</td><td>48</td><td>12.41</td><td>20.98</td><td>86</td><td>10.78</td><td>18.6</td><td>124</td><td>9.6</td><td>18.51</td><td>162</td><td>5.86</td><td>18.18</td></tr><tr><td>11</td><td>10.43</td><td>22.31</td><td>49</td><td>10.36</td><td>20.91</td><td>87</td><td>10.68</td><td>22.5</td><td>125</td><td>7.38</td><td>18.48</td><td>163</td><td>6.56</td><td>20.82</td></tr><tr><td>12</td><td>10.99</td><td>20.16</td><td>50</td><td>9.27</td><td>20.07</td><td>88</td><td>14.05</td><td>19.1</td><td>126</td><td>6.98</td><td>20.37</td><td>164</td><td>6.82</td><td>18.47</td></tr><tr><td>13</td><td>10.08</td><td>20.73</td><td>51</td><td>11.77</td><td>19.82</td><td>89</td><td>14.1</td><td>23.82</td><td>127</td><td>8.18</td><td>18.21</td><td>165</td><td>5.18</td><td>19.09</td></tr><tr><td>14</td><td>9.75</td><td>20.14</td><td>52</td><td>11.93</td><td>21.81</td><td>90</td><td>16.11</td><td>21.25</td><td>128</td><td>7.5</td><td>20.85</td><td>166</td><td>6.3</td><td>18.91</td></tr><tr><td>15</td><td>9.37</td><td>20.34</td><td>53</td><td>13.6</td><td>20.71</td><td>91</td><td>13.58</td><td>20.46</td><td>129</td><td>7.04</td><td>18.9</td><td>167</td><td>9.12</td><td>20.93</td></tr><tr><td>16</td><td>11.52</td><td>18.83</td><td>54</td><td>14.26</td><td>21.94</td><td>92</td><td>12.06</td><td>22.55</td><td>130</td><td>9.06</td><td>19.84</td><td>168</td><td>9.3</td><td>18.73</td></tr><tr><td>17</td><td>10.6</td><td>24.01</td><td>55</td><td>14.81</td><td>21.75</td><td>93</td><td>13.76</td><td>20.78</td><td>131</td><td>8.61</td><td>19.15</td><td>169</td><td>10.37</td><td>22.17</td></tr><tr><td>18</td><td>14.31</td><td>19.7</td><td>56</td><td>11.97</td><td>18.97</td><td>94</td><td>13.55</td><td>20.94</td><td>132</td><td>8.93</td><td>20.77</td><td>170</td><td>11.87</td><td>19.03</td></tr><tr><td>19</td><td>13.3</td><td>22.53</td><td>57</td><td>10.99</td><td>23.11</td><td>95</td><td>13.69</td><td>21.66</td><td>133</td><td>9.81</td><td>18.95</td><td>171</td><td>12.36</td><td>22.15</td></tr><tr><td>20</td><td>14.45</td><td>20.77</td><td>58</td><td>10.61</td><td>18.92</td><td>96</td><td>15.07</td><td>21.61</td><td>134</td><td>9.57</td><td>20.33</td><td>172</td><td>14.61</td><td>20.67</td></tr><tr><td>21</td><td>14.8</td><td>21.69</td><td>59</td><td>9.77</td><td>20.28</td><td>97</td><td>15.14</td><td>21.69</td><td>135</td><td>10.31</td><td>21.69</td><td>173</td><td>13.63</td><td>22.39</td></tr></table>

(continued)

TABLE 7.1 (Continued)   

<table><tr><td rowspan="2">Index</td><td colspan="2">Pressure</td><td colspan="2">Pressure</td><td>Index</td><td rowspan="2" colspan="2">Pressure</td><td rowspan="2">Index</td><td colspan="2">Pressure</td><td rowspan="2">Index</td><td colspan="2">Pressure</td><td></td></tr><tr><td>Front</td><td>Back</td><td>Index</td><td>Front</td><td>Back</td><td>Front</td><td>Back</td><td>Front</td><td>Back</td><td></td></tr><tr><td>22</td><td>15.09</td><td>20.87</td><td>60</td><td>11.5</td><td>21.18</td><td>98</td><td>14.01</td><td>21.85</td><td>136</td><td>11.88</td><td>18.66</td><td>174</td><td>13.12</td><td>19.75</td></tr><tr><td>23</td><td>12.96</td><td>21.42</td><td>61</td><td>10.52</td><td>19.29</td><td>99</td><td>12.69</td><td>20.87</td><td>137</td><td>12.36</td><td>22.35</td><td>175</td><td>10.07</td><td>18.94</td></tr><tr><td>24</td><td>11.28</td><td>18.95</td><td>62</td><td>12.58</td><td>19.9</td><td>100</td><td>11.6</td><td>20.93</td><td>138</td><td>12.18</td><td>19.34</td><td>176</td><td>10.14</td><td>21.47</td></tr><tr><td>25</td><td>10.78</td><td>22.61</td><td>63</td><td>12.33</td><td>19.87</td><td>101</td><td>12.15</td><td>20.57</td><td>139</td><td>12.94</td><td>22.76</td><td>177</td><td>11.02</td><td>19.79</td></tr><tr><td>26</td><td>10.42</td><td>19.93</td><td>64</td><td>9.77</td><td>19.43</td><td>102</td><td>12.99</td><td>21.17</td><td>140</td><td>14.25</td><td>19.6</td><td>178</td><td>11.37</td><td>21.94</td></tr><tr><td>27</td><td>9.79</td><td>21.88</td><td>65</td><td>10.71</td><td>21.32</td><td>103</td><td>11.89</td><td>19.53</td><td>141</td><td>12.86</td><td>23.74</td><td>179</td><td>10.98</td><td>18.73</td></tr><tr><td>28</td><td>11.66</td><td>18.3</td><td>66</td><td>10.01</td><td>17.85</td><td>104</td><td>10.85</td><td>21.14</td><td>142</td><td>12.14</td><td>18.06</td><td>180</td><td>10.04</td><td>21.41</td></tr><tr><td>29</td><td>10.81</td><td>20.76</td><td>67</td><td>9.48</td><td>21.55</td><td>105</td><td>11.81</td><td>20.09</td><td>143</td><td>10.06</td><td>20.11</td><td>181</td><td>11.3</td><td>19.2</td></tr><tr><td>30</td><td>9.79</td><td>17.66</td><td>68</td><td>9.39</td><td>19.04</td><td>106</td><td>9.46</td><td>18.48</td><td>144</td><td>10.17</td><td>19.56</td><td>182</td><td>10.59</td><td>23</td></tr><tr><td>31</td><td>10.02</td><td>23.09</td><td>69</td><td>9.05</td><td>19.04</td><td>107</td><td>9.25</td><td>20.33</td><td>145</td><td>7.56</td><td>19.27</td><td>183</td><td>11.69</td><td>17.47</td></tr><tr><td>32</td><td>11.09</td><td>17.86</td><td>70</td><td>9.06</td><td>21.39</td><td>108</td><td>9.26</td><td>19.82</td><td>146</td><td>7.77</td><td>18.59</td><td>184</td><td>10.73</td><td>21.59</td></tr><tr><td>33</td><td>10.28</td><td>20.9</td><td>71</td><td>9.87</td><td>17.66</td><td>109</td><td>8.55</td><td>20.07</td><td>147</td><td>9.03</td><td>21.85</td><td>185</td><td>13.64</td><td>21.62</td></tr><tr><td>34</td><td>9.24</td><td>19.5</td><td>72</td><td>7.84</td><td>21.61</td><td>110</td><td>8.86</td><td>19.81</td><td>148</td><td>10.8</td><td>19.21</td><td>186</td><td>12.92</td><td>20.23</td></tr><tr><td>35</td><td>10.32</td><td>22.6</td><td>73</td><td>7.78</td><td>18.05</td><td>111</td><td>10.32</td><td>20.64</td><td>149</td><td>9.41</td><td>19.42</td><td></td><td></td><td></td></tr><tr><td>36</td><td>10.65</td><td>19</td><td>74</td><td>6.44</td><td>19.07</td><td>112</td><td>11.39</td><td>20.04</td><td>150</td><td>7.81</td><td>19.79</td><td></td><td></td><td></td></tr><tr><td>37</td><td>8.51</td><td>20.39</td><td>75</td><td>7.67</td><td>19.92</td><td>113</td><td>11.78</td><td>21.52</td><td>151</td><td>7.99</td><td>18.81</td><td></td><td></td><td></td></tr><tr><td>38</td><td>11.46</td><td>19.23</td><td>76</td><td>8.48</td><td>18.3</td><td>114</td><td>13.13</td><td>20.35</td><td>152</td><td>5.78</td><td>18.46</td><td></td><td></td><td></td></tr></table>

![](images/24757ec01b77d7ae08a065e930891378b9d576604780e9b17d1337a79796c5cf.jpg)  
FIGURE 7.1 Time series plots of the pressure readings at both ends of the furnace.

![](images/f5524384c481a7a20370f73d5ef6d437a81ccf5dabd02aea096b5b959c09d1af.jpg)  
(a)

![](images/87d25cdc873fb01e85fc9c94ac3ae3aaba17bf512664319fa69d8b46d734b29c.jpg)  
(b)

![](images/e12ea8f870d002ecc57ad3153a93245614856040da050adbf7fe050f43f0eca2.jpg)  
(c)

![](images/52e2bf37d9ba4fef7174b5db8fd0e23c9980901dca89b9c3df0956e683bca018.jpg)  
  
FIGURE 7.2 The sample ACF plot for: (a) the pressure readings at the front end of the furnace, $y _ { 1 }$ ; (b) the pressure readings at the back end of the furnace, $y _ { 2 }$ ; (c) the cross correlation between $y _ { 1 }$ and $y _ { 2 }$ ; and (d) the cross correlation between the residuals from the AR(1) model for front pressure and the residuals from the AR(1) model for back pressure.

![](images/71b3411f4b4dd58857d40072ecac3e247e4943e5aac882b55882a8c8ce84db3f.jpg)  
FIGURE 7.3 Time series plots of the residuals from the VAR(1) model.

# 7.2 STATE SPACE MODELS

In this section we give a brief introduction to an approach to forecasting based on the state space model. This is a very general approach that can include regression models and ARIMA models. It can also incorporate a Bayesian approach to forecasting and models with time-varying coef--cients. State space models are based on the Markov property, which implies the independence of the future of a process from its past, given the present system state. In this type of system, the state of the process at the current time contains all of the past information that is required to predict future process behavior. We will let the system state at time $t$ be

TABLE 7.2 SAS Commands to Fit a VAR(1) Model to the Pressure Data   
```txt
proc varmax data=simul4; model y1 y2 / p=1 ; output out=residuals;   
run;   
proc print data=residuals;   
run; 
```

TABLE 7.3 SAS Output for the VAR(1) Model for the Pressure Data   

<table><tr><td colspan="7">The VARMAX Procedure</td></tr><tr><td colspan="4">Type of Model</td><td colspan="3">VAR(1)</td></tr><tr><td colspan="2">Estimation Method</td><td colspan="5">Least Squares Estimation</td></tr><tr><td colspan="7">Constant Estimates</td></tr><tr><td colspan="2">Variable</td><td colspan="5">Constant</td></tr><tr><td colspan="2">y1</td><td colspan="5">-6.76331</td></tr><tr><td colspan="2">y2</td><td colspan="5">27.23208</td></tr><tr><td colspan="7">AR Coefficient Estimates</td></tr><tr><td>Lag</td><td>Variable</td><td colspan="2">y1</td><td colspan="3">y2</td></tr><tr><td>1</td><td>y1</td><td colspan="2">0.73281</td><td colspan="3">0.47405</td></tr><tr><td></td><td>y2</td><td colspan="2">0.41047</td><td colspan="3">-0.56040</td></tr><tr><td colspan="7">Schematic</td></tr><tr><td colspan="7">Representation of</td></tr><tr><td colspan="7">Parameter Estimates</td></tr><tr><td colspan="7">Variable/</td></tr><tr><td colspan="2">Lag</td><td>C</td><td colspan="4">AR1</td></tr><tr><td colspan="2">y1</td><td>-</td><td colspan="4">++</td></tr><tr><td colspan="2">y2</td><td>+</td><td colspan="4">+-</td></tr><tr><td colspan="7">+ is &gt; 2*std error,</td></tr><tr><td colspan="7">- is &lt; -2*std error,</td></tr><tr><td colspan="7">. is between,</td></tr><tr><td colspan="7">* is N/A</td></tr><tr><td colspan="7">Model Parameter Estimates</td></tr><tr><td colspan="7">Standard</td></tr><tr><td>Equation</td><td>Parameter</td><td>Estimate</td><td>Error</td><td>t Value</td><td>Pr &gt; |t|</td><td>Variable</td></tr><tr><td>y1</td><td>CONST1</td><td>-6.76331</td><td>1.18977</td><td>-5.68</td><td>0.0001</td><td>1</td></tr><tr><td></td><td>AR1_1_1</td><td>0.73281</td><td>0.03772</td><td>19.43</td><td>0.0001</td><td>y1(t-1)</td></tr><tr><td></td><td>AR1_1_2</td><td>0.47405</td><td>0.06463</td><td>7.33</td><td>0.0001</td><td>y2(t-1)</td></tr><tr><td>y2</td><td>CONST2</td><td>27.23208</td><td>1.11083</td><td>24.51</td><td>0.0001</td><td>1</td></tr><tr><td></td><td>AR1_2_1</td><td>0.41047</td><td>0.03522</td><td>11.66</td><td>0.0001</td><td>y1(t-1)</td></tr><tr><td></td><td>AR1_2_2</td><td>-0.56040</td><td>0.06034</td><td>-9.29</td><td>0.0001</td><td>y2(t-1)</td></tr><tr><td colspan="7">Covariances of Innovations</td></tr><tr><td colspan="2">Variable</td><td colspan="2">y1</td><td colspan="3">y2</td></tr><tr><td colspan="2">y1</td><td colspan="2">1.25114</td><td colspan="3">0.59716</td></tr><tr><td colspan="2">y2</td><td colspan="2">0.59716</td><td colspan="3">1.09064</td></tr><tr><td colspan="7">Information</td></tr><tr><td colspan="7">Criteria</td></tr><tr><td colspan="2">AICC</td><td colspan="5">0.041153</td></tr><tr><td colspan="2">HQC</td><td colspan="5">0.082413</td></tr><tr><td colspan="2">AIC</td><td colspan="5">0.040084</td></tr><tr><td colspan="2">SBC</td><td colspan="5">0.144528</td></tr><tr><td colspan="2">FPEC</td><td colspan="5">1.040904</td></tr></table>

![](images/ae9783b78e457ff6d0676ec49303651f77617c6ca805bdacec0e75e9c872e48a.jpg)  
FIGURE 7.4 Actual and -tted values for the pressure readings at the front end of the furnace.

![](images/e4f8b4629caba1f733b7439b95a7d7e141387ae8b7cdb430947dc5e2c521129e.jpg)  
FIGURE 7.5 Actual and -tted values for the pressure readings at the back end of the furnace.

represented by the state vector $\mathbf { X } _ { t }$ . The elements of this vector are not necessarily observed. A state space model consists of two equations: an observation or measurement equation that describes how time series observations are produced from the state vector, and a state or system equation that describes how the state vector evolves through time. We will write these two equations as

$$
y _ {t} = \mathbf {h} ^ {\prime} _ {t} \mathbf {X} _ {t} + \varepsilon_ {t} (\text {o b s e r v a t i o n e q u a t i o n}) \tag {7.15}
$$

and

$$
\mathbf {X} _ {t} = \mathbf {A} \mathbf {X} _ {t - 1} + \mathbf {G} \mathbf {a} _ {t} (\text {s t a t e e q u a t i o n}) \tag {7.16}
$$

respectively. In the observation equation $\mathbf { h } _ { t }$ is a known vector of constants and $\varepsilon _ { t }$ is the observation error. If the time series is multivariate then $y _ { t }$ and $\varepsilon _ { t }$ become vectors $\mathbf { y } _ { t }$ and $\varepsilon _ { t }$ , and the vector $\mathbf { h } _ { t }$ becomes a matrix H. In the state equation A and G are known matrices and $\mathbf { a } _ { t }$ is the process noise. Note that the state equation resembles a multivariate AR(1) model, except that it represents the state variables rather than an observed time series, and it has an extra matrix G.

The state space model does not look like any of the time series models we have studied previously. However, we can put many of these models in the state space form. This is illustrated in the following two examples.

Example 7.3 Consider an AR(1) model, which we have previously written as

$$
y _ {t} = \phi y _ {t - 1} + \varepsilon_ {t}
$$

In this case we let $X _ { t } = y _ { t }$ and $\mathbf { a } _ { t } = \varepsilon _ { t }$ and write the state equation as

$$
\begin{array}{l} \mathbf {X} _ {t} = \mathbf {A} \mathbf {X} _ {t - 1} + \mathbf {G} \mathbf {a} _ {t} \\ [ y _ {t} ] = [ \phi ] [ y _ {t - 1} ] + [ 1 ] \varepsilon_ {t} \\ \end{array}
$$

and the observation equation is

$$
\begin{array}{l} y _ {t} = \mathbf {h} _ {t} ^ {\prime} \mathbf {X} _ {t} + \varepsilon_ {t} \\ y _ {t} = [ 1 ] \mathbf {X} _ {t} + 0 \\ y _ {t} = \phi y _ {t - 1} + \varepsilon_ {t} \\ \end{array}
$$

In the AR(1) model the state vector consists of previous consecutive observations of the time series $y _ { t }$ .

Any ARIMA model can be written in the state space form. Refer to Brockwell and Davis (1991).

Example 7.4 Now let us consider a regression model with one predictor variable and AR(1) errors. We will write this model as

$$
y _ {t} = \beta_ {0} + \beta_ {1} p _ {t} + \varepsilon_ {t}
$$

$$
\varepsilon_ {t} = \phi \varepsilon_ {t - 1} + a _ {t}
$$

where $p _ { t }$ is the predictor variable and $\varepsilon _ { t }$ is the AR(1) error term. To write this in state space form, de-ne the state vector as

$$
\mathbf {X} _ {t} = \left[ \begin{array}{c} \beta_ {0} \\ \beta_ {1} \\ p _ {t} - \varepsilon_ {t} \end{array} \right]
$$

The vector $\mathbf { h } _ { t }$ and the matrix A are

$$
\mathbf {h} _ {t} = \left[ \begin{array}{l} 1 \\ p _ {t} \\ 1 \end{array} \right], \quad \mathbf {A} = \left[ \begin{array}{l l l} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \phi \end{array} \right]
$$

and the state space representation of this model becomes

$$
y _ {t} = [ 1, p _ {t}, 1 ] \mathbf {X} _ {t} + \varepsilon_ {t}
$$

$$
\mathbf {X} _ {t} = \left[ \begin{array}{c c c} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \phi \end{array} \right] \left[ \begin{array}{c} \beta_ {0} \\ \beta_ {1} \\ p _ {t - 1} - \varepsilon_ {t - 1} \end{array} \right] + \left[ \begin{array}{c} 0 \\ 0 \\ \phi \varepsilon_ {t - 1} \end{array} \right]
$$

Multiplying these equations out will produce the time series regression model with one predictor and AR(1) errors.

The state space formulation does not admit any new forecasting techniques. Consequently, it does not produce better forecasts than any of the other methods. The state space approach does admit a Bayesian formulation of the problem, in which the model parameters have a prior distribution that represents our degree of belief concerning the values of these coef-cients. Then after some history of the process (observation) becomes available, this prior distribution is updated into a posterior distribution. Another formulation allows the coef-cients in the regression model to vary through time.

The state space formulation does allow a common mathematical framework to be used for model development. It also permits relatively easy generalization of many models. This has some advantages for researchers. It also would allow common computer software to be employed for making forecasts from a variety of techniques. This could have some practical appeal to forecasters.

# 7.3 ARCH AND GARCH MODELS

In the standard regression and time series models we have covered so far, many diagnostic checks were based on the assumptions that we imposed on the errors: independent, identically distributed with zero mean, and constant variance. Our main concern has mostly been about the independence of the errors. The constant variance assumption is often taken as a given. In many practical cases and particularly in -nance, it is fairly common to observe the violation of this assumption. Figure 7.6, for example, shows the S&P500 Index (weekly close) from 1995 to 1998. Most of the 1990s enjoyed a bull market up until toward the end when the dotcom bubble burst. The worrisome market resulted in high volatility (i.e., increasing variance). A linear trend model, an exponential smoother, or even an ARIMA model would have failed to capture this phenomenon, as all assume constant variance of the errors. This will in turn result in

![](images/1bc9c65c9d30d8b81294ce7a1ddee8f357d1f25c3ba518cfa3e638e1d760ad0d.jpg)  
FIGURE 7.6 Time series plot of S&P500 Index weekly close from 1995 to 1998.

the underestimation of the standard errors calculated using OLS and will lead to erroneous conclusions. There are different ways of dealing with this situation. For example, if the changes in the variance at certain time intervals are known, weighted regression can be employed. However, it is often the case that these changes are unknown to the analyst. Moreover, it is usually of great value to the analyst to know why, when, and how these changes in the variance occur. Hence, if possible, modeling these changes (i.e., the variance) can be quite bene-cial.

Consider, for example, the simple $\operatorname { A R } ( p )$ model from Chapter 5 given as

$$
y _ {t} = \delta + \phi_ {1} y _ {t - 1} + \phi_ {2} y _ {t - 2} + \dots + \phi_ {p} y _ {t - p} + e _ {t} \tag {7.17}
$$

where $e _ { t }$ is the uncorrelated, zero mean noise with changing variance. Please note that we used $e _ { t }$ to distinguish it from our general white noise error $\varepsilon _ { t }$ . Since we let the variance of $e _ { t }$ change in time, one approach is to model $e _ { t } ^ { 2 }$ as an $\mathrm { A R } ( l )$ model as

$$
e _ {t} ^ {2} = \xi_ {0} + \xi_ {1} e _ {t - 1} ^ {2} + \xi_ {2} e _ {t - 2} ^ {2} + \dots + \xi_ {l} e _ {t - l} ^ {2} + a _ {t} \tag {7.18}
$$

where $a _ { t }$ is a white noise sequence with zero mean and constant variance $\sigma _ { a } ^ { 2 }$ . In this notation $e _ { t }$ is said to follow an autoregressive conditional heteroskedastic process of order l, ARCH(l).

To check for a need for an ARCH model, once the ARIMA or regression model is -tted, not only the standard residual analysis and diagnostics checks have to be performed but also some serial dependence checks for $e _ { t } ^ { 2 }$ should be made.

To further generalize the ARCH model, we will consider the alternative representation originally proposed by Engle (1982). Let us assume that the error can be represented as

$$
e _ {t} = \sqrt {v _ {t}} w _ {t} \tag {7.19}
$$

where $w _ { t }$ is independent and identically distributed with mean 0 and variance 1, and

$$
v _ {t} = \zeta_ {0} + \zeta_ {1} e _ {t - 1} ^ {2} + \zeta_ {2} e _ {t - 2} ^ {2} + \dots + \zeta e _ {t - l} ^ {2} \tag {7.20}
$$

Hence the conditional variance of $e _ { t }$ is

$$
\begin{array}{l} \operatorname {V a r} \left(e _ {t} \mid e _ {t - 1}, \dots\right) = E \left(e _ {t} ^ {2} \mid e _ {t - 1} ^ {2}, \dots\right) \\ = v _ {t} \\ = \zeta_ {0} + \zeta_ {1} e _ {t - 1} ^ {2} + \zeta_ {2} e _ {t - 2} ^ {2} + \dots + l e _ {t - l} ^ {2} \tag {7.21} \\ \end{array}
$$

We can also argue that the current conditional variance should also depend on the previous conditional variances as

$$
v _ {t} = \zeta_ {0} + \varsigma_ {1} v _ {t - 1} + \varsigma_ {2} v _ {t - 2} + \dots + \varsigma_ {k} v _ {t - k} + \zeta_ {1} e _ {t - 1} ^ {2} + \zeta_ {2} e _ {t - 2} ^ {2} + \dots + \zeta_ {l} e _ {t - l} ^ {2} \tag {7.22}
$$

In this notation, the error term $e _ { t }$ is said to follow a generalized autoregressive conditional heteroskedastic process of orders $k$ and l, $\mathrm { G A R C H } ( k , l )$ , proposed by Bollerslev (1986). In Eq. (7.22) the model for conditional variance resembles an ARMA model. However, it should be noted that the model in Eq. (7.22) is not a proper ARMA model, as this would have required a white noise error term with a constant variance for the MA part. But none of the terms on the right-hand side of the equation possess this property. For further details, see Hamilton (1994), Bollerslev et al. (1992), and Degiannakis and Xekalaki (2004). Further extensions of ARCH models also exist for various speci-cations of $\nu _ { t }$ in Eq. (7.22); for example, Integrated GARCH (I-GARCH) by Engle and Bollerslev (1986), Exponential GARCH (E-GARCH) by Nelson (1991), Nonlinear GARCH by Glosten et al. (1993), and GARCH for multivariate data by Engle and Kroner (1993). But they are beyond the scope of this book. For a brief overview of these models, see Hamilton (1994).

Example 7.5 Consider the weekly closing values for the S&P500 Index from 1995 to 1998 given in Table 7.4. Figure 7.6 shows that the data exhibits nonstationarity. But before taking the -rst difference of the data, we decided to take the log transformation of the data -rst. As observed in Chapters 2 and 3, the log transformation is sometimes used for -nancial data when we are interested, for example, in the rate of change or percentage changes in the price of a stock. For further details, see Granger and Newbold (1986). The time series plot of the -rst differences of the log of the S&P500 Index is given in Figure 7.7, which shows that while the mean seems to be stable around 0, the changes in the variance are worrisome. The ACF and PACF plots of the -rst difference given in Figure 7.8 suggest that, except for some borderline signi-cant ACF values at seemingly arbitrary lags, there is no autocorrelation left in the data. As in the case of the Dow Jones Index in Chapter 5, this suggests that the S&P500 Index follows a random walk process. However, the time series plot of the differences does not exhibit a constant variance behavior. For that, we consider the ACF and PACF of the squared differences given in Figure 7.9, which suggests that an AR(3) model can be used. Thus we -t the ARCH(3) model for the variance using the AUTOREG procedure in SAS given in Table 7.5. The SAS output in

TABLE 7.4 Weekly Closing Values for the S&P500 Index from 1995 to 1998   

<table><tr><td>Date</td><td>Close</td><td>Date</td><td>Close</td><td>Date</td><td>Close</td><td>Date</td><td>Close</td><td>Date</td><td>Close</td></tr><tr><td>1/3/1995</td><td>460.68</td><td>8/14/1995</td><td>559.21</td><td>3/25/1996</td><td>645.5</td><td>11/4/1996</td><td>730.82</td><td>6/16/1997</td><td>898.7</td></tr><tr><td>1/9/1995</td><td>465.97</td><td>8/21/1995</td><td>560.1</td><td>4/1/1996</td><td>655.86</td><td>11/11/1996</td><td>737.62</td><td>6/23/1997</td><td>887.3</td></tr><tr><td>1/16/1995</td><td>464.78</td><td>8/28/1995</td><td>563.84</td><td>4/8/1996</td><td>636.71</td><td>11/18/1996</td><td>748.73</td><td>6/30/1997</td><td>916.92</td></tr><tr><td>1/23/1995</td><td>470.39</td><td>9/5/1995</td><td>572.68</td><td>4/15/1996</td><td>645.07</td><td>11/25/1996</td><td>757.02</td><td>7/7/1997</td><td>916.68</td></tr><tr><td>1/30/1995</td><td>478.65</td><td>9/11/1995</td><td>583.35</td><td>4/22/1996</td><td>653.46</td><td>12/2/1996</td><td>739.6</td><td>7/14/1997</td><td>915.3</td></tr><tr><td>2/6/1995</td><td>481.46</td><td>9/18/1995</td><td>581.73</td><td>4/29/1996</td><td>641.63</td><td>12/9/1996</td><td>728.64</td><td>7/21/1997</td><td>938.79</td></tr><tr><td>2/13/1995</td><td>481.97</td><td>9/25/1995</td><td>584.41</td><td>5/6/1996</td><td>652.09</td><td>12/16/1996</td><td>748.87</td><td>7/28/1997</td><td>947.14</td></tr><tr><td>2/21/1995</td><td>488.11</td><td>10/2/1995</td><td>582.49</td><td>5/13/1996</td><td>668.91</td><td>12/23/1996</td><td>756.79</td><td>8/4/1997</td><td>933.54</td></tr><tr><td>2/27/1995</td><td>485.42</td><td>10/9/1995</td><td>584.5</td><td>5/20/1996</td><td>678.51</td><td>12/30/1996</td><td>748.03</td><td>8/11/1997</td><td>900.81</td></tr><tr><td>3/6/1995</td><td>489.57</td><td>10/16/1995</td><td>587.46</td><td>5/28/1996</td><td>669.12</td><td>1/6/1997</td><td>759.5</td><td>8/18/1997</td><td>923.54</td></tr><tr><td>3/13/1995</td><td>495.52</td><td>10/23/1995</td><td>579.7</td><td>6/3/1996</td><td>673.31</td><td>1/13/1997</td><td>776.17</td><td>8/25/1997</td><td>899.47</td></tr><tr><td>3/20/1995</td><td>500.97</td><td>10/30/1995</td><td>590.57</td><td>6/10/1996</td><td>665.85</td><td>1/20/1997</td><td>770.52</td><td>9/2/1997</td><td>929.05</td></tr><tr><td>3/27/1995</td><td>500.71</td><td>11/6/1995</td><td>592.72</td><td>6/17/1996</td><td>666.84</td><td>1/27/1997</td><td>786.16</td><td>9/8/1997</td><td>923.91</td></tr><tr><td>4/3/1995</td><td>506.42</td><td>11/13/1995</td><td>600.07</td><td>6/24/1996</td><td>670.63</td><td>2/3/1997</td><td>789.56</td><td>9/15/1997</td><td>950.51</td></tr><tr><td>4/10/1995</td><td>509.23</td><td>11/20/1995</td><td>599.97</td><td>7/1/1996</td><td>657.44</td><td>2/10/1997</td><td>808.48</td><td>9/22/1997</td><td>945.22</td></tr><tr><td>4/17/1995</td><td>508.49</td><td>11/27/1995</td><td>606.98</td><td>7/8/1996</td><td>646.19</td><td>2/18/1997</td><td>801.77</td><td>9/29/1997</td><td>965.03</td></tr><tr><td>4/24/1995</td><td>514.71</td><td>12/4/1995</td><td>617.48</td><td>7/15/1996</td><td>638.73</td><td>2/24/1997</td><td>790.82</td><td>10/6/1997</td><td>966.98</td></tr><tr><td>5/1/1995</td><td>520.12</td><td>12/11/1995</td><td>616.34</td><td>7/22/1996</td><td>635.9</td><td>3/3/1997</td><td>804.97</td><td>10/13/1997</td><td>944.16</td></tr><tr><td>5/8/1995</td><td>525.55</td><td>12/18/1995</td><td>611.95</td><td>7/29/1996</td><td>662.49</td><td>3/10/1997</td><td>793.17</td><td>10/20/1997</td><td>941.64</td></tr><tr><td>5/15/1995</td><td>519.19</td><td>12/26/1995</td><td>615.93</td><td>8/5/1996</td><td>662.1</td><td>3/17/1997</td><td>784.1</td><td>10/27/1997</td><td>914.62</td></tr><tr><td>5/22/1995</td><td>523.65</td><td>1/2/1996</td><td>616.71</td><td>8/12/1996</td><td>665.21</td><td>3/24/1997</td><td>773.88</td><td>11/3/1997</td><td>927.51</td></tr><tr><td>5/30/1995</td><td>532.51</td><td>1/8/1996</td><td>601.81</td><td>8/19/1996</td><td>667.03</td><td>3/31/1997</td><td>757.9</td><td>11/10/1997</td><td>928.35</td></tr><tr><td>6/5/1995</td><td>527.94</td><td>1/15/1996</td><td>611.83</td><td>8/26/1996</td><td>651.99</td><td>4/7/1997</td><td>737.65</td><td>11/17/1997</td><td>963.09</td></tr><tr><td>6/12/1995</td><td>539.83</td><td>1/22/1996</td><td>621.62</td><td>9/3/1996</td><td>655.68</td><td>4/14/1997</td><td>766.34</td><td>11/24/1997</td><td>955.4</td></tr><tr><td>6/19/1995</td><td>549.71</td><td>1/29/1996</td><td>635.84</td><td>9/9/1996</td><td>680.54</td><td>4/21/1997</td><td>765.37</td><td>12/1/1997</td><td>983.79</td></tr><tr><td>6/26/1995</td><td>544.75</td><td>2/5/1996</td><td>656.37</td><td>9/16/1996</td><td>687.03</td><td>4/28/1997</td><td>812.97</td><td>12/8/1997</td><td>953.39</td></tr><tr><td>7/3/1995</td><td>556.37</td><td>2/12/1996</td><td>647.98</td><td>9/23/1996</td><td>686.19</td><td>5/5/1997</td><td>824.78</td><td>12/15/1997</td><td>946.78</td></tr><tr><td>7/10/1995</td><td>559.89</td><td>2/20/1996</td><td>659.08</td><td>9/30/1996</td><td>701.46</td><td>5/12/1997</td><td>829.75</td><td>12/22/1997</td><td>936.46</td></tr><tr><td>7/17/1995</td><td>553.62</td><td>2/26/1996</td><td>644.37</td><td>10/7/1996</td><td>700.66</td><td>5/19/1997</td><td>847.03</td><td>12/29/1997</td><td>975.04</td></tr><tr><td>7/24/1995</td><td>562.93</td><td>3/4/1996</td><td>633.5</td><td>10/14/1996</td><td>710.82</td><td>5/27/1997</td><td>848.28</td><td></td><td></td></tr><tr><td>7/31/1995</td><td>558.94</td><td>3/11/1996</td><td>641.43</td><td>10/21/1996</td><td>700.92</td><td>6/2/1997</td><td>858.01</td><td></td><td></td></tr><tr><td>8/7/1995</td><td>555.11</td><td>3/18/1996</td><td>650.62</td><td>10/28/1996</td><td>703.77</td><td>6/9/1997</td><td>893.27</td><td></td><td></td></tr></table>

![](images/28361f52d2f693b8c8b3084ec59673f684940fd856903a6fe1551813be45070c.jpg)  
FIGURE 7.7 Time series plot of the -rst difference of the log transformation of the weekly close for S&P500 Index from 1995 to 1998.

![](images/f658596a0989cce5350e5367fddead8bbe5bba056ee3a4fc751f8059939609d5.jpg)

![](images/b8b54cf59b575ca94f96be0cee0bc7afcd04eadad5fcbb6a8f91413e0b859f14.jpg)  
FIGURE 7.8 ACF and PACF plots of the -rst difference of the log transformation of the weekly close for the S&P500 Index from 1995 to 1998.

![](images/5f54ed641068378054c6a82368d11558d7eb3d57509a28d247d00a21d3ceb346.jpg)

![](images/5feec8cca77f98c274817733ae89e801a5fda7b249f850d4748800f7e5e8461e.jpg)  
FIGURE 7.9 ACF and PACF plots of the square of the -rst difference of the log transformation of the weekly close for S&P500 Index from 1995 to 1998.

# TABLE 7.5 SAS Commands to Fit the ARCH(3) Modela

proc autoreg data $\equiv$ sp5003; model dlogc $= /$ garch $\equiv$ (q=3); run;

a dlogc is the -rst difference of the log transformed data.

TABLE 7.6 SAS output for the ARCH(3) model   

<table><tr><td colspan="6">GARCH Estimates</td></tr><tr><td>SSE</td><td>0.04463228</td><td>Observations</td><td>156</td><td></td><td></td></tr><tr><td>MSE</td><td>0.0002861</td><td>Uncond Var</td><td>0.00030888</td><td></td><td></td></tr><tr><td>Log Likelihood</td><td>422.53308</td><td>Total R-Square</td><td>.</td><td></td><td></td></tr><tr><td>SBC</td><td>-824.86674</td><td>AIC</td><td>-837.06616</td><td></td><td></td></tr><tr><td>Normality Test</td><td>1.6976</td><td>Pr &gt; ChiSq</td><td>0.4279</td><td></td><td></td></tr><tr><td colspan="6">The AUTOREG Procedure</td></tr><tr><td>Variable</td><td>DF</td><td>Standard Estimate</td><td>Approx Error</td><td>t Value</td><td>Pr &gt; |t|</td></tr><tr><td>Intercept</td><td>1</td><td>0.004342</td><td>0.001254</td><td>3.46</td><td>0.0005</td></tr><tr><td>ARCH0</td><td>1</td><td>0.000132</td><td>0.0000385</td><td>3.42</td><td>0.0006</td></tr><tr><td>ARCH1</td><td>1</td><td>4.595E-10</td><td>3.849E-11</td><td>11.94</td><td>&lt;.0001</td></tr><tr><td>ARCH2</td><td>1</td><td>0.2377</td><td>0.1485</td><td>1.60</td><td>0.1096</td></tr><tr><td>ARCH3</td><td>1</td><td>0.3361</td><td>0.1684</td><td>2.00</td><td>0.0460</td></tr></table>

Table 7.6 gives the coef-cient estimates for the ARCH(3) model for the variance.

There are other studies on -nancial indices also yielding the ARCH(3) model for the variance, for example, Bodurtha and Mark (1991) and Attanasio (1991). In fact, successful implementations of reasonably simple, low-order ARCH/GARCH models have been reported in various research studies; see, for example, French et al. (1987).

# 7.4 DIRECT FORECASTING OF PERCENTILES

Throughout this book we have stressed the concept that a forecast should almost always be more than a point estimate of the value of some future event. A prediction interval should accompany most point forecasts, because the PI will give the decision maker some idea about the inherent

variability of the forecast and the likely forecast error that could be experienced. Most of the forecasting techniques in this book have been presented showing how both point forecasts and PIs are obtained.

A PI can be thought of as an estimate of the percentiles of the distribution of the forecast variable. Typically, a PI is obtained by forecasting the mean and then adding appropriate multiples of the standard deviation of forecast error to the estimate of the mean. In this section we present and illustrate a different method that directly smoothes the percentiles of the distribution of the forecast variable.

Suppose that the forecast variable $y _ { t }$ has a probability distribution $f ( y )$ . We will assume that the variable $y _ { t }$ is either stationary or is changing slowly with time. Therefore a model for $y _ { t }$ that is correct at least locally is

$$
y _ {t} = \mu + \varepsilon_ {t}
$$

Let the observations on $y _ { t }$ be classi-ed into a -nite number of bins, where the bins are de-ned with limits

$$
B _ {0} <   B _ {1} <   \dots <   B _ {n}
$$

The $n$ bins should be de-ned so that they do not overlap; that is, each observation can be classi-ed into one and only one bin. The bins do not have to be of equal width. In fact, there may be situations where bins may be de-ned with unequal width to obtain more information about speci-c percentiles that are of interest. Typically, $1 0 \leq n \leq 2 0$ bins are used.

Let $p _ { k }$ be the probability that the variable $y _ { t }$ falls in the bin de-ned by the limits $B _ { k - 1 }$ and $B _ { k }$ . That is,

$$
p _ {k} = P \left(B _ {k - 1} <   y _ {t} \leq B _ {k}\right), \quad k = 1, 2, \dots , n
$$

Assume that $\begin{array} { r } { \sum _ { k = 1 } ^ { n } p _ { k } = 1 } \end{array}$ . Also, note that $\begin{array} { r } { P ( \boldsymbol { y } _ { t } \leq B _ { k } ) = \sum _ { j = 1 } ^ { k } p _ { j } } \end{array}$ . N $n \times 1$ vector p de-ned as

$$
\mathbf {p} = \left[ \begin{array}{c} p _ {1} \\ p _ {2} \\ \vdots \\ p _ {n} \end{array} \right]
$$

Let the estimate of the vector $\mathbf { p }$ at time period $T$ be

$$
\hat {\mathbf {p}} (T) = \left[ \begin{array}{c} \hat {p} _ {1} (T) \\ \hat {p} _ {2} (T) \\ \vdots \\ \hat {p} _ {n} (T) \end{array} \right]
$$

Note that if we wanted to estimate the percentile of the distribution of $y _ { t }$ corresponding to $B _ { k }$ at time period $T$ we could do this by calculating $\textstyle \sum _ { j = 1 } ^ { k } { \hat { p } } _ { j } ( T )$ .

We will use an exponential smoothing procedure to compute the estimated probabilities in the vector ${ \hat { \mathbf { p } } } ( T )$ . Suppose that we are at the end of time period $t$ and the current observation $y _ { T }$ is known. Let $u _ { k } ( T )$ be an indicator variable de-ned as follows:

$$
u _ {k} (T) = \left\{ \begin{array}{l l} 1 & \text {i f} B _ {k - 1} <   y _ {T} \leq B _ {k} \\ 0 & \text {o t h e r w i s e} \end{array} \right.
$$

So the indicator variable $u _ { k } ( T )$ is equal to unity if the observation $y _ { T }$ in period $T$ falls in the kth bin. Note that $\textstyle \sum _ { t = 1 } ^ { T } u _ { k } ( t )$ is the total number of observations that fell in the kth bin during the time periods $t = 1 , 2 , \dots , T$ . De-ne the $n \times 1$ observation vector $\mathbf { u } ( T )$ as

$$
\mathbf {u} (T) = \left[ \begin{array}{c} u _ {1} (T) \\ u _ {2} (T) \\ \vdots \\ u _ {n} (T) \end{array} \right]
$$

This vector will have $n - 1$ elements equal to zero and one element equal to unity. The exponential smoothing procedure for revising the probabilities $\hat { p } _ { k } ( T - 1 )$ given that we have a new observation $y _ { T }$ is

$$
\hat {p} _ {k} (T) = \lambda u _ {k} (T) + (1 - \lambda) \hat {p} _ {k} (T - 1), \quad k = 1, 2, \dots , n \tag {7.23}
$$

where $0 < \lambda < 1$ is the smoothing constant. In vector form, Eq. (7.23) for updating the probabilities is

$$
\hat {\mathbf {p}} _ {k} (T) = \lambda \mathbf {u} _ {k} (T) + (1 - \lambda) \hat {\mathbf {p}} _ {k} (T - 1)
$$

This smoothing procedure produces an unbiased estimate of the probabilities $p _ { k }$ . Furthermore, because $u _ { k } ( T )$ is a Bernoulli random variable with parameter $p _ { k }$ , the variance of $\hat { p } _ { k } ( T )$ is

$$
V [ \hat {p} _ {k} (T) ] = \frac {\lambda}{2 - \lambda} p _ {k} (1 - p _ {k})
$$

Starting estimates or initial values of the probabilities at time $T = 0$ are required. These starting values $\hat { p } _ { k } ^ { \phantom { } } ( 0 )$ , k = 1, 2, 鈥?, n could be subjective estimates or they could be obtained from an analysis of historical data.

The estimated probabilities can be used to obtain estimates of speci-c percentiles of the distribution of the variable $y _ { t }$ . One way to do this would be to estimate the cumulative probability distribution of $y _ { t }$ at time $T$ as follows:

$$
F (y) = \left\{ \begin{array}{l l} 0, & \text {i f} y \leq B _ {0} \\ \sum_ {j = 1} ^ {k} \hat {p} _ {j} (T), & \text {i f} y = B _ {k}, k = 1, 2, \ldots , n \\ 1, & \text {i f} y \geq B _ {n} \end{array} \right.
$$

The values of the cumulative distribution could be plotted on a graph with $F ( y )$ on the vertical axis and $y$ on the horizontal axis and the points connected by a smooth curve. Then to obtain an estimate of any speci-c percentile, say, $\hat { F } _ { 1 - \gamma } = 1 - \gamma$ , all you would need to do is determine the value of $y$ on the horizontal axis corresponding to the desired percentile $1 - \gamma$ on the vertical axis. For example, to -nd the 95th percentile of the distribution of $y$ , -nd the value of y on the horizontal axis that corresponds to 0.95 on the vertical axis. This can also be done mathematically. If the desired percentile $1 - \gamma$ exactly matches one of the bin limits so that $F ( B _ { k } ) = 1 - \gamma$ , then the solution is easy and the desired percentile estimate is $\hat { F } _ { 1 - \gamma } = B _ { k }$ . However, if the desired percentile $1 - \gamma$ is between two of the bin limits, say, $F ( B _ { k - 1 } ) < 1 - \gamma < F ( B _ { k } )$ , then interpolation is required. A linear interpolation formula is

$$
\hat {F} _ {1 - \gamma} = \frac {[ F (B _ {k}) - (1 - \gamma) ] B _ {k - 1} + [ (1 - \gamma) - F (B _ {k - 1}) ] B _ {k}}{F (B _ {k}) - F (B _ {k - 1})} \quad (7. 2 4)
$$

In the extreme tails of the distribution or in cases where the bins are very wide, it may be desirable to use a nonlinear interpolation scheme.

Example 7.6 A -nancial institution is interested in forecasting the number of new automobile loan applications generated each week by a particular business channel. The information in Table 7.7 is known at the end of week $T - 1$ . The next-to-last column of this table is the cumulative distribution of loan applications at the end of week $T - 1$ . This cumulative distribution is shown in Figure 7.10.

Suppose that 74 loan applications are received during the current week, T. This number of loan applications fall into the eighth bin $( k = 7$ in Table 7.7). Therefore we can construct the observation vector $\mathbf { u } ( T )$

as follows:

$$
\mathbf {u} (T) = \left[ \begin{array}{l} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \end{array} \right]
$$

TABLE 7.7 Distribution of New Automobile Loan Applications   

<table><tr><td>k</td><td>Bk-1</td><td>Bk</td><td>藛pk(T-1)</td><td>F(Bk), at the end of week T-1</td><td>F(Bk), at the end of week T-1</td></tr><tr><td>0</td><td>0</td><td>10</td><td>0.02</td><td>0.02</td><td>0.018</td></tr><tr><td>1</td><td>10</td><td>20</td><td>0.04</td><td>0.06</td><td>0.054</td></tr><tr><td>2</td><td>20</td><td>30</td><td>0.05</td><td>0.11</td><td>0.099</td></tr><tr><td>3</td><td>30</td><td>40</td><td>0.05</td><td>0.16</td><td>0.144</td></tr><tr><td>4</td><td>40</td><td>50</td><td>0.09</td><td>0.25</td><td>0.225</td></tr><tr><td>5</td><td>50</td><td>60</td><td>0.10</td><td>0.35</td><td>0.315</td></tr><tr><td>6</td><td>60</td><td>70</td><td>0.13</td><td>0.48</td><td>0.432</td></tr><tr><td>7</td><td>70</td><td>80</td><td>0.16</td><td>0.64</td><td>0.676</td></tr><tr><td>8</td><td>80</td><td>90</td><td>0.20</td><td>0.84</td><td>0.856</td></tr><tr><td>9</td><td>90</td><td>100</td><td>0.10</td><td>0.94</td><td>0.946</td></tr><tr><td>10</td><td>100</td><td>110</td><td>0.06</td><td>1.00</td><td>1.000</td></tr></table>

![](images/608bde4f1fe79e90b58a09465874651cd999f99a7315838417404b421f3180c8.jpg)  
FIGURE 7.10 Cumulative distribution of the number of loan applications, week $T - 1$ .

Equation (7.23) is now used with $\lambda = 0 . 1 0$ to update the probabilities:

$$
\begin{array}{l} \hat {\mathbf {p}} _ {k} (T) = \lambda \mathbf {u} _ {k} (T) + (1 - \lambda) \hat {\mathbf {p}} _ {k} (T - 1) \\ = 0. 1 \left[ \begin{array}{l} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \end{array} \right] + 0. 9 \left[ \begin{array}{l} 0. 0 2 \\ 0. 0 4 \\ 0. 0 5 \\ 0. 0 5 \\ 0. 0 9 \\ 0. 1 0 \\ 0. 1 3 \\ 0. 1 6 \\ 0. 2 0 \\ 0. 1 0 \\ 0. 0 6 \end{array} \right] = \left[ \begin{array}{l} 0. 0 1 8 \\ 0. 0 3 6 \\ 0. 0 4 5 \\ 0. 0 4 5 \\ 0. 0 8 1 \\ 0. 0 9 0 \\ 0. 1 1 7 \\ 0. 2 4 4 \\ 0. 1 8 0 \\ 0. 0 9 0 \\ 0. 0 5 4 \end{array} \right] \\ \end{array}
$$

Therefore the new cumulative distribution of loan applications is found by summing the cumulative probabilities in $\hat { \mathbf p } _ { k } ( T - 1 )$ :

$$
F (B _ {k}) = \left[ \begin{array}{l} 0. 0 1 8 \\ 0. 0 5 4 \\ 0. 0 9 9 \\ 0. 1 4 4 \\ 0. 2 2 5 \\ 0. 3 1 5 \\ 0. 4 3 2 \\ 0. 6 7 6 \\ 0. 8 5 6 \\ 0. 9 4 6 \\ 1. 0 0 0 \end{array} \right]
$$

These cumulative probabilities are also listed in the last column of Table 7.7. The graph of the updated cumulative distribution is shown in Figure 7.11.

Now suppose that we want to -nd the number of loan applications that corresponds to a particular percentile of this distribution. If this percentile corresponds exactly to one of the cumulative probabilities, such as the 67.6 th percentile, the problem is easy. From the last column of Table 7.7 we would -nd that

$$
\hat {F} _ {0. 6 7 6} = 8 0
$$

That is, in about two of every three weeks we would expect to have 80 or fewer loan applications from this particular channel. However, if the desired

![](images/f8197b5704ee4657b2a4bfdebc73589601b9ce882c54a15373bbb0fac5726482.jpg)  
FIGURE 7.11 Cumulative distribution of the number of loan applications, week T.

percentile does not correspond to one of the cumulative probabilities in the last column of Table 7.7, we will need to interpolate using Eq. (7.24). For instance, if we want the 75th percentile, we would use Eq. (7.24) as follows:

$$
\begin{array}{l} \hat {F} _ {0. 7 5} = \frac {[ F (B _ {k}) - (0 . 7 5) ] B _ {k - 1} + [ (0 . 7 5) - [ F (B _ {k - 1}) ] B _ {k}}{F (B _ {k}) - F (B _ {k - 1})} \\ = \frac {(0 . 8 5 6 - 0 . 7 5) 9 0 + (0 . 7 5 - 0 . 6 7 6) 8 0}{0 . 8 5 6 - 0 . 6 7 6} \\ = 8 5. 8 9 \approx 8 6 \\ \end{array}
$$

Therefore, in about three of every four weeks, we would expect to have approximately 86 or fewer loan applications from this loan channel.

# 7.5 COMBINING FORECASTS TO IMPROVE PREDICTION PERFORMANCE

Readers have been sure to notice that any time series can be modeled and forecast using several methods. For example, it is not at all unusual to -nd that the time series $y _ { t }$ , $t = 1 , 2 , \ldots$ , which contains a trend (say), can be forecast by both an exponential smoothing approach and an ARIMA model. In such situations, it seems inef-cient to use one forecast and ignore all of the information in the other. It turns out that the forecasts from the two methods can be combined to produce a forecast that is superior in

terms of forecast error than either forecast alone. For a review paper on the combination of forecasts, see Clemen (1989).

Bates and Granger (1969) suggested using a linear combination of the two forecasts. Let $\hat { y } _ { 1 , T + \tau } ( T )$ and $\hat { y } _ { 2 , T + \tau } ( T )$ be the forecasts from two different methods at the end of time period $T$ for some future period $T + \tau$ for the time series $y _ { t }$ . The combined forecast is

$$
\hat {y} _ {T + \tau} ^ {\mathrm {c}} = k _ {1} \hat {y} _ {1, T + \tau} (T) + k _ {2} \hat {y} _ {2, T + \tau} (T) \tag {7.25}
$$

where $k _ { 1 }$ and $k _ { 2 }$ are weights. If these weights are chosen properly, the combined forecast $\hat { y } _ { T + \tau } ^ { \mathrm { c } }$ can have some nice properties. Let the two individual forecasts be unbiased. Then we should choose $k _ { 2 } = 1 - k _ { 1 }$ so that the combined forecast will also be unbiased. Let $k = k _ { 1 }$ so that the combined forecast is

$$
\hat {y} _ {T + \tau} ^ {\mathrm {c}} = k \hat {y} _ {1, T + \tau} (T) + (1 - k) \hat {y} _ {2, T + \tau} (T) \tag {7.26}
$$

Let the error from the combined forecast be $e _ { T + \tau } ^ { \mathrm { c } } ( T ) = y _ { T + \tau } - \hat { y } _ { T + \tau } ^ { \mathrm { c } } ( T )$ . The variance of this forecast error is

$$
\begin{array}{l} \operatorname {V a r} \left[ e _ {T + \tau} ^ {\mathrm {c}} (T) \right] = \operatorname {V a r} \left[ y _ {T + \tau} - \hat {y} _ {T + \tau} ^ {\mathrm {c}} (T) \right] \\ = \operatorname {V a r} [ k e _ {1, T + \tau} (T) + (1 - k) e _ {2, T + \tau} (T) ] \\ = k ^ {2} \sigma_ {1} ^ {2} + (1 - k) ^ {2} \sigma_ {2} ^ {2} + 2 k (1 - k) \rho \sigma_ {1} \sigma_ {2} \\ \end{array}
$$

where $e _ { 1 , T + \tau } ( T )$ and $e _ { 2 s , T + \tau } ( T )$ are the forecast errors in period $T + \tau$ for the two individual forecasting methods, $\sigma _ { 1 } ^ { 2 }$ and $\sigma _ { 2 } ^ { 2 }$ are the variances of the individual forecast errors for the two forecasting methods, and $\rho$ is the correlation between the two individual forecast errors. A good combined forecast would be one that minimizes the variance of the combined forecast error. If we choose the weight $k$ equal to

$$
k ^ {*} = \frac {\sigma_ {2} ^ {2} - \rho \sigma_ {1} \sigma_ {2}}{\sigma_ {1} ^ {2} + \sigma_ {2} ^ {2} - 2 \rho \sigma_ {1} \sigma_ {2}} \tag {7.27}
$$

this will minimize the variance of the combined forecast error. By choosing this value for the weight, the minimum variance of the combined forecast error is equal to

$$
\operatorname {M i n} \operatorname {V a r} \left[ e _ {T + \tau} ^ {\mathrm {c}} (T) \right] = \frac {\sigma_ {1} ^ {2} \sigma_ {2} ^ {2} \left(1 - \rho^ {2}\right)}{\sigma_ {1} ^ {2} + \sigma_ {2} ^ {2} - 2 \rho \sigma_ {1} \sigma_ {2}} \tag {7.28}
$$

and this minimum variance of the combined forecast error is less than or equal to the minimum of the variance of the forecast errors of the two

individual forecasting methods. That is,

$$
\operatorname {M i n} \operatorname {V a r} \left[ e _ {T + \tau} ^ {\mathrm {c}} (T) \right] \leq \min  \left(\sigma_ {1} ^ {2}, \sigma_ {2} ^ {2}\right)
$$

It turns out that the variance of the combined forecast error depends on the correlation coef-cient. Let $\sigma _ { 1 } ^ { 2 }$ be the smaller of the two individual forecast error variances. Then we have the following:

1. If $\rho = \sigma _ { 1 } / \sigma _ { 2 }$ , then Min Var $[ e _ { T + \tau } ^ { \mathrm { c } } ( T ) ] = \sigma _ { 1 } ^ { 2 }$ .   
2. If $\rho = 0$ , then Var $[ e _ { T + \tau } ^ { \mathrm { c } } ( T ) ] = \sigma _ { 1 } ^ { 2 } \sigma _ { 2 } ^ { 2 } / ( \sigma _ { 1 } ^ { 2 } + \sigma _ { 2 } ^ { 2 } )$   
3. If $\rho  - 1$ , then Var $[ e _ { T + \tau } ^ { \mathrm { c } } ( T ) ]  0$   
4. If $\rho  1$ , then Var $[ e _ { T + \tau } ^ { \mathrm { c } } ( T ) ]  0$ if $\sigma _ { 1 } ^ { 2 } \neq \sigma _ { 2 } ^ { 2 }$ .

Clearly, we would be happiest if the two forecasting methods have forecast errors with large negative correlation. The best possible case is when the two individual forecasting methods produce forecast errors that are perfectly negatively correlated. However, even if the two individual forecasting methods have forecast errors that are positively correlated, the combined forecast will still be superior to the individual forecasts, provided that $\rho \neq \sigma _ { 1 } / \sigma _ { 2 }$ .

Example 7.7 Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are $\sigma _ { 1 } ^ { 2 } = 2 0$ and $\sigma _ { 2 } ^ { 2 } = 4 0$ . If the correlation coef-cient $\rho = - 0 . 6$ , then we can calculate the optimum value of the weight from Eq. (7.27) as follows:

$$
\begin{array}{l} k ^ {*} = \frac {\sigma_ {2} ^ {2} - \rho \sigma_ {1} \sigma_ {2}}{\sigma_ {1} ^ {2} + \sigma_ {2} ^ {2} - 2 \rho \sigma_ {1} \sigma_ {2}} \\ = \frac {4 0 - (- 0 . 6) \sqrt {(4 0) (2 0)}}{4 0 + 2 0 - 2 (- 0 . 6) \sqrt {(4 0) (2 0)}} \\ = \frac {5 6 . 9 7 0 6}{9 3 . 9 4 1 1} \\ = 0. 6 0 6 5 \\ \end{array}
$$

So the combined forecasting equation is

$$
\hat {y} _ {T + \tau} ^ {\mathrm {c}} = 0. 6 0 6 5 \hat {y} _ {1, T + \tau} (T) + 0. 3 9 3 5 \hat {y} _ {2, T + \tau} (T)
$$

Forecasting method one, which has the smallest individual forecast error variance, receives about 1.5 times the weight of forecasting method two.

The variance of the forecast error for the combined forecast is computed from Eq. (7.28):

$$
\begin{array}{l} \operatorname {M i n} \operatorname {V a r} \left[ e _ {T + \tau} ^ {\mathrm {c}} (T) \right] = \frac {\sigma_ {1} ^ {2} \sigma_ {2} ^ {2} (1 - \rho^ {2})}{\sigma_ {1} ^ {2} + \sigma_ {2} ^ {2} - 2 \rho \sigma_ {1} \sigma_ {2}} \\ = \frac {(2 0) (4 0) [ 1 - (- 0 . 6) ^ {2} ]}{2 0 + 4 0 - 2 (- 0 . 6) \sqrt {(2 0) (4 0)}} \\ = \frac {5 1 2}{9 3 . 9 4 1 1} \\ = 5. 4 5 \\ \end{array}
$$

This is a considerable reduction in the variance of forecast error. If the correlation had been positive instead of negative, then

$$
\begin{array}{l} k ^ {*} = \frac {\sigma_ {2} ^ {2} - \rho \sigma_ {1} \sigma_ {2}}{\sigma_ {1} ^ {2} + \sigma_ {2} ^ {2} - 2 \rho \sigma_ {1} \sigma_ {2}} \\ = \frac {4 0 - (0 . 6) \sqrt {(4 0) (2 0)}}{4 0 + 2 0 - 2 (0 . 6) \sqrt {(4 0) (2 0)}} \\ = \frac {2 3 . 0 2 9 4}{2 6 . 0 5 8 9} \\ = 0. 8 8 3 7 \\ \end{array}
$$

Now forecasting method one, which has the smallest variance of forecast error, receives much more weight. The variance of the forecast error for the combined forecast is

$$
\begin{array}{l} \operatorname {M i n} \operatorname {V a r} \left[ e _ {T + \tau} ^ {\mathrm {c}} (T) \right] = \frac {\sigma_ {1} ^ {2} \sigma_ {2} ^ {2} (1 - \rho^ {2})}{\sigma_ {1} ^ {2} + \sigma_ {2} ^ {2} - 2 \rho \sigma_ {1} \sigma_ {2}} \\ = \frac {(2 0) (4 0) [ 1 - (0 . 6) ^ {2} ]}{2 0 + 4 0 - 2 (0 . 6) \sqrt {(2 0) (4 0)}} \\ = \frac {5 1 2}{2 6 . 0 5 8 9} \\ = 1 9. 6 4 7 8 \\ \end{array}
$$

In this situation, there is very little improvement in the forecast error resulting from the combination of forecasts.

Newbold and Granger (1974) have extended this technique to the combination of $n$ forecasts. Let $\hat { y } _ { i , T + \tau } ( T )$ , $i = 1 , 2 , \dots , n$ be the $n$ unbiased

forecasts at the end of period $T$ for some future period $T + \tau$ for the time series $y _ { t }$ . The combined forecast is

$$
\begin{array}{l} \hat {y} _ {T + \tau} ^ {\mathrm {c}} (T) = \sum_ {i = 1} ^ {n} k _ {i} \hat {y} _ {T + \tau} (T) \\ = \mathbf {k} ^ {\prime} \hat {\mathbf {y}} _ {T + \tau} (T) \\ \end{array}
$$

where $\mathbf { k } ^ { \prime } = [ k _ { 1 } , k _ { 2 } , \ldots , k _ { n } ]$ is the vector of weights, and $\hat { \mathbf { y } } _ { T + \tau } ^ { \mathrm { c } } ( T )$ is a vector of the individual forecasts. We require that all of the weights $0 \leq k _ { i } \leq 1$ and $\textstyle \sum _ { i = 1 } ^ { n } k _ { i } = 1$ . The variance of the forecast error is minimized if the weights are chosen as

$$
\mathbf {k} = \frac {\sum_ {T + \tau} ^ {- 1} (T) \mathbf {1}}{\mathbf {1} ^ {\prime} \sum_ {T + \tau} ^ {- 1} (T) \mathbf {1}}
$$

where $\Sigma _ { T + \tau } ( T )$ is the covariance matrix of the lead $\tau$ forecast errors given by

$$
\sum_ {T + \tau} (T) = E [ \mathbf {e} _ {T + \tau} (T) \mathbf {e} _ {T + \tau} ^ {\prime} (T) ]
$$

$\mathbf { 1 } ^ { \prime } = [ 1 , 1 , \dots , 1 ]$ is a vector of ones, and ${ \bf e } _ { T + \tau } ( T ) = y _ { T + \tau } { \bf 1 } - \hat { \bf y } _ { T + \tau } ( T )$ is a vector of the individual forecast errors.

The elements of the covariance matrix are usually unknown and will need to be estimated. This can be done by straightforward methods for estimating variances and covariances (refer to Chapter 2). It may also be desirable to regularly update the estimates of the covariance matrix so that these quantities reect current forecasting performance. Newbold and Granger (1974) suggested several methods for doing this, and Montgomery, Johnson, and Gardiner (1990) investigate several of these methods. They report that a smoothing approach for updating the elements of the covariance matrix seems to work well in practice.

# 7.6 AGGREGATION AND DISAGGREGATION OF FORECASTS

Suppose that you wish to forecast the unemployment level of the state in which you live. One way to do this would be to forecast this quantity directly, using the time series of current and previous unemployment data, plus any other predictors that you think are relevant. Another way to do this would be to forecast unemployment at a substate level (say, by county

and/or metropolitan area), and then to obtain the state level forecast by summing up the forecasts for each substate region. Thus individual forecasts of a collection of subseries are aggregated to form the forecasts of the quantity of interest. If the substate level forecasts are useful in their own right (as they probably are), this second approach seems very useful. However, there is another way to do this. First, forecast the state level unemployment and then disaggregate this forecast into the individual substate level regional forecasts. This disaggregation could be accomplished by multiplying the state level forecasts by a series of indices that reect the proportion of total statewide unemployment that is accounted for by each region at the substate level. These indices also evolve with time, so it will be necessary to forecast them as well as part of a complete system.

This problem is sometimes referred to as the top鈥揹own versus bottom鈥?up forecasting problem. In many, if not most, of these problems, we are interested in both forecasts for the top level quantity (the aggregate time series) and forecasts for the bottom level time series that are the components of the aggregate.

This leads to an obvious question: is it better to forecast the aggregate or top level quantity directly and then disaggregate, or to forecast the individual components directly and then aggregate them to form the forecast of the total? In other words, is it better to forecast from the top down or from the bottom up? The literature in statistical forecasting, business forecasting and econometrics, and time series analysis suggests that this question is far from settled at either the theoretical or empirical levels. Sometimes the aggregate quantity is more accurate than the disaggregated components, and sometimes the aggregate quantity is subject to less measurement error. It may be more complete and timely as well, and these aspects of the problem should encourage those who consider forecasting the aggregate quantity and then disaggregating. On the other hand, sometimes the bottom level data is easier to obtain and is at least thought to be more timely and accurate, and this would suggest that a bottom鈥搖p approach would be superior to the top鈥揹own approach.

In any speci-c practical application it will be dif-cult to argue on theoretical grounds what the correct approach should be. Therefore, in most situations, this question will have to be settled empirically by trying both approaches. With modern computer software for time series analysis and forecasting, this is not dif-cult. However, in conducting such a study it is a good idea to have an adequate amount of data for identifying and -tting the time series models for both the top level series and the bottom level series, and a reasonable amount of data for testing the two approaches. Obviously, data splitting should be done here, and the data used for model building

should not be used for investigating forecasting model performance. Once an approach is determined, the forecasts should be carefully monitored over time to make sure that the dynamics of the problem have not changed, and that the top鈥揹own approach that was found to be optimal in testing (say) is now no longer as effective as the bottom鈥搖p approach. The methods for monitoring forecasting model performance presented in Chapter 2 are useful in this regard.

There are some results available about the effect of adding time series together. This is a special case of a more general problem called temporal aggregation, in which several time series may be combined as, for instance, when monthly data are aggregated to form quarterly data. For example, suppose that we have a top level time series $Y _ { t }$ that is the sum of two independent time series $y _ { 1 , t }$ and $y _ { 2 , t }$ , and let us assume that both of the bottom level time series are moving average (MA) processes of orders $q _ { 1 }$ and $q _ { 2 }$ , respectively. So, using the notation for ARIMA models introduced in Chapter 5, we have

$$
Y _ {t} = \theta_ {1} (B) a _ {t} + \theta_ {2} (B) b _ {t}
$$

where $a _ { t }$ and $b _ { t }$ are independent white noise processes. Now let $q$ be the maximum of $q _ { 1 }$ and $q _ { 2 }$ . The autocorrelation function for the top level time series $Y _ { t }$ must be zero for all of the lags beyond $q$ . This means that there is a representation of the top level time series as an MA process

$$
Y _ {t} = \theta_ {3} (B) u _ {t}
$$

where $u _ { t }$ is white noise. This moving average process has the same order as the higher order bottom level time series.

Now consider the general ARIMA $( p _ { 1 } , d , q _ { 1 } )$ model

$$
\phi_ {1} (B) \nabla^ {d} y _ {t} = \theta_ {1} (B) a _ {t}
$$

and suppose that we are interested in the sum of two time series $z _ { t } = y _ { t } + w _ { t }$ . A practical situation where this occurs, in addition to the top鈥揹own versus bottom鈥搖p problem, is when the time series $y _ { t }$ we are interested in cannot be observed directly and $w _ { t }$ represents added noise due to measurement error. We want to know something about the nature of the sum of the two series, $z _ { t }$ . The sum can be written as

$$
\phi_ {1} (B) \nabla^ {d} z _ {t} = \theta_ {1} (B) a _ {t} + \phi_ {1} (B) \nabla^ {d} w _ {t}
$$

Assume that the time series $w _ { t }$ can be represented as a stationary $\mathbf { A R M A } ( p _ { 2 } , 0 , q _ { 2 } )$ model

$$
\phi_ {2} (B) w _ {t} = \theta_ {2} (B) b _ {t}
$$

where $b _ { t }$ is white noise independent of $a _ { t }$ . Then the top level time series is

$$
\phi_ {1} (B) \phi_ {2} (B) \nabla^ {d} z _ {t} = \phi_ {2} (B) \theta_ {1} (B) a _ {t} + \phi_ {1} (B) \theta_ {2} (B) \nabla^ {d} b _ {t}
$$

The term on the left-hand side is a polynomial of order $P = p _ { 1 } + p _ { 2 }$ , the -rst term on the right-hand side is a polynomial of order $q _ { 1 } + p _ { 2 }$ , and the second term on the right-hand side is a polynomial of order $p _ { 1 } + q _ { 2 } + d$ . Let $Q$ be the maximum of $q _ { 1 } + p _ { 2 }$ and $p _ { 1 } + q _ { 2 } + d$ . Then the top level time series is an ARIMA $( P , d , Q )$ model, say,

$$
\phi_ {3} (B) \nabla^ {d} z _ {t} = \theta_ {3} (B) u _ {t}
$$

where $u _ { t }$ is white noise.

Example 7.8 Suppose that we have a time series that is represented by an IMA(1, 1) model, and to this time series is added white noise. This could be a situation where measurements on a periodic sample of some characteristic in the output of a chemical process are made with a laboratory procedure, and the laboratory procedure has some built-in measurement error, represented by the white noise. Suppose that the underlying IMA(1, 1) model is

$$
y _ {t} = y _ {t - 1} - 0. 6 a _ {t - 1} + a _ {t}
$$

Let $D _ { t }$ be the -rst difference of the observed time series $z _ { t } = y _ { t } + w _ { t }$ , where $w _ { t }$ is white noise:

$$
\begin{array}{l} D _ {t} = z _ {t} - z _ {t - 1} \\ = (1 - \theta B) a _ {t} + (1 - B) w _ {t} \\ \end{array}
$$

The autocovariances of the differenced series are

$$
\begin{array}{l} \gamma_ {0} = \sigma_ {a} ^ {2} (1 + \theta^ {2}) + 2 \sigma_ {w} ^ {2} \\ \gamma_ {1} = - \sigma_ {a} ^ {2} \theta - \sigma_ {w} ^ {2} \\ \gamma_ {j} = 0, \quad j \geq 2 \\ \end{array}
$$

Because the autocovariances at and beyond lag 2 are zero, we know that the observed time series will be IMA(1, 1). In general, we could write this as

$$
z _ {t} = z _ {t - 1} - \theta^ {*} u _ {t - 1} + u _ {t}
$$

where the parameter $\theta ^ { * }$ is unknown. However, we can -nd $\theta ^ { * }$ easily. The autocovariances of the -rst differences of this observed time series are

$$
\begin{array}{l} \gamma_ {0} = \sigma_ {u} ^ {2} (1 + \theta^ {* 2}) \\ \gamma_ {1} = - \sigma_ {u} ^ {2} \theta^ {*} \\ \gamma_ {j} = 0, \quad j \geq 2 \\ \end{array}
$$

Now all we have to do is to equate the autocovariances for this observed series in terms of the parameter $\theta ^ { * }$ with the autocovariances of the time series $D _ { t }$ and we can solve for $\theta ^ { * }$ and $\sigma _ { u } ^ { 2 }$ . This gives the following:

$$
\begin{array}{l} \frac {\theta^ {*}}{1 - \theta^ {*}} = \frac {0 . 6}{1 - 0 . 6 + \sigma_ {w} ^ {2} / \sigma_ {a} ^ {2}} \\ \sigma_ {u} ^ {2} = \sigma_ {a} ^ {2} \frac {(0 . 6) ^ {2}}{\theta^ {* 2}} \\ \end{array}
$$

Suppose that $\sigma _ { a } ^ { 2 } = 2$ and $\sigma _ { w } ^ { 2 } = 1$ . Then it turns out that the solution is $\theta ^ { * } = 0 . 4$ and $\sigma _ { u } ^ { 2 } = 4 . 5 0$ . Adding the measurement error from the laboratory procedure to the original sample property has inated the variability of the observed value rather considerably over the original variability that was present in the sample property.

# 7.7 NEURAL NETWORKS AND FORECASTING

Neural networks, or more accurately arti-cial neural networks, have been motivated by the recognition that the human brain processes information in a way that is fundamentally different from the typical digital computer. The neuron is the basic structural element and informationprocessing module of the brain. A typical human brain has an enormous number of them (approximately 10 billion neurons in the cortex and 60 trillion synapses or connections between them) arranged in a highly complex, nonlinear, and parallel structure. Consequently, the human brain is a very ef-cient structure for information processing, learning, and reasoning.

An arti-cial neural network is a structure that is designed to solve certain types of problems by attempting to emulate the way the human brain would solve the problem. The general form of a neural network is a 鈥渂lack-box鈥?type of model that is often used to model high-dimensional, nonlinear data. In the forecasting environment, neural networks are sometimes used to solve prediction problems instead of using a formal model

building approach or development of the underlying knowledge of the system that would be required to develop an analytical forecasting procedure. If it was a successful approach that might be satisfactory. For example, a company might want to forecast demand for its products. If a neural network procedure can do this quickly and accurately, the company may have little interest in developing a speci-c analytical forecasting model to do it. Hill et al. (1994) is a basic reference on arti-cial neural networks and forecasting.

Multilayer feedforward arti-cial neural networks are multivariate statistical models used to relate $p$ predictor variables $x _ { 1 }$ , $x _ { 2 } , \ldots , x _ { p }$ to one or more output or response variables y. In a forecasting application, the inputs could be explanatory variables such as would be used in a regression model, and they could be previous values of the outcome or response variable (lagged variables). The model has several layers, each consisting of either the original or some constructed variables. The most common structure involves three layers: the inputs, which are the original predictors; the hidden layer, comprised of a set of constructed variables; and the output layer, made up of the responses. Each variable in a layer is called a node. Figure 7.12 shows a typical three-layer arti-cial neural network for forecasting the output variable y in terms of several predictors.

A node takes as its input a transformed linear combination of the outputs from the nodes in the layer below it. Then it sends as an output a transformation of itself that becomes one of the inputs to one or more nodes on the next layer. The transformation functions are usually either sigmoidal (S shaped) or linear and are usually called activation functions or transfer

![](images/504220c6de647a95f6b20541144fe535db89fa95c3f56e0faf054f51266b1a0b.jpg)  
FIGURE 7.12 Arti-cial neural network with one hidden layer.

functions. Let each of the $k$ hidden layer nodes $a _ { u }$ be a linear combination of the input variables:

$$
a _ {u} = \sum_ {j = 1} ^ {p} w _ {1 j u} x _ {j} + \theta_ {u}
$$

where the $w _ { 1 j u }$ are unknown parameters that must be estimated (called weights) and $\theta _ { u }$ is a parameter that plays the role of an intercept in linear regression (this parameter is sometimes called the bias node).

Each node is transformed by the activation function $g ( \mathbf { \xi } )$ . Much of the neural networks literature refers to these activation functions notationally as $\sigma _ { u }$ because of their S shape (the use of $\sigma$ is an unfortunate choice of notation so far as statisticians are concerned). Let the output of node $a _ { u }$ be denoted by $z _ { u } = g ( a _ { u } )$ . Now we form a linear combination of these outputs, say, $\begin{array} { r } { b = \sum _ { u = 1 } ^ { k } w _ { u e \nu } z _ { u } } \end{array}$ . Finally, the output response or the predicted value for $y$ is a transformation of the $^ b$ , say, $y = \tilde { g } ( b )$ , where ${ \tilde { g } } ( b )$ is the activation function for the response.

The response variable $y$ is a transformed linear combination of the original predictors. For the hidden layer, the activation function is often chosen to be either a logistic function or a hyperbolic tangent function. The choice of activation function for the output layer often depends on the nature of the response variable. If the response is bounded or dichotomous, the output activation function is usually taken to be sigmoidal, while if it is continuous, an identity function is often used.

The neural network model is a very exible form containing many parameters, and it is this feature that gives a neural network a nearly universal approximation property. That is, it will -t many historical data sets very well. However, the parameters in the underlying model must be estimated (parameter estimation is called 鈥渢raining鈥?in the neural network literature), and there are a lot of them. The usual approach is to estimate the parameters by minimizing the overall residual sum of squares taken over all responses and all observations. This is a nonlinear least squares problem, and a variety of algorithms can be used to solve it. Often a procedure called backpropagation (which is a variation of steepest descent) is used, although derivative-based gradient methods have also been employed. As in any nonlinear estimation procedure, starting values for the parameters must be speci-ed in order to use these algorithms. It is customary to standardize all the input variables, so small essentially random values are chosen for the starting values.

With so many parameters involved in a complex nonlinear function, there is considerable danger of over-tting. That is, a neural network will

provide a nearly perfect -t to a set of historical or 鈥渢raining鈥?data, but it will often predict new data very poorly. Over-tting is a familiar problem to statisticians trained in empirical model building. The neural network community has developed various methods for dealing with this problem, such as reducing the number of unknown parameters (this is called 鈥渙ptimal brain surgery鈥?, stopping the parameter estimation process before complete convergence and using cross-validation to determine the number of iterations to use, and adding a penalty function to the residual sum of squares that increases as a function of the sum of the squares of the parameter estimates.

There are also many different strategies for choosing the number of layers and number of neurons and the form of the activation functions. This is usually referred to as choosing the network architecture. Crossvalidation can be used to select the number of nodes in the hidden layer.

Arti-cial neural networks are an active area of research and application in many -elds, particularly for the analysis of large, complex, highly nonlinear problems. The over-tting issue is frequently overlooked by many users and even the advocates of neural networks, and because many members of the neural network community do not have sound training in empirical model building, they often do not appreciate the dif-culties over-tting may cause. Furthermore, many computer programs for implementing neural networks do not handle the over-tting problem particularly well. Studies of the ability of neural networks to predict future values of a time series that were not used in parameter estimation (-tting) have been, in many cases, disappointing. Our view is that neural networks are a complement to the familiar statistical tools of forecasting, and they might be one of the approaches you should consider, but they are not a replacement for them.

# 7.8 SPECTRAL ANALYSIS

This book has been focused on the analysis and modeling of time series in the time domain. This is a natural way to develop models, since time series all are observed as a function of time. However, there is another approach to describing and analyzing time series that uses a frequency domain approach. This approach consists of using the Fourier representation of a time series, given by

$$
y _ {t} = \sum_ {k = 1} ^ {T} a _ {k} \sin \left(2 \pi f _ {k} t\right) + b _ {k} \cos \left(2 \pi f _ {k} t\right) \tag {7.29}
$$

where $f _ { k } = k / T$ . This model is named after J.B.J Fourier, an $1 8 ^ { \mathrm { { t h } } }$ century French mathematician, who claimed that any periodic function could be represented as a series of harmonically related sinusoids. Other contributors to Fourier analysis include Euler, D. Bernoulli, Laplace, Lagrange, and Dirichlet. The original work of Fourier was focused on phenomena in continuous time, such as vibrating strings, and there are still many such applications today from such diverse -elds as geophysics, oceanography, atmospheric science, astronomy, and many disciplines of engineering. However, the key ideas carry over to discrete time series. We con-ne our discussion to stationary discrete time series.

Computing the constants $a _ { k }$ and $b _ { k }$ turns out to be quite simple:

$$
a _ {k} = \frac {2}{T} \sum_ {k = 1} ^ {T} \cos (2 \pi f _ {k} t) \tag {7.30}
$$

and

$$
b _ {k} = \frac {2}{T} \sum_ {k = 1} ^ {T} \sin (2 \pi f _ {k} t) \tag {7.31}
$$

These coef-cients are combined to form a periodogram

$$
I \left(f _ {k}\right) = \frac {T}{2} \left(a _ {k} ^ {2} + b _ {k} ^ {2}\right) \tag {7.32}
$$

The periodogram is then usually smoothed and scaled to produce the spectrum or a spectral density function. The spectral density function is just the Fourier transform of the autocorrelation function, so it conveys similar information as is found in the autocorrelations. However, sometimes the spectral density is easier to interpret than the autocorrelation function because adjacent sample autocorrelations can be highly correlated while estimates of the spectrum at adjacent frequencies are approximately independent. Generally, if the frequency $k / T$ is important then $I ( f _ { k } )$ will be large, and if the frequency $k / T$ is not important then $I ( f _ { k } )$ will be small.

It can be shown that

$$
\sum_ {k = 1} ^ {T} I \left(f _ {k}\right) = \sigma^ {2} \tag {7.33}
$$

where $\sigma ^ { 2 }$ is the variance of the time series. Thus the spectrum decomposes the variance of the time series into individual components, each of which is associated with a particular frequency. So we can think of spectral analysis

![](images/1ccbd546aa7ba7c84d769bea79ba69d9b03de204e9cc71a42d091099a4c8d3e2.jpg)  
FIGURE 7.13 The spectrum of a white noise process.

as an analysis of variance technique. It decomposes the variability in the time series by frequency.

It is helpful to know what the spectrum looks like for some simple ARMA models. If the time series white noise (uncorrelated observations with constant variance $\sigma ^ { 2 }$ ), it can be shown that the spectrum is a horizontal straight line as shown in Figure 7.13. This means that the contribution to the variance at all frequencies is equal. A logical use for the spectrum is to calculate it for the residuals from a time series model and see if the spectrum is reasonably at.

Now consider the AR(1) process. The shape of the spectrum depends on the value of the AR(1) parameter $\phi$ . When $\phi > 0$ , which results in a positively autocorrelated time series, the spectrum is dominated by lowfrequency components. These low-frequency or long-period components result in a relative smooth time series. When $\phi < 0$ , the time series is negatively autocorrelated, and the time series has a more ragged or volatile appearance. This produces a spectrum dominated by high-frequency or short-period components. Examples are shown in Figure 7.14.

![](images/e51563baea7e2ceb7d6b571b9977e1cd58239a8139544a3b88f4f7b51d0b4391.jpg)

![](images/754e7ec3846ad51df63bce2c8fe1b02e70abc9241085d53cc5d89f315189343d.jpg)  
FIGURE 7.14 The spectrum of AR(1) processes.

![](images/dc0c63758d82dd843c1eb0ba43a2da92782f2c3b4a9cf207d5738e0234854e80.jpg)

![](images/36910d2ce3d5a17373628010ab516ab50ca409f9a92a6bd263c45e12d2fd9512.jpg)  
FIGURE 7.15 The spectrum of MA(1) processes.

The spectrum of the MA(1) process is shown in Figure 7.15. When the MA(1) parameter is positive, that is, when $\theta > 0$ , the time series is negatively autocorrelated and has a more volatile appearance. Thus the spectrum is dominated by higher frequencies. When the MA(1) parameter is negative $( \theta > 0 )$ ), the time series is negatively autocorrelated and has a smoother appearance. This results in a spectrum that is dominated by low frequencies.

The spectrum of seasonal processes will exhibit peaks at the harmonically related seasonal frequencies. For example, consider the simple seasonal model with period 12, as might be used to represent monthly data with an annual seasonal cycle:

$$
(1 - \phi^ {*} B ^ {1 2}) y _ {t} = \varepsilon_ {t}
$$

If $\phi ^ { * }$ is positive, the spectrum will exhibit peaks at frequencies 0 and 2饾湅kt鈭? $2 , k = 1 , 2 , 3 , 4 , 5 , 6$ . Figure 7.16 shows the spectrum.

![](images/cdfe3326c1f3da9c4a7c034d110e8badbd23fecc3fa0356ee62bca909b898e96.jpg)  
FIGURE 7.16 The spectrum of the seasonal $( 1 - \phi ^ { * } B ^ { 1 2 } ) y _ { t } = \varepsilon _ { t }$ process.

Fisher鈥檚 Kappa statistic tests the null hypothesis that the values in the series are drawn from a normal distribution with variance 1 against the alternative hypothesis that the series has some periodic component. The test statistic kappa $( \kappa )$ is the ratio of the maximum value of the periodogram, $I ( f _ { k } )$ , to its average value. The probability of observing a larger Kappa if the null hypothesis is true is given by

$$
P (\kappa > k) = 1 - \sum_ {j = 0} ^ {q} (- 1) ^ {j} {\binom {q} {j}} \left[ \max \left(1 - \frac {j k}{q}, 0\right) \right] ^ {q - 1}
$$

where $k$ is the observed value of the kappa statistic, $q = T / 2$ if $T$ is even and $q = ( T - 1 ) / 2$ if $T$ is odd. The null hypothesis is rejected if the computed probability is less that the desired signi-cance level. There is also a Kolmogorov鈥揝mirnov test due to Bartlett that compares the normalized cumulative periodogram to the cumulative distribution function of the uniform distribution on the interval (0, 1). The test statistic equals the maximum absolute difference of the cumulative periodogram and the uniform CDF. If this quantity exceeds $a / { \sqrt { q } }$ , then we should reject the hypothesis that the series comes from a normal distribution. The values $a = 1 . 3 6$ and $a = 1 . 6 3$ correspond to signi-cance levels of 0.05 and 0.01, respectively.

In general, we have found it dif-cult to determine the exact form of an ARIMA model purely from examination of the spectrum. The autocorrelation and partial autocorrelation functions are almost always more useful and easier to interpret. However, the spectrum is a complimentary tool and should always be considered as a useful supplement to the ACF and PACF. For further reading on spectral analysis and its many applications, see Jenkins and Watts (1969), Percival and Walden (1992), and Priestley (1991).

Example 7.9 JMP can be used to compute and display the spectrum for time series. We will illustrate the JMP output using the monthly US beverage product shipments. These data are shown originally in Figure 1.5 and are in Appendix Table B. These data were also analyzed in Chapter 2 to illustrate decomposition techniques. Figure 7.17 presents the JMP output, including a time series plot, the sample ACF, PACF, and variogram, and the spectral density function. Notice that there is a prominent peak in the spectral density at frequency 0.0833 that corresponds to a seasonal period of 12 months. The JMP output also provides the Fisher kappa statistic and the $P$ -value indicates that there is at least one periodic component. The Bartlett Kolmogorov鈥揝mirnov test statistic is also signi-cant at the 0.01 level indicating that the data do not come from a normal distribution.

![](images/55f3c659dd716cfc9ae3c2683303ddd683bae659631ca1f59969c6c0b235ca1f.jpg)  
Time series beverage shipments

![](images/ab3a864cc8e2e87efe35c78ddc63851b9c3e7912023405868543a3d905089ed2.jpg)  
Time series basic diagnostics

![](images/68e19f58f16ba987645501ff627e5215304cb7213c919fd962c140fa4d7720ab.jpg)  
Spectral density   
FIGURE 7.17 JMP output showing the spectrum, ACF, PACF, and variogram for the beverage shipment data.

![](images/cbdf69d79c45c13d68815323b0799d5b321979ae40fcb7a0d665f438fa6d5d20.jpg)  
FIGURE 7.17 (Continued)

# White noise test

Fisher鈥檚 Kappa 28.784553

Prob $>$ Kappa 1.041e-13

Bartlett鈥檚 Kolmogorov-Smirnov 0.7545515

# 7.9 BAYESIAN METHODS IN FORECASTING

In many forecasting problems there is little or no historical information available at the time initial forecasts are required. Consequently, the initial forecasts must be based on subjective considerations. As information becomes available, this subjective information can be modi-ed in light of actual data. An example of this is forecasting demand for seasonal clothing, which, because of style obsolescence, has a relatively short life. In this industry a common practice is to, at the start of the season, make a forecast of total sales for a product during the season and then as the season progresses the original forecast can be modi-ed taking into account actual sales.

Bayesian methods can be useful in problems of this general type. The original subjective estimates of the forecast are translated into subjective estimates of the forecasting model parameters. Then Bayesian methods are used to update these parameter estimates when information in the form of time series data becomes available. This section gives a brief overview of the Bayesian approach to parameter estimation and demonstrates the methodology with a simple time series model.

The method of parameter estimation makes use of the Bayes鈥?theorem. Let y be a random variable with probability density function that is characterized by an unknown parameter 饾渻. We write this density as $f ( y | \theta )$ to show that the distribution depends on $\theta$ . Assume that $\theta$ is a random variable with probability distribution $h _ { 0 } ( \theta )$ which is called the prior distribution for $\theta$ .

The prior distribution summarizes the subjective information that we have about $\theta$ , and the treatment of $\theta$ as a random variable is the major difference between Bayesian and classical methods of estimation. If we are relatively con-dent about the value of $\theta$ we should choose prior distribution with a small variance and if we are relatively uncertain about the value of $\theta$ we should choose prior distribution with a large variance.

In a time series or forecasting scenario, the random variable $y$ is a sequence of observations, $\operatorname { s a y } y _ { 1 } , y _ { 2 } , \ldots , y _ { T }$ . The new estimate of the parameter $\theta$ will be in the form of a revised distribution, $h _ { 1 } ( \theta | y )$ , called the posterior distribution for $\theta$ . Using Bayes鈥?theorem the posterior distribution is

$$
h _ {1} (\theta | y) = \frac {h _ {0} (\theta) f (y | \theta)}{\int_ {\theta} h _ {0} (\theta) f (y | \theta) d \theta} = \frac {h _ {0} (\theta) f (y | \theta)}{g (y)} \tag {7.34}
$$

where $f ( y | \theta )$ is usually called the likelihood of $y$ , given the value of $\theta$ , and $g ( y )$ | is the unconditional distribution of $y$ averaged over all $\theta$ . If the parameter $\theta$ is a discrete random variable then the integral in Eq. (7.34) should be replaced by a summation sign. This equation basically blends the observed information with the prior information to obtain a revised description of the uncertainty in the value of $\theta$ in the form of a posterior probability distribution. The Bayes鈥?estimator of $\theta$ , which we will denote by $\theta ^ { * }$ , is de-ned as the expected value of the posterior distribution:

$$
\theta^ {*} = \int_ {\theta} \theta h _ {1} (\theta | y) d \theta \tag {7.35}
$$

Typically we would use $\theta ^ { * }$ as the estimate of $\theta$ in the forecasting model. In some cases, it turns out that $\theta ^ { * }$ is optimal in the sense of minimizing the variance of forecast error.

We will illustrate the procedure with a relatively simple example. Suppose that $y$ is normally distributed with mean $\mu$ and variance $\sigma _ { y } ^ { 2 }$ ; that is,

$$
f (y | \mu) = N \big (\mu , \sigma_ {y} ^ {2} \big) = \big (2 \pi \sigma_ {y} ^ {2} \big) ^ {- 1 / 2} \exp \left[ - \frac {1}{2} \left(\frac {y - \mu}{\sigma_ {y}}\right) ^ {2} \right]
$$

We will assume that $\sigma _ { y } ^ { 2 }$ is known. The prior distribution for $\mu$ is assumed to be normal with mean $\mu ^ { \prime }$ and variance $\sigma _ { \mu } ^ { 2 ^ { \prime } }$ :

$$
h _ {0} (\mu | y) = N \big (\mu^ {\prime}, \sigma_ {\mu} ^ {2 ^ {\prime}} \big) = \big (2 \pi \sigma_ {\mu} ^ {2 ^ {\prime}} \big) ^ {- 1 / 2} \exp \left[ - \frac {1}{2} \left(\frac {\mu - \mu^ {\prime}}{\sigma_ {\mu} ^ {\prime}}\right) ^ {2} \right]
$$

The posterior distribution of $\mu$ given the observation y is

$$
\begin{array}{l} h _ {1} (\theta | y) = \frac {2 \pi \left(\sigma_ {\mu} ^ {2 ^ {\prime}} \sigma_ {y} ^ {2}\right) ^ {- 1 / 2} \exp \left[ \frac {1}{2} (\mu - \mu^ {\prime}) / \sigma_ {\mu} ^ {2 ^ {\prime}} + (y - \mu) / \sigma_ {y} ^ {2} \right]}{\int_ {- \infty} ^ {\infty} 2 \pi \left(\sigma_ {\mu} ^ {2 ^ {\prime}} \sigma_ {y} ^ {2}\right) ^ {- 1 / 2} \exp \left[ \frac {1}{2} (\mu - \mu^ {\prime}) / \sigma_ {\mu} ^ {2 ^ {\prime}} + (y - \mu) / \sigma_ {y} ^ {2} \right] d \mu} \\ = \left(2 \pi \frac {\sigma_ {\mu} ^ {2 ^ {\prime}} \sigma_ {y} ^ {2}}{\sigma_ {\mu} ^ {2 ^ {\prime}} + \sigma_ {y} ^ {2}}\right) ^ {- 1 / 2} \exp \left[ - \frac {1}{2} \frac {\left[ (\mu - \left(y \sigma_ {\mu} ^ {2 ^ {\prime}} + \mu^ {\prime} \sigma_ {y} ^ {2}\right) / \left(\sigma_ {\mu} ^ {2 ^ {\prime}} + \sigma_ {y} ^ {2}\right) \right.}{\sigma_ {\mu} ^ {2 ^ {\prime}} \sigma_ {y} ^ {2} / \left(\sigma_ {\mu} ^ {2 ^ {\prime}} + \sigma_ {y} ^ {2}\right)} \right] \\ \end{array}
$$

which is a normal distribution with mean and variance

$$
\mu^ {\prime \prime} = E (\mu | y) = \frac {y \sigma_ {\mu} ^ {2 ^ {\prime}} + \mu^ {\prime} \sigma_ {y} ^ {2}}{\sigma_ {\mu} ^ {2 ^ {\prime}} + \sigma_ {y} ^ {2}}
$$

and

$$
\sigma_ {\mu} ^ {2 ^ {\prime \prime}} = V (\mu | y) = \frac {\sigma_ {\mu} ^ {2 ^ {\prime}} \sigma_ {y} ^ {2}}{\sigma_ {\mu} ^ {2 ^ {\prime}} + \sigma_ {y} ^ {2}}
$$

respectively. Refer to Winkler (2003), Raiffa and Schlaifer (1961), Berger (1985), and West and Harrison (1997) for more details of Bayesian statistical inference and decision making and additional examples.

Now let us consider a simple time series model, the constant process, de-ned in Eq. (4.1) as

$$
y _ {t} = \mu + \varepsilon_ {t}
$$

where $\mu$ is the unknown mean and the random component is $\varepsilon _ { t }$ , which we will assume to have a normal distribution with mean zero and known variance $\sigma _ { y } ^ { 2 }$ Consequently, we are assuming that the observation in any period $t$ has a normal distribution, say

$$
f (y _ {t} | \mu) = N \left(\mu , \sigma_ {y} ^ {2}\right)
$$

Since the variance $\sigma _ { \mathrm { y } } ^ { 2 }$ is known, the problem is to estimate $\mu$

Suppose that at the start of the forecasting process, time $t = 0$ , we estimate the mean demand rate to be $\mu ^ { \prime }$ and the variance $\sigma _ { \mu } ^ { 2 ^ { \prime } }$ captures the uncertainty in this estimate. So the prior distribution for $\mu$ is the normal prior

$$
h _ {0} (\mu) = N \left(\mu^ {\prime}, \sigma_ {\mu} ^ {2 ^ {\prime}}\right)
$$

After one period, the observation $y _ { 1 }$ is known. The estimate $\mu ^ { \prime }$ and the variance 饾湈2鈥?$\sigma _ { \mu } ^ { \bar { 2 } ^ { \prime } }$ can now be updated using the results obtained above for a normal sampling process and a normal prior:

$$
h _ {1} (\mu | y _ {1}) = N [ \mu^ {\prime \prime} (1), \sigma^ {2 ^ {\prime \prime}} (1) ]
$$

where

$$
\mu^ {\prime \prime} (1) = E (\mu | y _ {1}) = \frac {y \sigma_ {\mu} ^ {2 ^ {\prime}} + \mu^ {\prime} \sigma_ {y} ^ {2}}{\sigma_ {\mu} ^ {2 ^ {\prime}} + \sigma_ {y} ^ {2}}
$$

and

$$
\sigma_ {\mu} ^ {2 \prime \prime} (1) = V (\mu | y _ {1}) = \frac {\sigma_ {\mu} ^ {2 \prime} \sigma_ {y} ^ {2}}{\sigma_ {\mu} ^ {2 \prime} + \sigma_ {y} ^ {2}}
$$

At the end of period 2, when the next observation $y _ { 2 }$ becomes available, the Bayesian updating process transforms $h _ { 1 } ( \mu | y _ { 1 } )$ into $h _ { 2 } ( \mu | y _ { 1 , } y _ { 2 } )$ in the following way:

$$
h _ {2} (\mu | y _ {1}, y _ {2}) = \frac {h _ {1} (\mu | y _ {1}) f (y _ {2} | \mu)}{\int_ {\mu} h _ {1} (\mu | y _ {1}) f (y _ {2} | \mu) d \mu}
$$

Here the old posterior $h _ { 1 }$ is now used as a prior and combined with the likelihood of $y _ { 2 }$ to obtain the new posterior distribution of $\mu$ at the end of period 2. Using our previous results, we now have

$$
h _ {2} (\mu | y _ {1}, y _ {2}) = N [ \mu^ {\prime \prime} (2), \sigma^ {2 ^ {\prime \prime}} (2) ]
$$

and

$$
\mu^ {\prime \prime} (2) = E (\mu | y _ {1}, y _ {2}) = \frac {\bar {y} \sigma_ {\mu} ^ {2 \prime} + \mu^ {\prime} \left(\sigma_ {y} ^ {2} / 2\right)}{\sigma_ {\mu} ^ {2 \prime} + \left(\sigma_ {y} ^ {2} / 2\right)}
$$

$$
\sigma_ {\mu} ^ {2 \prime \prime} (2) = V (\mu | y _ {1}, y _ {2}) = \frac {\sigma_ {\mu} ^ {2 \prime} \sigma_ {y} ^ {2}}{2 \sigma_ {\mu} ^ {2 \prime} + \sigma_ {y} ^ {2}}
$$

where $\bar { y } = ( y _ { 1 } + y _ { 2 } ) / 2$ . It is easy to show that $h _ { 2 } ( \mu | y _ { 1 } , y _ { 2 } ) = h _ { 2 } ( \mu | \bar { y } )$ ; that is, the same posterior distribution is obtained using the sample average $\bar { y }$ as from using $y _ { 1 }$ and $y _ { 2 }$ sequentially because the sample average is a suf-cient statistic for estimating the mean $\mu$ .

In general, we can show that after observing $y _ { T }$ , we can calculate the posterior distribution as

$$
h _ {2} (\mu | y _ {1}, y _ {2}, \ldots , y _ {T}) = N [ \mu^ {\prime \prime} (T), \sigma^ {2 ^ {\prime \prime}} (T) ]
$$

where

$$
\mu^ {\prime \prime} (T) = \frac {\bar {y} \sigma_ {\mu} ^ {2 ^ {\prime}} + \mu^ {\prime} \left(\sigma_ {y} ^ {2} / T\right)}{\sigma_ {\mu} ^ {2 ^ {\prime}} + \left(\sigma_ {y} ^ {2} / T\right)}
$$

$$
\sigma_ {\mu} ^ {2 ^ {\prime \prime}} (T) = \frac {\sigma_ {\mu} ^ {2 ^ {\prime}} \sigma_ {y} ^ {2}}{T \sigma_ {\mu} ^ {2 ^ {\prime}} + \sigma_ {y} ^ {2}}
$$

where $\bar { y } = ( y _ { 1 } + y _ { 2 } + \cdots + y _ { T } ) / T .$ . The Bayes estimator of $\mu$ after $T$ periods is $\mu ^ { * } ( T ) = \mu ^ { \prime \prime } ( T )$ . We can write this as

$$
\mu^ {*} (T) = \frac {T}{r + T} \bar {y} + \frac {r}{r + T} \mu^ {\prime} \tag {7.36}
$$

where $r = \sigma _ { y } ^ { 2 } / \sigma _ { \mu } ^ { 2 ^ { \prime } }$ . Consequently, the Bayes estimator of $\mu$ is just a weighted average of the sample mean and the initial subjective estimate $\mu ^ { \prime }$ . The Bayes estimator can be written in a recursive form as

$$
\mu^ {*} (T) = \lambda (T) y _ {T} + [ 1 - \lambda (T) ] \mu^ {*} (T - 1) \tag {7.37}
$$

where

$$
\lambda (T) = \frac {1}{r + T} = \frac {\sigma_ {\mu} ^ {2 ^ {\prime}}}{T \sigma_ {\mu} ^ {2 ^ {\prime}} + \sigma_ {y} ^ {2}}
$$

Equation (7.36) shows that the estimate of the mean in period $T$ is updated at each period by a form that is similar to -rst-order exponential smoothing. However, notice that the smoothing factor $\lambda ( T )$ is a function of $T .$ , and it becomes smaller as $T$ increases. Furthermore, since $\sigma _ { \mu } ^ { 2 ^ { \prime \prime } } ( T ) = \lambda ( T ) \sigma _ { y } ^ { 2 }$ , the uncertainty in the estimate of the mean decreases to zero as time T becomes large. Also, the weight given to the prior estimate of the mean decreases as T becomes large. Eventually, as more data becomes available, a permanent forecasting procedure could be adopted, perhaps involving exponential smoothing. This estimator is optimal in the sense that it minimizes the variance of forecast error even if the process is not normally distributed.

We assumed that the variance of the demand process was known, or at least a reasonable estimate of it was available. Uncertainty in the value of this parameter could be handled by also treating it as a random variable. Then the prior distribution would be a joint distribution that would reect

the uncertainty in both the mean and the variance. The Bayesian updating process in this case is considerably more complicated than in the knownvariance case. Details of the procedure and some useful advice on choosing a prior are in Raiffa and Schlaifer (1961).

Once the prior has been determined, the forecasting process is relatively straightforward. For a constant process, the forecasting equation is

$$
\hat {y} _ {T + \tau} (T) = \hat {\mu} (T) = \mu^ {*} (T)
$$

using the Bayes estimate as the current estimate of the mean. Our uncertainty in the estimate of the mean is just the posterior variance. So the variance of the forecast is

$$
V [ \hat {y} _ {T + \tau} (T) ] = \sigma_ {\mu} ^ {2 \prime \prime} (T)
$$

and the variance of forecast error is

$$
V [ y _ {T + \tau} - \hat {y} _ {T + \tau} (T) ] = V [ e _ {\tau} (T + \tau) ] = \sigma_ {y} ^ {2} + \sigma_ {\mu} ^ {2 \prime \prime} (T) \tag {7.38}
$$

The variance of forecast error is independent of the lead time in the Bayesian case for a constant process. If we assume that $y$ and $\mu$ are normally distributed, then we can use Eq. (3.38) to -nd a $100 ( 1 - \alpha ) \%$ prediction interval on the forecast $V [ \hat { y } _ { T + \tau } ( T ) ]$ as follows:

$$
\mu^ {*} (T) \pm Z _ {\alpha / 2} \sqrt {\sigma_ {y} ^ {2} + \sigma_ {\mu} ^ {2 \prime \prime} (T)} \tag {7.39}
$$

where $Z _ { \alpha / 2 }$ is the usual $\alpha / 2$ percentage point of the standard normal distribution.

Example 7.10 Suppose that we are forecasting weekly demand for a new product. We think that demand is normally distributed, and that at least in the short run that a constant model is appropriate. There is no useful historical information, but a reasonable prior distribution for $\mu$ is $N ( 1 0 0 , 2 5 )$ and $\sigma _ { y } ^ { 2 }$ is estimated to be 150. At time period $T = 0$ the forecast for period 1 is

$$
\hat {y} _ {1} (0) = 1 0 0
$$

The variance of forecast error is $1 5 0 + 2 5 = 1 7 5$ , so a $96 \%$ prediction interval for $y _ { 1 }$ is

$$
1 0 0 \pm 1. 9 6 \sqrt {1 7 5} \quad \text {o r} \quad [ 7 4. 1, 1 2 5. 9 ]
$$

Suppose the actual demand experienced in period 1 is $y _ { 1 } = 8 6$ . We can use Eq. (7.37) to update the estimate of the mean. First, calculate $r = \sigma _ { y } ^ { 2 } / \sigma _ { \mu } ^ { 2 ^ { \prime } } =$ $1 5 0 / 2 5 = 6$ and $\lambda ( 1 ) = 1 / ( 6 + 1 ) = 0 . 1 4 3$ , then

$$
\begin{array}{l} \mu^ {*} (1) = \mu^ {\prime \prime} (1) = \lambda (1) y _ {1} + [ 1 - \lambda (1) ] \mu^ {*} (0) \\ = 0. 1 4 3 (8 6) + (1 - 0. 1 4 3) 1 0 0 \\ = 9 8. 0 \\ \end{array}
$$

and

$$
\begin{array}{l} \sigma_ {\mu} ^ {2 \prime \prime} (1) = \lambda (1) \sigma_ {y} ^ {2} \\ = 0. 1 4 3 (1 5 0) \\ = 2 1. 4 \\ \end{array}
$$

The forecast for period 2 is now

$$
\hat {y} _ {2} (1) = 9 8. 0
$$

The corresponding $9 5 \%$ prediction interval for $y _ { 2 }$ is

$$
9 8. 0 \pm 1. 9 6 \sqrt {1 5 0 + 2 1 . 4} \quad \text {o r} \quad [ 7 2. 3, 1 2 3. 7 ]
$$

In time period the actual demand experienced is 94. Now $\lambda ( 2 ) = 1 / ( 6 +$ $2 ) = 0 . 1 2 5$ , and

$$
\begin{array}{l} \mu^ {*} (2) = \mu^ {\prime \prime} (2) = \lambda (2) y _ {2} + [ 1 - \lambda (2) ] \mu^ {*} (1) \\ = 0. 1 2 5 (9 4) + (1 - 0. 1 2 5) 9 8. 0 \\ = 9 7. 5 \\ \end{array}
$$

So the forecast for period 3 is

$$
\hat {y} _ {3} (3) = 9 7. 5
$$

and the updated variance estimate is

$$
\begin{array}{l} \sigma_ {\mu} ^ {2 \prime \prime} (2) = \lambda (2) \sigma_ {y} ^ {2} \\ = 0. 1 2 5 (1 5 0) \\ = 1 8. 8 \\ \end{array}
$$

Therefore the $96 \%$ prediction interval for $y _ { 3 }$ is

$$
9 7. 5 \pm 1. 9 6 \sqrt {1 5 0 + 1 8 . 8} \quad \text {o r} \quad [ 7 2. 0, 1 2 3. 0 ]
$$

This procedure would be continued until it seems appropriate to change to a more permanent forecasting procedure. For example, a change to

-rst-order exponential smoothing could be made when $\lambda ( T )$ drop to a target level, say $0 . 0 5 < \lambda ( T ) < 0 . 1$ . Then after suf-cient data has been observed, an appropriate time series model could be -t to the data.

# 7.10 SOME COMMENTS ON PRACTICAL IMPLEMENTATION AND USE OF STATISTICAL FORECASTING PROCEDURES

Over the last 35 years there has been considerable information accumulated about forecasting techniques and how these methods are applied in a wide variety of settings. Despite the development of excellent analytical techniques, many business organizations still rely on judgment forecasts by their marketing, sales, and managerial/executive teams. The empirical evidence regarding judgment forecasts is that they are not as successful as statistically based forecasts. There are some -elds, such as -nancial investments, where there is considerable strong evidence that this is so. There are a number of reasons why we would expect judgment forecasts to be inferior to statistical methods.

Inconsistency, or changing one鈥檚 mind for no compelling or obvious reason, is a signi-cant source of judgment forecast errors. Formalizing the forecasting process through the use of analytical methods is one approach to eliminating inconsistency as a source of error. Formal decision rules that predict the variables of interest using relatively few inputs invariably predict better than humans, because humans are inconsistent over time in their choice of input factors to consider, and how to weight them.

Letting more recent events dominate one鈥檚 thinking, instead of weighting current and previous experience more evenly, is another source of judgment forecast errors. If these recent events are essentially random in nature, they can have undue impact on current forecasts. A good forecasting system will certainly monitor and evaluate recent events and experiences, but will only incorporate them into the forecasts if there is suf-cient evidence to indicate that they represent real effects.

Mistaking correlation for causality can also be a problem. This is the belief that two (or more) variables are related in a causal manner and taking action on this, when the variables exhibit only a correlation between them. It is not dif-cult to -nd correlative relationships; any two variables that are monotonically related will exhibit strong correlation. So company sales may appear to be related to some factor that over a short time period is moving synchronously with sales, but relying on this as a causal relationship will lead to problems. The statistical signi-cance of patterns and relationships does not necessarily imply a cause-and-effect relationship.

Judgment forecasts are often dominated by optimistic thinking. Most humans are naturally optimistic. An executive wants sales for the product line to increase because his/her bonus may depend on the results. A product manager wants his/her product to be successful. Sometimes bonus payouts are made for exceeding sales goals, and this can lead to unrealistically low forecasts, which in turn are used to set the goals. However, unrealistic forecasts, whether too high or too low, always result in problems downstream in the organization where forecast errors have meaningful impact on ef-ciency, effectiveness, and bottom-line results.

Humans are notorious for underestimating variability. Judgment forecasts rarely incorporate uncertainty in any formal way and, as a result, often underestimate its magnitude and impact. A judgment forecaster often completely fails to express any uncertainty in his/her forecast. Because all forecasts are wrong, one must have some understanding of the magnitude of forecast errors. Furthermore, planning for appropriate actions in the face of likely forecast error should be part of the decision-making process that is driven by the forecast. Statistical forecasting methods can be accompanied by prediction intervals. In our view, every forecast should be accompanied by a PI that adequately expresses for the decision maker how much uncertainty is associated with the point forecast.

In general, both the users of forecasts (decision makers) and the preparers (forecasters) have reasonably good awareness of many of the basic analytical forecasting techniques, such as exponential smoothing and regression-based methods. They are less familiar with time series models such as the ARIMA model, transfer function models, and other more sophisticated methods. Decision makers are often unsatis-ed with subjective and judgment methods and want better forecasts. They often feel that analytical methods can be helpful in this regard.

This leads to a discussion of expectations. What kind of results can one reasonably expect to obtain from analytical forecasting methods? By results, we mean forecast errors. Obviously, the results that a speci-c forecaster obtains are going to depend on the speci-c situation: what variables are being forecast, the availability and quality of data, the methods that can be applied to the problem, and the tools and expertise that are available. However, because there have been many surveys of both forecasters and users of forecasts, as well as forecast competitions (e.g., see Makridakis et al. (1993)) where many different techniques have been applied in headto-head challenges, some broad conclusions can be drawn.

In general, exponential smoothing type methods, including Winters鈥?method, typically experience mean absolute prediction errors ranging from $10 \%$ to $15 \%$ for lead-one forecasts. As the lead time increases, the

prediction error increases, with mean absolute prediction errors typically in the $1 7 - 2 5 \%$ range at lead times of six periods. At 12 period lead times, the mean absolute prediction error can range from $18 \%$ to $45 \%$ . More sophisticated time series models such as ARIMA models are not usually much better, with the mean absolute prediction error ranging from about $10 \%$ for lead-one forecasts, to about $17 \%$ for lead-six forecasts, and up to $25 \%$ for 12 period lead times. This probably accounts for some of the dissatisfaction that forecasters often express with the more sophisticated techniques; they can be much harder to use, but they do not have substantial payback in terms of reducing forecasting errors. Regression methods often produce mean absolute prediction errors ranging from $12 \%$ to $18 \%$ for lead-one forecasts. As the lead time increases, the prediction error increases, with mean absolute prediction errors typically in the $1 7 - 2 0 \%$ range for six period lead times. At 12 period lead times, the mean absolute prediction error can range from $20 \%$ to $2 5 \%$ . Seasonal time series are often easier to predict than nonseasonal ones, because seasonal patterns are relatively stable through time, and relatively simple methods such as Winters鈥?method and seasonal adjustment procedures typically work very well as forecasting techniques. Interestingly, seasonal adjustment techniques are not used nearly as widely as we would expect, given their relatively good performance.

When forecasting is done well in an organization, it is typically done by a group of individuals who have some training and experience in the techniques, have access to the right information, and have an opportunity to see how the forecasts are used. If higher levels of management routinely intervene in the process and use their judgment to modify the forecasts, it is highly desirable if the forecast preparers can interact with these managers to learn why the original forecasts require modi-cation. Unfortunately, in many organizations, forecasting is done in an informal way, and the forecasters are often marketing or sales personnel, or market researchers, for whom forecasting is only a (sometimes small) part of their responsibilities. There is often a great deal of turnover in these positions, and so no long-term experience base or continuity builds up. The lack of a formal, organized process is often a big part of the reason why forecasting is not as successful as it should be.

Any evaluation of a forecasting effort in an organization should consider at least the following questions:

1. What methods are being used? Are the methods appropriate to organizational needs, when planning horizons and other business issues are taken into account? Is there an opportunity to use more than

one forecasting procedure? Could forecasts be combined to improve results?

2. Are the forecasting methods being used correctly?   
3. Is an appropriate set of data being used in preparing the forecasts? Is data quality an issue? Are the underlying assumptions of the methods employed satis-ed at least well enough for the methods to be successful?   
4. Is uncertainty being addressed adequately? Are prediction intervals used as part of the forecast report? Do forecast users understand the PIs?   
5. Does the forecasting system take economic/market forces into account? Is there an ability to capitalize on current events, natural forces, and swings in customer preferences and tastes?   
6. Is forecasting separate from planning? Very often the forecast is really just a plan or schedule. For example, it may reect a production plan, not a forecast of what we could realistically expect to sell (i.e., demand). Many individuals do not understand the difference between a forecast and a plan.

In the short-to-medium term, most businesses can bene-t by taking advantage of the relative stability of seasonal patterns and the inertia present in most time series of interest. These are the methods we have focused on in this book.

# 7.11 R COMMANDS FOR CHAPTER 7

Example 7.11 The data for this example are in the array called pressure .data of which the two columns represent the viscosity and the temperature, respectively. To model the multivariate data we use the 鈥淰AR鈥?function in R package 鈥渧ars.鈥?But we -rst start with time series, acf, pacf, ccf plots as suggested in the example

library(vars)

pf<-pressure.data[,1]   
pb<-pressure.data[,2]   
plot(pf,type $=$ "o",pch $= 16$ ,cex $= .5$ ,xlab $\equiv$ 'Time'锛寉lab $\equiv$ 'Pressure'锛寉lim= c(4,25))   
lines(pb,type $=$ "o",pch $= 15$ ,cex $= .5$ 锛宑ol $\equiv$ "grey40")   
legend(1,7,c("Variable","Front","Back")锛?
pch $\scriptscriptstyle - \subset$ (NA,16,15),lwd ${ } = \mathbf { C }$ (NA,.5,.5),cex $\displaystyle =$ .55,col ${ } = { \mathsf { C } }$ ("black","black", "grey40"))

![](images/33ab1555234ea4e20ac167e9790962d7a815e79a02682358d0eb1df8a5cf3ec7.jpg)

res.pf<- as.vector(residuals(arima(pf,order ${ } = { \mathsf { C } }$ (1,0,0))))

res.pb<- as.vector(residuals(arima(pb,order ${ } = { \mathsf { C } }$ (1,0,0))))

par(mfrow ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = \mathbf { \boldsymbol { \mathbf { \mathit { C } } } }$ (2,2),oma=c(0,0,0,0))

acf(pf,lag.max=25,type $=$ "correlation",main $\cdot ^ { = }$ "ACF for Front Pressure")

acf(pb, lag.max $^ { = 2 5 }$ , type $: =$ "correlation",main $\cdot ^ { = }$ "PACF for Back Pressure",ylab $^ { 1 = }$ "PACF")

ccf(pb,pf,main='CCF of \nFront and Back Pressures',yla $^ { 1 = }$ 'CCF')

ccf(res.pb,res.pf,main $= 1$ CCF of Residuals for \nFront and Back Pressures',ylab $^ { \ast = }$ 'CCF')

![](images/ccf43d1e698951b350a3e4260a406eb5c50fef52ef83f1b3da704bd65c630a46.jpg)

![](images/1cd144075f36c42d24fddc1f47bc1658ea9be01e8ad7c0f8b3b866d149988e40.jpg)

![](images/049a70090802769df4581b4262228ea8ba5d14f2498b605bcce2d4c4d66452eb.jpg)

![](images/602760ed3cd6c23a6607944cbf92e0da0096a9af26beaa95d2f7023a71388cbc.jpg)

We now -t a VAR(1) model to the data using VAR function:

```txt
> pressure.var1<-VAR(pressure.data)  
> pressure.var1 
```

VAR Estimation Results:   
Estimated coefficients for equation pfront:   
Call:   
pfront $=$ pfront.l1 $^+$ pback.l1 $^+$ const   
pfront.l1 pback.l1 const 0.7329529 0.4735983 -6.7555089   
Estimated coefficients for equation pback:   
Call:   
pback $=$ pfront.l1 $^+$ pback.l1 $^+$ const   
pfront.l1 pback.l1 const 0.4104251 -0.5606308 27.2369791

Note that there is also a VARselect function that will automatically select the best p order of the VAR(p) model. In this case we tried p upto 5.

```txt
>VARselect(pressure.data,lag.max=5)   
$selection AIC(n) HQ(n) SC(n) FPE(n) 1 1 1   
$criteria 1 2 3 4 5 AIC(n) 0.05227715 0.09275642 0.1241682 0.1639090 0.1882266 HQ(n) 0.09526298 0.16439946 0.2244684 0.2928665 0.3458413 SC(n) 0.15830468 0.26946896 0.3715657 0.4819916 0.5769942 FPE(n) 1.05367413 1.09722530 1.1322937 1.1783005 1.2074691 
```

The output shows that VAR(1) was indeed the right choice. We now plot the residuals.

plot(residuals(pressure.var1),[1],type $=$ "p",pch $= 15$ ,cex $= .5$ ,xlab $\equiv$ 'Time'锛寉lab $=$ 'Residuals'锛寉lim=c(-3,3)) points(residuals(pressure.var1)[,2]锛宲ch $= 1$ ,cex $= .5$ 锛宑ol $=$ "grey40")

```txt
legend(100, 3, c("Residuals", "Front", "Back"),
pch=c(NA, 15, 1), lwd=c(NA, .5, .5), cex=.55, col=c("black", "black",
"grey40")) 
```

![](images/7bc418a3aae6c011aa1ffc5a7f900053f5429d4277097c05d2690e670846eaf2.jpg)

Example 7.12 The data for this example are in the array called sp500.data of which the two columns represent the date and the S&P 500 closing values respectively. To model the multivariate data we use the 鈥済arch鈥?function in R package 鈥渢series.鈥?But we -rst start with time series, acf, pacf plots as suggested in the example

library(tseries)  
sp<-ts(sp500.data[,2])  
logsp.d1<-diff(log(sp))  
T<-length(logsp.d1)  
plot(logsp.d1,type $=$ "o",pch $= 16$ ,cex $= .5$ ,xlab $\coloneqq$ 'Date'锛寉lab $\coloneqq$ 'log(SP(t))  
-log(SP(t-1))',xaxt $\coloneqq$ 'n')  
lablist<-as.vector(sp500.data[seq(1,T+1,40),1])  
axis(1锛宻eq(1,T+1,40)锛宭abels $\coloneqq$ FALSE)  
text(seq(1,T+1,40)锛宲ar("usr") [3]-.01锛?labels $\equiv$ lablist锛宻rt $= 45$ pos $= 1$ 锛寈pd $=$ TRUE)

![](images/8b40b94cade41d13e51fe6106e6da6eb6dbae5a679616f8ba3f0f8beccd06020.jpg)

# ACF and PACF of the first difference of the log transformation par(mfrow ${ \bf \Pi } = { \bf C }$ (1,2),oma ${ } = \mathbf { C }$ (0,0,0,0))

acf(logsp.d1,lag.max $^ { = 2 5 }$ ,type $=$ "correlation",main $. =$ "ACF of the wt") acf(logsp.d1, lag.max $^ { \cdot = 2 5 }$ ,type $: =$ "partial",mai $\cdot ^ { = }$ "PACF of the wt")

![](images/cd9759c5e7bd8f9728f560c03837c69e4ccdc0109887949c27760e846397b1d4.jpg)

![](images/a0fe7412e0436c653a9a61b4eb19b98a31deac20f8003d9fbcc326f39f2106fb.jpg)

# ACF and PACF of the square of the first difference of the log # transformation

par(mfrow ${ \bf \Phi } = { \bf C }$ (1,2),oma ${ } = { \mathsf { C } }$ (0,0,0,0))

acf(logsp.d1藛2,lag.max $^ { = 2 5 }$ ,type $=$ "correlation",main $\cdot ^ { = }$ "ACF of the wt藛2")

acf(logsp.d1藛2, lag.max $_ { : = 2 5 }$ ,typ $=$ "partial",mai $\cdot ^ { = }$ "PACF of the wt藛2")

![](images/6b6e07026a517c876fed42953b14bd0518f7eaf5cd9fa42c0ba3895505672218.jpg)

![](images/dd71db184593d4f2969c09c56e425fafd3b6bff049335aac76632a07e21f8c8e.jpg)

```txt
Fit a GARCH(0,3) model  
sp. arch3<-garch(logsp.d1, order = c(0,3), trace = FALSE)  
summary(sp. arch3)  
Call:  
    garch(x = logsp.d1, order = c(0,3), trace = FALSE)  
Model:  
    GARCH(0,3)  
Residuals:  
    Min 1Q Median 3Q Max  
-2.0351 -0.4486 0.3501 0.8869 2.9320  
Coefficient(s):  
    Estimate Std. Error t value Pr(>|t|)  
a0 2.376e-04 6.292e-05 3.776 0.000159 ***  
a1 2.869e-15 1.124e-01 0.000 1.000000  
a2 7.756e-02 8.042e-02 0.964 0.334876  
a3 9.941e-02 1.124e-01 0.884 0.376587  
Signif. codes: 0 '***' 0.001 '**' 0.01 '**' 0.05 '.' 0.1 '1'  
Diagnostic Tests:  
    Jarque Bera Test  
data: Residuals  
X-squared = 0.4316, df = 2, p-value = 0.8059  
Box-Ljung test  
data: Squared.Residuals  
X-squared = 2.2235, df = 1, p-value = 0.1359 
```

# EXERCISES

7.1 Show that an AR(2) model can be represented in state space form.   
7.2 Show that an MA(1) model can be written in state space form.   
7.3 Consider the information on weekly spare part demand shown in Table E7.1. Suppose that 74 requests for 55 parts are received during the current week, T. Find the new cumulative distribution of demand. Use $\lambda = 0 . 1$ . What is your forecast of the 70th percentile of the demand distribution?   
7.4 Consider the information on weekly luxury car rentals shown in Table E7.2. Suppose that 37 requests for rentals are received during

TABLE E7.1 Spare Part Demand Information for Exercise 7.3   

<table><tr><td>k</td><td>Bk-1</td><td>Bk</td><td>藛pk(T-1)</td><td>F(Bk), at the end of week T-1</td></tr><tr><td>0</td><td>0</td><td>5</td><td>0.02</td><td>0.02</td></tr><tr><td>1</td><td>5</td><td>10</td><td>0.03</td><td>0.05</td></tr><tr><td>2</td><td>10</td><td>15</td><td>0.04</td><td>0.09</td></tr><tr><td>3</td><td>15</td><td>20</td><td>0.05</td><td>0.14</td></tr><tr><td>4</td><td>20</td><td>25</td><td>0.08</td><td>0.22</td></tr><tr><td>5</td><td>25</td><td>30</td><td>0.09</td><td>0.31</td></tr><tr><td>6</td><td>30</td><td>35</td><td>0.12</td><td>0.43</td></tr><tr><td>7</td><td>35</td><td>40</td><td>0.17</td><td>0.60</td></tr><tr><td>8</td><td>45</td><td>50</td><td>0.21</td><td>0.81</td></tr><tr><td>9</td><td>50</td><td>55</td><td>0.11</td><td>0.92</td></tr><tr><td>10</td><td>55</td><td>60</td><td>0.08</td><td>1.00</td></tr></table>

the current week, T. Find the new cumulative distribution of demand. Use $\lambda = 0 . 1$ . What is your forecast of the 90th percentile of the demand distribution?

7.5 Rework Exercise 7.3 using $\lambda = 0 . 4$ . How much difference does changing the value of the smoothing parameter make in your estimate of the 70th percentile of the demand distribution?   
7.6 Rework Exercise 7.4 using $\lambda = 0 . 4$ . How much difference does changing the value of the smoothing parameter make in your estimate of the 70th percentile of the demand distribution?

TABLE E7.2 Luxury Car Rental Demand Information for Exercise 7.4   

<table><tr><td>k</td><td>Bk-1</td><td>Bk</td><td>藛pk(T-1)</td><td>F(Bk), at the end of week T-1</td></tr><tr><td>0</td><td>0</td><td>5</td><td>0.06</td><td>0.06</td></tr><tr><td>1</td><td>5</td><td>10</td><td>0.07</td><td>0.13</td></tr><tr><td>2</td><td>10</td><td>15</td><td>0.08</td><td>0.21</td></tr><tr><td>3</td><td>15</td><td>20</td><td>0.09</td><td>0.30</td></tr><tr><td>4</td><td>20</td><td>25</td><td>0.15</td><td>0.45</td></tr><tr><td>5</td><td>25</td><td>30</td><td>0.22</td><td>0.67</td></tr><tr><td>6</td><td>30</td><td>35</td><td>0.24</td><td>0.91</td></tr><tr><td>7</td><td>35</td><td>40</td><td>0.05</td><td>0.96</td></tr><tr><td>8</td><td>45</td><td>50</td><td>0.04</td><td>1.00</td></tr></table>

7.7 Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are $\sigma _ { 1 } ^ { 2 } = 1 0$ and $\sigma _ { 2 } ^ { 2 } =$ 25. If the correlation coef-cient $\rho = - 0 . 7 5$ , calculate the optimum value of the weight used to optimally combine the two individual forecasts. What is the variance of the combined forecast?   
7.8 Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are $\sigma _ { 1 } ^ { 2 } = 1 5$ and $\sigma _ { 2 } ^ { 2 } =$ 20. If the correlation coef-cient $\rho = - 0 . 4$ , calculate the optimum value of the weight used to optimally combine the two individual forecasts. What is the variance of the combined forecast?   
7.9 Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are $\sigma _ { 1 } ^ { 2 } = 8$ and $\sigma _ { 2 } ^ { 2 } =$ 16. If the correlation coef-cient $\rho = - 0 . 3$ , calculate the optimum value of the weight used to optimally combine the two individual forecasts. What is the variance of the combined forecast?   
7.10 Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are $\sigma _ { 1 } ^ { 2 } = 1$ and $\sigma _ { 2 } ^ { 2 } = 8$ . If the correlation coef-cient $\rho = - 0 . 6 5$ , calculate the optimum value of the weight used to optimally combine the two individual forecasts. What is the variance of the combined forecast?   
7.11 Rework Exercise 7.8 assuming that $\rho = 0 . 4$ . What effect does changing the sign of the correlation coef-cient have on the weight used to optimally combine the two forecasts? What is the variance of the combined forecast?   
7.12 Rework Exercise 7.9 assuming that $\rho = 0 . 3$ . What effect does changing the sign of the correlation coef-cient have on the weight used to optimally combine the two forecasts? What is the variance of the combined forecast?   
7.13 Suppose that there are three lead-one forecasts available for a time series, and the covariance matrix of the three forecasts is as follows:

$$
\boldsymbol {\Sigma} _ {T + 1} (T) = \left[ \begin{array}{c c c} 1 0 & - 4 & - 2 \\ - 4 & 6 & - 3 \\ - 2 & - 3 & 1 5 \end{array} \right]
$$

Find the optimum weights for combining these three forecasts. What is the variance of the combined forecast?

7.14 Suppose that there are three lead-one forecasts available for a time series, and the covariance matrix of the three forecasts is as follows:

$$
\boldsymbol {\Sigma} _ {T + 1} (T) = \left[ \begin{array}{c c c} 8 & - 2 & - 1 \\ - 2 & 3 & - 2 \\ - 1 & - 2 & 1 0 \end{array} \right]
$$

Find the optimum weights for combining these three forecasts. What is the variance of the combined forecast?

7.15 Table E7.3 presents 25 forecast errors for two different forecasting techniques applied to the same time series. Is it possible to combine the two forecasts to improve the forecast errors? What is the optimum weight for combining the forecasts? What is the variance of the combined forecast?

TABLE E7.3 Forecast Errors for Exercise 7.15   

<table><tr><td>Time period</td><td>Forecast errors, method 1</td><td>Forecast errors, method 2</td></tr><tr><td>1</td><td>-0.78434</td><td>6.9668</td></tr><tr><td>2</td><td>-0.31111</td><td>4.5512</td></tr><tr><td>3</td><td>2.15622</td><td>-1.2681</td></tr><tr><td>4</td><td>-1.81293</td><td>6.8967</td></tr><tr><td>5</td><td>-0.77498</td><td>1.6574</td></tr><tr><td>6</td><td>2.31673</td><td>-8.7601</td></tr><tr><td>7</td><td>-0.94866</td><td>0.7472</td></tr><tr><td>8</td><td>0.81314</td><td>-0.7457</td></tr><tr><td>9</td><td>-2.95718</td><td>-0.5355</td></tr><tr><td>10</td><td>0.08175</td><td>-1.3458</td></tr><tr><td>11</td><td>1.08915</td><td>-5.8232</td></tr><tr><td>12</td><td>-0.20637</td><td>1.2722</td></tr><tr><td>13</td><td>0.57157</td><td>-2.4561</td></tr><tr><td>14</td><td>0.41435</td><td>4.3111</td></tr><tr><td>15</td><td>0.47138</td><td>5.9894</td></tr><tr><td>16</td><td>1.23274</td><td>-6.8757</td></tr><tr><td>17</td><td>-0.66288</td><td>1.5996</td></tr><tr><td>18</td><td>1.71193</td><td>10.5031</td></tr><tr><td>19</td><td>-2.00317</td><td>9.8664</td></tr><tr><td>20</td><td>-2.87901</td><td>3.0399</td></tr><tr><td>21</td><td>-2.87901</td><td>14.1992</td></tr><tr><td>22</td><td>-0.16103</td><td>9.0080</td></tr><tr><td>23</td><td>2.12427</td><td>-0.4551</td></tr><tr><td>24</td><td>0.60598</td><td>0.7123</td></tr><tr><td>25</td><td>0.18259</td><td>1.7346</td></tr></table>

7.16 Show that when combining two forecasts, if the correlation between the two sets of forecast errors is $\rho = \sigma _ { 1 } / \sigma _ { 2 }$ , then Min Var $[ e _ { T + \tau } ^ { \mathrm { c } } ( T ) ] =$ $\sigma _ { 1 } ^ { 2 }$ , where $\sigma _ { 1 } ^ { 2 }$ is the smaller of the two forecast error variances.   
7.17 Show that when combining two forecasts, if the correlation between the two sets of forecast errors is $\rho = 0$ , then Var $[ e _ { T + \tau } ^ { \mathrm { c } } ( T ) ] =$ $\sigma _ { 1 } ^ { 2 } \sigma _ { 2 } ^ { 2 } / ( \sigma _ { 1 } ^ { 2 } + \sigma _ { 2 } ^ { 2 } )$ .   
7.18 Let $y _ { t }$ be an IMA(1, 1) time series with parameter $\theta = 0 . 4$ . Suppose that this time series is observed with an additive white noise error.

a. What is the model form of the observed error?   
b. Find the parameters of the observed time series, assuming that the variances of the errors in the original time series and the white noise are equal.

7.19 Show that an AR(1) time series that is observed with an additive white noise error is an ARMA(1, 1) process.   
7.20 Generate 100 observations of an ARIMA(1, 1, 0) time series. Add 100 observations of white noise to this time series. Calculate the sample ACF and sample PACF of the new time series. Identify the model form and estimate the parameters.   
7.21 Generate 100 observations of an ARIMA(1, 1, 0) time series. Generate another 100 observations of an AR(1) time series and add these observations to the original time series. Calculate the sample ACF and sample PACF of the new time series. Identify the model form and estimate the parameters.   
7.22 Generate 100 observations of an AR(2) time series. Generate another 100 observations of an AR(1) time series and add these observations to the original time series. Calculate the sample ACF and sample PACF of the new time series. Identify the model form and estimate the parameters.   
7.23 Generate 100 observations of an MA(2) time series. Generate another 100 observations of an MA(1) time series and add these observations to the original time series. Calculate the sample ACF and sample PACF of the new time series. Identify the model form and estimate the parameters.   
7.24 Table E7.4 presents data on the type of heating fuel used in new single-family houses built in the United States from 1971 through

TABLE E7.4 Data for Exercise 7.24   

<table><tr><td rowspan="2">Year</td><td colspan="5">Number of Houses (in thousands)</td></tr><tr><td>Total</td><td>Gas</td><td>Electricity</td><td>Oil</td><td>Other types or none</td></tr><tr><td>1971</td><td>1014</td><td>605</td><td>313</td><td>83</td><td>15</td></tr><tr><td>1972</td><td>1143</td><td>621</td><td>416</td><td>93</td><td>13</td></tr><tr><td>1973</td><td>1197</td><td>560</td><td>497</td><td>125</td><td>16</td></tr><tr><td>1974</td><td>940</td><td>385</td><td>458</td><td>85</td><td>11</td></tr><tr><td>1975</td><td>875</td><td>347</td><td>429</td><td>82</td><td>18</td></tr><tr><td>1976</td><td>1034</td><td>407</td><td>499</td><td>110</td><td>19</td></tr><tr><td>1977</td><td>1258</td><td>476</td><td>635</td><td>120</td><td>28</td></tr><tr><td>1978</td><td>1369</td><td>511</td><td>710</td><td>109</td><td>40</td></tr><tr><td>1979</td><td>1301</td><td>512</td><td>662</td><td>86</td><td>41</td></tr><tr><td>1980</td><td>957</td><td>394</td><td>482</td><td>29</td><td>52</td></tr><tr><td>1981</td><td>819</td><td>339</td><td>407</td><td>16</td><td>57</td></tr><tr><td>1982</td><td>632</td><td>252</td><td>315</td><td>17</td><td>48</td></tr><tr><td>1983</td><td>924</td><td>400</td><td>448</td><td>22</td><td>53</td></tr><tr><td>1984</td><td>1025</td><td>460</td><td>492</td><td>24</td><td>49</td></tr><tr><td>1985</td><td>1072</td><td>466</td><td>528</td><td>36</td><td>42</td></tr><tr><td>1986</td><td>1120</td><td>527</td><td>497</td><td>52</td><td>45</td></tr><tr><td>1987</td><td>1123</td><td>583</td><td>445</td><td>58</td><td>38</td></tr><tr><td>1988</td><td>1085</td><td>587</td><td>402</td><td>60</td><td>36</td></tr><tr><td>1989</td><td>1026</td><td>596</td><td>352</td><td>50</td><td>28</td></tr><tr><td>1990</td><td>966</td><td>573</td><td>318</td><td>48</td><td>27</td></tr><tr><td>1991</td><td>838</td><td>505</td><td>267</td><td>37</td><td>29</td></tr><tr><td>1992</td><td>964</td><td>623</td><td>283</td><td>36</td><td>22</td></tr><tr><td>1993</td><td>1039</td><td>682</td><td>303</td><td>34</td><td>20</td></tr><tr><td>1994</td><td>1160</td><td>772</td><td>333</td><td>39</td><td>16</td></tr><tr><td>1995</td><td>1066</td><td>708</td><td>305</td><td>37</td><td>16</td></tr><tr><td>1996</td><td>1129</td><td>781</td><td>299</td><td>37</td><td>11</td></tr><tr><td>1997</td><td>1116</td><td>771</td><td>296</td><td>38</td><td>11</td></tr><tr><td>1998</td><td>1160</td><td>809</td><td>307</td><td>34</td><td>10</td></tr><tr><td>1999</td><td>1270</td><td>884</td><td>343</td><td>35</td><td>9</td></tr><tr><td>2000</td><td>1242</td><td>868</td><td>329</td><td>37</td><td>8</td></tr><tr><td>2001</td><td>1256</td><td>875</td><td>336</td><td>35</td><td>9</td></tr><tr><td>2002</td><td>1325</td><td>907</td><td>371</td><td>38</td><td>10</td></tr><tr><td>2003</td><td>1386</td><td>967</td><td>377</td><td>31</td><td>12</td></tr><tr><td>2004</td><td>1532</td><td>1052</td><td>440</td><td>29</td><td>10</td></tr><tr><td>2005</td><td>1636</td><td>1082</td><td>514</td><td>31</td><td>9</td></tr></table>

2005. Develop an appropriate multivariate time series model for the gas, electricity, and oil time series.

7.25 Reconsider the data on heating fuel in Table E7.4. Suppose that you are interested in forecasting the aggregate series (the Total column in Table E7.4). One way to do this is to forecast the total directly. Another way is to forecast the individual component series and sum the forecasts of the components to obtain a forecast for the total. Investigate these approaches for this data and report on your conclusions.   
7.26 Reconsider the data on heating fuel in Table E7.4. Suppose that you are interested in forecasting the four individual components series (the Gas, Electricity, Oil, and Other Types columns in Table E7.4). One way to do this is to forecast the individual time series directly. Another way is to forecast the total and obtain forecasts of the individual component series by decomposing the forecast for the totals into component parts. Investigate these approaches for this data and report on your conclusions.   
7.27 Table E7.5 contains data on property crimes reported to the police in the United States. Both the number of property crimes and the crime rate per 100,000 individuals are shown. Using the data on the number of crimes reported, develop an appropriate multivariate time series model for the burglary, larceny-theft, and motor vehicle theft time series.   
7.28 Repeat Exercise 7.27 using the property crime rate data. Compare the models obtained using the number of crimes reported versus the crime rate.   
7.29 Reconsider the data on property crimes in Table E7.5. Suppose that you are interested in forecasting the aggregate crime rate series. One way to do this is to forecast the total directly. Another way is to forecast the individual component series and sum the forecasts of the components to obtain a forecast for the total. Investigate these approaches for this data and report on your conclusions.   
7.30 Reconsider the data on property crimes in Table E7.5. Suppose that you are interested in forecasting the four individual component series (the Burglary, Larceny-Theft, and Motor Vehicle Theft columns in Table E7.5). One way to do this is to forecast the individual time series directly. Another way is to forecast the total

and obtain forecasts of the individual component series by decomposing the forecast for the totals into component parts. Investigate these approaches using the crime rate data, and report on your conclusions.

7.31 Table B.1 contains data on market yield of US treasury securities at 10-year constant maturity. Compute the spectrum for this time series. What features of the time series are apparent from examination of the spectrum?   
7.32 Table B.3 contains on the viscosity of a chemical product. Compute the spectrum for this time series. What features of the time series are apparent from examination of the spectrum?   
7.33 Table B.6 contains the global mean surface air temperature anomaly and the global $\mathrm { C O } _ { 2 }$ concentration data. Compute the spectrum for both of these time series. What features of the two time series are apparent from examination of the spectrum?   
7.34 Table B.11 contains sales data on sales of Champagne. Compute the spectrum for this time series. Is the seasonal nature of the time series apparent from examination of the spectrum?   
7.35 Table B.21 contains data on the average monthly retail price of electricity in the residential sector for Arizona from 2001 through 2014. Take the -rst difference of this timer series and compute the spectrum. Is the seasonal nature of the time series apparent from examination of the spectrum?   
7.36 Table B.22 contains data on Danish crude oil production. Compute the spectrum for this time series. What features of the time series are apparent from examination of the spectrum?   
7.37 Table B.23 contains data on positive test results for inuenza in the US. Compute the spectrum for this time series. Is the seasonal nature of the time series apparent from examination of the spectrum?   
7.38 Table B.24 contains data on monthly mean daily solar radiation. Compute the spectrum for this time series. Is the seasonal nature of the time series apparent from examination of the spectrum?   
7.39 Table B.27 contains data on airline on-time arrival performance. Compute the spectrum for this time series. Is there any evidence of seasonality in the time series that is apparent from examination of the spectrum?

7.40 Table B.28 contains data on US automobile manufacturing shipments. Compute the spectrum for this time series. Is there any evidence of seasonality in the time series that is apparent from examination of the spectrum?

7.41 Weekly demand for a spare part is assumed to follow a Poisson distribution:

$$
f (y | \lambda) = \frac {e ^ {- \lambda} \lambda^ {y}}{y !}, \quad y = 0, 1, \ldots
$$

The mean $\lambda$ of the demand distribution is assumed to be a random variable with a gamma distribution

$$
h (\lambda) = \frac {b ^ {a}}{(a - 1) !} \lambda^ {a - 1} e ^ {- b \lambda}, \quad \lambda > 0
$$

where $a$ and $^ b$ are parameters having subjectively determined values. In the week following the establishment of this prior distribution $d$ parts were demanded. What is the posterior distribution of 饾渾?

# STATISTICAL TABLES

Table A.1 Cumulative Standard Normal Distribution

Table A.2 Percentage Points $t _ { \alpha , \nu }$ of the $t$ Distribution

Table A.3 Percentage Points $\chi _ { \alpha , \nu } ^ { 2 }$ of the Chi-Square Distribution

Table A.4 Percentage Points $f _ { \alpha , \mu , \nu }$ of the $\pmb { F }$ Distribution

Table A.5 Critical Values of the Durbin鈥揥atson Statistic

$$
\Phi (z) = P (Z \leq z) = \int_ {- \infty} ^ {z} \frac {1}{\sqrt {2 \pi}} e ^ {- u ^ {2} / 2} d u
$$
