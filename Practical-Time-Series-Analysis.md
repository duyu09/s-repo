# Practical Time Series Analysis

Prediction with Statistics & Machine Learning

![](images/e1a8899cdd3bf3d67b6d1d63a728a804d08a5b058ef1c3344e4f96f6486ea5f1.jpg)

Aileen Nielsen

# Practical Time Series Analysis

Time series data analysis is increasingly important due to the massive production of such data through the internet of things, the digitalization of healthcare, and the rise of smart cities. As continuous monitoring and data collection become more common, the need for competent time series analysis with both statistical and machine learning techniques will increase.

Covering innovations in time series data analysis and use cases from the real world, this practical guide will help you solve the most common data engineering and analysis challenges in time series, using both traditional statistical and modern machine learning techniques. Author Aileen Nielsen offers an accessible, well-rounded introduction to time series in both R and Python that will have data scientists, software engineers, and researchers up and running quickly.

You’ll get the guidance you need to condently:

• Find and wrangle time series data   
• Undertake exploratory time series data analysis   
• Store temporal data   
• Simulate time series data   
• Generate and select features for a time series   
• Measure error   
• Forecast and classify time series with machine or deep learning   
• Evaluate accuracy and performance

“An outstanding resource. At some point, every data scientist starts working with time series or other kinds of functional data. This book provides a high-level overview of machine learning and time series analysis, and features well-designed implementation examples.”

— Dr. Andreas W. Kempa-Liehr

Senior Lecturer,

Department of Engineering Science,

University of Auckland

Aileen Nielsen is a New York City–based software engineer and data analyst. She has worked on time series in a variety of disciplines, from a healthcare startup to a political campaign, from a physics research lab to a nancial trading rm. She currently develops neural networks for forecasting applications.

AI / DATA MODELING

US $69.99

CAN $92.99

ISBN: 978-1-492-04165-8

![](images/469a0885e4ec75aa2a432e63e2a07d22f4437314678bdd04efd6ac11e477dfd3.jpg)

![](images/badf6a7a97c5c149ca8c812107a526d4ae91123c65c0580b296421c0a6edeb0d.jpg)

Twitter: @oreillymedia facebook.com/oreilly

# Practical Time Series Analysis

Prediction with Statistics and

Machine Learning

Aileen Nielsen

# Practical Time Series Analysis

by Aileen Nielsen

Copyright $^ ©$ 2020 Aileen Nielsen. All rights reserved.

Printed in the United States of America.

Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.

O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institu‐ tional sales department: 800-998-9938 or corporate@oreilly.com.

Development Editor: Jeff Bleiel

Acquisitions Editor: Jonathan Hassell

Production Editor: Katherine Tozer

Copyeditor: Rachel Monaghan

Proofreader: Sonia Saruba

Indexer: WordCo Indexing Services, Inc.

Interior Designer: David Futato

Cover Designer: Karen Montgomery

Illustrator: Rebecca Demarest

October 2019:

First Edition

# Revision History for the First Edition

2019-09-20: First Release

See http://oreilly.com/catalog/errata.csp?isbn=9781492041658 for release details.

The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Practical Time Series Analysis, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.

The views expressed in this work are those of the author, and do not represent the publisher’s views. While the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.

# Table of Contents

# Preface. . . ix

# 1. Time Series: An Overview and a Quick History. . . . . . . .

The History of Time Series in Diverse Applications 2

Medicine as a Time Series Problem 2

Forecasting Weather 6

Forecasting Economic Growth 7

Astronomy 9

Time Series Analysis Takes Off 10

The Origins of Statistical Time Series Analysis 12

The Origins of Machine Learning Time Series Analysis 13

More Resources 13

# 2. Finding and Wrangling Time Series Data. . . . . . . . 17

Where to Find Time Series Data 18

Prepared Data Sets 18

Found Time Series 25

Retrofitting a Time Series Data Collection from a Collection of Tables 26

A Worked Example: Assembling a Time Series Data Collection 27

Constructing a Found Time Series 33

Timestamping Troubles 35

Whose Timestamp? 35

Guesstimating Timestamps to Make Sense of Data 36

What’s a Meaningful Time Scale? 39

Cleaning Your Data 40

Handling Missing Data 40

Upsampling and Downsampling 52

Smoothing Data 55

Seasonal Data 60

Time Zones 63

Preventing Lookahead 67

More Resources 69

# 3. Exploratory Data Analysis for Time Series. . . . . . 73

Familiar Methods 73

Plotting 74

Histograms 77

Scatter Plots 78

Time Series–Specific Exploratory Methods 81

Understanding Stationarity 82

Applying Window Functions 86

Understanding and Identifying Self-Correlation 91

Spurious Correlations 102

Some Useful Visualizations 104

1D Visualizations 104

2D Visualizations 105

3D Visualizations 113

More Resources 117

# 4. Simulating Time Series Data. . . . . . 119

What’s Special About Simulating Time Series? 120

Simulation Versus Forecasting 121

Simulations in Code 121

Doing the Work Yourself 122

Building a Simulation Universe That Runs Itself 128

A Physics Simulation 134

Final Notes on Simulations 140

Statistical Simulations 141

Deep Learning Simulations 141

More Resources 142

# 5. Storing Temporal Data. . . . 143

Defining Requirements 145

Live Data Versus Stored Data 146

Database Solutions 148

SQL Versus NoSQL 149

Popular Time Series Database and File Solutions 152

File Solutions 157

NumPy 158

Pandas 158

Standard R Equivalents 158

Xarray 159

More Resources 160

# 6. Statistical Models for Time Series. . . . . 163

Why Not Use a Linear Regression? 163

Statistical Methods Developed for Time Series 166

Autoregressive Models 166

Moving Average Models 181

Autoregressive Integrated Moving Average Models 186

Vector Autoregression 196

Variations on Statistical Models 201

Advantages and Disadvantages of Statistical Methods for Time Series 203

More Resources 204

# 7. State Space Models for Time Series. . . . . . . 207

State Space Models: Pluses and Minuses 209

The Kalman Filter 210

Overview 210

Code for the Kalman Filter 212

Hidden Markov Models 218

How the Model Works 218

How We Fit the Model 220

Fitting an HMM in Code 224

Bayesian Structural Time Series 229

Code for bsts 230

More Resources 235

# 8. Generating and Selecting Features for a Time Series. . . . . . . 239

Introductory Example 240

General Considerations When Computing Features 241

The Nature of the Time Series 242

Domain Knowledge 242

External Considerations 243

A Catalog of Places to Find Features for Inspiration 243

Open Source Time Series Feature Generation Libraries 244

Domain-Specific Feature Examples 249

How to Select Features Once You Have Generated Them 252

Concluding Thoughts 255

More Resources 256

# 9. Machine Learning for Time Series. . . . . . . 259

Time Series Classification 260

Selecting and Generating Features 260

Decision Tree Methods 264

Clustering 272

Generating Features from the Data 273

Temporally Aware Distance Metrics 280

Clustering Code 285

More Resources 287

# 10. Deep Learning for Time Series. . . . . . 289

Deep Learning Concepts 292

Programming a Neural Network 294

Data, Symbols, Operations, Layers, and Graphs 294

Building a Training Pipeline 298

Inspecting Our Data Set 299

Steps of a Training Pipeline 303

Feed Forward Networks 318

A Simple Example 319

Using an Attention Mechanism to Make Feed Forward

Networks More Time-Aware 322

CNNs 324

A Simple Convolutional Model 325

Alternative Convolutional Models 327

RNNs 329

Continuing Our Electric Example 332

The Autoencoder Innovation 334

Combination Architectures 335

Summing Up 340

More Resources 341

# 11. Measuring Error. . . . . . 343

The Basics: How to Test Forecasts 344

Model-Specific Considerations for Backtesting 347

When Is Your Forecast Good Enough? 348

Estimating Uncertainty in Your Model with a Simulation 350

Predicting Multiple Steps Ahead 353

Fit Directly to the Horizon of Interest 353

Recursive Approach to Distant Temporal Horizons 354

Multitask Learning Applied to Time Series 354

Model Validation Gotchas 355

More Resources 355

# 12. Performance Considerations in Fitting and Serving Time Series Models. . . . . . . . . . . . 357

Working with Tools Built for More General Use Cases 358

Models Built for Cross-Sectional Data Don’t “Share” Data Across Samples 358

Models That Don’t Precompute Create Unnecessary Lag Between Measuring Data and Making a Forecast 360

Data Storage Formats: Pluses and Minuses 361

Store Your Data in a Binary Format 361

Preprocess Your Data in a Way That Allows You to “Slide” Over It 362

Modifying Your Analysis to Suit Performance Considerations 362

Using All Your Data Is Not Necessarily Better 363

Complicated Models Don’t Always Do Better Enough 363

A Brief Mention of Alternative High-Performance Tools 364

More Resources 365

# 13. Healthcare Applications. . . . . . 367

Predicting the Flu 367

A Case Study of Flu in One Metropolitan Area 367

What Is State of the Art in Flu Forecasting? 383

Predicting Blood Glucose Levels 384

Data Cleaning and Exploration 385

Generating Features 390

Fitting a Model 396

More Resources 401

# 14. Financial Applications. . . . 403

Obtaining and Exploring Financial Data 404

Preprocessing Financial Data for Deep Learning 410

Adding Quantities of Interest to Our Raw Values 410

Scaling Quantities of Interest Without a Lookahead 411

Formatting Our Data for a Neural Network 413

Building and Training an RNN 416

More Resources 423

# 15. Time Series for Government. . . . 425

Obtaining Governmental Data 426

Exploring Big Time Series Data 428

Upsample and Aggregate the Data as We Iterate Through It 431

Sort the Data 432

Online Statistical Analysis of Time Series Data 436

Remaining Questions 446

Further Improvements 446

More Resources 447

# 16. Time Series Packages. . . . . . 449

Forecasting at Scale 449

Google’s Industrial In-house Forecasting 450

Facebook’s Open Source Prophet Package 452

Anomaly Detection 457

Twitter’s Open Source AnomalyDetection Package 457

Other Time Series Packages 460

More Resources 461

# 17. Forecasts About Forecasting. . . . . . . 463

Forecasting as a Service 463

Deep Learning Enhances Probabilistic Possibilities 464

Increasing Importance of Machine Learning Rather Than Statistics 465

Increasing Combination of Statistical and Machine Learning Methodologies 466

More Forecasts for Everyday Life 466

# Index. . . . 469

# Preface

Weather, stock markets, and heartbeats. They all form time series. If you’re interested in diverse data and forecasting the future, you’re interested in time series analysis.

Welcome to Practical Time Series Analysis! If you picked this book up, you’ve proba‐ bly already noticed that time series data is everywhere. Time series data grows increasingly ubiquitous and important as the big data ecosystem expands. For better or worse, sensors and tracking mechanisms are everywhere, and as a result there are unprecedented amounts of high-quality time series data available. Time series are uniquely interesting because they can address questions of causality, trends, and the likelihood of future outcomes. This book will take you through the major techniques that are most commonly applied to time series to address such questions.

Time series data spans a wide range of disciplines and use cases. It can be anything from customer purchase histories to conductance measurements of a nano-electronic system to digital recordings of human language. One point we discuss throughout the book is that time series analysis applies to a surprisingly diverse set of data. Any data that has an ordered axis can be analyzed with time series methods, even if that ordered axis is not time per se. Traditional time series data, such as stock data and weather patterns, can be analyzed with time series methods, but so can quirky data sets such as spectrographs of wine, where the “time” axis is actually an axis of fre‐ quency. Time series are everywhere.

# Who Should Read This Book

There are two kinds of intended readers for this book. The first and larger category of reader is that of a data scientist who has rarely worked with time series data. This per‐ son could be an industry veteran or a junior analyst. The more experienced data ana‐ lyst can skim the introductory conceptual areas of each chapter but will still benefit from this book’s discussions about best practices as well as pitfalls of working with time series data. A newer data analyst might consider working through the book in its entirety, although I have tried to keep each topic as self-contained as possible.

The second category of reader is someone supervising analytics at an organization with an extensive in-house data collection. If you are a member of this group, you will still need some technical background, but it’s not necessary that you be currently coding in your professional life. For such a reader, this book is useful to point out opportunities for your organization to use time series analysis even if it is not cur‐ rently practiced in-house. This book will point you to new kinds of questions and analyses your organization can address with your existing data resources.

# Expected Background

With respect to coding, you should have some familiarity with R and Python, espe‐ cially with certain fundamental packages (in Python: NumPy, Pandas, and scikitlearn; and in R: data.table). The code samples should be readable even without all the background, but in that case you may need to take a short detour to familiarize yourself with these packages. This is most likely the case with respect to R’s data.table, an underused but highly performant data frame package that has fantastic time functionality.

In all cases, I have provided brief overviews of the related packages, some example code, and descriptions of what the code does. I also point the reader toward more complete overviews of the most used packages.

With respect to statistics and machine learning, you should have some familiarity with:

Introductory statistics

Ideas such as variance, correlation, and probability distributions

Machine learning

Clustering and decision trees

Neural networks

What they are and how they are trained

For these cases, I provide a brief overview of such concepts within the text, but the uninitiated should read more deeply about them before continuing with some chap‐ ters. For most topics, I provide links to recommended free online resources for brief tutorials on the fundamentals of a given topic or technique.

# Why I Wrote This Book

I wrote this book for three reasons.

First, time series is an important aspect of data analysis but one that is not found in the standard data science toolkit. This is unfortunate both because time series data is increasingly available and also because it answers questions that cross-sectional data

cannot. An analyst who does not know fundamental time series analysis is not mak‐ ing the most of their data. I hoped that this book could fill an existing and important void.

Second, when I started writing this book, I knew of no centralized overview of the most important aspects of time series analysis from a modern data science perspec‐ tive. There are many excellent resources available for traditional time series analysis, most notably in the form of classic textbooks on statistical time series analysis. There are also many excellent individual blog posts on both traditional statistical methods and on machine learning or neural network approaches to time series. However, I could not identify a single centralized resource to outline all these topics and relate them to one another. The goal of this book is to provide that resource: a broad, modern, and practical overview of time series analysis covering the full pipeline for time series data and modeling. Again, I hoped that this book could fill an existing and important void.

Third, time series is an interesting topic with quirky data concerns. Problems associ‐ ated with data leakage, lookahead, and causality are particularly fun from a time ser‐ ies perspective, as are many techniques that apply uniquely to data ordered along some kind of time axis. Surveying these topics broadly and finding a way to catalog them was another motivation to write this book.

# Navigating This Book

This book is organized roughly as follows:

# History

Chapter 1 presents a history of time series forecasting, all the way from the ancient Greeks to modern times. This puts our learning in context so we can understand what a traditional and storied discipline we study in this book.

# All about the data

Chapters 2, 3, 4, and 5 tackle problems relating to obtaining, cleaning, simulat‐ ing, and storing time series data. These chapters are tied together by their con‐ cern for all that comes before you can actually perform time series analysis. Such topics are rarely discussed in existing resources but are important in most data pipelines. Data identification and cleaning represent a large portion of the work done by most time series analysts.

# Models, models, models

Chapters 6, 7, 8, 9, and 10 cover a variety of modeling techniques that can be used for time series analysis. We start with two chapters on statistical methods, covering standard statistical models, such as ARIMA and Bayesian state space models. We then apply more recently developed methods, such as machine learning and neural network, to time series data, highlighting the challenges of

data processing and data layout when time series data is used for fitting models that are not inherently time aware, such as decision trees.

# Post-modeling considerations

Chapters 11 and 12 cover accuracy metrics and performance considerations, respectively, to provide some guidance as to what you should consider after you have performed your first iterations of time series modeling.

# Real-world use cases

Chapters 13, 14, and 15 provide case studies from healthcare, finance, and gov‐ ernment data, respectively.

# Comments on recent happenings

Chapters 16 and 17 briefly cover recent time series developments and predictions for the future. Chapter 16 is an overview of a variety of automated time series packages, some open source and developed as an academic effort, and some coming from large tech companies. These tools are under active development as efforts ramp up to improve time series forecasting at scale via automated pro‐ cesses. Chapter 17 discusses some predictions for the future of time series analy‐ sis as the big data ecosystem grows and as we learn more about how big data can assist time series analysis.

In general, I recommend reading through a chapter before attempting to work with the code. There are usually a few new concepts introduced in each chapter, and it can be helpful to give these some attention before turning to your keyboard. Also, in most cases the code to execute certain models is relatively simple so that the conceptual understanding will be your main skill gained, with knowledge of the APIs of impor‐ tant packages being a secondary benefit that will come much more easily if you pay attention to the concepts.

Also, the book is written so that it is most sensibly read from beginning to end (with later chapters referring to concepts covered earlier in the book), but, again, I pur‐ posely kept the chapters as self-contained as possible to enable more experienced readers to jump around as convenient.

# Online Resources

O’Reilly hosts a GitHub repository for this book. The repository includes much of the code discussed in this book in a form where you can run it on data sets, which are also provided in the repository. In some cases, variable names, formatting, and such will not be identical to that produced in the book, but it should be easily linked (for example, in some cases variable names were shortened in the book due to formatting constraints).

Additionally, should you enjoy video presentations, I have two online tutorials cover‐ ing some of the content of this book with a Python focus. If you want to complement this book with a video presentation, consider the following resources:

• Time Series Analysis (SciPy 2016)   
• Modern Time Series Analysis (SciPy 2019)

# Conventions Used in This Book

The following typographical conventions are used in this book:

Italic

Indicates new terms, URLs, email addresses, filenames, and file extensions.

Constant width

Used for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, databases, data types, environment variables, statements, and keywords.

Constant width bold

Shows commands or other text that should be typed literally by the user.

Constant width italic

Shows text that should be replaced with user-supplied values or by values deter‐ mined by context.

![](images/dc72577a511f156e5bfc8c5e2d73ede0076519021a55fec540cdb764efa30465.jpg)

This element signifies a tip or suggestion.

![](images/fb4b84c699ac76447728e93311da514f70d70ad0aeca06b07881d3f03c71ae97.jpg)

This element signifies a general note.

![](images/4600a271534710d7b19e8af11bea3a9151cf27f2638b07d4dd29e290b8fb8f20.jpg)

This element indicates a warning or caution.

# Using Code Examples

Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/PracticalTimeSeriesAnalysis/BookRepo.

This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing a CD-ROM of examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a signifi‐ cant amount of example code from this book into your product’s documentation does require permission.

We appreciate, but do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “Practical Time Series Analysis by Aileen Nielsen (O’Reilly). Copyright 2020 Aileen Nielsen, 978-1-492-04165-8.”

If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.

# O’Reilly Online Learning

# O'REILLY

For almost 40 years, O’Reilly Media has provided technology and business training, knowledge, and insight to help compa‐ nies succeed.

Our unique network of experts and innovators share their knowledge and expertise through books, articles, conferences, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, indepth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and $^ { 2 0 0 + }$ other publishers. For more information, please visit http://oreilly.com.

# How to Contact Us

Please address comments and questions concerning this book to the publisher:

O’Reilly Media, Inc.

1005 Gravenstein Highway North

Sebastopol, CA 95472

800-998-9938 (in the United States or Canada)

707-829-0515 (international or local) 707-829-0104 (fax)

We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/practical-time-series-analysis.

Email bookquestions@oreilly.com to comment or ask technical questions about this book.

For more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com.

Find us on Facebook: http://facebook.com/oreilly

Follow us on Twitter: http://twitter.com/oreillymedia

Watch us on YouTube: http://www.youtube.com/oreillymedia

# Acknowledgments

Thank you to the two technical reviewers for this book, Professors Rob Hyndman and David Stoffer. Both were exceptionally gracious in allotting time to review this book and providing copious and helpful feedback and new ideas for what to cover. The book is substantially better than it would have been without their input. I partic‐ ularly thank Rob for pointing out missed opportunities in the original draft to high‐ light alternative methodologies and many interesting sources of time series data. I particularly thank David for offering skepticism regarding overly automated approaches to analysis and for pointing out when I was unduly optimistic about cer‐ tain tools. Thank you for all your help, knowledge, and deeply experienced perspec‐ tive on time series analysis.

I am grateful to my editor at O’Reilly, Jeff Bleiel, who has been most encouraging, supportive, and helpful in reviewing these many pages over the past year. I also owe many thanks to my production editor, Katie Tozer, who has been so patient with the process of cleaning up this book and guiding me through technical glitches in pro‐ duction. Thank you to Rachel Monaghan for careful and excellent copyediting. Thank you to Rebecca Demarest, who created many of the illustrations in this book and helped me clean up many untidy graphics. Thanks also go to Jonathan Hassell, who took this project and ran with it and somehow convinced O’Reilly to publish it. Finally, thank you to Nicole Tache, who first reached out to me a few years ago to work with O’Reilly. To everyone at O’Reilly, thank you for letting me undertake this project and for all your support throughout the process.

I am grateful to many readers who gave feedback or proofreading assistance along the way, including Wenfei Tong, Richard Krajunus, Zach Bogart, Gabe Fernando, Laura

Kennedy, Steven Finkelstein, Liana Hitts, and Jason Greenberg. Having their input was exceptionally helpful. Thank you for reading and for offering feedback.

I thank my mother (and role-model), Elizabeth, for lifelong love, support, and disci‐ pline. I thank my father, John, and Aunt Claire for all their love and support for my education over many years. Most importantly, I thank my husband, Ivan, and my son, Edmund Hillary, for their patience and enthusiasm in supporting the book and excusing the time I spent away from family while writing.

Any mistakes or bugs you find in this book are mine and mine alone. I welcome feed‐ back at aileen.a.nielsen@gmail.com.

# Time Series: An Overview and a Quick History

Time series data and its analysis are increasingly important due to the massive pro‐ duction of such data through, for example, the internet of things, the digitalization of healthcare, and the rise of smart cities. In the coming years we can expect the quan‐ tity, quality, and importance of time series data to grow rapidly.

As continuous monitoring and data collection become more common, the need for competent time series analysis with both statistical and machine learning techniques will increase. Indeed, the most promising new models combine both of these meth‐ odologies. For this reason, we will discuss each at length. We will study and use a broad range of time series techniques useful for analyzing and predicting human behavior, scientific phenomena, and private sector data, as all these areas offer rich arrays of time series data.

Let’s start with a definition. Time series analysis is the endeavor of extracting mean‐ ingful summary and statistical information from points arranged in chronological order. It is done to diagnose past behavior as well as to predict future behavior. In this book we will use a variety of approaches, ranging from hundred-year-old statisti‐ cal models to newly developed neural network architectures.

None of the techniques has developed in a vacuum or out of purely theoretical inter‐ est. Innovations in time series analysis result from new ways of collecting, recording, and visualizing data. Next we briefly discuss the emergence of time series analysis in a variety of applications.

# The History of Time Series in Diverse Applications

Time series analysis often comes down to the question of causality: how did the past influence the future? At times, such questions (and their answers) are treated strictly within their discipline rather than as part of the general discipline of time series anal‐ ysis. As a result, a variety of disciplines have contributed novel ways of thinking about time series data sets.

In this section we will survey a few historical examples of time series data and analy‐ sis in these disciplines:

• Medicine   
• Weather   
• Economics   
• Astronomy

As we will see, the pace of development in these disciplines and the contributions originating in each field were strongly tied to the nature of the contemporaneous time series data available.

# Medicine as a Time Series Problem

Medicine is a data-driven field that has contributed interesting time series analysis to human knowledge for a few centuries. Now, let’s study a few examples of time series data sources in medicine and how they emerged over time.

Medicine got a surprisingly slow start to thinking about the mathematics of predict‐ ing the future, despite the fact that prognoses are an essential part of medical practice. This was the case for many reasons. Statistics and a probabilistic way of thinking about the world are recent phenomena, and these disciplines were not available for many centuries even as the practice of medicine developed. Also, most doctors prac‐ ticed in isolation, without easy professional communication and without a formal recordkeeping infrastructure for patient or population health. Hence, even if physi‐ cians in earlier times had been trained as statistical thinkers, they likely wouldn’t have had reasonable data from which to draw conclusions.

This is not at all to criticize early physicians but to explain why it is not too surprising that one of the early time series innovations in population health came from a seller of hats rather than from a physician. When you think about it, this makes sense: in earlier centuries, an urban hat seller would likely have had more practice in record‐ keeping and the art of spotting trends than would a physician.

The innovator was John Graunt, a 17th-century London haberdasher. Graunt under‐ took a study of the death records that had been kept in London parishes since the

early 1500s. In doing so, he originated the discipline of demography. In 1662, he pub‐ lished Natural and Political Observations . . . Made upon the Bills of Mortality (See Figure 1-1).

![](images/85aeb0435968285f66349752dc59c7fc1c711030d8f8324a0679334fa381f887.jpg)  
Figure 1-1. John Graunt’s actuarial tables were one of the first results of time-seriesstyle thinking applied to medical questions. Source: Wikipedia.

In this book, Graunt presented the first life tables, which you may know as actuarial tables. These tables show the probability that a person of a given age will die before their next birthday. Graunt, as the first person known to have formulated and pub‐ lished life tables, was also the first documented statistician of human health. His life tables looked something like Table 1-1, which is taken from some Rice University sta‐ tistics course notes.

Table 1-1. A sample of John Graunt’s life tables   

<table><tr><td>Age</td><td>Proportion of deaths in the interval</td><td>Proportion surviving until start of interval</td></tr><tr><td>0–6</td><td>0.36</td><td>1.0</td></tr><tr><td>7–16</td><td>0.24</td><td>0.64</td></tr><tr><td>17–26</td><td>0.15</td><td>0.40</td></tr><tr><td>27–36</td><td>0.09</td><td>0.25</td></tr></table>

Unfortunately, Graunt’s way of thinking mathematically about human survival did not take. A more connected and data-driven world began to form—complete with nation states, accreditation, professional societies, scientific journals, and, much later, government-mandated health recordkeeping—but medicine continued to focus on physiology rather than statistics.

There were understandable reasons for this. First, the study of anatomy and physiol‐ ogy in small numbers of subjects had provided the major advances in medicine for centuries, and most humans (even scientists) hew to what works for them as long as

possible. While a focus on physiology was so successful, there was no reason to look further afield. Second, there was very little reporting infrastructure in place for physi‐ cians to tabulate and share information on the scale that would make statistical meth‐ ods superior to clinical observations.

Time series analysis has been even slower to come into mainstream medicine than other branches of statistics and data analysis, likely because time series analysis is more demanding of recordkeeping systems. Records must be linked together over time, and preferably collected at regular intervals. For this reason, time series as an epidemiological practice has only emerged very recently and incrementally, once suf‐ ficient governmental and scientific infrastructure was in place to ensure reasonably good and lengthy temporal records.

Likewise, individualized healthcare using time series analysis remains a young and challenging field because it can be quite difficult to create data sets that are consistent over time. Even for small case-study-based research, maintaining both contact with and participation from a group of individuals is excruciatingly difficult and expen‐ sive. When such studies are conducted for long periods of time, they tend to become canonical in their fields—and repeatedly, or even excessively researched—because their data can address important questions despite the challenges of funding and management.1

# Medical instruments

Time series analysis for individual patients has a far earlier and more successful his‐ tory than that of population-level health studies. Time series analysis made its way into medicine when the first practical electrocardiograms (ECGs), which can diag‐ nose cardiac conditions by recording the electrical signals passing through the heart, were invented in 1901 (see Figure 1-2). Another time series machine, the electroence‐ phalogram (EEG), which noninvasively measures electrical impulses in the brain, was introduced into medicine in 1924, creating more opportunities for medical practi‐ tioners to apply time series analysis to medical diagnosis (see Figure 1-3).

Both of these time series machines were part of a larger trend of enhancing medicine with repurposed ideas and technologies coming out of the second Industrial Revolution.

![](images/85790adbe999c27ace0422904cd4c569c99d91e36dbcc3f2716cd2d6b627bbc3.jpg)  
F.1.Man.Heart ledofftoeletrometer fromfront and back of chest (front to Hg;back toHSO). ee.eleetrometer.h.h.cardiograph.t.t.timeinseonds.

![](images/75b2507c0e5b0e3fb92bcaeb8dfc8f50bf7ff451e2f20fdf18f5e1c5d05a0b67.jpg)  
Figure 1-2. An early ECG recording from the original 1877 paper by Augustus D. Wal‐ ler, M.D, “A Demonstration on Man of Electromotive Changes Accompanying the Heart’s Beat”. The earliest ECGs were difficult to construct and use, so it was a few more decades before they became a practical tool for physicians.   
Figure 1-3. The first human EEG recording, from 1924. Source: Wikipedia.

ECG and EEG time series classification tools remain active areas of research for very practical purposes, such as estimating the risk of a sudden cardiac crisis or a seizure. These measurements are rich sources of data, but one “problem” with such data is that it tends to be available only for patients with specific ailments. These machines do not generate long-range time series that can tell us more broadly about human health and behavior, as their measurements are seldom applied for long periods of time or before a disease has emerged in a patient.

Luckily, from a data analysis point of view, we are moving past the era where ECGs and the like are the dominant medical time series available. With the advent of wear‐ able sensors and “smart” electronic medical devices, many healthy humans take rou‐ tine measurements automatically or with minimal manual input, leading to the ongoing collection of good longitudinal data about both sick and healthy people. This is in stark contrast to the last century’s medical time series data, which was almost exclusively measured on sick people and which was very limited in access.

As recent news coverage has shown, a variety of nontraditional players are entering the medical field, ranging from enormous social media companies to financial

institutions to retail giants.2 They likely all plan to use large data sets to streamline healthcare. There aren’t just new players in the healthcare field—there are also new techniques. The personalized DNA-driven medicine means that time series data is increasingly measured and valued. Thanks to burgeoning modern healthcare data sets, both healthcare and time series analysis will likely evolve in the coming years, particularly in response to the lucrative data sets of the healthcare sector. Hopefully this will happen in such a way that time series can benefit everyone.

# Forecasting Weather

For obvious reasons, predicting the weather has long been a preoccupation to many. The ancient Greek philosopher Aristotle delved into weather with an entire treatise (Meteorology), and his ideas about the causes and sequencing of the weather remained dominant until the Renaissance. At that time, scientists began to collect weather-related data with the help of newly invented instruments, such as the barom‐ eter, to measure the state of the atmosphere. They used these instruments to record time series at daily or even hourly intervals. The records were kept in a variety of locations, including private diaries and local town logbooks. For centuries this remained the only way that Western civilization tracked the weather.

Greater formalization and infrastructure for weather recording arrived in the 1850s when Robert FitzRoy was appointed the head of a new British government depart‐ ment to record and publish weather-related data for sailors.3 FitzRoy coined the term weather forecast. At the time, he was criticized for the quality of his forecasts, but he is now regarded to have been well ahead of his time in the science he used to develop them. He established the custom of printing weather forecasts in the newspaper; they were the first forecasts printed in The Times of London. FitzRoy is now celebrated as the “father of forecasting.”

In the late 19th century—hundreds of years after many atmospheric measurements had come into use—the telegraph allowed for fast compilations of atmospheric con‐ ditions in time series from many different locations. This practice became standard in many parts of the world by the 1870s and led to the creation of the first meaningful data sets for predicting local weather based on what was happening in other geo‐ graphic locations.

By the turn of the 20th century, the idea of forecasting the weather with computa‐ tional methods was vigorously pursued with the help of these compiled data sets. Early endeavors at computing the weather required a spectacular amount of effort but gave poor results. While physicists and chemists had well-proven ideas about the relevant natural laws, there were too many natural laws to apply all at once. The resulting system of equations was so complex that it was a notable scientific break‐ through the first time someone even attempted to do the calculations.

Several decades of research followed to simplify the physical equations in a way that increased accuracy and computational efficiency. These tricks of the trade have been handed down even to current weather prediction models, which operate on a mix of known physical principles and proven heuristics.

Nowadays many governments make highly granular weather measurements from hundreds or even thousands of weather stations around the world, and these predic‐ tions are grounded in data with precise information about weather station locations and equipment. The roots of these efforts trace back to the coordinated data sets of the 1870s and even earlier to the Renaissance practice of keeping local weather diaries.

Unfortunately, weather forecasting is an example of the increasing attacks on science that reach even into the domain of time series forecasting. Not only have time series debates about global temperatures been politicized, but so have more mundane time series forecasting tasks, such as predicting the path of a hurricane.

# Forecasting Economic Growth

Indicators of production and efficiency in markets have long provided interesting data to study from a time series analysis. Most interesting and urgent has been the question of forecasting future economic states based on the past. Such forecasts aren’t merely useful for making money—they also help promote prosperity and avert social catastrophes. Let’s discuss some important developments in the history of economic forecasting.

Economic forecasting grew out of the anxiety triggered by episodic banking crises in the United States and Europe in the late 19th and early 20th centuries. At that time, entrepreneurs and researchers alike drew inspiration from the idea that the economy could be likened to a cyclical system, just as the weather was thought to behave. With the right measurements, it was thought, predictions could be made and crashes averted.

Even the language of early economic forecasting mirrored the language of weather forecasting. This was unintentionally apt. In the early 20th century, economic and weather forecasting were indeed alike: both were pretty terrible. But economists’ aspirations created an environment in which progress could at least be hoped for,

and so a variety of public and private institutions were formed for tracking economic data. Early economic forecasting efforts led to the creation of economic indicators and tabulated, publicly available histories of those indicators that are still in use today. We will even use some of these in this book.

Nowadays, the United States and most other nations have thousands of government researchers and recordkeepers whose jobs are to record data as accurately as possible and make it available to the public (see Figure 1-4). This practice has proven invalua‐ ble to economic growth and the avoidance of economic catastrophe and painful boom and bust cycles. What’s more, businesses benefit from a data-rich atmosphere, as these public data sets permit transportation providers, manufacturers, small busi‐ ness owners, and even farmers to anticipate likely future market conditions. This all grew out of the attempt to identify “business cycles” that were thought to be the causes of cyclical banking failures, an early form of time series analysis in economics.

<table><tr><td colspan="2">BUSINESS CYCLEREFERENCE DATES</td><td colspan="4">DURATION IN MONTHS</td></tr><tr><td>Peak</td><td>Trough</td><td>Contraction</td><td>Expansion</td><td colspan="2">Cycle</td></tr><tr><td colspan="2">Quarterly datesare in parentheses</td><td>Peak toTrough</td><td>Previous troughtothis peak</td><td>Trough fromPreviousTrough</td><td>Peak fromPreviousPeak</td></tr><tr><td></td><td>December 1854 (IV)</td><td>--</td><td>--</td><td>--</td><td>--</td></tr><tr><td>June 1857(II)</td><td>December 1858 (IV)</td><td>18</td><td>30</td><td>48</td><td>--</td></tr><tr><td>October 1860(III)</td><td>June 1861 (III)</td><td>8</td><td>22</td><td>30</td><td>40</td></tr><tr><td>April 1865(I)</td><td>December 1867 (I)</td><td>32</td><td>46</td><td>78</td><td>54</td></tr><tr><td>June 1869(II)</td><td>December 1870 (IV)</td><td>18</td><td>18</td><td>36</td><td>50</td></tr><tr><td>October 1873(III)</td><td>March 1879 (I)</td><td>65</td><td>34</td><td>99</td><td>52</td></tr><tr><td>March 1882(I)</td><td>May 1885 (II)</td><td>38</td><td>36</td><td>74</td><td>101</td></tr><tr><td>March 1887(II)</td><td>April 1888 (I)</td><td>13</td><td>22</td><td>35</td><td>60</td></tr><tr><td>July 1890(III)</td><td>May 1891 (II)</td><td>10</td><td>27</td><td>37</td><td>40</td></tr><tr><td>January 1893(I)</td><td>June 1894 (II)</td><td>17</td><td>20</td><td>37</td><td>30</td></tr><tr><td>December 1895(IV)</td><td>June 1897 (II)</td><td>18</td><td>18</td><td>36</td><td>35</td></tr><tr><td>June 1899(III)</td><td>December 1900 (IV)</td><td>18</td><td>24</td><td>42</td><td>42</td></tr><tr><td>September 1902(IV)</td><td>August 1904 (III)</td><td>23</td><td>21</td><td>44</td><td>39</td></tr></table>

Figure 1-4. The US federal government funds many government agencies and related nonprofits that record vital statistics as well as formulate economic indicators. Source: National Bureau of Economic Research.

Much of the economic data collected by the government, particularly the most news‐ worthy, tends to be a proxy for the population’s overall economic well-being. One example of such vital information comes from the number of people requesting unemployment benefits. Examples include the government’s estimates of the gross domestic product and of the total tax returns received in a given year.

Thanks to this desire for economic forecasting, the government has become a curator of data as well as a collector of taxes. The collection of this data enabled modern eco‐

nomics, the modern finance industry, and data science generally to blossom. Thanks to time series analysis growing out of economic questions, we now safely avert many more banking and financial crises than any government could have in past centuries. Also, hundreds of time series textbooks have been written in the form of economics textbooks devoted to understanding the rhythms of these financial indicators.

# Trading markets

Let’s get back to the historical side of things. As government efforts at data collection met with great success, private organizations began to copy government recordkeep‐ ing. Over time, commodities and stock exchanges became increasingly technical. Financial almanacs became popular, too. This happened both because market partici‐ pants became more sophisticated and because emerging technologies enabled greater automation and new ways of competing and thinking about prices.

All this minute recordkeeping gave rise to the pursuit of making money off the mar‐ kets via math rather than intuition, in a way driven entirely by statistics (and, more recently, by machine learning). Early pioneers did this mathematical work by hand, whereas current “quants” do this by very complicated and proprietary time series analytic methods.

One of the pioneers of mechanical trading, or time series forecasting via algorithm, was Richard Dennis. Dennis was a self-made millionaire who famously turned ordi‐ nary people, called the Turtles, into star traders by teaching them a few select rules about how and when to trade. These rules were developed in the 1970s and 1980s and mirrored the “AI” thinking of the 1980s, in which heuristics still strongly ruled the paradigm of how to build intelligent machines to work in the real world.

Since then many “mechanical” traders have adapted these rules, which as a result have become less profitable in a crowded automated market. Mechanical traders con‐ tinue to grow in number and wealth, they are continually in search of the next best thing because there is so much competition.

# Astronomy

Astronomy has always relied heavily on plotting objects, trajectories, and measure‐ ments over time. For this reason, astronomers are masters of time series, both for cal‐ ibrating instruments and for studying their objects of interest. As an example of the long history of time series data, consider that sunspot time series were recorded in ancient China as early as 800 BC, making sunspot data collection one of the most well-recorded natural phenomena ever.

Some of the most exciting astronomy of the past century relates to time series analy‐ sis. The discovery of variable stars (which can be used to deduce galactic distances) and the observation of transitory events such as supernovae (which enhance our

understanding of how the universe changes over time) are the result of monitoring live streams of time series data based on the wavelengths and intensities of light. Time series have had a fundamental impact on what we can know and measure about the universe.

Incidentally, this monitoring of astronomical images has even allowed astronomers to catch events as they are happening (or rather as we are able to observe them, which may take millions of years).

In the last few decades, the availability of explicitly timestamped data, as formal time series, has exploded in astronomy with a wide array of new kinds of telescopes col‐ lecting all sorts of celestial data. Some astronomers have even referred to a time series “data deluge.”

# Time Series Analysis Takes Off

George Box, a pioneering statistician who helped develop a popular time series model, was a great pragmatist. He famously said, “All models are wrong, but some are useful.”

Box made this statement in response to a common attitude that proper time series modeling was a matter of finding the best model to fit the data. As he explained, the idea that any model can describe the real world is very unlikely. Box made this pro‐ nouncement in 1978, which seems bizarrely late into the history of a field as impor‐ tant as time series analysis, but in fact the formal discipline was surprisingly young.

For example, one of the achievements that made George Box famous, the Box-Jenkins method—considered a fundamental contribution to time series analysis— appeared only in 1970.4 Interestingly, this method first appeared not in an academic journal but rather in a statistics textbook, Time Series Analysis: Forecasting and Con‐ trol (Wiley). Incidentally this textbook remains popular and is now in its fifth edition.

The original Box-Jenkins model was applied to a data set of carbon dioxide levels emitted from a gas furnace. While there is nothing quaint about a gas furnace, the 300-point data set that was used to demonstrate the method does feel somewhat out‐ moded. Certainly, larger data sets were available in the 1970s, but remember that they were exceptionally difficult to work with then. This was a time that predated conveniences such as R, Python, and even $\mathrm { C } { + } { + }$ . Researchers had good reasons to focus on small data sets and methods that minimized computing resources.

Time series analysis and forecasting developed as computers did, with larger data sets and easier coding tools paving the way for more experimentation and the ability to

answer more interesting questions. Professor Rob Hyndman’s history of forecasting competitions provides apt examples of how time series forecasting competitions developed at a rate parallel to that of computers.

Professor Hyndman places the “earliest non-trivial study of time series forecast accu‐ racy” as occurring in a 1969 doctoral dissertation at the University of Nottingham, just a year before the publication of the Box-Jenkins method. That first effort was soon followed by organized time series forecasting competitions, the earliest ones fea‐ turing around 100 data sets in the early 1970s.5 Not bad, but surely something that could be done by hand if absolutely necessary.

By the end of the 1970s, researchers had put together a competition with around 1,000 data sets, an impressive scaling up. Incidentally, this era was also marked by the first commercial microprocessor, the development of floppy disks, Apple’s early per‐ sonal computers, and the computer language Pascal. It’s likely some of these innova‐ tions were helpful. A time series forecasting competition of the late 1990s included 3,000 data sets. While these collections of data sets were substantial and no doubt reflected tremendous amounts of work and ingenuity to collect and curate, they are dwarfed by the amount of data now available. Time series data is everywhere, and soon everything will be a time series.

This rapid growth in the size and quality of data sets owes its origins to the tremen‐ dous advances that have been made in computing in the past few decades. Hardware engineers succeeded in continuing the trend described by Moore’s Law—a prediction of exponential growth in computing capacity—during this time. As hardware became smaller, more powerful, and more efficient, it was easy to have much more of it, affordably—to create everything from miniature portable computers with attached sensors to massive data centers powering the modern internet in its data-hungry form. Most recently, wearables, machine learning techniques, and GPUs have revolu‐ tionized the quantity and quality of data available for study.6

Time series will no doubt benefit as computing power increases because many aspects of time series data are computationally demanding. With ramped-up computational and data resources, time series analysis can be expected to continue its rapid pace of development.

# The Origins of Statistical Time Series Analysis

Statistics is a very young science. Progress in statistics, data analysis, and time series has always depended strongly on when, where, and how data was available and in what quantity. The emergence of time series analysis as a discipline is linked not only to developments in probability theory but equally to the development of stable nation states, where recordkeeping first became a realizable and interesting goal. We cov‐ ered this earlier with respect to a variety of disciplines. Now we’ll think about time series itself as a discipline.

One benchmark for the beginning of time series analysis as a discipline is the applica‐ tion of autoregressive models to real data. This didn’t happen until the 1920s. Udny Yule, an experimental physicist turned statistical lecturer at Cambridge University, applied an autoregressive model to sunspot data, offering a novel way to think about the data in contrast to methods designed to fit the frequency of an oscillation. Yule pointed out that an autoregressive model did not begin with a model that assumed periodicity:

When periodogram analysis is applied to data respecting any physical phenomenon in the expectation of eliciting one or more true periodicities, there is usually, as it seems to me, a tendency to start from the initial hypothesis that the periodicity or periodici‐ ties are masked solely by such more or less random superposed fluctuations—fluctua‐ tions which do not in any way disturb the steady course of the underlying periodic function or functions…there seems no reason for assuming it to be the hypothesis most likely a priori.

Yule’s thinking was his own, but it’s likely that some historical influences led him to notice that the traditional model presupposed its own outcome. As a former experi‐ mental physicist who had worked abroad in Germany (the epicenter for the burgeon‐ ing theory of quantum mechanics), Yule would certainly have been aware of the recent developments that highlighted the probabilistic nature of quantum mechanics. He also would have recognized the dangers of narrowing one’s thinking to a model that presupposes too much, as classical physicists had done before the discovery of quantum mechanics.

As the world became a more orderly, recorded, and predictable place, particularly after World War II, early problems in practical time series analysis were presented by the business sector. Business-oriented time series problems were important and not overly theoretical in their origins. These included forecasting demand, estimating future raw materials prices, and hedging on manufacturing costs. In these industrial use cases, techniques were adopted when they worked and rejected when they didn’t. It probably helped that industrial workers had access to larger data sets than were available to academics at the time (as continues to be the case now). This meant that sometimes practical but theoretically underexplored techniques came into wide‐ spread use before they were well understood.

# The Origins of Machine Learning Time Series Analysis

Early machine learning in time series analysis dates back many decades. An oft-cited paper from 1969, “The Combination of Forecasts,” analyzed the idea of combining forecasts rather than choosing a “best one” as a way to improve forecast performance. This idea was, at first, abhorrent to traditional statisticians, but ensemble methods have come to be the gold standard in many forecasting problems. Ensembling rejects the idea of a perfect or even significantly superior forecasting model relative to all possible models.

More recently, practical uses for time series analysis and machine learning emerged as early as the 1980s, and included a wide variety of scenarios:

• Computer security specialists proposed anomaly detection as a method of identi‐ fying hackers/intrusions.   
• Dynamic time warping, one of the dominant methods for “measuring” the simi‐ larity of time series, came into use because the computing power would finally allow reasonably fast computation of “distances,” say between different audio recordings.   
• Recursive neural networks were invented and shown to be useful for extracting patterns from corrupted data.

Time series analysis and forecasting have yet to reach their golden period, and, to date, time series analysis remains dominated by traditional statistical methods as well as simpler machine learning techniques, such as ensembles of trees and linear fits. We are still waiting for a great leap forward for predicting the future.

# More Resources

• On the history of time series analysis and forecasting:

Kenneth F. Wallis, “Revisiting Francis Galton’s Forecasting Competition,” Statisti‐ cal Science 29, no. 3 (2014): 420–24, https://perma.cc/FJ6V-8HUY.

This is a historical and statistical discussion of a very early paper on forecast‐ ing the weight of a butchered ox while the animal was still alive at a county fair.

G. Udny Yule, “On a Method of Investigating Periodicities in Disturbed Series, with Special Reference to Wolfer’s Sunspot Numbers,” Philosophical Transactions

of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 226 (1927): 267–98, https://perma.cc/D6SL-7UZS.   
Udny Yule’s seminal paper, one of the first applications of autoregressive moving average analysis to real data, illustrates a way to remove the assump‐ tion of periodicity from analysis of a putatively periodic phenomenon.

J.M. Bates and C. W. J. Granger, “The Combination of Forecasts,” Organizational Research Quarterly 20, No. 4 (1969): 451–68, https://perma.cc/9AEE-QZ2J.   
This seminal paper describes the use of ensembling for time series forecast‐ ing. The idea that averaging models was better for forecasting than looking for a perfect model was both new and controversial to many traditional statisticians.

Jan De Gooijer and Rob Hyndman, “25 Years of Time Series Forecasting,” Interna‐ tional Journal of Forecasting 22, no. 3 (2006): 443–73, https://perma.cc/ 84RG-58BU.   
This is a thorough statistical summary of time series forecasting research in the 20th century.

Rob Hyndman, “A Brief History of Time Series Forecasting Competitions,” Hyndsight blog, April 11, 2018, https://perma.cc/32LJ-RFJW.

This shorter and more specific history gives specific numbers, locations, and authors of prominent time series forecasting competitions in the last 50 years.

• On domain-specific time series histories and commentary:

NASA, “Weather Forecasting Through the Ages,” Nasa.gov, February 22, 2002, https://perma.cc/8GK5-JAVT.   
NASA gives a history of how weather forecasting came to be, with emphasis on specific research challenges and successes in the 20th century.

Richard C. Cornes, “Early Meteorological Data from London and Paris: Extending the North Atlantic Oscillation Series,” PhD diss., School of Environmental Sciences, University of East Anglia, Norwich, UK, May 2010, https://perma.cc/NJ33- WVXH.

This doctoral thesis offers a fascinating account of the kinds of weather information available for two of Europe’s most important cities, complete with extensive listings of the locations and nature of historic weather in time series format.

Dan Mayer, “A Brief History of Medicine and Statistics,” in Essential Evidence-Based Medicine (Cambridge, UK: Cambridge University Press, 2004), https:// perma.cc/WKU3-9SUX.   
This chapter of Mayer’s book highlights how the relationship between medi‐ cine and statistics depended greatly on social and political factors that made data and statistical training available for medical practitioners.   
Simon Vaughan, “Random Time Series in Astronomy”, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 371, no. 1984 (2013): 1–28, https://perma.cc/J3VS-6JYB.   
Vaughan summarizes the many ways time series analysis is relevant to astronomy and warns about the danger of astronomers rediscovering time series principles or missing out on extremely promising collaborations with statisticians.

# Finding and Wrangling Time Series Data

In this chapter we discuss problems that might arise while you are preprocessing time series data. Some of these problems will be familiar to experienced data analysts, but there are specific difficulties posed by timestamps. As with any data analysis task, cleaning and properly processing data is often the most important step of a time‐ stamp pipeline. Fancy techniques can’t fix messy data.

Most data analysts will need to find, align, scrub, and smooth their own data either to learn time series analysis or to do meaningful work in their organizations. As you prepare data, you’ll need to do a variety of tasks, from joining disparate columns to resampling irregular or missing data to aligning time series with different time axes. This chapter helps you along the path to an interesting and properly prepared time series data set.

We discuss the following skills useful for finding and cleaning up time series data:

• Finding time series data from online repositories   
• Discovering and preparing time series data from sources not originally intended for time series   
• Addressing common conundrums you will encounter with time series data, espe‐ cially the difficulties that arise from timestamps

After reading this chapter, you will have the skills needed to identify and prepare interesting sources of time series data for downstream analysis.

# Where to Find Time Series Data

If you are interested in where to find time series data and how to clean it, the best resource for you in this chapter depends on which of these is your main goal:

• Finding an appropriate data set for learning or experimentation purposes   
• Creating a time series data set out of existing data that is not stored in an explic‐ itly time-oriented form

In the first case, you should find existing data sets with known benchmarks so you can see whether you are doing your analysis correctly. These are most often found as contest data sets (such as Kaggle) or repository data sets. In these cases, you will likely need to clean your data for a specific purpose even if some preliminary work has been done for you.

In the second case, you should think about effective ways to identify interesting time‐ stamped data, turn it into a series, clean it, and align it with other timestamped data to make interesting time series data. I will refer to these found in the wild data sets as found time series (this is my own term and not technical language).

We discuss both prepared data sets and found time series next.

# Prepared Data Sets

The best way to learn an analytical or modeling technique is to run through it on a variety of data sets and see both how to apply it and whether it helps you reach a con‐ crete goal. In such cases it is helpful to have prepared options.

While time series data is everywhere, it is not always easy to find the kind of data you want when you want it. If you often find yourself in this position, you will want to get familiar with some commonly used time series data repositories, so we’ll discuss a few options you should consider.

# The UCI Machine Learning Repository

The UCI Machine Learning Repository (see Figure 2-1) contains around 80 time ser‐ ies data sets, ranging from hourly air quality samples in an Italian city to Amazon file access logs to diabetes patients’ records of activity, food, and blood glucose informa‐ tion. These are very different kinds of data, and a look at the files shows they reflect distinct ways of tracking information across time, yet each is a time series.

![](images/4058ba723423e7c3d00e31530edb5243b5265d96ec1987aeba081b4cb5c6400e.jpg)  
Figure 2-1. The UCI Machine Learning Repository includes an annotated list of time series data sets.

Consider the very first data set listed under the Time Series section in the UCI reposi‐ tory, which is a data set about absenteeism at work (see Figure 2-2).

A quick look at the data reveals that the time columns are limited to “Month of absence,” “Day of the week,” and “Seasons,” with no column for year. There are duplicate time indices, but there is also a column indicating employee identity so that we can differentiate between these duplicate time points. Finally, there are various employee attribute columns.

# Absenteeismatwork Data Set

DownloadData FolderDataSetDescription

AbstractThedatabasewascreatedwithrecordsofabsenteesmatworkfromJuly2007toJuly2010atacouriercompanyinBrail

<table><tr><td>Data Set Characteristics:</td><td>Multivariate, Time-Series</td><td>Number of Instances:</td><td>740</td><td>Area:</td><td>Business</td></tr><tr><td>Attribute Characteristics:</td><td>Integer, Real</td><td>Number of Attributes:</td><td>21</td><td>Date Donated</td><td>2018-04-05</td></tr><tr><td>Associated Tasks:</td><td>Classification, Clustering</td><td>Missing Values?</td><td>N/A</td><td>Number of Web Hits:</td><td>92522</td></tr></table>

Figure 2-2. The absenteeism at work data set is the first in the list of time series data sets in the UCI Machine Learning Repository.

This data set could be quite challenging to process, because you would first need to determine whether the data was all from one year or whether the cycling of months from 1 to 12 several times through the row progressions indicates that the year is changing. You would also need to decide whether to look at the problem in the aggregate, from a total absenteeism per unit time perspective, or to look at absentee‐ ism per ID for those reported in the data set (see Figure 2-3). In the first case you would have a single time series, whereas in the latter case you would have multiple time series with overlapping timestamps. How you looked at the data would depend on what question you wanted to answer.

![](images/0a41cc817fabb4509bbf3dd51f19266915c572cdce59958fa881feaa7d3c01df.jpg)  
Figure 2-3. The first few lines of one file in the Australian sign language data set. As you can see, this is a wide file format.

Contrast the absenteeism data set with another data set early on in the list, the Aus‐ tralian Sign Language signs data set, which includes recordings from a Nintendo PowerGlove as subjects signed in Australian Sign Language. The data is formatted as wide CSV files, each within a folder indicating the individual measured and with a filename indicating the sign.

In this data set, the columns are unlabeled and do not have a timestamp. This is nonetheless time series data; the time axis counts the time steps forward, regardless of when the actual events happened. Notice that for the purpose of thinking about signs

as time series, it doesn’t matter what the unit of time is; the point is the sequencing rather than the exact time. In that case, all you would care about is the ordering of the event, and whether you could assume or confirm from reading the data description that the measurements were taken at regular intervals.

As we can see from inspecting these two data sets, you will run into all sorts of data munging challenges. Some of the problems we’ve already noticed are:

• Incomplete timestamps   
• Time axes can be horizontal or vertical in your data   
• Varying notions of time

# The UEA and UCR Time Series Classification Repository

The UEA and UCR Time Series Classification Repository is a newer effort providing a common set of time series data available for experimentation and research in time series classification tasks. It also shows a very diverse set of data. We can see this by looking at two data sets.

One data set is a yoga movement classification task. The classification task involves distinguishing between two individual actors who performed a series of transitions between yoga poses while images were recorded. The images were converted to a one-dimensional series. The data is stored in a CSV file, with the label as the leftmost column and the remaining columns representing time steps. Time passes from left to right across the columns, rather than from top to bottom across the rows.

Plots of two sample time series per gender are shown in Figure 2-4.

![](images/965b6d3393cf30cc17191191a79f1ddaa919cc1b317aa678eb2a5e67b675dc6d.jpg)

# Univariate Versus Multivariate Time Series

The data sets we have looked at so far are univariate time series; that is, they have just one variable measured against time.

Multivariate time series are series with multiple variables measured at each timestamp. They are particularly rich for analysis because often the measured variables are interrelated and show temporal dependencies between one another. We will encounter multivari‐ ate time series data later.

![](images/4be61f88f3c35c3f67f5628b6eb4052c5ddaccfc99da87df2c75a7cb502e2ac4.jpg)

![](images/636bf12c81cc43d67524703e81e38e2d7ec0f78d8fea80434a43c734a0e64a9e.jpg)  
Figure 2-4. Plots of a male and a female actor performing a yoga move repeatedly. We plot two sample time series per actor. There are no explicit time labels on the x-axis. Rather than the unit of time, what is important is whether the $x$ -axis data points are evenly spaced, as they are presented here.

Also consider the wine data set, in which wines were classified by region according to the shapes of their spectra. So what makes this relevant to time series analysis? A spectrum is a plot of light wavelength versus intensity. Here we see a time series clas‐ sification task where there is no passage of time at all. Time series analysis applies, however, because there is a unique and meaningful ordering of the x-axis, with a con‐ crete meaning of distance along that axis. Time series analysis distinguishes itself from cross-sectional analysis by using the additional information provided by order‐ ing on the x-axis, whether it is time or wavelength or something else. We can see a plot of such a “time” series in Figure 2-5. There is no temporal element, but we are nonetheless looking at an ordered series of data, so the usual ideas of time series apply.1

![](images/347323849f3952c212f86e4da986b730aa2458ba8201d1bcf50b72e0d41a1edf.jpg)  
Figure 2-5. A sample spectrum of the wine sampled in the UCI wine data set. Peaks in the curve indicate wavelength regions that have particularly high rates of absorption. The wavelengths are uniformly spaced along the x-axis, whereas the y-axis indicates the rate of absorption, also on a linear scale. We can use time series analysis to compare curves, such as the one above, to one another.

# Government time series data sets

The US government has been a reliable provider of time series data for decades, or even centuries. For example, the NOAA National Centers for Environmental Infor‐ mation publishes a variety of time series data relating to temperatures and precipita‐ tion at granularities as fine as every 15 minutes for all weather stations across the country. The Bureau of Labor Statistics publishes a monthly index of the national unemployment rate. The Centers for Disease Control and Prevention publishes weekly flu case counts during the flu season. The Federal Reserve Bank of St. Louis offers a particularly generous and helpful set of economic time series data.

For initial forays into time series analysis, I recommend that you access these realworld government data sets only for exploratory analysis and visualization. It can be difficult to learn on these data sets because they present extremely complicated prob‐ lems. For example, many economists spend their entire careers trying to predict the unemployment rate in advance of its official publication, with only limited success.

For the important but intractable problems faced by governments, predicting the future would not only be socially beneficial but also highly remunerative. Many smart and well-trained people are attacking these problems even as the state of the art remains somewhat disappointing. It is great to work on difficult problems, but it is not a good idea to learn on such problems.

# Found Time Series in Government Data

It’s not difficult to find a variety of timely and promising data sets from government websites, such as those just mentioned. However, there is also a lot of potential for found time series in government data. For example, imagine laying out parallel time series for economics and climate, or correlating different crimes with government spending patterns. That data would come from a variety of sources, and you would need to integrate them.

You should be wary of working with found time series in government data sets for a number of reasons. Recording conventions, column names, or even column defini‐ tions shift over time without accompanying documentation. Projects start and stop depending on politics, budgets, and other exogenous considerations. Also data for‐ mats from government websites can be very messy, and are usually messier and less continuous than comparable data sets in the private sector. This makes it particularly challenging to construct found time series.

# Additional helpful sources

While we cannot extensively cover all good sources of time series data, there are sev‐ eral other repositories that you should explore:

# CompEngine

This “self organizing database of time-series data” has more than 25,000 time series databases that total almost 140 million individual data points. The empha‐ sis of this repository, and the associated software it offers on its web interface, is to facilitate and promote highly comparative time-series analysis (hctsa). The goal of such analysis is to generate high-level insights and understanding of how many kinds of temporal behavior can be understood without discipline-specific data.

# Mcomp and M4comp2018 R packages

These R packages provide the competition data from the 1982 M-competition (1,001 time series), the M3 competition in 2000 (3,003 time series), and the M4 competition in 2018 (100,000 time series). These time series forecasting competi‐ tions were previously discussed in the Chapter 1 mention of Professor Rob Hyndman’s history of time series forecasting. Additional time series forecasting competition data is included in R’s tscompdata package. Finally, more specialized time series data sets can also be found in a variety of packages described in the CRAN repository listing of time series packages under the “Time Series Data” header.

# Found Time Series

Earlier in the chapter, we discussed the concept of a found time series, which is time series data we put together ourselves from data sources in the wild. More specifically, such time series would be put together from individual data points recorded without any special allowances for time series analysis but with enough information to con‐ struct a time series. Putting together a time series of transactions for a particular cus‐ tomer from a SQL database that stores a company’s transactions is a clean example. In such a case, the time series could be constructed so long as a timestamp, or some proxy for a timestamp, was saved in the database.2 We can also imagine other time series constructed from the same data, such as a time series of total transaction vol‐ ume per day for the company or total dollar volume for female customers per week. We can even imagine generating multivariate time series data, such as a time series that would separately indicate total volume of all customers under 18 per week, total dollars spent by women over 65 per week, and total ad spend by the company per week. This would give us three indicators at each time step, a multivariate time series.

Finding time series data in structured data not explicitly stored as a time series can be easy in the sense that timestamping is ubiquitous. Here are a few examples of where you’ll see timestamps in your database:

# Timestamped recordings of events

If there is a timestamp on your data, you have the potential to construct a time series. Even if all you do is record the time a file was accessed with no other information, you have a time series. For example, in that case, you could model the delta time between timestamps with each marked according to its later time‐ stamp, so that your time series would consist of time on your temporal axis and delta time on your value axis. You could go further, aggregating these delta times as means or totals over larger periods, or you could keep them individually recorded.

# “Timeless” measurements where another measurement substitutes for time

In some cases, time is not explicit in data but is accounted for in the underlying logic of the data set. For example, you may think of your data as “distance versus value” when the distance is being caused by a known experimental parameter, such as retracting a sensor from a position at a known rate. If you can map one of your variables to time, you have a time series. Alternately, if one of your axes has a known distance and ordering relationship (such as wavelength), you are also looking at time series data such as the case of the wine spectra mentioned earlier.

# Physical traces

Many scientific disciplines record physical traces, be it for medicine, audiology, or weather. These used to be physically generated traces collected via analog pro‐ cesses, but nowadays they are stored in a digital format. These are also time ser‐ ies, although they may stored in a format that doesn’t make this obvious, such as in an image file or in a single vector within a single field of a database.

# Retrofitting a Time Series Data Collection from a Collection of Tables

The quintessential example of a found time series is one extracted from state-type and event-type data stored in a SQL database. This is also the most relevant example because so much data continues to be stored in traditional structured SQL databases.

Imagine working for a large nonprofit organization. You have been tracking a variety of factors that could lend themselves to time series analysis:

• An email recipient’s reaction to emails over time: Did they open the emails or not?   
• A membership history: Were there periods when a member let their membership lapse?   
• Transaction history: When does an individual buy and can we predict this?

You could look at the data with several time series techniques:

• You can generate a 2D histogram of member responses to emails over time with a member-specific time line to get an idea of whether members develop fatigue from emails. (We’ll illustrate the use of 2D histograms for understanding time series in Chapter 3.)   
• You can turn donation predictions into a time series forecasting problem. (We’ll discuss classic statistical forecasting in Chapter 4.)   
• You could examine whether there are typical patterns of trajectories for member behavior in important situations. For example, is there a typical pattern of events that indicates when a member is about to leave your organization (perhaps three email deletes in a row)? In time series analysis, you could frame this as detecting a member’s underlying state based on external actions. (We’ll cover this when we discuss state space methods of time series analysis in Chapter 7.)

As we can see, there are many time series questions and answers in a simple SQL database. In many cases, organizations don’t plan for time series analysis when they are designing their database schema. In such examples, we need to collect and assem‐ ble time series from disparate tables and sources.

# A Worked Example: Assembling a Time Series Data Collection

If you are lucky enough to have several related data sources available, you will need to line them up together, possibly dealing with disparate timestamping conventions or different levels of granularity in the data. Let’s create some numbers for the nonprofit example we were using. Suppose you have data shown in Table 2-1 through Table 2-3:

Table 2-1. The year each member joined and current status of membership   

<table><tr><td>MemberId</td><td>YearJoined</td><td>MemberStatus</td></tr><tr><td>1</td><td>2017</td><td>gold</td></tr><tr><td>2</td><td>2018</td><td>silver</td></tr><tr><td>3</td><td>2016</td><td>inactive</td></tr></table>

Table 2-2. Number of emails you sent out in a given week that were opened by the member   

<table><tr><td>MemberId</td><td>Week</td><td>EmailsOpened</td></tr><tr><td>2</td><td>2017-01-08</td><td>3</td></tr><tr><td>2</td><td>2017-01-15</td><td>2</td></tr><tr><td>1</td><td>2017-01-15</td><td>1</td></tr></table>

Table 2-3. Time a member donated to your organization   

<table><tr><td>MemberId</td><td>Timestamp</td><td>DonationAmount</td></tr><tr><td>2</td><td>2017-05-22 11:27:49</td><td>1,000</td></tr><tr><td>2</td><td>2017-04-13 09:19:02</td><td>350</td></tr><tr><td>1</td><td>2018-01-01 00:15:45</td><td>25</td></tr></table>

You have likely worked with data in this tabular form. With such data, you can answer many questions, such as how the overall number of emails opened by a mem‐ ber correlates with the overall donations.

You can also answer time-oriented questions, such as whether a member donates soon after joining or long after. Without putting this data into a more time-seriesfriendly format, however, you cannot get at more granular behaviors that might help you predict when someone is likely to make a donation (say, based on whether they have recently been opening emails).

You need to put this data into a sensible format for time series analysis. There are a number of challenges you will need to address.

You should start by considering the temporal axes of the data we have. In the preced‐ ing tables we have three levels of temporal resolution:

• A yearly member status

• A weekly tally of emails opened   
• Instantaneous timestamps of donations

You will also need to examine whether the data means what you think it means. For example, you would want to determine whether the member status is a yearly status or just the most recent status. One way to answer this is to check whether any mem‐ ber has more than one entry:

```markdown
## python
>>> YearJoined.groupby('memberId').count()
groupby('memberStats').count()
1000 
```

Here we can see that all 1,000 members have only one status, so that the year they joined is indeed likely to be the YearJoined, accompanied by a status that may be the member’s current status or status when they joined. This distinction affects how you would use the status variable, so if you were going to analyze this data further you’d want to clarify with someone who knows the data pipeline. If you were applying a member’s current status to an analysis of past data, that would be a lookahead because you would be inputting something into a time series model that could not be known at the time. This is why you would not want to use a status variable, such as YearJoined, without knowing when it was assigned.

# What Is a Lookahead?

The term lookahead is used in time series analysis to denote any knowledge of the future. You shouldn’t have such knowledge when designing, training, or evaluating a model. A lookahead is a way, through data, to find out something about the future earlier than you ought to know it.

A lookahead is any way that information about what will happen in the future might propagate back in time in your modeling and affect how your model behaves earlier in time. For example, when choosing hyperparameters for a model, you might test the model at various times in your data set, then choose the best model and start at the beginning of your data to test this model. This is problematic because you chose the model for one time knowing things that would happen at a subsequent time—a lookahead.

Unfortunately, there is no automated code or statistical test for a lookahead, so it is something you must be vigilant and thoughtful about.

Looking at the emails table, both the column name week and its contents suggest that the data is a weekly timestamp or time period. This must be an aggregate over the

week, so we should think of these timestamps as weekly periods rather than time‐ stamps occurring one week apart.

You should assess some important characteristics. For example, you could start by asking how the weeks are reported in time. While we may not have information to restructure the table, if the week is divided in a somewhat strange way relative to our industry, we might want to know about this too. For analyzing human activities, it generally makes sense to look at the calendar week of Sunday through Saturday or Monday through Sunday rather than weeks that are less in line with the cycle of human activity. So, for example, don’t arbitrarily start your week with January 1st.

You could also ask whether null weeks are reported? That is, do the weeks in which the member opened 0 emails have a place in the table? This matters when we want to do time-oriented modeling. In such cases we need to always have the null weeks present in the data because a 0 week is still a data point.

```markdown
## python
>>> emails[emails.EmailsOpened < 1]
Empty DataFrame
Columns: [EmailsOpened, member, week]
Index: [] 
```

There are two possibilities: either nulls are not reported or members always have at least one email event. Anyone who has worked with email data knows that it’s diffi‐ cult to get people to open emails, so the hypothesis that members always open at least one email per week is quite unlikely. In this case, we can resolve this by looking at the history of just one user:

```txt
>>> emails[emails/member == 998]
EmailsOpened member week
25464 1 998 2017-12-04
25465 3 998 2017-12-11
25466 3 998 2017-12-18
25467 3 998 2018-01-01
25468 3 998 2018-01-08
25469 2 998 2018-01-15
25470 3 998 2018-01-22
25471 2 998 2018-01-29
25472 3 998 2018-02-05
25473 3 998 2018-02-12
25474 3 998 2018-02-19
25475 2 998 2018-02-26
25476 2 998 2018-03-05 
```

We can see that some weeks are missing. There aren’t any December 2017 email events after December 18, 2017.

We can check this more mathematically by calculating how many weekly observa‐ tions we should have between the first and last event for that member. First we calcu‐ late the length of the member’s tenure, in weeks:

```markdown
## python
>>> (max(emals[emails(member == 998].week) - min(emals[emails(member == 998].week)).days/7)
25.0 
```

Then we see how many weeks of data we have for that member:

```markdown
## python
>>> emails[emails/member == 998].shape
(24, 3) 
```

We have 24 rows here, but we should have 26. This shows some weeks of data are missing for this member. Incidentally, we could also run this calculation on all mem‐ bers simultaneously with group-by operations, but it’s more approachable to think about just one member for example purposes.

# Why 26 Rows?

You may be surprised that we need 26 instead of 25 given the subtraction we just per‐ formed, but that was an incomplete calculation. When you work with time series data, one thing you should always ask yourself after doing this kind of subtraction is whether you should add 1 to account for the offset at the end. In other words, did you subtract the positions you wanted to count?

Consider this example. Let’s say I have information for April 7th, 14th, 21st, and 28th. I want to know how many data points I should have in total. Subtracting 7 from 28 and dividing by 7 yields 21/7 or 3. However, I should obviously have four data points. I subtracted out April 7th and need to put it back in, so the proper calculation is the difference between the first and last days divided by 7, plus 1 to account for the subtracted start date.

We’ll move on to filling in the blanks so that we have a complete data set now that we have confirmed we do indeed have missing weeks. We can’t be sure of identifying all missing weeks, since some may have occurred before our earliest recorded date or after our last recorded date. What we can do, however, is fill in the missing values between the first and last time a member had a non-null event.

It’s a lot easier to fill in all missing weeks for all members by exploiting Pandas’ indexing functionality, rather than writing our own solution. We can generate a MultiIndex for a Pandas data frame, which will create all combinations of weeks and members—that is, a Cartesian product:

```txt
## python
>>> complete_idx = pd.MultiIndex.from_product((set emails.week), set emails/member))) 
```

We use this index to reindex the original table and fill in the missing values—in this case with 0 on the assumption that nothing recorded means there was nothing to record. We also reset the index to make the member and week information available as columns, and then name those columns:

```coffeescript
## python
>>> all_email = emails.set_index(['week', 'member']);
reindex(complete_idx, fill_value = 0);
reset_index()
>>> all_email.columns = ['week', 'member', 'EmailsOpened'] 
```

Let’s take a look at member 998 again:

```markdown
## python
>>> all_email[all_email/member == 998].sort_values('week')
week member EmailsOpened
2015-02-09 998 0
2015-02-16 998 0
2015-02-23 998 0
2015-03-02 998 0
2015-03-09 998 0 
```

# Python’s Pandas

Pandas is a data frame analysis package in Python that is used widely in the data sci‐ ence community. Its very name indicates its suitability for time series analysis: “Pan‐ das” refers to “panel data,” which is what social scientists call time series data.

Pandas is based on tables of data with row and column indices. It has SQL-like opera‐ tions built in, such as group by, row selection, and key indexing. It also has time ser‐ ies–specific functionality, such as indexing by time period, downsampling, and timebased grouping operations.

If you are unfamiliar with Pandas, I strongly recommend looking at a brief overview, such as that provided in the official documentation.

Notice that we have a large number of zeros at the start. These are likely before the member joined the organization, so they would not have been on an email list. There are not too many kinds of analyses where we’d want to keep the member’s truly null weeks around—specifically those weeks before the member ever indicated opening an email. If we had the precise date a member started receiving emails, we would have an objective cutoff. As it is, we will let the data guide us. For each member we determine the start_date and end_date cutoffs by grouping the email DataFrame per member and selecting the maximum and minimum week values:

```python
>>> cutoff_dates = emails.groupby('member').week.
>>> agg(['min', 'max'].reset_index)
>>> cutoff_dates = cutoff_dates.reset_index() 
```

We drop rows from the DataFrame that don’t contribute sensibly to the chronology, specifically 0 rows before each member’s first nonzero count:

```python
>>> for _, row in cutoff_dates(iterrows():
>>> member = row['member']
>>> start_date = row['min']
>>> end_date = row['max']
>>> all_email.drop(
  all_email[all_email/member == member]
  [all_email.week < start_date].index, inplace=True)
>>> all_email.drop(all_email[all_email/member == member]
  [all_email.week > end_date].index, inplace=True) 
```

![](images/e3e51340674c12dd9af4a0e6bc28eac716013fa8b727e0f89b5936492e60dc8e.jpg)

<or $<  = ?$

We use the $<$ and $>$ operations, without equality, because the start_date and end_date are inclusive of the meaningful data points and because we are dropping data, not retaining data, as our code is written. In this case we want to include those weeks in our analysis because they were the first and last meaningful data points.

You will do well to work with your data engineers and database administrators to convince them to store the data in a time aware manner, especially with respect to how timestamps are created and what they mean. The more you can solve problems upstream, the less work you have to do downstream in the data pipeline.

Now that we have cleaned up our email data, we can consider new questions. For example, if we want to think about the relationship of member email behavior to donations, we can do a few things:

• Aggregate the DonationAmount to weekly so that the time periods are compara‐ ble. Then it becomes reasonable to ask whether donations correlate in some way to member responses to emails.   
• Treat the prior week’s EmailsOpened as a predictor for a given week’s DonationA mount. Note that we have to use the prior week because EmailsOpened is a sum‐ mary statistic for the week. If we want to predict a Wednesday donation, and our EmailsOpened summarizes email opening behavior from Monday to Sunday, then using the same week’s information will potentially give us information about what the member did subsequent to when we could have known it (for example, whether they opened an email on the Friday after the donation).

# Constructing a Found Time Series

Consider how to relate the email and donations data to one another. We can down‐ sample the donation data to turn it into a weekly time series, comparable to the email data. As an organization, we are interested in the total weekly amounts, so we aggre‐ gate the timestamps into weekly periods by summing. More than one donation in a week is unlikely, so the weekly donation amounts will reflect the individual donation amounts for most donors.

```python
>>> donations.timestamp = pd.to datetime(donations.timestamp)
>>> donations.set_index('timestamp', inplace=True)
>>> agg_don = donations.groupby('member').apply(
    lambda df: df.amount.resample("W-MON").sum().dropna()) 
```

In this code we first convert a string character to a proper timestamped data class so as to benefit from Pandas’ built-in date-related indexing. We set the timestamp as an index, as is necessary for resampling a data frame. Finally for the data frame that we obtain from subsetting down to each member, we group and sum donations by week, drop weeks that do not have donations, and then collect these together.

Note that we resampled with an anchored week so that we will match the same weekly dates we already have in our email table. Note also that a week anchored to “Monday” makes sense from a human perspective.

We now have donation information and email information sampled at the same fre‐ quency, and we can join them. Pandas makes this simple so long as we anchor the weeks to the same day of the week, as we’ve done already. We can iterate through each member and merge the data frames per member:

```python
>>> for member, member_email in all_email.groupby('member'):  
>>> member_donations = agg_donations[agg_donations(member == member)]  
>>> member_donations.set_index('timestamp', inplace = True)  
>>> member_email.set_index('week', inplace = True)  
>>> member_email = all_email[all_email/member == member]  
>>> member_email.sort_values('week').set_index('week')  
>>> df = pd.merge(member_email, member_donations, how = 'left', left_index = True, right_index = True)  
>>> dffillna(0)  
>>> df['member'] = df.member_x  
>>> merged_df = merged_df.append(df.reset_index() ['member', 'week', 'emailsOpened', 'amount']) 
```

We now have our email and donations data lined up per member. For each member we include only meaningful weeks, and not weeks that appear to be before or after their membership period.

We might treat the email behavior as a “state” variable relative to the donation behav‐ ior, but we probably want to carry forward the state from the previous week so as to avoid a lookahead. Suppose, for example, we were building a model that uses email behavior to predict a member’s next donation. In this case we might consider looking at the pattern of email opening over time as a possible indicator. We would need to line up a given week’s donation with the previous week’s email behavior. We can easily take our data, processed to align week-to-week, and then shift by the appropri‐ ate number of weeks. If, say, we want to shift the donation a week forward, we can easily do so with the shift operator, although we would need to be sure to do this on a per-member basis:

```html
>>> df = merged_df[merged_df(member == 998)]
>>> df['target'] = df.amount-shift(1)
>>> df = dffillna(0)
>>> df 
```

It’s good practice to store this target in a new column rather than overwrite the old, particularly if you are not also bringing forward the timestamp for the donation amount separately. We have shifted the donation amount one week into the future using Pandas’ built-in shift functionality. You can also shift back in time with nega‐ tive numbers. Generally you will have more predictors than targets, so it makes sense to shift your targets. We can see the outcome of the code here:

<table><tr><td>amount</td><td>emailsOpened</td><td>member</td><td>week</td><td>target</td></tr><tr><td>0</td><td>1</td><td>998</td><td>2017-12-04</td><td>0</td></tr><tr><td>0</td><td>3</td><td>998</td><td>2017-12-11</td><td>0</td></tr><tr><td>0</td><td>3</td><td>998</td><td>2017-12-18</td><td>0</td></tr><tr><td>0</td><td>0</td><td>998</td><td>2017-12-25</td><td>0</td></tr><tr><td>0</td><td>3</td><td>998</td><td>2018-01-01</td><td>0</td></tr><tr><td>50</td><td>3</td><td>998</td><td>2018-01-08</td><td>0</td></tr><tr><td>0</td><td>2</td><td>998</td><td>2018-01-15</td><td>50</td></tr></table>

Now that we have filled in the missing rows, we have the desired 26 rows for member 998. Our data is now cleaner and more complete.

To recap, these are the time-series-specific techniques we used to restructure the data:

1. Recalibrate the resolution of our data to suit our question. Often data comes with more specific time information than we need.   
2. Understand how we can avoid lookahead by not using data for timestamps that produce the data’s availability.

3. Record all relevant time periods even if “nothing happened.” A zero count is just as informative as any other count.   
4. Avoid lookahead by not using data for timestamps that produce information we shouldn’t yet know about.

So far, we have created raw found time series by aligning our donation and email time series to be sampled for the same points in time and at the same frequency. However, we haven’t done a thorough job of cleaning up this data or fully exploring before analysis. That’s something we’ll do in Chapter 3.

# Timestamping Troubles

Timestamps are quite helpful for time series analysis. From timestamps, we can extrapolate a number of interesting features, such as time of day or day of the week. Such features can be important to understanding your data, especially for data con‐ cerning human behavior. However, timestamps are tricky. Here we discuss some of the difficulties of timestamped data.

# Whose Timestamp?

The first question you should ask when seeing a timestamp is what process generated the timestamp, how, and when. Often an event happening is not coincident with an event being recorded. For example, a researcher might write something down in their notebook and later transcribe it to a CSV file used as a log. Does the timestamp indi‐ cate when they wrote it down or when they transcribed it to the CSV file? Or a mobile app user may take actions that trigger logging when their phone is offline, so that the data is only later uploaded to your server with some combination of time‐ stamps. Such timestamps could reflect when the behavior took place, when the action was recorded by the app, when the metadata was uploaded to the server, or when the data was last accessed for download from the server back to the app (or any number of other events along the data pipeline).

Timestamps may appear at first to offer clarity but this quickly falls away if proper documentation is lacking. When you’re looking at novel timestamps, your first step should always be to find out as best you can what established the time of an event.

A concrete example will illustrate these difficulties. Suppose you are looking at data taken from a mobile app for weight loss and see a meal diary with entries such as those in Table 2-4.

Table 2-4. Sample meal diary from a weight loss app   

<table><tr><td>Time</td><td>Intake</td></tr><tr><td>Mon, April 7, 11:14:32</td><td>pancakes</td></tr><tr><td>Mon, April 7, 11:14:32</td><td>sandwich</td></tr><tr><td>Mon, April 7, 11:14:32</td><td>pizza</td></tr></table>

It’s possible this user had pancakes, a sandwich, and a pizza all at once, but there are more likely scenarios. Did the user specify this time or was it automatically created? Does the interface perhaps offer an automatic time that the user can adjust or choose to ignore? Some answers to these questions would explain the identical timestamps better than the possibility that a user trying to lose weight would have had pancakes with a side of pizza and a sandwich appetizer.

Even if the user did have all these foods at 11:14, where in the world was it 11:14? Was this the user’s local time or a global clock? Even in the unlikely case that the user had all this food at one meal, we still don’t know very much about the temporal aspect of the meal from these rows alone. We don’t know if this was breakfast, lunch, dinner, or a snack. To say something interesting to the user, we’d want to be able to talk in concrete terms about local hour of the day, which we can’t do without time zone information.

The best way to address these questions is to see all the code that collects and stores the data or talk to the people who wrote that code. After going through all available human and technical data specifications on the system, you should also try the full system out yourself to ensure that the data behaves as you have been told it does. The better you understand your data pipeline, the less likely you are to ask the wrong questions because your timestamps don’t really mean what you think they do.

You bear the ultimate responsibility for understanding the data. People who work upstream in the pipeline don’t know what you have in mind for analysis. Try to be as hands-on as possible in assessing how timestamps are generated. So, if you are ana‐ lyzing data from a mobile app pipeline, download the app, trigger an event in a vari‐ ety of scenarios, and see what your own data looks like. You’re likely to be surprised about how your actions were recorded after speaking to those who manage the data pipeline. It’s hard to track multiple clocks and contingencies, so most data sets will flatten the temporal realities. You need to know exactly how they do so.

# Guesstimating Timestamps to Make Sense of Data

If you are dealing with legacy data pipelines or undocumented data, you may not have the option of exploring a working pipeline and talking to those who maintain it. You will need to do some empirical investigation to understand whether you can make inferences about what the timestamps mean:

• Reading the data as we did in the previous example, you can generate initial hypotheses about what the timestamps mean. In the preceding case, look at data for multiple users to see whether the same pattern (multiple rows with identical timestamps and improbable single meal contents) held or whether this was an anomaly.

• Using aggregate-level analyses, you can test hypotheses about what timestamps mean or probably mean. For the preceding data, there are a couple of open questions:

— Is the timestamp local or universal time?

— Does the time reflect a user action or some external constraint, such as connectivity?

# Local or Universal Time?

Most timestamps are stored in universal (UTC) time or in a single time zone, depending on the server’s location but independent of the user’s location. It is quite unusual to store data according to local time. However, we should consider both pos‐ sibilities, because both are found in “the wild.”

We form the hypothesis that if the time is a local timestamp (local to each user), we should see daily trends in the data reflecting daytime and nighttime behavior. More specifically, we should expect not to see much activity during the night when our users are sleeping. For our mobile app example, if we create a histogram of meal time counts per hour, there should be hours with significantly fewer meals logged, because in most cultures people don’t eat in the middle of the night.

If we did not see a day-oriented pattern in the hours as displayed, we could conclude that the data was most likely timestamped by some universal clock and that the user base must be spread out across many time zones. In such a case, it would be quite challenging to extrapolate individual users’ local hours (assuming time zone informa‐ tion wasn’t available). We might consider individual explorations per user to see if we could code heuristics to label users approximately by time zone, but such work is both computationally taxing and also not always accurate.

Even if you cannot pinpoint an exact user time zone, it’s still useful to have a global timestamp available. For starters, you can determine likely usage patterns for your app’s servers, knowing when meals are most frequently logged at a given time of day and day of the week. You can also calculate the time differential between each meal the user recorded, knowing that since the timestamps are in absolute time, you don’t need to worry about whether the user changed time zones. In addition to sleuthing, this would be interesting as a form of feature generation:

```markdown
## python
>>> df['dt'] = df.time - df.time-shift(-1) 
```

The dt column would be a feature you could pass forward in your analysis. Using this time differential could also give you an opportunity to estimate each user’s time zone. You could look at the time of day when the user generally had a long dt, which could point to when nighttime occurs for that user. From there you could begin to figure out each individual’s “nighttime” without having to do peak-to-peak-style analysis.

# User behavior or network behavior?

To return to another question posed by our short data sample, we asked whether our user had a strange meal or whether our timestamps relate to upload activity.

The same analyses used to pinpoint a user’s time zone are applicable to determining whether the timestamps are a function of user or network behavior. Once you had a dt column (as previously calculated), you could look for clusters of 0s and qualita‐ tively determine whether they were likely a single behavioral event or a single net‐ work event. You could also look at whether the dts appeared to be periodic across different days. If they were a function of user behavior, they would be more likely to be periodic than if they were a function of network connectivity or other softwarerelated behaviors.

To summarize, here are some questions you could likely address with the available data set, even with little or no information about how timestamps are generated:

• Use differences in timestamps per user to get a sense of spacing between meals or spacing between data entries (depending on your working hypothesis of what the times indicate, a user behavior or a network behavior).   
• Describe aggregate user behavior to determine when your servers are most likely to be active in the 24-hour cycle.

# Time Zones

In the best-case scenario, your data is timestamped in UTC time. Most databases and other storage systems will default to this. However, there are some situations in which you are likely to run into data that is not UTC timestamped:

• Timestamps created in ways that did not use date-specific data objects, such as API calls. Such API calls use strings rather than time-specific objects.   
• Data created “by hand” in small, local organizations where time zones are not relevant, such as spreadsheets generated by business analysts or field biologists.

# What’s a Meaningful Time Scale?

You should take the temporal resolution of the timestamping you receive with a grain of salt, based on domain knowledge about the behavior you are studying and also based on the details you can determine relating to how the data was collected.

For example, imagine you are looking at daily sales data, but you know that in many cases managers wait until the end of the week to report figures, estimating rough daily numbers rather than recording them each day. The measurement error is likely to be substantial due to recall problems and innate cognitive biases. You might con‐ sider changing the resolution of your sales data from daily to weekly to reduce or average out this systematic error. Otherwise, you should build a model that factors in the possibility of biased errors across different days of the week. For example, it may be that managers systematically overestimate their Monday performance when they report the numbers on a Friday.

![](images/181b39a5e68b1d8bb47678a6cebd11c6764d802c7c081ce91e583ea654534e9e.jpg)

# Psychological Time Discounting

Time discounting is a manifestation of a phenomenon known as psychological distance, which names our tendency to be more opti‐ mistic (and less realistic) when making estimates or assessments that are more “distant” from us. Time discounting predicts that data reported from further in the past will be biased systematically compared to data reported from more recent memory. This is dis‐ tinct from the more general problem of forgetting and implies a nonrandom error. You should keep this in mind whenever you are looking at human-generated data that was entered manually but not contemporaneously with the event recorded.

Another situation involves physical knowledge of the system. For example, there is a limit to how quickly a person’s blood glucose can change, so if you are looking at a series of blood glucose measurements within seconds of one another, you should probably average them rather than treating them as distinct data points. Any physi‐ cian will tell you that you are studying the error of the device rather than the rate of change of blood glucose if you are looking at many measurements within a few sec‐ onds of one another.

![](images/bb96e8aa4d300536bced613e6c8eea7e5efd71c975aae83988b9387af70b9770.jpg)

# Humans Know Time Is Passing

Whenever you are measuring humans, keep in mind that people respond in more than one way to the passage of time. For example, recent research shows how manipulating the speed of a clock that is in a person’s view influences how quickly that person’s blood glucose level changes.

# Cleaning Your Data

In this section we will tackle the following common problems in time series data sets:

• Missing data   
• Changing the frequency of a time series (that is, upsampling and downsampling)   
• Smoothing data   
• Addressing seasonality in data   
• Preventing unintentional lookaheads

# Handling Missing Data

Missing data is surprisingly common. For example, in healthcare, missing data in medical time series can have a number of causes:

• The patient didn’t comply with a desired measurement.   
• The patient’s health stats were in good shape, so there was no need to take a par‐ ticular measurement.   
• The patient was forgotten or undertreated.   
• A medical device had a random technical malfunction.   
• There was a data entry error.

To generalize at great peril, missing data is even more common in time series analysis than it is in cross-sectional data analysis because the burden of longitudinal sampling is particularly heavy: incomplete time series are quite common and so methods have been developed to deal with holes in what is recorded.

The most common methods to address missing data in time series are:

# Imputation

When we fill in missing data based on observations about the entire data set.

# Interpolation

When we use neighboring data points to estimate the missing value. Interpola‐ tion can also be a form of imputation.

# Deletion of affected time periods

When we choose not to use time periods that have missing data at all.

We will discuss imputations and interpolations as well as illustrate the mechanics of these methods soon. We focus on preserving data, whereas a method such as deleting time periods with missing data will result in less data for your model. Whether to

preserve the data or throw out problematic time periods will depend on your use case and whether you can afford to sacrifice the time periods in question given the data needs of your model.

# Preparing a data set to test missing data imputation methodologies

We will work with monthly unemployment data that has been released by the US government since 1948 (this is freely available for download). We will then generate two data sets from this baseline data: one in which data is truly missing at random, and one in which are the highest unemployment months in the history of the time series. This will give us two test cases to see how imputation behaves in the presence of both random and systematic missing data.

![](images/f9e734fe7c754aa8233faf8f5fab61b7a78362819c5f6718e392781628965586.jpg)

We move to R for the next example. We will freely switch between R and Python throughout the book. I assume you have some back‐ ground in working with data frames, as well as matrices, in both R and Python.

## R   
$>$ require(zoo) ## zoo provides time series functionality   
$>$ require(data.table) ## data.table is a high performance data frame   
$>$ unemp $< \cdot$ fread("UNRATE.csv")   
$>$ unemp[, DATE := as.Date(DATE)]  
$>$ setkey(unemp, DATE)   
$>$ ## generate a data set where data is randomly missing   
$>$ rand.unemp.idx <- sample(1:nrow(unemp), .1*nrow(unemp))   
$>$ rand.unemp $< \overline { { \mathbf { \zeta } } }$ unemp[-rand.unemp.idx]   
$>$ ## generate a data set where data is more likely   
$>$ ## to be missing when unemployment is high   
$>$ high.unemp.idx $< \overline { { \mathbf { \zeta } } }$ which(unemp$UNRATE $> 8$ )   
$>$ num.to.select $< \overline { { \mathbf { \zeta } } }$ . $2 ~ \star$ length(high.unemp.idx)   
$>$ high.unemp.idx $< \overline { { \mathbf { \zeta } } }$ sample(high.unemp.idx,)   
$>$ bias.unemp $< \overline { { \mathbf { \zeta } } }$ unemp[-high.unemp.idx]

Because we have deleted rows from our data tables to create a data set with missing data, we will need to read the missing dates and NA values, and for this the data.table package’s rolling join functionality is quite useful.

# R’s data.table Package

R’s data.table package is a highly performant but woefully underutilized alternative to R’s mainstream data.frame functionality. While many users are familiar with the tidyverse selection of packages, data.table is a standalone package built only on top of data.frame. It has many helpful functions for time series analysis. This is likely due to its origins as a package developed by software engineers who worked in finance and who needed highly performant R operations to work with financial data, most often time series data.

While there is a steeper learning curve for data.table than for data.frame, I have found this package invaluable in working with time series data in R, and so I use it heavily throughout the book. I’ve provided a few quick examples of how data.table works and may differ from data.frame here:

$$
d t [, \text {n e w . c o l} := \text {o l d . c o l} + 7 ]
$$

In this example, we assume that the original data.table has an old column called old.col. We make a new column, in place, which means that we add a column to the data.table without copying the entire data.table over to a new object, thereby reducing memory use. You will see this operator := used frequently throughout the book.

To access a subset of columns in a data.table, we will usually simply put them in a list in the column indexing argument, which is the second argument to a data.table:

$$
d t [,.. (c o l 1, c o l 2, c o l 3) ]
$$

This will return a subsetted data.table including only the col1, col2, and col3 val‐ ues. Notice the .() operator surrounding the column names. This is shorthand for writing out list().

If we wish to do row selection rather than column selection, we use the first argument to the data.table to subset on rows like so:

$$
d t [ c o l 1 <   3 \& c o l 2 > 5 ]
$$

This will return all columns of the data.table for which the two logic conditions were met. And of course we can combine row and column selection, as follows:

$$
d t [ c o l 1 <   3 \& c o l 2 > 5, . (c o l 1, c o l 2, c o l 3) ]
$$

Finally, group-by operations are quite performant and fast. For example, to count the number of rows in each group among the groups designated in col1, we could use the following code.

$$
d t [,.. N, c o l 1 ]
$$

And if we wanted to do multiple operations on our grouped objects, we would again make a list and could even name the columns:

```txt
dt[,. (total = .N, mean = mean(col2), col1] 
```

The above would, by groups as designated by col1, return the total number of rows in the group and the mean of col2 for each group.

The data.table package is under active development, and it’s an invaluable tool for time series analysis, particularly for large data sets. I strongly recommend reading the official introduction to data.table.

```txt
## R
> all dates <- seq(from = unemp$DATE[1], to = tail(unemp$DATE, 1), by = "months")
> rand.unemp = rand.unemp[J(all dates), roll=0]
> bias.unemp = bias.unemp[J(all dates), roll=0]
> rand.unemp[, rpt := is.na(UNRATE)]
## here we label the missing data for easy plotting 
```

With a rolling join, we generate the sequence of all dates that should be available between the start and end date of the data set. This gives us rows in the data set to fill in as NA.

Now that we have data sets with missing values, we will take a look at a few specific ways to fill in numbers for those missing values:

• Forward fill   
• Moving average   
• Interpolation

We will compare the performance of these methods in both the randomly missing and systematically missing data sets. Since we have generated these data sets from a complete data set, we can in fact determine how they did instead of speculating. In the real world, of course, we will never have the missing data to check our data imputation.

# Rolling Joins

In data.table’s rolling join, we enjoy the benefit of a special spatially aware SQL-like join designed for timestamps. Although tables will not always match exactly on their timestamps, rolling joins can deal with timestamps intelligently.

```r
## R
> ## we have a small data.table of donation dates
> donations <- data.table(amt = c(99, 100, 5, 15, 11, 1200),
> dt = as.Date(c("2019-2-27", "2019-3-2", "2019-6-13",
> "2019-8-1", "2019-8-31", "2019-9-15")
>
) 
```

```txt
> ## we also have information on the dates of each
> ## publicity campaign
> publicity <- data.table(
>
    identifier = c("q4q42", "4299hj", "bbg2"),
>
    dt = as.Date(c("2019-1-1",
>
        "2019-4-1",
>
        "2019-7-1")) 
```

```r
> ## we set the primary key on each data.table
> setkey(publicity, "dt")
> setkey(donations, "dt")
> ## we wish to label each donation according to
> ## what publicity campaign most recently preceded it
> ## we can easily see this with roll = TRUE
> publicity[donations, roll = TRUE] 
```

This yields the following delightful output:

```markdown
## R
> publicity[donations, roll = TRUE]
identifier dt amt
1: q4q42 2019-02-27 99
2: q4q42 2019-03-02 100
3: 4299hj 2019-06-13 5
4: bbg2 2019-08-01 15
5: bbg2 2019-08-31 11
6: bbg2 2019-09-15 1200 
```

Each donation now is paired with the publicity identifier that indicates the campaign that most immediately preceded the donation.

# Forward fill

One of the simplest ways to fill in missing values is to carry forward the last known value prior to the missing one, an approach known as forward fill. No mathematics or complicated logic is required. Simply consider the experience of moving forward in time with the data that was available, and you can see that at a missing point in time, all you can be confident of is what data has already been recorded. In such a case, it makes sense to use the most recent known measurement.

Forward fill can be accomplished easily using na.locf from the zoo package:

```txt
## R
> rand.unemp[, impute.mm := na.locf(UNRATE, na.mm = FALSE)]
> bias.unemp[, impute.mm := na.locf(UNRATE, na.mm = FALSE)]
>
> ## to plot a sample graph showing the flat portions
> unemp[350:400, plot (DATE, UNRATE, 
```

```lua
col = 1, lwd = 2, type = 'b')  
> rand.unemp[350:400, lines(DATE, impute.mm), col = 2, lwd = 2, lty = 2)]  
> rand.unemp[350:400][rpt == TRUE, points(DATE, impute.mm), col = 2, pch = 6, cex = 2)] 
```

This will result in a plot that looks natural except where you see repeated values to account for missing data, as in Figure 2-6. As you will notice in the plot, the forwardfilled values usually do not deviate far from the true values.

![](images/c222b46e0254045f09356a4877338d343499a474b1c04904a773234e83d4462c.jpg)  
Figure 2-6. The original time series plotted with a solid line and the time series with forward filled values for randomly missing points plotted with a dashed line. The for‐ ward filled values are marked with downward pointing triangles.

We can also compare the values in the series by plotting the values of the series against one another. That is, for each time step, we plot the true known value against the value at the same time from the series with imputed values. Most values should match exactly since most data is present. We see that manifested in the 1:1 line in Figure 2-7. We also see scattered points off this line, but they do not appear to be systematically off.

![](images/6b03477c75b9630e0792a7f94f7706dcf4d3329513ba8300b74183dfb3fc789c.jpg)  
Figure 2-7. Plotting the true unemployment rate versus the forward-filled series. This plot shows that forward fill did not systematically distort the data.

![](images/e74232d2c3e544f1ff3e55e4e70a3b70a083693394db568fc9e888fdfadde5e2.jpg)

# Backward Fill

Just as you can bring values from the past forward to fill in missing data, you can also choose to propagate values backward. However, this is a case of a lookahead, so you should only do this when you are not looking to predict the future from the data and when, from domain knowledge, it makes more sense to fill in data backward rather than forward in time.

In some settings, forward fill makes sense as the best way to complete missing data, even if “fancier methods” are possible. For example, in medical settings, a missing value often indicates that a medical worker did not think it necessary to remeasure a value, likely because the patient’s measurement was expected to be normal. In many medical cases, this means we could apply forward fill to missing values with the last known value since this was the presumption motivating the medical worker not to retake a measurement.

There are many advantages to forward fill: it is not computationally demanding, it can be easily applied to live-streamed data, and it does a respectable job with imputa‐ tion. We will see an example soon.

# Moving average

We can also impute data with either a rolling mean or median. Known as a moving average, this is similar to a forward fill in that you are using past values to “predict”

missing future values (imputation can be a form of prediction). With a moving aver‐ age, however, you are using input from multiple recent times in the past.

There are many situations where a moving average data imputation is a better fit for the task than a forward fill. For example, if the data is noisy, and you have reason to doubt the value of any individual data point relative to an overall mean, you should use a moving average rather than a forward fill. Forward filling could include random noise more than the “true” metric that interests you, whereas averaging can remove some of this noise.

To prevent a lookahead, use only the data that occurred before the missing data point. So, your implementation would like something like this:

```txt
## R
>>> rolling mean without a lookahead
>>> rand.unemp[, impute.rb.nolookahead := rollapply(c(NA, NA, UNRATE), 3,
>>> function(x) {
>>> if (!is.na[x[3))) x[3] else mean(x, na.rm = TRUE)
>>> }])
>>> bias.unemp[, impute.rb.nolookahead := rollapply(c(NA, NA, UNRATE), 3,
>>> function(x) {
>>> if (!is.na[x[3])) x[3] else mean(x, na.rm = TRUE)
>>> }]) 
```

We set the value of the missing data to the mean of the values that come before it (because we index off the final value and use this value to determine whether it is missing and how to replace it).

![](images/5fc385e6b2c3a394daabd69373fc57530219e97d3f4329f1fdf78eb9ec3f5b17.jpg)

A moving average doesn’t have to be an arithmetic average. For example, exponentially weighted moving averages would give more weight to recent data than to past data. Alternately, a geometric mean can be helpful for time series that exhibit strong serial corre‐ lation and in cases where values compound over time.

When imputing missing data with a moving average, consider whether you need to know the value of the moving average with only forward-looking data, or whether you are comfortable building in a lookahead. If you are unconcerned about a look‐ ahead, your best estimate will include points both before and after the missing data because this will maximize the information that goes into your estimates. In that case, you can implement a rolling window, as illustrated here with the zoo package’s roll apply() functionality:

```markdown
## R
>>> rolling mean with a lookahead
>>> rand.unemp[, complete.rm := rollapply(c(NA, UNRATE, NA), 3,
>>> function(x) {
>>> if (!is.na(x[2])
>>> x[2] 
```

```txt
>>> else
>>> mean(x, nairmingham = TRUE)
>>> }]) 
```

Using both past and future information is useful for visualizations and recordkeeping applications, but, as mentioned before, it is not appropriate if you are preparing your data to be fed into a predictive model.

The results for both a forward-looking moving average and a moving average com‐ puted with future and past data are shown in Figure 2-8.

![](images/ed31bcfd56a7598cd7b902a6234324dbf4d6bde1a747c44d26b10a2d2831b644.jpg)  
Figure 2-8. The dotted line shows the moving average imputation without a lookahead, while the dashed line shows the moving average imputation with a lookahead. Like‐ wise, the squares show the nonlookahead imputed points while the upside down trian‐ gles show the moving average with a lookahead.

A rolling mean data imputation reduces variance in the data set. This is something you need to keep in mind when calculating accuracy, $\mathrm { R } ^ { 2 }$ statistics, or other error met‐ rics. Your calculation may overestimate your model’s performance, a frequent prob‐ lem when building time series models.

![](images/a51a7cbe393d5d5cd34ea0244facc9cd3f7e497377561953a5909cf04599071b.jpg)

# Using a Data Set’s Mean to Impute Missing Data

In a cross-sectional context, it is common to impute missing data by filling in the mean or median for that variable where it is miss‐ ing. While this can be done with time series data, it is not appropri‐ ate for most cases. Knowing the mean for the data set involves looking into the future…and that’s a lookahead!

# Interpolation

Interpolation is a method of determining the values of missing data points based on geometric constraints regarding how we want the overall data to behave. For exam‐ ple, a linear interpolation constrains the missing data to a linear fit consistent with known neighboring points.

Linear interpolation is particularly useful and interesting because it allows you to use your knowledge of how your system behaves over time. For example, if you know a system behaves in a linear fashion, you can build that knowledge in so that only lin‐ ear trends will be used to impute missing data. In Bayesian speak, it allows you to inject a prior into your imputation.

As with a moving average, interpolation can be done such that it is looking at both past and future data or looking in only one direction. The usual caveats apply: allow your interpolation to have access to future data only if you accept that this creates a lookahead and you are sure this is not a problem for your task.

Here we apply interpolation using both past and future data points (see Figure 2-9):

```txt
## R
> ## linear interpolation
> rand.unemp[, impute.li := na.approx(UNRATE)]
> bias.unemp[, impute.li := na.approx(UNRATE)]
>
> ## polynomial interpolation
> rand.unemp[, impute.sp := na.spline(UNRATE)]
> bias.unemp[, impute.sp := na.spline(UNRATE)]
>
> useidx = 90:120
> unemp[useidx, plot(DATE, UNRATE, col = 1, type = 'b'])]
> rand.unemp[useidx, lines(DATE, impute.li, col = 2, lwd = 2, lty = 2)]
> rand.unemp[useidx, lines(DATE, impute.sp, col = 3, lwd = 2, lty = 3)] 
```

![](images/45bfb00acc53f3c10ef872a365c1af6fdb6b8d608f46a26e873ff2272a1b2aab.jpg)  
Figure 2-9. The dashed line shows the linear interpolation while the dotted line shows the spline interpolation.

There are many situations where a linear (or spline) interpolation is appropriate. Consider mean average weekly temperature where there is a known trend of rising or falling temperatures depending on the time of year. Or consider yearly sales data for a growing business. If the trend has been for business volume to increase linearly from year to year, it is a reasonable data imputation to fill in missing data based on that trend. In other words, we could use a linear interpolation, which would account for the trend, rather than a moving average, which would not. If there were a trend of increasing values, the moving average would systematically underestimate the miss‐ ing values.

There are also plenty of situations for which linear (or spline) interpolation is not appropriate. For example, if you are missing precipitation data in a weather data set, you should not extrapolate linearly between the known days; as we all know, that’s not how precipitation works. Similarly, if we are looking at someone’s hours of sleep per day but are missing a few days of data, we should not linearly extrapolate the hours of sleep between the last known days. As an example, one of the known end‐ points might include an all-nighter of studying followed by a 30-minute catnap. This is unlikely to estimate the missing data.

# Overall comparison

Now that we have performed a few different kinds of imputations, we can take a look at the results to compare how the different data imputations behaved on this data set.

We generated two data sets with missing data, one in which data was missing at ran‐ dom and another in which unfavorable data points (high unemployment) were miss‐ ing. When we compare the methods we employed to see which yields the best results, we can see that the mean square error can differ by a high percentage:

```r
## R
> sort[rand.unemp[ , lapply(.SD, function(x) mean((x - unemp$UNRATE)^2,
> na.rs = TRUE)), 
> .SDcols = c("impute.ff", "impute.rs.lookahead",
> "impute.rs.nolookahead", "impute.li",
> "impute.sp")] 
impute.li impute.rs.lookahead impute.sp impute.ff impute.rs.nolookahead
0.0017 0.0019 0.0021 0.0056 0.0080
> sort(bias.unemp[ , lapply(.SD, function(x) mean((x - unemp$UNRATE)^2,
> na.rs = TRUE)), 
> .SDcols = c("impute.ff", "impute.rs.lookahead",
> "impute.rs.nolookahead", "impute.li",
> "impute.sp")] 
impute.sp impute.li impute.rs.lookahead impute.rs.nolookahead impute.ff
0.0012 0.0013 0.0017 0.0030 0.0052 
```

Remember that many of the preceding methods include a lookahead. The only meth‐ ods that do not are the forward fill and the moving average without a lookahead (there is also a moving average with a lookahead). For this reason, it’s not surprising that there is a range of difference in errors and that the methods without a lookahead do not perform as well as the others.

# Final notes

Here we covered the simplest and most often-used methods of missing data imputa‐ tion for time series applications. Data imputation remains an important area of data science research. The more significant the decisions you are making, the more important it is to carefully consider to the potential reasons for data to be missing and the potential ramifications of your corrections. Here are a few cautionary tips you should keep in mind:

• It is impossible to prove that data is truly missing at random, and it is unlikely that missingness is truly random in most real-world occurrences.   
• Sometimes the probability that a measurement is missing is explainable by the variables you have measured, but sometimes not. Wide data sets with many fea‐ tures are the best way to investigate possible explanations for patterns of missing data, but these are not the norm for time series analysis.   
• When you need to understand the uncertainty introduced by imputing values to missing data, you should run through a variety of scenarios and also speak to as many people involved in the data collection process as possible.

• How you handle missing data should account for your downstream use of that data. You must guard carefully against lookaheads or decide how seriously a lookahead will affect the validity of your subsequent work.

# Upsampling and Downsampling

Often, related time series data from different sources will not have the same sampling frequency. This is one reason, among many, that you might wish to change the sam‐ pling frequency of your data. Of course you cannot change the actual rate at which information was measured, but you can change the frequency of the timestamps in your data collection. This is called upsampling and downsampling, for increasing or decreasing the timestamp frequency, respectively.

We downsampled temporal data in “Retrofitting a Time Series Data Collection from a Collection of Tables” on page 26. Here, we address the topic more generally, learn‐ ing the how’s and why’s of both downsampling and upsampling.

![](images/f73942deb286094deed86b0f1cf82d567c726155ca3d9b31255adb0e1f0bbc33.jpg)

Downsampling is subsetting data such that the timestamps occur at a lower frequency than in the original time series. Upsampling is representing data as if it were collected more frequently than was actually the case.

# Downsampling

Anytime you reduce frequency of your data, you are downsampling. This is most often done in the following cases.

The original resolution of the data isn’t sensible. There can be many reasons that the original granularity of the data isn’t sensible. For example, you may be measuring something too often. Suppose you have a data set where someone had measured the outside air temperature every second. Common experience dictates that this meas‐ urement is unduly frequent and likely offers very little new information relative to the additional data storage and processing burden. In fact, it’s likely the measurement error could be as large as the second-to-second air temperature variation. So, you likely don’t want to store such excessive and uninformative data. In this case—that is, for regularly sampled data—downsampling is as simple as selecting out every nth element.

Focus on a particular portion of a seasonal cycle. Instead of worrying about seasonal data in a time series, you might choose to create a subseries focusing on only one season. For example, we can apply downsampling to create a subseries, as in this case, where we generate a time series of January measurements out of what was originally a

monthly time series. In the process, we have downsampled the data to a yearly frequency.

```txt
## R
> unemp[seq.int(from = 1, to = nrow(unemp), by = 12)]
DATE UNRATE
1948-01-01 3.4
1949-01-01 4.3
1950-01-01 6.5
1951-01-01 3.7
1952-01-01 3.2
1953-01-01 2.9
1954-01-01 4.9
1955-01-01 4.9
1956-01-01 4.0
1957-01-01 4.2 
```

Match against data at a lower frequency. You may want to downsample data so that you can match it with other low-frequency data. In such cases you likely want to aggre‐ gate the data or downsample rather than simply dropping points. This can be some‐ thing simple like a mean or a sum, or something more complicated like a weighted mean, with later values given more weight. We saw earlier in the donation data the idea of summing all donations over a single week, since it was the total amount dona‐ ted that was likely to be most interesting.

In contrast, for our economic data, what is most likely to be interesting is a yearly average. We use a mean instead of a rolling mean because we want to summarize the year rather than get the latest value of that year to emphasize recency. (Note the dif‐ ference from data imputation.) We group by formatting the date into a string, repre‐ senting its year as an example of how you can creatively exploit SQL-like operations for time series functionality:

```txt
## R
> unemp[, mean(UNRATE), by = format(DATE, "%Y")] 
format V1
1948 3.75
1949 6.05
1950 5.21
1951 3.28
1952 3.03
1953 2.93
1954 5.59
1955 4.37
1956 4.13
1957 4.30 
```

# Upsampling

Upsampling is not simply the inverse of downsampling. Downsampling makes inher‐ ent sense as something that can be done in the real world; it’s simple to decide to

measure less often. In contrast, upsampling can be like trying to get something for free—that is, not taking a measurement but still somehow thinking you can get highresolution data from infrequent measurements. To quote the author of the popular R time series package XTS:

It is not possible to convert a series from a lower periodicity to a higher periodicity - e.g., weekly to daily or daily to 5 minute bars, as that would require magic.

However, there are legitimate reasons to want to label data at a higher frequency than its default frequency. You simply need to keep in mind the data’s limitations as you do so. Remember that you are adding more time labels but not more information.

Let’s discuss a few situations where upsampling can make sense.

Irregular time series. A very common reason to upsample is that you have an irregu‐ larly sampled time series and you want to convert it to a regularly timed one. This is a form of upsampling because you are converting all the data to a frequency that is likely higher than indicated by the lags between your data. If you are upsampling for this reason, you already know how to do it with a rolling join, as we did this in the case of filling in missing economic data, via R:

```r
## R
> all dates <- seq(from = unemp$DATE[1], to = tail(unemp$DATE, 1),
> by = "months")
> rand.unemp = rand.unemp[J(all dates), roll=0] 
```

Inputs sampled at different frequencies. Sometimes you need to upsample lowfrequency information simply to carry it forward with your higher-frequency infor‐ mation in a model that requires your inputs to be aligned and sampled contemporaneously. You must be vigilant with respect to lookahead, but if we assume that known states are true until a new known state comes into the picture, we can safely upsample and carry our data forward. For example, suppose we know it’s (relatively) true that most new jobs start on the first of the month. We might decide we feel comfortable using the unemployment rate for a given month indicated by the jobs report for the entire month (not considering it a lookahead because we make the assumption that the unemployment rate stays steady for the month).

```txt
R
> daily.unemployment = unemp[J(all.dates), roll = 31]
> daily.unemployment
DATE UNRATE
1948-01-01 3.4
1948-01-02 3.4
1948-01-03 3.4
1948-01-04 3.4
1948-01-05 3.4 
```

Knowledge of time series dynamics. If you have underlying knowledge of the usual tem‐ poral behavior of a variable, you may also be able to treat an upsampling problem as a missing data problem. In that case, all the techniques we’ve discussed already still apply. An interpolation is the most likely way to produce new data points, but you would need to be sure the dynamics of your system could justify your interpolation decision.

As discussed earlier, upsampling and downsampling will routinely happen even in the cleanest data set because you will almost always want to compare variables of dif‐ ferent timescales. It should also be noted that Pandas has particularly handy upsam‐ pling and downsampling functionality with the resample method.

# Smoothing Data

Smoothing data can be done for a variety of reasons, and often real-world time series data is smoothed before analysis, especially for visualizations that aim to tell an understandable story about the data. In this section we discuss further why smooth‐ ing is done, as well as the most common time series smoothing technique: exponen‐ tial smoothing.

# Purposes of smoothing

While outlier detection is a topic in and of itself, if you have reason to believe your data should be smoothed, you can do so with a moving average to eliminate measure‐ ment spikes, errors of measurement, or both. Even if the spikes are accurate, they may not reflect the underlying process and may be more a matter of instrumentation problems; this is why it’s quite common to smooth data.

Smoothing data is strongly related to imputing missing data, and so some of those techniques are relevant here as well. For example, you can smooth data by applying a rolling mean, with or without a lookahead, as that is simply a matter of the point’s position relative to the window used to calculate its smoothed value.

When you are smoothing, you want to think about a number of questions:

• Why are you smoothing? Smoothing can serve a number of purposes:

# Data preparation

Is your raw data unsuitable? For example, you may know very high values are unlikely or unphysical, but you need a principled way to deal with them. Smoothing is the most straightforward solution.

# Feature generation

The practice of taking a sample of data, be it many characteristics about a person, image, or anything else, and summarizing it with a few metrics. In

this way a fuller sample is collapsed along a few dimensions or down to a few traits. Feature generation is especially important for machine learning.

# Prediction

The simplest form of prediction for some kinds of processes is mean rever‐ sion, which you get by making predictions from a smoothed feature.

# Visualization

Do you want to add some signal to what seems like a noisy scatter plot? If so, what is your intention in doing so?

• How will your outcomes be affected by smoothing or not smoothing?

— Does your model assume noisy and uncorrelated data, whereby your smooth‐ ing could compromise this assumption?   
— Will you need to smooth in a live production model? If so, you need to choose a smoothing method that does not employ a lookahead.   
— Do you have a principled way to smooth, or will you simply do a hyperpara‐ meter grid search? If the latter, how will you make sure that you use a timeaware form of cross-validation such that future data does not leak backward in time?

# What Are Hyperparameters?

The term hyperparameters can sound awfully scary, but they are simply the numbers you use to tune, or adjust, the performance of a statistical or machine learning model. To optimize a model, you try several variations of the hyperparameters of that model, often performing a grid search to identify these parameters. A grid search is exactly that: you construct a “grid” of all possible combinations of hyperparameters in a search space and try them all.

For example, imagine an algorithm takes hyperparameters A and B, where we think reasonable values for A are 0, 1, or 2, and we think reasonable values for B are 0.7, 0.75, or 0.8. Then we would have nine possible hyperparameter options for our grid: every possible combination of potential A values (of which there are 3) with every possible B value (of which there are 3) for a total of $3 \times 3 = 9$ possibilities. As you can see, hyperparameter tuning can quickly get quite labor-intensive.

# Exponential smoothing

You often won’t want to treat all time points equally when smoothing. In particular, you may want to treat more recent data as more informative data, in which case exponential smoothing is a good option. In contrast to the moving average we looked at before—where each point where data was missing could be imputed to the mean of

its surrounding points—exponential smoothing is geared to be more temporally aware, weighting more recent points higher than less recent points. So, for a given window, the nearest point in time is weighted most heavily and each point earlier in time is weighted exponentially less (hence the name).

The mechanics of exponential smoothing work as follows. For a given time period $t ,$ you find the smoothed value of a series by computing:

$$
\text {S m o o t h e d} t = S _ {t} = d \times ; S _ {t - 1} + (1 - d) \times x _ {t}
$$

Think about how this propagates over time. The smoothed value at time $( t - 1 )$ is itself a product of the same thing:

$$
S _ {t - 1} = d \times S _ {t - 2} + (1 - d) \times x _ {t - 1}
$$

So we can see a more complex expression for the smoothed value at time t:

$$
d \times (d \times S _ {t - 2} + (1 - d) \times x _ {t - 1}) + (1 - d) \times x _ {t}
$$

Mathematically inclined readers will notice that we have a series of the form:

$$
d ^ {3} \times x _ {t - 3} + d ^ {2} \times x _ {t - 2} + d \times x _ {t - 1}
$$

In fact, it is thanks to this form that exponential moving averages are quite tractable. More details are widely available online and in textbooks; see my favorite summaries in “More Resources” on page 69.

I will illustrate smoothing in Python, as Pandas includes a variety of smoothing options. Smoothing options are also widely available in R, including base R, as well as many time series packages.

While we have been looking at US unemployment rate data, we will switch to another commonly used data set: the airline passenger data set (which dates back to Box and Jenkins’s famous time series book and is widely available). The original data set is an account of the thousands of monthly airline passengers broken down by month:

```markdown
```
python
>>> air
Date Passengers
0 1949-01 112
1 1949-02 118
2 1949-03 132
3 1949-04 129
4 1949-05 121
5 1949-06 135 
```

<table><tr><td>6</td><td>1949-07</td><td>148</td></tr><tr><td>7</td><td>1949-08</td><td>148</td></tr><tr><td>8</td><td>1949-09</td><td>136</td></tr><tr><td>9</td><td>1949-10</td><td>119</td></tr><tr><td>10</td><td>1949-11</td><td>104</td></tr></table>

We can easily smooth the values of passengers using a variety of decay factors and applying the Pandas ewma() function, like so:

```python
>>> air['Smooth.5'] = pd ewma(air, alpha = .5).Passengers
>>> air['Smooth.9'] = pd ewma(air, alpha = .9).Passengers 
```

As we can see, the level of the alpha parameter, also called the smoothing factor, affects how much the value is updated to its current value versus retaining informa‐ tion from the existing average. The higher the alpha value, the more quickly the value is updated closer to its current price. Pandas accepts a number of parameters, all of which plug into the same equation, but they offer multiple ways to think about specifying an exponential moving average.3

<table><tr><td colspan="5">## python</td></tr><tr><td colspan="5">&gt;&gt;&gt; air</td></tr><tr><td></td><td>Date</td><td>Passengers</td><td>Smooth.5</td><td>Smooth.9</td></tr><tr><td>0</td><td>1949-01</td><td>112</td><td>112.000000</td><td>112.000000</td></tr><tr><td>1</td><td>1949-02</td><td>118</td><td>116.000000</td><td>117.454545</td></tr><tr><td>2</td><td>1949-03</td><td>132</td><td>125.142857</td><td>130.558559</td></tr><tr><td>3</td><td>1949-04</td><td>129</td><td>127.200000</td><td>129.155716</td></tr><tr><td>4</td><td>1949-05</td><td>121</td><td>124.000000</td><td>121.815498</td></tr><tr><td>5</td><td>1949-06</td><td>135</td><td>129.587302</td><td>133.681562</td></tr><tr><td>6</td><td>1949-07</td><td>148</td><td>138.866142</td><td>146.568157</td></tr><tr><td>7</td><td>1949-08</td><td>148</td><td>143.450980</td><td>147.856816</td></tr><tr><td>8</td><td>1949-09</td><td>136</td><td>139.718200</td><td>137.185682</td></tr><tr><td>9</td><td>1949-10</td><td>119</td><td>129.348974</td><td>120.818568</td></tr><tr><td>10</td><td>1949-11</td><td>104</td><td>116.668295</td><td>105.681857</td></tr></table>

However, simple exponential smoothing does not perform well (for prediction) in the case of data with a long-term trend. Holt’s Method and Holt–Winters smooth‐ ing , are two exponential smoothing methods applied to data with a trend, or with a trend and seasonality.

There are many other widely used smoothing techniques. For example, Kalman fil‐ ters smooth the data by modeling a time series process as a combination of known dynamics and measurement error. LOESS (short for “locally estimated scatter plot smoothing”) is a nonparametric method of locally smoothing data. These methods, and others, gradually offer more complex ways of understanding smoothing but at increased computational cost. Of note, Kalman and LOESS incorporate data both

earlier and later in time, so if you use these methods keep in mind the leak of infor‐ mation backward in time, as well as the fact that they are usually not appropriate for preparing data to be used in forecasting applications.

Smoothing is a commonly used form of forecasting, and you can use a smoothed time series (without lookahead) as one easy null model when you are testing whether a fancier method is actually producing a successful forecast.

# How to Start a Smoothing Calculation

One important reason to use a packaged exponential smoothing function is that it is difficult to get the start of your smoothed series correct. Imagine that the first point in your series is equal to 3 and the second point is equal to 6. Also imagine that your discounting factor (which you set according to how quickly you want the average to adapt to the most recent information) is equal to 0.7, so that you would compute the second point in your series as:

$$
3 \times 0. 7 + 6 \times (1 - 0. 7) = 3. 9
$$

In fact, you would be wrong. The reason is that multiplying your discounting factor of 0.7 by 3 implicitly assumes that 3 is the sum of your knowledge going back in time to infinity, rather than a mere single data point you just measured. Therefore, it unduly weighs 3 relative to 6 as though 3 has a significantly more longstanding set of knowledge than it actually does.

The algebra to derive the correct expression is too complicated for a short sidebar, but check the additional resources at the end of this book for more details. In the meantime, I will leave you with the general guidance that 6 should be weighed a little more heavily than it is here, and 3 a little less. As you get further along in a time ser‐ ies, your older values should be weighted closer to your true discounting factor of 0.7. How soon this happens can be seen in the algebra, so take a look at the additional resources if you are interested.

# Seasonal Data

Seasonality in data is any kind of recurring behavior in which the frequency of the behavior is stable. It can occur at many different frequencies at the same time. For example, human behavior tends to have a daily seasonality (lunch at the same time every day), a weekly seasonality (Mondays are similar to other Mondays), and a yearly seasonality (New Year’s Day has low traffic). Physical systems also demon‐ strate seasonality, such as the period the Earth takes to revolve around the sun.

Identifying and dealing with seasonality is part of the modeling process. On the other hand, it’s also a form of data cleaning, such as the economically important US Jobs Report. Indeed, many government statistics, particularly economic ones, are desea‐ sonalized in their released form.

To see what seasonal data smoothing can do, we return to the canonical data set on airline passenger counts. A plot quickly reveals that this is highly seasonal data, but only if you make the right plot.

Notice the difference between using R’s default plot (which uses points; see Figure 2-10) versus adding the argument to indicate that you want a line (Figure 2-11).

![](images/0d570b099c1388eaf7ceca0672d22b017f0087e9429575a0b3380900408548e7.jpg)  
Figure 2-10. The increasing mean and variance of the data are apparent from a scatter plot, but we do not see an obvious seasonal trend.

![](images/cb34a4d02c0fb2949c8c488cab86790d6d12c709fbf669c49eeb78236a668556.jpg)  
Figure 2-11. A line plot makes the seasonality abundantly clear.

If you were looking only at the default R plot, the seasonal nature of the data might elude you. Hopefully that wouldn’t remain the case for long, as no doubt you would also do things to explore your data more fully, perhaps with an autocorrelation plot (discussed in Chapter 3) or other diagnostics.

![](images/c40bc9247731b2880e869e032d5aa3b9ace8a217e5d9801e11840c909580dc10.jpg)

# Humans Are Creatures of Habit

With human behavior data there is almost always some form of seasonality, even with several cycles (an hourly pattern, a weekly pattern, a summer-winter pattern, etc.).

The scatter plot does show some information more clearly than the line plot. The var‐ iance of our data is increasing, as is the mean, which is most obvious when we see a cloud of data points beaming outward in a conical shape, tilted upward. This data clearly has a trend, and so we would likely transform it with a log transform or differ‐ encing, depending on the demands of our model. This data also clearly has a trend of increasing variance. We’ll talk more about model-specific data transformations on seasonal data in the modeling chapters, so we won’t elaborate here.

We also get useful information apart from the evidence for seasonality from the line plot. We get information about what kind of seasonality. That is, we see that the data is not only seasonal, but seasonal in a multiplicative way. As the overall values get larger, so do the season swings (think of this as sizing from peak to trough).

We can decompose the data into its seasonal, trend, and remainder components very easily as shown here with just one line of R:

```r
## R
> plot(stl(AirPassengers, "periodic")) 
```

The resulting plot seems very sensible based on the original data (see Figure 2-12). We can imagine adding the seasonal, trend, and remainder data back together to get the original series. We can also see that this particular decomposition did not take into account the fact that this series shows a multiplicative seasonality rather than an additive one because the residuals are greatest at the beginning and end of the time series. It seems like this decomposition settled on the average seasonal variance as the variance for the seasonal component.

![](images/eb8cd9001f45ca604c717a324889c3b49e2f61e1357eef5900c05cb95ec7d329.jpg)  
Figure 2-12. A decomposition of the original time series into a seasonal component, a trend, and the residuals. Pay attention to each plot’s y-axis, as they are quite different. Note that this is the reason for the gray bars on the right side of each plot. These gray bars are all the same absolute size (in units of the y-axis), so that their relatively differ‐ ent display is a visual reminder of the different y-axis scales across different components.

To get an initial understanding of how this works, we can look at the official R documentation:

The seasonal component is found by LOESS smoothing the seasonal subseries (the ser‐ ies of all January values, ...). If s.window $=$ “periodic” smoothing is effectively replaced by taking the mean. The seasonal values are removed, the remainder smoothed to find the trend. The overall level is removed from the seasonal component and added to the

trend component. This process is iterated a few times. The remainder component is the residuals from the seasonal plus trend fit.

LOESS, introduced earlier, is a computationally taxing method for smoothing data points that involves a moving window to estimate the smoothed value of each point based on its neighbors (I hope your lookahead alarm is sounding!).

# Seasonal Data and Cyclical Data

Seasonal time series are time series in which behaviors recur over a fixed period. There can be multiple periodicities reflecting different tempos of seasonality, such as the seasonality of the 24-hour day versus the 12-month calendar season, both of which exhibit strong features in most time series relating to human behavior.

Cyclical time series also exhibit recurring behaviors, but they have a variable period. A common example is a business cycle, such as the stock market’s boom and bust cycles, which have an uncertain duration. Likewise, volcanoes show cyclic but not seasonal behaviors. We know the approximate periods of eruption, but these are not precise and vary over time.

# Time Zones

Time zones are intrinsically tedious, painful, and difficult to get right even with a lot of effort. That is why you should never use your own solution. From their very inven‐ tion, time zones have been complicated, and they have only gotten more so with the advent of personal computers. There are many reasons for this:

• Time zones are shaped by political and social decisions.   
• There is no standard way to transport time zone information between languages or via an HTTP protocol.   
• There is no single protocol for naming time zones or for determining start and end dates of daylight savings offsets.   
• Because of daylight savings, some times occur twice a year in their time zones!

Most languages rely on the underlying operating system to get time zone informa‐ tion. Unfortunately, the built-in automatic time retrieval function in Python, date time.datetime.now(), does not return a time zone–aware timestamp. Part of this is by design. Some decisions made in the standard library include forbidding time zone information in the datetime module (because this information changes so often) and allowing datetime objects both with and without time zone information. However, comparing an object with a time zone to one without a time zone will cause a TypeError.

Some bloggers claim that a majority of libraries are written with the assumption that tzinfo $1 = =$ None. Although this is a difficult claim to substantiate, it is consistent with much experience. People also report difficulty pickling time zone–stamped objects, so this is also something to check on early if you plan to use pickling.4

So let’s have a look at working with time zones in Python. The main libraries you are likely to use are datetime, pytz, and dateutil. Additionally Pandas offers conve‐ nient time zone–related functionality built on top of the latter two libraries.

We’ll cover the most important time zone functionality next.

First, notice that when you retrieve a “now” from the datetime module, it does not come with time zone information, even though it will give you the appropriate time for your time zone. Note, for example, the difference in the responses for now() and utcnow():

```markdown
## python
>>> datetimedatetime.utcnow()
datetimedatetime(2018, 5, 31, 14, 49, 43, 187680)
>>> datetimedatetime-now()
datetimedatetime(2018, 5, 31, 10, 49, 59, 984947)
>>> # as we can see, my computer does not return UTC
>>> # even though there is no time zone attached
>>> datetimedatetime-nowdatetime时间和zone.utc)
datetimedatetime(2018, 5, 31, 14, 51, 35, 601355, tzinfo=datetime时间和zone.utc) 
```

Notice that if we do pass in a time zone, we will get the correct information, but this is not the default behavior. To work with time zones in Python, we create a time zone object, such western as for the US Pacific time zone:

```txt
## python
>>> western = pytztimezone('US/Pacific')
>>> westernzone
'US/Pacific' 
```

We can then use these objects to localize a time zone as follows:

```txt
## python
>>> # the API supports two ways of building a time zone-aware time,
>>> # either via 'localize' or to convert a time zone from one locale
>>> # to another
>>> # here we localize
>>> loc_dt = western.localize(dtime,(2018,5,15,12,34,0)) 
```

```txt
datetime datetime(2018, 5, 15, 12, 34, tzinfo=<DstTzInfo 'US/Pacific' PDT-1 day, 17:00:00 DST>) 
```

Note, however, that passing the time zone directly into the datetime constructor will often not produce the result we were expecting:

```python
>>> london_tz = pytztimezone('Europe/London')
>>> london_dt = loc_dt.astimezone(london_tz)
>>> london_dt
datetime.datetime(2018, 5, 15, 20, 34, tzinfo=<DstTzInfo 'Europe/London' BST+1:00:00 DST>) 
>>> f = '%Y-%m-%d %H:%M:%S %Z%z'
>>> datetime.datetime(2018, 5, 12, 12, 15, 0, tzinfo = london_tz).strftime(f)
'2018-05-12 12:15:00 LMT-0001'
>>> ## as highlighted in the pytz documentation using the tzinfo of
>>> ## the datetimedatetime initializer does not always lead to the
>>> ## desired outcome, such as with the London example
>>> ## according to the pytz documentation, this method does lead to
>>> ## the desired results in time zones without daylight savings 
```

This is important, such as when you are calculating time deltas. The first example of the following three is the gotcha:

```prolog
>>> # generally you want to store data in UTC and convert only when
>>> # generating human-readable output
>>> # you can also do date arithmetic with time zones
>>> event1 = datetime.datetime(2018, 5, 12, 12, 15, 0, tzinfo = london_tz)
>>> event2 = datetime.datetime(2018, 5, 13, 9, 15, 0, tzinfo = western)
>>> event2 - event1
>>> # this will yield the wrong time delta because the time zones
>>> # # haven't been labelled properly
>>> event1 = london_tz.location(
    datetime.datetime(2018, 5, 12, 12, 15, 0))
>>> event2 = western.location(
    datetime.datetime(2018, 5, 13, 9, 15, 0))
>>> event2 - event1
>>> event1 = london_tz.location(
    datetime.datetime(2018, 5, 12, 12, 15, 0)).
                    astimezonedatetime时间和zone.utc)
>>> event2 = western.location(
    datetime.datetime(2018, 5, 13, 9, 15, 0)). 
```

```txt
astimezone(matetime.timezone.utc) >> event2 - event1 
```

pytz provides a list of common time zones and time zones by country, both of which can be handy references:

```python
## python
## have a look at pytz(common_timezones
>>> pytz.common_timezones
(long output...)
## or country specific
>>> pytz_country_timezones('RU')
['Europe/Kaliningrad', 'Europe/Moscow', 'Europe/Simferopol',
'Europe/Volgograd', 'Europe/Kirov', 'Europe/Astrakhan',
'Europe/Saratov', 'Europe/Ulyanovsk', 'Europe/Samara',
'Asia/Yekaterinburg', 'Asia/Omsk', 'Asia/Novosibirsk',
'Asia/Barnaul', 'Asia/Tomsk', 'Asia/Novokuznetsk',
'Asia/Krasnoyarsk', 'Asia/Irkutsk', 'Asia/Chita',
'Asia/Yakutsk', 'Asia/Khandyga', 'Asia/Vladivostok',
'Asia/Ust-Nera', 'Asia/Magadan', 'Asia/Sakhalin',
'Asia/Srednekolymsk', 'Asia/Kamchatka', 'Asia/Anadyr']
>>> pytz_country_timezones('fr')
>>> ['Europe/Paris'] 
```

A particularly hairy issue is the matter of daylight savings. Certain human-readable times exist twice (falling behind in the autumn), while others do not exist at all (skip ahead in the spring):

```python
>>> #time zones
>>> ambig_time = western.localize(
    datetime涣time(2002, 10, 27, 1, 30, 00)).
    astimezone(datetime 时间zone.utc)
>>> ambig_timeEarlier = ambig_time - datetime.timedelta(hours=1)
>>> ambig_time_later = ambig_time + datetime.timedelta(hours=1)
>>> ambig_timeEarlier. astimezone(western)
>>> ambig_time. astimezone(western)
>>> ambig_time_later. astimezone(western)
>>> #results in this output
datetime.datetime(2002, 10, 27, 1, 30,
        tzinfo=<DstTzInfo 'US/Pacific' PDT-1 day, 17:00:00 DST>
datetime.datetime(2002, 10, 27, 1, 30,
        tzinfo=<DstTzInfo 'US/Pacific' PST-1 day, 16:00:00 STD>
datetime.datetime(2002, 10, 27, 2, 30,
        tzinfo=<DstTzInfo 'US/Pacific' PST-1 day, 16:00:00 STD>
>>> #notice that the last two timestamps are identical, no good!
>>> #in this case you need to use is_dst to indicate whether daylight
>>> # savings is in effect 
```

```python
>>> ambig_time = western.localize(
    datetimedatetime(2002, 10, 27, 1, 30, 00), is.dst = True).
    astimezonedatetime时间和zone.utc)
>>> ambig_time Earlier = ambig_time - datetime.timedelta(hours=1)
>>> ambig_time_later = ambig_time + datetime.timedelta(hours=1)
>>> ambig_timeEarlier. astimezone(western)
>>> ambig_time. astimezone(western)
>>> ambig_time_later. astimezone(western)
datetimedatetime(2002, 10, 27, 0, 30,
    tzinfo=<DstTzInfo 'US/Pacific' PDT-1 day, 17:00:00 DST>
) datetimedatetime(2002, 10, 27, 1, 30,
    tzinfo=<DstTzInfo 'US/Pacific' PDT-1 day, 17:00:00 DST>
) datetimedatetime(2002, 10, 27, 1, 30,
    tzinfo=<DstTzInfo 'US/Pacific' PST-1 day, 16:00:00 STD>) 
```

Time zone concerns may not be important to your work, so the utility of this knowl‐ edge will depend on the nature of your data. There are certainly situations where get‐ ting this wrong could be catastrophic (say, generating weather forecasts for commercial airliners flying during the time change and suddenly finding their posi‐ tions drastically altered).

# Preventing Lookahead

Lookahead is dangerously easy to introduce into a modeling pipeline, especially with vectorized functional data manipulation interfaces, such as those offered by R and Python. It is easy to shift a variable in the wrong direction, shift it more or less than you intended, or otherwise find yourself with data that is not entirely “honest” in that you have data available before it would have been known in your system.

Unfortunately, there isn’t a definitive statistical diagnosis for lookahead—after all, the whole endeavor of time series analysis is modeling the unknown. Unless a system is somewhat deterministic with known dynamical laws, it can be difficult to distinguish a very good model from a model with lookahead—that is, until you put a model into production and realize either that you are missing data when you planned to have it, or simply that your results in production do not reflect what you see during training.

The best way to prevent this embarrassment is constant vigilance. Whenever you are time-shifting data, smoothing data, imputing data, or upsampling data, ask yourself whether you could know something at a given time. Remember that doesn’t just include calendar time. It also includes realistic time lags to reflect how long a delay there is between something happening and your organization having that data avail‐ able. For example, if your organization scrapes Twitter only weekly to gather its sen‐ timent analysis data, you need to include this weekly periodicity in your training and validation data segmentation. Similarly, if you can retrain your model only once a

month, you need to figure out what model would apply to what data over time. You can’t, for example, train a model for July and then apply it to July for testing, because in a real situation, you wouldn’t have that model trained up in time if training takes you a long time.

Here are some other ideas to use as a general checklist. Keep them in mind both when planning to build a model and when auditing your process after the fact:

• If you are smoothing data or imputing missing data, think carefully about whether it might impact your results by introducing a lookahead. And don’t just think about it—experiment as we did earlier and see how the imputations and smoothing work. Do they seem to be forward looking? If so, can you justify using them? (Probably not.)   
• Build your entire process with a very small data set (only a few rows in a data.table or a few row time steps in whatever data format). Then, do random spot checks at each step in the process and see whether you accidentally shift any information temporally to an inappropriate place.   
• For each kind of data, find out what the lag is for it relative to its own timestamp. For example, if the timestamp is when the data “happened” but not when it was uploaded to your servers, you need to know that. Different columns of a data frame may have different lags. To address this, you can either customize your lag per data frame or (better and more realistic) pick the biggest lag and apply that to everything. While you won’t want to unduly pessimize your model, it’s a good starting point after which you can relax these overly constrained rules one at a time, carefully!   
• Use time-aware error (rolling) testing or cross-validation. This will be discussed in Chapter 11, but remember that randomizing your training versus testing data sets does not work with time series data. You do not want information from the future to leak into models for the past.   
• Intentionally introduce a lookahead and see how your model behaves. Try vari‐ ous degrees of lookahead, so you have an idea how it shifts accuracy. If you have some idea of the accuracy with lookahead, you have an idea of what the ceiling on a real model without unfair knowledge of the future will do. Remember that many time series problems are extremely difficult, so a model with a lookahead may seem great until you realize you are dealing with a high-noise/low-signal data set.   
• Add features slowly, particularly features you might be processing, so that you can look for jumps. One sign of a lookahead is when a particular feature is unex‐ pectedly good, and there isn’t a very good explanation. At the top of your explan‐ ation list should always be “lookahead.”

![](images/13bcb204d10b989c6007708426be5bb739168925e0aa1dd4904cd5ed5dbbe43d.jpg)

Processing and cleaning time-related data can be a tedious and detail-oriented process.

There is tremendous danger in data cleaning and processing of introducing a lookahead! You should have lookaheads only if they are intentional, and this is rarely appropriate.

# More Resources

• On missing data:

Steffen Moritz et al., “Comparison of Different Methods for Univariate Time Series Imputation in R,” unpublished research paper, October 13, 2015, https://perma.cc/ M4LJ-2DFB.

This thorough summary from 2015 outlines available methods for imputing time series data in the case of univariate time series data. Univariate time series data is a particular challenge because many advanced missing data imputation methods rely on looking at distributions among covariates, an option that is not available in the case of a univariate time series. This paper summarizes both the usability and performance of various R packages as well as empirical results of the methods available on a variety of data sets.

James Honaker and Gary King, “What to Do About Missing Values in Time-Series Cross-Section Data,” American Journal of Political Science 54, no. 2 (2010): 561– 81, https://perma.cc/8ZLG-SMSX.

This article explores best practices for missing data in time series with wide panels of covariates.

Léo Belzile, “Notes on Irregular Time Series and Missing Values,” n.d. https:// perma.cc/8LHP-92FP.

The author provides examples of working with irregular data as a missing data problem and an overview of some commonly used R packages.

• On time zones:

Tom Scott, The Problem with Time & Timezones, Computerphile video, December 30, 2013, https://oreil.ly/iKHkp.

This 10-minute YouTube video with over 1.5 million views outlies the perils and challenges of dealing with time zones, particularly in the context of a web application.

Wikipedia, “Time Zone,” https://perma.cc/J6PB-232C.

This is a fascinating history in short form that gives some insight into how timekeeping worked before the last century and how technology advance‐ ments (beginning with the railway) led to the increasing need for people in

different locations to coordinate their time. There are also some fun maps of time zones.

Declan Butler, “GPS Glitch Threatens Thousands of Scientific Instruments,” Nature, April 3, 2019, https://perma.cc/RPT6-AQBC.

This article is not directly related to time zones but to the problem of time‐ stamping more generally. It describes a recent problem in which a bug in the US Global Positioning System could cause a problem with timestamped data because it transmits a binary 10-digit “week number” that began counting from January 6, 1980. This system can cover only 1,024 weeks total (2 raised to the power of 10). This count was reached for the second time in April 2019. Devices that were not designed to accommodate this limitation reset to zero and incorrectly timestamped scientific and industrial data. This article describes some of the difficulties with a fairly limited representation of time combined with scientific devices that were not future-proofed to account for this problem.

• On smoothing and seasonality:

Rob J. Hyndman and George Athanasopoulos,“Exponential Smoothing,” in Fore‐ casting: Principles and Practices, 2nd ed. (Melbourne: OTexts, 2018), https:// perma.cc/UX4K-2V5N.

This chapter of Hyndman and Athanasopoulos’s introductory academic textbook covers exponential smoothing methods for time series data, includ‐ ing a helpful taxonomy of exponential smoothing and extensions of expo‐ nential smoothing to forecasting applications.

David Owen, “The Correct Way to Start an Exponential Moving Average,” For‐ ward Motion blog, January 31, 2017, https://perma.cc/ZPJ4-DJJK.

As highlighted in a side note earlier, exponential moving averages are con‐ ceptually simple and easy to calculate, with the complication that how to “start” off the calculation is a little complicated. We want to make sure our moving average adapts to new information in a way that recognizes how long it has been recording information. If we start a moving average without giving consideration to this, even a new moving average will behave as though it has infinite lookback and unduly discount new information rela‐ tive to what it should do. More details and a computational solution are pre‐ sented in this blog post.

Avner Abrami, Aleksandr Arovkin, and Younghun Kim, “Time Series Using Expo‐ nential Smoothing Cells,” unpublished research paper, last revised September 29, 2017, https://perma.cc/2JRX-K2JZ.   
This is an extremely accessible time series research paper elaborating on the idea of simple exponential smoothing to develop “exponential smoothing cells.”   
• On functional data analysis:   
Jane-Ling Wang, Jeng-Min Chiou, and Hans-Georg Müller, “Review of Functional Data Analysis,” Annual Reviews of Statistics and its Application, 2015, https:// perma.cc/3DNT-J9EZ.   
This statistics article offers an approachable mathematical overview of important functional data analysis techniques. There are also helpful visuali‐ zation techniques illustrated throughout the article.   
Shahid Ullah and Caroline F. Finch, “Applications of Functional Data Analysis: A Systematic Review,” BMC Medical Research Methodologyv, 13, no. 43 (2013), https://perma.cc/VGK5-ZEUX.   
This review takes a multidisciplinary approach to surveying recently pub‐ lished analyses using functional data analysis. The authors argue that the techniques of functional data analysis have much wider applicability than is currently appreciated and make the case that the biological and health scien‐ ces could benefit from more use of these techniques to look at medical time series data.

# Exploratory Data Analysis for Time Series

We will break down our discussion of exploratory data analysis for time series into two distinct sections. First, we discuss the application of commonly used data meth‐ ods on time series. In particular, we discuss how histograms, plotting, and group-by operations can be applied to time series data.

Second, we highlight fundamentally temporal methods for time series analysis—that is, methods developed specifically for time series data and that make sense only in the context of data points that have a temporal relationship with one another (rather than a cross-sectional one).

# Familiar Methods

We begin by thinking about how to apply commonly used data exploration techni‐ ques to time series data sets. The process is the same as what you have performed on non-time-series data to start with.

You will want to know the columns that are available, their value ranges, and what logical units of measurement work best. You will want to address the same explora‐ tory questions you would ask about any new data set, such as:

• Are any of the columns strongly correlated with one another?   
• What is the overall mean of an interesting variable? What is its variance?

To answer these, you can use familiar techniques such as plotting, taking summary statistics, applying histograms, and using targeted scatter plots. You’ll also want to answer explicitly time-oriented questions such as:

• What is the range of values you see, and do they vary by time period or some other logical unit of analysis?

• Does the data look consistent and uniformly measured, or does it suggest changes in either measurement or behavior over time?

To address these, you will want the previously mentioned methods—histograms, scatter plots, and summary statistics—to take into account the temporal axis. To implement this behavior, we will incorporate time into our statistics, as an axis in our graphs or as a group in group-by operations, such as those applied to histograms or scatter plots.

In the rest of this section, we will look at examples of exploratory data analysis with a few different kinds of time series data to explore how we can use traditional nontem‐ poral methods directly or with time-specific modifications. To demonstrate these exploratory methods, we will use European stock markets data, a time series data set available in base R.

# Group-by Operations

With time series data, there are groups just as there are with nontemporal data, so group-by-style thinking is still applicable. For example, in cross-sectional data you might take group means based on age, gender, or neighborhood. In time series analy‐ sis, you might find exploratory group-by operations useful, such as computing a monthly average or weekly medians. You can also combine temporal and nontempo‐ ral groups, such as the monthly average calorie intake per gender in a population, or the weekly median amount of sleep per age group in a set of hospital patients. There are many ways that grouping data can allow the temporal axis to interact with other relationships within the data. For example:

• You may want to analyze different time periods separately, if you have exoge‐ nous reasons to expect a regime change. Use your exploratory analysis to find sensible demarcations for different meaningful time periods.   
• You may discover that data presented as a single series (such as when donations come in) is clearer when broken down into many parallel processes (such as individual donation timelines).

# Plotting

Let’s explore a typical time series provided by R, the EuStockMarkets data set. To get a sense of it, we can look at the head of the data set:

```txt
## R
> head(EuStockMarkets)
DAX SMI CAC FTSE
[1,] 1628.75 1678.1 1772.8 2443.6
[2,] 1613.63 1688.5 1750.5 2460.2 
```

[3,] 1606.51 1678.6 1718.0 2448.2   
[4,] 1621.04 1684.1 1708.1 2470.4   
[5,] 1618.16 1686.6 1723.1 2484.7   
[6,] 1610.61 1671.6 1714.3 2466.8

This is a built-in data set in base R, and it contains the daily closing prices of four major European stock indexes from 1991 to 1998. The data set only includes business days.

This data set is better prepared than the ones we have looked at so far because it’s already nicely formatted and sampled for us. We don’t need to worry about missing values, time zones, or incorrect measurements, so we can jump right into exploratory data analysis.

Our first steps for looking at this data will be very similar to a non-time-based time series, although we have a simpler option with temporal data than we would with cross-sectional data. We can plot each value individually, and such a plot makes sense. Contrast this with cross-sectional data, where a plot of any individual feature against its index in the data set will only reveal, if anything, the order in which data was entered in a data table or returned from a SQL server (in other words, an arbi‐ trary order), but nothing about the underlying situation. However, in the case of time series, plotting is quite informative, as you can see in Figure 3-1.

```txt
R > plot(EuStockMarkets) 
```

![](images/4fad88a4a774d0f57cbc069a30534b72239a7748204bc908afdfad9b6c22883a.jpg)  
Figure 3-1. A simple plot of the time series data.

Notice that the image is automatically segmented into different time series with a simple plot() command. In fact, as shown here, we are using an R mts object (we would be using a ts object if there were only one time series, but we have several in parallel):

```markdown
## R
> class(EuStockMarkets)
[1] "mts" "ts" "matrix" 
```

A number of popular packages make heavy use of ts objects and derivative classes. These objects come with nice automatic calls to appropriate plotting functions, as we see in the previous example where a simple call to plot() created a nicely labeled, multipanel plot. ts objects also have a few convenience functions:

• frequency for finding out the yearly frequency of the data:

```markdown
## R
> frequency(EuStockMarkets)
[1] 260 
```

• start and end to find the first and last time represented in the series:

```txt
## R
> start(EuStockMarkets)
[1] 1991 130
> end(EuStockMarkets)
[1] 1998 169 
```

• window to take a temporal section of the data:

```julia
## R
> window(EuStockMarkets, start = 1997, end = 1998)
Time Series:
Start = c(1997, 1)
End = c(1998, 1)
Frequency = 260 DAX SMI CAC FTSE
1997.000 2844.09 3869.8 2289.6 4092.5
1997.004 2844.09 3869.8 2289.6 4092.5
1997.008 2844.09 3869.8 2303.8 4092.5
...
1997.988 4162.92 6115.1 2894.5 5168.3
1997.992 4055.35 5989.9 2822.9 5020.2
1997.996 4125.54 6049.3 2869.7 5018.2
1998.000 4132.79 6044.7 2858.1 5049.8 
```

There are pluses and minuses to the ts class. As mentioned earlier, ts and its deriva‐ tive classes are used in many time series packages. Also, automatic setup of plotting parameters can be helpful. However, indexing can sometimes be tricky, and the

process of accessing subsections of data with window can feel cumbersome over time. In this book, you will see several ways of containing and accessing time series data, and it will be up to you to choose the most convenient for your use cases.

# Histograms

Let’s continue our comparison of exploratory data analysis in time series and nontime-series data. We’d like to, for example, take a histogram of the data, just as with most exploratory data analysis. We add a wrinkle by also taking a histogram of the differenced data because we want to use our time axis (see Figure 3-2):

```txt
R  
> hist(EuStockMarkets[, "SMI"], 30)  
> hist(diff(EuStockMarkets[, "SMI"], 30)) 
```

![](images/825328356299f258a3b8d925ad5e83a47270a468f6c2e4a2aedc9e6d45c8d36c.jpg)

![](images/b508650f98b5f5eda845778eceb7b4b1308cd89a41f5f8f3abf0ea94c030aed0.jpg)  
Figure 3-2. The histogram of the untransformed data (top) is extremely wide and does not show a normal distribution. This is to be expected given that the underlying data has a trend. We difference the data to remove the trend, and this transforms the data to a more normally shaped distribution (bottom).

In a time series context, a hist() of the difference of the data is often more interest‐ ing than a hist() of the untransformed data. After all, in a time series context, often (and particularly in finance) what is most interesting is how a value changes from one measurement to the next rather than the value’s actual measurement. This is particu‐ larly true for plotting, because taking the histogram of data with a trend in it does not produce a very informative visualization.

Notice the new information we get from the histogram of the differenced series. While the original plots of the stocks in the previous section painted a very rosy eco‐ nomic picture with stocks inexorably rising, this histogram shows us the day-to-day experience of someone following the stocks. The histogram of the difference tells us that the value of the time series has gone both up (positive difference values) and down (negative difference values) about the same amount over time. The stock indexes did not go up and down exactly the same amount over time, because we know the stock indexes do have an increasing trend. However, we can see from this histogram that just a slight bias in favor of positive over negative differences is what accounts for that trend.

This is a good first example of why we need to pay attention to temporal scale when sampling, summarizing, and asking questions of data. Whether performance looks great (long-term plot) or just so-so (histogram of difference plots) will depend on what our temporal scale is: do we care about the day-to-day, or do we have a longerterm horizon? If we work in an organization that trades stock to make an annual profit, we need to think about the short-term experience we will have every time we report to our bosses a “diff” that leans toward negative values. However, if we are a large institutional investor—perhaps a university or a hospital—we may be able to afford to take the long view, which expects an upward climb. This latter scenario has its own challenges: how do we take long enough to maximize our profit but not so long that the party is over? These are the questions that keep the financial industry humming along with time series research and predictions.

# Scatter Plots

The traditional method of using scatter plots is just as useful for time series data as it is for other kinds of data. We can use scatter plots to determine both how two stocks are linked at a specific time and how their price shifts are related over time.

In this example, we plot both cases (see Figure 3-3):

• The values of two different stocks over time   
• The values of the daily changes in these two stocks over time (via differencing) with R’s diff() function

## R

> plot( EuStockMarkets[, "SMI"], EuStockMarkets[, "DAX"])   
> plot(diff(EuStockMarkets[, "SMI"]), diff(EuStockMarkets[, "DAX"]))

![](images/8d5f634644b680e94e713f7540b6a0c83e4fb3d004846028b38a46aa102e7a26.jpg)

![](images/77909c22c8938e8ef05322ccab1d9814575b940ec935283df28b67aeb00a8463.jpg)  
Figure 3-3. Simple scatter plots of two stock indexes suggest strong correlations. How‐ ever, there are reasons to view these plots with suspicion.

As we’ve already seen, the actual values are less informative than the differences between adjacent time points, so we have plotted the diffs in a second scatter plot. These look like very strong correlations, but the relationships are not as strong as they appear. (To delve deeper, skip ahead to “Spurious Correlations” on page 102.)

The apparent correlations in Figure 3-3 are interesting, but even if they are true cor‐ relations (and there is reason to doubt they are), these are not correlations we can monetize as stock traders. By the time we know whether a stock is going up or down, the stocks it is correlated with will have also gone up or down, since we are taking correlations of values at identical time points. What we need to do is find out whether the change in one stock earlier in time can predict the change in another stock later in time. To do this, we shift one of the differences of the stocks back by 1 before look‐ ing at the scatter plot. Read the following code carefully; notice that we are still differ‐ encing, but now we are also applying a lag to one of the differenced time series (see Figure 3-4):

```r
## R
> plot(lag(diff(EuStockMarkets[., "SMI"]), 1), diff(EuStockMarkets[., "DAX"]))) 
```

![](images/831af9d74f80c2df2ee5b850159004be570cd46ab08e8aea3e2e271a55058342.jpg)

The lines in these code examples are easy to read because each sec‐ tion is aligned. It can be tempting to write long lines of unreadable code, particularly in a functional programming language such as R or Python, but you should avoid this whenever possible!

![](images/3bde9d75cb467fe4e72746a1bd48b52c4f1d76453da09eacb86adbe81660d9d9.jpg)  
Figure 3-4. The correlation between the stocks disappears as soon as we put in a time lag, indicating that SMI does not appear to predict DAX.

This result tells us a number of important things:

• With time series data, while we may use the same exploratory techniques as with non-time-series data, mindless application won’t work. We need to think about how to exploit the same techniques but with reshaped data.   
• Oftentimes it’s the relationship between data at different points or the change over time that is most informative about how your data behaves.   
• The plot in Figure 3-4 shows why it can be difficult to be a stock trader. If you are a passive investor and wait it out, you might benefit from the long-term rising trend. However, if you are trying to make predictions about the future, you can see that it’s not easy!

![](images/edd252b67f30e8baf7510494beb278885ddac944b29c8d7bce36340336ef0e7b.jpg)

R’s lag() function may not move data in the temporal direction you expect. The lag() function is a forward time shift. This is important to remember, as you wouldn’t want to move data in the wrong temporal direction given a specific use case, and both for‐ ward and backward shifts in time are reasonable for different use cases.

# Time Series–Specific Exploratory Methods

Several methods of analyzing time series data focus on relations of values at different times in the same series, and you likely won’t have seen these before if you haven’t worked with time series data. In the rest of this chapter, we walk through a few con‐ cepts and related techniques used to classify time series.

The concepts we will explore are:

# Stationarity

What it means for a time series to be stationary and a statistical test for stationarity

# Self-correlation

What it means to say that a time series correlates with itself and what such a cor‐ relation indicates about the underlying dynamics of the time series

# Spurious correlations

What it means for a correlation to be spurious and when you should expect to run into spurious correlations

The methods we will learn to apply are:

• Rolling and expanding window functions   
• Self-correlation functions

— The autocorrelation function   
— The partial autocorrelation function

We will cover the concepts and their resulting methods in order from stationarity to self-correlations to spurious correlations. Before we dive into the specifics, let’s dis‐ cuss the logic behind this particular ordering.

The first question you will likely ask about a time series is whether it appears to reflect a system that is “stable” or one that is constantly changing. The level of stabil‐ ity, or stationarity, is important to assess because we need to know how much we should expect the system’s long-term past behavior to reflect its long-term future behavior. Once we have assessed the “stability” (this word is not used in a technical sense here) of a time series, we try to determine whether there are internal dynamics

in that series (seasonal changes, for example). That is, we are looking for selfcorrelations, answering the fundamental question of how tightly past data, distant or recent, predicts future data. Finally, when we think we have found certain behavioral dynamics within the system, we need to make sure we are not identifying relation‐ ships based on dynamics that do not in any way imply the causal relationships we wish to discover; hence, we must look for spurious correlations.

# Understanding Stationarity

Many traditional statistical time series models rely on a time series being stationary. Generally speaking, a stationary time series is one that has fairly stable statistical properties over time, particularly with respect to mean and variance. This seems rela‐ tively straightforward.

Nonetheless, stationarity can be a slippery concept, particularly when applied to real time series data. It is both too intuitive and too easy to fool yourself into relying on your natural intuition. We will walk through the concept both intuitively and with a somewhat formal definition before discussing a common test for stationarity and practical details for how to apply the concept.

# Intuition

A stationary time series is one in which a time series measurement reflects a system in a steady state. Sometimes it is difficult to assert what exactly this means, and it can be easier to rule things out as not being stationary rather than saying something is stationary. An easy example of data that is not stationary is the airline passengers data set we examined in Chapter 2, which is plotted in Figure 3-5 (as a reminder, it is available in R as AirPassengers and also widely available for download on the internet).

There are several traits that show this process is not stationary. First, the mean value is increasing over time, rather than remaining steady. Second, the distance between peak and trough on a yearly basis is growing, so the variance of the process is increas‐ ing over time. Third, the process displays strong seasonal behavior, the antithesis of stationarity.

![](images/016950f795b3b2a10f06f914750533c5540a61ce0ddbde8aebe4eac22a3d22a8.jpg)  
Figure 3-5. The airline passengers data set offers a clear example of a time series that is not stationary. As time passes, both the mean and the variance of the data are chang‐ ing. We also see evidence of seasonality, which intrinsically reflects a nonstationary process.

# Stationary definition and the Augmented Dickey–Fuller test

A simple definition of a stationary process is the following: a process is stationary if for all possible lags, $k ,$ the distribution of $y _ { t } , y _ { t + I } , . . . , y _ { t + k } ,$ does not depend on t.

Statistical tests for stationarity often come down to the question of whether there is a unit root—that is, whether 1 is a solution of the process’s characteristic equation.1 A linear time series is nonstationary if there is a unit root, although lack of a unit root does not prove stationarity. Addressing stationarity as a general question remains tricky, and determining whether a process has a unit root remains a current area of research.

Nonetheless, a simple intuition for what a unit root is can be gleaned from the exam‐ ple of a random walk:

$$
y _ {t} = \phi \times y _ {t - 1} + e _ {t}
$$

In this process, the value of a time series at a given time is a function of its value at the immediately preceding time and some random error. If $\phi$ is equal to 1, the series has a unit root, will “run away,” and will not be stationary. Interesting to note is that the series not being stationary does not mean it has to have a trend. A random walk is a good example of a nonstationary time series that does not have an underlying trend.2

Tests for determining whether a process is stationary are called hypothesis tests. The Augmented Dickey–Fuller (ADF) test is the most commonly used metric to assess a time series for stationarity problems. This test posits a null hypothesis that a unit root is present in a time series. Depending on the results of the test, this null hypothesis can be rejected for a specified significance level, meaning the presence of a unit root test can be rejected at a given significance level.

Note that tests for stationarity focus on whether the mean of a series is changing. The variance is handled by transformations rather than formally tested. The test of whether a series is stationary is thus a test of whether a series is integrated. An inte‐ grated series of order $d$ is a series that must be differenced $d$ times to become stationary.

The framing of the Dickey–Fuller test is as follows:

$$
\Delta y _ {t} = y _ {t} - y _ {t - 1} = (\phi - 1) \times y _ {t - 1} + \epsilon_ {t}
$$

Then the test of whether $\phi = 1$ is a simple $t { \cdot }$ -test of whether the parameter on the lag‐ ged $\boldsymbol { y } _ { t - l }$ is equal to 0. The difference that the ADF test makes is to account for more lags, so that the underlying model takes higher-order dynamics into account, which can be written as a series of differenced lags:

$$
Y _ {t} - \phi_ {1} \times y _ {t - 1} - \phi_ {2} \times y _ {t - 2} \dots = \epsilon_ {t}
$$

This requires somewhat more algebra to write as a series of differenced lags, and the expected distribution against which to test the null hypothesis is somewhat different

compared to the original Dickey–Fuller test. The ADF test is the most widely presen‐ ted test for stationarity in the time series literature.

Unfortunately, these tests are far from a panacea to your stationarity problems for a number of reasons:

• These tests have low power in distinguishing near unit roots from unit roots.   
• With low sample size, false positives for unit roots are fairly common.   
• Most tests do not test for or against all kinds of problems that can lead to a non‐ stationary time series. For example, some times will look specifically to testing whether the mean or the variance (but not both) is stationary. Other tests will look more generally at the overall distribution. It is important to understand the limits of the test applied when using it and to ensure that the limits are consistent with your beliefs about your data.

![](images/b9459b785fcccaa0c12593b4857563260c54b95331790bdeabbc0a5a12e27ad0.jpg)

# Setting an Alternate Null Hypothesis: KPSS Test

While the ADF test posits a null hypothesis of a unit root, the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test posits a null hypothesis of a stationary process. Unlike the ADF, the KPSS is not available in base R, but is still widely implemented. There are some nuances as to what these tests are for and how to use them prop‐ erly; these are beyond the scope of this text but widely discussed online.

# In practice

Stationarity matters in practice for a number of reasons. First, a large number of models assume a stationary process, such as traditional models with known strengths and statistical models. We will cover these classes of models in Chapter 6.

A broader point is that a model of a time series that is not stationary will vary in its accuracy as the metrics of the time series vary. That is, if you are seeking a model to help you estimate the mean of a time series with a nonstationary mean and variance, the bias and error in your model will vary over time, at which point the value of your model becomes questionable.

It is often the case that a time series can be made stationary enough with a few simple transformations. A log transformation and a square root transformation are two pop‐ ular options, particularly in the case of changing variance over time. Likewise, remov‐ ing a trend is most commonly done by differencing. Sometimes a series must be differenced more than once. However, if you find yourself differencing too much (more than two or three times) it is unlikely that you can fix your stationarity prob‐ lem with differencing.

![](images/453d4ce80e8bb7ee1180e6cacb32e2e7cc3692cb110510f4a1470e34000be515.jpg)

# Log or sqrt?

While a square root tends to be less computationally complex than a logarithm, you should explore both options. Think about the range of your data and how you want to compress large values rather than prematurely optimizing (that is, unduly pessimizing) your code and analysis.

Stationarity is not the only assumption forecasting models make. Another common but distinct assumption is the normality of the distribution of the input variables or predicted variable. In such cases, other transformations may be necessary. A com‐ mon one is the Box Cox transformation, which is implemented in the R forecast package and in scipy.stats in Python. The transformation makes non-normally distributed data (skewed data) more normal. However, just because you can trans‐ form your data doesn’t mean you should. Think carefully about the meaning of the distances between data points in your original data set before transforming them, and make sure, regardless of your task, that transformations preserve the most important information.

# Transformations Have Assumptions

You might think you’re choosing only transformations that don’t make underlying assumptions (log and sqrt seem simple, right?), but think carefully about whether that’s true.

As noted, log and square root transformations are commonly used to reduce the var‐ iance of variance over time. These make a number of assumptions. One is that your data will always be positive. Alternately, if you choose to shift your data before taking the square root or log, you are again either adding a bias or assuming it doesn’t mat‐ ter. Finally, when you take the square root or log, you make larger values less differ‐ ent from one another, effectively compressing the space between larger values but not between smaller values, de-emphasizing the differences between outliers. This may or may not be appropriate.

# Applying Window Functions

Let’s review the most important time series exploratory graphs, which you are likely to use for most initial time series analysis.

# Rolling windows

A common function distinct to time series is a window function, which is any sort of function where you aggregate data either to compress it (as we saw in the case of downsampling in the previous chapter) or to smooth it (also discussed in Chapter 2).

In addition to the applications already discussed, smoothed data and windowaggregated data make for informative exploratory visualizations.

We can calculate a moving average and other calculations that involve some linear function of a series of points with base R’s filter() function, as follows:

```r
## R
>>> calculate a rolling average using the base R
>>> filter function
>>> x <- rnorm(n = 100, mean = 0, sd = 10) + 1:100
>>> mn <- function(n) rep(1/n, n)
>>> plot(x, type = 'l', lwd = 1)
>>> lines滤(3x, mn(5)), col = 2, lwd = 3, lty = 2)
>>> lines.filter(x, mn(50)), col = 3, lwd = 3, lty = 3) 
```

This code produces the graph in Figure 3-6.

![](images/5eec07b8e184b6ed70ee2d51350811a94fea24a25fc8a71dad7ff9661ffb2fe1.jpg)  
Figure 3-6. Two exploratory curves produced via a rolling mean smoothing. We might use these to look for a trend in particularly noisy data or to decide what sorts of devia‐ tions from linear behavior are interesting to investigate versus which are likely just noise.

If we are looking for functions that are not linear combinations of all points in the window, we can’t use filter() because it relies on a linear transformation of the data. However, we can use zoo. The rollapply() function from the zoo package is quite handy (see Figure 3-7):

```r
## R
> ## you can also do more 'custom' functionality
> require(zoo)
> f1 <- rollapply(zoo(x), 20, function(w) min(w), 
```

![](images/fc71f9c6f0284e21a6ab9e984adb79c6a6b970ac406e9654e2ef89a7bde3e816.jpg)  
Figure 3-7. A rolling window minimum aligned either to the left (long dashes) or to the right (short dashes). The left alignment sees events in the future, whereas the right sees only events in the past. This is important to note for avoiding lookahead. However, sometimes a left alignment can be useful to ask exploratory questions such as “if I knew this in advance, would it even be useful?” Sometimes even a lookahead is not informa‐ tive, which means that a particular variable is not informative. When knowing the future of a measure is not helpful, it’s likely not a useful measure at any time in a time series context.

![](images/c8d9f3d230fdd6d50b824562da89f18f45e8d391d2f22a6b6029c782f6d0c52f.jpg)

Use zoo objects in zoo functions. If you pass numeric vectors directly to rollapply(), the align argument will not take effect. You can confirm this by deleting the zoo() wrapper around $\times$ in the preceding code. You will see that the two curves are identical and, in fact, this is a silent failure, which is particularly dangerous in time series analysis because it can introduce an unintended lookahead. This is an example of why you need to sanity-check often, even with your exploratory data analysis. Unfortunately, silent failure is not unusual in many popular R packages and in other scripting languages, so stay alert!

It is also possible to “roll your own” for this kind of function, which can be a good idea to limit dependencies. In such cases, I’d strongly recommend starting with the source code from an existing widely used package, such as zoo. This is because, even for a univariate time series, there are a surprising number of corner cases to think about, such as how to treat NAs, and how to treat the beginning and ending of a series where you will have fewer points than the specified size of the window.

# A Number of R Options for Time Series

Earlier in this chapter, we considered the ts class, which is available in base R to han‐ dle time series data. In this section we use zoo objects. You should also be aware of xts objects. Here is a brief summary of how zoo and xts objects improve on ts objects:

• A ts object assumes a regularly spaced complete time series and for this reason does not store individual time indices but instead only the start, end, and fre‐ quency of the time series.   
• ts objects support a recurring cycle, such as years or months.   
• zoo objects store timestamps as an index attribute, so that they do not require regularly spaced, periodic time series.   
• zoo objects can be printed horizontally or vertically.   
• The data part of a zoo object can be a vector or a matrix.   
• xts objects are an extension of zoo objects with even more options.

# Expanding windows

Expanding windows are less commonly used in time series analysis than rolling win‐ dows because their proper application is more limited. Expanding windows make sense only in cases where you are estimating a summary statistic that you believe is a stable process rather than evolving over time or oscillating significantly. An expand‐ ing window starts with a given minimum size, but as you progress into the time ser‐ ies, it grows to include every point up to a given time rather than only a finite and constant size.

An expanding window offers greater certainty in your estimate of test statistics over time, allowing you to benefit from being “deeper” into a particular time series. How‐ ever, it works only if you hypothesize that your underlying system is stationary. It can help you maintain “online” summary statistics as you would have estimated them in real time as you gathered more information.

If you look into base R, you will realize that many functions already exist as imple‐ mentations of an expanding window, such as cummax and cummin. You can also easily repurpose cumsum to create a cumulative mean. In the following plot, we show both an expanding window max and an expanding window mean (see Figure 3-8):

```txt
## R
> # expanding windows
> plot(x, type = 'l', lwd = 1)
> lines(cummax(x), col = 2, lwd = 3, lty = 2) # max
> lines(cumsum(x)/1:length(x), col = 3, lwd = 3, lty = 3) # mean 
```

![](images/d7352d9b5b51f2286a4bd164d1fb9746fb6318beed121ef6ecdca5c63e557978.jpg)  
Figure 3-8. An expanding window “max” (long dashes) and an expanding window “mean” (short dashes). Use of an expanding window means that the max always reflects the global max up until that time, making it a monotonic function. Thanks to the long “memory” of the expanding window, the expanding window mean is lower than the rolling mean (Figure 3-7) because the underlying trend is less prominent in the expanding mean. Whether this is good, bad, or neutral depends on our assumptions and knowledge of the underlying system.

If you need a custom function with a rolling window, you can use rollapply() as we did for a rolling window. In this case, you need to specify a sequence of window sizes rather than a single scalar. Running the following code will produce an identical plot to the one in Figure 3-8, but this time it was created with rollapply() rather than built-in R functions:

```r
## R
> plot(x, type = 'l', lwd = 1)
> lines(rollapply(zoo(x), seq_along(x), function(w) max(w),
> partial = TRUE, align = "right"),
> col = 2, lwd = 3, lty = 2)
> lines(rollapply(zoo(x), seq_along(x), function(w) mean(w),
> partial = TRUE, align = "right"),
> col = 3, lwd = 3, lty = 3) 
```

# Custom rolling functions

We only care about the option to apply a custom rolling function if we can actually imagine doing so. In practice, this is something you are likely to see when analyzing time series domains that have known underlying fundamental laws of behavior or useful heuristics that are necessary for proper analysis.

For example, we may be looking at windows that include a particular feature that is informative given domain knowledge. We might want to know when we are in a monotonic regime (say, blood sugar is increasing) versus an up-and-down regime that suggests instrumental noise rather than a trend. We could write a custom func‐ tion for this scenario and apply it with either a moving or expanding window.

# Understanding and Identifying Self-Correlation

At its most fundamental, self-correlation of a time series is the idea that a value in a time series at one given point in time may have a correlation to the value at another point in time. Note that “self-correlation” is being used here informally to describe a general idea rather than a technical one.

As an example of self correlation, if you take a yearly time series of daily temperature data, you may find that comparing May 15th of every year to August 15th of every year will give you some correlation, such that hotter May 15ths tend to correlate with hotter August 15ths (or tend to correlate with cooler August 15ths). You may feel you have learned a potentially interesting fact about the temperature system, indicating that there is a certain amount of long-term predictability. On the other hand, you may find the correlation closer to zero, in which case you will also have found some‐ thing interesting, namely that knowing the temperature on May 15th does not alone give you any information about the likely range of temperatures on August 15th. That is self-correlation in an anecdotal nutshell.

From this simple example, we are going to expand into autocorrelation, which gener‐ alizes self-correlation by not anchoring to a specific point in time. In particular, auto‐ correlation asks the more general question of whether there is a correlation between any two points in a specific time series with a specific fixed distance between them. We’ll look at this in more detail next, as well as final elaboration of partial autocorrelation.

# The autocorrelation function

We begin with Wikipedia’s excellent definition of autocorrelation:

Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of the delay. Informally, it is the similarity between observations as a function of the time lag between them.

Let’s translate that into plainer English. Autocorrelation gives you an idea of how data points at different points in time are linearly related to one another as a function of their time difference.

The autocorrelation function (ACF) can be intuitively understood with plotting. We can plot it easily in R (see Figure 3-9):

## R   
> x <- 1:100   
> y <- sin(x × pi /3)   
$>$ plot(y, type $=$ "b")  
> acf(y)

![](images/718202a78dea556454ff070e880e3f4fdca1b3d1092c9c2fdd679fb986f68f64.jpg)

![](images/008b0c2f2c23398f651c29242230659c5520b10f94dfbbd73d86bdd94641cc58.jpg)  
Seriesy   
Figure 3-9. Plot of a sine function and its ACF.

# Correlation in a Deterministic System

This sine series is a simple function and a fully determined system given a known input sequence. Nonetheless, we do not have a correlation of 1. Why is that? Inspect the series and think about what the ACF is calculating, and you will realize that for many values, the subsequent value can go either up or down depending on where you are in the cycle. If you know a few points in a row, you know which direction the process is going, but if you don’t (as with the ACF, which is a 1:1 correlation measure), you will have a correlation of less than 1 because most values do not have a unique subsequent value but rather more than one. So, remember that a nonunitary correlation does not mean you necessarily have a probabilistic or noisy time series.

From the ACF, we see that points that have a lag between them of 0 have a correla‐ tion of 1 (this is true for every time series), whereas points separated by 1 lag have a correlation of 0.5. Points separated by 2 lags have a correlation of –0.5, and so on.

Calculating the ACF is straightforward. We can do it ourselves using data.table’s shift() function:

```julia
## R
> cor(y, shift(y, 1), use = "pairwisecomplete Obs")
[1] 0.5000015
> cor(y, shift(y, 2), use = "pairwisecomplete Obs")
[1] -0.5003747 
```

Our calculations approximately match the graphical results in Figure 3-9.3 While it is straightforward to calculate the ACF with custom code, it’s usually better to use a prerolled version, such as R’s acf function. There are several advantages to doing so:

• Automatic plotting with helpful labels   
• A sensible (usually but not always) max number of lags for which to calculate ACF, as well as the option to override this max   
• A graceful way to handle multivariate time series

There are a few important facts about the ACF, mathematically speaking:

• The ACF of a periodic function has the same periodicity as the original process. You can see this in the preceding sine example plots.   
• The autocorrelation of the sum of periodic functions is the sum of the autocorre‐ lations of each function separately. You can easily formulate an example of this with some simple code.   
• All time series have an autocorrelation of 1 at lag 0.   
• The autocorrelation of a sample of white noise will have a value of approximately 0 at all lags other than 0.   
• The ACF is symmetric with respect to negative and positive lags, so only positive lags need to be considered explicitly. You can try plotting a manually calculated ACF to prove this.   
• A statistical rule for determining a significant nonzero ACF estimate is given by a “critical region” with bounds at $+ / { - 1 . 9 6 } \times \mathsf { s q r t } ( n )$ . This rule relies on a suffi‐ ciently large sample size and a finite variance for the process.

# The partial autocorrelation function

The partial autocorrelation function (PACF) can be trickier to understand than the ACF. The partial autocorrelation of a time series for a given lag is the partial correla‐ tion of the time series with itself at that lag given all the information between the two points in time.

It’s easy to nod your head here and think that sounds reasonable. But what exactly does it mean to account for the information between two points in time? It means you need to compute a number of conditional correlations and subtract these out of the total correlation. Computing the PACF is not straightforward, and there are a variety of methods for estimating it. We will not discuss these here, but you can find discussions in the related R and Python documentation.

The PACF is easier to understand graphically than conceptually. Its utility is also much more obvious in a graph than in a discussion (see Figure 3-10):

```txt
## R
> y <- sin(x × pi / 3)
> plot(y[1:30], type = "b")
> pacf(y) 
```

![](images/01ae48edbf78bda3881e7c18469616e95c006e6b05f2c4f56b7a8d58aba910a7.jpg)

![](images/75ec83c0dccfa1b949b9dff73ecc9f516f7d51a446a824c3c9a70309e243d4f9.jpg)  
Figure 3-10. Plot and PACF of a seasonal, noiseless process.

For the case of a sine series, the PACF provides a striking contrast to the ACF. The PACF shows which data points are informative and which are harmonics of shorter time periods.

For a seasonal and noiseless process, such as the sine function, with period T, the same ACF value will be seen at T, 2T, 3T, and so on up to infinity. An ACF fails to weed out these redundant correlations. The PACF, on the other hand, reveals which correlations are “true” informative correlations for specific lags rather than redun‐ dancies. This is invaluable for knowing when we have collected enough information to get a sufficiently long window at a proper temporal scale for our data.

The critical region for the PACF is the same as for the ACF. The critical region has bounds at $\pm 1 . 9 6 { \sqrt { n } }$ . Any lags with calculated PACF values falling inside the critical region are effectively zero.

We have so far looked only at examples of perfectly noiseless single-frequency pro‐ cesses. Now we look at a slightly more complicated example. We’ll consider the sum of two sine curves under no noise, low noise, and high noise conditions.

First, let’s look at the plots with no noise, each individually (see Figure 3-11):

```r
## R
> y1 <- sin(x * pi / 3)
> plot(y1, type = "b")
> acf(y1)
> pacf(y1)
> y2 <- sin(x * pi / 10)
> plot(y2, type = "b")
> acf(y2)
> pacf(y2) 
```

![](images/57847d5e00b7bf428c9a41a4b67ad72028a952db9d21a3da20cc83b4a00f9a99.jpg)

The ACF of stationary data should drop to zero quickly. For non‐ stationary data the value at lag 1 is positive and large.

![](images/f4b5cb1773c0407b82465b621af0bffa819f6a975f5356fd12167a88a7710f46.jpg)

![](images/66ccc02936608b3f27c9ef6d2a595d476313e52a27014094d4e6a0f98630561f.jpg)

![](images/2ba5491fa4cbab3e395bee0f13a026080cd7dff5be5fd3646d9b4823f890c6cc.jpg)

![](images/108b8161d95c33cc7f86e9feb56bc43f4dcdee8c8e14adc56995be9c579edb21.jpg)

![](images/efc1a415d872f47099b91fefeb9013a30c23767b4f3c70a51f55aa5d9b307a42.jpg)

![](images/528d57d0a4524283dce353b1b8b43e0c1d7aca6173dd140ee8858fec65ac8bcd.jpg)  
Figure 3-11. Plots of two sine functions, their ACF, and their PACF.

We combine the two series by summing and create the same plots for the summed series (see Figure 3-12):

R $y<-y1+y2$ plot(y, type = "b")  
acf(y)  
pacf(y)

As we can see, our ACF plot is consistent with the aforementioned property; the ACF of the sum of two periodic series is the sum of the individual ACFs. You can see this most clearly by noticing the positive negative positive negative sections of the ACF correlating to the more slowly oscillating ACF. Within these waves, you can see the faster fluctuation of the higher-frequency ACF.

![](images/e22b820816a3765f5e80302b7f98f3c8ae8d084984025f29d5b6e9a2fd1c4bb6.jpg)

![](images/d4d934734d74a326166a99e85669169897dfd0bd64813afa8e8b92dc6519417f.jpg)

![](images/5cab3b7f7bbe02145cb4abc1b093a5d2a6437f383fdafdc769f569ee923f6cbf.jpg)  
Figure 3-12. Plot, ACF, and PACF for the sum of the two sine series.

The PACF is not a straightforward sum of the PACF functions of the individual com‐ ponents. A PACF is simple enough to understand once it is calculated, but it’s not so easy to generate or predict. This PACF indicates that partial autocorrelation is more substantial in the summed series than in either of the original series. That is, the cor‐ relation between points separated by a certain lag, when accounting for the values of the points between them, is more informative in the summed series than in the origi‐ nal series. This is related to the two different periods of the series, which result in any given point being less determined by the values of neighboring points since the loca‐ tion within the cycle of the two periods is less fixed now as the oscillations continue at different frequencies.

Let’s look at the same situation, but with more noise (see Figure 3-13):

```r
## R
> noise1 <- rnorm(100, sd = 0.05)
> noise2 <- rnorm(100, sd = 0.05) 
```

```txt
> y1 <- y1 + noise1  
> y2 <- y2 + noise2  
> y <- y1 + y2 
```

```txt
> plot(y1, type = 'b')  
> acf(y1)  
> pacf(y1) 
```

```txt
> plot(y2, type = 'b')  
> acf(y2)  
> pacf(y2) 
```

```txt
> plot(y, type = 'b')  
> acf(y)  
> pacf(y) 
```

![](images/7f3cf71245175cb9437e4944a6091a25d207d74b8d28136e92ec0407ca636dbc.jpg)

![](images/3884778a7b3a85063be80a45617399fd54a0f3836e29e6f99660e844bc97aebc.jpg)

![](images/a49e436df7317c57750405637f4f4557617de76815504366bf4b5fd575b72a1f.jpg)

![](images/cb6e94737de3cd993b10885f141ba37826452585d7ea09bf9c99d23d54bd9af1.jpg)

![](images/7ff236a563f7b7815786dedbc679843844f2d0261cc49b10bf925c2adfef5672.jpg)

![](images/7d95bdf5d41884ea6fefb4ce520803cf3c2cc26819d4e6bc3e5c37bf3b81199b.jpg)

![](images/198d2fc21e46cc4294bd4f3278341657b21f24438258ea49147d77f72478177a.jpg)

![](images/b1c782211f4a6550d8d0473659560034e17485da5875e5a92d8603515f9f44a8.jpg)

![](images/3615862241eed123969751d7fff9cd30cb78192e0aaeff434b792e4712f8917c.jpg)  
Figure 3-13. Plot, ACF, and PACF of two noisy sine processes and their sum.

Finally we add more noise to the time series so that the initial data itself does not even look particularly sine-like. (We omit the code example because it is the same as the previous example, simply with a larger sd parameter for rnorm.) We can see that this adds further interpretation difficulties, particularly to the PACF. The only differ‐ ence between the plots in Figure 3-14 and the previous ones is a larger sd value for the noise variables.

![](images/96ce038f7d086b3af308d3a93d3473502a147f746914239758e16e443e81fb65.jpg)  
Figure 3-14. Plot, ACF, and PACF of very noisy sum of two sine processes.

# Nonstationary Data

Let’s consider how the ACF and PACF would look in the event of a series with a trend but no cycle (see Figure 3-15):

```txt
## R
> x <- 1:100
> plot(x)
> acf(x)
> pacf(x) 
```

![](images/c61b4cad1b952df52b8dd5c9ab252c8be501cb673f91803c2b71a86a997dcb3d.jpg)

![](images/a223f6125bf568f09a7ad811ba65ba774f498369b487c7017fd23302548114fd.jpg)

![](images/82517053193fe64ba4925724fdf54b310df02891ef23e71fc2ba2767ba8687aa.jpg)  
Figure 3-15. Plot, ACF, and PACF of a linearly trending process.

The ACF is not informative. It has a similar value for every lag, seemingly implying that all lags are equally correlated to the data. It’s not clear what this means, and it’s more a case of being able to compute something, not being able to make that compu‐ ted quantity meaningful or sensible.

Luckily, the PACF is not as difficult to contemplate and gives us the information we need, which is that the only significant PACF correlation is at lag 1. Why is that? For a given point in time, once you know the point just before it, you know all the infor‐ mation the series can possibly give you about your point in time. This is due to the next point in time being 1 plus the old point.

Let’s conclude with a look at the ACF and PACF of a real-world data set. Below, we examine the AirPassengers data. Given what we have seen so far, think about why the ACF has so many “critical” values (answer: it has a trend) and why the PACF has a critical value for a large lag (answer: the yearly seasonal cycle, which is identifiable even with a trend in the data).

![](images/021b3fff7b07bcd04bf60900e50e9bb78eb2c6c78ab1a83cccb338f3565ad569.jpg)

![](images/b45fbe382cb1684e3e24084d21a4287167589568d80b6959e55e6c08da1d1add.jpg)  
Figure 3-16. Plot of the ACF and PACF of the AirPassengers data. The lags here are not unit numbers because the lags are expressed as fractions of a year. This is because the AirPassengers data set comes in the form of a ts object, which has a built-in frequency that can be used for plotting (and other purposes).

# Spurious Correlations

Those new to time series analysis will often begin with standard exploratory data practices, such as plotting two variables against each other and calculating their cor‐ relation. The new analyst will be very excited early on in the data exploration process when they notice what appears to be a very strong correlation and helpful relation‐ ship. They will continue looking and find other surprisingly high correlations; what an amazing system! They will wish they had started working with time series data sooner in their career. They will think to themselves, “All I do is win.”

Then the analyst will do a little reading on time series analysis (best-case scenario), or present their findings to someone else and realize that it doesn’t all make sense (not the best-case scenario). A skeptic will point out to the analyst that the correlations are too high. It can seem like any two values are related. More problems will emerge as the analyst reruns their analysis with other sets of variables and finds that they too have surprisingly high correlations. At some point it will become clear that there can’t possibly be so many truly high correlations.

This trajectory is very much like the early history of econometrics. In the 19th cen‐ tury, when economists first began thinking of the idea of a business cycle, some of them went looking for external drivers of the cycle, such as sun spots (an 11-year cycle) or various meteorological cycles (such as a posited 4-year precipitation cycle). They invariably got very positive and strongly correlated results, even when they had no causal hypothesis to explain these results.

Many economists and statisticians remained skeptical, and rightly so. Udny Yule investigated the problem formally with a paper titled “Why Do We Sometimes Get Nonsense Correlations?”, and an area of research was born and continues to give trouble and pleasure to academics. Spurious correlations remain an important prob‐ lem to guard against, and they are hotly debated in litigation situations where one side asserts a relationship and the other side tries to discredit it. Similarly, one attempt to discredit climate change data relies on an argument that the correlation between increasing carbon emissions and warming global temperatures is a spurious correlation due to the trends in the two data sets (I do not find this argument con‐ vincing).

Economists have learned over time that data with an underlying trend is likely to produce spurious correlations. Here’s a simple way to think about this: there is more information in a trending time series than in a stationary time series, so there are more opportunities for data points to move together.

In addition to trends, some other common features of time series can introduce spu‐ rious correlations:

• Seasonality—for example, think of a spurious correlation between hot dog con‐ sumption and death by drowning (summer).   
• Level or slope shifts in data from regime changes over time (producing a dumbbell-like distribution with meaningless high correlation).   
• Cumulatively summed quantities (this is a trick used in certain industries to make models or correlations look better than they are).

# Cointegration

Cointegration refers to a real relationship between two time series. A commonly used example is a drunk pedestrian and their dog. Their individually measured walks might appear random taken alone, but they never stray too far from each other.

In the case of cointegration you will see high correlations. The difficulty will be in assessing whether two processes are cointegrated or whether you are seeing a spuri‐ ous correlation because in both cases you will see surprisingly high correlations. The important difference is that there need not be any relationship in the case of a spuri‐ ous correlation, whereas cointegrated time series are strongly related to one another.

There is a well-known blog (and now book) full of wonderful examples of spurious correlations, and I share one in Figure 3-17. Whenever you are tempted to think you have found a special and very strong relationship, make sure to check your data for obvious causes of trouble such as trends.

![](images/2be0ec7704bde774a2bdc7528b0026f4841ff92cbad7bedfcf0adefd42d8a83d.jpg)  
Figure 3-17. Some spurious correlations can look surprisingly convincing. This plot was taken from Tyler Vigen’s website of spurious correlations.

# Some Useful Visualizations

Graphs are fundamental to a thorough exploratory analysis of time series. You will certainly want to visualize data with respect to the temporal axis—preferably in a way that answers the general questions you have about the data set, such as the behavior of a particular variable or the overall temporal distribution of the data points.

Earlier in this chapter, we looked at some plotting techniques familiar to any data analyst, such as a plot of values against time or a scatter plot of different columns’ values over time. In this final section on exploratory data analysis, we discuss several visualizations that are particularly helpful for offering novel insights about time series behavior.

We will look at visualizations of varying degrees of complexity:

• A one-dimensional visualization to understand the overall temporal distribution of individuals with a found time series we assembled in Chapter 2   
• A two-dimensional histogram to understand the typical trajectory of a value over time in the case of many parallel measurements (say many years measured or many time series of the same phenomenon measured)   
• A three-dimensional visualization where time can take up as many as two of the dimensions or as few as none of the dimensions, but still be implicitly present

# 1D Visualizations

In the cases of many units of measurement (many users, members, etc.) we consider multiple time series in parallel. It can be interesting to stack these visually, emphasiz‐ ing individual units of analysis and their respective time frames. We ignore the values measured and rather take the existence of data over a given range as the information of interest. The time span itself becomes the unit of analysis. Here we use R’s timevis package, but there are many other options available. We look at a small subset of the donations data we prepared in Chapter 2 (see Figure 3-18):

```txt
## R
> require(timevis)
> donations <- fread("donations.csv")
> d <- donations[, .min(timestamp), max(timestamp)), user]
> names(d) <- c("content", "start", "end")
> d <- d[ start != end ]
> timevis(d[sample(1:nrow(d), 20)]) 
```

![](images/328a4c0f96b5dbe363756cebb9efd1b7c6ff2cdbacb7da2e2ea6f9ee90fb7109.jpg)  
Figure 3-18. A Gantt chart of a random sample of data can offer some idea of the dis‐ tribution of the range of “active” time periods for the users/donors.

The chart in Figure 3-18 helps us see that we probably have “busy” periods globally across the member population. We also glean some sense of the distribution of active donation spans in a member’s “lifetime” in our organization.

Gantt charts have been used for over a century, most often for project management tasks. They came about independently in many different industries, and the idea is intuitive as soon as you see one. Despite the project management origins, Gantt charts can be useful in time series analysis where there are many independent actors, rather than a single process being measured. The plot in Figure 3-18 quickly answered questions I had about the relative overlap among the entire user base with respect to their donation history, a distribution I found difficult to understand when merely reading through the tabular data.

# 2D Visualizations

Now we’ll use the AirPassengers data to see the seasonality and the trend, but we shouldn’t think of time as linear. In particular, time happens on more than one axis. There is, of course, the axis of time going forward from day to day and year to year, but we can also consider laying time out along the axis of hour of the day or day of the week, and so on. In this way, we can more easily think about seasonality, such as certain behaviors happening at a certain time of the day or month of the year. We

think in particular about how to understand our data in a seasonal fashion rather than just according to linear, chronological time visualizations.

We extract the data from the AirPassengers ts object and put it into appropriate matrix form:

```csv
## R
> t matrix(AirPassengers, nrow = 12, ncol = 12)
[.,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,] 112 118 132 129 121 135 148 148 136 119 104 118
[2,] 115 126 141 135 125 149 170 170 158 133 114 140
[3,] 145 150 178 163 172 178 199 199 184 162 146 166
[4,] 171 180 193 181 183 218 230 242 209 191 172 194
[5,] 196 196 236 235 229 243 264 272 237 211 180 201
[6,] 204 188 235 227 234 264 302 293 259 229 203 229
[7,] 242 233 267 269 270 315 364 347 312 274 237 278
[8,] 284 277 317 313 318 374 413 405 355 306 271 306
[9,] 315 301 356 348 355 422 465 467 404 347 305 336
[10,] 340 318 362 348 363 435 491 505 404 359 310 337
[11,] 360 342 406 396 420 472 548 559 463 407 362 405
[12,] 417 391 419 461 472 535 622 606 508 461 390 432 
```

Notice we had to transpose the data so that it will line up as presented by the ts object.

![](images/46b39be117f3a63150ec41aad8a6ef8b17d88cbf57f5069eb107595e4028efdd.jpg)

# Column Major Versus Row Major

R is column major by default, which is unusual and different from Python’s NumPy (row major) and also different from most SQL databases. It’s good to be aware of what behaviors are default and available in a given language, not just for display purposes but to think about how to manage and access memory effectively later on.

We plot each year on a set of axes that reflect the progression of the months across the year (see Figure 3-19):

```r
## R
> colors <- c("green", "red", "pink", "blue",
> "yellow", "lightsalmon", "black", "gray",
> "cyan", "lightblue", "maroon", "purple")
> matplotlib matrix(AirPassengers, nrow = 12, ncol = 12),
> type = 'l', col = colors, lty = 1, lwd = 2.5,
> xaxt = "n", ylab = "Passenger Count")
> legend("topleft", legend = 1949:1960, lty = 1, lwd = 2.5,
> col = colors)
> axis(1, at = 1:12, labels = c("Jan", "Feb", "Mar", "Apr",
> "May", "Jun", "Jul", "Aug",
> "Sep", "Oct", "Nov", "Dec")) 
```

![](images/015b82349876d459973cb309af3a0ea52b8e59cc983c0699d8cf8c666276551c.jpg)  
Figure 3-19. Per-year month-by-month counts.4

We can produce the same plot more easily with the forecast package (see Figure 3-20):

```txt
## R
> require_forecast)
> seasonplot(AirPassengers) 
```

The x-axis is month of the year for all years. Every year, the number of airline passen‐ gers peaks in July or August (months 7 and 8). We also see a local peak in March (month 3) most years. This graph can thus show us more details regarding the sea‐ sonality behavior.

![](images/30371be1c74e2d1cb2abfcef2a0e2659a140a69b00867d7cc7f723f1ff2755ea.jpg)  
Seasonal plot:AirPassengers   
Figure 3-20. We produce a similar seasonal plot more easily with the seasonplot() function.

The different years’ curves rarely cross. There was such robust growth that it was rarely the case that different years had the same number of passengers in the same month. There are a few exceptions, but not during the peak months. From these observations alone, we can already produce advice for an air travel company looking to make decisions about how to plan for growth.

An alternate plot of per-month curves against years is less standard but also helpful (see Figure 3-21):

```r
## R
> months <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun",
>     "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
> matplotlib(tmatrix(AirPassengers, nrow = 12, ncol = 12)),
>     type = 'l', col = colors, lty = 1, lwd = 2.5)
> legend("left", legend = months,
>     col = colors, lty = 1, lwd = 2.5) 
```

![](images/32a96775171deca28d5e0f27895fdcb6540faae7bb3a0b62e8e46f5921b1b42f.jpg)  
Figure 3-21. Per-month curves of year-to-year time series.5

Through the years, the growth trend is accelerating; that is, the rate of increase is itself increasing. Also, two months are growing faster than the others, namely July and August. We can get a similar visualization and similar insights with an easy visu‐ alization function supplied by the forecast package (see Figure 3-22):

## R

$>$ monthplot(AirPassengers)

There are two general observations we can make from these plots:

• Time series have more than one useful set of temporal axes against which to plot. We used both an axis of month of the year (January through December) and an axis of years of the data set (the first year up through the last/twelfth year).   
• We can glean a lot of useful information and predictive details from visualiza‐ tions that stack time series data rather than linearly plotting.

![](images/09a51940efed8ec92e093dea49b3261a5e1f0b42a2bf798d7901d7646a982d82.jpg)  
Figure 3-22. By using the monthplot() function, we can see how performance per month changes as the years pass.

We next consider a proper two-dimensional histogram. In a time series context, we can think of a two-dimensional histogram as having one axis for time (or a proxy for time) and another axis for a unit of interest. The “stacked” plots we just did are well on their way to being two-dimensional histograms, but some changes would be helpful:

• We need to bin data both on the time axis and on the number of passengers.   
• We need more data. A 2D histogram doesn’t make sense until the stacked curves run into one another; they can’t properly be seen alone. Otherwise, the 2D histo‐ gram fails to convey any additional information.

We generate the 2D histogram on this small data set for illustration and then move on to a more meaningful example. We build our own 2D histogram function from scratch, like so:

```r
## R
> hist2d <- function(data, nbins.y, xlabels) {
>     ## we make ybins evenly spaced to include
>     ## minimum and maximum points
>     ymin = min(data)
>     ymax = max(data) × 1.0001
>     ## the lazy way out to avoid worrying about inclusion/exclusion
>     ybins = seq(from = ymin, to = ymax, length.out = nbins.y + 1) 
```

```html
>>> make a zero matrix of the appropriate size
>>> hist.matrix = matrix(0, nrow = nbins.y, ncol = ncol(data))
>>> data comes in matrix form where each row
>>> ## represents one data point
>>> for (i in 1:nrow(data)) {
>>> ts = findInterval(data[i, ], ybins)
>>> for (j in 1:ncol(data)) {
>>> hist.matrix[ts[j], j] = hist.matrix[ts[j], j] + 1
>>> }
>>> hist.matrix 
```

We make a histogram with heat map coloring, as follows:

```r
## R
> h = hist2d(t matrix(AirPassengers, nrow = 12, ncol = 12)), 5, months)
> image(1:ncol(h), 1:nrow(h), t(h), col = heatcolors(5),
> axes = FALSE, xlab = "Time", ylab = "Passenger Count") 
```

However, the resulting image (Figure 3-23) is not very satisfying.

![](images/5cb5dd724ddfea1e60fbbd1c6dc3b35125166395a5411c609faf6064f16f9a7e.jpg)  
Figure 3-23. Heat map constructed from our homemade 2D histogram of the AirPas‐ sengers data.

This plot is not useful because there is not enough data. We only have 12 curves, and we’ve divided them across 5 buckets. However, an even more important issue is that we don’t have stationary data. The use of a histogram assumes a stationary data set. In this case there is a trend, so although we’d like to see seasonality, the trend will necessarily get in the way.

Now we look at a data set that features a larger number of samples and is not polluted by a trend. This is a subset of the FiftyWords data set taken from the UCR Time Ser‐ ies Classification Archive. This data set includes a representation of 50 different words as recorded by a univariate time series, each time series being of equal length.

The subset of the data used to plot Figure 3-24 is from the data set I used in a seg‐ ment on time series classification in a general tutorial on the subject. You can down‐ load that subset, although for the purposes of this exercise you can use either one:

```r
## R
> require(data.table)
> words <- fread(url.str)
> w1 <- words[V1 == 1]
> h = hist2d(w1, 25, 1:ncol(w1))
> colors <- graycolors(20, start = 1, end = .5)
> par(mfrow = c(1, 2))
> image(1:ncol(h), 1:nrow(h), t(h),
> col = colors, axes = FALSE, xlab = "Time", ylab = "Projection Value")
> image(1:ncol(h), 1:nrow(h), t(log(h)),
> col = colors, axes = FALSE, xlab = "Time", ylab = "Projection Value") 
```

![](images/a4e656fb9544d56042aa2ead7402711abba3be86707d63521a73d8283992add3.jpg)

![](images/34cf9095f3edf123e0a380fc11076ffb54edd4bccd6bcbfcda312d608a16b0c6.jpg)  
Figure 3-24. Two-dimensional histogram of an audio metric for a single word. The left plot has a linear count scale, while the right plot has a log count scale.

The plot on the right of Figure 3-24 looks better because the counts are colored according to a log transformation of the count rather than the count directly.

This is an application of the same idea as taking the log of a time series to decrease variance and to reduce the importance of, and distance between, outliers. Using a log transformation improves our visualization by not wasting a large portion of the range on the relatively sparse high-count values.

Our homemade option won’t be as visually appealing as many precanned options, so we should take a look at these to see what is different. To take advantage of the pre‐ canned solutions, we need to reshape our data because these options expect pairs of x-y values to be turned into a 2D histogram. Unlike our homegrown solution, the precanned options for 2D histograms are not designed specifically for time series data. They nonetheless offer excellent visualization solutions (see Figure 3-25):

![](images/9ad10dd479e8a35885b09b015750c4fe47bb43c83849391819e870b4b16a480c.jpg)  
Figure 3-25. An alternative 2D histogram visualization of the same data.

# 3D Visualizations

3D visualizations do not come with base R, but there are many packages available for them. Here I present some fast plots made with plotly, which I chose because it pro‐ duces plots that can be easily rotated in RStudio and exported to web interfaces. Also, downloading and installing plotly tends to be straightforward, which is not true of all visualization packages.

Let’s consider the AirPassengers data. We plot it in three dimensions, using two dimensions for time (month and year) and one dimension for data values:

```r
## R
> require(plotly)
> require(data.table)
> months = 1:12
> ap = data.tablematrix(AirPassengers, nrow = 12, ncol = 12)
> names(ap) = as.character(1949:1960)
> ap[, month := months]
> ap = melt(ap, id_vars = 'month')
> names(ap) = c("month", "year", "count")
> p <- plot_ly(ap, x = ~month, y = ~year, z = ~count,
> color = ~as.factor(month)) %>% 
> add/markers() %>% 
> layout(scene = list(xaxis = list(title = 'Month'),
> yaxis = list(title = 'Year'),
> zaxis = list(title = 'PassengerCount')) 
```

This 3D visualization helps us get a sense of the overall shape of the data. We have seen much of this before, but expanding to a three-dimensional scatter plot proves to be notably better than a two-dimensional histogram, perhaps because of the paucity of the data (see Figures 3-26 and 3-27).

![](images/06f2647495b39565dc7bbd59bf624e24fbd30b472dd94531c871201fe90747bb.jpg)  
Figure 3-26. A 3D scatterplot of the AirPassenger data. This perspective highlights the seasonality.6

![](images/19f790a87afa7bd130fbfecd49d0d14da69b5c4bef3e14dfe093a4f6c7b2a54f.jpg)  
Figure 3-27. Another perspective on the same data more clearly illustrates the increas‐ ing trend from one year to the next. I highly recommend running the code on your own computer, where you can rotate it yourself.7

We do not necessarily need to expend two axes on time. Instead, we might use two axes for location and one for time. We can visualize a two-dimensional random walk as follows, with a lightly modified example of some plotly demonstration code (see Figures 3-28 and 3-29):

```r
## R
> file.location <- 'https://raw.githubusercontent.com/plotly/datasets/master/\_3d-line-plot.csv'
> data <- read.csv(file.location)
> p <- plot_ly(data, x = ~x1, y = ~y1, z = ~z1,
> type = 'scatter3d', mode = 'lines',
> line = list(color = '#1f77b4', width = 1)) 
```

The interactive nature of the plot is key. Different perspectives can be misleading or illuminating in ways we can’t know until we are able to rotate the data.

![](images/c6ac5f5f39b94a739a76821d79d66a32b92117351f07e3a6010e0ac5fc77c9e6.jpg)  
Figure 3-28. One perspective on a two-dimensional random walk over time.

![](images/be703381b9bcfc0829b4bc1133a415e73bb9a96e92063036a1e1ec2f29fd9cbd.jpg)  
Figure 3-29. This perspective of the same random walk is much more revealing. Again, I encourage you to try this code out for yourself!

A good exercise would be to generate noisy seasonal motion data in two dimensions and visualize this in the same way we have visualized the random walk here. There should be a substantial difference in what you see in that plot compared to the ran‐ dom walk data. Packages like plotly can help you experiment quickly and with com‐ prehensive visual feedback.

# More Resources

• On spurious correlations:

Ai Deng, “A Primer on Spurious Statistical Significance in Time Series Regres‐ sions,” Economics Committee Newsletter 14, no. 1 (2015), https://perma.cc/9CQR-RWHC.

This industry write-up of what spurious correlations are and how they appear in data is useful for developing some practical insights into when and where to look for this problem in your own data sets. The material is written at a very approachable level.

Tyler Vigen, Spurious Correlations (New York: Hachette, 2015), https://perma.cc/ YY6R-SKWA.

This collection of ridiculous time series correlations is essential reading for any time series analyst or thinking person.

Antonio Noriega and Daniel Ventosa-Santaulària, “Spurious Regression Under Broken-Trend Stationarity,” Journal of Time Series Analysis 27, no. 5 (2006): 671– 84, https://perma.cc/V993-SF4F.

The authors develop both theory and simulation data to show that shifts in the level or trend of independently and randomly generated data sets influ‐ ence the presence of spurious correlations.

C.W.J. Granger and P. Newbold, “Spurious Regressions in Econometrics,” Journal of Econometrics 2, no. 2 (1974): 111–20, https://perma.cc/M8TE-AL6U.

This econometrics article led to a Nobel Prize for identifying the difficulties inherent in dealing with spurious correlations and arguing for a more robust approach to identifying related time series.

• On exploratory data analysis:

David R. Brillinger and Mark A. Finney, “An Exploratory Data Analysis of the Temperature Fluctuations in a Spreading Fire,” Environmetrics 25, no. 6 (2014): 443–53, https://perma.cc/QB3D-APKM.

This is a very thorough example of how some real-world laboratory data with a geotemporal grid was analyzed by academic and government researchers.

Robert H. Shumway and David S. Stoffer, “Time Series Regression and Exploratory Data Analysis,” in Time Series Analysis and Its Applications with R Examples (New York: Springer, 2011), https://perma.cc/UC5B-TPVS.

This is a chapter on exploratory data analysis from the authors’ canonical treatise on time series analysis for graduate students.

• More visualizations:

Christian Tominski and Wolfgang Aigner, “The TimeViz Browser,” https:// perma.cc/94ND-6ZA5.

This stunning catalog shows examples of, and source code for, many inter‐ esting time series visualizations from both academic research papers and industry use cases.

Oscar Perpiñán Lamigueiro, “GitHub Repository for Displaying Time Series, Spa‐ tial, and Space-time Data with R,” https://perma.cc/R69Y-5JPL.

This includes source code for a variety of R-based time series visualizations, including geospatial time series data.

Myles Harrison, “5 Ways to Do 2D Histograms in R,” R-bloggers, September 1, 2014, https://perma.cc/ZCX9-FQQY.

This is a practical guide to a variety of options offered by R packages to con‐ struct 2D histograms and give them meaningful coloring and binning. In addition to a basic overview, a related segment also provides a walkthrough of the tidyquant package for visualizing stock market data, an important source of time series.

• On assorted trends:

Halbert White and Clive W.J. Granger, “Considerations of Trends in Time Series,” Journal of Time Series Econometrics 3, no. 1 (2011), https://perma.cc/WF2H-TVTL.

This recent academic article points out that while trends are ubiquitous in data, even traditional statistics does not have a definitive set of definitions for describing different kinds of trends in data. In an approachable style, the authors offer statistical insights into different ways that nonstationary data may have an underlying trend and guidance on how to improve statistical methods for data.

# Simulating Time Series Data

Up to this point, we have discussed where to find time series data and how to process it. Now we will look at how to create times series data via simulation.

Our discussion proceeds in three parts. First, we compare simulations of time series data to other kinds of data simulations, noting what new areas of particular concern emerge when we have to account for time passing. Second, we look at a few codebased simulations. Third, we discuss some general trends in the simulation of time series.

The bulk of this chapter will focus on specific code examples for generating various kinds of time series data. We will run through the following examples:

• We simulate email opening and donation behavior of members of a nonprofit organization over the course of several years. This is related to the data we exam‐ ined in “Retrofitting a Time Series Data Collection from a Collection of Tables” on page 26.   
• We simulate events in a taxicab fleet of 1,000 vehicles with various shift start times and hour-of-the-day-dependent passenger pickup frequencies over the course of a single day.   
• We simulate step-by-step state evolution of a magnetic solid for a given tempera‐ ture and size using relevant laws of physics.

These three code examples correlate to three classes of time series simulations:

# Heuristic simulations

We decide how the world should work, ensure it makes sense, and code it up, one rule at a time.

# Discrete event simulations

We build individual actors with certain rules in our universe and then run those actors to see how the universe evolves over time.

# Physics-based simulations

We apply physical laws to see how a system evolves over time.

Simulating time series can be a valuable analytical exercise and one we will also demonstrate in later chapters as it relates to specific models.

# What’s Special About Simulating Time Series?

Simulating data is an area of data science that is rarely taught, but which is a particu‐ larly useful skill for time series data. This follows from one of the downsides of hav‐ ing temporal data: no two data points in the same time series are exactly comparable since they happen at different times. If we want to think about what could have hap‐ pened at a given time, we move into the world of simulation.

Simulations can be simple or complex. On the simpler side, you will encounter syn‐ thetic data in any statistics textbook on time series, such as in the form of a random walk. These are usually generated as cumulative sums of a random process (such as R’s rnorm) or by a periodic function (such as a sine curve). On the more complex side, many scientists and engineers make their careers out of simulating time series. Time series simulations remain an active area of research—and a computationally demanding one—in many fields, including:

• Meteorology   
• Finance   
• Epidemiology   
• Quantum chemistry   
• Plasma physics

In some of these cases, the fundamental rules of behavior are well understood, but it can still be difficult to account for everything that can happen due to the complexity of the equations (meteorology, quantum chemistry, plasma physics). In other cases, not all of the predictive variables can ever be known, and experts aren’t even sure that perfect predictions can be made due to the stochastic nonlinear nature of the systems studied (finance, epidemiology).

# Simulation Versus Forecasting

Simulation and forecasting are similar exercises. In both cases you must form hypotheses about underlying system dynamics and parameters, and then extrapolate from these hypotheses to generate data points.

Nonetheless, there are important differences to keep in mind when learning about and developing simulations rather than forecasts:

• It can be easier to integrate qualitative observations into a simulation than into a forecast.   
• Simulations are run at scale so that you can see many alternative scenarios (thou‐ sands or more), whereas forecasts should be more carefully produced.   
• Simulations have lower stakes than forecasts; there are no lives and no resources on the line, so you can be more creative and exploratory in your initial rounds of simulations. Of course, you eventually want to make sure you can justify how you build your simulations, just as you must justify your forecasts.

# Simulations in Code

Next we look at three examples of coding up simulations of time series. As you read these examples, consider what a wide array of data can be simulated to produce a “time series,” and how the temporal element can be very specific and human-driven, such as days of the week and times of day of donations, but can also be very nonspe‐ cific and essentially unlabeled, such as the "nth step” of a physics simulation.

The three examples of simulation we will discuss in this section are:

• Simulating a synthetic data set to test our hypotheses about how members of an organization may (or may not) have correlated behavior between receptiveness to organizational email and willingness to make donations. This is the most DIY example in that we hardcode relationships and generate tabular data with for loops and the like.   
• Simulating the synthetic data set to explore aggregate behavior in a fleet of taxis, complete with shift times and time-of-day-dependent frequency of passengers. In this data set, we make use of Python’s object-oriented attributes as well as gener‐ ators, which are quite helpful when we want to set a system going and see what it does.   
• Simulating the physical process of a magnetic material gradually orienting its individual magnetic elements, which begin in disarray but ultimately coalesce

into a well-ordered system. In this example, we see how physical laws can drive a time series simulation and insert natural temporal scaling into a process.

# Doing the Work Yourself

When you are programming simulations, you need to keep in mind the logical rules that apply to your system. Here we walk through an example where the programmer does most of the work of making sure the data makes sense (for example, by not specifying events that happen in an illogical order).

We start by defining the membership universe—that is, how many members we have and when each joined the organization. We also pair each member with a member status:

```python
>>> # membership status
>>> years = ['2014', '2015', '2016', '2017', '2018']
>>> memberStatus = ['bronze', 'silver', 'gold', 'inactive']
>>> memberYears = np.random.choice(years, 1000,
>>> p = [0.1, 0.1, 0.15, 0.30, 0.35])
>>> memberStats = np.random.choice(memberStatus, 1000,
>>> p = [0.5, 0.3, 0.1, 0.1])
>>> yearJoined = pd.DataFrame(['yearJoined': memberYears,
>>> 'memberStats': memberStats]) 
```

Notice that there are already many rules/assumptions built into the simulation just from these lines of code. We impose specific probabilities of the years the members joined. We also make the status of the member entirely independent on the year they joined. In the real world, it’s likely we can already do better than this because these two variables should have some connection, particularly if we want to incentivize people to remain members.

We make a table indicating when members opened emails each week. In this case, we define our organization’s behavior: we send three emails a week. We also define dif‐ ferent patterns of members behavior with respect to email:

• Never opening email   
• Constant level of engagement/email open rate   
• Increasing or decreasing level of engagement

We can imagine ways to make this more complex and nuanced depending on anec‐ dotal observations from veterans or novel hypotheses we have about unobservable processes affecting the data:

```python
>>> NUM_EMAILS_sent_WEEKLY = 3
>>> # we define several functions for different patterns
>>> def never Opens(period_rng):
>>> return []
>>> def constant_open_rate(period_rng):
>>> n, p = NUM_EMAILS_sent_WEEKLY, np.random.uniform(0, 1)
>>> num_opened = np.random.binomial(n, p, len(period_rng))
>>> return num_opened
>>> def increasing_open_rate(period_rng):
>>> return open_rate_with_factor_change(period_rng,
>>> np.random.uniform(1.01,
>>> 1.30))
>>> def decreasing_open_rate(period_rng):
>>> return open_rate_with_factor_change(period_rng,
>>> np.random.uniform(0.5,
>>> 0.99))
>>> def open_rate_with_factor_change(period_rng, fac):
if len(period_rng) < 1:
return []
times = np.random.randint(0, len(period_rng),
int(0.1 * len(period_rng)))
num_opened = np.zeros(len(period_rng))
for prd in range(0, len(period_rng), 2):
try:
n, p = NUM_EMAILS_sent_WEEKLY, np.random.uniform(0,
1)
num_opened[prd:(prd + 2)] = np.random.binomial(n, p,
2)
p = max(min(1, p * fac), 0)
except:
    num_opened[prd] = np.random.binomial(n, p, 1)
for t in range(len(times)):
    num_opened[times[t]] = 0
return num_opened 
```

We have defined functions to simulate four distinct kinds of behavior:

Members who never open the emails we send them (never_opens())

Members who open about the same number of emails each week (constant_open_rate())

Members who open a decreasing number of emails each week (decreasing_open_rate())

Members who open an increasing number of emails each week (increasing_open_rate())

We ensure that those who grow increasingly engaged or disengaged over time are simulated in the same way with the open_rate_with_factor_change() function via the functions increasing_open_rate() and decreasing_open_rate().

We also need to come up with a system to model donation behavior. We don’t want to be totally naive, or our simulation will not give us insights into what we should expect. That is, we want to build into the model our current hypotheses about mem‐ ber behavior and then test whether the simulations based on those hypotheses match what we see in our real data. Here, we make donation behavior loosely but not deter‐ ministically related to the number of emails a member has opened:

```python
>>> # # donation behavior
>>> def produce_donations(period_rng, member_behavior, num Emails,
>>>     use_id, member_join_year):
>>>     donation_amounts = np.array([0, 25, 50, 75, 100, 250, 500,
>>>     1000, 1500, 2000])
>>>     member_has = np.random.choice(donation_amounts)
>>>     email_fraction = num Emails /
>>>     (NUM_EMAILSSENT WEEKLY * len(period_rng))
>>>     member_gives = member_has * email_fraction
>>>     member_givesidx = np.where(member_gives
>>>     >= donation_amounts)[0][-1]
>>>     member_givesidx = max(min(member_givesidx,
>>>     len(donation_amounts) - 2),
>>>     1)
>>>     num(times_gave = np.random.POISSON(2) *
>>>     (2018 - member_join_year)
>>>     times = np.random.randint(θ, len(period_rng), num(times_gave)
>>>     dons = pd.DataFrame(['member': [], 
>>>     'amount': [], 
>>>     'timestamp': []
>>> for n in range(num-times_gave):
>>>     donation = donation_amounts[member_givesidx
>>>     + np.random.binomial(1, .3)]
>>>     ts = str(period_rng[times[n].start_time]
>>>     + randomWeeklyTime_delta())
>>>     dons = dons.append(pd.DataFrame(
>>>     {'member': [use_id],
>>>     'amount': [donation],
>>>     'timestamp': [ts]])
>>> if dons.shape[θ] > θ:
>>>     dons = dons[dons.amount != θ]
>>> # we don't report zero donation events as this would not
>>> # be recorded in a real world database 
```

```txt
>>> return dons 
```

There are a few steps we have taken here to make sure the code produces realistic behavior:

• We make the overall number of donations dependent on how long someone has been a member.   
• We generate a wealth status per member, building in a hypothesis about behavior that donation amount is related to a stable amount a person would have ear‐ marked for making donations.

Because our member behaviors are tied to a specific timestamp, we have to choose which weeks each member made donations and also when during that week they made the donation. We write a utility function to pick a random time during the week:

```python
def random.weekly_time deltas():
    days_of Week = [d for d in range(7)]
    hours_of_Day = [h for h in range(11, 23)]
    minute_of_Hour = [m for m in range(60)]
    second_of_Minute = [s for s in range(60)]
    return pd.Timedelta(str(np.random.choice(days_of Weekend))
                + "days") + 
                pd.Timedelta(str(np.random.choice(hours_of_day))
                + "hours") + 
                pd.Timedelta(str(np.random.choice(minute_of_hour))
                + "minutes") + 
                pd.Timedelta(str(np.random.choice(second_of_minute))
                + "seconds") 
```

You may have noticed that we only draw the hour of the timestamp from the range of 11 to 23 (hours_of_day $=$ [h for h in range(11, 23)]). We are postulating a uni‐ verse with people in a very limited range of time zones or even in just a single time zone, as we do not allow hours outside the range given. Here we are building in more of our underlying model as to how users behave.

We thus expect to see unified behavior from our users as though they are all in one or a few adjoining time zones, and we are further postulating that reasonable donation behavior is for people to donate from late morning to late evening, but not overnight and not first thing when they wake up.

Finally, we put all the components just developed together to simulate a certain num‐ ber of members and associated events in a way that ensures that events happen only once a member has joined and that a member’s email events have some relation (but not an unrealistically small relation) to their donation events:

```python
>>> behaviors = [never Opens,
>>>     constant_open_rate,
>>>     increasing_open_rate,
>>>     decreasing_open_rate]
>>> member_behaviors = np.random.choice(behaviors, 1000,
>>>     [0.2, 0.5, 0.1, 0.2])
>>>rng = pd-period_range('2015-02-14', '2018-06-01', freq = 'W')
>>> emails = pd.DataFrame({'member': [], [''],
>>>         'week': [], ['],
>>>         'emailsOpened': []
>>> donations = pd.DataFrame({'member': [], ['],
>>>             'amount': [], ['],
>>>                 'timestamp': []
>>> for idx in range(yearJoined.shape[0]):
>>>     # randomly generate the date when a member would have joined
>>>     join_date = pd.Timestamp(yearJoined.iloc[idx].yearJoined) +
>>>     pd.Timedelta(str(np.random.randint(0, 365)) +
>>>             'days')
>>>     join_date = min(join_date, pd.Timestamp('2018-06-01'))
>>>     # # member should not have action timestamps before joining
>>>     member_rng =rng[rng > join_date]
>>>    if len(member_rng) < 1:
>>>           continue
>>>    info = member_behaviors[idx](member_rng)
>>>    if len(info) == len(member_rng):
>>>        emails = emails.append(pdf.DataFrame(
>>>            ['member': [idx] * len(info),
>>>            'week': [str(r.start_time) for r in member_rng],
>>>            'emailsOpened': info))
>>>    donations = donations.append(
>>>    produce_donations(member_rng, member_behaviors[idx],
>>>                    sum(info), idx, join_date.year)) 
```

We then look at the temporal behavior of the donations to get a sense of how we might try this for further analysis or forecasting. We plot the total sum of donations we received for each month of the data set (see Figure 4-1):

```python
>>> df.set_index(pd.to datetime(df.timestamp), inplace = True)
>>> df.sort_index(inplace = True)
>>> df.groupby(pd.Grouper(freq='M')).amount.sum().plot() 
```

![](images/1e1c08039ace195111ab34e7c8c49cd08f2de92b1c981e8bef30eb6610543dcb.jpg)  
Figure 4-1. Total sum of donations received for each month of the data set.

It looks as though the number of donations and of emails opened rose over time from 2015 through 2018. This is not surprising, since the number of members also rose over time, as indicated in the cumulative sum of members and the year they joined. In fact, one built-in assumption of our model was that we got to keep a member indefinitely after they joined. We made no provision for termination other than allowing for members to open a decreasing number of emails. Even in that case, how‐ ever, we left open the possibility of continued donations. We see this assumption of indefinitely continuing membership (and correlated donation behavior) in Figure 4-1. We should probably go back and refine our code, as indefinite member‐ ship and donation is not a realistic scenario.

This is not a classic time series simulation, so it may feel quite a bit more like an exer‐ cise in generating tabular data. It absolutely is that as well, but we did have to be time series–aware:

• We had to make decisions about how many time series our users were in.   
• We had to make decisions about what kinds of trends we would model over time:

— In the case of email, we decided to have three trends: stable, increasing, and decreasing email open rates.

— In the case of donations, we made donations a stable behavioral pattern related to how many emails the member had ever opened in their lifetime. This included a lookahead, but since we were generating data, this was a way of deciding that a member’s overall affinity in the organization, which would lead to more emails opened, would also increase the frequency of donations.

• We had to be careful to make sure we did not have emails opened or donations made before the member joined the organization.   
• We had to make sure our data did not go into the future, to make it more realis‐ tic for consumers of the data. Note that for a simulation it is fine if our data goes into the future.

But it’s not perfect. The code presented here is ungainly, and it doesn’t produce a realistic universe. What’s more, since only the programmer checked the logic, they could have missed edge cases such that events take place in an illogical order. It would be good to establish external metrics and standards of validity before running the simulation as one protection against such errors.

We need software that enforces a logical and consistent universe. We will look at Python generators as a better option in the next section.

# Building a Simulation Universe That Runs Itself

Sometimes you have a specific system and you want to set up the rules for that system and see how it rolls along. Perhaps you want to envision what a universe of inde‐ pendent members accessing your application will use, or you want to attempt to vali‐ date an internal theory of decision making based on posited external behavior. In these cases, you are looking to see how individual agents contribute to your aggregate metrics over time. Python is an especially good fit for this job thanks to the availabil‐ ity of generators. When you start building software rather than staying purely in analysis, it makes sense to move to Python even if you are more comfortable in R.

Generators allow us to create a series of independent (or dependent!) actors and wind them up to watch what they do, without too much boilerplate code to keep track of everything.

In the next code example, we explore a taxicab simulation.1 We want to imagine how a fleet of taxis, scheduled to begin their shifts at different times, might behave in aggregate. To do so, we want to create many individual taxis, set them loose in a cyber city, and have them report their activities back.

Such a simulation could be exceptionally complicated. For demonstration purposes, we accept that we will build a simpler world than what we imagine to truly be the case (“All models are wrong…”). We start by trying to understand what a Python genera‐ tor is.

Let’s first consider a method I wrote to retrieve a taxi identification number:

```python
>>> import numpy as np
>>> def taxi_id_number(num_taxis):
>>> arr = np.arange(num_taxis)
>>> np.random.shuffle(arr)
>>> for i in range(num_taxis):
>>> yield arr[i] 
```

For those who are not familiar with generators, here is the preceding code in action:

```txt
>>> python
>>> ids = taxi_id_number(10)
>>> print(next(ids))
>>> print(next(ids))
>>> print(next(ids)) 
```

which might print out:

```txt
7 2 5 
```

This will iterate until it has emitted 10 numbers, at which point it will exit the for loop held within the generator and emit a StopIteration exception.

The taxi_id_number() produces single-use objects, all of which are independent of one another and keep their own state. This is a generator function. You can think of generators as tiny objects that maintain their own small bundle of state variables, which is useful when you want many objects parallel to one another, each one mind‐ ing its own variables.

In the case of this simple taxi simulation, we compartmentalize our taxis into differ‐ ent shifts, and we also use a generator to indicate shifts. We schedule more taxis in the middle of the day than in the evening or overnight shifts by setting different probabilities for starting a shift at a given time:

```python
def shift_info():
    start(times_and_freqs = [(0, 8), (8, 30), (16, 15)]
    indices = np.arange(len(start(times_and_freqs))
while True:
    idx = np.random.choice(indices, p = [0.25, 0.5, 0.25])
start = start(times_and_freqs[idx]
yield (start[0], start[0] + 7.5, start[1]) 
```

Pay attention to start_times_and_freqs. This is our first bit of code that will con‐ tribute to making this a time series simulation. We are indicating that different parts of the day have different likelihoods of having a taxi assigned to the shift. Addition‐ ally, different times of the day have a different mean number of trips.

Now we create a more complex generator that will use the preceding generators to establish individual taxi parameters as well as create individual taxi timelines:

```python
def taxi_process(taxi_id_generation, shift_info_generation):
    taxi_id = next(taxi_id_generation)
    shift_start, shift_end, shift_mean_trips = nextshift_info_generation)
    actual_trips = round(np.random.normal(loc = shift_mean_trips, scale = 2))
    average_trip_time = 6.5 / shift_mean_trips * 60
    # convert mean trip time to minutes
    between_events_time = 1.0 / (shift_mean_trips - 1) * 60
    # this is an efficient city where cabs are seldom unused
    time = shift_start
    yield TimePoint(taxi_id, 'start shift', time)
    deltaT = np.random.poisson BETWEEN_events_time) / 60
    time += deltaT
    for i in range(actual_trips):
        yield TimePoint(taxi_id, 'pick up ', time)
        deltaT = np.random.poisson(average_trip_time) / 60
        time += deltaT
        yield TimePoint(taxi_id, 'drop off ', time)
        deltaT = np.random.poisson(between_events_time) / 60
        time += deltaT
        deltaT = np.random.poisson(between_events_time) / 60
        time += deltaT
    yield TimePoint(taxi_id, 'end shift ', time) 
```

Here the taxi accesses generators to determine its ID number, shift start times, and mean number of trips for its start time. From there, it departs on its own individual journey as it runs through a certain number of trips on its own timeline and emits those to the client calling next() on this generator. In effect, this generator produces a time series of points for an individual taxi.

The taxi generator yields TimePoints, which are defined as follows:

```txt
## python
>>> from dataclasses import dataclass
>>> @dataclass
>>> class TimePoint:
>>> taxi_id: int
>>> name: str
>>> time: float
>>> def __lt__(self, other):
>>> return self.time < other.time 
```

We use the relatively new dataclass decorator to simplify the code (this requires Python 3.7). I recommend that all Python-using data scientists familiarize themselves with this new and data-friendly addition to Python.

![](images/bc6f504444c228fcbdf9b7ebdbfa6c84df422d76464e2b6da5792f50ffbab02c.jpg)

# Python’s Dunder Methods

Python’s dunder methods, whose names begin and end with two underscores, are a set of built-in methods for every class. Dunder methods are called automatically in the natural course using a given object. There are predefined implementations that can be overridden when you define them for your class yourself. There are many reasons you might want to do this, such as in the case of the preceding code, where we want TimePoints to be compared only based on their time and not based on their taxi_id or name attributes.

Dunder originated as an abbreviation of “double under.”

In addition to the automatically generated initializer for TimePoint, we need only two other dunder methods, __lt__ (to compare TimePoints) and __str__ (to print out TimePoints, not shown here). We need comparison because we will take all Time Points produced into a data structure that will keep them in order: a priority queue. A priority queue is an abstract data type into which objects can be inserted in any order but which will emit objects in a specified order based on their priority.

![](images/a3b6d26fd097d929b361a04af777021a71b77c91be96fc370bd656163700a763.jpg)

# Abstract Data Type

An abstract data type is a computational model defined by its behavior, which consists of an enumerated set of possible actions and input data and what the results of such actions should be for certain sets of data.

One commonly known abstract data type is a first-in-first-out (FIFO) data type. This requires that objects are emitted from the data structure in the same order in which they were fed into the data structure. How the programmer elects to accomplish this is a matter of implementation and not a definition.

We have a simulation class to run these taxi generators and keep them assembled. This is not merely a dataclass because it has quite a bit of functionality, even in the initializer, to arrange the inputs into a sensible array of information and processing. Note that the only public-facing functionality is the run() function:

```python
```
>>> import queue
>>> class Simulator:
>>> def __init__(self, num_taxis):
>>> self._time_points = queue.PriorityQueue()
>>> taxi_id_generation = taxi_id_number(num_taxis)
>>> shift_info_generation = shift_info()
>>> self._taxis = [taxi_process(taxi_id_generation, 
```

```python
>>> shift_info_generator) for
>>> i in range(num_taxis)]
>>> self._prepare_run()
>>> def _prepare_run(self):
>>> for t in self._taxis:
>>> while True:
>>> try:
>>> e = next(t)
>>> self._time_points.put(e)
>>> except:
>>> break
>>> def run(self):
>>> sim_time = 0
>>> while sim_time < 24:
>>> if self._time_points.empty():
>>> break
>>> p = self._time_points.get()
>>> sim_time = p.time()
>>> print(p) 
```

First, we create the number of taxi generators that we need to represent the right number of taxis. Then we run through each of these taxis while it still has TimePoints and push all these TimePoints into a priority queue. The priority of the object is determined for a custom class such as TimePoint by our implementation of a Time Point’s __lt__, where we compare start time. So, as the TimePoints are pushed into the priority queue, it will prepare them to be emitted in temporal order.

We run the simulation:

```markdown
## python
>>> sim = Simulator(1000)
>>> sim.run() 
```

Here’s what the output looks like (your output will be different, as we haven’t set a seed—and every time you run the code it will be different from the last iteration):

```txt
id: 0539 name: drop off time: 23:58  
id: 0318 name: pick up time: 23:58  
id: 0759 name: end shift time: 23:58  
id: 0977 name: pick up time: 23:58  
id: 0693 name: end shift time: 23:59  
id: 0085 name: end shift time: 23:59  
id: 0351 name: end shift time: 23:59  
id: 0036 name: end shift time: 23:59  
id: 0314 name: drop off time: 23:59 
```

![](images/272d07b2cb4d74a4900c07bab04f737cea6bc253a7ea5614e049fad16c08b008.jpg)

# Setting a Seed When Generating Random Numbers

When you write code that is generating random numbers, you might want to ensure that it’s reproducible (e.g., if you wanted to set up unit tests for code that is normally random or if you were trying to debug and wanted to narrow down sources of variation to make debugging easier). To ensure that random numbers come out in the same nonrandom order, you set a seed. This is a common operation, so there are guides on how to set a seed in any computer language.

We have rounded to the nearest minute for display simplicity, although we do have more fine-grained data available. What temporal resolution we use will depend on our purposes:

• If we want to make an educational display for people in our city of how the taxi fleet affects traffic, we might display hourly aggregates.   
• If we are a taxicab app and need to understand load on our server, we likely want to look at minute-by-minute data or even more highly resolved data to think about our infrastructure design and capacity.

We made the decision to report taxi TimePoints as they are “happening.” That is, we report the start of a taxi ride (“pick up”) without the time when the ride will end, even though we easily could have condensed this. This is one way of making the time series more realistic, in the sense that you likely would have recorded events in this way in a live stream.

Note that, as in the previous case, our time series simulation has not yet produced a time series. We have produced a log and can see our way through to making this a time series in a number of ways, however:

• Output to a CSV file or time series database as we run the simulation.   
• Run some kind of online model hooked up to our simulation to learn how to develop a real-time streaming data processing pipeline.   
• Save the output down to a file or database and then do more post-processing to package the data in a convenient (but possibly risky vis-à-vis lookahead) form, such as pairing together start and end times of a given ride to study how the length of a taxi ride behaves at different times of the day.

There are several advantages to simulating this data in addition to being able to test hypotheses about the dynamics of a taxi system. Here are a couple of situations where this synthetic time series data could be useful:.

• Testing the merits of various forecasting models relative to the known underly‐ ing dynamics of the simulation.   
• Building a pipeline for data you eventually expect to have based on your syn‐ thetic data while you await the real data.

You will be well served as a time series analyst by your ability to make use of genera‐ tors and object-oriented programming. This example offers just one example of how such knowledge can simplify your life and improve the quality of your code.

![](images/22a49f2a5c75e08ce7b9611cd6bb551ee6d2bc06d4b22f923435d0a90956a766.jpg)

# For Extensive Simulations, Consider Agent-Based Modeling

The solution we coded here was all right, but it was a fair amount of boilerplate to ensure that logical conditions would be respected. If a simulation of discrete events based on the actions of discrete actors would be a useful source of simulated time series data, you should consider a simulation-oriented module. The SimPy module is a helpful option, with an accessible API and quite a bit of flexibil‐ ity to do the sorts of simulation tasks we handled in this section.

# A Physics Simulation

In another kind of simulation scenario, you may be in full possession of the laws of physics that define a system. This doesn’t have to be physics per se, however; it can also apply to a number of other areas:

• Quantitative researchers in finance often hypothesize the “physical” rules of the market. So do economists, albeit at different timescales.   
• Psychologists posit the “psychophysical” rules of how humans make decisions. These can be used to generate “physical” rules about expected human responses to a variety of options over time.   
• Biologists research rules about how a system behaves over time in response to various stimuli.

One case of knowing some rules for a simple physical system is that of modeling a magnet. This is the case we are going to work on, via an oft-taught statistical mechan‐ ics model called the Ising model.2 We will look at a simplified version of how to sim‐ ulate its behavior over time. We will initialize a magnetic material so that its individual magnetic components are pointing in random directions. We will then

watch how this system evolves into order where all the magnetic components point in the same direction, under the action of known physical laws and a few lines of code.

Next we discuss how such a simulation is accomplished via a Markov Chain Monte Carlo (MCMC) method, discussing both how that method works in general and as applied to this specific system.

# Monte Carlo Simulations and Markov Chains

The idea behind a Monte Carlo simulation is to find clever ways to apply random numbers to situations that should in theory be solvable exactly, but in practice are much easier to solve probabilistically.

The Markov chain is a helpful addition to a general Monte Carlo simulation and is particularly applicable to time series simulation. A Monte Carlo simulation will help you figure out what a particular distribution or series of terms looks like, but not how those terms should evolve over time. This is where a Markov chain comes in. It calcu‐ lates a probability of transitioning between states, and when we factor that in, we take “steps” rather than simply calculating a global integral. Now we can have a time series simulation rather than merely the calculation of an integral.

In physics, an MCMC simulation can be used, for example, to understand how quan‐ tum transitions in individual molecules can affect aggregate ensemble measurements of that system over time. In this case, we need to apply a few specific rules:

1. In a Markov process, the probability of a transition to a state in the future depends only on the present state (not on past information).   
2. We will impose a physics-specific condition of requiring a Boltzmann distribu‐ tion for energy; that is, $T _ { i j } \left/ \right. T _ { j i } = e ^ { - b ( E _ { j } - E _ { i } ) }$ e -b(Ej -Ei ). For most of us, this is just an imple‐ mentation detail and not something nonphysicists need to worry about.

We implement an MCMC simulation as follows:

1. Select the starting state of each individual lattice site randomly.   
2. For each individual time step, choose an individual lattice site and flip its direction.   
3. Calculate the change in energy that would result from this flip given the physical laws you are working with. In this case this means:

• If the change in energy is negative, you are transitioning to a lower energy state, which will always be favored, so you keep the switch and move on to the next time step.

• If the change in energy is not negative, you accept it with the acceptance prob‐ ability of $e$ (-energy change). This is consistent with rule 2.

Continue steps 2 and 3 indefinitely until convergence to determine the most likely state for whatever aggregate measurement you are making.

Let’s take a look at the specific details of the Ising model. Imagine we have a twodimensional material composed of a grid of objects, each one having what boils down to a mini-magnet that can point up or down. We put those mini-magnets randomly in an up or down spin at time zero, and we then record the system as it evolves from a random state to an ordered state at low temperature.3

First we configure our system, as follows:

```perl
>>> python
>>> ## CONFIGURATION
>>> ## physical layout
>>> N = 5 # width of lattice
>>> M = 5 # height of lattice
>>> ## temperature settings
>>> temperature = 0.5
>>> BETA = 1 / temperature 
```

Then we have some utility methods, such as random initialization of our starting block:

```python
>>> def initRandState(N, M):  
>>> block = np.random.choice([-1, 1], size = (N, M))  
>>> return block 
```

We also calculate the energy for a given center state alignment relative to its neighbors:

```python
def costForCenterState(state, i, j, n, m):
    centerS = state[i, j]
    neighbors = [(i + 1) % n, j), ((i - 1) % n, j),
                (i, (j + 1) % m), (i, (j - 1) % m)]
>>> notice the % n because we impose periodic boundary cond
>>> # ignore this if it doesn't make sense - it's merely a
>>> # physical constraint on the system saying 2D system is like
>>> # the surface of a donut
>>> interactionE = [state[x, y] * centerS for (x, y) in neighbors]
>>> return np.sum(interactionE) 
```

And we want to determine the magnetization of the entire block for a given state:

```markdown
## python
>>> def magnetizationForState(state):
>>> return np.sum(state) 
```

Here’s where we introduce the MCMC steps discussed earlier:

```python
>>> def mcmcAdjust(state):
>>> n = state.shape[0]
>>> m = state.shape[1]
>>> x, y = np.random.randint(0, n), np.random.randint(0, m)
>>> centerS = state[x, y]
>>> cost = costForCenterState(state, x, y, n, m)
>>> if cost < 0:
>>> centerS *= -1
>>> elif np.random.choice() < np.exp(-cost * BETA):
>>> centerS *= -1
>>> state[x, y] = centerS
>>> return state 
```

Now to actually run a simulation, we need some recordkeeping as well as repeated calls to the MCMC adjustment:

```python
def runState(state, n_steps, snapsteps = None):
    if snapsteps is None:
        snapsteps = np.linspace(θ, n_steps, num = round(n_steps / (M * N * 100)), dtype = np.int32)
        saved_states = []
    sp = θ
    magnet歷史 = []
    for i in range(n_steps):
        state = mcmcAdjust(state)
        magnet歷史.append(magnetizationForState(state))
        if sp < len(snapsteps) and i == snapsteps[sp]:
            saved_states.append(np.copy(state))
        sp += 1
    return state, saved_states, magnet歷史 
```

And we run the simulation:

```erlang
>>> python
>>> RUN A SIMULATION
>>> init_state = initRandState(N, M)
>>> print(init_state)
>>> final_state = runState(np.copy(init_state), 1000) 
```

We can get some insights from this simulation by looking at the beginning and end‐ ing states (see Figure 4-2).

![](images/a4f39a5bc5ae386272f8266cbb47f945cc6a9b435d330f1044b05a180df6c02c.jpg)  
Figure 4-2. Initial state of a $5 \times 5$ simulated ferromagnetic material, initialized with each state randomly selected to be spin up or spin down with equal probability.

In Figure 4-2 we examine one randomly generated initial state. While you might expect to see the two states more mixed up, remember that probabilistically it’s not that likely to get a perfect checkerboard effect. Try generating the initial state many times, and you will see that the seemingly “random” or $^ { \infty } 5 0 / 5 0 ^ { \infty }$ checkerboard state is not at all likely. Notice, however, that we start with approximately half our sites in each state. Also realize that any patterns you find in the initial states is likely your brain following the very human tendency to see patterns even where there aren’t any.

We then pass the initial state into the runState() function, allow 1,000 time steps to pass, and then examine the outcome in Figure 4-3.

This is a snapshot of the state taken at step 1,000. There are at least two interesting observations at this point. First, the dominant state has reversed compared to step 1,000. Second, the dominant state is no more dominant numerically than was the other dominant state at step 1,000. This suggests that the temperature may continue to flip sites out of the dominant state even when it might otherwise be favored. To better understand these dynamics, we should consider plotting overall aggregate measurements, such as magnetization, or make movies where we can view our twodimensional data in a time series format.

![](images/a0d6ec8edf7d32955ca085c596fefab1db3c4b1522d5df43c0658c9793545aac.jpg)  
Figure 4-3. Final low temperature state in one run of our simulation, as seen at 1,000 time steps.

We do this with magnetization over time for many independent runs of the previous simulation, as pictured in Figure 4-4:

```python
```python
>>> we collect each time series as a separate element in results list
>>> results = []
>>> for i in range(100):
>>>     init_state = initRandState(N, M)
>>>     final_state, states, magnet_history = runState(init_state, 1000)
>>>     results.append(magnet_history)
>>> 
>>> # we plot each curve with some transparency so we can see
>>> # curves that overlap one another
>>> for mh in results:
>>>     plt.plot(mh, 'r', alpha=0.2) 
```

The magnetization curves are just one example of how we could picture the system evolving over time. We might also consider recording 2D time series, as the snapshot of the overall state at each point in time. Or there might be other interesting aggre‐ gate variables to measure at each step, such as a measure of layout entropy or a meas‐ ure of total energy. Quantities such as magnetization or entropy are related quantities, as they are a function of the geometric layout of the state at each lattice site, but each quantity is a slightly different measure.

![](images/c28074703589fc03e600d7654dbeed25f6cdf15cbb3eb51bf126253e83bce358.jpg)  
Figure 4-4. 100 independent simulations of potential ways the system could enter a magnetized state at a low temperature even when each original lattice site was initial‐ ized randomly.

We can use this data in similar ways to what we discussed with the taxicab data, even though the underlying system is quite different. For example, we could:

• Use the simulated data as the impetus to set up a pipeline.   
• Test machine learning methods on this synthetic data to see if they can be helpful on physical data before we go to the trouble of cleaning up real-world data for such modeling.   
• Watch the movie-like imagery of important metrics to develop better physical intuitions about the system.

# Final Notes on Simulations

We have looked at a number of very different examples of simulating measurements that describe behavior over time. We have looked at simulating data related to con‐ sumer behavior (NGO membership and donation), city infrastructure (taxicab pickup patterns), and the laws of physics (the gradual ordering of a randomized magnetic material). These examples should leave you feeling comfortable enough to begin reading code examples of simulated data and also come up with ideas for how your own work could benefit from simulations.

Chances are that, in the past, you have made assumptions about your data without knowing how to test those or alternate possibilities. Simulations give you a route to

do so, which means your conversations about data can expand to include hypotheti‐ cal examples paired with quantitative metrics from simulations. This will ground your discussions while opening new possibilities, both in the time series domain and in other branches of data science.

# Statistical Simulations

Statistical simulations are the most traditional route to simulated time series data. They are particularly useful when we know the underlying dynamics of a stochastic system and want to estimate a few unknown parameters or see how different assump‐ tions would impact the parameter estimation process (we will see an example of this later in the book). Even for physical systems, sometimes the statistical simulation is better.

Statistical simulations of time series data are also quite valuable when we need to have a definitive quantitative metric to define our own uncertainty about the accu‐ racy of our simulations. In traditional statistical simulations, such as an ARIMA model (to be discussed in Chapter 6), the formulas for the error are well established, meaning that to understand a system with a posited underlying statistical model, you do not need to run many simulations to make numerical assertions about error and variance.

# Deep Learning Simulations

Deep learning simulations for time series are a nascent but promising field. The advantages of deep learning are that very complicated, nonlinear dynamics can be captured in time series data even without the practitioner fully understanding the dynamics. This is also a disadvantage, however, in that the practitioner has no princi‐ pled basis for understanding the dynamics of the system.

Deep learning simulations also offer promise where privacy is a concern. For exam‐ ple, deep learning has been used to generate synthetic heterogeneous time series data for medical applications based on real time series data but without the potential to leak private information. Such a data set, if it can truly be produced without any pri‐ vacy leaks, would be invaluable because researchers could have access to a large array of (otherwise expensive and privacy-violating) medical data.

# More Resources

Cristóbal Esteban, Stephanie L. Hyland, and Gunnar Rätsch, “Real-Valued (Medical) Time Series Generation with Recurrent Conditional GANs,” unpublished manuscript, last revised December 4, 2017, https://perma.cc/Q69W-L44Z.   
The authors demonstrate how generative adversarial networks can be used to produce realistic-looking heterogenous medical time series data. This is an example of how deep learning simulation can be used to create ethical, legal, and (hopefully) privacy-preserving medical data sets to enable wider access to useful data for machine learning and deep learning in the healthcare context.   
Gordon Reikard and W. Erick Rogers, “Forecasting Ocean Waves: Comparing a Physics-based Model with Statistical Models,” Coastal Engineering 58 (2011): 409–16, https://perma.cc/89DJ-ZENZ.   
This article offers an accessible and practical comparison of two drastically dif‐ ferent ways of modeling a system, with physics or with statistics. The researchers conclude that for the particular problem they address, the timescale of interest to the forecaster should drive decisions about which paradigm to apply. While this article is about forecasting, simulation is strongly related and the same insights apply.   
Wolfgang Härdle, Joel Horowitz, and Jens-Peter Kreiss, “Bootstrap Methods for Time Series,” International Statistical Review / Revue Internationale de Statistique 71, no. 2 (2003): 435–59, https://perma.cc/6CQA-EG2E.   
A classic 2005 review of the difficulties of statistical simulation of time series data given temporal dependencies. The authors explain, in a highly technical statistics journal, why methods to bootstrap time series data lag behind methods for other kinds of data, as well as what promising methods were available at the time of writing. The state of the art has not changed too much, so this is a useful, if chal‐ lenging, read.

# Storing Temporal Data

Often, the value of time series data comes in its retrospective, rather than live stream‐ ing, scenarios. For this reason, storage of time series data is necessary for most time series analysis.

A good storage solution is one that enables ease of access and reliability of data without requiring a major investment of computing resources. In this chapter, we will discuss what aspects of a data set you should consider when designing for time series data storage. We will also discuss the advantages of SQL databases, NoSQL databases, and a variety of flat file formats.

Designing a general time series storage solution is a challenge because there are so many different kinds of time series data, each with different storage, read/write, and analysis patterns. Some data will be stored and examined repeatedly, whereas other data is useful only for a short period of time, after which it can be deleted altogether.

Here are a few use cases for time series storage that have different read, write, and query patterns:

1. You are collecting performance metrics on a production system. You need to store these performance metrics for years at a time, but the older the data gets, the less detailed it needs to be. Hence you need a form of storage that will auto‐ matically downsample and cull data as information ages.   
2. You have access to a remote open source time series data repository, but you need to keep a local copy on your computer to cut down on network traffic. The remote repository stores each time series in a folder of files available for down‐ load on a web server, but you’d like to collapse all these files into a single data‐ base for simplicity. The data should be immutable and able to be stored indefinitely, as it is intended to be a reliable copy of the remote repository.

3. You created your own time series data by integrating a variety of data sources at different timescales and with different preprocessing and formatting. The data collection and processing was laborious and time-consuming. You’d like to store the data in its final form rather than running a preprocessing step on it repeat‐ edly, but you’d also like to keep the raw data in case you later investigate prepro‐ cessing alternatives. You expect to revisit the processed and raw data often as you develop new machine learning models, refitting new models on the same data and also adding to your data over time as new, more recent raw data becomes available. You will never downsample or cull data in storage even if you do so for your analyses.

These use cases are quite varied in what their major demands on a system will be:

# Importance of how performance scales with size

In the first use case, we would look for a solution that could incorporate automa‐ ted scripting to delete old data. We would not be concerned with how the system scaled for large data sets because we would plan to keep the data set small. By contrast, in the second and third use cases we would expect to have a stable, large collection of data (use case 2), or a large and growing collection of data (use case 3).

# Importance of random versus sequential access of data points

In use case 2, we would expect all of the data to be accessed in equal amounts, as this time series data would all be the same “age” on insertion and would all refer‐ ence interesting data sets. In contrast, in use cases 1 and 3 (and especially 1, given the preceding description) we would expect the most recent data to be accessed more often.

# Importance of automation scripts

Use case 1 seems as though it might be automated, whereas use case 2 would not require automation (since that data would be immutable). Use case 3 suggests lit‐ tle automation but a great deal of data fetching and processing of all portions of the data, not just the most recent. In use case 1, we would want a storage solution that could be integrated with scripting or stored procedures, whereas in use case 3, we would want a solution that permitted a great deal of easy customization of data processing.

Just three examples already offer a good idea of the many use cases an all-purpose time series solution needs to meet.

In real-world use cases, you will be able to tailor your storage solution and not worry about finding a tool that fits all use cases. That said, you will always be choosing from a similar range of available technologies, which tend to boil down to:

• SQL databases

• NoSQL databases   
• Flat file formats

In this chapter, we cover all three options and discuss the advantages and disadvan‐ tages of each. Of course, the specifics will depend on the use case at hand, but this chapter will equip you with the foundation you’ll need when you begin your search for a time series storage option that works for your use case.

We first discuss what questions you should ask at the outset when picking a storage solution. Then we look at the great SQL versus NoSQL debate and examine some of the most popular time series storage solutions. Finally, we consider setting up policies to let old time series data expire and be deleted.

# Defining Requirements

When you are thinking about storage for time series data, you need to ask yourself a number of questions:

• How much time series data will you be storing? How quickly will that data grow? You want to choose a storage solution appropriate to the growth rate of the data you expect. Database administrators moving into time series work from transaction-oriented data sets are often surprised at just how quickly time series data sets can grow.   
• Do your measurements tend toward endless channels of updates (e.g., a constant stream of web traffic updates) or distinct events (e.g., an hourly air traffic time ser‐ ies for every major US holiday in the last 10 years)? If your data is like an endless channel, you will mostly look at recent data. On the other hand, if your data is a collection of distinct time series split into separate events, then events more dis‐ tant in time may still be fairly interesting. In the latter case, random access is a more likely pattern.   
• Will your data be regularly or irregularly spaced? If your data is regularly spaced, you can calculate more accurately in advance how much data you expect to col‐ lect and how often that data will be inputted. If your data is irregularly spaced, you must prepare for a less predictable style of data access that can efficiently allow for both dead periods and periods of write activity.   
• Will you continuously collect data or is there a well-defined end to your project? If you have a well-defined end to the data collection, this makes it easier to know how large a data set you need to accommodate. However, many organizations find that once they start collecting a particular kind of time series, they don’t want to stop!

• What will you be doing with your time series? Do you need real-time visualiza‐ tions? Preprocessed data for a neural network to iterate over thousands of times? Sharded data highly available to a large mobile user base? Your primary use case will indicate whether you are more likely to need sequential or random access to your data and how important a factor latency should be to your storage format selection.   
• How will you cull or downsample data? How will you prevent infinite growth? What should be the lifecycle of an individual data point in a time series? It is impossible to store all events for all time. It is better to make decisions about data deletion policies systematically and in advance rather than on an ad hoc basis. The more you can commit to up front, the better a choice you can make regard‐ ing storage formats. More on this in the next section.

The answers to these questions will indicate whether you should store raw or pro‐ cessed data, whether the data should be located in memory according to time or according to some other axis, and whether you need to store your data in a form that makes it easy to both read and write. Use cases will vary, so you should do a fresh inventory with every new data set.

![](images/e16b0e28ef7694ce893334ba82d74e45cdd5f40900753843512692dcaa905ee2.jpg)

Sharded data is data that is part of a large data system but spread into smaller, more manageable chunks, often across several servers on a network.

# Live Data Versus Stored Data

When thinking about what storage options are desirable for your data, it’s key to understand your data’s lifecycle. The more realistic you can be about your actual use cases for data, the less data you will need to save and the less time you will need to worry about finding the optimal storage system because you won’t quickly size up to an intractable amount of data. Often organizations over-record events of interest and are afraid to lose their data stores, but having more data stored in an intractable form is far less useful than having aggregated data stored at meaningful timescales.

For short-lived data, such as performance data that will be looked at only to ensure nothing is going wrong, it’s possible you may never need to store the data in the form in which it’s collected, at least not for very long. This is most important for eventdriven data, where no single event is important and instead aggregate statistics are the values of interest.

Suppose you are running a web server that records and reports to you the amount of time it took every single mobile device to fully load a given web page. The resulting irregularly spaced time series might look something like Table 5-1.

Table 5-1. Web server time series   

<table><tr><td>Timestamp</td><td>Time to load page</td></tr><tr><td>April 5, 2018 10:22:24 pm</td><td>23s</td></tr><tr><td>April 5, 2018 10:22:28 pm</td><td>15s</td></tr><tr><td>April 5, 2018 10:22:41 pm</td><td>14s</td></tr><tr><td>April 5, 2018 10:23:02 pm</td><td>11s</td></tr></table>

For many reasons, you would not be interested in any individual measurement of the time to load a page. You would want to aggregate the data (say, time to load averaged per minute), and even the aggregate statistics would only be interesting for a short while. Suppose you were on call overnight for that server. You’d want to make sure you could show performance was good while you were in charge. You could simplify this to a data point for your 12 hours of being on call, and you would have most of the information you could ever want, as shown in Table 5-2.

Table 5-2. A simplified data point for on-call hours   

<table><tr><td>Time period</td><td>Most popular hour of access</td><td>Num loads</td><td>Mean time to load</td><td>Max time to load</td></tr><tr><td>April 5, 2018 8pm–8 am</td><td>11 pm</td><td>3,470</td><td>21s</td><td>45s</td></tr></table>

In such a case, you should not plan to indefinitely store the individual events. Rather, you should build a storage solution that provides for staging of the individual events only as temporary storage until the data goes into its ultimate form. You will save yourself and your colleagues a lot of grief by preventing runaway data growth before it can even get started. Instead of having 3,470 individual events, none of which inter‐ ests anyone, you will have readily accessible and compact figures of interest. You should simplify data storage via aggregation and deduplication whenever possible.

Next we consider a few opportunities for reducing data without losing information.

# Slowly changing variables

If you are storing a state variable, consider recording only those data points where the value has changed. For example, if you are recording temperature at five-minute increments, your temperature curve might look like a step function, particularly if you only care about a value such as the nearest degree. In such a case, it’s not neces‐ sary to store repetitive values, and you will save storage space by not doing so.

# Noisy, high-frequency data

If your data is noisy, there are reasons for not caring very much about any particular data point. You might consider aggregating data points before recording them since the high level of noise makes any individual measurement less valuable. This will, of

course, be quite domain specific and you will need to ensure that downstream users are still able to assess the noise in the measurements for their purposes.

# Stale data

The older data is, the less likely your organization is going to use it, except in a very general way. Whenever you are beginning to record a new time series data set, you should think preemptively about when the time series data is likely to become irrelevant:

• Is there a natural expiration date?   
• If not, can you look through past research from your analytics department and see how far back they realistically go? When did any script in a Git repository actually access the oldest data in your data set?

If you can automate data deletion in a way that will not impede data analysis efforts, you will improve your data storage options by reducing the importance of scalability or the drag of slow queries on bloated data sets.

# Legal Considerations

If you work in an organization that is large or handles data required by outside par‐ ties (such as for regulatory audits), legal considerations should factor into your sys‐ tem requirements. You should assess how often you will likely get requests from outside parties for data, how much data they would require, and whether you would be able to choose the format or whether regulations impose a specific form.

It may seem unduly cautious to cite legal requirements when assessing data storage specifications, but recent developments in Europe and the United States suggest increased interest in the data sets used to power machine learning (e.g., the European Union’s General Data Protection Regulation).

So far, we have discussed the general range of use cases for time series storage. We have also reviewed a general set of queries related to how a time series data set will be produced and analyzed so that these queries can inform our selection of a storage for‐ mat. We now review two common options for storing time series: databases and files.

# Database Solutions

For almost any data analyst or data engineer, a database is an intuitive and familiar solution to the question of how to store data. As with relational data, a database will often be a good storage choice for time series data. This is particularly true if you want an out-of-the-box solution with any of these classic database characteristics:

• A storage system that can scale up to multiple servers   
• A low-latency read/write system   
• Functions already in place for computing commonly used metrics (such as com‐ puting the mean in a group-by query, where the group-by can apply to time metrics)   
• Troubleshooting and monitoring tools that can be used to tune system perfor‐ mance and analyze bottlenecks

These are good reasons among many to opt for a database rather than a filesystem, and you should always consider a database solution for data storage, particularly when working with a new data set. A database, particularly a NoSQL database, can help you preserve flexibility. Also, a database will get your project up and running sooner than if you are going to work with individual files because much of the boiler‐ plate you will need is already in place. Even if you ultimately decide on a file storage solution (and most people won’t), working with a database first can help you deter‐ mine how to structure your own files when your new data processes mature.

In the remainder of this section, we will cover the respective advantages of SQL and NoSQL databases for time series and then discuss currently popular database options for time series applications.

The good news is that time series graphs appear to be the fastest growth category of database at this time, so you can expect to see more and even better options for time series database solutions in the future.

# SQL Versus NoSQL

SQL versus NoSQL is just as lively a debate in the time series database community as it is more widely. Many expert database administrators insist that SQL is the only way to go and that there is no data in any shape that cannot be well described by a good set of relational tables. Nonetheless, in practice there is often a drop in performance when organizations try to scale SQL solutions to accommodate large amounts of time series data, and for this reason it’s always worth considering a NoSQL solution as well, particularly if you are seeking an open-ended solution that can scale to accom‐ modate cases where time series data collection begins with no finite time horizon in sight.

While both SQL and NoSQL solutions can be good for time series data, we first moti‐ vate our discussion of the difficulties of applying database logic to time series data by exploring how time series data differs from the kind of data for which SQL databases were developed.

# Characteristics of the data that originally inspired SQL databases

We can best understand the mismatch between SQL-like thinking and time series data by reviewing the history of SQL solutions. SQL solutions were based on transac‐ tional data, which is whatever data is needed to completely describe a discrete event. A transaction is composed of attributes that reflect many primary keys, such as prod‐ uct, participants, time, and value of a transaction. Notice that time can be present as a primary key but only as one among many, not as a privileged axis of information.

There are two important features of transactional data that are quite different from time series needs:

• Existing data points are often updated.   
• The data is accessed somewhat randomly as there is no underlying ordering required.

# Characteristics of time series data

Time series data elaborates the entire history of something, whereas a transaction recording tells us only the final state. Hence, time series data does not usually require updating, meaning that random access for write operations is a low priority.

This means that the performance objectives that have been key to decades of SQL database design are not very important for time series databases. In fact, when we consider objectives in designing a database for time series, we have quite different priorities because of how we will use our time series data. The primary features of our time series data use case are:

• Write operations predominate over read operations.   
• Data is neither written nor read nor updated in random order, but rather in the order related to temporal sequencing.   
• Concurrent reads are far more likely than they are for transaction data.   
• There are few, if any, primary keys other than time itself.   
• Bulk deletes are far more common than individual data point deletion.

These features support the use of a NoSQL database, because many generalapplication NoSQL databases offer much of what we want for time series databases, particularly the emphasis on write operations over read operations. Conceptually, NoSQL databases map well to time series data in that they natively reflect aspects of time series data collection, such as not all fields being collected for all data points. The flexible schemas of NoSQL are natural with time series data. Much of the data that motivated the current popularity of NoSQL databases is time series data.

For this reason, out-of-the-box NoSQL databases tend to perform better than SQL databases for write operations. Figure 5-1 plots performance of a commonly used SQL database versus a commonly used NoSQL database for the case of time series data point insertions (write operations).

![](images/7904178f5f8e94f76b0e4991eb23a785073b90d648d527e4811ce04a1d265f3b.jpg)  
Figure 5-1. One common feature of a database designed for time series use is that the data insertion rate is constant with respect to the database size. This is a striking advantage in comparison to a traditional SQL storage solution.

# How to choose between SQL and NoSQL

It may seem that I am attempting to push you toward the NoSQL use case, but there are also many good use cases for SQL databases. When thinking about your own data, keep in mind the principle that always applies to data storage, be it in a SQL database, a NoSQL database, or a plain old text file: data that tends to be requested at the same time should be stored in the same location. That’s the most important factor regardless of your use case.

Many conference presentations and blog posts herald NoSQL solutions as precisely catering to the situation of a high-write low-update scenario where data is not ordered randomly. Nonetheless, SQL databases are a viable and regularly used time series storage option. In particular, with some architectural changes in emphasis to traditional SQL databases and their internal memory structure, these challenges can be met while keeping some of the advantages of SQL. For example, something as sim‐ ple and seemingly obvious as structuring SQL tables’ internal memory representation to account for time can make a substantial performance difference.

Ultimately, the distinctions between NoSQL and SQL are highly implementationdependent and not as systematic or important as they are made out to be. Let your data drive your selection of a specific implementation of one of these technologies. As you consider the attributes of your time series data and access use patterns, you can keep some general limitations in mind:

# Pluses of SQL for time series

• If your time series is stored in a SQL database, you can easily relate it to relevant cross-sectional data also stored in that database.   
• Hierarchical time series data is a natural fit with relational tables. An appropriate set of SQL schema will help you group related time series and clearly delineate the hierarchy, whereas these could be easily scattered less systematically in a NoSQL solution.   
• If you are crafting a time series based on transactional data, where the data is best stored in a SQL database, it will be advantageous to also store your time series in the same database for ease of verification, cross-referencing, and so on.

# Pluses of NoSQL for time series

• Writes are fast.   
• They’re good if you don’t know enough about future data to design and intelli‐ gent and robust schema.   
• For the inexpert user, these databases are often the more performant out-of-thebox solution because you are less likely to design an awkward schema or to be locked into a mediocre schema design.

# Popular Time Series Database and File Solutions

Now we’ll discuss some popular database solutions for time series data. These give you a sense of what is available in addition to traditional SQL solutions. Note that the technologies discussed here occupy a crowded and fragmented technology landscape. What is commonly used this year may not be so popular next year. Thus, this discus‐ sion should not be viewed as a specific technology recommendation so much as a set of samples helping to illustrate the current state of the market.

# Time series–specific databases and related monitoring tools

First we discuss tools that are built specifically for the purpose of storing and moni‐ toring time series data. In particular, we take a look at a time series database (InfluxDB) and another product that is a performance monitoring tool, which can

double as a time series storage solution (Prometheus). The advantages of each tool necessarily reflect its distinct emphasis and use patterns.

InfluxDB. InfluxDB is a time series–specific database, per its own description on the GitHub project’s web page:

InfluxDB is an open source time series database...useful for recording metrics, events, and performing analytics.

In InfluxDB, data is organized by time series. A data point in InfluxDB consists of:

• A timestamp   
• A label indicating what the measurement consists of   
• One or more key/value fields (such as temperature=25.3)   
• Key/value pairs containing metadata tags

InfluxDB, as a time-aware database, automatically timestamps any data point that comes in without a timestamp. Also, InfluxDB uses SQL-like querying, such as:

SELECT * FROM access_counts WHERE value $>$ 10000

Other advantages of InfluxDB include:

• Data retention options that allow you to easily automate the designation and deletion of stale data   
• High data ingestion speed and aggressive data compression   
• Ability to tag individual time series to allow for fast indexing of time series that match a specific criterion   
• Membership in the mature TICK stack, which is a platform for capturing, stor‐ ing, monitoring, and displaying time series data

There are many other time series–specific databases; right now InfluxDB happens to be the most popular, so it is the one you are most likely to encounter. The options it offers are those commonly offered by time series–specialized databases, as these options reflect the most commonly desired attributes for storage of time series data.

As a database, InfluxDB is a push-based system, meaning that when you use it you push data into the database for ingestion. This is different from the next option we’ll discuss, Prometheus.

Given these specifications, a solution such as InfluxDB provides all general function‐ ality right out of the box in a time-aware manner. This means you can use your exist‐ ing SQL skills while also benefiting from advantages related to the need to capture but control the growth of time series data. Finally, you have the fast write so often necessary when capturing time series data as it is produced.

Prometheus. Prometheus describes itself as a “monitoring system and time series database” that works via HTTP. This description indicates its general emphasis: mon‐ itoring first and storage second. The great advantage of Prometheus is that it is a pullbased system, which means that the logic for how data is pulled for creating a time series and how often it is centralized can be easily adjusted and inspected.

Prometheus is a great resource to have during emergencies because it is atomic and self-reliant. However, it is also not guaranteed to be entirely up-to-date or accurate, due to its pull-based architecture. While it should be the go-to technology for quickand-dirty performance monitoring, it is not appropriate for applications where data has to be $1 0 0 \%$ accurate.

Prometheus uses a functional expression language called PromQL for its querying:

access_counts > 10000

Prometheus also offers, via PromQL, an API for many common time series tasks, even including sophisticated functions such as making a prediction (predict_lin ear()) and calculating the per-unit-of-time rate of increase for a time series (rate()). Aggregation over periods of time is also offered with a simple interface. Prometheus tends to emphasize monitoring and analysis over maintenance, so compared to InfluxDB there are fewer automated data-curating functionalities.

Prometheus is a helpful time series storage solution particularly for live streaming applications and when data availability is paramount. It has a steeper learning curve due to its custom scripting language and the less database-like architecture and API, but it is nonetheless widely used and enjoyed by many developers.

# General NoSQL databases

While time series–specific databases offer many advantages, you can also consider the case of more general-use NoSQL databases. These sorts of databases are based on a document structure rather than a table structure, and they do not usually have many explicit functions specifically for time series.

Nonetheless, the flexible schemas of NoSQL databases remain useful for time series data, particularly for new projects where the rhythm of data collection and the num‐ ber of input channels may change through the course of a data set’s lifetime. For example, a time series may initially start out as only one channel of data but gradually grow to include more kinds of data all timestamped together. Later, it may be decided that some of the input channels are not particularly useful, and they may be discontinued.

In such a case, storing the data in a SQL table would be difficult for several reasons, and would result in a great many NaNs where data was not available. A NoSQL data‐

base would simply leave the channels absent when they were not available rather than marking up a rectangular data store with a bunch of NaNs.

One popular and performant NoSQL time series database is MongoDB. Mongo is particularly aware of its value as a time series database and has a strong push to develop IoT-friendly architecture and instructions. It offers high-level aggregation features that can be applied to aggregation operations by time and time-related groupings, and it also offers many automated processes to divide time into humanrelevant markings, such as day of the week or day of the month:

• $dayOfWeek   
• $dayOfMonth   
• $hour

Also, Mongo has devoted extensive documentation efforts to demonstrating how time series can be handled.1 A ready set of documentation and institutional focus on time series data explicitly mean that users can expect this database to continue devel‐ oping even more time-friendly features.

More useful than all this functionality, however, is Mongo’s flexibility in evolving schema over time. This schema flexibility will save you a lot of grief if you work on a rapidly evolving time series data set with ever-changing data collection practices.

For example, imagine a sequence of time series data collection practices such as what might occur in a healthcare startup:

1. Your startup launches as a blood pressure app and collects only two metrics, sys‐ tolic and diastolic blood pressure, which you encourage users to measure several times daily…   
2. But your users want enhanced lifestyle advice and they are happy to give you more data. Users provide everything from their birth dates to monthly weight recordings to hourly step counts and calorie counts. Obviously, these different kinds of data are collected at drastically different rhythms…   
3. But later you realize some of this data is not very useful so you stop collecting it...   
4. But your users miss your display of the data even if you don’t use it, so you restart collection of the most popular data…

5. And then the government of one of your largest markets passes a law regarding the storage of health data and you need to purge that data or encrypt it, so you need a new encrypted field…   
6. And the changes continue.

When you need the querying and schema flexibility described in an application like this, you would be well served by a general NoSQL database that offers a reasonable balance between time-specific and more general flexibility.

Another advantage of a general-purpose NoSQL database rather than a time series– specific database is you can more easily integrate non-time series data into the same database for ease of cross-referencing the related data sets. Sometimes a generalpurpose NoSQL database is just the right mix of performance considerations and SQL-like functionality without the cleverness needed to optimize a SQL database schema for time series functionality.

In this section, we have examined NoSQL database solutions2 and surveyed some options now in popular use. While we can expect the particular technologies that dominate the market to evolve over time, the general principles on which these tech‐ nologies operate and the advantages they offer will remain the same. As a quick recap, some of the advantages we find in different varieties of databases are:

• High read or write capacities (or both)   
• Flexible data structures   
• Push or pull data ingestion   
• Automated data culling processes

A time series–specific database will offer the most out-of-the-box automation for your time series–specific tasks, but it will offer less schema flexibility and fewer opportunities to integrate related cross-sectional data than a more general, allpurpose NoSQL database.

Generally speaking, databases will offer more flexibility than flat file storage formats, but this flexibility means that databases are less streamlined and less I/O performant than simple flat files, which we discuss next.

# File Solutions

At the end of the day, a database is a piece of software that integrates both scripting and data storage. It is essentially a flat file wrapped in a special piece of software responsible for making that file as safe and easy to use as possible.

Sometimes it makes sense to take away this outer layer and take on the full responsi‐ bility of the data storage ourselves. While this is not very common in business appli‐ cations, it is frequently done in scientific research and in the rare industrial application (such as high-frequency trading) in which speed is paramount. In such cases, it is the analyst who will design a much more intricate data pipeline that involves allocating storage space, opening files, reading files, closing files, protecting files, and so on, rather than simply writing some database queries.

A flat file solution is a good option if any of these conditions apply:

• Your data format is mature so that you can commit to a specification for a rea‐ sonably long period of time.   
• Your data processing is I/O bound, so it makes sense to spend development time speeding it up.   
• You don’t need random access, but can instead read data sequentially.

In this section, we briefly survey some common flat file solutions. You should also remember that you can always create your own file storage format, although this is fairly complex and is usually worth doing only if you are working in a highly per‐ formant language, such as $\mathrm { C } { + } { + }$ or Java.

If your system is mature and performance-sensitive enough to suit a flat file system, there are several advantages to implementing one, even if it may mean the chore of migrating your data out of a database. These advantages include:

• A flat file format is system agnostic. If you need to share data, you simply make files available in a known shareable format. There is no need to ask a collaborator to remotely access your database or set up their own mirrored database.   
• A flat file format’s I/O overhead is necessarily less than that for a database, because this amounts to a simple read operation for a flat file rather than a retrieval and a read with a database.   
• A flat file format encodes the order in which data should be read, whereas not all databases will do so. This enforces serial reading of the data, which can be desira‐ ble, such as for deep learning training regimes.   
• Your data will occupy a much smaller amount of memory than it would on a database because you can maximize compression opportunities. Relatedly, you can tune the degree of data compression to explicitly balance your desire to

minimize both the data storage footprint and I/O time in your application. More compression will mean a smaller data footprint but longer I/O waits.

# NumPy

If your data is purely numeric, one widely used option for holding that data is a Python NumPy array. NumPy arrays can easily be saved in a variety of formats, and there are many benchmarking publications comparing their relative performance. For example, the array_storage_benchmark GitHub repo is designed to test the effi‐ ciency and speed of various NumPy file formats.

A downside of NumPy arrays is that they have a single data type, which means you cannot store heterogeneous time series data automatically, but must instead think about whether having just one data type can work for your data in raw or processed form (although there are ways around this restriction). Another downside of NumPy arrays is that it is not natural to add labels to rows or columns, so there is no straight‐ forward way to timestamp each row of an array, for example.

The advantage of using a NumPy array is that there are many options for saving one, including a compressed binary format that takes up less space and has faster I/O than you will see with a database solution. It is also as out-of-the-box as it gets in terms of a performant data structure for analysis as well as storage.

# Pandas

If you want easy labeling of data or easy storage of heterogenous time series data (or both), consider the less streamlined but more flexible Pandas data frame. It can be particularly useful when you have a time series that consists of many distinct kinds of data, perhaps including event counts (ints), state measurements (floats), and labels (strings or one-hot encodings). In such cases, you will likely want to stick to Pandas data frames (also remember that the name “pandas” actually comes from an elision of “panel data,” so this is a natural format for many use cases).

Pandas data frames are widely used, and there are several resources online to com‐ pare the various formats used to store this data.

# Standard R Equivalents

The native formats for storing R objects are .Rds and .Rdata objects. These are binary file formats, so they will necessarily be more efficient both for compression and I/O than text-based formats, and in this way are akin to Pandas data frames in Python. Relatedly, the feather format can be used in both R and Python to save data frames in a language-agnostic file format. For R users, the native binary formats will (of course) be the most performant. Table 5-3 compares the file format options.

Table 5-3. Size and performance comparison of file format options   

<table><tr><td>Format name</td><td>Relative size</td><td>Relative time to load</td></tr><tr><td>.RDS</td><td>1x</td><td>1x</td></tr><tr><td>feather</td><td>2x</td><td>1x</td></tr><tr><td>csv</td><td>3x</td><td>20x</td></tr></table>

As we can see, the native format is a clear winner to minimize both storage space and I/O, and those who use a text-based file format rather than binary will pay a heavy price both in storage and I/O slowdowns.

# Xarray

When your time series data reaches many dimensions wide, it may be time to think about a more industrial data solution—namely Xarray, which is useful for a number of reasons:

• Named dimensions   
• Vectorized mathematical operations, like NumPy   
• Group-by operations, like Pandas   
• Database-like functionality that permits indexing based on a time range   
• A variety of file storage options

Xarray is a data structure that supports many time series–specific operations, such as indexing and resampling on time, interpolating data, and accessing individual com‐ ponents of a date time. Xarray was built as a high-performance scientific computing instrument, and it is woefully underutilized and underappreciated for time series analysis applications.

Xarray is a data structure in Python, and it comes with a choice of storage options. It implements both pickling and another binary file format called netCDF, a universal scientific data format supported across many platforms and languages. If you are looking to up your time series game in Python, Xarray is a good place to start.

As you’ve seen, there are many options for flat file storage of time series data, some with associated functionality (Xarray) and some with very streamlined, purely numeric formats (NumPy). When migrating a data set from a database-oriented pipeline to a file-oriented pipeline, there will be some growing pains related to sim‐ plifying data and rewriting scripts to move logic out of the database and into the explicit ETL (extract-transform-load) scripts. In cases where performance is paramount, moving your data into files is likely the most important step to reduce latency. Common cases where this is desirable include:

• Latency-sensitive forecasting, such as for user-facing software

• I/O-intensive repetitive data access situations, such as training deep learning models

On the other hand, for many applications, the convenience, scalability, and flexibility of a database is well worth the higher latency. Your optimal storage situation will depend very much on the nature of the data you are storing and what you want to do with it.

# More Resources

• On time series database technologies:

Jason Moiron, “Thoughts on Time-Series Databases,” jmoiron blog, June 30, 2015, https://perma.cc/8GDC-6CTX.

This classic blog post from 2015 provides a glimpse into an earlier time period with time series databases and the cult of recording everything. This high-level overview of options for storing time series databases and typical use cases is highly informative for beginners to database administration and engineering.

Preetam Jinka, “List of Time Series Databases,” Misframe blog, April 9, 2016, https://perma.cc/9SCQ-9G57.

This lengthy list of time series databases is frequently updated to show data‐ bases currently on the market. Each database entry comes with highlights as to how a particular database relates to its competitors and predecessors.

Peter Zaitsev, “Percona Blog Poll: What Database Engine Are You Using to Store Time Series Data?” Percona blog, February 10, 2017, https://perma.cc/5PXF-BF7L.

This 2017 poll of database engineers indicated that relational databases (SQL databases) continued to dominate as a group, with $3 5 \%$ of respondents indi‐ cating they use these databases to store time series data. ElasticSearch, InfluxDB, MongoDB, and Prometheus were also favorites.

Rachel Stephens, “The State of the Time Series Database Market,” RedMonk, April 3, 2018, https://perma.cc/WLA7-ABRU.

This recent data-driven write-up by a tech analyst describes the most popu‐ lar solutions to time series database storage via an empirical investigation of GitHub and Stack Overflow activity. The report also indicates a high degree of fragmentation in the time series storage domain due to a wide variety of use cases for time series data as well as a larger trend toward segmentation in databases.

Prometheus.io, “Prometheus Documentation: Comparison to Alternatives,” n.d., https://perma.cc/M83E-NBHQ.   
This extremely detailed and comprehensive list compares Prometheus to a number of other popular time series storage solutions. You can use this model as a quick reference for the dominant data structures and storage structures used by the alternatives to Prometheus. This is a good place to get an overview of what’s on offer and what the trade-offs are between various databases designed for temporal data.   
• On adapting general database technologies:   
Gregory Trubetskoy, “Storing Time Series in PostreSQL Efficiently,” Notes to Self blog, September 23, 2015, https://perma.cc/QP2D-YBTS.   
This older but still relevant blog post explains how to store time series in Postgres in a performance-focused way. Trubetskoy explains the difficulties of the “naive” approach of a column of values and a column of timestamps, and gives practical advice for approaches based on Postgres arrays.   
Josiah Carlson, “Using Redis as a Time Series Database,” InfoQ, January 2, 2016, https://perma.cc/RDZ2-YM22.   
This article provides detailed advice and examples related to using Redis as a time series database. While Redis has been used for time series data since its creation, there are several gotchas the author points out as well as advanta‐ geous data structures that can be applied in many time series use case sce‐ narios. This article is useful for learning more about how to use Redis with time series data, but also as an example of how more general tools can be repurposed for time series–specific uses.   
Mike Freedman, “Time Series Data: Why (and How) to Use a Relational Database Instead of NoSQL,” Timescale blog, April 20, 2017, https://perma.cc/A6CU-6XTZ.   
This blog post by TimescaleDB’s founder describes the ways in which his team built a time series database as a relational database with memory layout modifications specific to temporal data. TimescaleDB’s contention is that the main problem with traditional SQL databases for time series data is that such databases do not scale, resulting in slow performance as data is shifted into and out of memory to perform time-related queries. TimescaleDB proposes laying out memory and memory mappings to reflect the temporal nature of data and reduce swapping disparate data into and out of memory.   
Hilmar Buchta, “Combining Multiple Tables with Valid from/to Date Ranges into a Single Dimension,” Oraylis blog, November 17, 2014, https://perma.cc/B8CT-BCEK.   
While the title is not very appealing, this is an extremely useful blog post for those of us who are not SQL database experts. It illustrates a good way to deal with the hairy problem of data that is marked as being valid only for a

certain range of time and how to combine this data and join with it across multiple tables. This is a surprisingly common and a surprisingly difficult task. The example covered involves a few human resources charts, including an employee’s valid dates of employment, valid dates for a given department, valid dates for a specific office location, and valid dates for a company car. A very likely task would be to determine an employee’s status on a particular day—was that person employed, and if so, in what department and in what location, and did the employee have a company car? This sounds intuitive, but the SQL to answer this question is difficult and well covered in this post.

# Statistical Models for Time Series

In this chapter, we study some linear statistical models for time series. These models are related to linear regression but account for the correlations that arise between data points in the same time series, in contrast to the standard methods applied to cross-sectional data, in which it is assumed that each data point is independent of the others in the sample.

The specific models we will discuss are:

• Autoregressive (AR) models, moving average (MA) models, and autoregressive integrated moving average (ARIMA) models   
• Vector autoregression (VAR)   
• Hierarchical models.

These models have traditionally been the workhorses of time series forecasting, and they continue to be applied in a wide range of situations, from academic research to industry modeling.

# Why Not Use a Linear Regression?

As a data analyst, you are probably already familiar with linear regressions. If you are not, they can be defined as follows: a linear regression assumes you have independ‐ ently and identically distributed (iid) data. As we have discussed at length in earlier chapters, this is not the case with time series data. In time series data, points near in time tend to be strongly correlated with one another. In fact, when there aren’t tem‐ poral correlations, time series data is hardly useful for traditional time series tasks, such as predicting the future or understanding temporal dynamics.

Sometimes time series tutorials and textbooks or give an undue impression that lin‐ ear regression is not useful for time series. They make students think that simple lin‐ ear regressions simply will not cut it. Luckily this is not the case at all. Ordinary least squares linear regression can be applied to time series data provided the following conditions hold:

# Assumptions with respect to the behavior of the time series

• The time series has a linear response to its predictors.   
• No input variable is constant over time or perfectly correlated with another input variable. This simply extends the traditional linear regression requirement of independent variables to account for the temporal dimension of the data.

# Assumptions with respect to the error

• For each point in time, the expected value of the error, given all explanatory vari‐ ables for all time periods (forward and backward), is 0.   
• The error at any given time period is uncorrelated with the inputs at any time period in the past or future. So a plot of the autocorrelation function of the errors will not indicate any pattern.   
• Variance of the error is independent of time.

If these assumptions hold, then ordinary least squares regression is an unbiased esti‐ mator of the coefficients given the inputs, even for time series data.1 In this case, the sample variances of the estimates have the same mathematical form as they do for standard linear regression. So if your data meets the assumptions just listed, you can apply a linear regression, which will no doubt help to offer clear and simple intuitions for your time series behavior.

The data requirements just described are similar to those for standard linear regres‐ sion applied to cross-sectional data. What we have added is an emphasis on the tem‐ poral qualities of the data set.

![](images/37e6d9e98bca08cbf00594ea1bb5b63cc58fbbb4a40f61f53d71f4623818d021.jpg)

Don’t force linear regression. Some of the consequences of apply‐ ing linear regression when your data doesn’t meet the required assumptions are:

• Your coefficients will not minimize the error of your model.   
• Your $\boldsymbol { p }$ -values for determining whether your coefficients are nonzero will be incorrect because they rely on assumptions that are not met. This means your assessments of coefficient significance could be wrong.

Linear regressions can be helpful in offering simplicity and trans‐ parency when appropriate, but an incorrect model certainly isn’t transparent!

It’s fair to question whether time series analysts are overly rigid in applying the assumptions required by standard linear regression so stringently they cannot use the linear regression technique. Real-world analysts take liberties with model assump‐ tions from time to time. This can be productive so long as the potential downsides of doing so are understood.

The importance of adhering to a model’s assumptions depends strongly on the domain. Sometimes a model is applied in full knowledge that its baseline assump‐ tions are not met because the consequences are not too serious relative to the payoff. For example, in high-frequency trading, linear models are quite popular for a number of reasons despite no one believing that the data strictly follows all the standard assumptions.2

![](images/236702e1afd702ee204d21d1923f074b665c42571508afebaf52790923b5bc14.jpg)

# What Is An Unbiased Estimator?

If an estimate is not an overestimate or underestimate, it is using an unbiased estimator. This tends to be a good thing, although you should be aware of the bias-variance trade-off, which is a descrip‐ tion of the problem for both statistical and machine learning prob‐ lems wherein models with a lower bias in their parameter estimates tend to have a higher variance of the estimate of the parameter. The variance of the parameter’s estimate reflects how variable an estimate will be across different samples of the data.

If you find yourself in a situation where a linear regression may be a good fit for your forecasting task, consider taking advantage of tslm(), a function in the forecast package designed to provide easy linear regression methodologies for time series data.

# Statistical Methods Developed for Time Series

We consider statistical methods developed specifically for time series data. We first study methods developed for univariate time series data, beginning with the very simple case of an autoregressive model, which is a model that says that future values of a time series are a function of its past values. We then work our way up to increas‐ ingly complex models, concluding with a discussion of vector autoregression for multivariate time series and of some additional specialized time series methods, such as GARCH models and hierarchical modeling.

# Autoregressive Models

The autoregressive (AR) model relies on the intuition that the past predicts the future and so posits a time series process in which the value at a point in time $t$ is a function of the series’s values at earlier points in time.

Our discussion of this model will be more detailed to give you a sense of how statisti‐ cians think about these models and their properties. For this reason, we start with a fairly lengthy theory overview. You can skim this if you are not interested in the mechanics behind how statistical model properties are derived for time series.

# Using algebra to understand constraints on AR processes

Autoregression looks like what many people would use as a first attempt to fit a time series, particularly if they had no information other than the time series itself. It is exactly what its name implies: a regression on past values to predict future values.

The simplest AR model, an AR(1) model, describes a system as follows:

$$
y _ {t} = b _ {0} + b _ {1} \times y _ {t - 1} + e _ {t}
$$

The value of the series at time $t$ is a function of a constant $b _ { o } ,$ its value at the previous time step multiplied by another constant $b _ { 1 } \times y _ { t - 1 }$ and an error term that also varies with time $e _ { t } .$ This error term is assumed to have a constant variance and a mean of 0. We denote an autoregressive term that looks back only to the immediately prior time as an AR(1) model because it includes a lookback of one lag.

Incidentally, the AR(1) model has an identical form to a simple linear regression model with only one explanatory variable. That is, it maps to:

$$
Y = b _ {0} + b _ {1} \times x + e
$$

We can calculate both the expected value of $\gamma _ { t }$ and its variance, given $y _ { t - 1 : }$ , if we know the value of $b _ { 0 }$ and $b _ { 1 }$ . See Equation 6-1.3

Equation 6-1. $E ( y _ { t } \mid y _ { t - 1 } ) = b _ { o } + b _ { \scriptscriptstyle { I } } \times y _ { t - 1 } + e _ { t }$

$$
\operatorname {V a r} \left(y _ {t} \mid y _ {t - 1}\right) = \operatorname {V a r} \left(e _ {t}\right) = \operatorname {V a r} (e)
$$

# What Is the | Symbol?

In statistics, you can condition probabilities, expected values, and so on, of one vari‐ able on another. Such conditioning is indicated with the | symbol, or pipe, which should be read as “given.” So for example, $\mathrm { P ( A | B ) }$ should be read as “the probability of A given B.”

Why do we use it in the previous case? In equation (0) we want to determine the expected value of the process at time $t , y _ { t }$ given what the value is at one time step pre‐ vious. This is a way of expressing that we know information up to time $t - 1$ . The expected value of $\gamma _ { t }$ if we do not have any information is the expected value we can calculate more generally from the process. However, if we happen to know the value one time step earlier, we can be more specific about the expected value at time t.

The generalization of this notation allows the present value of an AR process to depend on the $\boldsymbol { p }$ most recent values, producing an $\operatorname { A R } ( p )$ process.

We now switch to more traditional notation, which uses $\phi$ to denote the autoregres‐ sion coefficients:

$$
y _ {t} = \phi_ {0} + \phi_ {1} \times y _ {t - 1} + \phi_ {2} \times y _ {t - 2} + \dots + \phi_ {p} \times y _ {t - p} + e _ {t}
$$

As discussed in Chapter 3, stationarity is a key concept in time series analysis because it is required by many time series models, including AR models.

We can determine the conditions for an AR model to be stationary from the defini‐ tion of stationarity. We continue our focus on the simplest AR model, AR(1) in Equation 6-2.

$$
E q u a t i o n 6 - 2. y _ {t} = \phi_ {0} + \phi_ {1} \times y _ {t - 1} + e _ {t}
$$

We assume the process is stationary and then work “backward” to see what that implies about the coefficients. First, from the assumption of stationarity, we know that the expected value of the process must be the same at all times. We can rewrite $\gamma _ { t }$ per the equation for an AR(1) process:

$$
E \left(y _ {t}\right) = \mu = E \left(y _ {t - 1}\right)
$$

By definition, $e _ { t }$ has an expected value of 0. Additionally the phis are constants, so their expected values are their constant values. Equation 6-2 reduces on the lefthand side to:

$$
E \left(y _ {t}\right) = E \left(\phi_ {0} + \phi_ {1} \times y _ {t - 1} + e _ {t}\right)
$$

$$
E \left(y _ {t}\right) = \mu
$$

And on the righthand side to:

$$
\phi_ {0} + \phi_ {1} \times \mu + 0
$$

This simplifies to:

$$
\mu = \phi_ {0} + \phi_ {1} \times \mu
$$

which in turn implies that (Equation 6-3).

$$
E q u a t i o n 6 - 3. \mu = \frac {\phi_ {0}}{1 - \phi_ {l}}
$$

So we find a relationship between the mean of the process and the underlying AR(1) coefficients.

We can take similar steps to look at how a constant variance and covariance impose conditions on the $\phi$ coefficients. We begin by substituting the value of $\phi _ { 0 } ,$ which we can derive Equation 6-4 from Equation 6-3.

$$
E q u a t i o n 6 - 4. \phi_ {0} = \mu \times (1 - \phi_ {1})
$$

into Equation 6-2:

$$
y _ {t} = \phi_ {0} + \phi_ {1} \times y _ {t - 1} + e _ {t}
$$

$$
y _ {t} = \left(\mu - \mu \times \phi_ {1}\right) + \phi_ {1} \times y _ {t - 1} + e _ {t}
$$

$$
y _ {t} - \mu = \phi_ {1} \left(y _ {t - 1} - \mu\right) + e _ {t}
$$

If you inspect Equation 6-4, what should jump out at you is that it has very similar expressions on the lefthand and righthand sides, namely ${ { y } _ { t } } \mathrm { ~ - ~ } { { \mu } }$ and $\gamma _ { t - 1 } - \mu$ . Given that this time series is stationary, we know that the math at time $t - 1$ should be the same as the math at time t. We rewrite Equation 6-4 in the frame of a time one step earlier as Equation 6-5.

$$
E q u a t i o n 6 - 5. y _ {t - 1} - \mu = \phi_ {1} \left(y _ {t - 2} - \mu\right) + e _ {t - 1}
$$

We can then substitute this into Equation 6-4 as follows:

$$
y _ {t} - \mu = \phi_ {1} \left(\phi_ {1} \left(y _ {t - 2} - \mu\right) + e _ {t - 1}\right) + e _ {t}
$$

We rearrange for clarity in Equation 6-6.

$$
E q u a t i o n 6 - 6. y _ {t} - \mu = e _ {t} + \phi_ {1} \left(e _ {t - 1} + \phi_ {1} \left(y _ {t - 2} - \mu\right)\right)
$$

It should catch your eye that another substitution is possible in Equation 6-6 for $\gamma _ { t - 2 } - \mu$ using the same recursive substitution we used earlier, but instead of working on $\boldsymbol { y } _ { t - 1 }$ we will work on $\boldsymbol { y } _ { t - 2 }$ . If you do this substitution, the pattern becomes clear:

$$
\begin{array}{l} y _ {t} - \mu = e _ {t} + \phi_ {1} \left(e _ {t - 1} + \phi_ {1} \left(e _ {t - 2} + \phi_ {1} \left(y _ {t - 3} - \mu\right)\right)\right) \\ = e _ {t} + \phi \times e _ {t - 1} + \phi^ {2} \times e _ {t - 2} + \phi^ {3} \times e _ {t - 3} + (\text {e x p r e s s i o n s s t i l l t o b e s u b s t i t u t e d}) \\ \end{array}
$$

So we can conclude more generally that $\begin{array} { r } { \boldsymbol { y } _ { t } - \boldsymbol { \mu } = \sum _ { i = 1 } ^ { \infty } \boldsymbol { \phi } _ { 1 } ^ { i } \times \boldsymbol { e } _ { t - i } , } \end{array}$

In plain English, $\gamma _ { t }$ minus the process mean is a linear function of the error terms.

This result can then be used to compute the expectation of $E [ ( y _ { t } - \mu ) \times e _ { t + 1 } ] = 0$ given that the values of $e _ { t }$ at different $t$ values are independent. From this we can conclude that the covariance of $\boldsymbol { y } _ { t - 1 }$ and $e _ { t }$ is 0, as it should be. We can apply similar logic to calculate the variance of $\gamma _ { t }$ by squaring this equation:

$$
y _ {t} - \mu = \phi_ {1} \left(y _ {t - 1} - \mu\right) + e _ {t}
$$

$$
\operatorname {v a r} \left(y _ {t}\right) = \phi_ {1} ^ {2} \operatorname {v a r} \left(y _ {t - 1}\right) + \operatorname {v a r} \left(e _ {t}\right)
$$

Because the variance quantities on each side of the equation must be equal due to sta‐ tionarity, such that $( \nu a r ( y _ { \mathrm { t } } ) = \nu a r ( y _ { \mathrm { t } } - 1 )$ , this implies that:

$$
\operatorname {v a r} \left(y _ {t}\right) = \frac {\operatorname {v a r} \left(e _ {t}\right)}{1 - \phi_ {1} ^ {2}}
$$

Given that the variance must be greater than or equal to 0 by definition, we can see that $\phi _ { 1 } ^ { 2 }$ must be less than 1 to ensure a positive value on the righthand side of the preceding equation. This implies that for a stationary process we must have $- 1 < \phi _ { I } <$ 1. This is a necessary and sufficient condition for weak stationarity.

# Weak Versus Strong Stationarity

When we discuss stationarity in this book, we are always referring to weak stationar‐ ity. Weak stationarity requires only that the mean and variance of a process be time invariant.

Strong stationarity requires that the distribution of the random variables output by a process remain the same over time, so, for example, it demands that the statistical distribution of $\boldsymbol { y } _ { 1 } , \boldsymbol { y } _ { 2 } , \boldsymbol { y } _ { 3 }$ be the same as $y _ { 1 0 1 } , y _ { 1 0 2 } , y _ { 1 0 3 }$ for any measure of that distribu‐ tion rather than the first and second moments (the mean and variance).

Fun fact: while strong stationarity would seem to fully encompass the case of weak stationarity, this isn’t entirely so. Strong stationarity includes weak stationarity where the mean and variance of a process do exist, but if a process does not have a mean or a variance, it may nonetheless be strongly stationary even though it can’t be weakly stationary. The classic example of this is the Cauchy distribution, which has unde‐ fined mean and infinite variance. A Cauchy process cannot meet the requirements of weak stationarity but can meet the requirements of strong stationarity.

We studied the AR(1) process because it is the simplest autoregressive process. In practice, you will fit more complex models all the time. It is possible to derive similar conditions for stationarity for an arbitrary order $\operatorname { A R } ( p )$ , and there are many books where this is demonstrated. If you are interested in seeing this worked out more explicitly, take a look at the resources listed at the end of the chapter. The most important takeaway from this discussion is that time series are quite approachable with some algebra and statistics, and that stationarity is not simply a matter of graph‐ ing a model but rather a mathematical concept that can be worked out with respect to the specifics of any given statistical model.

![](images/6a26dfe1c9e0b4cb9f42f8affe4663d17183f80a08a17b932ec6cbc52fbb6542.jpg)

A distribution is a statistical function describing the probability for all possible values that a particular value will be generated by a process. While you might not have encountered this term formally, you have undoubtedly encountered this concept. For example, consider the bell curve, which is a probability distribution indicat‐ ing that most measurements will fall close to, and be distributed equally on both sides of, the mean. It’s usually called a normal or Gaussian distribution in statistics.

# Choosing parameters for an AR(p) model

To assess the appropriateness of an AR model for your data, begin by plotting the process and its partial autocorrelation function (PACF).The PACF of an AR process should cut off to zero beyond the order $\boldsymbol { p }$ of an $\operatorname { A R } ( p )$ process, giving a concrete and visual indication of the order of an AR process empirically seen in the data.

On the other hand, an AR process will not have an informative autocorrelation func‐ tion (ACF), although it will have the characteristic shape of an ACF: exponential tapering with increasing time offset.

Let’s take a look at this with some actual data. We use some demand forecasting data published in the UCI Machine Learning Repository.

First we plot the data in chronological order (Figure 6-1). Since we will model this as an AR process, we look to the PACF to set a cutoff on the order of the process (Figure 6-2).

![](images/b03d118da16ba8a3704a9a4cb421e1d77ecea77841cedb724872ce48704b4d55.jpg)  
Figure 6-1. Daily number of Banking orders (2).

![](images/d84ebfc803971125fe61cfa78db286a5a6b38f9e0462a331ab67393ce3f31e9d.jpg)  
Figure 6-2. PACF of the untransformed orders time series pictured in Figure 6-1.

We can see that the value of the PACF crosses the $5 \%$ significance threshold at lag 3. This is consistent with the results from the ar() function available in R’s stats

package. ar() automatically chooses the order of an autoregressive model if one is not specified:

R  
>fit<-ar(demand["Banking orders (2)"],method $=$ "mle")  
>fit  
Call:  
ar $\mathbf{\Phi}_{\mathbf{x}} =$ demand["Banking orders (2)]],method $=$ "mle")  
Coefficients:1 2 3-0.1360 -0.2014 -0.3175

If we look at the documentation for the ar() function, we can see that the order selected is determined (with the default parameters we left undisturbed) based on the Akaike information criterion (AIC). This is helpful to know because it shows that the visual selection we made by examining the PACF is consistent with the selection that would be made by minimizing an information criterion. These are two different ways of selecting the order of the model, but in this case they are consistent.

# Akaike Information Criterion

The AIC of a model is equal to $\mathrm { A I C } = 2 k - 2 \mathrm { l n } L$ where $k$ is the number of parameters of the model and $L$ is the maximum likelihood value for that function. In general, we want to lessen the complexity of the model (i.e., lessen $k$ ) while increasing the likeli‐ hood/goodness-of-fit of the model (i.e., L). So we will favor models with smaller AIC values over those with greater AIC values.

A likelihood function is a measure of how likely a particular set of parameters for a function is relative to other parameters for that function given the data. So imagine you’re fitting a linear regression of $x$ on y of the following data:

```txt
x y 1 1 2 2 3 3 
```

If you were fitting this to the model $y = b \times x ,$ your likelihood function would tell you that an estimate of $b = 1$ was far more likely than an estimate of $b = 0$ . You can think of likelihood functions as a tool for helping you identify the most likely true parame‐ ters of a model given a set of data.

Notice that the ar() function has also provided us with the coefficients for the model. We may, however, want to limit the coefficients. For example, looking at the PACF, we might wonder whether we really want to include a coefficient for the lag – 1 term

or whether we should assign that term a mandatory coefficient of 0 given that its PACF value is well below the threshold used for significance. In this case, we can use the arima() function also from the stats package.

Here, we demonstrate how to call the function to fit an AR(3), by setting the order parameter to $\mathsf { c } ( 3 , \ \Theta , \ \Theta )$ , where 3 refers to the order of the AR component (in later examples we will specify other components for the differencing and moving average parameters covered in the next few pages of this chapter):

```python
## R
> est <- arima(x = demand[['Banking orders (2)'], order = c(3, 0, 0))
> est
Call:
arima(x = demand[['Banking orders (2)'], order = c(3, 0, 0))
Coefficients:
ar1 ar2 ar3 intercept
-0.1358 -0.2013 -0.3176 79075.350
s.e. 0.1299 0.1289 0.1296 2981.125
sigma^2 estimated as 1.414e+09: log likelihood = -717.42, aic = 1444.83 
```

To inject prior knowledge or opinion into our model, we can constraint a coefficient to be 0. For example, if we want to constrain the lag – 1 term to remain 0 in our model, we use the following call:

```txt
## R
> est.1 <- arima(x = demand[['Banking orders (2)'], 
> order = c(3, 0, 0),
> fixed = c(0, NA, NA, NA))
> est.1 
```

```python
Call:  
arima(x = demand[["Banking orders (2)"], order = c(3, 0, 0), fixed = c(0, NA, NA, NA)) 
```

Coefficients: ar1 ar2 ar3 intercept 0 -0.1831 -0.3031 79190.705 s.e. 0 0.1289 0.1298 3345.253 sigma^2 estimated as $1.44\mathrm{e} + 09$ : log likelihood $\equiv$ -717.96, aic $= 1443.91$

Setting a value in the vector passed to the fixed parameter of the arima function to 0 rather than NA will constraint that value to remain 0:

```txt
R  
> fixed <- c(θ, NA, NA, NA) 
```

We now inspect our model performance on our training data to assess the goodness of fit of our model to this data set. We can do this in two ways. First, we plot the ACF of the residuals (that, is the errors) to see if there is a pattern of self-correlation that our model does not cover.

Plotting the residuals is quite simple thanks to the output of the arima() function (see Figure 6-3):

```txt
R
>acf(est.1\$residuals) 
```

![](images/0a2219f8c07d6c1cbf8417663183d3c46a8bc27c13b6a254a02eb6fcbddd2b82.jpg)  
Figure 6-3. ACF of the residuals of the AR(3) model we fit forcing the lag – 1 parameter to 0.

None of the values of the ACF cross the significance threshold. Of course, we should not blindly yield to a significance threshold to assess or reject significance, but this observation is a helpful data point in a model that we already believe reasonable for other reasons.

We do not see a pattern of self-correlation here among the residuals (i.e., the error terms). If we had seen such a pattern, we would likely want to return to our original model and consider including additional terms to add complexity to account for the significant autocorrelation of the residuals.

Another test that is commonly performed is the Ljung-Box test, an overall test of the randomness of a time series. More formally it poses the following null and alternate hypotheses:

• H0: The data does not exhibit serial correlation.   
• H1: The data does exhibit serial correlation.

This test is commonly applied to AR (and more generally, ARIMA) models, and more specifically to the residuals from the model fit rather than to the model itself:

```txt
## R
> Box.test(est.1$residuals, lag = 10, type = "Ljung", fitdf = 3)
Box-Ljung test
data:
  est.1$residuals
X-squared = 9.3261, df = 7, p-value = 0.2301 
```

We apply the Ljung-Box test to our est.1 model to assess its goodness of fit. We can‐ not reject the null hypothesis that the data does not exhibit serial correlation. This is confirmation of what we just found by plotting the ACF of the residuals.

# Forecasting with an AR(p) process

In the following sections we will illustrate how to make forecasts with AR processes. We first explore the case of one time step ahead and then discuss how predicting multiple steps ahead differs from the further case. The good news is that from a cod‐ ing perspective there is not much difference, although the underlying mathematics is more elaborate in the latter case.

Forecasting one time step ahead. We first consider the case that we want to forecast one time step ahead with a known (or estimated) AR model. In such a case, we actually have all the information we need.

We continue working with the model from the demand data, with the lag – 1 coeffi‐ cient constrained to 0 (fit as est.1 earlier).

We plot the forecast using the fitted() function from the forecast package. Here is the code in full; it’s quite easy to do:

```txt
## R
> require( forecast )
> plot(demand[ ["Banking orders (2)" ], type = 'l' )
> lines(fitted(est.1), col = 3, lwd = 2) ## use the forecast package 
```

This results in the plot shown in Figure 6-4.

![](images/8739a9b0ae488888703a3f842ec3c4ba2e7de87bd029bb4b5b3f4971a352d784.jpg)  
Figure 6-4. Here we see the original time series in the solid line and the fitted time series in the dashed line.

![](images/acc094639784b7769d882367f84e499c8b9dc98dd666357186bc713e04a57904.jpg)

# AR(p) Models Are Moving Window Functions

We have an option beyond using the forecast package’s fitted functionality to produce a forecast from our AR(3) model: we could handcode a forecast more explicitly by using the zoo pack‐ age’s rollapply() function, which we discussed previously. That function to compute window functions can also compute an AR process. To do this we would take the coefficients from the ar() fit and apply these weights to an input vector representing the values of the different lags to produce a forecast at each point. I leave this as an exercise for you.

Now let’s think about the quality of the forecast. If we calculate the correlation between the predicted value and the actual value, we get 0.29. This is not bad in some contexts, but remember that sometimes differencing the data will remove what seemed like a strong relationship and replace it with one that is essentially random.

This will be the case particularly if the data was not truly stationary when we fit it, so that an unidentified trend masquerades as good model performance when it is actually a trait of the data we should have addressed before modeling.

We can difference both the series and the predicted values to see whether the change from one time period to the next is well predicted by the model. Even after differenc‐ ing, our predictions and the data show similar patterns, suggesting our model is a meaningful one (see Figure 6-5).

![](images/65742d2be77b0553337b3be19ede66a339f4938f7f569982fdb1264dc678bcf6.jpg)  
Figure 6-5. The differenced series and the differenced predictions are strongly correla‐ ted, suggesting an underlying relationship is indeed identified by the model.

We can also test whether we predict the same moves at the same time, by plotting the differenced series and looking at that correlation. This plot reveals some correlation, which we can confirm by calculating the correlation value. The model works even for predicting the change from one time step to the next.

Looking back at the original plot of the forecast versus actual values, we see that the main difference between the forecast and the data is that the forecast is less variable than the data. It may predict the direction of the future correctly, but not the scale of the change from one time period to another. This is not a problem per se but rather reflects the fact that forecasts are means of the predicted distributions and so neces‐ sarily will have lower variability than sampled data.

This property of statistical models is sometimes forgotten in the ease of fast visualiza‐ tions that tend to suggest a more stable future than will usually be the case. When you present visualizations, make sure to remind your audience of what is being plotted. In this case, our forecasts suggest a much smoother future than is likely to be the case.

Forecasting many steps into the future. So far we have done a single-step-ahead fore‐ cast. However, we may want to predict even further into the future. Let’s imagine that

we wanted to produce a two-step-ahead forecast instead of a one-step-ahead forecast. What we would do is first produce the one-step-ahead forecast, and then use this to furnish the $\gamma _ { t }$ value we need to predict $\boldsymbol { y } _ { t + 1 }$ .

Notice that in our current model, the shift from a one-step-ahead forecast to a twostep-ahead forecast would actually not require these gymnastics because the $\boldsymbol { \ y } _ { t \mathrm { ~ - ~ } 1 }$ value is not used when predicting $y _ { t } .$ Everything we need to know to make a predic‐ tion two time steps ahead is known and there is no need to guesstimate. In fact, we should come up with the same series of values as we did for the one-step-ahead pre‐ diction—there will be no new sources of error or variability.

If we want to predict even further, however, we will need to generate forecasted future values as inputs into our prediction. Let’s predict $\gamma _ { t + 3 }$ . This will involve a model with coefficients depending on $\boldsymbol { y } _ { t + 1 }$ and $\gamma _ { t }$ . So we will need to predict both of these values— $\cdot y _ { t + 1 }$ and $\gamma _ { t }$ —and then in turn use these estimated values in the prediction for $\boldsymbol { y } _ { t + 3 }$ . As before, we can use the fitted() function for the forecast package to do this —it’s no more difficult codewise than the one-step-ahead forecast. As mentioned previously, this can also be done with a rollapply() method, but it’s more work and more error prone.

We use the fitted() function with an additional parameter now, h for horizon. As a reminder, our object est.1 represents an AR(3) process with the lag – 1 (time minus 1) coefficient constrained to be 0:

```txt
R  
> fitted(est.1, h = 3) 
```

We can use the ease of predicting many time steps into the future to generate many multi-step-ahead forecasts for different horizons. In the next example, we can see the variance for forecasts made increasingly far into the future from the same underlying model. (Note that in the display, rounding and comma delineations modify the origi‐ nal output so it’s easier to see what is happening with the variance of the estimation as the forward horizon increases.)

As you can see in Figure 6-6, the variance of the prediction decreases with increasing forward horizon. The reason for this—which highlights an important limitation of the model—is that the further forward in time we go, the less the actual data matters because the coefficients for input data look only at a finite previous set of time points (in this model, going back only to lag – 3; i.e., time – 3). One way of putting this is that forecasts further out in time converge to being the unconditional prediction— that is, unconditioned on data. The future prediction approaches the mean value of the series as the time horizon grows, and hence the variance of both the error term and of the forecast values shrinks to 0 as the forecast values tend toward the uncondi‐ tional mean value:

```r
## R
> var(fitted(est.1, h = 3), na.rm = TRUE) 
```

```txt
[1] 174,870,141  
> var(fitted(est.1, h = 5), na.rm = TRUE)  
[1] 32,323,722  
> var(fitted(est.1, h = 10), na.rm = TRUE)  
[1] 1,013,396  
> var(fitted(est.1, h = 20), na.rm = TRUE)  
[1] 1,176  
> var(fitted(est.1, h = 30), na.rm = TRUE)  
[1] 3.5 
```

Forecasts for sufficiently far out in the future will merely predict the mean of the pro‐ cess, which makes sense. At some point in the distant future, our current data does not give us specific sensible information relevant to the future, and so our forecast increasingly reverts to the baseline properties known about the process, such as its mean.

![](images/7713d1584e338c2a47055276ace73ff5da586f4d108808ec17c993f4993cff9a.jpg)

![](images/437eb907e9a06fc3086f995c174fbb6d56b488a99d755fca0669cae67ef0cc6f.jpg)

![](images/31d41d9e11b7bc7b4b7c8cc336c801d69d71e0c6c01c1bed660e7c2e2a0ff36a.jpg)  
Figure 6-6. Plots of predictions into the future. The values spanned by the y-axis increasingly narrow as we predict further into the future and the model more closely appears to provide a constant prediction of the mean value of the process. The horizons predicted increase from top to bottom and are 3, 10, and 30 time steps, respectively.

The important point to remember from this is that AR (as well as MA, ARMA, and ARIMA, which will be discussed shortly) models are best for making short-term pre‐ dictions. These models lose predictive power for large future horizons.

For the remaining models, we will go through a similar treatment as we have given to the autoregressive models, albeit with less overall detail. A full discussion is available in all the standard textbooks on time series analysis.

# Moving Average Models

A moving average (MA) model relies on a picture of a process in which the value at each point in time is a function of the recent past value “error” terms, each of which is independent from the others. We will review this model in the same series of steps we used to study AR models.

![](images/fe8f6263f82c43bb5c1fa15c4d5652c2802dfcc4c760f02417f363099dbc4b50.jpg)

# AR MA Equivalence

In many cases, an MA process can be expressed as an infinite order AR process. Likewise, in many cases an AR process can be expressed as an infinite order MA process. To learn more, look into invertibility of an MA process, the Wold representation theo‐ rem, and the duality of MA/AR processes generally. The mathe‐ matics involved is well beyond the scope of this book!

# The model

A moving average model can be expressed similarly to an autoregressive model except that the terms included in the linear equation refer to present and past error terms rather than present and past values of the process itself. So an MA model of order $q$ is expressed as:

$$
y _ {t} = \mu + e _ {t} + \theta_ {1} \times e _ {t - 1} + \theta_ {2} \times e _ {t - 2} \dots + \theta_ {q} \times e _ {t - q}
$$

![](images/a0c93c37fb863cb82c36797a11e57b09bc713754a975a67c0c2026a775586898.jpg)

Do not confuse the MA model with a moving average. They are not the same thing. Once you know how to fit a moving average process, you can even compare the fit of an MA model to a moving average of the underlying time series. I leave this as an exercise for you.

Economists talk about these error terms as “shocks” to the system, while someone with an electrical engineering background could talk about this as a series of impulses and the model itself as a finite impulse response filter, meaning that the effects of any particular impulse remain only for a finite period of time. The wording is unimpor‐ tant, but the concept of many independent events at different past times affecting the

current value of the process, each making an individual contribution, is the main idea.

# The Backshift Operator

The backshift operator, a.k.a. the lag operator, operates on time series points and moves them back by one time step each time it is applied. In general:

$$
B ^ {k} y _ {t} = y _ {t - k}
$$

The backshift operator helps to simplify the expression of time series models. For example, an MA model can be rewritten as:

$$
y _ {t} = \mu + \left(1 + \theta_ {1} \times B + \theta_ {2} \times B ^ {2} + \dots + \theta_ {q} \times B ^ {q}\right) e _ {t}
$$

MA models are by definition weakly stationary without the need to impose any con‐ straints on their parameters. This is because the mean and variance of an MA process are both finite and invariant with time because the error terms are assumed to be iid with mean 0. We can see this as:

$$
\begin{array}{l} E \big (y _ {t} = \mu + e _ {t} + \theta_ {1} \times e _ {t - 1} + \theta_ {2} \times e _ {t - 2} \dots + \theta_ {q} \times e _ {t - q} \big) \\ = E (\mu) + \theta_ {1} \times 0 + \theta_ {2} \times 0 + \dots = \mu \\ \end{array}
$$

For calculating the variance of the process, we use the fact that the $e _ { t }$ terms are iid and also the general statistical property that the variance of the sum of two random vari‐ ables is the same as their individual variances plus two times their covariance. For iid variables the covariance is 0. This yields the expression:

$$
V a r \left(y _ {t}\right) = \left(1 + \theta_ {1} ^ {2} + \theta_ {2} ^ {2} + \dots . + \theta_ {q} ^ {2}\right) \times \sigma_ {e} ^ {2}
$$

So both the mean and variance of an MA process are constant with time regardless of the parameter values.

# Selecting parameters for an MA(q) process

We fit an MA model to the same data used fit to AR model, and we can use the ACF to determine the order of the MA process (see Figure 6-7). Before reading on, think about how an MA process works and see if you can reason out why we use the ACF rather than the PACF to determine the order of the process:

```txt
## R
> acf(demand [["Banking orders (2)"]] 
```

![](images/5d5d632fe742c8355c36fb74532b6f66e13f8f78ca4d05c0d63b369e35c028a5.jpg)  
Series demand[["Fiscal sector orders"]]   
Figure 6-7. We use the ACF of the demand time series to determine the order of the MA model.

![](images/a7792a34a4da184d7c03e92ffb6f12a597188dcd03a5758c457fbdbf51d10723.jpg)

# ACF and PACF Patterns Differ from MA and AR Processes

Unlike an autoregressive process, which has a slowly decaying ACF, the definition of the MA process ensures a sharp cutoff of the ACF for any value greater than $q$ , the order of the MA process. This is because an autoregressive process depends on previous terms, and they incorporate previous impulses to the system, whereas an MA model, incorporating the impulses directly through their value, has a mechanism to stop the impulse propaga‐ tion from progressing indefinitely.

We see significant values at lags 3 and 9, so we fit an MA model with these lags. We need to be careful that we don’t accidentally constrain the wrong coefficients to 0 in the model, which we can confirm by printing the display:

```python
## R
> ma.est = arima(x = demand [["Banking orders (2)"], order = c(0, 0, 9), fixed = c(0, 0, NA, rep(0, 5), NA, NA))
> ma.est
Call:
arima(x = demand [["Banking orders (2)"]), order = c(0, 0, 9), 
```

fixed = c(0, 0, NA, rep(0, 5), NA, NA))

Coefficients:   

<table><tr><td></td><td>ma1</td><td>ma2</td><td>ma3</td><td>ma4</td><td>ma5</td><td>ma6</td><td>ma7</td><td>ma8</td><td>ma9</td><td>intercept</td></tr><tr><td></td><td>0</td><td>0</td><td>-0.4725</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-0.0120</td><td>79689.81</td></tr><tr><td>s.e.</td><td>0</td><td>0</td><td>0.1459</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.1444</td><td>2674.60</td></tr><tr><td colspan="11">sigma^2 estimated as 1.4e+09: log likelihood = -717.31, aic = 1442.61</td></tr></table>

We should also check our fit, as we did for the AR model, by plotting the ACF of the residuals of the model and, as a second, separate test of model performance, running the Ljung-Box test to check for overall randomness in any fit to the residuals. Note that the Box.test() input requires us to specify the number of degrees of freedom— that is, how many model parameters were free to be estimated rather than being con‐ strained to a specific value. In this case, the free parameters were the intercept as well as the MA3 and MA9 terms:

```txt
## R
> Box.test(ma.est$residuals, lag = 10, type = "Ljung", fitdf = 3)
Box-Ljung test
data:
  ma.est$residuals
X-squared = 7.6516, df = 7, p-value = 0.3643 
```

We cannot reject the null hypothesis that there is no temporal correlation between residual points. Likewise, a plot of the ACF of the residuals suggests no temporal cor‐ relation (this is left as an exercise for the reader).

# Forecasting an MA(q) process

We can generate a forecast again using the techniques shown earlier for an AR pro‐ cess, relying on the fitted() method of the forecast package:

```txt
## R
> fitted(ma.est, h=1)
Time Series:
Start = 1
End = 60
Frequency = 1
[1] 90116.64 80626.91 74090.45 38321.61 74734.77 101153.20 65930.90
[8] 106351.80 104138.05 86938.99 102868.16 80502.02 81466.01 77619.15
[15] 100984.93 81463.10 61622.54 79660.81 88563.91 65370.99 104679.89
[22] 48047.39 73070.29 115034.16 80034.03 70052.29 70728.85 90437.86
[29] 80684.44 91533.59 101668.18 42273.27 93055.40 68187.65 75863.50
[36] 40195.15 82368.91 90605.60 69924.83 54032.55 90866.20 85839.41
[43] 64932.70 43030.64 85575.32 76561.14 82047.95 95683.35 66553.13
[50] 89532.20 85102.64 80937.97 93926.74 47468.84 75223.67 100887.60
[57] 92059.32 84459.85 67112.16 80917.23 
```

MA models exhibit strong mean reversion and so forecasts rapidly converge to the mean of the process. This makes sense given that the process is considered to be a function of white noise.

If you forecast beyond the range of the model established by its order, the forecast will necessarily be the mean of the process by definition of the process. Consider an MA(1) model:

$$
y _ {t} = \mu + \theta_ {1} \times e _ {t - 1} + e _ {t}
$$

To predict one time step in the future, our estimate for $\boldsymbol { y } _ { t + 1 }$ is $\mu + \theta _ { \mathrm { { l } } } \times y _ { t } + e _ { t }$ . If we want to predict two time steps in the future, our estimate is:

$$
E \left(y _ {t + 2} = \mu + e _ {t + 2} + \theta_ {1} \times ; e _ {t + 1}\right) = \mu + 0 + \theta_ {1} \times 0 = \mu
$$

With an MA(1) process we cannot offer an informed prediction beyond one step ahead, and for an $\mathrm { M A } ( q )$ process in general we cannot offer a more informed predic‐ tion beyond $q$ steps than the mean value emitted by the process. By informed predic‐ tion, I mean one in which our most recent measurements have an impact on the forecast.

![](images/d284c671799a79eece6fe60ca05354df08591f2328c6b4b0b761997b4cf994a3.jpg)

# Traditional Notation Is Negative

Note that the MA model is not typically written as it was here. Tra‐ ditionally, the sign in front of the theta coefficients is negative. This is for reasons of how it can be derived, and one way to think about an MA model is as an AR model with constraints on its parame‐ ters. This formulation, with a lot of algebra, leads to negative coef‐ ficients on the theta.

We can see this by producing predictions with our MA(9) model that we just fit, and for which we now seek predictions 10 time steps ahead:

```txt
R  
> fitted(ma.est, h=10)  
Time Series:  
Start = 1  
End = 60  
Frequency = 1  
[1] NA NA NA NA NA NA NA  
[9] NA NA 79689.81 79689.81 79689.81 79689.81 79689.81 79689.81  
[17] 79689.81 79689.81 79689.81 79689.81 79689.81 79689.81  
[25] 79689.81 79689.81 79689.81 79689.81 79689.81 79689.81  
[33] 79689.81 79689.81 79689.81 79689.81 79689.81 79689.81  
[41] 79689.81 79689.81 79689.81 79689.81 79689.81 79689.81 
```

[49] 79689.81 79689.81 79689.81 79689.81 79689.81 79689.81 79689.81 79689.81   
[57] 79689.81 79689.81 79689.81 79689.81

When we attempt to predict 10 time steps into the future, we predict the mean for every time step. We could have done that without a fancy statistical model!

Common sense is important. If you apply a model without understanding how it works, you can do embarrassing things such as sending your boss the exact same forecast every day after you wasted previous time and computing resources chugging out a model inappropriate to the question you are asking.

# Autoregressive Integrated Moving Average Models

Now we that we have examined AR and MA models individually, we look to the Autoregressive Integrated Moving Average (ARIMA) model, which combines these, recognizing that the same time series can have both underlying AR and MA model dynamics. This alone would lead us to an ARMA model, but we extend to the ARIMA model, which accounts for differencing, a way of removing trends and ren‐ dering a time series stationary.

# What Is Differencing?

As discussed earlier in the book, differencing is converting a time series of values into a time series of changes in values over time. Most often this is done by calculating pairwise differences of adjacent points in time, so that the value of the differenced series at a time $t$ is the value at time t minus the value at time t – 1. However, differ‐ encing can also be performed on different lag windows, as convenient.

ARIMA models continue to deliver near state-of-the-art performance, particularly in cases of small data sets where more sophisticated machine learning or deep learning models are not at their best. However, even ARIMA models pose the danger of over‐ fitting despite their relative simplicity.

# The model

You are probably scratching your head at this point if you’ve been paying attention because I’ve just fit the same data to both an AR and an MA process without com‐ menting on it. This is the sort of irritating habit you may sometimes run into in time series analysis textbooks. Some authors will cop to this data laziness, while others will blandly ignore it. We have not investigated in depth whether either of our previous models is a particularly good fit, but it does seem clear from the fitting process we used that there are defensible arguments to describing the data with either an AR or an MA model. This raises the question: might it not be helpful to incorporate both behaviors into the same model?

Table 6-1 can be a handy way of examining a time series process to see whether an AR, MA, or ARMA description of the process is best.

Table 6-1. Determining which model best describes our time series   

<table><tr><td>Kind of plot</td><td>AR(p)</td><td>MA(q)</td><td>ARMA</td></tr><tr><td>ACF behavior</td><td>Falls off slowly</td><td>Sharp drop after lag = q</td><td>No sharp cutoff</td></tr><tr><td>PACF behavior</td><td>Sharp drop after lag = p</td><td>Falls off slowly</td><td>No sharp cutoff</td></tr></table>

This brings us to an autoregressive moving average (ARMA) model, which is applied in the case that neither AR nor MA terms alone sufficiently describe the empirical dynamics. This is a likely case when diagnostics for AR and MA order statistics (PACF and ACF, respectively) point to nonzero values, indicating a term of a given order for either an AR or an MA term. These can be combined with an ARMA model.

![](images/cb6c62ae57364dcdb7006b2ae18310436345b9cb4c21d536bf60ccdf370e5ffb.jpg)

# Wold’s Theorem

Wold’s theorem tells us that every covariance-stationary time series can be written as the sum of two time series, one deterministic and one stochastic. Based on this theorem, we can also say that a sta‐ tionary process can be reasonably approximated by an ARMA model, although it can of course be quite difficult to find the appropriate model.

Here we switch to more traditional statistics notation by applying negative signs to the MA process coefficients:

$$
y _ {t} = \phi_ {0} + \Sigma (\phi_ {i} \times ; r _ {t - i}) + e _ {t} - \Sigma (\theta_ {i} \times e _ {t - i})
$$

# An ARMA Model Is Not Necessarily Unique

Because there can be common factors between the AR and MA portions of the equa‐ tion it’s possible that an $\operatorname { A R M A } ( p , q )$ model could actually be reduced to another set of parameters. We need to avoid this sort of degenerate situation. In general, you must choose a parsimonious ARMA model. Further discussion is beyond the scope of this book, but additional references are provided at the end of the chapter.

One simple method of identifying cases where this may happen is to use the lag oper‐ ator (discussed before) formulation of the ARMA model and use good old-fashioned polynomial factorization to look for common factors in your AR and MA models that could divide out to yield an equivalent but simpler model. Here’s an example. One way of expressing an ARMA model is to put the AR components on one side and the MA components on another side.

Suppose we have the following ARMA model:

$$
y _ {t} = 0. 5 \times y _ {t - 1} + 0. 2 4 \times y _ {t - 2} + e _ {t} + 0. 6 \times e _ {t - 1} + 0. 0 9 \times e _ {t - 2}
$$

We can rewrite this in a factored form by moving the AR terms (the $x$ coefficients) to one side and leaving the MA terms on the other side:

$$
y _ {t} - 0. 5 \times y _ {t - 1} - 0. 2 4 \times y _ {t - 2} = e _ {t} + 0. 6 \times e _ {t - 1} + 0. 0 9 \times e _ {t - 2}
$$

We then re-express this using the lag operator. So, for example, $y _ { t - 2 }$ is re-expressed as $L ^ { 2 } y$ :

$$
y _ {t} - 0. 5 \times L \times y _ {t} - 0. 2 4 \times L ^ {2} \times y _ {t} = e _ {t} + 0. 6 \times L \times e _ {t} + 0. 0 9 \times L ^ {2} \times e _ {t}
$$

Or simplifying by factoring out the common factors of $\gamma _ { t }$ on the lefthand side and $e _ { t }$ on the righthand side, and then factoring the polynomials, we have:

$$
(1 + 0. 3 L) \times (1 - 0. 8 L) \times y _ {t} = (1 + 0. 3 L) \times (1 + 0. 3 L) \times e _ {t}
$$

Notice that when factoring we must keep the values operated on by the lag operator to the right of it so that we factor out $\gamma _ { t }$ to the right of the polynomial factors involv‐ ing the $L$ backshift operator rather than the left as may feel more natural.

Once we have done this factoring, we have a common factor on the lefthand side and righthand side, namely $\left( 1 + 0 . 3 L \right)$ , and we can cancel this out from both sides, leaving us with:

$$
(1 - 0. 8 L) \times y _ {t} = (1 + 0. 3 L) \times e _ {t}
$$

Post factoring, the order of the overall model has been reduced and now involves only values at time t – 1 rather than also values at t – 2. This should serve as a warn‐ ing that even a model that doesn’t seem too complicated, such as an ARMA(2,2), can sometimes mask the fact that it is actually equivalent to a simpler model form.

It is also good to keep this in mind when computationally fitting models directly. In real-world data sets you may end up with something such that the two sides are quite similar numerically, but not exact. Perhaps on the left side you can find a factor such as $\left( 1 - 0 . 2 9 L \right)$ whereas on the right side it is $\left( 1 - 0 . 3 3 L \right)$ . In such cases it is worth con‐ sidering that this is close enough to a common factor to drop for model complexity.

The stationarity of the ARMA process comes down to the stationarity of its AR com‐ ponent and is controlled by the same characteristic equation that controls whether an AR model is stationary.

From an ARMA model, it is a simple transition to an ARIMA model. The difference between an ARMA model and an ARIMA model is that the ARIMA model includes the term integrated, which refers to how many times the modeled time series must be differenced to produce stationarity.

ARIMA models are far more widely deployed in practice, particularly in academic research and forecasting problems, than are AR, MA, and ARMA models. A quick Google scholar search reveals ARIMA applied to a variety of forecasting problems including:

• Inbound air travel passengers to Taiwan   
• Energy demand in Turkey by fuel type   
• Daily sales in a wholesale vegetable market in India   
• Emergency room demand in the western United States

Importantly, the order of differencing should not be too great. In general, the value of each parameter of an $\mathrm { A R I M A } ( p , d , q )$ model should be kept as small as possible to avoid unwarranted complexity and overfitting to the sample data. As a not-at-alluniversal rule of thumb, you should be quite skeptical of values of $d$ over 2 and values of $\boldsymbol { p }$ and $q$ over 5 or so. Also, you should expect either the $\boldsymbol { p }$ or $q$ term to dominate and the other to be relatively small. These are practitioner notes gathered from ana‐ lysts and not hard-and-fast mathematical truths.

# Selecting parameters

The ARIMA model is specified in terms of the parameters $( p , d , q )$ . We select the val‐ ues of $p , d ,$ and $q$ that are appropriate given the data we have.

Here are some well-known examples from the Wikipedia description of ARIMA models:

• ARIMA(0, 0, 0) is a white noise model.   
• ARIMA(0, 1, 0) is a random walk, and ARIMA(0, 1, 0) with a nonzero constant is a random walk with drift.   
• ARIMA(0, 1, 1) is an exponential smoothing model, and an ARIMA(0, 2, 2) is the same as Holt’s linear method, which extends exponential smoothing to data with a trend, so that it can be used to forecast data with an underlying trend.

We choose the order of our model based on a combination of domain knowledge, various fitting evaluation metrics (such as the AIC), and general knowledge of how

the PACF and ACF should appear for a given underlying process (as described in Table 6-1). Next we will demonstrate fitting an ARIMA model to a process using both a manual iterative process based on the PACF and ACF and also based on an automated parameter selection tool via the forecast package’s auto.arima() function.

Manually fitting a model. There are heuristics for choosing the parameters of an ARIMA model, where parsimony is paramount. One popular and longstanding method is the Box-Jenkins method, which is an iterative multistep process:

1. Use your data, visualizations, and underlying knowledge to select a class of model appropriate to your data.   
2. Estimate the parameters given your training data.   
3. Evaluate the performance of your model based on your training data and tweak the parameters of the model to address the weaknesses you see in the perfor‐ mance diagnostics.

Let’s work through one example of fitting data. First, we need some data. In this case, for purposes of transparency and knowing the right answer, we generate our data from an ARMA process:

```txt
## R
> require_forecast
> set.seed(1017)
> ## order of arima model hidden on purpose
> y = arima.sim(n = 1000, list(ar = True, ma = True)) 
```

Try not to look at the order of the model created just yet; let’s focus on treating this as a mystery. First, we should plot the time series, as we always do, to see whether it appears stationary (Figure 6-8). Then we examine the ACF and PACF of $\boldsymbol { y }$ (Figure 6-9) and compare it with Table 6-1.

![](images/7074b506a2e1d3346b61b24c23a639755a6d68dca2a595b81fdc791395acf265.jpg)  
Figure 6-8. Plot of our time series.

![](images/23503d71e2a9fc247480c6a46e515a2f37c2e1d2bce1563a19b4213eb292ab11.jpg)

![](images/f8c6488c1752ebc2ba3795fff58f1549a1abbf597f5208e2aaa7e80bd5762fdf.jpg)  
Figure 6-9. ACF and PACF of our time series.

We can see that neither the ACF nor the PACF appears to have a sharp cutoff, sug‐ gesting (see Table 6-1) that this is an ARMA process. We start by fitting a relatively simple ARIMA(1, 0, 1) model, as we see no need for differencing and no evidence (Figure 6-10):

```r
## R
> ar1.ma1.model = Arima(y, order = c(1, 0, 1))
> par(mfrow = c(2, 1))
> acf(ar1.ma1.model\$residuals)
> pacf(ar1.ma1.model\$residuals) 
```

![](images/3073e806943879b8dc2c7974d5816760933a4e91fee071bb58b0158a169396e3.jpg)

![](images/1dab98feb3bdb0a23a912b92d5b3cf29e2f94270e2bfeb94c8325ec366f774ee.jpg)  
Figure 6-10. ACF and PACF of the residuals of an ARIMA(1, 0, 1) model.

The residuals in Figure 6-10 show particularly large PACF values, suggesting that we have not fully described the autoregressive behavior. For this reason, we build out the model by adding a higher-order AR component, testing an ARIMA(2, 0, 1) model in the following code, and then plotting the ACF and PACF of the residuals of this more complex model (Figure 6-11):

```r
## R
> ar2.ma1.model = Arima(y, order = c(2, 0, 1))
> plot(y, type = 'l')
> lines(ar2.ma1.model$fitted, col = 2)
> plot(y, ar2.ma1.model$fitted)
> par(mfrow = c(2, 1))
> acf(ar2.ma1.model$residuals)
> pacf(ar2.ma1.model$residuals) 
```

![](images/e68b179271e3f7b3885d45535f640d9e0261348800e5aaa1fae3c3491ee155ff.jpg)  
Series ar2.ma1.model$residuals

![](images/96daf23611d684ae883559f9f79057682f9cdc52e814236b15f4fcef5f85dd4f.jpg)  
Series ar2.ma1.model$residuals   
Figure 6-11. ACF and PACF of the residuals of an ARIMA(2, 0, 1) model.

The residuals in Figure 6-11 no longer show large values for either the ACF or the PACF. Given our desire for a parsimonious model and the dangers of overfitting an ARIMA model, a wise analyst would likely stop here, not seeing any further behavior in the residuals that needs to be fit via an autoregressive, moving average, or differ‐ encing component. It is left as an exercise for the reader to consider fitting more complicated models. Though the plots are not shown here, I attempted with the fol‐ lowing code to fit more complex models. What I saw was that they did not substan‐ tially improve the fit of the model to the data, nor did they contribute to diminishing the ACF or PACF values any further than the previous model. You can verify this for yourself:

```r
## R
> ar2.ma2.model = Arima(y, order = c(2, 0, 2))
> plot(y, type = 'l')
> lines(ar2.ma2.model$fitted, col = 2)
> plot(y, ar2.ma2.model$fitted)
> par(mfrow = c(2,1))
> acf(ar2.ma2.model$residuals)
> pacf(ar2.ma2.model$residuals)
>
> ar2.d1.ma2.model = Arima(y, order = c(2, 1, 2))
> plot(y, type = 'l')
> lines(ar2.d1.ma2.model$fitted, col = 2)
> plot(y, ar2.d1.ma2.model$fitted)
> par(mfrow = c(2,1))
> acf(ar2.d1.ma2.model$residuals)
> pacf(ar2.d1.ma2.model$residuals) 
```

One quick way of comparing the models is shown here, where I look at how well the forecasts from a fitted model correlate to the actual values:

```markdown
## R
> cor(y, ar1.ma1.model$fitted)
[1] 0.3018926
> cor(y, ar2.ma1.model$fitted)
[1] 0.4683598
> cor(y, ar2.ma2.model$fitted)
[1] 0.4684905
> cor(y, ar2.d1.ma2.model$fitted)
[1] 0.4688166 
```

We see a substantial improvement transitioning from an ARIMA(1, 0, 1) model to an ARIMA(2, 0, 1) model (the first two models), as the correlation goes from 0.3 to 0.47. On the other hand, as we add more complexity, we do not see substantial improve‐ ments in the correlation. This offers additional support for the conclusion we drew earlier that the ARIMA(2, 0, 1) model seemed to describe the model behavior well and that there was no need to add further AR or MA components to improve the fit.

Incidentally, we can see how well the fit did below, by comparing the original fit coef‐ ficients (shown here, though obscured before) with the fit coefficients:

```txt
## R
## original coefficients
> y = arima.sim(n = 1000, list(ar = c(0.8, -0.4), ma = c(-0.7)))
> ar2.ma1.model$coef
ar1 ar2 ma1 intercept
0.785028320 -0.462287054 -0.612708282 -0.005227573 
```

There is a good match between the fitted coefficients and the actual coefficients used to simulate the data.

There is more to fitting an ARIMA model manually than we have demonstrated here. Over decades, practitioners have developed good rules of thumb for identifying prob‐ lems such as when there are too many terms of a certain kind, when a model has been overdifferenced, when the pattern in residuals points to certain problems, and so on. One good starting resource is a guide written by a Penn State professor available online.

There are some legitimate criticisms of fitting an ARIMA process “by hand.” Fitting by hand can be a somewhat poorly specified process and puts a lot of pressure on the analyst’s discretion, and can be time-consuming and path-dependent as to the ulti‐ mate outcome. It’s a good solution that has worked well for decades to produce actual forecasts used in the real world, but it’s not perfect.

Using automated model fitting. Nowadays we can get away from a manually iterative fitting process in favor of an automated model selection in some cases. Our model

selection can be driven according to various information loss criteria, such as the AIC we discussed briefly via the auto.arima() function in the forecast package:

```python
## R
> est = auto.arima(demand [["Banking orders (2)"], stepwise = FALSE, ## this goes slower
## but gets us a more complete search
max.p = 3, max.q = 9)
> est
Series: demand [["Banking orders (2)"]
ARIMA(0,0,3) with non-zero mean
Coefficients:
ma1 ma2 ma3 mean
-0.0645 -0.1144 -0.4796 79914.783
s.e. 0.1327 0.1150 0.1915 1897.407
sigma^2 estimated as 1.467e+09: log likelihood=-716.71
AIC=1443.42 AICc=1444.53 BIC=1453.89 
```

In this case we put a one-liner with some prior knowledge from our earlier explora‐ tion into configuring the inputs. Namely, we specified the maximum order of the AR and MA process that we were prepared to accept, and in fact the model selection chose a much more parsimonious model than we specified, including no AR terms at all. Nonetheless, this model fits well, and we shouldn’t data-snoop too much trying to beat it unless we have a principled reason for doing so. Note that according to the AIC criteria, our hand-chosen MA model in the previous section does just slightly better than this model, but the difference does not seem meaningful when we look at plots.

Left for an exercise for the reader is plotting the model against the predictions as well as checking the residuals of this automatically selected model and confirming that the model used does not appear to leave any residual behavior that needs to be addressed by the addition of more terms. The code is no different from what we have used in the previous AR and MA sections. This is also true for the case of making forecasts, and so that is omitted here given that the ARIMA model we selected is not very dif‐ ferent from the MA model discussed and used for a forecast earlier.

We can also quickly study how auto.arima() would have performed on the model we generated in the previous section when we fit the model by hand:

```txt
R  
> auto.model = auto.arima(y)  
> auto.model  
Series: y  
ARIMA(2,0,1) with zero mean  
Coefficients:  
ar1 ar2 ma1  
0.7847 -0.4622 -0.6123 
```

s.e. 0.0487 0.0285 0.0522

sigma^2 estimated as 1.019: log likelihood=-1427.21

AIC=2862.41 AICc=2862.45 BIC=2882.04

We did not even use the optional parameters to suggest to auto.arima() where it should start its model search, and yet it converged to the same solution as we did. So, as we can see here, in some cases we will find the same solution with different meth‐ odologies. We drove our analysis by looking at the ACF and PACF of the residuals of simpler models to build more complex models, whereas auto.arima() is largely driven by a grid search to minimize the AIC. Of course, given that we generated the original data from an ARIMA process, this represents a more straightforward case than much real-world data. In the latter case, it won’t always be true that our manual fits and the automated model selection come to the same conclusion.

If you are going to make auto.arima() or a similar automated model selection tool an important part of your analysis, it’s important to read the documentation, experi‐ ment with synthetic data, and also read about other analysts’ experiences with this function. There are some known scenarios where the function will not perform as naively expected, and there are also known workarounds. Overall this is an excellent solution but not a perfect one.4 Also, for a good description of how auto.arima() works, take a look at the online textbook chapter on this topic written by the author of the function, Professor Rob Hyndman.

We have demonstrated two distinct ways of estimating parameters: either by follow‐ ing a Box-Jenkins approach to fitting the model or using an automated fit from the forecast package. In fact, practitioners have very strong opinions on this, with some fiercely advocating only in favor of manual iteration and others equally fiercely in favor of automated selection tools. This remains an ongoing debate in the field. In the long run, as big data takes over more and more of time series analysis, it is likely that automatic exploration and fitting of models will come to dominate large data set time series analysis.

# Vector Autoregression

In the real world, we are often lucky enough to have several time series in parallel that are presumably related to one another. We already examined how to clean and align such data, and now we can learn how to make maximal use of it. We can do so by generating an $\operatorname { A R } ( p )$ model to the case of multiple variables. The beauty of such a model is that it provides for the fact that variables both influence one another and are in turn influenced—that is, there is no privileged y while everything else is designated

as $x .$ . Instead, the fitting is symmetric with respect to all variables. Note that differenc‐ ing can be applied as is in other models previously if the series are not stationary.

![](images/a5b700d4cc6d34cf443bb8b6e0919dc2746cf5643a8cc815af41c6e0627cdfdc.jpg)

# Exogenous and Endogenous Variables

In statistical speak, once we employ a model where the variables influence one another, we call those variables endogenous, meaning their values are explained by what we see within the model. By con‐ trast, exogenous variables are those that are not explained within the model—that is, they cannot be explained by assumption—and so we accept their values and do not question the dynamics of how

Since every time series putatively predicts every other as well as itself, we will have they came to be. one equation per variable. Let’s say we have three time series: we will denote the value of these time series at time $t$ as $\boldsymbol { y } _ { 1 , \ t }$ and $y _ { 2 , \ t }$ and $\gamma _ { 3 , t } .$ Then we can write the vector autoregression (VAR) equations of order 2 (factoring in two time lags) as:

$$
y _ {1, t} = \phi_ {0 1} + \phi_ {1 1, 1} \times y _ {1, t - 1} + \phi_ {1 2, 1} \times y _ {2, t - 1} + \phi_ {1 3, 1} \times y _ {3, t - 1} + \phi_ {1 1, 2} \times y _ {1, t - 2} + \phi_ {1 2, 2} \times y _ {2, t - 2} + \phi_ {1 3, 2} \times y _ {3, t - 2}
$$

$$
y _ {2, t} = \phi_ {0 2} + \phi_ {2 1, 1} \times y _ {1, t - 1} + \phi_ {2 2, 1} \times y _ {2, t - 1} + \phi_ {2 3, 1} \times y _ {3, t - 1} + \phi_ {2 1, 2} \times y _ {1, t - 2} + \phi_ {2 2, 2} \times y _ {2, t - 2} + \phi_ {2 3, 2} \times y _ {3, t - 2}
$$

$$
y _ {1, t} = \phi_ {0 3} + \phi_ {3 1, 1} \times y _ {1, t - 1} + \phi_ {3 2, 1} \times y _ {2, t - 1} + \phi_ {3 3, 1} \times y _ {3, t - 1} + \phi_ {3 1, 2} \times y _ {1, t - 2} + \phi_ {3 2, 2} \times y _ {2, t - 2} + \phi_ {3 3, 2} \times y _ {3, t - 2}
$$

![](images/0067927f853a46a5aa9ea7b62fc816c1fac461ad3931aaef1a18a6b1814c1028.jpg)

# Matrix Multiplication

As you will have noticed if you are familiar with linear algebra, expressing the relationships shown in the three previous equations is much simpler when you are using matrix notation. In particular, you can write a VAR in a very similar manner to an AR. In the matrix form, the three equations could be expressed as:

$$
y = \phi_ {0} + \phi_ {1} \times y _ {t - 1} + \phi_ {2} \times y _ {t - 2}
$$

where the $y$ and $\phi _ { 0 }$ are $3 \times 1$ matrices, and the other $\phi$ matrices are $3 \times 3$ matrices.

Even for a simple case, you can see that the number of parameters in the model grows very quickly. For example, if we have $\boldsymbol { p }$ lags and $N$ variables, we can see that the pre‐ dictor equation for each variable is $1 + p \times N$ values. Since we have $N$ values to pre‐ dict, this translates to $N + p \times N ^ { 2 }$ total variables, meaning that the number of variables grows in $O \big ( N ^ { 2 } \big )$ proportion to the number of time series studied. Hence, we should not throw in time series gratuitously simply because we have the data but instead reserve this method for when we really expect a relationship.

VAR models are most often used in econometrics. They sometimes come under fire because they do not have any structure beyond the hypothesis that all values influ‐ ence one another. It is precisely for this reason that the model’s goodness of fit can be

difficult to assess. However, VAR models are still useful—for example, in testing whether one variable causes another variable. They are also useful for situations where a number of variables need to be forecasted and the analyst does not have domain knowledge to assert any particular kind of relationship. They can also some‐ times be helpful in determining how much variance in a forecast of a value is attrib‐ utable to its underlying “causes.”

Here’s a quick demonstration. Let’s look at the underlying UCI demand information and consider using a second column to predict Banking orders (2) rather than just its own data (note that we will also predict that column due to the symmetric way in which the variables are treated). Let’s consider using orders from the traffic control sector. That sounds like it should be quite different, so it may provide a fairly inde‐ pendent source of information relative to the fiscal sector’s own past orders. We can also imagine that each column offers underlying information about how the econ‐ omy is doing and whether demand will increase or decrease in the future.

To determine what parameters to use, we use the vars package, which comes with a VARselect() method:

```txt
## R
> VARselect(demand[, 11:12, with = FALSE], lag.max=4,
                    + type="const")
$selection
AIC(n) HQ(n) SC(n) FPE(n)
3 3 1 3
$criteria
1 2 3 4
AIC(n) 3.975854e+01 3.967373e+01 3.957496e+01 3.968281e+01
HQ(n) 3.984267e+01 3.981395e+01 3.977126e+01 3.993521e+01
SC(n) 3.997554e+01 4.003540e+01 4.008130e+01 4.033382e+01
FPE(n) 1.849280e+17 1.700189e+17 1.542863e+17 1.723729e+17 
```

We can see that the function provides a variety of information criteria to choose from. Also note that we indicated we wanted to fit a "const" term to accommodate a nonzero mean. We could also elect to fit a drift term or both or neither, but the "const" option seems the best fit for our data. Here we will start by looking at the three lags and see how that does:

```r
## R
> est.var <- VAR(demand[, 11:12, with = FALSE], p=3, type="const")
> est.var
> par(mfrow = c(2, 1))
> plot(demand\$'Banking orders (2)', type = "l")
> lines(fitted(est.var)[, 1], col = 2)
> plot(demand\$'Banking orders (3)', type = "l")
> lines(fitted(est.var)[, 2], col = 2) 
```

> par(mfrow = c(2, 1))  
> acf(demand\\(Banking orders (2) - fitted(est.var)[, 1])  
> acf(demand\\)Banking orders (3) -  
> fitted(est.var)[, 2])

This code produces the plots shown in Figures 6-12 and 6-13.

![](images/aea3a3190be12d77ede539052562e40e6f949621369f422e143940f2dbed0e8e.jpg)

![](images/325028fe0d9d622e0df5046e67fee2884e664c0871f2a4acaa841f82854172a4.jpg)  
Figure 6-12. At the top we see the actual (solid) versus predicted (dashed) values for Banking orders (2), and at the bottom we see the same for the Banking orders (3). Interestingly, the top plot looks more like a typical forecast, where the forecast is some‐ what “slow” in changing relative to the real data, whereas in the bottom plot we see the forecast actually predicted changes somewhat in advance of when they happen. This suggests that Banking orders (2) “lead” Banking orders (3), meaning that Banking orders (2) are helpful in predicting traffic controller orders, but possibly not the inverse, or at least not to such a degree.

![](images/b5714e0c195a020af0cc6d9957f193466b73e591e4ea2ce04956de4ed06f7053.jpg)

![](images/cc01f450fe2cf67112a0fdf46a2c31beb49404edc8bcecfc14a4989100605b66.jpg)  
Figure 6-13. We plot the autocorrelation function of the residuals for each time series. Note that for both series there is some borderline significant autocorrelation of the errors at lag 3 that may not be fully accounted for in the model.

The ACF is not as clearly supportive of no autocorrelation in the residuals as we might like, so in this case we can also apply the Portmanteau test for serial correlation via the vars package serial.test() method. This test is similar to the serial correla‐ tion tests we have looked at in the univariate case:

```r
## R
> serial.test(est.var, lags.pt = 8, type="PT.asymptotic")
Portmanteau Test (asymptotic) 
```

data: Residuals of VAR object est.var Chi-squared $= 20.463$ ,df $= 20$ ,p-value $= 0.4293$

Because the $\boldsymbol { p }$ -value is so high, we cannot reject the null hypothesis that there is no serial correlation in the residuals. This gives us further evidence that the model does an acceptable job.

Given that for univariate models we examined a variety of models, leading up to the ARMA and ARIMA versions, you may wonder whether there is a VARIMA model. There is indeed, but it is not used very much given that VAR performs relatively well and is already quite complicated. In practice for industrial and academic use cases, you will overwhelmingly see the use of VAR rather than VARIMA.

Another related class of model is the CVAR model, which is the cointegrated vector autoregression. This refers to the case where the individual time series are not sta‐ tionary, but a linear combination of the time series is stationary without differencing.

# Variations on Statistical Models

There are many other kinds of statistical models developed for time series data. Some of these expand the ARIMA model, while others make different underlying assump‐ tions about the temporal dynamics than those used in the ARIMA model. In this sec‐ tion we briefly discuss some of the most commonly used and well-known statistical time series models.

# Seasonal ARIMA

A Seasonal ARIMA (SARIMA) model assumes multiplicative seasonality. For this reason, a SARIMA model can be expressed as ARIMA $( p , \ d , \ q ) \times ( P , \ D , \ Q ) m$ . The model postulates that the seasonal behavior itself can be thought of as an ARIMA process, with m specifying the number of time steps per seasonal cycle. What is important in this factoring is that the model recognizes that adjacent points in time can have an influence on one another, either within the same season or in different seasons, but through the usual methods of being temporally proximate.

Identifying a SARIMA is even trickier than identifying an ARIMA model precisely because you need to address seasonal effects. Luckily, auto.arima() in the forecasts package can handle this just as it handles a standard ARIMA estimation task. As dis‐ cussed earlier, there are good reasons to go with automated parameter selection unless you have strong knowledge that suggests you override the selected model determined by automated methods.

# ARCH, GARCH, and their many brethren

ARCH stands for “Autoregressive Conditional Heteroskedasticity.” This model is used almost exclusively in the finance industry. It is often covered in time series cour‐ ses, so it is worth mentioning here. This class of models is based on the observation that stock prices do not have constant variance, and that in fact the variance itself seems autoregressive conditional on the earlier variances (for example, high-volatility days on the stock exchange come in clusters). In these models, it is the variance of a process that is modeled as an autoregressive process rather than the process itself.

# Hierarchical time series models

Hierarchical time series are quite common in the real world, although they are not presented as such. You can easily formulate situations in which they can arise:

• Total dollar monthly demand for a company’s products, which can then be bro‐ ken down by SKU number   
• Weekly political polling data for the electorate as a whole, and then that same polling data broken down by demographics (overlapping or nonoverlapping), such as female versus male or Hispanic versus African American   
• Total count of tourists landing in the EU daily versus counts of tourists landing in each member nation in particular

One convenient way to handle hierarchical time series is via R’s hts package. This package can be used both to visualize hierarchical time series data and to generate forecasts.

Forecasts can be generated with a number of different methodologies that have his‐ torically been available with this package:

• Generate the lowest-level forecasts (the most individualized) and aggregate these up to produce the higher-level forecasts.   
• Generate the highest-level forecasts and then generate lower-level forecasts based on the historical proportions of each component of the aggregate. This method‐ ology tends to be less accurate at making low-level predictions, although this can be somewhat mitigated with a variety of techniques for predicting how propor‐ tions of the aggregate will themselves change over time.   
• It is possible to try to obtain the best of each methodology by choosing a “mid‐ dle” approach where a mid-level forecast is generated (assuming you have several layers of hierarchy). This is then propagated up and down in generality to make the other predictions.

Ultimately, the gold standard with the hts package is that, for all levels of hierarchy, forecasts can be generated that are independent of other levels of the hierarchy. hts then combines these forecasts to assure consistency in a method as per Hyndman et al..

Many of the statistical models we have discussed in this chapter would in turn be applied to the problem of forecasting whatever level of time series was settled upon. The hierarchical portion of the analysis tends to be a wrapper around these funda‐ mental models.

# Advantages and Disadvantages of Statistical Methods for Time Series

When you are contemplating whether to apply one of the statistical models described here to a time series problem, it can be good to start with an inventory of advantages and disadvantages. These are laid out in this section.

# Advantages

• These models are simple and transparent, so they can be understood clearly in terms of their parameters.   
• Because of the simple mathematical expressions that define these models, it is possible to derive their properties of interest in a rigorous statistical way.   
• You can apply these models to fairly small data sets and still get good results.   
• These simple models and related modifications perform extremely well, even in comparison to very complicated machine learning models. So you get good per‐ formance without the danger of overfitting.   
• Well-developed automated methodologies for choosing orders of your models and estimating their parameters make it simple to generate these forecasts.

# Disadvantages

• Because these models are quite simple, they don’t always improve performance when given large data sets. If you are working with extremely large data sets, you may do better with the complex models of machine learning and neural network methodologies.   
• These statistical models put the focus on point estimates of the mean value of a distribution rather than on the distribution. True, you can derive sample varian‐ ces and the like as some proxy for uncertainty in your forecasts, but your funda‐ mental model offers only limited ways to express uncertainty relative to all the choices you make in selecting a model.   
• By definition, these models are not built to handle nonlinear dynamics and will do a poor job describing data where nonlinear relationships are dominant.

# More Resources

• Classic texts:

Rob J. Hyndman and George Athanasopoulos, Forecasting: Principles and Practice, 2nd ed. (Melbourne: OTexts, 2018), https://perma.cc/9JNK-K6US.

This practical and highly accessible textbook offers an excellent introduction in R, for free, on the internet, of all the basics of preprocessing time series data and using that data to make forecasts. The emphasis is on getting read‐ ers up to speed and competent in using practical methods for time series forecasting.

Ruey Tsay, Analysis of Financial Time Series (Hoboken, NJ: John Wiley & Sons, 2001).

This classic textbook introduces a variety of time series models, including a very thorough and accessible chapter on developing AR, MA, and ARIMA models, with applications to historical stock prices. Extensive R examples are also included. This is a middle-of-the-road book in terms of accessibility, presuming some familiarity with statistics and other mathematics but quite readable to anyone with high school calculus and an introductory statistics course under their belt.

Robert H. Shumway, Time Series Analysis and Its Applications (NY, NY: Springer-International, 2017).

This is another classic textbook written at a somewhat more theoretical and less accessible level. It’s best to have a look at Tsay’s book first, but this book includes additional discussions of the mathematical processes of statistical time series models. It also includes a wider variety of data analysis stemming from the sciences as well as from economic data sources. This book is a more challenging read than the other textbooks listed, but this is because it is dense and informative and not because it requires a much higher level of mathematical proficiency (it does not, although it can feel like it if you read too quickly).

• Heuristical guidance:

Robert Nau, “Summary of Rules for Identifying ARIMA Models,” course notes from Fuqua School of Business, Duke University, https://perma.cc/37BY-9RAZ.

These notes give you detailed guidance on how to choose the three parame‐ ters of an ARIMA model. You will immediately notice a strong emphasis on parsimonious models when you read this summary.

National Insitute of Standards and Technology (NIST), “Box-Jenkins Models,” in NIST/SEMATECH e-Handbook of Statistical Methods (Washington, DC: NIST, US Department of Commerce, 2003), https://perma.cc/3XSC-Y7AG.   
This section of the online NIST handbook offers concrete steps to imple‐ ment the Box-Jenkins method, a commonly used methodology for ARIMA parameter selection. It’s also a good example of the resources NIST produces for statistical analysis of time series more generally, as part of the larger meticulously compiled handbook of best practices for time series analysis.   
Rob J. Hyndman, “The ARIMAX Model Muddle,” Hyndsight blog, October 4, 2010, https://perma.cc/4W44-RQZB.   
This concise blog post from noted forecaster Rob Hyndman describes incor‐ porating covariates into ARIMA models, an alternative methodology to VAR for handling multivariate time series.   
Richard Hardy, “Cross Validation: Regularization for ARIMA Models,” question posted on Cross Validated, StackExchange, May 13, 2015, https://perma.cc/G8NQ-RCCU.   
In the scenario of a high order of autoregressive terms or many inputs in the case of VAR, it can make sense to regularize, and in many industries this leads to substantial performance improvements. This post on the Cross Vali‐ dated Q&A site provides some preliminary discussion as well as links to a computational implementation and related academic research.

# State Space Models for Time Series

State space models are similar to the statistical models we looked at in the previous chapter but with a more “real-world” motivation. They address concerns that emerge in real-world engineering problems, such as how to factor in measurement error when making estimates and how to inject prior knowledge or belief into estimates.

State space models posit a world in which the true state cannot be measured directly but only inferred from what can be measured. State space models also rely on specify‐ ing the dynamics of a system, such as how the true state of the world evolves over time, both due to internal dynamics and the external forces that are applied to a system.

While you may not have seen state space models before in a mathematical context, you have likely used them in your day-to-day life. For example, imagine you see a driver weaving in traffic. You try to determine where the driver is going and how you can best defend yourself. If the driver might be intoxicated, you would consider call‐ ing the police, whereas if the driver was temporarily distracted for a reason that won’t repeat itself, you’d probably mind your own business. In the next few seconds or minutes you would update your own state space model of that driver before deciding what to do.

A classic example of where you would use a state space model is a rocket ship launched into space. We know Newton’s laws, so we can write the rules for the dynamics of the system and what the motion should look like over time. We also know that our GPS or sensors or whatever we use to track location will have some measurement error that we can quantify and attempt to factor into the uncertainty about our calculations. Finally, we know that we can’t account for all the forces in the world acting on a particular rocket as there are many unknowns in the system, so we want a process that is robust to other unknown sources of noise, perhaps solar wind

or earthly wind or both. As it turns out, statistical and engineering advances in the last 50 years have proven quite useful for tackling these sorts of situations.

Two different historical trends led to the development of state space models and interest in the kinds of problems they address. First, around the middle of the 20th century, we entered an age of mechanical automation. There were rockets and space‐ ships in the sky, navigation systems for submarines, and all sorts of other automated inventions that required estimation of a system state that could not be measured. As researchers thought about how to estimate system state, they began developing state space methods, most importantly to disambiguate measurement errors from other kinds of uncertainty in the system. This led to the first uses of state space methods.

During this time period, too, recordkeeping technology and associated computing were also developing. This led to the creation of much larger data sets for time series, including much longer or more detailed time series data sets. As more time series data became available, more data-intensive methods could be developed for them in conjunction with the new thinking about state space modeling.

In this chapter we will study these commonly used state space methods:

• The Kalman filter applied to a linear Gaussian model   
• Hidden Markov Models   
• Bayesian structural time series

In each of these cases, the use of such models is quite accessible and well imple‐ mented. For each model, we will develop some intuition for the mathematics and dis‐ cuss what kind of data is appropriate for use with the method. Finally, we will see code examples for each method.

In each case we will distinguish between what we observe and the state that produced our observations. In estimating the underlying state based on observations, we can divide our work into different stages or categories:

# Filtering

Using the measurement at time $t$ to update our estimation of the state at time t

# Forecasting

Using the measurement at time $t - 1$ to generate a prediction for the expected state at time $t$ (allowing us to infer the expected measurement at time $t$ as well)

# Smoothing

Using measurement during a range of time that includes t, both before and after it, to estimate what the true state at time $t$ was

The mechanics of these operations will often be similar, but the distinctions are important. Filtering is a way of deciding how to weigh the most recent information

against past information in updating our estimate of state. Forecasting is the prediction of the future state without any information about the future. Smoothing is the use of future and past information in making a best estimate of the state at a given time.

# State Space Models: Pluses and Minuses

State space models can be used for both deterministic and stochastic applications, and they can be applied to both continuously sampled data and discretely sampled data.1

This alone gives you some inkling of their utility and tremendous flexibility. The flex‐ ibility of state space models is what drives both the advantages and disadvantages of this class of models.

There are many strengths of a state space model. A state space model allows for mod‐ eling what is often most interesting in a time series: the dynamic process and states producing the noisy data being analyzed, rather than just the noisy data itself. With a state space model, we inject a model of causality into the modeling process to explain what is generating a process in the first place. This is useful for cases where we have strong theories or reliable knowledge about how a system works, and where we want our model to help us suss out more details about general dynamics with which we are already familiar.

A state space model allows for changing coefficients and parameters over time, which means that it allows for changing behavior over time. We did not impose a condition of stationarity on our data when using state space models. This is quite different from the models we examined in Chapter 6, in which a stable process is assumed and mod‐ eled with only one set of coefficients rather than time-varying coefficients.

Nonetheless, there are also some disadvantages to a state space model, and sometimes the strength of the state space model is also its weakness:

• Because state space models are so flexible, there are many parameters to be set and many forms a state space model can take. This means that the properties of a particular state space model have often not been well studied. When you formu‐ late a state space model tailored to your time series data, you will rarely have stat‐ istical textbooks or academic research papers in which others have studied the model too. This leaves you in less certain territory as far as understanding how your model performs or where you may have committed errors.

• State space models can be very taxing computationally because there are many parameters. Also, the very high number of parameters for some kinds of state space models can leave you vulnerable to overfitting, particularly if you don’t have much data.

# The Kalman Filter

The Kalman filter is a well-developed and widely deployed method for incorporating new information from a time series and incorporating it in a smart way with previ‐ ously known information to estimate an underlying state. One of first uses of the Kal‐ man filter was on the Apollo 11 mission—the filter was chosen when NASA engineers realized that the onboard computing elements would not allow other, more memory-intensive techniques of position estimation. As you will see in this section, the benefits of the Kalman filter are that it is relatively easy to compute and does not require storage of past data to make present estimates or future forecasts.

# Overview

The mathematics of the Kalman filter can be intimidating to a newcomer, not because it is especially difficult but because there are a fair number of quantities to keep track of, and it’s an iterative, somewhat circular process with many related quantities. For this reason, we will not derive the Kalman filter equations here, but instead go through a high-level overview of those equations to get a sense of how they work.2

We begin with a linear Gaussian model, positing that our state and our observations have the following dynamics:

$$
x _ {t} = F \times x _ {t - 1} + B \times u _ {t} + w _ {t}
$$

$$
y _ {t} = A \times x _ {t} + v _ {t}
$$

That is, the state at time t is a function of the state at the previous time step $( F \times x _ { t - 1 } )$ , an external force term $( B \times u _ { t } )$ , and a stochastic term $( w _ { t } )$ . Likewise, the measurement at time $t$ is a function of the state at time $t$ and a stochastic error term, measurement error.

Let’s imagine that $x _ { t }$ is the real position of a spaceship, while $\gamma _ { t }$ is the position we measure with a sensor. $\nu _ { t }$ is the measurement error in our sensor device (or range of

devices). The fundamental equation applicable to the Kalman filter is this equation illustrating how to update our estimation given new information for time t:

$$
\hat {x} _ {t} = K _ {t} \times y _ {t} + (1 - K _ {t}) \times \hat {x} _ {t - 1}
$$

What we see here is a filtering step—that is, a decision about how to use the measure‐ ment at time $t$ to update our state estimation at time t. Remember, we have posited a situation in which we can observe only $\gamma _ { t }$ and make inferences about the state but can never be sure of the state exactly. We see above that the quantity $K _ { t }$ establishes a bal‐ ance in our estimation between old information $( \widehat { \boldsymbol { x } } _ { t - 1 } )$ and new information $( \boldsymbol { y } _ { t } )$ .

To go into the more detailed mechanics, we need to define some terms. We use $\mathrm { P _ { t } }$ to denote our estimate of the covariance of our state (this can be a scalar or a matrix, depending on whether the state is univariate or multivariate, the latter being more common). $\mathrm { P _ { \mathrm { ~ t ~ } } ^ { - } }$ is the estimate for $t$ before our measure at time $t$ is taken into account.

We also use $R$ to denote the measurement error variance, that is the variance of $\nu _ { t } ,$ which again can either be a scalar or a covariance matrix depending on the dimen‐ sionality of the measurements. $R$ is generally well defined for a system, as it describes well-known physical properties of a particular sensor or measuring device. The appo‐ site value for $w _ { t } , Q ,$ is less well defined and subject to tuning during the modeling process.

Then we begin with a process such that we have known or estimated values for $x$ and $P$ at time 0. Then, going forward for times after time 0, we follow an iterative process of a prediction and updating phase, with the prediction phase coming first, followed by the update/filtering phase, and so on:

Prediction:

$$
\hat {x} _ {\mathrm {t}} ^ {-} = \mathrm {F} \times \hat {x} _ {\mathrm {t - 1}} + \mathrm {B} \times \mathrm {u} _ {\mathrm {t}}
$$

$$
\mathrm {P} _ {\mathrm {t}} ^ {-} = \mathrm {F} \times \mathrm {P} _ {\mathrm {t - 1}} \times \mathrm {F} ^ {\mathrm {T}} + \mathrm {Q}
$$

Filtering:

$$
\hat {x} _ {\mathrm {t}} = \hat {x} ^ {-} _ {\mathrm {t}} + \mathrm {K} _ {\mathrm {t}} \times \left(\mathrm {y} _ {\mathrm {t}} - \mathrm {A} \times \hat {x} _ {\mathrm {t}} ^ {-}\right)
$$

$$
P _ {t} = \left(I - K _ {t} \times A\right) \times P _ {t}
$$

where $K _ { t } ,$ the Kalman gain, is:

$$
\mathrm {K} _ {\mathrm {t}} = \mathrm {P} ^ {-} _ {\mathrm {t}} \times \mathrm {A} ^ {\mathrm {T}} \times (\mathrm {A} \times \mathrm {P} ^ {-} _ {\mathrm {t}} \times \mathrm {A} ^ {\mathrm {T}} + \mathrm {R}) ^ {- 1}
$$

You will see many visualizations of this recursive process. Some will break this into many steps, perhaps up to four or five, but the simplest way of thinking about it is that there are the computations undertaken to predict the values at time $t _ { : }$ , without a measurement for $\gamma _ { t }$ (the prediction), and then there are the steps undertaken at time t, after the measurement $\gamma _ { t }$ is known (the filtering).

To start, you need the following values:

• Estimates for $R$ and Q—your covariance matrices for measurement error (easy to know) and state stochasticity (usually estimated), respectively   
• Estimates or known values for your state at time 0, $\hat { \boldsymbol { x } } _ { 0 }$ (estimated based on $y _ { 0 . }$   
• Advance knowledge of what forces are planned to be applied at time t and how these impact state—that is, the matrix $B$ and the value $u _ { t }$   
• Knowledge of system dynamics that determine the state transition from one time step to another, namely $F$   
• Knowledge of how the measurement depends on the state, namely A

There are many ways to derive the Kalman filter equations, including from a proba‐ bilistic perspective in expectation values, a least squares minimization problem, or as a maximum likelihood estimation problem. Derivations of the Kalman filter are widely available, and the interested reader can pursue this topic with a few internet searches.

# Code for the Kalman Filter

We consider a classic use case: trying to track an object subject to Newton’s mechan‐ ics with error-prone sensors. We generate a time series based on Newton’s laws of motion, namely that the position of an object is a function of its velocity and acceler‐ ation. We imagine taking discrete measurements even though the underlying move‐ ment is continuous. We first determine a series of accelerations, then assume that both position and velocity begin at 0. While it is not physically realistic, we assume instant acceleration changes at the beginning of each time step, and a steady accelera‐ tion value:

```r
## R
## rocket will take 100 time steps
ts.length <- 100
## the acceleration will drive the motion
a <- rep(θ, ts.length)
## position and velocity start at θ
x <- rep(θ, ts.length)
v <- rep(θ, ts.length)
for (ts in 2:ts.length) { 
```

$\begin{array}{rl} & {\mathrm{x[ts] <   - v[ts - 1]*2 + x[ts - 1] + 1 / 2*a[ts - 1]^{\wedge}2}}\\ & {\mathrm{x[ts] <   - x[ts] + rnorm(1,sd = 20)##stochastic component}}\\ & {\mathrm{v[ts] <   - v[ts - 1] + 2*a[ts - 1]}} \end{array}$

If you don’t know or remember Newton’s laws of motion, you may wish to familiar‐ ize yourself with them, although you can also take these at face value for the current purposes (the computation of x[ts] and v[ts]).

A quick plotting exercise shows us the motion we have created with the acceleration structured as just shown (see Figure 7-1):

```r
## R
par(mfrow = c(3, 1))
plot(x, main = "Position", type = 'l')
plot(v, main = "Velocity", type = 'l')
plot(acceleration, main = "Acceleration", type = 'l') 
```

We posit that these variables would represent a full description of the state, but that the only data available to us is the object’s position and that this data is available only from a noisy sensor. This sensor is $\times$ in the following code, and we plot how the measured value relates to the actual position in Figure 7-2:

```r
## R
z <- x + rnorm(ts.length, sd = 300)
plot(x,ylim = range(c(x, z)))
lines(z) 
```

In Figure 7-2 we see a constant acceleration (bottom plot) driving a linearly increas‐ ing velocity (middle plot) to produce a parabolic shape in displacement (top plot). If these mechanical relationships are not familiar, you can take them at face value or look for a quick review of basic mechanics in an introductory physics textbook.

![](images/60ce7d09e716718f506f03802d4d2eae81ddbb150d70448374f0189affc05349.jpg)

![](images/dae22641bff12730d50dce78ab88f02a8163e408e3723bb1ea83e07e97a3e32d.jpg)

![](images/bfe72db77fa814f57fe03f297d2610350994a02f0e603b1eb61d85df5bfaf9a0.jpg)  
Figure 7-1. The position, velocity, and acceleration of our rocket.

![](images/1281811031a1ed439b44ab429f6444a5902f50b3f52330ed5482aa0960720dbf.jpg)  
Figure 7-2. The true position (points) versus our noisy measurement (line). Notice that the position x does not reflect a perfect parabola due to the noise we put into the state transition equation.

Now we apply a Kalman filter. First, we code up a general function that reflects our discussion and derivation earlier in this section:

```r
## R
kalman.motion <- function(z, Q, R, A, H) {
    dimState = dim(Q)[1]
    xhatminus <- array(rep(0, ts.length * dimState),
                    c(ts.length, dimState))
    xhat <- array(rep(0, ts.length * dimState),
                    c(ts.length, dimState))
    Pminus <- array(rep(0, ts.length * dimState),
                    c(ts.length, dimState, dimState))
    P <- array(rep(0, ts.length * dimState),
                    c(ts.length, dimState, dimState))
    K <- array(rep(0, ts.length * dimState),
                    c(ts.length, dimState)) # Kalman gain
    # initial guesses = starting at 0 for all metrics
    xhat[1, ] <- rep(0, dimState)
    P[1, ,] <- diag(dimState)
    for (k in 2:ts.length) {
        # time update
        xhatminus[k, ] <- A %% matrix(xhat[k-1,])
        Pminus[k, ,] <- A %% P[k-1, ,] %% t(A) + Q
        K[k, ] <- Pminus[k, ,] %% H %% solve(t(H) %% Pminus[k, ,] %% H + R)
        xhat[k, ] <- xhatminus[k, ] + K[k, ] %%%
        (z[k] - t(H) %% xhatminus[k,])
        P[k, ,] <- (diag(dimState) - K[k,] %% t(H)) %% Pminus[k, ,]
    }
} 
```

We apply this function, making only the rocket’s position measurable (so not the acceleration or velocity):

```txt
## R
## noise parameters
R <- 10^2 ## measurement variance - this value should be set
## according to known physical limits of measuring tool
## we set it consistent with the noise we added to x
## to produce x in the data generation above
Q <- 10 ## process variance - usually regarded as hyperparameter
## to be tuned to maximize performance
## dynamical parameters 
```

```r
A <- matrix(1) ## x_t = A * x_t-1 (how prior x affects later x)
H <- matrix(1) ## y_t = H * x_t (translating state to measurement) 
```

```r
## run the data through the Kalman filtering method
xhat <- kalman-motion(z, diag(1) * Q, R, A, H)[[1]] 
```

We plot the true position, the measured position, and the estimated position in Figure 7-3.

![](images/d6f47c0c9555c4445f2d41b8082696f065ba3f981d5ccd58794769fed3e6cd1b.jpg)  
Figure 7-3. Many related quantities: the measured position, the actual/true position, the filtered estimation of the position (that is, best estimate for position at time t incor‐ porating the measurement at time t), and the forecast of the position (that is, the best estimate for position at time t incorporating only the known system dynamics plus measurements up to time t – 1 and not including time t).

The Kalman filter takes out much of the noise from the measurement error. How much it does so will depend on our value for R, the measurement noise parameter, which reflects how much the filter should weigh the most recent value relative to ear‐ lier values. As we can see, the filter does a satisfactory job of forecasting the data. In particular, note that there is not a lag between the forecast data and the actual data, which would suggest that the method is merely predicting the current value based on the last value.

Here we have worked through a simple example of a Kalman filter. The Kalman filter is widely studied because it is so useful in a variety of applications, particularly those where the internal dynamics of the system are very well understood. This makes it an

ideal tool in cases such as the simple rocket example, where we understand the dynamics that are driving the system.

Note that in this simple example, the whole power and utility of the Kalman filter is not fully realized. It is particularly useful when we have multiple kinds of measure‐ ments, say measuring different quantities or measuring the same thing simultane‐ ously with many devices. Also there are many extensions to the Kalman filter that are worth studying if this is a promising area for your domain of interest.

One of the great benefits of the Kalman filter, as we’ve illustrated, is that it is recur‐ sive. This means that it is not necessary to look at all data points on each iteration of the process. Rather, at each time step, all the information from prior time steps is already incorporated in the best possible way in the few estimated parameters, namely the most recent state and covariance estimate. The beauty of this method is that we can update in a smart way just using these “summary statistics” like meas‐ ures, and we already know how to weigh them intelligently relative to the most recent data. This makes the Kalman filter very useful for real-world applications where com‐ putational time or resources are at a premium. In many cases this also coincides with the dynamics of real systems, in that the processes are relatively Markovian (memo‐ ryless other than the immediately prior state), and a function of an underlying state that can only be measured with some error.

There are many useful extensions to the Kalman filter that we have not discussed here. One principal use of the Kalman filter is to adapt it to smoothing, which means using data both before and after time $t$ to make the best estimate of the true state at time t. The mathematics and code are similar to what has been presented already. Also similar is the Extended Kalman Filter (EKF), which adapts the Kalman filter to data with nonlinear dynamics. This is also relatively straightforward to implement and is widely available in a variety of R and Python packages.

The Kalman filter is $O ( T )$ with respect to the length of the time series but $O ( d ^ { 2 } )$ with respect to $d _ { \mathrm { \cdot } }$ , the dimension of the state. For this reason, it is important not to overspecify the state when a more streamlined specification will do just as well. However, it is this linearity with respect to the length of the time series that makes the Kalman filter commonly used in real production scenarios and much more popular than other filters developed for the purpose of state space modeling of time series.

# Hidden Markov Models

Hidden Markov Models (HMMs) are a particularly useful and interesting way of modeling a time series because it is a rare instance of unsupervised learning in time series analysis, meaning there is no labeled correct answer against which to train. An HMM is motivated by an intuition similar to what we used when experimenting with the Kalman filter earlier in this chapter, namely the idea that the variables we are able to observe may not be the most descriptive variables of the system. As with the Kal‐ man filter applied to a linear Gaussian model, we posit the idea that the process has a state, and our observations give information about this state. And again, as before, we need to have some opinion as to how the state variables influence what we can observe. In the case of an HMM, what we posit is that the process is a nonlinear one characterized by jumps between discrete states.

# How the Model Works

An HMM posits a system in which there are states that are not directly observable. The system is a Markov process, which means that it is “memoryless” in the sense that the probabilities of future events can be fully calculated given only the system’s current state. That is, knowing the system’s current state and its previous states is no more useful than simply knowing the system’s current state.

Markov processes are often described in terms of matrices. For example, suppose there was a system that fluctuated between state A and state B. When in either state, the system was statistically more likely to remain in the same state than to flip to the other state at any distinct time step. One such system would be described by the fol‐ lowing matrix:

<table><tr><td></td><td>A</td><td>B</td></tr><tr><td>A</td><td>0.7</td><td>0.3</td></tr><tr><td>B</td><td>0.2</td><td>0.8</td></tr></table>

Let’s imagine that our system is in state A, namely (1, 0). (State B would be (0, 1).) In such a case, the probability that the system would remain in state A is 0.7, whereas the probability that the system would flip is 0.3. We don’t need to know what states the system was in before its most recent moment in time. This is what it means to be a Markov process.

A Hidden Markov Model represents the same kind of system, except that we are not able to directly infer the state of the system from our observations. Instead, our observations offer clues as to the state of the system (see Figure 7-4).

![](images/d608f444cee5a10b1a8af2f4104d1edf3f74f8e6f325c200c9c4f7260f364dfd.jpg)  
Figure 7-4. The process for an HMM. The actual states of the system at a given time are represented by $x ( t )$ , whereas the observable data at a given time is represented by the $y ( t )$ . Only $x ( t )$ is relevant to thinking about y(t). In other words, $x ( t - 1 )$ offers no addi‐ tional information to predict $y ( t )$ if we know $x ( t )$ . Likewise, only $x ( t )$ has any bearing for predicting $x ( t + 1 )$ and there is no additional information from knowing $x ( t - 1 )$ . This is the Markov aspect of the system.

Note that for realistic applications, the states will usually produce outputs that over‐ lap, so that it is not $1 0 0 \%$ clear what state is producing output. For example, we are going to apply an HMM to data that looks like Figure 7-5. This is data simulated from four states, but it is not obvious from a simple visual inspection of the time ser‐ ies how many states there are, what their divisions are, or where the system transi‐ tions between states.

![](images/0bfbcf5ff9d840fb451cf717f0a5c8113b241acf422606863dd29e8240ca8517.jpg)  
Time series with four underlying states   
Figure 7-5. This time series was simulated with four states, but it’s not clear from a vis‐ ual observation that there are four states, nor is it clear where one state ends and another begins.

Some examples of real-life use cases of HMMs are:

• Identifying regime shifts in the financial markets   
• Classifying, predicting, and correcting DNA sequencing information   
• Recognizing stages of sleep as reflected in ECG data

# How We Fit the Model

We are positing that there is a state we cannot measure directly, and there is no way to get a demonstrably correct answer in many of the data sets where we could apply this technique. So how can this algorithm identify hidden states without knowing anything about them a priori? The answer is: iteratively. There is no magic bullet for deriving the most probable sequence of hidden states to explain the observations, but it is possible to edge our way toward an estimation once we have fully specified the system.

In an HMM, we posit that the system is fully described with the following information:

• The transition probability of going from $x ( t )$ to $x ( t ) + 1$ . This is equivalent to specifying a matrix like the one just mentioned: transitioning between states A and B. The size of the matrix would depend on the number of hypothesized states.   
• The emission probability, or the probability of observation $y ( t )$ given $x ( t )$   
• The initial state of the system.

More concretely, here is a listing of the variables needed to characterize and fit an HMM process:

• $Q = q _ { 1 } , q _ { 2 } , . . . q _ { N }$ the distinct states of the system   
• $A = a _ { i , j } = a _ { 1 , 1 } , a _ { 1 , 2 } , . . . a _ { N , N }$ the transition probability matrix indicating the transition at any given time step of changing from state i to state j   
• $O = o _ { \mathrm { 1 \cdot } }$ , o2, …oT a sequence of observations sampled from this process in order, that is a time series of observations   
• $b _ { i ( o t ) }$ indicating the emission probabilities, that is the probabilities of seeing a given observation value, $o _ { t }$ if the state is $q _ { i }$   
• $p = p _ { 1 } , p _ { 2 } , . . . p _ { N } ,$ the initial probability distributions, namely the probability that the system will start in state $q _ { 1 } , q _ { 2 } , . . . q _ { N } ,$ respectively

However, with real data generally none of these variables are known. All that is known is the actual sequence of observables $y _ { 1 } , y _ { 2 } , . . . y _ { t }$ .

# Baum Welch algorithm

In estimating the parameters of a Hidden Markov Model, we use the Baum Welch algorithm. This guides us in our complex task to estimate the values of all the param‐ eters, as they were detailed in the previous section. Our task is multifold. We are seeking to:

• Identify the distinct emission probabilities for each possible hidden state and identify the transition probabilities from each possible hidden state to each other possible hidden state. For this we use the Baum-Welch algorithm.   
• Identify the most likely hidden state for each time step given the full history of observations. For this we use the Viterbi algorithm (described shortly).

These are two related tasks, and each one is difficult and computationally taxing. What’s more, they are dependent. In the case of two interrelated tasks, parameter estimation and likelihood maximization, we can use the expectation maximization algorithm to iterate between these two steps until an acceptable solution is found.

To apply the Baum Welch algorithm, the first step is to specify the likelihood func‐ tion, which is the probability of observing a given sequence given the hypothesized parameters. In our case, the hypothesized parameters would be the mathematical parameters per postulated state.

For example, if we assumed the states produced Gaussian outputs with distinct means and standard deviations of the observed values that depended on the state, and if we assumed a two-state model, we might describe the model in terms of $\mu _ { 1 } , \sigma _ { 1 } , \mu _ { 2 } ,$ and $\sigma _ { 2 } ,$ with $\mu _ { u = i }$ indicating the ith state’s mean and $\sigma _ { i }$ indicating the ith state’s stan‐ dard deviation. These could describe the emission probabilities, and we collectively denote them as θ. We could also posit the state sequence as $x _ { 1 } , x _ { 2 } , . . . x _ { t }$ (I’ll summarize this as $X _ { t }$ for shorthand), which we were not able to observe, but imagine for the moment that we could.

Then the likelihood function would describe the likelihood of observing the sequence we had observed given the parameters of the emission probability (that is, the proba‐ bility of a given observation given a specific state) and the sequence of hidden states as the integral over all possible $X _ { t }$ of $\begin{array} { r } { p ( y _ { 1 } , y _ { 2 } , . . . y _ { t } | \mathsf { \Pi } _ { 1 } , \mathsf { \Pi } _ { 0 } , \mathsf { \Pi } _ { 1 } , \mathsf { \Pi } _ { 2 } , \mathsf { \Pi } _ { 2 } , . . . \mathsf { \Pi } _ { N } , \mathsf { \Pi } _ { 0 } , \mathsf { \Pi } _ { N t } ) = p ( y _ { 1 } , y _ { 2 } , . . . \mathsf { \Pi } _ { N } } \end{array}$ $y _ { t } \left| \mu _ { 1 } , \sigma _ { 1 } , \mu _ { 2 } , \sigma _ { 2 } , . . . \mu _ { N } , \sigma _ { N } \right)$ .

However, this is a difficult problem for several reasons, including the fact that the complexity grows exponentially with the number of time steps, meaning that an exhaustive grid search is not realistic. So, we simplify the task by applying the EM algorithm as follows:

1. Randomly initialize the emission probability variables.   
2. Compute the probability of each possible $X _ { t } ,$ assuming the emission probability values.   
3. Use these values of $X _ { t }$ to generate a better estimate of the emission probability variables.   
4. Repeat steps 2 and 3 until convergence is reached.

What this means, more informally, is that in the event of positing (at random) two distributions, we would then look at each time step and determine the probability that for each time step a particular state was occupied (e.g., at time step $t$ what was the probability of state A? of state B?) Once we had then assigned a putative state to each time step, we would use those labels to re-estimate the emission probabilities (zeroing in on a better mean and standard deviation for the state). Then we would repeat the process again, using the newly updated emission probability variables to improve our estimate of the $X _ { t }$ trajectory.

One important point to remember about such uses of the EM algorithm is that you are not guaranteed to find a globally optimal set of parameters. For this reason it’s worth running many fits to see what the global consensus is across numerous initiali‐ zations. It’s also important to remember that a fit will require a burn-in period, the appropriate length of which will depend on the details of your data and model.

# Viterbi algorithm

Once the parameters of an HMM process have been estimated, as via the Baum Welch algorithm, the next question of interest is what the most likely series of under‐ lying states was given the measured time series of observable values.

Unlike the Baum Welch algorithm, the Viterbi algorithm is guaranteed to provide you with the best solution to the question you are asking. This is because it is a dynamic programming algorithm designed to fully and efficiently explore the range of possible fits by saving the solutions to portions of a path so that as the path is lengthened there’s no need to recompute all possible paths for all path lengths (see Figure 7-6).

![](images/adba34a0ace46228c5b0b4a21de03298080c4d85f8f093ebea34a25ef0492bcb.jpg)  
Figure 7-6. The Viterbi algorithm searches all possible paths that could explain a given observed time series, where a path indicates which state was occupied at each time step.

# Dynamic Programming

Data scientists do not always have a standard algorithms course in their toolkit, but basic algorithms can be particularly helpful for time series analysis, where inference and recurrence are helpful concepts in thinking about temporally ordered data.

The easiest way to explain what dynamic programming is tends to be with an exam‐ ple, and the classic one is the Fibonacci sequence. Imagine you were asked to calcu‐ late the eighth Fibonacci number. The easiest way to do this would be if you already knew the sixth and seventh Fibonacci numbers. To find these out, you need the fourth and fifth Fibonacci numbers and so on. Since you know that solutions to later Fibonacci problems are determined from earlier ones, as you compute Fibonacci numbers, you should store them somewhere so that they can be used for later, more complicated problems. For this reason, dynamic programming is also described as a technique of memoization.

Hallmarks of a situation where dynamic programming could apply are:

• You can calculate a solution for a problem of size $N$ if you know the solution to a problem of size $N - 1$ . For this reason, remembering, or memoizing, the solutions to earlier smaller problems will help you solve later problems more efficiently.   
• The problem has a clearly defined ordering to scale from a smaller problem to a larger problem.   
• You can identify a single base case that is directly (and easily) calculable.

# Fitting an HMM in Code

While the HMM fitting process is very complicated, it is implemented in a number of packages in R. Here we will work with the depmixS4 package. First, we need to for‐ mulate an appropriate time series. We do that with the following code:

```r
## R
## notice in this case we have chosen to set a seed
## if you set the same seed, our numbers should match
set.seed(123)
## set parameters for the distribution of each of the four
## market states we want to represent
bull_mu <- 0.1
bull_sd <- 0.1
neutral_mu <- 0.02
neutral_sd <- 0.08
bear_mu <- -0.03
bear_sd <- 0.2
panic_mu <- -0.1
panic_sd <- 0.3
## collect these parameters in vectors for easy indexing
mus <- c(bull_mu, neutral_mu, bear_mu, panic_mu)
sds <- c(bull_sd, neutral_sd, bear_sd, panic_sd)
## set some constants describing the time series we will generate
NUM.PERIODS <- 10
SMALLEST.PERIOD <- 20
LONGEST.PERIOD <- 40
## stochastically determine a series of day counts, with
## each day count indicating one 'run' or one state of the market
days <- sample(SMALLEST.PERIOD:LONGEST.PERIOD, NUM.PERIODS,
replace = TRUE)
## for each number of days in the vector of days
## we generate a time series for that run of days in a particular
## state of the market and add this to our overall time series
returns <- numeric()
true.mean <- numeric()
for (d in days) {
idx = sample(1:4, 1, prob = c(0.2, 0.6, 0.18, 0.02))
returns <- c(rets, rnorm(d, mean = mus[idx], sd = fds[idx]))
true.mean <- c(true.mean, rep(mus[idx], d))
} 
```

In the preceding code we use a stock-market-inspired example, with bull, bear, neu‐ tral, and panic modes. A random number of days for a state to persist is selected, as

are variables describing the emission probability distribution for each state (that is, the _mu and _sd variables, which indicate what kind of values we expect to see meas‐ ured given a particular state).

We can get a sense of what our generated time series looks like, and the frequency of each state, by seeing how many days in the sample correspond to each true.mean, which is the variable we are using to track state:

```txt
R  
> table(true.mean)  
true.mean  
-0.03 0.02 0.1  
155 103 58 
```

Catastrophe! Although we intended to include four states in the simulated series, only three are included. This is likely because the fourth state had a very low proba‐ bility of inclusion (0.02). We see that the least probable state wasn’t even selected for inclusion in the series. We couldn’t always know that for a given time series—that not all possible states were actually included in a time series—which highlights part of why it’s difficult to fit HMMs and why things can be somewhat unfair to the algo‐ rithm. Nonetheless, we will move forward with the analysis specifying four groups, to see what this gives.3

We still need to fit an HMM. The resulting HMM will provide a time series of poste‐ rior probabilities for each state for as many states as we indicate are desired. Consis‐ tent with the earlier description of the EM algorithm, nothing more than the number of putative states needs to be specified. The rest will be gradually determined via an iterative back-and-forth process.

As is often the case, with the right package, the difficult bit of the analysis is actually quite easy in terms of the amount of code that needs to be written. In this case we use the depmi $\times { \mathsf { S } } 4 ^ { 4 }$ package in R. The model is fit in two steps. First, it is specified with the depmix() function, which indicates the expected distribution, the number of states, and the input data to be used in fitting. Then the model is fit via the fit function, which takes the model specification as its input. Finally, to generate the posterior dis‐ tribution of the state labels given the fit to the data, we use the posterior() function. At this point the model itself has already been fit, so this is a separate task to label the

data now that the parameters describing the state distributions and transition likeli‐ hoods have been estimated:

```r
## R
require(depmixS4)
hmm.model <- depmixreturns ~ 1, family = gaussian.,
nstates = 4, data=data.frameReturns=returns)
model.fit <- fit(hmm.model)
post_probs <- posterior(model.fit) 
```

Here we produce an hmm.model where we indicate the observable is the returns vec‐ tor. We also indicate the number of states (4) and that the emission probabilities come in a Gaussian distribution via our specification for the family parameter. We fit the model via a fit() function and then calculate the posterior probabilities with the posterior() function. The posterior probabilities give us the probability of a given state at a given time for the model parameters we have determined with the fit‐ ting process.

Now we can visualize the states along with the measured values as follows (see Figure 7-7):

```r
# R
plotreturns, type = 'l', lwd = 3, col = 1,
    yaxt = "n", xaxt = "n", xlab = "", ylab = "", ylim = c(-0.6, 0.6))
lapply(θ: (length(returns) - 1, function (i) {
    ## add a background rectangle of the appropriate color
    ## to indicate the state during a given time step
    rect(i, -0.6, (i + 1), 0.6,
                    col = rgb(0.0, 0.0, 0.0, 0.0, alpha=(0.2 * post_prob$state[i + 1]), border = NA)
} 
```

![](images/b844c8f25d92125c5fe6853d9e7f11de488ce48b3e35371a64db66f70a6a9582.jpg)  
Figure 7-7. In this plot the background indicates a distinct state, and the solid black line depicts the actual values. The vertical white line sort of objects are actually very narrow slices that represent the times when the process is estimated to be in what turns out to be the rarest of the four states.

We can access information about the hypothesized distributional parameters the model determined via its attributes. As we review this, let’s keep in mind our original data generation parameters:

bull_mu <- 0.1

bull_sd <- 0.1

neutral_mu <- 0.02

neutral_sd <- 0.08

bear_mu <- -0.03

bear_sd <- 0.2

panic_mu <- -0.1

panic_sd <- 0.3

If we were going to match (remembering that the panic regime didn’t make it into the data), it would look like, of the states actually present in the data, the following groups roughly correlated:

> attr(model.fit, "response")

[[1]][[1]] <- coincidentally has a mean near the panic regime but that regime did not actually produce any data in the sample that was fit. instead the algorithm allotted this fourth state to more negative values

Model of type gaussian (identity), formula: returns ~ 1

Coefficients:

(Intercept)

```txt
-0.09190191 sd 0.03165587
```

```txt
[[2]]  
[[2]][[1]] <- could be consistent with the bear market regime  
Model of type gaussian (identity), formula: returns ~ 1  
Coefficients:  
(Intercept)  
-0.05140387  
sd 0.2002024 
```

```txt
[[3]]  
[[3]][[1]] <- could be consistent with the bull market regime  
Model of type gaussian (identity), formula: returns ~ 1  
Coefficients:  
(Intercept)  
0.0853683  
sd 0.07115133 
```

```txt
[[4]]  
[[4]][[1]] <- could be somewhat consistent with the neutral market regime  
Model of type gaussian (identity), formula: returns ~ 1  
Coefficients:  
(Intercept)  
-0.0006163519  
sd 0.0496334 
```

One possible reason the fit doesn’t line up better with our baseline hidden states is that we didn’t use a proper transition matrix, whereas the model used one for a fit. Our transitions between states were not properly Markovian, and this could impact the fit. Additionally, we fit to a relatively short time series with few transitions between states, whereas HMMs will perform better on longer time series with more opportunities to observe/infer state transitions. I’d recommend coming up with more realistic synthetic data to test a proposed HMM when you are experimenting with this technique and preparing to fit it to real data. Remember, in most cases of real data you are positing an unobservable state, so you want to think about the limits of the model performance in a more controlled setting (with synthetic data) before set‐ ting yourself up for more ambitious projects.

HMMs are apt for analyzing many kinds of data. HMMs have been used to model whether a financial market is in a growth or recession phase, determine what stage folding a protein within a cell is in, and describe human motion (before the arrival of deep learning). These models continue to be useful, more often for understanding the dynamics of a system than for making predictions. Also, HMMs provide more than just a point estimate or forecast. Finally, we can inject prior knowledge or beliefs into our model, such as by specifying the number of states used to fit our HMM. In this

way, we reap the benefits of statistical methods but also have parameters to parame‐ terize our prior knowledge about the system.

The mathematics and computation of HMMs are quite interesting and approachable. You can learn a lot of accessible programming techniques and numerical optimiza‐ tion algorithms by looking into the most common ways that HMMs are fit to data. You’ll also learn about dynamic programming techniques, which are helpful to a data scientist or a software engineer.

As with a Kalman filter, HMMs can be used for a variety of tasks. In fact, the variety of inference problems related to HMM systems is even more complex due to the increased complexity of discrete states, each with its own emission probability. Some of the inference tasks you may face when using an HMM include:

• Determining the most likely description of the states producing a series of obser‐ vations. This involves estimating the emission probabilities of these states as well as the transmission matrix that describes how likely one state is to lead to another. We did this, although we did not look explicitly at the transition probabilities.   
• Determining the most likely sequence of states given a series of observations and a description of the states and their emission and transition probabilities. We did this as well as part of the previous task. This is sometimes referred to as the “most likely explanation” and is commonly calculated with the Viterbi algorithm.   
• Filtering and smoothing. In this situation, filtering would correspond to estimat‐ ing the hidden state of the most recent time step given the most recent observa‐ tion. Smoothing would correspond to determining the most likely distribution of the hidden state in a particular time step given observations before, during, and after that time step.

# Bayesian Structural Time Series

Bayesian structural time series (BSTS) are related to the linear Gaussian model we worked through with Kalman filtering earlier. The main difference is that Bayesian structural time series provide a way to use preexisting components to build more complex models that can reflect known facts or interesting hypotheses about a sys‐ tem. We can then design the structure, use robust fitting techniques to estimate parameters for the built model in the case of our data, and see whether the model does a good job of describing and predicting a system’s behavior.

The mathematics of BSTS models is fairly complicated and computationally taxing compared to the linear Gaussian model we covered in our discussion of the Kalman filter. For this reason, we’ll stick to a general overview and then apply the code.

There are four steps to fitting a BSTS model, carried out in the following order:

1. A structural model is defined, including specification of priors.   
2. A Kalman filter is applied to update estimates of state based on observed data.   
3. The spike-and-slab method is used to perform variable selection within the structural model.5   
4. Bayesian model averaging is applied to combine results for the purpose of gener‐ ating a forecast.

In the next example we will focus only on steps 1 and 2, wherein we define a flexible model out of preexisting modular components and then use this to fit our data with a Bayesian method that updates parameter estimation as time passes.

# Code for bsts

Here we use a popular and powerful BSTS package from Google, bsts, and an open data set from OpenEI.org.

We first plot the data to get a sense of what we are trying to model (Figure 7-8):

```r
## R
elec = fread("electric.csv")
require(bsts)
n = colnames(elec)[9]
par(mfrow = c(2, 1))
plot( elec [[n]][1:4000])
plot( elec [[n]][1:96])
## as discussed earlier in the book
## appropriate temporal scale is key to understanding ts data 
```

![](images/2d79c70847de59e732d97d563309181858046f91a610c749f6a78a5603ea367d.jpg)

![](images/6407a4f535fd82fc448bab441d26eaece28c48202ed9c80577f30e208ef55f85.jpg)  
Figure 7-8. The full series we fit (two thousand consecutive hourly measurements) in the upper plot and a shorter, more understandable subset of the data in the lower plot. The plot makes more sense once we look at it in a way that makes the daily patterns evident.

Looking at our data gives us a sense of how to model. We can see that there is cer‐ tainly a daily pattern, and there may even be a day-of-week pattern. These reflect the seasonal behavior we will describe in our model. We also want to allow for a trend in the data given the nonstationary behavior we see when we plot the whole data in the upper panel of Figure 7-8:

```r
## R
ss <- AddLocalLinearTrend(list(), elec[[n]])
ss <- AddSeasonal(ss, elec[[n]], nseasons = 24, season.duration = 1)
ss <- AddSeasonal(ss, elec[[n]], nseasons = 7, season.duration = 24) 
```

The local linear trend in this model assumes that both the mean and slope of a trend in the data follow a random walk.6

The seasonal component of the model takes two arguments, one indicating the num‐ ber of distinct seasons and one indicating the duration of the season. In the first sea‐ sonal component we add, which reflects a daily cycle, we want one season for each hour of the day, and each season lasts only the one hour. In the second seasonal com‐ ponent we add, which reflects a weekly cycle, we want one season for each day of the week and we want each season to last 24 hours.

While you might wonder whether we are truly starting at 12:01 am on Monday (or however we want to define the week), being consistent is more important than is

whether a seasonal label of day 1 exactly corresponds with a Monday. In the recur‐ rent pattern we see here, it seems like any way of chopping the data into 24 hours would likely be acceptable for seasonality analysis.

The most taxing part of the code, computationally, is shown next. The beauty of the bsts package is that we are able to run a number of Markov Chain Monte Carlo (MCMC) computations of the posterior:

```r
## R
model1 <- sts(elec[[n]], state.specification = ss, niter = 100)
plot(model1, xlim = c(1800, 1900)) 
```

We can also inspect the seasonal components. For example, we inspect the day of the week seasonal component like so (see Figures 7-9 and 7-10):

```txt
## R
plot(model1, "seasonal", nseasons = 7, season.duration = 24) 
```

![](images/ab736cabdd1e1faef76503172f5150fd47c25f8254fb1ffbcfee9b887ea9109c.jpg)

![](images/6bbf31aa10960144691d3efc048ff1b4261128bac5208d4814cff0eb3f3b74e5.jpg)  
Figure 7-9. The day of the week seasons, which show that there is a difference in differ‐ ent days of the week. It also shows that the day of the week parameter distributions are stable over time for each day of the week.

![](images/ebf62268f2eaf7c5193b4f31b998624dd92c4e0a3f1fd664b9cac9505416a4d8.jpg)

![](images/7384116ff2d8d54f2e7bae4875040ee975b9423ec4b3e16d3c51386cfc258cc4.jpg)

![](images/20f540f671b3aefcd48be92695bf54481174f10227fa3ce7def61220b2e53f18.jpg)  
Figure 7-10. Distributions of contributions from the trend of the data as well as the daily and day of week seasonal components. If you sum these three components, you will get the value for the prediction.

The day of the week seasonal component shows great stability, whereas the hour of the day seasonality shown in the middle plot in Figure 7-10 tends to show trends over time, likely relating to changing daylight hours. In Figure 7-10 we also see the param‐ eter fit for our local linear trend, which shows a decreasing pattern in electricity demand overall.

Finally, we forecast, complete with a full graph of the posterior distribution of the predictions into the future (see Figure 7-11). Notice that we have the flexibility up until the end of this modeling process to indicate how many time horizons we want to predict forward. Remember this is hourly data, so predicting 24 time horizons for‐ ward may seem quite ambitious but amounts to only one day. We also indicate that we want to see the 72 time periods prior to the prediction for context:

pred <- predict(model1, horizon $= ~ 2 4$ , quantiles $=$ c(0.05, 0.95)) plot(pred, plot.original = 72)

![](images/6f778bc8a1eb35bb29e8b41f206310905a7d5d37392029deb1e4183b6fb421aa.jpg)  
Figure 7-11. The last 72 hours in our data combined with a forecast for the next 24 hours as well as $5 \%$ and $9 5 \%$ quantile boundaries on the forecasts. Notice that the dis‐ tribution of the forecast grows more spread out as we forecast further into the future.

There is a fair amount of optionality in the bsts package and in Bayesian structural time series modeling that we haven’t even taken advantage of yet, namely:

• We did not specify nonstandard priors.   
• We did not use the spike-and-slab method to select out regressors.   
• We did not use Bayesian model averaging.

These are all possible to do with the bsts package, and you can readily find guidance in the documentation.

We have only scratched the surface in our example of what can be done with BSTS. Here are a few other important available options with BSTS:

• BSTS allow you to inject any kind of prior into your modeling. The standard lin‐ ear Gaussian model we covered when discussing the Kalman filter is just one way of injecting a fairly vanilla prior, while BSTS offers many options (for example, nonsymmetric priors).   
• BSTS models allow you to perform variable selection.

• BSTS models can be combined via Bayesian model averaging, which can help account for the uncertainty associated with selecting a model in the first place.

While we did not make use of these options in our current modeling case, they are not difficult to incorporate when using the bsts package, and you can find many examples online.

# More Resources

• On Kalman filters and linear Gaussian state space models:

Greg Welch and Gary Bishop, “An Introduction to the Kalman Filter,” technical report, University of North Carolina at Chapel Hill, 1995, https://perma.cc/ZCU8- MXEF.

This introductory overview of the Kalman filter offers a derivation and matrix-formulation of the filter. The introduction also discusses the Exten‐ ded Kalman Filter, a more common filtering in real-life scenarios that can be used for nonlinear processes or nonlinear measurement errors.

R.E. Kalman, “A New Approach to Linear Filtering and Prediction Problems,” Transactions of the ASME—Journal of Basic Engineering 82, Series D (1960): 35– 45, https://perma.cc/GNC4-YLEC.

This 1960 research article is the original introduction of the Kalman filter. The mathematics is quite approachable with a basic background in statistics and calculus, and this is an interesting read to learn the original motivations for the Kalman filter and the intellectual context for its creator.

R. Labbe, “Kalman and Bayesian Filters in Python,” GitHub repository, https:// perma.cc/CMU5-Y94A.

This epic GitHub repository includes dozens of examples of using Kalman filters and related techniques of “filtering” more generally. The repository is textbook-like, featuring worked examples, a related PDF book, and exercises with solutions.

Marie Auger-Méthé et al., “State-Space Models’ Dirty Little Secrets: Even Simple Linear Gaussian Models Can Have Estimation Problems,” Scientific Reports 6, no. 26677 (2016), https://perma.cc/9D8V-Z7KJ.

This article highlights a case where even simple linear Gaussian models, such as those we used in our discussion of the Kalman filter, are easily prone to extreme misspecification, particularly in the case of a measurement error that is large relative to the values of a time series. The authors focus on this problem from the perspective of ecologists, but the general concern remains valid for a variety of data-driven disciplines and offers a counterbalancing view compared to the many advantages that are emphasized for this method.

• On Hidden Markov Models:

Andrew Moore, “Hidden Markov Models,” lecture notes, School of Computer Sci‐ ence, Carnegie Mellon University, https://perma.cc/K3HP-28T8.

These comprehensive lecture notes offer an overview of HMMs, complete with illustrations of estimation algorithms and examples from robotics of how HMMs are used in real applications.

Dan Klein, “Artificial Intelligence: Hidden Markov Model,” lecture notes, Univer‐ sity of California Berkeley, https://perma.cc/V7U4-WPUA.

This is another set of accessible reference notes. These give examples of the usefulness of HMMs for digitizing speech and also for developing AI to play strategic games.

user34790, “What Is The Difference Between the Forward-Backward and Viterbi Algorithms?” question posted on Cross Validated, StackExchange, July 6, 2012, https://perma.cc/QNZ5-U3CN.

This StackExchange post offers an interesting discussion and outline relating to the many estimation algorithms deployed for specific HMM use cases. This post will help you get a sense of the ways HMMs can be used to under‐ stand time series data even if you are not interested in the details of the related modeling algorithms.

• On Bayesian structural time series:

Mark Steel, “Bayesian Time Series Analysis,” in Macroeconometrics and Time Ser‐ ies Analysis, ed. Steven N. Durlauf and Lawrence E. Blume (Basingstoke, UK: Pal‐ grave Macmillan, 2010), 35–45, https://perma.cc/578D-XCVH.

This brief read offers a comprehensive overview of different techniques asso‐ ciated with Bayesian analysis of time series as well as concise commentary as to the strengths and weaknesses of each method.

Steven Scott and Hal Varian, “Predicting the Present with Bayesian Structural Time Series,” unpublished paper, June 28, 2013, https://perma.cc/4EJX-6WGA.

This Google paper based on economic time series presents an example of applying a time series forecasting problem to data about a given time that becomes available with different lags. In particular, the authors use current Google searches to predict unemployment rates where the latter rates are only published periodically, whereas the Google search counts were continu‐ ously available. This is an example of what is often called “nowcasting” to indicate a forecast that is actually being made about the present due to a reporting lag. The paper uses a combination of Bayesian structural time ser‐ ies and ensemble techniques.

Jennifer Hoeting et al., “Bayesian Model Averaging: A Tutorial,” Statistical Science 14, no. 4 (1999): 382–401, https://perma.cc/BRP8-Y33X.

This article provides a comprehensive overview of how Bayesian model aver‐ aging works via several different methods. As this article describes, the pur‐ pose of Bayesian model averaging is to account for uncertainty in the modeling process due to model selection. Through worked examples, the authors provide a way to better estimate the uncertainty in predictions. A related, simpler summary is provided in an overview of BMA, an R package for Bayesian Model Averaging.

# Generating and Selecting Features for a Time Series

In the previous two chapters we examined methods of time series analysis that rely on using all the data points in a time series to fit a model. However, in preparation for the next chapter’s discussion of the application of machine learning to time series analysis, in this chapter we will study feature generation and selection for time series. If you are unfamiliar with the concept of feature generation, you will not remain so for long. It’s an intuitive process and one that enables a creative side to data analysis.

Feature generation is the process of finding a quantitative way to encapsulate the most important traits of time series data into just a few numeric values and categori‐ cal labels. You are compressing the raw times series data into a shorter representation via a set of features to describe that time series (we’ll work through a quick example momentarily). For example, a very simple feature generation could describe every time series with its mean value and the number of time steps in the series. This would be one way of describing that time series without going through all the raw data step by step.

The purpose of feature generation is to compress as much information about the full time series as possible into a few metrics or, alternately, to use those metrics to iden‐ tify the most important information about the time series and discard the rest. This is important for machine learning methods, most of which were developed on nontem‐ poral data but which can be fruitfully applied to time series problems, provided we can digest a time series into a properly formatted input. In this chapter we will focus particularly on packages that allow us to automatically generate commonly used time series features so there will be no need for us to reinvent or handcode them.

Once we have generated some putatively useful features, we must ensure that they are indeed useful. While you are unlikely to craft too many unhelpful features by hand,

this is a problem you will run into when you use code that automatically generates a large number of features of a time series for downstream use in machine learning. For this reason, we must inspect the features, once generated, to see which can be dis‐ carded in subsequent analyses.

Traditional machine learning models were not originally developed with time series in mind, and so they do not automatically lend themselves to time series analytical applications. However, one way to make these models work with temporal data is feature generation. For example, by describing a univariate time series not with a ser‐ ies of numbers detailing the step-by-step outputs of a process but rather by describ‐ ing it with a set of features, we can access methods designed for cross-sectional data.

In this chapter we will first work through a very simple example of feature generation for a short time series. We will then review feature generation packages for time ser‐ ies, both in R and Python. Finally, we’ll work through an example of automated fea‐ ture generation and feature selection. After reading this chapter you will have all the skills needed to preprocess a time series data set for downstream machine learning applications in Chapter 9.

# Introductory Example

Imagine the past week’s morning, midday, and evening temperatures were as shown in Table 8-1.

Table 8-1. Temperatures for the past week   

<table><tr><td>Time</td><td>Temperature (°F)</td></tr><tr><td>Monday morning</td><td>35</td></tr><tr><td>Monday midday</td><td>52</td></tr><tr><td>Monday evening</td><td>15</td></tr><tr><td>Tuesday morning</td><td>37</td></tr><tr><td>Tuesday midday</td><td>52</td></tr><tr><td>Tuesday evening</td><td>15</td></tr><tr><td>Wednesday morning</td><td>37</td></tr><tr><td>Wednesday midday</td><td>54</td></tr><tr><td>Wednesday evening</td><td>16</td></tr><tr><td>Thursday morning</td><td>39</td></tr><tr><td>Thursday midday</td><td>51</td></tr><tr><td>Thursday evening</td><td>12</td></tr><tr><td>Friday morning</td><td>41</td></tr><tr><td>Friday midday</td><td>55</td></tr><tr><td>Friday evening</td><td>20</td></tr><tr><td>Saturday morning</td><td>43</td></tr><tr><td>Saturday midday</td><td>58</td></tr><tr><td>Saturday evening</td><td>22</td></tr><tr><td>Sunday morning</td><td>46</td></tr><tr><td>Sunday midday</td><td>61</td></tr><tr><td>Sunday evening</td><td>35</td></tr></table>

You could plot this data and you’d see elements of periodicity (a daily cycle) and also a trend of overall increasing temperatures. But we can’t store an image of a plot in a database, and most methods that accept a picture as an input are data-intensive and seek to strip down the picture into summary metrics. So we should do the summary metrics ourselves. Instead of describing the 21 numbers in Table 8-1 as a time series, we could describe the series with a few words and numbers:

• Daily/periodic   
• Increasing trend; we could make this more quantitative by computing a slope   
• Mean values for each of morning, midday, and evening

By doing so, we’d summarize the 21-point time series with 2 to 5 numbers—quite a bit of data compression without losing too much detail. This is a simple case of fea‐ ture generation. Then, feature selection would entail paring away any features that were not descriptive enough to justify inclusion. What justifies inclusion will depend on our downstream use of the features.

# General Considerations When Computing Features

As with any aspect of analysis, when you are computing time series features for a time series data set, you will want to think through whether your analysis makes sense and whether the effort you put into generating features is more likely to lead to overfitting from a surfeit of features than it is to lead to meaningful insights.

The best approach is to develop a set of potentially useful features as you run through time series exploration and cleaning. As you visualize data and think about what dis‐ tinguishes different time series in the same data set or different time periods in the same time series, you will develop ideas about what kinds of measurements would be useful for labeling or predicting a time series. You can also draw useful assistance from any background knowledge you have about a system or even a working hypoth‐ eses you’d like to test with subsequent analysis.

Next we discuss a few distinct concerns you should keep in mind when generating time series features.

# The Nature of the Time Series

As you decide what time series features to generate, you need to keep in mind the underlying attributes of your time series, which you determined during data explora‐ tion and cleaning.

# Stationarity

Stationarity is one consideration. Many time series features assume stationarity and are useless unless the underlying data is stationary or at least ergodic. For example, using the mean of a time series as a feature is practical only where the time series is stationary so that the idea of a mean makes sense. This value is not very meaningful where we have a nonstationary time series, as the value measured as the mean in that case is more or less an accident, a result of too many entangled processes, such as a trend or a seasonal cycle.

![](images/8070f4b26a1f7ace768ef6e8f6b0e37c03a8b54c6cbe1e30f416c2cb1915dca3.jpg)

# Stationary Versus Ergodic Time Series

An ergodic time series is one in which every (reasonably large) sub‐ sample is equally representative of the series. This is a weaker label than stationarity, which requires that these subsamples have equal mean and variance. Ergodicity requires that each slice in time is “equal” in containing information about the time series, but not necessarily equal in its statistical measurements (mean and var‐ iance). You can find a helpful discussion on StackExchange.

# Length of time series

Another consideration for feature generation is the length of the time series. Some features may be sensible for a stationary time series but become unstable as the length of the time series increases, such as the minimum and maximum value of the series. For the same underlying process, a longer time series will likely measure more extreme maximum and minimum values than a shorter time series produced by the same process, simply because there were more opportunities for data collection.

# Domain Knowledge

Domain knowledge should be key for time series feature generation where you are lucky enough to have some insights. Some examples of how domain knowledge is applied to generate specific time series features are provided later in this chapter, but for now we’ll focus on the more general point.

For example, if you are working with a physics time series, you should quantify fea‐ tures that make sense on the timescale of the system you are studying, as well as make sure that the features you select would not be unduly influenced by the characteristics of, say, the error of a sensor rather than the characters of an underlying system.

As another example, imagine you are working with data from a specific financial market. To ensure financial stability, this market imposes maximum price changes in a given day. If the price changes too much, the market shuts down. You might con‐ sider whether, in this context, to generate a feature indicating the maximum price seen on a given day.

# External Considerations

The extent of your computational resources and associated storage resources is also important. Likewise, your motivation for generating features matters. Are you gener‐ ating features that will be stored so that you can throw out voluminous raw data? Or are you merely computing the features for a single analysis and planning to keep only the raw data?

The purpose of your time series feature generation may influence how many features you decide to compute and whether you should contemplate particularly computa‐ tionally demanding features. This may also depend on the overall size of the data set you are analyzing. For a small data set all these decisions will be low stakes, but in the case of extremely large time series data sets, you may risk embarking on a feature generation task that will be left half-done, wasting computational energy and coding.

After considering all these factors, try putting together a list of features and running them on a small data set to get an idea of how fast or slow they run. If the small set runs too slowly, you should pare down your time series substantially before continu‐ ing your analysis. Likewise, you might consider exploring the usefulness of computa‐ tionally taxing features on a subset of your data before undertaking an analysis with the full data set.

# A Catalog of Places to Find Features for Inspiration

Time series feature generation is limited only by your data, your imagination, your coding skill, and your domain knowledge. So long as you can think of a reasonably general and well-defined way to quantify the behavior of a time series, you can gener‐ ate a feature. Some simple and oft-used time series features amount to the same sum‐ mary statistical functions you will have used in other applications, such as:

• Mean and variance   
• Maximum and minimum   
• Difference between last and first values

You will also visually identify other features that are more computationally challeng‐ ing to compute but are often useful. Some examples include:

• Number of local maxima and minima

• Smoothness of the time series   
• Periodicity and autocorrelation of the time series

In such cases, you will need to make some implementation definitions, as there are different ways to identify these commonly used features. It will help to keep your own personal library of feature generation code available, but you may also want to look into feature generation libraries for time series data, particularly as you become inter‐ ested in the more computationally demanding features. In such cases, you should look for an excellent implementation such that the code is both reliable and efficient.

Now we’ll turn to the use of time series feature generation libraries, paying particular attention to the wide range of features you can benefit from via automatic feature generation.

# Open Source Time Series Feature Generation Libraries

There have been many efforts to automate the creation of time series features because they tend to be interesting, descriptive, and even predictive across domains.

# The tsfresh Python module

One particularly compelling example of automatic feature generation in Python is the tsfresh module, which implements a large and general set of features. We can get a sense of the breadth of implemented features by considering some of the general cat‐ egories of features that are available. These include:

# Descriptive statistics

These are driven by the traditional statistical time series methodologies we stud‐ ied in Chapter 6, including:

• An Augmented Dickey–Fuller test value  
• An AR(k) coefficient   
• The autocorrelation for a lag, $k$

Physics-inspired indicators of nonlinearity and complexity

This category includes:

• The function c3(), which is a proxy for calculating the expected value of $L \ ^ { 2 } { \left( \boldsymbol { X } \ ^ { 2 } \right) } \times L \ \left( \boldsymbol { X } \right) \times \boldsymbol { X }$ ( $L$ is the lag operator). This has been proposed as a measure of nonlinearity in a time series.   
• The function cid_ce(), which calculates the square root of the sum from 0 to $\mathrm { ~ n ~ - ~ } 2 \times \mathrm { l a g }$ of $( x _ { i } - x _ { i } + l ) ^ { 2 }$ . This has been proposed as a measure of the complexity of a time series.

• The function friedrich_coefficients(), returns coefficients of a model fitted to describe complex nonlinear motion.

# History-compressing counts

This category comprises features such as:

• The sum of the values in a time series that occur more than once   
• The length of the longest consecutive subsequence that is above or below the mean   
• The earliest occurrence within the time series of the minimum or maximum value

A module like tsfresh can help you save time and choose efficient implementations for feature selection. It can also educate you about ways of describing data that could be relevant but that you might not have come across in your own research. There are numerous other benefits to using a module, as there always are when you combine your analysis with open source, well-vetted tools, including:

• There is no need to reinvent the wheel when you are computing standard fea‐ tures. By using a shared library, you have the assurance of some accuracy check‐ ing by other users, rather than doubting your own code and having to verify it.   
• A library such as this one provides a framework for calculating features, not just a laundry list of features. For example, tsfresh has a feature calculator class, which you can use to extend this library for your own purposes but with the ben‐ efits of a systematic framework.   
• This library is designed to hook up with downstream consumers of features, most importantly with sklearn, so that your features can easily be passed to machine learning models.

The tsfresh library has a particularly technical flavor in that many of the features are derived from ideas about analyzing scientific experimental data.

# The Cesium time series analysis platform

A more approachable but equally extensive catalog of generated features is the list implemented in the Cesium library. The current list is available in its documentation, and next we pick out a few interesting features for discussion and inspection. The general categories are broken down in the source code, but we break them down fur‐ ther here:

• Features that describe the overall distribution of the data values, without regard to its temporal relationships. This category can include a diverse set of features that are all nonetheless time-agnostic:

— How many local peaks there are in a histogram of the data?   
— What percentage of the data points are within a fixed window of values close to the median of the data?

• Features that describe the distribution of the timing of data:

— Features that take the distribution of time between measurements as their own distribution and compute similar statistics to those just described, now on the distribution of time differences rather than data values   
— Features that compute the probability that the next observation will occur within $n$ time steps given the observed distribution

• Features that describe measures of the periodicity of the behavior within the time series. Often these features are associated with the Lomb-Scargle periodogram.

# Periodograms

A periodogram of a time series is an estimate of how much different underlying fre‐ quencies contribute to the time series. A perfect periodogram could translate a time series from the time-value phase space, which we most commonly work with, to a power-frequency space whereby a time series would be described in terms of how much of it was shaped by recurring processes at various timestamps. If you are unfa‐ miliar with this concept, Figure 8-1 shows it visually.

![](images/4b2a1af79f548baec350339d1f19cbe3a80bd457211a75c90de2eb4e965d4bf3.jpg)  
Figure 8-1. The same data represented in the time domain, where it appears similar to a sum of sine time series. In the frequency domain, we see all the spectral mass loading onto the three frequencies that characterize the sines that were summed to produce the series.

There are many ways of computing periodograms. One definition and computational method is the Fourier transform of the autocorrelation function. A more intuitive way of thinking about creating a periodogram is as a least squares fit of the original data to a set of sine curves of varying frequencies.

This latter is what the much-vaunted Lomb-Scargle method does. The Lomb-Scargle method constructs a periodogram for irregularly sampled time series data, unlike a traditional periodogram, which assumes a regularly sampled time series. Lomb, and then Scargle, developed techniques to study irregular time series with this method and established that the statistical properties of the periodogram constructed from such data had the same statistical properties as a traditional periodogram sampled from regularly sampled time series data. Lomb’s and Scargle’s advancements enabled the study of many natural science problems for which irregularly sampled times ser‐ ies are inevitable, such as climate data, astrophysical data, and geological data.

The features just described can be computed either over the entire time series or as rolling or expanding window functions. Given what we learned in earlier chapters about the mechanics of coding up rolling and expanding window functions, we could certainly implement these features ourselves, and we are in a good position to under‐ stand the documentation and meaning of what this library does. In this case, we would be applying rolling window functions to summarize data rather than to clean it. The same techniques in time series analysis come into use in many different but equally useful cases.

The cesium library provides supplementary functionality in addition to feature gen‐ eration. For example, it includes a web-based GUI to perform feature generation and also integrates with sklearn.

If you try these libraries on your data, you will notice that time series generation is extremely time consuming. For this reason, you should think carefully about how many features you need to generate for your data and when it truly makes sense to automatically generate features rather than thoughtfully developing your own.

Many of the features generated by these libraries are computationally taxing and— given how extensive the lists of features are—will often not address points of interest for the question you are trying to answer. With some domain knowledge, you may even recognize that a particular kind of feature is irrelevant, noisy, or not predictive. Do not calculate these useless features unnecessarily. It will slow down your entire analysis without adding any clarity. Automatic feature generation libraries are useful but should be used judiciously rather than indiscriminately.

# R’s tsfeatures package

tsfeatures, developed by Rob Hyndman et al., is a convenient R package for gener‐ ating a variety of commonly used and useful time series features. The documentation includes a listing of the features, which include helpful functions such as:

• acf_features() and pacf_features(), which each compute a number of related values given a general sense of how important autocorrelation is in the behavior of a series. For the acf_features() function, the documentation describes the following return values: “A vector of 6 values: first autocorrelation coefficient and sum of squared of first 10 autocorrelation coefficients of original series, firstdifferenced series, and twice-differenced series. For seasonal data, the autocorre‐ lation coefficient at the first seasonal lag is also returned.”   
• lumpiness() and stability(), which are tiled window–driven functions, and max_level_shift() and max_var_shift(), which are rolling window–driven functions. In each of these cases differences and variety-measuring statistics are applied to values measured on either overlapping (rolling) or nonoverlapping (tiled) windows of the time series.   
• unitroot_kpss() and unitroot_pp().

The tsfeatures package usefully consolidates and includes work from a variety of academic projects related to studying time series features, as well as from other ongo‐ ing efforts to improve the process of creating time series features that are useful in a variety of domains. These include:

• compengine() computes the same time series features developed by the compengine.org project, which have been found to be helpful across a wide variety of time series data from many domains.1   
• A number of features borrowed from the hctsa package, which is intended to run highly comparative time series analysis in Matlab. Some of these features are in: autocorr_features(), firstmin_ac(), pred_features(), and trev_num(). You will also find others from reading the documentation.

The tsfeatures documentation also includes helpful illustrations of the uses and outputs of each of the feature-generating functions, and an extensive bibliography

linking to related statistical and machine learning work on the time series feature set available in the package.

# Domain-Specific Feature Examples

Another source of inspiration can come from domain-specific features that have been developed for a variety of time series data. Often these features have been developed over decades, either from heuristics that empirically work even if they are not well understood or from scientific knowledge about how a system’s underlying mechanics work.

Next we review some domain-specific features for two kinds of time series data: financial and healthcare.

# Technical stock market indicators

Technical stock market indicators are probably the most widely documented and for‐ malized sets of indicators used for a domain-specific time series application. Econo‐ mists studying time series data over the last century have an extensive list of features they commonly use to quantify time series in the financial markets and make predic‐ tions. Even if you have no interest in the financial markets, this list is inspirational in showing how a domain-specific list of features can also be quite extensive, descrip‐ tive, and creative.

A nonexhaustive list of these features is included here so that you can get an idea of how complex and highly specific to the financial markets these indicators are. Given the complexity, it’s no surprise that people can spend their entire career attempting to understand how these “signals” can forecast the rise and fall of the financial markets.

# Relative Strength Index (RSI)

This measure is equal to $1 0 0 - 1 0 0 / \left( 1 + R S \right)$ where RS is the ratio of the mean gain during an “up” period (rising prices) divided by the mean loss during a “down” period (decreasing prices), and the lookback period for these up and down periods is an input parameter, so that you can have different RSI values for different lookback periods. Traders have developed rules of thumb about what cutoff RSI values indicate that an asset is undervalued or overvalued relative to its true worth. The RSI is known as a “momentum indicator” because it relies on measures of an asset’s movement.

# Moving Average Convergence/Divergence (MACD)

This indicator is itself composed of three time series:

• The MACD time series is a time series of the difference between a shortterm exponential moving average of the asset (“fast”) and a long-term expo‐ nential moving average of the asset (“slow”).

• The “average” time series is an exponential moving average of the MACD time series.   
• The “divergence” time series is the difference between the MACD time series and the “average” time series. This is the value that is usually used for mak‐ ing financial forecasts. The other input series (MACD and average) are usu‐ ally only prepared to create the “divergence” series.

# Chaikin Money Flow (CMF)

This indicator measures the direction in which spending is trending. To calculate it:

• Calculate the money flow multiplier: ((Close – Low) – (High – Close))/(High – Low).   
• Calculate the money flow volume, which is the day’s trading volume multi‐ plied by the money flow multiplier.   
• Sum this money flow volume for a particular period of days and divide it by the volume for that same period of days. This indicator is an “oscillator” that ranges between –1 and 1. It indicates “buying pressure” or “selling pressure,” a measure of the direction of the market.

As you can see from this small taste of the many technical features that can be con‐ structed from simple financial time series, there are many ways to describe a time ser‐ ies. The financial markets are a particularly rich and much-studied domain for feature generation.2

# Healthcare time series

Healthcare is another area where time series features have domain-specific meanings and even names. As we discussed in Chapter 1, health data offers quite a wide array of time series data. One example is EKG data (see Figure 8-2). Reading EKGs is both a science and an art, and various features are manually identified by physicians and used to read a time series. If you were going to be selecting features for a machine learning study on EKG data, you would certainly want to start by studying these features and speaking to a knowledgeable physician to understand the purpose of the features and what they indicate.

![](images/2257bb7bace274e93d9810d2def830498577d0819a9f4e318445600319ea4127.jpg)  
Figure 8-2. Examples of time series features medical professionals use to read EKG time series data.

Similarly, if you are analyzing high-resolution blood glucose time series data, it is also helpful to understand the kind of patterns that tend to affect daily data as well as how a healthcare professional would understand and label this data (see Figure 8-3).

The two time series illustrated in Figures 8-2 and 8-3 provide good examples of when we would be interested in finding local maxima or distances between them. Hence, we can easily envision specific features from the tsfresh or cesium libraries that are relevant to healthcare time series given domain-specific knowledge.

![](images/7701a4181a9fbe6889cd36b0ecd9d338a1dce3f19ebb24f3dc85510b4cabb743.jpg)  
Figure 8-3. In a one-day time series of blood glucose levels, we can see there are four features most healthcare professionals would identify in a typical day, one of which is not related to food but is known as the “dawn phenomenon.” This is something we would want to consider identifying in one or several features.

# How to Select Features Once You Have Generated Them

Suppose you have generated many features automatically to describe your large time series data set. You may not be able to look at all proposed features on a first pass through your data, so it can be helpful to complement the use of automatic feature generation with the use of automatic feature selection. One helpful feature selection algorithm is the FRESH algorithm, which is implemented in the tsfresh package described earlier. FRESH stands for feature extraction based on scalable hypothesis tests.

The FRESH algorithm is motivated by the ever-increasing amount of time series data that is available, often stored in a distributed manner, which facilitates parallelization of computation. The algorithm evaluates the significance of each input feature with respect to a target variable via the computation of a $\boldsymbol { p }$ -value for each feature. Once computed, the per-feature $\boldsymbol { p }$ -values are evaluated together via the Benjamini-Yekutieli procedure, which determines which features to keep based on input param‐ eters about acceptable error rates and the like. The Benjamini-Yekutieli procedure is a method of limiting the number of false positives discovered during hypothesis testing used to produce the $\boldsymbol { p }$ -values in the initial step of the FRESH algorithm.

To implement these steps on our own would be quite a daunting task, but we can accomplish this in a few lines of code via tsfresh. Here we follow the code used in an illustration in the module’s documentation. First, we download time series data relat‐ ing to robot execution failures:

```python
## python
>> from tsfreshexamplesrobot_execution_failures import
 download_robot_execution_failures,
 load_robot_execution_failures
>> download_robot_execution_failures()
>> timeseries, y = load_robot_execution_failures() 
```

Then we extract the features without needing to specify them, because the package automatically calculates all features. In this sense, it goes against the advice given in this chapter in being extremely inclusive without concern for computational resour‐ ces. In this test data set there are not too many data points, but you would likely not want to blindly deploy this on your data set without shrinking it down to a reason‐ able number of points:

```python
## python
>> from tsfresh import extract_features
>> extracted_features = extract_features(timeseries, column_id = "id", column_sort = "time") 
```

While tsfresh does provide a way for you to specify which features you would like to calculate, in this example we simply opt to include all features. You can also manually set the parameters for those features that take parameters into account for their cal‐ culation rather than using the defaults. This is all described and illustrated in the doc‐ umentation.

If you do perform a full extraction, as we have with the example data provided by tsfresh, you can see that there are numerous features calculated:

```python
```python
python
>> extracted_features.columns
Index(['F_x_abs_energy', 'F_x_ABS_sum_of Changes'],
    'F_x_agg_autocorrelation_f_agg_mean'],
    'F_x_agg_autocorrelation_f_agg_median'],
    'F_x_agg_autocorrelation_f_agg_var'],
    'F_x_agg(linear_trend_f_agg_max_chunk_len_10_attr',
        "intercept"', 'F_x_agg(linear_trend_f_agg_max_chunk_len_10_attr',
        "rvalue"', 'F_x_agg(linear_trend_f_agg_max_chunk_len_10_attr',
        "slope"', 'F_x_agg(linear_trend_f_agg_max_chunk_len_10_attr',
        "stderr"', 'F_x_agg(linear_trend_f_agg_max_chunk_len_50_attr',
        "intercept"', ...
    'T_z_time_reversal_asymmetry_statistic_lag_1',
    'T_z_time_reversal_asymmetry_statistic_lag_2',
    'T_z_time_reversal_asymmetry_statistic_lag_3',
    'T_z_value_count_value_inf', 'T_z_value_count_value_0',
    'T_z_value_count_value_1', 'T_z_value_count_value_inf', 
```

```python
'T_z_value_count_value_nan', 'T_z_variance', 'T_z_variance_larger Than_std_deviation'], dtype='object', name='variable', length=4764) 
```

There are 4,764 columns. This is a far greater number of features than we could have calculated by hand, but this is also very time-consuming to run on a realistic data set. As you decide how and when to deploy such an outsize set of features, try to be realis‐ tic about both your computing power and your ability to carefully review the results. Remember that for time series data, outliers can be particularly influential in a nasty and unhelpful way for subsequent analysis. You will want to ensure that the features you choose are resistant to outliers.

While the FRESH algorithm is helpful in accounting for dependence among the fea‐ tures, it is somewhat difficult to reason about. We can also use a more traditional and transparent feature selection technique, recursive feature elimination (RFE). We can use RFE to complement our use of the FRESH algorithm and enhance our under‐ standing of the degree of difference between those features selected by the FRESH algorithm and those not selected.

![](images/73cb9f4bf4c9349727dd55af6354ab65aae993ea1e11ec6cd5829dd5494898aa.jpg)

RFE describes an incremental approach to feature selection whereby features are gradually eliminated from a more inclusive model, creating a less inclusive model down to the minimum num‐ ber of features to be included, which is set at the start of the selec‐ tion procedure.

This technique is known as backward selection because you start with the most inclusive model and move “backward” to a simpler model. In contrast, in forward selection features are incrementally added until the maximum number of specified features, or some other stopping criterion, is reached.

We can use RFE both for feature selection and also as a way of ranking feature importance. To run an experiment, we combine 10 randomly selected features from the list of features kept by the FRESH algorithm with 10 randomly selected features from the list of features rejected by the FRESH algorithm:

```txt
## R
>> xidx = random.sample(range(len(features-filtered.columns)), 10)
>> selX = features filtied-iloc[:, xidx].values
>> unselected_features = list(set(extracted_features.columns)
    .difference(set/features filtied.columns))
>> unselected_features = random/sample(unselected_features, 10)
>> unsel_xidx = [idx for (idx, val) in enumerate( extracted_features.columns) if val in unselected_features]
>> unselX = extracted_features-iloc[:, unsel_xidx].values
>> mixed_X = np.hstack([selX, unselX]) 
```

With this set of 20 features, we can then perform RFE to get a sense of the ranked importance of these features for the data set and for the model we use within the RFE:

>>svc $=$ SVC(kernel $\equiv$ "linear",C=1)   
>>rfe $=$ RFE( estimator $\equiv$ svc，n_features_to_select $\equiv 1$ ，step $\equiv 1$ >>rfe.fit(mixed_X,y)   
>>rfe.ranking_   
array([9,12,8,1,2,3,6,4,10,11, 16,5,15,14,7,13,17,18,19,20])

Here we can see the relative rankings of the 20 features we fed into the RFE algo‐ rithm. We would hope that the first 10 features—those selected, among others, by the FRESH algorithm—would be ranked higher than those not selected by the FRESH algorithm. This is largely but not exclusively the case. For example, we can see that in the second half of the array, representing the ranks of the unselected features, we actually have the 5th and 7th most important features out of all 20. However, we would not expect a perfect match-up, and the results are largely consistent.

We can use RFE on the selected features as a way of further culling our features. We can also use it as a sanity check if we attempt to fine-tune the input parameters of the FRESH algorithm, or the number of features we are generating as input to the FRESH algorithm in the first place.

Notice that the FRESH algorithm itself is essentially parameter free, so the number and quality of the features we input are our best way of affecting its output. The other parameter we set for the FRESH algorithm is the fdr_level, which is the percentage of irrelevant features we expect after generating the features. This parameter defaults to .05, but you might decide to set this value much higher to enhance the selectivity of the feature filtering, particularly when you are generating a large number of fea‐ tures without any consideration of whether they are appropriate for your domain of interest.

# Concluding Thoughts

In this chapter we have discussed the motivation for feature selection as well as a sim‐ ple example of how feature generation can work to convert even a short time series to a more compressed set of numbers that is nearly as informative as the original. We also looked at examples of two Python modules designed to implement automated feature generation and selection on time series data, which can handily generate thousands of features of a time series. Because there is a danger that many of the fea‐ tures generated this way will not be especially useful, we also looked at methods for selecting the most useful features to pass on further down our analytical pipeline so that feature generation does not produce noisy or uninformative features.

Feature generation is useful for a number of purposes:

• Producing downstream data about time series in a format that is conducive to use in machine learning algorithms, most of which are designed to accept sets of features per data point rather than a time series.   
• Summarizing time series data in a way that compresses temporal observations into the shorthand of a few numbers and qualitative indicators. This can be use‐ ful not just for analysis but also for storing time series data in a more succinct and readable format in cases where we do not need to keep the full time series.   
• Providing a common set of metrics to describe, and identify similarities across, data that may have been measured under many different conditions. By summa‐ rizing our data more broadly, we can make data comparable that otherwise may not seem easy to compare.

In Chapter 9 we will use feature generation to prepare data input for a number of machine learning algorithms that rely on input of time series features rather than raw time series data for classification and forecasting purposes.

# More Resources

• On feature-based time series analysis:

Ben D. Fulcher, “Feature-Based Time-Series Analysis,” eprint arXiv:1709.08055, 2017, https://perma.cc/6LZ6-S3NC.

This approachable review paper was written by the same person who imple‐ mented the tsfresh algorithm in Matlab. It offers an extensive taxonomy of different kinds of features that can be implemented on time series data. It also includes helpful illustrations of the categories of features discussed. There is also an emphasis on the use of feature generation as a form of data exploration to understand the appropriate analysis for a given time series data set.

• On feature selection:

Maximilian Christ et al., “Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh—A Python package),” Neurocomputing 307 (2018): 72–7, https://oreil.ly/YDBM8.

This article introduces the tsfresh Python package and included FRESH algorithm, which were discussed in this chapter. Information about the com‐ putational efficiency of specific features to compute and sample usage pat‐ terns are also presented.

Maximilian Christ et al., “Distributed and Parallel Time Series Feature Extraction for Industrial Big Data Applications,” paper presented at ACML Workshop on

Learning on Big Data (WLBD), Hamilton, NZ, November 16 2016, https:// arxiv.org/pdf/1610.07717.pdf.   
This more technical explanation and testing of the FRESH algorithm pro‐ vides more details about how the algorithm works and why it is appropriate for industrial time series application, and an extensive list of background reading in the references section.   
• On domain-specific features:   
Wikipedia, “Technical Analysis for Financial Markets,” https://perma.cc/8533- XFSZ.   
This Wikipedia article gives a history of how technical analysis was devel‐ oped for the financial markets as well as an extensive listing of many popu‐ larly used indicators.   
Amjed S. Al-Fahoum and Ausilah A. Al-Fraihat, “Methods of EEG Signal Features Extraction Using Linear Analysis in Frequency and Time-Frequency Domains,” ISRN Neuroscience 2014, no. 730218 (2014), https://perma.cc/465U-QT53.   
This article provides an example of testing standard time series features in the domain of EEG signals. There is an entire academic industry of generat‐ ing features for medical time series, and this paper provides an accessible example of how it’s done.   
Juan Bautista Cabral et al., “From FATS to Feets: Further Improvements to an Astronomical Feature Extraction Tool Based on Machine Learning, Astronomy and Computing 25 (2018), https://perma.cc/8ZEM-Y892.   
This article discusses a recent redesign of a Python package designed for the extraction of features from astronomical time series data. While this docu‐ ment assumes some familiarity with astronomy and the specific packages, it’s still a useful read for getting an idea of how software is designed for feature extraction and for understanding some specific challenges and changes that have been occurring in astronomical time series data in particular.   
Alvin Rajkomar et al., “Supplementary Information for Scalable and Accurate Deep Learning for Electronic Health Records,” npj Digital Medicine 1, no. 18 (2018), https://perma.cc/2LKM-326C.   
This supplementary information relates to an interesting paper published by Google illustrating a highly successful use of deep learning to understanding electronic health records. The supplementary information provides detailed information about how healthcare records were turned into quantitative input features that could be accessed by a neural network. If you are not familiar with deep learning, it may be worth returning to this resource only after reading Chapter 10. Additionally, it would be a good idea to first get a basic understanding of embeddings for deep learning.

• On periodograms:

“The Periodogram,” lecture notes, Eberly College of Science, Pennsylvania State University, https://perma.cc/5DRZ-VPR9.   
These lecture notes from Penn State’s course on Applied Time Series Analy‐ sis offer an introduction to what a periodogram is, how to interpret one, and how to calculate one using R.

Jacob T. VanderPlas, “Understanding the Lomb-Scargle Periodogram,” Astrophys‐ ical Journal Supplement Series 236, no. 1 (2018), https://arxiv.org/pdf/ 1703.09824.pdf.

This expansive article provides an intuitive understanding of periodicity esti‐ mators generally and the Lomb-Scargle method in particular.

# Machine Learning for Time Series

In this chapter, we will look at a few examples of applying machine learning methods to time series analysis. This is a relatively young area of time series analysis but one that has shown promise. The machine learning methods we will study were not origi‐ nally developed for time series–specific data—unlike the statistical models we studied in the past two chapters—but they have proven useful for it.

This turn to machine learning is a shift from our previous work in forecasting in ear‐ lier chapters of this book. Up to this point we have focused on statistical models for time series forecasts. In developing such models, we formulated an underlying theory about the dynamics of a time series and the statistics describing the noise and uncer‐ tainty in its behavior. We then used the hypothesized dynamics of the process to make predictions and also to estimate our degree of uncertainty about the predic‐ tions. With such methods, both model identification and parameter estimation required that we think carefully about the best way to describe the dynamics of our data.

We now turn to methodologies in which we do not posit an underlying process or any rules about that underlying process. We instead focus on identifying patterns that describe the process’s behavior in ways relevant to predicting the outcome of interest, such as the appropriate classification label for a time series. We will also con‐ sider unsupervised learning for time series, in the form of time series clustering.

We cover prediction and classification with tree-based methodologies as well as clus‐ tering as a form of classification. In the case of tree-based methodologies, formulating features of our time series is a necessary step along the way of using the methodology, as trees are not a “time-aware” methodology, unlike, say, an ARIMA model.

In the case of clustering and distance-based classification, we will see that we have the option to use features or to use the original time series as an input. To use the time

series itself as an input, we study a distance metric known as dynamic time warping, which can be applied directly to time series, preserving the full chronological set of information in our data rather than collapsing it into a necessarily limited set of features.

# Time Series Classification

In this section we work through an example of converting raw electroencephalogram (EEG) time series data into features, which can in turn be used for machine learning algorithms. We then use decision tree methods to classify EEG data after we have extracted features from the EEG time series.

# Selecting and Generating Features

In the previous chapter, we went through a general discussion of the purposes of time series feature generation. We also worked through a brief example of generating fea‐ tures for a time series data set via tsfresh. Now we will generate features with another time series feature package that was discussed: cesium.

One of the very handy attributes of the cesium package is that it comes with a variety of helpful time series data sets, including an EEG data set originally derived from a 2001 research paper. In this paper, you can read more on the data preparation details. For our purposes it suffices to know that the five categories of EEG time series present in the data set all represent equal-length segments cut out of continuous time readings of EEG samples from the following:

• EEG recordings of healthy people with eyes both opened and closed (two sepa rate categories)   
• EEG recordings of epilepsy patients during seizure-free times from two non seizure-related areas of the brain (two separate categories)   
• An intracranial recording of EEG during a seizure (one category).

We download that data set via a convenience function supplied by cesium:

```markdown
## python
>>> from cesium import datasets
>>> eg = datasets fetching_andrzejak() 
```

It can be helpful to first view a few examples of the data we are analyzing to get an idea of how we would like to classify these time series:

```txt
>>> plt.subplot(3, 1, 1)
>>> plt.plot(eeg["measurements"]["0])
>>> plt.legend(eeg['classes'].0])
>>> plt.subplot(3, 1, 2) 
```

```txt
>>>plt.plot(eeg["measurements"][300])
>>>pltlegend(eeg['classes'] [300])
>>>plt.subplot(3, 1, 3)
>>>plt.plot(eeg["measurements"][450])
>>>pltlegend(eeg['classes'] [450]) 
```

These plots show some differences between the classes of EEG measurement (see Figure 9-1). It is not surprising that the EEG plots are markedly different: they are measuring activity in different parts of the brain during different activities in both healthy subjects and epilepsy patients.

![](images/69eec94798021fdaad41e25a45ede7335b2cb142d83f6c6a03f069967d1a900b.jpg)

![](images/9c2d20cb6458cea635fcef8f645b5a69d25d75f9b1280aafb57c717d702de4f8.jpg)

![](images/de2c68e5d15a4cf1b856ba53101cea743442cf3cb74faa5441f427d4a6604d48.jpg)  
Figure 9-1. We plot three randomly selected samples from the EEG data set. These are independent samples and not contemporaneous measurements from different parts of the same brain. Each is an independent time series measurement taken of a different patient at a different time.

These visualizations provide guidance for feature generation. For example, class Z and G seem to have less skewed data than class S. Additionally, each class has quite a different range of values, as we can see by inspecting the y-axis. This suggests that an amplitude feature could be useful. Also, it is not just the overall amplitude but the overall distribution of the points that seems characteristically different in the three

classes. We will use these features as well as a few others in our analysis, and next we show the code to generate such features.

# Visualization Meets Machine Learning

This EEG data would be a great opportunity to make use of a 2D histogram. In par‐ ticular, a 2D histogram would form a handy composite image of each kind of time series and could confirm (or negate) the likely representativeness of the features we’ve just discussed, such as establishing whether amplitude or skew is a consistent difference between classes.

Also useful would be univariate histograms, forming one histogram per class. You could also consider going a step further to compute and plot a kernel density estimate for each class.

A kernel density estimate is like a histogram, but it can be shaped to have properties such as continuity and smoothness, effectively transforming raw histogram data to a smooth but nonparametric estimate of a possible distribution for the underlying vari‐ able. Wikipedia offers a good overview of kernel density estimation, which can be accomplished by a variety of means, such as via scikit-learn in Python or the den sity() function in the R stats package.

Here, we generate the features with cesium:

```python
>>> from cesium import featurize featurize_time_series as ft
>>> features_to_use = ["amplitude",
>>> "percent_beyond_1_std",
>>> "percent_close_to_median",
>>> "skew",
>>> "max_slope"]
>>> fset_cesium = ft(times = eeg["times"], 
>>> values = eeg["measurements"], 
>>> errors = None,
>>> features_to_use = features_to_use,
>>> scheduler = None) 
```

This produces our features as depicted in Figure 9-2, a screenshot from a Jupyter notebook.

```txt
fset_cesium.head() 
```

<table><tr><td>feature</td><td>amplitude</td><td>percent_beyond_1_std</td><td>percent_close_to_median</td><td>skew</td><td>max_slope</td></tr><tr><td>channel</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>143.5</td><td>0.327313</td><td>0.505004</td><td>0.032805</td><td>11107.796610</td></tr><tr><td>1</td><td>211.5</td><td>0.290212</td><td>0.640469</td><td>-0.092715</td><td>20653.559322</td></tr><tr><td>2</td><td>165.0</td><td>0.302660</td><td>0.515987</td><td>-0.004100</td><td>13537.627119</td></tr><tr><td>3</td><td>171.5</td><td>0.300952</td><td>0.541128</td><td>0.063678</td><td>17008.813559</td></tr><tr><td>4</td><td>170.0</td><td>0.305101</td><td>0.566268</td><td>0.142753</td><td>13016.949153</td></tr></table>

Figure 9-2. Numerical values of features we generated for the first few samples in our data set.

Note that many of these values are not normalized, so that would be something we’d want to keep in mind were we using a technique that assumed normalized inputs.

We should also confirm that we understand what our features are indicating and that our understanding matches what cesium computes. As an illustration of error check‐ ing and common sense affirmation, we can verify the percent_beyond_1_std for one time series sample:

```python
>>> np.std(eeg_small["measurements"]["θ])
40.411
>>> np.mean(eeg_small["measurements"},{"θ})
-4.132
>>> sample-ts = eeg_small["measurements"},{"θ]
>>> sz = len(sample-ts)
>>> ll = -4.13 - 40.4
>>> ul = -4.13 + 40.4
>>> qual = [i for i in range(sz) if sample-ts[i] < ll or sample-ts[i] > ul ]
>>> len(quals)/len(ser)
0.327 ## this checks out with feature generated in Figure 9-2 
```

![](images/18b266eb43c636b2ecb944a320946c92329663bbc626e3c73e1e2afd915f0bd2.jpg)

# Features Should Be Ergodic

In choosing features to generate for a time series, be sure to select features that are ergodic, meaning that the values measured will each converge to a stable value as more data from the same process is collected. An example where this is not the case is a random walk, for which the measurement of the mean of the process is meaningless and will not be ergodic. The mean of a random walk will not converge to a specific value.

In the EEG data we’ve plotted, different subsamples from any given time series are clearly comparable and the series itself is weakly sta‐ tionary, so the features we generated make sense.

You should be able to verify any of the features you are using. This is a simple matter of responsible analysis. You should not be providing information to your algorithm that you cannot understand, explain, and verify.

![](images/4ff0ea92d189b803306fdff1422a9db65f8f10ac79c4b6e6e851ef813207d668.jpg)

Don’t overuse feature generation libraries. It’s not difficult to write your own code to generate features. If you work in a domain where a particular set of features is generated often and in the same com‐ binations repeatedly, you should write your own code even if you have initially used a package.

You will be able to optimize your code in ways that authors of a general exploratory package cannot. For example, if you have sev‐ eral features that rely on the calculation of the mean of a time ser‐ ies, you can create code to calculate that mean only once rather than once per separate feature calculated.

# Decision Tree Methods

Tree-based methods mirror the way humans make decisions: one step at a time, and in a highly nonlinear fashion. They mirror the way we make complicated decisions: one step at a time, thinking about how one variable should affect our decision, and then another, much like a flow chart.

I assume you have already worked with decision trees or can quickly intuit what deci‐ sion trees are. If you could use more support, pause here and check out some back‐ ground reading.

Figure 9-3 shows a simple example of a decision tree that might be used to make an estimate of someone’s body weight.

![](images/b92982c71f10f4769abd7ee6b6f03d5809e19639ffa72e80c0938b520711851f.jpg)  
Figure 9-3. In this simple regression tree, a series of logical branches are used to arrive at a predicted weight for a human. This is a crude model, but it illustrates the nonlin‐ ear and variable approach that even a simple tree model can apply to a regression problem.

There are abundant examples of humans behaving like a decision tree when analyz‐ ing time series data. For example, a discretionary stock market trader may use techni‐ cal indicators, but they will likely use them in a serial hierarchical fashion, just like a tree—first asking, for example, which direction the momentum is trending according to one technical indicator before then asking how volatility is evolving over time, with the answer to this second question interacting with the first in a tree-like nonlin‐ ear fashion. Likely they have something like a decision tree structure in their brain that they use to make forecasts about how the market will move.

Similarly, when a medical professional reads an EEG or ECG, it is not uncommon to first look for the presence of one feature before considering another, sequentially working through a series of factors. If one feature is present and another isn’t, it will lead to a different diagnosis than in the converse case and hence a different forecast regarding a patient’s prognosis.

We will use the features we generated from the EEG data as inputs to two different decision tree methods, random forests and gradient boosted trees, each of which can be used for classification tasks. Our task will be classifying the EEG data we’ve dis‐ cussed solely on the basis of the features we generated from the raw data.

# Random forest

A random forest is a model in which we use not one decision tree but many. Our classification or regression is the result of averaging the outputs of these trees. Ran‐ dom forests look to the “wisdom of the crowd” where the crowd is composed of many simple models, none of which may itself be particularly good, but all of which together often outperform a highly refined but single decision tree.

The idea of assembling a collection of models to produce a forecast rather than merely striving to find the single “best” model was articulated as early as 1969 in The Combination of Forecasts, a research paper by two venerable statisticians, J.M. Bates and C.W.J. Granger. That paper showed that combining two separate forecasts of air‐ line passenger data could lead to a model that had lower mean squared error, a sur‐ prising and, at the time, unintuitive result. The younger generation of analysts, who often have worked their way into data analysis via machine learning rather than sta‐ tistics, finds such an idea intuitive rather than unsettling, and the random forest has become a workhorse for all sorts of forecasting problems.

A random forest is constructed according to parameters specifying the number of trees to train, as well as the maximum allowed depth of those trees. Then, for each individual tree, a random sample of the data and of its features is used to train that tree. The trees are generally parameterized to be quite simple so that overfitting can be avoided and the model can average over many general models, none of which is especially good but all of which are sufficiently general to avoid “traps” in the data.

As mentioned earlier, we will input the features computed for each time series sample into the model as our training outputs. In theory, we could think of ways to input our raw time series data, rather than the computed features, but there are a number of problems with this:

• It would be complicated to deal with time series of unequal length.   
• Such a large number of inputs (equal to or close to the number of time steps) would result in computationally expensive models and training.   
• Assuming no particular time step is very important (since any given time step would correlate to one feature), there would be a great deal of noise and very lit‐ tle signal to train on from the perspective of the tree, which would be seeing each time step as an input.

So random forests are not a good tool for working with time series data in its raw form, but they can be useful for working with it once it has been compressed into summary features. Here are a few specific reasons:

• From an efficiency/computational resources perspective, it is wonderful to think that we can distill extremely long time series into a handful of features and find a model with reasonable accuracy.   
• It is helpful that a random forest reduces the risk of overfitting. As we have dis‐ cussed previously, overfitting is particularly a problem for time series analysis because of unfortunate synergies between overfitting and lookahead. Having a deliberately dumb/simple methodology combats some of this concern.

• Random forests may be particularly apt for time series data for which we do not have a working model or hypothesis regarding the underlying mechanics of the process.

As a general rule, analysts have had more success deploying random forests for cases of time series classification rather than for cases of time series forecasting. In con‐ trast, the next method we discuss, gradient boosted trees, has been largely successful at both tasks.

# Gradient boosted trees

Boosting is another way of building an ensemble of predictors. Boosting creates mod‐ els sequentially with the idea that later models should correct the mistakes of earlier models and that data misfit by earlier models should be more heavily weighted by later models.

Gradient boosted trees have become the go-to boosting methodology, and a particu‐ larly successful one for time series as evidenced by their success in some data science competitions in recent years.

XGBoost works by building trees sequentially, with each tree seeking to predict the residuals of the combination of prior trees. So, for example, the first tree built by XGBoost will attempt to match the data directly (a category or a numeric value). The second tree will attempt to predict the true value minus the predicted value. The third tree will attempt to predict the true value minus the first tree’s predicted value minus the second tree’s prediction of the first tree’s residuals.

However, XGBoost does not simply build models infinitely, attempting to minimize the residuals of the predicted residuals of the predicted residuals ad infinitum. The XGBoost algorithm minimizes a loss function that also includes a penalty term for model complexity, and this penalty term limits the number of trees that will be pro‐ duced. It is also possible to directly limit the number of trees produced.

Over the last few years many have reported much greater success with XGBoost than with machine learning methods for time series, such as in Kaggle competitions or at industry machine learning conferences.

![](images/2d18ef7b551a7dd73e7091c8260c24b54aec43c36e74ab4f9c31cef3c41c35c1.jpg)

Bagging (or, more formally, bootstrap aggregating) refers to a tech‐ nique for training models wherein randomly generated training sets are created for each different model in an ensemble. Random forests generally use a bagging methodology in model training.

Boosting, as noted, refers to a technique for training models in which an ensemble is composed of sequentially trained models, each of which focuses on correcting the error made by its predeces‐ sor. Boosting is at the core of how gradient boosted tree models are trained.

# Code example

In the case of random forests and XGBoost, it can be easier to code a machine learn‐ ing model than it is to understand how that model works. In this example, we will train both a random forest and a gradient boosted tree model to classify our EEG data based on the features we generated.

We use sklearn to divide our data into training and testing data sets:

```python
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = train_test_split(
    fset_cesium_values, eeg["classes"], random_state=21) 
```

We begin with a random forest classifier. Here we see the ease with which we can create a model to classify our EEG data:

```python
>>> from sklearn ensemble import RandomForestClassifier
>>> rf_clf = RandomForestClassifier(n_estimators = 10,
>>>     max_depth = 3,
>>>     random_state = 21)
>>> rf_clf.fit(X_train, y_train) 
```

We are then able to determine the out-of-sample accuracy of our data with a method call on the Classifier object:

```markdown
## python
>>> rf_clf.score(X_test, y_test)
0.616 
```

In just a few lines of code, we have a model that can do better than I would be able to do as a human classifier (one without a medical education). Remember, too, that thanks to feature selection this model sees only summary statistics rather than an entire EEG.

The code for the XGBoost classifier is similarly straightforward and succinct:

```txt
## python
>>> import xgboost as xgb
>>> xgb_clf = xgb.XGBClassifier(n_estimators = 10, 
```

```txt
>>> max_depth = 3,
>>> random_state = 21)
>>> xgb_clf.fit(X_train, y_train)
>>> xgb_clf.score(X_test, y_test)
0.648 
```

We can see that the XGBoost classifier model does slightly better than the random forest model. It also trains slightly faster, as we can see with this quick experiment to calculate the training time for each model:

```txt
>>> python
>>> start = time.time()
>>> xgb_clf.fit(X_train, y_train)
>>> end = time.time()
>>> end - start
0.0189
## Random Forest
>>> start = time.time()
>>> rf_clf.fit(X_train, y_train)
>>> end = time.time()
>>> end - start
0.027 
```

This execution speed is a substantial improvement, with the random forest taking $5 0 \%$ more time than XGBoost. While this is not a definitive test, it does point to an advantage of XGBoost, particularly if you are dealing with large data sets. You’d want to make sure that this advantage scaled up when you used larger data sets with more examples and more features.

# This Is Not the Way to Do Performance Testing!

There’s a lot to criticize about the preceding code. For starters, there are more com‐ pact ways to write a method to test performance. Also, you’d want to average the training time and run it on realistic data in realistic training conditions, not a toy example in a Jupyter notebook.

Most importantly, just because a commonly available algorithm runs slowly doesn’t mean the algorithm is intrinsically slow. Sometimes algorithms in modules are writ‐ ten more for code transparency than for performance, and this can mean they run slower than is necessary. Other times the algorithms are implemented for the most general case possible, whereas you may find that your data needs are highly special‐ ized in ways that mean you can modify standard implementations of algorithms to suit your purposes.

We will discuss performance considerations specific to time series in Chapter 12.

We could fairly ask whether there is something about our particular set of hyperpara‐ meters that gave the advantage to XGBoost over a random forest. For example, what if we use less complex trees by setting a lower depth? Or what if we allow fewer total decision trees to exist in the model? It’s quite easy to test these possibilities, and again we see that XGBoost tends to maintain its edge.

For example, if we allow the same number of decision trees in the ensemble but lessen the complexity by decreasing the depth of the trees, we see that the gradient boosted model maintains a higher accuracy than does the random forest model:

```python
>>> Test the same number of trees (10) but with less complexity
>>> ## (max_depth = 2)
>>> 
>>> XGBoost
>>> xgb_clf = xgb.XGBClassifier(n_estimators = 10,
                          max_depth = 2,
                          random_state = 21)
>>> xgb_clf.fit(X_train, y_train)
>>> xgb_clf.score(X_test, y_test)
0.616
>>> # Random Forest
>>> rf_clf = RandomForestClassifier(n_estimators = 10,
                          max_depth = 2,
                          random_state = 21)
>>> rf_clf.fit(X_train, y_train)
>>> rf_clf.score(X_test, y_test)
0.544 
```

This is true even when we reduce the tree complexity further:

```python
>>> Test the same number of trees (10) but with less complexity
>>> ## (max_depth = 1)
>>> # XGBoost
>>> xgb_clf = xgb.XGBClassifier(n_estimators = 10,
                          max_depth = 1,
                          random_state = 21)
>>> xgb_clf.fit(X_train, y_train)
>>> xgb_clf.score(X_test, y_test)
0.632
>>> # Random Forest
>>> rf_clf = RandomForestClassifier(n_estimators = 10,
                          max_depth = 1,
                          random_state = 21)
>>> rf_clf.fit(X_train, y_train)
>>> rf_clf.score(X_test, y_test)
0.376 
```

There are several possible reasons to explain the performance and the putative advantage of gradient boosted trees over random forests. One important considera‐ tion is that we do not know for certain that all the features we selected for our classifi‐ cation are particularly useful. This highlights an example of when boosting (gradient boosted trees) could be preferable to bagging (random forest). Boosting will be more likely to ignore useless features because it will always make use of the full set of fea‐ tures and privilege the relevant ones, whereas some trees that result from bagging will be forced to use less meaningful features.

This also suggests the helpfulness of boosting when paired with the supercharged fea‐ ture generation libraries we discussed in the last chapter. If you do take the approach of generating hundreds of time series features—far more than you can reasonably inspect—boosting may be a safeguard from truly disastrous results.

![](images/b22caac13c943eb1e6e1537719090ec933073d28937a69c4dfd9cde82ca6c888.jpg)

Gradient boosted trees are particularly useful for large data sets, including large time series data sets. The implementation you choose will likely affect your accuracy slightly and your training speed. In addition to XGBoost you should also consider LightGBM and CatBoost. These latter packages have sometimes been reported to perform quite a bit faster than XGBoost, though sometimes at a slight decrease in out-of-sample test accuracy.

# Classification versus regression

In the previous examples we considered random forests and gradient boosted tree methodologies for time series classification. These same methodologies can also be used for time series predictions.

Many statisticians argue that machine learning has been less successful—or no more successful—than traditional time series statistical analysis in the domain of forecast‐ ing. However, in the last several years, gradient boosted trees for prediction have taken off and are indeed often outperforming traditional statistical models when given sufficiently large data sets, in both forecasting competitions and in industry applications. In such cases, however, extensive time is taken to tune the models’ parameters as well as to prepare time series features.

One of the strengths of gradient boosted tree models is that they approach “autopi‐ lot” in their ability to weed out irrelevant or noisy features and focus on the most important ones. This tendency alone, however, will not be enough to get state-of-theart performance from a model. Even for a seemingly automatic method, such as gra‐ dient boosted trees, the outputs can only be as good as the inputs. The most important way to improve your model will still be to provide high-quality and welltested input features.

There are many options for improving upon the current model. We could learn from it by using XGBoost’s option to produce feature importance metrics. This could help us identify traits of useful features and nonuseful features, and we could then expand the data set by adding similar features to those that are already judged useful. We could also do a hyperparameter grid search to tune our model parameterization. Finally, we could look at our mislabeled data’s raw time series to see whether there are traits of the mislabeled data that may not be represented by our current set of fea‐ tures. We could consider adding features that might better describe the mislabeled data, augmenting our inputs further.

# Clustering

The general idea of clustering is that data points that are similar to one another con‐ stitute meaningful groups for purposes of analysis. This idea holds just as true for time series data as for other kinds of data.

As with our earlier discussion, I assume that you have some familiarity with the rele‐ vant machine learning concepts in a non-time-series context. If you are not familiar with clustering techniques, I recommend that you pursue some short background reading on clustering before continuing this section.

Clustering for time series can be used both for classification and for forecasting. In the case of classification, we can use clustering algorithms to identify the desired number of clusters during the training phase. We can then use these clusters to estab‐ lish types of time series and recognize when new samples belong to a particular group.

In the case of forecasting, the application can be pure clustering or can be inspired by clustering in the form of using relevant distance metrics (more on distance metrics soon). There are a few options for generating forecasts at a horizon, $h$ , in the future based on clustering and related techniques. Remember that in this case, we will not have fully observed a time series but only its first $N$ steps, from which we want to forecast its value at time step $N + h$ . In such a case there are a few options.

One option is to use class membership to generate a forecast based on typical behav‐ ior in that class. To do this, first determine which cluster a time series sample belongs to based on its first $N$ time steps and then infer likely future behavior based on cluster membership. Specifically, look at how values of time series in this cluster tend to change their values between time step $N$ and time step $N + h .$ . Note that you would want to perform the original clustering for all the time series based on their first $N$ steps rather than all parts of the time series to avoid lookahead.

Another option is to predict future behavior of a sample time series based on the behavior of its nearest neighbor (or neighbors) in the sample space. In this scenario, based on metrics from the first $N$ time steps, find a time series sample’s nearest

neighbor(s), for which the full trajectory is known. Then average the $N + h$ behavior of these nearest neighbors, and this is your forecast for the current sample.

In the case of both classification and forecasting, the most important consideration is how to assess similarity between time series. Clustering can be done with a variety of distance metrics, and much research has been devoted to thinking about how to measure distance in high-dimensional problems. For example, what is the “distance” between two job applicants? What is the “distance” between two blood samples? These challenges are already present in cross-sectional data, and they persist in time series data.

We have two broad classes of distance-metric options when applying clustering tech‐ niques to time series data:

# Distance based on features

Generate features for the time series and treat these as the coordinates for which to calculate data. This does not fully solve the problem of choosing a distance metric, but it reduces the problem to the same distance metric problem posed by any cross-sectional data set.

# Distance based on the raw time series data

Find a way to determine how “close” different time series are, preferably in a way that can handle different temporal scales, a different number of measurements, and other likely disparities between time series samples.

We will apply both of these distance metrics to a time series data set in which each sample represents the projection of a handwritten word from a 2D image to a 1D time series.

# Generating Features from the Data

We have already discussed ways of generating and selecting features. Here we con‐ sider how to assess the distance between time series data sets based on the similarity of their features.

In an ideal scenario, we would already have culled unimportant or uninteresting time series, perhaps by using a tree to assess feature importance. We do not want to include such features in a distance calculation since they may falsely indicate dissimi‐ larities between two time series, when in fact they simply do not indicate a relevant similarity relative to the classes in our classification task or the outcome in our fore‐ casting task.

As we did with the EEG data set, we start our analysis by taking a look at some class examples and noticing what obvious differences there are over time and in the struc‐ ture of the time series.

Our data is a subset of the FiftyWords data set available from the UEA and UCR Time Series Classification Repository. This data set was released by the authors of a 2003 paper on clustering handwritten words in historical documents. In that paper, the authors developed “word profiles” as a way of mapping the 2D image of a hand‐ written word into a 1D curve, consisting of the same number of measurements regardless of word length. The repository data set is not identical to that in the paper, but the same principles apply. The purpose of the original paper was to develop a method of tagging all similar or identical words in a document with a single label so that humans could go back and label these words digitally (a feat that nowadays might very well be directly accomplished by a neural network, with 20 years of tech‐ nological improvement to help with the task).

So in this example we see sample projection profiles of the words, where “projection” refers to the fact that they converted an image from a 2D space to a 1D space, and this latter space, since ordering mattered, is amenable to time series analysis. Note that the “time” axis is not actually time but rather left-to-right progressions of written words. Nonetheless the concept is the same—ordered and equally spaced data—so for simplicity I will use the words time and temporal in the analysis even though this is not strictly true. For our use case, there isn’t a distinction.

# In Figure 9-4 we see examples of a few distinct words.1

When we examine this data, and particularly when we consider the patterns obvious to the human eye in the plots, just as with the EEG we can formulate a starting point of some features to consider for our analysis—for example, the heights and locations of the peaks as well as their characteristics, such as how sharply they rise and what shape there is at the top of the peak.

Many of the features I am describing begin to sound more like image recognition fea‐ tures than like a time series, and this is a helpful perspective to have for feature gener‐ ation. After all, visual data is often data we can easily process and have more intuition about than time series data. It could be helpful to think in terms of images when thinking about features. It is also a perspective that illustrates why generating features can be surprisingly difficult. In some cases it may be obvious from examining a time series how to distinguish two classes, but we may find that how to write the code is not so obvious. Or we may find that we can write the code, but it is extremely taxing.

![](images/a1180575aec2df6c0384b584d6b9dc4de3c1c27b7db030c4d0c62a53da391f31.jpg)

![](images/82e487bccfce8e5458bc5bf307d05771400bd5314abedbe9a56209fd6b58cd07.jpg)

![](images/ba88003d79d2fdf3469d8a2c7ffdd1b55bfddd84e791290c32297a9af8f71236.jpg)  
Figure 9-4. The projection profiles of three different words (12, 23, and 9) are quite dif‐ ferent from one another. We also already see some features that could distinguish these words from one another: the temporal location (x-axis) of the largest peak or even of the second-largest peak, how many local peaks there are, the overall range of values, and the average convexity of the curves.

In the last decade, deep learning has emerged as the strongest performer for image classification, and with sufficient data, we could train a deep learning classifier on the images of these plots (more on this in Chapter 10). For now, it will be helpful for us to think of ways around the difficulty of programming. For example, it would be dif‐ ficult to generate features locating each peak, as peak finding is programmatically demanding and something of an art form.

We can also use a 1D histogram, either of all the class examples or of an individual example. This may suggest computationally less taxing ways either of identifying peaks or finding other proxy values that will map onto the overall shapes we see in the time series. Here we plot the same individual class members plotted before, now accompanied by their 1D histograms (see Figure 9-5):

```prolog
>>> plt.subplot(3, 2, 1)
>>> plt.plot words.iloc[1, 1:-1])
>>> plt.title("Word = " + str words(word[1]), fontweight='bold')
>>> plt.subplot(3, 2, 2)
>>> plt.hist words.iloc[1, 1:-1], 10)
>>> plt.subplot(3, 2, 3)
>>> plt.plot words.iloc[3, 1:-1]
>>> plt.title("Word = " + str words(word[3]), fontweight='bold')
>>> plt.subplot(3, 2, 4)
>>> plt.hist words.iloc[3, 1:-1], 10)
>>> plt.subplot(3, 2, 5)
>>> plt.plot words.iloc[5, 1:-1]
>>> plt.title("Word = " + str words(word[11]), fontweight='bold')
>>> plt.subplot(3, 2, 6)
>>> plt.hist (words.iloc[5, 1:-1], 10) 
```

![](images/d59f50dc2a053dce8b229fce3cc6ce948bba491aa74d64b4f0f121854a6dbef8.jpg)

![](images/213257a312139e0e11a22549147947bc28be36cf5d9ce10b33cffd4daf84f42a.jpg)

![](images/41b3c61c9156864592afdf928fba95c08bec5a3ccbf8f2e98cd7460682a2617f.jpg)

![](images/dd5a299ba87a76f2bd04ed44c0995a9ac5c7832f2dd62c12ce5a57acb17677dd.jpg)

![](images/f4e0b4c66ecad322dfb085603a8dd3f44fa5edea829f8448c9905d9d2244cf48.jpg)

![](images/0a5aa55d6500bec4e60c2cd448758880f4d4b45c2ac3bc3ad333b2d5173f94a6.jpg)  
Figure 9-5. Another way of measuring classes to brainstorm useful features. In particu‐ lar, the histogram of individual class examples indicates that attributes of the histo‐ gram, such as the number of local peaks, the skewness, and the kurtosis would be helpful and possibly good proxies for some of the attributes of the time series curves themselves that are obvious to the human eye but not easy to identify with code.

We also want to make sure that the examples we are looking at are not outliers com‐ pared to the other examples for these words. For this reason, we construct the 2D his‐ togram for two words to get an idea of the individual variation (see Figure 9-6):

```python
```python
>>> x = np.array([[])
>>> y = np.array([[])
>>> w = 12
>>> selected_words = words[words(word == w)] 
```

```python
>>> selected_words.shape   
>>>   
>>> for idx, row in selected_words.iterrrows(): y = np.hstack([y, row[1:271]])   
>>> x = np.hstack([x, np.array(range(270))])   
>>>   
>>> fig, ax = plt.subplot() 
```

![](images/817ab29848b5079a32fd00c5e65ea05911b9cbf94f343ca9ae98b5762eda7da0.jpg)  
Figure 9-6. A 2D histogram of the 1D word projections for word $\ l = 1 2 .$ . The y-axis is the value at a given time step, and the x-axis represents the 270 time steps for each time series sample/word projection.

Figure 9-6 shows the 2D histogram for all the word $= 1 2$ members of the data set. While the individual curve in Figure 9-5 suggested that we focus on finding the two large peaks that seemed to dominate the time series, here we see that what most of the members of this class have in common is likely the flat span between these peaks, which appears to go from around time steps 120 through 200, based on the intensity of the points in that region.

We can also use this 2D histogram to establish a cutoff point for the maximum peak for this class, which seems to vary in location between time steps 50 and 150. We may even want to code up a feature that is as specific as “is the maximum value reached between points 50 and 150?”

We plot another 2D histogram for the same reason, this time choosing word class 23, which has many small bumps in the example we plotted in Figure 9-5, a difficult-toquantize feature (see Figure 9-7).

![](images/237c9262b1cebf4c11b608d8ed3493eaba711199a89849d9ac7438cb239c0b60.jpg)  
Figure 9-7. A 2D histogram of the 1D word projections for word $l = 2 3$ . The y-axis is the value at a given time step, and the x-axis represents the 270 time steps for each time series sample/word projection.

It is not surprising to see a particularly “smeared” histogram for word class 23 in Figure 9-7, given that even the example we plotted in Figure 9-5 showed so many fea‐ tures that we would expect to see a lot of smearing in the 2D histogram if the features did not match up exactly between samples. However, we also see that the maximum point value in this class comes at a nonoverlapping range of time steps here, as com‐ pared to word class 12. For this class, the maximum value comes after 150, which makes sense given that the two largest peaks we saw in the word class 23 example were in this range. The 2D histogram tends to substantiate that the earlier peak is not as tall as the later peak, suggesting other ways to quantize the shape of this time series to distinguish it from other classes.

The 2D histograms are helpful in letting us know the variability of a feature within an individual class, so that we do not unduly rely on a single class example when think‐ ing about how to form our features.

In this case we choose to generate a set of features derived from the shape of the word projection, and an additional set of features derived from the shape of the word pro‐ jection’s histogram (projecting a 1D summary into a different 1D summary, from which we generate features). This is in response to the large “smears” we see in the 2D histograms, indicating that there are peaks but that their location is not

particularly stable. Using a histogram to generate a second characteristic shape for each word projection may prove more reliable and characteristic than the word pro‐ jection itself. Histograms characterize what kinds of values appear in the series without characterizing their location in the series, which is what is important to us given that the peaks in the projections don’t have especially stable temporal locations within the time series.

First, we generate features for the times series, which are 270 time steps in length apiece. In this case, we shorten the name of the function used to generate features for code readability:

```python
from cesium import featurize.featurize_time as ft
```python
>>> word_vals = words.iloc[:, 1:271]
>>> times = []
>>> word_values = []
>>> for idx, row in word_vals.iterrrows():
>>>     word_values.append(row.values)
>>>     times.append(np.array([i for i in range(row.values.shape[0])]))
>>> 
>>> features_to_use = ['amplitude',
>>>         'percent_beyond_1_std',
>>>         'percent_close_to_median']
>>> featurized_words = ft(times = times,
>>>             values = word_values,
>>>                 errors = None,
>>>                 features_to_use = features_to_use,
>>>                 scheduler = None) 
```

Next we generate histograms and use these as another time series for which to gener‐ ate features:2

```python
>>> create some features derived from histogram
>>> times = []
>>> hist_values = []
>>> for idx, row in words_features.iterrrows():
>>> hist_values.append(np.histogram(row.values,
>>> bins=10,
>>> range=(-2.5, 5.0))[0] + .0001)
>>> ## os cause downstream problems
>>> times.append(np.array([i for i in range(9)]))
>>> features_to_use = ["amplitude",
>>> "percent_close_to_median", 
```

```txt
>>> "skew"  
>>> ]  
>>>  
>>> featurized_hists = ft(times = times,  
>>> values = hist_values,  
>>> errors = None,  
>>> features_to_use = features_to_use,  
>>> scheduler = None) 
```

We made sure that all histograms use the same number of bins and the same range of values as the basis for the bins, via the parameters we pass to np.histogram(). This ensures that all the histograms are directly comparable, having the same range of bin values, which will be the “temporal” axis when these histograms are run through the time series feature generation. If we did not enforce this consistency, the features generated would not necessarily be meaningful in comparing one histogram to another.

Finally, we combine these two sources of features:

```txt
>>> features = pd.concat([featurized_words.reset_index.drop=True), >>> featurized_hists],
>>> axis=1) 
```

# Temporally Aware Distance Metrics

When running clustering analysis we have to choose a distance metric. With time series features, as we just did, we can apply a variety of standard distance metrics to them, as is done in standard clustering analysis on cross-sectional data. If you are not familiar with the process of choosing a distance metric in such cases, I recommend a short detour to do some background reading.

In this section we will focus on the problem of measuring similarity between time series by defining a distance metric between them. One of the most well-known examples of such metrics is dynamic time warping (DTW). DTW is apt for clustering a time series whose most salient feature is its overall shape, as is the case with our word projection data.

The technique’s name is inspired by the methodology, which relies on temporal “warping” to align time series along their temporal axis so as to compare their shapes. A picture is far more valuable than words for conveying the concept of dynamic time warping, so take a look at Figure 9-8. The temporal (x) axis is warped—that is, expan‐ ded or contracted as convenient—to find the best alignment of points between the two curves depicted (i.e., two time series), in order to compare their shape.

![](images/fd6996e64871f2d0853e686a0f91ececc70df956db5fd6a07a08ef894e8f6a57.jpg)  
Figure 9-8. How dynamic time warping works. Each point on one time series is mapped to a point on the opposite time series, but there is no requirement that there must be a one-to-one mapping of points. This has a few implications: (1) Time series need not be of the same length or even of the same timescale. What matters is the shape. (2) Time does not always move forward during the fitting process, and may not move at the same pace for each time series. By time moving, I mean progressing along the curve in the x-axis direction. Source: Wikipedia.

Note that the actual time values on the time axis of one curve as compared to another are not relevant in the standard form of this algorithm. We could be comparing one time series that is measured in nanoseconds to another that is measured in millennia (although this would likely not be a sensible exercise). The purpose of the algorithm is akin to comparing the visual “shape” of that algorithm rather than thinking about how much time is passing. Indeed, “time” is really only meant in the more general sense of an ordered evenly spaced set of points along the x-axis rather than time proper.

The rules of DTW are as follows:

• Every point in one time series must be matched with at least one point of the other time series.   
• The first and last indices of each time series must be matched with their counter‐ parts in the other time series.   
• The mapping of points must be such that time moves forward and not backward. There is no going back in time by matching a point in one time series with a point in the other time series that has already been passed on the time axis. How‐ ever, time does not need to move forward constantly. For example, two consecu‐ tive time steps in the original series could be warped by being condensed to the same place on the x-axis during the fit, as is illustrated in Figure 9-8 at the first “kink” in the upper curve/solid line.

There are many ways that the temporal alignment can be adjusted to follow these rules, but the selected match is the one that minimizes the distance between the curves. This distance, or cost function, is often measured as the sum of absolute dif‐ ferences between matched points, where the absolute difference is the difference between the points’ values.

Now that we have an idea of how DTW works intuitively, we can take a look at the code:

```python
def distDTW(ts1, ts2):
    ## this is setup
    DTW={'}
    for i in range(len(ts1)):
        DTW[(i,-1)] = np.inf
    for i in range(len(ts2)):
        DTW[-1,i] = np.inf
    DTW[-1,-1] = 0
    ## this is where we actually calculate the optimum
    ## one step at at time
    for i in range(len(ts1)):
        for j in range(len(ts2)):
            dist = (ts1[i] - ts2[j])**2
            DTW[(i,j)] = dist + min(DTW[(i-1,j]), DTW[(i,j-1)], DTW[(i-1,j-1)]) 
        ## this is an example of dynamic programming
    ## once we have found the complete path, we return
    ## the associated distance
    return sqrt(DTW[len(ts1)-1, len(ts2)-1]) 
```

As indicated in the comments, the solution to this problem is an example of dynamic programming, and DTW distance is a classic dynamic programming problem. We can take one step at a time on the path from the beginning of each time series to the end, and we know that we can build on the solution one step at a time and refer back to our earlier knowledge to make later decisions.

There are a number of different DTW implementations, with a variety of ideas for making the search for the optimum solution, or a near-optimum solution, more effi‐ cient. These should be explored, particularly if you are working with a larger data set.

There are also other ways of measuring distances between time series. Here are a few:

# Fréchet distance

This is the maximum distance between two curves during a time-warping-like traversal of the curves that always seeks to minimize the distance between two curves. This distance metric is often explained by the analogy of a dog and its human companion walking on the two curves with a leash between them. They

each need to traverse a separate curve from beginning to end, and they can go at different speeds and vary their speeds along the curve so long as they always move in the same direction. The Fréchet distance is the shortest length of leash necessary for them to complete the task following the optimal trajectory (assum‐ ing they can find it!).

# Pearson correlation

The correlation between two time series can be a way of measuring the distance between them. Unlike with other distance metrics, you will minimize the dis‐ tance between the time series by maximizing the metric of the correlation. Corre‐ lation is relatively easy to compute. However, this method requires that the time series have the same number of data points or that one be downsampled to match the fewer data points of the other. The time complexity of computing cor‐ relation is $O ( n )$ , which makes it a particularly efficient metric from a computa‐ tional resources perspective.

# Longest common subsequence

This distance measure is appropriate for time series that represent a sequence of categorical or integral values. In such cases, to consider the similarity of two time series, we can determine the length of the longest common subsequence, mean‐ ing the longest length of consecutive values that are exactly identical, although their exact location in the time series is not required to match. As with DTW, this means that we are more concerned with finding a shape of commonality rather than where in time the common shape is occurring. Also note that like DTW, but unlike Pearson correlation, this does not require that the time series have the same length. A related measure is edit distance, whereby we find the number of changes we would need to make to one time series to make it identical to another and use this value to define a distance metric.

![](images/0ba0cc6f9e135f89bb47665cbe78ad5e8d2c31c32aa77c78ed3064fd5a7037fa.jpg)

# Distance Versus Similarity

The literature on measuring distances between time series also uses the term similarity to describe these metrics. In most cases, you can treat these terms interchangeably, namely as a way to establish which time series are more or less like one another. That said, some metrics will be proper distances, such as the Fréchet distance, which can be computed with proper units (such as “feet” or ${ } ^ { \infty } \mathrm { k g } /$ dollar” or whatever metric a time series is measuring). Other meas‐ ures are unit-free, such as a correlation.

Sometimes a little creativity can go a long way toward finding a simple but apt solu‐ tion, so it is always a good idea to consider exactly what your needs are and define them as specifically as possible. Consider a Stack Overflow post seeking a distance

metric for a specific application, namely classifying time series to match them to one of three centroids from a prior clustering analysis. The three classes were:

• A flat line.   
• A peak at the beginning of the time series, and otherwise a flat line.   
• A peak at the end of the time series, and otherwise a flat line. The user found that several standard distance metrics, including Euclidean distance and DTW, failed to do the trick. In this case, DTW was too generous and rated any time series with a peak as equally close to the time series with a peak at the end and a peak at the beginning (so DTW is not a panacea, despite being computationally taxing!).

In this case, a clever commenter suggested a transform that would make distance metrics work better, namely to compare the cumulative summed time series rather than the original time series. After this transform, both the Euclidean and DTW dis‐ tances gave the correct ordering such that a time series with a peak at the beginning showed the least distance to the prototype of that class, rather than an equal distance to the prototype with a peak at the beginning of the time series and the prototype with a peak at the end of the time series. This should remind us of earlier analyses we have studied, in which transforming a time series can make ARIMA appropriate even if the raw data does not meet the necessary conditions.

# Avoid Euclidean Distance Measures for Time Series

You may have noticed that Euclidean distance was not mentioned in our discussion of time series similarity measures. Euclidean distances tend to perform poorly in assessing the similarity between time series in a way that we care about. To see why, let’s compare two sine curves and a flat line. If the sine curves have the same period but have a different phase (displacement along the time/x-axis) or if the sine curves have sufficiently different amplitudes, the straight line will have a shorter Euclidean distance to the sine curve than will the other sine curve. This is usually not the desired outcome but shows a few advantages offered by some of the previous distance metrics:

• Ability to compensate for phase—that is, displacement along the time axis that isn’t really important for comparison   
• Ability to recognize similarity of shapes rather than similarity of magnitudes

This is an easy exercise to code up, so give it a try.

Should you decide that a Euclidean distance measure is appropriate, consider using a Fourier transform distance rather than a Euclidean distance. In this case you can use dimension reduction by having a frequency ceiling, which is appropriate for most time series data where higher frequencies tend to be less important to the overall

shape and dynamics of a time series. Another option is to use the Symbolic Aggregate Approximation (SAX) technique (first developed in 2007) to reduce the dimensional‐ ity of the time series data before computing a lower bound on the Euclidean distance.

Unfortunately, there is no “autopilot” for choosing a distance metric. You will need to use your best judgment to find a balance of:

• Minimizing the use of computational resources.   
• Choosing a metric that emphasizes the features of a time series most relevant to your ultimate goal.   
• Making sure that your distance metric reflects the assumptions and strengths/ weaknesses of the analytical methods you are pairing it with. For example, $k$ - means clustering does not use pairwise distances but rather minimizes variances, such that only Euclidean-like distances make sense for this technique.

# Clustering Code

Now that we have discussed how to generate features for clustering analysis and how to measure distance directly between time series as a distance metric for clustering, we will perform the clustering with our selected features and with our pairwise DTW distance matrix to compare the results.

# Hierarchical clustering of normalized features

We computed features for our words-as-time-series for both the time series of the original recording and the histogram of the time series. These features may occur on quite different scales, so if we want to apply a single distance metric to them, we nor‐ malize them as is standard operating procedure for feature-based clustering generally:

```txt
## python
>>> from sklearn import preprocessing
>>> feature_values = preprocessing.scale features.values) 
```

We select a hierarchical clustering algorithm and perform a fit for 50 clusters, since we are seeking to match these clusters to the 50 words in our data set:

```python
```python
>>> from sklearn.cluster import AgglomerativeClustering
>>> feature_clustering = AgglomerativeClustering(n_clusters = 50,
>>>     linkage = 'ward')
>>> feature_clustering.fit(feature_values)
>>> words['feature_labels'] = feature_clustering.fit_prediction(p) 
```

Then we want to see whether the clusters (whose labels are arbitrary with respect to the original word labels) show useful correspondences to the word labels:

```txt
## python
>>> from sklearn.metrics.cluster import homogeneity_score
>>> homogeneity_score(words(word, words feature_labels)
0.508 
```

We are lucky that we are working with labeled data; we might otherwise draw incor‐ rect conclusions based on the clusters we formed. In this case fewer than half of the clusters relate strongly to a single word. If we go back and think about how to improve this result, we have a number of options:

• We only used six features. This is not a large number of features, so we could add more.   
• We could look for features that would be relatively uncorrelated, which we did not do here.   
• We are still missing obviously useful features. There were some features we noticed from the visual exploration of the data that we did not include, such as the number and location of distinctive peaks. We should probably rework this analysis to include that information or some better proxy for it.   
• We should explore using other distance metrics, perhaps some that will weight certain features more strongly than others, privileging the features the human eye finds useful.

# Hierarchical clustering with the DTW distance matrix

We have already completed the difficult portion of direct clustering based on time series clustering by calculating the pairwise distance matrix via DTW. This is compu‐ tationally taxing, which is why we are careful to save the results in case we want to revisit the analysis:

```python
>>> p = pairwise_distance(X, metric = distDTW)
>>> ## this takes some time to calculate so worth saving for reuse
>>> with open("pairwise_word_distance.npy", "wb") as f:
np.save(f, p) 
```

Now that we have them, we can use a hierarchical clustering algorithm:

```python
>>> from sklearn.cluster import AgglomerativeClustering
>>> dtw_clustering = AgglomerativeClustering(linkage = 'average',
>>> n_clusters = 50,
>>> affinity = 'precomputed')
>>> words['dtw_labels'] = dtw_clustering.fit_prediction(p) 
```

And finally, as before, we compare the correspondence between the fitted clusters and the known labels:

```markdown
## python
>>> from sklearn.metrics.cluster import homogeneity_score,
>>> completeness_score
>>> homogeneity_score words(word, words.dtw_labels)
0.828
>>> completeness_score (words-word, words.dtw_labels)
0.923 
```

We see that this DTW-based clustering does substantially better than our featurebased clustering. However, if you run the DTW distance computation code on your own computer—particularly if it’s a standard-issue laptop—you will see just how much longer the DTW takes to compute compared to the features we chose. It’s likely that we can improve our feature-based clustering, whereas the DTW distance clustering, now that it’s computed, does not leave any straightforward means for improvement. Our alternatives to improving this would be:

• Include features as well as the DTW distance. This is tricky both from a coding perspective and also from the conceptual perspective of deciding how to combine the features with the DTW distance.   
• Try other distance metrics. As discussed earlier, the appropriate distance metric will depend on your data, your goals, and your downstream analysis. We would need to define our goal for this word analysis more narrowly and geometrically, and then we could think about whether DTW is really the best metric for what we want to accomplish.

# More Resources

• On time series distance and similarity measures:

Meinard Müller, “Dynamic Time Warping,” in Information Retrieval for Music and Motion (Berlin: Springer, 2007), 69–84, https://perma.cc/R24Q-UR84.

This chapter of Müller’s book offers an extensive overview of dynamic time warping, including a discussion of common approximations made to reduce the computational complexity of computing DTW.

Stéphane Pelletier, “Computing the Fréchet Distance Between Two Polygonal

Curves,” (lecture notes, Computational Geometry, McGill University, 2002), https://perma.cc/5QER-Z89V.

This set of lecture notes from McGill University offers an intuitive visual and algorithmic explanation of what the Fréchet distance is and how it can be calculated.

Pjotr Roelofsen, “Time Series Clustering,” master’s thesis, Business Analytics, Vrije Universiteit Amsterdam, 2018, https://perma.cc/K8HJ-7FFE.   
This master’s thesis on time series clustering begins with a helpful and very thorough discussion of the mainstream techniques for calculating distances between time series, including information about the computational com‐ plexity of the distance calculation and helpful illustrations that assist with building intuition.

Joan Serrà and Josep Ll. Arcos, “An Empirical Evaluation of Similarity Measures for Time Series Classification,” Knowledge-Based Systems 67 (2014): 305–14, https://perma.cc/G2J4-TNMX.

This article offers empirical analysis of out-of-sample testing accuracy for classification models built with seven different measures of time series simi‐ larity: Euclidean distance, Fourier coefficients, AR models, DTW, edit dis‐ tance, time-warped edit distance, and minimum jump costs dissimilarity. The authors tested these measures on 45 publicly available data sets from the UCR time series repository.

• On machine learning for time series:

Keogh Eamonn, “Introduction to Time Series Data Mining,” slideshow tutorial, n.d., https://perma.cc/ZM9L-NW7J.

This series of slides gives an overview to preprocessing time series data for machine learning purposes, measuring distance between time series, and identifying “motifs” that can be used for analysis and comparison.

Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos, “The M4 Competition: Results, Findings, Conclusion and Way Forward,” International Journal of Forecasting 34, no. 4 (2018): 802–8, https://perma.cc/42HZ-YVUU.

This article summarizes results of the M4 competition in 2018, which com‐ pared a variety of time series forecasting techniques, including many ensem‐ ble techniques, on a randomly selected set of 100,000 time series, including data collected at various frequencies (yearly, hourly, etc.). In this overview of the competition results, the authors indicate that a few “hybrid” approaches, relying heavily on statistics but also with some machine learning compo‐ nents, took both first and second place in the competition. Results such as these point to the importance of understanding and deploying both statisti‐ cal and machine learning approaches to forecasting.

# Deep Learning for Time Series

Deep learning for time series is a relatively new endeavor, but it’s a promising one. Because deep learning is a highly flexible technique, it can be advantageous for time series analysis. Most promisingly, it offers the possibility of modeling highly complex and nonlinear temporal behavior without having to guess at functional forms—which could potentially be a game changer for nonstatistical forecasting techniques.

If you aren’t familiar with deep learning, here’s a one-paragraph summary (we’ll go into more details later). Deep learning describes a branch of machine learning in which a “graph” is built that connects input nodes to a complicated structure of nodes and edges. In passing from one node to another via an edge, a value is multi‐ plied by that edge’s weight and then, usually, passed through some kind of nonlinear activation function. It is this nonlinear activation function that makes deep learning so interesting: it enables us to fit highly complex, nonlinear data, something that had not been very successfully done previously.

Deep learning has come into its own primarily within the past 10 years, as improve‐ ments in commercially available hardware have been coupled with massive amounts of data to enable this kind of heavy-duty model fitting. Deep learning models can have millions of parameters, so one way of understanding them is to dream up just about any graph you can think of, with all sorts of matrix multiplications and nonlin‐ ear transforms, and then imagine setting loose a smart optimizer that optimizes your model one small group of data at a time, continuously adjusting the weights of this large model so they give increasingly good outputs. This is deep learning in a nut‐ shell.

Deep learning has not yet delivered the amazing results for forecasting that it has for other areas, such as image processing and natural language processing. However, there is good reason to be optimistic that deep learning will eventually improve the

art of forecasting while also lessening the brittle and highly uniform nature of assumptions and technical requirements common for traditional forecasting models.

Many of the headaches of preprocessing data to fit a model’s assumptions are gone when deep learning models are used:

• There is no requirement of stationarity.   
• There is no need to develop the art and skill of picking parameters, such as assessing seasonality and order of a seasonal ARIMA model.   
• There is no need to develop a hypothesis about the underlying dynamics of a sys‐ tem, as is helpful with state space modeling.

These advantages should sound familiar after Chapter 9, where we discussed many of the same advantages with respect to applying machine learning to time series. Deep learning is even more flexible for a number of reasons:

• Many machine learning algorithms tend to be fairly brittle in terms of the dimensionality and kinds of input data required for a training algorithm to work. In contrast, deep learning is highly flexible as to the model and the nature of the inputs.   
• Heterogenous data can be challenging with many commonly applied machine learning techniques, whereas it is quite commonly used for deep learning models.   
• Machine learning models rarely are developed for time series problems, whereas deep learning offers a great deal of flexibility to develop architectures specific to temporal data.

However, deep learning is not a magic bullet. Although there is no requirement of stationarity for deep learning applied to time series, in practice, deep learning does not do a good job of fitting data with a trend unless standard architectures are modi‐ fied to fit the trend. So we still need to preprocess our data or our technique.

Also, deep learning does best with numerical inputs in different channels all scaled to similar values between –1 and 1. This means that you will need to preprocess your data even though it’s not theoretically required. Also, you need to preprocess it in a way that avoids lookahead, which is not something the deep learning community as a whole has spent a lot of time perfecting.

Finally, deep learning optimization techniques and modeling for time-oriented neu‐ ral networks (the largest class of which is recurrent neural networks, or RNNs) are not as well developed as those for image processing (the largest class of which is con‐ volutional neural networks, or CNNs). This means you will find less guidance on best

practices and rules of thumb for selecting and training an architecture than you would for nontemporal tasks.

In addition to these difficulties of applying deep learning to time series, you will find the rewards for doing so are a mixed bag. For starters, deep learning’s performance for time series does not consistently outperform more traditional methods for time series forecasting and classification. Indeed, forecasting is an area ripe for improve‐ ments from deep learning, but as of this writing these improvements have not yet materialized.

Nonetheless, there is reason to expect both immediate and long-term benefits to adding deep learning to your time series analysis toolkit. First, large tech companies have begun launching deep learning for time series services with custom architec‐ tures they have developed in-house, often with industry-specific modeling tasks in mind. You can use these services or combine them with your own analyses to get good performance.

Second, you may have a data set that does amazingly well with deep learning for time series. In general, the stronger your signal-to-noise ratio is, the better you will do. I have had more than one fairly new programmer tell me that they succeeded to a sur‐ prising degree with some simple deep learning.

For example, a college found that a simple LSTM (more on this later) did as well as overworked guidance counselors at predicting which students were likely to fail or drop out soon, so that the college could reach out to those students and provide more resources. While the best outcome would be to have more guidance counselors, it is heartwarming to know that a simple LSTM applied to the heterogeneous time series data of a student’s grades and attendance record could help change vulnerable stu‐ dents by flagging them for outreach and enhanced support. In the future, I believe we can expect more of such innovative and assistive uses of deep learning for time series.

![](images/51016ace18ae4bb029e3d5b42bc78bc988c2d403fa96682c3cee17df574437fe.jpg)

Remeber that every model has assumptions. Machine learning models, including neural networks, invariably have assumptions built in, both in the architecture and in training methodologies. Even the fact that most neural networks work best with inputs scaled to $[ - 1 , \ 1 ]$ suggests there are strong assumptions built into the model, even if these have not been well identified yet.

It may even be that neural network forecasts haven’t reached their optimum performance yet, and that better understanding the theo‐ retical underpinnings of and requirements for forecasting neural networks will lead to performance enhancements.

If you are new to deep learning, this chapter will not provide you with all the concep‐ tual and programming equipment you need to get a competent start. However, it will give you a point of departure, and from here, there are many good tutorials, books,

and even online courses that you can use to learn more in depth. The good news about deep learning is that there need be no mathematics involved to get a general flavor of how it works and how it is programmed. Additionally, APIs are available at various levels of specificity. This means that a beginner can get by using fairly highlevel APIs to try out some introductory techniques, and frankly even experts will use these high-level APIs to save time. Later, when some specific architectural innovation is necessary, and as your understanding develops, you can use lower-level APIs where more is left for you to decide and specify concretely.

This chapter will provide a brief review of the concepts that inspired and supported deep learning as a mathematical and computer science pursuit, as well as concrete examples of the code you can use to apply a deep learning model to data.

# Deep Learning Concepts

Deep learning has roots in a number of areas. There was a biological inspiration, with computer scientists and quantitative analysts wondering if the way to build intelligent machines was, ultimately, to mimic brains, with their networks of neurons firing off in response to certain triggers. There was mathematical inspiration in the form of the universal approximation theorem being proven for various activations, with many of these proofs originating in the late 1980s and early 1990s. Finally, there was the growth of computing capabilities and availability coupled with a burgeoning field of machine learning, whose success showed that, with enough data and parameters, complicated systems could be modeled and predicted. Deep learning took all these ideas even further by creating networks described by millions of parameters, trained on large data sets, and with a theoretical grounding that a neural network should be able to represent an arbitrary and nonlinear function to a large degree of accuracy. Figure 10-1 shows a simple neural network, a multilevel perceptron (or fully connec‐ ted network).

In Figure 10-1, we can see how multichannel inputs are served to the model in the form of a vector of dimension d. The nodes represent the input values, while the edges represent the multipliers. All the edges entering into a node represent a previ‐ ous value multiplied by the value of the edge it traversed. These values from all the inputs at a single node are summed and then, usually, passed through a nonlinear activation function, creating a nonlinearity.

We can see that the input consists of three channels, or a vector of length 3. There are four hidden units. We multiply each of the three inputs by a different weight for each of the four hidden units for which it is destined, meaning that we need $3 \times 4 = 1 2$ weights to fully describe the problem. Also, since we will then sum the results of these various multiplications, matrix multiplication is not just analogous to what we are doing but exactly what we are doing.

![](images/00270a7e89adc7bcc76708b6aeba90019a2c4f9135936eaafe5596c3d8b9f038.jpg)  
Figure 10-1. A simple feed forward network.

If we wanted to write out the steps for what we are doing, they would go something like this:

1. Input vector X has three elements. Layer 1 weights are designated by $\mathrm { W } _ { \mathrm { 1 : } }$ , a $4 \times 3$ matrix, such that we compute the hidden layer values as $\mathrm { W } _ { 1 } \times \mathrm { X } _ { 1 }$ . This results in a $4 \times 1$ matrix, but this is not actually the output of the hidden layer:

$$
\mathrm {W} _ {1} \times \mathrm {X} _ {1}
$$

2. We need to apply a nonlinearity, which we can do with various “activation func‐ tions,” such as hyperbolic tan (tanh) or the sigmoid function $( \sigma )$ . We will usually also apply a bias, B1, inside the activation function, such that the output of the hidden layer is really:

$$
\mathrm {H} = \mathrm {a} \left(\mathrm {W} _ {1} \times \mathrm {X} _ {1} + \mathrm {B} _ {1}\right)
$$

3. In the neural network depicted in Figure 10-1, we have two outputs to predict. For this reason we need to convert the four-dimensional hidden state output to two outputs. Traditionally, the last layer does not include a nonlinear activation function, unless we count applying a softmax in the case of a categorization problem. Let’s assume we are just trying to predict two numbers, not two proba‐ bilities or categories, so we simply apply a last “dense layer” to combine the four outputs of the hidden layer per ultimate output. This dense layer will combine four inputs into two outputs, so we need a $2 \times 4$ matrix, $\mathrm { W } _ { 2 }$ :

$$
\mathrm {Y} = \mathrm {W} _ {2} \times \mathrm {H}
$$

Hopefully this gave you some sense of how these models work. The overall idea is to have lots of parameters and opportunities for nonlinearities.

It’s something of an art form to choose the right number and form of parameters, the right training hyperparameters, and, of course, a reasonably accessible problem. The magic comes in learning how to train these parameters, initialize them in a smart way from the start, and make sure the model goes in the right direction toward a reasona‐ bly good solution. This is a nonconvex class of models, and the intention is not about finding the global optimum. Instead, the thinking goes, so long as you find clever ways to regularize the model, you will find a “good enough” local optimum to suit your needs.

# Programming a Neural Network

Understanding how neural networks work in principle can feel quite a bit easier than understanding how the associated programming frameworks applied to this problem actually operate. As we’ll discuss here, however, there are a few broad themes that these frameworks tend to have in common.

# Symbolic and Imperative Programming Styles

You will have some leeway in your deep learning programming to use either a sym‐ bolic or an imperative style of programming. In a symbolic style of programming, you declare all the relationships up front without having them computed at the time of declaration. In contrast, in an imperative style of programming, the computation takes place when it is coded, line by line, without waiting to account for what will come later. There are pluses and minuses to each style, and there is no ironclad divi‐ sion between them. Here are some points to keep in mind when you are thinking about what you prefer, assuming you have the choice:

• Symbolic programming tends to be more efficient because you leave room for the framework to optimize computations rather than perform them when and how they are ordered.   
• Imperative programming tends to be easier to debug and understand.   
• TensorFlow and MXnet tend toward a symbolic programming style, while Torch has a more imperative flavor.

# Data, Symbols, Operations, Layers, and Graphs

Deep learning frameworks often focus on some notion of a graph and building that graph. The idea is that any architecture should be describable in terms of its individ‐ ual components and their relationship to one another. Additionally, the idea of

variables as divorced from actual values is also quite important. So you may have a symbol, A, and a symbol, B, and then a third symbol, C, which is the result of matrix multiplying A and B:

pseudo code symbol A; symbol B; symbol C $=$ matmul(A,B);

The distinction between symbols and data is essential to every framework because of the relationship between the symbol and the data. The symbol is used to learn a more general relationship; the data may have noise. There is only one symbol A, even as we may have millions or even billions of values to feed in for A, each paired with a respective B and C.

# Popular Frameworks

As with many other tech domains, the landscape of offerings in the deep learning world is constantly changing. And just as with the business world, there are often indiscriminate mergers and acquisitions, as with the merging of TensorFlow and Theano, and of MXNet and Caffe. No framework has particularly established itself as a leader for time series deep learning, although MXNet may be the best bet for the near future given Amazon’s own interest in forecasting and its newly unveiled deep learning for time series forecasting services. As someone who often needs to write custom data iterators for time series analysis projects in deep learning, I have found this most easily done in MXNet.

For someone new to the field, at the time of writing the most prominent and widely used libraries, and their respective tech behemoth parents, are:

• TensorFlow (Google)   
• MXNet (Amazon)   
• Torch (Facebook)

However, there are often academic and purely open source upstarts offering new innovations that can take over a domain, or at least make a substantial contribution to an existing framework. Dominance comes and goes quickly, although as the tech giants begin to roll out hardware to match their software, the market leaders may gain more endurance than has been seen in past years.

As we take a step back and think about what we do with data, these are the opera‐ tions. We might add or multiply symbols together, and we can think of these as oper‐ ations. We can also perform univariate operations, such as changing the shape of a

symbol (perhaps converting symbol A from a $2 \times 4$ matrix to an $8 \times 1$ matrix) or passing values through an activation function, such as computing tanh(A).

Taking a step even further back, we can think of layers as those traditional units of processing we associate with common architectures, such as a fully connected layer, which we explored in the previous section. Taking into account the activation func‐ tion and bias as well as the core matrix operation, we can say:

$$
\text {l a y e r} \mathrm {L} = \tanh  (\mathrm {A} \times \mathrm {B} + \text {b i a s})
$$

In many frameworks this layer may be expressible as one or two layers rather than several operations, depending on whether a combination of operations is popular enough to warrant its own designation as a unit of a layer.

Finally, we can relate multiple layers to one another, with one passing to the next:

$$
l a y e r L 1 = \tanh  (A \times B + b i a s 1)
$$

$$
\text {l a y e r} \mathrm {L} 2 = \tanh  (\mathrm {L} 1 \times \mathrm {D} + \text {b i a s} 2)
$$

The entire conglomerate of these symbols, operations, and layers results in a graph. A graph need not be fully connected—that is, it will not necessarily be the case that all symbols have dependencies on one another. A graph is used precisely to sort out what symbols depend on what other symbols and how. This is essential so that when we are computing gradients for the purposes of gradient descent, we can tweak our weights with each iteration for a better solution. The nice thing is that most modern deep learning packages do all this for us. We don’t need to specify what depends on what and how the gradient changes with each layer added—it’s all baked in.

What’s more, as I mentioned before, we can use high-level APIs such that we do not need to spell out matrix multiplications as I did in the preceding example. Rather, if we want to use a fully connected layer, which is the equivalent of a matrix multiplica‐ tion, followed by a matrix addition, and then an element-wise activation function of some kind, we don’t need to write out all the math. For example, in mxnet, we can accomplish this with the following simple code:

```python
## python
>>> import mxnet as mx
>>> fc1 = mx.gluon.nn(Dense(120, activation='relu')) 
```

This will give us a fully connected layer that converts inputs to an output dimension of 120. Furthermore, this one line represents all that I just mentioned: a matrix multi‐ plication, followed by a matrix addition, followed by an element-wise activation function.

You can verify this in the documentation, or by trying sample outputs and inputs when you know the weights. It’s really quite impressive how much the API doesn’t require you to do. You are not required to specify the input shape (although this must be a constant once you have built your graph so that it can be deduced). You are not required to specify a data type—this will default to the very sensible and most commonly used value of float32 (float64 is overkill for deep learning). If your data happens to come in a non-1D shape per example, this layer will also automatically “flatten” your data so that it is in the appropriate shape to be input into a fully con‐ nected/dense layer. This is helpful for a beginner, but once you develop a minimal level of competence it’s good to revisit the documentation to understand just how much is being decided and taken care of for you even in a simple deep learning model.

This simple code, of course, does not require everything you need even for a short deep learning task. You will need to describe your inputs, your targets, and how you want to measure loss. You also need to situate your layer inside a model. We do this in the following code:

```python
```python
>>> #create a net rather than a freestanding layer
>>> from mx.gluon import nn
>>> net = nnSequential()
>>> net.add(nn.Dense(120, activation='relu'),
>>>     nn.Dense(1))
>>> net.initialize(init=True.Xavier())
>>> 
>>> #define the loss we want
>>> L2Loss = gluon.loss.L2Loss()
>>> 
>>> trainer = gluon.Train(net.train_parameters(), 'sgd',
>>>     {'learning_rate': 0.01}) 
```

Finally, assuming we have also set up our data as needed, we can run through an epoch of training (that is, one pass through all the data) as follows:

```txt
## python
>>> for data, target in train_data:
>>>     ## calculate the gradient
>>>     with autograd.record():
>>>         out = net(data)
>>>             loss = L2Loss(output, data)
>>>                 lossbackward()
>>>                 ## apply the gradient to update the parameters
>>>                 trainer.step(batch_size) 
```

It is also easy in most packages, as it is with mxnet, to save your model and accompa‐ nying parameters for later production use or additional training:

```txt
## python
>>> net.save_parameters('model.params') 
```

In the examples that follow, we will use mxnet’s Module API to give you another fla‐ vor of creating graphs and training models.

As a deep learning practitioner, you will eventually want to become familiar with all the major packages and their various APIs (usually packages have, at the least, a very high-level API and a low-level API) so that you are easily able to read sample code. Working knowledge of all major deep learning modules is necessary to keep up with the latest industry practices and academic research, because this information is most easily learned through open source code.

# Automatic Differentiation

The magic of a deep learning model—apart from its sometimes impressive perfor‐ mance—is that it can even learn at all. Standard models can have thousands or even millions of parameters, and these parameters must be tuned to the problem at hand. Generally this involves taking advantage of sophisticated symbolic differentiation techniques made accessible by deep learning frameworks. In past decades, most mathematicians, physicists, and others needing to optimize functions relied on numerical optimization and differentiation, which means that a gradient was compu‐ ted empirically (say, via $f ( x + \delta ) - f ( x ) / \delta )$ . The upside of this was there was no need to teach computers how to perform differentiation explicitly, but the downside is that it was very slow and there was no taking advantage of the fact that the mathe‐ matical relationships are known or even designed.

Automatic differentiation uses the fact that, in computing the relationships between variables in different layers, we actually know exactly how the variables are mathe‐ matically related, and we can differentiate to get a direct expression for the value of a gradient rather than performing intensive numerical computations. More concretely, automatic differentiation makes use of the fact that, particularly in a computer more so than for math more generally, any sequence of mathematical operations is limited to a few simple operations for which the rules of differentiation are known and the chain rule can be applied. Automatic differentiation does this to come up with com‐ putationally tractable expressions for gradients. Once these gradients are available, it is possible to tune the weights over time and thereby improve and train the model.

# Building a Training Pipeline

Throughout this section we will model the same data set: a measurement of hourly electric use in a variety of locations for several years. We’ll preprocess this data so that we look at the change in electric use from hour to hour, a more difficult task than predicting the overall values because we are looking at the most unpredictable part of the time series.

# Inspecting Our Data Set

We use an open data repository of hourly electric measurements provided in a code base demonstrating a new neural network architecture (which we will discuss later in this post).1 To get a sense of what this data looks like, we read it in R and make some fast plots:

```txt
R  
> elec = fread("electricity.txt")  
> elec  
1: 14 69 234 415 215 1056 29 840 226 265 179 148 112 171 229 1001 49 162  
2: 18 92 312 556 292 1363 29 1102 271 340 235 192 143 213 301 1223 64 216  
3: 21 96 312 560 272 1240 29 1025 270 300 221 171 132 185 261 1172 61 197  
4: 20 92 312 443 213 845 24 833 179 211 170 149 116 151 209 813 40 173  
5: 22 91 312 346 190 647 16 733 186 179 142 170 99 136 148 688 29 144 
```

Already from a quick inspection, we can see how many rows and columns of data we have, as well as that there are no timestamps. We have been independently told these timestamps are hourly, but we don’t know when exactly these measurements took place:

```txt
## R
> ncol(elec)
[1] 321
> nrow(elec)
[1] 26304 
```

We can also plot some random samples of the data to get a sense of what it looks like (see Figure 10-2). Since this is hourly data, we know if we plot 24 data points we will have a full day’s worth of data:

```r
## R
> elec[125:148, plot(V4, type = 'l', col = 1,ylim = c(0, 1000))
> elec[125:148, lines(V14, type = 'l', col = 2)]
> elec[125:148, lines(V114, type = 'l', col = 3)] 
```

We also make a weekly plot (see Figure 10-3):

```python
## R
> elec[1:168, plot(V4, type = 'l', col = 1,ylim = c(0, 1000))
> elec[1:168, lines(V14, type = 'l', col = 2)]
> elec[1:168, lines(V114, type = 'l', col = 3)] 
```

Remember that we don’t know what the local hour is for any particular index, although we know the relation between them. However, consistent with our discus‐ sions back in Chapter 2, we could probably guess at what parts of the day represent standard human schedules based on the patterns of electric use. We could likely also

identify the weekend. We don’t do that here, but it could be a good idea to include models relating to time of day and day of the week for a more in-depth analysis and modeling of this data set.

![](images/0473801fe14ea363d2926179c01aacc93fa7ce09bc50b46fb13f4e255881cb37.jpg)  
Figure 10-2. 24 hours sampled from the data set for 3 different locations out of the 321 locations available in the data set. While we do not have a sense of which local hours these indices correspond to, we do see a coherent daily pattern.

![](images/98162f92147c8e503f774aa413ecc3c3cbfbdff5d17b2fa240adb91a0bc27a21.jpg)  
Figure 10-3. A full seven-day cycle of the sampled data for the same three locations as pictured in the daily plot. Our sense of a daily pattern is confirmed with this broader look at the data, indicating that one large peak per day along with some smaller fea‐ tures seems consistent in the behavior.

While we could predict the absolute values of the data as our forecasting technique, this has already been done in both academic papers and blog posts. Instead, we are going to predict the difference of the data. Predicting differences rather than total val‐ ues of a time series tends to be more challenging because the data is noisier, and we can see this even in analogous plots of what we have done earlier (see Figure 10-4):2

```r
## R
> elec(diff[1:168, plot(V4, type = 'l', col = 1,ylim = c(-350, 350)]) > elec(diff[1:168, lines(V14, type = 'l', col = 2])
> elec(diff[1:168, lines(V114, type = 'l', col = 3)) 
```

![](images/78e783838c12b16178eb3140c2a3f44a2e88df7765dfbd2100ddc8a8ce68c904.jpg)  
Figure 10-4. A week-long sample of the same three sites’ differenced power time series, representing the change from hour to hour of electric use. While this series still exhibits a pattern, as did the original series, the unpredictable components of the series are made more apparent as they represent a larger portion of the value of the differenced series than they do of the original series.

If we were going to run a traditional statistical model, a state space model, or even a machine learning model, we would need to do a fair amount of analysis at this point to look at correlations between different electricity-consuming sites in the data set. We would want to see whether there was drift over time in the data and assess stationarity.

You should do these things too for deep learning so you can assess appropriate mod‐ els for a data set and also establish expectations for how well you think your model can perform. However, the beauty of deep learning is that we can move forward even with somewhat messier data and even without passing any specific statistical tests for the quality of our data. In a production environment, we would spend more time on data exploration, but for the purposes of this chapter we will move on to our model‐ ing options.

# Steps of a Training Pipeline

In general when we are modeling with a neural network, there are a few common steps our scripts will always have. These scripts can often be more challenging to write than when we are fitting statistical or traditional machine learning models because our data sets tend to be larger. Also we fit these deep models in batches so that we are using iterators rather than arrays of the full data set.

A data pipeline will include the following steps:

• Make our code easily configurable by importing a preset list of parameters for default training values, which is particularly convenient since there are otherwise so many values to set.   
• Pull the data into memory and preprocess it.   
• Shape the data into the appropriately expected format.   
• Build iterators appropriate to the deep learning module you are using.   
• Build a graph that uses these iterators to know what shape of data to expect; this includes building out the entire model.   
• Set training parameters, such as your optimizer, your learning rate, and how many epochs you will train for.   
• Establish some recordkeeping system both for your weights and for your results from epoch to epoch.

# Making our code easily configurable

The following code shows how we have accomplished these tasks. We start with a list of standard imports:

```txt
>>> from math import floor
>>> 
>>> ## for archiving
>>> import os
>>> import argparse
>>> 
>>> #deep learning module
>>> import mxnet as mx
>>> 
>>> #data processing
>>> import numpy as np
>>> import pandas as pd
>>> 
>>> #custom reporting
>>> import perf 
```

Then, we use a mix of hardcoded variables and adjustable parameters. To some extent these are a matter of experience and your priorities when training. Don’t expect the variables to make much sense yet, as many of these parameters apply to neural network components we will discuss later in the chapter. The main thing is to notice the adjustability of the parameters:

```python
>>> # some hyperparameters we won't tune via command line inputs
>>> DATA_SEGMENTS = {'tr': 0.6, 'va': 0.2, 'tst': 0.2}
>>> THRESHOLD_EPOCHS = 5
>>> COR_THRESHOLD = 0.0005
>>> 
>>> # set up parser
>>> parser = argparse ArgumentParser()
>>> 
>>> # DATA SHAPING
>>> parser.add_argument('--win', type=int, default=24*7)
>>> parser.add_argument('--h', type=int, default=3)
>>> 
>>> # MODEL SPECIFICATIONS
>>> parser.add_argument('--model', type=str, default='rnn_model')
>>> # cnn components
>>> parser.add_argument('--sz-filt', type=str, default=8)
>>> parser.add_argument('--n-filt', type=int, default=10)
>>> # rnn components
>>> parser.add_argument('--rnn-units', type=int, default=10)
>>> 
>>> # TRAINING DETAILS
>>> parser.add_argument('--batch-n', type=int, default=1024)
>>> parser.add_argument('--lr', type=float, default=0.0001)
>>> parser.add_argument('--drop', type=float, default=0.2)
>>> parser.add_argument('--n-epochs', type=int, default=30)
>>> 
>>> # ARCHIVE WORK
>>> parser.add_argument('--data-dir', type=str, default='../data')
>>> parser.add_argument('--save-dir', type=str, default=None)
```

It’s crucial to have many adjustable parameters because training a deep learning model always involves hyperparameter searches to improve your model from your baseline. Commonly tuned hyperparameters affect all aspects of training, from data preparation (how far back in time to look) to model specification (how complex and what kind of a model to build) and training (how long to train, and how large a learn‐ ing rate to use).

There are distinct categories of parameters in the preceding code. First, the data shaping relates to how we want to take our raw input, in this case a CSV full of paral‐ lel differenced time series of electricity use at 321 stations. To shape our data, we need two parameters. The window variable is how far back in time we will allow our models to look when attempting to make predictions. The horizon variable is how far

forward we want to predict. Notice that these aren’t specific to units, such as $^ { \alpha } 5$ minutes,” but rather to time steps, consistent with our practices in earlier chapters. Like other statistical and machine learning models, neural networks care about our computational representation and don’t have any sense of 5 minutes versus 5 eons when looking at data.

The penultimate section, the training details, will often be the most important for hyperparameter optimization and the most commonly adjusted. It’s crucial at the outset to experiment with learning rate and make sure you have not picked some‐ thing wildly off. A good rule of thumb is to start with 0.001 and then adjust up and down by orders of magnitude. It’s not so important to have just the right learning rate, but it is important to have the right order of magnitude.

The model specifications allow us to specify a variety of models (such as an RNN ver‐ sus a CNN) and architectural details about those models. In general, we will want to tune hyperparameters.

For the current examples, we use the following hyperparameters fed to our script at the command line:

--drop=0.2 --win=96 --batch- $\mathtt { n } = \mathtt { 1 } 2 8$ --lr=0.001 --n-epochs=25   
--data-dir=/data/elec --save-dir=/archive/results   
--model=model_will_vary

# Prepping our input data

Once we have configurable parameters, there is a way to provide information to our script such as where to find a file, how far ahead we want to predict, and how far back in time we want to include in our inputs for a given horizon. These configurable parameters are important even at the preliminary step of reading in data and shaping it properly. We also need to arrange infrastructure to work through the data, because neural networks are trained via variations of stochastic gradient descent, which means that training takes place on small batches of data at a time, with one epoch meaning that all data has been used for training (although not all at the same time).

Next we discuss both the high-level process of providing data for training via itera‐ tors and the low-level details of shaping the data that goes into the iterators.

Shaping the input data. In the preceding section, we saw how to form iterators from NumPy arrays, and we took the provision of those NumPy arrays for granted. Now we will discuss how the data is shaped, first conceptually, and then with a code exam‐ ple. We will discuss two data formats, NC and NTC.

We begin our discussion of different input data formats with a worked example that has nothing to do with coding. Let’s imagine we have multivariate time series data, with columns A, B, and C.

<table><tr><td>Time</td><td>A</td><td>B</td><td>C</td></tr><tr><td>t-3</td><td>0</td><td>-1</td><td>-2</td></tr><tr><td>t-2</td><td>3</td><td>-2</td><td>-3</td></tr><tr><td>t-1</td><td>4</td><td>-2</td><td>-4</td></tr><tr><td>t</td><td>8</td><td>-3</td><td>-9</td></tr></table>

We want to build a model that predicts one step ahead, and we want to use the data from the previous two points in time to predict. We want to use the data from A, B, and C to predict A, B, and C. We will call our inputs X and our outputs Y.

We seek to predict Y at time t. At time t, these are the actual values of $\mathrm { Y } = \left[ \mathrm { A } , \mathrm { B } , \mathrm { C } \right]$ :

t 8 –3 –9

We specified that we would have the previous two time points available for all vari‐ ables to make the prediction. That amounts to the following:

<table><tr><td>A, t-1</td><td>A, t-2</td><td>B, t-1</td><td>B, t-2</td><td>C, t-1</td><td>C, t-2</td></tr><tr><td>4</td><td>3</td><td>-2</td><td>-2</td><td>-4</td><td>-3</td></tr></table>

Likewise, to predict Y at time t – 1, we have the following data as our target:

t – 1 4 –2 –4

and we expect to have the following values available to make the prediction:

<table><tr><td>A, t-2</td><td>A, t-3</td><td>B, t-2</td><td>B, t-3</td><td>C, t-2</td><td>C, t-3</td></tr><tr><td>3</td><td>0</td><td>-2</td><td>-1</td><td>-3</td><td>-2</td></tr></table>

If we wanted to store the inputs for both these data points in one data format, it would look like this:

<table><tr><td>Time</td><td>A, time - 1</td><td>A, time - 2</td><td>B, time - 1</td><td>B, time - 2</td><td>C, time - 1</td><td>C, time - 2</td></tr><tr><td>t - 1</td><td>3</td><td>0</td><td>-2</td><td>-1</td><td>-3</td><td>-2</td></tr><tr><td>t</td><td>4</td><td>3</td><td>-2</td><td>-2</td><td>-4</td><td>-3</td></tr></table>

This is called the NC data format, where $N$ indicates individual samples and C indi‐ cates channels, which is another way of describing multivariate information. We will use this data format for training a fully connected neural network, and it is the first option in the method we will eventually discuss that takes input data in CSV format and converts it to NumPy arrays of the proper shape and dimensionality.

On the other hand, we can shape the data differently, in a way that creates a specific axis for time. This is commonly done by putting the data in NTC format, which des‐ ignates the number of samples $\times$ time $\times$ channels. In this case, the sample is each row of the original data—that is, each slice in time for which we want to make a predic‐ tion (and for which we have available data to do so). The time dimension is how far back in time we will look, which in this example is two time steps (and which is speci‐ fied by --win in our sample script for this chapter).

In the NTC format, the input data we formatted before would look something like this for predicting the t – 1 horizon:

<table><tr><td>Time</td><td>A</td><td>B</td><td>C</td></tr><tr><td>t-1</td><td>0,3</td><td>-1,-2</td><td>-2,-3</td></tr></table>

Or if we wanted to represent the input data for both of the samples we produced ear‐ lier, we could do so compactly as follows:

<table><tr><td>Time</td><td>A</td><td>B</td><td>C</td></tr><tr><td>t-1</td><td>0,3</td><td>-1,-2</td><td>-2,-3</td></tr><tr><td>t</td><td>3,4</td><td>-2,-2</td><td>-3,-4</td></tr></table>

This could be joined to the labels we produced for Y like so:

<table><tr><td>Time</td><td>A</td><td>B</td><td>C</td></tr><tr><td>t-1</td><td>4</td><td>-2</td><td>-4</td></tr><tr><td>t</td><td>8</td><td>-3</td><td>-9</td></tr></table>

Neither of these representations is more accurate than the other, but one convenient aspect of the NTC representation is that time has meaning with an explicit temporal axis.

The reason we form two shapes of inputs is that some models prefer one format and some the other. We will use the NTC format for input into convolutional and recur‐ rent neural networks, which we discuss later in this chapter.

# NTC Versus TNC

There are a few things to note about the NTC data storage format just shown. One is that it is highly repetitive. You can avoid this if the order of your data is not impor‐ tant, namely by using the TNC format rather than the NTC format. Within TNC, you can use your “batch” axis to run through different parts of the data in parallel. So for example, we would train three batches at once on the following sequence of numbers:

```solidity
56 29 56 94 10 92 52 32 19 59 88 94 6 57 73 59 95 79 97 38 65 51 27  
18 77 39 4 19 60 38 73 51 65 4 96 96 6 12 62 59 21 49 65 37 64 69  
36 32 48 97 33 44 63 99 10 56 75 20 61 53 71 48 41 2 58 18 4 10 17  
66 64 53 24 36 23 33 38 1 17 59 11 36 43 61 96 55 21 45 44 53 26 55  
99 22 10 26 25 82 54 82 
```

If you prepared these in NTC format, with just one channel your data would look like this:

<table><tr><td>Time</td><td>Values</td></tr><tr><td>t</td><td>56, 29, 56, 94</td></tr><tr><td>t + 1</td><td>29, 56, 94, 10</td></tr><tr><td>t + 2</td><td>56, 94, 10, 92</td></tr></table>

There is a lot of repetition, but the good news is that the algorithm will train in the actual chronological order of the data, as all the batches are closely related in time. In contrast, if we elect the TNC format (N being the number of samples per batch), and if we supposed a batch size of 4 (unrealistically small), we could imagine our data looking like this:

<table><tr><td>Time</td><td>Batch member 1</td><td>Batch member 2</td><td>Batch member 3</td><td>Batch member 4</td></tr><tr><td>t</td><td>29</td><td>77</td><td>32</td><td>64</td></tr><tr><td>t+1</td><td>56</td><td>39</td><td>48</td><td>53</td></tr><tr><td>t+2</td><td>94</td><td>4</td><td>97</td><td>24</td></tr></table>

In this case we can see there is no data repetition in preparing our inputs. Whether we want to train on data sequenced in chronological order or not will depend on the data set. Those who have worked with natural language processing (NLP) in deep learning will recognize that format, which is quite successful for various NLP tasks but not always so helpful in numerical time series data.

Building iterators. Broadly, to provide data to a training routine, we need to provide iterators. Iterators are not unique to deep learning or to Python but rather reflect the general idea of an object that works its way through some kind of collection, keeping track of where it is and indicating when it has worked through an entire collection. Forming an iterator is simple in the case of training data coming from a NumPy array. If X and Y are NumPy arrays, we see that forming iterators is simple:

```markdown
## python
>>>  ____________.
>>>  ____________.
>>>  ____________.
>>>  ____________.
>>>  ____________.
>>>  ____________ 
```

```python
>>> def prepare_ipers(data_dir, win, h, model, batch_n):
>>> X, Y = prepared_data(data_dir, win, h, model)
>>> n_tr = int(Y.shape[0] * DATA_SEGMENTS['tr'])  
>>> n_ta = int(Y.shape[0] * DATA_SEGMENTS['va'])  
>>> X_tr, X_valid, X_test = X[ : n_tr],
>>> X[n_tr : n_tr + n_ta],
>>> X[n_tr + n_ta : ]
>>> Y_tr, Y_valid, Y_test = Y[ : n_tr],
>>> Y[n_tr : n_tr + n_ta],
>>> Y[n_tr + n_ta : ]
>>> iter_tr = mx.io.NDArrayIter(data = X_tr,
>>> label = Y_tr,
>>> batch_size = batch_n)
>>> iter_val = mx.io.NDArrayIter(data = X_valid,
>>> label = Y_valid,
>>> batch_size = batch_n)
>>> iter_test = mx.io.NDArrayIter(data = X_test,
>>> label = Y_test,
>>> batch_size = batch_n)
>>> return (iter_tr, iter_val, iter_test) 
```

Here we have a method to prepare the iterators for the data set, and these iterators wrap numpy arrays that are received by a method called prepared_data() (more on this in a moment). Once the arrays are available, they are broken down into training, validation, and testing data sources, with the training data being the earliest, the vali‐ dation being used as a way to tune hyperparameters with out-of-sample feedback, and testing data held until the end for true testing.3

Notice that the initializer for an iterator takes the input (data), the target value (label), and batch_size parameter, which reflects how many examples will be used per iteration to compute gradients and update the model weights.

# Shaping the data in code

Now that we know the two shapes of data we want to create, we can look at the code that shapes it:

```txt
## python
>>> def prepared_data(data_dir, win, h, model_name): 
```

```python
>>> df = pd.read_csv(os.path.join(data_dir, 'electricity.dff.txt'), sep=','，header=0)   
>>> x = df.as_matrix()   
>>> ## normalize data. notice this creates a lookahead since   
>>> # we normalize based on values measured across the data set   
>>> # so in a less basic pipeline we would compute these as   
>>> # rolling statistics to avoid the lookahead problem   
>>> x = (x - np.mean(x, axis = 0)) / (np.std(x, axis = 0))   
>>> if model_name == 'fc_model': # # NC data format   
>>> # # provide first and second step lookbacks in one flat input   
>>> X = np.hstack([x[1:-1], x[:h]])   
>>> Y = x[h:]   
>>> return (X, Y)   
>>> else: # # TNC data format   
>>> # preallocate X and Y   
>>> # X shape = num examples * time win * num channels (NTC)   
>>> X = np.zeros((x.shape[0] - win - h, win, x.shape[1]))   
>>> Y = np.zeros((x.shape[0] - win - h, x.shape[1]))   
>>> for i in range(win, x.shape[0] - h):   
>>> # # the target/label value is h steps ahead   
>>> Y[i-win] = x[i + h - 1 , :]   
>>> # # the input data are the previous win steps   
>>> X[i-win] = x[(i - win) : i , :]   
>>> return (X, Y) 
```

After reading our data in from a text file, we standardize each column. Notice that each column is standardized on its own rather than across the whole data set homo‐ geneously. This is because we saw even in our brief data exploration that different electric sites had very different values (see the plots in Figures 10-2 and 10-4):

```txt
>>> python
>>> x = (x - np.mean(x, axis = 0)) / (np.std(x, axis = 0)) 
```

NC data format. Producing the NC data format is fairly straightforward:

```python
python
>>> if model_name == 'fc_model':  # NC data format
>>>  # provide first and second step lookbacks in one flat input
>>> X = np.hstack([[x[1:-h], x[0:-(h+1)]])
>>> Y = x[(h+1):] 
```

To generate the $X$ inputs representing time $t - h$ , to make a prediction at t, we take $x$ and remove the last $h$ rows (since that input data would require label values later in time than the latest data we have). Then we shift this data back along the time axis to produce further lagged values, and we need to make sure the NumPy arrays repre‐ senting different lags have the same shape so they can be stacked together. This is what leads to the preceding formulation. It’s worth working through this on your

own computer and proving that it works out. You might also think about how to generalize the expression for an arbitrarily long lookback.

We check our work by setting a Pdb breakpoint and verifying that the values in X and Y match their expected counterparts in x:

```python
## python
(Pdb) X[0, 1:10] == x[1, 1:10]
array(['True, True, True, True, True, True, True, True', (Pdb) X[0, 322:331] == x[0, 1:10])
array(['True, True, True, True, True, True, True, True', (Pdb) Y[0, 1:10] == x[4, 1:10])
array(['True, True, True, True, True, True, True, True', (Pdb) 
```

The first half of the columns in X represent the last points in time we make available for prediction, and the label/target for the prediction is three steps ahead of this. That’s why X[0, 1:10] should match $\times [ 1 , ~ 1 { : } 1 0 ]$ , and Y[0, 1:10] should match x[4, 1:10], because it should be three time steps ahead (our input set the horizon to 3).

It can be confusing that time and samples (data point indices) often have the same label, but they are separate things. There is the time ahead we are forecasting, there is the time at which we are taking the snapshot of inputs to make a forecast, and there is the time back we look at to collect data to make the forecast. These values are neces‐ sarily interrelated, but it’s a good idea to keep the concepts separate.

NTC data format. Producing the NTC format is also not too bad:

```python
>>> # preallocate X and Y
>>> # X shape = num examples * time win * num channels (NTC)
>>> X = np.zeros((x.shape[0] - win - h, win, x.shape[1])
>>> Y = np.zeros((x.shape[0] - win - h, x.shape[1]))
>>> for i in range(win, x.shape[0] - h):
>>>     ## the target/label value is h steps ahead
>>>     Y[i-win] = x[i + h - 1 , :]
>>>     ## the input data are the previous win steps
>>>     X[i-win] = x[(i - win) : i , :] 
```

For any given example (the $N$ dimension, i.e., the first dimension), we took the last win rows of the input data across all columns. That is how we create three dimen‐ sions. The first dimension is essentially the data point index, and the values provided in that data point amount to 2D data, namely time $\times$ channels (here, electricity sites).

As before, we set a Pdb breakpoint to test this code. Note also that we confirm our own understanding of what we have done to test the code. Often the code to test a data format is more illuminating than the actual code because we use concrete num‐ bers to do spot checks:

```txt
## python
(Pdb) Y[0, 1:10] == x[98, 1:10]
array([ True, True, True, True, True, True, True, True, True])
(Pdb) X.shape
(26204, 96, 321)
(Pdb) X[0, :, 1] == x[0:96, 1]
array([ True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]) 
```

We see that the first data point we have prepared in X and Y (that is, the first row) corresponds to rows 0:96 (because we set our configurable window lookback to be 96 time steps in our parser inputs), and the forward horizon 3 time steps ahead corre‐ sponds to row 98 (because x ends at 95; remember, indexing on a slice excludes the last number in the slice, so x represents all the rows from 0 to 95 inclusive or 0 to 96 exclusive).

Data processing code is bug-prone, messy, and slow. However, you will find that the more times you write it and work through it, the more sense it makes. Nonetheless, it’s good to test your data processing code thoroughly and then keep it somewhere safe so you can avoid having to work through the sample problems every time you need to shape your data. It’s also wise to keep this code in a versioning system and have some way to track which version of your code was used in training a particular model.

# Setting training parameters and establishing a recordkeeping system

We will discuss the details of various models in the coming sections, so for now we bypass the portion of the code that relates to graph building and go directly to train‐ ing and recordkeeping logistics.

This is how we implement training in the simple examples we will focus on:

```python
def train():
    def train():
        def train:
            def names, iter_train, valid_iter, iter_test,
                data_names, label_names,
                save_dir):
                    save_training information/results
                    if not os.path.exists(args.save_dir):
                        os.makedirs(args.save_dir)
                    printFile = open(os.path.join(args.save_dir, 'log.txt'), 'w')
                    def print_to_file(msg):
                        print(name) 
```

print(msg)   
print(msg, file = printFile, flush = True)   
## archiving results header   
print_to_file('Epoch Training Cor Validation Cor')   
#storing prior epoch's values to set an improvement threshold   
#terminates early if progress slow   
buf $=$ RingBuffer(THRESHOLD_EPOCHS)   
old_val $=$ None   
#mxnet boilerplate   
#defaults to 1gpu, of which index is 0   
devs $=$ [mxgpu(0)]   
module $=$ mx.mod-modulesymbol, data_names $\equiv$ data_names, label_names $\equiv$ label_names, context $\equiv$ devs)   
module.bind(data_shapes $\equiv$ iter_trainprovide_data, label_shapes $\equiv$ iter_trainprovide_label)   
module.init_parameters(mx.initializer.Uniform(0.1))   
module.init_optimizer(optimizer $\equiv$ adam', optimizer_parameters={'learning_rate': args.lr})   
#training   
for epoch in range(args.n_epochs):   
iter_train.reset()   
iter_val.reset()   
for batch in iter_train: # compute predictions module.forward(batch,is_train=True) # compute gradients module.backup() # update parameters module.update()   
#training results   
train_pred $=$ module.predict(iterator_train).asnumpy()   
train_label $=$ iter_train.label[0][1].asnumpy()   
train_perf $=$ perf.write_eval(train_pred,train_label, save_dir,'train',epoch)   
# validation results   
val_pred $=$ module.predict(iterator_val).asnumpy()   
val_label $=$ iter_val.label[0][1].asnumpy()   
val_perf $=$ perf.write_eval(val_pred, val_label, save_dir,'valid',epoch)   
print_to_file(%d %f %f' % (epoch,train_perf['COR'],val_perf['COR'])   
#if we don't yet have measures of improvement, skip

```python
>>> if epoch > 0:  
>>> buf.append(val_perf['COR'] - old_val)  
>>> # if we do have measures of improvement, check them  
>>> if epoch > 2:  
>>> vals = buf.get()  
>>> vals = [v for v in vals if v != 0]  
>>> if sum([v < COR_THRESHOLD for v in vals]) == lenvals):  
>>> print_to_file('EARLY EXIT')  
>>> break  
>>> old_val = val_perf['COR']  
>>>  
>>> testing  
>>> test_pred = module.predict(iter_test).asnumpy()  
>>> test_label = iter_test.label[0][1].asnumpy()  
>>> test_perf = perf.write_eval(test_pred, test_label, save_dir, 'tst', epoch)  
>>> print_to_file('TESTING PERFORMANCE')  
>>> print_to_file(test_perf) 
```

# Why Use a Ring Buffer?

A ring buffer is a place to put values such that the first in is the first out when you retrieve a value. It also has the property of growing only up to a preset value. While performance and memory size are not an issue here, using this class helps us avoid boilerplate.

Python doesn’t have a built-in ring buffer, but you can find simple examples online. They are handy whenever you want to keep track of a small number of values in the order they were received.

The preceding code accomplishes a diverse set of tasks, all routine. First, we set up values to track the history of the validation accuracy scores as a way to make sure training is seeing improvements. If a model is not training sufficiently quickly, we don’t want to keep spinning our GPUs, wasting both time and electricity.

The MXNet boilerplate uses the Module API (as opposed to the Gluon API we saw earlier in this chapter):

```python
>>> #mxnet boilerplate
>>> #defaults to 1gpu of which index is 0
>>> devs = [mxgpu(0)]
>>> module = mx.modModulesymbol,
>>> data_names=data_names,
>>> label_names=label_names,
>>> context=devs)
>>> module.bind(data_shape=iter_trainprovide_data,
>>> label_shape=iter_trainprovide_label)
>>> module.init.params(mx.initializer.Uniform(0.1)) 
```

```python
>>> module.init_OPTimizer(optimizer='adam',  
>>> optimizer_parameters={'learning_rate':  
>>> args.lr}) 
```

These four lines of code accomplish the following:

1. Set up the raw component of a neural network as a computational graph.   
2. Set up the data shapes so the network knows what to expect and can optimize.   
3. Initialize all weights in the graph to random values (this is an art, not purely a random set of numbers drawn from infinite possibilities).   
4. Initialize an optimizer, which can come in a variety of flavors and for which we explicitly set the initial learning rate depending on our input parameters.

Next we use our training data iterator to increment our way through the data as we train:

```txt
```python
>>> for epoch in range(args.n_epochs):
>>> iter_train.reset()
>>> iter_val.reset()
>>> for batch in iter_train:
>>> module.forward(batch, is_train=True) # compute predictions
>>> module_backward()
>>> module.update()
```
```
```
```
```
```
```
```
```
`` 
```

We then measure the predicted results both for the training and validation sets (in the same outer for loop as before):

```python
```python
>>> # training results
>>> train_pred = module.predict(train_train).asnumpy()
>>> train_label = iter_train.label[0][1].asnumpy()
>>> train_perf = evaluate_and_write(train_pred, train_label,
>>> save_dir, 'train', epoch)
>>> #
>>> # validation results
>>> val_pred = module.predict(train_val).asnumpy()
>>> val_label = iter_val.label[0][1].asnumpy()
>>> val_perf = evaluate_and_write(val_pred, val_label,
>>> save_dir, 'valid', epoch) 
```

The loop concludes with some logic for early stopping:

```txt
```python
>>> if epoch > 0:
>>>   buf.append(val_perf['COR'] - old_val)
>>> if epoch > 2:
>>>   vals = buf.get()
>>>   vals = [v for v in vals if v != 0]
>>>   if sum([v < COR_THRESHOLD for v in vals]) == lenvals):
>>>   print_to_file('EARLY EXIT') 
```

```txt
>>> break
>>> old_val = val_perf['COR'] 
```

This clunky code does some recordkeeping and simple logic, recording each succes‐ sive correlation value between predicted and actual values. If the correlation from epoch to epoch has failed to improve sufficiently for enough epochs (or has even got‐ ten worse), training will stop.

# Evaluation metrics

Our function, evaluate_and_write, both records the correlations per epoch and the raw value for both the target value and estimated value. We do the same for the test‐ ing at the end of all training:

```python
def evaluate_and_write(pred, label, save_dir, mode, epoch):
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    pred_df = pd.DataFrame(pred)
    label_df = pd.DataFrame.label()
    pred_df.to_csv(os.path.join(save_dir, '%s_pred%d.csv')
                      % (mode, epoch)))
    label_df.to_csv(os.path.join(save_dir, '%s_label%d.csv'
                      % (mode, epoch)))
return {'COR': COR.label, pred} 
```

This in turn makes use of a correlation function we define as follows:

```python
def COR[label, pred):
    label_demeaned = label - label.mean(θ)
    label_sumsquares = np.sum(np(square[label_demeaned], θ))
    pred_demeaned = pred - pred.mean(θ)
    pred_sumsquares = np.sum(np(square(pred_demeaned), θ))
    corcoef = np.diagonal(np.dot[label_demeaned.T, pred_demeaned]) / np.sqrt[label_sumsquares * pred_sumsquares)
    return np.mean(corcoef) 
```

Occasionally in this data set there are cases of zero variance, which can create a NAN in a column, so we opt to use np.nanmean() rather than np.mean().

Notice that one basic functionality we don’t include here is to save the weights of the model, checkpointing throughout the training process. If we were training for pro‐ duction and needed to be able to reload the model and deploy it, we would want to use Module.save_checkpoint (to save the weights) and Module.load (to load a model back into memory, from which point you can continue training or deploy

your model to production). There is a lot to learn to get started with a proper deep learning pipeline, but here we are keeping it basic.

# Putting it together

We put our pipeline components together in the body of our __main__ scope:

```python
python
>>> if __name__ == '__main__':
>>> # parse command line args
>>> args = parser.parse_args()
>>> 
>>> # create data iterators
>>> iter_train, iter_val, iter_test = prepare_iters(
>>> args.data_dir, args.win, args.h,
>>> args.model, args.batch_n)
>>> 
>>> # prepare symbols
>>> input_feature_shape = iter_trainprovide_data[0][1]
>>> 
>>> X = mx.sym.Variable(iter_trainprovide_data[0].name)
>>> Y = mx.sym.Variable(iter_trainprovide_label[0].name)
>>> 
>>> # set up model
>>> model_dict = {
>>> 'fc_model': fc_model,
>>> 'rnn_model': rnn_model,
>>> 'CNN_model': cnn_model,
>>> 'simple_lstnet_model': simple_lstnet_model
}
>>> 
>>> model = model_dict(args.model)
>>> 
>>> symbol, data_names, label_names = model(iter_train,
>>> input_feature_shape,
>>> X, Y,
>>> args.win, args.szfilt,
>>> args.n filt, args.drop)
>>> 
>>> # train
>>> train侖symbol, iter_train, iter_val, iter_test,
>>> data_names, label_names, args.save_dir) 
```

Here we make use of the infrastructure we have just arranged. First, we parse our command-line arguments. Then we create iterators with the configurable inputs, including our prediction horizon, our lookback window, our batch size, and the name of the model we want to build. We create MXNet symbols and also record the input shape, and these are passed along as we create the model. Finally, we pass infor‐ mation about the model, along with our iterators and our save directory, to the train‐ ing function, which does the interesting part: train a model and output its performance metrics.

So in this case, we see a minimal but fully functional training pipeline, featuring data ingestion and reshaping, model construction, model training, and logging of impor‐ tant values for model assessments.

By the way, our print_to_file() method is just a handy wrapper for print():

```python
## python
def print_to_file(msg):
    print(msg, file =-pdfFile, flush = True)esfa
print_to_file(args) 
```

You will want to create a record of your model as it trains. It’s possible that your pre‐ ferred model weights will not be the ones where you completed your training run but sometime earlier. Having a record of how training progressed will help you tune hyperparameters related to both model structure and training, ranging from how many parameters to have (to avoid underfitting or overfitting) to what learning rate to train at (to avoid making overly large or small adjustments during training).

We now have a full minimal viable pipeline, except that we are missing our models. Now we will look at some basic kinds of models that can be applied to time series data, and we will train each to see its relative performance.

# Feed Forward Networks

This book is quite unusual in presenting feed forward networks in the context of time series analysis. Most modern time series analysis problems are undertaken with recurrent network structures, or, less commonly, convolutional network structures. We, however, begin with the feed forward neural network, as it’s the simplest and most historic network structure. There are several reasons this is a good place to begin:

• Feed forward networks are highly parallelizable, which means they are quite per‐ formant. If you can find a reasonably good feed forward model for your pur‐ poses, you can compute it very quickly.   
• Feed forward networks are good tests for whether there really are complex timeaxis dynamics in your sequence. Not all time series really are time series in the sense of having time-axis dynamics, where earlier values have a specific relation‐ ship to later values. It can be good to fit a feed forward neural network as one baseline, apart from a simpler linear model.   
• Feed forward network components are often integrated into larger and more complex time series deep learning architectures. Therefore, you need to know how these work even if they will not form an entire model in your work.

# A Simple Example

A feed forward network is the simplest kind of neural network, so we begin by apply‐ ing a feed forward architecture to a time series example. There is nothing in the structure of a standard feed forward neural network that indicates a temporal rela‐ tionship, but the idea is that the algorithm may nonetheless learn something of how past inputs predict future inputs. We saw an example of a feed forward network in Figure 10-1. A feed forward network is a series of fully connected layers, meaning the input to each layer connects to every node in the graph.

We start with this simple example, which uses the first kind of data formatting we saw earlier. That is, our input, X, will simply be two-dimensional data of $\mathrm { ~ N ~ } \times \mathrm { ~ C ~ }$ (i.e., samples $\times$ channels), where the time components has been flattened into channels. That corresponds to this data formatting:

<table><tr><td>A, t-2</td><td>A, t-3</td><td>B, t-2</td><td>B, t-3</td><td>C, t-2</td><td>C, t-3</td></tr><tr><td>3</td><td>0</td><td>-2</td><td>-1</td><td>-3</td><td>-2</td></tr></table>

As a reminder, in code that represents this branch of the data:

```python
```python
>>> if model_name == 'fc_model':
>>>     ## provide first and second step lookbacks in one flat input
>>>     X = np.hstack([[x[1:-1], x[::-2]])
>>>     Y = x[2:]
>>>     return (X, Y) 
```

We then build a fully connected model with MXNet’s Module API:

```python
>>> def fc_model(iterator_train, window, filter_size, num_filter, dropout):
>>> X = mx.sym.Variable(iterator_trainprovide_data[0].name)
>>> Y = mx.sym.Variable(iterator_trainprovide_label[0].name)
>>> 
>>> output = mx.symFULLConnected(data=X, num Hidden=20)
>>> output = mx.symActivation(output, act_type='relu')
>>> output = mx.symFULLConnected(data=output, num Hidden=10)
>>> output = mx.symActivation(output, act_type='relu')
>>> output = mx.symFULLConnected(data=output, num Hidden=321)
>>> 
>>> loss_grad = mx.sym.LinearRegressionOutput(data = output, label = Y)
>>> return loss_grad, [v.name for v in iterator_trainprovide_data],
>>> [v.name for v in iterator_trainprovide_label] 
```

Here we build a three-layer, fully connected network. The first layer has 20 hidden units, and the second layer has 10 hidden units. After the first two layers, there is an “activation” layer. This is what makes the model nonlinear, and without this it would

simply be a series of matrix multiplications that would come down to a linear model. A fully connected layer effectively is:

$$
Y = X W ^ {T} + b
$$

W is a set of weights corresponding to those of the correction dimensions for a matrix multiplication, with the output resulting in a vector of dimensions equal to the hidden units. This is then added with a bias. The weights in W and $^ { b }$ are both trainable. However, training one such set of weights, or even a series of such weights, would not be very interesting—it would be a roundabout way of running a linear regression. However, it is the activation applied after this set of matrix operations that creates the interest. The activation layer applies various nonlinear functions, such as the tanh and ReLU, which are pictured in Figures 10-5 and 10-6, respectively.

![](images/791301b7b59acf8ed6c5727b49fb94c8ae77ec4c10943186cda15362b709b98b.jpg)  
Figure 10-5. The tanh function exhibits nonlinear behavior at small values before becoming functionally constant, with a zero derivative, at higher values.

![](images/14b0b896e6f6455407f44bc204f1adb5786e2913f5e7c6c7f475167b45a43cc7.jpg)  
Figure 10-6. The ReLU function is easy to compute while also introducing a nonlinearity.

In the preceding code, you will notice we use the ReLU function as our activation function. It often makes sense to test a variety of activation functions (ReLU, tanh, sigmoid) in your early stages of model construction and hyperparameter tuning.

Now let’s train with this model and see how it performs:

<table><tr><td>Epoch</td><td>Training Cor</td><td>Validation Cor</td></tr><tr><td>0</td><td>0.578666</td><td>0.557897</td></tr><tr><td>1</td><td>0.628315</td><td>0.604025</td></tr><tr><td>2</td><td>0.645306</td><td>0.620324</td></tr><tr><td>3</td><td>0.654522</td><td>0.629658</td></tr><tr><td>4</td><td>0.663114</td><td>0.636299</td></tr><tr><td>5</td><td>0.670672</td><td>0.640713</td></tr><tr><td>6</td><td>0.677172</td><td>0.644602</td></tr><tr><td>7</td><td>0.682624</td><td>0.648478</td></tr><tr><td>8</td><td>0.688570</td><td>0.653288</td></tr><tr><td>9</td><td>0.694247</td><td>0.657925</td></tr><tr><td>10</td><td>0.699431</td><td>0.663191</td></tr><tr><td>11</td><td>0.703147</td><td>0.666099</td></tr><tr><td>12</td><td>0.706557</td><td>0.668977</td></tr><tr><td>13</td><td>0.708794</td><td>0.670228</td></tr><tr><td>14</td><td>0.711115</td><td>0.672429</td></tr><tr><td>15</td><td>0.712701</td><td>0.673287</td></tr><tr><td>16</td><td>0.714385</td><td>0.674821</td></tr><tr><td>17</td><td>0.715376</td><td>0.674976</td></tr><tr><td>18</td><td>0.716477</td><td>0.675744</td></tr><tr><td>19</td><td>0.717273</td><td>0.676195</td></tr><tr><td>20</td><td>0.717911</td><td>0.676139</td></tr><tr><td>21</td><td>0.718690</td><td>0.676634</td></tr><tr><td>22</td><td>0.719405</td><td>0.677273</td></tr><tr><td>23</td><td>0.719947</td><td>0.677286</td></tr><tr><td>24</td><td>0.720647</td><td>0.677451</td></tr></table>

TESTING PERFORMANCE {'COR': 0.66301745}

Is this good performance? It’s hard to know in a void. When you progress to a deep learning model, you should put it into production only if it can outperform the sub‐ stantially simpler models we have looked at in previous chapters. A deep learning model will take longer to compute a prediction and generally has higher overhead, so it should be used only when it can justify the additional costs. For this reason, when you are approaching a time series problem with deep learning tools, you usually want to have previous models you have fit to serve as an objective standard to beat.

In any case, we see that even a model without any temporal awareness can train and produce predictions that are reasonably correlated with measured values.

# Using an Attention Mechanism to Make Feed Forward Networks More Time-Aware

Although feed forward networks have not made great strides in time series problems, there is still research being done to imagine architectural variations that could enhance their performance on sequential data. One idea is attention, which describes mechanisms added to a neural network that allow it to learn what part of a sequence to focus on and what part of an incoming sequence might relate to desirable outputs.

The idea of attention is that a neural network architecture should provide a mecha‐ nism for a model to learn which information is important when. This is done through attention weights, which are fit for each time step so that the model learns how to combine information from different time steps. These attention weights mul‐ tiply what would otherwise be either the output or hidden state of a model, thereby converting that hidden state to a context vector, so-called because the hidden state is now better contextualized to account for and relate to all the information contained in the time series over time, hopefully including the temporal patterns.

# The Softmax Function

You might not be familiar with the softmax function; here’s the functional form: soft $m a x ( y _ { i } ) = e x p ( y _ { i } ) / \Sigma _ { j } e x p ( y _ { j } )$ , which serves two purposes:

• Outputs will now sum to 1, as a complete set of probabilities would. This is often the interpretation of outputs of a softmax.   
• Keeping in mind the general shape of the exponential function, larger inputs $( \gamma _ { i } )$ tend to get even larger, while smaller inputs tend to get relatively smaller. This accentuates strong activations/weights and de-emphasize weaker signals.

The softmax function is also known as the normalized exponential function, with the name clearly related to the functional form we see here.

Attention first came into use within recurrent neural networks (more on these later in this chapter), but its use and implementation is actually easier to understand in a variant proposed for a feed forward architecture in a research paper. The paper pro‐ poses a way to apply feed forward neural networks to sequential data such that the network’s reaction to each step of the sequence can be used as an input to the ulti‐ mate output of that network. This approach also allows variable-length sequences, just as variable-length time series are common in real-world problems.

Figure 10-7 shows an example architecture of how a feed forward neural network can be used for a task that involves processing sequential data.

![](images/b6da6f46e6d2b4f97b029e401201bbd359f05461b359151e224973a94e428024.jpg)  
Figure 10-7. A “feed forward attention mechanism.”

A feed forward network is applied to the inputs at each time step individually, gener‐ ating a “hidden state” for each time step $h _ { \scriptscriptstyle { I } \cdots } h _ { \scriptscriptstyle { T } } .$ This effectively creates a series of hid‐ den states. A second neural network, developed to learn $a ( h _ { t } )$ , is depicted in the corner, and this is the attention mechanism. The attention mechanism allows the net‐ work to learn how much to weight each input, where each input represents a state coming from a different point in time. This allows the mechanism to determine which time steps to weight more or less heavily in the final summation of inputs.

Then the hidden states of different times are combined with their attention coeffi‐ cients before final processing to produce whatever target/label is sought. The researchers who designed this network found that it performed respectably on a vari‐ ety of tasks usually thought to necessitate a recurrent neural network to meet the requirements of remembering earlier inputs and combining them with later ones.

This is a great example of how there are no simple architectures in deep learning models because there are so many possibilities for adapting even a basic model to a complex time series question.

As noted, feed forward neural networks are not the leading networks for time series problems. Nonetheless, they are a useful starting point to establish the performance of a relatively simple model. Interestingly, if we do not include activation functions, we can use these to code up AR and VAR models using the MXNet framework. This can sometimes come in handy in its own right. More importantly, there are architec‐ tural variations of fully connected models that can be quite accurate for certain time series data sets.

# CNNs

If you already have exposure to deep learning, you are likely quite familiar with con‐ volutional neural networks (CNNs). Most of the jaw-dropping, record-breaking, human-like functionality attributable to computers in the last years has been thanks to extremely sophisticated and complex convolutional architectures. All that notwith‐ standing, the idea of a convolution is fairly intuitive and long predates deep learning. Convolutions have long been used in more transparent human-driven forms of image processing and image recognition studies, starting with something as simple as a Gaussian blur. If you are not familiar with the concepts of image processing kernels and don’t have an idea of how you might code one up, Stack Overflow provides a good explanation of how this can be done both with a high-level API or manually via NumPy.

Convolution means applying a kernel (a matrix) to a larger matrix by sliding it across the larger matrix, forming a new one. Each element of the new matrix is the sum of element-wise multiplication of the kernel and a subsection of the larger matrix. This kernel is applied repeatedly as it slides across a matrix/image. This is done with a pre‐ specified number of kernels so that different features can emerge. A schematic of how this works on many layers is shown in Figure 10-8.

![](images/9daac572ead26ca75225ce924b0ea08e56f82816df31f93bfa19f824805a222a.jpg)  
Figure 10-8. A convolutional network. Many two-dimensional windows of specified kernel size slide across the original image, producing many feature maps out of the trainable weights applied to the image. Often these are pooled and otherwise postprocessed with an activation function. The process is repeated several times over several layers to collapse many features down into a smaller range of values, ultimately leading to, for example, classification ratings.

For many reasons, traditional convolution is a poor match to time series. A main fea‐ ture of convolutions is that all spaces are treated equally. This makes sense for images but not for time series, where some points in time are necessarily closer than others. Convolutional networks are usually structured to be scale invariant so that, say, a horse can be identified in an image whether it’s a larger or smaller portion of the image. However, in time series, again, we likely want to preserve scale and scaled fea‐ tures. A yearly seasonal oscillation should not be interpreted in the same way or

“triggered” by the same feature selector as a daily oscillation, although in some con‐ texts this might be helpful.

It is this double-edge nature of convolutions—that their strengths are their weak‐ nesses from a time series perspective—that leads to them being used most often as a component of a network for time series analysis rather than the entire network. Also, it has led to their being studied more often for purposes of classification than fore‐ casting, though, both uses are found in the field.

Some uses of convolutional networks for time series applications include:

• Establishing a “fingerprint” for an internet user’s browsing history, which helps detect anomalous browsing activity   
• Identifying anomalous heartbeat patterns from EKG data   
• Generating traffic predictions based on past recordings from multiple locations in a large city

Convolutions are not all that interesting per se to apply to a univariate time series. Multichannel time series can be more interesting because then we can develop a 2D (or even 3D) image where time is only one axis.

There are other architectural innovations worth mentioning, as we will discuss two examples of convolutions for time series in the following section.

# A Simple Convolutional Model

We can fit a convolutional model to our data rather than a fully connected one by swapping in a different model. In this case (and for the rest of the examples), we take the data in NTC format, which, as a reminder, looks like this:

<table><tr><td>Time</td><td>A</td><td>B</td><td>C</td></tr><tr><td>t-1</td><td>0,3</td><td>-1,-2</td><td>-2,-3</td></tr><tr><td>t</td><td>3,4</td><td>-2,-2</td><td>-3,-4</td></tr></table>

This is ${ \mathrm { ~ N ~ } \times \mathrm { ~ T ~ } \times \mathrm { ~ C ~ } }$ . However, the data expected by a convolutional layer is batch_size, channel, height $\times$ width:

```python
def cnn_model(iter_train, input_featureshape, X, Y, win, szfilt, n_filter, drop):
    conv_input = mx.sym.reshape(data=x, shape=(θ, 1, win, -1))
    ## Convolution expects 4d input (N x channel x height x width)
    ## in our case channel = 1 (similar to a black and white image)
    ## height = time and width = channels slash electric locations
    ## Conv_output = mx.sym.Convolution(data=conv_input, 
```

```python
>>> kernel=(szfilt,  
>>> input_feature_shape[2]),  
>>> num_filter=n_filter)  
>>> cnn_output = mx.sym.Activation(data=cnn_output, act_type='relu')  
>>> cnn_output = mx.sym.reshape(mx.sym.transpose(data=cnn_output, axes=(0, 2, 1, 3)), shape=(0, 0, 0))  
>>> cnn_output = mx.sym.Dropout(cnn_output, p=drop)  
>>> output = mx.symFULLConnected(data=cnn_output, num_hidden=add feature_shape[2])  
>>> loss_grad = mx.sym.LinearRegressionOutput(data=output, label=Y)  
>>> return (loss_grad,  
>>> [v.name for v in iter_trainprovide_data],  
>>> [v.name for v in iter_trainprovide_label]) 
```

Notice that, again, this does not include any explicit temporal awareness. What is dif‐ ferent is that now time is laid out along a single axis, giving it some ordering.

Does this temporal awareness improve performance? Possibly not:

```csv
0 0.330701 0.292515  
1 0.389125 0.349906  
2 0.443271 0.388266  
3 0.491140 0.442201  
4 0.478684 0.410715  
5 0.612608 0.564204  
6 0.581578 0.543928  
7 0.633367 0.596467  
8 0.662014 0.586691  
9 0.699139 0.600454  
10 0.692562 0.623640  
11 0.717497 0.650300  
12 0.710350 0.644042  
13 0.715771 0.651708  
14 0.717952 0.651409  
15 0.712251 0.655117  
16 0.708909 0.645550  
17 0.696493 0.650402  
18 0.695321 0.634691  
19 0.672669 0.620604  
20 0.662301 0.597580  
21 0.680593 0.631812  
22 0.670143 0.623459  
23 0.684297 0.633189  
24 0.660073 0.604098 
```

```txt
TESTING PERFORMANCE {'COR':0.5561901} 
```

It can be difficult to determine why one model doesn’t improve on another. In fact, often even a model’s creator can be wrong about why the model works so well. There are few analytic proofs, and given how much can depend on the contours of a data

set, this can also add confusion. Does the CNN not doing any better reflect the fact that most of the important information is in the nearest points in time? Or does it reflect a difference in the number of parameters? Or perhaps it reflects a failure to choose good hyperparameters. It would be important in a real use case to understand whether the performance seemed reasonable given the structure of the model, the structure of the data, and the overall number of parameters available.

We also see in the code a fault with our early stopping logic. It appears that it was too lenient. In this case, I revisited the issue and noticed that changes in correlation could look like the following over a series of epochs:

[-0.023024142, 0.03423196, -0.008353353, 0.009730637, -0.029091835]

This means that the change in correlation could be terrible—even negative—many times, so long as there was improvement, even a small one, every once in a while. This leniency turns out to be a bad decision, so it would be a good idea to backtrack to our pipeline and have a more stringent early stopping condition. This is an exam‐ ple of the give-and-take you will find as you fit deep learning models. It helps to do many trial runs as you build your pipeline to get an idea of what parameters work for your data set and models of interest—bearing in mind that these will vary substan‐ tially from one data set to the next.

# Alternative Convolutional Models

The simple convolutional model we just saw did surprisingly well even though it included no modifications to be time aware. Now we discuss two approaches from research and industry to use convolutional architectures in time series problems.

Why is this attractive? There are a number of reasons. First, convolutional architec‐ tures are tried-and-true methods, with a number of best practices that are well known by practitioners. This makes convolutional models appealing because they are a known quantity. Additionally, convolutional models have few parameters since the same filters are repeated over and over, meaning there are not too many weights to train. And, finally, large portions of convolutional models can be computed in paral‐ lel, meaning these can be very fast for purposes of inference.

# Causal convolutions

Causal convolutions are best understood with an image, as this can intuitively express how convolutions are modified to produce causality and a sense of time. Figure 10-9 shows an example of a dilated causal convolution. The causality part refers to the fact that only points earlier in time go into any given convolutional filter. That is why the image is not symmetric: earlier points flow into the convolutions used at later times, but not vice versa.

The dilation part refers to the fact that points are skipped in the arrangement of the convolutional filters, such that any given point goes only into one convolutional filter in each level of layering. This promotes model sparsity and reduces redundant or overlapping convolutions, allowing the model to look further back in time while keeping overall computations reasonably contained.

![](images/ce3dbfef32461c8e1ded875f9b31be88aa903bd08ac28d303db1296b69913a7b.jpg)  
Figure 10-9. This graph depicts an important component of WaveNet’s architecture. Here we see how convolutional neural networks have architectural modifications appropriate to time series.

This example of dilated causal convolution introduces the notion of temporal causal‐ ity by permitting only data from prior time points. That is, the convolutions in this image are not equal opportunity; they allow data to flow only from the past to the future and not the other way around. Every data point in the original input does have an impact on the final point. Note what dilation means here, which is that increas‐ ingly “deep” layers in the convolution skip between increasing numbers of data points in previous layers. The way the dilation here was set up, every data point in the original input is included in the final input, but only once. This is not required for dilation but just what was used in this case. Dilation could also be used to skip time points altogether.

While causal convolutions sound complicated and theoretical, they are surprisingly easy to perform. Simply add padding onto the left side of the matrix—that is, add ear‐ lier time step stand-ins that are zero, so that they won’t contribute to the value—and then set padding to “valid” so that the convolution will only run against the actual boundaries of the matrix without including imaginary empty cells. Given that convo‐ lutions work as the sum of element-wise products, adding the zeros to the left side of Figure 10-9 would mean that we could run a standard convolution and that the extra zero cells would not change the final outcome.

Causal convolutions have had great success in the wild, most particularly as part of the model used for Google’s text-to-speech and speech-recognition technologies.

# Converting a time series into pictures

Convolutional models are known to work very well on image analysis, so a good idea in trying to make them relevant to time series is to find a way to turn a time series into a picture. There are many ways to do this, one of which is interesting because it can turn even a univariate time series into a picture: constructing a recurrence plot (see Figure 10-10).

![](images/e789b4b67c91d8d509f36458fb0bb1da8993d499f72b2966938928ff582c44b5.jpg)

![](images/965faa6ac81c43622d6813fa5ef3fbd309c52459081add1dbfd5321348e84b58.jpg)

![](images/7a775b8a64bbabf5377b6649a499c40353c852866212211acbc4b72df5738e91.jpg)

![](images/cebd8a9e4cad3d253cc0f857fef4a501854050d1bcff3252a19389af44ad0d61.jpg)

![](images/c510d216fe74a51c7cbf846563b0912938e067e05172d7f93a06689579c636b5.jpg)

![](images/7a8282768d5633fe4e0450139e541923b2743f5cdbd73fea1e86d4f8ad0b928b.jpg)

![](images/dd8db7de776e246379d66d85e73e72031b0e4272a8ca411ecefe0efacc6ee915.jpg)

![](images/e87b77f5fd1664d02c6991ad6626ed9c7ba4b1cd4a986ef320107540157b2433.jpg)  
Figure 10-10. A beautiful and insightful visualization of four kinds of time series, from left to right: (1) white noise, (2) harmonic/seasonal series with two frequencies, (3) cha‐ otic data with a trend, and (4) an autoregressive process. Source: Wikipedia, provided by Norbert Marwan in 2006.

A recurrence plot is a way of describing, in phase-state space, when a time series revisits approximately the same phase and state it was in at an earlier moment in time. This is defined via the binary recurrence function. Recurrence is defined as $R ( i ,$ $j ) = 1$ if $f ( i ) - f ( j )$ is sufficiently small; 0 otherwise. This results in a binary black-andwhite image, such as the ones we see depicted in Figure 10-10. Notice that i and $j$ refer to the time values, and the time axis is not bounded or in any way restricted.

While it is relatively easy to code up your own recurrence plot, recurrence plot func‐ tionality is also available in packages such as pyts, and the source code for the plot‐ ting is easy to find and understand.

We can imagine extending this idea beyond a univariate time series by treating differ‐ ent variables of a multivariate time series as different “channels” of an image. This is just one example of how deep learning architectures and techniques are quite versa‐ tile and open to variation.

# RNNs

Recurrent neural networks (RNNs) are a broad class of networks wherein the same parameters are applied again and again even as the inputs change as time passes. This sounds much like the feed forward neural network we covered previously, and yet

RNNs make up—in whole or in part—many of the most successful models in aca‐ demia and industry for sequence-based tasks, language, forecasting, and time series classification. The important differences between an RNN and a feed forward net‐ work are:

• An RNN sees time steps one at a time, in order.   
• An RNN has state that it preserves from one time step to another, and it is this state as well as its static parameters that determine its response updates to each new piece of information at each step.   
• An RNN thus has parameters that help it “update” its state, including a hidden state, from time step to time step.

Often when RNNs are introduced, they are presented via the paradigm of “unrolling” because a recurrent architecture is cell-based. This describes the way the same parameters are used again and again, so that the number of parameters is fairly small even for very long time sequences (see Figure 10-11).

![](images/5b58c1a0a2afc71c098edceb9f116049fd82776626c60c76fd370b787e502b6e.jpg)  
Figure 10-11. How a recurrent neural network architecture is unrolled once for each time step when it is applied to data.

The easiest way to understand how an RNN works, however, may just be to see an example. My particular favorite RNN cell is the Gated Recurrent Unit (GRU). Some‐ times it can be more intimidating to see the expressions in mathematical form than in code, so here I provide an implementation of a GRU in Python with the help of NumPy. As you can see, there are two activation functions used: sigmoid and tanh. Other than this, all we do is perform matrix multiplication and addition as well as element-wise matrix multiplication (the Hadamard product):

```python
## python
>>> # this code is designed to work with weights exported
>>> # from TensorFlow's
>>> # https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnGRU
>>> # but can easily be repurposed to accommodate other weights
>>> def calc_gru(X, weights, num Inputs, num_features):
>>> Us = weights[(3*num_features*num_Input)] 
```

```python
>>> Us = np reshape(Us, [3, num_features, num輸入])
>>> Ws = weights[(3*num_features*num輸入):(3*num_features*num輸入 + 3*num_features*num輸入)]
>>> Ws = npreshape(Ws, [3, num_features, num輸入])
>>> Bs = weights[-6 * num輸入] : ]
>>> Bs = npreshape(Bs, [6, num輸入])
>>> s = np.zeros([129, num輸入])
>>> h = np.zeros([129, num輸入])
>>> for t in range(X.shape[0]):
>>> z = sigmoid(np/matmul(Us[0, :, :, X[t, :]] + np/matmul(Ws[0, :, :, s[t, :]] + Bs[0, :, :, X[t, :]] + np/matmul(Ws[1, :, :, s[t, :]] + Bs[1, :, :, X[4, :]])
>>> h[t+1, :, ] = np.tanh(np/matmul(Us[2, :, :, X[t, :]] + Bs[2, :,])
>>> r*(np/matmul(Ws[2, :, :, s[t, :]] + Bs[5, :]))
>>> s[t+1, :, ] = (1 - z)*h[t+1, :, ] + z*s[t, :]
>>> return h, s 
```

A GRU is currently one of the most widely used RNN cells. It is a simpler version of the Long Short-Term Memory (LSTM) cell, which operates in a similar way. The dif‐ ferences between a GRU and an LSTM are as follows:

• A GRU has two “gates,” while an LSTM has three. These gates are used to deter‐ mine how much new information is allowed in, how much old information is preserved, and so on. Because an LSTM has more gates, it has more parameters.   
• An LSTM tends have better performance, but a GRU is faster to train (due to the number of parameters). However, there are published results where a GRU out‐ performs an LSTM. A GRU is particularly likely to outperform an LSTM for nonlanguage tasks.

As you can see, the difference is more a matter of the degree of complexity appropri‐ ate for your training resources and of what you are trying to understand and predict.

It is important to be familiar with the matrix implementation of the GRU and LSTM so that you get a sense of how they work. Once you do, you may also develop an intu‐ ition when they don’t train on a particular data set as to why they don’t. There may be something about the dynamics that is not easily recognized by the format shown here.

Note that both GRUs and LSTMs help solve the problem that was first encountered when RNNs were used, namely exploding and vanishing gradients. Because of the recurrent application of the same parameters, it was often the case that gradients would quickly go to zero (not helpful) or to infinity (also not helpful), meaning that

backpropagation was difficult or even impossible as the recurrent network was unrol‐ led. This problem was addressed with GRU and LSTM because these tend to keep inputs and outputs from the cell in tractable value ranges. This is due both to the form of the activation function they use and to the way that the update gate can learn to pass information through or not, leading to reasonable gradient values being much more likely than in a vanilla RNN cell, which has no notion of a gate.

While both GRUs and LSTMs are reasonably easy to DIY, as just demonstrated, in reality you would not want to do this. The most important reason is that many of the matrix multiplication operations can be fused. The most efficient and accessible implementation for taking care of this and exploiting the hardware is NVIDIA’s cuDNN, which fuses the matrix multiplication operations needed for both GRU and LSTM cells. Using the cuDNN interface rather than another implementation is sub‐ stantially faster, and has even been cited by some Kaggle contest winners as the dif‐ ference between winning and not even coming close because it helps so much in speeding up training. All the main deep learning frameworks offer access to this implementation, although in some cases (such as TensorFlow’s tf.con trib.cudnn_rnn), you need to use the specially designated interface. In other cases, such as MXNet, you will default to using cuDNN so long as you don’t do anything fancy with custom unrolled cells.

# Continuing Our Electric Example

We can work through the electric forecasting example with an RNN as well. Again, we begin with the TNC data format as an input. This is the expected format for an RNN, so we don’t even need to change it:

```python
def rnn_model(iterator_train, window, filter_size, num_filter, dropout):
    inputFeature_shape = iterator_trainprovide_data[0][1]
    X = mx.sym.Variable(iterator_trainprovide_data[0].name)
    Y = mx.sym.Variable(iterator_trainprovide_label[0].name)
    rnn.cells = mx.rnnSequentialRNNCell()
    rnn.cells.add(mx.rnn.GRUCell(num_hidden=args.rnn.units))
    rnn.cells.add(mx.rnn.DropoutCellDropout))
    outputs, _ = rnn.cells.unroll(length=window, inputs=X, merge_outputs=False)
    output = mx.sym FullyConnected(data=outputs[-1],
                    num_hidden = input_feature_shape[2])
    loss_grad = mx.sym.LinearRegressionOutput(data = output,
                    label = Y)
    return loss_grad, [v.name for v in iterator_trainprovide_data],
                    [v.name for v in iterator_trainprovide_label] 
```

The performance of this model is rather disappointing given that it is designed to handle temporal data:

<table><tr><td>Epoch</td><td>Training Cor</td><td>Validation Cor</td></tr><tr><td>0</td><td>0.072042</td><td>0.069731</td></tr><tr><td>1</td><td>0.182215</td><td>0.172532</td></tr><tr><td>2</td><td>0.297282</td><td>0.286091</td></tr><tr><td>3</td><td>0.371913</td><td>0.362091</td></tr><tr><td>4</td><td>0.409293</td><td>0.400009</td></tr><tr><td>5</td><td>0.433166</td><td>0.422921</td></tr><tr><td>6</td><td>0.449039</td><td>0.438942</td></tr><tr><td>7</td><td>0.453482</td><td>0.443348</td></tr><tr><td>8</td><td>0.451456</td><td>0.444014</td></tr><tr><td>9</td><td>0.454096</td><td>0.448437</td></tr><tr><td>10</td><td>0.457957</td><td>0.452124</td></tr><tr><td>11</td><td>0.457557</td><td>0.452186</td></tr><tr><td>12</td><td>0.463094</td><td>0.455822</td></tr><tr><td>13</td><td>0.469880</td><td>0.461116</td></tr><tr><td>14</td><td>0.474144</td><td>0.464173</td></tr><tr><td>15</td><td>0.474631</td><td>0.464381</td></tr><tr><td>16</td><td>0.475872</td><td>0.466868</td></tr><tr><td>17</td><td>0.476915</td><td>0.468521</td></tr><tr><td>18</td><td>0.484525</td><td>0.477189</td></tr><tr><td>19</td><td>0.487937</td><td>0.483717</td></tr><tr><td>20</td><td>0.487227</td><td>0.485799</td></tr><tr><td>21</td><td>0.479950</td><td>0.478439</td></tr><tr><td>22</td><td>0.460862</td><td>0.455787</td></tr><tr><td>23</td><td>0.430904</td><td>0.427170</td></tr><tr><td>24</td><td>0.385353</td><td>0.387026</td></tr></table>

TESTING PERFORMANCE {'COR': 0.36212805}

It would be much more investigation to see why the performance is underwhelming. Did we not provide enough parameters to the RNN? Would it make sense to add some related model architectures that are commonly used, such as attention? (This applies to RNNs as well as feed forward networks, as was discussed earlier.)

The model reached its peak performance quite a bit earlier in the training process than did the feed forward or convolutional models. This is a lack of sufficient param‐ eters to describe the data set, or that an RNN is specially tailored to this data or something else. You would need to do additional experiments and tuning to get a sense of the reason.

# The Autoencoder Innovation

Occasionally you may happen upon a data set where a very simple model already does spectacular work for the mission you hope to accomplish.4 But sometimes thinking outside the box can prove fruitful as well. For example, early on it was dis‐ covered that a simple innovation with an RNN could also substantially improve per‐ formance in sequence-like modeling. While this model was originally developed for language learning and machine translation, it has often proven quite successful at more numeric tasks, such as predicting electric load forecasting or stock prices. Known as the autoencoder, or alternately as the seq2seq model, this is a very com‐ monly used model and one you should make a go-to part of your time series deep learning toolkit (see Figure 10-12). We will deploy it in several of the example chap‐ ters later in the book, where we analyze real-world time series data.

The autoencoder is two recurrent layers, but not in the traditional sense where each layer processes each input successively. Rather, the first layer runs to completion. Its hidden state is then passed onto the second layer, which takes this hidden state and its own outputs as new inputs for the next step. This model was particularly formula‐ ted with the idea of machine translation, whereby outputting a prediction at each time step was not meaningful given that in different languages, word and concept ordering can be drastically different and say the same thing. The idea was that once the first layer had fully processed the time series, its hidden state could come to have a kind of summarizing functionality. This summary would then be injected into the new model, which would gradually unroll the meaning into the new language by combining this summary with its own output at each time step so that it would know what it had said.

As mentioned, despite the natural language processing origins of this model, it is also useful for more traditional time series tasks, such as predicting univariate or multi‐ variate time series. For example, in a Kaggle competition to predict web traffic in Wikipedia posts, the first-place winner, after trying many hyperparameters and architectural components, eventually settled on an autoencoder model. It seems that the winner’s advantage was likely in smart training and hyperparameter search as well as a good study of useful feature generation and selection.

![](images/111f0e0ac87c8724dd1b9aa71cb4dbc4b996a2b776d9ed30bd7e6363d38f54ce.jpg)  
Figure 10-12. An autoencoder, also often referred to as a seq2seq model, is very popular for language processing and modeling but has also shown significant success in time series analysis.

# Combination Architectures

Very often, successful industry and contest applications of deep learning for time ser‐ ies forecasting will use some kind of novel architecture, be it a new spin on applying traditional LSTM cells or combining different components. One example of this is a 2018 neural network architecture proposed by researchers at Carnegie Mellon Uni‐ versity. The researchers looked to exploit the strengths of both convolutional and recurrent architectures, but they added other innovations as well.

They developed a “skip recurrent” layer whereby the recurrent model could be tuned to pay attention to periodicities (such as yearly, weekly, daily, depending on the nature of the data set) present in the data (and this periodicity could itself be explored as a hyperparameter).

They recognized that many time series have trends that are not well modeled by non‐ linear deep learning models. This meant that a deep learning model alone would not show the substantial scale changes over time that some time series data sets demon‐ strate. The researchers adapted by using a traditional linear model, namely the autor‐ egressive model we discussed in Chapter 6.

The final model, the modified LSTNet, was a sum of the outputs from the AR model and a model built with a traditional recurrent layer and a skip recurrent layer in par‐ allel. The inputs fed into each recurrent layer were the outputs of convolutional layers that convolved both along the time and channel axis (see Figure 10-13).

The researchers found that in three out of the four data sets they tried, they outper‐ formed state-of-the-art published results spanning a broad range of topic areas. They failed only in the case of foreign exchange rates, a domain of finance that is notori‐ ously difficult to predict, with high noise-to-signal ratios and a highly efficient mar‐ ket whereby any signal quickly dissipates as the market becomes efficient and investors try to find an edge.

![](images/8fc6d62a5e81f86b694e149006379adfc2cf750b9dc9cc25ea5b7d0866ad7078.jpg)  
Figure 10-13. In the modified LSTNet architecture, we can see there is an autoregres‐ sive component (bottom of image) in parallel to a neural network architecture. The neural network architecture puts a convolutional element and a recurrent element in consecutive order, operating on the same inputs one after the other.

The inspiration for the next elaboration comes from the researchers’ paper, and we will be working with code5 modified from the MXNet package catalog of examples, described in great detail by the example’s author, Oliver Pringle, in a blog post.

Here, as noted, we apply code based on modifications to the MXNet repository, sim‐ plifying it by removing the seasonal/skip connections and also by using only one con‐ volutional filter size. We apply a convolutional layer, just as we did in the cnn_model example earlier:

```python
>>> # # must be 4d or 5d to use padding functionality
>>> conv_input = mx.sym.reshape(data=X, shape=(0,1,win,-1))
>>> 
>>> # # convolutional element
>>> # we add padding at the end of the time win
>>> cnn_output = mx.sym_pad(data=conv_input, 
```

```python
>>> mode="constant",  
>>> constant_value=0,  
>>> pad_width=(0,0,  
>>> 0,0,  
>>> 0, szfilt - 1,  
>>> 0,0))  
>>> cnn_output = mx.sym.Convolution(data=cnn_output,  
>>> kernel=(szfilt,  
>>> input_feature_shape[2]),  
>>> num_filter=n_filter)  
>>> cnn_output = mx.sym.Activation(data=cnn_output, act_type='relu')  
>>> cnn_output = mx.sym.reshape(mx.sym.transpose(data=cnn_output,  
>>> axes=(0,2,1,3)),  
>>> shape=(0,0,0))  
>>> cnn_output = mx.sym.Dropout(cnn_output, pdrop) 
```

We then apply a RNN not to the original inputs but to the convolutional component, as follows:

```python
```python
>>> ## recurrent element
>>> stacked_rnn.cells = mx.rnnSequentialRNNCell()
>>> stacked_rnn.cells.add(mx.rnn.GRUCell(num Hidden=args.rnn.units))
>>> outputs, _ = stacked_rnn.cells.unroll(length=win,
>>> inputs=cnn_output,
>>> merge_outputs=False)
>>> rnn_output = outputs[-1]
>>> n_outputs = input_feature_shape[2]
>>> cnn_rnn_model = mx.sym FullyConnected(data=rnn_output,
>>> num Hidden=n_outputs) 
```

Finally, in parallel to this CNN/RNN combination, we train an AR model, with one AR model per electric station location (so 321 distinct AR models, one per column/ variable/electric site), as shown here. This uses every time point for every station, with the model being specified one station at a time:

```python
```python
>>> ## ar element
>>> ar_outputs = []
>>> for i in list(range(input_feature_shape[2])):
>>> ar_series = mx.sym.slice_axis(data=X, axis=2, begin=i, end=i+1)
>>> fc_ar = mx.symFULLConnected(data=ar_series, num Hidden=1)
>>> ar_outputs.append(fc_ar)
>>> ar_model = mx.sym.cat(*ar_outputs, dim=1) 
```

The full code is as follows:   
```python
def simple lstnet_model(iter_train, input_feature_shape, X, Y, win, szfilt, n_filter, drop):
    ## must be 4d or 5d to use padding functionality
    conv_input = mx.sym.reshape(data=X, shape=(0, 1, win, -1))
    ## convolutional element
    ## we add padding at the end of the time win
    cnn_output = mx.sym_pad(data=conv_input,
                          mode="constant",
                          constant_value=0,
                          pad_width=(0, 0,
                          0, 0,
                          0, szfilt - 1,
                          0, 0))
    cnn_output = mx.sym.Convolution(data = cnn_output,
                          kernel = (szfilt,
                          input_feature_shape[2]),
                          num_filter = n_filter)
    cnn_output = mx.sym Activate(data = cnn_output,
                          act_type = 'relu')
    cnn_output = mx.sym.reshape(mx.sym.transpose(data = cnn_output,
                          axes = (0, 2, 1, 3)),
                          shape=(0, 0, 0))
    cnn_output = mx.sym.Dropout(cnn_output, p = drop)
    ## recurrent element
    stacked_rnn_cells = mx:rnnSequentialRNNCell()
    stacked_rnn.cells.add(mx:rnn.GRUCell(num Hidden = args:rnn.units))
    outputs, _ = stacked_rnn.cells.unroll(length = win,
                          inputs = cnn_output,
                                  merge_outputs = False)
    rnn_output = outputs[-1]
    n Outputs = input_feature_shape[2]
    cnn_rnn_model = mx.symFULLConnected(data = rnn_output,
                          numHidden = n_outputs)
    ## ar element
    ar_outputs = []
    for i in list(range(input_feature_shape[2]):
        ar_series = mx.sym.slice_axis(data=X,
                            axis=2,
                            begin=i,
                            end=i+1)
        fc_ar = mx.sym FullyConnected(data = ar_series,
                                num Hidden = 1)
        ar_outputs.append(fc_ar)
        ar_model = mx.sym monetize(*ar_outputs, dim=1)
    output = cnn_rnn_model + ar_model
    loss_grad = mx.sym.LinearRegressionOutput(data=output, label=Y)
    return (loss_grad, 
```

```txt
>>> [v.name for v in iter_trainprovide_data],  
>>> [v.name for v in iter_trainprovide_label]) 
```

Notice that the performance is strikingly better in this model than any other:

<table><tr><td>Epoch</td><td>Training Cor</td><td>Validation Cor</td></tr><tr><td>0</td><td>0.256770</td><td>0.234937</td></tr><tr><td>1</td><td>0.434099</td><td>0.407904</td></tr><tr><td>2</td><td>0.533922</td><td>0.506611</td></tr><tr><td>3</td><td>0.591801</td><td>0.564167</td></tr><tr><td>4</td><td>0.630204</td><td>0.602560</td></tr><tr><td>5</td><td>0.657628</td><td>0.629978</td></tr><tr><td>6</td><td>0.678421</td><td>0.650730</td></tr><tr><td>7</td><td>0.694862</td><td>0.667147</td></tr><tr><td>8</td><td>0.708346</td><td>0.680659</td></tr><tr><td>9</td><td>0.719600</td><td>0.691968</td></tr><tr><td>10</td><td>0.729215</td><td>0.701734</td></tr><tr><td>11</td><td>0.737400</td><td>0.709933</td></tr><tr><td>12</td><td>0.744532</td><td>0.717168</td></tr><tr><td>13</td><td>0.750767</td><td>0.723566</td></tr><tr><td>14</td><td>0.756166</td><td>0.729052</td></tr><tr><td>15</td><td>0.760954</td><td>0.733959</td></tr><tr><td>16</td><td>0.765159</td><td>0.738307</td></tr><tr><td>17</td><td>0.768900</td><td>0.742223</td></tr><tr><td>18</td><td>0.772208</td><td>0.745687</td></tr><tr><td>19</td><td>0.775171</td><td>0.748792</td></tr><tr><td>20</td><td>0.777806</td><td>0.751554</td></tr><tr><td>21</td><td>0.780167</td><td>0.754034</td></tr><tr><td>22</td><td>0.782299</td><td>0.756265</td></tr><tr><td>23</td><td>0.784197</td><td>0.758194</td></tr><tr><td>24</td><td>0.785910</td><td>0.760000</td></tr></table>

```txt
TESTING PERFORMANCE {'COR':0.7622162} 
```

This model obviously does quite a bit better than the other models, so there is some‐ thing special and helpful about this architecture that uses convolutional images as the sequential data on which the recurrent layer trains. The traditional statistical tool of an AR model also adds quite a bit of functionality.6 This model is far and away the best one, and this is a good lesson for the small amount of data exploration and train‐ ing we did. It’s worth trying a variety of models, and you don’t even need to spend a lot of time training all models to be able to discover a clear leader.

# Summing Up

One interesting aspect of our training in this chapter is that the models’ performan‐ ces did not turn out the way we might have expected. The simple feed forward net‐ work substantially outperformed some models that seem more conceptually complicated. However, this doesn’t necessarily settle the question of which models are better or worse for our data set for a number of reasons:

• We did not check the number of parameters used in each model. It’s possible that different genres of model would have drastically different performance with different numbers of parameters. We could play with model complexity, such as number of convolutional/recurrent layers or number of filters/hidden units.   
• We did not tune hyperparameters. Sometimes getting the right hyperparameters can make an enormous difference in the performance of a model.   
• We did not explore our data enough to have a prior idea of what model we would expect to do better or worse given the correlations over time and between different columns/electric sites.

# How to Develop Your Skills

If you begin to browse academic journals, conferences, and arXiv postings looking specifically for deep learning resources for time series, you will often be frustrated by a few conventions in the literature:

• Much time series research sees itself as domain-specific first and time series sec‐ ond. This means you should really be looking in related domains, such as eco‐ nomics, healthcare, and climate studies.   
• Deep learning researchers often think of “deep learning” or “neural networks” as convolutional networks. So if you are really trying to dig in specifically to “recur‐ rent neural networks,” make sure to look for these words in the title. Most papers lacking these words (or similar) in their title won’t have RNNs in the body of the research.   
• Unfortunately, no one has yet written a deep learning model to help you find deep learning papers, but that would be a worthy project.

# More Resources

• Historical documents:

Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997):1735–80, https://perma.cc/AHR3-FU5H.   
This 1997 seminal work introduced the Long Short-Term Memory cell (LSTM) and also proposed several experimental benchmarks that remain in use today for studying neural network performance on sequence analysis.

Peter G. Zhang, Eddy Patuwo, and Michael Hu, “Forecasting with Artificial Neu‐ ral Networks: The State of the Art,” International Journal of Forecasting 14, no. 1 (1998): 35–62, https://perma.cc/Z32G-4ZQ3.

This article gives an overview of the state of the art for time series and deep learning in 1998.

• On RNNs:

Aurélien Geron, “Processing Sequences Using RNNs and CNNs,” in Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition (Sebas‐ topol: O’Reilly Media, Inc., 2019).   
Aurélien Geron offers an exceptionally extensive set of examples on how to apply deep learning to time series data in this popular book. If you are com‐ fortable with these tools, this Jupyter notebook offers an excellent example of many different kinds of models applied to sequence data, including some exercises with solutions.

Valentin Flunkert, David Salinas and Jan Gasthaus, “DeepAR: Probabilistic Fore‐ casting with Autoregressive Recurrent Networks,” unpublished paper, 2017, https:// perma.cc/MT7N-A2L6.

This groundbreaking paper illustrated Amazon’s model that was developed to fit time series for its retail data that occurs at a large variety of scales, and with trends. One particular innovation of the work was the ability to make probabilistic forecasts rather than the usual point estimates that tend to be the result of deep learning analysis.

Lingxue Zhu and Nikolay Laptev, “Deep and Confident Prediction for Time Series at Uber,” paper presented at the 2017 IEEE International Conference on Data Mining Workshops (ICDMW), New Orleans, LA, https://perma.cc/PV8R-PHV4.   
This paper illustrates another example of probability and statistically inspired modifications to typical RNNs. In this case, Uber proposed a novel Bayesian deep model that provided both a point estimate and an uncertainty estimation that could be deployed in production with reasonably fast performance.

Zhengping Che et al., “Recurrent Neural Networks for Multivariate Time Series with Missing Values,” Scientific Reports 8, no. 6085 (2018), https://perma.cc/ 4YM4-SFNX.

This paper provides a strong example of state-of-the-art work on medical time series. It demonstrates the use of a GRU, combined with novel architec‐ tures to account for missing data and make missingness into an informative attribute. The authors showcase neural networks that beat all existing clinical metrics currently deployed to make predictions about patient health and hospital stay statistics. This is a great example of how an intuitive and easyto-understand modification to a simple and widely used RNN structure (the GRU) can lead to groundbreaking results on a good data set.

• On CNNs:

Aäron van den Oord and Sander Dieleman, “WaveNet: A Generative Model for Raw Audio DeepMind,” DeepMind blog, September 8, 2016, https://perma.cc/ G37Y-WFCM.

This blog provides an extremely well done and accessible description of a groundbreaking CNN architecture that was used to enhance both text-tospeech and speech-to-text technologies in a variety of languages and with different speakers. The new architecture led to a significant boost in perfor‐ mance and was subsequently deployed on other sequence-related AI tasks, particularly with respect to time series forecasting.

• On the application of deep learning:

Vera Rimmer et al., “Automated Website Fingerprinting Through Deep Learning,” paper presented at NDSS 2018, San Diego, CA, https://perma.cc/YR2G-UJUW.

This paper illustrates a way in which deep learning can be used to uncover private information about a user’s internet browsing content throught Web‐ site Fingerprinting. In particular the authors highlighted a way in which var‐ ious neural network architectures could be used to formulate Website Fingerprinting attacks to pierce user privacy protections.

CPMP, “Second Place Solution to the Kaggle Web Traffic Forecasting Competi‐ tion,” Kaggle blog, 2017, https://perma.cc/UUR4-VNEU.

This blog, written before the conclusion of the competition, describes the second-place winner’s thinking in designing a mixed machine learning/deep learning solution to forecasting. There is also some retrospective commen‐ tary available in a related blog post. This is a great example of a mix of modern packages and a relevant can-do coding style. The GitHub repository for the first-place solution is also available alongside a discussion of its neu‐ ral network based architecture.

# Measuring Error

In previous chapters we used a variety of measures to compare models or judge how well a model performed its task. In this chapter we examine best practices for judging forecast accuracy, with emphasis placed on concerns specific to time series data.

For those new to time series forecasting, it’s most important to understand that stan‐ dard cross-validation usually isn’t advisable. You cannot select training, validation, and testing data sets by selecting randomly selected samples of the data for each of these categories in a time-agnostic way.

But it’s even trickier than that. You need to think about how different data samples relate to one another in time even if they appear independent. For example, suppose you are working on a time series classification task, so that you have many samples of separate time series, where each is its own data point. It could be tempting to think that in this case you can randomly pick time series for each of training, validation, and testing, but you would be wrong to follow through on such an idea. The problem with this approach is that it won’t mirror how you would use your model, namely that your model will be trained on earlier data and tested on later data.

You don’t want future information to leak into your model because that’s not how it will play out in a real-world modeling situation. This in turn means that the forecast error you measure in your model will be lower during testing than in production because in testing you will have cross-validated your model in a way that yielded future information (i.e., feedback on which model to use).

Here’s a concrete scenario demonstrating how this could happen. Imagine you are training an air quality monitor for major cities in the western United States. In your training set you include all 2017 and 2018 data for San Francisco, Salt Lake City, Denver, and San Diego. In your testing set, you include the same range of dates for Las Vegas, Los Angeles, Oakland, and Phoenix. You find that your air quality model

does particularly well for Las Vegas and Los Angeles, but also extremely well in 2018 overall. Great!

You then try to replicate your model training process on data from earlier decades and find that it does not test as well as it trains in other training/testing runs. You then remember the record-breaking wildfires in Southern California of 2018 and realize that they were “baked in” to the original testing/training run because your training set gave you a window into the future. This is an important reason to avoid standard cross-validation.

There can be times where it is not a problem for information from the future to prop‐ agate backward into the choice of a model. For example, if you are merely looking to understand the dynamics of a time series by testing how well you can forecast it, you are not looking to forecast, per se, so much as to test the best possible fit of a given model to data. In this case, including data from the future is helpful to understanding the dynamics, although you would want to be wary of overfitting. So arguably, even in this case, maintaining a valid test set—which requires not letting information leak backward from the future—would still warrant concerns about time series and crossvalidation.

With this general commentary complete, we turn to an example of the concrete mechanics of divvying up data for training, validating, and testing a model. We then talk more generally about how to determine when a forecast is good enough, or as good as it reasonably can be. We also look at how to estimate uncertainty of our fore‐ cast for when we use techniques that do not directly produce an uncertainty or error measure as part of the output. We conclude with a list of gotchas that can be helpful to review from time to time when you are constructing a time series model or prepar‐ ing to put one into production. It may help you avoid some embarrassment!

# The Basics: How to Test Forecasts

The most important element of generating a forecast is to make sure that you are building it solely with data you could access sufficiently in advance for that data to be used in generating the forecast. For this reason, you need to think not only about when events happen but also when the data would be available to you.1

While this seems simple enough, remember that common preprocessing, such as exponential smoothing, may inadvertently lead to leakage from the training period to the testing period. You can test this in an example yourself by fitting a linear regres‐ sion first to an autoregressive time series and then to an exponentially smoothed

autoregressive time series. You will see that the more you smooth the time series, and the longer your smoothing half-life, the “better” your predictions become. This is because in fact you are having to make less and less of a prediction as more and more of your value is composed of an exponential average of past values. This is quite a dangerous lookahead and is so insidious it still turns up in academic papers!

Because of these dangers and other hard-to-spot ways of feeding the future into the past, or vice versa, the gold standard for any model should be backtesting with train‐ ing, validation, and testing periods that roll forward.

In backtesting, a model is developed for one set of dates, or a range of dates, but then it is tested extensively on historical data, preferably representing the full range of pos‐ sible conditions and variability. Also important, practitioners need to have principled reasons for backtesting a particular model and should try to avoid trying out too many models. As most data analysts are aware, the more models you test, the more likely you are to overfit to your data—that is, choose a model that has picked out overly specific details about the current data set rather than generalizing in a robust way. Unfortunately for time series practitioners, this is tricky balancing act and one that can lead to embarrassing results when you put models into production.

Concretely, how do we implement some form of backtesting? We do this in a way that preserves a cross-validation-like structure, albeit a temporally aware one. The common paradigm, assuming you have data that represents time passing sequentially “in alphabetical order,” is as follows:

Train with [A] test with [B]

Train with [A B] test with [C]

Train with [A B C] test with [D]

Train with [A B C D] test with [E]

Train with [A B C D E] test with [F]

Figure 11-1 illustrates this testing structure.

![](images/82100e7ad6ab910d2cc694b74a1cd25a5e4d3e76e013d96092b46d577c384b03.jpg)  
Figure 11-1. The gold standard for evaluating the performance of a time series model, namely rolling forward your training, validation, and testing windows.

It is also possible to move the training data window rather than expand it. In this case, your training could look something like this:

Train with [A B] test with [C]

Train with [B C] test with [D]

Train with [C D] test with [E]

Train with [D E] test with [F]

Which method you choose depends in part on whether you think the behavior of your series is evolving over time. If you think it does, you are better served by a mov‐ ing window so that all testing periods are tested with a model trained to the most rel‐ evant data. Another consideration could be that you want to avoid overfitting, where using an expanding window will better discipline your model than a fixed-length window.

Because this kind of rolling split is a common training need, both R and Python offer easy ways to generate it:

• In Python, one easy way to generate data splits is with sklearn.model_selec tion.TimeSeriesSplit.   
• In R, tsCV from the forecast package will roll a model forward in time using this backtesting schema and report the errors.

There are other packages in both R and Python that will do the same. You can also write your own functions to split your data if you have specific ideas about how you would implement this model testing for a particular project. You might want to skip certain time periods because they exhibited anomalous dynamics, or you might want to weight performance of certain time periods more.

For example, suppose you work with financial data. Depending on your goals, it could be worthwhile to exclude data from extraordinary times, such as the financial crisis of 2008. Or if you work with retail data, you might want to weight model per‐ formance most heavily for Christmas shopping season even if you sacrifice some accuracy in the predictions of lower-volume seasons.

# Model-Specific Considerations for Backtesting

Consider the dynamics of the model you are training when structuring your backtest‐ ing, particularly with respect to what it means to train a model with a particular tem‐ poral range of data.

With traditional statistical models, such as ARIMA, all data points are factored in equally when selecting model parameters, so more data may make the model less accurate if you think the model parameters should be varied over time. This also is true for machine learning models for which all training data is factored in equally.2

Batched stochastic methods, on the other hand, can result in weights and estimates that evolve over time. So, neural network models trained with typical stochastic gra‐ dient descent methods will, to some extent, take the temporal nature of the data into account if you train on the data in chronological order. The most recent gradient adjustments to the weight will reflect the most recent data. In most cases, time series neural network models are trained on the data in chronological order, as this tends to produce better results than models trained on the data in random order.

![](images/4e13f820e93f96e09c501823046942d7f68f10e785f50ba6b8295da37b26d667.jpg)

Don’t make holes in your data. The very difficulty and beauty of time series data is that the data points are autocorrelated. For this reason, we cannot randomly select out points within our time ser‐ ies for validation or testing because this will destroy some of the autocorrelation in our data. As a result, our model’s recognition of an autoregressive component of our data would be undermined, a most undesirable result.

State space models also offer opportunities for the fitting to adapt over time with the mode. This cuts in favor of a longer training window because a long time window will not keep the posterior estimate from evolving over time.

![](images/3c54994333abd092115cf3243bca377af1636dee426dab019b16a588012d237a.jpg)

# Snapshot Your Model Over Time

Because you will test your models by rolling them forward in time after fitting them on past data, you need an easy way to save a model with a timestamp so that you know the earliest point in time that you could appropriately have used that model. This will help you avoid inadvertently testing a model on its own training data. It will also give you an opportunity to apply several different models from different time periods to testing data. This can be a way of seeing whether it matters how recently a model was trained with respect to that model’s accuracy on test data, which can ultimately help you choose a rhythm for how often your time series models need to be refit in production code.

# When Is Your Forecast Good Enough?

When your forecast is good enough depends on your overall goals, the minimum quality you can “get away with” for what you need to do, and the limits and nature of your data. If your data itself has a very high noise-to-signal ratio, you should have limited expectations for your model.

Remember, a time series model isn’t perfect. But you should hope to do as well as, or slightly better than, alternative methods, such as solving a system of differential equa‐ tions about climate change, asking a well-informed stock trader for a tip, or consult‐ ing a medical textbook that shows how to classify EEG traces. As you assess performance, keep in mind the known domain expert limits to forecasting as such measures indicate—for now, the upper limit to performance in many forecasting problems.

There are times when you know that your model is not yet good enough and that you can do better. Here are some actions you can take to identify these opportunities:

# Plot the outputs of your model for the test data set

The distribution produced by the model should match the distribution of the val‐ ues you are seeking to forecast, assuming there is no expected regime change or underlying trend. As an example, if you are trying to predict stock prices, and you know that stock prices go up and down about equally often, you have an inadequate model if the model always predicts a price increase. Sometimes the distributions will obviously be off, whereas at other times you can apply a test statistic to compare your model’s output with your actual targets.

# Plot the residuals of your model over time

If the residuals are not homogenous over time, your model is underspecified. The temporal behavior of the residuals can point you toward additional parame‐ ters needed in your model to describe the temporal behavior.

# Test your model against a simple temporally aware null model

A common null model is that every forecast for time t should be the value at time t – 1. If your model is not beating such a simple model, there isn’t any justifica‐ tion for it. If a simple, all-purpose naive model can beat the model you have craf‐ ted, it’s a fundamental problem with your model, loss function, or data preprocessing rather than a matter of a hyperparameter grid search. Alternately, it can be a sign of data that has a lot of noise relative to signal, which also sug‐ gests that your model is useless for its intended purposes.3

# Study how your model handles outliers

In many industries, outliers simply are what they are called: outliers. There is likely no way these events could have been predicted,4 which means the best your model can do is ignore these outliers rather than fit them. In fact, your model predicting outliers very well may even be a sign of overfitting or poor loss func‐ tion selection. This depends on the model you chose and the loss functions you have employed, but for most applications you will want a model whose predic‐ tions are not as extreme as the extreme values in your data set. Of course this advice does not hold when the cost of outlier events is high and where the fore‐ casting task is mainly to warn of outlier events when possible.

# Conduct temporal sensitivity analysis

Are qualitatively similar behaviors in related time series producing related out‐ comes in your model? Using your knowledge of the underlying dynamics of your system, ensure that this is true and that your model recognizes and treats similar temporal patterns in the same way. For example, if one time series shows a trend upward with a drift of 3 units per day and another shows a trend upward with a drift of 2.9 units per day, you’d likely want to be sure that the forecasts ultimately made for these series were similar. Also you’d want to make sure that the ranking of the forecasts as compared to the input data was sensible (greater drift should result in a larger forecast value). If this is not the case, your model may be overfit.

This is not a full list of the many ways you can test your time series model, but it can be a starting point that you build on as you gain experience in a particular domain.

# Estimating Uncertainty in Your Model with a Simulation

One advantage of traditional statistical time series analysis is that such analyses come with well-defined analytical formulas for the uncertainty in an estimate. However, even in such cases—and certainly in the case of nonstatistical methods—it can be helpful to understand the uncertainty associated with a forecasting model via compu‐ tational methods. One very intuitive and accessible way to do this is a simple simulation.

Suppose we have conducted an analysis of what we believe to be an AR(1) process. As a reminder, an AR(1) process can be expressed as:

$$
y _ {t} = \phi \times y _ {t - 1} + e _ {t}
$$

Following a model fit, we want to study how variable our estimate of the $\phi$ coefficient could be. In such a case, one way to study this is to run a number of Monte Carlo simulations. This can be accomplished easily in R so long as we remember what we learned about AR processes in Chapter 6:

```r
## R
> require(forecast)
>
>
phi <- 0.7
> time_steps <- 24
> N <- 1000
> sigma_error <- 1
>
>
sd_series <- sigma_error^2 / (1 - phi^2)
> starts <- rnorm(N, sd = sqrt(sd_series))
> estimates <- numeric(N)
> res <- numeric(time_steps)
>
>
for (i in 1:N) {
    err = rnorm(time_steps, sd = sigma_error)
    res[1] <- starts[i] + err[1]
}
for (t in 2:time_steps) {
    res[t] <- phi * tail(res, 1) + err[t]
}
estimates <- c(estimates, arima(res, c(1, 0, 0)) $coef[1])
}
hist(estimates,
main = "Estimated Phi for AR(1) when ts is AR(1)", 
```

This leads to the histogram shown in Figure 11-2 for the estimated $\phi$ .

![](images/13ff3fcb1b8bc69e78a1e8a82e5d0349617c6eacf547e34b70d4a6b2867d46d1.jpg)  
Estimated Phi for AR(1) when ts is AR(1)   
Figure 11-2. The distribution of estimates for $\phi$

We can also get a sense of the range of the estimates and the quantiles with the sum mary() function applied to estimates:

```txt
R  
> summary( estimates1) Min. 1st Qu. Median Mean 3rd Qu. Max. -0.3436 0.4909 0.6224 0.5919 0.7204 0.9331 
```

We can use bootstrapping to ask more complicated questions, too. Suppose we want to know what numerical costs we may pay for oversimplifying our model compared to ground truth. Imagine the process studied is an AR(2) even though we diagnosed it as an AR(1) process. To find out how this affects our estimate, we can modify the preceding R code as follows:

```txt
## R
>>> now let's assume we have a true AR(2) process
>>> because this is more complicated, we switch over to arima.sim
>>> phi_1 <- 0.7
>>> phi_2 <- -0.2
>>> estimates <- numeric(N)
>>> for (i in 1:N) {
>>> res <- arima.sim(list(order = c(2,0,0), 
>>> ar = c(phi_1, phi_2)), 
```

```txt
> n = time_steps)  
> estimates[i] <- arima(res, c(1, 0, 0))\(coef[1]  
> }  
>  
> hist( estimates,  
> main = "Estimated Phi for AR(1) when ts is AR(2)",  
> breaks = 50) 
```

We see the resulting distribution in Figure 11-3. As we can see, the distribution is not as smooth and well defined for this misspecified model as it was for the appropriately specified model from Figure 11-2.

![](images/a9896e6488d81da530f38c325654255d501d67e334f7dc9c8770432442e02d9f.jpg)  
Estimated Phi for AR(1) when ts is AR(2)   
Figure 11-3. The estimated distribution of the lag 1 coefficient for an AR(1) model fit‐ ted to a process for which the true description is an AR(2) process.

It may strike you that these distributions are not all that different, and you are right. We confirm this with the summary statistics:5

```txt
R  
> summary( estimates) Min. 1st Qu. Median Mean 3rd Qu. Max. -0.5252 0.4766 0.6143 0.5846 0.7215 0.9468 
```

We can see that the range of estimates is wider when the model is misspecified and that the estimate for the first order term is slightly worse than it was when the model was properly specified, but the deviation is not too large.

This can address concerns that underestimating the order of the model will affect our estimate of $\phi$ . We can run a variety of simulation scenarios to address potential issues and understand the range of likely misestimation given some imagined possibilities.

# Predicting Multiple Steps Ahead

While one-step-ahead forecasting is what we’ve covered in most of the past chapters, you might also want to predict multiple time steps ahead. This happens, among other reasons, when the time series data you have is at a greater temporal resolution than the time series values you would like to predict. For example, you might have daily stock prices available, but you would like to predict monthly stock prices so that you can execute a long-term strategy for your retirement savings. Alternately, you might have brain electrical activity readings that are taken every minute but you would like to predict a seizure at least five minutes in advance to give your users/patients as much warning as possible. In such cases, you have numerous options for generating multi-step-ahead forecasts.

# Fit Directly to the Horizon of Interest

This is as simple as setting your y (target) value to reflect the forecast horizon of interest. So if your data consisted of minute-by-minute indicators but you wanted a five-minute forward horizon for your forecast, you would cut off your model inputs at time $t$ and train them to a label generated with data up to time $t + 5$ . Then you would fit to this data as what you were trying to predict, be it via a simple linear regression or a machine learning model or even a deep learning network. This effec‐ tively functions as:

$$
m o d e l (X) = Y
$$

where you can choose Y to have any time horizon you want. So each of these would be a legitimate scenario depending on your forward horizon of interest (be it 10 steps or 3 steps as shown here):

• mode $l _ { 1 } ( X _ { t } )$ is fit to $Y _ { _ { t + 1 0 } }$   
• mode $l _ { 2 } (  { \boldsymbol { X } } _ { t } )$ is fit to $Y _ { t + 3 }$

# Recursive Approach to Distant Temporal Horizons

Using a recursive approach to fitting a variety of horizons, you build one model but prepare to feed its own output back into it as an input to predict more distant hori‐ zons. This idea should feel familiar because we demonstrated how to make multistep-ahead forecasts with ARIMA modeling using this very strategy. Suppose we developed a model to fit one step ahead, by training on model $( X _ { t } ) = Y _ { t + 1 }$ . If we wanted to fit to horizons three steps ahead, we would do the following:

• model $( X _ { t } ) $ estimate $Y _ { t + 1 }$   
• model $X _ { t }$ with estimate of $Y _ { { t + 1 } } ) $ estimate of $Y _ { _ { t + 2 } }$   
• model $X _ { t }$ with estimate of $Y _ { t + 1 }$ and estimate of $Y _ { t + 2 } ) $ estimate of $Y _ { t + 3 }$

The expected error for our estimate of $Y _ { t + 3 }$ would necessarily be greater than that of our estimate for $Y _ { t + 1 }$ . How much greater? This could be complicated to think about. A great option to get a sense would be to run a simulation, as discussed earlier in this chapter.

# Multitask Learning Applied to Time Series

Multitask learning is a general concept from deep learning that can be applied with a particular meaning for time series analysis. More generally, multitask learning describes the idea that a model can be built to serve multiple purposes at once or to learn to generalize by trying to predict several different but related targets at the same time. Some think of this as a means of regularization, encouraging the model to be more general by teaching it related tasks.

In the time series context, you can apply multitask learning by setting up targets for different time horizons in a forecasting context. In this case, fitting your model could look like this:

• model (X t ) = (Y t +1, Y t +10, Y t +100)   
• model (X t ) = (Y t +1, Y t +2, Y t +3)

In training such a model, you could also think about how to see the loss function: would you want to weight all forecasts equally or would you want to privilege certain forecast horizons over others?

If you were seeking to make a very distant forecast, you could use multitask horizons to teach your model by including shorter-term horizons that might point to salient features useful for the longer-time horizon but difficult to suss out directly from the low signal-to-noise data of far-future forecasts. Another scenario for multitask mod‐ eling would be to fit to several time windows in the future, all in the same season but perhaps different points in time (so springtime over several years or Mondays over

several weeks). This would be one way to fit both for seasonality and a trend at the same time.

# Model Validation Gotchas

These are the most important issues to consider when thinking about whether you have appropriately tested your model as compared to expected production implementations:

# Lookahead

Lookahead is emphasized throughout this book because it is so difficult to avoid and so potentially catastrophic and embarrassing when a model is put into pro‐ duction. Whenever possible you want to save yourself the danger of putting out models that suddenly stop validating in production. This is the hallmark of not being aware of lookahead in your system. Don’t let this happen to you!

# Structural Changes

Identifying structural changes is partly a matter of judgment, partly a matter of subject matter, and partly a matter of the quality of your data. The dynamics underlying a time series can change over time, and they can change enough that a model well suited to one portion of a time series may not be well suited to another. This is one of the reasons exploratory analysis is important—to make sure you are not training the same model across structural changes where doing so is neither justifiable nor sensible.

# More Resources

Christoph Bergmeir, Rob J. Hyndman, and Bonsoo Koo, “A Note on the Validity of Cross-Validation for Evaluating Autoregressive Time Series Prediction,” Computa‐ tional Statistics & Data Analysis 120 (2018): 70–83, https://perma.cc/YS3J-6DMD.

The authors of this article (including the estimable and highly prolific Rob Hyndman) discuss cases where cross-validation will produce good results even for time series analysis. This is a helpful article in particular because it offers insights into why errors emerge from cross-validation and also what kinds of models you can use with a standard cross-validation breakdown rather than backfitting.

Angelo Canty, “tsboot: Bootstrapping of Time Series,” n.d., https://perma.cc/MQ77- U5HL.

This description of the tsboot() function included in the boot package makes for educational reading, as it implements multiple widely accepted variations on block sampling as a form of bootstrapping. Exploring the options available in this function is a good way to get familiar with various ways to bootstrap time series, and familiarity with this function in turn will enable you to make

convenient, reliable estimates of test statistics and assess the uncertainty of your estimation even in the case of small data sets.

Hans R. Kunsch, “The Jackknife and the Bootstrap for General Stationary Observa‐ tions,” Annals of Statistics 17, no. 3 (1989): 1217–41, https://perma.cc/5B2T-XPBC.   
This classic and widely cited article demonstrates a statistical approach to extending jackknife (less commonly used) and bootstrapping methods to time series data, by eliminating the assumption of such methods that data points are independently distributed. This article introduces the idea of thinking about and sampling from time series with units of blocks of points rather than individual points.   
Christian Kleiber, “Structural Changes in (Economic) Time Series,” in Complexity and Synergetics, ed. Stefan Müller et al. (Cham, Switzerland: Springer International, 2018), https://perma.cc/7U8N-T4RC.   
This chapter summarizes a variety of canonical methods for identifying struc‐ tural changes in time series, and it is written from a business and economic time series perspective, although the lessons apply more broadly. It is particularly use‐ ful because it provides a listing of the R packages that can be used to apply these widely accepted structural change identification methods and because it offers helpful visualizations of how methods behave somewhat differently when analyz‐ ing the same time series.   
Robert Stambaugh, “Predictive Regressions,” Journal of Financial Economics 54 (1999): 375–421, https://perma.cc/YD7U-RXBM.   
This widely cited article demonstrates how to perform a Bayesian posterior esti‐ mation of regression coefficients and how estimations of quantities from finite samples can differ substantially given different assumptions about the underlying dynamics of financial time series models. This is an excellent, albeit domainspecific, example of how much starting assumptions can affect estimates of uncertainty in your estimates, parameters.

# Performance Considerations in Fitting and Serving Time Series Models

In the literature on machine learning and statistical analyses, the overwhelming focus tends to be on performance of models in terms of accuracy. While accuracy should usually be the primary concern when evaluating a model, sometimes computational performance considerations matter tremendously in the face of large data sets or widely deployed models to serve large populations of client applications.

Time series data sets get so large that analyses can’t be done at all—or can’t be done properly—because they are too intensive in their demands on available computing resources. In such cases, many organizations treat their options as follows:

• Upsize on computing resources (expensive and often wasteful both economically and environmentally).   
• Do the project badly (not enough hyperparameter tuning, not enough data, etc.).   
• Don’t do the project.1

None of these options are satisfying, particularly when you are just starting out with a new data set or a new analytical technique. It can be frustrating not to know whether your failures are the result of poor data, an overly difficult problem, or a lack of resources. Hopefully, we will find some workarounds to expand your options in the case of very demanding analyses or very large data sets.

This chapter is designed to guide you through some considerations of how to lessen the computing resources you need to train or infer using a particular model. For the

most part, such questions are specific to a given data set, the resources you have avail‐ able, and both your accuracy and speed targets. You will see this reality echoed in the concerns detailed in this chapter, but the hope is that they will partly cover the prob‐ lems you run into and can provide inspiration for further brainstorming. These are considerations to come when you have completed your first rounds of analysis and modeling and should not be a priority when you are first attaching a problem. How‐ ever, when it’s time to put something into production or extend a small research project into a larger one, you should revisit these concerns often.

# Working with Tools Built for More General Use Cases

One challenge of time series data is that most tools, particularly those for machine learning, are built for a more general use case, with most illustrative examples show‐ casing uses of cross-sectional data. There are a few ways these machine learning methods then fail to be as efficient as possible with time series data. The solutions to your individual problems will vary, but the general ideas are the same. In this section I discuss common problems and potential solutions.

# Models Built for Cross-Sectional Data Don’t “Share” Data Across Samples

In many cases where you are feeding discrete samples of time series data into an algo‐ rithm, most often with machine learning models, you will notice that large chunks of the data you are feeding between samples overlap. For example, suppose you have the following data on monthly sales:

<table><tr><td>Month</td><td>Widgets sold</td></tr><tr><td>Jan 2014</td><td>11,221</td></tr><tr><td>Feb 2014</td><td>9,880</td></tr><tr><td>Mar 2014</td><td>14,423</td></tr><tr><td>Apr 2014</td><td>16,720</td></tr><tr><td>May 2014</td><td>17,347</td></tr><tr><td>Jun 2014</td><td>22,020</td></tr><tr><td>Jul 2014</td><td>21,340</td></tr><tr><td>Aug 2014</td><td>25,973</td></tr><tr><td>Sep 2014</td><td>11,210</td></tr><tr><td>Oct 2014</td><td>11,583</td></tr><tr><td>Nov 2014</td><td>12,014</td></tr><tr><td>Dec 2014</td><td>11,400</td></tr><tr><td>Jan 2015</td><td>11,539</td></tr><tr><td>Feb 2015</td><td>10,240</td></tr></table>

You are seeking to make predictions by mapping each “shape” to a nearest neighbor curve. You prepare many shapes from this data. Just a smattering of those data points are listed here, as you might want to use six-month-long curves as the “shapes” of interest (note we are not doing any data preprocessing to normalize or create addi‐ tional features of interest, such as moving average or smoothed curves):2

11221, 9880, 14423, 16720, 17347, 22020

9880, 14423, 16720, 17347, 22020, 21340

14423, 16720, 17347, 22020, 21340, 25973

Interestingly, all we have managed to do with this preparation of inputs is to make our data set six times larger, without including any additional information. This is a catastrophe from a performance standpoint, but it’s often necessary for inputs into a variety of machine learning modules.

If this is a problem you run into, there are a few solutions you should consider.

# Don’t use overlapping data

Consider only producing a “data point” so that each individual month makes its way into only one curve. If you do so, the preceding data could look like the following table:

11221, 9880, 14423, 16720, 17347, 22020

21340, 25973, 11210, 11583, 12014, 11400

Note that this would be particularly simple because it amounts to a simple array reshape rather than a custom repetition of data.

# Employ a generator-like paradigm to iterate through the data set

Employing a generator-like paradigm to iterate through the data set, resampling from the same data structure as appropriate, is particularly easy to code up in Python but can also be done with R and other languages. If we imagine that the original data is stored in a 1D NumPy array, this could look something like the following code (note that this would have to be paired with a machine learning data structure or algorithm that accepts generators):

```txt
>>> def array_to-ts(arr):
>>> idx = 0 
```

>>> while idx $+ \quad < =$ arr.shape[0]:   
>>> yield arr[idx:(idx+6)]

Note that designing data modeling code that does not unnecessarily blow up a data set is desirable from both a training and production standpoint. In training this will allow you to fit more training examples in memory, and in production you can make multiple predictions with fewer training resources in the case of predictions (or clas‐ sifications) on overlapping data. If you are making frequent predictions for the same use case, you will likely be working with overlapping data, so this problem and its sol‐ utions will be quite relevant.

# Models That Don’t Precompute Create Unnecessary Lag Between Measuring Data and Making a Forecast

Usually machine learning models do not prepare for or take into account the possi‐ bility of precomputing part of a result in advance of having all the data. Yet for time series this is a very common scenario.

If you are serving your model in a time-sensitive application, such as for medical pre‐ dictions, vehicle location estimations, or stock price forecasting, you may find that the lag of computing a forecast only after all the data becomes available is too great. In such a case, you should consider whether the model you have chosen can actually be partly precomputed in advance. A few examples of where this is possible are as follows:

• If you are using a recurrent neural network that takes several channels of infor‐ mation over 100 different time steps, you can precompute/unroll the neural net‐ work for the first 99 time steps. Then when the last data point finally comes in, you only need to do 1 final set of matrix multiplications (and other activation function computations) rather than 100. In theory this could speed up your response time 100-fold.   
• If you are using an AR(5) model, you can precompute all but the most recent term in the sum that constitutes the mode. As a reminder, an AR(5) process looks like the equation that follows. If you are about to output a prediction, it means that you already know the values of $y _ { t - 4 } , y _ { t - 3 } , y _ { t - 2 } ,$ and $y _ { t - 1 } .$ , which means that you can have everything other than $p h i _ { 0 } \tilde { A } y _ { t }$ ready to go in advance of knowing y :

$$
y _ {t + 1} = p h i _ {4} \times y _ {t - 4} + p h i _ {3} \times y _ {t - 3} + p h i _ {2} \times y _ {t - 2} + p h i _ {1} \times y _ {t - 1} + p h i _ {0} \times y _ {t}
$$

• If you are using a clustering model to find nearest neighbors by summarizing fea‐ tures of a time series (mean, standard deviation, max, min, etc.), you can com‐ pute these features with a time series with one less data point and run your

model with that time series to identify several nearest neighbors. You can then update these features once the final value rolls in and rerun the analysis with only the nearest neighbors found in the first round of analysis. This will actually require more computation resources overall but will result in a lower lag time between taking the final measurement and delivering the forecast.

In many cases your model may not be nearly as slow as the network lag or other fac‐ tors, so precomputation is a worthwhile technique only where feedback time is extremely important and where you are confident that model computation is sub‐ stantially contributing to the time between an application receiving all needed infor‐ mation and outputting a useful prediction.

# Data Storage Formats: Pluses and Minuses

One overlooked area of performance bottlenecks for both training and productioniz‐ ing time series models is the method of storing data. Some common errors are:

• Storing data in a row-based data format even though the time series is formed by traversing a column. This results in data where time-adjacent points are not memory-adjacent.   
• Storing raw data and running analyses off this data. Preprocessed and downsam‐ pled data is preferable to the greatest extent possible for a given model.

Next we discuss these data storage factors to keep your model training and inference as speedy as possible.

# Store Your Data in a Binary Format

It is tempting to store data in a comma-separated text file, such as a CSV file. That is often how data is provided, so inertia pushes us to make this choice. Such file formats are also human readable, which makes it easy to check the data in the file against pipeline outputs. Finally, such data is usually easy to ship around different platforms.3

However, it’s not easy for your computer to read text files. If you are working with data sets so large that you are not able to fit all your data in memory during training, you will be dealing with I/O and related processing all tied to the file format you choose. By storing data in a binary format, you can substantially cut down on I/Orelated slowdowns in a number of ways:

• Because the data is in binary format, your data processing package already “understands” it. There is no need to read a CSV and turn it into a data frame. You will already have a data frame when you input your data.   
• Because the data is in binary format, it can be compressed far more than a CSV or other text-based file. This means that I/O itself will be shorter because there is less physical memory to read in a file so as to re-create its contents.

Binary storage formats are easily accessible in both R and Python. In R, use save() and load() for data.table. In Python, use pickling and note that both Pandas (pd.DataFrame.load() and pd.DataFrame.save()) and NumPy (np.load() and np.save()) include wrappers around pickling that you can use for their specific objects.

# Preprocess Your Data in a Way That Allows You to “Slide” Over It

This recommendation is related to “Models Built for Cross-Sectional Data Don’t “Share” Data Across Samples” on page 358. In this case, you should also think about how you preprocess your data and make sure the way you do so is consistent with using a moving window over that data to generate multiple test samples.

As an example, consider normalization or moving averages as preprocessing steps. If you plan to do these for each time window, this may lead to improved model accu‐ racy (although in my experience such gains are often marginal). However, there are several downsides:

• You need more computational resources to compute these preprocessing fea‐ tures many, many times on overlapping data—only to end up with very similar numbers.   
• You need to store overlapping data with slightly different preprocessing many, many times.   
• You cannot take optimum advantage of sliding a window over your data.

# Modifying Your Analysis to Suit Performance Considerations

Many of us are guilty of getting too comfortable with a particular set of analytical tools, and the accompanying suite of software and rules of thumb about how to fit a model. We also tend to assess accuracy needs once and not reassess them when we determine the computational cost of various possible model performances.

Time series data, often used to make a fast forecast, is particularly prone to needing models that can be both fit and productionized quickly. The models need to be fit quickly so they can be updated with new data coming in, and they need to perform quickly so that the consumers of the models’ forecasts have as much time as possible to act on those forecasts. For this reason, you may sometimes want to modify your expectations—and accompanying analysis—to make faster and more computation‐ ally streamlined processes for analysis and forecasting.

# Using All Your Data Is Not Necessarily Better

One important factor in thinking about how to streamline your analysis is to under‐ stand that not all data in a time series is equally important. More distant data is less important. Data during “exceptional” times is less important to building a model for ordinary times.

There are many ways you should consider lessening the amount of data you use to train a model. While many of these options have been discussed earlier in this book, it’s good to review them, particularly with respect to performance:

# Downsampling

It is often the case that you can use data that is less frequent to cover the same lookback window when making a prediction. This is a way to downsize your data by a multiplicative factor. Note that, depending on the analytical technique you use, you also have more creative options, such as downsampling at different rates depending on how far back the data goes.

# Training only on recent data

While machine learning loves data, there are many time series models where statistical or even deep learning techniques will actually do better just focusing on recent data rather than training on all data. This will help you reduce your input data by subtracting out data that is only marginally informative for your model.

# Reduce the lookback window used to make your prediction

In many time series models, the model’s performance will continue to improve, if only slightly, as you look further and further back into the past. You should make some decisions about how much accuracy is really required relative to per‐ formance. It may be that you are loading far more data into memory per sample than is really necessary for acceptable performance.

# Complicated Models Don’t Always Do Better Enough

It can be interesting and fun to try the latest and greatest when it comes to choosing an analytical model. However, the real question is whether a fancier model “pays” for any additional computational resources required.

Almost all the computational advances in machine learning of recent years have come from throwing more computing power at a problem. With problems such as image recognition, where there is definitely a right answer and $1 0 0 \%$ accuracy, this can certainly make sense.

On the other hand, with problems such as time series predictions, where there may be physical or mathematical limits to how accurate a prediction can be, you should make sure that choosing a more complex model is not simply automatic upgrading without a cost-benefit analysis. Think about whether the accuracy gains justify the additional lag the model may produce in computing a forecast, or the additional training time that will be required, or the additional computational resources that will be spun up. It may be that a less resource-intensive method with slightly poorer accuracy is a far better “deal” than a fancy model that barely improves upon the sim‐ ple version.

If you are the data analyst, this trade-off between complexity/accuracy and lag time/ computer resources is something you should analyze, thinking of it as another hyper‐ parameter to tune. It’s your job to point out these trade-offs rather than assume that a data engineer will take care of it. People upstream or downstream in the data pipeline cannot substitute your judgment for model selection, so you have to acknowledge the engineering side of data science while weighing pros and cons.

# A Brief Mention of Alternative High-Performance Tools

If you have fully explored the previous options, you can also consider changing your underlying code base, more specifically by moving away from slower scripting lan‐ guages, such as Python and R. There are a number of ways to do this:

• Go all in with $\mathrm { C } { + } { + }$ and Java. Even if you haven’t looked at these languages, just learning the basics can sometimes speed up the slow parts of your pipeline enough to transform impossible tasks into manageable ones. $\mathrm { C } { + } { + }$ in particular has evolved tremendously in terms of usability and standard libraries applicable to data processing. The STL and $\mathrm { C } { + } { + } \ 1 7$ syntax now offers many options quite comparable to Python for operating on sets of data in a variety of data structures. Even if you hated $\mathrm { C } { + } { + }$ and Java years ago, you should revisit them.4   
• In Python, you can use several different modules wherein you can write Python code that gets compiled into C or $\mathrm { C } { + } { + }$ code, speeding up execution time. This can be especially useful for very repetitive code with many for loops, which are slow in Python and can become much more performant in C or $\mathrm { C } { + } { + }$ without the need for clever design—simply implementing the same code in the faster

language can do the trick. Numba and Cython are both accessible drop-in Python modules that can help you speed up slow chunks of Python code this way.

• Likewise, in R, you can use Rcpp for similar functionality.

# More Resources

• On models performing equally:

Anthony Bagnall et al., “The Great Time Series Classification Bake Off: An Experi‐ mental Evaluation of Recently Proposed Algorithms,” Data Mining and Knowledge Discovery 31, no. 3 (2017): 606–60, https://perma.cc/T76B-M635.

This article runs an extensive array of experiments to assess performance of modern time series classification methodologies, comparing their perfor‐ mance on a wide variety of publicly available data sets. The computational complexity of the data sets ultimately ends up varying quite a bit more than does the actual performance of the methods tried. As the authors highlight, it remains an art and an area of research to determine, without trying them all, which method will work best on a given data set. From a computing resour‐ ces perspective, the lesson to learn here is that computational complexity should factor in significantly in methodological decisions. Unless you have a very compelling use case for a complicated and resource-intensive algo‐ rithm, choose something simpler.

• On building simpler models:

Yoon Kim and Alexander M. Rush, “Sequence Level Knowledge Distillation,” in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, ed. Jian Su, Kevin Duh, and Xavier Carreras (Austin, TX: Association for Computational Linguistics, 2016), 1317–27, https://perma.cc/V4U6-EJNU.

This article applies the general concept of “distillation” to sequence learning, as applied to a machine translation task. The concept of distillation is a broadly useful one. The idea is that first a complex model is designed and trained on the original data, and then a simpler model is trained on the out‐ puts of the complex model. The outputs of the complex model, rather than the data itself, reduces noise and simplifies the learning problem, making it easier for a simpler model to learn approximately the same relationship by cutting out the noise. While such a technique will not lessen training time, it should produce a model that executes faster and demands fewer resources when put into production.

# Healthcare Applications

In this chapter we will look at time series analysis in the healthcare context with two case studies: Flu forecasting and nowcasting and Blood glucose forecasting. $>$ These are both important healthcare applications for common health problems. Also, in both cases these are not solved problems but rather topics of ongoing research in aca‐ demia and in the healthcare industry.

# Predicting the Flu

Predicting the flu rate from week to week in a given geographic area is a longstanding and ongoing problem. Infectious disease specialists and global security professionals alike agree that infectious diseases pose a significant risk to human welfare. This is particularly the case for the flu, which strikes the vulnerable worldwide, inflicting hundreds of fatalities every year, mostly among the very young and very old. It’s cru‐ cial from both a health and national security standpoint to develop accurate models of how the flu will run its course in a given season. Flu prediction models are useful both to predict the virus specifically and also to help researchers explore general the‐ ories for how infectious diseases travel geographically.

# A Case Study of Flu in One Metropolitan Area

We’ll look at a data set of weekly flu reports for a variety of administrative regions in France for the years of 2004 through 2013. We will predict the flu rate for Île de France, the Paris metropolitan region. The data can be downloaded from Kaggle1 and is also available in the code repository for this book.

# Data exploration and some cleanup

We begin by familiarizing ourselves with the raw data, first examining it in tabular form:

```python
## R
> flu = fread("train.csv")
> flu[, flu RATE := as.numeric(TauxGrippe)]
> head(flu)
Id week region_code region_name TauxGrippe flu_rate
1: 3235 201352 42 ALSACE 7 7
2: 3236 201352 72 AQUITAINE 0 0
3: 3237 201352 83 AUVERGNE 88 88
4: 3238 201352 25 BASSE-NORMANDIE 15 15
5: 3239 201352 26 BOURGOGNE 0 0
6: 3240 201352 53 BRETAGNE 67 67 
```

We also do some basic quality checks, such as looking for NA in our variables of inter‐ est. We may not know where these NA values come from, but we need to account for them:

```shell
## R
> nrow(flu[is.na(flu RATE)]) / nrow(flu)
[1] 0.01393243
> unique(flu[is.na(flu_rate)]$region_name)
[1] "HAUTE-NORMANDIE" "NORD-PAS-DE-CALAIS" "PICARDIE"
[4] "LIMOUSIN" "FRANCHE-COMTE" "CENTRE"
[7] "AUVERGNE" "BASSE-NORMANDIE" "BOURGOGNE"
[10] "CHAMPagne-ARDENNE" "LANGUEDOC-ROUSSILLON" "PAYS-DE-LA-LOIRE"
[13] "CORSE" 
```

The overall rate of NA data points is not very high. Additionally, our region of inter‐ est, Île-de-France, is not included in the list of regions with NA values.

We perform some data cleanup, separating out the week and year portion of the timestamp column (which is currently in character format, not numerical or time‐ stamp format):

```txt
## R
> flu[, year := as.numeric(substr.week, 1, 4))]
> flu[, wk := as.numeric(substr.week, 5, 6))]
> ## style note it's not great we have 2 week columns 
```

We add a Date class column so we can have better plotting axes for time than if we treated the data as nontimestamped:

```txt
## R
> flu[, date:= as.Date(paste0(as.characterflu$week), "1"), "%Y%U%u")] 
```

This line of code is slightly tricky. To convert the month-week combinations into dates, we add a component indicating the day. This is the purpose of paste0(), which marks each date as the first day of the week, pasting a "1" onto a string that

already designates year and week of the year (out of 52 weeks of the year—more on this shortly).2 Notice the %U and %u in the format string: these have to do with mark‐ ing time according to the week of the year and day of the week, a somewhat unusual timestamping format.3

We then subset to the data relating specifically to Paris and sort the data by date:4

```txt
## R
## let's focus on Paris
> paris.flu = flu[region_name == "ILE-DE-FRANCE"]
> paris.flu = paris.flu[order(date, decreasing = FALSE)]
> paris.flu[, .(week, date, flu RATE)]
week date flu RATE
1: 200401 2004-01-05 66
2: 200402 2004-01-12 74
3: 200403 2004-01-19 88
4: 200404 2004-01-26 26
5: 200405 2004-02-02 17
--- 
518: 201350 2013-12-16 13
519: 201351 2013-12-23 49
520: 201352 2013-12-30 24
521: 200953 <NA> 145
522: 200453 <NA> 56 
```

If you are paying attention, the row count should surprise you. If there are 52 weeks in a year, and we have 10 years’ worth of data, why do we have 522 rows? We would have expected 52 weeks $\times 1 0$ years $= 5 2 0$ rows. Similarly, why are there two NA dates? You will see an explanation if you go back to the original data. There seems to be a 53rd week both for 2004 and for 2009. Every few years there is a year that has 53 weeks rather than 52—it’s not a mistake but rather a part of the Gregorian calendar system.

We then check that the data covers a full and regularly sampled date range by first making sure that each year has the same number of data points:5

```txt
## R
> paris.flu[, .N, year]
year N
1: 2004 53
2: 2005 52
...
9: 2012 52
10: 2013 52
>
Paris.flu[, .N, wk]
wk N
1: 1 10
2: 2 10
3: 3 10
...
51: 51 10
52: 52 10
53: 53 2
wk N 
```

We can see that the data is as expected; that is, each year (other than the two just dis‐ cussed) has 52 weeks, and each week-of-year label has 10 data points, one for each year (except week 53).

Now that we’ve considered the timestamps of the data, we inspect the actual values of the time series (so far we have considered only the time indexing). Is there a trend? Seasonality? Let’s find out (see Figure 13-1):

```txt
## R
> paris.flu[, plot(date, flu RATE, type = "l", xlab = "Date", ylab = "Flu rate")] 
```

It’s clear from a simple line plot that there is substantial seasonality (something you’ve likely experienced in your own community). This plot suggests a strong sea‐ sonal component but does not suggest there is a temporal drift apart from the seasonality.

The seasonal behavior complicates the 53rd week. If we want to fit a seasonal model, we need to define the seasonality in terms of weeks of the year, and we cannot have variable seasonal sizes (that is what distinguishes a season from a cycle, as discussed in Chapter 3). While we could imagine some creative solutions to the problem of the 53rd week, we will take the simple option of suppressing this data:

```txt
## R
> paris.flu <- paris.flu[week != 53] 
```

![](images/1e07ab868401728c7b2c786eefaabf9e24bdbe382b85285daf6b7f7b0156f736.jpg)  
Figure 13-1. By plotting the time series of the flu rate, we can see the seasonality of the flu rate in Paris.

Whether it is a significant problem to delete a data point depends on the data set and the question we are asking. I leave it as an exercise to the reader to explore other pos‐ sibilities for fitting the data while keeping the 53rd week data. There are a variety of options to do so. One is merging the 53rd week data into the 52nd week of data by averaging the two weeks. Another is to use a model that can account for cyclical behavior without having to be locked into exactly the same length cycle each year. A third option is that a machine learning model might be able to accommodate this with some creative labeling of the data to indicate seasonality to the model as an input feature.

# Fitting a seasonal ARIMA model

We first consider fitting a seasonal ARIMA model to the data because of the strong seasonality. In this case the periodicity of the data is 52, since the data is sampled weekly. We want to choose a fairly parsimonious model—that is, one without too many parameters—because at 520 data points our time series is not particularly long.

This time series is a good example of how we can go wrong if we rely too strongly on autopilot. For example, we might first consider whether to difference the data, and so we might consider the plot of the autocorrelation of the flu rate and the autocorrela‐ tion of the differenced time series of the flu rate, each shown in Figure 13-2:

R  
> acf(paris.flu\\(flu RATE, ）  
> acf(diff(paris.flu\\)flu RATE, 52))

![](images/0ad1aaafa1e2035f83c735810bfcfc672178fd2f858598eae2329612f7856d22.jpg)  
Series paris.flu$flu.rate

![](images/87b089bfdf8b0c567b3e733e67a0024ca640484a2eaadd65d824c8802e604ac8.jpg)  
Seriesdiff(paris.flu$flu.rate,52)   
Figure 13-2. Plot of the autocorrelation function for the Paris flu rate and the differ‐ enced Paris flu rate. We look only at a limited range of lag values.

If we were careless, we could very well congratulate ourselves on having solved the stationarity issue for this time series by differencing once. But this doesn’t make sense at all. This is weekly data, and we observed a strong seasonality. Why aren’t we seeing it in our autocorrelation plot? We took the default parameters of the acf() function, and this does not take us out nearly far enough in the lag space to see the seasonal effects, which we begin at lag 52 (one year). Let’s redo the acf() with an adequate window (Figure 13-3):

## R
> acf(paris.flu\\(flu RATE, lag.max = 104)
> acf(diff(paris.flu\\)flu rate, 52), lag.max = 104)

![](images/bf358bfd9968cfa321d28ab6bad7e7f785180132ea20967887a2d9f7b5e3eac8.jpg)  
Series paris.flu$flu.rate

![](images/c9bf888ec87738cf6595a1113f145d668080939271b3f89ddfe8bdd55ed03ea7.jpg)  
Seriesdiff(paris.flu$flu.rate,52)   
Figure 13-3. Plot of the autocorrelation function for the Paris flu rate and the differ‐ enced Paris flu rate. We now look at a more extensive range of lag values.

This gives us a more realistic picture of the autocorrelation of our series. As we can see there are substantial autocorrelations at various lags, and this makes sense (at least given my experience) living in a four-season climate. Flu rates will have a strong correlation with neighboring weeks—that is, close to their time of measurement.

Flu rates will also have a strong correlation, given the seasonality, with time periods lagged by around 52 or around 104 because this indicates the yearly seasonality. But flu rates also have a fairly strong relationship with time periods lagged by intermedi‐ ary values, such as half a year, such as half a year (26 weeks) because such lags also relate to seasonal differences and predictable weather variations. For example, we know that in half a year the flu value will likely have changed quite a bit. If it was high earlier, it should be low now and vice versa, again due to the seasonality. All this is depicted in the upper plot of Figure 13-3.

We then examine the differenced series as is illustrated in the lower plot in Figure 13-3. Now we see a substantial amount of the time series autocorrelation has been diminished. However, there remains quite a bit of autocorrelation over a range of values, not just at 52 or 104 weeks (one or two years), but also at intermediate values.

While we might be tempted to continue differencing, we need to remember that realworld data will never fit a SARIMA model perfectly. We instead seek out the most reasonable way to model the data. We can consider differencing again seasonally or taking a different tactic and differencing in linear time. We plot each of these possi‐ bilities here (see Figure 13-4):

![](images/cd38adb9a0dde1c0ea39ef4fdd008e489158d725fe1205c38b1bbc80757ee210.jpg)  
Figure 13-4. Plot of two versions of differencing our series to get a sense of seasonal behavior in the data.

While neither result is ideal, the latter, a standard first differencing of a seasonal dif‐ ferencing, is more satisfactory.

The decision of a fit or a parameter choice is a judgment call as much as it is a matter of applying tests. Here we choose to give weight to the seasonality we clearly observed but also to not overcomplicate the model or make it unduly opaque. So in a SARIMA $\left( p , d , q \right) \left( P , D , Q \right)$ model, we will fit with $d = 1$ and $D = 1$ . We then choose our AR and MA parameters for our standard ARIMA parameters, $\boldsymbol { p }$ and $q$ . We do this with stan‐ dard visualizations, using the following code (see Figure 13-5):

```txt
## R
> par(mfrow = c(2, 1))
> acf(diff(diff(paris.flu$flu_rate, 52), 1), lag.max = 104)
> pacf(diff(diff(paris.flu$flu_rate, 52), 1), lag.max = 104) 
```

![](images/8e8721b67f5ec67f23e5c6aaa2c608add9efd5e4bca1a4dc35ad61c75151235e.jpg)  
Seriesdiff(diff(paris.flu$flu.rate,52),52)

![](images/bd11ef84b5906c68155df8c88d8fabfc410f9201d2da7363255b07d416671cbc.jpg)  
Seriesdif(diffparis.flu$flu.rate,52),1)   
Figure 13-5. Plot of the partial autocorrelation function of the differenced series we have selected.

We have a limited data set and err on the side of a simpler model. The PACF model suggests an AR(2) model could be appropriate, so we will model our data parsimoni‐ ously as a SARIMA (2, 1, 0), (0, 1, 0).

We are interested in understanding how this model would work if we fit it continu‐ ously on new data as it becomes available, the way most models built for real-world systems will function. That is, if we were modeling the flu starting several years ago, each week modeling only with the data that was available up to that point in time, how would our model do? We answer this by rolling the fitting and evaluation of the model forward, as follows:

```r
## R
> ## arima fit
> ## let's estimate 2 weeks ahead
> first.fit.size <- 104
> h <- 2
> n <- nrow(paris.flu) - h - first.fit.size 
```

>>> get standard dimensions for fits we'll produce
>>> and related info, such as coefs
>>> first.fit <- arima(paris.flu\\(flu_rate[1:first.fit.size], order = c(2, 1, 0),
>>> seasonal = list(order = c(0,1,0), period = 52))
>>> first.order <- arimaorder(first.fit)
>>> 
>>> pre-allocate space to store our predictions and coefficients
>>> fit_preds <- array(0, dim = c(n, h))
>>> fit.coefs <- array(0, dim = c(n, length(first.fit\\)coef)))
>>> 
>>> after initial fit, we roll fit forward
>>> one week at a time, each time refitting the model
>>> and saving both the new coefs and the new forecast
>>> caution! this loop takes a while to run
>>> for (i in (first.fit.size + 1): (nrow(paris.flu) - h)) {
>>> # predict for an increasingly large window
data.to.fit = paris.flu[1:i]
fit = arima(data.to.fit\\)flu_rate, order = first.order[1:3],
		seasonal = first.order[4:6])
>>> fit_preds[i - first.fit.size, ] <- forecast(fit, h = 2)\$mean
>>> fit.coefs[i - first.fit.size, ] <- fit\\(coef
}

We then plot these rolling results (see Figure 13-6):

## R
>ylim <- range(paris.flu\\(flu RATE[300:400],
> fit.preds[, h][(300-h):(400-h)]
> par(mfrow = c(1, 1))
> plot(paris.flu\\)date[300:400], paris.flu\\(flu_rate[300:400],
>ylim =ylim, cex = 0.8,
>main = "Actual and predicted flu with SARIMA (2, 1, 0), (0, 1, 0)", xlab = "Date", ylab = "Flu rate")
>lines(paris.flu\\)date[300:400], fit.preds[, h][(300-h):(400-h)],
> col = 2, type = "l",
> lty = 2, lwd = 2)

The plot shows a helpful forecast but also highlights some limitations of this model. It is insufficiently realistic, demonstrated by the fact that it sometimes forecasts nega‐ tive flu rates. The model can do so because there is nothing intrinsic to an ARIMA model to enforce constraints such as flu rates having to be nonnegative. Enforcing these kinds of physical constraints is something we must do with data transforma‐ tions prior to fitting a model.

Also, the model appears more sensitive to outlier points than we would like. Early 2013 is a prime example where several times the model drastically overestimates the flu rate. When it comes to allocation of resources crucial to fighting diseases, this is not an acceptable model.

Finally, this model produces more extreme values at the peaks than are ever meas‐ ured in a given year. This could lead to overallocation of resources relative to actual need, which is not a good outcome, particularly when resources are highly con‐ strained. This is a concern about the model that is just as rooted in real-world resource constraints as it is in pure data analysis.

![](images/e6259df3e61de2137bd0838d7cf0c2b06802f74df551ee36efb5ce6494a964a4.jpg)  
ActualandpredictedfluwithSARIMA (2,1,0),(0,1,0)   
Figure 13-6. The flu rates (points) paired with our SARIMA predictions (dotted line). The predictions of this simple model could assist with public health planning.

Now that we have considered the fit of a basic ARIMA model to this problem, we will take a look at other modeling possibilities.

# An alternative ARIMA model: Exogenous harmonic regressors instead of seasonality

Given the performance of the SARIMA model discussed in the previous section, there are a number of modifications we can make. Here we consider two modifica‐ tions, each of which is independent of the other and could be applied separately.6

First, we would like to build in constraints to our model to prevent the prediction of negative values. One way to do this is a log transformation of the data, such that we are predicting the logarithm of the value of the time series rather than the value itself.

Then, when we want to see the “true” series representing the numbers actually meas‐ ured, we will use an exponential transform to back out the logarithmic transform and get the predictions in their real units.

Second, we would like to find a more transparent way of dealing with the seasonality of the data. While we described our seasonal ARIMA model as simple, in fact a model dealing with a seasonal recurrence of 52 is not simple at all. Seasonal ARIMA models do better on shorter seasonal cycles and less well on longer seasonal cycles (52 is a long seasonal cycle).

Here we will use dynamic harmonic regression. In this approach, we find a Fourier series that describes the periodicity in our data, and then use this series as an exoge‐ nous regressor that is fit alongside the ARIMA terms.7 Because we can extrapolate the Fourier series forward in time (due to its purely periodic nature), we can also pre‐ compute the values we expect for the future when generating forecasts.

The strength of this model is that the degrees of freedom of the model can be used to explain underlying behavior in addition to the seasonal behavior rather than devoting a lot of explanatory power to the seasonal behavior.

There are a few downsides to dynamic harmonic regression. First, we assume the behavior is quite regular and recurs at exactly the same interval. Second, we assume that the seasonal behavior is not changing; that is, the period and amplitude of the seasonal behavior are not changing. These limitations are similar to the SARIMA model, although the SARIMA model shows more flexibility in how the amplitude of seasonality affects data over time.

Here, we demonstrate how to perform a dynamic harmonic regression with R code similar to what we employed before:

```r
## R
>>> preallocate vectors to hold coefs and fits
>>> fit_preds <- array(0, dim = c(n, h))
>>> fit.coefs <- array(0, dim = c(n, 100))
>>> 
>>> #exogenous regressors
>>> #that is components of Fourier series fit to data
>>> flu.ts <- ts(log(paris.flu\\(flu RATE + 1) + 0.0001,
>>> frequency = 52)
>>> add small offsets because small/0 vals
>>> cause numerical problems in fitting
>> exog.regressors <- fourier(flu.ts, K = 2)
>> exog.colnames <- colnames(exog.regressors) 
```

```txt
> ## fit model anew each week with
> ## expanding window of training data
> for (i in (first.fit.size + 1): (nrow(paris.flu) - h)) {
> data.to.fit <- ts(flu.ts[1:i], frequency = 52)
> exogs.for.fit <- exog.regressors[1:i,]
> exogs.for.predict <- exog.regressors[(i + 1): (i + h),]
>
> fit <- auto.arima(data.to.fit,
> xreg = exogs.for.fit,
> seasonal = FALSE)
>
> fit_preds[i - first.fit.size, ] <- forecast(fit, h = h,
> xreg = exogs.for.predict)$mean
> fit.coefs[i - first.fit.size, 1:length(fit$coef)] = fit$coef
}
``` 
```

Here we have made a few adjustments to the code from the previous section. First, we use a ts object.8 With a ts object, we indicate explicitly the seasonality of the time series (52 weeks), when creating the ts object.

We log-transform the data at this point as well to ensure positive predictions of our ultimate value of interest, the flu rate:

```txt
## R
> flu.ts = ts(log(paris.flu\\(flu RATE + 1) + 0.0001,  # add epsilon
>                frequency = 52) 
```

We also add a small numerical offset $\left( + \begin{array} { l } { 0 . 0 0 0 1 } \end{array} \right)$ because numerical fitting does not play well with strict zero values or even very small values. One of our two adjust‐ ments is already accomplished with just this line of code (i.e., the log transformation to enforce a physical condition of nonnegative values).

Next we generate the exogenous harmonic regressors (that is, the Fourier approxima‐ tion) we are using in place of seasonal parameters in the SARIMA. We do this via the forecast package’s fourier() function:

```r
## R
> exog.regressors <- fourier(flu.ts, K = 2)
> exog.colnames <- colnames(exog.regressors) 
```

We generate the accompanying harmonic series for our entire time series first, and then we subset it as appropriate to our expanding window rolling fit later in the loop.

The hyperparameter K indicates how many separate sine/cosine pairs we will include in our fit, where each one represents a new frequency used to fit the sine/cosine. In general, K will be larger for larger seasonal period lengths and smaller for smaller

ones. In a more extended example, we could consider how to use an information cri‐ terion to tune K, but for this example we use $\mathsf K = 2$ as a reasonable model.

Finally, all that is left to do is to generate new fits that take into account the exoge‐ nous Fourier components we just fit. We do the fitting as follows, where the xreg parameter takes the fit Fourier series as additional regressors, which are fit along with the standard ARIMA parameters:

```r
## R
> fit <- auto.arima(data.to.fit,
> xreg = exogs.for.fit,
> seasonal = FALSE) 
```

We set the seasonal parameter to FALSE to ensure that we will not have redundant seasonal parameters given our decision to use dynamic harmonic regression in this case.

We also need to include regressors when we generate a forecast, meaning we need to indicate what the regressors will be at the time the forecast is targeting:

```txt
## R
> fit.predicts[i - first.fit.size, ] <- forecast(fit, h = 2h,
> xreg = exogs.for.predict) $ mean 
```

We plot the performance of this model, as follows (see Figure 13-7):

```txt
## R
>ylim = range(paris.flu\\(flu RATE)
>plot(paris.flu\\)date[300:400], paris.flu\\)flu RATE[300:400],
>
>ylim =ylim, cex = 0.8,
>main = "Actual and predicted flu with ARIMA +
>harmonic regressors",
>xlab = "Date", ylab = "Flu rate")
>lines(paris.flu\\)date[300:400], exp(fit.preds[, h][(300-h):(400-h)]),
>col = 2, type = 'l',
>fty = 2, lwd = 2) 
```

On the positive side, our predicted series no longer features negative values. How‐ ever, there are many disappointing aspects of the model’s performance. Most obvi‐ ously, many of the predictions are quite off. The peaks are far the wrong magnitude, and they occur at the wrong time.

![](images/62dc2e22952f0f6b871bda91500218bd20b6d2805a322249d21cca43995098fd.jpg)  
Actual and predictedfluwithARIMA+ harmonicregressors,   
Figure 13-7. Plot of the actual flu rates (points) compared to our ARIMA + dynamic harmonic regression model predictions (dashed line).

One explanation for the problems is that regular seasonality is not a good description of the seasonality of the flu.9 According to the Centers for Disease Control and Pre‐ vention (CDC), in the United States flu can peak each winter season as early as December or as late as March. We can see this in the test data. Consider the following code and the plot in Figure 13-8, which identify the peaks in the testing range of the data:

```txt
## R
> plot(test\(flu_rate)
> which(test\)flu_rate > 400)
Error in which(res\)flu_rate > 400): object 'res' not found
> which(test\)flu_rate > 400)
[1] 1 59 108 109 110 111 112 113
> abline(v = 59)
> which.max(test\)flu_rate) 
```

[1] 109 > abline $\mathbf { \tilde { v } } \ = \ 1 \boldsymbol { \Theta } 9$ )

![](images/71c972f0cfc0b2ba555c267b15a0183f8bde835ee7ad4a43c7f3866f13b5e740.jpg)  
Figure 13-8. Plot of only the test values for the flu and their apparent peak locations.

The peaks occur at indices 59 and 109, that is 50 weeks (not 52 weeks) apart from one another. Also the peak that came at index 59 came at least 59 weeks after the previous peak, and possibly more since we do not have the full extent of the peak around index $= 0$ in our plot.

With sample peak-to-peak distances of more than 59 in one case and 50 in another, we can see quite a bit of variability from year to year. Our dynamic harmonic regres‐ sion would not take this into account, and it enforces a more rigid model of seasonal‐ ity than does the SARIMA model, because the latter’s seasonal behavior can change over time. This mismatch between assumptions and the data could go a long way toward explaining the poor performance of this model, such that using a bad model actually drew our attention to an important feature of our data we had not noticed earlier.

Despite poor performance, this alternate model has proven useful for a few reasons. It showed us the value of building the physical limitations of the system into our data preprocessing, in this case via a log transformation. It has also shown us the value of experimenting with several classes of models with an underlying theory behind the model selection at each point. We chose this model to simplify our seasonality, but

instead what we discovered is that, likely, neither the SARIMA nor the dynamic har‐ monic regression model of seasonality is a very good one for this system.

# What Is State of the Art in Flu Forecasting?

The models we just explored were relatively simple but reasonably effective in mak‐ ing either forecasts or nowcasts of flu rates in the Paris metropolitan region for short forecast horizons. These models are a good start both in getting to know your data set and in recognizing the limitations of what could be predictive in a noisy but compli‐ cated system, such as flu in a given geographic region.

It’s interesting to compare our simple approaches with some cutting-edge approaches currently deployed by government agencies or recently published by academics seek‐ ing to discuss the state of the art. Next I provide a brief overview of how well flu fore‐ casts currently work and what novel approaches researchers have developed in recent years to try to improve upon traditional models.

# Research in flu forecasts

The CDC actively encourages researchers to work on forecasting the flu, even spon‐ soring an R package to make its data readily available. For more than five years, the CDC has also sponsored flu forecasting competitions, although it was only for the 2017–2018 flu season that it actually incorporated a flu forecast into its official com‐ munications and bulletins. The Delphi research group from Carnegie Mellon has won the majority of competitions so far, and the group forecasts the flu with what they describe as three distinct efforts:

• An Empirical Bayes approach that applies past flu season data with a series of manipulations to form priors for the current season’s data based on the overall “shapes” of the time series from past flu seasons   
• A crowdsourcing platform where anyone is welcome to submit a prediction of flu rates   
• A nowcasting method that uses sensor fusion, aggregating data from many sour‐ ces, such as Wikipedia access counts and relevant Twitter queries, to produce geographically localized flu predictions

This is the set of diverse approaches used in just one academic research group! If we look further afield into academia, we see even more diversity:

• Using deep convolutional networks (CNNs) to classify Instagram pictures and use the outputs of these CNNS along with text-related features from Twitter as inputs into a variety of machine learning models, including XGBoost trees. One such paper had the advantage of focusing on a small linguistic community (Fin‐

nish speakers), which enabled the use of mainstream social media platforms in a way that could still be regionally specific.

• Identifying reliable users within larger social media clusterings. One paper focu‐ ses on improving flu predictions by finding the users best placed and most trust‐ worthy on social media platforms.   
• Accessing electronic health records to have a more complete and complementary data source in addition to publicly available data. One paper showed that extremely large gains in forecast accuracy at a number of timescales could be obtained via the integration of electronic health records into forecasting input streams. Unfortunately, this is difficult to arrange and suggests that accurate flu forecasting abilities will go to wealthy data holders rather than to the most crea‐ tive researchers (although, of course, sometimes these can be one and the same).

As we can see from the diversity of approaches to this problem, there are many routes to make a flu forecast, and none is the definitive winner just yet. This continues to be an area of active research and development, even as some of the better strategies have been applied to governmental uses and public information.

Our discussion here is only the tip of the iceberg. There is a lot of biology, sociology, medicine, and economic policy that goes into determining the course of a flu season, and there are a variety of models that are less oriented toward time series analysis and more oriented toward other aspects of the flu behavior. Time series brings one rich perspective to a very complicated topic.

# Predicting Blood Glucose Levels

Another active area of machine learning research for time series data in health appli‐ cations is the prediction of blood glucose levels for individual patients. Diabetes patients themselves make such predictions all the time, particularly if they have a level of disease such that they inject themselves with bolus insulin, which is specifi‐ cally used at mealtimes. In such cases, diabetics need to estimate how the food they are about to eat will affect their blood sugar and adjust their dose accordingly.

Similarly, diabetes patients must time their meals and medication to optimize their blood sugar, which is best kept within a specific range, neither too high nor too low. In addition to needing to account for blood-sugar-changing activities such as eating and exercise, diabetics also need to account for specific time-of-day effects. For exam‐ ple, the dawn phenomenon is a rise in blood sugar that occurs in all humans but can be problematic for those with diabetes. On the other hand, for people with type 1 dia‐ betes, blood sugar lows during sleeping hours can be life-threatening events resulting from the failure to make an accurate prediction.

Here we look at a small data set: the self-reported continuous glucose monitor (CGM) data for one individual in several noncontiguous segments of time. This data was self-published on the internet and modified to preserve the privacy of the patient.

There are other options for obtaining diabetes data sets. In addition to large health‐ care organizations and some startups with large troves of CGM data, there are numerous self-published data sets available as individuals increasingly manage their diabetes via DIY efforts, such as the Night Scout project. There are also several diabe‐ tes CGM data sets open for research purposes.

In this section, we will explore the messiness of a real-world data set and attempt to forecast that data.

# Data Cleaning and Exploration

The data is stored in several files available on this book’s GitHub repository. We first load the files and combine them into one data.table:

```r
## R
> files <- list.files(full-names = TRUE)
> files <- grep("entries", files, value = TRUE)
> dt <- data.table()
> for (f in files) {
> dt <- rbindlist(list(dt, fread(f)))
}
>
> #
> ## remove the na columns
> dt <- dt[!is.na(sgv)] 
```

Strings of date information are available, but we don’t have a proper timestamp class column, so we make one with the information we have, which includes a time zone as well as a date string:

```txt
## R
> dt[, timestamp := as.POSIXct(date)]
> ## this works for me because my computer is on EST time
> ## but that might not be true for you
>
> ## proper way
> dt[, timestamp := force_tz(as.POSIXct(date), "EST")]>
>
> ## order chronologically
> dt = dt[order(timestamp, decreasing = FALSE)] 
```

Then we inspect the data post-processing:

```markdown
## R
> head(dt[., . (date, sgv]])
date sgv
1: 2015-02-18 06:30:09 162
2: 2015-02-18 06:30:09 162 
```

3: 2015-02-18 06:30:09 162   
4: 2015-02-18 06:30:09 162   
5: 2015-02-18 06:35:09 154   
6: 2015-02-18 06:35:09 154

There are many duplicated data entries, so we need to scrub these for two reasons:

• There is no reason a priori that certain data points should be privileged or weigh‐ ted more highly than others, but duplication will produce such an effect.   
• If we are going to form a feature based on time series windows, these will be non‐ sensical if there are duplicated time points.

We first see if we can solve this problem by removing all perfectly duplicated rows:

```r
## R
> dt <- dt![!duplicated(dt)] 
```

However, we should not assume this resolves the problem, so we check whether non‐ identical data points exist for the same timestamp:

```markdown
## R
> nrow(dt)
[1] 24861
> length(unique(dt$timestamp))
[1] 23273
> ## we still have duplicated data as far as timestamps
> ## we will delete them 
```

Since we still have duplicate time points, we do some data review to see what this means. We identify the timestamp with the most duplicate rows and examine these:

```txt
## R
> ## we can identify one example using
> ## dt[, .N, timestamp][order(N)]
> ## then look at most repeated data
> dt(date == "2015-03-10 06:27:19")
device      date      dateString svv direction type
1: dexcom 2015-03-10 06:27:19 Tue Mar 10 06:27:18 EDT 2015 66 Flat svv
2: dexcom 2015-03-10 06:27:19 Tue Mar 10 06:27:18 EDT 2015 70 Flat svv
3: dexcom 2015-03-10 06:27:19 Tue Mar 10 06:27:18 EDT 2015 66 Flat svv
4: dexcom 2015-03-10 06:27:19 Tue Mar 10 06:27:18 EDT 2015 70 Flat svv
5: dexcom 2015-03-10 06:27:19 Tue Mar 10 06:27:18 EDT 2015 66 Flat svv
6: dexcom 2015-03-10 06:27:19 Tue Mar 10 06:27:18 EDT 2015 70 Flat svv
7: dexcom 2015-03-10 06:27:19 Tue Mar 10 06:27:18 EDT 2015 66 Flat svv
8: dexcom 2015-03-10 06:27:19 Tue Mar 10 06:27:18 EDT 2015 70 Flat svv
## more inspection suggests this is not too important 
```

Looking at these values, we can see that there are distinct blood glucose values reported, but they are not wildly different,10 so we suppress the duplicate timestamps even though they are not exact duplicates:11

```r
## R
> dt <- unique(dt, by=c("timestamp")) 
```

Now that we have proper timestamps and single values per timestamp, we are in a position to plot our data and get a sense of its extent and behavior (see Figure 13-9).

![](images/2b34b70032596a12ef8d6dfe9b4908ccd4b40c73218483ab3b2c603dbbe964e9.jpg)  
Figure 13-9. The full time domain and value range of the available data in this naive plot of the time series. Unfortunately, the data is so spread out in time and disjointed that this plot does not give us a good understanding of the system’s behavior.

We zoom in on the period we see early in the time series, March 2015 (see Figure 13-10):

```r
## R
> ## let's look at some shorter time windows in the data
> start.date <- as.Date("2015-01-01")
> stop.date <- as.Date("2015-04-01") 
```

![](images/d7bcaf89d5ef43f33f190ffda47e214e280c0e08f90f36891c74af24a1658d03.jpg)  
Figure 13-10. Focusing on a specific segment of the time series is more helpful but still too compressed for us to get a sense of any sort of time series dynamics. We should zoom in even further on the time axis.

Even in the plot of just the March dates, we still don’t get a sense of the behavior of the time series. Is there seasonality? Drift? A daily pattern? We don’t know, so we plot an even shorter time window (see Figure 13-11):

```txt
## R
>>> also look at a single day to see what that can look like
>>> if we had more time we should probably do 2D histograms of days
>>> and see how that works
>>> par(mfrow = c(2, 1))
>>> start.date = as.Date("2015-07-04")
>>> stop.date = as.Date("2015-07-05")
>>> dt[var(range, start.date, stop.date), plot(timestamp, sgv, cex = .5)]
>>> start.date = as.Date("2015-07-05")
>>> stop.date = as.Date("2015-07-06")
>>> dt[var(range, start.date, stop.date), plot(timestamp, sgv, cex = .5)]
>>> ## if we had more days we could probably do some "day shapes"
>>> ## analysis, but here we don't seem to have enough data 
```

![](images/4695d721dd733518538f9c7310f48a6ebe24de37d98939cd09f61845181b604b.jpg)

![](images/54139829410a902222db2cab10f4c010fd2e45283d3da1cb44d1c00356d03f0e.jpg)  
Figure 13-11. Plot of the data from two specific days in July. We can see something of a daily pattern emerge. Finally, we are looking at the data at a temporal scale where the human eye can make sense of what is going on.

By doing more of these exploratory plots (left as an exercise for the reader), you should develop an intuition of the scale of the data, the qualitative dynamics, and what might be important in describing its behavior. Some further exploratory techni‐ ques you should apply to this data set include:

• Day-length 2D histograms to look for intraday patterns, and related clustering exercises. Are there different kinds of days?   
• Group-by statistics on hours of the day or seasons of the year to look for system‐ atic difference across time.   
• Smoothing of the data to look for long-term trends and particularly to compare interday relationships rather than intraday relationships. It would be valuable to develop ultra-long-range blood glucose predictions, which you can do only by looking beyond the more obvious intraday patterns.

# Generating Features

Armed with some background knowledge—as well as with observations from our brief exploration of the data set―we can generate features for our data that would be helpful for predicting blood glucose.

We begin by featurizing time itself. For example, we noticed the day appears to have a structure to it, which suggests that the time of day should be relevant for making predictions. Likewise, if you plot the time series of blood glucose for different months of the data, some months show greater variance than others.12 We generate a few time-related features here (see Figure 13-12):

```txt
## R
> ## we should consider putting in a feature for month of the year.
> ## perhaps there are seasonal effects
> dt[, month := strtotime(timestamp, "%m")] 
>
> dt[, local(hour := as.numeric(strftime(timestamp, "%H"))]
> ## we notice some hours of the day are greatly overrepresented
> ## this person is not consistently wearing the device or perhaps
> ## the device has some time of day functionality or recordkeeping
> ## issues. since it is a weird peak at midnight, this seems to
> ## suggest the device rather than a user (who puts device both
> ## on and off at midnight?)
>
> hist(dt$local(hour) 
```

We also consider qualities of the time series values and not just the timestamps of data collection. For example, we can see that the mean blood glucose values vary with the time of day (Figure 13-13):

```txt
## R
> ## blood glucose values tend to depend on the time of day
> plot(dt[, mean(sgv), local(hour][order(local(hour)]], type = 'l',
> ylab = "Mean blood glucose per local hour",
> xlab = "Local hour") 
```

![](images/ba9a38f8bca5dd8d8b59243561d0845d10192c99218a47ab0390c4bd67400ec5.jpg)  
Histogram ofdt$local.hour   
Figure 13-12. From the histogram of local hour we can see that the times the CGM is collecting and reporting data are not random. The heavy concentration around mid‐ night additionally suggests an irregularity in reporting or device functioning that is most likely not due to user behavior (the user is not likely manipulating their device at that time of day).

![](images/b16e44d985c0e994afa18b4b935b37166a0c00419b2f347c4742f8f2f185179e.jpg)  
Figure 13-13. Mean blood glucose varies substantially by the time of day.

There is other interesting data about the blood glucose values. The column named direction in the data set pulls information from the CGM device, applies the manu‐ facturer’s proprietary software, and comes up with a directional trend label for the data. We can use this rather than computing our own trend statistics, and so we try to

understand it a little, such as by asking whether there are different trends for different hours.

We define a function that will give us the nth most popular label for a given vector, and we use that first to find the most popular label for each local hour of the day:

```txt
## R
> nth(pos = function(x, pos) {
>     names(range(-table(x))) [pos]
>     ## this code snippet thanks to r user group
>
}
> dt[, nth(pos(direction, 1), local(hour)[order(local(hour)]]
local(hour V1
1:      0 Flat
2:      1 Flat
3:      2 Flat
...
21:      20 Flat
22:      21 Flat
23:      22 Flat
24:      23 Flat
    local(hour V1 
```

The most popular direction label is “flat” for all hours of the day, which is reassuring because the dynamics of the system would be questionable if the trend could be strongly predicted simply by the time of day.

However, the second most common directional label per hour of the day does vary with the time of day, as we can see here:

```txt
## R
> dt[, nth(pos(direction, 2), local(hour][order(local(hour)]
local(hour V1
1:     0    FortyFiveUp
2:     1    FortyFiveUp
3:     2    FortyFiveUp
4:     3    FortyFiveDown
5:     4    FortyFiveDown
6:     5    FortyFiveUp
7:     6    FortyFiveDown
8:     7    FortyFiveDown
9:     8    FortyFiveDown
10:     9    FortyFiveUp
11:     10    FortyFiveUp
12:     11    FortyFiveDown
13:     12    FortyFiveDown
14:     13    FortyFiveDown
15:     14    FortyFiveDown
16:     15    FortyFiveDown
17:     16    FortyFiveUp
18:     17    FortyFiveUp
19:     18    FortyFiveDown 
```

```txt
20: 19 FortyFiveUp  
21: 20 FortyFiveUp  
22: 21 FortyFiveUp  
23: 22 FortyFiveUp  
24: 23 FortyFiveDown  
local(hour V1 
```

It’s interesting to cross-reference these labels against the intraday data plots we included in the previous section. Do the results match? (Answering this is left as an exercise for the reader.)

Next we address a numeric label that may be predictive, namely the variance of the blood glucose measures in the most recent time window. We compute the standard deviation for a short lookback window.

To do so, we must account for the noncontiguous nature of the data. We don’t want to be computing standard deviations of data points that are not adjacent in time but merely adjacent in the data.table. We will compute time deltas between target win‐ dow beginnings and ends to ensure they are of the right timescale, but we need to be careful about how we do this. Here is an example of how we could easily go wrong:

```perl
## R
> ## FIRST don't do this the wrong way
> ## Note: beware calculating time differences naively!
> as.numeric(dt$timestamp[10] - dt$timestamp[1])
[1] 40
> as.numeric(dt$timestamp[1000] - dt$timestamp[1])
[1] 11.69274
> dt$timestamp[1000] - dt$timestamp[1]
Time difference of 11.69274 days 
```

It is possible the time differences are returned in different units, which would make the results of the preceding code nonsense. The correct way to compute a time delta, as well as the way we use this time delta to determine the validity of a standard devia‐ tion computation for a given row, is as follows:

```python
## R
> dt[, delta.t := as.numeric(difftime(timestamp, shift(timestamp, 6)), units = 'mins'))
> dt[, valid.sd := !is.na(delta.t) & delta.t < 31]
> dt[, .N, valid.sd]
valid.sd     N
1:     FALSE     1838
2:     TRUE     21435 
```

Once we have labeled the rows for which a standard deviation calculation is appro‐ priate, we run the calculations. We compute across all rows, and we plan to overwrite the invalid values with a column average as a simple form of missing value imputation:

```txt
## R
> dt[, sdwindow := 0]
> for (i in 7:nrow(dt)) {
>     dt[i,]$sdwindow = sd(dt[(i-6):i]$sgv)
}
> }
## we will impute the missing data for the non-valid sd cases
> ## by filling in the population mean (LOOKAHEAD alert, but we're aware)
> imputed.val = mean(dt[valid.sd == TRUE]$sdwindow)
> dt[valid.sd == FALSE, sdwindow := imputed.val] 
```

Next, we set up a column to establish the true value we should target for our forecast. We choose to forecast blood glucose values 30 minutes in advance, which would be enough time to warn someone with diabetes if a dangerously high or low blood sugar was predicted. We put the target forecast value into a column called target. We also create another column, pred.valid, which indicates whether the data points preced‐ ing the time when we want to make a forecast are sufficiently complete (that is, regu‐ larly sampled in the previous 30 minutes at 5-minute increments):

```txt
## R
>>> now we also need to fill in our y value
>>> the actual target
>>> this too will require that we check for validity as when
>>> computing sd due to nontemporally continuous data sitting
>>> in same data.table. let's try to predict 30 minutes ahead
>>> (shorter forecasts are easier)
>>> 
>>> we shift by 6 because we are sampling every 5 minutes
dt[, pred deltas.t := as.numeric(difftimeshift(timestamp, 6,
type = "lead"),
timestamp,
units = 'mins'))
>>> dt[, pred.valid := !is.na(pred deltas.t) & pred deltas.t < 31]
>>> 
>>> dt[, target := 0]
>>> for (i in 1:nrow(dt)) {
>>> dt[i, $target = dt[i + 6]$sgv
} 
```

We spot-check our work to see if it produces sensible results:

```markdown
## R
> ## now we should spot check our work
> i = 300
> dt[i + (-12:10), .(timestamp, svg, target, pred.valid)]
timestamp svg target pred.valid
1: 2015-02-19 16:15:05 146 158 TRUE
2: 2015-02-19 16:20:05 150 158 TRUE
3: 2015-02-19 16:25:05 154 151 FALSE
4: 2015-02-19 16:30:05 157 146 FALSE
5: 2015-02-19 16:35:05 160 144 FALSE
6: 2015-02-19 16:40:05 161 143 FALSE
7: 2015-02-19 16:45:05 158 144 FALSE 
```

<table><tr><td>8:</td><td>2015-02-19</td><td>16:50:05</td><td>158</td><td>145</td><td>FALSE</td></tr><tr><td>9:</td><td>2015-02-19</td><td>17:00:05</td><td>151</td><td>149</td><td>TRUE</td></tr><tr><td>10:</td><td>2015-02-19</td><td>17:05:05</td><td>146</td><td>153</td><td>TRUE</td></tr><tr><td>11:</td><td>2015-02-19</td><td>17:10:05</td><td>144</td><td>154</td><td>TRUE</td></tr><tr><td>12:</td><td>2015-02-19</td><td>17:15:05</td><td>143</td><td>155</td><td>TRUE</td></tr><tr><td>13:</td><td>2015-02-19</td><td>17:20:05</td><td>144</td><td>157</td><td>TRUE</td></tr><tr><td>14:</td><td>2015-02-19</td><td>17:25:05</td><td>145</td><td>158</td><td>TRUE</td></tr><tr><td>15:</td><td>2015-02-19</td><td>17:30:05</td><td>149</td><td>159</td><td>TRUE</td></tr><tr><td>16:</td><td>2015-02-19</td><td>17:35:05</td><td>153</td><td>161</td><td>TRUE</td></tr><tr><td>17:</td><td>2015-02-19</td><td>17:40:05</td><td>154</td><td>164</td><td>TRUE</td></tr><tr><td>18:</td><td>2015-02-19</td><td>17:45:05</td><td>155</td><td>166</td><td>TRUE</td></tr><tr><td>19:</td><td>2015-02-19</td><td>17:50:05</td><td>157</td><td>168</td><td>TRUE</td></tr><tr><td>20:</td><td>2015-02-19</td><td>17:55:05</td><td>158</td><td>170</td><td>TRUE</td></tr><tr><td>21:</td><td>2015-02-19</td><td>18:00:04</td><td>159</td><td>172</td><td>TRUE</td></tr><tr><td>22:</td><td>2015-02-19</td><td>18:05:04</td><td>161</td><td>153</td><td>FALSE</td></tr><tr><td>23:</td><td>2015-02-19</td><td>18:10:04</td><td>164</td><td>149</td><td>FALSE</td></tr><tr><td colspan="6">timestamp</td></tr></table>

Take a careful look at this output. Something in it should make you question whether we were too harsh in assessing “validity” for computing the standard deviation of a recent window on the blood glucose data. As an independent exercise, see if you can spot those points, and think about how you could rework the pred.valid labeling to be more correctly inclusive.

Now we have a wide array of features and a target value to use for training whatever model we will use to generate forecasts, but we are not done generating features. We should simplify some of the temporal features we have already generated to lessen the complications of our model. For example, rather than having local hour be an input of 23 binaries (one for each hour of the day minus one), we should lessen the number of “hour” categories. We do this like so:

```txt
## R
>>> Let's divide the day into quarters rather than
>>> into 24-hour segments. We do these based on a notion
>>> of a 'typical' day
dt[, day.q.1 := between(local(hour, 5, 10.99)]
dt[, day.q.2 := between(local(hour, 11, 16.99)]
dt[, day.q.3 := between(local(hour, 17, 22.99)]
dt[, day.q.4 := !day.q.1 & !day.q.2 & !day.q.3] 
```

We also simplify the month data to a simpler set of categories:

```txt
## R
> ## let's have a "winter/not winter" label rather than
> ## a month label. this decision is partly based on the
> ## temporal spread of our data
> dt[, is.winter := as.numeric(month) < 4] 
```

Finally, to use the direction column we need to one-hot-encode this value, just as we did the different times of the day. We also clean up some inconsistently labeled features ("NOT COMPUTABLE" versus "NOT_COMPUTABLE"):

```txt
## R
> ## we also need to one-hot encode a direction feature
> ## and clean that data somewhat
> dt[direction == "NOT COMPUTABLE", direction := "NOTCOMPUTABLE"]
> dir-names = character()
> for (nm in unique(dt$direction)) {
> new.col = pasteθ("dir=", nm)
> dir-names = c(dir-names, new.col)
> dt[, eval(IParse(text = pasteθ(new.col, " := (direction == "", nm, ""))))]
> } 
```

Now we have one-hot-encoded relevant features and simplified others, so we are ready to direct these features into a model.

# Fitting a Model

At last we get to the fun part of time series analysis: making a prediction.

![](images/7fb191253b2ddd16aa0f3535e82238a7610c2e3f5f24ea981faf26a6cb1e0c9f.jpg)

# How Much Time Should You Spend Modeling?

As you have seen with the two real-world models of this chapter, real-world data is messy, and each time it is scrubbed it has to be done so with domain knowledge and common sense. There is no general template. However, you should always proceed carefully and without rushing to fit a model. We only want to fit models once we are confident we are not putting garbage into them!

Our first order of business is to create training and testing data sets:13

```r
## R
> ## we need to set up training and testing data
> ## for testing, we don't want all the testing to come
> ## at end of test period since we hypothesized some of behavior
> ## could be seasonal let's make the testing data
> ## the end data of both "seasons"
> winter.data <- dt[var.winter == TRUE]
> train.train.cutoff <- round(nrow(winter.data) * .9)
> train.winter <- winter.data[1:train.train.cutoff]
> test.winter <- winter.data[(train.train.cutoff + 1): nrow(winter.data)]
>
> spring.data <- dt[var.winter == FALSE]
> train.train.cutoff <- round(nrow(spring.data) * .9) 
```

```r
> train.spring <- spring.data[1:train.train.cutoff]
> test.spring <- spring.data[(train.train.cutoff + 1): nrow(spring.data)]
>
> train.data <- rbindlist(list(train.winter, train.spring))
> test.data <- rbindlist(list(test.winter, test.spring))
>
> ## now include only the columns we should be using
> ## categorical values: valid.sd, day.q.1, day.q.2, day.q.3, is.winter
> ## plus all the 'dir_' colnames
> col-names <- c(dir-names, "sgv", "sdwindow", "valid.sd",
"day.q.1", "day.q.2", "day.q.3", "is.winter")
>
> train.X <- train.data[, col-names, with = FALSE]
> train.Y <- train.data\$target
>
> test.X <- test.data[, col-names, with = FALSE]
> test.Y <- test.data\$target 
```

Consistent with much cutting-edge work, we choose XGBoost gradient boosted trees as our model. In recent publications, these have met or exceeded state-of-the-art met‐ rics for glucose prediction in some use cases:14

```r
## R
> model <- xgboost(data = as.matrix(train.X), label = train.Y,
> max.depth = 2,
> eta = 1,
> nthread = 2,
> nrounds = 250,
> objective = "reg:linear")
> y_pred <- predict(model, as.matrix(test.X)) 
```

We can then inspect the results of this prediction (see Figure 13-14). We also look at our prediction for a specific day (see Figure 13-15):

```r
## R
>>> now let's look at a specific day
>>> test.data(date < as.Date("2015-03-17"))
>>> par(mfrow = c(1, 1))
>>> i <- 1
>>> j <- 102
>>>ylim <- range(test.Y[i:j], y_pred[i:j])
>>> plot(test.data$timestamp[i:j], test.Y[i:j],ylim =ylim)
>>> points(test.data$timestamp[i:j], y_pred[i:j], cex = .5, col = 2) 
```

![](images/c4897bc31b82d88799cf9a7be3bc6104c84b138a2927e576366a61fdccd1f749.jpg)

![](images/2d2f81a75a1f702ea3c380ef22277cbce970ef193e458ab07fe5092043d297da.jpg)  
Figure 13-14. At this large scale our predictions look good, but it’s hard to judge zoomed out so far. This is not how someone using the prediction would experience it.

![](images/43a00c45a7ece83fc4b2a7a66b166a7c66da8d678b02a6aa73bd2ea6542399ef.jpg)  
Figure 13-15. Examining single days at a time offers a better perspective on how well our prediction algorithm works.

The predictions look reasonable, but this plot suggests there may also be problems with the underlying data. The blob around midnight between Sunday and Monday seems unrealistically low, particularly as neighboring points are not low. It’s more likely the device malfunctioned than that someone experienced such a drastic but short-lived low blood glucose. We might consider more data scrubbing or perhaps additional labels to indicate that particular points seem suspect.15 It’s probably a sign of overfitting that our model forecasts these likely invalid low blood glucose data points. We want our model to forecast actual blood glucose values and not device malfunctions.

At this point, we should consider the goals of our algorithm. If we merely want to show that we can do a reasonably good job of predicting blood glucose half an hour in advance—without even the benefit of knowing what someone is eating or when they are exercising—we can already say “mission accomplished.” That’s already quite something—and it’s impressive that we can make reasonable forecasts without hav‐ ing relevant inputs, such as meal and exercise information.

However, we are aiming mostly for a forecast that will warn people of danger—that is, when their blood sugar will go too low or too high. Let’s focus on those data points as we consider how our predictions stack up against measured values.

Importantly, we plot the high and low blood glucose value separately. If we plotted them together, we’d see a seemingly high correlation by creating an artificially dumbbell-shaped distributed of high and low points (see Figure 13-16):

```txt
## R
> par(mfrow = c(2, 1))
> high.idx = which(test.Y > 300)
> plot(test.Y[high.idx], y_pred[high.idx], xlim = c(200, 400),
> ylim = c(200, 400))
> cor(test.Y[high.idx], y_pred[high.idx])
[1] 0.3304997
>
> low.idx = which((test.Y < 60 & test.Y > 10))
> plot(test.Y[low.idx], y_pred[low.idx], xlim = c(0, 200),
> ylim = c(0, 200))
> cor(test.Y[low.idx], y_pred[low.idx])
[1] 0.08747175 
```

These plots put our model’s performance in a harsher light. Particularly concerning is that the model does not do very well at low blood sugar levels. This may be because dangerously low blood sugar occurrences are rare incidents, so the model does not have much opportunity to train on them. We should consider data augmentation or

resampling to find ways to make this phenomenon more salient to our model. We can also modify our loss function to more heavily weight the examples already in our data.

![](images/400fc26ed136cdf756ede4dc819789412e11dc7c33345f93ec211083240e921b.jpg)

![](images/8e3b3b95b46158ec5c1d342bd3d8e1465dd1f0b0898116ba5a97d27be8a35a5b.jpg)  
Figure 13-16. Plot of the predictions and actual values against one another individually at each end of the extreme. On top we plot the values against one another for high measured blood glucose values and on the bottom for low measured blood glucose values.

Here we have seen a case where an initial model’s forecasts appear respectable but may not fulfill the most essential purpose of the model. In this case we can predict general blood glucose trends, but we should do more work to predict high and low blood sugars, with a particular emphasis on life-threateningly low levels of blood sugar.

In this case, as in the case of flu forecasting, we have seen that common sense is essential to modeling time series. We cannot blindly clean data or prepare features without thinking about what they mean in context. Similarly, the more we know about the domain of the time series, such as understanding the importance of the time of day for blood sugars in humans, the better equipped we are to clean data, pre‐ pare features, and examine our forecasts for important success or failure cases.

# More Resources

Vasileios Lampos et al.,“Advances in Nowcasting Influenza-Like Illness Rates Using Search Query Logs,” Scientific Reports 5, no. 12760 (2015), https://perma.cc/NQ6B-RUXF.

This 2015 Nature Communications paper is highly accessible and both a good introduction to flu nowcasting as well as a historical piece that shows how now‐ casting and forecasting were revolutionized by big data and real-time social media and internet data inputs. The authors work with both a traditional linear model and an innovative nonlinear model to compare different approaches and show the utility of nonlinear models in such a complex system as seasonal infec‐ tious diseases rates.

David Farrow, “Modeling the Past, Present, and Future of Influenza,” doctoral thesis, Computational Biology Department, School of Computer Science, Carnegie Mellon University, 2016, https://perma.cc/96CZ-5SX2.

This thesis details some of the theory and practice of “sensor fusion” for the pur‐ poses of integrating social media sources of various geographic and temporal granularity to produce one of the Delphi group’s principle influenza forecasts. It provides an extremely in-depth overview of predicting the flu, from virology to population dynamics to data science experiments on how data processing affects forecasts.

Rob J. Hyndman, “Dynamic Regression,” lecture notes, n.d., https://perma.cc/5TPY-PYZS.

These lecture notes offer practical examples of how to use dynamic regression to supplement traditional statistical forecasting models with a series of alternative models for seasonality when SARIMA is not expected to be a good fit—usually because the periodicity is too complex or the periods are too long relative to the amount of data or computing resources available.

# Financial Applications

Financial markets are the granddaddy of all time series data. If you pay for propriet‐ ary trading data on a high-tech exchange, you can receive terabyte-sized floods of data that can take days to process, even with high-performance computing and embarrassingly parallel processing.

High-frequency traders are among the newest and most infamous members of the finance community, and they trade on information and insights resulting from time series analysis at the microsecond level. On the other hand, traditional trading firms —looking at longer-term time series over hours, days, or even months—continue to succeed in the markets, showing that time series analysis for financial data can be conducted in a myriad of successful ways and at timescales spanning many orders of magnitude, from milliseconds to months.

![](images/17d38dc0932cef2e20b103b30104f2a4cd2f89880b1eac5a6849e19016910622.jpg)

Embarrassingly parallel describes data processing tasks where the results of processing one segment of data are in no way dependent on the values of another segment of data. In such cases it’s embar‐ rassingly easy to convert data analysis tasks to run in parallel rather than in sequence to take advantage of multicore or multimachine computing options.

Consider, for example, the task of computing the daily mean of minute-by-minute returns on a given stock. Each day can be treated separately and in parallel. In contrast, computing the expo‐ nentially weighted moving average of daily volatility is not embar‐ rassingly parallel because the value for a particular day depends on the values for the days before it. Sometimes tasks that are not embarrassingly parallel can still be executed partly in parallel, but it depends on the details.

Here, we will work through a classic example of time series analysis for fun and for profit: predicting tomorrow’s stock returns for the S&P 500.

# Obtaining and Exploring Financial Data

It can be exceedingly difficult to obtain financial data if you have a particular product or temporal resolution you are seeking. In such cases you usually need to buy data. But historical stock prices are widely available from a variety of services, including:

• Yahoo Finance. While Yahoo has discontinued servicing its historical data API,1 historical daily data is available for download.   
• Newer companies, such as AlphaVantage and Quandl offer a combination of his‐ torical and real-time price information for stock data.

We limit our analysis to freely available daily stock price data from Yahoo. We down‐ load data for the S&P 500 spanning dates from 1990 through 2019. In the following code, we see what columns are available in the downloaded data set and plot the daily closing price to start exploring our data (see Figure 14-1):

```txt
>>> df = pd.read_csv("sp500.csv")
>>> df.head()
>>> df.tail()
>>> df.index = df.Date
>>> df.Close.plot() 
```

We can see the values are notably different at the beginning and end of the date period covered by the CSV files. The change in values is even more apparent when we plot the full time series for the closing price (Figure 14-2) than it is when we look at samples from the data frame.

A look at Figure 14-2 shows that the time series is not stationary. We also see that there may be different “regimes.” For reasons clearly seen in the plot, financial ana‐ lysts are keen to develop models identifying regime shifts in stock prices. There appear to be different regimes even if we may not have an exact definition of where one ends and another begins.2

![](images/3f796eb7d536389ec26557f4365d334b3e081aebec0ede5662290822d7023b91.jpg)

![](images/1cfe8ccc925d76679c7a9c72b66d3b27e86fd59d1d9777055293fe942fe66cde.jpg)  
Figure 14-1. Raw data from the beginning and end of the CSV file. Notice that the val‐ ues change noticeably from 1990 to 2019—no surprise to anyone familiar with the his‐ tory of the US financial markets.

![](images/5a8ec3aa50d55216e60d74d2a0c92e6aedb8447ff68aeede4cdcb5301130feaa.jpg)  
Figure 14-2. The daily closing price of the S&P 500 is not a stationary time series.

The potential for change points and different regimes suggests that it could be a good idea to break the data set into different sub-data sets to be modeled separately. How‐ ever, we want to keep all the data together because daily data does not produce many data points in a few decades; we need to keep all the data we can. We consider

whether we can justify keeping all this data together if we are only interested in dayahead forecasts.

We can consider whether normalizing data can make data from different time peri‐ ods comparable. Let’s take a look at a week’s worth of scaled closing prices for three different decades within the time series (Figure 14-3):

```python
>>> # pick three weeks (Mon - Fri) from different years
>>> # scale closing prices for each day by the week's mean closing price
>>> # 1990
>>> vals = df['1990-05-07':'1990-05-11'].Close.values
>>> mean_val = np.meanvals)
>>> plt.plot([1,2,3,4,5], vals/mean_val)
>>> pltxticks([1,2,3,4,5],
                    labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']) 
```

We plot closing prices over three weeks in three different decades, each scaled by that week’s mean as per the preceding code. The relative percentage changes from day to day in the course of a week seem about the same across the decades.

These plots are promising. While the mean values and variance of the closing prices have changed substantially over time, the plots suggest similar behavior over time once we normalize the data to its mean value for a given decade.

Given this, we next consider whether we can find a way to make all data over the full time period similar enough to train meaningfully with a model. We want to know whether there is a way to transform the data that is financially meaningful but also makes the data comparable across the entire time period.

![](images/548fa59bea6ff4248f6af07687c0b73bf63480a7610b182da3db0434bd9a8ffd.jpg)

![](images/df58be729814a0493583797918bf45b6abb9b0cae1511285781bb9d3cdcbac9a.jpg)

![](images/a4b40b42994c066b65e1acba4fa9d60f61a48381e7b5dca4508aef7a08b0ce69.jpg)  
Figure 14-3. Scaled mean closing prices for a week in May 1990, 2000, and 2018.

We compute the daily return, that is, the change in price from the start to the end of each trading day (see Figure 14-4):

```txt
## python
>>> df['Return'] = df.Close - df.Open
>>> df.Return.plot() 
```

As we can see in Figure 14-4, this alone is not enough to make the data comparable. We will also have to find a way to normalize the data without a lookahead so that the values we are using for inputs and outputs into our model are more even throughout the time period of interest. We will see how to do this in the next section.

![](images/e92949877f3ec664e98255a6b889a6d79f58cd0773fa67f022ba99c0fe08db0e.jpg)  
Figure 14-4. Daily returns show a near-zero mean over time, but the variance of daily returns changes markedly in different time periods. It is behavior like this that has inspired models such as GARCH, which was briefly discussed in Chapter 6.

# Financial Markets as a Random Walk

Random walk is a good starting model for financial time series; indeed, stock prices are often cited as the quintessential example of a naturally occurring random walk. There are two important insights we should keep in mind from this as we analyze our data.

First, many regime shifts may just be an effect of the natural contours of a random walk. If you don’t believe me, consider some of the following examples, which were quickly generated in R (see Figure 14-5):

```r
## R
> ## used each of these seeds in turn to produce the plots
> ## set.seed(1)
> ## set.seed(100)
> ## set.seed(30)
> N <- 10000
> x <- cumsum(sample(c(-1, 1), N, TRUE))
> plot(x) 
```

![](images/17be76de994a30ea2c27c9c14c69474a065c54c4b1d523a61a43b8279ba2fb92.jpg)

![](images/47497959137e6ef64056e2def7fadb7119555c615e132d717232b64112002d00.jpg)

![](images/7ffc5f530bbf1ca3312feadcfe0b321bd36dabf380fb604a4a40452e217a346f.jpg)  
Figure 14-5. The output from a variety of random walks, all produced with different seeds from the preceding simple code.

Second, because random walks are a natural fit for stock data, our null or most basic model, against which we can begin to benchmark more complicated models, would be one in which we predict the price for tomorrow as being the price for today, con‐ sistent with our understanding of the stock market as a random walk. In such a case, we would expect a very high correlation:

```txt
R  
>cor(x, shift(x), use = "complete obs")  
[1] 0.9996601 
```

On the other hand, if we differenced the time series, such that the value at each time step represented the change in the value of the time series from one time step to another, the correlation would disappear because any correlation would have been due to trend rather than actual predictive capability of such a simple model:

```txt
## R
> cor(diff(x), shift(diff(x)), use = "complete Obs")
[1] -0.005288012 
```

So in finance, whether you produce a model with high or low correlation could have a lot to do with the kind of data you’re modeling. If you model returns (what you should do) rather than stock prices (a common mistake even in industry), you’ll have correlations that seem low but are much more likely to be predictive in the real world rather than models that promise the world but don’t deliver in production. This is why we work to model returns rather than stock price in the next example.

# Preprocessing Financial Data for Deep Learning

Our data preprocessing will be done in three steps:

1. We will form new, economically meaningful quantities of interest out of the raw inputs.   
2. We will compute an exponentially weighted moving average and variance of the quantities of interest so that we can scale them without a lookahead.   
3. We will package our results in a format appropriate for the recurrent deep learn‐ ing model we will use to fit the data.

![](images/1ca06589a66aa4164e98595fb594c5d4f747bf4097870106c7c4dbd67008f6b7.jpg)

# Financial Time Series Is Its Own Discipline

Financial time series is an entire discipline with thousands of aca‐ demics diligently trying to understand how the financial markets work, both for profit and for smart regulation. There are numerous statistical models that have been developed to treat some of the tricky aspects of financial data that we have touched on here, such as the GARCH model. If you are seeking to apply statistics and machine learning to financial time series modeling, you should study the history of quantitative finance and the major classes of commonly deployed models.

# Adding Quantities of Interest to Our Raw Values

We’ve already computed daily return in the previous section. Another quantity of interest we can form from the raw inputs is daily volatility, which is the difference between the highest and lowest prices recorded during the trading day. This can be easily computed given the raw data (see Figure 14-6):

```markdown
## python
>>> df['DailyVolatility'] = df.High - df.Low
>>> df.DailyVolatility.plot() 
```

Just as the daily return prices are a nonstationary time series, so too is the daily vola‐ tility time series. This further confirms that we need to find a way to scale these appropriately. We also want to do so without introducing a lookahead.

![](images/aeba572b7104c97dc147747347eb2aa16bf856ed35b3b00d5b5d4a2a4804e12b.jpg)  
Figure 14-6. Daily volatility is a positive value time series (by definition) that shows noticeably different degrees of variance at different points in the S&P 500 time series.

# Scaling Quantities of Interest Without a Lookahead

We will forecast the daily return one day ahead. Some quantities of interest that could be helpful include:

• Previous daily returns   
• Previous daily volatility   
• Previous daily volume

We will scale each of these quantities by subtracting out the exponentially weighted moving average and then dividing by the exponentially weighted standard deviation. Our earlier weekly data exploration showed that with appropriate preprocessing the various quantities of interest can be made into stationary time series.

First we compute the exponentially weighted moving average of every column in the data frame, and plot the daily volatility’s exponentially weighted moving average (see Figure 14-8). Contrast this with the plot in Figure 14-7. This plot is much smoother due to the averaging. Note that there is a parameter here that you should effectively consider part of your model, as a hyperparameter, even though it’s used in the data preprocessing step: the half-life of the exponential smoothing. Your model’s behavior will certainly depend on this parameter:

```txt
## python
>>> ewdf = df ewm(halflife = 10).mean()
>>> ewdf DailyVolatility.plot() 
```

![](images/6741d0fbbd018df35492e00f0dad83151e2134b005b005c0f06633dcb21d2f7e.jpg)  
Figure 14-7. The plot of the exponentially weighted moving average of daily volatility is much smoother than the plot of the raw values but still shows a nonstationary time series.

We can then use that value, as well as the exponentially weighted moving variance calculated here, to scale values of interest in a way that results in series with more consistent behavior over time (see Figure 14-8):

```python
>>> # compute exponentially weighted moving variance
>>> vewdf = df ewm(halflife = 10).var()
>>> 
>>> # scale by demeaning and normalizing
>>> scaled = df.DailyVolatility - ewdf.DailyVolatility
>>> scaled = scaled / vewdf.DailyVolatility**0.5
>>> scaled.plot() 
```

![](images/1250d4a9c273682a836d09612cfb91484cd0e922d02f8ffbea2ccb6c03f8db09.jpg)  
Figure 14-8. Transforming the data with exponentially weighted mean and variance results in a much more even time series with comparable values throughout the time period all the way from 1990 to 2019.

We transform all three raw inputs of interest into a scaled version as follows:

```python
>>> df['ScaledVolatility'] = ((df.DailyVolatility - ewdf.DailyVolatility)
>>> / vewdf.DailyVolatility**0.5)
>>> df['ScaledReturn'] = ((df.Return - ewdf.Return)
>>> / vewdf.Return**0.5)
>>> df['ScaledVolume'] = ((df.Volume - ewdf.Volume)
>>> / vewdf.Volume**0.5) 
```

Finally, we drop the NA results that come from exponential smoothing:3

```txt
>>> python
>>> df = df.dropna() 
```

# You Don’t Have to Use Daily Data

Hopefully you’ve noticed that all the data processing has taken place at the per-day timescale. You may remember from Chapters 2 and 3 that upsampling and down‐ sampling of data is a way to change the timescale of your analysis. Likewise, you may want to use inputs from a variety of timescales in your model. For example, I could consider computing exponentially weighted moving averages and variances at a vari‐ ety of half-lives and using each of them as a different input. Likewise, I could com‐ pute quantities of interest at different timescales, such as weekly returns or monthly volatility. All of this is something to explore when you are modeling a time series, although space is limited here.

It’s wise to stick to a timescale in your inputs similar to the timescale you have in mind for your targets, namely how far into the future you want to predict. The fur‐ ther into the future you want to predict, the further back in time you should be look‐ ing and the more you should smooth your series with a longer half-life so that you can recognize longer-term trends. This will help you make longer-term predictions.

# Formatting Our Data for a Neural Network

Our data is stored in a Pandas data frame at the moment, and our planned inputs are stored alongside many raw inputs we have no intention of using. Also, for a neural network we will shape our data into the TNC format, which, you might recall, stands for time $\times$ number of samples $\times$ channels. For this reason we need to do some more preprocessing even after the rescaling work we have already discussed.

First we portion out our data into training and testing components:4

```python
```python
>>> # break our data into training and testing components
>>> train_df = df[:7000]
>>> test_df = df[7000:]
>>> 
>>> # build our pipeline variables off training data
>>> # taking only values of interest from larger data frames
>>> horizon = 10
>>> X = train_df[(7000 - horizon)][["ScaledVolatility", "ScaledReturn",
"ScaledVolume)].values
>>> Y = train_df[horizon]:["ScaledReturn"].values 
```

Notice there is something of a problem with what we are predicting based on how we set up Y here. Think about this for a minute before you continue reading.

# Diluting the Forecasting Task

What we have done with our data has diluted the forecasting task. That is, we take a future value and “dilute” it with information from the past. While this is a way of pushing the past into the future, seemingly the opposite of lookahead, it’s a related problem. Information is moving along the time axis in a way we might forget about, which in turn may make us misunderstand our model’s performance. We should keep track of what we’ve done in preprocessing to bear in mind when evaluating our model.

The problem of forecasting smoothed values is rampant in the economics and finance literature—and it’s a problem that often goes unacknowledged. While it’s not wrong to do this, sometimes authors overstate their results and say they have beat a bench‐ mark when in fact they set themselves an easier task of predicting a smoothed value rather than the measured value.

The problem with this setup is that the Y value is the scaled return rather than just the return. This is better for training because the values fall within the appropriate range, but it also means that the Y we are predicting is not the actual return that interests us but rather that return adjusted by a moving average. We are forecasting how much our return differs from the exponentially weighted moving average rather than just predicting the return.

This is not wrong per se, but it does mean that we are making our task easier than the true forecasting task. We should be aware of this so that when our training looks bet‐

ter than our model’s actual performance, we’ll know it’s partly because in training we focus on a hybrid task, whereas ultimately making money off this model would depend only on the true forecasting.

We focus on the training data and now need to put X into the format expected by a recurrent neural network architecture, namely TNC. We do this with a series of NumPy operations.

Originally, X is two-dimensional, as it comes from a Pandas data frame. We want to add a third dimension, axis 1 (so pushing the second dimension out to axis 2, where the axes are numbered from 0 up):

```markdown
## python
>>> X = npexpand_dims(X, axis = 1) 
```

Our temporal axis is already axis 0 because the data frame was sorted temporally. The last axis, now axis 2, is already the “channel axis” because our inputs each occupy one column of that dimension.

We will try a model that will see 10 time steps—that is 10 days of data looking back‐ ward. So we need to cut off axis 0 to be of length 10. We chop along axis 0 every 10 rows, and we reform the resulting list of submatrices such that the number of samples (i.e., length of the resulting list) becomes the dimension of the second axis:5

```erlang
>>> x = np.split(X, X.shape[0]/10, axis = 0)
>>> x = np concatenate(x, axis = 1)
>>> x.shape
(10, 699, 3) 
```

Given the TNC format, we have time series of length 10, with three parallel inputs. Of these we have 699 examples. The batch size will determine how many batches make up an epoch, where an epoch is one cycle through our data.

We don’t have very much data to train on, given how few examples we appear to have. How did we go from 30 years of data to not much data? The answer is that each data point has, as of now, only been included in one sample time series. However, each data point could be in 10 different time series, occupying a different position in each one.

This may not be immediately obvious, so let’s look at a simple example. Assume we have the following time series:

1, 3, 5, 11, 3, 2, 22, 11, 5, 7, 9

We want to train a neural network with this time series, this time assuming a time window of length 3. If we use the data preparation we just performed, our time series examples would be:

• 1, 3, 5   
• 11, 3, 2   
• 22, 11, 5   
• 7, 9, _

However, there is no reason to privilege the start of our data as somehow having to set the beginnings and ends of each sample time series. The windows are arbitrary. Some equally valid time series, sliced out of the whole in a window, are:

• 3, 5, 11   
• 2, 22, 11   
• 5, 7, 9

So if we needed more data, we would be well served to generate time series samples as we slid a window over the entire data set. That would produce more individual time series samples than we have done with our method of chopping the data into nonoverlapping time series samples. Keep this in mind when preparing your own data sets. Below, you will see we use this sliding window method to preprocess our data.

# Building and Training an RNN

As mentioned in the introduction to this chapter, financial time series are notoriously difficult to model and understand. Even as the finance industry continues to be a mainstay of the Western economy, experts agree that predictions are very difficult to make. For this reason, we look for a technique that is well suited for a complicated system with potentially nonlinear dynamics, namely a deep learning neural network. However, due to our lack of data, we choose a simple recurrent neural network (LSTM) architecture and training regime described by these parameters:

```markdown
## python
>>> # architecture parameters
>>> NUM_HIDDEN = 4
>>> NUM_LAYERS = 2
>>> 
>>> # data formatting parameters
>>> BATCH_SIZE = 64
>>> WINDOW_SIZE = 20
>>> 
>>> # training parameters 
```

>>> LEARNING_RATE = 1e-2   
>>> EPOCHS = 30

In contrast to Chapter 10, we use the TensorFlow package rather than MXNet so you can see another example of a widely used deep learning framework. In TensorFlow, we define variables for all the quantities we will use in our network, even with chang‐ ing values that represent inputs. For the inputs we use placeholders, which are a way of letting the graph know what shape to expect:

```txt
```python
>>> Xinp = tfagna.dtype = tf.float32,
>>> shape = [WINDOW_SIZE, None, 3])
>>> Yinp = tfagna.dtype = tf.float32, shape = [None]) 
```

Then we build our network and implement loss calculation and optimization steps:

```python
>>> with tf variable_scope("scope1", reuse=tf.AUTO_REUSE):
>>> cells = [tf.nn.rnn_cell.LSTMCell(num.units=NUM HIDDEN)
>>> for n in range(NUMLAYERS)]
>>> stacked_rnn_cell = tf.nn.rnn_cell.MultiRTNNCell(cells)
>>> rnn_output, states = tf.nn.dynamic_rnn(stacked_rnn_cell,
>>> Xinp,
>>> dtype=tf.float32)
>>> W = tf.get_variable("W_fc", [NUM HIDDEN, 1],
>>> initializer =
>>> tf.random.uniform_initializer(-.2, .2))
>>> #
>>> output = tfsqueeze(tf/matmul(rnn_output[-1, :, :], W))
>>> loss = tf.nn.l2_loss(output - Yinp)
>>> opt = tf.train.GradientDescentOptimizer(LEARNING_RATE)
>>> train_step = opt.minimize(loss) 
```

We have a fairly complicated way of feeding in the data due to what we discussed earlier, namely that each data point should be in multiple time series depending on which offset we use. Here we treat the same data formatting problem we discussed at greater length in Chapter 10:

```txt
```python
>>> ## for each epoch
>>> y_hat_dict = {}
>>> Y_dict = {}
>>> in_sample_Y_dict = {}
>>> in_sample_y_hat_dict = {}
>>> for ep in range(EPOCHS):
>>> epoch_training_loss = 0.0
>>> for i in range(WINDOW_SIZE):
>>> X = train_df[(7000 - WINDOW_SIZE)][["ScaledVolatility"], 
```

```txt
"ScaledReturn", "ScaledVolume"].values  
Y = train_df[WINDOW_SIZE]:["ScaledReturn"].values  
## make it divisible by window size  
num_to_unpack = math.floor(X.shape[0] / WINDOW_SIZE)  
startidx = X.shape[0] - num_to_unpack * WINDOW_SIZE  
X = X[startidx:]  
Y = Y[startidx:]  
X = X[i:-(WINDOW_SIZE-i)]  
Y = Y[i:-(WINDOW_SIZE-i)]  
X = npexpand_dims(X, axis = 1)  
X = np.split(X, X.shape[0]/WINDOW_SIZE, axis = 0)  
X = np concatenate(X, axis = 1)  
Y = Y[:window_SIZE]  
## TRAINING  
## now batch it and run a sess  
for j in range(math.ceil(Y.shape[0] / BATCH_SIZE)):  
    ll = BATCH_SIZE * j  
    ul = BATCH_SIZE * (j + 1)  
if ul > X.shape[1]:  
    ul = X.shape[1] - 1  
    ll = X.shape[1] - BATCH_SIZE  
training_loss, _, y_hat = sess.run([[loss, train_step, output], feed_dict = { Xinp: X[:, ll:ul, :, Yinp: Y[ll:ul]})  
epoch_training_loss += training_loss  
in_sample_Y_dict[ep] = Y[ll:ul]  
## notice this will only net us the last part of  
## data trained on  
in_sample_y_hat_dict[ep] = y_hat  
## TESTING  
X = test_df[(test_df.shape[0] - WINDOW_SIZE)]  
[["ScaledVolatility", "ScaledReturn", "ScaledVolume"].values  
Y = test_df[WINDOW_SIZE]:["ScaledReturn"].values  
num_to_unpack = math.floor(X.shape[0] / WINDOW_SIZE)  
startidx = X.shape[0] - num_to_unpack * WINDOW_SIZE  
## better to throw away beginning than end of training  
## period when must delete  
X = X[startidx:]  
Y = Y[startidx:] 
```

```python
>>> X = npexpand_dims(X, axis = 1)
>>> X = np.split(X, X.shape[0]/WINDOW_SIZE, axis = 0)
>>> X = np concatenate(X, axis = 1)
>>> Y = Y[:window_size]
>>> testing_loss, y_hat = sess.run([[loss, output],
>>> feed_dict = {Xinp: X, Yinp: Y})
>>> #nb this is not great. we should really have a validation
>>> ## loss apart from testing
>>> print("Epoch: %d Training loss: %0.2f
>>> Testing loss %0.2f:"%
>>> (ep, epoch_training_loss, testing_loss))
>>> Y_dict[ep] = Y
>>> y_hat_dict[ep] = y_hat 
```

Here we see our training and testing metrics:

```txt
Epoch: 0 Training loss: 2670.27 Testing loss 526.937:  
Epoch: 1 Training loss: 2669.72 Testing loss 526.908:  
Epoch: 2 Training loss: 2669.53 Testing loss 526.889:  
Epoch: 3 Training loss: 2669.42 Testing loss 526.874:  
Epoch: 4 Training loss: 2669.34 Testing loss 526.862:  
Epoch: 5 Training loss: 2669.27 Testing loss 526.853:  
Epoch: 6 Training loss: 2669.21 Testing loss 526.845:  
Epoch: 7 Training loss: 2669.15 Testing loss 526.839:  
Epoch: 8 Training loss: 2669.09 Testing loss 526.834:  
Epoch: 9 Training loss: 2669.03 Testing loss 526.829:  
Epoch: 10 Training loss: 2668.97 Testing loss 526.824  
Epoch: 11 Training loss: 2668.92 Testing loss 526.819  
Epoch: 12 Training loss: 2668.86 Testing loss 526.814  
Epoch: 13 Training loss: 2668.80 Testing loss 526.808  
Epoch: 14 Training loss: 2668.73 Testing loss 526.802  
Epoch: 15 Training loss: 2668.66 Testing loss 526.797  
Epoch: 16 Training loss: 2668.58 Testing loss 526.792  
Epoch: 17 Training loss: 2668.49 Testing loss 526.788  
Epoch: 18 Training loss: 2668.39 Testing loss 526.786  
Epoch: 19 Training loss: 2668.28 Testing loss 526.784  
Epoch: 20 Training loss: 2668.17 Testing loss 526.783  
Epoch: 21 Training loss: 2668.04 Testing loss 526.781  
Epoch: 22 Training loss: 2667.91 Testing loss 526.778  
Epoch: 23 Training loss: 2667.77 Testing loss 526.773  
Epoch: 24 Training loss: 2667.62 Testing loss 526.768  
Epoch: 25 Training loss: 2667.47 Testing loss 526.762  
Epoch: 26 Training loss: 2667.31 Testing loss 526.755  
Epoch: 27 Training loss: 2667.15 Testing loss 526.748  
Epoch: 28 Training loss: 2666.98 Testing loss 526.741  
Epoch: 29 Training loss: 2666.80 Testing loss 526.734 
```

The error metric we chose does not give us an idea of how well our overall data matches the results, so plotting is helpful. We plot both out-of-sample performance (important) and in-sample performance (less important), as shown in Figure 14-9:

```txt
>>> plt.plot(test_y_dict[MAX_EPOCH])
>>> plt.plot(test_y_hat_dict[MAX_EPOCH], ['r--'])  
>>> plt.show() 
```

![](images/ab4128bf2023e927c9cea44df1437287995552657dce733a5b21203be2b98e1a.jpg)  
Figure 14-9. The actual return values for a subsection of the test period (solid line) are plotted against the forecasts from the neural network (dashed line). The scale of the forecast is so different from the actual data that it’s difficult to assess the model.

We can see that our predicted values for the returns don’t tend to be at the same value as the actual returns. We check the Pearson correlation next:

```markdown
## python
>>> Pearson(test_y_dict[MAX_EPOCH], test_y_hat_dict[MAX_EPOCH])
(0.03595786881773419, 0.20105107068949668) 
```

These numbers might look grim if you haven’t already worked with financial time series. In this industry, our model may be useful despite the plot and despite the $\boldsymbol { p }$ - value. In finance, a correlation that is positive is exciting and something that can be improved upon incrementally. In fact, many research projects, when starting out, can’t necessarily get to such “high” correlations.

We can get a better idea of whether the predictions at least go in the same direction by scaling the predicted returns up an order of magnitude and plotting again (Figure 14-10):

```txt
>>> plt.plot(test_y_dict[MAX_EPOCH][:100])
>>> plt.plot(test_y_hat_dict[MAX_EPOCH][:100] * 50, 'r--')
>>> plt.show() 
```

![](images/6ce45d3199609422cbd4177145a20510ceffc608b419cf8312079d92b6eac511.jpg)  
Figure 14-10. A better sense of how the model’s predictions (dashed line) compare to the actual data (solid line). However, with such a low correlation we are more likely to think we see a pattern than to actually see one. For this reason, quantitative metrics will serve us better than visual assessments for noisy financial data.

If you have read blogs about using deep learning for financial time series, you may very well find the performance here disappointing. For example, you have likely seen blog posts where someone applies a simple multilayer LSTM to some daily stock data and produces predictions that look almost identical to the actual stock market data, even out of sample. There are two important reasons those results may look good but are actually not impressive:

• Preprocessing the code to scale it with an out-of-the-box scaling solution such as sklearn.preprocessing.MinMaxScaler. This is not ideal because it includes a lookahead by using the values across all time to scale data.   
• Predicting price rather than returns. This is a much easier task—for starters, an excellent prediction of price on day $T + 1$ is price on day T. Thus, it’s easy to build a model that appears to predict price reasonably well and makes impressive graphs. Unfortunately, such models can’t be used to make money.

We have attempted a more realistic industry example, which means the challenge is greater and the plots won’t be as satisfying.

# Performance Considerations

We have used a standard LSTM tf.nn.rnn_cell.LSTMCell here for a number of rea‐ sons, not the least because it’s a general workhorse. We also used it because this book uses code that does not require the use of a GPU so that you can run these scripts even on a standard laptop.

If you do have a GPU, you should use the API for NVIDIA’s cuDNN, which is a hardware-tailored implementation available for the two main elaborations on a stan‐ dard RNN: the LSTM and the GRU. Assuming your GPU is from NVIDIA, which most are, this will be substantially faster for training than the standard implementa‐ tions. This is available in TensorFlow and also in the other mainstream deep learning packages, and the usage is quite similar to what we have coded here, apart from employing a different RNN cell.

This advice may change as Google’s TPUs are rolled out to the mainstream and as other hardware solutions emerge for deep learning.

Needless to say, we haven’t completed a full analysis of the model’s performance. Doing so would give us insight into how to build our next model, what we might have overlooked, and whether a deep learning model can justify its additional com‐ plexity relative to a linear model. There are a lot of paths to take from here to improve model performance.

There are many ways we can improve this model, which you should consider as extensions of this code:

• Add more inputs from the raw data by generating additional features based on these inputs. We did not use all the raw input columns, and there are other ways to re-express these quantities that could be useful. You might consider categori‐ cal variables, such as “Did the high or the low for the day coincide with the open‐ ing or closing?” (there are several binary conditions packed into that one question).   
• Integrate parallel time series for other stocks. This will add further information and data to train on.   
• Use data at several different timescales. One widely cited paper that does this dis‐ cusses an architecture called ClockworkRNN.   
• Augment your data by taking existing time series examples and adding jitter. This will help with the fact that this data set does not offer much data.   
• Allow your network architecture to grow if you have scaled up the number of inputs or the amount of data. A more complicated architecture is not always a

way to improve performance, but it can be appropriate if you see that your net‐ work is topping out.

• Try training the data in chronological order, rather than our approach of cycling through the data several times per epoch. Sometimes this be quite helpful (but it depends on the data set). Given that we saw the behavior of time series change over time, it might be better to end training with the last data last so that the changed behavior is reflected in the weights.   
• Consider different loss functions. Here we used an L2 norm, which tends to pun‐ ish larger differences much more than small differences. Given the domain, how‐ ever, we might want to evaluate success differently. Perhaps we just want to get the predicted sign of the daily return right and not worry so much about its mag‐ nitude. In this case we could consider setting the targets as categorical variables: positive, negative, zero. In the case of categorical data, we usually want to use a cross-entropy measure of loss. However, given that this is not purely categorical data, since it’s ranked (that is, zero is closer to negative than positive is to nega‐ tive), we might want to use a custom loss function to reflect this.   
• Consider building an ensemble of simple neural networks rather than a single one. Keep each individual network small. Ensembles are particularly useful for low signal-to-noise data, such as financial data.   
• Determine why the scale of the forecasts is so different from the scale of the actual values. Consider starting by assessing whether the loss function we used is problematic given the strong preponderance of zero value daily returns.

As you can see, there are myriad ways to improve a network’s performance or tune its functionality. How you do so will depend on the data set. It is always helpful to use visualizations of your network’s performance, domain knowledge, and a firmly defined goal (which in this case is likely “make money”) to drive how you refine your model. Otherwise, you can easily get lost in the overwhelming number of choices you have.

# More Resources

Joumana Ghosn and Yoshua Bengio, “Multi-Task Learning for Stock Selection,” Cam‐ bridge: MIT Press, 1996, https://perma.cc/GR7A-5PQ5.

This 1997 paper provides a very early example of applying a neural network to the problem of financial markets. In this case, the author used what would now be considered a quite simple network with minimal data, but they nonetheless found their network could learn to pick stocks profitably. Interestingly, this is also an early example of multitask learning.

Lawrence Takeuchi and Yu-Ying Lee, “Applying Deep Learning to Enhance Momen‐ tum Trading Strategies in Stocks,” 2013, https://perma.cc/GJZ5-4V6Z.

In this paper, the authors interpret their neural network through the lens of “momentum training”, a traditional way of quantitatively forecasting the finan‐ cial markets in a pre-machine learning world. This paper is interesting for its dis‐ cussion of how training decisions were made and model performance evaluated.

“Is anyone making money by using deep learning in trading?” Quora, https://perma.cc/ Z8C9-V8FX.

In this question and answer, we see a variety of opinions regarding the extent to which deep learning has been successful in financial applications. As described in some of the answers, any profitable IP would likely be heavily protected by nondisclosure agreements and the profit incentive so that in this industry it can be very difficult to assess what state-of-the-art performance is. The answers also point to a wide rang of potential financial applications - predicting returns is just one problem among many.

# Time Series for Government

Time series analysis is quite relevant and important for governmental applications for a number of reasons. First, governments both large and small, are the keepers of some of the most important time series data in the world, including the US jobs report, ocean temperature data (that is, global warming data), and local crime statis‐ tics. Second, governments by definition provide some of the most essential services we all rely on, and thus they need to be reasonably adept forecasters of demand if they don’t want to grossly overspend on, or understaff, those services. Thus, all aspects of time series are relevant to government purposes: storage, cleaning, explora‐ tion, and forecasting.

As I mentioned back in Chapter 2 when discussing “found” time series, a very high percentage of all government data can look a lot like time series data with some restructuring. Generally, most government data sets are the result of ongoing data collection rather than a single slice of time. However, government data sets can be daunting for a number of reasons:

• Inconsistent recordkeeping (due to organizational constraints or political forces changing over time)   
• Opaque or confusing data practices   
• Enormous data sets with relatively low information content

Nonetheless, it can be quite interesting to look at government data sets both for intel‐ lectual interest and for many practical purposes. In this chapter we explore a govern‐ mental data set that consists of all the complaints made in New York City from 2010 to the present to a city-run hotline that can be reached by dialing 311. Because the data set is continuously updated, the data seen in the book will likely differ from what you see when you download it; you will have even more information than I did when

preparing this chapter. Nonetheless, the results should be fairly similar. In this chap‐ ter we will go through a few topics:

• Interesting sources of government data, including the one we will analyze   
• Dealing with extremely large files of plain-text data   
• Online/rolling statistical analysis of large data sets and other options for analyz‐ ing data without keeping it all in memory

# Obtaining Governmental Data

Governmental data sets in the “found data” category can be a nightmare from a data consistency standpoint. These data sets, although they have a timestamp, are usually released for an open data initiative rather than for a specific time series purpose. Often there is little to no information available about the timestamping conventions of the data or other recording conventions. It can be difficult to confirm that the underlying recording practices were consistent.1

Nonetheless, if you are adventurous or keen to be the first to identify interesting tem‐ poral features in human behaviors related to governmental activities, you are lucky to live in the age of open government and open data. Many governments at all levels have made more efforts in recent years to make their time series data transparent to the public. Here are just a few examples of where you can obtain open governmental data with a time series component:

• Monthly hospital data from the United Kingdom’s National Health Services. This data set is surprisingly time series–aware: it includes having a tab named “MAR timeseries” and describes recording conventions and how they have evolved over time.   
• Jamaica’s open data portal also includes an appreciation and recognition of time series data, such as its timestamped data set of Chikungunya cases for 2014 and the associated data report, which includes an animation (i.e., a time series visuali‐ zation) and an epidemic curve.   
• Singapore’s open data portal features extensive data sets and advertises the time series nature of some of that data by including two time series plots on its main page, as shown in Figure 15-1.

![](images/cdd845fb024f5105cd126fb31e4381d3e5b2d1729e19a076a3ee983c0d09c739.jpg)  
Singaporeata glance

![](images/07ddf029f638296843d861ba906aa9369543795550a27c688ca193763be701a2.jpg)  
Figure 15-1. Two of the four graphs on the main page of Singapore’s open data website (as accessed in spring 2019) are time series visualizations used to show important infor‐ mation about the country.

As you might notice, all my examples are from English-speaking areas, but of course they do not have a monopoly on the open data movement in government. For exam‐ ple, the city of Paris, the nation of Serbia, and the African Development Bank Group all run open data websites.2

In this chapter’s examples we are pulling our information from the New York City Open Data portal, selected because NYC is a large and interesting place that happens to be my home. We dive into their 311 hotline data set in the next section.

# Exploring Big Time Series Data

When data is sufficiently large, you will be unable to fit it in memory. How large data needs to be before you reach this limit will depend on the hardware you are using.3 You will eventually need to understand how to iterate through your data, one man‐ ageable chunk at a time. For those familiar with deep learning, you have likely already done this, particularly if you work with image processing. In deep learning frame‐ works there are often Python iterators that wend their way through a data set with that data set stored in specified directories, each with many files.4

When I downloaded the 311 data set, it was over 3 gigabytes in CSV format. There was no way I would be able to open this on my computer, so my first idea was to use standard Unix operating system options, such as head.

Unfortunately, what printed out was already so large as to be unmanageable in a Unix command-line interface, at least for someone inexpert in Unix tools:

## linux os command line $ head 311.csv

Unique Key,Created Date,Closed Date,Agency,Agency Name,Complaint Type,Descripto 27863591,04/17/2014 12:00:00 AM,04/28/2014 12:00:00 AM,DOHMH,Department of Heal 27863592,04/17/2014 12:00:00 AM,04/22/2014 12:00:00 AM,DOHMH,Department of Heal 27863595,04/17/2014 10:23:00 AM,0417/2014 12:00:00 PM,DSNY,Queens East 12,Derel 27863602,04/17/2014 05:01:00 PM,04/17/2014 05:01:00 PM,DSNY,BCC - Queens East,D 27863603,04/17/2014 12:00:00 AM,04/23/2014 12:00:00 AM,HPD,Department of Housin 27863604,04/17/2014 12:00:00 AM,04/22/2014 12:00:00 AM,HPD,Department of Housin 27863605,04/17/2014 12:00:00 AM,04/21/2014 12:00:00 AM,HPD,Department of Housin

While the content is unwieldy, this view was sufficient to show that there were several timestamps as well as other interesting and ordered information, such as geographic coordinates. Clearly the data is very wide, so we need to be able to manipulate this information to get the columns we want.5

Even if you are new to Linux, you can easily learn about simple command line tools that can provide helpful information. We can get a line count of the CSV file so we have an idea of the scale we are looking at, namely how many data points we have. This is a one-liner:

```txt
## linux os command line
$ wc -l 311.csv
19811967 311.csv 
```

We can see that NYC has fielded about 20 million complaints to 311 since 2010. That’s more than two complaints per resident.

Armed with this knowledge, we use R’s data.table knowing that its fread() func‐ tion enables partial reading of files (seen in the documentation when you read about the nrows and skip parameters6) and that data.table is extremely performant when handling large data sets. We can use this to get initial information, as in the following code:

```txt
## R
> df = fread("311.csv", skip = 0, nrows = 10)
> colnames(df)
[1] "Unique Key"
[3] "Closed Date"
[5] "Agency Name"
[7] "Descriptor"
[9] "Incident Zip"
[11] "Street Name"
[13] "Cross Street 2"
[15] "Intersection Street 2"
[17] "City"
[19] "Facility Type"
[21] "Due Date"
[23] "Resolution Action Updated Date"
[25] "BBL"
[27] "X Coordinate (State Plane)"
[29] "Open Data Channel Type"
[31] "Park Borough"
[33] "Taxi Company Borough"
[35] "Bridge Highway Name"
[37] "Road Ramp"
[39] "Latitude"
[41] "Location" 
```

Just from reading the first 10 lines we can already see the column names. For all the strengths I listed for NoSQL approaches to time series data, it can be nice in a large data set to know the column names from the outset. Of course there are workarounds for this with NoSQL data, but most require some effort on the part of the user rather than happening automatically.

Several columns suggest useful information:

```txt
"Created Date"  
"Closed Date" 
```

```txt
"Due Date" "Resolution Action Updated Date" 
```

These will likely be of character type before we convert, but once we do a conversion to a POSIXct type, we can see what the time spans between these dates look like:

```txt
## R
> df\$CreatedDate = df[, CreatedDate := as.POSIXct(CreatedDate, format = "%m/%d/%Y %I:%M:%S %p") 
```

In the formatting string, we need to use a $\% \mathbb { I }$ for the hour since it’s expressed only in 01–12 format, and a $\% 0$ because the timestamps include an AM/PM designation.

To get a sense of how these dates tend to be spaced, particularly with respect to when a complaint is created versus closed out, let’s load in more rows and examine the dis‐ tribution of what I’ll call the complaint lifetime (i.e., the span between creation and closing):

```txt
## R
> summary(as.numeric(df\$ClosedDate - df\$CreatedDate, units = "days"))
Min. 1st Qu. Median Mean 3rd Qu. Max. NA's
-75.958 1.000 4.631 13.128 12.994 469.737 113 
```

As we can see, it’s a wide distribution. More surprising than the yearlong wait times for some complaints to be closed out is the fact that some complaints have negative, even extremely negative, times between their creation and closing date. If the nega‐ tive time were around –365 days (a year), we might imagine a data entry problem, but this seems less likely with numbers such as –75 days. This is a problem we’ll need to look into.

We can spot another problem by taking a range of the creation date:

```markdown
## R
> range(df\$CreatedDate)
[1] "2014-03-13 12:56:24 EDT" "2019-02-06 23:35:00 EST" 
```

Given the size of this CSV file and the fact that it is supposed to be continuously updated, it is surprising that the first lines are not from the 2010 date that is supposed to mark the earliest data sets. We would have expected the CSV to be continuously appended to. More surprising still is the fact that 2019 dates are in the first rows and that 2014 and 2019 dates are both in the same first 10,000 lines. This suggests that we cannot easily determine the date ordering of the data in the file. We can visualize the date distribution from one row to the next by performing a line plot of row index versus date, as we do in Figure 15-2.

![](images/c024fc943326f0eb202a86a88ac4c03b2be087275fbb8afa6ba736cd0c62baed.jpg)  
Figure 15-2. While most of the dates in the first 10,000 lines of code appear to fall in 2014, they also jump around as far forward as 2019—and often!

There is no way to avoid the problem of acting. If we want to understand how behav‐ ior changes over time, we will have to confront the unordered data. But we have a few options.

# Upsample and Aggregate the Data as We Iterate Through It

One option is to upsample the data as we iterate through it to construct a condensed time series with aggregate statistics. We could choose a resolution and aggregation counts from the start of our analysis and then calculate these as we iterate through. Then we could sort our results at the end of our analysis. This would look something like having, say, a dictionary/list of all dates from 2010 to the present and then adding to the appropriate date for each row. This method would produce a list/dictionary with a relatively small number of entries, and we could then sort this at the end based on date.

The upside of this is that it would be relatively straightforward to code and would be a way to combine data cleaning, exploration, and analysis into one exploratory step. The downside is that detailed data would still be lost in the morass of the unsorted file, so that if there was a specific time period of interest we would have to search the entire unsorted file to find all relevant entries.

Since we have already done examples of upsampling in Chapter 2, I leave this as an exercise for the reader.

# Sort the Data

Another option is to sort the data. This is a daunting task given the large size of the file and the relatively unordered dates from what we can observe. Even for the data within 2014 it does not seem as though the dates come in order. For this reason we have no indications that we can trust any slices of the data, so we should think of this as a pile of data in random order.

Sorting the full file would be extremely memory intensive, but there are two reasons it could be worthwhile. One is that we only need to sort once and then can save our results for whatever subsequent analysis we want to do. Second is that we can then have the full level of detail preserved so that if we spot specific time periods of inter‐ est in our analysis, we can examine the data in all its detail to understand what is happening.

Concretely, thinking about how we could accomplish this, we have a few options:

• Linux has a command-line tool for sorting.   
• Most databases can sort data, so we could transfer the data to a database and let the database take care of it.7   
• We could come up with our own sorting algorithm and implement it. We would need to formulate something that did not consume enormous amounts of mem‐ ory. Odds are strongly against our effort matching what is available in prepack‐ aged sorting options.

We choose to use the Linux command-line tool. While it may take some time to get this right, we will develop a new life skill as well as gaining access to a wellimplemented and correct sort for this large file.

We begin by first creating a small test file to use:

## linux command line

$\$ 5$ head -n 1000 311.csv | tail -n 999 > test.csv

Notice that this involves both the head (i.e., print out the start) and tail (i.e., print out the end) commands. head will include the first line of the file, which for this file provides the column names. If we include this and sort it along with the values, it will not preserve the column names as the top line in the file, so we cut it out before sorting.

If you are working on a Linux-based operating system, you can then apply the sort command as follows:

## linux command line

$ sort --field-separator $\because$ ,' --key=2,3 test.csv $>$ testsorted.csv

In this case we identify the field separator and then indicate we want to sort by the second and third columns—that is, by the creation date and close date (which we only know from previously inspecting the file). We output this into a new file, as it wouldn’t be very helpful to have it output to standard out.

Now we can inspect the sorted file in R, but unfortunately we will find that this also does not return a sorted file. Go back a few pages to where we ran the head command on the CSV file, and you will see why. We sorted according to a date column (which would be processed like a string, not in a date-aware manner). However, the current formatting of the dates begins with the month, so we will end with a column of dates by month rather than sorted by overall time, as we see when we review the “sorted” CSV resulting from our previous command:

1: 02/02/2019 10:00:27 AM 02/06/2019 07:44:37 AM   
2: 03/27/2014 07:38:15 AM 04/01/2014 12:00:00 AM   
3: 03/27/2014 11:07:31 AM 03/28/2014 01:09:00 PM   
4: 03/28/2014 06:35:13 AM 03/28/2014 10:47:00 PM   
5: 03/28/2014 08:31:38 AM 03/28/2014 10:37:00 AM

995: 07/03/2017 12:40:04 PM 07/03/2017 03:40:02 PM   
996: 07/04/2017 01:30:35 AM 07/04/2017 02:50:22 AM   
997: 09/03/2014 03:32:57 PM 09/04/2014 04:30:30 PM   
998: 09/05/2014 11:17:53 AM 09/08/2014 03:37:25 PM   
999: 11/06/2018 07:15:28 PM 11/06/2018 08:17:52 PM

As we can see, the sorting makes sense as a string sort rather than a date sort. In fact, this points to one of the virtues of using proper ISO date formatting, which is that it will still get the sort right when sorting as a string, unlike in the preceding format. This is an example of a very common problem with “found” time series data: the timestamping format available may not be the most conducive to time series analysis.

We revisit the NYC Open Data interface to see whether there is a workaround to this formatting (see Figure 15-3).

![](images/a6f10724a3ddc2ab80ec272fbd700e6e9799d2806a58ee3e27fed419be130b98.jpg)  
Figure 15-3. The NYC Open Data portal offers sorting via a web interface, which appears to be available to any column on this large data set. This is an impressive free resource when you think about the computational power required for that sort.

A glimpse at the web page tabular view of the data seems to concur with our CSV, in that the data does not appear to be sorted in time order. However, we see there is a sort option, so we apply this to the data set, which does indeed update the data after an understandable wait while the large data set is stored. This would be a great solu‐ tion, except that, unfortunately the resulting CSV downloaded is still out of order.

There are a number of other solutions we could explore at this time. We could see whether using the Open Data’s API rather than the web interface provides more digestible dates or a way to ensure sorting. We could work with more Linux command-line tools, such as awk, to extract the different parts of the timestamp into different columns or into a single rearranged column with ISO formatting.

Instead, we take a pared-back approach of seeing whether our available tools can handle this CSV if we read only certain columns. The first question that interests me about this data set is how the lag between the creation of a 311 complaint and the closing of that complaint may have varied over time. In this case, I hypothesize that I need only two columns: CreatedDate and ClosedDate. I will see whether just reading these two columns, which are a small portion of all the columns in terms of both count and character count (because some columns are quite long), is possible on my lightweight laptop. (I could also explore the lazy way of fixing the problem, which would be a hardware upgrade, either temporarily or permanently.)

So now we are able to read in all the rows of the data, and our subsequent analysis will be on the full data set rather than only the first 1,000 rows:

## R

> ## read in only the columns of interest

```awk
> df = fread("311.csv", select = c("Created Date", "Closed Date"))
>
> ## use data.table's recommended 'set' paradigm to set col names
> setnames(df, gsub("", "", colnames(df)))
>
> ## eliminate rows with a blank date field
> df = df[nchar(CreatedDate) > 1 & nchar(ClosedDate) > 1]
>
> ## convert string date columns to POSIXct
> fmt.str = "%m/%d/%Y %I:%M:%S %p"
> df[, CreatedDate := as.POSIXct(CreatedDate, format = fmt.str)]
> df[, ClosedDate := as.POSIXct(ClosedDate, format = fmt.str)]
>
> ## order in chronological order of CreatedDate
> setorder(df, CreatedDate)
>
> ## calculate the number of days between creating and closing
> ## a 311 complaint
> df[, LagTime := as.numeric(difftime(ClosedDate, CreatedDate,
		units = "days"))] 
```

This was done on a lightweight laptop manufactured in 2015, so chances are you can do it with whatever you use for work or home, too. Those new to “big data” may be surprised that 19 million rows is not a big deal, but this is often the case. We actually need only a small slice of the data to address a relevant time series question.

Once we look at the LagTime column, we notice some surprisingly incorrect numbers —numbering in tens of thousands of days, or negative days. We weed out these num‐ bers and put a cap on the data driven partly by a distribution of a random sample of data points:

```txt
## R
> summary(df$LagTime)
Min. 1st Qu. Median Mean 3rd Qu. Max. NA's
-42943.4 0.1 2.0 2.0 7.9 368961.1 609835
> nrow(df[LagTime < 0]) / nrow(df)
[1] 0.01362189
> nrow(df[LagTime > 1000]) / nrow(df)
[1] 0.0009169934
> df = df[LagTime < 1000]
> df = df[LagTime > 0]
> df.new = df[seq(1, nrow(df), 2), ]
> write.csv(df.new[order(ClosedDate)], "abridged.cf.csv") 
```

We discard data with negative lag times, as we lack documentation or domain knowl‐ edge to know what those are. We also reject data that we regard as having unrealistic

or unhelpfully extreme values by discarding data where the lag time to close out the 311 complaint was more than 1,000 days.8

![](images/61f8f6af64565464211eda9ee611bfc85209f418c625aa396ceddfb854a96b67.jpg)

Use caution when you discard data. In this exercise, we discarded around $1 . 3 \%$ of the data due to lag times in resolving a 311 com‐ plaint that didn’t make sense, either because they were so long as to warrant an explanation we would not be able to get or because they were negative, implying a data entry error or some other issue beyond what we could likely discover from the data alone.

Given that our question relates to the overall distribution of the data, it’s unlikely that such a small portion of points would affect our analysis about a distributional question. However, in the realworld you would want to investigate these data points and the pos‐ sible downstream effects on your analysis task. This is not time series–specific advice but just a matter of general practice.

Now that we are able to hold all of the data of interest in memory at the same time, we could ask questions about the series holistically. However, the question of interest is whether and how the distribution of lag time may have changed over time. We could do this with a sliding or rolling window over the time series, but that is compu‐ tationally taxing; we would have to do many related computations repeatedly as we slide a window over the data. Also we want to explore doing this in a way that could also work on a live stream, as we could imagine continuing this project onto current data as it comes in. It would be better not to have to store this multigigabyte data file indefinitely.

# Online Statistical Analysis of Time Series Data

We are going to use a fairly straightforward online quantile estimation tool called the $P$ -square algorithm, with one slight modification to make it time-aware. The original algorithm assumed that there was a stable distribution from which quantiles were being inferred, but we want to account for the case of distributions that change over time. As with an exponentially weighted moving average, we add this time awareness by weighting earlier observations less, and we do it in the same way by introducing a factor to scale down the weights of the previous measurements each time a new measurement is available (see Figure 15-4).

![](images/dff6d5f5706dad328258b3bee8e654f6c31895bce48a3a4c43ed5d03abe9cbb5.jpg)  
Figure 15-4. The computational structure of our online quantile estimation using the P-square algorithm. We maintain a series of markers indicating where we think the quantiles are and what the cumulative counts are for all data points less than or equal to each of those quantiles.

The algorithm requires a bit of recordkeeping, making it easier to implement in a more object-oriented programming language, so we’ll switch from R to Python. Note, however, that we will use the preprocessed data from R, as the data.table package has substantially better performance with that kind of big data than tools available in Python.

The version of the P-square algorithm we implement forms a histogram of the values. So, as we create a PQuantile object, we allocate a preset number of bins for our histo‐ gram counts, bin positions, and running tallies of observations:

```python
>>> ## imports and a utilit lambda
>>> import bisect
>>> import math
>>> sign = lambda x: (1, -1)[x < 0]
>>> 
>>> #beginning of class definition
>>> class PQuantile:
>>> def __init__(self, b, discount_factor):
>>> 
>>> 
>>> self.num_obs = 0  # 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
<|im_start|>assistant
```
python
>>> ##
imports and a utilit lambda
>>> import bisect
>>> import math
>>> sign = lambda x: (1, -1)[x < 0]
>>> 
>>> #beginning of class definition
>>> class PQuantile:
>>> def __init__(self, b, discount_factor):
>>> 
>>> #initialization
>>> 
>>> self.num_obs = 0  # 
## self-explanatory
>>> 
>>> #counts per quantile
>>> 
>>> self.n = [i for i in range(self.b+1)]
>>> 
>>> 
>>> self.q = [ ]  # the estimated quantile values
>>> 
```

```python
>>> ## b is the number of quantiles,   
>>> # including the 0th and 100th   
>>> # (min and max values) quantile   
>>> self.b = b   
>>> # the discount factor sets how we adjust prior counts   
>>> # when new data is available   
>>> selfdiscount_factor = discount_factor 
```

There are two configurable parameters: the number of evenly spaced quantiles to estimate and the discounting factor for old observations.

The other class members include a running total of the number of observations (this will be subject to time discounting by the configurable time discounting factor), the estimated quantiles, and the running count of observations less than or equal to a given quantile value.

There is only one public function, whose role is to accept the next observation. When a new observation arrives, what happens is a result of how early in the series we are. For the first self.b values, the inputs are accepted and necessarily make up the esti‐ mation of the quantile. self.q is sorted so that its values reflect the quantiles.

So, for example, imagine you input a $\textsf { b } = \textsf { 5 }$ for the number of desired quantile values and then input the sequence 2, 8, 1, 4, 3. At the end of this sequence, self.q would be equal to [1, 2, 3, 4, 8]. self.n, the counts of values less than or equal to each of the quantiles, would be equal to [1, 2, 3, 4, 5], the value it was already initialized to in __init__:

```python
>>> def next_obs(self, x):
>>> if self.num_obs < (self.b + 1):
>>> self.q.append(x)
>>> self.q.sort()
>>> self.num_obs = self.num_obs + 1
>>> else:
>>> self.next_obs2(x)
>>> self.next_obs = self.next_obs2 
```

Things get interesting once you have more than self.b values. At this point, the code begins to make decisions as to how to combine values to estimate quantiles without keeping all the data points stored for repeated analysis. In this case, the P-square algorithm does this with what we call self.next_obs2:

```python
#!/python
>>> def next_obs2(self, x):
>>>     ## discounting the number of observations
>>> if self.num_obs > self.b * 10:
>>>         corrected_obs = max(self(discount_factor * self(num_obs, self.b))
>>>             self.num_obs = corrected_obs + 1
>>>                 self.n = [math.ceil(nn * self.discount_factor)] 
```

```python
>>> for nn in self.n]
>>> 
>>> for i in range(len(self.n) - 1):
>>> if self.n[i + 1] - self.n[i] == 0:
>>> self.n[i + 1] = self.n[i + 1] + 1
>>> elif self.n[i + 1] < self.n[1]:
>>> # in practice this doesn't seem to happen
>>> self.n[i + 1] = self.n[i] - self.n[1 + 1] + 1
>>> else:
>>> self.num_obs = self.num_obs + 1
>>> 
>>> k = bisect.bisect(left(self.q, x)
>>> if k is 0:
>>> self.q[0] = x
>>> elif k is self.b + 1:
>>> self.q[-1] = x
>>> k = self.b
>>> if k is not 0:
>>> k = k - 1
>>> 
>>> self.n[(k + 1): (self.b + 1)] = [self.n[i] + 1
>>> for i in range((k + 1), 
>>> (self.b + 1))
>>> for i in range(1, self.b):
>>> np = (i)*(self.num_obs - 1)/(self.b)
>>> d = np - self.n[i]
>>> if (d >= 1 and (self.n[i + 1] - self.n[i]) > 1):
>>> self._update_val(i, d)
>>> elif (d <= -1 and (self.n[i - 1] - self.n[i]) < -1):
>>> self._update_val(i, d) 
```

Ideally, the ith quantile value should be evenly spaced so that exactly $i / b \ \times$ total observations are less than it. If this is not the case, the marker is moved one position to the left or right, and its associated quantile value is modified with a formula derived from the presumption of a locally parabolic shape of the histogram. That for‐ mula dictates the criteria indicated earlier for sizing the d variable to determine whether a particular quantile value and count have to be adjusted.

If the value does have to be adjusted, there is another decision to make as to whether the parabolic or linear adjustment is appropriate. This is implemented in the follow‐ ing code. For more details, see the derivation in the original paper. This paper is great because of the approachable mathematics used in the technique and also because it provides very clear instructions as to how to implement the method and then test your implementation:

```python
## python
>>>     ## overall update
>>>     ## as you can see both self.q and self.n are updated
>>>     ## as a quantile position is shifted
>>>     def_update_val(self, i, d):
>>>         d = sign(d) 
```

```python
>>> qp = self._adjust_parabolic(i, d)
>>> if self.q[i] < qp < self.q[i+1]:
>>> self.q[i] = qp
>>> else:
>>> self.q[i] = self._adjust_LINEAR(i, d)
>>> self.n[i] = self.n[i] + d
>>> 
>>> this is the primary update method
>>> def _adjust_parabolic(self, i, d):
>>> new_val = self.q[i]
>>> m1 = d/(self.n[i+1] - self.n[i-1])
>>> s1 = (self.n[i] - self.n[i-1] + d) *
>>> (self.q[i+1] - self.q[i]) / (self.n[i+1] - self.n[i])
>>> s2 = (self.n[i+1] - self.n[i] - d) *
>>> 
>>> this is the backup linear adjustment when parabolic
>>> 
>>> conditions are not met
>>> def _adjust_parabolic(self, i, d):
>>> new_val = self.q[i]
>>> new_val = new_val + d * (self.q[i + d] - self.q[i]) / (self.n[i+d] - self.n[i])
>>> return new_val 
```

For perspective on the simplicity of this method, all the class code is listed here in one place:

```python
#!/python
>>> class PQuantile:
>>>  ## INITIALIZATION
>>>  def __init__(self, b, discount_factor):
>>>  self.num_obs = 0
>>>  self.b = b
>>>  self.discount_factor = discount_factor
>>>  self.n = [i for i in range(self.b+1)]
>>>  self.q = []
>>>  #DATA INTAKE
>>>  def next_obs(self, x):
>>>  if self.num_obs < (self.b + 1):
>>>  self.q.append(x)
>>>  self.q.sort()
>>>  self(num_obs = self(num_obs + 1)
>>>  else:
>>>  self.next_obs2(x)
>>>  self.next_obs = self.next_obs2
>>>  def next_obs2(self, x):
>>>  #discounting the number of observations
>>>  if self.num_obs > self.b * 10:
>>>  corrected_obs = max(self.discount_factor)
>>>  * self.num_obs,
>>>  self.b) 
```

```python
self.num_obs = corrected_obs + 1
    self.n = [math.ceil(nn * selfdiscount_factor)
        for nn in self.n]
        for i in range(len(self.n) - 1):
            if self.n[i + 1] - self.n[i] == 0:
                self.n[i + 1] = self.n[i + 1] + 1
            elif self.n[i + 1] < self.n[1]:
                # in practice this doesn't seem to happen
                self.n[i + 1] = self.n[i] - self.n[1 + 1] + 1
        else:
            self.num_obs = self.num_obs + 1
        k = bisect.bisect_left(self.q, x)
        if k is 0:
            self.q[0] = x
        elif k is self.b + 1:
            self.q[-1] = x
            k = self.b
        if k is not 0:
            k = k - 1
        self.n[(k + 1): (self.b + 1)] = [self.n[i] + 1
            for i in range((k + 1), (self.b + 1))
            for i in range(1, self.b):
                np = (i)*(self.num_obs - 1)/(self.b)
                d = np - self.n[i]
                if (d >= 1 and (self.n[i + 1] - self.n[i]) > 1):
                    self._update_val(i, d)
                    elif (d <= -1 and (self.n[i - 1] - self.n[i]) < -1):
                        self._update_val(i, d)
        HISTOGRAM ADJUSTMENTS
       _update_val(self, i, d):
            d = sign(d)
            qp = self._adjust_parabolic(i, d)
            if self.q[i] < qp < self.q[i + 1]:
                self.q[i] = qp
        else:
            self.q[i] = self._adjust_LINEAR(i, d)
            self.n[i] = self.n[i] + d
        _adjust_parabolic(self, i, d):
            new_val = self.q[i]
            m1 = d/(self.n[i + 1] - self.n[i - 1])
            s1 = (self.n[i] - self.n[i - 1] + d) *
                (self.q[i + 1] - self.q[i]) / (self.n[i + 1] - self.n[i])
            s2 = (self.n[i + 1] - self.n[i] - d) *
                (self.q[i] - self.q[i - 1]) / (self.n[i] - self.n[i - 1])
        else:
            self(num_n = [n], n)
        else:
            self(num_n = [n], n)
        else:
            self(num_n = [n], n)
        else:
            self(num_n = [n], n)
        else:
            self(num_n = [n], n)
        else:
            self(num_n = [n], n)
        else:
            self(num_n = [n], n)
        else:
            self(num_n = [n], n)
        else:
            self(num_n = (n), n)
        else:
            self(num_n = (n), n)
        else:
            self(num_n = (n), n)
        else:
            self(num_n = (n), n)
        else:
            self(num_n = (n), n)
        else:
            self(num_n = (n), n)
        else:
            self(num_n = (n), n)
        else:
            self(num_n = (n), n)
        else:
            self(num_m) 
```

```txt
>>> new_val = new_val + m1 * (s1 + s2)
>>> return new_val
>>> def _adjust_linear(self, i, d):
>>> new_val = self.q[i]
>>> new_val = new_val + d * (self.q[i + d] - self.q[i]) / (self.n[i+d] - self.n[i])
>>> return new_val 
```

Now that we have this time-oriented method, we should convince ourselves that it works reasonably well with a toy example. We first try sampling data points from one distribution and then abruptly changing to another. In each case, we are sampling for the 40th percentile, although based on the configuration of 10 histogram points, we are maintaining a histogram that indicates the 0th, 10th, 20th, …90th, 100th percen‐ tiles. This is helpful because it means we can have a fairly detailed description of the changing distribution. For this toy example, we focus on just the 40th percentile (qt.q[4]), which results in the plot in Figure 15-5:

```python
>>> qt = PQuantile(10, 0.3)
>>> qt_estimators = []
>>> 
>>> for _ in range(100):
>>> b.next_obs(uniform())
>>> if len(b.q) > 10:
>>> qt_estimators.append(qt.q[4])
>>> for _ in range(100):
>>> b.next_obs(uniform(low = 0.9))
>>> qt_estimators.append(qt.q[4])
>>> 
>>> plt.plot(qt_estimators) 
```

![](images/f4a137268eb5dae733a9c8d3593d213f91b70a45b7183b8cd7f0781950ba4114.jpg)  
Figure 15-5. When we discount older measurements heavily (multiplying by a smaller discount factor), we more quickly see that the underlying distribution has changed.

In contrast, we see a slower adoption to the changing quantile in the case of a larger discounting factor (see Figure 15-6):

```python
>>> qt = PQuantile(10, 0.8)
>>> qt_estimators = []
>>> 
>>> for _ in range(100):
>>> b.next_obs(uniform())
>>> if len(b.q) > 10:
>>> qt_estimators.append(qt.q[4])
>>> for _ in range(100):
>>> b.next_obs(uniform(low = 0.9))
>>> qt_estimators.append(qt.q[4])
>>> 
>>> plt.plot(qt_estimators) 
```

![](images/730254c0dc3ec7dd4bf9e98f388b08b4d70da568c997dec2b5a47ca6163cb44f.jpg)  
Figure 15-6. When we discount older measurements less (multiplying by a larger dis‐ count factor), our quantile estimation is slower to recognize that the underlying distri‐ bution has changed.

Now we apply this rolling quantile to a subset of our data (see Figure 15-7). We do not do the entire data set, not because of the computational challenge, but because it was overly taxing for my workaday laptop to graph all the recorded quantiles!

```python
>>> import numpy
>>> nprows = 1000000
>>> qt_est1 = np.zeros(nrows)
>>> qt_est2 = np.zeros(nrows)
>>> qt_est3 = np.zeros(nrows)
>>> qt_est4 = np.zeros(nrows)
>>> qt_est5 = np.zeros(nrows)
>>> qt_est6 = np.zeros(nrows)
>>> qt_est7 = np.zeros(nrows)
>>> qt_est8 = np.zeros(nrows)
>>> qt_est9 = np.zeros(nrows)
>>> for idx, val in enumerate(df.LagTime[:nrows]):
>>> qt.next_obs(val)
>>> if len(qt.q) > 10:
>>> qt_est1[idx] = qt.q[1] 
```

```python
>>> qt_est2[idx] = qt.q[2]  
>>> qt_est3[idx] = qt.q[3]  
>>> qt_est4[idx] = qt.q[4]  
>>> qt_est5[idx] = qt.q[5]  
>>> qt_est6[idx] = qt.q[6]  
>>> qt_est7[idx] = qt.q[7]  
>>> qt_est8[idx] = qt.q[8]  
>>> qt_est9[idx] = qt.q[9]  
>>> plot(qt_est9, color = 'red')  
>>> plt.plot(qt_est7, color = 'pink')  
>>> plt.plot(qt_est5, color = 'blue')  
>>> plt.plot(qt_est3, color = 'gray')  
>>> plt.plot(qt_est2, color = 'orange' 
```

![](images/a6b39b0d2d0946bea948cfbe9c2fe8fac40bb6a09c80fef3ffdecd33b3d5cc7b.jpg)  
Figure 15-7. The 90th, 70th, 50th, 30th, and 20th percentile values over time for the first 100,000 rows in the data set when sorted by closed date.

Figure 15-7 shows the 90th, 70th 50th, 30th, and 20th percentile values plotted over time for the first 100,000 rows in the data set when sorted by closed date. Because they were sorted by closed date, it is likely that many quickly resolved 311 complaints were front-loaded, which explains the much smaller quantile estimations in the first portion of the data set.9

So did the distribution change? Visually it seems to, for a few reasons. One is left cen‐ soring, described shortly, which reflects how we sorted and selected data. The fact that we ordered the data by the ClosedData column, combined with the fact that this data set does not appear to have infinite lookback (that is, presumably 311 com‐ plaints made before a certain date did not make it into this system), makes the lag times we see at the beginning dates appear shorter. In other words, this apparent

change over time is simply an artifact of our incomplete data (and an incomplete underlying data set) combined with our choice of sorting.

On the other hand, we can see features that nonetheless do suggest changes in distri‐ butions over time. There appear to be peaks and troughs in the quantile curve esti‐ mates, and we might even consider whether there may be periodic behavior in our curves, as there may be predictable times when the quantile values rise and fall due to exogenous organizational factors (perhaps a drive to close out complaints before the end of a month, or certain funding cycles that increase the number of workers avail‐ able to close out complaints at certain times of the year).

Given that we now have preliminary results, the best option would be to establish some significant dates (i.e., where do we see spikes or periodic behavior?) and try to cross-reference them against any institutional facts we can establish about the rhythm of work. We should also run simulations to assess how we think the left censoring should affect early quantile estimations under different scenarios. In this way we could develop better qualitative and quantitative understanding of the unknown aspects of our system, and this information would be extremely helpful for making a final determination as to whether the distribution of resolution times is evolving over time and, if so, how.

Let’s say we had taken these steps—then what? We would need to look for a method‐ ology to compare distributions for similarity difference where only the quantiles of the distribution rather than all the sample points were available. One way we could do this would be to run simulations/bootstrap the entire process. This would lead to an answer where we could fully articulate and control the assumptions that went into the model by coding our simulations. In fact, many statistical approaches to doing such comparisons also focus on bootstrap methods.

# Left Censoring, Right Censoring, and Interval Censoring

For time series analysis, and related pursuits such as survival analysis, we must be aware of how our data selection, sorting, or presentation may affect our analysis, as we determined was the case here in that the earlier time periods appear to have a much narrower distribution than the later time periods. This is likely due to the fact that we sorted the data by closed date, a form of left censoring, meaning that the “event of interest” has already occurred before the start of the “study.” In this case, our event of interest is the start and end dates of the complaints, and we are limited by the start date of the data set. In contrast, the related concept of right censoring means the event of interest would occur after an event was over. Finally, interval cen‐ soring refers to situations in which we only have an inexact recording, such as an interval, for when an event occurred, which will also necessarily affect our analysis.

# Remaining Questions

Our visualization suggests new queries. One relates to the possibility of cyclical or seasonal behavior. There seem to be periodic bumps in all the estimated quantiles. We might consider further investigating this, and we have a number of options for doing so:

• We could attempt to fit harmonics (sines and cosines) to these quantile curves and see whether a common periodicity emerged.   
• We could model the quantile itself as an ARIMA or SARIMA process and look for evidence of seasonality. This would also entail preliminary steps such as exploring the ACF and PACF of the curves that we would model as time series.   
• We could ask the agency running the 311 service for more information and see whether they recognize any periodic behavior induced by their organizational structure and operating procedures.

In addition to the periodic behavior, we can also see a jump in the estimated quantile values at just under the index $= 7 0 { , } 0 0 0$ location. Given that all the quantiles jumped, it seems unlikely that this is due to just a single or a handful of outliers. There are a few ways we might investigate this:

• Go back to the raw data for this time period and see what features may suggest an explanation. Was there a surge in 311 complaints? Or a surge in a particular kind of complaint that tends to take longer to resolve?   
• Alternately, we could revisit the raw data to back out the approximate date of this jump in the quantiles and cross-reference this against local news, preferably with the help of someone who could point us in the right direction. Assistance from someone at the agency, or someone savvy about city government, might be most valuable. It’s also possible, however, that this date could correspond to a massive event in NYC that would explain the jump, such as superstorm Sandy in 2012.

# Further Improvements

We could make this an even more time-aware algorithm. Our modification to the Psquare algorithm discounts prior observations, but it assumes that all observations are evenly spaced. This is manifested in the fact that the input of the next observation does not feature a timestamp, and the same discounting factor is always applied. We could craft a more flexible algorithm by using the change in timestamp to old infor‐ mation as compared to new information such that the discounting would depend on the change in time since the last update was measured. It would also be more accu‐ rate for our 311 data set. This is left as an exercise for the reader but only involves

changing a few lines of code. Hint: the discounting factor should become a function of time.

We could also look into other ways of estimating quantiles over time, either with an online or window measurement. Because the importance of online data is growing— and especially for online big data—there is a variety of emerging research on this topic. Both statistical and machine learning approaches have dealt with this over the last years, and there are a good number of approachable academic papers that are accessible to practicing data scientists.

# More Resources

Ted Dunning and Otmar Ertal, “Computing Extremely Accurate Quantiles Using t-Digests,” research paper, 2019, https://perma.cc/Z2A6-H76H.

The f-digest algorithm for extremely efficient and flexible computation of quan‐ tiles of online time series data is rapidly gaining traction as a leading technique for handling quantile estimation of online time series even for the case of nonsta‐ tionary distributions. Implementations are available in a number of languages, including Python and high-performance $\mathrm { C } { + } { + }$ and Go variations. This approach is particularly useful because there is no need to decide in advance which quan‐ tiles interest you—rather, the entire distribution is modeled as a set of clusters from which you can infer any quantile you like.

Dana Draghicescu, Serge Guillas, and Wei Biao Wu, “Quantile Curve Estimation and Visualization for Nonstationary Time Series,” Journal of Computational and Graphi‐ cal Statistics 18, no. 1 (2009): 1–20, https://perma.cc/Z7T5-PSCB.

This article illustrates several nonparametric methods to model nonstationary distributions in time series quantile estimation. It is helpful because it covers real-world data, simulation, and nonstandard distributions (such as non-Gaussian distributions). There is also sample code (unusual for a statistics aca‐ demic journal article), albeit behind a paywall.

András A. Benczúr, Levente Kocsis, and Róbert Pálovics, “Online Machine Learning in Big Data Streams,” research paper, 2018, https://perma.cc/9TTY-VQL3.

This reference discusses technical approaches to a variety of common machine learning tasks related to time series. Particularly interesting is the discussion of online data processing for many kinds of machine learning and technical tips about parallelization of online tasks.

Sanjay Dasgupta, “Online and Streaming Algorithms for Clustering,” lecture notes, Computer Science and Engineering, University of California San Diego, Spring 2008, https://perma.cc/V3XL-GPK2.

While not specific to time series data, these lecture notes give a general overview of unsupervised clustering for online data. These notes are enough to get you started putting together potential solutions for a time series–specific application.

Ruofeng Wen et al., “A Multi-Horizon Quantile Recurrent Forecaster,” research paper, November 2017, https://perma.cc/22AE-N7F3.

This research paper from Amazon provides an example of using quantile infor‐ mation from data as a way to train recurrent neural networks effectively for time series forecasting. The researchers demonstrate effective training of a neural net‐ work that can produce probabilistic assessments rather than point estimates. This paper is a good illustration of another potential use case for quantile infor‐ mation, an underused resource in time series analysis.

# Time Series Packages

In the past several years, there have been a number of packages and papers released by large tech companies related to how they deal with the massive number of time series they collect as digital organizations with enormous customer bases, sophistica‐ ted logging, cutting-edge business analytics, and numerous forecasting and data pro‐ cessing needs. In this chapter we will discuss some of the main areas of research and development related to these ever-expanding time series data sets, specifically: fore‐ casting at scale and anomaly detection.

# Forecasting at Scale

For many large tech companies, dealing with time series is an increasingly important problem and one that arose naturally within their organizations. Over time, several of these companies responded by developing smart, automated time series packages specifically targeted to “forecasting at scale” because so many forecasts were needed in a wide variety of domains. Here’s how two data scientists at Google who developed the company’s automated forecasting package described the circumstances that moti‐ vated their product in a 2017 blog post (emphasis added):

The demand for time series forecasting at Google grew rapidly along with the com‐ pany over its first decade. Various business and engineering needs led to a multitude of forecasting approaches, most reliant on direct analyst support. The volume and variety of the approaches, and in some cases their inconsistency, called out for an attempt to unify, automate, and extend forecasting methods, and to distribute the results via tools that could be deployed reliably across the company. That is, for an attempt to develop methods and tools that would facilitate accurate large-scale time series forecasting at Google.

There is so much relevant data and so much to forecast that it would be extremely expensive and organizationally challenging to onboard and employ enough analysts

to generate every prediction of organizational interest. Instead, these packages move to a “good enough” philosophy; that is, a reasonably good forecast is far better than having no forecast while waiting for the perfect, artfully crafted one from a time ser‐ ies expert with domain knowledge. Next we discuss two automated forecasting frameworks in more detail, from Google and Facebook.

# Google’s Industrial In-house Forecasting

Google has released some information about its in-house automated forecasting tool, which grew out of an effort led by several data scientists in the company’s search infrastructure division. The task was to write a unified approach to making automa‐ ted forecasts throughout the organization. Because the tasks were automated, this meant that the results had to be safe—they couldn’t go too far off the rails and had to come with some estimation of uncertainty in the prediction. Moreover, because the team sought a widely applicable solution, the methodology had to address common problems in time series data sets related to humans, such as seasonality, missing data, holidays, and behaviors evolving over time.

The solution rolled out in Google comprises three interesting steps related to what we have covered in the previous chapters:

1. Automated and extensive data cleaning and smoothing   
2. Temporal aggregation and geographic/conceptual disaggregation of the data   
3. Combining of forecasts and simulation-based generation of uncertainty estimates

We will discuss each step in turn.

# Automated and extensive data cleaning and smoothing

In the Unofficial Google Data Science Blog post mentioned earlier, two leaders of the in-house project indicated that the data cleaning and smoothing tackled a number of problems:

# Imperfect data effects

• Missing data   
• Outlier detection   
• Level changes (such as those due to product launches or sudden but permanent changes in behavior)   
• Transforming data

Imperfect data is a fact of life. Missing data can occur due to technical glitches. Outliers can have similar causes or be “real values” but are not worth including

in a forecast if they are not likely to repeat themselves. Level changes (regime changes) can occur for a myriad of reasons, including that baseline behavior is changing drastically (evolving world), what is being measured is changing drasti‐ cally (evolving product), or what is being recorded is changing drastically (evolv‐ ing logging). Finally, data can come in distributions that are far from the normality or stationarity presumed in many time series models. The Google approach addresses each of these data imperfections in turn with automated methodologies to detect and “correct” these problems.

# Calendar-related effects

• Yearly seasonality   
• Weekly seasonality (effect of day of the week)   
• Holidays

The treatment of calendar-related effects was particularly tricky for an organiza‐ tion such as Google, with operations and users all over the world. The yearly sea‐ sonality would look very different in different parts of the world, particularly in opposite hemispheres with their own weather patterns and also in cultures with different underlying calendars. As the blog post pointed out, sometimes the same holiday could occur more than once in one (Gregorian) calendar year. Likewise, the “seasons” under which a particular group of people operate could shift within the Gregorian calendar year, which can often occur because the Islamic calendar has a different periodicity from the Gregorian calendar.

# Temporal aggregation and geographic/conceptual disaggregation

The Google team found that weekly data worked well for most of their predictions of interest, so after they cleaned the data in the previous step, they aggregated it into weekly increments for purposes of making forecasts. In this sense, they performed temporal aggregation.

However, the team also found it was helpful to disaggregate the data, sometimes geo‐ graphically, sometimes by category (such as device type), and sometimes by a combi‐ nation of factors (such as geographic region by device type). The team found that in such cases, it was more effective to make forecasts for the disaggregated subseries and then reconcile these to produce a global forecast if both the subseries and global ser‐ ies were of interest. We discussed various ways to fit hierarchical time series in Chap‐ ter 6, and this method reflects the methodology of starting with lower-level forecasts and propagating them upwards.

# Combining forecasts and simulation-based generation of uncertainty estimates

Google uses ensemble approaches, combining a number of different forecast model results to generate the final forecast. This is useful for a number of reasons. Google

believes this generates a wisdom-of-experts style of forecast, drawing the benefits of many well-performing and well-justified forecast models (such as exponential smoothing, ARIMA, and others). Also, an ensemble of forecasts generates a distribu‐ tion of forecasts, providing a basis to determine whether any of them are so different from the “crowd” as to be unreasonable. Finally, ensembling provides a way to quan‐ tify the uncertainty regarding the forecast, which Google also does by simulating for‐ ward propagation of the errors through time.

Ultimately, as the data science team concedes, Google’s approach benefits from largescale and highly parallel approaches appropriate to the enormous computing resour‐ ces available at the company. It also benefits from establishing parallel tasks (such as simulating the error propagated forward in time many times) and automated pro‐ cesses with room for input from an analyst but no need for it. In this way reasonable forecasts can be generated at scale and for a variety of data sets.

While your work may not have the same advantages or need the high level of auto‐ mation required at Google, there are many ideas you can incorporate into your work‐ flow from their model even as a solo data scientist. These include:

• Building a framework or “pipeline” for cleaning your data as a baseline “good enough” version of how you would like to prepare every time series data set before modeling.   
• Building an ensemble of respected forecasting models as your “go to” toolkit.

# Facebook’s Open Source Prophet Package

Facebook open-sourced its automated time series forecasting package, Prophet, at around the same time Google released information regarding its in-house package. In its own blog post about Prophet, Facebook highlighted some of the very same issues emphasized in Google’s approach, particularly:

• “Human-scale” seasonalities and irregularly spaced holidays   
• Level changes   
• Missing data and outliers

Additionally, the Facebook package brought strength, such as:

• Capacity to handle data sets at various granularity levels, such as minute-byminute or hourly data in addition to daily data   
• Trends that indicate a nonlinear growth, such as reaching a saturation point

Facebook noted that the results with its package were often just as good as those pro‐ duced by analysts. Like Google, Facebook indicated that it had found many instances

of highly parallel tasks while developing its time series fitting pipeline. Facebook said that Prophet had developed into such a reliable forecaster that its forecasts were used not only internally but in outward-facing products. Also, Facebook said it had devel‐ oped a work pattern of “analyst-in-the-loop” such that the automated process could be supervised and corrected where necessary, ultimately leading to a product that could assist or replace human analysis depending on the level of resources devoted to a task.

Facebook’s approach is quite different from Google’s; it includes three simple components:

• Yearly and weekly seasonal effects   
• A custom lists of holidays   
• A piecewise linear or logistic trend curve

These components are used to form an additive regression model. This is a nonpara‐ metric regression model, which means that no assumptions are made about the form of the underlying regression function and no linearity is imposed. The model is more flexible and interpretable than a linear regression but comes with a trade-off of higher variance (think of the bias-variance trade-off generally known in machine learning and statistics) and more problems in overfitting. This model makes sense for the gen‐ eral task Facebook seeks, in that complex nonlinear behaviors must be modeled in an automated way that avoids unduly taxing methodologies.

There are many advantages to Prophet, including:

• Simple API   
• Open source and under active development   
• Full and equal APIs in Python and R, which helps multilingual data science teams

Prophet is easy to use. We end this section with an example code snippet taken from the Quick Start guide so you can see the minimal code needed to roll out an automa‐ ted forecast. We pair our use of Prophet with use of pageviews, an R package that easily retrieves time series data related to Wikipedia page views. First we download some time series data:

```r
## R
> library(pageviews)
> df_wiki = article_pageviews.project = "en.wikipedia",
>
> article = "Facebook",
>
start = as.Date('2015-11-01'),
end = as.Date("2018-11-02"),
user_type = c("user"),
platform = c("mobile-web")) 
```

```txt
> colnames(df_wiki)  
[1] "project" "language" "article" "access" "agent" "granularity"  
[7] "date" "views" 
```

Now that we have some data at daily temporal resolution and over a few years, we can try to forecast this data with Prophet in just a few simple steps (see Figure 16-1):

```r
## R
>>> we subset the data down to what we need
>>> and give it the expected column names
>>> df = dfWiki[, c("date", "views")] 
>>> colnames(df) = c("ds", "y")] 
>>> we also log-transform the data because the data
>>> has such extreme changes in values that large
>>> values dwarf normal inter-day variation
>>> df = log(df|y)
>>> we make a 'future' data frame which
>>> includes the future dates we would like to predict
>>> we'll predict 365 days beyond our data
>>> m = prophet(df)
>>> future <- make_future_dataframe(m, periods = 365)
>>> tail(future)
ds
1458 2019-10-28
1459 2019-10-29
1460 2019-10-30
1461 2019-10-31
1462 2019-11-01
1463 2019-11-02
>>> we generate the forecast for the dates of interest
>>> forecast <- predict(m, future)
>>> tail(celeast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper'])))
ds yhat yhat_lower yhat_upper
1458 2019-10-28 9.119005 8.318483 9.959014
1459 2019-10-29 9.090555 8.283542 9.982579
1460 2019-10-30 9.064916 8.251723 9.908362
1461 2019-10-31 9.066713 8.254401 9.923814
1462 2019-11-01 9.015019 8.166530 9.883218
1463 2019-11-02 9.008619 8.195123 9.862962
>>> now we have predictions for our value of interest
>>> finally we plot to see how the package did qualitatively
>>> plot(df|ds, df|y, col = 1, type = 'l', xlim = range(celeast|ds),
>>> main = "Actual and predicted Wikipedia pageviews of 'Facebook'.")
>>> points(celeast|ds, forecast|yhat, type = 'l', col = 2) 
```

![](images/b96d7aa1c0f380a96d960537a206927b6eae9928f6c0195c50583246b839e901.jpg)  
ActualandpredictedWikipediapageviewsof'Facebook'   
Figure 16-1. Plot of the Wikipedia page count data (thin solid line) and Prophet’s pre‐ dictions for that data (thick dashed line).

Prophet also offers the option to plot the components (trend and seasonal) that form the prediction (see Figure 16-2):

```txt
## R
> prophet_plot_components(m, forecast) 
```

![](images/306fde4ce7c8959198f8e5e530f689df0f85ffd1c3dd0442faad275ff35791f8.jpg)

Restrict your use of prophet to daily data. By its own description, the Prophet package was developed for, and works best for, daily data. Unfortunately, this narrow specialization means that the same techniques can be quite unreliable for data at different time‐ scales. For this reason, you should approach this package and its associated techniques with caution when your data is not daily.

![](images/21c4129c2d32e538d6506303b3cbf325d93e8dc3bf146c0e0de2cd8ccf8a1c31.jpg)  
Figure 16-2. The prediction disaggregated into a trend, weekly, and yearly components. From this the prediction is formed by a sum of the components. Notice that different components are formed differently. The trend data has an underlying linear shape, while the yearly data is curvy due to its underlying Fourier series fit (read more in the Facebook blog post referenced earlier).

Over time, more automated time series open source packages and black box products are becoming available. These can be a good entry point for an organization that is new to forecasting and seeking to make a reasonable forecast. However, in the near future these packages likely will not make the best available forecast for every time series and every organization. When you can build domain knowledge and relevant organization constraints into your time series model, you will have a better result, and for now this remains the task of a human analyst until more general forecasting packages are built.

# Anomaly Detection

Anomaly detection is another area where tech companies are making significant efforts and sharing them with the open source community. Anomaly detection is important in time series for a few reasons:

• It can be helpful to remove outliers when fitting models that are not sufficiently robust to such outliers.   
• It can be helpful to identify outliers if we want to build a forecasting model specifically to predict the extent of such outlier events conditional on knowing they will happen.

Next we discuss the approach taken by Twitter in its open source work on anomaly detection.

# Twitter’s Open Source AnomalyDetection Package

Twitter open-sourced an outlier detection package, AnomalyDetection,1 four years ago, and the package remains useful and well performing. This package implements Seasonal Hybrid ESD (Extreme Studentized Deviant), which builds a more elaborate model than Generalized ESD for identifying outliers. The Generalized ESD test itself is built on another statistical test, the Grubbs test, which defines a statistic for testing the hypothesis that there is a single outlier in a data set. The Generalized ESD applies this test repeatedly, first to the most extreme outlier and then to successively smaller outliers, meanwhile adjusting the critical values on the test to account for multiple sequential tests. The Seasonal Hybrid ESD builds on Generalized ESD to account for seasonality in behavior via time series decomposition.

We can see the simple use of this package in the following R code. First we load some sample data provided by Twitter’s package:

```txt
## R
> library(AnomalyDetection)
> data(raw_data)
> head(raw_data)
timestamp count
1 1980-09-25 14:01:00 182.478
2 1980-09-25 14:02:00 176.231
3 1980-09-25 14:03:00 183.917
4 1980-09-25 14:04:00 177.798
5 1980-09-25 14:05:00 165.469
6 1980-09-25 14:06:00 181.878
```

Then we use Twitter’s automated anomaly detection function with two sets of parameters:

1. We look for a large portion of anomalies in either the positive or negative direc‐ tion.   
2. We look for a small portion of anomalies in the positive range only.

These use cases are demonstrated here:

```python
## R
> ## detect a high percentage of anomalies in either direction
> general_anoms = AnomalyDetectionTs(red_data, max_anoms=0.05,
> direction='both')
>
> ## detect a lower percentage of anomalies only in pos direction
> high_anoms = AnomalyDetectionTs(red_data, max_anoms=0.01,
> direction='pos') 
```

We plot the results in both cases in Figure 16-3.

![](images/8b313ac6e786590399cddb5de8b09687d6fa93a6b7190a5cef40faac3742f477.jpg)

![](images/ff894ac0d66d3ef7a1d8d7aea080ea2442ce8b779fb80674e9aa440948b230c2.jpg)  
Figure 16-3. The anomalies reported back from Twitter’s AnomalyDetectionTs() func‐ tion both with very inclusive settings (top) and with more limited settings (bottom).

We see that a large number of the anomalies are reported all in one portion of the time series, so we also crop our plot down to this portion of the time series to get a better understanding of what is happening (Figure 16-4).

![](images/da39e8006d64bb788303f77330366da98e2ea3118c2d12de263d4729554327d6.jpg)

![](images/b116a3ac41ba89cb242a9be147bd913c3fcae39658e222b112da50e37780b755.jpg)  
Figure 16-4. The same identified anomalies, now focusing on the anomalies that occur‐ red all in a cluster on the same day.

We can better understand why these points are anomalies after taking this closer look. They deviate from the daily pattern that is otherwise present. As to why we might want to look only for positive deviations, imagine that we are building infra‐ structure for a high-traffic, data-intensive website. While downward/negative anoma‐ lies may be of interest, the make-or-break business anomalies for us will be the times when our infrastructure cannot handle a high-traffic opportunity. We are most inter‐ ested in identifying spikes, for a number of reasons:

• If these numbers are fake, we’d like to weed them out so we can know the true ceiling of realistic usage. Anomalously high numbers will drive us to buy com‐ puting resources we don’t need, which isn’t true of anomalously low numbers. Using anomaly detection can help us clean our data as a preprocessing step.   
• If computing equipment is cheap, we’d rather just buy more even to accommo‐ date these anomalies. If we can label anomalies, that is the first step toward gen‐ erating labeled data to attempt to predict these anomalies. However, by definition anomalies are difficult to predict, so you should not have high expecta‐ tions of such efforts most of the time!

There are many parameters you can use with Twitter’s automated anomaly detection, and it’s a good tool to have available when you’re exploring a new data set both for cleaning and modeling data.2

# Other Time Series Packages

In this chapter, we have largely focused on widely used packages developed by some of the largest tech companies in conjunction with the enormous data sets and related forecasts these companies produce as part of their core business operations. How‐ ever, these companies are far from being the main or most senior providers of time series packages. There is an enormous ecosystem of time series packages dedicated to:

• Time series storage and infrastructure   
• Time series data sets   
• Breakpoint detection   
• Forecasting   
• Frequency domain analysis3   
• Nonlinear time series   
• Automated time series forecasting

This is not an exhaustive list. There are packages for everything, literally dozens or even hundreds of time series packages. The most extensive listing of open source packages is maintained by Professor Rob Hyndman on R’s official CRAN repository web page. It is worth taking a look at this listing, both to find specific packages that might fit your analytical needs for a particular project, but also more generally to educate yourself about the range of time series analysis methods available and being actively deployed in the community. Much like this book, that page offers an over‐ view and delineation of the tasks associated with time series data. In Python, there is no similarly extensive and consolidated listing of time series modules, but data scien‐ tist Max Christ has compiled a very helpful listing of them.

# More Resources

StatsNewbie123, “Is it Possible to Automate Time Series Forecasting?” post on Cross Validated, December 6, 2019, https://perma.cc/E3C4-RL4L.

This recent StackExchange post asks whether it’s possible to automate time series forecasting for any time series. There are two very useful and detailed responses with an overview of automated packages as well as a discussion of all the chal‐ lenges inherent in such a task.

CausalImpact, Google’s open source package for causal inference, https://perma.cc/ Y72Z-2SFD.

This open source Google package is built on top of another Google release, bsts, the Bayesian Structural Time Series package, which we used in Chapter 7. The CausalImpact package uses bsts to fit models and then, once fit, to construct counterfactual control examples to assess causality and effect size in time series data. The package’s GitHub repo contains links to relevant research papers and a helpful video overview by one of the package’s creators. It’s also worth checking out the associated research paper.

Murray Stokely, Farzan Rohani, and Eric Tassone, “Large-Scale Parallel Statistical Forecasting Computations in R,” in JSM Proceedings, Section on Physical and Engi‐ neering Sciences (Alexandria, VA: American Statistical Association, 2011), https:// perma.cc/25D2-RVVA.

This document offers a high-level, detailed explanation of how R packages were written for Google’s in-house time series forecasting to do time series in a highly parallel and scalable way. This is good reading not just for the details related to forecasting, but also for better understanding how to build time series data pipe‐ lines for large organizations with large quantities and varying types of time series data.

Danilo Poccia, “Amazon Forecast: Time Series Forecasting Made Easy,” AWS News Blog, November 28, 2018, https://perma.cc/Y2PE-EUDV.

One recent automated time series model we didn’t cover is Amazon’s new fore‐ casting service, Amazon Forecast. It’s not open source, but there are many prom‐ ising reviews. It offers a way to use the models Amazon developed with its retail expertise to help companies make business forecasts. While it’s a paid service, you can try the Free Tier, which offers fairly generous options. The service was designed to emphasize both accuracy and usability, and it is a good alternative for those organizations looking for a “good enough” model for high-volume forecasting situations. Amazon’s package uses a mixed approach of deep learning and traditional statistical models, similar to how the simple LSTNET model from Chapter 10 combined a deep learning model with an autoregressive component. It’s worth reading about Amazon’s signature neural network architecture for forecasting, DeepAR.

# Forecasts About Forecasting

There are many good quotes about the hopelessness of predicting the future, and yet I can’t help wanting to conclude this book with some thoughts about what’s coming.

# Forecasting as a Service

Because time series forecasting has fewer expert practitioners than other areas of data science, there has been a drive to develop time series analysis and forecasting as a ser‐ vice that can be easily packaged and rolled out in an efficient way. For example, and as noted in Chapter 16, Amazon recently rolled out a time series prediction service, and it’s not the only company to do so. The company’s model seems deliberately gen‐ eral, and it frames forecasting as just one step in a data pipeline (see Figure 17-1).

These forecasting-as-a-service modeling endeavors aim for a good enough general model that can accommodate a variety of fields without making terribly inaccurate forecasts. Most of them describe their models as using a mix of deep learning and tra‐ ditional statistical models. However, because the service is ultimately a black box, it will be difficult to understand what could make forecasts go wrong or even to retro‐ spectively investigate how they can be improved. This means there is a reasonably high quality level for the forecasts but probably a performance ceiling as well.

This service can be valuable for companies that need many forecasts but do not have the personnel available to generate them individually. However, for companies that have substantial amounts of historical data where more general heuristics and “laws” of their data could be discovered, it’s likely that an analyst could outperform these algorithms given familiarity with the domain.

![](images/bf344ca287e3a5cc3c601b374ca94e81942f11a687ce44429725867f1e035e98.jpg)  
Figure 17-1. An example time series forecasting-as-a-service pipeline. Such services are being offered by a variety of small startups and tech behemoths, most notably Amazon, which has rolled out a suite of products tailored to forecasting time series data auto‐ matically and at scale via a suite of products driven by both deep learning and statisti‐ cal models.

Notice that a fair amount of the product that is being sold in the area of forecastingas-a-service also has to do with good visualizations of the forecasts and pipeline utilit‐ ies to easily revise forecasts, change the forecasting frequency, and so on. Even if your organization will ultimately build out its own forecasting analyses, it can be helpful to see what is emerging as an industry standard.

# Deep Learning Enhances Probabilistic Possibilities

In the past few years, many of the largest tech companies have made some informa‐ tion public about how they do their own forecasting for their most important serv‐ ices. In this case we are not talking about the need for many parallel forecasts of the large number of metrics that affect the company’s business, but rather core concerns. In these cases, where the quality of the forecast is paramount, companies are often giving indications that they are using deep learning with a probabilistic component.

For example, Uber has blogged about forecasting demand for car rides, and Amazon has developed a well-regarded autoregressive recurrent neural network, inspired by statistical thinking for making predictions about product demands. The more that researchers are able to integrate statistical methodologies, such as the injection of pri‐ ors for domain knowledge and the quantification of uncertainty, the less reason there

will be to seek out statistical models when a deep learning model can offer the strengths of both statistics and deep learning.

However, making reasonably interpretable deep learning models—so that we can know just how “wrong” or extreme a forecast can be—remains a difficult endeavor, so it’s unlikely that traditional statistical models, with greater theoretical understand‐ ing and mechanistic clarity, will be discarded. For critical forecasts, where health and safety may be at risk, people reasonably may continue to rely on what has worked for decades until more transparent and inspectable methods can be developed for machine learning forecasts.

# Increasing Importance of Machine Learning Rather Than Statistics

Empirically there seems to be less and less use of proper statistics in modeling data and generating predictions. Do not despair: the field of statistics continues to thrive and answer interesting questions that are related to statistics. And yet—particularly for low-stakes forecasts that merely need to be good enough—machine learning tech‐ niques and results-oriented statistical methods, rather than fancy theories and closedform solutions or proofs of convergence, are winning out in actual deployment and real-world use cases.

From a practitioner’s perspective, this is a good thing. If you happily left your prob‐ lem sets behind long ago with no desire to prove things, you needn’t fear a return of proper proofs and the like. On the other hand, this is a worrying trend as these tech‐ nologies make their way into more and more fundamental aspects of life. I don’t mind surfing a retailer’s website that uses machine learning to guesstimate my likely future actions as a buyer. But I’d like to know that the time series predictions that modeled my health outcomes or my child’s academic progressions were more thor‐ ough and were statistically validated, because a biased model could really hurt some‐ one in these core areas.

For now, the leaders in time series thinking for industrial purposes are working in low-stakes areas. For problems such as predicting revenues from an advertising cam‐ paign or a social media product rollout, it’s not important whether the forecasts are fully validated. As more fundamental aspects of the care and feeding of human beings come into the modeling domain, let’s hope that statistics plays a more fundamental role in high-stakes forecasts.

# Increasing Combination of Statistical and Machine Learning Methodologies

A number of indications point toward combining machine learning and statistical methodologies1 rather than simply searching for the “best” method for forecasting. This is an extension of increasing acceptance and use of ensemble methods for fore‐ casting, a phenomenon we have discussed throughout the book.

An example of an extraordinarily robust test with many real-world data sets is the recent M4 competition, a time series competition measuring forecasting accuracy on 100,00 time series data sets, as mentioned briefly in Chapters 2 and 9. The winning entry to this competition combined elements of a statistical model and a neural net‐ work. Likewise, the runner-up incorporated both machine learning and statistics, in this case by using an ensemble of statistical models but then using a gradient boosted tree (XGBoost) to choose the relative weightings of each model in the ensemble. In this example we see two distinctive ways machine learning and statistical approaches can be combined: either as alternative models assembled together (as in the case of the winning entry) or with one method determining how to set the metaparameters of the other method (as in the case of the runner-up). A comprehensive and highly accessible summary of the competition results was subsequently published in the International Journal of Forecasting.

As such combinations gain traction, we will likely see research develop in the area of determining what problems are most amenable to combining statistical and machine learning models, as well as best practices emerging for tuning the performance of these models and selecting architectures. We expect to see the same refinement as in other complex architectures, such as neural networks, whereby standard design para‐ digms emerge over time with known strengths, weaknesses, and training techniques.

# More Forecasts for Everyday Life

More consumer-facing companies, such as mobile health and wellness applications, have rolled out or been asked to roll out, personalized predictions. As people grow more aware of just how much data their applications store about them and others, they are looking to take advantage by getting tailored forecasts for metrics such as health and fitness goals. Similarly, people often go looking for forecasts for every‐ thing from future real estate values (difficult to predict) to the likely arrival dates of migratory bird species.

More products will be explicitly driven by the demand to make forecasts, be it on eso‐ teric subjects or for individual metrics. This means that more forecasting pipelines will be integrated into places where they have not been too likely before, such as mobile applications and websites for casual readers rather than industry specialists. The more common this becomes, the more likely that time series jargon will be part of everyday speech. Hopefully people will also acquire enough time series education to understand the limits and assumptions of forecasts so that they do not rely too much on these products.”

# Index

# A

abstract data type, 131

accuracy, performance versus, 357

ACF (see autocorrelation function)

actuarial tables, 3

additive regression model, 453

agent-based modeling, 134

aggregation

Google's in-house forecasting framework, 451

of government data, 431

AirPassengers data set, 82, 101

2D visualizations, 105-113

3D visualizations, 113-116

smoothing data, 57

Akaike information criterion (AIC), 173

alpha parameter (smoothing factor), 58

AlphaVantage, 404

Amazon time series prediction service, 461, 463

AnomalyDetection (Twitter package), 457-460

AR(p) model, 167

choosing parameters for, 171-176

forecasting with, 176-180

as moving window function, 177

ARCH (autoregressive conditional heteroske‐ dasticity), 201

ARIMA (see autoregressive integrated moving average)

Aristotle, 6

assumptions, in machine learning models, 291

astronomy, 9

attention mechanism, for feed forward neural network, 322-323

attention weights, 322

Augmented Dickey–Fuller (ADF) test, 84

autocorrelation, 91

autocorrelation function (ACF), 91-92

AR(p) model and, 171

of ARIMA residuals, 190-193

MA/AR processes versus, 183

of nonstationary data, 100-101

PACF versus, 94-101

of stationary data, 95

autoencoder, 334

automatic differentiation, 298

autoregressive (AR) model, 12, 166-181

(see also vector autoregression (VAR))

ACF/PACF versus, 183

choosing parameters for AR(p) model,171-176

forecasting many steps into the future, 178-180

forecasting one time step ahead, 176-178

forecasting with an AR(p) process, 176-180

MA process as infinite order AR process, 181

using algebra to understand constraints on AR processes, 166-171

autoregressive integrated moving average

(ARIMA) model, 186-196

about, 186-189

ARMA model and, 187-189

automated model fitting, 194

flu prediction example, 371-377

manual model fitting, 190-194

parameter selection, 189-196

SARIMA, 201

autoregressive moving average (ARMA) model, 187-189

# B

backshift operator, 182

backtesting, 347

backward fill, 46

bagging (bootstrap aggregating), 268

Baum Welch algorithm, 220

Bayesian structural time series (BSTS), 229-235 code for, 230-235 learning resources, 236

bell curve, 171

Benjamini–Yekutieli procedure, 252

bias–variance tradeoff, 165

binary data format computational performance and, 361 NumPy and, 158 R and, 158, 362

blood glucose level prediction, 384-400 data cleaning/exploration, 385-389 feature generation, 251 fitting a model, 396-400 generating features, 390-396

boosting, 267 (see also gradient boosted trees)

bootstrap aggregating (bagging), 268

Box Cox transformation, 86

Box, George, 10

Box–Jenkins method, 10, 190

BreakoutDetection package, 460

BSTS (see Bayesian structural time series )

bsts package, 230-235

business cycles, 8

# C

$\mathrm { C } { + + } ,$ improving performance by using, 364

calendar-related effects, 451

CatBoost, 271

Cauchy distribution, 170

causal convolutions, 327-328

censoring, 445

cesium package, 245-247, 260-264

Chaikin money flow (CMF), 250

classification, 260-272

clustering and, 272

decision tree methods, 264-272

feature selection/generation, 260-264

UEA and UCR Time Series Classification Repository, 21

cleaning data, 40-69

blood glucose level prediction example, 385-389

flu prediction example, 368-371

Google's in-house forecasting framework, 450

handling missing data, 40-52

preventing lookahead, 67

smoothing, 55-63

time zones, 63-67

upsampling and downsampling, 52-55

clustering, 272-287

code for, 285-287

generating features from data, 273-280

hierarchical clustering of normalized fea‐ tures, 285

hierarchical clustering with the DTW dis‐ tance matrix, 286

reducing lag between measurement and forecast, 360

temporally aware distance metrics, 280-285

CNNs (see convolutional neural networks)

cointegration, 103

column-major order, 106

commodities markets, 9

CompEngine, 24

computational performance, 357-365

accuracy versus, 357

alternative high-performance tools, 364

avoiding overlapping data, 359

data storage formats and, 361

employing generator-like paradigm to iter‐ ate through the data set, 359

for financial applications, 422

learning resources, 365

limiting amount of training data, 363

modifying your analysis to suit performance considerations, 362

problems with models built for crosssectional data, 358-360

problems with models that don't precompute, 360

risks with new/complex models, 363

working with tools built for more general use cases, 358-361

conceptual disaggregation, 451

conditional probabilities, 167

context vector, 322

continuous glucose monitor (CGM) dataset

(see blood glucose level prediction)

convolution, defined, 324

convolutional neural networks (CNNs), 324-329

alternative models, 327-329

causal convolutions, 327-328

converting a time series into pictures, 329

learning resources, 342

simple model, 325-327

correlation

cointegration, 103

in a deterministic system, 92

partial autocorrelation function, 94-101

spurious, 102-103

CSV files, performance and, 361

cuDNN interface, 332

custom rolling functions, 91

CVAR (cointegrated vector autoregression)

model, 201

cyclical time series, 63

# D

data

assembling a time series data collection from disparate sources, 27-32

characteristics, 150

cleaning of, 40-69

discarding, 435

found time series construction, 33-35

government data sets, 23

prepared data sets, 18-24

retrofitting a time series data collection from a collection of tables, 26-35

simulating (see simulating time series data)

storing (see storing temporal data)

symbols versus, 295

timestamping difficulties, 35-39

where to find, 18-26

data imputation (see missing data, methods for handling)

data sets (see specific data sets)

data storage formats, computational perfor‐ mance and, 361

(see also storing temporal data)

data.table package, 42, 43

databases

characteristics of time series data, 150

general-use NoSQL databases, 154-156

InfluxDB, 153

learning resources, 160

popular time series database and file solu‐ tions, 152-156

Prometheus, 154

SQL versus NoSQL, 148-156

temporal data storage with, 148-156

time-series-specific databases and related monitoring tools, 152-154

daylight savings time, 66

decision trees, 264-272

classification versus regression, 271

code example, 268-271

gradient boosted trees, 267

random forests, 265

decomposition, 62

deep learning, 289-342

assumptions, 291

automatic differentiation, 298

basics, 289-292

CNNs, 324-329

combination architectures, 335-339

concepts, 292-294

data, symbols, operations, layers, and graphs, 294-298

enhancement of probabilistic possibilities, 464

feed forward neural network, 318-323

learning resources, 340-342

machine learning versus, 290

multitask learning applied to time series, 354

popular frameworks, 295

preprocessing financial data for, 410-416

RNNs, 329-334

softmax function, 322

time series data simulations, 141

for time series, 289-342

training pipeline for, 298-318

demography, origins of, 3

Dennis, Richard, 9

depmixS4 package, 224-229

diabetes (see blood glucose level prediction)

Dickey–Fuller test, 84

differencing, 186

differentiation, automatic, 298

dilated causal convolution, 327

diluting the forecasting task, 414

disaggregation, 451

discarding data, 435

distance

learning resources for, 287

similarity versus, 283

distance metrics

for clustering, 273

temporally aware, 280-285

distribution, defined, 171

domain knowledge, feature generation and, 242

domain-specific features, 249-251, 257

downsampling, 52-53

defined, 52

to focus on portion of seasonal cycle, 52

to match other low-frequency data, 53

when original resolution of data isn't sensi‐ ble, 52

dunder methods (Python), 131

dynamic harmonic regression, 377-383

dynamic programming, 223

dynamic time warping (DTW), 280-284, 286

#

economic forecasting, 7-9

(see also financial applications)

feature generation for technical stock mar‐ ket indicators, 249

government data sets and, 7

trading markets, 9

edit distance, 283

electricity use data set, 298-318, 332-333

electrocardiograms (EKG) data, 4, 250-251

electroencephalograms (EEG) data, 4

feature selection/generation, 260-264

visualization, 262

embarrassingly parallel data processing tasks, 403

endogenous variables, 197

ensemble methods, 13, 451

ergodic features, 264

ergodic time series, 242

error measurement, 343-356

determining adequacy of forecasts, 348

estimating uncertainty in a model with a simulation, 350-353

fitting forecast directly to horizon of inter‐ est, 353

learning resources, 355

lookahead, 355

model validation gotchas, 355

model-specific considerations for backtest‐ ing, 347

multitask learning applied to time series, 354

predicting multiple steps ahead, 353

recursive approach to distant temporal hori‐ zons, 354

structural changes to model, 355

testing forecasts, 344-348

estimates, unbiased estimators and, 165

Euclidean distance, 284

exogenous harmonic regressors, 377-383

exogenous variables, 197

expanding windows, 89

exploding gradients, 331

exploratory data analysis, 73-118

1D visualizations (Gantt charts), 104

2D visualizations, 105-113

3D visualizations, 113-116

applying window functions, 86-91

group-by operations, 74

histograms, 77

learning resources, 117

non-time-series processes, 73-81

plotting, 74-77

scatter plots, 78-81

self-correlation, 91-95

spurious correlations, 102-103

stationarity and, 82-86

time-series-specific exploratory methods, 81-103

useful visualizations, 104-116

exponential smoothing, 56-59

Extended Kalman Filter (EKF), 217

#

Facebook Prophet package, 452-455

feather format, 158

feature generation, 239-251

blood glucose level prediction example, 390-396

clustering and, 273-280

domain knowledge, 242

domain-specific examples, 249-251

ergodic time series, 242

external considerations, 243

general considerations when computing features, 241

healthcare time series, 250-251

introductory example, 240

length of time series, 242

manual coding for, 264

nature of the times series, 242

open source time series feature generation libraries, 244-249

periodograms and, 246

purpose of, 239

sources for, 243-251

stationarity, 242

technical stock market indicators, 249

time series classification, 260-264

for time series, 239-258

tsfeatures (R package), 248

tsfresh module, 244

feature generation libraries, 244-249

Cesium time series analysis platform, 245-247

tsfresh Python module, 244

feature selection, 252-255

ergodic features, 264

learning resources, 256

recursive feature elimination, 254

time series classification, 260-264

features

generating/selecting for time series, 239-258

hierarchical clustering of normalized fea‐ tures, 285

feed forward neural network, 318-323

attention mechanism for, 322-323

example, 319-321

RNN versus, 330

Fibonacci sequence, 223

FiftyWords data set, 111

FiftyWords dataset, 274-280

files, for temporal data storage

NumPy arrays, 158

Pandas data frame, 158

standard R equivalents, 158

for storing temporal data, 157-160

Xarray, 159

filtering, 208

financial applications, 403-423

adding quantities of interest to raw values, 410

building/training an RNN, 416-423

diluting the forecasting task, 414

economic forecasting, 7-9, 249

financial markets as random walk, 408-409

formatting data for a deep network, 413-416

obtaining/exploring data, 404-409

performance considerations, 422

preprocessing data for deep learning, 410-416

scaling quantities of interest without a look‐ ahead, 411-413

using timescale for inputs similar to time‐ scale for targets, 413

financial time series, as separate discipline, 410

first-in-first-out (FIFO) data type, 131

FitzRoy, Robert, 6

flat files, 157-160

flu prediction, 367-384

ARIMA model using exogenous harmonic regressors, 377-383

case study in one metropolitan area, 367

data exploration/cleanup, 368-371

fitting a seasonal ARIMA model, 371-377

research in flu forecasts, 383

state-of-the art approaches, 383

forecasting

at scale, 449-455, 450-452

clustering and, 272

defined, 208

diluting the forecasting task, 414

future of time series analysis, 463-467

measuring errors in (see error measure‐ ment)

predicting multiple steps ahead, 353

simulation versus, 121

forecasting-as-a-service, 463

forecasts

determining adequacy of, 348

fitting directly to horizon of interest, 353

multitask learning applied to time series, 354

recursive approach to distant temporal hori‐ zons, 354

testing, 344-348

forward fill, 44-46

found time series, 25

constructing, 33-35

in government data sets, 24

Fourier series, 378

Fréchet distance, 282

FRESH algorithm, 252-255

future of time series analysis, 463-467

consumer-facing forecasting, 466

deep learning enhancement of probabilistic possibilities, 464

forecasting-as-a-service, 463

increasing combinations of statistical and machine learning methodologies, 466

increasing importance of machine learning over statistics, 465

# G

GARCH (generalized autoregressive condi‐ tional heteroskedasticity) model, 201

gated recurrent unit (GRU), 330-332

general-use NoSQL databases, 154-156

Generalized ESD, 457

generators, 128-134, 359

geographic disaggregation, 451

geometric mean, 47

Google in-house framework for large-scale stat‐ istical forecasting computations in R, 450-452

government data sets, 23

economic forecasting and, 7

exploring big time series data, 428-436

found time series in, 24

learning resources, 447

obtaining, 426

online statistical analysis of time series data, 436-447

preparing a data set to test methodologies, 41-44

problems with, 425

sorting, 432-436

upsampling/aggregating data when iterating through it, 431

governmental applications, 425-448

discarding data, 435

exploring big time series data, 428-436

learning resources, 447

obtaining governmental data, 426

online statistical analysis of time series data, 436-447

problems with government data sets, 425

sorting data, 432-436

upsampling/aggregating data when iterating through it, 431

gradient boosted trees, 267, 271

graphs, deep learning and, 289, 296

Graunt, John, 2

grid search, 56

group-by operations, 74

Guassian (normal) distribution, 171

# H

health, population, 2-4

healthcare applications, 367-401

blood glucose level prediction, 384-400

causes of missing data in, 40

feature generation, 250-251

feature selection/generation for EEG data, 260-264

flu prediction, 367-384

learning resources, 401

heat maps, 111

Hidden Markov Models (HMMs), 218-229

Baum Welch algorithm and, 220

coding for, 224-229

fitting the model, 220-222

how the model works, 218

inference problems, 229

learning resources, 236

Viterbi algorithm and, 222

hidden state, 322, 323, 334

hierarchical clustering

of normalized features, 285

with the DTW distance matrix, 286

hierarchical time series models, 201

high-frequency data, 147

highly comparative time-series analysis (hctsa), 24

histograms, 77, 262

history of time series analysis astronomy, 9

economic forecasting, 7-9

learning resources, 13

medicine as time series problem, 2-6

origins of machine learning time series analysis, 13

post-1960s developments, 10

weather forecasting, 6

Holt's method, 58

Holt–Winters smoothing, 58

horizon of interest, 353

hyperparameters, 56

#

iid (independently and identically distributed) data, 163, 182

imperative programming style, 294

imputation, 40

(see also missing data, methods for han‐ dling)

comparison of methodologies, 50

health care data, 393

moving average for, 46-48

preparing a data set to test methodologies, 41-44

using data set's mean for, 48

influenza (see flu prediction)

InfluxDB, 153

informed prediction, 185

interpolation, 40, 49

interval censoring, 445

Ising model, 134, 136

iterators, for training pipeline, 308

#

Java, improving computational performance by using, 364

#

Kalman filter, 58, 210-217

code for, 212-217

learning resources, 235

mathematics overview, 210-212

kernel density estimate, 262

Kwiatkowski–Phillips–Schmidt– Shin (KPSS) test, 85

#

lag (backshift) operator, 182

lag(), 81

layers, in deep learning, 296

left censoring, 445

legal issues, in data storage, 148

life (actuarial) tables, 3

LightGBM, 271

linear Gaussian state space models, 210 (see also Kalman filter)

linear interpolation, 49

moving average versus, 50

when not to use, 50

when to use, 50

linear regression, 163

Linux command-line sort tool, 432

Ljung–Box test, 176, 184

local time, UTC time versus, 37

LOESS (locally estimated scatter plot smooth‐ ing), 58, 62

Lomb–Scargle periodogram, 247

long short-term memory (LSTM), 331

longest common subsequence, 283

lookahead

backward fill and, 46

defined, 28

model validation and, 355

preventing, 67

#

M4 competition, 466

MA(q) process

forecasting, 184-186

parameter selection for, 182-184

machine learning, 290

(see also deep learning)

assumptions, 291

clustering, 272-287

deep learning versus, 290

future combinations of statistical methodol‐ ogies with, 466

learning resources for, 287

statistics versus, 465

time series analysis origins, 13

time series classification, 260-272

for time series, 259-288

visualization and, 262

magnet, modeling of, 134-140

Markov Chain Monte Carlo (MCMC) simula‐ tion, 135, 232

Markov process, 218

(see also Hidden Markov Models (HMMs))

matrix multiplication, 197

Mcomp, 24

mechanical trading, 9

medicine, 2

(see also healthcare applications)

medical instruments, 4-6

population health, 2-4

as time series problem, 2

memoization, 223

meteorology (see weather forecasting)

missing data, methods for handling, 40-52

backward fill, 46

data.table package in R, 42

forward fill, 44-46

imputation methodologies compared, 50

interpolation, 49

learning resources, 69

moving average, 46-48

preparing a data set to test imputation methodologies, 41-44

model validation

error testing issues, 355

lookahead, 355

structural changes, 355

modified LSTNet, 335

MongoDB, 155

Monte Carol simulation

estimating uncertainty with, 350

Markov Chain Monte Carlo (MCMC) sim‐ ulation, 135

Moore's Law, 11

moving average, 46-48

alternate forms of, 47

linear interpolation versus, 50

MA models versus, 181

for smoothing, 55

moving average (MA) models, 181-186

ACF/PACF versus, 183

as infinite order AR process, 181

moving averages versus, 181

parameter selection for MA(q) process, 182-184

moving average convergence/divergence

(MACD), 249

multitask learning, 354

multivariate time series, 21

mxnet package, 295-298, 314

# N

NC data format, 305, 310

negative values, preventing, 377

neural networks, 291

(see also deep learning)

assumptions, 291

CNNs, 324-329

combination architectures, 335-339

feed forward network, 318-323

learning resources, 340-342

programming, 294-298

RNNs, 329-334

symbolic and imperative programming styles, 294

New York City, 311 call data set, 425-448

Newton's laws of motion, 212-217

noisy data, 147

nonprofit organizations, email/donation statis‐ tics, 122-128

normal (Guassian) distribution, 171

normalized exponential (softmax) function, 322

NoSQL databases

choosing between SQL and NoSQL, 151

general-use, 154-156

SQL versus, 148-156

time series data characteristics, 150

NTC data format, 307, 325

producing in code, 311

TNC versus, 307

NumPy arrays, 158

advantages, 158

disadvantages, 158

as row major, 106

# O

online statistical analysis, 436-447

outliers, 349

(see also AnomalyDetection)

overfitting

backtesting and, 345

random forests and, 266

# P

P-square algorithm, 436-447

packages (see time series packages)

Pandas, 31, 158

partial autocorrelation function (PACF), 94-101

AR(p) model and, 171-176

of ARIMA residuals, 190-193

MA/AR processes versus, 183

of nonstationary data, 100-101

Pearson correlation, 283

perceptron, 292-294

performance (see computational performance)

periodograms, 246, 258

physical system, simulation of, 134-140

pickling, 64, 362

pipeline, for deep learning training, 298-318

plotting, 74-77

population health, 2-4

Portmanteau test, 200

prepared data sets, 18-24

CompEngine, 24

government data sets, 23

Mcomp, 24

UCI Machine Learning Repository, 18-21

UEA and UCR Time Series Classification Repository, 21

univariate versus multivariate time series, 21

preprocessing, 362

(see also specific procedures, e.g., smooth‐ ing)

of financial data for deep learning, 410-416

to improve computation performance, 362

privacy, deep learning and, 141

probability, 167

Prometheus, 154, 161

PromQL, 154

Prophet (Facebook package), 452-455

psychological distance, time discounting and, 39

pull-based systems, 154

push-based systems, 153

Python

and binary data format, 158, 362

dunder methods, 131

P-square algorithm, 437

rolling split with, 346

smoothing in, 57

time zones in, 63-67

tsfresh module, 244

writing code that gets compiled into $_ { \mathrm { C / C + + } }$ 364

Xarray, 159

# R

and binary data format, 158, 362

as column major, 106

data.table package, 42, 43

Google's in-house framework for large-scale statistical forecasting computations in R, 450-452

lag(), 81

native formats for storing objects, 158

rolling split with, 346

tsfeatures package, 248

writing code that gets compiled into $_ { \mathrm { C / C + + } }$ 365

xts objects, 89

zoo objects, 89

random forests, 265

random numbers, 133, 135

random walk, financial markets as, 408-409

recurrent neural networks (RNNs), 329-334

autoencoders and, 334

building/training for financial applications, 416-423

electric forecasting example, 332-333

learning resources, 341

precomputing for, 360

recursive feature elimination (RFE), 254

regime changes, 451

relative strength index (RSI), 249

Renaissance, 6

right censoring, 445

ring buffer, 314

RNNs (see recurrent neural networks)

rolling join, 43, 54

rolling mean, 46-48

rolling mean smoothing, 55, 87

rolling windows, 86-88, 346

row-major order, 106

#

S&P 500 stock price data, 403-423

scatter plots, 78-81

seasonal ARIMA (SARIMA), 201

flu prediction example, 371-377

Seasonal Hybrid ESD, 457

seasonal time series, 63

seasonality

downsampling and, 52

and flu prediction, 370

Google's in-house forecasting framework, 451

human behavior and, 61

learning resources, 70

smoothing of, 60-63

seed, setting a, 133

self-correlation, 82, 91-95

autocorrelation function, 91-92

partial autocorrelation function, 94-101

seq2seq model, 334

(see also autoencoder)

serial correlation (autocorrelation), 91

sharded data, 146

similarity

distance versus, 283

Euclidean distance as poor measure of, 284

learning resources for, 287

simulating time series data, 119-142

agent-based modeling, 134

building a simulation universe that takes care of itself, 128-134

code examples, 121-140

forecasting versus, 121

learning resources, 142

manual coding, 122-128

physics simulation, 134-140

problems unique to, 120

statistical simulations, 141

smoothing, 55-63

defined, 208

exponential, 56-59

Google's in-house forecasting framework, 450

hyperparameters and, 56

in Python, 57

Kalman filter for, 217

learning resources, 70

lookahead and, 345

purposes of, 55

starting, 59

smoothing factor (alpha parameter), 58

softmax function, 322

sorting, 432-436

spline interpretation, 50

(see also linear interpolation)

spurious correlations, 102-103

cointegration versus, 103

learning resources, 117

SQL databases

characteristics of time series data, 150

choosing between SQL and NoSQL, 151

data characteristics of early SQL databases, 150

found time series and, 26

NoSQL versus, 148-156

stale data, 148

state space models, 207-237

advantages and disadvantages, 209

Bayesian structural time series, 229-235

Hidden Markov Models, 218-229

Kalman filter, 210-217

learning resources, 235

state variables, storing, 147

stationarity

and Augmented Dickey–Fuller test, 84

defined, 81

ergodic time series versus, 242

exploratory data analysis and, 82-86

feature selection and, 242

in practice, 85

and Kwiatkowski-Phillips-Schmidt-Shin test, 85

seasonal and cyclical data, 63

weak versus strong, 170

stationary process, defined, 83

statistical models for time series, 163-205

advantages and disadvantages, 203

ARCH, 201

ARIMA, 186-196

autoregressive models, 166-181

GARCH, 201

hierarchical time series models, 201

learning resources, 204

linear regression's problems/limitations, 163

models developed for time series, 166-202

moving average models, 181-186

seasonal ARIMA (SARIMA), 201

unbiased estimators, 165

vector autoregression, 196-201

weak versus strong stationarity, 170

ǀ symbol and, 167

statistical time series analysis, 12

statistical time series data simulations, 141

statistics

combinations of machine learning method‐ ologies with, 466

machine learning versus, 465

stock market

economic forecasting, 9

feature generation for technical stock mar‐ ket indicators, 249

storing temporal data, 143-162

avoiding bottlenecks, 361

binary format and computational perfor‐ mance, 361

computational performance and storage formats, 361

database solutions, 148-156

defining requirements for, 145-148

file solutions, 157-160

learning resources, 160

legal considerations, 148

live data versus stored data, 146-148

noisy, high-frequency data, 147

NumPy arrays, 158

Pandas data frame, 158

popular time series database and file solu‐ tions, 152-156

preprocessing and, 362

slowly changing variables, 147

stale data, 148

standard R equivalents, 158

use cases, 143

Xarray, 159

strong stationarity, 170

subseries, downsampling to create, 52

summary statistics

expanding windows when estimating, 89

Kalman filters and, 217

Symbolic Aggregate Approximation (SAX), 285

symbolic programming style, 294

symbols, in deep learning, 295

# T

tables, retrofitting time series data collection from, 26-35

taxicab simulation, 128-134

temporal aggregation, 451

temporal data, storing (see storing temporal data)

temporal resolution, of timestamps, 39

temporal sensitivity analysis, 349

TensorFlow package, 417

text data format, computational performance and, 361

time discounting, 39

time series (generally)

converting into pictures, 329

statistical models for, 163-205

univariate versus multivariate, 21

time series analysis (generally)

astronomy, 9

brief history of, 2-10

defined, 1

economic forecasting, 7-9

medicine as time series problem, 2-6

origins of machine learning time series analysis, 13

origins of statistical time series analysis, 12

post-1960s developments, 10

weather forecasting, 6

time series data (see data)

time series packages, 449-461

anomaly detection, 457-460

ecosystem of, 460

Facebook's Prophet package, 452-455

forecasting at scale, 449-455

Google's in-house framework for large-scale statistical forecasting computations in R, 450-452

learning resources, 461

time zones, 63-67

daylight savings, 66

learning resources, 69

local time versus universal time, 37-38

time, human response to passage of, 39

time-series-specific databases, 152-154

InfluxDB, 153

Prometheus, 154

timestamps/timestamping

difficulties with, 35-39

discrepancies between time of event and time of recording, 35

and found time series, 25

guesstimating to make sense of data, 36-38

local time versus universal time, 37, 38

rolling join in data.table package, 43

temporal resolution of, 39

test data and, 348

time zones and, 63-67

upsampling and downsampling, 52-55

user behavior versus network behavior, 38

TNC data format, 307

traces, 26

trading markets (see stock market)

training data, limiting amount of, 363

training pipeline

assembling pipeline components, 317

building iterators, 308

for deep learning, 298-318

evaluation metrics, 316

inspecting the data set, 299-302

making code easily configurable, 303-305

NC data format, 305, 310

NTC data format, 307, 311

preparing input data, 305-309

setting training parameters and establishing a recordkeeping system, 312-316

shaping data in code, 309-312

shaping input data, 305-308

steps of, 303-318

transactional data, 150

transformations

assumptions with, 86

making time series stationary with, 85

ts object, 89

tsfeatures (R package), 248

tsfresh (Python module), 244, 252-255

Twitter (AnomalyDetection package), 457-460

# U

UCI Machine Learning Repository, 18-21

UEA and UCR Time Series Classification

Repository, 21

unbiased estimators, 165

uncertainty, estimating, 350-353

unemployment data, 41-44

unit root, 83

univariate time series, 21

universal (UTC) time, 37

upsampling, 53

for aligning inputs in model, 54

converting irregularly sampled series to reg‐ ularly timed series, 54

defined, 52

of government data, 431

interpolation based on variable's usual behavior, 55

# V

vanishing gradients, 331

variables, exogenous versus endogenous, 197

vector autoregression (VAR), 196-201

visualizations

1D (Gantt charts), 104

2D, 105-113

3D, 113-116

converting a time series into pictures, 329

exploratory data analysis, 104-116

learning resources, 118

machine learning and, 262

Viterbi algorithm, 222

volatility, 410

# W

weak stationarity, 170

weather forecasting, 6

window functions

custom rolling functions, 91

expanding windows, 89

exploratory data analysis and, 86-91

rolling windows, 86-88

Wold's theorem, 187

# X

Xarray, 159

XGBoost, 267-272, 397, 466

xts objects, 89

# Y

Yahoo Finance, 404

Yule, Udny, 12, 102

# Z

zoo objects, 88

# About the Author

Aileen Nielsen is an New York City–based software engineer and data analyst. She has worked on time series and other data in a variety of disciplines, from a healthcare startup to a political campaign, from a physics research lab to a financial trading firm. She currently develops neural networks for forecasting applications.

# Colophon

The animal on the cover of Practical Time Series Analysis is a Bluefaced Leicester sheep (Ovis aries). The Bluefaced Leicester is a British breed also known as the Dish‐ ley Leicester; it was first bred in Dishley, Leicestershire, in the eighteenth century. Today, farmers throughout the UK and Canada breed this specialized strain of sheep.

Bluefaced Leicester sheep have white wool that makes a fine, shiny fleece and quality handspun yarn. Their brown eyes are prominent against their blue-gray faces. The sheep are hardy with long backs, strong shoulders and necks, broad muzzles, and dark hooves. They have no wool on their legs or faces.

When full-grown, a Bluefaced Leicester can weigh between 200 and 240 pounds. The average height from hoof to shoulder is 36 inches for rams and 33 inches for ewes. Like most sheep, the Bluefaced Leicester graze on grasses, weeds, and flowers. They graze in flocks of 20 ewes, on average.

Bluefaced Leicester sheep are highly sought after because the rams have a knack for producing what the industry call mules. Mules are ewes with high reproduction rates that breeders cross with other breeds to create lambs for the market.

Many of the animals on O’Reilly covers are endangered; all of them are important to the world.

The cover illustration is by Karen Montgomery, based on a black-and-white engrav‐ ing from Meyers Kleines Lexicon. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Con‐ densed; and the code font is Dalton Maag’s Ubuntu Mono.

# O'REILLY?

# There’s much more where this came from.

Experience books, videos, live online training courses, and more from O’Reilly and our 200+ partners—all in one place.

Learn more at oreilly.com/online-learning