# Springer Texts in Statistics

Advisors:

George Casella Stephen Fienberg Ingram Olkin

Alfred: Elements of Statistics for the Life and Social Sciences

Berger: An Introduction to Probability and Stochastic Processes

Bilodeau and Brenner: Theory of Multivariate Statistics

Blom: Probability and Statistics: Theory and Applications

Brockwell and Davis: Introduction to Times Series and Forecasting, Second Edition

Carmona: Statistical Analysis of Financial Data in S-Plus

Chow and Teicher: Probability Theory: Independence, Interchangeability, Martingales, Third Edition

Christensen: Advanced Linear Modeling: Multivariate, Time Series, and Spatial Data—Nonparametric Regression and Response Surface Maximization, Second Edition

Christensen: Log-Linear Models and Logistic Regression, Second Edition

Christensen: Plane Answers to Complex Questions: The Theory of Linear Models, Third Edition

Creighton: A First Course in Probability Models and Statistical Inference

Davis: Statistical Methods for the Analysis of Repeated Measurements

Dean and Voss: Design and Analysis of Experiments

du Toit, Steyn, and Stumpf: Graphical Exploratory Data Analysis

Durrett: Essentials of Stochastic Processes

Edwards: Introduction to Graphical Modelling, Second Edition

Finkelstein and Levin: Statistics for Lawyers

Flury: A First Course in Multivariate Statistics

Gut: Probability: A Graduate Course

Heiberger and Holland: Statistical Analysis and Data Display:

An Intermediate Course with Examples in S-PLUS, R, and SAS

Jobson: Applied Multivariate Data Analysis, Volume I: Regression and Experimental Design

Jobson: Applied Multivariate Data Analysis, Volume II: Categorical and Multivariate Methods

Kalbfleisch: Probability and Statistical Inference, Volume I: Probability, Second Edition

Kalbfleisch: Probability and Statistical Inference, Volume II: Statistical Inference, Second Edition

Karr: Probability

Keyfitz: Applied Mathematical Demography, Second Edition

Kiefer: Introduction to Statistical Inference

Kokoska and Nevison: Statistical Tables and Formulae

Kulkarni: Modeling, Analysis, Design, and Control of Stochastic Systems

Lange: Applied Probability

Lange: Optimization

Lehmann: Elements of Large-Sample Theory

Robert H. Shumway

David S. Stoffer

# Time Series Analysis and Its Applications

With R Examples

Second Edition

With 160 Illustrations

Robert H. Shumway

Department of Statistics

University of California, Davis

Davis, CA 95616

USA

rshumway@ucdavis.edu

or

shumway@wald.ucdavis.edu

David S. Stoffer

Department of Statistics

University of Pittsburgh

Pittsburgh, PA 15260

USA

stoffer@pitt.edu

Editorial Board

George Casella

Department of Statistics

University of Florida

Gainesville, FL 32611-8545

USA

Stephen Fienberg

Department of Statistics

Carnegie Mellon University

Pittsburgh, PA 15213-3890

USA

Ingram Olkin

Department of Statistics

Stanford University

Stanford, CA 94305

USA

Library of Congress Control Number: 2005935284

ISBN-10: 0-387-29317-5

ISBN-13: 978-0387-29317-2

Printed on acid-free paper.

$\textcircled { \mathrm { C } } 2 0 0 6$ Springer Science+Business Media, LLC

All rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excepts in connection with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.

The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.

Printed in the United States of America. (MVY)

9 8 7 6 5 4 3 2 1

springer.com

To my wife, Ruth, for her support and joie de vivre, and to the memory of my thesis adviser, Solomon Kullback.

R.H.S.

To my family, who constantly remind me what is important.

D.S.S.

# Preface to the Second Edition

The second edition marks a substantial change to the first edition. Perhaps the most significant change is the introduction of examples based on the freeware R package. The package, which runs on most operating systems, can be downloaded from The Comprehensive R Archive Network (CRAN) at http://cran.r-project.org/ or any one of its mirrors. Readers who have experience with the S-PLUS $\textsuperscript { \textregistered }$ package will have no problem working with R. For novices, R installs some help manuals, and CRAN supplies links to contributed tutorials such as $R$ for Beginners. In our examples, we assume the reader has downloaded and installed R and has downloaded the necessary data files. The data files can be downloaded from the website for the text, http://www.stat.pitt.edu/stoffer/tsa2/ or any one of its mirrors. We will also provide additional code and other information of interest on the text’s website. Most of the material that would be given in an introductory course on time series analysis has associated R code. Although examples are given in R, the material is not R-dependent. In courses we have given using a preliminary version of the new edition of the text, students were allowed to use any package of preference. Although most students used R (or S-PLUS), a number of them completed the course successfully using other programs such as ASTSA, MATLAB $\textsuperscript { \textregistered }$ , SAS $\textsuperscript { \textregistered }$ , and SPSS $\textsuperscript { \textregistered }$ .

Another substantial change from the first edition is that the material has been divided into smaller chapters. The introductory material is now contained in the first two chapters. The first chapter discusses the characteristics of time series, introducing the fundamental concepts of time plot, models for dependent data, auto- and cross-correlation, and their estimation. The second chapter provides a background in regression techniques for time series data. This chapter also includes the related topics of smoothing and exploratory data analysis for preprocessing nonstationary series.

In the first edition, we covered ARIMA and other special topics in the time domain in one chapter. In this edition, univariate ARIMA modeling is presented in its own chapter, Chapter 3. The material on additional time domain topics has been expanded, and moved to its own chapter, Chapter 5. The additional topics include long memory models, GARCH processes, threshold models, regression with autocorrelated errors, lagged regression, transfer function modeling, and multivariate ARMAX models. In this edition, we have removed the discussion on reduced rank models and contemporaneous models from the multivariate ARMAX section. The coverage of GARCH models has been considerably expanded in this edition. The coverage of long memory models has been consolidated, presenting time domain and frequency domain approaches in the same section. For this reason, the chapter is presented after the chapter on spectral analysis.

The chapter on spectral analysis and filtering, Chapter 4, has been expanded to include various types of spectral estimators. In particular, kernel based estimators and spectral window estimators have been included in the dis-

cussion. The chapter now includes a section on wavelets that was in another chapter in the first edition. The reader will also notice a change in notation from the previous edition.

In the first edition, topics were supplemented by theoretical sections at the end of the chapter. In this edition, we have put the theoretical topics in appendices at the end of the text. In particular, Appendix A can be used to supplement the material in the first chapter; it covers some fundamental topics in large sample theory for dependent data. The material in Appendix B includes theoretical material that expands the presentation of time domain topics, and this appendix may be used to supplement the coverage of the chapter on time series regression and the chapter on ARIMA models. Finally, Appendix C contains a theoretical basis for spectral analysis.

The remaining two chapters on state-space and dynamic linear models, Chapter 6, and on additional statistical methods in the frequency domain, Chapter 7, are comparable to the their first edition counterparts. We do mention that the section on multivariate ARMAX, which used to be in the statespace chapter, has been moved to Chapter 5. We have also removed spectral domain canonical correlation analysis and the discussion on wavelets (now in Chapter 4) that were previously in Chapter 7. The material on stochastic volatility models, now in Chapter 6, has been expanded. R programs for some Chapter 6 examples are available on the website for the text; these programs include code for the Kalman filter and smoother, maximum likelihood estimation, the EM algorithm, and fitting stochastic volatility models.

In the previous edition, we set off important definitions by highlighting phrases corresponding to the definition. We believe this practice made it difficult for readers to find important information. In this edition, we have set off definitions as numbered definitions that are presented in italics with the concept being defined in bold letters.

We thank John Kimmel, Executive Editor, Statistics, for his guidance in the preparation and production of this edition of the text. We are particularly grateful to Don Percival and Mike Keim at the University of Washington, for numerous suggestions that led to substantial improvement to the presentation. We also thank the many students and other readers who took the time to mention typographical errors and other corrections to the first edition. In particular, we appreciate the efforts of Jeongeun Kim, Sangdae Han, and Mark Gamalo at the University of Pittsburgh, and Joshua Kerr and Bo Zhou at the University of California, for providing comments on portions of the draft of this edition. Finally, we acknowledge the support of the National Science Foundation.

Robert H. Shumway Davis, CA David S. Stoffer Pittsburgh, PA August 2005

# Preface to the First Edition

The goals of this book are to develop an appreciation for the richness and versatility of modern time series analysis as a tool for analyzing data, and still maintain a commitment to theoretical integrity, as exemplified by the seminal works of Brillinger (1981) and Hannan (1970) and the texts by Brockwell and Davis (1991) and Fuller (1995). The advent of more powerful computing, especially in the last three years, has provided both real data and new software that can take one considerably beyond the fitting of simple time domain models, such as have been elegantly described in the landmark work of Box and Jenkins (see Box et al., 1994). This book is designed to be useful as a text for courses in time series on several different levels and as a reference work for practitioners facing the analysis of time-correlated data in the physical, biological, and social sciences.

We believe the book will be useful as a text at both the undergraduate and graduate levels. An undergraduate course can be accessible to students with a background in regression analysis and might include Sections 1.1-1.8, 2.1-2.9, and 3.1-3.8. Similar courses have been taught at the University of California (Berkeley and Davis) in the past using the earlier book on applied time series analysis by Shumway (1988). Such a course is taken by undergraduate students in mathematics, economics, and statistics and attracts graduate students from the agricultural, biological, and environmental sciences. At the master’s degree level, it can be useful to students in mathematics, environmental science, economics, statistics, and engineering by adding Sections 1.9, 2.10-2.14, 3.9, 3.10, 4.1-4.5, to those proposed above. Finally, a two-semester upper-level graduate course for mathematics, statistics and engineering graduate students can be crafted by adding selected theoretical sections from the last sections of Chapters 1, 2, and 3 for mathematics and statistics students and some advanced applications from Chapters 4 and 5. For the upper-level graduate course, we should mention that we are striving for a less rigorous level of coverage than that which is attained by Brockwell and Davis (1991), the classic entry at this level.

A useful feature of the presentation is the inclusion of data illustrating the richness of potential applications to medicine and in the biological, physical, and social sciences. We include data analysis in both the text examples and in the problem sets. All data sets are posted on the World Wide Web at the following URLs: http://www.stat.ucdavis.edu/˜shumway/tsa.html and http://www.stat.pitt.edu/˜stoffer/tsa.html, making them easily accessible to students and general researchers. In addition, an exploratory data analysis program written by McQuarrie and Shumway (1994) can be downloaded (as Freeware) from these websites to provide easy access to all of the techniques required for courses through the master’s level.

Advances in modern computing have made multivariate techniques in the time and frequency domain, anticipated by the theoretical developments in Brillinger (1981) and Hannan (1970), routinely accessible using higher level

languages, such as MATLAB and S-PLUS. Extremely large data sets driven by periodic phenomena, such as the functional magnetic resonance imaging series or the earthquake and explosion data, can now be handled using extensions to time series of classical methods, like multivariate regression, analysis of variance, principal components, factor analysis, and discriminant or cluster analysis. Chapters 4 and 5 illustrate some of the immense potential that methods have for analyzing high-dimensional data sets.

The many practical data sets are the results of collaborations with research workers in the medical, physical, and biological sciences. Some deserve special mention as a result of the pervasive use we have made of them in the text. The predominance of applications in seismology and geophysics is joint work of the first author with Dr. Robert R. Blandford of the Center for Monitoring Research and Dr. Zoltan Der of Ensco, Inc. We have also made extensive use of the El Ni˜no and Recruitment series contributed by Dr. Roy Mendelssohn of the National Marine Fisheries Service. In addition, Professor Nancy Day of the University of Pittsburgh provided the data used in Chapter 4 in a longitudinal analysis of the effects of prenatal smoking on growth, as well as some of the categorical sleep-state data posted on the World Wide Web. A large magnetic imaging data set that was developed during joint research on pain perception with Dr. Elizabeth Disbrow of the University of San Francisco Medical Center forms the basis for illustrating a number of multivariate techniques in Chapter 5. We are especially indebted to Professor Allan D.R. McQuarrie of the University of North Dakota, who incorporated subroutines in Shumway (1988) into ASTSA for Windows.

Finally, we are grateful to John Kimmel, Executive Editor, Statistics, for his patience, enthusiasm, and encouragement in guiding the preparation and production of this book. Three anonymous reviewers made numerous helpful comments, and Dr. Rahman Azari and Dr. Mitchell Watnik of the University of California, Davis, Division of Statistics, read portions of the draft. Any remaining errors are solely our responsibility.

Robert H. Shumway

Davis, CA

David S. Stoffer

Pittsburgh, PA

August 1999

# Contents

# 1 Characteristics of Time Series 1

1.1 Introduction . . 1   
1.2 The Nature of Time Series Data 4   
1.3 Time Series Statistical Models . . . 11   
1.4 Measures of Dependence: Autocorrelation and Cross-Correlation 18   
1.5 Stationary Time Series . . . . 23   
1.6 Estimation of Correlation . . 29   
1.7 Vector-Valued and Multidimensional Series . . 34

Problems 40

# 2 Time Series Regression and Exploratory Data Analysis 48

2.1 Introduction . . 48   
2.2 Classical Regression in the Time Series Context . . . . . . . 49   
2.3 Exploratory Data Analysis . . . . 57   
2.4 Smoothing in the Time Series Context . . . 71

Problems 79

# 3 ARIMA Models 84

3.1 Introduction . . 84   
3.2 Autoregressive Moving Average Models 85   
3.3 Difference Equations . . . 98   
3.4 Autocorrelation and Partial Autocorrelation Functions . . . . . 103   
3.5 Forecasting 110   
3.6 Estimation 122   
3.7 Integrated Models for Nonstationary Data . . . . . . 140   
3.8 Building ARIMA Models . . 143   
3.9 Multiplicative Seasonal ARIMA Models . . 154

Problems 165

# 4 Spectral Analysis and Filtering 174

4.1 Introduction . . 174   
4.2 Cyclical Behavior and Periodicity . . . 176   
4.3 The Spectral Density . . . 181

4.4 Periodogram and Discrete Fourier Transform . . . . . . . . . . 187   
4.5 Nonparametric Spectral Estimation . . . 197   
4.6 Multiple Series and Cross-Spectra . . . . 215   
4.7 Linear Filters . 220   
4.8 Parametric Spectral Estimation . . . 228   
4.9 Dynamic Fourier Analysis and Wavelets . . . 232   
4.10 Lagged Regression Models 245   
4.11 Signal Extraction and Optimum Filtering . . . . 251   
4.12 Spectral Analysis of Multidimensional Series . . . . . . 256

Problems 258

# 5 Additional Time Domain Topics 271

5.1 Introduction . . . 271   
5.2 Long Memory ARMA and Fractional Differencing . . . . . . . 271   
5.3 GARCH Models 280   
5.4 Threshold Models . . . 289   
5.5 Regression with Autocorrelated Errors . . . 293   
5.6 Lagged Regression: Transfer Function Modeling . . . . . . . . . 295   
5.7 Multivariate ARMAX Models . . . 302

Problems 320

# 6 State-Space Models 324

6.1 Introduction . 324   
6.2 Filtering, Smoothing, and Forecasting . . 330   
6.3 Maximum Likelihood Estimation . . 339   
6.4 Missing Data Modifications . . 348   
6.5 Structural Models: Signal Extraction and Forecasting . . . . . 352   
6.6 ARMAX Models in State-Space Form 355   
6.7 Bootstrapping State-Space Models . . . 357   
6.8 Dynamic Linear Models with Switching . . 362   
6.9 Nonlinear and Non-normal State-Space Models Using Monte Carlo Methods 376   
6.10 Stochastic Volatility 388   
6.11 State-Space and ARMAX Models for Longitudinal Data Analysis 394

Problems 404

# 7 Statistical Methods in the Frequency Domain 412

7.1 Introduction . . 412   
7.2 Spectral Matrices and Likelihood Functions . . . . 416   
7.3 Regression for Jointly Stationary Series 417   
7.4 Regression with Deterministic Inputs . . . . 426   
7.5 Random Coefficient Regression . . . 434   
7.6 Analysis of Designed Experiments . . 438   
7.7 Discrimination and Cluster Analysis . . . . 449

7.8 Principal Components and Factor Analysis . . . . 464   
7.9 The Spectral Envelope . . . . 479

Problems 495

# Appendix A: Large Sample Theory 501

A.1 Convergence Modes 501   
A.2 Central Limit Theorems . . 509   
A.3 The Mean and Autocorrelation Functions . . . 513

# Appendix B: Time Domain Theory 522

B.1 Hilbert Spaces and the Projection Theorem . . . 522   
B.2 Causal Conditions for ARMA Models 526   
B.3 Large Sample Distribution of the AR(p) Conditional Least Squares Estimators 528   
B.4 The Wold Decomposition 532

# Appendix C: Spectral Domain Theory 534

C.1 Spectral Representation Theorem . . . 534   
C.2 Large Sample Distribution of the DFT and Smoothed Periodogram 539   
C.3 The Complex Multivariate Normal Distribution . . . . . . . . . 550

# References 555

# Index 569

# Chapter 1

# Characteristics of Time Series

# 1.1 Introduction

The analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference. The obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed. The systematic approach by which one goes about answering the mathematical and statistical questions posed by these time correlations is commonly referred to as time series analysis.

The impact of time series analysis on scientific applications can be partially documented by producing an abbreviated listing of the diverse fields in which important time series problems may arise. For example, many familiar time series occur in the field of economics, where we are continually exposed to daily stock market quotations or monthly unemployment figures. Social scientists follow populations series, such as birthrates or school enrollments. An epidemiologist might be interested in the number of influenza cases observed over some time period. In medicine, blood pressure measurements traced over time could be useful for evaluating drugs used in treating hypertension. Functional magnetic resonance imaging of brain-wave time series patterns might be used to study how the brain reacts to certain stimuli under various experimental conditions.

Many of the most intensive and sophisticated applications of time series methods have been to problems in the physical and environmental sciences. This fact accounts for the basic engineering flavor permeating the language of

time series analysis. One of the earliest recorded series is the monthly sunspot numbers studied by Schuster (1906). More modern investigations may center on whether a warming is present in global temperature measurements or whether levels of pollution may influence daily mortality in Los Angeles. The modeling of speech series is an important problem related to the efficient transmission of voice recordings. Common features in a time series characteristic known as the power spectrum are used to help computers recognize and translate speech. Geophysical time series such those produced by yearly depositions of various kinds can provide long-range proxies for temperature and rainfall. Seismic recordings can aid in mapping fault lines or in distinguishing between earthquakes and nuclear explosions.

The above series are only examples of experimental databases that can be used to illustrate the process by which classical statistical methodology can be applied in the correlated time series framework. In our view, the first step in any time series investigation always involves careful scrutiny of the recorded data plotted over time. This scrutiny often suggests the method of analysis as well as statistics that will be of use in summarizing the information in the data. Before looking more closely at the particular statistical methods, it is appropriate to mention that two separate, but not necessarily mutually exclusive, approaches to time series analysis exist, commonly identified as the time domain approach and the frequency domain approach.

The time domain approach is generally motivated by the presumption that correlation between adjacent points in time is best explained in terms of a dependence of the current value on past values. The time domain approach focuses on modeling some future value of a time series as a parametric function of the current and past values. In this scenario, we begin with linear regressions of the present value of a time series on its own past values and on the past values of other series. This modeling leads one to use the results of the time domain approach as a forecasting tool and is particularly popular with economists for this reason.

One approach, advocated in the landmark work of Box and Jenkins (1970; see also Box et al., 1994), develops a systematic class of models called autoregressive integrated moving average (ARIMA) models to handle time-correlated modeling and forecasting. The approach includes a provision for treating more than one input series through multivariate ARIMA or through transfer function modeling. The defining feature of these models is that they are multiplicative models, meaning that the observed data are assumed to result from products of factors involving differential or difference equation operators responding to a white noise input.

A more recent approach to the same problem uses additive models more familiar to statisticians. In this approach, the observed data are assumed to result from sums of series, each with a specified time series structure; for example, in economics, assume a series is generated as the sum of trend, a seasonal effect, and error. The state-space model that results is then treated by making judicious use of the celebrated Kalman filters and smoothers, developed origi-

nally for estimation and control in space applications. Two relatively complete presentations from this point of view are in Harvey (1991) and Kitagawa and Gersch (1996). Time series regression is introduced in Chapter 2, and ARIMA and related time domain models are studied in Chapter 3, with the emphasis on classical, statistical, univariate linear regression. Special topics on time domain analysis are covered in Chapter 5; these topics include modern treatments of, for example, time series with long memory and GARCH models for the analysis of volatility. The state-space model, Kalman filtering and smoothing, and related topics are developed in Chapter 5.

Conversely, the frequency domain approach assumes the primary characteristics of interest in time series analyses relate to periodic or systematic sinusoidal variations found naturally in most data. These periodic variations are often caused by biological, physical, or environmental phenomena of interest. A series of periodic shocks may influence certain areas of the brain; wind may affect vibrations on an airplane wing; sea surface temperatures caused by El Ni˜no oscillations may affect the number of fish in the ocean. The study of periodicity extends to economics and social sciences, where one may be interested in yearly periodicities in such series as monthly unemployment or monthly birth rates.

In spectral analysis, the partition of the various kinds of periodic variation in a time series is accomplished by evaluating separately the variance associated with each periodicity of interest. This variance profile over frequency is called the power spectrum. In our view, no schism divides time domain and frequency domain methodology, although cliques are often formed that advocate primarily one or the other of the approaches to analyzing data. In many cases, the two approaches may produce similar answers for long series, but the comparative performance over short samples is better done in the time domain. In some cases, the frequency domain formulation simply provides a convenient means for carrying out what is conceptually a time domain calculation. Hopefully, this book will demonstrate that the best path to analyzing many data sets is to use the two approaches in a complementary fashion. Expositions emphasizing primarily the frequency domain approach can be found in Bloomfield (1976), Priestley (1981), or Jenkins and Watts (1968). On a more advanced level, Hannan (1970), Brillinger (1981), Brockwell and Davis (1991), and Fuller (1995) are available as theoretical sources. Our coverage of the frequency domain is given in Chapters 4 and 7.

The objective of this book is to provide a unified and reasonably complete exposition of statistical methods used in time series analysis, giving serious consideration to both the time and frequency domain approaches. Because a myriad of possible methods for analyzing any particular experimental series can exist, we have integrated real data from a number of subject fields into the exposition and have suggested methods for analyzing these data.

![](images/9513c607ea5bb1d9514a720efffa8578d178f01f67b4b916c2ff73075c33908d.jpg)  
Figure 1.1 Johnson & Johnson quarterly earnings per share, 84 quarters, 1960-I to 1980-IV.

# 1.2 The Nature of Time Series Data

Some of the problems and questions of interest to the prospective time series analyst can best be exposed by considering real experimental data taken from different subject areas. The following cases illustrate some of the common kinds of experimental time series data as well as some of the statistical questions that might be asked about such data.

# Example 1.1 Johnson & Johnson Quarterly Earnings

Figure 1.1 shows quarterly earnings per share for the U.S. company Johnson & Johnson, furnished by Professor Paul Griffin (personal communication) of the Graduate School of Management, University of California, Davis. There are 84 quarters (21 years) measured from the first quarter of 1960 to the last quarter of 1980. Modeling such series begins by observing the primary patterns in the time history. In this case, note the gradually increasing underlying trend and the rather regular variation superimposed on the trend that seems to repeat over quarters. Methods for analyzing data such as these are explored in Chapter 2 (see Problem 2.1) using regression techniques, and in Chapter 6, 6.5, using structural equation modeling.

To plot the data using the R statistical package, suppose you saved the data as jj.dat in the directory mydata. Then use the following steps to read in the data and plot the time series (the $>$ below are prompts, you

![](images/3f32e7538c5bfc78186bf4431715dbc843b04c8fd1f3bcda171069ac55bde38d.jpg)  
Figure 1.2 Yearly average global temperature deviations (1900–1997) in degrees centigrade.

do not type them):

> jj = scan("/mydata/jj.dat") # yes forward slash   
> jj=ts(jj,start=1960, frequency=4)   
> plot(jj, ylab $\underline { { \underline { { \mathbf { \Pi } } } } } =$ "Quarterly Earnings per Share")

You can replace scan with read.table in this example.

# Example 1.2 Global Warming

Consider a global temperature series record, discussed in Jones (1994) and Parker et al. (1994, 1995). The data in Figure 1.2 are a combination of land-air average temperature anomalies (from 1961-1990 average), measured in degrees centigrade, for the years 1900-1997. We note an apparent upward trend in the series that has been used as an argument for the global warming hypothesis. Note also the leveling off at about 1935 and then another rather sharp upward trend at about 1970. The question of interest for global warming proponents and opponents is whether the overall trend is natural or whether it is caused by some humaninduced interface. Problem 2.8 examines 634 years of glacial sediment data that might be taken as a long-term temperature proxy. Such percentage changes in temperature do not seem to be unusual over a time period of 100 years. Again, the question of trend is of more interest than particular periodicities.

![](images/d4f98a81c89a11e3dfcea3213f95286426477ceda8800563b734cb44dbd7e283.jpg)  
Figure 1.3 Speech recording of the syllable $a a a \cdots h h $ sampled at 10,000 points per second with $n = 1 0 2 0$ points.

# Example 1.3 Speech Data

More involved questions develop in applications to the physical sciences. Figure 1.3 shows a small .1 second (1000 point) sample of recorded speech for the phrase $a a a \cdots h h $ , and we note the repetitive nature of the signal and the rather regular periodicities. One current problem of great interest is computer recognition of speech, which would require converting this particular signal into the recorded phrase $a a a \cdots h h $ . Spectral analysis can be used in this context to produce a signature of this phrase that can be compared with signatures of various library syllables to look for a match. One can immediately notice the rather regular repetition of small wavelets. The separation between the packets is known as the pitch period and represents the response of the vocal tract filter to a periodic sequence of pulses stimulated by the opening and closing of the glottis.

# Example 1.4 New York Stock Exchange

As an example of financial time series data, Figure 1.4 shows the daily returns (or percent change) of the New York Stock Exchange (NYSE) from February 2, 1984 to December 31, 1991. It is easy to spot the crash of October 19, 1987 in the figure. The data shown in Figure 1.4 are typical of return data. The mean of the series appears to be stable

![](images/87dcb05b04970497453270c940601a444f3136e3d1c18365380c50eabd8ad52d.jpg)  
Figure 1.4 Returns of the NYSE. The data are daily value weighted market returns from February 2, 1984 to December 31, 1991 (2000 trading days). The crash of October 19, 1987 occurs at $t = 9 3 8$ .

with an average return of approximately zero, however, the volatility (or variability) of data changes over time. In fact, the data show volatility clustering; that is, highly volatile periods tend to be clustered together. A problem in the analysis of these type of financial data is to forecast the volatility of future returns. Models such as ARCH and GARCH models (Engle, 1982; Bollerslev, 1986) and stochastic volatility models (Harvey, Ruiz and Shephard, 1994) have been developed to handle these problems. We will discuss these models and the analysis of financial data in Chapters 5 and 6.

# Example 1.5 El Ni˜no and Fish Population

We may also be interested in analyzing several time series at once. Figure 1.5 shows monthly values of an environmental series called the Southern Oscillation Index (SOI) and associated Recruitment (number of new fish) furnished by Dr. Roy Mendelssohn of the Pacific Environmental Fisheries Group (personal communication). Both series are for a period of 453 months ranging over the years 1950-1987. The SOI measures changes in air pressure, related to sea surface temperatures in the central Pacific. The central Pacific Ocean warms every three to seven years due to the El Ni˜no effect, which has been blamed, in particular, for the 1997 floods in the midwestern portions of the U.S. Both series in Figure 1.5 tend to exhibit repetitive behavior, with regularly repeating cycles that are easily

![](images/750cae9b2ca66f17792a0b2570df3b564c2a4c298ca0f5f0de6fed5201ca0ed5.jpg)

![](images/a90bcbf533e1b5c310da0d93863f763327d26573ab187abddf7d143e0394bf82.jpg)  
Figure 1.5 Monthly SOI and Recruitment (Estimated new fish), 1950-1987.

visible. This periodic behavior is of interest because underlying processes of interest may be regular and the rate or frequency of oscillation characterizing the behavior of the underlying series would help to identify them. One can also remark that the cycles of the SOI are repeating at a faster rate than those of the Recruitment series. The Recruitment series also shows several kinds of oscillations, a faster frequency that seems to repeat about every 12 months and a slower frequency that seems to repeat about every 50 months. The study of the kinds of cycles and their strengths is the subject of Chapter 4. The two series also tend to be somewhat related; it is easy to imagine that somehow the fish population is dependent on the SOI. Perhaps, even a lagged relation exists, with the SOI signaling changes in the fish population. This possibility suggests trying some version of regression analysis as a procedure for relating the two series. Transfer function modeling, as considered in Chapter 5, can be applied in this case to obtain a model relating Recruitment to its own past and the past values of the SOI Index.

![](images/644660c6f06e8c41ff6973ba52e1b1762db51c128f81196577a1611cded02d36.jpg)

![](images/30693ac3f1abd7f75c1b49e7afc00e2d10742b82103a84f85e487d79e4b264d3.jpg)  
Thalamus & Cerebellum   
Time (1 pt =2 secs)   
Figure 1.6 fMRI data from various locations in the cortex, thalamus, and cerebellum; $n = 1 2 8$ points, one observation taken every 2 seconds.

# Example 1.6 fMRI Imaging

A fundamental problem in classical statistics occurs when we are given a collection of independent series or vectors of series, generated under varying experimental conditions or treatment configurations. Such a set of series is shown in Figure 1.6, where we observe data collected from various locations in the brain via functional magnetic resonance imaging (fMRI). In this example, five subjects were given periodic brushing on the hand. The stimulus was applied for 32 seconds and then stopped for 32 seconds; thus, the signal period is 64 seconds. The sampling rate was one observation every 2 seconds for 256 seconds ( $n = 1 2 8$ ). For this example, we averaged the results over subjects (these were evoked responses, and all subjects were in phase). The series shown in Figure 1.6 are consecutive measures of blood oxygenation-level dependent (bold) signal intensity, which measures areas of activation in the brain. Notice that the periodicities appear strongly in the motor cortex series and less strongly in the thalamus and cerebellum. The fact that one has series from different areas of the brain suggests testing whether the areas are

![](images/41a43bf616b485069039f42e4c42455bbfbd8c876ab44ccd8e798ef00603c939.jpg)

![](images/7d3214eb29107af1b7cb0ecbac9efa14929678753910fb9c1c8a0e40de48fb4f.jpg)  
Explosion   
Figure 1.7 Arrival phases from an earthquake (top) and explosion (bottom) at 40 points per second.

responding differently to the brush stimulus. Analysis of variance techniques accomplish this in classical statistics, and we show in Chapter 7 how these classical techniques extend to the time series case, leading to a spectral analysis of variance.

The data are in a file called fmri.dat, which consists of nine columns; the first column represents time, whereas the second through ninth columns represent the bold signals at eight locations. Assuming the data are located in the directory mydata, use the following commands in R to plot the data as in this example.

> fmri = read.table("/mydata/fmri.dat")   
> par(mfrow=c(2,1)) # sets up the graphics   
> ts.plot(fmri[,2:5], lty=c(1,4), ylab $=$ "BOLD")   
> ts.plot(fmri[,6:9], lty $\eqcirc$ (1,4), ylab $\mid =$ "BOLD")

# Example 1.7 Earthquakes and Explosions

As a final example, the series in Figure 1.7 represent two phases or arrivals along the surface, denoted by $\mathrm { P }$ ( $t = 1 , \dots , 1 0 2 4 )$ and S ( $t \ =$

$1 0 2 5 , \ldots , 2 0 4 8 )$ , at a seismic recording station. The recording instruments in Scandinavia are observing earthquakes and mining explosions with one of each shown in Figure 1.7. The general problem of interest is in distinguishing or discriminating between waveforms generated by earthquakes and those generated by explosions. Features that may be important are the rough amplitude ratios of the first phase P to the second phase S, which tend to be smaller for earthquakes than for explosions. In the case of the two events in Figure 1.7, the ratio of maximum amplitudes appears to be somewhat less than .5 for the earthquake and about 1 for the explosion. Otherwise, note a subtle difference exists in the periodic nature of the S phase for the earthquake. We can again think about spectral analysis of variance for testing the equality of the periodic components of earthquakes and explosions. We would also like to be able to classify future P and S components from events of unknown origin, leading to the time series discriminant analysis developed in Chapter 7.

The data are in the file eq5exp6.dat as one column with 4096 entries, the first 2048 observations correspond to an earthquake and the next 2048 observations correspond to an explosion. To read and plot the data as in this example, use the following commands in R:

> x = matrix(scan("/mydata/eq5exp6.dat"), ncol=2)   
> par(mfrow=c(2,1))  
> plot.ts(x[,1], main $\cdot ^ { = }$ "Earthquake", ylab="EQ5")   
> plot.ts(x[,2], main $\cdot ^ { = }$ "Explosion", ylab="EXP6")

# 1.3 Time Series Statistical Models

The primary objective of time series analysis is to develop mathematical models that provide plausible descriptions for sample data, like that encountered in the previous section. In order to provide a statistical setting for describing the character of data that seemingly fluctuate in a random fashion over time, we assume a time series can be defined as a collection of random variables indexed according to the order they are obtained in time. For example, we may consider a time series as a sequence of random variables, $x _ { 1 } , x _ { 2 } , x _ { 3 } , \dotsc .$ , where the random variable $x _ { 1 }$ denotes the value taken by the series at the first time point, the variable $x _ { 2 }$ denotes the value for the second time period, $x _ { 3 }$ denotes the value for the third time period, and so on. In general, a collection of random variables, $\{ x _ { t } \}$ , indexed by $t$ is referred to as a stochastic process. In this text, $t$ will typically be discrete and vary over the integers $t = 0 , \pm 1 , \pm 2 , \ldots$ , or some subset of the integers. The observed values of a stochastic process are referred to as a realization of the stochastic process. Because it will be clear from the context of our discussions, we use the term time series whether we are referring generically to the process or to a particular realization and make no notational distinction between the two concepts.

It is conventional to display a sample time series graphically by plotting the values of the random variables on the vertical axis, or ordinate, with the time scale as the abscissa. It is usually convenient to connect the values at adjacent time periods to reconstruct visually some original hypothetical continuous time series that might have produced these values as a discrete sample. Many of the series discussed in the previous section, for example, could have been observed at any continuous point in time and are conceptually more properly treated as continuous time series. The approximation of these series by discrete time parameter series sampled at equally spaced points in time is simply an acknowledgment that sampled data will, for the most part, be discrete because of restrictions inherent in the method of collection. Furthermore, the analysis techniques are then feasible using computers, which are limited to digital computations. Theoretical developments also rest on the idea that a continuous parameter time series should be specified in terms of finite-dimensional distribution functions defined over a finite number of points in time. This is not to say that the selection of the sampling interval or rate is not an extremely important consideration. The appearance of data can be changed completely by adopting an insufficient sampling rate. We have all seen wagon wheels in movies appear to be turning backwards because of the insufficient number of frames sampled by the camera. This phenomenon leads to a distortion called aliasing.

The fundamental visual characteristic distinguishing the different series shown in Examples 1.1–1.7 is their differing degrees of smoothness. One possible explanation for this smoothness is that it is being induced by the supposition that adjacent points in time are correlated, so the value of the series at time $t$ , say, $x _ { t }$ , depends in some way on the past values $x _ { t - 1 } , x _ { t - 2 } , . . .$ . This model expresses a fundamental way in which we might think about generating realistic-looking time series. To begin to develop an approach to using collections of random variables to model time series, consider Example 1.8.

# Example 1.8 White Noise

A simple kind of generated series might be a collection of uncorrelated random variables, $w _ { t }$ , with mean 0 and finite variance $\sigma _ { w } ^ { 2 }$ . The time series generated from uncorrelated variables is used as a model for noise in engineering applications, where it is called white noise; we shall sometimes denote this process as $w _ { t } \sim w n ( 0 , \sigma _ { w } ^ { 2 } )$ . The designation white originates from the analogy with white light and indicates that all possible periodic oscillations are present with equal strength.

We will, at times, also require the noise to be iid random variables with mean 0 and variance $\sigma _ { w } ^ { 2 }$ . We shall distinguish this case by saying white independent noise, or by writing $w _ { t } \sim \mathrm { i i d } ( 0 , \sigma _ { w } ^ { 2 } )$ . A particularly useful dent normal random variables, with mean 0 and variance white noise series is Gaussian white noise, wherein the $w _ { t }$ are indepen- $\sigma _ { w } ^ { 2 }$ ; or more succinctly, $w _ { t } \sim$ iid $\mathrm { N } ( 0 , \sigma _ { w } ^ { 2 } )$ . Figure 1.8 shows in the upper panel a collection of 500 such random variables, with $\sigma _ { w } ^ { 2 } = 1$ , plotted in the order in

![](images/fac43b29018e8cbd3c40c61a0e1d60a06536c82aef69fe70c41db5f69a411f0f.jpg)

![](images/331889bb3e74d808e51eb7c2bef960c77a899666c15a80bfb29e1869363c2e19.jpg)  
Figure 1.8 Gaussian white noise series (top) and three-point moving average of the Gaussian white noise series (bottom).

which they were drawn. The resulting series bears a slight resemblance to the explosion in Figure 1.7 but is not smooth enough to serve as a plausible model for any of the other experimental series. The plot tends to show visually a mixture of many different kinds of oscillations in the white noise series.

If the stochastic behavior of all time series could be explained in terms of the white noise model, classical statistical methods would suffice. Two ways of introducing serial correlation and more smoothness into time series models are given in Examples 1.9 and 1.10.

# Example 1.9 Moving Averages

We might replace the white noise series $w _ { t }$ by a moving average that smoothes the series. For example, consider replacing $w _ { t }$ in Example 1.8 by an average of its current value and its immediate neighbors in the past and future. That is, let

$$
v _ {t} = \frac {1}{3} \left(w _ {t - 1} + w _ {t} + w _ {t + 1}\right), \tag {1.1}
$$

which leads to the series shown in the lower panel of Figure 1.8. Inspecting the series shows a smoother version of the first series, reflecting the fact that the slower oscillations are more apparent and some of the faster

![](images/7ed5aca0ade61d114504bb375794e496c6c0ba75f65fdea3466ebcd01cc7f499.jpg)  
Figure 1.9 Autoregressive series generated from model (1.2).

oscillations are taken out. We begin to notice a similarity to the SOI in Figure 1.5, or perhaps, to some of the fMRI series in Figure 1.6.

To reproduce Figure 1.8 in R use the following commands:1

> w = rnorm(500,0,1) # 500 N(0,1) variates   
> v = filter(w, sides=2, rep(1,3)/3) # moving average   
> par(mfrow=c(2,1))  
> plot.ts(w)   
> plot.ts(v)

The speech series in Figure 1.3 and the Recruitment series in Figure 1.5, as well as some of the MRI series in Figure 1.6, differ from the moving average series because one particular kind of oscillatory behavior seems to predominate, producing a sinusoidal type of behavior. A number of methods exist for generating series with this quasi-periodic behavior; we illustrate a popular one based on the autoregressive model considered in Chapter 3.

# Example 1.10 Autoregressions

Suppose we consider the white noise series $w _ { t }$ of Example 1.8 as input and calculate the output using the second-order equation

$$
x _ {t} = x _ {t - 1} - . 9 0 x _ {t - 2} + w _ {t} \tag {1.2}
$$

successively for $t = 1 , 2 , \ldots , 5 0 0$ . Equation (1.2) represents a regression or prediction of the current value $x _ { t }$ of a time series as a function of

![](images/87e9c88ba9fa2d41056e4d693482dd9e158f8ec27543ac5ffb711c8b84f9e43c.jpg)  
Figure 1.10 Random walk, $\sigma _ { w } = 1$ , with drift $\delta = . 2$ (upper jagged line), without drift, $\delta \ = \ 0$ (lower jagged line), and a straight line with slope .2 (dashed line).

the past two values of the series, and, hence, the term autoregression is suggested for this model. A problem with startup values exists here because (1.2) also depends on the initial conditions $x _ { 0 }$ and $x _ { - 1 }$ , but, for now, we assume that we are given these values and generate the succeeding values by substituting into (1.2). The resulting output series is shown in Figure 1.9, and we note the periodic behavior of the series, which is similar to that displayed by the speech series in Figure 1.3. The autoregressive model above and its generalizations can be used as an underlying model for many observed series and will be studied in detail in Chapter 3.

One way to simulate and plot data from the model (1.2) in R is to use the following commands (another way is to use arima.sim).

> w = rnorm(550,0,1) # 50 extra to avoid startup problems

> x = filter(w, filter=c(1,-.9), method="recursive")

> plot.ts(x[51:550])

# Example 1.11 Random Walk

A model for analyzing trend is the random walk with drift model given by

$$
x _ {t} = \delta + x _ {t - 1} + w _ {t} \tag {1.3}
$$

for $t = 1 , 2 , \ldots$ , with initial condition $x _ { 0 } = 0$ , and where $w _ { t }$ is white noise. The constant $\delta$ is called the drift, and when $\delta = 0$ , (1.3) is called simply

a random walk. The term random walk comes from the fact that, when $\delta = 0$ , the value of the time series at time $t$ is the value of the series at time $t - 1$ plus a completely random movement determined by $w _ { t }$ . Note that we may rewrite (1.3) as a cumulative sum of white noise variates. That is,

$$
x _ {t} = \delta t + \sum_ {j = 1} ^ {t} w _ {j} \tag {1.4}
$$

for $t = 1 , 2 , \dots$ ; either use induction, or plug (1.4) into (1.3) to verify this statement. Figure 1.10 shows 200 observations generated from the model with $\delta = 0$ and .2, and with $\sigma _ { w } = 1$ . For comparison, we also superimposed the straight line . $2 t$ on the graph.

To reproduce Figure 1.10 in R:

```diff
> set.seed(154)  
> w = rnorm(200,0,1); x = cumsum(w)  
> wd = w + .2; xd = cumsum(wd)  
> plot.ts(xd,ylim=c(-5,55))  
> lines(x)  
> lines(.2*(1:200), lty="dashed") 
```

# Example 1.12 Signal in Noise

Many realistic models for generating time series assume an underlying signal with some consistent periodic variation, contaminated by adding a random noise. For example, it is easy to detect the regular cycle fMRI series displayed on the top of Figure 1.6. Consider the model

$$
x _ {t} = 2 \cos (2 \pi t / 5 0 +. 6 \pi) + w _ {t} \tag {1.5}
$$

for $t = 1 , 2 , \ldots , 5 0 0$ , where the first term is regarded as the signal, shown in the upper panel of Figure 1.11. We note that a sinusoidal waveform can be written as

$$
A \cos (2 \pi \omega t + \phi), \tag {1.6}
$$

where $A$ is the amplitude, $\omega$ is the frequency of oscillation, and $\phi$ is a phase shift. In (1.5), $A = 2$ , $\omega = 1 / 5 0$ (one cycle every 50 time points), and $\phi = . 6 \pi$ .

An additive noise term was taken to be white noise with $\sigma _ { w } = 1$ (middle panel) and $\sigma _ { w } = 5$ (bottom panel), drawn from a normal distribution. Adding the two together obscures the signal, as shown in the lower panels of Figure 1.11. Of course, the degree to which the signal is obscured depends on the amplitude of the signal and the size of $\sigma _ { w }$ . The ratio of the amplitude of the signal to $\sigma _ { w }$ (or some function of the ratio) is sometimes called the signal-to-noise ratio (SNR); the larger the SNR, the easier it is to detect the signal. Note that the signal is easily discernible

![](images/d058c875e41fda41cf4881d7a85512c869c4887a5057021d91f353e9d245e669.jpg)  
Figure 1.11 Cosine wave with period 50 points (top panel) compared with the cosine wave contaminated with additive white Gaussian noise, $\sigma _ { w } = 1$ (middle panel) and $\sigma _ { w } = 5$ (bottom panel); see (1.5).

in the middle panel of Figure 1.11, whereas the signal is obscured in the bottom panel. Typically, we will not observe the signal, but the signal obscured by noise.

To reproduce Figure 1.11 in R, use the following commands:

```txt
> t = 1:500  
> c = 2*cos(2*pi*t/50 + .6*pi)  
> w = rnorm(500,0,1)  
> par(mfrow=c(3,1))  
> plot.ts(c)  
> plot.ts(c + w)  
> plot.ts(c + 5*w) 
```

In Chapter 4, we will study the use of spectral analysis as a possible technique for detecting regular or periodic signals, such as the one described in Example 1.12. In general, we would emphasize the importance of simple additive models such as given above in the form

$$
x _ {t} = s _ {t} + v _ {t}, \tag {1.7}
$$

where $s _ { t }$ denotes some unknown signal and $v _ { t }$ denotes a time series that may be white or correlated over time. The problems of detecting a signal and then

in estimating or extracting the waveform of $s _ { t }$ are of great interest in many areas of engineering and the physical and biological sciences. In economics, the underlying signal may be a trend or it may be a seasonal component of a series. Models such as (1.7), where the signal has an autoregressive structure, form the motivation for the state-space model of Chapter 6.

In the above examples, we have tried to motivate the use of various combinations of random variables emulating real time series data. Smoothness characteristics of observed time series were introduced by combining the random variables in various ways. Averaging independent random variables over adjacent time points, as in Example 1.9, or looking at the output of difference equations that respond to white noise inputs, as in Example 1.10, are common ways of generating correlated data. In the next section, we introduce various theoretical measures used for describing how time series behave. As is usual in statistics, the complete description involves the multivariate distribution function of the jointly sampled values $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ , whereas more economical descriptions can be had in terms of the mean and autocorrelation functions. Because correlation is an essential feature of time series analysis, the most useful descriptive measures are those expressed in terms of covariance and correlation functions.

# 1.4 Measures of Dependence: Autocorrelation and Cross-Correlation

A complete description of a time series, observed as a collection of $n$ random variables at arbitrary integer time points $t _ { 1 } , t _ { 2 } , \ldots , t _ { n }$ , for any positive integer $n$ , is provided by the joint distribution function, evaluated as the probability that the values of the series are jointly less than the $n$ constants, $c _ { 1 } , c _ { 2 } , \ldots , c _ { n }$ , i.e.,

$$
F \left(c _ {1}, c _ {2}, \dots , c _ {n}\right) = P \left(x _ {t _ {1}} \leq c _ {1}, x _ {t _ {2}} \leq c _ {2}, \dots , x _ {t _ {n}} \leq c _ {n}\right). \tag {1.8}
$$

Unfortunately, the multidimensional distribution function cannot usually be written easily unless the random variables are jointly normal, in which case, expression (1.8) comes from the usual multivariate normal distribution (see Anderson, 1984, or Johnson and Wichern, 1992). A particular case in which the multidimensional distribution function is easy would be for independent and identically distributed standard normal random variables, for which the joint distribution function can be expressed as the product of the marginals, say,

$$
F \left(c _ {1}, c _ {2}, \dots , c _ {n}\right) = \prod_ {t = 1} ^ {n} \Phi \left(c _ {t}\right), \tag {1.9}
$$

where

$$
\Phi (x) = \frac {1}{\sqrt {2 \pi}} \int_ {- \infty} ^ {x} \exp \left\{- \frac {z ^ {2}}{2} \right\} d z \tag {1.10}
$$

# 1.4: Measures of Dependence

is the cumulative distribution function of the standard normal.

Although the multidimensional distribution function describes the data completely, it is an unwieldy tool for displaying and analyzing time series data. The distribution function (1.8) must be evaluated as a function of $n$ arguments, so any plotting of the corresponding multivariate density functions is virtually impossible. The one-dimensional distribution functions

$$
F _ {t} (x) = P \{x _ {t} \leq x \}
$$

or the corresponding one-dimensional density functions

$$
f _ {t} (x) = \frac {\partial F _ {t} (x)}{\partial x},
$$

when they exist, are often informative for determining whether a particular coordinate of the time series has a well-known density function, like the normal (Gaussian) distribution.

Definition 1.1 The mean function is defined as

$$
\mu_ {x t} = E \left(x _ {t}\right) = \int_ {- \infty} ^ {\infty} x f _ {t} (x) d x, \tag {1.11}
$$

provided it exists, where $E$ denotes the usual expected value operator. When no confusion exists about which time series we are referring to, we will drop a subscript and write $\mu _ { x t }$ as $\mu _ { t }$ .

The important thing to realize about $\mu _ { t }$ is that it is a theoretical mean for the series at one particular time point, where the mean is taken over all possible events that could have produced $x _ { t }$ .

# Example 1.13 Mean Function of a Moving Average Series

If $w _ { t }$ denotes a white noise series, then $\mu _ { w t } = E ( w _ { t } ) = 0$ for all $t$ . The top series in Figure 1.8 reflects this, as the series clearly fluctuates around a mean value of zero. Smoothing the series as in Example 1.9 does not change the mean because we can write

$$
\mu_ {v t} = E (v _ {t}) = \frac {1}{3} [ E (w _ {t - 1}) + E (w _ {t}) + E (w _ {t + 1}) ] = 0.
$$

# Example 1.14 Mean Function of a Random Walk with Drift

Consider the random walk with drift model given in (1.4),

$$
x _ {t} = \delta t + \sum_ {j = 1} ^ {t} w _ {j}, \qquad t = 1, 2, \dots .
$$

As in the previous example, because $E ( w _ { t } ) = 0$ for all $t$ , and $\delta$ is a constant, we have

$$
\mu_ {x t} = E (x _ {t}) = \delta t + \sum_ {j = 1} ^ {t} E (w _ {j}) = \delta t
$$

which is a straight line with slope $\delta$ . A realization of a random walk with drift can be compared to its mean function in Figure 1.10.

# Example 1.15 Mean Function of Signal Plus Noise

A great many practical applications depend on assuming the observed data have been generated by a fixed signal waveform superimposed on a zero-mean noise process, leading to an additive signal model of the form (1.5). It is clear, because the signal in (1.5) is a fixed function of time, we will have

$$
\begin{array}{l} \mu_ {x t} = E (x _ {t}) = E [ 2 \cos (2 \pi t / 5 0 +. 6 \pi) + w _ {t} ] \\ = 2 \cos (2 \pi t / 5 0 +. 6 \pi) + E \left(w _ {t}\right) \\ = 2 \cos (2 \pi t / 5 0 +. 6 \pi), \\ \end{array}
$$

and the mean function is just the cosine wave.

The lack of independence between two adjacent values $x _ { s }$ and $x _ { t }$ can be assessed numerically, as in classical statistics, using the notions of covariance and correlation. Assuming the variance of $x _ { t }$ is finite, we have the following definition.

Definition 1.2 The autocovariance function is defined as the second moment product

$$
\gamma_ {x} (s, t) = E \left[ \left(x _ {s} - \mu_ {s}\right) \left(x _ {t} - \mu_ {t}\right) \right], \tag {1.12}
$$

for all s and t. When no possible confusion exists about which time series we are referring to, we will drop the subscript and write $\gamma _ { x } ( s , t )$ as $\gamma ( s , t )$ .

Note that $\gamma _ { x } ( s , t ) = \gamma _ { x } ( t , s )$ for all time points $s$ and $t$ . The autocovariance measures the linear dependence between two points on the same series observed at different times. Very smooth series exhibit autocovariance functions that stay large even when the $t$ and $s$ are far apart, whereas choppy series tend to have autocovariance functions that are nearly zero for large separations. The autocovariance (1.12) is the average cross-product relative to the joint density $F ( x _ { s } , x _ { t } )$ . Recall from classical statistics that if $\gamma _ { x } ( s , t ) = 0$ , $x _ { s }$ and $x _ { t }$ are not linearly related, but there still may be some dependence structure between them. If, however, $x _ { s }$ and $x _ { t }$ are bivariate normal, $\gamma _ { x } ( s , t ) = 0$ ensures their independence. It is clear that, for $s = t$ , the autocovariance reduces to the (assumed finite) variance, because

$$
\gamma_ {x} (t, t) = E \left[ \left(x _ {t} - \mu_ {t}\right) ^ {2} \right]. \tag {1.13}
$$

# Example 1.16 Autocovariance of White Noise

The white noise series $w _ { t }$ , shown in the top panel of Figure 1.8, has $E ( w _ { t } ) = 0$ and

$$
\gamma_ {w} (s, t) = E (w _ {s} w _ {t}) = \left\{ \begin{array}{l l} \sigma_ {w} ^ {2}, & s = t \\ 0, & s \neq t \end{array} \right.
$$

where, in this example, $\sigma _ { w } ^ { 2 } = 1$ . Noting that $w _ { s }$ and $w _ { t }$ are uncorrelated for $s \neq t$ , we would have $E ( w _ { s } w _ { t } ) = E ( w _ { s } ) E ( w _ { t } ) = 0$ because the mean values of the white noise variates are zero.

# Example 1.17 Autocovariance of a Moving Average

Consider applying a three-point moving average to the white noise series $w _ { t }$ of the previous example, as in Example 1.9 ( $\sigma _ { w } ^ { 2 } = 1$ ). Because ${ \boldsymbol { v } } _ { t }$ in (1.1) has mean zero, we have

$$
\begin{array}{l} \gamma_ {v} (s, t) = E \left[ \left(v _ {s} - 0\right) \left(v _ {t} - 0\right) \right] \\ = \frac {1}{9} E \left[ \left(w _ {s - 1} + w _ {s} + w _ {s + 1}\right) \left(w _ {t - 1} + w _ {t} + w _ {t + 1}\right) \right]. \\ \end{array}
$$

It is convenient to calculate it as a function of the separation, $s - t = h$ , say, for $h = 0 , \pm 1 , \pm 2 , . . . .$ . For example, with $h = 0$ ,

$$
\begin{array}{l} \gamma_ {v} (t, t) = \frac {1}{9} E [ (w _ {t - 1} + w _ {t} + w _ {t + 1}) (w _ {t - 1} + w _ {t} + w _ {t + 1}) ] \\ = \frac {1}{9} \left[ E \left(w _ {t - 1} w _ {t - 1}\right) + E \left(w _ {t} w _ {t}\right) + E \left(w _ {t + 1} w _ {t + 1}\right) \right] \\ = \frac {3}{9}. \\ \end{array}
$$

When $h = 1$

$$
\begin{array}{l} \gamma_ {v} (t + 1, t) = \frac {1}{9} E [ (w _ {t} + w _ {t + 1} + w _ {t + 2}) (w _ {t - 1} + w _ {t} + w _ {t + 1}) ] \\ = \frac {1}{9} \left[ E \left(w _ {t} w _ {t}\right) + E \left(w _ {t + 1} w _ {t + 1}\right) \right] \\ = \begin{array}{c} \frac {2}{9}, \end{array} \\ \end{array}
$$

using the fact that we may drop terms with unequal subscripts. Similar computations give $\gamma _ { v } ( t - 1 , t ) = 2 / 9$ , $\gamma _ { v } ( t + 2 , t ) = \gamma _ { v } ( t - 2 , t ) = 1 / 9$ , and 0 for larger separations. We summarize the values for all $s$ and $t$ as

$$
\gamma_ {v} (s, t) = \left\{ \begin{array}{l l} 3 / 9, & s = t \\ 2 / 9, & | s - t | = 1 \\ 1 / 9, & | s - t | = 2 \\ 0, & | s - t | \geq 3. \end{array} \right. \tag {1.14}
$$

Example 1.17 shows clearly that the smoothing operation introduces a covariance function that decreases as the separation between the two time points increases and disappears completely when the time points are separated by three or more time points. This particular autocovariance is interesting because it only depends on the time separation or lag and not on the absolute location of the points along the series. We shall see later that this dependence suggests a mathematical model for the concept of weak stationarity.

# Example 1.18 Autocovariance of a Random Walk

For the random walk model, $\textstyle x _ { t } = \sum _ { j = 1 } ^ { t } w _ { j }$ , we have

$$
\gamma_ {x} (s, t) = \operatorname {c o v} \left(x _ {s}, x _ {t}\right) = \operatorname {c o v} \left(\sum_ {j = 1} ^ {s} w _ {j}, \sum_ {k = 1} ^ {t} w _ {k}\right) = \min  \{s, t \} \sigma_ {w} ^ {2},
$$

because the $w _ { t }$ are uncorrelated random variables. Note that, as opposed to the previous examples, the autocovariance function of a random walk depends on the particular time values $s$ and $t$ , and not on the time separation or lag. Also, notice that the variance of the random walk, $\mathrm { v a r } ( x _ { t } ) = \gamma _ { x } ( t , t ) = t \sigma _ { w } ^ { 2 }$ , increases without bound as time $t$ increases. The effect of this variance increase can be seen in Figure 1.10 as the processes starting to move away from their mean functions $\delta t$ (note, $\delta = 0$ and .2 in that example).

As in classical statistics, it is more convenient to deal with a measure of association between $^ { - 1 }$ and 1, and this leads to the following definition.

Definition 1.3 The autocorrelation function (ACF) is defined as

$$
\rho (s, t) = \frac {\gamma (s , t)}{\sqrt {\gamma (s , s) \gamma (t , t)}}. \tag {1.15}
$$

The ACF measures the linear predictability of the series at time $t$ , say, $x _ { t }$ , using only the value $x _ { s }$ . We can show easily that $- 1 \le \rho ( s , t ) \le 1$ using the Cauchy–Schwarz inequality.2 If we can predict $x _ { t }$ perfectly from $x _ { s }$ through a linear relationship, $x _ { t } = \beta _ { 0 } + \beta _ { 1 } x _ { s }$ , then the correlation will be 1 when $\beta _ { 1 } > 0$ , and $^ { - 1 }$ when $\beta _ { 1 } < 0$ . Hence, we have a rough measure of the ability to forecast the series at time $t$ from the value at time $s$ .

Often, we would like to measure the predictability of another series $y _ { t }$ from the series $x _ { s }$ . Assuming both series have finite variances, we have

Definition 1.4 The cross-covariance function between two series $x _ { t }$ and $y _ { t }$ i s

$$
\gamma_ {x y} (s, t) = E \left[ \left(x _ {s} - \mu_ {x s}\right) \left(y _ {t} - \mu_ {y t}\right) \right]. \tag {1.16}
$$

# 1.5: Stationary Time Series

The scaled version of the cross-covariance function is called

Definition 1.5 The cross-correlation function (CCF)

$$
\rho_ {x y} (s, t) = \frac {\gamma_ {x y} (s , t)}{\sqrt {\gamma_ {x} (s , s) \gamma_ {y} (t , t)}}. \tag {1.17}
$$

We may easily extend the above ideas to the case of more than two series, say, $x _ { t 1 } , x _ { t 2 } , \ldots , x _ { t r }$ ; that is, multivariate time series with $r$ components. For example, the extension of (1.12) in this case is

$$
\gamma_ {j k} (s, t) = E \left[ \left(x _ {s j} - \mu_ {s j}\right) \left(x _ {t k} - \mu_ {t k}\right) \right] \quad j, k = 1, 2, \dots , r. \tag {1.18}
$$

In the definitions above, the autocovariance and cross-covariance functions may change as one moves along the series because the values depend on both $s$ and $t$ , the locations of the points in time. In Example 1.17, the autocovariance function depends on the separation of $x _ { s }$ and $x _ { t }$ , say, $h = | s - t |$ , and not on where the points are located in time. As long as the points are separated by $h$ units, the location of the two points does not matter. This notion, called weak stationarity, when the mean is constant, is fundamental in allowing us to analyze sample time series data when only a single series is available.

# 1.5 Stationary Time Series

The preceding definitions of the mean and autocovariance functions are completely general. Although we have not made any special assumptions about the behavior of the time series, many of the preceding examples have hinted that a sort of regularity may exist over time in the behavior of a time series. We introduce the notion of regularity using a concept called stationarity.

Definition 1.6 A strictly stationary time series is one for which the probabilistic behavior of every collection of values

$$
\left\{x _ {t _ {1}}, x _ {t _ {2}}, \dots , x _ {t _ {k}} \right\}
$$

is identical to that of the time shifted set

$$
\left\{x _ {t _ {1} + h}, x _ {t _ {2} + h}, \dots , x _ {t _ {k} + h} \right\}.
$$

That is,

$$
P \left\{x _ {t _ {1}} \leq c _ {1}, \dots , x _ {t _ {k}} \leq c _ {k} \right\} = P \left\{x _ {t _ {1} + h} \leq c _ {1}, \dots , x _ {t _ {k} + h} \leq c _ {k} \right\} \tag {1.19}
$$

for all $k = 1 , 2 , \dots$ , all time points $t _ { 1 } , t _ { 2 } , \ldots , t _ { k }$ , all numbers $c _ { 1 } , c _ { 2 } , \ldots , c _ { k }$ , and all time shifts $h = 0 , \pm 1 , \pm 2 , \ldots$ .

If a time series is strictly stationary, then all of the multivariate distribution functions for subsets of variables must agree with their counterparts in the shifted set for all values of the shift parameter $h$ . For example, when $k = 1$ , (1.19) implies that

$$
P \left\{x _ {s} \leq c \right\} = P \left\{x _ {t} \leq c \right\} \tag {1.20}
$$

for any time points $s$ and $t$ . This statement implies, e.g., that the probability the value of a time series sampled hourly is negative at $1 ~ \mathrm { A M }$ is the same as at 10 am. In addition, if the mean function, $\mu _ { t }$ , of the series $x _ { t }$ exists, (1.20) implies that $\mu _ { s } = \mu _ { t }$ for all $s$ and $t$ , and hence $\mu _ { t }$ must be constant. Note, e.g., that a random walk process with drift is not strictly stationary because its mean function changes with time (see Example 1.14).

When $k = 2$ , we can write (1.19) as

$$
P \left\{x _ {s} \leq c _ {1}, x _ {t} \leq c _ {2} \right\} = P \left\{x _ {s + h} \leq c _ {1}, x _ {t + h} \leq c _ {2} \right\} \tag {1.21}
$$

for any time points $s$ and $t$ and shift $h$ . Thus, if the variance function of the process exists, (1.21) implies that the autocovariance function of the series $x _ { t }$ satisfies

$$
\gamma (s, t) = \gamma (s + h, t + h)
$$

for all $s$ and $t$ and $h$ . We may interpret this result by saying the autocovariance function of the process depends only on the time difference between $s$ and $t$ , and not on the actual times.

The version of stationarity in (1.19) is too strong for most applications. Moreover, it is difficult to assess strict stationarity from a single data set. Rather than impose conditions on all possible distributions of a time series, we will use a milder version that imposes conditions only on the first two moments of the series. We now have the following definition.

Definition 1.7 A weakly stationary time series, $x _ { t }$ , is a finite variance process such that

(i) the mean value function, $\mu _ { t }$ , defined in (1.11) is constant and does not depend on time $t$ , and   
(ii) the covariance function, $\gamma ( s , t )$ , defined in (1.12) depends on s and t only through their difference $| s - t |$ .

Henceforth, we will use the term stationary to mean weak stationarity; if a process is stationary in the strict sense, we will use the term strictly stationary.

It should be clear from the discussion of strict stationarity following Definition 1.6 that a strictly stationary, finite variance, time series is also stationary. The converse is not true unless there are further conditions. One important case where stationarity implies strict stationarity is if the time series is Gaussian [meaning all finite distributions, (1.19), of the series are Gaussian]. We will make this concept more precise at the end of this section.

# 1.5: Stationary Time Series

Because the mean function, $E ( x _ { t } ) ~ = ~ \mu _ { t }$ , of a stationary time series is independent of time $t$ , we will write

$$
\mu_ {t} = \mu . \tag {1.22}
$$

Also, because the covariance function of a stationary time series, $\gamma ( s , t )$ , depends on $s$ and $t$ only through their difference $\left| s - t \right|$ , we may simplify the notation. Let $s = t + h$ , where $h$ represents the time shift or lag, then

$$
\begin{array}{l} \gamma (t + h, t) = E \left[ \left(x _ {t + h} - \mu\right) \left(x _ {t} - \mu\right) \right] \\ = E \left[ \left(x _ {h} - \mu\right) \left(x _ {0} - \mu\right) \right] \tag {1.23} \\ = \gamma (h, 0) \\ \end{array}
$$

does not depend on the time argument $t$ ; we have assumed that $\mathrm { v a r } ( x _ { t } ) =$ $\gamma ( 0 , 0 ) < \infty$ . Henceforth, for convenience, we will drop the second argument of $\gamma ( h , 0 )$ .

Definition 1.8 The autocovariance function of a stationary time series will be written as

$$
\gamma (h) = E \left[ \left(x _ {t + h} - \mu\right) \left(x _ {t} - \mu\right) \right]. \tag {1.24}
$$

Definition 1.9 The autocorrelation function (ACF) of a stationary time series will be written using (1.15) as

$$
\rho (h) = \frac {\gamma (t + h , t)}{\sqrt {\gamma (t + h , t + h) \gamma (t , t)}} = \frac {\gamma (h)}{\gamma (0)}. \tag {1.25}
$$

The Cauchy–Schwarz inequality shows again that $- 1 \leq \rho ( h ) \leq 1$ for all $h$ , enabling one to assess the relative importance of a given autocorrelation value by comparing with the extreme values $^ { - 1 }$ and 1.

# Example 1.19 Stationarity of White Noise

The autocovariance function of the white noise series of Examples 1.8 and 1.16 is easily evaluated as

$$
\gamma_ {w} (h) = E (w _ {t + h} w _ {t}) = \left\{ \begin{array}{l l} \sigma_ {w} ^ {2}, & h = 0 \\ 0, & h \neq 0, \end{array} \right.
$$

where, in these examples, $\sigma _ { w } ^ { 2 } = 1$ . This means that the series is weakly stationary or stationary. If the white noise variates are also normally distributed or Gaussian, the series is also strictly stationary, as can be seen by evaluating (1.19) using the relationship (1.9).

![](images/d03eb5445f199da627ea428143abdcc4dab82a7a83057a55f5bc7721dae220ef.jpg)  
Figure 1.12 Autocovariance function of a three-point moving average.

# Example 1.20 Stationarity of a Moving Average

The three-point moving average process used in Examples 1.9 and 1.17 is stationary because we may write the autocovariance function obtained in (1.14) as

$$
\gamma_ {v} (h) = \left\{ \begin{array}{l l} 3 / 9, & h = 0 \\ 2 / 9, & h = \pm 1 \\ 1 / 9, & h = \pm 2 \\ 0, & | h | \geq 3. \end{array} \right.
$$

Figure 1.12 shows a plot of the autocovariance as a function of lag $h$ . Interestingly, the autocovariance is symmetric and decays as a function of lag.

The autocovariance function of a stationary process has several useful properties. First, the value at $h = 0$ , namely

$$
\gamma (0) = E \left[ \left(x _ {t} - \mu\right) ^ {2} \right] \tag {1.26}
$$

is the variance of the time series; note that the Cauchy–Schwarz inequality implies

$$
\left| \gamma (h) \right| \leq \gamma (0).
$$

A final useful property, noted in the previous example, is that autocovariance function of a stationary series is symmetric around the origin, that is,

$$
\gamma (h) = \gamma (- h) \tag {1.27}
$$

for all $h$ . This property follows because shifting the series by $h$ means that

$$
\begin{array}{l} \gamma (h) = \gamma (t + h - t) \\ = E \left[ \left(x _ {t + h} - \mu\right) \left(x _ {t} - \mu\right) \right] \\ = E \left[ \left(x _ {t} - \mu\right) \left(x _ {t + h} - \mu\right) \right] \\ = \gamma (t - (t + h)) \\ = \gamma (- h), \\ \end{array}
$$

# 1.5: Stationary Time Series

which shows how to use the notation as well as proving the result.

When several series are available, a notion of stationarity still applies with additional conditions.

Definition 1.10 Two time series, say, $x _ { t }$ and $y _ { t }$ , are said to be jointly stationary if they are each stationary, and the cross-covariance function

$$
\gamma_ {x y} (h) = E \left[ \left(x _ {t + h} - \mu_ {x}\right) \left(y _ {t} - \mu_ {y}\right) \right] \tag {1.28}
$$

is a function only of lag h.

Definition 1.11 The cross-correlation function (CCF) of jointly stationary time series $x _ { t }$ and $y _ { t }$ is defined as

$$
\rho_ {x y} (h) = \frac {\gamma_ {x y} (h)}{\sqrt {\gamma_ {x} (0) \gamma_ {y} (0)}}. \tag {1.29}
$$

Again, we have the result $- 1 \leq \rho _ { x y } ( h ) \leq 1$ which enables comparison with the extreme values $^ { - 1 }$ and 1 when looking at the relation between $x _ { t + h }$ and $y _ { t }$ . The cross-correlation function satisfies

$$
\rho_ {x y} (h) = \rho_ {y x} (- h), \tag {1.30}
$$

which can be shown by manipulations similar to those used to show (1.27).

# Example 1.21 Joint Stationarity

Consider the two series, $x _ { t }$ and $y _ { t }$ , formed from the sum and difference of two successive values of a white noise process, say,

$$
x _ {t} = w _ {t} + w _ {t - 1}
$$

and

$$
y _ {t} = w _ {t} - w _ {t - 1},
$$

$w _ { t }$ ero and $\sigma _ { w } ^ { 2 }$ $\gamma _ { x } ( 0 ) = \gamma _ { y } ( 0 ) = 2 \sigma _ { w } ^ { 2 }$ $\gamma _ { x } ( 1 ) = \gamma _ { x } ( - 1 ) =$ $\sigma _ { w } ^ { 2 } , \gamma _ { y } ( 1 ) = \gamma _ { y } ( - 1 ) = - \sigma _ { w } ^ { 2 }$

$$
\begin{array}{l} \gamma_ {x y} (1) = E \left[ \left(x _ {t + 1} - 0\right) \left(y _ {t} - 0\right) \right] \\ = E \left[ \left(w _ {t + 1} + w _ {t}\right) \left(w _ {t} - w _ {t - 1}\right) \right] \\ = \sigma_ {w} ^ {2} \\ \end{array}
$$

because only one product is nonzero. Similarly, $\gamma _ { x y } ( 0 ) = 0 , \gamma _ { x y } ( - 1 ) =$ $- \sigma _ { w } ^ { 2 }$ . We obtain, using (1.29),

$$
\rho_ {x y} (h) = \left\{ \begin{array}{c l} 0, & h = 0 \\ 1 / 2, & h = 1 \\ - 1 / 2, & h = - 1 \\ 0, & | h | \geq 2. \end{array} \right.
$$

Clearly, the autocovariance and cross-covariance functions depend only on the lag separation, $h$ , so the series are jointly stationary.

# Example 1.22 Prediction Using Cross-Correlation

As a simple example of cross-correlation, consider the problem of determining possible leading or lagging relations between two series $x _ { t }$ and $y _ { t }$ . If the model

$$
y _ {t} = A x _ {t - \ell} + w _ {t}
$$

holds, the series $x _ { t }$ is said to lead $y _ { t }$ for $\ell > 0$ and is said to lag yt for $\ell < 0$ . Hence, the analysis of leading and lagging relations might be important in predicting the value of $y _ { t }$ from $x _ { t }$ . Assuming, for convenience, that $x _ { t }$ and $y _ { t }$ have zero means, and the noise $w _ { t }$ is uncorrelated with the $x _ { t }$ series, the cross-covariance function can be computed as

$$
\begin{array}{l} \gamma_ {y x} (h) = E \left(y _ {t + h} x _ {t}\right) \\ = A E \left(x _ {t + h - \ell} x _ {t}\right) + E \left(w _ {t + h} x _ {t}\right) \\ = A \gamma_ {x} (h - \ell). \\ \end{array}
$$

The cross-covariance function will look like the autocovariance of the input series $x _ { t }$ , with a peak on the positive side if $x _ { t }$ leads $y _ { t }$ and a peak on the negative side if $x _ { t }$ lags $y _ { t }$ .

The concept of weak stationarity forms the basis for much of the analysis performed with time series. The fundamental properties of the mean and autocovariance functions (1.22) and (1.24) are satisfied by many theoretical models that appear to generate plausible sample realizations. In Examples 1.9 and 1.10, two series were generated that produced stationary looking realizations, and in Example 1.20, we showed that the series in Example 1.9 was, in fact, weakly stationary. Both examples are special cases of the so-called linear process.

Definition 1.12 A linear process, $x _ { t }$ , is defined to be a linear combination of white noise variates $w _ { t }$ , and is given by

$$
x _ {t} = \mu + \sum_ {j = - \infty} ^ {\infty} \psi_ {j} w _ {t - j} \tag {1.31}
$$

where the coefficients satisfy

$$
\sum_ {j = - \infty} ^ {\infty} | \psi_ {j} | <   \infty . \tag {1.32}
$$

For the linear process (see Problem 1.11), we may show that the autocovariance function is given by

$$
\gamma (h) = \sigma_ {w} ^ {2} \sum_ {j = - \infty} ^ {\infty} \psi_ {j + h} \psi_ {j} \tag {1.33}
$$

for $h \geq 0$ ; recall that $\gamma ( - h ) = \gamma ( h )$ . This method exhibits the autocovariance function of the process in terms of the lagged products of the coefficients. Note that, for Example 1.9, we have $\psi _ { 0 } = \psi _ { - 1 } = \psi _ { 1 } = 1 / 3$ and the result in Example 1.20 comes out immediately. The autoregressive series in Example 1.10 can also be put in this form, as can the general autoregressive moving average processes considered in Chapter 3.

Finally, as previously mentioned, an important case in which a weakly stationary series is also strictly stationary is the normal or Gaussian series.

Definition 1.13 A process, $\{ x _ { t } \}$ , is said to be a Gaussian process if the $k$ - dimensional vectors $\pmb { x } = ( x _ { t _ { 1 } } , x _ { t _ { 2 } } , \dots , x _ { t _ { k } } ) ^ { \prime }$ , for every collection of time points $t _ { 1 } , t _ { 2 } , \ldots , t _ { k }$ , and every positive integer $k$ , have a multivariate normal distribution.

Defining the $k \times 1$ mean vector $E ( \pmb { x } ) \equiv \pmb { \mu } = ( \mu _ { t _ { 1 } } , \mu _ { t _ { 2 } } , . . . , \mu _ { t _ { k } } ) ^ { \prime }$ and the $k \times k$ covariance matrix as $\operatorname { c o v } ( \pmb { x } ) \equiv \Gamma = \{ \gamma ( t _ { i } , t _ { j } )$ ; $i , j = 1 , \dots , k \}$ , the multivariate normal density function can be written as

$$
f (\boldsymbol {x}) = (2 \pi) ^ {- n / 2} | \Gamma | ^ {- 1 / 2} \exp \left\{- \frac {1}{2} (\boldsymbol {x} - \boldsymbol {\mu}) ^ {\prime} \Gamma^ {- 1} (\boldsymbol {x} - \boldsymbol {\mu}) \right\}, \tag {1.34}
$$

where $\left| \cdot \right|$ denotes the determinant. This distribution forms the basis for solving problems involving statistical inference for time series. If a Gaussian time series, $\{ x _ { t } \}$ , is weakly stationary, then $\mu _ { t } ~ = ~ \mu$ and $\gamma ( t _ { i } , t _ { j } ) ~ = ~ \gamma ( | t _ { i } - t _ { j } | )$ , so that the vector $\pmb { \mu }$ and the matrix $\Gamma$ are independent of time. These facts imply that all the finite distributions, (1.34), of the series $\{ x _ { t } \}$ depend only on time lag and not on the actual times, and hence the series must be strictly stationary. We use the multivariate normal density in the form given above as well as in a modified version, applicable to complex random variables in the sequel.

# 1.6 Estimation of Correlation

Although the theoretical autocorrelation and cross-correlation functions are useful for describing the properties of certain hypothesized models, most of the analyses must be performed using sampled data. This limitation means the sampled points $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ only are available for estimating the mean, autocovariance, and autocorrelation functions. From the point of view of classical statistics, this poses a problem because we will typically not have iid copies of $x _ { t }$ that are available for estimating the covariance and correlation functions. In the usual situation with only one realization, however, the assumption of stationarity becomes critical. Somehow, we must use averages over this single realization to estimate the population means and covariance functions.

Accordingly, if a time series is stationary, the mean function, (1.22), $\mu _ { t } = \mu$ is constant so that we can estimate it by the sample mean,

$$
\bar {x} = \frac {1}{n} \sum_ {t = 1} ^ {n} x _ {t}. \tag {1.35}
$$

The theoretical autocovariance function, (1.24), is estimated by the sample autocovariance function defined as follows.

Definition 1.14 The sample autocovariance function is defined as

$$
\widehat {\gamma} (h) = n ^ {- 1} \sum_ {t = 1} ^ {n - h} \left(x _ {t + h} - \bar {x}\right) \left(x _ {t} - \bar {x}\right), \tag {1.36}
$$

with ${ \widehat { \gamma } } ( - h ) = { \widehat { \gamma } } ( h )$ for $h = 0 , 1 , \ldots , n - 1$ .

The sum in (1.36) runs over a restricted range because $x _ { t + h }$ is not available for $t + h > n$ . The estimator in (1.36) is generally preferred to the one that would be obtained by dividing by $n { - } h$ because (1.36) is a non-negative definite function. This means that if we let $\widehat { \Gamma } = \{ \widehat { \gamma } ( i - j )$ ; $i , j = 1 , . . . , n \}$ be the $n \times n$ sample covariance matrix of the data $\pmb { x } = ( x _ { 1 } , \ldots , x _ { n } ) ^ { \prime }$ , then $\hat { \Gamma }$ is a non-negative definite matrix. So, if we let $\pmb { a } = ( a _ { 1 } , \ldots , a _ { n } ) ^ { \prime }$ be an $n \times 1$ vector of constants, then ${ \widehat { \mathrm { v a r } } } ( { \pmb a } ^ { \prime } { \pmb x } ) = { \pmb a } ^ { \prime } \widehat { \Gamma } { \pmb a } \geq 0$ . Thus, the non-negative definite property ensures sample variances of linear combinations of the variates $x _ { t }$ will always be nonnegative. Note that neither dividing by $n$ nor $n - h$ in (1.36) yields an unbiased estimate of $\gamma ( h )$ .

Definition 1.15 The sample autocorrelation function is defined, analogously to (1.25), as

$$
\widehat {\rho} (h) = \frac {\widehat {\gamma} (h)}{\widehat {\gamma} (0)}. \tag {1.37}
$$

The sample autocorrelation function has a sampling distribution that allows us to assess whether the data comes from a completely random or white series or whether correlations are statistically significant at some lags. Precise details are given in Theorem A.7 in Appendix A. We have

# Property P1.1: Large Sample Distribution of the ACF

Under general conditions, if $x _ { t }$ is white noise, then for n large, the sample ACF, ${ \widehat { \rho } } _ { x } ( h )$ , for $h = 1 , 2 , \ldots , H$ , where $H$ is fixed but arbitrary, is approximately normally distsributed with zero mean and standard deviation given by

$$
\sigma_ {\hat {\rho} _ {x} (h)} = \frac {1}{\sqrt {n}}. \tag {1.38}
$$

Based on the above result, we obtain a rough method of assessing whether peaks in ${ \widehat { \rho } } ( h )$ are significant by determining whether the observed peak is

outside the interval $\pm 2 / \sqrt { n }$ (or plus/minus two standard errors); for a white noise sequence, approximately 95% of the sample ACFs should be within these limits. The applications of this property develop because many statistical modeling procedures depend on reducing a time series to a white noise series by various kinds of transformations. After such a procedure is applied, the plotted ACFs of the residuals should then lie roughly within the limits given above.

Definition 1.16 The estimators for the cross-covariance function, $\gamma _ { x y } ( h )$ , as given in (1.28) and the cross-correlation, $\rho _ { x y } ( h )$ , in (1.29), are given, respectively, by the sample cross-covariance function

$$
\widehat {\gamma} _ {x y} (h) = n ^ {- 1} \sum_ {t = 1} ^ {n - h} \left(x _ {t + h} - \bar {x}\right) \left(y _ {t} - \bar {y}\right), \tag {1.39}
$$

where $\widehat { \gamma } _ { x y } ( - h ) = \widehat { \gamma } _ { y x } ( h )$ determines the function for negative lags, and the sample cross-correlation function

$$
\widehat {\rho} _ {x y} (h) = \frac {\widehat {\gamma} _ {x y} (h)}{\sqrt {\widehat {\gamma} _ {x} (0) \widehat {\gamma} _ {y} (0)}}. \tag {1.40}
$$

The sample cross-correlation function can be examined graphically as a function of lag $h$ to search for leading or lagging relations in the data using the property mentioned in Example 1.22 for the theoretical cross-covariance function. Because $- 1 \leq \widehat { \rho } _ { x y } ( h ) \leq 1$ , the practical importance of peaks can be assessed by comparing their magnitudes with their theoretical maximum values. Furthermore, for $x _ { t }$ and $y _ { t }$ independent linear processes of the form (1.31), we have

Property P1.2: Large Sample Distribution of the Cross-Correlation Under Independence

The large sample distribution of ${ \widehat { \rho } } _ { x y } ( h )$ is normal with mean zero and

$$
\sigma_ {\hat {\rho} _ {x y}} = \frac {1}{\sqrt {n}} \tag {1.41}
$$

if at least one of the processes is white independent noise (see Theorem A.8 in Appendix A).

# Example 1.23 A Simulated Time Series

To give an example of the procedure for calculating numerically the autocovariance and cross-covariance functions, consider a contrived set of data generated by tossing a fair coin, letting $x _ { t } ~ = ~ 1$ when a head is obtained and $x _ { t } = - 1$ when a tail is obtained. Construct $y _ { t }$ as

$$
y _ {t} = 5 + x _ {t} - . 7 x _ {t - 1}. \tag {1.42}
$$

Table 1.1 Sample Realization of the Contrived Series $y _ { t }$   

<table><tr><td>t</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>Coin</td><td>H</td><td>H</td><td>T</td><td>H</td><td>T</td><td>T</td><td>T</td><td>H</td><td>T</td><td>H</td></tr><tr><td>xt</td><td>1</td><td>1</td><td>-1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>-1</td><td>1</td></tr><tr><td>yt</td><td>6.7</td><td>5.3</td><td>3.3</td><td>6.7</td><td>3.3</td><td>4.7</td><td>4.7</td><td>6.7</td><td>3.3</td><td>6.7</td></tr><tr><td>yt - y</td><td>1.56</td><td>.16</td><td>-1.84</td><td>1.56</td><td>-1.84</td><td>-.44</td><td>-.44</td><td>1.56</td><td>-1.84</td><td>1.56</td></tr></table>

Table 1.1 shows sample realizations of the appropriate processes with $x _ { 0 } = - 1$ and $n = 1 0$ .

The sample autocorrelation for the series $y _ { t }$ can be calculated using (1.36) and (1.37) for $h = 0 , 1 , 2 , \ldots$ . It is not necessary to calculate for negative values because of the symmetry. For example, for $h = 3$ , the autocorrelation becomes the ratio of

$$
\begin{array}{l} \widehat {\gamma} _ {y} (3) = 1 0 ^ {- 1} \sum_ {t = 1} ^ {7} \left(y _ {t + 3} - \bar {y}\right) \left(y _ {t} - \bar {y}\right) \\ = 1 0 ^ {- 1} \left[ (1. 5 6) (1. 5 6) + (- 1. 8 4) (. 1 6) + (-. 4 4) (- 1. 8 4) \right. \\ + (-. 4 4) (1. 5 6) + (1. 5 6) (- 1. 8 4) + (- 1. 8 4) (-. 4 4) \\ + (1. 5 6) (-. 4 4) \bigg ] \\ = -. 0 4 8 4 8 \\ \end{array}
$$

to

$$
\widehat {\gamma} _ {y} (0) = \frac {1}{1 0} [ (1. 5 6) ^ {2} + (. 1 6) ^ {2} + \dots + (1. 5 6) ^ {2} ] = 2. 0 3 0 4
$$

so that

$$
\widehat {\rho} _ {y} (3) = \frac {- . 0 4 8 4 8}{2 . 0 3 0 4} = -. 0 2 3 8 8.
$$

The theoretical ACF can be obtained from the model (1.42) using the fact that the mean of $x _ { t }$ is zero and the variance of $x _ { t }$ is one. It can be shown that

$$
\rho_ {y} (1) = \frac {- . 7}{1 + . 7 ^ {2}} = -. 4 7
$$

and $\rho _ { y } ( h ) = 0$ for $| h | > 1$ (Problem 1.23). Table 1.2 compares the theoretical ACF with sample ACFs for a realization where $n = 1 0$ and another realization where $n = 1 0 0$ ; we note the increased variability in the smaller size sample.

Table 1.2 Theoretical and Sample ACFs for $n = 1 0$ and $n = 1 0 0$   

<table><tr><td>h</td><td>ρy(h)</td><td>ρy(h) n = 10</td><td>ρy(h) n = 100</td></tr><tr><td>0</td><td>1.00</td><td>1.00</td><td>1.00</td></tr><tr><td>±1</td><td>-.47</td><td>-.55</td><td>-.45</td></tr><tr><td>±2</td><td>.00</td><td>.17</td><td>-.12</td></tr><tr><td>±3</td><td>.00</td><td>-.02</td><td>.14</td></tr><tr><td>±4</td><td>.00</td><td>.15</td><td>.01</td></tr><tr><td>±5</td><td>.00</td><td>-.46</td><td>-.01</td></tr></table>

# Example 1.24 ACF of Speech Signal

Computing the sample ACF as in the previous example can be thought of as matching the time series $h$ units in the future, say, $x _ { t + h }$ against itself, $x _ { t }$ . Figure 1.13 shows the ACF of the speech series of Figure 1.3. The original series appears to contain a sequence of repeating short signals. The ACF confirms this behavior, showing repeating peaks spaced at about 106-109 points. Autocorrelation functions of the short signals appear, spaced at the intervals mentioned above. The distance between the repeating signals is known as the pitch period and is a fundamental parameter of interest in systems that encode and decipher speech. Because the series is sampled at 10,000 points per second, the pitch period appears to be between .0106 and .0109 seconds.

To compute the sample ACF in R, use

$$
\begin{array}{l} > \text {s p e c h} = \text {s c a n} (\prime \prime / \text {m y d a t a} / \text {s p e c h . d a t} ^ {\prime \prime}) \\ > a c f (s p e e c h, 2 5 0) \\ \end{array}
$$

# Example 1.25 Correlation Analysis of SOI and Recruitment Data

The autocorrelation and cross-correlation functions are also useful for analyzing the joint behavior of two stationary series whose behavior may be related in some unspecified way. In Example 1.5 (see Figure 1.5), we have considered simultaneous monthly readings of the SOI and the number of new fish (Recruitment) computed from a model. Figure 1.14 shows the autocorrelation and cross-correlation functions (ACFs and CCF) for these two series. Both of the ACFs exhibit periodicities corresponding to the correlation between values separated by 12 units. Observations 12 months or one year apart are strongly positively correlated, as are observations at multiples such as $2 4 , 3 6 , 4 8 , \ldots$ Observations separated by six months are negatively correlated, showing that positive excursions tend to be associated with negative excursions six months removed. This appearance is rather characteristic of the pattern that would be produced by

![](images/531214c40d49d4217bfc90b820e2388cc034534e4506bc6cb7a4595c436bc800.jpg)  
Figure 1.13 ACF of the speech series.

a sinusoidal component with a period of 12 months. The cross-correlation function peaks at $h = - 6$ , showing that the SOI measured at time $t - 6$ months is associated with the Recruitment series at time $t$ . We could say the SOI leads the Recruitment series by six months. The sign of the ACF is negative, leading to the conclusion that the two series move in different directions, i.e., increases in SOI lead to decreases in Recruitment and vice versa. Again, note the periodicity of 12 months in the CCF. The flat lines shown on the plots indicate $\pm 2 / \sqrt { 4 5 3 }$ , so that upper values would be exceeded about $2 . 5 \%$ of the time if the noise were white [see (1.38) and (1.41)].

To reproduce Figure 1.14 in R, use the following commands.

> soi=scan("/mydata/soi.dat")   
> rec=scan("/mydata/recruit.dat")   
> par(mfrow=c(3,1))   
> acf(soi, 50)   
> acf(rec, 50)   
> ccf(soi, rec, 50)

# 1.7 Vector-Valued and Multidimensional Series

We frequently encounter situations in which the relationships between a number of jointly measured time series are of interest. For example, in the previous sections, we considered discovering the relationships between the SOI and Recruitment series. Hence, it will be useful to consider the notion of a vector time

![](images/1cd42e340cc46a7dda1c96067af1b4a8384c4eea3e35d5cba456305e4614b790.jpg)

![](images/056993d453a50f5fe9a3cabea1ba2df2a4947db5236099d240bb37038f90926a.jpg)

![](images/8d69094aba855a85e84618c71d18134f4f7bbbf17bfe30580b7cecab4c9eef91.jpg)  
Figure 1.14 Sample ACFs of the SOI series (top) and of the Recruitment series (middle), and the sample CCF of the two series (bottom); negative lags indicate SOI leads Recruitment.

series $\pmb { x } _ { t } = ( x _ { t 1 } , x _ { t 2 } , . . . , x _ { t p } ) ^ { \prime }$ , which contains as its components $p$ univariate time series. We denote the $p \times 1$ column vector of the observed series as ${ \pmb x } _ { t }$ . The row vector $\pmb { x } _ { t } ^ { \prime }$ is its transpose. For the stationary case, the $p \times 1$ mean vector

$$
\boldsymbol {\mu} = E \left(\boldsymbol {x} _ {t}\right) \tag {1.43}
$$

of the form $\pmb { \mu } = ( \mu _ { t 1 } , \mu _ { t 2 } , . . . , \mu _ { t p } ) ^ { \prime }$ and the $p \times p$ autocovariance matrix

$$
\Gamma (h) = E \left[ \left(\boldsymbol {x} _ {t + h} - \boldsymbol {\mu}\right) \left(\boldsymbol {x} _ {t} - \boldsymbol {\mu}\right) ^ {\prime} \right] \tag {1.44}
$$

can be defined, where the elements of the matrix $\Gamma ( h )$ are the cross-covariance

functions

$$
\gamma_ {i j} (h) = E \left[ \left(x _ {t + h, i} - \mu_ {i}\right) \left(x _ {t j} - \mu_ {j}\right) \right] \tag {1.45}
$$

for $i , j = 1 , \dotsc , p$ . Because $\gamma _ { i j } ( h ) = \gamma _ { j i } ( - h )$ , it follows that

$$
\Gamma (- h) = \Gamma^ {\prime} (h). \tag {1.46}
$$

Now, the sample autocovariance matrix of the vector series ${ \pmb x } _ { t }$ is the $p \times p$ matrix of sample cross-covariances, defined as

$$
\widehat {\Gamma} (h) = n ^ {- 1} \sum_ {t = 1} ^ {n - h} \left(\boldsymbol {x} _ {t + h} - \bar {\boldsymbol {x}}\right) \left(\boldsymbol {x} _ {t} - \bar {\boldsymbol {x}}\right) ^ {\prime}, \tag {1.47}
$$

where

$$
\bar {\boldsymbol {x}} = n ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t} \tag {1.48}
$$

denotes the $p \times 1$ sample mean vector. The symmetry property of the theoretical autocovariance (1.46) extends to the sample autocovariance (1.47), which is defined for negative values by taking

$$
\widehat {\Gamma} (- h) = \widehat {\Gamma} (h) ^ {\prime}. \tag {1.49}
$$

In many applied problems, an observed series may be indexed by more than time alone. For example, the position in space of an experimental unit might be described by two coordinates, say, $s _ { 1 }$ and $s _ { 2 }$ . We may proceed in these cases by defining a multidimensional process $x \pmb { s }$ as a function of the $r \times 1$ vector $\pmb { \mathscr { s } } = ( \mathscr { s } _ { 1 } , \mathscr { s } _ { 2 } , \mathscr { . ~ . ~ . ~ } , \mathscr { s } _ { r } ) ^ { \prime }$ where $s _ { i }$ denotes the coordinate of the $i ^ { t h }$ index.

# Example 1.26 Soil Surface Temperatures

As an example, the two-dimensional ( $r = 2$ ) temperature series $x _ { s _ { 1 } , s _ { 2 } }$ in Figure 1.15 is indexed by a row number $s _ { 1 }$ and a column number $s _ { 2 }$ that represent positions on a $6 4 \times 3 6$ spatial grid set out on an agricultural field. The value of the temperature measured at row $s _ { 1 }$ and column $s _ { 2 }$ , is denoted by $x _ { \pmb { S } } = x _ { s 1 , s 2 }$ . We can note from the two-dimensional plot that a distinct change occurs in the character of the two-dimensional surface starting at about row 40, where the oscillations along the row axis become fairly stable and periodic. For example, averaging over the 36 columns, we may compute an average value for each $s _ { 1 }$ as in Figure 1.16. It is clear that the noise present in the first part of the two-dimensional series is nicely averaged out, and we see a clear and consistent temperature signal.

The autocovariance function of a stationary multidimensional process, $x \pmb { s }$ , can be defined as a function of the multidimensional lag vector, say, $\textbf { \em h } =$ $( h _ { 1 } , h _ { 2 } , \ldots , h _ { r } ) ^ { \prime }$ , as

$$
\gamma (\boldsymbol {h}) = E \left[ \left(x _ {\boldsymbol {s} + \boldsymbol {h}} - \mu\right) \left(x _ {\boldsymbol {s}} - \mu\right) \right], \tag {1.50}
$$

![](images/bacb403c6d04bbccc439f831c2bd1987df08d54c5c2a40b07c1b41fccb5427d8.jpg)  
Figure 1.15 Two-dimensional time series of temperature measurements taken on a rectangular field ( $6 4 \times 3 6$ with 17-foot spacing). Data are from Bazza et al. (1988).

where

$$
\mu = E \left(x _ {\boldsymbol {s}}\right) \tag {1.51}
$$

does not depend on the spatial coordinate $\pmb { s }$ . For the two dimensional temperature process, (1.50) becomes

$$
\gamma \left(h _ {1}, h _ {2}\right) = E \left[ \left(x _ {s _ {1} + h _ {1}, s _ {2} + h _ {2}} - \mu\right) \left(x _ {s _ {1}, s _ {2}} - \mu\right) \right], \tag {1.52}
$$

which is a function of lag, both in the row ( $h _ { 1 }$ ) and column ( $h _ { 2 }$ ) directions.

The multidimensional sample autocovariance function is defined as

$$
\widehat {\gamma} (\boldsymbol {h}) = \left(S _ {1} S _ {2} \dots S _ {r}\right) ^ {- 1} \sum_ {s _ {1}} \sum_ {s _ {2}} \dots \sum_ {s _ {r}} \left(x _ {\boldsymbol {s} + \boldsymbol {h}} - \bar {x}\right) \left(x _ {\boldsymbol {s}} - \bar {x}\right), \tag {1.53}
$$

where $\pmb { \mathscr { s } } = ( \mathscr { s } _ { 1 } , \mathscr { s } _ { 2 } , \mathscr { . . . } , \mathscr { s } _ { r } ) ^ { \prime }$ and the range of summation for each argument is $1 \leq s _ { i } \leq S _ { i } - h _ { i }$ , for $i = 1 , \dots , r$ . The mean is computed over the $r$ -dimensional array, that is,

$$
\bar {x} = \left(S _ {1} S _ {2} \dots S _ {r}\right) ^ {- 1} \sum_ {s _ {1}} \sum_ {s _ {2}} \dots \sum_ {s _ {r}} x _ {s _ {1}, s _ {2}, \dots , s _ {r}}, \tag {1.54}
$$

where the arguments $s _ { i }$ are summed over $1 \leq s _ { i } \leq S _ { i }$ . The multidimensional sample autocorrelation function follows, as usual, by taking the scaled ratio

$$
\widehat {\rho} (\boldsymbol {h}) = \frac {\widehat {\gamma} (\boldsymbol {h})}{\widehat {\gamma} (\boldsymbol {0})}. \tag {1.55}
$$

![](images/36a4f068d01318a5bfe2660813d0bd04d5ae7fb125045f1d2c5d19910ac98713.jpg)  
Figure 1.16 Row averages of the two-dimensional soil temperature profile. $\bar { x } _ { s _ { 1 } } = \sum _ { s _ { 2 } } x _ { s _ { 1 } , s _ { 2 } } / 3 6$ .

# Example 1.27 Sample ACF of the Soil Temperature Series

The autocorrelation function of the two-dimensional temperature process can be written in the form

$$
\widehat {\rho} (h _ {1}, h _ {2}) = \frac {\widehat {\gamma} (h _ {1} , h _ {2})}{\widehat {\gamma} (0 , 0)},
$$

where

$$
\widehat {\gamma} (h _ {1}, h _ {2}) = \left(S _ {1} S _ {2}\right) ^ {- 1} \sum_ {s _ {1}} \sum_ {s _ {2}} \left(x _ {s _ {1} + h _ {1}, s _ {2} + h _ {2}} - \bar {x}\right) \left(x _ {s _ {1}, s _ {2}} - \bar {x}\right)
$$

Figure 1.17 shows the autocorrelation function for the temperature data, and we note the systematic periodic variation that appears along the rows. The autocovariance over columns seems to be strongest for $h _ { 1 } = 0$ , implying columns may form replicates of some underlying process that has a periodicity over the rows. This idea can be investigated by examining the mean series over columns as shown in Figure 1.16.

The sampling requirements for multidimensional processes are rather severe because values must be available over some uniform grid in order to compute the ACF. In some areas of application, such as in soil science, we may prefer to sample a limited number of rows or transects and hope these are essentially replicates of the basic underlying phenomenon of interest. One-dimensional methods can then be applied. When observations are irregular in time space, modifications to the estimators need to be made. Systematic approaches to the

![](images/e0db53770b2649137d6523660e41ee84385cfe05486eb522f1adf569e21a0026.jpg)  
Figure 1.17 Two-dimensional autocorrelation function for the soil temperature data.

problems introduced by irregularly spaced observations have been developed by Journel and Huijbregts (1978) or Cressie (1993). We shall not pursue such methods in detail here, but it is worth noting that the introduction of the variogram

$$
2 V _ {x} (\boldsymbol {h}) = \operatorname {v a r} \left\{x _ {\boldsymbol {s} + \boldsymbol {h}} - x _ {\boldsymbol {s}} \right\} \tag {1.56}
$$

and its sample estimator

$$
2 \widehat {V} _ {x} (\boldsymbol {h}) = \frac {1}{N (\boldsymbol {h})} \sum_ {\boldsymbol {s}} \left(x _ {\boldsymbol {s} + \boldsymbol {h}} - x _ {\boldsymbol {s}}\right) ^ {2} \tag {1.57}
$$

play key roles, where $N ( \pmb { h } )$ denotes both the number of points located within $\pmb { h }$ , and the sum runs over the points in the neighborhood. Clearly, substantial indexing difficulties will develop from estimators of the kind, and often it will be difficult to find non-negative definite estimators for the covariance function. Problem 1.26 investigates the relation between the variogram and the autocovariance function in the stationary case.

# Problems

# Section 1.2

1.1 To compare the earthquake and explosion signals, plot the data displayed in Figure 1.7 on the same graph using different colors or different line types and comment on the results.

1.2 Consider a signal plus noise model of the general form $x _ { t } = s _ { t } + w _ { t }$ , where $w _ { t }$ is Gaussian white noise with $\sigma _ { w } ^ { 2 } = 1$ . Simulate and plot $n =$ 200 observations from each of the following two models (Save the data generated here for use in Problem 1.21 ):

(a) $x _ { t } = s _ { t } + w _ { t }$ , for $t = 1 , . . . , 2 0 0$ , where

$$
s _ {t} = \left\{ \begin{array}{l l} 0, & t = 1,..., 1 0 0 \\ 1 0 \exp \{- \frac {(t - 1 0 0)}{2 0} \} \cos (2 \pi t / 4), & t = 1 0 1,..., 2 0 0. \end{array} \right.
$$

(b) $x _ { t } = s _ { t } + w _ { t }$ , for $t = 1 , . . . , 2 0 0$ , where

$$
s _ {t} = \left\{ \begin{array}{l l} 0, & t = 1,..., 1 0 0 \\ 1 0 \exp \{- \frac {(t - 1 0 0)}{2 0 0} \} \cos (2 \pi t / 4), & t = 1 0 1,..., 2 0 0. \end{array} \right.
$$

(c) Compare the general appearance of the series (a) and (b) with the earthquake series and the explosion series shown in Figure 1.7. In addition, plot (or sketch) and compare the signal modulators (a) $\exp \{ - t / 2 0 \}$ and (b) $\exp \{ - t / 2 0 0 \}$ , for $t = 1 , 2 , . . . , 1 0 0$ .

# Section 1.3

1.3 (a) Generate $n = 1 0 0$ observations from the autoregression

$$
x _ {t} = -. 9 x _ {t - 2} + w _ {t}
$$

with $\sigma _ { w } = 1$ , using the method described in Example 1.10. Next, apply the moving average filter

$$
v _ {t} = (x _ {t} + x _ {t - 1} + x _ {t - 2} + x _ {t - 3}) / 4
$$

to $x _ { t }$ , the data you generated. Now plot $x _ { t }$ as a line and superimpose $v _ { t }$ as a dashed line. Comment on the behavior of $x _ { t }$ and how applying the moving average filter changes that behavior.

(b) Repeat (a) but with

$$
x _ {t} = \cos (2 \pi t / 4).
$$

(c) Repeat (b) but with added $\mathrm { { N } } ( 0 , 1 )$ noise,

$$
x _ {t} = \cos (2 \pi t / 4) + w _ {t}.
$$

(d) Compare and contrast (a)–(c).

# Section 1.4

1.4 Show that the autocovariance function can be written as

$$
\gamma (s, t) = E \left[ \left(x _ {s} - \mu_ {s}\right) \left(x _ {t} - \mu_ {t}\right) \right] = E \left(x _ {s} x _ {t}\right) - \mu_ {s} \mu_ {t},
$$

where $E [ x _ { t } ] = \mu _ { t }$

1.5 For the two series, $x _ { t }$ , in Problem 1.2 (a) and (b):

(a) compute and sketch the mean functions $\mu _ { x } ( t )$ ; for $t = 1 , \ldots , 2 0 0$   
(b) calculate the autocovariance functions, $\gamma _ { x } ( s , t )$ , for $s , t = 1 , \ldots , 2 0 0$ .

# Section 1.5

1.6 Consider the time series

$$
x _ {t} = \beta_ {1} + \beta_ {2} t + w _ {t},
$$

where with v $\beta _ { 1 }$ and ance $\beta _ { 2 }$ are known constants and . $w _ { t }$ is a white noise process $\sigma _ { w } ^ { 2 }$

(a) Determine whether $x _ { t }$ is stationary.   
(b) Show that the process $y _ { t } = x _ { t } - x _ { t - 1 }$ is stationary.   
(c) Show that the mean of the moving average

$$
v _ {t} = \frac {1}{2 q + 1} \sum_ {j = - q} ^ {q} x _ {t - j}
$$

is $\beta _ { 1 } + \beta _ { 2 } t$ , and give a simplified expression for the autocovariance function.

1.7 For a moving average process of the form

$$
x _ {t} = w _ {t - 1} + 2 w _ {t} + w _ {t + 1},
$$

where $w _ { t }$ are independent with zero means and variance $\sigma _ { w } ^ { 2 }$ , determine the autocovariance and autocorrelation functions as a function of lag $h = s - t$ and plot.

1.8 Consider the randow walk with drift model

$$
x _ {t} = \delta + x _ {t - 1} + w _ {t},
$$

for $t = 1 , 2 , \ldots$ , with $x _ { 0 } = 0$ , where $w _ { t }$ is white noise with variance $\sigma _ { w } ^ { 2 }$

(a) Show that the model can be written as $\begin{array} { r } { \boldsymbol { x } _ { t } = \boldsymbol { \delta t } + \sum _ { k = 1 } ^ { t } \boldsymbol { w } _ { k } } \end{array}$

(b) Find the mean function and the autocovariance function of $x _ { t }$   
(c) Show $\begin{array} { r } { \rho _ { x } ( t - 1 , t ) = \sqrt { \frac { t - 1 } { t } } \to 1 } \end{array}$ as $t \to \infty$ . What is the implication of this result?   
(d) Show that the series is not stationary.   
(e) Suggest a transformation to make the series stationary, and prove that the transformed series is stationary. (Hint: See Problem 1.6b.)

1.9 A time series with a periodic component can be constructed from

$$
x _ {t} = U _ {1} \sin (2 \pi \omega_ {0} t) + U _ {2} \cos (2 \pi \omega_ {0} t),
$$

where $U _ { 1 }$ and $U _ { 2 }$ are independent random variables with zero means and $E ( U _ { 1 } ^ { 2 } ) = E ( U _ { 2 } ^ { 2 } ) = \sigma ^ { 2 }$ . The constant $\omega _ { 0 }$ determines the period or time it takes the process to make one complete cycle. Show that this series is weakly stationary with autocovariance function

$$
\gamma (h) = \sigma^ {2} \cos (2 \pi \omega_ {0} h).
$$

1.10 Suppose we would like to predict a single stationary series $x _ { t }$ with zero mean and autocorrelation function $\gamma ( h )$ at some time in the future, say, $t + \ell$ , for $\ell > 0$ .

(a) If we predict using only $x _ { t }$ and some scale multiplier $A$ , show that the mean-square prediction error

$$
M S E (A) = E \left[ \left(x _ {t + \ell} - A x _ {t}\right) ^ {2} \right]
$$

is minimized by the value

$$
A = \rho (\ell).
$$

(b) Show that the minimum mean-square prediction error is

$$
M S E (A) = \gamma (0) [ 1 - \rho^ {2} (\ell) ].
$$

(c) Show that if $x _ { t + \ell } = A x _ { t }$ , then $\rho ( \ell ) = 1$ if $A > 0$ , and $\rho ( \ell ) = - 1$ if $A < 0$ .

1.11 Consider the linear process defined in (1.31).

(a) Verify that the autocovariance function of the process is given by (1.33). Use the result to verify your answer to Problem 1.7.   
(b) Show that $x _ { t }$ exists as a limit in mean square (see Appendix A) if (1.32) holds.

1.12 For two weakly stationary series $x _ { t }$ and $y _ { t }$ , verify (1.30).

1.13 Consider the two series

$$
x _ {t} = w _ {t}
$$

$$
y _ {t} = w _ {t} - \theta w _ {t - 1} + u _ {t},
$$

where $w _ { t }$ and $u _ { t }$ are independent white noise series with variances $\sigma _ { w } ^ { 2 }$ and $\sigma _ { u } ^ { 2 }$ , respectively, and $\theta$ is an unspecified constant.

(a) Express the ACF, function of $\sigma _ { w } ^ { 2 } , \sigma _ { u } ^ { 2 }$ , and $\rho _ { y } ( h )$ $\theta$ , for . $h = 0 , \pm 1 , \pm 2 , . . .$ of the series $y _ { t }$ as a   
(b) Determine the CCF, $\rho _ { x y } ( h )$ relating $x _ { t }$ and $y _ { t }$   
(c) Show that $x _ { t }$ and $y _ { t }$ are jointly stationary.

1.14 Let $x _ { t }$ be a stationary normal process with mean $\mu _ { x }$ and autocovariance function $\gamma ( h )$ . Define the nonlinear time series

$$
y _ {t} = \exp \left\{x _ {t} \right\}.
$$

(a) Express the mean function $E ( y _ { t } )$ in terms of $\mu _ { x }$ and $\gamma ( 0 )$ . The moment generating function of a normal random variable $x$ with mean $\mu$ and variance $\sigma ^ { 2 }$ is

$$
M _ {x} (\lambda) = E [ \exp \{\lambda x \} ] = \exp \biggl \{\mu \lambda + \frac {1}{2} \sigma^ {2} \lambda^ {2} \biggr \}.
$$

(b) Determine the autocovariance function of $y _ { t }$ . The sum of the two normal random variables $x _ { t + h } + x _ { t }$ is still a normal random variable.

1.15 Let $w _ { t }$ , for $t = 0 , \pm 1 , \pm 2 , . . .$ be a normal white noise process, and consider the series

$$
x _ {t} = w _ {t} w _ {t - 1}.
$$

Determine the mean and autocovariance function of $x _ { t }$ , and state whether it is stationary.

1.16 Consider the series

$$
x _ {t} = \sin (2 \pi U t),
$$

$t = 1 , 2 , \ldots$ , where $U$ has a uniform distribution on the interval $( 0 , 1 )$ .

(a) Prove $x _ { t }$ is weakly stationary.   
(b) Prove $x _ { t }$ is not strictly stationary. [Hint: consider the joint bivariate cdf (1.19) at the points $t = 1 , s = 2$ with $h = 1$ , and find values of $c _ { t } , c _ { s }$ where strict stationarity does not hold.]

1.17 Suppose we have the linear process $x _ { t }$ generated by

$$
x _ {t} = w _ {t} - \theta w _ {t - 1},
$$

$t = 0 , 1 , 2 , \ldots$ , where $\{ w _ { t } \}$ is independent and identically distributed with characteristic function $\phi _ { w } ( \cdot )$ , and $\theta$ is a fixed constant.

(a) Express the joint characteristic function of $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ , say,

$$
\phi_ {x _ {1}, x _ {2}, \dots , x _ {n}} (\lambda_ {1}, \lambda_ {2}, \dots , \lambda_ {n}),
$$

in terms of $\phi _ { w } ( \cdot )$

(b) Deduce from (a) that $x _ { t }$ is strictly stationary.

1.18 Suppose that $x _ { t }$ is a linear process of the form (1.31) satisfying the absolute summability condition (1.32). Prove

$$
\sum_ {h = - \infty} ^ {\infty} | \gamma (h) | <   \infty .
$$

# Section 1.6

1.19 (a) Simulate a series of $n = 5 0 0$ Gaussian white noise observations as in Example 1.8 and compute the sample ACF, ${ \widehat { \rho } } ( h )$ , to lag 20. Compare the sample ACF you obtain to the actual ACF, $\rho ( h )$ . [Recall Example 1.19.]

(b) Repeat part (a) using only $n = 5 0$ . How does changing $n$ affect the results?

1.20 (a) Simulate a series of $n \ = \ 5 0 0$ moving average observations as in Example 1.9 and compute the sample ACF, ${ \widehat { \rho } } ( h )$ , to lag 20. Compare the sample ACF you obtain to the actual ACF, $\rho ( h )$ . [Recall Example 1.20.]

(b) Repeat part (a) using only $n = 5 0$ . How does changing $n$ affect the results?

1.21 Although the model in Problem 1.2 is not stationary (Why?), the sample ACF can be informative. For the data you generated in that problem, calculate and plot the sample ACF, and then comment.

1.22 Simulate a series of $n = 5 0 0$ observations from the signal-plus-noise model presented in Example 1.12 with $\sigma _ { w } ^ { 2 } = 1$ . Compute the sample ACF to lag 100 of the data you generated and comment.

1.23 For the time series $y _ { t }$ described in Example 1.23, verify the stated result that $\rho _ { y } ( 1 ) = - . 4 7$ and $\rho _ { y } ( h ) = 0$ for $h > 1$ .

1.24 A real-valued function $g ( t )$ , defined on the integers, is non-negative definite if and only if

$$
\sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} a _ {s} g (s - t) a _ {t} \geq 0
$$

for all positive integers $n$ and for all vectors $\pmb { a } = ( a _ { 1 } , a _ { 2 } , \ldots , a _ { n } ) ^ { \prime }$ . For the matrix $G = \{ g ( s - t ) , s , t = 1 , 2 , \ldots , n \}$ , this implies that ${ \pmb a } ^ { \prime } G { \pmb a } \geq 0$ for all vectors $\textbf { \em a }$ .

# Problems

(a) Prove that $\gamma ( h )$ , the autocovariance function of a stationary process, is a non-negative definite function.   
(b) Verify that the sample autocovariance $\widehat { \gamma } ( h )$ is a non-negative definite function.

# Section 1.7

1.25 Consider a collection of time series $x _ { 1 t } , x _ { 2 t } , \dotsc , x _ { N t }$ that are observing some common signal $\mu _ { t }$ observed in noise processes $e _ { 1 t } , e _ { 2 t } , \ldots , e _ { N t }$ , with a model for the $j$ -th observed series given by

$$
x _ {j t} = \mu_ {t} + e _ {j t}.
$$

Suppose the noise series have zero means and are uncorrelated for different $j$ . The common autocovariance functions of all series are given by $\gamma _ { e } ( s , t )$ . Define the sample mean

$$
\bar {x} _ {t} = \frac {1}{N} \sum_ {j = 1} ^ {N} x _ {j t}.
$$

(a) Show that $E [ \bar { x } _ { t } ] = \mu _ { t }$   
(b) Show that $E [ ( \bar { x } _ { t } - \mu ) ^ { 2 } ) ] = N ^ { - 1 } \gamma _ { e } ( t , t )$   
(c) How can we use the results in estimating the common signal?

1.26 A concept used in geostatistics, see Journel and Huijbregts (1978) or Cressie (1993), is that of the variogram, defined for a spatial process $x \pmb { s }$ , $\pmb { s } = ( s _ { 1 } , s _ { 2 } )$ , for $s _ { 1 } , s _ { 2 } = 0 , \pm 1 , \pm 2 , \ldots$ , as

$$
V _ {x} (\pmb {h}) = \frac {1}{2} E [ (x _ {\pmb {s} + \pmb {h}} - x _ {\pmb {s}}) ^ {2} ],
$$

where $\pmb { h } = ( h _ { 1 } , h _ { 2 } )$ , for $h _ { 1 } , h _ { 2 } = 0 , \pm 1 , \pm 2 , \ldots$ Show that, for a stationary process, the variogram and autocovariance functions can be related through

$$
V _ {x} (\boldsymbol {h}) = \gamma (\boldsymbol {0}) - \gamma (\boldsymbol {h}),
$$

where $\gamma ( h )$ is the usual lag $\pmb { h }$ covariance function and $\mathbf { 0 } = ( 0 , 0 )$ . Note the easy extension to any spatial dimension.

The following problems require the supplemental material given in Appendix A

1.27 Suppose $x _ { t } = \beta _ { 0 } + \beta _ { 1 } t$ , where $\beta _ { 0 }$ and $\beta _ { 1 }$ are constants. Prove as $n \to \infty$ $\widehat { \rho } _ { x } ( h ) \to 1$ for fixed $h$ , where ${ \widehat { \rho } } _ { x } ( h )$ is the ACF (1.37).

1.28 (a) Suppose $x _ { t }$ is a weakly stationary time series with mean zero and with absolutely summable autocovariance function, $\gamma ( h )$ , such that

$$
\sum_ {h = - \infty} ^ {\infty} \gamma (h) = 0.
$$

Prove that $\sqrt { n } \ \bar { x } \overset { P } { \to } 0$ , where $x$ is the sample mean (1.35).

(b) Give an example of a process that satisfies the conditions of part (a). What is special about this process?

1.29 Let $x _ { t }$ be a linear process of the form (A.44)–(A.45). If we define

$$
\tilde {\gamma} (h) = n ^ {- 1} \sum_ {t = 1} ^ {n} \left(x _ {t + h} - \mu_ {x}\right) \left(x _ {t} - \mu_ {x}\right),
$$

show that

$$
n ^ {1 / 2} \big (\tilde {\gamma} (h) - \widehat {\gamma} (h) \big) = o _ {p} (1).
$$

Hint: The Markov Inequality

$$
P \{| x | \geq \epsilon \} <   \frac {E | x |}{\epsilon}
$$

can be helpful for the cross-product terms.

1.30 For a linear process of the form

$$
x _ {t} = \sum_ {j = 0} ^ {\infty} \phi^ {j} w _ {t - j},
$$

where $\{ w _ { t } \}$ satisfies the conditions of Theorem A.7 and $| \phi | < 1$ , show that

$$
\sqrt {n} \frac {\left(\widehat {\rho} _ {x} (1) - \rho_ {x} (1)\right)}{\sqrt {1 - \rho_ {x} ^ {2} (1)}} \stackrel {d} {\to} N (0, 1),
$$

and construct a $9 5 \%$ confidence interval for $\phi$ when $\widehat { \rho } _ { x } ( 1 ) = . 6 4$ and $n = 1 0 0$ .

1.31 Let $\{ x _ { t } ; ~ t = 0 , \pm 1 , \pm 2 , \ldots \}$ be iid $( 0 , \sigma ^ { 2 } )$ .

(a) For $h \geq 1$ and $k \geq 1$ , show that $x _ { t } x _ { t + h }$ and $x _ { s } x _ { s + k }$ are uncorrelated for all $s \neq t$ .   
(b) For fixed $h \geq 1$ , show that the $h \times 1$ vector

$$
\sigma^ {- 2} n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} (x _ {t} x _ {t + 1}, \dots , x _ {t} x _ {t + h}) ^ {\prime} \xrightarrow {d} (z _ {1}, \dots , z _ {h}) ^ {\prime}
$$

where $z _ { 1 } , \dots , z _ { h }$ are iid $\mathrm { { N } } ( 0 , 1 )$ random variables. [Note: the sequence $\{ x _ { t } x _ { t + h }$ ; $t = 1 , 2 , \ldots \}$ is $h$ -dependent and white noise $( 0 , \sigma ^ { 4 } )$ . Also, recall the Cram´er-Wold device.]

(c) Show, for each $h \geq 1$ ,

$$
n ^ {- 1 / 2} \left[ \sum_ {t = 1} ^ {n} x _ {t} x _ {t + h} - \sum_ {t = 1} ^ {n - h} (x _ {t} - \bar {x}) (x _ {t + h} - \bar {x}) \right] \stackrel {{p}} {{\to}} 0 \qquad \mathrm {a s} n \to \infty
$$

where ¯x = n−1 nt= ${ \bar { x } } = n ^ { - 1 } \sum _ { t = 1 } ^ { n } x _ { t }$ 1 xt.

(d) Noting that $\scriptstyle n ^ { - 1 } \sum _ { t = 1 } ^ { n } x _ { t } ^ { 2 } \stackrel { p } { \to } \sigma ^ { 2 }$ , conclude that

$$
n ^ {1 / 2} [ \widehat {\rho} (1), \dots , \widehat {\rho} (h) ] ^ {\prime} \xrightarrow {d} (z _ {1}, \dots , z _ {h}) ^ {\prime}
$$

where ${ \widehat { \rho } } ( h )$ is the sample ACF of the data $x _ { 1 } , \ldots , x _ { n }$

# Chapter 2

# Time Series Regression and Exploratory Data Analysis

# 2.1 Introduction

The linear model and its applications are at least as dominant in the time series context as in classical statistics. Regression models are important for time domain models discussed in Chapters 3, 5, and 6, and in the frequency domain models considered in Chapters 4 and 7. The primary ideas depend on being able to express a response series, say $x _ { t }$ , as a linear combination of inputs, say $z _ { t 1 } , z _ { t 2 } , \ldots , z _ { t q }$ . Estimating the coefficients $\beta _ { 1 } , \beta _ { 2 } , \ldots , \beta _ { q }$ in the linear combinations by least squares provides a method for modeling $x _ { t }$ in terms of the inputs.

In the time domain applications of Chapter 3, for example, we will express $x _ { t }$ as a linear combination of previous values $x _ { t - 1 } , x _ { t - 2 } , . . . , x _ { p }$ , of the currently observed series. The outputs $x _ { t }$ may also depend on lagged values of another series, say $y _ { t - 1 } , y _ { t - 2 } , \dotsc , y _ { t - q }$ , that have influence. It is easy to see that forecasting becomes an option when prediction models can be formulated in this form. Time series smoothing and filtering can be expressed in terms of local regression models. Polynomials and regression splines also provide important techniques for smoothing.

If one admits sines and cosines as inputs, the frequency domain ideas that lead to the periodogram and spectrum of Chapter 4 follow from a regression model. Extensions to filters of infinite extent can be handled using regression in the frequency domain. In particular, many regression problems in the frequency domain can be carried out as a function of the periodic components of the input and output series, providing useful scientific intuition into fields like acoustics, oceanographics, engineering, biomedicine, and geophysics.

The above considerations motivate us to include a separate chapter on re-

gression and some of its applications that is written on an elementary level and is formulated in terms of time series. The assumption of linearity, stationarity, and homogeneity of variances over time is critical in the regression context, and therefore we include some material on transformations and other techniques useful in exploratory data analysis.

# 2.2 Classical Regression in the Time Series Context

We begin our discussion of linear regression in the time series context by assuming some output or dependent time series, say, $x _ { t }$ , for $t = 1 , \ldots , n$ , is being influenced by a collection of possible inputs or independent series, say, $z _ { t 1 } , z _ { t 2 } , . . . , z _ { t q }$ , where we first regard the inputs as fixed and known. This assumption, necessary for applying conventional linear regression, will be relaxed later on. We express this relation through the linear regression model

$$
x _ {t} = \beta_ {1} z _ {t 1} + \beta_ {2} z _ {t 2} + \dots + \beta_ {q} z _ {t q} + w _ {t}, \tag {2.1}
$$

where $\beta _ { 1 } , \beta _ { 2 } , \ldots , \beta _ { q }$ are unknown fixed regression coefficients, and $\{ w _ { t } \}$ is a random error or noise process consisting of independent and identically distributed (iid) normal variables with mean zero and variance $\sigma _ { w } ^ { 2 }$ ; we will relax the iid assumption later. A more general setting within which to embed mean square estimation and linear regression is given in Appendix B, where we introduce Hilbert spaces and the Projection Theorem.

# Example 2.1 Estimating a Trend

Consider the global temperature data, say $x _ { t }$ , shown in Figure 1.2. As discussed in Example 1.2, there is an apparent upward trend in the series that has been used to argue the global warming hypothesis. We might use simple linear regression to estimate that trend by fitting the model

$$
x _ {t} = \beta_ {1} + \beta_ {2} t + w _ {t}, \quad t = 1 9 0 0, 1 9 0 1, \dots , 1 9 9 7.
$$

This is in the form of the regression model (2.1) when we make the identification $q \ : = \ : 2$ , $z _ { t 1 } = 1 , z _ { t 2 } = t$ . Note that we are making the assumption that the errors, $w _ { t }$ , are an iid normal sequence, which may not be true. We will address this problem further in 2.3; the problem of autocorrelated errors is discussed in detail in 5.5. Also note that we could have used, e.g., $t = 0 , \ldots , 9 7$ , without affecting the interpretation of the slope coefficient, $\beta _ { 2 }$ ; only the intercept, $\beta _ { 1 }$ , would be affected.

Using simple linear regression, we obtained the estimated coefficients $\widehat { \beta } _ { 1 } = - 1 2 . 1 8 6$ , and $\widehat { \beta } _ { 2 } = . 0 0 6$ (with a standard error of .0005) yielding a significant estimated increase of .6 degrees centigrade per 100 years. We

![](images/f5101f86774f1078641e8de4da3e380711fe3f19d1bedce233c6e786f41e799d.jpg)  
Figure 2.1 Global temperature deviations shown in Figure 1.2 with fitted linear trend line.

discuss the precise way in which the solution was accomplished below. Finally, Figure 2.1 shows the global temperature data, say $x _ { t }$ , with the estimated trend, say $\widehat { x } _ { t } = - 1 2 . 1 8 6 + . 0 0 6 t$ , superimposed. It is apparent that the estimated trend line obtained via simple linear regression does not quite capture the trend of the data and better models will be needed.

To perform this analysis in R, we note that the data file globtemp.dat has 142 observations starting from the year 1856. We are only using the final 98 observations corresponding to the years 1900 to 1997.

```txt
> gtemp = scan("/mydata/globtemp.dat")
> x = gtemp[45:142]
> t = 1900:1997
> fit=lm(x~t) # regress x on t
> summary(fit) # regression output
> plot(t,x, type="o", xlab="year", ylab="temp deviation")
> abline(fit) # add regression line to the plot 
```

The linear model described by (2.1) above can be conveniently written in a more general notation by defining the column vectors $\boldsymbol { z } _ { t } = ( z _ { t 1 } , z _ { t 2 } , \ldots , z _ { t q } ) ^ { \prime }$ and $\pmb { \beta } = ( \beta _ { 1 } , \beta _ { 2 } , \ldots , \beta _ { q } ) ^ { \prime }$ , where - denotes transpose, so (2.1) can be written in the alternate form

$$
x _ {t} = \boldsymbol {\beta} ^ {\prime} \mathbf {z} _ {t} + w _ {t}. \tag {2.2}
$$

where $w _ { t } \sim \mathrm { i i d } ( 0 , \sigma _ { w } ^ { 2 } )$ . It is natural to consider estimating the unknown coef-

# 2.2: Classical Regression

ficient vector $\beta$ by minimizing the residual sum of squares

$$
R S S = \sum_ {t = 1} ^ {n} \left(x _ {t} - \boldsymbol {\beta} ^ {\prime} \boldsymbol {z} _ {t}\right) ^ {2}, \tag {2.3}
$$

with respect to $\beta _ { 1 } , \beta _ { 2 } , \ldots , \beta _ { q }$ . Minimizing $R S S$ yields the ordinary least squares estimator. This minimization can be accomplished by differentiating (2.3) with respect to the vector $\beta$ or by using the properties of projections. In the notation above, this procedure gives the normal equations

$$
\left(\sum_ {t = 1} ^ {n} \boldsymbol {z} _ {t} \boldsymbol {z} _ {t} ^ {\prime}\right) \widehat {\boldsymbol {\beta}} = \sum_ {t = 1} ^ {n} \boldsymbol {z} _ {t} x _ {t}. \tag {2.4}
$$

A further simplification of notation results from defining the matrix $Z =$ $( z _ { 1 } , z _ { 2 } , \ldots , z _ { n } ) ^ { \prime }$ as the $n \times q$ matrix composed of the $n$ samples of the input variables and the observed $n \times 1$ vector $\pmb { x } = ( x _ { 1 } , x _ { 2 } , \ldots , x _ { n } ) ^ { \prime }$ . This identification yields

$$
\left(Z ^ {\prime} Z\right) \hat {\boldsymbol {\beta}} = Z ^ {\prime} \boldsymbol {x} \tag {2.5}
$$

and the solution

$$
\widehat {\boldsymbol {\beta}} = \left(Z ^ {\prime} Z\right) ^ {- 1} Z ^ {\prime} \boldsymbol {x} \tag {2.6}
$$

when the matrix $Z ^ { \prime } Z$ is of rank $q$ . The minimized residual sum of squares (2.3) has the equivalent matrix forms

$$
\begin{array}{l} R S S = (\boldsymbol {x} - Z \widehat {\boldsymbol {\beta}}) ^ {\prime} (\boldsymbol {x} - Z \widehat {\boldsymbol {\beta}}) \\ = \boldsymbol {x} ^ {\prime} \boldsymbol {x} - \widehat {\boldsymbol {\beta}} ^ {\prime} Z ^ {\prime} \boldsymbol {x} \\ = \boldsymbol {x} ^ {\prime} \boldsymbol {x} - \boldsymbol {x} ^ {\prime} Z \left(Z ^ {\prime} Z\right) ^ {- 1} Z ^ {\prime} \boldsymbol {x}, \tag {2.7} \\ \end{array}
$$

to give some useful versions for later reference. The ordinary least squares estimators are unbiased, i.e., $E ( { \widehat { \boldsymbol { \beta } } } ) = \beta$ , and have the smallest variance within the class of linear unbiased estimators.

If the errors $w _ { t }$ are normally distributed (Gaussian), $\widehat { \beta }$ is also the maximum likelihood estimator for $\beta$ and is normally distributed with

$$
\begin{array}{l} \operatorname {c o v} (\widehat {\boldsymbol {\beta}}) = \sigma_ {w} ^ {2} \left(\sum_ {t = 1} ^ {n} \boldsymbol {z} _ {t} \boldsymbol {z} _ {t} ^ {\prime}\right) ^ {- 1} \\ = \sigma_ {w} ^ {2} \left(Z ^ {\prime} Z\right) ^ {- 1} \\ = \sigma_ {w} ^ {2} C, \tag {2.8} \\ \end{array}
$$

where

$$
C = \left(Z ^ {\prime} Z\right) ^ {- 1} \tag {2.9}
$$

is a convenient notation for later equations. An unbiased estimator for the variance $\sigma _ { w } ^ { 2 }$ i s

$$
s _ {w} ^ {2} = \frac {R S S}{n - q}, \tag {2.10}
$$

Table 2.1 Analysis of Variance for Regression   

<table><tr><td>Source</td><td>df</td><td>Sum of Squares</td><td>Mean Square</td></tr><tr><td>zt,q1+1, ...,zt,q</td><td>q-q1</td><td>SSreg = RSS1 - RSS</td><td>MSreg = SSreg/(q-q1)</td></tr><tr><td>Error</td><td>n-q</td><td>RSS</td><td>sw2 = RSS/(n-q)</td></tr><tr><td>Total</td><td>n-q1</td><td>RSS1</td><td></td></tr></table>

contrasted with the maximum likelihood estimator ${ \widehat { \sigma } } _ { w } ^ { 2 } = R S S / n$ , which has the divisor $n$ . Under the normal assumption, $s _ { w } ^ { 2 }$ is distributed proportionally to a chi-squared random variable with $n - q$ degrees of freedom, denoted by $\chi _ { n - q } ^ { 2 }$ , and independently of $\widehat { \beta }$ . It follows that

$$
t _ {n - q} = \frac {\left(\widehat {\beta} _ {i} - \beta_ {i}\right)}{s _ {w} \sqrt {c _ {i i}}} \tag {2.11}
$$

has the t-distribution with $n { - } q$ degrees of freedom; $c _ { i i }$ denotes the $i ^ { t h }$ diagonal element of $C$ , as defined in (2.9).

Various competing models are of interest to isolate or select the best subset of independent variables. Suppose a proposed model specifies that only a subset $q _ { 1 } < q$ independent variables, say, $\boldsymbol { z } _ { 1 t } = ( z _ { t 1 } , z _ { t 2 } , \ldots , z _ { t q 1 } ) ^ { \prime }$ is influencing the dependent variable $x _ { t }$ , so the model

$$
x _ {t} = \beta_ {1} ^ {\prime} z _ {1 t} + w _ {t} \tag {2.12}
$$

becomes the null hypothesis, where $\pmb { \beta } _ { 1 } = ( \beta _ { 1 } , \beta _ { 2 } , \dots , \beta _ { q _ { 1 } } ) ^ { \prime }$ is a subset of coefficients of the original $q$ variables. We can test the reduced model (2.12) against the full model (2.2) by comparing the residual sums of squares under the two models using the F-statistic

$$
F _ {q - q _ {1}, n - q} = \frac {R S S _ {1} - R S S}{R S S} \frac {n - q}{q - q _ {1}}, \tag {2.13}
$$

which has the central $F ^ { \prime }$ -distribution with $q - q _ { 1 }$ and $n - q$ degrees of freedom when (2.12) is the correct model. The statistic, which follows from applying the likelihood ratio criterion, has the improvement per number of parameters added in the numerator compared with the error sum of squares under the full model in the denominator. The information involved in the test procedure is often summarized in an Analysis of Variance (ANOVA) table as given in Table 2.1 for this particular case. The difference in the numerator is often called the regression sum of squares

In terms of Table 2.1, it is conventional to write the $F ^ { \prime }$ -statistic (2.13) as the ratio of the two mean squares, obtaining

$$
F _ {q - q _ {1}, n - q} = \frac {M S _ {r e g}}{s _ {w} ^ {2}}. \tag {2.14}
$$

# 2.2: Classical Regression

A special case of interest is $q _ { 1 } = 1$ and $z _ { 1 t } = 1$ , so the model in (2.12) becomes

$$
x _ {t} = \beta_ {1} + w _ {t},
$$

and we may measure the proportion of variation accounted for by the other variables using

$$
R _ {x z} ^ {2} = \frac {R S S _ {0} - R S S}{R S S _ {0}}, \tag {2.15}
$$

where the residual sum of squares under the reduced model

$$
R S S _ {0} = \sum_ {t = 1} ^ {n} \left(x _ {t} - \bar {x}\right) ^ {2}, \tag {2.16}
$$

in this case is just the sum of squared deviations from the mean $x$ . The measure $R _ { x z } ^ { 2 }$ is also the squared multiple correlation between $x _ { t }$ and the variables $z _ { t 2 } , z _ { t 3 } , . . . , z _ { t q }$ .

The techniques discussed in the previous paragraph can be used to test various models against one another using the $F$ test given in (2.13), (2.14), and the ANOVA table. These tests have been used in the past in a stepwise manner, where variables are added or deleted when the values from the $F$ -test either exceed or fail to exceed some predetermined levels. The procedure, called stepwise multiple regression, is useful in arriving at a set of useful variables. An alternative is to focus on a procedure for model selection that does not proceed sequentially, but simply evaluates each model on its own merits. Suppose we consider a regression model with $k$ coefficients and denote the maximum likelihood estimator for the variance as

$$
\widehat {\sigma} _ {k} ^ {2} = \frac {R S S _ {k}}{n}, \tag {2.17}
$$

where $R S S _ { k }$ denotes the residual sum of squares under the model with $k$ regression coefficients. Then, Akaike (1969, 1973, 1974) suggested measuring the goodness of fit for this particular model by balancing the error of the fit against the number of parameters in the model; we define

# Definition 2.1 Akaike’s Information Criterion (AIC)

$$
\mathrm {A I C} = \ln \hat {\sigma} _ {k} ^ {2} + \frac {n + 2 k}{n}, \tag {2.18}
$$

where $\widehat { \sigma } _ { k } ^ { 2 }$ is given by (2.17) and $k$ is the number of parameters in the model.

The value of $k$ yielding the minimum AIC specifies the best model. The idea is roughly that minimizing $\widehat { \sigma } _ { k } ^ { 2 }$ would be a reasonable objective, except that it decreases monotonically as $k$ increases. Therefore, we ought penalize the error variance by a term proportional to the number of parameters. The choice for the penalty term given by (2.18) is not the only one, and a considerable

literature is available advocating different penalty terms. A corrected form, suggested by Sugiura (1978), and expanded by Hurvich and Tsai (1989), can be based on small-sample distributional results for the linear regression model (details are provided in Problems 2.4 and 2.5). The corrected form is defined as

Definition 2.2 AIC, Bias Corrected (AICc)

$$
\mathrm {A I C c} = \ln \hat {\sigma} _ {k} ^ {2} + \frac {n + k}{n - k - 2}, \tag {2.19}
$$

where $\widehat { \sigma } _ { k } ^ { 2 }$ is given by (2.17), k is the number of parameters in the model, and $n$ is the sample size.

We may also derive a correction term based on Bayesian arguments, as in Schwarz (1978), which leads to

Definition 2.3 Schwarz’s Information Criterion (SIC)

$$
\mathrm {S I C} = \ln \hat {\sigma} _ {k} ^ {2} + \frac {k \ln n}{n}, \tag {2.20}
$$

using the same notation as in Definition 2.2.

SIC is also called the Bayesian Information Criterion (BIC) (see also Rissanen, 1978, for an approach yielding the same statistic based on a minimum description length argument). Various simulation studies have tended to verify that SIC does well at getting the correct order in large samples, whereas AICc tends to be superior in smaller samples where the relative number of parameters is large (see McQuarrie and Tsai, 1998, for detailed comparisons). In fitting regression models, two measures that have been used in the past are adjusted R-squared, which is essentially $s _ { w } ^ { 2 }$ , and Mallows $C _ { p }$ , Mallows (1973), which we do not consider in this context.

# Example 2.2 Pollution, Temperature and Mortality

The data shown in Figure 2.2 are extracted series from a study by Shumway et al. (1988) of the possible effects of temperature and pollution on daily mortality in Los Angeles County. Note the strong seasonal components in all of the series, corresponding to winter-summer variations and the downward trend in the cardiovascular mortality over the 10-year period.

A scatterplot matrix, shown in Figure 2.3, indicates a possible linear relation between mortality and the pollutant particulates and a possible relation to temperature. Note the curvilinear shape of the temperature mortality curve, indicating that higher temperatures as well as lower temperatures are associated with increases in cardiovascular mortality.

![](images/e1986750b6e153f26cae43aff1d24f7fb8efb08d78f364bbc2558fe5748d704f.jpg)

![](images/9dc2f54c1a759d982720e7342c3702cb16c04eaed83078f824a2bc6e73d2ecba.jpg)

![](images/28e4f705155769aa47c7d1dd0ba8a2a5df9057e822a406352de28c9cf2ced5c4.jpg)  
Figure 2.2 Average daily cardiovascular mortality (top), temperature (middle) and particulate pollution (bottom) in Los Angeles County. There are 508 six-day smoothed averages obtained by filtering daily values over the 10 year period 1970-1979.

Based on the scatterplot matrix, we entertain, tentatively, four models where $M _ { t }$ denotes cardiovascular mortality, $T _ { t }$ denotes temperature and $P _ { t }$ denotes the particulate levels. They are

$$
M _ {t} = \beta_ {0} + \beta_ {1} t + w _ {t} \tag {2.21}
$$

$$
M _ {t} = \beta_ {0} + \beta_ {1} t + \beta_ {2} (T _ {t} - T.) + w _ {t} \tag {2.22}
$$

$$
M _ {t} = \beta_ {0} + \beta_ {1} t + \beta_ {2} \left(T _ {t} - T _ {\cdot}\right) + \beta_ {3} \left(T _ {t} - T _ {\cdot}\right) ^ {2} + w _ {t} \tag {2.23}
$$

$$
M _ {t} = \beta_ {0} + \beta_ {1} t + \beta_ {2} (T _ {t} - T.) + \beta_ {3} (T _ {t} - T.) ^ {2} + \beta_ {4} P _ {t} + w _ {t} \tag {2.24}
$$

where we adjust temperature for its mean, $T . \ = \ 7 4 . 6$ , to avoid scaling problems. It is clear that (2.21) is a trend only model, (2.22) is linear temperature, (2.23) is curvilinear temperature and (2.24) is curvilinear temperature and pollution. We summarize some the statistics given for this particular case in Table 2.2. The values of $R ^ { 2 }$ were computed by

![](images/3c6d70e1977f9887b3b169b39d660b334d7e3a8fd1af6914265cbf7b0791d023.jpg)  
Figure 2.3 Scatterplot matrix showing plausible relations between mortality, temperature, and pollution.

Table 2.2 Summary Statistics for Mortality Models   

<table><tr><td>Model</td><td>RSS (2.3)</td><td>sw2(2.10)</td><td>R2(2.15)</td><td>AICc (2.19)</td></tr><tr><td>(2.21)</td><td>40,020</td><td>79.09</td><td>.21</td><td>5.38</td></tr><tr><td>(2.22)</td><td>31,413</td><td>62.20</td><td>.38</td><td>5.14</td></tr><tr><td>(2.23)</td><td>27,985</td><td>55.52</td><td>.45</td><td>5.03</td></tr><tr><td>(2.24)</td><td>20,509</td><td>40.77</td><td>.60</td><td>4.72</td></tr></table>

noting that $R S S _ { 0 } = 5 0 , 6 8 7$ using (2.16).

We note that each model does substantially better than the one before it and that the model including both temperature, temperature squared and particulates does the best, accounting for some $6 0 \%$ of the variability

and with the best value for AICc. Note that one can compare any two models using the residual sums of squares and (2.13). Hence, a model with only trend could be compared to the full model using $q = 5 , q _ { 1 } =$ $2 , n = 5 0 8$ , so

$$
F _ {3, 5 0 3} = \frac {(4 0 , 0 2 0 - 2 0 , 5 0 9)}{2 0 , 5 0 9} \frac {5 0 3}{3} = 1 6 0,
$$

which exceeds $F _ { 3 , \infty } ( . 0 0 1 ) = 5 . 4 2 \ :$ . We obtain the best prediction model,

$$
\begin{array}{l} \widehat {M _ {t}} = 8 1. 5 9 -. 0 2 7 _ {(}. 0 0 2) t - . 4 7 3 _ {(}. 0 3 2) (T _ {t} - 7 4. 6) \\ + . 0 2 3 _ {(. 0 0 3)} \left(T _ {t} - 7 4. 6\right) ^ {2} +. 2 5 5 _ {(}. 0 1 9) P _ {t}, \\ \end{array}
$$

for mortality, where the standard errors, computed from (2.8)-(2.10), are given in parentheses. As expected, a negative trend is present in time as well as a negative coefficient for adjusted temperature. The quadratic effect of temperature can clearly be seen in the scatterplots of Figure 2.3. Pollution weights positively and can be interpreted as the incremental contribution to daily deaths per unit of particulate pollution. It would still be essential to check the residuals $\widehat { w } _ { t } = M _ { t } - \widehat { M } _ { t }$ for autocorrelation, but we defer this question to the section on correlated least squares, in which the incorporation of time correlation changes the estimated standard errors.

To display the scatterplot matrix, perform the final regression and compute AIC in R, use the following commands:

```julia
> mort = scan("/mydata/cmort.dat")
> temp = scan("/mydata/temp.dat")
> part = scan("/mydata/part.dat")
> temp = temp - mean(temp)
> temp2 = temp^2
> t = 1:length(mort)
> fit = lm(mort~t + temp + temp2 + part)
> summary(fit) # Results
> AIC(fit)/508 # R gives n*AIC
> pairs(cbind(mort, temp, part)) # scatterplot matrix 
```

# 2.3 Exploratory Data Analysis

In general, it is necessary for time series data to be stationary, so averaging lagged products over time, as in the previous section, will be a sensible thing to do. With time series data, it is the dependence between the values of the series that is important to measure; we must, at least, be able to estimate autocorrelations with precision. It would be difficult to measure that dependence if the dependence structure is not regular or is changing at every time point.

Hence, to achieve any meaningful statistical analysis of time series data, it will be crucial that, if nothing else, the mean and the autocovariance functions satisfy the conditions of stationarity (for at least some reasonable stretch of time) stated in Definition 1.7. Often, this is not the case, and we will mention some methods in this section for playing down the effects of nonstationarity so the stationary properties of the series may be studied.

A number of our examples came from clearly nonstationary series. The Johnson $\&$ Johnson series in Figure 1.1 has a mean that increases exponentially over time, and the increase in the magnitude of the fluctuations around this trend causes changes in the covariance function; the variance of the process, for example, clearly increases as one progresses over the length of the series. Also, the global temperature series shown in Figure 1.2 contains some evidence of a trend over time; human-induced global warming advocates seize on this as empirical evidence to advance their hypothesis that temperatures are increasing.

Perhaps the easiest form of nonstationarity to work with is the trend stationary model wherein the process has stationary behavior around a trend. We may write this type of model as

$$
x _ {t} = \mu_ {t} + y _ {t} \tag {2.25}
$$

where $x _ { t }$ are the observations, $\mu _ { t }$ denotes the trend, and $y _ { t }$ is a stationary process. Quite often, strong trend, $\mu _ { t }$ , will obscure the behavior of the stationary process, $y _ { t }$ , as we shall see in numerous examples in Chapter 3. Hence, there is some advantage to removing the trend as a first step in an exploratory analysis of such time series. The steps involved are to obtain a reasonable estimate of the trend component, say $\widehat { \mu } _ { t }$ , and then work with the residuals

$$
\widehat {y} _ {t} = x _ {t} - \widehat {\mu} _ {t}. \tag {2.26}
$$

Consider the following example.

# Example 2.3 Detrending Global Temperature

Here we suppose the model is of the form of (2.25),

$$
x _ {t} = \mu_ {t} + y _ {t},
$$

where, as we suggested in the analysis of the global temperature data presented in Example 2.1, a straight line might be a reasonable model for the trend, i.e.,

$$
\mu_ {t} = \beta_ {1} + \beta_ {2} t.
$$

In that example, we estimated the trend using ordinary least squares1

![](images/0ade5cb41f8537f5e0d0b22fc3df8fbb2ed86195961123fc88836a02f205826e.jpg)

![](images/fb67c15ed5d063d1674888558e27c05bb95a286e3965c5d08a623a5c1d924c68.jpg)  
Figure 2.4 Detrended (top) and differenced (bottom) global temperature series. The original data are shown in Figures 1.2 and 2.1.

and found

$$
\widehat {\mu} _ {t} = - 1 2. 1 8 6 +. 0 0 6 t.
$$

Figure 2.1 shows the data with the estimated trend line superimposed. To obtain the detrended series we simply subtract $\widehat { \mu } _ { t }$ from the observations, $x _ { t }$ , to obtain the detrended series

$$
\widehat {y} _ {t} = x _ {t} + 1 2. 1 8 6 -. 0 0 6 t.
$$

The top graph of Figure 2.4 shows the detrended series. Figure 2.5 shows the ACF of the original data (top panel) as well as the ACF of the detrended data (middle panel).

To detrend in R, assuming the data are in gtemp:

> x = gtemp[45:142] # use only 1900 to 1997   
> t = 1900:1997   
> fit = lm(x˜t) # detrended series in fit$resid   
> plot(t, fit$resid, type="o", ylab $\underline { { \underline { { \mathbf { \Pi } } } } } =$ "detrended gtemp")

In Example 1.11 and the corresponding Figure 1.10 we saw that a random walk might also be a good model for trend. That is, rather than modeling trend as fixed (as in Example 2.3), we might model trend as a stochastic component using the random walk with drift model,

$$
\mu_ {t} = \delta + \mu_ {t - 1} + w _ {t}, \tag {2.27}
$$

![](images/b4a740470cc7f1668f6186e18d66f101eb36b0bca68f419b870f560e0c6c14c7.jpg)

![](images/8ca992934137afe78837469d734ed3f68b441c06cadb7a979bf83d995ce0165b.jpg)

![](images/7e96fb22de0f17760a11894eebbbb91fdedc7f17636d5d56ceaeb7d95f175ae9.jpg)  
Figure 2.5 Sample ACFs of the global temperature (top), and of the detrended (middle) and the differenced (bottom) series.

where $w _ { t }$ is white noise and is independent of $y _ { t }$ . If the appropriate model is (2.25), then differencing the data, $x _ { t }$ , yields a stationary process; that is,

$$
\begin{array}{l} x _ {t} - x _ {t - 1} = \left(\mu_ {t} + y _ {t}\right) - \left(\mu_ {t - 1} + y _ {t - 1}\right) \\ = \delta + w _ {t} + y _ {t} - y _ {t - 1}. \tag {2.28} \\ \end{array}
$$

We leave it as an exercise (Problem 2.7) to show (2.28) is stationary.2

One advantage of differencing over detrending to remove trend is that no parameters are estimated in the differencing operation. One disadvantage, however, is that differencing does not yield an estimate of the stationary process $y _ { t }$ as can be seen in (2.28). If an estimate of $y _ { t }$ is essential, then detrending may be more appropriate. If the goal is to coerce the data to stationarity, then

differencing may be more appropriate. Differencing is also a viable tool if the trend is fixed, as in Example 2.3. That is, e.g., if $\mu _ { t } = \beta _ { 1 } + \beta _ { 2 } t$ in the model (2.25), differencing the data produces stationarity (see Problem 2.6):

$$
x _ {t} - x _ {t - 1} = \left(\mu_ {t} + y _ {t}\right) - \left(\mu_ {t - 1} + y _ {t - 1}\right) = \beta_ {2} + y _ {t} - y _ {t - 1}.
$$

Because differencing plays a central role in time series analysis, it receives its own notation. The first difference is denoted as

$$
\nabla x _ {t} = x _ {t} - x _ {t - 1}. \tag {2.29}
$$

As we have seen, the first difference eliminates a linear trend. A second difference, that is, the difference of (2.29), can eliminate a quadratic trend, and so on. In order to define higher differences, we need a variation in notation that we use, for the first time here, and often in our discussion of ARIMA models in Chapter 3.

Definition 2.4 We define the backshift operator by

$$
B x _ {t} = x _ {t - 1}
$$

and extend it to powers $B ^ { 2 } x _ { t } = B ( B x _ { t } ) = B x _ { t - 1 } = x _ { t - 2 }$ , and so on. Thus,

$$
B ^ {k} x _ {t} = x _ {t - k}. \tag {2.30}
$$

It is clear that we may then rewrite (2.29) as

$$
\nabla x _ {t} = (1 - B) x _ {t}, \tag {2.31}
$$

and we may extend the notion further. For example, the second difference becomes

$$
\begin{array}{l} \nabla^ {2} x _ {t} = (1 - B) ^ {2} x _ {t} = (1 - 2 B + B ^ {2}) x _ {t} \\ = x _ {t} - 2 x _ {t - 1} + x _ {t - 2} \\ \end{array}
$$

by the linearity of the operator. To check, just take the difference of the first difference $\nabla ( \nabla x _ { t } ) = \nabla ( x _ { t } - x _ { t - 1 } ) = ( x _ { t } - x _ { t - 1 } ) - ( x _ { t - 1 } - x _ { t - 2 } )$ .

Definition 2.5 Differences of order d are defined as

$$
\nabla^ {d} = (1 - B) ^ {d}, \tag {2.32}
$$

where we may expand the operator $( 1 - B ) ^ { d }$ algebraically to evaluate for higher integer values of $d$ . When $d = 1$ , we drop it from the notation.

The first difference (2.29) is an example of a linear filter applied to eliminate a trend. Other filters, formed by averaging values near $x _ { t }$ , can produce adjusted series that eliminate other kinds of unwanted fluctuations, as in Chapter 3. The differencing technique is an important component of the ARIMA model of Box and Jenkins (1970) (see also Box et al., 1994), to be discussed in Chapter 3.

# Example 2.4 Differencing Global Temperature

The first difference of the global temperature series, also shown in Figure 2.4, does not contain the long middle cycle we observe in the detrended series. The ACF of this series is also shown in Figure 2.5. In this case it appears that the differenced process may be white noise, which implies that the global temperature series is a random walk. Finally, notice that removing trend by detrending (i.e., regression techniques) produces different results than removing trend by differencing.

Continuing from Example 2.3, to difference and plot the data in R:

> x = gtemp[44:142] # start at 1899   
> plot(1900:1997, diff(x), type $=$ "o", xlab="year")

An alternative to differencing is a less-severe operation that still assumes stationarity of the underlying time series. This alternative, called fractional differencing, extends the notion of the difference operator (2.32) to fractional powers $- . 5 < d < . 5$ , which still define stationary processes. Granger and Joyeux (1980) and Hosking (1981) introduced long memory time series, which corresponds to the case when $0 < d < . 5$ . This model is often used for environmental time series arising in hydrology. We will discuss long memory processes in more detail in 5.2.

Often, obvious aberrations are present that can contribute nonstationary as well as nonlinear behavior in observed time series. In such cases, transformations may be useful to equalize the variability over the length of a single series. A particularly useful transformation is

$$
y _ {t} = \ln x _ {t}, \tag {2.33}
$$

which tends to suppress larger fluctuations that occur over portions of the series where the underlying values are larger. Other possibilities are power transformations in the Box–Cox family of the form

$$
y _ {t} = \left\{ \begin{array}{l l} (x _ {t} ^ {\lambda} - 1) / \lambda , & \lambda \neq 0 \\ \ln x _ {t}, & \lambda = 0. \end{array} \right. \tag {2.34}
$$

Methods for choosing the power $\lambda$ are available (see Johnson and Wichern, 1992, 4.7) but we do not pursue them here. Often, transformations are also used to improve the approximation to normality or to improve linearity in predicting the value of one series from another.

# Example 2.5 Paleoclimatic Glacial Varves

Melting glaciers deposit yearly layers of sand and silt during the spring melting seasons, which can be reconstructed yearly over a period ranging from the time deglaciation began in New England (about 12,600 years

![](images/f2e246a17dca57a040cb9d21d1cbcba957fcb0fcb17a98c41d89790c686e7003.jpg)

![](images/541152e71f1c3b58cc69d48f922866dd001068a685d3e8971eaeaa8595d86837.jpg)  
Figure 2.6 Glacial varve thicknesses (top) from Massachusetts for $n = 6 3 4$ years compared with log transformed thicknesses (bottom).

ago) to the time it ended (about 6,000 years ago). Such sedimentary deposits, called varves, can be used as proxies for paleoclimatic parameters, such as temperature, because, in a warm year, more sand and silt are deposited from the receding glacier. Figure 2.6 shows the thicknesses of the yearly varves collected from one location in Massachusetts for 634 years, beginning 11,834 years ago. For further information, see Shumway and Verosub (1992). Because the variation in thicknesses increases in proportion to the amount deposited, a logarithmic transformation could remove the nonstationarity observable in the variance as a function of time. Figure 2.6 shows the original and transformed varves, and it is clear that this improvement has occurred. We may also plot the histogram of the original and transformed data, as in Problem 2.8, to argue that the approximation to normality is improved. The ordinary first differences (2.31) are also computed in Problem 2.8, and we note that the first differences have a significant negative correlation at lag $h = 1$ . Later, in Chapter 5, we will show that perhaps the varve series has long memory and will propose using fractional differencing.

Next, we consider another preliminary data processing technique that is

![](images/d0a5228415d1655431d3f556bc1c16d58ff7a200fe07c4a6bb67bc4b8ea6773c.jpg)  
Figure 2.7 Scatterplot matrix relating current SOI values ( $x _ { t }$ ) to past SOI values $\left( x _ { t - h } \right)$ at lags $h = 1 , 2 , . . . , 1 2$ .

used for the purpose of visualizing the relations between series at different lags, namely, scatterplot matrices. In the definition of the ACF, we are essentially interested in relations between $x _ { t }$ and $x _ { t - h }$ ; the autocorrelation function tells us whether a substantial linear relation exists between the series and its own lagged values. The ACF gives a profile of the linear correlation at all possible lags and shows which values of $h$ lead to the best predictability. The restriction of this idea to linear predictability, however, may mask a possible nonlinear relation between current values, $x _ { t }$ , and past values, $x _ { t - h }$ . To check for nonlinear relations of this form, it is convenient to display a lagged scatterplot matrix, as in Figure 2.7, that displays values of $x _ { t }$ on the vertical axis plotted against $x _ { t - h }$ on the horizontal axis for the SOI $x _ { t }$ . Similarly, we might want to look at values of one series $y _ { t }$ plotted against another series at various

![](images/e2d71aa59920145a55ef94e1000ae2cedb8c395ae850eca649b577ecc0be4aa8.jpg)

![](images/6c8b748c48d7823b57bba69be637f877e4635e6021913086c8b5ce66be082a2e.jpg)

![](images/98b61302e64d295ad1689b5ea729f29392b56d65b6422df1a3e7b7bc5c6078b3.jpg)

![](images/0e194815a00c07442292e6daa3964f885fef6fcbde0d64122b60cd81d9605f7c.jpg)

![](images/944ffebdf57ef10c4aad86f21784ea4378101a8f0836b181d21a9406877045d8.jpg)

![](images/136c7f4610256227603e52dfbbbfec5c9018e94a5e1d0f6d512d62297a85b9b2.jpg)

![](images/6d05c90079861431c658bc1a745bba9ebf94f06ee1c9554f04ad6f5c48746a15.jpg)

![](images/7aa0807cd674e7543256afd6f4aa19632568257350ad1fe1f9ce3253c0453f05.jpg)

![](images/66fc4a57d04bbede643e93ac70a27454bdc3cba4a26d6d20c1f8c82d8dc37ae0.jpg)  
Figure 2.8 Scatterplot matrix of the Recruitment series, $y _ { t }$ , on the vertical axis plotted against the SOI series, $x _ { t - h }$ , on the horizontal axis at lags $h =$ $0 , 1 , \ldots , 8$ .

lags, $x _ { t - h }$ , to look for possible nonlinear relations between the two series. Because, for example, we might wish to predict the Recruitment series, say, $y _ { t }$ , from current or past values of the SOI series, $x _ { t - h }$ , for $h = 0 , 1 , 2 , \ldots$ it would be worthwhile to examine the scatterplot matrix. Figure 2.8 shows the lagged scatterplot of the Recruitment series $y _ { t }$ on the vertical axis plotted against the SOI index $x _ { t - h }$ on the horizontal axis.

# Example 2.6 Scatterplot Matrices, SOI, and Recruitment Series

Consider the possibility of looking for nonlinear functional relations at lags in the SOI series, $x _ { t - h }$ , for $h = 0 , 1 , 2 , \ldots$ , and the Recruitment series, $y _ { t }$ . Noting first the top panel in Figure 2.7, we see strong posi-

tive and linear relations at lags $h = 1 , 2 , 1 1 , 1 2$ , that is, between $x _ { t }$ and $x _ { t - 1 } , x _ { t - 2 } , x _ { t - 1 1 } , x _ { t - 1 2 }$ , and a negative linear relation at lags $h = 6 , 7$ . These results match up well with peaks noticed in the ACF in Figure 1.14. Figure 2.8 shows linearity in relating Recruitment, $y _ { t }$ , with the SOI series at $x _ { t - 5 } , x _ { t - 6 } , x _ { t - 7 } , x _ { t - 8 }$ , indicating the SOI series tends to lead the Recruitment series and the coefficients are negative, implying that increases in the SOI lead to decreases in the Recruitment, and vice versa. Some possible nonlinear behavior shows as the relation tends to flatten out at both extremes, indicating a logistic type transformation may be useful.

To reproduce Figure 2.7 in R assuming the data are in soi and rec as before:

> lag.plot(soi, lags=12, layout=c(3,4), diag=F)

Reproducing Figure 2.8 in R is not as easy, but here is how the figure was generated:

```txt
> soi=ts(soi) # make the series  
> rec=ts(rec) # time series objects  
> par(mfrow=c(3,3), mar=c(2.5, 4, 4, 1)) # set up plot area  
> for(h in 0:8) { # loop through lags 0-8  
> plot(lag(soi,-h), rec, main=paste("soi(t-",h,")", sep=""), + ylab="rec(t)", xlab="")  
} 
```

As a final exploratory tool, we discuss assessing periodic behavior in time series data using regression analysis and the periodogram; this material may be thought of as an introduction to spectral analysis, which we discuss in detail in Chapter 4. In Example 1.12, we briefly discussed the problem of identifying cyclic or periodic signals in time series. A number of the time series we have seen so far exhibit periodic behavior. For example, the data from the pollution study example shown in Figure 2.2 exhibit strong yearly cycles. Also, the Johnson & Johnson data shown in Figure 1.1 make one cycle every year (four quarters) on top of an increasing trend and the speech data in Figure 1.2 is highly repetitive. The monthly SOI and Recruitment series in Figure 1.6 show strong yearly cycles, but hidden in the series are clues to the El Ni˜no cycle.

# Example 2.7 Using Regression to Discover a Signal in Noise

Recall, in Example 1.12 we generated $n = 5 0 0$ observations from the model

$$
x _ {t} = A \cos (2 \pi \omega t + \phi) + w _ {t}, \tag {2.35}
$$

where $\omega = 1 / 5 0$ , $A = 2$ , $\phi = . 6 \pi$ , and $\sigma _ { w } = 5$ ; the data are shown on the bottom panel of Figure 1.11. At this point we assume the frequency of oscillation $\omega = 1 / 5 0$ is known, but $A$ and $\phi$ are unknown parameters. In

![](images/2aef241ddb90f5c82aaabda9ed6659e53f500d2a6718c796e3c1b503333f3f1c.jpg)  
Figure 2.9 Data generated by (2.35) [dashed line] with the fitted [solid] line, (2.37), superimposed.

this case the parameters appear in (2.35) in a nonlinear way, so we use a trigonometric identity and write

$$
\begin{array}{l} A \cos (2 \pi \omega t + \phi) = A \cos (\phi) \cos (2 \pi \omega t) - A \sin (\phi) \sin (2 \pi \omega t) \\ { = } { \beta _ { 1 } \cos ( 2 \pi \omega t ) + \beta _ { 2 } \sin ( 2 \pi \omega t ) , } \\ \end{array}
$$

where $\beta _ { 1 } = A \cos ( \phi )$ and $\beta _ { 2 } = - A \sin ( \phi )$ . Now the model (2.35) can be written in the usual linear regression form given by (no intercept term is needed here)

$$
x _ {t} = \beta_ {1} \cos (2 \pi t / 5 0) + \beta_ {2} \sin (2 \pi t / 5 0) + w _ {t}. \tag {2.36}
$$

Using linear regression on the generated data, the fitted model is

$$
\widehat {x} _ {t} = -. 8 4 _ {(. 3 2)} \cos (2 \pi t / 5 0) - 1. 9 9 _ {(. 3 2)} \sin (2 \pi t / 5 0) \qquad (2. 3 7)
$$

with $\widehat { \sigma } _ { w } = 5 . 0 8$ , where the values in parentheses are the standard errors. We note the actual values of the coefficients for this example are $\beta _ { 1 } =$ $2 \cos ( . 6 \pi ) = - . 6 2$ and $\beta _ { 2 } = - 2 \sin ( . 6 \pi ) = - 1 . 9 0$ . Because the parameter estimates are significant and close to the actual values, it is clear that we are able to detect the signal in the noise using regression, even though the signal appears to be obscured by the noise in the bottom panel of Figure 1.11. Figure 2.9 shows data generated by (2.35) with the fitted line, (2.37), superimposed.

# Example 2.8 Using the Periodogram to Discover a Signal in Noise

The analysis in Example 2.7 may seem like cheating because we assumed we knew the value of the frequency parameter $\omega$ . If we do not know $\omega$ ,

we could try to fit the model (2.35) using nonlinear regression with $\omega$ as a parameter. Another method is to try various values of $\omega$ in a systematic way. Using the regression results of §2.2 (also, see Problem 4.10), we can show the estimated regression coefficients in Example 2.7 take on the special form $^ { 3 }$ given by

$$
\widehat {\beta} _ {1} = \frac {\sum_ {t = 1} ^ {n} x _ {t} \cos (2 \pi t / 5 0)}{\sum_ {t = 1} ^ {n} \cos^ {2} (2 \pi t / 5 0)} = \frac {2}{n} \sum_ {t = 1} ^ {n} x _ {t} \cos (2 \pi t / 5 0); \tag {2.38}
$$

$$
\widehat {\beta} _ {2} = \frac {\sum_ {t = 1} ^ {n} x _ {t} \sin (2 \pi t / 5 0)}{\sum_ {t = 1} ^ {n} \sin^ {2} (2 \pi t / 5 0)} = \frac {2}{n} \sum_ {t = 1} ^ {n} x _ {t} \sin (2 \pi t / 5 0). \tag {2.39}
$$

This suggests looking at all possible regression parameter estimates, say

$$
\widehat {\beta} _ {1} (j / n) = \frac {2}{n} \sum_ {t = 1} ^ {n} x _ {t} \cos (2 \pi t j / n); \tag {2.40}
$$

$$
\widehat {\beta} _ {2} (j / n) = \frac {2}{n} \sum_ {t = 1} ^ {n} x _ {t} \sin (2 \pi t j / n), \tag {2.41}
$$

where, $n = 5 0 0$ and $j = 1 , \ldots , \frac { n } { 2 } - 1$ , and inspecting the results for large values. For the endpoints, $j = 0 , n / 2$ , we have $\begin{array} { r } { \widehat { \beta } _ { 1 } ( 0 ) = n ^ { - 1 } \sum _ { t = 1 } ^ { n } x _ { t } } \end{array}$ $\begin{array} { r } { \widehat { \beta } _ { 1 } ( \frac { n } { 2 } ) = n ^ { - 1 } \sum _ { t = 1 } ^ { n } ( - 1 ) ^ { t } x _ { t } } \end{array}$ and $\begin{array} { r } { \widehat { \beta } _ { 2 } ( 0 ) = \widehat { \beta } _ { 2 } ( \frac { n } { 2 } ) = 0 } \end{array}$ .

For this particular example, the values calculated in (2.38) and (2.39) are $\widehat { \beta } _ { 1 } ( 1 0 / 5 0 0 )$ and $\widehat { \beta } _ { 2 } ( 1 0 / 5 0 0 )$ . By doing this, we have regressed a series, $x _ { t }$ , of length $n$ using $n$ regression parameters, so that we will have a perfect fit. The point, however, is that if the data contain any cyclic behavior we are likely to catch it by performing these saturated regressions.

Next, note that the regression coefficients ${ \widehat { \beta } } _ { 1 } ( j / n )$ and $\widehat { \beta } _ { 2 } ( j / n )$ , for each $j$ , are essentially measuring the correlation of the data with a sinusoid oscillating at $j$ cycles in $n$ time points.4 Hence, an appropriate measure of the presence of a frequency of oscillation of $j$ cycles in $n$ time points in the data would be

$$
P (j / n) = \widehat {\beta} _ {1} ^ {2} (j / n) + \widehat {\beta} _ {2} ^ {2} (j / n), \tag {2.42}
$$

which is basically a measure of squared correlation. The quantity (2.42) is sometimes called the periodogram, but we will call $P ( j / n )$ the scaled periodogram and we will investigate its properties in Chapter 4. Figure 2.10 shows the scaled periodogram for the data generated by (2.35), and it

![](images/5c26606e3220b4e20288b46a74c75652d09781fedc90c4e6480aa25edbe8945f.jpg)  
Figure 2.10 The scaled periodogram, (2.42), of the 500 observations generated by (2.35). The data are displayed in Figures 1.11 and 2.9.

easily discovers the periodic component with frequency $\omega = . 0 2 = 1 0 / 5 0 0$ even though it is difficult to visually notice that component in Figure 1.11 due to the noise.

Finally, we mention that it is not necessary to run a large regression

$$
x _ {t} = \sum_ {j = 0} ^ {n / 2} \beta_ {1} (j / n) \cos (2 \pi t j / n) + \beta_ {2} (j / n) \sin (2 \pi t j / n) \tag {2.43}
$$

to obtain the values of $\beta _ { 1 } ( j / n )$ and $\beta _ { 2 } ( j / n )$ [with $\beta _ { 2 } ( 0 ) = \beta _ { 2 } ( 1 / 2 ) = 0 ]$ because they can be computed quickly if $n$ (assumed even here) is a highly composite integer. There is no error in (2.43) because there are $n$ observations and $n$ parameters; the regression fit will be perfect. The discrete Fourier transform (DFT) is a complex-valued weighted average of the data given by

$$
d (j / n) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} \exp (- 2 \pi i t j / n), \tag {2.44}
$$

and values $j / n$ are called the Fourier or fundamental frequencies. Because of a large number of redundancies in the calculation, (2.44) may be computed quickly using the fast Fourier transform (FFT), which is available in many computing packages such as Matlab, S-PLUS and R. We note that5

$$
\left| d (j / n) \right| ^ {2} = \frac {1}{n} \left(\sum_ {t = 1} ^ {n} x _ {t} \cos (2 \pi t j / n)\right) ^ {2} + \frac {1}{n} \left(\sum_ {t = 1} ^ {n} x _ {t} \sin (2 \pi t j / n)\right) ^ {2} \tag {2.45}
$$

and it is this quantity that is called the periodogram; we will write

$$
I (j / n) = | d (j / n) | ^ {2}.
$$

So, we may calculate the scaled periodogram, (2.42), using the periodogram as

$$
P (j / n) = \frac {4}{n} I (j / n). \tag {2.46}
$$

We will discuss this approach in more detail and provide examples with data in Chapter 4.

A figure similar to Figure 2.10 can be created in R using the following commands6:

```txt
> t = 1:500  
> x = 2*cos(2*pi*t/50 + .6*pi) + rnorm(500,0,5)  
> I = abs(fft(x)/sqrt(500))^2 # the periodogram  
> P = (4/500)*I # the scaled periodogram  
> f = 0:250/500  
> plot(f, P[1:251], type="l", xlab="frequency", ylab="")  
> abline(v=seq(0,.5,.02), lty="dotted") 
```

# Example 2.9 The Periodogram as a Matchmaker

Another way of understanding the results of the previous example is to consider the problem of matching the data with sinusoids oscillating at various frequency. For example, Figure 2.11 shows $n = 1 0 0$ observations (as a solid line) generated by the model

$$
x _ {t} = \cos (2 \pi t [ 2 / 1 0 0 ]) + w _ {t}, \tag {2.47}
$$

where $w _ { t }$ is Gaussian white noise with $\sigma _ { w } \ = \ 1$ . Superimposed on $x _ { t }$ are cosines oscillating at frequency 1/100, 2/100, and 3/100 (shown as dashed lines). Also included in the figure are correlations of $x _ { t }$ with the particular cosine, $\cos ( 2 \pi t j / 1 0 0 )$ , for $j = 1 , 2 , 3$ . Note that the data match up well with the cosine oscillating at 2 cycles every 100 points (with a correlation of .57), whereas the data do not match up well with the other two cosines. For example, in the top panel of Figure 2.11, there is a decreasing trend in the data until observation 25, and then the data start an increasing trend to observation 50, whereas the cosine making one cycle (1/100) continues to decrease until observation 50.

# 2.4: Smoothing

![](images/d6938c0481c16a685d78933123e9f4f5ae0ab478d2ffa0c5b15920270f64d313.jpg)

![](images/3a65f0d9257e685715f699c6ddf0cb15e0f4b8af9c029267163facb0201739b1.jpg)

![](images/6b339c291d6bc0a85b7dcc37750deee9c5373f3c8803223e019d969d1a3b5d2a.jpg)  
Figure 2.11 Data generated by (2.47) represented as a solid line with cosines oscillating at various frequencies superimposed (dashed lines). The correlation indicates the degree to which the two series line up.

# 2.4 Smoothing in the Time Series Context

In 1.4, we introduced the concept of smoothing a time series, and in Example 1.9, we discussed using a moving average to smooth white noise. This method is useful in discovering certain traits in a time series, such as long-term trend and seasonal components. In particular, if $x _ { t }$ represents the observations, then

$$
m _ {t} = \sum_ {j = - k} ^ {k} a _ {j} x _ {t - j}, \tag {2.48}
$$

where $a _ { j } = a _ { - j } \geq 0$ and $\textstyle \sum _ { j = - k } ^ { k } a _ { j } = 1$ is a symmetric moving average of the data.

# Example 2.10 Moving Average Smoother

For example, Figure 2.12 shows the weekly mortality series discussed in Example 2.2, a five-point moving average (which is essentially a monthly average with $k = 2$ ) that helps bring out the seasonal component and a 53-point moving average (which is essentially a yearly average with $k =$

![](images/aa03b2801db5de677ab1157ead23963f7dc43928b4c10f9326712ebe24b147a3.jpg)  
Figure 2.12 The weekly cardiovascular mortality series discussed in Example 2.2 smoothed using a five-week moving average and a 53-week moving average.

26) that helps bring out the (negative) trend in cardiovascular mortality. In both cases, the weights, $a _ { - k } , \dotsc , a _ { 0 } , \dotsc , a _ { k }$ , we used were all the same, and equal to $1 / ( 2 k + 1 )$ .7

To reproduce Figure 2.12 in R assuming the mortality series is in mort:

> t = 1:length(mort)   
> ma5 = filter(mort, sides=2, rep(1,5)/5)   
> ma53 = filter(mort, sides=2, rep(1,53)/53)   
> plot(t, mort, xlab="week", ylab="mortality")   
> lines(ma5)   
> lines(ma53)

Many other techniques are available for smoothing times series data based on methods from scatterplot smoothers. The general setup for a time plot is

$$
x _ {t} = f _ {t} + y _ {t}, \tag {2.49}
$$

where $f _ { t }$ is some smooth function of time, and $y _ { t }$ is a stationary process. We may think of the moving average smoother $m _ { t }$ , given in (2.48), as an estimator of $f _ { t }$ . An obvious choice for $f _ { t }$ in (2.49) is polynomial regression

$$
f _ {t} = \beta_ {0} + \beta_ {1} t + \dots + \beta_ {p} t ^ {p}. \tag {2.50}
$$

We have seen the results of a linear fit on the global temperature data in Example 2.1. For periodic data, one might employ periodic regression

$$
\begin{array}{l} f _ {t} = \alpha_ {0} + \alpha_ {1} \cos (2 \pi \omega_ {1} t) + \beta_ {1} \sin (2 \pi \omega_ {1} t) \\ + \dots + \alpha_ {p} \cos (2 \pi \omega_ {p} t) + \beta_ {p} \sin (2 \pi \omega_ {p} t), \tag {2.51} \\ \end{array}
$$

# 2.4: Smoothing

![](images/0ceb15251d13d28a052b6fb8fc1a3fe13fcddad746e0f83927b5fd1f8114f2c7.jpg)  
Figure 2.13 The weekly cardiovascular mortality series with a cubic trend and cubic trend plus periodic regression.

where $\omega _ { 1 } , \ldots , \omega _ { p }$ are distinct, specified frequencies. In addition, one might consider combining (2.50) and (2.51). These smoothers can be applied using classical linear regression.

# Example 2.11 Polynomial and Periodic Regression Smoothers

Figure 2.13 shows the weekly mortality series with an estimated (via ordinary least squares) cubic smoother

$$
\widehat {f _ {t}} = \widehat {\beta_ {0}} + \widehat {\beta_ {1}} t + \widehat {\beta_ {2}} t ^ {2} + \widehat {\beta_ {3}} t ^ {3}
$$

superimposed to emphasize the trend, and an estimated (via ordinary least squares) cubic smoother plus a periodic regression

$$
\widehat {f} _ {t} = \widehat {\beta} _ {0} + \widehat {\beta} _ {1} t + \widehat {\beta} _ {2} t ^ {2} + \widehat {\beta} _ {3} t ^ {3} + \widehat {\alpha} _ {1} \cos (2 \pi t / 5 2) + \widehat {\alpha} _ {2} \sin (2 \pi t / 5 2)
$$

superimposed to emphasize trend and seasonality.

The R commands for this example are:

> t = 1:length(mort)  
> t2 = t^2  
> t3 = t^3  
> c = cos(2*pi*t/52)  
> s = sin(2*pi*t/52)  
> fit1 = lm(mort^t + t2 + t3)  
> fit2 = lm(mort^t + t2 + t3 + c + s)  
> plot(t, mort)  
> lines.fit1\\(fit)  
> lines.fit2\\)fit

![](images/42821e7b2a2f63f92b9660fc489083d3595c64f59a7d2206e0af9a7ec023d37a.jpg)  
Figure 2.14 Kernel smoothers of the mortality data.

Modern regression techniques can be used to fit general smoothers to the pairs of points $( t , x _ { t } )$ where the estimate of $f _ { t }$ is smooth. Many of the techniques can easily be applied to time series data using the R or S-PLUS statistical packages; see Venables and Ripley (1994, Chapter 10) for details on applying these methods in S-PLUS (R is similar). A problem with the techniques used in Example 2.11 is that they assume $f _ { t }$ is the same function over the range of time, $t$ ; we might say that the technique is global. The moving average smoothers in Example 2.10 fit the data better because the technique is local; that is, moving average smoothers allow for the possibility that $f _ { t }$ is a different function over time. We describe some other local methods in the following examples.

# Example 2.12 Kernel Smoothing

Kernel smoothing is a moving average smoother that uses a weight function, or kernel, to average the observations. Figure 2.14 shows kernel smoothing of the mortality series, where $f _ { t }$ in (2.49) is estimated by

$$
\widehat {f} _ {t} = \sum_ {i = 1} ^ {n} w _ {t} (i) x _ {t}, \tag {2.52}
$$

where

$$
w _ {t} (i) = K \left(\frac {t - i}{b}\right) / \sum_ {j = 1} ^ {n} K \left(\frac {t - j}{b}\right). \tag {2.53}
$$

This estimator is called the Naradaya–Watson estimator (Watson, 1966). In (2.53), $K ( \cdot )$ is a kernel function; typically, the normal kernel, $K ( z ) =$ $\scriptstyle { \frac { 1 } { \sqrt { 2 \pi } } } \exp ( - z ^ { 2 } / 2 )$ , is used. To implement this in R, use the ksmooth function. The wider the bandwidth, $b$ , the smoother the result. In Figure 2.14, the values of $b$ for this example were $b = 1 0$ (roughly weighted

monthly averages; that is, $b / 2$ is the inner quartile range of the kernel) for the seasonal component, and $b = 1 0 4$ (roughly weighted yearly averages) for the trend component.

Figure 2.14 can be reproduced in R (or S-PLUS) as follows; we assume t and mort are available from the previous example:

> plot(t, mort)   
> lines(ksmooth(t, mort, "normal", bandwidth=5))   
> lines(ksmooth(t, mort, "normal", bandwidth=104))

# Example 2.13 Nearest Neighbor and Locally Weighted Regression

Another approach to smoothing a time plot is nearest neighbor regression. The technique is based on $k$ -nearest neighbors linear regression, wherein one uses the data $\left\{ x _ { t - k / 2 } , \ldots , x _ { t } , \ldots , x _ { t + k / 2 } \right\}$ to predict $x _ { t }$ using linear regression; the result is $\widehat { f } _ { t }$ . For example, Figure 2.15 shows cardiovascular mortality and the nearest neighbor method using the R (or S-PLUS) smoother supsmu. We used $k = n / 2$ to estimate the trend and $k = n / 1 0 0$ to estimate the seasonal component. In general, supsmu uses a variable window for smoothing (see Friedman, 1984), but it can be used for correlated data by fixing the smoothing window, as was done here.

Lowess is a method of smoothing that is rather complex, but the basic idea is close to nearest neighbor regression. Figure 2.15 shows smoothing of mortality using the R or S-PLUS function lowess (see Cleveland, 1979). First, a certain proportion of nearest neighbors to $x _ { t }$ are included in a weighting scheme; values closer to $x _ { t }$ in time get more weight. Then, a robust weighted regression is used to predict $x _ { t }$ and obtain the smoothed estimate of $f _ { t }$ . The larger the fraction of nearest neighbors included, the smoother the estimate $\widehat { f } _ { t }$ will be. In Figure 2.15, the smoother uses about two-thirds of the data to obtain an estimate of the trend component, and the seasonal component uses 2% of the data.

Figure 2.15 can be reproduced in R or S-PLUS as follows (assuming t and mort are available from the previous example):

> par(mfrow=c(2,1))  
> plot(t, mort, main="nearest neighbor")   
> lines(supsmu(t, mort, span $\cdot ^ { = }$ .5))   
> lines(supsmu(t, mort, span=.01))   
> plot(t, mort, main $\varepsilon$ "lowess")   
> lines(lowess(t, mort, .02))   
> lines(lowess(t, mort, 2/3))

![](images/ba0236c196713d03530c69e0371fbee8c78a4098121ca1afa6611099dae78457.jpg)  
nearest neighbor   
lowess

![](images/625e87177199d56342b8801971f1c6486b1da20e04dc43428fe3adf249a3c760.jpg)  
Figure 2.15 Nearest neightbor (supsmu) and locally weighted least squares (lowess) smoothers of the mortality data.

# Example 2.14 Smoothing Splines

An extension of polynomial regression is to first divide time $t = 1 , \ldots , n$ , into $k$ intervals, $[ t _ { 0 } = 1 , t _ { 1 } ]$ , $[ t _ { 1 } + 1 , t _ { 2 } ]$ , . . . , $[ t _ { k - 1 } + 1 , t _ { k } = n ]$ . The values $t _ { 0 } , t _ { 1 } , \ldots , t _ { k }$ are called knots. Then, in each interval, one fits a regression of the form (2.50); typically, $p = 3$ , and this is called cubic splines.

A related method is smoothing splines, which minimizes a compromise between the fit and the degree of smoothness given by

$$
\sum_ {t = 1} ^ {n} \left[ x _ {t} - f _ {t} \right] ^ {2} + \lambda \int \left(f _ {t} ^ {\prime \prime}\right) ^ {2} d t, \tag {2.54}
$$

where $f _ { t }$ is a cubic spline with a knot at each $t$ . The degree of smoothness is controlled by $\lambda > 0$ . Figure 2.16 shows smoothing splines on mortality using $\lambda = 1 0 ^ { - 7 }$ for the seasonal component, and $\lambda = 0 . 1$ for the trend.

![](images/82bdd433c29a1090106a9b14ffde82c0c88e6bf188ffac68ceb8c80b2e9af897.jpg)  
Figure 2.16 Smoothing splines fit to the mortality data.

Figure 2.16 can be reproduced in R or S-PLUS as follows (assuming t and mort are available from the previous example):

> plot(t, mort)   
> lines(smooth.spline(t, mort, spar=.0000001))   
> lines(smooth.spline(t, mort, spar=.1))

# Example 2.15 Smoothing One Series as a Function of Another

In addition to smoothing time plots, smoothing techniques can be applied to smoothing a time series as a function of another time series. In this example, we smooth the scatterplot of two contemporaneously measured time series, mortality as a function of temperature. In Example 2.2, we discovered a nonlinear relationship between mortality and temperature. Continuing along these lines, Figure 2.17 shows scatterplots of mortality, $M _ { t }$ , and temperature, $T _ { t }$ , along with $M _ { t }$ is smoothed as a function of $T _ { t }$ using lowess and using smoothing splines. In both cases, mortality increases at extreme temperatures, but in an asymmetric way; mortality is higher at colder temperatures than at hotter temperatures. The minimum mortality rate seems to occur at approximately $8 0 ^ { \circ } \mathrm { ~ F ~ }$ .

Figure 2.17 can be reproduced in R or S-PLUS as follows (assuming mort and temp contain the mortality and temperature data):

> par(mfrow=c(2,1))  
> plot(temp, mort, main="lowess")   
> lines(lowess(temp,mort))   
> plot(temp, mort, main="smoothing splines")   
> lines(smooth.spline(temp,mort))

![](images/1176416c4ff4fa1db2f99996e0c4500a9d3dfa9ffb15fd807234c7de094730a5.jpg)

![](images/da10a1be6c3dd643dab27f0837ea32e615dfd6345a68a75fd7fa83cb8ed32c83.jpg)  
Figure 2.17 Smoothers of mortality as a function of temperature using lowess and smoothing splines.

As a final word of caution, the methods mentioned above do not particularly take into account the fact that the data are serially correlated, and most of the techniques mentioned have been designed for independent observations. That is, for example, the smoothers shown in Figure 2.17 are calculated under the false assumption that the pairs $( M _ { t } , T _ { t } )$ , for $t = 1 , \ldots , 5 0 8$ , are iid pairs of observations. In addition, the degree of smoothness used in the previous examples were chosen arbitrarily to bring out what might be considered obvious features in the data set.

# Problems

# Section 2.2

2.1 For the Johnson & Johnson data, say $y _ { t }$ , for $t = 1 , \ldots , 8 4$ , shown in Figure 1.1, let $x _ { t } = \ln ( y _ { t } )$ .

(a) Fit the regression model

$$
x _ {t} = \beta t + \alpha_ {1} Q _ {1} (t) + \alpha_ {2} Q _ {2} (t) + \alpha_ {3} Q _ {3} (t) + \alpha_ {4} Q _ {4} (t) + w _ {t}
$$

where $Q _ { i } ( t ) = 1$ if time $t$ corresponds to quarter $i = 1 , 2 , 3 , 4$ , and zero otherwise. The $Q _ { i } ( t )$ ’s are called indicator variables. We will assume for now that $w _ { t }$ is a Gaussian white noise sequence. What is the interpretation of the parameters $\beta$ , $\alpha _ { 1 }$ , $\alpha _ { 2 }$ , $\alpha _ { 3 }$ , and $\alpha _ { 4 }$ ? [Note: In R, to regress $x$ on $z$ without an intercept, use $\mathtt { l m } ( \mathtt { x } ^ { \sim } 0 + \mathtt { z } )$ ; an easy way to generate $Q _ { 1 } ( t )$ is $\mathtt { Q 1 = r e p } ( \mathtt { c } ( 1 , 0 , 0 , 0 ) , 2 1 )$ .]

(b) What happens if you include an intercept term in the model in (a)?   
(c) Graph the data, $x _ { t }$ , and superimpose the fitted values, say $\widehat { x } _ { t }$ , on the graph. Examine the residuals, $x _ { t } - \widehat { x } _ { t }$ , and state your conclusions. Does it appear that the model fits the data well?

2.2 For the mortality data examined in Example 2.2:

(a) Add another component to the regression in (2.24) that accounts for the particulate count four weeks prior; that is, add $P _ { t - 4 }$ to the regression in (2.24). State your conclusion. [Note: In R, make sure the data are time series objects by using the ts() command, e.g., mort=ts(mort). Center the temperature series and let ${ \mathrm { ~ \bf ~ t ~ } } =$ ts(1:length(mort)). Then use ts.intersect(mort, t, temp, tempˆ2, part, lag(part,-4)) to combine the series into a time series matrix object with six columns and regress the first column on the other columns.]   
(b) Draw a scatterplot matrix of $M _ { t } , T _ { t } , P _ { t }$ and $P _ { t - 4 }$ and then calculate the pairwise correlations between the series. Compare the relationship between $M _ { t }$ and $P _ { t }$ versus $M _ { t }$ and $P _ { t - 4 }$ .

2.3 Generate a random walk with drift, (1.4), of length $n = 5 0 0$ with $\delta = . 1$ and $\sigma _ { w } \ = \ 1$ . Call the data $x _ { t }$ for $t = 1 , \ldots , 5 0 0$ . Fit the regression $x _ { t } = \beta t + w _ { t }$ using least squares. Plot the data, the mean function (i.e., $\mu _ { t } = . 1 t$ ) and the fitted line, $\widehat { x } _ { t } = \widehat { \beta } t$ , on the same graph. Discuss your results.

2.4 Kullback-Leibler Information. Given the random vector $\pmb { y }$ , we define the information for discriminating between two densities in the same family,

indexed by a parameter $\pmb \theta$ , say $f ( \pmb { y } ; \pmb { \theta } _ { 1 } )$ and $f ( \pmb { y } ; \pmb { \theta } _ { 2 } )$ , as

$$
I \left(\boldsymbol {\theta} _ {1}; \boldsymbol {\theta} _ {2}\right) = \frac {1}{n} E _ {1} \ln \frac {f (\boldsymbol {y} ; \boldsymbol {\theta} _ {1})}{f (\boldsymbol {y} ; \boldsymbol {\theta} _ {2})}, \tag {2.55}
$$

where $E _ { 1 }$ denotes expectation with respect to the density determined by $\pmb { \theta } _ { 1 }$ . For the Gaussian regression model, the parameters are $\pmb { \theta } = ( \beta ^ { \prime } , \sigma ^ { 2 } ) ^ { \prime }$ . Show that we obtain

$$
I \left(\boldsymbol {\theta} _ {1}; \boldsymbol {\theta} _ {2}\right) = \frac {1}{2} \left(\frac {\sigma_ {1} ^ {2}}{\sigma_ {2} ^ {2}} - \ln \frac {\sigma_ {1} ^ {2}}{\sigma_ {2} ^ {2}} - 1\right) + \frac {1}{2} \frac {\left(\boldsymbol {\beta} _ {1} - \boldsymbol {\beta} _ {2}\right) ^ {\prime} Z ^ {\prime} Z \left(\boldsymbol {\beta} _ {1} - \boldsymbol {\beta} _ {2}\right)}{n \sigma_ {2} ^ {2}} \tag {2.56}
$$

in that case.

2.5 Model Selection. Both selection criteria (2.18) and (2.19) are derived from information theoretic arguments, based on the well-known Kullback– Leibler discrimination information numbers (see Kullback and Leibler, 1951, Kullback, 1978). We give an argument due to Hurvich and Tsai (1989). We think of the measure (2.56) as measuring the discrepancy between the two densities, characterized by the parameter values $\pmb { \theta } _ { 1 } ^ { \prime } =$ $( \beta _ { 1 } ^ { \prime } , \sigma _ { 1 } ^ { 2 } ) ^ { \prime }$ and $\pmb { \theta } _ { 2 } ^ { \prime } = ( \beta _ { 2 } ^ { \prime } , \sigma _ { 2 } ^ { 2 } ) ^ { \prime }$ . Now, if the true value of the parameter vector is $\pmb { \theta } _ { 1 }$ , we argue that the best model would be one that minimizes the discrepancy between the theoretical value and the sample, say $I ( \pmb \theta _ { 1 } ; \widehat \pmb \theta )$ . Because $\pmb { \theta } _ { 1 }$ will not be known, Hurvich and Tsai (1989) considered finding an unbiased estimator for $E _ { 1 } [ I ( \pmb { \beta } _ { 1 } , \sigma _ { 1 } ^ { 2 } ; \hat { \pmb { \beta } } , \hat { \sigma } ^ { 2 } ) ]$ , where

$$
I (\boldsymbol {\beta} _ {1}, \sigma_ {1} ^ {2}; \hat {\boldsymbol {\beta}}, \hat {\sigma} ^ {2}) = \frac {1}{2} \left(\frac {\sigma_ {1} ^ {2}}{\hat {\sigma} ^ {2}} - \ln \frac {\sigma_ {1} ^ {2}}{\hat {\sigma} ^ {2}} - 1\right) + \frac {1}{2} \frac {(\boldsymbol {\beta} _ {1} - \hat {\boldsymbol {\beta}}) ^ {\prime} Z ^ {\prime} Z (\boldsymbol {\beta} _ {1} - \hat {\boldsymbol {\beta}})}{n \hat {\sigma} ^ {2}}
$$

and $\beta$ is a $k \times 1$ regression vector. Show that

$$
E _ {1} \left[ I \left(\boldsymbol {\beta} _ {1}, \sigma_ {1} ^ {2}; \hat {\boldsymbol {\beta}}, \hat {\sigma} ^ {2}\right) \right] = \frac {1}{2} \left(- \ln \sigma_ {1} ^ {2} + E _ {1} \ln \widehat {\sigma} ^ {2} + \frac {n + k}{n - k - 2} - 1\right), \tag {2.57}
$$

using the distributional properties of the regression coefficients and error variance. An unbiased estimator for $E _ { 1 } \log { \widehat { \sigma } } ^ { 2 }$ is $\log { \widehat { \sigma } } ^ { 2 }$ . Hence, we have shown that the expectation of the above discrimination information is as claimed. As models with differing dimensions $k$ are considered, only the second and third terms in (2.57) will vary and we only need unbiased estimators for those two terms. This gives the form of AICc quoted in (2.19) in the chapter. You will need the two distributional results

$$
\frac {n \hat {\sigma} ^ {2}}{\sigma_ {1} ^ {2}} \sim \chi_ {n - k} ^ {2}
$$

and

$$
\frac {(\widehat {\boldsymbol {\beta}} - \boldsymbol {\beta} _ {1}) ^ {\prime} Z ^ {\prime} Z (\widehat {\boldsymbol {\beta}} - \boldsymbol {\beta} _ {1})}{\sigma_ {1} ^ {2}} \sim \chi_ {k} ^ {2}
$$

The two quantities are distributed independently as chi-squared distributions with the indicated degrees of freedom. If $x \sim \chi _ { n } ^ { 2 }$ , $E ( 1 / x ) =$ $1 / ( n - 2 )$ .

# Section 2.3

2.6 Consider a process consisting of a linear trend with an additive noise term consisting of independent random variables $w _ { t }$ with zero means and variances $\sigma _ { w } ^ { 2 }$ , that is,

$$
x _ {t} = \beta_ {0} + \beta_ {1} t + w _ {t},
$$

where $\beta _ { 0 } , \beta _ { 1 }$ are fixed constants.

(a) Prove $x _ { t }$ is nonstationary.   
(b) Prove that the first difference series $\nabla x _ { t } = x _ { t } - x _ { t - 1 }$ is stationary by finding its mean and autocovariance function.   
(c) Repeat part (b) if $w _ { t }$ is replaced by a general stationary process, say $y _ { t }$ , with mean function $\mu _ { y }$ and autocovariance function $\gamma _ { y } ( h )$ .

2.7 Show (2.28) is stationary.

2.8 The glacial varve record plotted in Figure 2.6 exhibits some nonstationarity that can be improved by transforming to logarithms and some additional nonstationarity that can be corrected by differencing the logarithms.

(a) Verify that the untransformed glacial varves has intervals over which $\widehat { \gamma } ( 0 )$ changes by computing the zero-lag autocovariance over two different intervals. Argue that the transformation $y _ { t } = \ln x _ { t }$ stabilizes the variance over the series. Plot the histograms of $x _ { t }$ and $y _ { t }$ to see whether the approximation to normality is improved by transforming the data.   
(b) Examine the sample ACF, ${ \widehat { \rho } } _ { y } ( h )$ , of $y _ { t }$ and comment. Do any time intervals, of the order 100 years, exist where one can observe behavior comparable to that observed in the global temperature records in Figure 1.2?   
(c) Compute the first difference $u _ { t } = y _ { t } - y _ { t - 1 }$ of the log transformed varve records, and examine its time plot and autocorrelation function, $\widehat { \rho } _ { u } ( h )$ , and argue that a first difference produces a reasonably stationary series. Can you think of a practical interpretation for $u _ { t }$ ?   
(d) Based on the sample ACF of the differenced transformed series computed in (c), argue that a generalization of the model given by Example 1.23 might be reasonable. Assume

$$
u _ {t} = \mu_ {u} + w _ {t} - \theta w _ {t - 1}
$$

is stationary when the inputs $w _ { t }$ are assumed independent with mean 0 and variance $\sigma _ { w } ^ { 2 }$ . Show that

$$
\gamma_ {u} (h) = \left\{ \begin{array}{l l} \sigma_ {w} ^ {2} (1 + \theta^ {2}) & \text {i f} h = 0 \\ - \theta   \sigma_ {w} ^ {2} & \text {i f} h = \pm 1 \\ 0 & \text {i f} | h | \geq 1. \end{array} \right.
$$

Using the sample ACF and the printed autocovariance $\widehat { \gamma } _ { u } ( 0 )$ , derive estimators for $\theta$ and $\sigma ^ { 2 }$ . This is an application of the method of moments from classical statistics, where estimators of the parameters are derived by equating sample moments to theoretical moments.

2.9 Consider the two time series representing average wholesale U.S. gas and oil prices over 180 months, beginning in July 1973 and ending in December 1987. Analyze the data using some of the techniques in this chapter with the idea that we should be looking at how changes in oil prices influence changes in gas prices. For further reading, see Liu (1991). In particular,

(a) Plot the raw data, and look at the autocorrelation functions to argue that the untransformed data series are nonstationary.

(b) It is often argued in economics that price changes are important, in particular, the percentage change in prices from one month to the next. On this basis, argue that a transformation of the form $y _ { t } = \ln x _ { t } - \ln x _ { t - 1 }$ might be applied to the data, where $x _ { t }$ is the oil or gas price series.

(c) Use lagged multiple scatterplots and the autocorrelation and crosscorrelation functions of the transformed oil and gas price series to investigate the properties of these series. Is it possible to guess whether gas prices are raised more quickly in response to increasing oil prices than they are decreased when oil prices are decreased? Use the cross-correlation function over the first 100 months compared with the cross-correlation function over the last 80 months. Do you think that it might be possible to predict log percentage changes in gas prices from log percentage changes in oil prices? Plot the two series on the same scale.

2.10 In this problem, we will explore the periodic nature of $S _ { t }$ , the SOI series displayed in Figure 1.5.

(a) Detrend the series by fitting a regression of $S _ { t }$ on time $t$ . Is there a significant trend in the sea surface temperature? Comment.   
(b) Calculate the periodogram for the detrended series obtained in part (a). Identify the frequencies of the two main peaks (with an obvious one at the frequency of one cycle every 12 months). What is the probable El Ni˜no cycle indicated by the minor peak?

# Problems

# Section 2.4

2.11 For the data plotted in Figure 1.5, let $S _ { t }$ denote the SOI index series, and let $R _ { t }$ denote the Recruitment series.

(a) Draw a lag plot similar to the one in Figure 2.7 for $R _ { t }$ and comment.   
(b) Reexamine the scatterplot matrix of $R _ { t }$ versus $S _ { t - h }$ shown in Figure 2.8 and the CCF of the two series shown in Figure 1.14, and fit the regression

$$
\begin{array}{l} R _ {t} = \alpha + \beta_ {0} S _ {t} + \beta_ {1} S _ {t - 1} + \beta_ {2} S _ {t - 2} + \beta_ {3} S _ {t - 3} + \beta_ {4} S _ {t - 4} \\ + \beta_ {5} S _ {t - 5} + \beta_ {6} S _ {t - 6} + \beta_ {7} S _ {t - 7} + \beta_ {8} S _ {t - 8} + w _ {t}. \\ \end{array}
$$

Compare the magnitudes and signs of the coefficients $\beta _ { 0 } , \ldots , \beta _ { 8 }$ with the scatterplots in Figure 2.8 and with the CCF in Figure 1.14.

(c) Use some of the smoothing techniques described in §2.4 to discover whether a trend exists in the Recruitment series, $R _ { t }$ , and to explore the periodic behavior of the data.   
(d) In Example 2.6, some nonlinear behavior exists between the current value of Recruitment and past values of the SOI index. Use the smoothing techniques described in §2.4 to explore this possibility, concentrating on the scatterplot of $R _ { t }$ versus $S _ { t - 6 }$ .

2.12 Use a smoothing technique described in 2.4 to estimate the trend in the global temperature series displayed in Figure 1.2. Use the entire data set (see Example 2.1 for details).

# Chapter 3

# ARIMA Models

# 3.1 Introduction

In Chapters 1 and 2, we introduced autocorrelation and cross-correlation functions (ACFs and CCFs) as tools for clarifying relations that may occur within and between time series at various lags. In addition, we explained how to build linear models based on classical regression theory for exploiting the associations indicated by large values of the ACF or CCF. The time domain, or regression, methods of this chapter are appropriate when we are dealing with possibly nonstationary, shorter time series; these series are the rule rather than the exception in many applications. In addition, if the emphasis is on forecasting future values, then the problem is easily treated as a regression problem. This chapter develops a number of regression techniques for time series that are all related to classical ordinary and weighted or correlated least squares.

Classical regression is often insufficient for explaining all of the interesting dynamics of a time series. For example, the ACF of the residuals of the simple linear regression fit to the global temperature data (see Example 2.3 of Chapter 2) reveals additional structure in the data that the regression did not capture. Instead, the introduction of correlation as a phenomenon that may be generated through lagged linear relations leads to proposing the autoregressive (AR) and autoregressive moving average (ARMA) models. Adding nonstationary models to the mix leads to the autoregressive integrated moving average (ARIMA) model popularized in the landmark work by Box and Jenkins (1970). The Box–Jenkins method for identifying a plausible ARIMA model is given in this chapter along with techniques for parameter estimation and forecasting for these models. A partial theoretical justification of the use of ARMA models is discussed in Appendix B, §B.4.

# 3.2 Autoregressive Moving Average Models

The classical regression model of Chapter 2 was developed for the static case, namely, we only allow the dependent variable to be influenced by current values of the independent variables. In the time series case, it is desirable to allow the dependent variable to be influenced by the past values of the independent variables and possibly by its own past values. If the present can be plausibly modeled in terms of only the past values of the independent inputs, we have the enticing prospect that forecasting will be possible.

# Introduction to Autoregressive Models

Autoregressive models are based on the idea that the current value of the series, $x _ { t }$ , can be explained as a function of $p$ past values, $x _ { t - 1 } , x _ { t - 2 } , . . . , x _ { t - p }$ , where $p$ determines the number of steps into the past needed to forecast the current value. As a typical case, recall Example 1.10 in which data were generated using the model

$$
x _ {t} = x _ {t - 1} -. 9 0 x _ {t - 2} + w _ {t},
$$

where $w _ { t }$ is white Gaussian noise with $\sigma _ { w } ^ { 2 } = 1$ . We have now assumed the current value is a particular linear function of past values. The regularity that persists in Figure 1.9 gives an indication that forecasting for such a model might be a distinct possibility, say, through some version such as

$$
x _ {n + 1} ^ {n} = x _ {n} -. 9 0 x _ {n - 1},
$$

where the quantity on the left-hand side denotes the forecast at the next period $n { \mathrel { + { 1 } } }$ based on the observed data, $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ . We will make this notion more precise in our discussion of forecasting (§3.5).

The extent to which it might be possible to forecast a real data series from its own past values can be assessed by looking at the autocorrelation function and the lagged scatterplot matrices discussed in Chapter 2. For example, the lagged scatterplot matrix for the Southern Oscillation Index (SOI), shown in Figure 2.7, gives a distinct indication that lags 1 and 2, for example, are linearly associated with the current value. The ACF shown in Figure 1.14 shows relatively large positive values at lags 1, 2, 12, 24, and 36 and large negative values at 18, 30, and 42. We note also the possible relation between the SOI and Recruitment series indicated in the scatterplot matrix shown in Figure 2.8. We will indicate in later sections on transfer function and vector AR modeling how to handle the dependence on values taken by other series.

The preceding discussion motivates the following definition.

Definition 3.1 An autoregressive model of order p, abbreviated AR(p), is of the form

$$
x _ {t} = \phi_ {1} x _ {t - 1} + \phi_ {2} x _ {t - 2} + \dots + \phi_ {p} x _ {t - p} + w _ {t}, \tag {3.1}
$$

where $x _ { t }$ is stationary, $\phi _ { 1 } , \phi _ { 2 } , \ldots , \phi _ { p }$ are constants ( $\phi _ { p } \neq 0$ ). Unless otherwise stated, we assume that $w _ { t }$ is a Gaussian white noise series with mean zero and

variance $\sigma _ { w } ^ { 2 }$ . The mean of $x _ { t }$ in (3.1) is zero. If the mean, µ, of $x _ { t }$ is not zero, replace $x _ { t }$ by $x _ { t } - \mu$ in (3.1), i.e.,

$$
x _ {t} - \mu = \phi_ {1} (x _ {t - 1} - \mu) + \phi_ {2} (x _ {t - 2} - \mu) + \dots + \phi_ {p} (x _ {t - p} - \mu) + w _ {t},
$$

or write

$$
x _ {t} = \alpha + \phi_ {1} x _ {t - 1} + \phi_ {2} x _ {t - 2} + \dots + \phi_ {p} x _ {t - p} + w _ {t}, \tag {3.2}
$$

where $\alpha = \mu ( 1 - \phi _ { 1 } - \cdot \cdot \cdot - \phi _ { p } )$ .

We note that (3.2) is similar to the regression model of 2.2, and hence the term auto (or self) regression. Some technical difficulties, however, develop from applying that model because the regressors, $x _ { t - 1 } , \ldots , x _ { t - p }$ , are random components, whereas ${ z } _ { t }$ was assumed to be fixed. A useful form follows by using the backshift operator (2.30) to write the $\operatorname { A R } ( p )$ model, (3.1), as

$$
\left(1 - \phi_ {1} B - \phi_ {2} B ^ {2} - \dots - \phi_ {p} B ^ {p}\right) x _ {t} = w _ {t}, \tag {3.3}
$$

or even more concisely as

$$
\phi (B) x _ {t} = w _ {t}. \tag {3.4}
$$

The properties of $\phi ( B )$ are important in solving (3.4) for $x _ { t }$ . This leads to the following definition.

Definition 3.2 The autoregressive operator is defined to be

$$
\phi (B) = 1 - \phi_ {1} B - \phi_ {2} B ^ {2} - \dots - \phi_ {p} B ^ {p} \tag {3.5}
$$

We initiate the investigation of AR models by considering the first-order model, AR(1), given by $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ . Iterating backwards $k$ times, we get

$$
\begin{array}{l} x _ {t} = \phi x _ {t - 1} + w _ {t} = \phi \left(\phi x _ {t - 2} + w _ {t - 1}\right) + w _ {t} \\ = \phi^ {2} x _ {t - 2} + \phi w _ {t - 1} + w _ {t} \\ \begin{array}{c} \vdots \\ \vdots \\ \vdots \end{array} \\ = \phi^ {k} x _ {t - k} + \sum_ {j = 0} ^ {k - 1} \phi^ {j} w _ {t - j}. \\ \end{array}
$$

This method suggests that, by continuing to iterate backwards, and provided that $| \phi | < 1$ and $x _ { t }$ is stationary, we can represent an AR(1) model as a linear process given by1

$$
x _ {t} = \sum_ {j = 0} ^ {\infty} \phi^ {j} w _ {t - j}. \tag {3.6}
$$

# 3.2: ARMA Models

The AR(1) process defined by (3.6) is stationary with mean

$$
E (x _ {t}) = \sum_ {j = 0} ^ {\infty} \phi^ {j} E (w _ {t - j}) = 0,
$$

and autocovariance function,

$$
\begin{array}{l} \gamma (h) = \operatorname {c o v} (x _ {t + h}, x _ {t}) = E \left[ \left(\sum_ {j = 0} ^ {\infty} \phi^ {j} w _ {t + h - j}\right) \left(\sum_ {k = 0} ^ {\infty} \phi^ {k} w _ {t - k}\right) \right] \\ = \sigma_ {w} ^ {2} \sum_ {j = 0} ^ {\infty} \phi^ {j} \phi^ {j + h} = \sigma_ {w} ^ {2} \phi^ {h} \sum_ {j = 0} ^ {\infty} \phi^ {2 j} = \frac {\sigma_ {w} ^ {2} \phi^ {h}}{1 - \phi^ {2}}, \quad h \geq 0. \tag {3.7} \\ \end{array}
$$

Recall that $\gamma ( h ) = \gamma ( - h )$ , so we will only exhibit the autocovariance function for $h \geq 0$ . From (3.7), the ACF of an AR(1) is

$$
\rho (h) = \frac {\gamma (h)}{\gamma (0)} = \phi^ {h}, \quad h \geq 0, \tag {3.8}
$$

and $\rho ( h )$ satisfies the recursion

$$
\rho (h) = \phi \rho (h - 1), \quad h = 1, 2, \dots . \tag {3.9}
$$

We will discuss the ACF of a general AR(p) model in §3.4.

# Example 3.1 The Sample Path of an AR(1) Process

Figure 3.1 shows a time plot of two AR(1) processes, one with $\phi = . 9$ and one with $\phi = - . 9$ ; in both cases, $\sigma _ { w } ^ { 2 } = 1$ . In the first case, $\rho ( h ) = . 9 ^ { h }$ , for $h \geq 0$ , so observations close together in time are positively correlated with each other. This result means that observations at contiguous time points will tend to be close in value to each other; this fact shows up in the top of Figure 3.1 as a very smooth sample path for $x _ { t }$ . Now, contrast this to the case in which $\phi = - . 9$ , so that $\rho ( h ) \ : = \ : ( - . 9 ) ^ { h }$ , for $h \geq 0$ . This result means that observations at contiguous time points are negatively correlated but observations two time points apart are positively correlated. This fact shows up in the bottom of Figure 3.1, where, for example, if an observation, $x _ { t }$ , is positive, the next observation, $x _ { t + 1 }$ , is typically negative, and the next observation, $x _ { t + 2 }$ , is typically positive. Thus, in this case, the sample path is very choppy.

A figure similar to Figure 3.1 can be created in R using the following commands:

> par(mfrow=c(2,1))  
> plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100),   
+ ylab="x",main $\cdot ^ { = }$ (expression("AR(1) "*phi*" = +.9")))   
> plot(arima.sim(list(order $\Bumpeq { \bf c }$ (1,0,0), $\mathsf { a r } ^ { = - } \cdot \mathsf { 9 } _ { \ell } ^ { \cdot }$ ), $\mathtt { n } = 1 0 0 $ ),   
+ ylab="x",main $\cdot ^ { = }$ (expression("AR(1) "*phi*" = -.9")))

![](images/28f98e7444072069394d8a632d559a917eefae28db8f2ade470d0648e64296f2.jpg)

![](images/398c3805583dd8099e41071103a398a0efea5417d45754f604d37f24619f4caa.jpg)  
Figure 3.1 Simulated AR(1) models: $\phi = . 9$ (top); $\phi = - . 9$ (bottom).

# Example 3.2 Explosive AR Models and Causality

In Example 1.18, it was discovered that the random walk $x _ { t } = x _ { t - 1 } + w _ { t }$ is not stationary. We might wonder whether there is a stationary AR(1) process with $| \phi | > 1$ . Such processes are called explosive because the values of the time series quickly become large in magnitude. Clearly, because $| \phi | ^ { j }$ increases without bound as $j  \infty$ , $\sum _ { j = 0 } ^ { k - 1 } \phi ^ { j } w _ { t - j }$ will not converge (in mean square) as $k  \infty$ , so the intuition used to get (3.6) will not work directly. We can, however, modify that argument to obtain a stationary model as follows. Write $\boldsymbol { x } _ { t + 1 } = \boldsymbol { \phi } \boldsymbol { x } _ { t } + \boldsymbol { w } _ { t + 1 }$ , in which case,

$$
\begin{array}{l} x _ {t} = \phi^ {- 1} x _ {t + 1} - \phi^ {- 1} w _ {t + 1} = \phi^ {- 1} \left(\phi^ {- 1} x _ {t + 2} - \phi^ {- 1} w _ {t + 2}\right) - \phi^ {- 1} w _ {t + 1} \\ \begin{array}{c} \vdots \\ \vdots \\ \vdots \end{array} \\ = \phi^ {- k} x _ {t + k} - \sum_ {j = 1} ^ {k - 1} \phi^ {- j} w _ {t + j}, \tag {3.10} \\ \end{array}
$$

by iterating forward $k$ steps. Because $| \phi | ^ { - 1 } < 1$ , this result suggests the

# 3.2: ARMA Models

stationary future dependent AR(1) model

$$
x _ {t} = - \sum_ {j = 1} ^ {\infty} \phi^ {- j} w _ {t + j}.
$$

The reader can verify that this is stationary and of the AR(1) form $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ . Unfortunately, this model is useless because it requires us to know the future to be able to predict the future. When a process does not depend on the future, such as the AR(1) when $| \phi | < 1$ , we will say the process is causal. In the explosive case of this example, the process is stationary, but it is also future dependent, and not causal.

The technique of iterating backwards to get an idea of the stationary solution of AR models works well when $p = 1$ , but not for larger orders. A general technique is that of matching coefficients. Consider the AR(1) model in operator form

$$
\phi (B) x _ {t} = w _ {t}, \tag {3.11}
$$

where $\phi ( B ) = 1 - \phi B$ , and $| \phi | < 1$ . Also, write the model in equation (3.6) using operator form as

$$
x _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j} = \psi (B) w _ {t}, \tag {3.12}
$$

where $\begin{array} { r } { \psi ( B ) = \sum _ { j = 0 } ^ { \infty } \psi _ { j } B ^ { j } } \end{array}$ and $\psi _ { j } ~ = ~ \phi ^ { j }$ . Suppose we did not know that $\psi _ { j } = \phi ^ { j }$ . We could substitute $\psi ( B ) w _ { t }$ from (3.12) for $x _ { t }$ in (3.11) to obtain

$$
\phi (B) \psi (B) w _ {t} = w _ {t}. \tag {3.13}
$$

The coefficients of $B$ on the left-hand side of (3.13) must be equal to those on right-hand side of (3.13), which means

$$
(1 - \phi B) (1 + \psi_ {1} B + \psi_ {2} B ^ {2} + \dots + \psi_ {j} B ^ {j} + \dots) = 1. \tag {3.14}
$$

Reorganizing the coefficients in (3.14),

$$
1 + (\psi_ {1} - \phi) B + (\psi_ {2} - \psi_ {1} \phi) B ^ {2} + \dots + (\psi_ {j} - \psi_ {j - 1} \phi) B ^ {j} + \dots = 1,
$$

we see that for each $j = 1 , 2 , \dots$ , the coefficient of $B ^ { j }$ on the left must be zero because it is zero on the right. The coefficient of $B$ on the left is $( \psi _ { 1 } - \phi )$ , and equating this to zero, $\psi _ { 1 } - \phi = 0$ , leads to $\psi _ { 1 } = \phi$ . Continuing, the coefficient of $B ^ { 2 }$ is $( \psi _ { 2 } - \psi _ { 1 } \phi )$ , so $\psi _ { 2 } = \phi ^ { 2 }$ . In general,

$$
\psi_ {j} = \psi_ {j - 1} \phi ,
$$

with $\psi _ { 0 } = 1$ , which leads to the general solution $\psi _ { j } = \phi ^ { j }$ .

Another way to think about the operations we just performed is to consider the AR(1) model in operator form, $\phi ( B ) x _ { t } = w _ { t }$ . Now multiply both sides by $\phi ^ { - 1 } ( B )$ (assuming the inverse operator exists) to get

$$
\phi^ {- 1} (B) \phi (B) x _ {t} = \phi^ {- 1} (B) w _ {t},
$$

or

$$
x _ {t} = \phi^ {- 1} (B) w _ {t}.
$$

We know already that

$$
\phi^ {- 1} (B) = 1 + \phi B + \phi^ {2} B ^ {2} + \dots + \phi^ {j} B ^ {j} + \dots ,
$$

that is, $\phi ^ { - 1 } ( B )$ is $\psi ( B )$ in (3.12). Thus, we notice that working with operators is like working with polynomials. That is, consider the polynomial $\phi ( z ) = $ $1 - \phi z$ , where $z$ is a complex number and $| \phi | < 1$ . Then,

$$
\phi^ {- 1} (z) = \frac {1}{(1 - \phi z)} = 1 + \phi z + \phi^ {2} z ^ {2} + \dots + \phi^ {j} z ^ {j} + \dots , | z | \leq 1,
$$

and the coefficients of $B ^ { j }$ in $\phi ^ { - 1 } ( B )$ are the same as the coefficients of $z ^ { j }$ in $\phi ^ { - 1 } ( z )$ . In other words, we may treat the backshift operator, $B$ , as a complex number, $z$ . These results will be generalized in our discussion of ARMA models. We will find the polynomials corresponding to the operators useful in exploring the general properties of ARMA models.

# Introduction to Moving Average Models

As an alternative to the autoregressive representation in which the $x _ { t }$ on the left-hand side of the equation are assumed to be combined linearly, the moving average model of order $q$ , abbreviated as $\mathrm { M A } ( q )$ , assumes the white noise $w _ { t }$ on the right-hand side of the defining equation are combined linearly to form the observed data.

Definition 3.3 The moving average model of order q, or MA(q) model, is defined to be

$$
x _ {t} = w _ {t} + \theta_ {1} w _ {t - 1} + \theta_ {2} w _ {t - 2} + \dots + \theta_ {q} w _ {t - q} \tag {3.15}
$$

where there are $q$ lags in the moving average and $\theta _ { 1 } , \theta _ { 2 } , \ldots , \theta _ { q }$ ( ${ \theta } _ { q } \ne 0$ ) are parameters.2 The noise $w _ { t }$ is assumed to be Gaussian white noise.

The system is the same as the infinite moving average defined as the linear process (3.12), where $\psi _ { 0 } = 1$ , $\psi _ { j } = \theta _ { j }$ , for $j = 1 , \dotsc , q$ , and $\psi _ { j } = 0$ for other values. We may also write the $\mathrm { M A } ( q )$ process in the equivalent form

$$
x _ {t} = \theta (B) w _ {t}, \tag {3.16}
$$

using the following definition.

# 3.2: ARMA Models

Definition 3.4 The moving average operator is

$$
\theta (B) = 1 + \theta_ {1} B + \theta_ {2} B ^ {2} + \dots + \theta_ {q} B ^ {q} \tag {3.17}
$$

Unlike the autoregressive process, the moving average process is stationary for any values of the parameters $\theta _ { 1 } , \ldots , \theta _ { q }$ ; details of this result are provided in §3.4.

# Example 3.3 Autocorrelation and Sample Path of an MA(1) Process

Consider the MA(1) model $x _ { t } = w _ { t } + \theta w _ { t - 1 }$ . Then,

$$
\gamma (h) = \left\{ \begin{array}{l l} (1 + \theta^ {2}) \sigma_ {w} ^ {2}, & h = 0 \\ \theta \sigma_ {w} ^ {2}, & h = 1 \\ 0, & h > 1, \end{array} \right.
$$

and the autocorrelation function is

$$
\rho (h) = \left\{ \begin{array}{l l} \frac {\theta}{(1 + \theta^ {2})}, & h = 1 \\ 0, & h > 1. \end{array} \right.
$$

Note $| \rho ( 1 ) | \le 1 / 2$ for all values of $\theta$ (Problem 3.1). Also, $x _ { t }$ is correlated with $x _ { t - 1 }$ , but not with $x _ { t - 2 } , x _ { t - 3 } , . . .$ . Contrast this with the case of the AR(1) model in which the correlation between $x _ { t }$ and $x _ { t - k }$ is never zero. When $\theta = . 5$ , for example, $x _ { t }$ and $x _ { t - 1 }$ are positively correlated, and $\rho ( 1 ) = . 4$ . When $\theta = - . 5$ , $x _ { t }$ and $x _ { t - 1 }$ are negatively correlated, $\rho ( 1 ) = - . 4$ . Figure 3.2 shows a time plot of these two processes with $\sigma _ { w } ^ { 2 } = 1$ . The series in Figure 3.2 where $\theta = . 5$ is smoother than the series in Figure 3.2, where $\theta = - . 5$ .

A figure similar to Figure 3.2 can be created in R as follows:

> par(mfrow=c(2,1))  
> plot(arima.sim(list(order=c(0,0,1), ma=.5), n=100),   
+ ylab="x",main $\cdot ^ { = }$ (expression("MA(1) "*theta*" = +.5")))   
> plot(arima.sim(list(order=c(0,0,1), ma=-.5), n=100),   
+ ylab="x",main $\cdot ^ { = }$ (expression("MA(1) "*theta*" = -.5")))

# Example 3.4 Non-uniqueness of MA Models and Invertibility

Using Example 3.3, we note that for an MA(1) model, $\rho ( h )$ is the same for $\theta$ and $\frac { 1 } { \theta }$ ; try 5 and $\frac { 1 } { 5 }$ , for example. In addition, the pair $\sigma _ { w } ^ { 2 } = 1$ and $\theta = 5$ yield the same autocovariance function as the pair $\sigma _ { w } ^ { 2 } = 2 5$ and $\theta = 1 / 5$ , namely,

$$
\gamma (h) = \left\{ \begin{array}{l l} 2 6, & h = 0 \\ 5, & h = 1 \\ 0, & h > 1. \end{array} \right.
$$

![](images/59e5647cb32471250cb36d4c1b33f2ec85171b778c8c8ec7e656e5d7e8024c52.jpg)

![](images/5df68061c7beab90f820e651454181488c5368a99c4322deed459d4ed5a0a920.jpg)  
Figure 3.2 Simulated MA(1) models: $\theta = . 5$ (top); $\theta = - . 5$ (bottom).

Thus, the MA(1) processes

$$
x _ {t} = w _ {t} + \frac {1}{5} w _ {t - 1}, \quad w _ {t} \sim \mathrm {i i d} \quad \mathrm {N} (0, 2 5)
$$

and

$$
x _ {t} = v _ {t} + 5 v _ {t - 1}, \quad v _ {t} \sim \mathrm {i d} \mathrm {N} (0, 1)
$$

are the same because of normality (i.e., all finite distributions are the same). We can only observe the time series $x _ { t }$ and not the noise, $w _ { t }$ or $v _ { t }$ , so we cannot distinguish between the models. Hence, we will have to choose only one of them. For convenience, by mimicking the criterion of causality for AR models, we will choose the model with an infinite AR representation. Such a process is called an invertible process.

To discover which model is the invertible model, we can reverse the roles of $x _ { t }$ and $w _ { t }$ (because we are mimicking the AR case) and write the if MA(1) model as $| \theta | < 1$ , then $\begin{array} { r } { w _ { t } = \sum _ { j = 0 } ^ { \infty } ( - \theta ) ^ { j } x _ { t - j } } \end{array}$ $w _ { t } = - \theta w _ { t - 1 } + x _ { t }$ . Following the steps that led to (3.6), , which is the desired infinite AR representation of the model. Hence, given a choice, we will choose the model with $\sigma _ { w } ^ { 2 } = 2 5$ and $\theta = 1 / 5$ because it is invertible.

As in the AR case, the polynomial, $\theta ( z )$ , corresponding to the moving average operators, $\theta ( B )$ , will be useful in exploring general properties of MA processes. For example, following the steps of equations (3.11)–(3.14), we can write the MA(1) model as $x _ { t } = \theta ( B ) w _ { t }$ , where $\theta ( B ) = 1 + \theta B$ . If $| \theta | < 1$ , then we can write the model as $\pi ( B ) x _ { t } = w _ { t }$ , where $\pi ( B ) = \theta ^ { - 1 } ( B )$ . Let

# 3.2: ARMA Models

$\theta ( z ) = 1 + \theta z$ , for $| z | \le 1$ , then $\begin{array} { r } { \pi ( z ) = \theta ^ { - 1 } ( z ) = 1 / ( 1 + \theta z ) = \sum _ { j = 0 } ^ { \infty } ( - \theta ) ^ { j } z ^ { j } . } \end{array}$ and we determine that $\textstyle \pi ( B ) = \sum _ { j = 0 } ^ { \infty } ( - \theta ) ^ { j } B ^ { j }$ .

# Autoregressive Moving Average Models

We now proceed with the general development of autoregressive, moving average, and mixed autoregressive moving average (ARMA), models for stationary time series.

Definition 3.5 A time series $\{ x _ { t } ; ~ t = 0 , \pm 1 , \pm 2 , \ldots \}$ is ARMA(p, q) if it is stationary and

$$
x _ {t} = \phi_ {1} x _ {t - 1} + \dots + \phi_ {p} x _ {t - p} + w _ {t} + \theta_ {1} w _ {t - 1} + \dots + \theta_ {q} w _ {t - q}, \tag {3.18}
$$

with $\phi _ { p } \neq 0$ , ${ \theta } _ { q } \ne 0$ , and $\sigma _ { w } ^ { 2 } > 0$ . The parameters p and $q$ are called the autoregressive and the moving average orders, respectively. If $x _ { t }$ has a nonzero mean $\mu$ , we set $\alpha = \mu ( 1 - \phi _ { 1 } - \cdot \cdot \cdot - \phi _ { p } )$ and write the model as

$$
x _ {t} = \alpha + \phi_ {1} x _ {t - 1} + \dots + \phi_ {p} x _ {t - p} + w _ {t} + \theta_ {1} w _ {t - 1} + \dots + \theta_ {q} w _ {t - q}. \tag {3.19}
$$

Unless stated otherwise, $\{ w _ { t } ; \ t = 0 , \pm 1 , \pm 2 , . . . \}$ is a Gaussian white noise sequence.

As previously noted, when $q = 0$ , the model is called an autoregressive model of order $p$ , AR(p), and when $p = 0$ , the model is called a moving average model of order $q$ , $\mathrm { M A } ( q )$ . To aid in the investigation of ARMA models, it will be useful to write them using the AR operator, (3.5), and the MA operator, (3.17). In particular, the ARMA $( p , q )$ model in (3.18) can then be written in concise form as

$$
\phi (B) x _ {t} = \theta (B) w _ {t}. \tag {3.20}
$$

Before we discuss the conditions under which (3.18) is causal and invertible, we point out a potential problem with the ARMA model.

# Example 3.5 Parameter Redundancy

Consider a white noise process $x _ { t } = w _ { t }$ . Equivalently, we can write this as . $5 x _ { t - 1 } = . 5 w _ { t - 1 }$ by shifting back one unit of time and multiplying by .5. Now, subtract the two representations to obtain

$$
x _ {t} -. 5 x _ {t - 1} = w _ {t} -. 5 w _ {t - 1},
$$

or

$$
x _ {t} = . 5 x _ {t - 1} - . 5 w _ {t - 1} + w _ {t}, \tag {3.21}
$$

which looks like an ARMA $( 1 , 1 )$ model. Of course, $x _ { t }$ is still white noise; nothing has changed in this regard [i.e., $x _ { t } = w _ { t }$ is the solution to (3.21)],

but we have hidden the fact that $x _ { t }$ is white noise because of the parameter redundancy or over-parameterization. Write the parameter redundant model in operator form as $\phi ( B ) x _ { t } = \theta ( B ) w _ { t }$ , or

$$
(1 -. 5 B) x _ {t} = (1 -. 5 B) w _ {t}.
$$

Apply the operator $\phi ( B ) ^ { - 1 } = ( 1 - . 5 B ) ^ { - 1 }$ to both sides to obtain

$$
x _ {t} = (1 -. 5 B) ^ {- 1} (1 -. 5 B) x _ {t} = (1 -. 5 B) ^ {- 1} (1 -. 5 B) w _ {t} = w _ {t},
$$

which is the original model. We can easily detect the problem of overparameterization with the use of the operators or their associated polynomials. That is, write the AR polynomial $\phi ( z ) = ( 1 - . 5 z )$ , the MA polynomial $\theta ( z ) = ( 1 - . 5 z )$ , and note that both polynomials have a common factor, namely $( 1 - . 5 z )$ . This common factor immediately identifies the parameter redundancy. Discarding the common factor in each leaves $\phi ( z ) = 1$ and $\theta ( z ) = 1$ , from which we conclude $\phi ( B ) = 1$ and $\theta ( B ) = 1$ , and we deduce that the model is actually white noise. The consideration of parameter redundancy will be crucial when we discuss estimation for general ARMA models. As this example points out, we might fit an $\mathrm { A R M A } ( 1 , 1 )$ $( 1 , 1 )$ model to white noise data and find that the parameter estimates are significant. If we were unaware of parameter redundancy, we might claim the data are correlated when in fact they are not (Problem 3.19).

Examples 3.2, 3.4, and 3.5 point to a number of problems with the general definition of ARMA $( p , q )$ models, as given by (3.18), or, equivalently, by (3.20). To summarize, we have seen the following problems:

(i) parameter redundant models,   
(ii) stationary AR models that depend on the future, and   
(iii) MA models that are not unique.

To overcome these problems, we will require some additional restrictions on the model parameters. First, we make the following definitions.

Definition 3.6 The AR and MA polynomials are defined as

$$
\phi (z) = 1 - \phi_ {1} z - \dots - \phi_ {p} z ^ {p}, \quad \phi_ {p} \neq 0, \tag {3.22}
$$

and

$$
\theta (z) = 1 + \theta_ {1} z + \dots + \theta_ {q} z ^ {q}, \quad \theta_ {q} \neq 0, \tag {3.23}
$$

respectively, where $z$ is a complex number.

# 3.2: ARMA Models

To address the first problem, we will henceforth refer to an $\mathrm { A R M A } ( p , q )$ model to mean that it is in its simplest form. That is, in addition to the original definition given in equation (3.18), we will also require that $\phi ( z )$ and $\theta ( z )$ have no common factors. So, the process, $x _ { t } = . 5 x _ { t - 1 } - . 5 w _ { t - 1 } + w _ { t }$ , discussed in Example 3.5 is not referred to as an $\mathrm { A R M A } ( 1 , 1 )$ $( 1 , 1 )$ process because, in its reduced form, $x _ { t }$ is white noise.

To address the problem of future-dependent models, we formally introduce the concept of causality.

Definition 3.7 An ARMA(p, q) model, $\phi ( B ) x _ { t } = \theta ( B ) w _ { t }$ , is said to be causal, if the time series $\{ x _ { t } ; ~ t = 0 , \pm 1 , \pm 2 , \ldots \}$ can be written as a one-sided linear process:

$$
x _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j} = \psi (B) w _ {t}, \tag {3.24}
$$

where $\begin{array} { r } { \psi ( B ) = \sum _ { j = 0 } ^ { \infty } \psi _ { j } B ^ { j } } \end{array}$ , and $\textstyle \sum _ { j = 0 } ^ { \infty } | \psi _ { j } | < \infty$ ; we set $\psi _ { 0 } = 1$

In Example 3.2, the AR(1) process, $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ , is causal only when $| \phi | < 1$ . Equivalently, the process is causal only when the root of $\phi ( z ) = 1 - \phi z$ is bigger than one in absolute value. That is, the root, say, $z _ { 0 }$ , of $\phi ( z )$ is $z _ { 0 } = 1 / \phi$ (because $\phi ( z _ { 0 } ) = 0$ ) and $| z _ { 0 } | > 1$ because $| \phi | < 1$ . In general, we have the following property.

# Property P3.1: Causality of an ARMA $( p , q )$ Process

An $A R M A ( p , q )$ model is causal if and only if $\phi ( z ) \neq 0$ for $| z | \le 1$ . The coefficients of the linear process given in (3.24) can be determined by solving

$$
\psi (z) = \sum_ {j = 0} ^ {\infty} \psi_ {j} z ^ {j} = \frac {\theta (z)}{\phi (z)}, \quad | z | \leq 1.
$$

Another way to phrase Property P3.1 is that an ARMA process is causal only when the roots of $\phi ( z )$ lie outside the unit circle; that is, $\phi ( z ) = 0$ only when $| z | > 1$ . Finally, to address the problem of uniqueness discussed in Example 3.4, we choose the model that allows an infinite autoregressive representation.

Definition 3.8 An ARMA(p, q) model, $\phi ( B ) x _ { t } = \theta ( B ) w _ { t }$ , is said to be invertible, if the time series $\{ x _ { t } ; ~ t = 0 , \pm 1 , \pm 2 , \ldots \}$ can be written as

$$
\pi (B) x _ {t} = \sum_ {j = 0} ^ {\infty} \pi_ {j} x _ {t - j} = w _ {t}, \tag {3.25}
$$

where $\begin{array} { r } { \pi ( B ) = \sum _ { j = 0 } ^ { \infty } \pi _ { j } B ^ { j } } \end{array}$ , and $\textstyle \sum _ { j = 0 } ^ { \infty } | \pi _ { j } | < \infty$ ; we set $\pi _ { 0 } = 1$

Analogous to Property P3.1, we have the following property.

# Property P3.2: Invertibility of an ARMA(p, q) Process

An ARMA(p, q) model is invertible if and only if $\theta ( z ) \neq 0$ for $| z | \le 1$ . The coefficients $\boldsymbol { \mathscr { n } } _ { j }$ of $\pi ( B )$ given in (3.25) can be determined by solving

$$
\pi (z) = \sum_ {j = 0} ^ {\infty} \pi_ {j} z ^ {j} = \frac {\phi (z)}{\theta (z)}, \quad | z | \leq 1.
$$

Another way to phrase Property P3.2 is that an ARMA process is invertible only when the roots of $\theta ( z )$ lie outside the unit circle; that is, $\theta ( z ) = 0$ only when $| z | > 1$ . The proof of Property P3.1 is given in Appendix B (the proof of Property P3.2 is similar and, hence, is not provided). The following examples illustrate these concepts.

# Example 3.6 Parameter Redundancy, Causality, and Invertibility

Consider the process

$$
x _ {t} = . 4 x _ {t - 1} +. 4 5 x _ {t - 2} + w _ {t} + w _ {t - 1} +. 2 5 w _ {t - 2},
$$

or, in operator form,

$$
(1 -. 4 B -. 4 5 B ^ {2}) x _ {t} = (1 + B +. 2 5 B ^ {2}) w _ {t}.
$$

At first, $x _ { t }$ appears to be an ARMA $( 2 , 2 )$ process. But, the associated polynomials

$$
\phi (z) = 1 -. 4 z -. 4 5 z ^ {2} = (1 +. 5 z) (1 -. 9 z)
$$

$$
\theta (z) = (1 + z +. 2 5 z ^ {2}) = (1 +. 5 z) ^ {2}
$$

have a common factor that can be canceled. After cancellation, the polynomials become $\phi ( z ) = ( 1 - . 9 z )$ and $\theta ( z ) = ( 1 + . 5 z )$ , so the model is an ARMA(1, 1) model, $( 1 - . 9 B ) x _ { t } = ( 1 + . 5 B ) w _ { t }$ , or

$$
x _ {t} = . 9 x _ {t - 1} +. 5 w _ {t - 1} + w _ {t}. \tag {3.26}
$$

The model is causal because $\phi ( z ) = ( 1 - . 9 z ) = 0$ when $z = 1 0 / 9$ , which is outside the unit circle. The model is also invertible because the root of $\theta ( z ) = ( 1 + . 5 z )$ is $z = - 2$ , which is outside the unit circle.

To write the model as a linear process, we can obtain the $\psi$ -weights using Property P3.1:

$$
\begin{array}{l} \psi (z) = \frac {\theta (z)}{\phi (z)} = \frac {(1 + . 5 z)}{(1 - . 9 z)} \\ = (1 +. 5 z) \left(1 +. 9 z +. 9 ^ {2} z ^ {2} +. 9 ^ {3} z ^ {3} + \dots\right) | z | \leq 1. \\ \end{array}
$$

# 3.2: ARMA Models

The coefficient of $z ^ { j }$ in $\psi ( z )$ is $\psi _ { j } = ( . 5 + . 9 ) . 9 ^ { \jmath - 1 }$ , for $j \geq 1$ , so (3.26) can be written as

$$
x _ {t} = w _ {t} + 1. 4 \sum_ {j = 1} ^ {\infty}. 9 ^ {j - 1} w _ {t - j}.
$$

Similarly, to find the invertible representation using Property P3.2:

$$
\pi (z) = \frac {\phi (z)}{\theta (z)} = (1 -. 9 z) (1 -. 5 z +. 5 ^ {2} z ^ {2} -. 5 ^ {3} z ^ {3} + \dots) | z | \leq 1.
$$

In this case, the $\pi$ -weights are given by $\pi _ { j } = ( - 1 ) ^ { j } ( . 9 + . 5 ) . 5 ^ { j - 1 }$ , for $j \geq 1$ , and hence, we can also write (3.26) as

$$
x _ {t} = 1. 4 \sum_ {j = 1} ^ {\infty} (-. 5) ^ {j - 1} x _ {t - j} + w _ {t}.
$$

# Example 3.7 Causal Conditions for an AR(2) Process

For an AR(1) model, $( 1 - \phi B ) x _ { t } = w _ { t }$ , to be causal, the root of $\phi ( z ) =$ $1 - \phi z$ must lie outside of the unit circle. In this case, the root (or zero) occurs at $z _ { 0 } = 1 / \phi$ , i.e., $\phi ( z _ { 0 } ) = 0$ , so it is easy to go from the causal requirement on the root, that is, $| 1 / \phi | > 1$ , to a requirement on the parameter, that is, $| \phi | < 1$ . It is not so easy to establish this relationship for higher order models.

For example, the AR(2) model, $( 1 - \phi _ { 1 } B - \phi _ { 2 } B ^ { 2 } ) x _ { t } = w _ { t }$ , is causal when the two roots of $\phi ( z ) = 1 - \phi _ { 1 } z - \phi _ { 2 } z ^ { 2 }$ lie outside of the unit circle. Using the quadratic formula, this requirement can be written as

$$
\left| \frac {\phi_ {1} \pm \sqrt {\phi_ {1} ^ {2} + 4 \phi_ {2}}}{- 2 \phi_ {2}} \right| > 1.
$$

The roots of $\phi ( z )$ may be real and distinct, real and equal, or a complex conjugate pair. If we denote those roots by $z _ { 1 }$ and $z _ { 2 }$ , we can write $\phi ( z ) = ( 1 - z _ { 1 } ^ { - 1 } z ) ( 1 - z _ { 2 } ^ { - 1 } z )$ ; note that $\phi ( z _ { 1 } ) = \phi ( z _ { 2 } ) = 0 $ . The model can be written in operator form as $( 1 - z _ { 1 } ^ { - 1 } B ) ( 1 - z _ { 2 } ^ { - 1 } B ) x _ { t } = w _ { t }$ . From this representation, it follows that $\phi _ { 1 } = ( z _ { 1 } ^ { - 1 } + z _ { 2 } ^ { - 1 } )$ and $\phi _ { 2 } = - ( z _ { 1 } z _ { 2 } ) ^ { - 1 }$ . This relationship can be used to establish the following equivalent condition for causality:

$$
\phi_ {1} + \phi_ {2} <   1, \quad \phi_ {2} - \phi_ {1} <   1, \quad \text {a n d} \quad | \phi_ {2} | <   1. \tag {3.27}
$$

This causality condition specifies a triangular region in the parameter space. We leave the details of the equivalence to the reader (Problem 3.4).

# 3.3 Difference Equations

The study of the behavior of ARMA processes and their ACFs is greatly enhanced by a basic knowledge of difference equations, simply because they are difference equations. This topic is also useful in the study of time domain models and stochastic processes in general. We will give a brief and heuristic account of the topic along with some examples of the usefulness of the theory. For details, the reader is referred to Mickens (1987).

Suppose we have a sequence of numbers $u _ { 0 } , u _ { 1 } , u _ { 2 } , \ldots$ such that

$$
u _ {n} - \alpha u _ {n - 1} = 0, \quad \alpha \neq 0, \quad n = 1, 2, \dots . \tag {3.28}
$$

For example, recall (3.9) in which we showed that the ACF of an AR(1) process is a sequence, $\rho ( h )$ , satisfying

$$
\rho (h) - \phi \rho (h - 1) = 0, \quad h = 1, 2, \dots .
$$

Equation (3.28) represents a homogeneous difference equation of order 1. To solve the equation, we write:

$$
u _ {1} = \alpha u _ {0}
$$

$$
u _ {2} = \alpha u _ {1} = \alpha^ {2} u _ {0}
$$

$$
u _ {n} = \alpha u _ {n - 1} = \alpha^ {n} u _ {0}.
$$

Given an initial condition $u _ { 0 } = c$ , we may solve (3.28), namely, $u _ { n } = \alpha ^ { \pi } c$

In operator notation, (3.28) can be written as $( 1 - \alpha B ) u _ { n } = 0$ . The polynomial associated with (3.28) is $\alpha ( z ) = 1 - \alpha z$ , and the root, say, $z _ { 0 }$ , of this polynomial is $z _ { 0 } = 1 / \alpha$ ; that is $\alpha ( z _ { 0 } ) = 0$ . We know the solution to (3.28), with initial condition $u _ { 0 } = c$ , is

$$
u _ {n} = \alpha^ {n} c = \left(z _ {0} ^ {- 1}\right) ^ {n} c.
$$

That is, the solution to the difference equation (3.28) depends only on the initial condition and the inverse of the root to the associated polynomial $\alpha ( z )$ .

Now suppose that the sequence satisfies

$$
u _ {n} - \alpha_ {1} u _ {n - 1} - \alpha_ {2} u _ {n - 2} = 0, \quad \alpha_ {2} \neq 0, \quad n = 2, 3, \dots \tag {3.29}
$$

This equation is a homogeneous difference equation of order 2. The corresponding polynomial is

$$
\alpha (z) = 1 - \alpha_ {1} z - \alpha_ {2} z ^ {2},
$$

which has two roots, say, $z _ { 1 }$ and $z _ { 2 }$ ; that is, $\alpha ( z _ { 1 } ) = \alpha ( z _ { 2 } ) = 0$ . We will consider two cases. First suppose $z _ { 1 } \neq z _ { 2 }$ . Then the general solution to (3.29) is

$$
u _ {n} = c _ {1} z _ {1} ^ {- n} + c _ {2} z _ {2} ^ {- n}, \tag {3.30}
$$

# 3.3: Difference Equations

where $c _ { 1 }$ and $c _ { 2 }$ depend on the initial conditions. This claim can be verified by direct substitution of (3.30) into (3.29):

$$
\begin{array}{l} c _ {1} z _ {1} ^ {- n} + c _ {2} z _ {2} ^ {- n} - \alpha_ {1} \left(c _ {1} z _ {1} ^ {- (n - 1)} + c _ {2} z _ {2} ^ {- (n - 1)}\right) - \alpha_ {2} \left(c _ {1} z _ {1} ^ {- (n - 2)} + c _ {2} z _ {2} ^ {- (n - 2)}\right) \\ = c _ {1} z _ {1} ^ {- n} \left(1 - \alpha_ {1} z _ {1} - \alpha_ {2} z _ {1} ^ {2}\right) + c _ {2} z _ {2} ^ {- n} \left(1 - \alpha_ {1} z _ {2} - \alpha_ {2} z _ {2} ^ {2}\right) \\ = c _ {1} z _ {1} ^ {- n} \alpha (z _ {1}) + c _ {2} z _ {2} ^ {- n} \alpha (z _ {2}) \\ = 0. \\ \end{array}
$$

Given two initial conditions $u _ { 0 }$ and $u _ { 1 }$ , we may solve for $c _ { 1 }$ and $c _ { 2 }$

$$
\begin{array}{l} u _ {0} = c _ {1} + c _ {2} \\ u _ {1} = c _ {1} z _ {1} ^ {- 1} + c _ {2} z _ {2} ^ {- 1}, \\ \end{array}
$$

where $z _ { 1 }$ and $z _ { 2 }$ can be solved for in terms of $\alpha _ { 1 }$ and $\alpha _ { 2 }$ using the quadratic formula, for example.

When the roots are equal, $z _ { 1 } = z _ { 2 }$ ( $\mathbf { \mu } = \mathbf { \mu } _ { Z _ { 0 } }$ ), the general solution to (3.29) is

$$
u _ {n} = z _ {0} ^ {- n} \left(c _ {1} + c _ {2} n\right). \tag {3.31}
$$

This claim can also be verified by direct substitution of (3.31) into (3.29):

$$
\begin{array}{l} z _ {0} ^ {- n} \left(c _ {1} + c _ {2} n\right) - \alpha_ {1} \left(z _ {0} ^ {- (n - 1)} [ c _ {1} + c _ {2} (n - 1) ]\right) - \alpha_ {2} \left(z _ {0} ^ {- (n - 2)} [ c _ {1} + c _ {2} (n - 2) ]\right) \\ = z _ {0} ^ {- n} \left(c _ {1} + c _ {2} n\right) \left(1 - \alpha_ {1} z _ {0} - \alpha_ {2} z _ {0} ^ {2}\right) + c _ {2} z _ {0} ^ {- n + 1} \left(\alpha_ {1} + 2 \alpha_ {2} z _ {0}\right) \\ = c _ {2} z _ {0} ^ {- n + 1} \left(\alpha_ {1} + 2 \alpha_ {2} z _ {0}\right). \\ \end{array}
$$

To show that $( \alpha _ { 1 } + 2 \alpha _ { 2 } z _ { 0 } ) = 0$ , write $1 - \alpha _ { 1 } z - \alpha _ { 2 } z ^ { 2 } = ( 1 - z _ { 0 } ^ { - 1 } z ) ^ { 2 } $ , and take derivatives with respect to $z$ on both sides of the equation to obtain $( \alpha _ { 1 } + 2 \alpha _ { 2 } z ) = 2 z _ { 0 } ^ { - 1 } ( 1 - z _ { 0 } ^ { - 1 } z )$ . Thus, $( \alpha _ { 1 } + 2 \alpha _ { 2 } z _ { 0 } ) = 2 z _ { 0 } ^ { - 1 } ( 1 - z _ { 0 } ^ { - 1 } z _ { 0 } ) = 0$ , as was to be shown. Finally, given two initial conditions, $u _ { 0 }$ and $u _ { 1 }$ , we can solve for $c _ { 1 }$ and $c _ { 2 }$ :

$$
\begin{array}{l} u _ {0} = c _ {1} \\ u _ {1} = \left(c _ {1} + c _ {2}\right) z _ {0} ^ {- 1}. \\ \end{array}
$$

To summarize these results, in the case of distinct roots, the solution to the homogeneous difference equation of degree two was

$$
\begin{array}{l} u _ {n} = z _ {1} ^ {- n} \times (\text {a p o l y n o m i a l i n} n \text {o f d e g r e e} m _ {1} - 1) \\ + z _ {2} ^ {- n} \times (\text {a p o l y n o m i a l i n} n \text {o f d e g r e e} m _ {2} - 1), \\ \end{array}
$$

where $m _ { 1 }$ is the multiplicity of the root $z _ { 1 }$ and $m _ { 2 }$ is the multiplicity of the root $z _ { 2 }$ . In this example, of course, $m _ { 1 } = m _ { 2 } = 1$ , and we called the polynomials of degree zero $c _ { 1 }$ and $c _ { 2 }$ , respectively. In the case of the repeated root, the solution was

$$
u _ {n} = z _ {0} ^ {- n} \times (\text {a p o l y n o m i a l i n} n \text {o f d e g r e e} m _ {0} - 1),
$$

where $m _ { 0 }$ is the multiplicity of the root $z _ { 0 }$ ; that is, $m _ { 0 } = 2$ . In this case, we wrote the polynomial of degree one as $c _ { 1 } + c _ { 2 } n$ . In both cases, we solved for $c _ { 1 }$ and $c _ { 2 }$ given two initial conditions, $u _ { 0 }$ and $u _ { 1 }$ .

# Example 3.8 The ACF of an AR(2) Process

Suppose $x _ { t } = \phi _ { 1 } x _ { t - 1 } + \phi _ { 2 } x _ { t - 2 } + w _ { t }$ is a causal AR(2) process. Multiply each side of the model by $x _ { t - h }$ for $h > 0$ , and take expectation:

$$
E \left(x _ {t} x _ {t - h}\right) = \phi_ {1} E \left(x _ {t - 1} x _ {t - h}\right) + \phi_ {2} E \left(x _ {t - 2} x _ {t - h}\right) + E \left(w _ {t} x _ {t - h}\right).
$$

The result is

$$
\gamma (h) = \phi_ {1} \gamma (h - 1) + \phi_ {2} \gamma (h - 2), \quad h = 1, 2, \dots . \tag {3.32}
$$

In (3.32), we used the fact that $E ( x _ { t } ) = 0$ and for $h > 0$ ,

$$
E \left(w _ {t} x _ {t - h}\right) = E \left(w _ {t} \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - h - j}\right) = 0.
$$

Divide (3.32) through by $\gamma ( 0 )$ to obtain the difference equation for the ACF of the process:

$$
\rho (h) - \phi_ {1} \rho (h - 1) - \phi_ {2} \rho (h - 2) = 0, \quad h = 1, 2, \dots . \tag {3.33}
$$

The initial conditions are $\rho ( 0 ) = 1$ and $\rho ( - 1 ) = \phi _ { 1 } / ( 1 - \phi _ { 2 } )$ , which is obtained by evaluating (3.33) for $h = 1$ and noting that $\rho ( 1 ) = \rho ( - 1 )$ .

Using the results for the homogeneous difference equation of order two, let $z _ { 1 }$ and $z _ { 2 }$ be the roots of the associated polynomial, $\phi ( z ) = 1 - \phi _ { 1 } z -$ $\phi _ { 2 } z ^ { 2 }$ . Because the model is causal, we know the roots are outside the unit circle: $| z _ { 1 } | > 1$ and $| z _ { 2 } | > 1$ . Now, consider the solution for three cases:

(i) When $z _ { 1 }$ and $z _ { 2 }$ are real and distinct, then

$$
\rho (h) = c _ {1} z _ {1} ^ {- h} + c _ {2} z _ {2} ^ {- h},
$$

so $\rho ( h ) \to 0$ exponentially fast as $h  \infty$ .

(ii) When $z _ { 1 } = z _ { 2 } ( = z _ { 0 } )$ are real and equal, then

$$
\rho (h) = z _ {0} ^ {- h} \left(c _ {1} + c _ {2} h\right),
$$

so $\rho ( h ) \to 0$ exponentially fast as $h  \infty$ .

(iii) When $z _ { 1 } = \bar { z } _ { 2 }$ are a complex conjugate pair, then $c _ { 2 } = c _ { 1 }$ (because $\rho ( h )$ is real), and

$$
\rho (h) = c _ {1} z _ {1} ^ {- h} + \bar {c} _ {1} \bar {z} _ {1} ^ {- h}.
$$

Write $c _ { 1 }$ and $z _ { 1 }$ in polar coordinates, for example, $z _ { 1 } ~ = ~ | z _ { 1 } | e ^ { i \theta }$ , where $\theta$ is the angle whose tangent is the ratio of the imaginary

part and the real part of $z _ { 1 }$ (sometimes called $\mathrm { a r g } ( z _ { 1 } )$ ; the range of $\theta$ is $[ - \pi , \pi ]$ ). Then, using the fact that $e ^ { i \alpha } + e ^ { - i \alpha } = 2 \cos ( \alpha )$ , the solution has the form

$$
\rho (h) = a | z _ {1} | ^ {- h} \cos (h \theta + b),
$$

where $a$ and $b$ are determined by the initial conditions. Again, $\rho ( h )$ dampens to zero exponentially fast as $h  \infty$ , but it does so in a sinusoidal fashion. The implication of this result is shown in the next example.

# Example 3.9 The Sample Path of an AR(2) with Complex Roots

Figure 3.3 shows $n = 1 4 4$ observations from the AR(2) model

$$
x _ {t} = 1. 5 x _ {t - 1} -. 7 5 x _ {t - 2} + w _ {t},
$$

with $\sigma _ { w } ^ { 2 } = 1$ , and with complex roots chosen so the process exhibits pseudo-cyclic behavior at the rate of one cycle every 12 time points. The autoregressive polynomial for this model is $\phi ( z ) = 1 - 1 . 5 z + . 7 5 z ^ { 2 }$ . The roots of $\phi ( z )$ are $1 \pm i / \sqrt { 3 }$ , and $\theta = \tan ^ { - 1 } ( 1 / \sqrt { 3 } ) = 2 \pi / 1 2$ radians per unit time. To convert the angle to cycles per unit time, divide by $2 \pi$ to get $1 / 1 2$ cycles per unit time. The ACF for this model is shown in §3.4, Figure 3.4.

To reproduce Figure 3.3 in R:

$>$ set.seed(5)   
> ar2 = arima.sim(list(order $=$ c(2,0,0), ar =c(1.5,-.75)),   
+ $\ l { \textsc { n } } = \ 1 4 4 )$   
> plot.ts(ar2, axes=F); box(); axis(2)   
$>$ axis(1, seq(0,144,24))   
> abline(v=seq(0,144,12), lty="dotted")

To calculate and display the ACF for this model in R:

$>$ acf $=$ ARMAacf(ar=c(1.5,-.75), ma=0, 50)   
$>$ plot(acf, type="h", xlab="lag")   
> abline(h=0)

We now exhibit the solution for the general homogeneous difference equation of order $p$ :

$$
u _ {n} - \alpha_ {1} u _ {n - 1} - \dots - \alpha_ {p} u _ {n - p} = 0, \quad \alpha_ {p} \neq 0, \quad n = p, p + 1, \dots . \tag {3.34}
$$

The associated polynomial is

$$
\alpha (z) = 1 - \alpha_ {1} z - \dots - \alpha_ {p} z ^ {p}.
$$

![](images/024ba4754fab603c5eaaa6975a13f9ef623f880ba2d8e232986c6a2bb739f238.jpg)  
Figure 3.3 Simulated AR(2) model, $n = 1 4 4$ with $\phi _ { 1 } = 1 . 5$ and $\phi _ { 2 } = - . 7 5$

Suppose $\alpha ( z )$ has $r$ distinct roots, $z _ { 1 }$ with multiplicity $m _ { 1 }$ , $z _ { 2 }$ with multiplicity $m _ { 2 }$ , . . . , and $z _ { r }$ with multiplicity $m _ { r }$ , such that $m _ { 1 } + m _ { 2 } + \cdot \cdot \cdot + m _ { r } = p$ . The general solution to the difference equation (3.34) is

$$
u _ {n} = z _ {1} ^ {- n} P _ {1} (n) + z _ {2} ^ {- n} P _ {2} (n) + \dots + z _ {r} ^ {- n} P _ {r} (n), \tag {3.35}
$$

where $P _ { j } ( n )$ , for $j = 1 , 2 , \dots , r$ , is a polynomial in $n$ , of degree $m _ { j } - 1$ . Given $p$ initial conditions $u _ { 0 } , \ldots , u _ { p - 1 }$ , we can solve for the $P _ { j } ( n )$ explicitly.

# Example 3.10 Determining the $\psi$ -weights for a Causal ARMA $( p , q )$

For a causal ARMA $( p , q )$ model, $\phi ( B ) x _ { t } = \theta ( B ) w _ { t }$ , where the zeros of $\phi ( z )$ are outside the unit circle, recall that we may write

$$
x _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j},
$$

where the $\psi$ -weights are determined using Property P3.1.

For the pure $\mathrm { M A } ( q )$ model, $\psi _ { 0 } = 1$ , $\psi _ { j } = \theta _ { j }$ , for $j = 1 , \dots , q$ , and $\psi _ { j } = 0$ , otherwise. For the general case of ARMA $( p , q )$ models, the task of solving for the $\psi$ -weights is much more complicated, as was demonstrated in Example 3.6. The use of the theory of homogeneous difference equations can help here. To solve for the $\psi$ -weights in general, we must match the coefficients in $\psi ( z ) \phi ( z ) = \theta ( z )$ :

$$
(\psi_ {0} + \psi_ {1} z + \psi_ {2} z ^ {2} + \dots) (1 - \phi_ {1} z - \phi_ {2} z ^ {2} - \dots) = (1 + \theta_ {1} z + \theta_ {2} z ^ {2} + \dots).
$$

The first few values are

$$
\psi_ {0} = 1
$$

$$
\psi_ {1} - \phi_ {1} \psi_ {0} = \theta_ {1}
$$

$$
\psi_ {2} - \phi_ {1} \psi_ {1} - \phi_ {2} \psi_ {0} = \theta_ {2}
$$

$$
\psi_ {3} - \phi_ {1} \psi_ {2} - \phi_ {2} \psi_ {1} - \phi_ {3} \psi_ {0} = \theta_ {3}
$$

where we would take $\phi _ { j } = 0$ for $j > p$ , and $\theta _ { j } = 0$ for $j > q$ . The $\psi$ -weights satisfy the homogeneous difference equation given by

$$
\psi_ {j} - \sum_ {k = 1} ^ {p} \phi_ {k} \psi_ {j - k} = 0, \quad j \geq \max  (p, q + 1), \tag {3.36}
$$

with initial conditions

$$
\psi_ {j} - \sum_ {k = 1} ^ {j} \phi_ {k} \psi_ {j - k} = \theta_ {j}, \quad 0 \leq j \leq \max  (p, q + 1). \tag {3.37}
$$

The general solution depends on the roots of the AR polynomial $\phi ( z ) =$ $1 - \phi _ { 1 } z - \cdot \cdot \cdot - \phi _ { p } z ^ { p }$ , as seen from (3.36). The specific solution will, of course, depend on the initial conditions.

Consider the ARMA process given in (3.26), $x _ { t } = . 9 x _ { t - 1 } + . 5 w _ { t - 1 } + w _ { t }$ . Because $\operatorname* { m a x } ( p , q + 1 ) = 2$ , using (3.37), we have $\psi _ { 0 } = 1$ and $\psi _ { 1 } = . 9 \mathrm { + . 5 = }$ 1.4. By (3.36), for $j = 2 , 3 , \dots$ , the $\psi$ -weights satisfy $\psi _ { j } - . 9 \psi _ { j - 1 } = 0$ . The general solution is $\psi _ { j } ~ = ~ c . 9 ^ { j }$ . To find the specific solution, use the initial condition $\psi _ { 1 } ~ = ~ 1 . 4$ , so $1 . 4 ~ = ~ . 9 c$ or $c = 1 . 4 / . 9$ . Finally, $\psi _ { j } = 1 . 4 ( . 9 ) ^ { j - 1 }$ , for $j \geq 1$ , as we saw in Example 3.6.

To view, for example, the first 50 $\psi$ -weights in R, use:

> ARMAtoMA(ar=.9, ma=.5, 50) # for a list

> plot(ARMAtoMA(ar=.9, ma=.5, 50)) # for a graph

# 3.4 Autocorrelation and Partial Autocorrelation Functions

We begin by exhibiting the ACF of an MA(q) process, $x _ { t } = \theta ( B ) w _ { t }$ , where $\theta ( B ) = 1 + \theta _ { 1 } B + \cdot \cdot \cdot + \theta _ { q } B ^ { q }$ . Because $x _ { t }$ is a finite linear combination of white noise terms, the process is stationary with mean

$$
E (x _ {t}) = \sum_ {j = 0} ^ {q} \theta_ {j} E (w _ {t - j}) = 0,
$$

where we have written $\theta _ { 0 } = 1$ , and with autocovariance function

$$
\begin{array}{l} \gamma (h) = \operatorname {c o v} \left(x _ {t + h}, x _ {t}\right) = E \left[ \left(\sum_ {j = 0} ^ {q} \theta_ {j} w _ {t + h - j}\right) \left(\sum_ {k = 0} ^ {q} \theta_ {k} w _ {t - k}\right) \right] \\ = \left\{ \begin{array}{l l} \sigma_ {w} ^ {2} \sum_ {j = 0} ^ {q - h} \theta_ {j} \theta_ {j + h}, & 0 \leq h \leq q \\ 0, & h > q. \end{array} \right. \tag {3.38} \\ \end{array}
$$

Recall that $\gamma ( h ) = \gamma ( - h )$ , so we will only display the values for $h \geq 0$ . The cutting off of $\gamma ( h )$ after $q$ lags is the signature of the $\mathrm { M A } ( q )$ model. Dividing (3.38) by $\gamma ( 0 )$ yields the ACF of an $\mathrm { M A } ( q )$ :

$$
\rho (h) = \left\{ \begin{array}{l l} \frac {\sum_ {j = 0} ^ {q - h} \theta_ {j} \theta_ {j + h}}{1 + \theta_ {1} ^ {2} + \cdots + \theta_ {q} ^ {2}}, & 1 \leq h \leq q \\ 0, & h > q. \end{array} \right. \tag {3.39}
$$

For a causal ARMA $( p , q )$ model, $\phi ( B ) x _ { t } = \theta ( B ) w _ { t }$ , where the zeros of $\phi ( z )$ are outside the unit circle, write

$$
x _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j}.
$$

It follows immediately that $E ( x _ { t } ) = 0$ . Also, the autocovariance function of $x _ { t }$ can be written as:

$$
\gamma (h) = \operatorname {c o v} \left(x _ {t + h}, x _ {t}\right) = \sigma_ {w} ^ {2} \sum_ {j = 0} ^ {\infty} \psi_ {j} \psi_ {j + h}, \quad h \geq 0. \tag {3.40}
$$

We could then use (3.36) and (3.37) to solve for the $\psi$ -weights. In turn, we could solve for $\gamma ( h )$ , and the ACF $\rho ( h ) = \gamma ( h ) / \gamma ( 0 )$ . As in Example 3.8, it is also possible to obtain a homogeneous difference equation directly in terms of $\gamma ( h )$ . First, we write

$$
\begin{array}{l} \gamma (h) = \operatorname {c o v} (x _ {t + h}, x _ {t}) = E \left[ \left(\sum_ {j = 1} ^ {p} \phi_ {j} x _ {t + h - j} + \sum_ {j = 0} ^ {q} \theta_ {j} w _ {t + h - j}\right) x _ {t} \right] \\ = \sum_ {j = 1} ^ {p} \phi_ {j} \gamma (h - j) + \sigma_ {w} ^ {2} \sum_ {j = h} ^ {q} \theta_ {j} \psi_ {j - h}, \quad h \geq 0, \tag {3.41} \\ \end{array}
$$

where we have used the fact that $\begin{array} { r } { x _ { t } = \sum _ { k = 0 } ^ { \infty } \psi _ { k } w _ { t - k } } \end{array}$ and for $h \geq 0$ ,

$$
E (w _ {t + h - j} x _ {t}) = E \left[ w _ {t + h - j} \left(\sum_ {k = 0} ^ {\infty} \psi_ {k} w _ {t - k}\right) \right] = \psi_ {j - h} \sigma_ {w} ^ {2}.
$$

# 3.4: The ACF and PACF

From (3.41), we can write a general homogeneous equation for the ACF of a causal ARMA process:

$$
\gamma (h) - \phi_ {1} \gamma (h - 1) - \dots - \phi_ {p} \gamma (h - p) = 0, \quad h \geq \max  (p, q + 1), \tag {3.42}
$$

with initial conditions

$$
\gamma (h) - \sum_ {j = 1} ^ {p} \phi_ {j} \gamma (h - j) = \sigma_ {w} ^ {2} \sum_ {j = h} ^ {q} \theta_ {j} \psi_ {j - h}, \quad 0 \leq h <   \max  (p, q + 1). \tag {3.43}
$$

Dividing (3.42) and (3.43) through by $\gamma ( 0 )$ will allow us to solve for the ACF, $\rho ( h ) = \gamma ( h ) / \gamma ( 0 )$ .

# Example 3.11 The ACF of an ARMA $( 1 , 1 )$

Consider the causal ARMA $( 1 , 1 )$ process $x _ { t } = \phi x _ { t - 1 } + \theta w _ { t - 1 } + w _ { t }$ , where $| \phi | < 1$ . Based on (3.42), the autocovariance function satisfies

$$
\gamma (h) - \phi \gamma (h - 1) = 0, \quad h = 2, 3, \ldots ,
$$

so the general solution is $\gamma ( h ) = c \phi ^ { h }$ , for $h = 1 , 2 , \ldots$ . To obtain the initial conditions, we use (3.43):

$$
\begin{array}{l} \gamma (0) = \phi \gamma (1) + \sigma_ {w} ^ {2} [ 1 + \theta \phi + \theta^ {2} ] \\ \gamma (1) = \phi \gamma (0) + \sigma_ {w} ^ {2} \theta . \\ \end{array}
$$

Solving for $\gamma ( 0 )$ and $\gamma ( 1 )$ , we obtain:

$$
\begin{array}{l} \gamma (0) = \sigma_ {w} ^ {2} \frac {1 + 2 \theta \phi + \theta^ {2}}{1 - \phi^ {2}} \\ \gamma (1) = \sigma_ {w} ^ {2} \frac {(1 + \theta \phi) (\phi + \theta)}{1 - \phi^ {2}}. \\ \end{array}
$$

To solve for $c$ , note that $\gamma ( 1 ) = c \phi$ , in which case $c = \gamma ( 1 ) / \phi$ . Hence, the specific solution is

$$
\gamma (h) = \sigma_ {w} ^ {2} \frac {(1 + \theta \phi) (\phi + \theta)}{1 - \phi^ {2}} \phi^ {h - 1}.
$$

Finally, dividing through by $\gamma ( 0 )$ yields the ACF

$$
\rho (h) = \frac {(1 + \theta \phi) (\phi + \theta)}{1 + 2 \theta \phi + \theta^ {2}} \phi^ {h - 1}, \quad h \geq 1. \tag {3.44}
$$

# Example 3.12 The ACF of an $\mathbf { A R } ( { \pmb p } )$

For a causal AR(p), it follows immediately from (3.42) that

$$
\rho (h) - \phi_ {1} \rho (h - 1) - \dots - \phi_ {p} \rho (h - p) = 0, \quad h \geq p. \tag {3.45}
$$

Let $z _ { 1 } , \ldots , z _ { r }$ denote the roots of $\phi ( z )$ , each with multiplicity $m _ { 1 } , \ldots , m _ { r }$ , respectively, where $m _ { 1 } + \cdots + m _ { r } = p$ . Then, from (3.35), the general solution is

$$
\rho (h) = z _ {1} ^ {- h} P _ {1} (h) + z _ {2} ^ {- h} P _ {2} (h) + \dots + z _ {r} ^ {- h} P _ {r} (h), \quad h \geq p, \tag {3.46}
$$

where $P _ { j } ( h )$ is a polynomial in $h$ of degree $m _ { j } - 1$ .

Recall that for a causal model, all of the roots are outside the unit circle, $| z _ { i } | > 1$ , for $i = 1 , \ldots , r$ . If all the roots are real, then $\rho ( h )$ dampens exponentially fast to zero as $h  \infty$ . If some of the roots are complex, then they will be in conjugate pairs and $\rho ( h )$ will dampen, in a sinusoidal fashion, exponentially fast to zero as $h  \infty$ . In the case of complex roots, the time series will appear to be cyclic in nature. This, of course, is also true for ARMA models in which the AR part has complex roots.

# The Partial Autocorrelation Function (PACF)

We have seen in (3.39), for MA(q) models, the ACF will be zero for lags greater than $q$ . Moreover, because ${ \theta } _ { q } \ne 0$ , the ACF will not be zero at lag $q$ . Thus, the ACF provides a considerable amount of information about the order of the dependence when the process is a moving average process. If the process, however, is ARMA or AR, the ACF alone tells us little about the orders of dependence. Hence, it is worthwhile pursuing a function that will behave like the ACF of MA models, but for AR models, namely, the partial autocorrelation function (PACF).

To motivate the idea, consider a causal AR(1) model, $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ . Then,

$$
\begin{array}{l} \gamma (2) = \operatorname {c o v} \left(x _ {t}, x _ {t - 2}\right) = \operatorname {c o v} \left(\phi x _ {t - 1} + w _ {t}, x _ {t - 2}\right) \\ = \operatorname {c o v} \left(\phi^ {2} x _ {t - 2} + \phi w _ {t - 1} + w _ {t}, x _ {t - 2}\right) = \phi^ {2} \gamma (0). \\ \end{array}
$$

This result follows from causality because $x _ { t - 2 }$ involves $\{ w _ { t - 2 } , w _ { t - 3 } , . . . \}$ , which are all uncorrelated with $w _ { t }$ and $w _ { t - 1 }$ . The correlation between $x _ { t }$ and $x _ { t - 2 }$ is not zero, as it would be for an MA(1), because $x _ { t }$ is dependent on $x _ { t - 2 }$ through $x _ { t - 1 }$ . Suppose we break this chain of dependence by removing (or partialing out) $x _ { t - 1 }$ . That is, we consider the correlation between ${ x _ { t } - \phi x _ { t - 1 } }$ and $x _ { t - 2 } - \phi x _ { t - 1 }$ , because it is the correlation between $x _ { t }$ and $x _ { t - 2 }$ with the linear dependence of each on $x _ { t - 1 }$ removed. In this way, we have broken the dependence chain between $x _ { t }$ and $x _ { t - 2 }$ . In fact,

$$
\operatorname {c o v} \left(x _ {t} - \phi x _ {t - 1}, x _ {t - 2} - \phi x _ {t - 1}\right) = \operatorname {c o v} \left(w _ {t}, x _ {t - 2} - \phi x _ {t - 1}\right) = 0.
$$

To formally define the PACF for mean-zero stationary time series, let xh−h $x _ { h } ^ { h - 1 }$ h-1 denote the regression of $x _ { h }$ on $\left\{ x _ { h - 1 } , x _ { h - 2 } , \ldots , x _ { 1 } \right\}$ , which we write as3

$$
x _ {h} ^ {h - 1} = \beta_ {1} x _ {h - 1} + \beta_ {2} x _ {h - 2} + \dots + \beta_ {h - 1} x _ {1}. \tag {3.47}
$$

addition, let No intercept term is needed in (3.47) because the mean of $x _ { 0 } ^ { h - 1 }$ denote the regression of $x _ { 0 }$ on $\{ x _ { 1 } , x _ { 2 } , \dotsc , x _ { h - 1 } \}$ $x _ { t }$ is zero. In , then

$$
x _ {0} ^ {h - 1} = \beta_ {1} x _ {1} + \beta_ {2} x _ {2} + \dots + \beta_ {h - 1} x _ {h - 1}. \tag {3.48}
$$

The coefficients, $\beta _ { 1 } , \ldots , \beta _ { h - 1 }$ are the same in (3.47) and (3.48); we will explain this result in the next section.

Definition 3.9 The partial autocorrelation function (PACF) of a stationary process, $x _ { t }$ , denoted $\phi _ { h h }$ , for $h = 1 , 2 , \ldots$ , is

$$
\phi_ {1 1} = \operatorname {c o r r} \left(x _ {1}, x _ {0}\right) = \rho (1) \tag {3.49}
$$

and

$$
\phi_ {h h} = \operatorname {c o r r} \left(x _ {h} - x _ {h} ^ {h - 1}, x _ {0} - x _ {0} ^ {h - 1}\right), \quad h \geq 2. \tag {3.50}
$$

Both $( x _ { h } - x _ { h } ^ { h - 1 } )$ and $( x _ { 0 } - x _ { 0 } ^ { h - 1 } )$ are uncorrelated with $\{ x _ { 1 } , x _ { 2 } , \dotsc , x _ { h - 1 } \}$ . By stationarity, the PACF, $\phi _ { h h }$ , is the correlation between $x _ { t }$ and $x _ { t - h }$ with the linear dependence of $\{ x _ { t - 1 } , . . . , x _ { t - ( h - 1 ) } \}$ , on each, removed. If the process $x _ { t }$ is Gaussian, then $\phi _ { h h } = \mathrm { c o r r } ( x _ { t } , x _ { t - h } \vert \ x _ { t - 1 } , \dots , x _ { t - ( h - 1 ) } )$ . That is, $\phi _ { h h }$ i s the correlation coefficient between $x _ { t }$ and $x _ { t - h }$ in the bivariate distribution of $\left( \boldsymbol { x } _ { t } , \boldsymbol { x } _ { t - h } \right)$ conditional on $\{ x _ { t - 1 } , \ldots , x _ { t - ( h - 1 ) } \}$ .

# Example 3.13 The PACF of a Causal AR(1)

Consider the PACF of the AR(1) process given by $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ , with $| \phi | < 1$ . By definition, $\phi _ { 1 1 } = \rho ( 1 ) = \phi$ . To calculate $\phi _ { 2 2 }$ , consider the regression of $x _ { 2 }$ on $x _ { 1 }$ , say, $x _ { 2 } ^ { 1 } = \beta x _ { 1 }$ . We choose $\beta$ to minimize

$$
E \left(x _ {2} - \beta x _ {1}\right) ^ {2} = \gamma (0) - 2 \beta \gamma (1) + \beta^ {2} \gamma (0).
$$

Taking derivatives and setting the result equal to zero, we have $\beta =$ $\gamma ( 1 ) / \gamma ( 0 ) = \rho ( 1 ) = \phi$ . Thus, $x _ { 2 } ^ { 1 } = \phi x _ { 1 }$ . Next, consider the regression of $x _ { 0 }$ on $x _ { 1 }$ , say $x _ { 0 } ^ { 1 } = \beta x _ { 1 }$ . We choose $\beta$ to minimize

$$
E (x _ {0} - \beta x _ {1}) ^ {2} = \gamma (0) - 2 \beta \gamma (1) + \beta^ {2} \gamma (0).
$$

This is the same equation as before, so $\beta = \phi$ and $x _ { 0 } ^ { 1 } = \phi x _ { 1 }$ . Hence, $\phi _ { 2 2 } = \mathrm { c o r r } ( x _ { 2 } - \phi x _ { 1 } , x _ { 0 } - \phi x _ { 1 } )$ . But, note

$$
\operatorname {c o v} \left(x _ {2} - \phi x _ {1}, x _ {0} - \phi x _ {1}\right) = \gamma (2) - 2 \phi \gamma (1) + \phi^ {2} \gamma (0) = 0
$$

because $\gamma ( h ) = \gamma ( 0 ) \phi ^ { h }$ . Thus, $\phi _ { 2 2 } = 0$ . In the next example, we will see that in this case $\phi _ { h h } = 0$ , for all $h > 1$ .

# Example 3.14 The PACF of a Causal AR(p)

Let $\begin{array} { r } { x _ { t } = \sum _ { j = 1 } ^ { p } \phi _ { j } x _ { t - j } + w _ { t } } \end{array}$ , where the roots of $\phi ( z )$ are outside the unit circle. In particular, $\begin{array} { r } { x _ { h } = \sum _ { j = 1 } ^ { p } \phi _ { j } x _ { h - j } + w _ { h } } \end{array}$ . When $h > p$ , the regression of $x _ { h }$ on $x _ { h - 1 } , \ldots , x _ { 1 }$ , is

$$
x _ {h} ^ {h - 1} = \sum_ {j = 1} ^ {p} \phi_ {j} x _ {h - j}.
$$

We have not proved this obvious result yet, but we will prove it in the next section. Thus, when $h > p$ ,

$$
\begin{array}{l} \phi_ {h h} = \operatorname {c o r r} \left(x _ {h} - x _ {h} ^ {h - 1}, x _ {0} - x _ {0} ^ {h - 1}\right) \\ = \operatorname {c o r r} \left(w _ {h}, x _ {0} - x _ {0} ^ {h - 1}\right) = 0, \\ \end{array}
$$

because, by causality, $x _ { 0 } - x _ { 0 } ^ { h - 1 }$ depends only on $\{ w _ { h - 1 } , w _ { h - 2 } , . . . \}$ ; recall equation (3.48). When $h \leq p$ , $\phi _ { p p }$ is not zero, and $\phi _ { 1 1 } , \ldots , \phi _ { p - 1 , p - 1 }$ are not necessarily zero. Figure 3.4 shows the ACF and the PACF of the AR(2) model presented in Example 3.9.

To reproduce Figure 3.4 in R, use the following commands:

$>$ acf $=$ ARMAacf(ar=c(1.5,-.75), ma=0, 24)   
$>$ pacf $=$ ARMAacf(ar=c(1.5,-.75), ma=0, 24, pacf=T)   
> par(mfrow=c(1,2))  
> plot(acf, type="h", xlab="lag")   
$>$ abline $\scriptstyle \mathtt { h } = 0$   
> plot(pacf, type="h", xlab="lag")   
$>$ abline $\scriptstyle \mathtt { h } = 0$

# Example 3.15 The PACF of an Invertible MA(q)

For an invertible $\operatorname { M A } ( q )$ , we can write $\begin{array} { r } { x _ { t } = - \sum _ { j = 1 } ^ { \infty } \pi { _ j } x _ { t - j } + w _ { t } } \end{array}$ . Moreover, no finite representation exists. From this result, it should be apparent that the PACF will never cut off, as in the case of an $\operatorname { A R } ( p )$ .

For an MA(1), $x _ { t } = w _ { t } + \theta w _ { t - 1 }$ , with $| \theta | < 1$ , calculations similar to Example 3.13 will yield $\phi _ { 2 2 } = - \theta ^ { 2 } / ( 1 + \theta ^ { 2 } + \theta ^ { 4 } )$ . For the MA(1) in general, we can show that

$$
\phi_ {h h} = - \frac {(- \theta) ^ {h} (1 - \theta^ {2})}{1 - \theta^ {2 (h + 1)}}, \quad h \geq 1.
$$

In the next section, we will discuss methods of calculating the PACF. The PACF for MA models behaves much like the ACF for AR models. Also, the PACF for AR models behaves much like the ACF for MA models. Because an invertible ARMA model has an infinite AR representation, the PACF will not cut off. We may summarize these results in Table 3.1.

![](images/f5d2b3a4c4e279db90b4d63185076a4622ff7518e7b38b4397377d5cfbd5836a.jpg)

![](images/31618d57e8324091818e77fabbe8b0809c6129bf2ec6bf2375c827ae09e895b4.jpg)  
Figure 3.4 The ACF and PACF, to lag 24, of an AR(2) model with $\phi _ { 1 } = 1 . 5$ and $\phi _ { 2 } = - . 7 5$ .

Table 3.1 Behavior of the ACF and PACF for Causal and Invertible ARMA Models   

<table><tr><td></td><td>AR(p)</td><td>MA(q)</td><td>ARMA(p,q)</td></tr><tr><td>ACF</td><td>Tails off</td><td>Cuts off after lag q</td><td>Tails off</td></tr><tr><td>PACF</td><td>Cuts off after lag p</td><td>Tails off</td><td>Tails off</td></tr></table>

# Example 3.16 Preliminary Analysis of the Recruitment Series

We consider the problem of modeling the Recruitment series (number of new fish) shown in Figure 1.5. There are 453 months of observed recruitment ranging over the years 1950-1987. The ACF and the PACF given in Figure 3.5 are consistent with the behavior of an AR(2). The ACF has cycles corresponding roughly to a 12-month period, and the PACF has large values for $h = 1 , 2$ and then is essentially zero for higher order lags. Based on Table 3.1, these results suggest that a second-order ( $p = 2$ ) autoregressive model might provide a good fit. Although we will discuss estimation in detail in §3.6, we ran a regression (see §2.2) using the data triplets $\{ ( y ; z _ { 1 } , z _ { 2 } ) : ( x _ { 3 } ; x _ { 2 } , x _ { 1 } ) , ( x _ { 4 } ; x _ { 3 } , x _ { 2 } ) , \ldots , ( x _ { 4 5 3 } ; x _ { 4 5 2 } , x _ { 4 5 1 } ) \}$ to fit a model of the form

$$
x _ {t} = \phi_ {0} + \phi_ {1} x _ {t - 1} + \phi_ {2} x _ {t - 2} + w _ {t}
$$

![](images/c6ae9f59b8c9db41af967ffbee68e3940641df2233b441ba4232dd30b0c5dda3.jpg)

![](images/c7d790c1d1b797830c4d17c4018da91397536efa3074dce88c8a1f41539bf9d1.jpg)  
Figure 3.5 ACF and PACF of the Recruitment series.

for $t = 3 , 4 , \dots , 4 5 3$ . The values of the estimates were $\widehat { \phi } _ { 0 } = 6 . 7 4 ( 1 . 1 1 )$ , $\widehat { \phi } _ { 1 } = 1 . 3 5 ( . 0 4 ) , \widehat { \phi } _ { 2 } = - . 4 6 ( . 0 4 )$ , and $\widehat { \sigma } _ { w } ^ { 2 } = 9 0 . 3 1 $ , where the estimated standard errors are in parentheses.

To reproduce this analysis and the ACF and PACF in Figure 3.5 in R:

```txt
> rec = scan("/mydata/recruit.dat")
> par(mfrow=c(2,1))
> acf(rec, 48)
> pacf(rec, 48)
> fit=ar.ols(rec,aic=F,order.max=2,demean=F,intercept=T)
> fit # estimates
> fit$asy.se # standard errors 
```

# 3.5 Forecasting

In forecasting, the goal is to predict future values of a time series, $x _ { n + m }$ , $m =$ $1 , 2 , \ldots$ , based on the data collected to the present, $\pmb { x } = \{ x _ { n } , x _ { n - 1 } , \ldots , x _ { 1 } \}$ . Throughout this section, we will assume $x _ { t }$ is stationary and the model parameters are known. The problem of forecasting when the model parameters are unknown will be discussed in the next section; also, see Problem 3.25. The minimum mean square error predictor of $x _ { n + m }$ is

$$
x _ {n + m} ^ {n} = E \left(x _ {n + m} \mid x _ {n}, x _ {n - 1}, \dots , x _ {1}\right)
$$

# 3.5: Forecasting

because the conditional expectation minimizes the mean square error

$$
E \left[ x _ {n + m} - g (\boldsymbol {x}) \right] ^ {2}, \tag {3.51}
$$

where $g ( { \pmb x } )$ is a function of the observations $\pmb { x }$ ; see Problem 3.13.

First, we will restrict attention to predictors that are linear functions of the data, that is, predictors of the form

$$
x _ {n + m} ^ {n} = \alpha_ {0} + \sum_ {k = 1} ^ {n} \alpha_ {k} x _ {k}, \tag {3.52}
$$

where $\alpha _ { 0 } , \alpha _ { 1 } , \ldots , \alpha _ { n }$ are real numbers. Linear predictors of the form (3.52) that minimize the mean square prediction error (3.51) are called best linear predictors (BLPs). As we shall see, linear prediction depends only on the second-order moments of the process, which are easy to estimate from the data. Much of the material in this section is enhanced by the theoretical material presented in Appendix B. For example, Theorem B.3 states that if the process is Gaussian, minimum mean square error predictors and best linear predictors are the same. The following property, which is based on the projection theorem, Theorem B.1 of Appendix B, is a key result.

# Property P3.3: Best Linear Prediction for Stationary Processes

Given data $x _ { 1 } , \ldots , x _ { n }$ , the best linear predictor, $\begin{array} { r } { x _ { n + m } ^ { n } = \alpha _ { 0 } + \sum _ { k = 1 } ^ { n } \alpha _ { k } x _ { k } } \end{array}$ , of $x _ { n + m }$ , for $m \geq 1$ , is found by solving

$$
E \left[ \left(x _ {n + m} - x _ {n + m} ^ {n}\right) x _ {k} \right] = 0, \quad k = 0, 1, \dots , n, \tag {3.53}
$$

where $x _ { 0 } = 1$ .

The equations specified in (3.53) are called the prediction equations, and they are used to solve for the coefficients $\{ \alpha _ { 0 } , \alpha _ { 1 } , \ldots , \alpha _ { n } \}$ . If $E ( x _ { t } ) = \mu$ , the first equation ( $k = 0$ ) of (3.53) implies

$$
E \left(x _ {n + m} ^ {n}\right) = E \left(x _ {n + m}\right) = \mu .
$$

Thus, taking expectation in (3.52), we have

$$
\mu = \alpha_ {0} + \sum_ {k = 1} ^ {n} \alpha_ {k} \mu \qquad \text {o r} \qquad \alpha_ {0} = \mu \left(1 - \sum_ {k = 1} ^ {n} \alpha_ {k}\right).
$$

Hence, the form of the BLP is

$$
x _ {n + m} ^ {n} = \mu + \sum_ {k = 1} ^ {n} \alpha_ {k} (x _ {k} - \mu).
$$

Thus, until we discuss estimation, there is no loss of generality in considering the case that $\mu = 0$ , in which case, $\alpha _ { 0 } = 0$ .

Consider, first, one-step-ahead prediction. That is, given $\{ x _ { 1 } , \ldots , x _ { n } \}$ , we wish to forecast the value of the time series at the next time point, $x _ { n + 1 }$ . The BLP of $x _ { n + 1 }$ i s

$$
x _ {n + 1} ^ {n} = \phi_ {n 1} x _ {n} + \phi_ {n 2} x _ {n - 1} + \dots + \phi_ {n n} x _ {1}, \tag {3.54}
$$

where, for purposes that will become clear shortly, we have written $\alpha _ { k }$ in (3.52), as $\phi _ { n , n + 1 - k }$ in (3.54), for $k = 1 , \ldots , n$ . Using Property P3.3, the coefficients $\{ \phi _ { n 1 } , \phi _ { n 2 } , . . . , \phi _ { n n } \}$ satisfy

$$
E \left[ \left(x _ {n + 1} - \sum_ {j = 1} ^ {n} \phi_ {n j} x _ {n + 1 - j}\right) x _ {n + 1 - k} \right] = 0, \quad k = 1, \dots , n,
$$

or

$$
\sum_ {j = 1} ^ {n} \phi_ {n j} \gamma (k - j) = \gamma (k), \quad k = 1, \dots , n. \tag {3.55}
$$

The prediction equations (3.55) can be written in matrix notation as

$$
\Gamma_ {n} \phi_ {n} = \gamma_ {n}, \tag {3.56}
$$

where $\Gamma _ { n } = \{ \gamma ( k - j ) \} _ { j , k = 1 } ^ { n }$ is an $n \times n$ matrix, $\pmb { \phi } _ { n } = ( \phi _ { n 1 } , \ldots , \phi _ { n n } ) ^ { \prime }$ is an $n \times 1$ vector, and $\gamma _ { n } = ( \gamma ( 1 ) , \ldots , \gamma ( n ) ) ^ { \prime }$ is an $n \times 1$ vector.

The matrix $\Gamma _ { n }$ is nonnegative definite. If $\Gamma _ { n }$ is singular, there are many solutions to (3.56), but, by the projection theorem (Theorem B.1), $x _ { n + 1 } ^ { \pi }$ is unique. If $\Gamma _ { n }$ is nonsingular, the elements of $\phi _ { n }$ are unique, and are given by

$$
\boldsymbol {\phi} _ {n} = \Gamma_ {n} ^ {- 1} \boldsymbol {\gamma} _ {n}. \tag {3.57}
$$

For ARMA models, the fact that $\sigma _ { w } ^ { 2 } > 0$ and $\gamma ( h ) \to 0$ as $h \to \infty$ is enough to ensure that $\Gamma _ { n }$ is positive definite (Problem 3.11). It is sometimes convenient to write the one-step-ahead forecast in vector notation

$$
x _ {n + 1} ^ {n} = \boldsymbol {\phi} _ {n} ^ {\prime} \boldsymbol {x}, \tag {3.58}
$$

where $\pmb { x } = ( x _ { n } , x _ { n - 1 } , \ldots , x _ { 1 } ) ^ { \prime }$

The mean square one-step-ahead prediction error is

$$
P _ {n + 1} ^ {n} = E \left(x _ {n + 1} - x _ {n + 1} ^ {n}\right) ^ {2} = \gamma (0) - \boldsymbol {\gamma} _ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \boldsymbol {\gamma} _ {n}. \tag {3.59}
$$

To verify (3.59) using (3.57) and (3.58),

$$
\begin{array}{l} E \left(x _ {n + 1} - x _ {n + 1} ^ {n}\right) ^ {2} = E \left(x _ {n + 1} - \boldsymbol {\phi} _ {n} ^ {\prime} \boldsymbol {x}\right) ^ {2} = E \left(x _ {n + 1} - \boldsymbol {\gamma} _ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \boldsymbol {x}\right) ^ {2} \\ = E \left(x _ {n + 1} ^ {2} - 2 \gamma_ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \boldsymbol {x} x _ {n + 1} + \gamma_ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \boldsymbol {x} \boldsymbol {x} ^ {\prime} \Gamma_ {n} ^ {- 1} \boldsymbol {\gamma} _ {n}\right) \\ = \gamma (0) - 2 \boldsymbol {\gamma} _ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \boldsymbol {\gamma} _ {n} + \boldsymbol {\gamma} _ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \Gamma_ {n} \Gamma_ {n} ^ {- 1} \boldsymbol {\gamma} _ {n} \\ = \gamma (0) - \gamma_ {n} ^ {\prime} \Gamma_ {n} ^ {- 1} \gamma_ {n}. \\ \end{array}
$$

# 3.5: Forecasting

# Example 3.17 Prediction for an AR(2)

Suppose we have a causal AR(2) process $x _ { t } = \phi _ { 1 } x _ { t - 1 } + \phi _ { 2 } x _ { t - 2 } + w _ { t }$ , and one observation $x _ { 1 }$ . Then, using equation (3.57), the one-step-ahead prediction of $x _ { 2 }$ based on $x _ { 1 }$ is

$$
x _ {2} ^ {1} = \phi_ {1 1} x _ {1} = \frac {\gamma (1)}{\gamma (0)} x _ {1} = \rho (1) x _ {1}.
$$

Now, suppose we want the one-step-ahead prediction of $x _ { 3 }$ based on two observations $x _ { 1 }$ and $x _ { 2 }$ . We could use (3.57) again and solve

$$
x _ {3} ^ {2} = \phi_ {2 1} x _ {2} + \phi_ {2 2} x _ {1} = (\gamma (1), \gamma (2)) \left( \begin{array}{c c} \gamma (0) & \gamma (1) \\ \gamma (1) & \gamma (0) \end{array} \right) ^ {- 1} \left( \begin{array}{c} x _ {2} \\ x _ {1} \end{array} \right),
$$

but, it should be apparent from the model that $x _ { 3 } ^ { 2 } = \phi _ { 1 } x _ { 2 } + \phi _ { 2 } x _ { 1 }$ . Because $\phi _ { 1 } x _ { 2 } + \phi _ { 2 } x _ { 1 }$ satisfies the prediction equations (3.53),

$$
\begin{array}{l} E \left\{\left[ x _ {3} - \left(\phi_ {1} x _ {2} + \phi_ {2} x _ {1}\right) \right] x _ {1} \right\} = E \left(w _ {3} x _ {1}\right) = 0, \\ E \left\{\left[ x _ {3} - \left(\phi_ {1} x _ {2} + \phi_ {2} x _ {1}\right) \right] x _ {2} \right\} = E \left(w _ {3} x _ {2}\right) = 0, \\ \end{array}
$$

it follows that, indeed, $x _ { 3 } ^ { 2 } = \phi _ { 1 } x _ { 2 } + \phi _ { 2 } x _ { 1 }$ , and by the uniqueness of the coefficients in this case, that $\phi _ { 2 1 } = \phi _ { 1 }$ and $\phi _ { 2 2 } = \phi _ { 2 }$ . Continuing in this way, it is easy to verify that, for $n \geq 2$ ,

$$
x _ {n + 1} ^ {n} = \phi_ {1} x _ {n} + \phi_ {2} x _ {n - 1}.
$$

That is, $\phi _ { n 1 } = \phi _ { 1 } , \phi _ { n 2 } = \phi _ { 2 }$ , and $\phi _ { n j } = 0$ , for $j = 3 , 4 , \dotsc , n$ .

From Example 3.17, it should be clear (Problem 3.38) that, if the time series is a causal AR( $p$ ) process, then, for $n \geq p$ ,

$$
x _ {n + 1} ^ {n} = \phi_ {1} x _ {n} + \phi_ {2} x _ {n - 1} + \dots + \phi_ {p} x _ {n - p + 1}. \tag {3.60}
$$

For ARMA models in general, the prediction equations will not be as simple as the pure AR case. In addition, for $n$ large, the use of (3.57) is prohibitive because it requires the inversion of a large matrix. There are, however, iterative solutions that do not require any matrix inversion. In particular, we mention the recursive solution due to Levinson (1947) and Durbin (1960).

# Property P3.4: The Durbin–Levinson Algorithm

Equations (3.57) and (3.59) can be solved iteratively as follows:

$$
\phi_ {0 0} = 0, \quad P _ {1} ^ {0} = \gamma (0). \tag {3.61}
$$

For $n \geq 1$ ,

$$
\phi_ {n n} = \frac {\rho (n) - \sum_ {k = 1} ^ {n - 1} \phi_ {n - 1 , k} \rho (n - k)}{1 - \sum_ {k = 1} ^ {n - 1} \phi_ {n - 1 , k} \rho (k)}, \quad P _ {n + 1} ^ {n} = P _ {n} ^ {n - 1} (1 - \phi_ {n n} ^ {2}), \qquad (3. 6 2)
$$

where, for $n \geq 2$ ,

$$
\phi_ {n k} = \phi_ {n - 1, k} - \phi_ {n n} \phi_ {n - 1, n - k}, \quad k = 1, 2, \dots , n - 1. \tag {3.63}
$$

The proof of Property P3.4 is left as an exercise; see Problem 3.12.

# Example 3.18 Using the Durbin–Levinson Algorithm

To use the algorithm, start with $\phi _ { 0 0 } = 0$ , $P _ { 1 } ^ { 0 } = \gamma ( 0 )$ . Then, for $n = 1$ ,

$$
\phi_ {1 1} = \rho (1) \quad \text {a n d} \quad P _ {2} ^ {1} = \gamma (0) [ 1 - \phi_ {1 1} ^ {2} ].
$$

For $n = 2$

$$
\begin{array}{l} \phi_ {2 2} = \frac {\rho (2) - \phi_ {1 1} \rho (1)}{1 - \phi_ {1 1} \rho (1)} = \frac {\rho (2) - \rho (1) ^ {2}}{1 - \rho (1) ^ {2}} \\ \phi_ {2 1} = \phi_ {1 1} - \phi_ {2 2} \phi_ {1 1} = \rho (1) [ 1 - \phi_ {2 2} ] \\ P _ {3} ^ {2} = \gamma (0) [ 1 - \phi_ {1 1} ^ {2} ] [ 1 - \phi_ {2 2} ^ {2} ]. \\ \end{array}
$$

For $n = 3$

$$
\phi_ {3 3} = \frac {\rho (3) - \phi_ {2 1} \rho (2) - \phi_ {2 2} \rho (1)}{1 - \phi_ {2 1} \rho (1) - \phi_ {2 2} \rho (2)},
$$

and so on.

An important consequence of the Durbin–Levinson algorithm is (see Problem 3.12) as follows.

# Property P3.5: Iterative Solution for the PACF

The PACF of a stationary process $x _ { t }$ , can be obtained iteratively via (3.62) as $\phi _ { n n }$ , for $n = 1 , 2 , \ldots$ .

# Example 3.19 The PACF of an AR(2)

From Example 3.14, we know that for an AR(2), $\phi _ { h h } = 0$ for $h > 2$ , but we will use the results of Example 3.17 and Property P3.5 to calculate the first three values of the PACF. Recall (Example 3.8) that for an AR(2), $\rho ( 1 ) = \phi _ { 1 } / ( 1 - \phi _ { 2 } )$ , and in general $\rho ( h ) - \phi _ { 1 } \rho ( h - 1 ) - \phi _ { 2 } \rho ( h - 2 ) = 0$ , for $h \geq 2$ . Then,

$$
\begin{array}{l} \phi_ {1 1} = \rho (1) = \frac {\phi_ {1}}{1 - \phi_ {2}} \\ \phi_ {2 2} = \frac {\rho (2) - \rho (1) ^ {2}}{1 - \rho (1) ^ {2}} = \frac {\left[ \phi_ {1} \left(\frac {\phi_ {1}}{1 - \phi_ {2}}\right) + \phi_ {2} \right] - \left(\frac {\phi_ {1}}{1 - \phi_ {2}}\right) ^ {2}}{1 - \left(\frac {\phi_ {1}}{1 - \phi_ {2}}\right) ^ {2}} = \phi_ {2} \\ \begin{array}{c c c} \phi_ {2 1} & = & \phi_ {1} \end{array} \\ \phi_ {3 3} = \frac {\rho (3) - \phi_ {1} \rho (2) - \phi_ {2} \rho (1)}{1 - \phi_ {1} \rho (1) - \phi_ {2} \rho (2)} = 0. \\ \end{array}
$$

# 3.5: Forecasting

So far, we have concentrated on one-step-ahead prediction, but Property P3.3 allows us to calculate the BLP of $x _ { n + m }$ for any $m \geq 1$ . Given data, $\{ x _ { 1 } , \ldots , x _ { n } \}$ , the $m$ -step-ahead predictor is

$$
x _ {n + m} ^ {n} = \phi_ {n 1} ^ {(m)} x _ {n} + \phi_ {n 2} ^ {(m)} x _ {n - 1} + \dots + \phi_ {n n} ^ {(m)} x _ {1}, \tag {3.64}
$$

$\{ \phi _ { n 1 } ^ { ( m ) } , \phi _ { n 2 } ^ { ( m ) } , . . . , \phi _ { n n } ^ { ( m ) } \}$ φnn satisfy the prediction equations,

$$
\sum_ {j = 1} ^ {n} \phi_ {n j} ^ {(m)} E (x _ {n + 1 - j} x _ {n + 1 - k}) = E (x _ {n + m} x _ {n + 1 - k}), \quad k = 1, \ldots , n,
$$

or

$$
\sum_ {j = 1} ^ {n} \phi_ {n j} ^ {(m)} \gamma (k - j) = \gamma (m + k - 1), \quad k = 1, \dots , n. \tag {3.65}
$$

The prediction equations can again be written in matrix notation as

$$
\Gamma_ {n} \phi_ {n} ^ {(m)} = \gamma_ {n} ^ {(m)}, \tag {3.66}
$$

where γ(mn $\gamma _ { n } ^ { ( m ) } = ( \gamma ( m ) , \ldots , \gamma ( m + n - 1 ) ) ^ { \prime } , \mathrm { a n d } \phi _ { n } ^ { ( m ) } = ( \phi _ { n 1 } ^ { ( m ) } , \ldots , \phi _ { n n } ^ { ( m ) } ) ^ { \prime }$ $\gamma _ { n } ^ { ( m ) } = ( \gamma ( m ) , \ldots , \gamma ( m + n - 1 ) ) ^ { \prime }$ $\phi _ { n } ^ { ( m ) } = ( \phi _ { n 1 } ^ { ( m ) } , . . . , \phi _ { n n } ^ { ( m ) } ) ^ { \prime }$ are $n \times 1$ vectors.

The mean square m-step-ahead prediction error is

$$
P _ {n + m} ^ {n} = E \left(x _ {n + m} - x _ {n + m} ^ {n}\right) ^ {2} = \gamma (0) - \gamma_ {n} ^ {(m) ^ {\prime}} \Gamma_ {n} ^ {- 1} \gamma_ {n} ^ {(m)}. \tag {3.67}
$$

Another useful algorithm for calculating forecasts was given by Brockwell and Davis (1991, Chapter 5). This algorithm follows directly from applying the projection theorem (Theorem B.1) to the innovations, $x _ { t } - x _ { t } ^ { t - 1 }$ , for $t = 1 , \ldots , n$ , using the fact that the innovations $x _ { t } - x _ { t } ^ { t - 1 }$ t−1 and $x _ { s } - x _ { s } ^ { s - 1 }$ are uncorrelated for $s \neq t$ (see Problem 3.39). We present the case in which $x _ { t }$ is a mean-zero stationary time series.

# Property P3.6: The Innovations Algorithm

The one-step-ahead predictors, $\boldsymbol { x } _ { t + 1 } ^ { t }$ , and their mean-squared errors, $P _ { t + 1 } ^ { t }$ , can be calculated iteratively as

$$
x _ {1} ^ {0} = 0, \quad P _ {1} ^ {0} = \gamma (0)
$$

$$
x _ {t + 1} ^ {t} = \sum_ {j = 1} ^ {t} \theta_ {t j} \left(x _ {t + 1 - j} - x _ {t + 1 - j} ^ {t - j}\right), \quad t = 1, 2, \dots \tag {3.68}
$$

$$
P _ {t + 1} ^ {t} = \gamma (0) - \sum_ {j = 0} ^ {t - 1} \theta_ {t, t - j} ^ {2} P _ {j + 1} ^ {j} \quad t = 1, 2, \dots , \tag {3.69}
$$

where, for $j = 0 , 1 , \ldots , t - 1$ ,

$$
\theta_ {t, t - j} = \left(\gamma (t - j) - \sum_ {k = 0} ^ {j - 1} \theta_ {j, j - k} \theta_ {t, t - k} P _ {k + 1} ^ {k}\right) \left(P _ {j + 1} ^ {j}\right) ^ {- 1}. \tag {3.70}
$$

sively for Given data $t = 1$ $x _ { 1 } , \ldots , x _ { n }$ , then $t = 2$ , the innovations algorithm can be calculated succes- and so on, in which case the calculation of $x _ { n + 1 } ^ { \pi }$ and $P _ { n + 1 } ^ { n }$ is made at the final step $t = n$ . The $m$ -step-ahead predictor and its mean-square error based on the innovations algorithm (Problem 3.39) are given by

$$
x _ {n + m} ^ {n} = \sum_ {j = m} ^ {n + m - 1} \theta_ {n + m - 1, j} \left(x _ {n + m - j} - x _ {n + m - j} ^ {n + m - j - 1}\right), \tag {3.71}
$$

$$
P _ {n + m} ^ {n} = \gamma (0) - \sum_ {j = m} ^ {n + m - 1} \theta_ {n + m - 1, j} ^ {2} P _ {n + m - j} ^ {n}, \tag {3.72}
$$

where the $\theta _ { n + m - 1 , j }$ are obtained by continued iteration of (3.70).

# Example 3.20 Prediction for an MA(1)

The innovations algorithm lends itself well to prediction for moving average processes. Consider an MA(1) model, ${ x _ { t } } = { w _ { t } } + \theta { w _ { t - 1 } }$ . Recall that $\gamma ( 0 ) = ( 1 + \theta ^ { 2 } ) \sigma _ { w } ^ { 2 }$ , $\gamma ( 1 ) = \theta \sigma _ { w } ^ { 2 }$ , and $\gamma ( h ) = 0$ for $h > 1$ . Then, using Property P3.6, we have

$$
\theta_ {n 1} = \theta \sigma_ {w} ^ {2} / P _ {n} ^ {n - 1}
$$

$$
\theta_ {n j} = 0, \quad j = 2, \ldots , n
$$

$$
P _ {1} ^ {0} = (1 + \theta^ {2}) \sigma_ {w} ^ {2}
$$

$$
P _ {n + 1} ^ {n} = (1 + \theta^ {2} - \theta \theta_ {n 1}) \sigma_ {w} ^ {2}.
$$

Finally, from (3.68), the one-step-ahead predictor is

$$
x _ {n + 1} ^ {n} = \theta \left(x _ {n} - x _ {n} ^ {n - 1}\right) \sigma_ {w} ^ {2} / P _ {n} ^ {n - 1}.
$$

# Forecasting ARMA Processes

The general prediction equations (3.53) provide little insight into forecasting for ARMA models in general. There are a number of different ways to express these forecasts, and each aids in understanding the special structure of ARMA prediction. Throughout, we assume $x _ { t }$ is a causal and invertible ARMA $( p , q )$ process, $\phi ( B ) x _ { t } = \theta ( B ) w _ { t }$ , where $w _ { t } \sim$ iid $\mathrm { N } ( 0 , \sigma _ { w } ^ { 2 } )$ . In the non-zero mean case, $E ( x _ { t } ) = \mu$ , simply replace $x _ { t }$ with $x _ { t } - \mu$ in the model. First, we consider two types of forecasts. We write $x _ { n + m } ^ { n }$ to mean the minimum mean square error predictor of $x _ { n + m }$ based on the data $\{ x _ { n } , \ldots , x _ { 1 } \}$ , that is,

$$
x _ {n + m} ^ {n} = E (x _ {n + m} \mid x _ {n}, \dots , x _ {1}).
$$

For ARMA models, it is easier to calculate the predictor of $x _ { n + m }$ , assuming we have the complete history of the process $\{ x _ { n } , x _ { n - 1 } , . . . \} $ . We will denote the predictor of $x _ { n + m }$ based on the infinite past as

$$
\widetilde {x} _ {n + m} = E \left(x _ {n + m} \mid x _ {n}, x _ {n - 1}, \dots\right).
$$

# 3.5: Forecasting

The idea here is that, for large samples, $\widetilde { x } _ { n + m }$ will provide a good approximation to xnn+m. $x _ { n + m } ^ { n }$

Now, write $x _ { n + m }$ in its causal and invertible forms:

$$
x _ {n + m} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {n + m - j}, \quad \psi_ {0} = 1 \tag {3.73}
$$

$$
w _ {n + m} = \sum_ {j = 0} ^ {\infty} \pi_ {j} x _ {n + m - j}, \quad \pi_ {0} = 1. \tag {3.74}
$$

Then, taking conditional expectations in (3.73), we have

$$
\widetilde {x} _ {n + m} = \sum_ {j = 0} ^ {\infty} \psi_ {j} \widetilde {w} _ {n + m - j} = \sum_ {j = m} ^ {\infty} \psi_ {j} w _ {n + m - j}, \tag {3.75}
$$

because, by (3.74),

$$
\widetilde {w} _ {t} \equiv E (w _ {t} | x _ {n}, x _ {n - 1}, \ldots) = \left\{ \begin{array}{l l} 0, & t > n \\ w _ {t}, & t \leq n. \end{array} \right.
$$

Similarly, taking conditional expectations in (3.74), we have

$$
0 = \widetilde {x} _ {n + m} + \sum_ {j = 1} ^ {\infty} \pi_ {j} \widetilde {x} _ {n + m - j},
$$

or

$$
\widetilde {x} _ {n + m} = - \sum_ {j = 1} ^ {m - 1} \pi_ {j} \widetilde {x} _ {n + m - j} - \sum_ {j = m} ^ {\infty} \pi_ {j} x _ {n + m - j}, \tag {3.76}
$$

using the fact $E ( x _ { t } \mid x _ { n } , x _ { n - 1 } , . ~ . ~ . ) = x _ { t }$ , for $t \leq n$ . Prediction is accomplished recursively using (3.76), starting with the one-step-ahead predictor, $m = 1$ , and then continuing for $m = 2 , 3 , \ldots$ . Using (3.75), we can write

$$
x _ {n + m} - \widetilde {x} _ {n + m} = \sum_ {j = 0} ^ {m - 1} \psi_ {j} w _ {n + m - j},
$$

so the mean square prediction error can be written as

$$
P _ {n + m} ^ {n} = E \left(x _ {n + m} - \widetilde {x} _ {n + m}\right) ^ {2} = \sigma_ {w} ^ {2} \sum_ {j = 0} ^ {m - 1} \psi_ {j} ^ {2}. \tag {3.77}
$$

Also, we note, for a fixed sample size, $n$ , the prediction errors are correlated. That is, for $k \geq 1$ ,

$$
E \left\{\left(x _ {n + m} - \widetilde {x} _ {n + m}\right) \left(x _ {n + m + k} - \widetilde {x} _ {n + m + k}\right) \right\} = \sigma_ {w} ^ {2} \sum_ {j = 0} ^ {m - 1} \psi_ {j} \psi_ {j + k}. \tag {3.78}
$$

# Example 3.21 Long-Range Forecasts

Consider forecasting an ARMA process with mean $\mu$ . From the zeromean case in (3.75) we can deduce that the $m$ -step-ahead forecast can be written as

$$
\widetilde {x} _ {n + m} = \mu + \sum_ {j = m} ^ {\infty} \psi_ {j} w _ {n + m - j}. \tag {3.79}
$$

Noting that the $\psi$ -weights dampen to zero exponentially fast, it is clear that

$$
\widetilde {x} _ {n + m} \to \mu
$$

exponentially fast (in the mean square sense) as $m \to \infty$ . Moreover, by (3.77), the mean square prediction error

$$
P _ {n + m} ^ {n} \rightarrow \sigma_ {w} ^ {2} \sum_ {j = 0} ^ {\infty} \psi_ {j} ^ {2}, \tag {3.80}
$$

exponentially fast as $m \to \infty$ .

It should be clear from (3.79) and (3.80) that ARMA forecasts quickly settle to the mean with a constant prediction error as the forecast horizon, $m$ , grows. This effect can be seen in Figure 3.6 where the recruitment series is forecast for 24 months; see Example 3.23.

When $n$ is small, the general prediction equations (3.53) can be used easily. When $n$ is large, we would use (3.76) by truncating, because only the data $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ able. In this case, we can truncate (3.76) by setting. The truncated predictor is then written as $\scriptstyle \sum _ { j = n + m } ^ { \infty } \pi _ { j } x _ { n + m - j } = 0$

$$
\widetilde {x} _ {n + m} ^ {n} = - \sum_ {j = 1} ^ {m - 1} \pi_ {j} \widetilde {x} _ {n + m - j} ^ {n} - \sum_ {j = m} ^ {n + m - 1} \pi_ {j} x _ {n + m - j}, \tag {3.81}
$$

which is also calculated recursively, $m = 1 , 2 , \ldots$ . The mean square prediction error, in this case, is approximated using (3.77).

For $\operatorname { A R } ( p )$ models, and when $n > p$ , equation (3.60) yields the exact pre-+n+m diction error is dictor, $n > p$ , $x _ { n + m } ^ { n }$ $\tilde { x } _ { n + m } ^ { n } = \tilde { x } _ { n + m } = x _ { n + m } ^ { n }$ , of $E ( x _ { n + 1 } - x _ { n + 1 } ^ { n } ) ^ { 2 } = \sigma _ { w } ^ { 2 }$ $x _ { n + m }$ , and there is no need for approximations. That is, for . Also, in this case, the one-step-ahead pre- . For general ARMA $( p , q )$ models, the truncated predictors (Problem 3.15) for $m = 1 , 2 , \ldots$ , are

$$
\widetilde {x} _ {n + m} ^ {n} = \phi_ {1} \widetilde {x} _ {n + m - 1} ^ {n} + \dots + \phi_ {p} \widetilde {x} _ {n + m - p} ^ {n} + \theta_ {1} \widetilde {w} _ {n + m - 1} ^ {n} + \dots + \theta_ {q} \widetilde {w} _ {n + m - q} ^ {n}, \tag {3.82}
$$

where $\widetilde { x } _ { t } ^ { n } = x _ { t }$ for $1 \leq t \leq n$ and $\partial _ { t } ^ { n } = 0$ for $t \leq 0$ . The truncated prediction errors are given by: $\widetilde { w } _ { t } ^ { n } = 0$ for $t \leq 0$ or $t > n$ , and $\widetilde { w } _ { t } ^ { n } = \phi ( B ) \widetilde { x } _ { t } ^ { n } - \theta _ { 1 } \widetilde { w } _ { t - 1 } ^ { n } -$ $\cdot \cdot \cdot - \theta _ { q } \tilde { w } _ { t - q } ^ { n }$ for $1 \leq t \leq n$ .

# 3.5: Forecasting

# Example 3.22 Forecasting an ARMA $( 1 , 1 )$ Series

Given data $x _ { 1 } , \ldots , x _ { n }$ , for forecasting purposes, write the model as

$$
x _ {n + 1} = \phi x _ {n} + w _ {n + 1} + \theta w _ {n}.
$$

Then, based on (3.82), the one-step-ahead truncated forecast is

$$
\widetilde {x} _ {n + 1} ^ {n} = \phi x _ {n} + 0 + \theta \widetilde {w} _ {n} ^ {n}.
$$

For $m \geq 2$ , we have

$$
\widetilde {x} _ {n + m} ^ {n} = \phi \widetilde {x} _ {n + m - 1} ^ {n},
$$

which can be calculated recursively, $m = 2 , 3 , \ldots$ .

To calculate $\widetilde { w } _ { n } ^ { n }$ , which is needed to initialize the successive forecasts, the model can be written as $w _ { t } = x _ { t } - \phi x _ { t - 1 } - \theta w _ { t - 1 }$ for $t = 1 , \ldots , n$ . For truncated forecasting, using (3.82), put $\widetilde { w } _ { 0 } ^ { n } = 0$ , $\widetilde { w } _ { 1 } ^ { n } = x _ { 1 }$ , and then iterate the errors forward in time

$$
\widetilde {w} _ {t} ^ {n} = x _ {t} - \phi x _ {t - 1} - \theta \widetilde {w} _ {t - 1} ^ {n}, \quad t = 2, \ldots , n.
$$

The approximate forecast variance is computed from (3.77) using the $\psi$ -weights determined as in Example 3.10. In particular, the $\psi$ -weights satisfy $\psi _ { j } = ( \phi + \theta ) \phi ^ { j - 1 }$ , for $j \geq 1$ . This result gives

$$
\begin{array}{l} P _ {n + m} ^ {n} = \sigma_ {w} ^ {2} \left[ 1 + (\phi + \theta) ^ {2} \sum_ {j = 1} ^ {m - 1} \phi^ {2 (j - 1)} \right] \\ = \sigma_ {w} ^ {2} \left[ 1 + \frac {(\phi + \theta) ^ {2} (1 - \phi^ {2 (m - 1)})}{(1 - \phi^ {2})} \right]. \\ \end{array}
$$

To assess the precision of the forecasts, prediction intervals are typically calculated along with the forecasts. In general, $( 1 - \alpha )$ prediction intervals are of the form

$$
x _ {n + m} ^ {n} \pm c _ {\frac {\alpha}{2}} \sqrt {P _ {n + m} ^ {n}}, \tag {3.83}
$$

where $c _ { \alpha / 2 }$ is chosen to get the desired degree of confidence. For example, if the process is Gaussian, then choosing $c _ { \alpha / 2 } = 2$ will yield an approximate 95% prediction interval for $x _ { n + m }$ . If we are interested in establishing prediction intervals over more than one time period, then $c _ { \alpha / 2 }$ should be adjusted appropriately, for example, by using Bonferroni’s inequality [see (4.55) in Chapter 4 or Johnson and Wichern, 1992, Chapter 5].

![](images/31c0d2bb8356827144959685680eda446318890d01c0abf7f5c5d11e7e81c1f3.jpg)  
Figure 3.6 Twenty-four month forecasts for the Recruitment series. The actual data shown are from January 1980 to September 1987, and then forecasts plus and minus one standard error are displayed.

# Example 3.23 Forecasting the Recruitment Series

Using the parameter estimates as the actual parameter values, Figure 3.6 shows the result of forecasting the Recruitment series given in Example 3.16 over a 24-month horizon, $m = 1 , 2 , \ldots , 2 4$ . The actual forecasts are calculated as

$$
x _ {n + m} ^ {n} = 6. 7 4 + 1. 3 5 x _ {n + m - 1} ^ {n} -. 4 6 x _ {n + m - 2} ^ {n}
$$

for $n = 4 5 3$ and $m = 1 , 2 , \ldots , 1 2$ . Recall that $x _ { t } ^ { s } = x _ { t }$ when $t \leq s$ . The forecasts errors $P _ { n + m } ^ { n }$ are calculated using (3.77). Recall that $\widehat { \sigma } _ { w } ^ { 2 } = 9 0 . 3 1 $ , and using (3.36) from Example 3.10, we have $\psi _ { j } = 1 . 3 5 \psi _ { j - 1 } - . 4 6 \psi _ { j - 2 }$ for $j \geq 2$ , where $\psi _ { 0 } = 1$ and $\psi _ { 1 } = 1 . 3 5$ . Thus, for $n = 4 5 3$ ,

$$
P _ {n + 1} ^ {n} \quad = \quad 9 0. 3 1,
$$

$$
P _ {n + 2} ^ {n} = 9 0. 3 1 \left(1 + 1. 3 5 ^ {2}\right),
$$

$$
P _ {n + 3} ^ {n} = 9 0. 3 1 \left(1 + 1. 3 5 ^ {2} + \left[ 1. 3 5 ^ {2} - . 4 6 \right] ^ {2}\right),
$$

and so on.

Note how the forecast levels off quickly and the prediction intervals are wide, even though in this case the forecast limits are only based on one standard error; that is, xnn+m ± P $x _ { n + m } ^ { n } \pm \sqrt { P _ { n + m } ^ { n } }$ . We will revisit this problem, including appropriate R commands, in Example 3.26.

We complete this section with a brief discussion of backcasting. In backcasting, we want to predict $x _ { 1 - m }$ , $m = 1 , 2 , \ldots$ , based on the data $\{ x _ { 1 } , \ldots , x _ { n } \}$ .

# 3.5: Forecasting

Write the backcast as

$$
x _ {1 - m} ^ {n} = \sum_ {j = 1} ^ {n} \alpha_ {j} x _ {j}. \tag {3.84}
$$

Analogous to (3.65), the prediction equations (assuming $\mu = 0$ ) are

$$
\sum_ {j = 1} ^ {n} \alpha_ {j} E \left(x _ {j} x _ {k}\right) = E \left(x _ {1 - m} x _ {k}\right), \quad k = 1, \dots , n, \tag {3.85}
$$

or

$$
\sum_ {j = 1} ^ {n} \alpha_ {j} \gamma (k - j) = \gamma (m + k - 1), \quad k = 1, \dots , n. \tag {3.86}
$$

These equations are precisely the prediction equations for forward prediction. That is, $\alpha _ { j } \ \equiv \ \phi _ { n j } ^ { ( m ) }$ , for $j = 1 , \dotsc , n$ , where the $\phi _ { n j } ^ { ( m ) }$ are given by (3.66). Finally, the backcasts are given by

$$
x _ {1 - m} ^ {n} = \phi_ {n 1} ^ {(m)} x _ {1} + \dots + \phi_ {n n} ^ {(m)} x _ {n}, \quad m = 1, 2, \dots . \tag {3.87}
$$

# Example 3.24 Backcasting an ARMA $( 1 , 1 )$

Consider a causal and invertible ARMA(1,1) process, $x _ { t } ~ = ~ \phi x _ { t - 1 } + $ $\theta w _ { t - 1 } + w _ { t }$ ; we will call this the forward model. We have just seen that best linear prediction backward in time is the same as best linear prediction forward in time for stationary models. Because we are assuming ARMA models are Gaussian, we also have that minimum mean square error prediction backward in time is the same as forward in time for ARMA models. Thus, the process can equivalently be generated by the backward model $x _ { t } = \phi x _ { t + 1 } + \theta v _ { t + 1 } + v _ { t }$ , where $\{ v _ { t } \}$ is a Gaussian4 white noise process with variance $\sigma _ { w } ^ { 2 }$ . We may write $\begin{array} { r } { x _ { t } = \sum _ { j = 0 } ^ { \infty } \psi _ { j } v _ { t + j } } \end{array}$ , where $\psi _ { 0 } = 1$ ; this means that $x _ { t }$ is uncorrelated with $\{ v _ { t - 1 } , v _ { t - 2 } , . . . \}$ , in analogy to the forward model.

Given data $\{ x _ { 1 } , . . . . , x _ { n } \}$ , truncate $v _ { n } ^ { n } = E ( v _ { n } \mid x _ { 1 } , . . . . , x _ { n } )$ to zero. That is, put $\widetilde { v } _ { n } ^ { n } = 0$ , as an initial approximation, and then generate the errors backward

$$
\widetilde {v} _ {t} ^ {n} = x _ {t} - \phi x _ {t + 1} + \theta \widetilde {v} _ {t + 1} ^ {n}, \quad t = (n - 1), (n - 2), \dots , 1.
$$

Then,

$$
\widetilde {x} _ {0} ^ {n} = \phi x _ {1} + \theta \widetilde {v} _ {1} ^ {n} + \widetilde {v} _ {0} ^ {n} = \phi x _ {1} + \theta \widetilde {v} _ {1} ^ {n},
$$

because $\tilde { v } _ { t } ^ { n } = 0$ for $t \leq 0$ . Continuing, the general truncated backcasts are given by

$$
\widetilde {x} _ {1 - m} ^ {n} = \phi \widetilde {x} _ {2 - m} ^ {n}, \quad m = 2, 3, \dots .
$$

# 3.6 Estimation

Throughout this section, we assume we have $n$ observations, $x _ { 1 } , \ldots , x _ { n }$ , from a causal and invertible Gaussian ARMA $( p , q )$ process in which, initially, the order parameters, $p$ and $q$ , are known. Our goal is to estimate the parameters, $\phi _ { 1 } , \ldots , \phi _ { p }$ , $\theta _ { 1 } , \ldots , \theta _ { q }$ , and $\sigma _ { w } ^ { 2 }$ . We will discuss the problem of determining $p$ and $q$ later in this section.

We begin with method of moments estimators. The idea behind these estimators is that of equating population moments to sample moments and then solving for the parameters in terms of the sample moments. We immediately see that, if $E ( x _ { t } ) = \mu$ , then the method of moments estimator of $\mu$ is the sample average, $x$ . Thus, while discussing method of moments, we will assume $\mu = 0$ . Although the method of moments can produce good estimators, they can sometimes lead to suboptimal estimators. We first consider the case in which the method leads to optimal (efficient) estimators, that is, $\operatorname { A R } ( p )$ models.

When the process is $\operatorname { A R } ( p )$ ,

$$
x _ {t} = \phi_ {1} x _ {t - 1} + \dots + \phi_ {p} x _ {t - p} + w _ {t},
$$

the first $p + 1$ equations of (3.42) and (3.43), $h = 0 , 1 , \ldots , p$ , lead to the following:

Definition 3.10 The Yule–Walker equations are given by

$$
\begin{array}{l} \gamma (h) = \phi_ {1} \gamma (h - 1) + \dots + \phi_ {p} \gamma (h - p), \quad h = 1, 2, \dots , p, (3.88) \\ \sigma_ {w} ^ {2} = \gamma (0) - \phi_ {1} \gamma (1) - \dots - \phi_ {p} \gamma (p). (3.89) \\ \end{array}
$$

In matrix notation, the Yule–Walker equations are

$$
\Gamma_ {p} \boldsymbol {\phi} = \boldsymbol {\gamma} _ {p}, \quad \sigma_ {w} ^ {2} = \gamma (0) - \boldsymbol {\phi} ^ {\prime} \boldsymbol {\gamma} _ {p}, \tag {3.90}
$$

where $\Gamma _ { p } = \{ \gamma ( k - j ) \} _ { j , k = 1 } ^ { p }$ is a $p \times p$ matrix, $\pmb { \phi } = ( \phi _ { 1 } , \ldots , \phi _ { p } ) ^ { \prime }$ is a $p \times 1$ vector, and $\gamma _ { p } = ( \gamma ( 1 ) , \dots , \gamma ( p ) ) ^ { \prime }$ is a $p \times 1$ vector. Using the method of moments, we replace $\gamma ( h )$ in (3.90) by $\widehat { \gamma } ( h )$ [see equation (1.36)] and solve

$$
\widehat {\phi} = \widehat {\Gamma} _ {p} ^ {- 1} \widehat {\gamma} _ {p}, \quad \widehat {\sigma} _ {w} ^ {2} = \widehat {\gamma} (0) - \widehat {\gamma} _ {p} ^ {\prime} \widehat {\Gamma} _ {p} ^ {- 1} \widehat {\gamma} _ {p}. \tag {3.91}
$$

These estimators are typically called the Yule–Walker estimators. For calculation purposes, it is sometimes more convenient to work with the sample ACF. By factoring $\widehat { \gamma } ( 0 )$ in (3.91), we can write the Yule–Walker estimates as

$$
\widehat {\boldsymbol {\phi}} = \widehat {\boldsymbol {R}} _ {p} ^ {- 1} \widehat {\boldsymbol {\rho}} _ {p}, \quad \widehat {\sigma} _ {w} ^ {2} = \widehat {\gamma} (0) \left[ 1 - \widehat {\boldsymbol {\rho}} _ {p} ^ {\prime} \widehat {\boldsymbol {R}} _ {p} ^ {- 1} \widehat {\boldsymbol {\rho}} _ {p} \right], \tag {3.92}
$$

where $\widehat { R } _ { p } = \{ \widehat { \rho } ( k - j ) \} _ { j , k = 1 } ^ { p }$ is a $p \times p$ matrix and $\widehat { \pmb { \rho } } _ { p } = ( \widehat { \rho } ( 1 ) , \dots , \widehat { \rho } ( p ) ) ^ { \prime }$ is a $p \times 1$ vector.

# 3.6: Estimation

For AR(p) models, if the sample size is large, the Yule–Walker estimators are approximately normally distributed, and $\widehat { \sigma } _ { w } ^ { 2 }$ is close to the true value of σ 2w . $\sigma _ { w } ^ { 2 }$ We state these results in Property P3.7. For details, see Appendix B, B.3.

# Property P3.7: Large Sample Results for Yule–Walker Estimators

The asymptotic ( $n \to \infty$ ) behavior of the Yule–Walker estimators in the case of causal AR(p) processes is as follows:

$$
\sqrt {n} (\widehat {\boldsymbol {\phi}} - \boldsymbol {\phi}) \stackrel {d} {\rightarrow} N (\mathbf {0}, \sigma_ {w} ^ {2} \Gamma_ {p} ^ {- 1}), \quad \widehat {\sigma} _ {w} ^ {2} \stackrel {p} {\rightarrow} \sigma_ {w} ^ {2}. \tag {3.93}
$$

The Durbin–Levinson algorithm, (3.61)-(3.63), can be used to calculate $\widehat { \phi }$ without inverting $\widehat { \Gamma } _ { p }$ or $\widehat { R } _ { p }$ , by replacing $\gamma ( h )$ by $\widehat { \gamma } ( h )$ in the algorithm. In running the algorithm, we will iteratively calculate the $h \times 1$ vector, $\widehat { \phi } _ { h } =$ $( \widehat { \phi } _ { h 1 } , \ldots , \widehat { \phi } _ { h h } ) ^ { \prime }$ , for $h = 1 , 2 , \ldots$ . Thus, in addition to obtaining the desired forecasts, the Durbin–Levinson algorithm yields $\widehat { \phi } _ { h h }$ , the sample PACF. Using (3.93), we can show the following property.

# Property P3.8: Large Sample Distribution of the PACF

For a causal $A R ( p )$ process, asymptotically $\textstyle n \to \infty$ ),

$$
\sqrt {n} \widehat {\phi} _ {h h} \stackrel {{d}} {{\rightarrow}} \mathrm {N} (0, 1), \quad \text {f o r} \quad h > p. \tag {3.94}
$$

# Example 3.25 Yule–Walker Estimation for an AR(2) Process

The data shown in Figure 3.3 were $n = 1 4 4$ simulated observations from the AR(2) model

$$
x _ {t} = 1. 5 x _ {t - 1} - . 7 5 x _ {t - 2} + w _ {t},
$$

where $w _ { t } \sim$ iid $\mathrm { { N } } ( 0 , 1 )$ . For this data, $\widehat { \gamma } ( 0 ) = 8 . 4 3 4$ , $\widehat { \rho } ( 1 ) = . 8 3 4$ , and $\widehat { \rho } ( 2 ) = . 4 7 6$ . Thus,

$$
\widehat {\boldsymbol {\phi}} = \left( \begin{array}{c} \widehat {\phi} _ {1} \\ \widehat {\phi} _ {2} \end{array} \right) = \left[ \begin{array}{c c} 1 & . 8 3 4 \\ . 8 3 4 & 1 \end{array} \right] ^ {- 1} \left( \begin{array}{c} . 8 3 4 \\ . 4 7 6 \end{array} \right) = \left( \begin{array}{c} 1. 4 3 9 \\ -. 7 2 5 \end{array} \right)
$$

and

$$
\widehat {\sigma} _ {w} ^ {2} = 8. 4 3 4 \left[ 1 - (. 8 3 4, . 4 7 6) \left( \begin{array}{c} 1. 4 3 9 \\ -. 7 2 5 \end{array} \right) \right] = 1. 2 1 5.
$$

By Property P3.7, the asymptotic variance–covariance matrix of $\hat { \phi }$ ,

$$
\frac {1}{1 4 4} \frac {1 . 2 1 5}{8 . 4 3 4} \left[ \begin{array}{c c} 1 & . 8 3 4 \\ . 8 3 4 & 1 \end{array} \right] ^ {- 1} = \left[ \begin{array}{c c} . 0 5 7 ^ {2} & -. 0 0 3 \\ -. 0 0 3 & . 0 5 7 ^ {2} \end{array} \right],
$$

can be used to get confidence regions for, or make inferences about $\widehat { \phi }$ and its components. For example, an approximate 95% confidence interval

for $\phi _ { 2 }$ is $- . 7 2 5 { \pm } 2 ( . 0 5 7 )$ , or $( - . 8 3 9 , - . 6 1 1 )$ , which contains the true value of $\phi _ { 2 } = - . 7 5$ .

For this data, the first three sample partial autocorrelations were $\widehat { \phi } _ { 1 1 } =$ $\widehat { \rho } ( 1 ) = . 8 3 4$ , $\widehat { \phi } _ { 2 2 } = \widehat { \phi } _ { 2 } = - . 7 2 5$ , and $\hat { \phi } _ { 3 3 } = - . 0 7 5$ . According to Property P3.8, the asymptotic standard error of $\hat { \phi } _ { 3 3 }$ is $1 / \sqrt { 1 4 4 } = . 0 8 3$ , and the observed value, −.075, is less than one standard deviation from $\phi _ { 3 3 } = 0$ .

# Example 3.26 Yule–Walker Estimation of the Recruitment Series

In Example 3.16 we fit an AR(2) model to the recruitment series using regression. Below are the results of fitting the same model using Yule-Walker estimation in R (assuming the data are in rec), which are nearly identical to the values in Example 3.16.

```markdown
> rec.yw = ar.yw(rec, order=2)  
> rec.yw\(x.mean  
[1] 62.26278 # mean estimate  
> rec.yw\)ar  
[1] 1.3315874 -.4445447 # phi1 and phi2 estimates  
> sqrt(diag(rec.yw\)asy.var.coef))  
[1] .04222637 .04222637 # their standard errors  
> rec.yw\)var_pred  
[1] 94.79912 # error variance estimate 
```

To obtain the 24 month ahead predictions and their standard errors, and then plot the results as in Example 3.23, use the R commands:

```txt
> rec.pr = predict(rec.yw, n.ahead=24)
> U = rec.pr$pred + rec.pr$se
> L = rec.pr$pred - rec.pr$se
> month = 360:453
> plot(month, rec[month], type="o", xlim=c(360,480),
+ ylab="recruits")
> lines(rec.pr$pred, col="red", type="o")
> lines(U, col="blue", lty="dashed")
> lines(L, col="blue", lty="dashed") 
```

In the case of AR(p) models, the Yule–Walker estimators given in (3.92) are optimal in the sense that the asymptotic distribution, (3.93), is the best asymptotic normal distribution. This is because, given initial conditions, $\operatorname { A R } ( p )$ models are linear models, and the Yule–Walker estimators are essentially least squares estimators. If we use method of moments for MA or ARMA models, we will not get optimal estimators because such processes are nonlinear in the parameters.

# Example 3.27 Method of Moments Estimation for an MA(1) Process

Consider the time series

$$
x _ {t} = w _ {t} + \theta w _ {t - 1},
$$

where $| \theta | < 1$ . The model can then be written as

$$
x _ {t} = \sum_ {j = 1} ^ {\infty} (- \theta) ^ {j} x _ {t - j} + w _ {t},
$$

which is nonlinear in $\theta$ . The first two population autocovariances are $\gamma ( 0 ) = \sigma _ { w } ^ { 2 } ( 1 + \theta ^ { 2 } )$ and $\gamma ( 1 ) = \sigma _ { w } ^ { 2 } \theta$ , so the estimate of $\theta$ is found by solving:

$$
\widehat {\rho} (1) = \frac {\widehat {\gamma} (1)}{\widehat {\gamma} (0)} = \frac {\widehat {\theta}}{1 + \widehat {\theta} ^ {2}}.
$$

Two solutions exist, so we would pick the invertible one. If $\begin{array} { r } { | \widehat { \rho } ( 1 ) | \le \frac { 1 } { 2 } } \end{array}$ , the solutions are real, otherwise, a real solution does not exist. Even though $| \rho ( 1 ) | < \frac { 1 } { 2 }$ for an invertible MA(1), it may happen that $\begin{array} { r } { | \widehat { \rho } ( 1 ) | \geq \frac { 1 } { 2 } } \end{array}$ because it is an estimator. When $\begin{array} { r } { | \widehat { \rho } ( 1 ) | < \frac { 1 } { 2 } } \end{array}$ , the invertible estimate is

$$
\widehat {\theta} = \frac {1 - \sqrt {1 - 4 \widehat {\rho} (1) ^ {2}}}{2 \widehat {\rho} (1)}.
$$

It can be shown $^ 5$ that

$$
\widehat {\theta} \sim \mathrm {A N} \left(\theta , \frac {1 + \theta^ {2} + 4 \theta^ {4} + \theta^ {6} + \theta^ {8}}{n (1 - \theta^ {2}) ^ {2}}\right).
$$

The maximum likelihood estimator (which we discuss next) of $\theta$ , in this case, has an asymptotic variance of $\left( 1 - \theta ^ { 2 } \right) / n$ . When $\theta = . 5$ , for example, the ratio of the asymptotic variance of the method of moments estimator to the maximum likelihood estimator of $\theta$ is about 3.5. That is, for large samples, the variance of the method of moments estimator is about 3.5 times larger than the variance of the MLE of $\theta$ when $\theta = . 5$ .

# Maximum Likelihood and Least Squares Estimation

To fix ideas, we first focus on the causal AR(1) case. Let

$$
x _ {t} = \mu + \phi (x _ {t - 1} - \mu) + w _ {t}
$$

where $| \phi | < 1$ and $w _ { t } \sim$ iid N $( 0 , \sigma _ { w } ^ { 2 } )$ . Given data $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ , we seek the likelihood

$$
L (\mu , \phi , \sigma_ {w} ^ {2}) = f _ {\mu , \phi , \sigma_ {w} ^ {2}} (x _ {1}, x _ {2}, \ldots , x _ {n}).
$$

In the case of an AR(1), we may write the likelihood as

$$
L (\mu , \phi , \sigma_ {w} ^ {2}) = f (x _ {1}) f (x _ {2} \mid x _ {1}) \dots f (x _ {n} \mid x _ {n - 1}),
$$

where we have dropped the parameters in the densities, $f ( \cdot )$ , to ease the notation. Because $x _ { t } \mid x _ { t - 1 } \sim \mathrm { N } \left( \mu + \phi ( x _ { t - 1 } - \mu ) , \sigma _ { w } ^ { 2 } \right)$ , we have

$$
f \left(x _ {t} \mid x _ {t - 1}\right) = f _ {w} \left[ \left(x _ {t} - \mu\right) - \phi \left(x _ {t - 1} - \mu\right) \right],
$$

where $f _ { w } ( \cdot )$ is the density of $w _ { t }$ , that is, the normal density with mean zero and variance $\sigma _ { w } ^ { 2 }$ . We may then write the likelihood as

$$
L (\mu , \phi , \sigma_ {w}) = f (x _ {1}) \prod_ {t = 2} ^ {n} f _ {w} \left[ (x _ {t} - \mu) - \phi (x _ {t - 1} - \mu) \right].
$$

To find $f ( x _ { 1 } )$ , we can use the causal representation

$$
x _ {1} = \mu + \sum_ {j = 0} ^ {\infty} \phi^ {j} w _ {1 - j}
$$

to see that $x _ { 1 }$ is normal, with mean $\mu$ and variance $\sigma _ { w } ^ { 2 } / ( 1 - \phi ^ { 2 } )$ . Finally, for an AR(1), the likelihood is

$$
L (\mu , \phi , \sigma_ {w} ^ {2}) = (2 \pi \sigma_ {w} ^ {2}) ^ {- n / 2} (1 - \phi^ {2}) ^ {1 / 2} \exp \left[ - \frac {S (\mu , \phi)}{2 \sigma_ {w} ^ {2}} \right], \tag {3.95}
$$

where

$$
S (\mu , \phi) = (1 - \phi^ {2}) (x _ {1} - \mu) ^ {2} + \sum_ {t = 2} ^ {n} \left[ \left(x _ {t} - \mu\right) - \phi \left(x _ {t - 1} - \mu\right) \right] ^ {2}. \tag {3.96}
$$

Typically, $S ( \mu , \phi )$ is called the unconditional sum of squares. We could have also considered the estimation of $\mu$ and $\phi$ using unconditional least squares, that is, estimation by minimizing $S ( \mu , \phi )$ .

Taking the partial derivative of the log of (3.95) with respect to $\sigma _ { w } ^ { 2 }$ and setting the result equal to zero, we see that for any given values of $\mu$ and $\phi$ in the parameter space, $\sigma _ { w } ^ { 2 } = n ^ { - 1 } S ( \mu , \phi )$ maximizes the likelihood. Thus, the maximum likelihood estimate of $\sigma _ { w } ^ { 2 }$ is

$$
\hat {\sigma} _ {w} ^ {2} = n ^ {- 1} S (\widehat {\mu}, \widehat {\phi}), \tag {3.97}
$$

where $\widehat { \mu }$ and $\widehat { \phi }$ are the MLEs of $\mu$ and $\phi$ , respectively. If we replace $n$ in (3.97) by $n - 2$ , we would obtain the unconditional least squares estimate of $\sigma _ { w } ^ { 2 }$ .

If, in (3.95), we take logs, replace $\sigma _ { w } ^ { 2 }$ by $\widehat { \sigma } _ { w } ^ { 2 }$ , and ignore constants, $\widehat { \mu }$ and $\hat { \phi }$ are the values that minimize the criterion function

$$
l (\mu , \phi) = \ln [ n ^ {- 1} S (\mu , \phi) ] - n ^ {- 1} \ln (1 - \phi^ {2}). \tag {3.98}
$$

# 3.6: Estimation

That is, $l ( \mu , \phi ) \propto - 2 \ln { \cal L } ( \mu , \phi , \widehat { \sigma } _ { w } ^ { 2 } )$ .6 Because (3.96) and (3.98) are complicated functions of the parameters, the minimization of $l ( \mu , \phi )$ or $S ( \mu , \phi )$ is accomplished numerically. In the case of AR models, we have the advantage that, conditional on initial values, they are linear models. That is, we can drop the term in the likelihood that causes the nonlinearity. Conditioning on $x _ { 1 }$ , the conditional likelihood becomes

$$
\begin{array}{l} L (\mu , \phi , \sigma_ {w} ^ {2} | x _ {1}) = \prod_ {t = 2} ^ {n} f _ {w} [ (x _ {t} - \mu) - \phi (x _ {t - 1} - \mu) ] \\ = (2 \pi \sigma_ {w} ^ {2}) ^ {- (n - 1) / 2} \exp \left[ - \frac {S _ {c} (\mu , \phi)}{2 \sigma_ {w} ^ {2}} \right], \tag {3.99} \\ \end{array}
$$

where the conditional sum of squares is

$$
S _ {c} (\mu , \phi) = \sum_ {t = 2} ^ {n} [ (x _ {t} - \mu) - \phi (x _ {t - 1} - \mu) ] ^ {2}. \tag {3.100}
$$

The conditional MLE of $\sigma _ { w } ^ { 2 }$ is

$$
\hat {\sigma} _ {w} ^ {2} = S _ {c} (\widehat {\mu}, \widehat {\phi}) / (n - 1), \tag {3.101}
$$

and $\widehat { \mu }$ and $\widehat { \phi }$ are the values that minimize the conditional sum of squares, $S _ { c } ( \mu , \phi )$ . Letting $\alpha = \mu ( 1 - \phi )$ , the conditional sum of squares can be written as

$$
S _ {c} (\mu , \phi) = \sum_ {t = 2} ^ {n} \left[ x _ {t} - \left(\alpha + \phi x _ {t - 1}\right) \right] ^ {2}. \tag {3.102}
$$

The problem is now the linear regression problem stated in §2.2. Following the results from least squares estimation, we have $\begin{array} { r } { \bar { x } _ { ( 1 ) } = ( n - 1 ) ^ { - 1 } \sum _ { t = 1 } ^ { n - 1 } x _ { t } } \end{array}$ , and $\begin{array} { r } { \bar { x } _ { ( 2 ) } = ( n - 1 ) ^ { - 1 } \sum _ { t = 2 } ^ { n } x _ { t } } \end{array}$ $\widehat { \alpha } = \bar { x } _ { ( 2 ) } - \widehat { \phi } \bar { x } _ { ( 1 ) }$ , and the conditional , where

$$
\widehat {\mu} = \frac {\bar {x} _ {(2)} - \widehat {\phi} \bar {x} _ {(1)}}{1 - \widehat {\phi}} \tag {3.103}
$$

$$
\widehat {\phi} = \frac {\sum_ {t = 2} ^ {n} \left(x _ {t} - \bar {x} _ {(2)}\right) \left(x _ {t - 1} - \bar {x} _ {(1)}\right)}{\sum_ {t = 2} ^ {n} \left(x _ {t - 1} - \bar {x} _ {(1)}\right) ^ {2}}. \tag {3.104}
$$

From (3.103) and (3.104), we see that $\widehat { \mu } \approx \bar { x }$ and $\widehat { \phi } \approx \widehat { \rho } ( 1 )$ . That is, the Yule–Walker estimators and the conditional least squares estimators are approximately the same. The only difference is the inclusion or exclusion of terms involving the end points, $x _ { 1 }$ and $x _ { n }$ . We can also adjust the estimate of $\sigma _ { w } ^ { 2 }$ in (3.101) to be equivalent to the least squares estimator, that is, divide $S _ { c } ( \widehat { \mu } , \widehat { \phi } )$ by $( n - 3 )$ instead of $( n - 1 )$ in (3.101).

For general $\operatorname { A R } ( p )$ models, maximum likelihood estimation, unconditional least squares, and conditional least squares follow analogously to the AR(1)

example. For general ARMA models, it is difficult to write the likelihood as an explicit function of the parameters. Instead, it is advantageous to write the likelihood in terms of the innovations, or one-step-ahead prediction errors, $x _ { t } - x _ { t } ^ { t - 1 }$ This will also be useful in Chapter 6 when we study state-space models.

Suppose $x _ { t }$ is a causal ARMA $( p , q )$ process with $w _ { t } \sim$ iid $\mathrm { N } ( 0 , \sigma _ { w } ^ { 2 } )$ . Let $\pmb { \beta } = ( \mu , \phi _ { 1 } , \ldots , \phi _ { p } , \theta _ { 1 } , \ldots , \theta _ { q } ) ^ { \prime }$ be the $( p + q + 1 ) \times 1$ vector of the model parameters. The likelihood can be written as

$$
L (\boldsymbol {\beta}, \sigma_ {w} ^ {2}) = \prod_ {t = 1} ^ {n} f (x _ {t} \mid x _ {t - 1}, \dots , x _ {1}).
$$

nditionad variawhere ution of . In addies not de $x _ { t }$ given n, for Aend on $x _ { t - 1 } , \ldots , x _ { 1 }$ is Gaussian witls, we may write an readily be se $x _ { t } ^ { t - 1 }$ $P _ { t } ^ { t - 1 }$ $P _ { t } ^ { t - 1 } =$ $\sigma _ { w } ^ { 2 } r _ { t } ^ { t - 1 }$ $r _ { t } ^ { t - 1 }$ $\sigma _ { w } ^ { 2 }$ w t t Proposition P3.4 by noting $\begin{array} { r } { P _ { 1 } ^ { 0 } = \gamma ( 0 ) = \sigma _ { w } ^ { 2 } \sum _ { j = 0 } ^ { \infty } \psi _ { j } ^ { 2 } ) } \end{array}$ .

The likelihood of the data can now be written as

$$
L (\boldsymbol {\beta}, \sigma_ {w} ^ {2}) = (2 \pi \sigma_ {w} ^ {2}) ^ {- n / 2} \left[ r _ {1} ^ {0} (\boldsymbol {\beta}) r _ {2} ^ {1} (\boldsymbol {\beta}) \dots r _ {n} ^ {n - 1} (\boldsymbol {\beta}) \right] ^ {- 1 / 2} \exp \left[ - \frac {S (\boldsymbol {\beta})}{2 \sigma_ {w} ^ {2}} \right], \tag {3.105}
$$

where

$$
S (\boldsymbol {\beta}) = \sum_ {t = 1} ^ {n} \left[ \frac {\left(x _ {t} - x _ {t} ^ {t - 1} (\boldsymbol {\beta})\right) ^ {2}}{r _ {t} ^ {t - 1} (\boldsymbol {\beta})} \right]. \tag {3.106}
$$

Both $x _ { t } ^ { t - 1 }$ and $r _ { t } ^ { t - 1 }$ are functions of $\beta$ , and we make that fact explicit in (3.105)- (3.106). Given values for $\beta$ and $\sigma _ { w } ^ { 2 }$ , the likelihood may be evaluated using the techniques of §3.5. Maximum likelihood estimation would now proceed by maximizing (3.105) with respect to $\beta$ and $\sigma _ { w } ^ { 2 }$ . As in the AR(1) example, we have

$$
\hat {\sigma} _ {w} ^ {2} = n ^ {- 1} S (\hat {\boldsymbol {\beta}}), \tag {3.107}
$$

where $\widehat { \beta }$ is the value of $\beta$ that minimizes the criterion function

$$
l (\boldsymbol {\beta}) = \ln [ n ^ {- 1} S (\boldsymbol {\beta}) ] + n ^ {- 1} \sum_ {t = 1} ^ {n} \ln r _ {t} ^ {t - 1} (\boldsymbol {\beta}). \tag {3.108}
$$

For example, for the AR(1) model previously discussed, the generic $l ( \beta )$ in (3.108) is $l ( \mu , \phi )$ in (3.98), and the generic $S ( \beta )$ in (3.106) is $S ( \mu , \phi )$ given in (3.96). From (3.96) and (3.98) we see $x _ { 1 } ^ { 0 } = \mu$ , and $x _ { t } ^ { t - 1 } = \mu + \phi ( x _ { t - 1 } - \mu )$ for $t = 2 , \ldots , n$ . Also $r _ { 1 } ^ { 0 } = \left( 1 - \phi ^ { 2 } \right)$ , and $r _ { t } ^ { t - 1 } = 1$ for $t = 2 , \ldots , n$ .

Unconditional least squares would be performed by minimizing (3.106) with respect to $\beta$ . Conditional least squares estimation would involve minimizing (3.106) with respect to $\beta$ but where, to ease the computational burden, the predictions and their errors are obtained by conditioning on initial values of the data. In general, numerical optimization routines are used to obtain the actual estimates and their standard errors.

# Example 3.28 The Newton–Raphson and Scoring Algorithms

Two common numerical optimization routines for accomplishing maximum likelihood estimation are Newton–Raphson and scoring. We will give a brief account of the mathematical ideas here. The actual implementation of these algorithms is much more complicated than our discussion might imply. For details, the reader is referred to any of the Numerical Recipes books, for example, Press et al. (1993).

Let $l ( \beta )$ be a criterion function of $k$ parameters $\beta = ( \beta _ { 1 } , \ldots , \beta _ { k } )$ that we wish to minimize with respect to $\beta$ . For example, consider the likelihood function given by (3.98) or by (3.108). Suppose $l ( \widehat { \beta } )$ is the extremum that we are interested in finding, and $\widehat { \beta }$ is found by solving $\partial l ( \beta ) / \partial \beta _ { j } = 0$ , for $j = 1 , \dots , k$ . Let $l ^ { ( 1 ) } ( \beta )$ denote the $k \times 1$ vector of partials

$$
l ^ {(1)} (\boldsymbol {\beta}) = \left(\frac {\partial l (\boldsymbol {\beta})}{\partial \beta_ {1}}, \dots , \frac {\partial l (\boldsymbol {\beta})}{\partial \beta_ {k}}\right) ^ {\prime}.
$$

Note, $l ^ { ( 1 ) } ( \widehat { \beta } ) = \mathbf { 0 }$ , the $k \times 1$ zero vector. Let $l ^ { ( 2 ) } ( \beta )$ denote the $k \times k$ matrix of second-order partials

$$
l ^ {(2)} (\pmb {\beta}) = \left\{- \frac {\partial l ^ {2} (\pmb {\beta})}{\partial \beta_ {i} \partial \beta_ {j}} \right\} _ {i, j = 1} ^ {k},
$$

and assume $l ^ { ( 2 ) } ( \beta )$ is nonsingular. Let $\beta _ { ( 0 ) }$ be an initial estimator of $\beta$ . Then, using a Taylor expansion, we have the following approximation:

$$
\mathbf {0} = l ^ {(1)} (\widehat {\boldsymbol {\beta}}) \approx l ^ {(1)} (\boldsymbol {\beta} _ {(0)}) - l ^ {(2)} (\boldsymbol {\beta} _ {(0)}) \left[ \widehat {\boldsymbol {\beta}} - \boldsymbol {\beta} _ {0} \right].
$$

Setting the right-hand side equal to zero and solving for $\widehat { \beta }$ (call the solution $\beta _ { ( 1 ) }$ ), we get

$$
\pmb {\beta} _ {(1)} = \pmb {\beta} _ {(0)} + \left[ l ^ {(2)} (\pmb {\beta} _ {(0)}) \right] ^ {- 1} l ^ {(1)} (\pmb {\beta} _ {(0)}).
$$

The Newton–Raphson algorithm proceeds by iterating this result, replacing $\beta _ { ( 0 ) }$ by $\beta _ { ( 1 ) }$ to get $\beta _ { ( 2 ) }$ , and so on, until convergence. Under a set of appropriate conditions, the sequence of estimators, $\beta _ { ( 1 ) } , \beta _ { ( 2 ) } , \ldots .$ , will converge to $\widehat { \beta }$ , the MLE of $\beta$ .

For maximum likelihood estimation, the criterion function used is $l ( \beta )$ given by (3.108); $l ^ { ( 1 ) } ( \beta )$ is called the score vector, and $l ^ { ( 2 ) } ( \beta )$ is called the Hessian. In the method of scoring, we replace $l ^ { ( 2 ) } ( \beta )$ by $E [ l ^ { ( 2 ) } ( \beta ) ]$ , the information matrix. Under appropriate conditions, the inverse of the information matrix is the asymptotic variance–covariance matrix of the estimator $\widehat { \beta }$ . This is sometimes approximated by the inverse of the Hessian at $\widehat { \beta }$ . If the derivatives are difficult to obtain, it is possible to use quasi-maximum likelihood estimation where numerical techniques are used to approximate the derivatives.

# Example 3.29 MLE for the Recruitment Series

So far, we have fit an AR(2) model to the recruitment series using ordinary least squares (Example 3.16) and using Yule–Walker (Example 3.26). The following is an R session used to fit an AR(2) model via maximum likelihood estimation to the recruitment series; these results can be compared to the results in Examples 3.16 and 3.26. As before, we assume the data have been read into R as rec.

```txt
> rec.mle = ar.mle(rec, order=2)
> rec.mle\(x.mean
[1] 62.26153
> rec.mle\)ar
[1] 1.3512809 -.4612736
> sqrt(diag(rec.mle\)asy.var.coef))
[1] .04099159 .04099159
> rec.mle\)var_pred
[1] 89.33597 
```

We now discuss least squares for ARMA $( p , q )$ models via Gauss–Newton. For general and complete details of the Gauss–Newton procedure, the reader is referred to Fuller (1995). Let $x _ { t }$ be a causal and invertible Gaussian ARMA $( p , q )$ process. Write $\pmb { \beta } = ( \phi _ { 1 } , \ldots , \phi _ { p } , \theta _ { 1 } , \ldots , \theta _ { q } ) ^ { \prime }$ , and for the ease of discussion, we will put $\mu = 0$ . We write the model in terms of the errors

$$
w _ {t} (\boldsymbol {\beta}) = x _ {t} - \sum_ {j = 1} ^ {p} \phi_ {j} x _ {t - j} - \sum_ {k = 1} ^ {q} \theta_ {k} w _ {t - k} (\boldsymbol {\beta}), \tag {3.109}
$$

emphasizing the dependence of the errors on the parameters.

For conditional least squares, we approximate the residual sum of squares by conditioning on $x _ { 1 } , \ldots , x _ { p }$ ( $p > 0$ ) and $w _ { p } = w _ { p - 1 } = w _ { p - 2 } = \cdot \cdot \cdot = w _ { 1 - q } = 0$ ( $q > 0$ ), in which case we may evaluate (3.109) for $t = p + 1 , p + 2 , \ldots , n$ . Using this conditioning argument, the conditional error sum of squares is

$$
S _ {c} (\boldsymbol {\beta}) = \sum_ {t = p + 1} ^ {n} w _ {t} ^ {2} (\boldsymbol {\beta}).
$$

Minimizing $S _ { c } ( \beta )$ with respect to $\beta$ yields the conditional least squares estimates. If $q = 0$ , the problem is linear regression, and no iterative technique is needed to minimize $S _ { c } ( \phi _ { 1 } , \ldots , \phi _ { p } )$ . If $q > 0$ , the problem becomes nonlinear regression, and we will have to rely on numerical optimization.

When $n$ is large, conditioning on a few initial values will have little influence on the final parameter estimates. In the case of small to moderate sample sizes, one may wish to rely on unconditional least squares. The unconditional least squares problem is to choose $\beta$ to minimize the unconditional sum of squares, which we have generically denoted by $S ( \beta )$ in this section. The unconditional

# 3.6: Estimation

sum of squares can be written in various ways, and one useful form in the case of $\mathrm { A R M A } ( p , q )$ models is derived in Box et al. (1994, Appendix A7.3). They showed (see Problem 3.18) the unconditional sum of squares can be written as

$$
S (\boldsymbol {\beta}) = \sum_ {t = - \infty} ^ {n} \widehat {w} _ {t} ^ {2} (\boldsymbol {\beta}),
$$

where $\widehat { w } _ { t } ( \beta ) \ = \ E ( w _ { t } \ \mid \ x _ { 1 } , . . . , x _ { n } )$ . When $t ~ \leq ~ 0$ , the $\widehat { w } _ { t } ( \beta )$ are obtained by backcasting. As a practical matter, we approximate $S ( \beta )$ by starting the sum at $t ~ = ~ - M + 1$ , where $M$ is chosen large enough to guarantee $\begin{array} { r } { \sum _ { t = - \infty } ^ { - M } \widehat { w } _ { t } ^ { 2 } ( \pmb { \beta } ) \approx 0 } \end{array}$ . In the case of unconditional least squares estimation, a numerical optimization technique is needed even when $q = 0$ .

To employ Gauss–Newton, let $\pmb { \beta } _ { ( 0 ) } = ( \phi _ { 1 } ^ { ( 0 ) } , \dots , \phi _ { p } ^ { ( 0 ) } , \theta _ { 1 } ^ { ( 0 ) } , \dots , \theta _ { q } ^ { ( 0 ) } ) ^ { \prime }$ φp (0) (0) be an initial estimate of $\beta$ . For example, we could obtain $\beta _ { ( 0 ) }$ by method of moments. The first-order Taylor expansion of $w _ { t } ( \beta )$ is

$$
w _ {t} (\boldsymbol {\beta}) \approx w _ {t} \left(\boldsymbol {\beta} _ {(0)}\right) - \left(\boldsymbol {\beta} - \boldsymbol {\beta} _ {(0)}\right) ^ {\prime} \boldsymbol {z} _ {t} \left(\boldsymbol {\beta} _ {(0)}\right), \tag {3.110}
$$

where

$$
\pmb {z} _ {t} (\pmb {\beta} _ {(0)}) = \left(- \frac {\partial w _ {t} (\pmb {\beta} _ {(0)})}{\partial \beta_ {1}}, \dots , - \frac {\partial w _ {t} (\pmb {\beta} _ {(0)})}{\partial \beta_ {p + q}}\right) ^ {\prime}, t = 1, \dots , n.
$$

The linear approximation of $S _ { c } ( \beta )$ is

$$
Q (\boldsymbol {\beta}) = \sum_ {t = p + 1} ^ {n} \left[ w _ {t} \left(\boldsymbol {\beta} _ {(0)}\right) - \left(\boldsymbol {\beta} - \boldsymbol {\beta} _ {(0)}\right) ^ {\prime} \boldsymbol {z} _ {t} \left(\boldsymbol {\beta} _ {(0)}\right) \right] ^ {2} \tag {3.111}
$$

and this is the quantity that we will minimize. For approximate unconditional least squares, we would start the sum in (3.111) at $t = - M + 1$ , for a large value of $M$ , and work with the backcasted values.

Using the results of ordinary least squares (§2.2), we know

$$
\left(\widehat {\boldsymbol {\beta} - \boldsymbol {\beta}} _ {(0)}\right) = \left(n ^ {- 1} \sum_ {t = p + 1} ^ {n} z _ {t} \left(\boldsymbol {\beta} _ {(0)}\right) \boldsymbol {z} _ {t} ^ {\prime} \left(\boldsymbol {\beta} _ {(0)}\right)\right) ^ {- 1} \left(n ^ {- 1} \sum_ {t = p + 1} ^ {n} z _ {t} \left(\boldsymbol {\beta} _ {(0)}\right) w _ {t} \left(\boldsymbol {\beta} _ {(0)}\right)\right) \tag {3.112}
$$

minimizes $Q ( \beta )$ . From (3.112), we write the one-step Gauss–Newton estimate as

$$
\boldsymbol {\beta} _ {(1)} = \boldsymbol {\beta} _ {(0)} + \Delta (\boldsymbol {\beta} _ {(0)}), \tag {3.113}
$$

where $\Delta ( \beta _ { ( 0 ) } )$ denotes the right-hand side of (3.112). Gauss–Newton estimation is accomplished by replacing $\beta _ { ( 0 ) }$ by $\beta _ { ( 1 ) }$ in (3.113). This process is repeated by calculating, at iteration $j = 2 , 3 , \dots$ ,

$$
\boldsymbol {\beta} _ {(j)} = \boldsymbol {\beta} _ {(j - 1)} + \Delta (\boldsymbol {\beta} _ {(j - 1)})
$$

until convergence.

# Example 3.30 Gauss–Newton for an MA(1)

Consider an invertible MA(1) process, $x _ { t } = w _ { t } + \theta w _ { t - 1 }$ . Write the truncated errors as

$$
w _ {t} (\theta) = x _ {t} - \theta w _ {t - 1} (\theta), \quad t = 1, \dots , n, \tag {3.114}
$$

where we condition on $w _ { 0 } ( \theta ) = 0$ . Taking derivatives,

$$
- \frac {\partial w _ {t} (\theta)}{\partial \theta} = w _ {t - 1} (\theta) + \theta \frac {\partial w _ {t - 1} (\theta)}{\partial \theta}, \quad t = 1, \dots , n, \tag {3.115}
$$

where $\partial w _ { 0 } ( \theta ) / \partial \theta = 0$ . Using the notation of (3.110), we can also write (3.115) as

$$
z _ {t} (\theta) = w _ {t - 1} (\theta) - \theta z _ {t - 1} (\theta), \quad t = 1, \dots , n, \tag {3.116}
$$

where $z _ { 0 } ( \theta ) = 0$ .

Let $\theta _ { ( 0 ) }$ be an initial estimate of $\theta$ , for example, the estimate given in Example 3.27. Then, the Gauss–Newton procedure for conditional least squares is given by

$$
\theta_ {(j + 1)} = \theta_ {(j)} + \frac {\sum_ {t = 1} ^ {n} z _ {t} \left(\theta_ {(j)}\right) w _ {t} \left(\theta_ {(j)}\right)}{\sum_ {t = 1} ^ {n} z _ {t} ^ {2} \left(\theta_ {(j)}\right)}, \quad j = 0, 1, 2, \dots , \tag {3.117}
$$

where the values in (3.117) are calculated recursively using (3.114) and (3.116). The calculations are stopped when $| \theta _ { ( j + 1 ) } - \theta _ { ( j ) } |$ , or $| Q ( \theta _ { ( j + 1 ) } ) -$ $Q ( \theta _ { ( j ) } ) |$ , are smaller than some preset amount.

# Example 3.31 Fitting the Glacial Varve Series

Consider the series of glacial varve thicknesses from Massachusetts for $n = 6 3 4$ years, as analyzed in Example 2.5 and in Problem 1.8, where it was argued that a first-order moving average model might fit the logarithmically transformed and differenced varve series, say,

$$
\nabla [ \ln (x _ {t}) ] = \ln (x _ {t}) - \ln (x _ {t - 1}) = \ln \left(\frac {x _ {t}}{x _ {t - 1}}\right),
$$

which can be interpreted as being proportional to the percentage change in the thickness.

The sample ACF and PACF, shown in Figure 3.7, confirm the tendency of $\nabla [ \ln ( x _ { t } ) ]$ to behave as a first-order moving average process as the ACF has only a significant peak at lag one and the PACF decreases exponentially. Using Table 3.1, this sample behavior fits that of the MA(1) very well.

![](images/eb29f7fbc9e5252266a8060e72254d467608f4221fa74d5fd638147ab27935c7.jpg)

![](images/9657f69419e636dd481f143a4fe3d79bcc70f426bfcb75a8f865bade0c899514.jpg)  
Figure 3.7 ACF and PACF of transformed glacial varves.

Nine iterations of the Gauss–Newton procedure, (3.117), starting with $\widehat { \theta _ { 0 } } = - . 1$ yielded the values

$$
- . 4 4 2, - . 6 2 4, - . 7 1 7, - . 7 5 0, - . 7 6 3, - . 7 6 8, - . 7 7 1, - . 7 7 2, - . 7 7 2
$$

for $\theta _ { ( 1 ) } , \ldots , \theta _ { ( 9 ) }$ , and a final estimated error variance $\widehat { \sigma } _ { w } ^ { 2 } = . 2 3 6$ . Using the final value of $\widehat { \theta } = \theta _ { ( 9 ) } = - . 7 7 2$ and the vectors $z _ { t }$ of partial derivatives in (3.116) leads to a standard error of .025 and a $t$ -value of −.772/.025 = $- 3 0 . 8 8$ with 632 degrees of freedom (one is lost in differencing).

In the general case of causal and invertible ARMA $( p , q )$ models, maximum likelihood estimation and conditional and unconditional least squares estimation (and Yule–Walker estimation in the case of AR models) all lead to optimal estimators. The proof of this general result can be found in a number of texts on theoretical time series analysis (for example, Brockwell and Davis, 1991, or Hannan, 1970, to mention a few). We will denote the ARMA coefficient parameters by $\beta = ( \phi _ { 1 } , \ldots , \phi _ { p } , \theta _ { 1 } , \ldots , \theta _ { q } ) ^ { \prime }$ .

# Property P3.9: Large Sample Distribution of the Estimators

Under appropriate conditions, for causal and invertible ARMA processes, the maximum likelihood, the unconditional least squares, and the conditional least squares estimators, each initialized by the method of moments estimator, all provide optimal estimators of $\sigma _ { w } ^ { 2 }$ and $\beta$ , in the sense that $\widehat { \sigma } _ { w } ^ { 2 }$ is consistent, and the asymptotic distribution of $\widehat { \beta }$ is the best asymptotic normal distribution. In particular, as $n \to \infty$ ,

$$
\sqrt {n} (\widehat {\boldsymbol {\beta}} - \boldsymbol {\beta}) \xrightarrow {d} \mathrm {N} \left(\mathbf {0}, \sigma_ {w} ^ {2} \boldsymbol {\Gamma} _ {p, q} ^ {- 1}\right). \tag {3.118}
$$

In (3.118), the variance–covariance matrix of the estimator $\widehat { \beta }$ is the inverse of the information matrix. In this case, the $( p + q ) \times ( p + q )$ matrix $\Gamma _ { p , q }$ , has the form

$$
\Gamma_ {p, q} = \left( \begin{array}{c c} \Gamma_ {\phi \phi} & \Gamma_ {\phi \theta} \\ \Gamma_ {\theta \phi} & \Gamma_ {\theta \theta} \end{array} \right). \tag {3.119}
$$

The $p \times p$ matrix $\Gamma _ { \phi \phi }$ is given by (3.90), that is, the $_ { i j }$ -th element of $\Gamma _ { \phi \phi }$ , for $i , j = 1 , \dotsc , p$ , is $\gamma _ { x } ( i - j )$ from an $\operatorname { A R } ( p )$ process, $\phi ( B ) x _ { t } = w _ { t }$ . Similarly, $\Gamma _ { \theta \theta }$ is a $q \times q$ matrix with the $i j$ -th element, for $i , j = 1 , \ldots , q$ , equal to $\gamma _ { y } ( i - j )$ from an $\operatorname { A R } ( q )$ process, $\theta ( B ) y _ { t } = w _ { t }$ . The $p \times q$ matrix $\Gamma _ { \phi \theta } = \{ \gamma _ { x y } ( i - j ) \}$ , for $i = 1 , \ldots , p$ ; $j = 1 , \dotsc , q$ ; that is, the $_ { i j }$ -th element is the cross-covariance between the two AR processes given by $\phi ( B ) x _ { t } = w _ { t }$ and $\theta ( B ) y _ { t } = w _ { t }$ . Finally, $\Gamma _ { \theta \phi } = \Gamma _ { \phi \theta } ^ { \prime }$ is $q \times p$ . Further discussion of Property P3.9, including a proof for the case of least squares estimators for AR( $p$ ) processes, can be found in Appendix B, $\ S$ B.3.

# Example 3.32 Some Specific Asymptotic Distributions

The following are some specific cases of Property P3.9.

AR(1): $\gamma _ { x } ( 0 ) = \sigma _ { w } ^ { 2 } / ( 1 - \phi ^ { 2 } )$ , so $\sigma _ { w } ^ { 2 } \Gamma _ { 1 , 0 } ^ { - 1 } = ( 1 - \phi ^ { 2 } )$ . Thus,

$$
\widehat {\phi} \sim \mathrm {A N} [ \phi , n ^ {- 1} (1 - \phi^ {2}) ]. \tag {3.120}
$$

AR(2): The reader can verify that

$$
\gamma_ {x} (0) = \left(\frac {1 - \phi_ {2}}{1 + \phi_ {2}}\right) \frac {\sigma_ {w} ^ {2}}{(1 - \phi_ {2}) ^ {2} - \phi_ {1} ^ {2}}
$$

and $\gamma _ { x } ( 1 ) = \phi _ { 1 } \gamma _ { x } ( 0 ) + \phi _ { 2 } \gamma _ { x } ( 1 )$ . From these facts, we can compute $\Gamma _ { 2 , 0 } ^ { - 1 }$

$$
\binom {\widehat {\phi} _ {1}} {\phi_ {2}} \sim \mathrm {A N} \left[ \binom {\phi_ {1}} {\phi_ {2}}, n ^ {- 1} \left( \begin{array}{c c} 1 - \phi_ {2} ^ {2} & - \phi_ {1} (1 + \phi_ {2}) \\ \text {s y m} & 1 - \phi_ {2} ^ {2} \end{array} \right) \right]. \tag {3.121}
$$

MA(1): In this case, write $\theta ( B ) y _ { t } = w _ { t }$ , or $y _ { t } + \theta y _ { t - 1 } = w _ { t }$ . Then, analogous to the AR(1) case, $\gamma _ { y } ( 0 ) = \sigma _ { w } ^ { 2 } / ( 1 - \theta ^ { 2 } )$ , so $\sigma _ { w } ^ { 2 } \Gamma _ { 0 , 1 } ^ { - 1 } = ( 1 - \theta ^ { 2 } )$ Thus,

$$
\widehat {\theta} \sim \mathrm {A N} [ \theta , n ^ {- 1} (1 - \theta^ {2}) ]. \tag {3.122}
$$

MA(2): Write $y _ { t } + \theta _ { 1 } y _ { t - 1 } + \theta _ { 2 } y _ { t - 2 } = w _ { t }$ , so , analogous to the AR(2) case, we have

$$
\left( \begin{array}{c} \widehat {\theta} _ {1} \\ \widehat {\theta} _ {2} \end{array} \right) \sim \mathrm {A N} \left[ \left( \begin{array}{c} \theta_ {1} \\ \theta_ {2} \end{array} \right), n ^ {- 1} \left( \begin{array}{c c} 1 - \theta_ {2} ^ {2} & \theta_ {1} (1 + \theta_ {2}) \\ \text {s y m} & 1 - \theta_ {2} ^ {2} \end{array} \right) \right]. \tag {3.123}
$$

# 3.6: Estimation

ARMA(1,1): To calculate $\Gamma _ { \phi \theta }$ , we must find $\gamma _ { x y } ( 0 )$ , where $x _ { t } - \phi x _ { t - 1 } =$ $w _ { t }$ and $y _ { t } + \theta y _ { t - 1 } = w _ { t }$ . We have

$$
\begin{array}{l} \gamma_ {x y} (0) = \operatorname {c o v} \left(x _ {t}, y _ {t}\right) = \operatorname {c o v} \left(\phi x _ {t - 1} + w _ {t}, - \theta y _ {t - 1} + w _ {t}\right) \\ = - \phi \theta \gamma_ {x y} (0) + \sigma_ {w} ^ {2}. \\ \end{array}
$$

Solving, we find, $\gamma _ { x y } ( 0 ) = \sigma _ { w } ^ { 2 } / ( 1 + \phi \theta )$ . Thus,

$$
\left( \begin{array}{c} \widehat {\phi} \\ \widehat {\theta} \end{array} \right) \sim \mathrm {A N} \left[ \left( \begin{array}{c} \phi \\ \theta \end{array} \right), n ^ {- 1} \left[ \begin{array}{c c} (1 - \phi^ {2}) ^ {- 1} & (1 + \phi \theta) ^ {- 1} \\ \text {s y m} & (1 - \theta^ {2}) ^ {- 1} \end{array} \right] ^ {- 1} \right]. \tag {3.124}
$$

The reader might wonder, for example, why the asymptotic distributions of $\widehat { \phi }$ from an AR(1) [equation (3.120)] and $\widehat { \theta }$ from an MA(1) [equation (3.122)] are of the same form. It is possible to explain this unexpected result heuristically using the intuition of linear regression. That is, for the normal regression model presented in §2.2 with no intercept term, $x _ { t } = \beta z _ { t } + w _ { t }$ , we know $\widehat { \beta }$ i s normally distributed with mean $\beta$ , and from (2.8),

$$
\mathrm {v a r} \left\{\sqrt {n} \left(\widehat {\beta} - \beta\right) \right\} = n \sigma_ {w} ^ {2} \left(\sum_ {t = 1} ^ {n} z _ {t} ^ {2}\right) ^ {- 1} = \sigma_ {w} ^ {2} \left(n ^ {- 1} \sum_ {t = 1} ^ {n} z _ {t} ^ {2}\right) ^ {- 1}.
$$

For the causal AR(1) model given by $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ , the intuition of regression tells us to expect that, for $n$ large,

$$
\sqrt {n} \left(\widehat {\phi} - \phi\right)
$$

is approximately normal with mean zero and with variance given by

$$
\sigma_ {w} ^ {2} \left(n ^ {- 1} \sum_ {t = 2} ^ {n} x _ {t - 1} ^ {2}\right) ^ {- 1}.
$$

Now, $n ^ { - 1 } \textstyle \sum _ { t = 2 } ^ { n } x _ { t - 1 } ^ { 2 }$ is the sample variance (recall that the mean of $x _ { t }$ is zero) of the $x _ { t }$ , so as $n$ becomes large we would expect it to approach $\mathrm { v a r } ( x _ { t } ) =$ $\gamma ( 0 ) = \sigma _ { w } ^ { 2 } / ( 1 - \phi ^ { 2 } )$ . Thus, the large sample variance of $\sqrt { n } \left( { \widehat { \phi } } - \phi \right)$ is

$$
\sigma_ {w} ^ {2} \gamma_ {x} (0) ^ {- 1} = \sigma_ {w} ^ {2} \left(\frac {\sigma_ {w} ^ {2}}{1 - \phi^ {2}}\right) ^ {- 1} = (1 - \phi^ {2});
$$

that is, (3.120) holds.

In the case of an MA(1), we may use the discussion of Example 3.30 to write an approximate regression model for the MA(1). That is, consider the approximation (3.116) as the regression model

$$
z _ {t} (\widehat {\theta}) = - \theta z _ {t - 1} (\widehat {\theta}) + w _ {t - 1},
$$

where now, $z _ { t - 1 } ( \widehat { \theta } )$ as defined in Example 3.30, plays the role of the regressor. Continuing with the analogy, we would expect the asymptotic distribution of ${ \sqrt { n } } \left( { \widehat { \theta } } - \theta \right)$ to be normal, with mean zero, and approximate variance

$$
\sigma_ {w} ^ {2} \left(n ^ {- 1} \sum_ {t = 2} ^ {n} z _ {t - 1} ^ {2} (\widehat {\theta})\right) ^ {- 1}.
$$

As in the AR(1) case, $\scriptstyle n ^ { - 1 } \sum _ { t = 2 } ^ { n } z _ { t - 1 } ^ { 2 } ( { \widehat { \theta } } )$ is the sample variance of the $z _ { t } ( \widehat { \theta } )$ so, for large $n$ , this should be $\operatorname { v a r } \{ z _ { t } ( \theta ) \} = \gamma _ { z } ( 0 )$ , say. But note, as seen from (3.116), $z _ { t } ( \theta )$ is approximately an AR(1) process with parameter $- \theta$ . Thus,

$$
\sigma_ {w} ^ {2} \gamma_ {z} (0) ^ {- 1} = \sigma_ {w} ^ {2} \left(\frac {\sigma_ {w} ^ {2}}{1 - (- \theta) ^ {2}}\right) ^ {- 1} = (1 - \theta^ {2}),
$$

which agrees with (3.122). Finally, the asymptotic distributions of the AR parameters estimates and the MA parameter estimates are of the same form because in the MA case, the “regressors” are the differential processes $z _ { t } ( \theta )$ that have AR structure, and it is this structure that determines the asymptotic variance of the estimators. For a rigorous account of this approach for the general case, see Fuller (1995, Theorem 5.5.4).

In Example 3.31, the estimated standard error of $\widehat { \theta }$ was .025. In the example, this value was calculated as the square root of

$$
s _ {w} ^ {2} \left(n ^ {- 1} \sum_ {t = 2} ^ {n} z _ {t - 1} ^ {2} (\widehat {\theta})\right) ^ {- 1},
$$

where $n = 6 3 3$ , $s _ { w } ^ { 2 } = . 2 3 6$ , and $\widehat \theta = - . 7 7 2$ . Using (3.122), we could have also calculated this value using the asymptotic approximation, the square root of $( 1 - . 7 7 2 ^ { 2 } ) / 6 3 3$ , which is also .025.

The asymptotic behavior of the parameter estimators gives us an additional insight into the problem of fitting ARMA models to data. For example, suppose a time series follows an AR(1) process and we decide to fit an AR(2) to the data. Does any problem occur in doing this? More generally, why not simply fit large-order AR models to make sure that we capture the dynamics of the process? After all, if the process is truly an AR(1), the other autoregressive parameters will not be significant. The answer is that if we overfit, we will lose efficiency. For example, if we fit an AR(1) to an AR(1) process, for large $n$ , $\mathrm { v a r } ( { \widehat { \phi } } _ { 1 } ) \approx n ^ { - 1 } ( 1 - \phi _ { 1 } ^ { 2 } )$ . But if we fit an AR(2) to the AR(1) process, for large $n$ , $\mathrm { v a r } ( \hat { \phi _ { 1 } } ) \approx n ^ { - 1 } ( 1 - \phi _ { 2 } ^ { 2 } ) = n ^ { - 1 }$ because $\phi _ { 2 } = 0$ . Thus, the variance of $\phi _ { 1 }$ has been inflated, making the estimator less precise. We do want to mention that overfitting can be used as a diagnostic tool. For example, if we fit an AR(2) model to the data and are satisfied with that model, then adding one more parameter and fitting an AR(3) should lead to approximately the same model as in the AR(2) fit. We will discuss model diagnostics in more detail in §3.8.

![](images/935ca72192f2d962dba5df426158e217f175ef0b10323c8fcea353c849b0811a.jpg)  
Figure 3.8 One hundred observations generated from the AR(1) model in Example 3.33.

If $n$ is small, or if the parameters are close to the boundaries, the asymptotic approximations can be quite poor. The bootstrap can be helpful in this case; for a broad treatment of the bootstrap, see Efron and Tibshirani (1994). We discuss the case of an AR(1) here and leave the general discussion for Chapter 6. For now, we give a simple example of the bootstrap for an AR(1) process.

# Example 3.33 Bootstrapping an AR(1)

We consider an AR(1) model with a regression coefficient near the boundary of causality and an error process that is symmetric but not normal. Specifically, consider the stationary and causal model

$$
x _ {t} = \mu + \phi \left(x _ {t - 1} - \mu\right) + w _ {t}, \tag {3.125}
$$

where $\mu = 5 0$ , $\phi = . 9 5$ , and $w _ { t }$ are iid double exponential with location zero, and scale parameter $\beta = 2$ . The density of $w _ { t }$ is given by

$$
f _ {w _ {t}} (w) = \frac {1}{2 \beta} \exp \left\{- | w | / \beta \right\} - \infty <   w <   \infty .
$$

In this example, $E ( w _ { t } ) = 0$ and $\mathrm { v a r } ( w _ { t } ) = 2 \beta ^ { 2 } = 8$ . Figure 3.8 shows $n =$ 100 simulated observations from this process. This particular realization is interesting; the data look like they were generated from a nonstationary process with three different mean levels. In fact, the data were generated from a well-behaved, albeit non-normal, stationary and causal model. To show the advantages of the bootstrap, we will act as if we do not know the actual error distribution and we will proceed as if it were normal; of

![](images/bad4d08b71389d616f6430a58ab17523e29704458e960a9c6cd408b08d1f4fa0.jpg)  
Figure 3.9 Finite sample density of the Yule–Walker estimate of $\phi$ in Example 3.33.

course, this means, for example, that the normal based MLE of $\phi$ will not be the actual MLE because the data are not normal.

Using the data shown in Figure 3.8, we obtained the Yule–Walker estimates $\widehat { \mu } = 4 0 . 0 4 8$ , $\widehat { \phi } = . 9 5 7$ , and $s _ { w } ^ { 2 } = 1 5 . 3 0 2$ , where $s _ { w } ^ { 2 }$ is the estimate of $\mathrm { v a r } ( w _ { t } )$ . Based on Property P3.9, we would say that $\widehat { \phi }$ is approximately normal with mean $\phi$ (which we supposedly do not know) and variance $( 1 - \phi ^ { 2 } ) / 1 0 0$ , which we would approximate by $( 1 - . 9 5 7 ^ { 2 } ) / 1 0 0 = . 0 2 9 ^ { 2 }$ .

To assess the finite sample distribution of $\widehat { \phi }$ when $n = 1 0 0$ , we simulated 1000 realizations of this AR(1) process and estimated the parameters via Yule–Walker. The finite sampling density of the Yule–Walker estimate of $\phi$ , based on the 1000 repeated simulations, is shown in Figure 3.9. Clearly the sampling distribution is not close to normality for this sample size. The mean of the distribution shown in Figure 3.9 is .907, and the variance of the distribution is . $0 5 2 ^ { 2 }$ ; these values are considerably different than the asymptotic values. Some of the quantiles of the finite sample distribution are .81 (5%), .84 (10%), .88 (25%), .92 (50%), .95 (75%), .96 (90%), and .97 (95%).

Before discussing the bootstrap, we first investigate the sample innovation process, $x _ { t } - x _ { t } ^ { t - 1 }$ , with corresponding variances $P _ { t } ^ { t - 1 }$ . For the AR(1) model in this example,

$$
x _ {t} ^ {t - 1} = \mu + \phi (x _ {t - 1} - \mu), \qquad t = 2, \dots , 1 0 0.
$$

# 3.6: Estimation

From this, it follows that

$$
P _ {t} ^ {t - 1} = E \left(x _ {t} - x _ {t} ^ {t - 1}\right) ^ {2} = \sigma_ {w} ^ {2}, \quad t = 2, \dots , 1 0 0.
$$

When $t = 1$ , we have

$$
x _ {1} ^ {0} = \mu \quad \text {a n d} \quad P _ {1} ^ {0} = \sigma_ {w} ^ {2} / (1 - \phi^ {2}).
$$

Thus, the innovations have zero mean but different variances; in order that all of the innovations have the same variance, $\sigma _ { w } ^ { 2 }$ , we will write them as

$$
\begin{array}{l} \epsilon_ {1} = (x _ {1} - \mu) \sqrt {(1 - \phi^ {2})} \\ \epsilon_ {t} = (x _ {t} - \mu) - \phi (x _ {t - 1} - \mu), \quad \text {f o r} \quad t = 2, \dots , 1 0 0. \tag {3.126} \\ \end{array}
$$

From these equations, we can write the model in terms of the innovations $\epsilon _ { t }$ as

$$
\begin{array}{l} x _ {1} = \mu + \epsilon_ {1} / \sqrt {\left(1 - \phi^ {2}\right)} \\ x _ {t} = \mu + \phi \left(x _ {t - 1} - \mu\right) + \epsilon_ {t} \quad \text {f o r} \quad t = 2, \dots , 1 0 0. \tag {3.127} \\ \end{array}
$$

Next, replace the parameters with their estimates in (3.126), that is, $n = 1 0 0$ , $\widehat { \mu } = 4 0 . 0 4 8$ , and $\widehat { \phi } \ : = \ : . 9 5 7$ , and denote the resulting sample innovations as $\{ \widehat { \epsilon } _ { 1 } , \ldots , \widehat { \epsilon } _ { 1 0 0 } \}$ . To obtain one bootstrap sample, first randomly sample, with replacement, $n = 1 0 0$ values from the set of sample innovations; call the sampled values $\{ \epsilon _ { 1 } ^ { * } , \hdots , \epsilon _ { 1 0 0 } ^ { * } \}$ . Now, generate a bootstrapped data set sequentially by setting

$$
\begin{array}{l} x _ {1} ^ {*} = 4 0. 0 4 8 + \epsilon_ {1} ^ {*} / \sqrt {\left(1 . - 9 5 7 ^ {2}\right)} \\ x _ {t} ^ {*} = 4 0. 0 4 8 + . 9 5 7 \left(x _ {t - 1} ^ {*} - 4 0. 0 4 8\right) + \epsilon_ {t} ^ {*}, \quad t = 2, \dots , n. \tag {3.128} \\ \end{array}
$$

Next, estimate the parameters as if the data were $\boldsymbol { x } _ { t } ^ { * }$ . Call these estimates $\widehat { \mu } ( 1 )$ , $\widehat { \phi ( 1 ) }$ , and $s _ { w } ^ { 2 } ( 1 )$ . Repeat this process a large number, $B$ , of times, generating a collection of bootstrapped parameter estimates, $\{ \widehat { \mu } ( b ) , \widehat { \phi } ( b ) , s _ { w } ^ { 2 } ( b ) , b \ = \ 1 , \ldots , B \}$ . We can then approximate the finite sample distribution of an estimator from the bootstrapped parameter values. For example, we can approximate the distribution of ${ \widehat { \phi } } - \phi$ by the empirical distribution of ${ \widehat { \phi } } ( b ) - { \widehat { \phi } }$ , for $b = 1 , \ldots , B$ .

Figure 3.10 shows the bootstrap histogram of 200 bootstrapped estimates of $\phi$ using the data shown in Figure 3.8. In particular, the mean of the distribution of $\widehat { \phi } ( b )$ is .918 with a variance of . $0 4 6 ^ { 2 }$ . Some quantiles of this distribution are .83 (5%), .85 (10%), .90 (25%), .93 (50%), .95 (75%), .97 (90%), and .98 (95%). Clearly, the bootstrap distribution of $\widehat { \phi }$ is closer to the distribution of $\widehat { \phi }$ shown in Figure 3.9 than to the asymptotic (normal) approximation.

To perform a similar bootstrap exercise in R, use the following commands. We note that the R estimation procedure is conditional on the first observation, so the first residual is not returned. To get around this problem, we simply fix the first observation and bootstrap the remaining data. The simulated data are available in the file ar1boot.dat.7

$>$ x = scan("/mydata/ar1boot.dat") $>$ m = mean(x) # estimate of mu $>$ fit = ar.yw(x, order=1) $>$ phi = fit\$ar # estimate of phi $>$ nboot = 200 # number of bootstrap replicates $>$ resids = fit\$resid $>$ resids = resids[2:100] # the first resid is NA $>$ x_star = x # initialize x_star $>$ phi_star = matrix(0, nboot, 1) $>$ for (i in 1:nboot) {  
+ resid_star = sample(resids)  
+ for (t in 1:99) {  
+ x_star[t+1] = m + phi*(x_star[t]-m) + resid_star[t]  
+ }  
+ phi_star[i] = ar.yw(x_star, order=1) $\$ ar  
+ }

Now, 200 bootstrapped estimates are available in phi.star, and various methods can be used to evaluate the estimates. For example, to obtain a histogram of the estimates, hist(phi.star) can be used . Also consider the statistics mean(phi.star), sd(phi.star), for the mean and standard deviation, and quantile(phi.star, probs $=$ seq(0, 1, .25)) for some quantiles. Other interesting graphics are boxplot(phi.star) for a boxplot and stem(phi.star) for a stem-and-leaf diagram.

# 3.7 Integrated Models for Nonstationary Data

In Chapters 1 and 2, we saw that if $x _ { t }$ is a random walk, $x _ { t } = x _ { t - 1 } + w _ { t }$ , then by differencing $x _ { t }$ , we find that $\nabla x _ { t } = w _ { t }$ is stationary. In many situations, time series can be thought of as being composed of two components, a nonstationary trend component and a zero-mean stationary component. For example, in §2.2 we considered the model

$$
x _ {t} = \mu_ {t} + y _ {t}, \tag {3.129}
$$

# 3.7: Integrated Models

![](images/0ef5399c28427bc24fdbda54c84a679320cabe8c0de72ca0f1ed8cd6808a69e5.jpg)  
Figure 3.10 Bootstrap histogram of $\widehat { \phi }$ based on 200 bootstraps.

where $\mu _ { t } = \beta _ { 0 } + \beta _ { 1 } t$ and $y _ { t }$ is stationary. Differencing such a process will lead to a stationary process:

$$
\nabla x _ {t} = x _ {t} - x _ {t - 1} = \beta_ {1} + y _ {t} - y _ {t - 1} = \beta_ {1} + \nabla y _ {t}.
$$

Another model that leads to first differencing is the case in which $\mu _ { t }$ in (3.129) is stochastic and slowly varying according to a random walk. That is, in (3.129)

$$
\mu_ {t} = \mu_ {t - 1} + v _ {t}
$$

where $v _ { t }$ is stationary. In this case,

$$
\nabla x _ {t} = v _ {t} + \nabla y _ {t},
$$

is stationary. If $\mu _ { t }$ in (3.129) is a $k$ -th order polynomial, $\begin{array} { r } { \mu _ { t } = \sum _ { j = 0 } ^ { k } \beta _ { j } t ^ { j } } \end{array}$ , then (Problem 3.26) the differenced series $\nabla ^ { k } y _ { t }$ is stationary. Stochastic trend models can also lead to higher order differencing. For example, suppose in (3.129)

$$
\mu_ {t} = \mu_ {t - 1} + v _ {t} \quad \text {a n d} \quad v _ {t} = v _ {t - 1} + e _ {t},
$$

where $e _ { t }$ is stationary. Then, $\nabla x _ { t } = v _ { t } + \nabla y _ { t }$ is not stationary, but

$$
\nabla^ {2} x _ {t} = e _ {t} + \nabla^ {2} y _ {t}
$$

is stationary.

The integrated ARMA, or ARIMA model, is a broadening of the class of ARMA models to include differencing.

Definition 3.11 A process, $x _ { t }$ is said to be ARIMA $( p , d , q )$ i f

$$
\nabla^ {d} x _ {t} = (1 - B) ^ {d} x _ {t}
$$

is ARMA(p, q). In general, we will write the model as

$$
\phi (B) (1 - B) ^ {d} x _ {t} = \theta (B) w _ {t}. \tag {3.130}
$$

If $E ( \nabla ^ { d } x _ { t } ) = \mu$ , we write the model as

$$
\phi (B) (1 - B) ^ {d} x _ {t} = \alpha + \theta (B) w _ {t},
$$

where $\alpha = \mu ( 1 - \phi _ { 1 } - \cdot \cdot \cdot - \phi _ { p } )$ .

# Example 3.34 IMA $( 1 , 1 )$ and EWMA

The ARIMA(0,1,1), or IMA(1,1) model is of interest because many economic time series can be successfully modeled this way. In addition, the model leads to a frequently used, and abused, forecasting method called exponentially weighted moving averages (EWMA). We will write the model as

$$
x _ {t} = x _ {t - 1} + w _ {t} - \lambda w _ {t - 1} \tag {3.131}
$$

because this model formulation is easier to work with here, and it leads to the standard representation for EWMA. When $| \lambda | < 1$ , the model has an invertible representation,

$$
x _ {t} = \sum_ {j = 1} ^ {\infty} (1 - \lambda) \lambda^ {j - 1} x _ {t - j} + w _ {t}. \tag {3.132}
$$

Verification of (3.132) is left to the reader (Problem 3.27). From (3.132), we have that the one-step-ahead prediction, using the notation of §3.5, is

$$
\begin{array}{l} \tilde {x} _ {n + 1} = \sum_ {j = 1} ^ {\infty} (1 - \lambda) \lambda^ {j - 1} x _ {n + 1 - j} \\ = (1 - \lambda) x _ {n} + \lambda \sum_ {j = 1} ^ {\infty} (1 - \lambda) \lambda^ {j - 1} x _ {n - j} \\ = (1 - \lambda) x _ {n} + \lambda \tilde {x} _ {n}. \tag {3.133} \\ \end{array}
$$

Based on (3.133), the truncated forecasts are obtained by setting $\tilde { x } _ { 1 } ^ { 0 } = 0$ , and then updating as follows:

$$
\tilde {x} _ {n + 1} ^ {n} = (1 - \lambda) x _ {n} + \lambda \tilde {x} _ {n} ^ {n - 1}, \quad n \geq 1. \tag {3.134}
$$

From (3.134), we see that the new forecast is a linear combination of the old forecast and the new observation. In EWMA, the parameter $\lambda$ is called the smoothing constant and is restricted to be between zero and one. Larger values of $\lambda$ lead to smoother forecasts. This method of forecasting is popular because it is easy to use; we need only retain the previous forecast value and the current observation to forecast the next time period. Unfortunately, as previously suggested, the method is often abused because some forecasters do not verify that the observations follow an $\mathrm { I M A } ( 1 , 1 )$ process, and often arbitrarily pick values of $\lambda$ .

Finally, the model for the glacial varve series in Example 3.31 is an $\mathrm { I M A } ( 1 , 1 )$ on the logarithms of the data. Recall that the fitted model there was $\ln x _ { t } = \ln x _ { t - 1 } + w _ { t } - . 7 7 2 w _ { t - 1 }$ and $\mathrm { v a r } ( w _ { t } ) = . 2 3 6$ .

# 3.8 Building ARIMA Models

There are a few basic steps to fitting ARIMA models to time series data. These steps involve plotting the data, possibly transforming the data, identifying the dependence orders of the model, parameter estimation, diagnostics, and model choice. First, as with any data analysis, we should construct a time plot of the data, and inspect the graph for any anomalies. If, for example, the variability in the data grows with time, it will be necessary to transform the data to stabilize the variance. In such cases, the Box–Cox class of power transformations, equation (2.34), could be employed. Also, the particular application might suggest an appropriate transformation. For example, suppose a process evolves as a fairly small and stable percent change, such as an investment. For example, we might have

$$
x _ {t} = (1 + p _ {t}) x _ {t - 1},
$$

where $x _ { t }$ is the value of the investment at time $t$ and $p _ { t }$ is the percentage change from period $t - 1$ to $t$ , which may be negative. Taking logs we have

$$
\ln (x _ {t}) = \ln (1 + p _ {t}) + \ln (x _ {t - 1}),
$$

or

$$
\nabla [ \ln (x _ {t}) ] = \ln (1 + p _ {t}).
$$

If the percent change $p _ { t }$ stays relatively small in magnitude, then $\ln ( 1 + p _ { t } ) \approx p _ { t }$ and, thus,

$$
\nabla [ \ln (x _ {t}) ] \approx p _ {t},
$$

will be a relatively stable process. Frequently, $\nabla [ \ln ( x _ { t } ) ]$ is called the return or growth rate. This general idea was used in Example 3.31, and we will use it again in Example 3.35.

After suitably transforming the data, the next step is to identify preliminary values of the autoregressive order, $p$ , the order of differencing, $d$ , and the

![](images/389ab326dc70837d5b5cc5c7b0973537a2b98d3f360d6a4127ea4b0c0e4860e6.jpg)  
Figure 3.11 Quarterly U.S. GNP from 1947(1) to 2002(3).

moving average order, $q$ . We have already addressed, in part, the problem of selecting $d$ . A time plot of the data will typically suggest whether any differencing is needed. If differencing is called for, then difference the data once, $d = 1$ , and inspect the time plot of $\nabla x _ { t }$ . If additional differencing is necessary, then try differencing again and inspect a time plot of $\nabla ^ { 2 } \boldsymbol { x } _ { t }$ . Be careful not to overdifference because this may introduce dependence where none exists. For example, $x _ { t } = w _ { t }$ is serially uncorrelated, but $\nabla x _ { t } = w _ { t } - w _ { t - 1 }$ is MA(1). In addition to time plots, the sample ACF can help in indicating whether differencing is needed. Because the polynomial $\phi ( z ) ( 1 - z ) ^ { d }$ has a unit root, the sample ACF, $\widehat { \rho } ( h )$ , will not decay to zero fast as $h$ increases. Thus, a slow decay in ${ \widehat { \rho } } ( h )$ is an indication that differencing may be needed.

When preliminary values of $d$ have been settled, the next step is to look at the sample ACF and PACF of $\nabla ^ { d } x _ { t }$ for whatever values of $d$ have been chosen. Using Table 3.1 as a guide, preliminary values of $p$ and $q$ are chosen. Recall that, if $p = 0$ and $q > 0$ , the ACF cuts off after lag $q$ , and the PACF tails off. If $q = 0$ and $p > 0$ , the PACF cuts off after lag $p$ , and the ACF tails off. If $p > 0$ and $q > 0$ , both the ACF and PACF will tail off. Because we are dealing with estimates, it will not always be clear whether the sample ACF or PACF is tailing off or cutting off. Also, two models that are seemingly different can actually be very similar. With this in mind, we should not worry about being so precise at this stage of the model fitting. At this stage, a few preliminary values of $p$ , $d$ , and $q$ should be at hand, and we can start estimating the parameters.

# Example 3.35 Analysis of GNP Data

In this example, we consider the analysis of quarterly U.S. GNP from 1947(1) to 2002(3), $n = 2 2 3$ observations. The data are Real U.S. Gross National Product in billions of chained 1996 dollars and they have been seasonally adjusted. The data were obtained from the Federal Reserve

![](images/6da6c94bc94d2f5530ec82f4411d5c739fc70e524eeff7d148c1acee64200ce0.jpg)  
Figure 3.12 Sample ACF of the GNP data.

![](images/5890b847c37879313272cfc2e8bb88a8233aec056440c4275c17633ca23b602a.jpg)  
Figure 3.13 First difference of the U.S. GNP data.

Bank of St. Louis (http://research.stlouisfed.org/). Figure 3.11 shows a plot of the data, say, $y _ { t }$ . Because strong trend hides any other effect, it is not clear from Figure 3.11 that the variance is increasing with time. For the purpose of demonstration, the sample ACF of the data is displayed in Figure 3.12. Figure 3.13 shows the first difference of the data, $\nabla y _ { t }$ , and now that the trend has been removed we are able to notice that the variability in the second half of the data is larger than in the first half of the data. Also, it appears as though a trend is still present after differencing. The growth rate, say, $x _ { t } = \nabla \ln ( y _ { t } )$ , is plotted in Figure 3.14, and, appears to be a stable process. Moreover, we may interpret the values of $x _ { t }$ as the percentage quarterly growth of U.S. GNP.

The sample ACF and PACF of the quarterly growth rate are plotted in Figure 3.15. Inspecting the sample ACF and PACF, we might feel that the ACF is cutting off at lag 2 and the PACF is tailing off. This

![](images/bb001c6e994023207566e2fc49310c4b06ce6969da5cee75866a1bc4026d0e11.jpg)

![](images/29ac385f428aad58336316e51c483e7465e5a064b4cf9009d223e9c2b0c44d27.jpg)  
Figure 3.14 U.S. GNP quarterly growth rate.

![](images/6b451d1eddc6c7060a13964bc7a45bffeedf04321451cacf8251e0d54a8f8f75.jpg)  
Figure 3.15 Sample ACF and PACF of the GNP quarterly growth rate.

would suggest the GNP growth rate follows an MA(2) process, or log GNP follows an ARIMA $( 0 , 1 , 2 )$ model. Rather than focus on one model, we will also suggest that it appears that the ACF is tailing off and the PACF is cutting off at lag 1. This suggests an AR(1) model for the growth rate, or $\mathrm { A R I M A } ( 1 , 1 , 0 )$ for log GNP. As a preliminary analysis, we will fit both models.

Using MLE to fit the MA(2) model for the growth rate, $x _ { t }$ , the estimated

model is

$$
x _ {t} = . 0 0 8 _ {(. 0 0 1)} +. 3 0 3 _ {(. 0 6 5)} \widehat {w} _ {t - 1} +. 2 0 4 _ {(. 0 6 4)} \widehat {w} _ {t - 2} + \widehat {w} _ {t}, \tag {3.135}
$$

where $\widehat { \sigma } _ { w } = . 0 0 9 4$ is based on 219 degrees of freedom. The values in parentheses are the corresponding estimated standard errors. All of the regression coefficients are significant, including the constant. We make a special note of this because, as a default, some computer packages do not fit a constant in a differenced model. That is, these packages assume, by default, that there is no drift. In this example, not including a constant leads to the wrong conclusions about the nature of the U.S. economy. Not including a constant assumes the average quarterly growth rate is zero, whereas the U.S. GNP average quarterly growth rate is about 1% (which can be seen easily in Figure 3.14). We leave it to the reader to investigate what happens when the constant is not included.

The estimated AR(1) model is

$$
x _ {t} = . 0 0 5 \left(_ {\cdot 0 0 0 6}\right) +. 3 4 7 \left(_ {\cdot 0 6 3}\right) x _ {t - 1} + \widehat {w} _ {t}, \tag {3.136}
$$

where $\widehat { \sigma } _ { w } = . 0 0 9 5$ on 220 degrees of freedom.

We will discuss diagnostics next, but assuming both of these models fit well, how are we to reconcile the apparent differences of the estimated models (3.135) and (3.136)? In fact, the fitted models are nearly the same. To show this, consider an AR(1) model of the form in (3.136) without a constant term; that is,

$$
x _ {t} = . 3 5 x _ {t - 1} + w _ {t},
$$

and write it in its causal form, $\begin{array} { r } { x _ { t } = \sum _ { j = 0 } ^ { \infty } \psi _ { j } w _ { t - j } } \end{array}$ , where we recall $\psi _ { j } = $ . $3 5 ^ { j }$ . Thus, $\psi _ { 0 } = 1 , \psi _ { 1 } = . 3 5 0 , \psi _ { 2 } = . 1 2 3 , \psi _ { 3 } = . 0 4 3 , \psi _ { 4 } = . 0 1 5 , \psi _ { 5 } =$ . $0 0 5 , \psi _ { 6 } = . 0 0 2 , \psi _ { 7 } = . 0 0 1 , \psi _ { 8 } = 0 , \psi _ { 9 } = 0 , \psi _ { 1 0 } = 0$ , and so forth. Thus,

$$
x _ {t} \approx . 3 5 w _ {t - 1} +. 1 2 w _ {t - 2} + w _ {t},
$$

which is similar to the fitted MA(2) model in (3.136).

The analyses and graphics of the example can be performed in R using the following commands. We note that we did not fit integrated models to log GNP, but rather we fit nonintegrated models to the growth rate, $x _ { t }$ . We believe at the time of writing that there is a problem with fitting ARIMA models with a nonzero constant in R. The data are in a file called gnp96.dat; the file contains two columns, the first column is the quarter and the second column is the GNP.

> gnp96 = read.table("/mydata/gnp96.dat")  
> gnp = ts(gnp96[,2], start=1947, frequency=4)   
> plot(gnp)

```python
> acf(gnp, 50)
> gnpgr = diff(log(gnp)) # growth rate
> plot.ts(gnpgr)
> par(mfrow=c(2,1))
> acf(gnpgr, 24)
> pacf(gnpgr, 24)
> # ARIMA fits:
> gnpgr.ar = arima(gnpgr, order = c(1, 0, 0))
> gnpgr.ma = arima(gnpgr, order = c(0, 0, 2))
> # to view the results:
> gnpgr.ar # potential problem here (see below *)
> gnpgr.ma
> ARMAtoMA(ar=.35, ma=0, 10) # prints psi-weights 
```

∗At this time, the R output for the AR fit lists the estimated mean and its standard error, but calls it the intercept. That is, the output says it is giving you $\widehat { \alpha }$ when in fact it’s listing $\widehat { \mu }$ . In this case, $\widehat { \alpha } = \widehat { \mu } ( 1 - \widehat { \phi } )$ .

The next step in model fitting is diagnostics. This investigation includes the analysis of the residuals as well as model comparisons. Again, the first step involves a time plot of the innovations (or residuals), $x _ { t } - \widehat { x } _ { t } ^ { t - 1 }$ , or of the standardized innovations

$$
e _ {t} = \left(x _ {t} - \widehat {x} _ {t} ^ {t - 1}\right) / \sqrt {\widehat {P} _ {t} ^ {t - 1}}, \tag {3.137}
$$

where $\widehat { x } _ { t } ^ { t - 1 }$ is the one-step-ahead prediction of $x _ { t }$ based on the fitted model and $\widehat { P } _ { t } ^ { t - 1 }$ is the estimated one-step-ahead error variance. If the model fits well, the standardized residuals should behave as an iid sequence with mean zero and variance one. The time plot should be inspected for any obvious departures from this assumption. Unless the time series is Gaussian, it is not enough that the residuals are uncorrelated. For example, it is possible in the non-Gaussian case to have an uncorrelated process for which values contiguous in time are highly dependent. As an example, we mention the family of GARCH models that are discussed in Chapter 5.

Investigation of marginal normality can be accomplished visually by looking at a histogram of the residuals. In addition to this, a normal probability plot or a Q-Q plot can help in identifying departures from normality. See Johnson and Wichern (1992, Chapter 4) for details of this test as well as additional tests for multivariate normality.

There are several tests of randomness, for example the runs test, that could be applied to the residuals. We could also inspect the sample autocorrelations of the residuals, say, $\widehat { \rho } _ { e } ( h )$ , for any patterns or large values. Recall that, for a white noise sequence, the sample autocorrelations are approximately independently and normally distributed with zero means and variances $1 / n$ . Hence, a good check on the correlation structure of the residuals is to plot $\widehat { \rho } _ { e } ( h )$ versus $h$ along with the error bounds of $\pm 2 / \sqrt { n }$ . The residuals from a model fit,

![](images/5fe47001138492e0391e9ccca348464e57b5fc2c3ac46f94e539d3306497faaf.jpg)

![](images/2103d5815778c211cfa5d9ac973ef98ddd5aeda379623e06cbca815b8b723a52.jpg)

![](images/6c191b8185ec0314da017751a668cea5b3b21a63c4c8340ef38d2ca14117d93d.jpg)  
Figure 3.16 Diagnostics of the residuals from MA(2) fit on GNP growth rate.

however, will not quite have the properties of a white noise sequence and the variance of $\widehat { \rho } _ { e } ( h )$ can be much less than $1 / n$ . Details can be found in Box and Pierce (1970) and McLeod (1978). This part of the diagnostics can be viewed as a visual inspection of $\widehat { \rho } _ { e } ( h )$ with the main concern being the detection of obvious departures from the independence assumption.

In addition to plotting $\widehat { \rho } _ { e } ( h )$ , we can perform a general test that takes into consideration the magnitudes of $\widehat { \rho } _ { e } ( h )$ as a group. For example, it may be the case that, individually, each $\widehat { \rho } _ { e } ( h )$ is small in magnitude, say, each one is just slightly less that $2 / \sqrt { n }$ in magnitude, but, collectively, the values are large. The Ljung–Box–Pierce Q-statistic given by

$$
Q = n (n + 2) \sum_ {h = 1} ^ {H} \frac {\widehat {\rho} _ {e} ^ {2} (h)}{n - h} \tag {3.138}
$$

can be used to perform such a test. The value $H$ in (3.138) is chosen somewhat arbitrarily, typically, $H = 2 0$ . Under the null hypothesis of model adequacy, asymptotically ( $n  \infty$ ), $Q \sim \chi _ { H - p - q } ^ { 2 }$ . Thus, we would reject the null hypothesis at level $\alpha$ if the value of $Q$ − −  exceeds the $( 1 - \alpha )$ -quantile of the $\chi _ { H - p - q } ^ { 2 }$

![](images/7719e582e843a6a35686029e3b642720dde8e8dc3272361783ea3fd94d66a2f6.jpg)

![](images/6185d2a9e82239151b294f83c974fe8129dc095a9f8a19b3b31813a260f12551.jpg)  
Figure 3.17 Histogram of the residuals (top), and a normal Q-Q plot of the residuals (bottom).

distribution. Details can be found in Box and Pierce (1970), Ljung and Box (1978), and Davies et al. (1977).

# Example 3.36 Diagnostics for GNP Growth Rate Example

We will focus on the MA(2) fit from Example 3.35; the analysis of the AR(1) residuals is similar. Figure 3.16 displays a plot of the standardized residuals, the ACF of the residuals (note that R includes the correlation at lag zero which is always one), and the value of the Q-statistic, (3.138), at lags $H = 1$ through $H = 2 0$ . These diagnostics are provided by issuing the command

> tsdiag(gnpgr.ma, gof.lag=20)

where gnpgr.ma was described in the previous example.

Inspection of the time plot of the standardized residuals in Figure 3.16 shows no obvious patterns. Notice that there are outliers, however, with a few values exceeding 3 standard deviations in magnitude. The ACF of the standardized residuals shows no apparent departure from the model assumptions, and the Q-statistic is never significant at the lags shown.

Finally, Figure 3.17 shows a histogram of the residuals (top), and a normal Q-Q plot of the residuals (bottom). Here we see the residuals are somewhat close to normality except for a few extreme values in the tails.

![](images/c1b844f381d174e91459518cee70c556ce56869d2986e3db2510b4ebd5ce3e59.jpg)

![](images/7a89d8b606166a7514d65d54c249667c07a9059226a233681c76f215cbe67309.jpg)

![](images/cfb97ce22e98b748059cc8f336b7d026ed93bedc88faa6a8cdb2dc3da0cb39ac.jpg)  
Figure 3.18 Diagnostics for the ARIMA $( 0 , 1 , 1 )$ fit to the logged varve data.

Running a Shapiro–Wilk test (Royston, 1982) yields a p-value of .003, which indicates the residuals are not normal. Hence, the model appears to fit well except for the fact that a distribution with heavier tails than the normal distribution should be employed. We discuss some possibilities in Chapters 5 and 6. These diagnostics can be performed in R by issuing the commands:

> hist(gnpgr.ma$resid, br=12)   
> qqnorm(gnpgr.ma$resid)   
> shapiro.test(gnpgr.ma$resid)

# Example 3.37 Diagnostics for the Glacial Varve Series

In Example 3.31, we fit an ARIMA $( 0 , 1 , 1 )$ model to the logarithms of the glacial varve data. Figure 3.18 shows the diagnostics from that fit, and we notice a significant lag 1 correlation. In addition, the Q-statistic is significant for every value of $H$ displayed. Because the ACF of the residuals appear to be tailing off, an AR term is suggested.

![](images/a0ef4f599d8e8b70f8d863c8ec21b552162124f4922bb84e2c0e731cf976279f.jpg)

![](images/0938866d44bca5a5abb1b1156237d7329646b725a0cf6d0a7285589041d64162.jpg)

![](images/d502e5bec77ec0c618be89b7f5058319738ee0d8c5eaf16aabfa36aaafb22ce2.jpg)  
Figure 3.19 Diagnostics for the ARIMA $( 1 , 1 , 1 )$ fit to the logged varve data.

Next, we fit an ARIMA $( 1 , 1 , 1 )$ to the logged varve data and obtained the estimates $\widehat { \phi } = . 2 3 _ { ( . 0 5 ) }$ , $ { \hat { \theta } } = - . 8 9 _ { ( . 0 3 ) }$ , and $\widehat { \sigma } _ { w } ^ { 2 } = . 2 3$ . Hence the AR term is significant. Diagnostics for this model are displayed in Figure 3.19, and it appears this model fits the data well.

To implement these analyses in R, use the following commands (we assume the data are in varve):

> varve.ma $=$ arima(log(varve), order = c(0, 1, 1))   
> varve.ma # to display results   
> tsdiag(varve.ma)   
> varve.arma $=$ arima(log(varve), order = c(1, 1, 1))   
> varve.arma # to display results   
> tsdiag(varve.arma, gof.lag=20)

In Example 3.35, we have two competing models, an AR(1) and an MA(2) on the GNP growth rate, that each appear to fit the data well. In addition, we might also consider that an AR(2) or an MA(3) might do better for forecasting. Perhaps combining both models, that is, fitting an ARMA $( 1 , 2 )$ to the GNP

![](images/d6a174deaa008dca600c909c2e4ae435bbb99f4fb8e1cd575736e539f52fbc61.jpg)  
Figure 3.20 A perfect fit and a terrible forecast.

growth rate, would be the best. As previously mentioned, we have to be concerned with overfitting the model; it is not always the case that more is better. Overfitting leads to less-precise estimators, and adding more parameters may fit the data better but may also lead to bad forecasts. This result is illustrated in the following example.

# Example 3.38 A Problem with Overfitting

Figure 3.20 shows the U.S. population by official census, every 10 years from 1910 to 1990, as points. If we use these nine observations to predict the future population of the U.S., we can use an eight-degree polynomial so the fit to the nine observations is perfect. The model in this case is

$$
x _ {t} = \beta_ {0} + \beta_ {1} t + \beta_ {2} t ^ {2} + \dots + \beta_ {8} t ^ {8} + w _ {t}.
$$

The fitted model, which is plotted through the year 2010 as a line, passes through the nine observations. The model predicts that the population of the U.S. will be close to zero in the year 2000, and will cross zero sometime in the year 2002!

The final step of model fitting is model choice or model selection. That is, we must decide which model we will retain for forecasting. The most popular techniques, AIC, AICc, and SIC, were described in 2.2 in the context of regression models. A discussion of AIC based on Kullback–Leibler distance was given in Problems 2.4 and 2.5.

# Example 3.39 Model Choice for the U.S. GNP Series

Returning to the analysis of the U.S. GNP data presented in Examples 3.35 and 3.36, recall that two models, an AR(1) and an MA(2), fit the GNP growth rate well. To choose the final model, we compare the AIC, the AICc, and the SIC for both models.

Below are the R commands for the comparison. The effective sample size in this example is 222. We note that R returns AIC $^ { 8 }$ as part of the ARIMA fit.

```diff
> # AIC
> gnpgr.ma$aic
[1] -1431.929 # MA(2)
> gnpgr.ar$aic
[1] -1431.221 # AR(1)
> # AICc - see Section 2.2
> log(gnpgr.ma$sigma2) + (222+2)/(222-2-2)
[1] -8.297199 # MA(2)
> log(gnpgr.ar$sigma2) + (222+1)/(222-1-2)
[1] -8.294156 # AR(1)
> # SIC or BIC - see Section 2.2
> log(gnpgr.ma$sigma2) + (2*log(222)/222)
[1] -9.276049 # MA(2)
> log(gnpgr.ar$sigma2) + (1*log(222)/222)
[1] -9.288084 # AR(1) 
```

The AIC and AICc both prefer the MA(2) fit, whereas the SIC (or BIC) prefers the simpler AR(1) model. It is often the case that the SIC will select a model of smaller order than the AIC or AICc. It would not be unreasonable in this case to retain the AR(1) because pure autoregressive models are easier to work with.

# 3.9 Multiplicative Seasonal ARIMA Models

In this section, we introduce several modifications made to the ARIMA model to account for seasonal and nonstationary behavior. Often, the dependence on the past tends to occur most strongly at multiples of some underlying seasonal lag $s$ . For example, with monthly economic data, there is a strong yearly component occurring at lags that are multiples of $s \ = \ 1 2$ , because

of the strong connections of all activity to the calendar year. Data taken quarterly will exhibit the yearly repetitive period at $s = 4$ quarters. Natural phenomena such as temperature also have strong components corresponding to seasons. Hence, the natural variability of many physical, biological, and economic processes tends to match with seasonal fluctuations. Because of this, it is appropriate to introduce autoregressive and moving average polynomials that identify with the seasonal lags. The resulting pure seasonal autoregressive moving average model, say, ARMA $( P , Q ) _ { s }$ , then takes the form

$$
\Phi_ {P} \left(B ^ {s}\right) x _ {t} = \Theta_ {Q} \left(B ^ {s}\right) w _ {t}, \tag {3.139}
$$

with the following definition.

Definition 3.12 The operators

$$
\Phi_ {P} \left(B ^ {s}\right) = 1 - \Phi_ {1} B ^ {s} - \Phi_ {2} B ^ {2 s} - \dots - \Phi_ {P} B ^ {P s} \tag {3.140}
$$

and

$$
\Theta_ {Q} \left(B ^ {s}\right) = 1 + \Theta_ {1} B ^ {s} + \Theta_ {2} B ^ {2 s} + \dots + \Theta_ {Q} B ^ {Q s} \tag {3.141}
$$

are the seasonal autoregressive operator and the seasonal moving average operator of orders $P$ and $Q$ , respectively, with seasonal period $s$ .

Analogous to the properties of nonseasonal ARMA models, the pure seasonal $\mathrm { A R M A } ( P , Q ) _ { s }$ is causal only when the roots of $\Phi _ { P } ( z ^ { s } )$ lie outside the unit circle, and it is invertible only when the roots of $\Theta _ { Q } ( z ^ { s } )$ lie outside the unit circle.

# Example 3.40 A Seasonal ARMA Series

A first-order seasonal autoregressive moving average series that might run over months could be written as

$$
(1 - \Phi B ^ {1 2}) x _ {t} = (1 + \Theta B ^ {1 2}) w _ {t}
$$

or

$$
x _ {t} = \Phi x _ {t - 1 2} + w _ {t} + \Theta w _ {t - 1 2}.
$$

This model exhibits the series $x _ { t }$ in terms of past lags at the multiple of the yearly seasonal period $s = 1 2$ months. It is clear from the above form that estimation and forecasting for such a process involves only straightforward modifications of the unit lag case already treated. In particular, the causal condition requires $| \Phi | < 1$ , and the invertible condition requires $| \Theta | < 1$ .

For the first-order seasonal ( $s = 1 2$ ) MA model, $x _ { t } = w _ { t } + \Theta w _ { t - 1 2 }$ , it is easy to verify that

$$
\gamma (0) = (1 + \Theta^ {2}) \sigma^ {2}
$$

Table 3.2 Behavior of the ACF and PACF for Causal and Invertible Pure Seasonal ARMA Models   

<table><tr><td></td><td>AR(P)s</td><td>MA(Q)s</td><td>ARMA(P,Q)s</td></tr><tr><td>ACF*</td><td>Tails off at lags ks, k = 1, 2, ...,</td><td>Cuts off after lag Qs</td><td>Tails off at lags ks</td></tr><tr><td>PACF*</td><td>Cuts off after lag Ps</td><td>Tails off at lags ks k = 1, 2, ...,</td><td>Tails off at lags ks</td></tr></table>

*The values at nonseasonal lags $h \neq k s$ , for $k = 1 , 2 , \ldots$ , are zero.

$$
\gamma (\pm 1 2) = \Theta \sigma^ {2}
$$

$$
\begin{array}{r c l} \gamma (h) & = & 0, \quad \text {o t h e r w i s e}. \end{array}
$$

Thus, the only nonzero correlation, aside from lag zero, is

$$
\rho (\pm 1 2) = \Theta / (1 + \Theta^ {2}).
$$

For the first-order seasonal ( $s = 1 2$ ) AR model, using the techniques of the nonseasonal AR(1), we have

$$
\gamma (0) = \sigma^ {2} / (1 - \Phi^ {2})
$$

$$
\gamma (\pm 1 2 k) = \sigma^ {2} \Phi^ {k} / (1 - \Phi^ {2}) k = 1, 2, \dots
$$

$$
\begin{array}{r c l} \gamma (h) & = & 0, \quad \text {o t h e r w i s e}. \end{array}
$$

In this case, the only non-zero correlations are

$$
\rho (\pm 1 2 k) = \Phi^ {k}, \quad k = 0, 1, 2, \dots .
$$

These results can be verified using the general result that $\gamma ( h ) = \Phi \gamma ( h - 1 2 )$ , for $h \geq 1$ . For example, when $h = 1$ , $\gamma ( 1 ) = \Phi \gamma ( 1 1 )$ , but when $h = 1 1$ , we have $\gamma ( 1 1 ) = \Phi \gamma ( 1 )$ , which implies that $\gamma ( 1 ) = \gamma ( 1 1 ) = 0$ . In addition to these results, the PACF have the analogous extensions from nonseasonal to seasonal models.

As an initial diagnostic criterion, we can use the properties for the pure seasonal autoregressive and moving average series listed in Table 3.2. These properties may be considered as generalizations of the properties for nonseasonal models that were presented in Table 3.1.

In general, we can combine the seasonal and nonseasonal operators into a multiplicative seasonal autoregressive moving average model, denoted by ARMA $( p , q ) \times ( P , Q ) _ { s }$ , and write

$$
\Phi_ {P} \left(B ^ {s}\right) \phi (B) x _ {t} = \Theta_ {Q} \left(B ^ {s}\right) \theta (B) w _ {t} \tag {3.142}
$$

as the overall model. Although the diagnostic properties in Table 3.2 are not strictly true for the overall mixed model, the behavior of the ACF and PACF tends to show rough patterns of the indicated form. In fact, for mixed models, we tend to see a mixture of the facts listed in Tables 3.1 and 3.2. In fitting such models, focusing on the seasonal autoregressive and moving average components first generally leads to more satisfactory results.

# Example 3.41 A Mixed Seasonal Model

Consider an ARMA $( 0 , 1 ) \times ( 1 , 0 ) _ { 1 2 }$ model

$$
x _ {t} = \Phi x _ {t - 1 2} + w _ {t} + \theta w _ {t - 1},
$$

where $| \Phi | < 1$ and $| \theta | < 1$ . Then, because $x _ { t - 1 2 }$ , $w _ { t }$ , and $w _ { t - 1 }$ are uncorrelated, and $x _ { t }$ is stationary, $\gamma ( 0 ) = \Phi ^ { 2 } \gamma ( 0 ) + \sigma _ { w } ^ { 2 } + \theta ^ { 2 } \sigma _ { w } ^ { 2 }$ −, or

$$
\gamma (0) = \frac {1 + \theta^ {2}}{1 - \Phi^ {2}} \sigma_ {w} ^ {2}.
$$

In addition, multiplying the model by $x _ { t - h }$ , $h > 0$ , and taking expectations, we have $\gamma ( 1 ) = \Phi \gamma ( 1 1 ) + \theta \sigma _ { w } ^ { 2 }$ , and $\gamma ( h ) = \Phi \gamma ( h - 1 2 )$ , for $h \geq 2$ . Thus, the ACF for this model is

$$
\begin{array}{l} \rho (1 2 h) \quad = \quad \Phi^ {h} \quad h = 1, 2, \dots \\ \rho (1 2 h - 1) = \rho (1 2 h + 1) = \frac {\theta}{1 + \theta^ {2}} \Phi^ {h} h = 0, 1, 2, \dots , \\ \begin{array}{r c l} \rho (h) & = & 0, \quad \text {o t h e r w i s e}. \end{array} \\ \end{array}
$$

The ACF and PACF for this model, with $\Phi = . 8$ and $\theta = - . 5$ , are shown in Figure 3.21. These type of correlation relationships, although idealized here, are typically seen with seasonal data.

To reproduce Figure 3.21 in R, use the following commands:

> phi = c(rep(0,11),.8)   
$>$ acf $=$ ARMAacf(ar=phi, ma=-.5, 50)   
> pacf $=$ ARMAacf(ar=phi, ma=-.5, 50, pacf=T)  
> par(mfrow=c(1,2))  
> plot(acf, type="h", xlab $\mid =$ "lag")   
$>$ abline $\scriptstyle \mathtt { h } = 0$ )   
$>$ plot(pacf, type="h", xlab $\mid =$ "lag")   
> abline $\scriptstyle \mathtt { h } = 0$ )

Seasonal nonstationarity can occur, for example, when the process is nearly periodic in the season. For example, with average monthly temperatures over the years, each January would be approximately the same, each February would be approximately the same, and so on. In this case, we might think of average monthly temperature $x _ { t }$ as being modeled as

$$
x _ {t} = S _ {t} + w _ {t},
$$

![](images/95f1ebc9f55acf43ededd0d77fade484d6b9cf055297af7c9e0e14eb85ca402c.jpg)

![](images/46f038d10639ea6d05ea0d3a8ec6b8fbe4ef59412e6b1076729fee399b6b4e99.jpg)  
Figure 3.21 ACF and PACF of the mixed seasonal ARMA model $\begin{array} { r l } { x _ { t } } & { { } = } \end{array}$ . $8 x _ { t - 1 2 } + w _ { t } - . 5 w _ { t - 1 }$ .

where $S _ { t }$ is a seasonal component that varies slowly from one year to the next, according to a random walk,

$$
S _ {t} = S _ {t - 1 2} + v _ {t}.
$$

In this model, $w _ { t }$ and ${ \boldsymbol { v } } _ { t }$ are uncorrelated white noise processes. The tendency of data to follow this type of model will be exhibited in a sample ACF that is large and decays very slowly at lags $h = 1 2 k$ , for $k = 1 , 2 , \dots$ . If we subtract the effect of successive years from each other, we find that

$$
(1 - B ^ {1 2}) x _ {t} = x _ {t} - x _ {t - 1 2} = v _ {t} + w _ {t} - w _ {t - 1 2}.
$$

This model is a stationary $\mathrm { M A } ( 1 ) _ { 1 2 }$ , and its ACF will have a peak only at lag 12. In general, seasonal differencing can be indicated when the ACF decays slowly at multiples of some season $s$ , but is negligible between the periods. Then, a seasonal difference of order D is defined as

$$
\nabla_ {s} ^ {D} x _ {t} = \left(1 - B ^ {s}\right) ^ {D} x _ {t}, \tag {3.143}
$$

where $D = 1 , 2 , \ldots$ takes integer values. Typically, $D = 1$ is sufficient to obtain seasonal stationarity.

Incorporating these ideas into a general model leads to the following definition.

Definition 3.13 The multiplicative seasonal autoregressive integrated moving average model, or SARIMA model, of Box and Jenkins (1970) is given by

$$
\Phi_ {P} \left(B ^ {s}\right) \phi (B) \nabla_ {s} ^ {D} \nabla^ {d} x _ {t} = \alpha + \Theta_ {Q} \left(B ^ {s}\right) \theta (B) w _ {t}, \tag {3.144}
$$

where $w _ { t }$ is the usual Gaussian white noise process. The general model is denoted as ARIMA $( p , d , q ) \times ( P , D , Q ) _ { s }$ . The ordinary autoregressive and moving average components are represented by polynomials $\phi ( B )$ and $\theta ( B )$ of orders $p$ and $q$ , respectively [see (3.5) and (3.17)], and the seasonal autoregressive and moving average components by $\Phi _ { P } ( B ^ { s } )$ and $\Theta _ { Q } ( B ^ { s } )$ [see (3.140) and (3.141)] of orders $P$ and $Q$ and ordinary and seasonal difference components by $\nabla ^ { d } = ( 1 - B ) ^ { d }$ and $\nabla _ { s } ^ { D } = ( 1 - B ^ { s } ) ^ { D }$ .

# Example 3.42 A SARIMA Model

Consider the following model, which often provides a reasonable representation for seasonal, nonstationary, economic time series. We exhibit the equations for the model, denoted by $\mathrm { A R I M A } ( 0 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ in the notation given above, where the seasonal fluctuations occur every 12 months. Then, the model (3.144) becomes

$$
(1 - B ^ {1 2}) (1 - B) x _ {t} = (1 + \Theta B ^ {1 2}) (1 + \theta B) w _ {t}. \tag {3.145}
$$

Expanding both sides of (3.145) leads to the representation

$$
(1 - B - B ^ {1 2} + B ^ {1 3}) x _ {t} = (1 + \theta B + \Theta B ^ {1 2} + \Theta \theta B ^ {1 3}) w _ {t},
$$

or in difference equation form

$$
x _ {t} = x _ {t - 1} + x _ {t - 1 2} - x _ {t - 1 3} + w _ {t} + \theta w _ {t - 1} + \Theta w _ {t - 1 2} + \Theta \theta w _ {t - 1 3}.
$$

Selecting the appropriate model for a given set of data from all of those represented by the general form (3.144) is a daunting task, and we usually think first in terms of finding difference operators that produce a roughly stationary series and then in terms of finding a set of simple autoregressive moving average or multiplicative seasonal ARMA to fit the resulting residual series. Differencing operations are applied first, and then the residuals are constructed from a series of reduced length. Next, the ACF and the PACF of these residuals are evaluated. Peaks that appear in these functions can often be eliminated by fitting an autoregressive or moving average component in accordance with the general properties of Tables 3.1 and 3.2. In considering whether the model is satisfactory, the diagnostic techniques discussed in §3.8 still apply.

![](images/6c6c5c2e28319f3436f09db80d4c709cab51bc057b5c472275e42f97a524fe6f.jpg)

![](images/65c7f075a9a38068d41fcb595296f3dd6b0e4a9eb081e5621ff2f9b7b704e142.jpg)  
Figure 3.22 Values of the Monthly Federal Reserve Board Production Index and Unemployment (1948-1978, $n = 3 7 2$ months).

# Example 3.43 Analysis of the Federal Reserve Board Production Index.

A problem of great interest in economics involves first identifying a model within the Box–Jenkins class for a given time series and then producing forecasts based on the model. For example, we might consider applying this methodology to the Federal Reserve Board Production Index shown in Figure 3.22. The ACFs and PACFs for this series are shown in Figure 3.23, and we note the slow decay in the ACF and the peak at lag $h = 1$ in the PACF, indicating nonstationary behavior.

Following the recommended procedure, a first difference was taken, and the ACF and PACF of the first difference

$$
\nabla x _ {t} = x _ {t} - x _ {t - 1}
$$

are shown in Figure 3.24. Noting the peaks at 12, 24, 36, and 48 with relatively slow decay suggested a seasonal difference and Figure 3.25 shows the seasonal difference of the differenced production, say,

$$
\nabla_ {1 2} \nabla x _ {t} = (1 - B ^ {1 2}) (1 - B) x _ {t}.
$$

Characteristics of the ACF and PACF of this series tend to show a strong peak at $h = 1 2$ in the autocorrelation function, with smaller peaks appearing at $h = 2 4 , 3 6$ , combined with peaks at $h = 1 2 , 2 4 , 3 6 , 4 8$ , in the

![](images/dbdc64edf6c4b0043efedd890e89543537ea32b9dd10cfeeb2de05ecc6ab0a44.jpg)

![](images/02621d147578b7ca6c5d4d18ca71a8f5088c7df38adb020ad5d59a352b471c8b.jpg)  
Figure 3.23 ACF and PACF of the production series.

partial autocorrelation function. Using Table 3.2, this suggests either a seasonal moving average of order $Q = 1$ , a seasonal autoregression of possible order $P = 2$ , or due to the fact that both the ACF and PACF may be tailing off at the seasonal lags, perhaps both components, $P = 2$ and $Q = 1$ , are needed.

Inspecting the ACF and the PACF at the within season lags, $h \ =$ $1 , \ldots , 1 1$ , it appears that both the ACF and PACF are tailing off. Based on Table 3.1, this result indicates that we should consider fitting a model with both $p > 0$ and $q > 0$ for the nonseasonal components. Hence, at first we will consider $p = 1$ and $q = 1$ .

Fitting the three models suggested by these observations and computing the AIC for each, we obtain:

(i) $\mathrm { A R I M A } ( 1 , 1 , 1 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ , AIC = 1162.30   
(ii) $\mathrm { A R I M A } ( 1 , 1 , 1 ) \times ( 2 , 1 , 0 ) _ { 1 2 }$ , AIC = 1169.04   
(iii) $\mathrm { A R I M A } ( 1 , 1 , 1 ) \times ( 2 , 1 , 1 ) _ { 1 2 } , \quad \mathrm { A I C } = 1 1 4 8 . 4 3$

On the basis of the AICs, we prefer the $\mathrm { A R I M A } ( 1 , 1 , 1 ) \times ( 2 , 1 , 1 ) _ { 1 2 }$ model. Figure 3.26 shows the diagnostics for this model, leading to the conclusion that the model is adequate. We note, however, the presence of a few outliers.

![](images/fcd91994cfdb1122900e8af556f92d5a9c08e2a9e2670a5b1c23c84f41c4fcee.jpg)

![](images/15ad9d86d1464fcb3e3c1ebc308b64e0e2036e3969a48595e78aade96b2a0e6f.jpg)

![](images/727a164581c4ddbb3bf7e36bb5f5ee8dff6028104683a6b2f2c9a04887759558.jpg)  
Figure 3.24 ACF and PACF of differenced production, $( 1 - B ) x _ { t }$ .

![](images/e7855bf27c633befc845cb912660b2141c5822772e7bec88c4cde1010c0b6ede.jpg)  
Figure 3.25 ACF and PACF of first differenced and then seasonally differenced production, $( 1 - B ) ( 1 - B ^ { \scriptscriptstyle \mathrm { 1 2 } } ) x _ { t }$ .

![](images/514ca78caf80f30484802d07a8c746f383538eec7c85dd9e31dc13c95b211a07.jpg)

![](images/aabba37762f9f00b4a52b9b5d5f5624acffe19715c48aedcd0f6928799434f5f.jpg)

![](images/33b02d0a39713c48ffb0b33db63fb3e39340102c5cddc6ff42b84c5f15098cc7.jpg)  
Figure 3.26 Diagnostics for the $\mathrm { A R I M A } ( 1 , 1 , 1 ) \times ( 2 , 1 , 1 ) _ { 1 2 }$ fit on the Production data.

The fitted ARIMA $( 1 , 1 , 1 ) \times ( 2 , 1 , 1 ) _ { 1 2 }$ i s

$$
\begin{array}{l} (1 +. 2 2 _ {(}. 0 8) B ^ {1 2} +. 2 8 _ {(}. 0 6) B ^ {2 4}) (1 -. 5 8 _ {(}. 1 1) B) \nabla_ {1 2} \nabla \widehat {x} _ {t} \\ = (1 -. 5 0 _ {(. 0 7)} B ^ {1 2}) (1 -. 2 7 _ {(. 1 3)} B) \widehat {w} _ {t} \\ \end{array}
$$

with $\widehat { \sigma } _ { w } ^ { 2 } = 1 . 3 5$ . Forecasts based on the fitted model for the next 12 months are shown in Figure 3.27.

Finally, we present the R code necessary to reproduce most of the analyses performed in Example 3.43.

```julia
> prod=scan("/mydata/prod.dat")
> par(mfrow=c(2,1)) # (P)ACF of data
> acf(prod, 48)
> pacf(prod, 48)
> par(mfrow=c(2,1)) # (P)ACF of d1 data
> acf(diff(prod), 48)
> pacf(diff(prod), 48) 
```

![](images/6c13c77fb1992ea493efb4c81371f06b0ca43f3cd70c2e70c544fdb82c87d2f9.jpg)  
Figure 3.27 Forecasts and limits for production index. The vertical dotted line separates the data from the predictions.

> par(mfrow=c(2,1)) # (P)ACF of d1-d12 data  
> acf(diff(diff(prod),12), 48)   
> pacf(diff(diff(prod),12), 48)   
> ### fit model (iii)   
> prod.fit3 $=$ arima(prod, order $\Bumpeq { \bf c }$ (1,1,1),   
$^ +$ seasonal $\cdot ^ { = }$ list(order $\mathtt { \Omega } = \mathtt { \Omega }$ (2,1,1), period $= 1 2$ ))   
> prod.fit3 # to view the results   
> tsdiag(prod.fit3, gof.lag=48) # diagnostics

> ### forecasts for the final model   
> prod.pr $=$ predict(prod.fit3, n.ahead $= 1 2$ )   
> U = prod.pr$pred $^ +$ 2*prod.pr$se   
> L = prod.pr$pred - 2*prod.pr$se   
> month=337:372   
> plot(month, prod[month], type="o", xlim $_ { 1 } = \mathtt { c }$ (337,384),   
+ ylim=c(100,180), ylab $=$ "Production")   
> lines(prod.pr$pred, col $\cdot ^ { = }$ "red", type="o")   
$>$ lines(U, col $\cdot ^ { = }$ "blue", lty $\mathop { : = }$ "dashed")   
$>$ lines(L, col $\cdot ^ { = }$ "blue", lty $\mathop { : = }$ "dashed")   
$>$ abline(v=372.5,lty $=$ "dotted")

# Problems

# Section 3.2

3.1 For an MA(1), $x _ { t } = w _ { t } + \theta w _ { t - 1 }$ , show that $| \rho _ { x } ( 1 ) | \leq 1 / 2$ for any number $\theta$ . For which values of $\theta$ does $\rho _ { x } ( 1 )$ attain its maximum and minimum?

3.2 Let $w _ { t }$ be white noise with variance $\sigma _ { w } ^ { 2 }$ and let $| \phi | < 1$ be a constant. Consider the process

$$
\begin{array}{l} \begin{array}{r c l} x _ {1} & = & w _ {1} \end{array} \\ x _ {t} = \phi x _ {t - 1} + w _ {t} t = 2, 3, \dots . \\ \end{array}
$$

(a) Find the mean and the variance of $\{ x _ { t } , \ t = 1 , 2 , . . . \}$ . Is $x _ { t }$ stationary?   
(b) Show

$$
\operatorname {c o r r} (x _ {t}, x _ {t - h}) = \phi^ {h} \left[ \frac {\operatorname {v a r} (x _ {t - h})}{\operatorname {v a r} (x _ {t})} \right] ^ {1 / 2}
$$

for $h \geq 0$

(c) Argue that for large $t$ ,

$$
\mathrm {v a r} (x _ {t}) \approx \frac {\sigma_ {w} ^ {2}}{1 - \phi^ {2}}
$$

and

$$
\operatorname {c o r r} \left(x _ {t}, x _ {t - h}\right) \approx \phi^ {h}, \quad h \geq 0,
$$

so in a sense, $x _ { t }$ is “asymptotically stationary.”

(d) Comment on how you could use these results to simulate $n$ observations of a stationary Gaussian AR(1) model from simulated iid N(0,1) values.   
(e) Now suppose $x _ { 1 } = w _ { 1 } / \sqrt { 1 - \phi ^ { 2 } }$ . Is this process stationary?

3.3 Identify the following models as ARMA $( p , q )$ models (watch out for parameter redundancy), and determine whether they are causal and/or invertible:

(

3.4 Verify the causal conditions for an AR(2) model given in (3.27). That is, show that an AR(2) is causal if and only if (3.27) holds.

# Section 3.3

3.5 For the AR(2) model given by $x _ { t } = - . 9 x _ { t - 2 } + w _ { t }$ , find the roots of the autoregressive polynomial, and then sketch the ACF, $\rho ( h )$ .

3.6 For the AR(2) autoregressive series shown below, determine a set of difference equations that can be used to find $\psi _ { j } , j = 0 , 1 , \ldots$ in the representation (3.24) and the autocorrelation function $\rho ( h ) , h = 0 , 1 , \ldots$ . Solve for the constants in the ACF using the known initial conditions, and plot the first eight values.

(a)   
(b)   
(c)

# Section 3.4

3.7 Verify the calculations for the autocorrelation function of an ARMA $( 1 , 1 )$ process given in Example 3.11. Compare the form with that of the ACF for the $\mathrm { A R M A } ( 1 , 0 )$ and the ARMA $( 0 , 1 )$ series. Plot the ACFs of the three series on the same graph for $\phi = . 6 , \theta = . 9$ , and comment on the diagnostic capabilities of the ACF in this case.   
3.8 Generate $n = 1 0 0$ observations from each of the three models discussed in Problem 3.7. Compute the sample ACF for each model and compare it to the theoretical values. Compute the sample PACF for each of the generated series and compare the sample ACFs and PACFs with the general results given in Table 3.1.

# Section 3.5

3.9 Let $M _ { t }$ represent the cardiovascular mortality series discussed in Chapter 2, Example 2.2.

(a) Fit an AR(2) to $M _ { t }$ using linear regression as in Example 3.16.   
(b) Assuming the fitted model in (a) is the true model, find the forecasts over a four-week horizon, $x _ { n + m } ^ { n }$ , for $m = 1 , 2 , 3 , 4$ , and the corresponding 95% prediction intervals.

3.10 Consider the MA(1) series

$$
x _ {t} = w _ {t} + \theta w _ {t - 1},
$$

where $w _ { t }$ is white noise with variance $\sigma _ { w } ^ { 2 }$

(a) Derive the minimum mean square error one-step forecast based on the infinite past, and determine the mean square error of this forecast.   
(b) Let $\widetilde { x } _ { n + 1 } ^ { n }$ be the truncated one-step-ahead forecast as given in (3.82). Show that

$$
E \left[ (x _ {n + 1} - \widetilde {x} _ {n + 1} ^ {n}) ^ {2} \right] = \sigma^ {2} (1 + \theta^ {2 + 2 n}).
$$

Compare the result with (a), and indicate how well the finite approximation works in this case.

3.11 In the context of equation (3.56), show that, if $\gamma ( 0 ) > 0$ and $\gamma ( h ) \to 0$ as $h \to \infty$ , then $\Gamma _ { n }$ is positive definite.

3.12 Suppose $x _ { t }$ is stationary with zero mean and recall the definition of the PACF given by (3.49) and (3.50). That is, let

$$
\epsilon_ {t} = x _ {t} - \sum_ {i = 1} ^ {h - 1} a _ {i} x _ {t - i}
$$

and

$$
\delta_ {t - h} = x _ {t - h} - \sum_ {j = 1} ^ {h - 1} b _ {j} x _ {t - j}
$$

be the two residuals where $\{ a _ { 1 } , \dotsc , a _ { h - 1 } \}$ and $\big \{ b _ { 1 } , \dotsc , b _ { h - 1 } \big \}$ are chosen so that they minimize the mean-squared errors

$$
E [ \epsilon_ {t} ^ {2} ] \quad \text {a n d} \quad E [ \delta_ {t - h} ^ {2} ].
$$

The PACF at lag $h$ was defined as the cross-correlation between $\epsilon _ { t }$ and $\delta _ { t - h }$ ; that is,

$$
\phi_ {h h} = \frac {E (\epsilon_ {t} \delta_ {t - h})}{\sqrt {E (\epsilon_ {t} ^ {2}) E (\delta_ {t - h} ^ {2})}}.
$$

Let $R _ { h }$ be the $h \times h$ matrix with elements $\rho ( i - j ) , i , j = 1 , . . . , h$ , and let $\pmb { \rho } _ { h } = ( \rho ( 1 ) , \rho ( 2 ) , \dots , \rho ( h ) ) ^ { \prime }$ be the vector of lagged autocorrelations, $\rho ( h ) = \mathrm { c o r r } ( x _ { t + h } , x _ { t } )$ . Let $\widetilde { \pmb { \rho } } _ { h } = ( \rho ( h ) , \rho ( h - 1 ) , \dots , \rho ( 1 ) ) ^ { \prime }$ be the reversed vector. In addition, let $\boldsymbol { x } _ { t } ^ { h }$ +denote the BLP of $x _ { t }$ given $\{ x _ { t - 1 } , \ldots , x _ { t - h } \}$ :

$$
x _ {t} ^ {h} = \alpha_ {h 1} x _ {t - 1} + \dots + \alpha_ {h h} x _ {t - h},
$$

as described in Property P3.3. Prove

$$
\phi_ {h h} = \frac {\rho (h) - \widetilde {\pmb {\rho}} _ {h - 1} ^ {\prime} R _ {h - 1} ^ {- 1} \pmb {\rho} _ {h}}{1 - \widetilde {\pmb {\rho}} _ {h - 1} ^ {\prime} R _ {h - 1} ^ {- 1} \widetilde {\pmb {\rho}} _ {h - 1}} = \alpha_ {h h}.
$$

In particular, this result proves Property P3.4.

Hint: Divide the prediction equations [see (3.56)] by $\gamma ( 0 )$ and write the matrix equation in the partitioned form as

$$
\left( \begin{array}{c c} R _ {h - 1} & \widetilde {\boldsymbol {\rho}} _ {h - 1} \\ \widetilde {\boldsymbol {\rho}} _ {h - 1} ^ {\prime} & \rho (0) \end{array} \right) \left( \begin{array}{c} \boldsymbol {\alpha} _ {1} \\ \alpha_ {h h} \end{array} \right) = \left( \begin{array}{c} \boldsymbol {\rho} _ {h - 1} \\ \rho (h) \end{array} \right),
$$

where the $h \times 1$ vector of coefficients ${ \pmb \alpha } = ( \alpha _ { h 1 } , \ldots , \alpha _ { h h } ) ^ { \prime }$ is partitioned as ${ \pmb { \alpha } } = ( { \pmb { \alpha } } _ { 1 } ^ { \prime } , \alpha _ { h h } ) ^ { \prime }$ .

3.13 Suppose we wish to find a prediction function $g ( x )$ that minimizes

$$
M S E = E \left[ (y - g (x)) ^ {2} \right],
$$

where $x$ and $y$ are jointly distributed random variables with density function $f ( x , y )$ .

(a) Show that MSE is minimized by the choice

$$
g (x) = E (y \mid x).
$$

Hint:

$$
M S E = \int \left[ \int (y - g (x)) ^ {2} f (y | x) d y \right] f (x) d x.
$$

(b) Apply the above result to the model

$$
y = x ^ {2} + z,
$$

where $x$ and $z$ are independent zero-mean normal variables with variance one. Show that $M S E = 1$ .

(c) Suppose we restrict our choices for the function $g ( x )$ to linear functions of the form

$$
g (x) = a + b x
$$

and determine $a$ and $b$ to minimize $M S E$ . Show that $a = 1$ and

$$
b = \frac {E (x y)}{E (x ^ {2})} = 0
$$

and $M S E = 3$ . What do you interpret this to mean?

3.14 For an AR(1) model, determine the general form of the $m$ -step-ahead forecast $\boldsymbol { x } _ { t + m } ^ { t }$ and show

$$
E [ (x _ {t + m} - x _ {t + m} ^ {t}) ^ {2} ] = \sigma_ {w} ^ {2} \frac {1 - \phi^ {2 m}}{1 - \phi^ {2}}.
$$

3.15 Consider the ARMA(1,1) model discussed in Example 3.6, equation (3.26); that is, $x _ { t } = . 9 x _ { t - 1 } + . 5 w _ { t - 1 } + w _ { t }$ . Show that truncated prediction as defined in (3.81) is equivalent to truncated prediction using the recursive formula (3.82).

3.16 Verify statement (3.78), that for a fixed sample size, the ARMA prediction errors are correlated.

# Section 3.6

3.17 Let $M _ { t }$ represent the cardiovascular mortality series discussed in Chapter 2, Example 2.2. Fit an AR(2) model to the data using linear regression and using Yule–Walker.

(a) Compare the parameter estimates obtained by the two methods.   
(b) Compare the estimated standard errors of the coefficients obtained by linear regression with their corresponding asymptotic approximations, as given in Property P3.9.

3.18 Suppose $x _ { 1 } , \ldots , x _ { n }$ are observations from an AR(1) process with $\mu = 0$

(a) Show the backcasts can be written as $x _ { t } ^ { n } = \phi ^ { 1 - t } x _ { 1 }$ , for $t \leq 1$   
(b) In turn, show, for $t \leq 1$ , the backcasted errors are $\widehat { w } _ { t } ( \phi ) = x _ { t } ^ { n } -$ $\phi x _ { t - 1 } ^ { n } = \phi ^ { 1 - t } ( 1 - \phi ^ { 2 } ) x _ { 1 }$ .   
(c) Use the result of (b) to show $\begin{array} { r } { \sum _ { t = - \infty } ^ { 1 } \widehat { w } _ { t } ^ { 2 } ( \phi ) = ( 1 - \phi ^ { 2 } ) x _ { 1 } ^ { 2 } } \end{array}$   
(d) Use the result of (c) to verify the unconditional sum of squares, $S ( \phi )$ , can be written in the innovations form as $\scriptstyle \sum _ { t = - \infty } ^ { n } { \widehat { w } } _ { t } ^ { 2 } ( \phi )$ .   
(e) $x _ { t } ^ { t - 1 }$ $r _ { t } ^ { t - 1 }$ d show that . $S ( \phi )$ can also be written as $\scriptstyle \sum _ { t = 1 } ^ { n } ( x _ { t } - x _ { t } ^ { t - 1 } ) ^ { 2 } \ { \Big / } \ r _ { t } ^ { t - 1 }$ r t−1

3.19 Generate $n = 5 0 0$ observations from the ARMA model given by

$$
x _ {t} = . 9 x _ {t - 1} + w _ {t} -. 9 w _ {t - 1},
$$

with $w _ { t } \sim$ iid $\mathrm { { N } } ( 0 , 1 )$ . Plot the simulated data, compute the sample ACF and PACF of the simulated data, and fit an $\mathrm { A R M A } ( 1 , 1 )$ model to the data. What happened and how do you explain the results?

3.20 Generate 10 realizations of length $n = 2 0 0$ of a series from an ARMA(1,1) model with $\phi _ { 1 } = . 9 0 , \theta _ { 1 } = . 2$ and $\sigma ^ { 2 } = . 2 5$ . Fit the model by nonlinear least squares or maximum likelihood in each case and compare the estimators to the true values.   
3.21 Generate $n = 5 0$ observations from a Gaussian AR(1) model with $\phi = . 9 9$ and $\sigma _ { w } \ = \ 1$ . Using an estimation technique of your choice, compare the approximate asymptotic distribution of your estimate (the one you would use for inference) with the results of a bootstrap experiment (use $B = 2 0 0$ ).   
3.22 Using Example 3.30 as your guide, find the Gauss–Newton procedure for estimating the autoregressive parameter, $\phi$ , from the AR(1) model, $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ , given data $x _ { 1 } , \ldots , x _ { n }$ . Does this procedure produce the unconditional or the conditional estimator? Hint: Write the model as $w _ { t } ( \phi ) = x _ { t } - \phi x _ { t - 1 }$ ; your solution should work out to be a non-recursive procedure.

# 3.23 Consider the stationary series generated by

$$
x _ {t} = \alpha + \phi x _ {t - 1} + w _ {t} + \theta w _ {t - 1},
$$

where $E ( x _ { t } ) = \mu$ , $| \theta | < 1 , | \phi | < 1$ and the $w _ { t }$ are iid random variables with zero mean and variance $\sigma _ { w } ^ { 2 }$ .

(a) Determine the mean as a function of $\alpha$ for the above model. Find the autocovariance and ACF of the process $x _ { t }$ , and show that the process is weakly stationary. Is the process strictly stationary?

(b) Prove the limiting distribution as $n \to \infty$ of the sample mean,

$$
\bar {x} = n ^ {- 1} \sum_ {t = 1} ^ {n} x _ {t},
$$

is normal, and find its limiting mean and variance in terms of $\alpha$ , $\phi$ , $\theta$ , and $\sigma _ { w } ^ { 2 }$ . (Note: This part uses results from Appendix A.)

3.24 A problem of interest in the analysis of geophysical time series involves a simple model for observed data containing a signal and a reflected version of the signal with unknown amplification factor $a$ and unknown time delay $\delta$ . For example, the depth of an earthquake is proportional to the time delay $\delta$ for the P wave and its reflected form pP on a seismic record. Assume the signal is white and Gaussian with variance $\sigma _ { s } ^ { 2 }$ , and consider the generating model

$$
x _ {t} = s _ {t} + a s _ {t - \delta}.
$$

(a) Prove the process $x _ { t }$ is stationary. If $| a | < 1$ , show that

$$
s _ {t} = \sum_ {j = 0} ^ {\infty} (- a) ^ {j} x _ {t - \delta j}
$$

is a mean square convergent representation for the signal $s _ { t }$ , for $t = 1 , \pm 1 , \pm 2 , . . .$ .

(b) If the time delay $\delta$ is assumed to be known, suggest an approximate computational method for estimating the parameters $a$ and $\sigma _ { s } ^ { 2 }$ using maximum likelihood and the Gauss–Newton method.

(c) If the time delay $\delta$ is an unknown integer, specify how we could estimate the parameters including $\delta$ . Generate a $n = 5 0 0$ point series with $a = . 9$ , $\sigma _ { w } ^ { 2 } = 1$ and $\delta = 5$ . Estimate the integer time delay $\delta$ by searching over $\delta = 3 , 4 , \dots , 7$ .

3.25 Forecasting with estimated parameters: Let $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ be a sample of size $_ { n }$ from a causal AR(1) process, $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ . Let $\widehat { \phi }$ be the Yule–Walker estimator of $\phi$ .

(a) Show $\widehat \phi - \phi = { \cal O } _ { p } ( n ^ { - 1 / 2 } )$ . See Appendix A for the definition of $O _ { p } ( \cdot )$ .   
(b) Let $x _ { n + 1 } ^ { n }$ be the one-step-ahead forecast of $x _ { n + 1 }$ given the data $x _ { 1 } , \ldots , x _ { n }$ , based on the known parameter, $\phi$ , and let $\widehat { x } _ { n + 1 } ^ { n }$ be the one-step-ahead forecast when the parameter is replaced by $\widehat { \phi }$ . Show $x _ { n + 1 } ^ { n } - \widehat { x } _ { n + 1 } ^ { n } = O _ { p } ( n ^ { - 1 / 2 } )$ .

# Section 3.7

3.26 Suppose

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \dots + \beta_ {q} t ^ {q} + x _ {t}, \quad \beta_ {q} \neq 0,
$$

where $x _ { t }$ is stationary. First, show that $\nabla ^ { k } x _ { t }$ is stationary for any $k =$ $1 , 2 , \ldots$ , and then show that $\nabla ^ { k } y _ { t }$ is not stationary for $k \ : < \ : q$ , but is stationary for $k \geq q$ .

3.27 Verify that the IMA(1,1) model given in (3.131) can be inverted and written as (3.132).

3.28 For the logarithm of the glacial varve data, say, $x _ { t }$ , presented in Example 3.31, use the first 100 observations and calculate the EWMA, $\widetilde { \boldsymbol { x } } _ { t + 1 } ^ { t }$ , given in (3.134) for $t = 1 , \ldots , 1 0 0$ , using $\lambda = . 2 5 , . 5 0$ , and .75, and plot the EWMAs and the data superimposed on each other. Comment on the results.

# Section 3.8

3.29 In Example 3.36, we presented the diagnostics for the MA(2) fit to the GNP growth rate series. Using that example as a guide, complete the diagnostics for the AR(1) fit.

3.30 Using the gas price series described in Problem 2.9, fit an ARIMA $( p , d , q )$ model to the data, performing all necessary diagnostics. Comment.

3.31 The second column in the data file globtemp2.dat are annual global temperature deviations from 1880 to 2004. The data are an update to the Hansen-Lebedeff global temperature data and the url of the data source is in the file. Fit an ARIMA $( p , d , q )$ model to the data, performing all of the necessary diagnostics. After deciding on an appropriate model, forecast (with limits) the next 10 years. Comment. In R, use read.table to load the data file.

3.32 One of the series collected along with particulates, temperature, and mortality described in Example 2.2 is the sulfur dioxide series. Fit an ARIMA $( p , d , q )$ model to the data, performing all of the necessary diagnostics. After deciding on an appropriate model, forecast the data into the future four time periods ahead (about one month) and calculate 95% prediction intervals for each of the four forecasts. Comment.

# Section 3.9

3.33 Consider the ARIMA model

$$
x _ {t} = w _ {t} + \Theta w _ {t - 2}.
$$

(a) Identify the model using the notation ARIMA $( p , d , q ) \times ( P , D , Q ) _ { s }$ .   
(b) Show that the series is invertible for $| \Theta | < 1$ , and find the coefficients in the representation

$$
w _ {t} = \sum_ {k = 0} ^ {\infty} \pi_ {k} x _ {t - k}.
$$

(c) Develop equations for the $m$ -step ahead forecast, $\widetilde { x } _ { n + m }$ , and its variance based on the infinite past, $x _ { n } , x _ { n - 1 } , \ldots .$ .

3.34 Sketch the ACF of the seasonal ARIMA $( 0 , 1 ) \times ( 1 , 0 ) _ { 1 2 }$ model with $\Phi = . 8$ and $\theta = . 5$ .   
3.35 Fit a seasonal ARIMA model of your choice to the unemployment data displayed in Figure 3.22. Use the estimated model to forecast the next 12 months.   
3.36 Fit a seasonal ARIMA model of your choice to the U.S. Live Birth Series (birth.dat). Use the estimated model to forecast the next 12 months.   
3.37 Fit an appropriate seasonal ARIMA model to the log-transformed Johnson and Johnson earnings series of Example 1.1. Use the estimated model to forecast the next 4 quarters.

The following problems require the supplemental material given in Appendix B

3.38 Suppose $\begin{array} { r } { x _ { t } = \sum _ { j = 1 } ^ { p } \phi _ { j } x _ { t - j } + w _ { t } } \end{array}$ , where $\phi _ { p } \neq 0$ and $w _ { t }$ is white noise such that $w _ { t }$ is uncorrelated with $\{ x _ { k } ; k < t \}$ . Use the Projection Theorem to show that, for $n > p$ , the BLP of $x _ { n + 1 }$ on s $\bar { \ u { y } } \{ \ d { x } _ { k } , k \leq n \}$ i s

$$
\widehat {x} _ {n + 1} = \sum_ {j = 1} ^ {p} \phi_ {j} x _ {n + 1 - j}.
$$

3.39 Use the Projection Theorem to derive the Innovations Algorithm, Property P3.6, equations (3.68)-(3.70). Then, use Theorem B.2 to derive the $m$ -step-ahead forecast results given in (3.71) and (3.72).

3.40 Consider twith mean ries  and $x _ { t } = w _ { t } - w _ { t - 1 }$ $w _ { t }$ is a white noise process consider the problem of $\sigma _ { w } ^ { 2 }$ $x _ { n + 1 }$ $x _ { 1 } , \ldots , x _ { n }$ to answer the questions below.

(a) Show the best linear predictor is

$$
x _ {n + 1} ^ {n} = - \frac {1}{n + 1} \sum_ {k = 1} ^ {n} k x _ {k}.
$$

(b) Prove the mean square error is

$$
E (x _ {n + 1} - x _ {n + 1} ^ {n}) ^ {2} = \frac {n + 2}{n + 1} \sigma_ {w} ^ {2}.
$$

3.41 Use Theorem B.2 and B.3 to verify (3.105).   
3.42 Prove Theorem B.2.   
3.43 Prove Property P3.2.

# Chapter 4

# Spectral Analysis and Filtering

# 4.1 Introduction

The notion that a time series exhibits repetitive or regular behavior over time is of fundamental importance because it distinguishes time series analysis from classical statistics, which assumes complete independence over time. We have seen how dependence over time can be introduced through models that describe in detail the way certain empirical data behaves, even to the extent of producing forecasts based on the models. It is natural that models based on predicting the present as a regression on the past, such as are provided by the celebrated ARIMA or state-space forms, will be attractive to statisticians, who are trained to view nature in terms of linear models. In fact, the difference equations used to represent these kinds of models are simply the discrete versions of linear differential equations that may, in some instances, provide the ideal physical model for a certain phenomenon. An alternate version of the way nature behaves exists, however, and is based on a decomposition of an empirical series into its regular components.

In this chapter, we argue, the concept of regularity of a series can best be expressed in terms of periodic variations of the underlying phenomenon that produced the series, expressed as Fourier frequencies being driven by sines and cosines. Such a possibility was discussed in Chapters 1 and 2. From a regression point of view, we may imagine a system responding to various driving frequencies by producing linear combinations of sine and cosine functions. Expressed in these terms, the time domain approach may be thought of as regression of the present on the past, whereas the frequency domain approach may be considered as regression of the present on periodic sines and cosines. The frequency domain approaches are the focus of this chapter and

Chapter 7. To illustrate the two methods for generating series with a single primary periodic component, consider Figure 1.9, which was generated from a simple second-order autoregressive model, and the middle and bottom panels of Figure 1.11, which were generated by adding a cosine wave with a period of 50 points to white noise. Both series exhibit strong periodic fluctuations, illustrating that both models can generate time series with regular behavior. As discussed in Examples 2.7–2.9, a fundamental objective of spectral analysis is to identify the dominant frequencies in a series and to find an explanation of the system from which the measurements were derived.

Of course, the primary justification for any alternate model must lie in its potential for explaining the behavior of some empirical phenomenon. In this sense, an explanation involving only a few kinds of primary oscillations becomes simpler and more physically meaningful than a collection of parameters estimated for some selected difference equation. It is the tendency of observed data to show periodic kinds of fluctuations that justifies the use of frequency domain methods. Many of the examples in §1.2 are time series representing real phenomena that are driven by periodic components. The speech recording of the syllable aa...hh in Figure 1.3 contains a complicated mixture of frequencies related to the opening and closing of the glottis. Figure 1.5 shows the monthly SOI, which we later explain as a combination of two kinds of periodicities, a seasonal periodic component of 12 months and an El Ni˜no component of about three to five years. Of fundamental interest is the return period of the El Ni˜no phenomenon, which can have profound effects on local climate. Also of interest is whether the different periodic components of the new fish population depend on corresponding seasonal and El Ni˜no-type oscillations. We introduce the coherence as a tool for relating the common periodic behavior of two series. Seasonal periodic components are often pervasive in economic time series; this phenomenon can be seen in the quarterly earnings series shown in Figure 1.1. In Figure 1.6, we see the extent to which various parts of the brain will respond to a periodic stimulus generated by having the subject do alternate left and right finger tapping. Figure 1.7 shows series from an earthquake and a nuclear explosion. The relative amounts of energy at various frequencies for the two phases can produce statistics, useful for discriminating between earthquakes and explosions.

In this chapter, we summarize an approach to handling correlation generated in stationary time series that begins by transforming the series to the frequency domain. This simple linear transformation essentially matches sines and cosines of various frequencies against the underlying data and serves two purposes as discussed in Examples 2.7 and 2.8. The periodogram that was introduced in Example 2.8 has its population counterpart called the power spectrum, and its estimation is a main goal of spectral analysis. Another purpose of exploring this topic is statistical convenience resulting from the periodic components being nearly uncorrelated. This property facilitates writing likelihoods based on classical statistical methods

An important part of analyzing data in the frequency domain, as well as

the time domain, is the investigation and exploitation of the properties of the time-invariant linear filter. This special linear transformation is used similarly to linear regression in conventional statistics, and we use many of the same terms in the time series context. We have previously mentioned the coherence as a measure of the relation between two series at a given frequency, and we show later that this coherence also measures the performance of the best linear filter relating the two series. Linear filtering can also be an important step in isolating a signal embedded in noise. For example, the lower panels of Figure 1.11 contain a signal contaminated with an additive noise, whereas the upper panel contains the pure signal. It might also be appropriate to ask whether a linear filter transformation exists that could be applied to the lower panel to produce a series closer to the signal in the upper panel. The use of filtering for reducing noise will also be a part of the presentation in this chapter. We emphasize, throughout, the analogy between filtering techniques and conventional linear regression.

Many frequency scales will often coexist, depending on the nature of the problem. For example, in the Johnson & Johnson data set in Figure 1.1, the predominant frequency of oscillation is one cycle per year (4 quarters), or .25 cycles per observation. The predominant frequency in the SOI and fish populations series in Figure 1.5 is also one cycle per year, but this corresponds to 1 cycle every 12 months, or .083 cycles per observation. For simplicity, we measure frequency, $\omega$ , at cycles per time point and discuss the implications of certain frequencies in terms of the problem context. Of descriptive interest is the period of a time series, defined as the number of points in a cycle, i.e.,

$$
T = \frac {1}{\omega}. \tag {4.1}
$$

Hence, the predominant period of the Johnson & Johnson series is 1/.25 or 4 quarters per cycle, whereas the predominant period of the SOI series is 12 months per cycle.

# 4.2 Cyclical Behavior and Periodicity

As previously mentioned, we have already encountered the notion of periodicity in numerous examples in Chapters 1 and 2. The general notion of periodicity can be made more precise by introducing some terminology. In order to define the rate at which a series oscillates, we first define a cycle as one complete period of a sine or cosine function defined over a time interval of length $2 \pi$ . As in (1.5), we consider the periodic process

$$
x _ {t} = A \cos (2 \pi \omega t + \phi) \tag {4.2}
$$

for $t = 0 , \pm 1 , \pm 2 , . . .$ , where $\omega$ is a frequency index, defined in cycles per unit time with $A$ determining the height or amplitude of the function and $\phi$ , called

# 4.2: Periodicity

the phase, determining the start point of the cosine function. We can introduce random variation in this time series by allowing the amplitude and phase to vary randomly.

As discussed in Example 2.7, for purposes of data analysis, it is easier to use a trigonometric identity1 and write (4.2) as

$$
x _ {t} = U _ {1} \cos (2 \pi \omega t) + U _ {2} \sin (2 \pi \omega t), \tag {4.3}
$$

where $U _ { 1 } = A \cos \phi$ and $U _ { 2 } = - A \sin \phi$ are often taken to be normally distributed random variables. In this case, the amplitude is $A = \sqrt { U _ { 1 } ^ { 2 } + U _ { 2 } ^ { 2 } }$ and the phase is $\phi = \tan ^ { - 1 } ( - U _ { 2 } / U _ { 1 } )$ . From these facts we can show that if, and only if, in (4.2), $A$ and $\phi$ are independent random variables, where $A ^ { 2 }$ is chi-squared with 2 degrees of freedom, and $\phi$ is uniformly distributed on $( - \pi , \pi )$ , then $U _ { 1 }$ and $U _ { 2 }$ are independent, standard normal random variables (see Problem 4.2).

The above random process is also a function of its frequency, defined by the parameter $\omega$ . The frequency is measured in cycles per unit time, or in cycles per point in the above illustration. For $\omega = 1$ , the series makes one cycle per time unit; for $\omega = . 5 0$ , the series makes a cycle every two time units; for $\omega \ = \ . 2 5$ , every four units, and so on. In general, data that occurs at discrete time points will need at least two points to determine a cycle, so the highest frequency of interest is .5 cycles per point. This frequency is called the folding frequency and defines the highest frequency that can be seen in discrete sampling. Higher frequencies sampled this way will appear at lower frequencies, called aliases; an example is the way a camera samples a rotating wheel on a moving automobile in a movie, in which the wheel appears to be rotating at a different rate. For example, movies are recorded at 24 frames per second. If the camera is filming a wheel that is rotating at the rate of 24 cycles per second (or 24 Hertz), the wheel will appear to stand still (that’s about 110 miles per hour in case you were wondering).

Consider a generalization of (4.3) that allows mixtures of periodic series, with multiple frequencies and amplitudes.

$$
x _ {t} = \sum_ {k = 1} ^ {q} \left[ U _ {k 1} \cos (2 \pi \omega_ {k} t) + U _ {k 2} \sin (2 \pi \omega_ {k} t) \right], \tag {4.4}
$$

where $U _ { k 1 } , U _ { k 2 }$ , for $k = 1 , 2 , \ldots , q$ , are independent zero-mean random variables with variances $\sigma _ { k } ^ { 2 }$ , and the $\omega _ { k }$ are distinct frequencies. Notice that (4.4) exhibits the process as a sum of independent components, with variance $\sigma _ { k } ^ { 2 }$ for frequency $\omega _ { k }$ . Using the independence of the $U$ s and a trig identity,1 it is easy to show (Problem 4.3) that the autocovariance function of the process is

$$
\gamma (h) = \sum_ {k = 1} ^ {q} \sigma_ {k} ^ {2} \cos (2 \pi \omega_ {k} h), \tag {4.5}
$$

![](images/98315e08ce88f552c4f61fc59a5abc2f9df3e29ce6b5abd0d8feec55c36ccb77.jpg)

![](images/59cd0cc470fb3875ba8a5a332c7510925cee6cf522adc5c9e53747f9a708afed.jpg)

![](images/c136cfdc7e72e4ab789f21c062bf94e4d4a25e809d3c63849268fe2c02dede4d.jpg)

![](images/e58ed007b9273c973ebbd79ebab50e3f18ae4b1784b33ded9122d9faff4286c1.jpg)  
Figure 4.1 Periodic components and their sum as described in Example 4.1.

and we note the autocovariance function is the sum of periodic components with weights proportional to the variances $\sigma _ { k } ^ { 2 }$ . Hence, $x _ { t }$ is a mean-zero stationary processes with variance

$$
\gamma (0) = E \left(x _ {t} ^ {2}\right) = \sum_ {k = 1} ^ {q} \sigma_ {k} ^ {2}, \tag {4.6}
$$

which exhibits the overall variance as a sum of variances of each of the component parts.

# Example 4.1 A Periodic Series

Figure 4.1 shows an example of the mixture (4.4) with $q = 3$ constructed in the following way. First, for $t = 1 , \ldots , 1 0 0$ , we generated three series

$$
x _ {t 1} = 2 \cos (2 \pi t 6 / 1 0 0) + 3 \sin (2 \pi t 6 / 1 0 0)
$$

$$
x _ {t 2} = 4 \cos (2 \pi t 1 0 / 1 0 0) + 5 \sin (2 \pi t 1 0 / 1 0 0)
$$

$$
x _ {t 3} = 6 \cos (2 \pi t 4 0 / 1 0 0) + 7 \sin (2 \pi t 4 0 / 1 0 0)
$$

These three series are displayed in Figure 4.1 along with the corresponding frequencies and squared amplitudes. For example, the squared amplitude of $x _ { t 1 }$ is $2 ^ { 2 } + 3 ^ { 2 } = 1 3$ . Hence, the maximum and minimum values that $x _ { t 1 }$ will attain are $\pm \sqrt { 1 3 } = \pm 3 . 6 1$ .

# 4.2: Periodicity

Finally, we constructed

$$
x _ {t} = x _ {t 1} + x _ {t 2} + x _ {t 3}
$$

and this series is also displayed in Figure 4.1. We note that $x _ { t }$ appears to behave as some of the periodic series we saw in Chapters 1 and 2. The systematic sorting out of the essential frequency components in a time series, including their relative contributions, constitutes one of the main objectives of spectral analysis.

The R code to reproduce Figure 4.1 is

$> \mathrm { ~ t ~ } = \mathrm { ~ 1 } { : } 1 0 0$   
> x1 = 2*cos(2*pi*t*6/100) + 3*sin(2*pi*t*6/100)   
> x2 = 4*cos(2*pi*t*10/100) + 5*sin(2*pi*t*10/100)   
> x3 = 6*cos(2*pi*t*40/100) + 7*sin(2*pi*t*40/100)   
> x = x1 + x2 + x3   
> par(mfrow=c(2,2))  
> plot.ts(x1, ylim=c(-16,16), main $\cdot ^ { = }$ "freq=6/100, ampˆ2=13")   
> plot.ts(x2, ylim $\mathtt { \Omega } = \mathtt { \Omega }$ (-16,16), main $\cdot ^ { = }$ "freq=10/100, ampˆ2=41")   
> plot.ts(x3, ylim=c(-16,16), main $\cdot ^ { = }$ "freq=40/100, ampˆ2=85")   
> plot.ts(x, ylim $\mathtt { \mathtt { = c } }$ (-16,16), main $\cdot ^ { = }$ "sum")

# Example 4.2 The Scaled Periodogram for Example 4.1

In §2.3, Example 2.8, we introduced the periodogram as a way to discover the periodic components of a time series. Recall that the scaled periodogram is given by

$$
P (j / n) = \left(\frac {2}{n} \sum_ {t = 1} ^ {n} x _ {t} \cos (2 \pi t j / n)\right) ^ {2} + \left(\frac {2}{n} \sum_ {t = 1} ^ {n} x _ {t} \sin (2 \pi t j / n)\right) ^ {2} \tag {4.7}
$$

and it may regarded as a measure of the squared correlation of the data with sinusoids oscillating at a frequency of $\omega _ { j } = j / n$ , or $j$ cycles in $n$ time points. Recall that we are basically computing the regression of the data on the sinusoids varying at the fundamental frequencies, $j / n$ . As discussed in Example 2.8, the periodogram may be computed quickly using the fast Fourier transform (FFT), and there is no need to run repeated regressions.

The scaled periodogram of the data, $x _ { t }$ , simulated in Example 4.1 is shown in Figure 4.2, and it clearly identifies the three components $x _ { t 1 } , x _ { t 2 }$ , and $x _ { t 3 }$ of $x _ { t }$ . Moreover, the heights of the scaled periodogram shown in the figure are

$$
P (6 / 1 0 0) = 1 3, \quad P (1 0 / 1 0 0) = 4 1, \quad P (4 0 / 1 0 0) = 8 5
$$

and $P ( j / n ) = 0$ otherwise. These are exactly the values of the squared amplitudes of the components generated in Example 4.1. This outcome

![](images/6c6f74260efc30a26032bff5423e66e03d1124065c5f9d02b2b2a19031542c72.jpg)  
Figure 4.2 Periodogram of the data generated in Example 4.1.

suggests that the periodogram may provide some insight into the variance components, (4.6), of a real set of data.

Assuming the simulated data, x, were retained from the previous example, the R code to reproduce Figure 4.2 is

> P = abs(2*fft(x)/100)ˆ2   
> f = 0:50/100   
> plot(f, P[1:51], type="o", xlab="frequency",   
+ ylab="periodogram")

A curious reader may also wish to plot the entire periodogram over all fundamental frequencies between zero and one. A quick and easy way to do this is to use the command plot.ts(P).

If we consider the data $x _ { t }$ in Example 4.1 as a color (waveform) made up of primary colors $x _ { t 1 } , x _ { t 2 } , x _ { t 3 }$ at various strengths (amplitudes), then we might consider the periodogram as a prism that decomposes the color $x _ { t }$ into its primary colors (spectrum). Hence the term spectral analysis.

Another fact that may be of use in understanding the periodogram is that for any time series sample $x _ { 1 } , \ldots , x _ { n }$ , where $n$ is odd, we may write, exactly

$$
x _ {t} = a _ {0} + \sum_ {j = 1} ^ {(n - 1) / 2} \left[ a _ {j} \cos (2 \pi t j / n) + b _ {j} \sin (2 \pi t j / n) \right], \tag {4.8}
$$

for $t = 1 , \ldots , n$ and suitably chosen coefficients. If $n$ is even, the representation (4.8) can be modified by summing to $( n / 2 - 1 )$ and adding an additional component given by $a _ { n / 2 } \cos ( 2 \pi t 1 / 2 ) = a _ { n / 2 } ( - 1 ) ^ { t }$ . The crucial point here is that (4.8) is exact for any sample. Hence (4.4) may be thought of as an

# 4.3: Spectral Density

approximation to (4.8), the idea being that many of the coefficients in (4.8) may be close to zero. Recall from Example 2.8, that

$$
P (j / n) = a _ {j} ^ {2} + b _ {j} ^ {2}, \tag {4.9}
$$

so the scaled periodogram indicates which periodic components in (4.8) are large and which components are small. We also saw (4.9) in Example 4.2.

The periodogram, which was introduced in Schuster (1898) and used in Schuster (1906) for studying the periodicities in the sunspot series (shown in Figure 4.31 in the Problems section) is a sample based statistic. In Example 4.2, we discussed the fact that the periodogram may be giving us an idea of the variance components associated with each frequency, as presented in (4.6), of a time series. These variance components, however, are population parameters. The concepts of population parameters and sample statistics, as they relate to spectral analysis of time series can be generalized to cover stationary time series and that is the topic of the next section.

# 4.3 The Spectral Density

The idea that a time series is composed of periodic components, appearing in proportion to their underlying variances, is fundamental in the spectral representation given in Theorem C.2 of Appendix C. The result is quite technical because it involves stochastic integration; that is, integration with respect to a stochastic process. In nontechnical terms, Theorem C.2 says that (4.4) is approximately true for any stationary time series. In other words, any stationary time series may be thought of, approximately, as the random superposition of sines and cosines oscillating at various frequencies.

Given that (4.4) is approximately true for all stationary time series, the next question is whether a meaningful representation for its autocovariance function, like the one displayed in (4.5), also exists. The answer is yes, and this representation is given in Theorem C.1 of Appendix C. The following example will help explain the result.

# Example 4.3 A Periodic Stationary Process

Consider a periodic stationary random process given by (4.3), with a fixed frequency $\omega _ { 0 }$ , say,

$$
x _ {t} = U _ {1} \cos (2 \pi \omega_ {0} t) + U _ {2} \sin (2 \pi \omega_ {0} t),
$$

where $U _ { 1 }$ and $U _ { 2 }$ are independent zero-mean random variables with equal variance $\sigma ^ { 2 }$ . The number of time periods needed for the above series to

complete one cycle is exactly $1 / \omega _ { 0 }$ , and the process makes exactly $\omega _ { 0 }$ cycles per point for $t = 0 , \pm 1 , \pm 2 , . . . .$ . It is easily shown that2

$$
\begin{array}{l} \gamma (h) = \sigma^ {2} \cos (2 \pi \omega_ {0} h) = \frac {\sigma^ {2}}{2} \mathrm {e} ^ {- 2 \pi i \omega_ {0} h} + \frac {\sigma^ {2}}{2} \mathrm {e} ^ {2 \pi i \omega_ {0} h} \\ = \int_ {- 1 / 2} ^ {1 / 2} \mathrm {e} ^ {2 \pi i \omega h} d F (\omega) \\ \end{array}
$$

using a Riemann–Stieltjes integration, where $F ( \omega )$ is the function defined by

$$
F (\omega) = \left\{ \begin{array}{l l} 0 & \omega <   - \omega_ {0} \\ \sigma^ {2} / 2, & - \omega_ {0} \leq \omega <   \omega_ {0} \\ \sigma^ {2} & \omega \geq \omega_ {0}. \end{array} \right.
$$

The function $F ( \omega )$ behaves like a cumulative distribution function for a discrete random variable, except that $F ( \infty ) = \sigma ^ { 2 } = \gamma _ { x } ( 0 )$ instead of one. In fact, $F ( \omega )$ is a cumulative distribution function, not of probabilities, but rather of variances associated with the frequency $\omega _ { 0 }$ in an analysis of variance, with $F ( \infty )$ being the total variance of the process $x _ { t }$ . Hence, we term $F ( \omega )$ the spectral distribution function.

Theorem C.1 in Appendix C states that a representation such as the one given in Example 4.3 always exists for a stationary process. In particular, if $x _ { t }$ is stationary with autocovariance $\gamma ( h ) = E [ ( x _ { t + h } - \mu ) ( x _ { t } - \mu ) ]$ , then there exists a unique monotonically increasing function $F ( \omega )$ , called the spectral distribution function, that is bounded, with $F ( - \infty ) = F ( - 1 / 2 ) = 0$ , and $F ( \infty ) = F ( 1 / 2 ) = \gamma ( 0 )$ such that

$$
\gamma (h) = \int_ {- 1 / 2} ^ {1 / 2} \mathrm {e} ^ {2 \pi i \omega h} d F (\omega). \tag {4.10}
$$

A more important situation we use repeatedly is the one covered by Theorem C.3, where it is shown that, subject to absolute summability of the autocovariance, the spectral distribution function is absolutely continuous with $d F ( \omega ) = f ( \omega ) ~ d \omega$ , and the representation (4.10) becomes the motivation for the property given below.

# Property P4.1: The Spectral Density

If the autocovariance function, $\gamma ( h )$ , of a stationary process satisfies

$$
\sum_ {h = - \infty} ^ {\infty} | \gamma (h) | <   \infty , \tag {4.11}
$$

# 4.3: Spectral Density

then it has the representation

$$
\gamma (h) = \int_ {- 1 / 2} ^ {1 / 2} \mathrm {e} ^ {2 \pi i \omega h} f (\omega) d \omega \quad h = 0, \pm 1, \pm 2, \dots \tag {4.12}
$$

as the inverse transform of the spectral density, which has the representation

$$
f (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma (h) \mathrm {e} ^ {- 2 \pi i \omega h} - 1 / 2 \leq \omega \leq 1 / 2. \tag {4.13}
$$

This spectral density is the analogue of the probability density function; the fact that $\gamma ( h )$ is non-negative definite ensures

$$
f (\omega) \geq 0
$$

for all $\omega$ (see Appendix C, Theorem C.3 for details). It follows immediately from (4.12) and (4.13) that

$$
f (\omega) = f (- \omega)
$$

and

$$
f (\omega + 1) = f (\omega),
$$

verifying the spectral density is an even function of period one. Because of the evenness, we will typically only plot $f ( \omega )$ for $\omega \geq 0$ . In addition, putting $h = 0$ in (4.12) yields

$$
\gamma (0) = \operatorname {v a r} \left(x _ {t}\right) = \int_ {- 1 / 2} ^ {1 / 2} f (\omega) d \omega ,
$$

which expresses the total variance as the integrated spectral density over all of the frequencies. We show later on, that a linear filter can isolate the variance in certain frequency intervals or bands.

Analogous to probability theory, $\gamma ( h )$ in (4.12) is the characteristic function of the spectral density $f ( \omega )$ in (4.13). These facts should make it clear that, when the condition of Property P4.1 is satisfied, the autocovariance function $\gamma ( h )$ and the spectral density function $f ( \omega )$ contain the same information. That information, however, is expressed in different ways. The autocovariance function expresses information in terms of lags, whereas the spectral density expresses the same information in terms of cycles. Some problems are easier to work with when considering lagged information and we would tend to handle those problems in the time domain. Nevertheless, other problems are easier to work with when considering periodic information and we would tend to handle those problems in the spectral domain.

We also mention, at this point, that we have been focusing on the frequency $\omega$ , expressed in cycles per point rather than the more common (in statistics)

alternative $\lambda = 2 \pi \omega$ that would give radians per point. Finally, the absolute summability condition, (4.11), is not satisfied by (4.5), the example that we have used to introduce the idea of a spectral representation. The condition, however, is satisfied for ARMA models.

We note that the autocovariance function, $\gamma ( h )$ , in (4.12) and the spectral density, $f ( \omega )$ , in (4.13) are Fourier transform pairs. In general, we have the following definition.

Definition 4.1 For a general function $\{ a _ { t } ; t = 0 , \pm 1 , \pm 2 , . . . \}$ satisfying the absolute summability condition

$$
\sum_ {t = - \infty} ^ {\infty} \left| a _ {t} \right| <   \infty , \tag {4.14}
$$

we define a Fourier transform pair to be of the form

$$
A (\omega) = \sum_ {t = - \infty} ^ {\infty} a _ {t} \mathrm {e} ^ {- 2 \pi i \omega t} \tag {4.15}
$$

and

$$
a _ {t} = \int_ {- 1 / 2} ^ {1 / 2} A (\omega) \mathrm {e} ^ {2 \pi i \omega t} d \omega . \tag {4.16}
$$

The use of (4.12) and (4.13) as Fourier transform pairs is fundamental in the study of stationary discrete time processes. Under the summability condition (4.11), the Fourier transform pair (4.12) and (4.13) will exist and this relation is unique. If $f ( \omega )$ and $g ( \omega )$ are two spectral densities for which

$$
\int_ {- 1 / 2} ^ {1 / 2} f (\omega) \mathrm {e} ^ {2 \pi i \omega h} d \omega = \int_ {- 1 / 2} ^ {1 / 2} g (\omega) \mathrm {e} ^ {2 \pi i \omega h} d \omega \tag {4.17}
$$

for all $h = 0 , \pm 1 , \pm 2 , . . .$ , then

$$
f (\omega) = g (\omega) \tag {4.18}
$$

almost everywhere.

It is illuminating to examine the spectral density for the series that we have looked at in earlier discussions.

# Example 4.4 White Noise Series

As a simple example, consider the theoretical power spectrum of a sequence of uncorrelated random variables, $w _ { t }$ , with variance $\sigma _ { w } ^ { 2 }$ . A simulated set of data is displayed in the top of Figure 1.8. Because the autocovariance function was computed in Example 1.16 as $\gamma _ { w } ( h ) = \sigma _ { w } ^ { 2 }$ for $h = 0$ , and zero, otherwise, it follows from (4.13) that

$$
f _ {w} (\omega) = \sigma_ {w} ^ {2}
$$

![](images/4a2bc14a21270ef288d7a39fc0ec8d81aa82360d443b4071e843742f7a59616b.jpg)  
Figure 4.3 Theoretical spectra of white noise (top), smoothed white noise (middle), and a second-order autoregressive process (bottom).

for $- 1 / 2 \le \omega \le 1 / 2$ with the resulting equal power at all frequencies. This property is seen in the realization, which seems to contain all different frequencies in a roughly equal mix. In fact, the name white noise comes from the analogy to white light, which contains all frequencies in the color spectrum. Figure 4.3 shows a plot of the white noise spectrum for $\sigma _ { w } ^ { 2 } = 1$ .

# Example 4.5 A Simple Moving Average

A series that does not have an equal mix of frequencies is the smoothed white noise series shown in the bottom panel of Figure 1.8. Specifically, we construct the three-point moving average series, defined by

$$
v _ {t} = \frac {1}{3} \left(w _ {t - 1} + w _ {t} + w _ {t + 1}\right).
$$

It is clear from the sample realization that the series has less of the higher or faster frequencies, and we calculate its power spectrum to verify this observation. We have previously computed the autocovariance of this

process in Example 1.17, obtaining

$$
\gamma_ {v} (h) = \frac {\sigma_ {w} ^ {2}}{9} \left(3 - | h |\right)
$$

for $| h | \leq 2$ and $\gamma _ { y } ( h ) = 0$ for $| h | > 2$ . Then, using (4.13) gives

$$
\begin{array}{l} f _ {v} (\omega) = \sum_ {h = - 2} ^ {2} \gamma_ {y} (h) \mathrm {e} ^ {- 2 \pi i \omega h} \\ = \frac {\sigma_ {w} ^ {2}}{9} \left(\mathrm {e} ^ {- 4 \pi i \omega} + \mathrm {e} ^ {4 \pi i \omega}\right) + \frac {2 \sigma_ {w} ^ {2}}{9} \left(\mathrm {e} ^ {- 2 \pi i \omega} + \mathrm {e} ^ {2 \pi \omega}\right) + \frac {3 \sigma_ {w} ^ {2}}{9} \\ = \frac {\sigma_ {w} ^ {2}}{9} \left[ 3 + 4 \cos (2 \pi \omega) + 2 \cos (4 \pi \omega) \right]. \\ \end{array}
$$

Plotting the spectrum for $\sigma _ { w } ^ { 2 } = 1$ , as in Figure 4.3, shows the lower frequencies near zero have greater power and the higher or faster frequencies, say, $\omega > . 2$ , tend to have less power.

# Example 4.6 A Second-Order Autoregressive Series

As a final example, we consider the spectrum of an AR(2) series of the form

$$
x _ {t} - \phi_ {1} x _ {t - 1} - \phi_ {2} x _ {t - 2} = w _ {t},
$$

for the special case $\phi _ { 1 } = 1$ and $\phi _ { 2 } ~ = ~ - . 9$ . Recall Example 1.10 and Figure 1.9, which shows a sample realization of such a process for $\sigma _ { w } = 1$ . We note the data exhibit a strong periodic component that makes a cycle about every six points. First, computing the autocovariance function of the right side and equating it to the autocovariance on the left yields

$$
\begin{array}{l} \gamma_ {w} (h) = E \left[ \left(x _ {t + h} - \phi_ {1} x _ {t + h - 1} - \phi_ {2} x _ {t + h - 2}\right) \left(x _ {t} - \phi_ {1} x _ {t - 1} - \phi_ {2} x _ {t - 2}\right) \right] \\ = \left[ 1 + \phi_ {1} ^ {2} + \phi_ {2} ^ {2} \right] \gamma_ {x} (h) + \left(\phi_ {1} \phi_ {2} - \phi_ {1}\right) \left[ \gamma_ {x} (h + 1) + \gamma_ {x} (h - 1) \right] \\ - \phi_ {2} \left[ \gamma_ {x} (h + 2) + \gamma_ {x} (h - 2) \right] \\ = 2. 8 1 \gamma_ {x} (h) - 1. 9 0 [ \gamma_ {x} (h + 1) + \gamma_ {x} (h - 1) ] \\ + . 9 0 \left[ \gamma_ {x} (h + 2) + \gamma_ {x} (h - 2) \right], \\ \end{array}
$$

where we have substituted the values of $\phi _ { 1 } = 1$ and $\phi _ { 2 } = - . 9$ in the equation. Now, substituting the spectral representation (4.12) for $\gamma _ { x } ( h )$ in the above equation yields

$$
\begin{array}{l} \gamma_ {w} (h) = \int_ {- 1 / 2} ^ {1 / 2} [ 2. 8 1 - 1. 9 0 (\mathrm {e} ^ {2 \pi i \omega} + \mathrm {e} ^ {- 2 \pi i \omega}) \\ + . 9 0 \left(\mathrm {e} ^ {4 \pi i \omega} + \mathrm {e} ^ {- 4 \pi i \omega}\right) ] \mathrm {e} ^ {2 \pi i \omega h} f _ {x} (\omega) d \omega \\ { = } { \int _ { - 1 / 2 } ^ { 1 / 2 } \left[ 2 . 8 1 - 3 . 8 0 \cos ( 2 \pi \omega ) + 1 . 8 0 \cos ( 4 \pi \omega ) \right] \mathrm { e } ^ { 2 \pi i \omega h } f _ { x } ( \omega ) d \omega . } \\ \end{array}
$$

If the spectrum of the white noise process is $g _ { w } ( \omega )$ , the uniqueness of the Fourier transform allows us to identify

$$
g _ {w} (\omega) = \left[ 2. 8 1 - 3. 8 0 \cos (2 \pi \omega) + 1. 8 0 \cos (4 \pi \omega) \right] f _ {x} (\omega).
$$

But, as we have already seen, $g _ { w } ( \omega ) = \sigma _ { w } ^ { 2 }$ , from which we deduce that

$$
f _ {x} (\omega) = \frac {\sigma_ {w} ^ {2}}{2 . 8 1 - 3 . 8 0 \cos (2 \pi \omega) + 1 . 8 0 \cos (4 \pi \omega)}
$$

is the spectrum of the autoregressive series. Setting $\sigma _ { w } = 1$ , Figure 4.3 displays $f _ { x } ( \omega )$ and shows a strong power component at about $\omega = . 1 6$ cycles per point or a period between six and seven cycles per point and very little power at other frequencies. In this case, modifying the white noise series by applying the second-order AR operator has concentrated the power or variance of the resulting series in a very narrow frequency band.

The above examples have been given primarily to motivate the use of the power spectrum for describing the theoretical variance fluctuations of a stationary time series. Indeed, the interpretation of the spectral density function as the variance of the time series over a given frequency band gives us the intuitive explanation for its physical meaning. The plot of the function $f ( \omega )$ over the frequency argument $\omega$ can even be thought of as an analysis of variance, in which the columns or block effects are the frequencies, indexed by $\omega$ .

# 4.4 Periodogram and Discrete Fourier Transform

We are now ready to tie together the periodogram, which is the sample-based concept presented in §4.2, with the spectral density, which is the populationbased concept of 4.3.

Definition 4.2 Given data $x _ { 1 } , \ldots , x _ { n }$ , we define the discrete Fourier transform (DFT) to be

$$
d \left(\omega_ {j}\right) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} e ^ {- 2 \pi i \omega_ {j} t} \tag {4.19}
$$

for $j = 0 , 1 , \ldots , n - 1$ , where the frequencies $\omega _ { j } = j / n$ are called the Fourier or fundamental frequencies.

If $n$ is a highly composite integer (i.e., it has many factors), the DFT can be computed by the fast Fourier transform (FFT) introduced in Cooley and Tukey (1965). Also, different packages scale the FFT differently, so it is a good idea to consult the documentation. R computes the DFT defined in (4.19) without the factor $n ^ { - 1 / 2 }$ , but with an additional factor of $e ^ { 2 \pi i \omega _ { j } }$ that

can be ignored because we will be interested in the squared modulus of the DFT. Sometimes it is helpful to exploit the inversion result for DFTs which shows the linear transformation is one-to-one. For the inverse DFT we have,

$$
x _ {t} = n ^ {- 1 / 2} \sum_ {j = 0} ^ {n - 1} d \left(\omega_ {j}\right) \mathrm {e} ^ {2 \pi i \omega_ {j} t} \tag {4.20}
$$

for $t = 1 , \ldots , n$ . The following example shows how to calculate the DFT and its inverse in R for the data set $\{ 1 , 2 , 3 , 4 \}$ ; note that R writes a complex number $z = a + i b$ as $\tt a + b i$ .

$$
\begin{array}{l} > x = 1: 4 \\ > d f t = f f t (x) / \operatorname {s q r t} (4) \\ > d f t \\ \end{array}
$$

$$
\begin{array}{l} [ 1 ] \quad 5 + 0 i - 1 + 1 i - 1 + 0 i - 1 - 1 i \\ > \text {i d f t} = \text {f f t} (\text {d f t}, \text {i n v e r s e} = \mathrm {T}) / \text {s q r t} (4) \\ > \text {i d f t} \\ [ 1 ] \quad 1 + 0 i \quad 2 + 0 i \quad 3 + 0 i \quad 4 + 0 i \\ \end{array}
$$

We now define the periodogram as the squared modulus3 of the DFT.

Definition 4.3 Given data $x _ { 1 } , \ldots , x _ { n }$ , we define the periodogram to be

$$
I (\omega_ {j}) = | d (\omega_ {j}) | ^ {2} \tag {4.21}
$$

for $j = 0 , 1 , 2 , \ldots , n - 1$ .

Note that $I ( 0 ) = n \bar { x } ^ { 2 }$ , where $x$ is the sample mean. In addition, because $\begin{array} { r } { \sum _ { t = 1 } ^ { n } \exp ( - 2 \pi i \omega _ { j } t ) = 0 } \end{array}$ for $j \neq 0$ ,4 we can write the DFT as

$$
d \left(\omega_ {j}\right) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \left(x _ {t} - \bar {x}\right) e ^ {- 2 \pi i \omega_ {j} t} \tag {4.22}
$$

for $j \neq 0$ . Thus, for $j \neq 0$ ,

$$
\begin{array}{l} I (\omega_ {j}) = | d (\omega_ {j}) | ^ {2} = n ^ {- 1} \sum_ {t = 1} ^ {n} \sum_ {s = 1} ^ {n} (x _ {t} - \bar {x}) (x _ {s} - \bar {x}) e ^ {- 2 \pi i \omega_ {j} (t - s)} \\ { = } { n ^ { - 1 } \sum _ { h = - ( n - 1 ) } ^ { n - 1 } \sum _ { t = 1 } ^ { n - | h | } ( x _ { t + | h | } - \bar { x } ) ( x _ { t } - \bar { x } ) e ^ { - 2 \pi i \omega _ { j } h } } \\ = \sum_ {h = - (n - 1)} ^ {n - 1} \widehat {\gamma} (h) e ^ {- 2 \pi i \omega_ {j} h} \tag {4.23} \\ \end{array}
$$

# 4.4: DFT, Periodogram

where we have put $h = t - s$ , with $\widehat { \gamma } ( h )$ as given in (1.36).

Recall, $P ( \omega _ { j } ) = ( 4 / n ) I ( \omega _ { j } )$ where $P ( \omega _ { j } )$ is the scaled periodogram defined in (4.7). Henceforth we will work with $I ( \omega _ { j } )$ instead of $P ( \omega _ { j } )$ . Note that, in view of (4.23), $I ( \omega _ { j } )$ in (4.21) is the sample version of $f ( \omega _ { j } )$ given in (4.13). That is, we may think of the periodogram, $I ( \omega _ { j } )$ , as the “sample spectral density” of $x _ { t }$ .

It is sometimes useful to work with the real and imaginary parts of the DFT individually. To this end, we define the following transforms.

Definition 4.4 Given data $x _ { 1 } , \ldots , x _ { n }$ , we define the cosine transform

$$
d _ {c} \left(\omega_ {j}\right) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} \cos \left(2 \pi \omega_ {j} t\right) \tag {4.24}
$$

and the sine transform

$$
d _ {s} \left(\omega_ {j}\right) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} \sin \left(2 \pi \omega_ {j} t\right) \tag {4.25}
$$

where $\omega _ { j } = j / n$ for $j = 0 , 1 , \ldots , n - 1$ .

We note that $d ( \omega _ { j } ) = d _ { c } ( \omega _ { j } ) - i d _ { s } ( \omega _ { j } )$ and hence

$$
I (\omega_ {j}) = d _ {c} ^ {2} (\omega_ {j}) + d _ {s} ^ {2} (\omega_ {j}). \tag {4.26}
$$

We have also discussed the fact that spectral analysis can be thought of as an analysis of variance. The next example examines this notion.

# Example 4.7 Spectral ANOVA

Let $x _ { 1 } , \ldots , x _ { n }$ be a sample of size $n$ , where for ease, $n$ is odd. Then, recalling Example 2.8 and the discussion around (4.8) and (4.9),

$$
x _ {t} = a _ {0} + \sum_ {j = 1} ^ {m} \left[ a _ {j} \cos \left(2 \pi \omega_ {j} t\right) + b _ {j} \sin \left(2 \pi \omega_ {j} t\right) \right], \tag {4.27}
$$

where $m = ( n - 1 ) / 2$ , is exact for $t = 1 , \ldots , n$ . In particular, using multiple regression formulas, we have $a _ { 0 } = x$ ,

$$
a _ {j} = \frac {2}{n} \sum_ {t = 1} ^ {n} x _ {t} \cos (2 \pi \omega_ {j} t) = \frac {2}{\sqrt {n}} d _ {c} (\omega_ {j})
$$

$$
b _ {j} = \frac {2}{n} \sum_ {t = 1} ^ {n} x _ {t} \sin (2 \pi \omega_ {j} t) = \frac {2}{\sqrt {n}} d _ {s} (\omega_ {j}).
$$

Hence, we may write

$$
(x _ {t} - \bar {x}) = \frac {2}{\sqrt {n}} \sum_ {j = 1} ^ {m} [ d _ {c} (\omega_ {j}) \cos (2 \pi \omega_ {j} t) + d _ {s} (\omega_ {j}) \sin (2 \pi \omega_ {j} t) ]
$$

for $t = 1 , \ldots , n$ . Squaring both sides and summing we have5

$$
\sum_ {t = 1} ^ {n} (x _ {t} - \bar {x}) ^ {2} = 2 \sum_ {j = 1} ^ {m} \left[ d _ {c} ^ {2} (\omega_ {j}) + d _ {s} ^ {2} (\omega_ {j}) \right] = 2 \sum_ {j = 1} ^ {m} I (\omega_ {j}).
$$

Thus, we have partitioned the sum of squares into harmonic components represented by frequency $\omega _ { j }$ with the periodogram, $I ( \omega _ { j } )$ , being the mean square regression. This leads to the ANOVA table:

<table><tr><td>Source</td><td>df</td><td>SS</td><td>MS</td></tr><tr><td>ω1</td><td>2</td><td>2I(ω1)</td><td>I(ω1)</td></tr><tr><td>ω2</td><td>2</td><td>2I(ω2)</td><td>I(ω2)</td></tr><tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr><tr><td>ωm</td><td>2</td><td>2I(ωm)</td><td>I(ωm)</td></tr><tr><td>Total</td><td>n-1</td><td>∑t=1n(xt-х)2</td><td></td></tr></table>

This decomposition means that if the data contain some strong periodic components, then the periodogram values corresponding to those frequencies (or near those frequencies) will be large. On the other hand, the corresponding values of the periodogram will be small for periodic components not present in the data. The following is an R example to help explain this concept. We consider $n = 5$ observations given by $x _ { 1 } = 1 , x _ { 2 } = 2 , x _ { 3 } = 3 , x _ { 4 } = 2 , x _ { 5 } = 1$ . Note that the data complete one cycle, but not in a sinusoidal way. Thus, we should expect the $\omega _ { 1 } = 1 / 5$ component to be relatively large but not exhaustive, and the $\omega _ { 2 } = 2 / 5$ component to be small.

<table><tr><td colspan="7">&gt; x = c(1,2,3,2,1)</td></tr><tr><td colspan="7">&gt; t = 1:5</td></tr><tr><td colspan="7">&gt; c1 = cos(2*pi*t*1/5)</td></tr><tr><td colspan="7">&gt; s1 = sin(2*pi*t*1/5)</td></tr><tr><td colspan="7">&gt; c2 = cos(2*pi*t*2/5)</td></tr><tr><td colspan="7">&gt; s2 = sin(2*pi*t*2/5)</td></tr><tr><td colspan="7">&gt; creg = lm(x~c1+s1+c2+s2)</td></tr><tr><td colspan="7">&gt; anova(creg) # partial output and combined ANOVA shown</td></tr><tr><td></td><td colspan="3">#</td><td colspan="3">ANOVA</td></tr><tr><td></td><td>Df</td><td>Sum Sq #</td><td>Source</td><td>df</td><td>SS</td><td>MS</td></tr><tr><td>c1</td><td>1</td><td>1.79443 #</td><td></td><td></td><td></td><td></td></tr><tr><td>s1</td><td>1</td><td>0.94721 #</td><td>freq=1/5</td><td>2</td><td>2.74164</td><td>1.37082</td></tr><tr><td>c2</td><td>1</td><td>0.00557 #</td><td></td><td></td><td></td><td></td></tr><tr><td>s2</td><td>1</td><td>0.05279 #</td><td>freq=2/5</td><td>2</td><td>0.05836</td><td>0.02918</td></tr><tr><td>Residuals</td><td>0</td><td>0.00000 #</td><td></td><td></td><td></td><td></td></tr></table>

# 4.4: DFT, Periodogram

```txt
> abs(fft(x))^2/5 # the periodogram (as a check)  
[1] 16.2000 1.3708 0.02918 0.02918 1.3708  
> # I(0) I(1/5) I(2/5) I(3/5) I(4/5) 
```

Note that $\bar { x } = 1 . 8$ so $I ( 0 ) = 5 \times 1 . 8 ^ { 2 } = 1 6 . 2$ . Also, as a check

$$
I (1 / 5) = [ \mathrm {S S} (\mathrm {c} 1) + \mathrm {S S} (\mathrm {s} 1) ] / 2 = (1. 7 9 4 4 3 +. 9 4 7 2 1) / 2 = 1. 3 7 0 8,
$$

$$
I (2 / 5) = [ \mathrm {S S} (\mathrm {c} 2) + \mathrm {S S} (\mathrm {s} 2) ] / 2 = (. 0 0 5 5 7 +. 0 5 2 7 9) / 2 = . 0 2 9 1 8,
$$

and $I ( j / 5 ) = I ( 1 - j / 5 )$ , for $j = 3 , 4$ . Finally, we note that the sum of squares associated with the residuals is zero, indicating an exact fit.

We are now ready to present some large sample properties of the periodogram. First, let $\mu$ be the mean of a stationary process $x _ { t }$ with absolutely summable autocovariance function $\gamma ( h )$ and spectral density $f ( \omega )$ . We can use the same argument as in (4.23), replacing $x$ by $\mu$ in (4.22), to write

$$
I \left(\omega_ {j}\right) = n ^ {- 1} \sum_ {h = - (n - 1)} ^ {n - 1} \sum_ {t = 1} ^ {n - | h |} \left(x _ {t + | h |} - \mu\right) \left(x _ {t} - \mu\right) e ^ {- 2 \pi i \omega_ {j} h} \tag {4.28}
$$

where $\omega _ { j }$ is a non-zero fundamental frequency. Taking expectation in (4.28) we obtain

$$
E \left[ I \left(\omega_ {j}\right) \right] = \sum_ {h = - (n - 1)} ^ {n - 1} \left(\frac {n - | h |}{n}\right) \gamma (h) e ^ {- 2 \pi i \omega_ {j} h}. \tag {4.29}
$$

For any given $\omega \neq 0$ , choose a fundamental frequency $\omega _ { j : n }  \omega$ as $n \to \infty$ ,6 from which it follows by (4.29) that

$$
E \left[ I \left(\omega_ {j: n}\right)\right]\rightarrow f (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma (h) e ^ {- 2 \pi i h \omega} \tag {4.30}
$$

as $n \to \infty$ .7 In other words, under absolute summability of $\gamma ( h )$ , the spectral density is the long-term average of the periodogram.

To examine the asymptotic distribution of the periodogram, we note that if $x _ { t }$ is a normal time series, the sine and cosine transforms will also be jointly normal, because they are linear combinations of the jointly normal random variables $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ . In that case, the assumption that the covariance function satisfies the condition

$$
\theta = \sum_ {h = - \infty} ^ {\infty} | h | | \gamma (h) | <   \infty \tag {4.31}
$$

is enough to obtain simple large sample approximations for the variances and covariances. Using the same argument used to develop (4.29) we have

$$
\operatorname {c o v} \left[ d _ {c} \left(\omega_ {j}\right), d _ {c} \left(\omega_ {k}\right) \right] = \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} \gamma (s - t) \cos \left(2 \pi \omega_ {j} s\right) \cos \left(2 \pi \omega_ {k} t\right), \tag {4.32}
$$

$$
\operatorname {c o v} [ d _ {c} (\omega_ {j}), d _ {s} (\omega_ {k}) ] = \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} \gamma (s - t) \cos (2 \pi \omega_ {j} s) \sin (2 \pi \omega_ {k} t), \qquad (4. 3 3)
$$

and

$$
\operatorname {c o v} \left[ d _ {s} \left(\omega_ {j}\right), d _ {s} \left(\omega_ {k}\right) \right] = \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} \gamma (s - t) \sin \left(2 \pi \omega_ {j} s\right) \sin \left(2 \pi \omega_ {k} t\right), \tag {4.34}
$$

where the variance terms are obtained by setting $\omega _ { j } = \omega _ { k }$ in (4.32) and (4.34). In Appendix C, §C.2, we show the terms in (4.32)-(4.34) have interesting properties under assumption (4.31), namely, for $\omega _ { j } , \omega _ { k } \neq 0$ or $1 / 2$ ,

$$
\operatorname {c o v} \left[ d _ {c} \left(\omega_ {j}\right), d _ {c} \left(\omega_ {k}\right) \right] = \left\{ \begin{array}{l l} f \left(\omega_ {j}\right) / 2 + \epsilon_ {n}, & \omega_ {j} = \omega_ {k} \\ \epsilon_ {n}, & \omega_ {j} \neq \omega_ {k} \end{array} \right. \tag {4.35}
$$

$$
\operatorname {c o v} \left[ d _ {s} \left(\omega_ {j}\right), d _ {s} \left(\omega_ {k}\right) \right] = \left\{ \begin{array}{l l} f \left(\omega_ {j}\right) / 2 + \epsilon_ {n}, & \omega_ {j} = \omega_ {k} \\ \epsilon_ {n}, & \omega_ {j} \neq \omega_ {k} \end{array} \right. \tag {4.36}
$$

and

$$
\operatorname {c o v} \left[ d _ {c} \left(\omega_ {j}\right), d _ {s} \left(\omega_ {k}\right) \right] = \epsilon_ {n}, \tag {4.37}
$$

where the error term $\epsilon _ { n }$ in the approximations can be bounded,

$$
\left| \epsilon_ {n} \right| \leq \theta / n, \tag {4.38}
$$

and $\theta$ is given by (4.31). If $\omega _ { j } = \omega _ { k } = 0$ or 1/2 in (4.35), the multiplier 1/2 disappears; note that $d _ { s } ( 0 ) = d _ { s } ( 1 / 2 ) = 0$ , so (4.36) does not apply.

# Example 4.8 Covariance of Sines and Cosines for an MA Process

For the three-point moving average series of Example 4.5, the theoretical spectrum is shown in Figure 4.3. For $n = 2 5 6$ points, the theoretical covariance matrix of the vector

$$
\boldsymbol {d} = \left(d _ {c} \left(\omega_ {2 6}\right), d _ {s} \left(\omega_ {2 6}\right), d _ {c} \left(\omega_ {2 7}\right), d _ {s} \left(\omega_ {2 7}\right)\right) ^ {\prime}
$$

is

$$
\operatorname {c o v} (\boldsymbol {d}) = \left( \begin{array}{c c c c} . 3 7 5 2 & -. 0 0 0 9 & -. 0 0 2 2 & -. 0 0 1 0 \\ -. 0 0 0 9 & . 3 7 7 7 & -. 0 0 0 9 & . 0 0 0 3 \\ -. 0 0 2 2 & -. 0 0 0 9 & . 3 6 6 7 & -. 0 0 1 0 \\ -. 0 0 1 0 & . 0 0 0 3 & -. 0 0 1 0 & . 3 6 9 2 \end{array} \right).
$$

# 4.4: DFT, Periodogram

The diagonal elements can be compared with the theoretical spectral values of .7548 for the spectrum at frequency $\omega _ { 2 6 } = . 1 0 2$ , and of .7378 for the spectrum at $\omega _ { 2 7 } = . 1 0 5$ . Hence, the cosine and sine transforms produce nearly uncorrelated variables with variances approximately equal to one half of the theoretical spectrum. For this particular case, the uniform bound is determined from $\theta = 8 / 9$ , yielding $| \epsilon _ { 2 5 6 } | \le . 0 0 3 5$ for the bound on the approximation error.

If $x _ { t } \sim \operatorname { i i d } ( 0 , \sigma ^ { 2 } )$ , then it follows from (4.31)-(4.37) and the central limit theorem8 that

$$
d _ {c} \left(\omega_ {j: n}\right) \sim \mathrm {A N} \left(0, \sigma^ {2} / 2\right) \quad \text {a n d} \quad d _ {s} \left(\omega_ {j: n}\right) \sim \mathrm {A N} \left(0, \sigma^ {2} / 2\right) \tag {4.39}
$$

jointly and independently, and independent of $d _ { c } ( \omega _ { k : n } )$ and $d _ { s } ( \omega _ { k : n } )$ provided $\omega _ { j : n } \\to \omega _ { 1 }$ and $\omega _ { k : n } \to \omega _ { 2 }$ where $0 < \omega _ { 1 } \neq \omega _ { 2 } < 1 / 2$ . We note that in this case, $f ( \omega ) = \sigma ^ { 2 }$ . In view of (4.39), it follows immediately that as $n \to \infty$ ,

$$
\frac {2 I \left(\omega_ {j : n}\right)}{\sigma^ {2}} \stackrel {d} {\rightarrow} \chi_ {2} ^ {2} \quad \text {a n d} \quad \frac {2 I \left(\omega_ {k : n}\right)}{\sigma^ {2}} \stackrel {d} {\rightarrow} \chi_ {2} ^ {2} \tag {4.40}
$$

with $I ( \omega _ { j : n } )$ and $I ( \omega _ { k : n } )$ being asymptotically independent, where $\chi _ { \nu } ^ { 2 }$ denotes a chi-squared random variable with $\nu$ degrees of freedom.

Using the central limit theory of C.2, it is fairly easy to extend the results of the iid case to the case of a linear process.

Property P4.2: Distribution of the Periodogram Ordinates If

$$
x _ {t} = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} w _ {t - j}, \quad \sum_ {j = - \infty} ^ {\infty} | \psi_ {j} | <   \infty \tag {4.41}
$$

where $w _ { t } \sim i i d ( 0 , \sigma _ { w } ^ { 2 } )$ , and (4.31) holds, then for any collection of m distinct frequencies $\omega _ { j }$ with $\omega _ { j : n }  \omega _ { j }$

$$
\frac {2 I \left(\omega_ {j : n}\right)}{f \left(\omega_ {j}\right)} \stackrel {d} {\rightarrow} \mathrm {i i d} \chi_ {2} ^ {2} \tag {4.42}
$$

provided $f ( \omega _ { j } ) > 0$ , for $j = 1 , \ldots , m$ .

This result is stated more precisely in Theorem C.7 of §C.3. Other approaches to large sample normality of the periodogram ordinates are in terms of cumulants, as in Brillinger (1981), or in terms of mixing conditions, such as in Rosenblatt (1956). We adopt the approach here used by Hannan (1970), Fuller (1995), and Brockwell and Davis (1991).

The distributional result (4.42) can be used to derive an approximate confidence interval for the spectrum in the usual way. Let $\chi _ { \nu } ^ { 2 } ( \alpha )$ denote the lower $\alpha$ probability tail for the chi-squared distribution with $\nu$ degrees of freedom; that is,

$$
\Pr \left\{\chi_ {\nu} ^ {2} \leq \chi_ {\nu} ^ {2} (\alpha) \right\} = \alpha . \tag {4.43}
$$

Then, an approximate $1 0 0 ( 1 - \alpha ) \%$ confidence interval for the spectral density function would be of the form

$$
\frac {2 I \left(\omega_ {j : n}\right)}{\chi_ {2} ^ {2} (1 - \alpha / 2)} \leq f (\omega) \leq \frac {2 I \left(\omega_ {j : n}\right)}{\chi_ {2} ^ {2} (\alpha / 2)} \tag {4.44}
$$

Often, nonstationary trends are present that should be eliminated before computing the periodogram. Trends introduce extremely low frequency components in the periodogram that tend to obscure the appearance at higher frequencies. For this reason, it is usually conventional to center the data prior to a spectral analysis using either mean-adjusted data of the form $x _ { t } - \bar { x }$ to eliminate the zero or d-c component or to use detrended data of the form $x _ { t } - { \widehat { \beta } } _ { 1 } - { \widehat { \beta } } _ { 2 } t$ to eliminate the term that will be considered a half cycle by the spectral analysis. Note that higher order polynomial regressions in $t$ or nonparametric smoothing (linear filtering) could be used in cases where the trend is nonlinear.

As previously indicated, it is often convenient to calculate the DFTs, and hence the periodogram, using the fast Fourier transform algorithm. The FFT utilizes a number of redundancies in the calculation of the DFT when $n$ is highly composite; that is, an integer with many factors of $2 , 3$ , or 5, the best case being when $n = 2 ^ { p }$ is a factor of 2. Details may be found in Cooley and Tukey (1965). To accommodate this property, we can pad the centered (or detrended) data of length $n$ to the next highly composite integer $n ^ { \prime }$ by adding zeros, i.e., setting $x _ { n + 1 } ^ { c } = x _ { n + 2 } ^ { c } = \cdot \cdot \cdot = x _ { n ^ { \prime } } ^ { c } = 0$ , where $\ v { x } _ { t } ^ { c }$ denotes the centered data. This means that the fundamental frequency ordinates will be $\omega _ { j } = j / n ^ { \prime }$ instead of $j / n$ . We illustrate by considering the periodogram of the SOI and Recruitment series, as has been given in Figure 1.5 of Chapter 1. Recall that the series are monthly series and $n = 4 5 3$ . To find $n ^ { \prime }$ in R, use the command nextn(453) to see that $n ^ { \prime } = 4 8 0$ will be used in the spectral analyses by default (use help(spec.pgram) to see how to override this default).

# Example 4.9 Periodogram of SOI and Recruitment Series

Figure 4.4 shows the periodograms of each series. As previously indicated, the centered data have been padded to a series of length 480. We notice a narrow band peak at the obvious yearly cycle, $\omega = 1 / 1 2$ . In addition, there is considerable amount of power in a wide band at the lower frequencies that is centered around the four-year cycle $\omega = 1 / 4 8$ representing a possible El Ni˜no effect. This wide band activity suggests that the possible El Ni˜no cycle is irregular, but tends to be around four

![](images/a4e3e23ead2961f69ffc53ddcb03ffea8fa380678bf17609a271a0630da94f57.jpg)

![](images/98b216b6250ade4208bc46baf4a0b9ca3e7e9da2fddcaaad5008266ac2b6ce13.jpg)  
Figure 4.4 Periodogram of SOI and Recruitment, $n = 4 5 3$ ( $n ^ { \prime } = 4 8 0$ ), showing common peaks at $\omega = 1 / 1 2 = . 0 8 3$ and $\omega = 1 / 4 8 = . 0 2 1$ cycles/month.

years on average. We will continue to address this problem as we move to more sophisticated analyses.

Noting $\chi _ { 2 } ^ { 2 } ( . 0 2 5 ) = . 0 5$ and $\chi _ { 2 } ^ { 2 } ( . 9 7 5 ) = 7 . 3 8$ , we can obtain approximate 95% confidence intervals for the frequencies of interest. For example, the periodogram of the SOI series is $I _ { S } ( 1 / 1 2 ) = 1 1 . 6 4 $ at the yearly cycle. An approximate 95% confidence interval for the spectrum $f _ { S } ( 1 / 1 2 )$ is then

$$
[ 2 (1 1. 6 7) / 7. 3 8, 2 (1 1. 6 7) / {.} 0 5 ] = [ 3. 1 6, 4 6 0. 8 1 ],
$$

which is too wide to be of much use. We do notice, however, that the lower value of 3.16 is higher than any other periodogram ordinate, so it is safe to say that this value is significant. On the other hand, an approximate $9 5 \%$ confidence interval for the spectrum at the four-year cycle, $f _ { S } ( 1 / 4 8 )$ , is

$$
[ 2 (. 6 4) / 7. 3 8, 2 (. 6 4) / {.} 0 5 ] = [. 1 7, 2 5. 4 7 ],
$$

which again is extremely wide, and with which we are unable to establish

significance of the peak.

We now give the R commands that can be used to reproduce Figure 4.4. To calculate and graph the periodogram, we used the spec.pgram command in R. We have set log="no" because R will plot the periodogram on a $\log _ { 1 0 }$ scale by default. Figure 4.4 displays a bandwidth and by default, R tapers the data (which we override in the commands below). We will discuss bandwidth and tapering in the next section, so ignore these concepts for the time being.

```txt
> soi = scan("/mydata/soi.dat")
> rec = scan("/mydata/rec.dat")
> par(mfrow=c(2,1))
> soi.per = spec.pgram(soi, taper=0, log="no")
> abline(v=1/12, lty="dotted")
> abline(v=1/48, lty="dotted")
> rec.per = spec.pgram(rec, taper=0, log="no")
> abline(v=1/12, lty="dotted")
> abline(v=1/48, lty="dotted") 
```

The confidence intervals for the SOI series at the yearly cycle, $\omega = 1 / 1 2 =$ 40/480, and the possible El Ni˜no cycle of four years $\omega = 1 / 4 8 = 1 0 / 4 8 0$ can be computed in R as follows:

```diff
> soi.per\(spec[40] # soi pgram at freq 1/12 = 40/480 [1] 11.66677  
> soi.per\)spec[10] # soi pgram at freq 1/48 = 10/480 [1] 0.6447554  
> # -- conf intervals -- # returned value:  
> U = qchisq(.025,2) # 0.05063562  
> L = qchisq(.975,2) # 7.377759  
> 2*soi.per\)spec[10]/L # 0.1747835  
> 2*soi.per\)spec[10]/U # 25.46648  
> 2*soi.per\)spec[40]/L # 3.162688  
> 2*soi.per\)spec[40]/U # 460.813  
> #-- replace soi with rec above to get recruit values 
```

The example above makes it fairly clear the periodogram as an estimator is susceptible to large uncertainties, and we need to find a way to reduce the variance. Not surprisingly, this result follows if we think about the periodogram, $I ( \omega _ { j } )$ as an estimator of the spectral density $f ( \omega )$ and realize that it is the sum of squares of only two random variables for any sample size. The solution to this dilemma is suggested by the analogy with classical statistics where we look for independent random variables with the same variance and average the squares of these common variance observations. Independence and equality of variance do not hold in the time series case, but the covariance structure of the two adjacent estimators given in Example 4.8 suggests that for neighboring frequencies, these assumptions are approximately true.

# 4.5 Nonparametric Spectral Estimation

To continue the discussion that ended the previous section, we define a frequency band, $\boldsymbol { B }$ , of $L \ < < \ n$ contiguous fundamental frequencies centered around $\omega _ { j } = j / n$ that are close to the frequency of interest, $\omega$ , as

$$
\mathcal {B} = \left\{\omega : \omega_ {j} - \frac {m}{n} \leq \omega \leq \omega_ {j} + \frac {m}{n} \right\}, \tag {4.45}
$$

where

$$
L = 2 m + 1 \tag {4.46}
$$

is an odd number, chosen such that the spectral values in the interval $\boldsymbol { B }$ ,

$$
f (\omega_ {j} + k / n), \quad k = - m, \dots , 0, \dots , m
$$

are approximately equal to $f ( \omega )$ . This structure can be realized for large sample sizes, as shown formally in C.2. Values of the spectrum in this band should be relatively constant, as well, for the smoothed spectra defined below to be good estimators.

Using the above band, we may now define an averaged or smoothed periodogram as the average of the periodogram values, say,

$$
\bar {f} (\omega) = \frac {1}{L} \sum_ {k = - m} ^ {m} I \left(\omega_ {j} + k / n\right), \tag {4.47}
$$

as the average over the band $\boldsymbol { \beta }$ .

Under the assumption that the spectral density is fairly constant in the band $\boldsymbol { B }$ , and in view of (4.42) we can show that under appropriate conditions,9 for large $n$ , the periodograms in (4.47) are approximately distributed as independent $f ( \omega ) \chi _ { 2 } ^ { 2 } / 2$ random variables, for $0 < \omega < 1 / 2$ , as long as we keep $L$ fairly small relative to $n$ . This result is discussed formally in C.2. Thus, under these conditions, $L f ( \omega )$ is the sum of $L$ approximately independent $f ( \omega ) \chi _ { 2 } ^ { 2 } / 2$ random variables. It follows that, for large $n$ ,

$$
\frac {2 L \bar {f} (\omega)}{f (\omega)} \dot {\sim} \chi_ {2 L} ^ {2} \tag {4.48}
$$

where $\sim$ means approximately distributed as.

In this scenario, it seems reasonable to call the length of the interval defined by (4.45),

$$
B _ {w} = \frac {L}{n} \tag {4.49}
$$

the bandwidth. Bandwidth, of course, refers to the width of the frequencey band used in smoothing the periodogram. The concept of the bandwidth, however, becomes more complicated with the introduction of spectral estimators

that smooth with unequal weights. Note (4.49) implies the degrees of freedom can be expressed as

$$
2 L = 2 B _ {w} n, \tag {4.50}
$$

or twice the time-bandwidth product. The result (4.48) can be rearranged to obtain an approximate $1 0 0 ( 1 - \alpha ) \%$ confidence interval of the form

$$
\frac {2 L \bar {f} (\omega)}{\chi_ {2 L} ^ {2} (1 - \alpha / 2)} \leq f (\omega) \leq \frac {2 L \bar {f} (\omega)}{\chi_ {2 L} ^ {2} (\alpha / 2)} \tag {4.51}
$$

for the true spectrum, $f ( \omega )$

Many times, the visual impact of a spectral density plot will be improved by plotting the logarithm of the spectrum instead of the spectrum.10 This phenomenon can occur when regions of the spectrum exist with peaks of interest much smaller than some of the main power components. For the log spectrum, we obtain an interval of the form

$$
\left[ \ln \bar {f} (\omega) + \ln 2 L - \ln \chi_ {2 L} ^ {2} (1 - \alpha / 2), \ln \bar {f} (\omega) + \ln 2 L - \ln \chi_ {2 L} ^ {2} (\alpha / 2) \right]. \tag {4.52}
$$

We can also test hypotheses relating to the equality of spectra using the fact that the distributional result (4.48) implies that the ratio of spectra based on roughly independent samples will have an approximate $F _ { 2 L , 2 L }$ distribution. The independent estimators can either be from different frequency bands or from different series.

If zeros are appended before computing the spectral estimators, we need to adjust the degrees of freedom and an approximation is to replace $2 L$ by $2 L n / n ^ { \prime }$ . Hence, we define the adjusted degrees of freedom as

$$
d f = \frac {2 L n}{n ^ {\prime}} \tag {4.53}
$$

and use it instead of $2 L$ in the confidence intervals (4.51) and (4.52). For example, (4.51) becomes

$$
\frac {d f \bar {f} (\omega)}{\chi_ {d f} ^ {2} (1 - \alpha / 2)} \leq f (\omega) \leq \frac {d f \bar {f} (\omega)}{\chi_ {d f} ^ {2} (\alpha / 2)}. \tag {4.54}
$$

A number of assumptions are made in computing the approximate confidence intervals given above, which may not hold in practice. In such cases, it may be reasonable to employ resampling techniques such as one of the parametric bootstraps proposed by Hurvich and Zeger (1987) or a nonparametric local bootstrap proposed by Paparoditis and Politis (1999). To develop the bootstrap distributions, we assume that the contiguous DFTs in a frequency band of the form (4.45) all came from a time series with identical spectrum $f ( \omega )$ . This, in fact, is exactly the same assumption made in deriving the largesample theory. We may then simply resample the $L$ DFTs in the band, with

![](images/e7dcb58000c31c42460aadebe0dcbb62b3168f26a44d2b8396302b7b22b5ee48.jpg)

![](images/29b933b7a4e5ee27c3f18cc78472c88b82741c460e68a408a579c42b780634c5.jpg)  
Figure 4.5 The averaged periodogram of the SOI and Recruitment series $n = 4 5 3$ , $n ^ { \prime } = 4 8 0$ , $L = 9$ , df = 17, showing common peaks at the four year period, $\omega = 1 / 4 8 = . 0 2 1$ cycles/month, the yearly period, $\omega = 1 / 1 2 = . 0 8 3$ cycles/month and some of its harmonics $\omega = k / 1 2$ for $k = 2 , 3$ .

replacement, calculating a spectral estimate from each bootstrap sample. The sampling distribution of the bootstrap estimators approximates the distribution of the nonparametric spectral estimator. For further details, including the theoretical properties of such estimators, see Paparoditis and Politis (1999).

Before proceeding further, we pause to consider computing the average periodograms for the SOI and Recruitment series, as shown in Figure 4.5.

# Example 4.10 Averaged Periodogram of SOI and Recruitment Series

Generally, it is a good idea to try several bandwidths that seem to be compatible with the general overall shape of the spectrum, as suggested by the periodogram. The SOI and Recruitment series periodograms,

previously computed in Figure 4.4, suggest the power in the lower El Ni˜no frequency needs smoothing to identify the predominant overall period. Trying values of $L$ leads to the choice $L = 9$ as a reasonable value, and the result is displayed in Figure 4.5. In our notation, the bandwidth in this case is $B _ { w } = 9 / 4 8 0 = . 0 1 8 7 5$ cycles per month for the spectral estimator. This bandwidth means we are assuming a relatively constant spectrum over about .01875/.5 = 3.75% of the entire frequency interval (0, 1/2). The bandwidth reported in R is taken from Bloomfield (2000), and in the current case amounts to dividing (4.49) by $\sqrt { 1 2 }$ . An excellent discussion of the concept of bandwidth may be found in Percival and Walden (1993, 6.7). To obtain the bandwidth, $B _ { w } = . 0 1 8 7 5$ , from the one reported by R in Figure 4.5, we can multiply .00541 by $\sqrt { 1 2 }$ .

The smoothed spectra shown in Figure 4.5 provide a sensible compromise between the noisy version, shown in Figure 4.4, and a more heavily smoothed spectrum, which might lose some of the peaks. An undesirable effect of averaging can be noticed at the yearly cycle, $\omega = 1 / 1 2$ , where the narrow band peaks that appeared in the periodograms in Figure 4.4 have been flattened and spread out to nearby frequencies. We also notice, and have marked, the appearance of harmonics of the yearly cycle, that is, frequencies of the form $\omega = k / 1 2$ for $k = 1 , 2 , \ldots .$ Harmonics typically occur when a periodic component is present, but not in a sinusoidal fashion.

Figure 4.5 can be reproduced in R using the following commands. The basic call is to the function spec.pgram. To compute averaged periodograms, use the Daniell kernel, and specify $m$ , where $L = 2 m + 1$ ( $L = 9$ and $m = 4$ in this example). We will explain the kernel concept later in this section, specifically just prior to Example 4.11.

```diff
> par(mfrow=c(2,1))
> k = kernel("daniell",4)
> soi.ave = spec.pgram(soi, k, taper=0, log="no")
> abline(v=1/12, lty="dotted")
> abline(v=2/12, lty="dotted")
> abline(v=3/12, lty="dotted")
> abline(v=1/48, lty="dotted")
> #-- Repeat 5 lines above using rec in place of soi
> soi.ave$bandwidth # reported bandwidth
[1] 0.005412659
> soi.ave$bandwidth*sqrt(12) # Bw
[1] 0.01875 
```

The adjusted degrees of freedom are $d f = 2 ( 9 ) ( 4 5 3 ) / 4 8 0 \approx 1 7$ . We can use this value for the $9 5 \%$ confidence intervals, with $\chi _ { d f } ^ { 2 } ( . 0 2 5 ) = 7 . 5 6$ and $\chi _ { d f } ^ { 2 } ( . 9 7 5 ) = 3 0 . 1 7$ . Substituting into (4.54) gives the intervals in Table 4.1 for the two frequency bands identified as having the maximum

Table 4.1 Confidence Intervals for the Spectra of the SOI and Recruitment Series   

<table><tr><td>Series</td><td>ω</td><td>Period</td><td>Power</td><td>Lower</td><td>Upper</td></tr><tr><td rowspan="2">SOI</td><td>1/48</td><td>4 years</td><td>.59</td><td>.33</td><td>1.34</td></tr><tr><td>1/12</td><td>1 year</td><td>1.43</td><td>.80</td><td>3.21</td></tr><tr><td rowspan="2">Recruits ×103</td><td>1/48</td><td>4 years</td><td>7.91</td><td>4.45</td><td>17.78</td></tr><tr><td>1/12</td><td>1 year</td><td>2.63</td><td>1.48</td><td>5.92</td></tr></table>

power. To examine the two peak power possibilities, we may look at the 95% confidence intervals and see whether the lower limits are substantially larger than adjacent baseline spectral levels. For example, the El Ni˜no frequency of 48 months has lower limits that exceed the values the spectrum would have if there were simply a smooth underlying spectral function without the peaks. The relative distribution of power over frequencies is different, with the SOI index having less power at the lower frequency, relative to the seasonal periods, and the recruit series having relatively more power at the lower or El Ni˜no frequency.

The entries in Table 4.1 for SOI can be obtained in R as follows:

> df $=$ soi.ave$df # df = 16.9875 (returned values)   
> U = qchisq(.025,df) # U = 7.555916   
> L = qchisq(.975,df) # $\mathrm { ~ L ~ } = \ 3 0 . 1 7 4 2 5$   
> soi.ave$spec[10] # 0.5942431   
> soi.ave$spec[40] # 1.428959  
> # -- intervals --   
> df*soi.ave$spec[10]/L # 0.334547   
> df*soi.ave$spec[10]/U # 1.336000   
> df*soi.ave$spec[40]/L # 0.8044755  
> df*soi.ave$spec[40]/U # 3.212641  
> #-- repeat above commands with soi replaced by rec

Finally, Figure 4.6 shows the averaged periodograms in Figure 4.5 plotted on a $\log _ { 1 0 }$ scale. This is the default plot in R, and these graphs can be obtained by removing the statement $\scriptstyle 1 0 \mathbf { g } = " \mathbf { n } 0 ^ { " }$ in the spec.pgram call. Notice that the default plot also shows a generic confidence interval of the form (4.52) (with ln replaced by $\log _ { 1 0 }$ ) in the upper right-hand corner. To use it, imagine placing the tick mark on the averaged periodogram ordinate of interest; the resulting bar then constitutes an approximate $9 5 \%$ confidence interval for the spectrum at that frequency. Of course, actual intervals may be computed as was done in this example. We note that displaying the estimates on a log scale tends to emphasize the harmonic components.

This example points out the necessity for having some relatively systematic procedure for deciding whether peaks are significant. The question of deciding

![](images/e7357b6b2ea24e2d203991ea641c04728fc92b4af50b2653eca21dc8890ed506.jpg)

![](images/a05e722045d3ffd080dd52688c10f1c53830b1a32f60ffb024ed31753bd908fa.jpg)  
Figure 4.6 Figure 4.5 with the average periodogram ordinates plotted on a $\log _ { 1 0 }$ scale. The display in the upper right-hand corner represents a generic 95% confidence interval.

whether a single peak is significant usually rests on establishing what we might think of as a baseline level for the spectrum, defined rather loosely as the shape that one would expect to see if no spectral peaks were present. This profile can usually be guessed by looking at the overall shape of the spectrum that includes the peaks; usually, a kind of baseline level will be apparent, with the peaks seeming to emerge from this baseline level. If the lower confidence limit for the spectral value is still greater than the baseline level at some predetermined level of significance, we may claim that frequency value as a statistically significant peak. To maintain an $\alpha$ that is consistent with our stated indifference to the upper limits, we might use a one-sided confidence interval.

An important aspect of interpreting the significance of confidence intervals and tests involving spectra is that typically, more than one frequency will be of interest, so that we will potentially be interested in simultaneous statements

about a whole collection of frequencies. For example, it would be unfair to claim in Table 4.1 the two frequencies of interest as being statistically significant and all other potential candidates as nonsignificant at the overall level of $\alpha = . 0 5$ . In this case, we follow the usual statistical approach, noting that if $K$ statements $S _ { 1 } , S _ { 1 } , \ldots , S _ { k }$ are made at significance level $\alpha$ , i.e., $P \{ S _ { k } \} = 1 - \alpha$ , then the overall probability all statements are true satisfies the Bonferroni inequality

$$
P \left\{\text {a l l} S _ {k} \text {t r u e} \right\} \geq 1 - K \alpha . \tag {4.55}
$$

For this reason, it is desirable to set the significance level for testing each frequency at $\alpha / K$ if there are $K$ potential frequencies of interest. If, a priori, potentially $K = 1 0$ frequencies are of interest, setting $\alpha = . 0 1$ would give an overall significance level of bound of .10.

The use of the confidence intervals and the necessity for smoothing requires that we make a decision about the bandwidth $B _ { w }$ over which the spectrum will be essentially constant. Taking too broad a band will tend to smooth out valid peaks in the data when the constant variance assumption is not met over the band. Taking too narrow a band will lead to confidence intervals so wide that peaks are no longer statistically significant. Thus, we note that there is a conflict here between variance properties or bandwidth stability, which can be improved by increasing $B _ { w }$ and resolution, which can be improved by decreasing $B _ { w }$ . A common approach is to try a number of different bandwidths and to look qualitatively at the spectral estimators for each case.

To address the problem of resolution, it should be evident that the flattening of the peaks in Figures 4.5 and 4.6 was due to the fact that simple averaging was used in computing $f ( \omega )$ defined in (4.47). There is no particular reason to use simple averaging, and we might improve the estimator by employing a weighted average, say

$$
\widehat {f} (\omega) = \sum_ {k = - m} ^ {m} h _ {k} I \left(\omega_ {j} + k / n\right), \tag {4.56}
$$

using the same definitions as in (4.47) but where now, the weights satisfy

$$
h _ {- k} = h _ {k} > 0 \text {a l l} k \quad \text {a n d} \quad \sum_ {k = - m} ^ {m} h _ {k} = 1.
$$

In particular, it seems reasonable that the resolution of the estimator will improve if we use weights that decrease as distance from the center weight $h _ { 0 }$ increases; we will return to this idea shortly. To obtain the averaged periodogram, $f ( \omega )$ , in (4.56), set $h _ { k } \ = \ L ^ { - 1 }$ , for all $k$ , where $L = 2 m + 1$ . The asymptotic theory established for $f ( \omega )$ still holds for $\widehat { f } ( \omega )$ provided that the weights satisfy the additional condition that if $m \to \infty$ as $n  \infty$ but $m / n  0$ , then

$$
\sum_ {k = - m} ^ {m} h _ {k} ^ {2} \rightarrow 0.
$$

Under these conditions, as $n \to \infty$ ,

In (ii), replace $f ^ { 2 } ( \omega )$ by 0 if $\omega \neq \lambda$ and by $2 f ^ { 2 } ( \omega )$ if $\omega = \lambda = 0$ or $1 / 2$

We have aare constant, n these results, in which case $f ( \omega )$ , where the weights The distributional $h _ { k } = L ^ { - 1 }$ $\begin{array} { r } { \sum _ { k = - m } ^ { \prime \prime } h _ { k } ^ { - 1 } = L ^ { - 1 } } \end{array}$ properties of (4.56) are more difficult now because $\widehat { f } ( \omega )$ is a weighted linear combination of asymptotically independent $\chi ^ { 2 }$ random variables. An approximation that seems to work well is to replace $L$ by $\left( \sum _ { k = - m } ^ { m } h _ { k } ^ { 2 } \right) ^ { - 1 }$ . That is, define

$$
L _ {h} = \left(\sum_ {k = - m} ^ {m} h _ {k} ^ {2}\right) ^ {- 1} \tag {4.57}
$$

and use the approximation11

$$
\frac {2 L _ {h} \widehat {f} (\omega)}{f (\omega)} \dot {\sim} \chi_ {2 L _ {h}} ^ {2}. \tag {4.58}
$$

In analogy to (4.49), we will define the bandwidth in this case to be

$$
B _ {w} = \frac {L _ {h}}{n}. \tag {4.59}
$$

Using the approximation (4.58) we obtain an approximate $1 0 0 ( 1 - \alpha ) \%$ confidence interval of the form

$$
\frac {2 L _ {h} \widehat {f} (\omega)}{\chi_ {2 L _ {h}} ^ {2} (1 - \alpha / 2)} \leq f (\omega) \leq \frac {2 L _ {h} \widehat {f} (\omega)}{\chi_ {2 L _ {h}} ^ {2} (\alpha / 2)} \tag {4.60}
$$

for the true spectrum, $f ( \omega )$ . If the data are padded to $n ^ { \prime }$ , then replace $2 L _ { h }$ in (4.60) with $d f = 2 L _ { h } n / n ^ { \prime }$ as in (4.53).

An easy way to generate the weights in R is by repeated use of the Daniell kernel. For example, with $m = 1$ and $L = 2 m + 1 = 3$ , the Daniell kernel has weights $\{ h _ { k } \} = \{ \textstyle { \frac { 1 } { 3 } } , { \frac { 1 } { 3 } } , { \frac { 1 } { 3 } } \}$ ; applying this kernel to a sequence of numbers, $\{ u _ { t } \}$ , produces

$$
\widehat {u} _ {t} = \frac {1}{3} u _ {t - 1} + \frac {1}{3} u _ {t} + \frac {1}{3} u _ {t + 1}.
$$

We can apply the same kernel again to the $\widehat { u } _ { t }$

$$
\widehat {\widehat {u}} _ {t} = \frac {1}{3} \widehat {u} _ {t - 1} + \frac {1}{3} \widehat {u} _ {t} + \frac {1}{3} \widehat {u} _ {t + 1},
$$

which simplifies to

$$
\widehat {\widehat {u}} _ {t} = \frac {1}{9} u _ {t - 2} + \frac {2}{9} u _ {t - 1} + \frac {3}{9} u _ {t} + \frac {2}{9} u _ {t + 1} + \frac {1}{9} u _ {t + 2}.
$$

The modified Daniell kernel puts half weights at the end points, so with $m = 1$ the weights are $\begin{array} { r } { \{ h _ { k } \} = \{ \frac { 1 } { 4 } , \frac { 2 } { 4 } , \frac { 1 } { 4 } \} } \end{array}$ and

$$
\widehat {u} _ {t} = \frac {1}{4} u _ {t - 1} + \frac {1}{2} u _ {t} + \frac {1}{4} u _ {t + 1}.
$$

Applying the same kernel again yields

$$
\widehat {\widehat {u}} _ {t} = \frac {1}{1 6} u _ {t - 2} + \frac {4}{1 6} u _ {t - 1} + \frac {6}{1 6} u _ {t} + \frac {4}{1 6} u _ {t + 1} + \frac {1}{1 6} u _ {t + 2}.
$$

These coefficients can be obtained in R by issuing the kernel command. For example, kernel("modified.daniell",c(1,1)) would produce the coefficients of the last example. It is also possible to use different values of $m$ , e.g., try kernel("modified.daniell",c(1,2)) or kernel("daniell",c(1,2)). The other kernels that are currently available in R are the Dirichlet kernel and the Fej´er kernel, which we will discuss shortly.

# Example 4.11 Smoothed Periodogram of the SOI and Recruitment Series

In this example, we estimate the spectra of the SOI and Recruitment series using the smoothed periodogram estimate in (4.56). We used a l twice, with , which is c $m = 3$ both times. T the value of lds  us $L _ { h } =$ $\textstyle 1 / \sum _ { k = - m } ^ { m } h _ { k } ^ { 2 } \ = \ 9 . 2 3 2$ $L \ = \ 9$ Example 4.10. In this case, the bandwidth is $B _ { w } = 9 . 2 3 2 / 4 8 0 = . 0 1 9$ and the modified degrees of freedom is $d f = 2 L _ { h } 4 5 3 / 4 8 0 = 1 7 . 4 3$ . The weights, $h _ { k }$ , can be obtained in R as follows:

> kernel("modified.daniell", c(3,3))

coef[-6] = 0.006944 $\# \mathbf { \Sigma } =$ coef[ 6]

coef[-5] = 0.027778 # = coef[ 5]

coef[-4] = 0.055556 # = coef[ 4]

coef[-3] = 0.083333 # = coef[ 3]

coef[-2] = 0.111111 # = coef[ 2]

coef[-1] = 0.138889 # = coef[ 1]

coef[ 0] = 0.152778

The resulting spectral estimates can be viewed in Figure 4.7 and we notice that the estimates more appealing than those in Figure 4.5. Figure 4.7 was generated in R as follows; we also show how to obtain df and $B _ { w }$ .

> par(mfrow=c(2,1))  
> k = kernel("modified.daniell", c(3,3))   
> soi.smo $=$ spec.pgram(soi, k, taper=0, log="no")

![](images/8dd64a637f27977ab93099d3fbf6f7c4433efa5d952b36f62cc22dcde3eb3502.jpg)

![](images/c216c1ced79a47ee926617bb1e5b47d764e9238e605016893aa165d463158dd3.jpg)  
Figure 4.7 Smoothed spectral estimates of the SOI and Recruitment series; see Example 4.11 for details.

> abline( $\mathtt { v } { = } 1 / 1 2$ , lty $\Bumpeq$ "dotted")   
$>$ abline( $\mathtt { v } { = } 1 / 4 8$ , lty $=$ "dotted")   
> #-- Repeat above 3 lines with rec replacing soi   
> df $=$ soi.smo2$df # df=17.42618   
> Lh = 1/sum(k[-k$m:k$m]ˆ2) # Lh=9.232413   
> Bw = Lh/480 # Bw=0.01923419

The bandwidth reported by R is .00528, which is approximately $B _ { w } / \sqrt { 1 2 }$ ; type bandwidth.kernel to see how R computes bandwidth. Reissuing the spec.pgram commands with log="no" removed will result in a figure similar to Figure 4.6. Finally, we mention that R uses the modified Daniell kernel by default. For example, an easier way to obtain soi.smo is to issue the command:

> soi.smo $=$ spectrum(soi, spans=c(7,7), taper=0)

Notice that spans is a vector of odd integers, given in terms of $L = 2 m + 1$ instead of $m$ . These values give the widths of modified Daniell smoother to be used to smooth the periodogram.

We are now ready to introduce the concept of tapering; this will lead us to the notion of a spectral window. For example, suppose $x _ { t }$ is a mean-zero, stationary process with spectral density $f _ { x } ( \omega )$ . If we replace the original series by the tapered series

$$
y _ {t} = h _ {t} x _ {t}, \tag {4.61}
$$

for $t = 1 , 2 , \ldots , n$ , and use the modified DFT

$$
d _ {y} \left(\omega_ {j}\right) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} h _ {t} x _ {t} \mathrm {e} ^ {- 2 \pi i \omega_ {j} t}, \tag {4.62}
$$

and let $I _ { y } ( \omega _ { j } ) = | d _ { y } ( \omega _ { j } ) | ^ { 2 }$ , we obtain (see Problem 4.15)

$$
E \left[ I _ {y} \left(\omega_ {j}\right) \right] = \int_ {- 1 / 2} ^ {1 / 2} W _ {n} \left(\omega_ {j} - \omega\right) f _ {x} (\omega) d \omega \tag {4.63}
$$

where

$$
W _ {n} (\omega) = | H _ {n} (\omega) | ^ {2} \tag {4.64}
$$

and

$$
H _ {n} (\omega) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} h _ {t} \mathrm {e} ^ {- 2 \pi i \omega t}. \tag {4.65}
$$

The value $W _ { n } ( \omega )$ is called a spectral window because, in view of (4.63), it is determining which part of the spectral density $f _ { x } ( \omega )$ is being “seen” by the estimator $I _ { y } ( \omega _ { j } )$ on average. In the case that $h _ { t } = 1$ for all $t$ , $I _ { y } ( \omega _ { j } ) = I _ { x } ( \omega _ { j } )$ is simply the periodogram of the data and the window is

$$
W _ {n} (\omega) = \frac {\sin^ {2} (n \pi \omega)}{n \sin^ {2} (\pi \omega)} \tag {4.66}
$$

with $W _ { n } ( 0 ) = n$ , which is known as the Fej´er or modified Bartlett kernel. If we consider the averaged periodogram in (4.47), namely

$$
\bar {f} _ {x} (\omega) = \frac {1}{L} \sum_ {k = - m} ^ {m} I _ {x} (\omega_ {j} + k / n),
$$

the window, $W _ { n } ( \omega )$ , in (4.63) will take the form

$$
W _ {n} (\omega) = \frac {1}{n L} \sum_ {k = - m} ^ {m} \frac {\sin^ {2} [ n \pi (\omega + k / n) ]}{\sin^ {2} [ \pi (\omega + k / n) ]}. \tag {4.67}
$$

Tapers generally have a shape that enhances the center of the data relative to the extremities, such as a cosine bell of the form

$$
h _ {t} = . 5 \left[ 1 + \cos \left(\frac {2 \pi (t - \bar {t})}{n}\right) \right], \tag {4.68}
$$

![](images/87351b61a7bdd6868552eb6d640261512646de5728966579b6e6a1cbd756f4c7.jpg)  
Smoothed Fejer

![](images/b753daa6a0f191f14779e1ef7a5277b9b5632cd248baf157b2f9f47546ee7fdd.jpg)  
Smoothed Fejer − logged

![](images/ef04f225534cfd3ce89350560e1e9430f5451975799916927442d3e9f3878e3f.jpg)  
Cosine Taper

![](images/2040c058c7a19dc1cd5d4a546dc81b4fc897edf268d4548cba438dc2d432c0a6.jpg)  
Cosine Taper − logged   
Figure 4.8 Averaged Fej´er window (top row) and the corresponding cosine taper window (bottom row) for $L = 9 , \ n = 4 8 0$ .

where $t = ( n + 1 ) / 2$ , favored by Blackman and Tukey (1959). In Figure 4.8, we have plotted the shapes of two windows, $W _ { n } ( \omega )$ , for $n = 4 8 0$ and $L = 9$ , when (i) $h _ { t } \equiv 1$ , in which case, (4.67) applies, and (ii) $h _ { t }$ is the cosine taper in (4.68). In both cases the predicted bandwidth should be $B _ { w } = 9 / 4 8 0 = . 0 1 8 7 5$ cycles per point, which corresponds to the “width” of the windows shown in Figure 4.8. Both windows produce an integrated average spectrum over this band but the untapered window in the top panels shows considerable ripples over the band and outside the band. The ripples outside the band are called sidelobes and tend to introduce frequencies from outside the interval that may contaminate the desired spectral estimate within the band. For example, a large dynamic range for the values in the spectrum introduces spectra in contiguous frequency intervals several orders of magnitude greater than the value in the interval of interest. This effect is sometimes called leakage. Finally,

![](images/141c79d73dd7e0ad812daaab084f4651c07d622b567689407b72a742ab7b43a1.jpg)

![](images/787eb34b517adc1f3436cf99fd66c1463112f05a9b4f09783b15d3787f4f29c6.jpg)

![](images/c263105e183fd0d857b60670ea918e09628c7ccd76f22870197d4bb187a798cd.jpg)  
Figure 4.9 Smoothed spectral estimates of the SOI (on a log $^ { 1 0 }$ scale) without tapering (top), with 10% tapering (middle) and with 50% or complete tapering (bottom); see Example 4.12 for details.

the logged values in Figure 4.8 emphasize the suppression of the sidelobes in the Fej´er kernel when a cosine taper is used.

# Example 4.12 The Effect of Tapering the SOI Series

In this example we examine the effect of various tapers on the estimate of the spectrum of the SOI series. The results for the Recruitment series are similar. Figure 4.9 shows three spectral estimates plotted on a $\log _ { 1 0 }$ scale along with the corresponding approximate 95% confidence intervals in the upper right. The degree of smoothing here is the same as in Example 4.11. The top of Figure 4.9 shows the estimate without any tapering and hence it is the same as the estimated spectrum displayed in the top of Figure 4.7. The middle panel in Figure 4.9 shows the effect of

10% tapering (the R default), which means that the cosine taper is being applied only to the ends of the series, 10% on each side. The bottom panel shows the results with 50% tapering; that is, (4.68) is being applied to the entire set of data.

The three spectral estimates are qualitatively similar, but note that in the fully tapered case, the peak El Ni˜no cycle is at the 42 month (3.5 year) cycle instead of the 48 month (4 year) cycle. Also, notice that the confidence interval bands are increasing as the tapering increases. This occurrence is due to the fact that by tapering we are decreasing the amount of information, and hence the degrees of freedom; details, which are similar to the ideas discussed in (4.57)–(4.58), may be found in Bloomfield (2000, §9.5).

The following R session was used to generate Figure 4.9:

```txt
> par(mfrow=c(3,1))  
> spectrum(soi, spans=c(7,7), taper=0, main="No Taper")  
> abline(v=1/12, lty="dashed")  
> abline(v=1/48, lty="dashed")  
> spectrum(soi, spans=c(7,7), main="10% Taper")  
> abline(v=1/12, lty="dashed")  
> abline(v=1/48, lty="dashed")  
> spectrum(soi, spans=c(7,7), taper=.5, main="50% Taper")  
> abline(v=1/12, lty="dashed")  
> abline(v=1/48, lty="dashed") 
```

# Example 4.13 Spectra of $\mathbf { P }$ and S Components for Earthquake and Explosion

Figure 4.10 shows the spectra computed separately from the two phases of the earthquake and explosion in Figure 1.7 of Chapter 1. In all cases we used a modified Daniell smoother with $L = 2 1$ being passed twice, and with 10% tapering. This leads to approximately 54 degrees of freedom. Because the sampling rate is 40 points per second, the folding frequency is 20 cycles per second or 20 Hertz (Hz). The highest frequency shown in the plots is .25 cycles per point or 10 Hz because there is no signal activity at frequencies beyond $1 0 \ \mathrm { H z }$ . A fundamental problem in the analysis of seismic data is discriminating between earthquakes and explosions using the kind of instruments that might be used in monitoring a nuclear test ban treaty. If we plot an ensemble of earthquakes and explosions comparable to Figure 1.7, some gross features appear that may lead to discrimination. The most common differences that we look for are subtle differences between the spectra of the two classes of events. In this case, note the strong frequency components of the P and S components of the explosion are close to the frequency .10 cycles per point or 1 Hz. On the other hand, the spectral content of the earthquakes tends to

occur along a broader frequency band and at lower frequencies for both components. Often, we assume that the ratio of P to S power is in different proportions at different frequencies, and this distinction can form a basis for discriminating between the two classes. In 7.7, we test formally for discrimination using a random effects analysis of variance approach.

Figure 4.10 was generated in R as follows:

```julia
> x = matrix.scan("/mydata/eq5exp6.dat", ncol=2)
> eqP = x[1:1024, 1]; eqS = x[1025:2048, 1]
> exP = x[1:1024, 2]; exS = x[1025:2048, 2]
> par(mfrow=c(2,2))
> eqPs=spectrum(eqP, spans=c(21,21),
+ log="no", xlim=c(0,.25), ylim=c(0,.04))
> eqSs=spectrum(eqS, spans=c(21,21),
+ log="no", xlim=c(0,.25), ylim=c(0,.4))
> exPs=spectrum(exP, spans=c(21,21),
+ log="no", xlim=c(0,.25), ylim=c(0,.04))
> exSs=spectrum(exS, spans=c(21,21),
+ log="no", xlim=c(0,.25), ylim=c(0,.4))
> exSs$df
[1] 53.87862 
```

We close this section with a brief discussion of lag window estimators. First, consider the periodogram, $I ( \omega _ { j } )$ , which was shown in (4.23) to be of the form

$$
I (\omega_ {j}) = \sum_ {| h | <   n} \widehat {\gamma} (h) e ^ {- 2 \pi i \omega_ {j} h}.
$$

Thus, (4.56) can be written as

$$
\begin{array}{l} \widehat {f} (\omega) = \sum_ {| k | \leq m} h _ {k} I \left(\omega_ {j} + k / n\right) \\ = \sum_ {| k | \leq m} h _ {k} \sum_ {| h | <   n} \widehat {\gamma} (h) e ^ {- 2 \pi i (\omega_ {j} + k / n) h} \\ = \sum_ {| h | <   n} g (h / n) \hat {\gamma} (h) e ^ {- 2 \pi i \omega_ {j} h}. \tag {4.69} \\ \end{array}
$$

where $\begin{array} { r } { g ( h / n ) = \sum _ { | k | \leq m } h _ { k } \exp ( - 2 \pi i k h / n ) } \end{array}$ . Equation (4.69) suggests estimators of the form

$$
\widetilde {f} (\omega) = \sum_ {| h | \leq r} w (h / r) \widehat {\gamma} (h) e ^ {- 2 \pi i \omega h} \tag {4.70}
$$

where $w ( \cdot )$ is a weight function, called the lag window, that satisfies

![](images/fba53729fd9d592a5ff6aedf6fd5dda5752400c661143ba53ee0e44dcdb2a870.jpg)

![](images/fc9d050c620a7792a761bfc47da45695716e38946b31f6f1bb7b5345c5ee4be5.jpg)

![](images/00bc421fecd678da32ee161feada9e8ba8da19e55b5132dad07426648fdd0572.jpg)

![](images/9c8409c9a5bf2c7e0259649130a2c894d94d4a506db7100727e021427b8c3b2d.jpg)  
Figure 4.10 Spectral analysis of P and S components of an earthquake and an explosion, $n = 1 0 2 4$ . Each estimate is based on a modified Daniell smoother with $L = 2 1$ being passed twice, and with 10% tapering. This leads to approximately 54 degrees of freedom. Multiply frequency by 40 to convert to Hertz (cycles/second).

(i) $w ( 0 ) = 1$   
(ii) $| w ( x ) | \leq 1$ and $w ( x ) = 0$ for $| x | > 1$   
(iii) $w ( x ) = w ( - x )$ .

Note that if $w ( x ) = 1$ for $| x | < 1$ and $r ~ = ~ n$ , then $\tilde { f } ( \omega _ { j } ) = I ( \omega _ { j } )$ , the periodogram. This result indicates the problem with the periodogram as an estimator of the spectral density is that it gives too much weight to the values of $\widehat { \gamma } ( h )$ when $h$ is large, and hence is unreliable [e.g, there is only one pair of

observations used in the estimate $\widehat { \gamma } ( n - 1 )$ , and so on]. The smoothing window is defined to be

$$
W (\omega) = \sum_ {h = - r} ^ {r} w (h / r) e ^ {- 2 \pi i \omega h}, \tag {4.71}
$$

and it determines which part of the periodogram will be used to form the estimate of $f ( \omega )$ . The asymptotic theory for $\widehat { f } ( \omega )$ holds for $\tilde { f } ( \omega )$ under the same conditions and provided $r  \infty$ as $n \to \infty$ but with $r / n  0$ . We have

$$
E \{\widetilde {f} (\omega) \} \rightarrow f (\omega); \tag {4.72}
$$

$$
\frac {n}{r} \operatorname {c o v} \left(\widetilde {f} (\omega), \widetilde {f} (\lambda)\right)\rightarrow f ^ {2} (\omega) \int_ {- 1} ^ {1} w ^ {2} (x) d x \quad \omega = \lambda \neq 0, 1 / 2. \tag {4.73}
$$

In (4.73), replace $f ^ { 2 } ( \omega )$ by 0 if $\omega \neq \lambda$ and by $2 f ^ { 2 } ( \omega )$ if $\omega = \lambda = 0$ or $1 / 2$ .

Many authors have developed various windows and Brillinger (2001, Ch 3) and Brockwell and Davis (1991, Ch 10) are good sources of detailed information on this topic. We mention a few.

The rectangular lag window, which gives uniform weight in (4.70),

$$
w (x) = 1, \quad | x | \leq 1,
$$

corresponds to the Dirichlet smoothing window given by

$$
W (\omega) = \frac {\sin (2 \pi r + \pi) \omega}{\sin (\pi \omega)}. \tag {4.74}
$$

This smoothing window takes on negative values, which may lead to estimates of the spectral density that are negative a various frequencies. Using (4.73) in this case, for large $n$ we have

$$
\mathrm {v a r} \{\widetilde {f} (\omega) \} \approx \frac {2 r}{n} f ^ {2} (\omega).
$$

The Parzen lag window is defined to be

$$
w (x) = \left\{ \begin{array}{l l} 1 - 6 x + 6 | x | ^ {3} & | x | <   1 / 2, \\ 2 (1 - | x |) ^ {3} & 1 / 2 \leq x \leq 1, \\ 0 & \text {o t h e r w i s e .} \end{array} \right.
$$

This leads to an approximate smoothing window of

$$
W (\omega) = \frac {6}{\pi r ^ {3}} \frac {\sin^ {4} (r \omega / 4)}{\sin^ {4} (\omega / 2)}.
$$

For large $n$ , the variance of the estimator is approximately

$$
\mathrm {v a r} \{\widetilde {f} (\omega) \} \approx . 5 3 9 f ^ {2} (\omega) / n.
$$

The Tukey-Hanning lag window has the form

$$
w (x) = \frac {1}{2} (1 + \cos (x)), \quad | x | \leq 1
$$

which leads to the smoothing window

$$
W (\omega) = \frac {1}{4} D _ {r} (2 \pi \omega - \pi / r) + \frac {1}{2} D _ {r} (2 \pi \omega) + \frac {1}{4} D _ {r} (2 \pi \omega + \pi / r)
$$

where $D _ { r } ( \omega )$ is the Dirichlet kernel in (4.74). The approximate large sample variance of the estimator is

$$
\operatorname {v a r} \{\widetilde {f} (\omega) \} \approx \frac {3 r}{4 n} f ^ {2} (\omega).
$$

The triangular lag window, also known as the Bartlett or Fej´er window, given by

$$
w (x) = 1 - | x |, \quad | x | \leq 1
$$

leads to the Fej´er smoothing window:

$$
W (\omega) = \frac {\sin^ {2} (\pi r \omega)}{r \sin^ {2} (\pi \omega)}.
$$

In this case, (4.73) yields

$$
\operatorname {v a r} \{\widetilde {f} (\omega) \} \approx \frac {2 r}{3 n} f ^ {2} (\omega).
$$

The idealized rectangular smoothing window, also called the Daniell window, is given by

$$
W (\omega) = \left\{ \begin{array}{l l} r & | \omega | \leq 1 / 2 r \\ 0 & \text {o t h e r w i s e} \end{array} \right.,
$$

and leads to the sinc lag window, namely

$$
w (x) = \frac {\sin (\pi x)}{\pi x}, \quad | x | \leq 1.
$$

From (4.73) we have

$$
\operatorname {v a r} \{\widetilde {f} (\omega) \} \approx \frac {r}{n} f ^ {2} (\omega).
$$

For lag window estimators, the width of the rectangular window that leads to the same asymptotic variance as a given lag window estimator is sometimes called the bandwidth. For example, the bandwidth of the rectangular window of the triangular window is is $b _ { r } = 1 / r$ and the asymptotic variance is 3nf $\textstyle { \frac { 2 r } { 3 n } } f ^ { 2 }$ , so setting nbr $\begin{array} { r } { { \frac { 1 } { n b _ { r } } } f ^ { 2 } = { \frac { 2 r } { 3 n } } f ^ { 2 } } \end{array}$ $\textstyle { \frac { 1 } { n b _ { r } } } f ^ { 2 }$ 2 . The asymptotic variance 2r3n f 2 and solving we get $b _ { r } = 3 / 2 r$ as the corresponding bandwidth.

# 4.6: Cross-Spectra

# 4.6 Multiple Series and Cross-Spectra

The notion of analyzing frequency fluctuations using classical statistical ideas extends to the case in which there are several jointly stationary series, for example, $x _ { t }$ and $y _ { t }$ . In this case, we can introduce the idea of a correlation indexed by frequency, called the coherence. The results in Appendix C, §C.2, imply the covariance function

$$
\gamma_ {x y} (h) = E \left[ \left(x _ {t + h} - \mu_ {x}\right) \left(y _ {t} - \mu_ {y}\right) \right]
$$

has the representation

$$
\gamma_ {x y} (h) = \int_ {- 1 / 2} ^ {1 / 2} f _ {x y} (\omega) \mathrm {e} ^ {2 \pi i \omega h} d \omega \quad h = 0, \pm 1, \pm 2, \dots , \tag {4.75}
$$

where the cross-spectrum is defined as the Fourier transform

$$
f _ {x y} (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma_ {x y} (h) \mathrm {e} ^ {- 2 \pi i \omega h} - 1 / 2 \leq \omega \leq 1 / 2, \tag {4.76}
$$

assuming that the cross-covariance function is absolutely summable, as was the case for the autocovariance. The cross-spectrum is generally a complex-valued function, and it is often written as12

$$
f _ {x y} (\omega) = c _ {x y} (\omega) - i q _ {x y} (\omega), \tag {4.77}
$$

where

$$
c _ {x y} (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma_ {x y} (h) \cos (2 \pi \omega h) \tag {4.78}
$$

and

$$
q _ {x y} (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma_ {x y} (h) \sin (2 \pi \omega h) \tag {4.79}
$$

are defined as the cospectrum and quadspectrum, respectively. Because of the relationship $\gamma _ { y x } ( h ) = \gamma _ { x y } ( - h )$ , it follows, by substituting into (4.76) and rearranging, that

$$
f _ {y x} (\omega) = \overline {{f _ {x y} (\omega)}}. \tag {4.80}
$$

This result, in turn, implies that the cospectrum and quadspectrum satisfy

$$
c _ {y x} (\omega) = c _ {x y} (\omega) \tag {4.81}
$$

and

$$
q _ {y x} (\omega) = - q _ {x y} (\omega). \tag {4.82}
$$

An important example of the application of the cross-spectrum is to the problem of predicting an output series $y _ { t }$ from some input series $x _ { t }$ through a linear filter relation such as the three-point moving average considered below. A measure of the strength of such a relation is the squared coherence function, defined as

$$
\rho_ {y \cdot x} ^ {2} (\omega) = \frac {\left| f _ {y x} (\omega) \right| ^ {2}}{f _ {x x} (\omega) f _ {y y} (\omega)}, \tag {4.83}
$$

where $f _ { x x } ( \omega )$ and $f _ { y y } ( \omega )$ are the individual spectra of the $x _ { t }$ and $y _ { t }$ series, respectively. Although we consider a more general form of this that applies to multiple inputs later, it is instructive to display the single input case as (4.83) to emphasize the analogy with conventional squared correlation, which takes the form

$$
\rho_ {y x} ^ {2} = \frac {\sigma_ {y x} ^ {2}}{\sigma_ {x} ^ {2} \sigma_ {y} ^ {2}},
$$

for random variables with variances $\sigma _ { x } ^ { 2 }$ and $\sigma _ { y } ^ { 2 }$ and covariance $\sigma _ { y x } = \sigma _ { x y }$ . This motivates the interpretation of squared coherence and the squared correlation between two time series at frequency $\omega$ .

# Example 4.14 Cross-Spectrum and Coherence of a Process and a Three-Point Moving Average

As a simple example, we compute the cross-spectrum between $x _ { t }$ and the three-point moving average $y _ { t } = ( x _ { t - 1 } + x _ { t } + x _ { t + 1 } ) / 3$ , where $x _ { t }$ is a stationary input process with spectral density $f _ { x x } ( \omega )$ . First,

$$
\begin{array}{l} \gamma_ {x y} (h) = E \left[ x _ {t + h} y _ {t} \right] \\ = \frac {1}{3} E \left[ x _ {t + h} \left(x _ {t - 1} + x _ {t} + x _ {t + 1}\right) \right] \\ = \frac {1}{3} \left(\gamma_ {x x} (h + 1) + \gamma_ {x x} (h) + \gamma_ {x x} (h - 1)\right) \\ { = } { \frac { 1 } { 3 } \int _ { - 1 / 2 } ^ { 1 / 2 } ( e ^ { 2 \pi i \omega } + 1 + e ^ { - 2 \pi i \omega } ) e ^ { 2 \pi i \omega h } f _ { x x } ( \omega ) d \omega } \\ { = } { \frac { 1 } { 3 } \int _ { - 1 / 2 } ^ { 1 / 2 } [ 1 + 2 \cos ( 2 \pi \omega ) ] f _ { x x } ( \omega ) \mathrm { e } ^ { 2 \pi i \omega h } d \omega . } \\ \end{array}
$$

Using the uniqueness of the Fourier transform, we argue from the spectral representation (4.75) that the above must be the transform of $f _ { x y } ( \omega )$ , implying that

$$
f _ {x y} (\omega) = \frac {1}{3} \left[ 1 + \cos (2 \pi \omega) \right] f _ {x x} (\omega)
$$

so that the cross-spectrum is real in this case. From Example 4.5, the spectral density of $y _ { t }$ is

$$
{f _ {y y} (\omega)} = {\frac {1}{9} [ 3 + 4 \cos (2 \pi \omega) + 2 \cos (4 \pi \omega) ] f _ {x x} (\omega)}
$$

# 4.6: Cross-Spectra

$$
{ = } { \frac { 1 } { 9 } \left[ 1 + 2 \cos ( 2 \pi \omega ) \right] ^ { 2 } f _ { x x } ( \omega ) , }
$$

using the identity $\cos ( 2 \alpha ) = 2 \cos ^ { 2 } ( \alpha ) - 1$ in the last step. Substituting into (4.83) yields the squared coherence between $x _ { t }$ and $y _ { t }$ as unity over all frequencies. This is a characteristic inherited by more general linear filters, as will be shown in Problem 4.23. However, if some noise is added to the three-point moving average, the coherence is not unity; these kinds of models will be considered in detail later.

# Property P4.3: Spectral Representation of a Vector Stationary Process

If the elements of the $p \times p$ autocovariance function matrix

$$
\Gamma (h) = E \left[ \left(\boldsymbol {x} _ {t + h} - \boldsymbol {\mu}\right) \left(\boldsymbol {x} _ {t} - \boldsymbol {\mu}\right) ^ {\prime} \right]
$$

of a $p$ -dimensional stationary time series, $\pmb { x } _ { t } = ( x _ { t 1 } , x _ { t 2 } , . . . , x _ { t p } ) ^ { \prime }$ , has elements satisfying

$$
\sum_ {h = - \infty} ^ {\infty} | \gamma_ {j k} (h) | <   \infty \tag {4.84}
$$

for all $j , k = 1 , \dotsc , p$ , then $\Gamma ( h )$ has the representation

$$
\Gamma (h) = \int_ {- 1 / 2} ^ {1 / 2} \mathrm {e} ^ {2 \pi i \omega h} f (\omega) d \omega \quad h = 0, \pm 1, \pm 2, \dots , \tag {4.85}
$$

as the inverse transform of the spectral density matrix, $f ( \omega ) = \{ f _ { j k } ( \omega ) \}$ , for $j , k = 1 , \dotsc , p$ , with elements equal to the cross-spectral components. The matrix $f ( \omega )$ has the representation

$$
f (\omega) = \sum_ {h = - \infty} ^ {\infty} \Gamma (h) \mathrm {e} ^ {- 2 \pi i \omega h} - 1 / 2 \leq \omega \leq 1 / 2. \tag {4.86}
$$

# Example 4.15 Spectral Matrix of a Bivariate Process

Consider a jointly stationary bivariate process $( x _ { t } , y _ { t } )$ . We arrange the autocovariances in the matrix

$$
\Gamma (h) = \left( \begin{array}{c c} \gamma_ {x x} (h) & \gamma_ {x y} (h) \\ \gamma_ {y x} (h) & \gamma_ {y y} (h) \end{array} \right).
$$

The spectral matrix would be given by

$$
f (\omega) = \left( \begin{array}{c c} f _ {x x} (\omega) & f _ {x y} (\omega) \\ f _ {y x} (\omega) & f _ {y y} (\omega) \end{array} \right),
$$

where the Fourier transform (4.85) and (4.86) relate the autocovariance and spectral matrices.

The extension of spectral estimation to vector series is fairly obvious. For the vector series $\pmb { x } _ { t } = ( x _ { t 1 } , x _ { t 2 } , . . . , x _ { t p } ) ^ { \prime }$ , we may use the vector of DFTs, say $\pmb { d } ( \omega _ { j } ) = ( d _ { 1 } ( \omega _ { j } ) , d _ { 2 } ( \omega _ { j } ) , \dots , d _ { p } ( \omega _ { j } ) ) ^ { \prime }$ , and estimate the spectral matrix by

$$
\bar {f} (\omega) = L ^ {- 1} \sum_ {k = - m} ^ {m} I \left(\omega_ {j} + k / n\right) \tag {4.87}
$$

where now

$$
I (\omega_ {j}) = \boldsymbol {d} (\omega_ {j}) \boldsymbol {d} ^ {*} (\omega_ {j}) \tag {4.88}
$$

is a $p \times p$ complex matrix.13

Again, the series may be tapered before the DFT is taken in (4.87) and we can use weighted estimation,

$$
\widehat {f} (\omega) = \sum_ {k = - m} ^ {m} h _ {k} I \left(\omega_ {j} + k / n\right) \tag {4.89}
$$

where $\left\{ h _ { k } \right\}$ are weights as defined in (4.56). The estimate of squared coherence between two series, $y _ { t }$ and $x _ { t }$ is

$$
\widehat {\rho} _ {y \cdot x} ^ {2} (\omega) = \frac {| \widehat {f} _ {y x} (\omega) | ^ {2}}{\widehat {f} _ {x x} (\omega) \widehat {f} _ {y y} (\omega)}. \tag {4.90}
$$

If the spectral estimates in (4.90) are obtained using equal weights, we will write $\bar { \rho } _ { y \cdot x } ^ { 2 } ( \omega )$ for the estimate.

·Under general conditions, if $\rho _ { y \cdot x } ^ { 2 } ( \omega ) > 0$ then

$$
\left. \left| \hat {\rho} _ {y \cdot x} (\omega) \right| \sim A N \left(\left| \rho_ {y \cdot x} (\omega) \right|, \left(1 - \rho_ {y \cdot x} ^ {2} (\omega)\right) ^ {2} / 2 L _ {h}\right) \right. \tag {4.91}
$$

where $L _ { h }$ is defined in (4.57); the details of this result may be found in Brockwell and Davis (1991, Ch 11). We may use (4.91) to obtain approximate confidence intervals for the square coherency $\rho _ { y \cdot x } ^ { 2 } ( \omega )$ .

We can test the hypothesis that $\rho _ { y \cdot x } ^ { 2 } ( \omega ) = 0$ if we use $\bar { \rho } _ { y \cdot x } ^ { 2 } ( \omega )$ for the estimate with $L > 1$ , $^ { 1 4 }$ that is,

$$
\bar {\rho} _ {y \cdot x} ^ {2} (\omega) = \frac {| \bar {f} _ {y x} (\omega) | ^ {2}}{\bar {f} _ {x x} (\omega) \bar {f} _ {y y} (\omega)}. \tag {4.92}
$$

In this case, under the null hypothesis, the statistic

$$
F _ {2, 2 L - 2} = \frac {\bar {\rho} _ {y \cdot x} ^ {2} (\omega)}{(1 - \bar {\rho} _ {y \cdot x} ^ {2} (\omega))} (L - 1) \tag {4.93}
$$

![](images/acb5f286fa23e103ab6a1c7ea30c209e1f48f903e2fd1e9382426773cf55951b.jpg)  
Figure 4.11 Coherence function between the SOI and Recruitment series; $L = 1 9 , \ n = 4 5 3 , \ n ^ { \prime } = 4 8 0$ , and $\alpha = . 0 0 1$ .

has an approximate F-distribution with 2 and $2 L - 2$ degrees of freedom. When the series have been extended to length $n ^ { \prime }$ , we replace $2 L - 2$ by $d f - 2$ , where df is defined in (4.53). Solving (4.93) for a particular significance level $\alpha$ leads to

$$
C _ {\alpha} = \frac {F _ {2 , 2 L - 2} (\alpha)}{L - 1 + F _ {2 , 2 L - 2} (\alpha)} \tag {4.94}
$$

as the approximate value that must be exceeded for the original squared coherence to be able to reject $\rho _ { y \cdot x } ^ { 2 } ( \omega ) = 0$ at an a priori specified frequency.

# Example 4.16 Coherence Between SOI and Recruitment Series

Figure 4.11 shows the squared coherence between the SOI and Recruitment series over a wider band than was used for the spectrum. In this case, we used $L = 1 9 , \ d f = 2 ( 1 9 ) ( 4 5 3 / 4 8 0 ) \approx 3 6$ $L = 1 9$ and $F _ { 2 , d f - 2 } ( . 0 0 1 ) \approx$ 8.53 at the significance level $\alpha = . 0 0 1$ . Hence, we may reject the hypothesis of no coherence for values of $C _ { . 0 0 1 } > . 3 2$ . We emphasize that this method is crude because, in addition to the fact that the $F ^ { \prime }$ -statistic is approximate, we are examining the squared coherence across all frequencies with the Bonferroni inequality, (4.55), in mind. Figure 4.11 also exhibits confidence bands as part of the R plotting routine. We emphasize that these bands are only valid for $\omega$ where $\rho _ { y \cdot x } ^ { 2 } ( \omega ) > 0$ .

In this case, the seasonal frequency and the El Ni˜no frequencies ranging between about 3 and 7 year periods are strongly coherent. Other frequencies are also strongly coherent, although the strong coherence is less impressive because the underlying power spectrum at these higher frequencies is fairly small. Finally, we note that the coherence is persistent at the seasonal harmonic frequencies.

This example may be reproduced using the following R commands.

```txt
> x = ts(cbind(soi,rec))
> s = spec.pgram(x, kernel("daniell",9), taper=0)
> s$df # df = 35.8625
> f = qf(.999, 2, $df-2) # f = 8.529792
> c = f/(18+f) # c = 0.3188779
> plot(s, plot.type = "coh", ci.lty = 2)
> abline(h = c) 
```

# 4.7 Linear Filters

Some of the examples of the previous sections have hinted at the possibility the distribution of power or variance in a time series can be modified by making a linear transformation. In this section, we explore that notion further by defining a linear filter and showing how it can be used to extract signals from a time series. The linear filter modifies the spectral characteristics of a time series in a predictable way, and the systematic development of methods for taking advantage of the special properties of linear filters is an important topic in time series analysis.

A linear filter uses a set of specified coefficients $a _ { t }$ , for $t = 0 , \pm 1 , \pm 2 \ldots$ , to transform a stationary input series, $x _ { t }$ , producing an output series, $y _ { t }$ , of the form

$$
y _ {t} = \sum_ {r = - \infty} ^ {\infty} a _ {r} x _ {t - r}. \tag {4.95}
$$

The form (4.95) is also called a convolution in some statistical contexts. The coefficients, collectively called the impulse response function, are required to satisfy absolute summability

$$
\sum_ {t = - \infty} ^ {\infty} | a _ {t} | <   \infty , \tag {4.96}
$$

so (4.95) exists as a limit in mean square and the infinite Fourier transform

$$
A _ {y x} (\omega) = \sum_ {t = - \infty} ^ {\infty} a _ {t} \mathrm {e} ^ {- 2 \pi i \omega t}, \tag {4.97}
$$

# 4.7: Linear Filters

called the frequency response function, is well defined. We have already encountered several linear filters, for example, the simple three-point moving average in Example 4.5, which can be put into the form of (4.95) by letting $a _ { - 1 } = a _ { 0 } = a _ { 1 } = 1 / 3$ and taking $a _ { t } = 0$ for $| t | \geq 2$ .

The importance of the linear filter stems from its ability to enhance certain parts of the spectrum of the input series. To see this, the autocovariance function of the filtered output (4.95) can be derived as

$$
\begin{array}{l} \gamma_ {y y} (h) = E \left[ \left(y _ {t + h} - E y _ {t + h}\right) \left(y _ {t} - E y _ {t}\right) \right] \\ = E \left[ \sum_ {r} \sum_ {s} a _ {r} \left(x _ {t + h - r} - \mu\right) \left(x _ {t - s} - \mu\right) a _ {s} \right] \\ = \sum_ {r} \sum_ {s} a _ {r} \gamma_ {x x} (h - r + s) a _ {s} \\ = \sum_ {r} \sum_ {s} a _ {r} \left[ \int_ {- 1 / 2} ^ {1 / 2} \mathrm {e} ^ {2 \pi i \omega (h - r + s)} f _ {x x} (\omega) d \omega \right] a _ {s} \\ = \int_ {- 1 / 2} ^ {1 / 2} \left(\sum_ {r} a _ {r} \mathrm {e} ^ {- 2 \pi i \omega r}\right) \left(\sum_ {s} a _ {s} \mathrm {e} ^ {2 \pi i \omega s}\right) \mathrm {e} ^ {2 \pi i \omega h} f _ {x x} (\omega) d \omega \\ { = } { \int _ { - 1 / 2 } ^ { 1 / 2 } \mathrm { e } ^ { 2 \pi i \omega h } | A _ { y x } ( \omega ) | ^ { 2 } f _ { x x } ( \omega ) d \omega , } \\ \end{array}
$$

where we have first replaced $\gamma _ { x x } ( \cdot )$ by its representation (4.12) and then substituted $A _ { y x } ( \omega )$ from (4.97). The computation is one we do repeatedly, exploiting the uniqueness of the Fourier transform. Now, because the left-hand side is the Fourier transform of the spectral density of the output, say, $f _ { y y } ( \omega )$ , we get the important filtering property as follows.

# Property P4.4: Output Spectrum of a Filtered Stationary Series

The spectrum of the filtered output $y _ { t }$ in (4.95) is related to the spectrum of the input $x _ { t }$ by

$$
f _ {y y} (\omega) = \left| A _ {y x} (\omega) \right| ^ {2} f _ {x x} (\omega), \tag {4.98}
$$

where the frequency response function $A _ { y x } ( \omega )$ is defined in (4.97).

The result (4.98) enables us to calculate the exact effect on the spectrum of any given filtering operation. This important property shows the spectrum of the input series is changed by filtering and the effect of the change can be characterized as a frequency-by-frequency multiplication by the squared magnitude of the frequency response function. Again, an obvious analogy to a property of the variance in classical statistics holds, namely, if $x$ is a random variable with variance $\sigma _ { x } ^ { 2 }$ , then $y = a x$ will have variance $\sigma _ { y } ^ { 2 } = a ^ { 2 } \sigma _ { x } ^ { 2 }$ so the variance of the linearly transformed random variable is changed by multiplication by $a ^ { 2 }$ in much the same way as the linearly filtered spectrum is changed in (4.98).

![](images/a890955b3032aed7fe5b169f3c6601b674d0db6cae252f86986957c765aee86d.jpg)

![](images/fd1830ff4c17f23c50fb2042803a918c00e18e77e649eabbc42456d267493637.jpg)

![](images/7f15b4ee633d31f85dceb369ee53d4a09819c6c3c05683243eb5cef5f24b9c41.jpg)  
Figure 4.12 SOI series (top) compared with the differenced SOI (middle) and a centered 12-month moving average (bottom).

# Example 4.17 First Difference and Moving Average Filters

We illustrate the effect of filtering with two common examples, the first difference filter

$$
y _ {t} = \nabla x _ {t} = x _ {t} - x _ {t - 1}
$$

and the symmetric moving average filter

$$
y _ {t} = \frac {1}{2 4} \left(x _ {t - 6} + x _ {t + 6}\right) + \frac {1}{1 2} \sum_ {r = - 5} ^ {5} x _ {t - r},
$$

which is a modified Daniell kernel with $m = 6$ . The results of filtering the SOI series using the two filters are shown in the middle and bottom panels of Figure 4.12. Notice that the effect of differencing is to roughen the series because it tends to retain the higher or faster frequencies. The centered moving average smoothes the series because it retains the lower frequencies and tends to attenuate the higher frequencies. In general, differencing is an example of a high-pass filter because it retains or passes

# 4.7: Linear Filters

![](images/8ad2bf59ee978a78fef7cb0514f9cb7baf7f7e01cc94331289c19f1752800a15.jpg)  
Figure 4.13 Spectral analysis of the SOI series after applying a 12-month moving average filter. The vertical line corresponds to the 52-month cycle.

the higher frequencies, whereas the moving average is a low-pass filter because it passes the lower or slower frequencies.

Notice that the slower periods are enhanced in the symmetric moving average and the seasonal or yearly frequencies are attenuated. The filtered series makes about 9 cycles in the length of the data (about one cycle every 52 months) and the moving average filter tends to enhance or extract the signal that is associated with El Ni˜no. Moreover, by the low-pass filtering of the data, we get a better sense of the El Ni˜no effect and its irregularity. Figure 4.13 shows the results of a spectral analysis on the low-pass filtered SOI series. It is clear that all high frequency behavior has been removed and the El Ni˜no cycle is accentuated; the dotted vertical line in the figure corresponds to the 52 months cycle.

Now, having done the filtering, it is essential to determine the exact way in which the filters change the input spectrum. We shall use (4.97) and (4.98) for this purpose. The first difference filter can be written in the form (4.95) by letting $a _ { 0 } = 1 , a _ { 1 } = - 1$ , and $a _ { r } ~ = ~ 0$ otherwise. This implies that

$$
A _ {y x} (\omega) = 1 - \mathrm {e} ^ {- 2 \pi i \omega},
$$

and the squared frequency response becomes

$$
\begin{array}{l} {| A _ {y x} (\omega) | ^ {2}} = {(1 - \mathrm {e} ^ {- 2 \pi i \omega}) (1 - \mathrm {e} ^ {2 \pi i \omega})} \\ = 2 [ 1 - \cos (2 \pi \omega) ]. \tag {4.99} \\ \end{array}
$$

The top panel of Figure 4.14 shows that the first difference filter will attenuate the lower frequencies and enhance the higher frequencies be-

![](images/071c3a3b4fe577357fe6e4076f5ad98259823966856188faf8c1da0140542f75.jpg)

![](images/41387a6e8119e28c9849e226da6168836a9b8f6f0d36f89f421414f2fd421a1f.jpg)  
Figure 4.14 Squared frequency response functions of the first difference and 12-month moving average filters.

cause the multiplier of the spectrum, $| A _ { y x } ( \omega ) | ^ { 2 }$ , is large for the higher frequencies and small for the lower frequencies. Generally, the slow rise of this kind of filter does not particularly recommend it as a procedure for retaining only the high frequencies.

For the centered 12-month moving average, we can take $a _ { - 6 } = a _ { 6 } = 1 / 2 4$ , $a _ { k } = 1 / 1 2$ for $- 5 \leq k \leq 5$ and $a _ { k } = 0$ elsewhere. Substituting and recognizing the cosine terms gives

$$
A _ {y x} (\omega) = \frac {1}{1 2} \left[ 1 + \cos (1 2 \pi \omega) + 2 \sum_ {k = 1} ^ {5} \cos (2 \pi \omega k) \right]. \tag {4.100}
$$

Plotting the squared frequency response of this function as in Figure 4.14 shows that we can expect this filter to cut most of the frequency content above .05 cycles per point. This corresponds to eliminating periods shorter than $T = 1 / . 0 5 = 2 0$ points. In particular, this drives down the yearly components with periods of $T = 1 2$ months and enhances the El Ni˜no frequency, which is somewhat lower. The filter is not completely efficient at attenuating high frequencies; some power contributions are left at higher frequencies, as shown in the function $| A _ { y x } ( \omega ) | ^ { 2 }$ and in the filtered series in Figure 4.3.

The following R session shows how to filter the data, perform the spectral

# 4.7: Linear Filters

analysis of this example, and plot the squared frequency response curve of the difference filter.

> par(mfrow=c(3,1))  
> plot.ts(soi) # the data  
> plot.ts(diff(soi)) # first difference  
> k = kernel("modified.daniell", 6) #-- 12 month filter  
> soif = Kernapply(soi,k)  
> plot.ts(soif)  
> windows() # open new graphics device - use x11() in index  
> spectrum(soif, spans=9, log="no") #-- spectral analysis  
> abline(v=1/52, lty="dotted")  
> windows()  
> w = seq(0..5, length=1000) #-- frequency response  
> FR = abs(1-exp(2i*pi*w))^2  
> plot(w, FR, type="l")

The two filters discussed in the previous example were different in that the frequency response function of the first difference was complex-valued, whereas the frequency response of the moving average was purely real. A short derivation similar to that used to verify (4.98) shows, when $x _ { t }$ and $y _ { t }$ are related by the linear filter relation (4.95), the cross-spectrum satisfies

$$
f _ {y x} (\omega) = A _ {y x} (\omega) f _ {x x} (\omega),
$$

so the frequency response is of the form

$$
\begin{array}{l} A _ {y x} (\omega) = \frac {f _ {y x} (\omega)}{f _ {x x} (\omega)} (4.101) \\ = \frac {c _ {y x} (\omega)}{f _ {x x} (\omega)} - i \frac {q _ {y x} (\omega)}{f _ {x x} (\omega)}, (4.102) \\ \end{array}
$$

where we have used (4.77) to get the last form. Then, we may write (4.102) in polar coordinates as

$$
A _ {y x} (\omega) = \left| A _ {y x} (\omega) \right| \exp \{- i \phi_ {y x} (\omega) \}, \tag {4.103}
$$

where the amplitude and phase of the filter are defined by

$$
\left| A _ {y x} (\omega) \right| = \frac {\sqrt {c _ {y x} ^ {2} (\omega) + q _ {y x} ^ {2} (\omega)}}{f _ {x x} (\omega)} \tag {4.104}
$$

and

$$
\phi_ {y x} (\omega) = \tan^ {- 1} \left(- \frac {q _ {y x} (\omega)}{c _ {y x} (\omega)}\right). \tag {4.105}
$$

A simple interpretation of the phase of a linear filter is that it exhibits time delays as a function of frequency in the same way as the spectrum represents

the variance as a function of frequency. Additional insight can be gained by considering the simple delaying filter

$$
y _ {t} = A x _ {t - D},
$$

where the series gets replaced by a version, amplified by multiplying by $A$ and delayed by $D$ points. For this case,

$$
f _ {y x} (\omega) = A \mathrm {e} ^ {- 2 \pi i \omega D} f _ {x x} (\omega),
$$

and the amplitude is $| A |$ , and the phase is

$$
\phi_ {y x} (\omega) = - 2 \pi \omega D,
$$

or just a linear function of frequency $\omega$ . For this case, applying a simple time delay causes phase delays that depend on the frequency of the periodic component being delayed. Interpretation is further enhanced by setting $x _ { t } =$ $\cos ( 2 \pi \omega t )$ , in which case $y _ { t } = A \cos ( 2 \pi \omega t - 2 \pi \omega D )$ . Thus, the output series, $y _ { t }$ , has the same period as the input series, $x _ { t }$ , but the amplitude of the output has increased by a factor of $| A |$ and the phase has been changed by a factor of $- 2 \pi \omega D$ .

# Example 4.18 Amplitude and Phase of Difference and Moving Average

We consider calculating the amplitude and phase of the two filters discussed in Example 4.17. The case for the moving average is easy because $A _ { y x } ( \omega )$ given in (4.100) is purely real. So, the amplitude is just $| A _ { y x } ( \omega ) |$ and the phase is $\phi _ { y x } ( \omega ) = 0$ . In general, symmetric ( $a _ { t } = a _ { - t }$ ) filters have zero phase. The first difference, however, changes this, as we might expect from the example above involving the time delay filter. In this case, the squared amplitude is given in (4.99). To compute the phase, we write

$$
\begin{array}{l} A _ {y x} (\omega) = 1 - e ^ {- 2 \pi i \omega} \\ = e ^ {- i \pi \omega} \left(e ^ {i \pi \omega} - e ^ {- i \pi \omega}\right) \\ = 2 i e ^ {- i \pi \omega} \sin (\pi \omega) \\ = 2 \sin^ {2} (\pi \omega) + 2 i \cos (\pi \omega) \sin (\pi \omega) \\ = \frac {c _ {y x} (\omega)}{f _ {x x} (\omega)} - i \frac {q _ {y x} (\omega)}{f _ {x x} (\omega)}, \\ \end{array}
$$

so

$$
\begin{array}{l} \phi_ {y x} (\omega) = \tan^ {- 1} \left(- \frac {q _ {y x} (\omega)}{c _ {y x} (\omega)}\right) \\ = \tan^ {- 1} \left(\frac {\cos (\pi \omega)}{\sin (\pi \omega)}\right). \\ \end{array}
$$

# 4.7: Linear Filters

Noting that

$$
\cos (\pi \omega) = \sin (- \pi \omega + \pi / 2)
$$

and that

$$
\sin (\pi \omega) = \cos (- \pi \omega + \pi / 2),
$$

we get

$$
\phi_ {y x} (\omega) = - \pi \omega + \pi / 2,
$$

and the phase is again a linear function of frequency.

The above tendency of the frequencies to arrive at different times in the filtered version of the series remains as one of two annoying features of the difference type filters. The other weakness is the gentle increase in the frequency response function. If low frequencies are really unimportant and high frequencies are to be preserved, we would like to have a somewhat sharper response than is obvious in Figure 4.14. Similarly, if low frequencies are important and high frequencies are not, the moving average filters are also not very efficient at passing the low frequencies and attenuating the high frequencies. Improvement is possible by using longer filters, obtained by approximations to the infinite inverse Fourier transform. The design of filters will be discussed in §4.10 and §4.11.

We will occasionally use results for multivariate series $\pmb { x } _ { t } = ( x _ { t 1 } , \ldots , x _ { t p } ) ^ { \prime }$ that are comparable to the simple property shown in (4.98). Consider the matrix filter

$$
\boldsymbol {y} _ {t} = \sum_ {r = - \infty} ^ {\infty} A _ {r} \boldsymbol {x} _ {t - r}, \tag {4.106}
$$

where $\left\{ A _ { r } \right\}$ denotes a sequence of $q \times p$ matrices such that $\smash { \sum _ { r = - \infty } ^ { \infty } \| A _ { r } \| < \infty }$ $\pmb { x } _ { t } = ( x _ { t 1 } , \ldots , x _ { t p } ) ^ { \prime }$ is a $p \times 1$ stationary vector process with mean vector $\pmb { \mu } _ { x }$ and $p \times p$ , matrix covariance function $\Gamma _ { x x } ( h )$ and spectral matrix $f _ { x x } ( \omega )$ , and ${ \pmb y } _ { t }$ is the $q \times 1$ vector output process. Then, we can obtain the following property.

# Property P4.5: Output Spectral Matrix of a Linearly Filtered Stationary Vector Series

The spectral matrix of the filtered output ${ \pmb y } _ { t }$ in (4.106) is related to the spectrum of the input ${ \pmb x } _ { t }$ by

$$
f _ {y y} (\omega) = \mathcal {A} (\omega) f _ {x x} (\omega) \mathcal {A} ^ {*} (\omega), \tag {4.107}
$$

where the matrix frequency response function $\scriptstyle A ( \omega )$ is defined by

$$
\mathcal {A} (\omega) = \sum_ {t = - \infty} ^ {\infty} A _ {t} \exp (- 2 \pi i \omega t). \tag {4.108}
$$

# 4.8 Parametric Spectral Estimation

The methods of §4.5 lead to estimators generally referred to as nonparametric spectra because no assumption is made about the parametric form of the spectral density. In Example 4.6, we derived the spectrum of a second-order autoregressive series and we might consider basing a spectral estimator on this function, using the estimated parameters $\phi _ { 1 } , \phi _ { 2 }$ , and $\sigma _ { w } ^ { 2 }$ . Then, substituting the parameter estimates into the spectral density $f _ { x } ( \omega )$ determined in that example would lead to a parametric estimator for the spectrum. Similarly, we might fit a $p$ -th order autoregression, with the order $p$ determined by one of the model selection criteria, such as AIC, AICc, and SIC, defined in (2.18)-(2.20) for the regression model. Parametric autoregressive spectral estimators will often have superior resolution in problems when several closely spaced narrow spectral peaks are present and are preferred by engineers for a broad variety of problems (see Kay, 1988). The development of autoregressive spectral estimators has been summarized by Parzen (1983).

To be specific, consider the equation determining the order $p$ autoregressive model (2.1), written in the form

$$
x _ {t} - \sum_ {k = 1} ^ {p} \phi_ {k} x _ {t - k} = w _ {t}, \tag {4.109}
$$

where $w _ { t }$ is a white noise process with mean zero and variance $\sigma _ { w } ^ { 2 }$ . Then, note the linear filter Property P4.4, combined with equating the spectra of the leftand right-hand sides of the defining equation above yields

$$
\left| \phi \left(\mathrm {e} ^ {- 2 \pi i \omega}\right) \right| ^ {2} f _ {x} (\omega) = \sigma_ {w} ^ {2}, \tag {4.110}
$$

where

$$
\phi \left(\mathrm {e} ^ {- 2 \pi i \omega}\right) = 1 - \sum_ {k = 1} ^ {p} \phi_ {k} \mathrm {e} ^ {- 2 \pi i \omega k}. \tag {4.111}
$$

Then, denoting the maximum likelihood or least squares estimators of the model parameters by $\widehat { \phi } _ { 1 } , \widehat { \phi } _ { 2 } , \ldots , \widehat { \phi } _ { p }$ and $\widehat { \sigma } _ { w } ^ { 2 }$ , we may substitute them into the form of the spectrum implied by (4.110), obtaining

$$
\widehat {f} _ {x} (\omega) = \frac {\widehat {\sigma} _ {w} ^ {2}}{| \widehat {\phi} \left(\mathrm {e} ^ {- 2 \pi i \omega}\right) | ^ {2}}. \tag {4.112}
$$

The asymptotic distribution of the autoregressive spectral estimator has been obtained by Berk (1974) under the conditions $p \to \infty$ , $p ^ { 3 } / n  0$ as $) , n  \infty$ , which may be too severe for most applications. The limiting results imply a confidence interval of the form

$$
\frac {\widehat {f} _ {x} (\omega)}{(1 + C z _ {\alpha / 2})} \leq f _ {x} (\omega) \leq \frac {\widehat {f} _ {x} (\omega)}{(1 - C z _ {\alpha / 2})}, \tag {4.113}
$$

where

$$
C = \sqrt {2 p / n} \tag {4.114}
$$

and $z _ { \alpha / 2 }$ is ordinate corresponding to the upper $\alpha / 2$ probability of the standard normal distribution. If the sampling distribution is to be checked, we suggest applying the bootstrap estimator to get the sampling distribution of ${ \widehat { f } } _ { x } ( \omega )$ using a procedure similar to the one used for $p = 1$ in Example 3.33. An alternative for higher order autoregressive series is to put the AR(p) in statespace form and use the bootstrap procedure discussed in §6.7.

An interesting fact about rational spectra of the form (4.110) is that any spectral density can be approximated, arbitrarily close by the spectrum of an AR process.

# Property P4.6: Approximating a Spectral Density with an AR Spectrum

Let $g ( \omega )$ be the spectral density of a stationary process. Then, given $\epsilon > 0$ , there is a time series with the representation

$$
x _ {t} = \sum_ {k = 1} ^ {p} \phi_ {k} x _ {t - k} + w _ {t}
$$

where $w _ { t }$ is white noise with variance $\sigma _ { w } ^ { 2 }$ , such that

$$
\left| f _ {x} (\omega) - g (\omega) \right| <   \epsilon \quad \text {a l l} \omega \in [ - 1 / 2, 1 / 2 ].
$$

Moreover, p is finite and the roots of $\begin{array} { r } { \phi ( z ) = 1 - \sum _ { k = 1 } ^ { p } \phi _ { k } z ^ { k } } \end{array}$ are outside the unit circle.

One drawback of the property is that it does not tell us how large $p$ must be before the approximation is reasonable; in some situations $p$ may be extremely large. Property P4.6 also holds for MA and for ARMA processes in general, and a proof of the result may be found in Fuller (1996, Ch 4). For an $\mathrm { A R M A } ( p , q )$ $( p , q )$ process we would have

$$
f _ {x} (\omega) = \sigma_ {w} ^ {2} \frac {\left| \theta \left(e ^ {- 2 \pi i \omega}\right) \right| ^ {2}}{\left| \phi \left(e ^ {- 2 \pi i \omega}\right) \right| ^ {2}} \tag {4.115}
$$

where $\begin{array} { r } { \theta ( z ) = 1 + \sum _ { k = 1 } ^ { q } \theta _ { k } z ^ { k } } \end{array}$ . We demonstrate the technique in the following example.

# Example 4.19 Autoregressive Spectral Estimator of the SOI Series

Consider obtaining results comparable to the nonparametric estimators shown in Figure 4.5 for the SOI series. Fitting successively higher order models for $p = 1 , 2 , \ldots , 3 0$ yields a minimum SIC at $p = 1 5$ and a minimum AICc at $p = 1 6$ , as shown in Figure 4.15. We can see from Figure 4.15 that SIC is very definite about which model it chooses; that is, the minimum SIC is very distinct. On the other hand, it is not clear

![](images/553e7da5c62429a05f5998c78c05d3a277dbfd8f3a07d49ba09f88cf2594276c.jpg)  
Figure 4.15 Model selection criteria AICc and SIC as a function of order $p$ for autoregressive models fitted to the SOI series.

what is going to happen with AICc; that is, the minimum is not so clear, and there is some concern that AICc will start decreasing after $p = 3 0$ . Minimum AIC selects the $p = 1 5$ model (but suffers from the same uncertainty as AICc) as will be seen in the R example. The spectra of the two cases are almost identical, as shown in Figure 4.16, and we note the strong peaks at 52 months and 12 months corresponding to the nonparametric estimators obtained in 4.5. In addition, the harmonics of the yearly period is evident in the estimated spectrum.

To perform a similar analysis in R, the command spec.ar can be used to fit the best model via AIC and plot the resulting spectrum. A quick way to obtain the AIC values is to run the ar command as follows.

```txt
> spec.ar(soi, log="no") # plot min AIC spectrum
> abline(v=1/52, lty="dotted") # locate El Nino period
> abline(v=1/12, lty="dotted") # locate yearly period
> soi.ar = ar(soi, order.max=30) # obtain AICs
> plot(0:30, soi.ar$aic, type="l") # plot AICs
> soi.ar # results 
```

![](images/9aea2a2114f4f4a6e654f24f761e490ad817841d4fcb77bf30191c32ac5343cd.jpg)  
Figure 4.16 Autoregressive spectral estimators for the SOI series using models selected by AIC and SIC ( $p = 1 5$ , solid line) and by AICc ( $p = 1 6$ , dashed line). The first peak corresponds to the El Ni˜no period of 52 months.

<table><tr><td colspan="5">Coefficients:</td></tr><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>0.4237</td><td>0.0803</td><td>0.1411</td><td>0.0750</td><td>-0.0446</td></tr><tr><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>-0.0816</td><td>-0.0686</td><td>-0.0640</td><td>0.0159</td><td>0.1099</td></tr><tr><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td></tr><tr><td>0.1656</td><td>0.1482</td><td>0.0231</td><td>-0.1814</td><td>-0.1406</td></tr></table>

Order selected 15 sigmaˆ2 estimated as 0.07575

Use the command spec.ar(soi, order=16, log="no") to obtain the AR(16) spectrum.

Finally, it should be mentioned that any parametric spectrum, say $f ( \omega ; \pmb \theta )$ , depending on the vector parameter $\pmb \theta$ can be estimated via the approximate Whittle likelihood, see Whittle (1961), using the approximate properties of the discrete Fourier transform derived in Appendix C. We have that the DFTs, $d ( \omega _ { j } )$ , are approximately complex normally distributed with mean zero and variance $f ( \omega _ { j } ; \pmb { \theta } )$ and are approximately independent for $\omega _ { j } \neq \omega _ { k }$ . This implies

that an approximate log likelihood can be written in the form

$$
\ln L (\boldsymbol {x}; \boldsymbol {\theta}) \approx - \sum_ {0 <   \omega_ {j} <   1 / 2} \left(\ln f _ {x} \left(\omega_ {j}; \boldsymbol {\theta}\right) + \frac {\left| d \left(\omega_ {j}\right) \right| ^ {2}}{f _ {x} \left(\omega_ {j} ; \boldsymbol {\theta}\right)}\right), \tag {4.116}
$$

where the sum is sometimes expanded to include the frequencies $\omega _ { j } = 0 , 1 / 2$ . If the form with the two additional frequencies is used, the multiplier of the sum will be unity, except for the purely real points at $\omega _ { j } = 0 , 1 / 2$ for which the multiplier is $1 / 2$ . For a discussion of applying the Whittle approximation to the problem of estimating parameters in an ARMA spectrum, see Anderson (1978). Although this yields valid answers, it seems more involved than simply using the time domain methods discussed in Chapter 3. The Whittle likelihood will be useful in fitting long memory models that will be discussed in Chapter 5.

# 4.9 Dynamic Fourier Analysis and Wavelets

If a time series, $x _ { t }$ , is stationary, its second-order behavior remains the same, regardless of the time $t$ . It makes sense to match a stationary time series with sines and cosines because they, too, behave the same forever. Indeed, based on the Spectral Representation Theorem (Appendix C, §C.1), we may regard a stationary series as the superposition of sines and cosines that oscillate at various frequencies. As seen in this text, however, many time series are not stationary. Typically, the data are coerced into stationarity via transformations, or we restrict attention to parts of the data where stationarity appears to adhere. In some cases, the nonstationarity of a time series is of interest. That is to say, it is the local behavior of the process, and not the global behavior of the process, that is of concern to the investigator. As a case in point, we mention the explosion and earthquake series first presented in Example 1.7 (see Figure 1.7) and subsequently analyzed using Fourier methods in Example 4.13. The following example emphasizes the importance of dynamic (or time-frequency) Fourier analysis.

# Example 4.20 Dynamic Fourier Analysis of the Explosion and Earthquake Series

Consider the earthquake and explosion series displayed in Figure 1.7. As a summary of the local behavior of these series, the estimated spectra of the P and S waves in Example 4.13 leave a lot to be desired. Figures 4.17 and 4.18 show the time-frequency analysis of the earthquake and explosion series, respectively. The idea here is to summarize the spectral behavior of the signal as it evolves over time. First, a Fourier analysis is performed on a short section of the data. Then, the section is shifted, and a Fourier analysis is performed on the new section. This process is repeated until the end of the data, and the results are plotted as in Figures 4.17 and 4.18. Specifically, in this example, let $x _ { t }$ , for

#

![](images/a81a2d1788856cfebda733178693a8f6cca573a230a5bce58cae127b4edd5bcf.jpg)  
Figure 4.17 Time-frequency plot for the dynamic Fourier analysis of the earthquake series shown in Figure 1.7.

$t = 1 , \ldots , 2 0 4 8$ , represent the series of interest. Then, the sections of the data that were analyzed were $\{ x _ { t _ { k } + 1 } , \ldots , x _ { t _ { k } + 2 5 6 } \}$ , for $t _ { k } = 1 2 8 k$ , and $k = 0 , 1 , \ldots , 1 4$ . Each section was tapered using a cosine bell, and spectral estimation was performed using a triangular set of $L = 5$ weights. The sections overlap each other, however, this practice is not necessary and sometimes not desirable; see Percival and Walden (1993, 6.17) for a further discussion of this problem.

The results of the dynamic analysis are shown as the estimated spectra (for frequencies up to $\omega = . 2 5$ ) for each starting location (time), $t _ { k } =$ $1 2 8 k$ , with $k = 0 , 1 , \ldots , 1 4$ . The S component for the earthquake shows power at the low frequencies only, and the power remains strong for a long time. In contrast, the explosion shows power at higher frequencies than the earthquake, and the power of the signals (P and S waves) does not last as long as in the case of the earthquake.

The following is an R session that corresponds to a similar analysis of the explosion series in this example.

> eqexp $=$ matrix(scan("/mydata/eq5exp6.dat"), ncol=2)  
> ex $=$ eqexp[,2] # the explosion series   
> ## -- dynamic spectral analysis -- ##   
> nobs $=$ length(ex) # number of observations

![](images/4a36c412bb25aef27107ac2ccabe9b31c0a3076b5d5a27d9dd7eced589f4e627.jpg)  
Figure 4.18 Time-frequency plot for the dynamic Fourier analysis of the explosion series shown in Figure 1.7.

```txt
> wsize = 256 # window size
> overlap = 128 # overlap
> ovr = wsize-overlap
> nseg = floor(nobs/ovr)-1; # number of segments
> krnl = kernel("daniell", c(1,1)) # kernel
> ex.spec = matrix(0, wsize/2, nseg)
> for (k in 1:nseg) {
+ a = ovr*(k-1)+1
+ b = wsize+ovr*(k-1)
+ ex.spec[,k] = spectrum(ex[a:b], krnl, taper=.5, plot=F) $spec
+ }
> ## -- plot results -- ##>
x = seq(0, .5, len = nrow(ex.spec))
y = seq(0, ovr*nseg, len = ncol(ex.spec))
persp(x, y, ex.spec, zlab="Power", xlab="frequency",
+ ylab="time", ticktype = "detailed", theta=25, d=2) 
```

One way to view the time-frequency analysis of Example 4.20 is to consider it as being based on local transforms of the data $x _ { t }$ of the form

$$
d _ {j, k} = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} \psi_ {j, k} (t), \tag {4.117}
$$

![](images/eae28c1934053191714876b6aa7826255469f0b7e8f5adde5567f7d901f32b29.jpg)

![](images/03da452780e68dc6a62e849596fa00ac8fbcedef06590284cef70a0adbfc8b98.jpg)

![](images/dc64f66b9dd1b90878902e33fd0a2a3f69cca3a2ace64be1cd79ddb5f8ee9c95.jpg)

![](images/5b0fe979a61c23e3f3a774a483c548c229bb620ef1059f4910547ea6749341f7.jpg)  
Figure 4.19 Local, tapered cosines at various frequencies.

where

$$
\psi_ {j, k} (t) = \left\{ \begin{array}{l l} (n / m) ^ {1 / 2} h _ {t} e ^ {- 2 \pi i t j / m} & t \in [ t _ {k} + 1, t _ {k} + m ] \\ 0 & \text {o t h e r w i s e} \end{array} \right. \tag {4.118}
$$

where $h _ { t }$ is a taper and $m$ is some fraction of $n$ . In Example 4.20, $n = 2 0 4 8$ , $m \ : = \ : 2 5 6$ , $t _ { k } ~ = ~ 1 2 8 k$ , for $k = 0 , 1 , \ldots , 1 4$ , and $h _ { t }$ was a cosine bell taper over 256 points. In (4.117) and (4.118), $j$ indexes frequency, $\omega _ { j } = j / m$ , for $j = 1 , 2 , \dots , [ m / 2 ]$ , and $k$ indexes the location, or time shift, of the transform. In this case, the transforms are based on tapered cosines and sines that have been zeroed out over various regions in time. The key point here is that the transforms are based on local sinusoids. Figure 4.19 shows an example of four local, tapered cosine functions at various frequencies. In that figure, the length of the data is considered to be one, and the cosines are localized to a fourth of the data length.

In addition to dynamic Fourier analysis as a method to overcome the restriction of stationarity, researchers have sought various alternative methods. A recent, and successful, alternative is wavelet analysis. A website http://www.wavelet.org is devoted to wavelets, which includes information about books, technical papers, software, and links to other sites. In addi-

tion, we mention the monograph on wavelets by Daubechies (1992), the text by Percival and Walden (2000), and we note that many statistical software manufacturers have wavelet modules that sit on top of their base package. In this section, we rely primarily on the S-PLUS wavelets module (with a manual written by Bruce and Gao, 1996), however, we will present some R code where possible. The basic idea of wavelet analysis is to imitate dynamic Fourier analysis, but with functions (wavelets) that may be better suited to capture the local behavior of nonstationary time series.

Wavelets come in families generated by a father wavelet, $\phi$ , and a mother wavelet, $\psi$ . The father wavelets are used to capture the smooth, low-frequency nature of the data, whereas the mother wavelets are used to capture the detailed, and high-frequency nature of the data. The father wavelet integrates to one, and the mother wavelet integrates to zero

$$
\int \phi (t) d t = 1 \quad \text {a n d} \quad \int \psi (t) d t = 0. \tag {4.119}
$$

For a simple example, consider the Haar function,

$$
\psi (t) = \left\{ \begin{array}{r l} 1, & 0 \leq t <   1 / 2, \\ - 1, & 1 / 2 \leq t <   1, \\ 0, & \text {o t h e r w i s e .} \end{array} \right. \tag {4.120}
$$

The father in this case is $\phi ( t ) = 1$ for $t \in \mathsf { \Gamma } [ 0 , 1 )$ and zero otherwise. The Haar functions are useful for demonstrating properties of wavelets, but they do not have good time-frequency localization properties. Figure 4.20 displays two of the more commonly used wavelets that are available with the S-PLUS wavelets module, the daublet4 and symmlet8 wavelets, which are described in detail in Daubechies (1992). The number after the name refers to the width and smoothness of the wavelet; for example, the symmlet10 wavelet is wider and smoother than the symmlet8 wavelet. Daublets are one of the first type of continuous orthogonal wavelets with compact support, and symmlets were constructed to be closer to symmetry than daublets. In general, wavelets do not have an analytical form, but instead they are generated using numerical methods.

Figure 4.20 was generated in S-PLUS using the wavelet module as follows:

```r
> d4f <- wavelet("d4", mother = F)
> d4m <- wavelet("d4")
> s8f <- wavelet("s8", mother = F)
> s8m <- wavelet("s8")
> par(mfrow = c(2, 2))
> plot(d4f)
> plot(d4m)
> plot(s8f)
> plot(s8m) 
```

![](images/53645048f1dca7d861ab08770a31797d5f112dda814ef9e6261ba1fdae4279f7.jpg)

![](images/b6bdc9e88c61bf16bc5b151c0eec785bf346665f32aed6473cb33cf70d187b1e.jpg)

![](images/eb0580952b45e947475968213b3591dda72bb62830d14824fb278fc2c0bb6143.jpg)

![](images/5a46a3b03b1daef238b48c2d024db53a368b3fe872b78e937f27c8afed8f6371.jpg)  
Figure 4.20 Father and mother daublet4 wavelets (top row); father and mother symmlet8 wavelets (bottom row).

It is possible to draw some wavelets in R using the wavethresh package. In that package, daublets are called DaubExPhase and symmlets are called DaubLeAsymm. The following R session displays some of the available wavelets (this will not reproduce Figure 4.20) and it assumes the wavethresh package has been downloaded into R and then loaded at the start of the session. The filter.number determines the width and smoothness of the wavelet.

> par(mfrow=c(2,2))  
> draw.default(filter.number=2, family $\mathop { : = }$ "DaubExPhase")   
> draw.default(filter.number=4, family $\mathop { : = }$ "DaubExPhase")   
> draw.default(filter.number=4, family $\mathop { : = }$ "DaubLeAsymm")   
> draw.default(filter.number=9, family $\mathop { : = }$ "DaubLeAsymm")

When we depart from periodic functions, such as sines and cosines, the precise meaning of frequency, or cycles per unit time, is lost. When using wavelets, we typically refer to scale rather than frequency. The orthogonal

![](images/dfb9b5faba647833400960a3201f2d30966c0f08a555f0d2d1f4d859a2608fbf.jpg)

![](images/cfdbcda85738e89ad89a503f7582f35f10b0f6132cb6ef53bf85640f1be2b4f7.jpg)

![](images/be5aebd3ace257bd36804e1574b77e2a9de7fe4d56f63d41b1f4861ad21055c6.jpg)

![](images/b75e4f6a61b8c542e4ddffa1851a224651d865342b6231df4282b3fd3ec8dbf4.jpg)  
Figure 4.21 Scaled and translated daublet4 wavelets, $\psi _ { 1 , 0 } ( t )$ and $\psi _ { 2 , 1 } ( t )$ (top row); scaled and translated symmlet8 wavelets, $\psi _ { 1 , 0 } ( t )$ and $\psi _ { 2 , 1 } ( t )$ (bottom row).

wavelet decomposition of a time series, $x _ { t }$ , for $t = 1 , \ldots , n$ i s

$$
\begin{array}{l} x _ {t} = \sum_ {k} s _ {J, k} \phi_ {J, k} (t) + \sum_ {k} d _ {J, k} \psi_ {J, k} (t) \\ + \sum_ {k} d _ {J - 1, k} \psi_ {J - 1, k} (t) + \dots + \sum_ {k} d _ {1, k} \psi_ {1, k} (t), \tag {4.121} \\ \end{array}
$$

where $J$ is the number of scales, and $k$ ranges from one to the number of coefficients associated with the specified component (see Example 4.21). In (4.121), the wavelet functions $\phi _ { J , k } ( t ) , \psi _ { J , k } ( t ) , \psi _ { J - 1 , k } ( t ) , \ldots , \psi _ { 1 , k } ( t )$ are generated from the father wavelet, $\phi ( t )$ , and the mother wavelet, $\psi ( t )$ , by translation (shift) and scaling:

$$
\phi_ {J, k} (t) = 2 ^ {- J / 2} \phi \left(\frac {t - 2 ^ {J} k}{2 ^ {J}}\right), \tag {4.122}
$$

$$
\psi_ {j, k} (t) = 2 ^ {- j / 2} \psi \left(\frac {t - 2 ^ {j} k}{2 ^ {j}}\right), \quad j = 1, \dots , J. \tag {4.123}
$$

The choice of dyadic shifts and scales is arbitrary but convenient. The shift or translation parameter is $2 ^ { j } k$ , and scale parameter is $2 ^ { j }$ . The wavelet functions are spread out and shorter for larger values of $j$ (or scale parameter $2 ^ { j }$ ) and tall and narrow for small values of the scale. Figure 4.21 shows $\psi _ { 1 , 0 } ( t )$ and $\psi _ { 2 , 1 } ( t )$ generated from the daublet4 (top row), and the symmlet8 (bottom row) mother wavelets. We may think of $1 / 2 ^ { j }$ (or 1/scale) in wavelet analysis as being the analogue of frequency ( $\omega _ { j } = j / n$ ) in Fourier analysis. For example, when $j = 1$ , the scale parameter of 2 is akin to the Nyquist frequency of $1 / 2$ , and when $j = 6$ , the scale parameter of $2 ^ { 6 }$ is akin to a low frequency $1 / 2 ^ { 6 } \approx 0 . 0 1 6$ ). In other words, larger values of the scale refer to slower, smoother (or coarser) movements of the signal, and smaller values of the scale refer to faster, choppier (or finer) movements of the signal. Figure 4.21 was generated in S-PLUS using the wavelet module as follows:

> d4.1 <- wavelet("d4", level $\sp { = 1 }$ , shift=0)   
> d4.2 <- wavelet("d4", level=2, shift ${ } = 1$ )   
> s8.1 <- wavelet("s8", level $\sp { = 1 }$ , shift $\scriptstyle = 0$ )   
> s8.2 <- wavelet("s8", level $^ { = 2 }$ , shift ${ = } 1$ )   
> par(mfrow=c(2,2))  
> plot(d4.1, ylim=c(-.8,.8), xlim $\mathtt { = } \mathtt { \mathtt { c } }$ (-6,20))   
> plot(d4.2, ylim $_ { 1 } = \mathtt { c }$ (-.8,.8), xlim $\mathtt { = } \mathtt { \mathtt { c } }$ (-6,20))   
> plot(s8.1, ylim $\mathtt { \Omega } = \mathtt { \Omega }$ (-.8,.8), xlim $\mathtt { = } \mathtt { \mathtt { c } }$ (-6,20))   
> plot(s8.2, ylim $\mathtt { \Omega } = \mathtt { \Omega }$ (-.8,.8), xlim $\mathtt { = } \mathtt { \mathtt { c } }$ (-6,20))

The discrete wavelet transform (DWT) of the data $x _ { t }$ are the coefficients $^ { s _ { J , k } }$ and $d _ { j , k }$ for $j = J , J - 1 , \dotsc , 1$ , in (4.121). To some degree of approximation, they are given by15

$$
s _ {J, k} = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} \phi_ {J, k} (t), \tag {4.124}
$$

$$
d _ {j, k} = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} \psi_ {j, k} (t) j = J, J - 1, \dots , 1. \tag {4.125}
$$

It is the magnitudes of the coefficients that measure the importance of the corresponding wavelet term in describing the behavior of $x _ { t }$ . As in Fourier analysis, the DWT is not computed as shown but is calculated using a fast algorithm. The $^ { s _ { J , k } }$ are called the smooth coefficients because they represent the smooth behavior of the data. The $d _ { j , k }$ are called the detail coefficients because they tend to represent the finer, more high-frequency nature, of the data.

![](images/c957c9c4af9016e9f15c3a60fb35a719b7c584af9400a367d27c7269c7a845d4.jpg)

![](images/903184a9e65f93e65d1d03648c124615928a97db2720fd455aa3523913c90ecc.jpg)  
Figure 4.22 Discrete wavelet transform of the earthquake series using the symmlet8 wavelets, and $J = 6$ levels of scale.   
Figure 4.23 Discrete wavelet transform of the explosion series using the symmlet8 wavelets and $J = 6$ levels of scale.

# Example 4.21 Wavelet Analysis of the Explosion and Earthquake Series

Figures 4.22 and 4.23 show the DWTs, based on the symmlet8 wavelet basis, for the earthquake and explosion series, respectively. Each series is of length $n = 2 ^ { 1 1 } = 2 0 4 8$ , and in this example, the DWTs are calculated using $J = 6$ levels. In this case, $n / 2 = 2 ^ { 1 0 } = 1 0 2 4$ values are in $d 1 =$ $\{ d _ { 1 , k }$ ; $k = 1 , \ldots , 2 ^ { 1 0 } \}$ , $n / 2 ^ { 2 } = 2 ^ { 9 } = 5 1 2$ values are in $d 2 = \{ d _ { 2 , k }$ ; $k =$ $1 , \ldots , 2 ^ { 9 } \}$ , and so on, until finally, $n / 2 ^ { 6 } = 2 ^ { 5 } = 3 2$ values are in $d 6$ and in $s 6$ . The detail values $d _ { 1 , k } , \ldots , d _ { 6 , k }$ are plotted at the same scale, and hence, the relative importance of each value can be seen from the graph. The smooth values $s _ { 6 , k }$ are typically larger than the detail values and plotted on a different scale. The top of Figures 4.22 and 4.23 show the inverse DWT (IDWT) computed from all of the coefficients. The displayed IDWT is a reconstruction of the data, and it reproduces the data except for round-off error.

Comparing the DWTs, the earthquake is best represented by wavelets with larger scale than the explosion. One way to measure the importance of each level, $d 1 , d 2 , \dotsc , d 6 , s 6$ , is to evaluate the proportion of the total $x _ { t }$ wer, for ch le $t = 1 , \ldots , n$ $\begin{array} { r } { T P = \sum _ { t = 1 } ^ { n } x _ { t } ^ { 2 } } \end{array}$ The total power of a time series. The total power associated with $n = 2 ^ { 1 1 }$

$$
T P _ {6} ^ {s} = \sum_ {k = 1} ^ {n / 2 ^ {6}} s _ {6, k} ^ {2} \quad \text {a n d} \quad T P _ {j} ^ {d} = \sum_ {k = 1} ^ {n / 2 ^ {j}} d _ {j, k} ^ {2}, \quad j = 1, \dots , 6.
$$

Because we are working with an orthogonal basis, we have

$$
T P = T P _ {6} ^ {s} + \sum_ {j = 1} ^ {6} T P _ {j} ^ {d},
$$

and the proportion of the total power explained by each level of detail would be the ratios $T P _ { j } ^ { d } / T P$ for $j = 1 , \ldots , 6$ , and for the smooth level, it would be $T P _ { 6 } ^ { s } / T P$ . These values are listed in Table 4.2. From that table nearly 80% of the total power of the earthquake series is explained by the higher scale details $d 4$ and $d 5$ , whereas 90% of the total power is explained by the smaller scale details $d 2$ and $d 3$ for the explosion.

Figures 4.24 and 4.25 show the time-scale plots based on the DWT of the earthquake series and the explosion series, respectively. These figures are the wavelet analog of the time-frequency plots shown in Figures 4.17 and 4.18. The power axis represents the magnitude of each value $d _ { j k }$ or $s _ { 6 , k }$ . The time axis matches the time axis in the DWTs shown in Figures 4.22 and 4.23, and the scale axis is plotted as 1/scale, listed from the coarsest scale to the finest scale. On the 1/scale axis, the coarsest scale values,

Table 4.2 Fraction of the Total Power for the DWTs of the Earthquake and the Explosion   

<table><tr><td>Component</td><td>Earthquake</td><td>Explosion</td></tr><tr><td>s6</td><td>0.009</td><td>0.002</td></tr><tr><td>d6</td><td>0.043</td><td>0.002</td></tr><tr><td>d5</td><td>0.377</td><td>0.007</td></tr><tr><td>d4</td><td>0.367</td><td>0.015</td></tr><tr><td>d3</td><td>0.160</td><td>0.559</td></tr><tr><td>d2</td><td>0.040</td><td>0.349</td></tr><tr><td>d1</td><td>0.003</td><td>0.066</td></tr></table>

![](images/5d1d3988338f1755deec8d019591261431d12b4441abc482920e50e6affdc340.jpg)  
  
Figure 4.24 Time-scale plot of the earthquake series.

represented by the smooth coefficients $s 6$ , are plotted over the range $[ 0 , 2 ^ { - 6 } )$ , the coarsest detail values, $d 6$ , are plotted over $[ 2 ^ { - 6 } , 2 ^ { - 5 } )$ , and so on. In these figures, we did not plot the finest scale values, $d 1$ , so the finest scale values exhibited in Figures 4.24 and 4.25 are in $d 2$ , which are plotted over the range $[ 2 ^ { - 2 } , 2 ^ { - 1 } )$ .

The conclusions drawn from these plots are the same as those drawn from Figures 4.17 and 4.18. That is, the S wave for the earthquake shows power at the high scales (or low 1/scale) only, and the power remains strong for a long time. In contrast, the explosion shows power at smaller scales (or higher 1/scale) than the earthquake, and the power of the signals (P and S waves) do not last as long as in the case of the earthquake.

The analyses of this example were performed using the S-PLUS wavelets

![](images/dc0cc1f4d664cf68b6c7267037f3e3ca33f00035ceb496e1629d722b166a9196.jpg)  
Figure 4.25 Time-scale plot of the explosion series.

module (which must be loaded prior to the analyses) as follows:

> eqexp <- matrix(scan("/mydata/eq5exp6.dat"), ncol=2)   
> eq <- eqexp[,1] # the earthquake series   
> ex <- eqexp[,2] # the explosion series   
> eq.dwt <- dwt(eq)   
> ex.dwt <- dwt(ex)   
> plot(eq.dwt)  
> plot(ex.dwt)  
> # -- energy distributions (Table 4.2) --#   
> dotchart(eq.dwt) # a graphic   
> summary(eq.dwt) # numerical details   
> dotchart(ex.dwt)   
> summary(ex.dwt)   
> #-- time scale plots (Figs 4.24-4.25 but not in 3d) --#   
> time.scale.plot(eq.dwt)  
> time.scale.plot(ex.dwt)

Similar analyses may be performed in R using the wavethresh or the waveslim packages. We exhibit the analysis for the earthquake series using waveslim, assuming it has been downloaded into R and then loaded at the start of the R session.

> eq.dwt $=$ dwt(eq, n.levels=6)   
> # -- plot the dwt and calculate TP -- #   
> TP = matrix(0,7,1)   
> par(mfcol=c(7,1), pty="m", mar=c(3,4,2,2))   
> for(i in 1:6){

```txt
+ plot.ts(up.sample(eq.dwt[[i]], 2^i), type="h", axes=F, + ylab=names(eq.dwt)[i])
+ abline(h=0)
+ axis(side=2)
+ TP[i]=sum(eq.dwt[[i]]^2)
+ }
> plot.ts(up.sample(eq.dwt[[7]], 2^6), type="h", axes=F, + ylab=names(eq.dwt)[7])
> abline(h=0)
> axis(side=2)
> axis(side=1)
> TP[7]=sum(eq.dwt[[7]]^2)
> TP/sum(eq^2) # the energy distribution 
```

In the R code, we plotted the wavelet transform on different scales. To plot the ordinates of the wavelet transforms on the same scale, include a command like ylim=c(-1.5,1.5) in each plot.ts() command.

Wavelets can be used to perform nonparametric smoothing along the lines first discussed in §2.4, but with an emphasis on localized behavior. Although a considerable amount of literature exists on this topic, we will present the basic ideas. For further information, we refer the reader to Donoho and Johnstone (1994, 1995). As in §2.4, we suppose the data $x _ { t }$ can be written in terms of a signal plus noise model as

$$
x _ {t} = s _ {t} + \epsilon_ {t}. \tag {4.126}
$$

The goal here is to remove the noise from the data, and obtain an estimate of the signal, $s _ { t }$ , without having to specify a parametric form of the signal. The technique based on wavelets is referred to as waveshrink.

The basic idea behind waveshrink is to shrink the wavelet coefficients in the DWT of $x _ { t }$ toward zero in an attempt to denoise the data and then to estimate the signal via (4.121) with the new coefficients. One obvious way to shrink the coefficients toward zero is to simply zero out any coefficient smaller in magnitude than some predetermined value, $\lambda$ . Such a shrinkage rule is discontinuous and sometimes it is preferable to use a continuous shrinkage function. One such method, termed soft shrinkage, proceeds as follows. If the value of a coefficient is $a$ , we set that coefficient to zero if $| a | \le \lambda$ , and to $\mathrm { s i g n } ( a ) ( | a | - \lambda )$ if $| a | > \lambda$ . The choice of a shrinkage method is based on the goal of the signal extraction. This process entails choosing a value for the shrinkage threshold, $\lambda$ , and we may wish to use a different threshold value, say, $\lambda _ { j }$ , for each level of scale $j = 1 , \dots , J$ . One particular method that works well if we are interested in a relatively high degree of smoothness in the estimate is to choose $\lambda = \widehat { \sigma } _ { \epsilon } \sqrt { 2 \log n }$ for all scale levels, where $\widehat { \sigma } _ { \epsilon }$ is an estimate of the scale of the noise, $\sigma _ { \epsilon }$ . Typically a robust estimate of $\sigma _ { \epsilon }$ is used, e.g., the median of the absolute deviations of the data from the median (MAD). For other thresholding techniques or for a better understanding of waveshrink, see

Donoho and Johnstone (1994, 1995), or the S-PLUS wavelets module manual (Bruce and Gao, 1996, Ch 6).

# Example 4.22 Waveshrink Analysis of the Explosion and Earthquake Series

Figure 4.26 shows the results of a waveshrink analysis on the earthquake and explosion series. In this example, soft shrinkage was used with a universal threshold of $\lambda = \widehat { \sigma } _ { \epsilon } \sqrt { 2 \log n }$ where $\widehat { \sigma } _ { \epsilon }$ is the MAD. Figure 4.26 displays the data $x _ { t }$ , the estimated signal $\widehat { s } _ { t }$ , as well as the residuals $x _ { t } - \hat { s } _ { t }$ . According to this analysis, the earthquake is mostly signal and characterized by prolonged energy, whereas the explosion is comprised of short bursts of energy.

Figure 4.26 was generated in S-PLUS using the wavelets module. For example, the analysis of the earthquake series was performed as follows.

> eq.dwt <- dwt(eq)   
> eq.shrink <- waveshrink(eq.dwt, shrink.rule="universal",

+ shrink.fun $\cdot ^ { = }$ "soft")

In R, using the waveslim package for the earthquake series, use the following commands.

> eq.dwt $=$ dwt(eq, n.levels=6)   
> eq.trsh $=$ universal.thresh(eq.dwt, hard=F)   
> eq.smo $=$ idwt(eq.trsh)   
> par(mfrow=c(3,1))   
> plot.ts(eq, ylab $\underline { { \underline { { \mathbf { \Pi } } } } } =$ "Earthquake", ylim=c(-.5,.5))   
> plot.ts(eq.smo,ylab $\mid =$ "Smoothed Earthquake",ylim=c(-.5,.5))   
> plot.ts(eq-eq.smo, ylab $=$ "Noise", ylim=c(-.5,.5))

# 4.10 Lagged Regression Models

One of the intriguing possibilities offered by the coherence analysis of the relation between the SOI and Recruitment series discussed in Example 4.16 would be extending classical regression to the analysis of lagged regression models of the form

$$
y _ {t} = \sum_ {r = - \infty} ^ {\infty} \beta_ {r} x _ {t - r} + v _ {t}, \tag {4.127}
$$

where $v _ { t }$ is a stationary noise process, $x _ { t }$ is the observed input series, and $y _ { t }$ is the observed output series. We are interested in estimating the filter coefficients $\beta _ { r }$ relating the adjacent lagged values of $x _ { t }$ to the output series $y _ { t }$ .

In the case of SOI and Recruitment series, we might identify the El Ni˜no driving series, SOI, as the input, $x _ { t }$ , and $y _ { t }$ , the Recruitment series, as the

![](images/6bfa4fa2944c2fe7c86f84be72275d7e7d3a68ed345b761dc995314d8c999ab3.jpg)  
Figure 4.26 Waveshrink estimates of the earthquake signal and of the explosion signal.

output. In general, there will be more than a single possible input series and we may envision a $q \times 1$ vector of driving series. This multivariate input situation is covered in Chapter 7. The model given by (4.127) is useful under several different scenarios, corresponding to different assumptions that can be made about the components.

We assume that the inputs and outputs have zero means and are jointly stationary with the $2 \times 1$ vector process $( x _ { t } , y _ { t } ) ^ { \prime }$ having a spectral matrix of

# 4.10: Lagged Regression

the form

$$
f (\omega) = \left( \begin{array}{c c} f _ {x x} (\omega) & f _ {x y} (\omega) \\ f _ {y x} (\omega) & f _ {y y} (\omega) \end{array} \right). \tag {4.128}
$$

Here, $f _ { x y } ( \omega )$ is the cross-spectrum relating the input $x _ { t }$ to the output $y _ { t }$ , and $f _ { x x } ( \omega )$ and $f _ { y y } ( \omega )$ are the spectra of the input and output series, respectively. Generally, we observe two series, regarded as input and output and search for regression functions $\{ \beta _ { t } \}$ relating the inputs to the outputs. We assume all autocovariance functions satisfy the absolute summability conditions of the form (4.31).

Then, minimizing the mean squared error

$$
M S E = E \left(y _ {t} - \sum_ {r = - \infty} ^ {\infty} \beta_ {r} x _ {t - r}\right) ^ {2} \tag {4.129}
$$

leads to the usual orthogonality conditions

$$
E \left[ \left(y _ {t} - \sum_ {r = - \infty} ^ {\infty} \beta_ {r} x _ {t - r}\right) x _ {t - s} \right] = 0 \tag {4.130}
$$

for all $s = 0 , \pm 1 , \pm 2 , . . .$ . Taking the expectations inside leads to the normal equations

$$
\sum_ {r = - \infty} ^ {\infty} \beta_ {r} \gamma_ {x x} (s - r) = \gamma_ {y x} (s) \tag {4.131}
$$

for $s = 0 , \pm 1 , \pm 2 , . . .$ . These equations might be solved, with some effort, if the covariance functions were known exactly. If data $( x _ { t } , y _ { t } )$ for $t = 1 , . . . , n$ are available, we might use a finite approximation to the above equations with $\widehat { \gamma } _ { x x } ( h )$ and $\widehat { \gamma } _ { y x } ( h )$ substituted into (4.131). If the regression vectors are essentially zero for $| s | \ge M / 2$ , and $M < n$ , the system (4.131) would be of full rank and the solution would involve inverting an $( M - 1 ) \times ( M - 1 )$ matrix.

A frequency domain approximate solution is easier in this case for two reasons. First, the computations depend on spectra and cross-spectra that can be estimated from sample data using the techniques of §4.6. In addition, no matrices will have to be inverted, although the frequency domain ratio will have to be computed for each frequency. In order to develop the frequency domain solution, substitute the representation (4.85) into the normal equations, using the convention defined in (4.128). The left side of (4.131) can then be written in the form

$$
\int_ {- 1 / 2} ^ {1 / 2} \sum_ {r = - \infty} ^ {\infty} \beta_ {r} \mathrm {e} ^ {2 \pi i \omega (s - r)} f _ {x x} (\omega) d \omega = \int_ {- 1 / 2} ^ {1 / 2} \mathrm {e} ^ {2 \pi i \omega s} B (\omega) f _ {x x} (\omega) d \omega ,
$$

where

$$
B (\omega) = \sum_ {r = - \infty} ^ {\infty} \beta_ {r} \mathrm {e} ^ {- 2 \pi i \omega r} \tag {4.132}
$$

is the Fourier transform of the regression coefficients $\beta _ { t }$ . Now, because $\gamma _ { y x } ( s )$ is the inverse transform of the cross-spectrum $f _ { y x } ( \omega )$ , we might write the system of equations in the frequency domain, using the uniqueness of the Fourier transform, as

$$
B (\omega) f _ {x x} (\omega) = f _ {y x} (\omega), \tag {4.133}
$$

which then become the analogs of the usual normal equations. Then, we may take

$$
\widehat {B} \left(\omega_ {k}\right) = \frac {\widehat {f} _ {y x} \left(\omega_ {k}\right)}{\widehat {f} _ {x x} \left(\omega_ {k}\right)} \tag {4.134}
$$

as the estimator for the Fourier transform of the regression coefficients, evaluated at some subset of fundamental frequencies $\omega _ { k } = k / M$ with $M < < n$ . Generally, we assume smoothness of $B ( \cdot )$ over intervals of the form $\{ \omega _ { k } + \ell / n$ ; $\ell =$ $- ( L - 1 ) / 2 , \dots , ( L - 1 ) / 2 \}$ . The inverse transform of the function $\widehat B ( \omega )$ would give ${ \hat { \beta } } _ { t }$ , and we note that the discrete time approximation can be taken as

$$
\widehat {\beta} _ {t} = M ^ {- 1} \sum_ {k = 0} ^ {M - 1} \widehat {B} \left(\omega_ {k}\right) \mathrm {e} ^ {2 \pi i \omega_ {k} t} \tag {4.135}
$$

for $t = 0 , \pm 1 , \pm 2 , \dots , \pm ( M / 2 - 1 )$ . If we were to use (4.135) to define ${ \widehat { \beta } } _ { t }$ for $| t | \geq M / 2$ , we would end up with a sequence of coefficients that is periodic with a period of $M$ . In practice we define $\widehat { \beta } _ { t } = 0$ for $| t | \geq M / 2$ instead. Problem 4.32 explores the error resulting from this approximation.

# Example 4.23 Lagged Regression Results for SOI and Recruitment Series

The high coherence between the SOI and Recruitment series noted in Example 4.16 suggests a lagged regression relation between the two series. A natural direction for the implication in this situation is implied because we feel that the sea surface temperature or SOI should be the input and the Recruitment series should be the output. With this in mind, let $x _ { t }$ be the SOI series and $y _ { t }$ the Recruitment series.

Although we think naturally of the SOI as the input and the Recruitment as the output, two input-output configurations are of interest. With SOI as the input, the model is

$$
y _ {t} = \sum_ {r = - \infty} ^ {\infty} a _ {r} x _ {t - r} + w _ {t}
$$

whereas a model that reverses the two roles would be

$$
x _ {t} = \sum_ {r = - \infty} ^ {\infty} b _ {r} y _ {t - r} + v _ {t},
$$

![](images/54ae27156fdf85a49961fe88f3703dc6ad9262718d524f18d17768fb82d0e4a0.jpg)

![](images/989fac0d1ca45de3e7740c307a158c3645dcdd5d412500be13ea61c591fc15bc.jpg)  
Figure 4.27 Estimated impulse response functions relating SOI to Recruitment (top) and Recruitment to SOI (bottom) $L = 1 5 , M = 3 2$ .

where $w _ { t }$ and $v _ { t }$ are white noise processes. Even though there is no plausible environmental explanation for the second of these two models, displaying both possibilities helps to settle on a parsimonious transfer function model. The two estimated regression or impulse response functions with $M \ = \ 3 2$ and $L = 1 5$ are shown in Figure 4.27. Note the negative peak at a lag of five points in the first of the two situations where the SOI series is assumed to be the input. The fall-off after lag five seems to be approximately exponential. A possible model for this situation is

$$
y _ {t} = - 2 2 x _ {t - 5} - 1 5 x _ {t - 6} - 1 1 x _ {t - 7} - 1 0 x _ {t - 8} - 7 x _ {t - 9} - \ldots + w _ {t}.
$$

If we examine the inverse relation, namely, a regression model with the Recruitment series $y _ { t }$ as the input, we get a much simpler model that

seems to depend on only two coefficients, namely,

$$
x _ {t} = . 0 1 2 y _ {t + 4} - . 0 1 8 y _ {t + 5} + v _ {t},
$$

or, shifting by five points and transposing,

$$
y _ {t} = . 6 6 7 y _ {t - 1} - 5 6 x _ {t - 5} + \epsilon_ {t},
$$

where $\epsilon _ { t }$ is white noise. Using the backshift operator, we may write

$$
(1 -. 6 6 7 B) y _ {t} = - 5 6 B ^ {5} x _ {t} + \epsilon_ {t}.
$$

The analysis of this example was performed using the time series package ASTSA, which is available for download from the website of this text.

The example shows we can get a clean estimator for the transfer functions relating the two series if the coherence $\widehat { \rho } _ { x y } ^ { 2 } ( \omega )$ is large. The reason is that we can write the minimized mean squared error (4.129) as

$$
M S E = E \Big [ \big (y _ {t} - \sum_ {r = - \infty} ^ {\infty} \beta_ {r} x _ {t - r} \big) y _ {t} \Big ] = \gamma_ {y y} (0) - \sum_ {r = - \infty} ^ {\infty} \beta_ {r} \gamma_ {x y} (- r),
$$

using the result about the orthogonality of the data and error term in the Projection theorem. Then, substituting the spectral representations of the autocovariance and cross-covariance functions and identifying the Fourier transform (4.132) in the result leads to

$$
\begin{array}{l} M S E = \int_ {- 1 / 2} ^ {1 / 2} \left[ f _ {y y} (\omega) - B (\omega) f _ {x y} (\omega) \right] d \omega \\ = \int_ {- 1 / 2} ^ {1 / 2} f _ {y y} (\omega) \left[ 1 - \rho_ {y x} ^ {2} (\omega) \right] d \omega , \tag {4.136} \\ \end{array}
$$

where $\rho _ { y x } ^ { 2 } ( \omega )$ is just the squared coherence given by (4.83). The similarity of (4.136) to the usual mean square error that results from predicting $y$ from $x$ is obvious. In that case, we would have

$$
E (y - \beta x) ^ {2} = \sigma_ {y} ^ {2} (1 - \rho_ {x y} ^ {2})
$$

for jointly distributed random variables $x$ and $y$ with zero means, variances $\sigma _ { x } ^ { 2 }$ and $\sigma _ { y } ^ { 2 }$ , and covariance $\sigma _ { x y } = \rho _ { x y } \sigma _ { x } \sigma _ { y }$ . Because the mean squared error in (4.136) satisfies $M S E \ge 0$ with $f _ { y y } ( \omega )$ a non-negative function, it follows that the coherence satisfies

$$
0 \leq \rho_ {x y} ^ {2} (\omega) \leq 1
$$

for all $\omega$ . Furthermore, Problem 4.33 shows the squared coherence is one when the output are linearly related by the filter relation (4.127), and there

is no noise, i.e., $v _ { t } = 0$ . Hence, the multiple coherence gives a measure of the association or correlation between the input and output series as a function of frequency.

The matter of verifying that the $F$ -distribution claimed for (4.93) will hold when the sample coherence values are substituted for theoretical values still remains. Again, the form of the $F ^ { \prime }$ -statistic is exactly analogous to the usual $t$ - test for no correlation in a regression context. We give an argument leading to this conclusion later using the results in Appendix C, C.3. Another question that has not been resolved in this section is the extension to the case of multiple inputs $x _ { t 1 } , x _ { t 2 } , \ldots , x _ { t q }$ . Often, more than just a single input series is present that can possibly form a lagged predictor of the output series $y _ { t }$ . An example is the cardiovascular mortality series that depended on possibly a number of pollution series and temperature. We discuss this particular extension as a part of the multivariate time series techniques considered in Chapter 7.

# 4.11 Signal Extraction and Optimum Filtering

A model closely related to regression can be developed by assuming again that

$$
y _ {t} = \sum_ {r = - \infty} ^ {\infty} \beta_ {r} x _ {t - r} + v _ {t}, \tag {4.137}
$$

but where the $\beta$ s are known and $x _ { t }$ is some unknown random signal that is uncorrelated with the noise process $v _ { t }$ . In this case, we observe only $y _ { t }$ and are interested in an estimator for the signal $x _ { t }$ of the form

$$
\widehat {x} _ {t} = \sum_ {r = - \infty} ^ {\infty} a _ {r} y _ {t - r}. \tag {4.138}
$$

In the frequency domain, it is convenient to make the additional assumptions that the series $x _ { t }$ and $v _ { t }$ are both mean-zero stationary series with spectra $f _ { x x } ( \omega )$ and $f _ { v v } ( \omega )$ , often referred to as the signal spectrum and noise spectrum, respectively. Often, the special case $\beta _ { t } = \delta _ { t }$ , in which $\delta _ { t }$ is the Kronecker delta, is of interest because (4.137) reduces to the simple signal plus noise model

$$
y _ {t} = x _ {t} + v _ {t} \tag {4.139}
$$

in that case. In general, we seek the set of filter coefficients $a _ { t }$ that minimize the mean squared error of estimation, say,

$$
M S E = E \left[ \left(x _ {t} - \sum_ {r = - \infty} ^ {\infty} a _ {r} y _ {t - r}\right) ^ {2} \right]. \tag {4.140}
$$

This problem was originally solved by Kolmogorov (1941) and by Wiener (1949), who derived the result in 1941 and published it in classified reports during World War II.

We can apply the orthogonality principle to write

$$
E [ (x _ {t} - \sum_ {r = - \infty} ^ {\infty} a _ {r} y _ {t - r}) y _ {t - s} ] = 0
$$

for $s = 0 , \pm 1 , \pm 2 , . . .$ , which leads to

$$
\sum_ {r = - \infty} ^ {\infty} a _ {r} \gamma_ {y y} (s - r) = \gamma_ {x y} (s),
$$

to be solved for the filter coefficients. Substituting the spectral representations for the autocovariance functions into the above and identifying the spectral densities through the uniqueness of the Fourier transform produces

$$
A (\omega) f _ {y y} (\omega) = f _ {x y} (\omega), \tag {4.141}
$$

where $A ( \omega )$ and the optimal filter $a _ { t }$ are Fourier transform pairs, as in Definition 4.1 for $B ( \omega )$ and $\beta _ { t }$ . Now, a special consequence of the model is that (see Problem 4.23)

$$
f _ {x y} (\omega) = \overline {{B (\omega)}} f _ {x x} (\omega) \tag {4.142}
$$

and

$$
f _ {y y} (\omega) = | B (\omega) | ^ {2} f _ {x x} (\omega) + f _ {v v} (\omega), \tag {4.143}
$$

implying the optimal filter would be Fourier transform of

$$
A (\omega) = \frac {\overline {{B (\omega)}}}{\left(| B (\omega) | ^ {2} + \frac {f _ {v v} (\omega)}{f _ {x x} (\omega)}\right)}, \tag {4.144}
$$

where the second term in the denominator is just the inverse of the signal to noise ratio, say,

$$
\operatorname {S N R} (\omega) = \frac {f _ {x x} (\omega)}{f _ {v v} (\omega)}. \tag {4.145}
$$

The result shows the optimum filters can be computed for this model if the signal and noise spectra are both known or if we can assume knowledge of the signal-to-noise ratio SNR( $\omega$ ) as function of frequency. In Chapter 7, we show some methods for estimating these two parameters in conjunction with random effects analysis of variance models, but we assume here that it is possible to specify the signal-to-noise ratio a priori. If the signal-to-noise ratio is known, the optimal filter can be computed by the inverse transform of the function $A ( \omega )$ . It is more likely that the inverse transform will be intractable and a finite filter approximation like that used in the previous section can be applied to the data. In this case, we will have

$$
a _ {t} ^ {M} = M ^ {- 1} \sum_ {k = 0} ^ {M - 1} A \left(\omega_ {k}\right) \mathrm {e} ^ {2 \pi i \omega_ {k} t} \tag {4.146}
$$

![](images/6a8fd6e2cf3fa060688ac5d1e20bfca1825ea37f311d0de3692378e2f6a394b5.jpg)

![](images/5ee470d3ed66fb2e4cbec3fc6a395140bb30e8fd93379fb2b8035d1d17df825c.jpg)

![](images/c479d182d82e052485e88efe2a7f0db7359907ee775aa9bf9f1bbf4ae083bd3a.jpg)

![](images/e6532bb9794732941f6cb99d089b6d7deeaf2c04e7f7fc75bb614400f68062b9.jpg)  
Figure 4.28 Impulse Response and frequency response functions for designed SOI filters. Note the ripples in the top panel frequency response of the untapered filter.

as the estimated filter function. It will often be the case that the form of the specified frequency response will have some rather sharp transitions between regions where the signal-to-noise ratio is high and regions where there is little signal. In these cases, the shape of the frequency response function will have ripples that can introduce frequencies at different amplitudes. An aesthetic solution to this problem is to introduce tapering as was done with spectral estimation in (4.61)-(4.68). We use below the tapered filter $\tilde { a } _ { t } = h _ { t } a _ { t }$ where $h _ { t }$ is the cosine taper given in (4.68). The squared frequency response of the resulting filter will be $| \tilde { A } ( \omega ) | ^ { 2 }$ , where

$$
\tilde {A} (\omega) = \sum_ {t = - \infty} ^ {\infty} a _ {t} h _ {t} \mathrm {e} ^ {- 2 \pi i \omega t}. \tag {4.147}
$$

The results are illustrated in the following example that extracts the El Ni˜no component of the sea surface temperature series.

![](images/287f467a8260471bd674201bd8d69cfe8e8b488bdd93354895e1694d2ee715a9.jpg)

![](images/2939b99c8517e65e285ce15391ab0d3a532779a190b35a55d15d4b8852853555.jpg)  
Filtered SOI   
Figure 4.29 Original SOI series (top) compared to filtered version showing the estimated El Ni˜no temperature signal (bottom).

# Example 4.24 Estimating the El Ni˜no Signal Using Optimal Filters

Figure 4.5 shows the spectrum of the SOI series, and we note that essentially two components have power, the El Ni˜no frequency of about .02 cycles per month and a yearly frequency of about .08 cycles per month. We assume, for this example, that we wish to preserve the lower frequency as signal and to eliminate the higher order frequencies. In this case, we assume the simple signal plus noise model

$$
y _ {t} = x _ {t} + v _ {t},
$$

so that there is no convolving function $\beta _ { t }$ . Furthermore, the signal-tonoise ratio is assumed to be high to about .06 cycles per month and zero thereafter. The optimal frequency response was assumed to be unity to .05 cycles per point and then to decay linearly to zero in several steps. Figure 4.28 shows the Fourier transform, (4.146), at $M = 6 4$ frequencies, say, $a _ { t } ^ { M }$ and the tapered version $h _ { t } a _ { t } ^ { M }$ . The estimated squared frequency response, approximated as a long (256 point) transform of the form (4.147), has ripples when tapering is not applied and is relatively smooth for the tapered filter. Figure 4.28 shows both positive and negative frequencies. Figure 4.29 shows the original and filtered SOI index, and we see a smooth extracted signal that conveys the essence of the

underlying El Ni˜no signal. The frequency response of the designed filter can be compared with that of the symmetric 12-month moving average applied to the same series in Example 4.17. The filtered series, shown in Figure 4.3, shows a good deal of higher frequency chatter riding on the smoothed version, which has been introduced by the higher frequencies that leak through in the squared frequency response, as in Figure 4.14.

The analysis of this example was performed using the time series package ASTSA, which is available for download from the website of this text.

The design of finite filters with a specified frequency response requires some experimentation with various target frequency response functions and we have only touched on the methodology here. The filter designed here, sometimes called a low-pass filter reduces the high frequencies and keeps or passes the low frequencies. Alternately, we could design a high-pass filter to keep high frequencies if that is where the signal is located. An example of a simple high-pass filter is the first difference with a frequency response that is shown in Figure 4.14. We can also design band-pass filters that keep frequencies in specified bands. For example, seasonal adjustment filters are often used in economics to reject seasonal frequencies while keeping both high frequencies, lower frequencies, and trend (see, for example, Grether and Nerlove, 1970).

The filters we have discussed here are all symmetric two-sided filters, because the designed frequency response functions were purely real. Alternatively, we may design recursive filters to produce a desired response. An example of a recursive filter is one that replaces the input $x _ { t }$ by the filtered output

$$
y _ {t} = \sum_ {k = 1} ^ {p} \phi_ {k} y _ {t - k} + x _ {t} - \sum_ {k = 1} ^ {q} \theta_ {k} x _ {t - k}. \tag {4.148}
$$

Note the similarity between (4.148) and the ARIMA $( p , 1 , q )$ model, in which the white noise component is replaced by the input. Transposing the terms involving $y _ { t }$ and using the basic linear filter result in Property 4.4 leads to

$$
f _ {y} (\omega) = \frac {\left| \theta \left(\mathrm {e} ^ {- 2 \pi i \omega}\right) \right| ^ {2}}{\left| \phi \left(\mathrm {e} ^ {- 2 \pi i \omega}\right) \right| ^ {2}} f _ {x} (\omega), \tag {4.149}
$$

where

$$
\phi (\mathrm {e} ^ {- 2 \pi i \omega}) = 1 - \sum_ {k = 1} ^ {p} \phi_ {k} \mathrm {e} ^ {- 2 \pi i k \omega}
$$

and

$$
\theta (\mathrm {e} ^ {- 2 \pi i \omega}) = 1 - \sum_ {k = 1} ^ {q} \theta_ {k} \mathrm {e} ^ {- 2 \pi i k \omega}.
$$

Recursive filters such as those given by (4.149) distort the phases of arriving frequencies, and we do not consider the problem of designing such filters in any detail.

# 4.12 Spectral Analysis of Multidimensional Series

Multidimensional series of the form $x s$ , where $\pmb { \mathscr { s } } = ( \mathscr { s } _ { 1 } , \mathscr { s } _ { 2 } , \mathscr { . . . } , \mathscr { s } _ { r } ) ^ { \prime }$ is an $r$ - dimensional vector of spatial coordinates or a combination of space and time coordinates, were introduced in §1.7. The example given there, shown in Figure 1.15, was a collection of temperature measurements taking on a rectangular field. This data would form a two-dimensional process, indexed by row and column in space. In that section, the multidimensional autocovariance function of an $r$ -dimensional stationary series was given as $\gamma _ { x } ( h ) = E [ x _ { \pmb { s } + \pmb { h } ^ { x } } { \pmb { s } } ]$ , where the multidimensional lag vector is $\pmb { h } = ( h _ { 1 } , h _ { 2 } , \ldots , h _ { r } ) ^ { \prime }$ .

The multidimensional wavenumber spectrum is given as the Fourier transform of the autocovariance, namely,

$$
f _ {x} (\boldsymbol {\omega}) = \sum_ {\boldsymbol {h}} \gamma_ {x} (\boldsymbol {h}) \mathrm {e} ^ {- 2 \pi i \boldsymbol {\omega} ^ {\prime} \boldsymbol {h}}. \tag {4.150}
$$

Again, the inverse result

$$
\gamma_ {x} (\boldsymbol {h}) = \int_ {- 1 / 2} ^ {1 / 2} f _ {x} (\omega) e ^ {2 \pi i \boldsymbol {\omega} ^ {\prime} \boldsymbol {h}} d \omega \tag {4.151}
$$

holds, where the integral is over the multidimensional range of the vector $\omega$ . The wavenumber argument is exactly analogous to the frequency argument, and we have the corresponding intuitive interpretation as the cycling rate $\omega _ { i }$ per distance traveled $s _ { i }$ in the $i$ -th direction.

Two-dimensional processes occur often in practical applications, and the representations above reduce to

$$
f _ {x} \left(\omega_ {1}, \omega_ {2}\right) = \sum_ {h _ {1} = - \infty} ^ {\infty} \sum_ {h _ {2} = - \infty} ^ {\infty} \gamma_ {x} \left(h _ {1}, h _ {2}\right) \mathrm {e} ^ {- 2 \pi i \left(\omega_ {1} h _ {1} + \omega_ {2} h _ {2}\right)} \tag {4.152}
$$

and

$$
\gamma_ {x} \left(h _ {1}, h _ {2}\right) = \int_ {- 1 / 2} ^ {1 / 2} \int_ {- 1 / 2} ^ {1 / 2} f _ {x} \left(\omega_ {1}, \omega_ {2}\right) \mathrm {e} ^ {2 \pi i \left(\omega_ {1} h _ {1} + \omega_ {2} h _ {2}\right)} d \omega_ {1} d \omega_ {2} \tag {4.153}
$$

in the case $r \ = \ 2$ . The notion of linear filtering generalizes easily to the two-dimensional case by defining the impulse response function $a _ { s _ { 1 } , s _ { 2 } }$ and the spatial filter output as

$$
y _ {s _ {1}, s _ {2}} = \sum_ {u _ {1}} \sum_ {u _ {2}} a _ {u _ {1}, u _ {2}} x _ {s _ {1} - u _ {1}, s _ {2} - u _ {2}}. \tag {4.154}
$$

The spectrum of the output of this filter can be derived as

$$
f _ {y} \left(\omega_ {1}, \omega_ {2}\right) = \left| A \left(\omega_ {1}, \omega_ {2}\right) \right| ^ {2} f _ {x} \left(\omega_ {1}, \omega_ {2}\right), \tag {4.155}
$$

where

$$
A \left(\omega_ {1}, \omega_ {2}\right) = \sum_ {u _ {1}} \sum_ {u _ {2}} a _ {u _ {1}, u _ {2}} \mathrm {e} ^ {- 2 \pi i \left(\omega_ {1} u _ {1} + \omega_ {2} u _ {2}\right)}. \tag {4.156}
$$

These results are analogous to those in the one-dimensional case, described by Property P4.4.

The multidimensional DFT is also a straightforward generalization of the univariate expression. In the two-dimensional case with data on a rectangular grid, $\{ x _ { s _ { 1 } , s _ { 2 } }$ ; $s _ { 1 } = 1 , . . . , n _ { 1 } , \ s _ { 2 } = 1 , . . . , n _ { 2 } \}$ , we will write, for $- 1 / 2 \le \omega _ { 1 } , \omega _ { 2 } \le$ $1 / 2$ ,

$$
d \left(\omega_ {1}, \omega_ {2}\right) = \left(n _ {1} n _ {2}\right) ^ {- 1 / 2} \sum_ {s _ {1} = 1} ^ {n _ {1}} \sum_ {s _ {2} = 1} ^ {n _ {2}} x _ {s _ {1}, s _ {2}} \mathrm {e} ^ {- 2 \pi i \left(\omega_ {1} s _ {1} + \omega_ {2} s _ {2}\right)} \tag {4.157}
$$

as the two-dimensional DFT, where the frequencies $\omega _ { 1 } , \omega _ { 2 }$ are evaluated at multiples of $( 1 / n _ { 1 } , 1 / n _ { 2 } )$ on the spatial frequency scale. The two-dimensional wavenumber spectrum can be estimated by the smoothed sample wavenumber spectrum

$$
\bar {f} _ {x} \left(\omega_ {1}, \omega_ {2}\right) = \left(L _ {1} L _ {2}\right) ^ {- 1} \sum_ {\ell_ {1}, \ell_ {2}} \left| d \left(\omega_ {1} + \ell_ {1} / n _ {1}, \omega_ {2} + \ell_ {2} / n _ {2}\right) \right| ^ {2}, \tag {4.158}
$$

where the sum is taken over the grid $\{ - m _ { j } \ \leq \ \ell _ { j } \ \leq \ m _ { j }$ ; $j = 1 , 2 \}$ , where $L _ { 1 } = 2 m _ { 1 } + 1$ and $L _ { 2 } = 2 m _ { 2 } + 1$ . The statistic

$$
\frac {2 L _ {1} L _ {2} \bar {f} _ {x} (\omega_ {1} , \omega_ {2})}{f _ {x} (\omega_ {1} , \omega_ {2})} \dot {\sim} \chi_ {2 L _ {1} L _ {2}} ^ {2} \tag {4.159}
$$

can be used to set confidence intervals or make approximate tests against a fixed assumed spectrum $f _ { 0 } ( \omega _ { 1 } , \omega _ { 2 } )$ . We may also extend this analysis to weighted estimation and window estimation as discussed in §4.5.

# Example 4.25 Wavenumber Spectrum of Soil Surface Temperatures

As an example, consider the periodogram of the two-dimensional temperature series shown in Figure 1.15 and analyzed by Bazza et al. (1988). We recall the spatial coordinates in this case will be $( s _ { 1 } , s _ { 2 } )$ , which define the spatial coordinates rows and columns so that the frequencies in the two directions will be expressed as cycles per row and cycles per column. Figure 4.30 shows the periodogram of the two-dimensional temperature series, and we note the ridge of strong spectral peaks running over rows at a column frequency of zero. An obvious periodic component appears at frequencies of .0625 and −.0625 cycles per row, which corresponds to 16 rows or about 272 ft. On further investigation of previous irrigation patterns over this field, treatment levels of salt varied periodically over columns. This analysis is extended in Problem 4.17, where we recover the salt treatment profile over rows and compare it to a signal, computed by averaging over columns.

![](images/35e88c3e919bb63f0eb9577db9bab8ef0f5b0f5a7212a2760557125c57d5e36e.jpg)  
Figure 4.30 Two-dimensional periodogram of soil temperature profile showing peak at .0625 cycles/row. The period is 16 rows, and this corresponds to 16 $\times$ 17 ft = 272 ft.

Another application of two-dimensional spectral analysis of agricultural field trials is given in McBratney and Webster (1981), who used it to detect ridge and furrow patterns in yields. The requirement for regular, equally spaced samples on fairly large grids has tended to limit enthusiasm for strict two-dimensional spectral analysis. An exception is when a propagating signal from a given velocity and azimuth is present so predicting the wavenumber spectrum as a function of velocity and azimuth becomes feasible (see Shumway et al., 1999).

# Problems

# Section 4.2

4.1 Repeat the simulations and analyses in Examples 4.1 and 4.2 with the following changes:

(a) Change the sample size to $n = 1 2 8$ and generate and plot the same series as in Example 4.1:

$$
\begin{array}{l} x _ {t 1} = 2 \cos (2 \pi t 6 / 1 0 0) + 3 \sin (2 \pi t 6 / 1 0 0), \\ x _ {t 2} = 4 \cos (2 \pi t 1 0 / 1 0 0) + 5 \sin (2 \pi t 1 0 / 1 0 0), \\ \end{array}
$$

$$
\begin{array}{l} x _ {t 3} = 6 \cos (2 \pi t 4 0 / 1 0 0) + 7 \sin (2 \pi t 4 0 / 1 0 0), \\ x _ {t} = x _ {t 1} + x _ {t 2} + x _ {t 3}. \\ \end{array}
$$

What is the major difference between these series and the series generated in Example 4.1? (Hint: The answer is fundamental. But if your answer is the series are longer, you may be punished severely.)

(b) As in Example 4.2, compute and plot the periodogram of the series, $x _ { t }$ , generated in (a) and comment.   
(c) Repeat the analyses of (a) and (b) but with $n = 1 0 0$ (as in Example 4.1), and adding noise to $x _ { t }$ ; that is

$$
x _ {t} = x _ {t 1} + x _ {t 2} + x _ {t 3} + w _ {t}
$$

where $w _ { t } \sim$ iid $\mathrm { { N } ( 0 , 2 5 ) }$ . That is, you should simulate and plot the data, and then plot the periodogram of $x _ { t }$ and comment.

4.2 With reference to equations (4.2) and (4.3), let $Z _ { 1 } = U _ { 1 }$ and $Z _ { 2 } = - U _ { 2 }$ be independent, standard normal variables. Consider the polar coordinates of the point $( Z _ { 1 } , Z _ { 2 } )$ , that is,

$$
A ^ {2} = Z _ {1} ^ {2} + Z _ {2} ^ {2} \quad \text {a n d} \quad \phi = \tan^ {- 1} (Z _ {2} / Z _ {1}).
$$

(a) Find the joint density of $A ^ { 2 }$ and $\phi$ , and from the result, conclude that $A ^ { 2 }$ and $\phi$ are independent random variables, where $A ^ { 2 }$ is a chisquared random variable with 2 df, and $\phi$ is uniformly distributed on $( - \pi , \pi )$ .

(b) Going in reverse from polar coordinates to rectangular coordinates, suppose we assume that $A ^ { 2 }$ and $\phi$ are independent random variables, where $A ^ { 2 }$ is chi-squared with 2 df, and $\phi$ is uniformly distributed on $( - \pi , \pi )$ . With $Z _ { 1 } = A \cos ( \phi )$ and $Z _ { 2 } = A \sin ( \phi )$ , where $A$ is the positive square root of $A ^ { 2 }$ , show that $Z _ { 1 }$ and $Z _ { 2 }$ are independent, standard normal random variables.

# 4.3 Verify (4.5).

# Section 4.3

4.4 A time series was generated by first drawing the white noise series $w _ { t }$ from a normal distribution with mean zero and variance one. The observed series $x _ { t }$ was generated from

$$
x _ {t} = w _ {t} - \theta w _ {t - 1}, \quad t = 0, \pm 1, \pm 2, \dots ,
$$

where $\theta$ is a parameter.

(a) Derive the theoretical mean value and autocovariance functions for the series $x _ { t }$ and $w _ { t }$ . Are the series $x _ { t }$ and $w _ { t }$ stationary? Give your reasons.

(b) Give a formula for the power spectrum of $x _ { t }$ , expressed in terms of $\theta$ and $\omega$ .

4.5 A first-order autoregressive model is generated from the white noise series $w _ { t }$ using the generating equations

$$
x _ {t} = \phi x _ {t - 1} + w _ {t},
$$

where variab $\phi$ , for s wit $| \phi | < 1$ , is a parameter an zero and variance he . $w _ { t }$ are independent random $\sigma _ { w } ^ { 2 }$

(a) Show the power spectrum of $x _ { t }$ is given by

$$
f _ {x} (\omega) = \frac {\sigma_ {w} ^ {2}}{1 + \phi^ {2} - 2 \phi \cos (2 \pi \omega)}.
$$

(b) Verify the autocovariance function of this process is

$$
\gamma_ {x} (h) = \frac {\sigma_ {w} ^ {2} \phi^ {| h |}}{1 - \phi^ {2}},
$$

$h = 0 , \pm 1 , \pm 2 , . . .$ , by showing that the inverse transform of $\gamma _ { x } ( h )$ is the spectrum derived in part (a).

4.6 In applications, we will often observe series containing a signal that has been delayed by some unknown time $D$ , i.e.,

$$
x _ {t} = s _ {t} + A s _ {t - D} + n _ {t},
$$

where $s _ { t }$ and $n _ { t }$ are stationary and independent with zero means and spectral densities $f _ { s } ( \omega )$ and $f _ { n } ( \omega )$ , respectively. The delayed signal is multiplied by some unknown constant $A$ .

(a) Prove

$$
f _ {x} (\omega) = [ 1 + A ^ {2} + 2 A \cos (2 \pi \omega D) ] f _ {s} (\omega) + f _ {n} (\omega).
$$

(b) How could the periodicity expected in the spectrum derived in (a) be used to estimate the delay $D$ ? (Hint: Consider the case where $f _ { n } ( \omega ) = 0$ ; i.e., there is no noise.)

4.7 Suppose $x _ { t }$ and $y _ { t }$ are stationary zero-mean time series with $x _ { t }$ independent of $y _ { s }$ for all $s$ and $t$ . Consider the product series

$$
z _ {t} = x _ {t} y _ {t}.
$$

Prove the spectral density for $z _ { t }$ can be written as

$$
f _ {z} (\omega) = \int_ {- 1 / 2} ^ {1 / 2} f _ {x} (\omega - \nu) f _ {y} (\nu) d \nu .
$$

![](images/21a2fa57afe864f4a02e4fee17f96f04403280ec4e9b9bca67a5c3c86c720103.jpg)  
Figure 4.31 Smoothed 12-month sunspot numbers sampled twice per year, $n = 4 5 9$ .

# Section 4.4

4.8 Figure 4.31 shows the biyearly smoothed (12-month moving average) number of sunspots from June 1749 to December 1978 with $n = 4 5 9$ points that were taken twice per year. With Example 4.9 as a guide, perform a periodogram analysis of the sunspot data (the data are in the file sunspots.dat) identifying the predominant periods and obtaining confidence intervals for the identified periods. Interpret your findings.

4.9 The levels of salt concentration known to have occurred over rows, corresponding to the average temperature levels for the soil science data considered in Figures 1.15 and 1.16, are shown in Figure 4.32. The data are in the file salt.dat, which consists of one column of 128 observations; the first 64 observations correspond to the temperature series. Identify the dominant frequencies by performing separate spectral analyses on the two series. Include confidence intervals for the dominant frequencies and interpret your findings.

4.10 Let the observed series $x _ { t }$ be composed of a periodic signal and noise so it can be written as

$$
x _ {t} = \beta_ {1} \cos (2 \pi \omega_ {k} t) + \beta_ {2} \sin (2 \pi \omega_ {k} t) + w _ {t},
$$

where $w _ { t }$ is a white noise process with variance $\sigma _ { w } ^ { 2 }$ . The frequency $\omega _ { k }$ is assumed to be known and of the form $k / n$ in this problem. Suppose we consider estimating $\beta _ { 1 }$ , $\beta _ { 2 }$ and $\sigma _ { w } ^ { 2 }$ by least squares, or equivalently, by maximum likelihood if the $w _ { t }$ are assumed to be Gaussian.

(a) Prove, for a fixed $\omega _ { k }$ , the minimum squared error is attained by

$$
\left( \begin{array}{c} \widehat {\beta} _ {1} \\ \widehat {\beta} _ {2} \end{array} \right) = 2 n ^ {- 1 / 2} \left( \begin{array}{c} d _ {c} (\omega_ {k}) \\ d _ {s} (\omega_ {k}) \end{array} \right),
$$

![](images/00eb0a52e67a5317a86d80cafa6b54fcdf2faa84c21bd91667e0838586e78835.jpg)

![](images/7e9e83097b8feaa6a96dc395c50a92c58cda661a1d6f10da04f2134892d11523.jpg)  
Figure 4.32 Temperature and salt profiles over 64 rows at 17-ft spacing.

where the cosine and sine transforms (4.24) and (4.25) appear on the right-hand side.

(b) Prove that the error sum of squares can be written as

$$
S S E = \sum_ {t = 1} ^ {n} x _ {t} ^ {2} - 2 I _ {x} (\omega_ {k})
$$

so that the value of $\omega _ { k }$ that minimizes squared error is the same as the value that maximizes the periodogram $I _ { x } ( \omega _ { k } )$ estimator (4.21).

(c) Under the Gaussian assumption and fixed $\omega _ { k }$ , show that the $F$ -test of no regression leads to an $F$ -statistic that is a monotone function of $I _ { x } ( \omega _ { k } )$ .

4.11 Prove the convolution property of the DFT, namely,

$$
\sum_ {s = 1} ^ {n} a _ {s} x _ {t - s} = \sum_ {k = 0} ^ {n - 1} d _ {A} (\omega_ {k}) d _ {x} (\omega_ {k}) \exp \{2 \pi \omega_ {k} t \},
$$

for $t = 1 , 2 , \ldots , n$ , where $d _ { A } ( \omega _ { k } )$ and $d _ { x } ( \omega _ { k } )$ are the discrete Fourier transforms of $a _ { t }$ and $x _ { t }$ , respectively, and we assume that $x _ { t } = x _ { t + n }$ is periodic.

# Section 4.5

4.12 Repeat Problem 4.8 using a nonparametric spectral estimation procedure. In addition to discussing your findings in detail, comment on your choice of a spectral estimate with regard to smoothing and tapering.

4.13 Repeat Problem 4.9 using a nonparametric spectral estimation procedure. In addition to discussing your findings in detail, comment on your choice of a spectral estimate with regard to smoothing and tapering.

4.14 The periodic behavior of a time series induced by echoes can also be observed in the spectrum of the series; this fact can be seen from the results stated in Problem 4.6(a). Using the notation of that problem, suppose we observe $x _ { t } = s _ { t } + A s _ { t - D } + n _ { t }$ , which implies the spectra satisfy $f _ { x } ( \omega ) = [ 1 + A ^ { 2 } + 2 A \cos ( 2 \pi \omega D ) ] f _ { s } ( \omega ) + f _ { n } ( \omega )$ . If the noise is negligible ( $f _ { n } ( \omega ) \approx 0 )$ then $\log f _ { x } ( \omega )$ is approximately the sum of a periodic component, $\log [ 1 + A ^ { 2 } + 2 A \cos ( 2 \pi \omega D ) ]$ , and $\log f _ { s } ( \omega )$ . Bogart et al. (1962) proposed treating the detrended log spectrum as a pseudo time series and calculating its spectrum, or cepstrum, which should show a peak at a quefrency corresponding to $1 / D$ . The cepstrum can be plotted as a function of quefrency, from which the delaty $D$ can be estimated.

For the speech series presented in Example 1.3, estimate the pitch period using cepstral analysis as follows. The data are in the file speech.dat.

(a) Calculate and display the log-periodogram of the data. Is the periodogram periodic, as predicted?   
(b) Perform a cepstral (spectral) analysis on the detrended logged periodogram, and use the results to estimate the delay $D$ . How does your answer compare with the analysis of Example 1.24, which was based on the ACF?

4.15 Use Property P4.1 to verify (4.63). Then verify (4.66) and (4.67)

4.16 Consider two time series

$$
x _ {t} = w _ {t} - w _ {t - 1},
$$

$$
y _ {t} = \frac {1}{2} \left(w _ {t} + w _ {t - 1}\right),
$$

formed from the white noise series $w _ { t }$ with variance $\sigma _ { w } ^ { 2 } = 1$

(a) Are $x _ { t }$ and $y _ { t }$ jointly stationary? Recall the cross-covariance function must also be a function only of the lag $h$ and cannot depend on time.   
(b) Compute the spectra $f _ { y } ( \omega )$ and $f _ { x } ( \omega )$ , and comment on the difference between the two results.   
(c) Suppose sample spectral estimators $f _ { y } ( . 1 0 )$ are computed for the series using $L = 3$ . Find $a$ and $b$ such that

$$
P \left\{a \leq \bar {f} _ {y} (. 1 0) \leq b \right\} = . 9 0.
$$

This expression gives two points that will contain 90% of the sample spectral values. Put 5% of the area in each tail.

# Section 4.6

4.17 Analyze the coherency between the temperature and salt data discussed in Problem 4.9. Discuss your findings.

4.18 Consider two processes

$$
x _ {t} = w _ {t} \quad \mathrm {a n d} \quad y _ {t} = \phi x _ {t - D} + v _ {t}
$$

where $w _ { t }$ and $v _ { t }$ are independent white noise processes with common variance $\sigma ^ { 2 }$ , $\phi$ is a constant, and $D$ is a fixed integer delay.

(a) Compute the coherency between $x _ { t }$ and $y _ { t }$   
(b) Simulate $n = 1 0 2 4$ normal observations from $x _ { t }$ and $y _ { t }$ for $\phi = . 9$ $\sigma ^ { 2 } = 1$ , and $D = 0$ . Then estimate and plot the coherency between the simulated series for the following values of $L$ and comment:

(i) $L = 1$ , (ii) $L = 3$ , (iii) $L = 4 1$ , and (iv) $L = 1 0 1$

# Section 4.7

4.19 For the processes in Problem 4.18,

(a) Compute the phase between $x _ { t }$ and $y _ { t }$   
(b) Simulate $n = 1 0 2 4$ observations from $x _ { t }$ and $y _ { t }$ for $\phi = . 9$ , $\sigma ^ { 2 } = 1$ and $D = 1$ . Then estimate and plot the phase between the simulated series for the following values of $L$ and comment:

(i) $L = 1$ , (ii) $L = 3$ , (iii) $L = 4 1$ , and (iv) $L = 1 0 1$

4.20 Consider the bivariate time series records containing monthly U.S. production as measured monthly by the Federal Reserve Board Production Index and unemployment as given in Figure 3.22.

(a) Compute the spectrum and the log spectrum for each series, and identify statistically significant peaks. Explain what might be generating the peaks. Compute the coherence, and explain what is meant when a high coherence is observed at a particular frequency.   
(b) What would be the effect of applying the filter

$$
u _ {t} = x _ {t} - x _ {t - 1}
$$

followed by

$$
v _ {t} = u _ {t} - u _ {t - 1 2}
$$

to the series given above? Plot the predicted frequency responses of the simple difference filter and of the seasonal difference of the first difference.

(c) Apply the filters successively to one of the two series and plot the output. Examine the output after taking a first difference and comment on whether stationarity is a reasonable assumption. Why or why not? Plot after taking the seasonal difference of the first difference. What can be noticed about the output that is consistent with what you have predicted from the frequency response? Verify by computing the spectrum of the output after filtering.

4.21 Determine the theoretical power spectrum of the series formed by combining the white noise series $w _ { t }$ to form

$$
y _ {t} = w _ {t - 2} + 4 w _ {t - 1} + 6 w _ {t} + 4 w _ {t + 1} + w _ {t + 2}.
$$

Determine which frequencies are present by plotting the power spectrum.

4.22 Let $x _ { t } = \cos ( 2 \pi \omega t )$ , and consider the output

$$
y _ {t} = \sum_ {k = - \infty} ^ {\infty} a _ {k} x _ {t - k},
$$

where $\textstyle \sum _ { k } \left| a _ { k } \right| < \infty$ . Show

$$
y _ {t} = | A (\omega) | \cos (2 \pi \omega t + \phi (\omega)),
$$

where $| A ( \omega ) |$ and $\phi ( \omega )$ are the amplitude and phase of the filter, respectively. Interpret the result in terms of the relationship between the input series, $x _ { t }$ , and the output series, $y _ { t }$ .

4.23 Suppose $x _ { t }$ is a stationary series, and we apply two filtering operations in succession, say,

$$
y _ {t} = \sum_ {r} a _ {r} x _ {t - r},
$$

and then

$$
z _ {t} = \sum_ {s} b _ {s} y _ {t - s}.
$$

(a) Show the spectrum of the output is

$$
f _ {z} (\omega) = | A (\omega) | ^ {2} | B (\omega) | ^ {2} f _ {x} (\omega),
$$

where $A ( \omega )$ and $B ( \omega )$ are the Fourier transforms of the filter sequences $a _ { t }$ and $b _ { t }$ , respectively.

(b) What would be the effect of applying the filter

$$
u _ {t} = x _ {t} - x _ {t - 1}
$$

followed by

$$
v _ {t} = u _ {t} - u _ {t - 1 2}
$$

to a time series?

(c) Plot the predicted frequency responses of the simple difference filter and of the seasonal difference of the first difference. Filters like these are called seasonal adjustment filters in economics because they tend to attenuate frequencies at multiples of the monthly periods. The difference filter tends to attenuate low-frequency trends.

4.24 Suppose we are given a stationary zero-mean series $x _ { t }$ with spectrum $f _ { x } ( \omega )$ and then construct the derived series

$$
y _ {t} = a y _ {t - 1} + x _ {t}, \quad t = \pm 1, \pm 2, \dots .
$$

(a) Show how the theoretical $f _ { y } ( \omega )$ is related to $f _ { x } ( \omega )$ .

(b) Plot the function that multiplies $f _ { x } ( \omega )$ in part (a) for $a = . 1$ and for $a = . 8$ . This filter is called a recursive filter.

# Section 4.8

4.25 Often, the periodicities in the sunspot series are investigated by fitting an autoregressive spectrum of sufficiently high order. The main periodicity is often stated to be in the neighborhood of 11 years. Fit an autoregressive spectral estimator to the sunspot data using a model selection method of your choice. Compare the result with a conventional nonparametric spectral estimator found in Problem 4.8.

4.26 Fit an autoregressive spectral estimator to the Recruitment series and compare it to the results of Example 4.11.

4.27 Suppose a sample time series with $n = 2 5 6$ points is available from the first-order autoregressive model. Furthermore, suppose a sample spectrum computed with $L = 3$ yields the estimated value $f _ { x } ( 1 / 8 ) = 2 . 2 5$ . Is this sample value consistent with $\sigma _ { w } ^ { 2 } = 1 , \phi = . 5 ?$ Repeat using $L = 1 1$ if we just happen to obtain the same sample value.

4.28 Suppose we wish to test the noise alone hypothesis $H _ { 0 } : x _ { t } = n _ { t }$ against the signal-plus-noise hypothesis $H _ { 1 } : x _ { t } = s _ { t } + n _ { t }$ , where $s _ { t }$ and $n _ { t }$ are uncorrelated zero-mean stationary processes with spectra $f _ { s } ( \omega )$ and $f _ { n } ( \omega )$ . Suppose that we want the test over a band of $L = 2 m + 1$ frequencies of the form $\omega _ { j : n } + k / n$ , for $k = 0 , \pm 1 , \pm 2 , \ldots , \pm m$ near some fixed frequency $\omega$ . Assume that both the signal and noise spectra are approximately constant over the interval.

(a) Prove the approximate likelihood-based test statistic for testing $H _ { 0 }$ against $H _ { 1 }$ is proportional to

$$
T = \sum_ {k} | d _ {x} (\omega_ {j: n} + k / n) | ^ {2} \left(\frac {1}{f _ {n} (\omega)} - \frac {1}{f _ {s} (\omega) + f _ {n} (\omega)}\right).
$$

(b) Find the approximate distributions of $T$ under $H _ { 0 }$ and $H _ { 1 }$   
(c) Define the false alarm and signal detection probabilities as $P _ { F } =$ $P \{ T > K | H _ { 0 } \}$ and $P _ { d } = P \{ T > k | H _ { 1 } \}$ , respectively. Express these probabilities in terms of the signal-to-noise ratio $f _ { s } ( \omega ) / f _ { n } ( \omega )$ and appropriate chi-squared integrals.

# Section 4.9

4.29 Repeat the dynamic Fourier analysis of Example 4.20 on the remaining seven earthquakes and seven explosions in the data file eq+exp.dat. Do the conclusions about the difference between earthquakes and explosions stated in the example still seem valid?   
4.30 Repeat the wavelet analyses of Examples 4.21 and 4.22 on all earthquake and explosion series in the data file eq+exp.dat. Do the conclusions about the difference between earthquakes and explosions stated in Examples 4.21 and 4.22 still seem valid?   
4.31 Using Examples 4.20-4.22 as a guide, perform a dynamic Fourier analysis and wavelet analyses (dwt and waveshrink analysis) on the event of unknown origin that took place near the Russian nuclear test facility in Novaya Zemlya. State your conclusion about the nature of the event at Novaya Zemlya.

# Section 4.10

4.32 Consider the problem of approximating the filter output

$$
y _ {t} = \sum_ {k = - \infty} ^ {\infty} a _ {k} x _ {t - k}, \quad \sum_ {- \infty} ^ {\infty} | a _ {k} | <   \infty ,
$$

by

$$
y _ {t} ^ {M} = \sum_ {| k | <   M / 2} a _ {k} ^ {M} x _ {t - k}
$$

for $t = M / 2 - 1 , M / 2 , \dots , n - M / 2$ , where $x _ { t }$ is available for $t = 1 , \ldots , n$ and

$$
a _ {t} ^ {M} = M ^ {- 1} \sum_ {k = 0} ^ {M - 1} A (\omega_ {k}) \exp \{2 \pi i \omega_ {k} t \}
$$

with $\omega _ { k } = k / M$ . Prove

$$
E \left\{\left(y _ {t} - y _ {t} ^ {M}\right) ^ {2} \right\} \leq 4 \gamma_ {x} (0) \left(\sum_ {| k | \geq M / 2} \left| a _ {k} \right|\right) ^ {2}.
$$

![](images/5fd72da36d5c2b111cf5fa70489d38f55a77ef10cae89f28b8c24f723e3f75e5.jpg)

![](images/1fca6a41810247e67fc6f2503e779f47d4e1ae441181958d0325c0565900ed18.jpg)

![](images/be721e5b3a94343c40e4f58224bea7b1f50a28dca1f5adaa29ec693a89f7c5d3.jpg)

![](images/e8b7f2f6ad8b7bc993e6734c47a2a3a958697ef8b2165a41270575200cf460fa.jpg)

![](images/cc9a620284e520eb904ab6d8fe845ea4a424e58fd287f27652f2e9128fe7e159.jpg)

![](images/c0fb6acd3dbfe91d4dc88b2fda4b9637151695bd0022bea804499ef048ff0539.jpg)  
Figure 4.33 Monthly values of weather and inflow at Shasta Lake

4.33 Prove the squared coherence $\rho _ { y \cdot x } ^ { 2 } ( \omega ) = 1$ for all $\omega$ when

$$
y _ {t} = \sum_ {r = - \infty} ^ {\infty} a _ {r} x _ {t - r},
$$

that is, when $x _ { t }$ and $y _ { t }$ can be related exactly by a linear filter.

4.34 Figure 4.33 contains 454 months of measured values for the climatic variables air temperature, dew point, cloud cover, wind speed, precipitation, and inflow at Shasta Lake in California. We would like to look at possible relations among the weather factors and between the weather factors and the inflow to Shasta Lake.

(a) Argue the strongest determinant of the inflow series is precipitation using the coherence functions. Use transformed inflow $I _ { t } = \log i _ { t }$ , where $i _ { t }$ is inflow, and transformed precipitation $P _ { t } = \sqrt { p } _ { t }$ , where

$p _ { t }$ is precipitation. It should be mentioned here that Chapter 6 discusses methods for determining whether inflow might depend jointly on several input series.

(b) Using the estimated impulse response function, argue for the model

$$
I _ {t} = \alpha_ {0} + \frac {\alpha_ {1}}{1 - \phi B} P _ {t},
$$

where the notation is as discussed in Chapter 2. What would be a reasonable value for $\phi$ ? Assume the means are taken out of the series before the analysis begins.

# Section 4.11

4.35 Consider the signal plus noise model

$$
y _ {t} = \sum_ {r = - \infty} ^ {\infty} \beta_ {r} x _ {t - r} + v _ {t},
$$

where the signal and noise series, $x _ { t }$ and $v _ { t }$ are both stationary with spectra $f _ { x } ( \omega )$ and $f _ { v } ( \omega )$ , respectively. Assuming that $x _ { t }$ and $v _ { t }$ are independent of each other for all $t$ , verify (4.142) and (4.143).

4.36 Consider the model

$$
y _ {t} = x _ {t} + v _ {t},
$$

where

$$
x _ {t} = \phi_ {1} x _ {t - 1} + w _ {t},
$$

such that $v _ { t }$ is Gaussian white noise and independent of $x _ { t }$ with $\mathrm { v a r } ( v _ { t } ) =$ σ v , $\sigma _ { v } ^ { 2 }$ 2 and $w _ { t }$ is Gaussian white noise and independent of $v _ { t }$ , with $\mathrm { v a r } ( w _ { t } ) =$ σ w , $\sigma _ { w } ^ { 2 }$ 2 and $| \phi _ { 1 } | < 1$ and $E x _ { 0 } = 0$ . Prove that the spectrum of the observed series $y _ { t }$ is

$$
f _ {y} (\omega) = \frac {\sigma^ {2} | 1 - \theta_ {1} e ^ {- 2 \pi i \omega} | ^ {2}}{| 1 - \phi_ {1} e ^ {- 2 \pi i \omega} | ^ {2}},
$$

where

$$
\begin{array}{l} \theta_ {1} = \frac {c \pm \sqrt {c ^ {2} - 4}}{2}, \\ \sigma^ {2} = \frac {\sigma_ {v} ^ {2} \phi_ {1}}{\theta_ {1}}, \\ \end{array}
$$

and

$$
c = \frac {\sigma_ {w} ^ {2} + \sigma_ {v} ^ {2} (1 + \phi_ {1} ^ {2})}{\sigma_ {v} ^ {2} \phi_ {1}}.
$$

4.37 Consider the same model as in the preceding problem.

(a) Prove the optimal smoothed estimator of the form

$$
\widehat {x} _ {t} = \sum_ {s = - \infty} ^ {\infty} a _ {s} y _ {t - s}
$$

has

$$
a _ {s} = \frac {\sigma_ {w} ^ {2}}{\sigma^ {2}} \frac {\theta_ {1} ^ {| s |}}{1 - \theta_ {1} ^ {2}}.
$$

(b) Show the mean square error is given by

$$
E \left\{\left(x _ {t} - \widehat {x} _ {t}\right) ^ {2} \right\} = \frac {\sigma_ {v} ^ {2} \sigma_ {w} ^ {2}}{\sigma^ {2} \left(1 - \theta_ {1} ^ {2}\right)}.
$$

(c) Compare mean square error of the estimator in part (b) with that of the optimal finite estimator of the form

$$
\widehat {x} _ {t} = a _ {1} y _ {t - 1} + a _ {2} y _ {t - 2}
$$

when $\sigma _ { v } ^ { 2 } = . 0 5 3 , \sigma _ { w } ^ { 2 } = . 1 7 2$ , and $\phi _ { 1 } = . 9$

# Section 4.12

4.38 Consider the two-dimensional linear filter given as the output (4.154).

(a) Express the two-dimensional autocovariance function of the output, say, $\gamma _ { y } ( h _ { 1 } , h _ { 2 } )$ , in terms of an infinite sum involving the autocovariance function of $x s$ and the filter coefficients ${ a _ { s } } _ { 1 } , { s } _ { 2 }$ .   
(b) Use the expression derived in (a), combined with (4.153) and (4.156) to derive the spectrum of the filtered output (4.155).

The following problems require the supplemental material given in Appendix $C$

4.39 Let $w _ { t }$ be a Gaussian white noise series with variance $\sigma _ { w } ^ { 2 }$ . Prove that the results of Theorem C.4 hold without error for the DFT of $w _ { t }$ .   
4.40 Show that condition (4.41) implies (C.19) under the assumption that $w _ { t } \sim w n ( 0 , \sigma _ { w } ^ { 2 } )$ .   
4.41 Prove Lemma C.4.   
4.42 Finish the proof of Theorem C.5.   
4.43 For the zero-mean complex random vector $\pmb { z } = \pmb { x } _ { c } - i \pmb { x } _ { s }$ , with $\operatorname { c o v } ( z ) =$ $\Sigma = C - i Q$ , with $\Sigma = \Sigma ^ { * }$ , define

$$
w = 2 \operatorname {R e} \left(\boldsymbol {a} ^ {*} \boldsymbol {z}\right),
$$

where $\pmb { a } = \pmb { a } _ { c } - i \pmb { a } _ { s }$ is an arbitrary non-zero complex vector. Prove

$$
\operatorname {c o v} (w) = 2 \boldsymbol {a} ^ {*} \Sigma \boldsymbol {a}.
$$

Recall ∗ denotes the complex conjugate transpose.

# Chapter 5

# Additional Time Domain Topics

# 5.1 Introduction

In this chapter, we present material that may be considered special or advanced topics in the time domain. Chapter 6 is devoted to one of the most useful and interesting time domain topics, state-space models. So, we do not cover statespace models or related topics—of which there are many—in this chapter. This chapter consists of sections of independent topics that may be read in any order. Most of the sections depend on a basic knowledge of ARMA models, forecasting and estimation, which is the material that is covered in Chapter 3, 3.1- 3.8. A few sections, for example the first section on long memory models, require some knowledge of spectral analysis and related topics covered in Chapter 4. In addition to long memory, we discuss GARCH models, threshold models, regression with autocorrelated errors, lagged regression or transfer functions, and selected topics in multivariate ARMAX models.

# 5.2 Long Memory ARMA and Fractional Differencing

The conventional ARMA $( p , q )$ process is often referred to as a short memory process because the coefficients in the representation

$$
x _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j},
$$

obtained by solving

$$
\phi (z) \psi (z) = \theta (z),
$$

are dominated by exponential decay. As pointed out in 3.3, this result implies the ACF of the short memory process $\rho ( h ) \to 0$ exponentially fast as $h \to \infty$ . When the sample ACF of a time series decays slowly, the advice given in Chapter 3 has been to difference the series until it seems stationary. Following this advice with the glacial varve series first presented in Example 3.31 leads to the first difference of the logarithms of the data being represented as a firstorder moving average. In Example 3.37, further analysis of the residuals leads to fitting an ARIMA $( 1 , 1 , 1 )$ model,

$$
\nabla x _ {t} = \phi \nabla x _ {t - 1} + w _ {t} + \theta w _ {t - 1},
$$

where we understand $x _ { t }$ is the log-transformed varve series. In particular, the estimates of the parameters (and the standard errors) were $\widehat { \phi } = . 2 3 ( . 0 5 )$ , $\widehat { \theta } = - . 8 9 ( . 0 3 )$ , and $\widehat { \sigma } _ { w } ^ { 2 } = . 2 3$ . The use of the first difference $\nabla x _ { t } = ( 1 - B ) x _ { t }$ can be too severe a modification in the sense that the nonstationary model might represent an overdifferencing of the original process.

Long memory (or persistent) time series were considered in Hosking (1981) and Granger and Joyeux (1980) as intermediate compromises between the short memory ARMA type models and the fully integrated nonstationary processes in the Box–Jenkins class. The easiest way to generate a long memory series is to think of using the difference operator $( 1 - B ) ^ { d }$ for fractional values of $d$ , say, $0 < d < . 5$ , so a basic long memory series gets generated as

$$
(1 - B) ^ {d} x _ {t} = w _ {t}, \tag {5.1}
$$

where $w _ { t }$ still denotes white noise with variance $\sigma _ { w } ^ { 2 }$ . Now, $d$ becomes a parameter to be estimated along with $\sigma _ { w } ^ { 2 }$ . Differencing the original process, as in the Box–Jenkins approach, may be thought of as simply assigning a value of $d = 1$ . This idea has been extended to the class of fractionally integrated ARMA, or ARFIMA models, where we allow $- . 5 < d < . 5$ ; when $d$ is negative, the term antipersistent is used. Long memory processes occur in hydrology (see Hurst, 1951, and McLeod and Hipel, 1978) and in environmental series, such as the varve data we have previously analyzed, to mention a few examples. Long memory time series data tend to exhibit sample autocorrelations that are not necessarily large (as in the case of $d = 1$ ), but persist for a long time. Figure 5.1 shows the sample ACF, to lag 100, of the log-transformed varve series, which exhibits classic long memory behavior.

The fractionally differenced series (5.1), for $| d | < . 5$ , is often called fractional noise. To investigate its properties, we can use the binomial expansion ( $d > - . 5 )$ to write

$$
w _ {t} = (1 - B) ^ {d} x _ {t} = \sum_ {j = 0} ^ {\infty} \pi_ {j} B ^ {j} x _ {t} = \sum_ {j = 0} ^ {\infty} \pi_ {j} x _ {t - j} \tag {5.2}
$$

![](images/a4724d9c6efa0a87f5674a0cc06bf0c059ab7d7ffb7c9c9c9c7040a4e8c41cd6.jpg)  
Figure 5.1 Sample ACF of the log transformed varve series.

where

$$
\pi_ {j} = \frac {\Gamma (j - d)}{\Gamma (j + 1) \Gamma (- d)} \tag {5.3}
$$

with $\Gamma ( x + 1 ) = x \Gamma ( x )$ being the gamma function. Similarly ( $d < . 5$ ), we can write

$$
x _ {t} = (1 - B) ^ {- d} w _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} B ^ {j} w _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j} \tag {5.4}
$$

where

$$
\psi_ {j} = \frac {\Gamma (j + d)}{\Gamma (j + 1) \Gamma (d)}. \tag {5.5}
$$

The processes (5.2) and (5.4) are well-defined stationary processes (see Brockwell and Davis, 1991, for details). In the case of fractional differencing, however, the coefficients satisfy $\textstyle \sum \pi _ { j } ^ { 2 } < \infty$ and $\sum \psi _ { j } ^ { 2 } < \infty$ as opposed to the absolute summability of the coefficients in ARMA processes.

Using the representation (5.4)–(5.5), the ACF of $x _ { t }$ is seen to be

$$
\rho (h) = \frac {\Gamma (h + d) \Gamma (1 - d)}{\Gamma (h - d + 1) \Gamma (d)} \sim h ^ {2 d - 1} \tag {5.6}
$$

for large $h$ . From this we see that for $0 < d < . 5$

$$
\sum_ {h = - \infty} ^ {\infty} | \rho (h) | = \infty
$$

and hence the term long memory.

In order to examine a series such as the varve series for a possible long memory pattern, it is convenient to look at ways of estimating $d$ . Using (5.3) it is easy to derive the recursions

$$
\pi_ {j + 1} (d) = \frac {(j - d) \pi_ {j} (d)}{(j + 1)}, \tag {5.7}
$$

for $j = 0 , 1 , \ldots$ , with $\pi _ { 0 } ( d ) = 1$ . Maximizing the joint likelihood of the errors under normality, say, $w _ { t } ( d )$ , will involve minimizing the sum of squared errors

$$
Q (d) = \sum w _ {t} ^ {2} (d).
$$

The usual Gauss–Newton method, described in §3.6, leads to the expansion

$$
w _ {t} (d) = w _ {t} \left(d _ {0}\right) + w _ {t} ^ {\prime} \left(d _ {0}\right) \left(d - d _ {0}\right),
$$

where

$$
w _ {t} ^ {\prime} (d _ {0}) = \left. \frac {\partial w _ {t}}{\partial d} \right| _ {d = d _ {0}}
$$

and $d _ { 0 }$ is an initial estimate (guess) at to the value of $d$ . Setting up the usual regression leads to

$$
d = d _ {0} - \frac {\sum_ {t} w _ {t} ^ {\prime} \left(d _ {0}\right) w _ {t} \left(d _ {0}\right)}{\sum_ {t} w _ {t} ^ {\prime} \left(d _ {0}\right) ^ {2}}. \tag {5.8}
$$

The derivatives are computed recursively by differentiating (5.7) successively with respect to $d$ : $\pi _ { j + 1 } ^ { \prime } ( d ) = [ ( j - d ) \pi _ { j } ^ { \prime } ( d ) - \pi _ { j } ( d ) ] / ( j + 1 )$ , where $\pi _ { 0 } ^ { \prime } ( d ) = 0$ . The errors are computed from an approximation to (5.2), namely,

$$
w _ {t} (d) = \sum_ {j = 0} ^ {t} \pi_ {j} (d) x _ {t - j}. \tag {5.9}
$$

It is advisable to omit a number of initial terms from the computation and start the sum, (5.8), at some fairly large value of $t$ to have a reasonable approximation.

# Example 5.1 Long Memory Fitting of the Glacial Varve Series

We consider analyzing the glacial varve series discussed in Examples 2.5 and 3.31. Figure 2.6 shows the original and log-transformed series (which we denote by $x _ { t }$ ). In Example 3.37, we noted that $x _ { t }$ could be modeled as an ARMA $( 1 , 1 , 1 )$ process. We fit the fractionally differenced model, (5.1), to the mean-adjusted series, $x _ { t } - \bar { x }$ . Applying the Gauss–Newton iterative procedure previously described, starting with $d = . 1$ and omitting the first 30 points from the computation, leads to a final value of $d = . 3 8 4$ , which implies the set of coefficients $\pi _ { j } ( . 3 8 4 )$ , as given in Figure 5.2 with $\pi _ { 0 } ( . 3 8 4 ) = 1$ . We can compare roughly the performance

![](images/87928b248d458091b771a925a9dc7d51005618b21922bacf1146fb7bd053553f.jpg)  
Figure 5.2 Coefficients $\pi _ { j } ( . 3 8 4 ) , j = 1 , 2 , . . . , 3 0$ in the representation (5.7).

of the fractional difference operator with the ARIMA model by examining the autocorrelation functions of the two residual series as shown in Figure 5.3. The ACFs of the two residual series are roughly comparable with the white noise model.

To perform this analysis in R, first download and install the fracdiff package from CRAN. Then, load the package and issue the following commands (assuming the data are in varve).

```txt
> lvarve = log(varve) - mean(log(varve))
> varve.fd = fracdiff(lvarve, nar=0, nma=0, M=30)
> varve.fd$d
[1] 0.3841688
> varve.fd$stderr.dpq
[1] 4.589514e-06 
```

The R package uses a truncated maximum likelihood procedure that was discussed in Haslett and Raftery (1989), which is a little more elaborate than simply zeroing out initial values. The default truncation value in R is $M = 1 0 0$ . In the default case, the estimate is $\widehat { d } = . 3 7$ with approximately the same standard error. The standard error is obtained from the Hessian as described in Example 3.28. At this time the R package fracdiff does not supply the residuals for diagnostics or an estimate of $\sigma _ { w } ^ { 2 }$ , hence some additional programming would be necessary for a full analysis.

![](images/0e9239abcfb707397bc3f917127b9760ecf69ffd6a1622139f5e045f4562a13d.jpg)

![](images/784a3b673b07f8ba03b95534a20ac443a6dff85150657ef59e9859f854e8850e.jpg)  
Figure 5.3 ACF of residuals from the ARIMA(1, 1, 1) fit to the varve series (top) and of the residuals from the long memory model fit, $( 1 - B ) ^ { d } x _ { t } = w _ { t }$ , with $d = . 3 8 4$ (bottom).

Forecasting long memory processes is similar to forecasting ARIMA models. That is, (5.2) and (5.7) can be used to obtain the truncated forecasts

$$
\widetilde {x} _ {n + m} ^ {n} = \sum_ {j = 1} ^ {n} \pi_ {j} (\widehat {d}) \widetilde {x} _ {n + m - j} ^ {n}, \tag {5.10}
$$

for $m = 1 , 2 , \ldots$ . Error bounds can be approximated by using

$$
P _ {n + m} ^ {n} = \hat {\sigma} _ {w} ^ {2} \left(\sum_ {j = 0} ^ {m - 1} \psi_ {j} ^ {2} (\widehat {d})\right) \tag {5.11}
$$

where, as in (5.7),

$$
\psi_ {j} (\widehat {d}) = \frac {(j + \widehat {d}) \psi_ {j} (\widehat {d})}{(j + 1)}, \tag {5.12}
$$

with $\psi _ { 0 } ( \widehat { d } ) = 1$

No obvious short memory ARMA-type component can be seen in the ACF of the residuals from the fractionally differenced varve series shown in Figure 5.3. It is natural, however, that cases will exist in which substantial short memory-type components will also be present in data that exhibits long memory. Hence, it is natural to define the general ARFIMA $( p , d , q )$ , $- . 5 < d < . 5$ process as

$$
\phi (B) \nabla^ {d} \left(x _ {t} - \mu\right) = \theta (B) w _ {t}, \tag {5.13}
$$

where $\phi ( B )$ and $\theta ( B )$ are as given in Chapter 3. Writing the model in the form

$$
\phi (B) \pi_ {d} (B) \left(x _ {t} - \mu\right) = \theta (B) w _ {t} \tag {5.14}
$$

makes it clear how we go about estimating the parameters for the more general model. Forecasting for the ARFIMA $( p , d , q )$ series can be easily done, noting that we may equate coefficients in

$$
\phi (z) \psi (z) = (1 - z) ^ {- d} \theta (z) \tag {5.15}
$$

and

$$
\theta (z) \pi (z) = (1 - z) ^ {d} \phi (z) \tag {5.16}
$$

to obtain the representations

$$
x _ {t} = \mu + \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j}
$$

and

$$
w _ {t} = \sum_ {j = 0} ^ {\infty} \pi_ {j} (x _ {t - j} - \mu).
$$

We then can proceed as discussed in (5.10) and (5.11).

A comprehensive treatment of long memory models is given in Beran (1994), and it should be noted that several other techniques for estimating the parameters, especially, the long memory parameter, can be developed in the frequency domain. In this case, we may think of the equations as generated by an infinite order autoregressive series with coefficients $\pi _ { j }$ given by (5.7) . Using the same approach as before, we obtain

$$
\begin{array}{l} f _ {x} (\omega) = \frac {\sigma_ {w} ^ {2}}{\left| \sum_ {k = 0} ^ {\infty} \pi_ {k} e ^ {- 2 \pi i k \omega} \right| ^ {2}} (5.17) \\ = \sigma_ {w} ^ {2} | 1 - e ^ {- 2 \pi i \omega} | ^ {- 2 d} (5.18) \\ = [ 4 \sin^ {2} (\pi \omega) ] ^ {- d} \sigma_ {w} ^ {2} (5.19) \\ \end{array}
$$

as equivalent representations of the spectrum of a long memory process. The long memory spectrum approaches infinity as the frequency $\omega  0$ .

The main reason for defining the Whittle approximation to the log likelihood is to propose its use for estimating the parameter $d$ in the long memory case as an alternative to the time domain method previously mentioned. The time domain approach is useful because of its simplicity and easily computed standard errors. One may also use an exact likelihood approach by developing an innovations form of the likelihood as in Brockwell and Davis (1991).

For the approximate approach using the Whittle likelihood (4.116), we consider using the approach of Fox and Taqqu (1986) who showed that maximizing

the Whittle log likelihood leads to a consistent estimator with the usual asymptotic normal distribution that would be obtained by treating (4.116) as a conventional log likelihood (see also Dahlhaus, 1989; Robinson, 1995; Hurvich et al., 1998). Unfortunately, the periodogram ordinates are not asymptotically independent (Hurvich and Beltrao, 1993), although a quasi-likelihood in the form of the Whittle approximation works well and has good asymptotic properties.

To see how this would work for the purely long memory case, write the long memory spectrum as

$$
f _ {x} \left(\omega_ {k}; d, \sigma_ {w} ^ {2}\right) = \sigma_ {w} ^ {2} g _ {k} ^ {- d}, \tag {5.20}
$$

where

$$
g _ {k} = 4 \sin^ {2} \left(\pi \omega_ {k}\right). \tag {5.21}
$$

Then, differentiating the log likelihood, say,

$$
\ln L (\boldsymbol {x}; d, \sigma_ {w} ^ {2}) \approx - m \ln \sigma_ {w} ^ {2} + d \sum_ {k = 1} ^ {m} \ln g _ {k} - \frac {1}{\sigma_ {w} ^ {2}} \sum_ {k = 1} ^ {m} g _ {k} ^ {d} I (\omega_ {k}) \tag {5.22}
$$

at $m = n / 2 - 1$ frequencies and solving for $\sigma _ { w } ^ { 2 }$ yields

$$
\sigma_ {w} ^ {2} (d) = \frac {1}{m} \sum_ {k = 1} ^ {m} g _ {k} ^ {d} I \left(\omega_ {k}\right) \tag {5.23}
$$

as the approximate maximum likelihood estimator for the variance parameter. To estimate $d$ , we use a grid scan of the concentrated log likelihood

$$
\ln L (\boldsymbol {x}; d) \approx - m \ln \sigma_ {w} ^ {2} (d) - d \sum_ {k = 1} ^ {m} \ln g _ {k} - m \tag {5.24}
$$

over the interval $( - . 5 , . 5 )$ , followed by a Newton–Raphson procedure to convergence.

# Example 5.2 Long Memory Spectra for the Varve Series

We have previously examined the fit of the long memory model for the glacial varve data that is thought to be a reasonable surrogate for temperature. Fitting the long memory model using the Whittle approximation above gives $\widehat { d } = . 3 9 4$ , with an estimated standard error of .022. The earlier time domain method gave $\widehat { d } = . 3 8 4$ , with a standard error of $4 . 6 \times 1 0 ^ { - 6 }$ , so the results of the two methods are different. The error variance estimated was $\widehat { \sigma } _ { w } ^ { 2 } = . 2 3 2 0$ . One might also consider fitting an autoregressive model to this data using a procedure similar to that used in Example 4.19. Following this approach gave an autoregressive model with $p = 8$ and $\boldsymbol { \hat { \phi } } = ( . 3 4 , . 1 1 , . 0 3 , . 0 9 , . 0 9 , . 0 8 , . 0 2 , . 0 9 ) ^ { \prime }$ , with $\widehat { \sigma } _ { w } ^ { 2 } = . 2 3 0 3$

![](images/cb10bb82d93c6d49dc26e453c13a33b0ccb4b086d065f89cc454b4d8f7ef8cb8.jpg)  
Figure 5.4 Long Memory ( $d = . 3 9 4$ ) and autoregressive AR(8) spectral estimators for the paleoclimatic glacial varve series.

as the error variance. The two log spectra are plotted in Figure 5.4 for $\omega > 0$ , and we note that long memory spectrum is lower for the first frequency estimated ( $\omega _ { 1 } = 1 / 5 1 2$ ) but will eventually become infinite, whereas the AR(8) spectrum is higher at that point, but takes a finite value at $\omega = 0$ .

It should be noted that there is a strong likelihood that the spectrum will not be purely long memory, as it seemed to be in the example given above. A common situation has the long memory component multiplied by a short memory component, leading to an alternate version of (5.20) of the form

$$
f _ {x} \left(\omega_ {k}; d, \theta\right) = g _ {k} ^ {- d} f _ {0} \left(\omega_ {k}; \boldsymbol {\theta}\right), \tag {5.25}
$$

where $f _ { 0 } ( \omega _ { k } ; \theta )$ might be the spectrum of an autoregressive moving average process with vector parameter $\pmb \theta$ , or it might be unspecified. If the spectrum has a parametric form, the Whittle likelihood can be used. However, there is a substantial amount of semiparametric literature that develops the estimators when the underlying spectrum $f _ { 0 } ( \omega ; \pmb \theta )$ is unknown. A class of Gaussian semi-parametric estimators simply uses the same Whittle likelihood (5.24), evaluated over a sub-band of low frequencies, say $m ^ { \prime } = { \sqrt { n } }$ . There is some latitude in selecting a band that is relatively free from low frequency interference due to the short memory component in (5.25).

Geweke and Porter–Hudak (1983) developed an approximate method for estimating $d$ based on a regression model, derived from (5.24). Note that we may write a simple equation for the logarithm of the spectrum as

$$
\ln f _ {x} \left(\omega_ {k}; d\right) = \ln f _ {0} \left(\omega_ {k}; \boldsymbol {\theta}\right) - d \ln \left[ 4 \sin^ {2} \left(\pi \omega_ {k}\right) \right], \tag {5.26}
$$

with the frequencies $\omega _ { k } = k / n$ restricted to a range $k = 1 , 2 , \ldots , m ^ { \prime }$ near the zero frequency with $m ^ { \prime } = { \sqrt { n } }$ as the recommended value. Relationship (5.26) suggests using a simple linear regression model of the form,

$$
\ln I \left(\omega_ {k}\right) = \beta_ {0} - d \ln \left[ 4 \sin^ {2} \left(\pi \omega_ {k}\right) \right] + e _ {k} \tag {5.27}
$$

for the periodogram to estimate the parameters $\sigma _ { w } ^ { 2 }$ and $d$ . In this case, one performs least squares using $\ln { \cal I } ( \omega _ { k } )$ as the dependent variable, and $\ln [ 4 \sin ^ { 2 } ( \pi \omega _ { k } ) ]$ as the independent variable for $k = 1 , 2 , \ldots , m$ . The resulting slope estimate is then used as an estimate of $- d$ . For a good discussion of various alternative methods for selecting $m$ , see Hurvich and Deo (1999).

One of the above two procedures works well for estimating the long memory component but there will be cases (such as ARFIMA) where there will be a parameterized short memory component $f _ { 0 } ( \omega _ { k } ; \pmb \theta )$ that needs to be estimated. If the spectrum is highly parameterized, one might estimate using the Whittle log likelihood (5.21) and

$$
f _ {x} \left(\omega_ {k}; \boldsymbol {\theta}\right) = g _ {k} ^ {- d} f _ {0} \left(\omega_ {k}; \boldsymbol {\theta}\right)
$$

and jointly estimating the parameters $d$ and $\pmb \theta$ using the Newton–Raphson method. If we are interested in a nonparametric estimator, using the conventional smoothed spectral estimator for the periodogram, adjusted for the long memory component, say $g _ { k } ^ { d }$ $I ( \omega _ { k } )$ might be a possible approach.

# 5.3 GARCH Models

Recent problems in finance have motivated the study of the volatility, or variability, of a time series. Although ARMA models assume a constant variance, models such as the autoregressive conditionally heteroscedastic or ARCH model, first introduced by Engle (1982), were developed to model changes in volatility. These models were later extended to generalized ARCH, or GARCH models by Bollerslev (1986).

In §3.8, we discussed the return or growth rate of a series. For example, if $x _ { t }$ is the value of a stock at time $t$ , then the return or relative gain, $y _ { t }$ , of the stock at time $t$ is

$$
y _ {t} = \frac {x _ {t} - x _ {t - 1}}{x _ {t - 1}}. \tag {5.28}
$$

Definition (5.28) implies that $x _ { t } = ( 1 + y _ { t } ) x _ { t - 1 }$ . Thus, based on the discussion in §3.8, if the return represents a small (in magnitude) percentage change then

$$
\nabla \left[ \ln \left(x _ {t}\right) \right] \approx y _ {t}. \tag {5.29}
$$

# 5.3: GARCH Models

Either value, $\nabla [ \ln ( x _ { t } ) ]$ or $( x _ { t } - x _ { t - 1 } ) / x _ { t - 1 }$ , will be called the return, and will be denoted by $y _ { t }$ . It is the study of $y _ { t }$ that is the focus of ARCH, GARCH, and other volatility models. Recently there has been interest in stochastic volatility models and we will discuss these models in Chapter 6 because they are state-space models.

Typically, for financial series, the return $y _ { t }$ , does not have a constant variance, and highly volatile periods tend to be clustered together. In other words, there is a strong dependence of sudden bursts of variability in a return on the series own past. For example, Figure 1.4 shows the daily returns of the New York Stock Exchange (NYSE) from February 2, 1984 to December 31, 1991. In this case, as is typical, the return $y _ { t }$ is fairly stable, except for short-term bursts of high volatility.

The simplest ARCH model, the ARCH(1), models the return as

$$
y _ {t} = \sigma_ {t} \epsilon_ {t} \tag {5.30}
$$

$$
\sigma_ {t} ^ {2} = \alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2}, \tag {5.31}
$$

where $\epsilon _ { t }$ is standard Gaussian white noise; that is, $\epsilon _ { t } \sim$ iid $\mathrm { { N } } ( 0 , 1 )$ . As with ARMA models, we must impose some constraints on the model parameters to obtain desirable properties. One obvious constraint is that $\alpha _ { 1 }$ must not be negative, or else $\sigma _ { t } ^ { 2 }$ may be negative.

As we shall see, the ARCH(1) models return as a white noise process with nonconstant conditional variance, and that conditional variance depends on the previous return. First, notice that the conditional distribution of $y _ { t }$ given $_ { y _ { t - 1 } }$ is Gaussian:

$$
y _ {t} \mid y _ {t - 1} \sim \mathrm {N} \left(0, \alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2}\right). \tag {5.32}
$$

In addition, it is possible to write the ARCH(1) model as a non-Gaussian AR(1) model in the square of the returns $y _ { t } ^ { 2 }$ . To do this, rewrite (5.30)-(5.31) as

$$
{y _ {t} ^ {2}} = {\sigma_ {t} ^ {2} \epsilon_ {t} ^ {2}}
$$

$$
\alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2} = \sigma_ {t} ^ {2},
$$

and subtract the two equations to obtain

$$
y _ {t} ^ {2} - \left(\alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2}\right) = \sigma_ {t} ^ {2} \epsilon_ {t} ^ {2} - \sigma_ {t} ^ {2}.
$$

Now, write this equation as

$$
y _ {t} ^ {2} = \alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2} + v _ {t}, \tag {5.33}
$$

where $v _ { t } = \sigma _ { t } ^ { 2 } ( \epsilon _ { t } ^ { 2 } - 1 )$ . Because $\epsilon _ { t } ^ { 2 }$ is the square of a $\mathrm { { N } } ( 0 , 1 )$ random variable, $\epsilon _ { t } ^ { 2 } - 1$ is a shifted (to have mean-zero), $\chi _ { 1 } ^ { 2 }$ random variable.

To explore the properties of ARCH, we define $Y _ { s } = \{ y _ { s } , y _ { s - 1 } , \ldots \}$ . Then, using (5.32), we immediately see that $y _ { t }$ has a zero mean:

$$
E \left(y _ {t}\right) = E E \left(y _ {t} \mid Y _ {t - 1}\right) = E E \left(y _ {t} \mid y _ {t - 1}\right) = 0. \tag {5.34}
$$

Because $E ( y _ { t } \mid Y _ { t - 1 } ) = 0$ , the process $y _ { t }$ is said to be a martingale difference.

Because $y _ { t }$ is a martingale difference, it is also an uncorrelated sequence. For example, with $h > 0$ ,

$$
\begin{array}{l} \operatorname {c o v} \left(y _ {t + h}, y _ {t}\right) = E \left(y _ {t} y _ {t + h}\right) = E E \left(y _ {t} y _ {t + h} \mid Y _ {t + h - 1}\right) \\ = E \left\{y _ {t} E \left(y _ {t + h} \mid Y _ {t + h - 1}\right) \right\} = 0. \tag {5.35} \\ \end{array}
$$

The last line of (5.35) follows because $y _ { t }$ belongs to the information set $Y _ { t + h - 1 }$ for $h > 0$ , and, $E ( y _ { t + h } \mid Y _ { t + h - 1 } ) = 0$ , as determined in (5.34).

An argument similar to (5.34) and (5.35) will establish the fact that the error process $v _ { t }$ in (5.33) is also a martingale difference and, consequently, an uncorrelated sequence. If the variance of $v _ { t }$ is finite and constant with respect to time, and $0 \leq \alpha _ { 1 } < 1$ , then based on Property P3.1, (5.33) specifies a causal AR(1) process for $y _ { t } ^ { 2 }$ . Therefore, $E ( y _ { t } ^ { 2 } )$ and $\mathrm { v a r } ( y _ { t } ^ { 2 } )$ must be constant with respect to time $t$ . This, implies that

$$
E \left(y _ {t} ^ {2}\right) = \operatorname {v a r} \left(y _ {t}\right) = \frac {\alpha_ {0}}{1 - \alpha_ {1}} \tag {5.36}
$$

and, after some manipulations,

$$
E \left(y _ {t} ^ {4}\right) = \frac {3 \alpha_ {0} ^ {2}}{(1 - \alpha_ {1}) ^ {2}} \frac {1 - \alpha_ {1} ^ {2}}{1 - 3 \alpha_ {1} ^ {2}}, \tag {5.37}
$$

provided $3 \alpha _ { 1 } ^ { 2 } < 1$ . These results imply that the kurtosis, $\kappa$ , of $y _ { t }$ i s

$$
\kappa = \frac {E \left(y _ {t} ^ {4}\right)}{\left[ E \left(y _ {t} ^ {2}\right) \right] ^ {2}} = 3 \frac {1 - \alpha_ {1} ^ {2}}{1 - 3 \alpha_ {1} ^ {2}}, \tag {5.38}
$$

which is always larger than 3 (unless $\alpha _ { 1 } = 0$ ), the kurtosis of the normal distribution. Thus, the marginal distribution of the returns, $y _ { t }$ , is leptokurtic, or has “fat tails.”

In summary, an ARCH(1) process, $y _ { t }$ , as given by (5.30)-(5.31), or equivalently (5.32), is characterized by the following properties.

If $0 \leq \alpha _ { 1 } < 1$ , the process $y _ { t }$ itself is white noise and its unconditional distribution is symmetrically distributed around zero; this distribution is leptokurtic.   
• If, in addition, $3 \alpha _ { 1 } ^ { 2 } < 1$ , the square of the process, $y _ { t } ^ { 2 }$ , follows a causal AR(1) model with ACF given by $\rho _ { y ^ { 2 } } ( h ) = \alpha _ { 1 } ^ { h } \ge 0$ , for all $h > 0$ . If $3 \alpha _ { 1 } \geq 1$ , but $\alpha _ { 1 } < 1$ , then $y _ { t } ^ { 2 }$ is strictly stationary with infinite variance.

Estimation of the parameters $\alpha _ { 0 }$ and $\alpha _ { 1 }$ of the ARCH(1) model is typically accomplished by conditional MLE. The conditional likelihood of the data $y _ { 2 } , . . . . , y _ { n }$ given $y _ { 1 }$ , is given by

$$
L \left(\alpha_ {0}, \alpha_ {1} \mid y _ {1}\right) = \prod_ {t = 2} ^ {n} f _ {\alpha_ {0}, \alpha_ {1}} \left(y _ {t} \mid y _ {t - 1}\right), \tag {5.39}
$$

# 5.3: GARCH Models

where the density $f _ { \alpha _ { 0 } , \alpha _ { 1 } } ( y _ { t } \mid y _ { t - 1 } )$ is the normal density specified in (5.32). Hence, the criterion function to be minimized, $l ( \alpha _ { 0 } , \alpha _ { 1 } ) \propto - \ln { L ( \alpha _ { 0 } , \alpha _ { 1 } \mid y _ { 1 } ) }$ is given by

$$
l \left(\alpha_ {0}, \alpha_ {1}\right) = \frac {1}{2} \sum_ {t = 2} ^ {n} \ln \left(\alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2}\right) + \frac {1}{2} \sum_ {t = 2} ^ {n} \left(\frac {y _ {t} ^ {2}}{\alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2}}\right). \tag {5.40}
$$

Estimation is accomplished by numerical methods, as described in 3.6. In this case, analytic expressions for the gradient vector, $l ^ { ( 1 ) } ( \alpha _ { 0 } , \alpha _ { 1 } )$ , and Hessian matrix, $l ^ { ( 2 ) } ( \alpha _ { 0 } , \alpha _ { 1 } )$ , as described in Example 3.28, can be obtained by straightforward calculations. For example, the $2 \times 1$ gradient vector, $l ^ { ( 1 ) } ( \alpha _ { 0 } , \alpha _ { 1 } )$ , is given by

$$
\left( \begin{array}{c} \partial l / \partial \alpha_ {0} \\ \partial l / \partial \alpha_ {1} \end{array} \right) = \sum_ {t = 2} ^ {n} \binom {1} {y _ {t - 1} ^ {2}} \times \frac {\alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2} - y _ {t} ^ {2}}{2 \left(\alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2}\right) ^ {2}}. \tag {5.41}
$$

The calculation of the Hessian matrix is left as an exercise (Problem 5.7). The likelihood of the ARCH model tends to be flat unless $n$ is very large. A discussion of this problem can be found in Shephard (1996).

It is also possible to combine a regression or an ARMA model for the mean with an ARCH model for the errors. For example, a regression with ARCH(1) errors model would have the observations $x _ { t }$ as linear function of $p$ regressors, $\pmb { z } _ { t } = ( z _ { t 1 } , . . . , z _ { t p } ) ^ { \prime }$ , and ARCH(1) noise $y _ { t }$ , say,

$$
x _ {t} = \boldsymbol {\beta} ^ {\prime} \boldsymbol {z} _ {t} + y _ {t},
$$

where $y _ { t }$ satisfies (5.30)-(5.31), but, in this case, is unobserved. Similarly, for example, an AR(1) model for data $x _ { t }$ exhibiting ARCH(1) errors would be

$$
x _ {t} = \phi_ {0} + \phi_ {1} x _ {t - 1} + y _ {t}.
$$

These types of models were explored by Weiss (1984).

# Example 5.3 Analysis of U.S. GNP

In Example 3.35, we fit an MA(2) model and an AR(1) model to the U.S. GNP series and we concluded that the residuals from both fits appeared to behave like a white noise process. In Example 3.39 we concluded that the AR(1) is probably the better model in this case. It has been suggested that the U.S. GNP series has ARCH errors, and in this example, we will investigate this claim. If the GNP noise term is ARCH, the squares of the residuals from the fit should behave like a non-Gaussian AR(1) process, as pointed out in (5.33). Figure 5.5 shows the ACF and PACF of the squared residuals it appears that there may be some dependence, albeit small, left in the residuals.

We used the S-PLUS GARCH module to fit an AR(1)-ARCH(1) model to the U.S. GNP returns with the following results:

![](images/af106a884aea1cfc64ad7b06575c9b1a33bee4a9366a615abed34637c6dfa3b9.jpg)

![](images/658375fa4a1ac5010e94f4da2bfa60b0aa34d1a855441f34c5485194a265f1ca.jpg)  
Figure 5.5 ACF and PACF of the squares of the residuals from the AR(1) fit on U.S. GNP.

> gnp96 <- matrix(scan("/mydata/gnp96.dat"),ncol=2,byrow=T)  
> gnpr <- diff(log(gnp96[,2])) # gnp returns   
> gnpr.mod <- garch(gnpr˜ar(1),˜garch(1,0)) # model call   
> summary(gnpr.mod)

Estimated Coefficients: Value Std.Error t value $\mathbf{Pr}(|t|)$ C 0.00522 8.264e-004 6.326 6.990e-010 # AR cnst AR(1) 0.36721 7.888e-002 4.656 2.798e-006 # AR coef A 0.00007 6.978e-006 10.349 0.000e+000 # ARCH cnst ARCH(1) 0.20242 7.031e-002 2.879 2.193e-003 # ARCH coef

```txt
Residual Tests:  
Jarque-Bera P-value # tests normal skewness & kurtosis  
8.643 0.01328  
Shapiro-Wilk P-value # tests normal order statistics  
0.9827 0.4829  
Q-Statistic P-value Chi^2-d.f.  
13.88 0.3087 12 
```

In this example, we obtain $\widehat { \phi } _ { 0 } ~ = ~ . 0 0 5$ and $\widehat { \phi } _ { 1 } = . 3 6 7$ for the AR(1) parameter estimates; in Example 3.35 the values were .005 and .347, respectively. The ARCH(1) parameter estimates are $\widehat { \alpha } _ { 0 } \ = \ 0$ for the constant and $\widehat { \alpha } _ { 1 } = . 2 0 2$ , which is highly significant with a p-value of

# 5.3: GARCH Models

about .002. The Jarque–Bera statistic tests the residuals of the fit for normality based on the observed skewness and kurtosis, and it appears that the residuals have some non-normal skewness and kurtosis. The Shapiro–Wilk statistic tests the residuals of the fit for normality based on the empirical order statistics. In this case, the residuals appear to be normal. Finally, the Q-statistic is used on the squared residuals, and we conclude that the squared residuals appear to be an uncorrelated sequence.

To repeat the analysis in R without the simultaneous estimation, download the package tseries from CRAN and load it. Then, perform the AR estimation first and use those residuals for the ARCH fit as follows (assuming gnpr is available as in the S-PLUS example). We note that the results are similar to the simultaneous estimation results from S-PLUS.

> gnpr.ar $=$ ar.mle(gnpr, order.max=1) # recall phi1 $=$ .347   
> y = gnpr.ar$resid[2:length(gnpr)] # first resid is NA   
> arch.y $=$ garch(y,order=c(0,1))   
> summary.garch(arch.y) # partial output below

Coefficient(s):

Estimate Std. Error t value $\mathbb { P r } ( > | \mathtt { t } | )$

a0 7.403e-05 7.275e-06 10.175 < 2e-16 # ARCH cnst

a1 1.939e-01 6.781e-02 2.859 0.00425 # ARCH coef

Jarque Bera Test:

X-squared = 8.4801, df = 2, p-value = 0.01441

Box-Ljung test (squared residuals):

X-squared $=$ 3e-04, df = 1, p-value = 0.9865

The ARCH(1) model can be extended to the general ARCH(m) model in an obvious way. That is, (5.30) is retained,

$$
y _ {t} = \sigma_ {t} \epsilon_ {t}, \tag {5.30}
$$

but (5.31) is extended to

$$
\sigma_ {t} ^ {2} = \alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2} + \dots + \alpha_ {m} y _ {t - m} ^ {2}. \tag {5.42}
$$

Estimation for ARCH(m) also follows in an obvious way from the discussion of estimation for ARCH(1) models. That is, the conditional likelihood of the data $y _ { m + 1 } , . . . . , y _ { n }$ given $y _ { 1 } , \ldots , y _ { m }$ , is given by

$$
L (\boldsymbol {\alpha} \mid y _ {1}, \dots , y _ {m}) = \prod_ {t = m + 1} ^ {n} f _ {\boldsymbol {\alpha}} \left(y _ {t} \mid y _ {t - 1}, \dots , y _ {t - m}\right), \tag {5.43}
$$

where ${ \pmb { \alpha } } = ( \alpha _ { 0 } , \alpha _ { 1 } , \dots , \alpha _ { m } )$ and the conditional densities $f _ { \pmb { \alpha } } ( \cdot | \cdot )$ in (5.43) are normal densities; that is, for $t > m$ ,

$$
y _ {t} \mid y _ {t - 1}, \ldots , y _ {t - m} \sim \mathrm {N} (0, \alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2} + \dots + \alpha_ {m} y _ {t - m} ^ {2}).
$$

Another extension of ARCH is the generalized ARCH or GARCH model developed by Bollerslev (1986). For example, a GARCH $( 1 , 1 )$ model retains (5.30),

$$
y _ {t} = \sigma_ {t} \epsilon_ {t}, \tag {5.30}
$$

but extends (5.31) as follows:

$$
\sigma_ {t} ^ {2} = \alpha_ {0} + \alpha_ {1} y _ {t - 1} ^ {2} + \beta_ {1} \sigma_ {t - 1} ^ {2}. \tag {5.44}
$$

Under the condition that $\alpha _ { 1 } + \beta _ { 1 } ~ < ~ 1$ , using similar manipulations as in (5.33), the GARCH(1, 1) model, (5.30) and (5.44), admits a non-Gaussian $\mathrm { A R M A } ( 1 , 1 )$ model for the squared process

$$
y _ {t} ^ {2} = \alpha_ {0} + \left(\alpha_ {1} + \beta_ {1}\right) y _ {t - 1} ^ {2} + v _ {t} - \beta_ {1} v _ {t - 1}, \tag {5.45}
$$

where $v _ { t }$ is as defined in (5.33). Representation (5.45) follows by writing (5.30) as

$$
\begin{array}{l} y _ {t} ^ {2} - \sigma_ {t} ^ {2} = \sigma_ {t} ^ {2} \left(\epsilon_ {t} ^ {2} - 1\right) \\ \beta_ {1} \left(y _ {t - 1} ^ {2} - \sigma_ {t - 1} ^ {2}\right) = \beta_ {1} \sigma_ {t - 1} ^ {2} \left(\epsilon_ {t - 1} ^ {2} - 1\right), \\ \end{array}
$$

subtracting the second equation from the first, and using the fact that, from (5.44), $\sigma _ { t } ^ { 2 } - \beta _ { 1 } \sigma _ { t - 1 } ^ { 2 } = \alpha _ { 0 } + \alpha _ { 1 } y _ { t - 1 } ^ { 2 }$ , on the left-hand side of the result. The $\mathrm { G A R C H } ( m , r )$ model retains (5.30) and extends (5.44) to

$$
\sigma_ {t} ^ {2} = \alpha_ {0} + \sum_ {j = 1} ^ {m} \alpha_ {j} y _ {t - j} ^ {2} + \sum_ {j = 1} ^ {r} \beta_ {j} \sigma_ {t - j} ^ {2}. \tag {5.46}
$$

Conditional maximum likelihood estimation of the GARCH $( m , r )$ model parameters is similar to the ARCH( $m$ ) case, wherein the conditional likelihood, (5.43), is the product of $\mathrm { N } ( 0 , \sigma _ { t } ^ { 2 } )$ densities with $\sigma _ { t } ^ { 2 }$ given by (5.46) and where the conditioning is on the first $\operatorname* { m a x } ( m , r )$ observations, with $\sigma _ { 1 } ^ { 2 } = \cdot \cdot \cdot = \sigma _ { r } ^ { 2 } = 0$ . Once the parameter estimates are obtained, the model can be used to obtain one-step-ahead forecasts of the volatility, say $\widehat { \sigma } _ { t + 1 } ^ { 2 }$ , given by

$$
\hat {\sigma} _ {t + 1} ^ {2} = \hat {\alpha} _ {0} + \sum_ {j = 1} ^ {m} \hat {\alpha} _ {j} y _ {t + 1 - j} ^ {2} + \sum_ {j = 1} ^ {r} \hat {\beta} _ {j} \hat {\sigma} _ {t + 1 - j} ^ {2}. \tag {5.47}
$$

We explore these concepts in the following example.

# Example 5.4 GARCH Analysis of the NYSE Returns

As previously mentioned, the daily returns of the NYSE shown in Figure 1.4 exhibit classic GARCH features. We used the R tseries package to fit a GARCH $( 1 , 1 )$ model to the series with the following results:

>nyse $=$ scan("/mydata/nyse.dat") $>$ nyse.g $=$ garch(nyse,order=c(1,1))   
>summary.garch(nyse.g)

Coefficient(s): Estimate Std. Error t value $\mathrm{Pr}(>|t|)$ a0 6.552e-06 6.761e-07 9.691 <2e-16 # alpha0 a1 1.118e-01 4.056e-03 27.554 <2e-16 # alpha1 b1 8.086e-01 1.292e-02 62.566 <2e-16 # beta1

```txt
Diagnostic Tests:  
Jarque Bera Test - data: Residuals  
X-squared = 3983.873, df = 2, p-value < 2.2e-16  
Box-Ljung Test - data: Squared.Residuals  
X-squared = 1.5874, df = 1, p-value = 0.2077 
```

To explore the GARCH predictions, we calculated and plotted the middle of the data along (which includes the October 19, 1987 crash) with the one-step-ahead predictions of the corresponding volatility, $\sigma _ { t } ^ { 2 }$ . The results are displayed as $\pm \widehat { \sigma } _ { t }$ as a dashed line surrounding the data in Figure 5.6. These predictions can be obtained easily in R using the tseries package.

```python
> u = predict.garch(nyse.g)
> plot(800:1000, nyse[800:1000], type="l", xlab="Time",
+ ylab="NYSE Returns")
> lines(u[,1], col="blue", lty="dashed")
> lines(u[,2], col="blue", lty="dashed") 
```

Some key points can be gleaned from the examples of this section. First, it is apparent that the conditional distribution of the returns is rarely normal. S-PLUS allows for long tailed distributions to be fit to the data, whereas R does not. In particular, aside from the Gaussian distribution (the default), the S-PLUS Garch module allows for $t$ , double exponential, and generalized double exponential1 conditional distributions. Also, the predictions shown in Figure 5.6 leave something to be desired. It appears the model is better at telling you what the volatility was rather than what it is going to be; basically, increases or decreases in predicted volatility are a day late. In addition to these points, some other drawbacks of the GARCH model are: (i) the model assumes positive and negative returns have the same effect because volatility depends on squared returns; (ii) the model is restrictive because of the tight constraints on the model parameters (e.g., for an ARCH(1), $\begin{array} { r } { 0 \leq \alpha _ { 1 } ^ { 2 } < \frac { 1 } { 3 } } \end{array}$ ); (iii) the likelihood is flat unless $n$ is very large; (iv) the model tends to overpredict volatility because it responds slowly to large isolated returns.

![](images/69ddc30bc54ebdec91386aee4619fd705f18e7140ef0b83fa7cb6b1dc3b5ecc5.jpg)  
Figure 5.6 GARCH predictions of the NYSE volatility, $\pm \widehat { \sigma } _ { t }$ , displayed as dashed lines.

Various extensions to the original model have been proposed to overcome some of the shortcomings we have just mentioned. For example, we have already discussed the fact that the S-PLUS Garch module will fit some nonnormal, albeit symmetric, distributions. For asymmetric return dynamics, one can use the EGARCH (exponential GARCH) model, which is a complex model that has different components for positive returns and for negative returns. In the case of persistence in volatility, the integrated GARCH (IGARCH) model may be used. Recall (5.45) where we showed the GARCH $( 1 , 1 )$ model can be written as

$$
y _ {t} ^ {2} = \alpha_ {0} + (\alpha_ {1} + \beta_ {1}) y _ {t - 1} ^ {2} + v _ {t} - \beta_ {1} v _ {t - 1}
$$

and $y _ { t } ^ { 2 }$ is stationary if $\alpha _ { 1 } + \beta _ { 1 } < 1$ . The IGARCH model sets $\alpha _ { 1 } + \beta _ { 1 } = 1$ , in which case the IGARCH $( 1 , 1 )$ model is

$$
y _ {t} = \sigma_ {t} \epsilon_ {t} \quad \text {a n d} \quad \sigma_ {t} ^ {2} = \alpha_ {0} + (1 - \beta_ {1}) y _ {t - 1} ^ {2} + \beta_ {1} \sigma_ {t - 1} ^ {2}.
$$

There are many different extensions to the basic ARCH model that were developed to handle the various situations noticed in practice. Interested readers might find the general discussions in Bollerslev et al. (1994) and Shephard (1996) worthwhile reading. Also, Gouri´eroux (1997) gives a detailed presentation of ARCH and related models with financial applications and contains an extensive bibliography. Two excellent texts on financial time series analysis are Chan (2002) and Tsay (2001).

Finally, we briefly discuss stochastic volatility models; a detailed treatment of these models is given in Chapter 6. The volatility component, $\sigma _ { t } ^ { 2 }$ , in the GARCH model is conditionally nonstochastic. In the ARCH(1) model for example, any time the previous return is zero, i.e., $y _ { t - 1 } = 0$ , it must be the case that $\sigma _ { t } ^ { 2 } = \alpha _ { 0 }$ , and so on. This assumption seems a bit unrealistic. The stochastic volatility model adds a stochastic component to the volatility in the following way. In the GARCH model, a return, say $y _ { t }$ , is

$$
y _ {t} = \sigma_ {t} \epsilon_ {t} \quad \Rightarrow \quad \log y _ {t} ^ {2} = \log \sigma_ {t} ^ {2} + \log \epsilon_ {t} ^ {2}. \tag {5.48}
$$

In this way, we see that the observations $\log y _ { t } ^ { 2 }$ , are made up of two components, the unobserved volatility $\log \sigma _ { t } ^ { 2 }$ , which may be considered a latent variable, and unobserved noise $\log \epsilon _ { t } ^ { 2 }$ . While, for example, the $\mathrm { G A R C H } ( 1 , 1 )$ $( 1 , 1 )$ models volatility without error, $\sigma _ { t + 1 } ^ { 2 } = \alpha _ { 0 } + \alpha _ { 1 } r _ { t } ^ { 2 } + \beta _ { 1 } \sigma _ { t } ^ { 2 }$ , the basic stochastic volatility model assumes the latent variable is an autoregressive process,

$$
\log \sigma_ {t + 1} ^ {2} = \phi_ {0} + \phi_ {1} \log \sigma_ {t} ^ {2} + w _ {t} \tag {5.49}
$$

where $w _ { t } \sim$ iid $\mathrm { N } ( 0 , \sigma _ { w } ^ { 2 } )$ . The introduction of the noise term $w _ { t }$ makes the latent volatility process stochastic. Together (5.48) and (5.49) comprise the stochastic volatility model. Given $n$ observations, the goals are to estimate the parameters $\phi _ { 0 }$ , $\phi _ { 1 }$ and $\sigma _ { w } ^ { 2 }$ , and then predict future observations $\log { y _ { n + m } ^ { 2 } }$ . Details are provided in §6.10.

# 5.4 Threshold Models

In §3.5 we discussed the fact that, for a stationary time series, best linear prediction forward in time is the same as best linear prediction backward in time. This result followed from the fact that the variance–covariance matrix of $\pmb { x } _ { 1 : n } = ( x _ { 1 } , x _ { 2 } , . . . , x _ { n } ) ^ { \prime }$ , say, $\Gamma = \{ \gamma ( i - j ) \} _ { i , j = 1 } ^ { n }$ , is the same as the variance– covariance matrix of $\pmb { x } _ { n : 1 } = ( x _ { n } , x _ { n - 1 } , . . . , x _ { 1 } ) ^ { \prime }$ . In addition, if the process is Gaussian, the distributions of ${ \pmb x } _ { 1 : n }$ and $\pmb { x } _ { n : 1 }$ are identical. In this case, a time plot of ${ \pmb x } _ { 1 : n }$ (that is, the data plotted forward in time) should look similar to a time plot of $\pmb { x } _ { n : 1 }$ (that is, the data plotted backward in time).

There are, however, many series that do not fit into this category. For example, Figure 5.7 shows a plot of monthly pneumonia and influenza deaths per 10,000 in the U.S. for 11 years, 1968 to 1978. Typically, the number of deaths tends to increase slower than it decreases. Thus, if the data were plotted backward in time, the backward series would tend to increase faster than it decreases. Also, if monthly pneumonia and influenza deaths followed a linear Gaussian process, we would not expect to see such large bursts of positive and negative changes that occur periodically in this series. Moreover, although the number of deaths is typically largest during the winter months, the data are

![](images/a05517ad11aebed6c9ba13a62ae8276bc6c3d75237aabfedc68b4945a39dd5a5.jpg)  
Figure 5.7 U.S. monthly pneumonia and influenza deaths per 10,000 over 11 years from 1968 to 1978.

not perfectly seasonal. That is, although the peak of the series often occurs in January, in other years, the peak occurs in December, February, or March.

If our goal is to predict flu epidemics, then it should be clear that a Gaussian linear model would not be appropriate. Many approaches to modeling nonlinear series exist that could be used (see Priestley, 1988); here, we focus on the class of threshold autoregressive models presented in Tong (1983, 1990). The basic idea of these models is that of fitting local linear $\operatorname { A R } ( p )$ models, and their appeal is that we can use the intuition from fitting global linear $\operatorname { A R } ( p )$ models. Suppose we know $p$ , and given the vectors $\pmb { x } _ { t - 1 } = ( x _ { t - 1 } , . . . , x _ { t - p } ) ^ { \prime }$ , we can identify $r$ mutually exclusive and exhaustive regions for ${ \pmb x } _ { t - 1 }$ , say, $R _ { 1 } , . . . , R _ { r }$ , where the dynamics of the system changes. The threshold model is then written as $r$ $\cdot \operatorname { A R } ( p )$ models,

$$
x _ {t} = \alpha^ {(j)} + \phi_ {1} ^ {(j)} x _ {t - 1} + \dots + \phi_ {p} ^ {(j)} x _ {t - p} + w _ {t} ^ {(j)}, \quad \boldsymbol {x} _ {t - 1} \in R _ {j}, \tag {5.50}
$$

for $j = 1 , . . . , r$ . In (5.50), the $w _ { t } ^ { ( j ) }$ are independent white noise series, each with variance $\sigma _ { j } ^ { 2 }$ , for $j = 1 , . . . , r$ . Model estimation, identification, and diagnostics proceed as in the case in which $r = 1$ .

# Example 5.5 Threshold Modeling of the Influenza Series

As previously discussed, examination of Figure 5.7 leads us to believe that the monthly pneumonia and influenza deaths time series, say ${ \mathrm { \ f u } } _ { t }$ , is not linear. It is also evident from Figure 5.7 that there is a slight negative trend in the data. We have found that the most convenient way

to fit a threshold model to this data set, while removing the trend, is to work with the first difference of the data. The differenced data,

$$
x _ {t} = \mathrm {f l u} _ {t} - \mathrm {f l u} _ {t - 1}
$$

is exhibited in Figure 5.8 as the dark solid line with circles representing observations. The dashed line with squares in Figure 5.8 are the onemonth-ahead predictions, and we will discuss this series later.

The nonlinearity of the data is more pronounced in the plot of the first differences, $x _ { t }$ . Clearly, the change in the numbers of deaths, $x _ { t }$ , slowly rises for some months and, then, sometime in the winter, has a possibility of jumping to a large number once $x _ { t }$ exceeds about .05. If the processes does make a large jump, then a subsequent significant decrease occurs in flu deaths. As an initial analysis, we fit the following threshold model

$$
\begin{array}{l} x _ {t} = \alpha^ {(1)} + \sum_ {j = 1} ^ {p} \phi_ {j} ^ {(1)} x _ {t - j} + w _ {t} ^ {(1)}, x _ {t - 1} <  . 0 5 \\ x _ {t} = \alpha^ {(2)} + \sum_ {j = 1} ^ {p} \phi_ {j} ^ {(2)} x _ {t - j} + w _ {t} ^ {(2)}, \quad x _ {t - 1} \geq . 0 5, \tag {5.51} \\ \end{array}
$$

with $p = 6$ , assuming this would be larger than necessary.

Model (5.51) is easy to fit using two linear regression runs. That is, let $\delta _ { t } ^ { ( 1 ) } = 1$ if $x _ { t - 1 } < . 0 5$ , and zero otherwise, and let $\delta _ { t } ^ { ( 2 ) } = 1$ if $x _ { t - 1 } \geq . 0 5$ , and zero otherwise. Then, using the notation of §2.2, for $t = p + 1 , . . . , n$ , either equation in (5.51) can be written as

$$
y _ {t} = \boldsymbol {\beta} ^ {\prime} \boldsymbol {z} _ {t} + w _ {t}
$$

where, for $i = 1 , 2$ ,

$$
y _ {t} = \delta_ {t} ^ {(i)} x _ {t}, \quad \boldsymbol {z} _ {t} ^ {\prime}, = \delta_ {t} ^ {(i)} (1, x _ {t - 1}, \dots , x _ {t - p}), \quad w _ {t} = \delta_ {t} ^ {(i)} w _ {t} ^ {(i)},
$$

and

$$
\boldsymbol {\beta} ^ {\prime} = \left(\alpha^ {(i)}, \phi_ {1} ^ {(i)}, \phi_ {2} ^ {(i)}, \dots , \phi_ {p} ^ {(i)}\right).
$$

Parameter estimates can then be obtained using the regression techniques of §2.2 twice, once for $i = 1$ and again for $i = 2$ .

For each model, an order $p = 4$ model was finally selected. The final model was

$$
\begin{array}{l} \widehat {x} _ {t} = . 5 1 _ {(. 0 8)} x _ {t - 1} -. 2 0 _ {(. 0 6)} x _ {t - 2} +. 1 2 _ {(. 0 5)} x _ {t - 3} \\ - 1 1 _ {(. 5)} x _ {t - 4} + \widehat {w} _ {t} ^ {(1)}, \quad \text {w h e n} x _ {t - 1} <   . 0 5 \\ \end{array}
$$

$$
\begin{array}{l} \widehat {x} _ {t} = . 4 0 -. 7 5 _ {(. 1 7)} x _ {t - 1} - 1. 0 3 _ {(. 2 1)} x _ {t - 2} - 2. 0 5 _ {(1. 0 5)} x _ {t - 3} \\ - 6. 7 1 _ {(1. 2 5)} x _ {t - 4} + \hat {w} _ {t} ^ {(2)}, \quad \text {w h e n} x _ {t - 1} \geq . 0 5, \\ \end{array}
$$

![](images/af9645a1257d1e6d01df29eca3d3c494ba9218601ec465beaa9ecc62cf48ee9b.jpg)  
Figure 5.8 First differenced U.S. monthly pneumonia and influenza deaths per 1,000 (solid line - circles); one-month-ahead predictions (dashed line -squares).

where $\widehat { \sigma } _ { 1 } = . 0 5$ and $\widehat { \sigma } _ { 2 } = . 0 7$ . The threshold of .05 was exceeded 17 times. Using the final model, one-month-ahead predictions can be made, and these are shown in Figure 5.8 as a dashed line with squares. The model does extremely well at predicting a flu epidemic; the peak at $t = 9 6$ , however, was missed by this model. When we fit a model with a smaller threshold of .04, flu epidemics were somewhat underestimated, but the flu epidemic in the eighth year was predicted one month early. We chose the model with a threshold of .05 because the residual diagnostics showed no obvious departure from the model assumption (except for one outlier at $t = 9 6$ ); the model with a threshold of .04 still had some correlation left in the residuals and there were more than one outliers. Finally, prediction beyond one-month-ahead for this model is very complicated, but some approximate techniques exist (see Tong, 1983).

# 5.5 Regression with Autocorrelated Errors

In §2.2, we covered the classical regression model with uncorrelated errors $w _ { t }$ . In this section, we discuss the modifications that might be considered when the errors are correlated. That is, consider the regression model

$$
y _ {t} = \boldsymbol {\beta} ^ {\prime} z _ {t} + x _ {t}, \tag {5.52}
$$

$t = 1 , \ldots , n$ , where $x _ { t }$ is a process with some covariance function $\gamma ( s , t )$ . Then, we have the matrix form

$$
\boldsymbol {y} = Z \boldsymbol {\beta} + \boldsymbol {x}, \tag {5.53}
$$

where $\mathbf { x } ~ = ~ ( x _ { 1 } , \ldots , x _ { n } ) ^ { \prime }$ is a $n \times 1$ vector with $n \times n$ covariance matrix $\Gamma = \{ \gamma ( s , t ) \}$ . Note that $Z = [ z _ { 1 } , z _ { 2 } , \ldots , z _ { n } ] ^ { \prime }$ is the $n \times q$ matrix of input variables, as before. If we know the covariance matrix $\Gamma$ , it is possible to find a transformation matrix $A$ , such that $A \Gamma A ^ { \prime } = \sigma ^ { 2 } I$ , where $I$ denotes the $n \times n$ identity matrix. Then, the underlying model can be transformed into

$$
\begin{array}{l} A \boldsymbol {y} = A Z \boldsymbol {\beta} + A \boldsymbol {x} \\ = U \boldsymbol {\beta} + \boldsymbol {w}, \\ \end{array}
$$

where $U = A Z$ and $\mathbf { \Delta } _ { \mathbf { w } }$ is a white noise vector with covariance matrix $\sigma ^ { 2 } I$ as in §2.2. Then, applying least squares or maximum likelihood to the vector $_ { A \pmb { y } }$ gives

$$
\begin{array}{l} \widehat {\boldsymbol {\beta}} _ {w} = (U ^ {\prime} U) ^ {- 1} U ^ {\prime} A \boldsymbol {y} \\ = \left(Z ^ {\prime} A ^ {\prime} A Z\right) ^ {- 1} Z ^ {\prime} A ^ {\prime} A y \\ = \left(Z ^ {\prime} \Gamma^ {- 1} Z\right) ^ {- 1} Z ^ {\prime} \Gamma^ {- 1} \boldsymbol {y} \tag {5.54} \\ \end{array}
$$

because

$$
\sigma^ {2} \Gamma^ {- 1} = A ^ {\prime} A.
$$

The difficulty in applying (5.54) is, we do not know the form of the matrix $\Gamma$ .

It may be possible, however, in the time series case, to assume a stationary covariance structure for the error process $x _ { t }$ that corresponds to a linear process and try to find an ARMA representation for $x _ { t }$ . For example, if we have a pure $\operatorname { A R } ( p )$ error, then

$$
\phi (B) x _ {t} = w _ {t},
$$

and $\phi ( B )$ is the linear transformation that, when applied to the error process, produces the white noise $w _ { t }$ . Regarding this transformation as the appropriate matrix $A$ of the preceding paragraph produces the transformed regression equation

$$
\phi (B) y _ {t} = \boldsymbol {\beta} ^ {\prime} \phi (B) \boldsymbol {z} _ {t} + w _ {t},
$$

and we are back to the same model as before. Defining $u _ { t } ~ = ~ \phi ( B ) y _ { t }$ and ${ \pmb v } _ { t } = \phi ( B ) z _ { t }$ leads to the simple regression problem

$$
u _ {t} = \boldsymbol {\beta} ^ {\prime} \boldsymbol {v} _ {t} + w _ {t} \tag {5.55}
$$

considered before. The preceding discussion suggests an algorithm, due to Cochrane and Orcutt (1949), for fitting a regression model with autocorrelated errors.

(i) First, run an ordinary regression of $y _ { t }$ on $z _ { t }$ (acting as if the errors are uncorrelated). Retain the residuals.

(ii) Fit an ARMA model to the residuals $\widehat { x } _ { t } = y _ { t } - \widehat { \beta } ^ { \prime } z _ { t }$ , say,

$$
\widehat {\phi} (B) \widehat {x} _ {t} = \widehat {\theta} (B) w _ {t} \tag {5.56}
$$

(iii) Then, apply the ARMA transformation to both sides (5.52), that is,

$$
u _ {t} = \frac {\widehat {\phi} (B)}{\widehat {\theta} (B)} y _ {t}
$$

and

$$
\boldsymbol {v} _ {t} = \frac {\widehat {\phi} (B)}{\widehat {\theta} (B)} \boldsymbol {z} _ {t},
$$

to obtain the transformed regression model (5.55).

(iv) Run an ordinary least squares regression model assuming uncorrelated errors on the transformed regression model (5.55), obtaining

$$
\widehat {\boldsymbol {\beta}} _ {w} = \left(V ^ {\prime} V\right) ^ {- 1} V ^ {\prime} \boldsymbol {u}, \tag {5.57}
$$

where $V = [ \pmb { v } _ { 1 } , \ldots , \pmb { \mathscr { v } } _ { n } ] ^ { \prime }$ and $\pmb { u } = ( u _ { 1 } , \ldots , u _ { n } ) ^ { \prime }$ are the corresponding transformed components.

The above procedure can be repeated until convergence and will approach the maximum likelihood solution under normality of the errors (for details, see Sargan, 1964).

# Example 5.6 Pollution, Temperature, Mortality with Correlated Errors

We consider further the best regression obtained in Example 2.2 of Chapter 2, relating adjusted temperature $T _ { t } - T .$ , $( T _ { t } - T . ) ^ { 2 }$ and particulate levels $P _ { t }$ to cardiovascular mortality $M _ { t }$ . Identifying the vectors

$$
\boldsymbol {z} _ {t} = (1, t, (T _ {t} - T.), (T _ {t} - T.) ^ {2}, P _ {t}) ^ {\prime}
$$

leads to a model of the form (5.52). Taking the residuals from the least squares regression, as described in Step (i), the sample ACF and PACF, shown in Figure 5.9, suggest an AR(2) model for the residuals. Note, $\widehat { \sigma } ^ { 2 } = 4 0 . 7 7$ and $R ^ { 2 } = . 5 9$ for this model.

![](images/3ac060572f2a161fe4e1c82c7c5489c4dec26ee7f9686d31ae271cffdee9a40b.jpg)

![](images/d0b1552f9371e753b25b967df04dfe038bc2ab35ca080a667732fa195c2714c1.jpg)  
Figure 5.9 Sample ACF and PACF of the mortality residuals indicating an AR(2) process.

For the residuals, we obtain a second-order autoregressive model with operator

$$
\phi (B) = 1 -. 2 2 0 7 B -. 3 6 2 7 B ^ {2}
$$

which is applied to both sides of the defining equation (5.52) to produce the transformed equation (5.55), as in Step (ii) above. Running the regression, as in Step (iii), yields the model

$$
\begin{array}{l} \widehat {M _ {t}} = 8 3. 5 4 - . 0 2 8 _ {(. 0 0 4)} t - . 1 9 6 _ {(. 0 3 9)} \left(T _ {t} - 7 4. 6\right) \\ +. 0 1 7 _ {(. 0 0 2)} (T _ {t} - 7 4. 6) ^ {2} +. 2 2 9 _ {(. 0 2 3)} P _ {t} \\ \end{array}
$$

as the model for transformed mortality, where the coefficients and estimated variances have changed slightly because of the transformation. The linear temperature component has decreased in magnitude from .473 to $- . 1 9 6$ , whereas the other components stayed almost the same. The new residuals from the transformed model have sample ACF and PACF in Figure 5.10 that show no prominent peaks and can probably be taken as white noise.

# 5.6 Lagged Regression: Transfer Function Modeling

In §4.10, we considered lagged regression in a frequency domain approach based on coherency. In this section we focus on a time domain approach to the same

![](images/534f6535d8355a6fd282578fb2a741554f0743d6eb422f77d4a5b064d2c89fdf.jpg)

![](images/559a51c62187378e9ae0743738c4f3a775a4a47c2effcb8209fc4105316d7322.jpg)  
Figure 5.10 Sample ACF and PACF of the mortality residuals after fitting an AR(2) model.

problem. In the previous section, we looked at autocorrelated errors but, still regarded the input series $z _ { t }$ as being fixed unknown functions of time. This consideration made sense for the time argument $t$ , but was less satisfactory for the other inputs, which are probably stochastic processes. For example, consider the SOI and Recruitment series that were presented in Example 1.5. The series are displayed in Figure 1.5. In this case, the interest is in predicting the output Recruitment series, say, $y _ { t }$ , from the input SOI, say $x _ { t }$ . We might consider the lagged regression model

$$
y _ {t} = \sum_ {j = 0} ^ {\infty} \alpha_ {j} x _ {t - j} + \eta_ {t} = \alpha (B) x _ {t} + \eta_ {t}, \tag {5.58}
$$

where $\textstyle \sum _ { j } | \alpha _ { j } | < \infty$ . We assume the input process $x _ { t }$ and noise process $\eta _ { t }$ in (5.58) are both stationary and mutually independent. The coefficients $\alpha _ { 0 } , \alpha _ { 1 } , \ldots$ describe the weights assigned to past values of $x _ { t }$ used in predicting $y _ { t }$ and we have used the notation

$$
\alpha (B) = \sum_ {j = 0} ^ {\infty} \alpha_ {j} B ^ {j}. \tag {5.59}
$$

In the Box and Jenkins (1970) formulation, we assign ARIMA models, say, ARIMA $( p , d , q )$ and ARIMA $( p _ { \eta } , d _ { \eta } , q _ { \eta } )$ , to the series $x _ { t }$ and $\eta _ { t }$ , respectively. The components of (5.58) in backshift notation, for the case of simple $\mathrm { A R M A } ( p , q )$ $( p , q )$ modeling of the input and noise, would have the representation

$$
\phi (B) x _ {t} = \theta (B) w _ {t} \tag {5.60}
$$

# 5.6: Transfer Functions

and

$$
\phi_ {\eta} (B) \eta_ {t} = \theta_ {\eta} (B) z _ {t}, \tag {5.61}
$$

where $\sigma _ { z } ^ { 2 }$ , respectively. Box and Jenkins (1970) proposed that systematic patterns $w _ { t }$ and $z _ { t }$ are independent white noise processes with variances $\sigma _ { w } ^ { 2 }$ and often observed in the coefficients $\alpha _ { j }$ , for $j = 1 , 2 , \ldots$ , could often be expressed as a ratio of polynomials involving a small number of coefficients, along with a specified delay, $d$ , so

$$
\alpha (B) = \frac {\delta (B) B ^ {d}}{\omega (B)}, \tag {5.62}
$$

where

$$
\omega (B) = 1 - \omega_ {1} B - \omega_ {2} B ^ {2} - \dots - \omega_ {r} B ^ {r} \tag {5.63}
$$

and

$$
\delta (B) = \delta_ {0} + \delta_ {1} B + \dots + \delta_ {s} B ^ {s} \tag {5.64}
$$

are the indicated operators; in this section, we find it convenient to represent the inverse of an operator, say, $[ \omega ( B ) ] ^ { - 1 }$ , as $1 / \omega ( B )$ .

Determining a parsimonious model involving a simple form for $\alpha ( B )$ and estimating all of the parameters in the above model are the main tasks in the transfer function methodology. Because of the large number of parameters, it is necessary to develop a sequential methodology. Suppose we focus first on finding the ARIMA model for the input $x _ { t }$ and apply this operator to both sides of (5.58), obtaining the new model

$$
\begin{array}{l} \tilde {y} _ {t} = \frac {\phi (B)}{\theta (B)} y _ {t} \\ = \alpha (B) w _ {t} + \frac {\phi (B)}{\theta (B)} \eta_ {t} \\ = \alpha (B) w _ {t} + \tilde {\eta} _ {t}, \\ \end{array}
$$

where $w _ { t }$ and the transformed noise $\bar { \eta } _ { t }$ are independent.

The series $w _ { t }$ is a prewhitened version of the input series, and its crosscorrelation with the transformed output series $\bar { y } _ { t }$ will be just

$$
\begin{array}{l} \gamma_ {\tilde {y} w} (h) = E [ \tilde {y} _ {t + h} w _ {t} ] \\ = E [ \sum_ {j = 0} ^ {\infty} \alpha_ {j} w _ {t + h - j} w _ {t} ] \\ = \sigma_ {w} ^ {2} \alpha_ {h}, \tag {5.65} \\ \end{array}
$$

because the autocovariance function of white noise will be zero except when $j =$ $h$ in (5.65). Hence, computing the cross-correlation between the prewhitened input series and the transformed output series should yield a rough estimate of the behavior of $\alpha ( B )$ .

![](images/bba0e3f7b2860b353d8a5fc25b55f88983b3ed4312d0cb6002d04216bd6c8a15.jpg)

![](images/5b23dd0fe3b10f7c3d2e726354c39bb4a6d17f5685e4c223706f594dbc15e0d3.jpg)  
Figure 5.11 Sample ACF and PACF of SOI.

![](images/b93370c97ccd6a381150e7f60cfc28835d1f5229d305ae26055bb5557307f87c.jpg)  
Figure 5.12 Sample CCF of the prewhitened, detrended SOI and the similarly transformed Recruitment series; negative lags indicate that SOI leads Recruitment.

# Example 5.7 Relating the Prewhitened SOI to the Transformed Recruitment Series

We give a simple example of the suggested procedure for the SOI and the Recruitment series. Figure 5.11 shows the sample ACF and PACF of the detrended SOI index, and it is clear, from the PACF, that an

# 5.6: Transfer Functions

autoregressive series with $p = 1$ will do a reasonable job. Fitting the series gave $\widehat { \phi } = . 5 8 9 , \widehat { \sigma } _ { w } ^ { 2 } = . 0 9 2$ , and we applied the operator $\left( 1 - . 5 8 9 B \right)$ to both $x _ { t }$ and $y _ { t }$ and computed the cross-correlation function, which is shown in Figure 5.12. Noting the apparent shift of $d = 5$ months and the exponential decrease thereafter, it seems plausible to hypothesize a model of the form

$$
\begin{array}{l} \alpha (B) = \delta_ {0} B ^ {5} \left(1 + \omega_ {1} B + \omega_ {1} ^ {2} B ^ {2} + \dots\right) \\ = \frac {\delta_ {0} B ^ {5}}{1 - \omega_ {1} B} \\ \end{array}
$$

for the transfer function. In this case, we would expect $\omega _ { 1 }$ to be negative.

In some cases, we may postulate the form of the separate components $\delta ( B )$ and $\omega ( B )$ , so we might write the equation

$$
y _ {t} = \frac {\delta (B) B ^ {d}}{\omega (B)} x _ {t} + \eta_ {t}
$$

as

$$
\omega (B) y _ {t} = \delta (B) B ^ {d} x _ {t} + \omega (B) \eta_ {t},
$$

or in regression form

$$
y _ {t} = \sum_ {k = 1} ^ {r} \omega_ {k} y _ {t - k} + \sum_ {k = 0} ^ {s} \delta_ {k} x _ {t - d - k} + u _ {t}, \tag {5.66}
$$

where

$$
u _ {t} = \omega (B) \eta_ {t}. \tag {5.67}
$$

The form of (5.66) suggests doing a regression on the lagged versions of both the input and output series to obtain $\widehat { \beta }$ , the estimate of the $( r + s + 1 ) \times 1$ regression vector

$$
\boldsymbol {\beta} = \left(\omega_ {1}, \dots , \omega_ {r}, \delta_ {0}, \delta_ {1}, \dots , \delta_ {s}\right) ^ {\prime}.
$$

The residuals from the regression above, say,

$$
\widehat {u} _ {t} = y _ {t} - \widehat {\beta} ^ {\prime} z _ {t},
$$

where

$$
\boldsymbol {z} _ {t} = \left(y _ {t - 1}, \dots , y _ {t - r}, x _ {t - d}, \dots , x _ {t - d - s}\right) ^ {\prime}
$$

denotes the usual vector of independent variables, could be used to approximate the best ARMA model for the noise process $\eta _ { t }$ , because we can compute an estimator for that process from the (5.67), using $\widehat { u } _ { t }$ and $\widehat { \omega } ( B )$ and applying the moving average operator to get $\widehat { \eta _ { t } }$ . Fitting an $\mathrm { A R M A } ( p _ { \eta } , q _ { \eta } )$ model to the this estimated noise then completes the specification. The preceding suggests the following sequential procedure for fitting the transfer function model to data.

(i) Fit an ARMA model to the input series $x _ { t }$ to estimate the parameters $\phi _ { 1 } , \ldots , \phi _ { p } , \theta _ { 1 } , \ldots , \theta _ { q } , \sigma _ { w } ^ { 2 }$ in the specification (5.60). Retain ARMA coefficients for use in Step (ii) and the fitted residuals $\widehat { w } _ { t }$ for use in Step (iii).

(ii) Apply the operator determined in Step (i), that is,

$$
\widehat {\phi} (B) y _ {t} = \widehat {\theta} (B) \tilde {y} _ {t},
$$

to determine the transformed output series $\bar { y } _ { t }$ .

(iii) Use the cross-correlation function between $\bar { y } _ { t }$ and $\widehat { w } _ { t }$ in (i) and (ii) to suggest a form for the components of the polynomial

$$
\alpha (B) = \frac {\delta (B) B ^ {d}}{\omega (B)}
$$

and the estimated time delay $d$ .

(iv) Obtain $\widehat { \pmb { \beta } } = ( \widehat { \omega } _ { 1 } , \ldots , \widehat { \omega } _ { r } , \widehat { \delta } _ { 0 } , \widehat { \delta } _ { 1 } , \ldots , \widehat { \delta } _ { s } )$ by fitting a linear regression of the form (5.66). Retain the residuals $\widehat { u } _ { t }$ for use in Step (v).

(v) Apply the moving average transformation (5.67) to the residuals $\widehat { u } _ { t }$ to find the noise series $\widehat { \eta _ { t } }$ , and fit an ARMA model to the noise, obtaining the estimated coefficients in $\widehat { \phi } _ { \eta } ( B )$ and $\widehat { \theta } _ { \eta } ( B )$ .

The above procedure is fairly reasonable, but does not have any recognizable overall optimality. Simultaneous least squares estimation, based on the observed $x _ { t }$ and $y _ { t }$ , can be accomplished by noting that the transfer function model can be written as

$$
y _ {t} = \frac {\delta (B) B ^ {d}}{\omega (B)} x _ {t} + \frac {\theta_ {\eta} (B)}{\phi_ {\eta} (B)} z _ {t},
$$

which can be put in the form

$$
\omega (B) \phi_ {\eta} (B) y _ {t} = \phi_ {\eta} (B) \delta (B) B ^ {d} x _ {t} + \omega (B) \theta_ {\eta} (B) z _ {t}, \tag {5.68}
$$

and it is clear that we may use least squares to minimize $\textstyle \sum _ { t } z _ { t } ^ { 2 }$ , as in earlier sections. We may also express the transfer function in state-space form (see Brockwell and Davis, 1991, Chapter 12). It is often easier to fit a transfer function model in the spectral domain as presented in §4.10.

# Example 5.8 Transfer Function Model for the SOI and Recruitment Series

We illustrate the procedure for fitting a transfer function model of the form suggested in Example 5.7 to the detrended SOI series ( $x _ { t }$ ) and the detrended Recruitment series ( $y _ { t }$ ). The results reported here can be

![](images/9c8a72b9ecb6141f0d849422d347c013b67e37b96ba743539fd7bd373e5879db.jpg)

![](images/4e5c40919a1f869aa4378620fce0b06d9fdf28dd900ce9f6da2235a858c86d13.jpg)  
Figure 5.13 ACF and PACF of the estimated noise $\widehat { \eta _ { t } }$ departures from the transfer function model.

compared with the results obtained from the frequency domain approach used in Example 4.23. Note first that Steps (i)-(iii). have already been applied to determine the ARMA model

$$
(1 -. 5 8 9 B) x _ {t} = w _ {t},
$$

where $\widehat { \sigma } _ { w } ^ { 2 } = . 0 9 2$ . Using the model determined in Example 5.7, we run the regression

$$
y _ {t} = \omega_ {1} y _ {t - 1} + \delta_ {0} x _ {t - 5} + u _ {t},
$$

yielding $\widehat { \omega } _ { 1 } = . 8 4 8 , \widehat { \delta } _ { 0 } = - 2 0 . 5 4$ , where the residuals satisfy

$$
\widehat {u} _ {t} = (1 -. 8 4 8 B) \eta_ {t}.
$$

This completes Step (iv). To complete the specification, we apply the moving average operator above to estimate the original noise series $\eta _ { t }$ and fit a second-order autoregressive model, based on the ACF and PACF shown in Figure 5.13. We obtain

$$
(1 - 1. 2 5 5 B +. 4 1 0 B ^ {2}) \eta_ {t} = z _ {t},
$$

with $\widehat { \sigma } _ { z } ^ { 2 } = 5 2 . 4 6$ as the estimated error variance.

# 5.7 Multivariate ARMAX Models

To understand multivariate time series models and their capabilities, we first present an introduction to multivariate time series regression techniques. A useful extension of the basic univariate regression model presented in §2.2 is the case in which we have more than one output series, that is, multivariate regression analysis. Suppose, instead of a single output variable $y _ { t }$ , a collection of $k$ output variables $y _ { t 1 } , y _ { t 2 } , \ldots , y _ { t k }$ exist that are related to the inputs as

$$
y _ {t i} = \beta_ {i 1} z _ {t 1} + \beta_ {i 2} z _ {t 2} + \dots + \beta_ {i r} z _ {t r} + w _ {t i} \tag {5.69}
$$

for each of the $i = 1 , 2 , \ldots , k$ output variables. We assume the $w _ { t i }$ variables are correlated over the variable identifier $i$ , but are still independent over time. Formally, we assume $\mathrm { c o v } \{ w _ { s i } , w _ { t j } \} = \sigma _ { i j }$ for $s = t$ and is zero otherwise. Then, writing (5.69) in matrix notation, with $\pmb { y } _ { t } = ( y _ { t 1 } , y _ { t 2 } , \ldots , y _ { t k } ) ^ { \prime }$ being the vector of outputs, and $B = \{ \beta _ { i j } \} , i = 1 , \ldots , k$ , $j = 1 , \dots , r$ being an $k \times r$ matrix containing the regression coefficients, leads to the simple looking form

$$
\boldsymbol {y} _ {t} = \mathcal {B} \boldsymbol {z} _ {t} + \boldsymbol {w} _ {t}. \tag {5.70}
$$

Here, the $k \times 1$ vector process ${ \pmb w } _ { t }$ is assumed to be a collection of independent vectors with common covariance matrix $E \{ { \pmb w } _ { t } { \pmb w } _ { t } ^ { \prime } \} = \Sigma _ { w }$ , the $k \times k$ matrix containing the covariances $\sigma _ { i j }$ . The maximum likelihood estimator, under the assumption of normality, for the regression matrix in this case is

$$
\widehat {\mathcal {B}} = Y ^ {\prime} Z \left(Z ^ {\prime} Z\right) ^ {- 1}, \tag {5.71}
$$

where $Z ^ { \prime } = [ z _ { 1 } , z _ { 2 } , \ldots , z _ { n } ]$ is as before and $Y ^ { \prime } = [ { \pmb y } _ { 1 } , { \pmb y } _ { 2 } , \ldots , { \pmb y } _ { n } ]$ . The error covariance matrix $\Sigma _ { w }$ is estimated by

$$
\widehat {\Sigma} _ {w} = \frac {1}{(n - r)} \sum_ {t = 1} ^ {n} \left(\boldsymbol {y} _ {t} - \widehat {\boldsymbol {\mathcal {B}}} \boldsymbol {z} _ {t}\right) \left(\boldsymbol {y} _ {t} - \widehat {\boldsymbol {\mathcal {B}}} \boldsymbol {z} _ {t}\right) ^ {\prime}. \tag {5.72}
$$

The uncertainty in the estimators can be evaluated from

$$
\operatorname {s e} \left(\widehat {\beta} _ {i j}\right) = \sqrt {\widehat {\sigma} _ {j j} c _ {i i}}, \tag {5.73}
$$

for $i = 1 , \ldots , r$ , $j = 1 , \ldots , k$ , where se denotes estimated standard error, $\widehat { \sigma } _ { j j }$ is the $\textstyle { \bigl ( } \sum _ { t = 1 } ^ { n } z _ { t } z _ { t } ^ { \prime } { \bigr ) } ^ { - 1 }$ $j$ -th diagonal element of . $\widehat { \Sigma } _ { w }$ , and $c _ { i i }$ is the $i$ -th diagonal element of

Also, the information theoretic criterion changes to

$$
\mathrm {A I C} = \ln | \widehat {\Sigma} _ {w} | + \frac {2}{n} (k r + \frac {k (k + 1)}{2}). \tag {5.74}
$$

and SIC replaces the second term in (5.74) by $K \ln n / n$ where $\begin{array} { r } { K = k r + k ( k + \mathbf { \check { \rho } } } \end{array}$ $1 ) / 2$ . Bedrick and Tsai (1994) have given a corrected form for AIC in the multivariate case as

$$
\mathrm {A I C c} = \ln | \widehat {\Sigma} _ {w} | + \frac {k (r + n)}{n - k - r - 1}. \tag {5.75}
$$

Many data sets involve more than one time series, and we are often interested in the possible dynamics relating all series. In this situation, we are interested in modeling and forecasting $k \times 1$ vector-valued time series $\pmb { x } _ { t } = ( x _ { t 1 } , \ldots , x _ { t k } ) ^ { \prime }$ , $t = 0 , \pm 1 , \pm 2 , . . .$ . Unfortunately, extending univariate ARMA models to the multivariate case is not so simple. The multivariate autoregressive model, however, is a straight-forward extension of the univariate AR model.

For the first-order vector autoregressive model, VAR(1), we take

$$
\boldsymbol {x} _ {t} = \boldsymbol {\alpha} + \Phi \boldsymbol {x} _ {t - 1} + \boldsymbol {w} _ {t}, \tag {5.76}
$$

where $\Phi$ is a $k \times k$ transition matrix that expresses the dependence of ${ \pmb x } _ { t }$ on ${ \pmb x } _ { t - 1 }$ . The vector white noise process ${ \pmb w } _ { t }$ is assumed to be multivariate normal with mean-zero and covariance matrix

$$
E \left(\boldsymbol {w} _ {t} \boldsymbol {w} _ {t} ^ {\prime}\right) = \boldsymbol {\Sigma} _ {w}. \tag {5.77}
$$

The vector ${ \pmb { \alpha } } = ( \alpha _ { 1 } , \alpha _ { 2 } , . . . , \alpha _ { k } ) ^ { \prime }$ appears as the constant in the regression setting. If $E ( \pmb { x } _ { t } ) = \pmb { \mu }$ , then ${ \pmb { \alpha } } = ( I - \Phi ) { \pmb { \mu } }$ .

Note the similarity between the VAR model and the multivariate linear regression model (5.70). The regression formulas carry over, and we can, on observing $\pmb { x } _ { 1 } , \ldots , \pmb { x } _ { n }$ , set up the model (5.76) with $\mathbf { \boldsymbol { y } } _ { t } = \mathbf { \boldsymbol { x } } _ { t }$ , $\boldsymbol { B } = ( { \pmb { \alpha } } , { \Phi } )$ and $z _ { t } =$ $( 1 , \pmb { x } _ { t - 1 } ^ { \prime } ) ^ { \prime }$ . Then, write the solution as (5.71) with the conditional maximum likelihood estimator for the covariance matrix given by

$$
\widehat {\Sigma} _ {w} = (n - 1) ^ {- 1} \sum_ {t = 2} ^ {n} \left(\boldsymbol {x} _ {t} - \widehat {\boldsymbol {\alpha}} - \widehat {\Phi} \boldsymbol {x} _ {t - 1}\right) \left(\boldsymbol {x} _ {t} - \widehat {\boldsymbol {\alpha}} - \widehat {\Phi} \boldsymbol {x} _ {t - 1}\right) ^ {\prime}. \tag {5.78}
$$

# Example 5.9 Pollution, Weather, and Mortality

For example, for the three-dimensional series composed of detrended cardiovascular mortality $x _ { t 1 }$ , temperature $x _ { t 2 }$ , and particulate levels $x _ { t 3 }$ , introduced in Example 2.2, take $\mathbf { \mathscr { x } } _ { t } = (  { \boldsymbol { { x } } } _ { t 1 } ,  { \boldsymbol { { x } } } _ { t 2 } ,  { \boldsymbol { { x } } } _ { t 3 } ) ^ { \prime }$ as a vector of dimension $k = 3$ . We might envision dynamic relations among the three series defined as the first order relation,

$$
x _ {t 1} = \alpha_ {1} + \phi_ {1 1} x _ {t - 1, 1} + \phi_ {1 2} x _ {t - 1, 2} + \phi_ {1 3} x _ {t - 1, 3} + w _ {t 1},
$$

which expresses the current value of mortality as a linear combination of its immediate past value and the past values of temperature and particulate levels. Similarly,

$$
x _ {t 2} = \alpha_ {2} + \phi_ {2 1} x _ {t - 1, 1} + \phi_ {2 2} x _ {t - 1, 2} + \phi_ {2 3} x _ {t - 1, 3} + w _ {t 2}
$$

and

$$
x _ {t 3} = \alpha_ {3} + \phi_ {3 1} x _ {t - 1, 1} + \phi_ {3 2} x _ {t - 1, 2} + \phi_ {3 3} x _ {t - 1, 3} + w _ {t 3}
$$

express the dependence of temperature and particulate levels on the other series. Of course, methods for the preliminary identification of these models exist, and we will discuss these methods shortly.

For this particular case, we obtain $\widehat { \pmb { \alpha } } = ( - 4 . 5 7 , 6 . 0 9 , 1 9 . 7 8 ) ^ { \prime }$ and

$$
\widehat {\Phi} = \left( \begin{array}{c c c} . 4 7 (. 0 4) & -. 3 6 (. 0 3) & . 1 0 (. 0 2) \\ -. 2 4 (. 0 4) & . 4 9 (. 0 4) & -. 1 3 (. 0 2) \\ -. 1 3 (. 0 8) & -. 4 8 (. 0 7) & . 5 8 (. 0 4) \end{array} \right),
$$

where the standard errors, computed as in (5.73), are given in parentheses. Hence, for the vector $( x _ { t 1 } , x _ { t 2 } , x _ { t 3 } ) = ( M _ { t } , T _ { t } , P _ { t } )$ , with $M _ { t } , T _ { t }$ and $P _ { t }$ denoting mortality, temperature, and particulate level, respectively, we obtain the prediction equation for mortality,

$$
\widehat {M} _ {t} = - 4. 5 7 +. 4 7 M _ {t - 1} -. 3 6 T _ {t - 1} +. 1 0 P _ {t - 1}.
$$

Comparing observed and predicted mortality with this model leads to an $R ^ { 2 }$ of about .78, whereas the value in the regression model fitted by the method of Example 2.2 gave an $R ^ { 2 } = . 6 9$ .

It is easy to extend the VAR(1) process to higher orders, VAR(p). To do this, we use the notation of (5.70) and write the vector of regressors as

$$
\boldsymbol {z} _ {t} = \left(1, \boldsymbol {x} _ {t - 1} ^ {\prime}, \boldsymbol {x} _ {t - 2} ^ {\prime}, \dots \boldsymbol {x} _ {t - p} ^ {\prime}\right) ^ {\prime}
$$

and the regression matrix as $\boldsymbol { B } = ( \alpha , \Phi _ { 1 } , \Phi _ { 2 } , \ldots , \Phi _ { p } )$ . Then, this regression model can be written as

$$
\boldsymbol {x} _ {t} = \boldsymbol {\alpha} + \sum_ {j = 1} ^ {p} \Phi_ {j} \boldsymbol {x} _ {t - j} + \boldsymbol {w} _ {t} \tag {5.79}
$$

for $t = p + 1 , \ldots , n$ . The $k \times k$ error sum of products matrix becomes

$$
R S P = \sum_ {t = p + 1} ^ {n} \left(\boldsymbol {x} _ {t} - \mathcal {B} \boldsymbol {z} _ {t}\right) \left(\boldsymbol {x} _ {t} - \mathcal {B} \boldsymbol {z} _ {t}\right) ^ {\prime}, \tag {5.80}
$$

so that the conditional maximum likelihood estimator for the error covariance matrix $\Sigma _ { w }$ is

$$
\widehat {\Sigma} _ {w} = R S P / (n - p), \tag {5.81}
$$

as in the multivariate regression case, except now only $n - p$ residuals exist in (5.80). For the multivariate case, we have found that the Schwarz criterion

$$
\operatorname {S I C} = \log | \widehat {\Sigma} _ {w} | + k ^ {2} p \ln n / n, \tag {5.82}
$$

gives more reasonable classifications than either AIC or corrected version AICc. The result is consistent with those reported in simulations by L¨utkepohl (1985).

Table 5.1 Summary Statistics for Example 5.10   

<table><tr><td>Order (p)</td><td>k2p</td><td>|∑w|</td><td>SIC</td><td>AICc</td></tr><tr><td>1</td><td>505</td><td>118,520</td><td>11.79</td><td>14.71</td></tr><tr><td>2</td><td>503</td><td>74,708</td><td>11.44</td><td>14.26</td></tr><tr><td>3</td><td>501</td><td>70,146</td><td>11.49</td><td>14.21</td></tr><tr><td>4</td><td>499</td><td>65,268</td><td>11.53</td><td>14.15</td></tr><tr><td>5</td><td>497</td><td>59,684</td><td>11.55</td><td>14.08</td></tr></table>

# Example 5.10 Mortality, Pollution and Temperature Data

A trivariate AR(2) model for the data in Example 5.9 yields

$$
\begin{array}{l} \widehat {\Phi} _ {1} = \left( \begin{array}{c c c} . 3 0 (. 0 4) & -. 2 0 (. 0 4) & . 0 4 (. 0 2) \\ -. 1 1 (. 0 5) & . 2 6 (. 0 5) & -. 0 5 (. 0 3) \\ . 0 8 (. 0 9) & -. 3 9 (. 0 9) & . 3 9 (. 0 5) \end{array} \right), \\ \widehat {\Phi} _ {2} = \left( \begin{array}{c c c} . 2 8 (. 0 4) & -. 0 8 (. 0 4) & . 0 7 (. 0 3) \\ -. 0 4 (. 0 5) & . 3 6 (. 0 5) & -. 0 9 (. 0 3) \\ -. 3 3 (. 0 9) & . 0 5 (. 0 9) & . 3 8 (. 0 5) \end{array} \right). \\ \end{array}
$$

In Table 5.1, fitting successively higher order models beyond $p = 2$ does not improve the value of SIC, and we would tend to settle on the secondorder model. Note that the value of AICc continues to decrease as the model order increases.

A $k \times 1$ vector-valued time series ${ \pmb x } _ { t }$ , for $t = 0 , \pm 1 , \pm 2 , . . .$ , is said to be VARMA $( p , q )$ if ${ \pmb x } _ { t }$ is stationary and

$$
\boldsymbol {x} _ {t} = \boldsymbol {\alpha} + \Phi_ {1} \boldsymbol {x} _ {t - 1} + \dots + \Phi_ {p} \boldsymbol {x} _ {t - p} + \boldsymbol {w} _ {t} + \Theta_ {1} \boldsymbol {w} _ {t - 1} + \dots + \Theta_ {q} \boldsymbol {w} _ {t - q}, \tag {5.83}
$$

with $\Phi _ { p } \ne 0$ , $\Theta _ { q } \neq 0$ , and $\Sigma _ { w } ~ > ~ 0$ (that is, $\Sigma _ { w }$ is positive definite). The coefficient matrices $\Phi _ { j }$ ; $j = 1 , . . . , p$ and $\Theta _ { j }$ ; $j = 1 , . . . , q$ are, of course, $p \times p$ matrices. If $\pmb { x } _ { t }$ has mean $\pmb { \mu }$ then $\pmb { \alpha } = ( I - \Phi _ { 1 } - \cdot \cdot \cdot - \Phi _ { p } ) \pmb { \mu }$ . As in the univariate case, we will have to place a number of conditions on the multivariate ARMA model to ensure the model is unique and has desirable properties such as causality. These conditions will be discussed shortly.

The special form assumed for the constant component, $\pmb { \alpha }$ , of the vector ARMA model in (5.83) can be generalized to include a fixed $r \times 1$ vector of inputs, ${ \pmb u } _ { t }$ . That is, we could have proposed the vector ARMAX model,

$$
\boldsymbol {x} _ {t} = \Gamma \boldsymbol {u} _ {t} + \sum_ {j = 1} ^ {p} \Phi_ {j} \boldsymbol {x} _ {t - j} + \sum_ {k = 1} ^ {q} \Theta_ {k} \boldsymbol {w} _ {t - k} + \boldsymbol {w} _ {t}, \tag {5.84}
$$

where $\Gamma$ is a $p \times r$ parameter matrix. The X in ARMAX refers to the exogenous vector process we have denoted here by ${ \pmb u } _ { t }$ . The introduction of exogenous

variables through replacing $\pmb { \alpha }$ by $\Gamma { \mathbf { } } { \mathbf { } } _ { } { \mathbf { } } _ { }$ does not present any special problems in making inferences. For example, the case of the ARX model, that is, $q = 0$ in (5.84), can be estimated using standard regression results. In this case, the model can be written as a multivariate regression model in which the vector of regressors are

$$
\boldsymbol {z} _ {t} = \left(\boldsymbol {u} _ {t} ^ {\prime}, \boldsymbol {x} _ {t - 1} ^ {\prime}, \dots , \boldsymbol {x} _ {t - p} ^ {\prime}\right) ^ {\prime} \tag {5.85}
$$

and the new regression matrix is

$$
\mathcal {B} = \left[ \Gamma , \Phi_ {1}, \Phi_ {2}, \dots , \Phi_ {p} \right]. \tag {5.86}
$$

The general VARMA model, (5.83), is a special case of the vector ARMAX model, (5.84), with $r = 1$ , $\mathbf { \nabla } { \pmb u } _ { t } = 1$ , and $\Gamma = \alpha$ .

As previously indicated, extending univariate AR (or pure MA) models to the vector case is fairly easy, but extending univariate ARMA models to the multivariate case is not a simple matter. Our discussion will be brief, but interested readers can get more details in L¨utkepohl (1993), Reinsel (1997), and Tiao and Tsay (1989).

In the multivariate case, the autoregressive operator is

$$
\Phi (B) = I - \Phi_ {1} B - \dots - \Phi_ {p} B ^ {p}, \tag {5.87}
$$

and the moving average operator is

$$
\Theta (B) = I + \Theta_ {1} B + \dots + \Theta_ {q} B ^ {q}, \tag {5.88}
$$

The zero-mean VARMA $( p , q )$ model is then written in the concise form as

$$
\Phi (B) \boldsymbol {x} _ {t} = \Theta (B) \boldsymbol {w} _ {t}. \tag {5.89}
$$

The model is said to be causal if the roots of $| \Phi ( z ) |$ (where $| \cdot |$ denotes determinant) are outside the unit circle, $| z | > 1$ ; that is, $| \Phi ( z ) | \neq 0$ for any value $z$ such that $| z | \le 1$ . In this case, we can write

$$
\boldsymbol {x} _ {t} = \Psi (B) \boldsymbol {w} _ {t},
$$

where $\begin{array} { r } { \Psi ( B ) = \sum _ { j = 0 } ^ { \infty } \Psi _ { j } B ^ { j } } \end{array}$ , $\Psi _ { 0 } = I$ , and $\textstyle \sum _ { j = 0 } ^ { \infty } \vert \vert \Psi _ { j } \vert \vert < \infty$ . The model is said to be invertible if the roots of $| \Theta ( z ) |$ lie outside the unit circle. Then, we can write

$$
\boldsymbol {w} _ {t} = \Pi (B) \boldsymbol {x} _ {t},
$$

where $\begin{array} { r } { \Pi ( B ) = \sum _ { j = 0 } ^ { \infty } \Pi _ { j } B ^ { j } } \end{array}$ , $\Pi _ { 0 } ~ = ~ I$ , and $\textstyle \sum _ { j = 0 } ^ { \infty } \vert \vert \Pi _ { j } \vert \vert < \infty$ . Analogous to the univariate case, we can determine the matrices $\Psi _ { j }$ by solving $\Psi ( z ) =$ $\Phi ( z ) ^ { - 1 } \Theta ( z ) , | z | \le 1$ , and the matrices $\Pi _ { j }$ by solving $\Pi ( z ) = \Theta ( z ) ^ { - 1 } \Phi ( z ) , | z | \le$ 1.

For a causal model, we can write $\pmb { x } _ { t } = \Psi ( B ) \pmb { w } _ { t }$ so the general autocovariance structure of an ARMA $( p , q )$ model is

$$
\Gamma (h) = \operatorname {c o v} \left(\boldsymbol {x} _ {t + h}, \boldsymbol {x} _ {t}\right) = E \left(\boldsymbol {x} _ {t + h} \boldsymbol {x} _ {t} ^ {\prime}\right) = \sum_ {j = 0} ^ {\infty} \Psi_ {j + h} \Sigma_ {w} \Psi_ {j} ^ {\prime}. \tag {5.90}
$$

# 5.7: Multivariate ARMAX

Note, $\Gamma ( - h ) = \Gamma ^ { \prime } ( h )$ so we will only exhibit the autocovariances for $h \geq 0$ . For pure MA(q) processes, (5.90) becomes

$$
\Gamma (h) = \sum_ {j = 0} ^ {q - h} \Theta_ {j + h} \Sigma_ {w} \Theta_ {j} ^ {\prime}, \tag {5.91}
$$

where $\Theta _ { 0 } = I$ . Of course, (5.91) implies $\Gamma ( h ) = 0$ for $h > q$ . For pure $\operatorname { A R } ( p )$ models, the autocovariance structure leads to the multivariate version of the Yule–Walker equations:

$$
\Gamma (h) = \sum_ {j = 1} ^ {p} \Phi_ {j} \Gamma (h - j), \quad h = 1, 2, \dots , \tag {5.92}
$$

$$
\Gamma (0) = \sum_ {j = 1} ^ {p} \Phi_ {j} \Gamma (- j) + \Sigma_ {w}. \tag {5.93}
$$

As in the univariate case, we will need conditions for model uniqueness. These conditions are similar to the condition in the univariate case the the autoregressive and moving average polynomials have no common factors. To explore the uniqueness problems that we encounter with multivariate ARMA models, consider a bivariate AR(1) process, $\pmb { x } _ { t } = ( x _ { t , 1 } , x _ { t , 2 } ) ^ { \prime }$ , given by

$$
\begin{array}{l} x _ {t, 1} = \phi x _ {t - 1, 2} + w _ {t, 1}, \\ x _ {t, 2} = w _ {t, 2}, \\ \end{array}
$$

where $w _ { t , 1 }$ and $w _ { t , 2 }$ are independent white noise processes and $| \phi | < 1$ . Both processes, $x _ { t , 1 }$ and $x _ { t , 2 }$ are causal and invertible. Moreover, the processes are jointly stationary because $\mathrm { c o v } ( x _ { t + h , 1 } , x _ { t , 2 } ) = \phi$ $( x _ { t + h , 1 } , x _ { t , 2 } ) = \phi \operatorname { c o v } ( x _ { t + h - 1 , 2 } , x _ { t , 2 } ) \equiv \phi \gamma _ { 2 , 2 } ( h - $ $1 ) = \phi \sigma _ { w _ { 2 } } ^ { 2 } \delta _ { 1 } ^ { h }$ does not depend on $t$ ; note, $\delta _ { 1 } ^ { h } = 1$ when $h = 1$ , otherwise, $\delta _ { 1 } ^ { h } = 0$ . In matrix notation, we can write this model as

$$
\boldsymbol {x} _ {t} = \Phi \boldsymbol {x} _ {t - 1} + \boldsymbol {w} _ {t}, \tag {5.94}
$$

where

$$
\Phi = \left[ \begin{array}{c c} 0 & \phi \\ 0 & 0 \end{array} \right].
$$

We can write (5.94) in operator notation as

$$
\Phi (B) \boldsymbol {x} _ {t} = \boldsymbol {w} _ {t}
$$

where

$$
\Phi (z) = \left[ \begin{array}{c c} 1 & - \phi z \\ 0 & 1 \end{array} \right].
$$

In addition, model (5.94) can be written as a bivariate ARMA(1,1) model

$$
\boldsymbol {x} _ {t} = \Phi_ {1} \boldsymbol {x} _ {t - 1} + \Theta_ {1} \boldsymbol {w} _ {t - 1} + \boldsymbol {w} _ {t}, \tag {5.95}
$$

where

$$
\Phi_ {1} = \left[ \begin{array}{c c} 0 & \phi + \theta \\ 0 & 0 \end{array} \right] \quad \text {a n d} \quad \Theta_ {1} = \left[ \begin{array}{c c} 0 & - \theta \\ 0 & 0 \end{array} \right],
$$

and $\theta$ is arbitrary. To verify this, we write (5.95), as $\Phi _ { 1 } ( B ) \pmb { x } _ { t } = \Theta _ { 1 } ( B ) \pmb { w } _ { t }$ , or

$$
\Theta_ {1} (B) ^ {- 1} \Phi_ {1} (B) \boldsymbol {x} _ {t} = \boldsymbol {w} _ {t},
$$

where

$$
\Phi_ {1} (z) = \left[ \begin{array}{c c} 1 & - (\phi + \theta) z \\ 0 & 1 \end{array} \right] \quad \mathrm {a n d} \quad \Theta_ {1} (z) = \left[ \begin{array}{c c} 1 & - \theta z \\ 0 & 1 \end{array} \right].
$$

Then,

$$
\Theta_ {1} (z) ^ {- 1} \Phi_ {1} (z) = \left[ \begin{array}{c c} 1 & \theta z \\ 0 & 1 \end{array} \right] \left[ \begin{array}{c c} 1 & - (\phi + \theta) z \\ 0 & 1 \end{array} \right] = \left[ \begin{array}{c c} 1 & - \phi z \\ 0 & 1 \end{array} \right] = \Phi (z),
$$

where $\Phi ( z )$ is the polynomial associated with the bivariate AR(1) model in (5.94). Because $\theta$ is arbitrary, the parameters of the ARMA(1,1) model given in (5.95) are not identifiable. No problem exists, however, in fitting the AR(1) model given in (5.94).

The problem in the previous discussion was caused by the fact that both $\Theta ( B )$ and $\Theta ( B ) ^ { - 1 }$ are finite; such a matrix operator is called unimodular. If $U ( B )$ is unimodular, $| U ( z ) |$ is constant. It is also possible for two seemingly different multivariate $\mathrm { A R M A } ( p , q )$ $( p , q )$ models, say, $\Phi ( B ) \pmb { x } _ { t } = \Theta ( B ) \pmb { w } _ { t }$ and $\Phi _ { * } ( B ) \pmb { x } _ { t } = \Theta _ { * } ( B ) \pmb { w } _ { t }$ , to be related through a unimodular operator, $U ( B )$ as $\Phi _ { * } ( B ) = U ( B ) \Phi ( B )$ and $\Theta _ { * } ( B ) = U ( B ) \Theta ( B )$ , in such a way that the orders of $\Phi ( B )$ and $\Theta ( B )$ are the same as the orders of $\Phi _ { * } ( B )$ and $\Theta _ { * } ( B )$ , respectively. For example, consider the bivariate ARMA(1,1) models given by

$$
\Phi \boldsymbol {x} _ {t} \equiv \left[ \begin{array}{c c} 1 & - \phi B \\ 0 & 1 \end{array} \right] \boldsymbol {x} _ {t} = \left[ \begin{array}{c c} 1 & \theta B \\ 0 & 1 \end{array} \right] \boldsymbol {w} _ {t} \equiv \Theta w _ {t}
$$

and

$$
\Phi_ {*} (B) \boldsymbol {x} _ {t} \equiv \left[ \begin{array}{c c} 1 & (\alpha - \phi) B \\ 0 & 1 \end{array} \right] \boldsymbol {x} _ {t} = \left[ \begin{array}{c c} 1 & (\alpha + \theta) B \\ 0 & 1 \end{array} \right] \boldsymbol {w} _ {t} \equiv \Theta_ {*} (B) \boldsymbol {w} _ {t},
$$

where $\alpha$ , $\phi$ , and $\theta$ are arbitrary constants. Note,

$$
\Phi_ {*} (B) \equiv \left[ \begin{array}{c c} 1 & (\alpha - \phi) B \\ 0 & 1 \end{array} \right] = \left[ \begin{array}{c c} 1 & \alpha B \\ 0 & 1 \end{array} \right] \left[ \begin{array}{c c} 1 & - \phi B \\ 0 & 1 \end{array} \right] \equiv U (B) \Phi (B)
$$

and

$$
\Theta_ {*} (B) \equiv \left[ \begin{array}{c c} 1 & (\alpha + \theta) B \\ 0 & 1 \end{array} \right] = \left[ \begin{array}{c c} 1 & \alpha B \\ 0 & 1 \end{array} \right] \left[ \begin{array}{c c} 1 & \theta B \\ 0 & 1 \end{array} \right] \equiv U (B) \Theta (B).
$$

In this case, both models have the same infinite MA representation $\begin{array} { r l } { \pmb { x } _ { t } } & { { } = } \end{array}$ $\Psi ( B ) { \pmb w } _ { t }$ , where

$$
\Psi (B) = \Phi (B) ^ {- 1} \Theta (B) = \Phi (B) ^ {- 1} U (B) ^ {- 1} U (B) \Theta (B) = \Phi_ {*} (B) ^ {- 1} \Theta_ {*} (B).
$$

This result implies the two models have the same autocovariance function $\Gamma ( h )$ Two such ARMA $( p , q )$ models are said to be observationally equivalent.

As previously mentioned, in addition to requiring causality and invertiblity, we will need some additional assumptions in the multivariate case to make sure that the model is unique. To ensure the identifiability of the parameters of the multivariate $\mathrm { A R M A } ( p , q )$ model, we need the following additional two conditions: (i) the matrix operators $\Phi ( B )$ and $\Theta ( B )$ have no common left factors other than unimodular ones; that is, if $\Phi ( B ) = U ( B ) \Phi _ { * } ( B )$ and $\Theta ( B ) =$ $U ( B ) \Theta _ { * } ( B )$ , the common factor must be unimodular; and (ii) with $q$ as small as possible and $p$ as small as possible for that $q$ , the matrix $[ \Phi _ { p } , \Theta _ { q } ]$ must be full rank, $k$ . One suggestion for avoiding most of the aforementioned problems is to fit only vector $\operatorname { A R } ( p )$ models in multivariate situations. Although this suggestion might be reasonable for many situations, this philosophy is not in accordance with law of parsimony because we might have to fit a large number of parameters to describe the dynamics of a process.

Analogous to the univariate case, we can define a sequence of matrices, $\Phi _ { h h }$ , for $h = 1 , 2 , \ldots$ , called the partial autoregression matrices at lag $h$ . These matrices are obtained by solving the Yule–Walker equations of order $h$ , namely,

$$
\Gamma (\ell) = \sum_ {j = 1} ^ {h} \Phi_ {j h} \Gamma (\ell - j), \quad \ell = 1, 2, \dots , h. \tag {5.96}
$$

The partial autoregression matrices can be viewed as the result of successive $\operatorname { A R } ( h )$ fits to the data; that is,

$$
\boldsymbol {x} _ {t} = \sum_ {j = 1} ^ {h} \Phi_ {j h} \boldsymbol {x} _ {t - j} + \boldsymbol {w} _ {t}, \quad h = 1, 2, \dots . \tag {5.97}
$$

If the process is truly an AR(p), the partial autoregression matrices have the property that $\Phi _ { p p } = \Phi _ { p }$ and $\Phi _ { h h } = 0$ for $h > p$ . Unlike the univariate case, however, the elements of these matrices are not partial correlations, or correlations of any kind. As in the univariate case, the $\Phi _ { h h }$ can be obtained iteratively using a multivariate extension of the Durbin-Levinson algorithm; details can be found in Reinsel (1997).

The partial canonical correlations can be viewed as the multivariate extension of the PACF in the univariate case. In general, the first canonical correlation, $\lambda _ { 1 }$ , between the $k _ { 1 } \times 1$ random vector $X _ { 1 }$ and the $k _ { 2 } \times 1$ random vector $X _ { 2 }$ , $k _ { 1 } \leq k _ { 2 }$ , with variance–covariance matrices $\Sigma _ { 1 1 }$ and $\Sigma _ { 2 2 }$ , respectively, is the largest possible correlation between a linear combination of the components of $X _ { 1 }$ , say, $\pmb { \alpha } ^ { \prime } \pmb { X } _ { 1 }$ , and a linear combination of the components of $X _ { 2 }$ , say, $\beta ^ { \prime } X _ { 2 }$ , where $\pmb { \alpha }$ is $k _ { 1 } \times 1$ and $\beta$ is $k _ { 2 } \times 1$ . That is,

$$
\lambda_ {1} = \max  _ {\boldsymbol {\alpha}, \boldsymbol {\beta}} \operatorname {c o r r} \left(\boldsymbol {\alpha} ^ {\prime} \boldsymbol {X} _ {1}, \boldsymbol {\beta} ^ {\prime} \boldsymbol {X} _ {2}\right),
$$

subject to the constraints $\mathrm { v a r } ( { \pmb \alpha } ^ { \prime } { \pmb X } _ { 1 } ) = { \pmb \alpha } ^ { \prime } \Sigma _ { 1 1 } { \pmb \alpha } = 1$ and $\operatorname { v a r } ( \beta ^ { \prime } X _ { 2 } ) = \beta ^ { \prime } \Sigma _ { 2 2 } \beta =$ 1. If we let $\Sigma _ { i j } = \operatorname { c o v } ( { \pmb X } _ { i } , { \pmb X } _ { j } )$ , for $i , j = 1 , 2$ , then $\lambda _ { 1 } ^ { 2 }$ is the largest eigenvalue

of the matrix $\Sigma _ { 1 1 } ^ { - 1 } \Sigma _ { 1 2 } \Sigma _ { 2 2 } ^ { - 1 } \Sigma _ { 2 1 }$ ; see Johnson and Wichern (1992, Chapter 10) for details. We call the solutions $U _ { 1 } = \pmb { \alpha } _ { 1 } ^ { \prime } \pmb { X } _ { 1 }$ and $V _ { 1 } = \beta _ { 1 } ^ { \prime } X _ { 2 }$ the first canonical variates, that is, $\lambda _ { 1 } ~ = ~ \mathrm { c o r r } ( U _ { 1 } , V _ { 1 } )$ , and $\pmb { \alpha } _ { 1 }$ and $\beta _ { 1 }$ are the coefficients of the linear combinations that maximize the correlation. In a similar fashion, the second canonical correlation, $\lambda _ { 2 }$ , is then the largest possible correlation between $\pmb { \alpha } ^ { \prime } \pmb { X } _ { 1 }$ and $\beta ^ { \prime } X _ { 2 }$ such that $\pmb { \alpha }$ is orthogonal to $\pmb { \alpha } _ { 1 }$ (that is, ${ \pmb { \alpha } } ^ { \prime } { \pmb { \alpha } } _ { 1 } = 0$ ), and $\beta$ is orthogonal to $\beta _ { 1 }$ ( $\beta ^ { \prime } \beta _ { 1 } = 0$ ) . If we call the solutions $U _ { 2 } = \pmb { \alpha } _ { 2 } ^ { \prime } \pmb { X } _ { 1 }$ and $V _ { 2 } =$ ${ \beta } _ { 2 } ^ { \prime } X _ { 2 }$ , then $\operatorname { c o r r } ( U _ { 1 } , U _ { 2 } ) = 0 = \operatorname { c o r r } ( V _ { 1 } , V _ { 2 } )$ , $\operatorname { c o r r } ( U _ { i } , V _ { j } ) = 0$ for $i \neq j$ , and by design, $\lambda _ { 1 } ^ { 2 } \geq \lambda _ { 2 } ^ { 2 }$ . Also, $\lambda _ { 2 } ^ { 2 }$ is the second largest eigenvalue of $\Sigma _ { 1 1 } ^ { - 1 } \Sigma _ { 1 2 } \Sigma _ { 2 2 } ^ { - 1 } \Sigma _ { 2 1 }$ . Continuing this way, we obtain the squared canonical correlations $1 \geq \lambda _ { 1 } ^ { 2 } \geq$ $\lambda _ { 2 } ^ { 2 } \geq \cdots \geq \lambda _ { k _ { 1 } } ^ { 2 } \geq 0$ as the ordered eigenvalues of $\Sigma _ { 1 1 } ^ { - 1 } \Sigma _ { 1 2 } \Sigma _ { 2 2 } ^ { - 1 } \Sigma _ { 2 1 }$ . The canonical correlations, $\lambda _ { j }$ , are typically taken to be nonnegative.

We can extend this idea to obtain partial canonical correlations between $X _ { 1 }$ and $X _ { 2 }$ given another random $k _ { 3 } \times 1$ vector $X _ { 3 }$ . Let $\Sigma _ { i j } = \operatorname { c o v } ( X _ { i } , X _ { j } )$ , for $i , j = 1 , 2 , 3$ . The regression of $X _ { 1 }$ on $X _ { 3 }$ is $\Sigma _ { 1 3 } \Sigma _ { 3 3 } ^ { - 1 } X _ { 3 }$ so that $x _ { 1 \vert 3 } =$ $X _ { 1 } - \Sigma _ { 1 3 } \Sigma _ { 3 3 } ^ { - 1 } X _ { 3 }$ can be thought of as $X _ { 1 }$ with the linear effects of $X _ { 3 }$ removed (partialled out). Similarly, $X _ { 2 | 3 } = X _ { 2 } - \Sigma _ { 2 3 } \Sigma _ { 3 3 } ^ { - 1 } X _ { 3 }$ can be thought of as $X _ { 2 }$ with the linear effects of $X _ { 3 }$ partialled out. The partial variance–covariance matrices are $\Sigma _ { i j | 3 } = \mathrm { c o v } ( X _ { i | 3 } , X _ { j | 3 } ) = \Sigma _ { i j } - \Sigma _ { i 3 } \Sigma _ { 3 3 } ^ { - 1 } \Sigma _ { 3 j }$ , for $i , j = 1 , 2$ . The ordered eigenvalues of squared partial canonical correlations between $\Sigma _ { 1 1 | 3 } ^ { - 1 } \Sigma _ { 1 2 | 3 } \Sigma _ { 2 2 | 3 } ^ { - 1 } \Sigma _ { 2 1 | 3 }$ . $X _ { 1 }$ and $X _ { 2 }$ given $X _ { 3 }$ are the

For a stationary vector process $\pmb { x } _ { t }$ , the partial canonical correlations at lag $h$ , for $h = 2 , 3 , \ldots$ , denoted $\lambda _ { 1 } ( h ) \ge \lambda _ { 2 } ( h ) \ge \cdots \ge \lambda _ { k } ( h ) \ge 0$ , are defined to be the partial canonical correlations between $_ { x h }$ and $\pmb { x } _ { 0 }$ with the effects of $\pmb { X } \ = \ ( \pmb { x } _ { h - 1 } ^ { \prime } , . . . , \pmb { x } _ { 1 } ^ { \prime } ) ^ { \prime }$ removed. For ease of notation, we put $r ~ = ~ h - 1$ . Letsym $\Sigma _ { 0 0 | X } = \Gamma ( 0 ) - \Gamma _ { 1 } ^ { ( r ) } \Gamma _ { r , r } ^ { - 1 } \Gamma _ { 1 } ^ { ( r ) ^ { \prime } }$ $\Gamma _ { r , r } = \{ \Gamma ( i - j ) \} _ { i , j = 1 } ^ { r }$ $k r \times k r$ $\Gamma _ { 1 } ^ { ( r ) } = [ \Gamma ( r ) ^ { \prime } , \Gamma ( r - 1 ) ^ { \prime } , . . . , \Gamma ( 1 ) ^ { \prime } ]$ $\boldsymbol { k } \times \boldsymbol { k r }$ let $\Sigma _ { h h | X } = \Gamma ( 0 ) - \Gamma _ { r } ^ { ( 1 ) } \Gamma _ { r , r } ^ { - 1 } \Gamma _ { r } ^ { ( 1 ) ^ { \prime } }$ Γ−r,r Γr , where $\Gamma _ { r } ^ { ( 1 ) } = [ \Gamma ( 1 ) , \Gamma ( 2 ) , . . . , \Gamma ( r ) ]$ is $k \times k r$ Also needed are Σh0|X = Γ(r) − Γ(1)r Γ−1r,r Γ(r)1 a $\Sigma _ { h 0 | X } = \Gamma ( r ) - \Gamma _ { r } ^ { ( 1 ) } \Gamma _ { r , r } ^ { - 1 } \Gamma _ { 1 } ^ { ( r ) ^ { \prime } }$ nd $\Sigma _ { 0 h | X } = \Sigma _ { h 0 | X } ^ { \prime }$ . The squared partial canonical correlations, $\lambda _ { j } ^ { 2 } ( h )$ , $j = 1 , . . . , k$ at lag $h$ , $h = 2 , 3 , \ldots$ , are given by the ordered eigenvalues of $\Sigma _ { 0 0 | X } ^ { - 1 } \Sigma _ { 0 h | X } \Sigma _ { h h | X } ^ { - 1 } \Sigma _ { h 0 | X }$ . The inversion of $\Gamma _ { r , r }$ when $h$ is large will, be a problem; see Reinsel (1997) for methods that avoid having to invert $\Gamma _ { r , r }$ . Finally, we will define the partial canonical correlations between ${ \pmb x } _ { t }$ and ${ \pmb x } _ { t - 1 }$ to be the lag-one canonical correlations. In this case, $\lambda _ { j } ^ { 2 } ( 1 )$ , $j = 1 , . . . , k$ are the ordered eigenvalues of $\Gamma ( 0 ) ^ { - 1 } \Gamma ( 1 ) \Gamma ( 0 ) ^ { - 1 } \Gamma ( 1 ) ^ { \prime }$ .

Prediction and estimation for identifiable multivariate ARMA models follow analogously to the univariate case, except in the general case, the estimation of the coefficient parameters and $\Sigma _ { w }$ must be done simultaneously. Preliminary identification of the model uses the sample autocovariance matrices, the sample partial autoregression matrices, and the sample partial canonical correlations. We illustrate the techniques using the mortality data of Examples 2.2, 5.9, and 5.10.

# Example 5.11 Identification, Estimation and Prediction for the Mortality Series

As in Example 5.10, we consider the trivariate series composed of detrended cardiovascular mortality $x _ { t 1 }$ , temperature $x _ { t 2 }$ , and particulate levels $x _ { t 3 }$ , and set $\pmb { x } _ { t } = ( x _ { t 1 } , x _ { t 2 } , x _ { t 3 } ) ^ { \prime }$ as the three-dimensional data vector.

Estimation of the autocovariance matrix is similar to the univariate case, that is, with $\begin{array} { r } { \bar { \pmb x } = n ^ { - 1 } \sum _ { t = 1 } ^ { n } \pmb x _ { t } } \end{array}$ n , as an estimate of ${ \pmb \mu } = E { \pmb x } _ { t }$ ,

$$
\widehat {\Gamma} (h) = n ^ {- 1} \sum_ {t = 1} ^ {n - h} \left(\boldsymbol {x} _ {t + h} - \bar {\boldsymbol {x}}\right) \left(\boldsymbol {x} _ {t} - \bar {\boldsymbol {x}}\right) ^ {\prime}, \quad h = 0, 1, 2,.., n - 1, \tag {5.98}
$$

and $\widehat \Gamma ( - h ) = \widehat \Gamma ( h ) ^ { \prime }$ . If $\widehat { \gamma } _ { i , j } ( h )$ denotes the element in the $i$ -th row and $j$ -th column of $\widehat { \Gamma } ( h )$ , the cross-correlation functions (CCF), as discussed in (1.35), are estimated by

$$
\widehat {\rho} _ {i, j} (h) = \frac {\widehat {\gamma} _ {i , j} (h)}{\sqrt {\widehat {\gamma} _ {i , i} (0)} \sqrt {\widehat {\gamma} _ {j , j} (0)}} \quad h = 0, 1, 2,.., n - 1. \tag {5.99}
$$

When $i \ = \ j$ in (5.99), we get the estimated autocorrelation function (ACF) of the individual series. The first six estimated autocovariance matrices, $\widehat { \Gamma } ( h )$ , $h = 0 , 1 , . . . , 5$ , are (we have rounded the entries to integers to ease the display):

$$
\begin{array}{l} \widehat {\Gamma} (0) = \left[ \begin{array}{c c c} 7 9 & - 3 7 & 6 2 \\ - 3 7 & 8 1 & - 2 \\ 6 2 & - 2 & 2 2 7 \end{array} \right] \quad \widehat {\Gamma} (1) = \left[ \begin{array}{c c c} 5 6 & - 4 6 & 5 2 \\ - 4 5 & 4 9 & - 4 5 \\ 4 4 & - 3 5 & 1 2 5 \end{array} \right] \\ \widehat {\Gamma} (2) = \left[ \begin{array}{r r r} 5 6 & - 4 2 & 6 2 \\ - 4 2 & 5 0 & - 4 8 \\ 3 5 & - 2 0 & 1 3 6 \end{array} \right] \quad \widehat {\Gamma} (3) = \left[ \begin{array}{r r r} 4 7 & - 4 2 & 5 9 \\ - 4 1 & 4 4 & - 5 5 \\ 2 7 & - 1 8 & 1 2 3 \end{array} \right] \tag {5.100} \\ \widehat {\Gamma} (4) = \left[ \begin{array}{c c c} 4 4 & - 3 4 & 7 2 \\ - 3 9 & 4 6 & - 5 3 \\ 1 6 & - 9 & 1 2 0 \end{array} \right] \quad \widehat {\Gamma} (5) = \left[ \begin{array}{c c c} 3 8 & - 3 5 & 6 8 \\ - 3 9 & 3 9 & - 6 7 \\ 7 & 3 & 1 0 4 \end{array} \right]. \\ \end{array}
$$

Inspecting the autocovariance matrices, we find mortality, $x _ { t 1 }$ , and temperature, $x _ { t 2 }$ , are negatively correlated at about the same strength for both positive and negative lags. The strongest cross-correlation occurs at lag $\pm 1$ , where $\widehat { \rho } _ { 1 2 } ( - 1 ) \approx - 4 5 / \sqrt { 7 9 } \sqrt { 8 1 } = - . 5 6$ , and $\widehat { \rho } _ { 1 2 } ( 1 ) \approx$ $- 4 6 / \sqrt { 7 9 } \sqrt { 8 1 } = - . 5 8$ . Also, mortality $x _ { t 1 }$ and particulates $x _ { t 3 }$ are positively correlated, the strongest correlation being when particulates leads mortality by about one month, $\widehat { \rho } _ { 1 3 } ( 4 ) \approx 7 2 / \sqrt { 7 9 } \sqrt { 2 2 7 } = . 5 4$ . Finally, we note that particulates and temperature are negatively correlated, the strongest displayed value (which is approximately the strongest overall

correlation between the two series) is when particulates leads temperature by about five weeks, $\widehat { \rho } _ { 2 3 } ( 5 ) \approx - 6 7 / \sqrt { 8 1 } \sqrt { 2 2 7 } = . 4 9$ . The autocovariance matrices do not cut off at any small lag, and hence a pure moving average model is not indicated.

Replacing $\Gamma ( h )$ by $\widehat { \Gamma } ( h )$ in (5.96), we can obtain estimates of the partial autoregression matrices. The first four estimated matrices are

$$
\widehat {\Phi} _ {1 1} = \left[ \begin{array}{c c c} . 4 7 & -. 3 6 & . 1 0 \\ -. 2 5 & . 4 9 & -. 1 3 \\ -. 1 2 & -. 4 8 & . 5 8 \end{array} \right] \quad \widehat {\Phi} _ {2 2} = \left[ \begin{array}{c c c} . 2 7 & -. 0 8 & . 0 7 \\ -. 0 4 & . 3 5 & -. 0 9 \\ -. 3 3 & . 0 5 & . 3 8 \end{array} \right]
$$

$$
\widehat {\Phi} _ {3 3} = \left[ \begin{array}{c c c} -. 0 4 & . 0 2 & -. 0 1 \\ . 0 0 & . 1 1 & -. 0 3 \\ -. 2 1 & . 0 7 & . 1 7 \end{array} \right] \quad \widehat {\Phi} _ {4 4} = \left[ \begin{array}{c c c} -. 0 4 & . 0 8 & . 0 6 \\ -. 0 7 & . 1 7 & . 0 1 \\ -. 2 6 & . 1 2 & . 1 3 \end{array} \right].
$$

As explained above (5.97), we can use (5.96) to estimate successive $\operatorname { A R } ( h )$ models with parameter estimates $\widehat { \Phi } _ { j } = \widehat { \Phi } _ { j h }$ , $j = 1 , \dotsc , h$ , and $h = 1 , 2 , \ldots$ Note, $\widehat { \Phi } _ { 1 1 }$ is practically the same as $\widehat { \Phi }$ in Example 5.9, and $\widehat { \Phi } _ { 2 2 }$ is practically the same as $\widehat { \Phi } _ { 2 }$ in Example 5.10. The only difference in the estimates is that we are using Yule–Walker here, whereas regression was used in the other examples. These matrices contain small components after lag two, indicating the AR(2) relationship, although there is evidence of some relationship between mortality and particulates at lags of three and four weeks.

The estimated autocovariance matrices can also be used to obtain estimates of the partial canonical correlations. For example, to estimate the lag $h = 3$ partial canonical correlations, $\{ \widehat \lambda _ { 1 } ^ { 2 } ( 3 ) , \widehat \lambda _ { 2 } ^ { 2 } ( 3 ) , \widehat \lambda _ { 3 } ^ { 2 } ( 3 ) \}$ , we would put

$$
\widehat {\Gamma} _ {2 2} = \left[ \begin{array}{l l} \widehat {\Gamma} (0) & \widehat {\Gamma} (1) \\ \widehat {\Gamma} (1) ^ {\prime} & \widehat {\Gamma} (0) \end{array} \right], \tag {5.101}
$$

which represents, in this case, a $6 \times 6$ matrix of the estimated autocovariances that were displayed in (5.100). In addition, we will need the matrices

$$
\widehat {\Gamma} _ {1} ^ {(2)} = \left[ \widehat {\Gamma} (2) ^ {\prime}, \widehat {\Gamma} (1) ^ {\prime} \right] \quad \mathrm {a n d} \quad \widehat {\Gamma} _ {2} ^ {(1)} = \left[ \widehat {\Gamma} (1), \widehat {\Gamma} (2) \right],
$$

which are both, in this example, $3 \times 6$ matrices. From these matrices, we construct the $3 \times 3$ matrices

$$
\widehat {\Sigma} _ {0 0 | 2 1} = \widehat {\Gamma} (0) - \widehat {\Gamma} _ {1} ^ {(2)} \widehat {\Gamma} _ {2 2} ^ {- 1} \widehat {\Gamma} _ {1} ^ {(2) ^ {\prime}},
$$

$$
\widehat {\Sigma} _ {3 3 | 2 1} = \widehat {\Gamma} (0) - \widehat {\Gamma} _ {2} ^ {(1)} \widehat {\Gamma} _ {2 2} ^ {- 1} \widehat {\Gamma} _ {2} ^ {(1) ^ {\prime}},
$$

and

$$
\widehat {\Sigma} _ {3 0 | 2 1} = \widehat {\Gamma} (2) - \widehat {\Gamma} _ {2} ^ {(1)} \widehat {\Gamma} _ {2 2} ^ {- 1} \widehat {\Gamma} _ {1} ^ {(2) ^ {\prime}} = \widehat {\Sigma} _ {0 3 | 2 1} ^ {\prime}.
$$

Finally, the squared partial canonical correlations, $\lambda _ { j } ^ { 2 } ( 3 )$ , for $j = 1 , 2 , 3$ , are obtained as the ordered eigenvalues of Σ−1 $\widehat { \Sigma } _ { 0 0 | 2 1 } ^ { - 1 } \widehat { \Sigma } _ { 0 3 | 2 1 } \widehat { \Sigma } _ { 3 3 | 2 1 } ^ { - 1 } \widehat { \Sigma } _ { 3 3 | 2 1 }$ Σ−1 .

In this example we obtain

$$
\left(\widehat {\lambda} _ {1} ^ {2} (h), \widehat {\lambda} _ {2} ^ {2} (h), \widehat {\lambda} _ {3} ^ {2} (h)\right) = \left\{ \begin{array}{l l} (. 8 1, . 2 4,. 0 2) & h = 1 \\ (. 2 2, . 1 4,. 0 6) & h = 2 \\ (. 0 5, . 0 1,. 0 0) & h = 3 \\ (. 0 5, . 0 2,. 0 0) & h = 4, \end{array} \right.
$$

which also suggests an AR(2) model for the data.

In addition, successive Yule–Walker estimates, for $h = 1 , 2 , \ldots$ , of the error variance–covariance matrix can be obtained from (5.93), that is,

$$
\widehat {\Sigma} _ {w} ^ {(h)} = \widehat {\Gamma} (0) - \sum_ {j = 1} ^ {h} \widehat {\Phi} _ {j h} \widehat {\Gamma} (- j). \tag {5.102}
$$

For this data, we obtained (entries are rounded to integers)

$$
\widehat {\Sigma} _ {w} ^ {(1)} = \left[ \begin{array}{c c c} 3 1 & 6 & 1 7 \\ 6 & 4 1 & 4 2 \\ 1 7 & 4 2 & 1 4 4 \end{array} \right], \quad \widehat {\Sigma} _ {w} ^ {(2)} = \left[ \begin{array}{c c c} 2 8 & 7 & 1 6 \\ 7 & 3 7 & 4 0 \\ 1 6 & 4 0 & 1 2 3 \end{array} \right],
$$

$$
\widehat {\Sigma} _ {w} ^ {(3)} = \left[ \begin{array}{c c c} 2 8 & 7 & 1 6 \\ 7 & 3 7 & 4 0 \\ 1 6 & 4 0 & 1 1 8 \end{array} \right], \quad \widehat {\Sigma} _ {w} ^ {(4)} = \left[ \begin{array}{c c c} 2 7 & 6 & 1 4 \\ 6 & 3 6 & 3 8 \\ 1 4 & 3 8 & 1 1 4 \end{array} \right].
$$

The estimates stabilize (except for perhaps the variance of the particulate series) after $h = 2$ , indicating the AR(3) and AR(4) fits do not improve much over the AR(2) fit. Recall the comparison of the autoregressions of order one to five using the SIC, as reported in Table 5.1 also indicated the AR(2) model.

At this point, we would settle on the AR(2) model estimated in Example 5.10 on the detrended data. We will write the estimated model as

$$
\widehat {\boldsymbol {x}} _ {t} = \widehat {\Phi} _ {1} \boldsymbol {x} _ {t - 1} + \widehat {\Phi} _ {2} x _ {t - 2} + \widehat {\boldsymbol {w}} _ {t}, \tag {5.103}
$$

where $\widehat { \Phi } _ { 1 }$ and $\widehat { \Phi } _ { 2 }$ are given in Example 5.10. The estimate of $\Sigma _ { w }$ for this model is $\widehat { \Sigma } _ { w } ^ { ( 2 ) }$ , which is listed below (5.102). Residual analysis, performed on the residuals $\widehat { \pmb { w } } _ { t } = \widehat { \pmb { x } } _ { t } - \widehat { \Phi } _ { 1 } \pmb { x } _ { t - 1 } - \widehat { \Phi } _ { 2 } \pmb { x } _ { t - 2 }$ , for t=3,...,508, suggests the model fits well. Individual residual analyses on the $\widehat { \pmb { w } } _ { t i }$ , for $i = 1 , 2 , 3$ , show, except for the particulate series, $w _ { t 3 }$ , the residuals are Gaussian white noise. For the particulate series, a small, but significant, amount of autocorrelation is still left in that series. In this case, we may wish to fit a higher order (higher than two) model to the particulate series only. In addition, we might be inclined to fit a reduced rank model, and we

will discuss this matter later. Inspection of the pairwise CCF between all residual series shows no obvious departures from independence.

Once the model has been estimated, estimated forecasts can be obtained. Analogous to the univariate case, the $m$ -step-ahead forecast, $m = 1 , 2 , \ldots$ , in this example ( $n = 5 0 8$ ), is obtained as follows:

$$
\widehat {\boldsymbol {x}} _ {n + m} ^ {n} = \widehat {\Phi} _ {1} \widehat {\boldsymbol {x}} _ {n + m - 1} ^ {n} + \widehat {\Phi} _ {2} \widehat {\boldsymbol {x}} _ {n + m - 2} ^ {n}, \tag {5.104}
$$

where $\widehat { \pmb x } _ { t } ^ { n } = { \pmb x } _ { t }$ when $1 \leq t \leq n$ . The mean square prediction error matrices can be calculated in a manner similar to the univariate case, (3.67). In the general case of vector ARMA or ARMAX models, forecasts and their mean square prediction errors can be obtained by using the state-space formulation of the model and the Kalman filter (see §6.6). Analogous to (3.67), the general form of the $m$ -step-ahead mean square prediction error matrix is,

$$
\begin{array}{l} P _ {n + m} ^ {n} = E \left(\boldsymbol {x} _ {n + m} - \boldsymbol {x} _ {n + m} ^ {n}\right) \left(\boldsymbol {x} _ {n + m} - \boldsymbol {x} _ {n + m} ^ {n}\right) ^ {\prime} (5.105) \\ = \Gamma (0) - \Gamma_ {n} ^ {(m)} \Gamma_ {n n} ^ {- 1} \Gamma_ {n} ^ {(m) ^ {\prime}}, (5.106) \\ \end{array}
$$

where $\Gamma _ { n } ^ { ( m ) } = [ \Gamma ( m ) , \Gamma ( m + 1 ) , . . . , \Gamma ( m + n - 1 ) ]$ , is a $k \times n k$ matrix, and $\Gamma _ { n n } = \{ \Gamma ( i - j ) \} _ { i , j = 1 } ^ { n }$ , is an $n k \times n k$ symmetric matrix. Of course, $P _ { n + m } ^ { n }$ can be estimated by substituting $\widehat { \Gamma } ( h )$ for $\Gamma ( h )$ in (5.106). The analogue of (3.77) for multivariate ARMA models is

$$
P _ {n + m} ^ {n} = \sum_ {j = 0} ^ {m - 1} \Psi_ {j} \Sigma_ {w} \Psi_ {j} ^ {\prime}. \tag {5.107}
$$

When the model is autoregressive, as in this example, a simplification occurs by noticing a $k$ -dimensional AR(p) model can be written as a $k p$ - dimensional AR(1) model. For example, we can write the vector AR(2) model as

$$
\boldsymbol {X} _ {t} = \boldsymbol {\alpha} + A \left(\boldsymbol {X} _ {t - 1} - \boldsymbol {\alpha}\right) + \boldsymbol {\eta} _ {t}, \tag {5.108}
$$

where

$$
\boldsymbol {X} _ {t} = \left[ \begin{array}{l} \boldsymbol {x} _ {t} \\ \boldsymbol {x} _ {t - 1} \end{array} \right] \quad \boldsymbol {\alpha} = \left[ \begin{array}{l} \boldsymbol {\mu} \\ \boldsymbol {\mu} \end{array} \right] \quad A = \left[ \begin{array}{l l} \Phi_ {1} & \Phi_ {2} \\ I & 0 \end{array} \right] \quad \boldsymbol {\eta} _ {t} = \left[ \begin{array}{l} \boldsymbol {w} _ {t} \\ \boldsymbol {0} \end{array} \right].
$$

Of course, this technique generalizes to any dimension $k$ and any order $p$ . From (5.108) we immediately obtain the forecasts and mean square prediction errors as

$$
\boldsymbol {X} _ {n + m} ^ {n} = \boldsymbol {\alpha} + A ^ {m} (\boldsymbol {X} _ {n} - \boldsymbol {\alpha})
$$

and

$$
\begin{array}{l} Q _ {n + m} ^ {n} = E \left(\boldsymbol {X} _ {n + m} - \boldsymbol {X} _ {n + m} ^ {n}\right) \left(\boldsymbol {X} _ {n + m} - \boldsymbol {X} _ {n + m} ^ {n}\right) ^ {\prime} \\ = \Gamma_ {X} (0) - A ^ {m} \Gamma_ {X} (0) A ^ {m}, \\ \end{array}
$$

where

$$
\Gamma_ {X} (0) = \left[ \begin{array}{l l} \Gamma (0) & \Gamma (1) \\ \Gamma (1) ^ {\prime} & \Gamma (0) \end{array} \right].
$$

We can then obtain the desired mean square prediction error matrices $P _ { n + m } ^ { n }$ as submatrices of $Q _ { n + m } ^ { n }$ . In addition, Yule–Walker estimation and forecasting can be accomplished by substituting autocovariance matrices by their sample equivalents obtained via (5.98).

For this numerical example,

$$
\widehat {A} = \left[ \begin{array}{l l} \widehat {\Gamma} (1) & \widehat {\Gamma} (2) \\ \widehat {\Gamma} (0) & \widehat {\Gamma} (1) \end{array} \right] \left[ \begin{array}{l l} \widehat {\Gamma} (0) & \widehat {\Gamma} (1) \\ \widehat {\Gamma} (1) ^ {\prime} & \widehat {\Gamma} (0) \end{array} \right] ^ {- 1} = \left[ \begin{array}{l l} \widehat {\Phi} _ {1} & \widehat {\Phi} _ {2} \\ I & 0 \end{array} \right],
$$

where $\hat { \Phi } _ { 1 }$ and $\hat { \Phi } _ { 2 }$ are as given in Example 5.10. In the general case, we obtain the coefficient estimates from the top $k$ rows of $\widehat { A }$ . Similarly, estimated forecasts in this example are found as follows:

$$
\left[ \begin{array}{l} \widehat {\pmb {x}} _ {n + m} ^ {n} \\ \widehat {\pmb {x}} _ {n + m - 1} ^ {n} \end{array} \right] = \widehat {A} ^ {m} \left[ \begin{array}{l} \pmb {x} _ {n} \\ \pmb {x} _ {n - 1} \end{array} \right].
$$

Because $\pmb { x } _ { 5 0 7 } = ( 8 . 6 2 , - 1 . 8 5 , 1 2 . 1 6 ) ^ { \prime }$ and $\pmb { x } _ { 5 0 8 } = ( 4 . 7 1 , - 4 . 6 7 , 1 7 . 2 0 ) ^ { \prime }$ , we can, for example, calculate the one-step-ahead and two-step-ahead forecasts by putting $m = 2$ and using the numerical values given in Example 5.10 to construct $\widehat { A } ^ { 2 }$ ,

$$
\left[ \begin{array}{c} \widehat {\pmb {x}} _ {5 1 0} ^ {5 0 8} \\ \widehat {\pmb {x}} _ {5 0 9} ^ {5 0 8} \end{array} \right] = \widehat {A} ^ {2} \left[ \begin{array}{c} \widehat {\pmb {x}} _ {5 0 8} \\ \widehat {\pmb {x}} _ {5 0 7} \end{array} \right] = \left[ \begin{array}{c} 6. 1 3 \\ - 5. 9 4 \\ 1 1. 2 3 \\ 6. 4 3 \\ - 4. 7 7 \\ 1 0. 5 3 \end{array} \right].
$$

Substituting autocovariance matrices with their estimates, we may write

$$
\begin{array}{l} \widehat {Q} _ {5 1 0} ^ {5 0 8} = \left[ \begin{array}{c c} \widehat {\Gamma} (0) & \widehat {\Gamma} (1) \\ \widehat {\Gamma} (1) ^ {\prime} & \widehat {\Gamma} (0) \end{array} \right] - \widehat {A} ^ {2} \left[ \begin{array}{c c} \widehat {\Gamma} (0) & \widehat {\Gamma} (1) \\ \widehat {\Gamma} (1) ^ {\prime} & \widehat {\Gamma} (0) \end{array} \right] \widehat {A} ^ {\prime 2} \\ = \left[ \begin{array}{c c} \widehat {P} _ {5 1 0} ^ {5 0 8} & \widehat {P} _ {5 1 0, 5 0 9} ^ {5 0 8} \\ \hline \widehat {P} _ {5 0 9, 5 1 0} ^ {5 0 8} & \widehat {P} _ {5 0 9} ^ {5 0 8} \end{array} \right], \\ \end{array}
$$

where we have written $\widehat { P } _ { s , t } ^ { n }$ to be the estimate of $E \{ ( \pmb { x } _ { s } - \pmb { x } _ { s } ^ { n } ) ( \pmb { x } _ { t } - \pmb { x } _ { t } ^ { n } ) ^ { \prime } \}$ .

In this example, we found (entries are rounded)

$$
\widehat {Q} _ {5 1 0} ^ {5 0 8} = \left[ \begin{array}{c c c c c c} 3 1 & 5 & 1 9 & 8 & - 4 & 2 \\ 5 & 3 9 & 3 8 & - 2 & 7 & 2 \\ 1 9 & 3 8 & 1 3 5 & 6 & 2 & 3 3 \\ 8 & - 2 & 6 & 2 8 & 7 & 1 6 \\ - 4 & 7 & 2 & 7 & 3 7 & 4 0 \\ 2 & 2 & 3 3 & 1 6 & 4 0 & 1 2 3 \end{array} \right].
$$

Note, $\widehat { P } _ { 5 0 9 } ^ { 5 0 8 } = \widehat { \Sigma } _ { w } = \widehat { \Sigma } _ { w } ^ { ( 2 ) }$ Σw . The diagonal elements of $\widehat { Q } _ { 5 1 0 } ^ { 5 0 8 }$ give the individual mean-square prediction errors. For example, an approximate $9 5 \%$ prediction interval for $x _ { 5 1 0 , 1 } ^ { 5 0 8 }$ is $6 . 1 3 \pm 2 \sqrt { 3 1 }$ or $( - 5 . 0 , 1 7 . 2 )$ .

Although the estimation in Example 5.11 was performed using Yule–Walker estimation, we could have also used conditional or unconditional maximum likelihood estimation, or conditional (as in Example 5.10) or unconditional least squares estimation. Because, as we have seen, any $k$ -dimensional $\operatorname { A R } ( p )$ model can be written as a $k p$ -dimensional AR(1) model, any of these estimation techniques are straightforward multivariate extensions to the univariate case presented in equations (2.124)-(2.133). Also, as in the univariate case, the Yule–Walker estimators, the maximum likelihood estimators, and the least squares estimators are asymptotically equivalent. To exhibit the asymptotic distribution of the autoregression parameter estimators, we write

$$
\boldsymbol {\phi} = \operatorname {v e c} \left(\Phi_ {1}, \dots , \Phi_ {p}\right),
$$

where the vec operator stacks the columns of a matrix into a vector. For example, for a bivariate AR(2) model,

$$
\boldsymbol {\phi} = \operatorname {v e c} \left(\Phi_ {1}, \Phi_ {2}\right) = \left(\Phi_ {1 _ {1 1}}, \Phi_ {1 _ {2 1}}, \Phi_ {1 _ {1 2}}, \Phi_ {1 _ {2 2}} \Phi_ {2 _ {1 1}}, \Phi_ {2 _ {2 1}}, \Phi_ {2 _ {1 2}}, \Phi_ {2 _ {2 2}}\right) ^ {\prime},
$$

where $\Phi _ { \ell _ { i j } }$ is the $_ { i j }$ -th element of $\Phi _ { \ell }$ , $\ell = 1 , 2$ . Because $\big ( \Phi _ { 1 } , . . . , \Phi _ { p } \big )$ is a $k \times k p$ matrix, $\phi$ is a $k ^ { 2 } p \times 1$ vector. We now state the following property.

# Property P5.1: Large Sample Distribution of the Vector Autoregression Estimators

Let $\widehat { \phi }$ denote the vector of parameter estimators (obtained via Yule–Walker, least squares, or maximum likelihood) for a $k$ -dimensional AR(p) model. Then,

$$
\sqrt {n} (\widehat {\boldsymbol {\phi}} - \boldsymbol {\phi}) \sim \mathrm {A N} (\mathbf {0}, \Sigma_ {w} \otimes \Gamma_ {p p} ^ {- 1}), \tag {5.109}
$$

where $\Gamma _ { p p } = \{ \Gamma ( i - j ) \} _ { i , j = 1 } ^ { p }$ is a $k p \times k p$ matrix, $\Sigma _ { w } \otimes \Gamma _ { p p } ^ { - 1 } = \{ \sigma _ { i j } \Gamma _ { p p } ^ { - 1 } \} _ { i , j = 1 } ^ { k }$ is a $k ^ { 2 } p \times k ^ { 2 } p$ matrix, and $\sigma _ { i j }$ is the $i j$ -th element of $\Sigma _ { w }$ .

The variance–covariance matrix of the estimator $\widehat { \phi }$ is approximated by replacing $\Sigma _ { w }$ by $\widehat { \Sigma } _ { w }$ , and replacing $\Gamma ( h )$ by $\widehat { \Gamma } ( h )$ in $\Gamma _ { p p }$ . The square root of the

diagonal elements of $\widehat { \Sigma } _ { w } \otimes \widehat { \Gamma } _ { p p } ^ { - 1 }$ divided by $\sqrt { n }$ gives the individual standard errors. For the mortality data example, the estimated standard errors for the VAR(2) fit are listed in Example 5.10; although those standard errors were taken from a regression run, they could have also been calculated using Property P5.1 along with the numerical values taken from $\widehat { \Sigma } _ { w } ^ { ( 2 ) }$ given below (5.102) and $\widehat { \Gamma } _ { 2 2 }$ given in (5.101).

Asymptotic inference for the general case of vector ARMA models is more complicated than pure AR models; details can be found in Reinsel (1997) or L¨utkepohl (1993), for example. We also note that estimation for VARMA models can be recast into the problem of estimation for state-space models that will be discussed in Chapter 6.

A simple algorithm for fitting multivariate ARMA models from Spliid (1983) is worth mentioning because it repeatedly uses the multivariate regression equations. Consider a general ARMA $( p , q )$ model for a time series with a nonzero mean

$$
\boldsymbol {x} _ {t} = \boldsymbol {\alpha} + \Phi_ {1} \boldsymbol {x} _ {t - 1} + \dots + \Phi_ {p} \boldsymbol {x} _ {t - p} + \boldsymbol {w} _ {t} + \Theta_ {1} \boldsymbol {w} _ {t - 1} + \dots + \Theta_ {q} \boldsymbol {w} _ {t - q}. \tag {5.110}
$$

If ${ \pmb \mu } = E { \pmb x } _ { t }$ , then $\pmb { \alpha } = ( I - \Phi _ { 1 } - \cdot \cdot \cdot - \Phi _ { p } ) \pmb { \mu }$ . If $\pmb { w } _ { t - 1 } , . . . , \pmb { w } _ { t - q }$ were observed, we could rearrange (5.110) as a multivariate regression model

$$
\boldsymbol {x} _ {t} = \mathcal {B} \boldsymbol {z} _ {t} + \boldsymbol {w} _ {t}, \tag {5.111}
$$

with

$$
\boldsymbol {z} _ {t} = \left(1, \boldsymbol {x} _ {t - 1} ^ {\prime}, \dots , \boldsymbol {x} _ {t - p} ^ {\prime}, \boldsymbol {w} _ {t - 1} ^ {\prime}, \dots , \boldsymbol {w} _ {t - q} ^ {\prime}\right) ^ {\prime} \tag {5.112}
$$

and

$$
\mathcal {B} = \left[ \boldsymbol {\alpha}, \Phi_ {1}, \dots , \Phi_ {p}, \Theta_ {1}, \dots , \Theta_ {q} \right], \tag {5.113}
$$

for $t = p + 1 , . . . , n$ . Given an initial estimator $B _ { 0 }$ , of $\boldsymbol { \beta }$ , we can reconstruct $\{ \pmb { w } _ { t - 1 } , . . . , \pmb { w } _ { t - q } \}$ by setting

$$
\boldsymbol {w} _ {t - j} = \boldsymbol {x} _ {t - j} - \mathcal {B} _ {0} \boldsymbol {z} _ {t - j}, \quad t = p + 1, \dots , n, \quad j = 1, \dots , q, \tag {5.114}
$$

where, if $q > p$ , we put $\pmb { w } _ { t - j } = \mathbf { 0 }$ for $t - j \leq 0$ . The new values of $\{ \pmb { w } _ { t - 1 } , . . . , \pmb { w } _ { t - q } \}$ are then put into the regressors $\scriptstyle { z _ { t } }$ and a new estimate, say, $B _ { 1 }$ , is obtained. The initial value, $B _ { 0 }$ , can be computed by fitting a pure autoregression of order $p$ or higher, and taking $\Theta _ { 1 } = \dots = \Theta _ { q } = \mathbf { 0 }$ . The procedure is then iterated until the parameter estimates stabilize. The algorithm usually converges, but not to the maximum likelihood estimators. Experience suggests the estimators are reasonably close to the maximum likelihood estimators.

As previously discussed, the special form assumed for the constant component, $\pmb { \alpha }$ , of the general ARMA model in (5.110) can be generalized to include a fixed $r \times 1$ vector of inputs, say, $\mathbf { \pmb { u } } _ { t }$ . In this case we have a $k$ -dimensional ARMAX model:

$$
\boldsymbol {x} _ {t} = \Gamma \boldsymbol {u} _ {t} + \sum_ {j = 1} ^ {p} \Phi_ {j} \boldsymbol {x} _ {t - j} + \sum_ {j = 1} ^ {q} \Theta_ {j} \boldsymbol {w} _ {t - j} + \boldsymbol {w} _ {t}, \tag {5.115}
$$

![](images/bea6c6e9697611c90663558102ef1e70ac3d9c23d5dc9975ec26f3c7a351abb8.jpg)  
Figure 5.14 CCF between prewhitened mortality and temperature (positive lag means temperature leads mortality).

where $\Gamma$ is a $k \times r$ parameter matrix. Recall the X in ARMAX refers to the exogenous vector process we have denoted here by $\mathbf { \pmb { u } } _ { t }$ and the introduction of exogenous variables through setting $\pmb { \alpha } = \Gamma \pmb { u } _ { t }$ does not present any special problems in making inferences.

# Example 5.12 An ARMAX Model for Cardiovascular Mortality

In Example 2.2, we regressed the cardiovascular mortality series, $M _ { t }$ , on time $t$ , temperature $T _ { t }$ , and particulate pollution $P _ { t }$ . There, the interest was an analysis of the effect of temperature and pollution on cardiovascular mortality. In Example 5.10, we fit a multivariate ARMA model to the trivariate vector $( M _ { t } , T _ { t } , P _ { t } )$ , as if modeling the behavior of temperature and pollution was equally as important as modeling the behavior of mortality. In this example, we are interested in using temperature and pollution to explain some of the variation in the mortality series.

To examine the CCF between mortality and temperature, and between mortality and pollution, we first prewhitened mortality by fitting an AR(2) to the detrended data. That is, we first fit the model

$$
M _ {t} = \beta_ {0} + \beta_ {1} t + \phi_ {1} M _ {t - 1} + \phi_ {2} M _ {t - 2} + \epsilon_ {t}.
$$

Using the residuals of the fit, say, $\widehat { \epsilon } _ { t }$ , we then calculated the CCF between $\widehat { \epsilon } _ { t }$ and $T _ { t }$ , and between $\widehat { \epsilon } _ { t }$ and $P _ { t }$ . Figure 5.14 shows the cross-correlation of prewhitened mortality and temperature (positive lag means temperature leads mortality) and a significant correlation is seen at lag $h = 1$ .

![](images/9dd2e450c470181fff04472b5407cbddef9e054a05e3dcfe00f17220e6fd5bbd.jpg)  
Figure 5.15 CCF between prewhitened mortality and particulate pollution (positive lag means pollution leads mortality).

Figure 5.15 shows a similar plot for the CCF of prewhitened mortality and pollution, and significant correlations are seen at lags $h = 0 , 2 , 4 , 7$ . After some preliminary fitting, the final model uses the exogenous variables $\pmb { u } _ { t } = ( 1 , t , T _ { t - 1 } , T _ { t - 1 } ^ { 2 } , P _ { t } , P _ { t - 4 } ) ^ { \prime }$ , along with an AR(2) on mortality, $M _ { t }$ ; the inclusion of particulate pollution at lags two and seven were not significant when lags zero and four are in the model. In this case, the ARMAX model is

$$
M _ {t} = \Gamma \boldsymbol {u} _ {t} + \phi_ {1} M _ {t - 1} + \phi_ {2} M _ {t - 2} + w _ {t},
$$

where $\Gamma = [ \gamma _ { 0 } , \gamma _ { 1 } , \gamma _ { 2 } , \gamma _ { 3 } , \gamma _ { 4 } ]$ .

Estimation was accomplished using the regression approach described in (5.85) and (5.86). In this case, the fitted model was (values are rounded)

$$
\begin{array}{l} \widehat {M _ {t}} = 4 2. 9 -. 0 1 _ {(. 0 0 2)} t - . 1 8 _ {(. 0 3)} T _ {t - 1} +. 1 1 _ {(. 0 2)} P _ {t} +. 0 5 _ {(. 0 2)} P _ {t - 4} \\ + \quad . 3 1 _ {(. 0 4)} M _ {t - 1} +. 3 0 _ {(. 0 4)} M _ {t - 2} + \widehat {w} _ {t}, \\ \end{array}
$$

where $\widehat { \sigma } _ { w } ^ { 2 } = 2 5 . 7$ and $R ^ { 2 } = 7 4 . 3 \%$ . Each coefficient is significant, as seen from the estimated standard errors listed below each parameter estimate. Finally, an analysis of the residuals, $\widehat { w } _ { t }$ , shows, except for a few outliers, the model fits well. The value of the Ljung–Box–Pierce statistic for H=24 was Q=25.7, which when compared to a $\chi _ { 2 2 } ^ { 2 }$ , is not significant. In addition, a Q-Q plot shows no departure from the Gaussian assumption, except for the few outliers. Our general conclusions are that decrease in cardiovascular mortality occurred during the period studied, and an

increase in mortality is associated with lower temperatures the previous week and higher particulate pollution both currently and one month prior.

# Problems

# Section 5.2

5.1 The data set labeled fracdiff.dat is $n = 1 0 0 0$ simulated observations from a fractionally differenced ARIMA $( 1 , 1 , 0 )$ model with $\phi = . 7 5$ and $d = . 4$ .

(a) Plot of the data and comment.   
(b) Plot the ACF and PACF of the data and comment.   
(c) Estimate the parameters and test for the significance of the estimates $\hat { \phi }$ and $\hat { d }$ .   
(d) Explain why, using the results of part (a) and (b), it would seem reasonable to difference the data prior to the analysis. That is, if $x _ { t }$ represents the data, explain why we might choose to fit an ARMA model to $\nabla x _ { t }$ .   
(e) Plot the ACF and PACF of $\nabla x _ { t }$ and comment.   
(f) Fit an ARMA model to $\nabla \boldsymbol { x } _ { t }$ and comment.

5.2 The data in globtemp2.dat are annual global temperature deviations from 1880 to 2004 (there are three columns in the data file; work with the annual means and not the 5-year smoothed data). The data are an update to the Hansen–Lebedeff global temperature data displayed in Figure 1.2. The url of the data source is in the file, you can go there for further explanation of the data. Fit an ARFIMA model to this series.

5.3 Compute the sample ACF of the absolute values of the NYSE returns displayed in Figure 1.4 up to lag 200 and comment on whether the ACF indicates long memory. Fit an ARFIMA model to the absolute values and comment.

# Section 5.3

5.4 Investigate whether the monthly returns of a stock dividend yield listed in the file sdyr.dat exhibit GARCH behavior. If so, fit an appropriate model to the returns. The data are monthly returns of a stock dividend yield from January 1947 through May 1993 and are taken from Hamilton and Lin (1996).

5.5 Investigate whether the growth rate of the monthly Oil Prices exhibit GARCH behavior. If so, fit an appropriate model to the growth rate.   
5.6 The stats package of R contains the daily closing prices of four major European stock indices; type help(EuStockMarkets) for details. Fit a GARCH model to the returns of these series and discuss your findings. (Note: The data set contains actual values, and not returns. Hence, the data must be transformed prior to the model fitting.)   
5.7 The $2 \times 1$ gradient vector, $l ^ { ( 1 ) } ( \alpha _ { 0 } , \alpha _ { 1 } )$ , given for an ARCH(1) model was displayed in (5.41). Verify (5.41) and then use the result to calculate the $2 \times 2$ Hessian matrix

$$
l ^ {(2)} (\alpha_ {0}, \alpha_ {1}) = \left( \begin{array}{c c} \partial^ {2} l / \partial \alpha_ {0} ^ {2} & \partial^ {2} l / \partial \alpha_ {0} \partial \alpha_ {1} \\ \partial^ {2} l / \partial \alpha_ {0} \partial \alpha_ {1} & \partial^ {2} l / \partial \alpha_ {1} ^ {2} \end{array} \right).
$$

# Section 5.4

5.8 The sunspot data are plotted in Chapter 4, Figure 4.31. From a time plot of the data, discuss why is it reasonable to fit a threshold model to the data, and then fit a threshold model.

# Section 5.5

5.9 Let $S _ { t }$ represent the monthly sales data listed in sales.dat $n = 1 5 0$ ), and let $L _ { t }$ be the leading indicator listed in lead.dat. Fit the regression model $\nabla S _ { t } = \beta _ { 0 } + \beta _ { 1 } \nabla L _ { t - 3 } + x _ { t }$ , where $x _ { t }$ is an ARMA process.

5.10 Consider the correlated regression model, defined in the text by (5.53), say,

$$
\boldsymbol {y} = Z \boldsymbol {\beta} + \boldsymbol {x},
$$

where $\pmb { x }$ has mean zero and covariance matrix $\Gamma$ . In this case, we know that the weighted least squares estimator is (5.54), namely,

$$
\widehat {\boldsymbol {\beta}} _ {w} = \left(Z ^ {\prime} \Gamma^ {- 1} Z\right) ^ {- 1} Z ^ {\prime} \Gamma^ {- 1} \boldsymbol {y}.
$$

Now, a problem of interest in spatial series can be formulated in terms of this basic model. Suppose $y _ { i } = y ( \sigma _ { i } ) , i = 1 , 2 , . . . , n$ is a function of the spatial vector coordinates $\sigma _ { i } = ( s _ { i 1 } , s _ { i 2 } , . . . , s _ { i r } ) ^ { \prime }$ , the error is $x _ { i } = x ( \sigma _ { i } )$ , and the rows of $Z$ are defined as $z ( \sigma _ { i } ) ^ { \prime } , i \ = \ 1 , 2 , . . . , n$ . The Kriging estimator is defined as the best spatial predictor of $y _ { 0 } = z _ { 0 } ^ { \prime } \beta + x _ { 0 }$ using the estimator

$$
\widehat {y} _ {0} = \boldsymbol {a} ^ {\prime} \boldsymbol {y},
$$

subject to the unbiased condition $E \hat { y } _ { 0 } = E y _ { 0 }$ , and such that the mean square prediction error

$$
\mathrm {M S E} = E [ (y _ {0} - \widehat {y} _ {0}) ^ {2} ]
$$

is minimized.

(a) Prove the estimator is unbiased when $Z ^ { \prime } \mathbf { a } = z _ { 0 }$

(b) Show the MSE is minimized by solving the equations

$$
\Gamma \boldsymbol {a} + Z \boldsymbol {\lambda} = \gamma_ {0}
$$

and

$$
Z ^ {\prime} \boldsymbol {a} = z _ {0},
$$

where $\gamma _ { 0 } = E [ \pmb { x } x _ { 0 } ]$ represents the vector of covariances between the error vector of the observed data and the error of the new point the vector $\pmb { \lambda }$ is a $q \times 1$ vector of LaGrangian multipliers.

(c) Show the predicted value can be expressed as

$$
\widehat {y} _ {0} = z _ {0} ^ {\prime} \widehat {\boldsymbol {\beta}} _ {w} + \boldsymbol {\gamma} _ {0} ^ {\prime} \Gamma^ {- 1} (\boldsymbol {y} - Z \widehat {\boldsymbol {\beta}} _ {w}),
$$

so the optimal prediction is a linear combination of the usual predictor and the least squares residuals.

# Section 5.6

5.11 The file labeled clim-hyd has 454 months of measured values for the climatic variables air temperature, dew point, cloud cover, wind speed, precipitation ( $p _ { t }$ ), and inflow (it), at Shasta Lake. We would like to look at possible relations between the weather factors and the inflow to Shasta Lake.

(a) Fit ARIMA $( 0 , 0 , 0 ) \times ( 0 , 1 , 1 ) _ { 1 2 }$ models to (i) transformed precipitation $P _ { t } = \sqrt { p _ { t } }$ and (ii) transformed inflow $I _ { t } = \log i _ { t }$ .   
(b) Apply the ARIMA model fitted in part (a) for transformed precipitation to the flow series to generate the prewhitened flow residuals assuming the precipitation model. Compute the cross-correlation between the flow residuals using the precipitation ARIMA model and the precipitation residuals using the precipitation model and interpret. Use the coefficients from the ARIMA model to construct the transformed flow residuals.

5.12 Consider predicting the transformed flows $I _ { t } = \log i _ { t }$ from transformed precipitation values $P _ { t } = { \sqrt { p _ { t } } }$ using a transfer function model of the form

$$
(1 - B ^ {1 2}) I _ {t} = \alpha (B) (1 - B ^ {1 2}) P _ {t} + n _ {t},
$$

where we assume that seasonal differencing is a reasonable thing to do. The data are the 454 monthly values of precipitation and inflow from the Shasta Lake reservoir in the file clim-hyd. You may think of it as fitting

$$
y _ {t} = \alpha (B) x _ {t} + n _ {t},
$$

where $y _ { t }$ and $x _ { t }$ are the seasonally differenced transformed flows and precipitations.

(a) Argue that $x _ { t }$ can be fitted by a first-order seasonal moving average, and use the transformation obtained to prewhiten the series $x _ { t }$ .

(b) Apply the transformation applied in (a) to the series $y _ { t }$ , and compute the cross-correlation function relating the prewhitened series to the transformed series. Argue for a transfer function of the form

$$
\alpha (B) = \frac {\delta_ {0}}{1 - \omega_ {1} B}.
$$

(c) Write the overall model obtained in regression form to estimate $\delta _ { 0 }$ and $\omega _ { 1 }$ . Note that you will be minimizing the sums of squared residuals for the transformed noise series $( 1 - \widehat { \omega } _ { 1 } B ) n _ { t }$ . Retain the residuals for further modeling involving the noise $n _ { t }$ . The observed residual is $u _ { t } = ( 1 - \widehat { \omega } _ { 1 } B ) n _ { t }$ .   
(d) Fit the noise residuals obtained in (c) with an ARMA model, and give the final form suggested by your analysis in the previous parts.   
(e) Discuss the problem of forecasting $y _ { t + m }$ using the infinite past of $y _ { t }$ and the present and infinite past of $x _ { t }$ . Determine the predicted value and the forecast variance.

# Section 5.7

5.13 Consider the data set containing quarterly U.S. unemployment, U.S. GNP, consumption, and government and private investment from 1948-III to 1988-II. The seasonal component has been removed from the data. Concentrating on unemployment ( $U _ { t }$ ), GNP ( $G _ { t }$ ), and consumption ( $C _ { t }$ ), fit a vector ARMA model to the data after first logging each series, and then removing the linear trend. That is, fit a vector ARMA model to $\pmb { x } _ { t } = ( x _ { 1 t } , x _ { 2 t } , x _ { 3 t } ) ^ { \prime }$ , where, for example, $x _ { 1 t } = \log ( U _ { t } ) - \widehat { \beta } _ { 0 } - \widehat { \beta } _ { 1 } t$ , where ${ \widehat { \beta } } _ { 0 }$ and ${ \widehat { \beta } } _ { 1 }$ are the least squares estimates for the regression of $\log ( U _ { t } )$ on time, $t$ . Run a complete set of diagnostics on the residuals.

# Chapter 6

# State-Space Models

# 6.1 Introduction

A very general model that seems to subsume a whole class of special cases of interest in much the same way that linear regression does is the state-space model or the dynamic linear model, which was introduced in Kalman (1960) and Kalman and Bucy (1961). Although the model was originally introduced as a method primarily for use in aerospace-related research, it has been applied to modeling data from economics (Harrison and Stevens, 1976; Harvey and Pierse, 1984; Harvey and Todd, 1983; Kitagawa and Gersch 1984, Shumway and Stoffer, 1982), medicine (Jones, 1984) and the soil sciences (Shumway, 1985). An excellent modern treatment of time series analysis based on the state space model is the text by Durbin and Koopman (2001).

Although there are some packages available for R that focus on various aspects of state-space modeling and Kalman filtering, we prefer to write our own code. As a result, the code we have written is long and will most likely be subject to frequent updates. Hence, we have decided to distribute the R code for this chapter on the website for the text.

The state-space model or dynamic linear model (DLM), in its basic form, employs an order one, vector autoregression as the state equation,

$$
\boldsymbol {x} _ {t} = \Phi \boldsymbol {x} _ {t - 1} + \boldsymbol {w} _ {t}, \tag {6.1}
$$

where the state equation determines the rule for the generation of the $p \times 1$ state vector $\pmb { x } _ { t }$ from the past $p \times 1$ state ${ \pmb x } _ { t - 1 }$ , for time points $t = 1 , \ldots , n$ . We assume the ${ \pmb w } _ { t }$ are $p \times 1$ independent and identically distributed, zero-mean normal vectors with covariance matrix $Q$ . In the DLM, we assume the process starts with a normal vector $\pmb { x } _ { \mathrm { 0 } }$ that has mean $\pmb { \mu } _ { 0 }$ and $p \times p$ covariance matrix $\Sigma _ { 0 }$ .

The DLM, however, adds an additional component to the model in assuming we do not observe the state vector ${ \pmb x } _ { t }$ directly, but only a linear transformed

# 6.1: Introduction

version of it with noise added, say

$$
\boldsymbol {y} _ {t} = A _ {t} \boldsymbol {x} _ {t} + \boldsymbol {v} _ {t} \tag {6.2}
$$

where $A _ { t }$ is a $q \times p$ measurement or observation matrix; equation (6.2) is called the observation equation. The model arose originally in the space tracking setting, where the state equation defines the motion equations for the position or state of a spacecraft with location ${ \pmb x } _ { t }$ and ${ \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } \mathbf { } } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf } { \mathbf } { \mathbf } { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf $ reflects information that can be observed from a tracking device such as velocity and azimuth. The observed data are in the $q \times 1$ vectors $\pmb { y } _ { t }$ , which can be larger than or smaller than $p$ , the dimension of the underlying series of interest. The additive observation noise ${ \mathbf { } } _ { \pmb { v } _ { t } }$ is assumed to be white and Gaussian with $q \times q$ covariance matrix $R$ . In addition, we initially assume, for simplicity, $\{ { \pmb w } _ { t } \}$ and $\{ \pmb { v } _ { t } \}$ are uncorrelated; this assumption is not necessary, but it helps in the explanation of first concepts. The case of correlated errors is discussed in §6.6. Of course, we can further modify the basic model, (6.1) and (6.2), to include exogenous variables, and we will also discuss this in §6.6.

As in the ARMAX model of §5.7, exogenous variables, or fixed inputs, may enter into the states or into the observations. In this case, we suppose we have an $r \times 1$ vector of inputs $\mathbf { \pmb { u } } _ { t }$ , and write the model as

$$
\boldsymbol {x} _ {t} = \Phi \boldsymbol {x} _ {t - 1} + \Upsilon \boldsymbol {u} _ {t} + \boldsymbol {w} _ {t} \tag {6.3}
$$

$$
\boldsymbol {y} _ {t} = A _ {t} \boldsymbol {x} _ {t} + \Gamma \boldsymbol {u} _ {t} + \boldsymbol {v} _ {t} \tag {6.4}
$$

where $\Upsilon$ is $p \times r$ and $\Gamma$ is $q \times r$ .

# Example 6.1 A Biomedical Example

Suppose we consider the problem of monitoring the level of several biomedical parameters monitored after a cancer patient undergoes a bone marrow transplant. The data in Figure 6.1, used by Jones (1984), are measurements made for 91 days on the three variables, log(white blood count), log(platelet), and hematocrit (HCT), denoted $y _ { t 1 } , y _ { t 2 }$ , and $y _ { t 3 }$ , respectively. Approximately 40% of the values are missing, with missing values occurring primarily after the 35th day. The main objectives are to model the three variables using the state-space approach, and to estimate the missing values. According to Jones, “Platelet count at about 100 days post transplant has previously been shown to be a good indicator of subsequent long term survival.” For this particular situation, we model the three variables in terms of the state equation (6.1); that is,

$$
\left( \begin{array}{l} x _ {t 1} \\ x _ {t 2} \\ x _ {t 3} \end{array} \right) = \left( \begin{array}{c c c} \phi_ {1 1} & \phi_ {1 2} & \phi_ {1 3} \\ \phi_ {2 1} & \phi_ {2 2} & \phi_ {2 3} \\ \phi_ {3 1} & \phi_ {3 2} & \phi_ {3 3} \end{array} \right) \left( \begin{array}{l} x _ {t - 1, 1} \\ x _ {t - 1, 2} \\ x _ {t - 1, 3} \end{array} \right) + \left( \begin{array}{l} w _ {t 1} \\ w _ {t 2} \\ w _ {t 3} \end{array} \right). \tag {6.5}
$$

The 3 3 observation matrix, $A _ { t }$ , is either the identity matrix, or the identity matrix with all zeros in a row when that variable is missing. The covariance matrices $R$ and $Q$ are $3 \times 3$ matrices with $R = \mathrm { d i a g } \{ r _ { 1 1 } , r _ { 2 2 } , r _ { 3 3 } \}$ , a diagonal matrix, required for a simple approach when data are missing.

![](images/39892bd3f619619b7cad2ba73ba0dd45c144a02b50290b986eda95feddc896ef.jpg)

![](images/c632239609baf18684844aabfb311bb56eae3bfb9bc3ca7b53684c14fe2497b8.jpg)

![](images/ae3eb2fbd55bf4e10af1982c8fd322b597d496e8094b219963c1b37be599d651.jpg)  
Figure 6.1 Longitudinal series of blood parameter levels monitored, log (white blood count) [top], log (platelet) [middle], and hematocrit (HCT) [bottom], after a bone marrow transplant ( $n = 9 1$ days).

The model given in (6.1) involving only a single lag is not unduly restrictive. A multivariate model with $m$ lags, such as the VAR( $m$ ) discussed in 5.7, could be developed by replacing the $p \times 1$ state vector, ${ \pmb x } _ { t }$ , by the $p m \times 1$ state vector $\pmb { X } _ { t } = ( \pmb { x } _ { t } ^ { \prime } , \pmb { x } _ { t - 1 } ^ { \prime } , \ldots , \pmb { x } _ { t - m + 1 } ^ { \prime } ) ^ { \prime }$ and the transition matrix by

$$
\Phi = \left( \begin{array}{c c c c c} \Phi_ {1} & \Phi_ {2} & \dots & \Phi_ {m - 1} & \Phi_ {m} \\ I & 0 & \dots & 0 & 0 \\ 0 & I & \dots & 0 & 0 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & I & 0 \end{array} \right). \tag {6.6}
$$

Letting $\mathbf { } W _ { t } = ( \mathbf { \em { w } } _ { t } ^ { \prime } , \mathbf { 0 } ^ { \prime } , \dots , \mathbf { 0 } ^ { \prime } ) ^ { \prime }$ be the new $p m \times 1$ state error vector, the new state equation will be

$$
\boldsymbol {X} _ {t} = \Phi \boldsymbol {X} _ {t - 1} + \boldsymbol {W} _ {t}, \tag {6.7}
$$

where the new matrix “ $Q$ ” now has the form of a $p m \times p m$ matrix with $Q$ i n the upper right-hand corner and zeros elsewhere. The observation equation can then be written as

$$
\boldsymbol {y} _ {t} = \left[ A _ {t} \mid 0 \mid \dots \mid 0 \right] \boldsymbol {X} _ {t} + \boldsymbol {v} _ {t}. \tag {6.8}
$$

This simple recoding shows one way of handling higher order lags within the

![](images/1520ae2e86810029a344bb4c864fe6ab6de0a04522eaed48915f579fc4899f3d.jpg)  
Figure 6.2 Two average global temperature deviations for $n = 1 0 8$ years in degrees centigrade (1880-1987). The solid line is the land-based series whereas the dotted line shows the marine-based series.

context of the single lag structure. Further discussion of this notion is given in §6.6.

The real advantages of the state-space formulation, however, do not really come through in the simple example given above. The special forms that can be developed for various versions of the matrix $A _ { t }$ and for the transition scheme defined by the matrix $\Phi$ allow fitting more parsimonious structures with fewer parameters needed to describe a multivariate time series. We will give some examples of structural models in 6.5, but the simple example shown below is instructive.

# Example 6.2 Global Warming

Figure 6.2 shows two different estimators for the global temperature series from 1880 to 1987, plotted on the same scale. The solid line is considered in the first chapter (see Jones, 1994), which gives average surface air temperature computed from land-based observation stations. The second series (see Parker et al., 1996) gives averages from a number of marinebased stations. Conceptually, both series should be measuring the same underlying climatic signal, and we may consider the problem of extracting this underlying signal. We suppose both series are observing the same signal with different noises; that is,

$$
y _ {t 1} = x _ {t} + v _ {t 1}
$$

and

$$
y _ {t 2} = x _ {t} + v _ {t 2},
$$

where $x _ { t }$ is the unknown common signal. Suppose it can be modeled as a random walk of the form

$$
x _ {t} = x _ {t - 1} + w _ {t}, \tag {6.9}
$$

which we can argue for by noting the stability of the first difference as has been noted in Chapter 2. Furthermore, the first difference of the observed series will be a first-order moving average under this model, arguing from the fact that the first difference of the land-based series has a peak at lag 1. In this example, $p = 1 , q = 2$ , and $\Phi = 1$ is held at a constant value. The observation equation (6.2) can be written in the form

$$
\left( \begin{array}{c} y _ {t 1} \\ y _ {t 2} \end{array} \right) = \left( \begin{array}{c} 1 \\ 1 \end{array} \right) x _ {t} + \left( \begin{array}{c} v _ {t 1} \\ v _ {t 2} \end{array} \right), \tag {6.10}
$$

and we have the covariance matrices given by $Q = q _ { 1 1 }$ and

$$
R = \left( \begin{array}{c c} r _ {1 1} & r _ {1 2} \\ r _ {2 1} & r _ {2 2} \end{array} \right).
$$

The introduction of the state-space approach as a tool for modeling data in the social and biological sciences requires model identification and parameter estimation because there is rarely a well-defined differential equation describing the state transition. The questions of general interest for the dynamic linear model (6.3) and (6.4) relate to estimating the unknown parameters contained in $\Phi , \Upsilon , Q , \Gamma , A _ { t }$ , and $R$ , that define the particular model, and estimating or forecasting values of the underlying unobserved process ${ \pmb x } _ { t }$ . The advantages of the state-space formulation are in the ease with which we can treat various missing data configurations and in the incredible array of models that can be generated from (6.1) and (6.2). The analogy between the observation matrix $A _ { t }$ and the design matrix in the usual regression and analysis of variance setting is a useful one. We can generate fixed and random effect structures that are either constant or vary over time simply by making appropriate choices for the matrix $A _ { t }$ and the transition structure $\Phi$ . We will give only a few examples in this chapter; for further examples, see Durbin and Koopman (2001), Harvey (1993) or Shumway (1988) to mention a few.

Before continuing our investigation of the more complex model, it is instructive to consider a simple univariate state-space model.

# Example 6.3 An AR(1) Process with Observational Noise

Consider a univariate state-space model where the observations satisfy

$$
y _ {t} = x _ {t} + v _ {t}, \tag {6.11}
$$

and the signal (state) is an AR(1) process,

$$
x _ {t} = \phi x _ {t - 1} + w _ {t}, \tag {6.12}
$$

# 6.1: Introduction

for $t = 1 , 2 , \ldots , n$ , where $v _ { t } \sim$ iid $\mathrm { N } ( 0 , \sigma _ { v } ^ { 2 } )$ , $w _ { t } \sim$ iid $\mathrm { N } ( 0 , \sigma _ { w } ^ { 2 } )$ , and $x _ { 0 } \sim$ $\mathrm { N } \big ( 0 , \sigma _ { w } ^ { 2 } / ( 1 - \phi ^ { 2 } ) \big )$ ; $\{ v _ { t } \} , \{ w _ { t } \} , x _ { 0 }$ are independent.

In Chapter 3, we investigated the properties of the state, $x _ { t }$ , because it is a stationary AR(1) process (recall Problem 3.2e). For example, we know the autocovariance function of $x _ { t }$ is

$$
\gamma_ {x} (h) = \frac {\sigma_ {w} ^ {2}}{1 - \phi^ {2}} \phi^ {h}, \quad h = 0, 1, 2, \dots . \tag {6.13}
$$

But, here, we must investigate how the addition of observation noise affects the dynamics. Although it is not a necessary assumption, we have assumed in this example that $x _ { t }$ is stationary. In this case, the observations are also stationary because $y _ { t }$ is the sum of two independent stationary components $x _ { t }$ and $v _ { t }$ . We have

$$
\gamma_ {y} (0) = \operatorname {v a r} \left(y _ {t}\right) = \operatorname {v a r} \left(x _ {t} + v _ {t}\right) = \frac {\sigma_ {w} ^ {2}}{1 - \phi^ {2}} + \sigma_ {v} ^ {2}, \tag {6.14}
$$

and, when $h \geq 1$ ,

$$
\gamma_ {y} (h) = \operatorname {c o v} \left(y _ {t}, y _ {t - h}\right) = \operatorname {c o v} \left(x _ {t} + v _ {t}, x _ {t - h} + v _ {t - h}\right) = \gamma_ {x} (h). \tag {6.15}
$$

Consequently, for $h \geq 1$ , the ACF of the observations is

$$
\rho_ {y} (h) = \frac {\gamma_ {y} (h)}{\gamma_ {y} (0)} = \left(1 + \frac {\sigma_ {v} ^ {2}}{\sigma_ {w} ^ {2}} \left(1 - \phi^ {2}\right)\right) ^ {- 1} \phi^ {h}. \tag {6.16}
$$

It should be clear from the correlation structure given by (6.16) the observations, $y _ { t }$ , are not AR(1) unless $\sigma _ { v } ^ { 2 } = 0$ . In addition, the autocorrelation structure of $y _ { t }$ is identical to the autocorrelation structure of an ARMA(1,1) process, as presented in Example 3.11. Thus, the observations can also be written in an ARMA(1,1) form,

$$
y _ {t} = \phi y _ {t - 1} + \theta u _ {t - 1} + u _ {t},
$$

where $u _ { t }$ is Gaussian white noise with variance $\sigma _ { u } ^ { 2 }$ , and with $\theta$ and $\sigma _ { u } ^ { 2 }$ suitably chosen (see Example 6.11).

Although an equivalence exists between stationary ARMA models and stationary state-space models (see 6.6), it is sometimes easier to work with one form than another. As previously mentioned, in the case of missing data, complex multivariate systems, mixed effects, and certain types of nonstationarity, it is easier to work in the framework of state-space models; in this chapter, we explore some of these situations.

# 6.2 Filtering, Smoothing, and Forecasting

From a practical view, the primary aims of any analysis involving the statespace model as defined by (6.1)-(6.2), or by (6.3)-(6.4), would be to produce estimators for the underlying unobserved signal $\pmb { x } _ { t }$ , given the data $Y _ { s } =$ $\{ \pmb { y } _ { 1 } , \dots , \pmb { y } _ { s } \}$ , to time $s$ . When $s < t$ , the problem is called forecasting or prediction. When $s = t$ , the problem is called filtering, and when $s > t$ , the problem is called smoothing. In addition to these estimates, we would also want to measure their precision. The solution to these problems is accomplished via the Kalman filter and smoother and is the focus of this section.

Throughout this chapter, we will use the following definitions:

$$
\boldsymbol {x} _ {t} ^ {s} = E \left(\boldsymbol {x} _ {t} \mid Y _ {s}\right) \tag {6.17}
$$

and

$$
P _ {t _ {1}, t _ {2}} ^ {s} = E \left\{\left(\boldsymbol {x} _ {t _ {1}} - \boldsymbol {x} _ {t _ {1}} ^ {s}\right) \left(\boldsymbol {x} _ {t _ {2}} - \boldsymbol {x} _ {t _ {2}} ^ {s}\right) ^ {\prime} \right\}. \tag {6.18}
$$

When $t _ { 1 } = t _ { 2 }$ ( $= t$ say) in (6.18), we will write $P _ { t } ^ { s }$ for convenience.

In obtaining the filtering and smoothing equations, we will rely heavily on the Gaussian assumption. Some knowledge of the material covered in Appendix B, $\ S$ B.1, will be helpful in understanding the details of this section (although these details may be skipped on a casual reading of the material). Even in the non-Gaussian case, the estimators we obtain are the minimum mean-squared error estimators within the class of linear estimators. That is, we can think of $E$ in (6.17) as the projection operator in the sense of §B.1 rather than expectation and $P _ { t } ^ { s }$ as the corresponding mean-squared error. When we assume, as in this section, the processes are Gaussian, (6.18) is also the conditional error covariance; that is,

$$
P _ {t _ {1}, t _ {2}} ^ {s} = E \left\{\left(\boldsymbol {x} _ {t _ {1}} - \boldsymbol {x} _ {t _ {1}} ^ {s}\right) \left(\boldsymbol {x} _ {t _ {2}} - \boldsymbol {x} _ {t _ {2}} ^ {s}\right) ^ {\prime} \mid Y _ {s} \right\}.
$$

This fact can be seen, for example, by noting the covariance matrix between $( \pmb { x } _ { t } - \pmb { x } _ { t } ^ { s } )$ and $Y _ { s }$ , for any $t$ and $s$ , is zero; we could say they are orthogonal in the sense of §B.1. This result implies that $( \pmb { x } _ { t } - \pmb { x } _ { t } ^ { s } )$ and $Y _ { s }$ are independent (because of the normality), and hence, the conditional distribution of $( \pmb { x } _ { t } - \pmb { x } _ { t } ^ { s } )$ given $Y _ { s }$ is the unconditional distribution of $( \pmb { x } _ { t } - \pmb { x } _ { t } ^ { s } )$ . Derivations of the filtering and smoothing equations from a Bayesian perspective are given in Meinhold and Singpurwalla (1983); more traditional approaches based on the concept of projection and on multivariate normal distribution theory are given in Jazwinski (1970) and Anderson and Moore (1979).

First, we present the Kalman filter, which gives the filtering and forecasting equations. The name filter comes from the fact that $\mathbf { \Delta } _ { \mathbf { \boldsymbol { x } } _ { t } ^ { t } }$ is a linear filter of the observations $\pmb { y } _ { 1 } , \ldots , \pmb { y } _ { t }$ ; that is, $\begin{array} { r } { \pmb { x } _ { t } ^ { t } = \sum _ { s = 1 } ^ { t } B _ { s } \pmb { y } _ { s } } \end{array}$ for suitably chosen $p \times q$ matrices $B _ { s }$ . The advantage of the Kalman filter is that it specifies how to update the filter from $\pmb { x } _ { t - 1 } ^ { t - 1 }$ to $\mathbf { \Delta } \mathbf { x } _ { t } ^ { t }$ once a new observation ${ \pmb y } _ { t }$ is obtained, without having to reprocess the entire data set $\pmb { y } _ { 1 } , \ldots , \pmb { y } _ { t }$ .

# Property P6.1: The Kalman Filter

For the state-space model specified in (6.3) and (6.4), with initial conditions ${ \pmb x } _ { 0 } ^ { 0 } = { \pmb \mu } _ { 0 }$ and $P _ { 0 } ^ { 0 } = \Sigma _ { 0 }$ , for $t = 1 , \ldots , n$ ,

$$
\boldsymbol {x} _ {t} ^ {t - 1} = \Phi \boldsymbol {x} _ {t - 1} ^ {t - 1} + \Upsilon \boldsymbol {u} _ {t}, \tag {6.19}
$$

$$
P _ {t} ^ {t - 1} = \Phi P _ {t - 1} ^ {t - 1} \Phi^ {\prime} + Q, \tag {6.20}
$$

with

$$
\boldsymbol {x} _ {t} ^ {t} = \boldsymbol {x} _ {t} ^ {t - 1} + K _ {t} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {t - 1} - \Gamma \boldsymbol {u} _ {t}\right), \tag {6.21}
$$

$$
P _ {t} ^ {t} = \left[ I - K _ {t} A _ {t} \right] P _ {t} ^ {t - 1}, \tag {6.22}
$$

where

$$
K _ {t} = P _ {t} ^ {t - 1} A _ {t} ^ {\prime} \left[ A _ {t} P _ {t} ^ {t - 1} A _ {t} ^ {\prime} + R \right] ^ {- 1} \tag {6.23}
$$

is called the Kalman gain. Prediction for $t > n$ is accomplished via (6.19) and (6.20) with initial conditions ${ \pmb x } _ { n } ^ { \pi }$ and $P _ { n } ^ { n }$ .

Proof. The derivations of (6.19) and (6.20) follow from straight forward calculations, because from (6.3) we have

$$
\boldsymbol {x} _ {t} ^ {t - 1} = E (\boldsymbol {x} _ {t} \mid Y _ {t - 1}) = E \left(\Phi \boldsymbol {x} _ {t - 1} + \Upsilon \boldsymbol {u} _ {t} + \boldsymbol {w} _ {t} \mid Y _ {t - 1}\right) = \Phi x _ {t - 1} ^ {t - 1} + \Upsilon \boldsymbol {u} _ {t},
$$

and thus

$$
\begin{array}{l} P _ {t} ^ {t - 1} = E \left\{\left(\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t - 1}\right) \left(\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t - 1}\right) ^ {\prime} \right\} \\ = E \left\{\left[ \Phi \left(\boldsymbol {x} _ {t - 1} - \boldsymbol {x} _ {t - 1} ^ {t - 1}\right) + \boldsymbol {w} _ {t} \right] \left[ \Phi \left(\boldsymbol {x} _ {t - 1} - \boldsymbol {x} _ {t - 1} ^ {t - 1}\right) + \boldsymbol {w} _ {t} \right] ^ {\prime} \right\} \\ = \Phi P _ {t - 1} ^ {t - 1} \Phi^ {\prime} + Q. \\ \end{array}
$$

To derive (6.21), we first define the innovations as

$$
\boldsymbol {\epsilon} _ {t} = \boldsymbol {y} _ {t} - E \left(\boldsymbol {y} _ {t} \mid Y _ {t - 1}\right) = \boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {t - 1} - \Gamma \boldsymbol {u} _ {t}, \tag {6.24}
$$

for $t = 1 , \ldots , n$ . Note, $E ( \epsilon _ { t } ) = \mathbf { 0 }$ and

$$
\Sigma_ {t} \stackrel {\text {d e f}} {=} \operatorname {v a r} \left(\boldsymbol {\epsilon} _ {t}\right) = \operatorname {v a r} \left[ A _ {t} \left(\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t - 1}\right) + \boldsymbol {v} _ {t} \right] = A _ {t} P _ {t} ^ {t - 1} A _ {t} ^ {\prime} + R \tag {6.25}
$$

In addition, $E ( \epsilon _ { t } { \pmb y } _ { s } ^ { \prime } ) = 0$ for $s < t$ , which in view of the fact the innovation sequence is a Gaussian process, implies that the innovations are independent of the past observations. Furthermore, the conditional covariance between ${ \pmb x } _ { t }$ and $\epsilon _ { t }$ given $Y _ { t - 1 }$ is

$$
\begin{array}{l} \operatorname {c o v} \left(\boldsymbol {x} _ {t}, \boldsymbol {\epsilon} _ {t} \mid Y _ {t - 1}\right) = \operatorname {c o v} \left(\boldsymbol {x} _ {t}, \boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {t - 1} - \Gamma \boldsymbol {u} _ {t} \mid Y _ {t - 1}\right) \\ = \operatorname {c o v} \left(\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t - 1}, \boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {t - 1} - \Gamma \boldsymbol {u} _ {t} \mid Y _ {t - 1}\right) \\ = \operatorname {c o v} [ \boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t - 1}, A _ {t} (\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t - 1}) + \boldsymbol {v} _ {t} ] \\ = P _ {t} ^ {t - 1} A _ {t} ^ {\prime}. \tag {6.26} \\ \end{array}
$$

Using these results we have that joint conditional distribution of $\pmb { x } _ { t }$ and $\epsilon _ { t }$ given $Y _ { t - 1 }$ is normal

$$
\binom {\boldsymbol {x} _ {t}} {\boldsymbol {\epsilon} _ {t}} \Big | Y _ {t - 1} \sim \mathrm {N} \left(\left[ \begin{array}{c} \boldsymbol {x} _ {t} ^ {t - 1} \\ \boldsymbol {0} \end{array} \right], \left[ \begin{array}{c c} P _ {t} ^ {t - 1} & P _ {t} ^ {t - 1} A _ {t} ^ {\prime} \\ A _ {t} P _ {t} ^ {t - 1} & \Sigma_ {t} \end{array} \right]\right). \tag {6.27}
$$

Thus, using (B.9) of Appendix B, we can write

$$
\boldsymbol {x} _ {t} ^ {t} = E \left(\boldsymbol {x} _ {t} \mid \boldsymbol {y} _ {1}, \dots , \boldsymbol {y} _ {t - 1}, \boldsymbol {y} _ {t}\right) = E \left(\boldsymbol {x} _ {t} \mid Y _ {t - 1}, \boldsymbol {\epsilon} _ {t}\right) = \boldsymbol {x} _ {t} ^ {t - 1} + K _ {t} \boldsymbol {\epsilon} _ {t}, \tag {6.28}
$$

where

$$
K _ {t} = P _ {t} ^ {t - 1} A _ {t} ^ {\prime} \Sigma_ {t} ^ {- 1} = P _ {t} ^ {t - 1} A _ {t} ^ {\prime} (A _ {t} P _ {t} ^ {t - 1} A _ {t} ^ {\prime} + R) ^ {- 1}.
$$

The evaluation of $P _ { t } ^ { t }$ is easily computed from (6.27) [see (B.10)] as

$$
P _ {t} ^ {t} = \operatorname {c o v} \left(\boldsymbol {x} _ {t} \mid Y _ {t - 1}, \boldsymbol {\epsilon} _ {t}\right) = P _ {t} ^ {t - 1} - P _ {t} ^ {t - 1} A _ {t} ^ {\prime} \Sigma_ {t} ^ {- 1} A _ {t} P _ {t} ^ {t - 1},
$$

which simplifies to (6.22).

Next, we explore the model, prediction, and filtering from a density point of view. For the sake of brevity, consider the Gaussian DLM without inputs, as described in (6.1) and (6.2); that is,

$$
\boldsymbol {x} _ {t} = \Phi \boldsymbol {x} _ {t - 1} + \boldsymbol {w} _ {t} \quad \text {a n d} \quad \boldsymbol {y} _ {t} = A _ {t} \boldsymbol {x} _ {t} + \boldsymbol {v} _ {t}.
$$

Recall ${ \pmb w } _ { t }$ and ${ \pmb v } _ { t }$ are independent, white Gaussian sequences, and the initial state is normal, say, $\pmb { x } _ { 0 } \sim \mathrm { N } ( \pmb { \mu } _ { 0 } , \pmb { \Sigma } _ { 0 } )$ ; we will denote the initial $p$ -variate state normal density by $f _ { 0 } ( { \pmb x } _ { 0 } )$ . Now, letting $p _ { \Theta } ( \cdot )$ denote a generic density function with parameters represented by $\Theta$ , we could describe the state relationship as

$$
p _ {\Theta} \left(\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}, \boldsymbol {x} _ {t - 2}, \dots , \boldsymbol {x} _ {0}\right) = p _ {\Theta} \left(\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}\right) = f _ {w} \left(\boldsymbol {x} _ {t} - \Phi \boldsymbol {x} _ {t - 1}\right), \tag {6.29}
$$

where $f _ { w } ( \cdot )$ denotes the $p$ -variate normal density with mean zero and variancecovariance matrix $Q$ . In (6.29), we are stating the process is Markovian, linear, and Gaussian. The relationship of the observations to the state process is written as

$$
p _ {\Theta} \left(\boldsymbol {y} _ {t} \mid \boldsymbol {x} _ {t}, Y _ {t - 1}\right) = p _ {\Theta} \left(\boldsymbol {y} _ {t} \mid \boldsymbol {x} _ {t}\right) = f _ {v} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right), \tag {6.30}
$$

where $f _ { v } ( \cdot )$ denotes the $q$ -variate normal density with mean zero and variancecovariance matrix $R$ . In (6.30), we are stating the observations are conditionally independent given the state, and the observations are linear and Gaussian. Note, (6.29), (6.30), and the initial density, $f _ { 0 } ( \cdot )$ , completely specify the model in terms of densities, namely,

$$
p _ {\Theta} \left(\boldsymbol {x} _ {0}, \boldsymbol {x} _ {1}, \dots , \boldsymbol {x} _ {n}, \boldsymbol {y} _ {1}, \dots , \boldsymbol {y} _ {n}\right) = f _ {0} \left(\boldsymbol {x} _ {0}\right) \prod_ {t = 1} ^ {n} f _ {w} \left(\boldsymbol {x} _ {t} - \Phi \boldsymbol {x} _ {t - 1}\right) f _ {v} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right), \tag {6.31}
$$

where $\Theta = \{ \pmb { \mu } _ { 0 } , \Sigma _ { 0 } , \Phi , Q , R \}$ .

Given the data, $Y _ { t - 1 } ~ = ~ \{ { \pmb y } _ { 1 } , \dots , { \pmb y } _ { t - 1 } \}$ , and the current filter density, $p _ { \Theta } ( \pmb { x } _ { t - 1 } \mid Y _ { t - 1 } )$ , Property P6.1 tells us, via conditional means and variances, how to recursively generate the Gaussian forecast density, $p _ { \Theta } ( \pmb { x } _ { t } \vert Y _ { t - 1 } )$ , and how to update the density given the current observation, $\mathbf { \mathscr { y } } _ { t }$ , to obtain the Gaussian filter density, $p _ { \Theta } ( \pmb { x } _ { t } \vert Y _ { t } )$ . In terms of densities, the Kalman filter can be seen as a simple Bayesian updating scheme, where, to determine the forecast and filter densities, we have

$$
\begin{array}{l} p _ {\Theta} (\boldsymbol {x} _ {t} \mid Y _ {t - 1}) = \int_ {R ^ {p}} p _ {\Theta} (\boldsymbol {x} _ {t}, \boldsymbol {x} _ {t - 1} \mid Y _ {t - 1}) d \boldsymbol {x} _ {t - 1} \\ = \int_ {R ^ {p}} p _ {\Theta} (\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}) p _ {\Theta} (\boldsymbol {x} _ {t - 1} \mid Y _ {t - 1}) d \boldsymbol {x} _ {t - 1} \\ = \int_ {R ^ {p}} f _ {w} \left(\boldsymbol {x} _ {t} - \Phi \boldsymbol {x} _ {t - 1}\right) p _ {\Theta} \left(\boldsymbol {x} _ {t - 1} \mid Y _ {t - 1}\right) d \boldsymbol {x} _ {t - 1}, \tag {6.32} \\ \end{array}
$$

which simplifies to the $p$ -variate $\mathrm { N } ( \pmb { x } _ { t } ^ { t - 1 } , P _ { t } ^ { t - 1 } )$ density, and

$$
\begin{array}{l} p _ {\Theta} \left(\boldsymbol {x} _ {t} \mid Y _ {t}\right) = p _ {\Theta} \left(\boldsymbol {x} _ {t} \mid \boldsymbol {y} _ {t}, Y _ {t - 1}\right) \\ \propto \quad p _ {\Theta} (\boldsymbol {y} _ {t} \mid \boldsymbol {x} _ {t}) p _ {\Theta} (\boldsymbol {x} _ {t} \mid Y _ {t - 1}), \\ = f _ {v} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) p _ {\Theta} \left(\boldsymbol {x} _ {t} \mid Y _ {t - 1}\right), \tag {6.33} \\ \end{array}
$$

from which we can deduce $p _ { \Theta } ( \pmb { x } _ { t } \ | \ Y _ { t } )$ is the $p$ -variate $\mathrm { N } ( \pmb { x } _ { t } ^ { t } , P _ { t } ^ { t } )$ density. These statements are true for $t = 1 , \ldots , n$ , with initial condition $p _ { \Theta } ( { \pmb x } _ { 0 } \ \vert \ Y _ { 0 } ) =$ $f _ { 0 } ( { \pmb x } _ { 0 } )$ . The prediction and filter recursions of Property P6.1 could also have been calculated directly from the density relationships (6.32) and (6.33) using multivariate normal distribution theory. The following example illustrates the Bayesian updating scheme.

# Example 6.4 Bayesian Analysis of a Local Level Model

In this example, we suppose that we observe a univariate series $y _ { t }$ that consists of a trend component, $\mu _ { t }$ , and a noise component, $v _ { t }$ , where

$$
y _ {t} = \mu_ {t} + v _ {t} \tag {6.34}
$$

and $v _ { t } \sim$ iid $\mathrm { N } ( 0 , \sigma _ { v } ^ { 2 } )$ . In particular, we assume the trend is a random walk given by

$$
\mu_ {t} = \mu_ {t - 1} + w _ {t} \tag {6.35}
$$

where $w _ { t } \sim$ iid $\mathrm { N } ( 0 , \sigma _ { w } ^ { 2 } )$ is independent of $\{ v _ { t } \}$ . Recall Example 6.2, where we suggested this type of trend model for the global temperature series.

The model is, of course, a state-space model with (6.34) being the observation equation, and (6.35) being the state equation. For forecasting, we seek the posterior density $p ( \mu _ { t } \ | \ Y _ { t - 1 } )$ . We will use the following notation introduced in Blight (1974) for the multivariate case. Let

$$
\{x; \mu , \sigma^ {2} \} = \exp \left\{- \frac {1}{2 \sigma^ {2}} (x - \mu) ^ {2} \right\}, \tag {6.36}
$$

then simple manipulation shows

$$
\{x; \mu , \sigma^ {2} \} = \{\mu ; x, \sigma^ {2} \} \tag {6.37}
$$

and

$$
\begin{array}{l} \{x; \mu_ {1}, \sigma_ {1} ^ {2} \} \{x; \mu_ {2}, \sigma_ {2} ^ {2} \} = \left\{x; \frac {\mu_ {1} / \sigma_ {1} ^ {2} + \mu_ {2} / \sigma_ {2} ^ {2}}{1 / \sigma_ {1} ^ {2} + 1 / \sigma_ {2} ^ {2}}, (1 / \sigma_ {1} ^ {2} + 1 / \sigma_ {2} ^ {2}) ^ {- 1} \right\} \\ \times \quad \left\{\mu_ {1}; \mu_ {2}, \sigma_ {1} ^ {2} + \sigma_ {2} ^ {2} \right\}. \tag {6.38} \\ \end{array}
$$

Thus, using (6.32), (6.37) and (6.38) we have

$$
\begin{array}{l} p \left(\mu_ {t} \mid Y _ {t - 1}\right) \propto \int \left\{\mu_ {t}; \mu_ {t - 1}, \sigma_ {w} ^ {2} \right\} \left\{\mu_ {t - 1}; \mu_ {t - 1} ^ {t - 1}, P _ {t - 1} ^ {t - 1} \right\} d \mu_ {t - 1} \\ = \int \left\{\mu_ {t - 1}; \mu_ {t}, \sigma_ {w} ^ {2} \right\} \left\{\mu_ {t - 1}; \mu_ {t - 1} ^ {t - 1}, P _ {t - 1} ^ {t - 1} \right\} d \mu_ {t - 1} \\ = \left\{\mu_ {t}; \mu_ {t - 1} ^ {t - 1}, P _ {t - 1} ^ {t - 1} + \sigma_ {w} ^ {2} \right\}. \tag {6.39} \\ \end{array}
$$

From (6.39) we conclude that

$$
\mu_ {t} \mid Y _ {t - 1} \sim \mathrm {N} \left(\mu_ {t} ^ {t - 1}, P _ {t} ^ {t - 1}\right) \tag {6.40}
$$

where

$$
\mu_ {t} ^ {t - 1} = \mu_ {t - 1} ^ {t - 1} \quad \text {a n d} \quad P _ {t} ^ {t - 1} = P _ {t - 1} ^ {t - 1} + \sigma_ {w} ^ {2} \tag {6.41}
$$

which agrees with the first part of Property P6.1.

To derive the filter density using (6.33) and (6.37) we have

$$
\begin{array}{l} p \left(\mu_ {t} \mid Y _ {t}\right) \propto \left\{y _ {t}; \mu_ {t}, \sigma_ {v} ^ {2} \right\} \left\{\mu_ {t}; \mu_ {t} ^ {t - 1}, P _ {t} ^ {t - 1} \right\} \\ = \left\{\mu_ {t}; y _ {t}, \sigma_ {v} ^ {2} \right\} \left\{\mu_ {t}; \mu_ {t} ^ {t - 1}, P _ {t} ^ {t - 1} \right\}. \tag {6.42} \\ \end{array}
$$

An application of (6.38) gives

$$
\mu_ {t} \mid Y _ {t} \sim \mathrm {N} \left(\mu_ {t} ^ {t}, P _ {t} ^ {t}\right) \tag {6.43}
$$

with

$$
\mu_ {t} ^ {t} = \frac {\sigma_ {v} ^ {2} \mu_ {t} ^ {t - 1}}{P _ {t} ^ {t - 1} + \sigma_ {v} ^ {2}} + \frac {P _ {t} ^ {t - 1} y _ {t}}{P _ {t} ^ {t - 1} + \sigma_ {v} ^ {2}} = \mu_ {t} ^ {t - 1} + K _ {t} \left(y _ {t} - \mu_ {t} ^ {t - 1}\right), \tag {6.44}
$$

where we have defined

$$
K _ {t} = \frac {P _ {t} ^ {t - 1}}{P _ {t} ^ {t - 1} + \sigma_ {v} ^ {2}}, \tag {6.45}
$$

and

$$
P _ {t} ^ {t} = \left(\frac {1}{\sigma_ {v} ^ {2}} + \frac {1}{P _ {t} ^ {t - 1}}\right) ^ {- 1} = \frac {\sigma_ {v} ^ {2} P _ {t} ^ {t - 1}}{P _ {t} ^ {t - 1} + \sigma_ {v} ^ {2}} = (1 - K _ {t}) P _ {t} ^ {t - 1}. \tag {6.46}
$$

The filter for this specific case, of course, agrees with Property P6.1.

Next, we consider the problem of obtaining estimators for ${ \pmb x } _ { t }$ based on the entire data sample $\pmb { y } _ { 1 } , \ldots , \pmb { y } _ { n }$ , where $t \ \leq \ n$ , namely, $\pmb { x } _ { t } ^ { n }$ . These estimators are called smoothers because a time plot of the sequence $\{ \pmb { x } _ { t } ^ { n }$ ; $t = 1 , \ldots , n \}$ is typically smoother than the forecasts $\{ \pmb { x } _ { t } ^ { t - 1 }$ ; $t = 1 , \ldots , n \}$ or the filters $\{ \pmb { x } _ { t } ^ { t } ; ~ t = 1 , . . . , n \}$ . As is obvious from the above remarks, smoothing implies that each estimated value is a function of the present, future, and past, whereas the filtered estimator depends on the present and past. The forecast depends only on the past, as usual.

# Property P6.2: The Kalman Smoother

For the state-space model specified in (6.3) and (6.4), with initial conditions ${ \pmb x } _ { n } ^ { \pi }$ and $P _ { n } ^ { n }$ obtained via Property P6.1, for $t = n , n - 1 , \ldots , 1$ ,

$$
\boldsymbol {x} _ {t - 1} ^ {n} = \boldsymbol {x} _ {t - 1} ^ {t - 1} + J _ {t - 1} \left(\boldsymbol {x} _ {t} ^ {n} - \boldsymbol {x} _ {t} ^ {t - 1}\right), \tag {6.47}
$$

$$
P _ {t - 1} ^ {n} = P _ {t - 1} ^ {t - 1} + J _ {t - 1} \left(P _ {t} ^ {n} - P _ {t} ^ {t - 1}\right) J _ {t - 1} ^ {\prime}, \tag {6.48}
$$

where

$$
J _ {t - 1} = P _ {t - 1} ^ {t - 1} \Phi^ {\prime} \left[ P _ {t} ^ {t - 1} \right] ^ {- 1}. \tag {6.49}
$$

Proof. The smoother can be derived in many ways. Here we provide a proof that was given in Ansley and Kohn (1982). First, for $1 \leq t \leq n$ , define

$$
Y _ {t - 1} = \left\{\boldsymbol {y} _ {1}, \dots , \boldsymbol {y} _ {t - 1} \right\} \quad \text {a n d} \quad \eta_ {t} = \left\{\boldsymbol {v} _ {t}, \dots , \boldsymbol {v} _ {n}, \boldsymbol {w} _ {t + 1}, \dots , \boldsymbol {w} _ {n} \right\},
$$

with $Y _ { 0 }$ being empty, and let

$$
\boldsymbol {q} _ {t - 1} = E \{\boldsymbol {x} _ {t - 1} \mid Y _ {t - 1}, \boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t - 1}, \eta_ {t} \}.
$$

Then, because $Y _ { t - 1 }$ , $\{ \pmb { x } _ { t } - \pmb { x } _ { t } ^ { t - 1 } \}$ , and $\eta _ { t }$ are mutually independent, and ${ \pmb x } _ { t - 1 }$ and $\eta _ { t }$ are independent, using (B.9) we have

$$
\boldsymbol {q} _ {t - 1} = \boldsymbol {x} _ {t - 1} ^ {t - 1} + J _ {t - 1} \left(\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t - 1}\right), \tag {6.50}
$$

where

$$
J _ {t - 1} = \mathrm {c o v} (\pmb {x} _ {t - 1}, \pmb {x} _ {t} - \pmb {x} _ {t} ^ {t - 1}) [ P _ {t} ^ {t - 1} ] ^ {- 1} = P _ {t - 1} ^ {t - 1} \Phi^ {\prime} [ P _ {t} ^ {t - 1} ] ^ {- 1}.
$$

Finally, because $Y _ { t - 1 }$ , ${ \pmb x } _ { t } - { \pmb x } _ { t } ^ { t - 1 }$ , and $\eta _ { t }$ generate $Y _ { n } = \{ { \pmb y } _ { 1 } , \dots , { \pmb y } _ { n } \}$ ,

$$
x _ {t - 1} ^ {n} = E \{x _ {t - 1} \mid Y _ {n} \} = E \{\pmb {q} _ {t - 1} \mid Y _ {n} \} = \pmb {x} _ {t - 1} ^ {t - 1} + J _ {t - 1} (\pmb {x} _ {t} ^ {n} - \pmb {x} _ {t} ^ {t - 1}),
$$

which establishes (6.47).

The recursion for the error covariance, $P _ { t - 1 } ^ { n }$ , is obtained by straight-forward calculation. Using (6.47) we obtain

$$
\pmb {x} _ {t - 1} - \pmb {x} _ {t - 1} ^ {n} = \pmb {x} _ {t - 1} - \pmb {x} _ {t - 1} ^ {t - 1} - J _ {t - 1} \left(\pmb {x} _ {t} ^ {n} - \Phi \pmb {x} _ {t - 1} ^ {t - 1}\right),
$$

or

$$
\left(\boldsymbol {x} _ {t - 1} - \boldsymbol {x} _ {t - 1} ^ {n}\right) + J _ {t - 1} \boldsymbol {x} _ {t} ^ {n} = \left(\boldsymbol {x} _ {t - 1} - \boldsymbol {x} _ {t - 1} ^ {t - 1}\right) + J _ {t - 1} \Phi \boldsymbol {x} _ {t - 1} ^ {t - 1}. \tag {6.51}
$$

Multiplying each side of (6.51) by the transpose of itself and talking expectation, we have

$$
P _ {t - 1} ^ {n} + J _ {t - 1} E \left(\boldsymbol {x} _ {t} ^ {n} \boldsymbol {x} _ {t} ^ {n ^ {\prime}}\right) J _ {t - 1} ^ {\prime} = P _ {t - 1} ^ {t - 1} + J _ {t - 1} \Phi E \left(\boldsymbol {x} _ {t - 1} ^ {t - 1} \boldsymbol {x} _ {t - 1} ^ {t - 1 ^ {\prime}}\right) \Phi^ {\prime} J _ {t - 1} ^ {\prime}, \tag {6.52}
$$

using the fact the cross-product terms are zero. But,

$$
E \left(\boldsymbol {x} _ {t} ^ {n} \boldsymbol {x} _ {t} ^ {n ^ {\prime}}\right) = E \left(\boldsymbol {x} _ {t} \boldsymbol {x} _ {t} ^ {\prime}\right) - P _ {t} ^ {n} = \Phi E \left(\boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) \Phi^ {\prime} + Q - P _ {t} ^ {n},
$$

and

$$
E \left(\boldsymbol {x} _ {t - 1} ^ {t - 1} \boldsymbol {x} _ {t - 1} ^ {t - 1 ^ {\prime}}\right) = E \left(\boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) - P _ {t - 1} ^ {t - 1},
$$

so (6.52) simplifies to (6.48).

# Example 6.5 Prediction, Filtering and Smoothing for the Local Level Model

For this example, we simulated $n = 5 0$ observations from the local level trend model discussed in Example 6.4. We generated a random walk

$$
\mu_ {t} = \mu_ {t - 1} + w _ {t} \tag {6.53}
$$

with $w _ { t } \sim$ iid $\mathrm { { N } } ( 0 , 1 )$ and $\mu _ { 0 } \sim \mathrm { N } ( 0 , 1 )$ . We then supposed that we observe a univariate series $y _ { t }$ consisting of the trend component, $\mu _ { t }$ , and a noise component, $v _ { t } \sim$ iid $\mathrm { { N } } ( 0 , 1 )$ , where

$$
y _ {t} = \mu_ {t} + v _ {t}. \tag {6.54}
$$

The sequences $\{ w _ { t } \}$ , $\{ v _ { t } \}$ and $\mu _ { 0 }$ were generated independently. We then ran the Kalman filter and smoother, Properties P6.1 and P6.2, using the actual parameters. The top panel of Figure 6.3 shows the actual values of $\mu _ { t }$ as points, and the predictions $\mu _ { t } ^ { t - 1 }$ superimposed on the graph as a line. In addition, we display $\mu _ { t } ^ { t - 1 } \pm 2 \sqrt { P _ { t } ^ { t - 1 } }$ as dashed lines on the plot. The middle panel displays the filters, $\mu _ { t } ^ { t }$ as a line with $\mu _ { t } ^ { t } \pm 2 \sqrt { P _ { t } ^ { t } }$ as dashed lines. The bottom panel of Figure 6.3 shows a similar plot for the smoothers $\mu _ { t } ^ { \pi }$ .

Table 6.1 shows the first 10 observations as well as the corresponding state values, the predictions, filters and smoothers. Note that in Table 6.1, one-step-ahead prediction is more uncertain than the corresponding filtered value, which, in turn, is more uncertain than the corresponding smoother value (that is $P _ { t } ^ { t - 1 } > P _ { t } ^ { t } > P _ { t } ^ { n }$ ). Also, in each case, the error variances stabilize quickly. The R code for this example may be found on the website for the text.

![](images/5c6b59389ee0e4564e0ac85b27299c5a324db65fc7053498a93e003c697a4a37.jpg)

![](images/b394cc902452a1a748e04ee70cc9cb3f7ea259f80cad0d32ee76203b944d63eb.jpg)

![](images/57adeb40ce189ae5cc95f64f6139f86e77cdd43d90755fc0b6c38df2c90d8fce.jpg)  
Figure 6.3 Displays for Example 6.5. The simulated values of $1 , \ldots , 5 0$ , given by (6.53) are shown as points. Top: The predi $\mu _ { t }$ , for ons $\mu _ { t } ^ { t - 1 }$ $t =$ obtained via the Kalman filter are shown as a line. Error bounds, $\mu _ { t } ^ { t - 1 } \pm$ $2 \sqrt { P _ { t } ^ { t - 1 } }$ , are shown as dashed lines. Middle: The filter $\mu _ { t } ^ { t }$ obtained via the Kalman filter are shown as a line. Error bounds, $\mu _ { t } ^ { t } \pm 2 \sqrt { P _ { t } ^ { t } }$ , are shown as dashed lines. Bottom: The smoothers $\mu _ { t } ^ { \pi }$ obtained via the Kalman smoother are shown as a line. Error bounds, $\mu _ { t } ^ { n } \pm 2 \sqrt { P _ { t } ^ { n } }$ , are shown as dashed lines.

In the next section, we will need a set of recursions for obtaining $P _ { t , t - 1 } ^ { n }$ , as defined in (6.18). We give the necessary recursion in the following property.

# Property P6.3: The Lag-One Covariance Smoother

For the state-space model specified in (6.3) and (6.4), with $K _ { t }$ , $J _ { t }$ $( t = 1 , \ldots , n )$ , and $P _ { n } ^ { \pi }$ obtained from Properties P6.1 and P6.2, and with initial condition

$$
P _ {n, n - 1} ^ {n} = \left(I - K _ {n} A _ {n}\right) \Phi P _ {n - 1} ^ {n - 1}, \tag {6.55}
$$

Table 6.1 Forecasts, Filters, and Smoothers for Example 6.5.   

<table><tr><td>t</td><td>yt</td><td>μt</td><td>μt-1</td><td>Pt-1</td><td>μt</td><td>Pt</td><td>μtn</td><td>Ptn</td></tr><tr><td>0</td><td>—</td><td>-.63</td><td>—</td><td>—</td><td>.00</td><td>1.00</td><td>-.32</td><td>.62</td></tr><tr><td>1</td><td>-1.05</td><td>-.44</td><td>.00</td><td>2.00</td><td>-.70</td><td>.67</td><td>-.65</td><td>.47</td></tr><tr><td>2</td><td>-.94</td><td>-1.28</td><td>-.70</td><td>1.67</td><td>-.85</td><td>.63</td><td>-.57</td><td>.45</td></tr><tr><td>3</td><td>-.81</td><td>.32</td><td>-.85</td><td>1.63</td><td>-.83</td><td>.62</td><td>-.11</td><td>.45</td></tr><tr><td>4</td><td>2.08</td><td>.65</td><td>-.83</td><td>1.62</td><td>.97</td><td>.62</td><td>1.04</td><td>.45</td></tr><tr><td>5</td><td>1.81</td><td>-.17</td><td>.97</td><td>1.62</td><td>1.49</td><td>.62</td><td>1.16</td><td>.45</td></tr><tr><td>6</td><td>-.05</td><td>.31</td><td>1.49</td><td>1.62</td><td>.53</td><td>.62</td><td>.63</td><td>.45</td></tr><tr><td>7</td><td>.01</td><td>1.05</td><td>.53</td><td>1.62</td><td>.21</td><td>.62</td><td>.78</td><td>.45</td></tr><tr><td>8</td><td>2.20</td><td>1.63</td><td>.21</td><td>1.62</td><td>1.44</td><td>.62</td><td>1.70</td><td>.45</td></tr><tr><td>9</td><td>1.19</td><td>1.32</td><td>1.44</td><td>1.62</td><td>1.28</td><td>.62</td><td>2.12</td><td>.45</td></tr><tr><td>10</td><td>5.24</td><td>2.83</td><td>1.28</td><td>1.62</td><td>3.73</td><td>.62</td><td>3.48</td><td>.45</td></tr></table>

$$
f o r t = n, n - 1, \dots , 2,
$$

$$
P _ {t - 1, t - 2} ^ {n} = P _ {t - 1} ^ {t - 1} J _ {t - 2} ^ {\prime} + J _ {t - 1} \left(P _ {t, t - 1} ^ {n} - \Phi P _ {t - 1} ^ {t - 1}\right) J _ {t - 2} ^ {\prime}. \tag {6.56}
$$

Proof. Because we are computing covariances, we may assume $\mathbf { \boldsymbol { u } } _ { t } \equiv \mathbf { \boldsymbol { 0 } }$ without loss of generality. To derive the initial term (6.55), we first define

$$
\widetilde {\boldsymbol {x}} _ {t} ^ {s} = \boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {s}.
$$

Then, using (6.21) and (6.47), we write

$$
\begin{array}{l} P _ {t, t - 1} ^ {t} = E \left(\widetilde {\boldsymbol {x}} _ {t} ^ {t} \widetilde {\boldsymbol {x}} _ {t - 1} ^ {t ^ {\prime}}\right) \\ = E \left\{\left[ \widetilde {\boldsymbol {x}} _ {t} ^ {t - 1} - K _ {t} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {t - 1}\right) \right] \left[ \widetilde {\boldsymbol {x}} _ {t - 1} ^ {t - 1} - J _ {t - 1} K _ {t} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {t - 1}\right) \right] ^ {\prime} \right\} \\ = E \left\{\left[ \widetilde {\boldsymbol {x}} _ {t} ^ {t - 1} - K _ {t} \left(A _ {t} \widetilde {\boldsymbol {x}} _ {t} ^ {t - 1} + \boldsymbol {v} _ {t}\right) \right] \left[ \widetilde {\boldsymbol {x}} _ {t - 1} ^ {t - 1} - J _ {t - 1} K _ {t} \left(A _ {t} \widetilde {\boldsymbol {x}} _ {t} ^ {t - 1} + \boldsymbol {v} _ {t}\right) \right] ^ {\prime} \right\}. \\ \end{array}
$$

Expanding terms and taking expectation, we arrive at

$$
P _ {t, t - 1} ^ {t} = P _ {t, t - 1} ^ {t - 1} - P _ {t} ^ {t - 1} A _ {t} ^ {\prime} K _ {t} ^ {\prime} J _ {t - 1} ^ {\prime} - K _ {t} A _ {t} P _ {t, t - 1} ^ {t - 1} + K _ {t} (A _ {t} P _ {t} ^ {t - 1} A _ {t} ^ {\prime} + R) K _ {t} ^ {\prime} J _ {t - 1} ^ {\prime},
$$

noting $E ( \widetilde { \pmb { x } } _ { t } ^ { t - 1 } \pmb { v } _ { t } ^ { \prime } ) = \pmb { 0 }$ . The final simplification occurs by realizing that $K _ { t } ( A _ { t } P _ { t } ^ { t - 1 } A _ { t } ^ { \prime } + R ) = P _ { t } ^ { t - 1 } A _ { t } ^ { \prime }$ , and $P _ { t , t - 1 } ^ { t - 1 } = \Phi P _ { t - 1 } ^ { t - 1 }$ . These relationships hold for any $t = 1 , \ldots , n$ , and (6.55) is the case $t = n$ .

We give the basic steps in the derivation of (6.56). The first step is to use (6.47) to write

$$
\widetilde {\boldsymbol {x}} _ {t - 1} ^ {n} + J _ {t - 1} \boldsymbol {x} _ {t} ^ {n} = \widetilde {\boldsymbol {x}} _ {t - 1} ^ {t - 1} + J _ {t - 1} \Phi \boldsymbol {x} _ {t - 1} ^ {t - 1} \tag {6.57}
$$

and

$$
\widetilde {\boldsymbol {x}} _ {t - 2} ^ {n} + J _ {t - 2} \boldsymbol {x} _ {t - 1} ^ {n} = \widetilde {\boldsymbol {x}} _ {t - 2} ^ {t - 2} + J _ {t - 2} \Phi \boldsymbol {x} _ {t - 2} ^ {t - 2}. \tag {6.58}
$$

Next, multiply the left-hand side of (6.57) by the transpose of the left-hand side of (6.58), and equate that to the corresponding result of the right-hand sides of (6.57) and (6.58). Then, taking expectation of both sides, the left-hand side result reduces to

$$
P _ {t - 1, t - 2} ^ {n} + J _ {t - 1} E \left(\boldsymbol {x} _ {t} ^ {n} \boldsymbol {x} _ {t - 1} ^ {n ^ {\prime}}\right) J _ {t - 2} ^ {\prime} \tag {6.59}
$$

and the right-hand side result reduces to

$$
\begin{array}{l} P _ {t - 1, t - 2} ^ {t - 2} - K _ {t - 1} A _ {t - 1} P _ {t - 1, t - 2} ^ {t - 2} + J _ {t - 1} \Phi K _ {t - 1} A _ {t - 1} P _ {t - 1, t - 2} ^ {t - 2} \\ + J _ {t - 1} \Phi E \left(\boldsymbol {x} _ {t - 1} ^ {t - 1} \boldsymbol {x} _ {t - 2} ^ {t - 2 ^ {\prime}}\right) \Phi^ {\prime} J _ {t - 2} ^ {\prime}. \tag {6.60} \\ \end{array}
$$

In (6.59), write

$$
E (\pmb {x} _ {t} ^ {n} \pmb {x} _ {t - 1} ^ {n ^ {\prime}}) = E (\pmb {x} _ {t} \pmb {x} _ {t - 1} ^ {\prime}) - P _ {t, t - 1} ^ {n} = \Phi E (\pmb {x} _ {t - 1} \pmb {x} _ {t - 2} ^ {\prime}) \Phi^ {\prime} + \Phi Q - P _ {t, t - 1} ^ {n},
$$

and in (6.60), write

$$
E (\pmb {x} _ {t - 1} ^ {t - 1} \pmb {x} _ {t - 2} ^ {t - 2 ^ {\prime}}) = E (\pmb {x} _ {t - 1} ^ {t - 2} \pmb {x} _ {t - 2} ^ {t - 2 ^ {\prime}}) = E (\pmb {x} _ {t - 1} \pmb {x} _ {t - 2} ^ {\prime}) - P _ {t - 1, t - 2} ^ {t - 2}.
$$

Equating (6.59) to (6.60) using these relationships and simplifying the result leads to (6.56). ■

# 6.3 Maximum Likelihood Estimation

The estimation of the parameters that specify the state-space model, (6.3) and (6.4), is quite involved. We use $\Theta = \{ \pmb { \mu } _ { 0 } , \Sigma _ { 0 } , \Phi , Q , R , \Upsilon , \Gamma \}$ to represent the vector of parameters containing the elements of the initial mean and covariance and $\Sigma _ { 0 }$ , the transition matrix $\Phi$ , and the state and observation $\pmb { \mu } _ { \mathrm { 0 } }$ covariance matrices $Q$ and $R$ and the input coefficient matrices, $\Upsilon$ and $\Gamma$ . We use maximum likelihood under the assumption that the initial state is normal, $\pmb { x } _ { 0 } \sim \mathrm { N } ( \pmb { \mu } _ { 0 } , \pmb { \Sigma } _ { 0 } )$ , and the errors ${ \pmb w } _ { 1 } , \ldots , { \pmb w } _ { n }$ and $\pmb { v } _ { 1 } , \ldots , \pmb { v } _ { n }$ are jointly normal and uncorrelated vector variables. We continue to assume, for simplicity, $\{ { \pmb w } _ { t } \}$ and $\left\{ \pmb { v } _ { t } \right\}$ are uncorrelated.

The likelihood is computed using the innovations $\epsilon _ { 1 } , \epsilon _ { 2 } , \ldots , \epsilon _ { n }$ , defined by (6.24),

$$
\boldsymbol {\epsilon} _ {t} = \boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {t - 1} - \Gamma \boldsymbol {u} _ {t}.
$$

The innovations form of the likelihood function, which was first given by Schweppe (1965), is obtained using an argument similar to the one leading to (3.105) and proceeds by noting the innovations are independent Gaussian random vectors with zero means and, as shown in (6.25), covariance matrices

$$
\Sigma_ {t} = A _ {t} P _ {t} ^ {t - 1} A _ {t} ^ {\prime} + R. \tag {6.61}
$$

Hence, ignoring a constant, we may write the likelihood, $L _ { Y } ( \Theta )$ , as

$$
- \ln L _ {Y} (\Theta) = \frac {1}{2} \sum_ {t = 1} ^ {n} \log | \Sigma_ {t} (\Theta) | + \frac {1}{2} \sum_ {t = 1} ^ {n} \boldsymbol {\epsilon} _ {t} (\Theta) ^ {\prime} \Sigma_ {t} (\Theta) ^ {- 1} \boldsymbol {\epsilon} _ {t} (\Theta), \tag {6.62}
$$

where we have emphasized the dependence of the innovations on the parameters $\Theta$ . Of course, (6.62) is a highly nonlinear and complicated function of the unknown parameters. The usual procedure is to fix $\pmb { x } _ { 0 }$ and then develop a set of recursions for the log likelihood function and its first two derivatives (for example, Gupta and Mehra, 1974). Then, a Newton–Raphson algorithm (see Example 3.28) can be used successively to update the parameter values until the negative of the log likelihood is minimized. This approach is advocated, for example, by Jones (1980), who developed ARMA estimation by putting the ARMA model in state-space form. For the univariate case, (6.62) is identical, in form, to the likelihood for the ARMA model given in (3.105).

The steps involved in performing a Newton–Raphson estimation procedure are as follows.

1. Select initial values for the parameters, say, $\Theta ^ { ( 0 ) }$

2. Run the Kalman filter, Property P6.1, using the initial parameter values, $\Theta ^ { ( 0 ) }$ , to obtain a set of innovations and error covariances, say, $\{ \epsilon _ { t } ^ { ( 0 ) } ; t =$ $1 , \ldots , n \}$ and $\{ \Sigma _ { t } ^ { ( 0 ) } ; \ t = 1 , \ldots , n \}$ .

3. Run one iteration of a Newton–Raphson procedure with $- \ln L _ { Y } ( \Theta )$ as the criterion function (refer to Example 3.28 for details), to obtain a new set of estimates, say $\Theta ^ { ( 1 ) }$ .

4. At iteration obtain a new set of innovation values $j$ , $( j = 1 , 2 , \ldots )$ , repeat step 2 using $\{ \epsilon _ { t } ^ { ( j ) }$ ; $t = 1 , \ldots , n \}$ $\Theta ^ { ( j ) }$ in place of and $\{ \Sigma _ { t } ^ { ( j ) }$ $\Theta ^ { ( j - 1 ) }$ ; $t =$ t o $1 , \ldots , n \}$ { t }. Then repeat step 3 to obtain a new estimate $\Theta ^ { ( j + 1 ) }$ . Stop when the estimates or the likelihood stabilize; for example, stop when the values of $\Theta ^ { ( j + 1 ) }$ differ from $\Theta ^ { ( j ) }$ , or when $L _ { Y } ( \Theta ^ { ( j + 1 ) } )$ differs from $L _ { Y } ( \Theta ^ { ( j ) } )$ , by some predetermined, but small amount.

# Example 6.6 Newton–Raphson for Example 6.3

In this example, we generated $n = 1 0 0$ observations, $y _ { 1 } , \ldots , y _ { 1 0 0 }$ , using the model in Example 6.3, to perform a Newton–Raphson estimation of the parameters $\phi$ , $\sigma _ { w } ^ { 2 }$ , and $\sigma _ { v } ^ { 2 }$ . In the notation of 6.2, we would have $\Phi = \phi$ , $Q = \sigma _ { w } ^ { 2 }$ and $R = \sigma _ { v } ^ { 2 }$ . The actual values of the parameters are $\phi = . 8$ , $\sigma _ { w } ^ { 2 } = \sigma _ { v } ^ { 2 } = 1$ .

In the simple case of an AR(1) with observational noise, initial estimation can be accomplished using the results of Example 6.3. For example, using (6.16), we set

$$
\phi^ {(0)} = \widehat {\rho} _ {y} (2) / \widehat {\rho} _ {y} (1).
$$

Similarly, from (6.15), $\gamma _ { x } ( 1 ) = \gamma _ { y } ( 1 ) = \phi \sigma _ { w } ^ { 2 } / ( 1 - \phi ^ { 2 } )$ , so that, initially, we set

$$
\sigma_ {w} ^ {2 ^ {(0)}} = (1 - \phi^ {(0) 2}) \widehat {\gamma} _ {y} (1) / \phi^ {(0)}.
$$

Finally, using (6.14) we obtain an initial estimate of $\sigma _ { v } ^ { 2 }$ , namely,

$$
\sigma_ {v} ^ {2 ^ {(0)}} = \widehat {\gamma} _ {y} (0) - [ \sigma_ {w} ^ {2 ^ {(0)}} / (1 - \phi^ {(0) ^ {2}}) ].
$$

Newton–Raphson estimation was accomplished using the R program optim. The code used for this example can be obtained on the website for the text. In that program, we must provide an evaluation of the function to be minimized, namely, $- \ln L _ { Y } ( \Theta )$ . In this case, the “function call” combines steps 2 and 3, using the current values of the parameters, $\Theta ^ { ( j - 1 ) }$ , to obtain first the filtered values, then the innovation values, and then calculating the criterion function, $- \ln { \cal L } _ { Y } \big ( \Theta ^ { ( j - 1 ) } \big )$ , to be minimized. We can also provide analytic forms of the gradient or score vector, $- \partial \ln { \cal L } _ { Y } ( \Theta ) / \partial \Theta$ , and the Hessian matrix, $- \partial ^ { 2 } \ln L _ { Y } ( \Theta ) / \partial \Theta \partial \Theta ^ { \prime }$ , in the optimization routine, or allow the program to calculate these values numerically. In this example, we let the program proceed numerically and we note the need to be cautious when calculating gradients numerically. For better stability, we can also provide an iterative solution for obtaining analytic gradients and Hessians of the log likelihood function; for details, see Problems 6.11 and 6.12; also, see Gupta and Mehra (1974). The final estimates, along with their standard errors (in parentheses), were

$$
\widehat {\phi} = . 8 1 (. 0 8), \quad \widehat {\sigma} _ {w} = . 8 5 (. 1 7), \quad \widehat {\sigma} _ {v} = . 8 7 (. 1 4),
$$

and the algorithm converged in seven steps. The standard errors are a byproduct of the estimation procedure, and we will discuss their evaluation later in this section, after Property P6.4.

# Example 6.7 Newton–Raphson for the Global Temperature Series in Example 6.2

In Example 6.2 we considered two different global temperature series of $n = 1 0 8$ observations each, and they are plotted in Figure 6.2. In that example, we argued that both series should be measuring the same underlying climatic signal, $x _ { t }$ , which we model as a random walk,

$$
x _ {t} = x _ {t - 1} + w _ {t}.
$$

Recall that the observation equation was written as

$$
\left( \begin{array}{c} y _ {t 1} \\ y _ {t 2} \end{array} \right) = \left( \begin{array}{c} 1 \\ 1 \end{array} \right) x _ {t} + \left( \begin{array}{c} v _ {t 1} \\ v _ {t 2} \end{array} \right),
$$

![](images/71635f16973f9a1b9c1895629b290f8a66ee9ce1674b3e583e7aac4d735b7d37.jpg)  
Figure 6.4 Plot for Example 6.7. The thin solid and dashed lines are the two average global temperature deviations shown in Figure 6.2. The thick solid line is the estimated smoother $\widehat { x } _ { t } ^ { n }$ .

and the model covariance matrices are given by $Q = q _ { 1 1 }$ and

$$
R = \left( \begin{array}{c c} r _ {1 1} & r _ {1 2} \\ r _ {2 1} & r _ {2 2} \end{array} \right).
$$

Hence, there are four parameters to estimate, namely $q _ { 1 1 } , r _ { 1 1 } , r _ { 1 2 } , r _ { 2 2 }$ , noting that $r _ { 2 1 } = r _ { 1 2 }$ We hold the the initial state parameters fixed in this example at $\mu _ { 0 } = - . 3 5$ and $\Sigma _ { 0 } = . 0 1$ (these are, approximately, the mean and variance of the first observation in each series).

The final estimates are ${ \widehat { q } } _ { 1 1 } = . 0 5 , { \widehat { r } } _ { 1 1 } = . 0 1 9 , { \widehat { r } } _ { 1 2 } = . 0 0 6 , { \widehat { r } } _ { 2 2 } = . 0 0 5$ , with all values being significant. The observations and the smoothed estimate of the signal, $\widehat { x } _ { t } ^ { n }$ , are displayed in Figure 6.4.

In addition to Newton–Raphson, Shumway and Stoffer (1982) presented a conceptually simpler estimation procedure based on the EM (expectationmaximization) algorithm (Dempster et al., 1977). For the sake of brevity, we ignore the inputs and consider the model in the form of (6.1) and (6.2); the general case is left as an exercise (Problem 6.9). The basic idea is that if we could observe the states, $X _ { n } = \{ \pmb { x } _ { 0 } , \pmb { x } _ { 1 } , \ldots , \pmb { x } _ { n } \}$ , in addition to the observations $Y _ { n } = \{ { \pmb y } _ { 1 } , \dots , { \pmb y } _ { n } \}$ , then we would consider $\{ X _ { n } , Y _ { n } \}$ as the complete data, with

the joint density

$$
f _ {\Theta} \left(X _ {n}, Y _ {n}\right) = f _ {\mu_ {0}, \Sigma_ {0}} \left(\boldsymbol {x} _ {0}\right) \prod_ {t = 1} ^ {n} f _ {\Phi , Q} \left(\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}\right) \prod_ {t = 1} ^ {n} f _ {R} \left(\boldsymbol {y} _ {t} \mid \boldsymbol {x} _ {t}\right). \tag {6.63}
$$

Under the Gaussian assumption and ignoring constants, the complete data likelihood, (6.63), can be written as

$$
\begin{array}{l} - 2 \ln L _ {X, Y} (\Theta) = \ln | \Sigma_ {0} | + \left(\boldsymbol {x} _ {0} - \boldsymbol {\mu} _ {0}\right) ^ {\prime} \Sigma_ {0} ^ {- 1} \left(\boldsymbol {x} _ {0} - \boldsymbol {\mu} _ {0}\right) \\ + \quad n \ln | Q | + \sum_ {t = 1} ^ {n} (\boldsymbol {x} _ {t} - \Phi \boldsymbol {x} _ {t - 1}) ^ {\prime} Q ^ {- 1} (\boldsymbol {x} _ {t} - \Phi \boldsymbol {x} _ {t - 1}) \\ + \quad n \ln | R | + \sum_ {t = 1} ^ {n} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) ^ {\prime} R ^ {- 1} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right). \tag {6.64} \\ \end{array}
$$

Thus, in view of (6.64), if we did have the complete data, we could then use the results from multivariate normal theory to easily obtain the MLEs of $\Theta$ . We do not have the complete data; however, the EM algorithm gives us an iterative method for finding the MLEs of $\Theta$ based on the incomplete data, $Y _ { n }$ , by successively maximizing the conditional expectation of the complete data likelihood. To implement the EM algorithm, we write, at iteration $j$ , $( j = 1 , 2 , \dots ,$ ),

$$
Q (\Theta \mid \Theta^ {(j - 1)}) = E \left\{- 2 \ln L _ {X, Y} (\Theta) \mid Y _ {n}, \Theta^ {(j - 1)} \right\}. \tag {6.65}
$$

Calculation of (6.65) is the expectation step. Of course, given the current value of the parameters, $\Theta ^ { ( j - 1 ) }$ , we can use Property P6.2 to obtain the desired conditional expectations as smoothers. This property yields

$$
\begin{array}{l} Q \left(\Theta \mid \Theta^ {(j - 1)}\right) = \ln | \Sigma_ {0} | + \operatorname {t r} \left\{\Sigma_ {0} ^ {- 1} \left[ P _ {0} ^ {n} + \left(\boldsymbol {x} _ {0} ^ {n} - \boldsymbol {\mu} _ {0}\right) \left(\boldsymbol {x} _ {0} ^ {n} - \boldsymbol {\mu} _ {0}\right) ^ {\prime} \right] \right\} \\ + \quad n \ln | Q | + \operatorname {t r} \left\{Q ^ {- 1} \left[ S _ {1 1} - S _ {1 0} \Phi^ {\prime} - \Phi S _ {1 0} ^ {\prime} + \Phi S _ {0 0} \Phi^ {\prime} \right] \right\} \\ + \quad n \ln | R | \tag {6.66} \\ + \quad \operatorname {t r} \left\{R ^ {- 1} \sum_ {t = 1} ^ {n} \left[ \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {n}\right) \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {n}\right) ^ {\prime} + A _ {t} P _ {t} ^ {n} A _ {t} ^ {\prime} \right] \right\}, \\ \end{array}
$$

where

$$
S _ {1 1} = \sum_ {t = 1} ^ {n} \left(\boldsymbol {x} _ {t} ^ {n} \boldsymbol {x} _ {t} ^ {n \prime} + P _ {t} ^ {n}\right), \tag {6.67}
$$

$$
S _ {1 0} = \sum_ {t = 1} ^ {n} \left(\boldsymbol {x} _ {t} ^ {n} \boldsymbol {x} _ {t - 1} ^ {n} ^ {\prime} + P _ {t, t - 1} ^ {n}\right), \tag {6.68}
$$

and

$$
S _ {0 0} = \sum_ {t = 1} ^ {n} \left(\boldsymbol {x} _ {t - 1} ^ {n} \boldsymbol {x} _ {t - 1} ^ {n} ^ {\prime} + P _ {t - 1} ^ {n}\right). \tag {6.69}
$$

In (6.66)-(6.69), the smoothers are calculated under the current value of the parameters $\Theta ^ { ( j - 1 ) }$ ; for simplicity, we have not explicitly displayed this fact.

Minimizing (6.66) with respect to the parameters, at iteration $j$ , constitutes the maximization step, and is analogous to the usual multivariate regression approach, which yields the updated estimates

$$
\Phi^ {(j)} = S _ {1 0} S _ {0 0} ^ {- 1}, \tag {6.70}
$$

$$
Q ^ {(j)} = n ^ {- 1} \left(S _ {1 1} - S _ {1 0} S _ {0 0} ^ {- 1} S _ {1 0} ^ {\prime}\right), \tag {6.71}
$$

and

$$
R ^ {(j)} = n ^ {- 1} \sum_ {t = 1} ^ {n} [ (\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {n}) (\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {n}) ^ {\prime} + A _ {t} P _ {t} ^ {n} A _ {t} ^ {\prime} ]. \tag {6.72}
$$

The updates for the initial mean and variance–covariance matrix are

$$
\boldsymbol {\mu} _ {0} ^ {(j)} = \boldsymbol {x} _ {0} ^ {n} \quad \text {a n d} \quad \Sigma_ {0} ^ {(j)} = P _ {0} ^ {n} \tag {6.73}
$$

obtained from minimizing (6.66).

The overall procedure can be regarded as simply alternating between the Kalman filtering and smoothing recursions and the multivariate normal maximum likelihood estimators, as given by (6.70)–(6.73). Convergence results for the EM algorithm under general conditions can be found in Wu (1983). We summarize the iterative procedure as follows.

1. Initialize the procedure by selecting starting values for the parameters $\Theta ^ { ( 0 ) } = \{ \mu _ { 0 } , \Sigma _ { 0 } , \Phi , Q , R \}$ .

On iteration $j$ , $( j = 1 , 2 , \dots ,$ ):

2. Compute the incomplete-data likelihood, $- \ln { \cal L } _ { Y } \big ( \Theta ^ { ( j - 1 ) } \big )$ ; see equation (6.62).   
3. Perform the E-Step. Use Properties 6.1, 6.2, and 6.3 to obtain the smoothed values $\mathbf { \Delta } \mathbf { x } _ { t } ^ { n } , \mathbf { \Delta } \mathbf { P } _ { t } ^ { n }$ and $P _ { t , t - 1 } ^ { n }$ , for $t = 1 , \ldots , n$ , using the parameters $\Theta ^ { ( j - 1 ) }$ . Use the smoothed values to calculate $S _ { 1 1 } , S _ { 1 0 } , S _ { 0 0 }$ given in (6.67)–(6.69).   
4. Perform the M-Step. Update the estimates, $\mu _ { 0 } , \Sigma _ { 0 } , \Phi , Q$ , and $R$ using (6.70)–(6.73), to obtain $\Theta ^ { ( j ) }$ .   
5. Repeat Steps $2 - 4$ to convergence.

# Example 6.8 EM Algorithm for Example 6.3

Using the same data generated in Example 6.6, we performed an EM algorithm estimation of the parameters $\phi$ , $\sigma _ { w } ^ { 2 }$ and $\sigma _ { v } ^ { 2 }$ as well as the initial parameters $\mu _ { 0 }$ and $\Sigma _ { 0 }$ .

The convergence rate of the EM algorithm compared with the Newton– Raphson procedure is slow. In this example, with convergence being claimed when the log likelihood does not change by more that .001, convergence was attained after 38 iterations.

The final estimates, along with their standard errors (in parentheses), were

$$
\widehat {\phi} = . 8 3 (. 0 8), \quad \widehat {\sigma} _ {w} = . 8 1 (. 1 7), \quad \widehat {\sigma} _ {v} = . 9 1 (. 1 4),
$$

with $\widehat { \mu } _ { 0 } = - . 0 6$ and $\widehat { \Sigma } _ { 0 } = . 4 4$ .

Evaluation of the standard errors used a call to fdHess in the nlme R package to evaluate the Hessian at the final estimates. The nlme package must be loaded prior to the call to fdHess.

# Asymptotic Distribution of the MLEs

The asymptotic distribution of estimators of the model parameters, say, ${ \widehat { \Theta } } _ { n }$ , is studied extensively in Caines (1988, Chapters 7 and 8), and in Hannan and Deistler (1988, Chapter 4). In both of these references, the consistency and asymptotic normality of the estimators is established under general conditions. Although we will only state the basic result, some crucial elements are needed to establish large sample properties of the estimators. An essential condition is the stability of the filter. Stability of the filter assures that, for large $t$ , the innovations $\epsilon _ { t }$ are basically copies of each other (that is, independent and identically distributed) with a stable covariance matrix $\Sigma$ that does not depend on $t$ and that, asymptotically, the innovations contain all of the information about the unknown parameters. Although it is not necessary, for simplicity, we shall assume here that $A _ { t } \equiv A$ for all $t$ . Details on departures from this assumption can be found in Jazwinski (1970, Sections 7.6 and 7.8). We also drop the inputs as use the model in the form of (6.1) and (6.2).

For stability of the filter, we assume the eigenvalues of $\Phi$ are less than one in absolute value; this assumption can be weakened (for example, see Harvey, 1991, Section 4.3), but we retain it for simplicity. This assumption is enough to ensure the stability of the filter in that, as $t \to \infty$ , the filter error covariance matrix $P _ { t } ^ { t }$ converges to $P$ , the steady-state error covariance matrix, the gain matrix $K _ { t }$ converges to $K$ , the steady-state gain matrix, from which it follows that the innovation variance–covariance matrix $\Sigma _ { t }$ converges to $\Sigma$ , the steadystate variance–covariance matrix of the stable innovations; details can be found in Jazwinski (1970, Sections 7.6 and 7.8) and Anderson and Moore (1979, Section 4.4). In particular, the steady-state filter error covariance matrix, $P$ ,

satisfies the Riccati equation:

$$
P = \Phi \left[ P - P A ^ {\prime} \left(A P A ^ {\prime} + R\right) ^ {- 1} A P \right] \Phi^ {\prime} + Q;
$$

the steady-state gain matrix satisfies $K = P A ^ { \prime } [ A P A ^ { \prime } + R ] ^ { - 1 }$ . In Example 6.5, for all practical purposes, stability was reached by the fourth observation.

When the process is in steady-state, we may consider $\mathbf { \Delta } \mathbf { x } _ { t + 1 } ^ { t }$ as the steadystate predictor and interpret it as $\pmb { x } _ { t + 1 } ^ { t } = E ( \pmb { x } _ { t + 1 } \ \vert \ \pmb { y } _ { t } , \pmb { y } _ { t - 1 } , . . . )$ . As can be seen from (6.19) and (6.21), the steady-state predictor can be written as

$$
\begin{array}{l} \boldsymbol {x} _ {t + 1} ^ {t} = \Phi [ I - K A ] \boldsymbol {x} _ {t} ^ {t - 1} + \Phi K y _ {t} \\ = \Phi \boldsymbol {x} _ {t} ^ {t - 1} + \Phi K \boldsymbol {\epsilon} _ {t}, \tag {6.74} \\ \end{array}
$$

where $\mathbf { \epsilon } _ { \epsilon }$ is the steady-state innovation process given by

$$
\boldsymbol {\epsilon} _ {t} = y _ {t} - E (\boldsymbol {y} _ {t} \mid \boldsymbol {y} _ {t - 1}, \boldsymbol {y} _ {t - 2}, \dots).
$$

In this case, $\epsilon _ { t } \sim$ iid $\mathrm { N } ( \mathbf { 0 } , \Sigma )$ , where $\Sigma = A P A ^ { \prime } + R$ . In steady-state, the observations can be written as

$$
\boldsymbol {y} _ {t} = A \boldsymbol {x} _ {t} ^ {t - 1} + \boldsymbol {\epsilon} _ {t}. \tag {6.75}
$$

Together, (6.74) and (6.75) make up the steady-state innovations form of the dynamic linear model.

Two other conditions worth mentioning are observability and controllability. Observability focuses on the question of how much information can be gained about the $p$ -dimensional state vector ${ \pmb x } _ { t }$ from $p$ future observations $\{ \pmb { y } _ { t } , \pmb { y } _ { t + 1 } , \ldots , \pmb { y } _ { t + p - 1 } \}$ . Consider the state without any noise term,

$$
\boldsymbol {x} _ {t + p} = \Phi \boldsymbol {x} _ {t + p - 1} = \dots = \Phi^ {p} \boldsymbol {x} _ {t}.
$$

Then, the data (without observational noise) satisfy

$$
\boldsymbol {y} _ {t + j} = A \boldsymbol {x} _ {t + j} = A \Phi^ {j} \boldsymbol {x} _ {t}, \quad j = 0, \dots , p - 1,
$$

or

$$
\left(\boldsymbol {y} _ {t} ^ {\prime}, \dots , \boldsymbol {y} _ {t + p - 1} ^ {\prime}\right) = \boldsymbol {x} _ {t} ^ {\prime} \left[ A ^ {\prime}, \Phi^ {\prime} A ^ {\prime}, \dots , \Phi^ {\prime p - 1} A ^ {\prime} \right].
$$

Hence, if the observability matrix $\mathcal { O } ^ { \prime } = [ A ^ { \prime } , \Phi ^ { \prime } A ^ { \prime } , \dots , \Phi ^ { ' p - 1 } A ^ { \prime } ]$ has full rank $p$ , we may explicitly solve for ${ \pmb x } _ { t }$ in terms of $\pmb { y } _ { t : p } = ( \pmb { y } _ { t } ^ { \prime } , \ldots , \pmb { y } _ { t + p - 1 } ^ { \prime } ) ^ { \prime }$ , namely, $\pmb { x } _ { t } = ( \mathcal { O } ^ { \prime } \mathcal { O } ) ^ { - 1 } \mathcal { O } ^ { \prime } \pmb { y } _ { t : p }$ , and the system is said to be observable.

In a similar manner, to define controllability, write the state noise as ${ \pmb w } _ { t } =$ $B { \mathbf { } } { \mathbf { } } u _ { t }$ , where $B$ is $p \times r$ and ${ \pmb u } _ { t }$ is an $r$ -dimensional, nonsingular, white noise process. Thus, the state equation is $\pmb { x } _ { t } = \Phi \pmb { x } _ { t - 1 } + B \pmb { u } _ { t }$ . If the matrix ${ \mathcal { C } } =$ $[ B , \Phi B , \Phi ^ { 2 } B , \dots , \Phi ^ { p - 1 } B ]$ has full rank $p$ , the process is said to be controllable. Controllability has to do with the fact that the state equation satisfies

$$
\boldsymbol {x} _ {t + p} = \sum_ {j = 0} ^ {p - 1} \Phi^ {j} B \boldsymbol {u} _ {t + p - j} + \Phi^ {p} \boldsymbol {x} _ {t} = \mathcal {C} \boldsymbol {U} _ {t} + \Phi^ {p} \boldsymbol {x} _ {t},
$$

where $\pmb { U } _ { t } = ( \pmb { u } _ { t + p } ^ { \prime } , \ldots , \pmb { u } _ { t + 1 } ^ { \prime } ) ^ { \prime }$ . If we think of the variables $\{ \pmb { u } _ { t + p } , \dotsc , \pmb { u } _ { t + 1 } \}$ as “controlling” the state output ${ \pmb x } _ { t }$ , and we act as if we are free to choose the $\mathbf { \pmb { u } } _ { t }$ at will, the fact that $c$ is full rank means any desired value of $\mathbf { \Delta } \mathbf { x } _ { t + p }$ can be obtained from any initial state ${ \pmb x } _ { t }$ by control of $\boldsymbol { U } _ { t }$ . In particular, we can put $\pmb { U } _ { t } = \pmb { \mathcal { C } } ^ { \prime } ( \pmb { \mathcal { C } } \pmb { \mathcal { C } } ^ { \prime } ) ^ { - 1 } \pmb { x } _ { t + p } - \Phi ^ { p } \pmb { x } _ { t }$ .

The key point about controllability and observability is that these conditions are necessary and sufficient to ensure the state-space model has the smallest possible dimension; details can be found in Hannan and Diestler (1988, Section 2.3). As a simple example, suppose the state system is bivariate, $\pmb { x } _ { t } = ( x _ { t 1 } , x _ { t 2 } ) ^ { \prime }$ , where $x _ { t 1 }$ and $x _ { t 2 }$ are independent components with $\Phi = \mathrm { d i a g } \{ \phi _ { 1 } , \phi _ { 2 } \}$ , and $y _ { t } = [ 1 , 0 ] { \pmb x } _ { t } + v _ { t }$ ; that is, $y _ { t } = x _ { t 1 } + v _ { t }$ . Clearly we could not hope to reasonably estimate $\phi _ { 2 }$ . This system is not observable because $\boldsymbol { \mathcal { O } }$ has rank one. Additional details on this point can be found in Jazwinski (1970, Section 7.5).

In the following property, we assume the Gaussian state-space model (6.1) and (6.2), is time invariant, i.e., $A _ { t } \equiv A$ , the eigenvalues of $\Phi$ are within the unit circle and the system is observable and controllable. We denote the true parameters by $\Theta _ { 0 }$ , and we assume the dimension of $\Theta _ { 0 }$ is the dimension of the parameter space. Although it is not necessary to assume ${ \pmb w } _ { t }$ and ${ \pmb v } _ { t }$ are Gaussian, certain additional conditions would have to apply and adjustments to the asymptotic covariance matrix would have to be made (see Caines, 1988, Chapter 8).

# Property P6.4: Asymptotic Distribution of the Estimators

Under general conditions, let ${ \widehat { \Theta } } _ { n }$ be the estimator of $\Theta _ { 0 }$ obtained by maximizing the innovations likelihood, $L _ { Y } ( \Theta )$ , as given in (6.62). Then, as $n \to \infty$ ,

$$
\sqrt {n} \left(\widehat {\Theta} _ {n} - \Theta_ {0}\right) \xrightarrow {d} \mathrm {N} \left[ 0, \mathcal {I} (\Theta_ {0}) ^ {- 1} \right],
$$

where $\mathcal { T } ( \Theta )$ is the asymptotic information matrix given by

$$
\mathcal {I} (\Theta) = \lim _ {n \to \infty} n ^ {- 1} E \left[ - \partial^ {2} \ln L _ {Y} (\Theta) / \partial \Theta \partial \Theta^ {\prime} \right].
$$

Precise details and the proof of Property P6.4 are given in Caines (1988, Chapter 7) and in Hannan and Deistler (1988, Chapter 4). For a Newton procedure, the Hessian matrix (as described in Example 6.6) at the time of convergence can be used as an estimate of $n  { \mathcal { T } } ( \Theta _ { 0 } )$ to obtain estimates of the standard errors. In the case of the EM algorithm, no derivatives are calculated, but we may include a numerical evaluation of the Hessian matrix at the time of convergence to obtain estimated standard errors. Also, extensions of the EM algorithm exist, such as the SEM algorithm (Meng and Rubin, 1991), that include a procedure for the estimation of standard errors. In the examples of this section, the estimated standard errors were obtained from the numerical Hessian matrix of $- \ln L _ { Y } ( \widehat { \Theta } )$ , where $\widehat { \Theta }$ is the vector of parameters estimates at the time of convergence.

# 6.4 Missing Data Modifications

An attractive feature available within the state-space framework is its ability to treat time series that have been observed irregularly over time. For example, Palma and Chan (1997) used the state-space model for estimation and forecasting of long memory (specifically, fractionally integrated ARMA, or ARFIMA, processes) time series with missing observations. Throughout this section we assume the model is of the form (6.1) and (6.2). The EM algorithm allows parts of the observation vector ${ \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf } { \mathbf } { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf   \mathbf \mathbf { } \mathbf { } \mathbf { } \mathbf$ to be missing at a number of observation times. Shumway and Stoffer (1982) described the modifications necessary for the special case in which the subvectors of ${ \pmb v } _ { t }$ corresponding to the observed and unobserved parts of $\pmb { y } _ { t }$ happen to be uncorrelated. Here, we will also discuss the general case.

Suppose, at a given time $t$ , we define the partition of the $q \times 1$ observation vector $\pmb { y } _ { t } = ( \pmb { y } _ { t } ^ { ( 1 ) ^ { \prime } } , \pmb { y } _ { t } ^ { ( 2 ) ^ { \prime } } ) ^ { \prime }$ , where the first $q _ { 1 t } \times 1$ component is observed and the second $q _ { 2 t } \times 1$ component is unobserved, $q _ { 1 t } + q _ { 2 t } = q$ . Then, write the partitioned observation equation

$$
\left( \begin{array}{l} \boldsymbol {y} _ {t} ^ {(1)} \\ \boldsymbol {y} _ {t} ^ {(2)} \end{array} \right) = \left[ \begin{array}{l} A _ {t} ^ {(1)} \\ A _ {t} ^ {(2)} \end{array} \right] \boldsymbol {x} _ {t} + \left( \begin{array}{l} \boldsymbol {v} _ {t} ^ {(1)} \\ \boldsymbol {v} _ {t} ^ {(2)} \end{array} \right), \tag {6.76}
$$

where ${ A } _ { t } ^ { ( 1 ) }$ and ${ A } _ { t } ^ { ( 2 ) }$ are, respectively, the $q _ { 1 t } \times p$ and $q _ { 2 t } \times p$ partitioned observation matrices, and

$$
\operatorname {c o v} \binom {\boldsymbol {v} _ {t} ^ {(1)}} {\boldsymbol {v} _ {t} ^ {(2)}} = \left[ \begin{array}{l l} R _ {1 1 t} & R _ {1 2 t} \\ R _ {2 1 t} & R _ {2 2 t} \end{array} \right] \tag {6.77}
$$

denotes the covariance matrix of the measurement errors between the observed and unobserved parts. Stoffer (1982) established the filtering equations, Property P6.1, hold for the missing data case if, at update $t$ , we make the replacements

$$
\boldsymbol {y} _ {(t)} = \left( \begin{array}{c} \boldsymbol {y} _ {t} ^ {(1)} \\ \boldsymbol {0} \end{array} \right), \quad A _ {(t)} = \left[ \begin{array}{c} A _ {t} ^ {(1)} \\ 0 \end{array} \right], \quad R _ {(t)} = \left[ \begin{array}{c c} R _ {1 1 t} & 0 \\ 0 & R _ {2 2 t} \end{array} \right], \tag {6.78}
$$

for $\mathbf { \mathscr { y } } _ { t }$ , $A _ { t }$ , and $R$ , respectively, in (6.21)–(6.23).

Once the “missing data” filtered values have been obtained, Stoffer (1982) also established the smoother values can be processed using Properties P6.2 and P6.3 with the values obtained from the missing data-filtered values. The implication of these results is that, if $\mathbf { \mathscr { y } } _ { t }$ is incomplete, the filtered and smoothed estimators can be calculated from the usual equations by entering zeros in the observation vector when data are missing, by zeroing out the corresponding rows of the design matrix $A _ { t }$ , and by entering zeros in the offdiagonal elements of $R$ that correspond to $R _ { 1 2 t }$ and $R _ { 2 1 t }$ at update $t$ in the filter equation (6.23). In doing this procedure, the state estimators are

$$
\boldsymbol {x} _ {t} ^ {(s)} = E \left(\boldsymbol {x} _ {t} \mid \boldsymbol {y} _ {1} ^ {(1)}, \dots , \boldsymbol {y} _ {s} ^ {(1)}\right), \tag {6.79}
$$

# 6.4: Missing Data

with error variance–covariance matrix

$$
P _ {t} ^ {(s)} = E \left\{\left(\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {(s)}\right) \left(\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {(s)}\right) ^ {\prime} \right\}. \tag {6.80}
$$

The missing data lag-one smoother covariances will be denoted by P (n)t,t 1 $P _ { t , t - 1 } ^ { ( n ) }$ .

The maximum likelihood estimators, as computed in the EM procedure, must also be modified in the missing data case. Now, we consider

$$
Y _ {n} ^ {(1)} = \left\{\boldsymbol {y} _ {1} ^ {(1)}, \dots , \boldsymbol {y} _ {n} ^ {(1)} \right\} \tag {6.81}
$$

as the incomplete data, and $X _ { n } , Y _ { n }$ , as defined in (6.63), as the complete data. In this case, the complete data likelihood, (6.63), or equivalently (6.64), is the same, but to implement the E-step, at iteration $j$ , we must calculate

$$
\begin{array}{l} Q \left(\Theta \mid \Theta^ {(j - 1)}\right) = E \left\{- 2 \ln L _ {X, Y} (\Theta) \mid Y _ {n} ^ {(1)}, \Theta^ {(j - 1)} \right\} \\ = E _ {*} \left\{\ln | \Sigma_ {0} | + \operatorname {t r} \Sigma_ {0} ^ {- 1} (\boldsymbol {x} _ {0} - \boldsymbol {\mu} _ {0}) (\boldsymbol {x} _ {0} - \boldsymbol {\mu} _ {0}) ^ {\prime} \mid Y _ {n} ^ {(1)} \right\} \\ + \quad E _ {*} \left\{n \ln | Q | + \sum_ {t = 1} ^ {n} \operatorname {t r} \left[ Q ^ {- 1} \left(\boldsymbol {x} _ {t} - \Phi \boldsymbol {x} _ {t - 1}\right) \left(\boldsymbol {x} _ {t} - \Phi \boldsymbol {x} _ {t - 1}\right) ^ {\prime} \right] \mid Y _ {n} ^ {(1)} \right\} \\ + \quad E _ {*} \left\{n \ln | R | + \sum_ {t = 1} ^ {n} \operatorname {t r} \left[ R ^ {- 1} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) ^ {\prime} \right] \mid Y _ {n} ^ {(1)} \right\}, \tag {6.82} \\ \end{array}
$$

where $E _ { * }$ denotes the conditional expectation under $\Theta ^ { ( j - 1 ) }$ and tr denotes trace. The first two terms in (6.82) will be like the first two terms of (6.66) with the smoothers $\pmb { x } _ { t } ^ { n }$ , $P _ { t } ^ { n }$ , and $P _ { t , t - 1 } ^ { n }$ replaced by their missing data counterparts, $\pmb { x } _ { t } ^ { ( n ) }$ , $P _ { t } ^ { ( n ) }$ , and $P _ { t , t - 1 } ^ { ( n ) }$ − . What changes in the missing data case is the third term of (6.82), where we must evaluate $E _ { * } ( \pmb { y } _ { t } ^ { ( 2 ) } \mid Y _ { n } ^ { ( 1 ) } )$ and $E _ { * } ( { \pmb y } _ { t } ^ { ( 2 ) } { \pmb y } _ { t } ^ { ( 2 ) ^ { \prime } } \mid Y _ { n } ^ { ( 1 ) } )$ . In Stoffer (1982), it is shown that

$$
\begin{array}{l} E _ {*} \left\{\left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) ^ {\prime} \mid Y _ {n} ^ {(1)} \right\} \\ = \left( \begin{array}{c} \boldsymbol {y} _ {t} ^ {(1)} - A _ {t} ^ {(1)} \boldsymbol {x} _ {t} ^ {(n)} \\ R _ {* 2 1 t} R _ {* 1 1 t} ^ {- 1} (\boldsymbol {y} _ {t} ^ {(1)} - A _ {t} ^ {(1)} \boldsymbol {x} _ {t} ^ {(n)}) \end{array} \right) \left( \begin{array}{c} \boldsymbol {y} _ {t} ^ {(1)} - A _ {t} ^ {(1)} \boldsymbol {x} _ {t} ^ {(n)} \\ R _ {* 2 1 t} R _ {* 1 1 t} ^ {- 1} (\boldsymbol {y} _ {t} ^ {(1)} - A _ {t} ^ {(1)} \boldsymbol {{x}} _ {t} ^ {(n)}) \end{array} \right) ^ {\prime} \\ + \left( \begin{array}{c} A _ {t} ^ {(1)} \\ R _ {* 2 1 t} R _ {* 1 1 t} ^ {- 1} A _ {t} ^ {(1)} \end{array} \right) P _ {t} ^ {(n)} \left( \begin{array}{c} A _ {t} ^ {(1)} \\ R _ {* 2 1 t} R _ {* 1 1 t} ^ {- 1} A _ {t} ^ {(1)} \end{array} \right) ^ {\prime} \\ + \left( \begin{array}{c c} 0 & 0 \\ 0 & R _ {* 2 2 t} - R _ {* 2 1 t} R _ {* 1 1 t} ^ {- 1} R _ {* 1 2 t} \end{array} \right). \tag {6.83} \\ \end{array}
$$

In (6.83), the values of $R _ { * i k t }$ , for $i , k = 1 , 2$ , are the current values specified by $\Theta ^ { ( j - 1 ) }$ . In addition, $\pmb { x } _ { t } ^ { ( n ) }$ and $P _ { t } ^ { ( n ) }$ are the values obtained by running the smoother under the current parameter estimates specified by $\Theta ^ { ( j - 1 ) }$ .

In the case in which observed and unobserved components have uncorrelated errors, that is, $R _ { * 1 2 t }$ is the zero matrix, (6.83) can be simplified to

$$
\begin{array}{l} E _ {*} \left\{\left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) ^ {\prime} \mid Y _ {n} ^ {(1)} \right\} \\ = \left(\boldsymbol {y} _ {(t)} - A _ {(t)} \boldsymbol {x} _ {t} ^ {(n)}\right) \left(\boldsymbol {y} _ {(t)} - A _ {(t)} \boldsymbol {x} _ {t} ^ {(n)}\right) ^ {\prime} + A _ {(t)} P _ {t} ^ {(n)} A _ {(t)} ^ {\prime} \\ + \left( \begin{array}{c c} 0 & 0 \\ 0 & R _ {* 2 2 t} \end{array} \right), \tag {6.84} \\ \end{array}
$$

where $\mathbf { \mathscr { y } } _ { ( t ) }$ and $A _ { ( t ) }$ are defined in (6.78).

In this simplified case, the “missing data” M-step looks like the M-step given in (6.67)-(6.73). That is, with

$$
S _ {(1 1)} = \sum_ {t = 1} ^ {n} \left(\boldsymbol {x} _ {t} ^ {(n)} \boldsymbol {x} _ {t} ^ {(n) ^ {\prime}} + P _ {t} ^ {(n)}\right), \tag {6.85}
$$

$$
S _ {(1 0)} = \sum_ {t = 1} ^ {n} \left(\boldsymbol {x} _ {t} ^ {(n)} \boldsymbol {x} _ {t - 1} ^ {(n) ^ {\prime}} + P _ {t, t - 1} ^ {(n)}\right), \tag {6.86}
$$

and

$$
S _ {(0 0)} = \sum_ {t = 1} ^ {n} \left(\boldsymbol {x} _ {t - 1} ^ {(n)} \boldsymbol {x} _ {t - 1} ^ {(n) ^ {\prime}} + P _ {t - 1} ^ {(n)}\right), \tag {6.87}
$$

where the smoothers are calculated under the present value of the parameters $\Theta ^ { ( j - 1 ) }$ using the missing data modifications, at iteration $j$ , the maximization step is

$$
\Phi^ {(j)} = S _ {(1 0)} S _ {(0 0)} ^ {- 1}, \tag {6.88}
$$

$$
Q ^ {(j)} = n ^ {- 1} \left(S _ {(1 1)} - S _ {(1 0)} S _ {(0 0)} ^ {- 1} S _ {(1 0)} ^ {\prime}\right), \tag {6.89}
$$

and

$$
\begin{array}{l} R ^ {(j)} = n ^ {- 1} \sum_ {t = 1} ^ {n} D _ {t} \left\{\left(\boldsymbol {y} _ {(t)} - A _ {(t)} \boldsymbol {x} _ {t} ^ {(n)}\right) \left(\boldsymbol {y} _ {(t)} - A _ {(t)} \boldsymbol {x} _ {t} ^ {(n)}\right) ^ {\prime} + A _ {(t)} P _ {t} ^ {(n)} A _ {(t)} ^ {\prime} \right. \\ + \left( \begin{array}{c c} 0 & 0 \\ 0 & R _ {2 2 t} ^ {(j - 1)} \end{array} \right) \} D _ {t} ^ {\prime}, \tag {6.90} \\ \end{array}
$$

where $D _ { t }$ is a permutation matrix that reorders the variables at time $t$ in their original order and $\pmb { y } _ { ( t ) }$ and $A _ { ( t ) }$ are defined in (6.78). For example, suppose $q = 3$ and at time $t$ , $y _ { t 2 }$ is missing. Then,

$$
\boldsymbol {y} _ {(t)} = \left( \begin{array}{c} y _ {t 1} \\ y _ {t 3} \\ 0 \end{array} \right), \quad A _ {(t)} = \left[ \begin{array}{c} A _ {t 1} \\ A _ {t 3} \\ \mathbf {0} ^ {\prime} \end{array} \right], \quad \text {a n d} \quad D _ {t} = \left[ \begin{array}{c c c} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{array} \right],
$$

![](images/feaed6ff9fa6e6b0090e47341c511453d17f8d70555da80b2ae3b9707bde358e.jpg)

![](images/0bcdcb92d7c1779cee438efe7af73230cbb015d8571a44142806392f6ff42f24.jpg)

![](images/179622fd9404ae4e8be204087d344b2f964825417e36fb3b4a74be60ed0c82a9.jpg)  
Figure 6.5 Smoothed values for various components in the blood parameter tracking problem. The actual data are shown as points, the smoothed values are shown as solid lines, and $\pm 3$ standard error bounds are shown as dashed lines.

where $A _ { t i }$ is the $i$ th row of $A _ { t }$ and $\mathbf { 0 } ^ { \prime }$ is a $1 \times p$ vector of zeros. In (6.90), only $R _ { 1 1 t }$ gets updated, and $R _ { 2 2 t }$ at iteration $j$ is simply set to its value from the previous iteration, $j - 1$ . Of course, if we cannot assume $R _ { 1 2 t } = 0$ , (6.90) must be changed accordingly using (6.83), but (6.88) and (6.89) remain the same. As before, the parameter estimates for the initial state are updated as

$$
\boldsymbol {\mu} _ {0} ^ {(j)} = \boldsymbol {x} _ {0} ^ {(n)} \quad \text {a n d} \quad \Sigma_ {0} ^ {(j)} = P _ {0} ^ {(n)}. \tag {6.91}
$$

# Example 6.9 Longitudinal Biomedical Data

We consider the biomedical data in Example 6.1 which has portions of the three-dimensional vector missing after the 40th day. The maximum likelihood procedure yielded the estimators

$$
\widehat {\Phi} = \left( \begin{array}{c c c} 1. 0 2 & -. 0 9 & . 0 1 \\ . 0 8 & . 9 0 & . 0 1 \\ -. 9 0 & 1. 4 2 & . 8 7 \end{array} \right), \quad \widehat {Q} = \left( \begin{array}{c c c} . 0 1 8 & . 0 0 2 & . 0 0 0 \\ . 0 0 2 & . 0 0 4 & . 0 1 7 \\ . 0 0 0 & . 0 1 7 & 2. 2 7 \end{array} \right),
$$

and $\widehat { R } = \mathrm { d i a g } \{ . 0 0 4 , . 0 2 2 , 1 . 6 9 \}$ for the transition, state error covariance and observation error covariance matrices, respectively. The coupling between the first and second series is relatively weak, whereas the third series HCT is strongly related to the first two; that is,

$$
\widehat {x} _ {t 3} = -. 9 0 x _ {t - 1, 1} + 1. 4 2 x _ {t - 1, 2} + . 8 7 x _ {t - 1, 3}.
$$

Hence, the HCT is negatively correlated with white blood count and positively correlated with platelet count. Byproducts of the procedure are estimated trajectories for all three longitudinal series and their respective prediction intervals. In particular, Figure 6.5 shows the data as points, the estimated smoothed values $\widehat { x } _ { t } ^ { ( n ) }$ as solid lines, and error bounds, $\widehat { x } _ { t } ^ { ( n ) } { \pm } 3 \sqrt { \widehat { P } _ { t } ^ { ( n ) } }$ as dotted lines, for critical post-transplant platelet count.

# 6.5 Structural Models: Signal Extraction and Forecasting

In order to develop computing techniques for handling a versatile cross section of possible models, it is necessary to restrict the state-space model somewhat, and we consider one possible class of specializations in this section. The components of the model are taken as linear processes that can be adapted to represent fixed and disturbed trends and periodicities as well as classical autoregressions. The observed series is regarded as being a sum of component signal series. To illustrate the possibilities, consider the economic example given below that shows how to fit a sum of trend, seasonal, and irregular components the quarterly earnings data that we have considered before.

# Example 6.10 Johnson & Johnson Quarterly Earnings

Consider the quarterly earnings series from the U.S. company Johnson & Johnson as given in Figure 1.1. The series is highly nonstationary, and there is both a trend signal that is gradually increasing over time and a seasonal component that cycles every four quarters or once per year. The

![](images/d2746f907734e693d031cbf7774d61220c4154ceaf692423ff1bbceea206a423.jpg)

![](images/7cef51632704e77e91a3558faee71c5b5a7dce646b6cf30661b78eb5564c41c4.jpg)  
Figure 6.6 Estimated trend component, $T _ { t } ^ { n }$ (top), and estimated trend plus seasonal component, $S _ { t } ^ { n }$ (bottom), for the Johnson and Johnson quarterly earnings series.

seasonal component is getting larger over time as well. Transforming into logarithms or even taking the $n$ th root does not seem to make the series stationary, as there is a slight bend to the transformed curve. Suppose, however, we consider the series to be the sum of a trend component, a seasonal component, and a white noise. That is, let the observed series be expressed as

$$
y _ {t} = T _ {t} + S _ {t} + v _ {t}, \tag {6.92}
$$

where $T _ { t }$ is trend and $S _ { t }$ is the seasonal component. Suppose we allow trend to increase exponentially; that is,

$$
T _ {t} = \phi T _ {t - 1} + w _ {t 1}, \tag {6.93}
$$

where the coefficient $\phi > 1$ characterizes the increase. Let the seasonal component be modeled as

$$
S _ {t} + S _ {t - 1} + S _ {t - 2} + S _ {t - 3} = w _ {t 2}, \tag {6.94}
$$

which corresponds to assuming the seasonal component is expected to sum to zero over a complete period or four quarters. To express this model in state-space form, let $\pmb { x } _ { t } ^ { \prime } = ( T _ { t } , S _ { t } , S _ { t - 1 } , S _ { t - 2 } )$ be the state vector so the observation equation (6.2) can be written as

$$
y _ {t} = \left( \begin{array}{c c c c} 1 & 1 & 0 & 0 \end{array} \right) \left( \begin{array}{c} T _ {t} \\ S _ {t} \\ S _ {t - 1} \\ S _ {t - 2} \end{array} \right) + v _ {t},
$$

with the state equation written as

$$
\left( \begin{array}{c} T _ {t} \\ S _ {t} \\ S _ {t - 1} \\ S _ {t - 2} \end{array} \right) = \left( \begin{array}{c c c c} \phi & 0 & 0 & 0 \\ 0 & - 1 & - 1 & - 1 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{array} \right) \left( \begin{array}{c} T _ {t - 1} \\ S _ {t - 1} \\ S _ {t - 2} \\ S _ {t - 3} \end{array} \right) + \left( \begin{array}{c} w _ {t 1} \\ w _ {t 2} \\ 0 \\ 0 \end{array} \right),
$$

where $R = r _ { 1 1 }$ and

$$
Q = \left( \begin{array}{c c c c} q _ {1 1} & 0 & 0 & 0 \\ 0 & q _ {2 2} & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{array} \right).
$$

The model reduces to state-space form, (6.1) and (6.2), with $p \ = \ 4$ and $q = 1$ . The parameters to be estimated are $r _ { 1 1 }$ , the noise variance in the measurement equations, $q _ { 1 1 }$ and $q _ { 2 2 }$ , the model variances corresponding to the trend and seasonal components and $\phi$ , the transition parameter that models the growth rate. Growth is about 3% per year, and we began with $\phi \ : = \ : 1 . 0 3$ . The initial mean was fixed at $\pmb { \mu } _ { 0 } = ( . 5 , . 3 , . 2 , . 1 ) ^ { \prime }$ , with uncertainty modeled by the diagonal covariance matrix with $\Sigma _ { 0 i i } = . 0 1$ , for $i = 1 , \dots , 4$ . Initial state covariance values were taken as $q _ { 1 1 } = . 0 1 , q _ { 2 2 } = . 1 0$ , corresponding to relatively low uncertainty in the trend model compared with that in the seasonal model. The measurement error covariance was started at $r _ { 1 1 } ~ = ~ . 0 4$ . After 70 iterations of the EM algorithm the transition parameter stabilized at $\widehat { \phi } = 1 . 0 3 5$ , corresponding to exponential growth with inflation at about 3.5% per year. The measurement uncertainty was small at $\widehat { r } _ { 1 1 } = . 0 0 8 6$ , compared with the model uncertainties $\widehat { q } _ { 1 1 } = . 0 1 6 9$ and $\widehat { q } _ { 2 2 } = . 0 4 9 7$ . From initial guesses, the trend uncertainty increased and the seasonal uncertainty decreased. Figure 6.6 shows the smoothed trend estimate and the exponentially increasing seasonal components. We may also consider forecasting the Johnson & Johnson series, and the result of a 12-quarter forecast is shown in Figure 6.7 as basically an extension of the latter part of the observed data.

![](images/f63471098cf4c6f08d36c0ad8d975bded50020a67154234b08d51cf74e97eccb.jpg)  
Figure 6.7 A 12-quarter forecast for the Johnson & Johnson quarterly earnings series. The last three years of data (quarters 72-84), are shown as points connected by a solid line. The forecasts are shown as points connected by a solid line (quarters 85-96) and dotted lines are upper and lower 95% prediction intervals.

# 6.6 ARMAX Models in State-Space Form

Sometimes, it is advantageous to write the state-space model in a slightly different way, as is done by numerous authors; for example, Anderson and Moore (1970) and Hannan and Deistler (1988). Here, we write the state-space model as

$$
\boldsymbol {x} _ {t + 1} = \Phi \boldsymbol {x} _ {t} + \Upsilon \boldsymbol {u} _ {t} + \boldsymbol {w} _ {t} \quad t = 0, 1, \dots , n \tag {6.95}
$$

$$
\boldsymbol {y} _ {t} = A _ {t} \boldsymbol {x} _ {t} + \Gamma \boldsymbol {u} _ {t} + \boldsymbol {v} _ {t} \quad t = 1, \dots , n \tag {6.96}
$$

where, in the state equation, $\pmb { x } _ { 0 } \sim \mathrm { N } ( \pmb { \mu } _ { 0 } , \pmb { \Sigma } _ { 0 } )$ , $\Phi$ is $p \times p$ , and $\Upsilon$ is $p \times r$ . In the observation equation, $A _ { t }$ is $q \times p$ and $\Gamma$ is $q \times r$ . Now, ${ \pmb w } _ { t }$ and $\mathbf { \Delta } _ { \pmb { v } _ { t } }$ are still white noise series (both independent of $\pmb { x } _ { 0 }$ ), with $\mathrm { v a r } ( { \pmb w } _ { t } ) = Q$ , $\mathrm { v a r } ( { \pmb v } _ { t } ) = R$ , but we also allow the state noise and observation noise to be correlated at time $t$ ; that is,

$$
\operatorname {c o v} \left(\boldsymbol {w} _ {t}, \boldsymbol {v} _ {t}\right) = E \left(\boldsymbol {w} _ {t} \boldsymbol {v} _ {t} ^ {\prime}\right) = S \tag {6.97}
$$

and zero otherwise; note, $S$ is a $p \times q$ matrix. To obtain the innovations, $\mathbf { \epsilon } _ { \mathbf { \epsilon } _ { t } } = \mathbf { \epsilon } _ { \mathbf { \mathbf { y } } _ { t } } - A _ { t } \mathbf { x } _ { t } ^ { t - 1 } - \Gamma \mathbf { u } _ { t }$ , and the innovation variance $\Sigma _ { t } = A _ { t } P _ { t } ^ { t - 1 } A _ { t } ^ { \prime } + R$ , in this case, we need the one-step-ahead state predictions. Of course, the filtered estimates will also be of interest, and they will be needed for smoothing. Property P6.2 (the smoother) as displayed in §6.2 still holds. The following

property generates the predictor $\mathbf { \Delta } \pmb { x } _ { t + 1 } ^ { t }$ from the past predictor $\mathbf { \Delta } \pmb { x } _ { t } ^ { t - 1 }$ when the noise terms are correlated and exhibits the filter update.

# Property P6.5: The Kalman Filter with Correlated State and

# Measurement Noise

For the state-space model specified in (6.95) and (6.96), with initial conditions $\pmb { x } _ { 1 } ^ { 0 }$ and $P _ { 1 } ^ { 0 }$ , for $t = 1 , \ldots , n$ ,

$$
\boldsymbol {x} _ {t + 1} ^ {t} = \Phi \boldsymbol {x} _ {t} ^ {t - 1} + \Upsilon \boldsymbol {u} _ {t} + K _ {t} ^ {*} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {t - 1} - \Gamma \boldsymbol {u} _ {t}\right), \tag {6.98}
$$

$$
P _ {t + 1} ^ {t} = \left[ \Phi - K _ {t} ^ {*} A _ {t} \right] P _ {t} ^ {t - 1} \left[ \Phi - K _ {t} ^ {*} A _ {t} \right] ^ {\prime} + Q + K _ {t} ^ {*} R K _ {t} ^ {* ^ {\prime}} - S K _ {t} ^ {* ^ {\prime}} - K _ {t} ^ {*} S ^ {\prime}, \tag {6.99}
$$

where the new gain matrix is given by

$$
K _ {t} ^ {*} = \left[ \Phi P _ {t} ^ {t - 1} A _ {t} ^ {\prime} + S \right] \left[ \left[ A _ {t} P _ {t} ^ {t - 1} A _ {t} ^ {\prime} + R \right] ^ {- 1}. \right. \tag {6.100}
$$

The filter update, given a new observation $\pmb { y } _ { t + 1 }$ and input ${ \pmb u } _ { t + 1 }$ is given by

$$
\boldsymbol {x} _ {t + 1} ^ {t + 1} = \boldsymbol {x} _ {t + 1} ^ {t} + P _ {t + 1} ^ {t} A _ {t + 1} ^ {\prime} \left[ A _ {t + 1} P _ {t + 1} ^ {t} A _ {t + 1} ^ {\prime} + R \right] ^ {- 1} \boldsymbol {\epsilon} _ {t + 1}, \tag {6.101}
$$

$$
P _ {t + 1} ^ {t + 1} = P _ {t + 1} ^ {t} - P _ {t + 1} ^ {t} A _ {t + 1} ^ {\prime} \left[ A _ {t + 1} P _ {t + 1} ^ {t} A _ {t + 1} ^ {\prime} + R \right] ^ {- 1} A _ {t + 1} P _ {t + 1} ^ {t}. \tag {6.102}
$$

The derivation of Property P6.5 is similar to the derivation of the Kalman filter in Property P6.1 (Problem 6.17). Note, (6.101) and (6.102) are identical to (6.19) and (6.20).

Consider a $p$ -dimensional ARMAX model given by,

$$
\boldsymbol {y} _ {t} = \Gamma \boldsymbol {u} _ {t} + \sum_ {j = 1} ^ {m} \Phi_ {j} \boldsymbol {y} _ {t - j} + \sum_ {k = 1} ^ {q} \Theta_ {k} \boldsymbol {v} _ {t - k} + \boldsymbol {v} _ {t}. \tag {6.103}
$$

The $\Phi$ s and $\Theta \mathrm { s }$ are $p \times p$ matrices, $\Gamma$ is $p \times r$ , and ${ \pmb v } _ { t }$ is a $p \times 1$ white noise process; in fact, (6.103) and (5.84) are identical models, but here, we have written the observations as $\mathbf { \nabla } \pmb { y } _ { t }$ . We now have the following property.

# Property P6.6: A State-Space Form of ARMAX

For $m \geq q$ , the state-space model given by

$$
\boldsymbol {x} _ {t + 1} = \left[ \begin{array}{c c c c c} \Phi_ {1} & I & 0 & \dots & 0 \\ \Phi_ {2} & 0 & I & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \Phi_ {m - 1} & 0 & 0 & \dots & I \\ \Phi_ {m} & 0 & 0 & \dots & 0 \end{array} \right] \boldsymbol {x} _ {t} + \left[ \begin{array}{c} \Theta_ {1} + \Phi_ {1} \\ \vdots \\ \Theta_ {q} + \Phi_ {q} \\ \Phi_ {q + 1} \\ \vdots \\ \Phi_ {m} \end{array} \right] \boldsymbol {v} _ {t}, \tag {6.104}
$$

$$
\boldsymbol {y} _ {t} = [ I, 0, \dots , 0 ] \boldsymbol {x} _ {t} + \Gamma \boldsymbol {u} _ {t} + \boldsymbol {v} _ {t}, \tag {6.105}
$$

implies the ARMAX model (6.103). The state process, ${ \pmb x } _ { t }$ , is $p m \times 1$ , and the observations process $\mathbf { \mathscr { y } } _ { t }$ is $p \times 1$ . If $m < q$ , set $\Phi _ { m + 1 } = \cdot \cdot \cdot = \Phi _ { q } = 0$ , in which case $m = q$ and (6.104)–(6.105) still apply.

This form of the model is somewhat different than the form suggested in §5.1, equations (6.6)-(6.8). For example, in (6.8), by setting $A _ { t }$ equal to the $p \times p$ identity matrix (for all $t$ ) and setting $R = 0$ implies the data $y _ { t }$ in (6.8) follow a VAR( $m$ ) process. In doing so, however, we do not make use of the ability to allow for correlated state and observation error, so a singularity is introduced into the system in the form of $R = 0$ . The method in Property P6.6 avoids that problem, and points out the fact that the same model can take many forms. We do not prove Property P6.6 directly, but the following example should suggest how to establish the general result.

# Example 6.11 Univariate ARMA $( 1 , 1 )$ in State-Space Form

Consider the univariate ARMA $( 1 , 1 )$ model $y _ { t } = \phi y _ { t - 1 } + \theta v _ { t - 1 } + v _ { t }$ . Using Property P6.6, we can write the model as

$$
x _ {t + 1} = \phi x _ {t} + w _ {t}, \quad \text {(s t a t e e q n)}, \tag {6.106}
$$

where $w _ { t } = ( \theta + \phi ) v _ { t }$ and

$$
y _ {t} = x _ {t} + v _ {t}, \quad (\text {o b s e q n}). \tag {6.107}
$$

In this case, $\operatorname { c o v } ( w _ { t } , v _ { t } ) = ( \theta + \phi ) \operatorname { v a r } ( v _ { t } ) = ( \theta + \phi ) R$ , and $\mathrm { c o v } ( w _ { t } , v _ { s } ) = 0$ when $s \neq t$ , so Property P6.5 would apply. To verify (6.106) and (6.107) specify an ARMA(1,1) model, we have

$$
\begin{array}{l} y _ {t} = x _ {t} + v _ {t} \quad \text {f r o m (6 . 1 0 7)} \\ = \phi x _ {t - 1} + (\theta + \phi) v _ {t - 1} + v _ {t} \quad \text {f r o m (6 . 1 0 6)} \\ = \phi \left(x _ {t - 1} + v _ {t - 1}\right) + \theta v _ {t - 1} + v _ {t} \\ = \phi y _ {t - 1} + \theta v _ {t - 1} + v _ {t}, \quad \text {f r o m (6 . 1 0 7)}. \\ \end{array}
$$

Properties P6.5 and P6.6, together, can be used to accomplish maximum likelihood estimation for ARMAX models. In this case, the likelihood would be in the innovations form given in Chapter 2, equation (3.106), or equivalently (6.62), and estimation could be accomplished using Newton–Raphson or the EM algorithm as described 6.3.

# 6.7 Bootstrapping State-Space Models

Although, in §6.3, we discussed the fact that, under general conditions (which we assume to hold in this section), the MLEs of the parameters of a DLM are consistent and asymptotically normal, time series data are often of short or moderate length. Several researchers have found evidence that samples must be fairly large before asymptotic results are applicable (Dent and Min, 1978; Ansley and Newbold, 1980). Moreover, as we discussed in Example 3.31, problems occur if the parameters are near the boundary of the parameter

space. In this section, we discuss an algorithm for bootstrapping state-space models; this algorithm and its justification, including the non-Gaussian case, along with numerous examples, can be found in Stoffer and Wall (1991) and in Stoffer and Wall (2004). In view of §6.6, anything we do or say here about DLMs applies equally to ARMAX models.

Using the DLM given by (6.95)–(6.97) and Property P6.5, we write the innovations form of the filter as

$$
\boldsymbol {\epsilon} _ {t} = \boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t} ^ {t - 1} - \Gamma \boldsymbol {u} _ {t}, \tag {6.108}
$$

$$
\Sigma_ {t} = A _ {t} P _ {t} ^ {t - 1} A _ {t} ^ {\prime} + R, \tag {6.109}
$$

$$
\boldsymbol {x} _ {t + 1} ^ {t} = \Phi \boldsymbol {x} _ {t} ^ {t - 1} + \Upsilon \boldsymbol {u} _ {t} + K _ {t} \boldsymbol {\epsilon} _ {t}, \tag {6.110}
$$

$$
K _ {t} = \left[ \Phi P _ {t} ^ {t - 1} A _ {t} ^ {\prime} + S \right] \Sigma_ {t} ^ {- 1}, \tag {6.111}
$$

$$
P _ {t + 1} ^ {t} = \Phi P _ {t} ^ {t - 1} \Phi^ {\prime} + Q - K _ {t} \Sigma_ {t} K _ {t} ^ {\prime}. \tag {6.112}
$$

This form of the filter is just a rearrangement of the filter given in Property P6.5; we have dropped the * in the new form of the gain matrix.

In addition, we can rewrite the model to obtain the innovations form of the model,

$$
\boldsymbol {x} _ {t + 1} ^ {t} = \Phi \boldsymbol {x} _ {t} ^ {t - 1} + \Upsilon \boldsymbol {u} _ {t} + K _ {t} \boldsymbol {\epsilon} _ {t}, \tag {6.113}
$$

$$
\boldsymbol {y} _ {t} = A _ {t} \boldsymbol {x} _ {t} ^ {t - 1} + \Gamma \boldsymbol {u} _ {t} + \boldsymbol {\epsilon} _ {t}. \tag {6.114}
$$

This form of the model is a rewriting of (6.108) and (6.110), and it accommodates the bootstrapping algorithm.

As discussed in Example 6.5, although the innovations $\epsilon _ { t }$ are uncorrelated, initially, $\Sigma _ { t }$ can be different for different time points $t$ . Thus, in a resampling procedure, we can either ignore the first few values of $\mathbf { \epsilon } _ { \mathbf { \epsilon } }$ until $\Sigma _ { t }$ stabilizes or we can work with the standardized innovations

$$
\boldsymbol {e} _ {t} = \Sigma_ {t} ^ {- 1 / 2} \boldsymbol {\epsilon} _ {t}, \tag {6.115}
$$

so we are guaranteed these innovations have, at least, the same first two moments. In (6.115), $\Sigma _ { t } ^ { 1 / 2 }$ denotes the unique square root matrix of $\Sigma _ { t }$ defined by $\Sigma _ { t } ^ { 1 / 2 } \Sigma _ { t } ^ { 1 / 2 } = \Sigma _ { t }$ . In what follows, we base the bootstrap procedure on the standardized innovations, but we stress the fact that, even in this case, ignoring startup values might be necessary, as noted by Stoffer and Wall (1991).

The model coefficients and the correlation structure of the model are uniquely parameterized by a $k \times 1$ parameter vector $\Theta _ { 0 }$ ; that is, $\Phi = \Phi ( \Theta _ { 0 } )$ , $\Upsilon = \Upsilon ( \Theta _ { 0 } )$ , $Q = Q ( \Theta _ { 0 } )$ , $A _ { t } = A _ { t } ( \Theta _ { 0 } )$ , $\Gamma = \Gamma ( \Theta _ { 0 } )$ , and $R = R ( \Theta _ { 0 } )$ . Recall the innovations form of the Gaussian likelihood (ignoring a constant) is

$$
\begin{array}{l} - 2 \ln L _ {Y} (\Theta) = \sum_ {t = 1} ^ {n} \left[ \ln | \Sigma_ {t} (\Theta) | + \boldsymbol {\epsilon} _ {t} (\Theta) ^ {\prime} \Sigma_ {t} (\Theta) ^ {- 1} \boldsymbol {\epsilon} _ {t} (\Theta) \right] \\ = \sum_ {t = 1} ^ {n} \left[ \ln \left| \Sigma_ {t} (\Theta) \right| + e _ {t} (\Theta) ^ {\prime} e _ {t} (\Theta) \right]. \tag {6.116} \\ \end{array}
$$

We stress the fact that it is not necessary for the model to be Gaussian to consider (6.116) as the criterion function to be used for parameter estimation.

Let $\widehat { \Theta }$ denote the MLE of $\Theta _ { 0 }$ , that is, $\widehat { \Theta } = \mathrm { a r g m a x } _ { \Theta } L _ { Y } ( \Theta )$ , obtained by the methods discussed in 6.3. Let $\epsilon _ { t } ( \widehat { \Theta } )$ and $\Sigma _ { t } ( \widehat { \Theta } )$ be the innovation values obtained by running the filter, (6.108)–(6.112), under $\widehat { \Theta }$ . Once this has been done, the bootstrap procedure is accomplished by the following steps.

1. Construct the standardized innovations

$$
\pmb {e} _ {t} (\widehat {\Theta}) = \Sigma_ {t} ^ {- 1 / 2} (\widehat {\Theta}) \pmb {\epsilon} _ {t} (\widehat {\Theta}).
$$

2. Sample, with replacement, $n$ times from the set $\{ e _ { 1 } ( \widehat { \Theta } ) , \ldots , e _ { n } ( \widehat { \Theta } ) \}$ t o obtain $\{ e _ { 1 } ^ { * } ( \widehat { \Theta } ) , \ldots , e _ { n } ^ { * } ( \widehat { \Theta } ) \}$ , a bootstrap sample of standardized innovations.

3. Construct a bootstrap data set $\{ \pmb { y } _ { 1 } ^ { * } , \hdots , \pmb { y } _ { n } ^ { * } \}$ as follows. Define the $( p +$ $q ) \times 1$ vector $\pmb { \xi } _ { t } = ( \pmb { x } _ { t + 1 } ^ { t ^ { \prime } } , \pmb { y } _ { t } ^ { \prime } ) ^ { \prime }$ . Stacking (6.113) and (6.114) results in a vector first-order equation for $\xi _ { t }$ given by

$$
\boldsymbol {\xi} _ {t} = F _ {t} \boldsymbol {\xi} _ {t - 1} + G \boldsymbol {u} _ {t} + H _ {t} \boldsymbol {e} _ {t}, \tag {6.117}
$$

where

$$
F _ {t} = \left[ \begin{array}{c c} \Phi & 0 \\ A _ {t} & 0 \end{array} \right], \quad G = \left[ \begin{array}{c} \Upsilon \\ \Gamma \end{array} \right], \quad H _ {t} = \left[ \begin{array}{c} K _ {t} \Sigma_ {t} ^ {- 1 / 2} \\ \Sigma_ {t} ^ {- 1 / 2} \end{array} \right].
$$

Thus, to construct the bootstrap data set, solve (6.117) using $e _ { t } ^ { * } ( { \widehat { \Theta } } )$ in place of $e _ { t }$ . The exogenous variables $\mathbf { \Delta } \mathbf { u } _ { t }$ and the initial conditions of the Kalman filter remain fixed at their given values, and the parameter vector is held fixed at $\widehat { \Theta }$ .

4. Using the bootstrap data set $\{ \pmb { y } _ { t } ^ { * } ; t = 1 , . . . , n \}$ , construct a likelihood, $L _ { Y ^ { * } } ( \Theta )$ , and obtain the MLE of $\Theta$ , say, ${ \widehat { \Theta } } ^ { * }$ .   
5. Repeat steps 2 through 4, a large number, $B$ , of times, obtaining a bootstrapped set of parameter estimates $\{ \hat { \Theta } _ { b } ^ { * } ; \ \boldsymbol { b } = 1 , \dots , B \}$ . The finite sample distribution of $\widehat { \Theta } - \Theta _ { 0 }$ may be approximated by the distribution of $\widehat { \Theta } _ { b } ^ { \ast } - \widehat { \Theta }$ , $b = 1 , \dots , B$ .

In the next example, we discuss the case of a linear regression model, but where the regression coefficients are stochastic and allowed to vary with time. The state-space model provides a convenient setting for the analysis of such models.

# Example 6.12 Stochastic Regression

Figure 6.8 shows the interest rate recorded for three-month treasury bills (line–squares), $y _ { t }$ , and the quarterly inflation rate (dotted line–circles) in

![](images/95318f7f30ae16104fdc8cb052a71df908544c9115df7dec8dd0e3fc8cfa9aa0.jpg)  
Figure 6.8 Interest rate for three-month treasury bills (line–squares) and quarterly inflation rate (dotted line–circles) in the Consumer Price Index, 1953:1 to 1965:2.

the Consumer Price Index, $z _ { t }$ , from the first quarter of 1953 through the second quarter of 1965, $n = 5 0$ observations. These data were analyzed by Newbold and Bos (1985, pp. 61-73).

In this analysis, the treasury bill interest rate is modeled as being linearly related to quarterly inflation as

$$
y _ {t} = \alpha + \beta_ {t} z _ {t} + v _ {t},
$$

where $\alpha$ is a fixed constant, $\beta _ { t }$ is a stochastic regression coefficient, and $v _ { t }$ is white noise with variance $\sigma _ { v } ^ { 2 }$ . The stochastic regression term, which comprises the state variable, is specified by a first-order autoregression,

$$
(\beta_ {t} - b) = \phi (\beta_ {t - 1} - b) + w _ {t},
$$

where $b$ is a constant, and $w _ { t }$ is white noise with variance $\sigma _ { w } ^ { 2 }$ . The noise processes, ${ \boldsymbol { v } } _ { t }$ and $w _ { t }$ , are assumed to be uncorrelated.

Using the notation of the state-space model (6.95) and (6.96), we have in the state equation, $\sigma _ { w } ^ { 2 }$ , and in the observation equation, $\mathbfit { x } _ { t } ~ = ~ \beta _ { t }$ , $\Phi = \phi$ $A _ { t } ~ = ~ z _ { t }$ , $\mathbf { \delta } { \mathbf { } u } _ { t } \equiv 1$ , , $\Gamma = \alpha$ $\Upsilon = ( 1 - \phi ) b$ , $R = \sigma _ { v } ^ { 2 }$ , , and $Q =$ $S = 0$ . The parameter vector is $\Theta = ( \phi , \alpha , b , \sigma _ { w } , \sigma _ { v } ) ^ { \prime }$ . The results of the Newton–Raphson estimation procedure are listed in Table 6.2. Also shown in the Table 6.2 are the corresponding standard errors obtained from $B = 5 0 0$ runs of the bootstrap. These standard errors are simply

![](images/a80b09ca36285611e28ca8903838f28e28da2199b0a8ffe61867e8fd638d4c6f.jpg)  
Figure 6.9 Bootstrap distribution, $B = 5 0 0$ , of the estimator of $\phi$ .

![](images/93a2c40e68afa36320e58791eafe1dd2cfe50376c553e16bae48f5556be56d14.jpg)  
Figure 6.10 Bootstrap distribution, $B = 5 0 0$ , of the estimator of $\sigma _ { w }$

the standard deviations of the bootstrapped estimates, that is, the square root of $\textstyle \sum _ { b = 1 } ^ { B } ( \Theta _ { i b } ^ { * } - \bar { \Theta } _ { i } ^ { * } ) ^ { 2 } / ( B - 1 )$ , where $\Theta _ { i }$ , represents the $_ i$ th parameter, i = 1, . . . , 5, and Θ¯ ∗i = Bb=1 Θ∗ib/B. $i = 1 , \ldots , 5$ $\begin{array} { r } { \bar { \Theta } _ { i } ^ { * } = \sum _ { b = 1 } ^ { B } \Theta _ { i b } ^ { * } / B } \end{array}$

The asymptotic standard errors listed in Table 6.2 are typically smaller than those obtained from the bootstrap. This result is the most pronounced in the estimates of $\phi$ , $\sigma _ { w }$ , and $\sigma _ { v }$ , where the bootstrapped standard errors are about $5 0 \%$ larger than the corresponding asymptotic value. Also, asymptotic theory prescribes the use of normal theory when dealing with the parameter estimates. The bootstrap, however, allows us to investigate the small sample distribution of the estimators and, hence, provides more insight into the data analysis.

Table 6.2 Comparison of Asymptotic Standard Errors and Bootstrapped Standard Errors ( $B = 5 0 0$ )   

<table><tr><td>Parameter</td><td>MLE</td><td>Asymptotic Standard Error</td><td>Bootstrap Standard Error</td></tr><tr><td>φ</td><td>.841</td><td>.200</td><td>.304</td></tr><tr><td>α</td><td>-.771</td><td>.645</td><td>.645</td></tr><tr><td>b</td><td>.855</td><td>.278</td><td>.277</td></tr><tr><td>σw</td><td>.127</td><td>.092</td><td>.182</td></tr><tr><td>σv</td><td>1.131</td><td>.142</td><td>.217</td></tr></table>

For example, Figure 6.9 shows the bootstrap distribution of the estimator of $\phi$ . This distribution is highly skewed with values concentrated around .8, but with a long tail to the left. Some quantiles of the bootstrapped distribution of $\phi$ are -.09 (2.5%), .03 (5 $\%$ ), .16 (10%), .87 (90%), .92 (95%), .94 (97.5%), and they can be used to obtain confidence intervals. For example, a 90% confidence interval for $\phi$ would be approximated by (.03, .92). This interval is rather wide, and we will interpret this after we discuss the results of the estimation of $\sigma _ { w }$ .

Figure 6.10 shows the bootstrap distribution of $\widehat { \sigma } _ { w }$ . The distribution is concentrated at two locations, one at approximately $\widehat { \sigma } _ { w } = . 1 5$ and the other at $\hat { \sigma } _ { w } = 0$ . The cases in which $\widehat \sigma _ { w } \approx 0$ correspond to deterministic state dynamics. When $\sigma _ { w } = 0$ and $| \phi | < 1$ , then $\beta _ { t } \approx b$ for large $t$ , so the approximately 25% of the cases in which $\widehat { \sigma } _ { w } \approx 0$ suggest a fixed state, or constant coefficient model. The cases in which $\widehat { \sigma } _ { w }$ is away from zero would suggest a truly stochastic regression parameter. To investigate this matter further, Figure 6.11 shows the joint bootstrapped estimates, $( \widehat { \phi } , \widehat { \sigma } _ { w } )$ , for positive values of $\widehat { \phi }$ . The joint distribution suggests $\widehat { \sigma } _ { w } > 0$ corresponds to $\widehat { \phi } \approx 0$ . When $\phi = 0$ , the state dynamics are given by $\beta _ { t } = b + w _ { t }$ . If, in addition, $\sigma _ { w }$ is small relative to $b$ , the system is nearly deterministic; that is, $\beta _ { t } \approx b$ . Considering these results, the bootstrap analysis leads us to conclude the dynamics of the data are best described in terms of a fixed regression effect.

# 6.8 Dynamic Linear Models with Switching

The problem of modeling changes in regimes for vector-valued time series has been of interest in many different fields. In §5.4, we explored the idea that the dynamics of the system of interest might change over the course of time. In Example 5.5, we saw that pneumonia and influenza mortality rates behave differently when a flu epidemic occurs than when no epidemic occurs. As another example, some authors (for example, Hamilton, 1989, or McCulloch and Tsay,

![](images/70943ee1b8118cf99c5c9bda12520ebfa01ccd77dc5811605a4b2e2c2b49e2ce.jpg)  
Figure 6.11 Joint bootstrap distribution, $B = 5 0 0$ , of the estimators of $\phi$ and $\sigma _ { w }$ . Only the values corresponding to $\hat { \phi } ^ { \ast } \geq 0$ are shown.

1993) have explored the possibility the dynamics of the quarterly U.S. GNP series (say, $y _ { t }$ ) analyzed in Example 3.33 might be different during expansion ( $\nabla \log y _ { t } > 0 $ ) than during contraction ( $\nabla \log y _ { t } < 0 \}$ . In this section, we will concentrate on the method presented in Shumway and Stoffer (1991). One way of modeling change in an evolving time series is by assuming the dynamics of some underlying model changes discontinuously at certain undetermined points in time. Our starting point is the DLM given by (6.1) and (6.2), namely,

$$
\boldsymbol {x} _ {t} = \Phi \boldsymbol {x} _ {t - 1} + \boldsymbol {w} _ {t}, \tag {6.118}
$$

to describe the $p \times 1$ state dynamics, and

$$
\boldsymbol {y} _ {t} = A _ {t} \boldsymbol {x} _ {t} + \boldsymbol {v} _ {t} \tag {6.119}
$$

to describe the $q \times 1$ observation dynamics. Recall ${ \pmb w } _ { t }$ and ${ \pmb v } _ { t }$ are Gaussian white noise sequences with $\mathrm { v a r } ( { \pmb w } _ { t } ) = Q$ , $\mathrm { v a r } ( { \pmb v } _ { t } ) = R$ , and $\mathrm { c o v } ( { \pmb w } _ { t } , { \pmb v } _ { s } ) = 0$ for all $s$ and $t$ .

Generalizations of (6.118) and (6.119) to include the possibility of changes occurring over time have been approached by allowing changes in the error covariances (Harrison and Stevens, 1976, Gordon and Smith, 1988, 1990) or by assigning mixture distributions to the observation errors ${ \pmb v } _ { t }$ (Pe˜na and Guttman, 1988). Approximations to filtering were derived in all of the aforementioned articles. An application to monitoring renal transplants was described in Smith and West (1983) and in Gordon and Smith (1990). Changes can also be modeled in the classical regression case by allowing switches in the design matrix, as in Quandt (1972).

Switching via a stationary Markov chain with independent observations has been developed by Lindgren (1978) and Goldfeld and Quandt (1973). In the Markov chain approach, we declare the dynamics of the system at time $t$ i s generated by one of $m$ possible regimes evolving according to a Markov chain over time. As a simple example, suppose the dynamics of a univariate time series, $y _ { t }$ , is generated by either the model (1) $y _ { t } = \beta _ { 1 } y _ { t - 1 } + w _ { t }$ or the model (2) $y _ { t } = \beta _ { 2 } y _ { t - 1 } + w _ { t }$ . We will write the model as $y _ { t } = \phi _ { t } y _ { t - 1 } + w _ { t }$ such that $\mathrm { P r } ( \phi _ { t } = \beta _ { j } ) = \pi _ { j }$ , $j = 1 , 2$ , $\pi _ { 1 } + \pi _ { 2 } = 1$ , and with the Markov property

$$
\operatorname * {P r} (\phi_ {t} = \beta_ {j} \mid \phi_ {t - 1} = \beta_ {i}, \phi_ {t - 2} = \beta_ {i _ {2}}, \ldots) = \operatorname * {P r} (\phi_ {t} = \beta_ {j} \mid \phi_ {t - 1} = \beta_ {i}) = \pi_ {i j},
$$

for $i , j = 1 , 2$ (and $i _ { 2 } , . . . = 1 , 2$ ). As previously mentioned, Markov switching for dependent data has been applied by Hamilton (1989) to detect changes between positive and negative growth periods in the economy. Applications to speech recognition have been considered by Juang and Rabiner (1985). The case in which the particular regime is unknown to the observer comes under the heading of hidden Markov models, and the techniques related to analyzing these models are summarized in Rabiner and Juang (1986). An application of the idea of switching to the tracking of multiple targets has been considered in Bar-Shalom (1978), who obtained approximations to Kalman filtering in terms of weighted averages of the innovations.

# Example 6.13 Tracking Multiple Targets

The approach of Shumway and Stoffer (1991) was motivated primarily by the problem of tracking a large number of moving targets using a vector ${ \pmb y } _ { t }$ of sensors. In this problem, we do not know at any given point in time which target any given sensor has detected. Hence, it is the structure of the measurement matrix $A _ { t }$ in (6.119) that is changing, and not the dynamics of the signal $\pmb { x } _ { t }$ or the noises, ${ \pmb w } _ { t }$ or ${ \pmb v } _ { t }$ . As an example, consider a $3 \times 1$ vector of satellite measurements $\pmb { y } _ { t } = ( y _ { t 1 } , y _ { t 2 } , y _ { t 3 } ) ^ { \prime }$ that are observations on some combination of a $3 \times 1$ vector of targets or signals, $\pmb { x } _ { t } = ( x _ { t 1 } , x _ { t 2 } , x _ { t 3 } ) ^ { \prime }$ . For the measurement matrix

$$
A _ {t} = \left[ \begin{array}{l l l} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 0 & 0 \end{array} \right]
$$

in the model (6.119), all sensors are observing the first target, $x _ { t 1 }$ , whereas for the measurement matrix

$$
A _ {t} = \left[ \begin{array}{l l l} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{array} \right]
$$

the first sensor, $y _ { t 1 }$ , observes the second target, $x _ { t 2 }$ ; the second sensor, $y _ { t 2 }$ , observes the first target, $x _ { t 1 }$ ; and the third sensor, $y _ { t 3 }$ , observes the

third target, $x _ { t 3 }$ . All possible detection configurations will define a set of possible values for $A _ { t }$ , say, $\left\{ M _ { 1 } , M _ { 2 } , \ldots , M _ { m } \right\}$ , as a collection of plausible measurement matrices.

# Example 6.14 Modeling Economic Change

As another example of the switching model presented in this section, consider the case in which the dynamics of the linear model changes suddenly over the history of a given realization. For example, Lam (1990) has given the following generalization of Hamilton’s (1989) model for detecting positive and negative growth periods in the economy. Suppose the data are generated by

$$
y _ {t} = z _ {t} + n _ {t}, \tag {6.120}
$$

where $z _ { t }$ is an autoregressive series and $n _ { t }$ is a random walk with a drift that switches between two values $\alpha _ { 0 }$ and $\alpha _ { 0 } + \alpha _ { 1 }$ . That is,

$$
n _ {t} = n _ {t - 1} + \alpha_ {0} + \alpha_ {1} S _ {t}, \tag {6.121}
$$

with $S _ { t } = 0$ or 1, depending on whether the system is in state 1 or state 2. For the purpose of illustration, suppose

$$
z _ {t} = \phi_ {1} z _ {t - 1} + \phi_ {2} z _ {t - 2} + w _ {t} \tag {6.122}
$$

is an AR(2) series with $\mathrm { v a r } ( w _ { t } ) = \sigma _ { w } ^ { 2 }$ . Lam (1990) wrote (6.120) in a differenced form

$$
\nabla y _ {t} = z _ {t} - z _ {t - 1} + \alpha_ {0} + \alpha_ {1} S _ {t}, \tag {6.123}
$$

which we may take as the observation equation (6.119) with state vector

$$
\boldsymbol {x} _ {t} = \left(z _ {t}, z _ {t - 1}, \alpha_ {0}, \alpha_ {1}\right) ^ {\prime} \tag {6.124}
$$

and

$$
M _ {1} = [ 1, - 1, 1, 0 ] \quad \text {a n d} \quad M _ {2} = [ 1, - 1, 1, 1 ] \tag {6.125}
$$

determining the two possible economic conditions. The state equation, (6.118), is of the form

$$
\left( \begin{array}{c} z _ {t} \\ z _ {t - 1} \\ \alpha_ {0} \\ \alpha_ {1} \end{array} \right) = \left[ \begin{array}{c c c c} \phi_ {1} & \phi_ {2} & 0 & 0 \\ 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{array} \right] \left( \begin{array}{c} z _ {t - 1} \\ z _ {t - 2} \\ \alpha_ {0} \\ \alpha_ {1} \end{array} \right) + \left( \begin{array}{c} w _ {t} \\ 0 \\ 0 \\ 0 \end{array} \right). \tag {6.126}
$$

The observation equation, (6.119), in this case is

$$
\nabla y _ {t} = A _ {t} \boldsymbol {x} _ {t} + v _ {t}, \tag {6.127}
$$

where $\operatorname* { P r } ( A _ { t } = M _ { 1 } ) = 1 - \operatorname* { P r } ( A _ { t } = M _ { 2 } )$ , with $M _ { 1 }$ and $M _ { 2 }$ given in (6.125).

To incorporate a reasonable switching structure for the measurement matrix into the DLM that is compatible with both practical situations previously described, we assume that the $m$ possible configurations are states in a nonstationary, independent process defined by the time-varying probabilities

$$
\pi_ {j} (t) = \Pr \left(A _ {t} = M _ {j}\right), \tag {6.128}
$$

for $j = 1 , \ldots , m$ and $t = 1 , 2 , \ldots , n$ . Important information about the current state of the measurement process is given by the filtered probabilities of being in state $j$ , defined as the conditional probabilities

$$
\pi_ {j} (t | t) = \Pr \left(A _ {t} = M _ {j} \mid Y _ {t}\right), \tag {6.129}
$$

which also vary as a function of time. In (6.129), we have used the notation $Y _ { s } ~ = ~ \{ \pmb { y } _ { 1 } , \ldots , \pmb { y } _ { s } \}$ . The filtered probabilities (6.129) give the time-varying estimates of the probability of being in state $j$ given the data to time $t$ .

It will be important for us to obtain estimators of the configuration probabilities, $\pi _ { j } ( t | t )$ , the predicted and filtered state estimators, $\mathbf { \Delta } \pmb { x } _ { t } ^ { t - 1 }$ and $\mathbf { \Delta } _ { \pmb { x } _ { t } ^ { t } }$ , an d the corresponding error covariance matrices $P _ { t } ^ { t - 1 }$ and $P _ { t } ^ { t }$ . Of course, the predictor and filter estimators will depend on the parameters, $\Theta$ , of the DLM. In many situations, the parameters will be unknown and we will have to estimate them. Our focus will be on maximum likelihood estimation, but other authors have taken a Bayesian approach that assigns priors to the parameters, and then seeks posterior distributions of the model parameters; see, for example, Gordon and Smith (1990), Pe˜na and Guttman (1988), or McCulloch and Tsay (1993).

We now establish the recursions for the filters associated with the state §essential part of the maximum likelihood procedure. The predictors, ${ \pmb x } _ { t }$ and the switching process, $A _ { t }$ . As discussed in 6.3, the filters are also an ${ \pmb x } _ { t } ^ { t - 1 } =$ $E ( \pmb { x } _ { t } | Y _ { t - 1 } )$ , and filters, ${ \pmb x } _ { t } ^ { t } = E ( { \pmb x } _ { t } | Y _ { t } )$ , and their associated error variance– −covariance matrices, $P _ { t } ^ { t - 1 }$ a n d $P _ { t } ^ { t }$ , are given by

$$
\boldsymbol {x} _ {t} ^ {t - 1} = \Phi \boldsymbol {x} _ {t - 1} ^ {t - 1}, \tag {6.130}
$$

$$
P _ {t} ^ {t - 1} = \Phi P _ {t - 1} ^ {t - 1} \Phi^ {\prime} + Q, \tag {6.131}
$$

$$
\boldsymbol {x} _ {t} ^ {t} = \boldsymbol {x} _ {t} ^ {t - 1} + \sum_ {j = 1} ^ {m} \pi_ {j} (t | t) K _ {t j} \boldsymbol {\epsilon} _ {t j}, \tag {6.132}
$$

$$
P _ {t} ^ {t} = \sum_ {j = 1} ^ {m} \pi_ {j} (t | t) \left(I - K _ {t j} M _ {j}\right) P _ {t} ^ {t - 1}, \tag {6.133}
$$

$$
K _ {t j} = P _ {t} ^ {t - 1} M _ {j} ^ {\prime} \Sigma_ {t j} ^ {- 1}, \tag {6.134}
$$

where the innovation values in (6.132) and (6.134) are

$$
\boldsymbol {\epsilon} _ {t j} = \boldsymbol {y} _ {t} - M _ {j} \boldsymbol {x} _ {t} ^ {t - 1}, \tag {6.135}
$$

$$
\Sigma_ {t j} = M _ {j} P _ {t} ^ {t - 1} M _ {j} ^ {\prime} + R, \tag {6.136}
$$

for $j = 1 , \ldots , m$ .

Equations (6.130)-(6.134) exhibit the filter values as weighted linear combinations of the $m$ innovation values, (6.135)-(6.136), corresponding to each of the possible measurement matrices. The equations are similar to the approximations introduced by Bar-Shalom and Tse (1975), by Gordon and Smith (1990), and Pe˜na and Guttman (1988).

To verify (6.132), let the indicator $I ( A _ { t } = M _ { j } ) = 1$ when $A _ { t } = M _ { j }$ , and zero otherwise. Then, using (6.21),

$$
\begin{array}{l} \boldsymbol {x} _ {t} ^ {t} = E (\boldsymbol {x} _ {t} | Y _ {t}) = E [ E (\boldsymbol {x} _ {t} | Y _ {t}, A _ {t}) \mid Y _ {t} ] \\ = E \left\{\sum_ {j = 1} ^ {m} E \left(\boldsymbol {x} _ {t} \mid Y _ {t}, A _ {t} = M _ {j}\right) I \left(A _ {t} = M _ {j}\right) \mid Y _ {t} \right\} \\ = E \left\{\sum_ {j = 1} ^ {m} \left[ \boldsymbol {x} _ {t} ^ {t - 1} + K _ {t j} \left(\boldsymbol {y} _ {t} - M _ {j} \boldsymbol {x} _ {t} ^ {t - 1}\right) \right] I \left(A _ {t} = M _ {j}\right) \mid Y _ {t} \right\} \\ = \sum_ {j = 1} ^ {m} \pi_ {j} (t | t) \left[ \boldsymbol {x} _ {t} ^ {t - 1} + K _ {t j} \left(\boldsymbol {y} _ {t} - M _ {j} \boldsymbol {x} _ {t} ^ {t - 1}\right) \right], \\ \end{array}
$$

where $K _ { t j }$ is given by (6.134). Equation (6.133) is derived in a similar fashion; the other relationships, (6.130), (6.131), and (6.134), follow from straightforward applications of the Kalman filter results given in Property P6.1.

Next, we derive the filters $\pi _ { j } ( t | t )$ . Let $f _ { j } ( t | t - 1 )$ denote the conditional density of $\mathbf { \mathscr { y } } _ { t }$ given the past $\pmb { y } _ { 1 } , \ldots , \pmb { y } _ { t - 1 }$ , and $A _ { t } = M _ { j }$ , for $j = 1 , \ldots , m$ . Then,

$$
\pi_ {j} (t | t) = \frac {\pi_ {j} (t) f _ {j} (t | t - 1)}{\sum_ {k = 1} ^ {m} \pi_ {k} (t) f _ {k} (t | t - 1)}, \tag {6.137}
$$

where we assume the distribution $\pi _ { j } ( t )$ , for $j = 1 , \dots , m$ has been specified before observing $\pmb { y } _ { 1 } , \ldots , \pmb { y } _ { t }$ (details follow as in Example 6.15 below). If the investigator has no reason to prefer one state over another at time $t$ , the choice of uniform priors, $\pi _ { j } ( t ) = m ^ { - 1 }$ , for $j = 1 , \ldots , m$ , will suffice. Smoothness can be introduced by letting

$$
\pi_ {j} (t) = \sum_ {i = 1} ^ {m} \pi_ {i} (t - 1 | t - 1) \pi_ {i j}, \tag {6.138}
$$

where the non-negative weights $\pi _ { i j }$ are chosen so $\textstyle \sum _ { i = 1 } ^ { m } \pi _ { i j } \ = \ 1$ . If the $A _ { t }$ process was Markov with transition probabilities $\pi _ { i j }$ , then (6.138) would be the update for the filter probability, as shown in the next example.

# Example 6.15 Hidden Markov Chain Model

If $\left\{ A _ { t } \right\}$ is a hidden Markov chain with stationary transition probabilities $\pi _ { i j } = \mathrm { P r } ( A _ { t } = M _ { j } | A _ { t - 1 } = M _ { i } )$ ), for $i , j = 1 , \dots , m$ , letting $p ( \cdot )$ denote a generic probability function, we have

$$
\begin{array}{l} \pi_ {j} (t | t) = \frac {p \left(A _ {t} = M _ {j} , \boldsymbol {y} _ {t} , Y _ {t - 1}\right)}{p \left(\boldsymbol {y} _ {t} , Y _ {t - 1}\right)} \\ = \frac {p \left(Y _ {t - 1}\right) p \left(A _ {t} = M _ {j} \mid Y _ {t - 1}\right) p \left(\boldsymbol {y} _ {t} \mid A _ {t} = M _ {j} , Y _ {t - 1}\right)}{p \left(Y _ {t - 1}\right) p \left(\boldsymbol {y} _ {t} \mid Y _ {t - 1}\right)} \\ = \frac {\pi_ {j} (t | t - 1) f _ {j} (t | t - 1)}{\sum_ {k = 1} ^ {m} \pi_ {k} (t | t - 1) f _ {k} (t | t - 1)}. \tag {6.139} \\ \end{array}
$$

In the Markov case, the conditional probabilities

$$
\pi_ {j} (t | t - 1) = \Pr (A _ {t} = M _ {j} | Y _ {t - 1})
$$

in (6.139) replace the unconditional probabilities, $\pi _ { j } ( t ) = \operatorname* { P r } ( A _ { t } = M _ { j } )$ , in (6.137).

To evaluate (6.139), we must be able to calculate $\pi _ { j } ( t | t - 1 )$ and $f _ { j } ( t | t - 1 )$ . We will discuss the calculation of $f _ { j } ( t | t - 1 )$ after this example. To derive $\pi _ { j } ( t | t - 1 )$ , note,

$$
\begin{array}{l} \pi_ {j} (t | t - 1) = \Pr (A _ {t} = M _ {j} \mid Y _ {t - 1}) \\ = \sum_ {i = 1} ^ {m} \Pr (A _ {t} = M _ {j}, A _ {t - 1} = M _ {i} | Y _ {t - 1}) \\ = \sum_ {i = 1} ^ {m} \Pr \left(A _ {t} = M _ {j} \mid A _ {t - 1} = M _ {i}\right) \Pr \left(A _ {t - 1} = M _ {i} \mid Y _ {t - 1}\right) \\ = \sum_ {i = 1} ^ {m} \pi_ {i j} \pi_ {i} (t - 1 | t - 1). \tag {6.140} \\ \end{array}
$$

Expression (6.138) comes from equation (6.140), where, as previously noted, we replace $\pi _ { j } ( t | t - 1 )$ by $\pi _ { j } ( t )$ .

The difficulty in extending the approach here to the Markov case is the dependence among the $\pmb { y } _ { t }$ , which makes it necessary to enumerate over all possible histories to derive the filtering equations. This problem will be evident when we derive the conditional density $f _ { j } ( t | t - 1 )$ . Equation (6.138) has $\pi _ { j } ( t )$ as a function of the past observations, $Y _ { t - 1 }$ , which is inconsistent with our model assumption. Nevertheless, this seems to be a reasonable compromise that allows the data to modify the probabilities $\pi _ { j } ( t )$ , without having to develop a highly computer-intensive technique.

As previously suggested, the computation of $f _ { j } ( t | t - 1 )$ , without some approximations, is highly computer-intensive. To evaluate $f _ { j } ( t | t - 1 )$ , consider the event

$$
A _ {1} = M _ {j _ {1}}, \dots , A _ {t - 1} = M _ {j _ {t - 1}}, \tag {6.141}
$$

for $j _ { i } = 1 , \ldots , m$ , and $i = 1 , \ldots , t - 1$ , which specifies a specific set of measurement matrices through the past; we will write this event as $A _ { ( t - 1 ) } = M _ { ( \ell ) }$ . Because $m ^ { t - 1 }$ possible outcomes exist for $A _ { 1 } , \dotsc , A _ { t - 1 }$ , the index $\ell$ runs through $\ell = 1 , \dots , m ^ { t - 1 }$ . Using this notation, we may write

$$
\begin{array}{l} f _ {j} (t | t - 1) \\ = \sum_ {\ell = 1} ^ {m ^ {t - 1}} \Pr \left\{A _ {(t - 1)} = M _ {(\ell)} \mid Y _ {t - 1} \right\} f \left(\boldsymbol {y} _ {t} \mid Y _ {t - 1}, A _ {t} = M _ {j}, A _ {(t - 1)} = M _ {(\ell)}\right) \\ \equiv \sum_ {\ell = 1} ^ {m ^ {t - 1}} \alpha (\ell) \mathrm {N} \left(\boldsymbol {y} _ {t} \mid \boldsymbol {\mu} _ {t j} (\ell), \Sigma_ {t j} (\ell)\right), \quad j = 1, \dots , m, \tag {6.142} \\ \end{array}
$$

where the notation $\mathrm { N } ( \cdot \mid b , B )$ represents the normal density with mean vector $\pmb { b }$ and variance–covariance matrix $B$ . That is, $f _ { j } ( t | t - 1 )$ is a mixture of normals with non-negative weights $\alpha ( \ell ) = \operatorname* { P r } \{ A _ { ( t - 1 ) } = M _ { ( \ell ) } \ | \ Y _ { t - 1 } \}$ such that $\begin{array} { r } { \sum _ { \ell } \alpha ( \ell ) = 1 } \end{array}$ , and with each normal distribution having mean vector

$$
\boldsymbol {\mu} _ {t j} (\ell) = M _ {j} \boldsymbol {x} _ {t} ^ {t - 1} (\ell) = M _ {j} E [ \boldsymbol {x} _ {t} \mid Y _ {t - 1}, A _ {(t - 1)} = M _ {(\ell)} ] \tag {6.143}
$$

and covariance matrix

$$
\Sigma_ {t j} (\ell) = M _ {j} P _ {t} ^ {t - 1} (\ell) M _ {j} ^ {\prime} + R. \tag {6.144}
$$

This result follows because the conditional distribution of $\pmb { y } _ { t }$ in (6.142) is identical to the fixed measurement matrix case presented in Section 4.2. The values in (6.143) and (6.144), and hence the densities, $f _ { j } ( t | t - 1 )$ , for $j = 1 , \ldots , m$ , can be obtained directly from the Kalman filter, Property P6.1, with the measurement matrices $A _ { ( t - 1 ) }$ fixed at $M _ { ( \ell ) }$ .

Although $f _ { j } ( t | t - 1 )$ is given explicitly in (6.142), its evaluation is highly computer intensive. For example, with $m = 2$ states and $n = 2 0$ observations, we have to filter over $2 + 2 ^ { 2 } + \cdots + 2 ^ { 2 0 }$ possible sample paths; note, $2 ^ { 2 0 } =$ $1 , 0 4 8 , 5 7 6$ . One remedy is to trim (remove), at each $t$ , highly improbable sample paths; that is, remove events in (6.141) with extremely small probability of occurring, and then evaluate $f _ { j } ( t | t - 1 )$ as if the trimmed sample paths could not have occurred. Another alternative, as suggested by Gordon and Smith (1990) and Shumway and Stoffer (1991), is to approximate $f _ { j } ( t | t - 1 )$ using the closest (in the sense of Kulback–Leibler distance) normal distribution. In this case, the approximation leads to choosing normal distribution with the same mean and variance associated with $f _ { j } ( t | t - 1 )$ ; that is, we approximate $f _ { j } ( t | t - 1 )$ by a normal with mean $M _ { j } { \pmb x } _ { t } ^ { t - 1 }$ and variance $\Sigma _ { t j }$ given in (6.136).

To develop a procedure for maximum likelihood estimation, the joint density of the data is

$$
\begin{array}{l} f (\boldsymbol {y} _ {1}, \dots , \boldsymbol {y} _ {n}) = \prod_ {t = 1} ^ {n} f (\boldsymbol {y} _ {t} | Y _ {t - 1}) \\ = \prod_ {t = 1} ^ {n} \sum_ {j = 1} ^ {m} \Pr (A _ {t} = M _ {j} | Y _ {t - 1}) f (\boldsymbol {y} _ {t} | A _ {t} = M _ {j}, Y _ {t - 1}), \\ \end{array}
$$

and hence, the likelihood can be written as

$$
\ln L _ {Y} (\Theta) = \sum_ {t = 1} ^ {n} \ln \left(\sum_ {j = 1} ^ {m} \pi_ {j} (t) f _ {j} (t | t - 1)\right). \tag {6.145}
$$

For the hidden Markov model, $\pi _ { j } ( t )$ would be replaced by $\pi _ { j } ( t | t - 1 )$ . In (6.145), we will use the normal approximation to $f _ { j } ( t | t - 1 )$ . That is, henceforth, we will consider $f _ { j } ( t | t - 1 )$ as the normal, $\begin{array} { r } { \mathrm { N } ( M _ { j } \pmb { x } _ { t } ^ { t - 1 } , \Sigma _ { t j } ) } \end{array}$ , density, where $\mathbf { \Delta } \pmb { x } _ { t } ^ { t - 1 }$ is given in (6.130) and $\Sigma _ { t j }$ is given in (6.136). We may consider maximizing (6.145) directly as a function of the parameters $\Theta = \{ \mu _ { 0 } , \Phi , Q , R \}$ using a Newton method, or we may consider applying the EM algorithm to the complete data likelihood.

To apply the EM algorithm as in §6.3, we call $\pmb { x } _ { 0 } , \pmb { x } _ { 1 } , \ldots , \pmb { x } _ { n }$ , $A _ { 1 } , \ldots , A _ { n }$ , and $\pmb { y } _ { 1 } , \ldots , \pmb { y } _ { n }$ , the complete data, with likelihood given by

$$
\begin{array}{l} - 2 \ln L _ {X, A, Y} (\Theta) = \ln | \Sigma_ {0} | + \left(\boldsymbol {x} _ {0} - \boldsymbol {\mu} _ {0}\right) ^ {\prime} \Sigma_ {0} ^ {- 1} \left(\boldsymbol {x} _ {0} - \boldsymbol {\mu} _ {0}\right) \\ + n \ln | Q | + \sum_ {t = 1} ^ {n} \left(\boldsymbol {x} _ {t} - \Phi \boldsymbol {x} _ {t - 1}\right) ^ {\prime} Q ^ {- 1} \left(\boldsymbol {x} _ {t} - \Phi \boldsymbol {x} _ {t - 1}\right) \\ - 2 \sum_ {t = 1} ^ {n} \sum_ {j = 1} ^ {m} I \left(A _ {t} = M _ {j}\right) \ln \pi_ {j} (t) + n \ln | R | \\ + \sum_ {t = 1} ^ {n} \sum_ {j = 1} ^ {m} I \left(A _ {t} = M _ {j}\right) \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) ^ {\prime} R ^ {- 1} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right). \tag {6.146} \\ \end{array}
$$

As discussed in §6.3, we require the minimization of the conditional expectation

$$
Q \left(\Theta \mid \Theta^ {(k - 1)}\right) = E \left\{- 2 \ln L _ {X, A, Y} (\Theta) \mid Y _ {n}, \Theta^ {(k - 1)} \right\}, \tag {6.147}
$$

with respect to $\Theta$ at each iteration, $k = 1 , 2 , \dots$ . The calculation and maximization of (6.147) is similar to the case of (6.65). In particular, with

$$
\pi_ {j} (t | n) = E \left[ I \left(A _ {t} = M _ {j}\right) \mid Y _ {n} \right], \tag {6.148}
$$

we obtain on iteration $k$ ,

$$
\pi_ {j} ^ {(k)} (t) = \pi_ {j} (t | n), \tag {6.149}
$$

$$
\boldsymbol {\mu} _ {0} ^ {(k)} = \boldsymbol {x} _ {0} ^ {n}, \tag {6.150}
$$

$$
\Phi^ {(k)} = S _ {1 0} S _ {0 0} ^ {- 1}, \tag {6.151}
$$

$$
Q ^ {(k)} = n ^ {- 1} \left(S _ {1 1} - S _ {1 0} S _ {0 0} ^ {- 1} S _ {1 0} ^ {\prime}\right), \tag {6.152}
$$

and

$$
R ^ {(k)} = n ^ {- 1} \sum_ {t = 1} ^ {n} \sum_ {j = 1} ^ {m} \pi_ {j} (t | n) \left[ \left(\boldsymbol {y} _ {t} - M _ {j} \boldsymbol {x} _ {t} ^ {n}\right) \left(\boldsymbol {y} _ {t} - M _ {j} \boldsymbol {x} _ {t} ^ {n}\right) ^ {\prime} + M _ {j} P _ {t} ^ {n} M _ {j} ^ {\prime} \right]. \tag {6.153}
$$

where $S _ { 1 1 } , S _ { 1 0 } , S _ { 0 0 }$ are given in (6.67)-(6.69). As before, at iteration $k$ , the filters and the smoothers are calculated using the current values of the parameters, $\Theta ^ { ( k - 1 ) }$ , and $\Sigma _ { 0 }$ is held fixed. Filtering is accomplished by using (6.130)-(6.134). Smoothing is derived in a similar manner to the derivation of the filter, and one is led to the smoother given in Property P6.2 and P6.3, with one exception, the initial smoother covariance, (6.55), is now

$$
P _ {n, n - 1} ^ {n} = \sum_ {j = 1} ^ {m} \pi_ {j} (n | n) \left(I - K _ {t j} M _ {j}\right) \Phi P _ {n - 1} ^ {n - 1}. \tag {6.154}
$$

Unfortunately, the computation of $\pi _ { j } ( t | n )$ is excessively complicated, and requires integrating over mixtures of normal distributions. Shumway and Stoffer (1991) suggest approximating the smoother $\pi _ { j } ( t | n )$ by the filter $\pi _ { j } ( t | t )$ , and find the approximation works well.

# Example 6.16 Analysis of Influenza Data

We use the results of this section to analyze the U.S. monthly pneumonia and influenza mortality data presented in §5.4, Figure 5.7. Letting $y _ { t }$ denote the mortality caused by pneumonia and influenza at month $t$ , we model $y _ { t }$ in terms of a structural component model coupled with a hidden Markov process that determines whether a flu epidemic exists.

The model consists of three structural components. The first component, $x _ { t 1 }$ , is an AR(2) process chosen to represent the periodic (seasonal) component of the data,

$$
x _ {t 1} = \alpha_ {1} x _ {t - 1, 1} + \alpha_ {2} x _ {t - 2, 1} + w _ {t 1}, \tag {6.155}
$$

where $w _ { t 1 }$ is white noise, with $\mathrm { v a r } ( w _ { t 1 } ) = \sigma _ { 1 } ^ { 2 }$ . The second component, $x _ { t 2 }$ , is an AR(1) process with a nonzero constant term, which is chosen to represent the sharp rise in the data during an epidemic,

$$
x _ {t 2} = \beta_ {0} + \beta_ {1} x _ {t - 1, 2} + w _ {t 2}, \tag {6.156}
$$

where $w _ { t 2 }$ is white noise, with $\mathrm { v a r } ( w _ { t 2 } ) = \sigma _ { 2 } ^ { 2 }$ . The third component, $x _ { t 3 }$ is a fixed trend component given by,

$$
x _ {t 3} = x _ {t - 1, 3} + w _ {t 3}, \tag {6.157}
$$

where $\mathrm { v a r } ( w _ { t 3 } ) = 0$ . The case in which $\mathrm { v a r } ( w _ { t 3 } ) > 0$ , which corresponds to a stochastic trend (random walk), was tried here, but the estimation became unstable, and lead to us fitting a fixed, rather than stochastic, trend. Thus, in the final model, the trend component satisfies $\nabla x _ { t 3 } = 0$ ; recall in Example 2.42 the data were also differenced once before fitting the model.

Table 6.3 Estimation Results for Influenza Data   

<table><tr><td>Parameter</td><td>Initial Estimate</td><td>Final Estimate</td></tr><tr><td>α1</td><td>1.401 (.079)</td><td>1.379 (.073)</td></tr><tr><td>α2</td><td>-.618 (.091)</td><td>-.575 (.075)</td></tr><tr><td>β0</td><td>.162 (.042)</td><td>.201 (.028)</td></tr><tr><td>β1</td><td>.156 (.142)</td><td>—</td></tr><tr><td>σ1</td><td>.023 (.001)</td><td>.023 (.001)</td></tr><tr><td>σ2</td><td>.105 (.015)</td><td>.108 (.016)</td></tr><tr><td>σv</td><td>.000 (.032)</td><td>—</td></tr></table>

Estimated standard errors are shown in parentheses.

Throughout the years, periods of normal influenza mortality (state 1) are modeled as

$$
y _ {t} = x _ {t 1} + x _ {t 3} + v _ {t}, \tag {6.158}
$$

where the measurement error, $v _ { t }$ , is white noise with $\mathrm { v a r } ( v _ { t } ) = \sigma _ { v } ^ { 2 }$ . When an epidemic occurs (state 2), mortality is modeled as

$$
y _ {t} = x _ {t 1} + x _ {t 2} + x _ {t 3} + v _ {t}. \tag {6.159}
$$

The model specified in (6.155)–(6.159) can be written in the general state-space form. The state equation is

$$
\left( \begin{array}{c} x _ {t 1} \\ x _ {t - 1, 1} \\ x _ {t 2} \\ x _ {t 3} \end{array} \right) = \left[ \begin{array}{c c c c} \alpha_ {1} & \alpha_ {2} & 0 & 0 \\ 1 & 0 & 0 & 0 \\ 0 & 0 & \beta_ {1} & 0 \\ 0 & 0 & 0 & 1 \end{array} \right] \left( \begin{array}{c} x _ {t - 1, 1} \\ x _ {t - 2, 1} \\ x _ {t - 1, 2} \\ x _ {t - 1, 3} \end{array} \right) + \left( \begin{array}{c} 0 \\ 0 \\ \beta_ {0} \\ 0 \end{array} \right) + \left( \begin{array}{c} w _ {t 1} \\ 0 \\ w _ {t 2} \\ 0 \end{array} \right). \tag {6.160}
$$

Of course, (6.160) can be written in the standard state-equation form as

$$
\boldsymbol {x} _ {t} = \Phi \boldsymbol {x} _ {t - 1} + \Gamma u _ {t} + \boldsymbol {w} _ {t}, \tag {6.161}
$$

where $\pmb { x } _ { t } = ( x _ { t 1 } , x _ { t - 1 , 1 } , x _ { t 2 } , x _ { t 3 } ) ^ { \prime }$ , $\Gamma = ( 0 , 0 , \beta _ { 0 } , 0 ) ^ { \prime }$ , $u _ { t } \equiv 1$ , and $Q$ is a $4 \times 4$ matrix with $\sigma _ { 1 } ^ { 2 }$ as the (1,1)-element, $\sigma _ { 2 } ^ { 2 }$ as the (3,3)-element, and the remaining elements set equal to zero. The observation equation is

$$
y _ {t} = A _ {t} \boldsymbol {x} _ {t} + v _ {t}, \tag {6.162}
$$

where $A _ { t }$ is $1 \times 4$ , and $v _ { t }$ is white noise with $\mathrm { v a r } ( v _ { t } ) = R = \sigma _ { v } ^ { 2 }$ . We assume all components of variance $w _ { t 1 }$ , $w _ { t 2 }$ , and $v _ { t }$ are uncorrelated.

As discussed in (6.158) and (6.159), $A _ { t }$ can take one of two possible forms

$$
A _ {t} = M _ {1} = [ 1, 0, 0, 1 ] \quad \text {n o e p i d e m i c},
$$

$$
A _ {t} = M _ {2} = [ 1, 0, 1, 1 ] \text {e p i d e m i c},
$$

![](images/680f2b46b4b97252faa590c258224df5351013f9af314a4d031ed147c6e0972a.jpg)  
Figure 6.12 Influenza data, $y _ { t }$ , (dark line–squares) and the predicted probability that no epidemic occurs in month $t$ given the past, $\widehat { \pi } _ { 1 } ( t | t - 1 )$ (line–circles) for the ten-year period 1969-1978; 1968 is not shown.

corresponding to the two possible states of (1) no flu epidemic and (2) flu epidemic, such that $\operatorname* { P r } ( A _ { t } \ = \ M _ { 1 } ) \ = \ 1 - \operatorname* { P r } ( A _ { t } \ = \ M _ { 2 } )$ . In this example, we will assume $A _ { t }$ is a hidden Markov chain, and hence we use the updating equations given in Example 6.15, (6.139) and (6.140), with transition probabilities $\pi _ { 1 1 } = \pi _ { 2 2 } = . 7 5$ (and, thus, $\pi _ { 1 2 } = \pi _ { 2 1 } = . 2 5$ ).

Parameter estimation was accomplished using a quasi-Newton–Raphson procedure to maximize the approximate log likelihood given in (6.145), with initial values of $\pi _ { 1 } ( 1 | 0 ) = \pi _ { 2 } ( 1 | 0 ) = . 5$ . Table 6.3 shows the results of the estimation procedure. On the initial fit, two estimates are not significant, namely, ${ \widehat { \beta } } _ { 1 }$ and $\widehat { \sigma } _ { v }$ . When $\sigma _ { v } ^ { 2 } = 0$ , there is no measurement error, and the variability in data is explained solely by the variance components of the state system, namely, $\sigma _ { 1 } ^ { 2 }$ and $\sigma _ { 2 } ^ { 2 }$ . The case in which $\beta _ { 1 } = 0$ corresponds to a simple level shift during a flu epidemic. In the final model, with $\beta _ { 1 }$ and $\sigma _ { v } ^ { 2 }$ removed, the estimated level shift ( ${ \widehat { \beta } } _ { 0 }$ ) corresponds to an increase in mortality by about .2 per 1000 during a flu epidemic. The estimates for the final model are also listed in Table 6.3.

Figure 6.12 shows a plot of the data, $y _ { t }$ , for the ten-year period of 1969-1978 as well as the estimated approximate conditional probabili-

![](images/a9e93f931882ccff88b5d9393263322256aaf39a8af18bf52db3bbed0295e818.jpg)  
Figure 6.13 The three filtered structural components of influenza mortality: $\widehat { x } _ { t 1 } ^ { t }$ (cyclic trace), $\widehat { x } _ { t 2 } ^ { t }$ (spiked trace), and $\widehat { x } _ { t 3 } ^ { t }$ (negative linear trace) for the ten-year period 1969-1978.

ties $\widehat { \pi } _ { 1 } ( t | t - 1 )$ , that is, the predicted probability no epidemic occurs in month $t$ given the past, $y _ { 1 } , \ldots , y _ { t - 1 }$ . The results for the first year of the data, 1968, are not included in the figure because of initial instabilities of the filter. Of course, the estimated predicted probability a flu epidemic will occur next month is $\widehat { \pi } _ { 2 } ( t | t - 1 ) = 1 - \widehat { \pi } _ { 1 } ( t | t - 1 )$ . Thus, a good estimator would have small values of $\widehat { \pi } _ { 1 } ( t | t - 1 )$ corresponding to peaks in $y _ { t }$ . Except for initial values where instability exists, the estimated prediction probabilities are right on the mark. That is, the predicted probability of a flu epidemic exceeds the probability of no epidemic when indeed a flu epidemic occurred the next month.

Figure 6.13 shows the estimated filtered values (that is, filtering is done using the parameter estimates) of the three components of the model, xtt1 $\boldsymbol { x } _ { t 1 } ^ { t }$ , $x _ { t 2 } ^ { t }$ , and $x _ { t 3 } ^ { t }$ . Except for initial instability (which is not shown), $\widehat { x } _ { t 1 } ^ { t }$ represents the seasonal (cyclic) aspect of the data, $\widehat { x } _ { t 2 } ^ { t }$ represents the spikes during a flu epidemic, and $\widehat { x } _ { t 3 } ^ { t }$ represents the slow decline in flu mortality over the ten-year period of 1969-1978.

One-month-ahead prediction, say, $\widehat { y } _ { t } ^ { t - 1 }$ , is obtained as follows,

$$
\widehat {y} _ {t} ^ {t - 1} = M _ {1} \widehat {\boldsymbol {x}} _ {t} ^ {t - 1} \quad \text {i f} \quad \widehat {\pi} _ {1} (t | t - 1) > \widehat {\pi} _ {2} (t | t - 1),
$$

$$
\widehat {y} _ {t} ^ {t - 1} = M _ {2} \widehat {\boldsymbol {x}} _ {t} ^ {t - 1} \quad \text {i f} \quad \widehat {\pi} _ {1} (t | t - 1) \leq \widehat {\pi} _ {2} (t | t - 1).
$$

![](images/c83b473a8880ca4d77afd4d56ea2faea98919d0df20cd07977efb7c3756e02bc.jpg)  
Figure 6.14 One-month-ahead prediction, $\widehat { y } _ { t } ^ { t - 1 }$ (line), of the number of deaths caused by pneumonia and influenza, $y _ { t }$ (points) for 1969-1978. The standard error of the prediction is .02 when a flu epidemic is not predicted, and .11 when a flu epidemic is predicted.

Of course, $\widehat { \pmb { x } } _ { t } ^ { t - 1 }$ is the estimated state prediction, obtained via the filter presented in (6.130)-(6.134) (with the addition of the constant term in the model) using the estimated parameters. The results are shown in Figure 6.14. The precision of the forecasts can be measured by the innovation variances, $\Sigma _ { t 1 }$ when no epidemic is predicted, and $\Sigma _ { t 2 }$ when an epidemic is predicted. These values become stable quickly, and when no epidemic is predicted, the estimated standard error of the prediction is approximately .02 (this is the square root of $\Sigma _ { t 1 }$ for $t$ large); when a flu epidemic is predicted, the estimated standard error of the prediction is approximately .11.

The results of this analysis are impressive given the small number of parameters and the degree of approximation that was made to obtain a computationally simple method for fitting a complex model. In particular, as seen in Figure 6.12, the model is never fooled as to when a flu epidemic will occur. This result is particularly impressive, given that, for example, in the third year, around $t = 3 6$ , it appeared as though an epidemic was about to begin, but it never was realized, and the model

predicted no flu epidemic that year. As seen in Figure 6.14, the predicted mortality tends to be underestimated during the peaks, but the true values are typically within one standard error of the predicted value. Further evidence of the strength of this technique can be found in the example given in Shumway and Stoffer (1991).

# 6.9 Nonlinear and Non-normal State-Space Models Using Monte Carlo Methods

Most of this chapter has focused on linear dynamic models assumed to be Gaussian processes. Historically, these models were convenient because analyzing the data was a relatively simple matter. These assumptions cannot cover every situation, and it is advantageous to explore departures from these assumptions. As seen in §6.8, the solution to the nonlinear and non-Gaussian case will require computer-intensive techniques currently in vogue because of the availability of cheap and fast computers. In this section, we take a Bayesian approach to forecasting as our main objective; see West and Harrison (1997) for a detailed account of Bayesian forecasting with dynamic models. Prior to the mid-1980s, a number of approximation methods were developed to filter non-normal or nonlinear processes in an attempt to circumvent the computational complexity of the analysis of such models. For example, the extended Kalman filter and the Gaussian sum filter (Alspach and Sorensen, 1972) are two such methods described in detail in Anderson and Moore (1979). As in the previous section, these techniques typically rely on approximating the nonnormal distribution by one or several Gaussian distributions or by some other parametric function.

With the advent of cheap and fast computing, a number of authors developed computer-intensive methods based on numerical integration. For example, Kitagawa (1987) proposed a numerical method based on piecewise linear approximations to the density functions for prediction, filtering, and smoothing for non-Gaussian and nonstationary state-space models. Pole and West (1988) used Gaussian quadrature techniques in a Bayesian analysis of nonlinear dynamic models; West and Harrison (1997, Chapter 13) provide a detailed explanation of these and similar methods. Markov chain Monte Carlo (MCMC) methods refer to Monte Carlo integration methods that use a Markovian updating scheme. We will describe the method in more detail later. The most common MCMC method is the Gibbs sampler, which is essentially a modification of the Metropolis algorithm (Metropolis et al., 1953) developed by Hastings (1970) in the statistical setting and by Geman and Geman (1984) in the context of image restoration. Later, Tanner and Wong (1987) used the ideas in their substitution sampling approach, and Gelfand and Smith (1990) developed the Gibbs sampler for a wide class of parametric models. This technique

was first used by Carlin et al. (1992) in the context of general nonlinear and non-Gaussian state-space models. Fr¨uhwirth-Schnatter (1994) and Carter and Kohn (1994) built on these ideas to develop efficient Gibbs sampling schemes for more restrictive models.

If the model is linear, that is, (6.1) and (6.2) hold, but the distributions are not Gaussian, a non-Gaussian likelihood can be defined by (6.31) in §6.2, but where $f _ { 0 } ( \cdot )$ , $f _ { w } ( \cdot )$ and $f _ { v } ( \cdot )$ are not normal densities. In this case, prediction and filtering can be accomplished using numerical integration techniques (e.g., Kitagawa, 1987; Pole and West, 1988) or Monte Carlo techniques (e.g. Fr¨uhwirth-Schnatter, 1994; Carter and Kohn, 1994) to evaluate (6.32) and (6.33). Of course, the prediction and filter densities $p _ { \Theta } ( \pmb { x } _ { t } \ \vert \ Y _ { t - 1 } )$ and $p _ { \Theta } ( \pmb { x } _ { t } \ | \ Y _ { t } )$ will no longer be Gaussian and will not generally be of the locationscale form as in the Gaussian case. A rich class of non-normal densities is given in (6.173).

In general, the state-space model can be given by the following equations:

$$
\boldsymbol {x} _ {t} = F _ {t} \left(\boldsymbol {x} _ {t - 1}, \boldsymbol {w} _ {t}\right) \quad \text {a n d} \quad \boldsymbol {y} _ {t} = H _ {t} \left(\boldsymbol {x} _ {t}, \boldsymbol {v} _ {t}\right), \tag {6.163}
$$

where $F _ { t }$ and $H _ { t }$ are known functions that may depend on parameters $\Theta$ and ${ \pmb w } _ { t }$ and ${ \pmb v } _ { t }$ are white noise processes. The main component of the model retained by (6.163) is that the states are Markov, and the observations are conditionally independent, but we do not necessarily assume $F _ { t }$ and $H _ { t }$ are linear, or ${ \pmb w } _ { t }$ and ${ \pmb v } _ { t }$ are Gaussian. Of course, if $F _ { t } ( \pmb { x } _ { t - 1 } , \pmb { w } _ { t } ) = \Phi _ { t } \pmb { x } _ { t - 1 } + \pmb { w } _ { t }$ and $H _ { t } ( \pmb { x } _ { t } , \pmb { v } _ { t } ) = A _ { t } \pmb { x } _ { t } + \pmb { v } _ { t }$ and ${ \pmb w } _ { t }$ and ${ \pmb v } _ { t }$ are Gaussian, we have the standard DLM (exogenous variables can be added to the model in the usual way). In the general model, (6.163), the likelihood is given by

$$
L _ {X, Y} (\Theta) = p _ {\Theta} \left(\boldsymbol {x} _ {0}\right) \prod_ {t = 1} ^ {n} p _ {\Theta} \left(\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}\right) p _ {\Theta} \left(\boldsymbol {y} _ {t} \mid \boldsymbol {x} _ {t}\right), \tag {6.164}
$$

and the prediction and filter densities, as given by (6.32) and (6.33) in Section 4.2, still hold.

Because our focus is on simulation using MCMC methods, we first describe the technique in a general context.

# Example 6.17 MCMC Techniques and the Gibbs Sampler

The goal of a Monte Carlo technique, of course, is to simulate a pseudorandom sample of vectors from a desired density function $p _ { \Theta } ( z )$ . In Markov chain Monte Carlo, we simulate an ordered sequence of pseudorandom vectors, $z _ { 0 } \mapsto z _ { 1 } \mapsto z _ { 2 } \mapsto \cdot \cdot \cdot$ by specifying a starting value, $z _ { \mathrm { 0 } }$ and then sampling successive values from a transition density $\pi ( \boldsymbol { z } _ { t } | \boldsymbol { z } _ { t - 1 } )$ , for $t = 1 , 2 , \ldots$ . In this way, conditional on $z _ { t - 1 }$ , the $t$ -th pseudo-random vector, ${ z } _ { t }$ , is simulated independent of its predecessors. This technique alone does not yield a pseudo-random sample because contiguous draws are dependent on each other (that is, we obtain a first-order dependent

sequence of pseudo-random vectors). If done appropriately, the dependence between the pseudo-variates ${ z } _ { t }$ and $z _ { t + m }$ decays exponentially in $m$ , and we may regard the collection $\{ z _ { t + \ell m } ; \ell = 1 , 2 , . . . \}$ for $t$ and $m$ suitably large, as a pseudo-random sample. Alternately, one may repeat the process in parallel, retaining the $ { \boldsymbol { z } } _ { m } ^ { ( g ) }$ , for large $m$ . Under general con $m$ -th value, on run ions, the Markov chain converges $g = 1 , 2 , \ldots$ , say, tionary and the individual ${ \boldsymbol { z } } _ { t }$ are marginally distributed according to the stationary “target” density $p _ { \Theta } ( z )$ . Technical details may be found in Tierney (1994).

For Gibbs sampling, suppose we have a collection $\{ z _ { 1 } , \ldots , z _ { k } \}$ of random vectors with complete conditional densities denoted generically by

$$
p _ {\Theta} \left(\boldsymbol {z} _ {j} \mid \boldsymbol {z} _ {i}, i \neq j\right) \equiv p _ {\Theta} \left(\boldsymbol {z} _ {j} \mid \boldsymbol {z} _ {1}, \dots , \boldsymbol {z} _ {j - 1}, \boldsymbol {z} _ {j + 1}, \dots , \boldsymbol {z} _ {k}\right),
$$

for $j = 1 , \dots , k$ , available for sampling. Here, available means pseudosamples may be generated by some method given the values of the appropriate conditioning random vectors. Under mild conditions, these complete conditionals uniquely determine the full joint density $p _ { \Theta } ( z _ { 1 } , \dots , z _ { k } )$ and, consequently, all marginals, $p _ { \Theta } ( z _ { j } )$ for $j = 1 , \ldots , k$ ; details may be found in Besag (1974). The Gibbs sampler generates pseudo-samples from the joint distribution as follows. Start with an arbitrary set of starting values, say, $\{ z _ { 1 [ 0 ] } , \dotsc , z _ { k [ 0 ] } \}$ . Draw $z _ { 1 [ 1 ] }$ from $p _ { \Theta } ( z _ { 1 } | z _ { 2 [ 0 ] } , \dots , z _ { k [ 0 ] } )$ , then draw $z _ { 2 [ 1 ] }$ from $p _ { \Theta } ( z _ { 2 } | z _ { 1 [ 1 ] } , z _ { 3 [ 0 ] } , \dots , z _ { k [ 0 ] } )$ , and so on up to $z _ { k [ 1 ] }$ from $p _ { \Theta } ( z _ { k } | z _ { 1 [ 1 ] } , \dots , z _ { k - 1 [ 1 ] } )$ , to complete one iteration. After $\ell$ such iterations, we have the collection $\{ z _ { 1 [ \ell ] } , \ldots , z _ { k [ \ell ] } \}$ . Geman and Geman (1984) showed that under mild conditions, $\{ z _ { 1 [ \ell ] } , \ldots , z _ { k [ \ell ] } \}$ converges ( $\ell \to \infty$ ) in distribution to a random observation from $p _ { \Theta } ( z _ { 1 } , \dots , z _ { k } )$ . For this reason, we typically drop the subscript $[ \ell ]$ from the notation, assuming $\ell$ is sufficiently large for the generated sample to be thought of as a realization from the joint density; hence, we denote this first realization as $\{ z _ { 1 [ \ell ] } ^ { ( 1 ) } , \ldots , z _ { k [ \ell ] } ^ { ( 1 ) } \} \equiv \{ z _ { 1 } ^ { ( 1 ) } , \ldots , z _ { k } ^ { ( 1 ) } \}$ zk . This entire process is replicated in parallel, a large number, $G$ , of times providing pseudo-random iid collections $\{ z _ { 1 } ^ { ( g ) } , \ldots , z _ { k } ^ { ( g ) } \}$ . . , z(g)k } , for $g = 1 , \ldots , G$ from the joint distribution. These simulated values can the be used to estimate the marginal densities. In particular, if $p _ { \Theta } ( z _ { j } | z _ { i } , i \neq j )$ is available in closed form, then

$$
\widehat {p} _ {\Theta} \left(\boldsymbol {z} _ {j}\right) = G ^ {- 1} \sum_ {g = 1} ^ {G} p _ {\Theta} \left(\boldsymbol {z} _ {j} \mid \boldsymbol {z} _ {i} ^ {(g)}, i \neq j\right). \tag {6.165}
$$

Approximation (6.165) is based on the fact that, for random variables $x$ and $y$ with joint density $p ( x , y )$ , the marginal density of $x$ is obtained as follows: $\begin{array} { r } { p ( x ) = \int p ( x , y ) d y = \int p ( x | y ) p ( y ) d y } \end{array}$ . Because of the relatively recent appearance of Gibbs sampling methodology, several important theoretical and practical issues are under investigation. These issues

include the diagnosis of convergence, modification of the sampling order, efficient estimation, and sequential sampling schemes (as opposed to the parallel processing described above) to mention a few. At this time, the best advice can be obtained from the texts by Gelman et al. (1995) and Gilks et al. (1996), and we are certain that many more will follow.

Finally, it may be necessary to nest rejection sampling within the Gibbs sampling procedure. The need for rejection sampling arises when we want to sample from a density, say, $f ( z )$ , but $f ( z )$ is known only up to a proportionality constant, say, $p ( z ) \propto f ( z )$ . If a density $g ( z )$ is available, and there is a constant $c$ for which $p ( z ) \leq c g ( z )$ for all $z$ , the rejection algorithm generates pseudo-variates from $f ( z )$ by generating a value, $z ^ { * }$ from $g ( z )$ and accepting it as a value from $f ( z )$ with probability $\pi ( z ^ { * } ) = p ( z ^ { * } ) / [ c g ( z ^ { * } ) ]$ . This algorithm can be quite inefficient if $\pi ( \cdot )$ is close to zero; in such cases, more sophisticated envelope functions may be needed. Further discussion of these matters in the case of nonlinear state-space models can be found in Carlin et al. (1992, Examples 1.2 and 3.2).

In Example 6.17, the generic random vectors $z _ { j }$ can represent parameter values, such as components of $\Theta$ , state values ${ \pmb x } _ { t }$ , or future observations ${ \pmb y } _ { n + m }$ , for $m \geq 1$ . This will become evident in the following examples. Before discussing the general case of nonlinear and non-normal state-space models, we briefly introduce MCMC methods for the Gaussian DLM, as presented in Fr¨uhwirth-Schnatter (1994) and Carter and Kohn (1994).

# Example 6.18 Assessing Model Parameters for the Gaussian DLM

Consider the Gaussian DLM given by

$$
\boldsymbol {x} _ {t} = \Phi_ {t} \boldsymbol {x} _ {t - 1} + \boldsymbol {w} _ {t} \quad \text {a n d} \quad y _ {t} = \boldsymbol {a} _ {t} ^ {\prime} \boldsymbol {x} _ {t} + v _ {t}. \tag {6.166}
$$

The observations are univariate, and the state process is $p$ -dimensional; this DLM includes the structural models presented in 6.5. The prior on the initial state is $\pmb { x } _ { 0 } \sim \mathrm { N } ( \pmb { \mu } _ { 0 } , \pmb { \Sigma } _ { 0 } )$ , and we assume that $\mathbf { \psi } _ { { \pmb { w } } _ { t } } ~ \sim$ iid $\mathrm { N } ( \mathbf { 0 } , Q _ { t } )$ , independent of $v _ { t } \sim$ iid $\mathrm { N } ( 0 , r _ { t } )$ . The collection of unknown model parameters will be denoted by $\Theta$ .

To explore how we would assess the values of $\Theta$ using an MCMC technique, we focus on the problem obtaining the posterior distribution, $p ( \Theta \mid Y _ { n } )$ , of the parameters given the data, $Y _ { n } = \{ y _ { 1 } , . . . , y _ { n } \}$ and a prior $\pi ( \Theta )$ . Of course, these distributions depend on “hyperparameters” that are assumed to be known. (Some authors consider the states ${ \pmb x } _ { t }$ as the first level of parameters because they are unobserved. In this case, the values in $\Theta$ are regarded as the hyperparameters, and the parameters of their distributions are regarded as hyper-hyperparameters.) Denoting

the entire set of state vectors as $X _ { n } = \{ \pmb { x } _ { 0 } , \pmb { x } _ { 1 } , \ldots , \pmb { x } _ { n } \}$ , the posterior can be written as

$$
p (\Theta \mid Y _ {n}) = \int p (\Theta \mid X _ {n}, Y _ {n}) p (X _ {n}, \Theta^ {*} \mid Y _ {n}) d X _ {n} d \Theta^ {*}. \tag {6.167}
$$

Although the posterior, $p ( \Theta \mid Y _ { n } )$ , may be intractable, conditioning on the states can make the problem manageable in that

$$
p (\Theta \mid X _ {n}, Y _ {n}) \propto \pi (\Theta) p (x _ {0} \mid \Theta) \prod_ {t = 1} ^ {n} p \left(\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}, \Theta\right) p \left(y _ {t} \mid \boldsymbol {x} _ {t}, \Theta\right) \tag {6.168}
$$

can be easier to work with (either as members of conjugate families or using some rejection scheme); we will discuss this in more detail when we present the nonlinear, non-Gaussian case, but we will assume for the present $p ( \Theta \mid X _ { n } , Y _ { n } )$ is in closed form.

Suppose we can obtain $G$ pseudo-random draws, $X _ { n } ^ { ( g ) } \equiv ( X _ { n } , \Theta ^ { \ast } ) ^ { ( g ) }$ , for $g = 1 , \ldots , G$ , from the joint posterior density $p ( X _ { n } , \Theta ^ { * } \mid Y _ { n } )$ . Then (6.167) can be approximated by

$$
\widehat {p} (\Theta \mid Y _ {n}) = G ^ {- 1} \sum_ {g = 1} ^ {G} p (\Theta \mid X _ {n} ^ {(g)}, Y _ {n}).
$$

A sample from $p ( X _ { n } , \Theta ^ { * } \mid Y _ { n } )$ is obtained using two different MCMC methods. First, the Gibbs sampler is used, for each $g$ , as follows: sample $X _ { n \left[ \ell \right] }$ given $\Theta _ { [ \ell - 1 ] } ^ { * }$ from $p ( X _ { n } \mid \Theta _ { [ \ell - 1 ] } ^ { * } , Y _ { n } )$ , and then a sample $\Theta _ { [ \ell ] } ^ { * }$ from $p ( \Theta \mid X _ { n [ \ell ] } , Y _ { n } )$ as given by (6.168), for $\ell = 1 , 2 , \ldots$ . Stop when $\ell$ is sufficiently large, and retain the final values as $X _ { n } ^ { ( g ) }$ . This process is repeated $G$ times.

The first step of this method requires simultaneous generation of the state vectors. Because we are dealing with a Gaussian linear model, we can rely on the existing theory of the Kalman filter to accomplish this step. This step is conditional on $\Theta$ , and we assume at this point that $\Theta$ is fixed and known. In other words, our goal is to sample the entire set of state vectors, $X _ { n } = \{ \pmb { x } _ { 0 } , \pmb { x } _ { 1 } , \ldots , \pmb { x } _ { n } \}$ , from the multivariate normal posterior density $p _ { \Theta } ( X _ { n } \mid Y _ { n } )$ , where $Y _ { n } = \{ y _ { 1 } , \dots , y _ { n } \}$ represents the observations. Because of the Markov structure, we can write,

$$
p _ {\Theta} \left(X _ {n} \mid Y _ {n}\right) = p _ {\Theta} \left(\boldsymbol {x} _ {n} \mid Y _ {n}\right) p _ {\Theta} \left(\boldsymbol {x} _ {n - 1} \mid \boldsymbol {x} _ {n}, Y _ {n - 1}\right) \dots p _ {\Theta} \left(\boldsymbol {x} _ {0} \mid \boldsymbol {x} _ {1}\right). \tag {6.169}
$$

In view of (6.169), it is possible to sample the entire set of state vectors, $X _ { n }$ , by sequentially simulating the individual states backward. This process yields a simulation method that Fr¨uhwirth–Schnatter (1994) called the forward-filtering, backward-sampling algorithm. In particular,

because the processes are Gaussian, we need only obtain the conditional means and variances, say, ${ \pmb m } _ { t } = E _ { \Theta } ( { \pmb x } _ { t } \ \vert \ Y _ { t } , { \pmb x } _ { t + 1 } )$ , and $V _ { t } = \mathrm { v a r } _ { \Theta } ( \pmb { x } _ { t } \ |$ $Y _ { t } , \pmb { x } _ { t + 1 } )$ . This conditioning argument is akin to having $\pmb { x } _ { t + 1 }$ as an additional observation on state ${ \pmb x } _ { t }$ . In particular, using standard multivariate normal distribution theory,

$$
\boldsymbol {m} _ {t} = \boldsymbol {x} _ {t} ^ {t} + J _ {t} \left(\boldsymbol {x} _ {t + 1} - \boldsymbol {x} _ {t + 1} ^ {t}\right),
$$

$$
V _ {t} = P _ {t} ^ {t} - J _ {t} P _ {t + 1} ^ {t} J _ {t} ^ {\prime}, \tag {6.170}
$$

for $t = n - 1 , n - 2 , \ldots , 0$ , where $J _ { t }$ is defined in (6.49). To verify (6.170), the essential part of the Gaussian density (that is, the exponent) of $\pmb { x } _ { t } \ | \ \boldsymbol { Y } _ { t } , \pmb { x } _ { t + 1 }$ is

$$
\left(\boldsymbol {x} _ {t + 1} - \Phi_ {t + 1} \boldsymbol {x} _ {t}\right) ^ {\prime} \left[ Q _ {t + 1} \right] ^ {- 1} \left(\boldsymbol {x} _ {t + 1} - \Phi_ {t + 1} \boldsymbol {x} _ {t}\right) + \left(\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t}\right) ^ {\prime} \left[ P _ {t} ^ {t} \right] ^ {- 1} \left(\boldsymbol {x} _ {t} - \boldsymbol {x} _ {t} ^ {t}\right),
$$

and we simply complete the square; see Fr¨uhwirth–Schnatter (1994) or West and Harrison (1997, Section 4.7). Hence, the algorithm is to first sample ${ \pmb x } _ { n }$ from a $\mathrm { N } ( \pmb { x } _ { n } ^ { n } , P _ { n } ^ { n } )$ , where ${ \pmb x } _ { n } ^ { \pi }$ and $P _ { n } ^ { n }$ are obtained from the Kalman filter, Property P6.1, and then sample ${ \pmb x } _ { t }$ from a $\mathrm { N } ( \mathbf { m } _ { t } , V _ { t } )$ , for $\scriptstyle t = n - 1 , n - 2 , \dotsc , 0$ , where the conditioning value of $\pmb { x } _ { t + 1 }$ is the value previously sampled; ${ \pmb { m } } _ { t }$ and $V _ { t }$ are given in (6.170).

Next, we address an MCMC approach to nonlinear and non-Gaussian statespace modeling that was first presented in Carlin et al. (1992). We consider the general model given in (6.163), but with additive errors:

$$
\boldsymbol {x} _ {t} = F _ {t} \left(\boldsymbol {x} _ {t - 1}\right) + \boldsymbol {w} _ {t} \quad \text {a n d} \quad \boldsymbol {y} _ {t} = H _ {t} \left(\boldsymbol {x} _ {t}\right) + \boldsymbol {v} _ {t}, \tag {6.171}
$$

where $F _ { t }$ and $H _ { t }$ are given, but may also depend on unknown parameters, say, $\Phi _ { t }$ and $A _ { t }$ , respectively, the collection of which will be denoted by $\Theta$ . The errors are independent white noise sequences with $\mathrm { v a r } ( { \pmb w } _ { t } ) = Q _ { t }$ and $\mathrm { v a r } ( { \pmb v } _ { t } ) = R _ { t }$ . Although time-varying variance–covariance matrices are easily incorporated in this framework, to ease the discussion we focus on the case $Q _ { t } \equiv Q$ and $R _ { t } \equiv R$ . Also, although it is not necessary, we assume the initial state condition $\pmb { x } _ { 0 }$ is fixed and known; this is merely for notational convenience, so we do not have to carry along the additional terms involving $\pmb { x } _ { 0 }$ throughout the discussion.

In general, the likelihood specification for the model is given by

$$
L _ {X, Y} (\Theta , Q, R) = \prod_ {t = 1} ^ {n} f _ {1} \left(\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}, \Theta , Q\right) f _ {2} \left(\boldsymbol {y} _ {t} \mid \boldsymbol {x} _ {t}, \Theta , R\right), \tag {6.172}
$$

where it is assumed the densities $f _ { 1 } ( \cdot )$ and $f _ { 2 } ( \cdot )$ are scale mixtures of normals. Specifically, for $t = 1 , \ldots , n$ ,

$$
f _ {1} \left(\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}, \Theta , Q\right) = \int f \left(\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}, \Theta , Q, \lambda_ {t}\right) p _ {1} \left(\lambda_ {t}\right) d \lambda_ {t},
$$

$$
f _ {2} \left(\boldsymbol {y} _ {t} \mid \boldsymbol {x} _ {t}, \Theta , R\right) = \int f \left(\boldsymbol {y} _ {t} \mid \boldsymbol {x} _ {t}, \Theta , R, \omega_ {t}\right) p _ {2} \left(\omega_ {t}\right) d \omega_ {t}, \tag {6.173}
$$

where conditional on the independent sequences of nuisance parameters $\lambda =$ $( \lambda _ { t } ; t = 1 , \ldots , n )$ ) and $\pmb { \omega } = ( \omega _ { t } ; t = 1 , \ldots , n _ { }$ ),

$$
\begin{array}{l} \boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {t - 1}, \Theta , Q, \lambda_ {t} \sim \mathrm {N} \left(F _ {t} (\boldsymbol {x} _ {t - 1}; \Theta), \lambda_ {t} Q\right), \\ \boldsymbol {y} _ {t} \mid \boldsymbol {x} _ {t}, \Theta , R, \omega_ {t} \sim \mathrm {N} \left(H _ {t} \left(\boldsymbol {x} _ {t}; \Theta\right), \omega_ {t} R\right). \tag {6.174} \\ \end{array}
$$

By varying $p _ { 1 } ( \lambda _ { t } )$ and $p _ { 2 } ( \omega _ { t } )$ , we can have a wide variety of non-Gaussian error densities. These densities include, for example, double exponential, logistic, and $t$ distributions in the univariate case and a rich class of multivariate distributions; this is discussed further in Carlin et al. (1992). The key to the approach is the introduction of the nuisance parameters $\pmb { \lambda }$ and $\omega$ and the structure (6.174), which lends itself naturally to the Gibbs sampler and allows for the analysis of this general nonlinear and non-Gaussian problem.

According to Example 6.17, to implement the Gibbs sampler, we must be able to sample from the following complete conditional distributions:

(i)   
(ii)   
(iii)   
(iv)   
(v)   
(vi)

where $X _ { n } = \{ { \pmb x } _ { 1 } , \ldots , { \pmb x } _ { n } \}$ and $Y _ { n } = \{ { \pmb y } _ { 1 } , \dots , { \pmb y } _ { n } \}$ . The main difference between this method and the linear Gaussian case is that, because of the generality, we sample the states one-at-a-time rather than simultaneously generating all of them. As discussed in Carter and Kohn (1994), if possible, it is more efficient to generate the states simultaneously as in Example 6.18.

We will discuss items (i) and (ii) above. The third item follows in a similar manner to the second, and items (iv)-(vi) will follow from standard multivariate normal distribution theory and from Wishart distribution theory because of the conditioning on $\pmb { \lambda }$ and $\omega$ . We will discuss this matter further in the next example. First, consider the linear model, $F _ { t } ( \pmb { x } _ { t - 1 } ) = \Phi _ { t } \pmb { x } _ { t - 1 }$ , and $H _ { t } ( \pmb { x } _ { t } ) =$ ${ \boldsymbol { A } } _ { t } { \pmb { x } } _ { t }$ in (6.171). In this case, for $t = 1 , \dots , n , \pmb { x } _ { t } \ | \ \pmb { x } _ { s \neq t } , \pmb { \lambda } , \omega , \Theta , Q , R , Y _ { n }$ $t = 1 , \ldots , n$ has a $p$ -dimensional $\mathrm { N } _ { p } ( B _ { t } \pmb { b } _ { t } , B _ { t } )$ distribution, with

$$
\begin{array}{l} B _ {t} ^ {- 1} = \frac {Q ^ {- 1}}{\lambda_ {t}} + \frac {A _ {t} ^ {\prime} R ^ {- 1} A _ {t}}{\omega_ {t}} + \frac {\Phi_ {t + 1} ^ {\prime} Q ^ {- 1} \Phi_ {t + 1}}{\lambda_ {t + 1}}, \\ \boldsymbol {b} _ {t} = \frac {\boldsymbol {x} _ {t - 1} \Phi_ {t} ^ {\prime} Q ^ {- 1}}{\lambda_ {t}} + \frac {\boldsymbol {y} _ {t} R ^ {- 1} A _ {t}}{\omega_ {t}} + \frac {\boldsymbol {x} _ {t + 1} Q ^ {- 1} \Phi_ {t + 1}}{\lambda_ {t + 1}}, \tag {6.175} \\ \end{array}
$$

where, when $t = n$ in (6.175), terms in the sum with elements having a subscript of $n + 1$ are dropped (this is assumed to be the case in what follows, although we do not explicitly state it). This result follows by noting the essential part of the multivariate normal distribution (that is, the exponent) of $\mathbf { \Delta } _ { \mathbf { x } _ { t } } \mid \mathbf { x } _ { s \neq t } , \lambda , \omega , \Theta , Q , R , Y _ { n }$ is

$$
\begin{array}{l} \left(\boldsymbol {x} _ {t} - \Phi_ {t} \boldsymbol {x} _ {t - 1}\right) ^ {\prime} \left(\lambda_ {t} Q\right) ^ {- 1} \left(\boldsymbol {x} _ {t} - \Phi_ {t} \boldsymbol {x} _ {t - 1}\right) + \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) ^ {\prime} \left(\omega_ {t} R\right) ^ {- 1} \left(\boldsymbol {y} _ {t} - A _ {t} \boldsymbol {x} _ {t}\right) \\ + \left(\boldsymbol {x} _ {t + 1} - \Phi_ {t + 1} \boldsymbol {x} _ {t}\right) ^ {\prime} \left(\lambda_ {t + 1} Q\right) ^ {- 1} \left(\boldsymbol {x} _ {t + 1} - \Phi_ {t + 1} \boldsymbol {x} _ {t}\right), \tag {6.176} \\ \end{array}
$$

which upon manipulation yields (6.175).

# Example 6.19 Nonlinear Models

In the case of nonlinear models, we can use (6.175) with slight modifications. For example, consider the case in which $F _ { t }$ is nonlinear, but $H _ { t }$ is linear, so the observations are ${ \pmb y } _ { t } = A _ { t } { \pmb x } _ { t } + { \pmb v } _ { t }$ . Then,

$$
\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {s \neq t}, \boldsymbol {\lambda}, \omega , \Theta , Q, R, Y _ {n} \propto \eta_ {1} (\boldsymbol {x} _ {t}) \mathrm {N} _ {p} \left(B _ {1 t} \boldsymbol {b} _ {1 t}, B _ {1 t}\right), \tag {6.177}
$$

where

$$
B _ {1 t} ^ {- 1} = \frac {Q ^ {- 1}}{\lambda_ {t}} + \frac {A _ {t} ^ {\prime} R ^ {- 1} A _ {t}}{\omega_ {t}},
$$

$$
\boldsymbol {b} _ {1 t} = \frac {F _ {t} ^ {\prime} (\boldsymbol {x} _ {t - 1}) Q ^ {- 1}}{\lambda_ {t}} + \frac {\boldsymbol {y} _ {t} R ^ {- 1} A _ {t}}{\omega_ {t}},
$$

and

$$
\eta_ {1} (\boldsymbol {x} _ {t}) = \exp \left\{- \frac {1}{2 \lambda_ {t + 1}} \Big (\boldsymbol {x} _ {t + 1} - F _ {t + 1} (\boldsymbol {x} _ {t}) \Big) ^ {\prime} Q ^ {- 1} \Big (\boldsymbol {x} _ {t + 1} - F _ {t + 1} (\boldsymbol {x} _ {t}) \Big) \right\}.
$$

Because $0 \leq \eta _ { 1 } ( \pmb { x } _ { t } ) \leq 1$ , for all ${ \pmb x } _ { t }$ , the distribution we want to sample from is dominated by the $\mathrm { N } _ { p } ( B _ { 1 t } \pmb { b } _ { 1 t } , B _ { 1 t } )$ density. Hence, we may use rejection sampling as discussed in Example 6.17 to obtain an observation from the required density. That is, we generate a pseudo-variate from the $\mathrm { N } _ { p } ( B _ { 1 t } \pmb { b } _ { 1 t } , B _ { 1 t } )$ density and accept it with probability $\eta _ { 1 } ( { \pmb x } _ { t } )$ .

We proceed analogously in the case in which $F _ { t } ( \pmb { x } _ { t - 1 } ) = \Phi _ { t } \pmb { x } _ { t - 1 }$ is linear and $H _ { t } ( \pmb { x } _ { t } )$ is nonlinear. In this case,

$$
\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {s \neq t}, \boldsymbol {\lambda}, \omega , \Theta , Q, R, Y _ {n} \propto \eta_ {2} (\boldsymbol {x} _ {t}) \mathrm {N} _ {p} \left(B _ {2 t} \boldsymbol {b} _ {2 t}, B _ {2 t}\right), \tag {6.178}
$$

where

$$
B _ {2 t} ^ {- 1} = \frac {Q ^ {- 1}}{\lambda_ {t}} + \frac {\Phi_ {t + 1} ^ {\prime} Q ^ {- 1} \Phi_ {t + 1}}{\lambda_ {t + 1}},
$$

$$
\boldsymbol {b} _ {2 t} = \frac {\boldsymbol {x} _ {t - 1} \Phi_ {t} ^ {\prime} Q ^ {- 1}}{\lambda_ {t}} + \frac {\boldsymbol {x} _ {t + 1} Q ^ {- 1} \Phi_ {t + 1}}{\lambda_ {t + 1}},
$$

and

$$
\eta_ {2} (\boldsymbol {x} _ {t}) = \exp \left\{- \frac {1}{2 \omega_ {t}} \left(\boldsymbol {y} _ {t} - H _ {t} (\boldsymbol {x} _ {t})\right) ^ {\prime} R ^ {- 1} \left(\boldsymbol {y} _ {t} - H _ {t} (\boldsymbol {x} _ {t})\right) \right\}.
$$

Here, we generate a pseudo-variate from the $\mathrm { N } _ { p } ( B _ { 2 t } \pmb { b } _ { 2 t } , B _ { 2 t } )$ density and accept it with probability $\eta _ { 2 } ( { \pmb x } _ { t } )$ .

Finally, in the case in which both $F _ { t }$ and $H _ { t }$ are nonlinear, we have

$$
\boldsymbol {x} _ {t} \mid \boldsymbol {x} _ {s \neq t}, \boldsymbol {\lambda}, \boldsymbol {\omega}, \Theta , Q, R, Y _ {n} \propto \eta_ {1} (\boldsymbol {x} _ {t}) \eta_ {2} (\boldsymbol {x} _ {t}) \mathrm {N} _ {p} \left(F _ {t} \left(\boldsymbol {x} _ {t - 1}\right), \lambda_ {t} Q\right), \tag {6.179}
$$

so we sample from a $\mathrm { N } _ { p } ( F _ { t } ( \pmb { x } _ { t - 1 } ) , \lambda _ { t } Q )$ density and accept it with probability $\eta _ { 1 } ( { \pmb x } _ { t } ) \eta _ { 2 } ( { \pmb x } _ { t } )$ .

Determination of (ii), $\lambda _ { t } \ \Big | \ \Theta , Q , \pmb { x } _ { t } , \pmb { x } _ { t - 1 }$ follows directly from Bayes theorem; that is, $p ( \lambda _ { t } \ \vert \ \Theta , Q , x _ { t } , x _ { t - 1 } ) \propto p _ { 1 } ( \lambda _ { t } ) p ( { x _ { t } } \ \vert \ \lambda _ { t } , { x _ { t - 1 } } , \Theta , Q )$ . By (6.173), however, we know the normalization constant is given by $f _ { 1 } ( \pmb { x } _ { t } \ | \ \pmb { x } _ { t - 1 } , \Theta , Q )$ , and thus the complete conditional density for $\lambda _ { t }$ is of a known functional form.

Many examples of these techniques are given in Carlin et al. (1992), including the problem of model choice. In the next example, we consider a univariate nonlinear model in which the state noise process has a $t$ -distribution. As noted in Meinhold and Singpurwalla (1989), using $t$ -distributions for the error processes is a way of robustifying the Kalman filter against outliers. In this example we present a brief discussion of a detailed analysis presented in Carlin et al. (1992, Example 4.2); readers interested in more detail may find it in that article.

# Example 6.20 Analysis of a Nonlinear, Non-Gaussian State-Space Model

Kitagawa (1987) considered the analysis of data generated from the following univariate nonlinear model:

$$
x _ {t} = F _ {t} \left(x _ {t - 1}\right) + w _ {t} \quad \text {a n d} \quad y _ {t} = H _ {t} \left(x _ {t}\right) + v _ {t} \quad t = 1, \dots , 1 0 0, \tag {6.180}
$$

with

$$
\begin{array}{l} F _ {t} \left(x _ {t - 1}\right) = \alpha x _ {t - 1} + \beta x _ {t - 1} / \left(1 + x _ {t - 1} ^ {2}\right) + \gamma \cos [ 1. 2 (t - 1) ], \\ H _ {t} \left(x _ {t}\right) = x _ {t} ^ {2} / 2 0, \tag {6.181} \\ \end{array}
$$

where $x _ { 0 } = 0$ , $w _ { t }$ are independent random variables having a central $t$ -distribution with $\nu = 1 0$ degrees and scaled so $\mathrm { v a r } ( w _ { t } ) = \sigma _ { w } ^ { 2 } = 1 0$ [we denote this generically by $t ( 0 , \sigma , \nu ) ]$ , and $v _ { t }$ is white standard Gaussian noise, $\mathrm { v a r } ( v _ { t } ) = \sigma _ { v } ^ { 2 } = 1$ . The state noise and observation noise are mutually independent. Kitagawa (1987) discussed the analysis of data generated from this model with $\alpha = . 5$ , $\beta = 2 5$ , and $\gamma = 8$ assumed

known. We will use these values of the parameters in this example, but we will assume they are unknown. Figure 6.15 shows a typical data sequence $y _ { t }$ and the corresponding state process $x _ { t }$ .

Our goal here will be to obtain an estimate of the prediction density $p ( x _ { 1 0 1 } \mid Y _ { 1 0 0 } )$ . To accomplish this, we use $n = 1 0 1$ and consider $y _ { 1 0 1 }$ as a latent variable (we will discuss this in more detail shortly). The priors on the variance components are chosen from a conjugate family, that is, $\sigma _ { w } ^ { 2 } \sim \mathrm { I G } ( a _ { 0 } , b _ { 0 } )$ independent of $\sigma _ { v } ^ { 2 } \sim \mathrm { I G } ( c _ { 0 } , d _ { 0 } )$ , where IG denotes the inverse (reciprocal) gamma distribution $\lfloor z$ has an inverse gamma distribution if $1 / z$ has a gamma distribution; general properties can be found, for example, in Box and Tiao (1973, Section 8.5)]. Then,

$$
\begin{array}{l} \sigma_ {w} ^ {2} \mid \lambda , Y _ {n}, X _ {n} \sim \\ \operatorname {I G} \left(a _ {0} + \frac {n}{2}, \left\{\frac {1}{b _ {0}} + \frac {1}{2} \sum_ {t = 1} ^ {n} [ x _ {t} - F (x _ {t - 1}) ] ^ {2} / \lambda_ {t} \right\} ^ {- 1}\right), \\ \end{array}
$$

$$
\begin{array}{l} \sigma_ {v} ^ {2} \mid \omega , Y _ {n}, X _ {n} \sim \\ \left. \operatorname {I G} \left(c _ {0} + \frac {n}{2}, \left\{\frac {1}{d _ {0}} + \frac {1}{2} \sum_ {t = 1} ^ {n} \left[ y _ {t} - H \left(x _ {t}\right) \right] ^ {2} / \omega_ {t} \right\} ^ {- 1}\right). \right. \tag {6.182} \\ \end{array}
$$

Next, letting $\nu / \lambda _ { t } \sim \chi _ { \nu } ^ { 2 }$ , we get that, marginally, $w _ { t } \mid \sigma _ { w } \sim t ( 0 , \sigma _ { w } , \nu )$ , as required, leading to the complete conditional $\lambda _ { t } \mid \sigma _ { w } , \alpha , \beta , \gamma , Y _ { n } , X _ { n }$ , for $t = 1 , \ldots , n$ , being distributed as

$$
\left. \operatorname {I G} \left(\frac {\nu + 1}{2}, 2 \left\{\frac {\left[ x _ {t} - F \left(x _ {t - 1}\right) \right] ^ {2}}{\sigma_ {w} ^ {2}} + \nu \right\} ^ {- 1}\right). \right. \tag {6.183}
$$

We take $\omega _ { t } \equiv 1$ for $t = 1 , \ldots , n$ , because the observation noise is Gaussian.

For the states, $x _ { t }$ , we take a normal prior on the initial state, $x _ { 0 } ~ \sim$ $\mathrm { N } ( \mu _ { 0 } , \sigma _ { 0 } ^ { 2 } )$ , and then we use rejection sampling to conditionally generate a state value $x _ { t }$ , for $t = 1 , \ldots , n$ , as described in Example 6.19, equation (6.179). In this carespectively, with $\eta _ { 1 } ( x _ { t } )$ nd giv $\eta _ { 2 } ( x _ { t } )$ are give (6.181), d, $F _ { t }$ $H _ { t }$ $\Theta = ( \alpha , \beta , \gamma ) ^ { \prime }$ $Q = \sigma _ { w } ^ { 2 }$ and $R = \sigma _ { v } ^ { 2 }$ . Endpoints take some special consideration; we generate $x _ { 0 }$ from a $\mathrm { N } ( \mu _ { 0 } , \sigma _ { 0 } ^ { 2 } )$ and accept it with probability $\eta _ { 1 } ( x _ { 0 } )$ , and we generate $x _ { 1 0 1 }$ as usual and accept it with probability $\eta _ { 2 } ( x _ { 1 0 1 } )$ . The last complete conditional depends on $y _ { 1 0 1 }$ , a latent data value not observed but instead generated according to its complete conditional, which is $\mathrm { N } ( x _ { 1 0 1 } ^ { 2 } / 2 0 , \sigma _ { v } ^ { 2 } )$ , because $\omega _ { 1 0 1 } = 1$ .

The prior on $\Theta = ( \alpha , \beta , \gamma ) ^ { \prime }$ is taken to be trivariate normal with mean $( \mu _ { \alpha } , \mu _ { \beta } , \mu _ { \gamma } ) ^ { \prime }$ and diagonal variance–covariance matrix $\mathrm { d i a g } \{ \sigma _ { \alpha } ^ { 2 } , \sigma _ { \beta } ^ { 2 } , \sigma _ { \gamma } ^ { 2 } \}$ . The necessary conditionals can be found using standard normal theory,

![](images/fda1980a69d030f73401013d953b9a12b6f2c2699a1e9bea7b58700ff021a19a.jpg)

![](images/a0eea6eb15c6e5b57379cbf3ceb1ad5df76423db34bbf6fa3ca5195d9f85a461.jpg)  
Figure 6.15 The state process, $x _ { t }$ (top), and the observations, $y _ { t }$ (bottom), for $t = 1 , \ldots , 1 0 0$ generated from the model (6.180).

as done in (6.175). For example, the complete conditional distribution of $\alpha$ is of the form $\mathrm { N } ( B b , B )$ , where

$$
B ^ {- 1} = \frac {1}{\sigma_ {\alpha} ^ {2}} + \frac {1}{\sigma_ {w} ^ {2}} \sum_ {t = 1} ^ {n} \frac {x _ {t - 1} ^ {2}}{\lambda_ {t}}
$$

and

$$
b = \frac {\mu_ {\alpha}}{\sigma_ {\alpha} ^ {2}} + \frac {1}{\sigma_ {w} ^ {2}} \sum_ {t = 1} ^ {n} \frac {x _ {t - 1}}{\lambda_ {t}} \left(x _ {t} - \beta \frac {x _ {t - 1}}{1 + x _ {t - 1} ^ {2}} - \gamma \cos [ 1. 2 (t - 1) ]\right).
$$

The complete conditional for $\beta$ has the same form, with

$$
B ^ {- 1} = \frac {1}{\sigma_ {\beta} ^ {2}} + \frac {1}{\sigma_ {w} ^ {2}} \sum_ {t = 1} ^ {n} \frac {x _ {t - 1} ^ {2}}{\lambda_ {t} (1 + x _ {t - 1} ^ {2}) ^ {2}}
$$

and

$$
b = \frac {\mu_ {\beta}}{\sigma_ {\beta} ^ {2}} + \frac {1}{\sigma_ {w} ^ {2}} \sum_ {t = 1} ^ {n} \frac {x _ {t - 1}}{\lambda_ {t} (1 + x _ {t - 1} ^ {2})} \left(x _ {t} - \alpha x _ {t - 1} - \gamma \cos [ 1. 2 (t - 1) ]\right),
$$

![](images/7a6bd1a3ae715b417653bb422bd4e7ac55b30d24a1853d89a19dd5a519c0b040.jpg)  
Figure 6.16 Estimated one-step-ahead prediction posterior density $\widehat { p } ( x _ { 1 0 1 } | Y _ { 1 0 0 } )$ of the state process for the nonlinear and non-normal model given by (6.180) using Gibbs sampling, $G = 5 0 0$ .

and for $\gamma$ the values are

$$
B ^ {- 1} = \frac {1}{\sigma_ {\gamma} ^ {2}} + \frac {1}{\sigma_ {w} ^ {2}} \sum_ {t = 1} ^ {n} \frac {\cos^ {2} [ 1 . 2 (t - 1) ]}{\lambda_ {t}}
$$

and

$$
b = \frac {\mu_ {\gamma}}{\sigma_ {\gamma} ^ {2}} + \frac {1}{\sigma_ {w} ^ {2}} \sum_ {t = 1} ^ {n} \frac {\cos [ 1 . 2 (t - 1) ]}{\lambda_ {t}} \left(x _ {t} - \alpha x _ {t - 1} - \beta \frac {x _ {t - 1}}{1 + x _ {t - 1} ^ {2}}\right).
$$

In this example, we put $\mu _ { 0 } = 0$ , $\sigma _ { 0 } ^ { 2 } = 1 0$ , and $a _ { 0 } = 3$ , $b _ { 0 } = . 0 5$ (so the prior on $\sigma _ { w } ^ { 2 }$ has mean and standard deviation equal to 10), and $c _ { 0 } = 3$ , $d _ { 0 } = . 5$ (so the prior on $\sigma _ { v } ^ { 2 }$ has mean and standard deviation equal to one). The normal prior on $\Theta = ( \alpha , \beta , \gamma ) ^ { \prime }$ had corresponding mean vector equal to $\mu _ { \alpha } = . 5 , \mu _ { \beta } = 2 5 , \mu _ { \gamma } = 8 ) ^ { \prime }$ and diagonal variance matrix equal to $\mathrm { d i a g } \{ \sigma _ { \alpha } ^ { 2 } = . 2 5 , \sigma _ { \beta } ^ { 2 } = 1 0 , \sigma _ { \gamma } ^ { 2 } = 4 \}$ . The Gibbs sampler ran for $\ell = 5 0$ iterations for $G = 5 0 0$ parallel replications per iteration. We estimate the marginal posterior density of $x _ { 1 0 1 }$ as

$$
\widehat {p} \left(x _ {1 0 1} \mid Y _ {1 0 0}\right) = G ^ {- 1} \sum_ {g = 1} ^ {G} \mathrm {N} \left(x _ {1 0 1} \mid [ F _ {t} \left(x _ {t - 1}\right) ] ^ {(g)}, \lambda_ {1 0 1} ^ {(g)} \sigma_ {w} ^ {2 (g)}\right), \tag {6.184}
$$

where $\mathrm { N } ( \cdot | a , b )$ denotes the normal density with mean $a$ and variance $b$ , and

$$
[ F _ {t} (x _ {t - 1}) ] ^ {(g)} = \alpha^ {(g)} x _ {t - 1} ^ {(g)} + \beta^ {(g)} x _ {t - 1} ^ {(g)} / (1 + x _ {t - 1} ^ {2 (g)}) + \gamma^ {(g)} \cos [ 1. 2 (t - 1) ].
$$

The estimate, (6.184), with $G = 5 0 0$ , is shown in Figure 6.16. Other aspects of the analysis, for example, the marginal posteriors of the elements of $\Theta$ , can be found in Carlin et al. (1992).

# 6.10 Stochastic Volatility

Recently, there has been considerable interest in stochastic volatility models. These models are similar to the ARCH models presented in Chapter 5, but they add a stochastic noise term to the equation for $\sigma _ { t }$ . Recall from 5.2 that a $\mathrm { G A R C H } ( 1 , 1 )$ $( 1 , 1 )$ model for a return, which we denote here by $r _ { t }$ , is given by

$$
r _ {t} = \sigma_ {t} \epsilon_ {t} \tag {6.185}
$$

$$
\sigma_ {t} ^ {2} = \alpha_ {0} + \alpha_ {1} r _ {t - 1} ^ {2} + \beta_ {1} \sigma_ {t - 1} ^ {2}, \tag {6.186}
$$

where $\epsilon _ { t }$ is Gaussian white noise. If we define

$$
h _ {t} = \log \sigma_ {t} ^ {2} \quad \text {a n d} \quad y _ {t} = \log r _ {t} ^ {2},
$$

then (6.185) can be written as

$$
y _ {t} = h _ {t} + \log \epsilon_ {t} ^ {2}. \tag {6.187}
$$

Equation (6.187) is considered the observation equation, and the stochastic variance $h _ { t }$ is considered to be an unobserved state process. Similar to (6.186), the volatility process follows, in its basic form, an autoregression,

$$
h _ {t} = \phi_ {0} + \phi_ {1} h _ {t - 1} + w _ {t}, \tag {6.188}
$$

where $w _ { t }$ is white Gaussian noise with variance $\sigma _ { w } ^ { 2 }$

Together, (6.187) and (6.188) make up the stochastic volatility model due to Harvey, Ruiz and Shephard (1994). If $\epsilon _ { t } ^ { 2 }$ had a log-normal distribution, (6.187)-(6.188) would form a Gaussian state-space model, and we could then use standard DLM results to fit the model to data. Unfortunately, $y _ { t } = \log r _ { t } ^ { 2 }$ is rarely normal, so we typically keep the ARCH normality assumption on $\epsilon _ { t }$ ; in which case, $\log \epsilon _ { t } ^ { 2 }$ is distributed as the log of a chi-squared random variable with one degree of freedom. This density is given by

$$
f (x) = \frac {1}{\sqrt {2 \pi}} \exp \left\{- \frac {1}{2} \left(e ^ {x} - x\right) \right\} - \infty <   x <   \infty , \tag {6.189}
$$

and its mean and variance are $- 1 . 2 7$ and $\pi ^ { 2 } / 2$ , respectively; the density (6.189) is highly skewed with a long tail on the left (see Figure 6.18).

Various approaches to the fitting of stochastic volatility models have been examined; these methods include a wide range of assumptions on the observational noise process. A good summary of the proposed techniques, both Bayesian (via MCMC) and non-Bayesian approaches (such as quasi-maximum likelihood estimation and the EM algorithm), can be found in Jacquier et al. (1994), and Shephard (1996). Simulation methods for classical inference applied to stochastic volatility models are discussed in Danielson (1994) and Sandmann and Koopman (1998).

Kim, Shephard and Chib (1998) proposed modeling the log of a chi-squared random variable by a mixture of seven normals to approximate the first four moments of the observational error distribution; the mixture is fixed and no additional model parameters are added by using this technique. In an effort to keep matters simple, and perhaps somewhat more general (in that we allow the observational error dynamics to depend on parameters that will be fitted), our method of fitting stochastic volatility models is to retain the Gaussian state equation (6.188), but to write the observation equation, with $y _ { t } = \log r _ { t } ^ { 2 }$ , as

$$
y _ {t} = \alpha + h _ {t} + \eta_ {t}, \tag {6.190}
$$

where $\eta _ { t }$ is white noise, whose distribution is a mixture of two normals, one centered at zero. In particular, we write

$$
\eta_ {t} = u _ {t} z _ {t 0} + (1 - u _ {t}) z _ {t 1}, \tag {6.191}
$$

where $u _ { t }$ is an iid Bernoulli process, $\mathrm { P r } \{ u _ { t } = 0 \} = \pi _ { 0 }$ , $\mathrm { P r } \{ u _ { t } = 1 \} = \pi _ { 1 }$ ( $\pi _ { 0 } + \pi _ { 1 } = 1$ ), $z _ { t 0 } \sim$ iid $\mathrm { N } ( 0 , \sigma _ { 0 } ^ { 2 } )$ , and $z _ { t 1 } \sim$ iid $\mathrm { N } ( \mu _ { 1 } , \sigma _ { 1 } ^ { 2 } )$ .

The advantage to this model is that it is easy to fit because it uses normality. In fact, the model equations (6.188) and (6.190)-(6.191) are similar to those presented in Pe˜na and Guttman (1988), who used the idea to obtain a robust Kalman filter, and, as previously mentioned, in Kim, Shephard and Chib (1998). The material presented in §6.8 applies here, and in particular, the filtering equations for this model are

$$
h _ {t + 1} ^ {t} = \phi_ {0} + \phi_ {1} h _ {t} ^ {t - 1} + \sum_ {j = 0} ^ {1} \pi_ {t j} K _ {t j} \epsilon_ {t j}, \tag {6.192}
$$

$$
P _ {t + 1} ^ {t} = \phi_ {1} ^ {2} P _ {t} ^ {t - 1} + \sigma_ {w} ^ {2} - \sum_ {j = 0} ^ {1} \pi_ {t j} K _ {t j} ^ {2} \Sigma_ {t j}, \tag {6.193}
$$

$$
\epsilon_ {t 0} = y _ {t} - \alpha - h _ {t} ^ {t - 1}, \tag {6.194}
$$

$$
\epsilon_ {t 1} = y _ {t} - \alpha - h _ {t} ^ {t - 1} - \mu_ {1}, \tag {6.195}
$$

$$
\Sigma_ {t 0} = P _ {t} ^ {t - 1} + \sigma_ {0} ^ {2}, \tag {6.196}
$$

$$
\Sigma_ {t 1} = P _ {t} ^ {t - 1} + \sigma_ {1} ^ {2}, \tag {6.197}
$$

$$
K _ {t 0} = \phi_ {1} P _ {t} ^ {t - 1} / \Sigma_ {t 0}, \tag {6.198}
$$

$$
K _ {t 1} = \phi_ {1} P _ {t} ^ {t - 1} / \Sigma_ {t 1}. \tag {6.199}
$$

To complete the filtering, we must be able to assess the probabilities $\pi _ { t 1 } =$ $\operatorname* { P r } ( u _ { t } = 1 \mid y _ { 1 } , . . . , y _ { t } )$ , for $t = 1 , \ldots , n$ ; of course, $\pi _ { t 0 } = 1 - \pi _ { t 1 }$ . Let $f _ { j } ( t \mid t - 1 )$ denote the conditional density of $y _ { t }$ given the past $y _ { 1 } , \ldots , y _ { t - 1 }$ , and $u _ { t } = j$ $( j = 0 , 1 )$ . Then,

$$
\pi_ {t 1} = \frac {\pi_ {1} f _ {1} (t \mid t - 1)}{\pi_ {0} f _ {0} (t \mid t - 1) + \pi_ {1} f _ {1} (t \mid t - 1)}, \tag {6.200}
$$

where we assume the distribution $\pi _ { j }$ , for $j = 0 , 1$ has been specified a priori. If the investigator has no reason to prefer one state over another the choice of uniform priors, $\pi _ { 1 } = 1 / 2$ , will suffice. Unfortunately, it is computationally difficult to obtain the exact values of $f _ { j } ( t \mid t - 1 )$ ; although we can give an explicit expression of $f _ { j } ( t \mid t - 1 )$ , the actual computation of the conditional density is prohibitive. A viable approximation, however, is to choose $f _ { j } ( t \mid t - 1 )$ to be the normal density, $\mathrm { N } ( h _ { t } ^ { t - 1 } + \mu _ { j } , \Sigma _ { t j } )$ , for $j = 0 , 1$ and $\mu _ { 0 } = 0$ ; see §6.8 for details.

The innovations filter given in (6.192)–(6.137) can be derived from the Kalman filter by a simple conditioning argument. For example, to derive (6.192), we write

$$
\begin{array}{l} \operatorname {E} \left(h _ {t + 1} \mid y _ {1}, \dots , y _ {t}\right) = \sum_ {j = 0} ^ {1} \operatorname {E} \left(h _ {t + 1} \mid y _ {1}, \dots , y _ {t}, u _ {t} = j\right) \Pr (u _ {t} = j \mid y _ {1}, \dots , y _ {t}) \\ = \sum_ {j = 0} ^ {1} \left(\phi_ {0} + \phi_ {1} h _ {t} ^ {t - 1} + K _ {t j} \epsilon_ {t j}\right) \pi_ {t j} \\ = \phi_ {0} + \phi_ {1} h _ {t} ^ {t - 1} + \sum_ {j = 0} ^ {1} \pi_ {t j} K _ {t j} \epsilon_ {t j}. \\ \end{array}
$$

Estimation of the parameters, $\Theta = ( \phi _ { 0 } , \phi _ { 1 } , \sigma _ { 0 } ^ { 2 } , \mu _ { 1 } , \sigma _ { 1 } ^ { 2 } , \sigma _ { w } ^ { 2 } ) ^ { \prime }$ , is accomplished via MLE based on the likelihood given by

$$
\ln L _ {Y} (\Theta) = \sum_ {t = 1} ^ {n} \ln \left(\sum_ {j = 0} ^ {1} \pi_ {j} f _ {j} (t \mid t - 1)\right), \tag {6.201}
$$

where the density $f _ { j } ( t \mid t - 1 )$ is approximated by the normal density, $\mathrm { N } ( h _ { t } ^ { t - 1 } +$ $\mu _ { j }$ , $\sigma _ { j } ^ { 2 }$ ), previously mentioned. We may consider maximizing (6.201) directly as a function of the parameters $\Theta$ using a Newton method, or we may consider applying the EM algorithm to the complete data likelihood.

# Example 6.21 Analysis of the New York Stock Exchange Returns

Figure 6.17 shows the log of the squares of returns, $y _ { t } = \log r _ { t } ^ { 2 }$ , of 2000 daily observations of the NYSE previously displayed in Figure 1.4.

![](images/2f935c4b306ab341cfed8688d67c4a0d6933ccad0bc782226564b91e5804f637.jpg)  
Figure 6.17 Graph of $y _ { t } = \log r _ { t } ^ { 2 }$ , where $r _ { t }$ is the daily return of the NYSE, 2000 observations.

Table 6.4 Estimation Results for the NYSE Fit   

<table><tr><td>Parameter</td><td>Estimate</td><td>Estimated Standard Error</td></tr><tr><td>φ0</td><td>-.006</td><td>.016</td></tr><tr><td>φ1</td><td>.988</td><td>.007</td></tr><tr><td>σw</td><td>.091</td><td>.027</td></tr><tr><td>α</td><td>-9.607</td><td>1.266</td></tr><tr><td>σ0</td><td>1.220</td><td>.065</td></tr><tr><td>μ1</td><td>-2.292</td><td>.204</td></tr><tr><td>σ1</td><td>2.683</td><td>.105</td></tr></table>

Model (6.188) and (6.190)-(6.191), with and $\pi _ { 1 }$ fixed at .5, was fit to the data using a quasi-Newton–Raphson method to maximize (6.201). The results are given in Table 6.4. Figure 6.18 compares the density of the log of a $\chi _ { 1 } ^ { 2 }$ with the fitted normal mixture; we note the data indicate a substantial amount of probability in the upper tail that the log- $\chi _ { 1 } ^ { 2 }$ distribution misses.

Finally, Figure 6.19 shows $y _ { t }$ for $8 0 0 \leq t \leq 1 0 0 0$ , which includes the crash of October 19, 1987, with $y _ { t } ^ { t - 1 } = \widehat { \alpha } + h _ { t } ^ { t - 1 }$ superimposed on the graph; compare with Figure 5.6. Also displayed are error bounds.

It is possible to use the bootstrap procedure described in §6.7 for the stochastic volatility model, with some minor changes. The following procedure was described in Stoffer and Wall (2004). We develop a vector first-order equation, as was done in (6.117). First, using (6.194)–(6.195), and noting that

![](images/e149574615aa07ad060316cc40b6a86a3367af259c93c0fcdf2b92ed7eeff436.jpg)  
Figure 6.18 Density of the log of a $\chi _ { 1 } ^ { 2 }$ as given by (6.189) (solid line) and the fitted normal mixture (dashed line) form the NYSE example.

$y _ { t } = \pi _ { t 0 } y _ { t } + \pi _ { t 1 } y _ { t }$ , we may write

$$
y _ {t} = \alpha + h _ {t} ^ {t - 1} + \pi_ {t 0} \epsilon_ {t 0} + \pi_ {t 1} \left(\epsilon_ {t 1} + \mu_ {1}\right). \tag {6.202}
$$

Consider the standardized innovations

$$
e _ {t j} = \Sigma_ {t j} ^ {- 1 / 2} \epsilon_ {t j}, \quad j = 0, 1, \tag {6.203}
$$

and define the $2 \times 1$ vector

$$
\boldsymbol {e} _ {t} = \left[ \begin{array}{c} e _ {t 0} \\ e _ {t 1} \end{array} \right].
$$

Also, define the $2 \times 1$ vector

$$
\boldsymbol {\xi} _ {t} = \left[ \begin{array}{c} h _ {t + 1} ^ {t} \\ y _ {t} \end{array} \right].
$$

Combining (6.192) and (6.202) results in a vector first-order equation for $\pmb { \xi } _ { t }$ given by

$$
\boldsymbol {\xi} _ {t} = F \boldsymbol {\xi} _ {t - 1} + G _ {t} + H _ {t} \boldsymbol {e} _ {t}, \tag {6.204}
$$

where

$$
\begin{array}{r} F = \left[ \begin{array}{l l} \phi_ {1} & 0 \\ 1 & 0 \end{array} \right], G _ {t} = \left[ \begin{array}{l} \phi_ {0} \\ \alpha + \pi_ {t 1} \mu_ {1} \end{array} \right], H _ {t} = \left[ \begin{array}{l l} \pi_ {t 0} K _ {t 0} \Sigma_ {t 0} ^ {1 / 2} & \pi_ {t 1} K _ {t 1} \Sigma_ {t 1} ^ {1 / 2} \\ \pi_ {t 0} \Sigma_ {t 0} ^ {1 / 2} & \pi_ {t 1} \Sigma_ {t 1} ^ {1 / 2} \end{array} \right]. \end{array}
$$

![](images/62d95c5b124fa2e4ee2dac72e6ffe3ecb5093f45cbc7c0a154a805c11c206547.jpg)

![](images/8adca0fa2cf71cc0d09b35a02656af0909e295f63451ebc1fce403d5ff155be2.jpg)  
Figure 6.19 Two hundred observations of $y _ { t } = \log r _ { t } ^ { 2 }$ , for $8 0 1 \leq t \leq 1 0 0 0$ , where $r _ { t }$ is the daily return of the NYSE (top). Corresponding one-step-ahead predicted log volatility, $\log \sigma _ { t } ^ { 2 }$ , with $\pm 2$ standard prediction errors (bottom).

Hence, the steps in bootstrapping for this case are the same as steps 1 through 5 described in §5.7, but with (6.117) replaced by the following first-order equation:

$$
\boldsymbol {\xi} _ {t} ^ {*} = F (\widehat {\Theta}) \boldsymbol {\xi} _ {t - 1} ^ {*} + G _ {t} (\widehat {\Theta}; \widehat {\pi} _ {t 1}) + H _ {t} (\widehat {\Theta}; \widehat {\pi} _ {t 1}) \boldsymbol {e} _ {t} ^ {*}, \tag {6.205}
$$

where $\widehat { \Theta } = ( \widehat { \phi } _ { 0 } , \widehat { \phi } _ { 1 } , \widehat { \sigma } _ { 0 } ^ { 2 } , \widehat { \alpha } , \widehat { \mu } _ { 1 } , \widehat { \sigma } _ { 1 } ^ { 2 } , \widehat { \sigma } _ { w } ^ { 2 } ) ^ { \prime }$ is the MLE of $\Theta$ , and $\widehat { \pi } _ { t 1 }$ is estimated via (6.200), replacing $f _ { 1 } ( t \mid t - 1 )$ and $f _ { 0 } ( t \mid t - 1 )$ by their respective estimated normal densities ( $\widehat { \pi } _ { t 0 } = 1 - \widehat { \pi } _ { t 1 }$ ).

# Example 6.22 Analysis of the U.S. GNP Growth Rate

In Example 5.3, we fit an ARCH model to the U.S. GNP growth rate. In this example, we will fit a stochastic volatility model to the residuals from the MA(2) fit on the growth rate (see Example 3.35).

Figure 6.20 shows the log of the squared residuals, say $y _ { t }$ , from the MA(2) fit on the U.S. GNP series. The stochastic volatility model (6.187)– (6.191) was then fit to $y _ { t }$ . Table 6.5 shows the MLEs of the model

![](images/3e8bb22f3dd10c0c787b03561038b69349c328b3b1d898f3547b8d8af2fed671.jpg)  
Figure 6.20 Log of the squared residuals from an MA(2) fit on GNP growth rate.

parameters along with their asymptotic SEs assuming the model is correct. Also displayed in Table 6.5 are the means and SEs of $B = 5 0 0$ bootstrapped samples. There is some amount of agreement between the asymptotic values and the bootstrapped values. The interest here, however, is not so much in the SEs, but in the actual sampling distribution of the estimates. For example, Figure 6.21 compares the bootstrap histogram and asymptotic normal distribution of $\widehat { \phi } _ { 1 }$ . In this case, the bootstrap distribution exhibits positive kurtosis and skewness which is missed by the assumption of asymptotic normality.

# 6.11 State-Space and ARMAX Models for Longitudinal Data Analysis

In some studies, we may observe several independent $k$ -dimensional time series, say, $\pmb { y } _ { t \ell }$ , for $\ell = 1 , \ldots , N$ . For example, a new treatment may be given to $N$ patients with high blood pressure, and the systolic and diastolic blood pressures (SBP and DBP) are recorded at equal time intervals, for some time, using an ambulatory device. We may think of $\pmb { y } _ { t \ell }$ as being the bivariate, $k = 2$ , recordings of SBP and DBP at time $t$ for person $\ell$ . It is also reasonable to assume, in this example, exogenous variables may have been collected on each

![](images/c7542d53d5577d067bf6ca57b73076b6574f2780c33b6728bfcc7092204f4e13.jpg)  
Figure 6.21 Bootstrap histogram and asymptotic distribution of $\widehat { \phi } _ { 1 }$ for the U.S. GNP example.

Table 6.5 Estimates and Their Asymptotic and Bootstrap Standard Errors for U.S. GNP Example.   

<table><tr><td>Parameter</td><td>MLE</td><td>Asymptotic SE</td><td>Bootstrap Mean†</td><td>Bootstrap SE†</td></tr><tr><td>φ0</td><td>.068</td><td>.274</td><td>-.010</td><td>.353</td></tr><tr><td>φ1</td><td>.900</td><td>.099</td><td>.864</td><td>.102</td></tr><tr><td>σw</td><td>.378</td><td>.208</td><td>.696</td><td>.375</td></tr><tr><td>α</td><td>-10.524</td><td>2.321</td><td>-10.792</td><td>.748</td></tr><tr><td>μ1</td><td>-2.164</td><td>.567</td><td>-1.941</td><td>.416</td></tr><tr><td>σ1</td><td>3.007</td><td>.377</td><td>2.891</td><td>.422</td></tr><tr><td>σ0</td><td>.935</td><td>.198</td><td>.692</td><td>.362</td></tr></table>

† Based on 500 bootstrapped samples.

subject to help explain the variation in blood pressure (for example, gender, race, age, activity, and so on). We might expect to encounter missing data or irregularly spaced observations in this type of experiment; these problems are easier to handle from a state-space perspective.

An extension of the ARMAX model given in (6.103) that might handle the case of cross-sectional data, $\pmb { y } _ { t \ell }$ , is

$$
\boldsymbol {y} _ {t \ell} = \Gamma \boldsymbol {u} _ {t \ell} + \sum_ {j = 1} ^ {p} \Phi_ {j} \boldsymbol {y} _ {t - j, \ell} + \sum_ {j = 1} ^ {q} \Theta_ {j} \boldsymbol {w} _ {t - j, \ell} + \boldsymbol {w} _ {t \ell}, \tag {6.206}
$$

where, for $\ell = 1 , \ldots , N$ , $\mathrm { v a r } ( { \pmb w } _ { t \ell } ) = \Sigma _ { w }$ and ${ \pmb u } _ { t } \ell$ represents the $r \times 1$ vector of exogenous variables. As in §6.6, Property P6.6, we can write (6.206) in terms

of a state-space model. That is, for $\ell = 1 , \ldots , N$ ,

$$
\boldsymbol {x} _ {t + 1, \ell} = F \boldsymbol {x} _ {t, \ell} + G \boldsymbol {w} _ {t, \ell}, \tag {6.207}
$$

$$
\boldsymbol {y} _ {t, \ell} = [ I, 0, \dots , 0 ] \boldsymbol {x} _ {t \ell} + \Gamma \boldsymbol {u} _ {t \ell} + \boldsymbol {w} _ {t \ell}, \tag {6.208}
$$

where matrices $F$ and $G$ are as in (6.104), ${ \pmb x } _ { t , { \pmb \ell } }$ represents the unobserved state, and y is the observation at time $\mathbf { \Delta } \mathbf { y } _ { t , \ell }$ $t$ , replication $\ell$ . Maximum likelihood estimation for state space models with cross-sectional data, such as the example given here, was investigated by Goodrich and Caines (1979), and can be carried out with minor modifications to the methods described in §6.3. In particular, given data $\mathbf { \Delta } \mathbf { y } _ { t , \ell }$ , $t = 1 , \ldots , n$ , $\ell = 1 , \ldots , N$ , we can use Newton–Raphson to minimize the criterion function, which is, up to a constant term, proportional to the negative of the log likelihood function,

$$
l (\Theta) = N ^ {- 1} \sum_ {\ell = 1} ^ {N} \left(\sum_ {t = 1} ^ {n} \log \left| \Sigma_ {t, \ell} (\Theta) \right| + \sum_ {t = 1} ^ {n} \epsilon_ {t, \ell} (\Theta) ^ {\prime} \Sigma_ {t, \ell} (\Theta) ^ {- 1} \epsilon_ {t, \ell} (\Theta)\right), \tag {6.209}
$$

where $\epsilon _ { t , \ell } ( \Theta )$ and $\Sigma _ { t , \ell } ( \Theta )$ are the innovations and their variance–covariance matrices, respectively. For details, see Goodrich and Caines (1979).

Anderson (1978) did an extensive study of replicated ARX models, that is, the case in which $q = 0$ in (6.206). We can write this model using regression notation as

$$
\boldsymbol {y} _ {t \ell} = \mathcal {B} \boldsymbol {z} _ {t \ell} + \boldsymbol {w} _ {t \ell}, \tag {6.210}
$$

for $\ell = 1 , \ldots , N$ and $t = p + 1 , \ldots , n$ , where

$$
\boldsymbol {z} _ {t \ell} = \left(\boldsymbol {u} _ {t \ell} ^ {\prime}, \boldsymbol {y} _ {t - 1, \ell} ^ {\prime}, \dots , \boldsymbol {y} _ {t - p, \ell} ^ {\prime}\right) ^ {\prime} \tag {6.211}
$$

and the matrix of regression coefficients is

$$
\mathcal {B} = \left[ \Gamma , \Phi_ {1}, \Phi_ {2}, \dots , \Phi_ {p} \right]. \tag {6.212}
$$

The estimate of the regression matrix $\boldsymbol { \beta }$ in this case is

$$
\widehat {\mathcal {B}} = \left(\sum_ {\ell = 1} ^ {N} \sum_ {t = p + 1} ^ {n} \boldsymbol {y} _ {t \ell} \boldsymbol {z} _ {t \ell} ^ {\prime}\right) \left(\sum_ {\ell = 1} ^ {N} \sum_ {t = p + 1} ^ {n} \boldsymbol {z} _ {t \ell} \boldsymbol {z} _ {t \ell} ^ {\prime}\right) ^ {- 1}, \tag {6.213}
$$

and an estimate of $\Sigma _ { w }$ is

$$
\widehat {\Sigma} _ {w} = \frac {1}{N (n - p)} \sum_ {\ell = 1} ^ {N} \sum_ {t = p + 1} ^ {n} \left(\mathbf {y} _ {t \ell} - \widehat {\mathcal {B}} \mathbf {z} _ {t \ell}\right) \left(\mathbf {y} _ {t \ell} - \widehat {\mathcal {B}} \mathbf {z} _ {t \ell}\right) ^ {\prime}. \tag {6.214}
$$

Inference for $\widehat { B }$ follows as in multivariate regression. That is, the large sample standard error of the $i j$ -th element of $\boldsymbol { B }$ is $\sqrt { \widehat { \sigma } _ { j j } c _ { i i } }$ , where $\widehat { \sigma } _ { j j }$ is the $j$ -th diagonal element of $\widehat { \Sigma } _ { w }$ and $c _ { i i }$ is the $i$ -th diagonal element of

$$
\left(\sum_ {\ell = 1} ^ {N} \sum_ {t = p + 1} ^ {n} z _ {t \ell} z _ {t \ell} ^ {\prime}\right) ^ {- 1}.
$$

Model (6.206) may be somewhat restrictive in its assumption that the parameters do not change over time. Because replications exist, extending the model to the case of time-varying parameters is easy. The case of time-varying parameters in (6.210) was also presented in Anderson (1978). In particular, the model is written as

$$
\boldsymbol {y} _ {t \ell} = \Gamma_ {t} \boldsymbol {u} _ {t \ell} + \sum_ {j = 1} ^ {p _ {t}} \Phi_ {t j} \boldsymbol {y} _ {t - j, \ell} + \boldsymbol {w} _ {t \ell}, \tag {6.215}
$$

and $\mathrm { v a r } ( { \pmb w } _ { t \ell } ) = \Sigma _ { t }$ , for $\ell = 1 , \ldots , N$ . The order of the model, $p _ { t }$ , is also allowed to vary with time, and the equal spacing of time is not required. Of course, we can still use regression for estimation because the time-varying model can be written as $n$ regressions, one for each point in time,

$$
\boldsymbol {y} _ {t \ell} = \mathcal {B} _ {t} \boldsymbol {z} _ {t \ell} + \boldsymbol {w} _ {t \ell}, \tag {6.216}
$$

for $\ell = 1 , \ldots , N$ , where $z _ { t \ell }$ is as in (6.211), but with $p$ replaced by $p _ { t }$ , and where now,

$$
\mathcal {B} _ {t} = \left[ \Gamma_ {t}, \Phi_ {t 1}, \Phi_ {t 2}, \dots , \Phi_ {t p _ {t}} \right], \tag {6.217}
$$

assuming $t > p _ { t }$ . The estimate of $B _ { t }$ , for any time $t$ , is now given by

$$
\widehat {\boldsymbol {\mathcal {B}}} _ {t} = \left(\sum_ {\ell = 1} ^ {N} \boldsymbol {y} _ {t \ell} \boldsymbol {z} _ {t \ell} ^ {\prime}\right) \left(\sum_ {\ell = 1} ^ {N} \boldsymbol {z} _ {t \ell} \boldsymbol {z} _ {t \ell} ^ {\prime}\right) ^ {- 1}, \tag {6.218}
$$

and an estimate of $\Sigma _ { t }$ is

$$
\widehat {\Sigma} _ {t} = \frac {1}{N - p _ {t} - 1} \sum_ {\ell = 1} ^ {N} \left(\boldsymbol {y} _ {t \ell} - \widehat {\mathcal {B}} _ {t} \boldsymbol {z} _ {t \ell}\right) \left(\boldsymbol {y} _ {t \ell} - \widehat {\mathcal {B}} _ {t} \boldsymbol {z} _ {t \ell}\right) ^ {\prime}. \tag {6.219}
$$

# Example 6.23 The Effect of Prenatal Smoking on Growth

In this example, we use data taken from an epidemiologic study at the University of Pittsburgh that focused on the effects of substance use during pregnancy. In particular, we focus on the growth of $N = 3 1 8$ children followed from birth to six years of age. In this longitudinal study, the children were examined at birth ( $t = 0$ ), and at eight months ( $t = 1$ ), 18 months ( $t = 2$ ), 36 months ( $t = 3$ ), and 72 months ( $t = 4$ ) of age. At times $t = 1 , 2 , 3 , 4$ , a growth index, say, $y _ { t \ell }$ , was calculated for each child $\ell = 1 , \ldots , 3 1 8$ . The growth index is essentially a standardized score for a child’s weight adjusting for that child’s age, gender, and height, against the national averages. At birth, $y _ { 0 \ell }$ represents the standardized birthweight of child $\ell$ .

We might consider that children not prenatally exposed to teratogens would follow a certain growth curve, whereas exposed children would

![](images/90cb1d488977fe8e184abf648f7add6f4afdbb8dddf153275b0d32e51f1a4915.jpg)  
Figure 6.22 Average growth scores across time for four groups of children. A solid line represents children not prenatally exposed to cigarette smoke; a dashed line represents children prenatally exposed to cigarette smoke. A circle represents white children, and a cross represents black children.

follow another. To investigate this hypothesis, we propose the following time-varying ARX model for growth:

$$
\begin{array}{l} {y _ {t \ell}} = {\gamma_ {0 t} + \gamma_ {1 t} S _ {\ell} + \gamma_ {2 t} R _ {\ell} + \gamma_ {3 t} S _ {\ell} R _ {\ell}} \\ + \sum_ {j = 1} ^ {t} \phi_ {t j} \left(y _ {t - j, \ell} - \widehat {y} _ {t - j, \ell}\right) + w _ {t \ell}, \tag {6.220} \\ \end{array}
$$

for $t = 0 , 1 , 2 , 3 , 4$ , where $\mathrm { v a r } ( w _ { t \ell } ) = \sigma _ { t } ^ { 2 }$ , for $\ell = 1 , \ldots , 3 1 8$ . The exogenous variables in the model are, $S _ { \ell }$ , the average number of cigarettes per day the mother smoked during the second trimester of pregnancy, and $R _ { \ell }$ , which indicates race (0 = black, 1 = white). The model is written in terms of the innovation sequences, $\left( y _ { t - j , \ell } - \widehat { y } _ { t - j , \ell } \right)$ , where $\widehat { y } _ { t , \ell }$ is the prediction of $y _ { t , \ell }$ from the previous model. We did this to remove any effect of smoking or race on previous growth. Figure 6.22 shows the average growth scores over time for four groups: 68 black children not exposed to smoke prenatally (solid line-cross), 92 white children not exposed to smoke (solid line-circle), 83 black children exposed to smoke (dashed linecross), and 75 white children exposed to smoke (dashed line-circle). For display purposes in Figure 6.22, smoking has been dichotomized to no exposure versus any exposure, but in the analysis, the smoking variable is in average cigarettes per day.

For example, the model for birthweight, $t = 0$ , is

$$
y _ {0 \ell} = \gamma_ {0 0} + \gamma_ {1 0} S _ {\ell} + \gamma_ {2 0} R _ {\ell} + \gamma_ {3 0} S _ {\ell} R _ {\ell} + w _ {0 \ell}.
$$

Once the model has been estimated, the predicted values are calculated

$$
\widehat {y} _ {0 \ell} = \widehat {\gamma} _ {0 0} + \widehat {\gamma} _ {1 0} S _ {\ell} + \widehat {\gamma} _ {2 0} R _ {\ell} + \widehat {\gamma} _ {3 0} S _ {\ell} R _ {\ell}.
$$

Then, the model for growth at eight months, $t = 1$ , is

$$
y _ {1 \ell} = \gamma_ {0 1} + \gamma_ {1 1} S _ {\ell} + \gamma_ {2 1} R _ {\ell} + \gamma_ {3 1} S _ {\ell} R _ {\ell} + \phi_ {1 1} (y _ {0, \ell} - \widehat {y} _ {0, \ell}) + w _ {1 \ell},
$$

where $\left( { y _ { 0 , \ell } - \widehat { y } _ { 0 , \ell } } \right)$ represents birthweight with the effect of smoking and race removed. In this way, only $S _ { \ell }$ represents smoking and $R _ { \ell }$ represents race, because their effect on birthweight has been removed. The other cases, for $t = 2 , 3 , 4$ continue in the same way.

The following estimates are the results of the fit; we only report the final models. At birth,

$$
\widehat {y} _ {0 \ell} = 3. 2 9 5 -. 0 1 1 _ {(. 0 0 2)} S _ {\ell} +. 2 1 5 _ {(. 0 5 6)} R _ {\ell},
$$

with $\widehat { \sigma } _ { 0 } = . 4 7 2$ ; estimated standard errors are shown in parenthesis. We conclude that prenatal smoking significantly reduces birthweight, white babies are born slightly bigger, and no interaction exists between smoking and race. At eight months,

$$
\begin{array}{l} \widehat {y} _ {1 \ell} = -. 0 1 5 _ {(. 0 1 1) S _ {\ell}} - . 3 3 5 _ {(. 1 4 7)} R _ {\ell} \\ + \quad . 0 2 9 _ {(. 0 1 2)} S _ {\ell} R _ {\ell} +. 2 1 4 _ {(. 1 2 7)} (y _ {0, \ell} - \widehat {y} _ {0, \ell}), \\ \end{array}
$$

with $\widehat { \sigma } _ { 1 } ~ = ~ 1 . 0 6 6$ . The interaction term is significant, indicating that white, unexposed babies are slightly smaller than the others.

The estimated model for 18 months is,

$$
\begin{array}{l} \widehat {y} _ {2 \ell} = . 3 4 0 +. 2 7 8 _ {(. 1 2 5)} R _ {\ell} \\ + \quad . 6 6 1 _ {(. 0 5 6)} (y _ {1, \ell} - \widehat {y} _ {1, \ell}) +. 3 5 7 _ {(. 1 2 6)} (y _ {0, \ell} - \widehat {y} _ {0, \ell}), \\ \end{array}
$$

with $\widehat { \sigma } _ { 2 } ~ = ~ 1 . 0 5 9$ . Now, the effect of prenatal smoking is gone at 18 months, and, at this age, the white kids tend to be larger. The result at 36 months ( $t = 3$ ) is that prenatal smoking becomes significant again, but exposed children are slightly bigger at this age, and race is no longer significant (this result is not as unusual as it might seem; in fact, it has been hypothesized that children exposed prenatally to cigarette smoke tend to become obese as they grow older):

$$
\begin{array}{l} \widehat {y} _ {3 \ell} = . 3 3 4 +. 0 0 8 _ {(}. 0 0 4) S _ {\ell} +. 3 1 0 _ {(}. 0 4 4) (y _ {2, \ell} - \widehat {y} _ {2, \ell}) \\ + \quad . 4 5 0 _ {(. 0 4 3)} (y _ {1, \ell} - \widehat {y} _ {1, \ell}) +. 4 6 5 _ {(. 0 9 8)} (y _ {0, \ell} - \widehat {y} _ {0, \ell}), \\ \end{array}
$$

with $\widehat { \sigma } _ { 2 } = . 8 1 7$ . Finally, the result for 72 months is

$$
\begin{array}{l} \widehat {y} _ {4 \ell} = . 3 3 0 +. 9 3 3 _ {(. 0 8 2)} (y _ {3, \ell} - \widehat {y} _ {3, \ell}) \\ + \quad . 4 6 2 _ {(. 0 6 3)} (y _ {2, \ell} - \widehat {y} _ {2, \ell}) +. 4 8 4 _ {(. 0 6 2)} (y _ {1, \ell} - \widehat {y} _ {1, \ell}), \\ \end{array}
$$

with $\widehat { \sigma } _ { 2 } = 1 . 1 7 6$ . At this age, the effect of prenatal smoking and the effect of race are gone. Also growth at eight months ( $t = 1$ ) is still a predictor of growth at 72 months, but the effect of birthweight ( $t = 0$ ) is gone.

# Mixed Linear Models in State-Space Form

A widely used general mixed model for longitudinal data was introduced by Laird and Ware (1982). In this case, responses $\pmb { y } _ { \ell } = \{ y _ { t , \ell } , t = 1 , \dots , n _ { \ell } \}$ are obtained on $N$ subjects, $\ell = 1 , \ldots , N$ . Each response vector is modeled as

$$
\boldsymbol {y} _ {\ell} = X _ {\ell} \boldsymbol {\beta} + Z _ {\ell} \boldsymbol {\gamma} _ {\ell} + \boldsymbol {\epsilon} _ {\ell}, \tag {6.221}
$$

where $X _ { \ell }$ is an $n _ { \ell } \times b$ design matrix, $\beta$ is a $b \times 1$ vector of fixed parameters, and $Z _ { \ell }$ is an $n _ { \ell } \times g$ design matrix corresponding to the random $g \times 1$ vector of random effects, $\gamma _ { \ell }$ , which is assumed to be independent across subject, and distributed as $\gamma _ { \ell } \sim \mathrm { N } ( \mathbf { 0 } , D )$ , where $D > 0$ is an arbitrary variance–covariance matrix. The within-subject errors, $\boldsymbol { \epsilon } \ell$ , are independently distributed as $\epsilon _ { \ell } \sim$ $\mathrm { N } ( \mathbf { 0 } , \Sigma _ { \ell } )$ ; often, $\Sigma _ { \ell }$ is of the form $\sigma ^ { 2 } I$ . A good introduction to these models can be found in many texts; for example, Diggle et al. (1994), Jones (1993), and Fahrmeir and Tutz (1994). Jones (1993) focuses on the state-space approach, and so will we.

The model, (6.221), can be written as

$$
\boldsymbol {y} _ {\ell} \sim \mathrm {N} \left(X _ {\ell} \boldsymbol {\beta}, V _ {\ell}\right), \tag {6.222}
$$

independently, for $\ell = 1 , \ldots , N$ , where

$$
V _ {\ell} = Z _ {\ell} D Z _ {\ell} ^ {\prime} + \Sigma_ {\ell}. \tag {6.223}
$$

An example of a typical covariance structure for $V _ { \ell }$ is compound symmetry, wherein $g = 1$ , $Z _ { \ell }$ is a vector of ones, $D = \sigma _ { \gamma } ^ { 2 }$ is a scalar, and $\Sigma _ { \ell } = \sigma ^ { 2 } I$ . In this way, $V _ { \ell }$ is an $n \ell \times n \ell$ matrix given by

$$
V _ {\ell} = \left( \begin{array}{c c c c} \sigma^ {2} + \sigma_ {\gamma} ^ {2} & \sigma_ {\gamma} ^ {2} & \dots & \sigma_ {\gamma} ^ {2} \\ \sigma_ {\gamma} ^ {2} & \sigma^ {2} + \sigma_ {\gamma} ^ {2} & \dots & \sigma_ {\gamma} ^ {2} \\ \vdots & \vdots & \ddots & \vdots \\ \sigma_ {\gamma} ^ {2} & \sigma_ {\gamma} ^ {2} & \dots & \sigma^ {2} + \sigma_ {\gamma} ^ {2} \end{array} \right). \tag {6.224}
$$

Another useful covariance structure is the autoregressive structure, where $g = 0$ (that is, no random effects exist) and

$$
V _ {\ell} = \Sigma_ {\ell} = \sigma^ {2} \left( \begin{array}{c c c c c} 1 & \rho & \rho^ {2} & \dots & \rho^ {n _ {\ell} - 1} \\ \rho & 1 & \rho & \dots & \rho^ {n _ {\ell} - 2} \\ \vdots & \vdots & \ddots & & \vdots \\ \rho^ {n _ {\ell} - 1} & \rho^ {n _ {\ell} - 2} & & \dots & 1 \end{array} \right), \tag {6.225}
$$

with $| \rho | < 1$ .

For a particular subject, $\ell$ , the vector $\pmb { y } _ { \ell }$ consists of observations, $y _ { t \ell }$ , taken over time $t = 1 , 2 , \ldots , n _ { \ell }$ . For subject $\ell$ , model (6.221) states

$$
y _ {t \ell} = \boldsymbol {x} _ {t \ell} ^ {\prime} \boldsymbol {\beta} + \boldsymbol {z} _ {t \ell} ^ {\prime} \boldsymbol {\gamma} _ {\ell} + \epsilon_ {t \ell}, \tag {6.226}
$$

where $\pmb { x } _ { t \ell } ^ { \prime }$ is the $t$ -th row of $X _ { \ell }$ and $ { \boldsymbol { z } } _ { t \ell } ^ { \prime }$ is the $t$ -th row of $Z _ { \ell }$ . Using the form of the model given by (6.226), $y _ { t \ell }$ is normal with

$$
E (y _ {t \ell}) = \boldsymbol {x} _ {t \ell} ^ {\prime} \boldsymbol {\beta},
$$

$$
\operatorname {c o v} \left(y _ {t \ell}, y _ {s \ell}\right) = z _ {t \ell} ^ {\prime} D z _ {s \ell} + \sigma_ {\ell , t s},
$$

$$
\operatorname {c o v} \left(y _ {t \ell}, y _ {s k}\right) = 0 \quad \ell \neq k,
$$

where $\sigma _ { \ell , t s }$ is the $t s$ -th element of $\Sigma _ { \ell }$ . For the example given in (6.224), we would have

$$
\operatorname {v a r} (y _ {t \ell}) = \sigma^ {2} + \sigma_ {\gamma} ^ {2} \quad \mathrm {a n d} \quad \operatorname {c o v} (y _ {t \ell}, y _ {s \ell}) = \sigma_ {\gamma} ^ {2},
$$

for any $t \neq s$ , so the correlation between two observations on the same subject is given by $\rho = \sigma _ { \gamma } ^ { 2 } / ( \sigma ^ { 2 } + \sigma _ { \gamma } ^ { 2 } )$ . In the autoregressive case, (6.225), the correlation between two observations $y _ { t \ell }$ and $y _ { s \ell }$ on the same subject $t - s$ time units apart is, of course, $\rho ^ { | t - s | }$ .

The Laird–Ware model has a state space formulation; Jones (1993) provides a detailed presentation of these and related topics. If random effects exist, that is $g \geq 1$ , and $\Sigma _ { \ell } = \sigma ^ { 2 } I$ , let $\mathbf { \sigma } _ { \mathbf { s } _ { t , \ell } }$ denote a $g \times 1$ state vector with initial condition $\pmb { s } _ { 0 , \ell } \sim \mathrm { N } ( \mathbf { 0 } , D )$ . Then, for each $\ell = 1 , \ldots , N$ , (6.226) can be written as

$$
\boldsymbol {s} _ {t, \ell} = \boldsymbol {s} _ {t - 1, \ell} + \boldsymbol {w} _ {t, \ell}, \tag {6.227}
$$

$$
y _ {t \ell} = \boldsymbol {x} _ {t \ell} ^ {\prime} \boldsymbol {\beta} + \boldsymbol {z} _ {t \ell} ^ {\prime} \boldsymbol {s} _ {t, \ell} + \epsilon_ {t \ell}, \tag {6.228}
$$

for $t = 1 , \ldots , n _ { \ell }$ , where $\pmb { w } _ { t , \ell } \equiv \mathbf { 0 }$ , or, equivalently, ${ \pmb w } _ { t , \ell } \sim \mathrm { N } ( { \bf 0 } , Q )$ , where $Q = 0$ is the zero matrix. All other values are as defined in (6.226). The data $y _ { t \ell }$ as written in (6.227)-(6.228) have the same properties as the data written in (6.226).

If $g ~ = ~ 0$ , that is, no random effects exist, and the variance–covariance structure is autoregressive, as in (6.225), the state-space model can be written as

$$
s _ {t \ell} = \rho s _ {t - 1, \ell} + w _ {t, \ell}, \tag {6.229}
$$

$$
y _ {t \ell} = \boldsymbol {x} _ {t \ell} ^ {\prime} \boldsymbol {\beta} + s _ {t \ell}, \tag {6.230}
$$

where, now, the autoregressive structure is entered into the data via the (scalar, in this example) state, and there is no measurement error. In this case, $R =$ 0, which does not present a problem in running the Kalman filter, provided $P _ { 0 } ^ { 0 } > 0$ . To obtain a matrix of the form given in (6.225), $w _ { t \ell }$ is white Gaussian noise, with $Q = \sigma ^ { 2 }$ , and the initial state satisfies $s _ { 0 , \ell } \sim \mathrm { N } ( 0 , \sigma ^ { 2 } / ( 1 - \rho ^ { 2 } ) )$ . In this case, recall the states, $s _ { t \ell }$ , for a given subject $\ell$ , form a stationary AR(1) process with variance $\sigma ^ { 2 } / ( 1 - \rho ^ { 2 } )$ and ACF given by $\rho ( h ) = \rho ^ { | h | }$ .

In the more general case in which both random effects, $g \ > \ 0$ , and an autoregressive error structure exist, we can combine the ideas used to get (6.227)-(6.228) and (6.229)-(6.230). In this case, the state equation would be a $( g + 1 ) \times 1$ process made by stacking (6.227) and (6.229), and the observation equation would be

$$
y _ {t \ell} = \boldsymbol {x} _ {t \ell} ^ {\prime} \boldsymbol {\beta} + A _ {t} s _ {t \ell},
$$

where $A _ { t } = [ z _ { t \ell } ^ { \prime } , 1 ]$ .

We immediately see from (6.227)-(6.228), or from (6.229)-(6.230), that the likelihood of the data is the same as the one given in (6.209), but with $n$ set to $n \ell$ . Consequently, the methods presented in §5.3 can be used to estimate the parameters of the Laird–Ware model, namely, $\beta$ , and variance components in $V _ { \ell }$ , for $\ell = 1 , \ldots , n _ { \ell }$ . For simplicity, let $\Theta$ represent the vector of all of the parameters associated with the model.

In the notation of the algorithm presented in 6.3, Step 1 is to find initial estimates, $\Theta ^ { ( 0 ) }$ , of the parameters $\Theta$ . If the $V _ { \ell }$ were known, using a weighted least squares argument (see §4.4), the least squares estimate of $\beta$ in the model (6.222)-(6.223) is given by

$$
\widehat {\boldsymbol {\beta}} = \left(\sum_ {\ell = 1} ^ {N} X _ {\ell} ^ {\prime} V _ {\ell} ^ {- 1} X _ {\ell}\right) ^ {- 1} \left(\sum_ {\ell = 1} ^ {N} X _ {\ell} ^ {\prime} V _ {\ell} ^ {- 1} \mathbf {y} _ {\ell}\right). \tag {6.231}
$$

Initial guesses for $V _ { \ell }$ should reflect the variance–covariance structure of the model. We can use (6.231) with the initial values chosen for $V _ { \ell }$ to obtain the initial regression coefficients, ${ \boldsymbol { \beta } } ^ { ( 0 ) }$ .

To accomplish Step 2 of the algorithm, for each $\ell = 1 , \ldots , N$ , run the Kalman filter (Property P6.1 with the states denoted by $\mathbf { \Delta } _ { \mathbf { \delta } } \mathbf { \vec { s } } _ { t }$ ) for $t = 1 , \ldots , n _ { \ell }$ to obtain the initial innovations and their covariance matrices. For example, if the model is of the form given in (6.227)-(6.228), run the Kalman filter with $\Phi = I$ , $Q = 0$ , $A _ { t } = z _ { t \ell } ^ { \prime }$ , $R = [ \sigma ^ { ( 0 ) } ] ^ { 2 }$ , and initial conditions $\pmb { s } _ { 0 } ^ { 0 } = \mathbf { 0 }$ , $P _ { 0 } ^ { 0 } = D ^ { ( 0 ) } > 0$ . In addition, $y _ { t \ell }$ replaced by $y _ { t \ell } - \pmb { x } _ { t \ell } ^ { \prime } \pmb { \beta } ^ { ( 0 ) }$ ; this is also equivalent to running Property P6.6 with uncorrelated noises, wherein the rows of the fixed effects design matrix, $X _ { \ell }$ , are the exogenous variables. The Newton– Raphson procedure (steps 3 and 4 of the algorithm in 6.3) is performed on the criterion function given in (6.209). The following example may help in understanding the technique.

# Example 6.24 Response to Medication

As a simple example of how we can use the state-space formulation of the Laird–Ware model, we analyze the S+ data set drug.mult. The data are taken from an experiment in which six subjects are given a dose of medication and then observed immediately and at weekly intervals for three weeks. The data are given in Table 6.6.

Table 6.6 Weekly Response to Medication   

<table><tr><td>\( \ell \)</td><td>Gender</td><td>Week 0 \( y_1 \)</td><td>Week 1 \( y_2 \)</td><td>Week 2 \( y_3 \)</td><td>Week 3 \( y_4 \)</td></tr><tr><td>1</td><td>F</td><td>75.9</td><td>74.3</td><td>80.0</td><td>78.9</td></tr><tr><td>2</td><td>F</td><td>78.3</td><td>75.5</td><td>79.6</td><td>79.2</td></tr><tr><td>3</td><td>F</td><td>80.3</td><td>78.2</td><td>80.4</td><td>76.2</td></tr><tr><td>4</td><td>M</td><td>80.7</td><td>77.2</td><td>82.0</td><td>83.8</td></tr><tr><td>5</td><td>M</td><td>80.3</td><td>78.6</td><td>81.4</td><td>81.5</td></tr><tr><td>6</td><td>M</td><td>80.1</td><td>81.1</td><td>81.9</td><td>86.4</td></tr></table>

We fit model (6.229)-(6.230) to this data using gender as a grouping variable. In particular, if $\pmb { y } _ { t \ell }$ is the $4 \times 1$ vector of observations over time for a female ( $\ell = 1 , 2 , 3$ ), the model is

$$
\left( \begin{array}{l} y _ {1} \\ y _ {2} \\ y _ {3} \\ y _ {4} \end{array} \right) = \left( \begin{array}{l l} 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 1 & 0 \end{array} \right) \left( \begin{array}{l} \beta_ {1} \\ \beta_ {2} \end{array} \right) + \left( \begin{array}{l} \epsilon_ {1} \\ \epsilon_ {2} \\ \epsilon_ {3} \\ \epsilon_ {4} \end{array} \right),
$$

and for a male ( $\ell = 4 , 5 , 6$ ), the model is

$$
\left( \begin{array}{l} y _ {1} \\ y _ {2} \\ y _ {3} \\ y _ {4} \end{array} \right) = \left( \begin{array}{l l} 1 & 1 \\ 1 & 1 \\ 1 & 1 \\ 1 & 1 \end{array} \right) \left( \begin{array}{l} \beta_ {1} \\ \beta_ {2} \end{array} \right) + \left( \begin{array}{l} \epsilon_ {1} \\ \epsilon_ {2} \\ \epsilon_ {3} \\ \epsilon_ {4} \end{array} \right),
$$

where the $\epsilon _ { t }$ , in general, form an AR(1) process given by

$$
\begin{array}{l} \epsilon_ {0} = w _ {0} / \sqrt {1 - \rho^ {2}}, \\ \epsilon_ {t} \quad = \quad \rho \epsilon_ {t - 1} + w _ {t} \quad t = 1, 2, 3, 4, \\ \end{array}
$$

where $w _ { t }$ is white Gaussian noise, with $\mathrm { v a r } ( w _ { t } ) = \sigma _ { w } ^ { 2 }$ . Recall $\mathrm { v a r } ( \epsilon _ { t } ) =$ $\sigma _ { \epsilon } ^ { 2 } = \sigma _ { w } ^ { 2 } / ( 1 - \rho ^ { 2 } )$ and $\rho _ { \epsilon } ( h ) = \rho ^ { | h | }$ . A different value of $\rho$ was selected for each gender group, say, $\rho _ { 1 }$ for female subjects and $\rho _ { 2 }$ for male subjects.

We initialized the estimation procedure with $\rho _ { 1 } ^ { ( 0 ) } = \rho _ { 2 } ^ { ( 0 ) } = 0$ ρ2 (0) = 0, $\sigma _ { w } ^ { ( 0 ) } =$ 2, which, upon using (6.231), yields $\beta ^ { ( 0 ) } \ = \ ( 7 8 . 0 7 , 3 . 1 8 ) ^ { \prime }$ . The final estimates (and their estimated standard errors) were

$$
\begin{array}{l} \widehat {\beta} _ {1} = 7 8. 2 0 (. 5 6), \quad \widehat {\beta} _ {2} = 3. 2 4 (. 8 9), \\ \widehat {\rho} _ {1} = -. 4 7 (. 3 6), \quad \widehat {\rho} _ {2} = . 0 7 (. 5 3), \quad \widehat {\sigma} _ {w} = 2. 1 7 (. 3 6). \\ \end{array}
$$

Because $\widehat { \rho } _ { 1 }$ and $\widehat { \rho } _ { 2 }$ are not significantly different from zero, this would suggest either a simple linear regression is sufficient to describe the results, or the model is not correct.

Next, we fit the compound symmetry model using (6.227)-(6.228) with $g = 1$ . In this case, the model for a female subject is

$$
\left( \begin{array}{c} y _ {1} \\ y _ {2} \\ y _ {3} \\ y _ {4} \end{array} \right) = \left( \begin{array}{c c} 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 1 & 0 \end{array} \right) \left( \begin{array}{c} \beta_ {1} \\ \beta_ {2} \end{array} \right) + \left( \begin{array}{c} 1 \\ 1 \\ 1 \\ 1 \end{array} \right) \gamma_ {1} + \left( \begin{array}{c} \epsilon_ {1} \\ \epsilon_ {2} \\ \epsilon_ {3} \\ \epsilon_ {4} \end{array} \right),
$$

and for a male subject, the model is

$$
\left( \begin{array}{c} y _ {1} \\ y _ {2} \\ y _ {3} \\ y _ {4} \end{array} \right) = \left( \begin{array}{c c} 1 & 1 \\ 1 & 1 \\ 1 & 1 \\ 1 & 1 \end{array} \right) \left( \begin{array}{c} \beta_ {1} \\ \beta_ {2} \end{array} \right) + \left( \begin{array}{c} 1 \\ 1 \\ 1 \\ 1 \end{array} \right) \gamma_ {2} + \left( \begin{array}{c} \epsilon_ {1} \\ \epsilon_ {2} \\ \epsilon_ {3} \\ \epsilon_ {4} \end{array} \right),
$$

where $\gamma _ { 1 } \sim \mathrm { N } ( 0 , \sigma _ { \gamma _ { 1 } } ^ { 2 } )$ , $\gamma _ { 2 } \sim \mathrm { { N } } ( 0 , \sigma _ { \gamma _ { 2 } } ^ { 2 } )$ , and the $\epsilon _ { t }$ , for $t = 1 , 2 , 3 , 4$ are 1uncorrelated with variance $\sigma _ { \epsilon } ^ { 2 }$ .

In this case, thefemale subjects ( riable) and lar process with  for male subject $D = \sigma _ { \gamma _ { 1 } } ^ { 2 }$ or). $\ell = 1 , 2 , 3$ $D = \sigma _ { \gamma _ { 2 } } ^ { 2 }$ $\ell = 4 , 5 , 6$ Starting the estimation process off with $\sigma _ { \gamma _ { 1 } } ^ { ( 0 ) } = \sigma _ { \gamma _ { 2 } } ^ { ( 0 ) } = 1$ = σγ2 , $\sigma _ { \epsilon } ^ { ( 0 ) } = 2 $ , and $\pmb { \beta } ^ { ( 0 ) } = ( 7 8 , 3 ) ^ { \prime }$ , the final estimates were

$$
\begin{array}{l} \widehat {\beta} _ {1} = 7 8. 0 3 (. 6 7), \quad \widehat {\beta} _ {2} = 3. 5 1 (1. 0 5), \\ \widehat {\sigma} _ {\gamma_ {1}} = 2. 0 5 (. 4 5), \quad \widehat {\sigma} _ {\gamma_ {2}} = 2. 5 1 (. 5 9), \quad \widehat {\sigma} _ {\epsilon} = 2. 0 0 (. 1 3). \\ \end{array}
$$

This model fits the data well.

# Problems

# Section 6.1

6.1 Consider a system process given by

$$
x _ {t} = -. 9 x _ {t - 2} + w _ {t} \quad t = 1, \ldots , n
$$

where $x _ { 0 } \sim \mathrm { N } ( 0 , \sigma _ { 0 } ^ { 2 } )$ , $x _ { - 1 } \sim N ( 0 , \sigma _ { 1 } ^ { 2 } )$ , and $w _ { t }$ is Gaussian white noise with variance $\sigma _ { w } ^ { 2 }$ . The system process is observed with noise, say,

$$
y _ {t} = x _ {t} + v _ {t},
$$

where $v _ { t }$ is Gaussian white noise with variance $\sigma _ { v } ^ { 2 }$ . Further, suppose $x _ { 0 }$ , $x _ { - 1 }$ , $\{ w _ { t } \}$ and $\{ v _ { t } \}$ are independent.

(a) Write the system and observation equations in the form of a state space model.

(b) Find the values of $\sigma _ { 0 } ^ { 2 }$ and $\sigma _ { 1 } ^ { 2 }$ that make the observations, $y _ { t }$ , stationary.   
(c) Generate $n = 1 0 0$ observations with $\sigma _ { w } = 1$ , $\sigma _ { v } = 1$ and using the values of $\sigma _ { 0 } ^ { 2 }$ and $\sigma _ { 1 } ^ { 2 }$ found in (b). Do a time plot of $x _ { t }$ and of $y _ { t }$ and compare the two processes. Also, compare the sample ACF and PACF of $x _ { t }$ and of $y _ { t }$ .   
(d) Repeat (c), but with $\sigma _ { v } = 1 0$

6.2 Consider the state-space model presented in Example 6.3. Let $x _ { t } ^ { t - 1 } =$ $E ( x _ { t } | y _ { t - 1 } , \dots , y _ { 1 } )$ and let $P _ { t } ^ { t - 1 } = E ( x _ { t } - x _ { t } ^ { t - 1 } ) ^ { 2 } .$ t . The innovation se-−quence or residuals are $\epsilon _ { t } = y _ { t } - y _ { t } ^ { t - 1 }$ , where $y _ { t } ^ { t - 1 } = E ( y _ { t } | y _ { t - 1 } , \dots , y _ { 1 } )$ t−1t = E(yt|yt 1, . . . , y1). Find $\mathrm { c o v } \big ( \epsilon _ { s } , \epsilon _ { t } \big )$ in terms of $x _ { t } ^ { t - 1 }$ and $P _ { t } ^ { t - 1 }$ for (i) $s \neq t$ − and (ii) $s = t$ .

# Section 6.2

6.3 Simulate $n = 1 0 0$ observations from the following state-space model:

$$
x _ {t} = . 8 x _ {t - 1} + w _ {t} \quad \text {a n d} \quad y _ {t} = x _ {t} + v _ {t}
$$

where $x _ { 0 } \sim \mathrm { N } ( 0 , 2 . 7 8 )$ , $w _ { t } \sim$ iid $\mathrm { { N } } ( 0 , 1 )$ , and $v _ { t } \sim$ iid $\mathrm { { N } } ( 0 , 1 )$ are all mutually independent. Compute and plot the data, $y _ { t }$ , the one-stepahead predictors, $y _ { t } ^ { t - 1 }$ along with the root mean square prediction errors, $E ^ { 1 / 2 } ( y _ { t } - y _ { t } ^ { t - 1 } ) ^ { 2 }$ using Figure 6.3 as a guide.

6.4 Suppose the vector ${ \pmb z } = ( { \pmb x } ^ { \prime } , { \pmb y } ^ { \prime } ) ^ { \prime }$ , where $\pmb { x }$ $( p \times 1 )$ and $\pmb { y }$ $( q \times 1 )$ are jointly distributed with mean vectors ${ \pmb \mu } _ { x }$ and $\pmb { \mu } _ { y }$ and with covariance matrix

$$
\operatorname {c o v} (\boldsymbol {z}) = \left( \begin{array}{c c} \Sigma_ {x x} & \Sigma_ {x y} \\ \Sigma_ {y x} & \Sigma_ {y y} \end{array} \right).
$$

Consider projecting $_ { x }$ on $\mathcal { M } = \overline { { \operatorname { s p } } } \{ 1 , y \}$ , say, $\widehat { \pmb { x } } = \pmb { b } + B \pmb { y }$

(a) Show the orthogonality conditions can be written as

$$
\begin{array}{l} E (\boldsymbol {x} - \boldsymbol {b} - B \boldsymbol {y}) = 0, \\ E \left[ (\boldsymbol {x} - \boldsymbol {b} - B \boldsymbol {y}) \boldsymbol {y} ^ {\prime} \right] = 0, \\ \end{array}
$$

leading to the solutions

$$
\boldsymbol {b} = \boldsymbol {\mu} _ {x} - B \boldsymbol {\mu} _ {y} \quad \text {a n d} \quad B = \Sigma_ {x y} \Sigma_ {y y} ^ {- 1}.
$$

(b) Prove the mean square error matrix is

$$
M S E = E \left[ (\boldsymbol {x} - \boldsymbol {b} - B \boldsymbol {y}) \boldsymbol {x} ^ {\prime} \right] = \Sigma_ {x x} - \Sigma_ {x y} \Sigma_ {y y} ^ {- 1} \Sigma_ {y x}.
$$

(c) How can these results be used to justify the claim that, in the absence of normality, Property P6.1 yields the best linear estimate of the state ${ \pmb x } _ { t }$ given the data $Y _ { t }$ , namely, $\mathbf { \Delta } _ { \pmb { x } _ { t } ^ { t } }$ , and its corresponding MSE, namely, $P _ { t } ^ { t }$ ?

6.5 Derivation of Property P6.2 Based on the Projection Theorem. Throughout this problem, we use the notation of Property P6.2 and of the Projection Theorem given in Appendix B, where $\mathcal { H }$ is $L ^ { 2 }$ . If $\mathcal { L } _ { k + 1 } =$ $\overline { { \operatorname { s p } } } \{ \pmb { y } _ { 1 } , \dotsc , \pmb { y } _ { k + 1 } \}$ , and $\mathscr { V } _ { k + 1 } = \overline { { \operatorname { s p } } } \{ \pmb { y } _ { k + 1 } - \pmb { y } _ { k + 1 } ^ { k } \}$ , for $k = 0 , 1 , \ldots , n - 1$ , where $\pmb { y } _ { k + 1 } ^ { k }$ is the projection of $\pmb { y } _ { k + 1 }$ on $\mathcal { L } _ { k }$ , then, $\mathcal { L } _ { k + 1 } = \mathcal { L } _ { k } \oplus \mathcal { V } _ { k + 1 }$ . We assume $P _ { 0 } ^ { 0 } > 0$ and $R > 0$ .

(a) Show the projection of $\pmb { x } _ { k }$ on $\mathcal { L } _ { k + 1 }$ , that is, $\pmb { x } _ { k } ^ { k + 1 }$ , is given by

$$
\boldsymbol {x} _ {k} ^ {k + 1} = \boldsymbol {x} _ {k} ^ {k} + H _ {k + 1} (\boldsymbol {y} _ {k + 1} - \boldsymbol {y} _ {k + 1} ^ {k}),
$$

where $H _ { k + 1 }$ can be determined by the orthogonality property

$$
E \left\{\left(\boldsymbol {x} _ {k} - H _ {k + 1} \left(\boldsymbol {y} _ {k + 1} - \boldsymbol {y} _ {k + 1} ^ {k}\right)\right) \left(\boldsymbol {y} _ {k + 1} - \boldsymbol {y} _ {k + 1} ^ {k}\right) ^ {\prime} \right\} = 0.
$$

Show

$$
H _ {k + 1} = P _ {k} ^ {k} \Phi^ {\prime} A _ {k + 1} ^ {\prime} \left[ A _ {k + 1} P _ {k + 1} ^ {k} A _ {k + 1} ^ {\prime} + R \right] ^ {- 1}.
$$

(b) Define $J _ { k } = P _ { k } ^ { k } \Phi ^ { \prime } [ P _ { k + 1 } ^ { k } ] ^ { - 1 }$ , and show

$$
\pmb {x} _ {k} ^ {k + 1} = \pmb {x} _ {k} ^ {k} + J _ {k} (\pmb {x} _ {k + 1} ^ {k + 1} - \pmb {x} _ {k + 1} ^ {k}).
$$

(c) Repeating the process, show

$$
\boldsymbol {x} _ {k} ^ {k + 2} = \boldsymbol {x} _ {k} ^ {k} + J _ {k} (\boldsymbol {x} _ {k + 1} ^ {k + 1} - \boldsymbol {x} _ {k + 1} ^ {k}) + H _ {k + 2} (\boldsymbol {y} _ {k + 2} - \boldsymbol {y} _ {k + 2} ^ {k + 1}),
$$

solving for $H _ { k + 2 }$ . Simplify and show

$$
\pmb {x} _ {k} ^ {k + 2} = \pmb {x} _ {k} ^ {k} + J _ {k} (\pmb {x} _ {k + 1} ^ {k + 2} - \pmb {x} _ {k + 1} ^ {k}).
$$

(d) Using induction, conclude

$$
\boldsymbol {x} _ {k} ^ {n} = \boldsymbol {x} _ {k} ^ {k} + J _ {k} \left(\boldsymbol {x} _ {k + 1} ^ {n} - \boldsymbol {x} _ {k + 1} ^ {k}\right),
$$

which yields the smoother with $k = t - 1$

# Section 6.3

6.6 (a) Consider the univariate state-space model given by state conditions $x _ { 0 } = w _ { 0 }$ , $x _ { t } = x _ { t - 1 } + w _ { t }$ and observations $y _ { t } = x _ { t } + v _ { t }$ , $t = 1 , 2 , \ldots$ , where $w _ { t }$ and ${ \boldsymbol { v } } _ { t }$ are independent, Gaussian, white noise processes with $\mathrm { v a r } ( w _ { t } ) = \sigma _ { w } ^ { 2 }$ and $\mathrm { v a r } ( v _ { t } ) = \sigma _ { v } ^ { 2 }$ . Show the data follow an IMA(1,1) model, that is, $\nabla y _ { t }$ follows an MA(1) model.

(b) Fit the model specified in part (a) to the logarithm of the glacial varve series and compare the results to those presented in Example 3.31.

6.7 Let $y _ { t }$ represent the land-based global temperature series shown in Figure 6.2. The data file for this problem is HL.dat on the website.

(a) Using regression, fit a third-degree polynomial in time to $y _ { t }$ , that is, fit the model

$$
y _ {t} = \beta_ {0} + \beta_ {1} t + \beta_ {2} t ^ {2} + \beta_ {3} t ^ {3} + \epsilon_ {t},
$$

where $\epsilon _ { t }$ is white noise. Do a time plot of the data fit, $\widehat { y } _ { t }$ , superimposed on the data, $y _ { t }$ , for t=1,. . . ,108.

(b) Write the model $y _ { t } = x _ { t } + v _ { t }$ with $\nabla ^ { 2 } x _ { t } = w _ { t }$ , where $w _ { t }$ and $v _ { t }$ are independent white noise processes, in state-space form. Hint: The state will be a $2 \times 1$ vector, say, $\pmb { x } _ { t } = ( x _ { t } , x _ { t - 1 } ) ^ { \prime }$ . Fit the state-space model to the data, and do a time plot of the estimated filter, $\widehat { x } _ { t } ^ { t - 1 }$ , and the estimated smoother, $\widehat { x } _ { t } ^ { n }$ , superimposed on the data, $y _ { t }$ , for $\scriptstyle \mathrm { t = 1 , \dots , 1 0 8 }$ . Compare these results with the results of part (a).

6.8 Consider the model

$$
y _ {t} = x _ {t} + v _ {t},
$$

where $v _ { t }$ is Gaussian white noise with variance $\sigma _ { v } ^ { 2 }$ , $x _ { t }$ are independent Gaussian random variables with mean zero and $\mathrm { v a r } ( x _ { t } ) = r _ { t } \sigma _ { x } ^ { 2 }$ with $x _ { t }$ independent of $v _ { t }$ , and $r _ { 1 } , \ldots , r _ { n }$ are known constants. Show that applying the EM algorithm to the problem of estimating $\sigma _ { x } ^ { 2 }$ and $\sigma _ { v } ^ { 2 }$ leads to updates (represented by hats)

$$
\widehat {\sigma} _ {x} ^ {2} = \frac {1}{n} \sum_ {t = 1} ^ {n} \frac {\sigma_ {t} ^ {2} + \mu_ {t} ^ {2}}{r _ {t}} \quad \mathrm {a n d} \quad \widehat {\sigma} _ {v} ^ {2} = \frac {1}{n} \sum_ {t = 1} ^ {n} [ (y _ {t} - \mu_ {t}) ^ {2} + \sigma_ {t} ^ {2} ],
$$

where, based on the current estimates (represented by tildes),

$$
\mu_ {t} = \frac {r _ {t} \widetilde {\sigma} _ {x} ^ {2}}{r _ {t} \widetilde {\sigma} _ {x} ^ {2} + \widetilde {\sigma} _ {v} ^ {2}} y _ {t} \quad \mathrm {a n d} \quad \sigma_ {t} ^ {2} = \frac {r _ {t} \widetilde {\sigma} _ {x} ^ {2} \widetilde {\sigma} _ {v} ^ {2}}{r _ {t} \widetilde {\sigma} _ {x} ^ {2} + \widetilde {\sigma} _ {v} ^ {2}}.
$$

6.9 Develop the EM algorithm for the model with inputs, (6.3) and (6.4).

6.10 To explore the stability of the filter, consider a univariate state-space model. That is, for $t = 1 , 2 , \ldots$ , the observations are $y _ { t } = x _ { t } + v _ { t }$ and the state equation is $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ , where $\sigma _ { w } = \sigma _ { v } = 1$ and $| \phi | < 1$ . The initial state, $x _ { 0 }$ , has zero mean and variance one.

$P _ { t } ^ { t - 1 }$ $P _ { t - 1 } ^ { t - 2 }$

(b) Use the result of (a) to verify $P _ { t } ^ { t - 1 }$ approaches a limit ( $t \to \infty$ ) $P$ that is the positive solution of $P ^ { 2 } - \phi ^ { 2 } P - 1 = 0$ .   
(c) With $K = \operatorname* { l i m } _ { t  \infty } K _ { t }$ as given in Property P6.1, show $| 1 - K | < 1$ .   
(d) Show, in steady-state, the one-step-ahead predictor, $y _ { n + 1 } ^ { n } = E ( y _ { n + 1 } \mid$ $y _ { n } , y _ { n - 1 } , \ldots .$ , of a future observation satisfies

$$
y _ {n + 1} ^ {n} = \sum_ {j = 0} ^ {\infty} \phi^ {j} K (1 - K) ^ {j - 1} y _ {n + 1 - j}.
$$

6.11 In §6.3, we discussed that it is possible to obtain a recursion for the gradient vector, $- \partial \ln { \cal L } _ { Y } ( \Theta ) / \partial \Theta$ . Assume the model is given by (6.1) and (6.2) and $A _ { t }$ is a known design matrix that does not depend on $\Theta$ , in which case Property P6.1 applies. For the gradient vector, show

$$
\begin{array}{l} \partial \ln L _ {Y} (\Theta) / \partial \Theta_ {i} = \sum_ {t = 1} ^ {n} \left\{\boldsymbol {\epsilon} _ {t} ^ {\prime} \Sigma_ {t} ^ {- 1} \frac {\partial \boldsymbol {\epsilon} _ {t}}{\partial \Theta_ {i}} - \frac {1}{2} \boldsymbol {\epsilon} _ {t} ^ {\prime} \Sigma_ {t} ^ {- 1} \frac {\partial \Sigma_ {t}}{\partial \Theta_ {i}} \Sigma_ {t} ^ {- 1} \boldsymbol {\epsilon} _ {t} \right. \\ \left. + \frac {1}{2} \mathrm {t r} \left(\Sigma_ {t} ^ {- 1} \frac {\partial \Sigma_ {t}}{\partial \Theta_ {i}}\right) \right\}, \\ \end{array}
$$

where the dependence of the innovation values on $\Theta$ is understood. In addition, with the general definition $\partial _ { i } g = \partial g ( \Theta ) / \partial \Theta _ { i }$ , show the following recursions, for $t = 2 , \ldots , n$ apply:

$$
- \partial_ {i} K _ {t - 1} \Sigma_ {t} K _ {t - 1} ^ {\prime} - K _ {t - 1} \partial_ {i} \Sigma_ {t} K _ {t - 1} ^ {\prime} - K _ {t - 1} \Sigma_ {t} \partial_ {i} K _ {t - 1} ^ {\prime},
$$

using the fact that $P _ { t } ^ { t - 1 } = \Phi P _ { t - 1 } ^ { t - 2 } \Phi ^ { \prime } + Q - K _ { t - 1 } \Sigma _ { t } K _ { t - 1 } ^ { \prime } .$

6.12 Continuing with the previous problem, consider the evaluation of the Hessian matrix and the numerical evaluation of the asymptotic variance– covariance matrix of the parameter estimates. The information matrix satisfies

$$
E \left\{- \frac {\partial^ {2} \ln L _ {Y} (\Theta)}{\partial \Theta \partial \Theta^ {\prime}} \right\} = E \left\{\left(\frac {\partial \ln L _ {Y} (\Theta)}{\partial \Theta}\right) \left(\frac {\partial \ln L _ {Y} (\Theta)}{\partial \Theta}\right) ^ {\prime} \right\};
$$

see Anderson (1984, Section 4.4), for example. Show the $( i , j )$ -th element of the information matrix, say, ${ \mathcal { T } } _ { i j } ( \Theta ) = E \left\{ - \partial ^ { 2 } \ln L _ { Y } ( \Theta ) / \partial \Theta _ { i } \partial \Theta _ { j } \right\}$ , is

$$
\begin{array}{l} \mathcal {I} _ {i j} (\Theta) = \sum_ {t = 1} ^ {n} E \left\{\partial_ {i} \boldsymbol {\epsilon} _ {t} ^ {\prime} \Sigma_ {t} ^ {- 1} \partial_ {j} \boldsymbol {\epsilon} _ {t} + \frac {1}{2} \operatorname {t r} \left(\Sigma_ {t} ^ {- 1} \partial_ {i} \Sigma_ {t} \Sigma_ {t} ^ {- 1} \partial_ {j} \Sigma_ {t}\right) \right. \\ \left. + \frac {1}{4} \mathrm {t r} \left(\Sigma_ {t} ^ {- 1} \partial_ {i} \Sigma_ {t}\right) \mathrm {t r} \left(\Sigma_ {t} ^ {- 1} \partial_ {j} \Sigma_ {t}\right) \right\}. \\ \end{array}
$$

Consequently, an approximate Hessian matrix can be obtained from the sample by dropping the expectation, $E$ , in the above result and using only the recursions needed to calculate the gradient vector.

# Section 6.4

6.13 As an example of the way the state-space model handles the missing data problem, suppose the first-order autoregressive process

$$
x _ {t} = \phi x _ {t - 1} + w _ {t}
$$

has an observation missing at $t = m$ , leading to the observations $y _ { t } =$ $A _ { t } x _ { t }$ , where $A _ { t } ~ = ~ 1$ for all $t$ , except $t = m$ wherein $A _ { t } = 0$ . Assume $x _ { 0 } = 0$ with variance $\sigma _ { w } ^ { 2 } / ( 1 - \phi ^ { 2 } )$ , where the variance of $w _ { t }$ is $\sigma _ { w } ^ { 2 }$ . Show the Kalman smoother estimators in this case are

$$
x _ {t} ^ {n} = \left\{ \begin{array}{l l} \phi y _ {1}, & t = 0, \\ \frac {\phi}{1 + \phi^ {2}} (y _ {m - 1} + y _ {m + 1}), & t = m, \\ y _ {t}, & t \neq 0, m, \end{array} \right.
$$

with mean square covariances determined by

$$
P _ {t} ^ {n} = \left\{ \begin{array}{l l} \sigma_ {w} ^ {2}, & t = 0, \\ \frac {\sigma_ {w} ^ {2}}{1 + \phi^ {2}}, & t = m, \\ 0 & t \neq 0, m. \end{array} \right.
$$

6.14 The data set labeled ar1miss.dat is $n = 1 0 0$ observations generated from an AR(1) process, $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ , with $\phi = . 9$ and $\sigma _ { w } = 1$ , where 10% of the data has been zeroed out at random. Considering the zeroed out data to be missing data, use the results of Problem 6.13 to estimate the parameters of the model, $\phi$ and $\sigma _ { w }$ , using the EM algorithm, and then estimate the missing values.

# Section 6.5

6.15 Using Example 6.10 as a guide, fit a structural model to the Federal Reserve Board Production Index data and compare it with the model fit in Example 3.43.

# Section 6.6

6.16 Use Property P6.6 to complete the following exercises.

(a) Write a univariate AR(1) model, $y _ { t } = \phi y _ { t - 1 } + v _ { t }$ , in state-space form. Verify your answer is indeed an AR(1).   
(b) Repeat (a) for an MA(1) model, $y _ { t } = v _ { t } + \theta v _ { t - 1 }$   
(c) Write an IMA(1,1) model, $y _ { t } = y _ { t - 1 } + v _ { t } + \theta v _ { t - 1 }$ , in state-space form.

6.17 Verify Property P6.5.   
6.18 Verify Property P6.6.

# Section 6.7

6.19 Repeat the bootstrap analysis of Example 6.12 on the entire three-month treasury bills and rate of inflation data set of 110 observations. Do the conclusions of Example 6.12—that the dynamics of the data is best described in terms of a fixed, rather than stochastic, regression—still hold?

# Section 6.8

6.20 Argue that a switching model is reasonable in explaining the behavior of the number of sunspots (see Figure 4.31) and then fit a switching model to the sunspot data.

# Section 6.9

6.21 Use the material presented in Example 6.18 to perform a Bayesian analysis of the model for the Johnson & Johnson data presented in Example 6.10.   
6.22 Verify (6.169) and (6.170).   
6.23 Verify (6.175) and (6.182).

# Section 6.10

6.24 Fit a stochastic volatility model to the returns of one (or more) of the four financial time series available in the R datasets package as EuStockMarkets.

# Section 6.11

6.25 In a small pilot study, a psychiatrist wanted to examine the effects of the drug lithium on bulimics (bulimics have continuous abnormal hunger and frequently go on eating binges). Although evidence of the effectiveness of lithium on bulimics has been shown, he was not sure if depressed subjects would respond differently than those without depression. He treated eight teenage female patients with lithium for 12 weeks; four of the subjects were diagnosed with depression, and half of the subjects received behavioral therapy. At the end of each four-week period, he recorded the number of binges each subject had during that week. The following are the results:

Weekly Number of Binges   

<table><tr><td>Subject</td><td>Depression</td><td>Week 0</td><td>Week 4</td><td>Week 8</td><td>Week 12</td></tr><tr><td>1</td><td>No</td><td>13</td><td>3</td><td>0</td><td>0</td></tr><tr><td>2</td><td>No</td><td>15</td><td>4</td><td>3</td><td>1</td></tr><tr><td>3</td><td>No</td><td>16</td><td>4</td><td>3</td><td>2</td></tr><tr><td>4</td><td>No</td><td>14</td><td>2</td><td>1</td><td>2</td></tr><tr><td>5</td><td>Yes</td><td>10</td><td>7</td><td>4</td><td>3</td></tr><tr><td>6</td><td>Yes</td><td>18</td><td>7</td><td>2</td><td>4</td></tr><tr><td>7</td><td>Yes</td><td>16</td><td>6</td><td>5</td><td>4</td></tr><tr><td>8</td><td>Yes</td><td>19</td><td>8</td><td>5</td><td>7</td></tr></table>

Fit a longitudinal model that addresses the concerns of the psychiatrist. Because the data are counts (number of occurrences), consider a square root transformation prior to the analysis.

# Chapter 7

# Statistical Methods in the Frequency Domain

# 7.1 Introduction

In previous chapters, we saw many applied time series problems that involved relating series to each other or to evaluating the effects of treatments or design parameters that arise when time-varying phenomena are subjected to periodic stimuli. In many cases, the nature of the physical or biological phenomena under study are best described by their Fourier components rather than by the difference equations involved in ARIMA or state-space models. The fundamental tools we use in studying periodic phenomena are the discrete Fourier transforms (DFTs) of the processes and their statistical properties. Hence, in 7.2, we review the properties of the DFT of a multivariate time series and discuss various approximations to the likelihood function based on the large-sample properties and the properties of the complex multivariate normal distribution. This enables extension of the classical techniques discussed in the following paragraphs to the multivariate time series case.

An extremely important class of problems in classical statistics develops when we are interested in relating a collection of input series to some output series. For example, in Chapter 2, we have previously considered relating temperature and various pollutant levels to daily mortality, but have not investigated the frequencies that appear to be driving the relation and have not looked at the possibility of leading or lagging effects. In Chapter 4, we isolated a definite lag structure that could be used to relate sea surface temperature to the number of new recruits. In Problem 5.11 of Chapter 5, the possible driving processes that could be used to explain inflow to Shasta Lake were hypothesized in terms of the possible inputs precipitation, cloud cover, temperature, and other variables. Identifying the combination of input factors in Figure 4.33

![](images/4011e6877c5a19e125efbdeba4cc20536c0c0392733204e4fed8812227a84e20.jpg)

![](images/fde485390d32f7cd78707b31983931f4f0da9cb5316c8ea619dbf73784747421.jpg)

![](images/0761226ef905f2d90b9faa5a883acd3f288a586fef62b04c19a4ac68990a0284.jpg)

![](images/661cdf6cedffc1b3dac468c77569fb0cf436fe72c8f23efc23b2079c4d70171f.jpg)

![](images/b4fb0bb1c8c92349dd0dc4a64bd3bfe2bb719d83d363fd4ebf51d1455ef25296.jpg)

![](images/6af6001b4534300a86b1d35806a5099c5ad6afda64882a16f61ebf7d101b423c.jpg)  
Figure 7.1 Mean response of subjects to various combinations of periodic stimulae measured at the cortex (primary somatosensory, contralateral).

that produce the best prediction for inflow is an example of multiple regression in the frequency domain, with the models treated theoretically by considering the regression, conditional on the random input processes.

A situation somewhat different from that above would be one in which the input series are regarded as fixed and known. In this case, we have a model analogous to that occurring in analysis of variance, in which the analysis now can be performed on a frequency by frequency basis. This analysis works especially well when the inputs are dummy variables, depending on some configuration of treatment and other design effects and when effects are largely dependent on periodic stimuli. As an example, we will look at a designed experiment measuring the fMRI brain responses of a number of awake and mildly anesthetized subjects to several levels of periodic brushing, heat, and shock effects. Some limited data from this experiment have been discussed previously in Example 1.6 of Chapter 1. Figure 7.1 shows mean responses to various levels of periodic heat, brushing, and shock stimuli for subjects awake and subjects under mild anesthesia. The stimuli were periodic in nature, applied alternately for 32 seconds (16 points) and then stopped for 32 seconds. The periodic input signal comes through under all three design conditions when the subjects are awake, but is somewhat attenuated under anesthesia. The mean shock level response hardly shows on the input signal; shock levels were designed to simulate surgical incision without inflicting tissue damage. The

means in Figure 7.1 are from a single location. Actually, for each individual, some nine series were recorded at various locations in the brain. It is natural to consider testing the effects of brushing, heat, and shock under the two levels of consciousness, using a time series generalization of analysis of variance.

A generalization to random coefficient regression is also considered, paralleling the univariate approach to signal extraction and detection presented in §4.9. This method enables a treatment of multivariate ridge-type regressions and inversion problems. Also, the usual random effects analysis of variance in the frequency domain becomes a special case of the random coefficient model.

The extension of frequency domain methodology to more classical approaches to multivariate discrimination and clustering is of interest in the frequency dependent case. Many time series differ in their means and in their autocovariance functions, making the use of both the mean function and the spectral density matrices relevant. As an example of such data, consider the bivariate series consisting of the P and S components derived from several earthquakes and explosions, such as those shown in Figure 7.2, where the P and S components, representing different arrivals have been separated from the first and second halves, respectively, of wave forms like those shown originally in Figure 1.7 of Chapter 1.

Two earthquakes and two explosions from a set of eight earthquakes and explosions are shown in Figure 7.2 and some essential differences exist that might be used to characterize the two classes of events. Also, the frequency content of the two components of the earthquakes appears to be lower than those of the explosions, and relative amplitudes of the two classes appear to differ. For example, the ratio of the S to $\mathrm { P }$ amplitudes in the earthquake group is much higher for this restricted subset. Spectral differences were also noticed in Chapter 4, where the explosion processes had a stronger high-frequency component relative to the low-frequency contributions. Examples like these are typical of applications in which the essential differences between multivariate time series can be expressed by the behavior of either the frequency-dependent mean value functions or the spectral matrix. In discriminant analysis, these types of differences are exploited to develop combinations of linear and quadratic classification criteria. Such functions can then be used to classify events of unknown origin, such as the Novaya Zemlya event shown in Figure 7.2, which tends to bear a visual resemblance to the explosion group.

Finally, for multivariate processes, the structure of the spectral matrix is also of great interest. We might reduce the dimension of the underlying process to a smaller set of input processes that explain most of the variability in the cross-spectral matrix as a function of frequency. Principal component analysis can be used to decompose the spectral matrix into a smaller subset of component factors that explain decreasing amounts of power. For example, the hydrological data might be explained in terms of a component process that weights heavily on precipitation and inflow and one that weights heavily on temperature and cloud cover. Perhaps these two components could explain most of the power in the spectral matrix at a given frequency. The ideas

![](images/d2df3e17efd988c4dfa20d3e7ed1e49475ea0c3d82a8c5f485aef1a1711704f7.jpg)

![](images/0d4b5b23ca9479c1dca6724592cfa750ec5e1a71180dc9c4d067ded243860bd3.jpg)

![](images/682b8555f536cd5481b0276335f5f47c89d581f3971c19aa095f6d310017a6eb.jpg)

![](images/aad9c65b095b3aae259d1596f33f4e9b8fb31cbeb3ab9a11c6c206632b4c544c.jpg)

![](images/688f4d18fda93aa0f94b9f323d674225733a3c818d230a31fb2687f54c7fccc6.jpg)

![](images/7676004856002a31e60a86ad5b703af88719db83d89222c5e53a4a9fc4b9d16a.jpg)

![](images/7176b37e1aa8648a7daf9cb9bcd04e415569236b173696dda6569a4336e0a131.jpg)

![](images/cb24ddd171ed87710aaf6595db4ccbf985023d95789054ffbb076f1fd3004691.jpg)

![](images/c46607af401a381caee391ad76fe8c97d67f7a1c0e9dfd93d0f07d48c16e4923.jpg)

![](images/e67e03bfaa29d3f48bdd4f6e210e482d4fb9c29f97f37c648e55dc3f115857ba.jpg)  
Figure 7.2 Bivariate earthquakes and explosions (40 pts/sec) compared with an event NZ (Novaya Zemlya) of unknown origin.

behind principal component analysis can also be generalized to include an optimal scaling methodology for categorical data called the spectral envelope (see Stoffer et al., 1993). In succeeding sections, we also give an introduction to dynamic Fourier analysis and to wavelet analysis.

# 7.2 Spectral Matrices and Likelihood Functions

We have previously argued for an approximation to the log likelihood based on the joint distribution of the DFTs in (4.116), where we used approximation as an aid in estimating parameters for certain parameterized spectra. In this chapter, we make heavy use of the fact that the sine and cosine transforms of the $p \times 1$ vector process $\pmb { x } _ { t } = ( x _ { t 1 } , x _ { t 2 } , . . . , x _ { t p } ) ^ { \prime }$ with mean $E { \mathbf { x } } _ { t } = \pmb { \mu } _ { t }$ , say, with DFT1

$$
\begin{array}{l} \boldsymbol {X} (\omega_ {k}) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t} \mathrm {e} ^ {- 2 \pi i \omega_ {k} t} \\ = \boldsymbol {X} _ {c} \left(\omega_ {k}\right) - i \boldsymbol {X} _ {s} \left(\omega_ {k}\right) \tag {7.1} \\ \end{array}
$$

and mean

$$
\begin{array}{l} \boldsymbol {M} (\omega_ {k}) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {\mu} _ {t} \mathrm {e} ^ {- 2 \pi i \omega_ {k} t} \\ = \boldsymbol {M} _ {c} \left(\omega_ {k}\right) - i \boldsymbol {M} _ {s} \left(\omega_ {k}\right) \tag {7.2} \\ \end{array}
$$

will be approximately uncorrelated, where we evaluate at the usual Fourier frequencies $\{ \omega _ { k } = k / n , \ 0 < | \omega _ { k } | < 1 / 2 \}$ . By Theorem C.6, the approximate $2 p \times 2 p$ covariance matrix of the cosine and sine transforms, say, $X ( \omega _ { k } ) =$ $( X _ { c } ( \omega _ { k } ) ^ { \prime } , X _ { s } ( \omega _ { k } ) ^ { \prime } ) ^ { \prime }$ , is

$$
\Sigma \left(\omega_ {k}\right) = \frac {1}{2} \left( \begin{array}{c c} C \left(\omega_ {k}\right) & - Q \left(\omega_ {k}\right) \\ Q \left(\omega_ {k}\right) & C \left(\omega_ {k}\right) \end{array} \right), \tag {7.3}
$$

and the real and imaginary parts are jointly normal. This result implies, by the results stated in Appendix C, the density function of the vector DFT, say, $X ( \omega _ { k } )$ , can be approximated as

$$
p (\omega_ {k}) \approx | f (\omega_ {k}) | ^ {- 1} \exp \bigl \{- \bigl (\boldsymbol {X} (\omega_ {k}) - \boldsymbol {M} (\omega_ {k}) \bigr) ^ {*} f ^ {- 1} (\omega_ {k}) \bigl (\boldsymbol {X} (\omega_ {k}) - \boldsymbol {M} (\omega_ {k}) \bigr) \bigr \},
$$

where the spectral matrix is the usual

$$
f \left(\omega_ {k}\right) = C \left(\omega_ {k}\right) - i Q \left(\omega_ {k}\right). \tag {7.4}
$$

Certain computations that we do in the section on discriminant analysis will involve approximating the joint likelihood by the product of densities like the one given above over subsets of the frequency band $0 < \omega _ { k } < 1 / 2$ .

To use the likelihood function for estimating the spectral matrix, for example, we appeal to the limiting result implied by Theorem C.7 and again choose $L$ frequencies in the neighborhood of some target frequency $\omega$ , say, ${ \pmb X } ( \omega _ { k } \pm k / n )$ , for $k = 1 , \ldots , m$ and $L = 2 m + 1$ . Then, let $X _ { \ell }$ , for $\ell = 1 , \ldots , L$ denote the indexed values, and note the DFTs of the mean adjusted vector process are approximately jointly normal with mean zero and complex covariance matrix $f = f ( \omega )$ . Then, write the log likelihood over the $L$ sub-frequencies as

$$
\ln L \left(\boldsymbol {X} _ {1}, \dots , \boldsymbol {X} _ {L}; f\right) \approx - L \ln | f | - \sum_ {\ell = 1} ^ {L} \left(\boldsymbol {X} _ {\ell} - \boldsymbol {M} _ {\ell}\right) ^ {*} f ^ {- 1} \left(\boldsymbol {X} _ {\ell} - \boldsymbol {M} _ {\ell}\right), \tag {7.5}
$$

where we have suppressed the argument of $f = f ( \omega )$ for ease of notation. The use of spectral approximations to the likelihood has been fairly standard, beginning with the work of Whittle (1961) and continuing in Brillinger (1981) and Hannan (1970). In this case, assuming the mean adjusted series are available, i.e., that $\pmb { M } _ { \ell }$ is known, so that we may assume that $X _ { \ell }$ is the mean-adjusted series. We may obtain the maximum likelihood estimator for $f$ by writing the joint log likelihood of the real and imaginary parts in terms of $\pmb { Z } _ { \ell } = ( \pmb { X } _ { c \ell } ^ { \prime } , \pmb { X } _ { s \ell } ^ { \prime } ) ^ { \prime }$ and obtaining the maximum likelihood estimators for $C$ and $Q$ , the real and imaginary parts of $f$ . Problem 7.2 shows we will obtain

$$
\widehat {f} = L ^ {- 1} \sum_ {\ell = 1} ^ {L} \left(\boldsymbol {X} _ {\ell} - \boldsymbol {M} _ {\ell}\right) \left(\boldsymbol {X} _ {\ell} - \boldsymbol {M} _ {\ell}\right) ^ {*}, \tag {7.6}
$$

which is just the usual mean-adjusted estimator for the spectral matrix.

# 7.3 Regression for Jointly Stationary Series

In §4.8, we considered a model of the form

$$
y _ {t} = \sum_ {r = - \infty} ^ {\infty} \beta_ {1 r} x _ {t - r, 1} + v _ {t}, \tag {7.7}
$$

where $x _ { t 1 }$ is a single observed input series and $y _ { t }$ is the observed output series, and we are interested in estimating the filter coefficients $\beta _ { 1 r }$ relating the adjacent lagged values of $x _ { t 1 }$ to the output series $y _ { t }$ . In the case of the SOI and Recruitment series, we identified the El Ni˜no driving series as $x _ { t 1 }$ , the input and $y _ { t }$ , the Recruitment series, as the output. In general, more than a single plausible input series may exist. For example, the hydrological data shown in Figure 4.33 suggests there may be at least five possible series driving the inflow. Hence, we may envision a $q \times 1$ input vector of driving series,

say, $\pmb { x } _ { t } = ( x _ { t 1 } , x _ { t 2 } , . . . , x _ { t q } ) ^ { \prime }$ , and a set of $q \times 1$ vector of regression functions $\beta _ { r } = ( \beta _ { 1 r } , \beta _ { 2 r , } , \ldots , \beta _ { q r } ) ^ { \prime }$ , which are related as

$$
y _ {t} = \sum_ {r = - \infty} ^ {\infty} \beta_ {r} ^ {\prime} \boldsymbol {x} _ {t - r} + v _ {t}. \tag {7.8}
$$

Writing the matrix form out as

$$
y _ {t} = \sum_ {j = 1} ^ {q} \sum_ {r = - \infty} ^ {\infty} \beta_ {j r} x _ {t - r, j} + v _ {t} \tag {7.9}
$$

shows the output is basically a sum of linearly filtered versions of the input processes and a stationary noise process $v _ { t }$ , assumed to be uncorrelated with ${ \pmb x } _ { t }$ . Each filtered component in the sum over $j$ gives the contribution of lagged values of the $j$ -th input series to the output series. We assume the regression functions $\beta _ { j r }$ are fixed and unknown.

The model given by (7.8) is useful under several different scenarios, corresponding to a number of different assumptions that can be made about the components. Assuming the input and output processes are jointly stationary with zero means leads to the conventional regression analysis given in this section. The analysis depends on theory that assumes we observe the output process $y _ { t }$ conditional on fixed values of the input vector $\pmb { x } _ { t }$ ; this is the same as the assumptions made in conventional regression analysis. Assumptions considered later involve letting the coefficient vector $\beta _ { t }$ be a random unknown signal vector that can be estimated by Bayesian arguments, using the conditional expectation given the data. The answers to this approach, given in §7.5, allow signal extraction and deconvolution problems to be handled. Assuming the inputs are fixed allows various experimental designs and analysis of variance to be done for both fixed and random effects models. Estimation of the frequency-dependent random effects variance components in the analysis of variance model is also considered in §7.5.

For the approach in this section, assume the inputs and outputs have zero means and are jointly stationary with the $( q + 1 ) \times 1$ vector process $( \pmb { x } _ { t } ^ { \prime } , y _ { t } ) ^ { \prime }$ of inputs ${ \pmb x } _ { t }$ and outputs $y _ { t }$ assumed to have a spectral matrix of the form

$$
f (\omega) = \left( \begin{array}{c c} f _ {x x} (\omega) & f _ {x y} (\omega) \\ f _ {y x} (\omega) & f _ {y y} (\omega) \end{array} \right), \tag {7.10}
$$

where $f _ { y x } ( \omega ) = ( f _ { y x _ { 1 } } ( \omega ) , f _ { y x _ { 2 } } ( \omega ) , \ldots , f _ { y x _ { q } } ( \omega ) )$ is the $1 \times q$ vector of crossspectra relating the $q$ inputs to the output and $f _ { x x } ( \omega )$ is the $q \times q$ spectral matrix of the inputs. Generally, we observe the inputs and search for the vector of regression functions $\beta _ { t }$ relating the inputs to the outputs. We assume all autocovariance functions satisfy the absolute summability conditions of the form

$$
\sum_ {h = - \infty} ^ {\infty} | h | | \gamma_ {j k} (h) | <   \infty . \tag {7.11}
$$

$( j , k = 1 , \ldots , q + 1 )$ , where $\gamma _ { j k } ( h )$ is the autocovariance corresponding to the cross-spectrum $f _ { j k } ( \omega )$ in (7.10). We also need to assume a linear process of the form (C.35) as a condition for using Theorem C.7 on the joint distribution of the discrete Fourier transforms in the neighborhood of some fixed frequency.

# Estimation of the Regression Function

In order to estimate the regression function $\beta _ { r }$ , the Projection Theorem (Appendix B) applied to minimizing

$$
M S E = E \left[ \left(y _ {t} - \sum_ {r = - \infty} ^ {\infty} \boldsymbol {\beta} _ {r} ^ {\prime} \boldsymbol {x} _ {t - r}\right) ^ {2} \right] \tag {7.12}
$$

leads to the orthogonality conditions

$$
E \left[ \left(y _ {t} - \sum_ {r = - \infty} ^ {\infty} \beta_ {r} ^ {\prime} \boldsymbol {x} _ {t - r}\right) \boldsymbol {x} _ {t - s} ^ {\prime} \right] = \mathbf {0} ^ {\prime} \tag {7.13}
$$

for all $s = 0 , \pm 1 , \pm 2 , . . .$ , where $\mathbf { 0 } ^ { \prime }$ denotes the $1 \times q$ zero vector. Taking the expectations inside and substituting for the definitions of the autocovariance functions appearing and leads to the normal equations

$$
\sum_ {r = - \infty} ^ {\infty} \boldsymbol {\beta} _ {r} ^ {\prime} \Gamma_ {x x} (s - r) = \boldsymbol {\gamma} _ {y x} ^ {\prime} (s), \tag {7.14}
$$

for $s = 0 , \pm 1 , \pm 2 , . . .$ , where $\Gamma _ { x x } ( s )$ denotes the $q \times q$ autocovariance matrix of the vector series ${ \pmb x } _ { t }$ at lag $s$ and $\gamma _ { y x } ( s ) = ( \gamma _ { y x _ { 1 } } ( s ) , \ldots , \gamma _ { y x _ { q } } ( s ) )$ is a $1 \times q$ vector containing the lagged covariances between $y _ { t }$ and $\pmb { x } _ { t }$ . Again, a frequency domain approximate solution is easier in this case because the computations can be done frequency by frequency using cross-spectra that can be estimated from sample data using the DFT. In order to develop the frequency domain solution, substitute the representation into the normal equations, using the same approach as used in the simple case derived in §4.8. This approach yields

$$
\int_ {- 1 / 2} ^ {1 / 2} \sum_ {r = - \infty} ^ {\infty} \boldsymbol {\beta} _ {r} ^ {\prime} \mathrm {e} ^ {2 \pi i \omega (s - r)} f _ {x x} (\omega) d \omega = \boldsymbol {\gamma} _ {y x} ^ {\prime} (s).
$$

Now, because $\gamma _ { y x } ^ { \prime } ( s )$ is the Fourier transform of the cross-spectral vector $f _ { y x } ( \omega ) = f _ { x y } ^ { * } ( \omega )$ , we might write the system of equations in the frequency domain, using the uniqueness of the Fourier transform, as

$$
\boldsymbol {B} ^ {\prime} (\omega) f _ {x x} (\omega) = f _ {x y} ^ {*} (\omega), \tag {7.15}
$$

where $f _ { x } ( \omega )$ is the $q \times q$ spectral matrix of the inputs and $B ( \omega )$ is the $q \times 1$ vector Fourier transform of $\beta _ { t }$ . Multiplying (7.15) on the right by $f _ { x x } ^ { - 1 } ( \omega )$ , assuming $f _ { x x } ( \omega )$ is nonsingular at $\omega$ , leads to the frequency domain estimator

$$
\boldsymbol {B} ^ {\prime} (\omega) = f _ {x y} ^ {*} (\omega) f _ {x x} ^ {- 1} (\omega). \tag {7.16}
$$

Note, (7.16) implies the regression function would take the form

$$
\boldsymbol {\beta} _ {t} = \int_ {- 1 / 2} ^ {1 / 2} \boldsymbol {B} (\omega) \mathrm {e} ^ {2 \pi i \omega t} d \omega . \tag {7.17}
$$

As before, it is conventional to introduce the DFT as the approximate estimator for the integral (7.17) and write

$$
\boldsymbol {\beta} _ {t} ^ {M} = M ^ {- 1} \sum_ {k = 0} ^ {M - 1} \boldsymbol {B} \left(\omega_ {k}\right) \mathrm {e} ^ {2 \pi i \omega_ {k} t}, \tag {7.18}
$$

where $\omega _ { k } = k / M , M < < n$ . The approximation was shown in Problem 4.35 to hold exactly as long as $\boldsymbol { \beta } _ { t } = \mathbf { 0 }$ for $| t | \geq M$ and to have a mean squared error bounded by a function of the zero-lag autocovariance and the absolute sum of the neglected coefficients.

The mean squared error (7.12) can be written using the orthogonality principle, giving

$$
M S E = \int_ {- 1 / 2} ^ {1 / 2} f _ {y \cdot x} (\omega) d \omega , \tag {7.19}
$$

where

$$
f _ {y \cdot x} (\omega) = f _ {y y} (\omega) - f _ {x y} ^ {*} (\omega) f _ {x x} ^ {- 1} (\omega) f _ {x y} (\omega) \tag {7.20}
$$

denotes the residual or error spectrum The resemblance of (7.20) to the usual equations in regression analysis is striking. It is useful to pursue the multiple regression analogy further by noting a squared multiple coherence can be defined as

$$
\rho_ {y \cdot x} ^ {2} (\omega) = \frac {f _ {x y} ^ {*} (\omega) f _ {x x} ^ {- 1} (\omega) f _ {x y} (\omega)}{f _ {y y} (\omega)}. \tag {7.21}
$$

This expression leads to the mean squared error in the form

$$
M S E = \int_ {- 1 / 2} ^ {1 / 2} f _ {y y} (\omega) \left[ 1 - \rho_ {y \cdot x} ^ {2} (\omega) \right] d \omega , \tag {7.22}
$$

and we have an interpretation of $\rho _ { y \cdot x } ^ { 2 } ( \omega )$ as the proportion of power accounted for by the lagged regression on $\pmb { x } _ { t }$ at frequency $\omega$ . If $\rho _ { y \cdot x } ^ { 2 } ( \omega ) = 0$ for all $\omega$ , we have

$$
M S E = \int_ {- 1 / 2} ^ {1 / 2} f _ {y y} (\omega) d \omega = E [ y _ {t} ^ {2} ],
$$

which is the mean squared error when no predictive power exists. As long as $f _ { x x } ( \omega )$ is positive definite at all frequencies, $M S E \ge 0$ , and we will have

$$
0 \leq \rho_ {y \cdot x} ^ {2} (\omega) \leq 1 \tag {7.23}
$$

for all $\omega$ . If the multiple coherence is unity for all frequencies, the mean squared error in (7.22) is zero and the output series is perfectly predicted by a linearly

filtered combination of the inputs. Problem 7.3 shows the ordinary squared coherence between the series $y _ { t }$ and the linearly filtered combinations of the inputs appearing in (7.12) is exactly (7.21).

# Estimation Using Sampled Data

Clearly, the matrices of spectra and cross-spectra will not ordinarily be known, so the regression computations need to be based on sampled data. We assume, therefore, the inputs $x _ { t 1 } , x _ { t 2 } , \ldots , x _ { t q }$ and output $y _ { t }$ series are available at the time points $t = 1 , 2 , \ldots , n$ , as in Chapter 4. In order to develop reasonable estimates for the spectral quantities, some replication must be assumed. Often, only one replication of each of the inputs and the output will exist, so it is necessary to assume a band exists over which the spectra and crossspectra are approximately equal to $f _ { x x } ( \omega )$ and $f _ { x y } ( \omega )$ , respectively. Then, let $Y ( \omega _ { k } + \ell / n )$ and $X ( \omega _ { k } + \ell / n )$ be the DFTs of $y _ { t }$ and ${ \pmb x } _ { t }$ over the band, say, at frequencies of the form

$$
\omega_ {k} \pm \ell / n, \quad \ell = 1, \ldots , m,
$$

where $L = 2 m + 1$ as before. Then, simply substitute the sample spectral matrix

$$
\widehat {f} _ {x x} (\omega) = L ^ {- 1} \sum_ {\ell = - m} ^ {m} \boldsymbol {X} \left(\omega_ {k} + \ell / n\right) \boldsymbol {X} ^ {*} \left(\omega_ {k} + \ell / n\right) \tag {7.24}
$$

and the vector of sample cross-spectra

$$
\widehat {f} _ {x y} (\omega) = L ^ {- 1} \sum_ {\ell = - m} ^ {m} X \left(\omega_ {k} + \ell / n\right) \overline {{Y \left(\omega_ {k} + \ell / n\right)}} \tag {7.25}
$$

for the respective terms in (7.16) to get the regression estimator $\widehat B ( \omega )$ . For the regression estimator (7.18), we may use

$$
\widehat {\beta} _ {t} ^ {M} = \frac {1}{M} \sum_ {k = 0} ^ {M - 1} \widehat {f} _ {x y} ^ {*} (\omega_ {k}) \widehat {f} _ {x x} ^ {- 1} (\omega_ {k}) \mathrm {e} ^ {2 \pi i \omega_ {k} t} \tag {7.26}
$$

for $t = 0 , \pm 1 , \pm 2 , \dots , \pm ( M / 2 - 1 )$ , as the estimated regression function.

# Tests of Hypotheses

The estimated squared multiple coherence, corresponding to the theoretical coherence (7.21), becomes

$$
\widehat {\rho} _ {y \cdot x} ^ {2} (\omega) = \frac {\widehat {f} _ {x y} ^ {*} (\omega) \widehat {f} _ {x x} ^ {- 1} (\omega) \widehat {f} _ {x y} (\omega)}{\widehat {f} _ {y y} (\omega)}. \tag {7.27}
$$

We may obtain a distributional result for the multiple coherence function analogous to that obtained in the univariate case by writing the multiple regression model in the frequency domain, as was done in 4.6. We obtain the statistic

$$
F _ {2 q, 2 (L - q)} = \frac {(L - q)}{q} \frac {\widehat {\rho} _ {y \cdot x} ^ {2} (\omega)}{\left[ 1 - \widehat {\rho} _ {y \cdot x} ^ {2} (\omega) \right]}, \tag {7.28}
$$

which has an $F ^ { \prime }$ -distribution with $2 q$ and $2 ( L - q )$ degrees of freedom under the null hypothesis that $\rho _ { y \cdot x } ^ { 2 } ( \omega ) = 0$ , or equivalently, that $B ( \omega ) = 0$ , in the model

$$
Y \left(\omega_ {k} + \ell / n\right) = \boldsymbol {B} ^ {\prime} (\omega) X \left(\omega_ {k} + \ell / n\right) + V \left(\omega_ {k} + \ell / n\right), \tag {7.29}
$$

where the spectral density of the error $V ( \omega _ { k } + \ell / n )$ is $f _ { y \cdot x } ( \omega )$ . Problem 7.5 sketches a derivation of this result.

A second kind of hypothesis of interest is one that might be used to test whether a full model with $q$ inputs is significantly better than some submodel with $q _ { 1 } < q$ components. In the time domain, this hypothesis implies, for a partition of the vector of inputs into $q _ { 1 }$ and $q _ { 2 }$ components ( $q _ { 1 } + q _ { 2 } = q ,$ ), say, $\mathbf { \Delta } \mathbf { x } _ { t } = ( \mathbf { \Delta } \mathbf { x } _ { t 1 } ^ { \prime } , \mathbf { \Delta } \mathbf { x } _ { t 2 } ^ { \prime } ) ^ { \prime }$ , and the similarly partitioned vector of regression functions $\beta _ { t } = ( \beta _ { 1 t } ^ { \prime } , \beta _ { 2 t } ^ { \prime } ) ^ { \prime }$ , we would be interested in testing whether $\beta _ { 2 t } = 0$ in the partitioned regression model

$$
y _ {t} = \sum_ {r = - \infty} ^ {\infty} \beta_ {1 r} ^ {\prime} \boldsymbol {x} _ {t - r, 1} + \sum_ {r = - \infty} ^ {\infty} \beta_ {2 r} ^ {\prime} \boldsymbol {x} _ {t - r, 2} + v _ {t}. \tag {7.30}
$$

Rewriting the regression model (7.30) in the frequency domain in a form that is similar to (7.29) establishes that, under the partitions of the spectral matrix into its $q _ { i } \times q _ { j }$ ( $i , j = 1 , 2$ ) submatrices, say,

$$
\widehat {f} _ {x x} (\omega) = \left( \begin{array}{c c} \widehat {f} _ {1 1} (\omega) & \widehat {f} _ {1 2} (\omega) \\ \widehat {f} _ {2 1} (\omega) & \widehat {f} _ {2 2} (\omega) \end{array} \right), \tag {7.31}
$$

and the cross-spectral vector into its $q _ { i } \times 1$ ( $i = 1 , 2$ ) subvectors,

$$
\widehat {f} _ {x y} (\omega) = \left( \begin{array}{c} \widehat {f} _ {1 y} (\omega) \\ \widehat {f} _ {2 y} (\omega) \end{array} \right), \tag {7.32}
$$

we may test the hypothesis $\boldsymbol { \beta } _ { 2 t } = \mathbf { 0 }$ at frequency $\omega$ by comparing the estimated residual power

$$
\widehat {f} _ {y \cdot x} (\omega) = \widehat {f} _ {y y} (\omega) - \widehat {f} _ {x y} ^ {*} (\omega) \widehat {f} _ {x x} ^ {- 1} (\omega) \widehat {f} _ {x y} (\omega) \tag {7.33}
$$

under the full model with that under the reduced model, given by

$$
\widehat {f} _ {y \cdot 1} (\omega) = \widehat {f} _ {y y} (\omega) - \widehat {f} _ {1 y} ^ {*} (\omega) \widehat {f} _ {1 1} ^ {- 1} (\omega) \widehat {f} _ {1 y} (\omega). \tag {7.34}
$$

The power due to regression can be written as

$$
\operatorname {S S R} (\omega) = L \left[ \widehat {f} _ {y \cdot 1} (\omega) - \widehat {f} _ {y \cdot x} (\omega) \right], \tag {7.35}
$$

Table 7.1 Analysis of Power (ANOPOW) for Testing No Contribution from the Subset $\pmb { x } _ { t 2 }$ in the Partitioned Regression Model   

<table><tr><td>Source</td><td>Power</td><td>Degrees of Freedom</td></tr><tr><td>xt,q1+1, . . . , xt,q1+q2</td><td>SSR(ω) (7.35)</td><td>2q2</td></tr><tr><td>Error</td><td>SSE(ω) (7.36)</td><td>2(L - q1 - q2)</td></tr><tr><td>Total</td><td>Lf_y·1(ω)</td><td>2(L - q1)</td></tr></table>

with the usual error power given by

$$
\operatorname {S S E} (\omega) = L \widehat {f} _ {y \cdot x} (\omega). \tag {7.36}
$$

The test of no regression proceeds using the $F$ -statistic

$$
F _ {2 q _ {2}, 2 (L - q)} = \frac {(L - q)}{q _ {2}} \frac {\operatorname {S S R} (\omega)}{\operatorname {S S E} (\omega)}. \tag {7.37}
$$

The distribution of this $F$ -statistic with $2 q _ { 2 }$ numerator degrees of freedom and $2 ( L - q )$ denominator degrees of freedom follows from an argument paralleling that given in Chapter 4 for the case of a single input. The test results can be summarized in an Analysis of Power (ANOPOW) table that parallels the usual analysis of variance (ANOVA) table. Table 7.1 shows the components of power for testing $\boldsymbol { \beta } _ { 2 t } = \mathbf { 0 }$ at a particular frequency $\omega$ . The ratio of the two components divided by their respective degrees of freedom just yields the $F$ - statistic (7.37) used for testing whether the $q _ { 2 }$ add significantly to the predictive power of the regression on the $q _ { 1 }$ series.

# Example 7.1 Predicting Shasta Lake Inflow

We illustrate some of the preceding ideas by considering the problem of predicting the transformed inflow series shown in Figure 4.33 from some combination of the inputs. First, look for the best single input predictor using the squared coherence function (7.27). The results, exhibited in Figure 7.3, show transformed precipitation produces the most consistently high squared coherence values at all frequencies ( $L = 4 1$ ), with the seasonal frequencies .08, .17, .25, and .33 cycles per month corresponding to 12-month, six-month, four-month, and three-month periods contributing most significantly at the $\alpha = . 0 0 1$ level.

Other inputs, with the exception of wind speed, also appear to be plausible contributors. In order to evaluate the other contributors, we consider partitioned tests with models including each of the other variables and precipitation tested against models including precipitation alone. Figure 7.4 shows a plot of the $F$ -statistic (7.28) as a function of frequency for testing each of the inputs as a possible additional component. We see here some isolated significance points, particularly in the temperature series at some of the higher seasonal components, although the strong

![](images/2868dccb58039f61c8a469e6ea6c07e08c8dac6419d956cd3d053eb1c57a4536.jpg)

![](images/3ea0a141bcb772a2ed849cfe1e625910d0ca4264a1bc49813f8c371f05b14f42.jpg)

![](images/eb59239b504ec28626035e5e6dc23beaf6869ded39b3cd6c31a4ad3d9f8c0dfa.jpg)

![](images/9963e387a670c6925e7ca30999a0fc0d3d8d790996ba29a74f1bee1f91a94a97.jpg)

![](images/4f3e4811e229147904fefd12d7fe0ab64d02c2c235731204ddda0943d9a39e98.jpg)

![](images/1bc6b44aeb385b03d137dc0eb59931352f8c30cfbe4b1b8fd0719682fccc5621.jpg)  
Figure 7.3 Univariate coherence functions relating Shasta Lake inflow to various inputs (frequency scale is cycles per month).

![](images/fe4d6b1b0b964e0ada3c858ec3213ab2581fc76f98e3918925295975250b79e1.jpg)

![](images/087daf7e9497ec1664f4e6d58604d5fc15d7472c024489205861b59d4cd03b6a.jpg)

![](images/6e95833482eaece9873d8f2f1339756c2bd39a175fa6c4db28a0b027afeb6ab1.jpg)  
Figure 7.4 $F$ -statistics for testing whether various inputs combined with precipitation add to the ability to predict Shasta Lake inflow.

![](images/2a79148b7c2c678fcb08b4f5e6655b4846225804f67431012d1fe201836bc43d.jpg)

![](images/f6e53d7d5f7db5c72a4fe76d0a99e6e9cfdb7ff6ef60ad70453877d2ebdcd409.jpg)

![](images/9132b12e714c25f9c6bfd0fc9eaf1502ba42fb41df7723e50b1c0038b0753f3a.jpg)  
Figure 7.5 Multiple coherence between inflow and combined precipitation and temperature along with multiple impulse response functions for the regression relations.

coherence at the 12-month frequency seems to have been essentially eliminated by the incorporation of precipitation.

The additional contribution of temperature to the model seems somewhat marginal because the multiple coherence (7.27), shown in the top panel of Figure 7.5, seems only slightly better than the univariate coherence with precipitation shown in Figure 7.3. It is, however, instructive to produce the multiple regression functions, using (7.26) to see if a simple model for inflow exists that would involve some regression combination of inputs temperature and precipitation that would be useful for predicting inflow to Shasta Lake. With this in mind, denoting the possible inputs $P _ { t }$ for transformed precipitation and $T _ { t }$ for transformed temperature, the regression function has been plotted in the lower two panels of Figure 7.5. The time axes run over both positive and negative values and are centered at time $t = 0$ . Hence, the relation with temperature seems to be instantaneous and positive and an exponentially decaying relation to precipitation exists that has been noticed previously in the analysis in Problem 4.37 of Chapter 4. The plots suggest a transfer function model

of the general form fitted to the Recruitment and SOI series in Example 5.7 of Chapter 5. We might propose fitting the inflow output, say, $I _ { t }$ , using the model

$$
I _ {t} = \alpha_ {0} + \frac {\delta_ {0}}{(1 - \omega_ {1} B)} P _ {t} + \alpha_ {2} T _ {t} + \eta_ {t},
$$

which is the transfer function model, without the temperature component, considered in that section.

# 7.4 Regression with Deterministic Inputs

The previous section considered the case in which the input and output series were jointly stationary, but there are many circumstances where in which we might want to assume that the input functions are fixed and have a known functional form. This case happens in the analysis of data from designed experiments. For example, we may want to take a collection of earthquakes and explosions such as are shown in Figure 7.2 and test whether the mean functions are the same for either the P or S components or, perhaps, for them jointly. In certain other signal detection problems using arrays, the inputs are used as dummy variables to express lags corresponding to the arrival times of the signal at various elements, under a model corresponding to that of a plane wave from a fixed source propagating across the array. In Figure 7.1, we plotted the mean responses of the cortex as a function of various underlying design configurations corresponding to various stimuli applied to awake and mildly anesthetized subjects.

It is necessary to introduce a replicated version of the underlying model to handle even the univariate situation, and we replace (7.8) by

$$
y _ {j t} = \sum_ {r = - \infty} ^ {\infty} \boldsymbol {\beta} _ {r} ^ {\prime} \boldsymbol {z} _ {j, t - r} + v _ {j t} \tag {7.38}
$$

for $j = 1 , 2 , \dots , N$ series, where we assume the vector of known deterministic inputs, $\boldsymbol { z } _ { j t } = ( z _ { j t 1 } , \dots , z _ { j t q } ) ^ { \prime }$ , satisfies

$$
\sum_ {t = - \infty} ^ {\infty} | t | | z _ {j t k} | <   \infty
$$

for $j = 1 , \dots , N$ replicates of an underlying process involving $k = 1 , \ldots , q$ regression functions. The model can also be treated under the assumption that the deterministic function satisfy Grenanders’ conditions, as in Hannan (1970), but we do not need those conditions here and simply follow the approach in Shumway (1983, 1988).

# 7.4: Deterministic Inputs

It will sometimes be convenient in what follows to represent the model in matrix notation, writing (7.38) as

$$
\boldsymbol {y} _ {t} = \sum_ {r = - \infty} ^ {\infty} z _ {t - r} \boldsymbol {\beta} _ {r} + \boldsymbol {v} _ {t}, \tag {7.39}
$$

where $\boldsymbol { z } _ { t } ~ = ~ ( z _ { 1 t } , \ldots , z _ { N t } ) ^ { \prime }$ are the $N \times q$ matrices of independent inputs and $\pmb { y } _ { t }$ and ${ \pmb v } _ { t }$ are the $N \times 1$ output and error vectors. The error vector $\pmb { v } _ { t } = ( v _ { 1 t } , \ldots , v _ { N t } ) ^ { \prime }$ is assumed to be a multivariate, zero-mean, stationary, normal process with spectral matrix $f _ { v } ( \omega ) I _ { N }$ that is proportional to the $N \times N$ identity matrix. That is, we assume the error series $v _ { j t }$ are independently and identically distributed with spectral densities $f _ { v } ( \omega )$ .

# Example 7.2 An Infrasonic Signal from a Nuclear Explosion

Often, we will observe a common signal, say, $\beta _ { t }$ on an array of sensors, with the response at the $j$ -th sensor denoted by $y _ { j t } , j = 1 , \dotsc , N$ For example, Figure 7.6 shows an infrasonic or low-frequency acoustic signal from a nuclear explosion, as observed on a small triangular array of $N \ = \ 3$ acoustic sensors. These signals appear at slightly different times. Because of the way signals propagate, a plane wave signal of this kind, from a given source, traveling at a given velocity, will arrive at elements in the array at predictable time delays. In the case of the infrasonic signal in Figure 7.6, the delays were approximated by computing the cross-correlation between elements and simply reading off the time delay corresponding to the maximum. For a detailed discussion of the statistical analysis of array signals, see Shumway et al. (1999).

A simple additive signal plus noise model of the form

$$
y _ {j t} = \beta_ {t - \tau_ {j}} + v _ {j t} \tag {7.40}
$$

can be assumed, where $\tau _ { j } , j = 1 , 2 , \dots , N$ are the time delays that determine the start point of the signal at each element of the array. The model (7.40) is written in the form (7.38) by letting $z _ { j t } = \delta _ { t - \tau _ { j } }$ , where $\delta _ { t } = 1$ when $t = 0$ and is zero otherwise. In this case, we are interested in both the problem of detecting the presence of the signal and in estimating its waveform $\beta _ { t }$ . In this case, a plausible estimator of the waveform would be the unbiased beam, say,

$$
\widehat {\beta} _ {t} = \frac {\sum_ {j = 1} ^ {N} y _ {j , t + \tau_ {j}}}{N}, \tag {7.41}
$$

where time delays in this case were measured as $\tau _ { 1 } = - 1 7 , \tau _ { 2 } = 0$ , and $\tau _ { 3 } = 2 2$ from the cross-correlation function. The bottom panel of Figure 7.6 shows the computed beam in this case, and the noise in the individual channels has been reduced and the essential characteristics of the common signal are retained in the average.

![](images/b39d3979545591c19211c65a9aa656e3d2da2694ee6dc04fae2e55ed59cf9bc3.jpg)  
Figure 7.6 Three series for a nuclear explosion detonated 25 km south of Christmas Island and the delayed average or beam.

The above discussion and example serve to motivate a more detailed look at the estimation and detection problems in the case in which the input series $z _ { j t }$ are fixed and known. We consider the modifications needed for this case in the following sections.

# Estimation of the Regression Relation

Because the regression model (7.38) involves fixed functions, we may parallel the usual approach using the Gauss–Markov theorem to search for linearfiltered estimators of the form

$$
\widehat {\boldsymbol {\beta}} _ {t} = \sum_ {j = 1} ^ {N} \sum_ {r = - \infty} ^ {\infty} \boldsymbol {h} _ {j r} y _ {j, t - r}, \tag {7.42}
$$

where $\pmb { h } _ { j t } = ( h _ { j t 1 } \ldots , h _ { j t q } ) ^ { \prime }$ is a vector of filter coefficients, determined so the estimators are unbiased and have minimum variance. The equivalent matrix

# 7.4: Deterministic Inputs

form is

$$
\widehat {\boldsymbol {\beta}} _ {t} = \sum_ {r = - \infty} ^ {\infty} h _ {r} \boldsymbol {y} _ {t - r}, \tag {7.43}
$$

where $h _ { t } = ( h _ { 1 t } , . . . , h _ { N t } )$ is a $q \times N$ matrix of filter functions. The matrix form resembles the usual classical regression case and is more convenient for extending the the Gauss–Markov theorem to lagged regression. The unbiased condition is considered in Problem 7.7. It can be shown (see Shumway and Dean, 1968) that $\pmb { h } _ { j s }$ can be taken as the Fourier transform of

$$
\boldsymbol {H} _ {j} (\omega) = S _ {z} ^ {- 1} (\omega) \overline {{\boldsymbol {Z} _ {j} (\omega)}}, \tag {7.44}
$$

where

$$
\mathbf {Z} _ {j} (\omega) = \sum_ {t = - \infty} ^ {\infty} z _ {j t} \mathrm {e} ^ {- 2 \pi i \omega t} \tag {7.45}
$$

is the infinite Fourier transform of $z _ { j t }$ . The matrix

$$
S _ {z} (\omega) = \sum_ {j = 1} ^ {N} \overline {{\mathbf {Z} _ {j} (\omega)}} \mathbf {Z} _ {j} ^ {\prime} (\omega) \tag {7.46}
$$

can be written in the form

$$
S _ {z} (\omega) = Z ^ {*} (\omega) Z (\omega), \tag {7.47}
$$

where the $N \times q$ matrix $Z ( \omega )$ is defined by $Z ( \omega ) = ( Z _ { 1 } ( \omega ) , \ldots , Z _ { N } ( \omega ) ) ^ { \prime }$ . In matrix notation, the Fourier transform of the optimal filter becomes

$$
H (\omega) = S _ {z} ^ {- 1} (\omega) Z ^ {*} (\omega), \tag {7.48}
$$

where $H ( \omega ) = ( H _ { 1 } ( \omega ) , \dots , H _ { N } ( \omega ) )$ is the $q \times N$ matrix of frequency response functions. The optimal filter then becomes the Fourier transform

$$
h _ {t} = \int_ {- 1 / 2} ^ {1 / 2} H (\omega) \mathrm {e} ^ {2 \pi i \omega t} d \omega . \tag {7.49}
$$

If the transform is not tractable to compute, an approximation analogous to (7.26) may be used.

# Example 7.3 Estimation of the Infrasonic Signal in Example 7.2

We consider the problem of producing a best linearly filtered unbiased estimator for the infrasonic signal in Example 7.2. In this case, $q = 1$ and (7.45) becomes

$$
Z _ {j} (\omega) = \sum_ {t = - \infty} ^ {\infty} \delta_ {t - \tau_ {j}} \mathrm {e} ^ {- 2 \pi i \omega t} = \mathrm {e} ^ {- 2 \pi i \omega \tau_ {j}}
$$

and $S _ { z } ( \omega ) = N$ . Hence, we have

$$
H _ {j} (\omega) = \frac {1}{N} \mathrm {e} ^ {2 \pi i \omega \tau_ {j}}.
$$

Using (7.49), we obtain $\begin{array} { r } { h _ { j t } = \frac { 1 } { N } \delta ( t + \tau _ { j } ) } \end{array}$ . Substituting in (7.42), we obtain the best linear unbiased estimator as the beam, computed as in (7.41).

# Tests of Hypotheses

We consider first testing the hypothesis that the complete vector $\beta _ { t }$ is zero, i.e., that the vector signal is absent. We develop a test at each frequency $\omega$ by taking single adjacent frequencies of the form $\omega _ { k } = k / n$ , as in the initial section. We may approximate the DFT of the observed vector in the model (7.38) using a representation of the form

$$
Y _ {j} \left(\omega_ {k}\right) = \boldsymbol {B} ^ {\prime} \left(\omega_ {k}\right) \boldsymbol {Z} _ {j} \left(\omega_ {k}\right) + V _ {j} \left(\omega_ {k}\right) \tag {7.50}
$$

for $j = 1 , \ldots , N$ , where the error terms will be uncorrelated with common variance $f ( \omega _ { k } )$ , the spectral density of the error term. The independent variables $Z _ { j } ( \omega _ { k } )$ can either be the infinite Fourier transform, or they can be approximated by the DFT. Hence, we can obtain the matrix version of a complex regression model, written in the form

$$
\boldsymbol {Y} (\omega_ {k}) = Z (\omega_ {k}) \boldsymbol {B} (\omega_ {k}) + \boldsymbol {V} (\omega_ {k}), \tag {7.51}
$$

where the $N \times q$ matrix $Z ( \omega _ { k } )$ has been defined previously below (7.47) and $Y ( \omega _ { k } )$ and $V ( \omega _ { k } )$ are $N \times 1$ vectors with the error vector $V ( \omega _ { k } )$ having mean zero, with covariance matrix $f ( \omega _ { k } ) I _ { N }$ . The usual regression arguments show that the maximum likelihood estimator for the regression coefficient will be

$$
\widehat {\boldsymbol {B}} (\omega_ {k}) = S _ {z} ^ {- 1} (\omega_ {k}) \boldsymbol {s} _ {z y} (\omega_ {k}), \tag {7.52}
$$

where $S _ { z } ( \omega _ { k } )$ is given by (7.47) and

$$
\begin{array}{l} \boldsymbol {s} _ {z y} (\omega_ {k}) = Z ^ {*} (\omega_ {k}) \boldsymbol {Y} (\omega_ {k}) \\ = \sum_ {j = 1} ^ {N} \overline {{Z _ {j} \left(\omega_ {k}\right)}} Y _ {j} \left(\omega_ {k}\right). \tag {7.53} \\ \end{array}
$$

Also, the maximum likelihood estimator for the error spectral matrix is proportional to

$$
\begin{array}{l} s _ {y \cdot z} ^ {2} (\omega_ {k}) = \sum_ {j = 1} ^ {N} | Y _ {j} (\omega_ {k}) - \widehat {\boldsymbol {B}} (\omega k) ^ {\prime} \boldsymbol {Z} _ {j} (\omega_ {k}) | ^ {2} \\ = \boldsymbol {Y} ^ {*} (\omega_ {k}) \boldsymbol {Y} (\omega_ {k}) - \boldsymbol {Y} ^ {*} (\omega_ {k}) Z (\omega_ {k}) [ Z ^ {*} (\omega_ {k}) Z (\omega_ {k}) ] ^ {- 1} Z ^ {*} (\omega_ {k}) \boldsymbol {Y} (\omega_ {k}) \\ = s _ {y} ^ {2} \left(\omega_ {k}\right) - \boldsymbol {s} _ {z y} ^ {*} \left(\omega_ {k}\right) S _ {z} ^ {- 1} \left(\omega_ {k}\right) \boldsymbol {s} _ {z y} \left(\omega_ {k}\right), \tag {7.54} \\ \end{array}
$$

# 7.4: Deterministic Inputs

Table 7.2 Analysis of Power (ANOPOW) for Testing No Contribution from the Independent Series at Frequency $\omega$ in the Fixed Input Case   

<table><tr><td>Source</td><td>Power</td><td>Degrees of Freedom</td></tr><tr><td>Regression</td><td>SSR(ω)(7.56)</td><td>2Lq</td></tr><tr><td>Error</td><td>SSE(ω)(7.57)</td><td>2L(N-q)</td></tr><tr><td>Total</td><td>SST(ω)</td><td>2LN</td></tr></table>

where

$$
s _ {y} ^ {2} \left(\omega_ {k}\right) = \sum_ {j = 1} ^ {N} \left| Y _ {j} \left(\omega_ {k}\right) \right| ^ {2}. \tag {7.55}
$$

Under the null hypothesis that the regression coefficient $\pmb { { \cal B } } ( \omega _ { k } ) = \mathbf { 0 }$ , the estimator for the error power is just $s _ { y } ^ { 2 } ( \omega _ { k } )$ If smoothing is needed, we may replace the (7.54) and (7.55) by smoothed components over the frequencies $\omega _ { k } + \ell / n$ , for $\ell = - m , \ldots , m$ and $L = 2 m + 1$ , close to $\omega$ . In that case, we obtain the regression and error spectral components as

$$
\operatorname {S S R} (\omega) = \sum_ {\ell = - m} ^ {m} \mathbf {s} _ {z y} ^ {*} (\omega_ {k} + \ell / n) S _ {z} ^ {- 1} (\omega_ {k} + \ell / n) \mathbf {s} _ {z y} (\omega_ {k} + \ell / n) \tag {7.56}
$$

and

$$
\mathrm {S S E} (\omega) = \sum_ {\ell = - m} ^ {m} s _ {y \cdot z} ^ {2} \left(\omega_ {k} + \ell / n\right). \tag {7.57}
$$

The $F$ -statistic for testing no regression relation is

$$
F _ {2 L q, 2 L (N - q)} = \frac {N - q}{q} \frac {\operatorname {S S R} (\omega)}{\operatorname {S S E} (\omega)}. \tag {7.58}
$$

The analysis of power pertaining to this situation appears in Table 7.2.

In the fixed regression case, the partitioned hypothesis that is the analog of $\beta _ { 2 t } = 0$ in (7.28) with $\mathbf { \mathcal { x } } _ { t 1 } , \mathbf { \mathcal { x } } _ { t 2 }$ replaced by $z _ { t 1 } , z _ { t 2 }$ . Here, we partition $S _ { z } ( \omega )$ into $q _ { i } \times q _ { j }$ ( $i , j = 1 , 2$ ) submatrices, say,

$$
S _ {z} \left(\omega_ {k}\right) = \left( \begin{array}{c c} S _ {1 1} \left(\omega_ {k}\right) & S _ {1 2} \left(\omega_ {k}\right) \\ S _ {2 1} \left(\omega_ {k}\right) & S _ {2 2} \left(\omega_ {k}\right) \end{array} \right), \tag {7.59}
$$

and the cross-spectral vector into its $q _ { i } \times 1$ , for $i = 1 , 2$ , subvectors

$$
\boldsymbol {s} _ {z y} \left(\omega_ {k}\right) = \left( \begin{array}{l} \boldsymbol {s} _ {1 y} \left(\omega_ {k}\right) \\ \boldsymbol {s} _ {2 y} \left(\omega_ {k}\right) \end{array} \right). \tag {7.60}
$$

Here, we test the hypothesis $\boldsymbol { \beta } _ { 2 t } = \mathbf { 0 }$ at frequency $\omega$ by comparing the residual power (7.54) under the full model with the residual power under the reduced model, given by

$$
s _ {y \cdot 1} ^ {2} (\omega_ {k}) = s _ {y} ^ {2} (\omega_ {k}) - \boldsymbol {s} _ {1 y} ^ {*} (\omega_ {k}) S _ {1 1} ^ {- 1} (\omega_ {k}) \boldsymbol {s} _ {1 y} (\omega_ {k}). \tag {7.61}
$$

Table 7.3 Analysis of Power (ANOPOW) for Testing No Contribution from the Last $q _ { 2 }$ Inputs in the Fixed Input Case   

<table><tr><td>Source</td><td>Power</td><td>Degrees of Freedom</td></tr><tr><td>Regression</td><td>SSR(ω)(7.62)</td><td>2Lq2</td></tr><tr><td>Error</td><td>SSE(ω)(7.63)</td><td>2L(N-q)</td></tr><tr><td>Total</td><td>SST(ω)</td><td>2L(N-q1)</td></tr></table>

Again, it is desirable to add over adjacent frequencies with roughly comparable spectra so the regression and error power components can be taken as

$$
\operatorname {S S R} (\omega) = \sum_ {\ell = - m} ^ {m} \left[ s _ {y \cdot 1} ^ {2} \left(\omega_ {k} + \ell / n\right) - s _ {y \cdot z} ^ {2} \left(\omega_ {k} + \ell / n\right) \right] \tag {7.62}
$$

and

$$
\mathrm {S S E} (\omega) = \sum_ {\ell = - m} ^ {m} s _ {y \cdot z} ^ {2} \left(\omega_ {k} + \ell / n\right). \tag {7.63}
$$

The information can again be summarized as in Table 7.3, where the ratio of mean power regression and error components leads to the $F$ -statistic

$$
F _ {2 L q _ {2}, 2 L (N - q)} = \frac {(N - q)}{q _ {2}} \frac {\operatorname {S S R} (\omega)}{\operatorname {S S E} (\omega)}. \tag {7.64}
$$

We illustrate the analysis of power procedure using the infrasonic signal detection procedure of Example 7.2.

# Example 7.4 Detecting the Infrasonic Signal Using ANOPOW

We consider the problem of detecting the common signal for the three infrasonic series observing the common signal, as shown in Figure 7.3. The presence of the signal is obvious in the waveforms shown, so the test here mainly confirms the statistical significance and isolates the frequencies containing the strongest signal components. Each series contained $n = 1 0 2 4$ points, sampled at 10 points per second. We use the model in (7.40) so $Z _ { j } ( \omega ) = \mathrm { e } ^ { - 2 \pi i \omega \tau _ { j } }$ and $S _ { z } ( \omega ) = N$ as in Example 7.3, with $s _ { z y } ( \omega _ { k } )$ given as

$$
s _ {z y} (\omega_ {k}) = \sum_ {j = 1} ^ {N} \mathrm {e} ^ {2 \pi i \omega \tau_ {j}} Y _ {j} (\omega_ {k}),
$$

using (7.46) and (7.53). The above expression can be interpreted as being proportional to the weighted mean or beam, computed in frequency, and we introduce the notation

$$
B _ {w} \left(\omega_ {k}\right) = \frac {1}{N} \sum_ {j = 1} ^ {N} \mathrm {e} ^ {2 \pi i \omega \tau_ {j}} Y _ {j} \left(\omega_ {k}\right) \tag {7.65}
$$

![](images/94c8f636c3338c031d06e835ebca69f5b279b7d2686752ac87e4a36c322322cb.jpg)

![](images/b0f8339fc036f10be76428a57ae8acb691a55d200c4350c3ba9c026cc22a4c01.jpg)  
Figure 7.7 Analysis of power for infrasound array (top panel) and $F$ -statistic (bottom panel) showing detection at .033 cy/sec (10 pts/sec).

for that term. Substituting for the power components in Table 7.3 yields

$$
\pmb {\mathscr {s}} _ {z y} ^ {*} (\omega_ {k}) S _ {z} ^ {- 1} (\omega_ {k}) \pmb {\mathscr {s}} _ {z y} (\omega_ {k}) = N | B _ {w} (\omega_ {k}) | ^ {2}
$$

and

$$
\begin{array}{l} s _ {y \cdot z} ^ {2} (\omega_ {k}) = \sum_ {j = 1} ^ {N} | Y _ {j} (\omega_ {k}) - B _ {w} (\omega_ {k}) | ^ {2} \\ = \sum_ {j = 1} ^ {N} | Y _ {j} (\omega_ {k}) | ^ {2} - N | B _ {w} (\omega_ {k}) | ^ {2} \\ \end{array}
$$

for the regression signal and error components, respectively. Because only three elements in the array and a reasonable number of points in time exist, it seems advisable to employ some smoothing over frequency to obtain additional degrees of freedom. In this case, $L = 3$ , yielding $2 ( 3 ) \ : = \ : 6$ and $2 ( 3 ) ( 3 - 1 ) = 1 2$ degrees of freedom for the numerator and denominator of the $F$ -statistic (7.58). Figure 7.7 shows the analysis of power components due to error and the total power. The power is maximum at about .0044 cycles per point or about .044 cycles per second. The $F ^ { \prime }$ -statistic is compared with the $1 \%$ significance level $F _ { . 0 1 } ( 6 , 1 2 ) =$

4.82 in the bottom panel and has the strongest detection at about .034 cycles per second, a result mainly because the error power is decreasing more quickly than the regression or signal power in that band. Little power of consequence appears to exist in the higher range (.3-.5 cycles per second).

Although there are examples of detecting multiple regression functions of the general type considered above (see, for example, Shumway, 1983), we do not consider additional examples of partitioning in the fixed input case here. The reason is that several examples exist in the section on designed experiments that illustrate the partitioned approach.

# 7.5 Random Coefficient Regression

The lagged regression models considered so far have assumed the input process is either stochastic or fixed and the components of the vector of regression function $\beta _ { t }$ are fixed and unknown parameters to be estimated. There are many cases in time series analysis in which it is more natural to regard the regression vector as an unknown stochastic signal. For example, we have studied the state-space model in Chapter 6, where the state equation can be considered as involving a random parameter vector that is essentially a multivariate autoregressive process. In §4.10, we considered estimating the univariate regression function $\beta _ { t }$ as a signal extraction problem.

In this section, we consider a random coefficient regression model of (7.39) in the equivalent form

$$
\boldsymbol {y} _ {t} = \sum_ {r = - \infty} ^ {\infty} z _ {t - r} \boldsymbol {\beta} _ {r} + \boldsymbol {v} _ {t}, \tag {7.66}
$$

where $\pmb { y } _ { t } = ( y _ { 1 t } , \dotsc , y _ { N t } ) ^ { \prime }$ is the $N \times 1$ response vector and $z _ { t } = ( z _ { 1 t } , \ldots , z _ { N t } ) ^ { \prime }$ are the $N \times q$ matrices containing the fixed input processes. Here, the components of the $q \times 1$ regression vector $\beta _ { t }$ are zero-mean, uncorrelated, stationary series with common spectral matrix $f _ { \beta } ( \omega ) I _ { q }$ and the error series ${ \pmb v } _ { t }$ have zeromeans and spectral matrix $f _ { v } ( \omega ) I _ { N }$ , where $I _ { N }$ is the $N \times N$ identity matrix. Then, defining the $N \times q$ matrix $Z ( \omega ) = ( Z _ { 1 } ( \omega ) , Z _ { 2 } ( \omega ) , \ldots , Z _ { N } ( \omega ) ) ^ { \prime }$ of Fourier transforms of $z _ { t }$ , as in (7.45), it is easy to show the spectral matrix of the response vector $\pmb { y } _ { t }$ is given by

$$
f _ {y} (\omega) = f _ {\beta} (\omega) Z (\omega) Z ^ {*} (\omega) + f _ {v} (\omega) I _ {N}. \tag {7.67}
$$

The regression model with a stochastic stationary signal component is a general version of the simple additive noise model

$$
y _ {t} = \beta_ {t} + v _ {t},
$$

considered by Wiener (1949) and Kolmogorov (1941), who derived the minimum mean squared error estimators for $\beta _ { t }$ , as in §4.10. The more general multivariate version (7.66) represents the series as a convolution of the signal vector $\beta _ { t }$ and a known set of vector input series contained in the matrix $z _ { t }$ . Restricting the the covariance matrices of signal and noise to diagonal form is consistent with what is done in statistics using random effects models, which we consider here in a later section. The problem of estimating the regression function $\beta _ { t }$ is often called deconvolution in the engineering and geophysical literature.

# Estimation of the Regression Relation

The regression function $\beta _ { t }$ can be estimated by a general filter of the form (7.43), where we write that estimator in matrix form

$$
\widehat {\boldsymbol {\beta}} _ {t} = \sum_ {r = - \infty} ^ {\infty} h _ {t} \boldsymbol {y} _ {t - r}, \tag {7.68}
$$

where $h _ { t } = ( h _ { 1 t } , \ldots , h _ { N t } )$ , and apply the orthogonality principle, as in §3.9. A generalization of the argument in that section (see Problem 7.8) leads to the estimator

$$
H (\omega) = \left[ S _ {z} (\omega) + \theta (\omega) I _ {q} \right] ^ {- 1} Z ^ {*} (\omega) \tag {7.69}
$$

for the Fourier transform of the minimum mean-squared error filter, where the parameter

$$
\theta (\omega) = \frac {f _ {v} (\omega)}{f _ {\beta} (\omega)} \tag {7.70}
$$

is the inverse of the signal-to-noise ratio. It is clear from the frequency domain version of the linear model (7.51), the comparable version of the estimator (7.52) can be written as

$$
\widehat {\boldsymbol {B}} (\omega) = \left[ S _ {z} (\omega) + \theta (\omega) I _ {q} \right] ^ {- 1} \boldsymbol {s} _ {z y} (\omega). \tag {7.71}
$$

This version exhibits the estimator in the stochastic regressor case as the usual estimator, with a ridge correction, $\theta ( \omega )$ , that is proportional to the inverse of the signal-to-noise ratio.

The mean-squared covariance of the estimator is shown to be

$$
E \left[ (\widehat {\boldsymbol {B}} - \boldsymbol {B}) (\widehat {\boldsymbol {B}} - \boldsymbol {B}) ^ {*} \right] = f _ {v} (\omega) \left[ S _ {z} (\omega) + \theta (\omega) I _ {q} \right] ^ {- 1}, \tag {7.72}
$$

which again exhibits the close connection between this case and the variance of the estimator (7.52), which can be shown to be $f _ { v } ( \omega ) S _ { z } ^ { - 1 } ( \omega )$ .

# Example 7.5 Estimating the Random Infrasonic Signal

In Example 7.4, we have already determined the components needed in (7.69) and (7.70) to obtain the estimators for the random signal. The Fourier transform of the optimum filter at series $j$ has the form

$$
H _ {j} (\omega) = \frac {e ^ {2 \pi i \omega \tau_ {j}}}{N + \theta (\omega)} \tag {7.73}
$$

with the mean-squared error given by $f _ { v } ( \omega ) / [ N + \theta ( \omega ) ]$ from (7.72). The net effect of applying the filters will be the same as filtering the beam with the frequency response function

$$
\begin{array}{l} H _ {0} (\omega) = \frac {N}{N + \theta (\omega)} \\ = \frac {N f _ {\beta} (\omega)}{f _ {v} (\omega) + N f _ {\beta} (\omega)}, \tag {7.74} \\ \end{array}
$$

where the last form is more convenient in cases in which portions of the signal spectrum are essentially zero.

The optimal filters $h _ { t }$ have frequency response functions that depend on the signal spectrum $f _ { \beta } ( \omega )$ and noise spectrum $f _ { v } ( \omega )$ , so we will need estimators for these parameters to apply the optimal filters. Sometimes, there will be values, suggested from experience, for the signal-to-noise ratio $1 / \theta ( \omega )$ as a function of frequency. The analogy between the model here and the usual variance components model in statistics, however, suggests we try an approach along those lines as in the next section.

# Detection and Parameter Estimation

The analogy to the usual variance components situation suggests looking at the regression and error components of Table 7.2 under the stochastic signal assumptions. We consider the components of (7.56) and (7.57) at a single frequency $\omega _ { k }$ . In order to estimate the spectral components $f _ { \beta } ( \omega )$ and $f _ { v } ( \omega )$ , we reconsider the linear model (7.51) under the assumption that $B ( \omega _ { k } )$ is a random process with spectral matrix $f _ { \beta } ( \omega _ { k } ) I _ { q }$ . Then, the spectral matrix of the observed process is (7.67), evaluated at frequency $\omega _ { k }$ .

Consider first the component of the regression power, defined as

$$
\begin{array}{l} S S R (\omega_ {k}) = \boldsymbol {s} _ {z y} ^ {*} (\omega_ {k}) S _ {z} ^ {- 1} (\omega_ {k}) \boldsymbol {s} _ {z y} (\omega_ {k}) \\ = \mathbf {Y} ^ {*} (\omega_ {k}) Z (\omega_ {k}) S _ {z} ^ {- 1} (\omega_ {k}) Z ^ {*} (\omega_ {k}) \mathbf {Y} (\omega_ {k}). \\ \end{array}
$$

A computation shows

$$
E [ S S R (\omega_ {k}) ] = f _ {\beta} (\omega_ {k}) \operatorname {t r} \left\{S _ {z} (\omega_ {k}) \right\} + q f _ {v} (\omega_ {k}),
$$

# 7.5: Random Coefficients

where tr denotes the trace of a matix. If we can find a set of frequencies of the form $\omega _ { k } + \ell / n$ , where the spectra and the Fourier transforms $S _ { z } ( \omega _ { k } + \ell / n ) \approx$ $S _ { z } ( \omega )$ are relatively constant, the expectation of the averaged values in (7.56) yields

$$
E [ S S R (\omega) ] = L f _ {\beta} (\omega) \operatorname {t r} \left[ S _ {z} (\omega) \right] + L q f _ {v} (\omega). \tag {7.75}
$$

A similar computation establishes

$$
E [ S S E (\omega) ] = L (N - q) f _ {v} (\omega). \tag {7.76}
$$

We may obtain an approximately unbiased estimator for the spectra $f _ { v } ( \omega )$ and $f _ { \beta } ( \omega )$ by replacing the expected power components by their values and solving (7.75) and (7.76).

# Example 7.6 Estimating the Power Components and the Random Infrasonic Signal

In order to provide an optimum estimator for the infrasonic signal, we need to have estimators for the signal and noise spectra $f _ { \beta } ( \omega )$ and $f _ { v } ( \omega )$ for the case considered in Example 7.5. The form of the filter is $H _ { 0 } ( \omega )$ , given in (7.74), and with $q = 1$ and the matrix $S _ { z } ( \omega ) = N$ at all frequencies in this example simplifies the computations considerably. We may estimate

$$
\widehat {f} _ {v} (\omega) = \frac {S S E (\omega)}{L (N - 1)} \tag {7.77}
$$

and

$$
\widehat {f} _ {\beta} (\omega) = (L N) ^ {- 1} \left(S S R (\omega) - \frac {S S E (\omega)}{(N - 1)}\right), \tag {7.78}
$$

using (7.75) and (7.76) for this special case. Cases will exist in which (7.78) is negative and the estimated signal spectrum can be set to zero for those frequencies. The estimators can be substituted into the optimal filters to apply to the beam, say, $H _ { 0 } ( \omega )$ in (7.74), or to use in the filter applied to each level (7.73).

The analysis of variance estimators can be computed using the analysis of power given in Figure 7.7, and the results of that computation and applying (7.77) and (7.78) are shown in the top panel of Figure 7.8 for a bandwidth of B = 7/2048 = cycles per point or about .03 cycles per second (Hz). Neither spectrum contains any significant power for frequencies greater than .04 cycles per point or about .4 Hz. As expected, the signal spectral estimator is substantial over a narrow band, and this leads to an estimated filter, with estimated frequency response function $\hat { H } _ { 0 } ( \omega )$ , shown on the left-hand side of the second panel. The estimated optimal filter essentially deletes frequencies above .014 Hz and, subject to slight modification, differs little from a standard low-pass filter with

![](images/9e4ee8152dbeebd0005db63eb4562608b1f6c6ef3d85fdb5ea86da6aaa10f8c0.jpg)

![](images/00fee2f3aded0f8426b25453388597c04fc0782d7b471e4ce4a548b830da84bb.jpg)

![](images/be425d3908678d63922480f214db5338ca5cb99c4f65ef67b9563eea6671982a.jpg)

![](images/6230271b4a37fe25663ed1f2a554e699926be9a9377c87a8f40f58fea92621e7.jpg)

![](images/f38b488c4b9862f4d4015a20c441e077751e9fad04576a1f971174fd4d4cf5af.jpg)

![](images/72798eee175d7460ebc83f0ec43299b42447815399326263845001692af4244c.jpg)  
Figure 7.8 Estimated signal and noise spectra, filter responses, and beams.

that cutoff. Computing the time version with a cutoff at $M = 2 0 1$ points and using a taper leads to the estimated impulse response function $\widehat { h } _ { 0 } ( t )$ , as shown on the right-hand side of the middle panel. Finally, we apply the optimal filter to the beam and get the filtered beam ${ \widehat { \beta } } _ { t }$ shown in the bottom right-hand panel. It is smoother than the left-hand bottom panel, where we have reproduced the beam shown earlier in Figure 7.3. The analysis shows the primary signal as basically a low-frequency signal with primary power at about .05 Hz or, essentially, a wave with a 20-second period.

# 7.6 Analysis of Designed Experiments

An important special case (see Brillinger, 1973, 1980) of the regression model (7.50) occurs when the regression (7.39) is of the form

$$
\boldsymbol {y} _ {t} = z \boldsymbol {\beta} _ {t} + \boldsymbol {v} _ {t}, \tag {7.79}
$$

where ${ \boldsymbol { z } } = ( z _ { 1 } , z _ { 2 } , \dots , z _ { N } ) ^ { \prime }$ is a matrix that determines what is observed by the $j$ -th series; i.e.,

$$
y _ {j t} = \boldsymbol {z} _ {j} ^ {\prime} \boldsymbol {\beta} _ {t} + v _ {j t}. \tag {7.80}
$$

# 7.6: ANOPOW

In this case, the the matrix $z$ of independent variables is constant and we will have the frequency domain model.

$$
\boldsymbol {Y} (\omega_ {k}) = Z \boldsymbol {B} (\omega_ {k}) + \boldsymbol {V} (\omega_ {k}) \tag {7.81}
$$

corresponding to (7.51), where the matrix $Z ( \omega _ { k } )$ was a function of frequency $\omega _ { k }$ . The matrix is purely real, in this case, but the equations (7.52)-(7.58) can be applied with $Z ( \omega _ { k } )$ replaced by the constant matrix $Z$ .

# Equality of Means

A typical general problem that we encounter in analyzing real data is a simple equality of means test in which there might be a collection of time series $y _ { i j t }$ , $i = 1 , \ldots , I$ , $j = 1 , \ldots , N _ { i }$ , belonging to $I$ possible groups, with $N _ { i }$ series in group $i$ . To test equality of means, we may write the regression model in the form

$$
y _ {i j t} = \mu_ {t} + \alpha_ {i t} + v _ {i j t}, \tag {7.82}
$$

where $\mu _ { t }$ denotes the overall mean and $\alpha _ { i t }$ denotes the effect of the $i$ -th group at time $t$ and we require that $\textstyle \sum _ { i } \alpha _ { i t } = 0$ for all $t$ . In this case, the full model can be written in the general regression notation as

$$
y _ {i j t} = z _ {i j} ^ {\prime} \beta_ {t} + v _ {i j t}
$$

where

$$
\boldsymbol {\beta} _ {t} = \left(\mu_ {t}, \alpha_ {1 t}, \alpha_ {2 t}, \dots , \alpha_ {I - 1, t}\right) ^ {\prime}
$$

denotes the regression vector, subject to the constraint. The reduced model becomes

$$
y _ {i j t} = \mu_ {t} + v _ {i j t} \tag {7.83}
$$

under the assumption that the group means are equal. In the full model, there are $I$ possible values for the $I \times 1$ design vectors $z _ { i j }$ ; the first component is always one for the mean, and the rest have a one in the $i$ -th position for $i = 1 , \ldots , I - 1$ and zeros elsewhere. The vectors for the last group take the value $^ { - 1 }$ for $i = 2 , 3 , \dots , I - 1$ . Under the reduced model, each $z _ { i j }$ is a single column of ones. The rest of the analysis follows the approach summarized in (7.52)-(7.58). In this particular case, the power components in Table 7.3 (before smoothing) simplify to

$$
S S R \left(\omega_ {k}\right) = \sum_ {i = 1} ^ {I} \sum_ {j = 1} ^ {N _ {i}} \left| Y _ {i}. \left(\omega_ {k}\right) - Y. \left(\omega_ {k}\right) \right| ^ {2} \tag {7.84}
$$

and

$$
S S E \left(\omega_ {k}\right) = \sum_ {i = 1} ^ {I} \sum_ {j = 1} ^ {N _ {i}} \left| Y _ {i j} \left(\omega_ {k}\right) - Y _ {i}. \left(\omega_ {k}\right) \right| ^ {2}, \tag {7.85}
$$

which are analogous to the usual sums of squares in analysis of variance. Note that a dot (·) stands for a mean, taken over the appropriate subscript, so the

regression power component $S S R ( \omega _ { k } )$ is basically the power in the residuals of the group means from the overall mean and the error power component $S S E ( \omega _ { k } )$ reflects the departures of the group means from the original data values. Smoothing each component over $L$ frequencies leads to the usual $F$ - statistic (7.64) with $2 L ( I - 1 )$ and $2 L ( \textstyle \sum _ { i } N _ { i } - I )$ degrees of freedom at each frequency $\omega$ of interest.

# Example 7.7 Means Test for the Magnetic Resonance Imaging Data

Figure 7.1 showed the mean responses of subjects to various levels of periodic stimulation while awake and while under anesthesia, as collected in a pain perception experiment of Antognini et al. (1997). Three types of periodic stimuli were presented to awake and anesthetized subjects, namely, brushing, heat, and shock. The periodicity was introduced by applying the stimuli, brushing, heat, and shocks in on-off sequences lasting 32 seconds each and the sampling rate was one point every two seconds. The blood oxygenation level (BOLD) signal intensity (Ogawa et al., 1990) was measured at nine locations in the brain. Areas of activation were determined using a technique first described by Bandettini et al. (1993). The specfic locations of the brain where the signal was measured were Cortex 1: Primary Somatosensory, Contralateral, Cortex 2: Primary Somatosensory, Ipsilateral, Cortex 3: Secondary Somatosensory, Contralateral, Cortex 4: Secondary Somatosensory, Ipsilateral, Caudate, Thalamus 1: Contralateral, Thalamus 2: Ipsilateral, Cerebellum 1: Contralateral and Cerebellum 2: Ipsilateral. Figure 7.1 shows the mean response of subjects at Cortex 1 for each of the six treatment combinations, 1: Awake-Brush (5 subjects), 2: Awake-Heat (4 subjects), 3: Awake-Shock (5 subjects), 4: Low-Brush (3 subjects), 5: Low-Heat (5 subjects), and 6: Low-Shock( 4 subjects). The objective of this first analysis is to test equality of these six group means, paying special attention to the 64-second period band (1/64 cycles per second) expected from the periodic driving stimuli. Because a test of equality is needed at each of the nine brain locations, we took $\alpha = . 0 0 1$ to control for the overall error rate. Figure 7.9 shows $F$ -statistics, computed from (7.64), with $L = 3$ , and we see substantial signals for the four cortex locations and for the second cerebellum trace, but the effects are nonsignificant in the caudate and thalamus regions. Hence, we will retain the four cortex locations and the second cerebellum location for further analysis.

# An Analysis of Variance Model

The arrangement of treatments for the fMRI data in Figure 7.1 suggests more information might be available than was obtained from the simple equality of means test. Separate effects caused by state of consciousness as well as the

![](images/67c25e05c23cf668c65c80776bb3414ed2cff3a075eee83d69b2899c58513d7b.jpg)

![](images/72ed2f1a16b9873c456f7ec482532243ec12a720f2018b457f6105d8f87b1620.jpg)

![](images/e0e6c182a72b8459595ab0276648b405005e6a2ae430b84e642b06cca7d58b1c.jpg)

![](images/05dfa761480b8124f4a7c683e6f7e888eb65d24db77a58e950a715d02904952a.jpg)

![](images/ff40449dd3941bd2812a45c354556d7099bdab85e27ee6650229c23c5e1e75ec.jpg)

![](images/64645cb948b573fbcdb4cd5f8317cf06ac9adf5f1fb5073c3efc926f65f17948.jpg)

![](images/165be50c0ebf479176c03d06266f2b67544b1f1d54485e450669333b2dbd30ca.jpg)

![](images/824bcfab491b8a6db2d4aa51e656d121dcb034f284f0a6315612f88ab2d36203.jpg)

![](images/bf78d5ef8ac9efca9de1e0fcda59d76b52d171b86f45677b7f22e8e7f388602a.jpg)  
Figure 7.9 Frequency-dependent equality of means tests for fMRI data at 9 brain locations. $L = 3$ and critical value $F _ { \mathrm { . 0 0 1 } } ( 3 0 , 1 2 0 ) = 2 . 2 6$ .

separate treatments brush, heat, and shock might exist. The reduced signal present in the low shock mean suggests a possible interaction between the treatments and level of consciousness. The arrangement in the classical twoway table suggests looking at the analog of the two factor analysis of variance as a function of frequency. In this case, we would obtain a different version of the regression model (7.82) of the form

$$
y _ {i j k t} = \mu_ {t} + \alpha_ {i t} + \beta_ {j t} + \gamma_ {i j t} + v _ {i j k t} \tag {7.86}
$$

for $k$ -th individual undergoing the $i$ -th level of some factor A and the $j$ -th level of some other factor B, $i = 1 , \ldots I , j = 1 \ldots , J , k = 1 , \ldots n _ { i j }$ . The number of individuals in each cell can be different, as for the fMRI data in the next example. In the above model, we assume the response can be modeled as the sum of a mean, $\mu _ { t }$ , a row effect (type of stimulus), $\alpha _ { i t }$ , a column effect (level

Table 7.4 Rows of the Design Matrix $z _ { j } ^ { \prime }$ for fMRI Data. Number of Observations per Cell in Parentheses   

<table><tr><td></td><td colspan="6">Awake</td><td colspan="5">Low Anesthesia</td><td></td></tr><tr><td>Brush</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0 (5)</td><td>1</td><td>1</td><td>0</td><td>-1</td><td>-1</td><td>0 (3)</td></tr><tr><td>Heat</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1 (4)</td><td>1</td><td>0</td><td>1</td><td>-1</td><td>0</td><td>-1 (5)</td></tr><tr><td>Shock</td><td>1</td><td>-1</td><td>-1</td><td>1</td><td>-1</td><td>-1 (5)</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1 (4)</td></tr></table>

of consciousness), $\beta _ { j t }$ and an interaction, $\gamma _ { i j t }$ , with the usual restrictions

$$
\sum_ {i} \alpha_ {i t} = \sum_ {j} \beta_ {j t} = \sum_ {i} \gamma_ {i j t} = \sum_ {j} \gamma_ {i j t} = 0
$$

required for a full rank design matrix $Z$ in the overall regression model (7.81). If the number of observations in each cell were the same, the usual simple analogous version of the power components (7.84) and (7.85) would exist for testing various hypotheses. In the case of (7.86), we are interested in testing hypotheses obtained by dropping one set of terms at a time out of (7.86), so an A factor (testing $\alpha _ { i t } = 0$ ), a B factor ( $\beta _ { j t } = 0$ ), and an interaction term ( $\gamma _ { i j t } = 0$ ) will appear as components in the analysis of power. Because of the unequal numbers of observations in each cell, we often put the model in the form of the regression model (7.79)-(7.81).

# Example 7.8 Analysis of Power Tests for the Magnetic Resonance Imaging Data

For the fMRI data given as the means in Figure 7.1, a model of the form (7.86) is plausible and will yield more detailed information than the simple equality of means test described earlier. The results of that test, shown in Figure 7.9, were that the means were different for the four cortex locations and for the second cerebellum location. We may examine these differences further by testing whether the mean differences are because of the nature of the stimulus or the consciousness level, or perhaps due to an interaction between the two factors. Unequal numbers of observations exist in the cells that contributed the means in Figure 7.1. For the regression vector,

$$
\left(\mu_ {t}, \alpha_ {1 t}, \beta_ {1 t}, \beta_ {2 t}, \gamma_ {1 1 t}, \gamma_ {2 1 t}\right) ^ {\prime},
$$

the rows of the design matrix are as specified in Table 7.4. Note the restrictions given above for the parameters.

The results of testing the three hypotheses are shown in Figure 7.10 for the four cortex locations and the cerebellum, the components that showed some significant differences in the means in Figure 7.9. Again, the regression power components were smoothed over $L = 3$ frequencies.

![](images/039d9e0dc4e9cfefe4b4f01a94df0654b63b7e9d22af0e99176354dc9440b059.jpg)

![](images/2d63ef1e282b8b929843d4aaced6a00d736af122f7a739ab069f8657b31a9801.jpg)

![](images/2a541c444bb1eec6e1fe061ec1be9df13986eee65fff3987b5341c6987f0b7d1.jpg)

![](images/a379b3eac6b10e2b9e5a63acada8b47f57b54ba2a340aab124771539c5ecd877.jpg)

![](images/c7fb2e2f4e90727fbf493cb72df4ff13a81dd70fdab5c8a9062f836ac6b18a00.jpg)

![](images/f0a17b95aecdcb4a49f15beca23335cb41f9f034a482c37a93abf27f78666819.jpg)

![](images/0a42d5a2552e87937fff4fa9c19b1952de5fcad44d009949868d91618c15fb55.jpg)

![](images/0df5d46348b623d9eb8e066270af8b87534515043c19a95511405fd4fac26c8f.jpg)

![](images/c11623b9fd4e89c34fac74ae634f1a6d959f7c9ba8cc3055be860851a887b533.jpg)

![](images/aa963a6e7ab5270d14b9f6bff074914d8ec85260396649fbb71d28425bfe6100.jpg)

![](images/bf3b989c9545ed4e2258ea805a6ae2586cf6a7708d7a988fa1165a371f753758.jpg)

![](images/7dd9bfa30aceb294664b73840e2ae915199a7e7e7eabd048133990c6f64a7217.jpg)

![](images/938e31cc6f7ccd795dde8d8a6c5892057d2051491b060ebc94da8a1d3d015662.jpg)

![](images/9488697d25e84f820ee9b3edd0dfc4ec9ec60b72919769f0b4e93975b2eecf59.jpg)

![](images/c949ac4f27b4d124f3dcd4091ab5a8c7652e7bd9c434631d56b576b301ad4aa3.jpg)  
Figure 7.10 Analysis of power for fMRI data at five locations, $L = 3$ and critical values $F _ { . 0 0 1 } ( 6 , 1 2 0 ) = 4 . 0 4 $ for stimulus and $F _ { . 0 0 1 } ( 1 2 , 1 2 0 ) = 3 . 0 2$ for consciousness and interaction.

Appealing to the ANOPOW results summarized in Table 7.3 for each of the subhypotheses, $q _ { 2 } = 1$ when the stimulus effect is dropped, and $q _ { 2 } = 2$ when either the conciousness effect or the interaction terms are dropped. Hence, $2 L q _ { 2 } = 6 , 1 2$ for the two cases, with $\begin{array} { r } { N = \sum _ { i j } n _ { i j } = 2 6 } \end{array}$ total observations. Here, the form of the stimulus has the major effect, with the brushing, heat, and shock means substantially different at the

probe frequency in four out of five cases. The level of consciousness was less significant and did not show the strong component at the signal frequency. A significant interaction occurred, however, at the ipsilateral component of the primary somatosensory cortex location. The more detailed model does separate the stimuli as having the major effect, but does not isolate which of the three might be more substantial than the other two.

# Simultaneous Inference

In the previous examples involving the fMRI data, it would be helpful to focus on the components that contributed most to the rejection of the equal means hypothesis. One way to accomplish this is to develop a test for the significance of an arbitrary linear compound of the form

$$
\Psi \left(\omega_ {k}\right) = \boldsymbol {A} ^ {*} \left(\omega_ {k}\right) \boldsymbol {B} \left(\omega_ {k}\right), \tag {7.87}
$$

where the components of the vector $\pmb { A } ( \omega _ { k } ) = ( A _ { 1 } ( \omega _ { k } ) , A _ { 2 } ( \omega _ { k } ) , \ldots , A _ { q } ( \omega _ { k } ) ) ^ { \prime }$ are chosen in such a way as to isolate particular linear functions of parameters in the regression vector $B ( \omega _ { k } )$ in the regression model (7.81). This argument suggests developing a test of the hypothesis $\Psi ( \omega _ { k } ) = 0$ for all possible values of the linear coefficients in the compound (7.87) as is done in the conventional analysis of variance approach (see, for example, Scheff´e, 1959).

Recalling the material involving the regression models of the form (7.51), the linear compound (7.87) can be estimated by

$$
\widehat {\Psi} \left(\omega_ {k}\right) = \boldsymbol {A} ^ {*} \left(\omega_ {k}\right) \widehat {\boldsymbol {B}} \left(\omega_ {k}\right), \tag {7.88}
$$

where $\widehat { \bf B } ( \omega _ { k } )$ is the estimated vector of regression coefficients given by (7.52) and independent of the error spectrum $s _ { y \cdot z } ^ { 2 } ( \omega _ { k } )$ in (7.54). It is possible to show the maximum of the ratio

$$
F (\boldsymbol {A}) = \frac {N - q}{q} \frac {\left| \widehat {\Psi} \left(\omega_ {k}\right) - \Psi \left(\omega_ {k}\right) \right| ^ {2}}{s _ {y \cdot z} ^ {2} \left(\omega_ {k}\right) Q (\boldsymbol {A})}, \tag {7.89}
$$

where

$$
Q (\boldsymbol {A}) = \boldsymbol {A} ^ {*} (\omega_ {k}) S _ {z} ^ {- 1} (\omega_ {k}) \boldsymbol {A} (\omega_ {k}) \tag {7.90}
$$

is bounded by a statistic that has an $F$ -distribution with $2 q$ and $2 ( N - q )$ degrees of freedom. Testing the hypothesis that the compound has a particular value, usually $\Psi ( \omega _ { k } ) = 0$ , then proceeds naturally, by comparing the statistic (7.89) evaluated at the hypothesized value with the $\alpha$ level point on an $F _ { 2 q , 2 ( N - q ) }$ distribution. We can choose an infinite number of compounds of the form (7.87) and the test will still be valid at level $\alpha$ . As before, arguing the error spectrum is relatively constant over a band enables us to smooth the numerator and denominator of (7.89) separately over $L$ frequencies so distribution involving the smooth components is $F _ { 2 L q , 2 L ( N - q ) }$ .

# Example 7.9 Simultaneous Inference for Magnetic Resonance Imaging Data

As an example, consider the previous tests for significance of the fMRI factors, in which we have indicated the primary effects are among the stimuli but have not investigated which of the stimuli, heat, brushing, or shock, had the most effect. To analyze this further, consider the means model (7.82) and a $6 \times 1$ contrast vector of the form

$$
\widehat {\Psi} = \boldsymbol {A} ^ {*} (\omega_ {k}) \widehat {\boldsymbol {B}} (\omega_ {k}) = \sum_ {i = 1} ^ {6} A _ {i} ^ {*} (\omega_ {k}) \boldsymbol {Y} _ {i}. (\omega_ {k}), \tag {7.91}
$$

where the means are easily shown to be the regression coefficients in this particular case. In this case, the means are ordered by columns; the first three means are the the three levels of stimuli for the awake state, and the last three means are the levels for the anesthetized state. In this special case, the denominator terms are

$$
Q = \sum_ {i = 1} ^ {6} \frac {\left| A _ {i} \left(\omega_ {k}\right) \right| ^ {2}}{N _ {i}}, \tag {7.92}
$$

with $S S E ( \omega _ { k } )$ available in (7.85). In order to evaluate the effect of a particular stimulus, like brushing over the two levels of consciousness, we may take $A _ { 1 } ( \omega _ { k } ) = A _ { 4 } ( \omega _ { k } ) = 1$ for the two brush levels and $A ( \omega _ { k } ) = 0$ zero otherwise. From Figure 7.11, we see that, at the first and third cortex locations, brush and heat are both significant, whereas the fourth cortex shows only brush and the second cerebellum shows only heat. Shock appears to be transmitted relatively weakly, when averaged over the awake and mildly anesthetized states.

# Multivariate Tests

Although it is possible to develop multivariate regression along lines analogous to the usual real valued case, we will only look at tests involving equality of group means and spectral matrices, because these tests appear to be used most often in applications. For these results, consider the $p$ -variate time series $\pmb { y } _ { i j t } = ( y _ { i j t 1 } , \dots , y _ { i j t p } ) ^ { \prime }$ to have arisen from observations on $j = 1 , \ldots , N _ { i }$ individuals in group $i$ , all having mean $\pmb { \mu } _ { i t }$ and stationary autocovariance matrix $\Gamma _ { i } ( h )$ . Denote the DFTs of the group mean vectors as $Y _ { i \cdot } ( \omega _ { k } )$ and the $p \times p$ spectral matrices as $\widehat { f _ { i } } ( \omega _ { k } )$ for the $i = 1 , 2 , \dots , I$ groups. Assume the same general properties as for the vector series considered in §7.3.

In the multivariate case, we obtain the analogous versions of (7.84) and (7.85) as the between cross-power and within cross-power matrices

$$
S P R \left(\omega_ {k}\right) = \sum_ {i = 1} ^ {I} \sum_ {j = 1} ^ {N _ {i}} \left(\mathbf {Y} _ {i.} \left(\omega_ {k}\right) - \mathbf {Y}. \left(\omega_ {k}\right)\right) \left(\mathbf {Y} _ {i.} \left(\omega_ {k}\right) - \mathbf {Y}. \left(\omega_ {k}\right)\right) ^ {*} \tag {7.93}
$$

![](images/f15629b22fc290c657eb1ad6948f8f7556a77f4d662cca9df68072a914001aa1.jpg)

![](images/df7d15102d10399738252740fe3541a8ba49368b8c384d27e0d1a8512ef88172.jpg)

![](images/a9ed980608954907ccb7f180f1e1d404fb39692ed4a6d426e9847a5326d9549f.jpg)

![](images/bf78e5097a5f9f7a9b4d6a86b4b363e3737af16c63287f7ebfe43d4ba01ceff4.jpg)

![](images/cb1553acb8bbed85d464b6cd9ce1129fe3f1216e9d1f4f533ac58994e06a5ecd.jpg)

![](images/e7a6be1bddc0269edc729f3198758c4bf964393303a372ac7c9e42a878b7926b.jpg)

![](images/331eaee404b819d8a1b236b1eed5eddac4bdca59ecff18f26c7d802f0b13643f.jpg)

![](images/9d165276e95533e99b2cac059ab4b11cf34bfcf1b891c99aab12c59a8f25ee0c.jpg)

![](images/4f333a2afc8e0069d3c8df758420ac95842537f417e9297108286d0019928547.jpg)

![](images/482a9e23acf964800fb2c6c309938ba51a35a1a01e23e13e611642d270c0888f.jpg)

![](images/fa765a31f73e3d18f2c423d7667cbf1df4d0c334b2b6ab29955befda03bf453a.jpg)

![](images/45e1eaac2c659bd08b86e4caed47e4ffb961c27f6c5ccaeadf4ea683e674d636.jpg)

![](images/966993eda8e56bd87b8f5ff33bf1408b02fe798eb3c8cf8a8930dae41d8b2f0a.jpg)

![](images/7b611ac6c556937a0a36c8d72c93f3f44367b34676eac3b959ad3784abb01f8d.jpg)

![](images/7c0bae3ea447c302b9a089618969d00ee3de129a783ff66947944e542769912c.jpg)  
Figure 7.11 Power in simultaneous linear compounds at five locations, enhancing brush, heat, and shock effects, $L = 3 , F _ { . 0 0 1 } ( 3 6 , 1 2 0 ) = 1 . 8 0$ .

and

$$
S P E \left(\omega_ {k}\right) = \sum_ {i = 1} ^ {I} \sum_ {j = 1} ^ {N _ {i}} \left(\mathbf {Y} _ {i j} \left(\omega_ {k}\right) - \mathbf {Y} _ {i \cdot} \left(\omega_ {k}\right)\right) \left(\mathbf {Y} _ {i j} \left(\omega_ {k}\right) - \mathbf {Y} _ {i \cdot} \left(\omega_ {k}\right)\right) ^ {*}. \tag {7.94}
$$

The equality of means test is rejected using the fact that the likelihood ratio

# 7.6: ANOPOW

test yields a monotone function of

$$
\Lambda \left(\omega_ {k}\right) = \frac {\left| S P E \left(\omega_ {k}\right) \right|}{\left| S P E \left(\omega_ {k}\right) + S P R \left(\omega_ {k}\right) \right|}. \tag {7.95}
$$

Khatri (1965) and Hannan (1970) give the approximate distribution of the statistic

$$
\chi_ {2 (I - 1) p} ^ {2} = - 2 \left(\sum N _ {i} - I - p - 1\right) \log \Lambda \left(\omega_ {k}\right) \tag {7.96}
$$

as chi-squared with $2 ( I - 1 ) p$ degrees of freedom when the group means are equal.

The case of $I = 2$ groups reduces to Hotelling’s $T ^ { 2 }$ , as has been shown by Giri (1965), where

$$
T ^ {2} = \frac {N _ {1} N _ {2}}{\left(N _ {1} + N _ {2}\right)} \left[ \boldsymbol {Y} _ {1.} (\omega_ {k}) - \boldsymbol {Y} _ {2.} (\omega_ {k}) \right] ^ {*} \widehat {f} _ {v} ^ {- 1} (\omega_ {k}) \left[ \boldsymbol {Y} _ {1.} (\omega_ {k}) - \boldsymbol {Y} _ {2.} (\omega_ {k}) \right], \tag {7.97}
$$

where

$$
\widehat {f} _ {v} \left(\omega_ {k}\right) = \frac {S P E \left(\omega_ {k}\right)}{\sum_ {i} N _ {i} - I} \tag {7.98}
$$

is the pooled error spectrum given in (7.94),with $I = 2$ . The test statistic, in this case, is

$$
F _ {2 p, 2 \left(N _ {1} + N _ {2} - p - 1\right)} = \frac {\left(N _ {1} + N _ {2} - 2\right) p}{\left(N _ {1} + N _ {2} - p - 1\right)} T ^ {2}, \tag {7.99}
$$

which was shown by Giri (1965) to have the indicated limiting $F$ -distribution with $2 p$ and $2 ( N _ { 1 } + N _ { 2 } - p - 1 )$ degrees of freedom when the means are the same. The classical $t$ -test for inequality of two univariate means will be just (7.98) and (7.99) with $p = 1$ .

Testing equality of the spectral matrices is also of interest, not only for discrimination and pattern recognition, as considered in the next section, but also as a test indicating whether the equality of means test, which assumes equal spectral matrices, is valid. The test evolves from the likelihood ration criterion, which compares the single group spectral matrices

$$
\widehat {f} _ {i} \left(\omega_ {k}\right) = \frac {1}{N _ {i} - 1} \sum_ {j = 1} ^ {N _ {i}} \left(\mathbf {Y} _ {i j} \left(\omega_ {k}\right) - \mathbf {Y} _ {i.} \left(\omega_ {k}\right)\right) \left(\mathbf {Y} _ {i j} \left(\omega_ {k}\right) - \mathbf {Y} _ {i.} \left(\omega_ {k}\right)\right) ^ {*} \tag {7.100}
$$

with the pooled spectral matrix (7.98). A modification of the likelihood ratio test, which incorporates the degrees of freedom $M _ { i } = N _ { i } - 1$ and $M = \sum M _ { i }$ rather than the sample sizes into the likelihood ratio statistic, uses

$$
L ^ {\prime} \left(\omega_ {k}\right) = \frac {M ^ {M p}}{\prod_ {i = 1} ^ {I} M _ {i} ^ {M _ {i p}}} \frac {\prod \left| M _ {i} \widehat {f} _ {i} \left(\omega_ {k}\right) \right| ^ {M _ {i}}}{\left| M \widehat {f} _ {v} \left(\omega_ {k}\right) \right| ^ {M}}. \tag {7.101}
$$

Krishnaiah et al. (1976) have given the moments of $L ^ { \prime } ( \omega _ { k } )$ and calculated 95% critical points for $p = 3 , 4$ using a Pearson Type I approximation. For

reasonably large samples involving smoothed spectral estimators, the approximation involving the first term of the usual chi-squared series will suffice and Shumway (1982) has given

$$
\chi_ {(I - 1) p ^ {2}} ^ {2} = - 2 r \log L ^ {\prime} (\omega_ {k}), \tag {7.102}
$$

where

$$
1 - r = \frac {(p + 1) (p - 1)}{6 p (I - 1)} \left(\sum_ {i} M _ {i} ^ {- 1} - M ^ {- 1}\right), \tag {7.103}
$$

with an approximate chi-squared distribution with $( I - 1 ) p ^ { 2 }$ degrees of freedom when the spectral matrices are equal. Introduction of smoothing over $L$ frequencies leads to replacing $M _ { j }$ and $M$ by $L M _ { j }$ and $L M$ in the equations above.

Of course, it is often of great interest to use the above result for testing equality of two univariate spectra, and it is obvious from the material in Chapter 4

$$
F _ {2 L M _ {1}, 2 L M _ {2}} = \frac {\widehat {f} _ {1} (\omega)}{\widehat {f} _ {2} (\omega)} \tag {7.104}
$$

will have the requisite $F ^ { \prime }$ -distribution with $2 L M _ { 1 }$ and $2 L M _ { 2 }$ degrees of freedom when spectra are smoothed over $L$ frequencies.

# Example 7.10 Equality of Means and Spectral Matrices for Earthquakes and Explosions

An interesting problem arises when attempting to develop a methodology for discriminating between waveforms originating from explosions and those that came from the more commonly occurring earthquakes. Figure 7.2 shows a small subset of a larger population of bivariate series consisting of two phases from each of eight earthquakes and eight explosions. If the large–sample approximations to normality hold for the DFTs of these series, it is of interest to known whether the differences between the two classes are better represented by the mean functions or by the spectral matrices. The tests described above can be applied to look at these two questions. The upper left panel of Figure 7.12 shows the test statistic (7.99) with the straight line denoting the critical level for $\alpha = . 0 0 1$ , i.e., $F _ { . 0 0 1 } ( 4 , 2 6 ) = 7 . 3 6$ , for equal means using $L = 1$ , and the test statistic remains well below its critical value at all frequencies, implying that the means of the two classes of series are not significantly different. Checking Figure 7.2 shows little reason exists to suspect that either the earthquakes or explosions have a nonzero mean signal. Checking the equality of the spectra and the spectral matrices, however, leads to a different conclusion. Some smoothing ( $L = 2 1$ ) is useful here, and univariate tests on both the P and S components using (7.104) and $N _ { 1 } = N _ { 2 } = 8$ lead to strong rejections of the equal spectra

![](images/e847e5a78ec976cf32e098fbaad78b0df763318b41175a41f34c22ab41caade4.jpg)

![](images/a8315045522ab9315e599f42ba37cc4c1d4c77b4492d6af485d110e31e90dce8.jpg)

![](images/2195b5aba0c1ad66ca218fcb1c191dcdf416c19519c675cbf22a8bba5f1ad694.jpg)

![](images/78cd295f865bbc9337dcb5bd052404efb7d0b64f6af1ae666a0d43cda62013ee.jpg)  
Figure 7.12 Tests for equality of means, spectra, and spectral matrices for the earthquake and explosion data $p = 2 , L = 2 1 , n = 1 0 2 4$ points at 40 points per second.

hypotheses, with $F _ { \mathrm { . 0 0 1 } } ( \infty , \infty ) = 1 . 0 0$ exceeded at almost all frequencies. The rejection seems stronger for the S component and we might tentatively identify that component as being dominant. Testing equality of the spectral matrices using (7.102) and $\chi _ { . 0 0 1 } ^ { 2 } ( 4 ) = 1 8 . 4 7$ shows a similar strong rejection of the equality of spectral matrices. We use these results to suggest optimal discriminant functions based on spectral differences in the next section.

# 7.7 Discrimination and Cluster Analysis

The extension of classical pattern-recognition techniques to experimental time series is a problem of great practical interest. A series of observations indexed in time often produces a pattern that may form a basis for discriminating between different classes of events. As an example, consider Figure 7.2, which shows regional (100-2000 km) recordings of several typical Scandinavian earthquakes and mining explosions measured by stations in Scandinavia. A listing

of the events is given in Kakizawa et al. (1998). The problem of discriminating between mining explosions and earthquakes is a reasonable proxy for the problem of discriminating between nuclear explosions and earthquakes. This latter problem is one of critical importance for monitoring a comprehensive test-ban treaty. Time series classification problems are not restricted to geophysical applications, but occur under many and varied circumstances in other fields. Traditionally, the detecting of a signal embedded in a noise series has been analyzed in the engineering literature by statistical pattern recognition techniques (see Problems 7.13 and 7.14).

The historical approaches to the problem of discriminating among different classes of time series can be divided into two distinct categories. The optimality approach, as found in the engineering and statistics literature, makes specific Gaussian assumptions about the probability density functions of the separate groups and then develops solutions that satisfy well-defined minimum error criteria. Typically, in the time series case, we might assume the difference between classes is expressed through differences in the theoretical mean and covariance functions and use likelihood methods to develop an optimal classification function. A second class of techniques, which might be described as a feature extraction approach, proceeds more heuristically by looking at quantities that tend to be good visual discriminators for well-separated populations and have some basis in physical theory or intuition. Less attention is paid to finding functions that are approximations to some well-defined optimality criterion.

As in the case of regression, both time domain and frequency domain approaches to discrimination will exist. For relatively short univariate series, a time domain approach that follows conventional multivariate discriminant analysis as described in conventional multivariate texts, such as Anderson (1984) or Johnson and Wichern (1992) may be preferable. We might even characterize differences by the autocovariance functions generated by different ARMA or state-space models. For longer multivariate time series that can be regarded as stationary after the common mean has been subtracted, the frequency domain approach will be easier computationally because the $n p$ dimensional vector in the time domain, represented here as $\pmb { x } = ( \pmb { x } _ { 1 } ^ { \prime } , \pmb { x } _ { t } ^ { \prime } , \dots , \pmb { x } _ { n } ^ { \prime } ) ^ { \prime }$ , with $\pmb { x } _ { t } = ( x _ { t 1 } , \ldots , x _ { t p } ) ^ { \prime }$ , will reduced to separate computations made on the $p$ - dimensional DFTs. This happens because of the approximate independence of the DFTs, ${ \pmb X } ( \omega _ { k } ) , 0 \leq \omega _ { k } \leq 1$ , a property that we have often used in preceding chapters.

Finally, the grouping properties of measures like the discrimination information and likelihood-based statistics can be used to develop measures of disparity for clustering multivariate time series. In this section, we define a measure of disparity between two multivariate times series by the spectral matrices of the two processes and then apply hierarchical clustering and partitioning techniques to identify natural groupings within the bivariate earthquake and explosion populations.

# The General Discrimination Problem

The general problem of classifying a vector time series $\pmb { x }$ occurs in the following way. We observe a time series $\pmb { x }$ known to belong to one of $g$ populations, denoted by $\Pi _ { 1 } , \Pi _ { 2 } , \ldots , \Pi _ { g }$ . The general problem is to assign or classify this observation into one of the $g$ groups in some optimal fashion. An example might be the $g = 2$ populations of earthquakes and explosions shown in Figure 7.2. We would like to classify the unknown event, shown as NZ in the bottom two panels, as belonging to either the earthquake ( $\Pi _ { 1 }$ ) or explosion ( $\mathrm { { I I } _ { 2 } }$ ) populations. To solve this problem, we need an optimality criterion that leads to a statistic $T ( \pmb { x } )$ that can be used to assign the NZ event to either the earthquake or explosion populations. To measure the success of the classification, we need to evaluate errors that can be expected in the future relating to the number of earthquakes classified as explosions (false alarms) and the number of explosions classified as earthquakes (missed signals).

The problem can be formulated by assuming the observed series $\pmb { x }$ has a probability density $p _ { i } ( { \pmb x } )$ when the observed series is from population $\Pi _ { i }$ for $i = 1 , \ldots , g$ . Then, partition the space spanned by the $n p$ -dimensional process $\pmb { x }$ into $g$ mutually exclusive regions $R _ { 1 } , R _ { 2 } , \ldots , R _ { g }$ such that, if $\pmb { x }$ falls in $R _ { i }$ , we assign $_ { x }$ to population $\Pi _ { i }$ . The misclassification probability is defined as the probability of classifying the observation into population $\Pi _ { j }$ when it belongs to $\Pi _ { i }$ , for $j \neq i$ and would be given by the expression

$$
P (j \mid i) = \int_ {R _ {j}} p _ {i} (\boldsymbol {x}) d \boldsymbol {x}. \tag {7.105}
$$

The overall total error probability depends also on the prior probabilities, say, $\pi _ { 1 } , \pi _ { 2 } , \ldots , \pi _ { g }$ , of belonging to one of the $g$ groups. For example, the probability that an observation $_ { x }$ originates from $\Pi _ { i }$ and is then classified into $\Pi _ { j }$ is obviously $\pi _ { i } P ( j | i )$ , and the total error probability becomes

$$
P _ {e} = \sum_ {i = 1} ^ {g} \pi_ {i} \sum_ {j \neq i} P (j | i). \tag {7.106}
$$

Although costs have not been incorporated into (7.106), it is easy to do so by multiplying $P ( j | i )$ by $C ( j | i )$ , the cost of assigning a series from population $\Pi _ { i }$ to $\Pi _ { j }$ .

The overall error $P _ { e }$ is minimized by classifying $\pmb { x }$ into $\Pi _ { i }$ if

$$
\frac {p _ {i} (\boldsymbol {x})}{p _ {j} (\boldsymbol {x})} > \frac {\pi_ {j}}{\pi_ {i}} \tag {7.107}
$$

for all $j \neq i$ (see, for example, Anderson, 1984). A quantity of interest, from the Bayesian perspective, is the posterior probability an observation belongs to population $\Pi _ { i }$ , conditional on observing $_ { x }$ , say,

$$
P \left(\Pi_ {i} | \boldsymbol {x}\right) = \frac {\pi_ {i} p _ {i} (\boldsymbol {x})}{\sum_ {j} \pi_ {j} (\boldsymbol {x}) p _ {j} (\boldsymbol {x})}. \tag {7.108}
$$

The procedure that classifies $_ { x }$ into the population $\Pi _ { i }$ for which the posterior probability is largest is equivalent to that implied by using the criterion (7.107). The posterior probabilities give an intuitive idea of the relative odds of belonging to each of the plausible populations.

Many situations occur, such as in the classification of earthquakes and explosions, in which there are only $g = 2$ populations of interest. For two populations, the Neyman–Pearson lemma implies, in the absence of prior probabilities, classifying an observation into $\Pi _ { 1 }$ when

$$
\frac {p _ {1} (\boldsymbol {x})}{p _ {2} (\boldsymbol {x})} > K \tag {7.109}
$$

minimizes each of the error probabilities for a fixed value of the other. The rule is identical to the Bayes rule (7.107) when $K = \pi _ { 2 } / \pi _ { 1 }$ .

The theory given above takes a simple form when the vector $\pmb { x }$ has a $p$ - variate normal distribution with mean vectors ${ \pmb \mu } _ { j }$ and covariance matrices $\Sigma _ { j }$ under $\Pi _ { j }$ for $j = 1 , 2 , \dots , g$ . In this case, simply use

$$
p _ {j} (\boldsymbol {x}) = (2 \pi) ^ {- p / 2} \left| \Sigma_ {j} \right| ^ {- 1 / 2} \exp \left\{- \frac {1}{2} \left(\boldsymbol {x} - \boldsymbol {\mu} _ {j}\right) ^ {\prime} \Sigma_ {j} ^ {- 1} \left(\boldsymbol {x} - \boldsymbol {\mu} _ {j}\right) \right\}. \tag {7.110}
$$

The classification functions are conveniently expressed by quantities that are proportional to the logarithms of the densities, say,

$$
g _ {j} (\boldsymbol {x}) = - \frac {1}{2} \ln | \Sigma_ {j} | - \frac {1}{2} \boldsymbol {x} ^ {\prime} \Sigma_ {j} ^ {- 1} \boldsymbol {x} + \boldsymbol {\mu} _ {j} ^ {\prime} \Sigma_ {j} ^ {- 1} \boldsymbol {x} - \frac {1}{2} \boldsymbol {\mu} _ {j} ^ {\prime} \Sigma_ {j} ^ {- 1} \boldsymbol {\mu} _ {j} + \ln \pi_ {j}. \tag {7.111}
$$

In expressions involving the log likelihood, we will generally ignore terms involving the constant $- \ln 2 \pi$ . For this case, we may assign an observation $\pmb { x }$ to population $\Pi _ { i }$ whenever

$$
g _ {i} (\boldsymbol {x}) > g _ {j} (\boldsymbol {x}) \tag {7.112}
$$

for $j \neq i , j = 1 , \ldots , g$ and the posterior probability (7.108) has the form

$$
P (\Pi_ {i} | \boldsymbol {x}) = \frac {\exp \{g _ {i} (\boldsymbol {x}) \}}{\sum_ {j} \exp \{g _ {j} (\boldsymbol {x}) \}}.
$$

A common situation occurring in applications involves classification for $g =$ 2 groups under the assumption of multivariate normality and equal covariance matrices; i.e., $\Sigma _ { 1 } = \Sigma _ { 2 } = \Sigma$ . Then, the criterion (7.112) can be expressed in terms of the linear discriminant function

$$
\begin{array}{l} d _ {l} (\boldsymbol {x}) = g _ {1} (\boldsymbol {x}) - g _ {2} (\boldsymbol {x}) \\ = \left(\boldsymbol {\mu} _ {1} - \boldsymbol {\mu} _ {2}\right) ^ {\prime} \Sigma^ {- 1} \boldsymbol {x} - \frac {1}{2} \left(\boldsymbol {\mu} _ {1} - \boldsymbol {\mu} _ {2}\right) ^ {\prime} \Sigma^ {- 1} \left(\boldsymbol {\mu} _ {1} + \boldsymbol {\mu} _ {2}\right) + \ln \frac {\pi_ {1}}{\pi_ {2}}, \tag {7.113} \\ \end{array}
$$

where we classify into $\Pi _ { 1 }$ or $\mathrm { { I I } _ { 2 } }$ according to whether $d _ { l } ( { \pmb x } ) \geq 0$ or $d \pmb { l } ( \pmb { x } ) < 0$ . The linear discriminant function is clearly a combination of normal variables

and, for the case $\pi _ { 1 } = \pi _ { 2 } = . 5$ , will have mean $D ^ { 2 } / 2$ under $\Pi _ { 1 }$ and mean $- D ^ { 2 } / 2$ under $\mathrm { { I I } _ { 2 } }$ , with variances given by $D ^ { 2 }$ under both hypotheses, where

$$
D ^ {2} = \left(\boldsymbol {\mu} _ {1} - \boldsymbol {\mu} _ {2}\right) ^ {\prime} \Sigma^ {- 1} \left(\boldsymbol {\mu} _ {1} - \boldsymbol {\mu} _ {2}\right) \tag {7.114}
$$

is the Mahalanobis distance between the mean vectors $\pmb { \mu } _ { 1 }$ and $\pmb { \mu } _ { 2 }$ . In this case, the two misclassification probabilities (7.1) are

$$
\begin{array}{l} P (1 | 2) = P (2 | 1) \\ = \Phi \left(- \frac {D}{2}\right), \tag {7.115} \\ \end{array}
$$

and the performance is directly related to the Mahalanobis distance (7.114).

For the case in which the covariance matrices cannot be assumed to be the the same, the discriminant function takes a different form, with the difference $g _ { 1 } ( { \pmb x } ) - g _ { 2 } ( { \pmb x } )$ taking the form

$$
\begin{array}{l} d _ {q} (\boldsymbol {x}) = - \frac {1}{2} \ln \frac {| \Sigma_ {1} |}{| \Sigma_ {2} |} - \frac {1}{2} \boldsymbol {x} ^ {\prime} (\Sigma_ {1} ^ {- 1} - \Sigma_ {2} ^ {- 1}) \boldsymbol {x} \\ + \left(\boldsymbol {\mu} _ {1} ^ {\prime} \Sigma_ {1} ^ {- 1} - \boldsymbol {\mu} _ {2} ^ {\prime} \Sigma_ {2} ^ {- 1}\right) \boldsymbol {x} + \ln \frac {\pi_ {1}}{\pi_ {2}} \tag {7.116} \\ \end{array}
$$

for $g = 2$ groups. This discriminant function differs from the equal covariance case in the linear term and in a nonlinear quadratic term involving the differing covariance matrices. The distribution theory is not tractable for the quadratic case so no convenient expression like (7.115) is available for the error probabilities for the quadratic discriminant function.

A difficulty in applying the above theory to real data is that the group mean vectors ${ \pmb \mu } _ { j }$ and covariance matrices $\Sigma _ { j }$ are seldom known. Some engineering problems, such as the detection of a signal in white noise, assume the means and covariance parameters are known exactly, and this can lead to an optimal solution (see Problems 7.14 and 7.15). In the classical multivariate situation, it is possible to collect a sample of $N _ { i }$ training vectors from group $\Pi _ { i }$ , say, $\mathbf { \mathcal { x } } _ { i j }$ , for $j = 1 , \ldots , N _ { i }$ , and use them to estimate the mean vectors and covariance matrices for each of the groups $i = 1 , 2 , \dots , g$ ; i.e., simply choose $\pmb { x } _ { i }$ and

$$
S _ {i} = \left(N _ {i} - 1\right) ^ {- 1} \sum_ {j = 1} ^ {N _ {i}} \left(\boldsymbol {x} _ {i j} - \boldsymbol {x} _ {i \cdot}\right) \left(\boldsymbol {x} _ {i j} - \boldsymbol {x} _ {i \cdot}\right) ^ {\prime} \tag {7.117}
$$

as the estimators for $\pmb { \mu } _ { i }$ and $\Sigma _ { i }$ , respectively. In the case in which the covariance matrices are assumed to be equal, simply use the pooled estimator

$$
S = \left(\sum_ {i} N _ {i} - g\right) ^ {- 1} \sum_ {i} (N _ {i} - 1) S _ {i}. \tag {7.118}
$$

For the case of a linear discriminant function, we may use

$$
\widehat {g _ {i} (\boldsymbol {x})} = \boldsymbol {x} _ {i.} ^ {\prime} S ^ {- 1} \boldsymbol {x} - \frac {1}{2} \boldsymbol {x} _ {i.} ^ {\prime} S ^ {- 1} \boldsymbol {x} _ {i.} + \log \pi_ {i} \tag {7.119}
$$

as a simple estimator for $g _ { i } ( { \pmb x } )$ . For large samples, $\pmb { x } _ { i }$ · and $S$ converge to ${ \pmb \mu } _ { i }$ and $\Sigma$ in probability so $\bar { g } _ { i } ( \pmb { x } )$ converges in distribution to $g _ { i } ( { \pmb x } )$ in that case. The procedure works reasonably well for the case in which $N _ { i } , i = 1 , \dots g$ are large, relative to the length of the series $n$ , a case that is relatively rare in time series analysis. For this reason, we will resort to using spectral approximations for the case in which data are given as long time series.

The performance of sample discriminant functions can be evaluated in several different ways. If the population parameters are known, (7.114) and (7.115) can be evaluated directly. If the parameters are estimated, the estimated Mahalanobis distance $\widehat { D ^ { 2 } }$ can be substituted for the theoretical value in very large samples. Another approach is to calculate the apparent error rates using the result of applying the classification procedure to the training samples. If ${ { n } _ { i j } }$ denotes the number of observations from population $\Pi _ { j }$ classified into $\Pi _ { i }$ , the sample error rates can be estimated by the ratio

$$
\widehat {P (i \mid j)} = \frac {n _ {i j}}{\sum_ {i} n _ {i j}} \tag {7.120}
$$

for $i \neq j$ . If the training samples are not large, this procedure may be biased and a resampling option like cross validation or the bootstrap can be employed. A simple version of cross validation is the jacknife procedure proposed by Lachenbruch and Mickey (1968), which holds out the observation to be classified, deriving the classification function from the remaining observations. Repeating this procedure for each of the members of the training sample and computing (7.120) for the holdout samples leads to better estimators of the error rates.

# Example 7.11 Discriminant Analysis Using Amplitudes from Earthquakes and Explosions

We can give a simple example of applying the above procedures to the logarithms of the amplitudes of the separate P and S components of the original earthquake and explosion traces. The logarithms (base 10) of the maximum peak-to-peak amplitudes of the $\mathrm { P }$ and S components, denoted by $\log _ { 1 0 } P$ and $\log _ { 1 0 } S$ , can be considered as two-dimensional feature vectors, say, $\pmb { x } = ( x _ { 1 } , x _ { 2 } ) ^ { \prime } = ( \log _ { 1 0 } P , \log _ { 1 0 } S ) ^ { \prime }$ , from a bivariate normal population with differering means and covariances. The original data, from Kakizawa et al. (1998), are shown in Table 7.5 and in the left-hand panel of Figure 7.13. The table includes the Novaya Zemlya (NZ) event of unknown origin. The tendency of the earthquakes to have higher values for $\log _ { 1 0 } S$ , relative to $\log _ { 1 0 } P$ has been noted by many and the use of the logarithm of the ratio, i.e., $\log _ { 1 0 } P - \log _ { 1 0 } S$ in some references (see Lay, 1997, pp. 40-41) is a tacit indicator that a linear function of the two parameters will be a useful discriminant.

![](images/59f36178e2d07237045bd5f1378fe9bc936062a41eeb7edb227b6ce96099567b.jpg)

![](images/d3a8cf05bd8adb7f750821f23371b8f8d8887956bfcf95913455da376144e7a4.jpg)  
Figure 7.13 Classification of earthquakes and explosions using the magnitude features (left panel) and the K-L and Chernoff disparity measures (right panel).

The sample means $\pmb { x } _ { 1 }$ $\pmb { x } _ { 1 \cdot } = ( 4 . 2 5 , 4 . 9 5 ) ^ { \prime }$ and $\pmb { x } _ { 2 \cdot } = ( 4 . 6 4 , 4 . 7 3 ) ^ { \prime }$ , and covariance matrices

$$
S _ {1} = \left( \begin{array}{c c} . 3 0 9 6 & . 3 9 5 4 \\ . 3 9 5 4 & . 5 3 7 8 \end{array} \right)
$$

and

$$
S _ {2} = \left( \begin{array}{c c} . 0 9 5 4 & . 0 8 0 4 \\ . 0 8 0 4 & . 1 0 7 0 \end{array} \right)
$$

are immediate from (7.117), with the pooled covariance matrix given by

$$
S = \left( \begin{array}{c c} . 2 0 2 5 & . 2 3 7 9 \\ . 2 3 7 9 & . 3 2 3 8 \end{array} \right)
$$

from (7.118). Although the covariance matrices are not equal, we try the linear discriminant function anyway, which yields (with equal prior probabilities $\pi _ { 1 } = \pi _ { 2 } = . 5$ ) the sample discriminant functions

$$
\widehat {g _ {1} (\boldsymbol {x})} = 2 2. 1 2 x _ {1} - . 9 8 x _ {2} - 4 5. 2 3
$$

and

$$
\widehat {g _ {2} (\boldsymbol {x})} = 4 2. 6 1 x _ {2} - 1 6. 8 x _ {2} - 5 9. 8 0
$$

Table 7.5 Logarithms of Maximum Peak-to-Peak Amplitudes from P and S Components for Eight Earthquakes and Eight Explosions   

<table><tr><td>EQ</td><td>log10P</td><td>log10S</td><td>EXP</td><td>log10P</td><td>log10S</td></tr><tr><td>1</td><td>3.91</td><td>4.67</td><td>1</td><td>4.55</td><td>4.88</td></tr><tr><td>2</td><td>4.78</td><td>5.71</td><td>2</td><td>4.74</td><td>4.43</td></tr><tr><td>3</td><td>3.98</td><td>4.86</td><td>3</td><td>4.90</td><td>5.09</td></tr><tr><td>4</td><td>3.76</td><td>4.14</td><td>3</td><td>4.60</td><td>4.86</td></tr><tr><td>5</td><td>3.80</td><td>4.14</td><td>5</td><td>4.81</td><td>4.76</td></tr><tr><td>6</td><td>4.88</td><td>5.56</td><td>6</td><td>4.36</td><td>4.55</td></tr><tr><td>7</td><td>5.06</td><td>6.03</td><td>6</td><td>5.04</td><td>5.06</td></tr><tr><td>8</td><td>3.80</td><td>4.45</td><td>8</td><td>4.08</td><td>4.14</td></tr><tr><td>NZ</td><td>3.18</td><td>3.27</td><td></td><td></td><td></td></tr></table>

from (7.119), with the estimated linear discriminant function (7.113) as

$$
\widehat {d _ {l} (\boldsymbol {x})} = - 2 0. 4 9 x _ {1} + 1 5. 8 2 x _ {2} + 1 4. 5 7,
$$

indicating $\log _ { 1 0 } S - \log _ { 1 0 } P = x _ { 2 } - x _ { 1 }$ is not far from the optimal linear discriminant function. The jacknifed posterior probabilities of being an earthquake for the earthquake group ranged from .791 to 1.000, whereas the explosion probabilities for the explosion group ranged from .814 to .998, except for the first explosion, which was classified as an earthquake with a posterior probability of .949. Hence, $n _ { 1 2 } = 1$ for this particular example. The unknown event, NZ, was classified as an earthquake, with posterior probability .753. Components of the vector for the unknown event NZ were well outside the range of the values spanned by the training set, so the classification here is somewhat suspect. The quadratic discriminant might be more appropriate here, given the observed differences in the two covariance matrices. Applying the sample version of (7.116) leads to essentially the same results, namely, the misclassification of the first earthquake as an explosion with a posterior probability of .807 and the classification of the unknown NZ event into the earthquake group.

# Frequency Domain Discrimination

The feature extraction approach often works well for discriminating between classes of univariate or multivariate series when there is a simple lowdimensional vector that seems to capture the essence of the differences between the classes. It still seems sensible, however, to develop optimal methods for classification that exploit the differences between the multivariate means and covariance matrices in the time series case. Such methods can be based on the Whittle approximation to the log likelihood given in §7.2. In this case, the vector DFTs, say, $X ( \omega _ { k } )$ , are assumed to be approximately normal, with means $M _ { j } ( \omega _ { k } )$ and spectral matrices $f _ { j } ( \omega _ { k } )$ for population $\Pi _ { j }$ at frequencies

$\omega _ { k } = k / n$ , for $k = 0 , 1 , \ldots [ n / 2 ]$ , and are approximately uncorrelated at different frequencies, say, $\omega _ { k }$ and $\omega _ { \ell }$ for $k \neq \ell$ . Then, writing the complex normal densities as in §7.2 leads to a criterion similar to (7.111); namely,

$$
\begin{array}{l} g _ {j} (\boldsymbol {X}) = \ln \pi_ {j} - \sum_ {0 <   \omega_ {k} <   1 / 2} \left[ \ln | f _ {j} (\omega_ {k}) | + \boldsymbol {X} ^ {*} (\omega_ {k}) f _ {j} ^ {- 1} (\omega_ {k}) \boldsymbol {X} (\omega_ {k}) \right. \\ \left. - 2 \boldsymbol {M} _ {j} ^ {*} (\omega_ {k}) f _ {j} ^ {- 1} (\omega_ {k}) \boldsymbol {X} (\omega_ {k}) + \boldsymbol {M} _ {j} ^ {*} (k) f _ {j} ^ {- 1} (\omega_ {k}) \boldsymbol {M} _ {j} (\omega_ {k}) \right], \tag {7.121} \\ \end{array}
$$

where the sum goes over frequencies for which $\left| f _ { j } ( \omega _ { k } ) \right| \ne 0$ . The periodicity of the spectral density matrix and DFT allows adding over $0 < k < 1 / 2$ . The classification rule is as in (7.112).

In the time series case, it is more likely the discriminant analysis involves assuming the covariance matrices are different and the means are equal. For example, the tests, shown in Figure 7.12, imply, for the earthquakes and explosions, the primary differences are in the bivariate spectral matrices and the means are essentially the same. For this case, it will be convenient to write the Whittle approximation to the log likelihood in the form

$$
\ln p _ {j} (\boldsymbol {X}) = \sum_ {0 <   \omega_ {k} <   1 / 2} \left[ - \ln | f _ {j} (\omega_ {k}) | - \boldsymbol {X} ^ {*} (\omega_ {k}) f _ {j} ^ {- 1} (\omega_ {k}) \boldsymbol {X} (\omega_ {k}) \right], \tag {7.122}
$$

where we have omitted the prior probabilities from the equation. The quadratic detector in this case can be written in the form

$$
\ln p _ {j} (\boldsymbol {X}) = \sum_ {0 <   \omega_ {k} <   1 / 2} \left[ - \ln | f _ {j} (\omega_ {k}) | - \operatorname {t r} \left\{I \left(\omega_ {k}\right) f _ {j} ^ {- 1} \left(\omega_ {k}\right) \right\} \right], \tag {7.123}
$$

where

$$
I (\omega_ {k}) = \boldsymbol {X} (\omega_ {k}) \boldsymbol {X} ^ {*} (\omega_ {k}) \tag {7.124}
$$

denotes the periodogram matrix. For equal prior probabilities, we may assign an observation $_ { x }$ into population $\Pi _ { i }$ whenever

$$
\ln p _ {i} (\boldsymbol {X}) > \ln p _ {j} (\boldsymbol {X}) \tag {7.125}
$$

for $j \neq i , j = 1 , 2 , \ldots , g$ .

Numerous authors have considered various versions of discriminant analysis in the frequency domain. Shumway and Unger (1974) considered (7.121) for $p = 1$ and equal covariance matrices, so the criterion reduces to a simple linear one. They apply the criterion to discriminating between earthquakes and explosions using teleseismic P wave data in which the means over the two groups might be considered as fixed. Alag´on (1989) and Dargahi-Noubary and Laycock (1981) considered discriminant functions of the form (7.121) in the univariate case when the means are zero and the spectra for the two groups are different. Taniguchi et al. (1994) adopted (7.122) as a criterion and discussed

its non-Gaussian robustness. Shumway (1982) reviews general discriminant functions in both the univariate and multivariate time series cases.

# Measures of Disparity

Before proceeding to examples of discriminant and cluster analysis, it is useful to consider the relation to the Kullback–Leibler (K-L) discrimination information, as defined in Problem 2.4 of Chapter 2. Using the spectral approximation and noting the periodogram matrix has the approximate expectation

$$
E _ {j} I (\omega_ {k}) = f _ {j} (\omega_ {k})
$$

under the assumption that the data come from population $\Pi _ { j }$ , and approximating the ratio of the densities by

$$
\ln \frac {p _ {1} (\boldsymbol {X})}{p _ {2} (\boldsymbol {X})} = \sum_ {0 <   \omega_ {k} <   1 / 2} \left[ - \ln \frac {| f _ {1} (\omega_ {k}) |}{| f _ {2} (\omega_ {k}) |} - \mathrm {t r} \left\{\left(f _ {2} ^ {- 1} (\omega_ {k}) - f _ {1} ^ {- 1} (\omega_ {k})\right) I (\omega_ {k}) \right\} \right],
$$

we may write the approximate discrimination information as

$$
\begin{array}{l} I \left(f _ {1}; f _ {2}\right) = \frac {1}{n} E _ {1} \ln \frac {p _ {1} (\boldsymbol {X})}{p _ {2} (\boldsymbol {X})} \\ = \frac {1}{n} \sum_ {0 <   \omega_ {k} <   1 / 2} \left[ \operatorname {t r} \left\{f _ {1} \left(\omega_ {k}\right) f _ {2} ^ {- 1} \left(\omega_ {k}\right) \right\} - \ln \frac {\left| f _ {1} \left(\omega_ {k}\right) \right|}{\left| f _ {2} \left(\omega_ {k}\right) \right|} - p \right]. \tag {7.126} \\ \end{array}
$$

The approximation may be carefully justified by noting the multivatiate normal time series $\pmb { x } = ( \pmb { x } _ { 1 } ^ { \prime } , \pmb { x } _ { 2 } ^ { \prime } \dots , \pmb { x } _ { n } ^ { \prime } )$ with zero means and $n p \times n p$ stationary covariance matrices $\Gamma _ { 1 }$ and $\Gamma _ { 2 }$ will have $p$ , $n \times n$ blocks, with elements of the form γij $\gamma _ { i j } ^ { ( l ) } ( s - t ) , s , t = 1 , \ldots , n , i , j = 1 , \ldots , p$ for population $\Pi _ { \ell } , \ell = 1 , 2$ . The discrimination information, under these conditions, becomes

$$
\begin{array}{l} I (1; 2: \boldsymbol {x}) = \frac {1}{n} E _ {1} \ln \frac {p _ {1} (\boldsymbol {x})}{p _ {2} (\boldsymbol {x})} \\ = \frac {1}{2 n} \left[ \operatorname {t r} \left\{\Gamma_ {1} \Gamma_ {2} ^ {- 1} \right\} - \ln \frac {\left| \Gamma_ {1} \right|}{\left| \Gamma_ {2} \right|} - n p \right]. \tag {7.127} \\ \end{array}
$$

The limiting result

$$
\lim _ {n \to \infty} I (1; 2: \pmb {x}) = \frac {1}{2} \int_ {- 1 / 2} ^ {1 / 2} \left[ \mathrm {t r} \{f _ {1} (\omega) f _ {2} ^ {- 1} (\omega) \} - \ln \frac {| f _ {1} (\omega) |}{| f _ {2} (\omega) |} - p \right] d \omega
$$

has been shown, in various forms, by Pinsker (1964), Hannan (1970), and Kazakos and Papantoni-Kazakos (1980). The discrete version of (7.126) is just the approximation to the integral of the limiting form. The K-L measure of disparity is not a true distance, but it can be shown that $I ( 1 ; 2 ) \ge 0$ , with equality if and only if $f _ { 1 } ( \omega ) = f _ { 2 } ( \omega )$ almost everywhere. This result makes it potentially suitable as a measure of disparity between the two densities.

A connection exists, of course, between the discrimination information number, which is just the expectation of the likelihood criterion and the likelihood itself. For example, we may measure the disparity between the sample and the process defined by the theoretical spectrum $f _ { j } ( \omega _ { k } )$ corresponding to population $\Pi _ { j }$ in the sense of Kullback (1978), as $I ( \widehat { f } ; f _ { j } )$ , where

$$
\widehat {f} \left(\omega_ {k}\right) = L ^ {- 1} \sum_ {\ell = - m} ^ {m} I \left(\omega_ {k} + \ell / n\right) \tag {7.128}
$$

denotes the smoothed spectral matrix. The likelihood ratio criterion can be thought of as measuring the disparity between the periodogram and the theoretical spectrum for each of the populations. To make the discrimination information finite, we replace the periodogram implied by the log likelihood by the sample spectrum. In this case, the classification procedure can be regarded as finding the population closest, in the sense of minimizing disparity between the sample and theoretical spectral matrices. The classification in this case proceeds by simply choosing the population $\Pi _ { j }$ that minimizes $I ( \widehat { f } ; f _ { j } )$ , i.e., assigning $_ { x }$ to population $\Pi _ { i }$ whenever

$$
I (\widehat {f}; f _ {i}) <   I (\widehat {f}; f _ {j}) \tag {7.129}
$$

for $j \neq i , j = 1 , 2 , \ldots , g$ .

Kakizawa et al. (1998) proposed using the Chernoff (CH) information measure (Chernoff, 1952, Renyi, 1961), defined as

$$
B _ {\alpha} (1; 2) = - \ln E _ {2} \left\{\left(\frac {p _ {2} (\boldsymbol {x})}{p _ {1} (\boldsymbol {x})}\right) ^ {\alpha} \right\}, \tag {7.130}
$$

where the measure is indexed by a regularizing parameter $\alpha$ , for $0 < \alpha < 1$ . When $\alpha = . 5$ , the Chernoff measure is the symmetric divergence proposed by Bhattacharya (1943). For the multivariate normal case,

$$
B _ {\alpha} (1; 2: \boldsymbol {x}) = \frac {1}{n} \left[ \ln \frac {| \alpha \Gamma_ {1} + (1 - \alpha) \Gamma_ {2} |}{| \Gamma_ {2} |} - \alpha \ln \frac {| \Gamma_ {1} |}{| \Gamma_ {2} |} \right]. \tag {7.131}
$$

The large sample spectral approximation to the Chernoff information measure is analogous to that for the discrimination information, namely,

$$
\begin{array}{l} B _ {\alpha} (f _ {1}; f _ {2}) = \frac {1}{2 n} \sum_ {0 <   \omega_ {k} <   1 / 2} \left[ \ln \frac {| \alpha f _ {1} (\omega_ {k}) + (1 - \alpha) f _ {2} (\omega_ {k}) |}{| f _ {2} (\omega_ {k}) |} \right. \\ \left. - \alpha \ln \frac {\left| f _ {1} \left(\omega_ {k}\right) \right|}{\left| f _ {2} \left(\omega_ {k}\right) \right|} \right]. \tag {7.132} \\ \end{array}
$$

The Chernoff measure, when divided by $\alpha ( 1 - \alpha )$ , behaves like the discrimination information in the limit in the sense that it converges to $I ( 1 ; 2 : { \pmb x } )$ for $\alpha  0$ and to $I ( 2 ; 1 : { \pmb x } )$ for $\alpha  1$ . Hence, near the boundaries of the parameter $\alpha$ , it tends to behave like discrimination information and for other values

represents a compromise between the two information measures. The classification rule for the Chernoff measure reduces to assigning $\pmb { x }$ to population $\Pi _ { i }$ whenever

$$
B _ {\alpha} (\widehat {f}; f _ {i}) <   B _ {\alpha} (\widehat {f}; f _ {j}) \tag {7.133}
$$

for $j \neq i , j = 1 , 2 , \ldots , g$ .

Although the classification rules above are well defined if the group spectral matrices are known, this will not be the case in general. If there are $g$ training samples, $\pmb { x } _ { i j } , j = 1 , \dots , N _ { i } , i = 1 \dots , g$ , with $N _ { i }$ vector observations available in each group, the natural estimator for the spectral matrix of the group $i$ i s just the single-group spectral matrix (7.100), namely, with $X _ { i j } ( \omega _ { k } )$ denoting the vector DFTs,

$$
\widehat {f} _ {i} \left(\omega_ {k}\right) = \frac {1}{N _ {i} - 1} \sum_ {j = 1} ^ {N _ {i}} \left(\boldsymbol {X} _ {i j} \left(\omega_ {k}\right) - \boldsymbol {X} _ {i.} \left(\omega_ {k}\right)\right) ^ {*} \left(\boldsymbol {X} _ {i j} \left(\omega_ {k}\right) - \boldsymbol {X} _ {i.} \left(\omega_ {k}\right)\right), \tag {7.134}
$$

A second consideration is the choice of the regularization parameter $\alpha$ for the Chernoff criterion, (7.132). For the case of $g = 2$ groups, it should be chosen to maximize the disparity between the two group spectra, as defined in (7.132). Kakizawa et al. (1998) simply plot (7.132) as a function of $\alpha$ , using the estimated group spectra in (7.134), choosing the value that gives the maximum disparity between the two groups.

# Example 7.12 Discriminant Analysis for Earthquakes and Explosions

The simplest approaches to discriminating between the earthquake and explosion groups have been based on either the relative amplitudes of the P and S phases, as in Figure 7.4 or on relative power components in various frequency bands. Considerable effort has been expended on using various spectral ratios involving the bivariate P and S phases as discrimination features. Kakizawa et al. (1998) mention a number of measures that have be used in the seismological literature as features. These features include ratios of power for the two phases and ratios of power components in high- and low-frequency bands. The use of such features of the spectrum suggests an optimal procedure based on discriminating between the spectral matrices of two stationary processes would be reasonable. The fact that the hypothesis that the spectral matrices were equal, tested in Example 7.10, was also soundly rejected suggests the use of a discriminant function based on spectral differences. Recall the sampling rate is 40 points per second, leading to a folding frequency of 20 Hz. To avoid numerical problems, we used a broad band (2Hz, $L = 5 1$ ) and the criteria (7.126) and (7.132), summed over the interval from 0 to 8 Hz, where the spectra were both positive. Narrowing the bandwidth and summing over a broader interval did not substantially change the results.

Table 7.6 Discriminant Scores $I = I ( \widehat { f } ; f _ { 1 } ) - I ( \widehat { f } ; f _ { 2 } )$ and $B = B _ { . 3 } ( \widehat { f } ; f _ { 1 } ) - B _ { . 3 } ( \widehat { f } ; f _ { 2 } )$ for Earthquakes and Explosions   

<table><tr><td>EQ</td><td>I</td><td>B</td><td>EXP</td><td>I</td><td>B</td></tr><tr><td>1</td><td>8.51</td><td>.54</td><td>1</td><td>.29</td><td>-.25</td></tr><tr><td>2</td><td>.81</td><td>.50</td><td>2</td><td>-2.55</td><td>-.75</td></tr><tr><td>3</td><td>30.80</td><td>1.04</td><td>4</td><td>-1.82</td><td>-.61</td></tr><tr><td>4</td><td>2.73</td><td>.10</td><td>4</td><td>-1.89</td><td>-.44</td></tr><tr><td>5</td><td>7.69</td><td>.11</td><td>5</td><td>-1.16</td><td>-.45</td></tr><tr><td>6</td><td>21.50</td><td>.79</td><td>6</td><td>-2.12</td><td>-.61</td></tr><tr><td>7</td><td>20.31</td><td>.85</td><td>7</td><td>-2.10</td><td>-.59</td></tr><tr><td>8</td><td>15.54</td><td>.70</td><td>8</td><td>.93</td><td>-.21</td></tr></table>

The maximum value of the estimated Chernoff disparity $B _ { \alpha } ( \widehat { f _ { 1 } } ; \widehat { f _ { 2 } } )$ occurs for $\alpha \ : = \ : . 3$ , and we use that value in the discriminant criterion (7.132). Discriminant scores using the holdout classification functions are shown in Table 7.6 for both criteria. We note the generally good performance of the Chernoff measure, which separates the two populations well and makes no errors; the discrimination information misclassified explosions one and eight as earthquakes. The values for the two sets of scores are plotted in the right-hand panel of Figure 7.13, and the earthquake variances of the discrimination information have larger variances than do those for the explosions (the standard deviations were 9.34 and 1.25, respectively). The Chernoff discriminant scores are distributed on either side of the decision point 0, with means .58 and -.48 for the earthquake and explosion groups, respectively; the standard deviations of the two samples were .34 and .20. The NZ event was also classified using the average spectral matrices of the eight earthquakes and explosions, giving the value -.49 for the discimination information and -.31 for the Chernoff measure, putting the event in the explosion population by this criterion. Previously, in Example 7.11, the extracted log amplitudes classified this event in the earthquake group. The Russians have asserted no mine blasting or nuclear testing occurred in the area in question, so the event remains as somewhat of a mystery. The fact that it was relatively removed geographically from the test set may also have introduced some uncertainties into the procedure.

# Cluster Analysis

For the purpose of clustering, it may be more useful to consider a symmetric disparity measures and we introduce the J-Divergence measure

$$
J \left(f _ {1}; f _ {2}\right) = I \left(f _ {1}; f _ {2}\right) + I \left(f _ {2}; f _ {1}\right) \tag {7.135}
$$

and the symmetric Chernoff number

$$
J B _ {\alpha} \left(f _ {1}; f _ {2}\right) = B _ {\alpha} \left(f _ {1}; f _ {2}\right) + B _ {\alpha} \left(f _ {2}; f _ {1}\right) \tag {7.136}
$$

for that purpose. In this case, we define the disparity between the sample spectral matrix of a single vector, $_ { x }$ , and the population $\Pi _ { j }$ as

$$
J (\widehat {f}; f _ {j}) = I (\widehat {f}; f _ {j}) + I \left(f _ {j}; \widehat {f}\right) \tag {7.137}
$$

and

$$
J B _ {\alpha} (\widehat {f}; f _ {j}) = B _ {\alpha} (\widehat {f}; f _ {j}) + B _ {\alpha} (f _ {j}; \widehat {f}), \tag {7.138}
$$

respectively and use these as quasi-distances between the vector and population $\Pi _ { j }$ .

The measures of disparity can be used to cluster multivariate time series. The symmetric measures of disparity, as defined above ensure that the disparity between $f _ { i }$ and $f _ { j }$ is the same as the disparity between $f _ { j }$ and $f _ { i }$ . Hence, we will consider the symmetric forms (7.137) and (7.138) as quasi-distances for the purpose of defining a distance matrix for input into one of the standard clustering procedures (see Johnson and Wichern, 1992). In general, we may consider either hierarchical or partitioned clustering methods using the quasidistance matrix as an input.

For purposes of illustration, we may use the symmetric divergence (7.137), which implies the quasi-distance between sample series with estimated spectral matrices $\widehat { f } _ { i }$ and $\widehat { f } _ { j }$ would be (7.137); i.e.,

$$
J \left(\widehat {f} _ {i}; \widehat {f} _ {j}\right) = \frac {1}{n} \sum_ {0 <   \omega_ {k} <   1 / 2} \left[ \operatorname {t r} \left\{\widehat {f} _ {i} \left(\omega_ {k}\right) \widehat {f} _ {j} ^ {- 1} \left(\omega_ {k}\right) \right\} + \operatorname {t r} \left\{\widehat {f} _ {j} \left(\omega_ {k}\right) \widehat {f} _ {i} ^ {- 1} \left(\omega_ {k}\right) \right\} - 2 p \right], \tag {7.139}
$$

for $i \neq j$ . We can also use the comparable form for the Chernoff divergence, but we may not want to make an assumption for the regularization parameter $\alpha$ .

For hierarchical clustering, we begin by clustering the two members of the population that minimize the disparity measure (7.139). Then, these two items form a cluster, and we can compute distances between unclustered items as before. The distance between unnclustered items and a current cluster is defined here as the average of the distances to elements in the cluster. Again, we combine objects that are closest together. We may also compute the distance between the unclustered items and clustered items as the closest distance, rather than the average. Once a series is in a cluster, it stays there. At each stage, we have a fixed number of clusters, depending on the merging stage.

Alternatively, we may think of clustering as a partitioning of the sample into a prespecified number of groups. MacQueen (1967) has proposed this using k-means clustering, using the Mahalonobis distance between an observation and the group mean vectors. At each stage, a reassignment of an observation into its closest affinity group is possible. To see how this procedure applies

Table 7.7 Clustering Results for Earthquakes and Explosions   

<table><tr><td>Beginning</td><td>Cluster 1</td><td>Cluster 2</td><td>Cluster 3</td></tr><tr><td>Two Groups:</td><td></td><td></td><td></td></tr><tr><td>Random</td><td>EQ 123678</td><td>EX 12345678 EQ 45 NZ</td><td></td></tr><tr><td>Hierarchical</td><td>EQ 12345678</td><td>EX 12345678 NZ</td><td></td></tr><tr><td>Three Groups:</td><td></td><td></td><td></td></tr><tr><td>Random</td><td>EQ 123678</td><td>EX 1234567 EQ 4 NZ</td><td>EQ 5 EX 8</td></tr><tr><td>Hierarchical</td><td>EQ 123678</td><td>EX 1234567 NZ</td><td>EQ 45 EX 8</td></tr></table>

in the current context, consider a preliminary partition into a fixed number of groups and define the disparity between the spectral matrix of the observation, say, $\widehat { f }$ , and the average spectral matrix of the group, say, $\widehat { f } _ { i }$ , as $J ( \widehat { f } ; \widehat { f } _ { i } )$ , where the group spectral matrix can be estimated by (7.134). At any pass, a single series is reassigned to the group for which its disparity is minimized. The reassignment procedure is repeated until all observations stay in their current groups. Of course, the number of groups must be specified for each repetition of the partitioning algorithm and a starting partition must be chosen. This assignment can either be random or chosen from a preliminary hierarchical clustering, as described above. kip

# Example 7.13 Cluster Analysis for Earthquakes and Explosions

It is instructive to try the clustering procedure on the population of known earthquakes and explosions. Table 7.7 shows the results of applying partitioned clustering under the assumption that either two or three groups are appropriate. Two groups would be simple assuming the vectors classified naturally into the earthquake and explosion classes, whereas three groups would imply possible outliers from the two primary groups. The starting partitions were defined by either randomly assigning observations to groups or using the result of the hierarchichal clustering procedure. The two-group partition with the hierarchical start configuration tends to produce a final partition that agrees closely with the known configuration, assuming the NZ event is an explosion. The random starting partition puts two of the earthquakes into the explosion group. For the three-group partitions, one or two earthquakes and the last explosion join the third cluster that we have designated as the outlying group.

# 7.8 Principal Components and Factor Analysis

In this section, we introduce the related topics of spectral domain principal components analysis and factor analysis for time series. The topics of principal components and canonical analysis in the frequency domain are rigorously presented in Brillinger (1981, Chapters 9 and 10) and many of the details concerning these concepts can be found there.

The techniques presented here are related to each other in that they focus on extracting pertinent information from spectral matrices. This information is important because dealing directly with a high-dimensional spectral matrix $f ( \omega )$ itself is somewhat cumbersome because it is a function into the set of complex, nonnegative-definite, Hermitian matrices. We can view these techniques as easily understood, parsimonious tools for exploring the behavior of vectorvalued time series in the frequency domain with minimal loss of information. Because our focus is on spectral matrices, we assume for convenience that the time series of interest have zero means; the techniques are easily adjusted in the case of nonzero means.

In this and subsequent sections, it will be convenient to work occasionally with complex-valued time series. A $p \times 1$ complex-valued time series can be represented as ${ \pmb x } _ { t } = { \pmb x } _ { 1 t } - i { \pmb x } _ { 2 t }$ , where ${ \pmb x } _ { 1 t }$ is the real part and ${ \pmb x } _ { 2 t }$ is the imaginary part of ${ \pmb x } _ { t }$ . The process is said to be stationary if $E ( \pmb { x } _ { t } )$ and $E ( \pmb { x } _ { t + h } \pmb { x } _ { t } ^ { * } )$ exist and are independent of time $t$ . The $p \times p$ autocovariance function,

$$
\Gamma_ {x x} (h) = E \left(\boldsymbol {x} _ {t + h} \boldsymbol {x} _ {t} ^ {*}\right) - E \left(\boldsymbol {x} _ {t + h}\right) E \left(\boldsymbol {x} _ {t} ^ {*}\right),
$$

of ${ \pmb x } _ { t }$ satisfies conditions similar to those of the real-valued case. Writing $\Gamma _ { x x } ( h ) \ = \ \{ \gamma _ { i j } ( h ) \}$ , for $i , j ~ = ~ 1 , \dots , p$ , we have (i) $\gamma _ { i i } ( 0 ) \geq 0$ is real, (ii) $| \gamma _ { i j } ( h ) | ^ { 2 } \leq \gamma _ { i i } ( 0 ) \gamma _ { j j } ( 0 )$ for all integers $h$ , and (iii) $\Gamma _ { x x } ( h )$ is Hermitian, that is, $\Gamma _ { x x } ( h ) = \Gamma _ { x x } ( h ) ^ { * }$ . The spectral theory of complex-valued vector time series is analogous to the real-valued case. For example, $\Gamma _ { x } ( h )$ is a nonnegative-definite function on the integers, and if $\begin{array} { r } { \sum _ { h } | | \Gamma _ { x x } ( h ) | | < \infty } \end{array}$ , the spectral density matrix of the complex series ${ \pmb x } _ { t }$ is given by

$$
f _ {x x} (\omega) = \sum_ {h = - \infty} ^ {\infty} \Gamma_ {x x} (h) \exp (- 2 \pi i h \omega).
$$

# Principal Components

Classical principal component analysis (PCA) is concerned with explaining the variance–covariance structure among $p$ variables, $\pmb { x } = ( x _ { 1 } , \ldots , x _ { p } ) ^ { \prime }$ , through a few linear combinations of the components of $\pmb { x }$ . Suppose we wish to find a linear combination

$$
y = \boldsymbol {c} ^ {\prime} \boldsymbol {x} = c _ {1} x _ {1} + \dots + c _ {p} x _ {p} \tag {7.140}
$$

of the components of $_ { x }$ such that $\operatorname { v a r } ( y )$ is as large as possible. Because $\operatorname { v a r } ( y )$ can be increased by simply multiplying $\pmb { c }$ by a constant, it is common to restrict

$\pmb { c }$ to be of unit length; that is, $\pmb { c c } = 1$ . Noting that $\mathrm { v a r } ( y ) = { \pmb { c } } ^ { \prime } \Sigma _ { x x } { \pmb { c } }$ , where $\Sigma _ { x x }$ is the $p \times p$ variance–covariance matrix of $_ { x }$ , another way of stating the problem is to find $\pmb { c }$ such that

$$
\max  _ {\boldsymbol {c} \neq \boldsymbol {0}} \frac {\boldsymbol {c} ^ {\prime} \Sigma_ {x x} \boldsymbol {c}}{\boldsymbol {c} ^ {\prime} \boldsymbol {c}}. \tag {7.141}
$$

Denote the eigenvalue–eigenvector pairs of $\Sigma _ { x x }$ by $\{ ( \lambda _ { 1 } , \pmb { e } _ { 1 } ) , \ldots , ( \lambda _ { p } , \pmb { e } _ { p } ) \}$ , where $\lambda _ { 1 } \geq \lambda _ { 2 } \geq \cdot \cdot \cdot \geq \lambda _ { p } \geq 0$ , and the eigenvectors are of unit length. The solution to (7.141) is to choose ${ \pmb { c } } = { \pmb { e } } _ { 1 }$ , in which case the linear combination $y _ { 1 } = e _ { 1 } ^ { \prime } \pmb { x }$ has maximum variance, $\mathrm { v a r } ( y _ { 1 } ) = \lambda _ { 1 }$ . In other words,

$$
\max  _ {\boldsymbol {c} \neq \boldsymbol {0}} \frac {\boldsymbol {c} ^ {\prime} \Sigma_ {x x} \boldsymbol {c}}{\boldsymbol {c} ^ {\prime} \boldsymbol {c}} = \frac {\boldsymbol {e} _ {1} ^ {\prime} \Sigma_ {x x} \boldsymbol {e} _ {1}}{\boldsymbol {e} _ {1} ^ {\prime} \boldsymbol {e} _ {1}} = \lambda_ {1}. \tag {7.142}
$$

The linear combination, $y _ { 1 } = e _ { 1 } ^ { \prime } x$ , is called the first principal component. Because the eigenvalues of $\Sigma _ { x x }$ are not necessarily unique, the first principal component is not necessarily unique.

The second principal component is defined to be the linear combination $y _ { 2 } = { \pmb { c } } ^ { \prime } { \pmb x }$ that maximizes $\mathrm { v a r } ( y _ { 2 } )$ subject to $\pmb { c } ^ { \prime } \pmb { c } = 1$ and such that $\begin{array} { r l } { \mathrm { c o v } ( y _ { 1 } , y _ { 2 } ) = } \end{array}$ 0. The solution is to choose ${ \pmb { c } } = { \pmb { e } } _ { 2 }$ , in which case, $\mathrm { v a r } ( y _ { 2 } ) = \lambda _ { 2 }$ . In general, the $k$ -th principal component, for $k = 1 , 2 , \dotsc , p$ , is the the linear combination $y _ { k } = \pmb { c } ^ { \prime } \pmb { x }$ that maximizes $\mathrm { v a r } ( y _ { k } )$ subject to $\pmb { c } ^ { \prime } c = 1$ and such that $\begin{array} { r l } { \mathrm { c o v } ( y _ { k } , y _ { j } ) = } \end{array}$ $0$ , for $j = 1 , 2 , \dots , k - 1$ . The solution is to choose $\mathbf { \boldsymbol { c } } = \mathbf { \boldsymbol { e } } _ { k }$ , in which case $\mathrm { v a r } ( y _ { k } ) = \lambda _ { k }$ .

One measure of the importance of a principal component is to assess the proportion of the total variance attributed to that principal component. The total variance of $\pmb { x }$ is defined to be the sum of the variances of the individual components; that is, $\operatorname { v a r } ( x _ { 1 } ) + \cdots + \operatorname { v a r } ( x _ { p } ) = \sigma _ { 1 1 } + \cdots + \sigma _ { p p }$ , where $\sigma _ { j j }$ is the $j$ -th diagonal element of $\Sigma _ { x x }$ . This sum is also denoted as $\operatorname { t r } ( \Sigma _ { x x } )$ , or the trace of $\Sigma _ { x x }$ . Because $\operatorname { t r } ( \Sigma _ { x x } ) = \lambda _ { 1 } + \cdot \cdot \cdot + \lambda _ { p }$ , the proportion of the total variance attributed to the $k$ -th principal component is given simply by $\begin{array} { r } { \operatorname { v a r } ( y _ { k } ) \big / \operatorname { t r } ( \Sigma _ { x x } ) = \lambda _ { k } \big / \sum _ { j = 1 } ^ { p } \lambda _ { j } } \end{array}$ .

Given a random sample $\pmb { x } _ { 1 } , \ldots , \pmb { x } _ { n }$ , the sample principal components are defined as above, but with $\Sigma _ { x x }$ replaced by the sample variance–covariance matrix, $\begin{array} { r } { S _ { x x } = ( n - 1 ) ^ { - 1 } \sum _ { i = 1 } ^ { n } ( { \pmb x } _ { i } - { \bar { \pmb x } } ) ( { \pmb x } _ { i } - { \bar { \pmb x } } ) ^ { \prime } } \end{array}$ . Further details can be found in the introduction to classical principal component analysis in Johnson and Wichern (1992, Chapter 9).

For the case of time series, suppose we have a zero mean, $p \times 1$ , stationary vector process ${ \pmb x } _ { t }$ that has a $p \times p$ spectral density matrix given by $f _ { x x } ( \omega )$ . Recall $f _ { x x } ( \omega )$ is a complex-valued, nonnegative-definite, Hermitian matrix. Using the analogy of classical principal components, and in particular (7.140) and (7.141), suppose, for a fixed value of $\omega$ , we want to find a complexvalued univariate process $y _ { t } ( \omega ) = { \pmb { c } } ( \omega ) ^ { * } x _ { t }$ , where $\pmb { c } ( \omega )$ is complex, such that the spectral density of $y _ { t } ( \omega )$ is maximized at frequency $\omega$ , and $\pmb { c } ( \omega )$ is of unit length, $\pmb { c } ( \omega ) ^ { * } \pmb { c } ( \omega ) = 1$ . Because, at frequency $\omega$ , the spectral density of $y _ { t } ( \omega )$

is $f _ { y } ( \omega ) = \pmb { c } ( \omega ) ^ { * } f _ { x x } ( \omega ) \pmb { c } ( \omega )$ , the problem can be restated as: Find complex vector $\pmb { c } ( \omega )$ such that

$$
\max  _ {\boldsymbol {c} (\omega) \neq \mathbf {0}} \frac {\boldsymbol {c} (\omega) ^ {*} f _ {x x} (\omega) \boldsymbol {c} (\omega)}{\boldsymbol {c} (\omega) ^ {*} \boldsymbol {c} (\omega)}. \tag {7.143}
$$

Let $\{ ( \lambda _ { 1 } ( \omega ) , \pmb { e } _ { 1 } ( \omega ) ) , \ldots , ( \lambda _ { p } ( \omega ) , \pmb { e } _ { p } ( \omega ) ) \}$ denote the eigenvalue–eigenvector pairs of $f _ { x x } ( \omega )$ , where $\lambda _ { 1 } ( \omega ) \ge \lambda _ { 2 } ( \omega ) \ge \cdots \ge \lambda _ { p } ( \omega ) \ge 0$ , and the eigenvectors are of unit length. We note that the eigenvalues of a Hermitian matrix are real. The solution to (7.143) is to choose $\pmb { c } ( \omega ) = \pmb { e } _ { 1 } ( \omega )$ ; in which case the desired linear combination is $y _ { t } ( \omega ) = e _ { 1 } ( \omega ) ^ { * } x _ { t }$ . For this choice,

$$
\max  _ {\boldsymbol {c} (\omega) \neq \boldsymbol {0}} \frac {\boldsymbol {c} (\omega) ^ {*} f _ {x x} (\omega) \boldsymbol {c} (\omega)}{\boldsymbol {c} (\omega) ^ {*} \boldsymbol {c} (\omega)} = \frac {\boldsymbol {e} _ {1} (\omega) ^ {*} f _ {x} (\omega) \boldsymbol {e} _ {1} (\omega)}{\boldsymbol {e} _ {1} (\omega) ^ {*} \boldsymbol {e} _ {1} (\omega)} = \lambda_ {1} (\omega). \tag {7.144}
$$

This process may be repeated for any frequency $\omega$ , and the complex-valued process, $y _ { t 1 } ( \omega ) \ = \ e _ { 1 } ( \omega ) ^ { * } x _ { t }$ , is called the first principal component at frequency $\omega$ . The $k$ -th principal component at frequency $\omega$ , for $k = 1 , 2 , \dotsc , p$ , is the complex-valued time series $y _ { t k } ( \omega ) = e _ { k } ( \omega ) ^ { * } x _ { t }$ , in analogy to the classical case. In this case, the spectral density of $y _ { t k } ( \omega )$ at frequency $\omega$ is $f _ { y _ { k } } ( \omega ) = e _ { k } ( \omega ) ^ { * } f _ { x x } ( \omega ) e _ { k } ( \omega ) = \lambda _ { k } ( \omega )$ .

The previous development of spectral domain principal components is related to the spectral envelope methodology first discussed in Stoffer et al. (1993). We will present the spectral envelope in the next section, where we motivate the use of principal components as it is presented above. Another way to motivate the use of principal components in the frequency domain was given in Brillinger (1981, Chapter 9). Although this technique leads to the same analysis, the motivation may be more satisfactory to the reader at this point. In this case, we suppose we have a stationary, $p$ -dimensional, vector-valued process ${ \pmb x } _ { t }$ and we are only able to keep a univariate process $y _ { t }$ such that, when needed, we may reconstruct the vector-valued process, $\pmb { x } _ { t }$ , according to an optimality criterion.

Specifically, we suppose we want to approximate a mean-zero, stationary, vector-valued time series, ${ \pmb x } _ { t }$ , with spectral matrix $f _ { x x } ( \omega )$ , by a univariate process $y _ { t }$ defined by

$$
y _ {t} = \sum_ {j = - \infty} ^ {\infty} \boldsymbol {c} _ {t - j} ^ {*} \boldsymbol {x} _ {j}, \tag {7.145}
$$

where $\{ c _ { j } \}$ is a $p \times 1$ vector-valued filter, such that $\{ c _ { j } \}$ is absolutely summable; that is, $\sum _ { j = - \infty } ^ { \infty } \left| \pmb { c } _ { j } \right| < \infty$ . The approximation is accomplished so the reconstruction of $\pmb { x } _ { t }$ from $y _ { t }$ , say,

$$
\widehat {\boldsymbol {x}} _ {t} = \sum_ {j = - \infty} ^ {\infty} \boldsymbol {b} _ {t - j} y _ {j}, \tag {7.146}
$$

where $\{ \pmb { b } _ { j } \}$ is an absolutely summable $p \times 1$ filter, is such that the mean square approximation error

$$
E \left\{\left(\boldsymbol {x} _ {t} - \widehat {\boldsymbol {x}} _ {t}\right) ^ {*} \left(\boldsymbol {x} _ {t} - \widehat {\boldsymbol {x}} _ {t}\right) \right\} \tag {7.147}
$$

is minimized.

Let $\pmb { b } ( \omega )$ and $\pmb { c } ( \omega )$ be the transforms of $\{ \pmb { b } _ { j } \}$ and $\{ c _ { j } \}$ , respectively. For example,

$$
\boldsymbol {c} (\omega) = \sum_ {j = - \infty} ^ {\infty} \boldsymbol {c} _ {j} \exp (- 2 \pi i j \omega), \tag {7.148}
$$

and, consequently,

$$
\boldsymbol {c} _ {j} = \int_ {- 1 / 2} ^ {1 / 2} \boldsymbol {c} (\omega) \exp (2 \pi i j \omega) d \omega . \tag {7.149}
$$

Brillinger (1981, Theorem 9.3.1) shows the solution to the problem is to choose $\pmb { c } ( \omega )$ to satisfy (7.143) and to set $\pmb { b } ( \omega ) = \pmb { c } ( \omega )$ . This is precisely the previous problem, with the solution given by (7.144). That is, we choose $\pmb { c } ( \omega ) = \pmb { e } _ { 1 } ( \omega )$ and $\pmb { b } ( \omega ) = \overline { { \pmb { e } _ { 1 } ( \omega ) } }$ ; the filter values can be obtained via the inversion formula given by (7.149). Using these results, in view of (7.145), we may form the first principal component series, say $y _ { t 1 }$ .

This technique may be extended by requesting another series, say, $y _ { t 2 }$ , for approximating $\pmb { x } _ { t }$ with respect to minimum mean square error, but where the coherency between $y _ { t 2 }$ and $y _ { t 1 }$ is zero. In this case, we choose $\pmb { c } ( \omega ) = \pmb { e } _ { 2 } ( \omega )$ . Continuing this way, we can obtain the first $q \leq p$ principal components series, say, $\pmb { y } _ { t } = ( y _ { t 1 } , \dots , y _ { t q } ) ^ { \prime }$ , having spectral density $f _ { q } ( \omega ) = \mathrm { d i a g } \{ \lambda _ { 1 } ( \omega ) , \ldots , \lambda _ { q } ( \omega ) \}$ The series $y _ { t k }$ is the k-th principal component series.

As in the classical case, given observations, $\pmb { x } _ { 1 } , \pmb { x } _ { 2 } , \ldots , \pmb { x } _ { n }$ , from the process ${ \pmb x } _ { t }$ , we can form an estimate ${ \widehat { f } } _ { x x } ( \omega )$ of $f _ { x x } ( \omega )$ and define the sample principal component series by replacing $f _ { x x } ( \omega )$ with ${ \widehat { f } } _ { x x } ( \omega )$ in the previous discussion. Precise details pertaining to the asymptotic ( $n \to \infty$ ) behavior of the principal component series and their spectra can be found in Brillinger (1981, Chapter 9). To give a basic idea of what we can expect, we focus on the first principal component series and on the spectral estimator obtained by smoothing the periodogram matrix, $I _ { n } ( \omega _ { j } )$ ; that is

$$
\widehat {f} _ {x x} \left(\omega_ {j}\right) = \sum_ {\ell = - m} ^ {m} h _ {\ell} I _ {n} \left(\omega_ {j} + \ell / n\right), \tag {7.150}
$$

where $L = 2 m + 1$ is odd and the weights are chosen so $h _ { \ell } = h _ { - \ell }$ are positive and $\textstyle \sum _ { \ell } h _ { \ell } = 1$ . Under the conditions for which ${ \widehat { f } } _ { x x } ( \omega _ { j } )$ is a well-behaved estimator of $f _ { x x } ( \omega _ { j } )$ , and for which the largest eigenvalue of $f _ { x x } ( \omega _ { j } )$ is unique,

$$
\left\{\eta_ {n} \left[ \widehat {\lambda} _ {1} (\omega_ {j}) - \lambda_ {1} (\omega_ {j}) \right] / \lambda_ {1} (\omega_ {j}); \eta_ {n} \left[ \widehat {\boldsymbol {e}} _ {1} (\omega_ {j}) - \boldsymbol {e} _ {1} (\omega_ {j}) \right]; j = 1, \dots , J \right\} \tag {7.151}
$$

converges ( $n  \infty$ ) jointly in distribution to independent, zero-mean normal distributions, the first of which is standard normal. In (7.151), $\eta _ { n } ^ { - 2 } =$ $\scriptstyle \sum _ { \ell = - m } ^ { \prime \prime \ell }$ =  m , noting we must have $L  \infty$ and $\eta _ { n }  \infty$ , but $L / n \to 0$ as $n  \infty$ . The asymptotic variance–covariance matrix of $\widehat { e } _ { 1 } ( \omega )$ , say, $\Sigma _ { e _ { 1 } } ( \omega )$ , is

![](images/9b0e652f3f0d726602e9bc8c704a9557eb2b6174c8eb55c16046aa6887697eff.jpg)

![](images/a82f23a5a7c9cd49cf1096ce02f004602610eb255aca6776ef6e15a013238b05.jpg)

![](images/f4ae37ca596430d59c1b642fbeb51227ccf5c05b7d1e7e8472a7dac3a0c246f7.jpg)

![](images/be54e4f8bd0afeb618211c33e6fd80eb62ad42bdcb6313d9728de7c5c699c04f.jpg)

![](images/f82f7edc3c87007bef6aa26f94d512f082b3d21d3f3314d1022b5410599b143e.jpg)

![](images/e31bfd3c1f8d6b157103db26bf08ef024106cc178c467c6e8b9cb7744216bf60.jpg)

![](images/9342de7f5eae198fec858ec95a3a051bff2a5b9543a2c8aab5e64304a7706c34.jpg)

![](images/c2ad32f398727dcc1a9b9fc70c46301f9b41331d9692e18d9b8a4cabee1d4867.jpg)  
Figure 7.14 The individual periodograms of $x _ { t k }$ , for $k = 1 , \ldots , 8$ , in Example 7.14.

given by

$$
\Sigma_ {e _ {1}} (\omega) = \eta_ {n} ^ {- 2} \lambda_ {1} (\omega) \sum_ {\ell = 2} ^ {p} \lambda_ {\ell} (\omega) \left\{\lambda_ {1} (\omega) - \lambda_ {\ell} (\omega) \right\} ^ {- 2} \boldsymbol {e} _ {\ell} (\omega) \boldsymbol {e} _ {\ell} ^ {*} (\omega). \tag {7.152}
$$

The distribution of $\widehat { e } _ { 1 } ( \omega )$ depends on the other latent roots and vectors of $f _ { x } ( \omega )$ . Writing $\widehat { \pmb { e } } _ { 1 } ( \omega ) = ( \widehat { e } _ { 1 1 } ( \omega ) , \widehat { e } _ { 1 2 } ( \omega ) , \dots , \widehat { e } _ { 1 p } ( \omega ) ) ^ { \prime }$ , we may use this result to form confidence regions for the components of $\widehat { \pmb { e } } _ { 1 }$ by approximating the distribution of

$$
\frac {2 \left| \widehat {\boldsymbol {e}} _ {1 , j} (\omega) - \boldsymbol {e} _ {1 , j} (\omega) \right| ^ {2}}{s _ {j} ^ {2} (\omega)}, \tag {7.153}
$$

for $j = 1 , \dotsc , p$ , by a $\chi ^ { 2 }$ distribution with two degrees of freedom. In (7.153), $s _ { j } ^ { 2 } ( \omega )$ is the $j$ -th diagonal element of $\widehat { \Sigma } _ { e _ { 1 } } ( \omega )$ , the estimate of $\Sigma _ { e _ { 1 } } ( \omega )$ . We can use (7.153) to check whether the value of zero is in the confidence region by comparing $2 | \widehat { \pmb { e } } _ { 1 , j } ( \omega ) | ^ { 2 } / s _ { j } ^ { 2 } ( \omega )$ with $\chi _ { 2 } ^ { 2 } ( 1 - \alpha )$ , the $1 - \alpha$ upper tail cutoff of the $\chi _ { 2 } ^ { 2 }$ distribution.

![](images/d32080cd5ad44eed1fda235c8a2ff47c8f8abf5d3eb52db0219d535b566ff5ac.jpg)  
Figure 7.15 The estimated spectral density, $\widehat { \lambda } _ { 1 } ( j / 1 2 8 )$ , of the first principal component series in Example 7.14.

# Example 7.14 Principal Component Analysis of the fMRI Data

Recall Example 1.6 of Chapter 1, where the vector time series $\begin{array} { r l } { \pmb { x } _ { t } } & { { } = } \end{array}$ $( x _ { t 1 } , \ldots , x _ { t 8 } ) ^ { \prime }$ , $t = 1 , \ldots , 1 2 8$ , represents consecutive measures of average blood oxygenation level dependent (bold) signal intensity, which measures areas of activation in the brain. Recall subjects were given a non-painful brush on the hand and the stimulus was applied for 32 seconds and then stopped for 32 seconds; thus, the signal period is 64 seconds (the sampling rate was one observation every two seconds for 256 seconds). The series $x _ { t k }$ for $k = 1 , 2 , 3 , 4$ represent locations in cortex, series $x _ { t 5 }$ and $x _ { t 6 }$ represent locations in the thalamus, and $x _ { t 7 }$ and $x _ { t 8 }$ represent locations in the cerebellum.

As is evident from Figure 1.6 in Chapter 1, different areas of the brain are responding differently, and a principal component analysis may help in indicating which locations are responding with the most spectral power, and which locations do not contribute to the spectral power at the stimulus signal period. In this analysis, we will focus primarily on the signal period of 64 seconds, which translates to four cycles in 256 seconds or $\omega = 4 / 1 2 8$ cycles per time point. In addition, all calculations were performed using the standardized series; that is, we used $x _ { t k } / s _ { k }$ , for $k = 1 , \ldots , 8$ , where $s _ { k }$ is the sample standard deviation of the the $k$ -th series, in the computations.

Figure 7.14 shows individual periodograms of the series $x _ { t k }$ for $k \mathbf { \Psi } =$ $1 , \ldots , 8$ . As was evident from Figure 1.6, a strong response to the brush stimulus occurred in areas of the cortex. To estimate the spectral density of $\pmb { x } _ { t }$ , we used (7.150) with $L = 5$ and $\{ h _ { 0 } = 3 / 9 , h _ { \pm 1 } = 2 / 9 , h _ { \pm 2 } = 1 / 9 \}$ . Calling the estimated spectrum $\widehat { f } _ { x x } ( j / 1 2 8 )$ , for $j = 0 , 1 , \dotsc , 6 4$ , we can

Table 7.8 Magnitudes of the PC Vector at the Stimulus Frequency in Example 7.14   

<table><tr><td>Location</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>|ˆ1(4/128)|</td><td>0.46</td><td>0.40</td><td>0.45</td><td>0.40</td><td>0.28</td><td>0.15</td><td>0.09*</td><td>0.39</td></tr></table>

*The value of zero is in an approximate $9 9 \%$ confidence region for this component.

obtain the estimated spectrum of the first principal component series $y _ { t 1 }$ by calculating the largest eigenvalue, $\widehat { \lambda } _ { 1 } ( j / 1 2 8 )$ , of $\widehat { f } _ { x x } ( j / 1 2 8 )$ for each $j = 0 , 1 , \dotsc , 6 4$ . The result, $\widehat { \lambda } _ { 1 } ( j / 1 2 8 )$ , is shown in Figure 7.15. As expected, there is a large peak at the stimulus frequency 4/128, wherein $\hat { \lambda } _ { 1 } ( 4 / 1 2 8 ) = 0 . 3 3 9$ . The total power at the stimulus frequency is $\mathrm { t r } \Big ( \widehat { f } _ { x x } ( 4 / 1 2 8 ) \Big ) = 0 . 3 5 3$ , so the proportion of the power at frequency 4/128 attributed to the first principal component series is 0.339/0.353 $= 9 6 \%$ . Because the first principal component explains nearly all of the total power at the stimulus frequency, there is no need to explore the other principal component series at this frequency.

The estimated first principal component series at frequency 4/128 is given by $\widehat { y } _ { t 1 } ( 4 / 1 2 8 ) = \widehat { e } _ { 1 } ^ { * } ( 4 / 1 2 8 ) \pmb { x } _ { t }$ , and the components of $\widehat { \pmb { e } } _ { 1 } ( 4 / 1 2 8 )$ can give insight as to which locations of the brain are responding to the brush stimulus. Table 7.8 shows the magnitudes of $\widehat { \pmb { e } } _ { 1 } ( 4 / 1 2 8 )$ . In addition, an approximate 99% confidence interval was obtained for each component using (7.153). As expected, the analysis indicates that location 7 is not contributing to the power at this frequency, but surprisingly, the analysis suggests location 6 is responding to the stimulus.

# Factor Analysis

Classical factor analysis is similar to classical principal component analysis. Suppose $_ { x }$ is a mean-zero, $p { \times } 1$ , random vector with variance–covariance matrix $\Sigma _ { x x }$ . The factor model proposes that $_ { x }$ is dependent on a few unobserved common factors, $z _ { 1 } , \dotsc , z _ { q }$ , plus error. In this model, one hopes that $q$ will be much smaller than $p$ . The factor model is given by

$$
\boldsymbol {x} = \mathcal {B} z + \epsilon , \tag {7.154}
$$

where $\boldsymbol { \beta }$ is a $p \times q$ matrix of factor loadings, $\boldsymbol { z } = ( z _ { 1 } , \ldots , z _ { q } ) ^ { \prime }$ is a random $q \times 1$ vector of factors such that $E ( z ) = \mathbf { 0 }$ and $E ( z z ^ { \prime } ) = I _ { q }$ , the $q \times q$ identity matrix. The $p \times 1$ unobserved error vector $\mathbf { \epsilon } _ { \epsilon }$ is assumed to be independent of the factors, with zero mean and diagonal variance-covariance matrix $D =$

$\mathrm { d i a g } \{ \delta _ { 1 } ^ { 2 } , . . . , \delta _ { p } ^ { 2 } \}$ . Note, (7.154) differs from the multivariate regression model in §5.7 because the factors, $z$ , are unobserved. Equivalently, the factor model, (7.154), can be written in terms of the covariance structure of $_ { x }$ ,

$$
\Sigma_ {x x} = \mathcal {B} \mathcal {B} ^ {\prime} + D; \tag {7.155}
$$

i.e., the variance-covariance matrix of $_ { x }$ is the sum of a symmetric, nonnegativedefinite rank $q \ \leq \ p$ matrix and a nonnegative-definite diagonal matrix. If $q = p$ , then $\Sigma _ { x x }$ can be reproduced exactly as $B B ^ { \prime }$ , using the fact that $\Sigma _ { x x } =$ $\lambda _ { 1 } \pmb { e } _ { 1 } \pmb { e } _ { 1 } ^ { \prime } + \cdot \cdot \cdot + \lambda _ { p } \pmb { e } _ { p } \pmb { e } _ { p } ^ { \prime }$ , where $( \lambda _ { i } , \pmb { e } _ { i } )$ are the eigenvalue–eigenvector pairs of $\Sigma _ { x x }$ . As previously indicated, however, we hope $q$ will be much smaller than $p$ . Unfortunately, most covariance matrices cannot be factored as (7.155) when $q$ is much smaller than $p$ .

To motivate factor analysis, suppose the components of $_ { x }$ can be grouped into meaningful groups. Within each group, the components are highly correlated, but the correlation between variables that are not in the same group is small. A group is supposedly formed by a single construct, represented as an unobservable factor, responsible for the high correlations within a group. For example, a person competing in a decathlon performs $p = 1 0$ athletic events, and we may represent the outcome of the decathlon as a $1 0 \times 1$ vector of scores. The events in a decathlon involve running, jumping, or throwing, and it is conceivable the $1 0 \times 1$ vector of scores might be able to be factored into $q = 4$ factors, (1) arm strength, (2) leg strength, (3) running speed, and (4) running endurance. The model (7.154) specifies that $\operatorname { c o v } ( \pmb { x } , \pmb { z } ) = \mathcal { B }$ , or $\mathrm { c o v } ( x _ { i } , z _ { j } ) = b _ { i j }$ where $b _ { i j }$ is the $_ { i j }$ -th component of the factor loading matrix $\boldsymbol { \beta }$ , for $i = 1 , \dotsc , p$ and $j = 1 , \dotsc , q$ . Thus, the elements of $\boldsymbol { B }$ are used to identify which hypothetical factors the components of $\pmb { x }$ belong to, or load on.

At this point, some ambiguity is still associated with the factor model. Let $Q$ be a $q \times q$ orthogonal matrix; that is $Q ^ { \prime } Q = Q Q ^ { \prime } = I _ { q }$ . Let $B _ { * } = B Q$ and $z _ { * } = Q ^ { \prime } z$ so (7.154) can be written as

$$
\boldsymbol {x} = \mathcal {B} z + \epsilon = \mathcal {B} Q Q ^ {\prime} z + \epsilon = \mathcal {B} _ {*} z _ {*} + \epsilon . \tag {7.156}
$$

The model in terms of $\boldsymbol { B } _ { \ast }$ and $z _ { * }$ fulfills all of the factor model requirements, for example, $\mathrm { c o v } ( z _ { * } ) = { Q } ^ { \prime } \mathrm { c o v } ( z ) { Q } = { Q } { Q } ^ { \prime } = I _ { q }$ , so

$$
\Sigma_ {x x} = \mathcal {B} _ {*} \operatorname {c o v} \left(\boldsymbol {z} _ {*}\right) \mathcal {B} _ {*} ^ {\prime} + D = \mathcal {B} Q Q ^ {\prime} \mathcal {B} ^ {\prime} + D = \mathcal {B} \mathcal {B} ^ {\prime} + D. \tag {7.157}
$$

Hence, on the basis of observations on $_ { x }$ , we cannot distinguish between the loadings $\boldsymbol { \beta }$ and the rotated loadings $B _ { * } = B Q$ . Typically, $Q$ is chosen so the matrix $\boldsymbol { \beta }$ is easy to interpret, and this is the basis of what is called factor rotation.

Given a sample $\pmb { x } _ { 1 } , \ldots , \pmb { x } _ { n }$ , a number of methods are used to estimate the parameters of the factor model, and we discuss two of them here. The first method is the principal component method. Let $S _ { x x }$ denote the sample variance–covariance matrix, and let $( \widehat { \lambda } _ { i } , \widehat { \pmb { e } } _ { i } )$ be the eigenvalue–eigenvector pairs

of $S _ { x x }$ . The $p \times q$ matrix of estimated factor loadings is found by setting

$$
\widehat {\boldsymbol {B}} = \left[ \begin{array}{c c c c} \widehat {\lambda} _ {1} ^ {1 / 2} & \widehat {\boldsymbol {e}} _ {1} & \widehat {\lambda} _ {2} ^ {1 / 2} & \widehat {\boldsymbol {e}} _ {2} \\ \end{array} \right] & \dots & \left. \widehat {\lambda} _ {q} ^ {1 / 2} \widehat {\boldsymbol {e}} _ {q} \right]. \tag {7.158}
$$

The argument here is that if $q$ factors exist, then

$$
S _ {x x} \approx \widehat {\lambda_ {1}} \widehat {\mathbf {e}} _ {1} \widehat {\mathbf {e}} _ {1} ^ {\prime} + \dots + \widehat {\lambda} _ {q} \widehat {\mathbf {e}} _ {q} \widehat {\mathbf {e}} _ {q} ^ {\prime} = \widehat {\mathcal {B}} \widehat {\mathcal {B}} ^ {\prime}, \tag {7.159}
$$

because the remaining eigenvalues, $\hat { \lambda } _ { q + 1 } , \dotsc , \hat { \lambda } _ { p }$ , will be negligible. The estimated diagonal matrix of error variances is then obtained by setting $\widehat { D } =$ $\mathrm { d i a g } \{ \widehat { \delta } _ { 1 } ^ { 2 } , \ldots , \widehat { \delta } _ { p } ^ { 2 } \}$ , where $\widehat { \delta } _ { j } ^ { 2 }$ is the $j$ -th diagonal element of $S _ { x x } - \widehat { B } \widehat { B ^ { \prime } }$ .

The second method, which can give answers that are considerably different from the principal component method is maximum likelihood. Upon further assumption that in (7.154), $z$ and $\epsilon$ are multivariate normal, the log likelihood of $\boldsymbol { B }$ and $D$ ignoring a constant is

$$
- 2 \ln L (\mathcal {B}, D) = n \ln | \Sigma_ {x x} | + \sum_ {j = 1} ^ {n} \boldsymbol {x} _ {j} ^ {\prime} \Sigma_ {x x} ^ {- 1} \boldsymbol {x} _ {j}. \tag {7.160}
$$

The likelihood depends on $\boldsymbol { B }$ and $D$ through (7.155), $\Sigma _ { x x } = B B ^ { \prime } + D$ . As discussed in (7.156)-(7.157), the likelihood is not well defined because $\boldsymbol { B }$ can be rotated. Typically, restricting $B D ^ { - 1 } B ^ { \prime }$ to be a diagonal matrix is a computationally convenient uniqueness condition. The actual maximization of the likelihood is accomplished using numerical methods.

One obvious method of performing maximum likelihood for the Gaussian factor model is the EM algorithm. For example, suppose the factor vector $z$ is known. Then, the factor model is simply the multivariate regression model given in §5.7, that is, write $X ^ { \prime } = [ \pmb { x } _ { 1 } , \pmb { x } _ { 2 } , \dots , \pmb { x } _ { n } ]$ and $Z ^ { \prime } = [ z _ { 1 } , z _ { 2 } , \ldots , z _ { n } ]$ , and note that $X$ is $n \times p$ and $Z$ is $n \times q$ . Then, the MLE of $\boldsymbol { B }$ is

$$
\widehat {\mathcal {B}} = X ^ {\prime} Z \left(Z ^ {\prime} Z\right) ^ {- 1} = \left(n ^ {- 1} \sum_ {j = 1} ^ {n} \boldsymbol {x} _ {j} \boldsymbol {z} _ {j} ^ {\prime}\right) \left(n ^ {- 1} \sum_ {j = 1} ^ {n} \boldsymbol {z} _ {j} \boldsymbol {z} _ {j} ^ {\prime}\right) ^ {- 1} \stackrel {{\mathrm {d e f}}} {{=}} C _ {x z} C _ {z z} ^ {- 1}, \tag {7.161}
$$

and the MLE of $D$ is

$$
\widehat {D} = \operatorname {D i a g} \left\{n ^ {- 1} \sum_ {j = 1} ^ {n} \left(\boldsymbol {x} _ {j} - \widehat {\mathcal {B}} \boldsymbol {z} _ {j}\right) \left(\boldsymbol {x} _ {j} - \widehat {\mathcal {B}} \boldsymbol {z} _ {j}\right) ^ {\prime} \right\}; \tag {7.162}
$$

that is, only the diagonal elements of the right-hand side of (7.162) are used. The bracketed quantity in (7.162) reduces to

$$
C _ {x x} - C _ {x z} C _ {z z} ^ {- 1} C _ {x z} ^ {\prime}, \tag {7.163}
$$

where $\begin{array} { r } { C _ { x x } = n ^ { - 1 } \sum _ { j = 1 } ^ { n } \pmb { x } _ { j } \pmb { x } _ { j } ^ { \prime } } \end{array}$

Based on the derivation of the EM algorithm for the state-space model, (4.66)-(4.75), we conclude that, to employ the EM algorithm here, given the current parameter estimates, in $C _ { x z }$ , we replace $\pmb { x } _ { j } \pmb { z } _ { j } ^ { \prime }$ by $\pmb { x } _ { j } \widetilde { \pmb { z } } _ { j } ^ { \prime }$ , where $\widetilde { z } _ { j } ~ =$ $E ( z _ { j } \mid \pmb { x } _ { j } )$ , and in $C _ { z z }$ , we replace $\boldsymbol { z } _ { j } \boldsymbol { z } _ { j } ^ { \prime }$ by $P _ { z } + \widetilde { { z } } _ { j } \widetilde { { z } } _ { j } ^ { \prime }$ , where $P _ { z } = \operatorname { v a r } ( z _ { j } \mid x _ { j } )$ . Using the fact that the $( p + q ) \times 1$ vector $( \pmb { x } _ { j } ^ { \prime } , z _ { j } ^ { \prime } ) ^ { \prime }$ is multivariate normal with mean-zero, and variance–covariance matrix given by

$$
\left( \begin{array}{c c} \mathcal {B B} ^ {\prime} + D & \mathcal {B} \\ \mathcal {B} ^ {\prime} & I _ {q} \end{array} \right), \tag {7.164}
$$

we have

$$
\widetilde {\boldsymbol {z}} _ {j} \equiv E \left(\boldsymbol {z} _ {j} \mid \boldsymbol {x} _ {j}\right) = \mathcal {B} ^ {\prime} \left(\mathcal {B} ^ {\prime} \mathcal {B} + D\right) ^ {- 1} \boldsymbol {x} _ {j} \tag {7.165}
$$

and

$$
P _ {z} \equiv \operatorname {v a r} \left(\boldsymbol {z} _ {j} \mid \boldsymbol {x} _ {j}\right) = I _ {q} - \mathcal {B} ^ {\prime} \left(\mathcal {B} ^ {\prime} \mathcal {B} + D\right) ^ {- 1} \mathcal {B}. \tag {7.166}
$$

For time series, suppose $\pmb { x } _ { t }$ is a stationary $p \times 1$ process with $p \times p$ spectral matrix $f _ { x x } ( \omega )$ . Analogous to the classical model displayed in (7.155), we may postulate that at a given frequency of interest, $\omega$ , the spectral matrix of ${ \pmb x } _ { t }$ satisfies

$$
f _ {x x} (\omega) = \mathcal {B} (\omega) \mathcal {B} (\omega) ^ {*} + D (\omega), \tag {7.167}
$$

where $B ( \omega )$ is a complex-valued $p \times q$ matrix with ${ \mathrm { r a n k } } ( B ( \omega ) ) = q \leq p$ and $D ( \omega )$ is a real, nonnegative-definite, diagonal matrix. Typically, we expect $q$ will be much smaller than $p$ .

As an example of a model that gives rise to (7.167), let $\pmb { x } _ { t } = ( x _ { t 1 } , \ldots , x _ { t p } ) ^ { \prime }$ , and suppose

$$
x _ {t j} = c _ {j} s _ {t - \tau_ {j}} + \epsilon_ {t j}, \quad j = 1, \dots , p, \tag {7.168}
$$

where $c _ { j } \geq 0$ are individual amplitudes and $s _ { t }$ is a common unobserved signal (factor) with spectral density $f _ { s s } ( \omega )$ . The values $\tau _ { j }$ are the individual phase shifts. Assume $s _ { t }$ is independent of $\epsilon _ { t } = ( \epsilon _ { t 1 } , . . . , \epsilon _ { t p } ) ^ { \prime }$ and the spectral matrix of $\epsilon _ { t }$ , $D _ { \epsilon \epsilon } ( \omega )$ , is diagonal. The DFT of $x _ { t j }$ is given by

$$
X _ {j} (\omega) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t j} \exp (- 2 \pi i t \omega)
$$

and, in terms of the model (7.168),

$$
X _ {j} (\omega) = a _ {j} (\omega) X _ {s} (\omega) + X _ {\epsilon_ {j}} (\omega), \tag {7.169}
$$

where $a _ { j } ( \omega ) = c _ { j } \exp ( - 2 \pi i \tau _ { j } \omega )$ , and $X _ { s } ( \omega )$ and $X _ { \epsilon _ { j } } ( \omega )$ are the respective DFTs of the signal $s _ { t }$ and the noise $\epsilon _ { t j }$ . Stacking the individual elements of (7.169), we obtain a complex version of the classical factor model with one factor,

$$
\left( \begin{array}{c} X _ {1} (\omega) \\ \vdots \\ X _ {p} (\omega) \end{array} \right) = \left( \begin{array}{c} a _ {1} (\omega) \\ \vdots \\ a _ {p} (\omega) \end{array} \right) X _ {s} (\omega) + \left( \begin{array}{c} X _ {\epsilon_ {1}} (\omega) \\ \vdots \\ X _ {\epsilon_ {p}} (\omega) \end{array} \right),
$$

or more succinctly,

$$
\boldsymbol {X} (\omega) = \boldsymbol {a} (\omega) X _ {s} (\omega) + \boldsymbol {X} _ {\epsilon} (\omega). \tag {7.170}
$$

From (7.170), we can identify the spectral components of the model; that is,

$$
f _ {x x} (\omega) = \boldsymbol {b} (\omega) \boldsymbol {b} (\omega) ^ {*} + D _ {\epsilon \epsilon} (\omega), \tag {7.171}
$$

where $\pmb { b } ( \omega )$ is a $p \times 1$ complex-valued vector, $\pmb { b } ( \omega ) \pmb { b } ( \omega ) ^ { * } = \pmb { a } ( \omega ) f _ { s s } ( \omega ) \pmb { a } ( \omega ) ^ { * }$ . Model (7.171) could be considered the one-factor model for time series. This model can be extended to more than one factor by adding other independent signals into the original model (7.168). More details regarding this and related models can be found in Stoffer (1999).

# Example 7.15 Single Factor Analysis of the fMRI Data

The fMRI data analyzed in Example 7.14 is well suited for a single factor analysis using the model (7.168), or, equivalently, the complex-valued, single factor model (7.170). In terms of (7.168), we can think of the signal $s _ { t }$ as representing the brush stimulus signal. As before, the frequency of interest is $\omega = 4 / 1 2 8$ , which corresponds to a period of 32 time points, or 64 seconds.

A simple way to estimate the components $\pmb { b } ( \omega )$ and $D _ { \epsilon \epsilon } ( \omega )$ , as specified in (7.171), is to use the principal components method. Let ${ \widehat { f } } _ { x x } ( \omega )$ denote the estimate of the spectral density of $\pmb { x } _ { t } = ( x _ { t 1 } , \ldots , x _ { t 8 } ) ^ { \prime }$ obtained in Example 7.14. Then, analogous to (7.158) and (7.159), we set

$$
\widehat {\boldsymbol {b}} (\omega) = \sqrt {\widehat {\lambda} _ {1} (\omega)} \widehat {\boldsymbol {e}} _ {1} (\omega),
$$

where $\left( \widehat { \lambda } _ { 1 } ( \omega ) , \widehat { \pmb { e } } _ { 1 } ( \omega ) \right)$ is the first eigenvalue–eigenvector pair of ${ \widehat { f } } _ { x x } ( \omega )$ . The diagonal elements of $\widehat { D } _ { \epsilon \epsilon } ( \omega )$ are obtained from the diagonal elements of $\widehat { f } _ { x x } ( \omega ) - \widehat { \pmb { b } } ( \omega ) \widehat { \pmb { b } } ( \omega ) ^ { * }$ . The appropriateness of the model can be assessed by checking the elements of the residual matrix, $\widehat { f } _ { x x } ( \omega ) - \widehat { \vert } \widehat { \pmb { b } } ( \omega ) \widehat { \pmb { b } } ( \omega ) ^ { * } +$ $\widehat { D } _ { \epsilon \epsilon } ( \omega ) ]$ , are negligible in magnitude.

Concentrating on the stimulus frequency, recall $\hat { \lambda } _ { 1 } ( 4 / 1 2 8 ) = 0 . 3 3 9$ . The magnitudes of $\widehat { e } _ { 1 } ( 4 / 1 2 8 )$ are displayed in Table 7.8, indicating all locations load on the stimulus factor except for location 7, and location 6 could be considered borderline. The diagonal elements of ${ \widehat { f } } _ { x x } ( \omega ) -$ $\widehat { \pmb { b } } ( \omega ) \widehat { \pmb { b } } ( \omega ) ^ { * }$ yield

$$
\widehat {D} _ {\epsilon \epsilon} (4 / 1 2 8) = 0. 0 0 1 \times \operatorname {d i a g} \{0. 2 7, 1. 0 6, 0. 4 5, 1. 2 6, 1. 6 4, 4. 2 2, 4. 3 8, 1. 0 8 \}.
$$

The magnitudes of the elements of residual matrix at $\omega = 4 / 1 2 8$ are

$$
0. 0 0 1 \times \left( \begin{array}{c c c c c c c c} 0. 0 0 & 0. 1 9 & 0. 1 4 & 0. 1 9 & 0. 4 9 & 0. 4 9 & 0. 6 5 & 0. 4 6 \\ 0. 1 9 & 0. 0 0 & 0. 4 9 & 0. 8 6 & 0. 7 1 & 1. 1 1 & 1. 8 0 & 0. 5 8 \\ 0. 1 4 & 0. 4 9 & 0. 0 0 & 0. 6 2 & 0. 6 7 & 0. 6 5 & 0. 3 9 & 0. 2 2 \\ 0. 1 9 & 0. 8 6 & 0. 6 2 & 0. 0 0 & 1. 0 2 & 1. 3 3 & 1. 1 6 & 0. 1 4 \\ 0. 4 9 & 0. 7 1 & 0. 6 7 & 1. 0 2 & 0. 0 0 & 0. 8 5 & 1. 1 1 & 0. 5 7 \\ 0. 4 9 & 1. 1 1 & 0. 6 5 & 1. 3 3 & 0. 8 5 & 0. 0 0 & 1. 8 1 & 1. 3 6 \\ 0. 6 5 & 1. 8 0 & 0. 3 9 & 1. 1 6 & 1. 1 1 & 1. 8 1 & 0. 0 0 & 1. 5 7 \\ 0. 4 6 & 0. 5 8 & 0. 2 2 & 0. 1 4 & 0. 5 7 & 1. 3 6 & 1. 5 7 & 0. 0 0 \end{array} \right),
$$

indicating the model fit is good.

A number of authors have considered factor analysis in the spectral domain, for example Priestley et al. (1974); Priestley and Subba Rao (1975); Geweke (1977), and Geweke and Singleton (1981), to mention a few. An obvious extension of simple model (7.168) is the factor model

$$
\boldsymbol {x} _ {t} = \sum_ {j = - \infty} ^ {\infty} \Lambda_ {j} \boldsymbol {s} _ {t - j} + \boldsymbol {\epsilon} _ {t}, \tag {7.172}
$$

where $\{ \Lambda _ { j } \}$ is a real-valued $p \times q$ filter, $\mathbf { \Delta } _ { \mathbf { \delta } } \mathbf { \vec { s } } _ { t }$ is a $q \times 1$ stationary, unobserved signal, with independent components, and $\epsilon _ { t }$ is white noise. We assume the signal and noise process are independent, $\mathbf { \Delta } \mathbf { s } _ { t }$ has $q \times q$ real, diagonal spectral matrix $f _ { s s } ( \omega ) = \mathrm { d i a g } \{ f _ { s 1 } ( \omega ) , \ldots , f _ { s q } ( \omega ) \}$ , and $\epsilon _ { t }$ has a real, diagonal, $p \times p$ spectral matrix given by $D _ { \epsilon \epsilon } ( \omega ) = \mathrm { d i a g } \{ f _ { \epsilon 1 } ( \omega ) , \dots , f _ { \epsilon p } ( \omega ) \}$ . If, in addition, $\sum | | \Lambda _ { j } | | < \infty$ , the spectral matrix of $\pmb { x } _ { t }$ can be written as

$$
f _ {x x} (\omega) = \Lambda (\omega) f _ {s s} (\omega) \Lambda (\omega) ^ {*} + D _ {\epsilon \epsilon} (\omega) = \mathcal {B} (\omega) \mathcal {B} (\omega) ^ {*} + D _ {\epsilon \epsilon} (\omega), \tag {7.173}
$$

where

$$
\Lambda (\omega) = \sum_ {t = - \infty} ^ {\infty} \Lambda_ {t} \exp (- 2 \pi i t \omega) \tag {7.174}
$$

and $B ( \omega ) = \Lambda ( \omega ) f _ { s s } ^ { 1 / 2 } ( \omega )$ . Thus, by (7.173), the model (7.172) is seen to satisfy the basic requirement of the spectral domain factor analysis model; that is, the $p \times p$ spectral density matrix of the process of interest, $f _ { x x } ( \omega )$ , is the sum of a rank $q \leq p$ matrix, $B ( \omega ) B ( \omega ) ^ { * }$ , and a real, diagonal matrix, $D _ { \epsilon \epsilon } ( \omega )$ . For the purpose of identifiability we set $f _ { s s } ( \omega ) = I _ { q }$ for all $\omega$ ; in which case, $B ( \omega ) = \Lambda ( \omega )$ . As in the classical case [see (7.157)], the model is specified only up to rotations; for details, see Bloomfield and Davis (1994).

Parameter estimation for the model (7.172), or equivalently (7.173), can be accomplished using the principal component method. Let ${ \widehat { f } } _ { x x } ( \omega )$ be an estimate of $f _ { x x } ( \omega )$ , and let $\widehat { \lambda } _ { j } ( \omega ) , \widehat { \pmb { e } } _ { j } ( \omega ) \big )$ , for $j = 1 , \hdots , p$ , be the eigenvalue– eigenvector pairs, in the usual order, of ${ \widehat { f } } _ { x x } ( \omega )$ . Then, as in the classical case, the $p \times q$ matrix $\boldsymbol { \beta }$ is estimated by

$$
\widehat {\boldsymbol {B}} (\omega) = \left[ \widehat {\lambda} _ {1} (\omega) ^ {1 / 2} \widehat {\boldsymbol {e}} _ {1} (\omega) \mid \widehat {\lambda} _ {2} (\omega) ^ {1 / 2} \widehat {\boldsymbol {e}} _ {2} (\omega) \mid \dots \mid \widehat {\lambda} _ {q} (\omega) ^ {1 / 2} \widehat {\boldsymbol {e}} _ {q} (\omega) \right]. \tag {7.175}
$$

The estimated diagonal spectral density matrix of errors is then obtained by setting $\widehat { D } _ { \epsilon \epsilon } ( \omega ) = \mathrm { d i a g } \{ \widehat { f } _ { \epsilon 1 } ( \omega ) , \ldots , \widehat { f } _ { \epsilon p } ( \omega ) \}$ , where $\widehat { f } _ { \epsilon j } ( \omega )$ is the $j$ -th diagonal element of $\widehat { f } _ { x x } ( \omega ) - \widehat { B } ( \omega ) \widehat { B } ( \omega ) ^ { * }$ .

Alternatively, we can estimate the parameters by approximate likelihood methods. As in (7.170), let $X ( \omega _ { j } )$ denote the DFT of the data $\pmb { x } _ { 1 } , \ldots , \pmb { x } _ { n }$ at frequency $\omega _ { j } = j / n$ . Similarly, let $X _ { s } ( \omega _ { j } )$ and $X _ { \epsilon } ( \omega _ { j } )$ be the DFTs of the signal and of the noise processes, respectively. Then, under certain conditions (see Pawitan and Shumway, 1989), for $\ell = 0 , \pm 1 , \ldots , \pm m$ ,

$$
\boldsymbol {X} \left(\omega_ {j} + \ell / n\right) = \Lambda \left(\omega_ {j}\right) \boldsymbol {X} _ {s} \left(\omega_ {j} + \ell / n\right) + \boldsymbol {X} _ {\epsilon} \left(\omega_ {j} + \ell / n\right) + \mathrm {o} _ {a s} \left(n ^ {- \alpha}\right), \tag {7.176}
$$

where $\Lambda ( \omega _ { j } )$ is given by (7.174) and $\scriptscriptstyle { 0 } _ { a s } ( n ^ { - \alpha } ) \  \ 0$ almost surely for some $0 \leq \alpha < 1 / 2$ as $n  \infty$ . In (7.176), the ${ \pmb X } ( \omega _ { j } + \ell / n )$ are the DFTs of the data at the $L$ odd frequencies $\{ \omega _ { j } + \ell / n$ $\cdot \ell / n ; \ell = 0 , \pm 1 , \ldots , \pm m \}$ surrounding the central frequency of interest $\omega _ { j } = j / n$ .

Under appropriate conditions $\{ X ( \omega _ { j } + \ell / n ) ; \ell = 0 , \pm 1 , . . . , \pm m \}$ in (7.176) are approximately ( $n  \infty$ ) independent, complex Gaussian random vectors with variance–covariance matrix $f _ { x x } ( \omega _ { j } )$ . The approximate likelihood is given by

$$
\begin{array}{l} - 2 \ln L \left(\mathcal {B} \left(\omega_ {j}\right), D _ {\epsilon \epsilon} \left(\omega_ {j}\right)\right) \\ = n \ln \left| f _ {x x} \left(\omega_ {j}\right) \right| + \sum_ {\ell = - m} ^ {m} X ^ {*} \left(\omega_ {j} + \ell / n\right) f _ {x x} ^ {- 1} \left(\omega_ {j}\right) X \left(\omega_ {j} + \ell / n\right), \tag {7.177} \\ \end{array}
$$

with the constraint $f _ { x x } ( \omega _ { j } ) = \mathcal { B } ( \omega _ { j } ) \mathcal { B } ( \omega _ { j } ) ^ { * } + D _ { \epsilon \epsilon } ( \omega _ { j } )$ . As in the classical case, we can use various numerical methods to maximize $L ( B ( \omega _ { j } ) , D _ { \epsilon \epsilon } ( \omega _ { j } ) )$ at every frequency, $\omega _ { j }$ , of interest. For example, the EM algorithm discussed for the classical case, (7.161)-(7.166), can easily be extended to this case.

Assuming $f _ { s s } ( \omega ) = I _ { q }$ , the estimate of $B ( \omega _ { j } )$ is also the estimate of $\Lambda ( \omega _ { j } )$ . Calling this estimate $\widehat { \Lambda } ( \omega _ { j } )$ , the time domain filter can be estimated by

$$
\widehat {\Lambda} _ {t} ^ {M} = M ^ {- 1} \sum_ {j = 0} ^ {M - 1} \widehat {\Lambda} \left(\omega_ {j}\right) \exp \left(2 \pi i j t / n\right), \tag {7.178}
$$

for some $0 < M \leq n$ , which is the discrete and finite version of the inversion formula given by

$$
\Lambda_ {t} = \int_ {- 1 / 2} ^ {1 / 2} \Lambda (\omega) \exp (2 \pi i \omega t) d \omega . \tag {7.179}
$$

Note that we have used this approximation earlier in Chapter 4, (4.135), for estimating the time response of a frequency response function defined over a finite number of frequencies.

![](images/d5f7d4c21209fa4fb7419ba85b83ed4924bb0ac62db9bc0c5b3fedbd5f333b54.jpg)  
Consumption

![](images/1284c956d1d5324230fd5554bb646beb5dc316a6e36afd72e1aea1e68ce21fa2.jpg)  
Govt.Investment

![](images/9965d4cdf84622aef97fa8acb1fee8ba3158c9dcdebbf802fe9124bc952b60d9.jpg)  
Private Investment

![](images/b220eba286fe73529d7d08928482478595f8b302bc2de6ef06e316ef2871044e.jpg)

![](images/7b9fcc7d24490669dabde8d53752d82d504354277ff515bc242bd707edc56631.jpg)  
Figure 7.16 The seasonally adjusted, quarterly growth rate (as percentages) of five macroeconomic series, Unemployment, GNP, Consumption, Government Investment, and Private Investment in the U.S. between 1948 and 1988, $n =$ 160 values.

# Example 7.16 Government Spending, Private Investment, and Unemployment in the U.S.

Figure 7.16 shows the seasonally adjusted, quarterly growth rate (as percentages) of five macroeconomic series, Unemployment, GNP, Consumption, Government Investment, and Private Investment in the U.S. between 1948 and 1988, $n = 1 6 0$ values. These data are analyzed in the time domain by Young and Pedregal (1998), who were investigating how government spending and private capital investment influenced the rate of unemployment.

Spectral estimation was performed on the detrended, standardized, and tapered (using a cosine bell) growth rate values. Then, as described in (7.150), a set of $L \ = \ 1 1$ triangular weights, $\{ h _ { 0 } ~ = ~ 6 / 3 6 , h _ { \pm 1 } ~ =$ $5 / 3 6 , h _ { \pm 2 } = 4 / 3 6 , h _ { \pm 3 } = 3 / 3 6 , h _ { \pm 4 } = 2 / 3 6 , h _ { \pm 5 } = 1 / 3 6 \}$ , were used to smooth in $5 \times 5$ periodogram matrices. Figure 7.17 shows the individual estimated spectra (scaled by 1000) of each series in terms of the number of cycles. We focus on three interesting frequencies. First, we note the lack of spectral power near 40 cycles ( $\omega = 4 0 / 1 6 0 = 1 / 4$ ; one cycle every four quarters, or one year), indicating the data have been seasonally adjusted. In addition, because of the seasonal adjustment, some

![](images/ebc5045387f7a2a2415cdaa24deeb9b395fcd3efd9e99704e2d44ea5f12ed0e1.jpg)  
Consumption

![](images/29a117ac7d9c3ab6b6c3a0bddc18dabe674362da8274830997ba88b473927766.jpg)  
Govt. Investment

![](images/e91e1f8a92acb0b49566238426a911d114a7097c5c3db06ce04a9e7614cd522f.jpg)  
Private Investment

![](images/2a20b7c918a8450d35558540ab1e552045e8dfc16285aa8d9d5b54715d6639d0.jpg)

![](images/798e0e0ea5936696b819b82f9dcfc1c6099dc0f9e36fcea34acd650b957d1a9e.jpg)  
Figure 7.17 The individual estimated spectra (scaled by 1000) of each series show in Figure 7.16 in terms of the number of cycles in 160 quarters.

spectral power appears near the seasonal frequency; this is a distortion apparently caused by the method of seasonally adjusting the data. Next, we note spectral power appears near 10 cycles ( $\omega = 1 0 / 1 6 0 = 1 / 1 6$ ; one cycle every four years) in Unemployment, GNP, Consumption, and, to lesser degree, in Private Investment. Finally, spectral power appears near five cycles ( $\omega = 5 / 1 6 0 = 1 / 3 2$ ; one cycle every 8 years) in Government Investment, and perhaps to lesser degrees in Unemployment, GNP, and Consumption.

Figure 7.18 shows the coherences among various series. At the frequencies of interest ( $\omega ~ = ~ 5 / 1 6 0$ and 10/160), pairwise, GNP, Unemployment, Consumption, and Private Investment (except for Unemployment and Private Investment) are coherent. Government Investment is either not coherent or minimally coherent with the other series.

Figure 7.19 shows $\widehat { \lambda } _ { 1 } ( \omega )$ and $\widehat { \lambda } _ { 2 } ( \omega )$ , the first and second eigenvalues of the estimated spectral matrix $\widehat { f } _ { x x } ( \omega )$ . These eigenvalues suggest the first factor is identified by the frequency of one cycle every four years, whereas the second factor is identified by the frequency of one cycle every eight years. The modulus of the corresponding eigenvectors at the frequencies of interest, $\widehat { \pmb { e } } _ { 1 } ( 1 0 / 1 6 0 )$ and $\widehat { e } _ { 2 } ( 5 / 1 6 0 )$ , are shown in Table 7.9. These values confirm Unemployment, GNP, Consumption, and Private Investment load on the first factor, and Government Investment loads on the second factor. The remainder of the details involving the factor analysis of these data is left as an exercise.

![](images/8518473d6dd0e269d8b7c29188336f066edf441d30816656c56fef428438d13a.jpg)  
GNP - Govt. Invst.

![](images/4da636c6d1980e93d8055ab1dda0fa14dac8198f3fae2a0735e06c9e2cbd46bb.jpg)  
GNP - Pvt.Invst.

![](images/b5f4f5474c30ec856472da5360c71fdafa4c056068d8f945488af0f8fe39acc9.jpg)  
Unemployment - Consumption

![](images/b021f4a95a79060fc5b93237be094ed97ee759fc5b9665279cbe539a693e9e64.jpg)  
Unemployment - Govt. Invst.

![](images/802767fa96e8207f00bb0a195075ad678534fc0515430ed8c32d8128d2bfb326.jpg)  
Unemployment - Pvt. Invst.

![](images/7a66a0a5fffcec33eaeb6370efafb6d688d48346652027f0076ddf1dc5f164d2.jpg)  
Consumption - Govt. Invst.

![](images/8d3385bd15c29572acc0e40ff0eb74ccf49409dcf408ddd91a80913fd299f962.jpg)  
Consumption - Pvt. Invst.

![](images/577001fd38445739fc7730719d2394438a04da6a97eda410fc9731a8c27ab3ba.jpg)  
Govt.Invst.- Pvt. Invst.

![](images/66e67295842b1505528d8bc8c4bfa5b35d7de21ae4dc517b6e40ee7b3b503a0c.jpg)

![](images/c9a93be63423d2c00a737d620bc21f286c91bf8901e583d72cacabcfb0dea076.jpg)  
Figure 7.18 The squared coherencies between the various series displayed in Figure 7.16 in terms of the number of cycles in 160 quarters.

Table 7.9 Magnitudes of the Eigenvectors in Example 7.16   

<table><tr><td>Series</td><td>Unemp</td><td>GNP</td><td>Cons</td><td>G. Inv.</td><td>P. Inv.</td></tr><tr><td>|ˆ1(10/160) |</td><td>0.51</td><td>0.51</td><td>0.57</td><td>0.05</td><td>0.41</td></tr><tr><td>|ˆ2(5/160) |</td><td>0.17</td><td>0.03</td><td>0.39</td><td>0.87</td><td>0.27</td></tr></table>

# 7.9 The Spectral Envelope

The concept of spectral envelope for the spectral analysis and scaling of categorical time series was first introduced in Stoffer et al. (1993). Since then, the idea has been extended in various directions (not only restricted to categorical time series), and we will explore these problems as well. First, we give a brief

![](images/211f72278cec48f7484c9fa1f42b75cb7d6210033affc6ba7ee1065ffbf83117.jpg)

![](images/5c42215e27f4194aa894ca7de704cd2af3afc0a534efc2bca70d81b5a9154c0d.jpg)  
Figure 7.19 The first, $\widehat { \lambda } _ { 1 } ( \omega )$ , and second, $\widehat { \lambda } _ { 2 } ( \omega )$ , eigenvalues (scaled by 1000) of the estimated spectral matrix ${ \widehat { f } } _ { x x } ( \omega )$ in terms of the number of cycles in 160 quarters.

introduction to the concept of scaling time series.

The spectral envelope was motivated by collaborations with researchers who collected categorical-valued time series with an interest in the cyclic behavior of the data. For example, Table 7.10 shows the per-minute sleep state of an infant taken from a study on the effects of prenatal exposure to alcohol. Details can be found in Stoffer et al. (1988), but, briefly, an electroencephalographic (EEG) sleep recording of approximately two hours is obtained on a full-term infant 24 to 36 hours after birth, and the recording is scored by a pediatric neurologist for sleep state. Sleep state is categorized, per minute, into one of six possible states: qt: quiet sleep - trace alternant, qh: quiet sleep - high voltage, tr: transitional sleep, al: active sleep - low voltage, ah: active sleep - high voltage, and aw: awake. This particular infant was never awake during the study.

It is not difficult to notice a pattern in the data if we concentrate on active vs. quiet sleep (that is, focus on the first letter). But, it would be difficult to try to assess patterns in a longer sequence, or if more categories were present, without some graphical aid. One simple method would be to scale the data, that is, assign numerical values to the categories and then draw a time plot of the scaled series. Because the states have an order, one obvious scaling is

$$
1 = q t \quad 2 = q h \quad 3 = t r \quad 4 = a l \quad 5 = a h \quad 6 = a w, \tag {7.180}
$$

and Figure 7.20 shows the time plot using this scaling. Another interesting scaling might be to combine the quiet states and the active states:

$$
1 = q t \quad 1 = q h \quad 2 = t r \quad 3 = a l \quad 3 = a h \quad 4 = a w. \tag {7.181}
$$

The time plot using (7.181) would be similar to Figure 7.20 as far as the

![](images/10823bb68b2782d534f1031d5bf67aac4a210f6a334cb079b8a055b4c5c0dd66.jpg)  
Figure 7.20 Time plot of the EEG sleep state data in Table 7.10 using the scaling in (7.180).

cyclic (in and out of quiet sleep) behavior of this infant’s sleep pattern. Figure 7.21 shows the periodogram of the sleep data using the scaling in (7.180). A large peak exists at the frequency corresponding to one cycle every 60 minutes. As we might imagine, the general appearance of the periodogram using the scaling (7.181) (not shown) is similar to Figure 7.20. Most of us would feel comfortable with this analysis even though we made an arbitrary and ad hoc choice about the particular scaling. It is evident from the data (without any

Table 7.10 Infant EEG Sleep-States (per minute) (read down and across)   

<table><tr><td>ah</td><td>qt</td><td>qt</td><td>al</td><td>tr</td><td>qt</td><td>al</td><td>ah</td></tr><tr><td>ah</td><td>qt</td><td>qt</td><td>ah</td><td>tr</td><td>qt</td><td>al</td><td>ah</td></tr><tr><td>ah</td><td>qt</td><td>tr</td><td>ah</td><td>tr</td><td>qt</td><td>al</td><td>ah</td></tr><tr><td>ah</td><td>qt</td><td>al</td><td>ah</td><td>qh</td><td>qt</td><td>al</td><td>ah</td></tr><tr><td>ah</td><td>qt</td><td>al</td><td>ah</td><td>qh</td><td>qt</td><td>al</td><td>ah</td></tr><tr><td>ah</td><td>tr</td><td>al</td><td>ah</td><td>qt</td><td>qt</td><td>al</td><td>ah</td></tr><tr><td>ah</td><td>qt</td><td>al</td><td>ah</td><td>qt</td><td>qt</td><td>al</td><td>ah</td></tr><tr><td>ah</td><td>qt</td><td>al</td><td>ah</td><td>qt</td><td>qt</td><td>al</td><td>ah</td></tr><tr><td>tr</td><td>qt</td><td>tr</td><td>tr</td><td>qt</td><td>qt</td><td>al</td><td>tr</td></tr><tr><td>ah</td><td>qt</td><td>ah</td><td>tr</td><td>qt</td><td>tr</td><td>al</td><td></td></tr><tr><td>tr</td><td>qt</td><td>al</td><td>ah</td><td>qt</td><td>al</td><td>al</td><td></td></tr><tr><td>ah</td><td>qt</td><td>al</td><td>ah</td><td>qt</td><td>al</td><td>al</td><td></td></tr><tr><td>ah</td><td>qt</td><td>al</td><td>ah</td><td>qt</td><td>al</td><td>al</td><td></td></tr><tr><td>qh</td><td>qt</td><td>al</td><td>ah</td><td>qt</td><td>al</td><td>ah</td><td></td></tr></table>

![](images/d14dbd32a04e147a404097eaab08095a486feba3cfbb89c473c2e3560f88768a.jpg)  
Figure 7.21 Periodogram of the EEG sleep state data in Figure 7.20 based on the scaling in (7.180). The peak corresponds to a frequency of approximately one cycle every 60 minutes.

scaling) that if the interest is in infant sleep cycling, this particular sleep study indicates an infant cycles between active and quiet sleep at a rate of about one cycle per hour.

The intuition used in the previous example is lost when we consider a long DNA sequence. Briefly, a DNA strand can be viewed as a long string of linked nucleotides. Each nucleotide is composed of a nitrogenous base, a five carbon sugar, and a phosphate group. There are four different bases, and they can be grouped by size; the pyrimidines, thymine (T) and cytosine (C), and the purines, adenine (A) and guanine (G). The nucleotides are linked together by a backbone of alternating sugar and phosphate groups with the $5 ^ { \prime }$ carbon of one sugar linked to the $3 ^ { \prime }$ carbon of the next, giving the string direction. DNA molecules occur naturally as a double helix composed of polynucleotide strands with the bases facing inwards. The two strands are complementary, so it is sufficient to represent a DNA molecule by a sequence of bases on a single strand. Thus, a strand of DNA can be represented as a sequence of letters, termed base pairs (bp), from the finite alphabet $\{ \mathtt { A } , \mathtt { C } , \mathtt { G } , \mathtt { T } \}$ . The order of the nucleotides contains the genetic information specific to the organism. Expression of information stored in these molecules is a complex multistage process. One important task is to translate the information stored in the protein-coding sequences (CDS) of the DNA. A common problem in analyzing long DNA sequence data is in identifying CDS dispersed throughout the sequence and separated by regions of noncoding (which makes up most of the DNA). Table 7.11 shows part of the Epstein–Barr virus (EBV) DNA sequence. The entire EBV DNA sequence

Table 7.11 Part of the Epstein–Barr Virus DNA Sequence (read across and down)   

<table><tr><td>AGAATTCGTC</td><td>TTGCTCTATT</td><td>CACCCTTACT</td><td>TTTCTTCTTG</td><td>CCCGTTCTCT</td><td>TTCTTAGTAT</td></tr><tr><td>GAATCCAGTA</td><td>TGCCTGCCTG</td><td>TAATTGTTGC</td><td>GCCCTACCTC</td><td>TTTTGGCTGG</td><td>CGGCTATTGC</td></tr><tr><td>CGCCTCGTGT</td><td>TTCACGGCCT</td><td>CAGTTAGTAC</td><td>CGTTGTGACC</td><td>GCCACCGGCT</td><td>TGGCCCTCTC</td></tr><tr><td>ACTTCTACTC</td><td>TTGGCAGCAG</td><td>TGGCCAGCTC</td><td>ATATGCCGCT</td><td>GCACAAAGGA</td><td>AACTGCTGAC</td></tr><tr><td>ACCGGTGACA</td><td>GTGCTTACTG</td><td>CGGTTGTCAC</td><td>TTGTGAGTAC</td><td>ACACGCACCA</td><td>TTTACAATGC</td></tr><tr><td>ATGATGTTCG</td><td>TGAGATTGAT</td><td>CTGTCTCTAA</td><td>CAGTTCACTT</td><td>CCTCTGCTTT</td><td>TCTCCTCAGT</td></tr><tr><td>CTTTGCAATT</td><td>TGCCTAACAT</td><td>GGAGGATTGA</td><td>GGACCCACCT</td><td>TTTAATTCTC</td><td>TTCTGTTTGC</td></tr><tr><td>ATTGCTGGCC</td><td>GCAGCTGGCG</td><td>GACTACAAGG</td><td>CATTTACGGT</td><td>TAGTGTGCCT</td><td>CTGTTATGAA</td></tr><tr><td>ATGCAGGTTT</td><td>GACTTCATAT</td><td>GTATGCCTTG</td><td>GCATGACGTC</td><td>AACTTTACTT</td><td>TTATTTCAGT</td></tr><tr><td>TCTGGTGATG</td><td>CTTGTGCTCC</td><td>TGATACTAGC</td><td>GTACAGAAGG</td><td>AGATGGCGCC</td><td>GTTTGACTGT</td></tr><tr><td>TTGTGGCGGC</td><td>ATCATGTTTT</td><td>TGGCATGTGT</td><td>ACTTGTCCTC</td><td>ATCGTCGACG</td><td>CTGTTTTGCA</td></tr><tr><td>GCTGAGTCCC</td><td>CTCCTTGGAG</td><td>CTGTAACTGT</td><td>GGTTTCCATG</td><td>ACGCTGCTGC</td><td>TACTGGCTTT</td></tr><tr><td>CGTCCTCTGG</td><td>CTCTCTTCGC</td><td>CAGGGGGCCT</td><td>AGGTACTCTT</td><td>GGTGCAGCCC</td><td>TTTTAACATT</td></tr><tr><td>GGCAGCAGGT</td><td>AAGCCACACG</td><td>TGTGACATTG</td><td>CTTGCCTTTT</td><td>TGCCACATGT</td><td>TTTCTGGACA</td></tr><tr><td>CAGGACTAAC</td><td>CATGCCATCT</td><td>CTGATTATAG</td><td>CTCTGGCACT</td><td>GCTAGCGTCA</td><td>CTGATTTTGG</td></tr><tr><td>GCACACTTAA</td><td>CTTGACTACA</td><td>ATGTTCCTTC</td><td>TCATGCTCCT</td><td>ATGGACACTT</td><td>GGTAAGTTTT</td></tr><tr><td>CCCTTCCTTT</td><td>AACTCATTAC</td><td>TTGTTCTTTT</td><td>GTAATCGCAG</td><td>CTCTAACTTG</td><td>GCATCTCTTT</td></tr><tr><td>TACAGTGGTT</td><td>CTCCTGATTT</td><td>GCTCTTCGTG</td><td>CTCTTCATGT</td><td>CCACTGAGCA</td><td>AGATCCTTCT</td></tr></table>

consists of approximately 172,000 bp.

We could try scaling according to the purine–pyrimidine alphabet, that is $\mathtt { A } = \mathtt { G } = 0$ and ${ \mathsf C } = { \mathsf T } = 1$ , but this is not necessarily of interest for every CDS of EBV. Numerous possible alphabets of interest exist. For example, we might focus on the strong–weak hydrogen-bonding alphabet ${ \mathsf C } = { \mathsf G } = 0$ and $\mathtt { A } = \mathtt { T } = 1$ . Although model calculations as well as experimental data strongly agree that some kind of periodic signal exists in certain DNA sequences, a large disagreement about the exact type of periodicity exists. In addition, a disagreement exists about which nucleotide alphabets are involved in the signals.

If we consider the naive approach of arbitrarily assigning numerical values (scales) to the categories and then proceeding with a spectral analysis, the result will depend on the particular assignment of numerical values. For example, consider the artificial sequence ACGTACGTACGT. . . . Then, setting $\mathtt { A } = \mathtt { G } = 0$ and ${ \mathsf C } = { \mathsf T } = 1$ yields the numerical sequence 010101010101. . . , or one cycle every two base pairs. Another interesting scaling is $\mathtt { A } = 1 , \mathtt { C } = \mathtt { j } , \mathtt { G } = \mathtt { j }$ , and $\mathrm { T } = 4$ , which results in the sequence 123412341234. . . , or one cycle every four bp. In this example, both scalings (that is, $\{ \mathsf { A } , \mathsf { C } , \mathsf { G } , \mathsf { T } \} = \{ 0 , 1 , 0 , 1 \}$ and $\{ \mathtt { A }$ , ${ \sf C } , { \sf G } , { \sf T } \} = \left\{ 1 , 2 , 3 , 4 \right\} ,$ ) of the nucleotides are interesting and bring out different properties of the sequence. Hence, we do not want to focus on only one scaling. Instead, the focus should be on finding all possible scalings that bring out all of the interesting features in the data. Rather than choose values arbitrarily, the spectral envelope approach selects scales that help emphasize any periodic feature that exists in a categorical time series of virtually any length in a quick and automated fashion. In addition, the technique can help in determining whether a sequence is merely a random assignment of categories.

# The Spectral Envelope for Categorical Time Series

As a general description, the spectral envelope is a frequency-based, principal components technique applied to a multivariate time series. First, we will focus on the basic concept and its use in the analysis of categorical time series. Technical details can be found in Stoffer et al. (1993).

Briefly, in establishing the spectral envelope for categorical time series, the basic question of how to efficiently discover periodic components in categorical time series was addressed. This, was accomplished via nonparametric spectral analysis as follows. Let $x _ { t }$ , $t = 0$ , $\pm 1$ , ±2, . . . , be a categorical-valued time series with finite state-space $\mathcal { C } = \{ c _ { 1 } , c _ { 2 } , . . . , c _ { k } \}$ . Assume $x _ { t }$ is stationary and $p _ { j } = \mathrm { P r } \{ x _ { t } = c _ { j } \} > 0$ for $j = 1$ , 2, . . . , $k$ . For $\beta = ( \beta _ { 1 } , \beta _ { 2 } , . . . , \beta _ { k } ) ^ { \prime } \in \mathbf { R } ^ { k }$ , denote by $x _ { t } ( \beta )$ the real-valued stationary time series corresponding to the scaling that assigns the category $c _ { j }$ the numerical value $\beta _ { j }$ , $j = 1$ , 2, . . . , $k$ . The spectral density of $x _ { t } ( \beta )$ will be denoted by $f _ { x x } ( \omega ; \beta )$ . The goal is to find scalings $\beta$ , so the spectral density is in some sense interesting, and to summarize the spectral information by what is called the spectral envelope.

In particular, $\beta$ is chosen to maximize the power at each frequency, $\omega$ , of interest, relative to the total power $\sigma ^ { 2 } ( \beta ) = \mathrm { v a r } \{ x _ { t } ( \beta ) \}$ . That is, we chose $\beta ( \omega )$ , at each $\omega$ of interest, so

$$
\lambda (\omega) = \max  _ {\beta} \left\{\frac {f _ {x x} (\omega ; \boldsymbol {\beta})}{\sigma^ {2} (\boldsymbol {\beta})} \right\}, \tag {7.182}
$$

over all $\beta$ not proportional to ${ \bf 1 } _ { k }$ , the $k \times 1$ vector of ones. Note, $\lambda ( \omega )$ is not defined if $\beta = a \mathbf { 1 } _ { k }$ for $a \in \mathbf { R }$ because such a scaling corresponds to assigning each category the same value $a$ ; in this case, $f _ { x x } ( \omega ; \beta ) \equiv 0$ and $\sigma ^ { 2 } \left( \beta \right) = 0$ . The optimality criterion $\lambda ( \omega )$ possesses the desirable property of being invariant under location and scale changes of $\beta$ .

As in most scaling problems for categorical data, it is useful to represent the categories in terms of the unit vectors $\mathbf { u } _ { 1 }$ , $\mathbf { \delta } \mathbf { u } _ { 2 } , \ldots .$ , $\mathbf { \pmb { u } } _ { k }$ , where $\mathbf { \Delta } \mathbf { u } _ { j }$ represents the $k \times 1$ vector with a one in the $j$ -th row, and zeros elsewhere. We then define a $k$ -dimensional stationary time series $\pmb { y } _ { t }$ by ${ \bf { y } } _ { t } \ = \ { \bf { u } } _ { j }$ when $x _ { t } ~ = ~ c _ { j }$ . The time series $x _ { t } ( \beta )$ can be obtained from the ${ \pmb y } _ { t }$ time series by the relationship $x _ { t } ( \beta ) = \beta ^ { \prime } y _ { t }$ . Assume the vector process $\pmb { y } _ { t }$ has a continuous spectral density denoted by $f _ { y y } ( \omega )$ . For each $\omega$ , $f _ { y y } ( \omega )$ is, of course, a $k \times k$ complex-valued Hermitian matrix. The relationship $x _ { t } ( \beta ) \ = \ \beta ^ { \prime } y _ { t }$ implies $f _ { x x } ( \omega ; \beta ) = \beta ^ { \prime } f _ { y y } ( \omega ) \beta = \beta ^ { \prime } f _ { y y } ^ { r e } ( \omega ) \beta$ , where $f _ { y y } ^ { r e } ( \omega )$ denotes the real part2 of $f _ { y y } ( \omega )$ . The imaginary part disappears from the expression because it is skew-symmetric, that is, $f _ { y y } ^ { i m } ( \omega ) ^ { \prime } = - f _ { y y } ^ { i m } ( \omega )$ . The optimality criterion can thus be expressed as

$$
\lambda (\omega) = \max  _ {\beta} \left\{\frac {\beta^ {\prime} f _ {y y} ^ {r e} (\omega) \boldsymbol {\beta}}{\beta^ {\prime} V \boldsymbol {\beta}} \right\}, \tag {7.183}
$$

where $V$ is the variance–covariance matrix of ${ \pmb y } _ { t }$ . The resulting scaling $\beta ( \omega )$ is called the optimal scaling.

The ${ \pmb y } _ { t }$ process is a multivariate point process, and any particular component of ${ \pmb y } _ { t }$ is the individual point process for the corresponding state (for example, the first component of $\mathbf { \pmb { y } } _ { t }$ indicates whether the process is in state $c _ { 1 }$ at time $t$ ). For any fixed $t$ , ${ \pmb y } _ { t }$ represents a single observation from a simple multinomial sampling scheme. It readily follows that $V = D - p \ p ^ { \prime }$ , where $p = ( p _ { 1 } , . . . , p _ { k } ) ^ { \prime }$ , and $D$ is the $k \times k$ diagonal matrix $D = \mathrm { d i a g } \{ p _ { 1 } , . . . , p _ { k } \}$ . Because, by assumption, $p _ { j } > 0$ for $j = 1$ , $2 , \ldots , k$ , it follows that $\operatorname { r a n k } ( V ) = k - 1$ with the null space of $V$ being spanned by ${ \bf 1 } _ { k }$ . For any $\boldsymbol { k } \times ( \boldsymbol { k } - 1 )$ full rank matrix $Q$ whose columns are linearly independent of ${ \bf 1 } _ { k }$ , $Q ^ { \prime } V Q$ is a $\left( k - 1 \right) \times \left( k - 1 \right)$ positive-definite symmetric matrix.

With the matrix $Q$ as previously defined, define $\lambda ( \omega )$ to be the largest eigenvalue of the determinantal equation

$$
| Q ^ {\prime} f _ {y y} ^ {r e} (\omega) Q - \lambda (\omega) Q ^ {\prime} V Q | = 0,
$$

and let $\pmb { b } ( \omega ) \in \mathbf { R } ^ { k - 1 }$ be any corresponding eigenvector, that is,

$$
Q ^ {\prime} f _ {y y} ^ {r e} (\omega) Q \boldsymbol {b} (\omega) = \lambda (\omega) Q ^ {\prime} V Q \boldsymbol {b} (\omega).
$$

The eigenvalue $\lambda ( \omega ) \geq 0$ does not depend on the choice of $Q$ . Although the eigenvector $\pmb { b } ( \omega )$ depends on the particular choice of $Q$ , the equivalence class of scalings associated with $\beta ( \omega ) = Q b ( \omega )$ does not depend on $Q$ . A convenient choice of $Q$ is $Q = [ I _ { k - 1 } \ | \ \mathbf { 0 } \ ] ^ { \prime }$ , where $I _ { k - 1 }$ is the $( k - 1 ) \times ( k - 1 )$ identity matrix and $\mathbf { 0 }$ is the $( k - 1 ) \times 1$ vector of zeros . For this choice, $Q ^ { \prime } f _ { y y } ^ { r e } ( \omega ) Q$ and $Q ^ { \prime } V Q$ are the upper $( k - 1 ) \times ( k - 1 )$ blocks of $f _ { y y } ^ { r e } ( \omega )$ and $V$ , respectively. This choice corresponds to setting the last component of $\beta ( \omega )$ to zero.

The value $\lambda ( \omega )$ itself has a useful interpretation; specifically, $\lambda ( \omega ) d \omega$ represents the largest proportion of the total power that can be attributed to the frequencies $( \omega , \omega + d \omega )$ for any particular scaled process $x _ { t } ( \beta )$ , with the maximum being achieved by the scaling $\beta ( \omega )$ . Because of its central role, $\lambda ( \omega )$ is defined to be the spectral envelope of a stationary categorical time series.

The name spectral envelope is appropriate since $\lambda ( \omega )$ envelopes the standardized spectrum of any scaled process. That is, given any $\beta$ normalized so that $x _ { t } ( \beta )$ has total power one, $f _ { x x } ( \omega ; \beta ) \le \lambda ( \omega )$ with equality if and only if $\beta$ is proportional to $\beta ( \omega )$ .

Given observations $x _ { t }$ , for $t = 1 , \ldots , n$ , on a categorical time series, we form the multinomial point process $\mathbf { \mathscr { y } } _ { t }$ , for $t = 1 , \ldots , n$ . Then, the theory for estimating the spectral density of a multivariate, real-valued time series can be applied to estimating $f _ { y y } ( \omega )$ , the $k \times k$ spectral density of $\mathbf { \mathscr { y } } _ { t }$ . Given an estimate $\widehat { f } _ { y y } ( \omega )$ of $f _ { y y } ( \omega )$ , estimates $\widehat { \lambda } ( \omega )$ and $\widehat { \beta } ( \omega )$ of the spectral envelope, $\lambda ( \omega )$ , and the corresponding scalings, $\beta ( \omega )$ , can then be obtained. Details on estimation and inference for the sample spectral envelope and the optimal scalings can be found in Stoffer et al. (1993), but the main result of that paper is as follows:

If $\widehat { f } _ { y y } ( \omega )$ is a consistent spectral estimator and if for each $j = 1 , . . . , \ J$ $J$ , the largest root of $f _ { y y } ^ { r e } ( \omega _ { j } )$ is distinct, then

$$
\left\{\eta_ {n} \left[ \widehat {\lambda} \left(\omega_ {j}\right) - \lambda \left(\omega_ {j}\right) \right] / \lambda \left(\omega_ {j}\right), \eta_ {n} \left[ \widehat {\boldsymbol {\beta}} \left(\omega_ {j}\right) - \boldsymbol {\beta} \left(\omega_ {j}\right) \right]; j = 1, \dots , J \right\} \tag {7.184}
$$

converges ( $n \to \infty$ ) jointly in distribution to independent zero-mean, normal, distributions, the first of which is standard normal; the asymptotic covariance structure of $\widehat { \beta } ( \omega _ { j } )$ is discussed in Stoffer et al. (1993). Result (7.184) is similar to (7.151), but in this case, $\beta ( \omega )$ and $\widehat { \beta } ( \omega )$ are real. The term $\eta _ { n }$ is the same as in (7.184), and its value depends on the type of estimator being used. Based on these results, asymptotic normal confidence intervals and tests for $\lambda ( \omega )$ can be readily constructed. Similarly, for $\beta ( \omega )$ , asymptotic confidence ellipsoids and chi-square tests can be constructed; details can be found in Stoffer et al. (1993, Theorems 3.1 – 3.3).

Peak searching for the smoothed spectral envelope estimate can be aided using the following approximations. Using a first-order Taylor expansion, we have

$$
\log \widehat {\lambda} (\omega) \approx \log \lambda (\omega) + \frac {\widehat {\lambda} (\omega) - \lambda (\omega)}{\lambda (\omega)}, \tag {7.185}
$$

so $\eta _ { n } [ \log \widehat { \lambda } ( \omega ) - \log \lambda ( \omega ) ]$ is approximately standard normal. It follows that $E [ \log \widehat { \lambda } ( \omega ) ] \approx \log \lambda ( \omega )$ and $\mathrm { v a r } [ \log \widehat { \lambda } ( \omega ) ] \approx \eta _ { n } ^ { - 2 }$ . If no signal is present in a sequence of length $n$ , we expect $\lambda ( j / n ) \approx 2 / n$ for $1 < j < n / 2$ , and hence approximately $( 1 - \alpha ) \times 1 0 0 \%$ of the time, $\log \widehat { \lambda } ( \omega )$ will be less than $\log ( 2 / n ) + ( z _ { \alpha } / \eta _ { n } )$ , where $z _ { \alpha }$ is the $( 1 - \alpha )$ upper tail cutoff of the standard normal distribution. Exponentiating, the $\alpha$ critical value for $\widehat { \lambda } ( \omega )$ becomes $( 2 / n ) \exp ( z _ { \alpha } / \eta _ { n } )$ . Useful values of $z _ { \alpha }$ are $z _ { . 0 0 1 } = 3 . 0 9$ , $z _ { . 0 0 0 1 } = 3 . 7 1$ , and $z _ { . 0 0 0 0 1 } = 4 . 2 6$ , and from our experience, thresholding at these levels works well.

# Example 7.17 Spectral Analysis of DNA Sequences

We give explicit instructions for the calculations involved in estimating the spectral envelope of a DNA sequence, $x _ { t }$ , for $t = 1 , \ldots , n$ , using the nucleotide alphabet.

In this example, we hold the scale for T fixed at zero. In this case, we form the $3 \times 1$ data vectors ${ \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } \mathbf { } } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf } { \mathbf } { \mathbf { } } { \mathbf } { \mathbf } { \mathbf } { \mathbf } { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf { } \mathbf $ :

$$
\boldsymbol {y} _ {t} = (1, 0, 0) ^ {\prime} \text {i f} x _ {t} = \mathsf {A}; \quad \boldsymbol {y} _ {t} = (0, 1, 0) ^ {\prime} \text {i f} x _ {t} = \mathsf {C};
$$

$$
\boldsymbol {y} _ {t} = (0, 0, 1) ^ {\prime} \text {i f} x _ {t} = \mathrm {G}; \quad \boldsymbol {y} _ {t} = (0, 0, 0) ^ {\prime} \text {i f} x _ {t} = \mathrm {T}.
$$

The scaling vector is $\beta = ( \beta _ { 1 } , \beta _ { 2 } , \beta _ { 3 } ) ^ { \prime }$ , and the scaled process is $x _ { t } ( \beta ) =$ $\beta ^ { \prime } { \boldsymbol y } _ { t }$ .

Calculate the DFT of the data

$$
\boldsymbol {Y} (j / n) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {y} _ {t} \exp (- 2 \pi i t j / n).
$$

![](images/f9bf6b5148dbdb4139ec05221d1fee740fefa8369e9cb29d13bb1037aae85f03.jpg)  
Figure 7.22 Smoothed sample spectral envelope of the BNRF1 gene from the Epstein–Barr virus.

Note $Y ( j / n )$ is a 3×1 complex-valued vector. Calculate the periodogram, $I ( j / n ) = Y ( j / n ) { \cal Y } ^ { * } ( j / n )$ , for $j = 1 , \ldots , [ n / 2 ]$ , and retain only the real part, say, $I ^ { r e } ( j / n )$ .

• Smooth the $I ^ { r e } ( j / n )$ to obtain an estimate of $f _ { y y } ^ { r e } ( j / n )$ . For example, using (7.150) with $L = 3$ and triangular weighting, we would calculate

$$
\widehat {f} _ {y y} ^ {r e} (j / n) = \frac {1}{4} I ^ {r e} \left(\frac {j - 1}{n}\right) + \frac {1}{2} I ^ {r e} \left(\frac {j}{n}\right) + \frac {1}{4} I ^ {r e} \left(\frac {j + 1}{n}\right).
$$

• Calculate the $3 \times 3$ sample variance–covariance matrix,

$$
S _ {y y} = n ^ {- 1} \sum_ {t = 1} ^ {n} \left(\boldsymbol {y} _ {t} - \overline {{\boldsymbol {y}}}\right) \left(\boldsymbol {y} _ {t} - \overline {{\boldsymbol {y}}}\right) ^ {\prime},
$$

where $\begin{array} { r } { \pmb { \overline { { y } } } = n ^ { - 1 } \sum _ { t = 1 } ^ { n } \pmb { y } _ { t } } \end{array}$ is the sample mean of the data.

For each $\omega _ { j } = j / n$ , $j = 0 , 1 , \ldots , [ n / 2 ]$ , determine the largest eigenvalue and the corresponding eigenvector of the matrix 2n−1S−1/2yy fy $2 n ^ { - 1 } S _ { y y } ^ { - 1 / 2 } \widehat { f } _ { y y } ^ { r e } ( \omega _ { j } ) S _ { y y } ^ { - 1 / 2 }$ fre > rey (ωj )S−1/2yy . Note, $S _ { y y } ^ { 1 / 2 }$ is the unique square root matrix of $S _ { y y }$ .   
• The sample spectral envelope $\widehat { \lambda } ( \omega _ { j } )$ is the eigenvalue obtained in the previous step. If $\pmb { b } ( \omega _ { j } )$ denotes the eigenvector obtained in the previous step, the optimal sample scaling is $\widehat \beta ( \omega _ { j } ) = S _ { y y } ^ { - 1 / 2 } \pmb { b } ( \omega _ { j } )$ ; this will result in three values, the value corresponding to the fourth category, T being held fixed at zero.

![](images/0df4773e6341363a3eb2ac5119d994090a87af10d26859f470c2ef9fcc8bf853.jpg)

![](images/33d6593a076e446387b16536cdef2dd4c8c6c59a2d198ace1171c16e529c227e.jpg)

![](images/4706f3fa4d1d6337b27d007898ad679415ac3c3a9415844ac66e6d035d31d024.jpg)

![](images/8bfe8f3d7f4835c2d192aadaa32993d1e01198a82cc6743f7bba65fe9b0f877b.jpg)  
Figure 7.23 Smoothed sample spectral envelope of the BNRF1 gene from the Epstein–Barr virus: (a) first 1000 bp, (b) second 1000 bp, (c) third 1000 bp, and (d) last 954 bp.

# Example 7.18 Dynamic Analysis of the Gene Labeled BNRF1 of the Epstein–Barr Virus

In this example, we focus on a dynamic (or sliding-window) analysis of the gene labeled BNRF1 (bp 1736-5689) of Epstein–Barr. Figure 7.22 shows the spectral envelope, using (7.150) with $L = 1 1$ and $h _ { 0 } = 6 / 3 6 , h _ { 1 } =$ $5 / 3 6 , \ldots , h _ { 5 } = 1 / 3 6$ , of the entire coding sequence (3954 bp long). The figure also shows a strong signal at frequency 1/3; the corresponding optimal scaling was $\mathtt { A } \ = \ 0 . 0 4 , \mathtt { C } \ = \ 0 . 7 1 , \mathtt { G } \ = \ 0 . 7 0 , \mathtt { T } \ = \ 0$ , which indicates the signal is in the strong–weak bonding alphabet, $S = \{ \mathtt { C } , \mathtt { G } \}$ and $W = \{ { \sf A } , { \sf T } \}$ .

Figure 7.23 shows the result of computing the spectral envelope over three nonoverlapping 1000-bp windows and one window of 954 bp, across the CDS, namely, the first, second, third, and fourth quarters of BNRF1.

An approximate 0.0001 significance threshold is .69%. The first three quarters contain the signal at the frequency $1 / 3$ (Figure 7.23a-c); the corresponding sample optimal scalings for the first three windows were (a) $\mathtt { A } = 0 . 0 6 , \mathtt { C } = 0 . 6 9 , \mathtt { G } = 0 . 7 2 , \mathtt { T } = 0$ ; (b) $\mathtt { A } = 0 . 0 9 , \mathtt { C } = 0 . 7 0 , \mathtt { G } =$ $0 . 7 1 ,  { \mathbb { T } } = 0$ ; (c) $\mathtt { A } = 0 . 1 8 , \mathtt { C } = 0 . 5 9 , \mathtt { G } = 0 . 7 7 , \mathtt { T } = 0$ . The first two windows are consistent with the overall analysis. The third section, however, shows some minor departure from the strong-weak bonding alphabet. The most interesting outcome is that the fourth window shows that no signal is present. This leads to the conjecture that the fourth quarter of BNRF1 of Epstein–Barr is actually noncoding.

# The Spectral Envelope for Real-Valued Time Series

The concept of the spectral envelope for categorical time series was extended to real-valued time series, $\{ x _ { t } ; t = 0 , \pm 1 , \pm 2 , \ldots , \}$ , in McDougall et al. (1997). The process $x _ { t }$ can be vector-valued, but here we will concentrate on the univariate case. Further details can be found in McDougall et al. (1997). The concept is similar to projection pursuit (Friedman and Stuetzle, 1981). Let $\mathcal { G }$ denote a $k$ -dimensional vector space of continuous real-valued transformations with $\{ g _ { 1 } , \ldots , g _ { k } \}$ being a set of basis functions satisfying $E [ g _ { i } ( x _ { t } ) ^ { 2 } ] < \infty$ , $i = 1 , \ldots , k$ . Analogous to the categorical time series case, define the scaled time series with respect to the set $\vec { \mathcal { G } }$ to be the real-valued process

$$
x _ {t} (\boldsymbol {\beta}) = \boldsymbol {\beta} ^ {\prime} \boldsymbol {y} _ {t} = \beta_ {1} g _ {1} (x _ {t}) + \dots + \beta_ {k} g _ {k} (x _ {t})
$$

obtained from the vector process

$$
\boldsymbol {y} _ {t} = \left(g _ {1} \left(X _ {t}\right), \dots , g _ {k} \left(X _ {t}\right)\right) ^ {\prime},
$$

where $\beta = ( \beta _ { 1 } , \ldots , \beta _ { k } ) ^ { \prime } \in \mathbf { R } ^ { k }$ . If the vector process, $\pmb { y } _ { t }$ , is assumed to have a continuous spectral density, say, $f _ { y y } ( \omega )$ , then $x _ { t } ( \beta )$ will have a continuous spectral density $f _ { x x } ( \omega ; \beta )$ for all $\beta \neq 0$ . Noting, $f _ { x x } ( \omega ; \pmb { \beta } ) = \beta ^ { \prime } f _ { y y } ( \omega ) \pmb { \beta } =$ $\beta ^ { \prime } f _ { y y } ^ { r e } ( \omega ) \beta$ , and $\sigma ^ { 2 } ( \beta ) = \mathrm { v a r } [ x _ { t } ( \beta ) ] = \beta ^ { \prime } V \beta$ , where $V = \mathrm { v a r } ( { \pmb y } _ { t } )$ is assumed to be positive definite, the optimality criterion

$$
\lambda (\omega) = \sup  _ {\boldsymbol {\beta} \neq \mathbf {0}} \left\{\frac {\boldsymbol {\beta} ^ {\prime} f _ {y y} ^ {r e} (\omega) \boldsymbol {\beta}}{\boldsymbol {\beta} ^ {\prime} V \boldsymbol {\beta}} \right\}, \tag {7.186}
$$

is well defined and represents the largest proportion of the total power that can be attributed to the frequency $\omega$ for any particular scaled process $x _ { t } ( \beta )$ . This interpretation of $\lambda ( \omega )$ is consistent with the notion of the spectral envelope introduced in the previous section and provides the following working definition: The spectral envelope of a time series with respect to the space $\vec { \mathcal { G } }$ is defined to be $\lambda ( \omega )$ .

The solution to this problem, as in the categorical case, is attained by finding the largest scalar $\lambda ( \omega )$ such that

$$
f _ {y y} ^ {r e} (\omega) \boldsymbol {\beta} (\omega) = \lambda (\omega) V \boldsymbol {\beta} (\omega) \tag {7.187}
$$

for $\beta ( \omega ) \neq 0$ . That is, $\lambda ( \omega )$ is the largest eigenvalue of $f _ { y y } ^ { r e } ( \omega )$ in the metric of $V$ , and the optimal scaling, $\beta ( \omega )$ , is the corresponding eigenvector.

If $x _ { t }$ is a categorical time series taking values in the finite state-space ${ \boldsymbol { S } } =$ $\{ c _ { 1 } , c _ { 2 } , \ldots , c _ { k } \}$ , where $c _ { j }$ represents a particular category, an appropriate choice for $\mathcal { G }$ is the set of indicator functions $g _ { j } ( x _ { t } ) = I ( x _ { t } = c _ { j } )$ . Hence, this is a natural generalization of the categorical case. In the categorical case, $\vec { \mathcal { G } }$ does not consist of linearly independent $_ { g }$ ’s, but it was easy to overcome this problem by reducing the dimension by one. In the vector-valued case, $\mathbf { \nabla } _ { \pmb { x } _ { t } } =$ $( x _ { 1 t } , \ldots , x _ { p t } ) ^ { \prime }$ , we consider $\vec { \mathcal { G } }$ to be the class of transformations from $\mathbf { R } ^ { p }$ into $\mathbf { R }$ such that the spectral density of $g ( \pmb { x } _ { t } )$ exists. One class of transformations of interest are linear combinations of ${ \pmb x } _ { t }$ . In Tiao et al. (1993), for example, linear transformations of this type are used in a time domain approach to investigate contemporaneous relationships among the components of multivariate time series. Estimation and inference for the real-valued case are analogous to the methods described in the previous section for the categorical case. We focus on two examples here; numerous other examples can be found in McDougall et al. (1997).

# Example 7.19 Residual Analysis

A relevant situation may be when $x _ { t }$ is the residual process obtained from some modeling procedure. If the fitted model is appropriate, the residuals should exhibit properties similar to an iid sequence. Departures of the data from the fitted model may suggest model misspecification, non-Gaussian data, or the existence of a nonlinear structure, and the spectral envelope would provide a simple diagnostic tool to aid in a residual analysis.

The series considered here is the quarterly U.S. real GNP which was analyzed in Chapter 3, Examples (3.35) and (3.36). Recall an MA(2) model was fit to the growth rate, and the residuals from this fit are plotted in Figure 3.16 . As discussed in Example (3.36), the residuals from the model fit appear to be uncorrelated; there appears to be one or two outliers, but their magnitudes are not that extreme. In addition, the standard residual analyses showed no obvious structure among the residuals.

Although the MA(2) model appears to be appropriate, Tiao and Tsay (1994) investigated the possibility of nonlinearities in GNP growth rate. Their overall conclusion was that there is subtle nonlinear behavior in the data because the economy behaves differently during expansion periods than during recession periods.

The spectral envelope, used as a diagnostic tool on the residuals, clearly indicates the MA(2) model is not adequate, and that further analysis is warranted. Here, the generating set ${ \mathcal { G } } = \{ x , | x | , x ^ { 2 } \}$ —which seems natural for a residual analysis—was used to estimate the spectral envelope

![](images/e8e204388cc6e3e61294d2e722c49853a1e1324a5bbd59f28f9ffb77a02a679f.jpg)  
Figure 7.24 Spectral envelope with respect to $\mathcal { G } = \{ x , | x | , x ^ { 2 } \}$ of the residuals from an MA(2) fit to the U.S. GNP growth rate data.

for the residuals from the MA(2) fit, and the result is plotted in Figure 7.24. A smoothed periodogram estimate was obtained using $L = 2 1$ and triangular weighting, $h _ { 0 } = 1 1 / 1 2 1 , h _ { \pm 1 } = 1 0 / 1 2 1 , \ldots , h _ { \pm 1 0 } = 1 / 1 2 1$ in (7.150). Clearly, the residuals are not iid, and considerable power is present at the low frequencies. The presence of spectral power at very low frequencies in detrended economic series has been frequently reported and is typically associated with long-range dependence. In fact, our choice of $\mathcal { G }$ was partly influenced by the work of Ding et al. (1993) who applied transformations of the form $| x _ { t } | ^ { d }$ , for $d \in ( 0 , 3 ]$ , to the S&P 500 stock market series. The estimated optimal transformation at the first nonzero frequency, $\omega = 0 . 0 0 6$ , was $\widehat { \beta } ( 0 . 0 0 6 ) = ( 1 , 2 0 , - 2 9 1 6 ) ^ { \prime }$ , which leads to the transformation

$$
y = x + 2 0 | x | - 2 9 1 6 x ^ {2}. \tag {7.188}
$$

This transformation is plotted in Figure 7.25. The transformation given in (7.188) is basically the absolute value (with some slight curvature and asymmetry) for most of the residual values, but the effect of extremevalued residuals (outliers) is dampened.

# Example 7.20 Optimal Transformations

In this example, we consider a contrived data set, in which we know the optimal transformation, say, $g _ { 0 }$ , and we determine whether the technology can find the transformation when $g _ { 0 }$ is not in $\vec { \mathcal { G } }$ . The data, $x _ { t }$ , are

![](images/b94bb89092bb680a8301fd8d67391ce8d48b3f2974062d7708f088271b0c196f.jpg)  
Figure 7.25 Estimated optimal transformation, (7.188), for the GNP residuals at $\omega = 0 . 0 0 6$ .

![](images/ce40c1ed9970c97bad183adc6078f7771cb8756fbc408d2086d5d5d927e5da1b.jpg)  
Figure 7.26 Periodogram, in decibels, of the data generated from (7.189) after tapering by a cosine bell.

generated by the nonlinear model

$$
x _ {t} = \exp \{3 \sin (2 \pi t \omega_ {0}) + \epsilon_ {t} \}, \quad t = 1, \dots , 5 1 2, \tag {7.189}
$$

where $\omega _ { 0 } = 5 1 / 5 1 2$ and $\epsilon _ { t }$ is white Gaussian noise with a variance of 16. This example is adapted from Breiman and Friedman (1985), where the ACE algorithm is introduced. The optimal transformation in this case is $g _ { 0 } ( x _ { t } ) = \ln ( x _ { t } )$ , wherein the data are generated from a sinusoid plus

![](images/714b3b650b58ba35687204e67fb8f0d62c6c05fcc312f84891a5a4efc52e0f5a.jpg)  
Figure 7.27 Spectral envelope with respect to $\mathcal { G } = \{ x , \sqrt { x } , \sqrt [ 3 ] { x } \}$ of data generated from (7.189).

![](images/eb0e103e1f1e3518c1888b6166f648185c1364c49f0d34d2123f2ac4a2cc7718.jpg)  
Figure 7.28 Log transformation, $y = \ln ( x )$ (solid line), the estimated optimal transformation at $\omega _ { 0 }$ as given in (7.190) (dashed line), and the estimated optimal transformation at $\omega _ { 0 }$ using the inappropriate basis $\{ x , x ^ { 2 } , x ^ { 3 } \}$ (shortdashed line).

![](images/03348b9c03213a2d1846862bd996d80f955019517a5e46f70be1b9c069fd9fff.jpg)  
Figure 7.29 Spectral envelope with respect to $\mathcal { G } = \{ x , x ^ { 2 } , x ^ { 3 } \}$ .

noise. Of the 512 generated data, about 98% were less than 4000. Occasionally, the data values were extremely large (the data exceeded 100,000 about four times). The periodogram, in decibels $[ 1 0 \log _ { 1 0 } X ( \omega _ { j } ) ]$ , of the standardized and tapered (by a cosine bell) data is shown in Figure 7.26 and provides no evidence of any dominant frequency, including $\omega _ { 0 }$ .

In contrast, the sample spectral envelope (Figure 7.27) computed with respect to $\mathcal { G } = \{ x , \sqrt { x } , \sqrt [ 3 ] { x } \}$ has no difficulty in isolating $\omega _ { 0 }$ . No smoothing was used here; so, based on Stoffer et al. (1993, Theorem 3.2), an approximate 0.0001 null significance threshold for the spectral envelope is 4.84% (the null hypothesis being that $x _ { t }$ is iid).

Figure 7.28 compares the estimated optimal transformation with respect to $\mathcal { G }$ with the log transformation for values less than 4000. The estimated transformation at $\omega _ { 0 }$ is given by

$$
y = -. 6 + 0. 0 0 0 3 x - 0. 3 6 3 8 \sqrt {x} + 1. 9 3 0 4 \sqrt [ 3 ]{x}; \tag {7.190}
$$

that is, $\widehat { \pmb { \beta } } ( \omega _ { 0 } ) = ( 0 . 0 0 0 3 , - 0 . 3 6 3 8 , 1 . 9 3 0 4 ) ^ { \prime }$ after rescaling so (7.190) can be compared directly with $y = \ln ( x )$ .

Finally, it is worth mentioning the result obtained when the rather inappropriate basis, $\{ x , x ^ { 2 } , x ^ { 3 } \}$ , was used. Surprisingly, the spectral envelope in this case (Figure 7.29) looks similar to that of Figure 7.27. Also, the resulting estimated optimal transformation at $\omega _ { 0 }$ , is close to the log transformation. In fact, as seen in Figure 7.28, it looks like what we would imagine as a linear approximation to $y = \ln ( x )$ within the range of most of the data.

# Problems

# Section 7.2

7.1 Consider the complex Gaussian distribution for the random variable $\mathbf { \nabla } X =$ $X _ { c } \mathrm { ~ - ~ } i X _ { s }$ , as defined in (7.1)-(7.3), where the argument $\omega _ { k }$ has been suppressed. Now, the $2 p \times 1$ real random variable $\pmb { Z } = ( \pmb { X } _ { c } ^ { \prime } , \pmb { X } _ { s } ^ { \prime } ) ^ { \prime }$ has a multivariate normal distribution with density

$$
p (\boldsymbol {Z}) = (2 \pi) ^ {- p} | \Sigma | ^ {- 1 / 2} \exp \left\{- \frac {1}{2} (\boldsymbol {Z} - \boldsymbol {\mu}) ^ {\prime} \Sigma^ {- 1} (\boldsymbol {Z} - \boldsymbol {\mu}) \right\},
$$

where $\pmb { \mu } = ( \pmb { M } _ { c } ^ { \prime } , \pmb { M } _ { s } ^ { \prime } ) ^ { \prime }$ is the mean vector. Prove

$$
| \Sigma | = \left(\frac {1}{2}\right) ^ {2 p} | C - i Q | ^ {2},
$$

using the result that the eigenvectors and eigenvalues of $\Sigma$ occur in pairs, i.e., $( \pmb { v } _ { c } ^ { \prime } , \pmb { v } _ { s } ^ { \prime } ) ^ { \prime }$ and $( { \pmb v } _ { s } ^ { \prime } , - { \pmb v } _ { c } ^ { \prime } ) ^ { \prime }$ , where ${ \pmb v } _ { c } - i { \pmb v } _ { s }$ denotes the eigenvector of $f _ { x x }$ . Show that

$$
\frac {1}{2} \left(\boldsymbol {Z} - \boldsymbol {\mu}\right) ^ {\prime} \Sigma^ {- 1} \left(\boldsymbol {Z} - \boldsymbol {\mu}\right)) = \left(\boldsymbol {X} - \boldsymbol {M}\right) ^ {*} f ^ {- 1} \left(\boldsymbol {X} - \boldsymbol {M}\right)
$$

so $p ( \pmb { X } ) = p ( \pmb { Z } )$ and we can identify the density of the complex multivariate normal variable $\pmb { X }$ with that of the real multivariate normal $z$ .

7.2 Prove $\widehat { f }$ in (7.6) maximizes the log likelihood (7.5) by minimizing the negative of the log likelihood

$$
L \ln | f | + L \operatorname {t r} \left\{\widehat {f} f ^ {- 1} \right\}
$$

in the form

$$
L \sum_ {i} \left(\lambda_ {i} - \ln \lambda_ {i} - 1\right) + L p + L \ln | \widehat {f} |,
$$

where the $\lambda _ { i }$ values correspond to the eigenvalues in a simultaneous diagonalization of the matrices $f$ and $\hat { f }$ ; i.e., there exists a matrix $P$ such that $P ^ { * } f P = I$ and $P ^ { * } \hat { f } P = \mathrm { d i a g } \left( \lambda _ { 1 } , \ldots , \lambda _ { p } \right) = \Lambda .$ $( \lambda _ { 1 } , \ldots , \lambda _ { p } ) = \Lambda$ Note, $\lambda _ { i } - \ln \lambda _ { i } - 1 \geq 0$ with equality if and only if $\lambda _ { i } = 1$ , implying $\Lambda = I$ maximizes the log likelihood and $f = { \widehat { f } }$ is the maximimizing value.

# Section 7.3

7.3 Verify (7.19) and (7.20) for the mean-squared prediction error MSE in (7.12). Use the orthogonality principle, which implies

$$
M S E = E \left[ \left(y _ {t} - \sum_ {r = - \infty} ^ {\infty} \boldsymbol {\beta} _ {r} ^ {\prime} \boldsymbol {x} _ {t - r}\right) y _ {t} \right]
$$

and gives a set of equations involving the autocovariance functions. Then, use the spectral representations and Fourier transform results to get the final result.

7.4 Consider the predicted series

$$
\widehat {y} _ {t} = \sum_ {r = - \infty} ^ {\infty} \beta_ {r} ^ {\prime} \pmb {x} _ {t - r},
$$

where $\beta _ { r }$ satisfies (7.14). Show the ordinary coherence between $y _ { t }$ and $\widehat { y } _ { t }$ is exactly the multiple coherence (7.21).

7.5 Consider the complex regression model (7.29) in the form

$$
\boldsymbol {Y} = X \mathrm {B} + \boldsymbol {V},
$$

where $Y = ( Y _ { 1 } , Y _ { 2 } , \dots Y _ { L } ) ^ { \prime }$ denotes the observed DFTs after they have been re-indexed and $X = ( X _ { 1 } , X _ { 2 } , \ldots , X _ { L } ) ^ { \prime }$ is a matrix containing the reindexed input vectors. The model is a complex regression model with $Y = Y _ { c } - i { \pmb { Y } } _ { s } , X = X _ { c } - i X _ { s } , { \pmb { B } } = { \pmb { B } } _ { c } - i { \pmb { B } } _ { s }$ , and $V = V _ { c } - i V _ { s }$ denoting the representation in terms of the usual cosine and sine transforms. Show the partitioned real regression model involving the $2 L \times 1$ vector of cosine and sine transforms, say,

$$
\left( \begin{array}{c} \boldsymbol {Y} _ {c} \\ \boldsymbol {Y} _ {s} \end{array} \right) = \left( \begin{array}{c c} X _ {c c} & - X _ {s} \\ X _ {s} & X _ {c c} \end{array} \right) \left( \begin{array}{c} \boldsymbol {B} _ {c c} \\ \boldsymbol {B} _ {s} \end{array} \right) + \left( \begin{array}{c} \boldsymbol {V} _ {c c} \\ \boldsymbol {V} _ {s} \end{array} \right),
$$

is isomorphic to the complex regression regression model in the sense that the real and imaginary parts of the complex model appear as components of the vectors in the real regression model. Use the usual regression theory to verify (7.28) holds. For example, writing the real regression model as

$$
\boldsymbol {y} = x \boldsymbol {b} + \boldsymbol {v},
$$

the isomorphism would imply

$$
\begin{array}{l} L \left(\widehat {f} _ {y y} - \widehat {f} _ {x y} ^ {*} \widehat {f} _ {x x} ^ {- 1} \widehat {f} _ {x y}\right) = \boldsymbol {Y} ^ {*} \boldsymbol {Y} - \boldsymbol {Y} ^ {*} X \left(X ^ {*} X\right) ^ {- 1} X ^ {*} \boldsymbol {Y} \\ = \quad \boldsymbol {y} ^ {\prime} \boldsymbol {y} - \boldsymbol {y} ^ {\prime} x \left(x ^ {\prime} x\right) ^ {- 1} x ^ {\prime} \boldsymbol {y}. \\ \end{array}
$$

# Section 7.4

7.6 Consider estimating the function

$$
\psi_ {t} = \sum_ {r = - \infty} ^ {\infty} \boldsymbol {a} _ {r} ^ {\prime} \boldsymbol {\beta} _ {t - r}
$$

by a linear filter estimator of the form

$$
\widehat {\psi} _ {t} = \sum_ {r = - \infty} ^ {\infty} \boldsymbol {a} _ {r} ^ {\prime} \widehat {\boldsymbol {\beta}} _ {t - r},
$$

where $\hat { \beta } _ { t }$ is defined by (7.43). Show a sufficient condition for $\hat { \psi } _ { t }$ to be an unbiased estimator; i.e., $E$ $\widehat { \psi } _ { t } = \psi _ { t }$ , is

$$
H (\omega) Z (\omega) = I
$$

for all $\omega$ . Similarly, show any other unbiased estimator satisfying the above condition has minimum variance (see Shumway and Dean, 1968), so the estimator given is a best linear unbiased (BLUE) estimator.

7.7 Consider a linear model with mean value function $\mu _ { t }$ and a signal $\alpha _ { t }$ delayed by an amount $\tau _ { j }$ on each sensor, i.e.,

$$
y _ {j t} = \mu_ {t} + \alpha_ {t - \tau_ {j}} + v _ {j t}
$$

Show the estimators (7.43) for the mean and the signal are the Fourier transforms of

$$
\widehat {M} (\omega) = \frac {Y . (\omega) - \overline {{\phi (\omega)}} B _ {w} (\omega)}{1 - | \phi (\omega) | ^ {2}}
$$

and

$$
\widehat {A} (\omega) = \frac {B _ {w} (\omega) - \phi (\omega) Y _ {\cdot} (\omega)}{1 - | \phi (\omega) | ^ {2}},
$$

where

$$
\phi (\omega) = \frac {1}{N} \sum_ {j = 1} ^ {N} e ^ {2 \pi i \omega \tau_ {j}}
$$

and $B _ { w } ( \omega )$ is defined in (7.65).

# Section 7.5

7.8 Consider the estimator (7.68) as applied in the context of the random coefficient model (7.66). Prove the filter coefficients for the minimum mean square estimator can be determined from (7.69) and the mean square covariance is given by (7.72).

7.9 For the random coefficient model, verify the expected mean square of the regression power component is

$$
\begin{array}{l} E \left[ S S R \left(\omega_ {k}\right) \right] = E \left[ Y ^ {*} \left(\omega_ {k}\right) Z \left(\omega_ {k}\right) S _ {z} ^ {- 1} \left(\omega_ {k}\right) Z ^ {*} \left(\omega_ {k}\right) Y \left(\omega_ {k}\right) \right] \\ = L f _ {\beta} \left(\omega_ {k}\right) \operatorname {t r} \left\{S _ {z} \left(\omega_ {k}\right) \right\} + L q f _ {v} \left(\omega_ {k}\right). \\ \end{array}
$$

Recall, the underlying frequency domain model is

$$
\boldsymbol {Y} (\omega_ {k}) = Z (\omega_ {k}) \boldsymbol {B} (\omega_ {k}) + \boldsymbol {V} (\omega_ {k}),
$$

where $B ( \omega _ { k } )$ has spectrum $f _ { \beta } ( \omega _ { k } ) I _ { q }$ and $V ( \omega _ { k } )$ has spectrum $f _ { v } ( \omega _ { k } ) I _ { N }$ and the two processes are uncorrelated.

# Section 7.6

7.10 Suppose we have $I = 2$ groups and the models

$$
y _ {1 j t} = \mu_ {t} + \alpha_ {1 t} + v _ {1 j t}
$$

for the $j = 1 , \ldots , N$ observations in group 1 and

$$
y _ {2 j t} = \mu_ {t} + \alpha_ {2 t} + v _ {2 j t}
$$

for the $j = 1 , \ldots , N$ observations in group 2, with $\alpha _ { 1 t } + \alpha _ { 2 t } = 0$ . Suppose we want to test equality of the two group means; i.e.,

$$
y _ {i j t} = \mu_ {t} + v _ {i j t}, \quad i = 1, 2.
$$

Derive the residual and error power components corresponding to (7.84) and (7.85) for this particular case.

7.11 Verify the forms of the linear compounds involving the mean given in (7.91) and (7.92), using (7.89) and (7.90).

7.12 Show the ratio of the two smoothed spectra in (7.104) has the indicated $F$ -distribution when $f _ { 1 } ( \omega ) = f _ { 2 } ( \omega )$ . When the spectra are not equal, show the variable is proportional to an $F ^ { \prime }$ -distribution, where the proportionality constant depends on the ratio of the spectra.

# Section 7.7

7.13 The problem of detecting a signal in noise can be considered using the model

$$
x _ {t} = s _ {t} + w _ {t}, \quad t = 1, \dots , n,
$$

for $p _ { 1 } ( { \pmb x } )$ when a signal is present and the model

$$
x _ {t} = w _ {t}, \quad t = 1, \ldots , n,
$$

for $p _ { 2 } ( { \pmb x } )$ when no signal is present. Under multivariate normality, we might specialize even further by assuming the vector $\pmb { w } = ( w _ { 1 } , \ldots , w _ { n } ) ^ { \prime }$ has a multivariate normal distribution with mean 0 and covariance matrix $\Sigma = \sigma _ { w } ^ { 2 } I _ { n }$ , corresponding to white noise. Assuming the signal vector

$\pmb { \mathscr { s } } = ( \mathscr { s } _ { 1 } , \mathscr { \ldots } , \mathscr { s } _ { n } ) ^ { \prime }$ is fixed and known, show the discriminant function (7.113) becomes the matched filter

$$
\frac {1}{\sigma_ {w} ^ {2}} \sum_ {t = 1} ^ {n} s _ {t} x _ {t} - \frac {1}{2} \left(\frac {S}{N}\right) + \ln \frac {\pi_ {1}}{\pi_ {2}},
$$

where

$$
\left(\frac {S}{N}\right) = \frac {\sum_ {t = 1} ^ {n} s _ {t} ^ {2}}{\sigma_ {w} ^ {2}}
$$

denotes the signal-to-noise ratio. Give the decision criterion if the prior probabilities are assumed to be the same. Express the false alarm and missed signal probabilities in terms of the normal cdf and the signal-tonoise ratio.

7.14 Assume the same additive signal plus noise representations as in the previous problem, except, the signal is now a random process with a zero mean and covariance matrix $\sigma _ { s } ^ { 2 } I$ . Derive the comparable version of (7.116) as a quadratic detector, and characterize its performance under both hypotheses in terms of constant multiples of the chi-squared distribution.

# Section 7.8

7.15 The data set ch5fmri.dat contains data from other stimulus conditions in the fMRI experiment, as discussed in Example 7.14 (one location— Caudate—was left out of the analysis for brevity). Perform principal component analyses on the stimulus conditions (i) awake-heat and (ii) awake-shock, and compare your results to the results of Example 7.14.

7.16 For this problem, consider the first three earthquake series listed in eq+exp.dat.

(a) Estimate and compare the spectral density of the P component and then of the S component for each individual earthquake.   
(b) Estimate and compare the squared coherency between the $\mathrm { P }$ and S components of each individual earthquake. Comment on the strength of the coherence.   
(c) Let $x _ { t i }$ be the P component of earthquake $i = 1 , 2 , 3$ , and let $\mathbf { \nabla } _ { \pmb { x } _ { t } } =$ $( x _ { t 1 } , x _ { t 2 } , x _ { t 3 } ) ^ { \prime }$ be the $3 \times 1$ vector of P components. Estimate the spectral density, $\lambda _ { 1 } ( \omega )$ , of the first principal component series of ${ \pmb x } _ { t }$ Compare this to the corresponding spectra calculated in (a).   
(d) Analogous to part (c), let $\pmb { y } _ { t }$ denote the $3 \times 1$ vector series of S components of the first three earthquakes. Repeat the analysis of part (c) on ${ \pmb y } _ { t }$ .

7.17 In the factor analysis model (7.155), let $p = 3$ , $q = 1$ , and

$$
\Sigma_ {x x} = \left[ \begin{array}{c c c} 1 & . 4 & . 9 \\ . 4 & 1 & . 7 \\ . 9 & . 7 & 1 \end{array} \right].
$$

Show there is a unique choice for $\boldsymbol { \beta }$ and $D$ , but $\delta _ { 3 } ^ { 2 } < 0$ , so the choice is not valid.

7.18 Extend the EM algorithm for classical factor analysis, (7.161)-(7.166), to the time series case of maximizing $\ln { \cal L } \big ( B ( \omega _ { j } ) , D _ { \epsilon \epsilon } ( \omega _ { j } ) \big )$ in (7.177). Then, for the data used in Example 7.16, find the approximate maximum likelihood estimates of $B ( \omega _ { j } )$ and $D _ { \epsilon \epsilon } ( \omega _ { j } )$ , and, consequently, $\Lambda _ { t }$ .

# Section 7.9

7.19 Verify, as stated in (7.182), the imaginary part of a $k \times k$ spectral matrix, $f ^ { i m } ( \omega )$ , is skew symmetric, and then show $\beta ^ { \prime } f _ { y y } ^ { i m } ( \omega ) \beta = 0$ for a real $k \times 1$ vector, $\beta$ .

7.20 Repeat the analysis of Example 7.18 on BNRF1 of herpesvirus saimiri (the data file is bnrf1hvs.dat), and compare the results with the results obtained for Epstein–Barr.

7.21 For the NYSE returns, say, $r _ { t }$ , analyzed in Chapter 5, Example 5.4:

(a) Estimate the spectrum of the $r _ { t }$ . Does the spectral estimate appear to support the hypothesis that the returns are white?   
(b) Examine the possibility of spectral power near the zero frequency for a transformation of the returns, say, $g ( r _ { t } )$ , using the spectral envelope with Example 7.19 as your guide. Compare the optimal transformation near or at the zero frequency with the usual transformation $y _ { t } = r _ { t } ^ { 2 }$ .

# Appendix A Large Sample Theory

# A.1 Convergence Modes

The study of the optimality properties of various estimators (such as the sample autocorrelation function) depends, in part, on being able to assess the large-sample behavior of these estimators. We summarize briefly here the kinds of convergence useful in this setting, namely, mean square convergence, convergence in probability, and convergence in distribution.

We consider first a particular class of random variables that plays an important role in the study of second-order time series, namely, the class of random variables belonging to the space $L ^ { 2 }$ , satisfying $E | x | ^ { 2 } < \infty$ . In proving certain properties of the class $L ^ { 2 }$ we will often use, for random variables $x , y \in L ^ { 2 }$ , the Cauchy–Schwarz inequality,

$$
\left| E (x y) \right| ^ {2} \leq E \left(| x | ^ {2}\right) E \left(| y | ^ {2}\right), \tag {A.1}
$$

and the Tchebycheff inequality,

$$
P \{| x | \geq a \} \leq \frac {E (| x | ^ {2})}{a ^ {2}}, \tag {A.2}
$$

for $a > 0$ .

Next, we investigate the properties of mean square convergence of random variables in $L ^ { 2 }$ .

Definition A.1 A sequence of $L ^ { 2 }$ random variables $\{ x _ { n } \}$ , is said to converge in mean square to a random variable $x \in L ^ { 2 }$ , denoted by

$$
x _ {n} \stackrel {m s} {\rightarrow} x, \tag {A.3}
$$

if and only if

$$
E \left| x _ {n} - x \right| ^ {2} \rightarrow 0 \tag {A.4}
$$

$$
a s n \to \infty .
$$

# Example A.1 Mean Square Convergence of the Sample Mean

Consider the white noise sequence $w _ { t }$ and the signal plus noise series

$$
x _ {t} = \mu + w _ {t}.
$$

Then, because

$$
E | \bar {x} _ {n} - \mu | ^ {2} = \frac {\sigma_ {w} ^ {2}}{n} \rightarrow 0
$$

$n \to \infty$ , where $\begin{array} { r } { \bar { x } _ { n } = n ^ { - 1 } \sum _ { t = 1 } ^ { n } x _ { t } } \end{array}$ is the sample mean, we have $\bar { x } _ { n } ~ { \stackrel { m s } { \to } } ~ \mu$

We summarize some of the properties of mean square convergence as follows. If $x _ { n } \stackrel { m s } {  } x$ , and $y _ { n } \stackrel { m s } {  } y$ , then, as $n \to \infty$ ,

$$
\begin{array}{l} (i) \quad E \left(x _ {n}\right)\rightarrow E (x); (A.5) \\ (i i) \quad E \left(\left| x _ {n} \right| ^ {2}\right)\rightarrow E \left(\left| x \right| ^ {2}\right); (A.6) \\ (i i i) \quad E \left(x _ {n} y _ {n}\right)\rightarrow E (x y). (A.7) \\ \end{array}
$$

We also note the $L ^ { 2 }$ completeness theorem known as the Riesz–Fisher Theorem.

Theorem A.1 Let $\{ x _ { n } \}$ be a sequence in $L ^ { 2 }$ . Then, there exists an x in $L ^ { 2 }$ such that $x _ { n } \stackrel { m s } {  } x$ if and only if

$$
E \left| x _ {n} - x _ {m} \right| ^ {2} \rightarrow 0 \tag {A.8}
$$

for $m , n \to \infty$ .

Often the condition of Theorem A.1 is easier to verify to establish that a mean square limit $x$ exists without knowing what it is. Sequences that satisfy (A.8) are said to be Cauchy sequences in $L ^ { 2 }$ and (A.8) is also known as the Cauchy criterion for $L ^ { 2 }$ .

# Example A.2 Time Invariant Linear Filter

As an important example of the use of the Riesz–Fisher Theorem A.1 and the properties (i), (ii), and $( i i i )$ of mean square convergent series given in (A.5)–(A.7), a time-invariant linear filter is defined as a convolution of the form

$$
y _ {t} = \sum_ {j = - \infty} ^ {\infty} a _ {j} x _ {t - j} \tag {A.9}
$$

for each $t = 0 , \pm 1 , \pm 2 , . . .$ , where $x _ { t }$ is a weakly stationary input series with mean $\mu _ { x }$ and autocovariance function $\gamma _ { x } ( h )$ , and $a _ { j }$ , for $j =$ $0 , \pm 1 , \pm 2 , \ldots$ are constants satisfying

$$
\sum_ {j = - \infty} ^ {\infty} | a _ {j} | <   \infty . \tag {A.10}
$$

The output series $y _ { t }$ defines a filtering or smoothing of the input series that changes the character of the time series in a predictable way. We need to know the conditions under which the outputs $y _ { t }$ in (A.9) and the linear process (1.31) exist.

Considering the sequence

$$
y _ {t} ^ {n} = \sum_ {j = - n} ^ {n} a _ {j} x _ {t - j}, \tag {A.11}
$$

$n = 1 , 2 , \ldots$ , we need to show first that $y _ { t } ^ { n }$ has a mean square limit. By Theorem A.1, it is enough to show that

$$
E \left| y _ {t} ^ {n} - y _ {t} ^ {m} \right| ^ {2} \rightarrow 0
$$

as $m , n  \infty$ . For $n > m > 0$ ,

$$
\begin{array}{l} E \left| y _ {t} ^ {n} - y _ {t} ^ {m} \right| ^ {2} = E \left| \sum_ {m <   | j | \leq n} a _ {j} x _ {t - j} \right| ^ {2} \\ = \sum_ {m <   | j | \leq n} \sum_ {m \leq | k | \leq n} a _ {j} a _ {k} E \left(x _ {t - j} x _ {t - k}\right) \\ \leq \sum_ {m <   | j | \leq n} \sum_ {m \leq | k | \leq n} | a _ {j} | | a _ {k} | | E (x _ {t - j} x _ {t - k}) | \\ \leq \sum_ {m <   | j | \leq n} \sum_ {m \leq | k | \leq n} | a _ {j} | | a _ {k} | (E | x _ {t - j} | ^ {2}) ^ {1 / 2} (E | x _ {t - k} | ^ {2}) ^ {1 / 2} \\ = \gamma_ {x} (0) \left(\sum_ {m \leq | j | \leq n} | a _ {j} |\right) ^ {2} \rightarrow 0 \\ \end{array}
$$

as $m , n \to \infty$ , because $\gamma _ { x } ( 0 )$ is a constant and $\{ a _ { j } \}$ is absolutely summable (the second inequality follows from the Cauchy–Schwarz inequality).

Although we know that the sequence $\{ y _ { t } ^ { n } \}$ given by (A.11) converges in mean square, we have not established its mean square limit. It should be obvious, however, that $y _ { t } ^ { n } \overset { m s } {  } y _ { t }$ ms as $n \to \infty$ , where $y _ { t }$ is given by (A.9).1

Finally, we may use (A.5) and (A.7) to establish the mean, $\mu _ { y }$ and autocovariance function, $\gamma _ { y } ( h )$ of $y _ { t }$ . In particular we have,

$$
\mu_ {y} = \mu_ {x} \sum_ {j = - \infty} ^ {\infty} a _ {j}, \tag {A.12}
$$

and

$$
\begin{array}{l} \gamma_ {y} (h) = E \sum_ {j = - \infty} ^ {\infty} \sum_ {k = - \infty} ^ {\infty} a _ {j} \left(x _ {t + h - j} - \mu_ {x}\right) a _ {j} \left(x _ {t - k} - \mu_ {x}\right) \\ = \sum_ {j = - \infty} ^ {\infty} \sum_ {k = - \infty} ^ {\infty} a _ {j} \gamma_ {x} (h - j + k) a _ {k} \tag {A.13} \\ \end{array}
$$

A second important kind of convergence is convergence in probability.

Definition A.2 The sequence $\{ x _ { n } \}$ , for $n = 1 , 2 , \ldots$ , converges in probability to a random variable $x$ , denoted by

$$
x _ {n} \stackrel {{p}} {{\rightarrow}} x, \tag {A.14}
$$

if and only if

$$
P \left\{\left| x _ {n} - x \right| > \epsilon \right\}\rightarrow 0 \tag {A.15}
$$

for all $\epsilon > 0$ , as $n \to \infty$ .

An immediate consequence of the Tchebycheff inequality, (A.2), is that

$$
P \{| x _ {n} - x | \geq \epsilon \} \leq \frac {E (| x _ {n} - x | ^ {2})}{\epsilon^ {2}},
$$

so convergence in mean square implies convergence in probability, i.e.,

$$
x _ {n} \stackrel {m s} {\rightarrow} x \Rightarrow x _ {n} \stackrel {p} {\rightarrow} x. \tag {A.16}
$$

This result implies, for example, that the filter (A.9) exists as a limit in probability because it converges in mean square [it is also easily established that (A.9) exists with probability one]. We mention, at this point, the useful Weak Law of Large Numbers which states that, for an independent identically distributed sequence $x _ { n }$ of random variables with mean $\mu$ , we have

$$
\bar {x} _ {n} \stackrel {p} {\rightarrow} \mu \tag {A.17}
$$

$n \to \infty$ , where ${ \bar { x } } _ { n } = n ^ { - 1 } \sum _ { t = 1 } ^ { n } x _ { t }$ is the usual sample mean.

We also will make use of the following concepts.

Definition A.3 For order in probability we write

$$
x _ {n} = o _ {p} \left(a _ {n}\right) \tag {A.18}
$$

if and only if

$$
\frac {x _ {n}}{a _ {n}} \xrightarrow {p} 0. \tag {A.19}
$$

The term boundedness in probability, written $x _ { n } = O _ { p } ( a _ { n } )$ , means that for every $\epsilon > 0$ , there exists a $\delta ( \epsilon ) > 0$ such that

$$
P \left\{\left| \frac {x _ {n}}{a _ {n}} \right| > \delta (\epsilon) \right\} \leq \epsilon \tag {A.20}
$$

for all $n$

Under this convention, e.g., the notation for $x _ { n } \ { \overset { p } { \to } } \ x$ becomes $x _ { n } - x =$ $o _ { p } ( 1 )$ . The definitions can be compared with their nonrandom counterparts, namely, for a fixed sequence $x _ { n } = o ( 1 )$ if $x _ { n } \to 0$ and $x _ { n } = O ( 1 )$ if $x _ { n }$ , for $n = 1 , 2 , \ldots$ is bounded. Some handy properties of $o _ { p } ( \cdot )$ and $O _ { p } ( \cdot )$ are as follows.

(i) If $x _ { n } = o _ { p } ( a _ { n } )$ and $y _ { n } = o _ { p } ( b _ { n } )$ , then $x _ { n } y _ { n } = o _ { p } ( a _ { n } b _ { n } )$ and $x _ { n } + y _ { n } =$ $o _ { p } ( \operatorname* { m a x } ( a _ { n } , b _ { n } ) )$ .   
(ii) If $x _ { n } = o _ { p } ( a _ { n } )$ and $y _ { n } = O _ { p } ( b _ { n } )$ , then $x _ { n } y _ { n } = o _ { p } ( a _ { n } b _ { n } )$   
(iii) Statement (i) is true if $O _ { p } ( \cdot )$ replaces $o _ { p } ( \cdot )$

# Example A.3 Convergence and Order in Probability for the Sample Mean

For the sample mean, $x _ { n }$ , of iid random variables with mean $\mu$ and variance $\sigma ^ { 2 }$ , by the Tchebycheff inequality,

$$
\begin{array}{l} P \left\{\left| \bar {x} _ {n} - \mu \right| > \epsilon \right\} \leq \frac {E \left[ \left(\bar {x} _ {n} - \mu\right) ^ {2} \right]}{\epsilon^ {2}} \\ = \frac {\sigma^ {2}}{n \epsilon^ {2}} \rightarrow 0, \\ \end{array}
$$

as $n \to \infty$ . It follows that ${ \bar { x } } _ { n } ~ { \overset { p } { \to } } ~ \mu$ , or $\bar { x } _ { n } - \mu = o _ { p } ( 1 )$ . To find the rate, it follows that, for $\delta ( \epsilon ) > 0$ ,

$$
P \left\{\sqrt {n} | \bar {x} _ {n} - \mu | > \delta (\epsilon) \right\} \leq \frac {\sigma^ {2} / n}{\delta^ {2} (\epsilon) / n} = \frac {\sigma^ {2}}{\delta^ {2} (\epsilon)}
$$

by Tchebycheff’s inequality, so taking $\epsilon = \sigma ^ { 2 } / \delta ^ { 2 } ( \epsilon )$ shows that $\delta ( \epsilon ) = $ $\sigma / \sqrt \epsilon$ does the job and

$$
\bar {x} _ {n} - \mu = O _ {p} (n ^ {- 1 / 2}).
$$

For $k \times 1$ random vectors ${ \pmb x } _ { n }$ , convergence in probability, written ${ \pmb x } _ { n } \stackrel { \mathcal { P } } {  } { \pmb x }$ or $\pmb { x } _ { n } - \pmb { x } = o _ { p } ( 1 )$ is defined as element-by-element convergence in probability, or equivalently, as convergence in terms of the Euclidean distance

$$
\left\| \boldsymbol {x} _ {n} - \boldsymbol {x} \right\| \stackrel {p} {\rightarrow} 0, \tag {A.21}
$$

where $\begin{array} { r } { \| \pmb { a } \| = \sum _ { j } a _ { j } ^ { 2 } } \end{array}$ for any vector $\textbf { \em a }$ . In this context, we note the result that if $\pmb { x } _ { n } \overset { \mathcal { P } } {  } \pmb { x }$ and $g ( \pmb { x } _ { n } )$ is a continuous mapping,

$$
g \left(\boldsymbol {x} _ {n}\right) \stackrel {{p}} {{\rightarrow}} g (\boldsymbol {x}). \tag {A.22}
$$

Furthermore, if ${ \pmb x } _ { n } - { \pmb a } = O _ { p } ( \delta _ { n } )$ with $\delta _ { n }  0$ and $g ( \cdot )$ is a function with continuous first derivatives continuous in a neighborhood of $\pmb { a } = ( a _ { 1 } , a _ { 2 } , \ldots , a _ { k } ) ^ { \prime }$ , we have the Taylor series expansion in probability

$$
g (\boldsymbol {x} _ {n}) = g (\boldsymbol {a}) + \frac {\partial g (\boldsymbol {x})}{\partial \boldsymbol {x}} \bigg | _ {\boldsymbol {x} = \boldsymbol {a}} ^ {\prime} \left(\boldsymbol {x} _ {n} - \boldsymbol {a}\right) + O _ {p} \left(\delta_ {n}\right), \tag {A.23}
$$

where

$$
\left. \frac {\partial g (\boldsymbol {x})}{\partial \boldsymbol {x}} \right| _ {\boldsymbol {x} = \boldsymbol {a}} = \left(\left. \frac {\partial g (\boldsymbol {x})}{\partial x _ {1}} \right| _ {\boldsymbol {x} = \boldsymbol {a}}, \dots , \left. \frac {\partial g (\boldsymbol {x})}{\partial x _ {k}} \right| _ {\boldsymbol {x} = \boldsymbol {a}}\right) ^ {\prime}
$$

denotes the vector of partial derivatives with respect to $x _ { 1 } , x _ { 2 } , \ldots , x _ { k }$ , evaluated at $\textbf { \em a }$ . This result remains true if $O _ { p } ( \delta _ { n } )$ is replaced everywhere by $o _ { p } ( \delta _ { n } )$ .

# Example A.4 Expansion for the Logarithm of the Sample Mean

With the same conditions as Example A.3, consider $g ( \bar { x } _ { n } ) \ = \ \log \bar { x } _ { n }$ , which has a derivative at $\mu$ , for $\mu > 0$ . Then, because $\bar { x } _ { n } - \mu = O _ { p } ( n ^ { - 1 / 2 } )$ from Example A.3, the conditions for the Taylor expansion in probability, (A.23), are satisfied and we have

$$
\log \bar {x} _ {n} = \log \mu + \mu^ {- 1} (\bar {x} _ {n} - \mu) + O _ {p} (n ^ {- 1 / 2}).
$$

The large sample distributions of sample mean and sample autocorrelation functions defined earlier can be developed using the notion of convergence in distribution.

Definition A.4 A sequence of $k \times 1$ random vectors $\{ { \pmb x } _ { n } \}$ is said to converge in distribution, written

$$
\boldsymbol {x} _ {n} \stackrel {{d}} {{\rightarrow}} \boldsymbol {x} \tag {A.24}
$$

if and only if

$$
F _ {n} (\boldsymbol {x}) \rightarrow F (\boldsymbol {x}) \tag {A.25}
$$

at the continuity points of distribution function $F ( \cdot )$ .

# Example A.5 Convergence in Distribution

Consider a sequence $\{ x _ { n } \}$ of iid normal random variables with mean zero and variance $1 / n$ . Now, using the normal cdf (1.10), we have $F _ { n } ( x ) =$ $\Phi ( { \sqrt { n } } x )$ , so

$$
F _ {n} (x) \to \left\{ \begin{array}{l l} 0 & x <   0, \\ 1 / 2, & x = 0 \\ 1, & x > 0 \end{array} \right.
$$

and we may take

$$
F (x) = \left\{ \begin{array}{l l} 0, & x <   0 \\ 1, & x \geq 0, \end{array} \right.
$$

because the point where the two functions differ is not a continuity point of $F ( x )$ .

The distribution function relates uniquely to the characteristic function through the Fourier transform, defined as a function with vector argument $\pmb { \lambda } = ( \lambda _ { 1 } , \lambda _ { 2 } , \ldots , \lambda _ { k } ) ^ { \prime }$ , say

$$
\begin{array}{l} \phi (\boldsymbol {\lambda}) = E \left(\exp \left\{i \boldsymbol {\lambda} ^ {\prime} \boldsymbol {x} \right\}\right) \\ = \int \exp \left\{i \boldsymbol {\lambda} ^ {\prime} \boldsymbol {x} \right\} d F (\boldsymbol {x}). \tag {A.26} \\ \end{array}
$$

Hence, for a sequence $\{ { \pmb x } _ { n } \}$ we may characterize convergence in distribution of $F _ { n } ( \cdot )$ in terms of convergence of the sequence of characteristic functions $\phi _ { n } ( \cdot )$ , i.e.,

$$
\phi_ {n} (\boldsymbol {\lambda}) \rightarrow \phi (\boldsymbol {\lambda}) \Leftrightarrow F _ {n} (\boldsymbol {x}) \stackrel {d} {\rightarrow} F (\boldsymbol {x}), \tag {A.27}
$$

where $\Leftrightarrow$ means that the implication goes both directions. In this connection, the Cram´er–Wold device says that for every $\pmb { c } = ( c _ { 1 } , c _ { 2 } , \ldots , c _ { k } ) ^ { \prime }$

$$
\boldsymbol {c} ^ {\prime} \boldsymbol {x} _ {n} \stackrel {d} {\rightarrow} \boldsymbol {c} ^ {\prime} \boldsymbol {x} \Leftrightarrow \boldsymbol {x} _ {n} \stackrel {d} {\rightarrow} \boldsymbol {x}. \tag {A.28}
$$

Also, convergence in probability implies convergence in distribution, namely,

$$
\boldsymbol {x} _ {n} \stackrel {{p}} {{\rightarrow}} \boldsymbol {x} \Rightarrow \boldsymbol {x} _ {n} \stackrel {{d}} {{\rightarrow}} \boldsymbol {x}, \tag {A.29}
$$

but the converse is only true when $\pmb { x } _ { n } \overset { d } {  } \pmb { c }$ , where $\pmb { c }$ is a constant vector. If ${ \pmb x } _ { n } \stackrel { d } {  } { \pmb x }$ and ${ \pmb y } _ { n } \overset { d } {  } { \pmb c }$ are two sequences of random vectors and $\pmb { c }$ is a constant vector,

$$
\boldsymbol {x} _ {n} + \boldsymbol {y} _ {n} \stackrel {d} {\rightarrow} \boldsymbol {x} + \boldsymbol {c} \tag {A.30}
$$

and

$$
\boldsymbol {y} _ {n} ^ {\prime} \boldsymbol {x} _ {n} \stackrel {d} {\rightarrow} \boldsymbol {c} ^ {\prime} \boldsymbol {x}. \tag {A.31}
$$

For a continuous mapping $h ( { \pmb x } )$ ,

$$
\boldsymbol {x} _ {n} \stackrel {{d}} {{\rightarrow}} \boldsymbol {x} \Rightarrow h (\boldsymbol {x} _ {n}) \stackrel {{d}} {{\rightarrow}} h (\boldsymbol {x}). \tag {A.32}
$$

A number of results in time series depend on making a series of approximations to prove convergence in distribution. For example, we have that if ${ \pmb x } _ { n } \stackrel { d } {  } { \pmb x }$ can be approximated by the sequence ${ \pmb y } _ { n }$ in the sense that

$$
\boldsymbol {y} _ {n} - \boldsymbol {x} _ {n} = o _ {p} (1), \tag {A.33}
$$

then we have that ${ \pmb y } _ { n } \ \overset { d } {  } \ { \pmb x }$ , so the approximating sequence ${ \pmb y } _ { n }$ has the same limiting distribution as $_ { x }$ . We present the following Basic Approximation Theorem (BAT) that will be used later to derive asymptotic distributions for the sample mean and ACF.

Theorem A.2 Let $\pmb { x } _ { n }$ for $n = 1 , 2 , \ldots$ , and ${ \pmb y } _ { m n }$ for $m = 1 , 2 , \ldots$ , be random $k \times 1$ vectors such that

(i) ${ \pmb y } _ { m n } \stackrel { d } {  } { \pmb y } _ { m }$ as $n \to \infty$ for each $m$   
(ii) ${ \pmb y } _ { m } \overset { d } {  } { \pmb y }$ as $m  \infty$   
(iii) $\begin{array} { r } { \operatorname* { l i m } _ { m  \infty } \operatorname* { l i m } \operatorname* { s u p } _ { n  \infty } P \{ | \pmb { x } _ { n } - \pmb { y } _ { m n } | > \epsilon \} = 0 } \end{array}$ for every $\epsilon > 0$

Then, ${ \pmb x } _ { n } \overset { d } { \to } { \pmb y }$

As a practical matter, condition $( i i i )$ is implied by the Tchebycheff inequality if

$$
\left.\left(i i ^ {\prime}\right) \quad E \left\{\left| \boldsymbol {x} _ {n} - \boldsymbol {y} _ {m n} \right| ^ {2} \right\}\rightarrow 0 \right. \tag {A.34}
$$

as $m , n \to \infty$ , and $( i i i ^ { \prime } )$ is often much easier to establish than $( i i i )$ .

The theorem allows approximation of the underlying sequence in two steps, through the intermediary sequence ${ \pmb y } _ { m n }$ , depending on two arguments. In the time series case, $n$ is generally the sample length and $m$ is generally the number of terms in an approximation to the linear process of the form (A.11).

Proof. The proof of the theorem is a simple exercise in using the characteristic functions and appealing to (A.27). We need to show

$$
\left| \phi_ {\boldsymbol {x} _ {n}} - \phi_ {\boldsymbol {y}} \right|\rightarrow 0,
$$

where we use the shorthand notation $\phi \equiv \phi ( \lambda )$ for ease. First,

$$
\left| \phi \boldsymbol {x} _ {n} - \phi \boldsymbol {y} \right| \leq \left| \phi \boldsymbol {x} _ {n} - \phi \boldsymbol {y} _ {m n} \right| + \left| \phi \boldsymbol {y} _ {m n} - \phi \boldsymbol {y} _ {m} \right| + \left| \phi \boldsymbol {y} _ {m} - \phi \boldsymbol {y} \right|. \tag {A.35}
$$

By the condition $( i i )$ and (A.27), the last term converges to zero, and by condition $( i )$ and (A.27), the second term converges to zero and we only need consider the first term in (A.35). Now, write

$$
\begin{array}{l} \left| \phi_ {\boldsymbol {x} _ {n}} - \phi_ {\boldsymbol {y} _ {m n}} \right| = \left| E \left(e ^ {i \boldsymbol {\lambda} ^ {\prime} \boldsymbol {x} _ {n}} - e ^ {i \boldsymbol {\lambda} ^ {\prime} \boldsymbol {y} _ {m n}}\right) \right| \\ \leq E \left| e ^ {i \boldsymbol {\lambda} ^ {\prime} \boldsymbol {x} _ {n}} \left(1 - e ^ {i \boldsymbol {\lambda} ^ {\prime} \left(\boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n}\right)}\right) \right| \\ = E \left| 1 - e ^ {i \boldsymbol {\lambda} ^ {\prime} \left(\boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n}\right)} \right| \\ = E \left\{\left| 1 - e ^ {i \boldsymbol {\lambda} ^ {\prime} \left(\boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n}\right)} \right| I \left\{\left| \boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n} \right| <   \delta \right\} \right\} \\ + E \left\{\left| 1 - e ^ {i \boldsymbol {\lambda} ^ {\prime} (\boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n})} \right| I \left\{| \boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n} | \geq \delta \right\} \right\}, \\ \end{array}
$$

where $\delta > 0$ and $I \{ A \}$ denotes the indicator function of the set $A$ . Then, given $\pmb { \lambda }$ and $\epsilon > 0$ , choose $\delta ( \epsilon ) > 0$ such that

$$
\left| 1 - e ^ {i \boldsymbol {\lambda} ^ {\prime} (\boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n})} \right| <   \epsilon
$$

if $| { \pmb y } _ { m n } - { \pmb x } _ { n } | < \delta$ , and the first term is less than $\epsilon$ , an arbitrarily small constant. For the second term, note that

$$
\left| 1 - e ^ {i \boldsymbol {\lambda} ^ {\prime} (\boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n})} \right| \leq 2
$$

and we have

$$
E \left\{\left| 1 - e ^ {i \boldsymbol {\lambda} ^ {\prime} (\boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n})} \right| I \{| \boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n} | \geq \delta \} \right\} \leq 2 P \{| \boldsymbol {y} _ {m n} - \boldsymbol {x} _ {n} | \geq \delta \},
$$

which converges to zero as $n \to \infty$ by property $( i i i )$ .

# A.2 Central Limit Theorems

We will generally be concerned with the large-sample properties of estimators that turn out to be normally distributed as $n \to \infty$ .

Definition A.5 A sequence of random variables $\{ x _ { n } \}$ is said to be asymptotically normal with mean $\mu _ { n }$ and variance $\sigma _ { n } ^ { 2 }$ if, as $n \to \infty$ ,

$$
\sigma_ {n} ^ {- 1} \left(x _ {n} - \mu_ {n}\right) \stackrel {d} {\to} z,
$$

where $z$ has the standard normal distribution. We shall abbreviate this as

$$
x _ {n} \sim A N \left(\mu_ {n}, \sigma_ {n} ^ {2}\right), \tag {A.36}
$$

where $\sim$ will denote is distributed as.

We state the important Central Limit Theorem, as follows.

Theorem A.3 Let $x _ { 1 } , \ldots , x _ { n }$ be independent and identically distributed with mean $\mu$ and variance $\sigma ^ { 2 }$ . If ${ \bar { x } } _ { n } = ( x _ { 1 } + \cdot \cdot \cdot + x _ { n } ) / n$ denotes the sample mean, then

$$
\bar {x} _ {n} \sim A N \left(\mu , \sigma^ {2} / n\right). \tag {A.37}
$$

Often, we will be concerned with a sequence of $k \times 1$ vectors $\{ { \pmb x } _ { n } \}$ . The following definition is motivated by the Cram´er–Wold device considered earlier.

Definition A.6 We define asymptotic normality for the vector case as

$$
\boldsymbol {x} _ {n} \sim A N \left(\boldsymbol {\mu} _ {n}, \Sigma_ {n}\right) \tag {A.38}
$$

if and only if

$$
\boldsymbol {c} ^ {\prime} \boldsymbol {x} _ {n} \sim A N \left(\boldsymbol {c} ^ {\prime} \boldsymbol {\mu} _ {n}, \boldsymbol {c} ^ {\prime} \Sigma_ {n} \boldsymbol {c}\right) \tag {A.39}
$$

for all $\pmb { c }$ and $\Sigma _ { n }$ is positive definite.

In order to begin to consider what happens for dependent data in the limiting case, it is necessary to define, first of all, a particular kind of dependence known as M-dependence. We say that a time series $x _ { t }$ is M-dependent if the set of values $x _ { s } , s \le t$ is independent of the set of values $x _ { s } , s \ge t + M + 1$ , so time points separated by more than $M$ units are independent. A central limit theorem for such dependent processes, used in conjunction with the Basic Approximation Theorem, will allow us to develop large-sample distributional results for the sample mean $x$ and the sample ACF ${ \widehat { \rho } } _ { x } ( h )$ in the stationary case.

In the arguments that follow, we often make use of the formula for the variance of $x _ { n }$ in the stationary case, namely,

$$
\operatorname {v a r} \bar {x} _ {n} = n ^ {- 1} \sum_ {u = - (n - 1)} ^ {(n - 1)} \left(1 - \frac {| u |}{n}\right) \gamma (u). \tag {A.40}
$$

To prove the above formula, letting $u = s - t$ and $v = t$ i n

$$
\begin{array}{l} n ^ {2} E [ (\bar {x} _ {n} - \mu) ^ {2} ] = \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} E [ (x _ {s} - \mu) (x _ {t} - \mu) ] \\ = \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} \gamma (s - t) \\ = \sum_ {u = - (n - 1)} ^ {- 1} \sum_ {v = - (u - 1)} ^ {n} \gamma (u) + \sum_ {u = 0} ^ {n - 1} \sum_ {v = 1} ^ {n - u} \gamma (u) \\ = \sum_ {u = - (n - 1)} ^ {- 1} (n + u) \gamma (u) + \sum_ {u = 0} ^ {n - 1} (n - u) \gamma (u) \\ = \sum_ {u = - (n - 1)} ^ {(n - 1)} (n - | u |) \gamma (u) \\ \end{array}
$$

gives the required result. We shall also use the fact that, for

$$
\sum_ {u = - \infty} ^ {\infty} | \gamma (u) | <   \infty ,
$$

we would have, by dominated convergence,2

$$
n \operatorname {v a r} \bar {x} _ {n} \rightarrow \sum_ {u = - \infty} ^ {\infty} \gamma (u), \tag {A.41}
$$

because $| ( 1 - | u | / n ) \gamma ( u ) | \leq | \gamma ( u ) |$ and $( 1 - | u | / n ) \gamma ( u ) \to \gamma ( u )$ . We may now state the M-Dependent Central Limit Theorem as follows.

Theorem A.4 If $x _ { t }$ is a strictly stationary M-dependent sequence of random variables with mean zero and autocovariance function $\gamma ( \cdot )$ and if

$$
V _ {M} = \sum_ {u = - M} ^ {M} \gamma (u), \tag {A.42}
$$

where $V _ { M } \neq 0$ ,

$$
\bar {x} _ {n} \sim A N \left(0, V _ {M} / n\right). \tag {A.43}
$$

Proof. To prove the theorem, using Theorem A.2, the Basic Approximation Theorem, we may construct a sequence of variables $y _ { m n }$ approximating

$$
n ^ {1 / 2} \bar {x} _ {n} = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t}
$$

in the dependent case and then simply verify conditions (i), (ii), and $( i i i )$ of Theorem A.2. For $m > 2 M$ , we may first consider the approximation

$$
\begin{array}{l} y _ {m n} = n ^ {- 1 / 2} \left[ \left(x _ {1} + \dots + x _ {m - M}\right) + \left(x _ {m + 1} + \dots + x _ {2 m - M}\right) \right. \\ \left. + \left(x _ {2 m + 1} + \dots + x _ {3 m - M}\right) + \dots + \left(x _ {(r - 1) m + 1} + \dots + x _ {r m - M}\right) \right] \\ = n ^ {- 1 / 2} \left(z _ {1} + z _ {2} + \dots + z _ {r}\right), \\ \end{array}
$$

where $r = [ n / m ]$ , with $[ n / m ]$ denoting the greatest integer less than or equal to $n / m$ . This approximation contains only part of $n ^ { 1 / 2 } \bar { x } _ { n }$ , but the random variables $z _ { 1 } , z _ { 2 } , \ldots , z _ { r }$ are independent because they are separated by more than $M$ time points, e.g., $m + 1 - ( m - M ) = M + 1$ points separate $z _ { 1 }$ and $z _ { 2 }$ . Because of strict stationarity, $z _ { 1 } , z _ { 2 } , \ldots , z _ { r }$ are identically distributed with zero means and variances

$$
S _ {m - M} = \sum_ {| u | \leq M} (m - M - | u |) \gamma (u)
$$

by a computation similar to that producing (A.40). We now verify the conditions of the Basic Approximation Theorem hold.

(i): Applying the Central Limit Theorem to the sum $y _ { m n }$ gives

$$
y _ {m n} = n ^ {- 1 / 2} \sum_ {i = 1} ^ {r} z _ {i} = (n / r) ^ {- 1 / 2} r ^ {- 1 / 2} \sum_ {i = 1} ^ {r} z _ {i}.
$$

Because $( n / r ) ^ { - 1 / 2 } \to m ^ { 1 / 2 }$ and

$$
r ^ {- 1 / 2} \sum_ {i = 1} ^ {r} z _ {i} \stackrel {d} {\to} N (0, S _ {m - M}),
$$

it follows from (A.31) that

$$
y _ {m n} \stackrel {d} {\to} y _ {m} \sim N (0, S _ {m - M} / m).
$$

as $n \to \infty$ , for a fixed $m$ .

$( i i )$ : Note that as $m \to \infty$ , $S _ { m - M } / m \to V _ { M }$ using dominated convergence, where $V _ { M }$ is defined in (A.42). Hence, the characteristic function of $y _ { m }$ , say,

$$
\phi_ {m} (\lambda) = \exp \biggl \{- \frac {1}{2} \lambda^ {2} \frac {S _ {m - M}}{m} \biggr \} \to \exp \biggl \{- \frac {1}{2} \lambda^ {2} V _ {M} \biggr \},
$$

as $m \to \infty$ , which is the characteristic function of a random variable $y \sim N ( 0 , V _ { M } )$ and the result follows because of (A.27).

$( i i i )$ : To verify the last condition of the BAT theorem,

$$
\begin{array}{l} n ^ {1 / 2} \bar {x} _ {n} - y _ {m n} = n ^ {- 1 / 2} \left[ \left(x _ {m - M + 1} + \dots + x _ {m}\right) \right. \\ + \left(x _ {2 m - M + 1} + \dots + x _ {2 m}\right) \\ + \left(x _ {(r - 1) m - M + 1} + \dots + x _ {(r - 1) m}\right) \\ \begin{array}{c} \bullet \\ \bullet \\ \bullet \end{array} \\ \left. + \left(x _ {r m - M + 1} + \dots + x _ {n}\right) \right] \\ = n ^ {- 1 / 2} \left(w _ {1} + w _ {2} + \dots + w _ {r}\right), \\ \end{array}
$$

so the error is expressed as a scaled sum of iid variables with variance $S _ { M }$ for the first $r - 1$ variables and

$$
\begin{array}{l} \operatorname {v a r} \left(w _ {r}\right) = \sum_ {| u | \leq m - M} \left(n - \left[ n / m \right] m + M - | u |\right) \gamma (u) \\ \leq \sum_ {| u | \leq m - M} (m + M - | u |) \gamma (u). \\ \end{array}
$$

Hence,

$$
\operatorname {v a r} \left[ n ^ {1 / 2} \bar {x} - y _ {m n} \right] = n ^ {- 1} [ (r - 1) S _ {M} + \operatorname {v a r} w _ {r} ],
$$

which converges to $m ^ { - 1 } S _ { M }$ as $n \to \infty$ . Because $m ^ { - 1 } S _ { M }  0$ as $m \to \infty$ , the condition of (iii) holds by the Tchebycheff inequality.

# A.3 The Mean and Autocorrelation Functions

The background material in the previous two sections can be used to develop the asymptotic properties of the sample mean and ACF used to evaluate statistical significance. In particular, we are interested in verifying Property P1.1.

We begin with the distribution of the sample mean $x _ { n }$ , noting that (A.41) suggests a form for the limiting variance. In all of the asymptotics, we will use the assumption that $x _ { t }$ is a linear process, as defined in Definition 1.12, but with the added condition that $\{ w _ { t } \}$ is iid. That is, throughout this section we assume

$$
x _ {t} = \mu_ {x} + \sum_ {j = - \infty} ^ {\infty} \psi_ {j} w _ {t - j} \tag {A.44}
$$

where $w _ { t } \sim \mathrm { i i d } ( 0 , \sigma _ { w } ^ { 2 } )$ , and the coefficients satisfy

$$
\sum_ {j = - \infty} ^ {\infty} | \psi_ {j} | <   \infty . \tag {A.45}
$$

Before proceeding further, we should note that the exact sampling distribution of $x _ { n }$ is available if the distribution of the underlying vector ${ \pmb x } =$ $( x _ { 1 } , x _ { 2 } , \ldots , x _ { n } ) ^ { \prime }$ is multivariate normal. Then, $x _ { n }$ is just a linear combination of jointly normal variables that will have the normal distribution

$$
\bar {x} _ {n} \sim N \left(\mu_ {x}, n ^ {- 1} \sum_ {| u | <   n} \left(1 - \frac {| u |}{n}\right) \gamma_ {x} (u)\right), \tag {A.46}
$$

by (A.40). In the case where $x _ { t }$ are not jointly normally distributed, we have the following theorem.

Theorem A.5 If $x _ { t }$ is a linear process of the form (A.44) and $\textstyle \sum _ { j } \psi _ { j } \neq 0$ , then

$$
\bar {x} _ {n} \sim A N \left(\mu_ {x}, n ^ {- 1} V\right), \tag {A.47}
$$

where

$$
V = \sum_ {h = - \infty} ^ {\infty} \gamma_ {x} (h) = \sigma_ {w} ^ {2} \left(\sum_ {j = - \infty} ^ {\infty} \psi_ {j}\right) ^ {2} \tag {A.48}
$$

and $\gamma _ { x } ( \cdot )$ is the autocovariance function of $x _ { t }$

Proof. To prove the above, we can again use the Basic Approximation Theorem A.2 by first defining the strictly stationary $2 m$ -dependent linear process with finite limits

$$
x _ {t} ^ {m} = \sum_ {j = - m} ^ {m} \psi_ {j} w _ {t - j}
$$

as an approximation to $x _ { t }$ to use in the approximating mean

$$
\bar {x} _ {n, m} = n ^ {- 1} \sum_ {t = 1} ^ {n} x _ {t} ^ {m}.
$$

Then, take

$$
y _ {m n} = n ^ {1 / 2} (\bar {x} _ {n, m} - \mu_ {x})
$$

as an approximation to $n ^ { 1 / 2 } ( \bar { x } _ { n } - \mu _ { x } )$

(i): Applying Theorem A.4, we have

$$
y _ {m n} \stackrel {d} {\rightarrow} y _ {m} \sim N (0, V _ {m}),
$$

as $n \to \infty$ , where

$$
V _ {m} = \sum_ {h = - 2 m} ^ {2 m} \gamma_ {x} (h) = \sigma_ {w} ^ {2} \left(\sum_ {j = - m} ^ {m} \psi_ {j}\right) ^ {2}.
$$

To verify the above, we note that for the general linear process with infinite limits, (1.33) implies that

$$
\sum_ {h = - \infty} ^ {\infty} \gamma_ {x} (h) = \sigma_ {w} ^ {2} \sum_ {h = - \infty} ^ {\infty} \sum_ {j = - \infty} ^ {\infty} \psi_ {j + h} \psi_ {j} = \sigma_ {w} ^ {2} \left(\sum_ {j = - \infty} ^ {\infty} \psi_ {j}\right) ^ {2},
$$

so taking the special case $\psi _ { j } = 0$ , for $| j | > m$ , we obtain $V _ { m }$ .

$( i i )$ : Because $V _ { m } \to V$ in (A.48) as $m \to \infty$ , we may use the same characteristic function argument as under (ii) in the proof of Theorem A.4 to note that

$$
y _ {m} \stackrel {d} {\to} y \sim N (0, V),
$$

where $V$ is given by (A.48).

$( i i i )$ : Finally,

$$
\begin{array}{l} \operatorname {v a r} \left[ n ^ {1 / 2} (\bar {x} _ {n} - \mu_ {x}) - y _ {m n} \right] = n \operatorname {v a r} \left[ n ^ {- 1} \sum_ {t = 1} ^ {n} \sum_ {| j | > m} \psi_ {j} w _ {t - j} \right] \\ = \sigma_ {w} ^ {2} \left(\sum_ {| j | > m} \psi_ {j}\right) ^ {2} \rightarrow 0 \\ \end{array}
$$

as $m \to \infty$ .

In order to develop the sampling distribution of the sample autocovariance function, $\widehat { \gamma } _ { x } ( h )$ , and the sample autocorrelation function, ${ \widehat { \rho } } _ { x } ( h )$ , we need to develop some idea as to the mean and variance of ${ \widehat { \gamma } } _ { x } ( h )$ under some reasonable assumptions. These computations for $\widehat { \gamma } _ { x } ( h )$ are messy, and we consider a comparable quantity

$$
\widetilde {\gamma} _ {x} (h) = n ^ {- 1} \sum_ {t = 1} ^ {n} \left(x _ {t + h} - \mu_ {x}\right) \left(x _ {t} - \mu_ {x}\right) \tag {A.49}
$$

as an approximation. By Problem 1.29,

$$
n ^ {1 / 2} \left[ \tilde {\gamma} _ {x} (h) - \hat {\gamma} _ {x} (h) \right] = o _ {p} (1),
$$

so that limiting distributional results proved for $n ^ { 1 / 2 } \widetilde { \gamma } _ { x } ( h )$ will hold for $n ^ { 1 / 2 } \widehat { \gamma } _ { x } ( h )$ by (A.33).

We begin by proving formulas for the variance and for the limiting variance of $\widetilde { \gamma } _ { x } ( h )$ under the assumptions that $x _ { t }$ is a linear process of the form (A.44), +satisfying (A.45) with the white noise variates $w _ { t }$ having variance $\sigma _ { w } ^ { 2 }$ as before, but also required to have fourth moments satisfying

$$
E \left(w _ {t} ^ {4}\right) = \eta \sigma_ {w} ^ {4} <   \infty , \tag {A.50}
$$

where $\eta$ is some constant. We seek results comparable with (A.40) and (A.41) for $\widetilde { \gamma } _ { x } ( h )$ . To ease the notation, we will henceforth drop the subscript $x$ from the notation.

Using (A.49), $E [ \widetilde { \gamma } ( h ) ] = \gamma ( h )$ . Under the above assumptions, we show now that, for $p , q = 0 , 1 , 2 , \ldots$ ,

$$
\operatorname {c o v} [ \widetilde {\gamma} (p), \widetilde {\gamma} (q) ] = n ^ {- 1} \sum_ {u = - (n - 1)} ^ {(n - 1)} \left(1 - \frac {| u |}{n}\right) V _ {u}, \tag {A.51}
$$

where

$$
\begin{array}{l} V _ {u} = \gamma (u) \gamma (u + p - q) + \gamma (u + p) \gamma (u - q) \\ + (\eta - 3) \sigma_ {w} ^ {4} \sum_ {i} \psi_ {i + u + q} \psi_ {i + u} \psi_ {i + p} \psi_ {i}. \tag {A.52} \\ \end{array}
$$

The absolute summability of the $\psi _ { j }$ can then be shown to imply the absolute summability of the $V _ { u }$ .3 Thus, the dominated convergence theorem implies

$$
\begin{array}{l} n \operatorname {c o v} [ \widetilde {\gamma} (p), \widetilde {\gamma} (q) ] \rightarrow \sum_ {u = - \infty} ^ {\infty} V _ {u} \\ = (\eta - 3) \gamma (p) \gamma (q) \tag {A.53} \\ + \sum_ {u = - \infty} ^ {\infty} \left[ \gamma (u) \gamma (u + p - q) + \gamma (u + p) \gamma (u - q) \right]. \\ \end{array}
$$

To verify (A.51) is somewhat tedious, so we only go partially through the calculations, leaving the repetitive details to the reader. First, rewrite (A.44) as

$$
x _ {t} = \mu + \sum_ {i = - \infty} ^ {\infty} \psi_ {t - i} w _ {i},
$$

so that

$$
E [ \widetilde {\gamma} (p) \widetilde {\gamma} (q) ] = n ^ {- 2} \sum_ {s, t} \sum_ {i, j, k, \ell} \psi_ {s + p - i} \psi_ {s - j} \psi_ {t + q - k} \psi_ {t - \ell} E (w _ {i} w _ {j} w _ {k} w _ {\ell}).
$$

Then, evaluate, using the easily verified properties of the $w _ { t }$ series

$$
E (w _ {i} w _ {j} w _ {k} w _ {\ell}) = \left\{ \begin{array}{l l} \eta \sigma_ {w} ^ {4} & \text {i f} i = j = k = \ell \\ \sigma_ {w} ^ {4} & \text {i f} i = j \neq k = \ell \\ 0 & \text {i f} i \neq j, i \neq k \text {a n d} i \neq \ell . \end{array} \right.
$$

To apply the rules, we break the sum over the subscripts $i , j , k , \ell$ into four terms, namely,

$$
\begin{array}{l} \sum_ {i, j, k, \ell} = \sum_ {i = j = k = \ell} + \sum_ {i = j \neq k = \ell} + \sum_ {i = k \neq j = \ell} + \sum_ {i = \ell \neq j = k} \\ = S _ {1} + S _ {2} + S _ {3} + S _ {4}. \\ \end{array}
$$

Now,

$$
\begin{array}{l} {S _ {1}} = {\eta \sigma_ {w} ^ {4} \sum_ {i} \psi_ {s + p - i} \psi_ {s - i} \psi_ {t + q - i} \psi_ {t - i}} \\ = \eta \sigma_ {w} ^ {4} \sum_ {i} \psi_ {i + s - t + p} \psi_ {i + s - t} \psi_ {i + q} \psi_ {i}, \\ \end{array}
$$

where we have let $i ^ { \prime } = t - i$ to get the final form. For the second term,

$$
\begin{array}{l} S _ {2} = \sum_ {i = j \neq k = \ell} \psi_ {s + p - i} \psi_ {s - j} \psi_ {t + q - k} \psi_ {t - \ell} E \left(w _ {i} w _ {j} w _ {k} w _ {\ell}\right) \\ = \sum_ {i \neq k} \psi_ {s + p - i} \psi_ {s - i} \psi_ {t + q - k} \psi_ {t - k} E \left(w _ {i} ^ {2}\right) E \left(w _ {k} ^ {2}\right). \\ \end{array}
$$

Then, using the fact that

$$
\sum_ {i \neq k} = \sum_ {i, k} - \sum_ {i = k},
$$

we have

$$
\begin{array}{l} S _ {2} = \sigma_ {w} ^ {4} \sum_ {i, k} \psi_ {s + p - i} \psi_ {s - i} \psi_ {t + q - k} \psi_ {t - k} - \sigma_ {w} ^ {4} \sum_ {i} \psi_ {s + p - i} \psi_ {s - i} \psi_ {t + q - i} \psi_ {t - i} \\ = \gamma (p) \gamma (q) - \sigma_ {w} ^ {4} \sum_ {i} \psi_ {i + s - t + p} \psi_ {i + s - t} \psi_ {i + q} \psi_ {i}, \\ \end{array}
$$

letting $i ^ { \prime } = s - i , k ^ { \prime } = t - k$ in the first term and $i ^ { \prime } = s - i$ in the second term. Repeating the argument for $S _ { 3 }$ and $S _ { 4 }$ and substituting into the covariance expression yields

$$
\begin{array}{l} E [ \widetilde {\gamma} (p) \widetilde {\gamma} (q) ] = n ^ {- 2} \sum_ {s, t} \left[ \gamma (p) \gamma (q) + \gamma (s - t) \gamma (s - t + p - q) \right. \\ + \gamma (s - t + p) \gamma (s - t - q) \\ \left. + (\eta - 3) \sigma_ {w} ^ {4} \sum_ {i} \psi_ {i + s - t + p} \psi_ {i + s - t} \psi_ {i + q} \psi_ {i} \right]. \\ \end{array}
$$

Then, letting $u = s - t$ and subtracting $E [ \tilde { \gamma } ( p ) ] E [ \tilde { \gamma } ( q ) ] = \gamma ( p ) \gamma ( q )$ from the summation leads to the result (A.52). Summing (A.52) over $u$ and applying dominated convergence leads to (A.53).

The above results for the variances and covariances of the approximating statistics $\widetilde { \gamma } ( \cdot )$ enable proving the following central limit theorem for the autocovariance functions $\widehat { \gamma } ( \cdot )$ .

Theorem A.6 If $x _ { t }$ is a stationary linear process of the form (A.44) satisfying the fourth moment condition (A.50), then, for fixed $K$ ,

$$
\left( \begin{array}{c} \widehat {\gamma} (0) \\ \widehat {\gamma} (1) \\ \vdots \\ \widehat {\gamma} (K) \end{array} \right) \sim A N \left[ \left( \begin{array}{c} \gamma (0) \\ \gamma (1) \\ \vdots \\ \gamma (K) \end{array} \right), n ^ {- 1} V \right],
$$

where $V$ is the matrix with elements given by

$$
\begin{array}{l} v _ {p q} = (\eta - 3) \gamma (p) \gamma (q) \\ + \sum_ {u = - \infty} ^ {\infty} \left[ \gamma (u) \gamma (u - p + q) + \gamma (u + q) \gamma (u - p) \right]. \tag {A.54} \\ \end{array}
$$

Proof. It suffices to show the result for the approximate autocovariance (A.49) for $\widetilde { \gamma } ( \cdot )$ by the remark given below it (see also Problem 1.29). First, define the strictly stationary $( 2 m + K )$ -dependent $( K + 1 ) \times 1$ vector

$$
\boldsymbol {y} _ {t} ^ {m} = \left( \begin{array}{c} (x _ {t} ^ {m} - \mu) ^ {2} \\ (x _ {t + 1} ^ {m} - \mu) (x _ {t} ^ {m} - \mu) \\ \vdots \\ (x _ {t + K} ^ {m} - \mu) (x _ {t} ^ {m} - \mu) \end{array} \right),
$$

where

$$
x _ {t} ^ {m} = \mu + \sum_ {j = - m} ^ {m} \psi_ {j} w _ {t - j}
$$

is the usual approximation. The sample mean of the above vector is

$$
\bar {\boldsymbol {y}} _ {m n} = n ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {y} _ {t} ^ {m} = \left( \begin{array}{c} \widetilde {\gamma} ^ {m n} (0) \\ \widetilde {\gamma} ^ {m n} (1) \\ \vdots \\ \widetilde {\gamma} ^ {m n} (K) \end{array} \right),
$$

where

$$
\widetilde {\gamma} ^ {m n} (h) = n ^ {- 1} \sum_ {t = 1} ^ {n} (x _ {t + h} ^ {m} - \mu) (x _ {t} ^ {m} - \mu)
$$

denotes the sample autocovariance of the approximating series. Also,

$$
E \boldsymbol {y} _ {t} ^ {m} = \left( \begin{array}{c} \gamma^ {m} (0) \\ \gamma^ {m} (1) \\ \vdots \\ \gamma^ {m} (K) \end{array} \right),
$$

where $\gamma ^ { m } ( h )$ is the theoretical covariance function of the series $\boldsymbol { x } _ { t } ^ { m }$ . Then, consider the vector

$$
\boldsymbol {y} _ {m n} = n ^ {1 / 2} \left[ \bar {\boldsymbol {y}} _ {m n} - E \left(\bar {\boldsymbol {y}} _ {m n}\right) \right]
$$

as an approximation to

$$
\boldsymbol {y} _ {n} = n ^ {1 / 2} \left[ \left( \begin{array}{c} \tilde {\gamma} (0) \\ \tilde {\gamma} (1) \\ \vdots \\ \tilde {\gamma} (K) \end{array} \right) - \left( \begin{array}{c} \gamma (0) \\ \gamma (1) \\ \vdots \\ \gamma (K) \end{array} \right) \right],
$$

where $E ( \bar { \pmb y } _ { m n } )$ is the same as $E ( \pmb { y } _ { t } ^ { m } )$ given above. The elements of the vector approximation ${ \pmb y } _ { m n }$ are clearly $n ^ { 1 / 2 } ( \tilde { \gamma } ^ { m n } ( h ) - \tilde { \gamma } ^ { m } ( h ) )$ . Note that the elements of ${ \pmb y } _ { n }$ are based on the linear process $x _ { t }$ , whereas the elements of ${ \pmb y } _ { m n }$ are based on the $m$ -dependent linear process $\boldsymbol { x } _ { t } ^ { m }$ . To obtain a limiting distribution for ${ \pmb y } _ { n }$ , we apply the Basic Approximation Theorem A.2 using ${ \pmb y } _ { m n }$ as our approximation. We now verify (i), $( i i )$ , and $( i i i )$ of Theorem A.2.

$( i )$ : First, let $\pmb { c }$ be a $( K + 1 ) \times 1$ vector of constants, and apply the central limit theorem to the $( 2 m + K ) -$ dependent series $\pmb { c } ^ { \prime } \pmb { y } _ { m n }$ using the Cram´er–Wold device (A.28). We obtain

$$
\boldsymbol {c} ^ {\prime} \boldsymbol {y} _ {m n} = n ^ {1 / 2} \boldsymbol {c} ^ {\prime} [ \bar {\boldsymbol {y}} _ {m n} - E (\bar {\boldsymbol {y}} _ {m n}) ] \xrightarrow {d} \boldsymbol {c} ^ {\prime} \boldsymbol {y} _ {m} \sim N (0, \boldsymbol {c} ^ {\prime} V _ {m} \boldsymbol {c}),
$$

as $n  \infty$ , where $V _ { m }$ is a matrix containing the finite analogs of the elements $v _ { p q }$ defined in (A.54).

$( i i )$ : Note that, since $V _ { m } \to V$ as $m \to \infty$ , it follows that

$$
\boldsymbol {c} ^ {\prime} \boldsymbol {y} _ {m} \xrightarrow {d} \boldsymbol {c} ^ {\prime} \boldsymbol {y} \sim N (0, \boldsymbol {c} ^ {\prime} V \boldsymbol {c}),
$$

so, by the Cram´er–Wold device, the limiting $( K + 1 ) \times 1$ multivariate normal variable is $N ( \mathbf { 0 } , V )$ .

$( i i i )$ : To show condition $( i i i )$ of the Basic Approximation Theorem, we can focus on the element-by-element components of

$$
P \left\{\left| \mathbf {y} _ {n} - \mathbf {y} _ {m n} \right| > \epsilon \right\}.
$$

For example, using the Tchebycheff inequality, the $h$ -th element of the probability statement can be bounded by

$$
\begin{array}{l} n \epsilon^ {- 2} \operatorname {v a r} \left(\tilde {\gamma} (h) - \tilde {\gamma} ^ {m} (h)\right) \\ = \epsilon^ {- 2} \left\{n \operatorname {v a r} \tilde {\gamma} (h) + n \operatorname {v a r} \tilde {\gamma} ^ {m} (h) - 2 n \operatorname {c o v} [ \tilde {\gamma} (h), \tilde {\gamma} ^ {m} (h) ] \right\}. \\ \end{array}
$$

Using the results that led to (A.53), we see that the preceding expression approaches

$$
(v _ {h h} + v _ {h h} - 2 v _ {h h}) / \epsilon^ {2} = 0,
$$

as $m , n \to \infty$ .

To obtain a result comparable to Theorem A.6 for the autocorrelation function ACF, we note the following theorem.

Theorem A.7 If $x _ { t }$ is a stationary linear process of the form (1.31) satisfying the fourth moment condition (A.50), then for fixed $K$ ,

$$
\left( \begin{array}{c} \widehat {\rho} (1) \\ \vdots \\ \widehat {\rho} (K) \end{array} \right) \sim A N \left[ \left( \begin{array}{c} \rho (1) \\ \vdots \\ \rho (K) \end{array} \right), n ^ {- 1} W \right],
$$

where W is the matrix with elements given by

$$
\begin{array}{l} w _ {p q} = \sum_ {u = - \infty} ^ {\infty} \left[ \rho (u + p) \rho (u + q) + \rho (u - p) \rho (u + q) + 2 \rho (p) \rho (q) \rho^ {2} (u) \right. \\ \left. - 2 \rho (p) \rho (u) \rho (u + q) - 2 \rho (q) \rho (u) \rho (u + p) \right] \\ = \sum_ {u = 1} ^ {\infty} [ \rho (u + p) + \rho (u - p) - 2 \rho (p) \rho (u) ] \\ \times \left[ \rho (u + q) + \rho (u - q) - 2 \rho (q) \rho (u) \right], \tag {A.55} \\ \end{array}
$$

where the last form is more convenient.

Proof. To prove the theorem, we use the delta method4 for the limiting distribution of a function of the form

$$
\boldsymbol {g} \left(x _ {0}, x _ {1}, \dots , x _ {K}\right) = \left(x _ {1} / x _ {0}, \dots , x _ {K} / x _ {0}\right) ^ {\prime},
$$

where $x _ { h } = \widehat { \gamma } ( h )$ , for $h = 0 , 1 , \ldots , K$ . Hence, using the delta method and Theorem A.6,

$$
\boldsymbol {g} (\widehat {\gamma} (0), \widehat {\gamma} (1), \dots , \widehat {\gamma} (K)) = (\widehat {\rho} (1), \dots , \widehat {\rho} (K)) ^ {\prime}
$$

is asymptotically normal with mean vector $( \rho ( 1 ) , \ldots , \rho ( K ) ) ^ { \prime }$ and covariance matrix

$$
n ^ {- 1} W = n ^ {- 1} D V D ^ {\prime},
$$

where $V$ is defined by (A.54) and $D$ is the $( K + 1 ) \times K$ matrix of partial derivatives

$$
D = \frac {1}{x _ {0} ^ {2}} \left( \begin{array}{c c c c c} - x _ {1} & x _ {0} & 0 & \ldots & 0 \\ - x _ {2} & 0 & x _ {0} & \ldots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ - x _ {K} & 0 & 0 & \ldots & x _ {0}, \end{array} \right)
$$

Substituting $\gamma ( h )$ for $x _ { h }$ , we note that $D$ can be written as the patterned matrix

$$
D = \frac {1}{\gamma (0)} \left( \begin{array}{c c} - \boldsymbol {\rho} & I _ {K} \end{array} \right),
$$

where $\pmb { \rho } = ( \rho ( 1 ) , \rho ( 2 ) , \dots , \rho ( K ) ) ^ { \prime }$ is the $K \times 1$ matrix of autocorrelations and $I _ { K }$ is the $K \times K$ identity matrix. Then, it follows from writing the matrix $V$ in the partitioned form

$$
V = \left( \begin{array}{c c} v _ {0 0} & \boldsymbol {v} _ {1} ^ {\prime} \\ \boldsymbol {v} _ {1} & V _ {2 2} \end{array} \right)
$$

that

$$
W = \gamma^ {- 2} (0) \left[ v _ {0 0} \boldsymbol {\rho} \boldsymbol {\rho} ^ {\prime} - \boldsymbol {\rho} \boldsymbol {v} _ {1} ^ {\prime} - \boldsymbol {v} _ {1} \boldsymbol {\rho} ^ {\prime} + V _ {2 2} \right],
$$

where $\pmb { v } _ { 1 } = ( v _ { 1 0 } , v _ { 2 0 } , \ldots , v _ { K 0 } ) ^ { \prime }$ and $V _ { 2 2 } = \{ v _ { p q }$ ; $p , q = 1 , \ldots , K \}$ . Hence,

$$
\begin{array}{l} w _ {p q} = \gamma^ {- 2} (0) [ v _ {p q} - \rho (p) v _ {0 q} - \rho (q) v _ {p 0} + \rho (p) \rho (q) v _ {0 0} ] \\ = \sum_ {u = - \infty} ^ {\infty} \left[ \rho (u) \rho (u - p + q) + \rho (u - p) \rho (u + q) + 2 \rho (p) \rho (q) \rho^ {2} (u) \right. \\ \left. - 2 \rho (p) \rho (u) \rho (u + q) - 2 \rho (q) \rho (u) \rho (u - p) \right]. \\ \end{array}
$$

Interchanging the summations, we get the $w _ { p q }$ specified in the statement of the theorem, finishing the proof. ■

Specializing the theorem to the case of interest in this chapter, we note that if $\{ x _ { t } \}$ is iid with finite fourth moment, then $w _ { p q } = 1$ for $p = q$ and is zero otherwise. In this case, for $h = 1 , \ldots , K$ , the $\widehat { \rho } ( h )$ are asymptotically independent and jointly normal with

$$
\widehat {\rho} (h) \sim A N \left(0, n ^ {- 1}\right). \tag {A.56}
$$

This justifies the use of (1.38) and the discussion below it as a method for testing whether a series is white noise.

# Large Sample Theory

For the cross-correlation, it has been noted that the same kind of approximation holds and we quote the following theorem for the bivariate case, which can be proved using similar arguments (see Brockwell and Davis, 1991, p. 410).

# Theorem A.8 If

$$
x _ {t} = \sum_ {j = - \infty} ^ {\infty} \alpha_ {j} w _ {t - j, 1}
$$

and

$$
y _ {t} = \sum_ {j = - \infty} ^ {\infty} \beta_ {j} w _ {t - j, 2}
$$

are two linear processes of the form with absolutely summable coefficients and the two white noise sequences are iid and independent of each other with variances $\sigma _ { 1 } ^ { 2 }$ and $\sigma _ { 2 } ^ { 2 }$ , then for $h \geq 0$ ,

$$
\widehat {\rho} _ {x y} (h) \sim A N \left(\rho_ {x y} (h), n ^ {- 1} \sum_ {j} \rho_ {x} (j) \rho_ {y} (j)\right) \tag {A.57}
$$

and the joint distribution of $( \widehat { \rho } _ { x y } ( h ) , \widehat { \rho } _ { x y } ( k ) ) ^ { \prime }$ is asymptotically normal with mean vector zero and

$$
\operatorname {c o v} \left(\widehat {\rho} _ {x y} (h), \widehat {\rho} _ {x y} (k)\right) = n ^ {- 1} \sum_ {j} \rho_ {x} (j) \rho_ {y} (j + k - h). \tag {A.58}
$$

Again, specializing to the case of interest in this chapter, as long as at least one of the two series is white (iid) noise, we obtain

$$
\widehat {\rho} _ {x y} (h) \sim A N \left(0, n ^ {- 1}\right), \tag {A.59}
$$

which justifies Property P1.2.

# Appendix B Time Domain Theory

# B.1 Hilbert Spaces and the Projection Theorem

Most of the material on mean square estimation and regression can be embedded in a more general setting involving an inner product space that is also complete (that is, satisfies the Cauchy condition). Two examples of inner products are $E ( x y ^ { * } )$ , where the elements are random variables, and $\sum x _ { i } y _ { i } ^ { * }$ , where the elements are sequences. These examples include the possibility of complex elements, in which case, ∗ denotes the conjugation. We denote an inner product, in general, by the notation $\langle x , y \rangle$ . Now, define an inner product space by its properties, namely,

(i) $\left. x , y \right. = \left. y , x \right. ^ { * }$   
(ii) $\langle x + y , z \rangle = \langle x , z \rangle + \langle y , z \rangle$   
(iii) $\langle \alpha x , y \rangle = \alpha \langle x , y \rangle$   
(iv) $\langle x , x \rangle = \| x \| ^ { 2 } \geq 0$   
(v) $\langle x , x \rangle = 0$ iff $x = 0$ .

We introduced the notation $\| \cdot \|$ for the norm or distance in property (iv). The norm satisfies the triangle inequality

$$
\| x + y \| \leq \| x \| + \| y \| \tag {B.1}
$$

and the Cauchy–Schwarz inequality

$$
| \langle x, y \rangle | ^ {2} \leq \| x \| ^ {2} \| y \| ^ {2}, \tag {B.2}
$$

which we have seen before for random variables in (A.36). Now, a Hilbert space, $\mathcal { H }$ , is defined as an inner product space with the Cauchy property. In other words, $\mathcal { H }$ is a complete inner product space. This means that every Cauchy sequence converges in norm; that is, $x _ { n } \to x \in { \mathcal { H } }$ if an only if $\| x _ { n } - x _ { m } \| \to 0$

as $m , n \to \infty$ . This is just the $L ^ { 2 }$ completeness Theorem A.1 for random variables.

For a broad overview of Hilbert space techniques that are useful in statistical inference and in probability, see Small and McLeish (1994). Also, Brockwell and Davis (1991, Chapter 2) is a nice summary of Hilbert space techniques that are useful in time series analysis. In our discussions, we mainly use the projection theorem (Theorem B.1) and the associated orthogonality principle as a means for solving various kinds of linear estimation problems.

Theorem B.1 Let $\mathcal { M }$ be a closed subspace of the Hilbert space $\mathcal { H }$ and let $y$ be an element in $\mathcal { H }$ . Then, $y$ can be uniquely represented as

$$
y = \widehat {y} + z, \tag {B.3}
$$

where $\hat { y }$ belongs to $\mathcal { M }$ and $z$ is orthogonal to $\mathcal { M }$ ; that is, $\langle z , w \rangle = 0$ for all w in $\mathcal { M }$ . Furthermore, the point $\widehat { y }$ is closest to y in the sense that, for any w in $\mathcal { M }$ , $\| y - w \| \geq \| y - \widehat { y } \|$ , where equality holds if and only if $w = \hat { y }$ .

We note that (B.3) and the statement following it yield the orthogonality property

$$
\langle y - \widehat {y}, w \rangle = 0 \tag {B.4}
$$

for any $w$ belonging to $\mathcal { M }$ , which can sometimes be used easily to find an expression for the projection. The norm of the error can be written as

$$
\begin{array}{l} \| y - \widehat {y} \| ^ {2} = \langle y - \widehat {y}, y - \widehat {y} \rangle \\ = \langle y - \widehat {y}, y \rangle - \langle y - \widehat {y}, \widehat {y} \rangle \\ = \langle y - \widehat {y}, y \rangle \tag {B.5} \\ \end{array}
$$

because of orthogonality.

Using the notation of Theorem B.1, we call the mapping $P _ { \mathcal { M } } y = \widehat { y }$ , for $y \in \mathcal { H }$ , the projection mapping of $\mathcal { H }$ onto $\mathcal { M }$ . In addition, the closed span of a finite set $\{ x _ { 1 } , \ldots , x _ { n } \}$ of elements in a Hilbert space, $\mathcal { H }$ , is defined to be the set of all linear combinations $w = a _ { 1 } x _ { 1 } + \cdot \cdot \cdot + a _ { n } x _ { n }$ , where $a _ { 1 } , \ldots , a _ { n }$ are scalars. This subspace of $\mathcal { H }$ is denoted by ${ \mathcal { M } } = { \overline { { \operatorname { s p } } } } \{ x _ { 1 } , \dots , x _ { n } \}$ . By the projection theorem, the projection of $y \in { \mathcal { H } }$ onto ${ \mathcal { M } } = { \overline { { \operatorname { s p } } } } \{ x _ { 1 } , \dots , x _ { n } \}$ is unique and given by

$$
P _ {\mathcal {M}} y = a _ {1} x _ {1} + \dots + a _ {n} x _ {n},
$$

where $\{ a _ { 1 } , \ldots , a _ { n } \}$ are found using the orthogonality principle

$$
\langle y - P _ {\mathcal {M}} y, x _ {j} \rangle = 0 \quad j = 1, \dots , n.
$$

Evidently, $\{ a _ { 1 } , \ldots , a _ { n } \}$ can be obtained by solving

$$
\sum_ {i = 1} ^ {n} a _ {i} \left\langle x _ {i}, x _ {j} \right\rangle = \left\langle y, x _ {j} \right\rangle \quad j = 1, \dots , n. \tag {B.6}
$$

When the elements of $\mathcal { H }$ are vectors, this problem is the linear regression problem.

# Example B.1 Linear Regression Analysis

For the regression model introduced in 2.2, we want to find the regression coefficients $\beta _ { i }$ that minimize the residual sum of squares. Consider the vectors $\pmb { y } = ( y _ { 1 } , \dots , y _ { n } ) ^ { \prime }$ and $z _ { i } = ( z _ { 1 i } , \dots , z _ { n i } ) ^ { \prime }$ , for $i = 1 , \ldots , q$ and the inner product

$$
\langle \boldsymbol {z} _ {i}, \boldsymbol {y} \rangle = \sum_ {t = 1} ^ {n} z _ {t i} y _ {t} = \boldsymbol {z} _ {i} ^ {\prime} \boldsymbol {y}.
$$

We solve the problem of finding a projection of the observed $\pmb { y }$ on the linear space spanned by $\beta _ { 1 } z _ { 1 } + \cdot \cdot \cdot + \beta _ { q } z _ { q }$ , that is, linear combinations of the $z _ { i }$ . The orthogonality principle gives

$$
\langle \boldsymbol {y} - \sum_ {i = 1} ^ {q} \beta_ {i} \boldsymbol {z} _ {i}, \boldsymbol {z} _ {j} \rangle = 0
$$

for $j = 1 , \dotsc , q$ . Writing the orthogonality condition, as in (B.6), in vector form gives

$$
\boldsymbol {y} ^ {\prime} \boldsymbol {z} _ {j} = \sum_ {i = 1} ^ {q} \beta_ {i} \boldsymbol {z} _ {i} ^ {\prime} \boldsymbol {z} _ {j} \quad j = 1, \dots , q, \tag {B.7}
$$

which can be written in the usual matrix form by letting $Z = ( z _ { 1 } , \ldots , z _ { q } )$ , which is assumed to be full rank. That is, (B.7) can be written as

$$
\boldsymbol {y} ^ {\prime} Z = \boldsymbol {\beta} ^ {\prime} \left(Z ^ {\prime} Z\right), \tag {B.8}
$$

where $\pmb { \beta } = ( \beta _ { 1 } , \ldots , \beta _ { q } ) ^ { \prime }$ . Transposing both sides of (B.8) provides the solution for the coefficients,

$$
\widehat {\boldsymbol {\beta}} = (Z ^ {\prime} Z) ^ {- 1} Z ^ {\prime} \mathbf {y}.
$$

The mean square error in this case would be

$$
\begin{array}{l} \left\| \boldsymbol {y} - \sum_ {i = 1} ^ {q} \widehat {\beta} _ {i} \boldsymbol {z} _ {i} \right\| ^ {2} = \langle \boldsymbol {y} - \sum_ {i = 1} ^ {q} \widehat {\beta} _ {i} \boldsymbol {z} _ {i}, \boldsymbol {y} \rangle \\ = \langle \boldsymbol {y}, \boldsymbol {y} \rangle - \sum_ {i = 1} ^ {q} \widehat {\beta} _ {i} \left\langle \boldsymbol {z} _ {i}, \boldsymbol {y} \right\rangle \\ = \quad \boldsymbol {y} ^ {\prime} \boldsymbol {y} - \widehat {\beta} ^ {\prime} Z ^ {\prime} \boldsymbol {y}, \\ \end{array}
$$

which is in agreement with §2.2.

The extra generality in the above approach hardly seems necessary in the finite dimensional case, where differentiation works perfectly well. It is convenient, however, in many cases to regard the elements of $\mathcal { H }$ as infinite dimensional, so that the orthogonality principle becomes of use. For example, the

projection of the process $\{ x _ { t }$ ; $t = 0 \pm 1 , \pm 2 , \ldots \}$ on the linear manifold spanned by all filtered convolutions of the form

$$
\widehat {x} _ {t} = \sum_ {k = - \infty} ^ {\infty} a _ {k} x _ {t - k}
$$

would be in this form.

There are some useful results, that we state without proof, pertaining to projection mappings.

Theorem B.2 Under the established notation and conditions:

(i) $P _ { \mathcal { M } } ( a x + b y ) = a P _ { \mathcal { M } } x + b P _ { \mathcal { M } } y$ , for $x , y \in { \mathcal { H } }$ , where a and b are scalars.   
(ii) If $| | y _ { n } - y | |  0$ , then $P _ { \mathcal { M } } y _ { n } \to P _ { \mathcal { M } } y$ , as $n \to \infty$   
(iii) $w \in \mathcal { M }$ if and only if $P _ { \mathcal { M } } w = w$ . Consequently, a projection mapping can be characterized by the property that $P _ { \mathcal { M } } ^ { 2 } = P _ { \mathcal { M } }$ , in the sense that, for any $y \in \mathcal H$ , $P _ { \mathcal { M } } ( P _ { \mathcal { M } } y ) = P _ { \mathcal { M } } y$ .   
(iv) Let $\mathcal { M } _ { 1 }$ and $\mathcal { M } _ { 2 }$ be closed subspaces of $\mathcal { H }$ . Then, $\mathcal { M } _ { 1 } \subseteq \mathcal { M } _ { 2 }$ if and only if $P _ { \mathcal { M } _ { 1 } } ( P _ { \mathcal { M } _ { 2 } } y ) = P _ { \mathcal { M } _ { 1 } } y$ for all $y \in \mathcal H$ .   
(v) Let $\mathcal { M }$ be a closed subspace of $\mathcal { H }$ and let $\mathcal { M } _ { \perp }$ denote the orthogonal complement of $\mathcal { M }$ . Then, $\mathcal { M } _ { \perp }$ is also a closed subspace of $\mathcal { H }$ , and for any $y \in \mathcal { H }$ , $y = P _ { \mathcal M } y + P _ { \mathcal M _ { \bot } } y$ .

Part (iii) of Theorem B.2 leads to the well-known result, often used in linear models, that a square matrix $M$ is a projection matrix if and only if it is symmetric and idempotent (that is, $M ^ { 2 } = M$ ). For example, using notation of Example B.1 for linear regression, the projection of $\pmb { y }$ onto $\overline { { \operatorname { s p } } } \{ \pmb { z } _ { 1 } , \dotsc , \pmb { z } _ { q } \}$ , the space generated by the columns of $Z$ , is $P _ { Z } ( \pmb { y } ) = Z \widehat { \pmb { \beta } } = Z ( Z ^ { \prime } Z ) ^ { - 1 } Z ^ { \prime } \pmb { y }$ . The matrix $M = Z ( Z ^ { \prime } Z ) ^ { - 1 } Z ^ { \prime }$ is an $n \times n$ , symmetric and idempotent matrix of rank $q$ (which is the dimension of the space that $M$ projects $\pmb { y }$ onto). Parts (iv) and (v) of Theorem B.2 are useful for establishing recursive solutions for estimation and prediction.

By imposing extra structure, conditional expectation can be defined as a projection mapping for random variables in $L ^ { 2 }$ with the equivalence relation that, for $x , y \in L ^ { 2 }$ , $x = y$ if $\operatorname* { P r } ( x = y ) = 1$ . In particular, for $y \in L ^ { 2 }$ , if $\mathcal { M }$ is a closed subspace of $L ^ { 2 }$ containing 1, the conditional expectation of $y$ given $\mathcal { M }$ is defined to be the projection of $y$ onto $\mathcal { M }$ , namely, $E _ { \mathcal { M } } y = P _ { \mathcal { M } } y$ . This means that conditional expectation, $E _ { \mathcal { M } }$ , must satisfy the orthogonality principle of the Projection Theorem and that the results of Theorem B.2 remain valid (the most widely used tool in this case is item (iv) of the theorem). If we let $\mathcal M ( x )$ denote the closed subspace of all random variables in $L ^ { 2 }$ that can be written as a (measurable) function of $x$ , then we may define, for $x , y \in L ^ { 2 }$ , the conditional expectation of $y$ given $x$ as $E ( y | x ) = E _ { \mathcal { M } ( x ) } y$ . This idea may

be generalized in an obvious way to define the conditional expectation of $y$ given $\pmb { x } = ( x _ { 1 } , \dots , x _ { n } )$ ; that is $E ( y | \pmb { x } ) = E _ { \mathcal { M } ( \pmb { x } ) } y$ . Of particular interest to us is the following result which states that, in the Gaussian case, conditional expectation and linear prediction are equivalent.

Theorem B.3 Under the established notation and conditions, $i f ( y , x _ { 1 } , \ldots , x _ { n } )$ is multivariate normal, then

$$
E (y \mid x _ {1}, \dots , x _ {n}) = P _ {\overline {{\operatorname {s p}}} \{1, x _ {1}, \dots , x _ {n} \}} y.
$$

Proof. First, by the projection theorem, the conditional expectation of $y$ given $\pmb { x } = \{ x _ { 1 } , \dots , x _ { n } \}$ is the unique element $E _ { \mathcal { M } ( \pmb { x } ) } y$ that satisfies the orthogonality principle,

$$
E \left\{\left(y - E _ {\mathcal {M} (\boldsymbol {x})} y\right) w \right\} = 0 \quad \text {f o r a l l} w \in \mathcal {M} (\boldsymbol {x}).
$$

We will show that $\widehat { y } = P _ { \overline { { \mathrm { s p } } } \{ 1 , x _ { 1 } , . . . , x _ { n } \} } y$ is that element. In fact, by the projection theorem, $\widehat { y }$ satisfies

$$
\langle y - \widehat {y}, x _ {i} \rangle = 0 \quad \text {f o r} i = 0, 1, \dots , n,
$$

where we have set $x _ { 0 } = 1$ . But $\langle y - { \widehat { y } } , x _ { i } \rangle = \operatorname { c o v } ( y - { \widehat { y } } , x _ { i } ) = 0$ , implying that $y - \hat { y }$ and $( x _ { 1 } , \ldots , x _ { n } )$ are independent because the vector $( y - { \widehat { y } } , x _ { 1 } , \ldots , x _ { n } ) ^ { \prime }$ is multivariate normal. Thus, if $w \in \mathcal { M } ( \pmb { x } )$ , then $w$ and $y - \hat { y }$ are independent and, hence, $\langle y - { \widehat { y } } , w \rangle = E \{ ( y - { \widehat { y } } ) w \} = E ( y - { \widehat { y } } ) E ( w ) = 0$ , recalling that $0 = \langle y - { \widehat { y } } , 1 \rangle = E ( y - { \widehat { y } } )$ . ■

In the Gaussian case, conditional expectation has an explicit form. Let ${ \pmb y } =$ $( y _ { 1 } , \ldots , y _ { m } ) ^ { \prime }$ , $\pmb { x } = ( x _ { 1 } , \ldots , x _ { n } ) ^ { \prime }$ , and suppose the $( m + n ) \times 1$ vector $( \pmb { y } ^ { \prime } , \pmb { x } ^ { \prime } ) ^ { \prime }$ is multivariate normal. Then

$$
E (\boldsymbol {y} \mid \boldsymbol {x}) = \boldsymbol {\mu} _ {y} + \Sigma_ {y x} \Sigma_ {x x} ^ {- 1} (\boldsymbol {x} - \boldsymbol {\mu} _ {x}) \tag {B.9}
$$

$$
\operatorname {v a r} (\boldsymbol {y} \mid \boldsymbol {x}) = \Sigma_ {y y} - \Sigma_ {y x} \Sigma_ {x x} ^ {- 1} \Sigma_ {x y}, \tag {B.10}
$$

vector, where $\pmb { \mu } _ { y } = E ( \pmb { y } )$ $\Sigma _ { y x } = \Sigma _ { x y } ^ { \prime } = \operatorname { c o v } ( { \pmb y } , { \pmb x } )$ is $m \times 1$ , $\Sigma _ { y y } = \operatorname { v a r } ( { \pmb y } )$ is $m \times n$ , and is $\Sigma _ { x x } = \operatorname { v a r } ( { \pmb x } )$ $m \times m$ , $\pmb { \mu } _ { x } = E ( \pmb { x } )$ is an $n \times n$ is an matrix, $n \times 1$

# B.2 Causal Conditions for ARMA Models

In this section, we prove Property P3.1 of §3.2 pertaining to the causality of ARMA models. The proof of Property P3.2, which pertains to invertibility of ARMA models, is similar.

Proof of Property P3.1. Suppose first that the roots of $\phi ( z )$ , say, $z _ { 1 } , \dotsc , z _ { p }$ , lie outside the unit circle. We write the roots in the following order, $1 < | z _ { 1 } | \leq$ $| z _ { 2 } | \leq \cdots \leq | z _ { p } |$ , noting that $z _ { 1 } , \dotsc , z _ { p }$ are not necessarily unique, and put

$| z _ { 1 } | = 1 + \epsilon$ , for some $\epsilon > 0$ . Thus, $\phi ( z ) \neq 0$ as long as $| z | < | z _ { 1 } | = 1 + \epsilon$ and, hence, $\phi ^ { - 1 } ( z )$ exists and has a power series expansion,

$$
\frac {1}{\phi (z)} = \sum_ {j = 0} ^ {\infty} a _ {j} z ^ {j}, \quad | z | <   1 + \epsilon .
$$

Now, choose a value $\delta$ such that $0 < \delta < \epsilon$ , and set $z = 1 + \delta$ , which is inside the radius of convergence. It then follows that

$$
\phi^ {- 1} (1 + \delta) = \sum_ {j = 0} ^ {\infty} a _ {j} (1 + \delta) ^ {j} <   \infty . \tag {B.11}
$$

Thus, we can bound each of the terms in the sum in (B.11) by constant, say, $| a _ { j } ( 1 + \delta ) ^ { j } | < c$ , for $c > 0$ . In turn, $| a _ { j } | < c ( 1 + \delta ) ^ { - j }$ , from which it follows that

$$
\sum_ {j = 0} ^ {\infty} \left| a _ {j} \right| <   \infty . \tag {B.12}
$$

Hence, $\phi ^ { - 1 } ( B )$ exists and we may apply it to both sides of the ARMA model, $\phi ( B ) x _ { t } = \theta ( B ) w _ { t }$ , to obtain

$$
x _ {t} = \phi^ {- 1} (B) \phi (B) x _ {t} = \phi^ {- 1} (B) \theta (B) w _ {t}.
$$

Thus, putting $\psi ( B ) = \phi ^ { - 1 } ( B ) \theta ( B )$ , we have

$$
x _ {t} = \psi (B) w _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j},
$$

where the $\psi$ -weights, which are absolutely summable, can be evaluated by $\psi ( z ) = \phi ^ { - 1 } ( z ) \theta ( z )$ , for $| z | \le 1$ .

Now, suppose $x _ { t }$ is a causal process; that is, it has the representation

$$
x _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j}, \qquad \sum_ {j = 0} ^ {\infty} | \psi_ {j} | <   \infty .
$$

In this case, we write

$$
x _ {t} = \psi (B) w _ {t},
$$

and premultiplying by $\phi ( B )$ yields

$$
\phi (B) x _ {t} = \phi (B) \psi (B) w _ {t}. \tag {B.13}
$$

In addition to (B.13), the model is ARMA, and can be written as

$$
\phi (B) x _ {t} = \theta (B) w _ {t}. \tag {B.14}
$$

From (B.13) and (B.14), we see that

$$
\phi (B) \psi (B) w _ {t} = \theta (B) w _ {t}. \tag {B.15}
$$

Now, let

$$
a (z) = \phi (z) \psi (z) = \sum_ {j = 0} ^ {\infty} a _ {j} z ^ {j} \quad | z | \leq 1
$$

and, hence, we can write (B.15) as

$$
\sum_ {j = 0} ^ {\infty} a _ {j} w _ {t - j} = \sum_ {j = 0} ^ {q} \theta_ {j} w _ {t - j}. \tag {B.16}
$$

Next, multiply both sides of (B.16) by $w _ { t - h }$ , for $h = 0 , 1 , 2 , \ldots$ , and take expectation. In doing this, we obtain

$$
\begin{array}{l} a _ {h} = \theta_ {h}, h = 0, 1, \dots , q \\ a _ {h} = 0, \quad h > q. \tag {B.17} \\ \end{array}
$$

From (B.17), we conclude that

$$
\phi (z) \psi (z) = a (z) = \theta (z), \quad | z | \leq 1. \tag {B.18}
$$

If there is a complex number in the unit circle, say $z _ { 0 }$ , for which $\phi ( z _ { 0 } ) = 0$ , then by (B.18), $\theta ( z _ { 0 } ) = 0$ . But, if there is such a $z _ { 0 }$ , then $\phi ( z )$ and $\theta ( z )$ have a common factor which is not allowed. Thus, we may write $\psi ( z ) = \theta ( z ) / \phi ( z )$ . In addition, by hypothesis, we have that $| \psi ( z ) | < \infty$ for $| z | \leq 1$ , and hence

$$
| \psi (z) | = \left| \frac {\theta (z)}{\phi (z)} \right| <   \infty , \quad \text {f o r} | z | \leq 1. \tag {B.19}
$$

Finally, (B.19) implies $\phi ( z ) \neq 0$ for $| z | \le 1$ ; that is, the roots of $\phi ( z )$ lie outside the unit circle.

# B.3 Large Sample Distribution of the AR( $p$ ) Conditional Least Squares Estimators

In 3.6 we discussed the conditional least squares procedure for estimating the parameters $\phi _ { 1 } , \phi _ { 2 } , \ldots , \phi _ { p }$ and $\sigma _ { w } ^ { 2 }$ in the AR( $p$ ) model

$$
x _ {t} = \sum_ {k = 1} ^ {p} \phi_ {k} x _ {t - k} + w _ {t},
$$

where we assume $\mu = 0$ , for convenience. Write the model as

$$
x _ {t} = \phi^ {\prime} \boldsymbol {x} _ {t - 1} + w _ {t}, \tag {B.20}
$$

where $\pmb { x } _ { t - 1 } = ( x _ { t - 1 } , x _ { t - 2 } , . . . , x _ { t - p } ) ^ { \prime }$ is a $p \times 1$ vector of lagged values, and ${ \pmb { \phi } } = ( \phi _ { 1 } , \phi _ { 2 } , \ldots , \phi _ { p } ) ^ { \prime }$ is the $p \times 1$ vector of regression coefficients. Assuming observations are available at $x _ { 1 } , \ldots , x _ { n }$ , the conditional least squares procedure is to minimize

$$
S _ {c} (\pmb {\phi}) = \sum_ {t = p + 1} ^ {n} \left(x _ {t} - \pmb {\phi} ^ {\prime} \pmb {x} _ {t - 1}\right) ^ {2}
$$

with respect to $\phi$ . The solution is

$$
\widehat {\boldsymbol {\phi}} = \left(\sum_ {t = p + 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) ^ {- 1} \sum_ {t = p + 1} ^ {n} \boldsymbol {x} _ {t - 1} x _ {t} \tag {B.21}
$$

for the regression vector $\phi$ ; the conditional least squares estimate of $\sigma _ { w } ^ { 2 }$ i s

$$
\hat {\sigma} _ {w} ^ {2} = \frac {1}{n - p} \sum_ {t = p + 1} ^ {n} \left(x _ {t} - \widehat {\boldsymbol {\phi}} ^ {\prime} \boldsymbol {x} _ {t - 1}\right) ^ {2}. \tag {B.22}
$$

As pointed out following (3.104), Yule–Walker estimators and least squares estimators are approximately the same in that the estimators differ only by inclusion or exclusion of terms involving the endpoints of the data. Hence, it is easy to show the asymptotic equivalence of the two estimators; this is why, for $\operatorname { A R } ( p )$ models, (3.93) and (3.118), are equivalent. Details on the asymptotic equivalence can be found in Brockwell and Davis (1991, Chapter 8).

Here, we use the same approach as in Appendix A, replacing the lower limits of the sums in (B.21) and (B.22) by one and noting the asymptotic equivalence of the estimators

$$
\tilde {\boldsymbol {\phi}} = \left(\sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} x _ {t} \tag {B.23}
$$

and

$$
\widetilde {\sigma} _ {w} ^ {2} = \frac {1}{n} \sum_ {t = 1} ^ {n} \left(x _ {t} - \widetilde {\phi} ^ {\prime} \boldsymbol {x} _ {t - 1}\right) ^ {2} \tag {B.24}
$$

to those two estimators. In (B.23) and (B.24), we are acting as if we are able to observe $x _ { 1 - p } , \ldots , x _ { 0 }$ in addition to $x _ { 1 } , \ldots , x _ { n }$ . The asymptotic equivalence is then seen by arguing that for $n$ sufficiently large, it makes no difference whether or not we observe $x _ { 1 - p } , \ldots , x _ { 0 }$ . In the case of (B.23) and (B.24), we obtain the following theorem.

Theorem B.4 Let $x _ { t }$ be a causal $A R ( p )$ series with white (iid) noise $w _ { t }$ satisfying $E ( w _ { t } ^ { 4 } ) = \eta \sigma _ { w } ^ { 4 }$ . Then,

$$
\widetilde {\boldsymbol {\phi}} \sim \mathrm {A N} \left(\boldsymbol {\phi}, n ^ {- 1} \sigma_ {w} ^ {2} \Gamma_ {p} ^ {- 1}\right), \tag {B.25}
$$

where $\Gamma _ { p } = \{ \gamma ( i - j ) \} _ { i , j = 1 } ^ { p }$ is the $p \times p$ autocovariance matrix of the vector $\pmb { x } _ { t - 1 }$ . We also have, as $n \to \infty$ ,

$$
n ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime} \stackrel {{p}} {{\rightarrow}} \Gamma_ {p} \tag {B.26}
$$

and

$$
\widetilde {\sigma} _ {w} ^ {2} \xrightarrow {p} \sigma_ {w} ^ {2}. \tag {B.27}
$$

Proof. First, (B.26) follows from the fact that $E ( \pmb { x } _ { t - 1 } \pmb { x } _ { t - 1 } ^ { \prime } ) = \Gamma _ { p }$ , recalling that from Theorem 1.6, second-order sample moments converge in probability to their population moments for linear processes in which $w _ { t }$ has a finite fourth moment. To show (B.25), we can write

$$
\begin{array}{l} \widetilde {\boldsymbol {\phi}} = \left(\sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} \left(\boldsymbol {x} _ {t - 1} ^ {\prime} \boldsymbol {\phi} + w _ {t}\right) \\ = \quad \boldsymbol {\phi} + \left(\sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} w _ {t}, \\ \end{array}
$$

so that

$$
\begin{array}{l} n ^ {1 / 2} (\widetilde {\boldsymbol {\phi}} - \boldsymbol {\phi}) = \left(n ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) ^ {- 1} n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} w _ {t} \\ = \left(n ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) ^ {- 1} n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {u} _ {t}, \\ \end{array}
$$

where ${ \pmb u } _ { t } = { \pmb x } _ { t - 1 } { \pmb w } _ { t }$ . We use the fact that $w _ { t }$ and ${ \pmb x } _ { t - 1 }$ are independent to write $E \pmb { u } _ { t } = E ( \pmb { x } _ { t - 1 } ) E ( \boldsymbol { w } _ { t } ) = \mathbf { 0 }$ , because the errors have zero means. Also,

$$
\begin{array}{l} E \boldsymbol {u} _ {t} \boldsymbol {u} _ {t} ^ {\prime} = E \boldsymbol {x} _ {t - 1} w _ {t} w _ {t} \boldsymbol {x} _ {t - 1} ^ {\prime} \\ = E \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime} E w _ {t} ^ {2} \\ = \sigma_ {w} ^ {2} \Gamma_ {p}. \\ \end{array}
$$

In addition, we have, for $h > 0$ ,

$$
\begin{array}{l} E \boldsymbol {u} _ {t + h} \boldsymbol {u} _ {t} ^ {\prime} = E \boldsymbol {x} _ {t + h - 1} w _ {t + h} w _ {t} \boldsymbol {x} _ {t - 1} ^ {\prime} \\ = E \boldsymbol {x} _ {t + h - 1} w _ {t} \boldsymbol {x} _ {t - 1} ^ {\prime} E w _ {t + h} \\ = 0. \\ \end{array}
$$

A similar computation works for $h < 0$

Next, consider the mean square convergent approximation

$$
x _ {t} ^ {m} = \sum_ {j = 0} ^ {m} \psi_ {j} w _ {t - j}
$$

for $x _ { t }$ , and define the $( m + p )$ -dependent process $\pmb { u } _ { t } ^ { m } = w _ { t } ( x _ { t - 1 } ^ { m } , x _ { t - 2 } ^ { m } , . . . , x _ { t - p } ^ { m } ) ^ { \prime }$ . Note that we need only look at a central limit theorem for the sum

$$
y _ {n m} = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {\lambda^ {\prime}} \boldsymbol {u} _ {t} ^ {m},
$$

for arbitrary vectors $\pmb { \lambda } = ( \lambda _ { 1 } , \ldots , \lambda _ { p } ) ^ { \prime }$ , where $y _ { n m }$ is used as an approximation to

$$
S _ {n} = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {\lambda^ {\prime}} \boldsymbol {u} _ {t}.
$$

First, apply the $m$ -dependent central limit theorem to $y _ { n m }$ as $n \to \infty$ for fixed $m$ to establish (i) of Theorem A.4. This result shows $y _ { n m } \stackrel { d } { \to } y _ { m }$ , where $y _ { m }$ i s asymptotically normal with covariance λ-Γ(m)p λ $\lambda ^ { \prime } \Gamma _ { p } ^ { ( m ) } \lambda$ , where $\Gamma _ { p } ^ { ( m ) }$ is the covariance matrix of ${ \pmb u } _ { t } ^ { m }$ . Then, we hav e Γ(m)p $\Gamma _ { p } ^ { ( m ) }  \Gamma _ { p }$ , so that $y _ { m }$ converges in distribution to a normal random variable with mean zero and variance $\lambda ^ { \prime } \Gamma _ { p } \lambda$ and we have verified part (ii) of Theorem A.4. We verify part (iii) of Theorem A.4 by noting that

$$
E \left[ \left(S _ {n} - y _ {n m}\right) ^ {2} \right] = n ^ {- 1} \sum_ {t = 1} ^ {n} \lambda^ {\prime} E \left[ \left(\boldsymbol {u} _ {t} - \boldsymbol {u} _ {t} ^ {m}\right) \left(\boldsymbol {u} _ {t} - \boldsymbol {u} _ {t} ^ {m}\right) ^ {\prime} \right] \boldsymbol {\lambda}
$$

clearly converges to zero as $n , m  \infty$ because

$$
x _ {t} - x _ {t} ^ {m} = \sum_ {j = m + 1} ^ {\infty} \psi_ {j} w _ {t - j}
$$

form the components of ${ \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } { \mathbf { } } u _ { t } ^ { m } - { \mathbf { } } u _ { t } ^ { m }$ .

Now, the form for $\sqrt { n } ( \tilde { \phi } - \phi )$ contains the premultiplying matrix

$$
\left(n ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) ^ {- 1} \stackrel {{p}} {{\rightarrow}} \Gamma_ {p} ^ {- 1},
$$

because (A.22) can be applied to the function that defines the inverse of the matrix. Then, applying (A.31), shows that

$$
n ^ {1 / 2} \left(\widetilde {\boldsymbol {\phi}} - \boldsymbol {\phi}\right) \xrightarrow {d} \mathrm {N} \left(0, \sigma_ {w} ^ {2} \Gamma_ {p} ^ {- 1} \Gamma_ {p} \Gamma_ {p} ^ {- 1}\right),
$$

so we may regard it as being multivariate normal with mean zero and covariance matrix σ2wΓ−1p . $\sigma _ { w } ^ { 2 } \Gamma _ { p } ^ { - 1 }$

To investigate $\widetilde { \sigma } _ { w } ^ { 2 }$ , note

$$
\begin{array}{l} \widetilde {\sigma} _ {w} ^ {2} = n ^ {- 1} \sum_ {t = 1} ^ {n} \left(x _ {t} - \widetilde {\boldsymbol {\phi}} ^ {\prime} \boldsymbol {x} _ {t - 1}\right) ^ {2} \\ = n ^ {- 1} \sum_ {t = 1} ^ {n} x _ {t} ^ {2} - n ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} ^ {\prime} x _ {t} \left(n ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime}\right) ^ {- 1} n ^ {- 1} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t - 1} x _ {t} \\ \end{array}
$$

$$
\begin{array}{l} \stackrel {p} {\rightarrow} \quad \gamma (0) - \boldsymbol {\gamma} _ {p} ^ {\prime} \Gamma_ {p} ^ {- 1} \boldsymbol {\gamma} _ {p} \\ = \sigma_ {w} ^ {2}, \\ \end{array}
$$

and we have that the sample estimator converges in probability to $\sigma _ { w } ^ { 2 }$ , which is written in the form of (3.59). ■

The arguments above imply that, for sufficiently large $n$ , we may consider the estimator $\widehat { \phi }$ in (B.21) as being approximately multivariate normal with mean meter $\phi$ and variance–covariance matriare obtained by replacing the $\sigma _ { w } ^ { 2 } \Gamma _ { p } ^ { - 1 } / n$ w . Inferences about the para-by their estimates given by $\phi$ $\sigma _ { w } ^ { 2 }$ $\Gamma _ { p }$ (B.22) and

$$
\widehat {\Gamma} _ {p} = n ^ {- 1} \sum_ {t = p + 1} ^ {n} \boldsymbol {x} _ {t - 1} \boldsymbol {x} _ {t - 1} ^ {\prime},
$$

respectively. In the case of a nonzero mean, the data $x _ { t }$ are replaced by $x _ { t } - x$ in the estimates and the results of Theorem B.4 remain valid.

# B.4 The Wold Decomposition

The ARMA approach to modeling time series is generally implied by the assumption that the dependence between adjacent values in time is best explained in terms of a regression of the current values on the past values. This assumption is partially justified, in theory, by the Wold decomposition.

In this section we assume that $\{ x _ { t } ; ~ t = 0 , \pm 1 , \pm 2 , . . . \}$ is a stationary, mean-zero process. Using the notation of §B.1, we define

$$
\mathcal {M} _ {n} = \overline {{\operatorname {s p}}} \{x _ {t}, - \infty <   t \leq n \}, \quad \text {w i t h} \quad \mathcal {M} _ {- \infty} = \bigcap_ {n = - \infty} ^ {\infty} \mathcal {M} _ {n},
$$

and

$$
\sigma^ {2} = E \left(x _ {n + 1} - P _ {\mathcal {M} _ {n}} x _ {n + 1}\right) ^ {2}.
$$

Next, we say that $\mathit { \Psi } : t = 0 , \pm 1 , \pm 2 , \ldots \rbrace$ is a deterministic process if and only if $\sigma ^ { 2 } = 0$ . That is, a deterministic process is one in which its future is perfectly predictable from its past; a simple example is $v _ { t } = \cos ( . 2 \pi t )$ . We are now ready to present the decomposition.

Theorem B.5 (The Wold Decomposition) Under the conditions and notation of this section, if $\sigma ^ { 2 } > 0$ , then $x _ { t }$ can be expressed as

$$
x _ {t} = \sum_ {j = 0} ^ {\infty} \psi_ {j} w _ {t - j} + v _ {t}
$$

where

• $\begin{array} { r } { \sum _ { j = 0 } ^ { \infty } \psi _ { j } ^ { 2 } < \infty ~ ( \psi _ { 0 } = 1 ) } \end{array}$   
· $\{ w _ { t } \}$ is white noise with variance $\sigma ^ { 2 }$   
• wt ∈ Mt   
• $E ( w _ { t } v _ { t } ) = 0$ for all $s , t = 0 , \pm 1 , \pm 2 , . . .$ .   
• $v _ { t } \in \mathcal { M } _ { - \infty }$   
• $\{ v _ { t } \}$ is deterministic.

The proof of the decomposition follows from the theory of §B.1 by defining the unique sequences:

(i) $w _ { t } = x _ { t } - P _ { \cdot } { } _ { M _ { t - 1 } } x _ { t } ,$   
(ii) $\psi _ { j } = \sigma ^ { - 2 } < x _ { t } , w _ { t - j } > = \sigma ^ { - 2 } E ( x _ { t } w _ { t - j } )$ , and   
(iii) $\begin{array} { r } { v _ { t } = x _ { t } - \sum _ { j = 0 } ^ { \infty } \psi _ { j } w _ { t - j } , } \end{array}$

Although every stationary process can be represented by the Wold decomposition, it does not mean that the decomposition is the best way to describe the process. In addition, there may be some dependence structure among the $\{ w _ { t } \}$ ; we are only guaranteed that the sequence is an uncorrelated sequence. The theorem, in its generality, falls short of our needs because we would prefer the noise process, $\{ w _ { t } \}$ , to be white independent noise. But, the decomposition does give us the confidence that we will not be completely off the mark by fitting ARMA models to time series data.

# Appendix C Spectral Domain Theory

# C.1 Spectral Representation Theorem

In this section, we present a spectral representation for the process $x _ { t }$ itself, which allows us to think of a stationary process as a random sum of sines and cosines as described in (4.4). In addition, we present results that justify representing the autocovariance function $\gamma _ { x } ( h )$ of the weakly stationary process $x _ { t }$ in terms of a non-negative spectral density function. The spectral density function essentially measures the variance or power in a particular kind of periodic oscillation in the function. We denote this spectral density of variance function by $f ( \omega )$ , where the variance is measured as a function of the frequency of oscillation $\omega$ , measured in cycles per unit time.

First, we consider developing a representation for the autocovariance function of a stationary, possibly complex, series $x _ { t }$ with zero mean and autocovariance function $\gamma _ { x } ( h ) = E ( x _ { t + h } x _ { t } ^ { * } )$ . We prove the representation for arbitrary non-negative definite functions $\gamma ( h )$ and then simply note the autocovariance function is Hermitian non-negative definite, because, for any set of complex constants, $a _ { t } , t = 0 \pm 1 , \pm 2 , . . .$ , we may write, for any finite subset,

$$
E \left| \sum_ {s = 1} ^ {n} a _ {s} ^ {*} x _ {s} \right| ^ {2} = \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} a _ {s} ^ {*} \gamma (s - t) a _ {t} \geq 0.
$$

The representation is stated in terms of non-negative definite functions and a spectral distribution function $F ( \omega )$ that is monotone nondecreasing, and continuous from the right, taking the values $F ( - 1 / 2 ) = 0$ and $F ( 1 / 2 ) = \sigma ^ { 2 } =$ $\gamma _ { x } ( 0 )$ at $\omega = - 1 / 2$ and $1 / 2$ , respectively.

Theorem C.1 A function $\gamma ( h )$ , for $h = 0 \pm 1 , \pm 2 , . . .$ is Hermitian nonnegative definite if and only if it can be expressed as

$$
\gamma (h) = \int_ {- 1 / 2} ^ {1 / 2} \exp \{2 \pi i \omega h \} d F (\omega) \tag {C.1}
$$

where $F ( \cdot )$ is monotone non-decreasing. The function $F ( \cdot )$ is right continuous, bounded in $[ - 1 / 2 , 1 / 2 ]$ , and uniquely determined by the conditions $F ( - 1 / 2 ) =$ $0 , F ( 1 / 2 ) = \gamma ( 0 )$ .

Proof. To prove the result, note first if $\gamma ( h )$ has the representation above,

$$
\begin{array}{l} \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} a _ {s} ^ {*} \gamma (s - t) a _ {t} = \int_ {- 1 / 2} ^ {1 / 2} a _ {s} ^ {*} \gamma (s - t) a _ {t} e ^ {2 \pi i \omega (s - t)} d F (\omega) \\ = \int_ {- 1 / 2} ^ {1 / 2} \left| \sum_ {s = 1} ^ {n} a _ {s} e ^ {- 2 \pi i \omega s} \right| ^ {2} d F (\omega) \\ \geq 0 \\ \end{array}
$$

and $\gamma ( h )$ is non-negative definite. Conversely, suppose $\gamma ( h )$ is a non-negative definite function, and define the non-negative function

$$
\begin{array}{l} f _ {n} (\omega) = n ^ {- 1} \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} e ^ {- 2 \pi i \omega s} \gamma (s - t) e ^ {2 \pi i \omega t} \\ = n ^ {- 1} \sum_ {u = - (n - 1)} ^ {(n - 1)} (n - | u |) e ^ {- 2 \pi i \omega u} \gamma (u) \tag {C.2} \\ \geq 0. \\ \end{array}
$$

Now, let $F _ { n } ( \omega )$ be the distribution function corresponding to $f _ { n } ( \omega ) I _ { ( - 1 / 2 , 1 / 2 ] }$ , where $I _ { ( \cdot ) }$ denotes the indicator function of the interval in the subscript. Note that $F _ { n } ( \omega ) = 0 , \omega \leq - 1 / 2$ and $F _ { n } ( \omega ) = F _ { n } ( 1 / 2 )$ for $\omega \ge 1 / 2$ . Then,

$$
\begin{array}{l} \int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i \omega u} d F _ {n} (\omega) = \int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i \omega u} f _ {n} (\omega) d \omega \\ = \left\{ \begin{array}{l l} (1 - | u | / n) \gamma (u), & | u | <   n \\ 0, & \text {e l s e w h e r e .} \end{array} \right. \\ \end{array}
$$

We also have

$$
\begin{array}{l} F _ {n} (1 / 2) = \int_ {- 1 / 2} ^ {1 / 2} f _ {n} (\omega) d \omega \\ = \int_ {- 1 / 2} ^ {1 / 2} \sum_ {| u | <   n} (1 - | u | / n) \gamma (u) e ^ {- 2 \pi i \omega u} d \omega \\ = \gamma (0). \\ \end{array}
$$

Now, by Helly’s first convergence theorem (Bhat, 1985, p. 157), there exists a subsequence $F _ { n _ { k } }$ converging to $F ^ { \prime }$ , and by the Helly-Bray Lemma (see Bhat,

p. 157), this implies

$$
\int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i \omega u} d F _ {n _ {k}} (\omega) \rightarrow \int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i \omega u} d F (\omega)
$$

and, from the right-hand side of the earlier equation,

$$
(1 - | u | / n _ {k}) \gamma (u) \rightarrow \gamma (u)
$$

as $n _ { k } \to \infty$ , and the required result follows.

Next, present the version of the Spectral Representation Theorem in terms of a mean-zero, stationary process, $x _ { t }$ . We refer the reader to Hannan (1970, 2.3) for details. This version allows us to think of a stationary process as being generated (approximately) by a random sum of sines and cosines such as described in (4.4).

Theorem C.2 If $x _ { t }$ is a mean-zero stationary process, with spectral distribution $F ( \omega )$ as given in Theorem C.1, then there exists a complex-valued stochastic process $z ( \omega )$ , on the interval $\omega \in [ - 1 / 2 , 1 / 2 ]$ , having stationary uncorrelated increments, such that $x _ { t }$ can be written as the stochastic integral

$$
x _ {t} = \int_ {- 1 / 2} ^ {1 / 2} \exp (- 2 \pi i t \omega) d z (\omega)
$$

where, for $- 1 / 2 \le \omega _ { 1 } \le \omega _ { 2 } \le 1 / 2$ ,

$$
\operatorname {v a r} \left\{z \left(\omega_ {2}\right) - z \left(\omega_ {1}\right) \right\} = F \left(\omega_ {2}\right) - F \left(\omega_ {1}\right).
$$

An uncorrelated increment process such as $z ( \omega )$ is a mean-zero, finite variance, continuous-time stochastic process for which events occurring in nonoverlapping intervals are uncorrelated. The integral in this representation is a stochastic integral. To understand its meaning, let $\omega _ { 0 } , \omega _ { 1 } , \ldots , \omega _ { n }$ be a partition of the interval $[ - 1 / 2 , 1 / 2 ]$ . Define

$$
I _ {n} = \sum_ {j = 1} ^ {n} \exp (- 2 \pi i t \omega_ {j}) [ z (\omega_ {j}) - z (\omega_ {j - 1}) ].
$$

Then, assuming it exists, $\begin{array} { r } { I = \int _ { - 1 / 2 } ^ { 1 / 2 } \exp ( - 2 \pi i t \omega _ { j } ) d z ( \omega ) } \end{array}$ : 1/21/2 exp(−2πitωj )dz(ω) is defined to be the mean square limit of $I _ { n }$ as $n  \infty$ . Theorem C.2 allows us to think of a stationary process, approximately, as the random superposition of sines and cosines.

In general, the spectral distribution function can be a mixture of discrete and continuous distributions. The special case of greatest interest is the absolutely continuous case, namely, when $d F ( \omega ) = f ( \omega ) d \omega$ , and the resulting

function is the spectral density considered in §4.3. What made the proof of Theorem C.1 difficult was that, after we defined

$$
f _ {n} (\omega) = \sum_ {h = - (n - 1)} ^ {(n - 1)} \left(1 - \frac {| h |}{n}\right) \gamma (h) e ^ {- 2 \pi i \omega h}
$$

in (C.2), we could not simply allow $n \to \infty$ because $\gamma ( h )$ may not be absolutely summable. If, however, $\gamma ( h )$ is absolutely summable we may define $f ( \omega ) =$ $\scriptstyle \operatorname* { l i m } _ { n \to \infty } f _ { n } ( \omega )$ , and we have the following result.

Theorem C.3 If $\gamma ( h )$ is the autocovariance function of a stationary process, $x _ { t }$ , with

$$
\sum_ {h = - \infty} ^ {\infty} | \gamma (h) | <   \infty , \tag {C.3}
$$

then the spectral density of $x _ { t }$ is given by

$$
f (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma (h) e ^ {- 2 \pi i \omega h}. \tag {C.4}
$$

We may extend the representation to the vector case $\pmb { x } _ { t } = ( x _ { t 1 } , \ldots , x _ { t p } ) ^ { \prime }$ by considering linear combinations of the form

$$
y _ {t} = \sum_ {j = 1} ^ {p} a _ {j} ^ {*} x _ {t j},
$$

which will be stationary with autocovariance functions of the form

$$
\gamma_ {y} (h) = \sum_ {j = 1} ^ {p} \sum_ {k = 1} ^ {p} a _ {j} ^ {*} \gamma_ {j k} (h) a _ {k},
$$

where $\gamma _ { j k } ( h )$ is the usual cross-covariance function between $x _ { t j }$ and $x _ { t k }$ . To develop the spectral representation of $\gamma _ { j k } ( h )$ from the representations of the univariate series, consider the linear combinations

$$
y _ {t 1} = x _ {t j} + x _ {t k}
$$

and

$$
y _ {t 2} = x _ {t j} + i x _ {t k},
$$

which are both stationary series with respective representations

$$
\begin{array}{l} \gamma_ {y 1} (h) = \gamma_ {j} (h) + \gamma_ {j k} (h) + \gamma_ {k j} (h) + \gamma_ {k} (h) \\ = \int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i \omega h} d G _ {1} (\omega) \\ \end{array}
$$

and

$$
\begin{array}{l} \gamma_ {y 2} (h) = \gamma_ {j} (h) + i \gamma_ {k j} (h) - i \gamma_ {j k} (h) + \gamma_ {k} (h) \\ = \int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i \omega h} d G _ {2} (\omega). \\ \end{array}
$$

Introducing the spectral representations for $\gamma _ { j } ( h )$ and $\gamma _ { k } ( h )$ yields

$$
\gamma_ {j k} (h) = \int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i \omega h} d F _ {j k} (\omega),
$$

with

$$
F _ {j k} (\omega) = \frac {1}{2} \left[ G _ {1} (\omega) + i G _ {2} (\omega) - (1 + i) \left(F _ {j} (\omega) + F _ {k} (\omega) \right] \right.
$$

Now, under the summability condition

$$
\sum_ {h = - \infty} ^ {\infty} | \gamma_ {j k} (h) | <   \infty ,
$$

we have the representation

$$
\gamma_ {j k} (h) = \int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i \omega h} f _ {j k} (\omega) d \omega ,
$$

where the cross-spectral density function has the inverse Fourier representation

$$
f _ {j k} (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma_ {j k} (h) e ^ {- 2 \pi i \omega h}.
$$

The cross-covariance function satisfies $\gamma _ { j k } ( h ) = \gamma _ { k j } ( - h )$ , which implies $f _ { j k } ( \omega ) =$ $f _ { k j } ( - \omega )$ using the above representation.

Then, defining the autocovariance function of the general vector process ${ \pmb x } _ { t }$ as the $p \times p$ matrix

$$
\Gamma (h) = E \left[ \left(\boldsymbol {x} _ {t + h} - \boldsymbol {\mu} _ {x}\right) \left(\boldsymbol {x} _ {t} - \boldsymbol {\mu} _ {x}\right) ^ {\prime} \right],
$$

and the $p \times p$ spectral matrix as $f ( \omega ) = \{ f _ { j k } ( \omega ) , j , k = 1 , \ldots , p \}$ , we have the representation in matrix form, written as

$$
\Gamma (h) = \int_ {- 1 / 2} ^ {1 / 2} e ^ {2 \pi i \omega h} f (\omega) d \omega , \tag {C.5}
$$

and the inverse result

$$
f (\omega) = \sum_ {h = - \infty} ^ {\infty} \Gamma (h) e ^ {- 2 \pi i \omega h}. \tag {C.6}
$$

which appears as Property P4.3 in §4.6. Theorem C.2 can also be extended to the multivariate case.

# C.2 Large Sample Distribution of the DFT and Smoothed Periodogram

We have previously introduced the DFT, for the stationary zero-mean process $x _ { t }$ , observed at $t = 1 , \ldots , n$ as

$$
d (\omega) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} e ^ {- 2 \pi i \omega t}, \tag {C.7}
$$

as the result of matching sines and cosines of frequency $\omega$ against the series $x _ { t }$ . We will suppose now that $x _ { t }$ has an absolutely continuous spectrum $f ( \omega )$ corresponding to the absolutely summable autocovariance function $\gamma ( h )$ . Our purpose in this section is to examine the statistical properties of the complex random variables $d ( \omega _ { k } )$ , for $\omega _ { k } = k / n$ , $k = 0 , 1 , \ldots , n - 1$ in providing a basis for the estimation of $f ( \omega )$ . To develop the statistical properties, we examine the behavior of

$$
\begin{array}{l} S _ {n} (\omega , \omega) = E \left\{\left| d (\omega) \right| ^ {2} \right\} \\ = n ^ {- 1} E \left[ \sum_ {s = 1} ^ {n} x _ {s} e ^ {- 2 \pi i \omega s} \sum_ {t = 1} ^ {n} x _ {t} e ^ {2 \pi i \omega t} \right] \\ = n ^ {- 1} \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} e ^ {- 2 \pi i \omega s} e ^ {2 \pi i \omega t} \gamma (s - t) \\ = \sum_ {u = - (n - 1)} ^ {(n - 1)} (1 - | u | / n) \gamma (u) e ^ {- 2 \pi i \omega u}, \tag {C.8} \\ \end{array}
$$

where we have let $u = s - t$ . Using dominated convergence,

$$
S _ {n} (\omega , \omega) \rightarrow \sum_ {u = - \infty} ^ {\infty} \gamma (u) e ^ {- 2 \pi i \omega u} = f (\omega),
$$

as $n \to \infty$ , making the large sample variance of the Fourier transform equal to the spectrum evaluated at $\omega$ . We have already seen this result in Theorem C.3. For exact bounds it is also convenient to add an absolute summability assumption for the autocovariance function, namely,

$$
\theta = \sum_ {h = - \infty} ^ {\infty} | h | | \gamma (h) | <   \infty . \tag {C.9}
$$

# Example C.1 Condition (C.9) Verified for an AR(1)

We may write the condition for an AR(1) series. $x _ { t } = \phi x _ { t - 1 } + w _ { t }$ , as

$$
\theta = \frac {\sigma_ {w} ^ {2}}{1 - \phi^ {2}} \sum_ {h = - \infty} ^ {\infty} | h | \phi^ {| h |}
$$

being finite. Note the condition is equivalent to summability of

$$
\begin{array}{l} \sum_ {h = 1} ^ {\infty} h \phi^ {h} = \phi \sum_ {h = 1} ^ {\infty} h \phi^ {h - 1} \\ = \phi \frac {\partial}{\partial \phi} \sum_ {h = 1} ^ {\infty} \phi^ {h} \\ = \frac {\phi}{(1 - \phi) ^ {2}}, \\ \end{array}
$$

and hence,

$$
\theta = \frac {2 \sigma_ {w} ^ {2} \phi}{(1 - \phi) ^ {3} (1 + \phi)} <   \infty .
$$

To elaborate further, we derive two approximation lemmas.

Lemma C.1 For $S _ { n } ( \omega , \omega )$ as defined in (C.8) and θ in (C.9) finite, we have

$$
\left| S _ {n} (\omega , \omega) - f (\omega) \right| \leq \frac {\theta}{n} \tag {C.10}
$$

or

$$
S _ {n} (\omega , \omega) = f (\omega) + O \left(n ^ {- 1}\right). \tag {C.11}
$$

Proof. To prove the lemma, write

$$
\begin{array}{l} n \left| S _ {n} (\omega , \omega) - f _ {x} (\omega) \right| = \left| \sum_ {| u | <   n} (n - | u |) \gamma (u) e ^ {- 2 \pi i \omega u} - n \sum_ {u = - \infty} ^ {\infty} \gamma (u) e ^ {- 2 \pi i \omega u} \right| \\ = \left| - n \sum_ {| u | \geq n} \gamma (u) e ^ {- 2 \pi i \omega u} - \sum_ {| u | <   n} | u | \gamma (u) e ^ {- 2 \pi i \omega u} \right| \\ \leq \sum_ {| u | \geq n} | u | | \gamma (u) | + \sum_ {| u | <   n} | u | | \gamma (u) | \\ = \theta , \\ \end{array}
$$

which establishes the lemma.

Lemma C.2 For $\omega _ { k } = k / n$ , $\omega _ { \ell } = \ell / n$ , $\omega _ { k } - \omega _ { \ell } \neq 0 , \pm 1 , \pm 2 , \pm 3 , . . .$ , and $\theta$ in (C.9), we have

$$
\left| S _ {n} \left(\omega_ {k}, \omega_ {\ell}\right) \right| \leq \frac {\theta}{n} = O \left(n ^ {- 1}\right), \tag {C.12}
$$

where

$$
S _ {n} \left(\omega_ {k}, \omega_ {\ell}\right) = E \left\{d \left(\omega_ {k}\right) \overline {{d \left(\omega_ {\ell}\right)}} \right\}. \tag {C.13}
$$

# Proof. Write

$$
\begin{array}{l} n \left| S _ {n} \left(\omega_ {k}, \omega_ {\ell}\right) \right| = \sum_ {u = - (n - 1)} ^ {- 1} \gamma (u) \sum_ {v = - (u - 1)} ^ {n} e ^ {- 2 \pi i \left(\omega_ {k} - \omega_ {\ell}\right) v} e ^ {- 2 \pi i \omega_ {k} u} \\ + \sum_ {u = 0} ^ {n - 1} \gamma (u) \sum_ {v = 1} ^ {n - u} e ^ {- 2 \pi i (\omega_ {k} - \omega_ {\ell}) v} e ^ {- 2 \pi i \omega_ {k} u}. \\ \end{array}
$$

Now, for the first term, with $u < 0$ ,

$$
\begin{array}{l} \sum_ {v = - (u - 1)} ^ {n} e ^ {- 2 \pi i (\omega_ {k} - \omega_ {\ell}) v} = \left(\sum_ {v = 1} ^ {n} - \sum_ {v = 1} ^ {- u}\right) e ^ {- 2 \pi i (\omega_ {k} - \omega_ {\ell}) v} \\ = 0 - \sum_ {v = 1} ^ {- u} e ^ {- 2 \pi i (\omega_ {k} - \omega_ {\ell}) v}. \\ \end{array}
$$

For the second term with $u \geq 0$ ,

$$
\begin{array}{l} \sum_ {v = 1} ^ {n - u} e ^ {- 2 \pi i (\omega_ {k} - \omega_ {\ell}) v} = \left(\sum_ {v = 1} ^ {n} - \sum_ {v = n - u + 1} ^ {n}\right) e ^ {- 2 \pi i (\omega_ {k} - \omega_ {\ell}) v} \\ = \quad 0 - \sum_ {v = n - u + 1} ^ {n} e ^ {- 2 \pi i (\omega_ {k} - \omega_ {\ell}) v}. \\ \end{array}
$$

Consequently,

$$
\begin{array}{l} n \left| S _ {n} \left(\omega_ {k}, \omega_ {\ell}\right) \right| = - \sum_ {u = - (n - 1)} ^ {- 1} \gamma (u) \sum_ {v = 1} ^ {- u} e ^ {- 2 \pi i \left(\omega_ {k} - \omega_ {\ell}\right) v} e ^ {- 2 \pi i \omega_ {k} u} \\ - \sum_ {u = 1} ^ {n - 1} \gamma (u) \sum_ {v = n - u + 1} ^ {n} e ^ {- 2 \pi i (\omega_ {k} - \omega_ {\ell}) v} e ^ {- 2 \pi i \omega_ {k} u} \\ \leq \sum_ {u = - (n - 1)} ^ {0} (- u) | \gamma (u) | + \sum_ {u = 1} ^ {n - 1} u | \gamma (u) | \\ = \sum_ {u = - (n - 1)} ^ {(n - 1)} | u | | \gamma (u) |. \\ \end{array}
$$

Hence, we have

$$
S _ {n} \left(\omega_ {k}, \omega_ {\ell}\right) \leq \frac {\theta}{n},
$$

and the asserted relations of the lemma follow.

Because the DFTs are approximately uncorrelated, say, of order $1 / n$ , when the frequencies are of the form $\omega _ { k } = k / n$ , we shall compute at those frequencies. The behavior of $f _ { ( \omega ) }$ at neighboring frequencies, however, will often be of interest and we shall use Lemma C.3 below to handle such cases

Lemma C.3 For $| \omega _ { \boldsymbol { k } } - \omega | \leq L / 2 n$ and $\theta$ in (C.9), we have

$$
\left| f \left(\omega_ {k}\right) - f (\omega) \right| \leq \frac {\pi \theta L}{n} \tag {C.14}
$$

or

$$
f \left(\omega_ {k}\right) - f (\omega) = O (L / n). \tag {C.15}
$$

Proof. To prove Lemma C.3, we write the difference

$$
\begin{array}{l} | f (\omega_ {k}) - f (\omega) | = \left| \sum_ {h = - \infty} ^ {\infty} \gamma (h) \left(e ^ {- 2 \pi i \omega_ {k} h} - e ^ {- 2 \pi i \omega h}\right) \right| \\ \leq \sum_ {h = - \infty} ^ {\infty} | \gamma (h) | | e ^ {- \pi i (\omega_ {k} - \omega) h} - e ^ {\pi i (\omega_ {k} - \omega) h} | \\ = 2 \sum_ {h = - \infty} ^ {\infty} | \gamma (h) | | \sin [ \pi (\omega_ {k} - \omega) h ] | \\ \leq 2 \pi | \omega_ {k} - \omega | \sum_ {h = - \infty} ^ {\infty} | h | | \gamma (h) | \\ \leq \frac {\pi \theta L}{n} \\ \end{array}
$$

because $| \sin x | \leq | x |$ .

The main use of the properties described by Lemmas C.1 and C.2 is in identifying the covariance structure of the DFT, say,

$$
\begin{array}{l} d (\omega_ {k}) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} e ^ {- 2 \pi i \omega_ {k} t} \\ = d _ {c} \left(\omega_ {k}\right) - i d _ {s} \left(\omega_ {k}\right), \\ \end{array}
$$

where

$$
d _ {c} (\omega_ {k}) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} \cos (2 \pi \omega_ {k} t)
$$

and

$$
d _ {s} (\omega_ {k}) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t} \sin (2 \pi \omega_ {k} t)
$$

are the cosine and sine transforms, respectively, of the observed series, defined previously in (4.24) and (4.25). For example, assuming zero means for convenience, we will have

$$
\begin{array}{l} E \left[ d _ {c} \left(\omega_ {k}\right) d _ {c} \left(\omega_ {\ell}\right) \right] = \frac {1}{4} n ^ {- 1} \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} \gamma (s - t) \left(e ^ {2 \pi i \omega_ {k} s} + e ^ {- 2 \pi i \omega_ {k} s}\right) \\ \times \left(e ^ {2 \pi i \omega_ {\ell} t} + e ^ {- 2 \pi i \omega_ {\ell} t}\right) \\ = \frac {1}{4} \left[ S _ {n} \left(- \omega_ {k}, \omega_ {l}\right) + S _ {n} \left(\omega_ {k}, \omega_ {\ell}\right) + S _ {n} \left(\omega_ {\ell}, \omega_ {k}\right) \right. \\ \left. + S _ {n} \left(\omega_ {k}, - \omega_ {\ell}\right) \right]. \\ \end{array}
$$

Lemmas C.1 and C.2 imply, for $k = \ell$ ,

$$
\begin{array}{l} E \left[ d _ {c} \left(\omega_ {k}\right) d _ {c} \left(\omega_ {\ell}\right) \right] = \frac {1}{4} \left[ O \left(n ^ {- 1}\right) + f \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right) \right. \\ \left. + f \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right) + O \left(n ^ {- 1}\right) \right] \\ = \frac {1}{2} f \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right). \tag {C.16} \\ \end{array}
$$

For $k \neq \ell$ , all terms are $O ( n ^ { - 1 } )$ . Hence, we have

$$
E \left[ d _ {c} \left(\omega_ {k}\right) d _ {c} \left(\omega_ {\ell}\right) \right] = \left\{ \begin{array}{l l} \frac {1}{2} f \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right), & k = \ell \\ \hat {O} \left(n ^ {- 1}\right), & k \neq \ell . \end{array} \right. \tag {C.17}
$$

A similar argument gives

$$
E \left[ d _ {s} \left(\omega_ {k}\right) d _ {s} \left(\omega_ {\ell}\right) \right] = \left\{ \begin{array}{l l} \frac {1}{2} f \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right), & k = \ell , \\ \hat {O} \left(n ^ {- 1}\right), & k \neq \ell \end{array} \right. \tag {C.18}
$$

and we also have $E [ d _ { s } ( \omega _ { k } ) d _ { c } ( \omega _ { \ell } ) ] = O ( n ^ { - 1 } )$ for all $k , \ell$ . We may summarize the results of Lemmas C.1–C.3 as follows.

Theorem C.4 For a stationary mean zero process with autocovariance function satisfying (C.9) and frequencies $\omega _ { k : n }$ , such that $| \omega _ { \boldsymbol { k } : n } - \omega | < 1 / n$ , are close to some target frequency $\omega$ , the cosine and sine transforms (4.24) and (4.25) are approximately uncorrelated with variances equal to $( 1 / 2 ) f ( \omega )$ , and the error in the approximation can be uniformly bounded by $\pi \theta L / n$ .

Now, consider estimating the spectrum in a neighborhood of some target frequency $\omega$ , using the periodogram estimator

$$
I (\omega_ {k: n}) = | d (\omega_ {k: n}) | ^ {2} = d _ {c} ^ {2} (\omega_ {k: n}) + d _ {s} ^ {2} (\omega_ {k: n}),
$$

where we take $| \omega _ { \boldsymbol { k } : n } - \omega | \leq n ^ { - 1 }$ for each $n$ . In case the series $x _ { t }$ is Gaussian with zero mean,

$$
\left( \begin{array}{c} d _ {c} (\omega_ {k: n}) \\ d _ {s} (\omega_ {k: n}) \end{array} \right) \xrightarrow {d} N \Bigg \{\left( \begin{array}{c} 0 \\ 0 \end{array} \right), \frac {1}{2} \left( \begin{array}{c c} f (\omega) & 0 \\ 0 & f (\omega) \end{array} \right) \Bigg \},
$$

and we have that

$$
\frac {2 I (\omega_ {k : n})}{f (\omega)} \xrightarrow {d} \chi_ {2} ^ {2},
$$

where $\chi _ { \nu } ^ { 2 }$ denotes a chi-squared random variable with $\nu$ degrees of freedom, as usual. Unfortunately, the distribution does not become more concentrated as $n \to \infty$ , because the variance of the periodogram estimator does not go to zero.

We develop a fix for the deficiencies mentioned above by considering the average of the periodogram over a set of frequencies in the neighborhood of $\omega$ . For example, we can always find a set of $L = 2 m + 1$ frequencies of the form $\{ \omega _ { j : n } + k / n$ ; $k = 0 , \pm 1 , \pm 2 , \ldots , m \}$ , for which

$$
f \left(\omega_ {j: n} + k / n\right) = f (\omega) + O \left(L n ^ {- 1}\right)
$$

by Lemma C.3. As $n$ increases, the values of the separate frequencies change.

Now, we can consider the smoothed periodogram estimator, $\widehat { f } ( \omega )$ , given in (4.56); this case includes the averaged periodogram, $f ( \omega )$ . First, we note that (C.9), $\begin{array} { r } { \theta = \sum _ { h = - \infty } ^ { \infty } | h | | \gamma ( h ) | < \infty } \end{array}$ , is a crucial condition in the estimation of spectra. In investigating local averages of the periodogram, we will require a condition on the rate of (C.9), namely

$$
\sum_ {h = - n} ^ {n} | h | | \gamma (h) | = O \left(n ^ {- 1 / 2}\right). \tag {C.19}
$$

One can show that a sufficient condition for (C.19) is that the time series is the linear process given by,

$$
x _ {t} = \sum_ {j = - \infty} ^ {\infty} \psi_ {j} w _ {t - j}, \quad \sum_ {j = 0} ^ {\infty} \sqrt {j} | \psi_ {j} | <   \infty \tag {C.20}
$$

where $w _ { t } \sim \mathrm { i i d } ( 0 , \sigma _ { w } ^ { 2 } )$ and $w _ { t }$ has finite fourth moment,

$$
E (w _ {t} ^ {4}) = \eta \sigma_ {w} ^ {4} <   \infty .
$$

We leave it to the reader (Problem 4.40) to show (C.20) implies (C.19) under the condition that $w _ { t } \sim w n ( 0 , \sigma _ { w } ^ { 2 } )$ .

We now state the following lemma.

Lemma C.4 Suppose $x _ { t }$ is the linear process given by (C.20), and let $I ( \omega _ { j } )$ be the periodogram of the data $\{ x _ { 1 } , \ldots , x _ { n } \}$ . Then

$$
\operatorname {c o v} \left(I (\omega_ {j}), I (\omega_ {k})\right) = \left\{ \begin{array}{l l} 2 f ^ {2} (\omega_ {j}) + o (1) & \omega_ {j} = \omega_ {k} = 0,   1 / 2 \\ f ^ {2} (\omega_ {j}) + o (1) & \omega_ {j} = \omega_ {k} \neq 0,   1 / 2 \\ O (n ^ {- 1}) & \omega_ {j} \neq \omega_ {k}. \end{array} \right.
$$

The proof of Lemma C.4 is straight forward but tedious, and details may be found in Fuller (1976, Theorem 7.2.1) or in Brockwell and Davis (1991, Theorem 10.3.2). For demonstration purposes, we present the proof of the lemma for the pure white noise case; i.e., $x _ { t } = w _ { t }$ , in which case $f ( \omega ) \equiv \sigma _ { w } ^ { 2 }$ . By definition, the periodogram in this case is

$$
I (\omega_ {j}) = n ^ {- 1} \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} w _ {s} w _ {t} e ^ {2 \pi i \omega_ {j} (t - s)},
$$

where $\omega _ { j } = j / n$ , and hence

$$
E \{I (\omega_ {j}) I (\omega_ {k}) \} = n ^ {- 2} \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} \sum_ {u = 1} ^ {n} \sum_ {v = 1} ^ {n} w _ {s} w _ {t} w _ {u} w _ {v} e ^ {2 \pi i \omega_ {j} (t - s)} e ^ {2 \pi i \omega_ {k} (u - v)}.
$$

Now when all the subscripts match, $E ( w _ { s } w _ { t } w _ { u } w _ { v } ) = \eta \sigma _ { w } ^ { 4 }$ , when two of the subscripts match, $E ( w _ { s } w _ { t } w _ { u } w _ { v } ) = \sigma _ { w } ^ { 4 }$ , otherwise, $E ( w _ { s } w _ { t } w _ { u } w _ { v } ) = 0$ . Thus,

$$
E \left\{I (\omega_ {j}) I (\omega_ {k}) \right\} = n ^ {- 1} (\eta - 3) \sigma_ {w} ^ {4} + \sigma_ {w} ^ {4} \left(1 + n ^ {- 2} \left[ A (\omega_ {j} + \omega_ {k}) + A (\omega_ {k} - \omega_ {j}) \right]\right),
$$

where

$$
A (u) = \left| \sum_ {t = 1} ^ {n} e ^ {2 \pi i u t} \right| ^ {2}.
$$

Noting that $\begin{array} { r } { E I ( \omega _ { j } ) = n ^ { - 1 } \sum _ { t = 1 } ^ { n } E ( w _ { t } ^ { : 2 } ) = \sigma _ { w } ^ { 2 } } \end{array}$ , we have

$$
\begin{array}{l} \operatorname {c o v} \left\{I \left(\omega_ {j}\right), I \left(\omega_ {k}\right) \right\} = E \left\{I \left(\omega_ {j}\right) I \left(\omega_ {k}\right) \right\} - \sigma_ {w} ^ {4}. \\ = n ^ {- 1} (\eta - 3) \sigma_ {w} ^ {4} + n ^ {- 2} \sigma_ {w} ^ {4} [ A (\omega_ {j} + \omega_ {k}) + A (\omega_ {k} - \omega_ {j}) ]. \\ \end{array}
$$

Thus we conclude that

$$
\operatorname {v a r} \left\{I \left(\omega_ {j}\right) \right\} = n ^ {- 1} (\eta - 3) \sigma_ {w} ^ {4} + \sigma_ {w} ^ {4} \quad \text {f o r} \quad \omega_ {j} \neq 0, 1 / 2
$$

$$
\operatorname {v a r} \left\{I \left(\omega_ {j}\right) \right\} = n ^ {- 1} (\eta - 3) \sigma_ {w} ^ {4} + 2 \sigma_ {w} ^ {4} \quad \text {f o r} \quad \omega_ {j} = 0, 1 / 2
$$

$$
\operatorname {c o v} \left\{I \left(\omega_ {j}\right), I \left(\omega_ {k}\right) \right\} = n ^ {- 1} (\eta - 3) \sigma_ {w} ^ {4} \quad \text {f o r} \quad \omega_ {j} \neq \omega_ {k},
$$

which establishes the result in this case. We also note that if $w _ { t }$ is Gaussian, then $\eta = 3$ and the periodogram ordinates are independent. Using Lemma C.4, we may establish the following fundamental result.

Theorem C.5 Suppose $x _ { t }$ is the linear process given by (C.20). Then, with $\widehat { f } ( \omega )$ defined in (4.56) and corresponding conditions on the weights $h _ { k }$ , we have, as $n \to \infty$ ,

In (ii), replace $f ^ { 2 } ( \omega )$ by 0 if $\omega \neq \lambda$ and by $2 f ^ { 2 } ( \omega )$ if $\omega = \lambda = 0$ or $1 / 2$ .

Proof. (i): First, recall (4.29)

$$
E \left[ I \left(\omega_ {j: n}\right) \right] = \sum_ {h = - (n - 1)} ^ {n - 1} \left(\frac {n - | h |}{n}\right) \gamma (h) e ^ {- 2 \pi i \omega_ {j: n} h} \stackrel {\mathrm {d e f}} {=} f _ {n} \left(\omega_ {j: n}\right).
$$

But since $f _ { n } ( \omega _ { j : n } )  f ( \omega )$ uniformly, and $| f ( \omega _ { j : n } ) - f ( \omega _ { j : n } + k / n ) | \to 0$ by the continuity of $f$ , we have

$$
\begin{array}{l} E \widehat {f} (\omega) = \sum_ {k = - m} ^ {m} h _ {k} E I \left(\omega_ {j: n} + k / n\right) = \sum_ {k = - m} ^ {m} h _ {k} f _ {n} \left(\omega_ {j: n} + k / n\right) \\ = \sum_ {k = - m} ^ {m} h _ {k} [ f (\omega) + o (1) ] \rightarrow f (\omega_ {j}), \\ \end{array}
$$

because $\scriptstyle \sum _ { k = - m } ^ { m } h _ { k } = 1$

(ii): First, suppose we have $\omega _ { j : n }  \omega _ { 1 }$ and $\omega _ { \ell : n } \to \omega _ { 2 }$ , and $\omega _ { 1 } \neq \omega _ { 2 }$ . Then, for $n$ large enough to separate the bands, using Lemma C.4, we have

$$
\begin{array}{l} \left| \operatorname {cov} \left(\widehat {f} \left(\omega_ {1}\right), \widehat {f} \left(\omega_ {2}\right)\right) \right| = \left| \sum_ {| k | \leq m} \sum_ {| r | \leq m} h _ {k} h _ {r} \operatorname {cov} \left[ I \left(\omega_ {j: n} + k / n\right), I \left(\omega_ {\ell : n} + r / n\right) \right] \right| \\ = \left| \sum_ {| k | \leq m} \sum_ {| r | \leq m} h _ {k} h _ {r} O (n ^ {- 1}) \right| \\ \leq \frac {c}{n} \left(\sum_ {| k | \leq m} h _ {k}\right) ^ {2} \quad (\text {w h e r e c i s a c o n s t a n t}) \\ \leq \frac {c L}{n} \left(\sum_ {| k | \leq m} h _ {k} ^ {2}\right), \\ \end{array}
$$

which establishes (ii) for the case of different frequencies. The case of the same frequencies, i.e., $\operatorname { v a r } [ \widehat { f } ( \omega _ { 1 } ) ]$ is established in a similar manner to the above arguments. ■

Theorem C.5 justifies the distributional properties used throughout §4.5 and in Chapter 7. We may extend the results of this section to vector series of the form $\pmb { x } _ { t } = ( x _ { t 1 } , \ldots , x _ { t p } ) ^ { \prime }$ , when the cross-spectrum is given by

$$
\begin{array}{l} f _ {i j} (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma_ {i j} (h) e ^ {- 2 \pi i \omega h} \\ = c _ {i j} (\omega) - i q _ {i j} (\omega), \tag {C.21} \\ \end{array}
$$

where

$$
c _ {i j} (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma_ {i j} (h) \cos (2 \pi \omega h) \tag {C.22}
$$

and

$$
q _ {i j} (\omega) = \sum_ {h = - \infty} ^ {\infty} \gamma_ {i j} (h) \sin (2 \pi \omega h) \tag {C.23}
$$

denote the cospectrum and quadspectrum, respectively. We denote the DFT of the series $x _ { t j }$ by

$$
\begin{array}{l} d _ {j} \left(\omega_ {k}\right) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} x _ {t j} e ^ {- 2 \pi i \omega_ {k} t} \\ = d _ {c j} \left(\omega_ {k}\right) - i d _ {s j} \left(\omega_ {k}\right), \\ \end{array}
$$

where $d _ { c j }$ and $d _ { s j }$ are the cosine and sine transforms of $x _ { t j }$ , for $j = 1 , 2 , \dotsc , p$ . We bound the covariance structure as before and summarize the results as follows.

Theorem C.6 The covariance structure of the multivariate cosine and sine transforms, subject to

$$
\theta_ {i j} = \sum_ {h = - \infty} ^ {\infty} | h | | \gamma_ {i j} (h) | <   \infty , \tag {C.24}
$$

is given by

$$
E \left[ d _ {c i} \left(\omega_ {k}\right) d _ {c j} \left(\omega_ {\ell}\right) \right] = \left\{ \begin{array}{l l} \frac {1}{2} c _ {i j} \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right), & k = \ell \\ O \left(n ^ {- 1}\right), & k \neq \ell . \end{array} \right. \tag {C.25}
$$

$$
E \left[ d _ {c i} \left(\omega_ {k}\right) d _ {s j} \left(\omega_ {\ell}\right) \right] = \left\{ \begin{array}{l l} - \frac {1}{2} q _ {i j} \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right), & k = \ell \\ O \left(n ^ {- 1}\right), & k \neq \ell \end{array} \right. \tag {C.26}
$$

$$
E \left[ d _ {s i} \left(\omega_ {k}\right) d _ {c j} \left(\omega_ {\ell}\right) \right] = \left\{ \begin{array}{l l} \frac {1}{2} q _ {i j} \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right), & k = \ell \\ O \left(n ^ {- 1}\right), & k \neq \ell \end{array} \right. \tag {C.27}
$$

$$
E \left[ d _ {s i} \left(\omega_ {k}\right) d _ {s j} \left(\omega_ {\ell}\right) \right] = \left\{ \begin{array}{l l} \frac {1}{2} c _ {i j} \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right), & k = \ell \\ O \left(n ^ {- 1}\right), & k \neq \ell . \end{array} \right. \tag {C.28}
$$

Proof. We define

$$
S _ {n} ^ {i j} \left(\omega_ {k}, \omega_ {\ell}\right) = \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} \gamma_ {i j} (s - t) e ^ {- 2 \pi i \omega_ {k} s} e ^ {2 \pi i \omega_ {\ell} t}. \tag {C.29}
$$

Then, we may verify the theorem with manipulations like

$$
\begin{array}{l} E \left[ d _ {c i} \left(\omega_ {k}\right) d _ {s j} \left(\omega_ {k}\right) \right] = \frac {1}{4 i} \sum_ {s = 1} ^ {n} \sum_ {t = 1} ^ {n} \gamma_ {i j} (s - t) \left(e ^ {2 \pi i \omega_ {k} s} + e ^ {- 2 \pi i \omega_ {k} s}\right) \\ \times \left(e ^ {2 \pi i \omega_ {k} t} - e ^ {- 2 \pi i \omega_ {k} t}\right) \\ = \frac {1}{4 i} \left[ S _ {n} ^ {i j} \left(- \omega_ {k}, \omega_ {k}\right) + S _ {n} ^ {i j} \left(\omega_ {k}, \omega_ {k}\right) \right. \\ \left. - S _ {n} ^ {i j} \left(\omega_ {k}, \omega_ {k}\right) - S _ {n} ^ {i j} \left(\omega_ {k}, - \omega_ {k}\right) \right] \\ = \frac {1}{4 i} \left[ c _ {i j} (\omega_ {k}) - i q _ {i j} (\omega_ {k}) \right. \\ \left. - \left(c _ {i j} \left(\omega_ {k}\right) + i q _ {i j} \left(\omega_ {k}\right)\right) + O \left(n ^ {- 1}\right) \right] \\ = - \frac {1}{2} q _ {i j} \left(\omega_ {k}\right) + O \left(n ^ {- 1}\right), \\ \end{array}
$$

where we have used the fact that the properties given in Lemmas C.1–C.3 can be verified for the cross-spectral density functions $f _ { i j } ( \omega ) , ~ i , j = 1 , . ~ . . , p$ $f _ { i j } ( \omega )$ . ■

Now, if the underlying multivariate time series $\pmb { x } _ { t }$ is a normal process, it is clear that the DFTs will be jointly normal and we may define the vector DFT, $\pmb { d } ( \omega _ { k } ) = ( d _ { 1 } ( \omega _ { k } ) , \dots , d _ { p } ( \omega _ { k } ) ) ^ { \prime }$ as

$$
\begin{array}{l} \boldsymbol {d} (\omega_ {k}) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t} e ^ {- 2 \pi \omega_ {k} t} \\ = \boldsymbol {d} _ {c} \left(\omega_ {k}\right) - i \boldsymbol {d} _ {s} \left(\omega_ {k}\right), \tag {C.30} \\ \end{array}
$$

where

$$
\boldsymbol {d} _ {c} \left(\omega_ {k}\right) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t} \cos \left(2 \pi \omega_ {k} t\right) \tag {C.31}
$$

and

$$
\boldsymbol {d} _ {s} \left(\omega_ {k}\right) = n ^ {- 1 / 2} \sum_ {t = 1} ^ {n} \boldsymbol {x} _ {t} \sin \left(2 \pi \omega_ {k} t\right) \tag {C.32}
$$

are the cosine and sine transforms, respectively, of the observed vector series ${ \pmb x } _ { t }$ . Then, constructing the vector of real and imaginary parts $( { \pmb d } _ { c } ^ { \prime } ( \omega _ { k } ) , { \pmb d } _ { s } ^ { \prime } ( \omega _ { k } ) ) ^ { \prime }$ , we may note it has mean zero and $2 p \times 2 p$ covariance matrix

$$
\Sigma \left(\omega_ {k}\right) = \frac {1}{2} \left( \begin{array}{c c} C \left(\omega_ {k}\right) & - Q \left(\omega_ {k}\right) \\ Q \left(\omega_ {k}\right) & C \left(\omega_ {k}\right) \end{array} \right) \tag {C.33}
$$

to order $n ^ { - 1 }$ as long as $\omega _ { k } - \omega = O ( n ^ { - 1 } )$ . We have introduced the $p \times p$ matrices $C ( \omega _ { k } ) = \{ c _ { i j } ( \omega _ { k } ) \}$ and $Q = \{ q _ { i j } ( \omega _ { k } ) \}$ . The complex random variable

$\pmb { d } ( \omega _ { k } )$ has covariance

$$
\begin{array}{l} S (\omega_ {k}) = E [ \boldsymbol {d} (\omega_ {k}) \boldsymbol {d} ^ {*} (\omega_ {k}) ] \\ = E \left[ \left(\boldsymbol {d} _ {c} (\omega_ {k}) - i \boldsymbol {d} _ {s} (\omega_ {k})\right) \left(\boldsymbol {d} _ {c} (\omega_ {k}) - i \boldsymbol {d} _ {s} (\omega_ {k})\right) ^ {*} \right] \\ = E \left[ \boldsymbol {d} _ {c} \left(\omega_ {k}\right) \boldsymbol {d} _ {c} \left(\omega_ {k}\right) ^ {\prime} \right] + E \left[ \boldsymbol {d} _ {s} \left(\omega_ {k}\right) \boldsymbol {d} _ {s} \left(\omega_ {k}\right) ^ {\prime} \right] \\ - i \left(E \left[ \boldsymbol {d} _ {s} (\omega_ {k}) \boldsymbol {d} _ {c} (\omega_ {k}) ^ {\prime} \right] - E \left[ \boldsymbol {d} _ {c} (\omega_ {k}) \boldsymbol {d} _ {s} (\omega_ {k}) ^ {\prime} \right]\right) \\ = C \left(\omega_ {k}\right) - i Q \left(\omega_ {k}\right). \tag {C.34} \\ \end{array}
$$

If the process ${ \pmb x } _ { t }$ has a multivariate normal distribution, the complex vector $\pmb { d } ( \omega _ { k } )$ has approximately the complex multivariate normal distribution with mean zero and covariance matrix $S ( \omega _ { k } ) = C ( \omega _ { k } ) - i Q ( \omega _ { k } )$ if the real and imaginary parts have the covariance structure as specified above. In the next section, we work further with this distribution and show how it adapts to the real case. If we wish to estimate the spectral matrix $S ( \omega )$ , it is natural to take a band of frequencies of the form $\omega _ { k : n } + \ell / n$ , for $\ell = - m , \ldots , m$ as before, so that the estimator becomes (4.87) of §4.6. A discussion of further properties of the multivariate complex normal distribution is deferred.

It is also of interest to develop a large sample theory for cases in which the underlying distribution is not necessarily normal. If $x _ { t }$ is not necessarily a normal process, some additional conditions are needed to get asymptotic normality. In particular, introduce the notion of a generalized linear process

$$
\boldsymbol {y} _ {t} = \sum_ {r = - \infty} ^ {\infty} A _ {r} \boldsymbol {w} _ {t - r}, \tag {C.35}
$$

where ${ \pmb w } _ { t }$ is a $p \times 1$ vector white noise process with $p \times p$ covariance $E [ { \pmb w } _ { t } { \pmb w } _ { t } ^ { \prime } ] = G$ and the $p \times p$ matrices of filter coefficients $A _ { t }$ satisfy

$$
\sum_ {t = - \infty} ^ {\infty} \operatorname {t r} \left\{A _ {t} A _ {t} ^ {\prime} \right\} = \sum_ {t = - \infty} ^ {\infty} \| A _ {t} \| ^ {2} <   \infty . \tag {C.36}
$$

In particular, stable vector ARMA processes satisfy these conditions. For generalized linear processes, we state the following general result from Hannan (1970, p.224).

Theorem C.7 If ${ \pmb x } _ { t }$ is generated by a generalized linear process with a continuous spectrum that is not zero at $\omega$ and $\omega _ { k : n } + \ell / n$ are a set of frequencies within $L / n$ of $\omega$ , the joint density of the cosine and sine transforms (C.31) and (C.32) converges to that of $L$ independent $2 p \times 1$ normal vectors with covariance matrix $\Sigma ( \omega )$ with structure given by (C.33). At $\omega = 0$ or $\omega = 1 / 2$ , the distribution is real with covariance matrix $2 \Sigma ( \omega )$ .

The above result provides the basis for inference involving the Fourier transforms of stationary series because it justifies approximations to the likelihood

function based on multivariate normal theory. We make extensive use of this result in Chapter 7, but will still need a simple form to justify the distributional result for the sample coherence given in (4.93). The next section gives an elementary introduction to the complex normal distribution.

# C.3 The Complex Multivariate Normal Distribution

The multivariate normal distribution will be the fundamental tool for expressing the likelihood function and determining approximate maximum likelihood estimators and their large sample probability distributions. A detailed treatment of the multivariate normal distribution can be found in standard texts such as Anderson (1984). We will use the multivariate normal distribution of the $p \times 1$ vector $x = ( x _ { 1 } , x _ { 2 } , \ldots , x _ { p } ) ^ { \prime }$ , as defined by its density function

$$
p (\boldsymbol {x}) = (2 \pi) ^ {- p / 2} | \Sigma | ^ {- 1 / 2} \exp \left\{- \frac {1}{2} \left(\boldsymbol {x} - \boldsymbol {\mu}\right) ^ {\prime} \Sigma^ {- 1} (\boldsymbol {x} - \boldsymbol {\mu}) \right\}, \tag {C.37}
$$

which can be shown to have mean vector $E [ { \pmb x } ] = { \pmb \mu } = ( \mu _ { 1 } , \dots , \mu _ { p } ) ^ { \prime }$ and covariance matrix

$$
\Sigma = E \left[ (\boldsymbol {x} - \boldsymbol {\mu}) \left(\boldsymbol {x} - \boldsymbol {\mu}\right) ^ {\prime} \right]. \tag {C.38}
$$

We use the notation $\pmb { x } \sim N _ { p } \{ \pmb { \mu } , \Sigma \}$ for densities of the form (C.37) and note that linearly transformed multivariate normal variables of the form ${ \pmb y } = A { \pmb x }$ , with $A$ a $q \times p$ matrix $q \leq p$ , will also be multivariate normal with distribution

$$
\boldsymbol {y} \sim N _ {q} \left\{A \boldsymbol {\mu}, A \Sigma A ^ {\prime} \right\}. \tag {C.39}
$$

Often, the partitioned multivariate normal , based on the vector ${ \pmb x } =$ $( \pmb { x } _ { 1 } ^ { \prime } , \pmb { x } _ { 2 } ^ { \prime } ) ^ { \prime }$ , split into to $p _ { 1 } \times 1$ and $p _ { 2 } \times 1$ components $\pmb { x } _ { 1 }$ and $\pmb { x } _ { 2 }$ , respectively, will be used where $p = p _ { 1 } + p _ { 2 }$ . If the mean vector $\pmb { \mu } = ( \pmb { \mu } _ { 1 } ^ { \prime } , \pmb { \mu } _ { 2 } ^ { \prime } ) ^ { \prime }$ and covariance matrices

$$
\Sigma = \left( \begin{array}{c c} \Sigma_ {1 1} & \Sigma_ {1 2} \\ \Sigma_ {2 1} & \Sigma_ {2 2} \end{array} \right) \tag {C.40}
$$

are also compatibly partitioned, the marginal distribution of any subset of components is multivariate normal, say,

$$
\boldsymbol {x} _ {1} \sim N _ {p _ {1}} \left\{\boldsymbol {\mu} _ {1}, \Sigma_ {1 1} \right\},
$$

and that the conditional distribution $\pmb { x } _ { 2 }$ given $\pmb { x } _ { 1 }$ is normal with mean

$$
E \left[ \boldsymbol {x} _ {2} \mid \boldsymbol {x} _ {1} \right] = \boldsymbol {\mu} _ {2} + \Sigma_ {2 1} \Sigma_ {1 1} ^ {- 1} \left(\boldsymbol {x} _ {1} - \boldsymbol {\mu} _ {1}\right) \tag {C.41}
$$

and conditional covariance

$$
\operatorname {c o v} \left[ \boldsymbol {x} _ {2} \mid \boldsymbol {x} _ {1} \right] = \Sigma_ {2 2} - \Sigma_ {2 1} \Sigma_ {1 1} ^ {- 1} \Sigma_ {1 2}. \tag {C.42}
$$

In the previous section, the real and imaginary parts of the DFT had a partitioned covariance matrix as given in (C.33), and we use this result to say the complex $p \times 1$ vector

$$
\boldsymbol {z} = \boldsymbol {x} _ {1} - i \boldsymbol {x} _ {2} \tag {C.43}
$$

has a complex multivariate normal distribution, with mean vector $\pmb { \mu } _ { z } = \pmb { \mu } _ { 1 } - i \pmb { \mu } _ { 2 }$ and $p \times p$ covariance matrix

$$
\Sigma_ {z} = C - i Q \tag {C.44}
$$

if the real multivariate $2 p \times 1$ normal vector $\pmb { x } = ( \pmb { x } _ { 1 } ^ { \prime } , \pmb { x } _ { 2 } ^ { \prime } ) ^ { \prime }$ has a real multivariate normal distribution with mean vector $\pmb { \mu } = ( \pmb { \mu } _ { 1 } ^ { \prime } , \pmb { \mu } _ { 2 } ^ { \prime } ) ^ { \prime }$ and covariance matrix

$$
\Sigma = \frac {1}{2} \left( \begin{array}{c c} C & - Q \\ Q & C \end{array} \right). \tag {C.45}
$$

The restrictions $C ^ { \prime } = C$ and $Q ^ { \prime } = - Q$ are necessary for the matrix $\Sigma$ to be a covariance matrix, and these conditions then imply $\Sigma _ { z } = \Sigma _ { z } ^ { * }$ is Hermitian. The probability density function of the complex multivariate normal vector $z$ can be expressed in the concise form

$$
p _ {\boldsymbol {z}} (\boldsymbol {z}) = \pi^ {- p} \left| \Sigma_ {z} \right| ^ {- 1} \exp \left\{- \left(\boldsymbol {z} - \boldsymbol {\mu} _ {z}\right) ^ {*} \Sigma_ {z} ^ {- 1} \left(\boldsymbol {z} - \boldsymbol {\mu} _ {z}\right) \right\}, \tag {C.46}
$$

and this is the form that we will often use in the likelihood. The result follows from showing that $p \pmb { x } ( \pmb { x } _ { 1 } , \pmb { x } _ { 2 } ) = p \pmb { z } ( \pmb { z } )$ exactly, using the fact that the quadratic and Hermitian forms in the exponent are equal and that $| \Sigma _ { x } | = | \Sigma _ { z } | ^ { 2 }$ . The second assertion follows directly from the fact that the matrix $\Sigma _ { x }$ has repeated eigenvalues, $\lambda _ { 1 } , \lambda _ { 2 } , \ldots , \lambda _ { p }$ corresponding to eigenvectors $( \alpha _ { 1 } ^ { \prime } , \alpha _ { 2 } ^ { \prime } ) ^ { \prime }$ and the same set, $\lambda _ { 1 } , \lambda _ { 2 } , \ldots , \lambda _ { p }$ corresponding to $( \alpha _ { 2 } ^ { \prime } , - \alpha _ { 1 } ^ { \prime } ) ^ { \prime }$ . Hence

$$
| \Sigma_ {x} | = \prod_ {i = 1} ^ {p} \lambda_ {i} ^ {2} = | \Sigma_ {z} | ^ {2}.
$$

For further material relating to the complex multivariate normal distribution, see Goodman (1963), Giri (1965), or Khatri (1965).

# Example C.2 A Bivariate Complex Normal Distribution

Consider the joint distribution of the complex random variables $u _ { 1 } =$ $x _ { 1 } - i x _ { 2 }$ and $u _ { 2 } = y _ { 1 } - i y _ { 2 }$ , where the partitioned vector $( x _ { 1 } , x _ { 2 } , y _ { 1 } , y _ { 2 } ) ^ { \prime }$ has a real multivariate normal distribution with mean $( 0 , 0 , 0 , 0 ) ^ { \prime }$ and covariance matrix

$$
\Sigma = \frac {1}{2} \left( \begin{array}{c c c c} c _ {x x} & 0 & c _ {x y} & - q _ {x y} \\ 0 & c _ {x x} & q _ {x y} & c _ {x y} \\ c _ {x y} & q _ {x y} & c _ {y y} & 0 \\ - q _ {x y} & c _ {y x} & 0 & c _ {y y} \end{array} \right). \tag {C.47}
$$

Now, consider the conditional distribution of $\pmb { y } = ( y _ { 1 } , y _ { 2 } ) ^ { \prime }$ , given ${ \pmb x } =$ $( x _ { 1 } , x _ { 2 } ) ^ { \prime }$ . Using (C.41), we obtain

$$
E (\boldsymbol {y} \mid \boldsymbol {x}) = \left( \begin{array}{c c} x _ {1} & - x _ {2} \\ x _ {2} & x _ {1} \end{array} \right) \left( \begin{array}{l} b _ {1} \\ b _ {2} \end{array} \right), \tag {C.48}
$$

where

$$
\left(b _ {1}, b _ {2}\right) = \left(\frac {c _ {y x}}{c _ {x x}}, \frac {q _ {y x}}{c _ {x x}}\right). \tag {C.49}
$$

It is natural to identify the cross-spectrum

$$
f _ {x y} = c _ {x y} - i q _ {x y}, \tag {C.50}
$$

so that the complex variable identified with the pair is just

$$
\begin{array}{l} b = b _ {1} - i b _ {2} \\ = \frac {c _ {y x} - i q _ {y x}}{c _ {x x}} \\ = \begin{array}{c} f _ {y x} \\ f _ {x x} \end{array} , \\ \end{array}
$$

and we identify it as the complex regression coefficient. The conditional covariance follows from (C.42) and simplifies to

$$
\operatorname {c o v} (\boldsymbol {y} \mid \boldsymbol {x}) = \frac {1}{2} f _ {y \cdot x} I _ {2}, \tag {C.51}
$$

where $I _ { 2 }$ denotes the $2 \times 2$ identity matrix and

$$
\begin{array}{l} {f _ {y \cdot x}} = {c _ {y y} - \frac {c _ {x y} ^ {2} + q _ {x y} ^ {2}}{c _ {x x}}} \\ = f _ {y y} - \frac {\left| f _ {x y} \right| ^ {2}}{f _ {x x}} \tag {C.52} \\ \end{array}
$$

Example C.2 leads to an approach for justifying the distributional results for the function coherence given in (4.93). That equation suggests that the result can be derived using the regression results that lead to the F-statistics in 2.2. Suppose that we consider $L$ values of the sine and cosine transforms of the input $x _ { t }$ and output $y _ { t }$ , which we will denote by $d _ { x , c } ( \omega _ { k } \cdot +$ $\ell / n ) , d _ { x , s } ( \omega _ { k } + \ell / n ) , d _ { y , c } ( \omega _ { k } + \ell / n ) , d _ { y , s } ( \omega _ { k } + \ell / n ) |$ , sampled at $L = 2 m + 1$ frequencies, $\ell = - m , \ldots , m$ , in the neighborhood of some target frequency $\omega$ . Suppose these cosine and sine transforms are re-indexed and denoted by $d _ { x , c j } , d _ { x , s j } , d _ { y , c j } , d _ { y , s j }$ , for $j = 1 , 2 , \dots , L$ , producing $2 L$ real random variables with a large sample normal distribution that have limiting covariance matrices of the form (C.47) for each $j$ . Then, the conditional normal distribution of the $2 \times 1$ vector $d _ { y , c j } , d _ { y , s j }$ given $d _ { x , c j } , d _ { x , s j }$ , given in Example C.2, shows that we may write, approximately, the regression model

$$
\left( \begin{array}{c} d _ {y, c j} \\ d _ {y, s j} \end{array} \right) = \left( \begin{array}{c c} d _ {x, c j} & - d _ {x, s j} \\ d _ {x, s j} & d _ {x, c j} \end{array} \right) \left( \begin{array}{c} b _ {1} \\ b _ {2} \end{array} \right) + \left( \begin{array}{c} V _ {c j} \\ V _ {s j} \end{array} \right),
$$

where $V _ { c j } , V _ { s j }$ are approximately uncorrelated with approximate variances

$$
E \left[ V _ {c j} ^ {2} \right] = E \left[ V _ {s j} ^ {2} \right] = (1 / 2) f _ {y \cdot x}.
$$

Now, construct, by stacking, the $2 L \times 1$ vectors $\pmb { y } _ { c } = ( d _ { y , c 1 } , \dots , d _ { y , c L } ) ^ { \prime }$ , ${ \bf { \mathscr { y } } } _ { s } =$ $( d _ { y , s 1 } , \ldots , d _ { y , s L } ) ^ { \prime }$ , $\pmb { x } _ { c } = ( d _ { x , c 1 } , \ldots , d _ { x , c L } ) ^ { \prime }$ and $\pmb { x _ { s } } = ( d _ { x , s 1 } , \ldots , d _ { x , s L } ) ^ { \prime }$ , and rewrite the regression model as

$$
\left( \begin{array}{c} \boldsymbol {y} _ {c} \\ \boldsymbol {y} _ {s} \end{array} \right) = \left( \begin{array}{c c} \boldsymbol {x} _ {c c} & - \boldsymbol {x} _ {s} \\ \boldsymbol {x} _ {s} & \boldsymbol {x} _ {c c} \end{array} \right) \left( \begin{array}{c} b _ {1} \\ b _ {2} \end{array} \right) + \left( \begin{array}{c} \boldsymbol {v} _ {c c} \\ \boldsymbol {v} _ {s} \end{array} \right)
$$

where ${ \pmb v } _ { c }$ and ${ \pmb v } _ { s }$ are the error stacks. Finally, write the overall model as the regression model in Chapter 2, namely,

$$
\boldsymbol {y} = Z \boldsymbol {b} + \boldsymbol {v},
$$

making the obvious identifications in the previous equation. Conditional on $Z$ , the model becomes exactly the regression model considered in Chapter 2 where there are $q = 2$ regression coefficients and $2 L$ observations in the observation vector $\pmb { y }$ . To test the hypothesis of no regression for that model, we use an F-Statistic that depends on the difference between the residual sum of squares for the full model, say,

$$
R S S = \boldsymbol {y} ^ {\prime} \boldsymbol {y} - \boldsymbol {y} ^ {\prime} Z \left(Z ^ {\prime} Z\right) ^ {- 1} Z ^ {\prime} \boldsymbol {y} \tag {C.53}
$$

and the residual sum of squares for the reduced model, $R S S _ { 0 } = { \bf { y } } ^ { \prime } { \bf { y } }$ . Then,

$$
F _ {2, 2 L - 2} = (L - 1) \frac {R S S _ {0} - R S S}{R S S} \tag {C.54}
$$

has the F-distribution with 2 and $2 L - 2$ degrees of freedom. Also, it follows by substitution for $\pmb { y }$ that

$$
\begin{array}{l} R S S _ {0} = \boldsymbol {y} ^ {\prime} \boldsymbol {y} \\ = \quad \boldsymbol {y} _ {c} ^ {\prime} \boldsymbol {y} _ {c} + \boldsymbol {y} _ {s} ^ {\prime} \boldsymbol {y} _ {s} \\ = \sum_ {j = 1} ^ {L} \left(d _ {y, c j} ^ {2} + d _ {y, s j} ^ {2}\right) \\ = L \widehat {f} _ {y} (\omega), \\ \end{array}
$$

which is just the sample spectrum of the output series. Similarly,

$$
Z ^ {\prime} Z = \left( \begin{array}{c c} L \widehat {f} _ {x} & 0 \\ 0 & L \widehat {f} _ {x} \end{array} \right)
$$

and

$$
\begin{array}{l} Z ^ {\prime} \boldsymbol {y} = \left( \begin{array}{c} (\boldsymbol {x} _ {c} ^ {\prime} \boldsymbol {y} _ {c} + \boldsymbol {x} _ {s} ^ {\prime} \boldsymbol {y} _ {s}) \\ (\boldsymbol {x} _ {c} ^ {\prime} \boldsymbol {y} _ {s} - \boldsymbol {x} _ {s} ^ {\prime} \boldsymbol {y} _ {c}) \end{array} \right) \\ = \left( \begin{array}{c} \sum_ {j = 1} ^ {L} (d _ {x, c j} d _ {y, c j} + d _ {x, s j} d _ {y, s j}) \\ \sum_ {j = 1} ^ {L} (d _ {x, c j} d _ {y, s j} - d _ {x, s j} d _ {y, c j}) \end{array} \right) \\ = \left( \begin{array}{c} L \widehat {c c} _ {y x} \\ L \widehat {q} _ {y x} \end{array} \right). \\ \end{array}
$$

together imply that

$$
\mathbf {y} ^ {\prime} Z \left(Z ^ {\prime} Z\right) ^ {- 1} Z ^ {\prime} \mathbf {y} = L \left| \widehat {f} _ {x y} \right| ^ {2} / \widehat {f} _ {x}.
$$

Substituting into (C.54) gives

$$
F _ {2, 2 L - 2} = (L - 1) \frac {\left| \widehat {f} _ {x y} \right| ^ {2} / \widehat {f} _ {x}}{\left(\widehat {f} _ {y} - \left| \widehat {f} _ {x y} \right| ^ {2} / \widehat {f} _ {x}\right)},
$$

which converts directly into the F-statistic (4.93), using the sample coherence defined in (4.92).

# References

Akaike, H. (1969). Fitting autoregressive models for prediction. Ann. Inst. Stat. Math., 21, 243-247.   
Akaike, H. (1973). Information theory and an extension of the maximum likelihood principal. In 2nd Int. Symp. Inform. Theory, 267-281. B.N. Petrov and F. Csake, eds. Budapest: Akademia Kiado.   
Akaike, H. (1974). A new look at statistical model identification. IEEE Trans. Automat. Contr., AC-19, 716-723.   
Alag´on, J. (1989). Spectral discrimination for two groups of time series. J. Time Series Anal., 10, 203-214.   
Alspach, D.L. and H.W. Sorensen (1972). Nonlinear Bayesian estimation using Gaussian sum approximations. IEEE Trans. Automat. Contr., AC-17, 439-447.   
Anderson, B.D.O. and J.B. Moore (1979). Optimal Filtering. Englewood Cliffs, NJ: Prentice-Hall.   
Anderson, T.W. (1978). Estimation for autoregressive moving average models in the time and frequency domain. Ann. Stat., 5, 842-865.   
Anderson, T.W. (1984). An Introduction to Multivariate Statistical Analysis, 2nd ed. New York: Wiley.   
Ansley, C.F. and P. Newbold (1980). Finite sample properties of estimators for autoregressive moving average processes. J. Econ., 13, 159-183.   
Antognini, J.F., M.H. Buonocore, E.A. Disbrow, and E. Carstens (1997). Isoflurane anesthesia blunts cerebral responses to noxious and innocuous stimuli: a fMRI study. Life Sci., 61, PL349-PL354.   
Bandettini, A., A. Jesmanowicz, E.C. Wong, and J.S. Hyde (1993). Processing strategies for time-course data sets in functional MRI of the human brain. Magnetic Res. Med., 30, 161-173.   
Bar-Shalom, Y. (1978). Tracking methods in a multi-target environment. IEEE Trans. Automat. Contr., AC-23, 618-626.   
Bar-Shalom, Y. and E. Tse (1975). Tracking in a cluttered environment with probabilistic data association. Automatica, 11, 4451-4460.

Bazza, M., R.H. Shumway, and D.R. Nielsen (1988). Two-dimensional spectral analysis of soil surface temperatures. Hilgardia, 56, 1-28.   
Bedrick, E.J. and C.-L. Tsai (1994). Model selection for multivariate regression in small samples. Biometrics, 50, 226-231.   
Beran, J. (1994). Statistics for Long Memory Processes. New York: Chapman and Hall.   
Berk, K.N. (1974). Consistent autoregressive spectral estimates. Ann. Stat., 2, 489-502.   
Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems (with discussion). J. R. Stat. Soc. B, 36, 192-236.   
Bhat, R.R. (1985). Modern Probability Theory, 2nd ed. New York: Wiley.   
Bhattacharya, A. (1943). On a measure of divergence between two statistical populations. Bull. Calcutta Math. Soc., 35, 99-109.   
Blackman, R.B. and J.W. Tukey (1959). The Measurement of Power Spectra from the Point of View of Communications Engineering. New York: Dover.   
Blight, B.J.N. (1974). Recursive solutions for the estimation of a stochastic parameter J. Am. Stat. Assoc., 69, 477-481   
Bloomfield, P. (1976). Fourier Analysis of Time Series: An Introduction. New York: Wiley.   
Bloomfield, P. (2000). Fourier Analysis of Time Series: An Introduction, 2nd ed. New York: Wiley.   
Bloomfield, P. and J.M. Davis (1994). Orthogonal rotation of complex principal components. Int. J. Climatol., 14, 759-775.   
Bogart, B. P., M. J. R. Healy, and J.W. Tukey (1962). The Quefrency Analysis of Time Series for Echoes: Cepstrum, Pseudo-Autocovariance, Cross-Cepstrum and Saphe Cracking. In Proc. of the Symposium on Time Series Analysis, pp. 209-243, Brown University, Providence, USA.   
Bollerslev, T. (1986). Generalized autoregressive conditional heteroscedasticity. J. Econ., 31, 307- 327.   
Box, G.E.P. and G.M. Jenkins (1970). Time Series Analysis, Forecasting, and Control. Oakland, CA: Holden-Day.   
Box, G.E.P., G.M. Jenkins and G.C. Reinsel (1994). Time Series Analysis, Forecasting, and Control, 3rd ed. Englewood Cliffs, NJ: Prentice Hall.   
Box, G.E.P. and D.A. Pierce (1970). Distributions of residual autocorrelations in autoregressive integrated moving average models. J. Am. Stat. Assoc., 72, 397-402.   
Box, G.E.P. and G.C. Tiao (1973). Bayesian Inference in Statistical Analysis. New York: Wiley.   
Breiman, L. and J. Friedman (1985). Estimating optimal transformations for multiple regression and correlation (with discussion). J. Am. Stat. Assoc.,

80, 580-619.   
Brillinger, D.R. (1973). The analysis of time series collected in an experimental design. In Multivariate Analysis-III., pp. 241-256. P.R. Krishnaiah ed. New York: Academic Press.   
Brillinger, D.R. (1980). Analysis of variance and problems under time series models. In Handbook of Statistics, Vol I, pp. 237-278. P.R. Krishnaiah and D.R. Brillinger, eds. Amsterdam: North Holland.   
Brillinger, D.R. (1981). Time Series: Data Analysis and Theory, 2nd ed. San Francisco: Holden-Day. Republished in 2001 by the Society for Industrial and Applied Mathematics, Philadelphia.   
Brockwell, P.J. and R.A. Davis (1991). Time Series: Theory and Methods, 2nd ed. New York: Springer-Verlag.   
Bruce, A. and H-Y. Gao (1996). Applied Wavelet Analysis with S-PLUS. New York: Springer-Verlag.   
Caines, P.E. (1988). Linear Stochastic Systems. New York: Wiley.   
Carlin, B.P., N.G. Polson, and D.S. Stoffer (1992). A Monte Carlo approach to nonnormal and nonlinear state-space modeling. J. Am. Stat. Assoc., 87, 493-500.   
Carter, C. K. and R. Kohn (1994). On Gibbs sampling for state space models. Biometrika, 81, 541-553.   
Chernoff, H. (1952). A measure of asymptotic efficiency for tests of a hypothesis based on the sum of the observations. Ann. Math. Stat., 25, 573-578.   
Cleveland, W.S. (1979). Robust locally weighted regression and smoothing scatterplots. J. Am. Stat. Assoc., 74, 829-836.   
Cochrane, D. and G.H. Orcutt (1949). Applications of least squares regression to relationships containing autocorrelated errors. J. Am. Stat. Assoc., 44, 32-61.   
Cooley, J.W. and J.W. Tukey (1965). An algorithm for the machine computation of complex Fourier series. Math. Comput., 19, 297-301.   
Cressie, N.A.C. (1993). Statistics for Spatial Data. New York: Wiley.   
Dahlhaus, R. (1989). Efficient parameter estimation for self-similar processes. Ann. Stat., 17, 1749-1766.   
Dargahi-Noubary, G.R. and P.J. Laycock (1981). Spectral ratio discriminants and information theory. J. Time Series Anal., 16, 201-219.   
Danielson, J. (1994). Stochastic volatility in asset prices: Estimation with simulated maximum likelihood. J. Econometrics, 61, 375-400.   
Daubechies, I. (1992). Ten Lectures on Wavelets. Philadelphia: CBMS-NSF Regional Conference Series in Applied Mathematics.   
Davies, N., C.M. Triggs, and P. Newbold (1977). Significance levels of the

Box-Pierce portmanteau statistic in finite samples. Biometrika, 64, 517- 522.   
Dent, W. and A.-S. Min. (1978). A Monte Carlo study of autoregressiveintegrated-moving average processes. J. Econ., 7, 23-55.   
Dempster, A.P., N.M. Laird and D.B. Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm. J. R. Stat. Soc. B, 39, 1-38.   
Diggle, P.J., K.-Y. Liang, and S.L. Zeger (1994). The Analysis of Longitudinal Data. Oxford: Clarendon Press.   
Ding, Z., C.W.J. Granger, and R.F. Engle (1993). A long memory property of stock market returns and a new model. J. Empirical Finance, 1, 83-106.   
Donoho, D.L. and I.M. Johnstone (1994). Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81, 425-455.   
Donoho, D.L. and I.M. Johnstone (1995). Adapting to unknown smoothness via wavelet shrinkage. J. of Am. Stat. Assoc., 90, 1200-1224.   
Durbin, J. (1960). Estimation of parameters in time series regression models. J. R. Stat. Soc. B, 22, 139-153.   
Durbin, J. and S.J. Koopman (2001). Time Series Analysis by State Space Methods Oxford: Oxford University Press.   
Efron, B. and R. Tibshirani (1994). An Introduction to the Bootstrap. New York: Chapman and Hall.   
Engle, R.F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica, 50, 987-1007.   
Engle, R.F., D. Nelson, and T. Bollerslev (1994). ARCH Models. In Handbook of Econometrics, Vol IV, pp. 2959-3038. R. Engle and D. McFadden, eds. Amsterdam: North Holland.   
Fahrmeir, L. and G. Tutz (1994). Multivariate Statistical Modelling Based on Generalized Linear Models. New York: Springer-Verlag.   
Fox, R. and M.S. Taqqu (1986). Large sample properties of parameter estimates for strongly dependent stationary Gaussian time series. Ann. Stat., 14, 517-532.   
Friedman, J.H. (1984). A Variable Span Smoother. Tech. Rep. No. 5, Lab. for Computational Statistics, Dept. Statistics, Stanford Univ., California.   
Friedman, J.H. and W. Stuetzle. (1981). Projection pursuit regression. J. Am. Stat. Assoc., 76, 817-823.   
Fr¨uhwirth-Schnatter, S. (1994). Data Augmentation and Dynamic Linear Models. J. Time Series Anal., 15, 183–202.   
Fuller, W.A. (1976). Introduction to Statistical Time Series. New York: Wiley.   
Fuller, W.A. (1995). Introduction to Statistical Time Series, 2nd ed. New York: Wiley.

Gabr, M.M. and T. Subba-Rao (1981). The estimation and prediction of subset bilinear time series models with applications. J. Time Series Anal., 2, 155-171.   
Gelfand, A.E. and A.F.M. Smith (1990). Sampling-based approaches to calculating marginal densities. J. Am. Stat. Assoc., 85, 398-409.   
Gelman, A., J. Carlin, H. Stern, and D. Rubin (1995). Bayesian Data Analysis. London: Chapman and Hall.   
Geman, S. and D. Geman (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Trans. Pattern Anal. Machine Intell., 6, 721-741.   
Geweke, J.F. (1977). The dynamic factor analysis of economic time series models. In Latent Variables in Socio-Economic Models, pp 365-383. D. Aigner and A. Goldberger, eds. Amsterdam: North Holland.   
Geweke, J.F. and K.J. Singleton (1981). Latent variable models for time series: A frequency domain approach with an application to the Permanent Income Hypothesis. J. Econ., 17, 287-304.   
Geweke, J.F. and S. Porter-Hudak (1983). The estimation and application of long-memory time series models. J. Time Series Anal., 4, 221-238.   
Gilks, W.R., S. Richardson, and D.J. Spiegelhalter (eds.) (1996). Markov Chain Monte Carlo in Practice. London: Chapman and Hall.   
Giri, N. (1965). On complex analogues of $T ^ { 2 }$ and $R ^ { 2 }$ tests. Ann. Math. Stat., 36, 664-670.   
Goldfeld, S.M. and R.E. Quandt (1973). A Markov model for switching regressions. J. Econ., 1, 3-16.   
Goodman, N.R. (1963). Statistical analysis based on a certain multivariate complex Gaussian distribution. Ann. Math. Stat., 34, 152-177.   
Goodrich, R.L. and P.E. Caines (1979). Linear system identification from nonstationary cross-sectional data. IEEE Trans. Automat. Contr., AC-24, 403-411.   
Gordon, K. and A.F.M. Smith (1990). Modeling and monitoring biomedical time series. J. Am. Stat. Assoc., 85, 328-337.   
Gouri´eroux, C. (1997). ARCH Models and Financial Applications. New York: Springer-Verlag.   
Granger, C.W. and R. Joyeux (1980). An introduction to long-memory time series models and fractional differencing. J. Time Series Anal., 1, 15-29.   
Grether, D.M. and M. Nerlove (1970). Some properties of optimal seasonal adjustment. Econometrica, 38, 682-703.   
Gupta N.K. and R.K. Mehra (1974). Computational aspects of maximum likelihood estimation and reduction in sensitivity function calculations. IEEE Trans. Automat. Contr., AC-19, 774-783.

Hamilton, J.D. (1989). A new approach to the economic analysis of nonstationary time series and the business cycle. Econometrica, 57, 357-384.   
Hannan, E.J. (1970). Multiple Time Series. New York: Wiley.   
Hannan, E.J. (1973). The asymptotic theory of linear time series models. J. Appl. Prob., 10, 130-145.   
Hannan, E.J. and M. Deistler (1988). The Statistical Theory of Linear Systems. New York: Wiley.   
Hansen, J. and S. Lebedeff (1987). Global trends of measured surface air temperature. J. Geophys. Res., 92, 1345-1372.   
Hansen, J. and S. Lebedeff (1988). Global surface air temperatures: Update through 1987. J. Geophys. Lett., 15, 323-326.   
Harrison, P.J. and C.F. Stevens (1976). Bayesian forecasting (with discussion). J. R. Stat. Soc. B, 38, 205-247.   
Harvey, A.C. and P.H.J. Todd (1983). Forecasting economic time series with structural and Box-Jenkins models: A case study. J. Bus. Econ. Stat., 1, 299-307.   
Harvey, A.C. and R.G. Pierse (1984). Estimating missing observations in economic time series. J. Am. Stat. Assoc., 79, 125-131.   
Harvey, A.C. (1991). Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge: Cambridge University Press.   
Harvey, A.C. (1993). Time Series Models. Cambridge, MA: MIT Press.   
Harvey A.C., E. Ruiz and N. Shephard (1994). Multivariate stochastic volatility models. Rev. Economic Studies, 61, 247-264.   
Hastings, W.K. (1970). Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57, 97-109.   
Hosking, J.R.M. (1981). Fractional differencing. Biometrika, 68, 165-176.   
Hurst, H. (1951). Long term storage capacity of reservoirs. Trans. Am. Soc. Civil Eng., 116, 778-808.   
Hurvich, C.M. and S. Zeger (1987). Frequency domain bootstrap methods for time series. Tech. Report 87-115, Department of Statistics and Operations Research, Stern School of Business, New York University.   
Hurvich, C.M and C.-L. Tsai (1989). Regression and time series model selection in small samples. Biometrika, 76, 297-307.   
Hurvich, C.M. and K.I. Beltrao (1993). Asymptotics for the low-requency oridnates of the periodogram for a long-memory time series. J. Time Series Anal., 14, 455-472.   
Hurvich, C.M., R. Deo and J. Brodsky (1998). The mean squared error of Geweke and Porter-Hudak’s estimator of the memory parameter of a longmemory time series. J. Time Series Anal., 19, 19-46.

Jacquier, E., N.G. Polson, and P.E. Rossi (1994). Bayesian analysis of stochastic volatility models. J. Bus. Econ. Stat., 12, 371-417.   
Jazwinski, A.H. (1970). Stochastic Processes and Filtering Theory. New York: Academic Press.   
Jenkins, G.M. and D.G. Watts. (1968). Spectral Analysis and Its Applications. San Francisco: Holden-Day.   
Johnson, R.A. and D.W. Wichern (1992). Applied Multivariate Statistical Analysis, 3rd ed.. Englewood Cliffs, NJ: Prentice-Hall.   
Jones, P.D. (1994). Hemispheric surface air temperature variations: A reanalysis and an update to 1993. J. Clim., 7, 1794-1802.   
Jones, R.H. (1980). Maximum likelihood fitting of ARMA models to time series with missing observations. Technometrics, 22, 389-395.   
Jones, R.H. (1984). Fitting multivariate models to unequally spaced data. In Time Series Analysis of Irregularly Observed Data, pp. 158-188. E. Parzen, ed. Lecture Notes in Statistics, 25, New York: Springer-Verlag.   
Jones, R.H. (1993). Longitudinal Data With Serial Correlation : A State-Space Approach. London: Chapman and Hall.   
Journel, A.G. and C.H. Huijbregts (1978). Mining Geostatistics. New York: Academic Press.   
Juang, B.H. and L.R. Rabiner (1985). Mixture autoregressive hidden Markov models for speech signals, IEEE Trans. Acoust., Speech, Signal Process., ASSP-33, 1404-1413.   
Kakizawa, Y., R. H. Shumway, and M. Taniguchi (1998). Discrimination and clustering for multivariate time series. J. Am. Stat. Assoc., 93, 328-340.   
Kalman, R.E. (1960). A new approach to linear filtering and prediction problems. Trans ASME J. Basic Eng., 82, 35-45.   
Kalman, R.E. and R.S. Bucy (1961). New results in filtering and prediction theory. Trans. ASME J. Basic Eng., 83, 95-108.   
Kay, S.M. (1988). Modern Spectral Analysis: Theory and Applications. Englewood Cliffs, NJ: Prentice-Hall.   
Kazakos, D. and P. Papantoni-Kazakos (1980). Spectral distance measuring between Gaussian processes. IEEE Trans. Automat. Contr., AC-25, 950- 959.   
Khatri, C.G. (1965). Classical statistical analysis based on a certain multivariate complex Gaussian distribution. Ann. Math. Stat., 36, 115-119.   
Kim S., N. Shephard and S. Chib (1998). Stochastic volatility: likelihood inference and comparison with ARCH models. Rev. Economic Studies, 65, p.361-393.   
Kitagawa, G. (1987). Non-Gaussian state-space modeling of nonstationary time series (with discussion). J. Am. Stat. Assoc., 82, 1032-1041, (C/R: p1041-1063; C/R: V83 p1231).

Kitagawa, G. and W. Gersch (1984). A smoothness priors modeling of time series with trend and seasonality. J. Am. Stat. Assoc., 79, 378-389.   
Kitagawa, G. and W. Gersch (1996). Smoothness Priors Analysis of Time Series. New York: Springer-Verlag.   
Kolmogorov, A.N. (1941). Interpolation and extrapolation von station¨aren zuf¨alligen folgen. Bull. Acad. Sci. U.R.S.S., 5, 3-14.   
Krishnaiah, P.R., J.C. Lee, and T.C. Chang (1976). The distribution of likelihood ratio statistics for tests of certain covariance structures of complex multivariate normal populations. Biometrika, 63, 543-549.   
Kullback, S. and R.A. Leibler (1951). On information and sufficiency. Ann. Math. Stat., 22, 79-86.   
Kullback, S. (1958). Information Theory and Statistics. Gloucester, MA: Peter Smith.   
Lachenbruch, P.A. and M.R. Mickey (1968). Estimation of error rates in discriminant analysis. Technometrices, 10, 1-11.   
Laird, N. and J. Ware (1982). Random-effects models for longitudinal data. Biometrics, 38, 963-974.   
Lam, P.S. (1990). The Hamilton model with a general autoregressive component: Estimation and comparison with other modles of economic time series. J. Monetary Econ., 26, 409-432.   
Lay, T. (1997). Research required to support comprehensive nuclear test ban treaty monitoring. National Research Council Report, National Academy Press, 2101 Constitution Ave., Washington, DC 20055.   
Levinson, N. (1947). The Wiener (root mean square) error criterion in filter design and prediction. J. Math. Phys., 25, 262-278.   
Lindgren, G. (1978). Markov regime models for mixed distributions and switching regressions. Scand. J. Stat., 5, 81-91.   
Liu, L.M. (1991). Dynamic relationship analysis of U.S. gasoline and crude oil prices. J. Forecast., 10, 521-547.   
Ljung, G.M. and G.E.P. Box (1978). On a measure of lack of fit in time series models. Biometrika, 65, 297-303.   
L¨utkepohl, H. (1985). Comparison of criteria for estimating the order of a vector autoregressive process. J. Time Series Anal., 6, 35-52.   
L¨utkepohl, H. (1993). Introduction to Multiple Time Series Analysis, 2nd ed. Berlin: Springer-Verlag.   
McQuarrie, A.D.R. and R.H. Shumway (1994). ASTSA for Windows.   
McQuarrie, A.D.R. and C-L. Tsai (1998). Regression and Time Series Model Selection, Singapore: World Scientific.   
Mallows, C.L. (1973). Some comments on $C _ { p }$ . Technometrics, 15, 661-675.

McBratney, A.B. and R. Webster (1981). Detection of ridge and furrow pattern by spectral analysis of crop yield. Int. Stat. Rev., 49, 45-52.   
McCulloch, R.E. and R.S. Tsay (1993). Bayesian inference and prediction for mean and variance shifts in autoregressive time series. J. Am. Stat. Assoc., 88, 968-978.   
McDougall, A. J., D.S. Stoffer and D.E. Tyler (1997). Optimal transformations and the spectral envelope for real-valued time series. J. Stat. Plan. Infer., 57, 195-214.   
McLeod A.I. (1978). On the distribution of residual autocorrelations in Box-Jenkins models. J. R. Stat. Soc. B, 40, 296-302.   
McLeod, A.I. and K.W. Hipel (1978). Preservation of the rescaled adusted range, I. A reassessment of the Hurst phenomenon. Water Resour. Res., 14, 491-508.   
Meinhold, R.J. and N.D. Singpurwalla (1983). Understanding the Kalman filter. Am. Stat., 37, 123-127.   
Meinhold, R.J. and N.D. Singpurwalla (1989). Robustification of Kalman filter models. J. Am. Stat. Assoc., 84, 479-486.   
Meng X.L. and Rubin, D.B. (1991). Using EM to obtain asymptotic variance– covariance matrices: The SEM algorithm. J. Am. Stat. Assoc., 86, 899-909.   
Metropolis N., A.W. Rosenbluth, M.N. Rosenbluth, A. H. Teller, and E. Teller (1953). Equations of state calculations by fast computing machines. J. Chem. Phys., 21, 1087-1091.   
Mickens, R.E. (1987). Difference Equations. New York: Van Nostrand Reinhold.   
Newbold, P. and T. Bos (1985). Stochastic Parameter Regression Models. Beverly Hills: Sage.   
Ogawa, S., T.M. Lee, A. Nayak and P. Glynn (1990). Oxygenation-sensititive contrast in magnetic resonance image of rodent brain at high magnetic fields. Magn. Reson. Med., 14, 68-78.   
Palma, W. and N.H. Chan (1997). Estimation and forecasting of long-memory time series with missing values. J. Forecast., 16, 395-410.   
Paparoditis, E. and Politis, D.N. (1999). The local bootstrap for periodogram statistics. J. Time Series Anal., 20, 193-222.   
Parker, D.E., P.D. Jones, A. Bevan and C.K. Folland (1994). Interdecadal changes of surface temperature since the late 19th century. J. Geophysical Research, 90, 14373-14399.   
Parker, D.E., C.K. Folland and M. Jackson (1995). Marine surface temperature: observed variations and data requirements. Climatic Change, 31, 559-60

Parzen, E. (1961). Mathematical considerations in the estimation of spectra. Technometrics, 3, 167-190.   
Parzen, E. (1983). Autoregressive spectral estimation. In Time Series in the Frequency Domain, Handbook of Statistics, Vol. 3, pp. 211-243. D.R. Brillinger and P.R. Krishnaiah eds. Amsterdam: North Holland.   
Pawitan, Y. and R.H. Shumway (1989). Spectral estimation and deconvolution for a linear time series model. J. Time Series Anal., 10, 115-129.   
Pe˜na, D. and I. Guttman (1988). A Bayesian approach to robustifying the Kalman filter. In Bayesian Analysis of Time Series and Dynamic Linear Models, pp. 227-254. J.C. Spall, ed. New York: Marcel Dekker.   
Percival, D.B. and A.T. Walden (1993). Spectral Analysis for Physical Applications: Multitaper and Conventional Univariate Techniques Cambridge: Cambridge University Press.   
Percival, D.B. and A.T. Walden (2000). Wavelet Methods for Time Series Analysis. Cambridge: Cambridge University Press.   
Pinsker, M.S. (1964). Information and Information Stability of Random Variables and Processes, San Francisco: Holden Day.   
Pole, P.J. and M. West (1988). Nonnormal and nonlinear dynamic Bayesian modeling. In Bayesian Analysis of Time Series and Dynamic Linear Models, pp. 167-198. J.C. Spall, ed. New York: Marcel Dekker.   
Press, W.H., S.A. Teukolsky, W. T. Vetterling, and B.P. Flannery (1993). Numerical Recipes in C: The Art of Scientific Computing, 2nd ed. Cambridge: Cambridge University Press.   
Priestley, M.B., T. Subba-Rao and H. Tong (1974). Applications of principal components analysis and factor analysis in the identification of multivariable systems. IEEE Trans. Automat. Contr., AC-19, 730-734.   
Priestley, M.B. and T. Subba-Rao (1975). The estimation of factor scores and Kalman filtering for discrete parameter stationary processes. Int. J. Contr., 21, 971-975.   
Priestley, M.B. (1981). Spectral Analysis and Time Series. Vol. 1: Univariate Series; Vol 2: Multivariate Series, Prediction and Control. New York: Academic Press.   
Priestley, M.B. (1988). Nonlinear and Nonstationary Time Series Analysis. London: Academic Press.   
Quandt, R.E. (1972). A new approach to estimating switching regressions. J. Am. Stat. Assoc., 67, 306-310.   
Rabiner, L.R. and B.H. Juang (1986). An introduction to hidden Markov models, IEEE Acoust., Speech, Signal Process., ASSP-34, 4-16.   
Rao, C.R. (1973). Linear Statistical Inference and Its Applications. New York: Wiley.

Rauch, H.E., F. Tung, and C.T. Striebel (1965). Maximum likelihood estimation of linear dynamic systems. J. AIAA, 3, 1445-1450.   
Reinsel, G.C. (1997). Elements of Multivariate Time Series Analysis, 2nd ed. New York: Springer-Verlag.   
Renyi, A. (1961). On measures of entropy and information. In Proceedings of 4th Berkeley Symp. Math. Stat. and Probability, pp. 547-561, Berkeley: Univ. of California Press.   
Rissanen, J. (1978). Modeling by shortest data description. Automatica, 14, 465-471.   
Robinson, P.M. (1995). Gaussian semiparametric estimation of long range dependence. Ann. Stat., 23, 1630-1661.   
Rosenblatt, M. (1956). A central limit theorem and a strong mixing condition. Proc. Nat. Acad. Sci., 42, 43-47.   
Royston, P. (1982). An extension of Shapiro and Wilk’s W test for normality to large samples. Applied Statistics, 31, 115-124.   
Sandmann, G. and S.J. Koopman (1998). Estimation of stochastic volatility models via Monte Carlo maximum likelihood. J. Econometrics, 87 , 271- 301.   
Sargan, J.D. (1964). Wages and prices in the United Kingdom: A study in econometric methodology. In Econometric Analysis for National Economic Planning, eds. P. E. Hart, G. Mills and J. K. Whitaker. London: Butterworths. reprinted in Quantitative Economics and Econometric Analysis, pp. 275-314, eds. K. F. Wallis and D. F. Hendry (1984). Oxford: Basil Blackwell.   
Scheff´e, H. (1959). The Analysis of Variance. New York: Wiley.   
Schuster, A. (1898). On the investigation of hidden periodicities with application to a supposed 26 day period of meteorological phenomena. Terrestrial Magnetism, III, 11-41.   
Schuster, A. (1906). On the periodicities of sunspots. Phil. Trans. R. Soc., Ser. A, 206, 69-100.   
Schwarz, F. (1978). Estimating the dimension of a model. Ann. Stat., 6, 461-464.   
Schweppe, F.C. (1965). Evaluation of likelihood functions for Gaussian signals. IEEE Trans. Inform. Theory, IT-4, 294-305.   
Seber, G.A.G. (1977). Linear Regression Analysis. New York: Wiley.   
Shephard, N. (1996). Statistical aspects of ARCH and stochastic volatility. In Time Series Models in Econometrics, Finance and Other Fields , pp 1-100. D.R. Cox, D.V. Hinkley, and O.E. Barndorff-Nielson eds. London: Chapman and Hall.   
Shumway, R.H. and W.C. Dean (1968). Best linear unbiased estimation for multivariate stationary processes. Technometrics, 10, 523-534.

Shumway, R.H. (1970). Applied regression and analysis of variance for stationary time series. J. Am. Stat. Assoc., 65, 1527-1546.   
Shumway, R.H. (1971). On detecting a signal in $N$ stationarily correlated noise series. Technometrics, 10, 523-534.   
Shumway, R.H. and A.N. Unger (1974). Linear discriminant functions for stationary time series. J. Am. Stat. Assoc., 69, 948-956.   
Shumway, R.H. (1982). Discriminant analysis for time series. In Classification, Pattern Recognition and Reduction of Dimensionality, Handbook of Statistics Vol. 2, pp. 1-46. P.R. Krishnaiah and L.N. Kanal, eds. Amsterdam: North Holland.   
Shumway, R.H. and D.S. Stoffer (1982). An approach to time series smoothing and forecasting using the EM algorithm. J. Time Series Anal., 3, 253-264.   
Shumway, R.H. (1983). Replicated time series regression: An approach to signal estimation and detection. In Time Series in the Frequency Domain, Handbook of Statistics Vol. 3, pp. 383-408. D.R. Brillinger and P.R. Krishnaiah, eds. Amsterdam: North Holland.   
Shumway, R.H. (1988). Applied Statistical Time Series Analysis. Englewood Cliffs, NJ: Prentice-Hall.   
Shumway, R.H., R.S. Azari, and Y. Pawitan (1988). Modeling mortality fluctuations in Los Angeles as functions of pollution and weather effects. Environ. Res., 45, 224-241.   
Shumway, R.H. and D.S. Stoffer (1991). Dynamic linear models with switching. J. Am. Stat. Assoc., 86, 763-769, (Correction: V87 p. 913).   
Shumway, R.H. and K.L. Verosub (1992). State space modeling of paleoclimatic time series. In Pro. 5th Int. Meeting Stat. Climatol.. Toronto, pp. 22-26, June, 1992.   
Shumway, R.H. (1996). Statistical approaches to seismic discrimination. In Monitoring a Comprehensive Test Ban Treaty, pp. 791-803. A.M. Dainty and E.S. Husebye eds. Doordrecht, The Netherlands: Kluwer Academic   
Shumway, R.H., S.E. Kim and R.R. Blandford (1999). Nonlinear estimation for time series observed on arrays. Chapter 7, S. Ghosh, ed. Asymptotics, Nonparametrics and Time Series, pp. 227-258. New York: Marcel Dekker.   
Small, C.G. and D.L. McLeish (1994). Hilbert Space Methods in Probability and Statistical Inference. New York: Wiley.   
Smith, A.F.M. and M. West (1983). Monitoring renal transplants: An application of the multiprocess Kalman filter. Biometrics, 39, 867-878.   
Spliid, H. (1983). A fast estimation method for the vector autoregressive moving average model with exogenous variables. J. Am. Stat. Assoc., 78, 843-849.   
Stoffer, D.S. (1982). Estimation of Parameters in a Linear Dynamic System with Missing Observations. Ph.D. Dissertation. Univ. California, Davis.

Stoffer, D.S. (1987). Walsh-Fourier analysis of discrete-valued time series. J. Time Series Anal., 8, 449-467.   
Stoffer, D.S., M. Scher, G. Richardson, N. Day, and P. Coble (1988). A Walsh-Fourier analysis of the effects of moderate maternal alcohol consumption on neonatal sleep-state cycling. J. Am. Stat. Assoc., 83, 954-963.   
Stoffer, D.S. and K.D. Wall (1991). Bootstrapping state space models: Gaussian maximum likelihood estimation and the Kalman filter. J. Am. Stat. Assoc., 86, 1024-1033.   
Stoffer, D.S., D.E. Tyler, and A.J. McDougall (1993). Spectral analysis for categorical time series: Scaling and the spectral envelope. Biometrika, 80, 611-622.   
Stoffer, D.S. and D.E. Tyler (1998). Matching sequences: Cross-spectral analysis of categorical time series Biometrika, 85, 201-213.   
Stoffer, D.S. (1999). Detecting common signals in multiple time series using the spectral envelope. J. Am. Stat. Assoc., 94, 1341-1356.   
Stoffer, D.S. and K.D. Wall (2004). Resampling in State Space Models. In State Space and Unobserved Component Models Theory and Applications, Chapter 9, pp. 227-258. Andrew Harvey, Siem Jan Koopman, and Neil Shephard, eds. Cambridge: Cambridge University Press.   
Subba-Rao, T. (1981). On the theory of bilinear time series models. J. R. Stat. Soc. B, 43, 244-255.   
Sugiura, N. (1978). Further analysis of the data by Akaike’s information criterion and the finite corrections, Commun. Statist, A, Theory Methods, 7, 13-26.   
Taniguchi, M., M.L. Puri, and M. Kondo (1994). Nonparametric approach for non-Gaussian vector stationary processes. J. Mult. Anal., 56, 259-283.   
Tanner, M. and W.H. Wong (1987). The calculation of posterior distributions by data augmentation (with discussion). J. Am. Stat. Assoc., 82, 528-554.   
Tiao, G.C. and R.S. Tsay (1989). Model specification in multivariate time series (with discussion). J. Roy. Statist. Soc. B, 51, 157-213.   
Tiao, G. C. and R.S. Tsay (1994). Some advances in nonlinear and adaptive modeling in time series analysis. J. Forecast., 13, 109-131.   
Tiao, G.C., R.S. Tsay and T .Wang (1993). Usefulness of linear transformations in multivariate time series analysis. Empir. Econ., 18, 567-593.   
Tierney, L. (1994). Markov chains for exploring posterior distributions (with discussion). Ann. Stat., 22, 1701-1728.   
Tong, H. (1983). Threshold Models in Nonlinear Time Series Analysis. Springer Lecture Notes in Statistics, 21. New York: Springer-Verlag.   
Tong, H. (1990). Nonlinear Time Series: A Dynamical System Approach. Oxford: Oxford Univ. Press.

Tsay, R. (1987). Conditional hetereroscadasticity in time series analysis. J. Am. Stat. Assoc., 82, 590-604.   
Venables, W.N. and B.D. Ripley (1994). Modern Applied Statistics with S-Plus. New York: Springer-Verlag.   
Walker, G. (1931). On periodicity in series of related terms. Proc. R. Soc. Lond., Ser. A, 131, 518-532.   
Watson, G.S. (1966). Smooth regression analysis. Sankhya, 26, 359-378.   
Weiss, A.A. (1984). ARMA models with ARCH errors. J. Time Series Anal., 5, 129-143.   
West, M. and J. Harrison (1997). Bayesian Forecasting and Dynamic Models 2nd ed. New York: Springer-Verlag.   
Whittle, P. (1961). Gaussian estimation in stationary time series. Bull. Int. Stat. Inst., 33, 1-26.   
Wiener, N. (1949). The Extrapolation, Interpolation and Smoothing of Stationary Time Series with Engineering Applications. New York: Wiley.   
Wu, C.F. (1983). On the convergence properties of the EM algorithm. Ann. Stat., 11, 95-103.   
Young, P.C. and D.J. Pedregal (1998). Macro-economic relativity: Government spending, private investment and unemployment in the USA. Centre for Research on Environmental Systems and Statistics, Lancaster University, U.K.   
Yule, G.U. (1927). On a method of investigating periodicities in disturbed series with special reference to Wolfer’s Sunspot Numbers. Phil. Trans. R. Soc. Lond., A226, 267-298.

# Index

ACF, 22, 25

large sample distribution, 30, 519

multidimensional, 37

of an AR(p), 106

of an AR(1), 87

of an AR(2), 100

of an ARMA(1,1), 105

of an MA(q), 104

sample, 30

AIC, 53, 153

multivariate case, 302

AICc, 54, 153, 229

multivariate case, 302

Aliasing, 12

Amplitude, 176

of a filter, 225

Analysis of Power, see ANOPOW

ANOPOW, 423, 431, 432

designed experiments, 438

AR model, 14, 85

conditional sum of squares, 127

bootstrap, 137

conditional likelihood, 127

estimation

large sample distribution, 123, 529

likelihood, 126

maximum likelihood estimation, 126

missing data, 409

operator, 86

polynomial, 94

spectral density, 186, 228

threshold, 290

unconditional sum of squares,

126

vector, see VAR

with observational noise, 328

ARCH model

ARCH(m), 285

ARCH(1), 281

estimation, 282

GARCH, 286, 388

ARFIMA model, 272, 276

ARIMA model, 141

fractionally integrated, 276

multiplicative seasonal models, 159

multivariate, 302

ARMA model, 93

$\psi$ -weights, 102

conditional least squares, 128

pure seasonal models

behavior of ACF and PACF, 156

unconditional least squares, 128

backcasts, 121

behavior of ACF and PACF, 109

bootstrap, 359

causality of, 95

conditional least squares, 130

forecasts, 116

mean square prediction error, 117

based on infinite past, 116

prediction intervals, 119

truncated prediction, 118

Gauss–Newton, 131

in state-space form, 357

invertibilty of, 96

large sample distribution of

estimators, 133

likelihood, 128

MLE, 128

multiplicative seasonal model, 156

pure seasonal model, 155

unconditional least squares, 131

vector, see VARMA model

ARMAX model, 305, 317, 356

bootstrap, 359

for cross-sectional data, 395

in state-space form, 356

with time-varying parameters, 397

Autocorrelation function, see ACF

Autocovariance function, 20, 25, 87

multidimensional, 36, 37

random sum of sines and cosines, 177

sample, 30

Autocovariance matrix, 35 sample, 36

Autoregressive Integrated Moving Average Model, see ARIMA model

Autoregressive models, see AR model Autoregressive Moving Average Models, see ARMA model

Backcasting, 120

Backshift operator, 61

Bandwidth, 197, 214

Bartlett kernel, 207

Beam, 427

Best linear predictor, see BLP

BIC, 54

BLP, 111

$m$ -step-ahead prediction, 115

mean square prediction error, 115

one-step-ahead prediction, 112

definition, 111

one-step-ahead prediction

mean square prediction error, 112

stationary processes, 111

Bone marrow transplant series, 325, 352

Bonferroni inequality, 203

Bootstrap, 137, 198, 229, 359

stochastic volatility, 391

Bounded in probability $O _ { p }$ , 504

Canonical correlation, 309

Cauchy sequence, 522

Cauchy–Schwarz inequality, 501, 522

Causal, 89, 95, 526

conditions for an AR(2), 97

vector model, 306

CCF, 23, 27

large sample distribution, 31 sample, 31

Central Limit Theorem, 509

M-dependent, 511

Cepstral analysis, 263

Characteristic function, 507

Chernoff information, 459

Cluster analysis, 461

Coherence, 216

estimation, 218

hypothesis test, 218, 554

multiple, 420

Completeness of $L ^ { 2 }$ , 502

Complex normal distribution, 550

Complex roots, 101

Conditional least squares, 128

Convergence in distribution, 506

Basic Approximation Theorem, 507

Convergence in probability, 504

Convolution, 220

Cosine transform

large sample distribution, 539

of a vector process, 416

properties, 192

Cospectrum, 215

of a vector process, 416

Cram´er–Wold device, 507

Cross-correlation function, see CCF

Cross-covariance function, 22 sample, 31

Cross-spectrum, 215

Cycle, 176

Daniell kernel, 204, 205

modified, 205

Deconvolution, 435

Density function, 19

Designed experiments, see ANOPOW

Deterministic process, 532

Detrending, 49

DFT, 69

inverse, 188

large sample distribution, 539

multidimensional, 257

of a vector process, 416

likelihood, 417

Differencing, 60, 61

Discrete wavelet transform, see DWT

Discriminant analysis, 451

DLM, 324, 355

Bayesian approach, 376

bootstap, 359

for cross-sectional data, 396

form for mixed linear models, 400

innovations form, 358

maximum likelihood estimation large sample distribution, 347

via EM algorithm, 344, 350

via Newton-Raphson, 340

MCMC methods, 379

observability, 346

observation equation, 325

state equation, 324

steady-state, 346

with switching, 362

EM algorithm, 370

maximum likelihood

estimation, 369

DNA series, 482, 488

Durbin–Levinson algorithm, 113

DWT, 239

Dynamic Fourier analysis, 232

Earthquake series, 10, 210, 232, 241, 245, 414, 448, 454, 460, 463

EEG sleep data, 480

EM algorithm, 342

complete data likelihood, 343

DLM with missing observations, 350

expectation step, 343

maximization step, 344

Explosion series, 10, 210, 232, 241, 245, 414, 448, 454, 460, 463

Exponentially Weighted Moving

Averages, 142

Factor analysis, 470

EM algorithm, 472

Federal Reserve Board Index

production, 160

unemployment, 160

Fej´er kernel, 207, 214

FFT, 69

Filter, 61

amplitude, 225, 226

band-pass, 255

design, 255

high-pass, 222, 255

linear, 220

low-pass, 223, 255

matrix, 227

optimum, 252

phase, 225, 226

recursive, 255

seasonal adjustment, 255

spatial, 256

time-invariant, 502

fMRI, see Functional magnetic resonance imaging series

Folding frequency, 177

Fourier transform, 184

discrete, see DFT

fast, see FFT

finite, see DFT

pairs, 184

Fractional difference, 62, 272

fractional noise, 272

Frequency bands, 183, 197

Frequency response function, 221

of a first difference filter, 222

of a moving average filter, 222

Functional magnetic resonance imaging series, 9, 413, 440, 442, 445, 469, 474

Gaussian distribution, 19

Gibbs sampler, see MCMC

Glacial varve series, 62, 132, 151, 274

Global temperature series, 5, 58, 62, 327

Gradient vector, 341, 408

Growth rate, 143, 280

Hessian matrix, 341, 408

Hidden Markov model, 364, 368 estimation, 370

Hilbert space, 522 closed span, 523

conditional expectation, 525

projection mapping, 523

regression, 524

Homogeneous difference equation first order, 98

general solution, 102

second order, 98

solution, 99

Impulse response function, 220

Influenza series, 290, 371

Infrasound series, 427, 429, 432, 436, 437

Inner product space, 522

Innovations, 148, 331, 339

standardized, 148

steady-state, 346

Innovations algorithm, 115

Interest rate and inflation rate series, 359

Invertible, 92

vector model, 306

J-divergence measure, 461

Johnson & Johnson quarterly earnings series, 4, 352

Joint distribution function, 18

Kalman filter, 331

correlated noise, 356

innovations form, 358

Riccati equation, 346

stability, 345

with missing observations, 348

with switching, 366

Kalman smoother, 335, 406

for the lag-one covariance, 337

with missing observations, 348

Kullback-Leibler information, 79, 458

LA Pollution – Mortality Study, 54, 75, 77, 294, 303, 305, 311, 318

Lag, 22, 28

Lagged regression model, 296

Lead, 28

Leakage, 208

sidelobe, 208

Least squares estimation, see LSE

Likelihood

AR(1) model, 126

conditional, 127

innovations form, 128, 340

Linear filter, see Filter

Linear process, 28, 95

Ljung–Box–Pierce statistic, 149

Local level model, 333, 336

Long memory, 62, 272

estimation, 274

estimation of $d$ , 278

spectral density, 277

Longitudinal data, 394, 400

LSE, 51

conditional sum of squares, 127

Gauss–Newton, 130

unconditional, 126

MA model, 13, 90

autocovariance function, 21, 104

Gauss–Newton, 132

mean function, 19

operator, 91

polynomial, 94

spectral density, 185

Markov chain Monte Carlo, see MCMC

Maximum likelihood estimation, see MLE

MCMC, 377

nonlinear and non-Gaussian state-space models, 381, 3

rejection sampling, 379

Mean function, 19

Mean square convergence, 501

Method of moments estimators, see Yule–Walker

Minimum mean square error predictor, 110

Missing data, 348, 350

Mixed linear models, 400

MLE

ARMA model, 128

conditional likelihood, 127

DLM, 340

state-space model, 340

via EM algorithm, 342

via Newton–Raphson, 129, 340

via scoring, 129

Mortality series, see LA Pollution – Mortality Study

Moving average model, see MA model

New York Stock Exchange, 6, 390

Newton–Raphson, 129

Normal distribution, 19 multivariate, 550

NYSE, see New York Stock Exchange

Order in probability $o _ { p }$ , 504

Orthogonality property, 523

PACF, 107

of an MA(1), 108

iterative solution, 114

large sample results, 123

of an AR(p), 108

of an AR(1), 107

of an MA(q), 108

Parameter redundancy, 94

Partial autocorrelation function, see PACF

Partial autoregression matrices, 309

Partial canonical correlation, 310

Parzen window, 213

Period, 176

Periodogram, 70, 188

disribution, 193

matrix, 457

scaled, 68

Phase, 177

of a filter, 225

Pitch period, 6

Pollution series, see LA Pollution – Mortality Study

Prediction equations, 111

Prenatal smoking and growth series, 397

Prewhiten, 297

Principal components, 464

Projection Theorem, 523

Quadspectrum, 215

of a vector process, 416

Random sum of sines and cosines, 177, 534, 536

Random walk, 15, 19, 24

autocovariance function, 22

Recruitment series, 7, 33, 65, 109, 120, 194, 199, 205, 219, 248, 298

Regression

ANOVA table, 52

autocorrelated errors, 293

Cochrane-Orcutt procedure, 294

for jointly stationary series, 417

ANOPOW table, 423

Hilbert space, 524

lagged, 245

model, 49

multiple correlation, 53

multivariate, 302

normal equations, 51

periodic, 72

polynomial, 72

random coefficients, 434

spectral domain, 417

stochastic, 359, 434

ridge correction, 435

with deterministic inputs, 426

Return, 6, 143, 280

Riesz–Fisher Theorem, 502

Scaling, 480

Scatterplot matrix, 57, 64, 65

Scatterplot smoothers, 72 kernel, 74

lowess, 75, 77

nearest neighbors, 75

splines, 76, 77

Score vector, 341

Shasta Lake series, 412, 423

SIC, 54, 153, 229

multivariate case, 302, 304

Signal plus noise, 16, 17, 251, 427

mean function, 20

Signal-to-noise ratio, 16, 252

Sinc kernel, 214

Sine transform

large sample distribution, 539

of a vector process, 416

properties, 192

Soil surface temperature series, 36, 38, 257

Southern Oscillation Index, 7, 33, 65, 194, 199, 205, 209, 219, 222,

229, 248, 254, 298

Spectral density, 183

autoregression, 229

estimation, 197

adjusted degrees of freedom, 198

bandwidth stability, 203

confidence interval, 198

degrees of freedom, 198

large sample distribution, 197

nonparametric, 228

parametric, 228

resolution, 203

matrix, 217

linear filter, 227

of a filtered series, 221

of a moving average, 185

of an AR(2), 186

of white noise, 184

wavenumber, 256

Spectral distribution function, 182

Spectral envelope, 479

categorical time series, 484

real-valued time series, 489

Spectral Representation Theorem, 181, 182, 534, 536, 537

vector process, 217, 537

Speech series, 6, 33

State-space model

Bayesian approach, 333, 376

general, 377

linear, see DLM

non-Gaussian, 377, 384

nonlinear, 377, 384

MCMC methods, 381

Stationary

Gaussian series, 29

jointly, 27

strictly, 23

weakly, 24

Stochastic process, 11

realization, 11

Stochastic regression, 359

Stochastic trend, 141

Stochastic volatility

bootstrap, 391

Stochastic volatility model, 388

estimation, 390

Structural component model, 352, 371

Taper, 207, 209

cosine bell, 207

Taylor series expansion in probability, 506

Tchebycheff inequality, 501

Temperature series, see LA Pollution – Mortality Study

Threshold autoregressive model, 290

Time series, 11

categorical, 484

complex-valued, 464

multidimensional, 36, 256

multivariate, 23, 35

two-dimensional, 256

Transfer function model, 296

Transformation

Box-Cox, 62

via spectral envelope, 491

Triangle inequality, 522

Tukey-Hanning window, 214

U.S. GNP series, 144, 150, 154, 283, 490

U.S. macroeconomic series, 477

U.S. population series, 153

Unconditional least squares, 128

VAR model, 303, 304

estimation

large sample distribution, 316

operator, 306

Variogram, 39, 45

VARMA model, 305

autocovariance function, 306

estimation

Spliid algorithm, 317

identifiability of, 309

Varve series, 278

VMA model, 306

operator, 306

Volatility, 6, 280

Wavelet analysis, 235

waveshrink, 244

Wavenumber spectrum, 256

estimation, 257

Weak law of large numbers, 504

White noise, 12

autocovariance function, 21

Gaussian, 12

vector, 303

Whittle likelihood, 231, 457

Wold Decomposition, 532

Yule–Walker

equations, 122

vector model, 307, 309

estimators, 122

AR(2), 123

MA(1), 125

large sample results, 123

Lehmann and Romano: Testing Statistical Hypotheses, Third Edition

Lehmann and Casella: Theory of Point Estimation, Second Edition

Lindman: Analysis of Variance in Experimental Design

Lindsey: Applying Generalized Linear Models

Madansky: Prescriptions for Working Statisticians

McPherson: Applying and Interpreting Statistics: A Comprehensive Guide, Second Edition

Mueller: Basic Principles of Structural Equation Modeling: An Introduction to LISREL and EQS

Nguyen and Rogers: Fundamentals of Mathematical Statistics: Volume I: Probability for Statistics

Nguyen and Rogers: Fundamentals of Mathematical Statistics: Volume II: Statistical Inference

Noether: Introduction to Statistics: The Nonparametric Way

Nolan and Speed: Stat Labs: Mathematical Statistics Through Applications

Peters: Counting for Something: Statistical Principles and Personalities

Pfeiffer: Probability for Applications

Pitman: Probability

Rawlings, Pantula and Dickey: Applied Regression Analysis

Robert: The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation, Second Edition

Robert and Casella: Monte Carlo Statistical Methods

Rose and Smith: Mathematical Statistics with Mathematica

Ruppert: Statistics and Finance: An Introduction

Santner and Duffy: The Statistical Analysis of Discrete Data

Saville and Wood: Statistical Methods: The Geometric Approach

Sen and Srivastava: Regression Analysis: Theory, Methods, and Applications

Shao: Mathematical Statistics, Second Edition

Shorack: Probability for Statisticians

Shumway and Stoffer: Time Series Analysis and Its Applications: With R Examples, Second Edition

Simonoff: Analyzing Categorical Data

Terrell: Mathematical Statistics: A Unified Introduction

Timm: Applied Multivariate Analysis

Toutenburg: Statistical Analysis of Designed Experiments, Second Edition

Wasserman: All of Nonparametric Statistics

Wasserman: All of Statistics: A Concise Course in Statistical Inference

Weiss: Modeling Longitudinal Data

Whittle: Probability via Expectation, Fourth Edition

Zacks: Introduction to Reliability Analysis: Probability Models and Statistical Methods